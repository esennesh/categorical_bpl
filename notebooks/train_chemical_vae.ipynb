{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eli/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='chemical_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch, len(data_loader.dataset.alphabet), data_loader.dataset.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 500,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer, log_images=False, log_step=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [256/118836 (0%)] Loss: 15532.825195\n",
      "Train Epoch: 1 [33024/118836 (28%)] Loss: 13649.475586\n",
      "Train Epoch: 1 [65792/118836 (55%)] Loss: 13283.418945\n",
      "Train Epoch: 1 [98560/118836 (83%)] Loss: 12956.165039\n",
      "    epoch          : 1\n",
      "    loss           : 13462.489185600187\n",
      "    val_loss       : 12778.562715682812\n",
      "    val_log_likelihood: -12651.409677257805\n",
      "    val_log_marginal: -12673.81804152637\n",
      "Train Epoch: 2 [256/118836 (0%)] Loss: 12761.605469\n",
      "Train Epoch: 2 [33024/118836 (28%)] Loss: 12650.145508\n",
      "Train Epoch: 2 [65792/118836 (55%)] Loss: 12640.375977\n",
      "Train Epoch: 2 [98560/118836 (83%)] Loss: 12667.584961\n",
      "    epoch          : 2\n",
      "    loss           : 12646.095470171631\n",
      "    val_loss       : 12618.526757684005\n",
      "    val_log_likelihood: -12576.564716772644\n",
      "    val_log_marginal: -12599.938644158414\n",
      "Train Epoch: 3 [256/118836 (0%)] Loss: 12641.577148\n",
      "Train Epoch: 3 [33024/118836 (28%)] Loss: 12652.793945\n",
      "Train Epoch: 3 [65792/118836 (55%)] Loss: 12627.540039\n",
      "Train Epoch: 3 [98560/118836 (83%)] Loss: 12610.354492\n",
      "    epoch          : 3\n",
      "    loss           : 12606.822964162015\n",
      "    val_loss       : 12592.67585207355\n",
      "    val_log_likelihood: -12569.1071792287\n",
      "    val_log_marginal: -12585.684240669389\n",
      "Train Epoch: 4 [256/118836 (0%)] Loss: 12599.845703\n",
      "Train Epoch: 4 [33024/118836 (28%)] Loss: 12652.062500\n",
      "Train Epoch: 4 [65792/118836 (55%)] Loss: 12549.418945\n",
      "Train Epoch: 4 [98560/118836 (83%)] Loss: 12615.637695\n",
      "    epoch          : 4\n",
      "    loss           : 12590.151348770936\n",
      "    val_loss       : 12587.28955834878\n",
      "    val_log_likelihood: -12567.976368156793\n",
      "    val_log_marginal: -12582.2468197438\n",
      "Train Epoch: 5 [256/118836 (0%)] Loss: 12594.946289\n",
      "Train Epoch: 5 [33024/118836 (28%)] Loss: 12626.419922\n",
      "Train Epoch: 5 [65792/118836 (55%)] Loss: 12613.227539\n",
      "Train Epoch: 5 [98560/118836 (83%)] Loss: 12632.992188\n",
      "    epoch          : 5\n",
      "    loss           : 12584.497491308673\n",
      "    val_loss       : 12583.06028246757\n",
      "    val_log_likelihood: -12566.50969696676\n",
      "    val_log_marginal: -12576.339917911204\n",
      "Train Epoch: 6 [256/118836 (0%)] Loss: 12547.168945\n",
      "Train Epoch: 6 [33024/118836 (28%)] Loss: 12622.691406\n",
      "Train Epoch: 6 [65792/118836 (55%)] Loss: 12672.593750\n",
      "Train Epoch: 6 [98560/118836 (83%)] Loss: 12589.217773\n",
      "    epoch          : 6\n",
      "    loss           : 12578.471785340416\n",
      "    val_loss       : 12575.411240830825\n",
      "    val_log_likelihood: -12564.223064160722\n",
      "    val_log_marginal: -12573.098047002524\n",
      "Train Epoch: 7 [256/118836 (0%)] Loss: 12549.711914\n",
      "Train Epoch: 7 [33024/118836 (28%)] Loss: 12556.237305\n",
      "Train Epoch: 7 [65792/118836 (55%)] Loss: 12602.178711\n",
      "Train Epoch: 7 [98560/118836 (83%)] Loss: 12623.989258\n",
      "    epoch          : 7\n",
      "    loss           : 12576.186715519023\n",
      "    val_loss       : 12581.959916931446\n",
      "    val_log_likelihood: -12564.657240940343\n",
      "    val_log_marginal: -12572.567114631845\n",
      "Train Epoch: 8 [256/118836 (0%)] Loss: 12591.174805\n",
      "Train Epoch: 8 [33024/118836 (28%)] Loss: 12572.029297\n",
      "Train Epoch: 8 [65792/118836 (55%)] Loss: 12606.750000\n",
      "Train Epoch: 8 [98560/118836 (83%)] Loss: 12571.462891\n",
      "    epoch          : 8\n",
      "    loss           : 12577.457566461177\n",
      "    val_loss       : 12575.043604388024\n",
      "    val_log_likelihood: -12564.875682543683\n",
      "    val_log_marginal: -12572.213655394267\n",
      "Train Epoch: 9 [256/118836 (0%)] Loss: 12594.348633\n",
      "Train Epoch: 9 [33024/118836 (28%)] Loss: 12626.890625\n",
      "Train Epoch: 9 [65792/118836 (55%)] Loss: 12568.146484\n",
      "Train Epoch: 9 [98560/118836 (83%)] Loss: 12580.575195\n",
      "    epoch          : 9\n",
      "    loss           : 12573.23831226737\n",
      "    val_loss       : 12571.521516990746\n",
      "    val_log_likelihood: -12563.776046997777\n",
      "    val_log_marginal: -12570.142843011907\n",
      "Train Epoch: 10 [256/118836 (0%)] Loss: 12534.013672\n",
      "Train Epoch: 10 [33024/118836 (28%)] Loss: 12564.443359\n",
      "Train Epoch: 10 [65792/118836 (55%)] Loss: 12571.830078\n",
      "Train Epoch: 10 [98560/118836 (83%)] Loss: 12586.880859\n",
      "    epoch          : 10\n",
      "    loss           : 12571.3715247622\n",
      "    val_loss       : 12571.102406433409\n",
      "    val_log_likelihood: -12563.420485195667\n",
      "    val_log_marginal: -12570.071935824826\n",
      "Train Epoch: 11 [256/118836 (0%)] Loss: 12574.722656\n",
      "Train Epoch: 11 [33024/118836 (28%)] Loss: 12590.296875\n",
      "Train Epoch: 11 [65792/118836 (55%)] Loss: 12534.947266\n",
      "Train Epoch: 11 [98560/118836 (83%)] Loss: 12623.132812\n",
      "    epoch          : 11\n",
      "    loss           : 12570.439255873915\n",
      "    val_loss       : 12569.835000523972\n",
      "    val_log_likelihood: -12563.341322406173\n",
      "    val_log_marginal: -12568.744368300453\n",
      "Train Epoch: 12 [256/118836 (0%)] Loss: 12584.597656\n",
      "Train Epoch: 12 [33024/118836 (28%)] Loss: 12606.731445\n",
      "Train Epoch: 12 [65792/118836 (55%)] Loss: 12578.968750\n",
      "Train Epoch: 12 [98560/118836 (83%)] Loss: 12625.169922\n",
      "    epoch          : 12\n",
      "    loss           : 12569.985236701303\n",
      "    val_loss       : 12569.652381349684\n",
      "    val_log_likelihood: -12563.408247389372\n",
      "    val_log_marginal: -12568.526434493591\n",
      "Train Epoch: 13 [256/118836 (0%)] Loss: 12566.693359\n",
      "Train Epoch: 13 [33024/118836 (28%)] Loss: 12562.965820\n",
      "Train Epoch: 13 [65792/118836 (55%)] Loss: 12607.166992\n",
      "Train Epoch: 13 [98560/118836 (83%)] Loss: 12536.121094\n",
      "    epoch          : 13\n",
      "    loss           : 12569.751959425403\n",
      "    val_loss       : 12568.849860915185\n",
      "    val_log_likelihood: -12563.15513062836\n",
      "    val_log_marginal: -12567.969805195851\n",
      "Train Epoch: 14 [256/118836 (0%)] Loss: 12609.890625\n",
      "Train Epoch: 14 [33024/118836 (28%)] Loss: 12544.218750\n",
      "Train Epoch: 14 [65792/118836 (55%)] Loss: 12599.441406\n",
      "Train Epoch: 14 [98560/118836 (83%)] Loss: 12608.033203\n",
      "    epoch          : 14\n",
      "    loss           : 12568.784162563328\n",
      "    val_loss       : 12568.508850385764\n",
      "    val_log_likelihood: -12563.102339226634\n",
      "    val_log_marginal: -12567.439075836508\n",
      "Train Epoch: 15 [256/118836 (0%)] Loss: 12579.768555\n",
      "Train Epoch: 15 [33024/118836 (28%)] Loss: 12578.291016\n",
      "Train Epoch: 15 [65792/118836 (55%)] Loss: 12578.458984\n",
      "Train Epoch: 15 [98560/118836 (83%)] Loss: 12596.933594\n",
      "    epoch          : 15\n",
      "    loss           : 12568.421671771608\n",
      "    val_loss       : 12569.409919113808\n",
      "    val_log_likelihood: -12563.19965525486\n",
      "    val_log_marginal: -12567.306278688666\n",
      "Train Epoch: 16 [256/118836 (0%)] Loss: 12561.697266\n",
      "Train Epoch: 16 [33024/118836 (28%)] Loss: 12582.348633\n",
      "Train Epoch: 16 [65792/118836 (55%)] Loss: 12600.554688\n",
      "Train Epoch: 16 [98560/118836 (83%)] Loss: 12607.919922\n",
      "    epoch          : 16\n",
      "    loss           : 12578.89651959264\n",
      "    val_loss       : 12623.77657874412\n",
      "    val_log_likelihood: -12561.214569601685\n",
      "    val_log_marginal: -12565.192764344732\n",
      "Train Epoch: 17 [256/118836 (0%)] Loss: 12663.822266\n",
      "Train Epoch: 17 [33024/118836 (28%)] Loss: 12616.792969\n",
      "Train Epoch: 17 [65792/118836 (55%)] Loss: 12623.329102\n",
      "Train Epoch: 17 [98560/118836 (83%)] Loss: 12582.933594\n",
      "    epoch          : 17\n",
      "    loss           : 12592.764752959574\n",
      "    val_loss       : 12564.291694105985\n",
      "    val_log_likelihood: -12544.361912447011\n",
      "    val_log_marginal: -12548.689427132766\n",
      "Train Epoch: 18 [256/118836 (0%)] Loss: 12594.425781\n",
      "Train Epoch: 18 [33024/118836 (28%)] Loss: 12563.095703\n",
      "Train Epoch: 18 [65792/118836 (55%)] Loss: 12573.887695\n",
      "Train Epoch: 18 [98560/118836 (83%)] Loss: 12594.178711\n",
      "    epoch          : 18\n",
      "    loss           : 12565.91123539599\n",
      "    val_loss       : 12574.059198773512\n",
      "    val_log_likelihood: -12543.006619300559\n",
      "    val_log_marginal: -12547.970346058057\n",
      "Train Epoch: 19 [256/118836 (0%)] Loss: 12622.928711\n",
      "Train Epoch: 19 [33024/118836 (28%)] Loss: 12604.072266\n",
      "Train Epoch: 19 [65792/118836 (55%)] Loss: 12586.604492\n",
      "Train Epoch: 19 [98560/118836 (83%)] Loss: 12595.587891\n",
      "    epoch          : 19\n",
      "    loss           : 12565.79503657465\n",
      "    val_loss       : 12565.800301725065\n",
      "    val_log_likelihood: -12540.843967444685\n",
      "    val_log_marginal: -12545.160131806166\n",
      "Train Epoch: 20 [256/118836 (0%)] Loss: 12593.204102\n",
      "Train Epoch: 20 [33024/118836 (28%)] Loss: 12595.720703\n",
      "Train Epoch: 20 [65792/118836 (55%)] Loss: 12625.182617\n",
      "Train Epoch: 20 [98560/118836 (83%)] Loss: 12643.458984\n",
      "    epoch          : 20\n",
      "    loss           : 12569.480356312035\n",
      "    val_loss       : 12561.733013385825\n",
      "    val_log_likelihood: -12534.569794412997\n",
      "    val_log_marginal: -12539.858285888638\n",
      "Train Epoch: 21 [256/118836 (0%)] Loss: 12564.272461\n",
      "Train Epoch: 21 [33024/118836 (28%)] Loss: 12570.441406\n",
      "Train Epoch: 21 [65792/118836 (55%)] Loss: 12570.710938\n",
      "Train Epoch: 21 [98560/118836 (83%)] Loss: 12614.861328\n",
      "    epoch          : 21\n",
      "    loss           : 12558.348895329302\n",
      "    val_loss       : 12554.380620504895\n",
      "    val_log_likelihood: -12515.1507001525\n",
      "    val_log_marginal: -12519.735966095655\n",
      "Train Epoch: 22 [256/118836 (0%)] Loss: 12580.019531\n",
      "Train Epoch: 22 [33024/118836 (28%)] Loss: 12520.011719\n",
      "Train Epoch: 22 [65792/118836 (55%)] Loss: 12626.822266\n",
      "Train Epoch: 22 [98560/118836 (83%)] Loss: 12576.433594\n",
      "    epoch          : 22\n",
      "    loss           : 12559.228740500932\n",
      "    val_loss       : 12576.489884883998\n",
      "    val_log_likelihood: -12526.811918101219\n",
      "    val_log_marginal: -12532.386931077273\n",
      "Train Epoch: 23 [256/118836 (0%)] Loss: 12574.974609\n",
      "Train Epoch: 23 [33024/118836 (28%)] Loss: 12595.212891\n",
      "Train Epoch: 23 [65792/118836 (55%)] Loss: 12579.137695\n",
      "Train Epoch: 23 [98560/118836 (83%)] Loss: 12507.241211\n",
      "    epoch          : 23\n",
      "    loss           : 12552.544278103029\n",
      "    val_loss       : 12551.708357266301\n",
      "    val_log_likelihood: -12510.774378198666\n",
      "    val_log_marginal: -12517.935752658905\n",
      "Train Epoch: 24 [256/118836 (0%)] Loss: 12555.711914\n",
      "Train Epoch: 24 [33024/118836 (28%)] Loss: 12582.597656\n",
      "Train Epoch: 24 [65792/118836 (55%)] Loss: 12586.142578\n",
      "Train Epoch: 24 [98560/118836 (83%)] Loss: 12541.064453\n",
      "    epoch          : 24\n",
      "    loss           : 12537.364876544407\n",
      "    val_loss       : 12522.374587875021\n",
      "    val_log_likelihood: -12467.566520626551\n",
      "    val_log_marginal: -12474.733987490463\n",
      "Train Epoch: 25 [256/118836 (0%)] Loss: 12536.493164\n",
      "Train Epoch: 25 [33024/118836 (28%)] Loss: 12582.669922\n",
      "Train Epoch: 25 [65792/118836 (55%)] Loss: 12557.465820\n",
      "Train Epoch: 25 [98560/118836 (83%)] Loss: 12542.285156\n",
      "    epoch          : 25\n",
      "    loss           : 12516.655519799422\n",
      "    val_loss       : 12514.844202067838\n",
      "    val_log_likelihood: -12472.294098137665\n",
      "    val_log_marginal: -12479.518314647245\n",
      "Train Epoch: 26 [256/118836 (0%)] Loss: 12535.425781\n",
      "Train Epoch: 26 [33024/118836 (28%)] Loss: 12617.503906\n",
      "Train Epoch: 26 [65792/118836 (55%)] Loss: 12468.257812\n",
      "Train Epoch: 26 [98560/118836 (83%)] Loss: 12511.545898\n",
      "    epoch          : 26\n",
      "    loss           : 12520.387287563328\n",
      "    val_loss       : 12501.569608255\n",
      "    val_log_likelihood: -12443.79094971309\n",
      "    val_log_marginal: -12450.734492456271\n",
      "Train Epoch: 27 [256/118836 (0%)] Loss: 12499.527344\n",
      "Train Epoch: 27 [33024/118836 (28%)] Loss: 12492.666992\n",
      "Train Epoch: 27 [65792/118836 (55%)] Loss: 12554.557617\n",
      "Train Epoch: 27 [98560/118836 (83%)] Loss: 12536.738281\n",
      "    epoch          : 27\n",
      "    loss           : 12509.578634686466\n",
      "    val_loss       : 12505.37337220823\n",
      "    val_log_likelihood: -12460.710285812138\n",
      "    val_log_marginal: -12467.28364584726\n",
      "Train Epoch: 28 [256/118836 (0%)] Loss: 12506.008789\n",
      "Train Epoch: 28 [33024/118836 (28%)] Loss: 12497.011719\n",
      "Train Epoch: 28 [65792/118836 (55%)] Loss: 12572.460938\n",
      "Train Epoch: 28 [98560/118836 (83%)] Loss: 12596.283203\n",
      "    epoch          : 28\n",
      "    loss           : 12494.143711389837\n",
      "    val_loss       : 12497.498372338545\n",
      "    val_log_likelihood: -12446.932969202338\n",
      "    val_log_marginal: -12453.556555263525\n",
      "Train Epoch: 29 [256/118836 (0%)] Loss: 12558.188477\n",
      "Train Epoch: 29 [33024/118836 (28%)] Loss: 12508.804688\n",
      "Train Epoch: 29 [65792/118836 (55%)] Loss: 12513.403320\n",
      "Train Epoch: 29 [98560/118836 (83%)] Loss: 12544.089844\n",
      "    epoch          : 29\n",
      "    loss           : 12498.768834166925\n",
      "    val_loss       : 12508.056190959236\n",
      "    val_log_likelihood: -12437.626855872622\n",
      "    val_log_marginal: -12444.029329045386\n",
      "Train Epoch: 30 [256/118836 (0%)] Loss: 12511.810547\n",
      "Train Epoch: 30 [33024/118836 (28%)] Loss: 12481.958984\n",
      "Train Epoch: 30 [65792/118836 (55%)] Loss: 12505.201172\n",
      "Train Epoch: 30 [98560/118836 (83%)] Loss: 12675.395508\n",
      "    epoch          : 30\n",
      "    loss           : 12501.509795673077\n",
      "    val_loss       : 12491.069681907542\n",
      "    val_log_likelihood: -12443.771636877069\n",
      "    val_log_marginal: -12450.227736535297\n",
      "Train Epoch: 31 [256/118836 (0%)] Loss: 12589.997070\n",
      "Train Epoch: 31 [33024/118836 (28%)] Loss: 12493.355469\n",
      "Train Epoch: 31 [65792/118836 (55%)] Loss: 12491.152344\n",
      "Train Epoch: 31 [98560/118836 (83%)] Loss: 12466.415039\n",
      "    epoch          : 31\n",
      "    loss           : 12480.886572871434\n",
      "    val_loss       : 12479.331052001398\n",
      "    val_log_likelihood: -12411.099204533706\n",
      "    val_log_marginal: -12417.468977815039\n",
      "Train Epoch: 32 [256/118836 (0%)] Loss: 12541.945312\n",
      "Train Epoch: 32 [33024/118836 (28%)] Loss: 12491.578125\n",
      "Train Epoch: 32 [65792/118836 (55%)] Loss: 12442.613281\n",
      "Train Epoch: 32 [98560/118836 (83%)] Loss: 12492.978516\n",
      "    epoch          : 32\n",
      "    loss           : 12484.581995547715\n",
      "    val_loss       : 12535.362120426158\n",
      "    val_log_likelihood: -12438.190672495348\n",
      "    val_log_marginal: -12444.474834359218\n",
      "Train Epoch: 33 [256/118836 (0%)] Loss: 12697.855469\n",
      "Train Epoch: 33 [33024/118836 (28%)] Loss: 12622.593750\n",
      "Train Epoch: 33 [65792/118836 (55%)] Loss: 12489.528320\n",
      "Train Epoch: 33 [98560/118836 (83%)] Loss: 12543.355469\n",
      "    epoch          : 33\n",
      "    loss           : 12486.587107274865\n",
      "    val_loss       : 12477.785813493716\n",
      "    val_log_likelihood: -12410.774026668476\n",
      "    val_log_marginal: -12417.117775383813\n",
      "Train Epoch: 34 [256/118836 (0%)] Loss: 12547.689453\n",
      "Train Epoch: 34 [33024/118836 (28%)] Loss: 12409.710938\n",
      "Train Epoch: 34 [65792/118836 (55%)] Loss: 12512.558594\n",
      "Train Epoch: 34 [98560/118836 (83%)] Loss: 12542.393555\n",
      "    epoch          : 34\n",
      "    loss           : 12483.458210879342\n",
      "    val_loss       : 12481.23146539174\n",
      "    val_log_likelihood: -12421.242237095483\n",
      "    val_log_marginal: -12427.754851172791\n",
      "Train Epoch: 35 [256/118836 (0%)] Loss: 12510.960938\n",
      "Train Epoch: 35 [33024/118836 (28%)] Loss: 12521.624023\n",
      "Train Epoch: 35 [65792/118836 (55%)] Loss: 12476.851562\n",
      "Train Epoch: 35 [98560/118836 (83%)] Loss: 12511.650391\n",
      "    epoch          : 35\n",
      "    loss           : 12482.581619785205\n",
      "    val_loss       : 12535.788642824748\n",
      "    val_log_likelihood: -12477.245237702904\n",
      "    val_log_marginal: -12483.420129688635\n",
      "Train Epoch: 36 [256/118836 (0%)] Loss: 12614.283203\n",
      "Train Epoch: 36 [33024/118836 (28%)] Loss: 12546.933594\n",
      "Train Epoch: 36 [65792/118836 (55%)] Loss: 12481.599609\n",
      "Train Epoch: 36 [98560/118836 (83%)] Loss: 12522.495117\n",
      "    epoch          : 36\n",
      "    loss           : 12478.229897028794\n",
      "    val_loss       : 12470.508504494708\n",
      "    val_log_likelihood: -12406.244114777191\n",
      "    val_log_marginal: -12412.283872636048\n",
      "Train Epoch: 37 [256/118836 (0%)] Loss: 12477.423828\n",
      "Train Epoch: 37 [33024/118836 (28%)] Loss: 12458.160156\n",
      "Train Epoch: 37 [65792/118836 (55%)] Loss: 12496.474609\n",
      "Train Epoch: 37 [98560/118836 (83%)] Loss: 12436.282227\n",
      "    epoch          : 37\n",
      "    loss           : 12476.768720113472\n",
      "    val_loss       : 12483.478010360695\n",
      "    val_log_likelihood: -12426.825884479684\n",
      "    val_log_marginal: -12432.944884253611\n",
      "Train Epoch: 38 [256/118836 (0%)] Loss: 12528.463867\n",
      "Train Epoch: 38 [33024/118836 (28%)] Loss: 12584.201172\n",
      "Train Epoch: 38 [65792/118836 (55%)] Loss: 12415.197266\n",
      "Train Epoch: 38 [98560/118836 (83%)] Loss: 12489.334961\n",
      "    epoch          : 38\n",
      "    loss           : 12476.850235215054\n",
      "    val_loss       : 12481.948318273353\n",
      "    val_log_likelihood: -12416.652149891439\n",
      "    val_log_marginal: -12422.575176830696\n",
      "Train Epoch: 39 [256/118836 (0%)] Loss: 12549.751953\n",
      "Train Epoch: 39 [33024/118836 (28%)] Loss: 12445.146484\n",
      "Train Epoch: 39 [65792/118836 (55%)] Loss: 12490.992188\n",
      "Train Epoch: 39 [98560/118836 (83%)] Loss: 12542.858398\n",
      "    epoch          : 39\n",
      "    loss           : 12472.910409720067\n",
      "    val_loss       : 12467.816539131334\n",
      "    val_log_likelihood: -12402.391605762768\n",
      "    val_log_marginal: -12408.592542127046\n",
      "Train Epoch: 40 [256/118836 (0%)] Loss: 12492.291992\n",
      "Train Epoch: 40 [33024/118836 (28%)] Loss: 12480.687500\n",
      "Train Epoch: 40 [65792/118836 (55%)] Loss: 12496.839844\n",
      "Train Epoch: 40 [98560/118836 (83%)] Loss: 12521.445312\n",
      "    epoch          : 40\n",
      "    loss           : 12471.237278355045\n",
      "    val_loss       : 12466.824065326698\n",
      "    val_log_likelihood: -12393.922818121899\n",
      "    val_log_marginal: -12399.913415507235\n",
      "Train Epoch: 41 [256/118836 (0%)] Loss: 12472.077148\n",
      "Train Epoch: 41 [33024/118836 (28%)] Loss: 12552.807617\n",
      "Train Epoch: 41 [65792/118836 (55%)] Loss: 12454.185547\n",
      "Train Epoch: 41 [98560/118836 (83%)] Loss: 12569.057617\n",
      "    epoch          : 41\n",
      "    loss           : 12469.541657781483\n",
      "    val_loss       : 12468.960321505461\n",
      "    val_log_likelihood: -12397.879864880582\n",
      "    val_log_marginal: -12404.02789620819\n",
      "Train Epoch: 42 [256/118836 (0%)] Loss: 12520.008789\n",
      "Train Epoch: 42 [33024/118836 (28%)] Loss: 12536.789062\n",
      "Train Epoch: 42 [65792/118836 (55%)] Loss: 12545.256836\n",
      "Train Epoch: 42 [98560/118836 (83%)] Loss: 12445.485352\n",
      "    epoch          : 42\n",
      "    loss           : 12465.66628250362\n",
      "    val_loss       : 12459.605463305788\n",
      "    val_log_likelihood: -12389.969476646505\n",
      "    val_log_marginal: -12396.031063310937\n",
      "Train Epoch: 43 [256/118836 (0%)] Loss: 12521.812500\n",
      "Train Epoch: 43 [33024/118836 (28%)] Loss: 12528.439453\n",
      "Train Epoch: 43 [65792/118836 (55%)] Loss: 12493.891602\n",
      "Train Epoch: 43 [98560/118836 (83%)] Loss: 12489.728516\n",
      "    epoch          : 43\n",
      "    loss           : 12462.080750814206\n",
      "    val_loss       : 12461.489621227087\n",
      "    val_log_likelihood: -12385.559844460815\n",
      "    val_log_marginal: -12391.607344873111\n",
      "Train Epoch: 44 [256/118836 (0%)] Loss: 12496.148438\n",
      "Train Epoch: 44 [33024/118836 (28%)] Loss: 12522.392578\n",
      "Train Epoch: 44 [65792/118836 (55%)] Loss: 12440.646484\n",
      "Train Epoch: 44 [98560/118836 (83%)] Loss: 12526.068359\n",
      "    epoch          : 44\n",
      "    loss           : 12462.532430437088\n",
      "    val_loss       : 12463.258585022708\n",
      "    val_log_likelihood: -12387.433607804745\n",
      "    val_log_marginal: -12393.527857029876\n",
      "Train Epoch: 45 [256/118836 (0%)] Loss: 12520.552734\n",
      "Train Epoch: 45 [33024/118836 (28%)] Loss: 12460.971680\n",
      "Train Epoch: 45 [65792/118836 (55%)] Loss: 12375.511719\n",
      "Train Epoch: 45 [98560/118836 (83%)] Loss: 12415.126953\n",
      "    epoch          : 45\n",
      "    loss           : 12451.284126376397\n",
      "    val_loss       : 12449.402507724302\n",
      "    val_log_likelihood: -12363.246453196081\n",
      "    val_log_marginal: -12369.21134903531\n",
      "Train Epoch: 46 [256/118836 (0%)] Loss: 12526.186523\n",
      "Train Epoch: 46 [33024/118836 (28%)] Loss: 12466.998047\n",
      "Train Epoch: 46 [65792/118836 (55%)] Loss: 12515.277344\n",
      "Train Epoch: 46 [98560/118836 (83%)] Loss: 12441.842773\n",
      "    epoch          : 46\n",
      "    loss           : 12454.817417222393\n",
      "    val_loss       : 12448.728506581545\n",
      "    val_log_likelihood: -12372.70754319815\n",
      "    val_log_marginal: -12378.83131530195\n",
      "Train Epoch: 47 [256/118836 (0%)] Loss: 12459.273438\n",
      "Train Epoch: 47 [33024/118836 (28%)] Loss: 12458.079102\n",
      "Train Epoch: 47 [65792/118836 (55%)] Loss: 12489.160156\n",
      "Train Epoch: 47 [98560/118836 (83%)] Loss: 12465.754883\n",
      "    epoch          : 47\n",
      "    loss           : 12447.919704107217\n",
      "    val_loss       : 12445.973346775916\n",
      "    val_log_likelihood: -12362.182946423956\n",
      "    val_log_marginal: -12368.531167828482\n",
      "Train Epoch: 48 [256/118836 (0%)] Loss: 12454.695312\n",
      "Train Epoch: 48 [33024/118836 (28%)] Loss: 12477.076172\n",
      "Train Epoch: 48 [65792/118836 (55%)] Loss: 12473.079102\n",
      "Train Epoch: 48 [98560/118836 (83%)] Loss: 12503.987305\n",
      "    epoch          : 48\n",
      "    loss           : 12449.173748804538\n",
      "    val_loss       : 12442.341677551238\n",
      "    val_log_likelihood: -12361.492888783343\n",
      "    val_log_marginal: -12367.81454611804\n",
      "Train Epoch: 49 [256/118836 (0%)] Loss: 12496.421875\n",
      "Train Epoch: 49 [33024/118836 (28%)] Loss: 12420.687500\n",
      "Train Epoch: 49 [65792/118836 (55%)] Loss: 12394.283203\n",
      "Train Epoch: 49 [98560/118836 (83%)] Loss: 12488.514648\n",
      "    epoch          : 49\n",
      "    loss           : 12446.389023082093\n",
      "    val_loss       : 12446.291896239414\n",
      "    val_log_likelihood: -12361.833960465778\n",
      "    val_log_marginal: -12368.072082216539\n",
      "Train Epoch: 50 [256/118836 (0%)] Loss: 12492.639648\n",
      "Train Epoch: 50 [33024/118836 (28%)] Loss: 12435.453125\n",
      "Train Epoch: 50 [65792/118836 (55%)] Loss: 12418.845703\n",
      "Train Epoch: 50 [98560/118836 (83%)] Loss: 12517.390625\n",
      "    epoch          : 50\n",
      "    loss           : 12443.358144967433\n",
      "    val_loss       : 12436.586565963347\n",
      "    val_log_likelihood: -12349.605966804951\n",
      "    val_log_marginal: -12356.134076605036\n",
      "Train Epoch: 51 [256/118836 (0%)] Loss: 12426.703125\n",
      "Train Epoch: 51 [33024/118836 (28%)] Loss: 12512.942383\n",
      "Train Epoch: 51 [65792/118836 (55%)] Loss: 12534.832031\n",
      "Train Epoch: 51 [98560/118836 (83%)] Loss: 12373.041016\n",
      "    epoch          : 51\n",
      "    loss           : 12437.176798522796\n",
      "    val_loss       : 12431.256181866942\n",
      "    val_log_likelihood: -12347.137096935743\n",
      "    val_log_marginal: -12353.85846966096\n",
      "Train Epoch: 52 [256/118836 (0%)] Loss: 12380.071289\n",
      "Train Epoch: 52 [33024/118836 (28%)] Loss: 12433.810547\n",
      "Train Epoch: 52 [65792/118836 (55%)] Loss: 12502.547852\n",
      "Train Epoch: 52 [98560/118836 (83%)] Loss: 12502.206055\n",
      "    epoch          : 52\n",
      "    loss           : 12429.15694756772\n",
      "    val_loss       : 12431.581971137008\n",
      "    val_log_likelihood: -12347.469587468982\n",
      "    val_log_marginal: -12354.112703861983\n",
      "Train Epoch: 53 [256/118836 (0%)] Loss: 12382.278320\n",
      "Train Epoch: 53 [33024/118836 (28%)] Loss: 12513.938477\n",
      "Train Epoch: 53 [65792/118836 (55%)] Loss: 12448.384766\n",
      "Train Epoch: 53 [98560/118836 (83%)] Loss: 12444.255859\n",
      "    epoch          : 53\n",
      "    loss           : 12430.527757314929\n",
      "    val_loss       : 12434.31247162953\n",
      "    val_log_likelihood: -12345.705157445462\n",
      "    val_log_marginal: -12352.109010335862\n",
      "Train Epoch: 54 [256/118836 (0%)] Loss: 12502.867188\n",
      "Train Epoch: 54 [33024/118836 (28%)] Loss: 12453.530273\n",
      "Train Epoch: 54 [65792/118836 (55%)] Loss: 12424.434570\n",
      "Train Epoch: 54 [98560/118836 (83%)] Loss: 12489.583984\n",
      "    epoch          : 54\n",
      "    loss           : 12430.12779770213\n",
      "    val_loss       : 12436.97962156471\n",
      "    val_log_likelihood: -12343.935336376913\n",
      "    val_log_marginal: -12351.538365540804\n",
      "Train Epoch: 55 [256/118836 (0%)] Loss: 12333.458008\n",
      "Train Epoch: 55 [33024/118836 (28%)] Loss: 12387.384766\n",
      "Train Epoch: 55 [65792/118836 (55%)] Loss: 12439.361328\n",
      "Train Epoch: 55 [98560/118836 (83%)] Loss: 12397.251953\n",
      "    epoch          : 55\n",
      "    loss           : 12431.333940595274\n",
      "    val_loss       : 12414.21053508663\n",
      "    val_log_likelihood: -12322.640074603236\n",
      "    val_log_marginal: -12329.737848074567\n",
      "Train Epoch: 56 [256/118836 (0%)] Loss: 12517.394531\n",
      "Train Epoch: 56 [33024/118836 (28%)] Loss: 12490.277344\n",
      "Train Epoch: 56 [65792/118836 (55%)] Loss: 12462.410156\n",
      "Train Epoch: 56 [98560/118836 (83%)] Loss: 12486.355469\n",
      "    epoch          : 56\n",
      "    loss           : 12412.695426068807\n",
      "    val_loss       : 12410.395524150554\n",
      "    val_log_likelihood: -12319.779178621278\n",
      "    val_log_marginal: -12326.709979161546\n",
      "Train Epoch: 57 [256/118836 (0%)] Loss: 12417.060547\n",
      "Train Epoch: 57 [33024/118836 (28%)] Loss: 12389.414062\n",
      "Train Epoch: 57 [65792/118836 (55%)] Loss: 12455.983398\n",
      "Train Epoch: 57 [98560/118836 (83%)] Loss: 12498.302734\n",
      "    epoch          : 57\n",
      "    loss           : 12409.254708178247\n",
      "    val_loss       : 12404.619365027625\n",
      "    val_log_likelihood: -12311.03680097834\n",
      "    val_log_marginal: -12317.78142392751\n",
      "Train Epoch: 58 [256/118836 (0%)] Loss: 12393.212891\n",
      "Train Epoch: 58 [33024/118836 (28%)] Loss: 12354.906250\n",
      "Train Epoch: 58 [65792/118836 (55%)] Loss: 12455.541016\n",
      "Train Epoch: 58 [98560/118836 (83%)] Loss: 12500.433594\n",
      "    epoch          : 58\n",
      "    loss           : 12406.835281773418\n",
      "    val_loss       : 12409.404084973521\n",
      "    val_log_likelihood: -12305.620845287944\n",
      "    val_log_marginal: -12312.367545379502\n",
      "Train Epoch: 59 [256/118836 (0%)] Loss: 12428.455078\n",
      "Train Epoch: 59 [33024/118836 (28%)] Loss: 12458.300781\n",
      "Train Epoch: 59 [65792/118836 (55%)] Loss: 12461.680664\n",
      "Train Epoch: 59 [98560/118836 (83%)] Loss: 12443.193359\n",
      "    epoch          : 59\n",
      "    loss           : 12406.84718872777\n",
      "    val_loss       : 12402.389964985643\n",
      "    val_log_likelihood: -12301.390450688845\n",
      "    val_log_marginal: -12308.019950379283\n",
      "Train Epoch: 60 [256/118836 (0%)] Loss: 12455.529297\n",
      "Train Epoch: 60 [33024/118836 (28%)] Loss: 12529.947266\n",
      "Train Epoch: 60 [65792/118836 (55%)] Loss: 12479.846680\n",
      "Train Epoch: 60 [98560/118836 (83%)] Loss: 12365.709961\n",
      "    epoch          : 60\n",
      "    loss           : 12405.157303136632\n",
      "    val_loss       : 12402.273527393736\n",
      "    val_log_likelihood: -12299.159678550197\n",
      "    val_log_marginal: -12305.840291945602\n",
      "Train Epoch: 61 [256/118836 (0%)] Loss: 12540.601562\n",
      "Train Epoch: 61 [33024/118836 (28%)] Loss: 12382.369141\n",
      "Train Epoch: 61 [65792/118836 (55%)] Loss: 12492.123047\n",
      "Train Epoch: 61 [98560/118836 (83%)] Loss: 12437.041016\n",
      "    epoch          : 61\n",
      "    loss           : 12404.055646292132\n",
      "    val_loss       : 12400.8758695745\n",
      "    val_log_likelihood: -12298.85890763932\n",
      "    val_log_marginal: -12305.45883961272\n",
      "Train Epoch: 62 [256/118836 (0%)] Loss: 12443.545898\n",
      "Train Epoch: 62 [33024/118836 (28%)] Loss: 12483.419922\n",
      "Train Epoch: 62 [65792/118836 (55%)] Loss: 12377.692383\n",
      "Train Epoch: 62 [98560/118836 (83%)] Loss: 12363.175781\n",
      "    epoch          : 62\n",
      "    loss           : 12404.84485596309\n",
      "    val_loss       : 12399.629245002317\n",
      "    val_log_likelihood: -12297.109710698409\n",
      "    val_log_marginal: -12303.777977860893\n",
      "Train Epoch: 63 [256/118836 (0%)] Loss: 12448.312500\n",
      "Train Epoch: 63 [33024/118836 (28%)] Loss: 12454.274414\n",
      "Train Epoch: 63 [65792/118836 (55%)] Loss: 12404.079102\n",
      "Train Epoch: 63 [98560/118836 (83%)] Loss: 12428.986328\n",
      "    epoch          : 63\n",
      "    loss           : 12393.768692488627\n",
      "    val_loss       : 12387.427050426082\n",
      "    val_log_likelihood: -12286.462020846258\n",
      "    val_log_marginal: -12293.235402312766\n",
      "Train Epoch: 64 [256/118836 (0%)] Loss: 12396.597656\n",
      "Train Epoch: 64 [33024/118836 (28%)] Loss: 12371.973633\n",
      "Train Epoch: 64 [65792/118836 (55%)] Loss: 12365.383789\n",
      "Train Epoch: 64 [98560/118836 (83%)] Loss: 12359.000000\n",
      "    epoch          : 64\n",
      "    loss           : 12388.938074467536\n",
      "    val_loss       : 12385.402919296548\n",
      "    val_log_likelihood: -12283.236517137097\n",
      "    val_log_marginal: -12290.03258804109\n",
      "Train Epoch: 65 [256/118836 (0%)] Loss: 12357.605469\n",
      "Train Epoch: 65 [33024/118836 (28%)] Loss: 12343.187500\n",
      "Train Epoch: 65 [65792/118836 (55%)] Loss: 12370.546875\n",
      "Train Epoch: 65 [98560/118836 (83%)] Loss: 12417.345703\n",
      "    epoch          : 65\n",
      "    loss           : 12390.154246310225\n",
      "    val_loss       : 12386.054098169217\n",
      "    val_log_likelihood: -12289.280677955698\n",
      "    val_log_marginal: -12296.002815180498\n",
      "Train Epoch: 66 [256/118836 (0%)] Loss: 12419.633789\n",
      "Train Epoch: 66 [33024/118836 (28%)] Loss: 12445.358398\n",
      "Train Epoch: 66 [65792/118836 (55%)] Loss: 12434.396484\n",
      "Train Epoch: 66 [98560/118836 (83%)] Loss: 12404.299805\n",
      "    epoch          : 66\n",
      "    loss           : 12386.981474068187\n",
      "    val_loss       : 12387.190030823245\n",
      "    val_log_likelihood: -12282.119758226065\n",
      "    val_log_marginal: -12288.79481994466\n",
      "Train Epoch: 67 [256/118836 (0%)] Loss: 12313.339844\n",
      "Train Epoch: 67 [33024/118836 (28%)] Loss: 12427.857422\n",
      "Train Epoch: 67 [65792/118836 (55%)] Loss: 12372.478516\n",
      "Train Epoch: 67 [98560/118836 (83%)] Loss: 12404.278320\n",
      "    epoch          : 67\n",
      "    loss           : 12381.214441493486\n",
      "    val_loss       : 12387.776053985004\n",
      "    val_log_likelihood: -12285.410013763956\n",
      "    val_log_marginal: -12292.01235245085\n",
      "Train Epoch: 68 [256/118836 (0%)] Loss: 12389.699219\n",
      "Train Epoch: 68 [33024/118836 (28%)] Loss: 12390.017578\n",
      "Train Epoch: 68 [65792/118836 (55%)] Loss: 12480.623047\n",
      "Train Epoch: 68 [98560/118836 (83%)] Loss: 12319.298828\n",
      "    epoch          : 68\n",
      "    loss           : 12386.82313168812\n",
      "    val_loss       : 12388.872671199571\n",
      "    val_log_likelihood: -12285.462394185537\n",
      "    val_log_marginal: -12291.976727147367\n",
      "Train Epoch: 69 [256/118836 (0%)] Loss: 12376.560547\n",
      "Train Epoch: 69 [33024/118836 (28%)] Loss: 12489.796875\n",
      "Train Epoch: 69 [65792/118836 (55%)] Loss: 12372.398438\n",
      "Train Epoch: 69 [98560/118836 (83%)] Loss: 12343.039062\n",
      "    epoch          : 69\n",
      "    loss           : 12385.091067320616\n",
      "    val_loss       : 12376.864742393247\n",
      "    val_log_likelihood: -12286.474365436312\n",
      "    val_log_marginal: -12293.362988873863\n",
      "Train Epoch: 70 [256/118836 (0%)] Loss: 12484.929688\n",
      "Train Epoch: 70 [33024/118836 (28%)] Loss: 12330.641602\n",
      "Train Epoch: 70 [65792/118836 (55%)] Loss: 12351.996094\n",
      "Train Epoch: 70 [98560/118836 (83%)] Loss: 12424.595703\n",
      "    epoch          : 70\n",
      "    loss           : 12376.237573666254\n",
      "    val_loss       : 12372.367847040081\n",
      "    val_log_likelihood: -12277.837360260288\n",
      "    val_log_marginal: -12284.582271214345\n",
      "Train Epoch: 71 [256/118836 (0%)] Loss: 12489.695312\n",
      "Train Epoch: 71 [33024/118836 (28%)] Loss: 12358.644531\n",
      "Train Epoch: 71 [65792/118836 (55%)] Loss: 12418.035156\n",
      "Train Epoch: 71 [98560/118836 (83%)] Loss: 12471.667969\n",
      "    epoch          : 71\n",
      "    loss           : 12374.308536723274\n",
      "    val_loss       : 12371.492962899512\n",
      "    val_log_likelihood: -12277.542263104839\n",
      "    val_log_marginal: -12284.260037312404\n",
      "Train Epoch: 72 [256/118836 (0%)] Loss: 12392.112305\n",
      "Train Epoch: 72 [33024/118836 (28%)] Loss: 12328.548828\n",
      "Train Epoch: 72 [65792/118836 (55%)] Loss: 12377.570312\n",
      "Train Epoch: 72 [98560/118836 (83%)] Loss: 12408.095703\n",
      "    epoch          : 72\n",
      "    loss           : 12371.046435102617\n",
      "    val_loss       : 12368.031833947083\n",
      "    val_log_likelihood: -12276.708244643041\n",
      "    val_log_marginal: -12283.504647994121\n",
      "Train Epoch: 73 [256/118836 (0%)] Loss: 12373.616211\n",
      "Train Epoch: 73 [33024/118836 (28%)] Loss: 12357.111328\n",
      "Train Epoch: 73 [65792/118836 (55%)] Loss: 12308.423828\n",
      "Train Epoch: 73 [98560/118836 (83%)] Loss: 12328.730469\n",
      "    epoch          : 73\n",
      "    loss           : 12371.003522894696\n",
      "    val_loss       : 12370.866854436015\n",
      "    val_log_likelihood: -12275.710526842948\n",
      "    val_log_marginal: -12282.52618968212\n",
      "Train Epoch: 74 [256/118836 (0%)] Loss: 12305.913086\n",
      "Train Epoch: 74 [33024/118836 (28%)] Loss: 12299.203125\n",
      "Train Epoch: 74 [65792/118836 (55%)] Loss: 12255.962891\n",
      "Train Epoch: 74 [98560/118836 (83%)] Loss: 12441.326172\n",
      "    epoch          : 74\n",
      "    loss           : 12370.324766723532\n",
      "    val_loss       : 12369.10548716272\n",
      "    val_log_likelihood: -12273.084870147075\n",
      "    val_log_marginal: -12279.81226559173\n",
      "Train Epoch: 75 [256/118836 (0%)] Loss: 12460.152344\n",
      "Train Epoch: 75 [33024/118836 (28%)] Loss: 12448.791016\n",
      "Train Epoch: 75 [65792/118836 (55%)] Loss: 12282.343750\n",
      "Train Epoch: 75 [98560/118836 (83%)] Loss: 12478.007812\n",
      "    epoch          : 75\n",
      "    loss           : 12367.876271873709\n",
      "    val_loss       : 12363.08738171139\n",
      "    val_log_likelihood: -12272.994441590416\n",
      "    val_log_marginal: -12279.88845671126\n",
      "Train Epoch: 76 [256/118836 (0%)] Loss: 12406.367188\n",
      "Train Epoch: 76 [33024/118836 (28%)] Loss: 12320.510742\n",
      "Train Epoch: 76 [65792/118836 (55%)] Loss: 12406.495117\n",
      "Train Epoch: 76 [98560/118836 (83%)] Loss: 12368.289062\n",
      "    epoch          : 76\n",
      "    loss           : 12366.738739563947\n",
      "    val_loss       : 12374.802063692556\n",
      "    val_log_likelihood: -12278.307702485268\n",
      "    val_log_marginal: -12285.21386867982\n",
      "Train Epoch: 77 [256/118836 (0%)] Loss: 12391.251953\n",
      "Train Epoch: 77 [33024/118836 (28%)] Loss: 12308.822266\n",
      "Train Epoch: 77 [65792/118836 (55%)] Loss: 12326.360352\n",
      "Train Epoch: 77 [98560/118836 (83%)] Loss: 12425.808594\n",
      "    epoch          : 77\n",
      "    loss           : 12363.988338438276\n",
      "    val_loss       : 12367.889500976386\n",
      "    val_log_likelihood: -12273.530412046372\n",
      "    val_log_marginal: -12280.30682878466\n",
      "Train Epoch: 78 [256/118836 (0%)] Loss: 12413.044922\n",
      "Train Epoch: 78 [33024/118836 (28%)] Loss: 12381.734375\n",
      "Train Epoch: 78 [65792/118836 (55%)] Loss: 12386.338867\n",
      "Train Epoch: 78 [98560/118836 (83%)] Loss: 12365.682617\n",
      "    epoch          : 78\n",
      "    loss           : 12368.777771046578\n",
      "    val_loss       : 12366.610272584514\n",
      "    val_log_likelihood: -12275.492352764422\n",
      "    val_log_marginal: -12282.273845870248\n",
      "Train Epoch: 79 [256/118836 (0%)] Loss: 12402.355469\n",
      "Train Epoch: 79 [33024/118836 (28%)] Loss: 12433.612305\n",
      "Train Epoch: 79 [65792/118836 (55%)] Loss: 12380.888672\n",
      "Train Epoch: 79 [98560/118836 (83%)] Loss: 12447.229492\n",
      "    epoch          : 79\n",
      "    loss           : 12365.57783808933\n",
      "    val_loss       : 12369.94640542333\n",
      "    val_log_likelihood: -12273.052775408396\n",
      "    val_log_marginal: -12279.905512838173\n",
      "Train Epoch: 80 [256/118836 (0%)] Loss: 12391.697266\n",
      "Train Epoch: 80 [33024/118836 (28%)] Loss: 12413.408203\n",
      "Train Epoch: 80 [65792/118836 (55%)] Loss: 12367.965820\n",
      "Train Epoch: 80 [98560/118836 (83%)] Loss: 12204.895508\n",
      "    epoch          : 80\n",
      "    loss           : 12365.778846800042\n",
      "    val_loss       : 12368.042129611575\n",
      "    val_log_likelihood: -12274.803432588917\n",
      "    val_log_marginal: -12281.642544322098\n",
      "Train Epoch: 81 [256/118836 (0%)] Loss: 12395.738281\n",
      "Train Epoch: 81 [33024/118836 (28%)] Loss: 12429.968750\n",
      "Train Epoch: 81 [65792/118836 (55%)] Loss: 12405.829102\n",
      "Train Epoch: 81 [98560/118836 (83%)] Loss: 12427.875977\n",
      "    epoch          : 81\n",
      "    loss           : 12366.289206924628\n",
      "    val_loss       : 12366.225258665207\n",
      "    val_log_likelihood: -12271.935236055107\n",
      "    val_log_marginal: -12278.655250283264\n",
      "Train Epoch: 82 [256/118836 (0%)] Loss: 12436.261719\n",
      "Train Epoch: 82 [33024/118836 (28%)] Loss: 12507.083984\n",
      "Train Epoch: 82 [65792/118836 (55%)] Loss: 12389.849609\n",
      "Train Epoch: 82 [98560/118836 (83%)] Loss: 12409.562500\n",
      "    epoch          : 82\n",
      "    loss           : 12358.19536904208\n",
      "    val_loss       : 12362.346485952055\n",
      "    val_log_likelihood: -12271.118472297612\n",
      "    val_log_marginal: -12277.966736840806\n",
      "Train Epoch: 83 [256/118836 (0%)] Loss: 12447.774414\n",
      "Train Epoch: 83 [33024/118836 (28%)] Loss: 12432.436523\n",
      "Train Epoch: 83 [65792/118836 (55%)] Loss: 12393.474609\n",
      "Train Epoch: 83 [98560/118836 (83%)] Loss: 12508.714844\n",
      "    epoch          : 83\n",
      "    loss           : 12360.053120638182\n",
      "    val_loss       : 12364.335293210113\n",
      "    val_log_likelihood: -12271.665023069168\n",
      "    val_log_marginal: -12278.601848253154\n",
      "Train Epoch: 84 [256/118836 (0%)] Loss: 12419.408203\n",
      "Train Epoch: 84 [33024/118836 (28%)] Loss: 12359.386719\n",
      "Train Epoch: 84 [65792/118836 (55%)] Loss: 12408.936523\n",
      "Train Epoch: 84 [98560/118836 (83%)] Loss: 12392.062500\n",
      "    epoch          : 84\n",
      "    loss           : 12363.972598415528\n",
      "    val_loss       : 12363.042130594657\n",
      "    val_log_likelihood: -12271.04045408137\n",
      "    val_log_marginal: -12277.886434989985\n",
      "Train Epoch: 85 [256/118836 (0%)] Loss: 12408.608398\n",
      "Train Epoch: 85 [33024/118836 (28%)] Loss: 12342.785156\n",
      "Train Epoch: 85 [65792/118836 (55%)] Loss: 12360.601562\n",
      "Train Epoch: 85 [98560/118836 (83%)] Loss: 12360.673828\n",
      "    epoch          : 85\n",
      "    loss           : 12362.32498513751\n",
      "    val_loss       : 12363.572321229622\n",
      "    val_log_likelihood: -12270.112402424525\n",
      "    val_log_marginal: -12277.188131916255\n",
      "Train Epoch: 86 [256/118836 (0%)] Loss: 12405.932617\n",
      "Train Epoch: 86 [33024/118836 (28%)] Loss: 12340.164062\n",
      "Train Epoch: 86 [65792/118836 (55%)] Loss: 12306.532227\n",
      "Train Epoch: 86 [98560/118836 (83%)] Loss: 12358.765625\n",
      "    epoch          : 86\n",
      "    loss           : 12362.409548018764\n",
      "    val_loss       : 12359.350306138553\n",
      "    val_log_likelihood: -12269.584643655655\n",
      "    val_log_marginal: -12276.526401096884\n",
      "Train Epoch: 87 [256/118836 (0%)] Loss: 12337.835938\n",
      "Train Epoch: 87 [33024/118836 (28%)] Loss: 12434.891602\n",
      "Train Epoch: 87 [65792/118836 (55%)] Loss: 12335.956055\n",
      "Train Epoch: 87 [98560/118836 (83%)] Loss: 12326.412109\n",
      "    epoch          : 87\n",
      "    loss           : 12365.110317152607\n",
      "    val_loss       : 12360.62576056688\n",
      "    val_log_likelihood: -12269.948139119364\n",
      "    val_log_marginal: -12276.740320779143\n",
      "Train Epoch: 88 [256/118836 (0%)] Loss: 12355.587891\n",
      "Train Epoch: 88 [33024/118836 (28%)] Loss: 12433.209961\n",
      "Train Epoch: 88 [65792/118836 (55%)] Loss: 12411.234375\n",
      "Train Epoch: 88 [98560/118836 (83%)] Loss: 12387.723633\n",
      "    epoch          : 88\n",
      "    loss           : 12356.459682588917\n",
      "    val_loss       : 12360.826932872103\n",
      "    val_log_likelihood: -12266.38978575398\n",
      "    val_log_marginal: -12273.355900161887\n",
      "Train Epoch: 89 [256/118836 (0%)] Loss: 12484.418945\n",
      "Train Epoch: 89 [33024/118836 (28%)] Loss: 12386.294922\n",
      "Train Epoch: 89 [65792/118836 (55%)] Loss: 12374.583008\n",
      "Train Epoch: 89 [98560/118836 (83%)] Loss: 12349.091797\n",
      "    epoch          : 89\n",
      "    loss           : 12361.112945874691\n",
      "    val_loss       : 12357.643721364113\n",
      "    val_log_likelihood: -12270.719142078939\n",
      "    val_log_marginal: -12277.736550848847\n",
      "Train Epoch: 90 [256/118836 (0%)] Loss: 12246.951172\n",
      "Train Epoch: 90 [33024/118836 (28%)] Loss: 12476.886719\n",
      "Train Epoch: 90 [65792/118836 (55%)] Loss: 12394.906250\n",
      "Train Epoch: 90 [98560/118836 (83%)] Loss: 12491.818359\n",
      "    epoch          : 90\n",
      "    loss           : 12359.289890437603\n",
      "    val_loss       : 12362.580889095514\n",
      "    val_log_likelihood: -12270.182804907205\n",
      "    val_log_marginal: -12277.18421168262\n",
      "Train Epoch: 91 [256/118836 (0%)] Loss: 12378.616211\n",
      "Train Epoch: 91 [33024/118836 (28%)] Loss: 12315.593750\n",
      "Train Epoch: 91 [65792/118836 (55%)] Loss: 12467.443359\n",
      "Train Epoch: 91 [98560/118836 (83%)] Loss: 12336.963867\n",
      "    epoch          : 91\n",
      "    loss           : 12358.578999625206\n",
      "    val_loss       : 12361.004839754056\n",
      "    val_log_likelihood: -12266.941473777399\n",
      "    val_log_marginal: -12274.031541358936\n",
      "Train Epoch: 92 [256/118836 (0%)] Loss: 12376.732422\n",
      "Train Epoch: 92 [33024/118836 (28%)] Loss: 12340.658203\n",
      "Train Epoch: 92 [65792/118836 (55%)] Loss: 12333.875000\n",
      "Train Epoch: 92 [98560/118836 (83%)] Loss: 12369.289062\n",
      "    epoch          : 92\n",
      "    loss           : 12357.785576923077\n",
      "    val_loss       : 12361.89653025982\n",
      "    val_log_likelihood: -12269.077883484542\n",
      "    val_log_marginal: -12276.078222170072\n",
      "Train Epoch: 93 [256/118836 (0%)] Loss: 12359.019531\n",
      "Train Epoch: 93 [33024/118836 (28%)] Loss: 12318.198242\n",
      "Train Epoch: 93 [65792/118836 (55%)] Loss: 12356.995117\n",
      "Train Epoch: 93 [98560/118836 (83%)] Loss: 12329.257812\n",
      "    epoch          : 93\n",
      "    loss           : 12356.125922282103\n",
      "    val_loss       : 12356.797978272822\n",
      "    val_log_likelihood: -12268.112052832921\n",
      "    val_log_marginal: -12275.08018701867\n",
      "Train Epoch: 94 [256/118836 (0%)] Loss: 12413.998047\n",
      "Train Epoch: 94 [33024/118836 (28%)] Loss: 12350.361328\n",
      "Train Epoch: 94 [65792/118836 (55%)] Loss: 12454.322266\n",
      "Train Epoch: 94 [98560/118836 (83%)] Loss: 12361.441406\n",
      "    epoch          : 94\n",
      "    loss           : 12358.562444911859\n",
      "    val_loss       : 12360.377450522456\n",
      "    val_log_likelihood: -12271.810233308777\n",
      "    val_log_marginal: -12278.84762822514\n",
      "Train Epoch: 95 [256/118836 (0%)] Loss: 12437.738281\n",
      "Train Epoch: 95 [33024/118836 (28%)] Loss: 12422.415039\n",
      "Train Epoch: 95 [65792/118836 (55%)] Loss: 12515.183594\n",
      "Train Epoch: 95 [98560/118836 (83%)] Loss: 12388.353516\n",
      "    epoch          : 95\n",
      "    loss           : 12356.468877785102\n",
      "    val_loss       : 12362.39714149604\n",
      "    val_log_likelihood: -12269.305989744882\n",
      "    val_log_marginal: -12276.34554070484\n",
      "Train Epoch: 96 [256/118836 (0%)] Loss: 12516.871094\n",
      "Train Epoch: 96 [33024/118836 (28%)] Loss: 12506.841797\n",
      "Train Epoch: 96 [65792/118836 (55%)] Loss: 12409.949219\n",
      "Train Epoch: 96 [98560/118836 (83%)] Loss: 12340.298828\n",
      "    epoch          : 96\n",
      "    loss           : 12359.86767650822\n",
      "    val_loss       : 12361.293097166314\n",
      "    val_log_likelihood: -12269.202769754189\n",
      "    val_log_marginal: -12276.03858681322\n",
      "Train Epoch: 97 [256/118836 (0%)] Loss: 12311.026367\n",
      "Train Epoch: 97 [33024/118836 (28%)] Loss: 12298.808594\n",
      "Train Epoch: 97 [65792/118836 (55%)] Loss: 12348.418945\n",
      "Train Epoch: 97 [98560/118836 (83%)] Loss: 12358.497070\n",
      "    epoch          : 97\n",
      "    loss           : 12356.834559811829\n",
      "    val_loss       : 12361.140135066542\n",
      "    val_log_likelihood: -12270.67247305366\n",
      "    val_log_marginal: -12277.54099916113\n",
      "Train Epoch: 98 [256/118836 (0%)] Loss: 12448.537109\n",
      "Train Epoch: 98 [33024/118836 (28%)] Loss: 12340.728516\n",
      "Train Epoch: 98 [65792/118836 (55%)] Loss: 12402.151367\n",
      "Train Epoch: 98 [98560/118836 (83%)] Loss: 12294.543945\n",
      "    epoch          : 98\n",
      "    loss           : 12362.218477628723\n",
      "    val_loss       : 12356.613258418576\n",
      "    val_log_likelihood: -12268.936539107734\n",
      "    val_log_marginal: -12275.766645008673\n",
      "Train Epoch: 99 [256/118836 (0%)] Loss: 12377.313477\n",
      "Train Epoch: 99 [33024/118836 (28%)] Loss: 12464.197266\n",
      "Train Epoch: 99 [65792/118836 (55%)] Loss: 12399.263672\n",
      "Train Epoch: 99 [98560/118836 (83%)] Loss: 12395.305664\n",
      "    epoch          : 99\n",
      "    loss           : 12357.313042480873\n",
      "    val_loss       : 12358.309081140427\n",
      "    val_log_likelihood: -12268.477292862128\n",
      "    val_log_marginal: -12275.412639945469\n",
      "Train Epoch: 100 [256/118836 (0%)] Loss: 12425.065430\n",
      "Train Epoch: 100 [33024/118836 (28%)] Loss: 12419.424805\n",
      "Train Epoch: 100 [65792/118836 (55%)] Loss: 12388.062500\n",
      "Train Epoch: 100 [98560/118836 (83%)] Loss: 12482.682617\n",
      "    epoch          : 100\n",
      "    loss           : 12356.803591714484\n",
      "    val_loss       : 12360.543470028202\n",
      "    val_log_likelihood: -12269.504807530759\n",
      "    val_log_marginal: -12276.372056742117\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [256/118836 (0%)] Loss: 12339.994141\n",
      "Train Epoch: 101 [33024/118836 (28%)] Loss: 12329.314453\n",
      "Train Epoch: 101 [65792/118836 (55%)] Loss: 12484.779297\n",
      "Train Epoch: 101 [98560/118836 (83%)] Loss: 12455.900391\n",
      "    epoch          : 101\n",
      "    loss           : 12357.287741677006\n",
      "    val_loss       : 12358.455959241435\n",
      "    val_log_likelihood: -12268.309436065447\n",
      "    val_log_marginal: -12275.12453913005\n",
      "Train Epoch: 102 [256/118836 (0%)] Loss: 12372.528320\n",
      "Train Epoch: 102 [33024/118836 (28%)] Loss: 12388.692383\n",
      "Train Epoch: 102 [65792/118836 (55%)] Loss: 12371.597656\n",
      "Train Epoch: 102 [98560/118836 (83%)] Loss: 12349.291016\n",
      "    epoch          : 102\n",
      "    loss           : 12356.044451121796\n",
      "    val_loss       : 12360.180371263697\n",
      "    val_log_likelihood: -12272.706060664807\n",
      "    val_log_marginal: -12279.554226403425\n",
      "Train Epoch: 103 [256/118836 (0%)] Loss: 12318.365234\n",
      "Train Epoch: 103 [33024/118836 (28%)] Loss: 12365.030273\n",
      "Train Epoch: 103 [65792/118836 (55%)] Loss: 12333.020508\n",
      "Train Epoch: 103 [98560/118836 (83%)] Loss: 12405.857422\n",
      "    epoch          : 103\n",
      "    loss           : 12355.521272422975\n",
      "    val_loss       : 12356.391686262321\n",
      "    val_log_likelihood: -12269.497768687965\n",
      "    val_log_marginal: -12276.391736497037\n",
      "Train Epoch: 104 [256/118836 (0%)] Loss: 12340.527344\n",
      "Train Epoch: 104 [33024/118836 (28%)] Loss: 12418.253906\n",
      "Train Epoch: 104 [65792/118836 (55%)] Loss: 12431.128906\n",
      "Train Epoch: 104 [98560/118836 (83%)] Loss: 12290.157227\n",
      "    epoch          : 104\n",
      "    loss           : 12359.45171193264\n",
      "    val_loss       : 12357.304296597558\n",
      "    val_log_likelihood: -12268.905286684501\n",
      "    val_log_marginal: -12275.733751765016\n",
      "Train Epoch: 105 [256/118836 (0%)] Loss: 12472.400391\n",
      "Train Epoch: 105 [33024/118836 (28%)] Loss: 12438.820312\n",
      "Train Epoch: 105 [65792/118836 (55%)] Loss: 12388.819336\n",
      "Train Epoch: 105 [98560/118836 (83%)] Loss: 12351.154297\n",
      "    epoch          : 105\n",
      "    loss           : 12361.090266361662\n",
      "    val_loss       : 12356.190949282314\n",
      "    val_log_likelihood: -12270.714420976788\n",
      "    val_log_marginal: -12277.448814536116\n",
      "Train Epoch: 106 [256/118836 (0%)] Loss: 12362.321289\n",
      "Train Epoch: 106 [33024/118836 (28%)] Loss: 12387.324219\n",
      "Train Epoch: 106 [65792/118836 (55%)] Loss: 12403.912109\n",
      "Train Epoch: 106 [98560/118836 (83%)] Loss: 12319.574219\n",
      "    epoch          : 106\n",
      "    loss           : 12356.185145910877\n",
      "    val_loss       : 12357.10974013533\n",
      "    val_log_likelihood: -12267.725369946753\n",
      "    val_log_marginal: -12274.631293128465\n",
      "Train Epoch: 107 [256/118836 (0%)] Loss: 12349.749023\n",
      "Train Epoch: 107 [33024/118836 (28%)] Loss: 12459.861328\n",
      "Train Epoch: 107 [65792/118836 (55%)] Loss: 12435.882812\n",
      "Train Epoch: 107 [98560/118836 (83%)] Loss: 12453.472656\n",
      "    epoch          : 107\n",
      "    loss           : 12355.99100883995\n",
      "    val_loss       : 12363.325310298302\n",
      "    val_log_likelihood: -12267.588936815033\n",
      "    val_log_marginal: -12274.631457686239\n",
      "Train Epoch: 108 [256/118836 (0%)] Loss: 12438.163086\n",
      "Train Epoch: 108 [33024/118836 (28%)] Loss: 12371.587891\n",
      "Train Epoch: 108 [65792/118836 (55%)] Loss: 12317.357422\n",
      "Train Epoch: 108 [98560/118836 (83%)] Loss: 12355.753906\n",
      "    epoch          : 108\n",
      "    loss           : 12366.9888940046\n",
      "    val_loss       : 12362.603227436523\n",
      "    val_log_likelihood: -12268.184200042648\n",
      "    val_log_marginal: -12275.199188602259\n",
      "Train Epoch: 109 [256/118836 (0%)] Loss: 12443.682617\n",
      "Train Epoch: 109 [33024/118836 (28%)] Loss: 12400.293945\n",
      "Train Epoch: 109 [65792/118836 (55%)] Loss: 12306.849609\n",
      "Train Epoch: 109 [98560/118836 (83%)] Loss: 12405.105469\n",
      "    epoch          : 109\n",
      "    loss           : 12359.660951070098\n",
      "    val_loss       : 12362.833697297328\n",
      "    val_log_likelihood: -12268.549491121277\n",
      "    val_log_marginal: -12275.360355684006\n",
      "Train Epoch: 110 [256/118836 (0%)] Loss: 12386.337891\n",
      "Train Epoch: 110 [33024/118836 (28%)] Loss: 12416.495117\n",
      "Train Epoch: 110 [65792/118836 (55%)] Loss: 12352.485352\n",
      "Train Epoch: 110 [98560/118836 (83%)] Loss: 12393.087891\n",
      "    epoch          : 110\n",
      "    loss           : 12360.186349934087\n",
      "    val_loss       : 12357.84519298204\n",
      "    val_log_likelihood: -12267.973683054177\n",
      "    val_log_marginal: -12274.752052460475\n",
      "Train Epoch: 111 [256/118836 (0%)] Loss: 12373.390625\n",
      "Train Epoch: 111 [33024/118836 (28%)] Loss: 12468.176758\n",
      "Train Epoch: 111 [65792/118836 (55%)] Loss: 12376.392578\n",
      "Train Epoch: 111 [98560/118836 (83%)] Loss: 12346.390625\n",
      "    epoch          : 111\n",
      "    loss           : 12356.764615319995\n",
      "    val_loss       : 12355.202201960823\n",
      "    val_log_likelihood: -12268.608583733974\n",
      "    val_log_marginal: -12275.503869182226\n",
      "Train Epoch: 112 [256/118836 (0%)] Loss: 12348.110352\n",
      "Train Epoch: 112 [33024/118836 (28%)] Loss: 12327.069336\n",
      "Train Epoch: 112 [65792/118836 (55%)] Loss: 12358.353516\n",
      "Train Epoch: 112 [98560/118836 (83%)] Loss: 12318.732422\n",
      "    epoch          : 112\n",
      "    loss           : 12354.2383620244\n",
      "    val_loss       : 12353.568889990203\n",
      "    val_log_likelihood: -12266.272254316586\n",
      "    val_log_marginal: -12273.079299871832\n",
      "Train Epoch: 113 [256/118836 (0%)] Loss: 12343.287109\n",
      "Train Epoch: 113 [33024/118836 (28%)] Loss: 12372.191406\n",
      "Train Epoch: 113 [65792/118836 (55%)] Loss: 12470.425781\n",
      "Train Epoch: 113 [98560/118836 (83%)] Loss: 12426.013672\n",
      "    epoch          : 113\n",
      "    loss           : 12359.102233896816\n",
      "    val_loss       : 12355.104196824175\n",
      "    val_log_likelihood: -12269.194005247105\n",
      "    val_log_marginal: -12275.953100379147\n",
      "Train Epoch: 114 [256/118836 (0%)] Loss: 12317.932617\n",
      "Train Epoch: 114 [33024/118836 (28%)] Loss: 12532.730469\n",
      "Train Epoch: 114 [65792/118836 (55%)] Loss: 12336.396484\n",
      "Train Epoch: 114 [98560/118836 (83%)] Loss: 12343.539062\n",
      "    epoch          : 114\n",
      "    loss           : 12358.027280584418\n",
      "    val_loss       : 12355.834122904913\n",
      "    val_log_likelihood: -12268.356662757185\n",
      "    val_log_marginal: -12275.228978900986\n",
      "Train Epoch: 115 [256/118836 (0%)] Loss: 12418.140625\n",
      "Train Epoch: 115 [33024/118836 (28%)] Loss: 12319.178711\n",
      "Train Epoch: 115 [65792/118836 (55%)] Loss: 12386.203125\n",
      "Train Epoch: 115 [98560/118836 (83%)] Loss: 12426.941406\n",
      "    epoch          : 115\n",
      "    loss           : 12357.782510403742\n",
      "    val_loss       : 12358.522893092066\n",
      "    val_log_likelihood: -12266.680648553816\n",
      "    val_log_marginal: -12273.578978190806\n",
      "Train Epoch: 116 [256/118836 (0%)] Loss: 12368.868164\n",
      "Train Epoch: 116 [33024/118836 (28%)] Loss: 12338.288086\n",
      "Train Epoch: 116 [65792/118836 (55%)] Loss: 12365.920898\n",
      "Train Epoch: 116 [98560/118836 (83%)] Loss: 12285.240234\n",
      "    epoch          : 116\n",
      "    loss           : 12353.16080406069\n",
      "    val_loss       : 12360.38551814311\n",
      "    val_log_likelihood: -12269.171907632857\n",
      "    val_log_marginal: -12276.021169620652\n",
      "Train Epoch: 117 [256/118836 (0%)] Loss: 12311.497070\n",
      "Train Epoch: 117 [33024/118836 (28%)] Loss: 12416.750000\n",
      "Train Epoch: 117 [65792/118836 (55%)] Loss: 12363.350586\n",
      "Train Epoch: 117 [98560/118836 (83%)] Loss: 12303.692383\n",
      "    epoch          : 117\n",
      "    loss           : 12355.313625833593\n",
      "    val_loss       : 12358.264885040362\n",
      "    val_log_likelihood: -12270.153657949493\n",
      "    val_log_marginal: -12277.06362199554\n",
      "Train Epoch: 118 [256/118836 (0%)] Loss: 12307.038086\n",
      "Train Epoch: 118 [33024/118836 (28%)] Loss: 12358.890625\n",
      "Train Epoch: 118 [65792/118836 (55%)] Loss: 12450.344727\n",
      "Train Epoch: 118 [98560/118836 (83%)] Loss: 12395.789062\n",
      "    epoch          : 118\n",
      "    loss           : 12358.57615572012\n",
      "    val_loss       : 12355.434233609765\n",
      "    val_log_likelihood: -12269.397787750724\n",
      "    val_log_marginal: -12276.239063916895\n",
      "Train Epoch: 119 [256/118836 (0%)] Loss: 12348.842773\n",
      "Train Epoch: 119 [33024/118836 (28%)] Loss: 12356.158203\n",
      "Train Epoch: 119 [65792/118836 (55%)] Loss: 12494.204102\n",
      "Train Epoch: 119 [98560/118836 (83%)] Loss: 12335.331055\n",
      "    epoch          : 119\n",
      "    loss           : 12347.492493150332\n",
      "    val_loss       : 12357.347951679834\n",
      "    val_log_likelihood: -12266.98579469086\n",
      "    val_log_marginal: -12273.803479104117\n",
      "Train Epoch: 120 [256/118836 (0%)] Loss: 12459.732422\n",
      "Train Epoch: 120 [33024/118836 (28%)] Loss: 12409.043945\n",
      "Train Epoch: 120 [65792/118836 (55%)] Loss: 12419.438477\n",
      "Train Epoch: 120 [98560/118836 (83%)] Loss: 12382.146484\n",
      "    epoch          : 120\n",
      "    loss           : 12354.399596774194\n",
      "    val_loss       : 12359.232683184215\n",
      "    val_log_likelihood: -12267.600577698511\n",
      "    val_log_marginal: -12274.48453358369\n",
      "Train Epoch: 121 [256/118836 (0%)] Loss: 12405.333008\n",
      "Train Epoch: 121 [33024/118836 (28%)] Loss: 12434.933594\n",
      "Train Epoch: 121 [65792/118836 (55%)] Loss: 12392.573242\n",
      "Train Epoch: 121 [98560/118836 (83%)] Loss: 12300.104492\n",
      "    epoch          : 121\n",
      "    loss           : 12356.164474126344\n",
      "    val_loss       : 12357.257199564752\n",
      "    val_log_likelihood: -12270.0192886037\n",
      "    val_log_marginal: -12276.832712572645\n",
      "Train Epoch: 122 [256/118836 (0%)] Loss: 12369.292969\n",
      "Train Epoch: 122 [33024/118836 (28%)] Loss: 12495.002930\n",
      "Train Epoch: 122 [65792/118836 (55%)] Loss: 12278.358398\n",
      "Train Epoch: 122 [98560/118836 (83%)] Loss: 12411.820312\n",
      "    epoch          : 122\n",
      "    loss           : 12359.376090131307\n",
      "    val_loss       : 12356.744804479515\n",
      "    val_log_likelihood: -12268.185022002946\n",
      "    val_log_marginal: -12274.93874847538\n",
      "Train Epoch: 123 [256/118836 (0%)] Loss: 12358.349609\n",
      "Train Epoch: 123 [33024/118836 (28%)] Loss: 12482.238281\n",
      "Train Epoch: 123 [65792/118836 (55%)] Loss: 12399.042969\n",
      "Train Epoch: 123 [98560/118836 (83%)] Loss: 12337.415039\n",
      "    epoch          : 123\n",
      "    loss           : 12354.468123190654\n",
      "    val_loss       : 12354.30708241542\n",
      "    val_log_likelihood: -12266.639131642885\n",
      "    val_log_marginal: -12273.303266586672\n",
      "Train Epoch: 124 [256/118836 (0%)] Loss: 12292.539062\n",
      "Train Epoch: 124 [33024/118836 (28%)] Loss: 12313.378906\n",
      "Train Epoch: 124 [65792/118836 (55%)] Loss: 12393.388672\n",
      "Train Epoch: 124 [98560/118836 (83%)] Loss: 12465.913086\n",
      "    epoch          : 124\n",
      "    loss           : 12354.387938281896\n",
      "    val_loss       : 12357.770080275452\n",
      "    val_log_likelihood: -12267.882423975134\n",
      "    val_log_marginal: -12274.754946843515\n",
      "Train Epoch: 125 [256/118836 (0%)] Loss: 12251.984375\n",
      "Train Epoch: 125 [33024/118836 (28%)] Loss: 12396.933594\n",
      "Train Epoch: 125 [65792/118836 (55%)] Loss: 12316.855469\n",
      "Train Epoch: 125 [98560/118836 (83%)] Loss: 12398.360352\n",
      "    epoch          : 125\n",
      "    loss           : 12358.172362877378\n",
      "    val_loss       : 12357.820225871432\n",
      "    val_log_likelihood: -12269.19158944634\n",
      "    val_log_marginal: -12275.93945067541\n",
      "Train Epoch: 126 [256/118836 (0%)] Loss: 12404.222656\n",
      "Train Epoch: 126 [33024/118836 (28%)] Loss: 12373.638672\n",
      "Train Epoch: 126 [65792/118836 (55%)] Loss: 12467.454102\n",
      "Train Epoch: 126 [98560/118836 (83%)] Loss: 12322.795898\n",
      "    epoch          : 126\n",
      "    loss           : 12358.078380731751\n",
      "    val_loss       : 12354.092270707428\n",
      "    val_log_likelihood: -12266.726273004548\n",
      "    val_log_marginal: -12273.539054440394\n",
      "Train Epoch: 127 [256/118836 (0%)] Loss: 12296.869141\n",
      "Train Epoch: 127 [33024/118836 (28%)] Loss: 12373.851562\n",
      "Train Epoch: 127 [65792/118836 (55%)] Loss: 12367.208984\n",
      "Train Epoch: 127 [98560/118836 (83%)] Loss: 12300.937500\n",
      "    epoch          : 127\n",
      "    loss           : 12355.7400056219\n",
      "    val_loss       : 12355.434511922178\n",
      "    val_log_likelihood: -12265.480958404414\n",
      "    val_log_marginal: -12272.582737013769\n",
      "Train Epoch: 128 [256/118836 (0%)] Loss: 12438.041016\n",
      "Train Epoch: 128 [33024/118836 (28%)] Loss: 12377.518555\n",
      "Train Epoch: 128 [65792/118836 (55%)] Loss: 12366.167969\n",
      "Train Epoch: 128 [98560/118836 (83%)] Loss: 12372.138672\n",
      "    epoch          : 128\n",
      "    loss           : 12355.639775899503\n",
      "    val_loss       : 12355.966938151203\n",
      "    val_log_likelihood: -12268.719398295336\n",
      "    val_log_marginal: -12275.472281656566\n",
      "Train Epoch: 129 [256/118836 (0%)] Loss: 12387.414062\n",
      "Train Epoch: 129 [33024/118836 (28%)] Loss: 12366.640625\n",
      "Train Epoch: 129 [65792/118836 (55%)] Loss: 12384.071289\n",
      "Train Epoch: 129 [98560/118836 (83%)] Loss: 12463.552734\n",
      "    epoch          : 129\n",
      "    loss           : 12350.87480194117\n",
      "    val_loss       : 12354.463001989154\n",
      "    val_log_likelihood: -12265.710422482423\n",
      "    val_log_marginal: -12272.516643172567\n",
      "Train Epoch: 130 [256/118836 (0%)] Loss: 12353.936523\n",
      "Train Epoch: 130 [33024/118836 (28%)] Loss: 12309.410156\n",
      "Train Epoch: 130 [65792/118836 (55%)] Loss: 12363.260742\n",
      "Train Epoch: 130 [98560/118836 (83%)] Loss: 12391.662109\n",
      "    epoch          : 130\n",
      "    loss           : 12352.64414983328\n",
      "    val_loss       : 12356.76541992617\n",
      "    val_log_likelihood: -12267.038621471775\n",
      "    val_log_marginal: -12273.912756547541\n",
      "Train Epoch: 131 [256/118836 (0%)] Loss: 12366.312500\n",
      "Train Epoch: 131 [33024/118836 (28%)] Loss: 12461.682617\n",
      "Train Epoch: 131 [65792/118836 (55%)] Loss: 12401.564453\n",
      "Train Epoch: 131 [98560/118836 (83%)] Loss: 12398.224609\n",
      "    epoch          : 131\n",
      "    loss           : 12351.763676883013\n",
      "    val_loss       : 12350.601893585754\n",
      "    val_log_likelihood: -12266.574771569996\n",
      "    val_log_marginal: -12273.500002832512\n",
      "Train Epoch: 132 [256/118836 (0%)] Loss: 12394.994141\n",
      "Train Epoch: 132 [33024/118836 (28%)] Loss: 12320.907227\n",
      "Train Epoch: 132 [65792/118836 (55%)] Loss: 12365.364258\n",
      "Train Epoch: 132 [98560/118836 (83%)] Loss: 12460.560547\n",
      "    epoch          : 132\n",
      "    loss           : 12351.862901448769\n",
      "    val_loss       : 12353.811564325057\n",
      "    val_log_likelihood: -12267.661385474825\n",
      "    val_log_marginal: -12274.556013835483\n",
      "Train Epoch: 133 [256/118836 (0%)] Loss: 12347.723633\n",
      "Train Epoch: 133 [33024/118836 (28%)] Loss: 12312.070312\n",
      "Train Epoch: 133 [65792/118836 (55%)] Loss: 12367.914062\n",
      "Train Epoch: 133 [98560/118836 (83%)] Loss: 12357.060547\n",
      "    epoch          : 133\n",
      "    loss           : 12353.113334884201\n",
      "    val_loss       : 12350.158993855764\n",
      "    val_log_likelihood: -12269.018313656688\n",
      "    val_log_marginal: -12275.932332383702\n",
      "Train Epoch: 134 [256/118836 (0%)] Loss: 12402.254883\n",
      "Train Epoch: 134 [33024/118836 (28%)] Loss: 12410.886719\n",
      "Train Epoch: 134 [65792/118836 (55%)] Loss: 12383.879883\n",
      "Train Epoch: 134 [98560/118836 (83%)] Loss: 12404.704102\n",
      "    epoch          : 134\n",
      "    loss           : 12351.511672385494\n",
      "    val_loss       : 12351.503673513711\n",
      "    val_log_likelihood: -12266.671344958386\n",
      "    val_log_marginal: -12273.62872136751\n",
      "Train Epoch: 135 [256/118836 (0%)] Loss: 12277.249023\n",
      "Train Epoch: 135 [33024/118836 (28%)] Loss: 12374.505859\n",
      "Train Epoch: 135 [65792/118836 (55%)] Loss: 12314.987305\n",
      "Train Epoch: 135 [98560/118836 (83%)] Loss: 12371.429688\n",
      "    epoch          : 135\n",
      "    loss           : 12350.014681070357\n",
      "    val_loss       : 12349.524847530034\n",
      "    val_log_likelihood: -12267.346017498965\n",
      "    val_log_marginal: -12274.166108867435\n",
      "Train Epoch: 136 [256/118836 (0%)] Loss: 12399.782227\n",
      "Train Epoch: 136 [33024/118836 (28%)] Loss: 12269.215820\n",
      "Train Epoch: 136 [65792/118836 (55%)] Loss: 12347.173828\n",
      "Train Epoch: 136 [98560/118836 (83%)] Loss: 12342.523438\n",
      "    epoch          : 136\n",
      "    loss           : 12354.217261327802\n",
      "    val_loss       : 12354.555936726269\n",
      "    val_log_likelihood: -12267.143460989195\n",
      "    val_log_marginal: -12274.1667764305\n",
      "Train Epoch: 137 [256/118836 (0%)] Loss: 12386.025391\n",
      "Train Epoch: 137 [33024/118836 (28%)] Loss: 12339.347656\n",
      "Train Epoch: 137 [65792/118836 (55%)] Loss: 12387.484375\n",
      "Train Epoch: 137 [98560/118836 (83%)] Loss: 12316.476562\n",
      "    epoch          : 137\n",
      "    loss           : 12354.476895613629\n",
      "    val_loss       : 12355.489818592241\n",
      "    val_log_likelihood: -12268.792402198356\n",
      "    val_log_marginal: -12275.685811062414\n",
      "Train Epoch: 138 [256/118836 (0%)] Loss: 12345.769531\n",
      "Train Epoch: 138 [33024/118836 (28%)] Loss: 12343.338867\n",
      "Train Epoch: 138 [65792/118836 (55%)] Loss: 12448.460938\n",
      "Train Epoch: 138 [98560/118836 (83%)] Loss: 12404.647461\n",
      "    epoch          : 138\n",
      "    loss           : 12353.92637784972\n",
      "    val_loss       : 12351.696308921693\n",
      "    val_log_likelihood: -12267.790132922353\n",
      "    val_log_marginal: -12274.670751221\n",
      "Train Epoch: 139 [256/118836 (0%)] Loss: 12478.628906\n",
      "Train Epoch: 139 [33024/118836 (28%)] Loss: 12398.504883\n",
      "Train Epoch: 139 [65792/118836 (55%)] Loss: 12410.807617\n",
      "Train Epoch: 139 [98560/118836 (83%)] Loss: 12297.488281\n",
      "    epoch          : 139\n",
      "    loss           : 12357.471306671318\n",
      "    val_loss       : 12351.127681184056\n",
      "    val_log_likelihood: -12271.059822490179\n",
      "    val_log_marginal: -12278.021208476515\n",
      "Train Epoch: 140 [256/118836 (0%)] Loss: 12418.855469\n",
      "Train Epoch: 140 [33024/118836 (28%)] Loss: 12387.390625\n",
      "Train Epoch: 140 [65792/118836 (55%)] Loss: 12384.564453\n",
      "Train Epoch: 140 [98560/118836 (83%)] Loss: 12346.234375\n",
      "    epoch          : 140\n",
      "    loss           : 12355.450531172455\n",
      "    val_loss       : 12352.737339647163\n",
      "    val_log_likelihood: -12269.244962908395\n",
      "    val_log_marginal: -12276.093236626442\n",
      "Train Epoch: 141 [256/118836 (0%)] Loss: 12368.759766\n",
      "Train Epoch: 141 [33024/118836 (28%)] Loss: 12375.593750\n",
      "Train Epoch: 141 [65792/118836 (55%)] Loss: 12386.541992\n",
      "Train Epoch: 141 [98560/118836 (83%)] Loss: 12313.886719\n",
      "    epoch          : 141\n",
      "    loss           : 12359.683120250465\n",
      "    val_loss       : 12353.577997276261\n",
      "    val_log_likelihood: -12267.710824415839\n",
      "    val_log_marginal: -12274.638962301444\n",
      "Train Epoch: 142 [256/118836 (0%)] Loss: 12285.674805\n",
      "Train Epoch: 142 [33024/118836 (28%)] Loss: 12368.982422\n",
      "Train Epoch: 142 [65792/118836 (55%)] Loss: 12353.714844\n",
      "Train Epoch: 142 [98560/118836 (83%)] Loss: 12364.349609\n",
      "    epoch          : 142\n",
      "    loss           : 12352.9652949558\n",
      "    val_loss       : 12350.349161125736\n",
      "    val_log_likelihood: -12267.576358786962\n",
      "    val_log_marginal: -12274.393522558381\n",
      "Train Epoch: 143 [256/118836 (0%)] Loss: 12442.100586\n",
      "Train Epoch: 143 [33024/118836 (28%)] Loss: 12374.757812\n",
      "Train Epoch: 143 [65792/118836 (55%)] Loss: 12390.231445\n",
      "Train Epoch: 143 [98560/118836 (83%)] Loss: 12436.243164\n",
      "    epoch          : 143\n",
      "    loss           : 12352.799950727616\n",
      "    val_loss       : 12350.737633100794\n",
      "    val_log_likelihood: -12269.456416072167\n",
      "    val_log_marginal: -12276.30467886747\n",
      "Train Epoch: 144 [256/118836 (0%)] Loss: 12389.044922\n",
      "Train Epoch: 144 [33024/118836 (28%)] Loss: 12312.604492\n",
      "Train Epoch: 144 [65792/118836 (55%)] Loss: 12301.253906\n",
      "Train Epoch: 144 [98560/118836 (83%)] Loss: 12389.458008\n",
      "    epoch          : 144\n",
      "    loss           : 12349.237407594086\n",
      "    val_loss       : 12351.154553403385\n",
      "    val_log_likelihood: -12269.695042713503\n",
      "    val_log_marginal: -12276.56187821592\n",
      "Train Epoch: 145 [256/118836 (0%)] Loss: 12377.672852\n",
      "Train Epoch: 145 [33024/118836 (28%)] Loss: 12416.086914\n",
      "Train Epoch: 145 [65792/118836 (55%)] Loss: 12354.043945\n",
      "Train Epoch: 145 [98560/118836 (83%)] Loss: 12453.329102\n",
      "    epoch          : 145\n",
      "    loss           : 12355.426129872312\n",
      "    val_loss       : 12366.431940090511\n",
      "    val_log_likelihood: -12273.763892550662\n",
      "    val_log_marginal: -12280.92760682617\n",
      "Train Epoch: 146 [256/118836 (0%)] Loss: 12443.846680\n",
      "Train Epoch: 146 [33024/118836 (28%)] Loss: 12399.742188\n",
      "Train Epoch: 146 [65792/118836 (55%)] Loss: 12313.484375\n",
      "Train Epoch: 146 [98560/118836 (83%)] Loss: 12410.824219\n",
      "    epoch          : 146\n",
      "    loss           : 12356.379682491988\n",
      "    val_loss       : 12351.945862129396\n",
      "    val_log_likelihood: -12266.066534681297\n",
      "    val_log_marginal: -12273.04951042631\n",
      "Train Epoch: 147 [256/118836 (0%)] Loss: 12351.986328\n",
      "Train Epoch: 147 [33024/118836 (28%)] Loss: 12304.572266\n",
      "Train Epoch: 147 [65792/118836 (55%)] Loss: 12295.565430\n",
      "Train Epoch: 147 [98560/118836 (83%)] Loss: 12341.150391\n",
      "    epoch          : 147\n",
      "    loss           : 12347.463518726738\n",
      "    val_loss       : 12353.119102448518\n",
      "    val_log_likelihood: -12267.05814787531\n",
      "    val_log_marginal: -12273.971725044157\n",
      "Train Epoch: 148 [256/118836 (0%)] Loss: 12289.812500\n",
      "Train Epoch: 148 [33024/118836 (28%)] Loss: 12348.790039\n",
      "Train Epoch: 148 [65792/118836 (55%)] Loss: 12417.280273\n",
      "Train Epoch: 148 [98560/118836 (83%)] Loss: 12346.148438\n",
      "    epoch          : 148\n",
      "    loss           : 12350.283916686052\n",
      "    val_loss       : 12349.943336905022\n",
      "    val_log_likelihood: -12265.87069100884\n",
      "    val_log_marginal: -12272.796099525089\n",
      "Train Epoch: 149 [256/118836 (0%)] Loss: 12342.518555\n",
      "Train Epoch: 149 [33024/118836 (28%)] Loss: 12354.307617\n",
      "Train Epoch: 149 [65792/118836 (55%)] Loss: 12390.218750\n",
      "Train Epoch: 149 [98560/118836 (83%)] Loss: 12319.637695\n",
      "    epoch          : 149\n",
      "    loss           : 12359.999263983664\n",
      "    val_loss       : 12346.60372383785\n",
      "    val_log_likelihood: -12269.798887898056\n",
      "    val_log_marginal: -12276.866380432071\n",
      "Train Epoch: 150 [256/118836 (0%)] Loss: 12401.781250\n",
      "Train Epoch: 150 [33024/118836 (28%)] Loss: 12350.229492\n",
      "Train Epoch: 150 [65792/118836 (55%)] Loss: 12336.400391\n",
      "Train Epoch: 150 [98560/118836 (83%)] Loss: 12439.199219\n",
      "    epoch          : 150\n",
      "    loss           : 12350.209045117348\n",
      "    val_loss       : 12350.493541302321\n",
      "    val_log_likelihood: -12267.933430424162\n",
      "    val_log_marginal: -12274.836353106151\n",
      "Train Epoch: 151 [256/118836 (0%)] Loss: 12325.457031\n",
      "Train Epoch: 151 [33024/118836 (28%)] Loss: 12292.615234\n",
      "Train Epoch: 151 [65792/118836 (55%)] Loss: 12385.535156\n",
      "Train Epoch: 151 [98560/118836 (83%)] Loss: 12335.470703\n",
      "    epoch          : 151\n",
      "    loss           : 12348.35089562655\n",
      "    val_loss       : 12353.471550137781\n",
      "    val_log_likelihood: -12269.493338050559\n",
      "    val_log_marginal: -12276.392472589956\n",
      "Train Epoch: 152 [256/118836 (0%)] Loss: 12323.546875\n",
      "Train Epoch: 152 [33024/118836 (28%)] Loss: 12415.702148\n",
      "Train Epoch: 152 [65792/118836 (55%)] Loss: 12340.001953\n",
      "Train Epoch: 152 [98560/118836 (83%)] Loss: 12397.783203\n",
      "    epoch          : 152\n",
      "    loss           : 12351.569772927007\n",
      "    val_loss       : 12351.52720878071\n",
      "    val_log_likelihood: -12266.49063453138\n",
      "    val_log_marginal: -12273.437887795899\n",
      "Train Epoch: 153 [256/118836 (0%)] Loss: 12289.147461\n",
      "Train Epoch: 153 [33024/118836 (28%)] Loss: 12335.040039\n",
      "Train Epoch: 153 [65792/118836 (55%)] Loss: 12381.638672\n",
      "Train Epoch: 153 [98560/118836 (83%)] Loss: 12358.416016\n",
      "    epoch          : 153\n",
      "    loss           : 12350.37135368202\n",
      "    val_loss       : 12354.308052537443\n",
      "    val_log_likelihood: -12266.60942475703\n",
      "    val_log_marginal: -12273.741891117825\n",
      "Train Epoch: 154 [256/118836 (0%)] Loss: 12287.246094\n",
      "Train Epoch: 154 [33024/118836 (28%)] Loss: 12297.168945\n",
      "Train Epoch: 154 [65792/118836 (55%)] Loss: 12410.602539\n",
      "Train Epoch: 154 [98560/118836 (83%)] Loss: 12439.626953\n",
      "    epoch          : 154\n",
      "    loss           : 12351.547444459522\n",
      "    val_loss       : 12351.000061201483\n",
      "    val_log_likelihood: -12266.083975166719\n",
      "    val_log_marginal: -12273.206586174825\n",
      "Train Epoch: 155 [256/118836 (0%)] Loss: 12292.923828\n",
      "Train Epoch: 155 [33024/118836 (28%)] Loss: 12357.748047\n",
      "Train Epoch: 155 [65792/118836 (55%)] Loss: 12345.083984\n",
      "Train Epoch: 155 [98560/118836 (83%)] Loss: 12350.919922\n",
      "    epoch          : 155\n",
      "    loss           : 12351.119130447425\n",
      "    val_loss       : 12350.291269105737\n",
      "    val_log_likelihood: -12265.85245812655\n",
      "    val_log_marginal: -12272.991809531926\n",
      "Train Epoch: 156 [256/118836 (0%)] Loss: 12367.062500\n",
      "Train Epoch: 156 [33024/118836 (28%)] Loss: 12459.591797\n",
      "Train Epoch: 156 [65792/118836 (55%)] Loss: 12332.431641\n",
      "Train Epoch: 156 [98560/118836 (83%)] Loss: 12406.294922\n",
      "    epoch          : 156\n",
      "    loss           : 12349.485595662738\n",
      "    val_loss       : 12349.930526140986\n",
      "    val_log_likelihood: -12265.719760649297\n",
      "    val_log_marginal: -12272.885430942752\n",
      "Train Epoch: 157 [256/118836 (0%)] Loss: 12523.542969\n",
      "Train Epoch: 157 [33024/118836 (28%)] Loss: 12376.042969\n",
      "Train Epoch: 157 [65792/118836 (55%)] Loss: 12489.816406\n",
      "Train Epoch: 157 [98560/118836 (83%)] Loss: 12330.825195\n",
      "    epoch          : 157\n",
      "    loss           : 12349.286329740487\n",
      "    val_loss       : 12350.540491233243\n",
      "    val_log_likelihood: -12261.34495030759\n",
      "    val_log_marginal: -12268.536178547598\n",
      "Train Epoch: 158 [256/118836 (0%)] Loss: 12505.300781\n",
      "Train Epoch: 158 [33024/118836 (28%)] Loss: 12352.844727\n",
      "Train Epoch: 158 [65792/118836 (55%)] Loss: 12389.867188\n",
      "Train Epoch: 158 [98560/118836 (83%)] Loss: 12270.348633\n",
      "    epoch          : 158\n",
      "    loss           : 12350.8678072012\n",
      "    val_loss       : 12348.543769315338\n",
      "    val_log_likelihood: -12263.254107216708\n",
      "    val_log_marginal: -12270.325841461603\n",
      "Train Epoch: 159 [256/118836 (0%)] Loss: 12343.037109\n",
      "Train Epoch: 159 [33024/118836 (28%)] Loss: 12355.727539\n",
      "Train Epoch: 159 [65792/118836 (55%)] Loss: 12386.525391\n",
      "Train Epoch: 159 [98560/118836 (83%)] Loss: 12391.050781\n",
      "    epoch          : 159\n",
      "    loss           : 12347.989402075578\n",
      "    val_loss       : 12347.763555509291\n",
      "    val_log_likelihood: -12258.888389164598\n",
      "    val_log_marginal: -12266.404931341365\n",
      "Train Epoch: 160 [256/118836 (0%)] Loss: 12308.052734\n",
      "Train Epoch: 160 [33024/118836 (28%)] Loss: 12399.051758\n",
      "Train Epoch: 160 [65792/118836 (55%)] Loss: 12347.956055\n",
      "Train Epoch: 160 [98560/118836 (83%)] Loss: 12372.665039\n",
      "    epoch          : 160\n",
      "    loss           : 12343.22833921371\n",
      "    val_loss       : 12342.726896407712\n",
      "    val_log_likelihood: -12251.931386831833\n",
      "    val_log_marginal: -12259.409816200794\n",
      "Train Epoch: 161 [256/118836 (0%)] Loss: 12445.304688\n",
      "Train Epoch: 161 [33024/118836 (28%)] Loss: 12474.757812\n",
      "Train Epoch: 161 [65792/118836 (55%)] Loss: 12421.320312\n",
      "Train Epoch: 161 [98560/118836 (83%)] Loss: 12265.393555\n",
      "    epoch          : 161\n",
      "    loss           : 12337.266336945564\n",
      "    val_loss       : 12326.7340981091\n",
      "    val_log_likelihood: -12222.061569802006\n",
      "    val_log_marginal: -12230.20889662024\n",
      "Train Epoch: 162 [256/118836 (0%)] Loss: 12415.010742\n",
      "Train Epoch: 162 [33024/118836 (28%)] Loss: 12303.806641\n",
      "Train Epoch: 162 [65792/118836 (55%)] Loss: 12398.607422\n",
      "Train Epoch: 162 [98560/118836 (83%)] Loss: 12367.722656\n",
      "    epoch          : 162\n",
      "    loss           : 12321.67300923413\n",
      "    val_loss       : 12324.18320172563\n",
      "    val_log_likelihood: -12221.771013621796\n",
      "    val_log_marginal: -12229.685100045668\n",
      "Train Epoch: 163 [256/118836 (0%)] Loss: 12300.387695\n",
      "Train Epoch: 163 [33024/118836 (28%)] Loss: 12424.450195\n",
      "Train Epoch: 163 [65792/118836 (55%)] Loss: 12320.001953\n",
      "Train Epoch: 163 [98560/118836 (83%)] Loss: 12330.562500\n",
      "    epoch          : 163\n",
      "    loss           : 12322.383941403019\n",
      "    val_loss       : 12324.349025458578\n",
      "    val_log_likelihood: -12228.60958517499\n",
      "    val_log_marginal: -12236.778754282801\n",
      "Train Epoch: 164 [256/118836 (0%)] Loss: 12377.096680\n",
      "Train Epoch: 164 [33024/118836 (28%)] Loss: 12381.537109\n",
      "Train Epoch: 164 [65792/118836 (55%)] Loss: 12363.524414\n",
      "Train Epoch: 164 [98560/118836 (83%)] Loss: 12368.294922\n",
      "    epoch          : 164\n",
      "    loss           : 12336.756189742298\n",
      "    val_loss       : 12323.562951408387\n",
      "    val_log_likelihood: -12223.117553246484\n",
      "    val_log_marginal: -12231.229843505467\n",
      "Train Epoch: 165 [256/118836 (0%)] Loss: 12337.691406\n",
      "Train Epoch: 165 [33024/118836 (28%)] Loss: 12382.105469\n",
      "Train Epoch: 165 [65792/118836 (55%)] Loss: 12418.249023\n",
      "Train Epoch: 165 [98560/118836 (83%)] Loss: 12305.345703\n",
      "    epoch          : 165\n",
      "    loss           : 12321.355661154623\n",
      "    val_loss       : 12314.88403481216\n",
      "    val_log_likelihood: -12220.224246374844\n",
      "    val_log_marginal: -12227.756758583593\n",
      "Train Epoch: 166 [256/118836 (0%)] Loss: 12383.099609\n",
      "Train Epoch: 166 [33024/118836 (28%)] Loss: 12288.507812\n",
      "Train Epoch: 166 [65792/118836 (55%)] Loss: 12280.273438\n",
      "Train Epoch: 166 [98560/118836 (83%)] Loss: 12362.720703\n",
      "    epoch          : 166\n",
      "    loss           : 12316.072470953526\n",
      "    val_loss       : 12317.212440107509\n",
      "    val_log_likelihood: -12221.401858618952\n",
      "    val_log_marginal: -12229.164612612101\n",
      "Train Epoch: 167 [256/118836 (0%)] Loss: 12325.180664\n",
      "Train Epoch: 167 [33024/118836 (28%)] Loss: 12352.875000\n",
      "Train Epoch: 167 [65792/118836 (55%)] Loss: 12325.552734\n",
      "Train Epoch: 167 [98560/118836 (83%)] Loss: 12460.615234\n",
      "    epoch          : 167\n",
      "    loss           : 12318.636539107732\n",
      "    val_loss       : 12319.673646626417\n",
      "    val_log_likelihood: -12220.429160527812\n",
      "    val_log_marginal: -12227.962438404458\n",
      "Train Epoch: 168 [256/118836 (0%)] Loss: 12386.395508\n",
      "Train Epoch: 168 [33024/118836 (28%)] Loss: 12290.829102\n",
      "Train Epoch: 168 [65792/118836 (55%)] Loss: 12292.704102\n",
      "Train Epoch: 168 [98560/118836 (83%)] Loss: 12315.354492\n",
      "    epoch          : 168\n",
      "    loss           : 12318.254450992556\n",
      "    val_loss       : 12317.1041696298\n",
      "    val_log_likelihood: -12219.988903697527\n",
      "    val_log_marginal: -12227.492541191312\n",
      "Train Epoch: 169 [256/118836 (0%)] Loss: 12320.244141\n",
      "Train Epoch: 169 [33024/118836 (28%)] Loss: 12329.297852\n",
      "Train Epoch: 169 [65792/118836 (55%)] Loss: 12402.624023\n",
      "Train Epoch: 169 [98560/118836 (83%)] Loss: 12336.935547\n",
      "    epoch          : 169\n",
      "    loss           : 12315.065718213657\n",
      "    val_loss       : 12311.725148327216\n",
      "    val_log_likelihood: -12222.01181454844\n",
      "    val_log_marginal: -12229.50904953543\n",
      "Train Epoch: 170 [256/118836 (0%)] Loss: 12377.708984\n",
      "Train Epoch: 170 [33024/118836 (28%)] Loss: 12379.229492\n",
      "Train Epoch: 170 [65792/118836 (55%)] Loss: 12332.503906\n",
      "Train Epoch: 170 [98560/118836 (83%)] Loss: 12304.063477\n",
      "    epoch          : 170\n",
      "    loss           : 12317.7847924421\n",
      "    val_loss       : 12320.768755258054\n",
      "    val_log_likelihood: -12217.221027999638\n",
      "    val_log_marginal: -12224.659911098037\n",
      "Train Epoch: 171 [256/118836 (0%)] Loss: 12273.396484\n",
      "Train Epoch: 171 [33024/118836 (28%)] Loss: 12326.730469\n",
      "Train Epoch: 171 [65792/118836 (55%)] Loss: 12393.630859\n",
      "Train Epoch: 171 [98560/118836 (83%)] Loss: 12237.851562\n",
      "    epoch          : 171\n",
      "    loss           : 12315.230937079974\n",
      "    val_loss       : 12314.074302722509\n",
      "    val_log_likelihood: -12218.451917261165\n",
      "    val_log_marginal: -12226.045374215193\n",
      "Train Epoch: 172 [256/118836 (0%)] Loss: 12332.789062\n",
      "Train Epoch: 172 [33024/118836 (28%)] Loss: 12337.875000\n",
      "Train Epoch: 172 [65792/118836 (55%)] Loss: 12329.228516\n",
      "Train Epoch: 172 [98560/118836 (83%)] Loss: 12329.996094\n",
      "    epoch          : 172\n",
      "    loss           : 12318.831316719656\n",
      "    val_loss       : 12316.507237149608\n",
      "    val_log_likelihood: -12219.462608237696\n",
      "    val_log_marginal: -12226.845760758983\n",
      "Train Epoch: 173 [256/118836 (0%)] Loss: 12309.314453\n",
      "Train Epoch: 173 [33024/118836 (28%)] Loss: 12355.408203\n",
      "Train Epoch: 173 [65792/118836 (55%)] Loss: 12343.124023\n",
      "Train Epoch: 173 [98560/118836 (83%)] Loss: 12286.165039\n",
      "    epoch          : 173\n",
      "    loss           : 12319.999884654157\n",
      "    val_loss       : 12317.88386916928\n",
      "    val_log_likelihood: -12216.111641529673\n",
      "    val_log_marginal: -12223.616636479761\n",
      "Train Epoch: 174 [256/118836 (0%)] Loss: 12382.503906\n",
      "Train Epoch: 174 [33024/118836 (28%)] Loss: 12318.305664\n",
      "Train Epoch: 174 [65792/118836 (55%)] Loss: 12486.109375\n",
      "Train Epoch: 174 [98560/118836 (83%)] Loss: 12347.669922\n",
      "    epoch          : 174\n",
      "    loss           : 12317.712408401829\n",
      "    val_loss       : 12319.280023948373\n",
      "    val_log_likelihood: -12218.288320667907\n",
      "    val_log_marginal: -12225.969832078403\n",
      "Train Epoch: 175 [256/118836 (0%)] Loss: 12325.297852\n",
      "Train Epoch: 175 [33024/118836 (28%)] Loss: 12338.158203\n",
      "Train Epoch: 175 [65792/118836 (55%)] Loss: 12384.458008\n",
      "Train Epoch: 175 [98560/118836 (83%)] Loss: 12356.105469\n",
      "    epoch          : 175\n",
      "    loss           : 12314.233109426696\n",
      "    val_loss       : 12317.043520121475\n",
      "    val_log_likelihood: -12221.82653293657\n",
      "    val_log_marginal: -12229.233719649983\n",
      "Train Epoch: 176 [256/118836 (0%)] Loss: 12319.421875\n",
      "Train Epoch: 176 [33024/118836 (28%)] Loss: 12310.450195\n",
      "Train Epoch: 176 [65792/118836 (55%)] Loss: 12235.735352\n",
      "Train Epoch: 176 [98560/118836 (83%)] Loss: 12357.134766\n",
      "    epoch          : 176\n",
      "    loss           : 12316.737898540892\n",
      "    val_loss       : 12318.066354141492\n",
      "    val_log_likelihood: -12218.249985622157\n",
      "    val_log_marginal: -12225.705603479384\n",
      "Train Epoch: 177 [256/118836 (0%)] Loss: 12385.642578\n",
      "Train Epoch: 177 [33024/118836 (28%)] Loss: 12323.173828\n",
      "Train Epoch: 177 [65792/118836 (55%)] Loss: 12287.858398\n",
      "Train Epoch: 177 [98560/118836 (83%)] Loss: 12376.871094\n",
      "    epoch          : 177\n",
      "    loss           : 12317.721587766233\n",
      "    val_loss       : 12320.22382300821\n",
      "    val_log_likelihood: -12218.062166724825\n",
      "    val_log_marginal: -12225.544108519684\n",
      "Train Epoch: 178 [256/118836 (0%)] Loss: 12250.443359\n",
      "Train Epoch: 178 [33024/118836 (28%)] Loss: 12173.656250\n",
      "Train Epoch: 178 [65792/118836 (55%)] Loss: 12322.795898\n",
      "Train Epoch: 178 [98560/118836 (83%)] Loss: 12318.639648\n",
      "    epoch          : 178\n",
      "    loss           : 12319.066973932486\n",
      "    val_loss       : 12311.278995464143\n",
      "    val_log_likelihood: -12219.182552083334\n",
      "    val_log_marginal: -12226.611151538873\n",
      "Train Epoch: 179 [256/118836 (0%)] Loss: 12277.350586\n",
      "Train Epoch: 179 [33024/118836 (28%)] Loss: 12364.905273\n",
      "Train Epoch: 179 [65792/118836 (55%)] Loss: 12452.001953\n",
      "Train Epoch: 179 [98560/118836 (83%)] Loss: 12357.724609\n",
      "    epoch          : 179\n",
      "    loss           : 12318.402703196081\n",
      "    val_loss       : 12311.747676271432\n",
      "    val_log_likelihood: -12221.026614195616\n",
      "    val_log_marginal: -12228.59274923121\n",
      "Train Epoch: 180 [256/118836 (0%)] Loss: 12291.841797\n",
      "Train Epoch: 180 [33024/118836 (28%)] Loss: 12307.398438\n",
      "Train Epoch: 180 [65792/118836 (55%)] Loss: 12300.941406\n",
      "Train Epoch: 180 [98560/118836 (83%)] Loss: 12373.456055\n",
      "    epoch          : 180\n",
      "    loss           : 12312.79117475057\n",
      "    val_loss       : 12312.479275546131\n",
      "    val_log_likelihood: -12220.253137762356\n",
      "    val_log_marginal: -12227.584247381234\n",
      "Train Epoch: 181 [256/118836 (0%)] Loss: 12251.830078\n",
      "Train Epoch: 181 [33024/118836 (28%)] Loss: 12394.138672\n",
      "Train Epoch: 181 [65792/118836 (55%)] Loss: 12320.144531\n",
      "Train Epoch: 181 [98560/118836 (83%)] Loss: 12261.709961\n",
      "    epoch          : 181\n",
      "    loss           : 12316.521983883893\n",
      "    val_loss       : 12316.044007002394\n",
      "    val_log_likelihood: -12219.182371633324\n",
      "    val_log_marginal: -12226.497893871117\n",
      "Train Epoch: 182 [256/118836 (0%)] Loss: 12263.397461\n",
      "Train Epoch: 182 [33024/118836 (28%)] Loss: 12300.767578\n",
      "Train Epoch: 182 [65792/118836 (55%)] Loss: 12341.339844\n",
      "Train Epoch: 182 [98560/118836 (83%)] Loss: 12319.885742\n",
      "    epoch          : 182\n",
      "    loss           : 12315.881944498293\n",
      "    val_loss       : 12316.519747926102\n",
      "    val_log_likelihood: -12217.900766710609\n",
      "    val_log_marginal: -12225.305562632588\n",
      "Train Epoch: 183 [256/118836 (0%)] Loss: 12356.729492\n",
      "Train Epoch: 183 [33024/118836 (28%)] Loss: 12409.983398\n",
      "Train Epoch: 183 [65792/118836 (55%)] Loss: 12227.960938\n",
      "Train Epoch: 183 [98560/118836 (83%)] Loss: 12418.450195\n",
      "    epoch          : 183\n",
      "    loss           : 12318.974674963813\n",
      "    val_loss       : 12316.514870195273\n",
      "    val_log_likelihood: -12220.604843394593\n",
      "    val_log_marginal: -12228.133022346941\n",
      "Train Epoch: 184 [256/118836 (0%)] Loss: 12370.787109\n",
      "Train Epoch: 184 [33024/118836 (28%)] Loss: 12261.405273\n",
      "Train Epoch: 184 [65792/118836 (55%)] Loss: 12237.614258\n",
      "Train Epoch: 184 [98560/118836 (83%)] Loss: 12427.140625\n",
      "    epoch          : 184\n",
      "    loss           : 12316.762624554125\n",
      "    val_loss       : 12316.53824466847\n",
      "    val_log_likelihood: -12219.515792849203\n",
      "    val_log_marginal: -12226.901225558304\n",
      "Train Epoch: 185 [256/118836 (0%)] Loss: 12372.655273\n",
      "Train Epoch: 185 [33024/118836 (28%)] Loss: 12250.469727\n",
      "Train Epoch: 185 [65792/118836 (55%)] Loss: 12359.735352\n",
      "Train Epoch: 185 [98560/118836 (83%)] Loss: 12300.204102\n",
      "    epoch          : 185\n",
      "    loss           : 12314.319256132392\n",
      "    val_loss       : 12320.811695346636\n",
      "    val_log_likelihood: -12216.716829830955\n",
      "    val_log_marginal: -12224.184281889036\n",
      "Train Epoch: 186 [256/118836 (0%)] Loss: 12291.537109\n",
      "Train Epoch: 186 [33024/118836 (28%)] Loss: 12340.647461\n",
      "Train Epoch: 186 [65792/118836 (55%)] Loss: 12359.958008\n",
      "Train Epoch: 186 [98560/118836 (83%)] Loss: 12205.467773\n",
      "    epoch          : 186\n",
      "    loss           : 12312.955775531173\n",
      "    val_loss       : 12315.95525566181\n",
      "    val_log_likelihood: -12216.22168211073\n",
      "    val_log_marginal: -12223.70383196474\n",
      "Train Epoch: 187 [256/118836 (0%)] Loss: 12343.992188\n",
      "Train Epoch: 187 [33024/118836 (28%)] Loss: 12346.682617\n",
      "Train Epoch: 187 [65792/118836 (55%)] Loss: 12313.261719\n",
      "Train Epoch: 187 [98560/118836 (83%)] Loss: 12327.299805\n",
      "    epoch          : 187\n",
      "    loss           : 12310.988571553195\n",
      "    val_loss       : 12311.760266022715\n",
      "    val_log_likelihood: -12220.093556141439\n",
      "    val_log_marginal: -12227.525221802485\n",
      "Train Epoch: 188 [256/118836 (0%)] Loss: 12318.888672\n",
      "Train Epoch: 188 [33024/118836 (28%)] Loss: 12307.933594\n",
      "Train Epoch: 188 [65792/118836 (55%)] Loss: 12249.639648\n",
      "Train Epoch: 188 [98560/118836 (83%)] Loss: 12331.487305\n",
      "    epoch          : 188\n",
      "    loss           : 12310.54246617168\n",
      "    val_loss       : 12311.557958813844\n",
      "    val_log_likelihood: -12219.01612079327\n",
      "    val_log_marginal: -12226.387914034922\n",
      "Train Epoch: 189 [256/118836 (0%)] Loss: 12377.634766\n",
      "Train Epoch: 189 [33024/118836 (28%)] Loss: 12277.665039\n",
      "Train Epoch: 189 [65792/118836 (55%)] Loss: 12365.747070\n",
      "Train Epoch: 189 [98560/118836 (83%)] Loss: 12278.400391\n",
      "    epoch          : 189\n",
      "    loss           : 12313.917273282414\n",
      "    val_loss       : 12311.323735198961\n",
      "    val_log_likelihood: -12214.808377759253\n",
      "    val_log_marginal: -12222.373048614369\n",
      "Train Epoch: 190 [256/118836 (0%)] Loss: 12270.558594\n",
      "Train Epoch: 190 [33024/118836 (28%)] Loss: 12331.479492\n",
      "Train Epoch: 190 [65792/118836 (55%)] Loss: 12295.978516\n",
      "Train Epoch: 190 [98560/118836 (83%)] Loss: 12330.708984\n",
      "    epoch          : 190\n",
      "    loss           : 12315.93709709729\n",
      "    val_loss       : 12309.389634258827\n",
      "    val_log_likelihood: -12215.148315853754\n",
      "    val_log_marginal: -12222.605945996856\n",
      "Train Epoch: 191 [256/118836 (0%)] Loss: 12236.533203\n",
      "Train Epoch: 191 [33024/118836 (28%)] Loss: 12360.088867\n",
      "Train Epoch: 191 [65792/118836 (55%)] Loss: 12256.003906\n",
      "Train Epoch: 191 [98560/118836 (83%)] Loss: 12399.420898\n",
      "    epoch          : 191\n",
      "    loss           : 12314.203030009305\n",
      "    val_loss       : 12312.289274110357\n",
      "    val_log_likelihood: -12215.31135364971\n",
      "    val_log_marginal: -12222.723381279358\n",
      "Train Epoch: 192 [256/118836 (0%)] Loss: 12356.645508\n",
      "Train Epoch: 192 [33024/118836 (28%)] Loss: 12288.554688\n",
      "Train Epoch: 192 [65792/118836 (55%)] Loss: 12358.104492\n",
      "Train Epoch: 192 [98560/118836 (83%)] Loss: 12316.038086\n",
      "    epoch          : 192\n",
      "    loss           : 12314.345815886063\n",
      "    val_loss       : 12312.952278271152\n",
      "    val_log_likelihood: -12212.65769812345\n",
      "    val_log_marginal: -12220.070032321475\n",
      "Train Epoch: 193 [256/118836 (0%)] Loss: 12418.017578\n",
      "Train Epoch: 193 [33024/118836 (28%)] Loss: 12485.566406\n",
      "Train Epoch: 193 [65792/118836 (55%)] Loss: 12267.939453\n",
      "Train Epoch: 193 [98560/118836 (83%)] Loss: 12396.012695\n",
      "    epoch          : 193\n",
      "    loss           : 12313.27313443445\n",
      "    val_loss       : 12314.926201542165\n",
      "    val_log_likelihood: -12215.001169936415\n",
      "    val_log_marginal: -12222.382772267196\n",
      "Train Epoch: 194 [256/118836 (0%)] Loss: 12326.406250\n",
      "Train Epoch: 194 [33024/118836 (28%)] Loss: 12286.804688\n",
      "Train Epoch: 194 [65792/118836 (55%)] Loss: 12335.011719\n",
      "Train Epoch: 194 [98560/118836 (83%)] Loss: 12292.253906\n",
      "    epoch          : 194\n",
      "    loss           : 12309.872469499587\n",
      "    val_loss       : 12312.756162002139\n",
      "    val_log_likelihood: -12214.981785211177\n",
      "    val_log_marginal: -12222.687870157051\n",
      "Train Epoch: 195 [256/118836 (0%)] Loss: 12408.644531\n",
      "Train Epoch: 195 [33024/118836 (28%)] Loss: 12277.192383\n",
      "Train Epoch: 195 [65792/118836 (55%)] Loss: 12330.670898\n",
      "Train Epoch: 195 [98560/118836 (83%)] Loss: 12233.892578\n",
      "    epoch          : 195\n",
      "    loss           : 12308.714634059656\n",
      "    val_loss       : 12314.652611480475\n",
      "    val_log_likelihood: -12214.060735887097\n",
      "    val_log_marginal: -12221.627947111165\n",
      "Train Epoch: 196 [256/118836 (0%)] Loss: 12298.783203\n",
      "Train Epoch: 196 [33024/118836 (28%)] Loss: 12302.792969\n",
      "Train Epoch: 196 [65792/118836 (55%)] Loss: 12292.562500\n",
      "Train Epoch: 196 [98560/118836 (83%)] Loss: 12272.203125\n",
      "    epoch          : 196\n",
      "    loss           : 12308.232662098066\n",
      "    val_loss       : 12315.03679645474\n",
      "    val_log_likelihood: -12213.723499534739\n",
      "    val_log_marginal: -12221.240108668586\n",
      "Train Epoch: 197 [256/118836 (0%)] Loss: 12393.125977\n",
      "Train Epoch: 197 [33024/118836 (28%)] Loss: 12311.308594\n",
      "Train Epoch: 197 [65792/118836 (55%)] Loss: 12302.492188\n",
      "Train Epoch: 197 [98560/118836 (83%)] Loss: 12392.394531\n",
      "    epoch          : 197\n",
      "    loss           : 12310.753188973325\n",
      "    val_loss       : 12311.159001433332\n",
      "    val_log_likelihood: -12214.635577407724\n",
      "    val_log_marginal: -12222.227627465627\n",
      "Train Epoch: 198 [256/118836 (0%)] Loss: 12307.472656\n",
      "Train Epoch: 198 [33024/118836 (28%)] Loss: 12337.795898\n",
      "Train Epoch: 198 [65792/118836 (55%)] Loss: 12415.048828\n",
      "Train Epoch: 198 [98560/118836 (83%)] Loss: 12412.585938\n",
      "    epoch          : 198\n",
      "    loss           : 12312.083779369572\n",
      "    val_loss       : 12313.761275314702\n",
      "    val_log_likelihood: -12215.160810522642\n",
      "    val_log_marginal: -12222.718054604333\n",
      "Train Epoch: 199 [256/118836 (0%)] Loss: 12443.033203\n",
      "Train Epoch: 199 [33024/118836 (28%)] Loss: 12337.228516\n",
      "Train Epoch: 199 [65792/118836 (55%)] Loss: 12273.291992\n",
      "Train Epoch: 199 [98560/118836 (83%)] Loss: 12337.730469\n",
      "    epoch          : 199\n",
      "    loss           : 12311.479565853753\n",
      "    val_loss       : 12311.022577933498\n",
      "    val_log_likelihood: -12215.380914140043\n",
      "    val_log_marginal: -12222.957611518246\n",
      "Train Epoch: 200 [256/118836 (0%)] Loss: 12221.093750\n",
      "Train Epoch: 200 [33024/118836 (28%)] Loss: 12324.810547\n",
      "Train Epoch: 200 [65792/118836 (55%)] Loss: 12256.112305\n",
      "Train Epoch: 200 [98560/118836 (83%)] Loss: 12317.167969\n",
      "    epoch          : 200\n",
      "    loss           : 12310.9362542972\n",
      "    val_loss       : 12311.95110358722\n",
      "    val_log_likelihood: -12216.898767059552\n",
      "    val_log_marginal: -12224.433072536354\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [256/118836 (0%)] Loss: 12277.472656\n",
      "Train Epoch: 201 [33024/118836 (28%)] Loss: 12319.217773\n",
      "Train Epoch: 201 [65792/118836 (55%)] Loss: 12295.866211\n",
      "Train Epoch: 201 [98560/118836 (83%)] Loss: 12352.937500\n",
      "    epoch          : 201\n",
      "    loss           : 12316.95786774969\n",
      "    val_loss       : 12313.344892047155\n",
      "    val_log_likelihood: -12215.0378975716\n",
      "    val_log_marginal: -12222.594907010112\n",
      "Train Epoch: 202 [256/118836 (0%)] Loss: 12374.529297\n",
      "Train Epoch: 202 [33024/118836 (28%)] Loss: 12436.330078\n",
      "Train Epoch: 202 [65792/118836 (55%)] Loss: 12237.244141\n",
      "Train Epoch: 202 [98560/118836 (83%)] Loss: 12285.119141\n",
      "    epoch          : 202\n",
      "    loss           : 12311.136883691326\n",
      "    val_loss       : 12315.83712835938\n",
      "    val_log_likelihood: -12216.631077142783\n",
      "    val_log_marginal: -12224.195281868862\n",
      "Train Epoch: 203 [256/118836 (0%)] Loss: 12373.884766\n",
      "Train Epoch: 203 [33024/118836 (28%)] Loss: 12366.367188\n",
      "Train Epoch: 203 [65792/118836 (55%)] Loss: 12322.118164\n",
      "Train Epoch: 203 [98560/118836 (83%)] Loss: 12268.478516\n",
      "    epoch          : 203\n",
      "    loss           : 12309.387639093517\n",
      "    val_loss       : 12309.394092513125\n",
      "    val_log_likelihood: -12212.290982184399\n",
      "    val_log_marginal: -12219.81667274111\n",
      "Train Epoch: 204 [256/118836 (0%)] Loss: 12379.693359\n",
      "Train Epoch: 204 [33024/118836 (28%)] Loss: 12422.637695\n",
      "Train Epoch: 204 [65792/118836 (55%)] Loss: 12275.636719\n",
      "Train Epoch: 204 [98560/118836 (83%)] Loss: 12327.734375\n",
      "    epoch          : 204\n",
      "    loss           : 12312.490528393817\n",
      "    val_loss       : 12309.810633386729\n",
      "    val_log_likelihood: -12212.624041369418\n",
      "    val_log_marginal: -12220.30039766636\n",
      "Train Epoch: 205 [256/118836 (0%)] Loss: 12302.787109\n",
      "Train Epoch: 205 [33024/118836 (28%)] Loss: 12312.031250\n",
      "Train Epoch: 205 [65792/118836 (55%)] Loss: 12318.376953\n",
      "Train Epoch: 205 [98560/118836 (83%)] Loss: 12362.763672\n",
      "    epoch          : 205\n",
      "    loss           : 12314.216636780138\n",
      "    val_loss       : 12308.690536344558\n",
      "    val_log_likelihood: -12216.115500284326\n",
      "    val_log_marginal: -12223.61785691899\n",
      "Train Epoch: 206 [256/118836 (0%)] Loss: 12304.933594\n",
      "Train Epoch: 206 [33024/118836 (28%)] Loss: 12357.652344\n",
      "Train Epoch: 206 [65792/118836 (55%)] Loss: 12346.382812\n",
      "Train Epoch: 206 [98560/118836 (83%)] Loss: 12410.946289\n",
      "    epoch          : 206\n",
      "    loss           : 12308.65724821004\n",
      "    val_loss       : 12307.879793959632\n",
      "    val_log_likelihood: -12211.887727460711\n",
      "    val_log_marginal: -12219.333803886151\n",
      "Train Epoch: 207 [256/118836 (0%)] Loss: 12381.230469\n",
      "Train Epoch: 207 [33024/118836 (28%)] Loss: 12380.150391\n",
      "Train Epoch: 207 [65792/118836 (55%)] Loss: 12320.968750\n",
      "Train Epoch: 207 [98560/118836 (83%)] Loss: 12347.375000\n",
      "    epoch          : 207\n",
      "    loss           : 12311.658392621743\n",
      "    val_loss       : 12305.190668161777\n",
      "    val_log_likelihood: -12213.868250814206\n",
      "    val_log_marginal: -12221.328432173032\n",
      "Train Epoch: 208 [256/118836 (0%)] Loss: 12272.162109\n",
      "Train Epoch: 208 [33024/118836 (28%)] Loss: 12317.296875\n",
      "Train Epoch: 208 [65792/118836 (55%)] Loss: 12389.820312\n",
      "Train Epoch: 208 [98560/118836 (83%)] Loss: 12315.779297\n",
      "    epoch          : 208\n",
      "    loss           : 12311.701955548231\n",
      "    val_loss       : 12312.734377170967\n",
      "    val_log_likelihood: -12216.163917590726\n",
      "    val_log_marginal: -12223.753889687934\n",
      "Train Epoch: 209 [256/118836 (0%)] Loss: 12316.097656\n",
      "Train Epoch: 209 [33024/118836 (28%)] Loss: 12420.651367\n",
      "Train Epoch: 209 [65792/118836 (55%)] Loss: 12322.284180\n",
      "Train Epoch: 209 [98560/118836 (83%)] Loss: 12432.302734\n",
      "    epoch          : 209\n",
      "    loss           : 12311.60355358897\n",
      "    val_loss       : 12310.25990120113\n",
      "    val_log_likelihood: -12213.6640962637\n",
      "    val_log_marginal: -12221.071897460033\n",
      "Train Epoch: 210 [256/118836 (0%)] Loss: 12278.768555\n",
      "Train Epoch: 210 [33024/118836 (28%)] Loss: 12321.930664\n",
      "Train Epoch: 210 [65792/118836 (55%)] Loss: 12410.473633\n",
      "Train Epoch: 210 [98560/118836 (83%)] Loss: 12283.890625\n",
      "    epoch          : 210\n",
      "    loss           : 12311.580429816739\n",
      "    val_loss       : 12314.632054271511\n",
      "    val_log_likelihood: -12215.485830070049\n",
      "    val_log_marginal: -12223.489341130067\n",
      "Train Epoch: 211 [256/118836 (0%)] Loss: 12238.493164\n",
      "Train Epoch: 211 [33024/118836 (28%)] Loss: 12331.291016\n",
      "Train Epoch: 211 [65792/118836 (55%)] Loss: 12331.094727\n",
      "Train Epoch: 211 [98560/118836 (83%)] Loss: 12381.904297\n",
      "    epoch          : 211\n",
      "    loss           : 12307.74157199907\n",
      "    val_loss       : 12312.348684489445\n",
      "    val_log_likelihood: -12217.606142408498\n",
      "    val_log_marginal: -12225.113568470057\n",
      "Train Epoch: 212 [256/118836 (0%)] Loss: 12258.360352\n",
      "Train Epoch: 212 [33024/118836 (28%)] Loss: 12273.682617\n",
      "Train Epoch: 212 [65792/118836 (55%)] Loss: 12308.546875\n",
      "Train Epoch: 212 [98560/118836 (83%)] Loss: 12360.289062\n",
      "    epoch          : 212\n",
      "    loss           : 12314.583512652502\n",
      "    val_loss       : 12312.264407353381\n",
      "    val_log_likelihood: -12214.689680101066\n",
      "    val_log_marginal: -12222.539697051028\n",
      "Train Epoch: 213 [256/118836 (0%)] Loss: 12323.252930\n",
      "Train Epoch: 213 [33024/118836 (28%)] Loss: 12266.392578\n",
      "Train Epoch: 213 [65792/118836 (55%)] Loss: 12459.374023\n",
      "Train Epoch: 213 [98560/118836 (83%)] Loss: 12349.140625\n",
      "    epoch          : 213\n",
      "    loss           : 12317.50859795027\n",
      "    val_loss       : 12311.934909990174\n",
      "    val_log_likelihood: -12215.716712546528\n",
      "    val_log_marginal: -12223.455424738288\n",
      "Train Epoch: 214 [256/118836 (0%)] Loss: 12306.982422\n",
      "Train Epoch: 214 [33024/118836 (28%)] Loss: 12265.223633\n",
      "Train Epoch: 214 [65792/118836 (55%)] Loss: 12415.611328\n",
      "Train Epoch: 214 [98560/118836 (83%)] Loss: 12342.289062\n",
      "    epoch          : 214\n",
      "    loss           : 12308.92769140302\n",
      "    val_loss       : 12314.66310415502\n",
      "    val_log_likelihood: -12218.433344641748\n",
      "    val_log_marginal: -12225.870851764825\n",
      "Train Epoch: 215 [256/118836 (0%)] Loss: 12322.746094\n",
      "Train Epoch: 215 [33024/118836 (28%)] Loss: 12311.726562\n",
      "Train Epoch: 215 [65792/118836 (55%)] Loss: 12262.771484\n",
      "Train Epoch: 215 [98560/118836 (83%)] Loss: 12305.486328\n",
      "    epoch          : 215\n",
      "    loss           : 12309.239759583073\n",
      "    val_loss       : 12311.136957350836\n",
      "    val_log_likelihood: -12213.270620896661\n",
      "    val_log_marginal: -12220.873247515687\n",
      "Train Epoch: 216 [256/118836 (0%)] Loss: 12351.470703\n",
      "Train Epoch: 216 [33024/118836 (28%)] Loss: 12215.000000\n",
      "Train Epoch: 216 [65792/118836 (55%)] Loss: 12363.434570\n",
      "Train Epoch: 216 [98560/118836 (83%)] Loss: 12344.078125\n",
      "    epoch          : 216\n",
      "    loss           : 12308.806758717174\n",
      "    val_loss       : 12307.22374540166\n",
      "    val_log_likelihood: -12213.255187008892\n",
      "    val_log_marginal: -12220.63602846146\n",
      "Train Epoch: 217 [256/118836 (0%)] Loss: 12326.624023\n",
      "Train Epoch: 217 [33024/118836 (28%)] Loss: 12298.987305\n",
      "Train Epoch: 217 [65792/118836 (55%)] Loss: 12251.299805\n",
      "Train Epoch: 217 [98560/118836 (83%)] Loss: 12312.585938\n",
      "    epoch          : 217\n",
      "    loss           : 12309.620710394696\n",
      "    val_loss       : 12309.733230306427\n",
      "    val_log_likelihood: -12212.889359265148\n",
      "    val_log_marginal: -12220.322662038048\n",
      "Train Epoch: 218 [256/118836 (0%)] Loss: 12260.612305\n",
      "Train Epoch: 218 [33024/118836 (28%)] Loss: 12425.634766\n",
      "Train Epoch: 218 [65792/118836 (55%)] Loss: 12251.942383\n",
      "Train Epoch: 218 [98560/118836 (83%)] Loss: 12265.320312\n",
      "    epoch          : 218\n",
      "    loss           : 12307.0135238963\n",
      "    val_loss       : 12307.634277126996\n",
      "    val_log_likelihood: -12211.284423141542\n",
      "    val_log_marginal: -12218.722199309765\n",
      "Train Epoch: 219 [256/118836 (0%)] Loss: 12306.285156\n",
      "Train Epoch: 219 [33024/118836 (28%)] Loss: 12295.383789\n",
      "Train Epoch: 219 [65792/118836 (55%)] Loss: 12308.168945\n",
      "Train Epoch: 219 [98560/118836 (83%)] Loss: 12288.831055\n",
      "    epoch          : 219\n",
      "    loss           : 12312.679126441017\n",
      "    val_loss       : 12310.632716248587\n",
      "    val_log_likelihood: -12215.554950339898\n",
      "    val_log_marginal: -12223.099380764865\n",
      "Train Epoch: 220 [256/118836 (0%)] Loss: 12266.451172\n",
      "Train Epoch: 220 [33024/118836 (28%)] Loss: 12294.119141\n",
      "Train Epoch: 220 [65792/118836 (55%)] Loss: 12286.006836\n",
      "Train Epoch: 220 [98560/118836 (83%)] Loss: 12384.507812\n",
      "    epoch          : 220\n",
      "    loss           : 12308.595567262459\n",
      "    val_loss       : 12309.382650129808\n",
      "    val_log_likelihood: -12217.673154143404\n",
      "    val_log_marginal: -12225.263037947445\n",
      "Train Epoch: 221 [256/118836 (0%)] Loss: 12360.753906\n",
      "Train Epoch: 221 [33024/118836 (28%)] Loss: 12310.369141\n",
      "Train Epoch: 221 [65792/118836 (55%)] Loss: 12412.445312\n",
      "Train Epoch: 221 [98560/118836 (83%)] Loss: 12309.278320\n",
      "    epoch          : 221\n",
      "    loss           : 12313.843926734387\n",
      "    val_loss       : 12312.249561152448\n",
      "    val_log_likelihood: -12213.586158014114\n",
      "    val_log_marginal: -12221.033216057716\n",
      "Train Epoch: 222 [256/118836 (0%)] Loss: 12307.853516\n",
      "Train Epoch: 222 [33024/118836 (28%)] Loss: 12299.439453\n",
      "Train Epoch: 222 [65792/118836 (55%)] Loss: 12346.169922\n",
      "Train Epoch: 222 [98560/118836 (83%)] Loss: 12289.589844\n",
      "    epoch          : 222\n",
      "    loss           : 12316.995548845896\n",
      "    val_loss       : 12313.493339922323\n",
      "    val_log_likelihood: -12217.010771427833\n",
      "    val_log_marginal: -12225.045030245625\n",
      "Train Epoch: 223 [256/118836 (0%)] Loss: 12299.759766\n",
      "Train Epoch: 223 [33024/118836 (28%)] Loss: 12376.268555\n",
      "Train Epoch: 223 [65792/118836 (55%)] Loss: 12338.859375\n",
      "Train Epoch: 223 [98560/118836 (83%)] Loss: 12327.618164\n",
      "    epoch          : 223\n",
      "    loss           : 12307.832709431865\n",
      "    val_loss       : 12310.594703446646\n",
      "    val_log_likelihood: -12210.364704010288\n",
      "    val_log_marginal: -12217.762153786378\n",
      "Train Epoch: 224 [256/118836 (0%)] Loss: 12276.183594\n",
      "Train Epoch: 224 [33024/118836 (28%)] Loss: 12322.908203\n",
      "Train Epoch: 224 [65792/118836 (55%)] Loss: 12316.982422\n",
      "Train Epoch: 224 [98560/118836 (83%)] Loss: 12333.355469\n",
      "    epoch          : 224\n",
      "    loss           : 12309.098843795233\n",
      "    val_loss       : 12311.494366463365\n",
      "    val_log_likelihood: -12213.88484106829\n",
      "    val_log_marginal: -12221.384902979766\n",
      "Train Epoch: 225 [256/118836 (0%)] Loss: 12294.292969\n",
      "Train Epoch: 225 [33024/118836 (28%)] Loss: 12441.875000\n",
      "Train Epoch: 225 [65792/118836 (55%)] Loss: 12297.863281\n",
      "Train Epoch: 225 [98560/118836 (83%)] Loss: 12265.699219\n",
      "    epoch          : 225\n",
      "    loss           : 12310.344307343361\n",
      "    val_loss       : 12312.53628931071\n",
      "    val_log_likelihood: -12212.075067365851\n",
      "    val_log_marginal: -12219.648240736153\n",
      "Train Epoch: 226 [256/118836 (0%)] Loss: 12263.331055\n",
      "Train Epoch: 226 [33024/118836 (28%)] Loss: 12397.631836\n",
      "Train Epoch: 226 [65792/118836 (55%)] Loss: 12360.075195\n",
      "Train Epoch: 226 [98560/118836 (83%)] Loss: 12407.342773\n",
      "    epoch          : 226\n",
      "    loss           : 12313.536272067566\n",
      "    val_loss       : 12309.285162098273\n",
      "    val_log_likelihood: -12214.500936498396\n",
      "    val_log_marginal: -12221.928054636881\n",
      "Train Epoch: 227 [256/118836 (0%)] Loss: 12257.652344\n",
      "Train Epoch: 227 [33024/118836 (28%)] Loss: 12367.767578\n",
      "Train Epoch: 227 [65792/118836 (55%)] Loss: 12377.029297\n",
      "Train Epoch: 227 [98560/118836 (83%)] Loss: 12331.201172\n",
      "    epoch          : 227\n",
      "    loss           : 12314.099670601994\n",
      "    val_loss       : 12312.401188449005\n",
      "    val_log_likelihood: -12213.962263007908\n",
      "    val_log_marginal: -12221.3746469567\n",
      "Train Epoch: 228 [256/118836 (0%)] Loss: 12409.603516\n",
      "Train Epoch: 228 [33024/118836 (28%)] Loss: 12326.995117\n",
      "Train Epoch: 228 [65792/118836 (55%)] Loss: 12267.429688\n",
      "Train Epoch: 228 [98560/118836 (83%)] Loss: 12387.110352\n",
      "    epoch          : 228\n",
      "    loss           : 12312.310114408861\n",
      "    val_loss       : 12308.553513676174\n",
      "    val_log_likelihood: -12214.605659216035\n",
      "    val_log_marginal: -12222.01893841151\n",
      "Train Epoch: 229 [256/118836 (0%)] Loss: 12341.734375\n",
      "Train Epoch: 229 [33024/118836 (28%)] Loss: 12197.638672\n",
      "Train Epoch: 229 [65792/118836 (55%)] Loss: 12354.788086\n",
      "Train Epoch: 229 [98560/118836 (83%)] Loss: 12349.414062\n",
      "    epoch          : 229\n",
      "    loss           : 12312.087048309553\n",
      "    val_loss       : 12309.978592494366\n",
      "    val_log_likelihood: -12214.005523353495\n",
      "    val_log_marginal: -12221.853600552606\n",
      "Train Epoch: 230 [256/118836 (0%)] Loss: 12258.792969\n",
      "Train Epoch: 230 [33024/118836 (28%)] Loss: 12378.152344\n",
      "Train Epoch: 230 [65792/118836 (55%)] Loss: 12264.216797\n",
      "Train Epoch: 230 [98560/118836 (83%)] Loss: 12288.208008\n",
      "    epoch          : 230\n",
      "    loss           : 12310.972344783911\n",
      "    val_loss       : 12309.69977534481\n",
      "    val_log_likelihood: -12213.694781165994\n",
      "    val_log_marginal: -12221.233671628657\n",
      "Train Epoch: 231 [256/118836 (0%)] Loss: 12340.457031\n",
      "Train Epoch: 231 [33024/118836 (28%)] Loss: 12357.040039\n",
      "Train Epoch: 231 [65792/118836 (55%)] Loss: 12303.712891\n",
      "Train Epoch: 231 [98560/118836 (83%)] Loss: 12393.776367\n",
      "    epoch          : 231\n",
      "    loss           : 12310.958991160049\n",
      "    val_loss       : 12310.516238195863\n",
      "    val_log_likelihood: -12211.926501111457\n",
      "    val_log_marginal: -12219.573421376841\n",
      "Train Epoch: 232 [256/118836 (0%)] Loss: 12313.706055\n",
      "Train Epoch: 232 [33024/118836 (28%)] Loss: 12350.300781\n",
      "Train Epoch: 232 [65792/118836 (55%)] Loss: 12261.001953\n",
      "Train Epoch: 232 [98560/118836 (83%)] Loss: 12341.400391\n",
      "    epoch          : 232\n",
      "    loss           : 12310.488353785411\n",
      "    val_loss       : 12310.385270645493\n",
      "    val_log_likelihood: -12212.030900408396\n",
      "    val_log_marginal: -12219.653001105806\n",
      "Train Epoch: 233 [256/118836 (0%)] Loss: 12412.478516\n",
      "Train Epoch: 233 [33024/118836 (28%)] Loss: 12377.107422\n",
      "Train Epoch: 233 [65792/118836 (55%)] Loss: 12419.202148\n",
      "Train Epoch: 233 [98560/118836 (83%)] Loss: 12310.682617\n",
      "    epoch          : 233\n",
      "    loss           : 12310.901693839176\n",
      "    val_loss       : 12310.2162209523\n",
      "    val_log_likelihood: -12212.56637377869\n",
      "    val_log_marginal: -12220.104954430757\n",
      "Train Epoch: 234 [256/118836 (0%)] Loss: 12263.824219\n",
      "Train Epoch: 234 [33024/118836 (28%)] Loss: 12319.134766\n",
      "Train Epoch: 234 [65792/118836 (55%)] Loss: 12294.714844\n",
      "Train Epoch: 234 [98560/118836 (83%)] Loss: 12366.109375\n",
      "    epoch          : 234\n",
      "    loss           : 12309.359507146919\n",
      "    val_loss       : 12306.487371358824\n",
      "    val_log_likelihood: -12210.977862806296\n",
      "    val_log_marginal: -12218.59082497629\n",
      "Train Epoch: 235 [256/118836 (0%)] Loss: 12347.574219\n",
      "Train Epoch: 235 [33024/118836 (28%)] Loss: 12285.370117\n",
      "Train Epoch: 235 [65792/118836 (55%)] Loss: 12248.369141\n",
      "Train Epoch: 235 [98560/118836 (83%)] Loss: 12307.662109\n",
      "    epoch          : 235\n",
      "    loss           : 12308.695543030139\n",
      "    val_loss       : 12311.712905202605\n",
      "    val_log_likelihood: -12213.519915574596\n",
      "    val_log_marginal: -12221.067061241014\n",
      "Train Epoch: 236 [256/118836 (0%)] Loss: 12252.498047\n",
      "Train Epoch: 236 [33024/118836 (28%)] Loss: 12370.155273\n",
      "Train Epoch: 236 [65792/118836 (55%)] Loss: 12354.344727\n",
      "Train Epoch: 236 [98560/118836 (83%)] Loss: 12308.070312\n",
      "    epoch          : 236\n",
      "    loss           : 12312.908850612594\n",
      "    val_loss       : 12313.666486939252\n",
      "    val_log_likelihood: -12212.758334302625\n",
      "    val_log_marginal: -12220.351049548279\n",
      "Train Epoch: 237 [256/118836 (0%)] Loss: 12333.730469\n",
      "Train Epoch: 237 [33024/118836 (28%)] Loss: 12301.590820\n",
      "Train Epoch: 237 [65792/118836 (55%)] Loss: 12425.637695\n",
      "Train Epoch: 237 [98560/118836 (83%)] Loss: 12211.426758\n",
      "    epoch          : 237\n",
      "    loss           : 12311.512588851841\n",
      "    val_loss       : 12311.516248351165\n",
      "    val_log_likelihood: -12211.897449144437\n",
      "    val_log_marginal: -12219.547169421732\n",
      "Train Epoch: 238 [256/118836 (0%)] Loss: 12390.632812\n",
      "Train Epoch: 238 [33024/118836 (28%)] Loss: 12312.541992\n",
      "Train Epoch: 238 [65792/118836 (55%)] Loss: 12250.722656\n",
      "Train Epoch: 238 [98560/118836 (83%)] Loss: 12305.915039\n",
      "    epoch          : 238\n",
      "    loss           : 12306.969669697322\n",
      "    val_loss       : 12312.721568061252\n",
      "    val_log_likelihood: -12215.36240161678\n",
      "    val_log_marginal: -12222.810043230165\n",
      "Train Epoch: 239 [256/118836 (0%)] Loss: 12267.075195\n",
      "Train Epoch: 239 [33024/118836 (28%)] Loss: 12246.667969\n",
      "Train Epoch: 239 [65792/118836 (55%)] Loss: 12356.972656\n",
      "Train Epoch: 239 [98560/118836 (83%)] Loss: 12323.074219\n",
      "    epoch          : 239\n",
      "    loss           : 12309.550834399555\n",
      "    val_loss       : 12308.234669424477\n",
      "    val_log_likelihood: -12213.45325553143\n",
      "    val_log_marginal: -12220.99664858897\n",
      "Train Epoch: 240 [256/118836 (0%)] Loss: 12330.774414\n",
      "Train Epoch: 240 [33024/118836 (28%)] Loss: 12305.663086\n",
      "Train Epoch: 240 [65792/118836 (55%)] Loss: 12353.301758\n",
      "Train Epoch: 240 [98560/118836 (83%)] Loss: 12215.168945\n",
      "    epoch          : 240\n",
      "    loss           : 12307.64905849359\n",
      "    val_loss       : 12310.886231600147\n",
      "    val_log_likelihood: -12210.944480846774\n",
      "    val_log_marginal: -12218.542768044275\n",
      "Train Epoch: 241 [256/118836 (0%)] Loss: 12364.962891\n",
      "Train Epoch: 241 [33024/118836 (28%)] Loss: 12420.968750\n",
      "Train Epoch: 241 [65792/118836 (55%)] Loss: 12405.670898\n",
      "Train Epoch: 241 [98560/118836 (83%)] Loss: 12259.830078\n",
      "    epoch          : 241\n",
      "    loss           : 12308.828440666357\n",
      "    val_loss       : 12314.678592351323\n",
      "    val_log_likelihood: -12211.973826671061\n",
      "    val_log_marginal: -12219.600335909934\n",
      "Train Epoch: 242 [256/118836 (0%)] Loss: 12320.470703\n",
      "Train Epoch: 242 [33024/118836 (28%)] Loss: 12297.631836\n",
      "Train Epoch: 242 [65792/118836 (55%)] Loss: 12479.750977\n",
      "Train Epoch: 242 [98560/118836 (83%)] Loss: 12407.760742\n",
      "    epoch          : 242\n",
      "    loss           : 12309.209255130789\n",
      "    val_loss       : 12311.770050976123\n",
      "    val_log_likelihood: -12212.486279983457\n",
      "    val_log_marginal: -12220.147190809093\n",
      "Train Epoch: 243 [256/118836 (0%)] Loss: 12322.162109\n",
      "Train Epoch: 243 [33024/118836 (28%)] Loss: 12380.768555\n",
      "Train Epoch: 243 [65792/118836 (55%)] Loss: 12257.782227\n",
      "Train Epoch: 243 [98560/118836 (83%)] Loss: 12283.679688\n",
      "    epoch          : 243\n",
      "    loss           : 12310.119058881308\n",
      "    val_loss       : 12311.96986847946\n",
      "    val_log_likelihood: -12213.792497027502\n",
      "    val_log_marginal: -12221.768325643152\n",
      "Train Epoch: 244 [256/118836 (0%)] Loss: 12239.136719\n",
      "Train Epoch: 244 [33024/118836 (28%)] Loss: 12302.595703\n",
      "Train Epoch: 244 [65792/118836 (55%)] Loss: 12272.275391\n",
      "Train Epoch: 244 [98560/118836 (83%)] Loss: 12268.879883\n",
      "    epoch          : 244\n",
      "    loss           : 12307.050699990954\n",
      "    val_loss       : 12307.031713607132\n",
      "    val_log_likelihood: -12211.655142582971\n",
      "    val_log_marginal: -12219.163564073722\n",
      "Train Epoch: 245 [256/118836 (0%)] Loss: 12380.077148\n",
      "Train Epoch: 245 [33024/118836 (28%)] Loss: 12317.619141\n",
      "Train Epoch: 245 [65792/118836 (55%)] Loss: 12384.060547\n",
      "Train Epoch: 245 [98560/118836 (83%)] Loss: 12403.908203\n",
      "    epoch          : 245\n",
      "    loss           : 12311.315359413771\n",
      "    val_loss       : 12311.228948817252\n",
      "    val_log_likelihood: -12211.666621109905\n",
      "    val_log_marginal: -12219.141755665745\n",
      "Train Epoch: 246 [256/118836 (0%)] Loss: 12324.001953\n",
      "Train Epoch: 246 [33024/118836 (28%)] Loss: 12323.677734\n",
      "Train Epoch: 246 [65792/118836 (55%)] Loss: 12373.208984\n",
      "Train Epoch: 246 [98560/118836 (83%)] Loss: 12322.384766\n",
      "    epoch          : 246\n",
      "    loss           : 12310.328118214951\n",
      "    val_loss       : 12307.621121545795\n",
      "    val_log_likelihood: -12213.432545136735\n",
      "    val_log_marginal: -12221.069192788122\n",
      "Train Epoch: 247 [256/118836 (0%)] Loss: 12342.260742\n",
      "Train Epoch: 247 [33024/118836 (28%)] Loss: 12260.632812\n",
      "Train Epoch: 247 [65792/118836 (55%)] Loss: 12283.431641\n",
      "Train Epoch: 247 [98560/118836 (83%)] Loss: 12320.091797\n",
      "    epoch          : 247\n",
      "    loss           : 12311.248330716242\n",
      "    val_loss       : 12308.854713059383\n",
      "    val_log_likelihood: -12213.5528826768\n",
      "    val_log_marginal: -12221.01572165277\n",
      "Train Epoch: 248 [256/118836 (0%)] Loss: 12189.574219\n",
      "Train Epoch: 248 [33024/118836 (28%)] Loss: 12309.115234\n",
      "Train Epoch: 248 [65792/118836 (55%)] Loss: 12419.689453\n",
      "Train Epoch: 248 [98560/118836 (83%)] Loss: 12247.973633\n",
      "    epoch          : 248\n",
      "    loss           : 12307.631470029466\n",
      "    val_loss       : 12314.545535397405\n",
      "    val_log_likelihood: -12217.236822625879\n",
      "    val_log_marginal: -12225.026495592792\n",
      "Train Epoch: 249 [256/118836 (0%)] Loss: 12416.755859\n",
      "Train Epoch: 249 [33024/118836 (28%)] Loss: 12315.324219\n",
      "Train Epoch: 249 [65792/118836 (55%)] Loss: 12321.484375\n",
      "Train Epoch: 249 [98560/118836 (83%)] Loss: 12355.550781\n",
      "    epoch          : 249\n",
      "    loss           : 12312.212323265612\n",
      "    val_loss       : 12312.520024603724\n",
      "    val_log_likelihood: -12215.775485938791\n",
      "    val_log_marginal: -12223.369686130125\n",
      "Train Epoch: 250 [256/118836 (0%)] Loss: 12277.876953\n",
      "Train Epoch: 250 [33024/118836 (28%)] Loss: 12380.518555\n",
      "Train Epoch: 250 [65792/118836 (55%)] Loss: 12409.882812\n",
      "Train Epoch: 250 [98560/118836 (83%)] Loss: 12314.115234\n",
      "    epoch          : 250\n",
      "    loss           : 12315.775151532775\n",
      "    val_loss       : 12316.959539139727\n",
      "    val_log_likelihood: -12213.51053734362\n",
      "    val_log_marginal: -12221.19950899022\n",
      "Train Epoch: 251 [256/118836 (0%)] Loss: 12356.533203\n",
      "Train Epoch: 251 [33024/118836 (28%)] Loss: 12333.054688\n",
      "Train Epoch: 251 [65792/118836 (55%)] Loss: 12254.066406\n",
      "Train Epoch: 251 [98560/118836 (83%)] Loss: 12355.672852\n",
      "    epoch          : 251\n",
      "    loss           : 12308.79809404725\n",
      "    val_loss       : 12311.17923600491\n",
      "    val_log_likelihood: -12214.885799537326\n",
      "    val_log_marginal: -12222.450385106848\n",
      "Train Epoch: 252 [256/118836 (0%)] Loss: 12346.507812\n",
      "Train Epoch: 252 [33024/118836 (28%)] Loss: 12369.537109\n",
      "Train Epoch: 252 [65792/118836 (55%)] Loss: 12374.782227\n",
      "Train Epoch: 252 [98560/118836 (83%)] Loss: 12291.098633\n",
      "    epoch          : 252\n",
      "    loss           : 12311.660655112697\n",
      "    val_loss       : 12309.878546284646\n",
      "    val_log_likelihood: -12213.577037453473\n",
      "    val_log_marginal: -12221.067294764782\n",
      "Train Epoch: 253 [256/118836 (0%)] Loss: 12294.565430\n",
      "Train Epoch: 253 [33024/118836 (28%)] Loss: 12250.482422\n",
      "Train Epoch: 253 [65792/118836 (55%)] Loss: 12326.753906\n",
      "Train Epoch: 253 [98560/118836 (83%)] Loss: 12264.281250\n",
      "    epoch          : 253\n",
      "    loss           : 12307.856105575373\n",
      "    val_loss       : 12311.156937960794\n",
      "    val_log_likelihood: -12213.301901590932\n",
      "    val_log_marginal: -12220.841468933728\n",
      "Train Epoch: 254 [256/118836 (0%)] Loss: 12280.036133\n",
      "Train Epoch: 254 [33024/118836 (28%)] Loss: 12408.615234\n",
      "Train Epoch: 254 [65792/118836 (55%)] Loss: 12360.216797\n",
      "Train Epoch: 254 [98560/118836 (83%)] Loss: 12335.106445\n",
      "    epoch          : 254\n",
      "    loss           : 12306.320128334368\n",
      "    val_loss       : 12309.685420469546\n",
      "    val_log_likelihood: -12210.868025130532\n",
      "    val_log_marginal: -12218.43930664886\n",
      "Train Epoch: 255 [256/118836 (0%)] Loss: 12252.995117\n",
      "Train Epoch: 255 [33024/118836 (28%)] Loss: 12312.268555\n",
      "Train Epoch: 255 [65792/118836 (55%)] Loss: 12345.644531\n",
      "Train Epoch: 255 [98560/118836 (83%)] Loss: 12365.093750\n",
      "    epoch          : 255\n",
      "    loss           : 12309.416944369055\n",
      "    val_loss       : 12309.703468349137\n",
      "    val_log_likelihood: -12213.110537989816\n",
      "    val_log_marginal: -12221.022063460121\n",
      "Train Epoch: 256 [256/118836 (0%)] Loss: 12380.056641\n",
      "Train Epoch: 256 [33024/118836 (28%)] Loss: 12359.773438\n",
      "Train Epoch: 256 [65792/118836 (55%)] Loss: 12334.738281\n",
      "Train Epoch: 256 [98560/118836 (83%)] Loss: 12323.685547\n",
      "    epoch          : 256\n",
      "    loss           : 12310.022145109595\n",
      "    val_loss       : 12311.369263701159\n",
      "    val_log_likelihood: -12213.828233722343\n",
      "    val_log_marginal: -12221.369891820657\n",
      "Train Epoch: 257 [256/118836 (0%)] Loss: 12374.431641\n",
      "Train Epoch: 257 [33024/118836 (28%)] Loss: 12327.168945\n",
      "Train Epoch: 257 [65792/118836 (55%)] Loss: 12362.501953\n",
      "Train Epoch: 257 [98560/118836 (83%)] Loss: 12245.821289\n",
      "    epoch          : 257\n",
      "    loss           : 12306.628439858612\n",
      "    val_loss       : 12309.423566812973\n",
      "    val_log_likelihood: -12212.567031282311\n",
      "    val_log_marginal: -12220.12488023431\n",
      "Train Epoch: 258 [256/118836 (0%)] Loss: 12423.193359\n",
      "Train Epoch: 258 [33024/118836 (28%)] Loss: 12349.500000\n",
      "Train Epoch: 258 [65792/118836 (55%)] Loss: 12339.225586\n",
      "Train Epoch: 258 [98560/118836 (83%)] Loss: 12333.076172\n",
      "    epoch          : 258\n",
      "    loss           : 12314.429664075426\n",
      "    val_loss       : 12310.933126297461\n",
      "    val_log_likelihood: -12212.471597459162\n",
      "    val_log_marginal: -12220.241067581945\n",
      "Train Epoch: 259 [256/118836 (0%)] Loss: 12436.323242\n",
      "Train Epoch: 259 [33024/118836 (28%)] Loss: 12322.951172\n",
      "Train Epoch: 259 [65792/118836 (55%)] Loss: 12328.695312\n",
      "Train Epoch: 259 [98560/118836 (83%)] Loss: 12278.817383\n",
      "    epoch          : 259\n",
      "    loss           : 12310.203326612904\n",
      "    val_loss       : 12310.0409648879\n",
      "    val_log_likelihood: -12211.041198175144\n",
      "    val_log_marginal: -12218.63487203509\n",
      "Train Epoch: 260 [256/118836 (0%)] Loss: 12372.462891\n",
      "Train Epoch: 260 [33024/118836 (28%)] Loss: 12321.381836\n",
      "Train Epoch: 260 [65792/118836 (55%)] Loss: 12389.367188\n",
      "Train Epoch: 260 [98560/118836 (83%)] Loss: 12287.210938\n",
      "    epoch          : 260\n",
      "    loss           : 12309.96437509693\n",
      "    val_loss       : 12307.903339393188\n",
      "    val_log_likelihood: -12211.105361966242\n",
      "    val_log_marginal: -12218.778672642038\n",
      "Train Epoch: 261 [256/118836 (0%)] Loss: 12367.911133\n",
      "Train Epoch: 261 [33024/118836 (28%)] Loss: 12378.259766\n",
      "Train Epoch: 261 [65792/118836 (55%)] Loss: 12250.287109\n",
      "Train Epoch: 261 [98560/118836 (83%)] Loss: 12295.554688\n",
      "    epoch          : 261\n",
      "    loss           : 12306.194092968104\n",
      "    val_loss       : 12312.060627324096\n",
      "    val_log_likelihood: -12215.919453868124\n",
      "    val_log_marginal: -12223.867611046757\n",
      "Train Epoch: 262 [256/118836 (0%)] Loss: 12383.029297\n",
      "Train Epoch: 262 [33024/118836 (28%)] Loss: 12345.412109\n",
      "Train Epoch: 262 [65792/118836 (55%)] Loss: 12292.444336\n",
      "Train Epoch: 262 [98560/118836 (83%)] Loss: 12365.947266\n",
      "    epoch          : 262\n",
      "    loss           : 12312.303216275071\n",
      "    val_loss       : 12308.129341903808\n",
      "    val_log_likelihood: -12214.148353171528\n",
      "    val_log_marginal: -12221.836937627799\n",
      "Train Epoch: 263 [256/118836 (0%)] Loss: 12354.664062\n",
      "Train Epoch: 263 [33024/118836 (28%)] Loss: 12373.833008\n",
      "Train Epoch: 263 [65792/118836 (55%)] Loss: 12375.432617\n",
      "Train Epoch: 263 [98560/118836 (83%)] Loss: 12325.968750\n",
      "    epoch          : 263\n",
      "    loss           : 12307.673250103391\n",
      "    val_loss       : 12307.05392006475\n",
      "    val_log_likelihood: -12211.05597649788\n",
      "    val_log_marginal: -12218.809362967617\n",
      "Train Epoch: 264 [256/118836 (0%)] Loss: 12305.222656\n",
      "Train Epoch: 264 [33024/118836 (28%)] Loss: 12239.710938\n",
      "Train Epoch: 264 [65792/118836 (55%)] Loss: 12257.871094\n",
      "Train Epoch: 264 [98560/118836 (83%)] Loss: 12311.623047\n",
      "    epoch          : 264\n",
      "    loss           : 12303.124063501604\n",
      "    val_loss       : 12309.43880042872\n",
      "    val_log_likelihood: -12210.284705367298\n",
      "    val_log_marginal: -12218.241104674069\n",
      "Train Epoch: 265 [256/118836 (0%)] Loss: 12310.892578\n",
      "Train Epoch: 265 [33024/118836 (28%)] Loss: 12252.599609\n",
      "Train Epoch: 265 [65792/118836 (55%)] Loss: 12291.757812\n",
      "Train Epoch: 265 [98560/118836 (83%)] Loss: 12285.854492\n",
      "    epoch          : 265\n",
      "    loss           : 12317.618067294768\n",
      "    val_loss       : 12308.67294956865\n",
      "    val_log_likelihood: -12214.973236371743\n",
      "    val_log_marginal: -12222.853739010188\n",
      "Train Epoch: 266 [256/118836 (0%)] Loss: 12239.833984\n",
      "Train Epoch: 266 [33024/118836 (28%)] Loss: 12331.942383\n",
      "Train Epoch: 266 [65792/118836 (55%)] Loss: 12247.996094\n",
      "Train Epoch: 266 [98560/118836 (83%)] Loss: 12415.407227\n",
      "    epoch          : 266\n",
      "    loss           : 12309.914042791048\n",
      "    val_loss       : 12307.652027774706\n",
      "    val_log_likelihood: -12206.378642440808\n",
      "    val_log_marginal: -12214.06530581072\n",
      "Train Epoch: 267 [256/118836 (0%)] Loss: 12262.868164\n",
      "Train Epoch: 267 [33024/118836 (28%)] Loss: 12454.517578\n",
      "Train Epoch: 267 [65792/118836 (55%)] Loss: 12290.931641\n",
      "Train Epoch: 267 [98560/118836 (83%)] Loss: 12332.246094\n",
      "    epoch          : 267\n",
      "    loss           : 12309.245650298542\n",
      "    val_loss       : 12307.737168763479\n",
      "    val_log_likelihood: -12209.58422685975\n",
      "    val_log_marginal: -12217.236997465538\n",
      "Train Epoch: 268 [256/118836 (0%)] Loss: 12439.591797\n",
      "Train Epoch: 268 [33024/118836 (28%)] Loss: 12311.857422\n",
      "Train Epoch: 268 [65792/118836 (55%)] Loss: 12355.707031\n",
      "Train Epoch: 268 [98560/118836 (83%)] Loss: 12217.609375\n",
      "    epoch          : 268\n",
      "    loss           : 12305.484202465881\n",
      "    val_loss       : 12307.425799600687\n",
      "    val_log_likelihood: -12209.989231803143\n",
      "    val_log_marginal: -12217.770306971599\n",
      "Train Epoch: 269 [256/118836 (0%)] Loss: 12293.118164\n",
      "Train Epoch: 269 [33024/118836 (28%)] Loss: 12303.722656\n",
      "Train Epoch: 269 [65792/118836 (55%)] Loss: 12424.228516\n",
      "Train Epoch: 269 [98560/118836 (83%)] Loss: 12260.493164\n",
      "    epoch          : 269\n",
      "    loss           : 12306.934087604683\n",
      "    val_loss       : 12308.987849837751\n",
      "    val_log_likelihood: -12209.326995935431\n",
      "    val_log_marginal: -12217.000152605511\n",
      "Train Epoch: 270 [256/118836 (0%)] Loss: 12327.073242\n",
      "Train Epoch: 270 [33024/118836 (28%)] Loss: 12222.636719\n",
      "Train Epoch: 270 [65792/118836 (55%)] Loss: 12287.384766\n",
      "Train Epoch: 270 [98560/118836 (83%)] Loss: 12449.824219\n",
      "    epoch          : 270\n",
      "    loss           : 12313.369566629188\n",
      "    val_loss       : 12310.165107958588\n",
      "    val_log_likelihood: -12210.101706116882\n",
      "    val_log_marginal: -12217.668323361826\n",
      "Train Epoch: 271 [256/118836 (0%)] Loss: 12291.879883\n",
      "Train Epoch: 271 [33024/118836 (28%)] Loss: 12276.960938\n",
      "Train Epoch: 271 [65792/118836 (55%)] Loss: 12441.242188\n",
      "Train Epoch: 271 [98560/118836 (83%)] Loss: 12358.979492\n",
      "    epoch          : 271\n",
      "    loss           : 12309.003792196547\n",
      "    val_loss       : 12308.216091164439\n",
      "    val_log_likelihood: -12208.381706052263\n",
      "    val_log_marginal: -12216.199842939508\n",
      "Train Epoch: 272 [256/118836 (0%)] Loss: 12298.541016\n",
      "Train Epoch: 272 [33024/118836 (28%)] Loss: 12292.392578\n",
      "Train Epoch: 272 [65792/118836 (55%)] Loss: 12319.162109\n",
      "Train Epoch: 272 [98560/118836 (83%)] Loss: 12398.160156\n",
      "    epoch          : 272\n",
      "    loss           : 12310.885012633116\n",
      "    val_loss       : 12308.88105714838\n",
      "    val_log_likelihood: -12207.670077769593\n",
      "    val_log_marginal: -12215.192880092522\n",
      "Train Epoch: 273 [256/118836 (0%)] Loss: 12308.309570\n",
      "Train Epoch: 273 [33024/118836 (28%)] Loss: 12284.423828\n",
      "Train Epoch: 273 [65792/118836 (55%)] Loss: 12341.194336\n",
      "Train Epoch: 273 [98560/118836 (83%)] Loss: 12334.083984\n",
      "    epoch          : 273\n",
      "    loss           : 12314.365319188119\n",
      "    val_loss       : 12304.98043761045\n",
      "    val_log_likelihood: -12208.697865617245\n",
      "    val_log_marginal: -12216.581284838721\n",
      "Train Epoch: 274 [256/118836 (0%)] Loss: 12342.154297\n",
      "Train Epoch: 274 [33024/118836 (28%)] Loss: 12439.978516\n",
      "Train Epoch: 274 [65792/118836 (55%)] Loss: 12325.724609\n",
      "Train Epoch: 274 [98560/118836 (83%)] Loss: 12377.158203\n",
      "    epoch          : 274\n",
      "    loss           : 12309.563698692102\n",
      "    val_loss       : 12305.591466243532\n",
      "    val_log_likelihood: -12207.57830141129\n",
      "    val_log_marginal: -12215.563335210472\n",
      "Train Epoch: 275 [256/118836 (0%)] Loss: 12348.651367\n",
      "Train Epoch: 275 [33024/118836 (28%)] Loss: 12242.589844\n",
      "Train Epoch: 275 [65792/118836 (55%)] Loss: 12459.681641\n",
      "Train Epoch: 275 [98560/118836 (83%)] Loss: 12316.554688\n",
      "    epoch          : 275\n",
      "    loss           : 12309.883527999638\n",
      "    val_loss       : 12307.611298279231\n",
      "    val_log_likelihood: -12209.842528044872\n",
      "    val_log_marginal: -12217.624296072703\n",
      "Train Epoch: 276 [256/118836 (0%)] Loss: 12317.361328\n",
      "Train Epoch: 276 [33024/118836 (28%)] Loss: 12295.306641\n",
      "Train Epoch: 276 [65792/118836 (55%)] Loss: 12282.539062\n",
      "Train Epoch: 276 [98560/118836 (83%)] Loss: 12284.824219\n",
      "    epoch          : 276\n",
      "    loss           : 12306.905850489817\n",
      "    val_loss       : 12310.7057247263\n",
      "    val_log_likelihood: -12208.778114983974\n",
      "    val_log_marginal: -12216.577827772277\n",
      "Train Epoch: 277 [256/118836 (0%)] Loss: 12366.277344\n",
      "Train Epoch: 277 [33024/118836 (28%)] Loss: 12288.833984\n",
      "Train Epoch: 277 [65792/118836 (55%)] Loss: 12433.097656\n",
      "Train Epoch: 277 [98560/118836 (83%)] Loss: 12402.532227\n",
      "    epoch          : 277\n",
      "    loss           : 12304.886718265354\n",
      "    val_loss       : 12313.00785033586\n",
      "    val_log_likelihood: -12208.528054726272\n",
      "    val_log_marginal: -12216.384373931269\n",
      "Train Epoch: 278 [256/118836 (0%)] Loss: 12385.579102\n",
      "Train Epoch: 278 [33024/118836 (28%)] Loss: 12341.804688\n",
      "Train Epoch: 278 [65792/118836 (55%)] Loss: 12379.158203\n",
      "Train Epoch: 278 [98560/118836 (83%)] Loss: 12357.978516\n",
      "    epoch          : 278\n",
      "    loss           : 12309.334949144437\n",
      "    val_loss       : 12308.405245428818\n",
      "    val_log_likelihood: -12209.020771137044\n",
      "    val_log_marginal: -12216.60564441282\n",
      "Train Epoch: 279 [256/118836 (0%)] Loss: 12402.047852\n",
      "Train Epoch: 279 [33024/118836 (28%)] Loss: 12309.285156\n",
      "Train Epoch: 279 [65792/118836 (55%)] Loss: 12244.676758\n",
      "Train Epoch: 279 [98560/118836 (83%)] Loss: 12258.060547\n",
      "    epoch          : 279\n",
      "    loss           : 12305.412971076303\n",
      "    val_loss       : 12308.724589649553\n",
      "    val_log_likelihood: -12209.571084864816\n",
      "    val_log_marginal: -12217.242248662456\n",
      "Train Epoch: 280 [256/118836 (0%)] Loss: 12435.956055\n",
      "Train Epoch: 280 [33024/118836 (28%)] Loss: 12262.115234\n",
      "Train Epoch: 280 [65792/118836 (55%)] Loss: 12286.593750\n",
      "Train Epoch: 280 [98560/118836 (83%)] Loss: 12328.207031\n",
      "    epoch          : 280\n",
      "    loss           : 12309.377638091913\n",
      "    val_loss       : 12309.063463744631\n",
      "    val_log_likelihood: -12208.792122395833\n",
      "    val_log_marginal: -12216.604769233174\n",
      "Train Epoch: 281 [256/118836 (0%)] Loss: 12320.715820\n",
      "Train Epoch: 281 [33024/118836 (28%)] Loss: 12348.397461\n",
      "Train Epoch: 281 [65792/118836 (55%)] Loss: 12411.857422\n",
      "Train Epoch: 281 [98560/118836 (83%)] Loss: 12379.859375\n",
      "    epoch          : 281\n",
      "    loss           : 12308.527622098583\n",
      "    val_loss       : 12313.621624022793\n",
      "    val_log_likelihood: -12213.490806580852\n",
      "    val_log_marginal: -12221.066583253672\n",
      "Train Epoch: 282 [256/118836 (0%)] Loss: 12315.070312\n",
      "Train Epoch: 282 [33024/118836 (28%)] Loss: 12253.063477\n",
      "Train Epoch: 282 [65792/118836 (55%)] Loss: 12284.996094\n",
      "Train Epoch: 282 [98560/118836 (83%)] Loss: 12323.533203\n",
      "    epoch          : 282\n",
      "    loss           : 12311.383291976841\n",
      "    val_loss       : 12310.38928883016\n",
      "    val_log_likelihood: -12205.373725864609\n",
      "    val_log_marginal: -12212.960604897027\n",
      "Train Epoch: 283 [256/118836 (0%)] Loss: 12292.968750\n",
      "Train Epoch: 283 [33024/118836 (28%)] Loss: 12300.765625\n",
      "Train Epoch: 283 [65792/118836 (55%)] Loss: 12264.189453\n",
      "Train Epoch: 283 [98560/118836 (83%)] Loss: 12356.753906\n",
      "    epoch          : 283\n",
      "    loss           : 12307.234356583436\n",
      "    val_loss       : 12308.056078377378\n",
      "    val_log_likelihood: -12205.412438288358\n",
      "    val_log_marginal: -12213.026570700502\n",
      "Train Epoch: 284 [256/118836 (0%)] Loss: 12200.480469\n",
      "Train Epoch: 284 [33024/118836 (28%)] Loss: 12305.925781\n",
      "Train Epoch: 284 [65792/118836 (55%)] Loss: 12289.830078\n",
      "Train Epoch: 284 [98560/118836 (83%)] Loss: 12193.092773\n",
      "    epoch          : 284\n",
      "    loss           : 12309.9358904893\n",
      "    val_loss       : 12305.954414048681\n",
      "    val_log_likelihood: -12206.750860247364\n",
      "    val_log_marginal: -12214.44175894196\n",
      "Train Epoch: 285 [256/118836 (0%)] Loss: 12340.176758\n",
      "Train Epoch: 285 [33024/118836 (28%)] Loss: 12350.181641\n",
      "Train Epoch: 285 [65792/118836 (55%)] Loss: 12301.504883\n",
      "Train Epoch: 285 [98560/118836 (83%)] Loss: 12301.583008\n",
      "    epoch          : 285\n",
      "    loss           : 12309.672997602616\n",
      "    val_loss       : 12305.476125438487\n",
      "    val_log_likelihood: -12207.929723686932\n",
      "    val_log_marginal: -12215.633963827166\n",
      "Train Epoch: 286 [256/118836 (0%)] Loss: 12344.242188\n",
      "Train Epoch: 286 [33024/118836 (28%)] Loss: 12352.609375\n",
      "Train Epoch: 286 [65792/118836 (55%)] Loss: 12386.618164\n",
      "Train Epoch: 286 [98560/118836 (83%)] Loss: 12238.916016\n",
      "    epoch          : 286\n",
      "    loss           : 12309.229864880583\n",
      "    val_loss       : 12307.189814851872\n",
      "    val_log_likelihood: -12208.171191810121\n",
      "    val_log_marginal: -12215.86020684519\n",
      "Train Epoch: 287 [256/118836 (0%)] Loss: 12370.075195\n",
      "Train Epoch: 287 [33024/118836 (28%)] Loss: 12370.916016\n",
      "Train Epoch: 287 [65792/118836 (55%)] Loss: 12305.117188\n",
      "Train Epoch: 287 [98560/118836 (83%)] Loss: 12301.830078\n",
      "    epoch          : 287\n",
      "    loss           : 12308.770952071702\n",
      "    val_loss       : 12306.959147708594\n",
      "    val_log_likelihood: -12210.348914068962\n",
      "    val_log_marginal: -12217.990606439227\n",
      "Train Epoch: 288 [256/118836 (0%)] Loss: 12313.603516\n",
      "Train Epoch: 288 [33024/118836 (28%)] Loss: 12288.383789\n",
      "Train Epoch: 288 [65792/118836 (55%)] Loss: 12356.386719\n",
      "Train Epoch: 288 [98560/118836 (83%)] Loss: 12395.089844\n",
      "    epoch          : 288\n",
      "    loss           : 12309.43383752714\n",
      "    val_loss       : 12312.320744266488\n",
      "    val_log_likelihood: -12208.787180133375\n",
      "    val_log_marginal: -12216.556399763935\n",
      "Train Epoch: 289 [256/118836 (0%)] Loss: 12251.320312\n",
      "Train Epoch: 289 [33024/118836 (28%)] Loss: 12242.681641\n",
      "Train Epoch: 289 [65792/118836 (55%)] Loss: 12176.621094\n",
      "Train Epoch: 289 [98560/118836 (83%)] Loss: 12403.639648\n",
      "    epoch          : 289\n",
      "    loss           : 12304.439132773728\n",
      "    val_loss       : 12323.474162978766\n",
      "    val_log_likelihood: -12213.96808054177\n",
      "    val_log_marginal: -12222.300834841677\n",
      "Train Epoch: 290 [256/118836 (0%)] Loss: 12323.582031\n",
      "Train Epoch: 290 [33024/118836 (28%)] Loss: 12391.308594\n",
      "Train Epoch: 290 [65792/118836 (55%)] Loss: 12209.010742\n",
      "Train Epoch: 290 [98560/118836 (83%)] Loss: 12290.065430\n",
      "    epoch          : 290\n",
      "    loss           : 12311.94354257134\n",
      "    val_loss       : 12304.99371808023\n",
      "    val_log_likelihood: -12208.083522668527\n",
      "    val_log_marginal: -12215.75703221689\n",
      "Train Epoch: 291 [256/118836 (0%)] Loss: 12276.796875\n",
      "Train Epoch: 291 [33024/118836 (28%)] Loss: 12490.439453\n",
      "Train Epoch: 291 [65792/118836 (55%)] Loss: 12239.100586\n",
      "Train Epoch: 291 [98560/118836 (83%)] Loss: 12366.996094\n",
      "    epoch          : 291\n",
      "    loss           : 12308.437512116161\n",
      "    val_loss       : 12306.329276705419\n",
      "    val_log_likelihood: -12209.530724804848\n",
      "    val_log_marginal: -12217.137717854663\n",
      "Train Epoch: 292 [256/118836 (0%)] Loss: 12313.381836\n",
      "Train Epoch: 292 [33024/118836 (28%)] Loss: 12318.792969\n",
      "Train Epoch: 292 [65792/118836 (55%)] Loss: 12366.316406\n",
      "Train Epoch: 292 [98560/118836 (83%)] Loss: 12314.765625\n",
      "    epoch          : 292\n",
      "    loss           : 12309.800857662582\n",
      "    val_loss       : 12308.7731512539\n",
      "    val_log_likelihood: -12207.170091178145\n",
      "    val_log_marginal: -12214.7900348459\n",
      "Train Epoch: 293 [256/118836 (0%)] Loss: 12357.387695\n",
      "Train Epoch: 293 [33024/118836 (28%)] Loss: 12350.644531\n",
      "Train Epoch: 293 [65792/118836 (55%)] Loss: 12302.392578\n",
      "Train Epoch: 293 [98560/118836 (83%)] Loss: 12272.580078\n",
      "    epoch          : 293\n",
      "    loss           : 12303.625730362128\n",
      "    val_loss       : 12309.342920761392\n",
      "    val_log_likelihood: -12206.105204940808\n",
      "    val_log_marginal: -12213.943601872566\n",
      "Train Epoch: 294 [256/118836 (0%)] Loss: 12220.233398\n",
      "Train Epoch: 294 [33024/118836 (28%)] Loss: 12323.867188\n",
      "Train Epoch: 294 [65792/118836 (55%)] Loss: 12235.548828\n",
      "Train Epoch: 294 [98560/118836 (83%)] Loss: 12380.887695\n",
      "    epoch          : 294\n",
      "    loss           : 12316.011245088916\n",
      "    val_loss       : 12305.385055628507\n",
      "    val_log_likelihood: -12207.990855045493\n",
      "    val_log_marginal: -12215.63526682388\n",
      "Train Epoch: 295 [256/118836 (0%)] Loss: 12409.360352\n",
      "Train Epoch: 295 [33024/118836 (28%)] Loss: 12328.747070\n",
      "Train Epoch: 295 [65792/118836 (55%)] Loss: 12292.889648\n",
      "Train Epoch: 295 [98560/118836 (83%)] Loss: 12382.242188\n",
      "    epoch          : 295\n",
      "    loss           : 12309.860031049679\n",
      "    val_loss       : 12309.106103317015\n",
      "    val_log_likelihood: -12208.49507663875\n",
      "    val_log_marginal: -12216.247616234497\n",
      "Train Epoch: 296 [256/118836 (0%)] Loss: 12375.884766\n",
      "Train Epoch: 296 [33024/118836 (28%)] Loss: 12403.347656\n",
      "Train Epoch: 296 [65792/118836 (55%)] Loss: 12263.434570\n",
      "Train Epoch: 296 [98560/118836 (83%)] Loss: 12462.107422\n",
      "    epoch          : 296\n",
      "    loss           : 12310.891709800197\n",
      "    val_loss       : 12310.643860284681\n",
      "    val_log_likelihood: -12208.616635972394\n",
      "    val_log_marginal: -12216.353659322123\n",
      "Train Epoch: 297 [256/118836 (0%)] Loss: 12458.875000\n",
      "Train Epoch: 297 [33024/118836 (28%)] Loss: 12234.728516\n",
      "Train Epoch: 297 [65792/118836 (55%)] Loss: 12373.812500\n",
      "Train Epoch: 297 [98560/118836 (83%)] Loss: 12271.714844\n",
      "    epoch          : 297\n",
      "    loss           : 12309.711502597705\n",
      "    val_loss       : 12304.16562078942\n",
      "    val_log_likelihood: -12205.715912556865\n",
      "    val_log_marginal: -12213.366287676708\n",
      "Train Epoch: 298 [256/118836 (0%)] Loss: 12289.598633\n",
      "Train Epoch: 298 [33024/118836 (28%)] Loss: 12407.322266\n",
      "Train Epoch: 298 [65792/118836 (55%)] Loss: 12412.832031\n",
      "Train Epoch: 298 [98560/118836 (83%)] Loss: 12287.875000\n",
      "    epoch          : 298\n",
      "    loss           : 12313.659788888028\n",
      "    val_loss       : 12308.798014632755\n",
      "    val_log_likelihood: -12206.85146670156\n",
      "    val_log_marginal: -12214.425027481\n",
      "Train Epoch: 299 [256/118836 (0%)] Loss: 12291.128906\n",
      "Train Epoch: 299 [33024/118836 (28%)] Loss: 12238.930664\n",
      "Train Epoch: 299 [65792/118836 (55%)] Loss: 12308.893555\n",
      "Train Epoch: 299 [98560/118836 (83%)] Loss: 12292.324219\n",
      "    epoch          : 299\n",
      "    loss           : 12310.349632476478\n",
      "    val_loss       : 12306.82347947176\n",
      "    val_log_likelihood: -12210.400344583593\n",
      "    val_log_marginal: -12218.119600215952\n",
      "Train Epoch: 300 [256/118836 (0%)] Loss: 12300.927734\n",
      "Train Epoch: 300 [33024/118836 (28%)] Loss: 12421.099609\n",
      "Train Epoch: 300 [65792/118836 (55%)] Loss: 12352.165039\n",
      "Train Epoch: 300 [98560/118836 (83%)] Loss: 12252.615234\n",
      "    epoch          : 300\n",
      "    loss           : 12308.165865707713\n",
      "    val_loss       : 12308.612724127792\n",
      "    val_log_likelihood: -12211.775458313947\n",
      "    val_log_marginal: -12219.319372474136\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [256/118836 (0%)] Loss: 12331.660156\n",
      "Train Epoch: 301 [33024/118836 (28%)] Loss: 12239.377930\n",
      "Train Epoch: 301 [65792/118836 (55%)] Loss: 12289.066406\n",
      "Train Epoch: 301 [98560/118836 (83%)] Loss: 12236.796875\n",
      "    epoch          : 301\n",
      "    loss           : 12306.62284219267\n",
      "    val_loss       : 12305.465668510302\n",
      "    val_log_likelihood: -12207.543837236353\n",
      "    val_log_marginal: -12215.373789689476\n",
      "Train Epoch: 302 [256/118836 (0%)] Loss: 12341.729492\n",
      "Train Epoch: 302 [33024/118836 (28%)] Loss: 12273.866211\n",
      "Train Epoch: 302 [65792/118836 (55%)] Loss: 12221.203125\n",
      "Train Epoch: 302 [98560/118836 (83%)] Loss: 12276.129883\n",
      "    epoch          : 302\n",
      "    loss           : 12305.93156776649\n",
      "    val_loss       : 12309.279125059138\n",
      "    val_log_likelihood: -12211.694706207352\n",
      "    val_log_marginal: -12219.40232839982\n",
      "Train Epoch: 303 [256/118836 (0%)] Loss: 12344.760742\n",
      "Train Epoch: 303 [33024/118836 (28%)] Loss: 12232.145508\n",
      "Train Epoch: 303 [65792/118836 (55%)] Loss: 12347.294922\n",
      "Train Epoch: 303 [98560/118836 (83%)] Loss: 12434.910156\n",
      "    epoch          : 303\n",
      "    loss           : 12310.157949654933\n",
      "    val_loss       : 12313.744473267503\n",
      "    val_log_likelihood: -12207.56524374483\n",
      "    val_log_marginal: -12215.392473607933\n",
      "Train Epoch: 304 [256/118836 (0%)] Loss: 12417.545898\n",
      "Train Epoch: 304 [33024/118836 (28%)] Loss: 12288.420898\n",
      "Train Epoch: 304 [65792/118836 (55%)] Loss: 12347.634766\n",
      "Train Epoch: 304 [98560/118836 (83%)] Loss: 12400.934570\n",
      "    epoch          : 304\n",
      "    loss           : 12310.641108030914\n",
      "    val_loss       : 12308.473347262223\n",
      "    val_log_likelihood: -12204.879729502689\n",
      "    val_log_marginal: -12212.462676014098\n",
      "Train Epoch: 305 [256/118836 (0%)] Loss: 12330.928711\n",
      "Train Epoch: 305 [33024/118836 (28%)] Loss: 12290.844727\n",
      "Train Epoch: 305 [65792/118836 (55%)] Loss: 12400.058594\n",
      "Train Epoch: 305 [98560/118836 (83%)] Loss: 12252.982422\n",
      "    epoch          : 305\n",
      "    loss           : 12306.146026061053\n",
      "    val_loss       : 12315.338425262395\n",
      "    val_log_likelihood: -12213.117067307692\n",
      "    val_log_marginal: -12220.761359505774\n",
      "Train Epoch: 306 [256/118836 (0%)] Loss: 12216.871094\n",
      "Train Epoch: 306 [33024/118836 (28%)] Loss: 12294.875977\n",
      "Train Epoch: 306 [65792/118836 (55%)] Loss: 12296.713867\n",
      "Train Epoch: 306 [98560/118836 (83%)] Loss: 12284.087891\n",
      "    epoch          : 306\n",
      "    loss           : 12309.234735253825\n",
      "    val_loss       : 12305.2819177109\n",
      "    val_log_likelihood: -12207.856037886424\n",
      "    val_log_marginal: -12215.675964617503\n",
      "Train Epoch: 307 [256/118836 (0%)] Loss: 12331.548828\n",
      "Train Epoch: 307 [33024/118836 (28%)] Loss: 12318.343750\n",
      "Train Epoch: 307 [65792/118836 (55%)] Loss: 12397.324219\n",
      "Train Epoch: 307 [98560/118836 (83%)] Loss: 12345.898438\n",
      "    epoch          : 307\n",
      "    loss           : 12311.980444194583\n",
      "    val_loss       : 12314.713266273018\n",
      "    val_log_likelihood: -12209.138644088607\n",
      "    val_log_marginal: -12216.69434077685\n",
      "Train Epoch: 308 [256/118836 (0%)] Loss: 12391.794922\n",
      "Train Epoch: 308 [33024/118836 (28%)] Loss: 12413.111328\n",
      "Train Epoch: 308 [65792/118836 (55%)] Loss: 12313.220703\n",
      "Train Epoch: 308 [98560/118836 (83%)] Loss: 12239.950195\n",
      "    epoch          : 308\n",
      "    loss           : 12310.208413461538\n",
      "    val_loss       : 12306.483074719685\n",
      "    val_log_likelihood: -12205.207989395936\n",
      "    val_log_marginal: -12213.088447692975\n",
      "Train Epoch: 309 [256/118836 (0%)] Loss: 12325.106445\n",
      "Train Epoch: 309 [33024/118836 (28%)] Loss: 12259.090820\n",
      "Train Epoch: 309 [65792/118836 (55%)] Loss: 12321.193359\n",
      "Train Epoch: 309 [98560/118836 (83%)] Loss: 12343.188477\n",
      "    epoch          : 309\n",
      "    loss           : 12306.674411154623\n",
      "    val_loss       : 12312.508424987851\n",
      "    val_log_likelihood: -12210.406756940136\n",
      "    val_log_marginal: -12218.005251469833\n",
      "Train Epoch: 310 [256/118836 (0%)] Loss: 12370.776367\n",
      "Train Epoch: 310 [33024/118836 (28%)] Loss: 12284.193359\n",
      "Train Epoch: 310 [65792/118836 (55%)] Loss: 12298.617188\n",
      "Train Epoch: 310 [98560/118836 (83%)] Loss: 12271.091797\n",
      "    epoch          : 310\n",
      "    loss           : 12303.21554018688\n",
      "    val_loss       : 12307.466638489876\n",
      "    val_log_likelihood: -12206.545135280967\n",
      "    val_log_marginal: -12214.250796198092\n",
      "Train Epoch: 311 [256/118836 (0%)] Loss: 12305.443359\n",
      "Train Epoch: 311 [33024/118836 (28%)] Loss: 12292.485352\n",
      "Train Epoch: 311 [65792/118836 (55%)] Loss: 12369.191406\n",
      "Train Epoch: 311 [98560/118836 (83%)] Loss: 12269.324219\n",
      "    epoch          : 311\n",
      "    loss           : 12310.42729803169\n",
      "    val_loss       : 12312.16305475529\n",
      "    val_log_likelihood: -12209.571481951769\n",
      "    val_log_marginal: -12217.278066099398\n",
      "Train Epoch: 312 [256/118836 (0%)] Loss: 12393.169922\n",
      "Train Epoch: 312 [33024/118836 (28%)] Loss: 12304.867188\n",
      "Train Epoch: 312 [65792/118836 (55%)] Loss: 12314.243164\n",
      "Train Epoch: 312 [98560/118836 (83%)] Loss: 12396.620117\n",
      "    epoch          : 312\n",
      "    loss           : 12305.071805372469\n",
      "    val_loss       : 12309.63466361215\n",
      "    val_log_likelihood: -12208.550354114972\n",
      "    val_log_marginal: -12216.033978726913\n",
      "Train Epoch: 313 [256/118836 (0%)] Loss: 12294.656250\n",
      "Train Epoch: 313 [33024/118836 (28%)] Loss: 12364.384766\n",
      "Train Epoch: 313 [65792/118836 (55%)] Loss: 12313.812500\n",
      "Train Epoch: 313 [98560/118836 (83%)] Loss: 12353.744141\n",
      "    epoch          : 313\n",
      "    loss           : 12307.526153781533\n",
      "    val_loss       : 12306.646264258894\n",
      "    val_log_likelihood: -12205.666849863006\n",
      "    val_log_marginal: -12213.392745186418\n",
      "Train Epoch: 314 [256/118836 (0%)] Loss: 12301.500977\n",
      "Train Epoch: 314 [33024/118836 (28%)] Loss: 12244.519531\n",
      "Train Epoch: 314 [65792/118836 (55%)] Loss: 12310.428711\n",
      "Train Epoch: 314 [98560/118836 (83%)] Loss: 12191.123047\n",
      "    epoch          : 314\n",
      "    loss           : 12313.43652101427\n",
      "    val_loss       : 12316.344394985472\n",
      "    val_log_likelihood: -12214.13883907801\n",
      "    val_log_marginal: -12222.152930043447\n",
      "Train Epoch: 315 [256/118836 (0%)] Loss: 12355.014648\n",
      "Train Epoch: 315 [33024/118836 (28%)] Loss: 12227.642578\n",
      "Train Epoch: 315 [65792/118836 (55%)] Loss: 12337.961914\n",
      "Train Epoch: 315 [98560/118836 (83%)] Loss: 12304.076172\n",
      "    epoch          : 315\n",
      "    loss           : 12313.356888925506\n",
      "    val_loss       : 12317.637687904202\n",
      "    val_log_likelihood: -12209.808911516491\n",
      "    val_log_marginal: -12218.012061658514\n",
      "Train Epoch: 316 [256/118836 (0%)] Loss: 12291.066406\n",
      "Train Epoch: 316 [33024/118836 (28%)] Loss: 12460.501953\n",
      "Train Epoch: 316 [65792/118836 (55%)] Loss: 12295.595703\n",
      "Train Epoch: 316 [98560/118836 (83%)] Loss: 12358.943359\n",
      "    epoch          : 316\n",
      "    loss           : 12309.010524258169\n",
      "    val_loss       : 12306.009616678002\n",
      "    val_log_likelihood: -12208.52525863963\n",
      "    val_log_marginal: -12216.205853084266\n",
      "Train Epoch: 317 [256/118836 (0%)] Loss: 12289.871094\n",
      "Train Epoch: 317 [33024/118836 (28%)] Loss: 12294.791016\n",
      "Train Epoch: 317 [65792/118836 (55%)] Loss: 12360.740234\n",
      "Train Epoch: 317 [98560/118836 (83%)] Loss: 12394.339844\n",
      "    epoch          : 317\n",
      "    loss           : 12310.531052102719\n",
      "    val_loss       : 12313.33246141092\n",
      "    val_log_likelihood: -12209.56408818626\n",
      "    val_log_marginal: -12217.567386059169\n",
      "Train Epoch: 318 [256/118836 (0%)] Loss: 12442.031250\n",
      "Train Epoch: 318 [33024/118836 (28%)] Loss: 12408.790039\n",
      "Train Epoch: 318 [65792/118836 (55%)] Loss: 12379.229492\n",
      "Train Epoch: 318 [98560/118836 (83%)] Loss: 12435.253906\n",
      "    epoch          : 318\n",
      "    loss           : 12306.633816525797\n",
      "    val_loss       : 12309.084938151129\n",
      "    val_log_likelihood: -12209.41283133659\n",
      "    val_log_marginal: -12217.102430424758\n",
      "Train Epoch: 319 [256/118836 (0%)] Loss: 12240.784180\n",
      "Train Epoch: 319 [33024/118836 (28%)] Loss: 12280.180664\n",
      "Train Epoch: 319 [65792/118836 (55%)] Loss: 12323.160156\n",
      "Train Epoch: 319 [98560/118836 (83%)] Loss: 12404.847656\n",
      "    epoch          : 319\n",
      "    loss           : 12309.265952620968\n",
      "    val_loss       : 12307.781267554139\n",
      "    val_log_likelihood: -12207.378003676851\n",
      "    val_log_marginal: -12215.093232975561\n",
      "Train Epoch: 320 [256/118836 (0%)] Loss: 12351.624023\n",
      "Train Epoch: 320 [33024/118836 (28%)] Loss: 12293.462891\n",
      "Train Epoch: 320 [65792/118836 (55%)] Loss: 12199.865234\n",
      "Train Epoch: 320 [98560/118836 (83%)] Loss: 12334.199219\n",
      "    epoch          : 320\n",
      "    loss           : 12306.225443451456\n",
      "    val_loss       : 12311.471216820126\n",
      "    val_log_likelihood: -12207.480982798284\n",
      "    val_log_marginal: -12215.160459923578\n",
      "Train Epoch: 321 [256/118836 (0%)] Loss: 12297.892578\n",
      "Train Epoch: 321 [33024/118836 (28%)] Loss: 12282.615234\n",
      "Train Epoch: 321 [65792/118836 (55%)] Loss: 12281.220703\n",
      "Train Epoch: 321 [98560/118836 (83%)] Loss: 12364.705078\n",
      "    epoch          : 321\n",
      "    loss           : 12306.110355278122\n",
      "    val_loss       : 12312.862678309495\n",
      "    val_log_likelihood: -12207.155123035565\n",
      "    val_log_marginal: -12214.887613665345\n",
      "Train Epoch: 322 [256/118836 (0%)] Loss: 12381.778320\n",
      "Train Epoch: 322 [33024/118836 (28%)] Loss: 12346.757812\n",
      "Train Epoch: 322 [65792/118836 (55%)] Loss: 12276.371094\n",
      "Train Epoch: 322 [98560/118836 (83%)] Loss: 12296.017578\n",
      "    epoch          : 322\n",
      "    loss           : 12309.27269679875\n",
      "    val_loss       : 12307.600959617232\n",
      "    val_log_likelihood: -12206.496269999741\n",
      "    val_log_marginal: -12214.056218871507\n",
      "Train Epoch: 323 [256/118836 (0%)] Loss: 12307.461914\n",
      "Train Epoch: 323 [33024/118836 (28%)] Loss: 12263.501953\n",
      "Train Epoch: 323 [65792/118836 (55%)] Loss: 12316.062500\n",
      "Train Epoch: 323 [98560/118836 (83%)] Loss: 12392.115234\n",
      "    epoch          : 323\n",
      "    loss           : 12311.442079100754\n",
      "    val_loss       : 12303.516060185164\n",
      "    val_log_likelihood: -12208.455193793941\n",
      "    val_log_marginal: -12216.167804009277\n",
      "Train Epoch: 324 [256/118836 (0%)] Loss: 12392.889648\n",
      "Train Epoch: 324 [33024/118836 (28%)] Loss: 12354.742188\n",
      "Train Epoch: 324 [65792/118836 (55%)] Loss: 12242.357422\n",
      "Train Epoch: 324 [98560/118836 (83%)] Loss: 12266.622070\n",
      "    epoch          : 324\n",
      "    loss           : 12309.852077517577\n",
      "    val_loss       : 12309.12336813897\n",
      "    val_log_likelihood: -12207.959080819634\n",
      "    val_log_marginal: -12215.662830976518\n",
      "Train Epoch: 325 [256/118836 (0%)] Loss: 12313.729492\n",
      "Train Epoch: 325 [33024/118836 (28%)] Loss: 12314.274414\n",
      "Train Epoch: 325 [65792/118836 (55%)] Loss: 12412.566406\n",
      "Train Epoch: 325 [98560/118836 (83%)] Loss: 12341.910156\n",
      "    epoch          : 325\n",
      "    loss           : 12308.3764413384\n",
      "    val_loss       : 12306.810696705172\n",
      "    val_log_likelihood: -12211.741994287635\n",
      "    val_log_marginal: -12219.361537602885\n",
      "Train Epoch: 326 [256/118836 (0%)] Loss: 12309.345703\n",
      "Train Epoch: 326 [33024/118836 (28%)] Loss: 12389.563477\n",
      "Train Epoch: 326 [65792/118836 (55%)] Loss: 12276.626953\n",
      "Train Epoch: 326 [98560/118836 (83%)] Loss: 12363.779297\n",
      "    epoch          : 326\n",
      "    loss           : 12306.378354237748\n",
      "    val_loss       : 12304.881761067587\n",
      "    val_log_likelihood: -12209.053686382083\n",
      "    val_log_marginal: -12216.621603472273\n",
      "Train Epoch: 327 [256/118836 (0%)] Loss: 12336.021484\n",
      "Train Epoch: 327 [33024/118836 (28%)] Loss: 12329.094727\n",
      "Train Epoch: 327 [65792/118836 (55%)] Loss: 12348.545898\n",
      "Train Epoch: 327 [98560/118836 (83%)] Loss: 12325.017578\n",
      "    epoch          : 327\n",
      "    loss           : 12306.670614919354\n",
      "    val_loss       : 12305.585795879131\n",
      "    val_log_likelihood: -12206.621928634202\n",
      "    val_log_marginal: -12214.161946417265\n",
      "Train Epoch: 328 [256/118836 (0%)] Loss: 12362.951172\n",
      "Train Epoch: 328 [33024/118836 (28%)] Loss: 12352.724609\n",
      "Train Epoch: 328 [65792/118836 (55%)] Loss: 12439.342773\n",
      "Train Epoch: 328 [98560/118836 (83%)] Loss: 12330.987305\n",
      "    epoch          : 328\n",
      "    loss           : 12310.4450029725\n",
      "    val_loss       : 12307.563680532603\n",
      "    val_log_likelihood: -12211.0978850031\n",
      "    val_log_marginal: -12218.917666045934\n",
      "Train Epoch: 329 [256/118836 (0%)] Loss: 12366.182617\n",
      "Train Epoch: 329 [33024/118836 (28%)] Loss: 12447.521484\n",
      "Train Epoch: 329 [65792/118836 (55%)] Loss: 12357.917969\n",
      "Train Epoch: 329 [98560/118836 (83%)] Loss: 12197.628906\n",
      "    epoch          : 329\n",
      "    loss           : 12307.572595669199\n",
      "    val_loss       : 12304.974596698428\n",
      "    val_log_likelihood: -12208.989768145162\n",
      "    val_log_marginal: -12216.596749800792\n",
      "Train Epoch: 330 [256/118836 (0%)] Loss: 12326.259766\n",
      "Train Epoch: 330 [33024/118836 (28%)] Loss: 12228.975586\n",
      "Train Epoch: 330 [65792/118836 (55%)] Loss: 12339.203125\n",
      "Train Epoch: 330 [98560/118836 (83%)] Loss: 12462.289062\n",
      "    epoch          : 330\n",
      "    loss           : 12303.541821915063\n",
      "    val_loss       : 12310.884062794834\n",
      "    val_log_likelihood: -12209.980146137044\n",
      "    val_log_marginal: -12217.599577470919\n",
      "Train Epoch: 331 [256/118836 (0%)] Loss: 12366.333008\n",
      "Train Epoch: 331 [33024/118836 (28%)] Loss: 12373.023438\n",
      "Train Epoch: 331 [65792/118836 (55%)] Loss: 12381.500000\n",
      "Train Epoch: 331 [98560/118836 (83%)] Loss: 12247.364258\n",
      "    epoch          : 331\n",
      "    loss           : 12310.747604392835\n",
      "    val_loss       : 12315.104497474802\n",
      "    val_log_likelihood: -12215.181688443445\n",
      "    val_log_marginal: -12223.259188528169\n",
      "Train Epoch: 332 [256/118836 (0%)] Loss: 12271.080078\n",
      "Train Epoch: 332 [33024/118836 (28%)] Loss: 12241.437500\n",
      "Train Epoch: 332 [65792/118836 (55%)] Loss: 12368.187500\n",
      "Train Epoch: 332 [98560/118836 (83%)] Loss: 12246.881836\n",
      "    epoch          : 332\n",
      "    loss           : 12307.250387555574\n",
      "    val_loss       : 12311.602265988207\n",
      "    val_log_likelihood: -12209.531441758427\n",
      "    val_log_marginal: -12217.2256000185\n",
      "Train Epoch: 333 [256/118836 (0%)] Loss: 12272.730469\n",
      "Train Epoch: 333 [33024/118836 (28%)] Loss: 12327.413086\n",
      "Train Epoch: 333 [65792/118836 (55%)] Loss: 12361.001953\n",
      "Train Epoch: 333 [98560/118836 (83%)] Loss: 12310.515625\n",
      "    epoch          : 333\n",
      "    loss           : 12303.995152082041\n",
      "    val_loss       : 12312.069038068243\n",
      "    val_log_likelihood: -12209.270305391854\n",
      "    val_log_marginal: -12217.312094643907\n",
      "Train Epoch: 334 [256/118836 (0%)] Loss: 12366.197266\n",
      "Train Epoch: 334 [33024/118836 (28%)] Loss: 12433.355469\n",
      "Train Epoch: 334 [65792/118836 (55%)] Loss: 12391.660156\n",
      "Train Epoch: 334 [98560/118836 (83%)] Loss: 12343.089844\n",
      "    epoch          : 334\n",
      "    loss           : 12308.762538610163\n",
      "    val_loss       : 12310.429789822438\n",
      "    val_log_likelihood: -12207.090376214846\n",
      "    val_log_marginal: -12214.82179992458\n",
      "Train Epoch: 335 [256/118836 (0%)] Loss: 12458.019531\n",
      "Train Epoch: 335 [33024/118836 (28%)] Loss: 12220.570312\n",
      "Train Epoch: 335 [65792/118836 (55%)] Loss: 12288.476562\n",
      "Train Epoch: 335 [98560/118836 (83%)] Loss: 12331.909180\n",
      "    epoch          : 335\n",
      "    loss           : 12310.079370541254\n",
      "    val_loss       : 12310.288066829698\n",
      "    val_log_likelihood: -12208.063370909584\n",
      "    val_log_marginal: -12215.559539243692\n",
      "Train Epoch: 336 [256/118836 (0%)] Loss: 12305.125000\n",
      "Train Epoch: 336 [33024/118836 (28%)] Loss: 12335.738281\n",
      "Train Epoch: 336 [65792/118836 (55%)] Loss: 12312.229492\n",
      "Train Epoch: 336 [98560/118836 (83%)] Loss: 12298.433594\n",
      "    epoch          : 336\n",
      "    loss           : 12308.098987735215\n",
      "    val_loss       : 12309.990740052599\n",
      "    val_log_likelihood: -12207.276777683002\n",
      "    val_log_marginal: -12214.938841248937\n",
      "Train Epoch: 337 [256/118836 (0%)] Loss: 12419.206055\n",
      "Train Epoch: 337 [33024/118836 (28%)] Loss: 12336.227539\n",
      "Train Epoch: 337 [65792/118836 (55%)] Loss: 12398.634766\n",
      "Train Epoch: 337 [98560/118836 (83%)] Loss: 12313.173828\n",
      "    epoch          : 337\n",
      "    loss           : 12306.854795737696\n",
      "    val_loss       : 12306.263549947618\n",
      "    val_log_likelihood: -12205.332733502635\n",
      "    val_log_marginal: -12213.079438016792\n",
      "Train Epoch: 338 [256/118836 (0%)] Loss: 12341.928711\n",
      "Train Epoch: 338 [33024/118836 (28%)] Loss: 12241.765625\n",
      "Train Epoch: 338 [65792/118836 (55%)] Loss: 12248.169922\n",
      "Train Epoch: 338 [98560/118836 (83%)] Loss: 12345.498047\n",
      "    epoch          : 338\n",
      "    loss           : 12306.465453273624\n",
      "    val_loss       : 12311.100920088305\n",
      "    val_log_likelihood: -12206.326870735113\n",
      "    val_log_marginal: -12213.972446313252\n",
      "Train Epoch: 339 [256/118836 (0%)] Loss: 12295.044922\n",
      "Train Epoch: 339 [33024/118836 (28%)] Loss: 12289.904297\n",
      "Train Epoch: 339 [65792/118836 (55%)] Loss: 12369.375977\n",
      "Train Epoch: 339 [98560/118836 (83%)] Loss: 12238.493164\n",
      "    epoch          : 339\n",
      "    loss           : 12304.538980917854\n",
      "    val_loss       : 12303.251238677734\n",
      "    val_log_likelihood: -12207.384705044198\n",
      "    val_log_marginal: -12215.18011811996\n",
      "Train Epoch: 340 [256/118836 (0%)] Loss: 12285.350586\n",
      "Train Epoch: 340 [33024/118836 (28%)] Loss: 12281.748047\n",
      "Train Epoch: 340 [65792/118836 (55%)] Loss: 12281.599609\n",
      "Train Epoch: 340 [98560/118836 (83%)] Loss: 12406.284180\n",
      "    epoch          : 340\n",
      "    loss           : 12305.545741896713\n",
      "    val_loss       : 12305.539126786165\n",
      "    val_log_likelihood: -12210.430338380118\n",
      "    val_log_marginal: -12218.330790843333\n",
      "Train Epoch: 341 [256/118836 (0%)] Loss: 12305.344727\n",
      "Train Epoch: 341 [33024/118836 (28%)] Loss: 12310.236328\n",
      "Train Epoch: 341 [65792/118836 (55%)] Loss: 12317.516602\n",
      "Train Epoch: 341 [98560/118836 (83%)] Loss: 12329.281250\n",
      "    epoch          : 341\n",
      "    loss           : 12305.076942139682\n",
      "    val_loss       : 12305.503044871051\n",
      "    val_log_likelihood: -12207.152907232217\n",
      "    val_log_marginal: -12214.703232801248\n",
      "Train Epoch: 342 [256/118836 (0%)] Loss: 12275.248047\n",
      "Train Epoch: 342 [33024/118836 (28%)] Loss: 12327.536133\n",
      "Train Epoch: 342 [65792/118836 (55%)] Loss: 12425.994141\n",
      "Train Epoch: 342 [98560/118836 (83%)] Loss: 12247.922852\n",
      "    epoch          : 342\n",
      "    loss           : 12305.64903959238\n",
      "    val_loss       : 12302.661649983058\n",
      "    val_log_likelihood: -12205.86634437681\n",
      "    val_log_marginal: -12213.670607027645\n",
      "Train Epoch: 343 [256/118836 (0%)] Loss: 12295.535156\n",
      "Train Epoch: 343 [33024/118836 (28%)] Loss: 12326.861328\n",
      "Train Epoch: 343 [65792/118836 (55%)] Loss: 12377.181641\n",
      "Train Epoch: 343 [98560/118836 (83%)] Loss: 12353.464844\n",
      "    epoch          : 343\n",
      "    loss           : 12303.899891277657\n",
      "    val_loss       : 12303.822981338817\n",
      "    val_log_likelihood: -12207.39601039082\n",
      "    val_log_marginal: -12215.08262096442\n",
      "Train Epoch: 344 [256/118836 (0%)] Loss: 12268.177734\n",
      "Train Epoch: 344 [33024/118836 (28%)] Loss: 12321.296875\n",
      "Train Epoch: 344 [65792/118836 (55%)] Loss: 12243.844727\n",
      "Train Epoch: 344 [98560/118836 (83%)] Loss: 12337.752930\n",
      "    epoch          : 344\n",
      "    loss           : 12305.862468659534\n",
      "    val_loss       : 12304.817150456318\n",
      "    val_log_likelihood: -12207.97565039547\n",
      "    val_log_marginal: -12215.753643305676\n",
      "Train Epoch: 345 [256/118836 (0%)] Loss: 12319.464844\n",
      "Train Epoch: 345 [33024/118836 (28%)] Loss: 12331.868164\n",
      "Train Epoch: 345 [65792/118836 (55%)] Loss: 12457.208984\n",
      "Train Epoch: 345 [98560/118836 (83%)] Loss: 12287.205078\n",
      "    epoch          : 345\n",
      "    loss           : 12304.287788687707\n",
      "    val_loss       : 12305.26738612393\n",
      "    val_log_likelihood: -12208.777146175817\n",
      "    val_log_marginal: -12216.390535991999\n",
      "Train Epoch: 346 [256/118836 (0%)] Loss: 12367.157227\n",
      "Train Epoch: 346 [33024/118836 (28%)] Loss: 12326.566406\n",
      "Train Epoch: 346 [65792/118836 (55%)] Loss: 12268.519531\n",
      "Train Epoch: 346 [98560/118836 (83%)] Loss: 12390.602539\n",
      "    epoch          : 346\n",
      "    loss           : 12306.324056393454\n",
      "    val_loss       : 12303.857476971583\n",
      "    val_log_likelihood: -12206.58656931736\n",
      "    val_log_marginal: -12214.189959551908\n",
      "Train Epoch: 347 [256/118836 (0%)] Loss: 12246.892578\n",
      "Train Epoch: 347 [33024/118836 (28%)] Loss: 12370.837891\n",
      "Train Epoch: 347 [65792/118836 (55%)] Loss: 12232.694336\n",
      "Train Epoch: 347 [98560/118836 (83%)] Loss: 12376.762695\n",
      "    epoch          : 347\n",
      "    loss           : 12305.441277657155\n",
      "    val_loss       : 12304.378786902738\n",
      "    val_log_likelihood: -12207.965627423231\n",
      "    val_log_marginal: -12215.603609828595\n",
      "Train Epoch: 348 [256/118836 (0%)] Loss: 12411.236328\n",
      "Train Epoch: 348 [33024/118836 (28%)] Loss: 12440.912109\n",
      "Train Epoch: 348 [65792/118836 (55%)] Loss: 12386.760742\n",
      "Train Epoch: 348 [98560/118836 (83%)] Loss: 12257.175781\n",
      "    epoch          : 348\n",
      "    loss           : 12308.233447548335\n",
      "    val_loss       : 12306.339690763883\n",
      "    val_log_likelihood: -12205.875978339538\n",
      "    val_log_marginal: -12213.641487640947\n",
      "Train Epoch: 349 [256/118836 (0%)] Loss: 12359.058594\n",
      "Train Epoch: 349 [33024/118836 (28%)] Loss: 12375.746094\n",
      "Train Epoch: 349 [65792/118836 (55%)] Loss: 12293.694336\n",
      "Train Epoch: 349 [98560/118836 (83%)] Loss: 12251.769531\n",
      "    epoch          : 349\n",
      "    loss           : 12303.1228181219\n",
      "    val_loss       : 12310.081385611042\n",
      "    val_log_likelihood: -12210.191261663822\n",
      "    val_log_marginal: -12217.850040031999\n",
      "Train Epoch: 350 [256/118836 (0%)] Loss: 12464.155273\n",
      "Train Epoch: 350 [33024/118836 (28%)] Loss: 12297.544922\n",
      "Train Epoch: 350 [65792/118836 (55%)] Loss: 12310.629883\n",
      "Train Epoch: 350 [98560/118836 (83%)] Loss: 12315.495117\n",
      "    epoch          : 350\n",
      "    loss           : 12310.606582144334\n",
      "    val_loss       : 12310.629242493387\n",
      "    val_log_likelihood: -12210.760108431556\n",
      "    val_log_marginal: -12218.66069364592\n",
      "Train Epoch: 351 [256/118836 (0%)] Loss: 12340.726562\n",
      "Train Epoch: 351 [33024/118836 (28%)] Loss: 12248.081055\n",
      "Train Epoch: 351 [65792/118836 (55%)] Loss: 12382.033203\n",
      "Train Epoch: 351 [98560/118836 (83%)] Loss: 12375.693359\n",
      "    epoch          : 351\n",
      "    loss           : 12303.01204330154\n",
      "    val_loss       : 12306.334944111062\n",
      "    val_log_likelihood: -12205.895247395832\n",
      "    val_log_marginal: -12213.553806321777\n",
      "Train Epoch: 352 [256/118836 (0%)] Loss: 12401.202148\n",
      "Train Epoch: 352 [33024/118836 (28%)] Loss: 12345.399414\n",
      "Train Epoch: 352 [65792/118836 (55%)] Loss: 12361.806641\n",
      "Train Epoch: 352 [98560/118836 (83%)] Loss: 12347.772461\n",
      "    epoch          : 352\n",
      "    loss           : 12306.451244248863\n",
      "    val_loss       : 12310.64323797256\n",
      "    val_log_likelihood: -12207.309375969293\n",
      "    val_log_marginal: -12215.112210762543\n",
      "Train Epoch: 353 [256/118836 (0%)] Loss: 12379.934570\n",
      "Train Epoch: 353 [33024/118836 (28%)] Loss: 12312.533203\n",
      "Train Epoch: 353 [65792/118836 (55%)] Loss: 12384.212891\n",
      "Train Epoch: 353 [98560/118836 (83%)] Loss: 12255.675781\n",
      "    epoch          : 353\n",
      "    loss           : 12304.336606473584\n",
      "    val_loss       : 12303.494900350117\n",
      "    val_log_likelihood: -12206.633975328266\n",
      "    val_log_marginal: -12214.416842061735\n",
      "Train Epoch: 354 [256/118836 (0%)] Loss: 12442.685547\n",
      "Train Epoch: 354 [33024/118836 (28%)] Loss: 12289.447266\n",
      "Train Epoch: 354 [65792/118836 (55%)] Loss: 12390.789062\n",
      "Train Epoch: 354 [98560/118836 (83%)] Loss: 12268.616211\n",
      "    epoch          : 354\n",
      "    loss           : 12301.291109807951\n",
      "    val_loss       : 12306.570668325623\n",
      "    val_log_likelihood: -12211.577008859336\n",
      "    val_log_marginal: -12219.267634840126\n",
      "Train Epoch: 355 [256/118836 (0%)] Loss: 12393.300781\n",
      "Train Epoch: 355 [33024/118836 (28%)] Loss: 12282.239258\n",
      "Train Epoch: 355 [65792/118836 (55%)] Loss: 12343.128906\n",
      "Train Epoch: 355 [98560/118836 (83%)] Loss: 12391.116211\n",
      "    epoch          : 355\n",
      "    loss           : 12302.028991224668\n",
      "    val_loss       : 12306.201115901755\n",
      "    val_log_likelihood: -12206.756617200424\n",
      "    val_log_marginal: -12214.47376016145\n",
      "Train Epoch: 356 [256/118836 (0%)] Loss: 12266.884766\n",
      "Train Epoch: 356 [33024/118836 (28%)] Loss: 12421.071289\n",
      "Train Epoch: 356 [65792/118836 (55%)] Loss: 12353.972656\n",
      "Train Epoch: 356 [98560/118836 (83%)] Loss: 12412.048828\n",
      "    epoch          : 356\n",
      "    loss           : 12303.856705890716\n",
      "    val_loss       : 12306.045565811695\n",
      "    val_log_likelihood: -12206.069525434243\n",
      "    val_log_marginal: -12213.957290333767\n",
      "Train Epoch: 357 [256/118836 (0%)] Loss: 12378.367188\n",
      "Train Epoch: 357 [33024/118836 (28%)] Loss: 12359.745117\n",
      "Train Epoch: 357 [65792/118836 (55%)] Loss: 12287.304688\n",
      "Train Epoch: 357 [98560/118836 (83%)] Loss: 12415.555664\n",
      "    epoch          : 357\n",
      "    loss           : 12317.009755770523\n",
      "    val_loss       : 12310.339112526226\n",
      "    val_log_likelihood: -12208.796594228184\n",
      "    val_log_marginal: -12217.035350943455\n",
      "Train Epoch: 358 [256/118836 (0%)] Loss: 12357.363281\n",
      "Train Epoch: 358 [33024/118836 (28%)] Loss: 12291.212891\n",
      "Train Epoch: 358 [65792/118836 (55%)] Loss: 12270.939453\n",
      "Train Epoch: 358 [98560/118836 (83%)] Loss: 12418.630859\n",
      "    epoch          : 358\n",
      "    loss           : 12302.376817747105\n",
      "    val_loss       : 12306.761727942892\n",
      "    val_log_likelihood: -12206.731062926488\n",
      "    val_log_marginal: -12214.396079263654\n",
      "Train Epoch: 359 [256/118836 (0%)] Loss: 12302.710938\n",
      "Train Epoch: 359 [33024/118836 (28%)] Loss: 12340.275391\n",
      "Train Epoch: 359 [65792/118836 (55%)] Loss: 12343.541992\n",
      "Train Epoch: 359 [98560/118836 (83%)] Loss: 12172.919922\n",
      "    epoch          : 359\n",
      "    loss           : 12307.155328364091\n",
      "    val_loss       : 12306.886132066047\n",
      "    val_log_likelihood: -12203.729569407828\n",
      "    val_log_marginal: -12211.379705373105\n",
      "Train Epoch: 360 [256/118836 (0%)] Loss: 12293.398438\n",
      "Train Epoch: 360 [33024/118836 (28%)] Loss: 12417.845703\n",
      "Train Epoch: 360 [65792/118836 (55%)] Loss: 12264.323242\n",
      "Train Epoch: 360 [98560/118836 (83%)] Loss: 12343.680664\n",
      "    epoch          : 360\n",
      "    loss           : 12304.68177810303\n",
      "    val_loss       : 12307.156346632006\n",
      "    val_log_likelihood: -12205.944181335297\n",
      "    val_log_marginal: -12213.643739820594\n",
      "Train Epoch: 361 [256/118836 (0%)] Loss: 12316.752930\n",
      "Train Epoch: 361 [33024/118836 (28%)] Loss: 12306.800781\n",
      "Train Epoch: 361 [65792/118836 (55%)] Loss: 12318.702148\n",
      "Train Epoch: 361 [98560/118836 (83%)] Loss: 12331.084961\n",
      "    epoch          : 361\n",
      "    loss           : 12306.902193832713\n",
      "    val_loss       : 12303.403224286878\n",
      "    val_log_likelihood: -12208.317212701613\n",
      "    val_log_marginal: -12215.890504265166\n",
      "Train Epoch: 362 [256/118836 (0%)] Loss: 12343.507812\n",
      "Train Epoch: 362 [33024/118836 (28%)] Loss: 12316.431641\n",
      "Train Epoch: 362 [65792/118836 (55%)] Loss: 12200.590820\n",
      "Train Epoch: 362 [98560/118836 (83%)] Loss: 12451.355469\n",
      "    epoch          : 362\n",
      "    loss           : 12305.18992436285\n",
      "    val_loss       : 12301.502011401517\n",
      "    val_log_likelihood: -12208.889221948666\n",
      "    val_log_marginal: -12216.567758332436\n",
      "Train Epoch: 363 [256/118836 (0%)] Loss: 12297.811523\n",
      "Train Epoch: 363 [33024/118836 (28%)] Loss: 12356.226562\n",
      "Train Epoch: 363 [65792/118836 (55%)] Loss: 12433.833008\n",
      "Train Epoch: 363 [98560/118836 (83%)] Loss: 12408.619141\n",
      "    epoch          : 363\n",
      "    loss           : 12309.824320525744\n",
      "    val_loss       : 12301.49915518074\n",
      "    val_log_likelihood: -12209.6596510869\n",
      "    val_log_marginal: -12217.27994951136\n",
      "Train Epoch: 364 [256/118836 (0%)] Loss: 12393.539062\n",
      "Train Epoch: 364 [33024/118836 (28%)] Loss: 12501.291016\n",
      "Train Epoch: 364 [65792/118836 (55%)] Loss: 12241.403320\n",
      "Train Epoch: 364 [98560/118836 (83%)] Loss: 12372.801758\n",
      "    epoch          : 364\n",
      "    loss           : 12305.982522196806\n",
      "    val_loss       : 12304.163619432855\n",
      "    val_log_likelihood: -12208.01755389268\n",
      "    val_log_marginal: -12215.74097449245\n",
      "Train Epoch: 365 [256/118836 (0%)] Loss: 12405.490234\n",
      "Train Epoch: 365 [33024/118836 (28%)] Loss: 12363.675781\n",
      "Train Epoch: 365 [65792/118836 (55%)] Loss: 12397.588867\n",
      "Train Epoch: 365 [98560/118836 (83%)] Loss: 12334.097656\n",
      "    epoch          : 365\n",
      "    loss           : 12307.115907387304\n",
      "    val_loss       : 12310.461747214495\n",
      "    val_log_likelihood: -12208.99757822193\n",
      "    val_log_marginal: -12216.625446396678\n",
      "Train Epoch: 366 [256/118836 (0%)] Loss: 12371.091797\n",
      "Train Epoch: 366 [33024/118836 (28%)] Loss: 12341.523438\n",
      "Train Epoch: 366 [65792/118836 (55%)] Loss: 12295.602539\n",
      "Train Epoch: 366 [98560/118836 (83%)] Loss: 12311.095703\n",
      "    epoch          : 366\n",
      "    loss           : 12305.868220927678\n",
      "    val_loss       : 12307.88507984433\n",
      "    val_log_likelihood: -12205.88669968724\n",
      "    val_log_marginal: -12213.391534913852\n",
      "Train Epoch: 367 [256/118836 (0%)] Loss: 12357.798828\n",
      "Train Epoch: 367 [33024/118836 (28%)] Loss: 12348.690430\n",
      "Train Epoch: 367 [65792/118836 (55%)] Loss: 12329.743164\n",
      "Train Epoch: 367 [98560/118836 (83%)] Loss: 12386.662109\n",
      "    epoch          : 367\n",
      "    loss           : 12302.85345375181\n",
      "    val_loss       : 12307.670217608247\n",
      "    val_log_likelihood: -12206.529278458436\n",
      "    val_log_marginal: -12214.216093977768\n",
      "Train Epoch: 368 [256/118836 (0%)] Loss: 12247.002930\n",
      "Train Epoch: 368 [33024/118836 (28%)] Loss: 12286.559570\n",
      "Train Epoch: 368 [65792/118836 (55%)] Loss: 12473.819336\n",
      "Train Epoch: 368 [98560/118836 (83%)] Loss: 12247.388672\n",
      "    epoch          : 368\n",
      "    loss           : 12307.957748526675\n",
      "    val_loss       : 12306.095745177952\n",
      "    val_log_likelihood: -12207.147650434243\n",
      "    val_log_marginal: -12214.88246842631\n",
      "Train Epoch: 369 [256/118836 (0%)] Loss: 12272.476562\n",
      "Train Epoch: 369 [33024/118836 (28%)] Loss: 12400.991211\n",
      "Train Epoch: 369 [65792/118836 (55%)] Loss: 12274.490234\n",
      "Train Epoch: 369 [98560/118836 (83%)] Loss: 12207.146484\n",
      "    epoch          : 369\n",
      "    loss           : 12303.1736665762\n",
      "    val_loss       : 12302.95511496071\n",
      "    val_log_likelihood: -12205.695916207867\n",
      "    val_log_marginal: -12213.508963128994\n",
      "Train Epoch: 370 [256/118836 (0%)] Loss: 12224.956055\n",
      "Train Epoch: 370 [33024/118836 (28%)] Loss: 12303.059570\n",
      "Train Epoch: 370 [65792/118836 (55%)] Loss: 12259.286133\n",
      "Train Epoch: 370 [98560/118836 (83%)] Loss: 12298.298828\n",
      "    epoch          : 370\n",
      "    loss           : 12303.619786174006\n",
      "    val_loss       : 12311.913913636196\n",
      "    val_log_likelihood: -12205.700423904054\n",
      "    val_log_marginal: -12213.377224751179\n",
      "Train Epoch: 371 [256/118836 (0%)] Loss: 12337.487305\n",
      "Train Epoch: 371 [33024/118836 (28%)] Loss: 12341.111328\n",
      "Train Epoch: 371 [65792/118836 (55%)] Loss: 12315.263672\n",
      "Train Epoch: 371 [98560/118836 (83%)] Loss: 12408.335938\n",
      "    epoch          : 371\n",
      "    loss           : 12308.379871827181\n",
      "    val_loss       : 12303.232141239736\n",
      "    val_log_likelihood: -12206.787875116315\n",
      "    val_log_marginal: -12214.412316281592\n",
      "Train Epoch: 372 [256/118836 (0%)] Loss: 12338.160156\n",
      "Train Epoch: 372 [33024/118836 (28%)] Loss: 12280.063477\n",
      "Train Epoch: 372 [65792/118836 (55%)] Loss: 12396.356445\n",
      "Train Epoch: 372 [98560/118836 (83%)] Loss: 12253.117188\n",
      "    epoch          : 372\n",
      "    loss           : 12306.243981014784\n",
      "    val_loss       : 12298.914804040076\n",
      "    val_log_likelihood: -12205.037473344448\n",
      "    val_log_marginal: -12212.70158130521\n",
      "Train Epoch: 373 [256/118836 (0%)] Loss: 12207.399414\n",
      "Train Epoch: 373 [33024/118836 (28%)] Loss: 12287.667969\n",
      "Train Epoch: 373 [65792/118836 (55%)] Loss: 12242.484375\n",
      "Train Epoch: 373 [98560/118836 (83%)] Loss: 12306.816406\n",
      "    epoch          : 373\n",
      "    loss           : 12302.988129071031\n",
      "    val_loss       : 12305.395527491904\n",
      "    val_log_likelihood: -12206.69585126525\n",
      "    val_log_marginal: -12214.439103773471\n",
      "Train Epoch: 374 [256/118836 (0%)] Loss: 12383.577148\n",
      "Train Epoch: 374 [33024/118836 (28%)] Loss: 12331.115234\n",
      "Train Epoch: 374 [65792/118836 (55%)] Loss: 12274.614258\n",
      "Train Epoch: 374 [98560/118836 (83%)] Loss: 12359.606445\n",
      "    epoch          : 374\n",
      "    loss           : 12303.336186608252\n",
      "    val_loss       : 12306.015260908163\n",
      "    val_log_likelihood: -12208.350830845482\n",
      "    val_log_marginal: -12215.978257985977\n",
      "Train Epoch: 375 [256/118836 (0%)] Loss: 12285.037109\n",
      "Train Epoch: 375 [33024/118836 (28%)] Loss: 12257.501953\n",
      "Train Epoch: 375 [65792/118836 (55%)] Loss: 12291.189453\n",
      "Train Epoch: 375 [98560/118836 (83%)] Loss: 12360.572266\n",
      "    epoch          : 375\n",
      "    loss           : 12303.997246077595\n",
      "    val_loss       : 12304.390968935193\n",
      "    val_log_likelihood: -12207.267225948615\n",
      "    val_log_marginal: -12214.934058786803\n",
      "Train Epoch: 376 [256/118836 (0%)] Loss: 12294.245117\n",
      "Train Epoch: 376 [33024/118836 (28%)] Loss: 12320.614258\n",
      "Train Epoch: 376 [65792/118836 (55%)] Loss: 12331.804688\n",
      "Train Epoch: 376 [98560/118836 (83%)] Loss: 12346.824219\n",
      "    epoch          : 376\n",
      "    loss           : 12304.716470384874\n",
      "    val_loss       : 12302.760054171918\n",
      "    val_log_likelihood: -12210.487424233612\n",
      "    val_log_marginal: -12218.184469363712\n",
      "Train Epoch: 377 [256/118836 (0%)] Loss: 12331.904297\n",
      "Train Epoch: 377 [33024/118836 (28%)] Loss: 12309.511719\n",
      "Train Epoch: 377 [65792/118836 (55%)] Loss: 12430.830078\n",
      "Train Epoch: 377 [98560/118836 (83%)] Loss: 12305.948242\n",
      "    epoch          : 377\n",
      "    loss           : 12307.378120638181\n",
      "    val_loss       : 12306.839225269136\n",
      "    val_log_likelihood: -12209.167178614816\n",
      "    val_log_marginal: -12217.046669916901\n",
      "Train Epoch: 378 [256/118836 (0%)] Loss: 12286.551758\n",
      "Train Epoch: 378 [33024/118836 (28%)] Loss: 12347.501953\n",
      "Train Epoch: 378 [65792/118836 (55%)] Loss: 12304.598633\n",
      "Train Epoch: 378 [98560/118836 (83%)] Loss: 12314.500000\n",
      "    epoch          : 378\n",
      "    loss           : 12305.42761046707\n",
      "    val_loss       : 12306.762823317682\n",
      "    val_log_likelihood: -12205.878267324493\n",
      "    val_log_marginal: -12213.525602060368\n",
      "Train Epoch: 379 [256/118836 (0%)] Loss: 12344.995117\n",
      "Train Epoch: 379 [33024/118836 (28%)] Loss: 12364.371094\n",
      "Train Epoch: 379 [65792/118836 (55%)] Loss: 12397.296875\n",
      "Train Epoch: 379 [98560/118836 (83%)] Loss: 12278.642578\n",
      "    epoch          : 379\n",
      "    loss           : 12306.568811226995\n",
      "    val_loss       : 12310.58017600758\n",
      "    val_log_likelihood: -12206.864495450785\n",
      "    val_log_marginal: -12214.490978418786\n",
      "Train Epoch: 380 [256/118836 (0%)] Loss: 12365.903320\n",
      "Train Epoch: 380 [33024/118836 (28%)] Loss: 12402.128906\n",
      "Train Epoch: 380 [65792/118836 (55%)] Loss: 12345.777344\n",
      "Train Epoch: 380 [98560/118836 (83%)] Loss: 12353.259766\n",
      "    epoch          : 380\n",
      "    loss           : 12310.236183861922\n",
      "    val_loss       : 12304.158123656427\n",
      "    val_log_likelihood: -12208.000815821442\n",
      "    val_log_marginal: -12215.626756633214\n",
      "Train Epoch: 381 [256/118836 (0%)] Loss: 12321.108398\n",
      "Train Epoch: 381 [33024/118836 (28%)] Loss: 12322.368164\n",
      "Train Epoch: 381 [65792/118836 (55%)] Loss: 12390.228516\n",
      "Train Epoch: 381 [98560/118836 (83%)] Loss: 12287.613281\n",
      "    epoch          : 381\n",
      "    loss           : 12307.73431296526\n",
      "    val_loss       : 12305.024731269883\n",
      "    val_log_likelihood: -12205.760018448873\n",
      "    val_log_marginal: -12213.328701522305\n",
      "Train Epoch: 382 [256/118836 (0%)] Loss: 12234.607422\n",
      "Train Epoch: 382 [33024/118836 (28%)] Loss: 12297.389648\n",
      "Train Epoch: 382 [65792/118836 (55%)] Loss: 12275.926758\n",
      "Train Epoch: 382 [98560/118836 (83%)] Loss: 12353.644531\n",
      "    epoch          : 382\n",
      "    loss           : 12307.041186866729\n",
      "    val_loss       : 12305.240710067112\n",
      "    val_log_likelihood: -12204.166745664032\n",
      "    val_log_marginal: -12211.90269881354\n",
      "Train Epoch: 383 [256/118836 (0%)] Loss: 12300.404297\n",
      "Train Epoch: 383 [33024/118836 (28%)] Loss: 12379.044922\n",
      "Train Epoch: 383 [65792/118836 (55%)] Loss: 12370.035156\n",
      "Train Epoch: 383 [98560/118836 (83%)] Loss: 12419.091797\n",
      "    epoch          : 383\n",
      "    loss           : 12304.007182944324\n",
      "    val_loss       : 12301.467082189969\n",
      "    val_log_likelihood: -12201.50748665607\n",
      "    val_log_marginal: -12209.212132417648\n",
      "Train Epoch: 384 [256/118836 (0%)] Loss: 12325.765625\n",
      "Train Epoch: 384 [33024/118836 (28%)] Loss: 12349.030273\n",
      "Train Epoch: 384 [65792/118836 (55%)] Loss: 12306.125000\n",
      "Train Epoch: 384 [98560/118836 (83%)] Loss: 12430.298828\n",
      "    epoch          : 384\n",
      "    loss           : 12301.063701923078\n",
      "    val_loss       : 12306.716575484179\n",
      "    val_log_likelihood: -12205.773012142008\n",
      "    val_log_marginal: -12213.838904838165\n",
      "Train Epoch: 385 [256/118836 (0%)] Loss: 12303.105469\n",
      "Train Epoch: 385 [33024/118836 (28%)] Loss: 12313.250000\n",
      "Train Epoch: 385 [65792/118836 (55%)] Loss: 12247.093750\n",
      "Train Epoch: 385 [98560/118836 (83%)] Loss: 12370.966797\n",
      "    epoch          : 385\n",
      "    loss           : 12302.03728029363\n",
      "    val_loss       : 12308.222307801185\n",
      "    val_log_likelihood: -12204.61698411006\n",
      "    val_log_marginal: -12212.618401970782\n",
      "Train Epoch: 386 [256/118836 (0%)] Loss: 12399.496094\n",
      "Train Epoch: 386 [33024/118836 (28%)] Loss: 12258.595703\n",
      "Train Epoch: 386 [65792/118836 (55%)] Loss: 12310.084961\n",
      "Train Epoch: 386 [98560/118836 (83%)] Loss: 12420.998047\n",
      "    epoch          : 386\n",
      "    loss           : 12310.640313210815\n",
      "    val_loss       : 12309.152022219805\n",
      "    val_log_likelihood: -12205.447289049574\n",
      "    val_log_marginal: -12213.512030202557\n",
      "Train Epoch: 387 [256/118836 (0%)] Loss: 12215.479492\n",
      "Train Epoch: 387 [33024/118836 (28%)] Loss: 12388.208984\n",
      "Train Epoch: 387 [65792/118836 (55%)] Loss: 12250.871094\n",
      "Train Epoch: 387 [98560/118836 (83%)] Loss: 12316.945312\n",
      "    epoch          : 387\n",
      "    loss           : 12305.760644450475\n",
      "    val_loss       : 12305.479075752048\n",
      "    val_log_likelihood: -12202.825244100237\n",
      "    val_log_marginal: -12210.742198816119\n",
      "Train Epoch: 388 [256/118836 (0%)] Loss: 12353.696289\n",
      "Train Epoch: 388 [33024/118836 (28%)] Loss: 12347.849609\n",
      "Train Epoch: 388 [65792/118836 (55%)] Loss: 12402.560547\n",
      "Train Epoch: 388 [98560/118836 (83%)] Loss: 12410.079102\n",
      "    epoch          : 388\n",
      "    loss           : 12306.279735318445\n",
      "    val_loss       : 12304.604067474782\n",
      "    val_log_likelihood: -12203.486268028846\n",
      "    val_log_marginal: -12211.330614438453\n",
      "Train Epoch: 389 [256/118836 (0%)] Loss: 12315.974609\n",
      "Train Epoch: 389 [33024/118836 (28%)] Loss: 12294.580078\n",
      "Train Epoch: 389 [65792/118836 (55%)] Loss: 12372.999023\n",
      "Train Epoch: 389 [98560/118836 (83%)] Loss: 12283.017578\n",
      "    epoch          : 389\n",
      "    loss           : 12307.032786329095\n",
      "    val_loss       : 12300.96144087127\n",
      "    val_log_likelihood: -12198.406640140354\n",
      "    val_log_marginal: -12206.442362742966\n",
      "Train Epoch: 390 [256/118836 (0%)] Loss: 12334.928711\n",
      "Train Epoch: 390 [33024/118836 (28%)] Loss: 12327.623047\n",
      "Train Epoch: 390 [65792/118836 (55%)] Loss: 12288.537109\n",
      "Train Epoch: 390 [98560/118836 (83%)] Loss: 12297.715820\n",
      "    epoch          : 390\n",
      "    loss           : 12301.17438562991\n",
      "    val_loss       : 12302.636234432297\n",
      "    val_log_likelihood: -12200.480053084937\n",
      "    val_log_marginal: -12208.222794210913\n",
      "Train Epoch: 391 [256/118836 (0%)] Loss: 12423.810547\n",
      "Train Epoch: 391 [33024/118836 (28%)] Loss: 12413.722656\n",
      "Train Epoch: 391 [65792/118836 (55%)] Loss: 12319.679688\n",
      "Train Epoch: 391 [98560/118836 (83%)] Loss: 12453.886719\n",
      "    epoch          : 391\n",
      "    loss           : 12301.32808703603\n",
      "    val_loss       : 12302.74986579844\n",
      "    val_log_likelihood: -12198.276005156638\n",
      "    val_log_marginal: -12206.091696900525\n",
      "Train Epoch: 392 [256/118836 (0%)] Loss: 12469.925781\n",
      "Train Epoch: 392 [33024/118836 (28%)] Loss: 12437.814453\n",
      "Train Epoch: 392 [65792/118836 (55%)] Loss: 12289.247070\n",
      "Train Epoch: 392 [98560/118836 (83%)] Loss: 12292.604492\n",
      "    epoch          : 392\n",
      "    loss           : 12306.364267828525\n",
      "    val_loss       : 12303.097392282294\n",
      "    val_log_likelihood: -12199.06310209238\n",
      "    val_log_marginal: -12207.140676416304\n",
      "Train Epoch: 393 [256/118836 (0%)] Loss: 12403.363281\n",
      "Train Epoch: 393 [33024/118836 (28%)] Loss: 12297.048828\n",
      "Train Epoch: 393 [65792/118836 (55%)] Loss: 12279.921875\n",
      "Train Epoch: 393 [98560/118836 (83%)] Loss: 12413.824219\n",
      "    epoch          : 393\n",
      "    loss           : 12306.913276080439\n",
      "    val_loss       : 12306.881465889348\n",
      "    val_log_likelihood: -12199.212521001344\n",
      "    val_log_marginal: -12207.026893790764\n",
      "Train Epoch: 394 [256/118836 (0%)] Loss: 12319.228516\n",
      "Train Epoch: 394 [33024/118836 (28%)] Loss: 12182.019531\n",
      "Train Epoch: 394 [65792/118836 (55%)] Loss: 12384.412109\n",
      "Train Epoch: 394 [98560/118836 (83%)] Loss: 12291.039062\n",
      "    epoch          : 394\n",
      "    loss           : 12305.802823711487\n",
      "    val_loss       : 12304.90527682789\n",
      "    val_log_likelihood: -12199.964256358562\n",
      "    val_log_marginal: -12207.904312552302\n",
      "Train Epoch: 395 [256/118836 (0%)] Loss: 12329.072266\n",
      "Train Epoch: 395 [33024/118836 (28%)] Loss: 12281.690430\n",
      "Train Epoch: 395 [65792/118836 (55%)] Loss: 12344.363281\n",
      "Train Epoch: 395 [98560/118836 (83%)] Loss: 12260.057617\n",
      "    epoch          : 395\n",
      "    loss           : 12304.411492258581\n",
      "    val_loss       : 12302.330251641775\n",
      "    val_log_likelihood: -12200.48655946288\n",
      "    val_log_marginal: -12208.37782130879\n",
      "Train Epoch: 396 [256/118836 (0%)] Loss: 12255.287109\n",
      "Train Epoch: 396 [33024/118836 (28%)] Loss: 12407.791016\n",
      "Train Epoch: 396 [65792/118836 (55%)] Loss: 12338.114258\n",
      "Train Epoch: 396 [98560/118836 (83%)] Loss: 12283.505859\n",
      "    epoch          : 396\n",
      "    loss           : 12309.73573685639\n",
      "    val_loss       : 12299.417674038805\n",
      "    val_log_likelihood: -12200.184859323304\n",
      "    val_log_marginal: -12208.029803135141\n",
      "Train Epoch: 397 [256/118836 (0%)] Loss: 12301.572266\n",
      "Train Epoch: 397 [33024/118836 (28%)] Loss: 12229.467773\n",
      "Train Epoch: 397 [65792/118836 (55%)] Loss: 12327.752930\n",
      "Train Epoch: 397 [98560/118836 (83%)] Loss: 12416.789062\n",
      "    epoch          : 397\n",
      "    loss           : 12300.3392309954\n",
      "    val_loss       : 12305.475552211668\n",
      "    val_log_likelihood: -12197.804371995193\n",
      "    val_log_marginal: -12205.69672016304\n",
      "Train Epoch: 398 [256/118836 (0%)] Loss: 12291.729492\n",
      "Train Epoch: 398 [33024/118836 (28%)] Loss: 12286.979492\n",
      "Train Epoch: 398 [65792/118836 (55%)] Loss: 12307.195312\n",
      "Train Epoch: 398 [98560/118836 (83%)] Loss: 12266.032227\n",
      "    epoch          : 398\n",
      "    loss           : 12302.601491580077\n",
      "    val_loss       : 12305.77861815319\n",
      "    val_log_likelihood: -12203.215897371278\n",
      "    val_log_marginal: -12211.071715079015\n",
      "Train Epoch: 399 [256/118836 (0%)] Loss: 12443.195312\n",
      "Train Epoch: 399 [33024/118836 (28%)] Loss: 12316.176758\n",
      "Train Epoch: 399 [65792/118836 (55%)] Loss: 12242.255859\n",
      "Train Epoch: 399 [98560/118836 (83%)] Loss: 12385.368164\n",
      "    epoch          : 399\n",
      "    loss           : 12297.406354683624\n",
      "    val_loss       : 12301.683254750977\n",
      "    val_log_likelihood: -12202.204690407878\n",
      "    val_log_marginal: -12210.071824176632\n",
      "Train Epoch: 400 [256/118836 (0%)] Loss: 12293.109375\n",
      "Train Epoch: 400 [33024/118836 (28%)] Loss: 12324.834961\n",
      "Train Epoch: 400 [65792/118836 (55%)] Loss: 12308.207031\n",
      "Train Epoch: 400 [98560/118836 (83%)] Loss: 12327.669922\n",
      "    epoch          : 400\n",
      "    loss           : 12305.068186679333\n",
      "    val_loss       : 12306.037818248218\n",
      "    val_log_likelihood: -12200.073475140869\n",
      "    val_log_marginal: -12207.94647720012\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [256/118836 (0%)] Loss: 12270.015625\n",
      "Train Epoch: 401 [33024/118836 (28%)] Loss: 12401.842773\n",
      "Train Epoch: 401 [65792/118836 (55%)] Loss: 12292.202148\n",
      "Train Epoch: 401 [98560/118836 (83%)] Loss: 12389.080078\n",
      "    epoch          : 401\n",
      "    loss           : 12304.215639054744\n",
      "    val_loss       : 12302.79192230556\n",
      "    val_log_likelihood: -12203.535723124742\n",
      "    val_log_marginal: -12211.444232374104\n",
      "Train Epoch: 402 [256/118836 (0%)] Loss: 12479.000977\n",
      "Train Epoch: 402 [33024/118836 (28%)] Loss: 12260.974609\n",
      "Train Epoch: 402 [65792/118836 (55%)] Loss: 12246.341797\n",
      "Train Epoch: 402 [98560/118836 (83%)] Loss: 12306.926758\n",
      "    epoch          : 402\n",
      "    loss           : 12305.941806891025\n",
      "    val_loss       : 12302.639790570161\n",
      "    val_log_likelihood: -12201.664123565448\n",
      "    val_log_marginal: -12209.287922604353\n",
      "Train Epoch: 403 [256/118836 (0%)] Loss: 12336.171875\n",
      "Train Epoch: 403 [33024/118836 (28%)] Loss: 12329.404297\n",
      "Train Epoch: 403 [65792/118836 (55%)] Loss: 12440.718750\n",
      "Train Epoch: 403 [98560/118836 (83%)] Loss: 12319.122070\n",
      "    epoch          : 403\n",
      "    loss           : 12308.461095817824\n",
      "    val_loss       : 12299.181919291663\n",
      "    val_log_likelihood: -12203.109182756927\n",
      "    val_log_marginal: -12210.933984778774\n",
      "Train Epoch: 404 [256/118836 (0%)] Loss: 12238.129883\n",
      "Train Epoch: 404 [33024/118836 (28%)] Loss: 12319.489258\n",
      "Train Epoch: 404 [65792/118836 (55%)] Loss: 12257.757812\n",
      "Train Epoch: 404 [98560/118836 (83%)] Loss: 12370.882812\n",
      "    epoch          : 404\n",
      "    loss           : 12302.922908427678\n",
      "    val_loss       : 12303.48737242957\n",
      "    val_log_likelihood: -12202.21821398108\n",
      "    val_log_marginal: -12210.113850560369\n",
      "Train Epoch: 405 [256/118836 (0%)] Loss: 12397.073242\n",
      "Train Epoch: 405 [33024/118836 (28%)] Loss: 12282.267578\n",
      "Train Epoch: 405 [65792/118836 (55%)] Loss: 12344.562500\n",
      "Train Epoch: 405 [98560/118836 (83%)] Loss: 12301.224609\n",
      "    epoch          : 405\n",
      "    loss           : 12298.442306076819\n",
      "    val_loss       : 12304.778778999114\n",
      "    val_log_likelihood: -12200.458160476117\n",
      "    val_log_marginal: -12208.330208593201\n",
      "Train Epoch: 406 [256/118836 (0%)] Loss: 12289.154297\n",
      "Train Epoch: 406 [33024/118836 (28%)] Loss: 12265.390625\n",
      "Train Epoch: 406 [65792/118836 (55%)] Loss: 12235.972656\n",
      "Train Epoch: 406 [98560/118836 (83%)] Loss: 12266.961914\n",
      "    epoch          : 406\n",
      "    loss           : 12305.644644172611\n",
      "    val_loss       : 12305.46128424113\n",
      "    val_log_likelihood: -12199.817160682898\n",
      "    val_log_marginal: -12207.859254287137\n",
      "Train Epoch: 407 [256/118836 (0%)] Loss: 12314.868164\n",
      "Train Epoch: 407 [33024/118836 (28%)] Loss: 12185.490234\n",
      "Train Epoch: 407 [65792/118836 (55%)] Loss: 12372.500000\n",
      "Train Epoch: 407 [98560/118836 (83%)] Loss: 12355.106445\n",
      "    epoch          : 407\n",
      "    loss           : 12306.75745014604\n",
      "    val_loss       : 12305.508468768194\n",
      "    val_log_likelihood: -12198.795535275796\n",
      "    val_log_marginal: -12206.538605016773\n",
      "Train Epoch: 408 [256/118836 (0%)] Loss: 12390.363281\n",
      "Train Epoch: 408 [33024/118836 (28%)] Loss: 12432.482422\n",
      "Train Epoch: 408 [65792/118836 (55%)] Loss: 12368.508789\n",
      "Train Epoch: 408 [98560/118836 (83%)] Loss: 12353.518555\n",
      "    epoch          : 408\n",
      "    loss           : 12303.03928349876\n",
      "    val_loss       : 12308.608898855493\n",
      "    val_log_likelihood: -12199.913682375673\n",
      "    val_log_marginal: -12207.766723818877\n",
      "Train Epoch: 409 [256/118836 (0%)] Loss: 12340.831055\n",
      "Train Epoch: 409 [33024/118836 (28%)] Loss: 12396.631836\n",
      "Train Epoch: 409 [65792/118836 (55%)] Loss: 12308.227539\n",
      "Train Epoch: 409 [98560/118836 (83%)] Loss: 12346.573242\n",
      "    epoch          : 409\n",
      "    loss           : 12303.244223014888\n",
      "    val_loss       : 12302.861461578368\n",
      "    val_log_likelihood: -12199.568766477978\n",
      "    val_log_marginal: -12207.259494004535\n",
      "Train Epoch: 410 [256/118836 (0%)] Loss: 12387.439453\n",
      "Train Epoch: 410 [33024/118836 (28%)] Loss: 12243.902344\n",
      "Train Epoch: 410 [65792/118836 (55%)] Loss: 12248.326172\n",
      "Train Epoch: 410 [98560/118836 (83%)] Loss: 12325.578125\n",
      "    epoch          : 410\n",
      "    loss           : 12302.76483438017\n",
      "    val_loss       : 12304.173809263091\n",
      "    val_log_likelihood: -12205.625603384771\n",
      "    val_log_marginal: -12213.609117763886\n",
      "Train Epoch: 411 [256/118836 (0%)] Loss: 12373.886719\n",
      "Train Epoch: 411 [33024/118836 (28%)] Loss: 12237.281250\n",
      "Train Epoch: 411 [65792/118836 (55%)] Loss: 12309.261719\n",
      "Train Epoch: 411 [98560/118836 (83%)] Loss: 12290.748047\n",
      "    epoch          : 411\n",
      "    loss           : 12303.168323188069\n",
      "    val_loss       : 12303.89932590488\n",
      "    val_log_likelihood: -12201.822362231183\n",
      "    val_log_marginal: -12209.716271780768\n",
      "Train Epoch: 412 [256/118836 (0%)] Loss: 12367.654297\n",
      "Train Epoch: 412 [33024/118836 (28%)] Loss: 12268.955078\n",
      "Train Epoch: 412 [65792/118836 (55%)] Loss: 12347.129883\n",
      "Train Epoch: 412 [98560/118836 (83%)] Loss: 12285.058594\n",
      "    epoch          : 412\n",
      "    loss           : 12302.894253870709\n",
      "    val_loss       : 12303.239162155623\n",
      "    val_log_likelihood: -12198.86116140664\n",
      "    val_log_marginal: -12206.70555605215\n",
      "Train Epoch: 413 [256/118836 (0%)] Loss: 12322.064453\n",
      "Train Epoch: 413 [33024/118836 (28%)] Loss: 12288.079102\n",
      "Train Epoch: 413 [65792/118836 (55%)] Loss: 12438.853516\n",
      "Train Epoch: 413 [98560/118836 (83%)] Loss: 12281.732422\n",
      "    epoch          : 413\n",
      "    loss           : 12300.720368395885\n",
      "    val_loss       : 12304.571290594626\n",
      "    val_log_likelihood: -12200.427363782052\n",
      "    val_log_marginal: -12208.237637274779\n",
      "Train Epoch: 414 [256/118836 (0%)] Loss: 12288.853516\n",
      "Train Epoch: 414 [33024/118836 (28%)] Loss: 12318.208984\n",
      "Train Epoch: 414 [65792/118836 (55%)] Loss: 12370.364258\n",
      "Train Epoch: 414 [98560/118836 (83%)] Loss: 12273.345703\n",
      "    epoch          : 414\n",
      "    loss           : 12303.114231964691\n",
      "    val_loss       : 12305.681531099248\n",
      "    val_log_likelihood: -12200.626427445202\n",
      "    val_log_marginal: -12208.62554859717\n",
      "Train Epoch: 415 [256/118836 (0%)] Loss: 12241.386719\n",
      "Train Epoch: 415 [33024/118836 (28%)] Loss: 12366.543945\n",
      "Train Epoch: 415 [65792/118836 (55%)] Loss: 12311.033203\n",
      "Train Epoch: 415 [98560/118836 (83%)] Loss: 12414.453125\n",
      "    epoch          : 415\n",
      "    loss           : 12304.990691396559\n",
      "    val_loss       : 12299.108544408502\n",
      "    val_log_likelihood: -12199.107461292908\n",
      "    val_log_marginal: -12206.941053608445\n",
      "Train Epoch: 416 [256/118836 (0%)] Loss: 12289.688477\n",
      "Train Epoch: 416 [33024/118836 (28%)] Loss: 12329.033203\n",
      "Train Epoch: 416 [65792/118836 (55%)] Loss: 12291.361328\n",
      "Train Epoch: 416 [98560/118836 (83%)] Loss: 12370.335938\n",
      "    epoch          : 416\n",
      "    loss           : 12304.560534758839\n",
      "    val_loss       : 12302.203372157797\n",
      "    val_log_likelihood: -12197.941952769592\n",
      "    val_log_marginal: -12205.793782085311\n",
      "Train Epoch: 417 [256/118836 (0%)] Loss: 12429.497070\n",
      "Train Epoch: 417 [33024/118836 (28%)] Loss: 12277.250977\n",
      "Train Epoch: 417 [65792/118836 (55%)] Loss: 12308.214844\n",
      "Train Epoch: 417 [98560/118836 (83%)] Loss: 12463.755859\n",
      "    epoch          : 417\n",
      "    loss           : 12301.296747053351\n",
      "    val_loss       : 12301.843600473534\n",
      "    val_log_likelihood: -12197.865165878307\n",
      "    val_log_marginal: -12205.587772249903\n",
      "Train Epoch: 418 [256/118836 (0%)] Loss: 12412.275391\n",
      "Train Epoch: 418 [33024/118836 (28%)] Loss: 12404.805664\n",
      "Train Epoch: 418 [65792/118836 (55%)] Loss: 12318.166992\n",
      "Train Epoch: 418 [98560/118836 (83%)] Loss: 12271.073242\n",
      "    epoch          : 418\n",
      "    loss           : 12302.518975360577\n",
      "    val_loss       : 12306.416382993635\n",
      "    val_log_likelihood: -12204.130730943703\n",
      "    val_log_marginal: -12212.259694530027\n",
      "Train Epoch: 419 [256/118836 (0%)] Loss: 12385.640625\n",
      "Train Epoch: 419 [33024/118836 (28%)] Loss: 12299.260742\n",
      "Train Epoch: 419 [65792/118836 (55%)] Loss: 12274.244141\n",
      "Train Epoch: 419 [98560/118836 (83%)] Loss: 12237.626953\n",
      "    epoch          : 419\n",
      "    loss           : 12303.523919400071\n",
      "    val_loss       : 12305.479105714157\n",
      "    val_log_likelihood: -12201.389982843517\n",
      "    val_log_marginal: -12209.270541867218\n",
      "Train Epoch: 420 [256/118836 (0%)] Loss: 12374.199219\n",
      "Train Epoch: 420 [33024/118836 (28%)] Loss: 12312.079102\n",
      "Train Epoch: 420 [65792/118836 (55%)] Loss: 12357.918945\n",
      "Train Epoch: 420 [98560/118836 (83%)] Loss: 12406.569336\n",
      "    epoch          : 420\n",
      "    loss           : 12306.08522749302\n",
      "    val_loss       : 12304.148991698426\n",
      "    val_log_likelihood: -12199.292506881979\n",
      "    val_log_marginal: -12207.358692883945\n",
      "Train Epoch: 421 [256/118836 (0%)] Loss: 12293.152344\n",
      "Train Epoch: 421 [33024/118836 (28%)] Loss: 12293.471680\n",
      "Train Epoch: 421 [65792/118836 (55%)] Loss: 12360.858398\n",
      "Train Epoch: 421 [98560/118836 (83%)] Loss: 12363.632812\n",
      "    epoch          : 421\n",
      "    loss           : 12302.73676883013\n",
      "    val_loss       : 12301.434916712522\n",
      "    val_log_likelihood: -12202.500073343155\n",
      "    val_log_marginal: -12210.324776475132\n",
      "Train Epoch: 422 [256/118836 (0%)] Loss: 12292.592773\n",
      "Train Epoch: 422 [33024/118836 (28%)] Loss: 12308.813477\n",
      "Train Epoch: 422 [65792/118836 (55%)] Loss: 12360.506836\n",
      "Train Epoch: 422 [98560/118836 (83%)] Loss: 12266.695312\n",
      "    epoch          : 422\n",
      "    loss           : 12305.595384550765\n",
      "    val_loss       : 12307.523485937445\n",
      "    val_log_likelihood: -12199.245130111403\n",
      "    val_log_marginal: -12207.108522976509\n",
      "Train Epoch: 423 [256/118836 (0%)] Loss: 12360.944336\n",
      "Train Epoch: 423 [33024/118836 (28%)] Loss: 12185.359375\n",
      "Train Epoch: 423 [65792/118836 (55%)] Loss: 12317.196289\n",
      "Train Epoch: 423 [98560/118836 (83%)] Loss: 12329.260742\n",
      "    epoch          : 423\n",
      "    loss           : 12302.036737974306\n",
      "    val_loss       : 12303.245880817396\n",
      "    val_log_likelihood: -12200.339088832456\n",
      "    val_log_marginal: -12208.225736984175\n",
      "Train Epoch: 424 [256/118836 (0%)] Loss: 12297.723633\n",
      "Train Epoch: 424 [33024/118836 (28%)] Loss: 12496.674805\n",
      "Train Epoch: 424 [65792/118836 (55%)] Loss: 12266.371094\n",
      "Train Epoch: 424 [98560/118836 (83%)] Loss: 12222.722656\n",
      "    epoch          : 424\n",
      "    loss           : 12303.394661296785\n",
      "    val_loss       : 12301.815949480739\n",
      "    val_log_likelihood: -12206.219540296734\n",
      "    val_log_marginal: -12213.931689461195\n",
      "Train Epoch: 425 [256/118836 (0%)] Loss: 12409.242188\n",
      "Train Epoch: 425 [33024/118836 (28%)] Loss: 12333.925781\n",
      "Train Epoch: 425 [65792/118836 (55%)] Loss: 12232.636719\n",
      "Train Epoch: 425 [98560/118836 (83%)] Loss: 12314.350586\n",
      "    epoch          : 425\n",
      "    loss           : 12302.23715654725\n",
      "    val_loss       : 12301.856560365242\n",
      "    val_log_likelihood: -12203.832835601479\n",
      "    val_log_marginal: -12211.767920914739\n",
      "Train Epoch: 426 [256/118836 (0%)] Loss: 12413.848633\n",
      "Train Epoch: 426 [33024/118836 (28%)] Loss: 12327.418945\n",
      "Train Epoch: 426 [65792/118836 (55%)] Loss: 12298.191406\n",
      "Train Epoch: 426 [98560/118836 (83%)] Loss: 12409.099609\n",
      "    epoch          : 426\n",
      "    loss           : 12303.397274348634\n",
      "    val_loss       : 12299.626071704173\n",
      "    val_log_likelihood: -12197.439530668424\n",
      "    val_log_marginal: -12205.04395719453\n",
      "Train Epoch: 427 [256/118836 (0%)] Loss: 12362.397461\n",
      "Train Epoch: 427 [33024/118836 (28%)] Loss: 12263.258789\n",
      "Train Epoch: 427 [65792/118836 (55%)] Loss: 12355.796875\n",
      "Train Epoch: 427 [98560/118836 (83%)] Loss: 12368.466797\n",
      "    epoch          : 427\n",
      "    loss           : 12300.62101830671\n",
      "    val_loss       : 12305.311501630462\n",
      "    val_log_likelihood: -12200.970528167649\n",
      "    val_log_marginal: -12208.72253501578\n",
      "Train Epoch: 428 [256/118836 (0%)] Loss: 12400.998047\n",
      "Train Epoch: 428 [33024/118836 (28%)] Loss: 12275.650391\n",
      "Train Epoch: 428 [65792/118836 (55%)] Loss: 12390.110352\n",
      "Train Epoch: 428 [98560/118836 (83%)] Loss: 12256.615234\n",
      "    epoch          : 428\n",
      "    loss           : 12304.475688682536\n",
      "    val_loss       : 12304.013310940743\n",
      "    val_log_likelihood: -12199.35512400486\n",
      "    val_log_marginal: -12207.43538041259\n",
      "Train Epoch: 429 [256/118836 (0%)] Loss: 12329.189453\n",
      "Train Epoch: 429 [33024/118836 (28%)] Loss: 12516.615234\n",
      "Train Epoch: 429 [65792/118836 (55%)] Loss: 12265.300781\n",
      "Train Epoch: 429 [98560/118836 (83%)] Loss: 12324.272461\n",
      "    epoch          : 429\n",
      "    loss           : 12304.819474223274\n",
      "    val_loss       : 12299.234052451206\n",
      "    val_log_likelihood: -12195.810866418527\n",
      "    val_log_marginal: -12203.539807766274\n",
      "Train Epoch: 430 [256/118836 (0%)] Loss: 12318.720703\n",
      "Train Epoch: 430 [33024/118836 (28%)] Loss: 12340.709961\n",
      "Train Epoch: 430 [65792/118836 (55%)] Loss: 12258.006836\n",
      "Train Epoch: 430 [98560/118836 (83%)] Loss: 12339.500000\n",
      "    epoch          : 430\n",
      "    loss           : 12298.306792642421\n",
      "    val_loss       : 12302.361174106827\n",
      "    val_log_likelihood: -12199.678940659893\n",
      "    val_log_marginal: -12207.43820739933\n",
      "Train Epoch: 431 [256/118836 (0%)] Loss: 12234.999023\n",
      "Train Epoch: 431 [33024/118836 (28%)] Loss: 12263.767578\n",
      "Train Epoch: 431 [65792/118836 (55%)] Loss: 12252.812500\n",
      "Train Epoch: 431 [98560/118836 (83%)] Loss: 12399.675781\n",
      "    epoch          : 431\n",
      "    loss           : 12299.36427154415\n",
      "    val_loss       : 12304.494966971173\n",
      "    val_log_likelihood: -12200.452113058313\n",
      "    val_log_marginal: -12208.730382800917\n",
      "Train Epoch: 432 [256/118836 (0%)] Loss: 12390.256836\n",
      "Train Epoch: 432 [33024/118836 (28%)] Loss: 12374.837891\n",
      "Train Epoch: 432 [65792/118836 (55%)] Loss: 12303.237305\n",
      "Train Epoch: 432 [98560/118836 (83%)] Loss: 12380.421875\n",
      "    epoch          : 432\n",
      "    loss           : 12304.101089485112\n",
      "    val_loss       : 12301.199317850276\n",
      "    val_log_likelihood: -12200.487921319273\n",
      "    val_log_marginal: -12208.325086714192\n",
      "Train Epoch: 433 [256/118836 (0%)] Loss: 12303.052734\n",
      "Train Epoch: 433 [33024/118836 (28%)] Loss: 12276.974609\n",
      "Train Epoch: 433 [65792/118836 (55%)] Loss: 12275.760742\n",
      "Train Epoch: 433 [98560/118836 (83%)] Loss: 12359.323242\n",
      "    epoch          : 433\n",
      "    loss           : 12302.942806231908\n",
      "    val_loss       : 12303.493093157776\n",
      "    val_log_likelihood: -12199.292414314516\n",
      "    val_log_marginal: -12207.013800353398\n",
      "Train Epoch: 434 [256/118836 (0%)] Loss: 12306.958984\n",
      "Train Epoch: 434 [33024/118836 (28%)] Loss: 12292.070312\n",
      "Train Epoch: 434 [65792/118836 (55%)] Loss: 12356.216797\n",
      "Train Epoch: 434 [98560/118836 (83%)] Loss: 12389.375000\n",
      "    epoch          : 434\n",
      "    loss           : 12301.110336376913\n",
      "    val_loss       : 12302.50906943399\n",
      "    val_log_likelihood: -12201.829428537272\n",
      "    val_log_marginal: -12209.66507736267\n",
      "Train Epoch: 435 [256/118836 (0%)] Loss: 12325.822266\n",
      "Train Epoch: 435 [33024/118836 (28%)] Loss: 12282.876953\n",
      "Train Epoch: 435 [65792/118836 (55%)] Loss: 12245.656250\n",
      "Train Epoch: 435 [98560/118836 (83%)] Loss: 12416.994141\n",
      "    epoch          : 435\n",
      "    loss           : 12300.180993460506\n",
      "    val_loss       : 12304.713690172526\n",
      "    val_log_likelihood: -12200.828247453992\n",
      "    val_log_marginal: -12208.616998029682\n",
      "Train Epoch: 436 [256/118836 (0%)] Loss: 12326.012695\n",
      "Train Epoch: 436 [33024/118836 (28%)] Loss: 12270.939453\n",
      "Train Epoch: 436 [65792/118836 (55%)] Loss: 12228.039062\n",
      "Train Epoch: 436 [98560/118836 (83%)] Loss: 12277.289062\n",
      "    epoch          : 436\n",
      "    loss           : 12305.589978320151\n",
      "    val_loss       : 12303.162516782399\n",
      "    val_log_likelihood: -12206.260461092586\n",
      "    val_log_marginal: -12214.634177679945\n",
      "Train Epoch: 437 [256/118836 (0%)] Loss: 12301.335938\n",
      "Train Epoch: 437 [33024/118836 (28%)] Loss: 12350.205078\n",
      "Train Epoch: 437 [65792/118836 (55%)] Loss: 12297.735352\n",
      "Train Epoch: 437 [98560/118836 (83%)] Loss: 12362.634766\n",
      "    epoch          : 437\n",
      "    loss           : 12305.066542920285\n",
      "    val_loss       : 12301.759430814489\n",
      "    val_log_likelihood: -12197.474054777967\n",
      "    val_log_marginal: -12205.21265792563\n",
      "Train Epoch: 438 [256/118836 (0%)] Loss: 12315.975586\n",
      "Train Epoch: 438 [33024/118836 (28%)] Loss: 12324.348633\n",
      "Train Epoch: 438 [65792/118836 (55%)] Loss: 12276.822266\n",
      "Train Epoch: 438 [98560/118836 (83%)] Loss: 12409.628906\n",
      "    epoch          : 438\n",
      "    loss           : 12305.429156327544\n",
      "    val_loss       : 12303.022152608915\n",
      "    val_log_likelihood: -12200.345404905915\n",
      "    val_log_marginal: -12208.164003710552\n",
      "Train Epoch: 439 [256/118836 (0%)] Loss: 12335.173828\n",
      "Train Epoch: 439 [33024/118836 (28%)] Loss: 12221.904297\n",
      "Train Epoch: 439 [65792/118836 (55%)] Loss: 12355.452148\n",
      "Train Epoch: 439 [98560/118836 (83%)] Loss: 12337.187500\n",
      "    epoch          : 439\n",
      "    loss           : 12302.619855478442\n",
      "    val_loss       : 12300.429132247824\n",
      "    val_log_likelihood: -12196.624288054436\n",
      "    val_log_marginal: -12204.478715124369\n",
      "Train Epoch: 440 [256/118836 (0%)] Loss: 12286.788086\n",
      "Train Epoch: 440 [33024/118836 (28%)] Loss: 12247.013672\n",
      "Train Epoch: 440 [65792/118836 (55%)] Loss: 12330.636719\n",
      "Train Epoch: 440 [98560/118836 (83%)] Loss: 12361.900391\n",
      "    epoch          : 440\n",
      "    loss           : 12308.465571527347\n",
      "    val_loss       : 12311.161579339405\n",
      "    val_log_likelihood: -12202.00577068471\n",
      "    val_log_marginal: -12210.494333724862\n",
      "Train Epoch: 441 [256/118836 (0%)] Loss: 12337.223633\n",
      "Train Epoch: 441 [33024/118836 (28%)] Loss: 12279.671875\n",
      "Train Epoch: 441 [65792/118836 (55%)] Loss: 12279.811523\n",
      "Train Epoch: 441 [98560/118836 (83%)] Loss: 12397.072266\n",
      "    epoch          : 441\n",
      "    loss           : 12303.638767027243\n",
      "    val_loss       : 12304.898321177847\n",
      "    val_log_likelihood: -12202.058203609646\n",
      "    val_log_marginal: -12209.716329775723\n",
      "Train Epoch: 442 [256/118836 (0%)] Loss: 12335.318359\n",
      "Train Epoch: 442 [33024/118836 (28%)] Loss: 12322.572266\n",
      "Train Epoch: 442 [65792/118836 (55%)] Loss: 12402.440430\n",
      "Train Epoch: 442 [98560/118836 (83%)] Loss: 12331.173828\n",
      "    epoch          : 442\n",
      "    loss           : 12305.984747531533\n",
      "    val_loss       : 12303.166853374722\n",
      "    val_log_likelihood: -12199.674463496433\n",
      "    val_log_marginal: -12207.682273648743\n",
      "Train Epoch: 443 [256/118836 (0%)] Loss: 12464.741211\n",
      "Train Epoch: 443 [33024/118836 (28%)] Loss: 12367.478516\n",
      "Train Epoch: 443 [65792/118836 (55%)] Loss: 12334.496094\n",
      "Train Epoch: 443 [98560/118836 (83%)] Loss: 12381.006836\n",
      "    epoch          : 443\n",
      "    loss           : 12301.00244617194\n",
      "    val_loss       : 12303.681406287977\n",
      "    val_log_likelihood: -12198.874608244158\n",
      "    val_log_marginal: -12206.864907777073\n",
      "Train Epoch: 444 [256/118836 (0%)] Loss: 12277.445312\n",
      "Train Epoch: 444 [33024/118836 (28%)] Loss: 12374.564453\n",
      "Train Epoch: 444 [65792/118836 (55%)] Loss: 12264.769531\n",
      "Train Epoch: 444 [98560/118836 (83%)] Loss: 12265.747070\n",
      "    epoch          : 444\n",
      "    loss           : 12299.042727234542\n",
      "    val_loss       : 12304.774981737255\n",
      "    val_log_likelihood: -12199.5191743887\n",
      "    val_log_marginal: -12207.38527445014\n",
      "Train Epoch: 445 [256/118836 (0%)] Loss: 12290.066406\n",
      "Train Epoch: 445 [33024/118836 (28%)] Loss: 12400.174805\n",
      "Train Epoch: 445 [65792/118836 (55%)] Loss: 12446.642578\n",
      "Train Epoch: 445 [98560/118836 (83%)] Loss: 12306.083984\n",
      "    epoch          : 445\n",
      "    loss           : 12300.65851313715\n",
      "    val_loss       : 12298.482484790906\n",
      "    val_log_likelihood: -12203.8617657607\n",
      "    val_log_marginal: -12211.85723435666\n",
      "Train Epoch: 446 [256/118836 (0%)] Loss: 12394.577148\n",
      "Train Epoch: 446 [33024/118836 (28%)] Loss: 12266.689453\n",
      "Train Epoch: 446 [65792/118836 (55%)] Loss: 12419.487305\n",
      "Train Epoch: 446 [98560/118836 (83%)] Loss: 12356.023438\n",
      "    epoch          : 446\n",
      "    loss           : 12299.306757101685\n",
      "    val_loss       : 12300.80227442858\n",
      "    val_log_likelihood: -12198.509389216295\n",
      "    val_log_marginal: -12206.34990272025\n",
      "Train Epoch: 447 [256/118836 (0%)] Loss: 12300.043945\n",
      "Train Epoch: 447 [33024/118836 (28%)] Loss: 12265.480469\n",
      "Train Epoch: 447 [65792/118836 (55%)] Loss: 12344.257812\n",
      "Train Epoch: 447 [98560/118836 (83%)] Loss: 12333.253906\n",
      "    epoch          : 447\n",
      "    loss           : 12303.925105491368\n",
      "    val_loss       : 12298.946313850125\n",
      "    val_log_likelihood: -12201.443160831524\n",
      "    val_log_marginal: -12209.478968689044\n",
      "Train Epoch: 448 [256/118836 (0%)] Loss: 12362.504883\n",
      "Train Epoch: 448 [33024/118836 (28%)] Loss: 12341.832031\n",
      "Train Epoch: 448 [65792/118836 (55%)] Loss: 12422.183594\n",
      "Train Epoch: 448 [98560/118836 (83%)] Loss: 12278.194336\n",
      "    epoch          : 448\n",
      "    loss           : 12307.187435703576\n",
      "    val_loss       : 12303.930764658135\n",
      "    val_log_likelihood: -12199.282033350135\n",
      "    val_log_marginal: -12207.276145285667\n",
      "Train Epoch: 449 [256/118836 (0%)] Loss: 12234.514648\n",
      "Train Epoch: 449 [33024/118836 (28%)] Loss: 12426.748047\n",
      "Train Epoch: 449 [65792/118836 (55%)] Loss: 12237.544922\n",
      "Train Epoch: 449 [98560/118836 (83%)] Loss: 12283.007812\n",
      "    epoch          : 449\n",
      "    loss           : 12305.40517763906\n",
      "    val_loss       : 12308.390807021246\n",
      "    val_log_likelihood: -12202.097564005635\n",
      "    val_log_marginal: -12210.34671124806\n",
      "Train Epoch: 450 [256/118836 (0%)] Loss: 12293.123047\n",
      "Train Epoch: 450 [33024/118836 (28%)] Loss: 12289.427734\n",
      "Train Epoch: 450 [65792/118836 (55%)] Loss: 12382.421875\n",
      "Train Epoch: 450 [98560/118836 (83%)] Loss: 12198.388672\n",
      "    epoch          : 450\n",
      "    loss           : 12303.247881610578\n",
      "    val_loss       : 12300.464194918357\n",
      "    val_log_likelihood: -12199.031287479322\n",
      "    val_log_marginal: -12206.833705829647\n",
      "Train Epoch: 451 [256/118836 (0%)] Loss: 12299.651367\n",
      "Train Epoch: 451 [33024/118836 (28%)] Loss: 12344.443359\n",
      "Train Epoch: 451 [65792/118836 (55%)] Loss: 12349.675781\n",
      "Train Epoch: 451 [98560/118836 (83%)] Loss: 12257.974609\n",
      "    epoch          : 451\n",
      "    loss           : 12300.596895032051\n",
      "    val_loss       : 12296.856455571064\n",
      "    val_log_likelihood: -12197.609167086694\n",
      "    val_log_marginal: -12205.378375169015\n",
      "Train Epoch: 452 [256/118836 (0%)] Loss: 12447.783203\n",
      "Train Epoch: 452 [33024/118836 (28%)] Loss: 12229.095703\n",
      "Train Epoch: 452 [65792/118836 (55%)] Loss: 12364.570312\n",
      "Train Epoch: 452 [98560/118836 (83%)] Loss: 12286.200195\n",
      "    epoch          : 452\n",
      "    loss           : 12301.353808674525\n",
      "    val_loss       : 12303.28089213466\n",
      "    val_log_likelihood: -12198.384224436519\n",
      "    val_log_marginal: -12206.226656898316\n",
      "Train Epoch: 453 [256/118836 (0%)] Loss: 12330.345703\n",
      "Train Epoch: 453 [33024/118836 (28%)] Loss: 12227.482422\n",
      "Train Epoch: 453 [65792/118836 (55%)] Loss: 12332.130859\n",
      "Train Epoch: 453 [98560/118836 (83%)] Loss: 12358.676758\n",
      "    epoch          : 453\n",
      "    loss           : 12303.39590376861\n",
      "    val_loss       : 12302.783307923122\n",
      "    val_log_likelihood: -12200.139963457661\n",
      "    val_log_marginal: -12207.903600051663\n",
      "Train Epoch: 454 [256/118836 (0%)] Loss: 12344.531250\n",
      "Train Epoch: 454 [33024/118836 (28%)] Loss: 12264.605469\n",
      "Train Epoch: 454 [65792/118836 (55%)] Loss: 12321.008789\n",
      "Train Epoch: 454 [98560/118836 (83%)] Loss: 12306.202148\n",
      "    epoch          : 454\n",
      "    loss           : 12301.945953687191\n",
      "    val_loss       : 12306.70876521679\n",
      "    val_log_likelihood: -12197.389611604372\n",
      "    val_log_marginal: -12205.348663097044\n",
      "Train Epoch: 455 [256/118836 (0%)] Loss: 12295.619141\n",
      "Train Epoch: 455 [33024/118836 (28%)] Loss: 12274.298828\n",
      "Train Epoch: 455 [65792/118836 (55%)] Loss: 12375.555664\n",
      "Train Epoch: 455 [98560/118836 (83%)] Loss: 12273.886719\n",
      "    epoch          : 455\n",
      "    loss           : 12298.992567301231\n",
      "    val_loss       : 12302.104855289583\n",
      "    val_log_likelihood: -12199.765839859905\n",
      "    val_log_marginal: -12207.549547808147\n",
      "Train Epoch: 456 [256/118836 (0%)] Loss: 12294.056641\n",
      "Train Epoch: 456 [33024/118836 (28%)] Loss: 12270.771484\n",
      "Train Epoch: 456 [65792/118836 (55%)] Loss: 12257.419922\n",
      "Train Epoch: 456 [98560/118836 (83%)] Loss: 12177.216797\n",
      "    epoch          : 456\n",
      "    loss           : 12300.837610014732\n",
      "    val_loss       : 12304.245429924662\n",
      "    val_log_likelihood: -12200.55472804875\n",
      "    val_log_marginal: -12208.29211723827\n",
      "Train Epoch: 457 [256/118836 (0%)] Loss: 12402.581055\n",
      "Train Epoch: 457 [33024/118836 (28%)] Loss: 12367.821289\n",
      "Train Epoch: 457 [65792/118836 (55%)] Loss: 12352.516602\n",
      "Train Epoch: 457 [98560/118836 (83%)] Loss: 12328.580078\n",
      "    epoch          : 457\n",
      "    loss           : 12303.92576784145\n",
      "    val_loss       : 12300.979198651672\n",
      "    val_log_likelihood: -12201.22785699054\n",
      "    val_log_marginal: -12209.285730652668\n",
      "Train Epoch: 458 [256/118836 (0%)] Loss: 12401.590820\n",
      "Train Epoch: 458 [33024/118836 (28%)] Loss: 12369.273438\n",
      "Train Epoch: 458 [65792/118836 (55%)] Loss: 12305.392578\n",
      "Train Epoch: 458 [98560/118836 (83%)] Loss: 12312.738281\n",
      "    epoch          : 458\n",
      "    loss           : 12301.79603866186\n",
      "    val_loss       : 12303.145950719905\n",
      "    val_log_likelihood: -12200.150294018817\n",
      "    val_log_marginal: -12208.072745687836\n",
      "Train Epoch: 459 [256/118836 (0%)] Loss: 12230.172852\n",
      "Train Epoch: 459 [33024/118836 (28%)] Loss: 12338.207031\n",
      "Train Epoch: 459 [65792/118836 (55%)] Loss: 12355.943359\n",
      "Train Epoch: 459 [98560/118836 (83%)] Loss: 12363.017578\n",
      "    epoch          : 459\n",
      "    loss           : 12302.931970507652\n",
      "    val_loss       : 12297.341519198288\n",
      "    val_log_likelihood: -12198.491473292752\n",
      "    val_log_marginal: -12206.493183969058\n",
      "Train Epoch: 460 [256/118836 (0%)] Loss: 12266.048828\n",
      "Train Epoch: 460 [33024/118836 (28%)] Loss: 12260.657227\n",
      "Train Epoch: 460 [65792/118836 (55%)] Loss: 12300.522461\n",
      "Train Epoch: 460 [98560/118836 (83%)] Loss: 12313.764648\n",
      "    epoch          : 460\n",
      "    loss           : 12302.1962218582\n",
      "    val_loss       : 12305.319874203551\n",
      "    val_log_likelihood: -12198.822974501138\n",
      "    val_log_marginal: -12206.648717071377\n",
      "Train Epoch: 461 [256/118836 (0%)] Loss: 12311.840820\n",
      "Train Epoch: 461 [33024/118836 (28%)] Loss: 12212.783203\n",
      "Train Epoch: 461 [65792/118836 (55%)] Loss: 12258.654297\n",
      "Train Epoch: 461 [98560/118836 (83%)] Loss: 12253.519531\n",
      "    epoch          : 461\n",
      "    loss           : 12299.406893287325\n",
      "    val_loss       : 12304.776369980787\n",
      "    val_log_likelihood: -12207.89515741315\n",
      "    val_log_marginal: -12215.953256690016\n",
      "Train Epoch: 462 [256/118836 (0%)] Loss: 12382.795898\n",
      "Train Epoch: 462 [33024/118836 (28%)] Loss: 12341.245117\n",
      "Train Epoch: 462 [65792/118836 (55%)] Loss: 12293.529297\n",
      "Train Epoch: 462 [98560/118836 (83%)] Loss: 12257.518555\n",
      "    epoch          : 462\n",
      "    loss           : 12301.39537841191\n",
      "    val_loss       : 12302.361127798915\n",
      "    val_log_likelihood: -12199.150233599566\n",
      "    val_log_marginal: -12206.897101751516\n",
      "Train Epoch: 463 [256/118836 (0%)] Loss: 12218.023438\n",
      "Train Epoch: 463 [33024/118836 (28%)] Loss: 12414.282227\n",
      "Train Epoch: 463 [65792/118836 (55%)] Loss: 12316.303711\n",
      "Train Epoch: 463 [98560/118836 (83%)] Loss: 12465.126953\n",
      "    epoch          : 463\n",
      "    loss           : 12300.441857617348\n",
      "    val_loss       : 12303.8105946317\n",
      "    val_log_likelihood: -12198.02874857837\n",
      "    val_log_marginal: -12206.172759959798\n",
      "Train Epoch: 464 [256/118836 (0%)] Loss: 12275.290039\n",
      "Train Epoch: 464 [33024/118836 (28%)] Loss: 12235.221680\n",
      "Train Epoch: 464 [65792/118836 (55%)] Loss: 12221.356445\n",
      "Train Epoch: 464 [98560/118836 (83%)] Loss: 12281.944336\n",
      "    epoch          : 464\n",
      "    loss           : 12303.188718077958\n",
      "    val_loss       : 12296.217440479704\n",
      "    val_log_likelihood: -12198.800272371278\n",
      "    val_log_marginal: -12206.897766145405\n",
      "Train Epoch: 465 [256/118836 (0%)] Loss: 12372.079102\n",
      "Train Epoch: 465 [33024/118836 (28%)] Loss: 12308.115234\n",
      "Train Epoch: 465 [65792/118836 (55%)] Loss: 12351.623047\n",
      "Train Epoch: 465 [98560/118836 (83%)] Loss: 12269.601562\n",
      "    epoch          : 465\n",
      "    loss           : 12294.68224804849\n",
      "    val_loss       : 12291.132192399757\n",
      "    val_log_likelihood: -12193.695540930004\n",
      "    val_log_marginal: -12201.701423581551\n",
      "Train Epoch: 466 [256/118836 (0%)] Loss: 12305.041992\n",
      "Train Epoch: 466 [33024/118836 (28%)] Loss: 12300.282227\n",
      "Train Epoch: 466 [65792/118836 (55%)] Loss: 12263.828125\n",
      "Train Epoch: 466 [98560/118836 (83%)] Loss: 12275.453125\n",
      "    epoch          : 466\n",
      "    loss           : 12287.483878237437\n",
      "    val_loss       : 12290.854053745998\n",
      "    val_log_likelihood: -12197.926156043217\n",
      "    val_log_marginal: -12205.652600663101\n",
      "Train Epoch: 467 [256/118836 (0%)] Loss: 12337.835938\n",
      "Train Epoch: 467 [33024/118836 (28%)] Loss: 12206.788086\n",
      "Train Epoch: 467 [65792/118836 (55%)] Loss: 12214.925781\n",
      "Train Epoch: 467 [98560/118836 (83%)] Loss: 12330.863281\n",
      "    epoch          : 467\n",
      "    loss           : 12292.585281288772\n",
      "    val_loss       : 12286.638187414535\n",
      "    val_log_likelihood: -12192.219753056504\n",
      "    val_log_marginal: -12200.28639466172\n",
      "Train Epoch: 468 [256/118836 (0%)] Loss: 12300.454102\n",
      "Train Epoch: 468 [33024/118836 (28%)] Loss: 12379.076172\n",
      "Train Epoch: 468 [65792/118836 (55%)] Loss: 12316.525391\n",
      "Train Epoch: 468 [98560/118836 (83%)] Loss: 12361.824219\n",
      "    epoch          : 468\n",
      "    loss           : 12290.555655661963\n",
      "    val_loss       : 12286.929596566404\n",
      "    val_log_likelihood: -12191.116260209885\n",
      "    val_log_marginal: -12199.21604639797\n",
      "Train Epoch: 469 [256/118836 (0%)] Loss: 12309.902344\n",
      "Train Epoch: 469 [33024/118836 (28%)] Loss: 12255.916992\n",
      "Train Epoch: 469 [65792/118836 (55%)] Loss: 12450.365234\n",
      "Train Epoch: 469 [98560/118836 (83%)] Loss: 12307.675781\n",
      "    epoch          : 469\n",
      "    loss           : 12288.945405875207\n",
      "    val_loss       : 12292.035766992982\n",
      "    val_log_likelihood: -12192.61906114299\n",
      "    val_log_marginal: -12200.832343759614\n",
      "Train Epoch: 470 [256/118836 (0%)] Loss: 12251.384766\n",
      "Train Epoch: 470 [33024/118836 (28%)] Loss: 12267.751953\n",
      "Train Epoch: 470 [65792/118836 (55%)] Loss: 12203.774414\n",
      "Train Epoch: 470 [98560/118836 (83%)] Loss: 12321.678711\n",
      "    epoch          : 470\n",
      "    loss           : 12291.62076499819\n",
      "    val_loss       : 12289.213587656333\n",
      "    val_log_likelihood: -12195.698385481286\n",
      "    val_log_marginal: -12203.564933575351\n",
      "Train Epoch: 471 [256/118836 (0%)] Loss: 12202.636719\n",
      "Train Epoch: 471 [33024/118836 (28%)] Loss: 12302.489258\n",
      "Train Epoch: 471 [65792/118836 (55%)] Loss: 12370.841797\n",
      "Train Epoch: 471 [98560/118836 (83%)] Loss: 12337.233398\n",
      "    epoch          : 471\n",
      "    loss           : 12283.919683913617\n",
      "    val_loss       : 12291.966795436178\n",
      "    val_log_likelihood: -12193.661446055623\n",
      "    val_log_marginal: -12201.816176558257\n",
      "Train Epoch: 472 [256/118836 (0%)] Loss: 12428.108398\n",
      "Train Epoch: 472 [33024/118836 (28%)] Loss: 12171.216797\n",
      "Train Epoch: 472 [65792/118836 (55%)] Loss: 12369.705078\n",
      "Train Epoch: 472 [98560/118836 (83%)] Loss: 12283.708984\n",
      "    epoch          : 472\n",
      "    loss           : 12285.533967897021\n",
      "    val_loss       : 12291.88475502585\n",
      "    val_log_likelihood: -12195.862992723843\n",
      "    val_log_marginal: -12203.914598985444\n",
      "Train Epoch: 473 [256/118836 (0%)] Loss: 12298.587891\n",
      "Train Epoch: 473 [33024/118836 (28%)] Loss: 12300.058594\n",
      "Train Epoch: 473 [65792/118836 (55%)] Loss: 12469.562500\n",
      "Train Epoch: 473 [98560/118836 (83%)] Loss: 12342.039062\n",
      "    epoch          : 473\n",
      "    loss           : 12287.245211532\n",
      "    val_loss       : 12290.399023867332\n",
      "    val_log_likelihood: -12194.844019948046\n",
      "    val_log_marginal: -12202.962795842808\n",
      "Train Epoch: 474 [256/118836 (0%)] Loss: 12349.352539\n",
      "Train Epoch: 474 [33024/118836 (28%)] Loss: 12412.058594\n",
      "Train Epoch: 474 [65792/118836 (55%)] Loss: 12281.733398\n",
      "Train Epoch: 474 [98560/118836 (83%)] Loss: 12345.680664\n",
      "    epoch          : 474\n",
      "    loss           : 12285.87159358199\n",
      "    val_loss       : 12291.061152506558\n",
      "    val_log_likelihood: -12190.999740875724\n",
      "    val_log_marginal: -12199.229179742093\n",
      "Train Epoch: 475 [256/118836 (0%)] Loss: 12277.400391\n",
      "Train Epoch: 475 [33024/118836 (28%)] Loss: 12380.296875\n",
      "Train Epoch: 475 [65792/118836 (55%)] Loss: 12315.293945\n",
      "Train Epoch: 475 [98560/118836 (83%)] Loss: 12238.966797\n",
      "    epoch          : 475\n",
      "    loss           : 12289.78059928143\n",
      "    val_loss       : 12290.120634783745\n",
      "    val_log_likelihood: -12194.096820558056\n",
      "    val_log_marginal: -12202.14775097003\n",
      "Train Epoch: 476 [256/118836 (0%)] Loss: 12411.588867\n",
      "Train Epoch: 476 [33024/118836 (28%)] Loss: 12312.363281\n",
      "Train Epoch: 476 [65792/118836 (55%)] Loss: 12356.589844\n",
      "Train Epoch: 476 [98560/118836 (83%)] Loss: 12296.735352\n",
      "    epoch          : 476\n",
      "    loss           : 12291.295495050144\n",
      "    val_loss       : 12288.144797847302\n",
      "    val_log_likelihood: -12192.931018015923\n",
      "    val_log_marginal: -12201.082792386802\n",
      "Train Epoch: 477 [256/118836 (0%)] Loss: 12350.707031\n",
      "Train Epoch: 477 [33024/118836 (28%)] Loss: 12311.788086\n",
      "Train Epoch: 477 [65792/118836 (55%)] Loss: 12400.767578\n",
      "Train Epoch: 477 [98560/118836 (83%)] Loss: 12221.458984\n",
      "    epoch          : 477\n",
      "    loss           : 12290.58899109543\n",
      "    val_loss       : 12288.870093192956\n",
      "    val_log_likelihood: -12192.129920938016\n",
      "    val_log_marginal: -12200.206782645282\n",
      "Train Epoch: 478 [256/118836 (0%)] Loss: 12391.211914\n",
      "Train Epoch: 478 [33024/118836 (28%)] Loss: 12219.133789\n",
      "Train Epoch: 478 [65792/118836 (55%)] Loss: 12309.574219\n",
      "Train Epoch: 478 [98560/118836 (83%)] Loss: 12268.223633\n",
      "    epoch          : 478\n",
      "    loss           : 12288.392692016905\n",
      "    val_loss       : 12294.921717520447\n",
      "    val_log_likelihood: -12193.966301889475\n",
      "    val_log_marginal: -12202.30130507011\n",
      "Train Epoch: 479 [256/118836 (0%)] Loss: 12295.962891\n",
      "Train Epoch: 479 [33024/118836 (28%)] Loss: 12298.355469\n",
      "Train Epoch: 479 [65792/118836 (55%)] Loss: 12251.524414\n",
      "Train Epoch: 479 [98560/118836 (83%)] Loss: 12271.873047\n",
      "    epoch          : 479\n",
      "    loss           : 12288.281674711798\n",
      "    val_loss       : 12286.045692985823\n",
      "    val_log_likelihood: -12191.481667280552\n",
      "    val_log_marginal: -12199.47230372538\n",
      "Train Epoch: 480 [256/118836 (0%)] Loss: 12225.627930\n",
      "Train Epoch: 480 [33024/118836 (28%)] Loss: 12282.494141\n",
      "Train Epoch: 480 [65792/118836 (55%)] Loss: 12343.272461\n",
      "Train Epoch: 480 [98560/118836 (83%)] Loss: 12248.669922\n",
      "    epoch          : 480\n",
      "    loss           : 12290.901547314414\n",
      "    val_loss       : 12284.056090014077\n",
      "    val_log_likelihood: -12193.785745418476\n",
      "    val_log_marginal: -12201.712503293118\n",
      "Train Epoch: 481 [256/118836 (0%)] Loss: 12325.762695\n",
      "Train Epoch: 481 [33024/118836 (28%)] Loss: 12420.962891\n",
      "Train Epoch: 481 [65792/118836 (55%)] Loss: 12321.359375\n",
      "Train Epoch: 481 [98560/118836 (83%)] Loss: 12310.654297\n",
      "    epoch          : 481\n",
      "    loss           : 12290.60596712805\n",
      "    val_loss       : 12289.359336363274\n",
      "    val_log_likelihood: -12195.986672385494\n",
      "    val_log_marginal: -12204.064095741795\n",
      "Train Epoch: 482 [256/118836 (0%)] Loss: 12221.316406\n",
      "Train Epoch: 482 [33024/118836 (28%)] Loss: 12319.535156\n",
      "Train Epoch: 482 [65792/118836 (55%)] Loss: 12242.826172\n",
      "Train Epoch: 482 [98560/118836 (83%)] Loss: 12284.236328\n",
      "    epoch          : 482\n",
      "    loss           : 12289.69995670492\n",
      "    val_loss       : 12293.136420205798\n",
      "    val_log_likelihood: -12192.37784616677\n",
      "    val_log_marginal: -12200.785379061334\n",
      "Train Epoch: 483 [256/118836 (0%)] Loss: 12366.961914\n",
      "Train Epoch: 483 [33024/118836 (28%)] Loss: 12231.509766\n",
      "Train Epoch: 483 [65792/118836 (55%)] Loss: 12202.433594\n",
      "Train Epoch: 483 [98560/118836 (83%)] Loss: 12350.772461\n",
      "    epoch          : 483\n",
      "    loss           : 12292.8405104619\n",
      "    val_loss       : 12287.777225144006\n",
      "    val_log_likelihood: -12193.046806987955\n",
      "    val_log_marginal: -12201.190536083048\n",
      "Train Epoch: 484 [256/118836 (0%)] Loss: 12307.910156\n",
      "Train Epoch: 484 [33024/118836 (28%)] Loss: 12247.257812\n",
      "Train Epoch: 484 [65792/118836 (55%)] Loss: 12201.037109\n",
      "Train Epoch: 484 [98560/118836 (83%)] Loss: 12187.695312\n",
      "    epoch          : 484\n",
      "    loss           : 12287.666663112594\n",
      "    val_loss       : 12286.544479155044\n",
      "    val_log_likelihood: -12193.354347278226\n",
      "    val_log_marginal: -12201.527718505855\n",
      "Train Epoch: 485 [256/118836 (0%)] Loss: 12355.256836\n",
      "Train Epoch: 485 [33024/118836 (28%)] Loss: 12293.547852\n",
      "Train Epoch: 485 [65792/118836 (55%)] Loss: 12269.917969\n",
      "Train Epoch: 485 [98560/118836 (83%)] Loss: 12312.485352\n",
      "    epoch          : 485\n",
      "    loss           : 12284.727547786135\n",
      "    val_loss       : 12289.977743413796\n",
      "    val_log_likelihood: -12197.90317766491\n",
      "    val_log_marginal: -12206.416926624524\n",
      "Train Epoch: 486 [256/118836 (0%)] Loss: 12236.941406\n",
      "Train Epoch: 486 [33024/118836 (28%)] Loss: 12419.838867\n",
      "Train Epoch: 486 [65792/118836 (55%)] Loss: 12254.664062\n",
      "Train Epoch: 486 [98560/118836 (83%)] Loss: 12361.647461\n",
      "    epoch          : 486\n",
      "    loss           : 12294.720332532052\n",
      "    val_loss       : 12294.58738729588\n",
      "    val_log_likelihood: -12198.182690853753\n",
      "    val_log_marginal: -12206.379623443254\n",
      "Train Epoch: 487 [256/118836 (0%)] Loss: 12302.085938\n",
      "Train Epoch: 487 [33024/118836 (28%)] Loss: 12210.398438\n",
      "Train Epoch: 487 [65792/118836 (55%)] Loss: 12283.796875\n",
      "Train Epoch: 487 [98560/118836 (83%)] Loss: 12334.441406\n",
      "    epoch          : 487\n",
      "    loss           : 12287.384249476581\n",
      "    val_loss       : 12293.92673102608\n",
      "    val_log_likelihood: -12191.684664172353\n",
      "    val_log_marginal: -12199.617583929763\n",
      "Train Epoch: 488 [256/118836 (0%)] Loss: 12254.789062\n",
      "Train Epoch: 488 [33024/118836 (28%)] Loss: 12268.377930\n",
      "Train Epoch: 488 [65792/118836 (55%)] Loss: 12281.781250\n",
      "Train Epoch: 488 [98560/118836 (83%)] Loss: 12359.810547\n",
      "    epoch          : 488\n",
      "    loss           : 12288.009297133478\n",
      "    val_loss       : 12287.313331857182\n",
      "    val_log_likelihood: -12192.218825281741\n",
      "    val_log_marginal: -12200.191423035743\n",
      "Train Epoch: 489 [256/118836 (0%)] Loss: 12256.847656\n",
      "Train Epoch: 489 [33024/118836 (28%)] Loss: 12362.857422\n",
      "Train Epoch: 489 [65792/118836 (55%)] Loss: 12225.800781\n",
      "Train Epoch: 489 [98560/118836 (83%)] Loss: 12200.483398\n",
      "    epoch          : 489\n",
      "    loss           : 12283.785595662737\n",
      "    val_loss       : 12286.872884238897\n",
      "    val_log_likelihood: -12192.26090648263\n",
      "    val_log_marginal: -12200.329842256191\n",
      "Train Epoch: 490 [256/118836 (0%)] Loss: 12266.843750\n",
      "Train Epoch: 490 [33024/118836 (28%)] Loss: 12305.018555\n",
      "Train Epoch: 490 [65792/118836 (55%)] Loss: 12286.787109\n",
      "Train Epoch: 490 [98560/118836 (83%)] Loss: 12280.976562\n",
      "    epoch          : 490\n",
      "    loss           : 12289.95777889785\n",
      "    val_loss       : 12289.093261620446\n",
      "    val_log_likelihood: -12195.269538519695\n",
      "    val_log_marginal: -12203.359567724585\n",
      "Train Epoch: 491 [256/118836 (0%)] Loss: 12292.000000\n",
      "Train Epoch: 491 [33024/118836 (28%)] Loss: 12318.256836\n",
      "Train Epoch: 491 [65792/118836 (55%)] Loss: 12319.698242\n",
      "Train Epoch: 491 [98560/118836 (83%)] Loss: 12289.210938\n",
      "    epoch          : 491\n",
      "    loss           : 12290.803599953473\n",
      "    val_loss       : 12289.288803492815\n",
      "    val_log_likelihood: -12194.702869106699\n",
      "    val_log_marginal: -12203.065642300631\n",
      "Train Epoch: 492 [256/118836 (0%)] Loss: 12328.217773\n",
      "Train Epoch: 492 [33024/118836 (28%)] Loss: 12341.251953\n",
      "Train Epoch: 492 [65792/118836 (55%)] Loss: 12332.877930\n",
      "Train Epoch: 492 [98560/118836 (83%)] Loss: 12205.904297\n",
      "    epoch          : 492\n",
      "    loss           : 12287.466230484906\n",
      "    val_loss       : 12288.570009219995\n",
      "    val_log_likelihood: -12190.904502849722\n",
      "    val_log_marginal: -12199.179646722703\n",
      "Train Epoch: 493 [256/118836 (0%)] Loss: 12182.334961\n",
      "Train Epoch: 493 [33024/118836 (28%)] Loss: 12293.977539\n",
      "Train Epoch: 493 [65792/118836 (55%)] Loss: 12282.425781\n",
      "Train Epoch: 493 [98560/118836 (83%)] Loss: 12321.003906\n",
      "    epoch          : 493\n",
      "    loss           : 12286.959092612697\n",
      "    val_loss       : 12287.198539612398\n",
      "    val_log_likelihood: -12192.181578105614\n",
      "    val_log_marginal: -12200.418174804943\n",
      "Train Epoch: 494 [256/118836 (0%)] Loss: 12292.075195\n",
      "Train Epoch: 494 [33024/118836 (28%)] Loss: 12397.825195\n",
      "Train Epoch: 494 [65792/118836 (55%)] Loss: 12359.582031\n",
      "Train Epoch: 494 [98560/118836 (83%)] Loss: 12306.560547\n",
      "    epoch          : 494\n",
      "    loss           : 12285.045223486612\n",
      "    val_loss       : 12292.345597903332\n",
      "    val_log_likelihood: -12193.288928091399\n",
      "    val_log_marginal: -12201.579160478188\n",
      "Train Epoch: 495 [256/118836 (0%)] Loss: 12322.406250\n",
      "Train Epoch: 495 [33024/118836 (28%)] Loss: 12247.163086\n",
      "Train Epoch: 495 [65792/118836 (55%)] Loss: 12273.048828\n",
      "Train Epoch: 495 [98560/118836 (83%)] Loss: 12294.402344\n",
      "    epoch          : 495\n",
      "    loss           : 12288.697161587572\n",
      "    val_loss       : 12287.196794284515\n",
      "    val_log_likelihood: -12193.611498882083\n",
      "    val_log_marginal: -12201.669212268445\n",
      "Train Epoch: 496 [256/118836 (0%)] Loss: 12303.831055\n",
      "Train Epoch: 496 [33024/118836 (28%)] Loss: 12195.886719\n",
      "Train Epoch: 496 [65792/118836 (55%)] Loss: 12414.671875\n",
      "Train Epoch: 496 [98560/118836 (83%)] Loss: 12310.296875\n",
      "    epoch          : 496\n",
      "    loss           : 12284.929335000515\n",
      "    val_loss       : 12284.07261074608\n",
      "    val_log_likelihood: -12195.071733806348\n",
      "    val_log_marginal: -12203.217596890865\n",
      "Train Epoch: 497 [256/118836 (0%)] Loss: 12218.824219\n",
      "Train Epoch: 497 [33024/118836 (28%)] Loss: 12324.496094\n",
      "Train Epoch: 497 [65792/118836 (55%)] Loss: 12242.656250\n",
      "Train Epoch: 497 [98560/118836 (83%)] Loss: 12193.666016\n",
      "    epoch          : 497\n",
      "    loss           : 12288.0215989131\n",
      "    val_loss       : 12288.712097687983\n",
      "    val_log_likelihood: -12192.386275136994\n",
      "    val_log_marginal: -12200.752632677746\n",
      "Train Epoch: 498 [256/118836 (0%)] Loss: 12360.846680\n",
      "Train Epoch: 498 [33024/118836 (28%)] Loss: 12354.466797\n",
      "Train Epoch: 498 [65792/118836 (55%)] Loss: 12288.890625\n",
      "Train Epoch: 498 [98560/118836 (83%)] Loss: 12344.486328\n",
      "    epoch          : 498\n",
      "    loss           : 12287.042680062294\n",
      "    val_loss       : 12283.299217846194\n",
      "    val_log_likelihood: -12190.770610557536\n",
      "    val_log_marginal: -12198.806140611678\n",
      "Train Epoch: 499 [256/118836 (0%)] Loss: 12211.703125\n",
      "Train Epoch: 499 [33024/118836 (28%)] Loss: 12300.517578\n",
      "Train Epoch: 499 [65792/118836 (55%)] Loss: 12335.640625\n",
      "Train Epoch: 499 [98560/118836 (83%)] Loss: 12305.327148\n",
      "    epoch          : 499\n",
      "    loss           : 12286.86648314723\n",
      "    val_loss       : 12287.425446036905\n",
      "    val_log_likelihood: -12190.093195402967\n",
      "    val_log_marginal: -12198.053744327088\n",
      "Train Epoch: 500 [256/118836 (0%)] Loss: 12397.376953\n",
      "Train Epoch: 500 [33024/118836 (28%)] Loss: 12384.034180\n",
      "Train Epoch: 500 [65792/118836 (55%)] Loss: 12429.424805\n",
      "Train Epoch: 500 [98560/118836 (83%)] Loss: 12253.730469\n",
      "    epoch          : 500\n",
      "    loss           : 12289.753979754705\n",
      "    val_loss       : 12289.138345205563\n",
      "    val_log_likelihood: -12191.29856011554\n",
      "    val_log_marginal: -12199.418871727987\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch500.pth ...\n",
      "Train Epoch: 501 [256/118836 (0%)] Loss: 12343.343750\n",
      "Train Epoch: 501 [33024/118836 (28%)] Loss: 12232.771484\n",
      "Train Epoch: 501 [65792/118836 (55%)] Loss: 12329.578125\n",
      "Train Epoch: 501 [98560/118836 (83%)] Loss: 12300.447266\n",
      "    epoch          : 501\n",
      "    loss           : 12287.330823995811\n",
      "    val_loss       : 12286.441542329125\n",
      "    val_log_likelihood: -12190.09574593543\n",
      "    val_log_marginal: -12198.36929079651\n",
      "Train Epoch: 502 [256/118836 (0%)] Loss: 12327.964844\n",
      "Train Epoch: 502 [33024/118836 (28%)] Loss: 12311.828125\n",
      "Train Epoch: 502 [65792/118836 (55%)] Loss: 12235.680664\n",
      "Train Epoch: 502 [98560/118836 (83%)] Loss: 12369.784180\n",
      "    epoch          : 502\n",
      "    loss           : 12283.821789863781\n",
      "    val_loss       : 12286.70415770104\n",
      "    val_log_likelihood: -12191.906115752947\n",
      "    val_log_marginal: -12199.777503610778\n",
      "Train Epoch: 503 [256/118836 (0%)] Loss: 12303.384766\n",
      "Train Epoch: 503 [33024/118836 (28%)] Loss: 12304.339844\n",
      "Train Epoch: 503 [65792/118836 (55%)] Loss: 12279.277344\n",
      "Train Epoch: 503 [98560/118836 (83%)] Loss: 12368.952148\n",
      "    epoch          : 503\n",
      "    loss           : 12283.684502623553\n",
      "    val_loss       : 12290.236303147458\n",
      "    val_log_likelihood: -12192.232159681296\n",
      "    val_log_marginal: -12200.687678122737\n",
      "Train Epoch: 504 [256/118836 (0%)] Loss: 12298.521484\n",
      "Train Epoch: 504 [33024/118836 (28%)] Loss: 12373.767578\n",
      "Train Epoch: 504 [65792/118836 (55%)] Loss: 12358.462891\n",
      "Train Epoch: 504 [98560/118836 (83%)] Loss: 12291.093750\n",
      "    epoch          : 504\n",
      "    loss           : 12291.263967186209\n",
      "    val_loss       : 12284.894434666334\n",
      "    val_log_likelihood: -12190.873139604011\n",
      "    val_log_marginal: -12199.14660734135\n",
      "Train Epoch: 505 [256/118836 (0%)] Loss: 12369.803711\n",
      "Train Epoch: 505 [33024/118836 (28%)] Loss: 12365.289062\n",
      "Train Epoch: 505 [65792/118836 (55%)] Loss: 12306.081055\n",
      "Train Epoch: 505 [98560/118836 (83%)] Loss: 12211.054688\n",
      "    epoch          : 505\n",
      "    loss           : 12285.261138951353\n",
      "    val_loss       : 12287.632877400181\n",
      "    val_log_likelihood: -12192.304107539807\n",
      "    val_log_marginal: -12200.250624799248\n",
      "Train Epoch: 506 [256/118836 (0%)] Loss: 12333.540039\n",
      "Train Epoch: 506 [33024/118836 (28%)] Loss: 12284.433594\n",
      "Train Epoch: 506 [65792/118836 (55%)] Loss: 12349.863281\n",
      "Train Epoch: 506 [98560/118836 (83%)] Loss: 12359.476562\n",
      "    epoch          : 506\n",
      "    loss           : 12283.489049252998\n",
      "    val_loss       : 12284.414329701462\n",
      "    val_log_likelihood: -12187.777471858199\n",
      "    val_log_marginal: -12195.922312574949\n",
      "Train Epoch: 507 [256/118836 (0%)] Loss: 12344.083984\n",
      "Train Epoch: 507 [33024/118836 (28%)] Loss: 12245.285156\n",
      "Train Epoch: 507 [65792/118836 (55%)] Loss: 12271.450195\n",
      "Train Epoch: 507 [98560/118836 (83%)] Loss: 12301.361328\n",
      "    epoch          : 507\n",
      "    loss           : 12286.340125814206\n",
      "    val_loss       : 12284.768407606201\n",
      "    val_log_likelihood: -12190.310332176645\n",
      "    val_log_marginal: -12198.458430492183\n",
      "Train Epoch: 508 [256/118836 (0%)] Loss: 12211.932617\n",
      "Train Epoch: 508 [33024/118836 (28%)] Loss: 12209.134766\n",
      "Train Epoch: 508 [65792/118836 (55%)] Loss: 12302.661133\n",
      "Train Epoch: 508 [98560/118836 (83%)] Loss: 12357.767578\n",
      "    epoch          : 508\n",
      "    loss           : 12285.080676501757\n",
      "    val_loss       : 12287.196698626962\n",
      "    val_log_likelihood: -12190.112111313585\n",
      "    val_log_marginal: -12198.239398250867\n",
      "Train Epoch: 509 [256/118836 (0%)] Loss: 12241.034180\n",
      "Train Epoch: 509 [33024/118836 (28%)] Loss: 12298.936523\n",
      "Train Epoch: 509 [65792/118836 (55%)] Loss: 12329.630859\n",
      "Train Epoch: 509 [98560/118836 (83%)] Loss: 12327.404297\n",
      "    epoch          : 509\n",
      "    loss           : 12287.979539359749\n",
      "    val_loss       : 12284.330412313153\n",
      "    val_log_likelihood: -12192.277344073096\n",
      "    val_log_marginal: -12200.541621210705\n",
      "Train Epoch: 510 [256/118836 (0%)] Loss: 12361.625000\n",
      "Train Epoch: 510 [33024/118836 (28%)] Loss: 12247.427734\n",
      "Train Epoch: 510 [65792/118836 (55%)] Loss: 12314.033203\n",
      "Train Epoch: 510 [98560/118836 (83%)] Loss: 12279.187500\n",
      "    epoch          : 510\n",
      "    loss           : 12283.930851620658\n",
      "    val_loss       : 12289.702523077578\n",
      "    val_log_likelihood: -12191.438951354427\n",
      "    val_log_marginal: -12199.55922188246\n",
      "Train Epoch: 511 [256/118836 (0%)] Loss: 12267.332031\n",
      "Train Epoch: 511 [33024/118836 (28%)] Loss: 12217.941406\n",
      "Train Epoch: 511 [65792/118836 (55%)] Loss: 12293.982422\n",
      "Train Epoch: 511 [98560/118836 (83%)] Loss: 12407.501953\n",
      "    epoch          : 511\n",
      "    loss           : 12287.505780862284\n",
      "    val_loss       : 12287.099115488974\n",
      "    val_log_likelihood: -12194.275512432796\n",
      "    val_log_marginal: -12202.80358237759\n",
      "Train Epoch: 512 [256/118836 (0%)] Loss: 12220.087891\n",
      "Train Epoch: 512 [33024/118836 (28%)] Loss: 12350.980469\n",
      "Train Epoch: 512 [65792/118836 (55%)] Loss: 12451.313477\n",
      "Train Epoch: 512 [98560/118836 (83%)] Loss: 12323.099609\n",
      "    epoch          : 512\n",
      "    loss           : 12286.77207838994\n",
      "    val_loss       : 12283.446593962377\n",
      "    val_log_likelihood: -12187.329071675971\n",
      "    val_log_marginal: -12195.38476528012\n",
      "Train Epoch: 513 [256/118836 (0%)] Loss: 12316.503906\n",
      "Train Epoch: 513 [33024/118836 (28%)] Loss: 12223.509766\n",
      "Train Epoch: 513 [65792/118836 (55%)] Loss: 12278.126953\n",
      "Train Epoch: 513 [98560/118836 (83%)] Loss: 12313.890625\n",
      "    epoch          : 513\n",
      "    loss           : 12282.94076102409\n",
      "    val_loss       : 12283.908504658792\n",
      "    val_log_likelihood: -12191.044072936053\n",
      "    val_log_marginal: -12199.094753494974\n",
      "Train Epoch: 514 [256/118836 (0%)] Loss: 12295.579102\n",
      "Train Epoch: 514 [33024/118836 (28%)] Loss: 12299.490234\n",
      "Train Epoch: 514 [65792/118836 (55%)] Loss: 12334.276367\n",
      "Train Epoch: 514 [98560/118836 (83%)] Loss: 12360.523438\n",
      "    epoch          : 514\n",
      "    loss           : 12287.186145897953\n",
      "    val_loss       : 12289.435004246216\n",
      "    val_log_likelihood: -12190.593655009305\n",
      "    val_log_marginal: -12198.748569690639\n",
      "Train Epoch: 515 [256/118836 (0%)] Loss: 12360.825195\n",
      "Train Epoch: 515 [33024/118836 (28%)] Loss: 12400.534180\n",
      "Train Epoch: 515 [65792/118836 (55%)] Loss: 12366.552734\n",
      "Train Epoch: 515 [98560/118836 (83%)] Loss: 12359.216797\n",
      "    epoch          : 515\n",
      "    loss           : 12283.56698685639\n",
      "    val_loss       : 12284.869711673458\n",
      "    val_log_likelihood: -12188.49418392008\n",
      "    val_log_marginal: -12196.385486139296\n",
      "Train Epoch: 516 [256/118836 (0%)] Loss: 12224.287109\n",
      "Train Epoch: 516 [33024/118836 (28%)] Loss: 12367.024414\n",
      "Train Epoch: 516 [65792/118836 (55%)] Loss: 12361.265625\n",
      "Train Epoch: 516 [98560/118836 (83%)] Loss: 12323.664062\n",
      "    epoch          : 516\n",
      "    loss           : 12287.64544303143\n",
      "    val_loss       : 12284.972553363594\n",
      "    val_log_likelihood: -12189.389317908655\n",
      "    val_log_marginal: -12197.298302109471\n",
      "Train Epoch: 517 [256/118836 (0%)] Loss: 12296.188477\n",
      "Train Epoch: 517 [33024/118836 (28%)] Loss: 12331.556641\n",
      "Train Epoch: 517 [65792/118836 (55%)] Loss: 12248.000000\n",
      "Train Epoch: 517 [98560/118836 (83%)] Loss: 12345.027344\n",
      "    epoch          : 517\n",
      "    loss           : 12283.530912524557\n",
      "    val_loss       : 12288.70792923326\n",
      "    val_log_likelihood: -12191.294975186103\n",
      "    val_log_marginal: -12199.220722255179\n",
      "Train Epoch: 518 [256/118836 (0%)] Loss: 12257.148438\n",
      "Train Epoch: 518 [33024/118836 (28%)] Loss: 12239.847656\n",
      "Train Epoch: 518 [65792/118836 (55%)] Loss: 12323.697266\n",
      "Train Epoch: 518 [98560/118836 (83%)] Loss: 12363.550781\n",
      "    epoch          : 518\n",
      "    loss           : 12287.885586616005\n",
      "    val_loss       : 12280.551140246724\n",
      "    val_log_likelihood: -12191.91753515302\n",
      "    val_log_marginal: -12199.809691846243\n",
      "Train Epoch: 519 [256/118836 (0%)] Loss: 12265.612305\n",
      "Train Epoch: 519 [33024/118836 (28%)] Loss: 12321.877930\n",
      "Train Epoch: 519 [65792/118836 (55%)] Loss: 12309.987305\n",
      "Train Epoch: 519 [98560/118836 (83%)] Loss: 12267.380859\n",
      "    epoch          : 519\n",
      "    loss           : 12287.671497137357\n",
      "    val_loss       : 12286.403397114786\n",
      "    val_log_likelihood: -12192.818000575113\n",
      "    val_log_marginal: -12201.13054805557\n",
      "Train Epoch: 520 [256/118836 (0%)] Loss: 12303.114258\n",
      "Train Epoch: 520 [33024/118836 (28%)] Loss: 12521.785156\n",
      "Train Epoch: 520 [65792/118836 (55%)] Loss: 12313.275391\n",
      "Train Epoch: 520 [98560/118836 (83%)] Loss: 12304.184570\n",
      "    epoch          : 520\n",
      "    loss           : 12289.89235712624\n",
      "    val_loss       : 12288.369481016232\n",
      "    val_log_likelihood: -12191.169877610628\n",
      "    val_log_marginal: -12199.439940894234\n",
      "Train Epoch: 521 [256/118836 (0%)] Loss: 12352.931641\n",
      "Train Epoch: 521 [33024/118836 (28%)] Loss: 12357.892578\n",
      "Train Epoch: 521 [65792/118836 (55%)] Loss: 12386.218750\n",
      "Train Epoch: 521 [98560/118836 (83%)] Loss: 12370.212891\n",
      "    epoch          : 521\n",
      "    loss           : 12288.99836351065\n",
      "    val_loss       : 12283.051455337609\n",
      "    val_log_likelihood: -12188.530775369623\n",
      "    val_log_marginal: -12196.742309118783\n",
      "Train Epoch: 522 [256/118836 (0%)] Loss: 12201.693359\n",
      "Train Epoch: 522 [33024/118836 (28%)] Loss: 12271.395508\n",
      "Train Epoch: 522 [65792/118836 (55%)] Loss: 12260.496094\n",
      "Train Epoch: 522 [98560/118836 (83%)] Loss: 12208.796875\n",
      "    epoch          : 522\n",
      "    loss           : 12289.341785566585\n",
      "    val_loss       : 12283.960114150737\n",
      "    val_log_likelihood: -12191.061819233355\n",
      "    val_log_marginal: -12199.226839403202\n",
      "Train Epoch: 523 [256/118836 (0%)] Loss: 12274.495117\n",
      "Train Epoch: 523 [33024/118836 (28%)] Loss: 12247.372070\n",
      "Train Epoch: 523 [65792/118836 (55%)] Loss: 12218.714844\n",
      "Train Epoch: 523 [98560/118836 (83%)] Loss: 12248.813477\n",
      "    epoch          : 523\n",
      "    loss           : 12284.438299505015\n",
      "    val_loss       : 12288.341291873026\n",
      "    val_log_likelihood: -12191.04308732682\n",
      "    val_log_marginal: -12199.195213089179\n",
      "Train Epoch: 524 [256/118836 (0%)] Loss: 12295.594727\n",
      "Train Epoch: 524 [33024/118836 (28%)] Loss: 12288.533203\n",
      "Train Epoch: 524 [65792/118836 (55%)] Loss: 12329.908203\n",
      "Train Epoch: 524 [98560/118836 (83%)] Loss: 12391.384766\n",
      "    epoch          : 524\n",
      "    loss           : 12284.723364479942\n",
      "    val_loss       : 12284.885097579026\n",
      "    val_log_likelihood: -12191.451672676281\n",
      "    val_log_marginal: -12199.680167296332\n",
      "Train Epoch: 525 [256/118836 (0%)] Loss: 12391.063477\n",
      "Train Epoch: 525 [33024/118836 (28%)] Loss: 12263.355469\n",
      "Train Epoch: 525 [65792/118836 (55%)] Loss: 12258.367188\n",
      "Train Epoch: 525 [98560/118836 (83%)] Loss: 12267.858398\n",
      "    epoch          : 525\n",
      "    loss           : 12280.752522099876\n",
      "    val_loss       : 12288.925996330292\n",
      "    val_log_likelihood: -12194.107682614764\n",
      "    val_log_marginal: -12202.046453021005\n",
      "Train Epoch: 526 [256/118836 (0%)] Loss: 12363.328125\n",
      "Train Epoch: 526 [33024/118836 (28%)] Loss: 12352.300781\n",
      "Train Epoch: 526 [65792/118836 (55%)] Loss: 12255.954102\n",
      "Train Epoch: 526 [98560/118836 (83%)] Loss: 12283.577148\n",
      "    epoch          : 526\n",
      "    loss           : 12291.411207286497\n",
      "    val_loss       : 12286.807538621992\n",
      "    val_log_likelihood: -12193.38262655733\n",
      "    val_log_marginal: -12201.625720054828\n",
      "Train Epoch: 527 [256/118836 (0%)] Loss: 12257.470703\n",
      "Train Epoch: 527 [33024/118836 (28%)] Loss: 12230.187500\n",
      "Train Epoch: 527 [65792/118836 (55%)] Loss: 12293.523438\n",
      "Train Epoch: 527 [98560/118836 (83%)] Loss: 12353.399414\n",
      "    epoch          : 527\n",
      "    loss           : 12284.374784978547\n",
      "    val_loss       : 12282.426297305296\n",
      "    val_log_likelihood: -12191.379770536083\n",
      "    val_log_marginal: -12199.436213207391\n",
      "Train Epoch: 528 [256/118836 (0%)] Loss: 12261.613281\n",
      "Train Epoch: 528 [33024/118836 (28%)] Loss: 12246.107422\n",
      "Train Epoch: 528 [65792/118836 (55%)] Loss: 12360.176758\n",
      "Train Epoch: 528 [98560/118836 (83%)] Loss: 12376.828125\n",
      "    epoch          : 528\n",
      "    loss           : 12287.550817275382\n",
      "    val_loss       : 12287.792966710878\n",
      "    val_log_likelihood: -12189.28400876887\n",
      "    val_log_marginal: -12197.270104872992\n",
      "Train Epoch: 529 [256/118836 (0%)] Loss: 12391.753906\n",
      "Train Epoch: 529 [33024/118836 (28%)] Loss: 12328.550781\n",
      "Train Epoch: 529 [65792/118836 (55%)] Loss: 12261.773438\n",
      "Train Epoch: 529 [98560/118836 (83%)] Loss: 12299.705078\n",
      "    epoch          : 529\n",
      "    loss           : 12279.67686023444\n",
      "    val_loss       : 12288.141881382406\n",
      "    val_log_likelihood: -12192.166705923024\n",
      "    val_log_marginal: -12200.297073935257\n",
      "Train Epoch: 530 [256/118836 (0%)] Loss: 12273.414062\n",
      "Train Epoch: 530 [33024/118836 (28%)] Loss: 12278.873047\n",
      "Train Epoch: 530 [65792/118836 (55%)] Loss: 12315.531250\n",
      "Train Epoch: 530 [98560/118836 (83%)] Loss: 12272.799805\n",
      "    epoch          : 530\n",
      "    loss           : 12287.677087048956\n",
      "    val_loss       : 12287.306836332571\n",
      "    val_log_likelihood: -12192.297026532775\n",
      "    val_log_marginal: -12200.427650502888\n",
      "Train Epoch: 531 [256/118836 (0%)] Loss: 12376.020508\n",
      "Train Epoch: 531 [33024/118836 (28%)] Loss: 12332.274414\n",
      "Train Epoch: 531 [65792/118836 (55%)] Loss: 12256.660156\n",
      "Train Epoch: 531 [98560/118836 (83%)] Loss: 12385.621094\n",
      "    epoch          : 531\n",
      "    loss           : 12289.001321146092\n",
      "    val_loss       : 12286.574035880707\n",
      "    val_log_likelihood: -12191.566578784119\n",
      "    val_log_marginal: -12199.598422389266\n",
      "Train Epoch: 532 [256/118836 (0%)] Loss: 12426.750977\n",
      "Train Epoch: 532 [33024/118836 (28%)] Loss: 12254.728516\n",
      "Train Epoch: 532 [65792/118836 (55%)] Loss: 12315.560547\n",
      "Train Epoch: 532 [98560/118836 (83%)] Loss: 12256.578125\n",
      "    epoch          : 532\n",
      "    loss           : 12283.453232268404\n",
      "    val_loss       : 12290.234118099965\n",
      "    val_log_likelihood: -12191.794277779933\n",
      "    val_log_marginal: -12200.161311331947\n",
      "Train Epoch: 533 [256/118836 (0%)] Loss: 12327.293945\n",
      "Train Epoch: 533 [33024/118836 (28%)] Loss: 12479.390625\n",
      "Train Epoch: 533 [65792/118836 (55%)] Loss: 12241.556641\n",
      "Train Epoch: 533 [98560/118836 (83%)] Loss: 12205.596680\n",
      "    epoch          : 533\n",
      "    loss           : 12289.122057873243\n",
      "    val_loss       : 12284.342622773294\n",
      "    val_log_likelihood: -12190.18922970301\n",
      "    val_log_marginal: -12198.28833247593\n",
      "Train Epoch: 534 [256/118836 (0%)] Loss: 12255.613281\n",
      "Train Epoch: 534 [33024/118836 (28%)] Loss: 12301.859375\n",
      "Train Epoch: 534 [65792/118836 (55%)] Loss: 12252.041016\n",
      "Train Epoch: 534 [98560/118836 (83%)] Loss: 12272.984375\n",
      "    epoch          : 534\n",
      "    loss           : 12284.20717648237\n",
      "    val_loss       : 12284.194929020516\n",
      "    val_log_likelihood: -12190.21870670492\n",
      "    val_log_marginal: -12198.099131634697\n",
      "Train Epoch: 535 [256/118836 (0%)] Loss: 12343.292969\n",
      "Train Epoch: 535 [33024/118836 (28%)] Loss: 12236.247070\n",
      "Train Epoch: 535 [65792/118836 (55%)] Loss: 12354.898438\n",
      "Train Epoch: 535 [98560/118836 (83%)] Loss: 12268.968750\n",
      "    epoch          : 535\n",
      "    loss           : 12283.361103087522\n",
      "    val_loss       : 12282.531146457475\n",
      "    val_log_likelihood: -12190.089791892835\n",
      "    val_log_marginal: -12198.02940212792\n",
      "Train Epoch: 536 [256/118836 (0%)] Loss: 12393.149414\n",
      "Train Epoch: 536 [33024/118836 (28%)] Loss: 12306.406250\n",
      "Train Epoch: 536 [65792/118836 (55%)] Loss: 12271.666016\n",
      "Train Epoch: 536 [98560/118836 (83%)] Loss: 12269.095703\n",
      "    epoch          : 536\n",
      "    loss           : 12285.103529841295\n",
      "    val_loss       : 12284.638389003556\n",
      "    val_log_likelihood: -12194.482437868332\n",
      "    val_log_marginal: -12202.744858932832\n",
      "Train Epoch: 537 [256/118836 (0%)] Loss: 12431.937500\n",
      "Train Epoch: 537 [33024/118836 (28%)] Loss: 12336.321289\n",
      "Train Epoch: 537 [65792/118836 (55%)] Loss: 12204.984375\n",
      "Train Epoch: 537 [98560/118836 (83%)] Loss: 12319.350586\n",
      "    epoch          : 537\n",
      "    loss           : 12283.645173406483\n",
      "    val_loss       : 12281.649702264596\n",
      "    val_log_likelihood: -12191.147131862594\n",
      "    val_log_marginal: -12199.27260042881\n",
      "Train Epoch: 538 [256/118836 (0%)] Loss: 12297.916992\n",
      "Train Epoch: 538 [33024/118836 (28%)] Loss: 12299.243164\n",
      "Train Epoch: 538 [65792/118836 (55%)] Loss: 12306.669922\n",
      "Train Epoch: 538 [98560/118836 (83%)] Loss: 12394.498047\n",
      "    epoch          : 538\n",
      "    loss           : 12289.320454339848\n",
      "    val_loss       : 12283.012928843682\n",
      "    val_log_likelihood: -12190.398134919096\n",
      "    val_log_marginal: -12198.415757820447\n",
      "Train Epoch: 539 [256/118836 (0%)] Loss: 12325.346680\n",
      "Train Epoch: 539 [33024/118836 (28%)] Loss: 12241.427734\n",
      "Train Epoch: 539 [65792/118836 (55%)] Loss: 12369.787109\n",
      "Train Epoch: 539 [98560/118836 (83%)] Loss: 12226.851562\n",
      "    epoch          : 539\n",
      "    loss           : 12287.309812474152\n",
      "    val_loss       : 12283.392155464606\n",
      "    val_log_likelihood: -12189.086713095792\n",
      "    val_log_marginal: -12197.149342089146\n",
      "Train Epoch: 540 [256/118836 (0%)] Loss: 12233.500000\n",
      "Train Epoch: 540 [33024/118836 (28%)] Loss: 12274.876953\n",
      "Train Epoch: 540 [65792/118836 (55%)] Loss: 12370.016602\n",
      "Train Epoch: 540 [98560/118836 (83%)] Loss: 12361.166016\n",
      "    epoch          : 540\n",
      "    loss           : 12286.69596580335\n",
      "    val_loss       : 12286.1993117641\n",
      "    val_log_likelihood: -12191.860195991005\n",
      "    val_log_marginal: -12199.786228767522\n",
      "Train Epoch: 541 [256/118836 (0%)] Loss: 12348.938477\n",
      "Train Epoch: 541 [33024/118836 (28%)] Loss: 12244.250977\n",
      "Train Epoch: 541 [65792/118836 (55%)] Loss: 12275.934570\n",
      "Train Epoch: 541 [98560/118836 (83%)] Loss: 12210.100586\n",
      "    epoch          : 541\n",
      "    loss           : 12288.300594661134\n",
      "    val_loss       : 12285.755711364147\n",
      "    val_log_likelihood: -12191.247561743952\n",
      "    val_log_marginal: -12199.4791634612\n",
      "Train Epoch: 542 [256/118836 (0%)] Loss: 12258.300781\n",
      "Train Epoch: 542 [33024/118836 (28%)] Loss: 12319.438477\n",
      "Train Epoch: 542 [65792/118836 (55%)] Loss: 12224.246094\n",
      "Train Epoch: 542 [98560/118836 (83%)] Loss: 12316.099609\n",
      "    epoch          : 542\n",
      "    loss           : 12284.943447257549\n",
      "    val_loss       : 12286.245653683858\n",
      "    val_log_likelihood: -12191.27579061983\n",
      "    val_log_marginal: -12199.021675296854\n",
      "Train Epoch: 543 [256/118836 (0%)] Loss: 12215.801758\n",
      "Train Epoch: 543 [33024/118836 (28%)] Loss: 12302.482422\n",
      "Train Epoch: 543 [65792/118836 (55%)] Loss: 12321.654297\n",
      "Train Epoch: 543 [98560/118836 (83%)] Loss: 12207.051758\n",
      "    epoch          : 543\n",
      "    loss           : 12283.144635287428\n",
      "    val_loss       : 12287.407243446094\n",
      "    val_log_likelihood: -12191.888497886941\n",
      "    val_log_marginal: -12199.908592174052\n",
      "Train Epoch: 544 [256/118836 (0%)] Loss: 12260.716797\n",
      "Train Epoch: 544 [33024/118836 (28%)] Loss: 12412.968750\n",
      "Train Epoch: 544 [65792/118836 (55%)] Loss: 12317.524414\n",
      "Train Epoch: 544 [98560/118836 (83%)] Loss: 12348.869141\n",
      "    epoch          : 544\n",
      "    loss           : 12288.862584651572\n",
      "    val_loss       : 12283.676708842386\n",
      "    val_log_likelihood: -12190.290247783549\n",
      "    val_log_marginal: -12198.171457191651\n",
      "Train Epoch: 545 [256/118836 (0%)] Loss: 12419.131836\n",
      "Train Epoch: 545 [33024/118836 (28%)] Loss: 12325.852539\n",
      "Train Epoch: 545 [65792/118836 (55%)] Loss: 12294.625000\n",
      "Train Epoch: 545 [98560/118836 (83%)] Loss: 12221.587891\n",
      "    epoch          : 545\n",
      "    loss           : 12287.59732232863\n",
      "    val_loss       : 12291.025031629757\n",
      "    val_log_likelihood: -12197.186449932795\n",
      "    val_log_marginal: -12205.555653440608\n",
      "Train Epoch: 546 [256/118836 (0%)] Loss: 12256.347656\n",
      "Train Epoch: 546 [33024/118836 (28%)] Loss: 12256.486328\n",
      "Train Epoch: 546 [65792/118836 (55%)] Loss: 12230.722656\n",
      "Train Epoch: 546 [98560/118836 (83%)] Loss: 12416.647461\n",
      "    epoch          : 546\n",
      "    loss           : 12288.249554609956\n",
      "    val_loss       : 12285.703139212252\n",
      "    val_log_likelihood: -12186.505088625672\n",
      "    val_log_marginal: -12194.535532667718\n",
      "Train Epoch: 547 [256/118836 (0%)] Loss: 12248.272461\n",
      "Train Epoch: 547 [33024/118836 (28%)] Loss: 12270.869141\n",
      "Train Epoch: 547 [65792/118836 (55%)] Loss: 12214.969727\n",
      "Train Epoch: 547 [98560/118836 (83%)] Loss: 12361.500000\n",
      "    epoch          : 547\n",
      "    loss           : 12282.199890308364\n",
      "    val_loss       : 12282.09008228911\n",
      "    val_log_likelihood: -12189.975223745088\n",
      "    val_log_marginal: -12198.059196090057\n",
      "Train Epoch: 548 [256/118836 (0%)] Loss: 12222.212891\n",
      "Train Epoch: 548 [33024/118836 (28%)] Loss: 12344.912109\n",
      "Train Epoch: 548 [65792/118836 (55%)] Loss: 12245.027344\n",
      "Train Epoch: 548 [98560/118836 (83%)] Loss: 12319.099609\n",
      "    epoch          : 548\n",
      "    loss           : 12284.177939380428\n",
      "    val_loss       : 12284.959359651051\n",
      "    val_log_likelihood: -12187.449811149452\n",
      "    val_log_marginal: -12195.522050469228\n",
      "Train Epoch: 549 [256/118836 (0%)] Loss: 12283.388672\n",
      "Train Epoch: 549 [33024/118836 (28%)] Loss: 12297.821289\n",
      "Train Epoch: 549 [65792/118836 (55%)] Loss: 12452.243164\n",
      "Train Epoch: 549 [98560/118836 (83%)] Loss: 12365.303711\n",
      "    epoch          : 549\n",
      "    loss           : 12289.261686117143\n",
      "    val_loss       : 12282.277007874813\n",
      "    val_log_likelihood: -12189.410946546734\n",
      "    val_log_marginal: -12197.429149314934\n",
      "Train Epoch: 550 [256/118836 (0%)] Loss: 12255.475586\n",
      "Train Epoch: 550 [33024/118836 (28%)] Loss: 12266.195312\n",
      "Train Epoch: 550 [65792/118836 (55%)] Loss: 12329.350586\n",
      "Train Epoch: 550 [98560/118836 (83%)] Loss: 12373.342773\n",
      "    epoch          : 550\n",
      "    loss           : 12279.556058564673\n",
      "    val_loss       : 12285.163805415155\n",
      "    val_log_likelihood: -12187.369449506308\n",
      "    val_log_marginal: -12195.630681193936\n",
      "Train Epoch: 551 [256/118836 (0%)] Loss: 12162.291016\n",
      "Train Epoch: 551 [33024/118836 (28%)] Loss: 12213.040039\n",
      "Train Epoch: 551 [65792/118836 (55%)] Loss: 12290.326172\n",
      "Train Epoch: 551 [98560/118836 (83%)] Loss: 12188.728516\n",
      "    epoch          : 551\n",
      "    loss           : 12283.737510500672\n",
      "    val_loss       : 12282.39587585159\n",
      "    val_log_likelihood: -12188.974471412324\n",
      "    val_log_marginal: -12197.015547396024\n",
      "Train Epoch: 552 [256/118836 (0%)] Loss: 12312.772461\n",
      "Train Epoch: 552 [33024/118836 (28%)] Loss: 12355.625977\n",
      "Train Epoch: 552 [65792/118836 (55%)] Loss: 12319.978516\n",
      "Train Epoch: 552 [98560/118836 (83%)] Loss: 12448.631836\n",
      "    epoch          : 552\n",
      "    loss           : 12287.181465990747\n",
      "    val_loss       : 12284.50767577668\n",
      "    val_log_likelihood: -12187.723381119467\n",
      "    val_log_marginal: -12195.731062449164\n",
      "Train Epoch: 553 [256/118836 (0%)] Loss: 12330.149414\n",
      "Train Epoch: 553 [33024/118836 (28%)] Loss: 12369.373047\n",
      "Train Epoch: 553 [65792/118836 (55%)] Loss: 12317.080078\n",
      "Train Epoch: 553 [98560/118836 (83%)] Loss: 12293.927734\n",
      "    epoch          : 553\n",
      "    loss           : 12282.59666078629\n",
      "    val_loss       : 12283.435759228561\n",
      "    val_log_likelihood: -12189.343512684813\n",
      "    val_log_marginal: -12197.417993786572\n",
      "Train Epoch: 554 [256/118836 (0%)] Loss: 12216.234375\n",
      "Train Epoch: 554 [33024/118836 (28%)] Loss: 12393.521484\n",
      "Train Epoch: 554 [65792/118836 (55%)] Loss: 12280.780273\n",
      "Train Epoch: 554 [98560/118836 (83%)] Loss: 12245.871094\n",
      "    epoch          : 554\n",
      "    loss           : 12282.081749508892\n",
      "    val_loss       : 12289.675788089042\n",
      "    val_log_likelihood: -12189.94544852409\n",
      "    val_log_marginal: -12198.064623904296\n",
      "Train Epoch: 555 [256/118836 (0%)] Loss: 12372.426758\n",
      "Train Epoch: 555 [33024/118836 (28%)] Loss: 12266.792969\n",
      "Train Epoch: 555 [65792/118836 (55%)] Loss: 12293.507812\n",
      "Train Epoch: 555 [98560/118836 (83%)] Loss: 12434.740234\n",
      "    epoch          : 555\n",
      "    loss           : 12287.66109000207\n",
      "    val_loss       : 12283.988688342813\n",
      "    val_log_likelihood: -12188.19460394696\n",
      "    val_log_marginal: -12196.315276429956\n",
      "Train Epoch: 556 [256/118836 (0%)] Loss: 12340.820312\n",
      "Train Epoch: 556 [33024/118836 (28%)] Loss: 12316.190430\n",
      "Train Epoch: 556 [65792/118836 (55%)] Loss: 12320.491211\n",
      "Train Epoch: 556 [98560/118836 (83%)] Loss: 12216.863281\n",
      "    epoch          : 556\n",
      "    loss           : 12284.601410159481\n",
      "    val_loss       : 12285.194017274856\n",
      "    val_log_likelihood: -12185.012081427058\n",
      "    val_log_marginal: -12192.804756226342\n",
      "Train Epoch: 557 [256/118836 (0%)] Loss: 12372.111328\n",
      "Train Epoch: 557 [33024/118836 (28%)] Loss: 12369.881836\n",
      "Train Epoch: 557 [65792/118836 (55%)] Loss: 12267.824219\n",
      "Train Epoch: 557 [98560/118836 (83%)] Loss: 12318.064453\n",
      "    epoch          : 557\n",
      "    loss           : 12284.222858832196\n",
      "    val_loss       : 12283.372850022732\n",
      "    val_log_likelihood: -12189.457323007133\n",
      "    val_log_marginal: -12197.47707362431\n",
      "Train Epoch: 558 [256/118836 (0%)] Loss: 12318.076172\n",
      "Train Epoch: 558 [33024/118836 (28%)] Loss: 12234.508789\n",
      "Train Epoch: 558 [65792/118836 (55%)] Loss: 12281.765625\n",
      "Train Epoch: 558 [98560/118836 (83%)] Loss: 12208.570312\n",
      "    epoch          : 558\n",
      "    loss           : 12286.797902127275\n",
      "    val_loss       : 12283.198567615766\n",
      "    val_log_likelihood: -12186.765001583177\n",
      "    val_log_marginal: -12194.854738926027\n",
      "Train Epoch: 559 [256/118836 (0%)] Loss: 12252.000000\n",
      "Train Epoch: 559 [33024/118836 (28%)] Loss: 12250.962891\n",
      "Train Epoch: 559 [65792/118836 (55%)] Loss: 12382.029297\n",
      "Train Epoch: 559 [98560/118836 (83%)] Loss: 12368.139648\n",
      "    epoch          : 559\n",
      "    loss           : 12291.66078935975\n",
      "    val_loss       : 12287.652948908872\n",
      "    val_log_likelihood: -12189.499253321443\n",
      "    val_log_marginal: -12197.59256445888\n",
      "Train Epoch: 560 [256/118836 (0%)] Loss: 12297.337891\n",
      "Train Epoch: 560 [33024/118836 (28%)] Loss: 12359.483398\n",
      "Train Epoch: 560 [65792/118836 (55%)] Loss: 12414.226562\n",
      "Train Epoch: 560 [98560/118836 (83%)] Loss: 12284.499023\n",
      "    epoch          : 560\n",
      "    loss           : 12284.70387927135\n",
      "    val_loss       : 12288.757998431758\n",
      "    val_log_likelihood: -12188.611745244005\n",
      "    val_log_marginal: -12196.630896151992\n",
      "Train Epoch: 561 [256/118836 (0%)] Loss: 12231.750000\n",
      "Train Epoch: 561 [33024/118836 (28%)] Loss: 12273.729492\n",
      "Train Epoch: 561 [65792/118836 (55%)] Loss: 12294.078125\n",
      "Train Epoch: 561 [98560/118836 (83%)] Loss: 12281.061523\n",
      "    epoch          : 561\n",
      "    loss           : 12283.916796390355\n",
      "    val_loss       : 12283.995880986291\n",
      "    val_log_likelihood: -12188.915116929022\n",
      "    val_log_marginal: -12196.978430067884\n",
      "Train Epoch: 562 [256/118836 (0%)] Loss: 12296.110352\n",
      "Train Epoch: 562 [33024/118836 (28%)] Loss: 12301.635742\n",
      "Train Epoch: 562 [65792/118836 (55%)] Loss: 12217.927734\n",
      "Train Epoch: 562 [98560/118836 (83%)] Loss: 12231.208984\n",
      "    epoch          : 562\n",
      "    loss           : 12283.598002125982\n",
      "    val_loss       : 12281.973188480639\n",
      "    val_log_likelihood: -12188.010905674888\n",
      "    val_log_marginal: -12196.035438761914\n",
      "Train Epoch: 563 [256/118836 (0%)] Loss: 12296.929688\n",
      "Train Epoch: 563 [33024/118836 (28%)] Loss: 12301.446289\n",
      "Train Epoch: 563 [65792/118836 (55%)] Loss: 12323.278320\n",
      "Train Epoch: 563 [98560/118836 (83%)] Loss: 12287.713867\n",
      "    epoch          : 563\n",
      "    loss           : 12279.49428876525\n",
      "    val_loss       : 12285.819978309333\n",
      "    val_log_likelihood: -12188.035023133789\n",
      "    val_log_marginal: -12196.185049261905\n",
      "Train Epoch: 564 [256/118836 (0%)] Loss: 12232.527344\n",
      "Train Epoch: 564 [33024/118836 (28%)] Loss: 12268.983398\n",
      "Train Epoch: 564 [65792/118836 (55%)] Loss: 12272.163086\n",
      "Train Epoch: 564 [98560/118836 (83%)] Loss: 12280.085938\n",
      "    epoch          : 564\n",
      "    loss           : 12281.225862347499\n",
      "    val_loss       : 12284.257589436356\n",
      "    val_log_likelihood: -12187.824922941223\n",
      "    val_log_marginal: -12196.045195052096\n",
      "Train Epoch: 565 [256/118836 (0%)] Loss: 12242.514648\n",
      "Train Epoch: 565 [33024/118836 (28%)] Loss: 12274.055664\n",
      "Train Epoch: 565 [65792/118836 (55%)] Loss: 12398.095703\n",
      "Train Epoch: 565 [98560/118836 (83%)] Loss: 12387.876953\n",
      "    epoch          : 565\n",
      "    loss           : 12285.242748558983\n",
      "    val_loss       : 12287.82909108111\n",
      "    val_log_likelihood: -12186.868887962675\n",
      "    val_log_marginal: -12194.840789524027\n",
      "Train Epoch: 566 [256/118836 (0%)] Loss: 12240.023438\n",
      "Train Epoch: 566 [33024/118836 (28%)] Loss: 12352.983398\n",
      "Train Epoch: 566 [65792/118836 (55%)] Loss: 12286.142578\n",
      "Train Epoch: 566 [98560/118836 (83%)] Loss: 12446.208008\n",
      "    epoch          : 566\n",
      "    loss           : 12288.385955108819\n",
      "    val_loss       : 12277.348600422441\n",
      "    val_log_likelihood: -12183.730536277399\n",
      "    val_log_marginal: -12191.678563325255\n",
      "Train Epoch: 567 [256/118836 (0%)] Loss: 12191.001953\n",
      "Train Epoch: 567 [33024/118836 (28%)] Loss: 12349.203125\n",
      "Train Epoch: 567 [65792/118836 (55%)] Loss: 12272.391602\n",
      "Train Epoch: 567 [98560/118836 (83%)] Loss: 12326.166992\n",
      "    epoch          : 567\n",
      "    loss           : 12292.8971375168\n",
      "    val_loss       : 12286.166565348394\n",
      "    val_log_likelihood: -12185.916868925766\n",
      "    val_log_marginal: -12193.926415930282\n",
      "Train Epoch: 568 [256/118836 (0%)] Loss: 12226.457031\n",
      "Train Epoch: 568 [33024/118836 (28%)] Loss: 12275.769531\n",
      "Train Epoch: 568 [65792/118836 (55%)] Loss: 12311.306641\n",
      "Train Epoch: 568 [98560/118836 (83%)] Loss: 12378.972656\n",
      "    epoch          : 568\n",
      "    loss           : 12278.51974110189\n",
      "    val_loss       : 12284.31627677407\n",
      "    val_log_likelihood: -12187.60360156896\n",
      "    val_log_marginal: -12195.612093139074\n",
      "Train Epoch: 569 [256/118836 (0%)] Loss: 12291.237305\n",
      "Train Epoch: 569 [33024/118836 (28%)] Loss: 12217.486328\n",
      "Train Epoch: 569 [65792/118836 (55%)] Loss: 12324.922852\n",
      "Train Epoch: 569 [98560/118836 (83%)] Loss: 12239.420898\n",
      "    epoch          : 569\n",
      "    loss           : 12285.982599417131\n",
      "    val_loss       : 12285.722447578139\n",
      "    val_log_likelihood: -12189.40372305366\n",
      "    val_log_marginal: -12197.24099673545\n",
      "Train Epoch: 570 [256/118836 (0%)] Loss: 12264.837891\n",
      "Train Epoch: 570 [33024/118836 (28%)] Loss: 12395.613281\n",
      "Train Epoch: 570 [65792/118836 (55%)] Loss: 12338.907227\n",
      "Train Epoch: 570 [98560/118836 (83%)] Loss: 12328.291992\n",
      "    epoch          : 570\n",
      "    loss           : 12289.69148314723\n",
      "    val_loss       : 12279.500264133063\n",
      "    val_log_likelihood: -12187.040004814155\n",
      "    val_log_marginal: -12195.241526981878\n",
      "Train Epoch: 571 [256/118836 (0%)] Loss: 12300.210938\n",
      "Train Epoch: 571 [33024/118836 (28%)] Loss: 12450.374023\n",
      "Train Epoch: 571 [65792/118836 (55%)] Loss: 12215.943359\n",
      "Train Epoch: 571 [98560/118836 (83%)] Loss: 12256.835938\n",
      "    epoch          : 571\n",
      "    loss           : 12281.966712546528\n",
      "    val_loss       : 12278.111045802685\n",
      "    val_log_likelihood: -12189.601187383683\n",
      "    val_log_marginal: -12197.55326812813\n",
      "Train Epoch: 572 [256/118836 (0%)] Loss: 12283.194336\n",
      "Train Epoch: 572 [33024/118836 (28%)] Loss: 12332.012695\n",
      "Train Epoch: 572 [65792/118836 (55%)] Loss: 12313.601562\n",
      "Train Epoch: 572 [98560/118836 (83%)] Loss: 12369.917969\n",
      "    epoch          : 572\n",
      "    loss           : 12284.9392780061\n",
      "    val_loss       : 12281.59479319479\n",
      "    val_log_likelihood: -12188.494566629188\n",
      "    val_log_marginal: -12196.663659518177\n",
      "Train Epoch: 573 [256/118836 (0%)] Loss: 12366.031250\n",
      "Train Epoch: 573 [33024/118836 (28%)] Loss: 12265.253906\n",
      "Train Epoch: 573 [65792/118836 (55%)] Loss: 12447.319336\n",
      "Train Epoch: 573 [98560/118836 (83%)] Loss: 12196.453125\n",
      "    epoch          : 573\n",
      "    loss           : 12281.094142725135\n",
      "    val_loss       : 12282.582173985127\n",
      "    val_log_likelihood: -12184.759852053608\n",
      "    val_log_marginal: -12192.838277489825\n",
      "Train Epoch: 574 [256/118836 (0%)] Loss: 12304.133789\n",
      "Train Epoch: 574 [33024/118836 (28%)] Loss: 12319.312500\n",
      "Train Epoch: 574 [65792/118836 (55%)] Loss: 12358.875000\n",
      "Train Epoch: 574 [98560/118836 (83%)] Loss: 12374.601562\n",
      "    epoch          : 574\n",
      "    loss           : 12292.200805966966\n",
      "    val_loss       : 12292.877611616543\n",
      "    val_log_likelihood: -12187.185661736197\n",
      "    val_log_marginal: -12195.524625850578\n",
      "Train Epoch: 575 [256/118836 (0%)] Loss: 12369.007812\n",
      "Train Epoch: 575 [33024/118836 (28%)] Loss: 12311.985352\n",
      "Train Epoch: 575 [65792/118836 (55%)] Loss: 12288.188477\n",
      "Train Epoch: 575 [98560/118836 (83%)] Loss: 12350.798828\n",
      "    epoch          : 575\n",
      "    loss           : 12281.78315999147\n",
      "    val_loss       : 12280.322800421203\n",
      "    val_log_likelihood: -12185.67622437836\n",
      "    val_log_marginal: -12193.771035913183\n",
      "Train Epoch: 576 [256/118836 (0%)] Loss: 12242.236328\n",
      "Train Epoch: 576 [33024/118836 (28%)] Loss: 12242.195312\n",
      "Train Epoch: 576 [65792/118836 (55%)] Loss: 12381.915039\n",
      "Train Epoch: 576 [98560/118836 (83%)] Loss: 12302.542969\n",
      "    epoch          : 576\n",
      "    loss           : 12278.888993518662\n",
      "    val_loss       : 12281.675770336698\n",
      "    val_log_likelihood: -12182.70853591553\n",
      "    val_log_marginal: -12190.674007940503\n",
      "Train Epoch: 577 [256/118836 (0%)] Loss: 12323.421875\n",
      "Train Epoch: 577 [33024/118836 (28%)] Loss: 12299.897461\n",
      "Train Epoch: 577 [65792/118836 (55%)] Loss: 12202.807617\n",
      "Train Epoch: 577 [98560/118836 (83%)] Loss: 12268.489258\n",
      "    epoch          : 577\n",
      "    loss           : 12283.59981389578\n",
      "    val_loss       : 12281.52592023869\n",
      "    val_log_likelihood: -12186.007102169924\n",
      "    val_log_marginal: -12193.936949809846\n",
      "Train Epoch: 578 [256/118836 (0%)] Loss: 12273.154297\n",
      "Train Epoch: 578 [33024/118836 (28%)] Loss: 12322.976562\n",
      "Train Epoch: 578 [65792/118836 (55%)] Loss: 12293.061523\n",
      "Train Epoch: 578 [98560/118836 (83%)] Loss: 12238.194336\n",
      "    epoch          : 578\n",
      "    loss           : 12282.348977396092\n",
      "    val_loss       : 12283.757835655806\n",
      "    val_log_likelihood: -12186.361215687035\n",
      "    val_log_marginal: -12194.343429076078\n",
      "Train Epoch: 579 [256/118836 (0%)] Loss: 12276.884766\n",
      "Train Epoch: 579 [33024/118836 (28%)] Loss: 12308.330078\n",
      "Train Epoch: 579 [65792/118836 (55%)] Loss: 12316.689453\n",
      "Train Epoch: 579 [98560/118836 (83%)] Loss: 12282.989258\n",
      "    epoch          : 579\n",
      "    loss           : 12283.327767654053\n",
      "    val_loss       : 12283.85214968478\n",
      "    val_log_likelihood: -12188.875983670647\n",
      "    val_log_marginal: -12196.900293420695\n",
      "Train Epoch: 580 [256/118836 (0%)] Loss: 12351.013672\n",
      "Train Epoch: 580 [33024/118836 (28%)] Loss: 12204.382812\n",
      "Train Epoch: 580 [65792/118836 (55%)] Loss: 12237.991211\n",
      "Train Epoch: 580 [98560/118836 (83%)] Loss: 12286.739258\n",
      "    epoch          : 580\n",
      "    loss           : 12283.392700417442\n",
      "    val_loss       : 12281.021942830035\n",
      "    val_log_likelihood: -12188.510241063119\n",
      "    val_log_marginal: -12196.429238502806\n",
      "Train Epoch: 581 [256/118836 (0%)] Loss: 12201.984375\n",
      "Train Epoch: 581 [33024/118836 (28%)] Loss: 12306.804688\n",
      "Train Epoch: 581 [65792/118836 (55%)] Loss: 12293.267578\n",
      "Train Epoch: 581 [98560/118836 (83%)] Loss: 12259.884766\n",
      "    epoch          : 581\n",
      "    loss           : 12284.907710562706\n",
      "    val_loss       : 12286.355417106684\n",
      "    val_log_likelihood: -12185.88181719784\n",
      "    val_log_marginal: -12194.066126416617\n",
      "Train Epoch: 582 [256/118836 (0%)] Loss: 12322.424805\n",
      "Train Epoch: 582 [33024/118836 (28%)] Loss: 12361.537109\n",
      "Train Epoch: 582 [65792/118836 (55%)] Loss: 12276.177734\n",
      "Train Epoch: 582 [98560/118836 (83%)] Loss: 12347.076172\n",
      "    epoch          : 582\n",
      "    loss           : 12280.557218323513\n",
      "    val_loss       : 12282.551103279879\n",
      "    val_log_likelihood: -12185.704086538462\n",
      "    val_log_marginal: -12193.710624818397\n",
      "Train Epoch: 583 [256/118836 (0%)] Loss: 12343.986328\n",
      "Train Epoch: 583 [33024/118836 (28%)] Loss: 12378.247070\n",
      "Train Epoch: 583 [65792/118836 (55%)] Loss: 12362.375000\n",
      "Train Epoch: 583 [98560/118836 (83%)] Loss: 12365.917969\n",
      "    epoch          : 583\n",
      "    loss           : 12279.373402928557\n",
      "    val_loss       : 12288.497911220564\n",
      "    val_log_likelihood: -12190.511399044924\n",
      "    val_log_marginal: -12198.662134414955\n",
      "Train Epoch: 584 [256/118836 (0%)] Loss: 12306.039062\n",
      "Train Epoch: 584 [33024/118836 (28%)] Loss: 12290.216797\n",
      "Train Epoch: 584 [65792/118836 (55%)] Loss: 12245.847656\n",
      "Train Epoch: 584 [98560/118836 (83%)] Loss: 12232.210938\n",
      "    epoch          : 584\n",
      "    loss           : 12277.685943315757\n",
      "    val_loss       : 12282.444314117147\n",
      "    val_log_likelihood: -12190.744134324597\n",
      "    val_log_marginal: -12198.900751441768\n",
      "Train Epoch: 585 [256/118836 (0%)] Loss: 12413.065430\n",
      "Train Epoch: 585 [33024/118836 (28%)] Loss: 12321.290039\n",
      "Train Epoch: 585 [65792/118836 (55%)] Loss: 12270.734375\n",
      "Train Epoch: 585 [98560/118836 (83%)] Loss: 12376.117188\n",
      "    epoch          : 585\n",
      "    loss           : 12277.866724662686\n",
      "    val_loss       : 12281.569711710648\n",
      "    val_log_likelihood: -12189.001868796528\n",
      "    val_log_marginal: -12196.981385115007\n",
      "Train Epoch: 586 [256/118836 (0%)] Loss: 12360.703125\n",
      "Train Epoch: 586 [33024/118836 (28%)] Loss: 12303.871094\n",
      "Train Epoch: 586 [65792/118836 (55%)] Loss: 12324.033203\n",
      "Train Epoch: 586 [98560/118836 (83%)] Loss: 12387.345703\n",
      "    epoch          : 586\n",
      "    loss           : 12278.15934139785\n",
      "    val_loss       : 12285.610082698813\n",
      "    val_log_likelihood: -12186.827566041151\n",
      "    val_log_marginal: -12194.9456641393\n",
      "Train Epoch: 587 [256/118836 (0%)] Loss: 12384.957031\n",
      "Train Epoch: 587 [33024/118836 (28%)] Loss: 12304.890625\n",
      "Train Epoch: 587 [65792/118836 (55%)] Loss: 12306.749023\n",
      "Train Epoch: 587 [98560/118836 (83%)] Loss: 12267.274414\n",
      "    epoch          : 587\n",
      "    loss           : 12277.448251234233\n",
      "    val_loss       : 12281.150589243785\n",
      "    val_log_likelihood: -12186.348054144695\n",
      "    val_log_marginal: -12194.18184407979\n",
      "Train Epoch: 588 [256/118836 (0%)] Loss: 12265.347656\n",
      "Train Epoch: 588 [33024/118836 (28%)] Loss: 12233.259766\n",
      "Train Epoch: 588 [65792/118836 (55%)] Loss: 12222.585938\n",
      "Train Epoch: 588 [98560/118836 (83%)] Loss: 12335.454102\n",
      "    epoch          : 588\n",
      "    loss           : 12280.430476504342\n",
      "    val_loss       : 12283.443882093508\n",
      "    val_log_likelihood: -12185.651994481492\n",
      "    val_log_marginal: -12193.521831034532\n",
      "Train Epoch: 589 [256/118836 (0%)] Loss: 12207.054688\n",
      "Train Epoch: 589 [33024/118836 (28%)] Loss: 12279.730469\n",
      "Train Epoch: 589 [65792/118836 (55%)] Loss: 12310.407227\n",
      "Train Epoch: 589 [98560/118836 (83%)] Loss: 12274.576172\n",
      "    epoch          : 589\n",
      "    loss           : 12287.18210281612\n",
      "    val_loss       : 12282.126504841179\n",
      "    val_log_likelihood: -12187.708621374844\n",
      "    val_log_marginal: -12196.134608598542\n",
      "Train Epoch: 590 [256/118836 (0%)] Loss: 12328.356445\n",
      "Train Epoch: 590 [33024/118836 (28%)] Loss: 12485.519531\n",
      "Train Epoch: 590 [65792/118836 (55%)] Loss: 12309.758789\n",
      "Train Epoch: 590 [98560/118836 (83%)] Loss: 12274.419922\n",
      "    epoch          : 590\n",
      "    loss           : 12281.671058855458\n",
      "    val_loss       : 12286.843661291094\n",
      "    val_log_likelihood: -12188.075868001706\n",
      "    val_log_marginal: -12196.45383897024\n",
      "Train Epoch: 591 [256/118836 (0%)] Loss: 12310.884766\n",
      "Train Epoch: 591 [33024/118836 (28%)] Loss: 12333.528320\n",
      "Train Epoch: 591 [65792/118836 (55%)] Loss: 12423.221680\n",
      "Train Epoch: 591 [98560/118836 (83%)] Loss: 12192.999023\n",
      "    epoch          : 591\n",
      "    loss           : 12287.424892408499\n",
      "    val_loss       : 12278.345199548387\n",
      "    val_log_likelihood: -12187.565942928039\n",
      "    val_log_marginal: -12195.623992355835\n",
      "Train Epoch: 592 [256/118836 (0%)] Loss: 12296.059570\n",
      "Train Epoch: 592 [33024/118836 (28%)] Loss: 12248.214844\n",
      "Train Epoch: 592 [65792/118836 (55%)] Loss: 12264.071289\n",
      "Train Epoch: 592 [98560/118836 (83%)] Loss: 12277.541016\n",
      "    epoch          : 592\n",
      "    loss           : 12278.525101937294\n",
      "    val_loss       : 12287.10665696108\n",
      "    val_log_likelihood: -12185.79483318471\n",
      "    val_log_marginal: -12193.809160732448\n",
      "Train Epoch: 593 [256/118836 (0%)] Loss: 12330.335938\n",
      "Train Epoch: 593 [33024/118836 (28%)] Loss: 12298.828125\n",
      "Train Epoch: 593 [65792/118836 (55%)] Loss: 12348.870117\n",
      "Train Epoch: 593 [98560/118836 (83%)] Loss: 12266.490234\n",
      "    epoch          : 593\n",
      "    loss           : 12282.774513253464\n",
      "    val_loss       : 12281.091388270272\n",
      "    val_log_likelihood: -12187.053608192464\n",
      "    val_log_marginal: -12194.99213929807\n",
      "Train Epoch: 594 [256/118836 (0%)] Loss: 12364.066406\n",
      "Train Epoch: 594 [33024/118836 (28%)] Loss: 12284.605469\n",
      "Train Epoch: 594 [65792/118836 (55%)] Loss: 12325.021484\n",
      "Train Epoch: 594 [98560/118836 (83%)] Loss: 12228.048828\n",
      "    epoch          : 594\n",
      "    loss           : 12281.039450055574\n",
      "    val_loss       : 12280.118742330224\n",
      "    val_log_likelihood: -12185.245000387717\n",
      "    val_log_marginal: -12193.156138649583\n",
      "Train Epoch: 595 [256/118836 (0%)] Loss: 12254.812500\n",
      "Train Epoch: 595 [33024/118836 (28%)] Loss: 12302.232422\n",
      "Train Epoch: 595 [65792/118836 (55%)] Loss: 12277.556641\n",
      "Train Epoch: 595 [98560/118836 (83%)] Loss: 12262.952148\n",
      "    epoch          : 595\n",
      "    loss           : 12278.204528374432\n",
      "    val_loss       : 12281.346106920075\n",
      "    val_log_likelihood: -12188.556687958799\n",
      "    val_log_marginal: -12196.618402604416\n",
      "Train Epoch: 596 [256/118836 (0%)] Loss: 12250.786133\n",
      "Train Epoch: 596 [33024/118836 (28%)] Loss: 12247.667969\n",
      "Train Epoch: 596 [65792/118836 (55%)] Loss: 12357.664062\n",
      "Train Epoch: 596 [98560/118836 (83%)] Loss: 12369.068359\n",
      "    epoch          : 596\n",
      "    loss           : 12283.680105426747\n",
      "    val_loss       : 12281.754648607597\n",
      "    val_log_likelihood: -12190.594832376964\n",
      "    val_log_marginal: -12198.37616405785\n",
      "Train Epoch: 597 [256/118836 (0%)] Loss: 12345.181641\n",
      "Train Epoch: 597 [33024/118836 (28%)] Loss: 12278.208984\n",
      "Train Epoch: 597 [65792/118836 (55%)] Loss: 12341.056641\n",
      "Train Epoch: 597 [98560/118836 (83%)] Loss: 12214.580078\n",
      "    epoch          : 597\n",
      "    loss           : 12277.824651539238\n",
      "    val_loss       : 12278.118638372456\n",
      "    val_log_likelihood: -12185.456213974618\n",
      "    val_log_marginal: -12193.533957444311\n",
      "Train Epoch: 598 [256/118836 (0%)] Loss: 12372.902344\n",
      "Train Epoch: 598 [33024/118836 (28%)] Loss: 12320.846680\n",
      "Train Epoch: 598 [65792/118836 (55%)] Loss: 12410.381836\n",
      "Train Epoch: 598 [98560/118836 (83%)] Loss: 12274.992188\n",
      "    epoch          : 598\n",
      "    loss           : 12283.09841585246\n",
      "    val_loss       : 12279.294343685855\n",
      "    val_log_likelihood: -12185.699364628566\n",
      "    val_log_marginal: -12193.953636423868\n",
      "Train Epoch: 599 [256/118836 (0%)] Loss: 12294.364258\n",
      "Train Epoch: 599 [33024/118836 (28%)] Loss: 12293.850586\n",
      "Train Epoch: 599 [65792/118836 (55%)] Loss: 12238.296875\n",
      "Train Epoch: 599 [98560/118836 (83%)] Loss: 12306.847656\n",
      "    epoch          : 599\n",
      "    loss           : 12281.642732727203\n",
      "    val_loss       : 12281.108633867343\n",
      "    val_log_likelihood: -12188.033944472447\n",
      "    val_log_marginal: -12196.078676252864\n",
      "Train Epoch: 600 [256/118836 (0%)] Loss: 12316.913086\n",
      "Train Epoch: 600 [33024/118836 (28%)] Loss: 12282.287109\n",
      "Train Epoch: 600 [65792/118836 (55%)] Loss: 12315.835938\n",
      "Train Epoch: 600 [98560/118836 (83%)] Loss: 12340.203125\n",
      "    epoch          : 600\n",
      "    loss           : 12280.40749327957\n",
      "    val_loss       : 12280.697460017065\n",
      "    val_log_likelihood: -12185.804315453113\n",
      "    val_log_marginal: -12193.67428500116\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch600.pth ...\n",
      "Train Epoch: 601 [256/118836 (0%)] Loss: 12308.458984\n",
      "Train Epoch: 601 [33024/118836 (28%)] Loss: 12357.541992\n",
      "Train Epoch: 601 [65792/118836 (55%)] Loss: 12309.472656\n",
      "Train Epoch: 601 [98560/118836 (83%)] Loss: 12338.575195\n",
      "    epoch          : 601\n",
      "    loss           : 12283.4663119055\n",
      "    val_loss       : 12282.85291212522\n",
      "    val_log_likelihood: -12188.245567747106\n",
      "    val_log_marginal: -12196.565200538349\n",
      "Train Epoch: 602 [256/118836 (0%)] Loss: 12356.894531\n",
      "Train Epoch: 602 [33024/118836 (28%)] Loss: 12253.672852\n",
      "Train Epoch: 602 [65792/118836 (55%)] Loss: 12294.577148\n",
      "Train Epoch: 602 [98560/118836 (83%)] Loss: 12236.795898\n",
      "    epoch          : 602\n",
      "    loss           : 12288.9972454314\n",
      "    val_loss       : 12285.888765446207\n",
      "    val_log_likelihood: -12189.963536981752\n",
      "    val_log_marginal: -12198.006239993778\n",
      "Train Epoch: 603 [256/118836 (0%)] Loss: 12332.413086\n",
      "Train Epoch: 603 [33024/118836 (28%)] Loss: 12466.977539\n",
      "Train Epoch: 603 [65792/118836 (55%)] Loss: 12237.804688\n",
      "Train Epoch: 603 [98560/118836 (83%)] Loss: 12334.833984\n",
      "    epoch          : 603\n",
      "    loss           : 12284.046251744727\n",
      "    val_loss       : 12279.71124702853\n",
      "    val_log_likelihood: -12190.665660379189\n",
      "    val_log_marginal: -12198.638076861069\n",
      "Train Epoch: 604 [256/118836 (0%)] Loss: 12200.935547\n",
      "Train Epoch: 604 [33024/118836 (28%)] Loss: 12406.256836\n",
      "Train Epoch: 604 [65792/118836 (55%)] Loss: 12415.308594\n",
      "Train Epoch: 604 [98560/118836 (83%)] Loss: 12242.590820\n",
      "    epoch          : 604\n",
      "    loss           : 12278.782099100497\n",
      "    val_loss       : 12281.896680581978\n",
      "    val_log_likelihood: -12186.8884765625\n",
      "    val_log_marginal: -12194.958564705987\n",
      "Train Epoch: 605 [256/118836 (0%)] Loss: 12241.857422\n",
      "Train Epoch: 605 [33024/118836 (28%)] Loss: 12269.797852\n",
      "Train Epoch: 605 [65792/118836 (55%)] Loss: 12272.179688\n",
      "Train Epoch: 605 [98560/118836 (83%)] Loss: 12369.024414\n",
      "    epoch          : 605\n",
      "    loss           : 12277.896426702078\n",
      "    val_loss       : 12281.668741641211\n",
      "    val_log_likelihood: -12185.634090027916\n",
      "    val_log_marginal: -12193.609677172964\n",
      "Train Epoch: 606 [256/118836 (0%)] Loss: 12311.341797\n",
      "Train Epoch: 606 [33024/118836 (28%)] Loss: 12300.472656\n",
      "Train Epoch: 606 [65792/118836 (55%)] Loss: 12276.470703\n",
      "Train Epoch: 606 [98560/118836 (83%)] Loss: 12380.082031\n",
      "    epoch          : 606\n",
      "    loss           : 12282.728973777397\n",
      "    val_loss       : 12282.31831601478\n",
      "    val_log_likelihood: -12185.791410934915\n",
      "    val_log_marginal: -12193.894335233144\n",
      "Train Epoch: 607 [256/118836 (0%)] Loss: 12409.398438\n",
      "Train Epoch: 607 [33024/118836 (28%)] Loss: 12292.248047\n",
      "Train Epoch: 607 [65792/118836 (55%)] Loss: 12374.939453\n",
      "Train Epoch: 607 [98560/118836 (83%)] Loss: 12348.813477\n",
      "    epoch          : 607\n",
      "    loss           : 12284.918796526053\n",
      "    val_loss       : 12281.729792429724\n",
      "    val_log_likelihood: -12189.621952381876\n",
      "    val_log_marginal: -12197.921169269881\n",
      "Train Epoch: 608 [256/118836 (0%)] Loss: 12267.431641\n",
      "Train Epoch: 608 [33024/118836 (28%)] Loss: 12307.406250\n",
      "Train Epoch: 608 [65792/118836 (55%)] Loss: 12320.531250\n",
      "Train Epoch: 608 [98560/118836 (83%)] Loss: 12357.003906\n",
      "    epoch          : 608\n",
      "    loss           : 12282.349887723583\n",
      "    val_loss       : 12283.52805344832\n",
      "    val_log_likelihood: -12186.081540626292\n",
      "    val_log_marginal: -12194.249313446071\n",
      "Train Epoch: 609 [256/118836 (0%)] Loss: 12219.390625\n",
      "Train Epoch: 609 [33024/118836 (28%)] Loss: 12253.150391\n",
      "Train Epoch: 609 [65792/118836 (55%)] Loss: 12266.124023\n",
      "Train Epoch: 609 [98560/118836 (83%)] Loss: 12248.285156\n",
      "    epoch          : 609\n",
      "    loss           : 12283.852411600497\n",
      "    val_loss       : 12281.212297375394\n",
      "    val_log_likelihood: -12185.15771944789\n",
      "    val_log_marginal: -12193.254478286968\n",
      "Train Epoch: 610 [256/118836 (0%)] Loss: 12278.774414\n",
      "Train Epoch: 610 [33024/118836 (28%)] Loss: 12361.601562\n",
      "Train Epoch: 610 [65792/118836 (55%)] Loss: 12337.041016\n",
      "Train Epoch: 610 [98560/118836 (83%)] Loss: 12287.243164\n",
      "    epoch          : 610\n",
      "    loss           : 12283.464637452182\n",
      "    val_loss       : 12308.162227157783\n",
      "    val_log_likelihood: -12184.085754626756\n",
      "    val_log_marginal: -12192.53879968843\n",
      "Train Epoch: 611 [256/118836 (0%)] Loss: 12317.164062\n",
      "Train Epoch: 611 [33024/118836 (28%)] Loss: 12225.003906\n",
      "Train Epoch: 611 [65792/118836 (55%)] Loss: 12232.611328\n",
      "Train Epoch: 611 [98560/118836 (83%)] Loss: 12306.350586\n",
      "    epoch          : 611\n",
      "    loss           : 12286.073180637406\n",
      "    val_loss       : 12279.958719359123\n",
      "    val_log_likelihood: -12185.004022888234\n",
      "    val_log_marginal: -12192.95827588548\n",
      "Train Epoch: 612 [256/118836 (0%)] Loss: 12251.792969\n",
      "Train Epoch: 612 [33024/118836 (28%)] Loss: 12420.086914\n",
      "Train Epoch: 612 [65792/118836 (55%)] Loss: 12286.151367\n",
      "Train Epoch: 612 [98560/118836 (83%)] Loss: 12207.921875\n",
      "    epoch          : 612\n",
      "    loss           : 12281.618826574131\n",
      "    val_loss       : 12286.861324245903\n",
      "    val_log_likelihood: -12185.502562487076\n",
      "    val_log_marginal: -12193.557945043\n",
      "Train Epoch: 613 [256/118836 (0%)] Loss: 12323.414062\n",
      "Train Epoch: 613 [33024/118836 (28%)] Loss: 12294.599609\n",
      "Train Epoch: 613 [65792/118836 (55%)] Loss: 12311.054688\n",
      "Train Epoch: 613 [98560/118836 (83%)] Loss: 12366.915039\n",
      "    epoch          : 613\n",
      "    loss           : 12279.580932879704\n",
      "    val_loss       : 12283.529931777457\n",
      "    val_log_likelihood: -12185.626556845793\n",
      "    val_log_marginal: -12193.818149039149\n",
      "Train Epoch: 614 [256/118836 (0%)] Loss: 12314.531250\n",
      "Train Epoch: 614 [33024/118836 (28%)] Loss: 12258.824219\n",
      "Train Epoch: 614 [65792/118836 (55%)] Loss: 12318.556641\n",
      "Train Epoch: 614 [98560/118836 (83%)] Loss: 12324.513672\n",
      "    epoch          : 614\n",
      "    loss           : 12279.877186886115\n",
      "    val_loss       : 12277.7770881111\n",
      "    val_log_likelihood: -12187.357726717586\n",
      "    val_log_marginal: -12195.429668565064\n",
      "Train Epoch: 615 [256/118836 (0%)] Loss: 12285.468750\n",
      "Train Epoch: 615 [33024/118836 (28%)] Loss: 12354.878906\n",
      "Train Epoch: 615 [65792/118836 (55%)] Loss: 12235.990234\n",
      "Train Epoch: 615 [98560/118836 (83%)] Loss: 12271.995117\n",
      "    epoch          : 615\n",
      "    loss           : 12280.36272632987\n",
      "    val_loss       : 12285.400985075774\n",
      "    val_log_likelihood: -12188.869098945408\n",
      "    val_log_marginal: -12197.345210358855\n",
      "Train Epoch: 616 [256/118836 (0%)] Loss: 12219.511719\n",
      "Train Epoch: 616 [33024/118836 (28%)] Loss: 12303.711914\n",
      "Train Epoch: 616 [65792/118836 (55%)] Loss: 12310.916016\n",
      "Train Epoch: 616 [98560/118836 (83%)] Loss: 12332.082031\n",
      "    epoch          : 616\n",
      "    loss           : 12284.277090118383\n",
      "    val_loss       : 12278.674565474039\n",
      "    val_log_likelihood: -12188.913826477203\n",
      "    val_log_marginal: -12197.210354822048\n",
      "Train Epoch: 617 [256/118836 (0%)] Loss: 12381.816406\n",
      "Train Epoch: 617 [33024/118836 (28%)] Loss: 12267.437500\n",
      "Train Epoch: 617 [65792/118836 (55%)] Loss: 12369.099609\n",
      "Train Epoch: 617 [98560/118836 (83%)] Loss: 12368.720703\n",
      "    epoch          : 617\n",
      "    loss           : 12282.774963974618\n",
      "    val_loss       : 12280.10275924925\n",
      "    val_log_likelihood: -12186.12649771893\n",
      "    val_log_marginal: -12194.446976192809\n",
      "Train Epoch: 618 [256/118836 (0%)] Loss: 12311.558594\n",
      "Train Epoch: 618 [33024/118836 (28%)] Loss: 12341.629883\n",
      "Train Epoch: 618 [65792/118836 (55%)] Loss: 12381.148438\n",
      "Train Epoch: 618 [98560/118836 (83%)] Loss: 12403.535156\n",
      "    epoch          : 618\n",
      "    loss           : 12277.287955082971\n",
      "    val_loss       : 12281.978979443433\n",
      "    val_log_likelihood: -12185.302934372416\n",
      "    val_log_marginal: -12193.447794038822\n",
      "Train Epoch: 619 [256/118836 (0%)] Loss: 12359.349609\n",
      "Train Epoch: 619 [33024/118836 (28%)] Loss: 12232.942383\n",
      "Train Epoch: 619 [65792/118836 (55%)] Loss: 12410.592773\n",
      "Train Epoch: 619 [98560/118836 (83%)] Loss: 12238.484375\n",
      "    epoch          : 619\n",
      "    loss           : 12275.746705535308\n",
      "    val_loss       : 12278.598071448774\n",
      "    val_log_likelihood: -12185.103790419511\n",
      "    val_log_marginal: -12193.14313044267\n",
      "Train Epoch: 620 [256/118836 (0%)] Loss: 12397.389648\n",
      "Train Epoch: 620 [33024/118836 (28%)] Loss: 12329.396484\n",
      "Train Epoch: 620 [65792/118836 (55%)] Loss: 12272.451172\n",
      "Train Epoch: 620 [98560/118836 (83%)] Loss: 12384.399414\n",
      "    epoch          : 620\n",
      "    loss           : 12279.51620964511\n",
      "    val_loss       : 12279.799169599288\n",
      "    val_log_likelihood: -12185.603501731803\n",
      "    val_log_marginal: -12193.97131578818\n",
      "Train Epoch: 621 [256/118836 (0%)] Loss: 12339.128906\n",
      "Train Epoch: 621 [33024/118836 (28%)] Loss: 12336.073242\n",
      "Train Epoch: 621 [65792/118836 (55%)] Loss: 12342.922852\n",
      "Train Epoch: 621 [98560/118836 (83%)] Loss: 12298.378906\n",
      "    epoch          : 621\n",
      "    loss           : 12278.468658401829\n",
      "    val_loss       : 12279.360070943643\n",
      "    val_log_likelihood: -12186.879164728081\n",
      "    val_log_marginal: -12195.501204411345\n",
      "Train Epoch: 622 [256/118836 (0%)] Loss: 12303.586914\n",
      "Train Epoch: 622 [33024/118836 (28%)] Loss: 12282.428711\n",
      "Train Epoch: 622 [65792/118836 (55%)] Loss: 12238.279297\n",
      "Train Epoch: 622 [98560/118836 (83%)] Loss: 12286.541992\n",
      "    epoch          : 622\n",
      "    loss           : 12281.705948065292\n",
      "    val_loss       : 12285.320601901296\n",
      "    val_log_likelihood: -12190.004462300973\n",
      "    val_log_marginal: -12198.665326521193\n",
      "Train Epoch: 623 [256/118836 (0%)] Loss: 12367.844727\n",
      "Train Epoch: 623 [33024/118836 (28%)] Loss: 12276.333984\n",
      "Train Epoch: 623 [65792/118836 (55%)] Loss: 12282.870117\n",
      "Train Epoch: 623 [98560/118836 (83%)] Loss: 12291.753906\n",
      "    epoch          : 623\n",
      "    loss           : 12284.339675900797\n",
      "    val_loss       : 12278.498588430853\n",
      "    val_log_likelihood: -12186.5114899969\n",
      "    val_log_marginal: -12194.571807461238\n",
      "Train Epoch: 624 [256/118836 (0%)] Loss: 12274.126953\n",
      "Train Epoch: 624 [33024/118836 (28%)] Loss: 12281.166992\n",
      "Train Epoch: 624 [65792/118836 (55%)] Loss: 12208.861328\n",
      "Train Epoch: 624 [98560/118836 (83%)] Loss: 12188.590820\n",
      "    epoch          : 624\n",
      "    loss           : 12283.328847123139\n",
      "    val_loss       : 12294.034122516283\n",
      "    val_log_likelihood: -12190.291347930883\n",
      "    val_log_marginal: -12199.139522847332\n",
      "Train Epoch: 625 [256/118836 (0%)] Loss: 12351.190430\n",
      "Train Epoch: 625 [33024/118836 (28%)] Loss: 12335.625000\n",
      "Train Epoch: 625 [65792/118836 (55%)] Loss: 12223.636719\n",
      "Train Epoch: 625 [98560/118836 (83%)] Loss: 12391.528320\n",
      "    epoch          : 625\n",
      "    loss           : 12281.137494184242\n",
      "    val_loss       : 12280.943273975487\n",
      "    val_log_likelihood: -12185.726547475962\n",
      "    val_log_marginal: -12193.973515698104\n",
      "Train Epoch: 626 [256/118836 (0%)] Loss: 12304.198242\n",
      "Train Epoch: 626 [33024/118836 (28%)] Loss: 12226.353516\n",
      "Train Epoch: 626 [65792/118836 (55%)] Loss: 12345.339844\n",
      "Train Epoch: 626 [98560/118836 (83%)] Loss: 12384.563477\n",
      "    epoch          : 626\n",
      "    loss           : 12277.655918017213\n",
      "    val_loss       : 12274.719427655938\n",
      "    val_log_likelihood: -12184.3295931555\n",
      "    val_log_marginal: -12192.552992355659\n",
      "Train Epoch: 627 [256/118836 (0%)] Loss: 12224.300781\n",
      "Train Epoch: 627 [33024/118836 (28%)] Loss: 12216.964844\n",
      "Train Epoch: 627 [65792/118836 (55%)] Loss: 12236.781250\n",
      "Train Epoch: 627 [98560/118836 (83%)] Loss: 12221.673828\n",
      "    epoch          : 627\n",
      "    loss           : 12274.105031922043\n",
      "    val_loss       : 12283.35104984938\n",
      "    val_log_likelihood: -12184.178541795905\n",
      "    val_log_marginal: -12192.329316471076\n",
      "Train Epoch: 628 [256/118836 (0%)] Loss: 12269.282227\n",
      "Train Epoch: 628 [33024/118836 (28%)] Loss: 12337.664062\n",
      "Train Epoch: 628 [65792/118836 (55%)] Loss: 12235.660156\n",
      "Train Epoch: 628 [98560/118836 (83%)] Loss: 12240.759766\n",
      "    epoch          : 628\n",
      "    loss           : 12279.387763647643\n",
      "    val_loss       : 12278.338743107472\n",
      "    val_log_likelihood: -12185.930864544562\n",
      "    val_log_marginal: -12194.193682305518\n",
      "Train Epoch: 629 [256/118836 (0%)] Loss: 12281.096680\n",
      "Train Epoch: 629 [33024/118836 (28%)] Loss: 12369.386719\n",
      "Train Epoch: 629 [65792/118836 (55%)] Loss: 12314.052734\n",
      "Train Epoch: 629 [98560/118836 (83%)] Loss: 12227.244141\n",
      "    epoch          : 629\n",
      "    loss           : 12283.507217515767\n",
      "    val_loss       : 12282.150221849499\n",
      "    val_log_likelihood: -12189.201386411807\n",
      "    val_log_marginal: -12197.727145401212\n",
      "Train Epoch: 630 [256/118836 (0%)] Loss: 12346.767578\n",
      "Train Epoch: 630 [33024/118836 (28%)] Loss: 12370.567383\n",
      "Train Epoch: 630 [65792/118836 (55%)] Loss: 12239.871094\n",
      "Train Epoch: 630 [98560/118836 (83%)] Loss: 12246.878906\n",
      "    epoch          : 630\n",
      "    loss           : 12284.175199189673\n",
      "    val_loss       : 12277.383971877805\n",
      "    val_log_likelihood: -12186.335045265972\n",
      "    val_log_marginal: -12194.677680331091\n",
      "Train Epoch: 631 [256/118836 (0%)] Loss: 12285.161133\n",
      "Train Epoch: 631 [33024/118836 (28%)] Loss: 12243.475586\n",
      "Train Epoch: 631 [65792/118836 (55%)] Loss: 12371.999023\n",
      "Train Epoch: 631 [98560/118836 (83%)] Loss: 12454.165039\n",
      "    epoch          : 631\n",
      "    loss           : 12277.270525905966\n",
      "    val_loss       : 12282.30517477384\n",
      "    val_log_likelihood: -12185.309609730408\n",
      "    val_log_marginal: -12193.413829068912\n",
      "Train Epoch: 632 [256/118836 (0%)] Loss: 12402.820312\n",
      "Train Epoch: 632 [33024/118836 (28%)] Loss: 12280.650391\n",
      "Train Epoch: 632 [65792/118836 (55%)] Loss: 12373.349609\n",
      "Train Epoch: 632 [98560/118836 (83%)] Loss: 12292.876953\n",
      "    epoch          : 632\n",
      "    loss           : 12274.471733160153\n",
      "    val_loss       : 12276.538420435123\n",
      "    val_log_likelihood: -12181.928132431245\n",
      "    val_log_marginal: -12189.955288138064\n",
      "Train Epoch: 633 [256/118836 (0%)] Loss: 12397.454102\n",
      "Train Epoch: 633 [33024/118836 (28%)] Loss: 12399.010742\n",
      "Train Epoch: 633 [65792/118836 (55%)] Loss: 12332.682617\n",
      "Train Epoch: 633 [98560/118836 (83%)] Loss: 12392.991211\n",
      "    epoch          : 633\n",
      "    loss           : 12280.679211738781\n",
      "    val_loss       : 12275.427975041293\n",
      "    val_log_likelihood: -12184.858299731182\n",
      "    val_log_marginal: -12193.155425504483\n",
      "Train Epoch: 634 [256/118836 (0%)] Loss: 12393.726562\n",
      "Train Epoch: 634 [33024/118836 (28%)] Loss: 12358.144531\n",
      "Train Epoch: 634 [65792/118836 (55%)] Loss: 12189.911133\n",
      "Train Epoch: 634 [98560/118836 (83%)] Loss: 12369.320312\n",
      "    epoch          : 634\n",
      "    loss           : 12276.651269773574\n",
      "    val_loss       : 12280.699944528973\n",
      "    val_log_likelihood: -12184.135427651985\n",
      "    val_log_marginal: -12192.18986877805\n",
      "Train Epoch: 635 [256/118836 (0%)] Loss: 12249.258789\n",
      "Train Epoch: 635 [33024/118836 (28%)] Loss: 12279.046875\n",
      "Train Epoch: 635 [65792/118836 (55%)] Loss: 12270.850586\n",
      "Train Epoch: 635 [98560/118836 (83%)] Loss: 12248.957031\n",
      "    epoch          : 635\n",
      "    loss           : 12278.786800816792\n",
      "    val_loss       : 12278.28018918776\n",
      "    val_log_likelihood: -12185.660478055212\n",
      "    val_log_marginal: -12194.036144064497\n",
      "Train Epoch: 636 [256/118836 (0%)] Loss: 12212.958008\n",
      "Train Epoch: 636 [33024/118836 (28%)] Loss: 12366.693359\n",
      "Train Epoch: 636 [65792/118836 (55%)] Loss: 12298.562500\n",
      "Train Epoch: 636 [98560/118836 (83%)] Loss: 12348.060547\n",
      "    epoch          : 636\n",
      "    loss           : 12281.995941086383\n",
      "    val_loss       : 12310.295296961107\n",
      "    val_log_likelihood: -12194.881760171113\n",
      "    val_log_marginal: -12203.622079684565\n",
      "Train Epoch: 637 [256/118836 (0%)] Loss: 12297.488281\n",
      "Train Epoch: 637 [33024/118836 (28%)] Loss: 12367.588867\n",
      "Train Epoch: 637 [65792/118836 (55%)] Loss: 12306.648438\n",
      "Train Epoch: 637 [98560/118836 (83%)] Loss: 12312.294922\n",
      "    epoch          : 637\n",
      "    loss           : 12286.758535592433\n",
      "    val_loss       : 12276.04801142864\n",
      "    val_log_likelihood: -12184.385738633428\n",
      "    val_log_marginal: -12192.820413110108\n",
      "Train Epoch: 638 [256/118836 (0%)] Loss: 12341.359375\n",
      "Train Epoch: 638 [33024/118836 (28%)] Loss: 12380.161133\n",
      "Train Epoch: 638 [65792/118836 (55%)] Loss: 12265.274414\n",
      "Train Epoch: 638 [98560/118836 (83%)] Loss: 12294.478516\n",
      "    epoch          : 638\n",
      "    loss           : 12281.204653413202\n",
      "    val_loss       : 12280.543702888404\n",
      "    val_log_likelihood: -12185.732651435846\n",
      "    val_log_marginal: -12193.842246043445\n",
      "Train Epoch: 639 [256/118836 (0%)] Loss: 12318.914062\n",
      "Train Epoch: 639 [33024/118836 (28%)] Loss: 12260.974609\n",
      "Train Epoch: 639 [65792/118836 (55%)] Loss: 12225.855469\n",
      "Train Epoch: 639 [98560/118836 (83%)] Loss: 12277.614258\n",
      "    epoch          : 639\n",
      "    loss           : 12282.439235680315\n",
      "    val_loss       : 12277.17271627859\n",
      "    val_log_likelihood: -12183.923755104943\n",
      "    val_log_marginal: -12192.13203858841\n",
      "Train Epoch: 640 [256/118836 (0%)] Loss: 12243.710938\n",
      "Train Epoch: 640 [33024/118836 (28%)] Loss: 12250.541992\n",
      "Train Epoch: 640 [65792/118836 (55%)] Loss: 12287.027344\n",
      "Train Epoch: 640 [98560/118836 (83%)] Loss: 12208.212891\n",
      "    epoch          : 640\n",
      "    loss           : 12282.27877264914\n",
      "    val_loss       : 12280.517667023018\n",
      "    val_log_likelihood: -12182.035911005996\n",
      "    val_log_marginal: -12190.527183193908\n",
      "Train Epoch: 641 [256/118836 (0%)] Loss: 12278.130859\n",
      "Train Epoch: 641 [33024/118836 (28%)] Loss: 12277.610352\n",
      "Train Epoch: 641 [65792/118836 (55%)] Loss: 12284.323242\n",
      "Train Epoch: 641 [98560/118836 (83%)] Loss: 12424.305664\n",
      "    epoch          : 641\n",
      "    loss           : 12283.27238290943\n",
      "    val_loss       : 12282.158907333658\n",
      "    val_log_likelihood: -12186.496662886424\n",
      "    val_log_marginal: -12194.940801985218\n",
      "Train Epoch: 642 [256/118836 (0%)] Loss: 12313.281250\n",
      "Train Epoch: 642 [33024/118836 (28%)] Loss: 12252.331055\n",
      "Train Epoch: 642 [65792/118836 (55%)] Loss: 12336.976562\n",
      "Train Epoch: 642 [98560/118836 (83%)] Loss: 12366.325195\n",
      "    epoch          : 642\n",
      "    loss           : 12278.237442973274\n",
      "    val_loss       : 12278.80289152502\n",
      "    val_log_likelihood: -12183.866905597342\n",
      "    val_log_marginal: -12191.887175704427\n",
      "Train Epoch: 643 [256/118836 (0%)] Loss: 12275.792969\n",
      "Train Epoch: 643 [33024/118836 (28%)] Loss: 12255.783203\n",
      "Train Epoch: 643 [65792/118836 (55%)] Loss: 12360.929688\n",
      "Train Epoch: 643 [98560/118836 (83%)] Loss: 12345.064453\n",
      "    epoch          : 643\n",
      "    loss           : 12275.908786962365\n",
      "    val_loss       : 12277.308020884704\n",
      "    val_log_likelihood: -12180.499554609956\n",
      "    val_log_marginal: -12188.66791843634\n",
      "Train Epoch: 644 [256/118836 (0%)] Loss: 12267.738281\n",
      "Train Epoch: 644 [33024/118836 (28%)] Loss: 12279.969727\n",
      "Train Epoch: 644 [65792/118836 (55%)] Loss: 12332.128906\n",
      "Train Epoch: 644 [98560/118836 (83%)] Loss: 12279.061523\n",
      "    epoch          : 644\n",
      "    loss           : 12281.18361281276\n",
      "    val_loss       : 12279.139280597856\n",
      "    val_log_likelihood: -12183.74415548749\n",
      "    val_log_marginal: -12191.855010741088\n",
      "Train Epoch: 645 [256/118836 (0%)] Loss: 12317.671875\n",
      "Train Epoch: 645 [33024/118836 (28%)] Loss: 12349.959961\n",
      "Train Epoch: 645 [65792/118836 (55%)] Loss: 12232.582031\n",
      "Train Epoch: 645 [98560/118836 (83%)] Loss: 12328.980469\n",
      "    epoch          : 645\n",
      "    loss           : 12281.09656498785\n",
      "    val_loss       : 12277.567605137932\n",
      "    val_log_likelihood: -12185.954989111611\n",
      "    val_log_marginal: -12193.963878648978\n",
      "Train Epoch: 646 [256/118836 (0%)] Loss: 12289.490234\n",
      "Train Epoch: 646 [33024/118836 (28%)] Loss: 12363.989258\n",
      "Train Epoch: 646 [65792/118836 (55%)] Loss: 12341.001953\n",
      "Train Epoch: 646 [98560/118836 (83%)] Loss: 12278.798828\n",
      "    epoch          : 646\n",
      "    loss           : 12281.116901235524\n",
      "    val_loss       : 12281.5218490474\n",
      "    val_log_likelihood: -12184.732584393092\n",
      "    val_log_marginal: -12192.805312911654\n",
      "Train Epoch: 647 [256/118836 (0%)] Loss: 12235.552734\n",
      "Train Epoch: 647 [33024/118836 (28%)] Loss: 12297.997070\n",
      "Train Epoch: 647 [65792/118836 (55%)] Loss: 12247.578125\n",
      "Train Epoch: 647 [98560/118836 (83%)] Loss: 12262.816406\n",
      "    epoch          : 647\n",
      "    loss           : 12277.345551107579\n",
      "    val_loss       : 12272.709801854475\n",
      "    val_log_likelihood: -12186.812876731803\n",
      "    val_log_marginal: -12195.048980559442\n",
      "Train Epoch: 648 [256/118836 (0%)] Loss: 12218.452148\n",
      "Train Epoch: 648 [33024/118836 (28%)] Loss: 12308.755859\n",
      "Train Epoch: 648 [65792/118836 (55%)] Loss: 12273.226562\n",
      "Train Epoch: 648 [98560/118836 (83%)] Loss: 12249.824219\n",
      "    epoch          : 648\n",
      "    loss           : 12279.519771473066\n",
      "    val_loss       : 12283.085903642726\n",
      "    val_log_likelihood: -12184.931229967948\n",
      "    val_log_marginal: -12192.976839076835\n",
      "Train Epoch: 649 [256/118836 (0%)] Loss: 12245.436523\n",
      "Train Epoch: 649 [33024/118836 (28%)] Loss: 12380.865234\n",
      "Train Epoch: 649 [65792/118836 (55%)] Loss: 12291.572266\n",
      "Train Epoch: 649 [98560/118836 (83%)] Loss: 12225.354492\n",
      "    epoch          : 649\n",
      "    loss           : 12275.26488688353\n",
      "    val_loss       : 12280.381225109657\n",
      "    val_log_likelihood: -12185.745043198149\n",
      "    val_log_marginal: -12193.855554405276\n",
      "Train Epoch: 650 [256/118836 (0%)] Loss: 12355.218750\n",
      "Train Epoch: 650 [33024/118836 (28%)] Loss: 12313.357422\n",
      "Train Epoch: 650 [65792/118836 (55%)] Loss: 12377.221680\n",
      "Train Epoch: 650 [98560/118836 (83%)] Loss: 12206.119141\n",
      "    epoch          : 650\n",
      "    loss           : 12280.486826826147\n",
      "    val_loss       : 12281.04677248594\n",
      "    val_log_likelihood: -12185.51021812319\n",
      "    val_log_marginal: -12193.94342094839\n",
      "Train Epoch: 651 [256/118836 (0%)] Loss: 12241.346680\n",
      "Train Epoch: 651 [33024/118836 (28%)] Loss: 12396.650391\n",
      "Train Epoch: 651 [65792/118836 (55%)] Loss: 12383.125000\n",
      "Train Epoch: 651 [98560/118836 (83%)] Loss: 12255.597656\n",
      "    epoch          : 651\n",
      "    loss           : 12278.24872974178\n",
      "    val_loss       : 12281.947352212665\n",
      "    val_log_likelihood: -12188.672069827853\n",
      "    val_log_marginal: -12196.933030560433\n",
      "Train Epoch: 652 [256/118836 (0%)] Loss: 12199.153320\n",
      "Train Epoch: 652 [33024/118836 (28%)] Loss: 12261.512695\n",
      "Train Epoch: 652 [65792/118836 (55%)] Loss: 12353.712891\n",
      "Train Epoch: 652 [98560/118836 (83%)] Loss: 12267.641602\n",
      "    epoch          : 652\n",
      "    loss           : 12281.272043172303\n",
      "    val_loss       : 12284.201577872573\n",
      "    val_log_likelihood: -12183.344375032311\n",
      "    val_log_marginal: -12191.397571614372\n",
      "Train Epoch: 653 [256/118836 (0%)] Loss: 12333.180664\n",
      "Train Epoch: 653 [33024/118836 (28%)] Loss: 12375.238281\n",
      "Train Epoch: 653 [65792/118836 (55%)] Loss: 12268.794922\n",
      "Train Epoch: 653 [98560/118836 (83%)] Loss: 12325.559570\n",
      "    epoch          : 653\n",
      "    loss           : 12281.79155180547\n",
      "    val_loss       : 12283.385895360392\n",
      "    val_log_likelihood: -12187.16340531948\n",
      "    val_log_marginal: -12195.781015066968\n",
      "Train Epoch: 654 [256/118836 (0%)] Loss: 12283.787109\n",
      "Train Epoch: 654 [33024/118836 (28%)] Loss: 12284.971680\n",
      "Train Epoch: 654 [65792/118836 (55%)] Loss: 12352.272461\n",
      "Train Epoch: 654 [98560/118836 (83%)] Loss: 12233.187500\n",
      "    epoch          : 654\n",
      "    loss           : 12282.83110993719\n",
      "    val_loss       : 12273.831360394262\n",
      "    val_log_likelihood: -12183.595015411756\n",
      "    val_log_marginal: -12191.956177748134\n",
      "Train Epoch: 655 [256/118836 (0%)] Loss: 12382.605469\n",
      "Train Epoch: 655 [33024/118836 (28%)] Loss: 12416.817383\n",
      "Train Epoch: 655 [65792/118836 (55%)] Loss: 12302.708984\n",
      "Train Epoch: 655 [98560/118836 (83%)] Loss: 12336.149414\n",
      "    epoch          : 655\n",
      "    loss           : 12281.621271292131\n",
      "    val_loss       : 12274.979210934222\n",
      "    val_log_likelihood: -12184.167321585506\n",
      "    val_log_marginal: -12192.196454046241\n",
      "Train Epoch: 656 [256/118836 (0%)] Loss: 12301.910156\n",
      "Train Epoch: 656 [33024/118836 (28%)] Loss: 12264.546875\n",
      "Train Epoch: 656 [65792/118836 (55%)] Loss: 12219.525391\n",
      "Train Epoch: 656 [98560/118836 (83%)] Loss: 12410.019531\n",
      "    epoch          : 656\n",
      "    loss           : 12282.083487450887\n",
      "    val_loss       : 12278.600369744725\n",
      "    val_log_likelihood: -12184.2149109543\n",
      "    val_log_marginal: -12192.255052429962\n",
      "Train Epoch: 657 [256/118836 (0%)] Loss: 12313.604492\n",
      "Train Epoch: 657 [33024/118836 (28%)] Loss: 12205.742188\n",
      "Train Epoch: 657 [65792/118836 (55%)] Loss: 12258.617188\n",
      "Train Epoch: 657 [98560/118836 (83%)] Loss: 12286.302734\n",
      "    epoch          : 657\n",
      "    loss           : 12280.032020910878\n",
      "    val_loss       : 12276.019839268658\n",
      "    val_log_likelihood: -12184.44450572529\n",
      "    val_log_marginal: -12192.742190087345\n",
      "Train Epoch: 658 [256/118836 (0%)] Loss: 12230.419922\n",
      "Train Epoch: 658 [33024/118836 (28%)] Loss: 12249.857422\n",
      "Train Epoch: 658 [65792/118836 (55%)] Loss: 12268.031250\n",
      "Train Epoch: 658 [98560/118836 (83%)] Loss: 12324.781250\n",
      "    epoch          : 658\n",
      "    loss           : 12276.597298257859\n",
      "    val_loss       : 12285.70727487545\n",
      "    val_log_likelihood: -12192.22826764759\n",
      "    val_log_marginal: -12200.954212895762\n",
      "Train Epoch: 659 [256/118836 (0%)] Loss: 12237.693359\n",
      "Train Epoch: 659 [33024/118836 (28%)] Loss: 12283.495117\n",
      "Train Epoch: 659 [65792/118836 (55%)] Loss: 12350.173828\n",
      "Train Epoch: 659 [98560/118836 (83%)] Loss: 12213.187500\n",
      "    epoch          : 659\n",
      "    loss           : 12280.344636256721\n",
      "    val_loss       : 12279.946378998562\n",
      "    val_log_likelihood: -12186.8178340183\n",
      "    val_log_marginal: -12195.188477233227\n",
      "Train Epoch: 660 [256/118836 (0%)] Loss: 12271.213867\n",
      "Train Epoch: 660 [33024/118836 (28%)] Loss: 12330.966797\n",
      "Train Epoch: 660 [65792/118836 (55%)] Loss: 12429.844727\n",
      "Train Epoch: 660 [98560/118836 (83%)] Loss: 12264.148438\n",
      "    epoch          : 660\n",
      "    loss           : 12279.913583023159\n",
      "    val_loss       : 12281.70012782777\n",
      "    val_log_likelihood: -12185.044247731856\n",
      "    val_log_marginal: -12193.34551281398\n",
      "Train Epoch: 661 [256/118836 (0%)] Loss: 12225.285156\n",
      "Train Epoch: 661 [33024/118836 (28%)] Loss: 12334.392578\n",
      "Train Epoch: 661 [65792/118836 (55%)] Loss: 12374.670898\n",
      "Train Epoch: 661 [98560/118836 (83%)] Loss: 12270.269531\n",
      "    epoch          : 661\n",
      "    loss           : 12280.290170078577\n",
      "    val_loss       : 12280.289190120302\n",
      "    val_log_likelihood: -12189.405624321496\n",
      "    val_log_marginal: -12197.548055811802\n",
      "Train Epoch: 662 [256/118836 (0%)] Loss: 12414.984375\n",
      "Train Epoch: 662 [33024/118836 (28%)] Loss: 12347.597656\n",
      "Train Epoch: 662 [65792/118836 (55%)] Loss: 12252.531250\n",
      "Train Epoch: 662 [98560/118836 (83%)] Loss: 12367.010742\n",
      "    epoch          : 662\n",
      "    loss           : 12276.059283724928\n",
      "    val_loss       : 12277.630756829576\n",
      "    val_log_likelihood: -12184.634028639372\n",
      "    val_log_marginal: -12192.847663635168\n",
      "Train Epoch: 663 [256/118836 (0%)] Loss: 12279.144531\n",
      "Train Epoch: 663 [33024/118836 (28%)] Loss: 12353.660156\n",
      "Train Epoch: 663 [65792/118836 (55%)] Loss: 12284.462891\n",
      "Train Epoch: 663 [98560/118836 (83%)] Loss: 12338.627930\n",
      "    epoch          : 663\n",
      "    loss           : 12280.920115248915\n",
      "    val_loss       : 12280.33712773544\n",
      "    val_log_likelihood: -12189.015915303195\n",
      "    val_log_marginal: -12197.165233491634\n",
      "Train Epoch: 664 [256/118836 (0%)] Loss: 12198.395508\n",
      "Train Epoch: 664 [33024/118836 (28%)] Loss: 12320.710938\n",
      "Train Epoch: 664 [65792/118836 (55%)] Loss: 12298.798828\n",
      "Train Epoch: 664 [98560/118836 (83%)] Loss: 12378.927734\n",
      "    epoch          : 664\n",
      "    loss           : 12278.034644948046\n",
      "    val_loss       : 12284.050923434164\n",
      "    val_log_likelihood: -12186.779390250207\n",
      "    val_log_marginal: -12195.180524855237\n",
      "Train Epoch: 665 [256/118836 (0%)] Loss: 12324.271484\n",
      "Train Epoch: 665 [33024/118836 (28%)] Loss: 12375.432617\n",
      "Train Epoch: 665 [65792/118836 (55%)] Loss: 12215.778320\n",
      "Train Epoch: 665 [98560/118836 (83%)] Loss: 12372.458984\n",
      "    epoch          : 665\n",
      "    loss           : 12281.99548907284\n",
      "    val_loss       : 12282.596706654373\n",
      "    val_log_likelihood: -12189.925485777245\n",
      "    val_log_marginal: -12198.103308478328\n",
      "Train Epoch: 666 [256/118836 (0%)] Loss: 12371.035156\n",
      "Train Epoch: 666 [33024/118836 (28%)] Loss: 12347.026367\n",
      "Train Epoch: 666 [65792/118836 (55%)] Loss: 12224.448242\n",
      "Train Epoch: 666 [98560/118836 (83%)] Loss: 12309.894531\n",
      "    epoch          : 666\n",
      "    loss           : 12282.87435073537\n",
      "    val_loss       : 12281.476295827943\n",
      "    val_log_likelihood: -12186.538753941792\n",
      "    val_log_marginal: -12195.000678982788\n",
      "Train Epoch: 667 [256/118836 (0%)] Loss: 12348.355469\n",
      "Train Epoch: 667 [33024/118836 (28%)] Loss: 12429.486328\n",
      "Train Epoch: 667 [65792/118836 (55%)] Loss: 12368.006836\n",
      "Train Epoch: 667 [98560/118836 (83%)] Loss: 12273.660156\n",
      "    epoch          : 667\n",
      "    loss           : 12275.198579339847\n",
      "    val_loss       : 12283.187938290863\n",
      "    val_log_likelihood: -12188.039736481596\n",
      "    val_log_marginal: -12196.376565628487\n",
      "Train Epoch: 668 [256/118836 (0%)] Loss: 12323.720703\n",
      "Train Epoch: 668 [33024/118836 (28%)] Loss: 12295.452148\n",
      "Train Epoch: 668 [65792/118836 (55%)] Loss: 12202.427734\n",
      "Train Epoch: 668 [98560/118836 (83%)] Loss: 12236.089844\n",
      "    epoch          : 668\n",
      "    loss           : 12275.562406301695\n",
      "    val_loss       : 12289.482482884936\n",
      "    val_log_likelihood: -12189.115810296475\n",
      "    val_log_marginal: -12197.538164297044\n",
      "Train Epoch: 669 [256/118836 (0%)] Loss: 12264.207031\n",
      "Train Epoch: 669 [33024/118836 (28%)] Loss: 12239.155273\n",
      "Train Epoch: 669 [65792/118836 (55%)] Loss: 12321.098633\n",
      "Train Epoch: 669 [98560/118836 (83%)] Loss: 12291.576172\n",
      "    epoch          : 669\n",
      "    loss           : 12283.661714065085\n",
      "    val_loss       : 12284.73924313893\n",
      "    val_log_likelihood: -12190.185829585402\n",
      "    val_log_marginal: -12198.461736280686\n",
      "Train Epoch: 670 [256/118836 (0%)] Loss: 12242.546875\n",
      "Train Epoch: 670 [33024/118836 (28%)] Loss: 12342.929688\n",
      "Train Epoch: 670 [65792/118836 (55%)] Loss: 12350.854492\n",
      "Train Epoch: 670 [98560/118836 (83%)] Loss: 12255.951172\n",
      "    epoch          : 670\n",
      "    loss           : 12282.68778012562\n",
      "    val_loss       : 12276.412023903791\n",
      "    val_log_likelihood: -12189.421473712779\n",
      "    val_log_marginal: -12197.611111851005\n",
      "Train Epoch: 671 [256/118836 (0%)] Loss: 12290.343750\n",
      "Train Epoch: 671 [33024/118836 (28%)] Loss: 12286.188477\n",
      "Train Epoch: 671 [65792/118836 (55%)] Loss: 12216.803711\n",
      "Train Epoch: 671 [98560/118836 (83%)] Loss: 12252.011719\n",
      "    epoch          : 671\n",
      "    loss           : 12279.033180508168\n",
      "    val_loss       : 12280.824188692015\n",
      "    val_log_likelihood: -12184.63627562164\n",
      "    val_log_marginal: -12192.740443879453\n",
      "Train Epoch: 672 [256/118836 (0%)] Loss: 12278.199219\n",
      "Train Epoch: 672 [33024/118836 (28%)] Loss: 12348.688477\n",
      "Train Epoch: 672 [65792/118836 (55%)] Loss: 12292.029297\n",
      "Train Epoch: 672 [98560/118836 (83%)] Loss: 12307.351562\n",
      "    epoch          : 672\n",
      "    loss           : 12281.19960129756\n",
      "    val_loss       : 12281.851189794224\n",
      "    val_log_likelihood: -12183.012512439256\n",
      "    val_log_marginal: -12191.38670057217\n",
      "Train Epoch: 673 [256/118836 (0%)] Loss: 12334.747070\n",
      "Train Epoch: 673 [33024/118836 (28%)] Loss: 12289.400391\n",
      "Train Epoch: 673 [65792/118836 (55%)] Loss: 12262.758789\n",
      "Train Epoch: 673 [98560/118836 (83%)] Loss: 12313.945312\n",
      "    epoch          : 673\n",
      "    loss           : 12278.484850599669\n",
      "    val_loss       : 12275.76571107726\n",
      "    val_log_likelihood: -12185.669511541048\n",
      "    val_log_marginal: -12194.01847798688\n",
      "Train Epoch: 674 [256/118836 (0%)] Loss: 12230.965820\n",
      "Train Epoch: 674 [33024/118836 (28%)] Loss: 12225.674805\n",
      "Train Epoch: 674 [65792/118836 (55%)] Loss: 12229.765625\n",
      "Train Epoch: 674 [98560/118836 (83%)] Loss: 12323.371094\n",
      "    epoch          : 674\n",
      "    loss           : 12279.962017776828\n",
      "    val_loss       : 12276.44618956748\n",
      "    val_log_likelihood: -12184.166850670752\n",
      "    val_log_marginal: -12192.288547172371\n",
      "Train Epoch: 675 [256/118836 (0%)] Loss: 12287.138672\n",
      "Train Epoch: 675 [33024/118836 (28%)] Loss: 12419.407227\n",
      "Train Epoch: 675 [65792/118836 (55%)] Loss: 12208.411133\n",
      "Train Epoch: 675 [98560/118836 (83%)] Loss: 12295.444336\n",
      "    epoch          : 675\n",
      "    loss           : 12281.570364357165\n",
      "    val_loss       : 12279.897963504449\n",
      "    val_log_likelihood: -12181.36268335789\n",
      "    val_log_marginal: -12189.460672061832\n",
      "Train Epoch: 676 [256/118836 (0%)] Loss: 12336.427734\n",
      "Train Epoch: 676 [33024/118836 (28%)] Loss: 12314.978516\n",
      "Train Epoch: 676 [65792/118836 (55%)] Loss: 12299.996094\n",
      "Train Epoch: 676 [98560/118836 (83%)] Loss: 12309.173828\n",
      "    epoch          : 676\n",
      "    loss           : 12279.971754161497\n",
      "    val_loss       : 12282.70024447965\n",
      "    val_log_likelihood: -12183.931797488885\n",
      "    val_log_marginal: -12192.248437150862\n",
      "Train Epoch: 677 [256/118836 (0%)] Loss: 12302.144531\n",
      "Train Epoch: 677 [33024/118836 (28%)] Loss: 12320.279297\n",
      "Train Epoch: 677 [65792/118836 (55%)] Loss: 12364.438477\n",
      "Train Epoch: 677 [98560/118836 (83%)] Loss: 12313.052734\n",
      "    epoch          : 677\n",
      "    loss           : 12282.52544183597\n",
      "    val_loss       : 12278.582935519571\n",
      "    val_log_likelihood: -12185.556992962935\n",
      "    val_log_marginal: -12193.829176364216\n",
      "Train Epoch: 678 [256/118836 (0%)] Loss: 12297.214844\n",
      "Train Epoch: 678 [33024/118836 (28%)] Loss: 12367.134766\n",
      "Train Epoch: 678 [65792/118836 (55%)] Loss: 12249.745117\n",
      "Train Epoch: 678 [98560/118836 (83%)] Loss: 12295.420898\n",
      "    epoch          : 678\n",
      "    loss           : 12284.724172547043\n",
      "    val_loss       : 12278.759857751353\n",
      "    val_log_likelihood: -12181.432237870915\n",
      "    val_log_marginal: -12189.509514719537\n",
      "Train Epoch: 679 [256/118836 (0%)] Loss: 12248.986328\n",
      "Train Epoch: 679 [33024/118836 (28%)] Loss: 12330.144531\n",
      "Train Epoch: 679 [65792/118836 (55%)] Loss: 12313.997070\n",
      "Train Epoch: 679 [98560/118836 (83%)] Loss: 12338.310547\n",
      "    epoch          : 679\n",
      "    loss           : 12274.004309475806\n",
      "    val_loss       : 12281.007375787527\n",
      "    val_log_likelihood: -12180.6143117698\n",
      "    val_log_marginal: -12189.01170149525\n",
      "Train Epoch: 680 [256/118836 (0%)] Loss: 12320.147461\n",
      "Train Epoch: 680 [33024/118836 (28%)] Loss: 12360.451172\n",
      "Train Epoch: 680 [65792/118836 (55%)] Loss: 12364.145508\n",
      "Train Epoch: 680 [98560/118836 (83%)] Loss: 12336.747070\n",
      "    epoch          : 680\n",
      "    loss           : 12279.727957473893\n",
      "    val_loss       : 12278.095457303434\n",
      "    val_log_likelihood: -12185.182709754963\n",
      "    val_log_marginal: -12193.364672725858\n",
      "Train Epoch: 681 [256/118836 (0%)] Loss: 12346.208008\n",
      "Train Epoch: 681 [33024/118836 (28%)] Loss: 12324.256836\n",
      "Train Epoch: 681 [65792/118836 (55%)] Loss: 12332.233398\n",
      "Train Epoch: 681 [98560/118836 (83%)] Loss: 12264.217773\n",
      "    epoch          : 681\n",
      "    loss           : 12279.972629109801\n",
      "    val_loss       : 12280.53384346984\n",
      "    val_log_likelihood: -12182.634601814516\n",
      "    val_log_marginal: -12190.817343968993\n",
      "Train Epoch: 682 [256/118836 (0%)] Loss: 12166.517578\n",
      "Train Epoch: 682 [33024/118836 (28%)] Loss: 12270.032227\n",
      "Train Epoch: 682 [65792/118836 (55%)] Loss: 12365.495117\n",
      "Train Epoch: 682 [98560/118836 (83%)] Loss: 12193.380859\n",
      "    epoch          : 682\n",
      "    loss           : 12277.332159358199\n",
      "    val_loss       : 12275.620902766277\n",
      "    val_log_likelihood: -12182.05914996252\n",
      "    val_log_marginal: -12190.138679923528\n",
      "Train Epoch: 683 [256/118836 (0%)] Loss: 12237.470703\n",
      "Train Epoch: 683 [33024/118836 (28%)] Loss: 12346.485352\n",
      "Train Epoch: 683 [65792/118836 (55%)] Loss: 12242.212891\n",
      "Train Epoch: 683 [98560/118836 (83%)] Loss: 12297.264648\n",
      "    epoch          : 683\n",
      "    loss           : 12283.641152618382\n",
      "    val_loss       : 12285.875634857573\n",
      "    val_log_likelihood: -12191.1683256113\n",
      "    val_log_marginal: -12199.826463675148\n",
      "Train Epoch: 684 [256/118836 (0%)] Loss: 12241.825195\n",
      "Train Epoch: 684 [33024/118836 (28%)] Loss: 12370.904297\n",
      "Train Epoch: 684 [65792/118836 (55%)] Loss: 12301.841797\n",
      "Train Epoch: 684 [98560/118836 (83%)] Loss: 12213.688477\n",
      "    epoch          : 684\n",
      "    loss           : 12277.902868137406\n",
      "    val_loss       : 12278.019915594039\n",
      "    val_log_likelihood: -12182.87767637898\n",
      "    val_log_marginal: -12191.240052344883\n",
      "Train Epoch: 685 [256/118836 (0%)] Loss: 12293.604492\n",
      "Train Epoch: 685 [33024/118836 (28%)] Loss: 12215.291016\n",
      "Train Epoch: 685 [65792/118836 (55%)] Loss: 12303.728516\n",
      "Train Epoch: 685 [98560/118836 (83%)] Loss: 12215.094727\n",
      "    epoch          : 685\n",
      "    loss           : 12283.950299349928\n",
      "    val_loss       : 12274.252279633773\n",
      "    val_log_likelihood: -12180.307255156638\n",
      "    val_log_marginal: -12188.305086394534\n",
      "Train Epoch: 686 [256/118836 (0%)] Loss: 12371.872070\n",
      "Train Epoch: 686 [33024/118836 (28%)] Loss: 12292.873047\n",
      "Train Epoch: 686 [65792/118836 (55%)] Loss: 12343.100586\n",
      "Train Epoch: 686 [98560/118836 (83%)] Loss: 12219.356445\n",
      "    epoch          : 686\n",
      "    loss           : 12281.342013673491\n",
      "    val_loss       : 12277.945488004787\n",
      "    val_log_likelihood: -12184.656001861043\n",
      "    val_log_marginal: -12192.67215252287\n",
      "Train Epoch: 687 [256/118836 (0%)] Loss: 12292.791016\n",
      "Train Epoch: 687 [33024/118836 (28%)] Loss: 12323.529297\n",
      "Train Epoch: 687 [65792/118836 (55%)] Loss: 12271.379883\n",
      "Train Epoch: 687 [98560/118836 (83%)] Loss: 12277.083984\n",
      "    epoch          : 687\n",
      "    loss           : 12283.852537931658\n",
      "    val_loss       : 12289.615675736743\n",
      "    val_log_likelihood: -12197.236286768508\n",
      "    val_log_marginal: -12205.636531549091\n",
      "Train Epoch: 688 [256/118836 (0%)] Loss: 12278.662109\n",
      "Train Epoch: 688 [33024/118836 (28%)] Loss: 12313.979492\n",
      "Train Epoch: 688 [65792/118836 (55%)] Loss: 12262.201172\n",
      "Train Epoch: 688 [98560/118836 (83%)] Loss: 12332.123047\n",
      "    epoch          : 688\n",
      "    loss           : 12277.58888851194\n",
      "    val_loss       : 12277.152643800171\n",
      "    val_log_likelihood: -12185.05527069117\n",
      "    val_log_marginal: -12193.408465391098\n",
      "Train Epoch: 689 [256/118836 (0%)] Loss: 12315.389648\n",
      "Train Epoch: 689 [33024/118836 (28%)] Loss: 12296.075195\n",
      "Train Epoch: 689 [65792/118836 (55%)] Loss: 12308.797852\n",
      "Train Epoch: 689 [98560/118836 (83%)] Loss: 12271.697266\n",
      "    epoch          : 689\n",
      "    loss           : 12281.342282975342\n",
      "    val_loss       : 12280.320413922978\n",
      "    val_log_likelihood: -12183.452132444168\n",
      "    val_log_marginal: -12191.721104128283\n",
      "Train Epoch: 690 [256/118836 (0%)] Loss: 12341.246094\n",
      "Train Epoch: 690 [33024/118836 (28%)] Loss: 12227.539062\n",
      "Train Epoch: 690 [65792/118836 (55%)] Loss: 12353.970703\n",
      "Train Epoch: 690 [98560/118836 (83%)] Loss: 12313.184570\n",
      "    epoch          : 690\n",
      "    loss           : 12286.729017718673\n",
      "    val_loss       : 12274.722508658298\n",
      "    val_log_likelihood: -12185.3378436143\n",
      "    val_log_marginal: -12193.38248844284\n",
      "Train Epoch: 691 [256/118836 (0%)] Loss: 12267.470703\n",
      "Train Epoch: 691 [33024/118836 (28%)] Loss: 12274.420898\n",
      "Train Epoch: 691 [65792/118836 (55%)] Loss: 12279.130859\n",
      "Train Epoch: 691 [98560/118836 (83%)] Loss: 12344.800781\n",
      "    epoch          : 691\n",
      "    loss           : 12280.961041537428\n",
      "    val_loss       : 12277.531323069807\n",
      "    val_log_likelihood: -12181.705485389526\n",
      "    val_log_marginal: -12189.803370858654\n",
      "Train Epoch: 692 [256/118836 (0%)] Loss: 12267.402344\n",
      "Train Epoch: 692 [33024/118836 (28%)] Loss: 12267.468750\n",
      "Train Epoch: 692 [65792/118836 (55%)] Loss: 12342.909180\n",
      "Train Epoch: 692 [98560/118836 (83%)] Loss: 12296.829102\n",
      "    epoch          : 692\n",
      "    loss           : 12278.83988139087\n",
      "    val_loss       : 12275.690038315102\n",
      "    val_log_likelihood: -12180.034738161703\n",
      "    val_log_marginal: -12188.10585935819\n",
      "Train Epoch: 693 [256/118836 (0%)] Loss: 12313.453125\n",
      "Train Epoch: 693 [33024/118836 (28%)] Loss: 12175.988281\n",
      "Train Epoch: 693 [65792/118836 (55%)] Loss: 12352.935547\n",
      "Train Epoch: 693 [98560/118836 (83%)] Loss: 12291.588867\n",
      "    epoch          : 693\n",
      "    loss           : 12281.1503366677\n",
      "    val_loss       : 12279.859498730793\n",
      "    val_log_likelihood: -12184.272047372571\n",
      "    val_log_marginal: -12192.438730405394\n",
      "Train Epoch: 694 [256/118836 (0%)] Loss: 12307.140625\n",
      "Train Epoch: 694 [33024/118836 (28%)] Loss: 12441.320312\n",
      "Train Epoch: 694 [65792/118836 (55%)] Loss: 12233.622070\n",
      "Train Epoch: 694 [98560/118836 (83%)] Loss: 12244.994141\n",
      "    epoch          : 694\n",
      "    loss           : 12277.855471657878\n",
      "    val_loss       : 12287.965503969292\n",
      "    val_log_likelihood: -12187.824770762252\n",
      "    val_log_marginal: -12196.146032133132\n",
      "Train Epoch: 695 [256/118836 (0%)] Loss: 12240.129883\n",
      "Train Epoch: 695 [33024/118836 (28%)] Loss: 12336.384766\n",
      "Train Epoch: 695 [65792/118836 (55%)] Loss: 12275.546875\n",
      "Train Epoch: 695 [98560/118836 (83%)] Loss: 12279.435547\n",
      "    epoch          : 695\n",
      "    loss           : 12287.42561453164\n",
      "    val_loss       : 12284.159001589864\n",
      "    val_log_likelihood: -12180.795326716294\n",
      "    val_log_marginal: -12189.183621209415\n",
      "Train Epoch: 696 [256/118836 (0%)] Loss: 12248.974609\n",
      "Train Epoch: 696 [33024/118836 (28%)] Loss: 12242.668945\n",
      "Train Epoch: 696 [65792/118836 (55%)] Loss: 12260.995117\n",
      "Train Epoch: 696 [98560/118836 (83%)] Loss: 12435.968750\n",
      "    epoch          : 696\n",
      "    loss           : 12289.999645238835\n",
      "    val_loss       : 12281.832116478237\n",
      "    val_log_likelihood: -12183.31497977409\n",
      "    val_log_marginal: -12191.580821846452\n",
      "Train Epoch: 697 [256/118836 (0%)] Loss: 12283.080078\n",
      "Train Epoch: 697 [33024/118836 (28%)] Loss: 12406.739258\n",
      "Train Epoch: 697 [65792/118836 (55%)] Loss: 12244.911133\n",
      "Train Epoch: 697 [98560/118836 (83%)] Loss: 12262.983398\n",
      "    epoch          : 697\n",
      "    loss           : 12281.537099358975\n",
      "    val_loss       : 12288.890383608246\n",
      "    val_log_likelihood: -12185.138077213865\n",
      "    val_log_marginal: -12193.581336104238\n",
      "Train Epoch: 698 [256/118836 (0%)] Loss: 12303.051758\n",
      "Train Epoch: 698 [33024/118836 (28%)] Loss: 12272.384766\n",
      "Train Epoch: 698 [65792/118836 (55%)] Loss: 12261.408203\n",
      "Train Epoch: 698 [98560/118836 (83%)] Loss: 12236.815430\n",
      "    epoch          : 698\n",
      "    loss           : 12281.514645368074\n",
      "    val_loss       : 12278.628586368633\n",
      "    val_log_likelihood: -12184.534977415478\n",
      "    val_log_marginal: -12193.011434752596\n",
      "Train Epoch: 699 [256/118836 (0%)] Loss: 12326.418945\n",
      "Train Epoch: 699 [33024/118836 (28%)] Loss: 12337.364258\n",
      "Train Epoch: 699 [65792/118836 (55%)] Loss: 12306.146484\n",
      "Train Epoch: 699 [98560/118836 (83%)] Loss: 12322.853516\n",
      "    epoch          : 699\n",
      "    loss           : 12277.261264797871\n",
      "    val_loss       : 12280.117785966693\n",
      "    val_log_likelihood: -12182.75171193264\n",
      "    val_log_marginal: -12190.99880657741\n",
      "Train Epoch: 700 [256/118836 (0%)] Loss: 12258.016602\n",
      "Train Epoch: 700 [33024/118836 (28%)] Loss: 12248.168945\n",
      "Train Epoch: 700 [65792/118836 (55%)] Loss: 12229.400391\n",
      "Train Epoch: 700 [98560/118836 (83%)] Loss: 12361.208984\n",
      "    epoch          : 700\n",
      "    loss           : 12279.120726711124\n",
      "    val_loss       : 12282.194132505185\n",
      "    val_log_likelihood: -12182.249637807588\n",
      "    val_log_marginal: -12190.422096186112\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch700.pth ...\n",
      "Train Epoch: 701 [256/118836 (0%)] Loss: 12260.609375\n",
      "Train Epoch: 701 [33024/118836 (28%)] Loss: 12354.964844\n",
      "Train Epoch: 701 [65792/118836 (55%)] Loss: 12233.718750\n",
      "Train Epoch: 701 [98560/118836 (83%)] Loss: 12410.614258\n",
      "    epoch          : 701\n",
      "    loss           : 12282.045978888804\n",
      "    val_loss       : 12277.638524194428\n",
      "    val_log_likelihood: -12183.373524897903\n",
      "    val_log_marginal: -12191.503678277044\n",
      "Train Epoch: 702 [256/118836 (0%)] Loss: 12354.862305\n",
      "Train Epoch: 702 [33024/118836 (28%)] Loss: 12252.543945\n",
      "Train Epoch: 702 [65792/118836 (55%)] Loss: 12288.322266\n",
      "Train Epoch: 702 [98560/118836 (83%)] Loss: 12362.676758\n",
      "    epoch          : 702\n",
      "    loss           : 12278.761117626913\n",
      "    val_loss       : 12280.308838769399\n",
      "    val_log_likelihood: -12182.796320564516\n",
      "    val_log_marginal: -12190.953144088979\n",
      "Train Epoch: 703 [256/118836 (0%)] Loss: 12314.248047\n",
      "Train Epoch: 703 [33024/118836 (28%)] Loss: 12171.527344\n",
      "Train Epoch: 703 [65792/118836 (55%)] Loss: 12370.660156\n",
      "Train Epoch: 703 [98560/118836 (83%)] Loss: 12348.972656\n",
      "    epoch          : 703\n",
      "    loss           : 12277.135513919045\n",
      "    val_loss       : 12278.904793802945\n",
      "    val_log_likelihood: -12187.086162699028\n",
      "    val_log_marginal: -12195.417976631552\n",
      "Train Epoch: 704 [256/118836 (0%)] Loss: 12226.233398\n",
      "Train Epoch: 704 [33024/118836 (28%)] Loss: 12265.085938\n",
      "Train Epoch: 704 [65792/118836 (55%)] Loss: 12298.199219\n",
      "Train Epoch: 704 [98560/118836 (83%)] Loss: 12309.608398\n",
      "    epoch          : 704\n",
      "    loss           : 12285.076009033808\n",
      "    val_loss       : 12291.058720930469\n",
      "    val_log_likelihood: -12191.1840247622\n",
      "    val_log_marginal: -12199.928130281085\n",
      "Train Epoch: 705 [256/118836 (0%)] Loss: 12336.377930\n",
      "Train Epoch: 705 [33024/118836 (28%)] Loss: 12313.949219\n",
      "Train Epoch: 705 [65792/118836 (55%)] Loss: 12351.614258\n",
      "Train Epoch: 705 [98560/118836 (83%)] Loss: 12334.216797\n",
      "    epoch          : 705\n",
      "    loss           : 12276.91708087779\n",
      "    val_loss       : 12276.78975149357\n",
      "    val_log_likelihood: -12182.434876770574\n",
      "    val_log_marginal: -12190.740970545105\n",
      "Train Epoch: 706 [256/118836 (0%)] Loss: 12275.439453\n",
      "Train Epoch: 706 [33024/118836 (28%)] Loss: 12408.162109\n",
      "Train Epoch: 706 [65792/118836 (55%)] Loss: 12220.253906\n",
      "Train Epoch: 706 [98560/118836 (83%)] Loss: 12289.208008\n",
      "    epoch          : 706\n",
      "    loss           : 12276.246284216035\n",
      "    val_loss       : 12278.927974009335\n",
      "    val_log_likelihood: -12184.016382825424\n",
      "    val_log_marginal: -12192.141795434272\n",
      "Train Epoch: 707 [256/118836 (0%)] Loss: 12369.840820\n",
      "Train Epoch: 707 [33024/118836 (28%)] Loss: 12317.881836\n",
      "Train Epoch: 707 [65792/118836 (55%)] Loss: 12317.403320\n",
      "Train Epoch: 707 [98560/118836 (83%)] Loss: 12324.919922\n",
      "    epoch          : 707\n",
      "    loss           : 12276.953648741212\n",
      "    val_loss       : 12274.922597942394\n",
      "    val_log_likelihood: -12185.203479276519\n",
      "    val_log_marginal: -12193.299929505361\n",
      "Train Epoch: 708 [256/118836 (0%)] Loss: 12233.937500\n",
      "Train Epoch: 708 [33024/118836 (28%)] Loss: 12205.740234\n",
      "Train Epoch: 708 [65792/118836 (55%)] Loss: 12204.145508\n",
      "Train Epoch: 708 [98560/118836 (83%)] Loss: 12360.485352\n",
      "    epoch          : 708\n",
      "    loss           : 12280.611199855251\n",
      "    val_loss       : 12277.877781466219\n",
      "    val_log_likelihood: -12180.793072625878\n",
      "    val_log_marginal: -12188.925174884549\n",
      "Train Epoch: 709 [256/118836 (0%)] Loss: 12276.134766\n",
      "Train Epoch: 709 [33024/118836 (28%)] Loss: 12248.616211\n",
      "Train Epoch: 709 [65792/118836 (55%)] Loss: 12195.601562\n",
      "Train Epoch: 709 [98560/118836 (83%)] Loss: 12379.795898\n",
      "    epoch          : 709\n",
      "    loss           : 12284.897465460866\n",
      "    val_loss       : 12299.768582037714\n",
      "    val_log_likelihood: -12194.358266290581\n",
      "    val_log_marginal: -12203.24718238304\n",
      "Train Epoch: 710 [256/118836 (0%)] Loss: 12267.134766\n",
      "Train Epoch: 710 [33024/118836 (28%)] Loss: 12223.183594\n",
      "Train Epoch: 710 [65792/118836 (55%)] Loss: 12304.625977\n",
      "Train Epoch: 710 [98560/118836 (83%)] Loss: 12317.995117\n",
      "    epoch          : 710\n",
      "    loss           : 12285.409760778535\n",
      "    val_loss       : 12280.44510503731\n",
      "    val_log_likelihood: -12184.333989221464\n",
      "    val_log_marginal: -12192.577477352994\n",
      "Train Epoch: 711 [256/118836 (0%)] Loss: 12354.965820\n",
      "Train Epoch: 711 [33024/118836 (28%)] Loss: 12187.526367\n",
      "Train Epoch: 711 [65792/118836 (55%)] Loss: 12208.054688\n",
      "Train Epoch: 711 [98560/118836 (83%)] Loss: 12288.996094\n",
      "    epoch          : 711\n",
      "    loss           : 12283.598660275797\n",
      "    val_loss       : 12283.428882662725\n",
      "    val_log_likelihood: -12183.472198259149\n",
      "    val_log_marginal: -12191.581000263574\n",
      "Train Epoch: 712 [256/118836 (0%)] Loss: 12370.535156\n",
      "Train Epoch: 712 [33024/118836 (28%)] Loss: 12306.013672\n",
      "Train Epoch: 712 [65792/118836 (55%)] Loss: 12267.210938\n",
      "Train Epoch: 712 [98560/118836 (83%)] Loss: 12392.589844\n",
      "    epoch          : 712\n",
      "    loss           : 12277.432061782723\n",
      "    val_loss       : 12279.29436455635\n",
      "    val_log_likelihood: -12183.15593756462\n",
      "    val_log_marginal: -12191.649749484122\n",
      "Train Epoch: 713 [256/118836 (0%)] Loss: 12305.038086\n",
      "Train Epoch: 713 [33024/118836 (28%)] Loss: 12304.095703\n",
      "Train Epoch: 713 [65792/118836 (55%)] Loss: 12317.295898\n",
      "Train Epoch: 713 [98560/118836 (83%)] Loss: 12344.368164\n",
      "    epoch          : 713\n",
      "    loss           : 12276.035985479995\n",
      "    val_loss       : 12276.801683459527\n",
      "    val_log_likelihood: -12181.509183564673\n",
      "    val_log_marginal: -12189.729723765127\n",
      "Train Epoch: 714 [256/118836 (0%)] Loss: 12296.603516\n",
      "Train Epoch: 714 [33024/118836 (28%)] Loss: 12234.315430\n",
      "Train Epoch: 714 [65792/118836 (55%)] Loss: 12336.371094\n",
      "Train Epoch: 714 [98560/118836 (83%)] Loss: 12408.009766\n",
      "    epoch          : 714\n",
      "    loss           : 12279.125345068238\n",
      "    val_loss       : 12276.697296795668\n",
      "    val_log_likelihood: -12182.668469874381\n",
      "    val_log_marginal: -12190.703924197189\n",
      "Train Epoch: 715 [256/118836 (0%)] Loss: 12347.005859\n",
      "Train Epoch: 715 [33024/118836 (28%)] Loss: 12329.384766\n",
      "Train Epoch: 715 [65792/118836 (55%)] Loss: 12294.441406\n",
      "Train Epoch: 715 [98560/118836 (83%)] Loss: 12283.658203\n",
      "    epoch          : 715\n",
      "    loss           : 12277.201157497157\n",
      "    val_loss       : 12280.335511667401\n",
      "    val_log_likelihood: -12184.149841359078\n",
      "    val_log_marginal: -12192.505611128076\n",
      "Train Epoch: 716 [256/118836 (0%)] Loss: 12261.019531\n",
      "Train Epoch: 716 [33024/118836 (28%)] Loss: 12286.699219\n",
      "Train Epoch: 716 [65792/118836 (55%)] Loss: 12302.031250\n",
      "Train Epoch: 716 [98560/118836 (83%)] Loss: 12321.359375\n",
      "    epoch          : 716\n",
      "    loss           : 12282.332337546526\n",
      "    val_loss       : 12278.035533908565\n",
      "    val_log_likelihood: -12187.062740061518\n",
      "    val_log_marginal: -12195.147844491597\n",
      "Train Epoch: 717 [256/118836 (0%)] Loss: 12397.892578\n",
      "Train Epoch: 717 [33024/118836 (28%)] Loss: 12221.393555\n",
      "Train Epoch: 717 [65792/118836 (55%)] Loss: 12295.040039\n",
      "Train Epoch: 717 [98560/118836 (83%)] Loss: 12285.454102\n",
      "    epoch          : 717\n",
      "    loss           : 12276.980934010546\n",
      "    val_loss       : 12280.073980730709\n",
      "    val_log_likelihood: -12183.465920472756\n",
      "    val_log_marginal: -12191.691422812437\n",
      "Train Epoch: 718 [256/118836 (0%)] Loss: 12216.781250\n",
      "Train Epoch: 718 [33024/118836 (28%)] Loss: 12291.505859\n",
      "Train Epoch: 718 [65792/118836 (55%)] Loss: 12300.911133\n",
      "Train Epoch: 718 [98560/118836 (83%)] Loss: 12234.963867\n",
      "    epoch          : 718\n",
      "    loss           : 12274.632243202026\n",
      "    val_loss       : 12281.912783504618\n",
      "    val_log_likelihood: -12185.683859174678\n",
      "    val_log_marginal: -12193.810928587101\n",
      "Train Epoch: 719 [256/118836 (0%)] Loss: 12221.378906\n",
      "Train Epoch: 719 [33024/118836 (28%)] Loss: 12309.132812\n",
      "Train Epoch: 719 [65792/118836 (55%)] Loss: 12297.781250\n",
      "Train Epoch: 719 [98560/118836 (83%)] Loss: 12232.830078\n",
      "    epoch          : 719\n",
      "    loss           : 12281.9860586616\n",
      "    val_loss       : 12278.790666301054\n",
      "    val_log_likelihood: -12184.737417771661\n",
      "    val_log_marginal: -12192.91953471597\n",
      "Train Epoch: 720 [256/118836 (0%)] Loss: 12260.017578\n",
      "Train Epoch: 720 [33024/118836 (28%)] Loss: 12274.242188\n",
      "Train Epoch: 720 [65792/118836 (55%)] Loss: 12278.849609\n",
      "Train Epoch: 720 [98560/118836 (83%)] Loss: 12230.205078\n",
      "    epoch          : 720\n",
      "    loss           : 12282.550877533085\n",
      "    val_loss       : 12281.599816438691\n",
      "    val_log_likelihood: -12179.058073078217\n",
      "    val_log_marginal: -12187.256595827817\n",
      "Train Epoch: 721 [256/118836 (0%)] Loss: 12365.705078\n",
      "Train Epoch: 721 [33024/118836 (28%)] Loss: 12305.638672\n",
      "Train Epoch: 721 [65792/118836 (55%)] Loss: 12343.640625\n",
      "Train Epoch: 721 [98560/118836 (83%)] Loss: 12301.220703\n",
      "    epoch          : 721\n",
      "    loss           : 12284.186422631048\n",
      "    val_loss       : 12280.931468240453\n",
      "    val_log_likelihood: -12181.627689464434\n",
      "    val_log_marginal: -12189.8612559627\n",
      "Train Epoch: 722 [256/118836 (0%)] Loss: 12282.467773\n",
      "Train Epoch: 722 [33024/118836 (28%)] Loss: 12283.566406\n",
      "Train Epoch: 722 [65792/118836 (55%)] Loss: 12236.972656\n",
      "Train Epoch: 722 [98560/118836 (83%)] Loss: 12309.923828\n",
      "    epoch          : 722\n",
      "    loss           : 12275.25976804823\n",
      "    val_loss       : 12276.51803756143\n",
      "    val_log_likelihood: -12181.999947658189\n",
      "    val_log_marginal: -12190.088521110494\n",
      "Train Epoch: 723 [256/118836 (0%)] Loss: 12288.250000\n",
      "Train Epoch: 723 [33024/118836 (28%)] Loss: 12225.583008\n",
      "Train Epoch: 723 [65792/118836 (55%)] Loss: 12356.704102\n",
      "Train Epoch: 723 [98560/118836 (83%)] Loss: 12221.803711\n",
      "    epoch          : 723\n",
      "    loss           : 12280.96085123294\n",
      "    val_loss       : 12280.574325104502\n",
      "    val_log_likelihood: -12183.021641561983\n",
      "    val_log_marginal: -12191.295712534216\n",
      "Train Epoch: 724 [256/118836 (0%)] Loss: 12234.939453\n",
      "Train Epoch: 724 [33024/118836 (28%)] Loss: 12327.322266\n",
      "Train Epoch: 724 [65792/118836 (55%)] Loss: 12268.599609\n",
      "Train Epoch: 724 [98560/118836 (83%)] Loss: 12364.284180\n",
      "    epoch          : 724\n",
      "    loss           : 12278.094422366108\n",
      "    val_loss       : 12275.78858378118\n",
      "    val_log_likelihood: -12179.923921015561\n",
      "    val_log_marginal: -12188.01258075059\n",
      "Train Epoch: 725 [256/118836 (0%)] Loss: 12389.298828\n",
      "Train Epoch: 725 [33024/118836 (28%)] Loss: 12203.158203\n",
      "Train Epoch: 725 [65792/118836 (55%)] Loss: 12271.208008\n",
      "Train Epoch: 725 [98560/118836 (83%)] Loss: 12255.378906\n",
      "    epoch          : 725\n",
      "    loss           : 12279.74908514914\n",
      "    val_loss       : 12278.964570334972\n",
      "    val_log_likelihood: -12180.008496982267\n",
      "    val_log_marginal: -12188.151488099422\n",
      "Train Epoch: 726 [256/118836 (0%)] Loss: 12313.063477\n",
      "Train Epoch: 726 [33024/118836 (28%)] Loss: 12404.253906\n",
      "Train Epoch: 726 [65792/118836 (55%)] Loss: 12320.415039\n",
      "Train Epoch: 726 [98560/118836 (83%)] Loss: 12197.837891\n",
      "    epoch          : 726\n",
      "    loss           : 12281.009173548646\n",
      "    val_loss       : 12277.435549171036\n",
      "    val_log_likelihood: -12178.737699189673\n",
      "    val_log_marginal: -12186.748071863618\n",
      "Train Epoch: 727 [256/118836 (0%)] Loss: 12254.944336\n",
      "Train Epoch: 727 [33024/118836 (28%)] Loss: 12323.486328\n",
      "Train Epoch: 727 [65792/118836 (55%)] Loss: 12363.531250\n",
      "Train Epoch: 727 [98560/118836 (83%)] Loss: 12326.111328\n",
      "    epoch          : 727\n",
      "    loss           : 12283.077455864866\n",
      "    val_loss       : 12272.18364019221\n",
      "    val_log_likelihood: -12182.245396828474\n",
      "    val_log_marginal: -12190.366794255502\n",
      "Train Epoch: 728 [256/118836 (0%)] Loss: 12193.720703\n",
      "Train Epoch: 728 [33024/118836 (28%)] Loss: 12406.844727\n",
      "Train Epoch: 728 [65792/118836 (55%)] Loss: 12283.558594\n",
      "Train Epoch: 728 [98560/118836 (83%)] Loss: 12242.679688\n",
      "    epoch          : 728\n",
      "    loss           : 12280.46162327466\n",
      "    val_loss       : 12278.112428967419\n",
      "    val_log_likelihood: -12178.5630319802\n",
      "    val_log_marginal: -12186.669462584126\n",
      "Train Epoch: 729 [256/118836 (0%)] Loss: 12229.394531\n",
      "Train Epoch: 729 [33024/118836 (28%)] Loss: 12482.986328\n",
      "Train Epoch: 729 [65792/118836 (55%)] Loss: 12504.113281\n",
      "Train Epoch: 729 [98560/118836 (83%)] Loss: 12387.542969\n",
      "    epoch          : 729\n",
      "    loss           : 12280.865946643662\n",
      "    val_loss       : 12282.18032129672\n",
      "    val_log_likelihood: -12185.587422456576\n",
      "    val_log_marginal: -12193.984631195684\n",
      "Train Epoch: 730 [256/118836 (0%)] Loss: 12332.666016\n",
      "Train Epoch: 730 [33024/118836 (28%)] Loss: 12222.712891\n",
      "Train Epoch: 730 [65792/118836 (55%)] Loss: 12424.439453\n",
      "Train Epoch: 730 [98560/118836 (83%)] Loss: 12273.778320\n",
      "    epoch          : 730\n",
      "    loss           : 12281.083365319995\n",
      "    val_loss       : 12274.413743328705\n",
      "    val_log_likelihood: -12182.079005602513\n",
      "    val_log_marginal: -12190.356154587913\n",
      "Train Epoch: 731 [256/118836 (0%)] Loss: 12414.221680\n",
      "Train Epoch: 731 [33024/118836 (28%)] Loss: 12221.144531\n",
      "Train Epoch: 731 [65792/118836 (55%)] Loss: 12256.738281\n",
      "Train Epoch: 731 [98560/118836 (83%)] Loss: 12300.795898\n",
      "    epoch          : 731\n",
      "    loss           : 12278.728344221721\n",
      "    val_loss       : 12275.419192417652\n",
      "    val_log_likelihood: -12183.562902256514\n",
      "    val_log_marginal: -12191.68336208944\n",
      "Train Epoch: 732 [256/118836 (0%)] Loss: 12241.167969\n",
      "Train Epoch: 732 [33024/118836 (28%)] Loss: 12417.019531\n",
      "Train Epoch: 732 [65792/118836 (55%)] Loss: 12400.386719\n",
      "Train Epoch: 732 [98560/118836 (83%)] Loss: 12218.635742\n",
      "    epoch          : 732\n",
      "    loss           : 12274.514238588194\n",
      "    val_loss       : 12279.329002813738\n",
      "    val_log_likelihood: -12183.238846024606\n",
      "    val_log_marginal: -12191.36144926322\n",
      "Train Epoch: 733 [256/118836 (0%)] Loss: 12218.541016\n",
      "Train Epoch: 733 [33024/118836 (28%)] Loss: 12216.623047\n",
      "Train Epoch: 733 [65792/118836 (55%)] Loss: 12332.797852\n",
      "Train Epoch: 733 [98560/118836 (83%)] Loss: 12328.846680\n",
      "    epoch          : 733\n",
      "    loss           : 12284.822165303194\n",
      "    val_loss       : 12278.972350139114\n",
      "    val_log_likelihood: -12184.110566583953\n",
      "    val_log_marginal: -12192.675758699981\n",
      "Train Epoch: 734 [256/118836 (0%)] Loss: 12241.798828\n",
      "Train Epoch: 734 [33024/118836 (28%)] Loss: 12234.421875\n",
      "Train Epoch: 734 [65792/118836 (55%)] Loss: 12339.324219\n",
      "Train Epoch: 734 [98560/118836 (83%)] Loss: 12260.336914\n",
      "    epoch          : 734\n",
      "    loss           : 12280.609977577025\n",
      "    val_loss       : 12278.973549195334\n",
      "    val_log_likelihood: -12179.371688572684\n",
      "    val_log_marginal: -12187.696243816692\n",
      "Train Epoch: 735 [256/118836 (0%)] Loss: 12285.275391\n",
      "Train Epoch: 735 [33024/118836 (28%)] Loss: 12345.199219\n",
      "Train Epoch: 735 [65792/118836 (55%)] Loss: 12168.426758\n",
      "Train Epoch: 735 [98560/118836 (83%)] Loss: 12362.181641\n",
      "    epoch          : 735\n",
      "    loss           : 12286.348007457093\n",
      "    val_loss       : 12283.814903063847\n",
      "    val_log_likelihood: -12198.402579288151\n",
      "    val_log_marginal: -12207.019797467488\n",
      "Train Epoch: 736 [256/118836 (0%)] Loss: 12362.011719\n",
      "Train Epoch: 736 [33024/118836 (28%)] Loss: 12307.554688\n",
      "Train Epoch: 736 [65792/118836 (55%)] Loss: 12202.535156\n",
      "Train Epoch: 736 [98560/118836 (83%)] Loss: 12318.408203\n",
      "    epoch          : 736\n",
      "    loss           : 12281.68700436828\n",
      "    val_loss       : 12281.319909304924\n",
      "    val_log_likelihood: -12185.085653981854\n",
      "    val_log_marginal: -12193.50296331745\n",
      "Train Epoch: 737 [256/118836 (0%)] Loss: 12275.536133\n",
      "Train Epoch: 737 [33024/118836 (28%)] Loss: 12441.998047\n",
      "Train Epoch: 737 [65792/118836 (55%)] Loss: 12237.040039\n",
      "Train Epoch: 737 [98560/118836 (83%)] Loss: 12282.208984\n",
      "    epoch          : 737\n",
      "    loss           : 12273.805787485784\n",
      "    val_loss       : 12281.297614583316\n",
      "    val_log_likelihood: -12181.645493596205\n",
      "    val_log_marginal: -12189.748394723807\n",
      "Train Epoch: 738 [256/118836 (0%)] Loss: 12314.834961\n",
      "Train Epoch: 738 [33024/118836 (28%)] Loss: 12341.873047\n",
      "Train Epoch: 738 [65792/118836 (55%)] Loss: 12463.066406\n",
      "Train Epoch: 738 [98560/118836 (83%)] Loss: 12296.216797\n",
      "    epoch          : 738\n",
      "    loss           : 12279.895778729839\n",
      "    val_loss       : 12282.12939294584\n",
      "    val_log_likelihood: -12185.222671435587\n",
      "    val_log_marginal: -12193.376710383021\n",
      "Train Epoch: 739 [256/118836 (0%)] Loss: 12338.272461\n",
      "Train Epoch: 739 [33024/118836 (28%)] Loss: 12297.890625\n",
      "Train Epoch: 739 [65792/118836 (55%)] Loss: 12255.553711\n",
      "Train Epoch: 739 [98560/118836 (83%)] Loss: 12440.750000\n",
      "    epoch          : 739\n",
      "    loss           : 12277.728959238006\n",
      "    val_loss       : 12278.467491544458\n",
      "    val_log_likelihood: -12184.394192482165\n",
      "    val_log_marginal: -12192.913216119654\n",
      "Train Epoch: 740 [256/118836 (0%)] Loss: 12437.473633\n",
      "Train Epoch: 740 [33024/118836 (28%)] Loss: 12299.647461\n",
      "Train Epoch: 740 [65792/118836 (55%)] Loss: 12302.582031\n",
      "Train Epoch: 740 [98560/118836 (83%)] Loss: 12320.387695\n",
      "    epoch          : 740\n",
      "    loss           : 12282.666610286135\n",
      "    val_loss       : 12276.113508255836\n",
      "    val_log_likelihood: -12179.37137936828\n",
      "    val_log_marginal: -12187.577920823202\n",
      "Train Epoch: 741 [256/118836 (0%)] Loss: 12281.886719\n",
      "Train Epoch: 741 [33024/118836 (28%)] Loss: 12285.206055\n",
      "Train Epoch: 741 [65792/118836 (55%)] Loss: 12432.781250\n",
      "Train Epoch: 741 [98560/118836 (83%)] Loss: 12257.287109\n",
      "    epoch          : 741\n",
      "    loss           : 12273.590972653019\n",
      "    val_loss       : 12273.57914044479\n",
      "    val_log_likelihood: -12183.72450695306\n",
      "    val_log_marginal: -12191.756010387979\n",
      "Train Epoch: 742 [256/118836 (0%)] Loss: 12252.881836\n",
      "Train Epoch: 742 [33024/118836 (28%)] Loss: 12319.382812\n",
      "Train Epoch: 742 [65792/118836 (55%)] Loss: 12279.355469\n",
      "Train Epoch: 742 [98560/118836 (83%)] Loss: 12203.821289\n",
      "    epoch          : 742\n",
      "    loss           : 12282.59076086254\n",
      "    val_loss       : 12280.390123281872\n",
      "    val_log_likelihood: -12183.110557698768\n",
      "    val_log_marginal: -12191.674048021845\n",
      "Train Epoch: 743 [256/118836 (0%)] Loss: 12384.441406\n",
      "Train Epoch: 743 [33024/118836 (28%)] Loss: 12303.136719\n",
      "Train Epoch: 743 [65792/118836 (55%)] Loss: 12239.454102\n",
      "Train Epoch: 743 [98560/118836 (83%)] Loss: 12325.256836\n",
      "    epoch          : 743\n",
      "    loss           : 12280.61582838994\n",
      "    val_loss       : 12278.221343631543\n",
      "    val_log_likelihood: -12179.358005873915\n",
      "    val_log_marginal: -12187.565156670567\n",
      "Train Epoch: 744 [256/118836 (0%)] Loss: 12366.505859\n",
      "Train Epoch: 744 [33024/118836 (28%)] Loss: 12308.158203\n",
      "Train Epoch: 744 [65792/118836 (55%)] Loss: 12291.213867\n",
      "Train Epoch: 744 [98560/118836 (83%)] Loss: 12291.430664\n",
      "    epoch          : 744\n",
      "    loss           : 12278.58356547896\n",
      "    val_loss       : 12274.929597371734\n",
      "    val_log_likelihood: -12183.153971838812\n",
      "    val_log_marginal: -12191.56700527935\n",
      "Train Epoch: 745 [256/118836 (0%)] Loss: 12417.418945\n",
      "Train Epoch: 745 [33024/118836 (28%)] Loss: 12352.459961\n",
      "Train Epoch: 745 [65792/118836 (55%)] Loss: 12328.037109\n",
      "Train Epoch: 745 [98560/118836 (83%)] Loss: 12390.535156\n",
      "    epoch          : 745\n",
      "    loss           : 12277.727836473843\n",
      "    val_loss       : 12280.410420367396\n",
      "    val_log_likelihood: -12180.608588095793\n",
      "    val_log_marginal: -12188.571366326843\n",
      "Train Epoch: 746 [256/118836 (0%)] Loss: 12399.359375\n",
      "Train Epoch: 746 [33024/118836 (28%)] Loss: 12338.484375\n",
      "Train Epoch: 746 [65792/118836 (55%)] Loss: 12273.633789\n",
      "Train Epoch: 746 [98560/118836 (83%)] Loss: 12226.212891\n",
      "    epoch          : 746\n",
      "    loss           : 12275.78466045673\n",
      "    val_loss       : 12277.030551164298\n",
      "    val_log_likelihood: -12181.473887251861\n",
      "    val_log_marginal: -12189.7228325285\n",
      "Train Epoch: 747 [256/118836 (0%)] Loss: 12254.558594\n",
      "Train Epoch: 747 [33024/118836 (28%)] Loss: 12299.198242\n",
      "Train Epoch: 747 [65792/118836 (55%)] Loss: 12271.662109\n",
      "Train Epoch: 747 [98560/118836 (83%)] Loss: 12301.694336\n",
      "    epoch          : 747\n",
      "    loss           : 12281.174896285671\n",
      "    val_loss       : 12280.489402199702\n",
      "    val_log_likelihood: -12180.231365668942\n",
      "    val_log_marginal: -12188.28402807276\n",
      "Train Epoch: 748 [256/118836 (0%)] Loss: 12338.230469\n",
      "Train Epoch: 748 [33024/118836 (28%)] Loss: 12280.516602\n",
      "Train Epoch: 748 [65792/118836 (55%)] Loss: 12369.322266\n",
      "Train Epoch: 748 [98560/118836 (83%)] Loss: 12279.749023\n",
      "    epoch          : 748\n",
      "    loss           : 12281.122806328836\n",
      "    val_loss       : 12276.574247934781\n",
      "    val_log_likelihood: -12181.07871643016\n",
      "    val_log_marginal: -12188.993293121734\n",
      "Train Epoch: 749 [256/118836 (0%)] Loss: 12458.992188\n",
      "Train Epoch: 749 [33024/118836 (28%)] Loss: 12233.685547\n",
      "Train Epoch: 749 [65792/118836 (55%)] Loss: 12335.082031\n",
      "Train Epoch: 749 [98560/118836 (83%)] Loss: 12346.042969\n",
      "    epoch          : 749\n",
      "    loss           : 12277.12234219913\n",
      "    val_loss       : 12274.393885426258\n",
      "    val_log_likelihood: -12180.405697018456\n",
      "    val_log_marginal: -12188.477881819459\n",
      "Train Epoch: 750 [256/118836 (0%)] Loss: 12278.224609\n",
      "Train Epoch: 750 [33024/118836 (28%)] Loss: 12294.612305\n",
      "Train Epoch: 750 [65792/118836 (55%)] Loss: 12224.482422\n",
      "Train Epoch: 750 [98560/118836 (83%)] Loss: 12293.179688\n",
      "    epoch          : 750\n",
      "    loss           : 12283.282478255533\n",
      "    val_loss       : 12285.004896727865\n",
      "    val_log_likelihood: -12184.016876841657\n",
      "    val_log_marginal: -12192.766900946708\n",
      "Train Epoch: 751 [256/118836 (0%)] Loss: 12462.763672\n",
      "Train Epoch: 751 [33024/118836 (28%)] Loss: 12271.400391\n",
      "Train Epoch: 751 [65792/118836 (55%)] Loss: 12355.029297\n",
      "Train Epoch: 751 [98560/118836 (83%)] Loss: 12309.473633\n",
      "    epoch          : 751\n",
      "    loss           : 12278.517891691223\n",
      "    val_loss       : 12273.20743832798\n",
      "    val_log_likelihood: -12183.02971140922\n",
      "    val_log_marginal: -12191.193850749409\n",
      "Train Epoch: 752 [256/118836 (0%)] Loss: 12221.136719\n",
      "Train Epoch: 752 [33024/118836 (28%)] Loss: 12291.620117\n",
      "Train Epoch: 752 [65792/118836 (55%)] Loss: 12284.343750\n",
      "Train Epoch: 752 [98560/118836 (83%)] Loss: 12413.987305\n",
      "    epoch          : 752\n",
      "    loss           : 12278.031351614194\n",
      "    val_loss       : 12279.526245989598\n",
      "    val_log_likelihood: -12180.74897077259\n",
      "    val_log_marginal: -12188.905849798886\n",
      "Train Epoch: 753 [256/118836 (0%)] Loss: 12315.791992\n",
      "Train Epoch: 753 [33024/118836 (28%)] Loss: 12232.585938\n",
      "Train Epoch: 753 [65792/118836 (55%)] Loss: 12216.459961\n",
      "Train Epoch: 753 [98560/118836 (83%)] Loss: 12254.067383\n",
      "    epoch          : 753\n",
      "    loss           : 12279.009764494158\n",
      "    val_loss       : 12277.172539919145\n",
      "    val_log_likelihood: -12181.622306981493\n",
      "    val_log_marginal: -12189.670318580082\n",
      "Train Epoch: 754 [256/118836 (0%)] Loss: 12341.113281\n",
      "Train Epoch: 754 [33024/118836 (28%)] Loss: 12318.967773\n",
      "Train Epoch: 754 [65792/118836 (55%)] Loss: 12297.933594\n",
      "Train Epoch: 754 [98560/118836 (83%)] Loss: 12333.041992\n",
      "    epoch          : 754\n",
      "    loss           : 12281.587893855976\n",
      "    val_loss       : 12276.089150302008\n",
      "    val_log_likelihood: -12178.024697903742\n",
      "    val_log_marginal: -12186.076779191775\n",
      "Train Epoch: 755 [256/118836 (0%)] Loss: 12337.937500\n",
      "Train Epoch: 755 [33024/118836 (28%)] Loss: 12355.810547\n",
      "Train Epoch: 755 [65792/118836 (55%)] Loss: 12203.724609\n",
      "Train Epoch: 755 [98560/118836 (83%)] Loss: 12271.763672\n",
      "    epoch          : 755\n",
      "    loss           : 12276.962342005274\n",
      "    val_loss       : 12285.2118489433\n",
      "    val_log_likelihood: -12184.651408220896\n",
      "    val_log_marginal: -12193.333076636061\n",
      "Train Epoch: 756 [256/118836 (0%)] Loss: 12331.810547\n",
      "Train Epoch: 756 [33024/118836 (28%)] Loss: 12284.659180\n",
      "Train Epoch: 756 [65792/118836 (55%)] Loss: 12274.615234\n",
      "Train Epoch: 756 [98560/118836 (83%)] Loss: 12283.695312\n",
      "    epoch          : 756\n",
      "    loss           : 12278.661996775485\n",
      "    val_loss       : 12283.883798936811\n",
      "    val_log_likelihood: -12185.381388931968\n",
      "    val_log_marginal: -12194.016715889347\n",
      "Train Epoch: 757 [256/118836 (0%)] Loss: 12351.694336\n",
      "Train Epoch: 757 [33024/118836 (28%)] Loss: 12306.552734\n",
      "Train Epoch: 757 [65792/118836 (55%)] Loss: 12224.740234\n",
      "Train Epoch: 757 [98560/118836 (83%)] Loss: 12275.998047\n",
      "    epoch          : 757\n",
      "    loss           : 12280.119617840157\n",
      "    val_loss       : 12280.980032002288\n",
      "    val_log_likelihood: -12182.130476181244\n",
      "    val_log_marginal: -12190.414239220941\n",
      "Train Epoch: 758 [256/118836 (0%)] Loss: 12238.292969\n",
      "Train Epoch: 758 [33024/118836 (28%)] Loss: 12440.823242\n",
      "Train Epoch: 758 [65792/118836 (55%)] Loss: 12328.069336\n",
      "Train Epoch: 758 [98560/118836 (83%)] Loss: 12291.136719\n",
      "    epoch          : 758\n",
      "    loss           : 12283.605141775228\n",
      "    val_loss       : 12280.94109689915\n",
      "    val_log_likelihood: -12181.796945112179\n",
      "    val_log_marginal: -12189.795357502026\n",
      "Train Epoch: 759 [256/118836 (0%)] Loss: 12298.775391\n",
      "Train Epoch: 759 [33024/118836 (28%)] Loss: 12277.429688\n",
      "Train Epoch: 759 [65792/118836 (55%)] Loss: 12306.587891\n",
      "Train Epoch: 759 [98560/118836 (83%)] Loss: 12224.212891\n",
      "    epoch          : 759\n",
      "    loss           : 12275.773418275694\n",
      "    val_loss       : 12277.74943147064\n",
      "    val_log_likelihood: -12182.001060567876\n",
      "    val_log_marginal: -12190.017955040794\n",
      "Train Epoch: 760 [256/118836 (0%)] Loss: 12252.541016\n",
      "Train Epoch: 760 [33024/118836 (28%)] Loss: 12203.577148\n",
      "Train Epoch: 760 [65792/118836 (55%)] Loss: 12211.371094\n",
      "Train Epoch: 760 [98560/118836 (83%)] Loss: 12361.768555\n",
      "    epoch          : 760\n",
      "    loss           : 12277.248811323925\n",
      "    val_loss       : 12274.33368648425\n",
      "    val_log_likelihood: -12180.75765628231\n",
      "    val_log_marginal: -12188.919239816534\n",
      "Train Epoch: 761 [256/118836 (0%)] Loss: 12292.539062\n",
      "Train Epoch: 761 [33024/118836 (28%)] Loss: 12275.849609\n",
      "Train Epoch: 761 [65792/118836 (55%)] Loss: 12322.918945\n",
      "Train Epoch: 761 [98560/118836 (83%)] Loss: 12247.692383\n",
      "    epoch          : 761\n",
      "    loss           : 12278.756806697167\n",
      "    val_loss       : 12279.853425903331\n",
      "    val_log_likelihood: -12181.440952782517\n",
      "    val_log_marginal: -12189.517587705704\n",
      "Train Epoch: 762 [256/118836 (0%)] Loss: 12356.188477\n",
      "Train Epoch: 762 [33024/118836 (28%)] Loss: 12291.252930\n",
      "Train Epoch: 762 [65792/118836 (55%)] Loss: 12242.945312\n",
      "Train Epoch: 762 [98560/118836 (83%)] Loss: 12259.701172\n",
      "    epoch          : 762\n",
      "    loss           : 12280.030909778226\n",
      "    val_loss       : 12279.718833560382\n",
      "    val_log_likelihood: -12178.207076322115\n",
      "    val_log_marginal: -12186.233241425942\n",
      "Train Epoch: 763 [256/118836 (0%)] Loss: 12404.470703\n",
      "Train Epoch: 763 [33024/118836 (28%)] Loss: 12349.079102\n",
      "Train Epoch: 763 [65792/118836 (55%)] Loss: 12241.362305\n",
      "Train Epoch: 763 [98560/118836 (83%)] Loss: 12322.949219\n",
      "    epoch          : 763\n",
      "    loss           : 12277.495172114091\n",
      "    val_loss       : 12280.781776259539\n",
      "    val_log_likelihood: -12182.584897287272\n",
      "    val_log_marginal: -12190.56556500753\n",
      "Train Epoch: 764 [256/118836 (0%)] Loss: 12245.784180\n",
      "Train Epoch: 764 [33024/118836 (28%)] Loss: 12423.508789\n",
      "Train Epoch: 764 [65792/118836 (55%)] Loss: 12273.778320\n",
      "Train Epoch: 764 [98560/118836 (83%)] Loss: 12259.232422\n",
      "    epoch          : 764\n",
      "    loss           : 12281.070327039391\n",
      "    val_loss       : 12277.229212861654\n",
      "    val_log_likelihood: -12180.253693651779\n",
      "    val_log_marginal: -12188.241888228533\n",
      "Train Epoch: 765 [256/118836 (0%)] Loss: 12278.352539\n",
      "Train Epoch: 765 [33024/118836 (28%)] Loss: 12247.449219\n",
      "Train Epoch: 765 [65792/118836 (55%)] Loss: 12222.849609\n",
      "Train Epoch: 765 [98560/118836 (83%)] Loss: 12225.822266\n",
      "    epoch          : 765\n",
      "    loss           : 12275.893322218777\n",
      "    val_loss       : 12282.168895168823\n",
      "    val_log_likelihood: -12180.642408983405\n",
      "    val_log_marginal: -12188.692759147913\n",
      "Train Epoch: 766 [256/118836 (0%)] Loss: 12245.990234\n",
      "Train Epoch: 766 [33024/118836 (28%)] Loss: 12207.012695\n",
      "Train Epoch: 766 [65792/118836 (55%)] Loss: 12304.981445\n",
      "Train Epoch: 766 [98560/118836 (83%)] Loss: 12263.490234\n",
      "    epoch          : 766\n",
      "    loss           : 12276.34136990152\n",
      "    val_loss       : 12279.949739133466\n",
      "    val_log_likelihood: -12179.38238278019\n",
      "    val_log_marginal: -12187.63457477371\n",
      "Train Epoch: 767 [256/118836 (0%)] Loss: 12351.687500\n",
      "Train Epoch: 767 [33024/118836 (28%)] Loss: 12430.376953\n",
      "Train Epoch: 767 [65792/118836 (55%)] Loss: 12354.056641\n",
      "Train Epoch: 767 [98560/118836 (83%)] Loss: 12298.669922\n",
      "    epoch          : 767\n",
      "    loss           : 12280.712410501963\n",
      "    val_loss       : 12274.880741142872\n",
      "    val_log_likelihood: -12181.90706889087\n",
      "    val_log_marginal: -12189.959518018473\n",
      "Train Epoch: 768 [256/118836 (0%)] Loss: 12223.166016\n",
      "Train Epoch: 768 [33024/118836 (28%)] Loss: 12361.357422\n",
      "Train Epoch: 768 [65792/118836 (55%)] Loss: 12242.055664\n",
      "Train Epoch: 768 [98560/118836 (83%)] Loss: 12282.367188\n",
      "    epoch          : 768\n",
      "    loss           : 12277.886341210453\n",
      "    val_loss       : 12275.703432864555\n",
      "    val_log_likelihood: -12181.973138150071\n",
      "    val_log_marginal: -12190.304048994794\n",
      "Train Epoch: 769 [256/118836 (0%)] Loss: 12340.445312\n",
      "Train Epoch: 769 [33024/118836 (28%)] Loss: 12207.811523\n",
      "Train Epoch: 769 [65792/118836 (55%)] Loss: 12244.609375\n",
      "Train Epoch: 769 [98560/118836 (83%)] Loss: 12362.853516\n",
      "    epoch          : 769\n",
      "    loss           : 12277.853372331214\n",
      "    val_loss       : 12275.162729454274\n",
      "    val_log_likelihood: -12180.208216695099\n",
      "    val_log_marginal: -12188.156381444736\n",
      "Train Epoch: 770 [256/118836 (0%)] Loss: 12179.860352\n",
      "Train Epoch: 770 [33024/118836 (28%)] Loss: 12418.703125\n",
      "Train Epoch: 770 [65792/118836 (55%)] Loss: 12381.180664\n",
      "Train Epoch: 770 [98560/118836 (83%)] Loss: 12237.540039\n",
      "    epoch          : 770\n",
      "    loss           : 12275.62358160153\n",
      "    val_loss       : 12274.638514993938\n",
      "    val_log_likelihood: -12180.20354664237\n",
      "    val_log_marginal: -12188.345756733079\n",
      "Train Epoch: 771 [256/118836 (0%)] Loss: 12239.941406\n",
      "Train Epoch: 771 [33024/118836 (28%)] Loss: 12384.971680\n",
      "Train Epoch: 771 [65792/118836 (55%)] Loss: 12296.999023\n",
      "Train Epoch: 771 [98560/118836 (83%)] Loss: 12265.621094\n",
      "    epoch          : 771\n",
      "    loss           : 12273.261188546836\n",
      "    val_loss       : 12275.871741998426\n",
      "    val_log_likelihood: -12181.981046771609\n",
      "    val_log_marginal: -12190.175806735899\n",
      "Train Epoch: 772 [256/118836 (0%)] Loss: 12216.322266\n",
      "Train Epoch: 772 [33024/118836 (28%)] Loss: 12262.971680\n",
      "Train Epoch: 772 [65792/118836 (55%)] Loss: 12359.332031\n",
      "Train Epoch: 772 [98560/118836 (83%)] Loss: 12271.333984\n",
      "    epoch          : 772\n",
      "    loss           : 12282.181784080336\n",
      "    val_loss       : 12278.296487625561\n",
      "    val_log_likelihood: -12178.3425711784\n",
      "    val_log_marginal: -12186.55875230249\n",
      "Train Epoch: 773 [256/118836 (0%)] Loss: 12252.894531\n",
      "Train Epoch: 773 [33024/118836 (28%)] Loss: 12315.820312\n",
      "Train Epoch: 773 [65792/118836 (55%)] Loss: 12340.871094\n",
      "Train Epoch: 773 [98560/118836 (83%)] Loss: 12405.386719\n",
      "    epoch          : 773\n",
      "    loss           : 12279.789332609596\n",
      "    val_loss       : 12278.44777003824\n",
      "    val_log_likelihood: -12177.325889810794\n",
      "    val_log_marginal: -12185.333002713\n",
      "Train Epoch: 774 [256/118836 (0%)] Loss: 12420.169922\n",
      "Train Epoch: 774 [33024/118836 (28%)] Loss: 12246.856445\n",
      "Train Epoch: 774 [65792/118836 (55%)] Loss: 12388.103516\n",
      "Train Epoch: 774 [98560/118836 (83%)] Loss: 12347.356445\n",
      "    epoch          : 774\n",
      "    loss           : 12271.689704818033\n",
      "    val_loss       : 12273.632675070909\n",
      "    val_log_likelihood: -12178.740875562191\n",
      "    val_log_marginal: -12186.975579997057\n",
      "Train Epoch: 775 [256/118836 (0%)] Loss: 12338.039062\n",
      "Train Epoch: 775 [33024/118836 (28%)] Loss: 12316.230469\n",
      "Train Epoch: 775 [65792/118836 (55%)] Loss: 12440.236328\n",
      "Train Epoch: 775 [98560/118836 (83%)] Loss: 12306.536133\n",
      "    epoch          : 775\n",
      "    loss           : 12277.607775182227\n",
      "    val_loss       : 12277.775281023492\n",
      "    val_log_likelihood: -12179.146761108095\n",
      "    val_log_marginal: -12187.295588445366\n",
      "Train Epoch: 776 [256/118836 (0%)] Loss: 12288.287109\n",
      "Train Epoch: 776 [33024/118836 (28%)] Loss: 12328.516602\n",
      "Train Epoch: 776 [65792/118836 (55%)] Loss: 12209.686523\n",
      "Train Epoch: 776 [98560/118836 (83%)] Loss: 12317.664062\n",
      "    epoch          : 776\n",
      "    loss           : 12277.04746643016\n",
      "    val_loss       : 12279.603444437245\n",
      "    val_log_likelihood: -12183.503804474256\n",
      "    val_log_marginal: -12191.565959175789\n",
      "Train Epoch: 777 [256/118836 (0%)] Loss: 12287.218750\n",
      "Train Epoch: 777 [33024/118836 (28%)] Loss: 12328.305664\n",
      "Train Epoch: 777 [65792/118836 (55%)] Loss: 12362.230469\n",
      "Train Epoch: 777 [98560/118836 (83%)] Loss: 12290.753906\n",
      "    epoch          : 777\n",
      "    loss           : 12277.80092131281\n",
      "    val_loss       : 12272.956988049866\n",
      "    val_log_likelihood: -12177.970040774919\n",
      "    val_log_marginal: -12186.061464676364\n",
      "Train Epoch: 778 [256/118836 (0%)] Loss: 12278.948242\n",
      "Train Epoch: 778 [33024/118836 (28%)] Loss: 12281.198242\n",
      "Train Epoch: 778 [65792/118836 (55%)] Loss: 12288.675781\n",
      "Train Epoch: 778 [98560/118836 (83%)] Loss: 12209.871094\n",
      "    epoch          : 778\n",
      "    loss           : 12272.675011146868\n",
      "    val_loss       : 12282.154567350695\n",
      "    val_log_likelihood: -12178.708654169252\n",
      "    val_log_marginal: -12186.862899303747\n",
      "Train Epoch: 779 [256/118836 (0%)] Loss: 12275.624023\n",
      "Train Epoch: 779 [33024/118836 (28%)] Loss: 12257.986328\n",
      "Train Epoch: 779 [65792/118836 (55%)] Loss: 12283.246094\n",
      "Train Epoch: 779 [98560/118836 (83%)] Loss: 12302.462891\n",
      "    epoch          : 779\n",
      "    loss           : 12278.542876828733\n",
      "    val_loss       : 12278.5190140645\n",
      "    val_log_likelihood: -12179.45937742323\n",
      "    val_log_marginal: -12187.639459809894\n",
      "Train Epoch: 780 [256/118836 (0%)] Loss: 12355.351562\n",
      "Train Epoch: 780 [33024/118836 (28%)] Loss: 12324.365234\n",
      "Train Epoch: 780 [65792/118836 (55%)] Loss: 12404.992188\n",
      "Train Epoch: 780 [98560/118836 (83%)] Loss: 12334.644531\n",
      "    epoch          : 780\n",
      "    loss           : 12277.818294270834\n",
      "    val_loss       : 12270.6332843467\n",
      "    val_log_likelihood: -12177.318026099825\n",
      "    val_log_marginal: -12185.369024183368\n",
      "Train Epoch: 781 [256/118836 (0%)] Loss: 12267.227539\n",
      "Train Epoch: 781 [33024/118836 (28%)] Loss: 12204.132812\n",
      "Train Epoch: 781 [65792/118836 (55%)] Loss: 12303.719727\n",
      "Train Epoch: 781 [98560/118836 (83%)] Loss: 12306.164062\n",
      "    epoch          : 781\n",
      "    loss           : 12281.14235454146\n",
      "    val_loss       : 12273.378832597504\n",
      "    val_log_likelihood: -12179.338266064413\n",
      "    val_log_marginal: -12187.32289527791\n",
      "Train Epoch: 782 [256/118836 (0%)] Loss: 12273.285156\n",
      "Train Epoch: 782 [33024/118836 (28%)] Loss: 12211.770508\n",
      "Train Epoch: 782 [65792/118836 (55%)] Loss: 12310.447266\n",
      "Train Epoch: 782 [98560/118836 (83%)] Loss: 12263.411133\n",
      "    epoch          : 782\n",
      "    loss           : 12275.02903565059\n",
      "    val_loss       : 12279.21385957333\n",
      "    val_log_likelihood: -12178.666604470378\n",
      "    val_log_marginal: -12186.795956016447\n",
      "Train Epoch: 783 [256/118836 (0%)] Loss: 12272.914062\n",
      "Train Epoch: 783 [33024/118836 (28%)] Loss: 12326.435547\n",
      "Train Epoch: 783 [65792/118836 (55%)] Loss: 12288.893555\n",
      "Train Epoch: 783 [98560/118836 (83%)] Loss: 12346.938477\n",
      "    epoch          : 783\n",
      "    loss           : 12274.107794244987\n",
      "    val_loss       : 12276.518007772129\n",
      "    val_log_likelihood: -12178.685447522486\n",
      "    val_log_marginal: -12186.819445839823\n",
      "Train Epoch: 784 [256/118836 (0%)] Loss: 12227.586914\n",
      "Train Epoch: 784 [33024/118836 (28%)] Loss: 12227.993164\n",
      "Train Epoch: 784 [65792/118836 (55%)] Loss: 12207.500000\n",
      "Train Epoch: 784 [98560/118836 (83%)] Loss: 12323.135742\n",
      "    epoch          : 784\n",
      "    loss           : 12275.054829662944\n",
      "    val_loss       : 12273.575493952523\n",
      "    val_log_likelihood: -12179.064697871434\n",
      "    val_log_marginal: -12186.96697704027\n",
      "Train Epoch: 785 [256/118836 (0%)] Loss: 12284.569336\n",
      "Train Epoch: 785 [33024/118836 (28%)] Loss: 12281.205078\n",
      "Train Epoch: 785 [65792/118836 (55%)] Loss: 12213.810547\n",
      "Train Epoch: 785 [98560/118836 (83%)] Loss: 12259.754883\n",
      "    epoch          : 785\n",
      "    loss           : 12276.814559101012\n",
      "    val_loss       : 12281.562755533105\n",
      "    val_log_likelihood: -12179.032948200993\n",
      "    val_log_marginal: -12187.192959526474\n",
      "Train Epoch: 786 [256/118836 (0%)] Loss: 12287.863281\n",
      "Train Epoch: 786 [33024/118836 (28%)] Loss: 12331.173828\n",
      "Train Epoch: 786 [65792/118836 (55%)] Loss: 12340.032227\n",
      "Train Epoch: 786 [98560/118836 (83%)] Loss: 12316.300781\n",
      "    epoch          : 786\n",
      "    loss           : 12277.74106796681\n",
      "    val_loss       : 12276.696752499309\n",
      "    val_log_likelihood: -12180.439936479012\n",
      "    val_log_marginal: -12188.538586733122\n",
      "Train Epoch: 787 [256/118836 (0%)] Loss: 12220.728516\n",
      "Train Epoch: 787 [33024/118836 (28%)] Loss: 12315.007812\n",
      "Train Epoch: 787 [65792/118836 (55%)] Loss: 12254.941406\n",
      "Train Epoch: 787 [98560/118836 (83%)] Loss: 12373.784180\n",
      "    epoch          : 787\n",
      "    loss           : 12275.971931380533\n",
      "    val_loss       : 12273.511580934086\n",
      "    val_log_likelihood: -12179.203773779984\n",
      "    val_log_marginal: -12187.246292914448\n",
      "Train Epoch: 788 [256/118836 (0%)] Loss: 12213.197266\n",
      "Train Epoch: 788 [33024/118836 (28%)] Loss: 12358.513672\n",
      "Train Epoch: 788 [65792/118836 (55%)] Loss: 12305.994141\n",
      "Train Epoch: 788 [98560/118836 (83%)] Loss: 12235.113281\n",
      "    epoch          : 788\n",
      "    loss           : 12275.167484749794\n",
      "    val_loss       : 12278.674239940161\n",
      "    val_log_likelihood: -12179.600098060122\n",
      "    val_log_marginal: -12187.737659952334\n",
      "Train Epoch: 789 [256/118836 (0%)] Loss: 12241.089844\n",
      "Train Epoch: 789 [33024/118836 (28%)] Loss: 12207.381836\n",
      "Train Epoch: 789 [65792/118836 (55%)] Loss: 12379.953125\n",
      "Train Epoch: 789 [98560/118836 (83%)] Loss: 12304.215820\n",
      "    epoch          : 789\n",
      "    loss           : 12277.11224669148\n",
      "    val_loss       : 12276.783567918232\n",
      "    val_log_likelihood: -12176.014017266336\n",
      "    val_log_marginal: -12183.943111546214\n",
      "Train Epoch: 790 [256/118836 (0%)] Loss: 12520.240234\n",
      "Train Epoch: 790 [33024/118836 (28%)] Loss: 12380.939453\n",
      "Train Epoch: 790 [65792/118836 (55%)] Loss: 12334.832031\n",
      "Train Epoch: 790 [98560/118836 (83%)] Loss: 12269.519531\n",
      "    epoch          : 790\n",
      "    loss           : 12275.590302871693\n",
      "    val_loss       : 12272.287520074075\n",
      "    val_log_likelihood: -12176.035181451613\n",
      "    val_log_marginal: -12184.269166812266\n",
      "Train Epoch: 791 [256/118836 (0%)] Loss: 12229.945312\n",
      "Train Epoch: 791 [33024/118836 (28%)] Loss: 12290.397461\n",
      "Train Epoch: 791 [65792/118836 (55%)] Loss: 12249.524414\n",
      "Train Epoch: 791 [98560/118836 (83%)] Loss: 12377.136719\n",
      "    epoch          : 791\n",
      "    loss           : 12272.48326095947\n",
      "    val_loss       : 12273.832748831479\n",
      "    val_log_likelihood: -12178.92162476091\n",
      "    val_log_marginal: -12187.01844579793\n",
      "Train Epoch: 792 [256/118836 (0%)] Loss: 12166.423828\n",
      "Train Epoch: 792 [33024/118836 (28%)] Loss: 12269.673828\n",
      "Train Epoch: 792 [65792/118836 (55%)] Loss: 12349.115234\n",
      "Train Epoch: 792 [98560/118836 (83%)] Loss: 12383.154297\n",
      "    epoch          : 792\n",
      "    loss           : 12277.571863530036\n",
      "    val_loss       : 12275.820971768\n",
      "    val_log_likelihood: -12177.330504290736\n",
      "    val_log_marginal: -12185.472686578967\n",
      "Train Epoch: 793 [256/118836 (0%)] Loss: 12423.515625\n",
      "Train Epoch: 793 [33024/118836 (28%)] Loss: 12354.334961\n",
      "Train Epoch: 793 [65792/118836 (55%)] Loss: 12358.723633\n",
      "Train Epoch: 793 [98560/118836 (83%)] Loss: 12277.336914\n",
      "    epoch          : 793\n",
      "    loss           : 12278.641533550455\n",
      "    val_loss       : 12279.914737878262\n",
      "    val_log_likelihood: -12174.823069330283\n",
      "    val_log_marginal: -12182.907677773836\n",
      "Train Epoch: 794 [256/118836 (0%)] Loss: 12321.349609\n",
      "Train Epoch: 794 [33024/118836 (28%)] Loss: 12200.103516\n",
      "Train Epoch: 794 [65792/118836 (55%)] Loss: 12325.757812\n",
      "Train Epoch: 794 [98560/118836 (83%)] Loss: 12391.568359\n",
      "    epoch          : 794\n",
      "    loss           : 12275.06742594603\n",
      "    val_loss       : 12275.12907158057\n",
      "    val_log_likelihood: -12178.573855426748\n",
      "    val_log_marginal: -12186.710701767333\n",
      "Train Epoch: 795 [256/118836 (0%)] Loss: 12232.314453\n",
      "Train Epoch: 795 [33024/118836 (28%)] Loss: 12307.833984\n",
      "Train Epoch: 795 [65792/118836 (55%)] Loss: 12319.988281\n",
      "Train Epoch: 795 [98560/118836 (83%)] Loss: 12338.788086\n",
      "    epoch          : 795\n",
      "    loss           : 12269.92952853598\n",
      "    val_loss       : 12283.249251117457\n",
      "    val_log_likelihood: -12178.78169991341\n",
      "    val_log_marginal: -12186.949996862595\n",
      "Train Epoch: 796 [256/118836 (0%)] Loss: 12373.142578\n",
      "Train Epoch: 796 [33024/118836 (28%)] Loss: 12361.638672\n",
      "Train Epoch: 796 [65792/118836 (55%)] Loss: 12272.098633\n",
      "Train Epoch: 796 [98560/118836 (83%)] Loss: 12176.216797\n",
      "    epoch          : 796\n",
      "    loss           : 12278.942969396196\n",
      "    val_loss       : 12283.822450927239\n",
      "    val_log_likelihood: -12178.51786923594\n",
      "    val_log_marginal: -12186.878068455104\n",
      "Train Epoch: 797 [256/118836 (0%)] Loss: 12258.899414\n",
      "Train Epoch: 797 [33024/118836 (28%)] Loss: 12352.723633\n",
      "Train Epoch: 797 [65792/118836 (55%)] Loss: 12259.391602\n",
      "Train Epoch: 797 [98560/118836 (83%)] Loss: 12229.139648\n",
      "    epoch          : 797\n",
      "    loss           : 12282.395566454714\n",
      "    val_loss       : 12274.13957569701\n",
      "    val_log_likelihood: -12177.300330205748\n",
      "    val_log_marginal: -12185.35023114157\n",
      "Train Epoch: 798 [256/118836 (0%)] Loss: 12250.207031\n",
      "Train Epoch: 798 [33024/118836 (28%)] Loss: 12205.624023\n",
      "Train Epoch: 798 [65792/118836 (55%)] Loss: 12328.282227\n",
      "Train Epoch: 798 [98560/118836 (83%)] Loss: 12329.035156\n",
      "    epoch          : 798\n",
      "    loss           : 12277.852171538978\n",
      "    val_loss       : 12275.609232432027\n",
      "    val_log_likelihood: -12178.356268093466\n",
      "    val_log_marginal: -12186.308010303757\n",
      "Train Epoch: 799 [256/118836 (0%)] Loss: 12354.514648\n",
      "Train Epoch: 799 [33024/118836 (28%)] Loss: 12296.972656\n",
      "Train Epoch: 799 [65792/118836 (55%)] Loss: 12274.552734\n",
      "Train Epoch: 799 [98560/118836 (83%)] Loss: 12255.126953\n",
      "    epoch          : 799\n",
      "    loss           : 12274.884793411393\n",
      "    val_loss       : 12273.333383859554\n",
      "    val_log_likelihood: -12177.404831924629\n",
      "    val_log_marginal: -12185.382805661877\n",
      "Train Epoch: 800 [256/118836 (0%)] Loss: 12278.455078\n",
      "Train Epoch: 800 [33024/118836 (28%)] Loss: 12292.506836\n",
      "Train Epoch: 800 [65792/118836 (55%)] Loss: 12213.348633\n",
      "Train Epoch: 800 [98560/118836 (83%)] Loss: 12334.354492\n",
      "    epoch          : 800\n",
      "    loss           : 12274.995879213193\n",
      "    val_loss       : 12273.95727261893\n",
      "    val_log_likelihood: -12180.246001182537\n",
      "    val_log_marginal: -12188.386045426294\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch800.pth ...\n",
      "Train Epoch: 801 [256/118836 (0%)] Loss: 12357.755859\n",
      "Train Epoch: 801 [33024/118836 (28%)] Loss: 12189.587891\n",
      "Train Epoch: 801 [65792/118836 (55%)] Loss: 12215.455078\n",
      "Train Epoch: 801 [98560/118836 (83%)] Loss: 12281.345703\n",
      "    epoch          : 801\n",
      "    loss           : 12275.47655733044\n",
      "    val_loss       : 12280.58334346385\n",
      "    val_log_likelihood: -12179.922413280605\n",
      "    val_log_marginal: -12188.042326138191\n",
      "Train Epoch: 802 [256/118836 (0%)] Loss: 12175.876953\n",
      "Train Epoch: 802 [33024/118836 (28%)] Loss: 12244.034180\n",
      "Train Epoch: 802 [65792/118836 (55%)] Loss: 12256.287109\n",
      "Train Epoch: 802 [98560/118836 (83%)] Loss: 12270.003906\n",
      "    epoch          : 802\n",
      "    loss           : 12280.553090751653\n",
      "    val_loss       : 12277.434354523826\n",
      "    val_log_likelihood: -12177.318833036084\n",
      "    val_log_marginal: -12185.330992266476\n",
      "Train Epoch: 803 [256/118836 (0%)] Loss: 12276.825195\n",
      "Train Epoch: 803 [33024/118836 (28%)] Loss: 12222.497070\n",
      "Train Epoch: 803 [65792/118836 (55%)] Loss: 12245.951172\n",
      "Train Epoch: 803 [98560/118836 (83%)] Loss: 12288.637695\n",
      "    epoch          : 803\n",
      "    loss           : 12275.939393513494\n",
      "    val_loss       : 12272.666702061286\n",
      "    val_log_likelihood: -12175.235454792182\n",
      "    val_log_marginal: -12183.355799134193\n",
      "Train Epoch: 804 [256/118836 (0%)] Loss: 12250.552734\n",
      "Train Epoch: 804 [33024/118836 (28%)] Loss: 12279.128906\n",
      "Train Epoch: 804 [65792/118836 (55%)] Loss: 12279.566406\n",
      "Train Epoch: 804 [98560/118836 (83%)] Loss: 12269.794922\n",
      "    epoch          : 804\n",
      "    loss           : 12275.502599643301\n",
      "    val_loss       : 12276.03654569787\n",
      "    val_log_likelihood: -12178.322947684037\n",
      "    val_log_marginal: -12186.46975604365\n",
      "Train Epoch: 805 [256/118836 (0%)] Loss: 12360.826172\n",
      "Train Epoch: 805 [33024/118836 (28%)] Loss: 12369.891602\n",
      "Train Epoch: 805 [65792/118836 (55%)] Loss: 12409.414062\n",
      "Train Epoch: 805 [98560/118836 (83%)] Loss: 12327.066406\n",
      "    epoch          : 805\n",
      "    loss           : 12275.06989360396\n",
      "    val_loss       : 12276.519893018309\n",
      "    val_log_likelihood: -12178.705144844655\n",
      "    val_log_marginal: -12186.786767599904\n",
      "Train Epoch: 806 [256/118836 (0%)] Loss: 12282.140625\n",
      "Train Epoch: 806 [33024/118836 (28%)] Loss: 12359.154297\n",
      "Train Epoch: 806 [65792/118836 (55%)] Loss: 12229.708984\n",
      "Train Epoch: 806 [98560/118836 (83%)] Loss: 12216.103516\n",
      "    epoch          : 806\n",
      "    loss           : 12277.499785947839\n",
      "    val_loss       : 12281.063508270954\n",
      "    val_log_likelihood: -12177.669069543526\n",
      "    val_log_marginal: -12185.844095276709\n",
      "Train Epoch: 807 [256/118836 (0%)] Loss: 12359.905273\n",
      "Train Epoch: 807 [33024/118836 (28%)] Loss: 12250.546875\n",
      "Train Epoch: 807 [65792/118836 (55%)] Loss: 12206.097656\n",
      "Train Epoch: 807 [98560/118836 (83%)] Loss: 12226.243164\n",
      "    epoch          : 807\n",
      "    loss           : 12273.615511592741\n",
      "    val_loss       : 12275.995254604846\n",
      "    val_log_likelihood: -12175.961281437396\n",
      "    val_log_marginal: -12183.944306376927\n",
      "Train Epoch: 808 [256/118836 (0%)] Loss: 12239.028320\n",
      "Train Epoch: 808 [33024/118836 (28%)] Loss: 12235.283203\n",
      "Train Epoch: 808 [65792/118836 (55%)] Loss: 12372.913086\n",
      "Train Epoch: 808 [98560/118836 (83%)] Loss: 12354.505859\n",
      "    epoch          : 808\n",
      "    loss           : 12273.116864725496\n",
      "    val_loss       : 12275.876588369916\n",
      "    val_log_likelihood: -12180.691216914807\n",
      "    val_log_marginal: -12188.91284623326\n",
      "Train Epoch: 809 [256/118836 (0%)] Loss: 12311.922852\n",
      "Train Epoch: 809 [33024/118836 (28%)] Loss: 12449.334961\n",
      "Train Epoch: 809 [65792/118836 (55%)] Loss: 12224.482422\n",
      "Train Epoch: 809 [98560/118836 (83%)] Loss: 12252.726562\n",
      "    epoch          : 809\n",
      "    loss           : 12276.229843071496\n",
      "    val_loss       : 12271.083747988143\n",
      "    val_log_likelihood: -12178.02285834755\n",
      "    val_log_marginal: -12186.193078837015\n",
      "Train Epoch: 810 [256/118836 (0%)] Loss: 12257.767578\n",
      "Train Epoch: 810 [33024/118836 (28%)] Loss: 12292.206055\n",
      "Train Epoch: 810 [65792/118836 (55%)] Loss: 12306.314453\n",
      "Train Epoch: 810 [98560/118836 (83%)] Loss: 12221.417969\n",
      "    epoch          : 810\n",
      "    loss           : 12277.666142763906\n",
      "    val_loss       : 12277.56582344727\n",
      "    val_log_likelihood: -12178.716185574338\n",
      "    val_log_marginal: -12186.739850062533\n",
      "Train Epoch: 811 [256/118836 (0%)] Loss: 12317.635742\n",
      "Train Epoch: 811 [33024/118836 (28%)] Loss: 12247.244141\n",
      "Train Epoch: 811 [65792/118836 (55%)] Loss: 12231.218750\n",
      "Train Epoch: 811 [98560/118836 (83%)] Loss: 12502.748047\n",
      "    epoch          : 811\n",
      "    loss           : 12277.799877869107\n",
      "    val_loss       : 12275.084410315207\n",
      "    val_log_likelihood: -12177.609130092018\n",
      "    val_log_marginal: -12185.827907822615\n",
      "Train Epoch: 812 [256/118836 (0%)] Loss: 12236.011719\n",
      "Train Epoch: 812 [33024/118836 (28%)] Loss: 12310.851562\n",
      "Train Epoch: 812 [65792/118836 (55%)] Loss: 12325.406250\n",
      "Train Epoch: 812 [98560/118836 (83%)] Loss: 12349.880859\n",
      "    epoch          : 812\n",
      "    loss           : 12279.057210569168\n",
      "    val_loss       : 12278.848186627985\n",
      "    val_log_likelihood: -12177.650420349979\n",
      "    val_log_marginal: -12185.609726667702\n",
      "Train Epoch: 813 [256/118836 (0%)] Loss: 12367.048828\n",
      "Train Epoch: 813 [33024/118836 (28%)] Loss: 12317.561523\n",
      "Train Epoch: 813 [65792/118836 (55%)] Loss: 12342.138672\n",
      "Train Epoch: 813 [98560/118836 (83%)] Loss: 12394.802734\n",
      "    epoch          : 813\n",
      "    loss           : 12273.90111937164\n",
      "    val_loss       : 12281.017666751488\n",
      "    val_log_likelihood: -12177.577377998346\n",
      "    val_log_marginal: -12185.539723901684\n",
      "Train Epoch: 814 [256/118836 (0%)] Loss: 12294.466797\n",
      "Train Epoch: 814 [33024/118836 (28%)] Loss: 12265.200195\n",
      "Train Epoch: 814 [65792/118836 (55%)] Loss: 12242.959961\n",
      "Train Epoch: 814 [98560/118836 (83%)] Loss: 12249.800781\n",
      "    epoch          : 814\n",
      "    loss           : 12274.056866470224\n",
      "    val_loss       : 12276.73374928147\n",
      "    val_log_likelihood: -12178.061394844655\n",
      "    val_log_marginal: -12186.452211951062\n",
      "Train Epoch: 815 [256/118836 (0%)] Loss: 12188.541016\n",
      "Train Epoch: 815 [33024/118836 (28%)] Loss: 12342.810547\n",
      "Train Epoch: 815 [65792/118836 (55%)] Loss: 12191.794922\n",
      "Train Epoch: 815 [98560/118836 (83%)] Loss: 12291.152344\n",
      "    epoch          : 815\n",
      "    loss           : 12279.475702898832\n",
      "    val_loss       : 12274.954191000312\n",
      "    val_log_likelihood: -12178.717566493486\n",
      "    val_log_marginal: -12186.759538257875\n",
      "Train Epoch: 816 [256/118836 (0%)] Loss: 12333.621094\n",
      "Train Epoch: 816 [33024/118836 (28%)] Loss: 12341.094727\n",
      "Train Epoch: 816 [65792/118836 (55%)] Loss: 12290.662109\n",
      "Train Epoch: 816 [98560/118836 (83%)] Loss: 12241.806641\n",
      "    epoch          : 816\n",
      "    loss           : 12275.244397002947\n",
      "    val_loss       : 12274.345597965093\n",
      "    val_log_likelihood: -12176.739755382805\n",
      "    val_log_marginal: -12185.04621377419\n",
      "Train Epoch: 817 [256/118836 (0%)] Loss: 12206.376953\n",
      "Train Epoch: 817 [33024/118836 (28%)] Loss: 12311.719727\n",
      "Train Epoch: 817 [65792/118836 (55%)] Loss: 12253.674805\n",
      "Train Epoch: 817 [98560/118836 (83%)] Loss: 12328.625977\n",
      "    epoch          : 817\n",
      "    loss           : 12276.881108806348\n",
      "    val_loss       : 12276.328482111927\n",
      "    val_log_likelihood: -12178.416481854838\n",
      "    val_log_marginal: -12186.746432893191\n",
      "Train Epoch: 818 [256/118836 (0%)] Loss: 12336.608398\n",
      "Train Epoch: 818 [33024/118836 (28%)] Loss: 12231.160156\n",
      "Train Epoch: 818 [65792/118836 (55%)] Loss: 12308.514648\n",
      "Train Epoch: 818 [98560/118836 (83%)] Loss: 12334.651367\n",
      "    epoch          : 818\n",
      "    loss           : 12275.884239137458\n",
      "    val_loss       : 12275.20789051947\n",
      "    val_log_likelihood: -12178.743088296113\n",
      "    val_log_marginal: -12186.997893512063\n",
      "Train Epoch: 819 [256/118836 (0%)] Loss: 12273.699219\n",
      "Train Epoch: 819 [33024/118836 (28%)] Loss: 12266.004883\n",
      "Train Epoch: 819 [65792/118836 (55%)] Loss: 12371.063477\n",
      "Train Epoch: 819 [98560/118836 (83%)] Loss: 12359.100586\n",
      "    epoch          : 819\n",
      "    loss           : 12275.149920679538\n",
      "    val_loss       : 12271.282214314082\n",
      "    val_log_likelihood: -12178.641706084574\n",
      "    val_log_marginal: -12186.807582013364\n",
      "Train Epoch: 820 [256/118836 (0%)] Loss: 12304.050781\n",
      "Train Epoch: 820 [33024/118836 (28%)] Loss: 12362.349609\n",
      "Train Epoch: 820 [65792/118836 (55%)] Loss: 12286.537109\n",
      "Train Epoch: 820 [98560/118836 (83%)] Loss: 12309.980469\n",
      "    epoch          : 820\n",
      "    loss           : 12278.120566616264\n",
      "    val_loss       : 12279.409519512734\n",
      "    val_log_likelihood: -12174.914335032827\n",
      "    val_log_marginal: -12183.05407263905\n",
      "Train Epoch: 821 [256/118836 (0%)] Loss: 12253.613281\n",
      "Train Epoch: 821 [33024/118836 (28%)] Loss: 12370.452148\n",
      "Train Epoch: 821 [65792/118836 (55%)] Loss: 12308.110352\n",
      "Train Epoch: 821 [98560/118836 (83%)] Loss: 12430.675781\n",
      "    epoch          : 821\n",
      "    loss           : 12277.63250798051\n",
      "    val_loss       : 12278.37507551377\n",
      "    val_log_likelihood: -12178.234001660721\n",
      "    val_log_marginal: -12186.40150942181\n",
      "Train Epoch: 822 [256/118836 (0%)] Loss: 12256.411133\n",
      "Train Epoch: 822 [33024/118836 (28%)] Loss: 12357.372070\n",
      "Train Epoch: 822 [65792/118836 (55%)] Loss: 12357.619141\n",
      "Train Epoch: 822 [98560/118836 (83%)] Loss: 12362.781250\n",
      "    epoch          : 822\n",
      "    loss           : 12271.908690033086\n",
      "    val_loss       : 12276.191754959897\n",
      "    val_log_likelihood: -12179.840784125568\n",
      "    val_log_marginal: -12187.921406170313\n",
      "Train Epoch: 823 [256/118836 (0%)] Loss: 12195.839844\n",
      "Train Epoch: 823 [33024/118836 (28%)] Loss: 12287.919922\n",
      "Train Epoch: 823 [65792/118836 (55%)] Loss: 12223.996094\n",
      "Train Epoch: 823 [98560/118836 (83%)] Loss: 12352.377930\n",
      "    epoch          : 823\n",
      "    loss           : 12275.886018920595\n",
      "    val_loss       : 12276.43963384327\n",
      "    val_log_likelihood: -12180.616616263442\n",
      "    val_log_marginal: -12188.914083358679\n",
      "Train Epoch: 824 [256/118836 (0%)] Loss: 12415.739258\n",
      "Train Epoch: 824 [33024/118836 (28%)] Loss: 12247.229492\n",
      "Train Epoch: 824 [65792/118836 (55%)] Loss: 12410.634766\n",
      "Train Epoch: 824 [98560/118836 (83%)] Loss: 12247.662109\n",
      "    epoch          : 824\n",
      "    loss           : 12279.502117743226\n",
      "    val_loss       : 12269.745289398874\n",
      "    val_log_likelihood: -12176.967824002275\n",
      "    val_log_marginal: -12185.011955552669\n",
      "Train Epoch: 825 [256/118836 (0%)] Loss: 12235.705078\n",
      "Train Epoch: 825 [33024/118836 (28%)] Loss: 12293.807617\n",
      "Train Epoch: 825 [65792/118836 (55%)] Loss: 12210.527344\n",
      "Train Epoch: 825 [98560/118836 (83%)] Loss: 12300.485352\n",
      "    epoch          : 825\n",
      "    loss           : 12275.642466656327\n",
      "    val_loss       : 12272.597339477323\n",
      "    val_log_likelihood: -12175.454965687035\n",
      "    val_log_marginal: -12183.60482133946\n",
      "Train Epoch: 826 [256/118836 (0%)] Loss: 12410.557617\n",
      "Train Epoch: 826 [33024/118836 (28%)] Loss: 12397.865234\n",
      "Train Epoch: 826 [65792/118836 (55%)] Loss: 12323.868164\n",
      "Train Epoch: 826 [98560/118836 (83%)] Loss: 12206.291016\n",
      "    epoch          : 826\n",
      "    loss           : 12273.572140586228\n",
      "    val_loss       : 12275.864994704336\n",
      "    val_log_likelihood: -12176.31299288539\n",
      "    val_log_marginal: -12184.565414058612\n",
      "Train Epoch: 827 [256/118836 (0%)] Loss: 12240.535156\n",
      "Train Epoch: 827 [33024/118836 (28%)] Loss: 12229.206055\n",
      "Train Epoch: 827 [65792/118836 (55%)] Loss: 12244.954102\n",
      "Train Epoch: 827 [98560/118836 (83%)] Loss: 12201.401367\n",
      "    epoch          : 827\n",
      "    loss           : 12274.771237043786\n",
      "    val_loss       : 12277.509977954736\n",
      "    val_log_likelihood: -12176.091182020264\n",
      "    val_log_marginal: -12184.214605014015\n",
      "Train Epoch: 828 [256/118836 (0%)] Loss: 12239.470703\n",
      "Train Epoch: 828 [33024/118836 (28%)] Loss: 12350.670898\n",
      "Train Epoch: 828 [65792/118836 (55%)] Loss: 12366.036133\n",
      "Train Epoch: 828 [98560/118836 (83%)] Loss: 12233.589844\n",
      "    epoch          : 828\n",
      "    loss           : 12273.01515230821\n",
      "    val_loss       : 12270.662951382486\n",
      "    val_log_likelihood: -12176.789309185018\n",
      "    val_log_marginal: -12184.877992202593\n",
      "Train Epoch: 829 [256/118836 (0%)] Loss: 12298.560547\n",
      "Train Epoch: 829 [33024/118836 (28%)] Loss: 12271.777344\n",
      "Train Epoch: 829 [65792/118836 (55%)] Loss: 12198.751953\n",
      "Train Epoch: 829 [98560/118836 (83%)] Loss: 12234.849609\n",
      "    epoch          : 829\n",
      "    loss           : 12273.974048154467\n",
      "    val_loss       : 12273.958915444635\n",
      "    val_log_likelihood: -12178.530307685845\n",
      "    val_log_marginal: -12186.573432701083\n",
      "Train Epoch: 830 [256/118836 (0%)] Loss: 12274.456055\n",
      "Train Epoch: 830 [33024/118836 (28%)] Loss: 12256.080078\n",
      "Train Epoch: 830 [65792/118836 (55%)] Loss: 12274.687500\n",
      "Train Epoch: 830 [98560/118836 (83%)] Loss: 12211.417969\n",
      "    epoch          : 830\n",
      "    loss           : 12275.056110098738\n",
      "    val_loss       : 12281.737959272423\n",
      "    val_log_likelihood: -12179.202506268093\n",
      "    val_log_marginal: -12187.547369840066\n",
      "Train Epoch: 831 [256/118836 (0%)] Loss: 12280.742188\n",
      "Train Epoch: 831 [33024/118836 (28%)] Loss: 12266.737305\n",
      "Train Epoch: 831 [65792/118836 (55%)] Loss: 12358.800781\n",
      "Train Epoch: 831 [98560/118836 (83%)] Loss: 12326.208984\n",
      "    epoch          : 831\n",
      "    loss           : 12274.59376890121\n",
      "    val_loss       : 12276.696202661742\n",
      "    val_log_likelihood: -12179.559258361765\n",
      "    val_log_marginal: -12187.876059839798\n",
      "Train Epoch: 832 [256/118836 (0%)] Loss: 12196.644531\n",
      "Train Epoch: 832 [33024/118836 (28%)] Loss: 12288.816406\n",
      "Train Epoch: 832 [65792/118836 (55%)] Loss: 12257.039062\n",
      "Train Epoch: 832 [98560/118836 (83%)] Loss: 12357.993164\n",
      "    epoch          : 832\n",
      "    loss           : 12276.001167190085\n",
      "    val_loss       : 12274.787821187907\n",
      "    val_log_likelihood: -12180.93943680857\n",
      "    val_log_marginal: -12189.144581496495\n",
      "Train Epoch: 833 [256/118836 (0%)] Loss: 12235.425781\n",
      "Train Epoch: 833 [33024/118836 (28%)] Loss: 12301.865234\n",
      "Train Epoch: 833 [65792/118836 (55%)] Loss: 12275.042969\n",
      "Train Epoch: 833 [98560/118836 (83%)] Loss: 12332.026367\n",
      "    epoch          : 833\n",
      "    loss           : 12275.113926152813\n",
      "    val_loss       : 12275.065727204515\n",
      "    val_log_likelihood: -12177.68473719241\n",
      "    val_log_marginal: -12185.74424389742\n",
      "Train Epoch: 834 [256/118836 (0%)] Loss: 12281.044922\n",
      "Train Epoch: 834 [33024/118836 (28%)] Loss: 12202.755859\n",
      "Train Epoch: 834 [65792/118836 (55%)] Loss: 12312.597656\n",
      "Train Epoch: 834 [98560/118836 (83%)] Loss: 12277.248047\n",
      "    epoch          : 834\n",
      "    loss           : 12272.456232875827\n",
      "    val_loss       : 12275.451551407894\n",
      "    val_log_likelihood: -12178.48563701923\n",
      "    val_log_marginal: -12186.54786633415\n",
      "Train Epoch: 835 [256/118836 (0%)] Loss: 12290.000000\n",
      "Train Epoch: 835 [33024/118836 (28%)] Loss: 12259.390625\n",
      "Train Epoch: 835 [65792/118836 (55%)] Loss: 12231.367188\n",
      "Train Epoch: 835 [98560/118836 (83%)] Loss: 12233.464844\n",
      "    epoch          : 835\n",
      "    loss           : 12276.893447257547\n",
      "    val_loss       : 12276.232655017602\n",
      "    val_log_likelihood: -12178.367159713607\n",
      "    val_log_marginal: -12186.499068408213\n",
      "Train Epoch: 836 [256/118836 (0%)] Loss: 12295.421875\n",
      "Train Epoch: 836 [33024/118836 (28%)] Loss: 12381.445312\n",
      "Train Epoch: 836 [65792/118836 (55%)] Loss: 12256.294922\n",
      "Train Epoch: 836 [98560/118836 (83%)] Loss: 12208.852539\n",
      "    epoch          : 836\n",
      "    loss           : 12274.123365772333\n",
      "    val_loss       : 12274.964057359197\n",
      "    val_log_likelihood: -12176.401290774918\n",
      "    val_log_marginal: -12184.461393970141\n",
      "Train Epoch: 837 [256/118836 (0%)] Loss: 12259.627930\n",
      "Train Epoch: 837 [33024/118836 (28%)] Loss: 12293.775391\n",
      "Train Epoch: 837 [65792/118836 (55%)] Loss: 12298.382812\n",
      "Train Epoch: 837 [98560/118836 (83%)] Loss: 12298.906250\n",
      "    epoch          : 837\n",
      "    loss           : 12274.372391633064\n",
      "    val_loss       : 12275.148172542895\n",
      "    val_log_likelihood: -12177.821951735681\n",
      "    val_log_marginal: -12185.77143278187\n",
      "Train Epoch: 838 [256/118836 (0%)] Loss: 12218.894531\n",
      "Train Epoch: 838 [33024/118836 (28%)] Loss: 12278.654297\n",
      "Train Epoch: 838 [65792/118836 (55%)] Loss: 12413.390625\n",
      "Train Epoch: 838 [98560/118836 (83%)] Loss: 12222.720703\n",
      "    epoch          : 838\n",
      "    loss           : 12275.56764032129\n",
      "    val_loss       : 12271.02388663471\n",
      "    val_log_likelihood: -12182.371160146558\n",
      "    val_log_marginal: -12190.440963581696\n",
      "Train Epoch: 839 [256/118836 (0%)] Loss: 12301.224609\n",
      "Train Epoch: 839 [33024/118836 (28%)] Loss: 12293.134766\n",
      "Train Epoch: 839 [65792/118836 (55%)] Loss: 12330.732422\n",
      "Train Epoch: 839 [98560/118836 (83%)] Loss: 12230.205078\n",
      "    epoch          : 839\n",
      "    loss           : 12274.258214433416\n",
      "    val_loss       : 12270.65029903493\n",
      "    val_log_likelihood: -12180.423840887355\n",
      "    val_log_marginal: -12188.625929969712\n",
      "Train Epoch: 840 [256/118836 (0%)] Loss: 12376.286133\n",
      "Train Epoch: 840 [33024/118836 (28%)] Loss: 12237.543945\n",
      "Train Epoch: 840 [65792/118836 (55%)] Loss: 12359.878906\n",
      "Train Epoch: 840 [98560/118836 (83%)] Loss: 12338.702148\n",
      "    epoch          : 840\n",
      "    loss           : 12275.795486003413\n",
      "    val_loss       : 12277.757677044287\n",
      "    val_log_likelihood: -12178.261694840778\n",
      "    val_log_marginal: -12186.760824419733\n",
      "Train Epoch: 841 [256/118836 (0%)] Loss: 12281.707031\n",
      "Train Epoch: 841 [33024/118836 (28%)] Loss: 12424.694336\n",
      "Train Epoch: 841 [65792/118836 (55%)] Loss: 12289.916992\n",
      "Train Epoch: 841 [98560/118836 (83%)] Loss: 12246.867188\n",
      "    epoch          : 841\n",
      "    loss           : 12275.25460527166\n",
      "    val_loss       : 12269.349393883793\n",
      "    val_log_likelihood: -12177.035971909894\n",
      "    val_log_marginal: -12185.26741805901\n",
      "Train Epoch: 842 [256/118836 (0%)] Loss: 12265.957031\n",
      "Train Epoch: 842 [33024/118836 (28%)] Loss: 12305.245117\n",
      "Train Epoch: 842 [65792/118836 (55%)] Loss: 12263.349609\n",
      "Train Epoch: 842 [98560/118836 (83%)] Loss: 12335.949219\n",
      "    epoch          : 842\n",
      "    loss           : 12272.595363387873\n",
      "    val_loss       : 12272.419234361136\n",
      "    val_log_likelihood: -12179.997149632962\n",
      "    val_log_marginal: -12188.380813934693\n",
      "Train Epoch: 843 [256/118836 (0%)] Loss: 12255.820312\n",
      "Train Epoch: 843 [33024/118836 (28%)] Loss: 12226.546875\n",
      "Train Epoch: 843 [65792/118836 (55%)] Loss: 12356.897461\n",
      "Train Epoch: 843 [98560/118836 (83%)] Loss: 12357.755859\n",
      "    epoch          : 843\n",
      "    loss           : 12274.356742723843\n",
      "    val_loss       : 12269.246669932405\n",
      "    val_log_likelihood: -12179.875462837314\n",
      "    val_log_marginal: -12188.223495538368\n",
      "Train Epoch: 844 [256/118836 (0%)] Loss: 12258.681641\n",
      "Train Epoch: 844 [33024/118836 (28%)] Loss: 12213.938477\n",
      "Train Epoch: 844 [65792/118836 (55%)] Loss: 12201.479492\n",
      "Train Epoch: 844 [98560/118836 (83%)] Loss: 12358.580078\n",
      "    epoch          : 844\n",
      "    loss           : 12275.925481415426\n",
      "    val_loss       : 12276.738085982592\n",
      "    val_log_likelihood: -12178.534054971828\n",
      "    val_log_marginal: -12186.562831322259\n",
      "Train Epoch: 845 [256/118836 (0%)] Loss: 12279.760742\n",
      "Train Epoch: 845 [33024/118836 (28%)] Loss: 12212.759766\n",
      "Train Epoch: 845 [65792/118836 (55%)] Loss: 12351.839844\n",
      "Train Epoch: 845 [98560/118836 (83%)] Loss: 12321.031250\n",
      "    epoch          : 845\n",
      "    loss           : 12272.301538429228\n",
      "    val_loss       : 12273.720878534847\n",
      "    val_log_likelihood: -12178.403548257858\n",
      "    val_log_marginal: -12186.445916689296\n",
      "Train Epoch: 846 [256/118836 (0%)] Loss: 12338.833984\n",
      "Train Epoch: 846 [33024/118836 (28%)] Loss: 12289.865234\n",
      "Train Epoch: 846 [65792/118836 (55%)] Loss: 12235.616211\n",
      "Train Epoch: 846 [98560/118836 (83%)] Loss: 12270.962891\n",
      "    epoch          : 846\n",
      "    loss           : 12278.4988615656\n",
      "    val_loss       : 12272.550150626665\n",
      "    val_log_likelihood: -12178.375557020265\n",
      "    val_log_marginal: -12186.767717803434\n",
      "Train Epoch: 847 [256/118836 (0%)] Loss: 12183.753906\n",
      "Train Epoch: 847 [33024/118836 (28%)] Loss: 12300.458984\n",
      "Train Epoch: 847 [65792/118836 (55%)] Loss: 12308.914062\n",
      "Train Epoch: 847 [98560/118836 (83%)] Loss: 12273.972656\n",
      "    epoch          : 847\n",
      "    loss           : 12273.556929797352\n",
      "    val_loss       : 12272.811119213724\n",
      "    val_log_likelihood: -12181.038740371692\n",
      "    val_log_marginal: -12189.248730109719\n",
      "Train Epoch: 848 [256/118836 (0%)] Loss: 12344.651367\n",
      "Train Epoch: 848 [33024/118836 (28%)] Loss: 12294.927734\n",
      "Train Epoch: 848 [65792/118836 (55%)] Loss: 12201.451172\n",
      "Train Epoch: 848 [98560/118836 (83%)] Loss: 12297.263672\n",
      "    epoch          : 848\n",
      "    loss           : 12276.799931826406\n",
      "    val_loss       : 12273.82448085139\n",
      "    val_log_likelihood: -12176.509404078784\n",
      "    val_log_marginal: -12184.426347800336\n",
      "Train Epoch: 849 [256/118836 (0%)] Loss: 12380.134766\n",
      "Train Epoch: 849 [33024/118836 (28%)] Loss: 12490.504883\n",
      "Train Epoch: 849 [65792/118836 (55%)] Loss: 12317.154297\n",
      "Train Epoch: 849 [98560/118836 (83%)] Loss: 12228.496094\n",
      "    epoch          : 849\n",
      "    loss           : 12274.80474678841\n",
      "    val_loss       : 12275.515539889297\n",
      "    val_log_likelihood: -12176.737694827854\n",
      "    val_log_marginal: -12184.782259148113\n",
      "Train Epoch: 850 [256/118836 (0%)] Loss: 12227.745117\n",
      "Train Epoch: 850 [33024/118836 (28%)] Loss: 12333.197266\n",
      "Train Epoch: 850 [65792/118836 (55%)] Loss: 12379.618164\n",
      "Train Epoch: 850 [98560/118836 (83%)] Loss: 12246.523438\n",
      "    epoch          : 850\n",
      "    loss           : 12276.142151474618\n",
      "    val_loss       : 12273.301537438796\n",
      "    val_log_likelihood: -12178.23921209419\n",
      "    val_log_marginal: -12186.43093061298\n",
      "Train Epoch: 851 [256/118836 (0%)] Loss: 12383.017578\n",
      "Train Epoch: 851 [33024/118836 (28%)] Loss: 12228.912109\n",
      "Train Epoch: 851 [65792/118836 (55%)] Loss: 12279.718750\n",
      "Train Epoch: 851 [98560/118836 (83%)] Loss: 12272.835938\n",
      "    epoch          : 851\n",
      "    loss           : 12273.602305301385\n",
      "    val_loss       : 12275.823087328356\n",
      "    val_log_likelihood: -12177.870311369157\n",
      "    val_log_marginal: -12186.007368237784\n",
      "Train Epoch: 852 [256/118836 (0%)] Loss: 12326.981445\n",
      "Train Epoch: 852 [33024/118836 (28%)] Loss: 12363.313477\n",
      "Train Epoch: 852 [65792/118836 (55%)] Loss: 12339.922852\n",
      "Train Epoch: 852 [98560/118836 (83%)] Loss: 12364.021484\n",
      "    epoch          : 852\n",
      "    loss           : 12278.867876667184\n",
      "    val_loss       : 12275.038630449419\n",
      "    val_log_likelihood: -12174.136890961021\n",
      "    val_log_marginal: -12182.432570071067\n",
      "Train Epoch: 853 [256/118836 (0%)] Loss: 12354.152344\n",
      "Train Epoch: 853 [33024/118836 (28%)] Loss: 12231.125977\n",
      "Train Epoch: 853 [65792/118836 (55%)] Loss: 12284.134766\n",
      "Train Epoch: 853 [98560/118836 (83%)] Loss: 12355.190430\n",
      "    epoch          : 853\n",
      "    loss           : 12273.04862053479\n",
      "    val_loss       : 12273.298918268956\n",
      "    val_log_likelihood: -12178.089226310483\n",
      "    val_log_marginal: -12186.210317972056\n",
      "Train Epoch: 854 [256/118836 (0%)] Loss: 12429.377930\n",
      "Train Epoch: 854 [33024/118836 (28%)] Loss: 12349.378906\n",
      "Train Epoch: 854 [65792/118836 (55%)] Loss: 12237.843750\n",
      "Train Epoch: 854 [98560/118836 (83%)] Loss: 12313.390625\n",
      "    epoch          : 854\n",
      "    loss           : 12273.205756306865\n",
      "    val_loss       : 12273.220553180958\n",
      "    val_log_likelihood: -12178.118684895833\n",
      "    val_log_marginal: -12186.140359457524\n",
      "Train Epoch: 855 [256/118836 (0%)] Loss: 12332.528320\n",
      "Train Epoch: 855 [33024/118836 (28%)] Loss: 12384.091797\n",
      "Train Epoch: 855 [65792/118836 (55%)] Loss: 12263.771484\n",
      "Train Epoch: 855 [98560/118836 (83%)] Loss: 12321.566406\n",
      "    epoch          : 855\n",
      "    loss           : 12274.274015521609\n",
      "    val_loss       : 12270.14466515205\n",
      "    val_log_likelihood: -12176.979613833746\n",
      "    val_log_marginal: -12185.026714346854\n",
      "Train Epoch: 856 [256/118836 (0%)] Loss: 12215.633789\n",
      "Train Epoch: 856 [33024/118836 (28%)] Loss: 12302.262695\n",
      "Train Epoch: 856 [65792/118836 (55%)] Loss: 12343.952148\n",
      "Train Epoch: 856 [98560/118836 (83%)] Loss: 12275.558594\n",
      "    epoch          : 856\n",
      "    loss           : 12268.340052147953\n",
      "    val_loss       : 12272.417385336461\n",
      "    val_log_likelihood: -12177.793998300507\n",
      "    val_log_marginal: -12185.840866633704\n",
      "Train Epoch: 857 [256/118836 (0%)] Loss: 12297.951172\n",
      "Train Epoch: 857 [33024/118836 (28%)] Loss: 12188.010742\n",
      "Train Epoch: 857 [65792/118836 (55%)] Loss: 12298.521484\n",
      "Train Epoch: 857 [98560/118836 (83%)] Loss: 12257.796875\n",
      "    epoch          : 857\n",
      "    loss           : 12273.098901629704\n",
      "    val_loss       : 12273.942087082587\n",
      "    val_log_likelihood: -12176.916463922922\n",
      "    val_log_marginal: -12185.404849220871\n",
      "Train Epoch: 858 [256/118836 (0%)] Loss: 12344.320312\n",
      "Train Epoch: 858 [33024/118836 (28%)] Loss: 12277.605469\n",
      "Train Epoch: 858 [65792/118836 (55%)] Loss: 12325.210938\n",
      "Train Epoch: 858 [98560/118836 (83%)] Loss: 12290.474609\n",
      "    epoch          : 858\n",
      "    loss           : 12276.564337456059\n",
      "    val_loss       : 12273.74269325386\n",
      "    val_log_likelihood: -12175.504906075528\n",
      "    val_log_marginal: -12183.523791557984\n",
      "Train Epoch: 859 [256/118836 (0%)] Loss: 12352.337891\n",
      "Train Epoch: 859 [33024/118836 (28%)] Loss: 12254.847656\n",
      "Train Epoch: 859 [65792/118836 (55%)] Loss: 12388.410156\n",
      "Train Epoch: 859 [98560/118836 (83%)] Loss: 12390.149414\n",
      "    epoch          : 859\n",
      "    loss           : 12275.416893965828\n",
      "    val_loss       : 12279.821659131629\n",
      "    val_log_likelihood: -12179.457103785411\n",
      "    val_log_marginal: -12187.68221358853\n",
      "Train Epoch: 860 [256/118836 (0%)] Loss: 12376.402344\n",
      "Train Epoch: 860 [33024/118836 (28%)] Loss: 12411.526367\n",
      "Train Epoch: 860 [65792/118836 (55%)] Loss: 12332.734375\n",
      "Train Epoch: 860 [98560/118836 (83%)] Loss: 12449.708008\n",
      "    epoch          : 860\n",
      "    loss           : 12273.637327788978\n",
      "    val_loss       : 12272.098822822854\n",
      "    val_log_likelihood: -12179.236444440136\n",
      "    val_log_marginal: -12187.355915131187\n",
      "Train Epoch: 861 [256/118836 (0%)] Loss: 12266.056641\n",
      "Train Epoch: 861 [33024/118836 (28%)] Loss: 12334.470703\n",
      "Train Epoch: 861 [65792/118836 (55%)] Loss: 12343.237305\n",
      "Train Epoch: 861 [98560/118836 (83%)] Loss: 12387.606445\n",
      "    epoch          : 861\n",
      "    loss           : 12263.990702866522\n",
      "    val_loss       : 12271.905562924441\n",
      "    val_log_likelihood: -12178.279837901935\n",
      "    val_log_marginal: -12186.42698290173\n",
      "Train Epoch: 862 [256/118836 (0%)] Loss: 12292.164062\n",
      "Train Epoch: 862 [33024/118836 (28%)] Loss: 12333.625000\n",
      "Train Epoch: 862 [65792/118836 (55%)] Loss: 12274.393555\n",
      "Train Epoch: 862 [98560/118836 (83%)] Loss: 12264.880859\n",
      "    epoch          : 862\n",
      "    loss           : 12278.610414404982\n",
      "    val_loss       : 12279.587951580244\n",
      "    val_log_likelihood: -12176.864384951406\n",
      "    val_log_marginal: -12184.950200849311\n",
      "Train Epoch: 863 [256/118836 (0%)] Loss: 12342.019531\n",
      "Train Epoch: 863 [33024/118836 (28%)] Loss: 12233.008789\n",
      "Train Epoch: 863 [65792/118836 (55%)] Loss: 12344.513672\n",
      "Train Epoch: 863 [98560/118836 (83%)] Loss: 12450.876953\n",
      "    epoch          : 863\n",
      "    loss           : 12272.386781592482\n",
      "    val_loss       : 12272.41894032482\n",
      "    val_log_likelihood: -12177.121038823409\n",
      "    val_log_marginal: -12185.190346824631\n",
      "Train Epoch: 864 [256/118836 (0%)] Loss: 12222.262695\n",
      "Train Epoch: 864 [33024/118836 (28%)] Loss: 12299.228516\n",
      "Train Epoch: 864 [65792/118836 (55%)] Loss: 12303.990234\n",
      "Train Epoch: 864 [98560/118836 (83%)] Loss: 12311.035156\n",
      "    epoch          : 864\n",
      "    loss           : 12268.36685600186\n",
      "    val_loss       : 12267.054835248735\n",
      "    val_log_likelihood: -12178.459723622313\n",
      "    val_log_marginal: -12186.62109138844\n",
      "Train Epoch: 865 [256/118836 (0%)] Loss: 12320.792969\n",
      "Train Epoch: 865 [33024/118836 (28%)] Loss: 12244.607422\n",
      "Train Epoch: 865 [65792/118836 (55%)] Loss: 12201.071289\n",
      "Train Epoch: 865 [98560/118836 (83%)] Loss: 12226.080078\n",
      "    epoch          : 865\n",
      "    loss           : 12273.948366903172\n",
      "    val_loss       : 12277.75928782622\n",
      "    val_log_likelihood: -12178.537597252378\n",
      "    val_log_marginal: -12186.736337463371\n",
      "Train Epoch: 866 [256/118836 (0%)] Loss: 12234.827148\n",
      "Train Epoch: 866 [33024/118836 (28%)] Loss: 12226.149414\n",
      "Train Epoch: 866 [65792/118836 (55%)] Loss: 12286.916992\n",
      "Train Epoch: 866 [98560/118836 (83%)] Loss: 12297.923828\n",
      "    epoch          : 866\n",
      "    loss           : 12277.60215102228\n",
      "    val_loss       : 12275.243730645803\n",
      "    val_log_likelihood: -12176.40540687681\n",
      "    val_log_marginal: -12184.502754326913\n",
      "Train Epoch: 867 [256/118836 (0%)] Loss: 12345.491211\n",
      "Train Epoch: 867 [33024/118836 (28%)] Loss: 12274.263672\n",
      "Train Epoch: 867 [65792/118836 (55%)] Loss: 12290.363281\n",
      "Train Epoch: 867 [98560/118836 (83%)] Loss: 12372.783203\n",
      "    epoch          : 867\n",
      "    loss           : 12275.728780888132\n",
      "    val_loss       : 12270.957044098026\n",
      "    val_log_likelihood: -12172.844775673335\n",
      "    val_log_marginal: -12180.927014215555\n",
      "Train Epoch: 868 [256/118836 (0%)] Loss: 12395.905273\n",
      "Train Epoch: 868 [33024/118836 (28%)] Loss: 12359.289062\n",
      "Train Epoch: 868 [65792/118836 (55%)] Loss: 12329.539062\n",
      "Train Epoch: 868 [98560/118836 (83%)] Loss: 12337.611328\n",
      "    epoch          : 868\n",
      "    loss           : 12268.59928353107\n",
      "    val_loss       : 12272.293576728298\n",
      "    val_log_likelihood: -12171.958738336178\n",
      "    val_log_marginal: -12180.160912259218\n",
      "Train Epoch: 869 [256/118836 (0%)] Loss: 12368.534180\n",
      "Train Epoch: 869 [33024/118836 (28%)] Loss: 12324.283203\n",
      "Train Epoch: 869 [65792/118836 (55%)] Loss: 12193.762695\n",
      "Train Epoch: 869 [98560/118836 (83%)] Loss: 12265.155273\n",
      "    epoch          : 869\n",
      "    loss           : 12275.247440743899\n",
      "    val_loss       : 12273.963370121568\n",
      "    val_log_likelihood: -12173.95471043993\n",
      "    val_log_marginal: -12182.010523227631\n",
      "Train Epoch: 870 [256/118836 (0%)] Loss: 12263.468750\n",
      "Train Epoch: 870 [33024/118836 (28%)] Loss: 12283.523438\n",
      "Train Epoch: 870 [65792/118836 (55%)] Loss: 12309.548828\n",
      "Train Epoch: 870 [98560/118836 (83%)] Loss: 12312.324219\n",
      "    epoch          : 870\n",
      "    loss           : 12272.252077840672\n",
      "    val_loss       : 12271.823685072779\n",
      "    val_log_likelihood: -12169.50908550455\n",
      "    val_log_marginal: -12177.581714595155\n",
      "Train Epoch: 871 [256/118836 (0%)] Loss: 12317.773438\n",
      "Train Epoch: 871 [33024/118836 (28%)] Loss: 12300.024414\n",
      "Train Epoch: 871 [65792/118836 (55%)] Loss: 12256.523438\n",
      "Train Epoch: 871 [98560/118836 (83%)] Loss: 12242.300781\n",
      "    epoch          : 871\n",
      "    loss           : 12270.326489964587\n",
      "    val_loss       : 12277.208554902978\n",
      "    val_log_likelihood: -12174.47217257289\n",
      "    val_log_marginal: -12182.844618402396\n",
      "Train Epoch: 872 [256/118836 (0%)] Loss: 12284.188477\n",
      "Train Epoch: 872 [33024/118836 (28%)] Loss: 12325.889648\n",
      "Train Epoch: 872 [65792/118836 (55%)] Loss: 12285.073242\n",
      "Train Epoch: 872 [98560/118836 (83%)] Loss: 12319.643555\n",
      "    epoch          : 872\n",
      "    loss           : 12270.717291375879\n",
      "    val_loss       : 12270.465505839615\n",
      "    val_log_likelihood: -12173.240410301643\n",
      "    val_log_marginal: -12181.283486380402\n",
      "Train Epoch: 873 [256/118836 (0%)] Loss: 12280.257812\n",
      "Train Epoch: 873 [33024/118836 (28%)] Loss: 12261.133789\n",
      "Train Epoch: 873 [65792/118836 (55%)] Loss: 12197.306641\n",
      "Train Epoch: 873 [98560/118836 (83%)] Loss: 12363.458008\n",
      "    epoch          : 873\n",
      "    loss           : 12271.969897158033\n",
      "    val_loss       : 12273.307426303412\n",
      "    val_log_likelihood: -12173.713315498346\n",
      "    val_log_marginal: -12181.599957265595\n",
      "Train Epoch: 874 [256/118836 (0%)] Loss: 12303.446289\n",
      "Train Epoch: 874 [33024/118836 (28%)] Loss: 12195.878906\n",
      "Train Epoch: 874 [65792/118836 (55%)] Loss: 12292.592773\n",
      "Train Epoch: 874 [98560/118836 (83%)] Loss: 12443.021484\n",
      "    epoch          : 874\n",
      "    loss           : 12274.656801527606\n",
      "    val_loss       : 12273.468842379627\n",
      "    val_log_likelihood: -12175.212221651416\n",
      "    val_log_marginal: -12183.333587594205\n",
      "Train Epoch: 875 [256/118836 (0%)] Loss: 12259.066406\n",
      "Train Epoch: 875 [33024/118836 (28%)] Loss: 12322.173828\n",
      "Train Epoch: 875 [65792/118836 (55%)] Loss: 12362.080078\n",
      "Train Epoch: 875 [98560/118836 (83%)] Loss: 12264.756836\n",
      "    epoch          : 875\n",
      "    loss           : 12277.463785120708\n",
      "    val_loss       : 12269.05045526387\n",
      "    val_log_likelihood: -12174.85596163539\n",
      "    val_log_marginal: -12182.811568233414\n",
      "Train Epoch: 876 [256/118836 (0%)] Loss: 12314.409180\n",
      "Train Epoch: 876 [33024/118836 (28%)] Loss: 12314.425781\n",
      "Train Epoch: 876 [65792/118836 (55%)] Loss: 12305.697266\n",
      "Train Epoch: 876 [98560/118836 (83%)] Loss: 12283.023438\n",
      "    epoch          : 876\n",
      "    loss           : 12272.473459147539\n",
      "    val_loss       : 12272.75647676197\n",
      "    val_log_likelihood: -12172.788341507703\n",
      "    val_log_marginal: -12180.933755902646\n",
      "Train Epoch: 877 [256/118836 (0%)] Loss: 12252.355469\n",
      "Train Epoch: 877 [33024/118836 (28%)] Loss: 12351.990234\n",
      "Train Epoch: 877 [65792/118836 (55%)] Loss: 12332.043945\n",
      "Train Epoch: 877 [98560/118836 (83%)] Loss: 12325.273438\n",
      "    epoch          : 877\n",
      "    loss           : 12272.51893319634\n",
      "    val_loss       : 12275.111358549788\n",
      "    val_log_likelihood: -12172.189203209005\n",
      "    val_log_marginal: -12180.318090917763\n",
      "Train Epoch: 878 [256/118836 (0%)] Loss: 12360.982422\n",
      "Train Epoch: 878 [33024/118836 (28%)] Loss: 12367.787109\n",
      "Train Epoch: 878 [65792/118836 (55%)] Loss: 12241.272461\n",
      "Train Epoch: 878 [98560/118836 (83%)] Loss: 12233.024414\n",
      "    epoch          : 878\n",
      "    loss           : 12268.014910308106\n",
      "    val_loss       : 12277.260326345873\n",
      "    val_log_likelihood: -12172.17955406069\n",
      "    val_log_marginal: -12180.290611401972\n",
      "Train Epoch: 879 [256/118836 (0%)] Loss: 12205.288086\n",
      "Train Epoch: 879 [33024/118836 (28%)] Loss: 12341.275391\n",
      "Train Epoch: 879 [65792/118836 (55%)] Loss: 12206.747070\n",
      "Train Epoch: 879 [98560/118836 (83%)] Loss: 12301.672852\n",
      "    epoch          : 879\n",
      "    loss           : 12271.648945570978\n",
      "    val_loss       : 12270.901658911722\n",
      "    val_log_likelihood: -12172.343984568857\n",
      "    val_log_marginal: -12180.519541767459\n",
      "Train Epoch: 880 [256/118836 (0%)] Loss: 12244.228516\n",
      "Train Epoch: 880 [33024/118836 (28%)] Loss: 12242.629883\n",
      "Train Epoch: 880 [65792/118836 (55%)] Loss: 12165.175781\n",
      "Train Epoch: 880 [98560/118836 (83%)] Loss: 12355.742188\n",
      "    epoch          : 880\n",
      "    loss           : 12268.693282962418\n",
      "    val_loss       : 12270.006618628144\n",
      "    val_log_likelihood: -12171.80733431555\n",
      "    val_log_marginal: -12180.176971201754\n",
      "Train Epoch: 881 [256/118836 (0%)] Loss: 12212.017578\n",
      "Train Epoch: 881 [33024/118836 (28%)] Loss: 12168.479492\n",
      "Train Epoch: 881 [65792/118836 (55%)] Loss: 12380.861328\n",
      "Train Epoch: 881 [98560/118836 (83%)] Loss: 12343.752930\n",
      "    epoch          : 881\n",
      "    loss           : 12270.546068709937\n",
      "    val_loss       : 12272.550932597629\n",
      "    val_log_likelihood: -12169.073263188844\n",
      "    val_log_marginal: -12177.333835702506\n",
      "Train Epoch: 882 [256/118836 (0%)] Loss: 12371.079102\n",
      "Train Epoch: 882 [33024/118836 (28%)] Loss: 12210.666992\n",
      "Train Epoch: 882 [65792/118836 (55%)] Loss: 12347.694336\n",
      "Train Epoch: 882 [98560/118836 (83%)] Loss: 12327.867188\n",
      "    epoch          : 882\n",
      "    loss           : 12271.628966992348\n",
      "    val_loss       : 12267.63428653371\n",
      "    val_log_likelihood: -12173.632677445203\n",
      "    val_log_marginal: -12181.707838978988\n",
      "Train Epoch: 883 [256/118836 (0%)] Loss: 12240.803711\n",
      "Train Epoch: 883 [33024/118836 (28%)] Loss: 12277.350586\n",
      "Train Epoch: 883 [65792/118836 (55%)] Loss: 12293.386719\n",
      "Train Epoch: 883 [98560/118836 (83%)] Loss: 12313.777344\n",
      "    epoch          : 883\n",
      "    loss           : 12272.881315104167\n",
      "    val_loss       : 12279.098926726647\n",
      "    val_log_likelihood: -12170.294812344913\n",
      "    val_log_marginal: -12178.405254282194\n",
      "Train Epoch: 884 [256/118836 (0%)] Loss: 12283.068359\n",
      "Train Epoch: 884 [33024/118836 (28%)] Loss: 12455.195312\n",
      "Train Epoch: 884 [65792/118836 (55%)] Loss: 12295.578125\n",
      "Train Epoch: 884 [98560/118836 (83%)] Loss: 12242.911133\n",
      "    epoch          : 884\n",
      "    loss           : 12268.936269482787\n",
      "    val_loss       : 12269.973117410565\n",
      "    val_log_likelihood: -12171.23859093905\n",
      "    val_log_marginal: -12179.219090189448\n",
      "Train Epoch: 885 [256/118836 (0%)] Loss: 12308.093750\n",
      "Train Epoch: 885 [33024/118836 (28%)] Loss: 12221.382812\n",
      "Train Epoch: 885 [65792/118836 (55%)] Loss: 12305.738281\n",
      "Train Epoch: 885 [98560/118836 (83%)] Loss: 12190.988281\n",
      "    epoch          : 885\n",
      "    loss           : 12271.578789934865\n",
      "    val_loss       : 12269.366661729175\n",
      "    val_log_likelihood: -12171.726341178144\n",
      "    val_log_marginal: -12179.819969750157\n",
      "Train Epoch: 886 [256/118836 (0%)] Loss: 12231.046875\n",
      "Train Epoch: 886 [33024/118836 (28%)] Loss: 12272.554688\n",
      "Train Epoch: 886 [65792/118836 (55%)] Loss: 12246.620117\n",
      "Train Epoch: 886 [98560/118836 (83%)] Loss: 12334.432617\n",
      "    epoch          : 886\n",
      "    loss           : 12273.862259938482\n",
      "    val_loss       : 12273.614615799654\n",
      "    val_log_likelihood: -12171.821853998656\n",
      "    val_log_marginal: -12179.704844197187\n",
      "Train Epoch: 887 [256/118836 (0%)] Loss: 12188.437500\n",
      "Train Epoch: 887 [33024/118836 (28%)] Loss: 12256.360352\n",
      "Train Epoch: 887 [65792/118836 (55%)] Loss: 12318.617188\n",
      "Train Epoch: 887 [98560/118836 (83%)] Loss: 12331.260742\n",
      "    epoch          : 887\n",
      "    loss           : 12267.636722304072\n",
      "    val_loss       : 12272.238131074257\n",
      "    val_log_likelihood: -12170.96181842561\n",
      "    val_log_marginal: -12179.05380765939\n",
      "Train Epoch: 888 [256/118836 (0%)] Loss: 12324.173828\n",
      "Train Epoch: 888 [33024/118836 (28%)] Loss: 12246.891602\n",
      "Train Epoch: 888 [65792/118836 (55%)] Loss: 12368.605469\n",
      "Train Epoch: 888 [98560/118836 (83%)] Loss: 12217.567383\n",
      "    epoch          : 888\n",
      "    loss           : 12268.88887397255\n",
      "    val_loss       : 12273.24593254664\n",
      "    val_log_likelihood: -12171.42449241367\n",
      "    val_log_marginal: -12179.676713294522\n",
      "Train Epoch: 889 [256/118836 (0%)] Loss: 12212.558594\n",
      "Train Epoch: 889 [33024/118836 (28%)] Loss: 12273.873047\n",
      "Train Epoch: 889 [65792/118836 (55%)] Loss: 12292.337891\n",
      "Train Epoch: 889 [98560/118836 (83%)] Loss: 12331.142578\n",
      "    epoch          : 889\n",
      "    loss           : 12270.732735118123\n",
      "    val_loss       : 12271.802961523585\n",
      "    val_log_likelihood: -12170.105897662066\n",
      "    val_log_marginal: -12178.202584030529\n",
      "Train Epoch: 890 [256/118836 (0%)] Loss: 12267.455078\n",
      "Train Epoch: 890 [33024/118836 (28%)] Loss: 12453.731445\n",
      "Train Epoch: 890 [65792/118836 (55%)] Loss: 12360.943359\n",
      "Train Epoch: 890 [98560/118836 (83%)] Loss: 12295.005859\n",
      "    epoch          : 890\n",
      "    loss           : 12276.268931096205\n",
      "    val_loss       : 12281.179254814766\n",
      "    val_log_likelihood: -12172.905268752585\n",
      "    val_log_marginal: -12181.214267670463\n",
      "Train Epoch: 891 [256/118836 (0%)] Loss: 12224.262695\n",
      "Train Epoch: 891 [33024/118836 (28%)] Loss: 12327.361328\n",
      "Train Epoch: 891 [65792/118836 (55%)] Loss: 12257.220703\n",
      "Train Epoch: 891 [98560/118836 (83%)] Loss: 12276.867188\n",
      "    epoch          : 891\n",
      "    loss           : 12269.260729263597\n",
      "    val_loss       : 12270.367497740268\n",
      "    val_log_likelihood: -12173.021948827802\n",
      "    val_log_marginal: -12181.189633099979\n",
      "Train Epoch: 892 [256/118836 (0%)] Loss: 12241.679688\n",
      "Train Epoch: 892 [33024/118836 (28%)] Loss: 12384.640625\n",
      "Train Epoch: 892 [65792/118836 (55%)] Loss: 12194.620117\n",
      "Train Epoch: 892 [98560/118836 (83%)] Loss: 12273.609375\n",
      "    epoch          : 892\n",
      "    loss           : 12265.734744623656\n",
      "    val_loss       : 12273.45102676024\n",
      "    val_log_likelihood: -12171.597510048334\n",
      "    val_log_marginal: -12179.838567901465\n",
      "Train Epoch: 893 [256/118836 (0%)] Loss: 12296.173828\n",
      "Train Epoch: 893 [33024/118836 (28%)] Loss: 12300.865234\n",
      "Train Epoch: 893 [65792/118836 (55%)] Loss: 12339.976562\n",
      "Train Epoch: 893 [98560/118836 (83%)] Loss: 12308.674805\n",
      "    epoch          : 893\n",
      "    loss           : 12268.458689386891\n",
      "    val_loss       : 12273.903281671826\n",
      "    val_log_likelihood: -12173.841782174059\n",
      "    val_log_marginal: -12182.065086682294\n",
      "Train Epoch: 894 [256/118836 (0%)] Loss: 12289.905273\n",
      "Train Epoch: 894 [33024/118836 (28%)] Loss: 12298.198242\n",
      "Train Epoch: 894 [65792/118836 (55%)] Loss: 12330.897461\n",
      "Train Epoch: 894 [98560/118836 (83%)] Loss: 12348.458984\n",
      "    epoch          : 894\n",
      "    loss           : 12274.572853501086\n",
      "    val_loss       : 12273.119041267391\n",
      "    val_log_likelihood: -12170.517605749845\n",
      "    val_log_marginal: -12178.653826064537\n",
      "Train Epoch: 895 [256/118836 (0%)] Loss: 12225.592773\n",
      "Train Epoch: 895 [33024/118836 (28%)] Loss: 12230.472656\n",
      "Train Epoch: 895 [65792/118836 (55%)] Loss: 12219.728516\n",
      "Train Epoch: 895 [98560/118836 (83%)] Loss: 12267.166016\n",
      "    epoch          : 895\n",
      "    loss           : 12272.796110712625\n",
      "    val_loss       : 12274.50157415933\n",
      "    val_log_likelihood: -12172.182841901882\n",
      "    val_log_marginal: -12180.332117473126\n",
      "Train Epoch: 896 [256/118836 (0%)] Loss: 12311.875000\n",
      "Train Epoch: 896 [33024/118836 (28%)] Loss: 12313.822266\n",
      "Train Epoch: 896 [65792/118836 (55%)] Loss: 12269.219727\n",
      "Train Epoch: 896 [98560/118836 (83%)] Loss: 12261.620117\n",
      "    epoch          : 896\n",
      "    loss           : 12271.478461344603\n",
      "    val_loss       : 12270.167861807395\n",
      "    val_log_likelihood: -12170.6461275137\n",
      "    val_log_marginal: -12178.651408160425\n",
      "Train Epoch: 897 [256/118836 (0%)] Loss: 12286.262695\n",
      "Train Epoch: 897 [33024/118836 (28%)] Loss: 12332.317383\n",
      "Train Epoch: 897 [65792/118836 (55%)] Loss: 12259.201172\n",
      "Train Epoch: 897 [98560/118836 (83%)] Loss: 12345.826172\n",
      "    epoch          : 897\n",
      "    loss           : 12271.841872964485\n",
      "    val_loss       : 12272.211784182698\n",
      "    val_log_likelihood: -12169.59413141672\n",
      "    val_log_marginal: -12177.765366889234\n",
      "Train Epoch: 898 [256/118836 (0%)] Loss: 12280.092773\n",
      "Train Epoch: 898 [33024/118836 (28%)] Loss: 12348.475586\n",
      "Train Epoch: 898 [65792/118836 (55%)] Loss: 12323.586914\n",
      "Train Epoch: 898 [98560/118836 (83%)] Loss: 12241.873047\n",
      "    epoch          : 898\n",
      "    loss           : 12271.207935277089\n",
      "    val_loss       : 12272.820479869579\n",
      "    val_log_likelihood: -12170.929815608199\n",
      "    val_log_marginal: -12179.007795666706\n",
      "Train Epoch: 899 [256/118836 (0%)] Loss: 12285.515625\n",
      "Train Epoch: 899 [33024/118836 (28%)] Loss: 12409.734375\n",
      "Train Epoch: 899 [65792/118836 (55%)] Loss: 12281.709961\n",
      "Train Epoch: 899 [98560/118836 (83%)] Loss: 12303.446289\n",
      "    epoch          : 899\n",
      "    loss           : 12275.661541046318\n",
      "    val_loss       : 12271.56153010843\n",
      "    val_log_likelihood: -12174.692414476065\n",
      "    val_log_marginal: -12183.137339198296\n",
      "Train Epoch: 900 [256/118836 (0%)] Loss: 12325.519531\n",
      "Train Epoch: 900 [33024/118836 (28%)] Loss: 12302.318359\n",
      "Train Epoch: 900 [65792/118836 (55%)] Loss: 12295.241211\n",
      "Train Epoch: 900 [98560/118836 (83%)] Loss: 12359.701172\n",
      "    epoch          : 900\n",
      "    loss           : 12270.503711099049\n",
      "    val_loss       : 12272.613883192458\n",
      "    val_log_likelihood: -12171.065271531224\n",
      "    val_log_marginal: -12179.277153454485\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch900.pth ...\n",
      "Train Epoch: 901 [256/118836 (0%)] Loss: 12194.260742\n",
      "Train Epoch: 901 [33024/118836 (28%)] Loss: 12366.373047\n",
      "Train Epoch: 901 [65792/118836 (55%)] Loss: 12250.324219\n",
      "Train Epoch: 901 [98560/118836 (83%)] Loss: 12207.492188\n",
      "    epoch          : 901\n",
      "    loss           : 12268.673884020885\n",
      "    val_loss       : 12274.228355450385\n",
      "    val_log_likelihood: -12173.302858767576\n",
      "    val_log_marginal: -12181.405119211659\n",
      "Train Epoch: 902 [256/118836 (0%)] Loss: 12224.751953\n",
      "Train Epoch: 902 [33024/118836 (28%)] Loss: 12245.519531\n",
      "Train Epoch: 902 [65792/118836 (55%)] Loss: 12368.638672\n",
      "Train Epoch: 902 [98560/118836 (83%)] Loss: 12218.587891\n",
      "    epoch          : 902\n",
      "    loss           : 12271.521503922406\n",
      "    val_loss       : 12271.863283712235\n",
      "    val_log_likelihood: -12170.905601704664\n",
      "    val_log_marginal: -12179.067788719787\n",
      "Train Epoch: 903 [256/118836 (0%)] Loss: 12218.471680\n",
      "Train Epoch: 903 [33024/118836 (28%)] Loss: 12284.445312\n",
      "Train Epoch: 903 [65792/118836 (55%)] Loss: 12276.033203\n",
      "Train Epoch: 903 [98560/118836 (83%)] Loss: 12360.361328\n",
      "    epoch          : 903\n",
      "    loss           : 12269.99535304875\n",
      "    val_loss       : 12269.332842562539\n",
      "    val_log_likelihood: -12172.8224947012\n",
      "    val_log_marginal: -12180.976872256424\n",
      "Train Epoch: 904 [256/118836 (0%)] Loss: 12351.545898\n",
      "Train Epoch: 904 [33024/118836 (28%)] Loss: 12413.738281\n",
      "Train Epoch: 904 [65792/118836 (55%)] Loss: 12269.904297\n",
      "Train Epoch: 904 [98560/118836 (83%)] Loss: 12220.141602\n",
      "    epoch          : 904\n",
      "    loss           : 12271.46204459393\n",
      "    val_loss       : 12276.615091888158\n",
      "    val_log_likelihood: -12168.393126098532\n",
      "    val_log_marginal: -12176.58298077661\n",
      "Train Epoch: 905 [256/118836 (0%)] Loss: 12250.533203\n",
      "Train Epoch: 905 [33024/118836 (28%)] Loss: 12269.863281\n",
      "Train Epoch: 905 [65792/118836 (55%)] Loss: 12319.141602\n",
      "Train Epoch: 905 [98560/118836 (83%)] Loss: 12190.654297\n",
      "    epoch          : 905\n",
      "    loss           : 12269.0174973506\n",
      "    val_loss       : 12269.13879214887\n",
      "    val_log_likelihood: -12171.752512083849\n",
      "    val_log_marginal: -12179.923516205645\n",
      "Train Epoch: 906 [256/118836 (0%)] Loss: 12238.581055\n",
      "Train Epoch: 906 [33024/118836 (28%)] Loss: 12349.075195\n",
      "Train Epoch: 906 [65792/118836 (55%)] Loss: 12197.848633\n",
      "Train Epoch: 906 [98560/118836 (83%)] Loss: 12312.195312\n",
      "    epoch          : 906\n",
      "    loss           : 12274.170066945824\n",
      "    val_loss       : 12271.361558517365\n",
      "    val_log_likelihood: -12170.95444630764\n",
      "    val_log_marginal: -12179.25296817165\n",
      "Train Epoch: 907 [256/118836 (0%)] Loss: 12340.121094\n",
      "Train Epoch: 907 [33024/118836 (28%)] Loss: 12359.845703\n",
      "Train Epoch: 907 [65792/118836 (55%)] Loss: 12285.489258\n",
      "Train Epoch: 907 [98560/118836 (83%)] Loss: 12305.673828\n",
      "    epoch          : 907\n",
      "    loss           : 12274.00445438508\n",
      "    val_loss       : 12272.003674090676\n",
      "    val_log_likelihood: -12171.015771686312\n",
      "    val_log_marginal: -12179.294880562793\n",
      "Train Epoch: 908 [256/118836 (0%)] Loss: 12268.814453\n",
      "Train Epoch: 908 [33024/118836 (28%)] Loss: 12330.277344\n",
      "Train Epoch: 908 [65792/118836 (55%)] Loss: 12345.623047\n",
      "Train Epoch: 908 [98560/118836 (83%)] Loss: 12216.259766\n",
      "    epoch          : 908\n",
      "    loss           : 12268.816203506254\n",
      "    val_loss       : 12276.826646750887\n",
      "    val_log_likelihood: -12173.448489518714\n",
      "    val_log_marginal: -12182.02068233873\n",
      "Train Epoch: 909 [256/118836 (0%)] Loss: 12287.803711\n",
      "Train Epoch: 909 [33024/118836 (28%)] Loss: 12327.679688\n",
      "Train Epoch: 909 [65792/118836 (55%)] Loss: 12251.408203\n",
      "Train Epoch: 909 [98560/118836 (83%)] Loss: 12386.146484\n",
      "    epoch          : 909\n",
      "    loss           : 12270.790387684812\n",
      "    val_loss       : 12272.673611671557\n",
      "    val_log_likelihood: -12173.902870075994\n",
      "    val_log_marginal: -12182.191572705788\n",
      "Train Epoch: 910 [256/118836 (0%)] Loss: 12372.039062\n",
      "Train Epoch: 910 [33024/118836 (28%)] Loss: 12258.986328\n",
      "Train Epoch: 910 [65792/118836 (55%)] Loss: 12285.329102\n",
      "Train Epoch: 910 [98560/118836 (83%)] Loss: 12341.623047\n",
      "    epoch          : 910\n",
      "    loss           : 12270.608673393559\n",
      "    val_loss       : 12269.596685900602\n",
      "    val_log_likelihood: -12170.240904317876\n",
      "    val_log_marginal: -12178.476767098648\n",
      "Train Epoch: 911 [256/118836 (0%)] Loss: 12278.124023\n",
      "Train Epoch: 911 [33024/118836 (28%)] Loss: 12335.583984\n",
      "Train Epoch: 911 [65792/118836 (55%)] Loss: 12289.539062\n",
      "Train Epoch: 911 [98560/118836 (83%)] Loss: 12202.649414\n",
      "    epoch          : 911\n",
      "    loss           : 12271.823559307795\n",
      "    val_loss       : 12273.54875548877\n",
      "    val_log_likelihood: -12171.222586784015\n",
      "    val_log_marginal: -12179.19042550575\n",
      "Train Epoch: 912 [256/118836 (0%)] Loss: 12231.452148\n",
      "Train Epoch: 912 [33024/118836 (28%)] Loss: 12371.072266\n",
      "Train Epoch: 912 [65792/118836 (55%)] Loss: 12368.344727\n",
      "Train Epoch: 912 [98560/118836 (83%)] Loss: 12335.350586\n",
      "    epoch          : 912\n",
      "    loss           : 12266.627771531224\n",
      "    val_loss       : 12274.140929585079\n",
      "    val_log_likelihood: -12171.247332667752\n",
      "    val_log_marginal: -12179.374480441118\n",
      "Train Epoch: 913 [256/118836 (0%)] Loss: 12248.416992\n",
      "Train Epoch: 913 [33024/118836 (28%)] Loss: 12229.376953\n",
      "Train Epoch: 913 [65792/118836 (55%)] Loss: 12191.027344\n",
      "Train Epoch: 913 [98560/118836 (83%)] Loss: 12222.535156\n",
      "    epoch          : 913\n",
      "    loss           : 12271.967837733924\n",
      "    val_loss       : 12271.322503174046\n",
      "    val_log_likelihood: -12173.021266607217\n",
      "    val_log_marginal: -12181.260742109333\n",
      "Train Epoch: 914 [256/118836 (0%)] Loss: 12345.585938\n",
      "Train Epoch: 914 [33024/118836 (28%)] Loss: 12236.395508\n",
      "Train Epoch: 914 [65792/118836 (55%)] Loss: 12297.777344\n",
      "Train Epoch: 914 [98560/118836 (83%)] Loss: 12297.993164\n",
      "    epoch          : 914\n",
      "    loss           : 12273.275215829199\n",
      "    val_loss       : 12268.020194738197\n",
      "    val_log_likelihood: -12172.041242762612\n",
      "    val_log_marginal: -12180.212201869912\n",
      "Train Epoch: 915 [256/118836 (0%)] Loss: 12306.208984\n",
      "Train Epoch: 915 [33024/118836 (28%)] Loss: 12363.759766\n",
      "Train Epoch: 915 [65792/118836 (55%)] Loss: 12378.820312\n",
      "Train Epoch: 915 [98560/118836 (83%)] Loss: 12256.582031\n",
      "    epoch          : 915\n",
      "    loss           : 12268.408353365385\n",
      "    val_loss       : 12274.091548593638\n",
      "    val_log_likelihood: -12171.732158388906\n",
      "    val_log_marginal: -12180.005043497265\n",
      "Train Epoch: 916 [256/118836 (0%)] Loss: 12324.546875\n",
      "Train Epoch: 916 [33024/118836 (28%)] Loss: 12226.099609\n",
      "Train Epoch: 916 [65792/118836 (55%)] Loss: 12365.332031\n",
      "Train Epoch: 916 [98560/118836 (83%)] Loss: 12226.314453\n",
      "    epoch          : 916\n",
      "    loss           : 12270.321443503153\n",
      "    val_loss       : 12271.537565616018\n",
      "    val_log_likelihood: -12171.948832486818\n",
      "    val_log_marginal: -12180.104676761077\n",
      "Train Epoch: 917 [256/118836 (0%)] Loss: 12340.739258\n",
      "Train Epoch: 917 [33024/118836 (28%)] Loss: 12409.668945\n",
      "Train Epoch: 917 [65792/118836 (55%)] Loss: 12333.322266\n",
      "Train Epoch: 917 [98560/118836 (83%)] Loss: 12399.155273\n",
      "    epoch          : 917\n",
      "    loss           : 12271.712572373863\n",
      "    val_loss       : 12269.372625816992\n",
      "    val_log_likelihood: -12169.95629458747\n",
      "    val_log_marginal: -12178.055240696765\n",
      "Train Epoch: 918 [256/118836 (0%)] Loss: 12262.941406\n",
      "Train Epoch: 918 [33024/118836 (28%)] Loss: 12185.335938\n",
      "Train Epoch: 918 [65792/118836 (55%)] Loss: 12362.220703\n",
      "Train Epoch: 918 [98560/118836 (83%)] Loss: 12311.232422\n",
      "    epoch          : 918\n",
      "    loss           : 12269.88530875207\n",
      "    val_loss       : 12268.79207681041\n",
      "    val_log_likelihood: -12172.899666240179\n",
      "    val_log_marginal: -12180.909700196185\n",
      "Train Epoch: 919 [256/118836 (0%)] Loss: 12211.437500\n",
      "Train Epoch: 919 [33024/118836 (28%)] Loss: 12271.064453\n",
      "Train Epoch: 919 [65792/118836 (55%)] Loss: 12274.749023\n",
      "Train Epoch: 919 [98560/118836 (83%)] Loss: 12325.947266\n",
      "    epoch          : 919\n",
      "    loss           : 12267.268481990539\n",
      "    val_loss       : 12269.819256323075\n",
      "    val_log_likelihood: -12173.583043999432\n",
      "    val_log_marginal: -12181.872584651856\n",
      "Train Epoch: 920 [256/118836 (0%)] Loss: 12342.720703\n",
      "Train Epoch: 920 [33024/118836 (28%)] Loss: 12339.412109\n",
      "Train Epoch: 920 [65792/118836 (55%)] Loss: 12229.373047\n",
      "Train Epoch: 920 [98560/118836 (83%)] Loss: 12296.937500\n",
      "    epoch          : 920\n",
      "    loss           : 12273.677366205282\n",
      "    val_loss       : 12270.473302574785\n",
      "    val_log_likelihood: -12170.05524242013\n",
      "    val_log_marginal: -12178.274315786206\n",
      "Train Epoch: 921 [256/118836 (0%)] Loss: 12239.973633\n",
      "Train Epoch: 921 [33024/118836 (28%)] Loss: 12219.277344\n",
      "Train Epoch: 921 [65792/118836 (55%)] Loss: 12231.319336\n",
      "Train Epoch: 921 [98560/118836 (83%)] Loss: 12226.846680\n",
      "    epoch          : 921\n",
      "    loss           : 12275.294802813534\n",
      "    val_loss       : 12269.314778880494\n",
      "    val_log_likelihood: -12171.510298736042\n",
      "    val_log_marginal: -12179.584749551193\n",
      "Train Epoch: 922 [256/118836 (0%)] Loss: 12334.755859\n",
      "Train Epoch: 922 [33024/118836 (28%)] Loss: 12398.326172\n",
      "Train Epoch: 922 [65792/118836 (55%)] Loss: 12389.086914\n",
      "Train Epoch: 922 [98560/118836 (83%)] Loss: 12371.945312\n",
      "    epoch          : 922\n",
      "    loss           : 12274.777923710193\n",
      "    val_loss       : 12279.04250281814\n",
      "    val_log_likelihood: -12175.317730627068\n",
      "    val_log_marginal: -12183.46538293554\n",
      "Train Epoch: 923 [256/118836 (0%)] Loss: 12311.629883\n",
      "Train Epoch: 923 [33024/118836 (28%)] Loss: 12346.770508\n",
      "Train Epoch: 923 [65792/118836 (55%)] Loss: 12246.006836\n",
      "Train Epoch: 923 [98560/118836 (83%)] Loss: 12198.573242\n",
      "    epoch          : 923\n",
      "    loss           : 12272.007780836435\n",
      "    val_loss       : 12270.648528421782\n",
      "    val_log_likelihood: -12170.871056755324\n",
      "    val_log_marginal: -12179.067352384587\n",
      "Train Epoch: 924 [256/118836 (0%)] Loss: 12251.833984\n",
      "Train Epoch: 924 [33024/118836 (28%)] Loss: 12283.997070\n",
      "Train Epoch: 924 [65792/118836 (55%)] Loss: 12241.280273\n",
      "Train Epoch: 924 [98560/118836 (83%)] Loss: 12303.700195\n",
      "    epoch          : 924\n",
      "    loss           : 12273.366185574338\n",
      "    val_loss       : 12270.36282134797\n",
      "    val_log_likelihood: -12170.253414495452\n",
      "    val_log_marginal: -12178.462081115355\n",
      "Train Epoch: 925 [256/118836 (0%)] Loss: 12282.009766\n",
      "Train Epoch: 925 [33024/118836 (28%)] Loss: 12351.408203\n",
      "Train Epoch: 925 [65792/118836 (55%)] Loss: 12225.916016\n",
      "Train Epoch: 925 [98560/118836 (83%)] Loss: 12275.587891\n",
      "    epoch          : 925\n",
      "    loss           : 12269.48097649788\n",
      "    val_loss       : 12267.223197360468\n",
      "    val_log_likelihood: -12173.329925946031\n",
      "    val_log_marginal: -12181.279803587251\n",
      "Train Epoch: 926 [256/118836 (0%)] Loss: 12351.982422\n",
      "Train Epoch: 926 [33024/118836 (28%)] Loss: 12257.188477\n",
      "Train Epoch: 926 [65792/118836 (55%)] Loss: 12349.709961\n",
      "Train Epoch: 926 [98560/118836 (83%)] Loss: 12231.189453\n",
      "    epoch          : 926\n",
      "    loss           : 12271.04212158809\n",
      "    val_loss       : 12272.275651967666\n",
      "    val_log_likelihood: -12172.391501402244\n",
      "    val_log_marginal: -12180.481019057317\n",
      "Train Epoch: 927 [256/118836 (0%)] Loss: 12351.498047\n",
      "Train Epoch: 927 [33024/118836 (28%)] Loss: 12298.361328\n",
      "Train Epoch: 927 [65792/118836 (55%)] Loss: 12296.312500\n",
      "Train Epoch: 927 [98560/118836 (83%)] Loss: 12200.583008\n",
      "    epoch          : 927\n",
      "    loss           : 12268.59684640586\n",
      "    val_loss       : 12271.922672840368\n",
      "    val_log_likelihood: -12173.267037098065\n",
      "    val_log_marginal: -12181.568364680426\n",
      "Train Epoch: 928 [256/118836 (0%)] Loss: 12199.199219\n",
      "Train Epoch: 928 [33024/118836 (28%)] Loss: 12219.171875\n",
      "Train Epoch: 928 [65792/118836 (55%)] Loss: 12306.810547\n",
      "Train Epoch: 928 [98560/118836 (83%)] Loss: 12346.472656\n",
      "    epoch          : 928\n",
      "    loss           : 12268.463498371588\n",
      "    val_loss       : 12272.0762926653\n",
      "    val_log_likelihood: -12170.872937344913\n",
      "    val_log_marginal: -12179.202847277096\n",
      "Train Epoch: 929 [256/118836 (0%)] Loss: 12352.638672\n",
      "Train Epoch: 929 [33024/118836 (28%)] Loss: 12342.900391\n",
      "Train Epoch: 929 [65792/118836 (55%)] Loss: 12203.486328\n",
      "Train Epoch: 929 [98560/118836 (83%)] Loss: 12258.021484\n",
      "    epoch          : 929\n",
      "    loss           : 12269.582082299421\n",
      "    val_loss       : 12273.856503986535\n",
      "    val_log_likelihood: -12172.199743460505\n",
      "    val_log_marginal: -12180.578855147161\n",
      "Train Epoch: 930 [256/118836 (0%)] Loss: 12352.967773\n",
      "Train Epoch: 930 [33024/118836 (28%)] Loss: 12314.687500\n",
      "Train Epoch: 930 [65792/118836 (55%)] Loss: 12208.922852\n",
      "Train Epoch: 930 [98560/118836 (83%)] Loss: 12303.426758\n",
      "    epoch          : 930\n",
      "    loss           : 12269.969853701406\n",
      "    val_loss       : 12274.115106316309\n",
      "    val_log_likelihood: -12170.12669852409\n",
      "    val_log_marginal: -12178.4485122292\n",
      "Train Epoch: 931 [256/118836 (0%)] Loss: 12389.325195\n",
      "Train Epoch: 931 [33024/118836 (28%)] Loss: 12273.095703\n",
      "Train Epoch: 931 [65792/118836 (55%)] Loss: 12348.974609\n",
      "Train Epoch: 931 [98560/118836 (83%)] Loss: 12193.908203\n",
      "    epoch          : 931\n",
      "    loss           : 12272.32672550274\n",
      "    val_loss       : 12269.83304767345\n",
      "    val_log_likelihood: -12173.703023547352\n",
      "    val_log_marginal: -12181.862602221836\n",
      "Train Epoch: 932 [256/118836 (0%)] Loss: 12313.230469\n",
      "Train Epoch: 932 [33024/118836 (28%)] Loss: 12230.000000\n",
      "Train Epoch: 932 [65792/118836 (55%)] Loss: 12344.112305\n",
      "Train Epoch: 932 [98560/118836 (83%)] Loss: 12289.486328\n",
      "    epoch          : 932\n",
      "    loss           : 12272.651469286342\n",
      "    val_loss       : 12272.828762761046\n",
      "    val_log_likelihood: -12172.744868079248\n",
      "    val_log_marginal: -12180.922584498236\n",
      "Train Epoch: 933 [256/118836 (0%)] Loss: 12192.397461\n",
      "Train Epoch: 933 [33024/118836 (28%)] Loss: 12353.493164\n",
      "Train Epoch: 933 [65792/118836 (55%)] Loss: 12274.173828\n",
      "Train Epoch: 933 [98560/118836 (83%)] Loss: 12320.722656\n",
      "    epoch          : 933\n",
      "    loss           : 12268.173062222137\n",
      "    val_loss       : 12273.991769977505\n",
      "    val_log_likelihood: -12171.506185865126\n",
      "    val_log_marginal: -12179.699015517732\n",
      "Train Epoch: 934 [256/118836 (0%)] Loss: 12331.070312\n",
      "Train Epoch: 934 [33024/118836 (28%)] Loss: 12255.984375\n",
      "Train Epoch: 934 [65792/118836 (55%)] Loss: 12260.999023\n",
      "Train Epoch: 934 [98560/118836 (83%)] Loss: 12230.900391\n",
      "    epoch          : 934\n",
      "    loss           : 12273.807413959108\n",
      "    val_loss       : 12268.047055916124\n",
      "    val_log_likelihood: -12175.600478830645\n",
      "    val_log_marginal: -12183.750257769621\n",
      "Train Epoch: 935 [256/118836 (0%)] Loss: 12188.009766\n",
      "Train Epoch: 935 [33024/118836 (28%)] Loss: 12241.206055\n",
      "Train Epoch: 935 [65792/118836 (55%)] Loss: 12223.938477\n",
      "Train Epoch: 935 [98560/118836 (83%)] Loss: 12244.960938\n",
      "    epoch          : 935\n",
      "    loss           : 12275.870323646868\n",
      "    val_loss       : 12269.619997801612\n",
      "    val_log_likelihood: -12174.536341856649\n",
      "    val_log_marginal: -12182.61176366541\n",
      "Train Epoch: 936 [256/118836 (0%)] Loss: 12257.996094\n",
      "Train Epoch: 936 [33024/118836 (28%)] Loss: 12306.429688\n",
      "Train Epoch: 936 [65792/118836 (55%)] Loss: 12237.269531\n",
      "Train Epoch: 936 [98560/118836 (83%)] Loss: 12332.429688\n",
      "    epoch          : 936\n",
      "    loss           : 12271.884146893093\n",
      "    val_loss       : 12270.481737462593\n",
      "    val_log_likelihood: -12169.804794929953\n",
      "    val_log_marginal: -12177.82859127996\n",
      "Train Epoch: 937 [256/118836 (0%)] Loss: 12232.614258\n",
      "Train Epoch: 937 [33024/118836 (28%)] Loss: 12264.791016\n",
      "Train Epoch: 937 [65792/118836 (55%)] Loss: 12251.980469\n",
      "Train Epoch: 937 [98560/118836 (83%)] Loss: 12309.376953\n",
      "    epoch          : 937\n",
      "    loss           : 12267.979235971103\n",
      "    val_loss       : 12270.764917540542\n",
      "    val_log_likelihood: -12170.041959393093\n",
      "    val_log_marginal: -12178.039446002696\n",
      "Train Epoch: 938 [256/118836 (0%)] Loss: 12342.075195\n",
      "Train Epoch: 938 [33024/118836 (28%)] Loss: 12431.437500\n",
      "Train Epoch: 938 [65792/118836 (55%)] Loss: 12341.578125\n",
      "Train Epoch: 938 [98560/118836 (83%)] Loss: 12295.661133\n",
      "    epoch          : 938\n",
      "    loss           : 12272.581671803919\n",
      "    val_loss       : 12269.850704774573\n",
      "    val_log_likelihood: -12170.310698246227\n",
      "    val_log_marginal: -12178.310103310252\n",
      "Train Epoch: 939 [256/118836 (0%)] Loss: 12304.262695\n",
      "Train Epoch: 939 [33024/118836 (28%)] Loss: 12178.191406\n",
      "Train Epoch: 939 [65792/118836 (55%)] Loss: 12399.348633\n",
      "Train Epoch: 939 [98560/118836 (83%)] Loss: 12258.386719\n",
      "    epoch          : 939\n",
      "    loss           : 12269.860598570616\n",
      "    val_loss       : 12274.891878108527\n",
      "    val_log_likelihood: -12170.822891465054\n",
      "    val_log_marginal: -12179.215732010363\n",
      "Train Epoch: 940 [256/118836 (0%)] Loss: 12272.224609\n",
      "Train Epoch: 940 [33024/118836 (28%)] Loss: 12331.327148\n",
      "Train Epoch: 940 [65792/118836 (55%)] Loss: 12328.528320\n",
      "Train Epoch: 940 [98560/118836 (83%)] Loss: 12237.907227\n",
      "    epoch          : 940\n",
      "    loss           : 12267.856054041304\n",
      "    val_loss       : 12269.380351032669\n",
      "    val_log_likelihood: -12170.92923758659\n",
      "    val_log_marginal: -12179.143654499765\n",
      "Train Epoch: 941 [256/118836 (0%)] Loss: 12390.034180\n",
      "Train Epoch: 941 [33024/118836 (28%)] Loss: 12327.472656\n",
      "Train Epoch: 941 [65792/118836 (55%)] Loss: 12215.798828\n",
      "Train Epoch: 941 [98560/118836 (83%)] Loss: 12323.386719\n",
      "    epoch          : 941\n",
      "    loss           : 12270.253541472808\n",
      "    val_loss       : 12268.113778389004\n",
      "    val_log_likelihood: -12170.591056335297\n",
      "    val_log_marginal: -12178.760262830245\n",
      "Train Epoch: 942 [256/118836 (0%)] Loss: 12282.181641\n",
      "Train Epoch: 942 [33024/118836 (28%)] Loss: 12282.318359\n",
      "Train Epoch: 942 [65792/118836 (55%)] Loss: 12235.474609\n",
      "Train Epoch: 942 [98560/118836 (83%)] Loss: 12274.950195\n",
      "    epoch          : 942\n",
      "    loss           : 12272.86344796836\n",
      "    val_loss       : 12268.860499031343\n",
      "    val_log_likelihood: -12170.333369843362\n",
      "    val_log_marginal: -12178.385756721069\n",
      "Train Epoch: 943 [256/118836 (0%)] Loss: 12329.817383\n",
      "Train Epoch: 943 [33024/118836 (28%)] Loss: 12305.164062\n",
      "Train Epoch: 943 [65792/118836 (55%)] Loss: 12240.706055\n",
      "Train Epoch: 943 [98560/118836 (83%)] Loss: 12381.835938\n",
      "    epoch          : 943\n",
      "    loss           : 12273.14168766801\n",
      "    val_loss       : 12267.943608850856\n",
      "    val_log_likelihood: -12171.375270917339\n",
      "    val_log_marginal: -12179.390136130194\n",
      "Train Epoch: 944 [256/118836 (0%)] Loss: 12216.724609\n",
      "Train Epoch: 944 [33024/118836 (28%)] Loss: 12172.289062\n",
      "Train Epoch: 944 [65792/118836 (55%)] Loss: 12397.193359\n",
      "Train Epoch: 944 [98560/118836 (83%)] Loss: 12236.662109\n",
      "    epoch          : 944\n",
      "    loss           : 12270.225936498397\n",
      "    val_loss       : 12266.695869978843\n",
      "    val_log_likelihood: -12173.375791266026\n",
      "    val_log_marginal: -12181.376946458575\n",
      "Train Epoch: 945 [256/118836 (0%)] Loss: 12298.254883\n",
      "Train Epoch: 945 [33024/118836 (28%)] Loss: 12275.734375\n",
      "Train Epoch: 945 [65792/118836 (55%)] Loss: 12279.657227\n",
      "Train Epoch: 945 [98560/118836 (83%)] Loss: 12268.856445\n",
      "    epoch          : 945\n",
      "    loss           : 12267.973816816584\n",
      "    val_loss       : 12268.779061622687\n",
      "    val_log_likelihood: -12170.176015818857\n",
      "    val_log_marginal: -12178.241676750768\n",
      "Train Epoch: 946 [256/118836 (0%)] Loss: 12236.234375\n",
      "Train Epoch: 946 [33024/118836 (28%)] Loss: 12383.067383\n",
      "Train Epoch: 946 [65792/118836 (55%)] Loss: 12366.143555\n",
      "Train Epoch: 946 [98560/118836 (83%)] Loss: 12336.267578\n",
      "    epoch          : 946\n",
      "    loss           : 12271.206898295337\n",
      "    val_loss       : 12275.18739215544\n",
      "    val_log_likelihood: -12177.48633087133\n",
      "    val_log_marginal: -12185.86781133332\n",
      "Train Epoch: 947 [256/118836 (0%)] Loss: 12355.019531\n",
      "Train Epoch: 947 [33024/118836 (28%)] Loss: 12286.474609\n",
      "Train Epoch: 947 [65792/118836 (55%)] Loss: 12330.708984\n",
      "Train Epoch: 947 [98560/118836 (83%)] Loss: 12290.502930\n",
      "    epoch          : 947\n",
      "    loss           : 12269.408262898056\n",
      "    val_loss       : 12276.816803187403\n",
      "    val_log_likelihood: -12169.915274277553\n",
      "    val_log_marginal: -12178.041507491687\n",
      "Train Epoch: 948 [256/118836 (0%)] Loss: 12336.020508\n",
      "Train Epoch: 948 [33024/118836 (28%)] Loss: 12256.860352\n",
      "Train Epoch: 948 [65792/118836 (55%)] Loss: 12284.490234\n",
      "Train Epoch: 948 [98560/118836 (83%)] Loss: 12245.421875\n",
      "    epoch          : 948\n",
      "    loss           : 12274.098709548181\n",
      "    val_loss       : 12272.87345340834\n",
      "    val_log_likelihood: -12171.75999195487\n",
      "    val_log_marginal: -12179.87997874761\n",
      "Train Epoch: 949 [256/118836 (0%)] Loss: 12260.996094\n",
      "Train Epoch: 949 [33024/118836 (28%)] Loss: 12243.990234\n",
      "Train Epoch: 949 [65792/118836 (55%)] Loss: 12240.732422\n",
      "Train Epoch: 949 [98560/118836 (83%)] Loss: 12258.136719\n",
      "    epoch          : 949\n",
      "    loss           : 12271.789265566842\n",
      "    val_loss       : 12273.46074471403\n",
      "    val_log_likelihood: -12171.071195364195\n",
      "    val_log_marginal: -12179.655391035958\n",
      "Train Epoch: 950 [256/118836 (0%)] Loss: 12267.230469\n",
      "Train Epoch: 950 [33024/118836 (28%)] Loss: 12219.039062\n",
      "Train Epoch: 950 [65792/118836 (55%)] Loss: 12411.727539\n",
      "Train Epoch: 950 [98560/118836 (83%)] Loss: 12285.599609\n",
      "    epoch          : 950\n",
      "    loss           : 12271.62034077104\n",
      "    val_loss       : 12272.91067022786\n",
      "    val_log_likelihood: -12172.751813708386\n",
      "    val_log_marginal: -12181.027555531566\n",
      "Train Epoch: 951 [256/118836 (0%)] Loss: 12245.790039\n",
      "Train Epoch: 951 [33024/118836 (28%)] Loss: 12282.682617\n",
      "Train Epoch: 951 [65792/118836 (55%)] Loss: 12222.297852\n",
      "Train Epoch: 951 [98560/118836 (83%)] Loss: 12269.488281\n",
      "    epoch          : 951\n",
      "    loss           : 12269.170162582714\n",
      "    val_loss       : 12269.125261039579\n",
      "    val_log_likelihood: -12170.574627630014\n",
      "    val_log_marginal: -12178.805504243799\n",
      "Train Epoch: 952 [256/118836 (0%)] Loss: 12462.375977\n",
      "Train Epoch: 952 [33024/118836 (28%)] Loss: 12281.550781\n",
      "Train Epoch: 952 [65792/118836 (55%)] Loss: 12320.983398\n",
      "Train Epoch: 952 [98560/118836 (83%)] Loss: 12316.365234\n",
      "    epoch          : 952\n",
      "    loss           : 12270.10391174266\n",
      "    val_loss       : 12266.698220915812\n",
      "    val_log_likelihood: -12174.767307530758\n",
      "    val_log_marginal: -12183.052726697142\n",
      "Train Epoch: 953 [256/118836 (0%)] Loss: 12273.252930\n",
      "Train Epoch: 953 [33024/118836 (28%)] Loss: 12222.411133\n",
      "Train Epoch: 953 [65792/118836 (55%)] Loss: 12311.230469\n",
      "Train Epoch: 953 [98560/118836 (83%)] Loss: 12300.838867\n",
      "    epoch          : 953\n",
      "    loss           : 12273.142120618795\n",
      "    val_loss       : 12271.065890622534\n",
      "    val_log_likelihood: -12171.344930921734\n",
      "    val_log_marginal: -12179.402624038925\n",
      "Train Epoch: 954 [256/118836 (0%)] Loss: 12317.886719\n",
      "Train Epoch: 954 [33024/118836 (28%)] Loss: 12269.072266\n",
      "Train Epoch: 954 [65792/118836 (55%)] Loss: 12271.902344\n",
      "Train Epoch: 954 [98560/118836 (83%)] Loss: 12319.615234\n",
      "    epoch          : 954\n",
      "    loss           : 12266.405780377636\n",
      "    val_loss       : 12270.145792570456\n",
      "    val_log_likelihood: -12174.245800538925\n",
      "    val_log_marginal: -12182.457018111187\n",
      "Train Epoch: 955 [256/118836 (0%)] Loss: 12339.361328\n",
      "Train Epoch: 955 [33024/118836 (28%)] Loss: 12204.184570\n",
      "Train Epoch: 955 [65792/118836 (55%)] Loss: 12206.128906\n",
      "Train Epoch: 955 [98560/118836 (83%)] Loss: 12312.639648\n",
      "    epoch          : 955\n",
      "    loss           : 12268.975782380841\n",
      "    val_loss       : 12267.954670791765\n",
      "    val_log_likelihood: -12171.07176724695\n",
      "    val_log_marginal: -12179.19012369837\n",
      "Train Epoch: 956 [256/118836 (0%)] Loss: 12373.149414\n",
      "Train Epoch: 956 [33024/118836 (28%)] Loss: 12229.841797\n",
      "Train Epoch: 956 [65792/118836 (55%)] Loss: 12355.629883\n",
      "Train Epoch: 956 [98560/118836 (83%)] Loss: 12214.632812\n",
      "    epoch          : 956\n",
      "    loss           : 12271.737245722186\n",
      "    val_loss       : 12266.173620153706\n",
      "    val_log_likelihood: -12172.210753819014\n",
      "    val_log_marginal: -12180.367193137672\n",
      "Train Epoch: 957 [256/118836 (0%)] Loss: 12343.702148\n",
      "Train Epoch: 957 [33024/118836 (28%)] Loss: 12312.164062\n",
      "Train Epoch: 957 [65792/118836 (55%)] Loss: 12253.261719\n",
      "Train Epoch: 957 [98560/118836 (83%)] Loss: 12280.328125\n",
      "    epoch          : 957\n",
      "    loss           : 12272.348367387822\n",
      "    val_loss       : 12268.248297458815\n",
      "    val_log_likelihood: -12169.568001059759\n",
      "    val_log_marginal: -12177.648955943854\n",
      "Train Epoch: 958 [256/118836 (0%)] Loss: 12239.634766\n",
      "Train Epoch: 958 [33024/118836 (28%)] Loss: 12348.660156\n",
      "Train Epoch: 958 [65792/118836 (55%)] Loss: 12177.826172\n",
      "Train Epoch: 958 [98560/118836 (83%)] Loss: 12384.007812\n",
      "    epoch          : 958\n",
      "    loss           : 12269.22258484543\n",
      "    val_loss       : 12268.51437387323\n",
      "    val_log_likelihood: -12170.7482352409\n",
      "    val_log_marginal: -12178.895944574411\n",
      "Train Epoch: 959 [256/118836 (0%)] Loss: 12328.316406\n",
      "Train Epoch: 959 [33024/118836 (28%)] Loss: 12223.107422\n",
      "Train Epoch: 959 [65792/118836 (55%)] Loss: 12305.535156\n",
      "Train Epoch: 959 [98560/118836 (83%)] Loss: 12344.586914\n",
      "    epoch          : 959\n",
      "    loss           : 12273.877971528638\n",
      "    val_loss       : 12271.754318566282\n",
      "    val_log_likelihood: -12171.857007663875\n",
      "    val_log_marginal: -12180.091099560601\n",
      "Train Epoch: 960 [256/118836 (0%)] Loss: 12301.680664\n",
      "Train Epoch: 960 [33024/118836 (28%)] Loss: 12209.412109\n",
      "Train Epoch: 960 [65792/118836 (55%)] Loss: 12277.827148\n",
      "Train Epoch: 960 [98560/118836 (83%)] Loss: 12232.622070\n",
      "    epoch          : 960\n",
      "    loss           : 12267.883750129238\n",
      "    val_loss       : 12267.903095534813\n",
      "    val_log_likelihood: -12171.742313669614\n",
      "    val_log_marginal: -12180.277594660232\n",
      "Train Epoch: 961 [256/118836 (0%)] Loss: 12268.320312\n",
      "Train Epoch: 961 [33024/118836 (28%)] Loss: 12216.519531\n",
      "Train Epoch: 961 [65792/118836 (55%)] Loss: 12249.098633\n",
      "Train Epoch: 961 [98560/118836 (83%)] Loss: 12319.331055\n",
      "    epoch          : 961\n",
      "    loss           : 12272.004455031276\n",
      "    val_loss       : 12268.423744066584\n",
      "    val_log_likelihood: -12170.597626525021\n",
      "    val_log_marginal: -12178.691569685121\n",
      "Train Epoch: 962 [256/118836 (0%)] Loss: 12238.358398\n",
      "Train Epoch: 962 [33024/118836 (28%)] Loss: 12349.193359\n",
      "Train Epoch: 962 [65792/118836 (55%)] Loss: 12433.163086\n",
      "Train Epoch: 962 [98560/118836 (83%)] Loss: 12321.273438\n",
      "    epoch          : 962\n",
      "    loss           : 12272.582773243643\n",
      "    val_loss       : 12271.301935547515\n",
      "    val_log_likelihood: -12170.091689929695\n",
      "    val_log_marginal: -12178.230256082172\n",
      "Train Epoch: 963 [256/118836 (0%)] Loss: 12310.089844\n",
      "Train Epoch: 963 [33024/118836 (28%)] Loss: 12296.322266\n",
      "Train Epoch: 963 [65792/118836 (55%)] Loss: 12286.388672\n",
      "Train Epoch: 963 [98560/118836 (83%)] Loss: 12277.811523\n",
      "    epoch          : 963\n",
      "    loss           : 12272.228407064207\n",
      "    val_loss       : 12269.504471677214\n",
      "    val_log_likelihood: -12171.183982113316\n",
      "    val_log_marginal: -12179.415602861014\n",
      "Train Epoch: 964 [256/118836 (0%)] Loss: 12293.833984\n",
      "Train Epoch: 964 [33024/118836 (28%)] Loss: 12368.207031\n",
      "Train Epoch: 964 [65792/118836 (55%)] Loss: 12291.265625\n",
      "Train Epoch: 964 [98560/118836 (83%)] Loss: 12247.236328\n",
      "    epoch          : 964\n",
      "    loss           : 12273.406485053505\n",
      "    val_loss       : 12277.377576718904\n",
      "    val_log_likelihood: -12174.104161174006\n",
      "    val_log_marginal: -12182.406727960428\n",
      "Train Epoch: 965 [256/118836 (0%)] Loss: 12329.665039\n",
      "Train Epoch: 965 [33024/118836 (28%)] Loss: 12188.458984\n",
      "Train Epoch: 965 [65792/118836 (55%)] Loss: 12269.877930\n",
      "Train Epoch: 965 [98560/118836 (83%)] Loss: 12356.744141\n",
      "    epoch          : 965\n",
      "    loss           : 12272.001103378308\n",
      "    val_loss       : 12265.221452457745\n",
      "    val_log_likelihood: -12172.830419316067\n",
      "    val_log_marginal: -12180.996145951596\n",
      "Train Epoch: 966 [256/118836 (0%)] Loss: 12334.552734\n",
      "Train Epoch: 966 [33024/118836 (28%)] Loss: 12292.679688\n",
      "Train Epoch: 966 [65792/118836 (55%)] Loss: 12222.873047\n",
      "Train Epoch: 966 [98560/118836 (83%)] Loss: 12251.113281\n",
      "    epoch          : 966\n",
      "    loss           : 12278.886062700321\n",
      "    val_loss       : 12269.911626917356\n",
      "    val_log_likelihood: -12172.649364143921\n",
      "    val_log_marginal: -12180.891724116618\n",
      "Train Epoch: 967 [256/118836 (0%)] Loss: 12230.051758\n",
      "Train Epoch: 967 [33024/118836 (28%)] Loss: 12369.910156\n",
      "Train Epoch: 967 [65792/118836 (55%)] Loss: 12204.647461\n",
      "Train Epoch: 967 [98560/118836 (83%)] Loss: 12312.499023\n",
      "    epoch          : 967\n",
      "    loss           : 12268.57774358328\n",
      "    val_loss       : 12268.883171943724\n",
      "    val_log_likelihood: -12171.034637678351\n",
      "    val_log_marginal: -12179.103287956139\n",
      "Train Epoch: 968 [256/118836 (0%)] Loss: 12299.705078\n",
      "Train Epoch: 968 [33024/118836 (28%)] Loss: 12328.822266\n",
      "Train Epoch: 968 [65792/118836 (55%)] Loss: 12276.450195\n",
      "Train Epoch: 968 [98560/118836 (83%)] Loss: 12226.094727\n",
      "    epoch          : 968\n",
      "    loss           : 12266.529900259771\n",
      "    val_loss       : 12267.671470528963\n",
      "    val_log_likelihood: -12170.02355478443\n",
      "    val_log_marginal: -12178.085503617465\n",
      "Train Epoch: 969 [256/118836 (0%)] Loss: 12323.368164\n",
      "Train Epoch: 969 [33024/118836 (28%)] Loss: 12213.201172\n",
      "Train Epoch: 969 [65792/118836 (55%)] Loss: 12203.597656\n",
      "Train Epoch: 969 [98560/118836 (83%)] Loss: 12321.491211\n",
      "    epoch          : 969\n",
      "    loss           : 12276.314238426645\n",
      "    val_loss       : 12272.13721973599\n",
      "    val_log_likelihood: -12170.661632967585\n",
      "    val_log_marginal: -12178.802183882623\n",
      "Train Epoch: 970 [256/118836 (0%)] Loss: 12257.757812\n",
      "Train Epoch: 970 [33024/118836 (28%)] Loss: 12375.547852\n",
      "Train Epoch: 970 [65792/118836 (55%)] Loss: 12341.505859\n",
      "Train Epoch: 970 [98560/118836 (83%)] Loss: 12325.997070\n",
      "    epoch          : 970\n",
      "    loss           : 12273.634663849256\n",
      "    val_loss       : 12266.486666788975\n",
      "    val_log_likelihood: -12169.241381533033\n",
      "    val_log_marginal: -12177.326807330088\n",
      "Train Epoch: 971 [256/118836 (0%)] Loss: 12168.174805\n",
      "Train Epoch: 971 [33024/118836 (28%)] Loss: 12213.988281\n",
      "Train Epoch: 971 [65792/118836 (55%)] Loss: 12197.727539\n",
      "Train Epoch: 971 [98560/118836 (83%)] Loss: 12250.224609\n",
      "    epoch          : 971\n",
      "    loss           : 12271.742295414599\n",
      "    val_loss       : 12274.836951562067\n",
      "    val_log_likelihood: -12173.299672379033\n",
      "    val_log_marginal: -12181.926797777289\n",
      "Train Epoch: 972 [256/118836 (0%)] Loss: 12257.040039\n",
      "Train Epoch: 972 [33024/118836 (28%)] Loss: 12366.161133\n",
      "Train Epoch: 972 [65792/118836 (55%)] Loss: 12259.136719\n",
      "Train Epoch: 972 [98560/118836 (83%)] Loss: 12206.589844\n",
      "    epoch          : 972\n",
      "    loss           : 12268.708920886322\n",
      "    val_loss       : 12272.464961450234\n",
      "    val_log_likelihood: -12172.222212475444\n",
      "    val_log_marginal: -12180.23579755864\n",
      "Train Epoch: 973 [256/118836 (0%)] Loss: 12164.824219\n",
      "Train Epoch: 973 [33024/118836 (28%)] Loss: 12312.113281\n",
      "Train Epoch: 973 [65792/118836 (55%)] Loss: 12267.632812\n",
      "Train Epoch: 973 [98560/118836 (83%)] Loss: 12224.544922\n",
      "    epoch          : 973\n",
      "    loss           : 12270.436235719086\n",
      "    val_loss       : 12272.90809559673\n",
      "    val_log_likelihood: -12172.079146149968\n",
      "    val_log_marginal: -12180.279492360938\n",
      "Train Epoch: 974 [256/118836 (0%)] Loss: 12342.256836\n",
      "Train Epoch: 974 [33024/118836 (28%)] Loss: 12208.394531\n",
      "Train Epoch: 974 [65792/118836 (55%)] Loss: 12265.948242\n",
      "Train Epoch: 974 [98560/118836 (83%)] Loss: 12220.893555\n",
      "    epoch          : 974\n",
      "    loss           : 12271.720323969965\n",
      "    val_loss       : 12267.511502324402\n",
      "    val_log_likelihood: -12171.457950301128\n",
      "    val_log_marginal: -12179.473773732992\n",
      "Train Epoch: 975 [256/118836 (0%)] Loss: 12210.845703\n",
      "Train Epoch: 975 [33024/118836 (28%)] Loss: 12335.946289\n",
      "Train Epoch: 975 [65792/118836 (55%)] Loss: 12243.001953\n",
      "Train Epoch: 975 [98560/118836 (83%)] Loss: 12250.181641\n",
      "    epoch          : 975\n",
      "    loss           : 12272.04122046888\n",
      "    val_loss       : 12270.95461570271\n",
      "    val_log_likelihood: -12170.941720462419\n",
      "    val_log_marginal: -12178.980345248643\n",
      "Train Epoch: 976 [256/118836 (0%)] Loss: 12458.415039\n",
      "Train Epoch: 976 [33024/118836 (28%)] Loss: 12227.269531\n",
      "Train Epoch: 976 [65792/118836 (55%)] Loss: 12258.037109\n",
      "Train Epoch: 976 [98560/118836 (83%)] Loss: 12347.495117\n",
      "    epoch          : 976\n",
      "    loss           : 12271.682429629342\n",
      "    val_loss       : 12272.147322807268\n",
      "    val_log_likelihood: -12170.848462701613\n",
      "    val_log_marginal: -12178.90742488374\n",
      "Train Epoch: 977 [256/118836 (0%)] Loss: 12195.510742\n",
      "Train Epoch: 977 [33024/118836 (28%)] Loss: 12215.876953\n",
      "Train Epoch: 977 [65792/118836 (55%)] Loss: 12269.550781\n",
      "Train Epoch: 977 [98560/118836 (83%)] Loss: 12275.722656\n",
      "    epoch          : 977\n",
      "    loss           : 12270.493551456524\n",
      "    val_loss       : 12270.010942795921\n",
      "    val_log_likelihood: -12169.567951464278\n",
      "    val_log_marginal: -12177.939206290115\n",
      "Train Epoch: 978 [256/118836 (0%)] Loss: 12339.360352\n",
      "Train Epoch: 978 [33024/118836 (28%)] Loss: 12352.294922\n",
      "Train Epoch: 978 [65792/118836 (55%)] Loss: 12440.529297\n",
      "Train Epoch: 978 [98560/118836 (83%)] Loss: 12304.306641\n",
      "    epoch          : 978\n",
      "    loss           : 12273.962192572633\n",
      "    val_loss       : 12268.318149211063\n",
      "    val_log_likelihood: -12169.732789075424\n",
      "    val_log_marginal: -12177.808992793185\n",
      "Train Epoch: 979 [256/118836 (0%)] Loss: 12263.121094\n",
      "Train Epoch: 979 [33024/118836 (28%)] Loss: 12301.072266\n",
      "Train Epoch: 979 [65792/118836 (55%)] Loss: 12190.586914\n",
      "Train Epoch: 979 [98560/118836 (83%)] Loss: 12237.260742\n",
      "    epoch          : 979\n",
      "    loss           : 12266.273729580233\n",
      "    val_loss       : 12273.087231990037\n",
      "    val_log_likelihood: -12171.235606486507\n",
      "    val_log_marginal: -12179.408664463184\n",
      "Train Epoch: 980 [256/118836 (0%)] Loss: 12250.248047\n",
      "Train Epoch: 980 [33024/118836 (28%)] Loss: 12257.381836\n",
      "Train Epoch: 980 [65792/118836 (55%)] Loss: 12270.344727\n",
      "Train Epoch: 980 [98560/118836 (83%)] Loss: 12341.976562\n",
      "    epoch          : 980\n",
      "    loss           : 12270.94938320668\n",
      "    val_loss       : 12264.775677088086\n",
      "    val_log_likelihood: -12170.961419400071\n",
      "    val_log_marginal: -12179.174278758499\n",
      "Train Epoch: 981 [256/118836 (0%)] Loss: 12284.336914\n",
      "Train Epoch: 981 [33024/118836 (28%)] Loss: 12376.527344\n",
      "Train Epoch: 981 [65792/118836 (55%)] Loss: 12281.539062\n",
      "Train Epoch: 981 [98560/118836 (83%)] Loss: 12226.204102\n",
      "    epoch          : 981\n",
      "    loss           : 12267.42307126887\n",
      "    val_loss       : 12273.592093037478\n",
      "    val_log_likelihood: -12174.523686931348\n",
      "    val_log_marginal: -12182.857663637034\n",
      "Train Epoch: 982 [256/118836 (0%)] Loss: 12398.279297\n",
      "Train Epoch: 982 [33024/118836 (28%)] Loss: 12388.105469\n",
      "Train Epoch: 982 [65792/118836 (55%)] Loss: 12355.437500\n",
      "Train Epoch: 982 [98560/118836 (83%)] Loss: 12397.188477\n",
      "    epoch          : 982\n",
      "    loss           : 12271.639108541407\n",
      "    val_loss       : 12270.355555332699\n",
      "    val_log_likelihood: -12171.563438113884\n",
      "    val_log_marginal: -12179.8620052774\n",
      "Train Epoch: 983 [256/118836 (0%)] Loss: 12251.048828\n",
      "Train Epoch: 983 [33024/118836 (28%)] Loss: 12281.470703\n",
      "Train Epoch: 983 [65792/118836 (55%)] Loss: 12404.094727\n",
      "Train Epoch: 983 [98560/118836 (83%)] Loss: 12275.883789\n",
      "    epoch          : 983\n",
      "    loss           : 12266.852714019851\n",
      "    val_loss       : 12264.954534311744\n",
      "    val_log_likelihood: -12170.120371142213\n",
      "    val_log_marginal: -12178.143899062074\n",
      "Train Epoch: 984 [256/118836 (0%)] Loss: 12222.499023\n",
      "Train Epoch: 984 [33024/118836 (28%)] Loss: 12373.108398\n",
      "Train Epoch: 984 [65792/118836 (55%)] Loss: 12320.023438\n",
      "Train Epoch: 984 [98560/118836 (83%)] Loss: 12289.746094\n",
      "    epoch          : 984\n",
      "    loss           : 12273.293420278898\n",
      "    val_loss       : 12269.160776142931\n",
      "    val_log_likelihood: -12172.873116179435\n",
      "    val_log_marginal: -12181.13298320412\n",
      "Train Epoch: 985 [256/118836 (0%)] Loss: 12276.285156\n",
      "Train Epoch: 985 [33024/118836 (28%)] Loss: 12289.236328\n",
      "Train Epoch: 985 [65792/118836 (55%)] Loss: 12408.167969\n",
      "Train Epoch: 985 [98560/118836 (83%)] Loss: 12292.094727\n",
      "    epoch          : 985\n",
      "    loss           : 12278.154767143558\n",
      "    val_loss       : 12266.581542446638\n",
      "    val_log_likelihood: -12172.045376634875\n",
      "    val_log_marginal: -12180.147278103646\n",
      "Train Epoch: 986 [256/118836 (0%)] Loss: 12250.542969\n",
      "Train Epoch: 986 [33024/118836 (28%)] Loss: 12250.905273\n",
      "Train Epoch: 986 [65792/118836 (55%)] Loss: 12286.785156\n",
      "Train Epoch: 986 [98560/118836 (83%)] Loss: 12248.957031\n",
      "    epoch          : 986\n",
      "    loss           : 12269.934784041563\n",
      "    val_loss       : 12264.777808458766\n",
      "    val_log_likelihood: -12170.416905597343\n",
      "    val_log_marginal: -12178.363427580827\n",
      "Train Epoch: 987 [256/118836 (0%)] Loss: 12435.041016\n",
      "Train Epoch: 987 [33024/118836 (28%)] Loss: 12297.541016\n",
      "Train Epoch: 987 [65792/118836 (55%)] Loss: 12351.428711\n",
      "Train Epoch: 987 [98560/118836 (83%)] Loss: 12267.406250\n",
      "    epoch          : 987\n",
      "    loss           : 12272.812270762253\n",
      "    val_loss       : 12272.367900260964\n",
      "    val_log_likelihood: -12167.645692301232\n",
      "    val_log_marginal: -12175.650455630372\n",
      "Train Epoch: 988 [256/118836 (0%)] Loss: 12339.186523\n",
      "Train Epoch: 988 [33024/118836 (28%)] Loss: 12425.730469\n",
      "Train Epoch: 988 [65792/118836 (55%)] Loss: 12430.642578\n",
      "Train Epoch: 988 [98560/118836 (83%)] Loss: 12226.544922\n",
      "    epoch          : 988\n",
      "    loss           : 12277.702928556659\n",
      "    val_loss       : 12272.13740349398\n",
      "    val_log_likelihood: -12169.016119500879\n",
      "    val_log_marginal: -12177.043600983305\n",
      "Train Epoch: 989 [256/118836 (0%)] Loss: 12252.134766\n",
      "Train Epoch: 989 [33024/118836 (28%)] Loss: 12339.518555\n",
      "Train Epoch: 989 [65792/118836 (55%)] Loss: 12418.144531\n",
      "Train Epoch: 989 [98560/118836 (83%)] Loss: 12345.951172\n",
      "    epoch          : 989\n",
      "    loss           : 12275.431681496846\n",
      "    val_loss       : 12269.188804580224\n",
      "    val_log_likelihood: -12169.972347691792\n",
      "    val_log_marginal: -12177.993754998024\n",
      "Train Epoch: 990 [256/118836 (0%)] Loss: 12339.205078\n",
      "Train Epoch: 990 [33024/118836 (28%)] Loss: 12272.856445\n",
      "Train Epoch: 990 [65792/118836 (55%)] Loss: 12235.300781\n",
      "Train Epoch: 990 [98560/118836 (83%)] Loss: 12255.943359\n",
      "    epoch          : 990\n",
      "    loss           : 12267.001331162119\n",
      "    val_loss       : 12272.653990825744\n",
      "    val_log_likelihood: -12171.405339995605\n",
      "    val_log_marginal: -12179.644375262373\n",
      "Train Epoch: 991 [256/118836 (0%)] Loss: 12258.896484\n",
      "Train Epoch: 991 [33024/118836 (28%)] Loss: 12261.731445\n",
      "Train Epoch: 991 [65792/118836 (55%)] Loss: 12170.648438\n",
      "Train Epoch: 991 [98560/118836 (83%)] Loss: 12321.449219\n",
      "    epoch          : 991\n",
      "    loss           : 12266.395877920802\n",
      "    val_loss       : 12273.743981856005\n",
      "    val_log_likelihood: -12171.764658615075\n",
      "    val_log_marginal: -12179.890502903585\n",
      "Train Epoch: 992 [256/118836 (0%)] Loss: 12364.970703\n",
      "Train Epoch: 992 [33024/118836 (28%)] Loss: 12319.523438\n",
      "Train Epoch: 992 [65792/118836 (55%)] Loss: 12253.435547\n",
      "Train Epoch: 992 [98560/118836 (83%)] Loss: 12254.687500\n",
      "    epoch          : 992\n",
      "    loss           : 12271.57022897927\n",
      "    val_loss       : 12264.70270852616\n",
      "    val_log_likelihood: -12170.490485906483\n",
      "    val_log_marginal: -12178.72340984155\n",
      "Train Epoch: 993 [256/118836 (0%)] Loss: 12241.817383\n",
      "Train Epoch: 993 [33024/118836 (28%)] Loss: 12387.966797\n",
      "Train Epoch: 993 [65792/118836 (55%)] Loss: 12237.109375\n",
      "Train Epoch: 993 [98560/118836 (83%)] Loss: 12270.675781\n",
      "    epoch          : 993\n",
      "    loss           : 12270.963556852254\n",
      "    val_loss       : 12271.655664600874\n",
      "    val_log_likelihood: -12171.037006306864\n",
      "    val_log_marginal: -12179.138964237389\n",
      "Train Epoch: 994 [256/118836 (0%)] Loss: 12440.721680\n",
      "Train Epoch: 994 [33024/118836 (28%)] Loss: 12330.208008\n",
      "Train Epoch: 994 [65792/118836 (55%)] Loss: 12328.215820\n",
      "Train Epoch: 994 [98560/118836 (83%)] Loss: 12270.180664\n",
      "    epoch          : 994\n",
      "    loss           : 12278.14264258297\n",
      "    val_loss       : 12274.513493117824\n",
      "    val_log_likelihood: -12172.087864777191\n",
      "    val_log_marginal: -12180.28311863355\n",
      "Train Epoch: 995 [256/118836 (0%)] Loss: 12331.111328\n",
      "Train Epoch: 995 [33024/118836 (28%)] Loss: 12232.489258\n",
      "Train Epoch: 995 [65792/118836 (55%)] Loss: 12218.332031\n",
      "Train Epoch: 995 [98560/118836 (83%)] Loss: 12362.021484\n",
      "    epoch          : 995\n",
      "    loss           : 12271.66376201923\n",
      "    val_loss       : 12272.975695074947\n",
      "    val_log_likelihood: -12170.583262736507\n",
      "    val_log_marginal: -12178.689162996838\n",
      "Train Epoch: 996 [256/118836 (0%)] Loss: 12366.376953\n",
      "Train Epoch: 996 [33024/118836 (28%)] Loss: 12314.972656\n",
      "Train Epoch: 996 [65792/118836 (55%)] Loss: 12216.531250\n",
      "Train Epoch: 996 [98560/118836 (83%)] Loss: 12303.154297\n",
      "    epoch          : 996\n",
      "    loss           : 12270.346278400279\n",
      "    val_loss       : 12271.965439211537\n",
      "    val_log_likelihood: -12169.450288849255\n",
      "    val_log_marginal: -12177.483580765731\n",
      "Train Epoch: 997 [256/118836 (0%)] Loss: 12299.294922\n",
      "Train Epoch: 997 [33024/118836 (28%)] Loss: 12201.535156\n",
      "Train Epoch: 997 [65792/118836 (55%)] Loss: 12349.604492\n",
      "Train Epoch: 997 [98560/118836 (83%)] Loss: 12372.304688\n",
      "    epoch          : 997\n",
      "    loss           : 12269.172948007134\n",
      "    val_loss       : 12273.415584233548\n",
      "    val_log_likelihood: -12172.10559928143\n",
      "    val_log_marginal: -12180.134957250784\n",
      "Train Epoch: 998 [256/118836 (0%)] Loss: 12271.790039\n",
      "Train Epoch: 998 [33024/118836 (28%)] Loss: 12406.116211\n",
      "Train Epoch: 998 [65792/118836 (55%)] Loss: 12352.343750\n",
      "Train Epoch: 998 [98560/118836 (83%)] Loss: 12258.654297\n",
      "    epoch          : 998\n",
      "    loss           : 12272.137788041511\n",
      "    val_loss       : 12273.167616716823\n",
      "    val_log_likelihood: -12170.874407600548\n",
      "    val_log_marginal: -12178.92694035782\n",
      "Train Epoch: 999 [256/118836 (0%)] Loss: 12326.976562\n",
      "Train Epoch: 999 [33024/118836 (28%)] Loss: 12327.101562\n",
      "Train Epoch: 999 [65792/118836 (55%)] Loss: 12328.867188\n",
      "Train Epoch: 999 [98560/118836 (83%)] Loss: 12312.388672\n",
      "    epoch          : 999\n",
      "    loss           : 12268.447029440653\n",
      "    val_loss       : 12270.836209065821\n",
      "    val_log_likelihood: -12171.50860311983\n",
      "    val_log_marginal: -12179.55049890074\n",
      "Train Epoch: 1000 [256/118836 (0%)] Loss: 12200.361328\n",
      "Train Epoch: 1000 [33024/118836 (28%)] Loss: 12262.758789\n",
      "Train Epoch: 1000 [65792/118836 (55%)] Loss: 12397.600586\n",
      "Train Epoch: 1000 [98560/118836 (83%)] Loss: 12346.802734\n",
      "    epoch          : 1000\n",
      "    loss           : 12269.199884492607\n",
      "    val_loss       : 12276.107496634639\n",
      "    val_log_likelihood: -12175.624183209264\n",
      "    val_log_marginal: -12183.779399317218\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch1000.pth ...\n",
      "Train Epoch: 1001 [256/118836 (0%)] Loss: 12352.828125\n",
      "Train Epoch: 1001 [33024/118836 (28%)] Loss: 12269.705078\n",
      "Train Epoch: 1001 [65792/118836 (55%)] Loss: 12214.799805\n",
      "Train Epoch: 1001 [98560/118836 (83%)] Loss: 12347.652344\n",
      "    epoch          : 1001\n",
      "    loss           : 12272.14524788048\n",
      "    val_loss       : 12270.379623751052\n",
      "    val_log_likelihood: -12170.79836609543\n",
      "    val_log_marginal: -12179.269672974879\n",
      "Train Epoch: 1002 [256/118836 (0%)] Loss: 12348.264648\n",
      "Train Epoch: 1002 [33024/118836 (28%)] Loss: 12256.333984\n",
      "Train Epoch: 1002 [65792/118836 (55%)] Loss: 12285.113281\n",
      "Train Epoch: 1002 [98560/118836 (83%)] Loss: 12193.526367\n",
      "    epoch          : 1002\n",
      "    loss           : 12274.154866334522\n",
      "    val_loss       : 12266.25409407838\n",
      "    val_log_likelihood: -12172.253095275022\n",
      "    val_log_marginal: -12180.42360978983\n",
      "Train Epoch: 1003 [256/118836 (0%)] Loss: 12289.393555\n",
      "Train Epoch: 1003 [33024/118836 (28%)] Loss: 12270.747070\n",
      "Train Epoch: 1003 [65792/118836 (55%)] Loss: 12282.569336\n",
      "Train Epoch: 1003 [98560/118836 (83%)] Loss: 12242.578125\n",
      "    epoch          : 1003\n",
      "    loss           : 12269.295415245037\n",
      "    val_loss       : 12268.452917921531\n",
      "    val_log_likelihood: -12166.59051999328\n",
      "    val_log_marginal: -12174.644782739875\n",
      "Train Epoch: 1004 [256/118836 (0%)] Loss: 12299.094727\n",
      "Train Epoch: 1004 [33024/118836 (28%)] Loss: 12267.449219\n",
      "Train Epoch: 1004 [65792/118836 (55%)] Loss: 12402.077148\n",
      "Train Epoch: 1004 [98560/118836 (83%)] Loss: 12305.166992\n",
      "    epoch          : 1004\n",
      "    loss           : 12273.184248022642\n",
      "    val_loss       : 12265.654579677257\n",
      "    val_log_likelihood: -12168.68009298749\n",
      "    val_log_marginal: -12176.863856770475\n",
      "Train Epoch: 1005 [256/118836 (0%)] Loss: 12362.970703\n",
      "Train Epoch: 1005 [33024/118836 (28%)] Loss: 12353.102539\n",
      "Train Epoch: 1005 [65792/118836 (55%)] Loss: 12267.459961\n",
      "Train Epoch: 1005 [98560/118836 (83%)] Loss: 12384.341797\n",
      "    epoch          : 1005\n",
      "    loss           : 12268.394827853597\n",
      "    val_loss       : 12268.537661369377\n",
      "    val_log_likelihood: -12170.435771266284\n",
      "    val_log_marginal: -12178.791120663318\n",
      "Train Epoch: 1006 [256/118836 (0%)] Loss: 12201.752930\n",
      "Train Epoch: 1006 [33024/118836 (28%)] Loss: 12306.143555\n",
      "Train Epoch: 1006 [65792/118836 (55%)] Loss: 12383.761719\n",
      "Train Epoch: 1006 [98560/118836 (83%)] Loss: 12195.774414\n",
      "    epoch          : 1006\n",
      "    loss           : 12272.440032923647\n",
      "    val_loss       : 12264.09755402794\n",
      "    val_log_likelihood: -12172.584071611353\n",
      "    val_log_marginal: -12180.758555294744\n",
      "Train Epoch: 1007 [256/118836 (0%)] Loss: 12433.461914\n",
      "Train Epoch: 1007 [33024/118836 (28%)] Loss: 12299.142578\n",
      "Train Epoch: 1007 [65792/118836 (55%)] Loss: 12260.856445\n",
      "Train Epoch: 1007 [98560/118836 (83%)] Loss: 12218.803711\n",
      "    epoch          : 1007\n",
      "    loss           : 12270.411564470896\n",
      "    val_loss       : 12266.286096701555\n",
      "    val_log_likelihood: -12169.10925513079\n",
      "    val_log_marginal: -12177.454812736762\n",
      "Train Epoch: 1008 [256/118836 (0%)] Loss: 12189.573242\n",
      "Train Epoch: 1008 [33024/118836 (28%)] Loss: 12193.412109\n",
      "Train Epoch: 1008 [65792/118836 (55%)] Loss: 12343.355469\n",
      "Train Epoch: 1008 [98560/118836 (83%)] Loss: 12293.641602\n",
      "    epoch          : 1008\n",
      "    loss           : 12273.981844499587\n",
      "    val_loss       : 12267.880819746637\n",
      "    val_log_likelihood: -12170.922946553195\n",
      "    val_log_marginal: -12178.931238159952\n",
      "Train Epoch: 1009 [256/118836 (0%)] Loss: 12185.988281\n",
      "Train Epoch: 1009 [33024/118836 (28%)] Loss: 12239.667969\n",
      "Train Epoch: 1009 [65792/118836 (55%)] Loss: 12299.545898\n",
      "Train Epoch: 1009 [98560/118836 (83%)] Loss: 12233.093750\n",
      "    epoch          : 1009\n",
      "    loss           : 12275.887706782465\n",
      "    val_loss       : 12276.318686420951\n",
      "    val_log_likelihood: -12177.494089737127\n",
      "    val_log_marginal: -12186.221860619109\n",
      "Train Epoch: 1010 [256/118836 (0%)] Loss: 12270.543945\n",
      "Train Epoch: 1010 [33024/118836 (28%)] Loss: 12251.442383\n",
      "Train Epoch: 1010 [65792/118836 (55%)] Loss: 12326.147461\n",
      "Train Epoch: 1010 [98560/118836 (83%)] Loss: 12332.216797\n",
      "    epoch          : 1010\n",
      "    loss           : 12271.592731111716\n",
      "    val_loss       : 12265.451077561545\n",
      "    val_log_likelihood: -12168.636673516336\n",
      "    val_log_marginal: -12176.838426028728\n",
      "Train Epoch: 1011 [256/118836 (0%)] Loss: 12246.883789\n",
      "Train Epoch: 1011 [33024/118836 (28%)] Loss: 12321.365234\n",
      "Train Epoch: 1011 [65792/118836 (55%)] Loss: 12278.950195\n",
      "Train Epoch: 1011 [98560/118836 (83%)] Loss: 12267.859375\n",
      "    epoch          : 1011\n",
      "    loss           : 12270.692639836641\n",
      "    val_loss       : 12264.468358820463\n",
      "    val_log_likelihood: -12169.420427522746\n",
      "    val_log_marginal: -12177.571394183296\n",
      "Train Epoch: 1012 [256/118836 (0%)] Loss: 12310.169922\n",
      "Train Epoch: 1012 [33024/118836 (28%)] Loss: 12357.422852\n",
      "Train Epoch: 1012 [65792/118836 (55%)] Loss: 12170.136719\n",
      "Train Epoch: 1012 [98560/118836 (83%)] Loss: 12372.574219\n",
      "    epoch          : 1012\n",
      "    loss           : 12274.480933041254\n",
      "    val_loss       : 12271.037165602258\n",
      "    val_log_likelihood: -12172.240443742245\n",
      "    val_log_marginal: -12180.413396781883\n",
      "Train Epoch: 1013 [256/118836 (0%)] Loss: 12354.213867\n",
      "Train Epoch: 1013 [33024/118836 (28%)] Loss: 12291.297852\n",
      "Train Epoch: 1013 [65792/118836 (55%)] Loss: 12222.111328\n",
      "Train Epoch: 1013 [98560/118836 (83%)] Loss: 12351.011719\n",
      "    epoch          : 1013\n",
      "    loss           : 12267.713815653433\n",
      "    val_loss       : 12272.53025597986\n",
      "    val_log_likelihood: -12172.378506416719\n",
      "    val_log_marginal: -12180.838602565243\n",
      "Train Epoch: 1014 [256/118836 (0%)] Loss: 12416.273438\n",
      "Train Epoch: 1014 [33024/118836 (28%)] Loss: 12274.714844\n",
      "Train Epoch: 1014 [65792/118836 (55%)] Loss: 12266.112305\n",
      "Train Epoch: 1014 [98560/118836 (83%)] Loss: 12266.003906\n",
      "    epoch          : 1014\n",
      "    loss           : 12274.859740746484\n",
      "    val_loss       : 12267.081343240747\n",
      "    val_log_likelihood: -12169.454139041822\n",
      "    val_log_marginal: -12177.6133322154\n",
      "Train Epoch: 1015 [256/118836 (0%)] Loss: 12213.727539\n",
      "Train Epoch: 1015 [33024/118836 (28%)] Loss: 12308.936523\n",
      "Train Epoch: 1015 [65792/118836 (55%)] Loss: 12385.539062\n",
      "Train Epoch: 1015 [98560/118836 (83%)] Loss: 12245.609375\n",
      "    epoch          : 1015\n",
      "    loss           : 12266.202768946443\n",
      "    val_loss       : 12275.092230351336\n",
      "    val_log_likelihood: -12171.29769389087\n",
      "    val_log_marginal: -12179.804914369162\n",
      "Train Epoch: 1016 [256/118836 (0%)] Loss: 12401.741211\n",
      "Train Epoch: 1016 [33024/118836 (28%)] Loss: 12290.356445\n",
      "Train Epoch: 1016 [65792/118836 (55%)] Loss: 12351.962891\n",
      "Train Epoch: 1016 [98560/118836 (83%)] Loss: 12224.882812\n",
      "    epoch          : 1016\n",
      "    loss           : 12268.085049304695\n",
      "    val_loss       : 12268.895037188322\n",
      "    val_log_likelihood: -12167.485338638597\n",
      "    val_log_marginal: -12175.613856724716\n",
      "Train Epoch: 1017 [256/118836 (0%)] Loss: 12145.548828\n",
      "Train Epoch: 1017 [33024/118836 (28%)] Loss: 12360.416992\n",
      "Train Epoch: 1017 [65792/118836 (55%)] Loss: 12174.400391\n",
      "Train Epoch: 1017 [98560/118836 (83%)] Loss: 12330.714844\n",
      "    epoch          : 1017\n",
      "    loss           : 12266.674126667183\n",
      "    val_loss       : 12271.8243839056\n",
      "    val_log_likelihood: -12170.922884518455\n",
      "    val_log_marginal: -12179.433607484289\n",
      "Train Epoch: 1018 [256/118836 (0%)] Loss: 12200.413086\n",
      "Train Epoch: 1018 [33024/118836 (28%)] Loss: 12249.306641\n",
      "Train Epoch: 1018 [65792/118836 (55%)] Loss: 12310.689453\n",
      "Train Epoch: 1018 [98560/118836 (83%)] Loss: 12292.234375\n",
      "    epoch          : 1018\n",
      "    loss           : 12267.513557821547\n",
      "    val_loss       : 12267.235356634339\n",
      "    val_log_likelihood: -12169.876424214226\n",
      "    val_log_marginal: -12177.961442825444\n",
      "Train Epoch: 1019 [256/118836 (0%)] Loss: 12189.759766\n",
      "Train Epoch: 1019 [33024/118836 (28%)] Loss: 12191.486328\n",
      "Train Epoch: 1019 [65792/118836 (55%)] Loss: 12357.537109\n",
      "Train Epoch: 1019 [98560/118836 (83%)] Loss: 12374.073242\n",
      "    epoch          : 1019\n",
      "    loss           : 12272.813832777607\n",
      "    val_loss       : 12269.208984068775\n",
      "    val_log_likelihood: -12170.398893713813\n",
      "    val_log_marginal: -12178.514059912255\n",
      "Train Epoch: 1020 [256/118836 (0%)] Loss: 12256.116211\n",
      "Train Epoch: 1020 [33024/118836 (28%)] Loss: 12186.324219\n",
      "Train Epoch: 1020 [65792/118836 (55%)] Loss: 12269.516602\n",
      "Train Epoch: 1020 [98560/118836 (83%)] Loss: 12305.679688\n",
      "    epoch          : 1020\n",
      "    loss           : 12266.245366780398\n",
      "    val_loss       : 12269.232044715367\n",
      "    val_log_likelihood: -12167.162624554127\n",
      "    val_log_marginal: -12175.507042340278\n",
      "Train Epoch: 1021 [256/118836 (0%)] Loss: 12200.138672\n",
      "Train Epoch: 1021 [33024/118836 (28%)] Loss: 12229.086914\n",
      "Train Epoch: 1021 [65792/118836 (55%)] Loss: 12351.175781\n",
      "Train Epoch: 1021 [98560/118836 (83%)] Loss: 12282.759766\n",
      "    epoch          : 1021\n",
      "    loss           : 12266.357388919045\n",
      "    val_loss       : 12272.654945391108\n",
      "    val_log_likelihood: -12174.74897497286\n",
      "    val_log_marginal: -12183.46045314169\n",
      "Train Epoch: 1022 [256/118836 (0%)] Loss: 12360.080078\n",
      "Train Epoch: 1022 [33024/118836 (28%)] Loss: 12255.573242\n",
      "Train Epoch: 1022 [65792/118836 (55%)] Loss: 12424.588867\n",
      "Train Epoch: 1022 [98560/118836 (83%)] Loss: 12207.106445\n",
      "    epoch          : 1022\n",
      "    loss           : 12268.550460737179\n",
      "    val_loss       : 12267.607585042453\n",
      "    val_log_likelihood: -12168.12851320177\n",
      "    val_log_marginal: -12176.257633926247\n",
      "Train Epoch: 1023 [256/118836 (0%)] Loss: 12211.472656\n",
      "Train Epoch: 1023 [33024/118836 (28%)] Loss: 12354.083008\n",
      "Train Epoch: 1023 [65792/118836 (55%)] Loss: 12221.137695\n",
      "Train Epoch: 1023 [98560/118836 (83%)] Loss: 12381.743164\n",
      "    epoch          : 1023\n",
      "    loss           : 12266.851099339588\n",
      "    val_loss       : 12269.731897495534\n",
      "    val_log_likelihood: -12168.52850367039\n",
      "    val_log_marginal: -12176.673728578515\n",
      "Train Epoch: 1024 [256/118836 (0%)] Loss: 12344.644531\n",
      "Train Epoch: 1024 [33024/118836 (28%)] Loss: 12293.643555\n",
      "Train Epoch: 1024 [65792/118836 (55%)] Loss: 12254.471680\n",
      "Train Epoch: 1024 [98560/118836 (83%)] Loss: 12290.501953\n",
      "    epoch          : 1024\n",
      "    loss           : 12265.791354554383\n",
      "    val_loss       : 12273.10682260148\n",
      "    val_log_likelihood: -12168.823851388028\n",
      "    val_log_marginal: -12176.895208690681\n",
      "Train Epoch: 1025 [256/118836 (0%)] Loss: 12333.954102\n",
      "Train Epoch: 1025 [33024/118836 (28%)] Loss: 12408.211914\n",
      "Train Epoch: 1025 [65792/118836 (55%)] Loss: 12378.050781\n",
      "Train Epoch: 1025 [98560/118836 (83%)] Loss: 12324.223633\n",
      "    epoch          : 1025\n",
      "    loss           : 12266.802504006411\n",
      "    val_loss       : 12270.216667203786\n",
      "    val_log_likelihood: -12170.981747570306\n",
      "    val_log_marginal: -12179.060519931187\n",
      "Train Epoch: 1026 [256/118836 (0%)] Loss: 12244.153320\n",
      "Train Epoch: 1026 [33024/118836 (28%)] Loss: 12295.955078\n",
      "Train Epoch: 1026 [65792/118836 (55%)] Loss: 12409.755859\n",
      "Train Epoch: 1026 [98560/118836 (83%)] Loss: 12218.244141\n",
      "    epoch          : 1026\n",
      "    loss           : 12271.330935141388\n",
      "    val_loss       : 12267.97480581998\n",
      "    val_log_likelihood: -12168.579000109852\n",
      "    val_log_marginal: -12176.777254643719\n",
      "Train Epoch: 1027 [256/118836 (0%)] Loss: 12213.952148\n",
      "Train Epoch: 1027 [33024/118836 (28%)] Loss: 12362.307617\n",
      "Train Epoch: 1027 [65792/118836 (55%)] Loss: 12318.567383\n",
      "Train Epoch: 1027 [98560/118836 (83%)] Loss: 12393.074219\n",
      "    epoch          : 1027\n",
      "    loss           : 12275.260568037893\n",
      "    val_loss       : 12272.166260568798\n",
      "    val_log_likelihood: -12169.643381184087\n",
      "    val_log_marginal: -12178.057271530546\n",
      "Train Epoch: 1028 [256/118836 (0%)] Loss: 12397.014648\n",
      "Train Epoch: 1028 [33024/118836 (28%)] Loss: 12234.558594\n",
      "Train Epoch: 1028 [65792/118836 (55%)] Loss: 12365.633789\n",
      "Train Epoch: 1028 [98560/118836 (83%)] Loss: 12334.164062\n",
      "    epoch          : 1028\n",
      "    loss           : 12265.58253092044\n",
      "    val_loss       : 12272.048607917957\n",
      "    val_log_likelihood: -12166.664654253256\n",
      "    val_log_marginal: -12174.873647860237\n",
      "Train Epoch: 1029 [256/118836 (0%)] Loss: 12383.255859\n",
      "Train Epoch: 1029 [33024/118836 (28%)] Loss: 12210.601562\n",
      "Train Epoch: 1029 [65792/118836 (55%)] Loss: 12340.364258\n",
      "Train Epoch: 1029 [98560/118836 (83%)] Loss: 12303.172852\n",
      "    epoch          : 1029\n",
      "    loss           : 12264.69334031224\n",
      "    val_loss       : 12268.326836661954\n",
      "    val_log_likelihood: -12170.210312629239\n",
      "    val_log_marginal: -12178.343049801442\n",
      "Train Epoch: 1030 [256/118836 (0%)] Loss: 12503.051758\n",
      "Train Epoch: 1030 [33024/118836 (28%)] Loss: 12236.296875\n",
      "Train Epoch: 1030 [65792/118836 (55%)] Loss: 12221.083008\n",
      "Train Epoch: 1030 [98560/118836 (83%)] Loss: 12214.733398\n",
      "    epoch          : 1030\n",
      "    loss           : 12268.461208740435\n",
      "    val_loss       : 12265.389370494404\n",
      "    val_log_likelihood: -12170.515999631667\n",
      "    val_log_marginal: -12178.59302315293\n",
      "Train Epoch: 1031 [256/118836 (0%)] Loss: 12340.142578\n",
      "Train Epoch: 1031 [33024/118836 (28%)] Loss: 12279.220703\n",
      "Train Epoch: 1031 [65792/118836 (55%)] Loss: 12286.359375\n",
      "Train Epoch: 1031 [98560/118836 (83%)] Loss: 12218.333984\n",
      "    epoch          : 1031\n",
      "    loss           : 12269.206569866627\n",
      "    val_loss       : 12270.545112919823\n",
      "    val_log_likelihood: -12168.958757398934\n",
      "    val_log_marginal: -12177.110430791368\n",
      "Train Epoch: 1032 [256/118836 (0%)] Loss: 12251.425781\n",
      "Train Epoch: 1032 [33024/118836 (28%)] Loss: 12228.212891\n",
      "Train Epoch: 1032 [65792/118836 (55%)] Loss: 12315.712891\n",
      "Train Epoch: 1032 [98560/118836 (83%)] Loss: 12391.576172\n",
      "    epoch          : 1032\n",
      "    loss           : 12270.45424421009\n",
      "    val_loss       : 12268.207319549068\n",
      "    val_log_likelihood: -12170.160850102098\n",
      "    val_log_marginal: -12178.222763293683\n",
      "Train Epoch: 1033 [256/118836 (0%)] Loss: 12224.605469\n",
      "Train Epoch: 1033 [33024/118836 (28%)] Loss: 12350.374023\n",
      "Train Epoch: 1033 [65792/118836 (55%)] Loss: 12270.049805\n",
      "Train Epoch: 1033 [98560/118836 (83%)] Loss: 12317.320312\n",
      "    epoch          : 1033\n",
      "    loss           : 12263.51780703965\n",
      "    val_loss       : 12265.415843892737\n",
      "    val_log_likelihood: -12166.300610169821\n",
      "    val_log_marginal: -12174.469227567231\n",
      "Train Epoch: 1034 [256/118836 (0%)] Loss: 12322.798828\n",
      "Train Epoch: 1034 [33024/118836 (28%)] Loss: 12342.541016\n",
      "Train Epoch: 1034 [65792/118836 (55%)] Loss: 12371.752930\n",
      "Train Epoch: 1034 [98560/118836 (83%)] Loss: 12252.722656\n",
      "    epoch          : 1034\n",
      "    loss           : 12268.315252145369\n",
      "    val_loss       : 12269.0710085189\n",
      "    val_log_likelihood: -12169.111319562913\n",
      "    val_log_marginal: -12177.26827749993\n",
      "Train Epoch: 1035 [256/118836 (0%)] Loss: 12335.474609\n",
      "Train Epoch: 1035 [33024/118836 (28%)] Loss: 12239.146484\n",
      "Train Epoch: 1035 [65792/118836 (55%)] Loss: 12207.040039\n",
      "Train Epoch: 1035 [98560/118836 (83%)] Loss: 12293.211914\n",
      "    epoch          : 1035\n",
      "    loss           : 12269.9353717561\n",
      "    val_loss       : 12270.92943850716\n",
      "    val_log_likelihood: -12168.402491082505\n",
      "    val_log_marginal: -12176.472015223111\n",
      "Train Epoch: 1036 [256/118836 (0%)] Loss: 12324.083008\n",
      "Train Epoch: 1036 [33024/118836 (28%)] Loss: 12229.693359\n",
      "Train Epoch: 1036 [65792/118836 (55%)] Loss: 12286.703125\n",
      "Train Epoch: 1036 [98560/118836 (83%)] Loss: 12296.359375\n",
      "    epoch          : 1036\n",
      "    loss           : 12271.946914741004\n",
      "    val_loss       : 12265.716016179955\n",
      "    val_log_likelihood: -12167.551551999328\n",
      "    val_log_marginal: -12175.73984086111\n",
      "Train Epoch: 1037 [256/118836 (0%)] Loss: 12304.458984\n",
      "Train Epoch: 1037 [33024/118836 (28%)] Loss: 12388.927734\n",
      "Train Epoch: 1037 [65792/118836 (55%)] Loss: 12362.285156\n",
      "Train Epoch: 1037 [98560/118836 (83%)] Loss: 12302.818359\n",
      "    epoch          : 1037\n",
      "    loss           : 12268.681100728907\n",
      "    val_loss       : 12274.334666238488\n",
      "    val_log_likelihood: -12167.75498539599\n",
      "    val_log_marginal: -12176.0710854549\n",
      "Train Epoch: 1038 [256/118836 (0%)] Loss: 12219.984375\n",
      "Train Epoch: 1038 [33024/118836 (28%)] Loss: 12252.348633\n",
      "Train Epoch: 1038 [65792/118836 (55%)] Loss: 12269.804688\n",
      "Train Epoch: 1038 [98560/118836 (83%)] Loss: 12271.633789\n",
      "    epoch          : 1038\n",
      "    loss           : 12269.437164947787\n",
      "    val_loss       : 12265.270805557619\n",
      "    val_log_likelihood: -12167.687210504548\n",
      "    val_log_marginal: -12175.705550352539\n",
      "Train Epoch: 1039 [256/118836 (0%)] Loss: 12339.547852\n",
      "Train Epoch: 1039 [33024/118836 (28%)] Loss: 12375.096680\n",
      "Train Epoch: 1039 [65792/118836 (55%)] Loss: 12216.240234\n",
      "Train Epoch: 1039 [98560/118836 (83%)] Loss: 12317.326172\n",
      "    epoch          : 1039\n",
      "    loss           : 12266.231660657051\n",
      "    val_loss       : 12271.326069649662\n",
      "    val_log_likelihood: -12163.60581220275\n",
      "    val_log_marginal: -12171.708705886087\n",
      "Train Epoch: 1040 [256/118836 (0%)] Loss: 12332.065430\n",
      "Train Epoch: 1040 [33024/118836 (28%)] Loss: 12237.391602\n",
      "Train Epoch: 1040 [65792/118836 (55%)] Loss: 12244.530273\n",
      "Train Epoch: 1040 [98560/118836 (83%)] Loss: 12209.088867\n",
      "    epoch          : 1040\n",
      "    loss           : 12268.40943978107\n",
      "    val_loss       : 12267.070053931737\n",
      "    val_log_likelihood: -12165.755079902037\n",
      "    val_log_marginal: -12173.8112419769\n",
      "Train Epoch: 1041 [256/118836 (0%)] Loss: 12507.779297\n",
      "Train Epoch: 1041 [33024/118836 (28%)] Loss: 12267.001953\n",
      "Train Epoch: 1041 [65792/118836 (55%)] Loss: 12354.108398\n",
      "Train Epoch: 1041 [98560/118836 (83%)] Loss: 12216.162109\n",
      "    epoch          : 1041\n",
      "    loss           : 12265.376458624121\n",
      "    val_loss       : 12264.736934771232\n",
      "    val_log_likelihood: -12168.996294070512\n",
      "    val_log_marginal: -12177.211372654338\n",
      "Train Epoch: 1042 [256/118836 (0%)] Loss: 12418.988281\n",
      "Train Epoch: 1042 [33024/118836 (28%)] Loss: 12338.792969\n",
      "Train Epoch: 1042 [65792/118836 (55%)] Loss: 12418.841797\n",
      "Train Epoch: 1042 [98560/118836 (83%)] Loss: 12238.310547\n",
      "    epoch          : 1042\n",
      "    loss           : 12269.100422611662\n",
      "    val_loss       : 12273.659706802324\n",
      "    val_log_likelihood: -12174.493246452388\n",
      "    val_log_marginal: -12182.749433805908\n",
      "Train Epoch: 1043 [256/118836 (0%)] Loss: 12353.721680\n",
      "Train Epoch: 1043 [33024/118836 (28%)] Loss: 12255.634766\n",
      "Train Epoch: 1043 [65792/118836 (55%)] Loss: 12308.604492\n",
      "Train Epoch: 1043 [98560/118836 (83%)] Loss: 12208.156250\n",
      "    epoch          : 1043\n",
      "    loss           : 12265.73606673904\n",
      "    val_loss       : 12275.62195958114\n",
      "    val_log_likelihood: -12177.499827142783\n",
      "    val_log_marginal: -12185.914499526307\n",
      "Train Epoch: 1044 [256/118836 (0%)] Loss: 12348.693359\n",
      "Train Epoch: 1044 [33024/118836 (28%)] Loss: 12389.027344\n",
      "Train Epoch: 1044 [65792/118836 (55%)] Loss: 12383.085938\n",
      "Train Epoch: 1044 [98560/118836 (83%)] Loss: 12176.806641\n",
      "    epoch          : 1044\n",
      "    loss           : 12271.479064406276\n",
      "    val_loss       : 12266.183565445266\n",
      "    val_log_likelihood: -12166.209464982683\n",
      "    val_log_marginal: -12174.320289486246\n",
      "Train Epoch: 1045 [256/118836 (0%)] Loss: 12257.642578\n",
      "Train Epoch: 1045 [33024/118836 (28%)] Loss: 12278.813477\n",
      "Train Epoch: 1045 [65792/118836 (55%)] Loss: 12275.028320\n",
      "Train Epoch: 1045 [98560/118836 (83%)] Loss: 12267.729492\n",
      "    epoch          : 1045\n",
      "    loss           : 12269.756415910619\n",
      "    val_loss       : 12261.165462946967\n",
      "    val_log_likelihood: -12170.351487541357\n",
      "    val_log_marginal: -12178.531811914041\n",
      "Train Epoch: 1046 [256/118836 (0%)] Loss: 12270.687500\n",
      "Train Epoch: 1046 [33024/118836 (28%)] Loss: 12164.164062\n",
      "Train Epoch: 1046 [65792/118836 (55%)] Loss: 12362.605469\n",
      "Train Epoch: 1046 [98560/118836 (83%)] Loss: 12325.906250\n",
      "    epoch          : 1046\n",
      "    loss           : 12272.188952000619\n",
      "    val_loss       : 12268.97955723453\n",
      "    val_log_likelihood: -12166.735655274246\n",
      "    val_log_marginal: -12174.722185878314\n",
      "Train Epoch: 1047 [256/118836 (0%)] Loss: 12270.716797\n",
      "Train Epoch: 1047 [33024/118836 (28%)] Loss: 12224.791016\n",
      "Train Epoch: 1047 [65792/118836 (55%)] Loss: 12369.586914\n",
      "Train Epoch: 1047 [98560/118836 (83%)] Loss: 12340.143555\n",
      "    epoch          : 1047\n",
      "    loss           : 12268.383461603082\n",
      "    val_loss       : 12267.438277531257\n",
      "    val_log_likelihood: -12167.685269011063\n",
      "    val_log_marginal: -12175.865993479745\n",
      "Train Epoch: 1048 [256/118836 (0%)] Loss: 12268.414062\n",
      "Train Epoch: 1048 [33024/118836 (28%)] Loss: 12287.349609\n",
      "Train Epoch: 1048 [65792/118836 (55%)] Loss: 12383.510742\n",
      "Train Epoch: 1048 [98560/118836 (83%)] Loss: 12310.687500\n",
      "    epoch          : 1048\n",
      "    loss           : 12273.863641665375\n",
      "    val_loss       : 12265.513427130321\n",
      "    val_log_likelihood: -12168.08914731312\n",
      "    val_log_marginal: -12176.397965803133\n",
      "Train Epoch: 1049 [256/118836 (0%)] Loss: 12276.031250\n",
      "Train Epoch: 1049 [33024/118836 (28%)] Loss: 12507.107422\n",
      "Train Epoch: 1049 [65792/118836 (55%)] Loss: 12397.290039\n",
      "Train Epoch: 1049 [98560/118836 (83%)] Loss: 12233.641602\n",
      "    epoch          : 1049\n",
      "    loss           : 12267.627108534945\n",
      "    val_loss       : 12270.210819885677\n",
      "    val_log_likelihood: -12171.703445512821\n",
      "    val_log_marginal: -12179.838438123867\n",
      "Train Epoch: 1050 [256/118836 (0%)] Loss: 12290.687500\n",
      "Train Epoch: 1050 [33024/118836 (28%)] Loss: 12251.875000\n",
      "Train Epoch: 1050 [65792/118836 (55%)] Loss: 12330.164062\n",
      "Train Epoch: 1050 [98560/118836 (83%)] Loss: 12275.406250\n",
      "    epoch          : 1050\n",
      "    loss           : 12266.281326412582\n",
      "    val_loss       : 12264.265722361717\n",
      "    val_log_likelihood: -12169.05002568626\n",
      "    val_log_marginal: -12177.05764008921\n",
      "Train Epoch: 1051 [256/118836 (0%)] Loss: 12247.421875\n",
      "Train Epoch: 1051 [33024/118836 (28%)] Loss: 12334.992188\n",
      "Train Epoch: 1051 [65792/118836 (55%)] Loss: 12300.176758\n",
      "Train Epoch: 1051 [98560/118836 (83%)] Loss: 12229.511719\n",
      "    epoch          : 1051\n",
      "    loss           : 12269.727757153381\n",
      "    val_loss       : 12274.114580387912\n",
      "    val_log_likelihood: -12170.928268455335\n",
      "    val_log_marginal: -12178.945361541464\n",
      "Train Epoch: 1052 [256/118836 (0%)] Loss: 12308.911133\n",
      "Train Epoch: 1052 [33024/118836 (28%)] Loss: 12328.785156\n",
      "Train Epoch: 1052 [65792/118836 (55%)] Loss: 12436.135742\n",
      "Train Epoch: 1052 [98560/118836 (83%)] Loss: 12312.620117\n",
      "    epoch          : 1052\n",
      "    loss           : 12268.7397090183\n",
      "    val_loss       : 12267.336461208337\n",
      "    val_log_likelihood: -12168.652658447065\n",
      "    val_log_marginal: -12176.590779403265\n",
      "Train Epoch: 1053 [256/118836 (0%)] Loss: 12215.246094\n",
      "Train Epoch: 1053 [33024/118836 (28%)] Loss: 12304.185547\n",
      "Train Epoch: 1053 [65792/118836 (55%)] Loss: 12401.798828\n",
      "Train Epoch: 1053 [98560/118836 (83%)] Loss: 12250.409180\n",
      "    epoch          : 1053\n",
      "    loss           : 12264.826582370502\n",
      "    val_loss       : 12267.67459745526\n",
      "    val_log_likelihood: -12167.242551954094\n",
      "    val_log_marginal: -12175.243799497162\n",
      "Train Epoch: 1054 [256/118836 (0%)] Loss: 12366.060547\n",
      "Train Epoch: 1054 [33024/118836 (28%)] Loss: 12268.914062\n",
      "Train Epoch: 1054 [65792/118836 (55%)] Loss: 12414.163086\n",
      "Train Epoch: 1054 [98560/118836 (83%)] Loss: 12223.271484\n",
      "    epoch          : 1054\n",
      "    loss           : 12264.430439348118\n",
      "    val_loss       : 12268.55018740852\n",
      "    val_log_likelihood: -12167.815981376654\n",
      "    val_log_marginal: -12176.153767759666\n",
      "Train Epoch: 1055 [256/118836 (0%)] Loss: 12379.443359\n",
      "Train Epoch: 1055 [33024/118836 (28%)] Loss: 12332.894531\n",
      "Train Epoch: 1055 [65792/118836 (55%)] Loss: 12246.037109\n",
      "Train Epoch: 1055 [98560/118836 (83%)] Loss: 12172.667969\n",
      "    epoch          : 1055\n",
      "    loss           : 12267.755470365488\n",
      "    val_loss       : 12267.062077648448\n",
      "    val_log_likelihood: -12170.192583779208\n",
      "    val_log_marginal: -12178.498486939703\n",
      "Train Epoch: 1056 [256/118836 (0%)] Loss: 12403.246094\n",
      "Train Epoch: 1056 [33024/118836 (28%)] Loss: 12371.267578\n",
      "Train Epoch: 1056 [65792/118836 (55%)] Loss: 12245.399414\n",
      "Train Epoch: 1056 [98560/118836 (83%)] Loss: 12209.895508\n",
      "    epoch          : 1056\n",
      "    loss           : 12270.200586260597\n",
      "    val_loss       : 12268.752173774317\n",
      "    val_log_likelihood: -12168.321402631307\n",
      "    val_log_marginal: -12176.468813610494\n",
      "Train Epoch: 1057 [256/118836 (0%)] Loss: 12386.781250\n",
      "Train Epoch: 1057 [33024/118836 (28%)] Loss: 12283.939453\n",
      "Train Epoch: 1057 [65792/118836 (55%)] Loss: 12374.330078\n",
      "Train Epoch: 1057 [98560/118836 (83%)] Loss: 12341.233398\n",
      "    epoch          : 1057\n",
      "    loss           : 12268.033895684708\n",
      "    val_loss       : 12265.17604121313\n",
      "    val_log_likelihood: -12167.529228055211\n",
      "    val_log_marginal: -12175.481134055983\n",
      "Train Epoch: 1058 [256/118836 (0%)] Loss: 12223.626953\n",
      "Train Epoch: 1058 [33024/118836 (28%)] Loss: 12264.916016\n",
      "Train Epoch: 1058 [65792/118836 (55%)] Loss: 12199.077148\n",
      "Train Epoch: 1058 [98560/118836 (83%)] Loss: 12271.429688\n",
      "    epoch          : 1058\n",
      "    loss           : 12267.103102544717\n",
      "    val_loss       : 12267.753340672\n",
      "    val_log_likelihood: -12169.394494739972\n",
      "    val_log_marginal: -12177.909642174385\n",
      "Train Epoch: 1059 [256/118836 (0%)] Loss: 12319.171875\n",
      "Train Epoch: 1059 [33024/118836 (28%)] Loss: 12295.447266\n",
      "Train Epoch: 1059 [65792/118836 (55%)] Loss: 12258.709961\n",
      "Train Epoch: 1059 [98560/118836 (83%)] Loss: 12198.380859\n",
      "    epoch          : 1059\n",
      "    loss           : 12265.756022862388\n",
      "    val_loss       : 12269.191352066275\n",
      "    val_log_likelihood: -12167.60729812862\n",
      "    val_log_marginal: -12175.748251505105\n",
      "Train Epoch: 1060 [256/118836 (0%)] Loss: 12206.698242\n",
      "Train Epoch: 1060 [33024/118836 (28%)] Loss: 12236.890625\n",
      "Train Epoch: 1060 [65792/118836 (55%)] Loss: 12236.360352\n",
      "Train Epoch: 1060 [98560/118836 (83%)] Loss: 12311.878906\n",
      "    epoch          : 1060\n",
      "    loss           : 12261.245224132805\n",
      "    val_loss       : 12269.043005148675\n",
      "    val_log_likelihood: -12171.287259453835\n",
      "    val_log_marginal: -12179.580614662656\n",
      "Train Epoch: 1061 [256/118836 (0%)] Loss: 12352.945312\n",
      "Train Epoch: 1061 [33024/118836 (28%)] Loss: 12204.458984\n",
      "Train Epoch: 1061 [65792/118836 (55%)] Loss: 12233.563477\n",
      "Train Epoch: 1061 [98560/118836 (83%)] Loss: 12255.007812\n",
      "    epoch          : 1061\n",
      "    loss           : 12263.018303640663\n",
      "    val_loss       : 12268.699195149118\n",
      "    val_log_likelihood: -12166.739182046113\n",
      "    val_log_marginal: -12174.941569407767\n",
      "Train Epoch: 1062 [256/118836 (0%)] Loss: 12372.966797\n",
      "Train Epoch: 1062 [33024/118836 (28%)] Loss: 12233.605469\n",
      "Train Epoch: 1062 [65792/118836 (55%)] Loss: 12264.652344\n",
      "Train Epoch: 1062 [98560/118836 (83%)] Loss: 12321.520508\n",
      "    epoch          : 1062\n",
      "    loss           : 12272.505119319945\n",
      "    val_loss       : 12263.460504774723\n",
      "    val_log_likelihood: -12170.95187218905\n",
      "    val_log_marginal: -12179.1041471571\n",
      "Train Epoch: 1063 [256/118836 (0%)] Loss: 12277.627930\n",
      "Train Epoch: 1063 [33024/118836 (28%)] Loss: 12385.821289\n",
      "Train Epoch: 1063 [65792/118836 (55%)] Loss: 12185.121094\n",
      "Train Epoch: 1063 [98560/118836 (83%)] Loss: 12392.913086\n",
      "    epoch          : 1063\n",
      "    loss           : 12267.793287808881\n",
      "    val_loss       : 12270.256366790802\n",
      "    val_log_likelihood: -12170.161101148935\n",
      "    val_log_marginal: -12178.409200781236\n",
      "Train Epoch: 1064 [256/118836 (0%)] Loss: 12294.351562\n",
      "Train Epoch: 1064 [33024/118836 (28%)] Loss: 12255.265625\n",
      "Train Epoch: 1064 [65792/118836 (55%)] Loss: 12301.986328\n",
      "Train Epoch: 1064 [98560/118836 (83%)] Loss: 12374.462891\n",
      "    epoch          : 1064\n",
      "    loss           : 12269.633046584211\n",
      "    val_loss       : 12265.832737912848\n",
      "    val_log_likelihood: -12166.258288584315\n",
      "    val_log_marginal: -12174.348825365321\n",
      "Train Epoch: 1065 [256/118836 (0%)] Loss: 12306.391602\n",
      "Train Epoch: 1065 [33024/118836 (28%)] Loss: 12275.143555\n",
      "Train Epoch: 1065 [65792/118836 (55%)] Loss: 12384.281250\n",
      "Train Epoch: 1065 [98560/118836 (83%)] Loss: 12283.621094\n",
      "    epoch          : 1065\n",
      "    loss           : 12264.892064238265\n",
      "    val_loss       : 12266.7128859655\n",
      "    val_log_likelihood: -12165.166315782672\n",
      "    val_log_marginal: -12173.566655441635\n",
      "Train Epoch: 1066 [256/118836 (0%)] Loss: 12298.290039\n",
      "Train Epoch: 1066 [33024/118836 (28%)] Loss: 12189.113281\n",
      "Train Epoch: 1066 [65792/118836 (55%)] Loss: 12241.924805\n",
      "Train Epoch: 1066 [98560/118836 (83%)] Loss: 12168.208008\n",
      "    epoch          : 1066\n",
      "    loss           : 12262.041457784067\n",
      "    val_loss       : 12261.446326030617\n",
      "    val_log_likelihood: -12172.990778309812\n",
      "    val_log_marginal: -12180.974813738034\n",
      "Train Epoch: 1067 [256/118836 (0%)] Loss: 12203.769531\n",
      "Train Epoch: 1067 [33024/118836 (28%)] Loss: 12283.733398\n",
      "Train Epoch: 1067 [65792/118836 (55%)] Loss: 12281.707031\n",
      "Train Epoch: 1067 [98560/118836 (83%)] Loss: 12303.971680\n",
      "    epoch          : 1067\n",
      "    loss           : 12267.04437890948\n",
      "    val_loss       : 12271.546509895386\n",
      "    val_log_likelihood: -12166.33986281276\n",
      "    val_log_marginal: -12174.530906835369\n",
      "Train Epoch: 1068 [256/118836 (0%)] Loss: 12443.832031\n",
      "Train Epoch: 1068 [33024/118836 (28%)] Loss: 12335.669922\n",
      "Train Epoch: 1068 [65792/118836 (55%)] Loss: 12275.957031\n",
      "Train Epoch: 1068 [98560/118836 (83%)] Loss: 12187.753906\n",
      "    epoch          : 1068\n",
      "    loss           : 12267.15153810613\n",
      "    val_loss       : 12270.744042737975\n",
      "    val_log_likelihood: -12169.611182246434\n",
      "    val_log_marginal: -12178.110875884064\n",
      "Train Epoch: 1069 [256/118836 (0%)] Loss: 12358.296875\n",
      "Train Epoch: 1069 [33024/118836 (28%)] Loss: 12271.117188\n",
      "Train Epoch: 1069 [65792/118836 (55%)] Loss: 12307.316406\n",
      "Train Epoch: 1069 [98560/118836 (83%)] Loss: 12280.559570\n",
      "    epoch          : 1069\n",
      "    loss           : 12271.858506029\n",
      "    val_loss       : 12268.228191449616\n",
      "    val_log_likelihood: -12168.687779317876\n",
      "    val_log_marginal: -12176.852915404848\n",
      "Train Epoch: 1070 [256/118836 (0%)] Loss: 12268.892578\n",
      "Train Epoch: 1070 [33024/118836 (28%)] Loss: 12249.595703\n",
      "Train Epoch: 1070 [65792/118836 (55%)] Loss: 12321.712891\n",
      "Train Epoch: 1070 [98560/118836 (83%)] Loss: 12229.468750\n",
      "    epoch          : 1070\n",
      "    loss           : 12263.78118408809\n",
      "    val_loss       : 12270.497357329332\n",
      "    val_log_likelihood: -12171.140566357786\n",
      "    val_log_marginal: -12179.318037358493\n",
      "Train Epoch: 1071 [256/118836 (0%)] Loss: 12301.970703\n",
      "Train Epoch: 1071 [33024/118836 (28%)] Loss: 12355.157227\n",
      "Train Epoch: 1071 [65792/118836 (55%)] Loss: 12286.716797\n",
      "Train Epoch: 1071 [98560/118836 (83%)] Loss: 12267.407227\n",
      "    epoch          : 1071\n",
      "    loss           : 12271.490228397693\n",
      "    val_loss       : 12268.176200611402\n",
      "    val_log_likelihood: -12167.961758652555\n",
      "    val_log_marginal: -12176.047675033378\n",
      "Train Epoch: 1072 [256/118836 (0%)] Loss: 12286.182617\n",
      "Train Epoch: 1072 [33024/118836 (28%)] Loss: 12322.838867\n",
      "Train Epoch: 1072 [65792/118836 (55%)] Loss: 12286.531250\n",
      "Train Epoch: 1072 [98560/118836 (83%)] Loss: 12136.255859\n",
      "    epoch          : 1072\n",
      "    loss           : 12267.206729153742\n",
      "    val_loss       : 12271.290105019636\n",
      "    val_log_likelihood: -12168.257248694686\n",
      "    val_log_marginal: -12176.400089161332\n",
      "Train Epoch: 1073 [256/118836 (0%)] Loss: 12345.028320\n",
      "Train Epoch: 1073 [33024/118836 (28%)] Loss: 12385.075195\n",
      "Train Epoch: 1073 [65792/118836 (55%)] Loss: 12198.225586\n",
      "Train Epoch: 1073 [98560/118836 (83%)] Loss: 12252.970703\n",
      "    epoch          : 1073\n",
      "    loss           : 12267.960835562706\n",
      "    val_loss       : 12270.381401685467\n",
      "    val_log_likelihood: -12169.21464439878\n",
      "    val_log_marginal: -12177.531116926057\n",
      "Train Epoch: 1074 [256/118836 (0%)] Loss: 12350.407227\n",
      "Train Epoch: 1074 [33024/118836 (28%)] Loss: 12347.478516\n",
      "Train Epoch: 1074 [65792/118836 (55%)] Loss: 12317.075195\n",
      "Train Epoch: 1074 [98560/118836 (83%)] Loss: 12258.882812\n",
      "    epoch          : 1074\n",
      "    loss           : 12267.405433047716\n",
      "    val_loss       : 12265.814023019298\n",
      "    val_log_likelihood: -12170.106952414186\n",
      "    val_log_marginal: -12178.166810392584\n",
      "Train Epoch: 1075 [256/118836 (0%)] Loss: 12183.482422\n",
      "Train Epoch: 1075 [33024/118836 (28%)] Loss: 12338.130859\n",
      "Train Epoch: 1075 [65792/118836 (55%)] Loss: 12328.324219\n",
      "Train Epoch: 1075 [98560/118836 (83%)] Loss: 12191.508789\n",
      "    epoch          : 1075\n",
      "    loss           : 12272.18913245063\n",
      "    val_loss       : 12268.146741029046\n",
      "    val_log_likelihood: -12169.770873882082\n",
      "    val_log_marginal: -12177.74436551136\n",
      "Train Epoch: 1076 [256/118836 (0%)] Loss: 12240.420898\n",
      "Train Epoch: 1076 [33024/118836 (28%)] Loss: 12366.284180\n",
      "Train Epoch: 1076 [65792/118836 (55%)] Loss: 12262.150391\n",
      "Train Epoch: 1076 [98560/118836 (83%)] Loss: 12403.655273\n",
      "    epoch          : 1076\n",
      "    loss           : 12262.060541866987\n",
      "    val_loss       : 12268.940620819523\n",
      "    val_log_likelihood: -12168.125301126964\n",
      "    val_log_marginal: -12176.257331747589\n",
      "Train Epoch: 1077 [256/118836 (0%)] Loss: 12270.609375\n",
      "Train Epoch: 1077 [33024/118836 (28%)] Loss: 12314.823242\n",
      "Train Epoch: 1077 [65792/118836 (55%)] Loss: 12354.222656\n",
      "Train Epoch: 1077 [98560/118836 (83%)] Loss: 12263.611328\n",
      "    epoch          : 1077\n",
      "    loss           : 12271.848924569633\n",
      "    val_loss       : 12268.406877870057\n",
      "    val_log_likelihood: -12167.096841074752\n",
      "    val_log_marginal: -12175.228257815817\n",
      "Train Epoch: 1078 [256/118836 (0%)] Loss: 12188.666016\n",
      "Train Epoch: 1078 [33024/118836 (28%)] Loss: 12361.814453\n",
      "Train Epoch: 1078 [65792/118836 (55%)] Loss: 12408.715820\n",
      "Train Epoch: 1078 [98560/118836 (83%)] Loss: 12341.312500\n",
      "    epoch          : 1078\n",
      "    loss           : 12269.427830981183\n",
      "    val_loss       : 12266.204387867927\n",
      "    val_log_likelihood: -12167.311233134305\n",
      "    val_log_marginal: -12175.46215209349\n",
      "Train Epoch: 1079 [256/118836 (0%)] Loss: 12263.129883\n",
      "Train Epoch: 1079 [33024/118836 (28%)] Loss: 12387.996094\n",
      "Train Epoch: 1079 [65792/118836 (55%)] Loss: 12248.632812\n",
      "Train Epoch: 1079 [98560/118836 (83%)] Loss: 12392.238281\n",
      "    epoch          : 1079\n",
      "    loss           : 12266.741831607993\n",
      "    val_loss       : 12269.073770648443\n",
      "    val_log_likelihood: -12167.212494022693\n",
      "    val_log_marginal: -12175.233048882988\n",
      "Train Epoch: 1080 [256/118836 (0%)] Loss: 12293.397461\n",
      "Train Epoch: 1080 [33024/118836 (28%)] Loss: 12442.744141\n",
      "Train Epoch: 1080 [65792/118836 (55%)] Loss: 12262.447266\n",
      "Train Epoch: 1080 [98560/118836 (83%)] Loss: 12372.519531\n",
      "    epoch          : 1080\n",
      "    loss           : 12266.145440769747\n",
      "    val_loss       : 12265.491680089608\n",
      "    val_log_likelihood: -12169.514186892577\n",
      "    val_log_marginal: -12177.627062231031\n",
      "Train Epoch: 1081 [256/118836 (0%)] Loss: 12311.919922\n",
      "Train Epoch: 1081 [33024/118836 (28%)] Loss: 12306.316406\n",
      "Train Epoch: 1081 [65792/118836 (55%)] Loss: 12228.743164\n",
      "Train Epoch: 1081 [98560/118836 (83%)] Loss: 12250.777344\n",
      "    epoch          : 1081\n",
      "    loss           : 12267.407991334521\n",
      "    val_loss       : 12268.327643508743\n",
      "    val_log_likelihood: -12166.93283802471\n",
      "    val_log_marginal: -12174.924196017806\n",
      "Train Epoch: 1082 [256/118836 (0%)] Loss: 12285.658203\n",
      "Train Epoch: 1082 [33024/118836 (28%)] Loss: 12257.302734\n",
      "Train Epoch: 1082 [65792/118836 (55%)] Loss: 12211.388672\n",
      "Train Epoch: 1082 [98560/118836 (83%)] Loss: 12260.123047\n",
      "    epoch          : 1082\n",
      "    loss           : 12270.274580296214\n",
      "    val_loss       : 12266.680457157765\n",
      "    val_log_likelihood: -12170.048742342586\n",
      "    val_log_marginal: -12178.47992959464\n",
      "Train Epoch: 1083 [256/118836 (0%)] Loss: 12313.098633\n",
      "Train Epoch: 1083 [33024/118836 (28%)] Loss: 12308.933594\n",
      "Train Epoch: 1083 [65792/118836 (55%)] Loss: 12318.908203\n",
      "Train Epoch: 1083 [98560/118836 (83%)] Loss: 12258.284180\n",
      "    epoch          : 1083\n",
      "    loss           : 12266.913546351581\n",
      "    val_loss       : 12265.998530546409\n",
      "    val_log_likelihood: -12168.422638641181\n",
      "    val_log_marginal: -12176.908105860915\n",
      "Train Epoch: 1084 [256/118836 (0%)] Loss: 12313.347656\n",
      "Train Epoch: 1084 [33024/118836 (28%)] Loss: 12248.796875\n",
      "Train Epoch: 1084 [65792/118836 (55%)] Loss: 12381.327148\n",
      "Train Epoch: 1084 [98560/118836 (83%)] Loss: 12380.968750\n",
      "    epoch          : 1084\n",
      "    loss           : 12269.169915574597\n",
      "    val_loss       : 12268.979366514312\n",
      "    val_log_likelihood: -12166.751384796318\n",
      "    val_log_marginal: -12174.974046178626\n",
      "Train Epoch: 1085 [256/118836 (0%)] Loss: 12269.719727\n",
      "Train Epoch: 1085 [33024/118836 (28%)] Loss: 12285.181641\n",
      "Train Epoch: 1085 [65792/118836 (55%)] Loss: 12249.229492\n",
      "Train Epoch: 1085 [98560/118836 (83%)] Loss: 12451.924805\n",
      "    epoch          : 1085\n",
      "    loss           : 12263.84149510184\n",
      "    val_loss       : 12264.707121919548\n",
      "    val_log_likelihood: -12169.13521149969\n",
      "    val_log_marginal: -12177.124912628076\n",
      "Train Epoch: 1086 [256/118836 (0%)] Loss: 12177.902344\n",
      "Train Epoch: 1086 [33024/118836 (28%)] Loss: 12398.061523\n",
      "Train Epoch: 1086 [65792/118836 (55%)] Loss: 12274.544922\n",
      "Train Epoch: 1086 [98560/118836 (83%)] Loss: 12306.936523\n",
      "    epoch          : 1086\n",
      "    loss           : 12265.77924017137\n",
      "    val_loss       : 12267.865229584793\n",
      "    val_log_likelihood: -12170.935128463607\n",
      "    val_log_marginal: -12178.9438710494\n",
      "Train Epoch: 1087 [256/118836 (0%)] Loss: 12236.201172\n",
      "Train Epoch: 1087 [33024/118836 (28%)] Loss: 12298.724609\n",
      "Train Epoch: 1087 [65792/118836 (55%)] Loss: 12241.395508\n",
      "Train Epoch: 1087 [98560/118836 (83%)] Loss: 12332.961914\n",
      "    epoch          : 1087\n",
      "    loss           : 12264.2241581692\n",
      "    val_loss       : 12264.421752347114\n",
      "    val_log_likelihood: -12169.413663312913\n",
      "    val_log_marginal: -12177.538190845915\n",
      "Train Epoch: 1088 [256/118836 (0%)] Loss: 12238.292969\n",
      "Train Epoch: 1088 [33024/118836 (28%)] Loss: 12197.087891\n",
      "Train Epoch: 1088 [65792/118836 (55%)] Loss: 12261.143555\n",
      "Train Epoch: 1088 [98560/118836 (83%)] Loss: 12294.626953\n",
      "    epoch          : 1088\n",
      "    loss           : 12265.792735150435\n",
      "    val_loss       : 12268.378301908577\n",
      "    val_log_likelihood: -12168.04085940731\n",
      "    val_log_marginal: -12176.095936720985\n",
      "Train Epoch: 1089 [256/118836 (0%)] Loss: 12271.545898\n",
      "Train Epoch: 1089 [33024/118836 (28%)] Loss: 12193.039062\n",
      "Train Epoch: 1089 [65792/118836 (55%)] Loss: 12304.791016\n",
      "Train Epoch: 1089 [98560/118836 (83%)] Loss: 12282.857422\n",
      "    epoch          : 1089\n",
      "    loss           : 12263.605099126344\n",
      "    val_loss       : 12269.336526809333\n",
      "    val_log_likelihood: -12168.805846935744\n",
      "    val_log_marginal: -12177.066888694057\n",
      "Train Epoch: 1090 [256/118836 (0%)] Loss: 12172.711914\n",
      "Train Epoch: 1090 [33024/118836 (28%)] Loss: 12352.477539\n",
      "Train Epoch: 1090 [65792/118836 (55%)] Loss: 12280.693359\n",
      "Train Epoch: 1090 [98560/118836 (83%)] Loss: 12315.185547\n",
      "    epoch          : 1090\n",
      "    loss           : 12267.8627155061\n",
      "    val_loss       : 12267.471682565441\n",
      "    val_log_likelihood: -12166.877815310949\n",
      "    val_log_marginal: -12174.874682954378\n",
      "Train Epoch: 1091 [256/118836 (0%)] Loss: 12323.163086\n",
      "Train Epoch: 1091 [33024/118836 (28%)] Loss: 12222.164062\n",
      "Train Epoch: 1091 [65792/118836 (55%)] Loss: 12252.919922\n",
      "Train Epoch: 1091 [98560/118836 (83%)] Loss: 12435.072266\n",
      "    epoch          : 1091\n",
      "    loss           : 12264.392842418838\n",
      "    val_loss       : 12269.86210056974\n",
      "    val_log_likelihood: -12170.440538086747\n",
      "    val_log_marginal: -12178.621118582045\n",
      "Train Epoch: 1092 [256/118836 (0%)] Loss: 12269.393555\n",
      "Train Epoch: 1092 [33024/118836 (28%)] Loss: 12337.417969\n",
      "Train Epoch: 1092 [65792/118836 (55%)] Loss: 12325.635742\n",
      "Train Epoch: 1092 [98560/118836 (83%)] Loss: 12282.857422\n",
      "    epoch          : 1092\n",
      "    loss           : 12268.692985874171\n",
      "    val_loss       : 12268.41674240187\n",
      "    val_log_likelihood: -12169.75776952802\n",
      "    val_log_marginal: -12178.096281175227\n",
      "Train Epoch: 1093 [256/118836 (0%)] Loss: 12233.640625\n",
      "Train Epoch: 1093 [33024/118836 (28%)] Loss: 12365.514648\n",
      "Train Epoch: 1093 [65792/118836 (55%)] Loss: 12298.733398\n",
      "Train Epoch: 1093 [98560/118836 (83%)] Loss: 12385.978516\n",
      "    epoch          : 1093\n",
      "    loss           : 12271.056963722602\n",
      "    val_loss       : 12273.403642276959\n",
      "    val_log_likelihood: -12169.255553886218\n",
      "    val_log_marginal: -12177.454686814264\n",
      "Train Epoch: 1094 [256/118836 (0%)] Loss: 12310.122070\n",
      "Train Epoch: 1094 [33024/118836 (28%)] Loss: 12337.901367\n",
      "Train Epoch: 1094 [65792/118836 (55%)] Loss: 12280.503906\n",
      "Train Epoch: 1094 [98560/118836 (83%)] Loss: 12254.749023\n",
      "    epoch          : 1094\n",
      "    loss           : 12265.33609921035\n",
      "    val_loss       : 12271.114024250324\n",
      "    val_log_likelihood: -12168.93917558416\n",
      "    val_log_marginal: -12177.188651360235\n",
      "Train Epoch: 1095 [256/118836 (0%)] Loss: 12288.664062\n",
      "Train Epoch: 1095 [33024/118836 (28%)] Loss: 12321.894531\n",
      "Train Epoch: 1095 [65792/118836 (55%)] Loss: 12201.128906\n",
      "Train Epoch: 1095 [98560/118836 (83%)] Loss: 12274.376953\n",
      "    epoch          : 1095\n",
      "    loss           : 12266.381920589072\n",
      "    val_loss       : 12267.277898661008\n",
      "    val_log_likelihood: -12166.371484698097\n",
      "    val_log_marginal: -12174.7247716617\n",
      "Train Epoch: 1096 [256/118836 (0%)] Loss: 12178.850586\n",
      "Train Epoch: 1096 [33024/118836 (28%)] Loss: 12186.613281\n",
      "Train Epoch: 1096 [65792/118836 (55%)] Loss: 12518.314453\n",
      "Train Epoch: 1096 [98560/118836 (83%)] Loss: 12346.939453\n",
      "    epoch          : 1096\n",
      "    loss           : 12267.816608670646\n",
      "    val_loss       : 12265.216089883097\n",
      "    val_log_likelihood: -12165.853105614144\n",
      "    val_log_marginal: -12174.028406922538\n",
      "Train Epoch: 1097 [256/118836 (0%)] Loss: 12269.424805\n",
      "Train Epoch: 1097 [33024/118836 (28%)] Loss: 12369.391602\n",
      "Train Epoch: 1097 [65792/118836 (55%)] Loss: 12231.916992\n",
      "Train Epoch: 1097 [98560/118836 (83%)] Loss: 12284.089844\n",
      "    epoch          : 1097\n",
      "    loss           : 12263.612507592794\n",
      "    val_loss       : 12265.711174208504\n",
      "    val_log_likelihood: -12168.212783195047\n",
      "    val_log_marginal: -12176.324705068397\n",
      "Train Epoch: 1098 [256/118836 (0%)] Loss: 12243.787109\n",
      "Train Epoch: 1098 [33024/118836 (28%)] Loss: 12371.130859\n",
      "Train Epoch: 1098 [65792/118836 (55%)] Loss: 12268.259766\n",
      "Train Epoch: 1098 [98560/118836 (83%)] Loss: 12313.374023\n",
      "    epoch          : 1098\n",
      "    loss           : 12267.160985156896\n",
      "    val_loss       : 12269.877114137518\n",
      "    val_log_likelihood: -12169.938834393093\n",
      "    val_log_marginal: -12178.013907619303\n",
      "Train Epoch: 1099 [256/118836 (0%)] Loss: 12235.898438\n",
      "Train Epoch: 1099 [33024/118836 (28%)] Loss: 12188.962891\n",
      "Train Epoch: 1099 [65792/118836 (55%)] Loss: 12338.251953\n",
      "Train Epoch: 1099 [98560/118836 (83%)] Loss: 12390.892578\n",
      "    epoch          : 1099\n",
      "    loss           : 12268.989568955489\n",
      "    val_loss       : 12268.458274889077\n",
      "    val_log_likelihood: -12170.392191861818\n",
      "    val_log_marginal: -12178.515688860687\n",
      "Train Epoch: 1100 [256/118836 (0%)] Loss: 12264.205078\n",
      "Train Epoch: 1100 [33024/118836 (28%)] Loss: 12299.960938\n",
      "Train Epoch: 1100 [65792/118836 (55%)] Loss: 12312.126953\n",
      "Train Epoch: 1100 [98560/118836 (83%)] Loss: 12283.786133\n",
      "    epoch          : 1100\n",
      "    loss           : 12271.591515618538\n",
      "    val_loss       : 12270.228304413651\n",
      "    val_log_likelihood: -12166.397227822581\n",
      "    val_log_marginal: -12174.550651326248\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch1100.pth ...\n",
      "Train Epoch: 1101 [256/118836 (0%)] Loss: 12171.421875\n",
      "Train Epoch: 1101 [33024/118836 (28%)] Loss: 12275.494141\n",
      "Train Epoch: 1101 [65792/118836 (55%)] Loss: 12264.916016\n",
      "Train Epoch: 1101 [98560/118836 (83%)] Loss: 12257.765625\n",
      "    epoch          : 1101\n",
      "    loss           : 12269.55961328448\n",
      "    val_loss       : 12267.218419651063\n",
      "    val_log_likelihood: -12167.842113995295\n",
      "    val_log_marginal: -12175.934238764758\n",
      "Train Epoch: 1102 [256/118836 (0%)] Loss: 12243.738281\n",
      "Train Epoch: 1102 [33024/118836 (28%)] Loss: 12426.114258\n",
      "Train Epoch: 1102 [65792/118836 (55%)] Loss: 12253.468750\n",
      "Train Epoch: 1102 [98560/118836 (83%)] Loss: 12264.127930\n",
      "    epoch          : 1102\n",
      "    loss           : 12274.711021343826\n",
      "    val_loss       : 12269.311407715168\n",
      "    val_log_likelihood: -12168.662722452698\n",
      "    val_log_marginal: -12176.778177220873\n",
      "Train Epoch: 1103 [256/118836 (0%)] Loss: 12302.718750\n",
      "Train Epoch: 1103 [33024/118836 (28%)] Loss: 12236.841797\n",
      "Train Epoch: 1103 [65792/118836 (55%)] Loss: 12434.520508\n",
      "Train Epoch: 1103 [98560/118836 (83%)] Loss: 12222.158203\n",
      "    epoch          : 1103\n",
      "    loss           : 12266.435468362284\n",
      "    val_loss       : 12276.510597583723\n",
      "    val_log_likelihood: -12167.445400059449\n",
      "    val_log_marginal: -12175.621959231577\n",
      "Train Epoch: 1104 [256/118836 (0%)] Loss: 12204.201172\n",
      "Train Epoch: 1104 [33024/118836 (28%)] Loss: 12420.664062\n",
      "Train Epoch: 1104 [65792/118836 (55%)] Loss: 12324.369141\n",
      "Train Epoch: 1104 [98560/118836 (83%)] Loss: 12292.858398\n",
      "    epoch          : 1104\n",
      "    loss           : 12268.325067042751\n",
      "    val_loss       : 12263.113885096418\n",
      "    val_log_likelihood: -12167.996022183881\n",
      "    val_log_marginal: -12176.0250827202\n",
      "Train Epoch: 1105 [256/118836 (0%)] Loss: 12347.552734\n",
      "Train Epoch: 1105 [33024/118836 (28%)] Loss: 12205.522461\n",
      "Train Epoch: 1105 [65792/118836 (55%)] Loss: 12293.046875\n",
      "Train Epoch: 1105 [98560/118836 (83%)] Loss: 12252.919922\n",
      "    epoch          : 1105\n",
      "    loss           : 12264.925925028432\n",
      "    val_loss       : 12267.443884393902\n",
      "    val_log_likelihood: -12167.813969770988\n",
      "    val_log_marginal: -12175.855678945232\n",
      "Train Epoch: 1106 [256/118836 (0%)] Loss: 12194.135742\n",
      "Train Epoch: 1106 [33024/118836 (28%)] Loss: 12294.924805\n",
      "Train Epoch: 1106 [65792/118836 (55%)] Loss: 12230.560547\n",
      "Train Epoch: 1106 [98560/118836 (83%)] Loss: 12325.822266\n",
      "    epoch          : 1106\n",
      "    loss           : 12272.55317491858\n",
      "    val_loss       : 12270.020907836364\n",
      "    val_log_likelihood: -12169.033437532311\n",
      "    val_log_marginal: -12177.072992238085\n",
      "Train Epoch: 1107 [256/118836 (0%)] Loss: 12243.076172\n",
      "Train Epoch: 1107 [33024/118836 (28%)] Loss: 12289.999023\n",
      "Train Epoch: 1107 [65792/118836 (55%)] Loss: 12295.565430\n",
      "Train Epoch: 1107 [98560/118836 (83%)] Loss: 12283.042969\n",
      "    epoch          : 1107\n",
      "    loss           : 12266.067477803195\n",
      "    val_loss       : 12266.633175813673\n",
      "    val_log_likelihood: -12167.787640870554\n",
      "    val_log_marginal: -12175.780306053462\n",
      "Train Epoch: 1108 [256/118836 (0%)] Loss: 12337.003906\n",
      "Train Epoch: 1108 [33024/118836 (28%)] Loss: 12312.459961\n",
      "Train Epoch: 1108 [65792/118836 (55%)] Loss: 12185.138672\n",
      "Train Epoch: 1108 [98560/118836 (83%)] Loss: 12331.219727\n",
      "    epoch          : 1108\n",
      "    loss           : 12268.81016545828\n",
      "    val_loss       : 12265.14003698463\n",
      "    val_log_likelihood: -12168.723473525382\n",
      "    val_log_marginal: -12176.713155677558\n",
      "Train Epoch: 1109 [256/118836 (0%)] Loss: 12357.609375\n",
      "Train Epoch: 1109 [33024/118836 (28%)] Loss: 12298.093750\n",
      "Train Epoch: 1109 [65792/118836 (55%)] Loss: 12395.076172\n",
      "Train Epoch: 1109 [98560/118836 (83%)] Loss: 12257.869141\n",
      "    epoch          : 1109\n",
      "    loss           : 12267.123458339795\n",
      "    val_loss       : 12271.158003804649\n",
      "    val_log_likelihood: -12167.664815155862\n",
      "    val_log_marginal: -12175.725221991916\n",
      "Train Epoch: 1110 [256/118836 (0%)] Loss: 12244.948242\n",
      "Train Epoch: 1110 [33024/118836 (28%)] Loss: 12285.005859\n",
      "Train Epoch: 1110 [65792/118836 (55%)] Loss: 12280.273438\n",
      "Train Epoch: 1110 [98560/118836 (83%)] Loss: 12240.974609\n",
      "    epoch          : 1110\n",
      "    loss           : 12267.242069569376\n",
      "    val_loss       : 12268.409386286625\n",
      "    val_log_likelihood: -12167.180016413358\n",
      "    val_log_marginal: -12175.289322019249\n",
      "Train Epoch: 1111 [256/118836 (0%)] Loss: 12218.212891\n",
      "Train Epoch: 1111 [33024/118836 (28%)] Loss: 12281.777344\n",
      "Train Epoch: 1111 [65792/118836 (55%)] Loss: 12222.365234\n",
      "Train Epoch: 1111 [98560/118836 (83%)] Loss: 12262.812500\n",
      "    epoch          : 1111\n",
      "    loss           : 12267.272495508943\n",
      "    val_loss       : 12267.179677987924\n",
      "    val_log_likelihood: -12167.374696772902\n",
      "    val_log_marginal: -12175.504437952259\n",
      "Train Epoch: 1112 [256/118836 (0%)] Loss: 12304.309570\n",
      "Train Epoch: 1112 [33024/118836 (28%)] Loss: 12325.374023\n",
      "Train Epoch: 1112 [65792/118836 (55%)] Loss: 12423.342773\n",
      "Train Epoch: 1112 [98560/118836 (83%)] Loss: 12360.238281\n",
      "    epoch          : 1112\n",
      "    loss           : 12274.473072238163\n",
      "    val_loss       : 12270.318441187459\n",
      "    val_log_likelihood: -12172.001285282258\n",
      "    val_log_marginal: -12180.478469961252\n",
      "Train Epoch: 1113 [256/118836 (0%)] Loss: 12347.622070\n",
      "Train Epoch: 1113 [33024/118836 (28%)] Loss: 12284.728516\n",
      "Train Epoch: 1113 [65792/118836 (55%)] Loss: 12251.308594\n",
      "Train Epoch: 1113 [98560/118836 (83%)] Loss: 12248.898438\n",
      "    epoch          : 1113\n",
      "    loss           : 12273.902318871484\n",
      "    val_loss       : 12272.977823664076\n",
      "    val_log_likelihood: -12170.254945493436\n",
      "    val_log_marginal: -12178.251489043627\n",
      "Train Epoch: 1114 [256/118836 (0%)] Loss: 12299.235352\n",
      "Train Epoch: 1114 [33024/118836 (28%)] Loss: 12280.437500\n",
      "Train Epoch: 1114 [65792/118836 (55%)] Loss: 12303.906250\n",
      "Train Epoch: 1114 [98560/118836 (83%)] Loss: 12276.814453\n",
      "    epoch          : 1114\n",
      "    loss           : 12269.621690026623\n",
      "    val_loss       : 12264.864267304685\n",
      "    val_log_likelihood: -12167.66881187319\n",
      "    val_log_marginal: -12175.779852238089\n",
      "Train Epoch: 1115 [256/118836 (0%)] Loss: 12271.773438\n",
      "Train Epoch: 1115 [33024/118836 (28%)] Loss: 12219.532227\n",
      "Train Epoch: 1115 [65792/118836 (55%)] Loss: 12301.591797\n",
      "Train Epoch: 1115 [98560/118836 (83%)] Loss: 12236.989258\n",
      "    epoch          : 1115\n",
      "    loss           : 12268.37721984207\n",
      "    val_loss       : 12269.16477450084\n",
      "    val_log_likelihood: -12168.644752894956\n",
      "    val_log_marginal: -12176.684713599256\n",
      "Train Epoch: 1116 [256/118836 (0%)] Loss: 12262.460938\n",
      "Train Epoch: 1116 [33024/118836 (28%)] Loss: 12215.926758\n",
      "Train Epoch: 1116 [65792/118836 (55%)] Loss: 12251.125000\n",
      "Train Epoch: 1116 [98560/118836 (83%)] Loss: 12286.035156\n",
      "    epoch          : 1116\n",
      "    loss           : 12262.866411581112\n",
      "    val_loss       : 12266.882550843851\n",
      "    val_log_likelihood: -12170.564950856855\n",
      "    val_log_marginal: -12178.713875020789\n",
      "Train Epoch: 1117 [256/118836 (0%)] Loss: 12330.144531\n",
      "Train Epoch: 1117 [33024/118836 (28%)] Loss: 12380.783203\n",
      "Train Epoch: 1117 [65792/118836 (55%)] Loss: 12195.176758\n",
      "Train Epoch: 1117 [98560/118836 (83%)] Loss: 12311.503906\n",
      "    epoch          : 1117\n",
      "    loss           : 12268.34372334445\n",
      "    val_loss       : 12266.88926916243\n",
      "    val_log_likelihood: -12165.43647771919\n",
      "    val_log_marginal: -12173.590733259058\n",
      "Train Epoch: 1118 [256/118836 (0%)] Loss: 12360.714844\n",
      "Train Epoch: 1118 [33024/118836 (28%)] Loss: 12388.662109\n",
      "Train Epoch: 1118 [65792/118836 (55%)] Loss: 12291.208008\n",
      "Train Epoch: 1118 [98560/118836 (83%)] Loss: 12167.513672\n",
      "    epoch          : 1118\n",
      "    loss           : 12268.16556571159\n",
      "    val_loss       : 12263.342435012639\n",
      "    val_log_likelihood: -12168.363571230098\n",
      "    val_log_marginal: -12176.383687378744\n",
      "Train Epoch: 1119 [256/118836 (0%)] Loss: 12207.271484\n",
      "Train Epoch: 1119 [33024/118836 (28%)] Loss: 12298.598633\n",
      "Train Epoch: 1119 [65792/118836 (55%)] Loss: 12416.368164\n",
      "Train Epoch: 1119 [98560/118836 (83%)] Loss: 12311.671875\n",
      "    epoch          : 1119\n",
      "    loss           : 12266.965136153329\n",
      "    val_loss       : 12266.989150803714\n",
      "    val_log_likelihood: -12170.031827698513\n",
      "    val_log_marginal: -12177.992942791148\n",
      "Train Epoch: 1120 [256/118836 (0%)] Loss: 12245.645508\n",
      "Train Epoch: 1120 [33024/118836 (28%)] Loss: 12282.174805\n",
      "Train Epoch: 1120 [65792/118836 (55%)] Loss: 12205.992188\n",
      "Train Epoch: 1120 [98560/118836 (83%)] Loss: 12302.189453\n",
      "    epoch          : 1120\n",
      "    loss           : 12262.947115546165\n",
      "    val_loss       : 12271.710818686817\n",
      "    val_log_likelihood: -12167.593103643248\n",
      "    val_log_marginal: -12175.705472988613\n",
      "Train Epoch: 1121 [256/118836 (0%)] Loss: 12256.667969\n",
      "Train Epoch: 1121 [33024/118836 (28%)] Loss: 12300.172852\n",
      "Train Epoch: 1121 [65792/118836 (55%)] Loss: 12301.782227\n",
      "Train Epoch: 1121 [98560/118836 (83%)] Loss: 12454.994141\n",
      "    epoch          : 1121\n",
      "    loss           : 12266.550556050972\n",
      "    val_loss       : 12275.591789053047\n",
      "    val_log_likelihood: -12166.639122273056\n",
      "    val_log_marginal: -12175.050925927726\n",
      "Train Epoch: 1122 [256/118836 (0%)] Loss: 12343.005859\n",
      "Train Epoch: 1122 [33024/118836 (28%)] Loss: 12256.926758\n",
      "Train Epoch: 1122 [65792/118836 (55%)] Loss: 12338.771484\n",
      "Train Epoch: 1122 [98560/118836 (83%)] Loss: 12301.249023\n",
      "    epoch          : 1122\n",
      "    loss           : 12265.982780997983\n",
      "    val_loss       : 12266.817010897843\n",
      "    val_log_likelihood: -12168.05712704844\n",
      "    val_log_marginal: -12176.348051680305\n",
      "Train Epoch: 1123 [256/118836 (0%)] Loss: 12292.701172\n",
      "Train Epoch: 1123 [33024/118836 (28%)] Loss: 12352.686523\n",
      "Train Epoch: 1123 [65792/118836 (55%)] Loss: 12235.139648\n",
      "Train Epoch: 1123 [98560/118836 (83%)] Loss: 12217.036133\n",
      "    epoch          : 1123\n",
      "    loss           : 12264.107824293063\n",
      "    val_loss       : 12269.95349619809\n",
      "    val_log_likelihood: -12169.254031450322\n",
      "    val_log_marginal: -12177.319845410964\n",
      "Train Epoch: 1124 [256/118836 (0%)] Loss: 12297.691406\n",
      "Train Epoch: 1124 [33024/118836 (28%)] Loss: 12253.205078\n",
      "Train Epoch: 1124 [65792/118836 (55%)] Loss: 12278.234375\n",
      "Train Epoch: 1124 [98560/118836 (83%)] Loss: 12245.748047\n",
      "    epoch          : 1124\n",
      "    loss           : 12266.94660165943\n",
      "    val_loss       : 12271.476355805713\n",
      "    val_log_likelihood: -12165.35416488963\n",
      "    val_log_marginal: -12173.373393093863\n",
      "Train Epoch: 1125 [256/118836 (0%)] Loss: 12310.232422\n",
      "Train Epoch: 1125 [33024/118836 (28%)] Loss: 12223.142578\n",
      "Train Epoch: 1125 [65792/118836 (55%)] Loss: 12297.360352\n",
      "Train Epoch: 1125 [98560/118836 (83%)] Loss: 12290.195312\n",
      "    epoch          : 1125\n",
      "    loss           : 12269.329047282103\n",
      "    val_loss       : 12272.625482360869\n",
      "    val_log_likelihood: -12167.893108005066\n",
      "    val_log_marginal: -12176.275053849165\n",
      "Train Epoch: 1126 [256/118836 (0%)] Loss: 12383.970703\n",
      "Train Epoch: 1126 [33024/118836 (28%)] Loss: 12296.253906\n",
      "Train Epoch: 1126 [65792/118836 (55%)] Loss: 12328.746094\n",
      "Train Epoch: 1126 [98560/118836 (83%)] Loss: 12345.811523\n",
      "    epoch          : 1126\n",
      "    loss           : 12269.31022765457\n",
      "    val_loss       : 12264.874311444146\n",
      "    val_log_likelihood: -12168.42892369727\n",
      "    val_log_marginal: -12176.648852169768\n",
      "Train Epoch: 1127 [256/118836 (0%)] Loss: 12362.063477\n",
      "Train Epoch: 1127 [33024/118836 (28%)] Loss: 12190.996094\n",
      "Train Epoch: 1127 [65792/118836 (55%)] Loss: 12292.161133\n",
      "Train Epoch: 1127 [98560/118836 (83%)] Loss: 12324.871094\n",
      "    epoch          : 1127\n",
      "    loss           : 12267.572191958747\n",
      "    val_loss       : 12267.472766839066\n",
      "    val_log_likelihood: -12170.500348783862\n",
      "    val_log_marginal: -12178.585204252324\n",
      "Train Epoch: 1128 [256/118836 (0%)] Loss: 12255.648438\n",
      "Train Epoch: 1128 [33024/118836 (28%)] Loss: 12235.410156\n",
      "Train Epoch: 1128 [65792/118836 (55%)] Loss: 12311.758789\n",
      "Train Epoch: 1128 [98560/118836 (83%)] Loss: 12380.001953\n",
      "    epoch          : 1128\n",
      "    loss           : 12261.055073924732\n",
      "    val_loss       : 12266.526550894838\n",
      "    val_log_likelihood: -12167.154695900537\n",
      "    val_log_marginal: -12175.440176758202\n",
      "Train Epoch: 1129 [256/118836 (0%)] Loss: 12246.343750\n",
      "Train Epoch: 1129 [33024/118836 (28%)] Loss: 12335.560547\n",
      "Train Epoch: 1129 [65792/118836 (55%)] Loss: 12299.548828\n",
      "Train Epoch: 1129 [98560/118836 (83%)] Loss: 12308.593750\n",
      "    epoch          : 1129\n",
      "    loss           : 12266.477934533965\n",
      "    val_loss       : 12270.841430317843\n",
      "    val_log_likelihood: -12166.500372046887\n",
      "    val_log_marginal: -12174.709220595121\n",
      "Train Epoch: 1130 [256/118836 (0%)] Loss: 12305.521484\n",
      "Train Epoch: 1130 [33024/118836 (28%)] Loss: 12182.668945\n",
      "Train Epoch: 1130 [65792/118836 (55%)] Loss: 12290.283203\n",
      "Train Epoch: 1130 [98560/118836 (83%)] Loss: 12347.166992\n",
      "    epoch          : 1130\n",
      "    loss           : 12270.34392657284\n",
      "    val_loss       : 12265.662857983376\n",
      "    val_log_likelihood: -12164.515920149659\n",
      "    val_log_marginal: -12172.619165652866\n",
      "Train Epoch: 1131 [256/118836 (0%)] Loss: 12267.255859\n",
      "Train Epoch: 1131 [33024/118836 (28%)] Loss: 12279.468750\n",
      "Train Epoch: 1131 [65792/118836 (55%)] Loss: 12248.787109\n",
      "Train Epoch: 1131 [98560/118836 (83%)] Loss: 12310.378906\n",
      "    epoch          : 1131\n",
      "    loss           : 12270.101783175662\n",
      "    val_loss       : 12267.881706744352\n",
      "    val_log_likelihood: -12167.342990882185\n",
      "    val_log_marginal: -12175.545602562659\n",
      "Train Epoch: 1132 [256/118836 (0%)] Loss: 12239.369141\n",
      "Train Epoch: 1132 [33024/118836 (28%)] Loss: 12417.641602\n",
      "Train Epoch: 1132 [65792/118836 (55%)] Loss: 12300.171875\n",
      "Train Epoch: 1132 [98560/118836 (83%)] Loss: 12310.629883\n",
      "    epoch          : 1132\n",
      "    loss           : 12267.85324389992\n",
      "    val_loss       : 12269.336403862497\n",
      "    val_log_likelihood: -12166.841204475548\n",
      "    val_log_marginal: -12174.958829001676\n",
      "Train Epoch: 1133 [256/118836 (0%)] Loss: 12257.529297\n",
      "Train Epoch: 1133 [33024/118836 (28%)] Loss: 12280.967773\n",
      "Train Epoch: 1133 [65792/118836 (55%)] Loss: 12302.054688\n",
      "Train Epoch: 1133 [98560/118836 (83%)] Loss: 12193.351562\n",
      "    epoch          : 1133\n",
      "    loss           : 12272.584148993226\n",
      "    val_loss       : 12268.14269652839\n",
      "    val_log_likelihood: -12166.66103442928\n",
      "    val_log_marginal: -12174.949497888249\n",
      "Train Epoch: 1134 [256/118836 (0%)] Loss: 12212.565430\n",
      "Train Epoch: 1134 [33024/118836 (28%)] Loss: 12177.101562\n",
      "Train Epoch: 1134 [65792/118836 (55%)] Loss: 12331.718750\n",
      "Train Epoch: 1134 [98560/118836 (83%)] Loss: 12317.955078\n",
      "    epoch          : 1134\n",
      "    loss           : 12270.079877966036\n",
      "    val_loss       : 12270.287033723478\n",
      "    val_log_likelihood: -12171.710869487955\n",
      "    val_log_marginal: -12179.91105909848\n",
      "Train Epoch: 1135 [256/118836 (0%)] Loss: 12293.945312\n",
      "Train Epoch: 1135 [33024/118836 (28%)] Loss: 12328.751953\n",
      "Train Epoch: 1135 [65792/118836 (55%)] Loss: 12208.880859\n",
      "Train Epoch: 1135 [98560/118836 (83%)] Loss: 12367.712891\n",
      "    epoch          : 1135\n",
      "    loss           : 12267.038059928143\n",
      "    val_loss       : 12262.681868248636\n",
      "    val_log_likelihood: -12167.358192947424\n",
      "    val_log_marginal: -12175.307988087341\n",
      "Train Epoch: 1136 [256/118836 (0%)] Loss: 12377.542969\n",
      "Train Epoch: 1136 [33024/118836 (28%)] Loss: 12338.341797\n",
      "Train Epoch: 1136 [65792/118836 (55%)] Loss: 12280.808594\n",
      "Train Epoch: 1136 [98560/118836 (83%)] Loss: 12251.252930\n",
      "    epoch          : 1136\n",
      "    loss           : 12268.192317223687\n",
      "    val_loss       : 12265.638155691046\n",
      "    val_log_likelihood: -12169.644655642576\n",
      "    val_log_marginal: -12177.825318326197\n",
      "Train Epoch: 1137 [256/118836 (0%)] Loss: 12214.113281\n",
      "Train Epoch: 1137 [33024/118836 (28%)] Loss: 12279.880859\n",
      "Train Epoch: 1137 [65792/118836 (55%)] Loss: 12299.289062\n",
      "Train Epoch: 1137 [98560/118836 (83%)] Loss: 12298.017578\n",
      "    epoch          : 1137\n",
      "    loss           : 12271.661549931505\n",
      "    val_loss       : 12271.497353942794\n",
      "    val_log_likelihood: -12168.772442682486\n",
      "    val_log_marginal: -12177.31428912116\n",
      "Train Epoch: 1138 [256/118836 (0%)] Loss: 12201.002930\n",
      "Train Epoch: 1138 [33024/118836 (28%)] Loss: 12290.554688\n",
      "Train Epoch: 1138 [65792/118836 (55%)] Loss: 12335.233398\n",
      "Train Epoch: 1138 [98560/118836 (83%)] Loss: 12313.648438\n",
      "    epoch          : 1138\n",
      "    loss           : 12268.596289708696\n",
      "    val_loss       : 12271.466674232392\n",
      "    val_log_likelihood: -12168.030959696805\n",
      "    val_log_marginal: -12176.017068254028\n",
      "Train Epoch: 1139 [256/118836 (0%)] Loss: 12190.720703\n",
      "Train Epoch: 1139 [33024/118836 (28%)] Loss: 12260.865234\n",
      "Train Epoch: 1139 [65792/118836 (55%)] Loss: 12219.604492\n",
      "Train Epoch: 1139 [98560/118836 (83%)] Loss: 12323.611328\n",
      "    epoch          : 1139\n",
      "    loss           : 12269.180118027554\n",
      "    val_loss       : 12268.833470737434\n",
      "    val_log_likelihood: -12168.429129025797\n",
      "    val_log_marginal: -12176.470509213023\n",
      "Train Epoch: 1140 [256/118836 (0%)] Loss: 12362.382812\n",
      "Train Epoch: 1140 [33024/118836 (28%)] Loss: 12241.088867\n",
      "Train Epoch: 1140 [65792/118836 (55%)] Loss: 12386.300781\n",
      "Train Epoch: 1140 [98560/118836 (83%)] Loss: 12326.365234\n",
      "    epoch          : 1140\n",
      "    loss           : 12269.9173086616\n",
      "    val_loss       : 12267.458003714522\n",
      "    val_log_likelihood: -12167.961057530758\n",
      "    val_log_marginal: -12175.962472007976\n",
      "Train Epoch: 1141 [256/118836 (0%)] Loss: 12265.641602\n",
      "Train Epoch: 1141 [33024/118836 (28%)] Loss: 12306.396484\n",
      "Train Epoch: 1141 [65792/118836 (55%)] Loss: 12223.210938\n",
      "Train Epoch: 1141 [98560/118836 (83%)] Loss: 12201.457031\n",
      "    epoch          : 1141\n",
      "    loss           : 12269.277226142473\n",
      "    val_loss       : 12265.609581407816\n",
      "    val_log_likelihood: -12169.473110202129\n",
      "    val_log_marginal: -12177.503923220502\n",
      "Train Epoch: 1142 [256/118836 (0%)] Loss: 12309.404297\n",
      "Train Epoch: 1142 [33024/118836 (28%)] Loss: 12193.146484\n",
      "Train Epoch: 1142 [65792/118836 (55%)] Loss: 12282.474609\n",
      "Train Epoch: 1142 [98560/118836 (83%)] Loss: 12332.036133\n",
      "    epoch          : 1142\n",
      "    loss           : 12268.2774582881\n",
      "    val_loss       : 12265.485929806095\n",
      "    val_log_likelihood: -12166.693018022384\n",
      "    val_log_marginal: -12174.76967628514\n",
      "Train Epoch: 1143 [256/118836 (0%)] Loss: 12247.332031\n",
      "Train Epoch: 1143 [33024/118836 (28%)] Loss: 12260.130859\n",
      "Train Epoch: 1143 [65792/118836 (55%)] Loss: 12344.494141\n",
      "Train Epoch: 1143 [98560/118836 (83%)] Loss: 12449.702148\n",
      "    epoch          : 1143\n",
      "    loss           : 12276.862850722446\n",
      "    val_loss       : 12267.3214850368\n",
      "    val_log_likelihood: -12171.192907684554\n",
      "    val_log_marginal: -12179.237559714647\n",
      "Train Epoch: 1144 [256/118836 (0%)] Loss: 12300.829102\n",
      "Train Epoch: 1144 [33024/118836 (28%)] Loss: 12303.541016\n",
      "Train Epoch: 1144 [65792/118836 (55%)] Loss: 12238.251953\n",
      "Train Epoch: 1144 [98560/118836 (83%)] Loss: 12219.219727\n",
      "    epoch          : 1144\n",
      "    loss           : 12266.774575288204\n",
      "    val_loss       : 12264.622383994207\n",
      "    val_log_likelihood: -12168.578136146867\n",
      "    val_log_marginal: -12176.877653197647\n",
      "Train Epoch: 1145 [256/118836 (0%)] Loss: 12361.126953\n",
      "Train Epoch: 1145 [33024/118836 (28%)] Loss: 12307.787109\n",
      "Train Epoch: 1145 [65792/118836 (55%)] Loss: 12332.020508\n",
      "Train Epoch: 1145 [98560/118836 (83%)] Loss: 12350.940430\n",
      "    epoch          : 1145\n",
      "    loss           : 12266.922948814878\n",
      "    val_loss       : 12271.971070102847\n",
      "    val_log_likelihood: -12165.730680055833\n",
      "    val_log_marginal: -12173.918735489784\n",
      "Train Epoch: 1146 [256/118836 (0%)] Loss: 12266.305664\n",
      "Train Epoch: 1146 [33024/118836 (28%)] Loss: 12485.097656\n",
      "Train Epoch: 1146 [65792/118836 (55%)] Loss: 12357.442383\n",
      "Train Epoch: 1146 [98560/118836 (83%)] Loss: 12332.261719\n",
      "    epoch          : 1146\n",
      "    loss           : 12268.841412711952\n",
      "    val_loss       : 12266.141175012657\n",
      "    val_log_likelihood: -12163.865826451354\n",
      "    val_log_marginal: -12171.92461300402\n",
      "Train Epoch: 1147 [256/118836 (0%)] Loss: 12265.155273\n",
      "Train Epoch: 1147 [33024/118836 (28%)] Loss: 12229.083984\n",
      "Train Epoch: 1147 [65792/118836 (55%)] Loss: 12304.878906\n",
      "Train Epoch: 1147 [98560/118836 (83%)] Loss: 12186.628906\n",
      "    epoch          : 1147\n",
      "    loss           : 12263.225804836125\n",
      "    val_loss       : 12268.990375497637\n",
      "    val_log_likelihood: -12170.459498423284\n",
      "    val_log_marginal: -12178.715832412103\n",
      "Train Epoch: 1148 [256/118836 (0%)] Loss: 12234.561523\n",
      "Train Epoch: 1148 [33024/118836 (28%)] Loss: 12274.351562\n",
      "Train Epoch: 1148 [65792/118836 (55%)] Loss: 12292.220703\n",
      "Train Epoch: 1148 [98560/118836 (83%)] Loss: 12275.618164\n",
      "    epoch          : 1148\n",
      "    loss           : 12264.687072541874\n",
      "    val_loss       : 12269.581562914243\n",
      "    val_log_likelihood: -12169.329824493383\n",
      "    val_log_marginal: -12177.473715418499\n",
      "Train Epoch: 1149 [256/118836 (0%)] Loss: 12399.392578\n",
      "Train Epoch: 1149 [33024/118836 (28%)] Loss: 12188.516602\n",
      "Train Epoch: 1149 [65792/118836 (55%)] Loss: 12159.707031\n",
      "Train Epoch: 1149 [98560/118836 (83%)] Loss: 12258.929688\n",
      "    epoch          : 1149\n",
      "    loss           : 12267.30843737076\n",
      "    val_loss       : 12271.267712746365\n",
      "    val_log_likelihood: -12168.447466753258\n",
      "    val_log_marginal: -12176.541227081592\n",
      "Train Epoch: 1150 [256/118836 (0%)] Loss: 12325.580078\n",
      "Train Epoch: 1150 [33024/118836 (28%)] Loss: 12257.935547\n",
      "Train Epoch: 1150 [65792/118836 (55%)] Loss: 12224.608398\n",
      "Train Epoch: 1150 [98560/118836 (83%)] Loss: 12433.346680\n",
      "    epoch          : 1150\n",
      "    loss           : 12265.343482313638\n",
      "    val_loss       : 12263.904225315588\n",
      "    val_log_likelihood: -12168.713610163359\n",
      "    val_log_marginal: -12177.123590624913\n",
      "Train Epoch: 1151 [256/118836 (0%)] Loss: 12231.759766\n",
      "Train Epoch: 1151 [33024/118836 (28%)] Loss: 12252.169922\n",
      "Train Epoch: 1151 [65792/118836 (55%)] Loss: 12304.574219\n",
      "Train Epoch: 1151 [98560/118836 (83%)] Loss: 12334.011719\n",
      "    epoch          : 1151\n",
      "    loss           : 12273.378883471618\n",
      "    val_loss       : 12269.129049820605\n",
      "    val_log_likelihood: -12168.461468026262\n",
      "    val_log_marginal: -12176.650453881646\n",
      "Train Epoch: 1152 [256/118836 (0%)] Loss: 12411.400391\n",
      "Train Epoch: 1152 [33024/118836 (28%)] Loss: 12255.972656\n",
      "Train Epoch: 1152 [65792/118836 (55%)] Loss: 12267.091797\n",
      "Train Epoch: 1152 [98560/118836 (83%)] Loss: 12301.683594\n",
      "    epoch          : 1152\n",
      "    loss           : 12264.601283182124\n",
      "    val_loss       : 12269.818651130632\n",
      "    val_log_likelihood: -12168.19016846309\n",
      "    val_log_marginal: -12176.326078743099\n",
      "Train Epoch: 1153 [256/118836 (0%)] Loss: 12349.759766\n",
      "Train Epoch: 1153 [33024/118836 (28%)] Loss: 12190.254883\n",
      "Train Epoch: 1153 [65792/118836 (55%)] Loss: 12327.250000\n",
      "Train Epoch: 1153 [98560/118836 (83%)] Loss: 12359.685547\n",
      "    epoch          : 1153\n",
      "    loss           : 12266.74154905914\n",
      "    val_loss       : 12270.45163251337\n",
      "    val_log_likelihood: -12168.285511495811\n",
      "    val_log_marginal: -12176.50241654318\n",
      "Train Epoch: 1154 [256/118836 (0%)] Loss: 12386.603516\n",
      "Train Epoch: 1154 [33024/118836 (28%)] Loss: 12379.418945\n",
      "Train Epoch: 1154 [65792/118836 (55%)] Loss: 12330.501953\n",
      "Train Epoch: 1154 [98560/118836 (83%)] Loss: 12288.429688\n",
      "    epoch          : 1154\n",
      "    loss           : 12263.82501195461\n",
      "    val_loss       : 12268.610324380277\n",
      "    val_log_likelihood: -12170.004627080749\n",
      "    val_log_marginal: -12178.076014233437\n",
      "Train Epoch: 1155 [256/118836 (0%)] Loss: 12306.252930\n",
      "Train Epoch: 1155 [33024/118836 (28%)] Loss: 12355.934570\n",
      "Train Epoch: 1155 [65792/118836 (55%)] Loss: 12330.388672\n",
      "Train Epoch: 1155 [98560/118836 (83%)] Loss: 12355.474609\n",
      "    epoch          : 1155\n",
      "    loss           : 12266.2516366509\n",
      "    val_loss       : 12263.166656443611\n",
      "    val_log_likelihood: -12170.215800926644\n",
      "    val_log_marginal: -12178.259426584254\n",
      "Train Epoch: 1156 [256/118836 (0%)] Loss: 12297.365234\n",
      "Train Epoch: 1156 [33024/118836 (28%)] Loss: 12160.417969\n",
      "Train Epoch: 1156 [65792/118836 (55%)] Loss: 12354.801758\n",
      "Train Epoch: 1156 [98560/118836 (83%)] Loss: 12260.525391\n",
      "    epoch          : 1156\n",
      "    loss           : 12265.990306748863\n",
      "    val_loss       : 12265.539375859951\n",
      "    val_log_likelihood: -12169.32973596464\n",
      "    val_log_marginal: -12177.305458815477\n",
      "Train Epoch: 1157 [256/118836 (0%)] Loss: 12309.313477\n",
      "Train Epoch: 1157 [33024/118836 (28%)] Loss: 12256.747070\n",
      "Train Epoch: 1157 [65792/118836 (55%)] Loss: 12237.502930\n",
      "Train Epoch: 1157 [98560/118836 (83%)] Loss: 12259.236328\n",
      "    epoch          : 1157\n",
      "    loss           : 12266.583670324131\n",
      "    val_loss       : 12268.193064938312\n",
      "    val_log_likelihood: -12169.26180227073\n",
      "    val_log_marginal: -12177.248352478447\n",
      "Train Epoch: 1158 [256/118836 (0%)] Loss: 12249.603516\n",
      "Train Epoch: 1158 [33024/118836 (28%)] Loss: 12344.891602\n",
      "Train Epoch: 1158 [65792/118836 (55%)] Loss: 12314.250000\n",
      "Train Epoch: 1158 [98560/118836 (83%)] Loss: 12269.378906\n",
      "    epoch          : 1158\n",
      "    loss           : 12268.830252597705\n",
      "    val_loss       : 12269.378252374985\n",
      "    val_log_likelihood: -12168.157486332972\n",
      "    val_log_marginal: -12176.355518625302\n",
      "Train Epoch: 1159 [256/118836 (0%)] Loss: 12359.075195\n",
      "Train Epoch: 1159 [33024/118836 (28%)] Loss: 12425.170898\n",
      "Train Epoch: 1159 [65792/118836 (55%)] Loss: 12300.275391\n",
      "Train Epoch: 1159 [98560/118836 (83%)] Loss: 12241.482422\n",
      "    epoch          : 1159\n",
      "    loss           : 12266.976123087263\n",
      "    val_loss       : 12296.104390611828\n",
      "    val_log_likelihood: -12172.608391329353\n",
      "    val_log_marginal: -12180.76787575674\n",
      "Train Epoch: 1160 [256/118836 (0%)] Loss: 12384.347656\n",
      "Train Epoch: 1160 [33024/118836 (28%)] Loss: 12329.402344\n",
      "Train Epoch: 1160 [65792/118836 (55%)] Loss: 12192.255859\n",
      "Train Epoch: 1160 [98560/118836 (83%)] Loss: 12248.869141\n",
      "    epoch          : 1160\n",
      "    loss           : 12275.95545808778\n",
      "    val_loss       : 12268.734640365172\n",
      "    val_log_likelihood: -12166.887554441944\n",
      "    val_log_marginal: -12174.857521957396\n",
      "Train Epoch: 1161 [256/118836 (0%)] Loss: 12351.160156\n",
      "Train Epoch: 1161 [33024/118836 (28%)] Loss: 12236.479492\n",
      "Train Epoch: 1161 [65792/118836 (55%)] Loss: 12288.887695\n",
      "Train Epoch: 1161 [98560/118836 (83%)] Loss: 12226.203125\n",
      "    epoch          : 1161\n",
      "    loss           : 12271.945362741677\n",
      "    val_loss       : 12270.569470719733\n",
      "    val_log_likelihood: -12170.030724966398\n",
      "    val_log_marginal: -12178.151899942668\n",
      "Train Epoch: 1162 [256/118836 (0%)] Loss: 12313.332031\n",
      "Train Epoch: 1162 [33024/118836 (28%)] Loss: 12287.418945\n",
      "Train Epoch: 1162 [65792/118836 (55%)] Loss: 12318.918945\n",
      "Train Epoch: 1162 [98560/118836 (83%)] Loss: 12256.286133\n",
      "    epoch          : 1162\n",
      "    loss           : 12271.042042429177\n",
      "    val_loss       : 12269.35632071235\n",
      "    val_log_likelihood: -12167.153391393973\n",
      "    val_log_marginal: -12175.177088148323\n",
      "Train Epoch: 1163 [256/118836 (0%)] Loss: 12319.077148\n",
      "Train Epoch: 1163 [33024/118836 (28%)] Loss: 12264.493164\n",
      "Train Epoch: 1163 [65792/118836 (55%)] Loss: 12272.833008\n",
      "Train Epoch: 1163 [98560/118836 (83%)] Loss: 12220.501953\n",
      "    epoch          : 1163\n",
      "    loss           : 12264.689575094346\n",
      "    val_loss       : 12269.523787342545\n",
      "    val_log_likelihood: -12168.77219971309\n",
      "    val_log_marginal: -12177.001343497654\n",
      "Train Epoch: 1164 [256/118836 (0%)] Loss: 12285.599609\n",
      "Train Epoch: 1164 [33024/118836 (28%)] Loss: 12282.714844\n",
      "Train Epoch: 1164 [65792/118836 (55%)] Loss: 12363.494141\n",
      "Train Epoch: 1164 [98560/118836 (83%)] Loss: 12191.693359\n",
      "    epoch          : 1164\n",
      "    loss           : 12266.584053356339\n",
      "    val_loss       : 12267.48284758686\n",
      "    val_log_likelihood: -12169.422538157827\n",
      "    val_log_marginal: -12177.541427559443\n",
      "Train Epoch: 1165 [256/118836 (0%)] Loss: 12222.056641\n",
      "Train Epoch: 1165 [33024/118836 (28%)] Loss: 12121.755859\n",
      "Train Epoch: 1165 [65792/118836 (55%)] Loss: 12237.341797\n",
      "Train Epoch: 1165 [98560/118836 (83%)] Loss: 12273.732422\n",
      "    epoch          : 1165\n",
      "    loss           : 12266.097613762666\n",
      "    val_loss       : 12267.595763472083\n",
      "    val_log_likelihood: -12167.244732216708\n",
      "    val_log_marginal: -12175.379134127867\n",
      "Train Epoch: 1166 [256/118836 (0%)] Loss: 12332.735352\n",
      "Train Epoch: 1166 [33024/118836 (28%)] Loss: 12212.432617\n",
      "Train Epoch: 1166 [65792/118836 (55%)] Loss: 12325.745117\n",
      "Train Epoch: 1166 [98560/118836 (83%)] Loss: 12318.610352\n",
      "    epoch          : 1166\n",
      "    loss           : 12270.781681981493\n",
      "    val_loss       : 12267.162271667468\n",
      "    val_log_likelihood: -12164.504995250465\n",
      "    val_log_marginal: -12172.74471101324\n",
      "Train Epoch: 1167 [256/118836 (0%)] Loss: 12297.319336\n",
      "Train Epoch: 1167 [33024/118836 (28%)] Loss: 12459.864258\n",
      "Train Epoch: 1167 [65792/118836 (55%)] Loss: 12327.541016\n",
      "Train Epoch: 1167 [98560/118836 (83%)] Loss: 12327.017578\n",
      "    epoch          : 1167\n",
      "    loss           : 12271.50831782465\n",
      "    val_loss       : 12266.57687434938\n",
      "    val_log_likelihood: -12167.161607604427\n",
      "    val_log_marginal: -12175.305606565418\n",
      "Train Epoch: 1168 [256/118836 (0%)] Loss: 12268.228516\n",
      "Train Epoch: 1168 [33024/118836 (28%)] Loss: 12375.457031\n",
      "Train Epoch: 1168 [65792/118836 (55%)] Loss: 12313.181641\n",
      "Train Epoch: 1168 [98560/118836 (83%)] Loss: 12267.732422\n",
      "    epoch          : 1168\n",
      "    loss           : 12264.787054771505\n",
      "    val_loss       : 12264.995522200034\n",
      "    val_log_likelihood: -12170.280116896713\n",
      "    val_log_marginal: -12178.383539882656\n",
      "Train Epoch: 1169 [256/118836 (0%)] Loss: 12395.248047\n",
      "Train Epoch: 1169 [33024/118836 (28%)] Loss: 12313.372070\n",
      "Train Epoch: 1169 [65792/118836 (55%)] Loss: 12229.287109\n",
      "Train Epoch: 1169 [98560/118836 (83%)] Loss: 12256.087891\n",
      "    epoch          : 1169\n",
      "    loss           : 12269.956131100082\n",
      "    val_loss       : 12269.520445992974\n",
      "    val_log_likelihood: -12174.936822141231\n",
      "    val_log_marginal: -12183.080942151417\n",
      "Train Epoch: 1170 [256/118836 (0%)] Loss: 12352.425781\n",
      "Train Epoch: 1170 [33024/118836 (28%)] Loss: 12175.089844\n",
      "Train Epoch: 1170 [65792/118836 (55%)] Loss: 12332.992188\n",
      "Train Epoch: 1170 [98560/118836 (83%)] Loss: 12296.226562\n",
      "    epoch          : 1170\n",
      "    loss           : 12271.423343317048\n",
      "    val_loss       : 12268.439436139472\n",
      "    val_log_likelihood: -12168.812365429849\n",
      "    val_log_marginal: -12176.885399572437\n",
      "Train Epoch: 1171 [256/118836 (0%)] Loss: 12295.254883\n",
      "Train Epoch: 1171 [33024/118836 (28%)] Loss: 12315.449219\n",
      "Train Epoch: 1171 [65792/118836 (55%)] Loss: 12392.185547\n",
      "Train Epoch: 1171 [98560/118836 (83%)] Loss: 12223.550781\n",
      "    epoch          : 1171\n",
      "    loss           : 12271.137369630118\n",
      "    val_loss       : 12264.992162341407\n",
      "    val_log_likelihood: -12170.280803963762\n",
      "    val_log_marginal: -12178.302262102923\n",
      "Train Epoch: 1172 [256/118836 (0%)] Loss: 12231.912109\n",
      "Train Epoch: 1172 [33024/118836 (28%)] Loss: 12200.043945\n",
      "Train Epoch: 1172 [65792/118836 (55%)] Loss: 12231.613281\n",
      "Train Epoch: 1172 [98560/118836 (83%)] Loss: 12277.375000\n",
      "    epoch          : 1172\n",
      "    loss           : 12266.385919729631\n",
      "    val_loss       : 12269.805256868554\n",
      "    val_log_likelihood: -12168.241832092639\n",
      "    val_log_marginal: -12176.305919666145\n",
      "Train Epoch: 1173 [256/118836 (0%)] Loss: 12183.780273\n",
      "Train Epoch: 1173 [33024/118836 (28%)] Loss: 12361.510742\n",
      "Train Epoch: 1173 [65792/118836 (55%)] Loss: 12269.866211\n",
      "Train Epoch: 1173 [98560/118836 (83%)] Loss: 12280.777344\n",
      "    epoch          : 1173\n",
      "    loss           : 12264.443402023884\n",
      "    val_loss       : 12273.068670420435\n",
      "    val_log_likelihood: -12165.44764558778\n",
      "    val_log_marginal: -12173.629534098216\n",
      "Train Epoch: 1174 [256/118836 (0%)] Loss: 12247.914062\n",
      "Train Epoch: 1174 [33024/118836 (28%)] Loss: 12256.455078\n",
      "Train Epoch: 1174 [65792/118836 (55%)] Loss: 12271.856445\n",
      "Train Epoch: 1174 [98560/118836 (83%)] Loss: 12247.650391\n",
      "    epoch          : 1174\n",
      "    loss           : 12265.859262562035\n",
      "    val_loss       : 12267.22804416831\n",
      "    val_log_likelihood: -12168.04160705516\n",
      "    val_log_marginal: -12176.16703316198\n",
      "Train Epoch: 1175 [256/118836 (0%)] Loss: 12487.722656\n",
      "Train Epoch: 1175 [33024/118836 (28%)] Loss: 12321.403320\n",
      "Train Epoch: 1175 [65792/118836 (55%)] Loss: 12219.688477\n",
      "Train Epoch: 1175 [98560/118836 (83%)] Loss: 12331.740234\n",
      "    epoch          : 1175\n",
      "    loss           : 12269.863846347705\n",
      "    val_loss       : 12268.101016133332\n",
      "    val_log_likelihood: -12165.488102577026\n",
      "    val_log_marginal: -12173.544038907383\n",
      "Train Epoch: 1176 [256/118836 (0%)] Loss: 12228.745117\n",
      "Train Epoch: 1176 [33024/118836 (28%)] Loss: 12212.558594\n",
      "Train Epoch: 1176 [65792/118836 (55%)] Loss: 12399.799805\n",
      "Train Epoch: 1176 [98560/118836 (83%)] Loss: 12366.990234\n",
      "    epoch          : 1176\n",
      "    loss           : 12269.097690175247\n",
      "    val_loss       : 12270.109534653464\n",
      "    val_log_likelihood: -12166.309291963917\n",
      "    val_log_marginal: -12174.408356568138\n",
      "Train Epoch: 1177 [256/118836 (0%)] Loss: 12221.390625\n",
      "Train Epoch: 1177 [33024/118836 (28%)] Loss: 12213.771484\n",
      "Train Epoch: 1177 [65792/118836 (55%)] Loss: 12409.081055\n",
      "Train Epoch: 1177 [98560/118836 (83%)] Loss: 12349.684570\n",
      "    epoch          : 1177\n",
      "    loss           : 12268.451176721464\n",
      "    val_loss       : 12269.541870220137\n",
      "    val_log_likelihood: -12169.146169354839\n",
      "    val_log_marginal: -12177.262280257082\n",
      "Train Epoch: 1178 [256/118836 (0%)] Loss: 12229.986328\n",
      "Train Epoch: 1178 [33024/118836 (28%)] Loss: 12295.326172\n",
      "Train Epoch: 1178 [65792/118836 (55%)] Loss: 12250.978516\n",
      "Train Epoch: 1178 [98560/118836 (83%)] Loss: 12252.340820\n",
      "    epoch          : 1178\n",
      "    loss           : 12265.46849539909\n",
      "    val_loss       : 12267.015843913712\n",
      "    val_log_likelihood: -12166.96279224178\n",
      "    val_log_marginal: -12175.163646469424\n",
      "Train Epoch: 1179 [256/118836 (0%)] Loss: 12227.187500\n",
      "Train Epoch: 1179 [33024/118836 (28%)] Loss: 12217.519531\n",
      "Train Epoch: 1179 [65792/118836 (55%)] Loss: 12249.731445\n",
      "Train Epoch: 1179 [98560/118836 (83%)] Loss: 12351.056641\n",
      "    epoch          : 1179\n",
      "    loss           : 12269.645075992556\n",
      "    val_loss       : 12264.93988021872\n",
      "    val_log_likelihood: -12165.335520865645\n",
      "    val_log_marginal: -12173.454725718106\n",
      "Train Epoch: 1180 [256/118836 (0%)] Loss: 12384.119141\n",
      "Train Epoch: 1180 [33024/118836 (28%)] Loss: 12323.620117\n",
      "Train Epoch: 1180 [65792/118836 (55%)] Loss: 12256.618164\n",
      "Train Epoch: 1180 [98560/118836 (83%)] Loss: 12218.003906\n",
      "    epoch          : 1180\n",
      "    loss           : 12270.492384427987\n",
      "    val_loss       : 12267.974864488691\n",
      "    val_log_likelihood: -12166.45510623449\n",
      "    val_log_marginal: -12174.654356372777\n",
      "Train Epoch: 1181 [256/118836 (0%)] Loss: 12199.191406\n",
      "Train Epoch: 1181 [33024/118836 (28%)] Loss: 12284.230469\n",
      "Train Epoch: 1181 [65792/118836 (55%)] Loss: 12291.731445\n",
      "Train Epoch: 1181 [98560/118836 (83%)] Loss: 12317.231445\n",
      "    epoch          : 1181\n",
      "    loss           : 12261.989349733767\n",
      "    val_loss       : 12268.878853458871\n",
      "    val_log_likelihood: -12166.580953073306\n",
      "    val_log_marginal: -12174.794372772407\n",
      "Train Epoch: 1182 [256/118836 (0%)] Loss: 12170.909180\n",
      "Train Epoch: 1182 [33024/118836 (28%)] Loss: 12277.789062\n",
      "Train Epoch: 1182 [65792/118836 (55%)] Loss: 12287.150391\n",
      "Train Epoch: 1182 [98560/118836 (83%)] Loss: 12346.865234\n",
      "    epoch          : 1182\n",
      "    loss           : 12265.676555230304\n",
      "    val_loss       : 12269.901570694918\n",
      "    val_log_likelihood: -12167.175733270005\n",
      "    val_log_marginal: -12175.154926983485\n",
      "Train Epoch: 1183 [256/118836 (0%)] Loss: 12239.494141\n",
      "Train Epoch: 1183 [33024/118836 (28%)] Loss: 12245.648438\n",
      "Train Epoch: 1183 [65792/118836 (55%)] Loss: 12206.455078\n",
      "Train Epoch: 1183 [98560/118836 (83%)] Loss: 12281.347656\n",
      "    epoch          : 1183\n",
      "    loss           : 12263.612688850548\n",
      "    val_loss       : 12270.848727666702\n",
      "    val_log_likelihood: -12175.003347775797\n",
      "    val_log_marginal: -12183.447740162619\n",
      "Train Epoch: 1184 [256/118836 (0%)] Loss: 12208.333984\n",
      "Train Epoch: 1184 [33024/118836 (28%)] Loss: 12282.197266\n",
      "Train Epoch: 1184 [65792/118836 (55%)] Loss: 12320.813477\n",
      "Train Epoch: 1184 [98560/118836 (83%)] Loss: 12290.357422\n",
      "    epoch          : 1184\n",
      "    loss           : 12269.016685567876\n",
      "    val_loss       : 12267.081002780224\n",
      "    val_log_likelihood: -12167.115980892007\n",
      "    val_log_marginal: -12175.176195154105\n",
      "Train Epoch: 1185 [256/118836 (0%)] Loss: 12176.099609\n",
      "Train Epoch: 1185 [33024/118836 (28%)] Loss: 12317.117188\n",
      "Train Epoch: 1185 [65792/118836 (55%)] Loss: 12329.508789\n",
      "Train Epoch: 1185 [98560/118836 (83%)] Loss: 12180.328125\n",
      "    epoch          : 1185\n",
      "    loss           : 12266.715037931657\n",
      "    val_loss       : 12265.879501811223\n",
      "    val_log_likelihood: -12165.25810344939\n",
      "    val_log_marginal: -12173.218328931682\n",
      "Train Epoch: 1186 [256/118836 (0%)] Loss: 12284.807617\n",
      "Train Epoch: 1186 [33024/118836 (28%)] Loss: 12346.753906\n",
      "Train Epoch: 1186 [65792/118836 (55%)] Loss: 12291.699219\n",
      "Train Epoch: 1186 [98560/118836 (83%)] Loss: 12341.275391\n",
      "    epoch          : 1186\n",
      "    loss           : 12265.247106984078\n",
      "    val_loss       : 12269.60357510709\n",
      "    val_log_likelihood: -12168.401466055366\n",
      "    val_log_marginal: -12176.381724605646\n",
      "Train Epoch: 1187 [256/118836 (0%)] Loss: 12308.520508\n",
      "Train Epoch: 1187 [33024/118836 (28%)] Loss: 12221.321289\n",
      "Train Epoch: 1187 [65792/118836 (55%)] Loss: 12215.480469\n",
      "Train Epoch: 1187 [98560/118836 (83%)] Loss: 12251.126953\n",
      "    epoch          : 1187\n",
      "    loss           : 12268.301327608044\n",
      "    val_loss       : 12270.528516365886\n",
      "    val_log_likelihood: -12167.128512070927\n",
      "    val_log_marginal: -12175.368403669641\n",
      "Train Epoch: 1188 [256/118836 (0%)] Loss: 12233.604492\n",
      "Train Epoch: 1188 [33024/118836 (28%)] Loss: 12216.556641\n",
      "Train Epoch: 1188 [65792/118836 (55%)] Loss: 12249.455078\n",
      "Train Epoch: 1188 [98560/118836 (83%)] Loss: 12325.187500\n",
      "    epoch          : 1188\n",
      "    loss           : 12265.621925726322\n",
      "    val_loss       : 12263.091503834157\n",
      "    val_log_likelihood: -12165.952461842173\n",
      "    val_log_marginal: -12174.127248180468\n",
      "Train Epoch: 1189 [256/118836 (0%)] Loss: 12154.507812\n",
      "Train Epoch: 1189 [33024/118836 (28%)] Loss: 12328.125977\n",
      "Train Epoch: 1189 [65792/118836 (55%)] Loss: 12258.961914\n",
      "Train Epoch: 1189 [98560/118836 (83%)] Loss: 12237.626953\n",
      "    epoch          : 1189\n",
      "    loss           : 12264.188967509306\n",
      "    val_loss       : 12266.959570155546\n",
      "    val_log_likelihood: -12164.613475431657\n",
      "    val_log_marginal: -12172.60646389844\n",
      "Train Epoch: 1190 [256/118836 (0%)] Loss: 12197.589844\n",
      "Train Epoch: 1190 [33024/118836 (28%)] Loss: 12268.338867\n",
      "Train Epoch: 1190 [65792/118836 (55%)] Loss: 12423.821289\n",
      "Train Epoch: 1190 [98560/118836 (83%)] Loss: 12324.759766\n",
      "    epoch          : 1190\n",
      "    loss           : 12265.662242975859\n",
      "    val_loss       : 12266.005261353863\n",
      "    val_log_likelihood: -12164.32057663229\n",
      "    val_log_marginal: -12172.414851998146\n",
      "Train Epoch: 1191 [256/118836 (0%)] Loss: 12400.699219\n",
      "Train Epoch: 1191 [33024/118836 (28%)] Loss: 12241.509766\n",
      "Train Epoch: 1191 [65792/118836 (55%)] Loss: 12275.568359\n",
      "Train Epoch: 1191 [98560/118836 (83%)] Loss: 12262.604492\n",
      "    epoch          : 1191\n",
      "    loss           : 12268.377954727563\n",
      "    val_loss       : 12262.086143197721\n",
      "    val_log_likelihood: -12166.118286031846\n",
      "    val_log_marginal: -12174.221673609327\n",
      "Train Epoch: 1192 [256/118836 (0%)] Loss: 12242.254883\n",
      "Train Epoch: 1192 [33024/118836 (28%)] Loss: 12236.412109\n",
      "Train Epoch: 1192 [65792/118836 (55%)] Loss: 12399.926758\n",
      "Train Epoch: 1192 [98560/118836 (83%)] Loss: 12322.272461\n",
      "    epoch          : 1192\n",
      "    loss           : 12271.06179968595\n",
      "    val_loss       : 12268.932672243307\n",
      "    val_log_likelihood: -12167.21846971283\n",
      "    val_log_marginal: -12175.30661716251\n",
      "Train Epoch: 1193 [256/118836 (0%)] Loss: 12195.519531\n",
      "Train Epoch: 1193 [33024/118836 (28%)] Loss: 12259.832031\n",
      "Train Epoch: 1193 [65792/118836 (55%)] Loss: 12344.421875\n",
      "Train Epoch: 1193 [98560/118836 (83%)] Loss: 12278.889648\n",
      "    epoch          : 1193\n",
      "    loss           : 12270.250847808105\n",
      "    val_loss       : 12262.152830744895\n",
      "    val_log_likelihood: -12171.810243324804\n",
      "    val_log_marginal: -12179.899226005804\n",
      "Train Epoch: 1194 [256/118836 (0%)] Loss: 12291.687500\n",
      "Train Epoch: 1194 [33024/118836 (28%)] Loss: 12259.721680\n",
      "Train Epoch: 1194 [65792/118836 (55%)] Loss: 12330.639648\n",
      "Train Epoch: 1194 [98560/118836 (83%)] Loss: 12365.050781\n",
      "    epoch          : 1194\n",
      "    loss           : 12269.688096438173\n",
      "    val_loss       : 12271.237488962326\n",
      "    val_log_likelihood: -12168.180230142421\n",
      "    val_log_marginal: -12176.172580727705\n",
      "Train Epoch: 1195 [256/118836 (0%)] Loss: 12266.958008\n",
      "Train Epoch: 1195 [33024/118836 (28%)] Loss: 12164.256836\n",
      "Train Epoch: 1195 [65792/118836 (55%)] Loss: 12266.119141\n",
      "Train Epoch: 1195 [98560/118836 (83%)] Loss: 12407.875000\n",
      "    epoch          : 1195\n",
      "    loss           : 12266.650723092433\n",
      "    val_loss       : 12275.038348308182\n",
      "    val_log_likelihood: -12166.45728730485\n",
      "    val_log_marginal: -12174.740398986696\n",
      "Train Epoch: 1196 [256/118836 (0%)] Loss: 12254.708984\n",
      "Train Epoch: 1196 [33024/118836 (28%)] Loss: 12187.309570\n",
      "Train Epoch: 1196 [65792/118836 (55%)] Loss: 12408.121094\n",
      "Train Epoch: 1196 [98560/118836 (83%)] Loss: 12277.635742\n",
      "    epoch          : 1196\n",
      "    loss           : 12274.67771563534\n",
      "    val_loss       : 12267.346337837185\n",
      "    val_log_likelihood: -12166.417853404157\n",
      "    val_log_marginal: -12174.564922280044\n",
      "Train Epoch: 1197 [256/118836 (0%)] Loss: 12295.625000\n",
      "Train Epoch: 1197 [33024/118836 (28%)] Loss: 12338.048828\n",
      "Train Epoch: 1197 [65792/118836 (55%)] Loss: 12261.529297\n",
      "Train Epoch: 1197 [98560/118836 (83%)] Loss: 12333.294922\n",
      "    epoch          : 1197\n",
      "    loss           : 12266.482662259616\n",
      "    val_loss       : 12269.678326221803\n",
      "    val_log_likelihood: -12167.069500555726\n",
      "    val_log_marginal: -12175.017492762052\n",
      "Train Epoch: 1198 [256/118836 (0%)] Loss: 12355.428711\n",
      "Train Epoch: 1198 [33024/118836 (28%)] Loss: 12232.676758\n",
      "Train Epoch: 1198 [65792/118836 (55%)] Loss: 12323.740234\n",
      "Train Epoch: 1198 [98560/118836 (83%)] Loss: 12236.343750\n",
      "    epoch          : 1198\n",
      "    loss           : 12267.072291795905\n",
      "    val_loss       : 12265.363329581347\n",
      "    val_log_likelihood: -12167.981134492607\n",
      "    val_log_marginal: -12176.266486013785\n",
      "Train Epoch: 1199 [256/118836 (0%)] Loss: 12288.996094\n",
      "Train Epoch: 1199 [33024/118836 (28%)] Loss: 12369.912109\n",
      "Train Epoch: 1199 [65792/118836 (55%)] Loss: 12381.443359\n",
      "Train Epoch: 1199 [98560/118836 (83%)] Loss: 12259.186523\n",
      "    epoch          : 1199\n",
      "    loss           : 12270.767925131824\n",
      "    val_loss       : 12267.10327297956\n",
      "    val_log_likelihood: -12166.694499586434\n",
      "    val_log_marginal: -12174.890102718375\n",
      "Train Epoch: 1200 [256/118836 (0%)] Loss: 12275.461914\n",
      "Train Epoch: 1200 [33024/118836 (28%)] Loss: 12214.681641\n",
      "Train Epoch: 1200 [65792/118836 (55%)] Loss: 12268.234375\n",
      "Train Epoch: 1200 [98560/118836 (83%)] Loss: 12332.724609\n",
      "    epoch          : 1200\n",
      "    loss           : 12271.607268080543\n",
      "    val_loss       : 12272.896482266186\n",
      "    val_log_likelihood: -12166.84374725367\n",
      "    val_log_marginal: -12175.123512431108\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch1200.pth ...\n",
      "Train Epoch: 1201 [256/118836 (0%)] Loss: 12289.029297\n",
      "Train Epoch: 1201 [33024/118836 (28%)] Loss: 12418.512695\n",
      "Train Epoch: 1201 [65792/118836 (55%)] Loss: 12283.767578\n",
      "Train Epoch: 1201 [98560/118836 (83%)] Loss: 12306.960938\n",
      "    epoch          : 1201\n",
      "    loss           : 12269.582195222034\n",
      "    val_loss       : 12269.024495228457\n",
      "    val_log_likelihood: -12168.304636127481\n",
      "    val_log_marginal: -12176.28143457773\n",
      "Train Epoch: 1202 [256/118836 (0%)] Loss: 12244.778320\n",
      "Train Epoch: 1202 [33024/118836 (28%)] Loss: 12309.958984\n",
      "Train Epoch: 1202 [65792/118836 (55%)] Loss: 12296.089844\n",
      "Train Epoch: 1202 [98560/118836 (83%)] Loss: 12198.630859\n",
      "    epoch          : 1202\n",
      "    loss           : 12267.321661594033\n",
      "    val_loss       : 12268.155278071896\n",
      "    val_log_likelihood: -12167.82389145213\n",
      "    val_log_marginal: -12175.79267384444\n",
      "Train Epoch: 1203 [256/118836 (0%)] Loss: 12190.510742\n",
      "Train Epoch: 1203 [33024/118836 (28%)] Loss: 12171.306641\n",
      "Train Epoch: 1203 [65792/118836 (55%)] Loss: 12357.163086\n",
      "Train Epoch: 1203 [98560/118836 (83%)] Loss: 12296.548828\n",
      "    epoch          : 1203\n",
      "    loss           : 12266.391109323305\n",
      "    val_loss       : 12264.52867631168\n",
      "    val_log_likelihood: -12167.196342696701\n",
      "    val_log_marginal: -12175.237633535822\n",
      "Train Epoch: 1204 [256/118836 (0%)] Loss: 12239.449219\n",
      "Train Epoch: 1204 [33024/118836 (28%)] Loss: 12278.341797\n",
      "Train Epoch: 1204 [65792/118836 (55%)] Loss: 12267.699219\n",
      "Train Epoch: 1204 [98560/118836 (83%)] Loss: 12187.354492\n",
      "    epoch          : 1204\n",
      "    loss           : 12266.090634046734\n",
      "    val_loss       : 12266.548804693288\n",
      "    val_log_likelihood: -12167.454303336952\n",
      "    val_log_marginal: -12175.565733231706\n",
      "Train Epoch: 1205 [256/118836 (0%)] Loss: 12224.206055\n",
      "Train Epoch: 1205 [33024/118836 (28%)] Loss: 12212.563477\n",
      "Train Epoch: 1205 [65792/118836 (55%)] Loss: 12204.932617\n",
      "Train Epoch: 1205 [98560/118836 (83%)] Loss: 12291.198242\n",
      "    epoch          : 1205\n",
      "    loss           : 12266.037713244417\n",
      "    val_loss       : 12265.236239392463\n",
      "    val_log_likelihood: -12168.103593168424\n",
      "    val_log_marginal: -12176.259503370653\n",
      "Train Epoch: 1206 [256/118836 (0%)] Loss: 12244.948242\n",
      "Train Epoch: 1206 [33024/118836 (28%)] Loss: 12339.531250\n",
      "Train Epoch: 1206 [65792/118836 (55%)] Loss: 12140.462891\n",
      "Train Epoch: 1206 [98560/118836 (83%)] Loss: 12404.050781\n",
      "    epoch          : 1206\n",
      "    loss           : 12264.636463018249\n",
      "    val_loss       : 12268.299577315938\n",
      "    val_log_likelihood: -12166.512314380429\n",
      "    val_log_marginal: -12174.456849550195\n",
      "Train Epoch: 1207 [256/118836 (0%)] Loss: 12287.607422\n",
      "Train Epoch: 1207 [33024/118836 (28%)] Loss: 12238.508789\n",
      "Train Epoch: 1207 [65792/118836 (55%)] Loss: 12338.125000\n",
      "Train Epoch: 1207 [98560/118836 (83%)] Loss: 12413.273438\n",
      "    epoch          : 1207\n",
      "    loss           : 12266.641355685224\n",
      "    val_loss       : 12268.555757179376\n",
      "    val_log_likelihood: -12168.351801753774\n",
      "    val_log_marginal: -12176.452572976425\n",
      "Train Epoch: 1208 [256/118836 (0%)] Loss: 12303.603516\n",
      "Train Epoch: 1208 [33024/118836 (28%)] Loss: 12291.280273\n",
      "Train Epoch: 1208 [65792/118836 (55%)] Loss: 12305.056641\n",
      "Train Epoch: 1208 [98560/118836 (83%)] Loss: 12446.862305\n",
      "    epoch          : 1208\n",
      "    loss           : 12269.617389274452\n",
      "    val_loss       : 12267.329451122374\n",
      "    val_log_likelihood: -12166.069544820099\n",
      "    val_log_marginal: -12174.059639559924\n",
      "Train Epoch: 1209 [256/118836 (0%)] Loss: 12314.988281\n",
      "Train Epoch: 1209 [33024/118836 (28%)] Loss: 12315.360352\n",
      "Train Epoch: 1209 [65792/118836 (55%)] Loss: 12255.902344\n",
      "Train Epoch: 1209 [98560/118836 (83%)] Loss: 12329.019531\n",
      "    epoch          : 1209\n",
      "    loss           : 12271.08846444634\n",
      "    val_loss       : 12272.065323030569\n",
      "    val_log_likelihood: -12167.078464252481\n",
      "    val_log_marginal: -12175.309637430835\n",
      "Train Epoch: 1210 [256/118836 (0%)] Loss: 12308.949219\n",
      "Train Epoch: 1210 [33024/118836 (28%)] Loss: 12190.414062\n",
      "Train Epoch: 1210 [65792/118836 (55%)] Loss: 12189.210938\n",
      "Train Epoch: 1210 [98560/118836 (83%)] Loss: 12255.289062\n",
      "    epoch          : 1210\n",
      "    loss           : 12264.66178740824\n",
      "    val_loss       : 12267.261313596327\n",
      "    val_log_likelihood: -12168.700914689309\n",
      "    val_log_marginal: -12176.7396782775\n",
      "Train Epoch: 1211 [256/118836 (0%)] Loss: 12279.199219\n",
      "Train Epoch: 1211 [33024/118836 (28%)] Loss: 12421.763672\n",
      "Train Epoch: 1211 [65792/118836 (55%)] Loss: 12422.638672\n",
      "Train Epoch: 1211 [98560/118836 (83%)] Loss: 12204.324219\n",
      "    epoch          : 1211\n",
      "    loss           : 12270.230378282671\n",
      "    val_loss       : 12268.637876564986\n",
      "    val_log_likelihood: -12164.532351762822\n",
      "    val_log_marginal: -12172.491073194205\n",
      "Train Epoch: 1212 [256/118836 (0%)] Loss: 12399.299805\n",
      "Train Epoch: 1212 [33024/118836 (28%)] Loss: 12212.912109\n",
      "Train Epoch: 1212 [65792/118836 (55%)] Loss: 12284.900391\n",
      "Train Epoch: 1212 [98560/118836 (83%)] Loss: 12282.974609\n",
      "    epoch          : 1212\n",
      "    loss           : 12265.10289188508\n",
      "    val_loss       : 12266.784508839097\n",
      "    val_log_likelihood: -12164.94865413694\n",
      "    val_log_marginal: -12172.969058964942\n",
      "Train Epoch: 1213 [256/118836 (0%)] Loss: 12384.641602\n",
      "Train Epoch: 1213 [33024/118836 (28%)] Loss: 12343.291016\n",
      "Train Epoch: 1213 [65792/118836 (55%)] Loss: 12307.113281\n",
      "Train Epoch: 1213 [98560/118836 (83%)] Loss: 12272.046875\n",
      "    epoch          : 1213\n",
      "    loss           : 12267.68864360396\n",
      "    val_loss       : 12272.296231688402\n",
      "    val_log_likelihood: -12167.531848376757\n",
      "    val_log_marginal: -12175.525077825041\n",
      "Train Epoch: 1214 [256/118836 (0%)] Loss: 12170.354492\n",
      "Train Epoch: 1214 [33024/118836 (28%)] Loss: 12327.961914\n",
      "Train Epoch: 1214 [65792/118836 (55%)] Loss: 12225.699219\n",
      "Train Epoch: 1214 [98560/118836 (83%)] Loss: 12255.978516\n",
      "    epoch          : 1214\n",
      "    loss           : 12270.332196999068\n",
      "    val_loss       : 12262.245587979774\n",
      "    val_log_likelihood: -12167.727680256152\n",
      "    val_log_marginal: -12175.890931114498\n",
      "Train Epoch: 1215 [256/118836 (0%)] Loss: 12261.912109\n",
      "Train Epoch: 1215 [33024/118836 (28%)] Loss: 12247.248047\n",
      "Train Epoch: 1215 [65792/118836 (55%)] Loss: 12237.597656\n",
      "Train Epoch: 1215 [98560/118836 (83%)] Loss: 12312.385742\n",
      "    epoch          : 1215\n",
      "    loss           : 12268.289508697788\n",
      "    val_loss       : 12266.741876367456\n",
      "    val_log_likelihood: -12168.484707628979\n",
      "    val_log_marginal: -12176.4377935605\n",
      "Train Epoch: 1216 [256/118836 (0%)] Loss: 12215.703125\n",
      "Train Epoch: 1216 [33024/118836 (28%)] Loss: 12408.149414\n",
      "Train Epoch: 1216 [65792/118836 (55%)] Loss: 12181.162109\n",
      "Train Epoch: 1216 [98560/118836 (83%)] Loss: 12325.750000\n",
      "    epoch          : 1216\n",
      "    loss           : 12270.959666110939\n",
      "    val_loss       : 12265.80859395723\n",
      "    val_log_likelihood: -12168.690971683727\n",
      "    val_log_marginal: -12176.745644905166\n",
      "Train Epoch: 1217 [256/118836 (0%)] Loss: 12332.604492\n",
      "Train Epoch: 1217 [33024/118836 (28%)] Loss: 12229.312500\n",
      "Train Epoch: 1217 [65792/118836 (55%)] Loss: 12417.335938\n",
      "Train Epoch: 1217 [98560/118836 (83%)] Loss: 12378.013672\n",
      "    epoch          : 1217\n",
      "    loss           : 12272.040200126654\n",
      "    val_loss       : 12269.060699171401\n",
      "    val_log_likelihood: -12166.530427555055\n",
      "    val_log_marginal: -12174.691843030161\n",
      "Train Epoch: 1218 [256/118836 (0%)] Loss: 12438.330078\n",
      "Train Epoch: 1218 [33024/118836 (28%)] Loss: 12358.081055\n",
      "Train Epoch: 1218 [65792/118836 (55%)] Loss: 12337.595703\n",
      "Train Epoch: 1218 [98560/118836 (83%)] Loss: 12318.440430\n",
      "    epoch          : 1218\n",
      "    loss           : 12265.757194091191\n",
      "    val_loss       : 12268.265160626592\n",
      "    val_log_likelihood: -12170.007574538617\n",
      "    val_log_marginal: -12177.963909901298\n",
      "Train Epoch: 1219 [256/118836 (0%)] Loss: 12333.681641\n",
      "Train Epoch: 1219 [33024/118836 (28%)] Loss: 12235.614258\n",
      "Train Epoch: 1219 [65792/118836 (55%)] Loss: 12302.956055\n",
      "Train Epoch: 1219 [98560/118836 (83%)] Loss: 12264.322266\n",
      "    epoch          : 1219\n",
      "    loss           : 12269.27486930702\n",
      "    val_loss       : 12269.727471379367\n",
      "    val_log_likelihood: -12170.38643846283\n",
      "    val_log_marginal: -12178.91748232038\n",
      "Train Epoch: 1220 [256/118836 (0%)] Loss: 12353.125977\n",
      "Train Epoch: 1220 [33024/118836 (28%)] Loss: 12260.833984\n",
      "Train Epoch: 1220 [65792/118836 (55%)] Loss: 12304.706055\n",
      "Train Epoch: 1220 [98560/118836 (83%)] Loss: 12397.812500\n",
      "    epoch          : 1220\n",
      "    loss           : 12270.64392366496\n",
      "    val_loss       : 12265.071685150286\n",
      "    val_log_likelihood: -12165.871524439102\n",
      "    val_log_marginal: -12173.899483656242\n",
      "Train Epoch: 1221 [256/118836 (0%)] Loss: 12173.876953\n",
      "Train Epoch: 1221 [33024/118836 (28%)] Loss: 12384.041016\n",
      "Train Epoch: 1221 [65792/118836 (55%)] Loss: 12248.025391\n",
      "Train Epoch: 1221 [98560/118836 (83%)] Loss: 12316.560547\n",
      "    epoch          : 1221\n",
      "    loss           : 12267.466026287222\n",
      "    val_loss       : 12267.563284462693\n",
      "    val_log_likelihood: -12169.083948188068\n",
      "    val_log_marginal: -12177.179737681168\n",
      "Train Epoch: 1222 [256/118836 (0%)] Loss: 12230.234375\n",
      "Train Epoch: 1222 [33024/118836 (28%)] Loss: 12300.182617\n",
      "Train Epoch: 1222 [65792/118836 (55%)] Loss: 12223.705078\n",
      "Train Epoch: 1222 [98560/118836 (83%)] Loss: 12210.022461\n",
      "    epoch          : 1222\n",
      "    loss           : 12267.113630680054\n",
      "    val_loss       : 12266.881770137954\n",
      "    val_log_likelihood: -12167.197268855976\n",
      "    val_log_marginal: -12175.270359704093\n",
      "Train Epoch: 1223 [256/118836 (0%)] Loss: 12358.296875\n",
      "Train Epoch: 1223 [33024/118836 (28%)] Loss: 12314.270508\n",
      "Train Epoch: 1223 [65792/118836 (55%)] Loss: 12263.630859\n",
      "Train Epoch: 1223 [98560/118836 (83%)] Loss: 12318.945312\n",
      "    epoch          : 1223\n",
      "    loss           : 12268.321404569893\n",
      "    val_loss       : 12266.37192483082\n",
      "    val_log_likelihood: -12169.680346457559\n",
      "    val_log_marginal: -12177.746119991534\n",
      "Train Epoch: 1224 [256/118836 (0%)] Loss: 12290.642578\n",
      "Train Epoch: 1224 [33024/118836 (28%)] Loss: 12294.644531\n",
      "Train Epoch: 1224 [65792/118836 (55%)] Loss: 12175.906250\n",
      "Train Epoch: 1224 [98560/118836 (83%)] Loss: 12256.724609\n",
      "    epoch          : 1224\n",
      "    loss           : 12269.58145064361\n",
      "    val_loss       : 12268.27407237114\n",
      "    val_log_likelihood: -12171.17841174912\n",
      "    val_log_marginal: -12179.297213203794\n",
      "Train Epoch: 1225 [256/118836 (0%)] Loss: 12281.138672\n",
      "Train Epoch: 1225 [33024/118836 (28%)] Loss: 12240.362305\n",
      "Train Epoch: 1225 [65792/118836 (55%)] Loss: 12415.331055\n",
      "Train Epoch: 1225 [98560/118836 (83%)] Loss: 12273.147461\n",
      "    epoch          : 1225\n",
      "    loss           : 12272.416544374224\n",
      "    val_loss       : 12265.017469718648\n",
      "    val_log_likelihood: -12166.832116386218\n",
      "    val_log_marginal: -12174.933379871532\n",
      "Train Epoch: 1226 [256/118836 (0%)] Loss: 12373.759766\n",
      "Train Epoch: 1226 [33024/118836 (28%)] Loss: 12181.395508\n",
      "Train Epoch: 1226 [65792/118836 (55%)] Loss: 12496.189453\n",
      "Train Epoch: 1226 [98560/118836 (83%)] Loss: 12267.986328\n",
      "    epoch          : 1226\n",
      "    loss           : 12264.991226607734\n",
      "    val_loss       : 12269.15063748386\n",
      "    val_log_likelihood: -12170.631385054798\n",
      "    val_log_marginal: -12178.604300987523\n",
      "Train Epoch: 1227 [256/118836 (0%)] Loss: 12224.773438\n",
      "Train Epoch: 1227 [33024/118836 (28%)] Loss: 12421.718750\n",
      "Train Epoch: 1227 [65792/118836 (55%)] Loss: 12204.653320\n",
      "Train Epoch: 1227 [98560/118836 (83%)] Loss: 12255.621094\n",
      "    epoch          : 1227\n",
      "    loss           : 12270.205493466967\n",
      "    val_loss       : 12269.157016660147\n",
      "    val_log_likelihood: -12167.947736701302\n",
      "    val_log_marginal: -12176.216755556594\n",
      "Train Epoch: 1228 [256/118836 (0%)] Loss: 12360.923828\n",
      "Train Epoch: 1228 [33024/118836 (28%)] Loss: 12283.208008\n",
      "Train Epoch: 1228 [65792/118836 (55%)] Loss: 12244.503906\n",
      "Train Epoch: 1228 [98560/118836 (83%)] Loss: 12253.693359\n",
      "    epoch          : 1228\n",
      "    loss           : 12269.667205916563\n",
      "    val_loss       : 12266.953635211617\n",
      "    val_log_likelihood: -12169.016640172662\n",
      "    val_log_marginal: -12177.01349293371\n",
      "Train Epoch: 1229 [256/118836 (0%)] Loss: 12224.371094\n",
      "Train Epoch: 1229 [33024/118836 (28%)] Loss: 12202.701172\n",
      "Train Epoch: 1229 [65792/118836 (55%)] Loss: 12455.701172\n",
      "Train Epoch: 1229 [98560/118836 (83%)] Loss: 12360.294922\n",
      "    epoch          : 1229\n",
      "    loss           : 12268.719236746536\n",
      "    val_loss       : 12271.271165701202\n",
      "    val_log_likelihood: -12168.707649820359\n",
      "    val_log_marginal: -12177.108727803212\n",
      "Train Epoch: 1230 [256/118836 (0%)] Loss: 12336.463867\n",
      "Train Epoch: 1230 [33024/118836 (28%)] Loss: 12254.540039\n",
      "Train Epoch: 1230 [65792/118836 (55%)] Loss: 12343.084961\n",
      "Train Epoch: 1230 [98560/118836 (83%)] Loss: 12301.595703\n",
      "    epoch          : 1230\n",
      "    loss           : 12269.116791220793\n",
      "    val_loss       : 12264.034284813186\n",
      "    val_log_likelihood: -12167.425841830802\n",
      "    val_log_marginal: -12175.66947075004\n",
      "Train Epoch: 1231 [256/118836 (0%)] Loss: 12329.392578\n",
      "Train Epoch: 1231 [33024/118836 (28%)] Loss: 12272.794922\n",
      "Train Epoch: 1231 [65792/118836 (55%)] Loss: 12291.308594\n",
      "Train Epoch: 1231 [98560/118836 (83%)] Loss: 12239.726562\n",
      "    epoch          : 1231\n",
      "    loss           : 12270.058111688379\n",
      "    val_loss       : 12267.110438318763\n",
      "    val_log_likelihood: -12166.608638660566\n",
      "    val_log_marginal: -12174.691558834515\n",
      "Train Epoch: 1232 [256/118836 (0%)] Loss: 12296.815430\n",
      "Train Epoch: 1232 [33024/118836 (28%)] Loss: 12194.738281\n",
      "Train Epoch: 1232 [65792/118836 (55%)] Loss: 12257.272461\n",
      "Train Epoch: 1232 [98560/118836 (83%)] Loss: 12336.185547\n",
      "    epoch          : 1232\n",
      "    loss           : 12274.152274930211\n",
      "    val_loss       : 12269.169360039321\n",
      "    val_log_likelihood: -12167.452559902295\n",
      "    val_log_marginal: -12175.517089049743\n",
      "Train Epoch: 1233 [256/118836 (0%)] Loss: 12355.708984\n",
      "Train Epoch: 1233 [33024/118836 (28%)] Loss: 12289.572266\n",
      "Train Epoch: 1233 [65792/118836 (55%)] Loss: 12219.659180\n",
      "Train Epoch: 1233 [98560/118836 (83%)] Loss: 12178.248047\n",
      "    epoch          : 1233\n",
      "    loss           : 12263.006356137563\n",
      "    val_loss       : 12276.599899346182\n",
      "    val_log_likelihood: -12168.91354522074\n",
      "    val_log_marginal: -12176.874756624062\n",
      "Train Epoch: 1234 [256/118836 (0%)] Loss: 12353.287109\n",
      "Train Epoch: 1234 [33024/118836 (28%)] Loss: 12377.190430\n",
      "Train Epoch: 1234 [65792/118836 (55%)] Loss: 12328.226562\n",
      "Train Epoch: 1234 [98560/118836 (83%)] Loss: 12199.637695\n",
      "    epoch          : 1234\n",
      "    loss           : 12272.906572451406\n",
      "    val_loss       : 12268.175306507828\n",
      "    val_log_likelihood: -12172.729103339538\n",
      "    val_log_marginal: -12180.773984121643\n",
      "Train Epoch: 1235 [256/118836 (0%)] Loss: 12303.622070\n",
      "Train Epoch: 1235 [33024/118836 (28%)] Loss: 12280.199219\n",
      "Train Epoch: 1235 [65792/118836 (55%)] Loss: 12369.312500\n",
      "Train Epoch: 1235 [98560/118836 (83%)] Loss: 12253.246094\n",
      "    epoch          : 1235\n",
      "    loss           : 12268.208778561828\n",
      "    val_loss       : 12262.538909654377\n",
      "    val_log_likelihood: -12168.625156540787\n",
      "    val_log_marginal: -12176.724939475964\n",
      "Train Epoch: 1236 [256/118836 (0%)] Loss: 12179.016602\n",
      "Train Epoch: 1236 [33024/118836 (28%)] Loss: 12181.570312\n",
      "Train Epoch: 1236 [65792/118836 (55%)] Loss: 12248.798828\n",
      "Train Epoch: 1236 [98560/118836 (83%)] Loss: 12256.486328\n",
      "    epoch          : 1236\n",
      "    loss           : 12267.638943276985\n",
      "    val_loss       : 12261.810674656268\n",
      "    val_log_likelihood: -12166.459337682227\n",
      "    val_log_marginal: -12174.405492319276\n",
      "Train Epoch: 1237 [256/118836 (0%)] Loss: 12357.527344\n",
      "Train Epoch: 1237 [33024/118836 (28%)] Loss: 12327.120117\n",
      "Train Epoch: 1237 [65792/118836 (55%)] Loss: 12322.189453\n",
      "Train Epoch: 1237 [98560/118836 (83%)] Loss: 12380.940430\n",
      "    epoch          : 1237\n",
      "    loss           : 12267.13391313198\n",
      "    val_loss       : 12266.340313098868\n",
      "    val_log_likelihood: -12168.905103165063\n",
      "    val_log_marginal: -12176.909689872726\n",
      "Train Epoch: 1238 [256/118836 (0%)] Loss: 12314.339844\n",
      "Train Epoch: 1238 [33024/118836 (28%)] Loss: 12299.492188\n",
      "Train Epoch: 1238 [65792/118836 (55%)] Loss: 12305.754883\n",
      "Train Epoch: 1238 [98560/118836 (83%)] Loss: 12277.876953\n",
      "    epoch          : 1238\n",
      "    loss           : 12266.471995030759\n",
      "    val_loss       : 12265.123491881834\n",
      "    val_log_likelihood: -12164.403635978857\n",
      "    val_log_marginal: -12172.500208523437\n",
      "Train Epoch: 1239 [256/118836 (0%)] Loss: 12337.845703\n",
      "Train Epoch: 1239 [33024/118836 (28%)] Loss: 12390.800781\n",
      "Train Epoch: 1239 [65792/118836 (55%)] Loss: 12353.347656\n",
      "Train Epoch: 1239 [98560/118836 (83%)] Loss: 12184.772461\n",
      "    epoch          : 1239\n",
      "    loss           : 12270.92424265922\n",
      "    val_loss       : 12264.670890178326\n",
      "    val_log_likelihood: -12164.806944013648\n",
      "    val_log_marginal: -12172.89036333137\n",
      "Train Epoch: 1240 [256/118836 (0%)] Loss: 12308.541016\n",
      "Train Epoch: 1240 [33024/118836 (28%)] Loss: 12307.367188\n",
      "Train Epoch: 1240 [65792/118836 (55%)] Loss: 12275.600586\n",
      "Train Epoch: 1240 [98560/118836 (83%)] Loss: 12382.646484\n",
      "    epoch          : 1240\n",
      "    loss           : 12265.204971987438\n",
      "    val_loss       : 12267.987124972495\n",
      "    val_log_likelihood: -12172.249338942307\n",
      "    val_log_marginal: -12180.558262493149\n",
      "Train Epoch: 1241 [256/118836 (0%)] Loss: 12359.377930\n",
      "Train Epoch: 1241 [33024/118836 (28%)] Loss: 12271.028320\n",
      "Train Epoch: 1241 [65792/118836 (55%)] Loss: 12349.536133\n",
      "Train Epoch: 1241 [98560/118836 (83%)] Loss: 12267.102539\n",
      "    epoch          : 1241\n",
      "    loss           : 12261.324451057177\n",
      "    val_loss       : 12265.265607625966\n",
      "    val_log_likelihood: -12166.358528645833\n",
      "    val_log_marginal: -12174.526613166581\n",
      "Train Epoch: 1242 [256/118836 (0%)] Loss: 12339.047852\n",
      "Train Epoch: 1242 [33024/118836 (28%)] Loss: 12199.754883\n",
      "Train Epoch: 1242 [65792/118836 (55%)] Loss: 12318.953125\n",
      "Train Epoch: 1242 [98560/118836 (83%)] Loss: 12306.933594\n",
      "    epoch          : 1242\n",
      "    loss           : 12266.39082838994\n",
      "    val_loss       : 12267.94573178789\n",
      "    val_log_likelihood: -12170.793045324132\n",
      "    val_log_marginal: -12179.278698571603\n",
      "Train Epoch: 1243 [256/118836 (0%)] Loss: 12390.248047\n",
      "Train Epoch: 1243 [33024/118836 (28%)] Loss: 12391.468750\n",
      "Train Epoch: 1243 [65792/118836 (55%)] Loss: 12354.213867\n",
      "Train Epoch: 1243 [98560/118836 (83%)] Loss: 12296.778320\n",
      "    epoch          : 1243\n",
      "    loss           : 12268.638501602563\n",
      "    val_loss       : 12268.84653014947\n",
      "    val_log_likelihood: -12165.282036904207\n",
      "    val_log_marginal: -12173.44938074925\n",
      "Train Epoch: 1244 [256/118836 (0%)] Loss: 12297.727539\n",
      "Train Epoch: 1244 [33024/118836 (28%)] Loss: 12376.687500\n",
      "Train Epoch: 1244 [65792/118836 (55%)] Loss: 12260.453125\n",
      "Train Epoch: 1244 [98560/118836 (83%)] Loss: 12323.652344\n",
      "    epoch          : 1244\n",
      "    loss           : 12267.450579798646\n",
      "    val_loss       : 12268.73221252374\n",
      "    val_log_likelihood: -12170.851076561208\n",
      "    val_log_marginal: -12178.81451168958\n",
      "Train Epoch: 1245 [256/118836 (0%)] Loss: 12248.624023\n",
      "Train Epoch: 1245 [33024/118836 (28%)] Loss: 12213.301758\n",
      "Train Epoch: 1245 [65792/118836 (55%)] Loss: 12253.379883\n",
      "Train Epoch: 1245 [98560/118836 (83%)] Loss: 12375.674805\n",
      "    epoch          : 1245\n",
      "    loss           : 12267.586819071805\n",
      "    val_loss       : 12266.692682215395\n",
      "    val_log_likelihood: -12169.136515683158\n",
      "    val_log_marginal: -12177.26646946636\n",
      "Train Epoch: 1246 [256/118836 (0%)] Loss: 12252.925781\n",
      "Train Epoch: 1246 [33024/118836 (28%)] Loss: 12299.232422\n",
      "Train Epoch: 1246 [65792/118836 (55%)] Loss: 12202.944336\n",
      "Train Epoch: 1246 [98560/118836 (83%)] Loss: 12250.292969\n",
      "    epoch          : 1246\n",
      "    loss           : 12266.091399788047\n",
      "    val_loss       : 12270.821183646072\n",
      "    val_log_likelihood: -12167.838908382446\n",
      "    val_log_marginal: -12176.132404713915\n",
      "Train Epoch: 1247 [256/118836 (0%)] Loss: 12352.240234\n",
      "Train Epoch: 1247 [33024/118836 (28%)] Loss: 12306.660156\n",
      "Train Epoch: 1247 [65792/118836 (55%)] Loss: 12332.429688\n",
      "Train Epoch: 1247 [98560/118836 (83%)] Loss: 12300.093750\n",
      "    epoch          : 1247\n",
      "    loss           : 12269.608575494985\n",
      "    val_loss       : 12269.658364177225\n",
      "    val_log_likelihood: -12163.103845184552\n",
      "    val_log_marginal: -12171.192439249762\n",
      "Train Epoch: 1248 [256/118836 (0%)] Loss: 12345.815430\n",
      "Train Epoch: 1248 [33024/118836 (28%)] Loss: 12300.677734\n",
      "Train Epoch: 1248 [65792/118836 (55%)] Loss: 12325.970703\n",
      "Train Epoch: 1248 [98560/118836 (83%)] Loss: 12248.602539\n",
      "    epoch          : 1248\n",
      "    loss           : 12270.571908117505\n",
      "    val_loss       : 12270.915388232172\n",
      "    val_log_likelihood: -12168.149303886219\n",
      "    val_log_marginal: -12176.448795736795\n",
      "Train Epoch: 1249 [256/118836 (0%)] Loss: 12194.497070\n",
      "Train Epoch: 1249 [33024/118836 (28%)] Loss: 12258.446289\n",
      "Train Epoch: 1249 [65792/118836 (55%)] Loss: 12362.318359\n",
      "Train Epoch: 1249 [98560/118836 (83%)] Loss: 12372.057617\n",
      "    epoch          : 1249\n",
      "    loss           : 12267.951987857992\n",
      "    val_loss       : 12271.103672153939\n",
      "    val_log_likelihood: -12169.497883226066\n",
      "    val_log_marginal: -12177.81876776617\n",
      "Train Epoch: 1250 [256/118836 (0%)] Loss: 12243.918945\n",
      "Train Epoch: 1250 [33024/118836 (28%)] Loss: 12493.670898\n",
      "Train Epoch: 1250 [65792/118836 (55%)] Loss: 12465.828125\n",
      "Train Epoch: 1250 [98560/118836 (83%)] Loss: 12252.968750\n",
      "    epoch          : 1250\n",
      "    loss           : 12265.004994927367\n",
      "    val_loss       : 12261.663661659717\n",
      "    val_log_likelihood: -12165.225247492763\n",
      "    val_log_marginal: -12173.314295171034\n",
      "Train Epoch: 1251 [256/118836 (0%)] Loss: 12240.398438\n",
      "Train Epoch: 1251 [33024/118836 (28%)] Loss: 12231.433594\n",
      "Train Epoch: 1251 [65792/118836 (55%)] Loss: 12280.950195\n",
      "Train Epoch: 1251 [98560/118836 (83%)] Loss: 12276.111328\n",
      "    epoch          : 1251\n",
      "    loss           : 12265.60835514242\n",
      "    val_loss       : 12269.67767825554\n",
      "    val_log_likelihood: -12164.43036503567\n",
      "    val_log_marginal: -12172.427170746341\n",
      "Train Epoch: 1252 [256/118836 (0%)] Loss: 12364.986328\n",
      "Train Epoch: 1252 [33024/118836 (28%)] Loss: 12293.002930\n",
      "Train Epoch: 1252 [65792/118836 (55%)] Loss: 12337.009766\n",
      "Train Epoch: 1252 [98560/118836 (83%)] Loss: 12288.582031\n",
      "    epoch          : 1252\n",
      "    loss           : 12267.584574997414\n",
      "    val_loss       : 12263.773211048761\n",
      "    val_log_likelihood: -12168.068770355148\n",
      "    val_log_marginal: -12176.07125750482\n",
      "Train Epoch: 1253 [256/118836 (0%)] Loss: 12224.625977\n",
      "Train Epoch: 1253 [33024/118836 (28%)] Loss: 12306.962891\n",
      "Train Epoch: 1253 [65792/118836 (55%)] Loss: 12356.429688\n",
      "Train Epoch: 1253 [98560/118836 (83%)] Loss: 12383.360352\n",
      "    epoch          : 1253\n",
      "    loss           : 12266.180504129188\n",
      "    val_loss       : 12262.255965135693\n",
      "    val_log_likelihood: -12167.846762562034\n",
      "    val_log_marginal: -12176.080113989756\n",
      "Train Epoch: 1254 [256/118836 (0%)] Loss: 12212.797852\n",
      "Train Epoch: 1254 [33024/118836 (28%)] Loss: 12261.046875\n",
      "Train Epoch: 1254 [65792/118836 (55%)] Loss: 12401.053711\n",
      "Train Epoch: 1254 [98560/118836 (83%)] Loss: 12234.235352\n",
      "    epoch          : 1254\n",
      "    loss           : 12269.129265372983\n",
      "    val_loss       : 12263.06996460229\n",
      "    val_log_likelihood: -12167.4384629924\n",
      "    val_log_marginal: -12175.487096356164\n",
      "Train Epoch: 1255 [256/118836 (0%)] Loss: 12278.142578\n",
      "Train Epoch: 1255 [33024/118836 (28%)] Loss: 12230.724609\n",
      "Train Epoch: 1255 [65792/118836 (55%)] Loss: 12227.777344\n",
      "Train Epoch: 1255 [98560/118836 (83%)] Loss: 12283.796875\n",
      "    epoch          : 1255\n",
      "    loss           : 12271.487585136218\n",
      "    val_loss       : 12264.036207854248\n",
      "    val_log_likelihood: -12169.020805385391\n",
      "    val_log_marginal: -12177.364198380063\n",
      "Train Epoch: 1256 [256/118836 (0%)] Loss: 12204.033203\n",
      "Train Epoch: 1256 [33024/118836 (28%)] Loss: 12316.277344\n",
      "Train Epoch: 1256 [65792/118836 (55%)] Loss: 12298.349609\n",
      "Train Epoch: 1256 [98560/118836 (83%)] Loss: 12206.043945\n",
      "    epoch          : 1256\n",
      "    loss           : 12265.85978452621\n",
      "    val_loss       : 12271.134649341615\n",
      "    val_log_likelihood: -12165.74866382987\n",
      "    val_log_marginal: -12173.814206238269\n",
      "Train Epoch: 1257 [256/118836 (0%)] Loss: 12301.304688\n",
      "Train Epoch: 1257 [33024/118836 (28%)] Loss: 12283.728516\n",
      "Train Epoch: 1257 [65792/118836 (55%)] Loss: 12377.088867\n",
      "Train Epoch: 1257 [98560/118836 (83%)] Loss: 12165.137695\n",
      "    epoch          : 1257\n",
      "    loss           : 12265.697608431556\n",
      "    val_loss       : 12270.028737574443\n",
      "    val_log_likelihood: -12166.444497163204\n",
      "    val_log_marginal: -12174.494116901744\n",
      "Train Epoch: 1258 [256/118836 (0%)] Loss: 12310.248047\n",
      "Train Epoch: 1258 [33024/118836 (28%)] Loss: 12218.640625\n",
      "Train Epoch: 1258 [65792/118836 (55%)] Loss: 12272.875977\n",
      "Train Epoch: 1258 [98560/118836 (83%)] Loss: 12312.442383\n",
      "    epoch          : 1258\n",
      "    loss           : 12269.41207981157\n",
      "    val_loss       : 12267.862975077303\n",
      "    val_log_likelihood: -12181.203879432898\n",
      "    val_log_marginal: -12189.729325090351\n",
      "Train Epoch: 1259 [256/118836 (0%)] Loss: 12277.076172\n",
      "Train Epoch: 1259 [33024/118836 (28%)] Loss: 12264.972656\n",
      "Train Epoch: 1259 [65792/118836 (55%)] Loss: 12208.416016\n",
      "Train Epoch: 1259 [98560/118836 (83%)] Loss: 12342.204102\n",
      "    epoch          : 1259\n",
      "    loss           : 12268.086775938275\n",
      "    val_loss       : 12269.111190309564\n",
      "    val_log_likelihood: -12167.566576683985\n",
      "    val_log_marginal: -12175.672212785374\n",
      "Train Epoch: 1260 [256/118836 (0%)] Loss: 12262.795898\n",
      "Train Epoch: 1260 [33024/118836 (28%)] Loss: 12235.287109\n",
      "Train Epoch: 1260 [65792/118836 (55%)] Loss: 12313.666016\n",
      "Train Epoch: 1260 [98560/118836 (83%)] Loss: 12282.474609\n",
      "    epoch          : 1260\n",
      "    loss           : 12268.120965157155\n",
      "    val_loss       : 12264.908159859266\n",
      "    val_log_likelihood: -12166.783918786186\n",
      "    val_log_marginal: -12174.74750730015\n",
      "Train Epoch: 1261 [256/118836 (0%)] Loss: 12308.621094\n",
      "Train Epoch: 1261 [33024/118836 (28%)] Loss: 12469.006836\n",
      "Train Epoch: 1261 [65792/118836 (55%)] Loss: 12359.496094\n",
      "Train Epoch: 1261 [98560/118836 (83%)] Loss: 12385.422852\n",
      "    epoch          : 1261\n",
      "    loss           : 12269.078351652966\n",
      "    val_loss       : 12268.443886413148\n",
      "    val_log_likelihood: -12166.049210187914\n",
      "    val_log_marginal: -12174.069786103106\n",
      "Train Epoch: 1262 [256/118836 (0%)] Loss: 12230.107422\n",
      "Train Epoch: 1262 [33024/118836 (28%)] Loss: 12335.283203\n",
      "Train Epoch: 1262 [65792/118836 (55%)] Loss: 12271.909180\n",
      "Train Epoch: 1262 [98560/118836 (83%)] Loss: 12269.698242\n",
      "    epoch          : 1262\n",
      "    loss           : 12263.705018513494\n",
      "    val_loss       : 12261.591953328361\n",
      "    val_log_likelihood: -12164.949999515355\n",
      "    val_log_marginal: -12172.953627870926\n",
      "Train Epoch: 1263 [256/118836 (0%)] Loss: 12238.163086\n",
      "Train Epoch: 1263 [33024/118836 (28%)] Loss: 12245.571289\n",
      "Train Epoch: 1263 [65792/118836 (55%)] Loss: 12219.771484\n",
      "Train Epoch: 1263 [98560/118836 (83%)] Loss: 12248.641602\n",
      "    epoch          : 1263\n",
      "    loss           : 12263.277708204094\n",
      "    val_loss       : 12260.507511783178\n",
      "    val_log_likelihood: -12167.787326981235\n",
      "    val_log_marginal: -12175.718041453201\n",
      "Train Epoch: 1264 [256/118836 (0%)] Loss: 12208.463867\n",
      "Train Epoch: 1264 [33024/118836 (28%)] Loss: 12311.834961\n",
      "Train Epoch: 1264 [65792/118836 (55%)] Loss: 12374.867188\n",
      "Train Epoch: 1264 [98560/118836 (83%)] Loss: 12350.755859\n",
      "    epoch          : 1264\n",
      "    loss           : 12263.498618111558\n",
      "    val_loss       : 12265.53500957517\n",
      "    val_log_likelihood: -12167.769989402397\n",
      "    val_log_marginal: -12175.823719169543\n",
      "Train Epoch: 1265 [256/118836 (0%)] Loss: 12284.120117\n",
      "Train Epoch: 1265 [33024/118836 (28%)] Loss: 12298.723633\n",
      "Train Epoch: 1265 [65792/118836 (55%)] Loss: 12361.929688\n",
      "Train Epoch: 1265 [98560/118836 (83%)] Loss: 12345.530273\n",
      "    epoch          : 1265\n",
      "    loss           : 12266.598448485318\n",
      "    val_loss       : 12265.25411434531\n",
      "    val_log_likelihood: -12167.177171700529\n",
      "    val_log_marginal: -12175.281101507593\n",
      "Train Epoch: 1266 [256/118836 (0%)] Loss: 12281.256836\n",
      "Train Epoch: 1266 [33024/118836 (28%)] Loss: 12262.320312\n",
      "Train Epoch: 1266 [65792/118836 (55%)] Loss: 12372.189453\n",
      "Train Epoch: 1266 [98560/118836 (83%)] Loss: 12340.385742\n",
      "    epoch          : 1266\n",
      "    loss           : 12266.201716456007\n",
      "    val_loss       : 12267.337831091445\n",
      "    val_log_likelihood: -12169.639788823408\n",
      "    val_log_marginal: -12177.606027229984\n",
      "Train Epoch: 1267 [256/118836 (0%)] Loss: 12310.267578\n",
      "Train Epoch: 1267 [33024/118836 (28%)] Loss: 12221.497070\n",
      "Train Epoch: 1267 [65792/118836 (55%)] Loss: 12201.545898\n",
      "Train Epoch: 1267 [98560/118836 (83%)] Loss: 12397.234375\n",
      "    epoch          : 1267\n",
      "    loss           : 12266.502316125156\n",
      "    val_loss       : 12266.552567540693\n",
      "    val_log_likelihood: -12169.538839885752\n",
      "    val_log_marginal: -12177.753711176087\n",
      "Train Epoch: 1268 [256/118836 (0%)] Loss: 12193.750977\n",
      "Train Epoch: 1268 [33024/118836 (28%)] Loss: 12246.554688\n",
      "Train Epoch: 1268 [65792/118836 (55%)] Loss: 12356.232422\n",
      "Train Epoch: 1268 [98560/118836 (83%)] Loss: 12307.327148\n",
      "    epoch          : 1268\n",
      "    loss           : 12265.819669697321\n",
      "    val_loss       : 12269.877443633874\n",
      "    val_log_likelihood: -12167.601758620243\n",
      "    val_log_marginal: -12175.657482911221\n",
      "Train Epoch: 1269 [256/118836 (0%)] Loss: 12257.604492\n",
      "Train Epoch: 1269 [33024/118836 (28%)] Loss: 12267.237305\n",
      "Train Epoch: 1269 [65792/118836 (55%)] Loss: 12330.824219\n",
      "Train Epoch: 1269 [98560/118836 (83%)] Loss: 12291.771484\n",
      "    epoch          : 1269\n",
      "    loss           : 12267.853389616936\n",
      "    val_loss       : 12266.920899161278\n",
      "    val_log_likelihood: -12167.129034196649\n",
      "    val_log_marginal: -12175.235368936843\n",
      "Train Epoch: 1270 [256/118836 (0%)] Loss: 12393.226562\n",
      "Train Epoch: 1270 [33024/118836 (28%)] Loss: 12199.066406\n",
      "Train Epoch: 1270 [65792/118836 (55%)] Loss: 12371.816406\n",
      "Train Epoch: 1270 [98560/118836 (83%)] Loss: 12275.470703\n",
      "    epoch          : 1270\n",
      "    loss           : 12268.48653539211\n",
      "    val_loss       : 12267.033736885352\n",
      "    val_log_likelihood: -12168.564341656327\n",
      "    val_log_marginal: -12176.679327890486\n",
      "Train Epoch: 1271 [256/118836 (0%)] Loss: 12357.282227\n",
      "Train Epoch: 1271 [33024/118836 (28%)] Loss: 12332.099609\n",
      "Train Epoch: 1271 [65792/118836 (55%)] Loss: 12277.871094\n",
      "Train Epoch: 1271 [98560/118836 (83%)] Loss: 12161.490234\n",
      "    epoch          : 1271\n",
      "    loss           : 12266.443965506101\n",
      "    val_loss       : 12266.529105064461\n",
      "    val_log_likelihood: -12165.475652980253\n",
      "    val_log_marginal: -12173.641033792803\n",
      "Train Epoch: 1272 [256/118836 (0%)] Loss: 12194.952148\n",
      "Train Epoch: 1272 [33024/118836 (28%)] Loss: 12242.855469\n",
      "Train Epoch: 1272 [65792/118836 (55%)] Loss: 12299.604492\n",
      "Train Epoch: 1272 [98560/118836 (83%)] Loss: 12407.641602\n",
      "    epoch          : 1272\n",
      "    loss           : 12267.058539308055\n",
      "    val_loss       : 12264.996766628325\n",
      "    val_log_likelihood: -12164.50959842199\n",
      "    val_log_marginal: -12172.581763654161\n",
      "Train Epoch: 1273 [256/118836 (0%)] Loss: 12279.865234\n",
      "Train Epoch: 1273 [33024/118836 (28%)] Loss: 12285.341797\n",
      "Train Epoch: 1273 [65792/118836 (55%)] Loss: 12236.812500\n",
      "Train Epoch: 1273 [98560/118836 (83%)] Loss: 12342.279297\n",
      "    epoch          : 1273\n",
      "    loss           : 12266.432736572064\n",
      "    val_loss       : 12267.343276980366\n",
      "    val_log_likelihood: -12167.05722106984\n",
      "    val_log_marginal: -12175.02936539879\n",
      "Train Epoch: 1274 [256/118836 (0%)] Loss: 12355.812500\n",
      "Train Epoch: 1274 [33024/118836 (28%)] Loss: 12342.328125\n",
      "Train Epoch: 1274 [65792/118836 (55%)] Loss: 12282.089844\n",
      "Train Epoch: 1274 [98560/118836 (83%)] Loss: 12265.523438\n",
      "    epoch          : 1274\n",
      "    loss           : 12265.977709819583\n",
      "    val_loss       : 12263.033596242269\n",
      "    val_log_likelihood: -12168.836801624535\n",
      "    val_log_marginal: -12176.895229087995\n",
      "Train Epoch: 1275 [256/118836 (0%)] Loss: 12217.767578\n",
      "Train Epoch: 1275 [33024/118836 (28%)] Loss: 12310.685547\n",
      "Train Epoch: 1275 [65792/118836 (55%)] Loss: 12324.689453\n",
      "Train Epoch: 1275 [98560/118836 (83%)] Loss: 12289.973633\n",
      "    epoch          : 1275\n",
      "    loss           : 12269.984063372365\n",
      "    val_loss       : 12263.13828394233\n",
      "    val_log_likelihood: -12167.523802923388\n",
      "    val_log_marginal: -12175.511045642741\n",
      "Train Epoch: 1276 [256/118836 (0%)] Loss: 12344.402344\n",
      "Train Epoch: 1276 [33024/118836 (28%)] Loss: 12159.793945\n",
      "Train Epoch: 1276 [65792/118836 (55%)] Loss: 12267.504883\n",
      "Train Epoch: 1276 [98560/118836 (83%)] Loss: 12262.397461\n",
      "    epoch          : 1276\n",
      "    loss           : 12265.214375258478\n",
      "    val_loss       : 12270.828475358005\n",
      "    val_log_likelihood: -12169.650690459574\n",
      "    val_log_marginal: -12177.607731888544\n",
      "Train Epoch: 1277 [256/118836 (0%)] Loss: 12328.320312\n",
      "Train Epoch: 1277 [33024/118836 (28%)] Loss: 12228.307617\n",
      "Train Epoch: 1277 [65792/118836 (55%)] Loss: 12318.998047\n",
      "Train Epoch: 1277 [98560/118836 (83%)] Loss: 12272.625000\n",
      "    epoch          : 1277\n",
      "    loss           : 12265.61938569453\n",
      "    val_loss       : 12267.337394524657\n",
      "    val_log_likelihood: -12169.340900279158\n",
      "    val_log_marginal: -12177.851048005603\n",
      "Train Epoch: 1278 [256/118836 (0%)] Loss: 12333.832031\n",
      "Train Epoch: 1278 [33024/118836 (28%)] Loss: 12368.824219\n",
      "Train Epoch: 1278 [65792/118836 (55%)] Loss: 12306.756836\n",
      "Train Epoch: 1278 [98560/118836 (83%)] Loss: 12341.996094\n",
      "    epoch          : 1278\n",
      "    loss           : 12265.534215066687\n",
      "    val_loss       : 12268.107367968425\n",
      "    val_log_likelihood: -12168.793329973118\n",
      "    val_log_marginal: -12176.864575961452\n",
      "Train Epoch: 1279 [256/118836 (0%)] Loss: 12449.480469\n",
      "Train Epoch: 1279 [33024/118836 (28%)] Loss: 12369.771484\n",
      "Train Epoch: 1279 [65792/118836 (55%)] Loss: 12362.714844\n",
      "Train Epoch: 1279 [98560/118836 (83%)] Loss: 12232.875000\n",
      "    epoch          : 1279\n",
      "    loss           : 12270.52817766491\n",
      "    val_loss       : 12265.08208677681\n",
      "    val_log_likelihood: -12166.503364576873\n",
      "    val_log_marginal: -12174.59000036148\n",
      "Train Epoch: 1280 [256/118836 (0%)] Loss: 12379.318359\n",
      "Train Epoch: 1280 [33024/118836 (28%)] Loss: 12239.855469\n",
      "Train Epoch: 1280 [65792/118836 (55%)] Loss: 12371.752930\n",
      "Train Epoch: 1280 [98560/118836 (83%)] Loss: 12287.885742\n",
      "    epoch          : 1280\n",
      "    loss           : 12265.133680178607\n",
      "    val_loss       : 12272.593211987869\n",
      "    val_log_likelihood: -12165.244096037532\n",
      "    val_log_marginal: -12173.20996186647\n",
      "Train Epoch: 1281 [256/118836 (0%)] Loss: 12261.716797\n",
      "Train Epoch: 1281 [33024/118836 (28%)] Loss: 12295.985352\n",
      "Train Epoch: 1281 [65792/118836 (55%)] Loss: 12234.373047\n",
      "Train Epoch: 1281 [98560/118836 (83%)] Loss: 12296.294922\n",
      "    epoch          : 1281\n",
      "    loss           : 12264.497859155295\n",
      "    val_loss       : 12265.170970019013\n",
      "    val_log_likelihood: -12166.985431529158\n",
      "    val_log_marginal: -12174.93349241142\n",
      "Train Epoch: 1282 [256/118836 (0%)] Loss: 12344.695312\n",
      "Train Epoch: 1282 [33024/118836 (28%)] Loss: 12401.218750\n",
      "Train Epoch: 1282 [65792/118836 (55%)] Loss: 12291.251953\n",
      "Train Epoch: 1282 [98560/118836 (83%)] Loss: 12321.040039\n",
      "    epoch          : 1282\n",
      "    loss           : 12267.861706149193\n",
      "    val_loss       : 12264.51409585811\n",
      "    val_log_likelihood: -12168.769454675868\n",
      "    val_log_marginal: -12176.781435977107\n",
      "Train Epoch: 1283 [256/118836 (0%)] Loss: 12259.614258\n",
      "Train Epoch: 1283 [33024/118836 (28%)] Loss: 12408.054688\n",
      "Train Epoch: 1283 [65792/118836 (55%)] Loss: 12350.531250\n",
      "Train Epoch: 1283 [98560/118836 (83%)] Loss: 12242.367188\n",
      "    epoch          : 1283\n",
      "    loss           : 12269.750818729322\n",
      "    val_loss       : 12270.708935417118\n",
      "    val_log_likelihood: -12169.80161904208\n",
      "    val_log_marginal: -12177.895005650484\n",
      "Train Epoch: 1284 [256/118836 (0%)] Loss: 12309.443359\n",
      "Train Epoch: 1284 [33024/118836 (28%)] Loss: 12325.989258\n",
      "Train Epoch: 1284 [65792/118836 (55%)] Loss: 12324.501953\n",
      "Train Epoch: 1284 [98560/118836 (83%)] Loss: 12206.355469\n",
      "    epoch          : 1284\n",
      "    loss           : 12269.261909054489\n",
      "    val_loss       : 12273.348298941522\n",
      "    val_log_likelihood: -12167.6560143003\n",
      "    val_log_marginal: -12175.771533160225\n",
      "Train Epoch: 1285 [256/118836 (0%)] Loss: 12226.335938\n",
      "Train Epoch: 1285 [33024/118836 (28%)] Loss: 12395.513672\n",
      "Train Epoch: 1285 [65792/118836 (55%)] Loss: 12210.250000\n",
      "Train Epoch: 1285 [98560/118836 (83%)] Loss: 12224.305664\n",
      "    epoch          : 1285\n",
      "    loss           : 12271.84751069453\n",
      "    val_loss       : 12274.632858848077\n",
      "    val_log_likelihood: -12167.49268587805\n",
      "    val_log_marginal: -12176.008529289355\n",
      "Train Epoch: 1286 [256/118836 (0%)] Loss: 12333.795898\n",
      "Train Epoch: 1286 [33024/118836 (28%)] Loss: 12283.459961\n",
      "Train Epoch: 1286 [65792/118836 (55%)] Loss: 12407.242188\n",
      "Train Epoch: 1286 [98560/118836 (83%)] Loss: 12186.689453\n",
      "    epoch          : 1286\n",
      "    loss           : 12264.19714688663\n",
      "    val_loss       : 12268.965620514693\n",
      "    val_log_likelihood: -12164.431181503307\n",
      "    val_log_marginal: -12172.586695249347\n",
      "Train Epoch: 1287 [256/118836 (0%)] Loss: 12283.119141\n",
      "Train Epoch: 1287 [33024/118836 (28%)] Loss: 12278.286133\n",
      "Train Epoch: 1287 [65792/118836 (55%)] Loss: 12442.613281\n",
      "Train Epoch: 1287 [98560/118836 (83%)] Loss: 12291.056641\n",
      "    epoch          : 1287\n",
      "    loss           : 12269.914568470844\n",
      "    val_loss       : 12262.615200693923\n",
      "    val_log_likelihood: -12168.97360179513\n",
      "    val_log_marginal: -12177.12552263832\n",
      "Train Epoch: 1288 [256/118836 (0%)] Loss: 12401.176758\n",
      "Train Epoch: 1288 [33024/118836 (28%)] Loss: 12214.336914\n",
      "Train Epoch: 1288 [65792/118836 (55%)] Loss: 12403.938477\n",
      "Train Epoch: 1288 [98560/118836 (83%)] Loss: 12365.700195\n",
      "    epoch          : 1288\n",
      "    loss           : 12267.229041789444\n",
      "    val_loss       : 12259.72029066455\n",
      "    val_log_likelihood: -12167.466907535929\n",
      "    val_log_marginal: -12175.560943342573\n",
      "Train Epoch: 1289 [256/118836 (0%)] Loss: 12298.578125\n",
      "Train Epoch: 1289 [33024/118836 (28%)] Loss: 12286.410156\n",
      "Train Epoch: 1289 [65792/118836 (55%)] Loss: 12373.533203\n",
      "Train Epoch: 1289 [98560/118836 (83%)] Loss: 12314.912109\n",
      "    epoch          : 1289\n",
      "    loss           : 12269.563085614403\n",
      "    val_loss       : 12262.993661128225\n",
      "    val_log_likelihood: -12166.498717140974\n",
      "    val_log_marginal: -12174.381447357428\n",
      "Train Epoch: 1290 [256/118836 (0%)] Loss: 12280.753906\n",
      "Train Epoch: 1290 [33024/118836 (28%)] Loss: 12358.216797\n",
      "Train Epoch: 1290 [65792/118836 (55%)] Loss: 12265.813477\n",
      "Train Epoch: 1290 [98560/118836 (83%)] Loss: 12360.545898\n",
      "    epoch          : 1290\n",
      "    loss           : 12263.728347452698\n",
      "    val_loss       : 12269.698694275086\n",
      "    val_log_likelihood: -12166.795255311725\n",
      "    val_log_marginal: -12174.831877156972\n",
      "Train Epoch: 1291 [256/118836 (0%)] Loss: 12280.222656\n",
      "Train Epoch: 1291 [33024/118836 (28%)] Loss: 12317.825195\n",
      "Train Epoch: 1291 [65792/118836 (55%)] Loss: 12294.806641\n",
      "Train Epoch: 1291 [98560/118836 (83%)] Loss: 12276.501953\n",
      "    epoch          : 1291\n",
      "    loss           : 12263.026926954095\n",
      "    val_loss       : 12265.2415868995\n",
      "    val_log_likelihood: -12167.156208320408\n",
      "    val_log_marginal: -12175.35443342496\n",
      "Train Epoch: 1292 [256/118836 (0%)] Loss: 12316.424805\n",
      "Train Epoch: 1292 [33024/118836 (28%)] Loss: 12257.796875\n",
      "Train Epoch: 1292 [65792/118836 (55%)] Loss: 12234.475586\n",
      "Train Epoch: 1292 [98560/118836 (83%)] Loss: 12393.070312\n",
      "    epoch          : 1292\n",
      "    loss           : 12269.871221535102\n",
      "    val_loss       : 12267.328767701807\n",
      "    val_log_likelihood: -12165.909564335194\n",
      "    val_log_marginal: -12173.85718920083\n",
      "Train Epoch: 1293 [256/118836 (0%)] Loss: 12298.484375\n",
      "Train Epoch: 1293 [33024/118836 (28%)] Loss: 12270.462891\n",
      "Train Epoch: 1293 [65792/118836 (55%)] Loss: 12222.966797\n",
      "Train Epoch: 1293 [98560/118836 (83%)] Loss: 12326.621094\n",
      "    epoch          : 1293\n",
      "    loss           : 12268.533174207765\n",
      "    val_loss       : 12263.550960818116\n",
      "    val_log_likelihood: -12166.461043314464\n",
      "    val_log_marginal: -12174.533989358657\n",
      "Train Epoch: 1294 [256/118836 (0%)] Loss: 12239.624023\n",
      "Train Epoch: 1294 [33024/118836 (28%)] Loss: 12387.636719\n",
      "Train Epoch: 1294 [65792/118836 (55%)] Loss: 12314.960938\n",
      "Train Epoch: 1294 [98560/118836 (83%)] Loss: 12311.130859\n",
      "    epoch          : 1294\n",
      "    loss           : 12268.863257825426\n",
      "    val_loss       : 12267.496657148238\n",
      "    val_log_likelihood: -12168.333920886322\n",
      "    val_log_marginal: -12176.619076704897\n",
      "Train Epoch: 1295 [256/118836 (0%)] Loss: 12333.646484\n",
      "Train Epoch: 1295 [33024/118836 (28%)] Loss: 12325.764648\n",
      "Train Epoch: 1295 [65792/118836 (55%)] Loss: 12289.700195\n",
      "Train Epoch: 1295 [98560/118836 (83%)] Loss: 12415.696289\n",
      "    epoch          : 1295\n",
      "    loss           : 12267.524010513596\n",
      "    val_loss       : 12264.695722851135\n",
      "    val_log_likelihood: -12167.363285773366\n",
      "    val_log_marginal: -12175.401718856865\n",
      "Train Epoch: 1296 [256/118836 (0%)] Loss: 12339.130859\n",
      "Train Epoch: 1296 [33024/118836 (28%)] Loss: 12296.125000\n",
      "Train Epoch: 1296 [65792/118836 (55%)] Loss: 12357.200195\n",
      "Train Epoch: 1296 [98560/118836 (83%)] Loss: 12375.502930\n",
      "    epoch          : 1296\n",
      "    loss           : 12268.23423219086\n",
      "    val_loss       : 12275.820204518443\n",
      "    val_log_likelihood: -12168.987940058933\n",
      "    val_log_marginal: -12177.68422462928\n",
      "Train Epoch: 1297 [256/118836 (0%)] Loss: 12372.152344\n",
      "Train Epoch: 1297 [33024/118836 (28%)] Loss: 12324.311523\n",
      "Train Epoch: 1297 [65792/118836 (55%)] Loss: 12278.284180\n",
      "Train Epoch: 1297 [98560/118836 (83%)] Loss: 12256.247070\n",
      "    epoch          : 1297\n",
      "    loss           : 12270.210675790942\n",
      "    val_loss       : 12269.406988384504\n",
      "    val_log_likelihood: -12168.678627901416\n",
      "    val_log_marginal: -12177.058875286235\n",
      "Train Epoch: 1298 [256/118836 (0%)] Loss: 12232.311523\n",
      "Train Epoch: 1298 [33024/118836 (28%)] Loss: 12215.804688\n",
      "Train Epoch: 1298 [65792/118836 (55%)] Loss: 12272.338867\n",
      "Train Epoch: 1298 [98560/118836 (83%)] Loss: 12296.590820\n",
      "    epoch          : 1298\n",
      "    loss           : 12271.4674635869\n",
      "    val_loss       : 12269.593962613322\n",
      "    val_log_likelihood: -12168.969102499485\n",
      "    val_log_marginal: -12176.967668497198\n",
      "Train Epoch: 1299 [256/118836 (0%)] Loss: 12272.372070\n",
      "Train Epoch: 1299 [33024/118836 (28%)] Loss: 12310.339844\n",
      "Train Epoch: 1299 [65792/118836 (55%)] Loss: 12270.468750\n",
      "Train Epoch: 1299 [98560/118836 (83%)] Loss: 12295.075195\n",
      "    epoch          : 1299\n",
      "    loss           : 12269.228319666305\n",
      "    val_loss       : 12267.8552762269\n",
      "    val_log_likelihood: -12164.869185050919\n",
      "    val_log_marginal: -12172.87343298525\n",
      "Train Epoch: 1300 [256/118836 (0%)] Loss: 12332.671875\n",
      "Train Epoch: 1300 [33024/118836 (28%)] Loss: 12322.985352\n",
      "Train Epoch: 1300 [65792/118836 (55%)] Loss: 12330.222656\n",
      "Train Epoch: 1300 [98560/118836 (83%)] Loss: 12299.204102\n",
      "    epoch          : 1300\n",
      "    loss           : 12266.22174139914\n",
      "    val_loss       : 12267.76403657753\n",
      "    val_log_likelihood: -12169.61253085582\n",
      "    val_log_marginal: -12177.641689987484\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch1300.pth ...\n",
      "Train Epoch: 1301 [256/118836 (0%)] Loss: 12259.513672\n",
      "Train Epoch: 1301 [33024/118836 (28%)] Loss: 12227.155273\n",
      "Train Epoch: 1301 [65792/118836 (55%)] Loss: 12260.015625\n",
      "Train Epoch: 1301 [98560/118836 (83%)] Loss: 12361.012695\n",
      "    epoch          : 1301\n",
      "    loss           : 12268.652693341604\n",
      "    val_loss       : 12263.595021031824\n",
      "    val_log_likelihood: -12168.086064800456\n",
      "    val_log_marginal: -12176.05966768681\n",
      "Train Epoch: 1302 [256/118836 (0%)] Loss: 12257.198242\n",
      "Train Epoch: 1302 [33024/118836 (28%)] Loss: 12238.108398\n",
      "Train Epoch: 1302 [65792/118836 (55%)] Loss: 12306.122070\n",
      "Train Epoch: 1302 [98560/118836 (83%)] Loss: 12221.222656\n",
      "    epoch          : 1302\n",
      "    loss           : 12269.81593226582\n",
      "    val_loss       : 12265.014878424538\n",
      "    val_log_likelihood: -12168.248952679125\n",
      "    val_log_marginal: -12176.164050906824\n",
      "Train Epoch: 1303 [256/118836 (0%)] Loss: 12203.928711\n",
      "Train Epoch: 1303 [33024/118836 (28%)] Loss: 12207.733398\n",
      "Train Epoch: 1303 [65792/118836 (55%)] Loss: 12296.287109\n",
      "Train Epoch: 1303 [98560/118836 (83%)] Loss: 12292.666992\n",
      "    epoch          : 1303\n",
      "    loss           : 12270.426502565395\n",
      "    val_loss       : 12269.034081607826\n",
      "    val_log_likelihood: -12169.39695173568\n",
      "    val_log_marginal: -12177.51165451122\n",
      "Train Epoch: 1304 [256/118836 (0%)] Loss: 12293.323242\n",
      "Train Epoch: 1304 [33024/118836 (28%)] Loss: 12322.001953\n",
      "Train Epoch: 1304 [65792/118836 (55%)] Loss: 12286.077148\n",
      "Train Epoch: 1304 [98560/118836 (83%)] Loss: 12231.500000\n",
      "    epoch          : 1304\n",
      "    loss           : 12270.48652359905\n",
      "    val_loss       : 12271.320881904352\n",
      "    val_log_likelihood: -12167.67290810458\n",
      "    val_log_marginal: -12175.744201017358\n",
      "Train Epoch: 1305 [256/118836 (0%)] Loss: 12252.615234\n",
      "Train Epoch: 1305 [33024/118836 (28%)] Loss: 12155.729492\n",
      "Train Epoch: 1305 [65792/118836 (55%)] Loss: 12335.449219\n",
      "Train Epoch: 1305 [98560/118836 (83%)] Loss: 12208.358398\n",
      "    epoch          : 1305\n",
      "    loss           : 12259.381224636838\n",
      "    val_loss       : 12266.65691027643\n",
      "    val_log_likelihood: -12169.870882444167\n",
      "    val_log_marginal: -12177.823530648937\n",
      "Train Epoch: 1306 [256/118836 (0%)] Loss: 12307.857422\n",
      "Train Epoch: 1306 [33024/118836 (28%)] Loss: 12355.923828\n",
      "Train Epoch: 1306 [65792/118836 (55%)] Loss: 12436.777344\n",
      "Train Epoch: 1306 [98560/118836 (83%)] Loss: 12234.527344\n",
      "    epoch          : 1306\n",
      "    loss           : 12266.174572703423\n",
      "    val_loss       : 12262.69893136519\n",
      "    val_log_likelihood: -12169.995131403795\n",
      "    val_log_marginal: -12177.928511260827\n",
      "Train Epoch: 1307 [256/118836 (0%)] Loss: 12224.535156\n",
      "Train Epoch: 1307 [33024/118836 (28%)] Loss: 12337.970703\n",
      "Train Epoch: 1307 [65792/118836 (55%)] Loss: 12344.076172\n",
      "Train Epoch: 1307 [98560/118836 (83%)] Loss: 12216.977539\n",
      "    epoch          : 1307\n",
      "    loss           : 12269.409574512769\n",
      "    val_loss       : 12267.683594301985\n",
      "    val_log_likelihood: -12168.083562086435\n",
      "    val_log_marginal: -12176.456338265965\n",
      "Train Epoch: 1308 [256/118836 (0%)] Loss: 12305.461914\n",
      "Train Epoch: 1308 [33024/118836 (28%)] Loss: 12373.669922\n",
      "Train Epoch: 1308 [65792/118836 (55%)] Loss: 12324.937500\n",
      "Train Epoch: 1308 [98560/118836 (83%)] Loss: 12223.452148\n",
      "    epoch          : 1308\n",
      "    loss           : 12269.561643629808\n",
      "    val_loss       : 12270.288203296934\n",
      "    val_log_likelihood: -12166.37638883504\n",
      "    val_log_marginal: -12174.529825222076\n",
      "Train Epoch: 1309 [256/118836 (0%)] Loss: 12204.530273\n",
      "Train Epoch: 1309 [33024/118836 (28%)] Loss: 12306.501953\n",
      "Train Epoch: 1309 [65792/118836 (55%)] Loss: 12295.751953\n",
      "Train Epoch: 1309 [98560/118836 (83%)] Loss: 12342.103516\n",
      "    epoch          : 1309\n",
      "    loss           : 12274.477717412376\n",
      "    val_loss       : 12267.659075320895\n",
      "    val_log_likelihood: -12167.544863717432\n",
      "    val_log_marginal: -12175.882568945954\n",
      "Train Epoch: 1310 [256/118836 (0%)] Loss: 12261.809570\n",
      "Train Epoch: 1310 [33024/118836 (28%)] Loss: 12214.567383\n",
      "Train Epoch: 1310 [65792/118836 (55%)] Loss: 12348.931641\n",
      "Train Epoch: 1310 [98560/118836 (83%)] Loss: 12297.088867\n",
      "    epoch          : 1310\n",
      "    loss           : 12262.737719060175\n",
      "    val_loss       : 12267.788324100691\n",
      "    val_log_likelihood: -12168.089647629757\n",
      "    val_log_marginal: -12176.280790859155\n",
      "Train Epoch: 1311 [256/118836 (0%)] Loss: 12246.347656\n",
      "Train Epoch: 1311 [33024/118836 (28%)] Loss: 12321.391602\n",
      "Train Epoch: 1311 [65792/118836 (55%)] Loss: 12349.216797\n",
      "Train Epoch: 1311 [98560/118836 (83%)] Loss: 12228.999023\n",
      "    epoch          : 1311\n",
      "    loss           : 12269.238877688173\n",
      "    val_loss       : 12260.806873428766\n",
      "    val_log_likelihood: -12169.174329572477\n",
      "    val_log_marginal: -12177.197532610693\n",
      "Train Epoch: 1312 [256/118836 (0%)] Loss: 12322.269531\n",
      "Train Epoch: 1312 [33024/118836 (28%)] Loss: 12237.217773\n",
      "Train Epoch: 1312 [65792/118836 (55%)] Loss: 12345.084961\n",
      "Train Epoch: 1312 [98560/118836 (83%)] Loss: 12216.963867\n",
      "    epoch          : 1312\n",
      "    loss           : 12265.075732623813\n",
      "    val_loss       : 12267.373415409797\n",
      "    val_log_likelihood: -12166.184747854631\n",
      "    val_log_marginal: -12174.236952963245\n",
      "Train Epoch: 1313 [256/118836 (0%)] Loss: 12321.234375\n",
      "Train Epoch: 1313 [33024/118836 (28%)] Loss: 12201.801758\n",
      "Train Epoch: 1313 [65792/118836 (55%)] Loss: 12219.103516\n",
      "Train Epoch: 1313 [98560/118836 (83%)] Loss: 12261.662109\n",
      "    epoch          : 1313\n",
      "    loss           : 12266.840806257755\n",
      "    val_loss       : 12268.296123036194\n",
      "    val_log_likelihood: -12167.528445997468\n",
      "    val_log_marginal: -12175.56079190421\n",
      "Train Epoch: 1314 [256/118836 (0%)] Loss: 12327.102539\n",
      "Train Epoch: 1314 [33024/118836 (28%)] Loss: 12258.166016\n",
      "Train Epoch: 1314 [65792/118836 (55%)] Loss: 12313.710938\n",
      "Train Epoch: 1314 [98560/118836 (83%)] Loss: 12388.455078\n",
      "    epoch          : 1314\n",
      "    loss           : 12272.732143849515\n",
      "    val_loss       : 12265.85961607168\n",
      "    val_log_likelihood: -12164.467256642885\n",
      "    val_log_marginal: -12172.474418622576\n",
      "Train Epoch: 1315 [256/118836 (0%)] Loss: 12410.922852\n",
      "Train Epoch: 1315 [33024/118836 (28%)] Loss: 12191.063477\n",
      "Train Epoch: 1315 [65792/118836 (55%)] Loss: 12264.774414\n",
      "Train Epoch: 1315 [98560/118836 (83%)] Loss: 12188.572266\n",
      "    epoch          : 1315\n",
      "    loss           : 12266.364182853857\n",
      "    val_loss       : 12266.92494766856\n",
      "    val_log_likelihood: -12168.118689742298\n",
      "    val_log_marginal: -12176.138712665897\n",
      "Train Epoch: 1316 [256/118836 (0%)] Loss: 12364.404297\n",
      "Train Epoch: 1316 [33024/118836 (28%)] Loss: 12272.434570\n",
      "Train Epoch: 1316 [65792/118836 (55%)] Loss: 12267.527344\n",
      "Train Epoch: 1316 [98560/118836 (83%)] Loss: 12345.424805\n",
      "    epoch          : 1316\n",
      "    loss           : 12267.602733567257\n",
      "    val_loss       : 12265.71298629061\n",
      "    val_log_likelihood: -12169.672244785206\n",
      "    val_log_marginal: -12177.816041370046\n",
      "Train Epoch: 1317 [256/118836 (0%)] Loss: 12275.334961\n",
      "Train Epoch: 1317 [33024/118836 (28%)] Loss: 12331.111328\n",
      "Train Epoch: 1317 [65792/118836 (55%)] Loss: 12238.270508\n",
      "Train Epoch: 1317 [98560/118836 (83%)] Loss: 12277.177734\n",
      "    epoch          : 1317\n",
      "    loss           : 12270.054769566792\n",
      "    val_loss       : 12265.841388422807\n",
      "    val_log_likelihood: -12165.528190750363\n",
      "    val_log_marginal: -12173.612949853481\n",
      "Train Epoch: 1318 [256/118836 (0%)] Loss: 12222.566406\n",
      "Train Epoch: 1318 [33024/118836 (28%)] Loss: 12328.259766\n",
      "Train Epoch: 1318 [65792/118836 (55%)] Loss: 12423.683594\n",
      "Train Epoch: 1318 [98560/118836 (83%)] Loss: 12209.872070\n",
      "    epoch          : 1318\n",
      "    loss           : 12266.897407787945\n",
      "    val_loss       : 12264.555660057513\n",
      "    val_log_likelihood: -12166.288965893817\n",
      "    val_log_marginal: -12174.434715604402\n",
      "Train Epoch: 1319 [256/118836 (0%)] Loss: 12319.043945\n",
      "Train Epoch: 1319 [33024/118836 (28%)] Loss: 12253.886719\n",
      "Train Epoch: 1319 [65792/118836 (55%)] Loss: 12331.697266\n",
      "Train Epoch: 1319 [98560/118836 (83%)] Loss: 12358.675781\n",
      "    epoch          : 1319\n",
      "    loss           : 12267.739340525484\n",
      "    val_loss       : 12264.721516349597\n",
      "    val_log_likelihood: -12168.284310542029\n",
      "    val_log_marginal: -12176.197055125454\n",
      "Train Epoch: 1320 [256/118836 (0%)] Loss: 12261.093750\n",
      "Train Epoch: 1320 [33024/118836 (28%)] Loss: 12252.324219\n",
      "Train Epoch: 1320 [65792/118836 (55%)] Loss: 12218.204102\n",
      "Train Epoch: 1320 [98560/118836 (83%)] Loss: 12337.171875\n",
      "    epoch          : 1320\n",
      "    loss           : 12268.249306147902\n",
      "    val_loss       : 12268.660220921598\n",
      "    val_log_likelihood: -12167.93203108845\n",
      "    val_log_marginal: -12176.132556997229\n",
      "Train Epoch: 1321 [256/118836 (0%)] Loss: 12337.905273\n",
      "Train Epoch: 1321 [33024/118836 (28%)] Loss: 12262.650391\n",
      "Train Epoch: 1321 [65792/118836 (55%)] Loss: 12285.069336\n",
      "Train Epoch: 1321 [98560/118836 (83%)] Loss: 12359.760742\n",
      "    epoch          : 1321\n",
      "    loss           : 12263.901535036703\n",
      "    val_loss       : 12261.682886753331\n",
      "    val_log_likelihood: -12169.194010578216\n",
      "    val_log_marginal: -12177.342945574464\n",
      "Train Epoch: 1322 [256/118836 (0%)] Loss: 12224.534180\n",
      "Train Epoch: 1322 [33024/118836 (28%)] Loss: 12335.236328\n",
      "Train Epoch: 1322 [65792/118836 (55%)] Loss: 12303.968750\n",
      "Train Epoch: 1322 [98560/118836 (83%)] Loss: 12279.340820\n",
      "    epoch          : 1322\n",
      "    loss           : 12265.490550364453\n",
      "    val_loss       : 12270.419391032594\n",
      "    val_log_likelihood: -12166.996495360318\n",
      "    val_log_marginal: -12175.053558807133\n",
      "Train Epoch: 1323 [256/118836 (0%)] Loss: 12306.987305\n",
      "Train Epoch: 1323 [33024/118836 (28%)] Loss: 12209.396484\n",
      "Train Epoch: 1323 [65792/118836 (55%)] Loss: 12347.640625\n",
      "Train Epoch: 1323 [98560/118836 (83%)] Loss: 12238.025391\n",
      "    epoch          : 1323\n",
      "    loss           : 12269.074922618123\n",
      "    val_loss       : 12265.661097886614\n",
      "    val_log_likelihood: -12169.435140418218\n",
      "    val_log_marginal: -12177.430284364978\n",
      "Train Epoch: 1324 [256/118836 (0%)] Loss: 12371.611328\n",
      "Train Epoch: 1324 [33024/118836 (28%)] Loss: 12343.833008\n",
      "Train Epoch: 1324 [65792/118836 (55%)] Loss: 12263.364258\n",
      "Train Epoch: 1324 [98560/118836 (83%)] Loss: 12320.826172\n",
      "    epoch          : 1324\n",
      "    loss           : 12266.787726976065\n",
      "    val_loss       : 12265.685647163038\n",
      "    val_log_likelihood: -12165.631172941223\n",
      "    val_log_marginal: -12173.739636532096\n",
      "Train Epoch: 1325 [256/118836 (0%)] Loss: 12288.854492\n",
      "Train Epoch: 1325 [33024/118836 (28%)] Loss: 12302.775391\n",
      "Train Epoch: 1325 [65792/118836 (55%)] Loss: 12176.531250\n",
      "Train Epoch: 1325 [98560/118836 (83%)] Loss: 12274.739258\n",
      "    epoch          : 1325\n",
      "    loss           : 12262.550497731854\n",
      "    val_loss       : 12262.509532712733\n",
      "    val_log_likelihood: -12166.784959160463\n",
      "    val_log_marginal: -12174.836404834168\n",
      "Train Epoch: 1326 [256/118836 (0%)] Loss: 12254.782227\n",
      "Train Epoch: 1326 [33024/118836 (28%)] Loss: 12361.045898\n",
      "Train Epoch: 1326 [65792/118836 (55%)] Loss: 12287.198242\n",
      "Train Epoch: 1326 [98560/118836 (83%)] Loss: 12307.228516\n",
      "    epoch          : 1326\n",
      "    loss           : 12268.576687538773\n",
      "    val_loss       : 12268.080012143604\n",
      "    val_log_likelihood: -12166.557083430263\n",
      "    val_log_marginal: -12174.67919185146\n",
      "Train Epoch: 1327 [256/118836 (0%)] Loss: 12285.875000\n",
      "Train Epoch: 1327 [33024/118836 (28%)] Loss: 12269.638672\n",
      "Train Epoch: 1327 [65792/118836 (55%)] Loss: 12215.343750\n",
      "Train Epoch: 1327 [98560/118836 (83%)] Loss: 12261.919922\n",
      "    epoch          : 1327\n",
      "    loss           : 12264.800989809502\n",
      "    val_loss       : 12267.504980896008\n",
      "    val_log_likelihood: -12169.335617956473\n",
      "    val_log_marginal: -12177.38434929075\n",
      "Train Epoch: 1328 [256/118836 (0%)] Loss: 12421.400391\n",
      "Train Epoch: 1328 [33024/118836 (28%)] Loss: 12344.463867\n",
      "Train Epoch: 1328 [65792/118836 (55%)] Loss: 12368.413086\n",
      "Train Epoch: 1328 [98560/118836 (83%)] Loss: 12276.120117\n",
      "    epoch          : 1328\n",
      "    loss           : 12268.027104657775\n",
      "    val_loss       : 12264.657094561453\n",
      "    val_log_likelihood: -12167.482875342483\n",
      "    val_log_marginal: -12175.478378149706\n",
      "Train Epoch: 1329 [256/118836 (0%)] Loss: 12323.557617\n",
      "Train Epoch: 1329 [33024/118836 (28%)] Loss: 12262.172852\n",
      "Train Epoch: 1329 [65792/118836 (55%)] Loss: 12426.925781\n",
      "Train Epoch: 1329 [98560/118836 (83%)] Loss: 12358.375977\n",
      "    epoch          : 1329\n",
      "    loss           : 12269.195918146454\n",
      "    val_loss       : 12269.596107954047\n",
      "    val_log_likelihood: -12163.743049685949\n",
      "    val_log_marginal: -12171.889452173082\n",
      "Train Epoch: 1330 [256/118836 (0%)] Loss: 12293.418945\n",
      "Train Epoch: 1330 [33024/118836 (28%)] Loss: 12337.714844\n",
      "Train Epoch: 1330 [65792/118836 (55%)] Loss: 12281.267578\n",
      "Train Epoch: 1330 [98560/118836 (83%)] Loss: 12227.579102\n",
      "    epoch          : 1330\n",
      "    loss           : 12266.360457538512\n",
      "    val_loss       : 12274.577376776906\n",
      "    val_log_likelihood: -12165.717217548077\n",
      "    val_log_marginal: -12173.909025635543\n",
      "Train Epoch: 1331 [256/118836 (0%)] Loss: 12182.115234\n",
      "Train Epoch: 1331 [33024/118836 (28%)] Loss: 12212.718750\n",
      "Train Epoch: 1331 [65792/118836 (55%)] Loss: 12212.833008\n",
      "Train Epoch: 1331 [98560/118836 (83%)] Loss: 12290.877930\n",
      "    epoch          : 1331\n",
      "    loss           : 12265.110931684243\n",
      "    val_loss       : 12272.313801718987\n",
      "    val_log_likelihood: -12168.736461887407\n",
      "    val_log_marginal: -12176.925484586605\n",
      "Train Epoch: 1332 [256/118836 (0%)] Loss: 12384.706055\n",
      "Train Epoch: 1332 [33024/118836 (28%)] Loss: 12286.594727\n",
      "Train Epoch: 1332 [65792/118836 (55%)] Loss: 12276.135742\n",
      "Train Epoch: 1332 [98560/118836 (83%)] Loss: 12215.026367\n",
      "    epoch          : 1332\n",
      "    loss           : 12269.809856738522\n",
      "    val_loss       : 12265.860978897206\n",
      "    val_log_likelihood: -12167.944209929436\n",
      "    val_log_marginal: -12175.995441603456\n",
      "Train Epoch: 1333 [256/118836 (0%)] Loss: 12187.562500\n",
      "Train Epoch: 1333 [33024/118836 (28%)] Loss: 12293.366211\n",
      "Train Epoch: 1333 [65792/118836 (55%)] Loss: 12295.001953\n",
      "Train Epoch: 1333 [98560/118836 (83%)] Loss: 12317.045898\n",
      "    epoch          : 1333\n",
      "    loss           : 12269.334662072219\n",
      "    val_loss       : 12269.528896533106\n",
      "    val_log_likelihood: -12168.889751828732\n",
      "    val_log_marginal: -12177.398688901108\n",
      "Train Epoch: 1334 [256/118836 (0%)] Loss: 12232.016602\n",
      "Train Epoch: 1334 [33024/118836 (28%)] Loss: 12401.453125\n",
      "Train Epoch: 1334 [65792/118836 (55%)] Loss: 12213.463867\n",
      "Train Epoch: 1334 [98560/118836 (83%)] Loss: 12323.455078\n",
      "    epoch          : 1334\n",
      "    loss           : 12272.819488601117\n",
      "    val_loss       : 12270.020162526078\n",
      "    val_log_likelihood: -12166.520112179489\n",
      "    val_log_marginal: -12174.565886060323\n",
      "Train Epoch: 1335 [256/118836 (0%)] Loss: 12256.653320\n",
      "Train Epoch: 1335 [33024/118836 (28%)] Loss: 12238.012695\n",
      "Train Epoch: 1335 [65792/118836 (55%)] Loss: 12316.342773\n",
      "Train Epoch: 1335 [98560/118836 (83%)] Loss: 12428.342773\n",
      "    epoch          : 1335\n",
      "    loss           : 12268.16341985887\n",
      "    val_loss       : 12271.115122406256\n",
      "    val_log_likelihood: -12170.559372415219\n",
      "    val_log_marginal: -12178.620126419735\n",
      "Train Epoch: 1336 [256/118836 (0%)] Loss: 12306.215820\n",
      "Train Epoch: 1336 [33024/118836 (28%)] Loss: 12215.750977\n",
      "Train Epoch: 1336 [65792/118836 (55%)] Loss: 12347.174805\n",
      "Train Epoch: 1336 [98560/118836 (83%)] Loss: 12248.039062\n",
      "    epoch          : 1336\n",
      "    loss           : 12267.89935735887\n",
      "    val_loss       : 12271.196074056805\n",
      "    val_log_likelihood: -12167.35108173077\n",
      "    val_log_marginal: -12175.40992456476\n",
      "Train Epoch: 1337 [256/118836 (0%)] Loss: 12358.603516\n",
      "Train Epoch: 1337 [33024/118836 (28%)] Loss: 12318.484375\n",
      "Train Epoch: 1337 [65792/118836 (55%)] Loss: 12324.909180\n",
      "Train Epoch: 1337 [98560/118836 (83%)] Loss: 12294.588867\n",
      "    epoch          : 1337\n",
      "    loss           : 12265.128405125619\n",
      "    val_loss       : 12263.46515703004\n",
      "    val_log_likelihood: -12166.626032296837\n",
      "    val_log_marginal: -12174.606617148951\n",
      "Train Epoch: 1338 [256/118836 (0%)] Loss: 12355.173828\n",
      "Train Epoch: 1338 [33024/118836 (28%)] Loss: 12473.379883\n",
      "Train Epoch: 1338 [65792/118836 (55%)] Loss: 12381.843750\n",
      "Train Epoch: 1338 [98560/118836 (83%)] Loss: 12261.827148\n",
      "    epoch          : 1338\n",
      "    loss           : 12264.063650873655\n",
      "    val_loss       : 12263.776393902248\n",
      "    val_log_likelihood: -12170.152209987593\n",
      "    val_log_marginal: -12178.11369460568\n",
      "Train Epoch: 1339 [256/118836 (0%)] Loss: 12268.300781\n",
      "Train Epoch: 1339 [33024/118836 (28%)] Loss: 12312.720703\n",
      "Train Epoch: 1339 [65792/118836 (55%)] Loss: 12258.848633\n",
      "Train Epoch: 1339 [98560/118836 (83%)] Loss: 12262.112305\n",
      "    epoch          : 1339\n",
      "    loss           : 12267.58637529725\n",
      "    val_loss       : 12267.892165549692\n",
      "    val_log_likelihood: -12167.21579074907\n",
      "    val_log_marginal: -12175.53720374798\n",
      "Train Epoch: 1340 [256/118836 (0%)] Loss: 12284.256836\n",
      "Train Epoch: 1340 [33024/118836 (28%)] Loss: 12277.261719\n",
      "Train Epoch: 1340 [65792/118836 (55%)] Loss: 12243.997070\n",
      "Train Epoch: 1340 [98560/118836 (83%)] Loss: 12311.839844\n",
      "    epoch          : 1340\n",
      "    loss           : 12268.800194343208\n",
      "    val_loss       : 12267.188198452453\n",
      "    val_log_likelihood: -12165.617995244003\n",
      "    val_log_marginal: -12173.565358568427\n",
      "Train Epoch: 1341 [256/118836 (0%)] Loss: 12238.005859\n",
      "Train Epoch: 1341 [33024/118836 (28%)] Loss: 12334.333984\n",
      "Train Epoch: 1341 [65792/118836 (55%)] Loss: 12289.104492\n",
      "Train Epoch: 1341 [98560/118836 (83%)] Loss: 12348.656250\n",
      "    epoch          : 1341\n",
      "    loss           : 12269.74338764604\n",
      "    val_loss       : 12266.03982317065\n",
      "    val_log_likelihood: -12166.877165561673\n",
      "    val_log_marginal: -12174.96872968859\n",
      "Train Epoch: 1342 [256/118836 (0%)] Loss: 12368.342773\n",
      "Train Epoch: 1342 [33024/118836 (28%)] Loss: 12209.792969\n",
      "Train Epoch: 1342 [65792/118836 (55%)] Loss: 12222.699219\n",
      "Train Epoch: 1342 [98560/118836 (83%)] Loss: 12330.973633\n",
      "    epoch          : 1342\n",
      "    loss           : 12268.122892918993\n",
      "    val_loss       : 12269.430965846008\n",
      "    val_log_likelihood: -12168.22359759486\n",
      "    val_log_marginal: -12176.226475213158\n",
      "Train Epoch: 1343 [256/118836 (0%)] Loss: 12222.310547\n",
      "Train Epoch: 1343 [33024/118836 (28%)] Loss: 12336.332031\n",
      "Train Epoch: 1343 [65792/118836 (55%)] Loss: 12309.683594\n",
      "Train Epoch: 1343 [98560/118836 (83%)] Loss: 12258.285156\n",
      "    epoch          : 1343\n",
      "    loss           : 12261.152202071702\n",
      "    val_loss       : 12266.55387885782\n",
      "    val_log_likelihood: -12167.998559630892\n",
      "    val_log_marginal: -12176.14752450362\n",
      "Train Epoch: 1344 [256/118836 (0%)] Loss: 12303.742188\n",
      "Train Epoch: 1344 [33024/118836 (28%)] Loss: 12274.051758\n",
      "Train Epoch: 1344 [65792/118836 (55%)] Loss: 12220.148438\n",
      "Train Epoch: 1344 [98560/118836 (83%)] Loss: 12365.114258\n",
      "    epoch          : 1344\n",
      "    loss           : 12266.93684281948\n",
      "    val_loss       : 12261.832193245144\n",
      "    val_log_likelihood: -12167.607378095274\n",
      "    val_log_marginal: -12175.976361138057\n",
      "Train Epoch: 1345 [256/118836 (0%)] Loss: 12289.726562\n",
      "Train Epoch: 1345 [33024/118836 (28%)] Loss: 12264.412109\n",
      "Train Epoch: 1345 [65792/118836 (55%)] Loss: 12308.693359\n",
      "Train Epoch: 1345 [98560/118836 (83%)] Loss: 12287.462891\n",
      "    epoch          : 1345\n",
      "    loss           : 12267.908797624586\n",
      "    val_loss       : 12264.387481743439\n",
      "    val_log_likelihood: -12165.645858696496\n",
      "    val_log_marginal: -12173.642465779718\n",
      "Train Epoch: 1346 [256/118836 (0%)] Loss: 12443.294922\n",
      "Train Epoch: 1346 [33024/118836 (28%)] Loss: 12228.490234\n",
      "Train Epoch: 1346 [65792/118836 (55%)] Loss: 12262.963867\n",
      "Train Epoch: 1346 [98560/118836 (83%)] Loss: 12303.164062\n",
      "    epoch          : 1346\n",
      "    loss           : 12268.330818018507\n",
      "    val_loss       : 12263.902300413005\n",
      "    val_log_likelihood: -12168.627064593673\n",
      "    val_log_marginal: -12176.685694106158\n",
      "Train Epoch: 1347 [256/118836 (0%)] Loss: 12330.560547\n",
      "Train Epoch: 1347 [33024/118836 (28%)] Loss: 12154.161133\n",
      "Train Epoch: 1347 [65792/118836 (55%)] Loss: 12151.275391\n",
      "Train Epoch: 1347 [98560/118836 (83%)] Loss: 12373.439453\n",
      "    epoch          : 1347\n",
      "    loss           : 12261.284957060328\n",
      "    val_loss       : 12265.796265034922\n",
      "    val_log_likelihood: -12165.974676094655\n",
      "    val_log_marginal: -12174.076426883488\n",
      "Train Epoch: 1348 [256/118836 (0%)] Loss: 12371.820312\n",
      "Train Epoch: 1348 [33024/118836 (28%)] Loss: 12312.092773\n",
      "Train Epoch: 1348 [65792/118836 (55%)] Loss: 12151.604492\n",
      "Train Epoch: 1348 [98560/118836 (83%)] Loss: 12359.950195\n",
      "    epoch          : 1348\n",
      "    loss           : 12267.11827876215\n",
      "    val_loss       : 12264.22573093132\n",
      "    val_log_likelihood: -12167.050771395523\n",
      "    val_log_marginal: -12175.03894542443\n",
      "Train Epoch: 1349 [256/118836 (0%)] Loss: 12429.376953\n",
      "Train Epoch: 1349 [33024/118836 (28%)] Loss: 12233.083008\n",
      "Train Epoch: 1349 [65792/118836 (55%)] Loss: 12279.738281\n",
      "Train Epoch: 1349 [98560/118836 (83%)] Loss: 12272.693359\n",
      "    epoch          : 1349\n",
      "    loss           : 12265.52180359543\n",
      "    val_loss       : 12262.882538902477\n",
      "    val_log_likelihood: -12165.409944459521\n",
      "    val_log_marginal: -12173.49392382512\n",
      "Train Epoch: 1350 [256/118836 (0%)] Loss: 12388.242188\n",
      "Train Epoch: 1350 [33024/118836 (28%)] Loss: 12213.214844\n",
      "Train Epoch: 1350 [65792/118836 (55%)] Loss: 12270.862305\n",
      "Train Epoch: 1350 [98560/118836 (83%)] Loss: 12300.411133\n",
      "    epoch          : 1350\n",
      "    loss           : 12263.105329171836\n",
      "    val_loss       : 12264.672677938095\n",
      "    val_log_likelihood: -12168.74064890922\n",
      "    val_log_marginal: -12176.76606988517\n",
      "Train Epoch: 1351 [256/118836 (0%)] Loss: 12268.910156\n",
      "Train Epoch: 1351 [33024/118836 (28%)] Loss: 12333.433594\n",
      "Train Epoch: 1351 [65792/118836 (55%)] Loss: 12366.914062\n",
      "Train Epoch: 1351 [98560/118836 (83%)] Loss: 12208.986328\n",
      "    epoch          : 1351\n",
      "    loss           : 12263.778512555573\n",
      "    val_loss       : 12264.068060494224\n",
      "    val_log_likelihood: -12168.92115158447\n",
      "    val_log_marginal: -12176.882761147192\n",
      "Train Epoch: 1352 [256/118836 (0%)] Loss: 12243.987305\n",
      "Train Epoch: 1352 [33024/118836 (28%)] Loss: 12302.939453\n",
      "Train Epoch: 1352 [65792/118836 (55%)] Loss: 12153.235352\n",
      "Train Epoch: 1352 [98560/118836 (83%)] Loss: 12195.238281\n",
      "    epoch          : 1352\n",
      "    loss           : 12263.638731486506\n",
      "    val_loss       : 12273.845537412311\n",
      "    val_log_likelihood: -12167.072571598428\n",
      "    val_log_marginal: -12175.2830758489\n",
      "Train Epoch: 1353 [256/118836 (0%)] Loss: 12192.814453\n",
      "Train Epoch: 1353 [33024/118836 (28%)] Loss: 12216.081055\n",
      "Train Epoch: 1353 [65792/118836 (55%)] Loss: 12273.509766\n",
      "Train Epoch: 1353 [98560/118836 (83%)] Loss: 12225.384766\n",
      "    epoch          : 1353\n",
      "    loss           : 12266.567014642782\n",
      "    val_loss       : 12267.654692532598\n",
      "    val_log_likelihood: -12167.523119410413\n",
      "    val_log_marginal: -12175.62111164398\n",
      "Train Epoch: 1354 [256/118836 (0%)] Loss: 12227.893555\n",
      "Train Epoch: 1354 [33024/118836 (28%)] Loss: 12292.638672\n",
      "Train Epoch: 1354 [65792/118836 (55%)] Loss: 12265.478516\n",
      "Train Epoch: 1354 [98560/118836 (83%)] Loss: 12398.916016\n",
      "    epoch          : 1354\n",
      "    loss           : 12269.043574396454\n",
      "    val_loss       : 12266.44477466499\n",
      "    val_log_likelihood: -12165.404195099256\n",
      "    val_log_marginal: -12173.420517171215\n",
      "Train Epoch: 1355 [256/118836 (0%)] Loss: 12350.632812\n",
      "Train Epoch: 1355 [33024/118836 (28%)] Loss: 12323.766602\n",
      "Train Epoch: 1355 [65792/118836 (55%)] Loss: 12324.109375\n",
      "Train Epoch: 1355 [98560/118836 (83%)] Loss: 12300.025391\n",
      "    epoch          : 1355\n",
      "    loss           : 12264.913428582506\n",
      "    val_loss       : 12262.712068624523\n",
      "    val_log_likelihood: -12168.355348557692\n",
      "    val_log_marginal: -12176.43825901904\n",
      "Train Epoch: 1356 [256/118836 (0%)] Loss: 12291.647461\n",
      "Train Epoch: 1356 [33024/118836 (28%)] Loss: 12222.619141\n",
      "Train Epoch: 1356 [65792/118836 (55%)] Loss: 12297.630859\n",
      "Train Epoch: 1356 [98560/118836 (83%)] Loss: 12241.423828\n",
      "    epoch          : 1356\n",
      "    loss           : 12269.147889688016\n",
      "    val_loss       : 12270.550554328218\n",
      "    val_log_likelihood: -12167.838783343674\n",
      "    val_log_marginal: -12175.897212845557\n",
      "Train Epoch: 1357 [256/118836 (0%)] Loss: 12374.228516\n",
      "Train Epoch: 1357 [33024/118836 (28%)] Loss: 12227.193359\n",
      "Train Epoch: 1357 [65792/118836 (55%)] Loss: 12393.089844\n",
      "Train Epoch: 1357 [98560/118836 (83%)] Loss: 12252.482422\n",
      "    epoch          : 1357\n",
      "    loss           : 12265.78990998501\n",
      "    val_loss       : 12267.542146128695\n",
      "    val_log_likelihood: -12164.896150938275\n",
      "    val_log_marginal: -12172.91224719125\n",
      "Train Epoch: 1358 [256/118836 (0%)] Loss: 12325.124023\n",
      "Train Epoch: 1358 [33024/118836 (28%)] Loss: 12344.957031\n",
      "Train Epoch: 1358 [65792/118836 (55%)] Loss: 12337.375000\n",
      "Train Epoch: 1358 [98560/118836 (83%)] Loss: 12358.765625\n",
      "    epoch          : 1358\n",
      "    loss           : 12266.81384020885\n",
      "    val_loss       : 12268.557124949057\n",
      "    val_log_likelihood: -12167.334101497881\n",
      "    val_log_marginal: -12175.374838657486\n",
      "Train Epoch: 1359 [256/118836 (0%)] Loss: 12324.162109\n",
      "Train Epoch: 1359 [33024/118836 (28%)] Loss: 12282.961914\n",
      "Train Epoch: 1359 [65792/118836 (55%)] Loss: 12355.426758\n",
      "Train Epoch: 1359 [98560/118836 (83%)] Loss: 12360.577148\n",
      "    epoch          : 1359\n",
      "    loss           : 12267.109669988109\n",
      "    val_loss       : 12264.8839859339\n",
      "    val_log_likelihood: -12167.979954540167\n",
      "    val_log_marginal: -12175.983482597961\n",
      "Train Epoch: 1360 [256/118836 (0%)] Loss: 12277.846680\n",
      "Train Epoch: 1360 [33024/118836 (28%)] Loss: 12323.322266\n",
      "Train Epoch: 1360 [65792/118836 (55%)] Loss: 12276.716797\n",
      "Train Epoch: 1360 [98560/118836 (83%)] Loss: 12293.613281\n",
      "    epoch          : 1360\n",
      "    loss           : 12266.855768907672\n",
      "    val_loss       : 12267.436409418793\n",
      "    val_log_likelihood: -12166.453504639681\n",
      "    val_log_marginal: -12174.509227462375\n",
      "Train Epoch: 1361 [256/118836 (0%)] Loss: 12308.330078\n",
      "Train Epoch: 1361 [33024/118836 (28%)] Loss: 12210.718750\n",
      "Train Epoch: 1361 [65792/118836 (55%)] Loss: 12205.363281\n",
      "Train Epoch: 1361 [98560/118836 (83%)] Loss: 12290.576172\n",
      "    epoch          : 1361\n",
      "    loss           : 12262.462914534222\n",
      "    val_loss       : 12261.707485190374\n",
      "    val_log_likelihood: -12168.9869140625\n",
      "    val_log_marginal: -12177.201768421874\n",
      "Train Epoch: 1362 [256/118836 (0%)] Loss: 12217.409180\n",
      "Train Epoch: 1362 [33024/118836 (28%)] Loss: 12237.539062\n",
      "Train Epoch: 1362 [65792/118836 (55%)] Loss: 12318.324219\n",
      "Train Epoch: 1362 [98560/118836 (83%)] Loss: 12245.833984\n",
      "    epoch          : 1362\n",
      "    loss           : 12265.708892453733\n",
      "    val_loss       : 12264.348555734583\n",
      "    val_log_likelihood: -12167.005386844758\n",
      "    val_log_marginal: -12175.057599854561\n",
      "Train Epoch: 1363 [256/118836 (0%)] Loss: 12205.053711\n",
      "Train Epoch: 1363 [33024/118836 (28%)] Loss: 12204.139648\n",
      "Train Epoch: 1363 [65792/118836 (55%)] Loss: 12240.277344\n",
      "Train Epoch: 1363 [98560/118836 (83%)] Loss: 12367.591797\n",
      "    epoch          : 1363\n",
      "    loss           : 12268.20013311621\n",
      "    val_loss       : 12262.183372889302\n",
      "    val_log_likelihood: -12165.660105362127\n",
      "    val_log_marginal: -12173.587937845667\n",
      "Train Epoch: 1364 [256/118836 (0%)] Loss: 12305.619141\n",
      "Train Epoch: 1364 [33024/118836 (28%)] Loss: 12242.050781\n",
      "Train Epoch: 1364 [65792/118836 (55%)] Loss: 12336.083984\n",
      "Train Epoch: 1364 [98560/118836 (83%)] Loss: 12427.764648\n",
      "    epoch          : 1364\n",
      "    loss           : 12262.712719383271\n",
      "    val_loss       : 12267.109024344325\n",
      "    val_log_likelihood: -12167.322540742607\n",
      "    val_log_marginal: -12175.324993414237\n",
      "Train Epoch: 1365 [256/118836 (0%)] Loss: 12202.734375\n",
      "Train Epoch: 1365 [33024/118836 (28%)] Loss: 12252.005859\n",
      "Train Epoch: 1365 [65792/118836 (55%)] Loss: 12233.643555\n",
      "Train Epoch: 1365 [98560/118836 (83%)] Loss: 12256.886719\n",
      "    epoch          : 1365\n",
      "    loss           : 12265.612421810381\n",
      "    val_loss       : 12266.725631319568\n",
      "    val_log_likelihood: -12165.56561950734\n",
      "    val_log_marginal: -12173.598718509209\n",
      "Train Epoch: 1366 [256/118836 (0%)] Loss: 12313.417969\n",
      "Train Epoch: 1366 [33024/118836 (28%)] Loss: 12414.686523\n",
      "Train Epoch: 1366 [65792/118836 (55%)] Loss: 12193.848633\n",
      "Train Epoch: 1366 [98560/118836 (83%)] Loss: 12302.808594\n",
      "    epoch          : 1366\n",
      "    loss           : 12265.428499147021\n",
      "    val_loss       : 12260.384272944188\n",
      "    val_log_likelihood: -12166.779344693445\n",
      "    val_log_marginal: -12174.708622919752\n",
      "Train Epoch: 1367 [256/118836 (0%)] Loss: 12310.270508\n",
      "Train Epoch: 1367 [33024/118836 (28%)] Loss: 12356.583008\n",
      "Train Epoch: 1367 [65792/118836 (55%)] Loss: 12227.566406\n",
      "Train Epoch: 1367 [98560/118836 (83%)] Loss: 12417.283203\n",
      "    epoch          : 1367\n",
      "    loss           : 12261.856128030655\n",
      "    val_loss       : 12267.529434182945\n",
      "    val_log_likelihood: -12165.845209593414\n",
      "    val_log_marginal: -12173.8380599268\n",
      "Train Epoch: 1368 [256/118836 (0%)] Loss: 12155.084961\n",
      "Train Epoch: 1368 [33024/118836 (28%)] Loss: 12358.936523\n",
      "Train Epoch: 1368 [65792/118836 (55%)] Loss: 12182.087891\n",
      "Train Epoch: 1368 [98560/118836 (83%)] Loss: 12236.605469\n",
      "    epoch          : 1368\n",
      "    loss           : 12263.59305227073\n",
      "    val_loss       : 12262.631222777452\n",
      "    val_log_likelihood: -12169.102445041099\n",
      "    val_log_marginal: -12177.170809454281\n",
      "Train Epoch: 1369 [256/118836 (0%)] Loss: 12258.411133\n",
      "Train Epoch: 1369 [33024/118836 (28%)] Loss: 12224.638672\n",
      "Train Epoch: 1369 [65792/118836 (55%)] Loss: 12238.130859\n",
      "Train Epoch: 1369 [98560/118836 (83%)] Loss: 12213.098633\n",
      "    epoch          : 1369\n",
      "    loss           : 12267.558652069118\n",
      "    val_loss       : 12265.305447936398\n",
      "    val_log_likelihood: -12168.201543114144\n",
      "    val_log_marginal: -12176.170617508353\n",
      "Train Epoch: 1370 [256/118836 (0%)] Loss: 12199.613281\n",
      "Train Epoch: 1370 [33024/118836 (28%)] Loss: 12341.794922\n",
      "Train Epoch: 1370 [65792/118836 (55%)] Loss: 12322.722656\n",
      "Train Epoch: 1370 [98560/118836 (83%)] Loss: 12191.248047\n",
      "    epoch          : 1370\n",
      "    loss           : 12267.932158388907\n",
      "    val_loss       : 12267.451022964948\n",
      "    val_log_likelihood: -12168.061450902089\n",
      "    val_log_marginal: -12176.339713083036\n",
      "Train Epoch: 1371 [256/118836 (0%)] Loss: 12293.312500\n",
      "Train Epoch: 1371 [33024/118836 (28%)] Loss: 12344.476562\n",
      "Train Epoch: 1371 [65792/118836 (55%)] Loss: 12226.725586\n",
      "Train Epoch: 1371 [98560/118836 (83%)] Loss: 12348.854492\n",
      "    epoch          : 1371\n",
      "    loss           : 12265.087580128204\n",
      "    val_loss       : 12265.557815682534\n",
      "    val_log_likelihood: -12168.665304325632\n",
      "    val_log_marginal: -12176.666698536394\n",
      "Train Epoch: 1372 [256/118836 (0%)] Loss: 12268.046875\n",
      "Train Epoch: 1372 [33024/118836 (28%)] Loss: 12343.327148\n",
      "Train Epoch: 1372 [65792/118836 (55%)] Loss: 12214.141602\n",
      "Train Epoch: 1372 [98560/118836 (83%)] Loss: 12346.667969\n",
      "    epoch          : 1372\n",
      "    loss           : 12267.654041950993\n",
      "    val_loss       : 12266.462534804232\n",
      "    val_log_likelihood: -12169.75347927652\n",
      "    val_log_marginal: -12177.659642627397\n",
      "Train Epoch: 1373 [256/118836 (0%)] Loss: 12384.869141\n",
      "Train Epoch: 1373 [33024/118836 (28%)] Loss: 12342.233398\n",
      "Train Epoch: 1373 [65792/118836 (55%)] Loss: 12327.303711\n",
      "Train Epoch: 1373 [98560/118836 (83%)] Loss: 12279.473633\n",
      "    epoch          : 1373\n",
      "    loss           : 12269.917527560225\n",
      "    val_loss       : 12264.5203239566\n",
      "    val_log_likelihood: -12167.05426440369\n",
      "    val_log_marginal: -12175.227791297548\n",
      "Train Epoch: 1374 [256/118836 (0%)] Loss: 12232.804688\n",
      "Train Epoch: 1374 [33024/118836 (28%)] Loss: 12253.934570\n",
      "Train Epoch: 1374 [65792/118836 (55%)] Loss: 12331.892578\n",
      "Train Epoch: 1374 [98560/118836 (83%)] Loss: 12337.798828\n",
      "    epoch          : 1374\n",
      "    loss           : 12266.550240223065\n",
      "    val_loss       : 12263.386969033734\n",
      "    val_log_likelihood: -12165.640932750466\n",
      "    val_log_marginal: -12173.613495417016\n",
      "Train Epoch: 1375 [256/118836 (0%)] Loss: 12339.101562\n",
      "Train Epoch: 1375 [33024/118836 (28%)] Loss: 12244.790039\n",
      "Train Epoch: 1375 [65792/118836 (55%)] Loss: 12374.724609\n",
      "Train Epoch: 1375 [98560/118836 (83%)] Loss: 12372.400391\n",
      "    epoch          : 1375\n",
      "    loss           : 12265.98516028872\n",
      "    val_loss       : 12263.939365964556\n",
      "    val_log_likelihood: -12168.239935994365\n",
      "    val_log_marginal: -12176.201517407018\n",
      "Train Epoch: 1376 [256/118836 (0%)] Loss: 12362.933594\n",
      "Train Epoch: 1376 [33024/118836 (28%)] Loss: 12358.527344\n",
      "Train Epoch: 1376 [65792/118836 (55%)] Loss: 12246.296875\n",
      "Train Epoch: 1376 [98560/118836 (83%)] Loss: 12271.286133\n",
      "    epoch          : 1376\n",
      "    loss           : 12263.910331045801\n",
      "    val_loss       : 12266.210829509355\n",
      "    val_log_likelihood: -12167.612727299162\n",
      "    val_log_marginal: -12175.534010886147\n",
      "Train Epoch: 1377 [256/118836 (0%)] Loss: 12264.203125\n",
      "Train Epoch: 1377 [33024/118836 (28%)] Loss: 12189.958984\n",
      "Train Epoch: 1377 [65792/118836 (55%)] Loss: 12231.513672\n",
      "Train Epoch: 1377 [98560/118836 (83%)] Loss: 12340.002930\n",
      "    epoch          : 1377\n",
      "    loss           : 12265.29922408111\n",
      "    val_loss       : 12268.85012125634\n",
      "    val_log_likelihood: -12166.633662246692\n",
      "    val_log_marginal: -12174.754808886128\n",
      "Train Epoch: 1378 [256/118836 (0%)] Loss: 12327.491211\n",
      "Train Epoch: 1378 [33024/118836 (28%)] Loss: 12199.227539\n",
      "Train Epoch: 1378 [65792/118836 (55%)] Loss: 12325.105469\n",
      "Train Epoch: 1378 [98560/118836 (83%)] Loss: 12238.601562\n",
      "    epoch          : 1378\n",
      "    loss           : 12264.584652056192\n",
      "    val_loss       : 12267.93151246554\n",
      "    val_log_likelihood: -12165.153479114972\n",
      "    val_log_marginal: -12173.1176357239\n",
      "Train Epoch: 1379 [256/118836 (0%)] Loss: 12360.490234\n",
      "Train Epoch: 1379 [33024/118836 (28%)] Loss: 12414.417969\n",
      "Train Epoch: 1379 [65792/118836 (55%)] Loss: 12431.211914\n",
      "Train Epoch: 1379 [98560/118836 (83%)] Loss: 12189.193359\n",
      "    epoch          : 1379\n",
      "    loss           : 12265.141744048544\n",
      "    val_loss       : 12269.880820148703\n",
      "    val_log_likelihood: -12166.504033712003\n",
      "    val_log_marginal: -12174.461053850871\n",
      "Train Epoch: 1380 [256/118836 (0%)] Loss: 12369.540039\n",
      "Train Epoch: 1380 [33024/118836 (28%)] Loss: 12366.189453\n",
      "Train Epoch: 1380 [65792/118836 (55%)] Loss: 12333.730469\n",
      "Train Epoch: 1380 [98560/118836 (83%)] Loss: 12228.699219\n",
      "    epoch          : 1380\n",
      "    loss           : 12260.89536516491\n",
      "    val_loss       : 12269.344181242102\n",
      "    val_log_likelihood: -12166.864821294717\n",
      "    val_log_marginal: -12175.001233679632\n",
      "Train Epoch: 1381 [256/118836 (0%)] Loss: 12334.030273\n",
      "Train Epoch: 1381 [33024/118836 (28%)] Loss: 12332.458984\n",
      "Train Epoch: 1381 [65792/118836 (55%)] Loss: 12256.541992\n",
      "Train Epoch: 1381 [98560/118836 (83%)] Loss: 12274.365234\n",
      "    epoch          : 1381\n",
      "    loss           : 12265.683435755273\n",
      "    val_loss       : 12264.370669435384\n",
      "    val_log_likelihood: -12166.59975493047\n",
      "    val_log_marginal: -12174.629872691936\n",
      "Train Epoch: 1382 [256/118836 (0%)] Loss: 12337.802734\n",
      "Train Epoch: 1382 [33024/118836 (28%)] Loss: 12335.968750\n",
      "Train Epoch: 1382 [65792/118836 (55%)] Loss: 12334.175781\n",
      "Train Epoch: 1382 [98560/118836 (83%)] Loss: 12215.083984\n",
      "    epoch          : 1382\n",
      "    loss           : 12260.239221948666\n",
      "    val_loss       : 12267.734475790676\n",
      "    val_log_likelihood: -12166.642743712522\n",
      "    val_log_marginal: -12174.789981175547\n",
      "Train Epoch: 1383 [256/118836 (0%)] Loss: 12220.085938\n",
      "Train Epoch: 1383 [33024/118836 (28%)] Loss: 12205.407227\n",
      "Train Epoch: 1383 [65792/118836 (55%)] Loss: 12202.887695\n",
      "Train Epoch: 1383 [98560/118836 (83%)] Loss: 12233.454102\n",
      "    epoch          : 1383\n",
      "    loss           : 12264.61396960944\n",
      "    val_loss       : 12258.359897906079\n",
      "    val_log_likelihood: -12166.100737954921\n",
      "    val_log_marginal: -12174.116842436704\n",
      "Train Epoch: 1384 [256/118836 (0%)] Loss: 12376.063477\n",
      "Train Epoch: 1384 [33024/118836 (28%)] Loss: 12211.908203\n",
      "Train Epoch: 1384 [65792/118836 (55%)] Loss: 12300.026367\n",
      "Train Epoch: 1384 [98560/118836 (83%)] Loss: 12250.538086\n",
      "    epoch          : 1384\n",
      "    loss           : 12260.092293152917\n",
      "    val_loss       : 12262.89772787855\n",
      "    val_log_likelihood: -12168.36915952621\n",
      "    val_log_marginal: -12176.288316228374\n",
      "Train Epoch: 1385 [256/118836 (0%)] Loss: 12285.321289\n",
      "Train Epoch: 1385 [33024/118836 (28%)] Loss: 12328.204102\n",
      "Train Epoch: 1385 [65792/118836 (55%)] Loss: 12300.251953\n",
      "Train Epoch: 1385 [98560/118836 (83%)] Loss: 12350.876953\n",
      "    epoch          : 1385\n",
      "    loss           : 12268.964406437397\n",
      "    val_loss       : 12266.638941116382\n",
      "    val_log_likelihood: -12166.45981796681\n",
      "    val_log_marginal: -12174.464761752719\n",
      "Train Epoch: 1386 [256/118836 (0%)] Loss: 12257.238281\n",
      "Train Epoch: 1386 [33024/118836 (28%)] Loss: 12355.723633\n",
      "Train Epoch: 1386 [65792/118836 (55%)] Loss: 12203.851562\n",
      "Train Epoch: 1386 [98560/118836 (83%)] Loss: 12216.264648\n",
      "    epoch          : 1386\n",
      "    loss           : 12266.8870352241\n",
      "    val_loss       : 12265.509374656609\n",
      "    val_log_likelihood: -12167.914202401262\n",
      "    val_log_marginal: -12176.145318914785\n",
      "Train Epoch: 1387 [256/118836 (0%)] Loss: 12294.265625\n",
      "Train Epoch: 1387 [33024/118836 (28%)] Loss: 12281.523438\n",
      "Train Epoch: 1387 [65792/118836 (55%)] Loss: 12296.116211\n",
      "Train Epoch: 1387 [98560/118836 (83%)] Loss: 12380.322266\n",
      "    epoch          : 1387\n",
      "    loss           : 12270.592605426747\n",
      "    val_loss       : 12268.495037490136\n",
      "    val_log_likelihood: -12167.524268345484\n",
      "    val_log_marginal: -12175.829399679396\n",
      "Train Epoch: 1388 [256/118836 (0%)] Loss: 12233.243164\n",
      "Train Epoch: 1388 [33024/118836 (28%)] Loss: 12360.604492\n",
      "Train Epoch: 1388 [65792/118836 (55%)] Loss: 12273.818359\n",
      "Train Epoch: 1388 [98560/118836 (83%)] Loss: 12160.148438\n",
      "    epoch          : 1388\n",
      "    loss           : 12266.464802555054\n",
      "    val_loss       : 12270.220016549849\n",
      "    val_log_likelihood: -12164.340767162945\n",
      "    val_log_marginal: -12172.617744049685\n",
      "Train Epoch: 1389 [256/118836 (0%)] Loss: 12258.210938\n",
      "Train Epoch: 1389 [33024/118836 (28%)] Loss: 12342.828125\n",
      "Train Epoch: 1389 [65792/118836 (55%)] Loss: 12373.664062\n",
      "Train Epoch: 1389 [98560/118836 (83%)] Loss: 12228.646484\n",
      "    epoch          : 1389\n",
      "    loss           : 12266.35943719629\n",
      "    val_loss       : 12259.69760079275\n",
      "    val_log_likelihood: -12164.476575747001\n",
      "    val_log_marginal: -12172.606738719243\n",
      "Train Epoch: 1390 [256/118836 (0%)] Loss: 12225.363281\n",
      "Train Epoch: 1390 [33024/118836 (28%)] Loss: 12344.892578\n",
      "Train Epoch: 1390 [65792/118836 (55%)] Loss: 12287.657227\n",
      "Train Epoch: 1390 [98560/118836 (83%)] Loss: 12244.640625\n",
      "    epoch          : 1390\n",
      "    loss           : 12262.742339678969\n",
      "    val_loss       : 12257.934017716698\n",
      "    val_log_likelihood: -12166.508428485577\n",
      "    val_log_marginal: -12174.543722910435\n",
      "Train Epoch: 1391 [256/118836 (0%)] Loss: 12170.101562\n",
      "Train Epoch: 1391 [33024/118836 (28%)] Loss: 12222.823242\n",
      "Train Epoch: 1391 [65792/118836 (55%)] Loss: 12207.717773\n",
      "Train Epoch: 1391 [98560/118836 (83%)] Loss: 12258.513672\n",
      "    epoch          : 1391\n",
      "    loss           : 12266.536657846102\n",
      "    val_loss       : 12267.367027735541\n",
      "    val_log_likelihood: -12165.422155933364\n",
      "    val_log_marginal: -12173.531481033893\n",
      "Train Epoch: 1392 [256/118836 (0%)] Loss: 12339.853516\n",
      "Train Epoch: 1392 [33024/118836 (28%)] Loss: 12331.088867\n",
      "Train Epoch: 1392 [65792/118836 (55%)] Loss: 12243.999023\n",
      "Train Epoch: 1392 [98560/118836 (83%)] Loss: 12205.855469\n",
      "    epoch          : 1392\n",
      "    loss           : 12271.168231912996\n",
      "    val_loss       : 12269.646206691905\n",
      "    val_log_likelihood: -12166.708801986404\n",
      "    val_log_marginal: -12174.819749327924\n",
      "Train Epoch: 1393 [256/118836 (0%)] Loss: 12149.803711\n",
      "Train Epoch: 1393 [33024/118836 (28%)] Loss: 12217.121094\n",
      "Train Epoch: 1393 [65792/118836 (55%)] Loss: 12268.855469\n",
      "Train Epoch: 1393 [98560/118836 (83%)] Loss: 12359.966797\n",
      "    epoch          : 1393\n",
      "    loss           : 12262.61640495761\n",
      "    val_loss       : 12263.543074851194\n",
      "    val_log_likelihood: -12164.96431580852\n",
      "    val_log_marginal: -12173.032269674666\n",
      "Train Epoch: 1394 [256/118836 (0%)] Loss: 12184.318359\n",
      "Train Epoch: 1394 [33024/118836 (28%)] Loss: 12330.016602\n",
      "Train Epoch: 1394 [65792/118836 (55%)] Loss: 12397.503906\n",
      "Train Epoch: 1394 [98560/118836 (83%)] Loss: 12173.177734\n",
      "    epoch          : 1394\n",
      "    loss           : 12267.022619416874\n",
      "    val_loss       : 12273.382786134036\n",
      "    val_log_likelihood: -12165.45676453293\n",
      "    val_log_marginal: -12173.473416894123\n",
      "Train Epoch: 1395 [256/118836 (0%)] Loss: 12311.417969\n",
      "Train Epoch: 1395 [33024/118836 (28%)] Loss: 12385.800781\n",
      "Train Epoch: 1395 [65792/118836 (55%)] Loss: 12299.048828\n",
      "Train Epoch: 1395 [98560/118836 (83%)] Loss: 12429.867188\n",
      "    epoch          : 1395\n",
      "    loss           : 12296.18560616341\n",
      "    val_loss       : 12289.366613977501\n",
      "    val_log_likelihood: -12170.238759111353\n",
      "    val_log_marginal: -12178.519775400768\n",
      "Train Epoch: 1396 [256/118836 (0%)] Loss: 12304.067383\n",
      "Train Epoch: 1396 [33024/118836 (28%)] Loss: 12376.730469\n",
      "Train Epoch: 1396 [65792/118836 (55%)] Loss: 12220.035156\n",
      "Train Epoch: 1396 [98560/118836 (83%)] Loss: 12331.949219\n",
      "    epoch          : 1396\n",
      "    loss           : 12286.741265540995\n",
      "    val_loss       : 12286.355072040908\n",
      "    val_log_likelihood: -12170.58870305392\n",
      "    val_log_marginal: -12178.52819489115\n",
      "Train Epoch: 1397 [256/118836 (0%)] Loss: 12310.902344\n",
      "Train Epoch: 1397 [33024/118836 (28%)] Loss: 12380.916016\n",
      "Train Epoch: 1397 [65792/118836 (55%)] Loss: 12466.696289\n",
      "Train Epoch: 1397 [98560/118836 (83%)] Loss: 12200.151367\n",
      "    epoch          : 1397\n",
      "    loss           : 12287.875994332868\n",
      "    val_loss       : 12281.826949756049\n",
      "    val_log_likelihood: -12168.713048942824\n",
      "    val_log_marginal: -12176.957276158688\n",
      "Train Epoch: 1398 [256/118836 (0%)] Loss: 12419.259766\n",
      "Train Epoch: 1398 [33024/118836 (28%)] Loss: 12242.496094\n",
      "Train Epoch: 1398 [65792/118836 (55%)] Loss: 12212.128906\n",
      "Train Epoch: 1398 [98560/118836 (83%)] Loss: 12205.035156\n",
      "    epoch          : 1398\n",
      "    loss           : 12279.92252684941\n",
      "    val_loss       : 12273.04005998949\n",
      "    val_log_likelihood: -12167.322409241882\n",
      "    val_log_marginal: -12175.683604037613\n",
      "Train Epoch: 1399 [256/118836 (0%)] Loss: 12256.002930\n",
      "Train Epoch: 1399 [33024/118836 (28%)] Loss: 12249.418945\n",
      "Train Epoch: 1399 [65792/118836 (55%)] Loss: 12329.359375\n",
      "Train Epoch: 1399 [98560/118836 (83%)] Loss: 12278.363281\n",
      "    epoch          : 1399\n",
      "    loss           : 12274.549264952957\n",
      "    val_loss       : 12267.637420815325\n",
      "    val_log_likelihood: -12169.374172062397\n",
      "    val_log_marginal: -12177.462339078194\n",
      "Train Epoch: 1400 [256/118836 (0%)] Loss: 12263.142578\n",
      "Train Epoch: 1400 [33024/118836 (28%)] Loss: 12267.108398\n",
      "Train Epoch: 1400 [65792/118836 (55%)] Loss: 12267.033203\n",
      "Train Epoch: 1400 [98560/118836 (83%)] Loss: 12211.375977\n",
      "    epoch          : 1400\n",
      "    loss           : 12268.224798387097\n",
      "    val_loss       : 12267.043587275492\n",
      "    val_log_likelihood: -12168.814602880739\n",
      "    val_log_marginal: -12177.003019521326\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch1400.pth ...\n",
      "Train Epoch: 1401 [256/118836 (0%)] Loss: 12238.857422\n",
      "Train Epoch: 1401 [33024/118836 (28%)] Loss: 12209.228516\n",
      "Train Epoch: 1401 [65792/118836 (55%)] Loss: 12218.937500\n",
      "Train Epoch: 1401 [98560/118836 (83%)] Loss: 12247.107422\n",
      "    epoch          : 1401\n",
      "    loss           : 12270.600199189672\n",
      "    val_loss       : 12272.46908517841\n",
      "    val_log_likelihood: -12166.86273812293\n",
      "    val_log_marginal: -12175.023161225932\n",
      "Train Epoch: 1402 [256/118836 (0%)] Loss: 12272.009766\n",
      "Train Epoch: 1402 [33024/118836 (28%)] Loss: 12256.938477\n",
      "Train Epoch: 1402 [65792/118836 (55%)] Loss: 12327.058594\n",
      "Train Epoch: 1402 [98560/118836 (83%)] Loss: 12382.708984\n",
      "    epoch          : 1402\n",
      "    loss           : 12270.474271576457\n",
      "    val_loss       : 12272.83654576445\n",
      "    val_log_likelihood: -12170.281058564671\n",
      "    val_log_marginal: -12178.697638176953\n",
      "Train Epoch: 1403 [256/118836 (0%)] Loss: 12354.460938\n",
      "Train Epoch: 1403 [33024/118836 (28%)] Loss: 12161.736328\n",
      "Train Epoch: 1403 [65792/118836 (55%)] Loss: 12421.109375\n",
      "Train Epoch: 1403 [98560/118836 (83%)] Loss: 12264.075195\n",
      "    epoch          : 1403\n",
      "    loss           : 12269.449269637873\n",
      "    val_loss       : 12263.295914766188\n",
      "    val_log_likelihood: -12165.337652502069\n",
      "    val_log_marginal: -12173.256391137933\n",
      "Train Epoch: 1404 [256/118836 (0%)] Loss: 12210.778320\n",
      "Train Epoch: 1404 [33024/118836 (28%)] Loss: 12292.824219\n",
      "Train Epoch: 1404 [65792/118836 (55%)] Loss: 12312.255859\n",
      "Train Epoch: 1404 [98560/118836 (83%)] Loss: 12158.591797\n",
      "    epoch          : 1404\n",
      "    loss           : 12266.886215363938\n",
      "    val_loss       : 12268.118916829866\n",
      "    val_log_likelihood: -12165.903112399194\n",
      "    val_log_marginal: -12173.833677298064\n",
      "Train Epoch: 1405 [256/118836 (0%)] Loss: 12201.025391\n",
      "Train Epoch: 1405 [33024/118836 (28%)] Loss: 12224.315430\n",
      "Train Epoch: 1405 [65792/118836 (55%)] Loss: 12310.397461\n",
      "Train Epoch: 1405 [98560/118836 (83%)] Loss: 12322.265625\n",
      "    epoch          : 1405\n",
      "    loss           : 12266.119725593207\n",
      "    val_loss       : 12266.00803687918\n",
      "    val_log_likelihood: -12165.171308125258\n",
      "    val_log_marginal: -12173.087573816438\n",
      "Train Epoch: 1406 [256/118836 (0%)] Loss: 12251.434570\n",
      "Train Epoch: 1406 [33024/118836 (28%)] Loss: 12248.371094\n",
      "Train Epoch: 1406 [65792/118836 (55%)] Loss: 12236.178711\n",
      "Train Epoch: 1406 [98560/118836 (83%)] Loss: 12316.256836\n",
      "    epoch          : 1406\n",
      "    loss           : 12268.469079074906\n",
      "    val_loss       : 12264.726751772963\n",
      "    val_log_likelihood: -12164.282270826872\n",
      "    val_log_marginal: -12172.195690815448\n",
      "Train Epoch: 1407 [256/118836 (0%)] Loss: 12307.369141\n",
      "Train Epoch: 1407 [33024/118836 (28%)] Loss: 12299.000000\n",
      "Train Epoch: 1407 [65792/118836 (55%)] Loss: 12193.115234\n",
      "Train Epoch: 1407 [98560/118836 (83%)] Loss: 12216.870117\n",
      "    epoch          : 1407\n",
      "    loss           : 12265.023454301076\n",
      "    val_loss       : 12267.714223110357\n",
      "    val_log_likelihood: -12164.48774652347\n",
      "    val_log_marginal: -12172.491306887052\n",
      "Train Epoch: 1408 [256/118836 (0%)] Loss: 12283.582031\n",
      "Train Epoch: 1408 [33024/118836 (28%)] Loss: 12232.757812\n",
      "Train Epoch: 1408 [65792/118836 (55%)] Loss: 12333.276367\n",
      "Train Epoch: 1408 [98560/118836 (83%)] Loss: 12361.536133\n",
      "    epoch          : 1408\n",
      "    loss           : 12265.120480349204\n",
      "    val_loss       : 12264.795930738775\n",
      "    val_log_likelihood: -12167.719257101686\n",
      "    val_log_marginal: -12175.894581457604\n",
      "Train Epoch: 1409 [256/118836 (0%)] Loss: 12381.917969\n",
      "Train Epoch: 1409 [33024/118836 (28%)] Loss: 12283.708984\n",
      "Train Epoch: 1409 [65792/118836 (55%)] Loss: 12246.400391\n",
      "Train Epoch: 1409 [98560/118836 (83%)] Loss: 12255.824219\n",
      "    epoch          : 1409\n",
      "    loss           : 12262.20684934605\n",
      "    val_loss       : 12265.554510228221\n",
      "    val_log_likelihood: -12167.157137064463\n",
      "    val_log_marginal: -12175.28416534066\n",
      "Train Epoch: 1410 [256/118836 (0%)] Loss: 12301.640625\n",
      "Train Epoch: 1410 [33024/118836 (28%)] Loss: 12253.179688\n",
      "Train Epoch: 1410 [65792/118836 (55%)] Loss: 12315.322266\n",
      "Train Epoch: 1410 [98560/118836 (83%)] Loss: 12401.951172\n",
      "    epoch          : 1410\n",
      "    loss           : 12269.672570629136\n",
      "    val_loss       : 12263.098093333647\n",
      "    val_log_likelihood: -12164.945810716501\n",
      "    val_log_marginal: -12173.010038643057\n",
      "Train Epoch: 1411 [256/118836 (0%)] Loss: 12246.823242\n",
      "Train Epoch: 1411 [33024/118836 (28%)] Loss: 12205.978516\n",
      "Train Epoch: 1411 [65792/118836 (55%)] Loss: 12230.672852\n",
      "Train Epoch: 1411 [98560/118836 (83%)] Loss: 12392.180664\n",
      "    epoch          : 1411\n",
      "    loss           : 12264.17502552471\n",
      "    val_loss       : 12262.982585715697\n",
      "    val_log_likelihood: -12165.36738830516\n",
      "    val_log_marginal: -12173.383576968443\n",
      "Train Epoch: 1412 [256/118836 (0%)] Loss: 12214.291992\n",
      "Train Epoch: 1412 [33024/118836 (28%)] Loss: 12260.083008\n",
      "Train Epoch: 1412 [65792/118836 (55%)] Loss: 12334.175781\n",
      "Train Epoch: 1412 [98560/118836 (83%)] Loss: 12228.861328\n",
      "    epoch          : 1412\n",
      "    loss           : 12265.683538177213\n",
      "    val_loss       : 12266.954473104566\n",
      "    val_log_likelihood: -12169.37331876163\n",
      "    val_log_marginal: -12177.73739785514\n",
      "Train Epoch: 1413 [256/118836 (0%)] Loss: 12345.895508\n",
      "Train Epoch: 1413 [33024/118836 (28%)] Loss: 12335.974609\n",
      "Train Epoch: 1413 [65792/118836 (55%)] Loss: 12263.239258\n",
      "Train Epoch: 1413 [98560/118836 (83%)] Loss: 12306.613281\n",
      "    epoch          : 1413\n",
      "    loss           : 12269.587806458074\n",
      "    val_loss       : 12261.457922990605\n",
      "    val_log_likelihood: -12163.602669432383\n",
      "    val_log_marginal: -12171.704253281998\n",
      "Train Epoch: 1414 [256/118836 (0%)] Loss: 12316.084961\n",
      "Train Epoch: 1414 [33024/118836 (28%)] Loss: 12214.349609\n",
      "Train Epoch: 1414 [65792/118836 (55%)] Loss: 12315.619141\n",
      "Train Epoch: 1414 [98560/118836 (83%)] Loss: 12315.116211\n",
      "    epoch          : 1414\n",
      "    loss           : 12265.924030545646\n",
      "    val_loss       : 12264.653883296363\n",
      "    val_log_likelihood: -12167.17093607837\n",
      "    val_log_marginal: -12175.378000492265\n",
      "Train Epoch: 1415 [256/118836 (0%)] Loss: 12403.333984\n",
      "Train Epoch: 1415 [33024/118836 (28%)] Loss: 12222.885742\n",
      "Train Epoch: 1415 [65792/118836 (55%)] Loss: 12276.571289\n",
      "Train Epoch: 1415 [98560/118836 (83%)] Loss: 12432.151367\n",
      "    epoch          : 1415\n",
      "    loss           : 12264.003199797096\n",
      "    val_loss       : 12260.222068716701\n",
      "    val_log_likelihood: -12167.839891245349\n",
      "    val_log_marginal: -12175.993368420022\n",
      "Train Epoch: 1416 [256/118836 (0%)] Loss: 12288.992188\n",
      "Train Epoch: 1416 [33024/118836 (28%)] Loss: 12225.125977\n",
      "Train Epoch: 1416 [65792/118836 (55%)] Loss: 12221.802734\n",
      "Train Epoch: 1416 [98560/118836 (83%)] Loss: 12335.298828\n",
      "    epoch          : 1416\n",
      "    loss           : 12267.40549443626\n",
      "    val_loss       : 12264.791657361315\n",
      "    val_log_likelihood: -12168.860300836177\n",
      "    val_log_marginal: -12177.09970308072\n",
      "Train Epoch: 1417 [256/118836 (0%)] Loss: 12243.064453\n",
      "Train Epoch: 1417 [33024/118836 (28%)] Loss: 12200.041016\n",
      "Train Epoch: 1417 [65792/118836 (55%)] Loss: 12300.426758\n",
      "Train Epoch: 1417 [98560/118836 (83%)] Loss: 12264.232422\n",
      "    epoch          : 1417\n",
      "    loss           : 12261.39673461409\n",
      "    val_loss       : 12267.466372047978\n",
      "    val_log_likelihood: -12165.246542047922\n",
      "    val_log_marginal: -12173.590980663701\n",
      "Train Epoch: 1418 [256/118836 (0%)] Loss: 12254.144531\n",
      "Train Epoch: 1418 [33024/118836 (28%)] Loss: 12254.706055\n",
      "Train Epoch: 1418 [65792/118836 (55%)] Loss: 12265.591797\n",
      "Train Epoch: 1418 [98560/118836 (83%)] Loss: 12303.447266\n",
      "    epoch          : 1418\n",
      "    loss           : 12266.04034600522\n",
      "    val_loss       : 12266.7242779902\n",
      "    val_log_likelihood: -12166.044030287168\n",
      "    val_log_marginal: -12174.12973304452\n",
      "Train Epoch: 1419 [256/118836 (0%)] Loss: 12350.426758\n",
      "Train Epoch: 1419 [33024/118836 (28%)] Loss: 12298.115234\n",
      "Train Epoch: 1419 [65792/118836 (55%)] Loss: 12248.748047\n",
      "Train Epoch: 1419 [98560/118836 (83%)] Loss: 12283.230469\n",
      "    epoch          : 1419\n",
      "    loss           : 12268.5754379588\n",
      "    val_loss       : 12265.913852524267\n",
      "    val_log_likelihood: -12165.963669936415\n",
      "    val_log_marginal: -12174.34531761195\n",
      "Train Epoch: 1420 [256/118836 (0%)] Loss: 12496.818359\n",
      "Train Epoch: 1420 [33024/118836 (28%)] Loss: 12232.492188\n",
      "Train Epoch: 1420 [65792/118836 (55%)] Loss: 12274.695312\n",
      "Train Epoch: 1420 [98560/118836 (83%)] Loss: 12337.154297\n",
      "    epoch          : 1420\n",
      "    loss           : 12268.59577307563\n",
      "    val_loss       : 12258.891239959445\n",
      "    val_log_likelihood: -12167.568620922508\n",
      "    val_log_marginal: -12175.733834500346\n",
      "Train Epoch: 1421 [256/118836 (0%)] Loss: 12280.868164\n",
      "Train Epoch: 1421 [33024/118836 (28%)] Loss: 12256.097656\n",
      "Train Epoch: 1421 [65792/118836 (55%)] Loss: 12221.376953\n",
      "Train Epoch: 1421 [98560/118836 (83%)] Loss: 12250.320312\n",
      "    epoch          : 1421\n",
      "    loss           : 12268.766548251397\n",
      "    val_loss       : 12268.071813324847\n",
      "    val_log_likelihood: -12165.132145141904\n",
      "    val_log_marginal: -12173.18408695513\n",
      "Train Epoch: 1422 [256/118836 (0%)] Loss: 12288.737305\n",
      "Train Epoch: 1422 [33024/118836 (28%)] Loss: 12313.739258\n",
      "Train Epoch: 1422 [65792/118836 (55%)] Loss: 12240.810547\n",
      "Train Epoch: 1422 [98560/118836 (83%)] Loss: 12370.849609\n",
      "    epoch          : 1422\n",
      "    loss           : 12261.243479405759\n",
      "    val_loss       : 12261.902368306222\n",
      "    val_log_likelihood: -12166.2619475031\n",
      "    val_log_marginal: -12174.478659635612\n",
      "Train Epoch: 1423 [256/118836 (0%)] Loss: 12343.937500\n",
      "Train Epoch: 1423 [33024/118836 (28%)] Loss: 12275.320312\n",
      "Train Epoch: 1423 [65792/118836 (55%)] Loss: 12296.922852\n",
      "Train Epoch: 1423 [98560/118836 (83%)] Loss: 12253.994141\n",
      "    epoch          : 1423\n",
      "    loss           : 12266.10265893171\n",
      "    val_loss       : 12269.229914778247\n",
      "    val_log_likelihood: -12166.9616445991\n",
      "    val_log_marginal: -12175.238350844791\n",
      "Train Epoch: 1424 [256/118836 (0%)] Loss: 12201.294922\n",
      "Train Epoch: 1424 [33024/118836 (28%)] Loss: 12332.328125\n",
      "Train Epoch: 1424 [65792/118836 (55%)] Loss: 12407.990234\n",
      "Train Epoch: 1424 [98560/118836 (83%)] Loss: 12275.570312\n",
      "    epoch          : 1424\n",
      "    loss           : 12271.010385003101\n",
      "    val_loss       : 12270.179163856428\n",
      "    val_log_likelihood: -12166.466740656017\n",
      "    val_log_marginal: -12174.746879825268\n",
      "Train Epoch: 1425 [256/118836 (0%)] Loss: 12200.298828\n",
      "Train Epoch: 1425 [33024/118836 (28%)] Loss: 12215.480469\n",
      "Train Epoch: 1425 [65792/118836 (55%)] Loss: 12326.611328\n",
      "Train Epoch: 1425 [98560/118836 (83%)] Loss: 12326.051758\n",
      "    epoch          : 1425\n",
      "    loss           : 12263.374999192258\n",
      "    val_loss       : 12264.598407662941\n",
      "    val_log_likelihood: -12167.487031993123\n",
      "    val_log_marginal: -12175.565399062509\n",
      "Train Epoch: 1426 [256/118836 (0%)] Loss: 12297.285156\n",
      "Train Epoch: 1426 [33024/118836 (28%)] Loss: 12313.796875\n",
      "Train Epoch: 1426 [65792/118836 (55%)] Loss: 12458.029297\n",
      "Train Epoch: 1426 [98560/118836 (83%)] Loss: 12284.677734\n",
      "    epoch          : 1426\n",
      "    loss           : 12261.882339162015\n",
      "    val_loss       : 12267.247802837637\n",
      "    val_log_likelihood: -12166.053414333903\n",
      "    val_log_marginal: -12174.097478380045\n",
      "Train Epoch: 1427 [256/118836 (0%)] Loss: 12226.833984\n",
      "Train Epoch: 1427 [33024/118836 (28%)] Loss: 12230.426758\n",
      "Train Epoch: 1427 [65792/118836 (55%)] Loss: 12248.709961\n",
      "Train Epoch: 1427 [98560/118836 (83%)] Loss: 12248.500000\n",
      "    epoch          : 1427\n",
      "    loss           : 12264.37769253386\n",
      "    val_loss       : 12264.894942042387\n",
      "    val_log_likelihood: -12168.576148450426\n",
      "    val_log_marginal: -12177.013027281735\n",
      "Train Epoch: 1428 [256/118836 (0%)] Loss: 12251.564453\n",
      "Train Epoch: 1428 [33024/118836 (28%)] Loss: 12225.151367\n",
      "Train Epoch: 1428 [65792/118836 (55%)] Loss: 12206.939453\n",
      "Train Epoch: 1428 [98560/118836 (83%)] Loss: 12274.003906\n",
      "    epoch          : 1428\n",
      "    loss           : 12265.409900033603\n",
      "    val_loss       : 12262.17514623593\n",
      "    val_log_likelihood: -12165.688788674785\n",
      "    val_log_marginal: -12173.670992540372\n",
      "Train Epoch: 1429 [256/118836 (0%)] Loss: 12433.416016\n",
      "Train Epoch: 1429 [33024/118836 (28%)] Loss: 12257.760742\n",
      "Train Epoch: 1429 [65792/118836 (55%)] Loss: 12341.735352\n",
      "Train Epoch: 1429 [98560/118836 (83%)] Loss: 12263.550781\n",
      "    epoch          : 1429\n",
      "    loss           : 12264.118933519436\n",
      "    val_loss       : 12267.900587153672\n",
      "    val_log_likelihood: -12169.70547828138\n",
      "    val_log_marginal: -12178.148182039673\n",
      "Train Epoch: 1430 [256/118836 (0%)] Loss: 12324.955078\n",
      "Train Epoch: 1430 [33024/118836 (28%)] Loss: 12404.876953\n",
      "Train Epoch: 1430 [65792/118836 (55%)] Loss: 12180.942383\n",
      "Train Epoch: 1430 [98560/118836 (83%)] Loss: 12352.971680\n",
      "    epoch          : 1430\n",
      "    loss           : 12265.971364667339\n",
      "    val_loss       : 12264.916809741311\n",
      "    val_log_likelihood: -12165.512077388337\n",
      "    val_log_marginal: -12173.694559263311\n",
      "Train Epoch: 1431 [256/118836 (0%)] Loss: 12292.910156\n",
      "Train Epoch: 1431 [33024/118836 (28%)] Loss: 12282.299805\n",
      "Train Epoch: 1431 [65792/118836 (55%)] Loss: 12180.667969\n",
      "Train Epoch: 1431 [98560/118836 (83%)] Loss: 12304.572266\n",
      "    epoch          : 1431\n",
      "    loss           : 12264.019740778795\n",
      "    val_loss       : 12267.905026557028\n",
      "    val_log_likelihood: -12167.484029770214\n",
      "    val_log_marginal: -12175.564140687331\n",
      "Train Epoch: 1432 [256/118836 (0%)] Loss: 12350.018555\n",
      "Train Epoch: 1432 [33024/118836 (28%)] Loss: 12349.774414\n",
      "Train Epoch: 1432 [65792/118836 (55%)] Loss: 12224.779297\n",
      "Train Epoch: 1432 [98560/118836 (83%)] Loss: 12313.749023\n",
      "    epoch          : 1432\n",
      "    loss           : 12265.44693719629\n",
      "    val_loss       : 12268.366448239378\n",
      "    val_log_likelihood: -12167.66057805392\n",
      "    val_log_marginal: -12175.779035160072\n",
      "Train Epoch: 1433 [256/118836 (0%)] Loss: 12252.349609\n",
      "Train Epoch: 1433 [33024/118836 (28%)] Loss: 12256.726562\n",
      "Train Epoch: 1433 [65792/118836 (55%)] Loss: 12343.462891\n",
      "Train Epoch: 1433 [98560/118836 (83%)] Loss: 12217.841797\n",
      "    epoch          : 1433\n",
      "    loss           : 12264.663668805573\n",
      "    val_loss       : 12267.140044215563\n",
      "    val_log_likelihood: -12165.330636437657\n",
      "    val_log_marginal: -12173.330854619615\n",
      "Train Epoch: 1434 [256/118836 (0%)] Loss: 12232.984375\n",
      "Train Epoch: 1434 [33024/118836 (28%)] Loss: 12220.022461\n",
      "Train Epoch: 1434 [65792/118836 (55%)] Loss: 12303.376953\n",
      "Train Epoch: 1434 [98560/118836 (83%)] Loss: 12402.193359\n",
      "    epoch          : 1434\n",
      "    loss           : 12257.785506810897\n",
      "    val_loss       : 12266.42548432103\n",
      "    val_log_likelihood: -12165.748860919408\n",
      "    val_log_marginal: -12173.75497542676\n",
      "Train Epoch: 1435 [256/118836 (0%)] Loss: 12319.932617\n",
      "Train Epoch: 1435 [33024/118836 (28%)] Loss: 12332.203125\n",
      "Train Epoch: 1435 [65792/118836 (55%)] Loss: 12342.356445\n",
      "Train Epoch: 1435 [98560/118836 (83%)] Loss: 12210.782227\n",
      "    epoch          : 1435\n",
      "    loss           : 12262.16534600522\n",
      "    val_loss       : 12259.39794235185\n",
      "    val_log_likelihood: -12165.573938301282\n",
      "    val_log_marginal: -12173.655584519547\n",
      "Train Epoch: 1436 [256/118836 (0%)] Loss: 12279.281250\n",
      "Train Epoch: 1436 [33024/118836 (28%)] Loss: 12309.830078\n",
      "Train Epoch: 1436 [65792/118836 (55%)] Loss: 12351.046875\n",
      "Train Epoch: 1436 [98560/118836 (83%)] Loss: 12223.638672\n",
      "    epoch          : 1436\n",
      "    loss           : 12263.676491741626\n",
      "    val_loss       : 12270.442204393996\n",
      "    val_log_likelihood: -12166.58092561001\n",
      "    val_log_marginal: -12174.940336647956\n",
      "Train Epoch: 1437 [256/118836 (0%)] Loss: 12336.460938\n",
      "Train Epoch: 1437 [33024/118836 (28%)] Loss: 12356.683594\n",
      "Train Epoch: 1437 [65792/118836 (55%)] Loss: 12206.046875\n",
      "Train Epoch: 1437 [98560/118836 (83%)] Loss: 12290.488281\n",
      "    epoch          : 1437\n",
      "    loss           : 12268.964169122208\n",
      "    val_loss       : 12261.793409902906\n",
      "    val_log_likelihood: -12169.788847640095\n",
      "    val_log_marginal: -12177.722959166927\n",
      "Train Epoch: 1438 [256/118836 (0%)] Loss: 12212.963867\n",
      "Train Epoch: 1438 [33024/118836 (28%)] Loss: 12291.763672\n",
      "Train Epoch: 1438 [65792/118836 (55%)] Loss: 12291.595703\n",
      "Train Epoch: 1438 [98560/118836 (83%)] Loss: 12257.411133\n",
      "    epoch          : 1438\n",
      "    loss           : 12262.695877759254\n",
      "    val_loss       : 12264.635904698565\n",
      "    val_log_likelihood: -12167.982414120657\n",
      "    val_log_marginal: -12175.936674715746\n",
      "Train Epoch: 1439 [256/118836 (0%)] Loss: 12307.158203\n",
      "Train Epoch: 1439 [33024/118836 (28%)] Loss: 12358.825195\n",
      "Train Epoch: 1439 [65792/118836 (55%)] Loss: 12323.348633\n",
      "Train Epoch: 1439 [98560/118836 (83%)] Loss: 12308.079102\n",
      "    epoch          : 1439\n",
      "    loss           : 12267.979704462625\n",
      "    val_loss       : 12265.173746461644\n",
      "    val_log_likelihood: -12167.1394597485\n",
      "    val_log_marginal: -12175.389138634539\n",
      "Train Epoch: 1440 [256/118836 (0%)] Loss: 12368.905273\n",
      "Train Epoch: 1440 [33024/118836 (28%)] Loss: 12341.823242\n",
      "Train Epoch: 1440 [65792/118836 (55%)] Loss: 12349.798828\n",
      "Train Epoch: 1440 [98560/118836 (83%)] Loss: 12254.216797\n",
      "    epoch          : 1440\n",
      "    loss           : 12263.140410301645\n",
      "    val_loss       : 12269.17380616267\n",
      "    val_log_likelihood: -12165.651213716139\n",
      "    val_log_marginal: -12173.665589568078\n",
      "Train Epoch: 1441 [256/118836 (0%)] Loss: 12203.366211\n",
      "Train Epoch: 1441 [33024/118836 (28%)] Loss: 12249.072266\n",
      "Train Epoch: 1441 [65792/118836 (55%)] Loss: 12316.057617\n",
      "Train Epoch: 1441 [98560/118836 (83%)] Loss: 12334.566406\n",
      "    epoch          : 1441\n",
      "    loss           : 12265.61477493021\n",
      "    val_loss       : 12266.785691267\n",
      "    val_log_likelihood: -12168.281004284274\n",
      "    val_log_marginal: -12176.369870018536\n",
      "Train Epoch: 1442 [256/118836 (0%)] Loss: 12332.123047\n",
      "Train Epoch: 1442 [33024/118836 (28%)] Loss: 12389.279297\n",
      "Train Epoch: 1442 [65792/118836 (55%)] Loss: 12330.979492\n",
      "Train Epoch: 1442 [98560/118836 (83%)] Loss: 12221.476562\n",
      "    epoch          : 1442\n",
      "    loss           : 12259.213154111094\n",
      "    val_loss       : 12265.659442845134\n",
      "    val_log_likelihood: -12167.683941726116\n",
      "    val_log_marginal: -12175.730138732302\n",
      "Train Epoch: 1443 [256/118836 (0%)] Loss: 12196.353516\n",
      "Train Epoch: 1443 [33024/118836 (28%)] Loss: 12279.278320\n",
      "Train Epoch: 1443 [65792/118836 (55%)] Loss: 12262.546875\n",
      "Train Epoch: 1443 [98560/118836 (83%)] Loss: 12241.271484\n",
      "    epoch          : 1443\n",
      "    loss           : 12262.674286923593\n",
      "    val_loss       : 12263.600501861296\n",
      "    val_log_likelihood: -12166.0318021738\n",
      "    val_log_marginal: -12173.959142326523\n",
      "Train Epoch: 1444 [256/118836 (0%)] Loss: 12285.550781\n",
      "Train Epoch: 1444 [33024/118836 (28%)] Loss: 12320.408203\n",
      "Train Epoch: 1444 [65792/118836 (55%)] Loss: 12348.513672\n",
      "Train Epoch: 1444 [98560/118836 (83%)] Loss: 12299.599609\n",
      "    epoch          : 1444\n",
      "    loss           : 12263.680490882187\n",
      "    val_loss       : 12264.07081679697\n",
      "    val_log_likelihood: -12168.281104121434\n",
      "    val_log_marginal: -12176.306991631162\n",
      "Train Epoch: 1445 [256/118836 (0%)] Loss: 12364.595703\n",
      "Train Epoch: 1445 [33024/118836 (28%)] Loss: 12323.086914\n",
      "Train Epoch: 1445 [65792/118836 (55%)] Loss: 12321.568359\n",
      "Train Epoch: 1445 [98560/118836 (83%)] Loss: 12303.298828\n",
      "    epoch          : 1445\n",
      "    loss           : 12258.632744972601\n",
      "    val_loss       : 12263.978964402884\n",
      "    val_log_likelihood: -12164.201895775175\n",
      "    val_log_marginal: -12172.206383010975\n",
      "Train Epoch: 1446 [256/118836 (0%)] Loss: 12317.326172\n",
      "Train Epoch: 1446 [33024/118836 (28%)] Loss: 12229.976562\n",
      "Train Epoch: 1446 [65792/118836 (55%)] Loss: 12185.940430\n",
      "Train Epoch: 1446 [98560/118836 (83%)] Loss: 12209.410156\n",
      "    epoch          : 1446\n",
      "    loss           : 12271.920993105097\n",
      "    val_loss       : 12263.88421081393\n",
      "    val_log_likelihood: -12169.155301708539\n",
      "    val_log_marginal: -12177.691063104456\n",
      "Train Epoch: 1447 [256/118836 (0%)] Loss: 12256.439453\n",
      "Train Epoch: 1447 [33024/118836 (28%)] Loss: 12336.967773\n",
      "Train Epoch: 1447 [65792/118836 (55%)] Loss: 12213.007812\n",
      "Train Epoch: 1447 [98560/118836 (83%)] Loss: 12317.810547\n",
      "    epoch          : 1447\n",
      "    loss           : 12265.111432485524\n",
      "    val_loss       : 12265.768892346108\n",
      "    val_log_likelihood: -12165.962502907878\n",
      "    val_log_marginal: -12173.949440384624\n",
      "Train Epoch: 1448 [256/118836 (0%)] Loss: 12344.107422\n",
      "Train Epoch: 1448 [33024/118836 (28%)] Loss: 12261.855469\n",
      "Train Epoch: 1448 [65792/118836 (55%)] Loss: 12283.828125\n",
      "Train Epoch: 1448 [98560/118836 (83%)] Loss: 12310.319336\n",
      "    epoch          : 1448\n",
      "    loss           : 12267.26251615488\n",
      "    val_loss       : 12267.418941519205\n",
      "    val_log_likelihood: -12166.679069252741\n",
      "    val_log_marginal: -12174.694219949211\n",
      "Train Epoch: 1449 [256/118836 (0%)] Loss: 12307.421875\n",
      "Train Epoch: 1449 [33024/118836 (28%)] Loss: 12270.809570\n",
      "Train Epoch: 1449 [65792/118836 (55%)] Loss: 12280.551758\n",
      "Train Epoch: 1449 [98560/118836 (83%)] Loss: 12234.357422\n",
      "    epoch          : 1449\n",
      "    loss           : 12270.665904802523\n",
      "    val_loss       : 12267.080161111719\n",
      "    val_log_likelihood: -12164.615081226737\n",
      "    val_log_marginal: -12172.618656230728\n",
      "Train Epoch: 1450 [256/118836 (0%)] Loss: 12348.325195\n",
      "Train Epoch: 1450 [33024/118836 (28%)] Loss: 12261.932617\n",
      "Train Epoch: 1450 [65792/118836 (55%)] Loss: 12358.685547\n",
      "Train Epoch: 1450 [98560/118836 (83%)] Loss: 12293.370117\n",
      "    epoch          : 1450\n",
      "    loss           : 12264.006972446237\n",
      "    val_loss       : 12267.521141286807\n",
      "    val_log_likelihood: -12165.505523353495\n",
      "    val_log_marginal: -12173.49601538626\n",
      "Train Epoch: 1451 [256/118836 (0%)] Loss: 12291.323242\n",
      "Train Epoch: 1451 [33024/118836 (28%)] Loss: 12488.214844\n",
      "Train Epoch: 1451 [65792/118836 (55%)] Loss: 12247.232422\n",
      "Train Epoch: 1451 [98560/118836 (83%)] Loss: 12209.453125\n",
      "    epoch          : 1451\n",
      "    loss           : 12263.27851142473\n",
      "    val_loss       : 12260.657070527985\n",
      "    val_log_likelihood: -12167.541871187448\n",
      "    val_log_marginal: -12175.69012867117\n",
      "Train Epoch: 1452 [256/118836 (0%)] Loss: 12285.725586\n",
      "Train Epoch: 1452 [33024/118836 (28%)] Loss: 12259.518555\n",
      "Train Epoch: 1452 [65792/118836 (55%)] Loss: 12238.570312\n",
      "Train Epoch: 1452 [98560/118836 (83%)] Loss: 12281.372070\n",
      "    epoch          : 1452\n",
      "    loss           : 12262.36655584419\n",
      "    val_loss       : 12266.275164258155\n",
      "    val_log_likelihood: -12163.451009033808\n",
      "    val_log_marginal: -12171.804381102113\n",
      "Train Epoch: 1453 [256/118836 (0%)] Loss: 12243.554688\n",
      "Train Epoch: 1453 [33024/118836 (28%)] Loss: 12265.177734\n",
      "Train Epoch: 1453 [65792/118836 (55%)] Loss: 12240.276367\n",
      "Train Epoch: 1453 [98560/118836 (83%)] Loss: 12272.259766\n",
      "    epoch          : 1453\n",
      "    loss           : 12267.461174330541\n",
      "    val_loss       : 12260.90736928647\n",
      "    val_log_likelihood: -12164.832437706782\n",
      "    val_log_marginal: -12172.796181419553\n",
      "Train Epoch: 1454 [256/118836 (0%)] Loss: 12295.646484\n",
      "Train Epoch: 1454 [33024/118836 (28%)] Loss: 12225.656250\n",
      "Train Epoch: 1454 [65792/118836 (55%)] Loss: 12218.623047\n",
      "Train Epoch: 1454 [98560/118836 (83%)] Loss: 12366.196289\n",
      "    epoch          : 1454\n",
      "    loss           : 12264.275148786446\n",
      "    val_loss       : 12265.167898253523\n",
      "    val_log_likelihood: -12164.951222278225\n",
      "    val_log_marginal: -12173.062455861129\n",
      "Train Epoch: 1455 [256/118836 (0%)] Loss: 12244.385742\n",
      "Train Epoch: 1455 [33024/118836 (28%)] Loss: 12214.954102\n",
      "Train Epoch: 1455 [65792/118836 (55%)] Loss: 12476.453125\n",
      "Train Epoch: 1455 [98560/118836 (83%)] Loss: 12412.354492\n",
      "    epoch          : 1455\n",
      "    loss           : 12267.410518927058\n",
      "    val_loss       : 12265.39045780644\n",
      "    val_log_likelihood: -12167.138514364919\n",
      "    val_log_marginal: -12175.200991501075\n",
      "Train Epoch: 1456 [256/118836 (0%)] Loss: 12211.630859\n",
      "Train Epoch: 1456 [33024/118836 (28%)] Loss: 12267.413086\n",
      "Train Epoch: 1456 [65792/118836 (55%)] Loss: 12212.117188\n",
      "Train Epoch: 1456 [98560/118836 (83%)] Loss: 12286.008789\n",
      "    epoch          : 1456\n",
      "    loss           : 12264.434932504912\n",
      "    val_loss       : 12266.076563068664\n",
      "    val_log_likelihood: -12164.421941881204\n",
      "    val_log_marginal: -12172.400573585508\n",
      "Train Epoch: 1457 [256/118836 (0%)] Loss: 12424.421875\n",
      "Train Epoch: 1457 [33024/118836 (28%)] Loss: 12189.171875\n",
      "Train Epoch: 1457 [65792/118836 (55%)] Loss: 12247.691406\n",
      "Train Epoch: 1457 [98560/118836 (83%)] Loss: 12216.798828\n",
      "    epoch          : 1457\n",
      "    loss           : 12260.015483967898\n",
      "    val_loss       : 12264.193684902897\n",
      "    val_log_likelihood: -12166.968718174887\n",
      "    val_log_marginal: -12174.961344665358\n",
      "Train Epoch: 1458 [256/118836 (0%)] Loss: 12213.123047\n",
      "Train Epoch: 1458 [33024/118836 (28%)] Loss: 12224.516602\n",
      "Train Epoch: 1458 [65792/118836 (55%)] Loss: 12287.173828\n",
      "Train Epoch: 1458 [98560/118836 (83%)] Loss: 12180.976562\n",
      "    epoch          : 1458\n",
      "    loss           : 12267.424113258634\n",
      "    val_loss       : 12267.75168104157\n",
      "    val_log_likelihood: -12164.287612114867\n",
      "    val_log_marginal: -12172.236722638714\n",
      "Train Epoch: 1459 [256/118836 (0%)] Loss: 12284.038086\n",
      "Train Epoch: 1459 [33024/118836 (28%)] Loss: 12343.431641\n",
      "Train Epoch: 1459 [65792/118836 (55%)] Loss: 12204.384766\n",
      "Train Epoch: 1459 [98560/118836 (83%)] Loss: 12176.001953\n",
      "    epoch          : 1459\n",
      "    loss           : 12269.452385752687\n",
      "    val_loss       : 12264.385382136878\n",
      "    val_log_likelihood: -12168.035035411496\n",
      "    val_log_marginal: -12176.177310819716\n",
      "Train Epoch: 1460 [256/118836 (0%)] Loss: 12252.732422\n",
      "Train Epoch: 1460 [33024/118836 (28%)] Loss: 12277.177734\n",
      "Train Epoch: 1460 [65792/118836 (55%)] Loss: 12347.392578\n",
      "Train Epoch: 1460 [98560/118836 (83%)] Loss: 12328.681641\n",
      "    epoch          : 1460\n",
      "    loss           : 12264.827673632652\n",
      "    val_loss       : 12267.047762808581\n",
      "    val_log_likelihood: -12166.210328461022\n",
      "    val_log_marginal: -12174.183046923104\n",
      "Train Epoch: 1461 [256/118836 (0%)] Loss: 12220.271484\n",
      "Train Epoch: 1461 [33024/118836 (28%)] Loss: 12339.064453\n",
      "Train Epoch: 1461 [65792/118836 (55%)] Loss: 12308.152344\n",
      "Train Epoch: 1461 [98560/118836 (83%)] Loss: 12186.804688\n",
      "    epoch          : 1461\n",
      "    loss           : 12261.450070112178\n",
      "    val_loss       : 12263.93412147333\n",
      "    val_log_likelihood: -12166.990604160206\n",
      "    val_log_marginal: -12175.043579341658\n",
      "Train Epoch: 1462 [256/118836 (0%)] Loss: 12192.039062\n",
      "Train Epoch: 1462 [33024/118836 (28%)] Loss: 12370.518555\n",
      "Train Epoch: 1462 [65792/118836 (55%)] Loss: 12312.719727\n",
      "Train Epoch: 1462 [98560/118836 (83%)] Loss: 12249.929688\n",
      "    epoch          : 1462\n",
      "    loss           : 12266.17706782465\n",
      "    val_loss       : 12264.079632410281\n",
      "    val_log_likelihood: -12168.358476950216\n",
      "    val_log_marginal: -12176.542655461864\n",
      "Train Epoch: 1463 [256/118836 (0%)] Loss: 12288.896484\n",
      "Train Epoch: 1463 [33024/118836 (28%)] Loss: 12209.182617\n",
      "Train Epoch: 1463 [65792/118836 (55%)] Loss: 12214.296875\n",
      "Train Epoch: 1463 [98560/118836 (83%)] Loss: 12190.089844\n",
      "    epoch          : 1463\n",
      "    loss           : 12265.88438307744\n",
      "    val_loss       : 12261.633738530943\n",
      "    val_log_likelihood: -12164.65863946831\n",
      "    val_log_marginal: -12172.616238592493\n",
      "Train Epoch: 1464 [256/118836 (0%)] Loss: 12459.491211\n",
      "Train Epoch: 1464 [33024/118836 (28%)] Loss: 12289.999023\n",
      "Train Epoch: 1464 [65792/118836 (55%)] Loss: 12235.335938\n",
      "Train Epoch: 1464 [98560/118836 (83%)] Loss: 12250.885742\n",
      "    epoch          : 1464\n",
      "    loss           : 12263.596637846362\n",
      "    val_loss       : 12265.228473717409\n",
      "    val_log_likelihood: -12165.288672682744\n",
      "    val_log_marginal: -12173.338568933825\n",
      "Train Epoch: 1465 [256/118836 (0%)] Loss: 12253.614258\n",
      "Train Epoch: 1465 [33024/118836 (28%)] Loss: 12234.035156\n",
      "Train Epoch: 1465 [65792/118836 (55%)] Loss: 12291.330078\n",
      "Train Epoch: 1465 [98560/118836 (83%)] Loss: 12214.451172\n",
      "    epoch          : 1465\n",
      "    loss           : 12258.456760332661\n",
      "    val_loss       : 12264.122077844777\n",
      "    val_log_likelihood: -12163.329683138181\n",
      "    val_log_marginal: -12171.299430314179\n",
      "Train Epoch: 1466 [256/118836 (0%)] Loss: 12255.537109\n",
      "Train Epoch: 1466 [33024/118836 (28%)] Loss: 12224.200195\n",
      "Train Epoch: 1466 [65792/118836 (55%)] Loss: 12277.550781\n",
      "Train Epoch: 1466 [98560/118836 (83%)] Loss: 12222.236328\n",
      "    epoch          : 1466\n",
      "    loss           : 12263.574749922456\n",
      "    val_loss       : 12262.931514184611\n",
      "    val_log_likelihood: -12163.608757722033\n",
      "    val_log_marginal: -12171.689859733635\n",
      "Train Epoch: 1467 [256/118836 (0%)] Loss: 12245.669922\n",
      "Train Epoch: 1467 [33024/118836 (28%)] Loss: 12339.977539\n",
      "Train Epoch: 1467 [65792/118836 (55%)] Loss: 12246.976562\n",
      "Train Epoch: 1467 [98560/118836 (83%)] Loss: 12254.915039\n",
      "    epoch          : 1467\n",
      "    loss           : 12260.827789463141\n",
      "    val_loss       : 12254.333965216865\n",
      "    val_log_likelihood: -12167.514314193033\n",
      "    val_log_marginal: -12175.478790868614\n",
      "Train Epoch: 1468 [256/118836 (0%)] Loss: 12206.908203\n",
      "Train Epoch: 1468 [33024/118836 (28%)] Loss: 12203.511719\n",
      "Train Epoch: 1468 [65792/118836 (55%)] Loss: 12301.615234\n",
      "Train Epoch: 1468 [98560/118836 (83%)] Loss: 12261.390625\n",
      "    epoch          : 1468\n",
      "    loss           : 12264.709417002688\n",
      "    val_loss       : 12260.803608161881\n",
      "    val_log_likelihood: -12165.795521059503\n",
      "    val_log_marginal: -12173.701270158237\n",
      "Train Epoch: 1469 [256/118836 (0%)] Loss: 12196.869141\n",
      "Train Epoch: 1469 [33024/118836 (28%)] Loss: 12299.326172\n",
      "Train Epoch: 1469 [65792/118836 (55%)] Loss: 12267.914062\n",
      "Train Epoch: 1469 [98560/118836 (83%)] Loss: 12298.341797\n",
      "    epoch          : 1469\n",
      "    loss           : 12264.883247712469\n",
      "    val_loss       : 12268.007552566514\n",
      "    val_log_likelihood: -12163.404855995399\n",
      "    val_log_marginal: -12171.433641340655\n",
      "Train Epoch: 1470 [256/118836 (0%)] Loss: 12294.446289\n",
      "Train Epoch: 1470 [33024/118836 (28%)] Loss: 12302.119141\n",
      "Train Epoch: 1470 [65792/118836 (55%)] Loss: 12412.545898\n",
      "Train Epoch: 1470 [98560/118836 (83%)] Loss: 12229.408203\n",
      "    epoch          : 1470\n",
      "    loss           : 12265.311675616469\n",
      "    val_loss       : 12263.867883941555\n",
      "    val_log_likelihood: -12161.470275666874\n",
      "    val_log_marginal: -12169.466111413874\n",
      "Train Epoch: 1471 [256/118836 (0%)] Loss: 12246.938477\n",
      "Train Epoch: 1471 [33024/118836 (28%)] Loss: 12332.462891\n",
      "Train Epoch: 1471 [65792/118836 (55%)] Loss: 12413.677734\n",
      "Train Epoch: 1471 [98560/118836 (83%)] Loss: 12273.031250\n",
      "    epoch          : 1471\n",
      "    loss           : 12257.406424311155\n",
      "    val_loss       : 12266.431258626882\n",
      "    val_log_likelihood: -12165.585186136528\n",
      "    val_log_marginal: -12173.477104241721\n",
      "Train Epoch: 1472 [256/118836 (0%)] Loss: 12322.310547\n",
      "Train Epoch: 1472 [33024/118836 (28%)] Loss: 12202.499023\n",
      "Train Epoch: 1472 [65792/118836 (55%)] Loss: 12246.306641\n",
      "Train Epoch: 1472 [98560/118836 (83%)] Loss: 12233.796875\n",
      "    epoch          : 1472\n",
      "    loss           : 12258.811948633944\n",
      "    val_loss       : 12263.61795805676\n",
      "    val_log_likelihood: -12165.649679002532\n",
      "    val_log_marginal: -12173.586478987208\n",
      "Train Epoch: 1473 [256/118836 (0%)] Loss: 12220.106445\n",
      "Train Epoch: 1473 [33024/118836 (28%)] Loss: 12246.519531\n",
      "Train Epoch: 1473 [65792/118836 (55%)] Loss: 12332.985352\n",
      "Train Epoch: 1473 [98560/118836 (83%)] Loss: 12221.355469\n",
      "    epoch          : 1473\n",
      "    loss           : 12260.275136347187\n",
      "    val_loss       : 12266.148892310726\n",
      "    val_log_likelihood: -12164.317925293373\n",
      "    val_log_marginal: -12172.2497443807\n",
      "Train Epoch: 1474 [256/118836 (0%)] Loss: 12289.825195\n",
      "Train Epoch: 1474 [33024/118836 (28%)] Loss: 12271.832031\n",
      "Train Epoch: 1474 [65792/118836 (55%)] Loss: 12260.863281\n",
      "Train Epoch: 1474 [98560/118836 (83%)] Loss: 12314.379883\n",
      "    epoch          : 1474\n",
      "    loss           : 12267.813263318083\n",
      "    val_loss       : 12257.689959643536\n",
      "    val_log_likelihood: -12165.921852383168\n",
      "    val_log_marginal: -12174.035875875219\n",
      "Train Epoch: 1475 [256/118836 (0%)] Loss: 12346.994141\n",
      "Train Epoch: 1475 [33024/118836 (28%)] Loss: 12304.960938\n",
      "Train Epoch: 1475 [65792/118836 (55%)] Loss: 12391.311523\n",
      "Train Epoch: 1475 [98560/118836 (83%)] Loss: 12251.835938\n",
      "    epoch          : 1475\n",
      "    loss           : 12263.821877100134\n",
      "    val_loss       : 12262.688791751214\n",
      "    val_log_likelihood: -12168.350765741316\n",
      "    val_log_marginal: -12176.315517479323\n",
      "Train Epoch: 1476 [256/118836 (0%)] Loss: 12228.946289\n",
      "Train Epoch: 1476 [33024/118836 (28%)] Loss: 12254.745117\n",
      "Train Epoch: 1476 [65792/118836 (55%)] Loss: 12277.702148\n",
      "Train Epoch: 1476 [98560/118836 (83%)] Loss: 12366.385742\n",
      "    epoch          : 1476\n",
      "    loss           : 12256.36820735758\n",
      "    val_loss       : 12261.736093878253\n",
      "    val_log_likelihood: -12168.292988782052\n",
      "    val_log_marginal: -12176.375337370888\n",
      "Train Epoch: 1477 [256/118836 (0%)] Loss: 12156.769531\n",
      "Train Epoch: 1477 [33024/118836 (28%)] Loss: 12396.749023\n",
      "Train Epoch: 1477 [65792/118836 (55%)] Loss: 12179.956055\n",
      "Train Epoch: 1477 [98560/118836 (83%)] Loss: 12249.923828\n",
      "    epoch          : 1477\n",
      "    loss           : 12265.582073737334\n",
      "    val_loss       : 12262.56619098829\n",
      "    val_log_likelihood: -12163.438003547612\n",
      "    val_log_marginal: -12171.517588313221\n",
      "Train Epoch: 1478 [256/118836 (0%)] Loss: 12277.832031\n",
      "Train Epoch: 1478 [33024/118836 (28%)] Loss: 12408.242188\n",
      "Train Epoch: 1478 [65792/118836 (55%)] Loss: 12240.968750\n",
      "Train Epoch: 1478 [98560/118836 (83%)] Loss: 12205.502930\n",
      "    epoch          : 1478\n",
      "    loss           : 12265.223102447788\n",
      "    val_loss       : 12261.230248320437\n",
      "    val_log_likelihood: -12165.243947251085\n",
      "    val_log_marginal: -12173.469979792693\n",
      "Train Epoch: 1479 [256/118836 (0%)] Loss: 12213.928711\n",
      "Train Epoch: 1479 [33024/118836 (28%)] Loss: 12278.255859\n",
      "Train Epoch: 1479 [65792/118836 (55%)] Loss: 12308.818359\n",
      "Train Epoch: 1479 [98560/118836 (83%)] Loss: 12239.546875\n",
      "    epoch          : 1479\n",
      "    loss           : 12259.24323142835\n",
      "    val_loss       : 12263.874455355084\n",
      "    val_log_likelihood: -12166.65541883142\n",
      "    val_log_marginal: -12174.567613134017\n",
      "Train Epoch: 1480 [256/118836 (0%)] Loss: 12187.901367\n",
      "Train Epoch: 1480 [33024/118836 (28%)] Loss: 12309.369141\n",
      "Train Epoch: 1480 [65792/118836 (55%)] Loss: 12333.989258\n",
      "Train Epoch: 1480 [98560/118836 (83%)] Loss: 12229.613281\n",
      "    epoch          : 1480\n",
      "    loss           : 12264.19750148625\n",
      "    val_loss       : 12263.652762682217\n",
      "    val_log_likelihood: -12165.804225955077\n",
      "    val_log_marginal: -12173.918270555489\n",
      "Train Epoch: 1481 [256/118836 (0%)] Loss: 12249.292969\n",
      "Train Epoch: 1481 [33024/118836 (28%)] Loss: 12311.282227\n",
      "Train Epoch: 1481 [65792/118836 (55%)] Loss: 12198.115234\n",
      "Train Epoch: 1481 [98560/118836 (83%)] Loss: 12292.327148\n",
      "    epoch          : 1481\n",
      "    loss           : 12260.232170989713\n",
      "    val_loss       : 12256.556827029122\n",
      "    val_log_likelihood: -12165.228321927989\n",
      "    val_log_marginal: -12173.216224395566\n",
      "Train Epoch: 1482 [256/118836 (0%)] Loss: 12264.997070\n",
      "Train Epoch: 1482 [33024/118836 (28%)] Loss: 12263.903320\n",
      "Train Epoch: 1482 [65792/118836 (55%)] Loss: 12271.306641\n",
      "Train Epoch: 1482 [98560/118836 (83%)] Loss: 12369.099609\n",
      "    epoch          : 1482\n",
      "    loss           : 12261.30025718569\n",
      "    val_loss       : 12269.424958015506\n",
      "    val_log_likelihood: -12164.180194117038\n",
      "    val_log_marginal: -12172.14956009344\n",
      "Train Epoch: 1483 [256/118836 (0%)] Loss: 12374.549805\n",
      "Train Epoch: 1483 [33024/118836 (28%)] Loss: 12309.598633\n",
      "Train Epoch: 1483 [65792/118836 (55%)] Loss: 12226.783203\n",
      "Train Epoch: 1483 [98560/118836 (83%)] Loss: 12331.404297\n",
      "    epoch          : 1483\n",
      "    loss           : 12263.697050280449\n",
      "    val_loss       : 12261.840662540775\n",
      "    val_log_likelihood: -12161.341388318082\n",
      "    val_log_marginal: -12169.518775939225\n",
      "Train Epoch: 1484 [256/118836 (0%)] Loss: 12343.273438\n",
      "Train Epoch: 1484 [33024/118836 (28%)] Loss: 12227.563477\n",
      "Train Epoch: 1484 [65792/118836 (55%)] Loss: 12356.020508\n",
      "Train Epoch: 1484 [98560/118836 (83%)] Loss: 12296.624023\n",
      "    epoch          : 1484\n",
      "    loss           : 12258.595041259565\n",
      "    val_loss       : 12263.136273397433\n",
      "    val_log_likelihood: -12164.258105872621\n",
      "    val_log_marginal: -12172.3540355718\n",
      "Train Epoch: 1485 [256/118836 (0%)] Loss: 12251.684570\n",
      "Train Epoch: 1485 [33024/118836 (28%)] Loss: 12205.595703\n",
      "Train Epoch: 1485 [65792/118836 (55%)] Loss: 12343.894531\n",
      "Train Epoch: 1485 [98560/118836 (83%)] Loss: 12173.050781\n",
      "    epoch          : 1485\n",
      "    loss           : 12259.718423025228\n",
      "    val_loss       : 12257.60776156056\n",
      "    val_log_likelihood: -12161.95241499302\n",
      "    val_log_marginal: -12169.984720769055\n",
      "Train Epoch: 1486 [256/118836 (0%)] Loss: 12258.821289\n",
      "Train Epoch: 1486 [33024/118836 (28%)] Loss: 12344.383789\n",
      "Train Epoch: 1486 [65792/118836 (55%)] Loss: 12359.213867\n",
      "Train Epoch: 1486 [98560/118836 (83%)] Loss: 12272.808594\n",
      "    epoch          : 1486\n",
      "    loss           : 12267.793645801023\n",
      "    val_loss       : 12263.186369149624\n",
      "    val_log_likelihood: -12166.087822774503\n",
      "    val_log_marginal: -12174.07537112679\n",
      "Train Epoch: 1487 [256/118836 (0%)] Loss: 12270.141602\n",
      "Train Epoch: 1487 [33024/118836 (28%)] Loss: 12374.085938\n",
      "Train Epoch: 1487 [65792/118836 (55%)] Loss: 12344.552734\n",
      "Train Epoch: 1487 [98560/118836 (83%)] Loss: 12217.261719\n",
      "    epoch          : 1487\n",
      "    loss           : 12266.016835162065\n",
      "    val_loss       : 12261.419639986929\n",
      "    val_log_likelihood: -12164.774639423078\n",
      "    val_log_marginal: -12172.748136041928\n",
      "Train Epoch: 1488 [256/118836 (0%)] Loss: 12280.795898\n",
      "Train Epoch: 1488 [33024/118836 (28%)] Loss: 12212.798828\n",
      "Train Epoch: 1488 [65792/118836 (55%)] Loss: 12276.494141\n",
      "Train Epoch: 1488 [98560/118836 (83%)] Loss: 12316.242188\n",
      "    epoch          : 1488\n",
      "    loss           : 12267.543867607526\n",
      "    val_loss       : 12263.55298236554\n",
      "    val_log_likelihood: -12163.80308994391\n",
      "    val_log_marginal: -12171.74669643799\n",
      "Train Epoch: 1489 [256/118836 (0%)] Loss: 12247.358398\n",
      "Train Epoch: 1489 [33024/118836 (28%)] Loss: 12328.236328\n",
      "Train Epoch: 1489 [65792/118836 (55%)] Loss: 12293.818359\n",
      "Train Epoch: 1489 [98560/118836 (83%)] Loss: 12462.091797\n",
      "    epoch          : 1489\n",
      "    loss           : 12262.344493770677\n",
      "    val_loss       : 12268.348805341293\n",
      "    val_log_likelihood: -12164.21093733845\n",
      "    val_log_marginal: -12172.272963891779\n",
      "Train Epoch: 1490 [256/118836 (0%)] Loss: 12242.203125\n",
      "Train Epoch: 1490 [33024/118836 (28%)] Loss: 12284.445312\n",
      "Train Epoch: 1490 [65792/118836 (55%)] Loss: 12404.839844\n",
      "Train Epoch: 1490 [98560/118836 (83%)] Loss: 12359.066406\n",
      "    epoch          : 1490\n",
      "    loss           : 12263.20980585065\n",
      "    val_loss       : 12260.244188336565\n",
      "    val_log_likelihood: -12166.921444957092\n",
      "    val_log_marginal: -12174.847041518688\n",
      "Train Epoch: 1491 [256/118836 (0%)] Loss: 12221.017578\n",
      "Train Epoch: 1491 [33024/118836 (28%)] Loss: 12271.280273\n",
      "Train Epoch: 1491 [65792/118836 (55%)] Loss: 12226.059570\n",
      "Train Epoch: 1491 [98560/118836 (83%)] Loss: 12286.718750\n",
      "    epoch          : 1491\n",
      "    loss           : 12263.844286988213\n",
      "    val_loss       : 12261.01986835311\n",
      "    val_log_likelihood: -12162.839828725962\n",
      "    val_log_marginal: -12171.01734608584\n",
      "Train Epoch: 1492 [256/118836 (0%)] Loss: 12279.386719\n",
      "Train Epoch: 1492 [33024/118836 (28%)] Loss: 12254.646484\n",
      "Train Epoch: 1492 [65792/118836 (55%)] Loss: 12276.967773\n",
      "Train Epoch: 1492 [98560/118836 (83%)] Loss: 12267.327148\n",
      "    epoch          : 1492\n",
      "    loss           : 12263.588185128463\n",
      "    val_loss       : 12261.301204315263\n",
      "    val_log_likelihood: -12165.479070222033\n",
      "    val_log_marginal: -12173.387833688617\n",
      "Train Epoch: 1493 [256/118836 (0%)] Loss: 12320.657227\n",
      "Train Epoch: 1493 [33024/118836 (28%)] Loss: 12169.904297\n",
      "Train Epoch: 1493 [65792/118836 (55%)] Loss: 12155.827148\n",
      "Train Epoch: 1493 [98560/118836 (83%)] Loss: 12318.135742\n",
      "    epoch          : 1493\n",
      "    loss           : 12261.451829701717\n",
      "    val_loss       : 12262.524561315\n",
      "    val_log_likelihood: -12166.936628928866\n",
      "    val_log_marginal: -12175.00803483915\n",
      "Train Epoch: 1494 [256/118836 (0%)] Loss: 12322.652344\n",
      "Train Epoch: 1494 [33024/118836 (28%)] Loss: 12215.562500\n",
      "Train Epoch: 1494 [65792/118836 (55%)] Loss: 12206.463867\n",
      "Train Epoch: 1494 [98560/118836 (83%)] Loss: 12206.984375\n",
      "    epoch          : 1494\n",
      "    loss           : 12259.993758723635\n",
      "    val_loss       : 12263.410700140803\n",
      "    val_log_likelihood: -12165.154961648315\n",
      "    val_log_marginal: -12173.114374813886\n",
      "Train Epoch: 1495 [256/118836 (0%)] Loss: 12257.746094\n",
      "Train Epoch: 1495 [33024/118836 (28%)] Loss: 12216.652344\n",
      "Train Epoch: 1495 [65792/118836 (55%)] Loss: 12501.820312\n",
      "Train Epoch: 1495 [98560/118836 (83%)] Loss: 12294.515625\n",
      "    epoch          : 1495\n",
      "    loss           : 12262.701205154053\n",
      "    val_loss       : 12263.805552499623\n",
      "    val_log_likelihood: -12163.919478908188\n",
      "    val_log_marginal: -12172.04832729268\n",
      "Train Epoch: 1496 [256/118836 (0%)] Loss: 12133.441406\n",
      "Train Epoch: 1496 [33024/118836 (28%)] Loss: 12248.670898\n",
      "Train Epoch: 1496 [65792/118836 (55%)] Loss: 12237.397461\n",
      "Train Epoch: 1496 [98560/118836 (83%)] Loss: 12253.109375\n",
      "    epoch          : 1496\n",
      "    loss           : 12263.211717457609\n",
      "    val_loss       : 12264.507699267311\n",
      "    val_log_likelihood: -12163.520911199856\n",
      "    val_log_marginal: -12171.60830961517\n",
      "Train Epoch: 1497 [256/118836 (0%)] Loss: 12369.587891\n",
      "Train Epoch: 1497 [33024/118836 (28%)] Loss: 12323.611328\n",
      "Train Epoch: 1497 [65792/118836 (55%)] Loss: 12393.676758\n",
      "Train Epoch: 1497 [98560/118836 (83%)] Loss: 12298.420898\n",
      "    epoch          : 1497\n",
      "    loss           : 12262.920713625672\n",
      "    val_loss       : 12261.78207353392\n",
      "    val_log_likelihood: -12166.564849888853\n",
      "    val_log_marginal: -12174.89810712096\n",
      "Train Epoch: 1498 [256/118836 (0%)] Loss: 12201.229492\n",
      "Train Epoch: 1498 [33024/118836 (28%)] Loss: 12266.015625\n",
      "Train Epoch: 1498 [65792/118836 (55%)] Loss: 12342.245117\n",
      "Train Epoch: 1498 [98560/118836 (83%)] Loss: 12324.257812\n",
      "    epoch          : 1498\n",
      "    loss           : 12259.811403406742\n",
      "    val_loss       : 12265.38299702518\n",
      "    val_log_likelihood: -12169.103338405966\n",
      "    val_log_marginal: -12177.174163238014\n",
      "Train Epoch: 1499 [256/118836 (0%)] Loss: 12217.636719\n",
      "Train Epoch: 1499 [33024/118836 (28%)] Loss: 12261.837891\n",
      "Train Epoch: 1499 [65792/118836 (55%)] Loss: 12354.005859\n",
      "Train Epoch: 1499 [98560/118836 (83%)] Loss: 12287.755859\n",
      "    epoch          : 1499\n",
      "    loss           : 12269.573330070049\n",
      "    val_loss       : 12261.365122948924\n",
      "    val_log_likelihood: -12163.987072380323\n",
      "    val_log_marginal: -12172.031507179616\n",
      "Train Epoch: 1500 [256/118836 (0%)] Loss: 12257.318359\n",
      "Train Epoch: 1500 [33024/118836 (28%)] Loss: 12210.787109\n",
      "Train Epoch: 1500 [65792/118836 (55%)] Loss: 12178.467773\n",
      "Train Epoch: 1500 [98560/118836 (83%)] Loss: 12408.962891\n",
      "    epoch          : 1500\n",
      "    loss           : 12262.217945002325\n",
      "    val_loss       : 12261.774488158846\n",
      "    val_log_likelihood: -12165.34293773263\n",
      "    val_log_marginal: -12173.22483828146\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch1500.pth ...\n",
      "Train Epoch: 1501 [256/118836 (0%)] Loss: 12235.957031\n",
      "Train Epoch: 1501 [33024/118836 (28%)] Loss: 12223.985352\n",
      "Train Epoch: 1501 [65792/118836 (55%)] Loss: 12223.439453\n",
      "Train Epoch: 1501 [98560/118836 (83%)] Loss: 12261.056641\n",
      "    epoch          : 1501\n",
      "    loss           : 12261.908009589537\n",
      "    val_loss       : 12264.238762409253\n",
      "    val_log_likelihood: -12163.598533459986\n",
      "    val_log_marginal: -12171.728013147598\n",
      "Train Epoch: 1502 [256/118836 (0%)] Loss: 12335.469727\n",
      "Train Epoch: 1502 [33024/118836 (28%)] Loss: 12194.788086\n",
      "Train Epoch: 1502 [65792/118836 (55%)] Loss: 12225.199219\n",
      "Train Epoch: 1502 [98560/118836 (83%)] Loss: 12251.718750\n",
      "    epoch          : 1502\n",
      "    loss           : 12258.243712036032\n",
      "    val_loss       : 12270.11536409771\n",
      "    val_log_likelihood: -12165.272469984233\n",
      "    val_log_marginal: -12173.290364577026\n",
      "Train Epoch: 1503 [256/118836 (0%)] Loss: 12219.811523\n",
      "Train Epoch: 1503 [33024/118836 (28%)] Loss: 12193.560547\n",
      "Train Epoch: 1503 [65792/118836 (55%)] Loss: 12233.474609\n",
      "Train Epoch: 1503 [98560/118836 (83%)] Loss: 12275.581055\n",
      "    epoch          : 1503\n",
      "    loss           : 12260.945674369314\n",
      "    val_loss       : 12263.912966954636\n",
      "    val_log_likelihood: -12167.272292119003\n",
      "    val_log_marginal: -12175.464581132439\n",
      "Train Epoch: 1504 [256/118836 (0%)] Loss: 12267.441406\n",
      "Train Epoch: 1504 [33024/118836 (28%)] Loss: 12215.818359\n",
      "Train Epoch: 1504 [65792/118836 (55%)] Loss: 12193.153320\n",
      "Train Epoch: 1504 [98560/118836 (83%)] Loss: 12276.123047\n",
      "    epoch          : 1504\n",
      "    loss           : 12264.048522636218\n",
      "    val_loss       : 12263.05097358603\n",
      "    val_log_likelihood: -12166.196854321754\n",
      "    val_log_marginal: -12174.257616463545\n",
      "Train Epoch: 1505 [256/118836 (0%)] Loss: 12323.401367\n",
      "Train Epoch: 1505 [33024/118836 (28%)] Loss: 12221.646484\n",
      "Train Epoch: 1505 [65792/118836 (55%)] Loss: 12193.963867\n",
      "Train Epoch: 1505 [98560/118836 (83%)] Loss: 12247.939453\n",
      "    epoch          : 1505\n",
      "    loss           : 12264.045806677781\n",
      "    val_loss       : 12265.33699697474\n",
      "    val_log_likelihood: -12165.780781993124\n",
      "    val_log_marginal: -12173.838466613885\n",
      "Train Epoch: 1506 [256/118836 (0%)] Loss: 12316.796875\n",
      "Train Epoch: 1506 [33024/118836 (28%)] Loss: 12267.857422\n",
      "Train Epoch: 1506 [65792/118836 (55%)] Loss: 12287.786133\n",
      "Train Epoch: 1506 [98560/118836 (83%)] Loss: 12350.326172\n",
      "    epoch          : 1506\n",
      "    loss           : 12264.364260720378\n",
      "    val_loss       : 12260.422972878087\n",
      "    val_log_likelihood: -12164.305761961074\n",
      "    val_log_marginal: -12172.245773378467\n",
      "Train Epoch: 1507 [256/118836 (0%)] Loss: 12342.394531\n",
      "Train Epoch: 1507 [33024/118836 (28%)] Loss: 12343.033203\n",
      "Train Epoch: 1507 [65792/118836 (55%)] Loss: 12338.659180\n",
      "Train Epoch: 1507 [98560/118836 (83%)] Loss: 12212.464844\n",
      "    epoch          : 1507\n",
      "    loss           : 12264.852779770214\n",
      "    val_loss       : 12261.872919856189\n",
      "    val_log_likelihood: -12164.801915645678\n",
      "    val_log_marginal: -12172.911625276232\n",
      "Train Epoch: 1508 [256/118836 (0%)] Loss: 12258.510742\n",
      "Train Epoch: 1508 [33024/118836 (28%)] Loss: 12222.306641\n",
      "Train Epoch: 1508 [65792/118836 (55%)] Loss: 12363.050781\n",
      "Train Epoch: 1508 [98560/118836 (83%)] Loss: 12236.791016\n",
      "    epoch          : 1508\n",
      "    loss           : 12260.968708320408\n",
      "    val_loss       : 12263.725591471577\n",
      "    val_log_likelihood: -12163.523129426438\n",
      "    val_log_marginal: -12171.614988713096\n",
      "Train Epoch: 1509 [256/118836 (0%)] Loss: 12237.337891\n",
      "Train Epoch: 1509 [33024/118836 (28%)] Loss: 12351.978516\n",
      "Train Epoch: 1509 [65792/118836 (55%)] Loss: 12387.943359\n",
      "Train Epoch: 1509 [98560/118836 (83%)] Loss: 12354.943359\n",
      "    epoch          : 1509\n",
      "    loss           : 12256.994735447684\n",
      "    val_loss       : 12264.868771672407\n",
      "    val_log_likelihood: -12162.890409816999\n",
      "    val_log_marginal: -12170.847935024745\n",
      "Train Epoch: 1510 [256/118836 (0%)] Loss: 12250.320312\n",
      "Train Epoch: 1510 [33024/118836 (28%)] Loss: 12293.453125\n",
      "Train Epoch: 1510 [65792/118836 (55%)] Loss: 12201.056641\n",
      "Train Epoch: 1510 [98560/118836 (83%)] Loss: 12257.917969\n",
      "    epoch          : 1510\n",
      "    loss           : 12264.633754814155\n",
      "    val_loss       : 12267.893908147851\n",
      "    val_log_likelihood: -12163.226759427987\n",
      "    val_log_marginal: -12171.253708617156\n",
      "Train Epoch: 1511 [256/118836 (0%)] Loss: 12298.056641\n",
      "Train Epoch: 1511 [33024/118836 (28%)] Loss: 12295.599609\n",
      "Train Epoch: 1511 [65792/118836 (55%)] Loss: 12340.408203\n",
      "Train Epoch: 1511 [98560/118836 (83%)] Loss: 12186.937500\n",
      "    epoch          : 1511\n",
      "    loss           : 12263.766344699907\n",
      "    val_loss       : 12265.72375411232\n",
      "    val_log_likelihood: -12165.43939367504\n",
      "    val_log_marginal: -12173.645044675446\n",
      "Train Epoch: 1512 [256/118836 (0%)] Loss: 12318.265625\n",
      "Train Epoch: 1512 [33024/118836 (28%)] Loss: 12255.253906\n",
      "Train Epoch: 1512 [65792/118836 (55%)] Loss: 12210.430664\n",
      "Train Epoch: 1512 [98560/118836 (83%)] Loss: 12289.005859\n",
      "    epoch          : 1512\n",
      "    loss           : 12265.447502617091\n",
      "    val_loss       : 12264.479173713738\n",
      "    val_log_likelihood: -12162.66140906095\n",
      "    val_log_marginal: -12170.9071711638\n",
      "Train Epoch: 1513 [256/118836 (0%)] Loss: 12294.264648\n",
      "Train Epoch: 1513 [33024/118836 (28%)] Loss: 12347.888672\n",
      "Train Epoch: 1513 [65792/118836 (55%)] Loss: 12313.593750\n",
      "Train Epoch: 1513 [98560/118836 (83%)] Loss: 12402.871094\n",
      "    epoch          : 1513\n",
      "    loss           : 12264.613399342174\n",
      "    val_loss       : 12260.63381498857\n",
      "    val_log_likelihood: -12165.807501033913\n",
      "    val_log_marginal: -12173.824799842752\n",
      "Train Epoch: 1514 [256/118836 (0%)] Loss: 12194.400391\n",
      "Train Epoch: 1514 [33024/118836 (28%)] Loss: 12225.430664\n",
      "Train Epoch: 1514 [65792/118836 (55%)] Loss: 12405.817383\n",
      "Train Epoch: 1514 [98560/118836 (83%)] Loss: 12294.193359\n",
      "    epoch          : 1514\n",
      "    loss           : 12263.826530351787\n",
      "    val_loss       : 12265.569229161445\n",
      "    val_log_likelihood: -12161.831485215054\n",
      "    val_log_marginal: -12169.821882513905\n",
      "Train Epoch: 1515 [256/118836 (0%)] Loss: 12296.213867\n",
      "Train Epoch: 1515 [33024/118836 (28%)] Loss: 12300.622070\n",
      "Train Epoch: 1515 [65792/118836 (55%)] Loss: 12279.814453\n",
      "Train Epoch: 1515 [98560/118836 (83%)] Loss: 12253.128906\n",
      "    epoch          : 1515\n",
      "    loss           : 12266.871566603339\n",
      "    val_loss       : 12262.78913857952\n",
      "    val_log_likelihood: -12166.17974662686\n",
      "    val_log_marginal: -12174.458282860916\n",
      "Train Epoch: 1516 [256/118836 (0%)] Loss: 12217.843750\n",
      "Train Epoch: 1516 [33024/118836 (28%)] Loss: 12293.404297\n",
      "Train Epoch: 1516 [65792/118836 (55%)] Loss: 12244.018555\n",
      "Train Epoch: 1516 [98560/118836 (83%)] Loss: 12368.560547\n",
      "    epoch          : 1516\n",
      "    loss           : 12266.483337048956\n",
      "    val_loss       : 12258.961009669489\n",
      "    val_log_likelihood: -12166.082850302419\n",
      "    val_log_marginal: -12174.117904481653\n",
      "Train Epoch: 1517 [256/118836 (0%)] Loss: 12232.619141\n",
      "Train Epoch: 1517 [33024/118836 (28%)] Loss: 12383.088867\n",
      "Train Epoch: 1517 [65792/118836 (55%)] Loss: 12348.810547\n",
      "Train Epoch: 1517 [98560/118836 (83%)] Loss: 12301.885742\n",
      "    epoch          : 1517\n",
      "    loss           : 12266.01320306038\n",
      "    val_loss       : 12261.9906468858\n",
      "    val_log_likelihood: -12167.671132521711\n",
      "    val_log_marginal: -12175.792417684152\n",
      "Train Epoch: 1518 [256/118836 (0%)] Loss: 12291.996094\n",
      "Train Epoch: 1518 [33024/118836 (28%)] Loss: 12284.666016\n",
      "Train Epoch: 1518 [65792/118836 (55%)] Loss: 12369.004883\n",
      "Train Epoch: 1518 [98560/118836 (83%)] Loss: 12217.046875\n",
      "    epoch          : 1518\n",
      "    loss           : 12265.992424007445\n",
      "    val_loss       : 12259.83006483469\n",
      "    val_log_likelihood: -12163.4012999832\n",
      "    val_log_marginal: -12171.386556899955\n",
      "Train Epoch: 1519 [256/118836 (0%)] Loss: 12224.976562\n",
      "Train Epoch: 1519 [33024/118836 (28%)] Loss: 12240.524414\n",
      "Train Epoch: 1519 [65792/118836 (55%)] Loss: 12199.610352\n",
      "Train Epoch: 1519 [98560/118836 (83%)] Loss: 12292.733398\n",
      "    epoch          : 1519\n",
      "    loss           : 12261.263827608043\n",
      "    val_loss       : 12264.378070570725\n",
      "    val_log_likelihood: -12164.643448549938\n",
      "    val_log_marginal: -12172.825056608284\n",
      "Train Epoch: 1520 [256/118836 (0%)] Loss: 12187.976562\n",
      "Train Epoch: 1520 [33024/118836 (28%)] Loss: 12232.162109\n",
      "Train Epoch: 1520 [65792/118836 (55%)] Loss: 12358.554688\n",
      "Train Epoch: 1520 [98560/118836 (83%)] Loss: 12201.684570\n",
      "    epoch          : 1520\n",
      "    loss           : 12264.00299705335\n",
      "    val_loss       : 12262.649576921538\n",
      "    val_log_likelihood: -12165.913287550404\n",
      "    val_log_marginal: -12174.257832722085\n",
      "Train Epoch: 1521 [256/118836 (0%)] Loss: 12174.672852\n",
      "Train Epoch: 1521 [33024/118836 (28%)] Loss: 12238.685547\n",
      "Train Epoch: 1521 [65792/118836 (55%)] Loss: 12257.720703\n",
      "Train Epoch: 1521 [98560/118836 (83%)] Loss: 12350.439453\n",
      "    epoch          : 1521\n",
      "    loss           : 12260.121676133427\n",
      "    val_loss       : 12265.887166698749\n",
      "    val_log_likelihood: -12167.862387077388\n",
      "    val_log_marginal: -12175.902853527807\n",
      "Train Epoch: 1522 [256/118836 (0%)] Loss: 12301.694336\n",
      "Train Epoch: 1522 [33024/118836 (28%)] Loss: 12218.910156\n",
      "Train Epoch: 1522 [65792/118836 (55%)] Loss: 12256.021484\n",
      "Train Epoch: 1522 [98560/118836 (83%)] Loss: 12227.480469\n",
      "    epoch          : 1522\n",
      "    loss           : 12259.2575430366\n",
      "    val_loss       : 12268.51780397752\n",
      "    val_log_likelihood: -12164.895700540219\n",
      "    val_log_marginal: -12172.928102113057\n",
      "Train Epoch: 1523 [256/118836 (0%)] Loss: 12255.163086\n",
      "Train Epoch: 1523 [33024/118836 (28%)] Loss: 12445.829102\n",
      "Train Epoch: 1523 [65792/118836 (55%)] Loss: 12177.367188\n",
      "Train Epoch: 1523 [98560/118836 (83%)] Loss: 12244.462891\n",
      "    epoch          : 1523\n",
      "    loss           : 12260.388816945824\n",
      "    val_loss       : 12260.813978642045\n",
      "    val_log_likelihood: -12164.81498639759\n",
      "    val_log_marginal: -12172.809180372793\n",
      "Train Epoch: 1524 [256/118836 (0%)] Loss: 12329.420898\n",
      "Train Epoch: 1524 [33024/118836 (28%)] Loss: 12222.999023\n",
      "Train Epoch: 1524 [65792/118836 (55%)] Loss: 12253.416016\n",
      "Train Epoch: 1524 [98560/118836 (83%)] Loss: 12327.389648\n",
      "    epoch          : 1524\n",
      "    loss           : 12262.116012555573\n",
      "    val_loss       : 12257.821240257084\n",
      "    val_log_likelihood: -12162.110133956265\n",
      "    val_log_marginal: -12170.258475440341\n",
      "Train Epoch: 1525 [256/118836 (0%)] Loss: 12250.470703\n",
      "Train Epoch: 1525 [33024/118836 (28%)] Loss: 12421.192383\n",
      "Train Epoch: 1525 [65792/118836 (55%)] Loss: 12208.508789\n",
      "Train Epoch: 1525 [98560/118836 (83%)] Loss: 12238.691406\n",
      "    epoch          : 1525\n",
      "    loss           : 12264.670790361353\n",
      "    val_loss       : 12267.164521626095\n",
      "    val_log_likelihood: -12162.678411426023\n",
      "    val_log_marginal: -12170.694373488048\n",
      "Train Epoch: 1526 [256/118836 (0%)] Loss: 12177.882812\n",
      "Train Epoch: 1526 [33024/118836 (28%)] Loss: 12145.993164\n",
      "Train Epoch: 1526 [65792/118836 (55%)] Loss: 12314.458984\n",
      "Train Epoch: 1526 [98560/118836 (83%)] Loss: 12218.962891\n",
      "    epoch          : 1526\n",
      "    loss           : 12262.07223460763\n",
      "    val_loss       : 12267.147578528757\n",
      "    val_log_likelihood: -12162.339231964692\n",
      "    val_log_marginal: -12170.395147000478\n",
      "Train Epoch: 1527 [256/118836 (0%)] Loss: 12279.828125\n",
      "Train Epoch: 1527 [33024/118836 (28%)] Loss: 12320.441406\n",
      "Train Epoch: 1527 [65792/118836 (55%)] Loss: 12257.835938\n",
      "Train Epoch: 1527 [98560/118836 (83%)] Loss: 12364.682617\n",
      "    epoch          : 1527\n",
      "    loss           : 12266.029413513234\n",
      "    val_loss       : 12266.471622201738\n",
      "    val_log_likelihood: -12168.911451709833\n",
      "    val_log_marginal: -12177.171396910351\n",
      "Train Epoch: 1528 [256/118836 (0%)] Loss: 12217.908203\n",
      "Train Epoch: 1528 [33024/118836 (28%)] Loss: 12212.089844\n",
      "Train Epoch: 1528 [65792/118836 (55%)] Loss: 12265.888672\n",
      "Train Epoch: 1528 [98560/118836 (83%)] Loss: 12262.996094\n",
      "    epoch          : 1528\n",
      "    loss           : 12261.555927225496\n",
      "    val_loss       : 12267.641329073022\n",
      "    val_log_likelihood: -12165.686451709833\n",
      "    val_log_marginal: -12173.860977633103\n",
      "Train Epoch: 1529 [256/118836 (0%)] Loss: 12447.239258\n",
      "Train Epoch: 1529 [33024/118836 (28%)] Loss: 12399.541992\n",
      "Train Epoch: 1529 [65792/118836 (55%)] Loss: 12309.587891\n",
      "Train Epoch: 1529 [98560/118836 (83%)] Loss: 12334.580078\n",
      "    epoch          : 1529\n",
      "    loss           : 12266.04566952285\n",
      "    val_loss       : 12259.87075855622\n",
      "    val_log_likelihood: -12163.411360273212\n",
      "    val_log_marginal: -12171.428309149502\n",
      "Train Epoch: 1530 [256/118836 (0%)] Loss: 12320.140625\n",
      "Train Epoch: 1530 [33024/118836 (28%)] Loss: 12365.576172\n",
      "Train Epoch: 1530 [65792/118836 (55%)] Loss: 12244.982422\n",
      "Train Epoch: 1530 [98560/118836 (83%)] Loss: 12263.538086\n",
      "    epoch          : 1530\n",
      "    loss           : 12263.480715111922\n",
      "    val_loss       : 12259.004247480489\n",
      "    val_log_likelihood: -12163.819743525124\n",
      "    val_log_marginal: -12171.792864629362\n",
      "Train Epoch: 1531 [256/118836 (0%)] Loss: 12322.607422\n",
      "Train Epoch: 1531 [33024/118836 (28%)] Loss: 12255.398438\n",
      "Train Epoch: 1531 [65792/118836 (55%)] Loss: 12287.412109\n",
      "Train Epoch: 1531 [98560/118836 (83%)] Loss: 12254.179688\n",
      "    epoch          : 1531\n",
      "    loss           : 12263.622577737284\n",
      "    val_loss       : 12261.451911121447\n",
      "    val_log_likelihood: -12165.604688630841\n",
      "    val_log_marginal: -12173.753990105753\n",
      "Train Epoch: 1532 [256/118836 (0%)] Loss: 12260.991211\n",
      "Train Epoch: 1532 [33024/118836 (28%)] Loss: 12284.862305\n",
      "Train Epoch: 1532 [65792/118836 (55%)] Loss: 12283.835938\n",
      "Train Epoch: 1532 [98560/118836 (83%)] Loss: 12260.369141\n",
      "    epoch          : 1532\n",
      "    loss           : 12268.200602900124\n",
      "    val_loss       : 12265.973909585378\n",
      "    val_log_likelihood: -12166.556401209677\n",
      "    val_log_marginal: -12174.593213392773\n",
      "Train Epoch: 1533 [256/118836 (0%)] Loss: 12293.048828\n",
      "Train Epoch: 1533 [33024/118836 (28%)] Loss: 12226.323242\n",
      "Train Epoch: 1533 [65792/118836 (55%)] Loss: 12261.887695\n",
      "Train Epoch: 1533 [98560/118836 (83%)] Loss: 12344.994141\n",
      "    epoch          : 1533\n",
      "    loss           : 12267.309577097549\n",
      "    val_loss       : 12263.491463721846\n",
      "    val_log_likelihood: -12165.93047828138\n",
      "    val_log_marginal: -12173.786419947955\n",
      "Train Epoch: 1534 [256/118836 (0%)] Loss: 12315.201172\n",
      "Train Epoch: 1534 [33024/118836 (28%)] Loss: 12244.356445\n",
      "Train Epoch: 1534 [65792/118836 (55%)] Loss: 12242.501953\n",
      "Train Epoch: 1534 [98560/118836 (83%)] Loss: 12308.050781\n",
      "    epoch          : 1534\n",
      "    loss           : 12262.816762949753\n",
      "    val_loss       : 12269.4096716926\n",
      "    val_log_likelihood: -12167.794670989713\n",
      "    val_log_marginal: -12175.8419376408\n",
      "Train Epoch: 1535 [256/118836 (0%)] Loss: 12238.767578\n",
      "Train Epoch: 1535 [33024/118836 (28%)] Loss: 12295.833984\n",
      "Train Epoch: 1535 [65792/118836 (55%)] Loss: 12190.254883\n",
      "Train Epoch: 1535 [98560/118836 (83%)] Loss: 12287.078125\n",
      "    epoch          : 1535\n",
      "    loss           : 12264.90571381953\n",
      "    val_loss       : 12264.291655385257\n",
      "    val_log_likelihood: -12165.335618764217\n",
      "    val_log_marginal: -12173.455431658254\n",
      "Train Epoch: 1536 [256/118836 (0%)] Loss: 12434.566406\n",
      "Train Epoch: 1536 [33024/118836 (28%)] Loss: 12208.249023\n",
      "Train Epoch: 1536 [65792/118836 (55%)] Loss: 12289.386719\n",
      "Train Epoch: 1536 [98560/118836 (83%)] Loss: 12270.529297\n",
      "    epoch          : 1536\n",
      "    loss           : 12270.61570852073\n",
      "    val_loss       : 12264.126484660534\n",
      "    val_log_likelihood: -12165.291171519593\n",
      "    val_log_marginal: -12173.31059479757\n",
      "Train Epoch: 1537 [256/118836 (0%)] Loss: 12407.509766\n",
      "Train Epoch: 1537 [33024/118836 (28%)] Loss: 12243.611328\n",
      "Train Epoch: 1537 [65792/118836 (55%)] Loss: 12300.305664\n",
      "Train Epoch: 1537 [98560/118836 (83%)] Loss: 12277.120117\n",
      "    epoch          : 1537\n",
      "    loss           : 12259.475500962832\n",
      "    val_loss       : 12260.982777645431\n",
      "    val_log_likelihood: -12166.811239434708\n",
      "    val_log_marginal: -12174.796479537916\n",
      "Train Epoch: 1538 [256/118836 (0%)] Loss: 12391.175781\n",
      "Train Epoch: 1538 [33024/118836 (28%)] Loss: 12270.055664\n",
      "Train Epoch: 1538 [65792/118836 (55%)] Loss: 12307.787109\n",
      "Train Epoch: 1538 [98560/118836 (83%)] Loss: 12316.007812\n",
      "    epoch          : 1538\n",
      "    loss           : 12261.814610473532\n",
      "    val_loss       : 12269.604998826144\n",
      "    val_log_likelihood: -12166.202137775279\n",
      "    val_log_marginal: -12174.42862877138\n",
      "Train Epoch: 1539 [256/118836 (0%)] Loss: 12335.381836\n",
      "Train Epoch: 1539 [33024/118836 (28%)] Loss: 12325.480469\n",
      "Train Epoch: 1539 [65792/118836 (55%)] Loss: 12356.123047\n",
      "Train Epoch: 1539 [98560/118836 (83%)] Loss: 12263.010742\n",
      "    epoch          : 1539\n",
      "    loss           : 12263.31427671371\n",
      "    val_loss       : 12264.83940718514\n",
      "    val_log_likelihood: -12166.007848686932\n",
      "    val_log_marginal: -12174.008040938463\n",
      "Train Epoch: 1540 [256/118836 (0%)] Loss: 12208.369141\n",
      "Train Epoch: 1540 [33024/118836 (28%)] Loss: 12312.163086\n",
      "Train Epoch: 1540 [65792/118836 (55%)] Loss: 12259.269531\n",
      "Train Epoch: 1540 [98560/118836 (83%)] Loss: 12255.111328\n",
      "    epoch          : 1540\n",
      "    loss           : 12261.60161225703\n",
      "    val_loss       : 12260.421945827831\n",
      "    val_log_likelihood: -12164.003437920026\n",
      "    val_log_marginal: -12172.030753974122\n",
      "Train Epoch: 1541 [256/118836 (0%)] Loss: 12354.934570\n",
      "Train Epoch: 1541 [33024/118836 (28%)] Loss: 12237.591797\n",
      "Train Epoch: 1541 [65792/118836 (55%)] Loss: 12322.821289\n",
      "Train Epoch: 1541 [98560/118836 (83%)] Loss: 12307.923828\n",
      "    epoch          : 1541\n",
      "    loss           : 12264.935105523677\n",
      "    val_loss       : 12261.001882592513\n",
      "    val_log_likelihood: -12164.549571411031\n",
      "    val_log_marginal: -12172.627393015402\n",
      "Train Epoch: 1542 [256/118836 (0%)] Loss: 12234.564453\n",
      "Train Epoch: 1542 [33024/118836 (28%)] Loss: 12268.072266\n",
      "Train Epoch: 1542 [65792/118836 (55%)] Loss: 12274.793945\n",
      "Train Epoch: 1542 [98560/118836 (83%)] Loss: 12265.567383\n",
      "    epoch          : 1542\n",
      "    loss           : 12262.60031938198\n",
      "    val_loss       : 12266.542674598606\n",
      "    val_log_likelihood: -12166.123160443807\n",
      "    val_log_marginal: -12174.183318599333\n",
      "Train Epoch: 1543 [256/118836 (0%)] Loss: 12350.086914\n",
      "Train Epoch: 1543 [33024/118836 (28%)] Loss: 12295.123047\n",
      "Train Epoch: 1543 [65792/118836 (55%)] Loss: 12245.830078\n",
      "Train Epoch: 1543 [98560/118836 (83%)] Loss: 12214.853516\n",
      "    epoch          : 1543\n",
      "    loss           : 12257.734551249741\n",
      "    val_loss       : 12263.242376612397\n",
      "    val_log_likelihood: -12164.324706627378\n",
      "    val_log_marginal: -12172.303116514879\n",
      "Train Epoch: 1544 [256/118836 (0%)] Loss: 12265.988281\n",
      "Train Epoch: 1544 [33024/118836 (28%)] Loss: 12276.114258\n",
      "Train Epoch: 1544 [65792/118836 (55%)] Loss: 12260.689453\n",
      "Train Epoch: 1544 [98560/118836 (83%)] Loss: 12209.132812\n",
      "    epoch          : 1544\n",
      "    loss           : 12267.715058286807\n",
      "    val_loss       : 12262.542144982324\n",
      "    val_log_likelihood: -12163.421734614092\n",
      "    val_log_marginal: -12171.44295877091\n",
      "Train Epoch: 1545 [256/118836 (0%)] Loss: 12250.420898\n",
      "Train Epoch: 1545 [33024/118836 (28%)] Loss: 12319.427734\n",
      "Train Epoch: 1545 [65792/118836 (55%)] Loss: 12304.023438\n",
      "Train Epoch: 1545 [98560/118836 (83%)] Loss: 12255.675781\n",
      "    epoch          : 1545\n",
      "    loss           : 12263.152798671423\n",
      "    val_loss       : 12261.470018689652\n",
      "    val_log_likelihood: -12162.897093090882\n",
      "    val_log_marginal: -12170.933166483515\n",
      "Train Epoch: 1546 [256/118836 (0%)] Loss: 12194.339844\n",
      "Train Epoch: 1546 [33024/118836 (28%)] Loss: 12223.768555\n",
      "Train Epoch: 1546 [65792/118836 (55%)] Loss: 12217.197266\n",
      "Train Epoch: 1546 [98560/118836 (83%)] Loss: 12288.423828\n",
      "    epoch          : 1546\n",
      "    loss           : 12263.836208740435\n",
      "    val_loss       : 12264.847587757431\n",
      "    val_log_likelihood: -12166.001526797714\n",
      "    val_log_marginal: -12174.008060600092\n",
      "Train Epoch: 1547 [256/118836 (0%)] Loss: 12328.885742\n",
      "Train Epoch: 1547 [33024/118836 (28%)] Loss: 12333.900391\n",
      "Train Epoch: 1547 [65792/118836 (55%)] Loss: 12294.130859\n",
      "Train Epoch: 1547 [98560/118836 (83%)] Loss: 12332.917969\n",
      "    epoch          : 1547\n",
      "    loss           : 12259.44556855485\n",
      "    val_loss       : 12264.087304022058\n",
      "    val_log_likelihood: -12165.554092838865\n",
      "    val_log_marginal: -12173.456894373798\n",
      "Train Epoch: 1548 [256/118836 (0%)] Loss: 12316.628906\n",
      "Train Epoch: 1548 [33024/118836 (28%)] Loss: 12349.522461\n",
      "Train Epoch: 1548 [65792/118836 (55%)] Loss: 12278.604492\n",
      "Train Epoch: 1548 [98560/118836 (83%)] Loss: 12272.598633\n",
      "    epoch          : 1548\n",
      "    loss           : 12260.216680721413\n",
      "    val_loss       : 12266.894239654943\n",
      "    val_log_likelihood: -12165.630042584264\n",
      "    val_log_marginal: -12173.648025124336\n",
      "Train Epoch: 1549 [256/118836 (0%)] Loss: 12351.280273\n",
      "Train Epoch: 1549 [33024/118836 (28%)] Loss: 12275.657227\n",
      "Train Epoch: 1549 [65792/118836 (55%)] Loss: 12315.807617\n",
      "Train Epoch: 1549 [98560/118836 (83%)] Loss: 12328.552734\n",
      "    epoch          : 1549\n",
      "    loss           : 12265.18614202078\n",
      "    val_loss       : 12262.166508841843\n",
      "    val_log_likelihood: -12167.892731596361\n",
      "    val_log_marginal: -12175.896635643408\n",
      "Train Epoch: 1550 [256/118836 (0%)] Loss: 12238.947266\n",
      "Train Epoch: 1550 [33024/118836 (28%)] Loss: 12256.857422\n",
      "Train Epoch: 1550 [65792/118836 (55%)] Loss: 12190.077148\n",
      "Train Epoch: 1550 [98560/118836 (83%)] Loss: 12340.493164\n",
      "    epoch          : 1550\n",
      "    loss           : 12267.38782729787\n",
      "    val_loss       : 12257.552059098456\n",
      "    val_log_likelihood: -12162.864402721774\n",
      "    val_log_marginal: -12170.83446168352\n",
      "Train Epoch: 1551 [256/118836 (0%)] Loss: 12290.283203\n",
      "Train Epoch: 1551 [33024/118836 (28%)] Loss: 12369.412109\n",
      "Train Epoch: 1551 [65792/118836 (55%)] Loss: 12344.349609\n",
      "Train Epoch: 1551 [98560/118836 (83%)] Loss: 12427.693359\n",
      "    epoch          : 1551\n",
      "    loss           : 12263.703138893197\n",
      "    val_loss       : 12261.343663970762\n",
      "    val_log_likelihood: -12165.296060955592\n",
      "    val_log_marginal: -12173.318469540394\n",
      "Train Epoch: 1552 [256/118836 (0%)] Loss: 12222.229492\n",
      "Train Epoch: 1552 [33024/118836 (28%)] Loss: 12354.561523\n",
      "Train Epoch: 1552 [65792/118836 (55%)] Loss: 12228.306641\n",
      "Train Epoch: 1552 [98560/118836 (83%)] Loss: 12321.984375\n",
      "    epoch          : 1552\n",
      "    loss           : 12261.358963858302\n",
      "    val_loss       : 12262.67786062952\n",
      "    val_log_likelihood: -12164.681857100393\n",
      "    val_log_marginal: -12172.70826421666\n",
      "Train Epoch: 1553 [256/118836 (0%)] Loss: 12383.899414\n",
      "Train Epoch: 1553 [33024/118836 (28%)] Loss: 12254.123047\n",
      "Train Epoch: 1553 [65792/118836 (55%)] Loss: 12247.063477\n",
      "Train Epoch: 1553 [98560/118836 (83%)] Loss: 12218.197266\n",
      "    epoch          : 1553\n",
      "    loss           : 12262.638119055004\n",
      "    val_loss       : 12268.699347250396\n",
      "    val_log_likelihood: -12163.977726459108\n",
      "    val_log_marginal: -12172.132942250357\n",
      "Train Epoch: 1554 [256/118836 (0%)] Loss: 12376.514648\n",
      "Train Epoch: 1554 [33024/118836 (28%)] Loss: 12396.635742\n",
      "Train Epoch: 1554 [65792/118836 (55%)] Loss: 12287.871094\n",
      "Train Epoch: 1554 [98560/118836 (83%)] Loss: 12305.546875\n",
      "    epoch          : 1554\n",
      "    loss           : 12265.55031776649\n",
      "    val_loss       : 12261.022364723685\n",
      "    val_log_likelihood: -12163.356938520988\n",
      "    val_log_marginal: -12171.380958547114\n",
      "Train Epoch: 1555 [256/118836 (0%)] Loss: 12403.785156\n",
      "Train Epoch: 1555 [33024/118836 (28%)] Loss: 12264.195312\n",
      "Train Epoch: 1555 [65792/118836 (55%)] Loss: 12297.505859\n",
      "Train Epoch: 1555 [98560/118836 (83%)] Loss: 12342.528320\n",
      "    epoch          : 1555\n",
      "    loss           : 12259.791118854682\n",
      "    val_loss       : 12262.072318380247\n",
      "    val_log_likelihood: -12166.178991870864\n",
      "    val_log_marginal: -12174.302208427205\n",
      "Train Epoch: 1556 [256/118836 (0%)] Loss: 12276.278320\n",
      "Train Epoch: 1556 [33024/118836 (28%)] Loss: 12342.468750\n",
      "Train Epoch: 1556 [65792/118836 (55%)] Loss: 12353.249023\n",
      "Train Epoch: 1556 [98560/118836 (83%)] Loss: 12273.838867\n",
      "    epoch          : 1556\n",
      "    loss           : 12262.479676676232\n",
      "    val_loss       : 12255.465387941098\n",
      "    val_log_likelihood: -12163.638925668167\n",
      "    val_log_marginal: -12171.554676428423\n",
      "Train Epoch: 1557 [256/118836 (0%)] Loss: 12213.244141\n",
      "Train Epoch: 1557 [33024/118836 (28%)] Loss: 12228.702148\n",
      "Train Epoch: 1557 [65792/118836 (55%)] Loss: 12258.306641\n",
      "Train Epoch: 1557 [98560/118836 (83%)] Loss: 12266.602539\n",
      "    epoch          : 1557\n",
      "    loss           : 12262.831589575579\n",
      "    val_loss       : 12266.669588347915\n",
      "    val_log_likelihood: -12167.186319886012\n",
      "    val_log_marginal: -12175.534428696248\n",
      "Train Epoch: 1558 [256/118836 (0%)] Loss: 12254.573242\n",
      "Train Epoch: 1558 [33024/118836 (28%)] Loss: 12427.999023\n",
      "Train Epoch: 1558 [65792/118836 (55%)] Loss: 12229.447266\n",
      "Train Epoch: 1558 [98560/118836 (83%)] Loss: 12266.237305\n",
      "    epoch          : 1558\n",
      "    loss           : 12262.720847549628\n",
      "    val_loss       : 12264.874844594664\n",
      "    val_log_likelihood: -12164.39230187655\n",
      "    val_log_marginal: -12172.721866561096\n",
      "Train Epoch: 1559 [256/118836 (0%)] Loss: 12335.439453\n",
      "Train Epoch: 1559 [33024/118836 (28%)] Loss: 12373.841797\n",
      "Train Epoch: 1559 [65792/118836 (55%)] Loss: 12222.453125\n",
      "Train Epoch: 1559 [98560/118836 (83%)] Loss: 12208.479492\n",
      "    epoch          : 1559\n",
      "    loss           : 12262.758101187706\n",
      "    val_loss       : 12266.27388886755\n",
      "    val_log_likelihood: -12164.715268946444\n",
      "    val_log_marginal: -12172.91278365862\n",
      "Train Epoch: 1560 [256/118836 (0%)] Loss: 12302.250977\n",
      "Train Epoch: 1560 [33024/118836 (28%)] Loss: 12183.104492\n",
      "Train Epoch: 1560 [65792/118836 (55%)] Loss: 12288.152344\n",
      "Train Epoch: 1560 [98560/118836 (83%)] Loss: 12273.465820\n",
      "    epoch          : 1560\n",
      "    loss           : 12257.821087611146\n",
      "    val_loss       : 12265.404124624953\n",
      "    val_log_likelihood: -12167.7295729619\n",
      "    val_log_marginal: -12175.733439974889\n",
      "Train Epoch: 1561 [256/118836 (0%)] Loss: 12366.657227\n",
      "Train Epoch: 1561 [33024/118836 (28%)] Loss: 12266.546875\n",
      "Train Epoch: 1561 [65792/118836 (55%)] Loss: 12328.364258\n",
      "Train Epoch: 1561 [98560/118836 (83%)] Loss: 12239.381836\n",
      "    epoch          : 1561\n",
      "    loss           : 12261.412126176076\n",
      "    val_loss       : 12262.129868166747\n",
      "    val_log_likelihood: -12166.864526306608\n",
      "    val_log_marginal: -12174.922000564571\n",
      "Train Epoch: 1562 [256/118836 (0%)] Loss: 12227.221680\n",
      "Train Epoch: 1562 [33024/118836 (28%)] Loss: 12138.763672\n",
      "Train Epoch: 1562 [65792/118836 (55%)] Loss: 12269.763672\n",
      "Train Epoch: 1562 [98560/118836 (83%)] Loss: 12251.666016\n",
      "    epoch          : 1562\n",
      "    loss           : 12259.763274949597\n",
      "    val_loss       : 12263.634157218665\n",
      "    val_log_likelihood: -12164.506389416614\n",
      "    val_log_marginal: -12172.678394537443\n",
      "Train Epoch: 1563 [256/118836 (0%)] Loss: 12304.257812\n",
      "Train Epoch: 1563 [33024/118836 (28%)] Loss: 12251.803711\n",
      "Train Epoch: 1563 [65792/118836 (55%)] Loss: 12327.379883\n",
      "Train Epoch: 1563 [98560/118836 (83%)] Loss: 12232.983398\n",
      "    epoch          : 1563\n",
      "    loss           : 12262.433937525848\n",
      "    val_loss       : 12255.973333825223\n",
      "    val_log_likelihood: -12162.934238491263\n",
      "    val_log_marginal: -12170.886461025433\n",
      "Train Epoch: 1564 [256/118836 (0%)] Loss: 12229.125000\n",
      "Train Epoch: 1564 [33024/118836 (28%)] Loss: 12219.440430\n",
      "Train Epoch: 1564 [65792/118836 (55%)] Loss: 12224.056641\n",
      "Train Epoch: 1564 [98560/118836 (83%)] Loss: 12197.500000\n",
      "    epoch          : 1564\n",
      "    loss           : 12259.447371601012\n",
      "    val_loss       : 12259.702536666378\n",
      "    val_log_likelihood: -12164.141667151313\n",
      "    val_log_marginal: -12172.137190005384\n",
      "Train Epoch: 1565 [256/118836 (0%)] Loss: 12231.024414\n",
      "Train Epoch: 1565 [33024/118836 (28%)] Loss: 12301.814453\n",
      "Train Epoch: 1565 [65792/118836 (55%)] Loss: 12284.195312\n",
      "Train Epoch: 1565 [98560/118836 (83%)] Loss: 12267.191406\n",
      "    epoch          : 1565\n",
      "    loss           : 12263.94132789883\n",
      "    val_loss       : 12260.59748096481\n",
      "    val_log_likelihood: -12164.859111998552\n",
      "    val_log_marginal: -12173.094893972151\n",
      "Train Epoch: 1566 [256/118836 (0%)] Loss: 12194.210938\n",
      "Train Epoch: 1566 [33024/118836 (28%)] Loss: 12227.067383\n",
      "Train Epoch: 1566 [65792/118836 (55%)] Loss: 12219.464844\n",
      "Train Epoch: 1566 [98560/118836 (83%)] Loss: 12178.923828\n",
      "    epoch          : 1566\n",
      "    loss           : 12262.928243899918\n",
      "    val_loss       : 12256.756652201704\n",
      "    val_log_likelihood: -12163.406451451354\n",
      "    val_log_marginal: -12171.355291001255\n",
      "Train Epoch: 1567 [256/118836 (0%)] Loss: 12288.046875\n",
      "Train Epoch: 1567 [33024/118836 (28%)] Loss: 12302.703125\n",
      "Train Epoch: 1567 [65792/118836 (55%)] Loss: 12317.611328\n",
      "Train Epoch: 1567 [98560/118836 (83%)] Loss: 12204.958984\n",
      "    epoch          : 1567\n",
      "    loss           : 12269.151333262253\n",
      "    val_loss       : 12265.318920251493\n",
      "    val_log_likelihood: -12163.076247156741\n",
      "    val_log_marginal: -12171.332940410155\n",
      "Train Epoch: 1568 [256/118836 (0%)] Loss: 12184.513672\n",
      "Train Epoch: 1568 [33024/118836 (28%)] Loss: 12362.916016\n",
      "Train Epoch: 1568 [65792/118836 (55%)] Loss: 12454.181641\n",
      "Train Epoch: 1568 [98560/118836 (83%)] Loss: 12209.613281\n",
      "    epoch          : 1568\n",
      "    loss           : 12262.364051030036\n",
      "    val_loss       : 12270.934927295742\n",
      "    val_log_likelihood: -12174.59611782077\n",
      "    val_log_marginal: -12182.79992507766\n",
      "Train Epoch: 1569 [256/118836 (0%)] Loss: 12229.453125\n",
      "Train Epoch: 1569 [33024/118836 (28%)] Loss: 12246.608398\n",
      "Train Epoch: 1569 [65792/118836 (55%)] Loss: 12421.635742\n",
      "Train Epoch: 1569 [98560/118836 (83%)] Loss: 12225.242188\n",
      "    epoch          : 1569\n",
      "    loss           : 12266.358719434967\n",
      "    val_loss       : 12265.665931514852\n",
      "    val_log_likelihood: -12166.349580780861\n",
      "    val_log_marginal: -12174.639958787187\n",
      "Train Epoch: 1570 [256/118836 (0%)] Loss: 12344.369141\n",
      "Train Epoch: 1570 [33024/118836 (28%)] Loss: 12293.512695\n",
      "Train Epoch: 1570 [65792/118836 (55%)] Loss: 12406.944336\n",
      "Train Epoch: 1570 [98560/118836 (83%)] Loss: 12318.347656\n",
      "    epoch          : 1570\n",
      "    loss           : 12262.554289928401\n",
      "    val_loss       : 12261.367966741744\n",
      "    val_log_likelihood: -12164.793525124069\n",
      "    val_log_marginal: -12172.806852389089\n",
      "Train Epoch: 1571 [256/118836 (0%)] Loss: 12187.716797\n",
      "Train Epoch: 1571 [33024/118836 (28%)] Loss: 12210.732422\n",
      "Train Epoch: 1571 [65792/118836 (55%)] Loss: 12239.457031\n",
      "Train Epoch: 1571 [98560/118836 (83%)] Loss: 12414.755859\n",
      "    epoch          : 1571\n",
      "    loss           : 12265.069561136528\n",
      "    val_loss       : 12263.924234557664\n",
      "    val_log_likelihood: -12165.643273592586\n",
      "    val_log_marginal: -12173.644280165046\n",
      "Train Epoch: 1572 [256/118836 (0%)] Loss: 12288.911133\n",
      "Train Epoch: 1572 [33024/118836 (28%)] Loss: 12381.940430\n",
      "Train Epoch: 1572 [65792/118836 (55%)] Loss: 12247.207031\n",
      "Train Epoch: 1572 [98560/118836 (83%)] Loss: 12306.450195\n",
      "    epoch          : 1572\n",
      "    loss           : 12261.284183726219\n",
      "    val_loss       : 12260.253174402591\n",
      "    val_log_likelihood: -12166.933582764681\n",
      "    val_log_marginal: -12175.121277472492\n",
      "Train Epoch: 1573 [256/118836 (0%)] Loss: 12231.044922\n",
      "Train Epoch: 1573 [33024/118836 (28%)] Loss: 12284.969727\n",
      "Train Epoch: 1573 [65792/118836 (55%)] Loss: 12268.190430\n",
      "Train Epoch: 1573 [98560/118836 (83%)] Loss: 12336.205078\n",
      "    epoch          : 1573\n",
      "    loss           : 12264.701774290479\n",
      "    val_loss       : 12264.533294782668\n",
      "    val_log_likelihood: -12168.29712540064\n",
      "    val_log_marginal: -12176.330143039811\n",
      "Train Epoch: 1574 [256/118836 (0%)] Loss: 12268.296875\n",
      "Train Epoch: 1574 [33024/118836 (28%)] Loss: 12368.814453\n",
      "Train Epoch: 1574 [65792/118836 (55%)] Loss: 12395.444336\n",
      "Train Epoch: 1574 [98560/118836 (83%)] Loss: 12262.778320\n",
      "    epoch          : 1574\n",
      "    loss           : 12270.161108257083\n",
      "    val_loss       : 12266.968252085884\n",
      "    val_log_likelihood: -12164.510285973687\n",
      "    val_log_marginal: -12172.79331114237\n",
      "Train Epoch: 1575 [256/118836 (0%)] Loss: 12355.966797\n",
      "Train Epoch: 1575 [33024/118836 (28%)] Loss: 12306.744141\n",
      "Train Epoch: 1575 [65792/118836 (55%)] Loss: 12261.369141\n",
      "Train Epoch: 1575 [98560/118836 (83%)] Loss: 12271.992188\n",
      "    epoch          : 1575\n",
      "    loss           : 12259.261984013132\n",
      "    val_loss       : 12260.721686809416\n",
      "    val_log_likelihood: -12166.363719531895\n",
      "    val_log_marginal: -12174.345708203067\n",
      "Train Epoch: 1576 [256/118836 (0%)] Loss: 12266.583984\n",
      "Train Epoch: 1576 [33024/118836 (28%)] Loss: 12394.347656\n",
      "Train Epoch: 1576 [65792/118836 (55%)] Loss: 12275.919922\n",
      "Train Epoch: 1576 [98560/118836 (83%)] Loss: 12329.409180\n",
      "    epoch          : 1576\n",
      "    loss           : 12262.067441777814\n",
      "    val_loss       : 12261.422565721792\n",
      "    val_log_likelihood: -12164.482504587984\n",
      "    val_log_marginal: -12172.409109693717\n",
      "Train Epoch: 1577 [256/118836 (0%)] Loss: 12299.561523\n",
      "Train Epoch: 1577 [33024/118836 (28%)] Loss: 12341.261719\n",
      "Train Epoch: 1577 [65792/118836 (55%)] Loss: 12294.150391\n",
      "Train Epoch: 1577 [98560/118836 (83%)] Loss: 12185.685547\n",
      "    epoch          : 1577\n",
      "    loss           : 12261.362296933157\n",
      "    val_loss       : 12261.693909279858\n",
      "    val_log_likelihood: -12164.677191732579\n",
      "    val_log_marginal: -12172.678109095546\n",
      "Train Epoch: 1578 [256/118836 (0%)] Loss: 12317.056641\n",
      "Train Epoch: 1578 [33024/118836 (28%)] Loss: 12264.541992\n",
      "Train Epoch: 1578 [65792/118836 (55%)] Loss: 12249.598633\n",
      "Train Epoch: 1578 [98560/118836 (83%)] Loss: 12323.408203\n",
      "    epoch          : 1578\n",
      "    loss           : 12261.51470497958\n",
      "    val_loss       : 12261.089970768557\n",
      "    val_log_likelihood: -12164.787698866574\n",
      "    val_log_marginal: -12172.74613024021\n",
      "Train Epoch: 1579 [256/118836 (0%)] Loss: 12247.100586\n",
      "Train Epoch: 1579 [33024/118836 (28%)] Loss: 12261.722656\n",
      "Train Epoch: 1579 [65792/118836 (55%)] Loss: 12280.108398\n",
      "Train Epoch: 1579 [98560/118836 (83%)] Loss: 12282.048828\n",
      "    epoch          : 1579\n",
      "    loss           : 12262.308811840881\n",
      "    val_loss       : 12263.090150476928\n",
      "    val_log_likelihood: -12163.880913655397\n",
      "    val_log_marginal: -12171.748385911014\n",
      "Train Epoch: 1580 [256/118836 (0%)] Loss: 12407.716797\n",
      "Train Epoch: 1580 [33024/118836 (28%)] Loss: 12401.757812\n",
      "Train Epoch: 1580 [65792/118836 (55%)] Loss: 12316.423828\n",
      "Train Epoch: 1580 [98560/118836 (83%)] Loss: 12293.118164\n",
      "    epoch          : 1580\n",
      "    loss           : 12263.253459406018\n",
      "    val_loss       : 12262.504891827957\n",
      "    val_log_likelihood: -12163.946721043993\n",
      "    val_log_marginal: -12171.988827200832\n",
      "Train Epoch: 1581 [256/118836 (0%)] Loss: 12332.662109\n",
      "Train Epoch: 1581 [33024/118836 (28%)] Loss: 12194.497070\n",
      "Train Epoch: 1581 [65792/118836 (55%)] Loss: 12330.916016\n",
      "Train Epoch: 1581 [98560/118836 (83%)] Loss: 12231.037109\n",
      "    epoch          : 1581\n",
      "    loss           : 12268.122274348634\n",
      "    val_loss       : 12261.18367513405\n",
      "    val_log_likelihood: -12167.081291679591\n",
      "    val_log_marginal: -12175.068974214731\n",
      "Train Epoch: 1582 [256/118836 (0%)] Loss: 12375.275391\n",
      "Train Epoch: 1582 [33024/118836 (28%)] Loss: 12202.824219\n",
      "Train Epoch: 1582 [65792/118836 (55%)] Loss: 12241.571289\n",
      "Train Epoch: 1582 [98560/118836 (83%)] Loss: 12388.552734\n",
      "    epoch          : 1582\n",
      "    loss           : 12263.225139255066\n",
      "    val_loss       : 12267.214059208836\n",
      "    val_log_likelihood: -12162.265369268249\n",
      "    val_log_marginal: -12170.227387349893\n",
      "Train Epoch: 1583 [256/118836 (0%)] Loss: 12188.697266\n",
      "Train Epoch: 1583 [33024/118836 (28%)] Loss: 12354.958008\n",
      "Train Epoch: 1583 [65792/118836 (55%)] Loss: 12257.371094\n",
      "Train Epoch: 1583 [98560/118836 (83%)] Loss: 12398.402344\n",
      "    epoch          : 1583\n",
      "    loss           : 12261.14291220792\n",
      "    val_loss       : 12266.42631853073\n",
      "    val_log_likelihood: -12165.487532632858\n",
      "    val_log_marginal: -12173.552332836378\n",
      "Train Epoch: 1584 [256/118836 (0%)] Loss: 12359.232422\n",
      "Train Epoch: 1584 [33024/118836 (28%)] Loss: 12362.857422\n",
      "Train Epoch: 1584 [65792/118836 (55%)] Loss: 12219.540039\n",
      "Train Epoch: 1584 [98560/118836 (83%)] Loss: 12271.957031\n",
      "    epoch          : 1584\n",
      "    loss           : 12266.081465183002\n",
      "    val_loss       : 12263.679601546748\n",
      "    val_log_likelihood: -12167.053785249947\n",
      "    val_log_marginal: -12175.057702304002\n",
      "Train Epoch: 1585 [256/118836 (0%)] Loss: 12218.406250\n",
      "Train Epoch: 1585 [33024/118836 (28%)] Loss: 12361.325195\n",
      "Train Epoch: 1585 [65792/118836 (55%)] Loss: 12357.572266\n",
      "Train Epoch: 1585 [98560/118836 (83%)] Loss: 12324.778320\n",
      "    epoch          : 1585\n",
      "    loss           : 12266.569133355304\n",
      "    val_loss       : 12267.368790480099\n",
      "    val_log_likelihood: -12167.880508006358\n",
      "    val_log_marginal: -12175.883682602176\n",
      "Train Epoch: 1586 [256/118836 (0%)] Loss: 12385.962891\n",
      "Train Epoch: 1586 [33024/118836 (28%)] Loss: 12329.771484\n",
      "Train Epoch: 1586 [65792/118836 (55%)] Loss: 12372.165039\n",
      "Train Epoch: 1586 [98560/118836 (83%)] Loss: 12356.953125\n",
      "    epoch          : 1586\n",
      "    loss           : 12261.579761812447\n",
      "    val_loss       : 12259.036756254529\n",
      "    val_log_likelihood: -12166.133422346722\n",
      "    val_log_marginal: -12174.136794738153\n",
      "Train Epoch: 1587 [256/118836 (0%)] Loss: 12277.078125\n",
      "Train Epoch: 1587 [33024/118836 (28%)] Loss: 12372.392578\n",
      "Train Epoch: 1587 [65792/118836 (55%)] Loss: 12204.911133\n",
      "Train Epoch: 1587 [98560/118836 (83%)] Loss: 12183.319336\n",
      "    epoch          : 1587\n",
      "    loss           : 12263.861665115797\n",
      "    val_loss       : 12264.125748778952\n",
      "    val_log_likelihood: -12165.63964827595\n",
      "    val_log_marginal: -12173.669093368711\n",
      "Train Epoch: 1588 [256/118836 (0%)] Loss: 12157.163086\n",
      "Train Epoch: 1588 [33024/118836 (28%)] Loss: 12242.234375\n",
      "Train Epoch: 1588 [65792/118836 (55%)] Loss: 12341.529297\n",
      "Train Epoch: 1588 [98560/118836 (83%)] Loss: 12276.482422\n",
      "    epoch          : 1588\n",
      "    loss           : 12264.72624053324\n",
      "    val_loss       : 12263.865591507858\n",
      "    val_log_likelihood: -12167.381057595378\n",
      "    val_log_marginal: -12175.592832381342\n",
      "Train Epoch: 1589 [256/118836 (0%)] Loss: 12296.580078\n",
      "Train Epoch: 1589 [33024/118836 (28%)] Loss: 12367.900391\n",
      "Train Epoch: 1589 [65792/118836 (55%)] Loss: 12152.943359\n",
      "Train Epoch: 1589 [98560/118836 (83%)] Loss: 12385.270508\n",
      "    epoch          : 1589\n",
      "    loss           : 12263.07932498449\n",
      "    val_loss       : 12260.09703432396\n",
      "    val_log_likelihood: -12166.11089162014\n",
      "    val_log_marginal: -12174.077050242811\n",
      "Train Epoch: 1590 [256/118836 (0%)] Loss: 12372.246094\n",
      "Train Epoch: 1590 [33024/118836 (28%)] Loss: 12303.503906\n",
      "Train Epoch: 1590 [65792/118836 (55%)] Loss: 12321.909180\n",
      "Train Epoch: 1590 [98560/118836 (83%)] Loss: 12310.510742\n",
      "    epoch          : 1590\n",
      "    loss           : 12259.231459367245\n",
      "    val_loss       : 12260.280842464896\n",
      "    val_log_likelihood: -12165.463148779983\n",
      "    val_log_marginal: -12173.513034204554\n",
      "Train Epoch: 1591 [256/118836 (0%)] Loss: 12225.502930\n",
      "Train Epoch: 1591 [33024/118836 (28%)] Loss: 12411.242188\n",
      "Train Epoch: 1591 [65792/118836 (55%)] Loss: 12304.439453\n",
      "Train Epoch: 1591 [98560/118836 (83%)] Loss: 12222.375977\n",
      "    epoch          : 1591\n",
      "    loss           : 12265.17763453784\n",
      "    val_loss       : 12263.401146462416\n",
      "    val_log_likelihood: -12164.505256474875\n",
      "    val_log_marginal: -12172.541094222408\n",
      "Train Epoch: 1592 [256/118836 (0%)] Loss: 12205.310547\n",
      "Train Epoch: 1592 [33024/118836 (28%)] Loss: 12332.541016\n",
      "Train Epoch: 1592 [65792/118836 (55%)] Loss: 12272.100586\n",
      "Train Epoch: 1592 [98560/118836 (83%)] Loss: 12222.590820\n",
      "    epoch          : 1592\n",
      "    loss           : 12264.695159674835\n",
      "    val_loss       : 12258.432446435314\n",
      "    val_log_likelihood: -12166.398467871175\n",
      "    val_log_marginal: -12174.329826228071\n",
      "Train Epoch: 1593 [256/118836 (0%)] Loss: 12237.323242\n",
      "Train Epoch: 1593 [33024/118836 (28%)] Loss: 12280.116211\n",
      "Train Epoch: 1593 [65792/118836 (55%)] Loss: 12371.056641\n",
      "Train Epoch: 1593 [98560/118836 (83%)] Loss: 12266.590820\n",
      "    epoch          : 1593\n",
      "    loss           : 12261.35890230821\n",
      "    val_loss       : 12257.249636004108\n",
      "    val_log_likelihood: -12162.962163978495\n",
      "    val_log_marginal: -12170.979949848943\n",
      "Train Epoch: 1594 [256/118836 (0%)] Loss: 12332.088867\n",
      "Train Epoch: 1594 [33024/118836 (28%)] Loss: 12281.814453\n",
      "Train Epoch: 1594 [65792/118836 (55%)] Loss: 12322.273438\n",
      "Train Epoch: 1594 [98560/118836 (83%)] Loss: 12292.567383\n",
      "    epoch          : 1594\n",
      "    loss           : 12265.79102644877\n",
      "    val_loss       : 12264.896128379367\n",
      "    val_log_likelihood: -12165.900915012406\n",
      "    val_log_marginal: -12173.864674538867\n",
      "Train Epoch: 1595 [256/118836 (0%)] Loss: 12292.058594\n",
      "Train Epoch: 1595 [33024/118836 (28%)] Loss: 12341.256836\n",
      "Train Epoch: 1595 [65792/118836 (55%)] Loss: 12268.962891\n",
      "Train Epoch: 1595 [98560/118836 (83%)] Loss: 12263.633789\n",
      "    epoch          : 1595\n",
      "    loss           : 12262.425743770678\n",
      "    val_loss       : 12256.51218260554\n",
      "    val_log_likelihood: -12164.504069414288\n",
      "    val_log_marginal: -12172.49447631721\n",
      "Train Epoch: 1596 [256/118836 (0%)] Loss: 12310.060547\n",
      "Train Epoch: 1596 [33024/118836 (28%)] Loss: 12189.167969\n",
      "Train Epoch: 1596 [65792/118836 (55%)] Loss: 12232.233398\n",
      "Train Epoch: 1596 [98560/118836 (83%)] Loss: 12314.148438\n",
      "    epoch          : 1596\n",
      "    loss           : 12262.41060874819\n",
      "    val_loss       : 12262.401981203719\n",
      "    val_log_likelihood: -12166.590383484543\n",
      "    val_log_marginal: -12174.384568497402\n",
      "Train Epoch: 1597 [256/118836 (0%)] Loss: 12268.056641\n",
      "Train Epoch: 1597 [33024/118836 (28%)] Loss: 12291.186523\n",
      "Train Epoch: 1597 [65792/118836 (55%)] Loss: 12260.717773\n",
      "Train Epoch: 1597 [98560/118836 (83%)] Loss: 12269.742188\n",
      "    epoch          : 1597\n",
      "    loss           : 12264.788543928351\n",
      "    val_loss       : 12263.788618589522\n",
      "    val_log_likelihood: -12165.789239880583\n",
      "    val_log_marginal: -12173.74063366095\n",
      "Train Epoch: 1598 [256/118836 (0%)] Loss: 12223.787109\n",
      "Train Epoch: 1598 [33024/118836 (28%)] Loss: 12259.594727\n",
      "Train Epoch: 1598 [65792/118836 (55%)] Loss: 12324.761719\n",
      "Train Epoch: 1598 [98560/118836 (83%)] Loss: 12282.240234\n",
      "    epoch          : 1598\n",
      "    loss           : 12258.035947354476\n",
      "    val_loss       : 12262.821322304422\n",
      "    val_log_likelihood: -12163.088007101685\n",
      "    val_log_marginal: -12171.087407986231\n",
      "Train Epoch: 1599 [256/118836 (0%)] Loss: 12349.096680\n",
      "Train Epoch: 1599 [33024/118836 (28%)] Loss: 12316.467773\n",
      "Train Epoch: 1599 [65792/118836 (55%)] Loss: 12308.244141\n",
      "Train Epoch: 1599 [98560/118836 (83%)] Loss: 12369.677734\n",
      "    epoch          : 1599\n",
      "    loss           : 12264.8634427988\n",
      "    val_loss       : 12260.295722680637\n",
      "    val_log_likelihood: -12163.59303756979\n",
      "    val_log_marginal: -12171.561809125955\n",
      "Train Epoch: 1600 [256/118836 (0%)] Loss: 12217.980469\n",
      "Train Epoch: 1600 [33024/118836 (28%)] Loss: 12288.740234\n",
      "Train Epoch: 1600 [65792/118836 (55%)] Loss: 12322.527344\n",
      "Train Epoch: 1600 [98560/118836 (83%)] Loss: 12365.996094\n",
      "    epoch          : 1600\n",
      "    loss           : 12263.186095817824\n",
      "    val_loss       : 12267.96202667903\n",
      "    val_log_likelihood: -12163.088360570462\n",
      "    val_log_marginal: -12171.223612100262\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch1600.pth ...\n",
      "Train Epoch: 1601 [256/118836 (0%)] Loss: 12265.324219\n",
      "Train Epoch: 1601 [33024/118836 (28%)] Loss: 12206.661133\n",
      "Train Epoch: 1601 [65792/118836 (55%)] Loss: 12198.098633\n",
      "Train Epoch: 1601 [98560/118836 (83%)] Loss: 12290.335938\n",
      "    epoch          : 1601\n",
      "    loss           : 12263.530198478857\n",
      "    val_loss       : 12260.536046277075\n",
      "    val_log_likelihood: -12162.499553317566\n",
      "    val_log_marginal: -12170.546785590397\n",
      "Train Epoch: 1602 [256/118836 (0%)] Loss: 12260.909180\n",
      "Train Epoch: 1602 [33024/118836 (28%)] Loss: 12238.625000\n",
      "Train Epoch: 1602 [65792/118836 (55%)] Loss: 12217.840820\n",
      "Train Epoch: 1602 [98560/118836 (83%)] Loss: 12186.957031\n",
      "    epoch          : 1602\n",
      "    loss           : 12266.957235609234\n",
      "    val_loss       : 12260.726580287437\n",
      "    val_log_likelihood: -12164.179603333074\n",
      "    val_log_marginal: -12172.119558821565\n",
      "Train Epoch: 1603 [256/118836 (0%)] Loss: 12295.074219\n",
      "Train Epoch: 1603 [33024/118836 (28%)] Loss: 12405.781250\n",
      "Train Epoch: 1603 [65792/118836 (55%)] Loss: 12271.164062\n",
      "Train Epoch: 1603 [98560/118836 (83%)] Loss: 12266.451172\n",
      "    epoch          : 1603\n",
      "    loss           : 12260.965997208437\n",
      "    val_loss       : 12262.560948647597\n",
      "    val_log_likelihood: -12164.110118286031\n",
      "    val_log_marginal: -12172.135295055696\n",
      "Train Epoch: 1604 [256/118836 (0%)] Loss: 12362.971680\n",
      "Train Epoch: 1604 [33024/118836 (28%)] Loss: 12387.672852\n",
      "Train Epoch: 1604 [65792/118836 (55%)] Loss: 12242.596680\n",
      "Train Epoch: 1604 [98560/118836 (83%)] Loss: 12253.924805\n",
      "    epoch          : 1604\n",
      "    loss           : 12261.022453344705\n",
      "    val_loss       : 12259.802278230347\n",
      "    val_log_likelihood: -12164.538513072528\n",
      "    val_log_marginal: -12172.741995233482\n",
      "Train Epoch: 1605 [256/118836 (0%)] Loss: 12308.096680\n",
      "Train Epoch: 1605 [33024/118836 (28%)] Loss: 12404.103516\n",
      "Train Epoch: 1605 [65792/118836 (55%)] Loss: 12320.016602\n",
      "Train Epoch: 1605 [98560/118836 (83%)] Loss: 12335.916992\n",
      "    epoch          : 1605\n",
      "    loss           : 12260.624794509924\n",
      "    val_loss       : 12258.884961367708\n",
      "    val_log_likelihood: -12167.362617769077\n",
      "    val_log_marginal: -12175.333358604426\n",
      "Train Epoch: 1606 [256/118836 (0%)] Loss: 12314.603516\n",
      "Train Epoch: 1606 [33024/118836 (28%)] Loss: 12331.699219\n",
      "Train Epoch: 1606 [65792/118836 (55%)] Loss: 12259.733398\n",
      "Train Epoch: 1606 [98560/118836 (83%)] Loss: 12196.243164\n",
      "    epoch          : 1606\n",
      "    loss           : 12268.83402460065\n",
      "    val_loss       : 12261.382405612096\n",
      "    val_log_likelihood: -12163.722092606235\n",
      "    val_log_marginal: -12171.53722589911\n",
      "Train Epoch: 1607 [256/118836 (0%)] Loss: 12304.606445\n",
      "Train Epoch: 1607 [33024/118836 (28%)] Loss: 12240.314453\n",
      "Train Epoch: 1607 [65792/118836 (55%)] Loss: 12190.144531\n",
      "Train Epoch: 1607 [98560/118836 (83%)] Loss: 12400.541016\n",
      "    epoch          : 1607\n",
      "    loss           : 12258.966819007186\n",
      "    val_loss       : 12260.064614919567\n",
      "    val_log_likelihood: -12164.234341720947\n",
      "    val_log_marginal: -12172.24799309617\n",
      "Train Epoch: 1608 [256/118836 (0%)] Loss: 12317.169922\n",
      "Train Epoch: 1608 [33024/118836 (28%)] Loss: 12197.450195\n",
      "Train Epoch: 1608 [65792/118836 (55%)] Loss: 12275.132812\n",
      "Train Epoch: 1608 [98560/118836 (83%)] Loss: 12302.402344\n",
      "    epoch          : 1608\n",
      "    loss           : 12262.013619856287\n",
      "    val_loss       : 12262.845898238058\n",
      "    val_log_likelihood: -12162.528535010855\n",
      "    val_log_marginal: -12170.72670028778\n",
      "Train Epoch: 1609 [256/118836 (0%)] Loss: 12287.769531\n",
      "Train Epoch: 1609 [33024/118836 (28%)] Loss: 12358.025391\n",
      "Train Epoch: 1609 [65792/118836 (55%)] Loss: 12205.267578\n",
      "Train Epoch: 1609 [98560/118836 (83%)] Loss: 12247.798828\n",
      "    epoch          : 1609\n",
      "    loss           : 12265.379281204767\n",
      "    val_loss       : 12260.026324936067\n",
      "    val_log_likelihood: -12164.164536161084\n",
      "    val_log_marginal: -12172.549898778805\n",
      "Train Epoch: 1610 [256/118836 (0%)] Loss: 12257.326172\n",
      "Train Epoch: 1610 [33024/118836 (28%)] Loss: 12297.894531\n",
      "Train Epoch: 1610 [65792/118836 (55%)] Loss: 12423.164062\n",
      "Train Epoch: 1610 [98560/118836 (83%)] Loss: 12379.429688\n",
      "    epoch          : 1610\n",
      "    loss           : 12260.655134990178\n",
      "    val_loss       : 12268.293627144621\n",
      "    val_log_likelihood: -12167.045578247777\n",
      "    val_log_marginal: -12175.21792114987\n",
      "Train Epoch: 1611 [256/118836 (0%)] Loss: 12338.837891\n",
      "Train Epoch: 1611 [33024/118836 (28%)] Loss: 12243.771484\n",
      "Train Epoch: 1611 [65792/118836 (55%)] Loss: 12352.945312\n",
      "Train Epoch: 1611 [98560/118836 (83%)] Loss: 12381.096680\n",
      "    epoch          : 1611\n",
      "    loss           : 12264.743428517886\n",
      "    val_loss       : 12265.945147619763\n",
      "    val_log_likelihood: -12165.444336422146\n",
      "    val_log_marginal: -12173.661212855926\n",
      "Train Epoch: 1612 [256/118836 (0%)] Loss: 12258.972656\n",
      "Train Epoch: 1612 [33024/118836 (28%)] Loss: 12357.310547\n",
      "Train Epoch: 1612 [65792/118836 (55%)] Loss: 12353.836914\n",
      "Train Epoch: 1612 [98560/118836 (83%)] Loss: 12277.719727\n",
      "    epoch          : 1612\n",
      "    loss           : 12266.229663590779\n",
      "    val_loss       : 12262.65959703014\n",
      "    val_log_likelihood: -12165.074050416151\n",
      "    val_log_marginal: -12173.212773741396\n",
      "Train Epoch: 1613 [256/118836 (0%)] Loss: 12266.143555\n",
      "Train Epoch: 1613 [33024/118836 (28%)] Loss: 12273.938477\n",
      "Train Epoch: 1613 [65792/118836 (55%)] Loss: 12274.701172\n",
      "Train Epoch: 1613 [98560/118836 (83%)] Loss: 12345.972656\n",
      "    epoch          : 1613\n",
      "    loss           : 12261.893379730149\n",
      "    val_loss       : 12263.868492263768\n",
      "    val_log_likelihood: -12163.348053983147\n",
      "    val_log_marginal: -12171.250129150432\n",
      "Train Epoch: 1614 [256/118836 (0%)] Loss: 12187.542969\n",
      "Train Epoch: 1614 [33024/118836 (28%)] Loss: 12178.353516\n",
      "Train Epoch: 1614 [65792/118836 (55%)] Loss: 12372.329102\n",
      "Train Epoch: 1614 [98560/118836 (83%)] Loss: 12419.878906\n",
      "    epoch          : 1614\n",
      "    loss           : 12267.459262723583\n",
      "    val_loss       : 12264.303838496362\n",
      "    val_log_likelihood: -12165.214006765664\n",
      "    val_log_marginal: -12173.170410641684\n",
      "Train Epoch: 1615 [256/118836 (0%)] Loss: 12307.237305\n",
      "Train Epoch: 1615 [33024/118836 (28%)] Loss: 12383.074219\n",
      "Train Epoch: 1615 [65792/118836 (55%)] Loss: 12409.781250\n",
      "Train Epoch: 1615 [98560/118836 (83%)] Loss: 12289.623047\n",
      "    epoch          : 1615\n",
      "    loss           : 12264.304372156741\n",
      "    val_loss       : 12264.841127238811\n",
      "    val_log_likelihood: -12162.681749024247\n",
      "    val_log_marginal: -12170.651149370964\n",
      "Train Epoch: 1616 [256/118836 (0%)] Loss: 12201.233398\n",
      "Train Epoch: 1616 [33024/118836 (28%)] Loss: 12288.554688\n",
      "Train Epoch: 1616 [65792/118836 (55%)] Loss: 12289.963867\n",
      "Train Epoch: 1616 [98560/118836 (83%)] Loss: 12295.987305\n",
      "    epoch          : 1616\n",
      "    loss           : 12263.554245340933\n",
      "    val_loss       : 12259.65022897282\n",
      "    val_log_likelihood: -12165.945799084988\n",
      "    val_log_marginal: -12174.148659119914\n",
      "Train Epoch: 1617 [256/118836 (0%)] Loss: 12195.087891\n",
      "Train Epoch: 1617 [33024/118836 (28%)] Loss: 12341.917969\n",
      "Train Epoch: 1617 [65792/118836 (55%)] Loss: 12273.021484\n",
      "Train Epoch: 1617 [98560/118836 (83%)] Loss: 12330.810547\n",
      "    epoch          : 1617\n",
      "    loss           : 12263.804436453163\n",
      "    val_loss       : 12262.562663119324\n",
      "    val_log_likelihood: -12165.894833992452\n",
      "    val_log_marginal: -12174.166053962403\n",
      "Train Epoch: 1618 [256/118836 (0%)] Loss: 12310.539062\n",
      "Train Epoch: 1618 [33024/118836 (28%)] Loss: 12163.271484\n",
      "Train Epoch: 1618 [65792/118836 (55%)] Loss: 12266.062500\n",
      "Train Epoch: 1618 [98560/118836 (83%)] Loss: 12322.421875\n",
      "    epoch          : 1618\n",
      "    loss           : 12262.949645238834\n",
      "    val_loss       : 12272.893571369548\n",
      "    val_log_likelihood: -12163.49802248113\n",
      "    val_log_marginal: -12171.541492400167\n",
      "Train Epoch: 1619 [256/118836 (0%)] Loss: 12283.374023\n",
      "Train Epoch: 1619 [33024/118836 (28%)] Loss: 12225.939453\n",
      "Train Epoch: 1619 [65792/118836 (55%)] Loss: 12337.046875\n",
      "Train Epoch: 1619 [98560/118836 (83%)] Loss: 12436.240234\n",
      "    epoch          : 1619\n",
      "    loss           : 12268.391155526262\n",
      "    val_loss       : 12262.724240948091\n",
      "    val_log_likelihood: -12164.482285043166\n",
      "    val_log_marginal: -12172.552239940576\n",
      "Train Epoch: 1620 [256/118836 (0%)] Loss: 12231.552734\n",
      "Train Epoch: 1620 [33024/118836 (28%)] Loss: 12245.590820\n",
      "Train Epoch: 1620 [65792/118836 (55%)] Loss: 12316.079102\n",
      "Train Epoch: 1620 [98560/118836 (83%)] Loss: 12404.760742\n",
      "    epoch          : 1620\n",
      "    loss           : 12263.502294154518\n",
      "    val_loss       : 12282.765295630701\n",
      "    val_log_likelihood: -12164.295321062087\n",
      "    val_log_marginal: -12172.475050777808\n",
      "Train Epoch: 1621 [256/118836 (0%)] Loss: 12184.006836\n",
      "Train Epoch: 1621 [33024/118836 (28%)] Loss: 12294.337891\n",
      "Train Epoch: 1621 [65792/118836 (55%)] Loss: 12273.072266\n",
      "Train Epoch: 1621 [98560/118836 (83%)] Loss: 12312.892578\n",
      "    epoch          : 1621\n",
      "    loss           : 12263.485735079354\n",
      "    val_loss       : 12265.036333191445\n",
      "    val_log_likelihood: -12165.053970223325\n",
      "    val_log_marginal: -12173.238956306053\n",
      "Train Epoch: 1622 [256/118836 (0%)] Loss: 12251.086914\n",
      "Train Epoch: 1622 [33024/118836 (28%)] Loss: 12396.666992\n",
      "Train Epoch: 1622 [65792/118836 (55%)] Loss: 12271.515625\n",
      "Train Epoch: 1622 [98560/118836 (83%)] Loss: 12222.276367\n",
      "    epoch          : 1622\n",
      "    loss           : 12263.795771298594\n",
      "    val_loss       : 12264.137593826414\n",
      "    val_log_likelihood: -12164.424252998346\n",
      "    val_log_marginal: -12172.568802081589\n",
      "Train Epoch: 1623 [256/118836 (0%)] Loss: 12304.875000\n",
      "Train Epoch: 1623 [33024/118836 (28%)] Loss: 12268.454102\n",
      "Train Epoch: 1623 [65792/118836 (55%)] Loss: 12300.117188\n",
      "Train Epoch: 1623 [98560/118836 (83%)] Loss: 12169.938477\n",
      "    epoch          : 1623\n",
      "    loss           : 12263.444585530398\n",
      "    val_loss       : 12258.384824054176\n",
      "    val_log_likelihood: -12163.529873604219\n",
      "    val_log_marginal: -12171.467718753089\n",
      "Train Epoch: 1624 [256/118836 (0%)] Loss: 12301.253906\n",
      "Train Epoch: 1624 [33024/118836 (28%)] Loss: 12215.302734\n",
      "Train Epoch: 1624 [65792/118836 (55%)] Loss: 12460.651367\n",
      "Train Epoch: 1624 [98560/118836 (83%)] Loss: 12238.255859\n",
      "    epoch          : 1624\n",
      "    loss           : 12265.185387587882\n",
      "    val_loss       : 12265.528511640126\n",
      "    val_log_likelihood: -12164.72324202595\n",
      "    val_log_marginal: -12172.810854075873\n",
      "Train Epoch: 1625 [256/118836 (0%)] Loss: 12364.240234\n",
      "Train Epoch: 1625 [33024/118836 (28%)] Loss: 12284.988281\n",
      "Train Epoch: 1625 [65792/118836 (55%)] Loss: 12318.272461\n",
      "Train Epoch: 1625 [98560/118836 (83%)] Loss: 12440.196289\n",
      "    epoch          : 1625\n",
      "    loss           : 12268.006111229579\n",
      "    val_loss       : 12265.596773202067\n",
      "    val_log_likelihood: -12164.19618647901\n",
      "    val_log_marginal: -12172.260233931305\n",
      "Train Epoch: 1626 [256/118836 (0%)] Loss: 12314.683594\n",
      "Train Epoch: 1626 [33024/118836 (28%)] Loss: 12283.685547\n",
      "Train Epoch: 1626 [65792/118836 (55%)] Loss: 12256.739258\n",
      "Train Epoch: 1626 [98560/118836 (83%)] Loss: 12320.083984\n",
      "    epoch          : 1626\n",
      "    loss           : 12262.504803815136\n",
      "    val_loss       : 12262.322002692821\n",
      "    val_log_likelihood: -12166.922669335452\n",
      "    val_log_marginal: -12175.068875062738\n",
      "Train Epoch: 1627 [256/118836 (0%)] Loss: 12231.724609\n",
      "Train Epoch: 1627 [33024/118836 (28%)] Loss: 12252.082031\n",
      "Train Epoch: 1627 [65792/118836 (55%)] Loss: 12215.106445\n",
      "Train Epoch: 1627 [98560/118836 (83%)] Loss: 12241.091797\n",
      "    epoch          : 1627\n",
      "    loss           : 12263.26617975858\n",
      "    val_loss       : 12262.380638450997\n",
      "    val_log_likelihood: -12164.45773447193\n",
      "    val_log_marginal: -12172.45586191744\n",
      "Train Epoch: 1628 [256/118836 (0%)] Loss: 12432.098633\n",
      "Train Epoch: 1628 [33024/118836 (28%)] Loss: 12331.648438\n",
      "Train Epoch: 1628 [65792/118836 (55%)] Loss: 12248.833984\n",
      "Train Epoch: 1628 [98560/118836 (83%)] Loss: 12151.275391\n",
      "    epoch          : 1628\n",
      "    loss           : 12269.997446075009\n",
      "    val_loss       : 12260.21945100971\n",
      "    val_log_likelihood: -12165.72021686311\n",
      "    val_log_marginal: -12173.650533027265\n",
      "Train Epoch: 1629 [256/118836 (0%)] Loss: 12251.545898\n",
      "Train Epoch: 1629 [33024/118836 (28%)] Loss: 12296.683594\n",
      "Train Epoch: 1629 [65792/118836 (55%)] Loss: 12215.017578\n",
      "Train Epoch: 1629 [98560/118836 (83%)] Loss: 12217.783203\n",
      "    epoch          : 1629\n",
      "    loss           : 12261.41711609543\n",
      "    val_loss       : 12264.441434384238\n",
      "    val_log_likelihood: -12165.546986791769\n",
      "    val_log_marginal: -12173.52636409694\n",
      "Train Epoch: 1630 [256/118836 (0%)] Loss: 12215.591797\n",
      "Train Epoch: 1630 [33024/118836 (28%)] Loss: 12254.044922\n",
      "Train Epoch: 1630 [65792/118836 (55%)] Loss: 12303.960938\n",
      "Train Epoch: 1630 [98560/118836 (83%)] Loss: 12221.227539\n",
      "    epoch          : 1630\n",
      "    loss           : 12266.827309017008\n",
      "    val_loss       : 12268.27690623071\n",
      "    val_log_likelihood: -12162.803363122932\n",
      "    val_log_marginal: -12170.823840778956\n",
      "Train Epoch: 1631 [256/118836 (0%)] Loss: 12352.779297\n",
      "Train Epoch: 1631 [33024/118836 (28%)] Loss: 12351.811523\n",
      "Train Epoch: 1631 [65792/118836 (55%)] Loss: 12311.382812\n",
      "Train Epoch: 1631 [98560/118836 (83%)] Loss: 12264.046875\n",
      "    epoch          : 1631\n",
      "    loss           : 12264.860175958953\n",
      "    val_loss       : 12261.095846835417\n",
      "    val_log_likelihood: -12166.57932239971\n",
      "    val_log_marginal: -12174.680715417971\n",
      "Train Epoch: 1632 [256/118836 (0%)] Loss: 12256.033203\n",
      "Train Epoch: 1632 [33024/118836 (28%)] Loss: 12276.771484\n",
      "Train Epoch: 1632 [65792/118836 (55%)] Loss: 12282.485352\n",
      "Train Epoch: 1632 [98560/118836 (83%)] Loss: 12271.099609\n",
      "    epoch          : 1632\n",
      "    loss           : 12262.08043110913\n",
      "    val_loss       : 12269.145740429096\n",
      "    val_log_likelihood: -12168.444560490332\n",
      "    val_log_marginal: -12176.668879673398\n",
      "Train Epoch: 1633 [256/118836 (0%)] Loss: 12327.135742\n",
      "Train Epoch: 1633 [33024/118836 (28%)] Loss: 12280.628906\n",
      "Train Epoch: 1633 [65792/118836 (55%)] Loss: 12376.224609\n",
      "Train Epoch: 1633 [98560/118836 (83%)] Loss: 12264.417969\n",
      "    epoch          : 1633\n",
      "    loss           : 12263.400734885494\n",
      "    val_loss       : 12257.375951246322\n",
      "    val_log_likelihood: -12164.551028258116\n",
      "    val_log_marginal: -12172.595476755043\n",
      "Train Epoch: 1634 [256/118836 (0%)] Loss: 12264.888672\n",
      "Train Epoch: 1634 [33024/118836 (28%)] Loss: 12276.758789\n",
      "Train Epoch: 1634 [65792/118836 (55%)] Loss: 12238.147461\n",
      "Train Epoch: 1634 [98560/118836 (83%)] Loss: 12262.744141\n",
      "    epoch          : 1634\n",
      "    loss           : 12264.948806315913\n",
      "    val_loss       : 12265.75394375604\n",
      "    val_log_likelihood: -12164.25762655733\n",
      "    val_log_marginal: -12172.196664634119\n",
      "Train Epoch: 1635 [256/118836 (0%)] Loss: 12245.043945\n",
      "Train Epoch: 1635 [33024/118836 (28%)] Loss: 12237.446289\n",
      "Train Epoch: 1635 [65792/118836 (55%)] Loss: 12198.531250\n",
      "Train Epoch: 1635 [98560/118836 (83%)] Loss: 12210.749023\n",
      "    epoch          : 1635\n",
      "    loss           : 12261.56951638751\n",
      "    val_loss       : 12262.607080901562\n",
      "    val_log_likelihood: -12164.217404944686\n",
      "    val_log_marginal: -12172.195660908694\n",
      "Train Epoch: 1636 [256/118836 (0%)] Loss: 12184.086914\n",
      "Train Epoch: 1636 [33024/118836 (28%)] Loss: 12349.791992\n",
      "Train Epoch: 1636 [65792/118836 (55%)] Loss: 12285.567383\n",
      "Train Epoch: 1636 [98560/118836 (83%)] Loss: 12294.146484\n",
      "    epoch          : 1636\n",
      "    loss           : 12267.100815498346\n",
      "    val_loss       : 12264.870452538582\n",
      "    val_log_likelihood: -12167.177299485627\n",
      "    val_log_marginal: -12175.231293047238\n",
      "Train Epoch: 1637 [256/118836 (0%)] Loss: 12361.999023\n",
      "Train Epoch: 1637 [33024/118836 (28%)] Loss: 12199.598633\n",
      "Train Epoch: 1637 [65792/118836 (55%)] Loss: 12288.064453\n",
      "Train Epoch: 1637 [98560/118836 (83%)] Loss: 12340.405273\n",
      "    epoch          : 1637\n",
      "    loss           : 12263.77345898599\n",
      "    val_loss       : 12263.94647610984\n",
      "    val_log_likelihood: -12164.594681975032\n",
      "    val_log_marginal: -12172.572959354928\n",
      "Train Epoch: 1638 [256/118836 (0%)] Loss: 12235.395508\n",
      "Train Epoch: 1638 [33024/118836 (28%)] Loss: 12267.980469\n",
      "Train Epoch: 1638 [65792/118836 (55%)] Loss: 12232.178711\n",
      "Train Epoch: 1638 [98560/118836 (83%)] Loss: 12193.217773\n",
      "    epoch          : 1638\n",
      "    loss           : 12261.232433829611\n",
      "    val_loss       : 12262.776524627672\n",
      "    val_log_likelihood: -12167.628990901572\n",
      "    val_log_marginal: -12175.63389740327\n",
      "Train Epoch: 1639 [256/118836 (0%)] Loss: 12401.071289\n",
      "Train Epoch: 1639 [33024/118836 (28%)] Loss: 12228.107422\n",
      "Train Epoch: 1639 [65792/118836 (55%)] Loss: 12241.869141\n",
      "Train Epoch: 1639 [98560/118836 (83%)] Loss: 12446.524414\n",
      "    epoch          : 1639\n",
      "    loss           : 12262.109992924163\n",
      "    val_loss       : 12263.412270627095\n",
      "    val_log_likelihood: -12165.962056063896\n",
      "    val_log_marginal: -12174.004253188434\n",
      "Train Epoch: 1640 [256/118836 (0%)] Loss: 12322.374023\n",
      "Train Epoch: 1640 [33024/118836 (28%)] Loss: 12331.464844\n",
      "Train Epoch: 1640 [65792/118836 (55%)] Loss: 12243.851562\n",
      "Train Epoch: 1640 [98560/118836 (83%)] Loss: 12290.619141\n",
      "    epoch          : 1640\n",
      "    loss           : 12265.009485822477\n",
      "    val_loss       : 12261.7198454028\n",
      "    val_log_likelihood: -12166.879377164754\n",
      "    val_log_marginal: -12174.85816899996\n",
      "Train Epoch: 1641 [256/118836 (0%)] Loss: 12366.551758\n",
      "Train Epoch: 1641 [33024/118836 (28%)] Loss: 12267.838867\n",
      "Train Epoch: 1641 [65792/118836 (55%)] Loss: 12290.160156\n",
      "Train Epoch: 1641 [98560/118836 (83%)] Loss: 12354.178711\n",
      "    epoch          : 1641\n",
      "    loss           : 12261.018397177419\n",
      "    val_loss       : 12263.181294252956\n",
      "    val_log_likelihood: -12165.953228714328\n",
      "    val_log_marginal: -12174.151892269234\n",
      "Train Epoch: 1642 [256/118836 (0%)] Loss: 12399.375000\n",
      "Train Epoch: 1642 [33024/118836 (28%)] Loss: 12209.302734\n",
      "Train Epoch: 1642 [65792/118836 (55%)] Loss: 12306.912109\n",
      "Train Epoch: 1642 [98560/118836 (83%)] Loss: 12163.867188\n",
      "    epoch          : 1642\n",
      "    loss           : 12264.641722562552\n",
      "    val_loss       : 12264.065626495212\n",
      "    val_log_likelihood: -12167.019367439516\n",
      "    val_log_marginal: -12174.994283253129\n",
      "Train Epoch: 1643 [256/118836 (0%)] Loss: 12213.653320\n",
      "Train Epoch: 1643 [33024/118836 (28%)] Loss: 12402.369141\n",
      "Train Epoch: 1643 [65792/118836 (55%)] Loss: 12314.099609\n",
      "Train Epoch: 1643 [98560/118836 (83%)] Loss: 12387.460938\n",
      "    epoch          : 1643\n",
      "    loss           : 12264.206324635546\n",
      "    val_loss       : 12266.646675179156\n",
      "    val_log_likelihood: -12167.515885901314\n",
      "    val_log_marginal: -12175.614313783239\n",
      "Train Epoch: 1644 [256/118836 (0%)] Loss: 12384.458984\n",
      "Train Epoch: 1644 [33024/118836 (28%)] Loss: 12269.610352\n",
      "Train Epoch: 1644 [65792/118836 (55%)] Loss: 12234.449219\n",
      "Train Epoch: 1644 [98560/118836 (83%)] Loss: 12263.248047\n",
      "    epoch          : 1644\n",
      "    loss           : 12257.312367045337\n",
      "    val_loss       : 12262.080938352392\n",
      "    val_log_likelihood: -12164.532069537066\n",
      "    val_log_marginal: -12172.416764068168\n",
      "Train Epoch: 1645 [256/118836 (0%)] Loss: 12281.960938\n",
      "Train Epoch: 1645 [33024/118836 (28%)] Loss: 12204.227539\n",
      "Train Epoch: 1645 [65792/118836 (55%)] Loss: 12263.774414\n",
      "Train Epoch: 1645 [98560/118836 (83%)] Loss: 12144.718750\n",
      "    epoch          : 1645\n",
      "    loss           : 12260.800920666614\n",
      "    val_loss       : 12265.2473495817\n",
      "    val_log_likelihood: -12163.863859271609\n",
      "    val_log_marginal: -12171.928487211742\n",
      "Train Epoch: 1646 [256/118836 (0%)] Loss: 12204.028320\n",
      "Train Epoch: 1646 [33024/118836 (28%)] Loss: 12199.651367\n",
      "Train Epoch: 1646 [65792/118836 (55%)] Loss: 12375.189453\n",
      "Train Epoch: 1646 [98560/118836 (83%)] Loss: 12250.490234\n",
      "    epoch          : 1646\n",
      "    loss           : 12263.25877420001\n",
      "    val_loss       : 12261.502920691595\n",
      "    val_log_likelihood: -12165.217050829713\n",
      "    val_log_marginal: -12173.243669490075\n",
      "Train Epoch: 1647 [256/118836 (0%)] Loss: 12226.264648\n",
      "Train Epoch: 1647 [33024/118836 (28%)] Loss: 12326.359375\n",
      "Train Epoch: 1647 [65792/118836 (55%)] Loss: 12239.731445\n",
      "Train Epoch: 1647 [98560/118836 (83%)] Loss: 12242.441406\n",
      "    epoch          : 1647\n",
      "    loss           : 12269.728049233612\n",
      "    val_loss       : 12264.006833359565\n",
      "    val_log_likelihood: -12164.572228307226\n",
      "    val_log_marginal: -12172.44137974119\n",
      "Train Epoch: 1648 [256/118836 (0%)] Loss: 12221.238281\n",
      "Train Epoch: 1648 [33024/118836 (28%)] Loss: 12307.950195\n",
      "Train Epoch: 1648 [65792/118836 (55%)] Loss: 12234.831055\n",
      "Train Epoch: 1648 [98560/118836 (83%)] Loss: 12297.666992\n",
      "    epoch          : 1648\n",
      "    loss           : 12262.488695299575\n",
      "    val_loss       : 12264.555425800823\n",
      "    val_log_likelihood: -12164.326638589486\n",
      "    val_log_marginal: -12172.350206476689\n",
      "Train Epoch: 1649 [256/118836 (0%)] Loss: 12173.369141\n",
      "Train Epoch: 1649 [33024/118836 (28%)] Loss: 12218.008789\n",
      "Train Epoch: 1649 [65792/118836 (55%)] Loss: 12299.964844\n",
      "Train Epoch: 1649 [98560/118836 (83%)] Loss: 12202.421875\n",
      "    epoch          : 1649\n",
      "    loss           : 12264.63522086952\n",
      "    val_loss       : 12264.510063125354\n",
      "    val_log_likelihood: -12165.72850076251\n",
      "    val_log_marginal: -12173.838385419976\n",
      "Train Epoch: 1650 [256/118836 (0%)] Loss: 12195.259766\n",
      "Train Epoch: 1650 [33024/118836 (28%)] Loss: 12235.898438\n",
      "Train Epoch: 1650 [65792/118836 (55%)] Loss: 12285.683594\n",
      "Train Epoch: 1650 [98560/118836 (83%)] Loss: 12520.022461\n",
      "    epoch          : 1650\n",
      "    loss           : 12264.259762394024\n",
      "    val_loss       : 12263.89591399467\n",
      "    val_log_likelihood: -12164.506030455179\n",
      "    val_log_marginal: -12172.47030170398\n",
      "Train Epoch: 1651 [256/118836 (0%)] Loss: 12262.675781\n",
      "Train Epoch: 1651 [33024/118836 (28%)] Loss: 12311.936523\n",
      "Train Epoch: 1651 [65792/118836 (55%)] Loss: 12231.824219\n",
      "Train Epoch: 1651 [98560/118836 (83%)] Loss: 12188.955078\n",
      "    epoch          : 1651\n",
      "    loss           : 12261.389556839329\n",
      "    val_loss       : 12259.214690498187\n",
      "    val_log_likelihood: -12164.536997744779\n",
      "    val_log_marginal: -12172.650698516076\n",
      "Train Epoch: 1652 [256/118836 (0%)] Loss: 12175.279297\n",
      "Train Epoch: 1652 [33024/118836 (28%)] Loss: 12220.753906\n",
      "Train Epoch: 1652 [65792/118836 (55%)] Loss: 12208.225586\n",
      "Train Epoch: 1652 [98560/118836 (83%)] Loss: 12211.453125\n",
      "    epoch          : 1652\n",
      "    loss           : 12260.238596916357\n",
      "    val_loss       : 12263.952962362244\n",
      "    val_log_likelihood: -12165.744967270213\n",
      "    val_log_marginal: -12173.877691974329\n",
      "Train Epoch: 1653 [256/118836 (0%)] Loss: 12298.393555\n",
      "Train Epoch: 1653 [33024/118836 (28%)] Loss: 12359.365234\n",
      "Train Epoch: 1653 [65792/118836 (55%)] Loss: 12242.843750\n",
      "Train Epoch: 1653 [98560/118836 (83%)] Loss: 12239.124023\n",
      "    epoch          : 1653\n",
      "    loss           : 12263.622556897488\n",
      "    val_loss       : 12264.825847116872\n",
      "    val_log_likelihood: -12165.785670459833\n",
      "    val_log_marginal: -12174.027806389782\n",
      "Train Epoch: 1654 [256/118836 (0%)] Loss: 12338.698242\n",
      "Train Epoch: 1654 [33024/118836 (28%)] Loss: 12175.153320\n",
      "Train Epoch: 1654 [65792/118836 (55%)] Loss: 12282.227539\n",
      "Train Epoch: 1654 [98560/118836 (83%)] Loss: 12366.236328\n",
      "    epoch          : 1654\n",
      "    loss           : 12261.811364635028\n",
      "    val_loss       : 12264.002250785807\n",
      "    val_log_likelihood: -12165.778511101635\n",
      "    val_log_marginal: -12173.76835121702\n",
      "Train Epoch: 1655 [256/118836 (0%)] Loss: 12323.042969\n",
      "Train Epoch: 1655 [33024/118836 (28%)] Loss: 12315.659180\n",
      "Train Epoch: 1655 [65792/118836 (55%)] Loss: 12282.554688\n",
      "Train Epoch: 1655 [98560/118836 (83%)] Loss: 12224.574219\n",
      "    epoch          : 1655\n",
      "    loss           : 12259.55639814025\n",
      "    val_loss       : 12264.673008112268\n",
      "    val_log_likelihood: -12164.98079475548\n",
      "    val_log_marginal: -12173.005690924918\n",
      "Train Epoch: 1656 [256/118836 (0%)] Loss: 12369.060547\n",
      "Train Epoch: 1656 [33024/118836 (28%)] Loss: 12246.955078\n",
      "Train Epoch: 1656 [65792/118836 (55%)] Loss: 12249.181641\n",
      "Train Epoch: 1656 [98560/118836 (83%)] Loss: 12425.799805\n",
      "    epoch          : 1656\n",
      "    loss           : 12269.414514675094\n",
      "    val_loss       : 12263.664898119034\n",
      "    val_log_likelihood: -12164.259630085557\n",
      "    val_log_marginal: -12172.280098796355\n",
      "Train Epoch: 1657 [256/118836 (0%)] Loss: 12226.490234\n",
      "Train Epoch: 1657 [33024/118836 (28%)] Loss: 12215.347656\n",
      "Train Epoch: 1657 [65792/118836 (55%)] Loss: 12387.335938\n",
      "Train Epoch: 1657 [98560/118836 (83%)] Loss: 12220.910156\n",
      "    epoch          : 1657\n",
      "    loss           : 12262.62147516672\n",
      "    val_loss       : 12267.741476752728\n",
      "    val_log_likelihood: -12164.830997176128\n",
      "    val_log_marginal: -12173.005727765789\n",
      "Train Epoch: 1658 [256/118836 (0%)] Loss: 12188.842773\n",
      "Train Epoch: 1658 [33024/118836 (28%)] Loss: 12308.416992\n",
      "Train Epoch: 1658 [65792/118836 (55%)] Loss: 12363.760742\n",
      "Train Epoch: 1658 [98560/118836 (83%)] Loss: 12238.680664\n",
      "    epoch          : 1658\n",
      "    loss           : 12262.185415374275\n",
      "    val_loss       : 12262.761617399214\n",
      "    val_log_likelihood: -12167.46422954146\n",
      "    val_log_marginal: -12175.26942088087\n",
      "Train Epoch: 1659 [256/118836 (0%)] Loss: 12238.570312\n",
      "Train Epoch: 1659 [33024/118836 (28%)] Loss: 12267.935547\n",
      "Train Epoch: 1659 [65792/118836 (55%)] Loss: 12281.541992\n",
      "Train Epoch: 1659 [98560/118836 (83%)] Loss: 12328.011719\n",
      "    epoch          : 1659\n",
      "    loss           : 12261.169380040323\n",
      "    val_loss       : 12261.909290078831\n",
      "    val_log_likelihood: -12166.880229496226\n",
      "    val_log_marginal: -12174.80649667808\n",
      "Train Epoch: 1660 [256/118836 (0%)] Loss: 12298.741211\n",
      "Train Epoch: 1660 [33024/118836 (28%)] Loss: 12322.519531\n",
      "Train Epoch: 1660 [65792/118836 (55%)] Loss: 12210.123047\n",
      "Train Epoch: 1660 [98560/118836 (83%)] Loss: 12260.100586\n",
      "    epoch          : 1660\n",
      "    loss           : 12266.549053324028\n",
      "    val_loss       : 12264.80749378309\n",
      "    val_log_likelihood: -12166.165477829041\n",
      "    val_log_marginal: -12174.129390696502\n",
      "Train Epoch: 1661 [256/118836 (0%)] Loss: 12331.159180\n",
      "Train Epoch: 1661 [33024/118836 (28%)] Loss: 12292.774414\n",
      "Train Epoch: 1661 [65792/118836 (55%)] Loss: 12283.989258\n",
      "Train Epoch: 1661 [98560/118836 (83%)] Loss: 12433.283203\n",
      "    epoch          : 1661\n",
      "    loss           : 12263.633265805935\n",
      "    val_loss       : 12264.47495442204\n",
      "    val_log_likelihood: -12166.74013599178\n",
      "    val_log_marginal: -12174.744184171912\n",
      "Train Epoch: 1662 [256/118836 (0%)] Loss: 12224.858398\n",
      "Train Epoch: 1662 [33024/118836 (28%)] Loss: 12361.958008\n",
      "Train Epoch: 1662 [65792/118836 (55%)] Loss: 12379.188477\n",
      "Train Epoch: 1662 [98560/118836 (83%)] Loss: 12317.656250\n",
      "    epoch          : 1662\n",
      "    loss           : 12265.475202905294\n",
      "    val_loss       : 12262.310394501768\n",
      "    val_log_likelihood: -12167.144326890766\n",
      "    val_log_marginal: -12175.064445010043\n",
      "Train Epoch: 1663 [256/118836 (0%)] Loss: 12334.974609\n",
      "Train Epoch: 1663 [33024/118836 (28%)] Loss: 12300.315430\n",
      "Train Epoch: 1663 [65792/118836 (55%)] Loss: 12303.608398\n",
      "Train Epoch: 1663 [98560/118836 (83%)] Loss: 12300.554688\n",
      "    epoch          : 1663\n",
      "    loss           : 12262.87963903536\n",
      "    val_loss       : 12257.705055961185\n",
      "    val_log_likelihood: -12162.607679706885\n",
      "    val_log_marginal: -12170.608873967043\n",
      "Train Epoch: 1664 [256/118836 (0%)] Loss: 12217.983398\n",
      "Train Epoch: 1664 [33024/118836 (28%)] Loss: 12210.702148\n",
      "Train Epoch: 1664 [65792/118836 (55%)] Loss: 12258.937500\n",
      "Train Epoch: 1664 [98560/118836 (83%)] Loss: 12292.655273\n",
      "    epoch          : 1664\n",
      "    loss           : 12261.530454210608\n",
      "    val_loss       : 12261.40640941524\n",
      "    val_log_likelihood: -12163.841378625157\n",
      "    val_log_marginal: -12172.000666651533\n",
      "Train Epoch: 1665 [256/118836 (0%)] Loss: 12197.929688\n",
      "Train Epoch: 1665 [33024/118836 (28%)] Loss: 12187.585938\n",
      "Train Epoch: 1665 [65792/118836 (55%)] Loss: 12197.670898\n",
      "Train Epoch: 1665 [98560/118836 (83%)] Loss: 12267.019531\n",
      "    epoch          : 1665\n",
      "    loss           : 12263.941755033862\n",
      "    val_loss       : 12257.668399744565\n",
      "    val_log_likelihood: -12162.74725367039\n",
      "    val_log_marginal: -12170.784798586867\n",
      "Train Epoch: 1666 [256/118836 (0%)] Loss: 12285.373047\n",
      "Train Epoch: 1666 [33024/118836 (28%)] Loss: 12314.889648\n",
      "Train Epoch: 1666 [65792/118836 (55%)] Loss: 12422.767578\n",
      "Train Epoch: 1666 [98560/118836 (83%)] Loss: 12263.950195\n",
      "    epoch          : 1666\n",
      "    loss           : 12268.410938307743\n",
      "    val_loss       : 12265.770831961265\n",
      "    val_log_likelihood: -12165.403038248294\n",
      "    val_log_marginal: -12173.495846379237\n",
      "Train Epoch: 1667 [256/118836 (0%)] Loss: 12325.054688\n",
      "Train Epoch: 1667 [33024/118836 (28%)] Loss: 12318.943359\n",
      "Train Epoch: 1667 [65792/118836 (55%)] Loss: 12352.812500\n",
      "Train Epoch: 1667 [98560/118836 (83%)] Loss: 12284.035156\n",
      "    epoch          : 1667\n",
      "    loss           : 12264.830593142577\n",
      "    val_loss       : 12264.039086982912\n",
      "    val_log_likelihood: -12163.271813288358\n",
      "    val_log_marginal: -12171.231043170897\n",
      "Train Epoch: 1668 [256/118836 (0%)] Loss: 12232.009766\n",
      "Train Epoch: 1668 [33024/118836 (28%)] Loss: 12178.984375\n",
      "Train Epoch: 1668 [65792/118836 (55%)] Loss: 12276.111328\n",
      "Train Epoch: 1668 [98560/118836 (83%)] Loss: 12273.834961\n",
      "    epoch          : 1668\n",
      "    loss           : 12263.212291117401\n",
      "    val_loss       : 12261.153162786264\n",
      "    val_log_likelihood: -12167.921201179952\n",
      "    val_log_marginal: -12175.834012778849\n",
      "Train Epoch: 1669 [256/118836 (0%)] Loss: 12192.916992\n",
      "Train Epoch: 1669 [33024/118836 (28%)] Loss: 12293.706055\n",
      "Train Epoch: 1669 [65792/118836 (55%)] Loss: 12276.469727\n",
      "Train Epoch: 1669 [98560/118836 (83%)] Loss: 12303.401367\n",
      "    epoch          : 1669\n",
      "    loss           : 12264.80650314697\n",
      "    val_loss       : 12262.796297135632\n",
      "    val_log_likelihood: -12164.471239790117\n",
      "    val_log_marginal: -12172.528145603768\n",
      "Train Epoch: 1670 [256/118836 (0%)] Loss: 12195.964844\n",
      "Train Epoch: 1670 [33024/118836 (28%)] Loss: 12275.914062\n",
      "Train Epoch: 1670 [65792/118836 (55%)] Loss: 12258.619141\n",
      "Train Epoch: 1670 [98560/118836 (83%)] Loss: 12230.099609\n",
      "    epoch          : 1670\n",
      "    loss           : 12264.721154492348\n",
      "    val_loss       : 12261.37219033709\n",
      "    val_log_likelihood: -12164.939980904932\n",
      "    val_log_marginal: -12172.86394638712\n",
      "Train Epoch: 1671 [256/118836 (0%)] Loss: 12309.158203\n",
      "Train Epoch: 1671 [33024/118836 (28%)] Loss: 12302.906250\n",
      "Train Epoch: 1671 [65792/118836 (55%)] Loss: 12351.369141\n",
      "Train Epoch: 1671 [98560/118836 (83%)] Loss: 12363.950195\n",
      "    epoch          : 1671\n",
      "    loss           : 12263.419046603598\n",
      "    val_loss       : 12264.310601584342\n",
      "    val_log_likelihood: -12164.837503554074\n",
      "    val_log_marginal: -12172.859654952092\n",
      "Train Epoch: 1672 [256/118836 (0%)] Loss: 12525.539062\n",
      "Train Epoch: 1672 [33024/118836 (28%)] Loss: 12414.871094\n",
      "Train Epoch: 1672 [65792/118836 (55%)] Loss: 12276.916016\n",
      "Train Epoch: 1672 [98560/118836 (83%)] Loss: 12217.899414\n",
      "    epoch          : 1672\n",
      "    loss           : 12262.628256500724\n",
      "    val_loss       : 12264.85745124321\n",
      "    val_log_likelihood: -12166.086071747053\n",
      "    val_log_marginal: -12174.152460408828\n",
      "Train Epoch: 1673 [256/118836 (0%)] Loss: 12268.208008\n",
      "Train Epoch: 1673 [33024/118836 (28%)] Loss: 12306.833984\n",
      "Train Epoch: 1673 [65792/118836 (55%)] Loss: 12292.943359\n",
      "Train Epoch: 1673 [98560/118836 (83%)] Loss: 12224.198242\n",
      "    epoch          : 1673\n",
      "    loss           : 12263.66895322839\n",
      "    val_loss       : 12266.982895525407\n",
      "    val_log_likelihood: -12163.844072289858\n",
      "    val_log_marginal: -12171.958873362206\n",
      "Train Epoch: 1674 [256/118836 (0%)] Loss: 12282.251953\n",
      "Train Epoch: 1674 [33024/118836 (28%)] Loss: 12365.300781\n",
      "Train Epoch: 1674 [65792/118836 (55%)] Loss: 12236.719727\n",
      "Train Epoch: 1674 [98560/118836 (83%)] Loss: 12234.923828\n",
      "    epoch          : 1674\n",
      "    loss           : 12259.085447522488\n",
      "    val_loss       : 12262.796174529063\n",
      "    val_log_likelihood: -12163.398481764372\n",
      "    val_log_marginal: -12171.457572529484\n",
      "Train Epoch: 1675 [256/118836 (0%)] Loss: 12210.025391\n",
      "Train Epoch: 1675 [33024/118836 (28%)] Loss: 12243.709961\n",
      "Train Epoch: 1675 [65792/118836 (55%)] Loss: 12328.452148\n",
      "Train Epoch: 1675 [98560/118836 (83%)] Loss: 12376.048828\n",
      "    epoch          : 1675\n",
      "    loss           : 12267.225712914857\n",
      "    val_loss       : 12257.200724433565\n",
      "    val_log_likelihood: -12165.340354405758\n",
      "    val_log_marginal: -12173.364981154073\n",
      "Train Epoch: 1676 [256/118836 (0%)] Loss: 12265.423828\n",
      "Train Epoch: 1676 [33024/118836 (28%)] Loss: 12492.884766\n",
      "Train Epoch: 1676 [65792/118836 (55%)] Loss: 12253.073242\n",
      "Train Epoch: 1676 [98560/118836 (83%)] Loss: 12320.417969\n",
      "    epoch          : 1676\n",
      "    loss           : 12260.48860984026\n",
      "    val_loss       : 12259.713590015057\n",
      "    val_log_likelihood: -12166.14029350186\n",
      "    val_log_marginal: -12174.0628018802\n",
      "Train Epoch: 1677 [256/118836 (0%)] Loss: 12342.103516\n",
      "Train Epoch: 1677 [33024/118836 (28%)] Loss: 12332.724609\n",
      "Train Epoch: 1677 [65792/118836 (55%)] Loss: 12227.324219\n",
      "Train Epoch: 1677 [98560/118836 (83%)] Loss: 12268.123047\n",
      "    epoch          : 1677\n",
      "    loss           : 12263.177558448357\n",
      "    val_loss       : 12265.659328491462\n",
      "    val_log_likelihood: -12165.931991670543\n",
      "    val_log_marginal: -12173.894168495275\n",
      "Train Epoch: 1678 [256/118836 (0%)] Loss: 12178.212891\n",
      "Train Epoch: 1678 [33024/118836 (28%)] Loss: 12253.733398\n",
      "Train Epoch: 1678 [65792/118836 (55%)] Loss: 12347.481445\n",
      "Train Epoch: 1678 [98560/118836 (83%)] Loss: 12293.421875\n",
      "    epoch          : 1678\n",
      "    loss           : 12260.604469247572\n",
      "    val_loss       : 12264.32639365064\n",
      "    val_log_likelihood: -12165.568470035929\n",
      "    val_log_marginal: -12173.543813047025\n",
      "Train Epoch: 1679 [256/118836 (0%)] Loss: 12242.216797\n",
      "Train Epoch: 1679 [33024/118836 (28%)] Loss: 12174.496094\n",
      "Train Epoch: 1679 [65792/118836 (55%)] Loss: 12179.949219\n",
      "Train Epoch: 1679 [98560/118836 (83%)] Loss: 12317.912109\n",
      "    epoch          : 1679\n",
      "    loss           : 12262.112838444737\n",
      "    val_loss       : 12263.909577882807\n",
      "    val_log_likelihood: -12162.564824202595\n",
      "    val_log_marginal: -12170.526349603844\n",
      "Train Epoch: 1680 [256/118836 (0%)] Loss: 12192.345703\n",
      "Train Epoch: 1680 [33024/118836 (28%)] Loss: 12267.167969\n",
      "Train Epoch: 1680 [65792/118836 (55%)] Loss: 12302.188477\n",
      "Train Epoch: 1680 [98560/118836 (83%)] Loss: 12333.023438\n",
      "    epoch          : 1680\n",
      "    loss           : 12268.849467535152\n",
      "    val_loss       : 12263.94269375335\n",
      "    val_log_likelihood: -12163.584042047922\n",
      "    val_log_marginal: -12171.675333103783\n",
      "Train Epoch: 1681 [256/118836 (0%)] Loss: 12322.042969\n",
      "Train Epoch: 1681 [33024/118836 (28%)] Loss: 12264.271484\n",
      "Train Epoch: 1681 [65792/118836 (55%)] Loss: 12351.391602\n",
      "Train Epoch: 1681 [98560/118836 (83%)] Loss: 12181.861328\n",
      "    epoch          : 1681\n",
      "    loss           : 12258.615364421785\n",
      "    val_loss       : 12265.809869483872\n",
      "    val_log_likelihood: -12163.04160172405\n",
      "    val_log_marginal: -12171.124971971312\n",
      "Train Epoch: 1682 [256/118836 (0%)] Loss: 12443.797852\n",
      "Train Epoch: 1682 [33024/118836 (28%)] Loss: 12292.136719\n",
      "Train Epoch: 1682 [65792/118836 (55%)] Loss: 12275.320312\n",
      "Train Epoch: 1682 [98560/118836 (83%)] Loss: 12385.837891\n",
      "    epoch          : 1682\n",
      "    loss           : 12264.217865358767\n",
      "    val_loss       : 12263.67147588368\n",
      "    val_log_likelihood: -12167.140076380274\n",
      "    val_log_marginal: -12175.553443620822\n",
      "Train Epoch: 1683 [256/118836 (0%)] Loss: 12312.039062\n",
      "Train Epoch: 1683 [33024/118836 (28%)] Loss: 12205.905273\n",
      "Train Epoch: 1683 [65792/118836 (55%)] Loss: 12273.036133\n",
      "Train Epoch: 1683 [98560/118836 (83%)] Loss: 12295.827148\n",
      "    epoch          : 1683\n",
      "    loss           : 12264.796669186828\n",
      "    val_loss       : 12265.4395490655\n",
      "    val_log_likelihood: -12163.371773547353\n",
      "    val_log_marginal: -12171.378108239549\n",
      "Train Epoch: 1684 [256/118836 (0%)] Loss: 12334.496094\n",
      "Train Epoch: 1684 [33024/118836 (28%)] Loss: 12278.642578\n",
      "Train Epoch: 1684 [65792/118836 (55%)] Loss: 12320.691406\n",
      "Train Epoch: 1684 [98560/118836 (83%)] Loss: 12282.301758\n",
      "    epoch          : 1684\n",
      "    loss           : 12259.77106273263\n",
      "    val_loss       : 12264.690666692586\n",
      "    val_log_likelihood: -12163.286930378928\n",
      "    val_log_marginal: -12171.4111165668\n",
      "Train Epoch: 1685 [256/118836 (0%)] Loss: 12272.458008\n",
      "Train Epoch: 1685 [33024/118836 (28%)] Loss: 12302.535156\n",
      "Train Epoch: 1685 [65792/118836 (55%)] Loss: 12382.606445\n",
      "Train Epoch: 1685 [98560/118836 (83%)] Loss: 12234.863281\n",
      "    epoch          : 1685\n",
      "    loss           : 12264.358578079768\n",
      "    val_loss       : 12260.215946214961\n",
      "    val_log_likelihood: -12162.954539844395\n",
      "    val_log_marginal: -12170.930327246211\n",
      "Train Epoch: 1686 [256/118836 (0%)] Loss: 12268.742188\n",
      "Train Epoch: 1686 [33024/118836 (28%)] Loss: 12163.038086\n",
      "Train Epoch: 1686 [65792/118836 (55%)] Loss: 12229.572266\n",
      "Train Epoch: 1686 [98560/118836 (83%)] Loss: 12508.025391\n",
      "    epoch          : 1686\n",
      "    loss           : 12266.050150563482\n",
      "    val_loss       : 12266.400846048648\n",
      "    val_log_likelihood: -12166.83129975703\n",
      "    val_log_marginal: -12174.690360398055\n",
      "Train Epoch: 1687 [256/118836 (0%)] Loss: 12361.584961\n",
      "Train Epoch: 1687 [33024/118836 (28%)] Loss: 12253.732422\n",
      "Train Epoch: 1687 [65792/118836 (55%)] Loss: 12298.590820\n",
      "Train Epoch: 1687 [98560/118836 (83%)] Loss: 12225.360352\n",
      "    epoch          : 1687\n",
      "    loss           : 12264.05323695332\n",
      "    val_loss       : 12259.067965732216\n",
      "    val_log_likelihood: -12163.562204042597\n",
      "    val_log_marginal: -12171.46275737679\n",
      "Train Epoch: 1688 [256/118836 (0%)] Loss: 12302.197266\n",
      "Train Epoch: 1688 [33024/118836 (28%)] Loss: 12268.106445\n",
      "Train Epoch: 1688 [65792/118836 (55%)] Loss: 12398.390625\n",
      "Train Epoch: 1688 [98560/118836 (83%)] Loss: 12285.404297\n",
      "    epoch          : 1688\n",
      "    loss           : 12270.622848169975\n",
      "    val_loss       : 12261.802574637344\n",
      "    val_log_likelihood: -12167.87036484181\n",
      "    val_log_marginal: -12176.22229485317\n",
      "Train Epoch: 1689 [256/118836 (0%)] Loss: 12274.177734\n",
      "Train Epoch: 1689 [33024/118836 (28%)] Loss: 12182.934570\n",
      "Train Epoch: 1689 [65792/118836 (55%)] Loss: 12229.908203\n",
      "Train Epoch: 1689 [98560/118836 (83%)] Loss: 12288.948242\n",
      "    epoch          : 1689\n",
      "    loss           : 12260.260593077957\n",
      "    val_loss       : 12261.434577679307\n",
      "    val_log_likelihood: -12163.07571937681\n",
      "    val_log_marginal: -12171.114249405722\n",
      "Train Epoch: 1690 [256/118836 (0%)] Loss: 12393.715820\n",
      "Train Epoch: 1690 [33024/118836 (28%)] Loss: 12289.641602\n",
      "Train Epoch: 1690 [65792/118836 (55%)] Loss: 12208.980469\n",
      "Train Epoch: 1690 [98560/118836 (83%)] Loss: 12325.289062\n",
      "    epoch          : 1690\n",
      "    loss           : 12262.18982468724\n",
      "    val_loss       : 12263.313696257688\n",
      "    val_log_likelihood: -12164.506035140095\n",
      "    val_log_marginal: -12172.48267423403\n",
      "Train Epoch: 1691 [256/118836 (0%)] Loss: 12257.441406\n",
      "Train Epoch: 1691 [33024/118836 (28%)] Loss: 12346.078125\n",
      "Train Epoch: 1691 [65792/118836 (55%)] Loss: 12281.461914\n",
      "Train Epoch: 1691 [98560/118836 (83%)] Loss: 12270.873047\n",
      "    epoch          : 1691\n",
      "    loss           : 12262.702171538978\n",
      "    val_loss       : 12264.494162026738\n",
      "    val_log_likelihood: -12168.726307575993\n",
      "    val_log_marginal: -12176.722961419506\n",
      "Train Epoch: 1692 [256/118836 (0%)] Loss: 12173.402344\n",
      "Train Epoch: 1692 [33024/118836 (28%)] Loss: 12325.203125\n",
      "Train Epoch: 1692 [65792/118836 (55%)] Loss: 12289.194336\n",
      "Train Epoch: 1692 [98560/118836 (83%)] Loss: 12280.362305\n",
      "    epoch          : 1692\n",
      "    loss           : 12262.212853953422\n",
      "    val_loss       : 12264.667677444055\n",
      "    val_log_likelihood: -12165.763744571961\n",
      "    val_log_marginal: -12173.813144482909\n",
      "Train Epoch: 1693 [256/118836 (0%)] Loss: 12170.108398\n",
      "Train Epoch: 1693 [33024/118836 (28%)] Loss: 12277.484375\n",
      "Train Epoch: 1693 [65792/118836 (55%)] Loss: 12227.992188\n",
      "Train Epoch: 1693 [98560/118836 (83%)] Loss: 12328.699219\n",
      "    epoch          : 1693\n",
      "    loss           : 12261.446466604631\n",
      "    val_loss       : 12267.246588713868\n",
      "    val_log_likelihood: -12168.560211015045\n",
      "    val_log_marginal: -12176.5493649504\n",
      "Train Epoch: 1694 [256/118836 (0%)] Loss: 12235.996094\n",
      "Train Epoch: 1694 [33024/118836 (28%)] Loss: 12238.252930\n",
      "Train Epoch: 1694 [65792/118836 (55%)] Loss: 12312.517578\n",
      "Train Epoch: 1694 [98560/118836 (83%)] Loss: 12194.102539\n",
      "    epoch          : 1694\n",
      "    loss           : 12267.760182097809\n",
      "    val_loss       : 12263.21967664202\n",
      "    val_log_likelihood: -12164.140106428351\n",
      "    val_log_marginal: -12172.12766145334\n",
      "Train Epoch: 1695 [256/118836 (0%)] Loss: 12251.708984\n",
      "Train Epoch: 1695 [33024/118836 (28%)] Loss: 12189.927734\n",
      "Train Epoch: 1695 [65792/118836 (55%)] Loss: 12224.694336\n",
      "Train Epoch: 1695 [98560/118836 (83%)] Loss: 12329.853516\n",
      "    epoch          : 1695\n",
      "    loss           : 12266.006760171113\n",
      "    val_loss       : 12264.920877081102\n",
      "    val_log_likelihood: -12166.955813010494\n",
      "    val_log_marginal: -12174.922257672988\n",
      "Train Epoch: 1696 [256/118836 (0%)] Loss: 12314.625000\n",
      "Train Epoch: 1696 [33024/118836 (28%)] Loss: 12318.183594\n",
      "Train Epoch: 1696 [65792/118836 (55%)] Loss: 12222.609375\n",
      "Train Epoch: 1696 [98560/118836 (83%)] Loss: 12300.173828\n",
      "    epoch          : 1696\n",
      "    loss           : 12263.664486242504\n",
      "    val_loss       : 12262.5569353503\n",
      "    val_log_likelihood: -12164.024625045233\n",
      "    val_log_marginal: -12172.137577757885\n",
      "Train Epoch: 1697 [256/118836 (0%)] Loss: 12177.741211\n",
      "Train Epoch: 1697 [33024/118836 (28%)] Loss: 12264.291992\n",
      "Train Epoch: 1697 [65792/118836 (55%)] Loss: 12299.815430\n",
      "Train Epoch: 1697 [98560/118836 (83%)] Loss: 12294.387695\n",
      "    epoch          : 1697\n",
      "    loss           : 12262.697178873294\n",
      "    val_loss       : 12263.644728225625\n",
      "    val_log_likelihood: -12163.172148825217\n",
      "    val_log_marginal: -12171.231235439851\n",
      "Train Epoch: 1698 [256/118836 (0%)] Loss: 12249.523438\n",
      "Train Epoch: 1698 [33024/118836 (28%)] Loss: 12296.264648\n",
      "Train Epoch: 1698 [65792/118836 (55%)] Loss: 12280.412109\n",
      "Train Epoch: 1698 [98560/118836 (83%)] Loss: 12330.764648\n",
      "    epoch          : 1698\n",
      "    loss           : 12262.915490752946\n",
      "    val_loss       : 12255.96804128044\n",
      "    val_log_likelihood: -12165.74817886037\n",
      "    val_log_marginal: -12173.995877674422\n",
      "Train Epoch: 1699 [256/118836 (0%)] Loss: 12352.983398\n",
      "Train Epoch: 1699 [33024/118836 (28%)] Loss: 12257.511719\n",
      "Train Epoch: 1699 [65792/118836 (55%)] Loss: 12321.014648\n",
      "Train Epoch: 1699 [98560/118836 (83%)] Loss: 12249.969727\n",
      "    epoch          : 1699\n",
      "    loss           : 12269.407895212986\n",
      "    val_loss       : 12261.026969221693\n",
      "    val_log_likelihood: -12163.898819401365\n",
      "    val_log_marginal: -12172.027098485472\n",
      "Train Epoch: 1700 [256/118836 (0%)] Loss: 12265.831055\n",
      "Train Epoch: 1700 [33024/118836 (28%)] Loss: 12270.650391\n",
      "Train Epoch: 1700 [65792/118836 (55%)] Loss: 12306.193359\n",
      "Train Epoch: 1700 [98560/118836 (83%)] Loss: 12249.353516\n",
      "    epoch          : 1700\n",
      "    loss           : 12255.760185490331\n",
      "    val_loss       : 12260.719831971242\n",
      "    val_log_likelihood: -12162.996369029157\n",
      "    val_log_marginal: -12171.324417841795\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch1700.pth ...\n",
      "Train Epoch: 1701 [256/118836 (0%)] Loss: 12224.778320\n",
      "Train Epoch: 1701 [33024/118836 (28%)] Loss: 12260.938477\n",
      "Train Epoch: 1701 [65792/118836 (55%)] Loss: 12191.720703\n",
      "Train Epoch: 1701 [98560/118836 (83%)] Loss: 12296.773438\n",
      "    epoch          : 1701\n",
      "    loss           : 12262.762826651675\n",
      "    val_loss       : 12263.723372031569\n",
      "    val_log_likelihood: -12164.344205729167\n",
      "    val_log_marginal: -12172.586230215114\n",
      "Train Epoch: 1702 [256/118836 (0%)] Loss: 12296.470703\n",
      "Train Epoch: 1702 [33024/118836 (28%)] Loss: 12231.672852\n",
      "Train Epoch: 1702 [65792/118836 (55%)] Loss: 12189.994141\n",
      "Train Epoch: 1702 [98560/118836 (83%)] Loss: 12249.979492\n",
      "    epoch          : 1702\n",
      "    loss           : 12258.189383335919\n",
      "    val_loss       : 12261.01068400046\n",
      "    val_log_likelihood: -12164.03829045828\n",
      "    val_log_marginal: -12172.164497533722\n",
      "Train Epoch: 1703 [256/118836 (0%)] Loss: 12318.625000\n",
      "Train Epoch: 1703 [33024/118836 (28%)] Loss: 12377.270508\n",
      "Train Epoch: 1703 [65792/118836 (55%)] Loss: 12359.980469\n",
      "Train Epoch: 1703 [98560/118836 (83%)] Loss: 12283.249023\n",
      "    epoch          : 1703\n",
      "    loss           : 12257.282670175506\n",
      "    val_loss       : 12262.618313725197\n",
      "    val_log_likelihood: -12164.457734310381\n",
      "    val_log_marginal: -12172.656160088545\n",
      "Train Epoch: 1704 [256/118836 (0%)] Loss: 12252.950195\n",
      "Train Epoch: 1704 [33024/118836 (28%)] Loss: 12309.872070\n",
      "Train Epoch: 1704 [65792/118836 (55%)] Loss: 12290.036133\n",
      "Train Epoch: 1704 [98560/118836 (83%)] Loss: 12297.467773\n",
      "    epoch          : 1704\n",
      "    loss           : 12261.352728397693\n",
      "    val_loss       : 12263.289559976987\n",
      "    val_log_likelihood: -12165.331139823718\n",
      "    val_log_marginal: -12173.404603651074\n",
      "Train Epoch: 1705 [256/118836 (0%)] Loss: 12340.550781\n",
      "Train Epoch: 1705 [33024/118836 (28%)] Loss: 12207.177734\n",
      "Train Epoch: 1705 [65792/118836 (55%)] Loss: 12308.224609\n",
      "Train Epoch: 1705 [98560/118836 (83%)] Loss: 12269.412109\n",
      "    epoch          : 1705\n",
      "    loss           : 12257.826923884668\n",
      "    val_loss       : 12260.797313388046\n",
      "    val_log_likelihood: -12165.568500407102\n",
      "    val_log_marginal: -12173.708771205307\n",
      "Train Epoch: 1706 [256/118836 (0%)] Loss: 12190.863281\n",
      "Train Epoch: 1706 [33024/118836 (28%)] Loss: 12325.894531\n",
      "Train Epoch: 1706 [65792/118836 (55%)] Loss: 12345.052734\n",
      "Train Epoch: 1706 [98560/118836 (83%)] Loss: 12171.182617\n",
      "    epoch          : 1706\n",
      "    loss           : 12260.702635830232\n",
      "    val_loss       : 12260.497887128773\n",
      "    val_log_likelihood: -12166.27604134357\n",
      "    val_log_marginal: -12174.456246658148\n",
      "Train Epoch: 1707 [256/118836 (0%)] Loss: 12294.148438\n",
      "Train Epoch: 1707 [33024/118836 (28%)] Loss: 12286.958984\n",
      "Train Epoch: 1707 [65792/118836 (55%)] Loss: 12357.894531\n",
      "Train Epoch: 1707 [98560/118836 (83%)] Loss: 12209.990234\n",
      "    epoch          : 1707\n",
      "    loss           : 12260.985119093777\n",
      "    val_loss       : 12266.689927410835\n",
      "    val_log_likelihood: -12167.646435748811\n",
      "    val_log_marginal: -12175.924949260378\n",
      "Train Epoch: 1708 [256/118836 (0%)] Loss: 12265.800781\n",
      "Train Epoch: 1708 [33024/118836 (28%)] Loss: 12224.921875\n",
      "Train Epoch: 1708 [65792/118836 (55%)] Loss: 12341.037109\n",
      "Train Epoch: 1708 [98560/118836 (83%)] Loss: 12217.075195\n",
      "    epoch          : 1708\n",
      "    loss           : 12260.707027211281\n",
      "    val_loss       : 12259.531867079631\n",
      "    val_log_likelihood: -12164.906384085505\n",
      "    val_log_marginal: -12172.941832798753\n",
      "Train Epoch: 1709 [256/118836 (0%)] Loss: 12262.862305\n",
      "Train Epoch: 1709 [33024/118836 (28%)] Loss: 12273.909180\n",
      "Train Epoch: 1709 [65792/118836 (55%)] Loss: 12359.455078\n",
      "Train Epoch: 1709 [98560/118836 (83%)] Loss: 12377.904297\n",
      "    epoch          : 1709\n",
      "    loss           : 12256.668942243074\n",
      "    val_loss       : 12261.444238375885\n",
      "    val_log_likelihood: -12166.209436065446\n",
      "    val_log_marginal: -12174.697267960813\n",
      "Train Epoch: 1710 [256/118836 (0%)] Loss: 12257.319336\n",
      "Train Epoch: 1710 [33024/118836 (28%)] Loss: 12263.015625\n",
      "Train Epoch: 1710 [65792/118836 (55%)] Loss: 12215.845703\n",
      "Train Epoch: 1710 [98560/118836 (83%)] Loss: 12196.652344\n",
      "    epoch          : 1710\n",
      "    loss           : 12255.047721192617\n",
      "    val_loss       : 12263.283415045715\n",
      "    val_log_likelihood: -12165.269837708074\n",
      "    val_log_marginal: -12173.38949068\n",
      "Train Epoch: 1711 [256/118836 (0%)] Loss: 12383.104492\n",
      "Train Epoch: 1711 [33024/118836 (28%)] Loss: 12413.772461\n",
      "Train Epoch: 1711 [65792/118836 (55%)] Loss: 12310.524414\n",
      "Train Epoch: 1711 [98560/118836 (83%)] Loss: 12311.531250\n",
      "    epoch          : 1711\n",
      "    loss           : 12263.168400246846\n",
      "    val_loss       : 12261.633390360577\n",
      "    val_log_likelihood: -12167.324094034328\n",
      "    val_log_marginal: -12175.50660977562\n",
      "Train Epoch: 1712 [256/118836 (0%)] Loss: 12284.841797\n",
      "Train Epoch: 1712 [33024/118836 (28%)] Loss: 12412.261719\n",
      "Train Epoch: 1712 [65792/118836 (55%)] Loss: 12310.355469\n",
      "Train Epoch: 1712 [98560/118836 (83%)] Loss: 12309.460938\n",
      "    epoch          : 1712\n",
      "    loss           : 12261.799493382961\n",
      "    val_loss       : 12260.18668690805\n",
      "    val_log_likelihood: -12164.987718090882\n",
      "    val_log_marginal: -12173.16773645663\n",
      "Train Epoch: 1713 [256/118836 (0%)] Loss: 12343.580078\n",
      "Train Epoch: 1713 [33024/118836 (28%)] Loss: 12269.210938\n",
      "Train Epoch: 1713 [65792/118836 (55%)] Loss: 12248.777344\n",
      "Train Epoch: 1713 [98560/118836 (83%)] Loss: 12379.685547\n",
      "    epoch          : 1713\n",
      "    loss           : 12264.608777592535\n",
      "    val_loss       : 12267.256101108454\n",
      "    val_log_likelihood: -12166.08347791951\n",
      "    val_log_marginal: -12174.205081298043\n",
      "Train Epoch: 1714 [256/118836 (0%)] Loss: 12267.431641\n",
      "Train Epoch: 1714 [33024/118836 (28%)] Loss: 12293.430664\n",
      "Train Epoch: 1714 [65792/118836 (55%)] Loss: 12223.050781\n",
      "Train Epoch: 1714 [98560/118836 (83%)] Loss: 12315.883789\n",
      "    epoch          : 1714\n",
      "    loss           : 12262.371289385597\n",
      "    val_loss       : 12263.133329907358\n",
      "    val_log_likelihood: -12161.527192540323\n",
      "    val_log_marginal: -12169.631064545218\n",
      "Train Epoch: 1715 [256/118836 (0%)] Loss: 12208.486328\n",
      "Train Epoch: 1715 [33024/118836 (28%)] Loss: 12304.931641\n",
      "Train Epoch: 1715 [65792/118836 (55%)] Loss: 12312.845703\n",
      "Train Epoch: 1715 [98560/118836 (83%)] Loss: 12344.508789\n",
      "    epoch          : 1715\n",
      "    loss           : 12259.671885500673\n",
      "    val_loss       : 12257.131092378697\n",
      "    val_log_likelihood: -12161.70360576923\n",
      "    val_log_marginal: -12169.872018018059\n",
      "Train Epoch: 1716 [256/118836 (0%)] Loss: 12262.541016\n",
      "Train Epoch: 1716 [33024/118836 (28%)] Loss: 12263.364258\n",
      "Train Epoch: 1716 [65792/118836 (55%)] Loss: 12308.387695\n",
      "Train Epoch: 1716 [98560/118836 (83%)] Loss: 12391.177734\n",
      "    epoch          : 1716\n",
      "    loss           : 12254.252220972912\n",
      "    val_loss       : 12256.948525108728\n",
      "    val_log_likelihood: -12165.31210679022\n",
      "    val_log_marginal: -12173.331419995386\n",
      "Train Epoch: 1717 [256/118836 (0%)] Loss: 12206.556641\n",
      "Train Epoch: 1717 [33024/118836 (28%)] Loss: 12269.259766\n",
      "Train Epoch: 1717 [65792/118836 (55%)] Loss: 12180.250977\n",
      "Train Epoch: 1717 [98560/118836 (83%)] Loss: 12364.691406\n",
      "    epoch          : 1717\n",
      "    loss           : 12261.099740391077\n",
      "    val_loss       : 12258.235285482026\n",
      "    val_log_likelihood: -12161.69394288927\n",
      "    val_log_marginal: -12169.715049847175\n",
      "Train Epoch: 1718 [256/118836 (0%)] Loss: 12230.305664\n",
      "Train Epoch: 1718 [33024/118836 (28%)] Loss: 12310.968750\n",
      "Train Epoch: 1718 [65792/118836 (55%)] Loss: 12288.905273\n",
      "Train Epoch: 1718 [98560/118836 (83%)] Loss: 12347.406250\n",
      "    epoch          : 1718\n",
      "    loss           : 12258.413059443496\n",
      "    val_loss       : 12255.401069859461\n",
      "    val_log_likelihood: -12164.761067708334\n",
      "    val_log_marginal: -12172.877253716973\n",
      "Train Epoch: 1719 [256/118836 (0%)] Loss: 12210.970703\n",
      "Train Epoch: 1719 [33024/118836 (28%)] Loss: 12295.771484\n",
      "Train Epoch: 1719 [65792/118836 (55%)] Loss: 12244.664062\n",
      "Train Epoch: 1719 [98560/118836 (83%)] Loss: 12300.385742\n",
      "    epoch          : 1719\n",
      "    loss           : 12262.444530603805\n",
      "    val_loss       : 12264.665357146736\n",
      "    val_log_likelihood: -12164.894730278123\n",
      "    val_log_marginal: -12173.072006596305\n",
      "Train Epoch: 1720 [256/118836 (0%)] Loss: 12203.141602\n",
      "Train Epoch: 1720 [33024/118836 (28%)] Loss: 12239.511719\n",
      "Train Epoch: 1720 [65792/118836 (55%)] Loss: 12254.726562\n",
      "Train Epoch: 1720 [98560/118836 (83%)] Loss: 12460.229492\n",
      "    epoch          : 1720\n",
      "    loss           : 12258.972063204354\n",
      "    val_loss       : 12263.360478217362\n",
      "    val_log_likelihood: -12163.062336189516\n",
      "    val_log_marginal: -12171.257477351797\n",
      "Train Epoch: 1721 [256/118836 (0%)] Loss: 12295.179688\n",
      "Train Epoch: 1721 [33024/118836 (28%)] Loss: 12459.107422\n",
      "Train Epoch: 1721 [65792/118836 (55%)] Loss: 12296.136719\n",
      "Train Epoch: 1721 [98560/118836 (83%)] Loss: 12340.033203\n",
      "    epoch          : 1721\n",
      "    loss           : 12262.071932996021\n",
      "    val_loss       : 12259.727541530016\n",
      "    val_log_likelihood: -12161.398161574649\n",
      "    val_log_marginal: -12169.508338317135\n",
      "Train Epoch: 1722 [256/118836 (0%)] Loss: 12345.635742\n",
      "Train Epoch: 1722 [33024/118836 (28%)] Loss: 12245.912109\n",
      "Train Epoch: 1722 [65792/118836 (55%)] Loss: 12282.305664\n",
      "Train Epoch: 1722 [98560/118836 (83%)] Loss: 12319.255859\n",
      "    epoch          : 1722\n",
      "    loss           : 12257.150729231285\n",
      "    val_loss       : 12259.32630837843\n",
      "    val_log_likelihood: -12163.813484316843\n",
      "    val_log_marginal: -12171.966185268715\n",
      "Train Epoch: 1723 [256/118836 (0%)] Loss: 12401.574219\n",
      "Train Epoch: 1723 [33024/118836 (28%)] Loss: 12274.967773\n",
      "Train Epoch: 1723 [65792/118836 (55%)] Loss: 12246.467773\n",
      "Train Epoch: 1723 [98560/118836 (83%)] Loss: 12249.781250\n",
      "    epoch          : 1723\n",
      "    loss           : 12262.528678466191\n",
      "    val_loss       : 12258.240134965841\n",
      "    val_log_likelihood: -12166.738678336953\n",
      "    val_log_marginal: -12174.982866806418\n",
      "Train Epoch: 1724 [256/118836 (0%)] Loss: 12339.363281\n",
      "Train Epoch: 1724 [33024/118836 (28%)] Loss: 12317.552734\n",
      "Train Epoch: 1724 [65792/118836 (55%)] Loss: 12224.253906\n",
      "Train Epoch: 1724 [98560/118836 (83%)] Loss: 12192.740234\n",
      "    epoch          : 1724\n",
      "    loss           : 12260.625698698563\n",
      "    val_loss       : 12258.558364559442\n",
      "    val_log_likelihood: -12162.16400030371\n",
      "    val_log_marginal: -12170.235292683366\n",
      "Train Epoch: 1725 [256/118836 (0%)] Loss: 12167.112305\n",
      "Train Epoch: 1725 [33024/118836 (28%)] Loss: 12209.876953\n",
      "Train Epoch: 1725 [65792/118836 (55%)] Loss: 12218.502930\n",
      "Train Epoch: 1725 [98560/118836 (83%)] Loss: 12175.426758\n",
      "    epoch          : 1725\n",
      "    loss           : 12257.754959548181\n",
      "    val_loss       : 12245.359393127943\n",
      "    val_log_likelihood: -12163.391660527814\n",
      "    val_log_marginal: -12171.426966337693\n",
      "Train Epoch: 1726 [256/118836 (0%)] Loss: 12207.250977\n",
      "Train Epoch: 1726 [33024/118836 (28%)] Loss: 12169.436523\n",
      "Train Epoch: 1726 [65792/118836 (55%)] Loss: 12353.102539\n",
      "Train Epoch: 1726 [98560/118836 (83%)] Loss: 12248.460938\n",
      "    epoch          : 1726\n",
      "    loss           : 12245.724339588503\n",
      "    val_loss       : 12253.468874386084\n",
      "    val_log_likelihood: -12163.87217548077\n",
      "    val_log_marginal: -12172.330835983481\n",
      "Train Epoch: 1727 [256/118836 (0%)] Loss: 12252.259766\n",
      "Train Epoch: 1727 [33024/118836 (28%)] Loss: 12251.961914\n",
      "Train Epoch: 1727 [65792/118836 (55%)] Loss: 12271.636719\n",
      "Train Epoch: 1727 [98560/118836 (83%)] Loss: 12199.535156\n",
      "    epoch          : 1727\n",
      "    loss           : 12246.642347594861\n",
      "    val_loss       : 12241.990749825052\n",
      "    val_log_likelihood: -12162.89501896583\n",
      "    val_log_marginal: -12171.109605489532\n",
      "Train Epoch: 1728 [256/118836 (0%)] Loss: 12302.685547\n",
      "Train Epoch: 1728 [33024/118836 (28%)] Loss: 12238.022461\n",
      "Train Epoch: 1728 [65792/118836 (55%)] Loss: 12236.380859\n",
      "Train Epoch: 1728 [98560/118836 (83%)] Loss: 12253.698242\n",
      "    epoch          : 1728\n",
      "    loss           : 12242.854406728185\n",
      "    val_loss       : 12245.397690961874\n",
      "    val_log_likelihood: -12162.716240016285\n",
      "    val_log_marginal: -12170.757043196296\n",
      "Train Epoch: 1729 [256/118836 (0%)] Loss: 12229.380859\n",
      "Train Epoch: 1729 [33024/118836 (28%)] Loss: 12208.335938\n",
      "Train Epoch: 1729 [65792/118836 (55%)] Loss: 12281.265625\n",
      "Train Epoch: 1729 [98560/118836 (83%)] Loss: 12272.422852\n",
      "    epoch          : 1729\n",
      "    loss           : 12245.647200520832\n",
      "    val_loss       : 12242.820493343665\n",
      "    val_log_likelihood: -12163.627037776572\n",
      "    val_log_marginal: -12171.728866580497\n",
      "Train Epoch: 1730 [256/118836 (0%)] Loss: 12249.898438\n",
      "Train Epoch: 1730 [33024/118836 (28%)] Loss: 12214.949219\n",
      "Train Epoch: 1730 [65792/118836 (55%)] Loss: 12328.402344\n",
      "Train Epoch: 1730 [98560/118836 (83%)] Loss: 12380.099609\n",
      "    epoch          : 1730\n",
      "    loss           : 12247.801653451976\n",
      "    val_loss       : 12242.993239059238\n",
      "    val_log_likelihood: -12164.581333359181\n",
      "    val_log_marginal: -12172.689140989767\n",
      "Train Epoch: 1731 [256/118836 (0%)] Loss: 12286.920898\n",
      "Train Epoch: 1731 [33024/118836 (28%)] Loss: 12243.568359\n",
      "Train Epoch: 1731 [65792/118836 (55%)] Loss: 12291.376953\n",
      "Train Epoch: 1731 [98560/118836 (83%)] Loss: 12342.883789\n",
      "    epoch          : 1731\n",
      "    loss           : 12242.64893329327\n",
      "    val_loss       : 12241.240202823612\n",
      "    val_log_likelihood: -12162.248373365126\n",
      "    val_log_marginal: -12170.289793937683\n",
      "Train Epoch: 1732 [256/118836 (0%)] Loss: 12260.755859\n",
      "Train Epoch: 1732 [33024/118836 (28%)] Loss: 12211.735352\n",
      "Train Epoch: 1732 [65792/118836 (55%)] Loss: 12254.449219\n",
      "Train Epoch: 1732 [98560/118836 (83%)] Loss: 12271.201172\n",
      "    epoch          : 1732\n",
      "    loss           : 12249.257335446393\n",
      "    val_loss       : 12248.236572216607\n",
      "    val_log_likelihood: -12161.932339162015\n",
      "    val_log_marginal: -12170.353335274895\n",
      "Train Epoch: 1733 [256/118836 (0%)] Loss: 12424.146484\n",
      "Train Epoch: 1733 [33024/118836 (28%)] Loss: 12210.679688\n",
      "Train Epoch: 1733 [65792/118836 (55%)] Loss: 12344.543945\n",
      "Train Epoch: 1733 [98560/118836 (83%)] Loss: 12303.298828\n",
      "    epoch          : 1733\n",
      "    loss           : 12243.084561750413\n",
      "    val_loss       : 12242.1470245611\n",
      "    val_log_likelihood: -12161.760497602616\n",
      "    val_log_marginal: -12169.945129869975\n",
      "Train Epoch: 1734 [256/118836 (0%)] Loss: 12217.531250\n",
      "Train Epoch: 1734 [33024/118836 (28%)] Loss: 12203.025391\n",
      "Train Epoch: 1734 [65792/118836 (55%)] Loss: 12260.937500\n",
      "Train Epoch: 1734 [98560/118836 (83%)] Loss: 12227.842773\n",
      "    epoch          : 1734\n",
      "    loss           : 12247.675520348686\n",
      "    val_loss       : 12242.530282736023\n",
      "    val_log_likelihood: -12162.147325236507\n",
      "    val_log_marginal: -12170.34411366115\n",
      "Train Epoch: 1735 [256/118836 (0%)] Loss: 12304.785156\n",
      "Train Epoch: 1735 [33024/118836 (28%)] Loss: 12271.335938\n",
      "Train Epoch: 1735 [65792/118836 (55%)] Loss: 12230.094727\n",
      "Train Epoch: 1735 [98560/118836 (83%)] Loss: 12185.424805\n",
      "    epoch          : 1735\n",
      "    loss           : 12243.265792526106\n",
      "    val_loss       : 12245.039326626891\n",
      "    val_log_likelihood: -12163.34777611921\n",
      "    val_log_marginal: -12171.418583309689\n",
      "Train Epoch: 1736 [256/118836 (0%)] Loss: 12294.098633\n",
      "Train Epoch: 1736 [33024/118836 (28%)] Loss: 12301.562500\n",
      "Train Epoch: 1736 [65792/118836 (55%)] Loss: 12286.557617\n",
      "Train Epoch: 1736 [98560/118836 (83%)] Loss: 12291.009766\n",
      "    epoch          : 1736\n",
      "    loss           : 12243.101225993849\n",
      "    val_loss       : 12242.735194565888\n",
      "    val_log_likelihood: -12162.911268028845\n",
      "    val_log_marginal: -12171.028890487783\n",
      "Train Epoch: 1737 [256/118836 (0%)] Loss: 12264.019531\n",
      "Train Epoch: 1737 [33024/118836 (28%)] Loss: 12296.955078\n",
      "Train Epoch: 1737 [65792/118836 (55%)] Loss: 12231.399414\n",
      "Train Epoch: 1737 [98560/118836 (83%)] Loss: 12317.416016\n",
      "    epoch          : 1737\n",
      "    loss           : 12246.380324002275\n",
      "    val_loss       : 12246.582743498027\n",
      "    val_log_likelihood: -12161.505852428401\n",
      "    val_log_marginal: -12169.565517033794\n",
      "Train Epoch: 1738 [256/118836 (0%)] Loss: 12147.816406\n",
      "Train Epoch: 1738 [33024/118836 (28%)] Loss: 12300.052734\n",
      "Train Epoch: 1738 [65792/118836 (55%)] Loss: 12304.658203\n",
      "Train Epoch: 1738 [98560/118836 (83%)] Loss: 12230.375000\n",
      "    epoch          : 1738\n",
      "    loss           : 12245.564479619004\n",
      "    val_loss       : 12243.308152854232\n",
      "    val_log_likelihood: -12160.440967321909\n",
      "    val_log_marginal: -12168.478041327895\n",
      "Train Epoch: 1739 [256/118836 (0%)] Loss: 12233.380859\n",
      "Train Epoch: 1739 [33024/118836 (28%)] Loss: 12240.344727\n",
      "Train Epoch: 1739 [65792/118836 (55%)] Loss: 12265.156250\n",
      "Train Epoch: 1739 [98560/118836 (83%)] Loss: 12279.828125\n",
      "    epoch          : 1739\n",
      "    loss           : 12248.92938023418\n",
      "    val_loss       : 12248.00710239141\n",
      "    val_log_likelihood: -12162.285264972343\n",
      "    val_log_marginal: -12170.369681773287\n",
      "Train Epoch: 1740 [256/118836 (0%)] Loss: 12201.021484\n",
      "Train Epoch: 1740 [33024/118836 (28%)] Loss: 12226.759766\n",
      "Train Epoch: 1740 [65792/118836 (55%)] Loss: 12289.515625\n",
      "Train Epoch: 1740 [98560/118836 (83%)] Loss: 12233.127930\n",
      "    epoch          : 1740\n",
      "    loss           : 12244.664615319996\n",
      "    val_loss       : 12243.909167610478\n",
      "    val_log_likelihood: -12157.969108315241\n",
      "    val_log_marginal: -12165.956020483283\n",
      "Train Epoch: 1741 [256/118836 (0%)] Loss: 12251.638672\n",
      "Train Epoch: 1741 [33024/118836 (28%)] Loss: 12228.154297\n",
      "Train Epoch: 1741 [65792/118836 (55%)] Loss: 12267.828125\n",
      "Train Epoch: 1741 [98560/118836 (83%)] Loss: 12241.492188\n",
      "    epoch          : 1741\n",
      "    loss           : 12242.54887093543\n",
      "    val_loss       : 12243.49051897232\n",
      "    val_log_likelihood: -12162.76568444996\n",
      "    val_log_marginal: -12170.779425684519\n",
      "Train Epoch: 1742 [256/118836 (0%)] Loss: 12227.447266\n",
      "Train Epoch: 1742 [33024/118836 (28%)] Loss: 12294.542969\n",
      "Train Epoch: 1742 [65792/118836 (55%)] Loss: 12303.451172\n",
      "Train Epoch: 1742 [98560/118836 (83%)] Loss: 12221.367188\n",
      "    epoch          : 1742\n",
      "    loss           : 12242.82931981493\n",
      "    val_loss       : 12244.987193304134\n",
      "    val_log_likelihood: -12162.541518364867\n",
      "    val_log_marginal: -12170.684006176354\n",
      "Train Epoch: 1743 [256/118836 (0%)] Loss: 12255.777344\n",
      "Train Epoch: 1743 [33024/118836 (28%)] Loss: 12288.351562\n",
      "Train Epoch: 1743 [65792/118836 (55%)] Loss: 12190.953125\n",
      "Train Epoch: 1743 [98560/118836 (83%)] Loss: 12243.345703\n",
      "    epoch          : 1743\n",
      "    loss           : 12245.734507146919\n",
      "    val_loss       : 12247.753645659479\n",
      "    val_log_likelihood: -12159.408249166407\n",
      "    val_log_marginal: -12167.830873735606\n",
      "Train Epoch: 1744 [256/118836 (0%)] Loss: 12288.057617\n",
      "Train Epoch: 1744 [33024/118836 (28%)] Loss: 12337.185547\n",
      "Train Epoch: 1744 [65792/118836 (55%)] Loss: 12263.022461\n",
      "Train Epoch: 1744 [98560/118836 (83%)] Loss: 12215.531250\n",
      "    epoch          : 1744\n",
      "    loss           : 12247.33303882987\n",
      "    val_loss       : 12245.917727351738\n",
      "    val_log_likelihood: -12163.88663280604\n",
      "    val_log_marginal: -12172.354374195585\n",
      "Train Epoch: 1745 [256/118836 (0%)] Loss: 12215.094727\n",
      "Train Epoch: 1745 [33024/118836 (28%)] Loss: 12250.750977\n",
      "Train Epoch: 1745 [65792/118836 (55%)] Loss: 12315.994141\n",
      "Train Epoch: 1745 [98560/118836 (83%)] Loss: 12218.271484\n",
      "    epoch          : 1745\n",
      "    loss           : 12248.679112870916\n",
      "    val_loss       : 12248.079928734778\n",
      "    val_log_likelihood: -12162.479894767112\n",
      "    val_log_marginal: -12170.670583484281\n",
      "Train Epoch: 1746 [256/118836 (0%)] Loss: 12292.308594\n",
      "Train Epoch: 1746 [33024/118836 (28%)] Loss: 12185.409180\n",
      "Train Epoch: 1746 [65792/118836 (55%)] Loss: 12213.673828\n",
      "Train Epoch: 1746 [98560/118836 (83%)] Loss: 12321.930664\n",
      "    epoch          : 1746\n",
      "    loss           : 12245.438474785462\n",
      "    val_loss       : 12246.127722318859\n",
      "    val_log_likelihood: -12162.176641658913\n",
      "    val_log_marginal: -12170.601468262008\n",
      "Train Epoch: 1747 [256/118836 (0%)] Loss: 12244.672852\n",
      "Train Epoch: 1747 [33024/118836 (28%)] Loss: 12260.141602\n",
      "Train Epoch: 1747 [65792/118836 (55%)] Loss: 12296.963867\n",
      "Train Epoch: 1747 [98560/118836 (83%)] Loss: 12304.609375\n",
      "    epoch          : 1747\n",
      "    loss           : 12242.967473764475\n",
      "    val_loss       : 12244.043621272229\n",
      "    val_log_likelihood: -12163.266725955076\n",
      "    val_log_marginal: -12171.440774626175\n",
      "Train Epoch: 1748 [256/118836 (0%)] Loss: 12249.914062\n",
      "Train Epoch: 1748 [33024/118836 (28%)] Loss: 12259.077148\n",
      "Train Epoch: 1748 [65792/118836 (55%)] Loss: 12241.728516\n",
      "Train Epoch: 1748 [98560/118836 (83%)] Loss: 12287.832031\n",
      "    epoch          : 1748\n",
      "    loss           : 12244.303495916047\n",
      "    val_loss       : 12246.166102802696\n",
      "    val_log_likelihood: -12161.564584625725\n",
      "    val_log_marginal: -12169.871667416028\n",
      "Train Epoch: 1749 [256/118836 (0%)] Loss: 12306.618164\n",
      "Train Epoch: 1749 [33024/118836 (28%)] Loss: 12227.773438\n",
      "Train Epoch: 1749 [65792/118836 (55%)] Loss: 12266.199219\n",
      "Train Epoch: 1749 [98560/118836 (83%)] Loss: 12215.377930\n",
      "    epoch          : 1749\n",
      "    loss           : 12245.78505786678\n",
      "    val_loss       : 12247.021888761561\n",
      "    val_log_likelihood: -12162.19942343233\n",
      "    val_log_marginal: -12170.485589023854\n",
      "Train Epoch: 1750 [256/118836 (0%)] Loss: 12206.537109\n",
      "Train Epoch: 1750 [33024/118836 (28%)] Loss: 12319.941406\n",
      "Train Epoch: 1750 [65792/118836 (55%)] Loss: 12208.179688\n",
      "Train Epoch: 1750 [98560/118836 (83%)] Loss: 12279.555664\n",
      "    epoch          : 1750\n",
      "    loss           : 12243.290105459058\n",
      "    val_loss       : 12244.136241338778\n",
      "    val_log_likelihood: -12163.421890024038\n",
      "    val_log_marginal: -12171.670666364578\n",
      "Train Epoch: 1751 [256/118836 (0%)] Loss: 12171.782227\n",
      "Train Epoch: 1751 [33024/118836 (28%)] Loss: 12235.401367\n",
      "Train Epoch: 1751 [65792/118836 (55%)] Loss: 12326.744141\n",
      "Train Epoch: 1751 [98560/118836 (83%)] Loss: 12268.294922\n",
      "    epoch          : 1751\n",
      "    loss           : 12245.864115326456\n",
      "    val_loss       : 12249.189017438077\n",
      "    val_log_likelihood: -12159.76543938043\n",
      "    val_log_marginal: -12167.94546314\n",
      "Train Epoch: 1752 [256/118836 (0%)] Loss: 12254.880859\n",
      "Train Epoch: 1752 [33024/118836 (28%)] Loss: 12262.220703\n",
      "Train Epoch: 1752 [65792/118836 (55%)] Loss: 12306.549805\n",
      "Train Epoch: 1752 [98560/118836 (83%)] Loss: 12335.520508\n",
      "    epoch          : 1752\n",
      "    loss           : 12247.872446075009\n",
      "    val_loss       : 12246.456279446686\n",
      "    val_log_likelihood: -12161.89984103598\n",
      "    val_log_marginal: -12170.07031085014\n",
      "Train Epoch: 1753 [256/118836 (0%)] Loss: 12263.606445\n",
      "Train Epoch: 1753 [33024/118836 (28%)] Loss: 12317.880859\n",
      "Train Epoch: 1753 [65792/118836 (55%)] Loss: 12192.313477\n",
      "Train Epoch: 1753 [98560/118836 (83%)] Loss: 12306.483398\n",
      "    epoch          : 1753\n",
      "    loss           : 12241.710238962985\n",
      "    val_loss       : 12243.850861852077\n",
      "    val_log_likelihood: -12161.856957583746\n",
      "    val_log_marginal: -12170.18926526437\n",
      "Train Epoch: 1754 [256/118836 (0%)] Loss: 12201.400391\n",
      "Train Epoch: 1754 [33024/118836 (28%)] Loss: 12320.923828\n",
      "Train Epoch: 1754 [65792/118836 (55%)] Loss: 12245.010742\n",
      "Train Epoch: 1754 [98560/118836 (83%)] Loss: 12227.986328\n",
      "    epoch          : 1754\n",
      "    loss           : 12241.447427012252\n",
      "    val_loss       : 12245.935577852375\n",
      "    val_log_likelihood: -12161.599473835557\n",
      "    val_log_marginal: -12169.75718396864\n",
      "Train Epoch: 1755 [256/118836 (0%)] Loss: 12247.492188\n",
      "Train Epoch: 1755 [33024/118836 (28%)] Loss: 12276.373047\n",
      "Train Epoch: 1755 [65792/118836 (55%)] Loss: 12244.317383\n",
      "Train Epoch: 1755 [98560/118836 (83%)] Loss: 12207.712891\n",
      "    epoch          : 1755\n",
      "    loss           : 12238.502861837003\n",
      "    val_loss       : 12244.757442390599\n",
      "    val_log_likelihood: -12162.36942478934\n",
      "    val_log_marginal: -12170.578585735338\n",
      "Train Epoch: 1756 [256/118836 (0%)] Loss: 12191.156250\n",
      "Train Epoch: 1756 [33024/118836 (28%)] Loss: 12296.666992\n",
      "Train Epoch: 1756 [65792/118836 (55%)] Loss: 12278.672852\n",
      "Train Epoch: 1756 [98560/118836 (83%)] Loss: 12234.266602\n",
      "    epoch          : 1756\n",
      "    loss           : 12250.325152663616\n",
      "    val_loss       : 12242.560160787243\n",
      "    val_log_likelihood: -12160.48594024633\n",
      "    val_log_marginal: -12168.62526025678\n",
      "Train Epoch: 1757 [256/118836 (0%)] Loss: 12239.341797\n",
      "Train Epoch: 1757 [33024/118836 (28%)] Loss: 12184.486328\n",
      "Train Epoch: 1757 [65792/118836 (55%)] Loss: 12256.312500\n",
      "Train Epoch: 1757 [98560/118836 (83%)] Loss: 12352.067383\n",
      "    epoch          : 1757\n",
      "    loss           : 12245.950913396919\n",
      "    val_loss       : 12242.485082099527\n",
      "    val_log_likelihood: -12164.1304217393\n",
      "    val_log_marginal: -12172.356221922997\n",
      "Train Epoch: 1758 [256/118836 (0%)] Loss: 12231.239258\n",
      "Train Epoch: 1758 [33024/118836 (28%)] Loss: 12306.451172\n",
      "Train Epoch: 1758 [65792/118836 (55%)] Loss: 12311.796875\n",
      "Train Epoch: 1758 [98560/118836 (83%)] Loss: 12248.694336\n",
      "    epoch          : 1758\n",
      "    loss           : 12252.110209238006\n",
      "    val_loss       : 12243.976865090095\n",
      "    val_log_likelihood: -12162.287850883995\n",
      "    val_log_marginal: -12170.309388599087\n",
      "Train Epoch: 1759 [256/118836 (0%)] Loss: 12252.384766\n",
      "Train Epoch: 1759 [33024/118836 (28%)] Loss: 12257.187500\n",
      "Train Epoch: 1759 [65792/118836 (55%)] Loss: 12206.574219\n",
      "Train Epoch: 1759 [98560/118836 (83%)] Loss: 12252.486328\n",
      "    epoch          : 1759\n",
      "    loss           : 12245.69873119572\n",
      "    val_loss       : 12244.155455027416\n",
      "    val_log_likelihood: -12161.298682084884\n",
      "    val_log_marginal: -12169.38542160083\n",
      "Train Epoch: 1760 [256/118836 (0%)] Loss: 12244.619141\n",
      "Train Epoch: 1760 [33024/118836 (28%)] Loss: 12235.208008\n",
      "Train Epoch: 1760 [65792/118836 (55%)] Loss: 12378.775391\n",
      "Train Epoch: 1760 [98560/118836 (83%)] Loss: 12227.924805\n",
      "    epoch          : 1760\n",
      "    loss           : 12245.93798626189\n",
      "    val_loss       : 12247.549477508695\n",
      "    val_log_likelihood: -12161.172389856027\n",
      "    val_log_marginal: -12169.286675286867\n",
      "Train Epoch: 1761 [256/118836 (0%)] Loss: 12204.201172\n",
      "Train Epoch: 1761 [33024/118836 (28%)] Loss: 12183.972656\n",
      "Train Epoch: 1761 [65792/118836 (55%)] Loss: 12227.560547\n",
      "Train Epoch: 1761 [98560/118836 (83%)] Loss: 12229.448242\n",
      "    epoch          : 1761\n",
      "    loss           : 12242.62743938689\n",
      "    val_loss       : 12247.909761608107\n",
      "    val_log_likelihood: -12161.459155455179\n",
      "    val_log_marginal: -12169.530684803392\n",
      "Train Epoch: 1762 [256/118836 (0%)] Loss: 12303.388672\n",
      "Train Epoch: 1762 [33024/118836 (28%)] Loss: 12207.956055\n",
      "Train Epoch: 1762 [65792/118836 (55%)] Loss: 12413.164062\n",
      "Train Epoch: 1762 [98560/118836 (83%)] Loss: 12340.164062\n",
      "    epoch          : 1762\n",
      "    loss           : 12245.831133684864\n",
      "    val_loss       : 12241.180100892936\n",
      "    val_log_likelihood: -12164.472896311518\n",
      "    val_log_marginal: -12172.563391640255\n",
      "Train Epoch: 1763 [256/118836 (0%)] Loss: 12303.793945\n",
      "Train Epoch: 1763 [33024/118836 (28%)] Loss: 12193.501953\n",
      "Train Epoch: 1763 [65792/118836 (55%)] Loss: 12410.168945\n",
      "Train Epoch: 1763 [98560/118836 (83%)] Loss: 12338.712891\n",
      "    epoch          : 1763\n",
      "    loss           : 12246.19593203965\n",
      "    val_loss       : 12245.70867241431\n",
      "    val_log_likelihood: -12161.568682472602\n",
      "    val_log_marginal: -12169.711844369655\n",
      "Train Epoch: 1764 [256/118836 (0%)] Loss: 12319.903320\n",
      "Train Epoch: 1764 [33024/118836 (28%)] Loss: 12351.829102\n",
      "Train Epoch: 1764 [65792/118836 (55%)] Loss: 12295.357422\n",
      "Train Epoch: 1764 [98560/118836 (83%)] Loss: 12332.132812\n",
      "    epoch          : 1764\n",
      "    loss           : 12246.51704226763\n",
      "    val_loss       : 12244.26382552687\n",
      "    val_log_likelihood: -12162.094754025797\n",
      "    val_log_marginal: -12170.461525890483\n",
      "Train Epoch: 1765 [256/118836 (0%)] Loss: 12315.794922\n",
      "Train Epoch: 1765 [33024/118836 (28%)] Loss: 12395.618164\n",
      "Train Epoch: 1765 [65792/118836 (55%)] Loss: 12327.177734\n",
      "Train Epoch: 1765 [98560/118836 (83%)] Loss: 12202.509766\n",
      "    epoch          : 1765\n",
      "    loss           : 12246.952760869004\n",
      "    val_loss       : 12247.142138419198\n",
      "    val_log_likelihood: -12162.997197128308\n",
      "    val_log_marginal: -12171.265657840882\n",
      "Train Epoch: 1766 [256/118836 (0%)] Loss: 12272.020508\n",
      "Train Epoch: 1766 [33024/118836 (28%)] Loss: 12224.716797\n",
      "Train Epoch: 1766 [65792/118836 (55%)] Loss: 12232.152344\n",
      "Train Epoch: 1766 [98560/118836 (83%)] Loss: 12228.058594\n",
      "    epoch          : 1766\n",
      "    loss           : 12244.085344131256\n",
      "    val_loss       : 12247.842782636122\n",
      "    val_log_likelihood: -12162.731023185484\n",
      "    val_log_marginal: -12170.846820573863\n",
      "Train Epoch: 1767 [256/118836 (0%)] Loss: 12247.621094\n",
      "Train Epoch: 1767 [33024/118836 (28%)] Loss: 12186.832031\n",
      "Train Epoch: 1767 [65792/118836 (55%)] Loss: 12221.699219\n",
      "Train Epoch: 1767 [98560/118836 (83%)] Loss: 12271.860352\n",
      "    epoch          : 1767\n",
      "    loss           : 12245.934854638388\n",
      "    val_loss       : 12245.858879581283\n",
      "    val_log_likelihood: -12163.863112269955\n",
      "    val_log_marginal: -12172.014618537916\n",
      "Train Epoch: 1768 [256/118836 (0%)] Loss: 12287.674805\n",
      "Train Epoch: 1768 [33024/118836 (28%)] Loss: 12186.974609\n",
      "Train Epoch: 1768 [65792/118836 (55%)] Loss: 12281.362305\n",
      "Train Epoch: 1768 [98560/118836 (83%)] Loss: 12306.684570\n",
      "    epoch          : 1768\n",
      "    loss           : 12246.090039708695\n",
      "    val_loss       : 12247.49736713859\n",
      "    val_log_likelihood: -12159.085888389165\n",
      "    val_log_marginal: -12167.20655630185\n",
      "Train Epoch: 1769 [256/118836 (0%)] Loss: 12236.210938\n",
      "Train Epoch: 1769 [33024/118836 (28%)] Loss: 12159.807617\n",
      "Train Epoch: 1769 [65792/118836 (55%)] Loss: 12301.673828\n",
      "Train Epoch: 1769 [98560/118836 (83%)] Loss: 12268.872070\n",
      "    epoch          : 1769\n",
      "    loss           : 12243.655255990228\n",
      "    val_loss       : 12245.838497205023\n",
      "    val_log_likelihood: -12162.575770749329\n",
      "    val_log_marginal: -12170.935139909801\n",
      "Train Epoch: 1770 [256/118836 (0%)] Loss: 12245.423828\n",
      "Train Epoch: 1770 [33024/118836 (28%)] Loss: 12242.288086\n",
      "Train Epoch: 1770 [65792/118836 (55%)] Loss: 12269.213867\n",
      "Train Epoch: 1770 [98560/118836 (83%)] Loss: 12160.310547\n",
      "    epoch          : 1770\n",
      "    loss           : 12245.100074473998\n",
      "    val_loss       : 12249.338395537514\n",
      "    val_log_likelihood: -12160.917743389422\n",
      "    val_log_marginal: -12169.123479563576\n",
      "Train Epoch: 1771 [256/118836 (0%)] Loss: 12278.636719\n",
      "Train Epoch: 1771 [33024/118836 (28%)] Loss: 12183.855469\n",
      "Train Epoch: 1771 [65792/118836 (55%)] Loss: 12256.564453\n",
      "Train Epoch: 1771 [98560/118836 (83%)] Loss: 12242.060547\n",
      "    epoch          : 1771\n",
      "    loss           : 12245.99097119908\n",
      "    val_loss       : 12246.08749922311\n",
      "    val_log_likelihood: -12164.461639429539\n",
      "    val_log_marginal: -12172.568540637461\n",
      "Train Epoch: 1772 [256/118836 (0%)] Loss: 12220.265625\n",
      "Train Epoch: 1772 [33024/118836 (28%)] Loss: 12279.082031\n",
      "Train Epoch: 1772 [65792/118836 (55%)] Loss: 12196.992188\n",
      "Train Epoch: 1772 [98560/118836 (83%)] Loss: 12200.805664\n",
      "    epoch          : 1772\n",
      "    loss           : 12240.951735680314\n",
      "    val_loss       : 12244.289736538249\n",
      "    val_log_likelihood: -12161.065058933002\n",
      "    val_log_marginal: -12169.180693016799\n",
      "Train Epoch: 1773 [256/118836 (0%)] Loss: 12371.783203\n",
      "Train Epoch: 1773 [33024/118836 (28%)] Loss: 12258.000977\n",
      "Train Epoch: 1773 [65792/118836 (55%)] Loss: 12249.663086\n",
      "Train Epoch: 1773 [98560/118836 (83%)] Loss: 12268.005859\n",
      "    epoch          : 1773\n",
      "    loss           : 12246.440010145265\n",
      "    val_loss       : 12242.843307515679\n",
      "    val_log_likelihood: -12162.685744610731\n",
      "    val_log_marginal: -12170.802012892453\n",
      "Train Epoch: 1774 [256/118836 (0%)] Loss: 12362.310547\n",
      "Train Epoch: 1774 [33024/118836 (28%)] Loss: 12296.875977\n",
      "Train Epoch: 1774 [65792/118836 (55%)] Loss: 12255.025391\n",
      "Train Epoch: 1774 [98560/118836 (83%)] Loss: 12298.754883\n",
      "    epoch          : 1774\n",
      "    loss           : 12251.054398812294\n",
      "    val_loss       : 12248.419304272094\n",
      "    val_log_likelihood: -12162.724334580489\n",
      "    val_log_marginal: -12170.853487297683\n",
      "Train Epoch: 1775 [256/118836 (0%)] Loss: 12282.917969\n",
      "Train Epoch: 1775 [33024/118836 (28%)] Loss: 12272.971680\n",
      "Train Epoch: 1775 [65792/118836 (55%)] Loss: 12192.890625\n",
      "Train Epoch: 1775 [98560/118836 (83%)] Loss: 12197.098633\n",
      "    epoch          : 1775\n",
      "    loss           : 12242.67354654544\n",
      "    val_loss       : 12245.5827242229\n",
      "    val_log_likelihood: -12160.03373898237\n",
      "    val_log_marginal: -12168.146773170925\n",
      "Train Epoch: 1776 [256/118836 (0%)] Loss: 12185.563477\n",
      "Train Epoch: 1776 [33024/118836 (28%)] Loss: 12233.093750\n",
      "Train Epoch: 1776 [65792/118836 (55%)] Loss: 12247.169922\n",
      "Train Epoch: 1776 [98560/118836 (83%)] Loss: 12232.162109\n",
      "    epoch          : 1776\n",
      "    loss           : 12247.594566629188\n",
      "    val_loss       : 12241.675760506683\n",
      "    val_log_likelihood: -12160.995532691017\n",
      "    val_log_marginal: -12169.099874522795\n",
      "Train Epoch: 1777 [256/118836 (0%)] Loss: 12206.671875\n",
      "Train Epoch: 1777 [33024/118836 (28%)] Loss: 12216.931641\n",
      "Train Epoch: 1777 [65792/118836 (55%)] Loss: 12350.839844\n",
      "Train Epoch: 1777 [98560/118836 (83%)] Loss: 12398.429688\n",
      "    epoch          : 1777\n",
      "    loss           : 12246.956958068393\n",
      "    val_loss       : 12247.079230967054\n",
      "    val_log_likelihood: -12161.83852648108\n",
      "    val_log_marginal: -12170.094118170382\n",
      "Train Epoch: 1778 [256/118836 (0%)] Loss: 12212.182617\n",
      "Train Epoch: 1778 [33024/118836 (28%)] Loss: 12251.959961\n",
      "Train Epoch: 1778 [65792/118836 (55%)] Loss: 12311.264648\n",
      "Train Epoch: 1778 [98560/118836 (83%)] Loss: 12313.271484\n",
      "    epoch          : 1778\n",
      "    loss           : 12249.880574726012\n",
      "    val_loss       : 12244.632065682197\n",
      "    val_log_likelihood: -12159.971471289547\n",
      "    val_log_marginal: -12168.089664381776\n",
      "Train Epoch: 1779 [256/118836 (0%)] Loss: 12215.588867\n",
      "Train Epoch: 1779 [33024/118836 (28%)] Loss: 12200.258789\n",
      "Train Epoch: 1779 [65792/118836 (55%)] Loss: 12310.625000\n",
      "Train Epoch: 1779 [98560/118836 (83%)] Loss: 12332.279297\n",
      "    epoch          : 1779\n",
      "    loss           : 12247.30646679849\n",
      "    val_loss       : 12247.016735516916\n",
      "    val_log_likelihood: -12162.25991118047\n",
      "    val_log_marginal: -12170.335690552565\n",
      "Train Epoch: 1780 [256/118836 (0%)] Loss: 12175.432617\n",
      "Train Epoch: 1780 [33024/118836 (28%)] Loss: 12344.444336\n",
      "Train Epoch: 1780 [65792/118836 (55%)] Loss: 12256.380859\n",
      "Train Epoch: 1780 [98560/118836 (83%)] Loss: 12231.224609\n",
      "    epoch          : 1780\n",
      "    loss           : 12241.362249760909\n",
      "    val_loss       : 12251.74486538786\n",
      "    val_log_likelihood: -12161.763206129808\n",
      "    val_log_marginal: -12169.89619968061\n",
      "Train Epoch: 1781 [256/118836 (0%)] Loss: 12275.465820\n",
      "Train Epoch: 1781 [33024/118836 (28%)] Loss: 12271.642578\n",
      "Train Epoch: 1781 [65792/118836 (55%)] Loss: 12240.167969\n",
      "Train Epoch: 1781 [98560/118836 (83%)] Loss: 12230.960938\n",
      "    epoch          : 1781\n",
      "    loss           : 12247.514150544095\n",
      "    val_loss       : 12243.225685507425\n",
      "    val_log_likelihood: -12161.811723434916\n",
      "    val_log_marginal: -12169.965792853349\n",
      "Train Epoch: 1782 [256/118836 (0%)] Loss: 12241.416016\n",
      "Train Epoch: 1782 [33024/118836 (28%)] Loss: 12338.861328\n",
      "Train Epoch: 1782 [65792/118836 (55%)] Loss: 12394.857422\n",
      "Train Epoch: 1782 [98560/118836 (83%)] Loss: 12216.070312\n",
      "    epoch          : 1782\n",
      "    loss           : 12246.103614654414\n",
      "    val_loss       : 12247.636758050334\n",
      "    val_log_likelihood: -12162.808738174628\n",
      "    val_log_marginal: -12170.857980786184\n",
      "Train Epoch: 1783 [256/118836 (0%)] Loss: 12186.676758\n",
      "Train Epoch: 1783 [33024/118836 (28%)] Loss: 12237.060547\n",
      "Train Epoch: 1783 [65792/118836 (55%)] Loss: 12257.331055\n",
      "Train Epoch: 1783 [98560/118836 (83%)] Loss: 12223.962891\n",
      "    epoch          : 1783\n",
      "    loss           : 12245.22379193807\n",
      "    val_loss       : 12244.435984065685\n",
      "    val_log_likelihood: -12164.209347698252\n",
      "    val_log_marginal: -12172.405010304032\n",
      "Train Epoch: 1784 [256/118836 (0%)] Loss: 12260.429688\n",
      "Train Epoch: 1784 [33024/118836 (28%)] Loss: 12363.829102\n",
      "Train Epoch: 1784 [65792/118836 (55%)] Loss: 12207.669922\n",
      "Train Epoch: 1784 [98560/118836 (83%)] Loss: 12245.501953\n",
      "    epoch          : 1784\n",
      "    loss           : 12247.558080186362\n",
      "    val_loss       : 12243.709331546379\n",
      "    val_log_likelihood: -12161.976421629444\n",
      "    val_log_marginal: -12170.176365182828\n",
      "Train Epoch: 1785 [256/118836 (0%)] Loss: 12249.466797\n",
      "Train Epoch: 1785 [33024/118836 (28%)] Loss: 12267.801758\n",
      "Train Epoch: 1785 [65792/118836 (55%)] Loss: 12387.500000\n",
      "Train Epoch: 1785 [98560/118836 (83%)] Loss: 12159.178711\n",
      "    epoch          : 1785\n",
      "    loss           : 12243.182849171577\n",
      "    val_loss       : 12242.614585500762\n",
      "    val_log_likelihood: -12161.248096308933\n",
      "    val_log_marginal: -12169.383612776732\n",
      "Train Epoch: 1786 [256/118836 (0%)] Loss: 12263.236328\n",
      "Train Epoch: 1786 [33024/118836 (28%)] Loss: 12294.759766\n",
      "Train Epoch: 1786 [65792/118836 (55%)] Loss: 12323.031250\n",
      "Train Epoch: 1786 [98560/118836 (83%)] Loss: 12400.606445\n",
      "    epoch          : 1786\n",
      "    loss           : 12248.614222110215\n",
      "    val_loss       : 12247.79201933132\n",
      "    val_log_likelihood: -12162.094935606649\n",
      "    val_log_marginal: -12170.496934341016\n",
      "Train Epoch: 1787 [256/118836 (0%)] Loss: 12279.144531\n",
      "Train Epoch: 1787 [33024/118836 (28%)] Loss: 12248.511719\n",
      "Train Epoch: 1787 [65792/118836 (55%)] Loss: 12186.425781\n",
      "Train Epoch: 1787 [98560/118836 (83%)] Loss: 12239.222656\n",
      "    epoch          : 1787\n",
      "    loss           : 12246.526045866936\n",
      "    val_loss       : 12246.783437992512\n",
      "    val_log_likelihood: -12160.994827853598\n",
      "    val_log_marginal: -12169.048002139882\n",
      "Train Epoch: 1788 [256/118836 (0%)] Loss: 12257.278320\n",
      "Train Epoch: 1788 [33024/118836 (28%)] Loss: 12276.412109\n",
      "Train Epoch: 1788 [65792/118836 (55%)] Loss: 12328.158203\n",
      "Train Epoch: 1788 [98560/118836 (83%)] Loss: 12273.081055\n",
      "    epoch          : 1788\n",
      "    loss           : 12248.01877293993\n",
      "    val_loss       : 12242.787927984453\n",
      "    val_log_likelihood: -12164.00065718052\n",
      "    val_log_marginal: -12172.430564158994\n",
      "Train Epoch: 1789 [256/118836 (0%)] Loss: 12267.561523\n",
      "Train Epoch: 1789 [33024/118836 (28%)] Loss: 12316.072266\n",
      "Train Epoch: 1789 [65792/118836 (55%)] Loss: 12365.207031\n",
      "Train Epoch: 1789 [98560/118836 (83%)] Loss: 12245.762695\n",
      "    epoch          : 1789\n",
      "    loss           : 12244.881837714538\n",
      "    val_loss       : 12250.898087336174\n",
      "    val_log_likelihood: -12161.649070125104\n",
      "    val_log_marginal: -12169.774300290072\n",
      "Train Epoch: 1790 [256/118836 (0%)] Loss: 12216.674805\n",
      "Train Epoch: 1790 [33024/118836 (28%)] Loss: 12244.382812\n",
      "Train Epoch: 1790 [65792/118836 (55%)] Loss: 12215.029297\n",
      "Train Epoch: 1790 [98560/118836 (83%)] Loss: 12224.613281\n",
      "    epoch          : 1790\n",
      "    loss           : 12242.884913119055\n",
      "    val_loss       : 12245.386833152595\n",
      "    val_log_likelihood: -12160.572817314154\n",
      "    val_log_marginal: -12168.750455681235\n",
      "Train Epoch: 1791 [256/118836 (0%)] Loss: 12324.165039\n",
      "Train Epoch: 1791 [33024/118836 (28%)] Loss: 12221.955078\n",
      "Train Epoch: 1791 [65792/118836 (55%)] Loss: 12228.242188\n",
      "Train Epoch: 1791 [98560/118836 (83%)] Loss: 12284.774414\n",
      "    epoch          : 1791\n",
      "    loss           : 12245.498695493436\n",
      "    val_loss       : 12245.795356266997\n",
      "    val_log_likelihood: -12162.032044173904\n",
      "    val_log_marginal: -12170.161980324492\n",
      "Train Epoch: 1792 [256/118836 (0%)] Loss: 12203.154297\n",
      "Train Epoch: 1792 [33024/118836 (28%)] Loss: 12293.488281\n",
      "Train Epoch: 1792 [65792/118836 (55%)] Loss: 12292.764648\n",
      "Train Epoch: 1792 [98560/118836 (83%)] Loss: 12189.013672\n",
      "    epoch          : 1792\n",
      "    loss           : 12246.353397694376\n",
      "    val_loss       : 12246.690861230372\n",
      "    val_log_likelihood: -12162.533540277347\n",
      "    val_log_marginal: -12170.55859648826\n",
      "Train Epoch: 1793 [256/118836 (0%)] Loss: 12295.478516\n",
      "Train Epoch: 1793 [33024/118836 (28%)] Loss: 12247.453125\n",
      "Train Epoch: 1793 [65792/118836 (55%)] Loss: 12264.359375\n",
      "Train Epoch: 1793 [98560/118836 (83%)] Loss: 12242.441406\n",
      "    epoch          : 1793\n",
      "    loss           : 12241.826964594964\n",
      "    val_loss       : 12244.092077579216\n",
      "    val_log_likelihood: -12161.486756713968\n",
      "    val_log_marginal: -12169.628072684654\n",
      "Train Epoch: 1794 [256/118836 (0%)] Loss: 12309.867188\n",
      "Train Epoch: 1794 [33024/118836 (28%)] Loss: 12289.187500\n",
      "Train Epoch: 1794 [65792/118836 (55%)] Loss: 12357.369141\n",
      "Train Epoch: 1794 [98560/118836 (83%)] Loss: 12196.333008\n",
      "    epoch          : 1794\n",
      "    loss           : 12244.664199331834\n",
      "    val_loss       : 12245.454386029027\n",
      "    val_log_likelihood: -12162.244918159377\n",
      "    val_log_marginal: -12170.482741164475\n",
      "Train Epoch: 1795 [256/118836 (0%)] Loss: 12209.518555\n",
      "Train Epoch: 1795 [33024/118836 (28%)] Loss: 12252.553711\n",
      "Train Epoch: 1795 [65792/118836 (55%)] Loss: 12214.020508\n",
      "Train Epoch: 1795 [98560/118836 (83%)] Loss: 12291.241211\n",
      "    epoch          : 1795\n",
      "    loss           : 12244.078363284481\n",
      "    val_loss       : 12245.27515563934\n",
      "    val_log_likelihood: -12161.321116851479\n",
      "    val_log_marginal: -12169.392151987086\n",
      "Train Epoch: 1796 [256/118836 (0%)] Loss: 12277.616211\n",
      "Train Epoch: 1796 [33024/118836 (28%)] Loss: 12180.293945\n",
      "Train Epoch: 1796 [65792/118836 (55%)] Loss: 12368.136719\n",
      "Train Epoch: 1796 [98560/118836 (83%)] Loss: 12167.731445\n",
      "    epoch          : 1796\n",
      "    loss           : 12241.426292551954\n",
      "    val_loss       : 12243.265293633978\n",
      "    val_log_likelihood: -12162.973712456058\n",
      "    val_log_marginal: -12171.112743551486\n",
      "Train Epoch: 1797 [256/118836 (0%)] Loss: 12298.765625\n",
      "Train Epoch: 1797 [33024/118836 (28%)] Loss: 12272.013672\n",
      "Train Epoch: 1797 [65792/118836 (55%)] Loss: 12296.459961\n",
      "Train Epoch: 1797 [98560/118836 (83%)] Loss: 12427.228516\n",
      "    epoch          : 1797\n",
      "    loss           : 12246.30542416253\n",
      "    val_loss       : 12238.859806266779\n",
      "    val_log_likelihood: -12162.369514772021\n",
      "    val_log_marginal: -12170.590694885668\n",
      "Train Epoch: 1798 [256/118836 (0%)] Loss: 12291.418945\n",
      "Train Epoch: 1798 [33024/118836 (28%)] Loss: 12291.696289\n",
      "Train Epoch: 1798 [65792/118836 (55%)] Loss: 12199.500000\n",
      "Train Epoch: 1798 [98560/118836 (83%)] Loss: 12272.998047\n",
      "    epoch          : 1798\n",
      "    loss           : 12243.630955334987\n",
      "    val_loss       : 12244.174333200754\n",
      "    val_log_likelihood: -12162.946791156173\n",
      "    val_log_marginal: -12171.025371224017\n",
      "Train Epoch: 1799 [256/118836 (0%)] Loss: 12382.730469\n",
      "Train Epoch: 1799 [33024/118836 (28%)] Loss: 12365.380859\n",
      "Train Epoch: 1799 [65792/118836 (55%)] Loss: 12203.665039\n",
      "Train Epoch: 1799 [98560/118836 (83%)] Loss: 12286.934570\n",
      "    epoch          : 1799\n",
      "    loss           : 12245.643440795595\n",
      "    val_loss       : 12242.948566940382\n",
      "    val_log_likelihood: -12161.398714233095\n",
      "    val_log_marginal: -12169.691254176576\n",
      "Train Epoch: 1800 [256/118836 (0%)] Loss: 12397.544922\n",
      "Train Epoch: 1800 [33024/118836 (28%)] Loss: 12234.338867\n",
      "Train Epoch: 1800 [65792/118836 (55%)] Loss: 12321.680664\n",
      "Train Epoch: 1800 [98560/118836 (83%)] Loss: 12232.719727\n",
      "    epoch          : 1800\n",
      "    loss           : 12247.626716132909\n",
      "    val_loss       : 12248.437345296414\n",
      "    val_log_likelihood: -12163.529557130118\n",
      "    val_log_marginal: -12171.78470598107\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch1800.pth ...\n",
      "Train Epoch: 1801 [256/118836 (0%)] Loss: 12320.402344\n",
      "Train Epoch: 1801 [33024/118836 (28%)] Loss: 12294.441406\n",
      "Train Epoch: 1801 [65792/118836 (55%)] Loss: 12268.267578\n",
      "Train Epoch: 1801 [98560/118836 (83%)] Loss: 12260.853516\n",
      "    epoch          : 1801\n",
      "    loss           : 12244.859521686312\n",
      "    val_loss       : 12245.632194451955\n",
      "    val_log_likelihood: -12161.625811298078\n",
      "    val_log_marginal: -12169.710322626339\n",
      "Train Epoch: 1802 [256/118836 (0%)] Loss: 12218.535156\n",
      "Train Epoch: 1802 [33024/118836 (28%)] Loss: 12197.558594\n",
      "Train Epoch: 1802 [65792/118836 (55%)] Loss: 12370.698242\n",
      "Train Epoch: 1802 [98560/118836 (83%)] Loss: 12288.741211\n",
      "    epoch          : 1802\n",
      "    loss           : 12243.81741528381\n",
      "    val_loss       : 12242.635127664722\n",
      "    val_log_likelihood: -12161.840706097499\n",
      "    val_log_marginal: -12169.907635178062\n",
      "Train Epoch: 1803 [256/118836 (0%)] Loss: 12290.380859\n",
      "Train Epoch: 1803 [33024/118836 (28%)] Loss: 12295.285156\n",
      "Train Epoch: 1803 [65792/118836 (55%)] Loss: 12179.478516\n",
      "Train Epoch: 1803 [98560/118836 (83%)] Loss: 12149.216797\n",
      "    epoch          : 1803\n",
      "    loss           : 12244.37267531276\n",
      "    val_loss       : 12245.118380563445\n",
      "    val_log_likelihood: -12160.6764883491\n",
      "    val_log_marginal: -12168.77463622841\n",
      "Train Epoch: 1804 [256/118836 (0%)] Loss: 12338.416016\n",
      "Train Epoch: 1804 [33024/118836 (28%)] Loss: 12220.767578\n",
      "Train Epoch: 1804 [65792/118836 (55%)] Loss: 12267.388672\n",
      "Train Epoch: 1804 [98560/118836 (83%)] Loss: 12188.683594\n",
      "    epoch          : 1804\n",
      "    loss           : 12246.80282888105\n",
      "    val_loss       : 12247.950773547687\n",
      "    val_log_likelihood: -12161.428869739972\n",
      "    val_log_marginal: -12169.411087308443\n",
      "Train Epoch: 1805 [256/118836 (0%)] Loss: 12278.123047\n",
      "Train Epoch: 1805 [33024/118836 (28%)] Loss: 12380.807617\n",
      "Train Epoch: 1805 [65792/118836 (55%)] Loss: 12292.030273\n",
      "Train Epoch: 1805 [98560/118836 (83%)] Loss: 12189.350586\n",
      "    epoch          : 1805\n",
      "    loss           : 12244.648700824544\n",
      "    val_loss       : 12243.023247185783\n",
      "    val_log_likelihood: -12159.628097859802\n",
      "    val_log_marginal: -12167.88342397008\n",
      "Train Epoch: 1806 [256/118836 (0%)] Loss: 12307.703125\n",
      "Train Epoch: 1806 [33024/118836 (28%)] Loss: 12160.331055\n",
      "Train Epoch: 1806 [65792/118836 (55%)] Loss: 12336.220703\n",
      "Train Epoch: 1806 [98560/118836 (83%)] Loss: 12259.173828\n",
      "    epoch          : 1806\n",
      "    loss           : 12243.208796816842\n",
      "    val_loss       : 12253.001939272843\n",
      "    val_log_likelihood: -12161.37632696185\n",
      "    val_log_marginal: -12169.694235240124\n",
      "Train Epoch: 1807 [256/118836 (0%)] Loss: 12275.875000\n",
      "Train Epoch: 1807 [33024/118836 (28%)] Loss: 12341.932617\n",
      "Train Epoch: 1807 [65792/118836 (55%)] Loss: 12216.782227\n",
      "Train Epoch: 1807 [98560/118836 (83%)] Loss: 12221.722656\n",
      "    epoch          : 1807\n",
      "    loss           : 12247.477650208075\n",
      "    val_loss       : 12248.699400135063\n",
      "    val_log_likelihood: -12162.402273637821\n",
      "    val_log_marginal: -12170.534822556361\n",
      "Train Epoch: 1808 [256/118836 (0%)] Loss: 12179.148438\n",
      "Train Epoch: 1808 [33024/118836 (28%)] Loss: 12325.713867\n",
      "Train Epoch: 1808 [65792/118836 (55%)] Loss: 12321.783203\n",
      "Train Epoch: 1808 [98560/118836 (83%)] Loss: 12325.923828\n",
      "    epoch          : 1808\n",
      "    loss           : 12245.714112095482\n",
      "    val_loss       : 12244.105898768066\n",
      "    val_log_likelihood: -12163.265396893092\n",
      "    val_log_marginal: -12171.328107666555\n",
      "Train Epoch: 1809 [256/118836 (0%)] Loss: 12251.574219\n",
      "Train Epoch: 1809 [33024/118836 (28%)] Loss: 12228.531250\n",
      "Train Epoch: 1809 [65792/118836 (55%)] Loss: 12217.675781\n",
      "Train Epoch: 1809 [98560/118836 (83%)] Loss: 12321.083984\n",
      "    epoch          : 1809\n",
      "    loss           : 12242.182098777397\n",
      "    val_loss       : 12247.974239989224\n",
      "    val_log_likelihood: -12162.753265385909\n",
      "    val_log_marginal: -12170.903365236676\n",
      "Train Epoch: 1810 [256/118836 (0%)] Loss: 12253.961914\n",
      "Train Epoch: 1810 [33024/118836 (28%)] Loss: 12286.905273\n",
      "Train Epoch: 1810 [65792/118836 (55%)] Loss: 12256.954102\n",
      "Train Epoch: 1810 [98560/118836 (83%)] Loss: 12219.082031\n",
      "    epoch          : 1810\n",
      "    loss           : 12250.829150350237\n",
      "    val_loss       : 12245.556053129723\n",
      "    val_log_likelihood: -12162.342844680521\n",
      "    val_log_marginal: -12170.687545130164\n",
      "Train Epoch: 1811 [256/118836 (0%)] Loss: 12370.168945\n",
      "Train Epoch: 1811 [33024/118836 (28%)] Loss: 12308.320312\n",
      "Train Epoch: 1811 [65792/118836 (55%)] Loss: 12221.771484\n",
      "Train Epoch: 1811 [98560/118836 (83%)] Loss: 12201.052734\n",
      "    epoch          : 1811\n",
      "    loss           : 12246.404502526622\n",
      "    val_loss       : 12247.520920083181\n",
      "    val_log_likelihood: -12164.459617969396\n",
      "    val_log_marginal: -12173.06720017949\n",
      "Train Epoch: 1812 [256/118836 (0%)] Loss: 12237.910156\n",
      "Train Epoch: 1812 [33024/118836 (28%)] Loss: 12257.704102\n",
      "Train Epoch: 1812 [65792/118836 (55%)] Loss: 12264.053711\n",
      "Train Epoch: 1812 [98560/118836 (83%)] Loss: 12255.562500\n",
      "    epoch          : 1812\n",
      "    loss           : 12243.111428285256\n",
      "    val_loss       : 12244.119346154062\n",
      "    val_log_likelihood: -12161.52784907465\n",
      "    val_log_marginal: -12169.629235670776\n",
      "Train Epoch: 1813 [256/118836 (0%)] Loss: 12228.765625\n",
      "Train Epoch: 1813 [33024/118836 (28%)] Loss: 12227.468750\n",
      "Train Epoch: 1813 [65792/118836 (55%)] Loss: 12205.515625\n",
      "Train Epoch: 1813 [98560/118836 (83%)] Loss: 12232.749023\n",
      "    epoch          : 1813\n",
      "    loss           : 12247.936826018404\n",
      "    val_loss       : 12245.597322019354\n",
      "    val_log_likelihood: -12162.833228488162\n",
      "    val_log_marginal: -12170.985641837355\n",
      "Train Epoch: 1814 [256/118836 (0%)] Loss: 12344.610352\n",
      "Train Epoch: 1814 [33024/118836 (28%)] Loss: 12263.215820\n",
      "Train Epoch: 1814 [65792/118836 (55%)] Loss: 12289.695312\n",
      "Train Epoch: 1814 [98560/118836 (83%)] Loss: 12415.649414\n",
      "    epoch          : 1814\n",
      "    loss           : 12242.161563016956\n",
      "    val_loss       : 12247.985562607404\n",
      "    val_log_likelihood: -12161.913201444893\n",
      "    val_log_marginal: -12170.181004692357\n",
      "Train Epoch: 1815 [256/118836 (0%)] Loss: 12211.030273\n",
      "Train Epoch: 1815 [33024/118836 (28%)] Loss: 12245.614258\n",
      "Train Epoch: 1815 [65792/118836 (55%)] Loss: 12255.409180\n",
      "Train Epoch: 1815 [98560/118836 (83%)] Loss: 12361.206055\n",
      "    epoch          : 1815\n",
      "    loss           : 12247.666393487643\n",
      "    val_loss       : 12244.969762255752\n",
      "    val_log_likelihood: -12161.54334305857\n",
      "    val_log_marginal: -12169.680719823302\n",
      "Train Epoch: 1816 [256/118836 (0%)] Loss: 12357.455078\n",
      "Train Epoch: 1816 [33024/118836 (28%)] Loss: 12235.720703\n",
      "Train Epoch: 1816 [65792/118836 (55%)] Loss: 12192.912109\n",
      "Train Epoch: 1816 [98560/118836 (83%)] Loss: 12223.144531\n",
      "    epoch          : 1816\n",
      "    loss           : 12243.228717237904\n",
      "    val_loss       : 12245.100048588185\n",
      "    val_log_likelihood: -12162.004747434605\n",
      "    val_log_marginal: -12170.11294334452\n",
      "Train Epoch: 1817 [256/118836 (0%)] Loss: 12242.384766\n",
      "Train Epoch: 1817 [33024/118836 (28%)] Loss: 12215.666016\n",
      "Train Epoch: 1817 [65792/118836 (55%)] Loss: 12311.600586\n",
      "Train Epoch: 1817 [98560/118836 (83%)] Loss: 12189.398438\n",
      "    epoch          : 1817\n",
      "    loss           : 12244.696333326872\n",
      "    val_loss       : 12246.346460403274\n",
      "    val_log_likelihood: -12162.67436414392\n",
      "    val_log_marginal: -12171.207275313029\n",
      "Train Epoch: 1818 [256/118836 (0%)] Loss: 12247.699219\n",
      "Train Epoch: 1818 [33024/118836 (28%)] Loss: 12294.312500\n",
      "Train Epoch: 1818 [65792/118836 (55%)] Loss: 12200.953125\n",
      "Train Epoch: 1818 [98560/118836 (83%)] Loss: 12328.887695\n",
      "    epoch          : 1818\n",
      "    loss           : 12247.843300894334\n",
      "    val_loss       : 12242.08369102083\n",
      "    val_log_likelihood: -12162.699063340055\n",
      "    val_log_marginal: -12170.862813561918\n",
      "Train Epoch: 1819 [256/118836 (0%)] Loss: 12271.724609\n",
      "Train Epoch: 1819 [33024/118836 (28%)] Loss: 12256.130859\n",
      "Train Epoch: 1819 [65792/118836 (55%)] Loss: 12257.486328\n",
      "Train Epoch: 1819 [98560/118836 (83%)] Loss: 12206.435547\n",
      "    epoch          : 1819\n",
      "    loss           : 12247.626928246484\n",
      "    val_loss       : 12247.533340360887\n",
      "    val_log_likelihood: -12161.683517175868\n",
      "    val_log_marginal: -12170.052388372083\n",
      "Train Epoch: 1820 [256/118836 (0%)] Loss: 12276.087891\n",
      "Train Epoch: 1820 [33024/118836 (28%)] Loss: 12179.494141\n",
      "Train Epoch: 1820 [65792/118836 (55%)] Loss: 12365.113281\n",
      "Train Epoch: 1820 [98560/118836 (83%)] Loss: 12397.965820\n",
      "    epoch          : 1820\n",
      "    loss           : 12245.136935063843\n",
      "    val_loss       : 12244.589391337779\n",
      "    val_log_likelihood: -12163.361701787375\n",
      "    val_log_marginal: -12171.437575628901\n",
      "Train Epoch: 1821 [256/118836 (0%)] Loss: 12275.644531\n",
      "Train Epoch: 1821 [33024/118836 (28%)] Loss: 12152.832031\n",
      "Train Epoch: 1821 [65792/118836 (55%)] Loss: 12220.422852\n",
      "Train Epoch: 1821 [98560/118836 (83%)] Loss: 12241.408203\n",
      "    epoch          : 1821\n",
      "    loss           : 12246.656787149763\n",
      "    val_loss       : 12244.458983583096\n",
      "    val_log_likelihood: -12161.979022403588\n",
      "    val_log_marginal: -12170.091923846127\n",
      "Train Epoch: 1822 [256/118836 (0%)] Loss: 12263.363281\n",
      "Train Epoch: 1822 [33024/118836 (28%)] Loss: 12367.071289\n",
      "Train Epoch: 1822 [65792/118836 (55%)] Loss: 12377.244141\n",
      "Train Epoch: 1822 [98560/118836 (83%)] Loss: 12234.169922\n",
      "    epoch          : 1822\n",
      "    loss           : 12247.63626350548\n",
      "    val_loss       : 12242.809988643143\n",
      "    val_log_likelihood: -12162.42822079844\n",
      "    val_log_marginal: -12170.534807697217\n",
      "Train Epoch: 1823 [256/118836 (0%)] Loss: 12272.541992\n",
      "Train Epoch: 1823 [33024/118836 (28%)] Loss: 12261.643555\n",
      "Train Epoch: 1823 [65792/118836 (55%)] Loss: 12171.212891\n",
      "Train Epoch: 1823 [98560/118836 (83%)] Loss: 12321.115234\n",
      "    epoch          : 1823\n",
      "    loss           : 12244.123618434656\n",
      "    val_loss       : 12247.801430476338\n",
      "    val_log_likelihood: -12162.117268759046\n",
      "    val_log_marginal: -12170.255836775192\n",
      "Train Epoch: 1824 [256/118836 (0%)] Loss: 12189.338867\n",
      "Train Epoch: 1824 [33024/118836 (28%)] Loss: 12233.144531\n",
      "Train Epoch: 1824 [65792/118836 (55%)] Loss: 12285.980469\n",
      "Train Epoch: 1824 [98560/118836 (83%)] Loss: 12339.914062\n",
      "    epoch          : 1824\n",
      "    loss           : 12243.637058648677\n",
      "    val_loss       : 12245.567467044511\n",
      "    val_log_likelihood: -12161.445831717845\n",
      "    val_log_marginal: -12169.533777460463\n",
      "Train Epoch: 1825 [256/118836 (0%)] Loss: 12295.945312\n",
      "Train Epoch: 1825 [33024/118836 (28%)] Loss: 12234.761719\n",
      "Train Epoch: 1825 [65792/118836 (55%)] Loss: 12206.335938\n",
      "Train Epoch: 1825 [98560/118836 (83%)] Loss: 12273.789062\n",
      "    epoch          : 1825\n",
      "    loss           : 12245.34547970301\n",
      "    val_loss       : 12244.170397196975\n",
      "    val_log_likelihood: -12164.073942178453\n",
      "    val_log_marginal: -12172.369700062443\n",
      "Train Epoch: 1826 [256/118836 (0%)] Loss: 12267.881836\n",
      "Train Epoch: 1826 [33024/118836 (28%)] Loss: 12337.672852\n",
      "Train Epoch: 1826 [65792/118836 (55%)] Loss: 12286.548828\n",
      "Train Epoch: 1826 [98560/118836 (83%)] Loss: 12274.032227\n",
      "    epoch          : 1826\n",
      "    loss           : 12245.415756339175\n",
      "    val_loss       : 12246.063807811683\n",
      "    val_log_likelihood: -12163.341944530604\n",
      "    val_log_marginal: -12171.427309394594\n",
      "Train Epoch: 1827 [256/118836 (0%)] Loss: 12233.680664\n",
      "Train Epoch: 1827 [33024/118836 (28%)] Loss: 12322.757812\n",
      "Train Epoch: 1827 [65792/118836 (55%)] Loss: 12297.774414\n",
      "Train Epoch: 1827 [98560/118836 (83%)] Loss: 12280.515625\n",
      "    epoch          : 1827\n",
      "    loss           : 12249.150483515561\n",
      "    val_loss       : 12246.852774710671\n",
      "    val_log_likelihood: -12162.628519017526\n",
      "    val_log_marginal: -12170.878779169741\n",
      "Train Epoch: 1828 [256/118836 (0%)] Loss: 12176.605469\n",
      "Train Epoch: 1828 [33024/118836 (28%)] Loss: 12272.040039\n",
      "Train Epoch: 1828 [65792/118836 (55%)] Loss: 12416.145508\n",
      "Train Epoch: 1828 [98560/118836 (83%)] Loss: 12193.615234\n",
      "    epoch          : 1828\n",
      "    loss           : 12252.626224701457\n",
      "    val_loss       : 12247.063068996385\n",
      "    val_log_likelihood: -12162.24969305728\n",
      "    val_log_marginal: -12170.777677729058\n",
      "Train Epoch: 1829 [256/118836 (0%)] Loss: 12206.535156\n",
      "Train Epoch: 1829 [33024/118836 (28%)] Loss: 12272.155273\n",
      "Train Epoch: 1829 [65792/118836 (55%)] Loss: 12280.406250\n",
      "Train Epoch: 1829 [98560/118836 (83%)] Loss: 12306.741211\n",
      "    epoch          : 1829\n",
      "    loss           : 12250.701161697425\n",
      "    val_loss       : 12255.051052354424\n",
      "    val_log_likelihood: -12164.017196062086\n",
      "    val_log_marginal: -12172.666906469673\n",
      "Train Epoch: 1830 [256/118836 (0%)] Loss: 12273.389648\n",
      "Train Epoch: 1830 [33024/118836 (28%)] Loss: 12321.173828\n",
      "Train Epoch: 1830 [65792/118836 (55%)] Loss: 12212.478516\n",
      "Train Epoch: 1830 [98560/118836 (83%)] Loss: 12211.060547\n",
      "    epoch          : 1830\n",
      "    loss           : 12252.02268064387\n",
      "    val_loss       : 12244.452893842023\n",
      "    val_log_likelihood: -12161.611376912737\n",
      "    val_log_marginal: -12169.871235183993\n",
      "Train Epoch: 1831 [256/118836 (0%)] Loss: 12293.061523\n",
      "Train Epoch: 1831 [33024/118836 (28%)] Loss: 12269.908203\n",
      "Train Epoch: 1831 [65792/118836 (55%)] Loss: 12272.818359\n",
      "Train Epoch: 1831 [98560/118836 (83%)] Loss: 12279.487305\n",
      "    epoch          : 1831\n",
      "    loss           : 12247.469655642577\n",
      "    val_loss       : 12243.481609352039\n",
      "    val_log_likelihood: -12162.627585911652\n",
      "    val_log_marginal: -12170.737415516378\n",
      "Train Epoch: 1832 [256/118836 (0%)] Loss: 12258.660156\n",
      "Train Epoch: 1832 [33024/118836 (28%)] Loss: 12232.007812\n",
      "Train Epoch: 1832 [65792/118836 (55%)] Loss: 12239.551758\n",
      "Train Epoch: 1832 [98560/118836 (83%)] Loss: 12164.001953\n",
      "    epoch          : 1832\n",
      "    loss           : 12247.016229515613\n",
      "    val_loss       : 12244.148178656937\n",
      "    val_log_likelihood: -12160.844168088297\n",
      "    val_log_marginal: -12168.84030204062\n",
      "Train Epoch: 1833 [256/118836 (0%)] Loss: 12317.559570\n",
      "Train Epoch: 1833 [33024/118836 (28%)] Loss: 12206.553711\n",
      "Train Epoch: 1833 [65792/118836 (55%)] Loss: 12351.216797\n",
      "Train Epoch: 1833 [98560/118836 (83%)] Loss: 12188.827148\n",
      "    epoch          : 1833\n",
      "    loss           : 12244.416875064619\n",
      "    val_loss       : 12246.477781043708\n",
      "    val_log_likelihood: -12162.691617071185\n",
      "    val_log_marginal: -12170.67057121092\n",
      "Train Epoch: 1834 [256/118836 (0%)] Loss: 12281.507812\n",
      "Train Epoch: 1834 [33024/118836 (28%)] Loss: 12248.653320\n",
      "Train Epoch: 1834 [65792/118836 (55%)] Loss: 12221.320312\n",
      "Train Epoch: 1834 [98560/118836 (83%)] Loss: 12335.185547\n",
      "    epoch          : 1834\n",
      "    loss           : 12243.616370224618\n",
      "    val_loss       : 12244.223630580738\n",
      "    val_log_likelihood: -12161.306458397952\n",
      "    val_log_marginal: -12169.598703336249\n",
      "Train Epoch: 1835 [256/118836 (0%)] Loss: 12239.457031\n",
      "Train Epoch: 1835 [33024/118836 (28%)] Loss: 12252.448242\n",
      "Train Epoch: 1835 [65792/118836 (55%)] Loss: 12239.917969\n",
      "Train Epoch: 1835 [98560/118836 (83%)] Loss: 12394.291016\n",
      "    epoch          : 1835\n",
      "    loss           : 12245.920565485421\n",
      "    val_loss       : 12243.706040543459\n",
      "    val_log_likelihood: -12162.345915400125\n",
      "    val_log_marginal: -12170.498433716239\n",
      "Train Epoch: 1836 [256/118836 (0%)] Loss: 12193.170898\n",
      "Train Epoch: 1836 [33024/118836 (28%)] Loss: 12303.458008\n",
      "Train Epoch: 1836 [65792/118836 (55%)] Loss: 12319.738281\n",
      "Train Epoch: 1836 [98560/118836 (83%)] Loss: 12225.243164\n",
      "    epoch          : 1836\n",
      "    loss           : 12254.679005925609\n",
      "    val_loss       : 12247.757806145819\n",
      "    val_log_likelihood: -12161.701153619986\n",
      "    val_log_marginal: -12169.877057043155\n",
      "Train Epoch: 1837 [256/118836 (0%)] Loss: 12325.924805\n",
      "Train Epoch: 1837 [33024/118836 (28%)] Loss: 12272.137695\n",
      "Train Epoch: 1837 [65792/118836 (55%)] Loss: 12246.475586\n",
      "Train Epoch: 1837 [98560/118836 (83%)] Loss: 12224.322266\n",
      "    epoch          : 1837\n",
      "    loss           : 12237.117713987542\n",
      "    val_loss       : 12245.452386329573\n",
      "    val_log_likelihood: -12162.020007657415\n",
      "    val_log_marginal: -12170.12195877698\n",
      "Train Epoch: 1838 [256/118836 (0%)] Loss: 12217.521484\n",
      "Train Epoch: 1838 [33024/118836 (28%)] Loss: 12235.841797\n",
      "Train Epoch: 1838 [65792/118836 (55%)] Loss: 12202.726562\n",
      "Train Epoch: 1838 [98560/118836 (83%)] Loss: 12308.580078\n",
      "    epoch          : 1838\n",
      "    loss           : 12244.322069666305\n",
      "    val_loss       : 12244.609199611416\n",
      "    val_log_likelihood: -12161.807379064569\n",
      "    val_log_marginal: -12169.936524285631\n",
      "Train Epoch: 1839 [256/118836 (0%)] Loss: 12261.921875\n",
      "Train Epoch: 1839 [33024/118836 (28%)] Loss: 12322.160156\n",
      "Train Epoch: 1839 [65792/118836 (55%)] Loss: 12308.847656\n",
      "Train Epoch: 1839 [98560/118836 (83%)] Loss: 12201.862305\n",
      "    epoch          : 1839\n",
      "    loss           : 12246.60027334057\n",
      "    val_loss       : 12245.663838722798\n",
      "    val_log_likelihood: -12161.19022629756\n",
      "    val_log_marginal: -12169.356701930194\n",
      "Train Epoch: 1840 [256/118836 (0%)] Loss: 12252.641602\n",
      "Train Epoch: 1840 [33024/118836 (28%)] Loss: 12201.265625\n",
      "Train Epoch: 1840 [65792/118836 (55%)] Loss: 12266.423828\n",
      "Train Epoch: 1840 [98560/118836 (83%)] Loss: 12202.708008\n",
      "    epoch          : 1840\n",
      "    loss           : 12242.87441147772\n",
      "    val_loss       : 12247.924422749433\n",
      "    val_log_likelihood: -12161.403728223222\n",
      "    val_log_marginal: -12169.312944434152\n",
      "Train Epoch: 1841 [256/118836 (0%)] Loss: 12357.805664\n",
      "Train Epoch: 1841 [33024/118836 (28%)] Loss: 12259.835938\n",
      "Train Epoch: 1841 [65792/118836 (55%)] Loss: 12234.147461\n",
      "Train Epoch: 1841 [98560/118836 (83%)] Loss: 12345.352539\n",
      "    epoch          : 1841\n",
      "    loss           : 12245.067187015355\n",
      "    val_loss       : 12243.44486615585\n",
      "    val_log_likelihood: -12160.680479250672\n",
      "    val_log_marginal: -12168.754408870598\n",
      "Train Epoch: 1842 [256/118836 (0%)] Loss: 12166.460938\n",
      "Train Epoch: 1842 [33024/118836 (28%)] Loss: 12194.485352\n",
      "Train Epoch: 1842 [65792/118836 (55%)] Loss: 12271.804688\n",
      "Train Epoch: 1842 [98560/118836 (83%)] Loss: 12295.412109\n",
      "    epoch          : 1842\n",
      "    loss           : 12245.60455406069\n",
      "    val_loss       : 12242.83171280208\n",
      "    val_log_likelihood: -12159.524247828786\n",
      "    val_log_marginal: -12167.899970731658\n",
      "Train Epoch: 1843 [256/118836 (0%)] Loss: 12246.534180\n",
      "Train Epoch: 1843 [33024/118836 (28%)] Loss: 12259.867188\n",
      "Train Epoch: 1843 [65792/118836 (55%)] Loss: 12289.199219\n",
      "Train Epoch: 1843 [98560/118836 (83%)] Loss: 12369.829102\n",
      "    epoch          : 1843\n",
      "    loss           : 12244.01129306891\n",
      "    val_loss       : 12249.467609442201\n",
      "    val_log_likelihood: -12159.794237069633\n",
      "    val_log_marginal: -12168.070871042904\n",
      "Train Epoch: 1844 [256/118836 (0%)] Loss: 12219.057617\n",
      "Train Epoch: 1844 [33024/118836 (28%)] Loss: 12142.808594\n",
      "Train Epoch: 1844 [65792/118836 (55%)] Loss: 12235.506836\n",
      "Train Epoch: 1844 [98560/118836 (83%)] Loss: 12200.894531\n",
      "    epoch          : 1844\n",
      "    loss           : 12248.814591087676\n",
      "    val_loss       : 12252.688681589418\n",
      "    val_log_likelihood: -12160.777226627119\n",
      "    val_log_marginal: -12168.970710006628\n",
      "Train Epoch: 1845 [256/118836 (0%)] Loss: 12193.257812\n",
      "Train Epoch: 1845 [33024/118836 (28%)] Loss: 12295.357422\n",
      "Train Epoch: 1845 [65792/118836 (55%)] Loss: 12301.899414\n",
      "Train Epoch: 1845 [98560/118836 (83%)] Loss: 12272.689453\n",
      "    epoch          : 1845\n",
      "    loss           : 12243.285008755945\n",
      "    val_loss       : 12244.60581077693\n",
      "    val_log_likelihood: -12162.716163119056\n",
      "    val_log_marginal: -12170.788552513923\n",
      "Train Epoch: 1846 [256/118836 (0%)] Loss: 12275.833984\n",
      "Train Epoch: 1846 [33024/118836 (28%)] Loss: 12355.596680\n",
      "Train Epoch: 1846 [65792/118836 (55%)] Loss: 12305.177734\n",
      "Train Epoch: 1846 [98560/118836 (83%)] Loss: 12171.887695\n",
      "    epoch          : 1846\n",
      "    loss           : 12249.81512920673\n",
      "    val_loss       : 12247.231217913497\n",
      "    val_log_likelihood: -12160.66431984724\n",
      "    val_log_marginal: -12168.90322112207\n",
      "Train Epoch: 1847 [256/118836 (0%)] Loss: 12276.459961\n",
      "Train Epoch: 1847 [33024/118836 (28%)] Loss: 12230.600586\n",
      "Train Epoch: 1847 [65792/118836 (55%)] Loss: 12346.633789\n",
      "Train Epoch: 1847 [98560/118836 (83%)] Loss: 12270.521484\n",
      "    epoch          : 1847\n",
      "    loss           : 12246.434009253517\n",
      "    val_loss       : 12247.513095965569\n",
      "    val_log_likelihood: -12162.911512452181\n",
      "    val_log_marginal: -12171.163519300662\n",
      "Train Epoch: 1848 [256/118836 (0%)] Loss: 12298.254883\n",
      "Train Epoch: 1848 [33024/118836 (28%)] Loss: 12186.011719\n",
      "Train Epoch: 1848 [65792/118836 (55%)] Loss: 12241.609375\n",
      "Train Epoch: 1848 [98560/118836 (83%)] Loss: 12307.480469\n",
      "    epoch          : 1848\n",
      "    loss           : 12243.912020200063\n",
      "    val_loss       : 12244.87314144268\n",
      "    val_log_likelihood: -12162.314291899296\n",
      "    val_log_marginal: -12170.447246697355\n",
      "Train Epoch: 1849 [256/118836 (0%)] Loss: 12273.132812\n",
      "Train Epoch: 1849 [33024/118836 (28%)] Loss: 12261.956055\n",
      "Train Epoch: 1849 [65792/118836 (55%)] Loss: 12301.063477\n",
      "Train Epoch: 1849 [98560/118836 (83%)] Loss: 12258.988281\n",
      "    epoch          : 1849\n",
      "    loss           : 12247.3171537363\n",
      "    val_loss       : 12244.913653073578\n",
      "    val_log_likelihood: -12161.110487263493\n",
      "    val_log_marginal: -12169.274319689031\n",
      "Train Epoch: 1850 [256/118836 (0%)] Loss: 12293.754883\n",
      "Train Epoch: 1850 [33024/118836 (28%)] Loss: 12256.593750\n",
      "Train Epoch: 1850 [65792/118836 (55%)] Loss: 12269.351562\n",
      "Train Epoch: 1850 [98560/118836 (83%)] Loss: 12320.202148\n",
      "    epoch          : 1850\n",
      "    loss           : 12248.730497667235\n",
      "    val_loss       : 12242.855835929784\n",
      "    val_log_likelihood: -12161.533481312035\n",
      "    val_log_marginal: -12169.753194427461\n",
      "Train Epoch: 1851 [256/118836 (0%)] Loss: 12176.719727\n",
      "Train Epoch: 1851 [33024/118836 (28%)] Loss: 12171.358398\n",
      "Train Epoch: 1851 [65792/118836 (55%)] Loss: 12335.539062\n",
      "Train Epoch: 1851 [98560/118836 (83%)] Loss: 12258.739258\n",
      "    epoch          : 1851\n",
      "    loss           : 12241.22363265095\n",
      "    val_loss       : 12243.981199158015\n",
      "    val_log_likelihood: -12162.226017434346\n",
      "    val_log_marginal: -12170.278946659238\n",
      "Train Epoch: 1852 [256/118836 (0%)] Loss: 12164.277344\n",
      "Train Epoch: 1852 [33024/118836 (28%)] Loss: 12319.734375\n",
      "Train Epoch: 1852 [65792/118836 (55%)] Loss: 12401.904297\n",
      "Train Epoch: 1852 [98560/118836 (83%)] Loss: 12159.905273\n",
      "    epoch          : 1852\n",
      "    loss           : 12245.510574822943\n",
      "    val_loss       : 12253.21850685289\n",
      "    val_log_likelihood: -12163.19765625\n",
      "    val_log_marginal: -12171.453323468842\n",
      "Train Epoch: 1853 [256/118836 (0%)] Loss: 12312.660156\n",
      "Train Epoch: 1853 [33024/118836 (28%)] Loss: 12299.341797\n",
      "Train Epoch: 1853 [65792/118836 (55%)] Loss: 12182.933594\n",
      "Train Epoch: 1853 [98560/118836 (83%)] Loss: 12186.361328\n",
      "    epoch          : 1853\n",
      "    loss           : 12246.26709460944\n",
      "    val_loss       : 12249.181540720458\n",
      "    val_log_likelihood: -12162.673829740488\n",
      "    val_log_marginal: -12170.779921271876\n",
      "Train Epoch: 1854 [256/118836 (0%)] Loss: 12330.334961\n",
      "Train Epoch: 1854 [33024/118836 (28%)] Loss: 12176.371094\n",
      "Train Epoch: 1854 [65792/118836 (55%)] Loss: 12297.886719\n",
      "Train Epoch: 1854 [98560/118836 (83%)] Loss: 12275.607422\n",
      "    epoch          : 1854\n",
      "    loss           : 12243.872502940188\n",
      "    val_loss       : 12248.325644573953\n",
      "    val_log_likelihood: -12161.783476950217\n",
      "    val_log_marginal: -12169.814944245954\n",
      "Train Epoch: 1855 [256/118836 (0%)] Loss: 12255.946289\n",
      "Train Epoch: 1855 [33024/118836 (28%)] Loss: 12369.366211\n",
      "Train Epoch: 1855 [65792/118836 (55%)] Loss: 12222.143555\n",
      "Train Epoch: 1855 [98560/118836 (83%)] Loss: 12196.023438\n",
      "    epoch          : 1855\n",
      "    loss           : 12246.429674737645\n",
      "    val_loss       : 12240.230905180215\n",
      "    val_log_likelihood: -12163.393031592483\n",
      "    val_log_marginal: -12171.378133213819\n",
      "Train Epoch: 1856 [256/118836 (0%)] Loss: 12448.395508\n",
      "Train Epoch: 1856 [33024/118836 (28%)] Loss: 12219.043945\n",
      "Train Epoch: 1856 [65792/118836 (55%)] Loss: 12258.502930\n",
      "Train Epoch: 1856 [98560/118836 (83%)] Loss: 12260.827148\n",
      "    epoch          : 1856\n",
      "    loss           : 12244.08484252223\n",
      "    val_loss       : 12250.512953932757\n",
      "    val_log_likelihood: -12161.8154296875\n",
      "    val_log_marginal: -12170.07688864444\n",
      "Train Epoch: 1857 [256/118836 (0%)] Loss: 12238.343750\n",
      "Train Epoch: 1857 [33024/118836 (28%)] Loss: 12241.135742\n",
      "Train Epoch: 1857 [65792/118836 (55%)] Loss: 12191.035156\n",
      "Train Epoch: 1857 [98560/118836 (83%)] Loss: 12314.212891\n",
      "    epoch          : 1857\n",
      "    loss           : 12245.954381203474\n",
      "    val_loss       : 12245.70729442199\n",
      "    val_log_likelihood: -12160.681794419457\n",
      "    val_log_marginal: -12168.704604886256\n",
      "Train Epoch: 1858 [256/118836 (0%)] Loss: 12309.830078\n",
      "Train Epoch: 1858 [33024/118836 (28%)] Loss: 12204.882812\n",
      "Train Epoch: 1858 [65792/118836 (55%)] Loss: 12169.989258\n",
      "Train Epoch: 1858 [98560/118836 (83%)] Loss: 12376.111328\n",
      "    epoch          : 1858\n",
      "    loss           : 12245.990511269645\n",
      "    val_loss       : 12244.713260243296\n",
      "    val_log_likelihood: -12159.09562218905\n",
      "    val_log_marginal: -12167.304578718802\n",
      "Train Epoch: 1859 [256/118836 (0%)] Loss: 12283.989258\n",
      "Train Epoch: 1859 [33024/118836 (28%)] Loss: 12216.501953\n",
      "Train Epoch: 1859 [65792/118836 (55%)] Loss: 12192.845703\n",
      "Train Epoch: 1859 [98560/118836 (83%)] Loss: 12227.326172\n",
      "    epoch          : 1859\n",
      "    loss           : 12247.161707603133\n",
      "    val_loss       : 12244.13681574283\n",
      "    val_log_likelihood: -12161.526291098015\n",
      "    val_log_marginal: -12169.565659198845\n",
      "Train Epoch: 1860 [256/118836 (0%)] Loss: 12359.376953\n",
      "Train Epoch: 1860 [33024/118836 (28%)] Loss: 12319.105469\n",
      "Train Epoch: 1860 [65792/118836 (55%)] Loss: 12250.606445\n",
      "Train Epoch: 1860 [98560/118836 (83%)] Loss: 12372.545898\n",
      "    epoch          : 1860\n",
      "    loss           : 12244.347137516801\n",
      "    val_loss       : 12248.524783110977\n",
      "    val_log_likelihood: -12160.113674944427\n",
      "    val_log_marginal: -12168.449350863848\n",
      "Train Epoch: 1861 [256/118836 (0%)] Loss: 12370.290039\n",
      "Train Epoch: 1861 [33024/118836 (28%)] Loss: 12299.341797\n",
      "Train Epoch: 1861 [65792/118836 (55%)] Loss: 12201.410156\n",
      "Train Epoch: 1861 [98560/118836 (83%)] Loss: 12350.181641\n",
      "    epoch          : 1861\n",
      "    loss           : 12246.530719635288\n",
      "    val_loss       : 12241.283508968088\n",
      "    val_log_likelihood: -12159.815506100082\n",
      "    val_log_marginal: -12167.973896217667\n",
      "Train Epoch: 1862 [256/118836 (0%)] Loss: 12172.693359\n",
      "Train Epoch: 1862 [33024/118836 (28%)] Loss: 12211.658203\n",
      "Train Epoch: 1862 [65792/118836 (55%)] Loss: 12269.203125\n",
      "Train Epoch: 1862 [98560/118836 (83%)] Loss: 12228.862305\n",
      "    epoch          : 1862\n",
      "    loss           : 12250.588388033759\n",
      "    val_loss       : 12244.969729786591\n",
      "    val_log_likelihood: -12160.411790316119\n",
      "    val_log_marginal: -12168.677258047026\n",
      "Train Epoch: 1863 [256/118836 (0%)] Loss: 12230.550781\n",
      "Train Epoch: 1863 [33024/118836 (28%)] Loss: 12223.343750\n",
      "Train Epoch: 1863 [65792/118836 (55%)] Loss: 12292.868164\n",
      "Train Epoch: 1863 [98560/118836 (83%)] Loss: 12355.550781\n",
      "    epoch          : 1863\n",
      "    loss           : 12244.317000426488\n",
      "    val_loss       : 12247.520219314449\n",
      "    val_log_likelihood: -12161.315025977048\n",
      "    val_log_marginal: -12169.44820763433\n",
      "Train Epoch: 1864 [256/118836 (0%)] Loss: 12295.974609\n",
      "Train Epoch: 1864 [33024/118836 (28%)] Loss: 12274.677734\n",
      "Train Epoch: 1864 [65792/118836 (55%)] Loss: 12335.674805\n",
      "Train Epoch: 1864 [98560/118836 (83%)] Loss: 12338.323242\n",
      "    epoch          : 1864\n",
      "    loss           : 12249.728141477979\n",
      "    val_loss       : 12249.565247212888\n",
      "    val_log_likelihood: -12162.388076890767\n",
      "    val_log_marginal: -12170.598933281703\n",
      "Train Epoch: 1865 [256/118836 (0%)] Loss: 12261.031250\n",
      "Train Epoch: 1865 [33024/118836 (28%)] Loss: 12219.455078\n",
      "Train Epoch: 1865 [65792/118836 (55%)] Loss: 12253.784180\n",
      "Train Epoch: 1865 [98560/118836 (83%)] Loss: 12198.909180\n",
      "    epoch          : 1865\n",
      "    loss           : 12248.212880124327\n",
      "    val_loss       : 12244.868691238014\n",
      "    val_log_likelihood: -12163.379971341243\n",
      "    val_log_marginal: -12171.53087405019\n",
      "Train Epoch: 1866 [256/118836 (0%)] Loss: 12237.240234\n",
      "Train Epoch: 1866 [33024/118836 (28%)] Loss: 12305.146484\n",
      "Train Epoch: 1866 [65792/118836 (55%)] Loss: 12304.377930\n",
      "Train Epoch: 1866 [98560/118836 (83%)] Loss: 12312.714844\n",
      "    epoch          : 1866\n",
      "    loss           : 12247.175015831783\n",
      "    val_loss       : 12250.669625919407\n",
      "    val_log_likelihood: -12165.598303737594\n",
      "    val_log_marginal: -12173.709251168542\n",
      "Train Epoch: 1867 [256/118836 (0%)] Loss: 12203.812500\n",
      "Train Epoch: 1867 [33024/118836 (28%)] Loss: 12172.957031\n",
      "Train Epoch: 1867 [65792/118836 (55%)] Loss: 12302.663086\n",
      "Train Epoch: 1867 [98560/118836 (83%)] Loss: 12304.119141\n",
      "    epoch          : 1867\n",
      "    loss           : 12242.868943212365\n",
      "    val_loss       : 12246.385695438838\n",
      "    val_log_likelihood: -12162.098266742918\n",
      "    val_log_marginal: -12170.295103440123\n",
      "Train Epoch: 1868 [256/118836 (0%)] Loss: 12151.988281\n",
      "Train Epoch: 1868 [33024/118836 (28%)] Loss: 12258.932617\n",
      "Train Epoch: 1868 [65792/118836 (55%)] Loss: 12205.345703\n",
      "Train Epoch: 1868 [98560/118836 (83%)] Loss: 12300.405273\n",
      "    epoch          : 1868\n",
      "    loss           : 12243.764957965\n",
      "    val_loss       : 12245.873510652307\n",
      "    val_log_likelihood: -12162.486251550868\n",
      "    val_log_marginal: -12170.584039855617\n",
      "Train Epoch: 1869 [256/118836 (0%)] Loss: 12273.757812\n",
      "Train Epoch: 1869 [33024/118836 (28%)] Loss: 12331.463867\n",
      "Train Epoch: 1869 [65792/118836 (55%)] Loss: 12242.841797\n",
      "Train Epoch: 1869 [98560/118836 (83%)] Loss: 12246.132812\n",
      "    epoch          : 1869\n",
      "    loss           : 12245.635827969914\n",
      "    val_loss       : 12246.736764979038\n",
      "    val_log_likelihood: -12160.105308170492\n",
      "    val_log_marginal: -12168.450696215226\n",
      "Train Epoch: 1870 [256/118836 (0%)] Loss: 12207.967773\n",
      "Train Epoch: 1870 [33024/118836 (28%)] Loss: 12362.327148\n",
      "Train Epoch: 1870 [65792/118836 (55%)] Loss: 12260.291016\n",
      "Train Epoch: 1870 [98560/118836 (83%)] Loss: 12278.755859\n",
      "    epoch          : 1870\n",
      "    loss           : 12247.652810949132\n",
      "    val_loss       : 12244.570883946826\n",
      "    val_log_likelihood: -12162.400673981596\n",
      "    val_log_marginal: -12170.495475696547\n",
      "Train Epoch: 1871 [256/118836 (0%)] Loss: 12354.635742\n",
      "Train Epoch: 1871 [33024/118836 (28%)] Loss: 12304.224609\n",
      "Train Epoch: 1871 [65792/118836 (55%)] Loss: 12351.847656\n",
      "Train Epoch: 1871 [98560/118836 (83%)] Loss: 12231.501953\n",
      "    epoch          : 1871\n",
      "    loss           : 12250.691193328681\n",
      "    val_loss       : 12247.680105483243\n",
      "    val_log_likelihood: -12161.595515082196\n",
      "    val_log_marginal: -12169.67376762135\n",
      "Train Epoch: 1872 [256/118836 (0%)] Loss: 12248.922852\n",
      "Train Epoch: 1872 [33024/118836 (28%)] Loss: 12294.580078\n",
      "Train Epoch: 1872 [65792/118836 (55%)] Loss: 12272.463867\n",
      "Train Epoch: 1872 [98560/118836 (83%)] Loss: 12242.945312\n",
      "    epoch          : 1872\n",
      "    loss           : 12251.84832974695\n",
      "    val_loss       : 12248.317930942276\n",
      "    val_log_likelihood: -12160.915136799525\n",
      "    val_log_marginal: -12168.977057824026\n",
      "Train Epoch: 1873 [256/118836 (0%)] Loss: 12220.292969\n",
      "Train Epoch: 1873 [33024/118836 (28%)] Loss: 12224.385742\n",
      "Train Epoch: 1873 [65792/118836 (55%)] Loss: 12259.280273\n",
      "Train Epoch: 1873 [98560/118836 (83%)] Loss: 12269.059570\n",
      "    epoch          : 1873\n",
      "    loss           : 12244.31114218233\n",
      "    val_loss       : 12245.510954290476\n",
      "    val_log_likelihood: -12161.486450255892\n",
      "    val_log_marginal: -12169.756371636082\n",
      "Train Epoch: 1874 [256/118836 (0%)] Loss: 12229.492188\n",
      "Train Epoch: 1874 [33024/118836 (28%)] Loss: 12218.778320\n",
      "Train Epoch: 1874 [65792/118836 (55%)] Loss: 12296.871094\n",
      "Train Epoch: 1874 [98560/118836 (83%)] Loss: 12268.841797\n",
      "    epoch          : 1874\n",
      "    loss           : 12250.104139849565\n",
      "    val_loss       : 12246.10275295849\n",
      "    val_log_likelihood: -12161.954233386321\n",
      "    val_log_marginal: -12170.23161920212\n",
      "Train Epoch: 1875 [256/118836 (0%)] Loss: 12302.351562\n",
      "Train Epoch: 1875 [33024/118836 (28%)] Loss: 12249.121094\n",
      "Train Epoch: 1875 [65792/118836 (55%)] Loss: 12298.763672\n",
      "Train Epoch: 1875 [98560/118836 (83%)] Loss: 12373.403320\n",
      "    epoch          : 1875\n",
      "    loss           : 12245.216993802987\n",
      "    val_loss       : 12246.450091908277\n",
      "    val_log_likelihood: -12161.75667681193\n",
      "    val_log_marginal: -12169.793585863757\n",
      "Train Epoch: 1876 [256/118836 (0%)] Loss: 12301.310547\n",
      "Train Epoch: 1876 [33024/118836 (28%)] Loss: 12238.331055\n",
      "Train Epoch: 1876 [65792/118836 (55%)] Loss: 12234.739258\n",
      "Train Epoch: 1876 [98560/118836 (83%)] Loss: 12328.453125\n",
      "    epoch          : 1876\n",
      "    loss           : 12244.78106809605\n",
      "    val_loss       : 12247.606936734535\n",
      "    val_log_likelihood: -12161.542553084937\n",
      "    val_log_marginal: -12169.600531684357\n",
      "Train Epoch: 1877 [256/118836 (0%)] Loss: 12208.767578\n",
      "Train Epoch: 1877 [33024/118836 (28%)] Loss: 12431.041016\n",
      "Train Epoch: 1877 [65792/118836 (55%)] Loss: 12197.989258\n",
      "Train Epoch: 1877 [98560/118836 (83%)] Loss: 12224.595703\n",
      "    epoch          : 1877\n",
      "    loss           : 12242.425706291357\n",
      "    val_loss       : 12250.095258732299\n",
      "    val_log_likelihood: -12162.126663629548\n",
      "    val_log_marginal: -12170.155704977557\n",
      "Train Epoch: 1878 [256/118836 (0%)] Loss: 12255.623047\n",
      "Train Epoch: 1878 [33024/118836 (28%)] Loss: 12272.857422\n",
      "Train Epoch: 1878 [65792/118836 (55%)] Loss: 12401.517578\n",
      "Train Epoch: 1878 [98560/118836 (83%)] Loss: 12317.438477\n",
      "    epoch          : 1878\n",
      "    loss           : 12250.794509440913\n",
      "    val_loss       : 12247.036766660452\n",
      "    val_log_likelihood: -12163.624039430832\n",
      "    val_log_marginal: -12171.82005031169\n",
      "Train Epoch: 1879 [256/118836 (0%)] Loss: 12240.292969\n",
      "Train Epoch: 1879 [33024/118836 (28%)] Loss: 12246.229492\n",
      "Train Epoch: 1879 [65792/118836 (55%)] Loss: 12203.442383\n",
      "Train Epoch: 1879 [98560/118836 (83%)] Loss: 12270.347656\n",
      "    epoch          : 1879\n",
      "    loss           : 12247.370369849825\n",
      "    val_loss       : 12249.043698861467\n",
      "    val_log_likelihood: -12161.088083837365\n",
      "    val_log_marginal: -12169.261725915254\n",
      "Train Epoch: 1880 [256/118836 (0%)] Loss: 12254.824219\n",
      "Train Epoch: 1880 [33024/118836 (28%)] Loss: 12186.653320\n",
      "Train Epoch: 1880 [65792/118836 (55%)] Loss: 12290.728516\n",
      "Train Epoch: 1880 [98560/118836 (83%)] Loss: 12206.105469\n",
      "    epoch          : 1880\n",
      "    loss           : 12248.80545647229\n",
      "    val_loss       : 12248.769647199842\n",
      "    val_log_likelihood: -12164.38850095637\n",
      "    val_log_marginal: -12172.65092976363\n",
      "Train Epoch: 1881 [256/118836 (0%)] Loss: 12236.900391\n",
      "Train Epoch: 1881 [33024/118836 (28%)] Loss: 12226.953125\n",
      "Train Epoch: 1881 [65792/118836 (55%)] Loss: 12197.415039\n",
      "Train Epoch: 1881 [98560/118836 (83%)] Loss: 12338.048828\n",
      "    epoch          : 1881\n",
      "    loss           : 12246.972537026984\n",
      "    val_loss       : 12247.75092672499\n",
      "    val_log_likelihood: -12164.64783282284\n",
      "    val_log_marginal: -12172.977605602815\n",
      "Train Epoch: 1882 [256/118836 (0%)] Loss: 12293.020508\n",
      "Train Epoch: 1882 [33024/118836 (28%)] Loss: 12226.619141\n",
      "Train Epoch: 1882 [65792/118836 (55%)] Loss: 12181.582031\n",
      "Train Epoch: 1882 [98560/118836 (83%)] Loss: 12385.041992\n",
      "    epoch          : 1882\n",
      "    loss           : 12246.471203603185\n",
      "    val_loss       : 12244.42220312795\n",
      "    val_log_likelihood: -12162.079493641439\n",
      "    val_log_marginal: -12170.115254150043\n",
      "Train Epoch: 1883 [256/118836 (0%)] Loss: 12269.666016\n",
      "Train Epoch: 1883 [33024/118836 (28%)] Loss: 12253.821289\n",
      "Train Epoch: 1883 [65792/118836 (55%)] Loss: 12211.822266\n",
      "Train Epoch: 1883 [98560/118836 (83%)] Loss: 12285.264648\n",
      "    epoch          : 1883\n",
      "    loss           : 12249.491118854683\n",
      "    val_loss       : 12256.70259023283\n",
      "    val_log_likelihood: -12160.314241011425\n",
      "    val_log_marginal: -12168.81980828652\n",
      "Train Epoch: 1884 [256/118836 (0%)] Loss: 12223.961914\n",
      "Train Epoch: 1884 [33024/118836 (28%)] Loss: 12202.595703\n",
      "Train Epoch: 1884 [65792/118836 (55%)] Loss: 12385.957031\n",
      "Train Epoch: 1884 [98560/118836 (83%)] Loss: 12237.133789\n",
      "    epoch          : 1884\n",
      "    loss           : 12251.68054532413\n",
      "    val_loss       : 12250.249348718045\n",
      "    val_log_likelihood: -12162.45496342535\n",
      "    val_log_marginal: -12170.5057387768\n",
      "Train Epoch: 1885 [256/118836 (0%)] Loss: 12374.351562\n",
      "Train Epoch: 1885 [33024/118836 (28%)] Loss: 12230.416016\n",
      "Train Epoch: 1885 [65792/118836 (55%)] Loss: 12324.256836\n",
      "Train Epoch: 1885 [98560/118836 (83%)] Loss: 12326.634766\n",
      "    epoch          : 1885\n",
      "    loss           : 12248.324149445565\n",
      "    val_loss       : 12251.394394161729\n",
      "    val_log_likelihood: -12160.801217108665\n",
      "    val_log_marginal: -12168.950722924379\n",
      "Train Epoch: 1886 [256/118836 (0%)] Loss: 12354.602539\n",
      "Train Epoch: 1886 [33024/118836 (28%)] Loss: 12338.043945\n",
      "Train Epoch: 1886 [65792/118836 (55%)] Loss: 12410.520508\n",
      "Train Epoch: 1886 [98560/118836 (83%)] Loss: 12430.399414\n",
      "    epoch          : 1886\n",
      "    loss           : 12247.065883962729\n",
      "    val_loss       : 12246.895548834054\n",
      "    val_log_likelihood: -12161.64078800274\n",
      "    val_log_marginal: -12169.69505102581\n",
      "Train Epoch: 1887 [256/118836 (0%)] Loss: 12243.494141\n",
      "Train Epoch: 1887 [33024/118836 (28%)] Loss: 12234.690430\n",
      "Train Epoch: 1887 [65792/118836 (55%)] Loss: 12311.313477\n",
      "Train Epoch: 1887 [98560/118836 (83%)] Loss: 12213.274414\n",
      "    epoch          : 1887\n",
      "    loss           : 12245.881715260546\n",
      "    val_loss       : 12246.986909303287\n",
      "    val_log_likelihood: -12161.15391820461\n",
      "    val_log_marginal: -12169.317795078556\n",
      "Train Epoch: 1888 [256/118836 (0%)] Loss: 12232.769531\n",
      "Train Epoch: 1888 [33024/118836 (28%)] Loss: 12184.152344\n",
      "Train Epoch: 1888 [65792/118836 (55%)] Loss: 12249.628906\n",
      "Train Epoch: 1888 [98560/118836 (83%)] Loss: 12227.226562\n",
      "    epoch          : 1888\n",
      "    loss           : 12246.295292144852\n",
      "    val_loss       : 12249.267344239037\n",
      "    val_log_likelihood: -12162.042481357268\n",
      "    val_log_marginal: -12170.220288777886\n",
      "Train Epoch: 1889 [256/118836 (0%)] Loss: 12212.390625\n",
      "Train Epoch: 1889 [33024/118836 (28%)] Loss: 12159.679688\n",
      "Train Epoch: 1889 [65792/118836 (55%)] Loss: 12391.715820\n",
      "Train Epoch: 1889 [98560/118836 (83%)] Loss: 12327.139648\n",
      "    epoch          : 1889\n",
      "    loss           : 12250.028001576717\n",
      "    val_loss       : 12245.512481995236\n",
      "    val_log_likelihood: -12163.34289201432\n",
      "    val_log_marginal: -12171.675496220436\n",
      "Train Epoch: 1890 [256/118836 (0%)] Loss: 12273.745117\n",
      "Train Epoch: 1890 [33024/118836 (28%)] Loss: 12260.250977\n",
      "Train Epoch: 1890 [65792/118836 (55%)] Loss: 12304.334961\n",
      "Train Epoch: 1890 [98560/118836 (83%)] Loss: 12326.181641\n",
      "    epoch          : 1890\n",
      "    loss           : 12248.095478087522\n",
      "    val_loss       : 12250.611040009848\n",
      "    val_log_likelihood: -12161.396281631254\n",
      "    val_log_marginal: -12169.572000675098\n",
      "Train Epoch: 1891 [256/118836 (0%)] Loss: 12437.627930\n",
      "Train Epoch: 1891 [33024/118836 (28%)] Loss: 12190.306641\n",
      "Train Epoch: 1891 [65792/118836 (55%)] Loss: 12235.506836\n",
      "Train Epoch: 1891 [98560/118836 (83%)] Loss: 12200.777344\n",
      "    epoch          : 1891\n",
      "    loss           : 12250.3084722653\n",
      "    val_loss       : 12250.468798916354\n",
      "    val_log_likelihood: -12162.842289275744\n",
      "    val_log_marginal: -12170.822992558586\n",
      "Train Epoch: 1892 [256/118836 (0%)] Loss: 12333.133789\n",
      "Train Epoch: 1892 [33024/118836 (28%)] Loss: 12338.277344\n",
      "Train Epoch: 1892 [65792/118836 (55%)] Loss: 12265.750000\n",
      "Train Epoch: 1892 [98560/118836 (83%)] Loss: 12357.050781\n",
      "    epoch          : 1892\n",
      "    loss           : 12251.430429170543\n",
      "    val_loss       : 12244.03056368826\n",
      "    val_log_likelihood: -12161.494002339226\n",
      "    val_log_marginal: -12169.611650533234\n",
      "Train Epoch: 1893 [256/118836 (0%)] Loss: 12190.028320\n",
      "Train Epoch: 1893 [33024/118836 (28%)] Loss: 12230.719727\n",
      "Train Epoch: 1893 [65792/118836 (55%)] Loss: 12241.486328\n",
      "Train Epoch: 1893 [98560/118836 (83%)] Loss: 12213.070312\n",
      "    epoch          : 1893\n",
      "    loss           : 12246.218818496693\n",
      "    val_loss       : 12245.74396622424\n",
      "    val_log_likelihood: -12163.24813281896\n",
      "    val_log_marginal: -12171.307585482215\n",
      "Train Epoch: 1894 [256/118836 (0%)] Loss: 12159.909180\n",
      "Train Epoch: 1894 [33024/118836 (28%)] Loss: 12398.583984\n",
      "Train Epoch: 1894 [65792/118836 (55%)] Loss: 12290.722656\n",
      "Train Epoch: 1894 [98560/118836 (83%)] Loss: 12132.795898\n",
      "    epoch          : 1894\n",
      "    loss           : 12249.843913972034\n",
      "    val_loss       : 12248.193282286096\n",
      "    val_log_likelihood: -12162.282545136735\n",
      "    val_log_marginal: -12170.269853210972\n",
      "Train Epoch: 1895 [256/118836 (0%)] Loss: 12272.902344\n",
      "Train Epoch: 1895 [33024/118836 (28%)] Loss: 12356.422852\n",
      "Train Epoch: 1895 [65792/118836 (55%)] Loss: 12188.650391\n",
      "Train Epoch: 1895 [98560/118836 (83%)] Loss: 12257.555664\n",
      "    epoch          : 1895\n",
      "    loss           : 12246.01256591191\n",
      "    val_loss       : 12249.56070470509\n",
      "    val_log_likelihood: -12163.386156075529\n",
      "    val_log_marginal: -12171.501645365233\n",
      "Train Epoch: 1896 [256/118836 (0%)] Loss: 12225.024414\n",
      "Train Epoch: 1896 [33024/118836 (28%)] Loss: 12187.937500\n",
      "Train Epoch: 1896 [65792/118836 (55%)] Loss: 12404.980469\n",
      "Train Epoch: 1896 [98560/118836 (83%)] Loss: 12361.255859\n",
      "    epoch          : 1896\n",
      "    loss           : 12247.195867097033\n",
      "    val_loss       : 12248.252521364817\n",
      "    val_log_likelihood: -12162.421537201457\n",
      "    val_log_marginal: -12170.475213864422\n",
      "Train Epoch: 1897 [256/118836 (0%)] Loss: 12289.953125\n",
      "Train Epoch: 1897 [33024/118836 (28%)] Loss: 12196.958984\n",
      "Train Epoch: 1897 [65792/118836 (55%)] Loss: 12319.285156\n",
      "Train Epoch: 1897 [98560/118836 (83%)] Loss: 12307.112305\n",
      "    epoch          : 1897\n",
      "    loss           : 12252.34634560458\n",
      "    val_loss       : 12247.913364084716\n",
      "    val_log_likelihood: -12165.133266129033\n",
      "    val_log_marginal: -12173.340599084178\n",
      "Train Epoch: 1898 [256/118836 (0%)] Loss: 12345.357422\n",
      "Train Epoch: 1898 [33024/118836 (28%)] Loss: 12218.588867\n",
      "Train Epoch: 1898 [65792/118836 (55%)] Loss: 12327.903320\n",
      "Train Epoch: 1898 [98560/118836 (83%)] Loss: 12207.602539\n",
      "    epoch          : 1898\n",
      "    loss           : 12246.760073860112\n",
      "    val_loss       : 12249.295800210535\n",
      "    val_log_likelihood: -12161.105296700527\n",
      "    val_log_marginal: -12169.227372510642\n",
      "Train Epoch: 1899 [256/118836 (0%)] Loss: 12284.898438\n",
      "Train Epoch: 1899 [33024/118836 (28%)] Loss: 12240.792969\n",
      "Train Epoch: 1899 [65792/118836 (55%)] Loss: 12415.482422\n",
      "Train Epoch: 1899 [98560/118836 (83%)] Loss: 12178.718750\n",
      "    epoch          : 1899\n",
      "    loss           : 12244.177672824906\n",
      "    val_loss       : 12249.664552146416\n",
      "    val_log_likelihood: -12162.201196753516\n",
      "    val_log_marginal: -12170.48527929873\n",
      "Train Epoch: 1900 [256/118836 (0%)] Loss: 12289.297852\n",
      "Train Epoch: 1900 [33024/118836 (28%)] Loss: 12236.295898\n",
      "Train Epoch: 1900 [65792/118836 (55%)] Loss: 12188.328125\n",
      "Train Epoch: 1900 [98560/118836 (83%)] Loss: 12267.771484\n",
      "    epoch          : 1900\n",
      "    loss           : 12249.296134944945\n",
      "    val_loss       : 12248.098726005486\n",
      "    val_log_likelihood: -12160.01183716527\n",
      "    val_log_marginal: -12168.10433965741\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch1900.pth ...\n",
      "Train Epoch: 1901 [256/118836 (0%)] Loss: 12268.611328\n",
      "Train Epoch: 1901 [33024/118836 (28%)] Loss: 12292.712891\n",
      "Train Epoch: 1901 [65792/118836 (55%)] Loss: 12237.578125\n",
      "Train Epoch: 1901 [98560/118836 (83%)] Loss: 12259.648438\n",
      "    epoch          : 1901\n",
      "    loss           : 12246.924372867556\n",
      "    val_loss       : 12247.803181276638\n",
      "    val_log_likelihood: -12162.431988924214\n",
      "    val_log_marginal: -12170.705567104878\n",
      "Train Epoch: 1902 [256/118836 (0%)] Loss: 12254.484375\n",
      "Train Epoch: 1902 [33024/118836 (28%)] Loss: 12265.849609\n",
      "Train Epoch: 1902 [65792/118836 (55%)] Loss: 12196.485352\n",
      "Train Epoch: 1902 [98560/118836 (83%)] Loss: 12213.861328\n",
      "    epoch          : 1902\n",
      "    loss           : 12244.143844829145\n",
      "    val_loss       : 12247.198867266237\n",
      "    val_log_likelihood: -12161.390793010753\n",
      "    val_log_marginal: -12169.47375554982\n",
      "Train Epoch: 1903 [256/118836 (0%)] Loss: 12158.140625\n",
      "Train Epoch: 1903 [33024/118836 (28%)] Loss: 12340.668945\n",
      "Train Epoch: 1903 [65792/118836 (55%)] Loss: 12274.939453\n",
      "Train Epoch: 1903 [98560/118836 (83%)] Loss: 12235.527344\n",
      "    epoch          : 1903\n",
      "    loss           : 12246.46887390793\n",
      "    val_loss       : 12248.54149667484\n",
      "    val_log_likelihood: -12161.958775330851\n",
      "    val_log_marginal: -12170.043053691003\n",
      "Train Epoch: 1904 [256/118836 (0%)] Loss: 12213.738281\n",
      "Train Epoch: 1904 [33024/118836 (28%)] Loss: 12235.077148\n",
      "Train Epoch: 1904 [65792/118836 (55%)] Loss: 12276.509766\n",
      "Train Epoch: 1904 [98560/118836 (83%)] Loss: 12245.712891\n",
      "    epoch          : 1904\n",
      "    loss           : 12245.356797488885\n",
      "    val_loss       : 12250.134682504364\n",
      "    val_log_likelihood: -12162.539090447943\n",
      "    val_log_marginal: -12170.568348373232\n",
      "Train Epoch: 1905 [256/118836 (0%)] Loss: 12264.500000\n",
      "Train Epoch: 1905 [33024/118836 (28%)] Loss: 12273.503906\n",
      "Train Epoch: 1905 [65792/118836 (55%)] Loss: 12158.815430\n",
      "Train Epoch: 1905 [98560/118836 (83%)] Loss: 12278.973633\n",
      "    epoch          : 1905\n",
      "    loss           : 12247.778067488627\n",
      "    val_loss       : 12244.736272290163\n",
      "    val_log_likelihood: -12160.956448866573\n",
      "    val_log_marginal: -12168.952032359362\n",
      "Train Epoch: 1906 [256/118836 (0%)] Loss: 12205.275391\n",
      "Train Epoch: 1906 [33024/118836 (28%)] Loss: 12281.257812\n",
      "Train Epoch: 1906 [65792/118836 (55%)] Loss: 12348.832031\n",
      "Train Epoch: 1906 [98560/118836 (83%)] Loss: 12238.986328\n",
      "    epoch          : 1906\n",
      "    loss           : 12247.461934256102\n",
      "    val_loss       : 12247.261184441308\n",
      "    val_log_likelihood: -12161.385462384977\n",
      "    val_log_marginal: -12169.617157765702\n",
      "Train Epoch: 1907 [256/118836 (0%)] Loss: 12212.703125\n",
      "Train Epoch: 1907 [33024/118836 (28%)] Loss: 12285.480469\n",
      "Train Epoch: 1907 [65792/118836 (55%)] Loss: 12333.625000\n",
      "Train Epoch: 1907 [98560/118836 (83%)] Loss: 12256.415039\n",
      "    epoch          : 1907\n",
      "    loss           : 12245.724354935639\n",
      "    val_loss       : 12248.956680700183\n",
      "    val_log_likelihood: -12162.011212617608\n",
      "    val_log_marginal: -12170.333615337395\n",
      "Train Epoch: 1908 [256/118836 (0%)] Loss: 12289.582031\n",
      "Train Epoch: 1908 [33024/118836 (28%)] Loss: 12299.272461\n",
      "Train Epoch: 1908 [65792/118836 (55%)] Loss: 12257.868164\n",
      "Train Epoch: 1908 [98560/118836 (83%)] Loss: 12248.853516\n",
      "    epoch          : 1908\n",
      "    loss           : 12252.477598997106\n",
      "    val_loss       : 12246.47116640529\n",
      "    val_log_likelihood: -12163.516354715932\n",
      "    val_log_marginal: -12171.739250138851\n",
      "Train Epoch: 1909 [256/118836 (0%)] Loss: 12306.476562\n",
      "Train Epoch: 1909 [33024/118836 (28%)] Loss: 12337.464844\n",
      "Train Epoch: 1909 [65792/118836 (55%)] Loss: 12247.216797\n",
      "Train Epoch: 1909 [98560/118836 (83%)] Loss: 12342.715820\n",
      "    epoch          : 1909\n",
      "    loss           : 12252.254047282102\n",
      "    val_loss       : 12242.105176625153\n",
      "    val_log_likelihood: -12161.895396182279\n",
      "    val_log_marginal: -12169.980885643807\n",
      "Train Epoch: 1910 [256/118836 (0%)] Loss: 12471.432617\n",
      "Train Epoch: 1910 [33024/118836 (28%)] Loss: 12220.628906\n",
      "Train Epoch: 1910 [65792/118836 (55%)] Loss: 12225.807617\n",
      "Train Epoch: 1910 [98560/118836 (83%)] Loss: 12260.036133\n",
      "    epoch          : 1910\n",
      "    loss           : 12247.436905015767\n",
      "    val_loss       : 12246.106800228808\n",
      "    val_log_likelihood: -12162.90500672043\n",
      "    val_log_marginal: -12171.026894443366\n",
      "Train Epoch: 1911 [256/118836 (0%)] Loss: 12237.890625\n",
      "Train Epoch: 1911 [33024/118836 (28%)] Loss: 12289.234375\n",
      "Train Epoch: 1911 [65792/118836 (55%)] Loss: 12260.656250\n",
      "Train Epoch: 1911 [98560/118836 (83%)] Loss: 12349.939453\n",
      "    epoch          : 1911\n",
      "    loss           : 12244.891801236818\n",
      "    val_loss       : 12249.878082369849\n",
      "    val_log_likelihood: -12156.90831895549\n",
      "    val_log_marginal: -12164.97327311942\n",
      "Train Epoch: 1912 [256/118836 (0%)] Loss: 12284.410156\n",
      "Train Epoch: 1912 [33024/118836 (28%)] Loss: 12213.634766\n",
      "Train Epoch: 1912 [65792/118836 (55%)] Loss: 12290.474609\n",
      "Train Epoch: 1912 [98560/118836 (83%)] Loss: 12269.444336\n",
      "    epoch          : 1912\n",
      "    loss           : 12242.223303899142\n",
      "    val_loss       : 12249.2844424345\n",
      "    val_log_likelihood: -12160.448763020833\n",
      "    val_log_marginal: -12168.617200240791\n",
      "Train Epoch: 1913 [256/118836 (0%)] Loss: 12342.074219\n",
      "Train Epoch: 1913 [33024/118836 (28%)] Loss: 12363.140625\n",
      "Train Epoch: 1913 [65792/118836 (55%)] Loss: 12246.030273\n",
      "Train Epoch: 1913 [98560/118836 (83%)] Loss: 12251.485352\n",
      "    epoch          : 1913\n",
      "    loss           : 12246.262433280344\n",
      "    val_loss       : 12248.356387542039\n",
      "    val_log_likelihood: -12160.175081097499\n",
      "    val_log_marginal: -12168.433240334241\n",
      "Train Epoch: 1914 [256/118836 (0%)] Loss: 12249.484375\n",
      "Train Epoch: 1914 [33024/118836 (28%)] Loss: 12198.097656\n",
      "Train Epoch: 1914 [65792/118836 (55%)] Loss: 12262.316406\n",
      "Train Epoch: 1914 [98560/118836 (83%)] Loss: 12219.181641\n",
      "    epoch          : 1914\n",
      "    loss           : 12248.642524813897\n",
      "    val_loss       : 12247.251714993914\n",
      "    val_log_likelihood: -12160.101585924574\n",
      "    val_log_marginal: -12168.62387270189\n",
      "Train Epoch: 1915 [256/118836 (0%)] Loss: 12150.799805\n",
      "Train Epoch: 1915 [33024/118836 (28%)] Loss: 12287.296875\n",
      "Train Epoch: 1915 [65792/118836 (55%)] Loss: 12221.013672\n",
      "Train Epoch: 1915 [98560/118836 (83%)] Loss: 12171.426758\n",
      "    epoch          : 1915\n",
      "    loss           : 12245.091273779983\n",
      "    val_loss       : 12245.363035446208\n",
      "    val_log_likelihood: -12160.370648683054\n",
      "    val_log_marginal: -12168.74411688564\n",
      "Train Epoch: 1916 [256/118836 (0%)] Loss: 12252.271484\n",
      "Train Epoch: 1916 [33024/118836 (28%)] Loss: 12274.186523\n",
      "Train Epoch: 1916 [65792/118836 (55%)] Loss: 12331.250000\n",
      "Train Epoch: 1916 [98560/118836 (83%)] Loss: 12237.900391\n",
      "    epoch          : 1916\n",
      "    loss           : 12246.582558222188\n",
      "    val_loss       : 12248.640573898561\n",
      "    val_log_likelihood: -12158.781127384462\n",
      "    val_log_marginal: -12166.933222967937\n",
      "Train Epoch: 1917 [256/118836 (0%)] Loss: 12175.643555\n",
      "Train Epoch: 1917 [33024/118836 (28%)] Loss: 12330.929688\n",
      "Train Epoch: 1917 [65792/118836 (55%)] Loss: 12244.560547\n",
      "Train Epoch: 1917 [98560/118836 (83%)] Loss: 12264.797852\n",
      "    epoch          : 1917\n",
      "    loss           : 12246.041269418167\n",
      "    val_loss       : 12253.043349112064\n",
      "    val_log_likelihood: -12160.915848098894\n",
      "    val_log_marginal: -12169.11375332654\n",
      "Train Epoch: 1918 [256/118836 (0%)] Loss: 12254.209961\n",
      "Train Epoch: 1918 [33024/118836 (28%)] Loss: 12381.703125\n",
      "Train Epoch: 1918 [65792/118836 (55%)] Loss: 12271.171875\n",
      "Train Epoch: 1918 [98560/118836 (83%)] Loss: 12241.404297\n",
      "    epoch          : 1918\n",
      "    loss           : 12247.52103591553\n",
      "    val_loss       : 12246.974981313764\n",
      "    val_log_likelihood: -12157.2686086448\n",
      "    val_log_marginal: -12165.54667644981\n",
      "Train Epoch: 1919 [256/118836 (0%)] Loss: 12393.651367\n",
      "Train Epoch: 1919 [33024/118836 (28%)] Loss: 12250.118164\n",
      "Train Epoch: 1919 [65792/118836 (55%)] Loss: 12276.269531\n",
      "Train Epoch: 1919 [98560/118836 (83%)] Loss: 12178.408203\n",
      "    epoch          : 1919\n",
      "    loss           : 12245.040011760753\n",
      "    val_loss       : 12246.993289736693\n",
      "    val_log_likelihood: -12156.628873455593\n",
      "    val_log_marginal: -12164.70805627122\n",
      "Train Epoch: 1920 [256/118836 (0%)] Loss: 12316.432617\n",
      "Train Epoch: 1920 [33024/118836 (28%)] Loss: 12288.759766\n",
      "Train Epoch: 1920 [65792/118836 (55%)] Loss: 12187.267578\n",
      "Train Epoch: 1920 [98560/118836 (83%)] Loss: 12336.169922\n",
      "    epoch          : 1920\n",
      "    loss           : 12249.736129581524\n",
      "    val_loss       : 12249.789311302166\n",
      "    val_log_likelihood: -12160.202339872829\n",
      "    val_log_marginal: -12168.444377456906\n",
      "Train Epoch: 1921 [256/118836 (0%)] Loss: 12250.872070\n",
      "Train Epoch: 1921 [33024/118836 (28%)] Loss: 12197.708008\n",
      "Train Epoch: 1921 [65792/118836 (55%)] Loss: 12311.458984\n",
      "Train Epoch: 1921 [98560/118836 (83%)] Loss: 12256.361328\n",
      "    epoch          : 1921\n",
      "    loss           : 12247.235265133892\n",
      "    val_loss       : 12246.528851812905\n",
      "    val_log_likelihood: -12160.143187163978\n",
      "    val_log_marginal: -12168.610825580954\n",
      "Train Epoch: 1922 [256/118836 (0%)] Loss: 12174.864258\n",
      "Train Epoch: 1922 [33024/118836 (28%)] Loss: 12337.842773\n",
      "Train Epoch: 1922 [65792/118836 (55%)] Loss: 12316.089844\n",
      "Train Epoch: 1922 [98560/118836 (83%)] Loss: 12365.995117\n",
      "    epoch          : 1922\n",
      "    loss           : 12245.001568800404\n",
      "    val_loss       : 12249.997228017695\n",
      "    val_log_likelihood: -12162.096108774038\n",
      "    val_log_marginal: -12170.637069644015\n",
      "Train Epoch: 1923 [256/118836 (0%)] Loss: 12258.083008\n",
      "Train Epoch: 1923 [33024/118836 (28%)] Loss: 12293.885742\n",
      "Train Epoch: 1923 [65792/118836 (55%)] Loss: 12474.735352\n",
      "Train Epoch: 1923 [98560/118836 (83%)] Loss: 12272.494141\n",
      "    epoch          : 1923\n",
      "    loss           : 12244.551426152813\n",
      "    val_loss       : 12249.158245678176\n",
      "    val_log_likelihood: -12158.382340454404\n",
      "    val_log_marginal: -12166.566577408008\n",
      "Train Epoch: 1924 [256/118836 (0%)] Loss: 12218.486328\n",
      "Train Epoch: 1924 [33024/118836 (28%)] Loss: 12246.960938\n",
      "Train Epoch: 1924 [65792/118836 (55%)] Loss: 12304.508789\n",
      "Train Epoch: 1924 [98560/118836 (83%)] Loss: 12263.011719\n",
      "    epoch          : 1924\n",
      "    loss           : 12246.763086745243\n",
      "    val_loss       : 12248.883257106743\n",
      "    val_log_likelihood: -12159.988264448924\n",
      "    val_log_marginal: -12168.261016351571\n",
      "Train Epoch: 1925 [256/118836 (0%)] Loss: 12188.382812\n",
      "Train Epoch: 1925 [33024/118836 (28%)] Loss: 12241.999023\n",
      "Train Epoch: 1925 [65792/118836 (55%)] Loss: 12225.264648\n",
      "Train Epoch: 1925 [98560/118836 (83%)] Loss: 12310.591797\n",
      "    epoch          : 1925\n",
      "    loss           : 12249.700267363265\n",
      "    val_loss       : 12251.305789690214\n",
      "    val_log_likelihood: -12167.211305508168\n",
      "    val_log_marginal: -12175.706245452431\n",
      "Train Epoch: 1926 [256/118836 (0%)] Loss: 12184.062500\n",
      "Train Epoch: 1926 [33024/118836 (28%)] Loss: 12335.688477\n",
      "Train Epoch: 1926 [65792/118836 (55%)] Loss: 12221.905273\n",
      "Train Epoch: 1926 [98560/118836 (83%)] Loss: 12182.697266\n",
      "    epoch          : 1926\n",
      "    loss           : 12243.675850392887\n",
      "    val_loss       : 12247.937561950697\n",
      "    val_log_likelihood: -12157.679306729477\n",
      "    val_log_marginal: -12165.79314929537\n",
      "Train Epoch: 1927 [256/118836 (0%)] Loss: 12218.060547\n",
      "Train Epoch: 1927 [33024/118836 (28%)] Loss: 12258.724609\n",
      "Train Epoch: 1927 [65792/118836 (55%)] Loss: 12224.767578\n",
      "Train Epoch: 1927 [98560/118836 (83%)] Loss: 12236.909180\n",
      "    epoch          : 1927\n",
      "    loss           : 12243.67376592871\n",
      "    val_loss       : 12247.910865455577\n",
      "    val_log_likelihood: -12159.478516109646\n",
      "    val_log_marginal: -12167.793009201518\n",
      "Train Epoch: 1928 [256/118836 (0%)] Loss: 12340.964844\n",
      "Train Epoch: 1928 [33024/118836 (28%)] Loss: 12217.845703\n",
      "Train Epoch: 1928 [65792/118836 (55%)] Loss: 12254.231445\n",
      "Train Epoch: 1928 [98560/118836 (83%)] Loss: 12275.951172\n",
      "    epoch          : 1928\n",
      "    loss           : 12244.04966850186\n",
      "    val_loss       : 12244.878803563415\n",
      "    val_log_likelihood: -12156.665887839898\n",
      "    val_log_marginal: -12164.95562039627\n",
      "Train Epoch: 1929 [256/118836 (0%)] Loss: 12325.210938\n",
      "Train Epoch: 1929 [33024/118836 (28%)] Loss: 12166.364258\n",
      "Train Epoch: 1929 [65792/118836 (55%)] Loss: 12211.484375\n",
      "Train Epoch: 1929 [98560/118836 (83%)] Loss: 12231.935547\n",
      "    epoch          : 1929\n",
      "    loss           : 12244.218657755635\n",
      "    val_loss       : 12247.491953357921\n",
      "    val_log_likelihood: -12157.398294206214\n",
      "    val_log_marginal: -12165.45390888087\n",
      "Train Epoch: 1930 [256/118836 (0%)] Loss: 12247.272461\n",
      "Train Epoch: 1930 [33024/118836 (28%)] Loss: 12271.727539\n",
      "Train Epoch: 1930 [65792/118836 (55%)] Loss: 12231.873047\n",
      "Train Epoch: 1930 [98560/118836 (83%)] Loss: 12245.874023\n",
      "    epoch          : 1930\n",
      "    loss           : 12243.26228061673\n",
      "    val_loss       : 12245.903379494992\n",
      "    val_log_likelihood: -12157.505128851322\n",
      "    val_log_marginal: -12165.740346879718\n",
      "Train Epoch: 1931 [256/118836 (0%)] Loss: 12269.290039\n",
      "Train Epoch: 1931 [33024/118836 (28%)] Loss: 12317.585938\n",
      "Train Epoch: 1931 [65792/118836 (55%)] Loss: 12218.231445\n",
      "Train Epoch: 1931 [98560/118836 (83%)] Loss: 12222.300781\n",
      "    epoch          : 1931\n",
      "    loss           : 12241.393211719396\n",
      "    val_loss       : 12242.4969573209\n",
      "    val_log_likelihood: -12157.499460588553\n",
      "    val_log_marginal: -12165.593095603117\n",
      "Train Epoch: 1932 [256/118836 (0%)] Loss: 12206.480469\n",
      "Train Epoch: 1932 [33024/118836 (28%)] Loss: 12427.220703\n",
      "Train Epoch: 1932 [65792/118836 (55%)] Loss: 12216.211914\n",
      "Train Epoch: 1932 [98560/118836 (83%)] Loss: 12206.421875\n",
      "    epoch          : 1932\n",
      "    loss           : 12246.178590906742\n",
      "    val_loss       : 12246.805655847033\n",
      "    val_log_likelihood: -12156.754889436\n",
      "    val_log_marginal: -12164.804337261607\n",
      "Train Epoch: 1933 [256/118836 (0%)] Loss: 12193.454102\n",
      "Train Epoch: 1933 [33024/118836 (28%)] Loss: 12267.375000\n",
      "Train Epoch: 1933 [65792/118836 (55%)] Loss: 12325.388672\n",
      "Train Epoch: 1933 [98560/118836 (83%)] Loss: 12265.259766\n",
      "    epoch          : 1933\n",
      "    loss           : 12250.630831103961\n",
      "    val_loss       : 12248.751726819768\n",
      "    val_log_likelihood: -12159.441104153742\n",
      "    val_log_marginal: -12167.920076025863\n",
      "Train Epoch: 1934 [256/118836 (0%)] Loss: 12245.957031\n",
      "Train Epoch: 1934 [33024/118836 (28%)] Loss: 12282.745117\n",
      "Train Epoch: 1934 [65792/118836 (55%)] Loss: 12358.302734\n",
      "Train Epoch: 1934 [98560/118836 (83%)] Loss: 12258.916016\n",
      "    epoch          : 1934\n",
      "    loss           : 12244.129083792131\n",
      "    val_loss       : 12239.292549700476\n",
      "    val_log_likelihood: -12157.009523624896\n",
      "    val_log_marginal: -12165.308717348877\n",
      "Train Epoch: 1935 [256/118836 (0%)] Loss: 12274.772461\n",
      "Train Epoch: 1935 [33024/118836 (28%)] Loss: 12280.089844\n",
      "Train Epoch: 1935 [65792/118836 (55%)] Loss: 12316.203125\n",
      "Train Epoch: 1935 [98560/118836 (83%)] Loss: 12338.886719\n",
      "    epoch          : 1935\n",
      "    loss           : 12247.69900599023\n",
      "    val_loss       : 12250.062949125837\n",
      "    val_log_likelihood: -12158.08827058778\n",
      "    val_log_marginal: -12166.18696234117\n",
      "Train Epoch: 1936 [256/118836 (0%)] Loss: 12274.224609\n",
      "Train Epoch: 1936 [33024/118836 (28%)] Loss: 12214.349609\n",
      "Train Epoch: 1936 [65792/118836 (55%)] Loss: 12203.249023\n",
      "Train Epoch: 1936 [98560/118836 (83%)] Loss: 12335.146484\n",
      "    epoch          : 1936\n",
      "    loss           : 12244.188041511581\n",
      "    val_loss       : 12242.53253921365\n",
      "    val_log_likelihood: -12159.944051126964\n",
      "    val_log_marginal: -12168.132309496548\n",
      "Train Epoch: 1937 [256/118836 (0%)] Loss: 12273.028320\n",
      "Train Epoch: 1937 [33024/118836 (28%)] Loss: 12244.250000\n",
      "Train Epoch: 1937 [65792/118836 (55%)] Loss: 12244.654297\n",
      "Train Epoch: 1937 [98560/118836 (83%)] Loss: 12275.134766\n",
      "    epoch          : 1937\n",
      "    loss           : 12241.490556987956\n",
      "    val_loss       : 12244.418421085607\n",
      "    val_log_likelihood: -12156.818044516389\n",
      "    val_log_marginal: -12164.86845522859\n",
      "Train Epoch: 1938 [256/118836 (0%)] Loss: 12195.024414\n",
      "Train Epoch: 1938 [33024/118836 (28%)] Loss: 12244.963867\n",
      "Train Epoch: 1938 [65792/118836 (55%)] Loss: 12315.488281\n",
      "Train Epoch: 1938 [98560/118836 (83%)] Loss: 12324.976562\n",
      "    epoch          : 1938\n",
      "    loss           : 12240.960119093776\n",
      "    val_loss       : 12241.559875014967\n",
      "    val_log_likelihood: -12158.6731951768\n",
      "    val_log_marginal: -12166.653556060457\n",
      "Train Epoch: 1939 [256/118836 (0%)] Loss: 12189.597656\n",
      "Train Epoch: 1939 [33024/118836 (28%)] Loss: 12213.130859\n",
      "Train Epoch: 1939 [65792/118836 (55%)] Loss: 12234.497070\n",
      "Train Epoch: 1939 [98560/118836 (83%)] Loss: 12217.366211\n",
      "    epoch          : 1939\n",
      "    loss           : 12243.889556516233\n",
      "    val_loss       : 12239.824399752608\n",
      "    val_log_likelihood: -12156.606178110784\n",
      "    val_log_marginal: -12164.698409883238\n",
      "Train Epoch: 1940 [256/118836 (0%)] Loss: 12246.019531\n",
      "Train Epoch: 1940 [33024/118836 (28%)] Loss: 12209.827148\n",
      "Train Epoch: 1940 [65792/118836 (55%)] Loss: 12288.777344\n",
      "Train Epoch: 1940 [98560/118836 (83%)] Loss: 12258.019531\n",
      "    epoch          : 1940\n",
      "    loss           : 12243.155374728598\n",
      "    val_loss       : 12242.883850877357\n",
      "    val_log_likelihood: -12159.190743092173\n",
      "    val_log_marginal: -12167.252556997177\n",
      "Train Epoch: 1941 [256/118836 (0%)] Loss: 12321.748047\n",
      "Train Epoch: 1941 [33024/118836 (28%)] Loss: 12319.624023\n",
      "Train Epoch: 1941 [65792/118836 (55%)] Loss: 12198.614258\n",
      "Train Epoch: 1941 [98560/118836 (83%)] Loss: 12253.522461\n",
      "    epoch          : 1941\n",
      "    loss           : 12244.067310923283\n",
      "    val_loss       : 12246.573044203573\n",
      "    val_log_likelihood: -12158.492161329095\n",
      "    val_log_marginal: -12166.538270472867\n",
      "Train Epoch: 1942 [256/118836 (0%)] Loss: 12246.175781\n",
      "Train Epoch: 1942 [33024/118836 (28%)] Loss: 12260.708984\n",
      "Train Epoch: 1942 [65792/118836 (55%)] Loss: 12292.256836\n",
      "Train Epoch: 1942 [98560/118836 (83%)] Loss: 12308.017578\n",
      "    epoch          : 1942\n",
      "    loss           : 12241.814189638906\n",
      "    val_loss       : 12243.974642264011\n",
      "    val_log_likelihood: -12158.734698582248\n",
      "    val_log_marginal: -12166.896012186704\n",
      "Train Epoch: 1943 [256/118836 (0%)] Loss: 12340.015625\n",
      "Train Epoch: 1943 [33024/118836 (28%)] Loss: 12259.297852\n",
      "Train Epoch: 1943 [65792/118836 (55%)] Loss: 12310.473633\n",
      "Train Epoch: 1943 [98560/118836 (83%)] Loss: 12150.923828\n",
      "    epoch          : 1943\n",
      "    loss           : 12244.584204566016\n",
      "    val_loss       : 12246.102255640466\n",
      "    val_log_likelihood: -12158.989727757962\n",
      "    val_log_marginal: -12167.125450227966\n",
      "Train Epoch: 1944 [256/118836 (0%)] Loss: 12254.072266\n",
      "Train Epoch: 1944 [33024/118836 (28%)] Loss: 12201.586914\n",
      "Train Epoch: 1944 [65792/118836 (55%)] Loss: 12399.600586\n",
      "Train Epoch: 1944 [98560/118836 (83%)] Loss: 12345.108398\n",
      "    epoch          : 1944\n",
      "    loss           : 12244.027645361612\n",
      "    val_loss       : 12243.299275782938\n",
      "    val_log_likelihood: -12155.205684902294\n",
      "    val_log_marginal: -12163.414491801956\n",
      "Train Epoch: 1945 [256/118836 (0%)] Loss: 12256.676758\n",
      "Train Epoch: 1945 [33024/118836 (28%)] Loss: 12263.580078\n",
      "Train Epoch: 1945 [65792/118836 (55%)] Loss: 12314.177734\n",
      "Train Epoch: 1945 [98560/118836 (83%)] Loss: 12293.281250\n",
      "    epoch          : 1945\n",
      "    loss           : 12240.920868550971\n",
      "    val_loss       : 12240.303374964322\n",
      "    val_log_likelihood: -12158.848596948666\n",
      "    val_log_marginal: -12167.034233071981\n",
      "Train Epoch: 1946 [256/118836 (0%)] Loss: 12263.456055\n",
      "Train Epoch: 1946 [33024/118836 (28%)] Loss: 12286.791016\n",
      "Train Epoch: 1946 [65792/118836 (55%)] Loss: 12399.316406\n",
      "Train Epoch: 1946 [98560/118836 (83%)] Loss: 12232.103516\n",
      "    epoch          : 1946\n",
      "    loss           : 12239.397451083023\n",
      "    val_loss       : 12247.223991661294\n",
      "    val_log_likelihood: -12156.548160766904\n",
      "    val_log_marginal: -12164.77919162643\n",
      "Train Epoch: 1947 [256/118836 (0%)] Loss: 12272.569336\n",
      "Train Epoch: 1947 [33024/118836 (28%)] Loss: 12274.134766\n",
      "Train Epoch: 1947 [65792/118836 (55%)] Loss: 12187.377930\n",
      "Train Epoch: 1947 [98560/118836 (83%)] Loss: 12180.134766\n",
      "    epoch          : 1947\n",
      "    loss           : 12242.45796225574\n",
      "    val_loss       : 12245.318692344601\n",
      "    val_log_likelihood: -12157.838046196495\n",
      "    val_log_marginal: -12166.004622024393\n",
      "Train Epoch: 1948 [256/118836 (0%)] Loss: 12248.617188\n",
      "Train Epoch: 1948 [33024/118836 (28%)] Loss: 12306.289062\n",
      "Train Epoch: 1948 [65792/118836 (55%)] Loss: 12216.532227\n",
      "Train Epoch: 1948 [98560/118836 (83%)] Loss: 12191.644531\n",
      "    epoch          : 1948\n",
      "    loss           : 12244.089068154208\n",
      "    val_loss       : 12246.568395759856\n",
      "    val_log_likelihood: -12157.737957506204\n",
      "    val_log_marginal: -12165.734184231398\n",
      "Train Epoch: 1949 [256/118836 (0%)] Loss: 12238.067383\n",
      "Train Epoch: 1949 [33024/118836 (28%)] Loss: 12360.888672\n",
      "Train Epoch: 1949 [65792/118836 (55%)] Loss: 12288.169922\n",
      "Train Epoch: 1949 [98560/118836 (83%)] Loss: 12347.224609\n",
      "    epoch          : 1949\n",
      "    loss           : 12246.468800403225\n",
      "    val_loss       : 12251.329041753988\n",
      "    val_log_likelihood: -12161.561728766026\n",
      "    val_log_marginal: -12169.826071527992\n",
      "Train Epoch: 1950 [256/118836 (0%)] Loss: 12437.188477\n",
      "Train Epoch: 1950 [33024/118836 (28%)] Loss: 12236.691406\n",
      "Train Epoch: 1950 [65792/118836 (55%)] Loss: 12278.000000\n",
      "Train Epoch: 1950 [98560/118836 (83%)] Loss: 12300.701172\n",
      "    epoch          : 1950\n",
      "    loss           : 12245.895169852409\n",
      "    val_loss       : 12246.251541494328\n",
      "    val_log_likelihood: -12157.231255977307\n",
      "    val_log_marginal: -12165.3983558336\n",
      "Train Epoch: 1951 [256/118836 (0%)] Loss: 12232.304688\n",
      "Train Epoch: 1951 [33024/118836 (28%)] Loss: 12218.159180\n",
      "Train Epoch: 1951 [65792/118836 (55%)] Loss: 12191.791992\n",
      "Train Epoch: 1951 [98560/118836 (83%)] Loss: 12204.686523\n",
      "    epoch          : 1951\n",
      "    loss           : 12243.486510836694\n",
      "    val_loss       : 12242.84423373193\n",
      "    val_log_likelihood: -12160.064825494987\n",
      "    val_log_marginal: -12168.267717484183\n",
      "Train Epoch: 1952 [256/118836 (0%)] Loss: 12388.792969\n",
      "Train Epoch: 1952 [33024/118836 (28%)] Loss: 12358.568359\n",
      "Train Epoch: 1952 [65792/118836 (55%)] Loss: 12318.996094\n",
      "Train Epoch: 1952 [98560/118836 (83%)] Loss: 12213.340820\n",
      "    epoch          : 1952\n",
      "    loss           : 12245.581012200166\n",
      "    val_loss       : 12242.404686514137\n",
      "    val_log_likelihood: -12157.394158880013\n",
      "    val_log_marginal: -12165.448504680928\n",
      "Train Epoch: 1953 [256/118836 (0%)] Loss: 12267.705078\n",
      "Train Epoch: 1953 [33024/118836 (28%)] Loss: 12290.131836\n",
      "Train Epoch: 1953 [65792/118836 (55%)] Loss: 12282.294922\n",
      "Train Epoch: 1953 [98560/118836 (83%)] Loss: 12390.881836\n",
      "    epoch          : 1953\n",
      "    loss           : 12248.102492859543\n",
      "    val_loss       : 12266.45882070612\n",
      "    val_log_likelihood: -12156.63114353934\n",
      "    val_log_marginal: -12164.960151829227\n",
      "Train Epoch: 1954 [256/118836 (0%)] Loss: 12214.424805\n",
      "Train Epoch: 1954 [33024/118836 (28%)] Loss: 12290.495117\n",
      "Train Epoch: 1954 [65792/118836 (55%)] Loss: 12452.205078\n",
      "Train Epoch: 1954 [98560/118836 (83%)] Loss: 12213.182617\n",
      "    epoch          : 1954\n",
      "    loss           : 12251.484303756979\n",
      "    val_loss       : 12242.070528538226\n",
      "    val_log_likelihood: -12156.60887258323\n",
      "    val_log_marginal: -12164.762054683843\n",
      "Train Epoch: 1955 [256/118836 (0%)] Loss: 12229.525391\n",
      "Train Epoch: 1955 [33024/118836 (28%)] Loss: 12154.315430\n",
      "Train Epoch: 1955 [65792/118836 (55%)] Loss: 12151.316406\n",
      "Train Epoch: 1955 [98560/118836 (83%)] Loss: 12195.719727\n",
      "    epoch          : 1955\n",
      "    loss           : 12241.29734623785\n",
      "    val_loss       : 12245.594142081973\n",
      "    val_log_likelihood: -12157.592053899141\n",
      "    val_log_marginal: -12166.162611847063\n",
      "Train Epoch: 1956 [256/118836 (0%)] Loss: 12236.669922\n",
      "Train Epoch: 1956 [33024/118836 (28%)] Loss: 12398.109375\n",
      "Train Epoch: 1956 [65792/118836 (55%)] Loss: 12162.129883\n",
      "Train Epoch: 1956 [98560/118836 (83%)] Loss: 12236.026367\n",
      "    epoch          : 1956\n",
      "    loss           : 12251.18724685303\n",
      "    val_loss       : 12247.484311308972\n",
      "    val_log_likelihood: -12163.597682582455\n",
      "    val_log_marginal: -12171.846125692564\n",
      "Train Epoch: 1957 [256/118836 (0%)] Loss: 12202.005859\n",
      "Train Epoch: 1957 [33024/118836 (28%)] Loss: 12183.541016\n",
      "Train Epoch: 1957 [65792/118836 (55%)] Loss: 12330.000000\n",
      "Train Epoch: 1957 [98560/118836 (83%)] Loss: 12177.701172\n",
      "    epoch          : 1957\n",
      "    loss           : 12242.467357126241\n",
      "    val_loss       : 12242.976799737107\n",
      "    val_log_likelihood: -12156.539595287944\n",
      "    val_log_marginal: -12164.779614084538\n",
      "Train Epoch: 1958 [256/118836 (0%)] Loss: 12257.159180\n",
      "Train Epoch: 1958 [33024/118836 (28%)] Loss: 12274.921875\n",
      "Train Epoch: 1958 [65792/118836 (55%)] Loss: 12330.021484\n",
      "Train Epoch: 1958 [98560/118836 (83%)] Loss: 12325.063477\n",
      "    epoch          : 1958\n",
      "    loss           : 12244.915238252172\n",
      "    val_loss       : 12243.576833518022\n",
      "    val_log_likelihood: -12156.69803879756\n",
      "    val_log_marginal: -12164.816552479355\n",
      "Train Epoch: 1959 [256/118836 (0%)] Loss: 12294.444336\n",
      "Train Epoch: 1959 [33024/118836 (28%)] Loss: 12211.542969\n",
      "Train Epoch: 1959 [65792/118836 (55%)] Loss: 12195.041992\n",
      "Train Epoch: 1959 [98560/118836 (83%)] Loss: 12365.274414\n",
      "    epoch          : 1959\n",
      "    loss           : 12250.34059414418\n",
      "    val_loss       : 12246.523140989553\n",
      "    val_log_likelihood: -12155.420559508117\n",
      "    val_log_marginal: -12163.415116181948\n",
      "Train Epoch: 1960 [256/118836 (0%)] Loss: 12432.916016\n",
      "Train Epoch: 1960 [33024/118836 (28%)] Loss: 12372.152344\n",
      "Train Epoch: 1960 [65792/118836 (55%)] Loss: 12335.799805\n",
      "Train Epoch: 1960 [98560/118836 (83%)] Loss: 12236.349609\n",
      "    epoch          : 1960\n",
      "    loss           : 12244.309848337985\n",
      "    val_loss       : 12244.93417018394\n",
      "    val_log_likelihood: -12156.539925493693\n",
      "    val_log_marginal: -12164.600561532137\n",
      "Train Epoch: 1961 [256/118836 (0%)] Loss: 12159.795898\n",
      "Train Epoch: 1961 [33024/118836 (28%)] Loss: 12346.821289\n",
      "Train Epoch: 1961 [65792/118836 (55%)] Loss: 12260.706055\n",
      "Train Epoch: 1961 [98560/118836 (83%)] Loss: 12206.305664\n",
      "    epoch          : 1961\n",
      "    loss           : 12244.096016045027\n",
      "    val_loss       : 12243.58155762025\n",
      "    val_log_likelihood: -12156.74224161885\n",
      "    val_log_marginal: -12165.012182969434\n",
      "Train Epoch: 1962 [256/118836 (0%)] Loss: 12339.292969\n",
      "Train Epoch: 1962 [33024/118836 (28%)] Loss: 12243.906250\n",
      "Train Epoch: 1962 [65792/118836 (55%)] Loss: 12171.051758\n",
      "Train Epoch: 1962 [98560/118836 (83%)] Loss: 12305.953125\n",
      "    epoch          : 1962\n",
      "    loss           : 12243.235969002017\n",
      "    val_loss       : 12241.111068399687\n",
      "    val_log_likelihood: -12156.143666802367\n",
      "    val_log_marginal: -12164.408880287388\n",
      "Train Epoch: 1963 [256/118836 (0%)] Loss: 12150.664062\n",
      "Train Epoch: 1963 [33024/118836 (28%)] Loss: 12262.770508\n",
      "Train Epoch: 1963 [65792/118836 (55%)] Loss: 12278.734375\n",
      "Train Epoch: 1963 [98560/118836 (83%)] Loss: 12379.221680\n",
      "    epoch          : 1963\n",
      "    loss           : 12245.209482914597\n",
      "    val_loss       : 12244.398034928898\n",
      "    val_log_likelihood: -12159.37751741496\n",
      "    val_log_marginal: -12167.56536671029\n",
      "Train Epoch: 1964 [256/118836 (0%)] Loss: 12211.527344\n",
      "Train Epoch: 1964 [33024/118836 (28%)] Loss: 12263.962891\n",
      "Train Epoch: 1964 [65792/118836 (55%)] Loss: 12290.800781\n",
      "Train Epoch: 1964 [98560/118836 (83%)] Loss: 12342.918945\n",
      "    epoch          : 1964\n",
      "    loss           : 12245.593235143973\n",
      "    val_loss       : 12247.867175807052\n",
      "    val_log_likelihood: -12156.518609290995\n",
      "    val_log_marginal: -12165.000607880076\n",
      "Train Epoch: 1965 [256/118836 (0%)] Loss: 12130.636719\n",
      "Train Epoch: 1965 [33024/118836 (28%)] Loss: 12197.236328\n",
      "Train Epoch: 1965 [65792/118836 (55%)] Loss: 12302.222656\n",
      "Train Epoch: 1965 [98560/118836 (83%)] Loss: 12371.535156\n",
      "    epoch          : 1965\n",
      "    loss           : 12241.34499828758\n",
      "    val_loss       : 12248.484287063597\n",
      "    val_log_likelihood: -12156.225600961538\n",
      "    val_log_marginal: -12164.415001087182\n",
      "Train Epoch: 1966 [256/118836 (0%)] Loss: 12230.562500\n",
      "Train Epoch: 1966 [33024/118836 (28%)] Loss: 12218.623047\n",
      "Train Epoch: 1966 [65792/118836 (55%)] Loss: 12282.611328\n",
      "Train Epoch: 1966 [98560/118836 (83%)] Loss: 12273.337891\n",
      "    epoch          : 1966\n",
      "    loss           : 12240.681156786342\n",
      "    val_loss       : 12243.199843929626\n",
      "    val_log_likelihood: -12155.31097772565\n",
      "    val_log_marginal: -12163.51864211299\n",
      "Train Epoch: 1967 [256/118836 (0%)] Loss: 12284.750977\n",
      "Train Epoch: 1967 [33024/118836 (28%)] Loss: 12425.439453\n",
      "Train Epoch: 1967 [65792/118836 (55%)] Loss: 12356.665039\n",
      "Train Epoch: 1967 [98560/118836 (83%)] Loss: 12214.013672\n",
      "    epoch          : 1967\n",
      "    loss           : 12239.98275030371\n",
      "    val_loss       : 12246.146160745086\n",
      "    val_log_likelihood: -12157.090514985268\n",
      "    val_log_marginal: -12165.475388408564\n",
      "Train Epoch: 1968 [256/118836 (0%)] Loss: 12287.310547\n",
      "Train Epoch: 1968 [33024/118836 (28%)] Loss: 12354.542969\n",
      "Train Epoch: 1968 [65792/118836 (55%)] Loss: 12234.605469\n",
      "Train Epoch: 1968 [98560/118836 (83%)] Loss: 12197.143555\n",
      "    epoch          : 1968\n",
      "    loss           : 12245.968397662065\n",
      "    val_loss       : 12245.170167639058\n",
      "    val_log_likelihood: -12157.180182000879\n",
      "    val_log_marginal: -12165.499844641628\n",
      "Train Epoch: 1969 [256/118836 (0%)] Loss: 12183.847656\n",
      "Train Epoch: 1969 [33024/118836 (28%)] Loss: 12305.189453\n",
      "Train Epoch: 1969 [65792/118836 (55%)] Loss: 12199.224609\n",
      "Train Epoch: 1969 [98560/118836 (83%)] Loss: 12275.170898\n",
      "    epoch          : 1969\n",
      "    loss           : 12243.053706575682\n",
      "    val_loss       : 12248.956702641497\n",
      "    val_log_likelihood: -12156.560057059036\n",
      "    val_log_marginal: -12164.839108627772\n",
      "Train Epoch: 1970 [256/118836 (0%)] Loss: 12271.488281\n",
      "Train Epoch: 1970 [33024/118836 (28%)] Loss: 12202.771484\n",
      "Train Epoch: 1970 [65792/118836 (55%)] Loss: 12206.381836\n",
      "Train Epoch: 1970 [98560/118836 (83%)] Loss: 12328.577148\n",
      "    epoch          : 1970\n",
      "    loss           : 12246.387223590002\n",
      "    val_loss       : 12243.33002296068\n",
      "    val_log_likelihood: -12154.401521951251\n",
      "    val_log_marginal: -12162.651641969127\n",
      "Train Epoch: 1971 [256/118836 (0%)] Loss: 12250.412109\n",
      "Train Epoch: 1971 [33024/118836 (28%)] Loss: 12341.085938\n",
      "Train Epoch: 1971 [65792/118836 (55%)] Loss: 12284.356445\n",
      "Train Epoch: 1971 [98560/118836 (83%)] Loss: 12227.383789\n",
      "    epoch          : 1971\n",
      "    loss           : 12241.625274632961\n",
      "    val_loss       : 12243.373815031897\n",
      "    val_log_likelihood: -12157.33348454301\n",
      "    val_log_marginal: -12165.575977445478\n",
      "Train Epoch: 1972 [256/118836 (0%)] Loss: 12263.385742\n",
      "Train Epoch: 1972 [33024/118836 (28%)] Loss: 12199.288086\n",
      "Train Epoch: 1972 [65792/118836 (55%)] Loss: 12249.343750\n",
      "Train Epoch: 1972 [98560/118836 (83%)] Loss: 12239.106445\n",
      "    epoch          : 1972\n",
      "    loss           : 12241.294868886993\n",
      "    val_loss       : 12242.991391793681\n",
      "    val_log_likelihood: -12158.007605879086\n",
      "    val_log_marginal: -12166.061864013447\n",
      "Train Epoch: 1973 [256/118836 (0%)] Loss: 12423.758789\n",
      "Train Epoch: 1973 [33024/118836 (28%)] Loss: 12235.553711\n",
      "Train Epoch: 1973 [65792/118836 (55%)] Loss: 12315.348633\n",
      "Train Epoch: 1973 [98560/118836 (83%)] Loss: 12243.554688\n",
      "    epoch          : 1973\n",
      "    loss           : 12245.224747822322\n",
      "    val_loss       : 12243.150061605415\n",
      "    val_log_likelihood: -12157.977643423024\n",
      "    val_log_marginal: -12166.1645817139\n",
      "Train Epoch: 1974 [256/118836 (0%)] Loss: 12232.462891\n",
      "Train Epoch: 1974 [33024/118836 (28%)] Loss: 12349.926758\n",
      "Train Epoch: 1974 [65792/118836 (55%)] Loss: 12208.042969\n",
      "Train Epoch: 1974 [98560/118836 (83%)] Loss: 12292.461914\n",
      "    epoch          : 1974\n",
      "    loss           : 12244.033981305573\n",
      "    val_loss       : 12240.008307471428\n",
      "    val_log_likelihood: -12157.012592244364\n",
      "    val_log_marginal: -12165.257205415064\n",
      "Train Epoch: 1975 [256/118836 (0%)] Loss: 12280.107422\n",
      "Train Epoch: 1975 [33024/118836 (28%)] Loss: 12240.703125\n",
      "Train Epoch: 1975 [65792/118836 (55%)] Loss: 12259.328125\n",
      "Train Epoch: 1975 [98560/118836 (83%)] Loss: 12229.797852\n",
      "    epoch          : 1975\n",
      "    loss           : 12238.125211790479\n",
      "    val_loss       : 12244.040064469269\n",
      "    val_log_likelihood: -12157.510097607787\n",
      "    val_log_marginal: -12165.573082475215\n",
      "Train Epoch: 1976 [256/118836 (0%)] Loss: 12315.898438\n",
      "Train Epoch: 1976 [33024/118836 (28%)] Loss: 12282.689453\n",
      "Train Epoch: 1976 [65792/118836 (55%)] Loss: 12196.916016\n",
      "Train Epoch: 1976 [98560/118836 (83%)] Loss: 12305.734375\n",
      "    epoch          : 1976\n",
      "    loss           : 12244.244356615747\n",
      "    val_loss       : 12246.726005711924\n",
      "    val_log_likelihood: -12158.896552710143\n",
      "    val_log_marginal: -12167.144830716095\n",
      "Train Epoch: 1977 [256/118836 (0%)] Loss: 12235.544922\n",
      "Train Epoch: 1977 [33024/118836 (28%)] Loss: 12268.274414\n",
      "Train Epoch: 1977 [65792/118836 (55%)] Loss: 12267.332031\n",
      "Train Epoch: 1977 [98560/118836 (83%)] Loss: 12303.993164\n",
      "    epoch          : 1977\n",
      "    loss           : 12236.171281631256\n",
      "    val_loss       : 12237.822518865094\n",
      "    val_log_likelihood: -12156.76579737257\n",
      "    val_log_marginal: -12164.924600926064\n",
      "Train Epoch: 1978 [256/118836 (0%)] Loss: 12272.923828\n",
      "Train Epoch: 1978 [33024/118836 (28%)] Loss: 12269.989258\n",
      "Train Epoch: 1978 [65792/118836 (55%)] Loss: 12324.551758\n",
      "Train Epoch: 1978 [98560/118836 (83%)] Loss: 12239.726562\n",
      "    epoch          : 1978\n",
      "    loss           : 12244.113737302263\n",
      "    val_loss       : 12250.3281552752\n",
      "    val_log_likelihood: -12157.569691991057\n",
      "    val_log_marginal: -12165.743513515225\n",
      "Train Epoch: 1979 [256/118836 (0%)] Loss: 12219.158203\n",
      "Train Epoch: 1979 [33024/118836 (28%)] Loss: 12270.729492\n",
      "Train Epoch: 1979 [65792/118836 (55%)] Loss: 12245.969727\n",
      "Train Epoch: 1979 [98560/118836 (83%)] Loss: 12221.056641\n",
      "    epoch          : 1979\n",
      "    loss           : 12244.843640793011\n",
      "    val_loss       : 12242.567860211633\n",
      "    val_log_likelihood: -12159.193371006513\n",
      "    val_log_marginal: -12167.32363715669\n",
      "Train Epoch: 1980 [256/118836 (0%)] Loss: 12176.923828\n",
      "Train Epoch: 1980 [33024/118836 (28%)] Loss: 12266.903320\n",
      "Train Epoch: 1980 [65792/118836 (55%)] Loss: 12281.953125\n",
      "Train Epoch: 1980 [98560/118836 (83%)] Loss: 12152.776367\n",
      "    epoch          : 1980\n",
      "    loss           : 12241.77329469086\n",
      "    val_loss       : 12241.084922064752\n",
      "    val_log_likelihood: -12155.413023095018\n",
      "    val_log_marginal: -12163.540587734507\n",
      "Train Epoch: 1981 [256/118836 (0%)] Loss: 12478.129883\n",
      "Train Epoch: 1981 [33024/118836 (28%)] Loss: 12297.375000\n",
      "Train Epoch: 1981 [65792/118836 (55%)] Loss: 12167.428711\n",
      "Train Epoch: 1981 [98560/118836 (83%)] Loss: 12269.282227\n",
      "    epoch          : 1981\n",
      "    loss           : 12246.42387109698\n",
      "    val_loss       : 12246.011059178598\n",
      "    val_log_likelihood: -12155.874269637872\n",
      "    val_log_marginal: -12164.128875379174\n",
      "Train Epoch: 1982 [256/118836 (0%)] Loss: 12190.996094\n",
      "Train Epoch: 1982 [33024/118836 (28%)] Loss: 12171.773438\n",
      "Train Epoch: 1982 [65792/118836 (55%)] Loss: 12361.384766\n",
      "Train Epoch: 1982 [98560/118836 (83%)] Loss: 12182.536133\n",
      "    epoch          : 1982\n",
      "    loss           : 12239.036984820874\n",
      "    val_loss       : 12244.065007587342\n",
      "    val_log_likelihood: -12158.024518423024\n",
      "    val_log_marginal: -12166.14570407774\n",
      "Train Epoch: 1983 [256/118836 (0%)] Loss: 12348.919922\n",
      "Train Epoch: 1983 [33024/118836 (28%)] Loss: 12305.663086\n",
      "Train Epoch: 1983 [65792/118836 (55%)] Loss: 12317.332031\n",
      "Train Epoch: 1983 [98560/118836 (83%)] Loss: 12281.743164\n",
      "    epoch          : 1983\n",
      "    loss           : 12240.77315737438\n",
      "    val_loss       : 12240.834684934287\n",
      "    val_log_likelihood: -12155.978499147022\n",
      "    val_log_marginal: -12164.001915623003\n",
      "Train Epoch: 1984 [256/118836 (0%)] Loss: 12305.384766\n",
      "Train Epoch: 1984 [33024/118836 (28%)] Loss: 12396.318359\n",
      "Train Epoch: 1984 [65792/118836 (55%)] Loss: 12240.260742\n",
      "Train Epoch: 1984 [98560/118836 (83%)] Loss: 12183.795898\n",
      "    epoch          : 1984\n",
      "    loss           : 12244.748326354425\n",
      "    val_loss       : 12244.162468876226\n",
      "    val_log_likelihood: -12156.636145413306\n",
      "    val_log_marginal: -12164.788807707882\n",
      "Train Epoch: 1985 [256/118836 (0%)] Loss: 12248.731445\n",
      "Train Epoch: 1985 [33024/118836 (28%)] Loss: 12201.376953\n",
      "Train Epoch: 1985 [65792/118836 (55%)] Loss: 12287.356445\n",
      "Train Epoch: 1985 [98560/118836 (83%)] Loss: 12164.390625\n",
      "    epoch          : 1985\n",
      "    loss           : 12244.101901267835\n",
      "    val_loss       : 12241.789962283268\n",
      "    val_log_likelihood: -12156.681895872105\n",
      "    val_log_marginal: -12164.990005759131\n",
      "Train Epoch: 1986 [256/118836 (0%)] Loss: 12293.420898\n",
      "Train Epoch: 1986 [33024/118836 (28%)] Loss: 12224.508789\n",
      "Train Epoch: 1986 [65792/118836 (55%)] Loss: 12262.123047\n",
      "Train Epoch: 1986 [98560/118836 (83%)] Loss: 12279.228516\n",
      "    epoch          : 1986\n",
      "    loss           : 12239.857428336953\n",
      "    val_loss       : 12244.269296150025\n",
      "    val_log_likelihood: -12158.133583895524\n",
      "    val_log_marginal: -12166.256822233727\n",
      "Train Epoch: 1987 [256/118836 (0%)] Loss: 12251.829102\n",
      "Train Epoch: 1987 [33024/118836 (28%)] Loss: 12201.433594\n",
      "Train Epoch: 1987 [65792/118836 (55%)] Loss: 12379.207031\n",
      "Train Epoch: 1987 [98560/118836 (83%)] Loss: 12235.872070\n",
      "    epoch          : 1987\n",
      "    loss           : 12242.508023159637\n",
      "    val_loss       : 12240.74917719125\n",
      "    val_log_likelihood: -12156.05651154751\n",
      "    val_log_marginal: -12164.231345217402\n",
      "Train Epoch: 1988 [256/118836 (0%)] Loss: 12247.376953\n",
      "Train Epoch: 1988 [33024/118836 (28%)] Loss: 12198.618164\n",
      "Train Epoch: 1988 [65792/118836 (55%)] Loss: 12270.373047\n",
      "Train Epoch: 1988 [98560/118836 (83%)] Loss: 12268.377930\n",
      "    epoch          : 1988\n",
      "    loss           : 12244.653718691841\n",
      "    val_loss       : 12246.077056559918\n",
      "    val_log_likelihood: -12154.721105704612\n",
      "    val_log_marginal: -12162.890518252107\n",
      "Train Epoch: 1989 [256/118836 (0%)] Loss: 12296.407227\n",
      "Train Epoch: 1989 [33024/118836 (28%)] Loss: 12409.595703\n",
      "Train Epoch: 1989 [65792/118836 (55%)] Loss: 12229.621094\n",
      "Train Epoch: 1989 [98560/118836 (83%)] Loss: 12184.420898\n",
      "    epoch          : 1989\n",
      "    loss           : 12242.848224901778\n",
      "    val_loss       : 12242.953069558973\n",
      "    val_log_likelihood: -12157.738051042958\n",
      "    val_log_marginal: -12165.866977929627\n",
      "Train Epoch: 1990 [256/118836 (0%)] Loss: 12232.456055\n",
      "Train Epoch: 1990 [33024/118836 (28%)] Loss: 12312.819336\n",
      "Train Epoch: 1990 [65792/118836 (55%)] Loss: 12252.248047\n",
      "Train Epoch: 1990 [98560/118836 (83%)] Loss: 12315.721680\n",
      "    epoch          : 1990\n",
      "    loss           : 12240.802382360162\n",
      "    val_loss       : 12241.11512579464\n",
      "    val_log_likelihood: -12155.811903884925\n",
      "    val_log_marginal: -12163.903966704349\n",
      "Train Epoch: 1991 [256/118836 (0%)] Loss: 12195.379883\n",
      "Train Epoch: 1991 [33024/118836 (28%)] Loss: 12269.064453\n",
      "Train Epoch: 1991 [65792/118836 (55%)] Loss: 12275.665039\n",
      "Train Epoch: 1991 [98560/118836 (83%)] Loss: 12230.443359\n",
      "    epoch          : 1991\n",
      "    loss           : 12238.91130243874\n",
      "    val_loss       : 12240.637564877297\n",
      "    val_log_likelihood: -12156.216645826871\n",
      "    val_log_marginal: -12164.279899705625\n",
      "Train Epoch: 1992 [256/118836 (0%)] Loss: 12303.146484\n",
      "Train Epoch: 1992 [33024/118836 (28%)] Loss: 12175.445312\n",
      "Train Epoch: 1992 [65792/118836 (55%)] Loss: 12340.076172\n",
      "Train Epoch: 1992 [98560/118836 (83%)] Loss: 12219.281250\n",
      "    epoch          : 1992\n",
      "    loss           : 12235.767328047456\n",
      "    val_loss       : 12242.422464213942\n",
      "    val_log_likelihood: -12155.676266542598\n",
      "    val_log_marginal: -12163.818885099587\n",
      "Train Epoch: 1993 [256/118836 (0%)] Loss: 12293.799805\n",
      "Train Epoch: 1993 [33024/118836 (28%)] Loss: 12207.240234\n",
      "Train Epoch: 1993 [65792/118836 (55%)] Loss: 12337.125000\n",
      "Train Epoch: 1993 [98560/118836 (83%)] Loss: 12236.788086\n",
      "    epoch          : 1993\n",
      "    loss           : 12245.58127293993\n",
      "    val_loss       : 12242.31690509936\n",
      "    val_log_likelihood: -12155.835473854942\n",
      "    val_log_marginal: -12164.003249341673\n",
      "Train Epoch: 1994 [256/118836 (0%)] Loss: 12250.628906\n",
      "Train Epoch: 1994 [33024/118836 (28%)] Loss: 12205.291992\n",
      "Train Epoch: 1994 [65792/118836 (55%)] Loss: 12313.434570\n",
      "Train Epoch: 1994 [98560/118836 (83%)] Loss: 12261.474609\n",
      "    epoch          : 1994\n",
      "    loss           : 12241.73383478081\n",
      "    val_loss       : 12244.117132625232\n",
      "    val_log_likelihood: -12162.763468161962\n",
      "    val_log_marginal: -12171.04498986694\n",
      "Train Epoch: 1995 [256/118836 (0%)] Loss: 12323.442383\n",
      "Train Epoch: 1995 [33024/118836 (28%)] Loss: 12237.813477\n",
      "Train Epoch: 1995 [65792/118836 (55%)] Loss: 12288.346680\n",
      "Train Epoch: 1995 [98560/118836 (83%)] Loss: 12239.482422\n",
      "    epoch          : 1995\n",
      "    loss           : 12239.668974391285\n",
      "    val_loss       : 12241.394463922183\n",
      "    val_log_likelihood: -12159.06549705335\n",
      "    val_log_marginal: -12167.30891118762\n",
      "Train Epoch: 1996 [256/118836 (0%)] Loss: 12157.049805\n",
      "Train Epoch: 1996 [33024/118836 (28%)] Loss: 12300.518555\n",
      "Train Epoch: 1996 [65792/118836 (55%)] Loss: 12165.824219\n",
      "Train Epoch: 1996 [98560/118836 (83%)] Loss: 12344.451172\n",
      "    epoch          : 1996\n",
      "    loss           : 12242.969308474203\n",
      "    val_loss       : 12244.65609832477\n",
      "    val_log_likelihood: -12155.545946094397\n",
      "    val_log_marginal: -12163.619780180046\n",
      "Train Epoch: 1997 [256/118836 (0%)] Loss: 12285.806641\n",
      "Train Epoch: 1997 [33024/118836 (28%)] Loss: 12342.308594\n",
      "Train Epoch: 1997 [65792/118836 (55%)] Loss: 12201.263672\n",
      "Train Epoch: 1997 [98560/118836 (83%)] Loss: 12118.891602\n",
      "    epoch          : 1997\n",
      "    loss           : 12240.688777204818\n",
      "    val_loss       : 12242.624753611775\n",
      "    val_log_likelihood: -12159.97373232656\n",
      "    val_log_marginal: -12168.478168720312\n",
      "Train Epoch: 1998 [256/118836 (0%)] Loss: 12277.132812\n",
      "Train Epoch: 1998 [33024/118836 (28%)] Loss: 12179.148438\n",
      "Train Epoch: 1998 [65792/118836 (55%)] Loss: 12191.775391\n",
      "Train Epoch: 1998 [98560/118836 (83%)] Loss: 12337.601562\n",
      "    epoch          : 1998\n",
      "    loss           : 12246.794189251188\n",
      "    val_loss       : 12242.430996090172\n",
      "    val_log_likelihood: -12156.984033162738\n",
      "    val_log_marginal: -12165.177291459102\n",
      "Train Epoch: 1999 [256/118836 (0%)] Loss: 12330.829102\n",
      "Train Epoch: 1999 [33024/118836 (28%)] Loss: 12209.009766\n",
      "Train Epoch: 1999 [65792/118836 (55%)] Loss: 12218.221680\n",
      "Train Epoch: 1999 [98560/118836 (83%)] Loss: 12365.758789\n",
      "    epoch          : 1999\n",
      "    loss           : 12238.282954339847\n",
      "    val_loss       : 12240.5437536773\n",
      "    val_log_likelihood: -12155.83285837986\n",
      "    val_log_marginal: -12164.129037757182\n",
      "Train Epoch: 2000 [256/118836 (0%)] Loss: 12160.154297\n",
      "Train Epoch: 2000 [33024/118836 (28%)] Loss: 12284.138672\n",
      "Train Epoch: 2000 [65792/118836 (55%)] Loss: 12244.111328\n",
      "Train Epoch: 2000 [98560/118836 (83%)] Loss: 12222.803711\n",
      "    epoch          : 2000\n",
      "    loss           : 12248.650764448925\n",
      "    val_loss       : 12245.571107997583\n",
      "    val_log_likelihood: -12155.311840234699\n",
      "    val_log_marginal: -12163.385749967625\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch2000.pth ...\n",
      "Train Epoch: 2001 [256/118836 (0%)] Loss: 12171.560547\n",
      "Train Epoch: 2001 [33024/118836 (28%)] Loss: 12182.181641\n",
      "Train Epoch: 2001 [65792/118836 (55%)] Loss: 12271.396484\n",
      "Train Epoch: 2001 [98560/118836 (83%)] Loss: 12243.911133\n",
      "    epoch          : 2001\n",
      "    loss           : 12241.745631881979\n",
      "    val_loss       : 12239.623190452616\n",
      "    val_log_likelihood: -12156.62996229451\n",
      "    val_log_marginal: -12164.884203981392\n",
      "Train Epoch: 2002 [256/118836 (0%)] Loss: 12251.323242\n",
      "Train Epoch: 2002 [33024/118836 (28%)] Loss: 12252.681641\n",
      "Train Epoch: 2002 [65792/118836 (55%)] Loss: 12217.006836\n",
      "Train Epoch: 2002 [98560/118836 (83%)] Loss: 12321.261719\n",
      "    epoch          : 2002\n",
      "    loss           : 12242.908029460039\n",
      "    val_loss       : 12239.791622293249\n",
      "    val_log_likelihood: -12155.567561000827\n",
      "    val_log_marginal: -12163.90275026972\n",
      "Train Epoch: 2003 [256/118836 (0%)] Loss: 12316.242188\n",
      "Train Epoch: 2003 [33024/118836 (28%)] Loss: 12292.709961\n",
      "Train Epoch: 2003 [65792/118836 (55%)] Loss: 12278.113281\n",
      "Train Epoch: 2003 [98560/118836 (83%)] Loss: 12220.167969\n",
      "    epoch          : 2003\n",
      "    loss           : 12244.205851782206\n",
      "    val_loss       : 12239.586659925048\n",
      "    val_log_likelihood: -12158.310537020523\n",
      "    val_log_marginal: -12166.41656494988\n",
      "Train Epoch: 2004 [256/118836 (0%)] Loss: 12243.509766\n",
      "Train Epoch: 2004 [33024/118836 (28%)] Loss: 12260.172852\n",
      "Train Epoch: 2004 [65792/118836 (55%)] Loss: 12255.722656\n",
      "Train Epoch: 2004 [98560/118836 (83%)] Loss: 12351.488281\n",
      "    epoch          : 2004\n",
      "    loss           : 12236.730391206576\n",
      "    val_loss       : 12240.877948366127\n",
      "    val_log_likelihood: -12156.536467864713\n",
      "    val_log_marginal: -12164.71252425382\n",
      "Train Epoch: 2005 [256/118836 (0%)] Loss: 12236.252930\n",
      "Train Epoch: 2005 [33024/118836 (28%)] Loss: 12447.024414\n",
      "Train Epoch: 2005 [65792/118836 (55%)] Loss: 12183.991211\n",
      "Train Epoch: 2005 [98560/118836 (83%)] Loss: 12328.146484\n",
      "    epoch          : 2005\n",
      "    loss           : 12239.87432795699\n",
      "    val_loss       : 12238.122745392777\n",
      "    val_log_likelihood: -12155.067660676437\n",
      "    val_log_marginal: -12163.100119778836\n",
      "Train Epoch: 2006 [256/118836 (0%)] Loss: 12406.168945\n",
      "Train Epoch: 2006 [33024/118836 (28%)] Loss: 12257.644531\n",
      "Train Epoch: 2006 [65792/118836 (55%)] Loss: 12274.698242\n",
      "Train Epoch: 2006 [98560/118836 (83%)] Loss: 12235.530273\n",
      "    epoch          : 2006\n",
      "    loss           : 12240.622567559709\n",
      "    val_loss       : 12240.480659806572\n",
      "    val_log_likelihood: -12157.322626525021\n",
      "    val_log_marginal: -12165.957495781304\n",
      "Train Epoch: 2007 [256/118836 (0%)] Loss: 12236.953125\n",
      "Train Epoch: 2007 [33024/118836 (28%)] Loss: 12271.306641\n",
      "Train Epoch: 2007 [65792/118836 (55%)] Loss: 12229.643555\n",
      "Train Epoch: 2007 [98560/118836 (83%)] Loss: 12378.763672\n",
      "    epoch          : 2007\n",
      "    loss           : 12246.935007786653\n",
      "    val_loss       : 12241.284817207468\n",
      "    val_log_likelihood: -12156.334026539238\n",
      "    val_log_marginal: -12164.610955022372\n",
      "Train Epoch: 2008 [256/118836 (0%)] Loss: 12196.123047\n",
      "Train Epoch: 2008 [33024/118836 (28%)] Loss: 12257.908203\n",
      "Train Epoch: 2008 [65792/118836 (55%)] Loss: 12253.304688\n",
      "Train Epoch: 2008 [98560/118836 (83%)] Loss: 12249.894531\n",
      "    epoch          : 2008\n",
      "    loss           : 12240.838583184708\n",
      "    val_loss       : 12244.90362012271\n",
      "    val_log_likelihood: -12157.60583788901\n",
      "    val_log_marginal: -12165.752950880875\n",
      "Train Epoch: 2009 [256/118836 (0%)] Loss: 12339.021484\n",
      "Train Epoch: 2009 [33024/118836 (28%)] Loss: 12311.926758\n",
      "Train Epoch: 2009 [65792/118836 (55%)] Loss: 12270.269531\n",
      "Train Epoch: 2009 [98560/118836 (83%)] Loss: 12327.754883\n",
      "    epoch          : 2009\n",
      "    loss           : 12243.655793786187\n",
      "    val_loss       : 12244.70211548897\n",
      "    val_log_likelihood: -12157.207702162168\n",
      "    val_log_marginal: -12165.482561356843\n",
      "Train Epoch: 2010 [256/118836 (0%)] Loss: 12340.729492\n",
      "Train Epoch: 2010 [33024/118836 (28%)] Loss: 12294.982422\n",
      "Train Epoch: 2010 [65792/118836 (55%)] Loss: 12194.131836\n",
      "Train Epoch: 2010 [98560/118836 (83%)] Loss: 12185.380859\n",
      "    epoch          : 2010\n",
      "    loss           : 12240.094240462158\n",
      "    val_loss       : 12242.520331029917\n",
      "    val_log_likelihood: -12155.858655300093\n",
      "    val_log_marginal: -12164.029243665316\n",
      "Train Epoch: 2011 [256/118836 (0%)] Loss: 12313.125977\n",
      "Train Epoch: 2011 [33024/118836 (28%)] Loss: 12361.185547\n",
      "Train Epoch: 2011 [65792/118836 (55%)] Loss: 12276.951172\n",
      "Train Epoch: 2011 [98560/118836 (83%)] Loss: 12288.397461\n",
      "    epoch          : 2011\n",
      "    loss           : 12240.545838341346\n",
      "    val_loss       : 12239.534745605444\n",
      "    val_log_likelihood: -12157.09709163694\n",
      "    val_log_marginal: -12165.324728445052\n",
      "Train Epoch: 2012 [256/118836 (0%)] Loss: 12417.514648\n",
      "Train Epoch: 2012 [33024/118836 (28%)] Loss: 12201.292969\n",
      "Train Epoch: 2012 [65792/118836 (55%)] Loss: 12357.036133\n",
      "Train Epoch: 2012 [98560/118836 (83%)] Loss: 12341.320312\n",
      "    epoch          : 2012\n",
      "    loss           : 12244.52813420828\n",
      "    val_loss       : 12248.206969858922\n",
      "    val_log_likelihood: -12157.54703267163\n",
      "    val_log_marginal: -12166.186571509461\n",
      "Train Epoch: 2013 [256/118836 (0%)] Loss: 12222.187500\n",
      "Train Epoch: 2013 [33024/118836 (28%)] Loss: 12256.486328\n",
      "Train Epoch: 2013 [65792/118836 (55%)] Loss: 12336.865234\n",
      "Train Epoch: 2013 [98560/118836 (83%)] Loss: 12336.228516\n",
      "    epoch          : 2013\n",
      "    loss           : 12246.118376499173\n",
      "    val_loss       : 12246.864234322225\n",
      "    val_log_likelihood: -12156.812069633996\n",
      "    val_log_marginal: -12165.28317488126\n",
      "Train Epoch: 2014 [256/118836 (0%)] Loss: 12267.440430\n",
      "Train Epoch: 2014 [33024/118836 (28%)] Loss: 12220.282227\n",
      "Train Epoch: 2014 [65792/118836 (55%)] Loss: 12283.290039\n",
      "Train Epoch: 2014 [98560/118836 (83%)] Loss: 12211.256836\n",
      "    epoch          : 2014\n",
      "    loss           : 12245.388480116573\n",
      "    val_loss       : 12241.28044586661\n",
      "    val_log_likelihood: -12156.824736352357\n",
      "    val_log_marginal: -12164.949848496659\n",
      "Train Epoch: 2015 [256/118836 (0%)] Loss: 12231.066406\n",
      "Train Epoch: 2015 [33024/118836 (28%)] Loss: 12238.295898\n",
      "Train Epoch: 2015 [65792/118836 (55%)] Loss: 12265.654297\n",
      "Train Epoch: 2015 [98560/118836 (83%)] Loss: 12362.247070\n",
      "    epoch          : 2015\n",
      "    loss           : 12246.67150343776\n",
      "    val_loss       : 12245.265052136036\n",
      "    val_log_likelihood: -12156.89893313172\n",
      "    val_log_marginal: -12165.149246850591\n",
      "Train Epoch: 2016 [256/118836 (0%)] Loss: 12259.075195\n",
      "Train Epoch: 2016 [33024/118836 (28%)] Loss: 12220.398438\n",
      "Train Epoch: 2016 [65792/118836 (55%)] Loss: 12227.837891\n",
      "Train Epoch: 2016 [98560/118836 (83%)] Loss: 12274.483398\n",
      "    epoch          : 2016\n",
      "    loss           : 12241.968187810175\n",
      "    val_loss       : 12245.533871508414\n",
      "    val_log_likelihood: -12156.863021964175\n",
      "    val_log_marginal: -12165.114375235295\n",
      "Train Epoch: 2017 [256/118836 (0%)] Loss: 12247.257812\n",
      "Train Epoch: 2017 [33024/118836 (28%)] Loss: 12344.356445\n",
      "Train Epoch: 2017 [65792/118836 (55%)] Loss: 12282.712891\n",
      "Train Epoch: 2017 [98560/118836 (83%)] Loss: 12291.531250\n",
      "    epoch          : 2017\n",
      "    loss           : 12239.372466591707\n",
      "    val_loss       : 12240.472120606148\n",
      "    val_log_likelihood: -12155.565030985059\n",
      "    val_log_marginal: -12163.784913996313\n",
      "Train Epoch: 2018 [256/118836 (0%)] Loss: 12153.069336\n",
      "Train Epoch: 2018 [33024/118836 (28%)] Loss: 12245.679688\n",
      "Train Epoch: 2018 [65792/118836 (55%)] Loss: 12177.367188\n",
      "Train Epoch: 2018 [98560/118836 (83%)] Loss: 12217.588867\n",
      "    epoch          : 2018\n",
      "    loss           : 12244.969542719964\n",
      "    val_loss       : 12244.792654613811\n",
      "    val_log_likelihood: -12155.575702737284\n",
      "    val_log_marginal: -12163.721522054326\n",
      "Train Epoch: 2019 [256/118836 (0%)] Loss: 12191.208008\n",
      "Train Epoch: 2019 [33024/118836 (28%)] Loss: 12200.072266\n",
      "Train Epoch: 2019 [65792/118836 (55%)] Loss: 12171.841797\n",
      "Train Epoch: 2019 [98560/118836 (83%)] Loss: 12207.774414\n",
      "    epoch          : 2019\n",
      "    loss           : 12242.452141006255\n",
      "    val_loss       : 12241.004701370777\n",
      "    val_log_likelihood: -12156.209507146918\n",
      "    val_log_marginal: -12164.374841592182\n",
      "Train Epoch: 2020 [256/118836 (0%)] Loss: 12287.879883\n",
      "Train Epoch: 2020 [33024/118836 (28%)] Loss: 12303.261719\n",
      "Train Epoch: 2020 [65792/118836 (55%)] Loss: 12258.244141\n",
      "Train Epoch: 2020 [98560/118836 (83%)] Loss: 12298.918945\n",
      "    epoch          : 2020\n",
      "    loss           : 12238.002690433726\n",
      "    val_loss       : 12240.518214928497\n",
      "    val_log_likelihood: -12157.98800112438\n",
      "    val_log_marginal: -12166.245778189244\n",
      "Train Epoch: 2021 [256/118836 (0%)] Loss: 12385.195312\n",
      "Train Epoch: 2021 [33024/118836 (28%)] Loss: 12413.335938\n",
      "Train Epoch: 2021 [65792/118836 (55%)] Loss: 12241.469727\n",
      "Train Epoch: 2021 [98560/118836 (83%)] Loss: 12285.612305\n",
      "    epoch          : 2021\n",
      "    loss           : 12244.305011728442\n",
      "    val_loss       : 12234.659909247299\n",
      "    val_log_likelihood: -12154.763905474565\n",
      "    val_log_marginal: -12162.89685859648\n",
      "Train Epoch: 2022 [256/118836 (0%)] Loss: 12310.669922\n",
      "Train Epoch: 2022 [33024/118836 (28%)] Loss: 12250.646484\n",
      "Train Epoch: 2022 [65792/118836 (55%)] Loss: 12275.825195\n",
      "Train Epoch: 2022 [98560/118836 (83%)] Loss: 12337.053711\n",
      "    epoch          : 2022\n",
      "    loss           : 12241.54949758323\n",
      "    val_loss       : 12241.706706468252\n",
      "    val_log_likelihood: -12155.311446217172\n",
      "    val_log_marginal: -12163.273074725008\n",
      "Train Epoch: 2023 [256/118836 (0%)] Loss: 12298.149414\n",
      "Train Epoch: 2023 [33024/118836 (28%)] Loss: 12200.826172\n",
      "Train Epoch: 2023 [65792/118836 (55%)] Loss: 12350.502930\n",
      "Train Epoch: 2023 [98560/118836 (83%)] Loss: 12296.009766\n",
      "    epoch          : 2023\n",
      "    loss           : 12238.125255247105\n",
      "    val_loss       : 12242.013499317956\n",
      "    val_log_likelihood: -12157.415141969086\n",
      "    val_log_marginal: -12165.618056823612\n",
      "Train Epoch: 2024 [256/118836 (0%)] Loss: 12166.444336\n",
      "Train Epoch: 2024 [33024/118836 (28%)] Loss: 12306.105469\n",
      "Train Epoch: 2024 [65792/118836 (55%)] Loss: 12280.684570\n",
      "Train Epoch: 2024 [98560/118836 (83%)] Loss: 12150.626953\n",
      "    epoch          : 2024\n",
      "    loss           : 12241.395871781948\n",
      "    val_loss       : 12243.952913192623\n",
      "    val_log_likelihood: -12156.80067220456\n",
      "    val_log_marginal: -12164.982417305142\n",
      "Train Epoch: 2025 [256/118836 (0%)] Loss: 12279.132812\n",
      "Train Epoch: 2025 [33024/118836 (28%)] Loss: 12302.662109\n",
      "Train Epoch: 2025 [65792/118836 (55%)] Loss: 12167.774414\n",
      "Train Epoch: 2025 [98560/118836 (83%)] Loss: 12261.529297\n",
      "    epoch          : 2025\n",
      "    loss           : 12240.731861785309\n",
      "    val_loss       : 12239.662268334441\n",
      "    val_log_likelihood: -12155.929143726738\n",
      "    val_log_marginal: -12164.101514542686\n",
      "Train Epoch: 2026 [256/118836 (0%)] Loss: 12382.802734\n",
      "Train Epoch: 2026 [33024/118836 (28%)] Loss: 12301.593750\n",
      "Train Epoch: 2026 [65792/118836 (55%)] Loss: 12312.068359\n",
      "Train Epoch: 2026 [98560/118836 (83%)] Loss: 12303.202148\n",
      "    epoch          : 2026\n",
      "    loss           : 12244.694135455438\n",
      "    val_loss       : 12242.920315849138\n",
      "    val_log_likelihood: -12164.129032096514\n",
      "    val_log_marginal: -12172.379945662831\n",
      "Train Epoch: 2027 [256/118836 (0%)] Loss: 12229.234375\n",
      "Train Epoch: 2027 [33024/118836 (28%)] Loss: 12287.984375\n",
      "Train Epoch: 2027 [65792/118836 (55%)] Loss: 12281.061523\n",
      "Train Epoch: 2027 [98560/118836 (83%)] Loss: 12322.075195\n",
      "    epoch          : 2027\n",
      "    loss           : 12243.983210556245\n",
      "    val_loss       : 12241.817440133578\n",
      "    val_log_likelihood: -12157.055667616572\n",
      "    val_log_marginal: -12165.382235136158\n",
      "Train Epoch: 2028 [256/118836 (0%)] Loss: 12275.786133\n",
      "Train Epoch: 2028 [33024/118836 (28%)] Loss: 12334.195312\n",
      "Train Epoch: 2028 [65792/118836 (55%)] Loss: 12260.548828\n",
      "Train Epoch: 2028 [98560/118836 (83%)] Loss: 12266.492188\n",
      "    epoch          : 2028\n",
      "    loss           : 12244.65161758814\n",
      "    val_loss       : 12240.937898646123\n",
      "    val_log_likelihood: -12160.175782703938\n",
      "    val_log_marginal: -12168.509044692004\n",
      "Train Epoch: 2029 [256/118836 (0%)] Loss: 12153.884766\n",
      "Train Epoch: 2029 [33024/118836 (28%)] Loss: 12299.802734\n",
      "Train Epoch: 2029 [65792/118836 (55%)] Loss: 12243.238281\n",
      "Train Epoch: 2029 [98560/118836 (83%)] Loss: 12295.538086\n",
      "    epoch          : 2029\n",
      "    loss           : 12242.124124567048\n",
      "    val_loss       : 12250.901561786797\n",
      "    val_log_likelihood: -12159.876067352925\n",
      "    val_log_marginal: -12168.256164132725\n",
      "Train Epoch: 2030 [256/118836 (0%)] Loss: 12239.775391\n",
      "Train Epoch: 2030 [33024/118836 (28%)] Loss: 12235.334961\n",
      "Train Epoch: 2030 [65792/118836 (55%)] Loss: 12186.328125\n",
      "Train Epoch: 2030 [98560/118836 (83%)] Loss: 12248.873047\n",
      "    epoch          : 2030\n",
      "    loss           : 12245.380988290943\n",
      "    val_loss       : 12242.666767273562\n",
      "    val_log_likelihood: -12156.046700527295\n",
      "    val_log_marginal: -12164.39264330541\n",
      "Train Epoch: 2031 [256/118836 (0%)] Loss: 12307.841797\n",
      "Train Epoch: 2031 [33024/118836 (28%)] Loss: 12214.867188\n",
      "Train Epoch: 2031 [65792/118836 (55%)] Loss: 12294.044922\n",
      "Train Epoch: 2031 [98560/118836 (83%)] Loss: 12217.347656\n",
      "    epoch          : 2031\n",
      "    loss           : 12245.814038913875\n",
      "    val_loss       : 12245.454859692047\n",
      "    val_log_likelihood: -12155.53128101737\n",
      "    val_log_marginal: -12163.863784781479\n",
      "Train Epoch: 2032 [256/118836 (0%)] Loss: 12353.426758\n",
      "Train Epoch: 2032 [33024/118836 (28%)] Loss: 12308.655273\n",
      "Train Epoch: 2032 [65792/118836 (55%)] Loss: 12268.442383\n",
      "Train Epoch: 2032 [98560/118836 (83%)] Loss: 12252.232422\n",
      "    epoch          : 2032\n",
      "    loss           : 12242.550912750725\n",
      "    val_loss       : 12240.119314582853\n",
      "    val_log_likelihood: -12155.868334173387\n",
      "    val_log_marginal: -12164.079566211138\n",
      "Train Epoch: 2033 [256/118836 (0%)] Loss: 12278.542969\n",
      "Train Epoch: 2033 [33024/118836 (28%)] Loss: 12221.073242\n",
      "Train Epoch: 2033 [65792/118836 (55%)] Loss: 12260.212891\n",
      "Train Epoch: 2033 [98560/118836 (83%)] Loss: 12279.109375\n",
      "    epoch          : 2033\n",
      "    loss           : 12244.435702446495\n",
      "    val_loss       : 12240.878198980727\n",
      "    val_log_likelihood: -12156.65334163694\n",
      "    val_log_marginal: -12164.88435552451\n",
      "Train Epoch: 2034 [256/118836 (0%)] Loss: 12164.789062\n",
      "Train Epoch: 2034 [33024/118836 (28%)] Loss: 12196.552734\n",
      "Train Epoch: 2034 [65792/118836 (55%)] Loss: 12214.896484\n",
      "Train Epoch: 2034 [98560/118836 (83%)] Loss: 12311.186523\n",
      "    epoch          : 2034\n",
      "    loss           : 12242.604841940654\n",
      "    val_loss       : 12243.71629720375\n",
      "    val_log_likelihood: -12156.403089943911\n",
      "    val_log_marginal: -12164.476113934783\n",
      "Train Epoch: 2035 [256/118836 (0%)] Loss: 12287.252930\n",
      "Train Epoch: 2035 [33024/118836 (28%)] Loss: 12211.466797\n",
      "Train Epoch: 2035 [65792/118836 (55%)] Loss: 12279.351562\n",
      "Train Epoch: 2035 [98560/118836 (83%)] Loss: 12255.875977\n",
      "    epoch          : 2035\n",
      "    loss           : 12242.255434501654\n",
      "    val_loss       : 12241.886727133655\n",
      "    val_log_likelihood: -12153.618608321702\n",
      "    val_log_marginal: -12161.839706988627\n",
      "Train Epoch: 2036 [256/118836 (0%)] Loss: 12307.233398\n",
      "Train Epoch: 2036 [33024/118836 (28%)] Loss: 12238.583984\n",
      "Train Epoch: 2036 [65792/118836 (55%)] Loss: 12343.806641\n",
      "Train Epoch: 2036 [98560/118836 (83%)] Loss: 12323.844727\n",
      "    epoch          : 2036\n",
      "    loss           : 12242.635784190188\n",
      "    val_loss       : 12244.979429577535\n",
      "    val_log_likelihood: -12154.86230452595\n",
      "    val_log_marginal: -12162.987285074496\n",
      "Train Epoch: 2037 [256/118836 (0%)] Loss: 12275.042969\n",
      "Train Epoch: 2037 [33024/118836 (28%)] Loss: 12252.153320\n",
      "Train Epoch: 2037 [65792/118836 (55%)] Loss: 12260.648438\n",
      "Train Epoch: 2037 [98560/118836 (83%)] Loss: 12322.790039\n",
      "    epoch          : 2037\n",
      "    loss           : 12246.465411917132\n",
      "    val_loss       : 12242.686580113907\n",
      "    val_log_likelihood: -12157.681118014629\n",
      "    val_log_marginal: -12166.075213133572\n",
      "Train Epoch: 2038 [256/118836 (0%)] Loss: 12248.113281\n",
      "Train Epoch: 2038 [33024/118836 (28%)] Loss: 12218.941406\n",
      "Train Epoch: 2038 [65792/118836 (55%)] Loss: 12298.606445\n",
      "Train Epoch: 2038 [98560/118836 (83%)] Loss: 12180.196289\n",
      "    epoch          : 2038\n",
      "    loss           : 12241.604030965673\n",
      "    val_loss       : 12246.894945522194\n",
      "    val_log_likelihood: -12155.567958733975\n",
      "    val_log_marginal: -12163.68931137394\n",
      "Train Epoch: 2039 [256/118836 (0%)] Loss: 12230.029297\n",
      "Train Epoch: 2039 [33024/118836 (28%)] Loss: 12345.712891\n",
      "Train Epoch: 2039 [65792/118836 (55%)] Loss: 12228.512695\n",
      "Train Epoch: 2039 [98560/118836 (83%)] Loss: 12247.328125\n",
      "    epoch          : 2039\n",
      "    loss           : 12243.976113717432\n",
      "    val_loss       : 12245.621329662254\n",
      "    val_log_likelihood: -12156.281352745036\n",
      "    val_log_marginal: -12164.45188500918\n",
      "Train Epoch: 2040 [256/118836 (0%)] Loss: 12287.884766\n",
      "Train Epoch: 2040 [33024/118836 (28%)] Loss: 12199.056641\n",
      "Train Epoch: 2040 [65792/118836 (55%)] Loss: 12327.071289\n",
      "Train Epoch: 2040 [98560/118836 (83%)] Loss: 12227.687500\n",
      "    epoch          : 2040\n",
      "    loss           : 12237.584739130996\n",
      "    val_loss       : 12241.799609221443\n",
      "    val_log_likelihood: -12155.052248436208\n",
      "    val_log_marginal: -12163.16570370349\n",
      "Train Epoch: 2041 [256/118836 (0%)] Loss: 12210.298828\n",
      "Train Epoch: 2041 [33024/118836 (28%)] Loss: 12212.020508\n",
      "Train Epoch: 2041 [65792/118836 (55%)] Loss: 12181.176758\n",
      "Train Epoch: 2041 [98560/118836 (83%)] Loss: 12286.596680\n",
      "    epoch          : 2041\n",
      "    loss           : 12242.558419277295\n",
      "    val_loss       : 12245.899601733445\n",
      "    val_log_likelihood: -12156.281413648934\n",
      "    val_log_marginal: -12164.42788693626\n",
      "Train Epoch: 2042 [256/118836 (0%)] Loss: 12232.527344\n",
      "Train Epoch: 2042 [33024/118836 (28%)] Loss: 12243.639648\n",
      "Train Epoch: 2042 [65792/118836 (55%)] Loss: 12181.307617\n",
      "Train Epoch: 2042 [98560/118836 (83%)] Loss: 12243.097656\n",
      "    epoch          : 2042\n",
      "    loss           : 12236.475570267266\n",
      "    val_loss       : 12243.651281544067\n",
      "    val_log_likelihood: -12156.590525001291\n",
      "    val_log_marginal: -12164.641886253587\n",
      "Train Epoch: 2043 [256/118836 (0%)] Loss: 12242.736328\n",
      "Train Epoch: 2043 [33024/118836 (28%)] Loss: 12238.184570\n",
      "Train Epoch: 2043 [65792/118836 (55%)] Loss: 12256.759766\n",
      "Train Epoch: 2043 [98560/118836 (83%)] Loss: 12256.716797\n",
      "    epoch          : 2043\n",
      "    loss           : 12240.529452446495\n",
      "    val_loss       : 12251.73163765193\n",
      "    val_log_likelihood: -12161.15604257134\n",
      "    val_log_marginal: -12169.69260759597\n",
      "Train Epoch: 2044 [256/118836 (0%)] Loss: 12358.105469\n",
      "Train Epoch: 2044 [33024/118836 (28%)] Loss: 12246.140625\n",
      "Train Epoch: 2044 [65792/118836 (55%)] Loss: 12372.855469\n",
      "Train Epoch: 2044 [98560/118836 (83%)] Loss: 12254.408203\n",
      "    epoch          : 2044\n",
      "    loss           : 12243.91349271738\n",
      "    val_loss       : 12241.674243043299\n",
      "    val_log_likelihood: -12158.964624689826\n",
      "    val_log_marginal: -12167.100422113548\n",
      "Train Epoch: 2045 [256/118836 (0%)] Loss: 12225.538086\n",
      "Train Epoch: 2045 [33024/118836 (28%)] Loss: 12192.587891\n",
      "Train Epoch: 2045 [65792/118836 (55%)] Loss: 12311.144531\n",
      "Train Epoch: 2045 [98560/118836 (83%)] Loss: 12234.237305\n",
      "    epoch          : 2045\n",
      "    loss           : 12241.514298684346\n",
      "    val_loss       : 12242.760702402336\n",
      "    val_log_likelihood: -12154.194177619676\n",
      "    val_log_marginal: -12162.409213906269\n",
      "Train Epoch: 2046 [256/118836 (0%)] Loss: 12209.142578\n",
      "Train Epoch: 2046 [33024/118836 (28%)] Loss: 12214.581055\n",
      "Train Epoch: 2046 [65792/118836 (55%)] Loss: 12224.244141\n",
      "Train Epoch: 2046 [98560/118836 (83%)] Loss: 12242.556641\n",
      "    epoch          : 2046\n",
      "    loss           : 12243.808555462934\n",
      "    val_loss       : 12244.377480110894\n",
      "    val_log_likelihood: -12155.21085914883\n",
      "    val_log_marginal: -12163.501057213965\n",
      "Train Epoch: 2047 [256/118836 (0%)] Loss: 12168.976562\n",
      "Train Epoch: 2047 [33024/118836 (28%)] Loss: 12143.712891\n",
      "Train Epoch: 2047 [65792/118836 (55%)] Loss: 12201.727539\n",
      "Train Epoch: 2047 [98560/118836 (83%)] Loss: 12264.832031\n",
      "    epoch          : 2047\n",
      "    loss           : 12242.249084341398\n",
      "    val_loss       : 12242.233831116542\n",
      "    val_log_likelihood: -12154.622086305832\n",
      "    val_log_marginal: -12162.729832694758\n",
      "Train Epoch: 2048 [256/118836 (0%)] Loss: 12255.085938\n",
      "Train Epoch: 2048 [33024/118836 (28%)] Loss: 12269.095703\n",
      "Train Epoch: 2048 [65792/118836 (55%)] Loss: 12189.603516\n",
      "Train Epoch: 2048 [98560/118836 (83%)] Loss: 12185.179688\n",
      "    epoch          : 2048\n",
      "    loss           : 12243.470153212882\n",
      "    val_loss       : 12241.653019915335\n",
      "    val_log_likelihood: -12154.700222775797\n",
      "    val_log_marginal: -12162.902675480633\n",
      "Train Epoch: 2049 [256/118836 (0%)] Loss: 12245.792969\n",
      "Train Epoch: 2049 [33024/118836 (28%)] Loss: 12222.787109\n",
      "Train Epoch: 2049 [65792/118836 (55%)] Loss: 12183.373047\n",
      "Train Epoch: 2049 [98560/118836 (83%)] Loss: 12284.457031\n",
      "    epoch          : 2049\n",
      "    loss           : 12241.971327672662\n",
      "    val_loss       : 12239.193478684936\n",
      "    val_log_likelihood: -12154.374062693858\n",
      "    val_log_marginal: -12162.466187043236\n",
      "Train Epoch: 2050 [256/118836 (0%)] Loss: 12179.433594\n",
      "Train Epoch: 2050 [33024/118836 (28%)] Loss: 12507.892578\n",
      "Train Epoch: 2050 [65792/118836 (55%)] Loss: 12280.868164\n",
      "Train Epoch: 2050 [98560/118836 (83%)] Loss: 12194.424805\n",
      "    epoch          : 2050\n",
      "    loss           : 12243.574535062553\n",
      "    val_loss       : 12248.374991592244\n",
      "    val_log_likelihood: -12158.106575197737\n",
      "    val_log_marginal: -12166.174805866365\n",
      "Train Epoch: 2051 [256/118836 (0%)] Loss: 12262.919922\n",
      "Train Epoch: 2051 [33024/118836 (28%)] Loss: 12276.053711\n",
      "Train Epoch: 2051 [65792/118836 (55%)] Loss: 12195.503906\n",
      "Train Epoch: 2051 [98560/118836 (83%)] Loss: 12286.529297\n",
      "    epoch          : 2051\n",
      "    loss           : 12245.421932834472\n",
      "    val_loss       : 12246.935318963655\n",
      "    val_log_likelihood: -12155.466742109957\n",
      "    val_log_marginal: -12163.778257555583\n",
      "Train Epoch: 2052 [256/118836 (0%)] Loss: 12275.371094\n",
      "Train Epoch: 2052 [33024/118836 (28%)] Loss: 12310.187500\n",
      "Train Epoch: 2052 [65792/118836 (55%)] Loss: 12291.459961\n",
      "Train Epoch: 2052 [98560/118836 (83%)] Loss: 12363.716797\n",
      "    epoch          : 2052\n",
      "    loss           : 12240.869875187396\n",
      "    val_loss       : 12239.67436046355\n",
      "    val_log_likelihood: -12155.669566467639\n",
      "    val_log_marginal: -12163.880773018245\n",
      "Train Epoch: 2053 [256/118836 (0%)] Loss: 12212.843750\n",
      "Train Epoch: 2053 [33024/118836 (28%)] Loss: 12267.001953\n",
      "Train Epoch: 2053 [65792/118836 (55%)] Loss: 12423.137695\n",
      "Train Epoch: 2053 [98560/118836 (83%)] Loss: 12188.133789\n",
      "    epoch          : 2053\n",
      "    loss           : 12242.015483806348\n",
      "    val_loss       : 12246.35353204455\n",
      "    val_log_likelihood: -12155.871380176022\n",
      "    val_log_marginal: -12164.137281738935\n",
      "Train Epoch: 2054 [256/118836 (0%)] Loss: 12174.338867\n",
      "Train Epoch: 2054 [33024/118836 (28%)] Loss: 12305.550781\n",
      "Train Epoch: 2054 [65792/118836 (55%)] Loss: 12161.230469\n",
      "Train Epoch: 2054 [98560/118836 (83%)] Loss: 12353.229492\n",
      "    epoch          : 2054\n",
      "    loss           : 12242.836146382599\n",
      "    val_loss       : 12243.772071863006\n",
      "    val_log_likelihood: -12153.574956381824\n",
      "    val_log_marginal: -12161.918690793327\n",
      "Train Epoch: 2055 [256/118836 (0%)] Loss: 12258.039062\n",
      "Train Epoch: 2055 [33024/118836 (28%)] Loss: 12242.194336\n",
      "Train Epoch: 2055 [65792/118836 (55%)] Loss: 12284.345703\n",
      "Train Epoch: 2055 [98560/118836 (83%)] Loss: 12310.444336\n",
      "    epoch          : 2055\n",
      "    loss           : 12246.679313676075\n",
      "    val_loss       : 12241.365188587419\n",
      "    val_log_likelihood: -12155.46897891465\n",
      "    val_log_marginal: -12163.479025007864\n",
      "Train Epoch: 2056 [256/118836 (0%)] Loss: 12259.922852\n",
      "Train Epoch: 2056 [33024/118836 (28%)] Loss: 12167.115234\n",
      "Train Epoch: 2056 [65792/118836 (55%)] Loss: 12245.500000\n",
      "Train Epoch: 2056 [98560/118836 (83%)] Loss: 12321.535156\n",
      "    epoch          : 2056\n",
      "    loss           : 12241.595171306348\n",
      "    val_loss       : 12240.026537923193\n",
      "    val_log_likelihood: -12155.80949196133\n",
      "    val_log_marginal: -12163.981808035813\n",
      "Train Epoch: 2057 [256/118836 (0%)] Loss: 12280.708008\n",
      "Train Epoch: 2057 [33024/118836 (28%)] Loss: 12279.961914\n",
      "Train Epoch: 2057 [65792/118836 (55%)] Loss: 12253.779297\n",
      "Train Epoch: 2057 [98560/118836 (83%)] Loss: 12212.306641\n",
      "    epoch          : 2057\n",
      "    loss           : 12246.86380256798\n",
      "    val_loss       : 12243.476542292803\n",
      "    val_log_likelihood: -12155.742805585713\n",
      "    val_log_marginal: -12164.159433219109\n",
      "Train Epoch: 2058 [256/118836 (0%)] Loss: 12400.282227\n",
      "Train Epoch: 2058 [33024/118836 (28%)] Loss: 12370.344727\n",
      "Train Epoch: 2058 [65792/118836 (55%)] Loss: 12401.499023\n",
      "Train Epoch: 2058 [98560/118836 (83%)] Loss: 12249.492188\n",
      "    epoch          : 2058\n",
      "    loss           : 12241.725223745088\n",
      "    val_loss       : 12240.45925051106\n",
      "    val_log_likelihood: -12157.526572677576\n",
      "    val_log_marginal: -12165.597104987599\n",
      "Train Epoch: 2059 [256/118836 (0%)] Loss: 12312.230469\n",
      "Train Epoch: 2059 [33024/118836 (28%)] Loss: 12247.230469\n",
      "Train Epoch: 2059 [65792/118836 (55%)] Loss: 12281.914062\n",
      "Train Epoch: 2059 [98560/118836 (83%)] Loss: 12194.127930\n",
      "    epoch          : 2059\n",
      "    loss           : 12239.528520309916\n",
      "    val_loss       : 12240.159968362885\n",
      "    val_log_likelihood: -12157.648344770989\n",
      "    val_log_marginal: -12165.7660449644\n",
      "Train Epoch: 2060 [256/118836 (0%)] Loss: 12211.302734\n",
      "Train Epoch: 2060 [33024/118836 (28%)] Loss: 12169.164062\n",
      "Train Epoch: 2060 [65792/118836 (55%)] Loss: 12210.804688\n",
      "Train Epoch: 2060 [98560/118836 (83%)] Loss: 12270.337891\n",
      "    epoch          : 2060\n",
      "    loss           : 12239.996364505789\n",
      "    val_loss       : 12242.344860218185\n",
      "    val_log_likelihood: -12156.302622906327\n",
      "    val_log_marginal: -12164.61081878227\n",
      "Train Epoch: 2061 [256/118836 (0%)] Loss: 12269.058594\n",
      "Train Epoch: 2061 [33024/118836 (28%)] Loss: 12281.298828\n",
      "Train Epoch: 2061 [65792/118836 (55%)] Loss: 12257.534180\n",
      "Train Epoch: 2061 [98560/118836 (83%)] Loss: 12354.248047\n",
      "    epoch          : 2061\n",
      "    loss           : 12240.451590124843\n",
      "    val_loss       : 12245.278575817907\n",
      "    val_log_likelihood: -12158.182477932434\n",
      "    val_log_marginal: -12166.280378539075\n",
      "Train Epoch: 2062 [256/118836 (0%)] Loss: 12285.887695\n",
      "Train Epoch: 2062 [33024/118836 (28%)] Loss: 12308.695312\n",
      "Train Epoch: 2062 [65792/118836 (55%)] Loss: 12189.175781\n",
      "Train Epoch: 2062 [98560/118836 (83%)] Loss: 12192.521484\n",
      "    epoch          : 2062\n",
      "    loss           : 12246.96793256307\n",
      "    val_loss       : 12245.889064374867\n",
      "    val_log_likelihood: -12155.556035301644\n",
      "    val_log_marginal: -12163.73079759602\n",
      "Train Epoch: 2063 [256/118836 (0%)] Loss: 12190.431641\n",
      "Train Epoch: 2063 [33024/118836 (28%)] Loss: 12292.382812\n",
      "Train Epoch: 2063 [65792/118836 (55%)] Loss: 12289.437500\n",
      "Train Epoch: 2063 [98560/118836 (83%)] Loss: 12322.320312\n",
      "    epoch          : 2063\n",
      "    loss           : 12239.878902211281\n",
      "    val_loss       : 12245.22348152751\n",
      "    val_log_likelihood: -12157.481166640819\n",
      "    val_log_marginal: -12165.594540181257\n",
      "Train Epoch: 2064 [256/118836 (0%)] Loss: 12186.221680\n",
      "Train Epoch: 2064 [33024/118836 (28%)] Loss: 12373.921875\n",
      "Train Epoch: 2064 [65792/118836 (55%)] Loss: 12338.669922\n",
      "Train Epoch: 2064 [98560/118836 (83%)] Loss: 12201.009766\n",
      "    epoch          : 2064\n",
      "    loss           : 12244.11922947684\n",
      "    val_loss       : 12241.812239552846\n",
      "    val_log_likelihood: -12154.587334574027\n",
      "    val_log_marginal: -12162.808844971409\n",
      "Train Epoch: 2065 [256/118836 (0%)] Loss: 12333.780273\n",
      "Train Epoch: 2065 [33024/118836 (28%)] Loss: 12187.498047\n",
      "Train Epoch: 2065 [65792/118836 (55%)] Loss: 12245.000000\n",
      "Train Epoch: 2065 [98560/118836 (83%)] Loss: 12328.687500\n",
      "    epoch          : 2065\n",
      "    loss           : 12242.611794031742\n",
      "    val_loss       : 12238.882699371501\n",
      "    val_log_likelihood: -12156.06998778691\n",
      "    val_log_marginal: -12164.17093262661\n",
      "Train Epoch: 2066 [256/118836 (0%)] Loss: 12220.729492\n",
      "Train Epoch: 2066 [33024/118836 (28%)] Loss: 12184.030273\n",
      "Train Epoch: 2066 [65792/118836 (55%)] Loss: 12339.688477\n",
      "Train Epoch: 2066 [98560/118836 (83%)] Loss: 12290.555664\n",
      "    epoch          : 2066\n",
      "    loss           : 12241.625994817514\n",
      "    val_loss       : 12243.116807234073\n",
      "    val_log_likelihood: -12160.88098344448\n",
      "    val_log_marginal: -12169.321343982343\n",
      "Train Epoch: 2067 [256/118836 (0%)] Loss: 12262.337891\n",
      "Train Epoch: 2067 [33024/118836 (28%)] Loss: 12240.162109\n",
      "Train Epoch: 2067 [65792/118836 (55%)] Loss: 12254.991211\n",
      "Train Epoch: 2067 [98560/118836 (83%)] Loss: 12310.117188\n",
      "    epoch          : 2067\n",
      "    loss           : 12242.091317075063\n",
      "    val_loss       : 12237.295039939694\n",
      "    val_log_likelihood: -12154.420816532258\n",
      "    val_log_marginal: -12162.507267103876\n",
      "Train Epoch: 2068 [256/118836 (0%)] Loss: 12243.052734\n",
      "Train Epoch: 2068 [33024/118836 (28%)] Loss: 12216.940430\n",
      "Train Epoch: 2068 [65792/118836 (55%)] Loss: 12356.738281\n",
      "Train Epoch: 2068 [98560/118836 (83%)] Loss: 12318.316406\n",
      "    epoch          : 2068\n",
      "    loss           : 12242.537254122724\n",
      "    val_loss       : 12240.034886588832\n",
      "    val_log_likelihood: -12156.721151907568\n",
      "    val_log_marginal: -12164.801693568385\n",
      "Train Epoch: 2069 [256/118836 (0%)] Loss: 12222.820312\n",
      "Train Epoch: 2069 [33024/118836 (28%)] Loss: 12220.340820\n",
      "Train Epoch: 2069 [65792/118836 (55%)] Loss: 12258.916992\n",
      "Train Epoch: 2069 [98560/118836 (83%)] Loss: 12208.743164\n",
      "    epoch          : 2069\n",
      "    loss           : 12244.865876046837\n",
      "    val_loss       : 12243.874473424567\n",
      "    val_log_likelihood: -12154.211087740385\n",
      "    val_log_marginal: -12162.293133375233\n",
      "Train Epoch: 2070 [256/118836 (0%)] Loss: 12268.521484\n",
      "Train Epoch: 2070 [33024/118836 (28%)] Loss: 12196.996094\n",
      "Train Epoch: 2070 [65792/118836 (55%)] Loss: 12277.512695\n",
      "Train Epoch: 2070 [98560/118836 (83%)] Loss: 12292.545898\n",
      "    epoch          : 2070\n",
      "    loss           : 12244.196085349462\n",
      "    val_loss       : 12242.314435852073\n",
      "    val_log_likelihood: -12155.411700494986\n",
      "    val_log_marginal: -12163.516247956159\n",
      "Train Epoch: 2071 [256/118836 (0%)] Loss: 12310.402344\n",
      "Train Epoch: 2071 [33024/118836 (28%)] Loss: 12332.152344\n",
      "Train Epoch: 2071 [65792/118836 (55%)] Loss: 12149.958984\n",
      "Train Epoch: 2071 [98560/118836 (83%)] Loss: 12273.123047\n",
      "    epoch          : 2071\n",
      "    loss           : 12242.507501033911\n",
      "    val_loss       : 12247.76188768862\n",
      "    val_log_likelihood: -12155.130434501654\n",
      "    val_log_marginal: -12163.35631939289\n",
      "Train Epoch: 2072 [256/118836 (0%)] Loss: 12227.293945\n",
      "Train Epoch: 2072 [33024/118836 (28%)] Loss: 12245.154297\n",
      "Train Epoch: 2072 [65792/118836 (55%)] Loss: 12231.574219\n",
      "Train Epoch: 2072 [98560/118836 (83%)] Loss: 12162.108398\n",
      "    epoch          : 2072\n",
      "    loss           : 12242.365203842277\n",
      "    val_loss       : 12242.064995684983\n",
      "    val_log_likelihood: -12155.491482501035\n",
      "    val_log_marginal: -12163.717615973635\n",
      "Train Epoch: 2073 [256/118836 (0%)] Loss: 12191.980469\n",
      "Train Epoch: 2073 [33024/118836 (28%)] Loss: 12246.601562\n",
      "Train Epoch: 2073 [65792/118836 (55%)] Loss: 12198.830078\n",
      "Train Epoch: 2073 [98560/118836 (83%)] Loss: 12289.839844\n",
      "    epoch          : 2073\n",
      "    loss           : 12239.478632101685\n",
      "    val_loss       : 12245.46410144148\n",
      "    val_log_likelihood: -12154.962660741057\n",
      "    val_log_marginal: -12163.574116869828\n",
      "Train Epoch: 2074 [256/118836 (0%)] Loss: 12267.732422\n",
      "Train Epoch: 2074 [33024/118836 (28%)] Loss: 12265.922852\n",
      "Train Epoch: 2074 [65792/118836 (55%)] Loss: 12271.064453\n",
      "Train Epoch: 2074 [98560/118836 (83%)] Loss: 12218.867188\n",
      "    epoch          : 2074\n",
      "    loss           : 12242.932958863214\n",
      "    val_loss       : 12242.681179779605\n",
      "    val_log_likelihood: -12159.546658524607\n",
      "    val_log_marginal: -12167.958031886581\n",
      "Train Epoch: 2075 [256/118836 (0%)] Loss: 12295.031250\n",
      "Train Epoch: 2075 [33024/118836 (28%)] Loss: 12173.605469\n",
      "Train Epoch: 2075 [65792/118836 (55%)] Loss: 12276.449219\n",
      "Train Epoch: 2075 [98560/118836 (83%)] Loss: 12250.000977\n",
      "    epoch          : 2075\n",
      "    loss           : 12245.608092464072\n",
      "    val_loss       : 12246.628997716958\n",
      "    val_log_likelihood: -12156.268717367142\n",
      "    val_log_marginal: -12164.51338312085\n",
      "Train Epoch: 2076 [256/118836 (0%)] Loss: 12248.902344\n",
      "Train Epoch: 2076 [33024/118836 (28%)] Loss: 12224.748047\n",
      "Train Epoch: 2076 [65792/118836 (55%)] Loss: 12341.215820\n",
      "Train Epoch: 2076 [98560/118836 (83%)] Loss: 12269.785156\n",
      "    epoch          : 2076\n",
      "    loss           : 12238.976309029933\n",
      "    val_loss       : 12244.54008884322\n",
      "    val_log_likelihood: -12155.246631869055\n",
      "    val_log_marginal: -12163.323421426896\n",
      "Train Epoch: 2077 [256/118836 (0%)] Loss: 12300.447266\n",
      "Train Epoch: 2077 [33024/118836 (28%)] Loss: 12195.056641\n",
      "Train Epoch: 2077 [65792/118836 (55%)] Loss: 12205.368164\n",
      "Train Epoch: 2077 [98560/118836 (83%)] Loss: 12210.267578\n",
      "    epoch          : 2077\n",
      "    loss           : 12244.648913422767\n",
      "    val_loss       : 12244.013801426936\n",
      "    val_log_likelihood: -12158.46422017163\n",
      "    val_log_marginal: -12166.77370635814\n",
      "Train Epoch: 2078 [256/118836 (0%)] Loss: 12174.061523\n",
      "Train Epoch: 2078 [33024/118836 (28%)] Loss: 12254.767578\n",
      "Train Epoch: 2078 [65792/118836 (55%)] Loss: 12347.273438\n",
      "Train Epoch: 2078 [98560/118836 (83%)] Loss: 12186.828125\n",
      "    epoch          : 2078\n",
      "    loss           : 12244.350940375569\n",
      "    val_loss       : 12238.258817508242\n",
      "    val_log_likelihood: -12157.599130867451\n",
      "    val_log_marginal: -12165.929021806303\n",
      "Train Epoch: 2079 [256/118836 (0%)] Loss: 12246.012695\n",
      "Train Epoch: 2079 [33024/118836 (28%)] Loss: 12366.800781\n",
      "Train Epoch: 2079 [65792/118836 (55%)] Loss: 12281.281250\n",
      "Train Epoch: 2079 [98560/118836 (83%)] Loss: 12305.588867\n",
      "    epoch          : 2079\n",
      "    loss           : 12244.63684782749\n",
      "    val_loss       : 12244.746055959391\n",
      "    val_log_likelihood: -12156.244422689206\n",
      "    val_log_marginal: -12164.618076554321\n",
      "Train Epoch: 2080 [256/118836 (0%)] Loss: 12162.578125\n",
      "Train Epoch: 2080 [33024/118836 (28%)] Loss: 12377.511719\n",
      "Train Epoch: 2080 [65792/118836 (55%)] Loss: 12241.270508\n",
      "Train Epoch: 2080 [98560/118836 (83%)] Loss: 12390.335938\n",
      "    epoch          : 2080\n",
      "    loss           : 12237.314563139733\n",
      "    val_loss       : 12243.425596520134\n",
      "    val_log_likelihood: -12154.976163959109\n",
      "    val_log_marginal: -12163.023793792849\n",
      "Train Epoch: 2081 [256/118836 (0%)] Loss: 12230.963867\n",
      "Train Epoch: 2081 [33024/118836 (28%)] Loss: 12358.036133\n",
      "Train Epoch: 2081 [65792/118836 (55%)] Loss: 12198.597656\n",
      "Train Epoch: 2081 [98560/118836 (83%)] Loss: 12226.232422\n",
      "    epoch          : 2081\n",
      "    loss           : 12245.35856531741\n",
      "    val_loss       : 12241.637675412632\n",
      "    val_log_likelihood: -12155.104327407724\n",
      "    val_log_marginal: -12163.171035167834\n",
      "Train Epoch: 2082 [256/118836 (0%)] Loss: 12278.638672\n",
      "Train Epoch: 2082 [33024/118836 (28%)] Loss: 12202.891602\n",
      "Train Epoch: 2082 [65792/118836 (55%)] Loss: 12246.515625\n",
      "Train Epoch: 2082 [98560/118836 (83%)] Loss: 12256.189453\n",
      "    epoch          : 2082\n",
      "    loss           : 12243.290404808984\n",
      "    val_loss       : 12242.696563580119\n",
      "    val_log_likelihood: -12156.499839097394\n",
      "    val_log_marginal: -12164.472622460371\n",
      "Train Epoch: 2083 [256/118836 (0%)] Loss: 12188.899414\n",
      "Train Epoch: 2083 [33024/118836 (28%)] Loss: 12250.668945\n",
      "Train Epoch: 2083 [65792/118836 (55%)] Loss: 12223.834961\n",
      "Train Epoch: 2083 [98560/118836 (83%)] Loss: 12277.549805\n",
      "    epoch          : 2083\n",
      "    loss           : 12243.203887833437\n",
      "    val_loss       : 12241.747024450395\n",
      "    val_log_likelihood: -12155.19002290762\n",
      "    val_log_marginal: -12163.20438283829\n",
      "Train Epoch: 2084 [256/118836 (0%)] Loss: 12277.722656\n",
      "Train Epoch: 2084 [33024/118836 (28%)] Loss: 12238.804688\n",
      "Train Epoch: 2084 [65792/118836 (55%)] Loss: 12356.613281\n",
      "Train Epoch: 2084 [98560/118836 (83%)] Loss: 12224.069336\n",
      "    epoch          : 2084\n",
      "    loss           : 12244.056219467277\n",
      "    val_loss       : 12243.80862151193\n",
      "    val_log_likelihood: -12156.460337830851\n",
      "    val_log_marginal: -12164.868839279907\n",
      "Train Epoch: 2085 [256/118836 (0%)] Loss: 12234.644531\n",
      "Train Epoch: 2085 [33024/118836 (28%)] Loss: 12255.789062\n",
      "Train Epoch: 2085 [65792/118836 (55%)] Loss: 12258.251953\n",
      "Train Epoch: 2085 [98560/118836 (83%)] Loss: 12277.215820\n",
      "    epoch          : 2085\n",
      "    loss           : 12242.733612166563\n",
      "    val_loss       : 12242.798204317396\n",
      "    val_log_likelihood: -12156.490748584833\n",
      "    val_log_marginal: -12164.873017487673\n",
      "Train Epoch: 2086 [256/118836 (0%)] Loss: 12268.533203\n",
      "Train Epoch: 2086 [33024/118836 (28%)] Loss: 12283.439453\n",
      "Train Epoch: 2086 [65792/118836 (55%)] Loss: 12249.556641\n",
      "Train Epoch: 2086 [98560/118836 (83%)] Loss: 12313.785156\n",
      "    epoch          : 2086\n",
      "    loss           : 12241.283520083747\n",
      "    val_loss       : 12246.609401109597\n",
      "    val_log_likelihood: -12156.117251634874\n",
      "    val_log_marginal: -12164.168781571949\n",
      "Train Epoch: 2087 [256/118836 (0%)] Loss: 12339.951172\n",
      "Train Epoch: 2087 [33024/118836 (28%)] Loss: 12263.106445\n",
      "Train Epoch: 2087 [65792/118836 (55%)] Loss: 12394.792969\n",
      "Train Epoch: 2087 [98560/118836 (83%)] Loss: 12188.344727\n",
      "    epoch          : 2087\n",
      "    loss           : 12240.969573737335\n",
      "    val_loss       : 12238.903673870844\n",
      "    val_log_likelihood: -12155.456159371124\n",
      "    val_log_marginal: -12163.550829286834\n",
      "Train Epoch: 2088 [256/118836 (0%)] Loss: 12291.751953\n",
      "Train Epoch: 2088 [33024/118836 (28%)] Loss: 12251.673828\n",
      "Train Epoch: 2088 [65792/118836 (55%)] Loss: 12254.046875\n",
      "Train Epoch: 2088 [98560/118836 (83%)] Loss: 12202.941406\n",
      "    epoch          : 2088\n",
      "    loss           : 12245.668766477978\n",
      "    val_loss       : 12239.54569904773\n",
      "    val_log_likelihood: -12157.422631694582\n",
      "    val_log_marginal: -12165.577052397084\n",
      "Train Epoch: 2089 [256/118836 (0%)] Loss: 12385.493164\n",
      "Train Epoch: 2089 [33024/118836 (28%)] Loss: 12215.772461\n",
      "Train Epoch: 2089 [65792/118836 (55%)] Loss: 12242.302734\n",
      "Train Epoch: 2089 [98560/118836 (83%)] Loss: 12209.380859\n",
      "    epoch          : 2089\n",
      "    loss           : 12243.070184876447\n",
      "    val_loss       : 12237.425161537343\n",
      "    val_log_likelihood: -12157.82775327621\n",
      "    val_log_marginal: -12166.05451598323\n",
      "Train Epoch: 2090 [256/118836 (0%)] Loss: 12194.083008\n",
      "Train Epoch: 2090 [33024/118836 (28%)] Loss: 12343.597656\n",
      "Train Epoch: 2090 [65792/118836 (55%)] Loss: 12275.785156\n",
      "Train Epoch: 2090 [98560/118836 (83%)] Loss: 12271.097656\n",
      "    epoch          : 2090\n",
      "    loss           : 12238.235564483819\n",
      "    val_loss       : 12239.69968856635\n",
      "    val_log_likelihood: -12156.480524484337\n",
      "    val_log_marginal: -12164.56050821404\n",
      "Train Epoch: 2091 [256/118836 (0%)] Loss: 12446.724609\n",
      "Train Epoch: 2091 [33024/118836 (28%)] Loss: 12298.259766\n",
      "Train Epoch: 2091 [65792/118836 (55%)] Loss: 12181.291992\n",
      "Train Epoch: 2091 [98560/118836 (83%)] Loss: 12274.184570\n",
      "    epoch          : 2091\n",
      "    loss           : 12243.632663875103\n",
      "    val_loss       : 12239.949773196644\n",
      "    val_log_likelihood: -12154.48674298232\n",
      "    val_log_marginal: -12162.720407837662\n",
      "Train Epoch: 2092 [256/118836 (0%)] Loss: 12198.804688\n",
      "Train Epoch: 2092 [33024/118836 (28%)] Loss: 12236.728516\n",
      "Train Epoch: 2092 [65792/118836 (55%)] Loss: 12230.941406\n",
      "Train Epoch: 2092 [98560/118836 (83%)] Loss: 12235.139648\n",
      "    epoch          : 2092\n",
      "    loss           : 12243.097327013544\n",
      "    val_loss       : 12239.402177880245\n",
      "    val_log_likelihood: -12154.02121264992\n",
      "    val_log_marginal: -12162.154818831586\n",
      "Train Epoch: 2093 [256/118836 (0%)] Loss: 12269.090820\n",
      "Train Epoch: 2093 [33024/118836 (28%)] Loss: 12301.666016\n",
      "Train Epoch: 2093 [65792/118836 (55%)] Loss: 12246.798828\n",
      "Train Epoch: 2093 [98560/118836 (83%)] Loss: 12246.457031\n",
      "    epoch          : 2093\n",
      "    loss           : 12239.056662918736\n",
      "    val_loss       : 12240.379381753377\n",
      "    val_log_likelihood: -12156.223033951097\n",
      "    val_log_marginal: -12164.374251210966\n",
      "Train Epoch: 2094 [256/118836 (0%)] Loss: 12316.197266\n",
      "Train Epoch: 2094 [33024/118836 (28%)] Loss: 12299.616211\n",
      "Train Epoch: 2094 [65792/118836 (55%)] Loss: 12245.458984\n",
      "Train Epoch: 2094 [98560/118836 (83%)] Loss: 12415.487305\n",
      "    epoch          : 2094\n",
      "    loss           : 12241.733315239866\n",
      "    val_loss       : 12243.872438032524\n",
      "    val_log_likelihood: -12157.613660243485\n",
      "    val_log_marginal: -12166.024585824644\n",
      "Train Epoch: 2095 [256/118836 (0%)] Loss: 12457.672852\n",
      "Train Epoch: 2095 [33024/118836 (28%)] Loss: 12272.538086\n",
      "Train Epoch: 2095 [65792/118836 (55%)] Loss: 12243.999023\n",
      "Train Epoch: 2095 [98560/118836 (83%)] Loss: 12274.511719\n",
      "    epoch          : 2095\n",
      "    loss           : 12242.506203312396\n",
      "    val_loss       : 12245.54541578833\n",
      "    val_log_likelihood: -12154.621097627172\n",
      "    val_log_marginal: -12162.670474085537\n",
      "Train Epoch: 2096 [256/118836 (0%)] Loss: 12327.102539\n",
      "Train Epoch: 2096 [33024/118836 (28%)] Loss: 12288.375977\n",
      "Train Epoch: 2096 [65792/118836 (55%)] Loss: 12226.480469\n",
      "Train Epoch: 2096 [98560/118836 (83%)] Loss: 12193.228516\n",
      "    epoch          : 2096\n",
      "    loss           : 12241.46495974204\n",
      "    val_loss       : 12241.563920202889\n",
      "    val_log_likelihood: -12155.575482707816\n",
      "    val_log_marginal: -12163.746276193104\n",
      "Train Epoch: 2097 [256/118836 (0%)] Loss: 12208.580078\n",
      "Train Epoch: 2097 [33024/118836 (28%)] Loss: 12302.254883\n",
      "Train Epoch: 2097 [65792/118836 (55%)] Loss: 12219.550781\n",
      "Train Epoch: 2097 [98560/118836 (83%)] Loss: 12365.618164\n",
      "    epoch          : 2097\n",
      "    loss           : 12242.846554325632\n",
      "    val_loss       : 12242.629378778402\n",
      "    val_log_likelihood: -12157.893451942464\n",
      "    val_log_marginal: -12166.237032300343\n",
      "Train Epoch: 2098 [256/118836 (0%)] Loss: 12205.937500\n",
      "Train Epoch: 2098 [33024/118836 (28%)] Loss: 12325.421875\n",
      "Train Epoch: 2098 [65792/118836 (55%)] Loss: 12247.395508\n",
      "Train Epoch: 2098 [98560/118836 (83%)] Loss: 12240.305664\n",
      "    epoch          : 2098\n",
      "    loss           : 12238.422003431297\n",
      "    val_loss       : 12241.082332059905\n",
      "    val_log_likelihood: -12158.340389623398\n",
      "    val_log_marginal: -12166.560131424872\n",
      "Train Epoch: 2099 [256/118836 (0%)] Loss: 12289.306641\n",
      "Train Epoch: 2099 [33024/118836 (28%)] Loss: 12353.861328\n",
      "Train Epoch: 2099 [65792/118836 (55%)] Loss: 12238.216797\n",
      "Train Epoch: 2099 [98560/118836 (83%)] Loss: 12221.761719\n",
      "    epoch          : 2099\n",
      "    loss           : 12239.057954986043\n",
      "    val_loss       : 12239.552518326296\n",
      "    val_log_likelihood: -12156.775237799835\n",
      "    val_log_marginal: -12164.960315262846\n",
      "Train Epoch: 2100 [256/118836 (0%)] Loss: 12359.382812\n",
      "Train Epoch: 2100 [33024/118836 (28%)] Loss: 12275.041992\n",
      "Train Epoch: 2100 [65792/118836 (55%)] Loss: 12214.687500\n",
      "Train Epoch: 2100 [98560/118836 (83%)] Loss: 12187.035156\n",
      "    epoch          : 2100\n",
      "    loss           : 12241.934992762614\n",
      "    val_loss       : 12235.915584323986\n",
      "    val_log_likelihood: -12157.347581291357\n",
      "    val_log_marginal: -12165.584299783246\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch2100.pth ...\n",
      "Train Epoch: 2101 [256/118836 (0%)] Loss: 12171.707031\n",
      "Train Epoch: 2101 [33024/118836 (28%)] Loss: 12244.524414\n",
      "Train Epoch: 2101 [65792/118836 (55%)] Loss: 12283.226562\n",
      "Train Epoch: 2101 [98560/118836 (83%)] Loss: 12318.671875\n",
      "    epoch          : 2101\n",
      "    loss           : 12240.884432349825\n",
      "    val_loss       : 12241.633259671324\n",
      "    val_log_likelihood: -12156.71153959238\n",
      "    val_log_marginal: -12164.866312292139\n",
      "Train Epoch: 2102 [256/118836 (0%)] Loss: 12247.085938\n",
      "Train Epoch: 2102 [33024/118836 (28%)] Loss: 12242.624023\n",
      "Train Epoch: 2102 [65792/118836 (55%)] Loss: 12217.860352\n",
      "Train Epoch: 2102 [98560/118836 (83%)] Loss: 12204.395508\n",
      "    epoch          : 2102\n",
      "    loss           : 12244.442723842018\n",
      "    val_loss       : 12239.93416568333\n",
      "    val_log_likelihood: -12155.022772242039\n",
      "    val_log_marginal: -12163.098290553475\n",
      "Train Epoch: 2103 [256/118836 (0%)] Loss: 12331.322266\n",
      "Train Epoch: 2103 [33024/118836 (28%)] Loss: 12282.226562\n",
      "Train Epoch: 2103 [65792/118836 (55%)] Loss: 12291.132812\n",
      "Train Epoch: 2103 [98560/118836 (83%)] Loss: 12223.512695\n",
      "    epoch          : 2103\n",
      "    loss           : 12235.680528361507\n",
      "    val_loss       : 12236.92752998054\n",
      "    val_log_likelihood: -12153.855482804745\n",
      "    val_log_marginal: -12161.927204055757\n",
      "Train Epoch: 2104 [256/118836 (0%)] Loss: 12299.745117\n",
      "Train Epoch: 2104 [33024/118836 (28%)] Loss: 12262.994141\n",
      "Train Epoch: 2104 [65792/118836 (55%)] Loss: 12202.511719\n",
      "Train Epoch: 2104 [98560/118836 (83%)] Loss: 12210.937500\n",
      "    epoch          : 2104\n",
      "    loss           : 12242.933483735267\n",
      "    val_loss       : 12237.158827671174\n",
      "    val_log_likelihood: -12155.496737845067\n",
      "    val_log_marginal: -12163.672032211634\n",
      "Train Epoch: 2105 [256/118836 (0%)] Loss: 12195.960938\n",
      "Train Epoch: 2105 [33024/118836 (28%)] Loss: 12225.756836\n",
      "Train Epoch: 2105 [65792/118836 (55%)] Loss: 12270.584961\n",
      "Train Epoch: 2105 [98560/118836 (83%)] Loss: 12299.708008\n",
      "    epoch          : 2105\n",
      "    loss           : 12238.868932227047\n",
      "    val_loss       : 12242.392628809454\n",
      "    val_log_likelihood: -12155.27850076251\n",
      "    val_log_marginal: -12163.435525909004\n",
      "Train Epoch: 2106 [256/118836 (0%)] Loss: 12212.197266\n",
      "Train Epoch: 2106 [33024/118836 (28%)] Loss: 12295.937500\n",
      "Train Epoch: 2106 [65792/118836 (55%)] Loss: 12179.006836\n",
      "Train Epoch: 2106 [98560/118836 (83%)] Loss: 12241.642578\n",
      "    epoch          : 2106\n",
      "    loss           : 12238.288101930832\n",
      "    val_loss       : 12241.25269682389\n",
      "    val_log_likelihood: -12155.135181774709\n",
      "    val_log_marginal: -12163.32279959615\n",
      "Train Epoch: 2107 [256/118836 (0%)] Loss: 12293.885742\n",
      "Train Epoch: 2107 [33024/118836 (28%)] Loss: 12240.562500\n",
      "Train Epoch: 2107 [65792/118836 (55%)] Loss: 12231.587891\n",
      "Train Epoch: 2107 [98560/118836 (83%)] Loss: 12245.166016\n",
      "    epoch          : 2107\n",
      "    loss           : 12245.53755670363\n",
      "    val_loss       : 12240.346653176273\n",
      "    val_log_likelihood: -12154.969068089587\n",
      "    val_log_marginal: -12163.114730448775\n",
      "Train Epoch: 2108 [256/118836 (0%)] Loss: 12183.240234\n",
      "Train Epoch: 2108 [33024/118836 (28%)] Loss: 12231.937500\n",
      "Train Epoch: 2108 [65792/118836 (55%)] Loss: 12305.871094\n",
      "Train Epoch: 2108 [98560/118836 (83%)] Loss: 12327.197266\n",
      "    epoch          : 2108\n",
      "    loss           : 12244.41110615695\n",
      "    val_loss       : 12239.195142824079\n",
      "    val_log_likelihood: -12155.099734090672\n",
      "    val_log_marginal: -12163.293218603923\n",
      "Train Epoch: 2109 [256/118836 (0%)] Loss: 12243.554688\n",
      "Train Epoch: 2109 [33024/118836 (28%)] Loss: 12232.304688\n",
      "Train Epoch: 2109 [65792/118836 (55%)] Loss: 12203.755859\n",
      "Train Epoch: 2109 [98560/118836 (83%)] Loss: 12208.994141\n",
      "    epoch          : 2109\n",
      "    loss           : 12239.639031159533\n",
      "    val_loss       : 12241.813068415773\n",
      "    val_log_likelihood: -12154.623152689464\n",
      "    val_log_marginal: -12162.60111131383\n",
      "Train Epoch: 2110 [256/118836 (0%)] Loss: 12265.462891\n",
      "Train Epoch: 2110 [33024/118836 (28%)] Loss: 12312.703125\n",
      "Train Epoch: 2110 [65792/118836 (55%)] Loss: 12241.908203\n",
      "Train Epoch: 2110 [98560/118836 (83%)] Loss: 12285.718750\n",
      "    epoch          : 2110\n",
      "    loss           : 12238.861800655242\n",
      "    val_loss       : 12248.424492301536\n",
      "    val_log_likelihood: -12157.246129452285\n",
      "    val_log_marginal: -12165.30478261275\n",
      "Train Epoch: 2111 [256/118836 (0%)] Loss: 12264.460938\n",
      "Train Epoch: 2111 [33024/118836 (28%)] Loss: 12334.552734\n",
      "Train Epoch: 2111 [65792/118836 (55%)] Loss: 12346.767578\n",
      "Train Epoch: 2111 [98560/118836 (83%)] Loss: 12362.666992\n",
      "    epoch          : 2111\n",
      "    loss           : 12241.14082435122\n",
      "    val_loss       : 12240.208837646273\n",
      "    val_log_likelihood: -12155.494618324803\n",
      "    val_log_marginal: -12163.623317475889\n",
      "Train Epoch: 2112 [256/118836 (0%)] Loss: 12262.105469\n",
      "Train Epoch: 2112 [33024/118836 (28%)] Loss: 12267.591797\n",
      "Train Epoch: 2112 [65792/118836 (55%)] Loss: 12356.487305\n",
      "Train Epoch: 2112 [98560/118836 (83%)] Loss: 12400.709961\n",
      "    epoch          : 2112\n",
      "    loss           : 12240.746738652813\n",
      "    val_loss       : 12242.68104986932\n",
      "    val_log_likelihood: -12157.968848706318\n",
      "    val_log_marginal: -12166.33533189295\n",
      "Train Epoch: 2113 [256/118836 (0%)] Loss: 12269.468750\n",
      "Train Epoch: 2113 [33024/118836 (28%)] Loss: 12293.524414\n",
      "Train Epoch: 2113 [65792/118836 (55%)] Loss: 12366.246094\n",
      "Train Epoch: 2113 [98560/118836 (83%)] Loss: 12298.434570\n",
      "    epoch          : 2113\n",
      "    loss           : 12242.279799937965\n",
      "    val_loss       : 12240.265556756043\n",
      "    val_log_likelihood: -12156.368083611196\n",
      "    val_log_marginal: -12164.60594013771\n",
      "Train Epoch: 2114 [256/118836 (0%)] Loss: 12266.942383\n",
      "Train Epoch: 2114 [33024/118836 (28%)] Loss: 12210.957031\n",
      "Train Epoch: 2114 [65792/118836 (55%)] Loss: 12217.525391\n",
      "Train Epoch: 2114 [98560/118836 (83%)] Loss: 12296.500977\n",
      "    epoch          : 2114\n",
      "    loss           : 12244.03962016646\n",
      "    val_loss       : 12236.17603527008\n",
      "    val_log_likelihood: -12156.43401409998\n",
      "    val_log_marginal: -12164.705215420961\n",
      "Train Epoch: 2115 [256/118836 (0%)] Loss: 12260.297852\n",
      "Train Epoch: 2115 [33024/118836 (28%)] Loss: 12300.964844\n",
      "Train Epoch: 2115 [65792/118836 (55%)] Loss: 12246.083984\n",
      "Train Epoch: 2115 [98560/118836 (83%)] Loss: 12240.507812\n",
      "    epoch          : 2115\n",
      "    loss           : 12244.089400298542\n",
      "    val_loss       : 12245.957119207\n",
      "    val_log_likelihood: -12156.186227964745\n",
      "    val_log_marginal: -12164.879526281607\n",
      "Train Epoch: 2116 [256/118836 (0%)] Loss: 12237.046875\n",
      "Train Epoch: 2116 [33024/118836 (28%)] Loss: 12248.595703\n",
      "Train Epoch: 2116 [65792/118836 (55%)] Loss: 12356.718750\n",
      "Train Epoch: 2116 [98560/118836 (83%)] Loss: 12241.410156\n",
      "    epoch          : 2116\n",
      "    loss           : 12239.309327343104\n",
      "    val_loss       : 12241.998650119112\n",
      "    val_log_likelihood: -12154.306960653174\n",
      "    val_log_marginal: -12162.392339503142\n",
      "Train Epoch: 2117 [256/118836 (0%)] Loss: 12265.366211\n",
      "Train Epoch: 2117 [33024/118836 (28%)] Loss: 12253.833984\n",
      "Train Epoch: 2117 [65792/118836 (55%)] Loss: 12276.685547\n",
      "Train Epoch: 2117 [98560/118836 (83%)] Loss: 12207.528320\n",
      "    epoch          : 2117\n",
      "    loss           : 12242.513840047302\n",
      "    val_loss       : 12239.914430471466\n",
      "    val_log_likelihood: -12158.396274038463\n",
      "    val_log_marginal: -12166.382763382378\n",
      "Train Epoch: 2118 [256/118836 (0%)] Loss: 12235.509766\n",
      "Train Epoch: 2118 [33024/118836 (28%)] Loss: 12295.906250\n",
      "Train Epoch: 2118 [65792/118836 (55%)] Loss: 12267.289062\n",
      "Train Epoch: 2118 [98560/118836 (83%)] Loss: 12238.223633\n",
      "    epoch          : 2118\n",
      "    loss           : 12250.282223008428\n",
      "    val_loss       : 12240.916926324187\n",
      "    val_log_likelihood: -12154.507606202182\n",
      "    val_log_marginal: -12162.791535695464\n",
      "Train Epoch: 2119 [256/118836 (0%)] Loss: 12213.179688\n",
      "Train Epoch: 2119 [33024/118836 (28%)] Loss: 12310.461914\n",
      "Train Epoch: 2119 [65792/118836 (55%)] Loss: 12229.246094\n",
      "Train Epoch: 2119 [98560/118836 (83%)] Loss: 12254.433594\n",
      "    epoch          : 2119\n",
      "    loss           : 12242.706300080128\n",
      "    val_loss       : 12244.422436508272\n",
      "    val_log_likelihood: -12154.458580987644\n",
      "    val_log_marginal: -12162.563017947106\n",
      "Train Epoch: 2120 [256/118836 (0%)] Loss: 12194.888672\n",
      "Train Epoch: 2120 [33024/118836 (28%)] Loss: 12257.202148\n",
      "Train Epoch: 2120 [65792/118836 (55%)] Loss: 12390.863281\n",
      "Train Epoch: 2120 [98560/118836 (83%)] Loss: 12249.218750\n",
      "    epoch          : 2120\n",
      "    loss           : 12239.222509079042\n",
      "    val_loss       : 12243.564585270076\n",
      "    val_log_likelihood: -12156.03009007961\n",
      "    val_log_marginal: -12164.21097571298\n",
      "Train Epoch: 2121 [256/118836 (0%)] Loss: 12358.755859\n",
      "Train Epoch: 2121 [33024/118836 (28%)] Loss: 12291.766602\n",
      "Train Epoch: 2121 [65792/118836 (55%)] Loss: 12408.309570\n",
      "Train Epoch: 2121 [98560/118836 (83%)] Loss: 12185.638672\n",
      "    epoch          : 2121\n",
      "    loss           : 12243.768531586022\n",
      "    val_loss       : 12242.342521452905\n",
      "    val_log_likelihood: -12155.255981344346\n",
      "    val_log_marginal: -12163.474065230701\n",
      "Train Epoch: 2122 [256/118836 (0%)] Loss: 12197.712891\n",
      "Train Epoch: 2122 [33024/118836 (28%)] Loss: 12218.862305\n",
      "Train Epoch: 2122 [65792/118836 (55%)] Loss: 12337.508789\n",
      "Train Epoch: 2122 [98560/118836 (83%)] Loss: 12351.798828\n",
      "    epoch          : 2122\n",
      "    loss           : 12246.224774639422\n",
      "    val_loss       : 12243.47724614482\n",
      "    val_log_likelihood: -12158.742362457351\n",
      "    val_log_marginal: -12167.351205377021\n",
      "Train Epoch: 2123 [256/118836 (0%)] Loss: 12198.316406\n",
      "Train Epoch: 2123 [33024/118836 (28%)] Loss: 12150.582031\n",
      "Train Epoch: 2123 [65792/118836 (55%)] Loss: 12328.150391\n",
      "Train Epoch: 2123 [98560/118836 (83%)] Loss: 12307.879883\n",
      "    epoch          : 2123\n",
      "    loss           : 12242.943102673957\n",
      "    val_loss       : 12240.679488350896\n",
      "    val_log_likelihood: -12155.189522106339\n",
      "    val_log_marginal: -12163.42350753547\n",
      "Train Epoch: 2124 [256/118836 (0%)] Loss: 12234.152344\n",
      "Train Epoch: 2124 [33024/118836 (28%)] Loss: 12188.578125\n",
      "Train Epoch: 2124 [65792/118836 (55%)] Loss: 12182.376953\n",
      "Train Epoch: 2124 [98560/118836 (83%)] Loss: 12286.237305\n",
      "    epoch          : 2124\n",
      "    loss           : 12240.085810199544\n",
      "    val_loss       : 12235.698042135084\n",
      "    val_log_likelihood: -12155.434039947788\n",
      "    val_log_marginal: -12163.693038623382\n",
      "Train Epoch: 2125 [256/118836 (0%)] Loss: 12214.463867\n",
      "Train Epoch: 2125 [33024/118836 (28%)] Loss: 12295.508789\n",
      "Train Epoch: 2125 [65792/118836 (55%)] Loss: 12301.660156\n",
      "Train Epoch: 2125 [98560/118836 (83%)] Loss: 12313.759766\n",
      "    epoch          : 2125\n",
      "    loss           : 12241.665499638131\n",
      "    val_loss       : 12241.669530235296\n",
      "    val_log_likelihood: -12155.086555424163\n",
      "    val_log_marginal: -12163.282238155292\n",
      "Train Epoch: 2126 [256/118836 (0%)] Loss: 12242.869141\n",
      "Train Epoch: 2126 [33024/118836 (28%)] Loss: 12248.883789\n",
      "Train Epoch: 2126 [65792/118836 (55%)] Loss: 12226.092773\n",
      "Train Epoch: 2126 [98560/118836 (83%)] Loss: 12282.117188\n",
      "    epoch          : 2126\n",
      "    loss           : 12242.987549110834\n",
      "    val_loss       : 12239.811984225991\n",
      "    val_log_likelihood: -12156.864388990125\n",
      "    val_log_marginal: -12165.035396096171\n",
      "Train Epoch: 2127 [256/118836 (0%)] Loss: 12331.038086\n",
      "Train Epoch: 2127 [33024/118836 (28%)] Loss: 12269.084961\n",
      "Train Epoch: 2127 [65792/118836 (55%)] Loss: 12228.609375\n",
      "Train Epoch: 2127 [98560/118836 (83%)] Loss: 12171.312500\n",
      "    epoch          : 2127\n",
      "    loss           : 12245.295280190241\n",
      "    val_loss       : 12243.202158622325\n",
      "    val_log_likelihood: -12157.278018216242\n",
      "    val_log_marginal: -12165.53530159724\n",
      "Train Epoch: 2128 [256/118836 (0%)] Loss: 12218.944336\n",
      "Train Epoch: 2128 [33024/118836 (28%)] Loss: 12244.303711\n",
      "Train Epoch: 2128 [65792/118836 (55%)] Loss: 12218.278320\n",
      "Train Epoch: 2128 [98560/118836 (83%)] Loss: 12320.275391\n",
      "    epoch          : 2128\n",
      "    loss           : 12244.426560722963\n",
      "    val_loss       : 12249.069522771602\n",
      "    val_log_likelihood: -12157.454272319583\n",
      "    val_log_marginal: -12165.662251129605\n",
      "Train Epoch: 2129 [256/118836 (0%)] Loss: 12356.181641\n",
      "Train Epoch: 2129 [33024/118836 (28%)] Loss: 12304.594727\n",
      "Train Epoch: 2129 [65792/118836 (55%)] Loss: 12154.648438\n",
      "Train Epoch: 2129 [98560/118836 (83%)] Loss: 12259.238281\n",
      "    epoch          : 2129\n",
      "    loss           : 12246.620065168787\n",
      "    val_loss       : 12240.105202108622\n",
      "    val_log_likelihood: -12156.840386230873\n",
      "    val_log_marginal: -12165.212720609281\n",
      "Train Epoch: 2130 [256/118836 (0%)] Loss: 12200.584961\n",
      "Train Epoch: 2130 [33024/118836 (28%)] Loss: 12374.818359\n",
      "Train Epoch: 2130 [65792/118836 (55%)] Loss: 12222.988281\n",
      "Train Epoch: 2130 [98560/118836 (83%)] Loss: 12205.416992\n",
      "    epoch          : 2130\n",
      "    loss           : 12243.996895678247\n",
      "    val_loss       : 12235.672479115408\n",
      "    val_log_likelihood: -12155.70188155888\n",
      "    val_log_marginal: -12163.879152615606\n",
      "Train Epoch: 2131 [256/118836 (0%)] Loss: 12234.769531\n",
      "Train Epoch: 2131 [33024/118836 (28%)] Loss: 12222.228516\n",
      "Train Epoch: 2131 [65792/118836 (55%)] Loss: 12200.279297\n",
      "Train Epoch: 2131 [98560/118836 (83%)] Loss: 12167.355469\n",
      "    epoch          : 2131\n",
      "    loss           : 12241.236362373345\n",
      "    val_loss       : 12237.440758060731\n",
      "    val_log_likelihood: -12156.134859484853\n",
      "    val_log_marginal: -12164.391086939988\n",
      "Train Epoch: 2132 [256/118836 (0%)] Loss: 12296.165039\n",
      "Train Epoch: 2132 [33024/118836 (28%)] Loss: 12281.488281\n",
      "Train Epoch: 2132 [65792/118836 (55%)] Loss: 12278.836914\n",
      "Train Epoch: 2132 [98560/118836 (83%)] Loss: 12349.087891\n",
      "    epoch          : 2132\n",
      "    loss           : 12244.728266839847\n",
      "    val_loss       : 12243.700130791776\n",
      "    val_log_likelihood: -12157.389760229269\n",
      "    val_log_marginal: -12165.879542900046\n",
      "Train Epoch: 2133 [256/118836 (0%)] Loss: 12279.594727\n",
      "Train Epoch: 2133 [33024/118836 (28%)] Loss: 12264.616211\n",
      "Train Epoch: 2133 [65792/118836 (55%)] Loss: 12293.905273\n",
      "Train Epoch: 2133 [98560/118836 (83%)] Loss: 12256.318359\n",
      "    epoch          : 2133\n",
      "    loss           : 12248.978681050972\n",
      "    val_loss       : 12246.879431947633\n",
      "    val_log_likelihood: -12156.253918850807\n",
      "    val_log_marginal: -12164.599275539644\n",
      "Train Epoch: 2134 [256/118836 (0%)] Loss: 12253.614258\n",
      "Train Epoch: 2134 [33024/118836 (28%)] Loss: 12229.224609\n",
      "Train Epoch: 2134 [65792/118836 (55%)] Loss: 12245.033203\n",
      "Train Epoch: 2134 [98560/118836 (83%)] Loss: 12256.900391\n",
      "    epoch          : 2134\n",
      "    loss           : 12243.745475018093\n",
      "    val_loss       : 12247.41792918041\n",
      "    val_log_likelihood: -12156.671254329507\n",
      "    val_log_marginal: -12164.767780053468\n",
      "Train Epoch: 2135 [256/118836 (0%)] Loss: 12451.725586\n",
      "Train Epoch: 2135 [33024/118836 (28%)] Loss: 12248.186523\n",
      "Train Epoch: 2135 [65792/118836 (55%)] Loss: 12298.081055\n",
      "Train Epoch: 2135 [98560/118836 (83%)] Loss: 12276.797852\n",
      "    epoch          : 2135\n",
      "    loss           : 12243.476415329042\n",
      "    val_loss       : 12242.49407826857\n",
      "    val_log_likelihood: -12154.276284959162\n",
      "    val_log_marginal: -12162.49265005841\n",
      "Train Epoch: 2136 [256/118836 (0%)] Loss: 12317.847656\n",
      "Train Epoch: 2136 [33024/118836 (28%)] Loss: 12254.135742\n",
      "Train Epoch: 2136 [65792/118836 (55%)] Loss: 12352.157227\n",
      "Train Epoch: 2136 [98560/118836 (83%)] Loss: 12252.921875\n",
      "    epoch          : 2136\n",
      "    loss           : 12238.860524742813\n",
      "    val_loss       : 12241.95050732109\n",
      "    val_log_likelihood: -12152.853943406224\n",
      "    val_log_marginal: -12160.942601048579\n",
      "Train Epoch: 2137 [256/118836 (0%)] Loss: 12313.085938\n",
      "Train Epoch: 2137 [33024/118836 (28%)] Loss: 12196.749023\n",
      "Train Epoch: 2137 [65792/118836 (55%)] Loss: 12285.533203\n",
      "Train Epoch: 2137 [98560/118836 (83%)] Loss: 12193.600586\n",
      "    epoch          : 2137\n",
      "    loss           : 12236.619260332662\n",
      "    val_loss       : 12231.845160710549\n",
      "    val_log_likelihood: -12155.028696721205\n",
      "    val_log_marginal: -12163.166375469382\n",
      "Train Epoch: 2138 [256/118836 (0%)] Loss: 12230.714844\n",
      "Train Epoch: 2138 [33024/118836 (28%)] Loss: 12325.298828\n",
      "Train Epoch: 2138 [65792/118836 (55%)] Loss: 12260.744141\n",
      "Train Epoch: 2138 [98560/118836 (83%)] Loss: 12266.357422\n",
      "    epoch          : 2138\n",
      "    loss           : 12242.955154376034\n",
      "    val_loss       : 12244.652980748991\n",
      "    val_log_likelihood: -12152.833211363988\n",
      "    val_log_marginal: -12160.972434318313\n",
      "Train Epoch: 2139 [256/118836 (0%)] Loss: 12173.710938\n",
      "Train Epoch: 2139 [33024/118836 (28%)] Loss: 12222.526367\n",
      "Train Epoch: 2139 [65792/118836 (55%)] Loss: 12257.317383\n",
      "Train Epoch: 2139 [98560/118836 (83%)] Loss: 12406.093750\n",
      "    epoch          : 2139\n",
      "    loss           : 12240.75120547715\n",
      "    val_loss       : 12239.190001430843\n",
      "    val_log_likelihood: -12152.389843588451\n",
      "    val_log_marginal: -12160.608587614961\n",
      "Train Epoch: 2140 [256/118836 (0%)] Loss: 12189.270508\n",
      "Train Epoch: 2140 [33024/118836 (28%)] Loss: 12278.996094\n",
      "Train Epoch: 2140 [65792/118836 (55%)] Loss: 12240.228516\n",
      "Train Epoch: 2140 [98560/118836 (83%)] Loss: 12312.101562\n",
      "    epoch          : 2140\n",
      "    loss           : 12241.532627526623\n",
      "    val_loss       : 12244.962718898074\n",
      "    val_log_likelihood: -12155.883709095842\n",
      "    val_log_marginal: -12164.009159875379\n",
      "Train Epoch: 2141 [256/118836 (0%)] Loss: 12431.691406\n",
      "Train Epoch: 2141 [33024/118836 (28%)] Loss: 12173.050781\n",
      "Train Epoch: 2141 [65792/118836 (55%)] Loss: 12357.699219\n",
      "Train Epoch: 2141 [98560/118836 (83%)] Loss: 12321.020508\n",
      "    epoch          : 2141\n",
      "    loss           : 12238.719657096515\n",
      "    val_loss       : 12241.436176202735\n",
      "    val_log_likelihood: -12153.679690730976\n",
      "    val_log_marginal: -12161.776714169375\n",
      "Train Epoch: 2142 [256/118836 (0%)] Loss: 12199.506836\n",
      "Train Epoch: 2142 [33024/118836 (28%)] Loss: 12270.513672\n",
      "Train Epoch: 2142 [65792/118836 (55%)] Loss: 12117.734375\n",
      "Train Epoch: 2142 [98560/118836 (83%)] Loss: 12296.591797\n",
      "    epoch          : 2142\n",
      "    loss           : 12240.903719499587\n",
      "    val_loss       : 12242.600228815709\n",
      "    val_log_likelihood: -12154.94653930159\n",
      "    val_log_marginal: -12163.117594052203\n",
      "Train Epoch: 2143 [256/118836 (0%)] Loss: 12213.570312\n",
      "Train Epoch: 2143 [33024/118836 (28%)] Loss: 12184.177734\n",
      "Train Epoch: 2143 [65792/118836 (55%)] Loss: 12232.783203\n",
      "Train Epoch: 2143 [98560/118836 (83%)] Loss: 12280.608398\n",
      "    epoch          : 2143\n",
      "    loss           : 12237.285702123398\n",
      "    val_loss       : 12240.363844412384\n",
      "    val_log_likelihood: -12154.676874450734\n",
      "    val_log_marginal: -12162.841433238154\n",
      "Train Epoch: 2144 [256/118836 (0%)] Loss: 12250.326172\n",
      "Train Epoch: 2144 [33024/118836 (28%)] Loss: 12229.431641\n",
      "Train Epoch: 2144 [65792/118836 (55%)] Loss: 12328.954102\n",
      "Train Epoch: 2144 [98560/118836 (83%)] Loss: 12235.269531\n",
      "    epoch          : 2144\n",
      "    loss           : 12245.40467554539\n",
      "    val_loss       : 12240.229437710414\n",
      "    val_log_likelihood: -12154.387960252532\n",
      "    val_log_marginal: -12162.634434089388\n",
      "Train Epoch: 2145 [256/118836 (0%)] Loss: 12166.966797\n",
      "Train Epoch: 2145 [33024/118836 (28%)] Loss: 12309.975586\n",
      "Train Epoch: 2145 [65792/118836 (55%)] Loss: 12274.811523\n",
      "Train Epoch: 2145 [98560/118836 (83%)] Loss: 12251.501953\n",
      "    epoch          : 2145\n",
      "    loss           : 12239.275303065551\n",
      "    val_loss       : 12245.37413232758\n",
      "    val_log_likelihood: -12154.192805424162\n",
      "    val_log_marginal: -12162.52836214402\n",
      "Train Epoch: 2146 [256/118836 (0%)] Loss: 12233.931641\n",
      "Train Epoch: 2146 [33024/118836 (28%)] Loss: 12229.372070\n",
      "Train Epoch: 2146 [65792/118836 (55%)] Loss: 12302.096680\n",
      "Train Epoch: 2146 [98560/118836 (83%)] Loss: 12340.320312\n",
      "    epoch          : 2146\n",
      "    loss           : 12236.530887322942\n",
      "    val_loss       : 12240.211149659131\n",
      "    val_log_likelihood: -12153.457795214279\n",
      "    val_log_marginal: -12161.610143935599\n",
      "Train Epoch: 2147 [256/118836 (0%)] Loss: 12188.535156\n",
      "Train Epoch: 2147 [33024/118836 (28%)] Loss: 12338.188477\n",
      "Train Epoch: 2147 [65792/118836 (55%)] Loss: 12329.653320\n",
      "Train Epoch: 2147 [98560/118836 (83%)] Loss: 12268.052734\n",
      "    epoch          : 2147\n",
      "    loss           : 12240.402508368226\n",
      "    val_loss       : 12240.5797745409\n",
      "    val_log_likelihood: -12152.589297715054\n",
      "    val_log_marginal: -12160.733790853643\n",
      "Train Epoch: 2148 [256/118836 (0%)] Loss: 12174.991211\n",
      "Train Epoch: 2148 [33024/118836 (28%)] Loss: 12332.021484\n",
      "Train Epoch: 2148 [65792/118836 (55%)] Loss: 12280.157227\n",
      "Train Epoch: 2148 [98560/118836 (83%)] Loss: 12272.847656\n",
      "    epoch          : 2148\n",
      "    loss           : 12240.729624819065\n",
      "    val_loss       : 12237.26773458265\n",
      "    val_log_likelihood: -12155.45414485758\n",
      "    val_log_marginal: -12163.572102028924\n",
      "Train Epoch: 2149 [256/118836 (0%)] Loss: 12223.074219\n",
      "Train Epoch: 2149 [33024/118836 (28%)] Loss: 12201.161133\n",
      "Train Epoch: 2149 [65792/118836 (55%)] Loss: 12194.240234\n",
      "Train Epoch: 2149 [98560/118836 (83%)] Loss: 12213.733398\n",
      "    epoch          : 2149\n",
      "    loss           : 12241.8907865488\n",
      "    val_loss       : 12239.446032967819\n",
      "    val_log_likelihood: -12155.01961735551\n",
      "    val_log_marginal: -12163.137313027813\n",
      "Train Epoch: 2150 [256/118836 (0%)] Loss: 12238.494141\n",
      "Train Epoch: 2150 [33024/118836 (28%)] Loss: 12269.995117\n",
      "Train Epoch: 2150 [65792/118836 (55%)] Loss: 12163.618164\n",
      "Train Epoch: 2150 [98560/118836 (83%)] Loss: 12290.733398\n",
      "    epoch          : 2150\n",
      "    loss           : 12238.837418902502\n",
      "    val_loss       : 12243.32702238394\n",
      "    val_log_likelihood: -12155.344911212778\n",
      "    val_log_marginal: -12163.424525649281\n",
      "Train Epoch: 2151 [256/118836 (0%)] Loss: 12337.592773\n",
      "Train Epoch: 2151 [33024/118836 (28%)] Loss: 12217.233398\n",
      "Train Epoch: 2151 [65792/118836 (55%)] Loss: 12294.625977\n",
      "Train Epoch: 2151 [98560/118836 (83%)] Loss: 12320.318359\n",
      "    epoch          : 2151\n",
      "    loss           : 12243.379778936622\n",
      "    val_loss       : 12244.189408399337\n",
      "    val_log_likelihood: -12154.684022823614\n",
      "    val_log_marginal: -12163.170842175035\n",
      "Train Epoch: 2152 [256/118836 (0%)] Loss: 12287.428711\n",
      "Train Epoch: 2152 [33024/118836 (28%)] Loss: 12271.798828\n",
      "Train Epoch: 2152 [65792/118836 (55%)] Loss: 12230.855469\n",
      "Train Epoch: 2152 [98560/118836 (83%)] Loss: 12305.386719\n",
      "    epoch          : 2152\n",
      "    loss           : 12239.598424091451\n",
      "    val_loss       : 12236.416766493194\n",
      "    val_log_likelihood: -12152.86313860241\n",
      "    val_log_marginal: -12161.048944642862\n",
      "Train Epoch: 2153 [256/118836 (0%)] Loss: 12291.121094\n",
      "Train Epoch: 2153 [33024/118836 (28%)] Loss: 12187.582031\n",
      "Train Epoch: 2153 [65792/118836 (55%)] Loss: 12293.997070\n",
      "Train Epoch: 2153 [98560/118836 (83%)] Loss: 12463.579102\n",
      "    epoch          : 2153\n",
      "    loss           : 12248.482930753722\n",
      "    val_loss       : 12242.409514339197\n",
      "    val_log_likelihood: -12155.83046858845\n",
      "    val_log_marginal: -12164.333358645175\n",
      "Train Epoch: 2154 [256/118836 (0%)] Loss: 12271.363281\n",
      "Train Epoch: 2154 [33024/118836 (28%)] Loss: 12279.644531\n",
      "Train Epoch: 2154 [65792/118836 (55%)] Loss: 12350.253906\n",
      "Train Epoch: 2154 [98560/118836 (83%)] Loss: 12258.574219\n",
      "    epoch          : 2154\n",
      "    loss           : 12245.378814005635\n",
      "    val_loss       : 12251.858650537464\n",
      "    val_log_likelihood: -12157.873319892473\n",
      "    val_log_marginal: -12166.277823319\n",
      "Train Epoch: 2155 [256/118836 (0%)] Loss: 12315.740234\n",
      "Train Epoch: 2155 [33024/118836 (28%)] Loss: 12302.830078\n",
      "Train Epoch: 2155 [65792/118836 (55%)] Loss: 12199.592773\n",
      "Train Epoch: 2155 [98560/118836 (83%)] Loss: 12212.650391\n",
      "    epoch          : 2155\n",
      "    loss           : 12241.135874819065\n",
      "    val_loss       : 12246.096812436317\n",
      "    val_log_likelihood: -12152.653749386114\n",
      "    val_log_marginal: -12160.909508622923\n",
      "Train Epoch: 2156 [256/118836 (0%)] Loss: 12322.285156\n",
      "Train Epoch: 2156 [33024/118836 (28%)] Loss: 12395.942383\n",
      "Train Epoch: 2156 [65792/118836 (55%)] Loss: 12266.415039\n",
      "Train Epoch: 2156 [98560/118836 (83%)] Loss: 12252.091797\n",
      "    epoch          : 2156\n",
      "    loss           : 12240.879046151262\n",
      "    val_loss       : 12240.74573883966\n",
      "    val_log_likelihood: -12154.382233024453\n",
      "    val_log_marginal: -12162.486199942019\n",
      "Train Epoch: 2157 [256/118836 (0%)] Loss: 12316.113281\n",
      "Train Epoch: 2157 [33024/118836 (28%)] Loss: 12282.020508\n",
      "Train Epoch: 2157 [65792/118836 (55%)] Loss: 12281.031250\n",
      "Train Epoch: 2157 [98560/118836 (83%)] Loss: 12321.868164\n",
      "    epoch          : 2157\n",
      "    loss           : 12246.71540836306\n",
      "    val_loss       : 12242.34140933319\n",
      "    val_log_likelihood: -12154.54518309941\n",
      "    val_log_marginal: -12162.787832412243\n",
      "Train Epoch: 2158 [256/118836 (0%)] Loss: 12309.382812\n",
      "Train Epoch: 2158 [33024/118836 (28%)] Loss: 12328.014648\n",
      "Train Epoch: 2158 [65792/118836 (55%)] Loss: 12312.426758\n",
      "Train Epoch: 2158 [98560/118836 (83%)] Loss: 12269.788086\n",
      "    epoch          : 2158\n",
      "    loss           : 12241.783429131772\n",
      "    val_loss       : 12240.866010342776\n",
      "    val_log_likelihood: -12155.41745147074\n",
      "    val_log_marginal: -12163.807517736584\n",
      "Train Epoch: 2159 [256/118836 (0%)] Loss: 12340.474609\n",
      "Train Epoch: 2159 [33024/118836 (28%)] Loss: 12407.479492\n",
      "Train Epoch: 2159 [65792/118836 (55%)] Loss: 12245.634766\n",
      "Train Epoch: 2159 [98560/118836 (83%)] Loss: 12368.050781\n",
      "    epoch          : 2159\n",
      "    loss           : 12250.747580160514\n",
      "    val_loss       : 12237.788470517356\n",
      "    val_log_likelihood: -12156.145982281327\n",
      "    val_log_marginal: -12164.250025821884\n",
      "Train Epoch: 2160 [256/118836 (0%)] Loss: 12212.773438\n",
      "Train Epoch: 2160 [33024/118836 (28%)] Loss: 12326.167969\n",
      "Train Epoch: 2160 [65792/118836 (55%)] Loss: 12355.800781\n",
      "Train Epoch: 2160 [98560/118836 (83%)] Loss: 12224.603516\n",
      "    epoch          : 2160\n",
      "    loss           : 12243.274851213553\n",
      "    val_loss       : 12242.986318384135\n",
      "    val_log_likelihood: -12153.5886920686\n",
      "    val_log_marginal: -12161.75179448326\n",
      "Train Epoch: 2161 [256/118836 (0%)] Loss: 12200.676758\n",
      "Train Epoch: 2161 [33024/118836 (28%)] Loss: 12274.600586\n",
      "Train Epoch: 2161 [65792/118836 (55%)] Loss: 12229.869141\n",
      "Train Epoch: 2161 [98560/118836 (83%)] Loss: 12263.560547\n",
      "    epoch          : 2161\n",
      "    loss           : 12248.78097100522\n",
      "    val_loss       : 12243.275148935678\n",
      "    val_log_likelihood: -12159.015399154776\n",
      "    val_log_marginal: -12167.504718957283\n",
      "Train Epoch: 2162 [256/118836 (0%)] Loss: 12239.562500\n",
      "Train Epoch: 2162 [33024/118836 (28%)] Loss: 12264.526367\n",
      "Train Epoch: 2162 [65792/118836 (55%)] Loss: 12177.750977\n",
      "Train Epoch: 2162 [98560/118836 (83%)] Loss: 12217.661133\n",
      "    epoch          : 2162\n",
      "    loss           : 12240.073445415892\n",
      "    val_loss       : 12243.38936936467\n",
      "    val_log_likelihood: -12155.012547818445\n",
      "    val_log_marginal: -12163.315250948624\n",
      "Train Epoch: 2163 [256/118836 (0%)] Loss: 12234.654297\n",
      "Train Epoch: 2163 [33024/118836 (28%)] Loss: 12328.930664\n",
      "Train Epoch: 2163 [65792/118836 (55%)] Loss: 12285.255859\n",
      "Train Epoch: 2163 [98560/118836 (83%)] Loss: 12200.044922\n",
      "    epoch          : 2163\n",
      "    loss           : 12242.444742555832\n",
      "    val_loss       : 12239.40940862099\n",
      "    val_log_likelihood: -12154.158992290892\n",
      "    val_log_marginal: -12162.423078437618\n",
      "Train Epoch: 2164 [256/118836 (0%)] Loss: 12387.164062\n",
      "Train Epoch: 2164 [33024/118836 (28%)] Loss: 12339.119141\n",
      "Train Epoch: 2164 [65792/118836 (55%)] Loss: 12180.388672\n",
      "Train Epoch: 2164 [98560/118836 (83%)] Loss: 12343.002930\n",
      "    epoch          : 2164\n",
      "    loss           : 12243.088500148624\n",
      "    val_loss       : 12237.687186555713\n",
      "    val_log_likelihood: -12153.743720921215\n",
      "    val_log_marginal: -12161.829891688918\n",
      "Train Epoch: 2165 [256/118836 (0%)] Loss: 12211.132812\n",
      "Train Epoch: 2165 [33024/118836 (28%)] Loss: 12228.629883\n",
      "Train Epoch: 2165 [65792/118836 (55%)] Loss: 12324.082031\n",
      "Train Epoch: 2165 [98560/118836 (83%)] Loss: 12253.698242\n",
      "    epoch          : 2165\n",
      "    loss           : 12240.989321624274\n",
      "    val_loss       : 12244.695335886467\n",
      "    val_log_likelihood: -12153.344517033705\n",
      "    val_log_marginal: -12161.564259694514\n",
      "Train Epoch: 2166 [256/118836 (0%)] Loss: 12308.470703\n",
      "Train Epoch: 2166 [33024/118836 (28%)] Loss: 12183.409180\n",
      "Train Epoch: 2166 [65792/118836 (55%)] Loss: 12262.970703\n",
      "Train Epoch: 2166 [98560/118836 (83%)] Loss: 12175.601562\n",
      "    epoch          : 2166\n",
      "    loss           : 12244.868480375053\n",
      "    val_loss       : 12240.0423104875\n",
      "    val_log_likelihood: -12153.860829262303\n",
      "    val_log_marginal: -12162.306545151992\n",
      "Train Epoch: 2167 [256/118836 (0%)] Loss: 12223.411133\n",
      "Train Epoch: 2167 [33024/118836 (28%)] Loss: 12230.940430\n",
      "Train Epoch: 2167 [65792/118836 (55%)] Loss: 12302.830078\n",
      "Train Epoch: 2167 [98560/118836 (83%)] Loss: 12295.311523\n",
      "    epoch          : 2167\n",
      "    loss           : 12242.305064877997\n",
      "    val_loss       : 12239.653586639331\n",
      "    val_log_likelihood: -12154.237532471308\n",
      "    val_log_marginal: -12162.355608792428\n",
      "Train Epoch: 2168 [256/118836 (0%)] Loss: 12323.927734\n",
      "Train Epoch: 2168 [33024/118836 (28%)] Loss: 12292.838867\n",
      "Train Epoch: 2168 [65792/118836 (55%)] Loss: 12305.561523\n",
      "Train Epoch: 2168 [98560/118836 (83%)] Loss: 12403.955078\n",
      "    epoch          : 2168\n",
      "    loss           : 12239.90685500026\n",
      "    val_loss       : 12243.153243494866\n",
      "    val_log_likelihood: -12155.72552939542\n",
      "    val_log_marginal: -12164.080080672971\n",
      "Train Epoch: 2169 [256/118836 (0%)] Loss: 12347.570312\n",
      "Train Epoch: 2169 [33024/118836 (28%)] Loss: 12245.806641\n",
      "Train Epoch: 2169 [65792/118836 (55%)] Loss: 12359.635742\n",
      "Train Epoch: 2169 [98560/118836 (83%)] Loss: 12218.104492\n",
      "    epoch          : 2169\n",
      "    loss           : 12242.87161086771\n",
      "    val_loss       : 12243.15041808154\n",
      "    val_log_likelihood: -12153.393684895833\n",
      "    val_log_marginal: -12161.511028993136\n",
      "Train Epoch: 2170 [256/118836 (0%)] Loss: 12278.459961\n",
      "Train Epoch: 2170 [33024/118836 (28%)] Loss: 12289.228516\n",
      "Train Epoch: 2170 [65792/118836 (55%)] Loss: 12237.803711\n",
      "Train Epoch: 2170 [98560/118836 (83%)] Loss: 12189.530273\n",
      "    epoch          : 2170\n",
      "    loss           : 12243.47042429177\n",
      "    val_loss       : 12240.348845347798\n",
      "    val_log_likelihood: -12155.160972556088\n",
      "    val_log_marginal: -12163.334265981808\n",
      "Train Epoch: 2171 [256/118836 (0%)] Loss: 12218.882812\n",
      "Train Epoch: 2171 [33024/118836 (28%)] Loss: 12275.939453\n",
      "Train Epoch: 2171 [65792/118836 (55%)] Loss: 12237.034180\n",
      "Train Epoch: 2171 [98560/118836 (83%)] Loss: 12185.386719\n",
      "    epoch          : 2171\n",
      "    loss           : 12240.779585562706\n",
      "    val_loss       : 12244.626974843424\n",
      "    val_log_likelihood: -12154.36646828474\n",
      "    val_log_marginal: -12162.63474303186\n",
      "Train Epoch: 2172 [256/118836 (0%)] Loss: 12246.018555\n",
      "Train Epoch: 2172 [33024/118836 (28%)] Loss: 12214.732422\n",
      "Train Epoch: 2172 [65792/118836 (55%)] Loss: 12330.165039\n",
      "Train Epoch: 2172 [98560/118836 (83%)] Loss: 12373.730469\n",
      "    epoch          : 2172\n",
      "    loss           : 12242.479203499794\n",
      "    val_loss       : 12241.501332617905\n",
      "    val_log_likelihood: -12155.68148149943\n",
      "    val_log_marginal: -12163.805369727843\n",
      "Train Epoch: 2173 [256/118836 (0%)] Loss: 12260.146484\n",
      "Train Epoch: 2173 [33024/118836 (28%)] Loss: 12253.669922\n",
      "Train Epoch: 2173 [65792/118836 (55%)] Loss: 12269.099609\n",
      "Train Epoch: 2173 [98560/118836 (83%)] Loss: 12243.055664\n",
      "    epoch          : 2173\n",
      "    loss           : 12236.627895116057\n",
      "    val_loss       : 12241.104536490906\n",
      "    val_log_likelihood: -12155.003816267317\n",
      "    val_log_marginal: -12163.278793955511\n",
      "Train Epoch: 2174 [256/118836 (0%)] Loss: 12343.503906\n",
      "Train Epoch: 2174 [33024/118836 (28%)] Loss: 12269.126953\n",
      "Train Epoch: 2174 [65792/118836 (55%)] Loss: 12238.912109\n",
      "Train Epoch: 2174 [98560/118836 (83%)] Loss: 12278.021484\n",
      "    epoch          : 2174\n",
      "    loss           : 12243.738597239453\n",
      "    val_loss       : 12244.584650895713\n",
      "    val_log_likelihood: -12153.663538758788\n",
      "    val_log_marginal: -12161.72573544152\n",
      "Train Epoch: 2175 [256/118836 (0%)] Loss: 12243.517578\n",
      "Train Epoch: 2175 [33024/118836 (28%)] Loss: 12386.324219\n",
      "Train Epoch: 2175 [65792/118836 (55%)] Loss: 12243.254883\n",
      "Train Epoch: 2175 [98560/118836 (83%)] Loss: 12223.548828\n",
      "    epoch          : 2175\n",
      "    loss           : 12238.137801450062\n",
      "    val_loss       : 12240.246607265575\n",
      "    val_log_likelihood: -12153.408005227719\n",
      "    val_log_marginal: -12161.502659361508\n",
      "Train Epoch: 2176 [256/118836 (0%)] Loss: 12285.576172\n",
      "Train Epoch: 2176 [33024/118836 (28%)] Loss: 12317.507812\n",
      "Train Epoch: 2176 [65792/118836 (55%)] Loss: 12173.859375\n",
      "Train Epoch: 2176 [98560/118836 (83%)] Loss: 12286.833984\n",
      "    epoch          : 2176\n",
      "    loss           : 12245.392492665685\n",
      "    val_loss       : 12238.51682903913\n",
      "    val_log_likelihood: -12156.849468019798\n",
      "    val_log_marginal: -12164.905215901954\n",
      "Train Epoch: 2177 [256/118836 (0%)] Loss: 12255.286133\n",
      "Train Epoch: 2177 [33024/118836 (28%)] Loss: 12222.214844\n",
      "Train Epoch: 2177 [65792/118836 (55%)] Loss: 12169.962891\n",
      "Train Epoch: 2177 [98560/118836 (83%)] Loss: 12197.478516\n",
      "    epoch          : 2177\n",
      "    loss           : 12244.841780073924\n",
      "    val_loss       : 12245.323740331023\n",
      "    val_log_likelihood: -12153.599407762096\n",
      "    val_log_marginal: -12161.583245035505\n",
      "Train Epoch: 2178 [256/118836 (0%)] Loss: 12359.392578\n",
      "Train Epoch: 2178 [33024/118836 (28%)] Loss: 12261.685547\n",
      "Train Epoch: 2178 [65792/118836 (55%)] Loss: 12380.615234\n",
      "Train Epoch: 2178 [98560/118836 (83%)] Loss: 12260.754883\n",
      "    epoch          : 2178\n",
      "    loss           : 12239.386861074494\n",
      "    val_loss       : 12243.350005715489\n",
      "    val_log_likelihood: -12157.706595714435\n",
      "    val_log_marginal: -12166.016318490703\n",
      "Train Epoch: 2179 [256/118836 (0%)] Loss: 12277.918945\n",
      "Train Epoch: 2179 [33024/118836 (28%)] Loss: 12383.179688\n",
      "Train Epoch: 2179 [65792/118836 (55%)] Loss: 12280.312500\n",
      "Train Epoch: 2179 [98560/118836 (83%)] Loss: 12237.759766\n",
      "    epoch          : 2179\n",
      "    loss           : 12240.80715790426\n",
      "    val_loss       : 12239.797562550857\n",
      "    val_log_likelihood: -12153.622114092226\n",
      "    val_log_marginal: -12161.795279623177\n",
      "Train Epoch: 2180 [256/118836 (0%)] Loss: 12242.318359\n",
      "Train Epoch: 2180 [33024/118836 (28%)] Loss: 12249.624023\n",
      "Train Epoch: 2180 [65792/118836 (55%)] Loss: 12265.925781\n",
      "Train Epoch: 2180 [98560/118836 (83%)] Loss: 12248.182617\n",
      "    epoch          : 2180\n",
      "    loss           : 12240.597735408912\n",
      "    val_loss       : 12246.036878472514\n",
      "    val_log_likelihood: -12151.999199687241\n",
      "    val_log_marginal: -12160.06280398494\n",
      "Train Epoch: 2181 [256/118836 (0%)] Loss: 12335.607422\n",
      "Train Epoch: 2181 [33024/118836 (28%)] Loss: 12199.609375\n",
      "Train Epoch: 2181 [65792/118836 (55%)] Loss: 12155.946289\n",
      "Train Epoch: 2181 [98560/118836 (83%)] Loss: 12160.324219\n",
      "    epoch          : 2181\n",
      "    loss           : 12245.954430637406\n",
      "    val_loss       : 12239.32817594659\n",
      "    val_log_likelihood: -12152.522531695875\n",
      "    val_log_marginal: -12160.511548973269\n",
      "Train Epoch: 2182 [256/118836 (0%)] Loss: 12232.233398\n",
      "Train Epoch: 2182 [33024/118836 (28%)] Loss: 12157.577148\n",
      "Train Epoch: 2182 [65792/118836 (55%)] Loss: 12268.760742\n",
      "Train Epoch: 2182 [98560/118836 (83%)] Loss: 12225.193359\n",
      "    epoch          : 2182\n",
      "    loss           : 12235.985602609337\n",
      "    val_loss       : 12244.316983959214\n",
      "    val_log_likelihood: -12153.055341772642\n",
      "    val_log_marginal: -12161.195278415893\n",
      "Train Epoch: 2183 [256/118836 (0%)] Loss: 12318.494141\n",
      "Train Epoch: 2183 [33024/118836 (28%)] Loss: 12310.169922\n",
      "Train Epoch: 2183 [65792/118836 (55%)] Loss: 12288.434570\n",
      "Train Epoch: 2183 [98560/118836 (83%)] Loss: 12192.743164\n",
      "    epoch          : 2183\n",
      "    loss           : 12239.899223273365\n",
      "    val_loss       : 12240.49642729429\n",
      "    val_log_likelihood: -12155.456349352513\n",
      "    val_log_marginal: -12163.687723414496\n",
      "Train Epoch: 2184 [256/118836 (0%)] Loss: 12214.791992\n",
      "Train Epoch: 2184 [33024/118836 (28%)] Loss: 12250.750977\n",
      "Train Epoch: 2184 [65792/118836 (55%)] Loss: 12236.968750\n",
      "Train Epoch: 2184 [98560/118836 (83%)] Loss: 12252.251953\n",
      "    epoch          : 2184\n",
      "    loss           : 12246.174134259978\n",
      "    val_loss       : 12241.61723908319\n",
      "    val_log_likelihood: -12155.079949855251\n",
      "    val_log_marginal: -12163.147577764854\n",
      "Train Epoch: 2185 [256/118836 (0%)] Loss: 12207.678711\n",
      "Train Epoch: 2185 [33024/118836 (28%)] Loss: 12322.001953\n",
      "Train Epoch: 2185 [65792/118836 (55%)] Loss: 12275.919922\n",
      "Train Epoch: 2185 [98560/118836 (83%)] Loss: 12200.706055\n",
      "    epoch          : 2185\n",
      "    loss           : 12237.737157839641\n",
      "    val_loss       : 12240.090487799947\n",
      "    val_log_likelihood: -12153.237467851788\n",
      "    val_log_marginal: -12161.49331222922\n",
      "Train Epoch: 2186 [256/118836 (0%)] Loss: 12186.464844\n",
      "Train Epoch: 2186 [33024/118836 (28%)] Loss: 12262.090820\n",
      "Train Epoch: 2186 [65792/118836 (55%)] Loss: 12210.864258\n",
      "Train Epoch: 2186 [98560/118836 (83%)] Loss: 12364.193359\n",
      "    epoch          : 2186\n",
      "    loss           : 12241.426126479786\n",
      "    val_loss       : 12236.736395691145\n",
      "    val_log_likelihood: -12152.401892705748\n",
      "    val_log_marginal: -12160.699956107645\n",
      "Train Epoch: 2187 [256/118836 (0%)] Loss: 12379.679688\n",
      "Train Epoch: 2187 [33024/118836 (28%)] Loss: 12182.668945\n",
      "Train Epoch: 2187 [65792/118836 (55%)] Loss: 12257.892578\n",
      "Train Epoch: 2187 [98560/118836 (83%)] Loss: 12170.794922\n",
      "    epoch          : 2187\n",
      "    loss           : 12243.3491717393\n",
      "    val_loss       : 12237.847778372492\n",
      "    val_log_likelihood: -12153.911668508325\n",
      "    val_log_marginal: -12162.168985385937\n",
      "Train Epoch: 2188 [256/118836 (0%)] Loss: 12378.205078\n",
      "Train Epoch: 2188 [33024/118836 (28%)] Loss: 12237.399414\n",
      "Train Epoch: 2188 [65792/118836 (55%)] Loss: 12189.232422\n",
      "Train Epoch: 2188 [98560/118836 (83%)] Loss: 12223.503906\n",
      "    epoch          : 2188\n",
      "    loss           : 12241.163971871123\n",
      "    val_loss       : 12242.034231959271\n",
      "    val_log_likelihood: -12151.014683816687\n",
      "    val_log_marginal: -12159.145108942492\n",
      "Train Epoch: 2189 [256/118836 (0%)] Loss: 12285.958008\n",
      "Train Epoch: 2189 [33024/118836 (28%)] Loss: 12392.612305\n",
      "Train Epoch: 2189 [65792/118836 (55%)] Loss: 12242.854492\n",
      "Train Epoch: 2189 [98560/118836 (83%)] Loss: 12209.124023\n",
      "    epoch          : 2189\n",
      "    loss           : 12241.012771725082\n",
      "    val_loss       : 12242.515041860523\n",
      "    val_log_likelihood: -12151.120342709624\n",
      "    val_log_marginal: -12159.397143944294\n",
      "Train Epoch: 2190 [256/118836 (0%)] Loss: 12293.189453\n",
      "Train Epoch: 2190 [33024/118836 (28%)] Loss: 12317.085938\n",
      "Train Epoch: 2190 [65792/118836 (55%)] Loss: 12252.525391\n",
      "Train Epoch: 2190 [98560/118836 (83%)] Loss: 12178.421875\n",
      "    epoch          : 2190\n",
      "    loss           : 12240.372695991004\n",
      "    val_loss       : 12239.373197650571\n",
      "    val_log_likelihood: -12152.0778151494\n",
      "    val_log_marginal: -12160.241284068423\n",
      "Train Epoch: 2191 [256/118836 (0%)] Loss: 12326.274414\n",
      "Train Epoch: 2191 [33024/118836 (28%)] Loss: 12264.720703\n",
      "Train Epoch: 2191 [65792/118836 (55%)] Loss: 12350.604492\n",
      "Train Epoch: 2191 [98560/118836 (83%)] Loss: 12183.521484\n",
      "    epoch          : 2191\n",
      "    loss           : 12242.452442294767\n",
      "    val_loss       : 12236.753021126651\n",
      "    val_log_likelihood: -12151.278247777089\n",
      "    val_log_marginal: -12159.62973592306\n",
      "Train Epoch: 2192 [256/118836 (0%)] Loss: 12191.013672\n",
      "Train Epoch: 2192 [33024/118836 (28%)] Loss: 12203.882812\n",
      "Train Epoch: 2192 [65792/118836 (55%)] Loss: 12238.442383\n",
      "Train Epoch: 2192 [98560/118836 (83%)] Loss: 12284.062500\n",
      "    epoch          : 2192\n",
      "    loss           : 12241.971400531173\n",
      "    val_loss       : 12240.772834490728\n",
      "    val_log_likelihood: -12149.532082137872\n",
      "    val_log_marginal: -12157.636152250432\n",
      "Train Epoch: 2193 [256/118836 (0%)] Loss: 12293.658203\n",
      "Train Epoch: 2193 [33024/118836 (28%)] Loss: 12255.587891\n",
      "Train Epoch: 2193 [65792/118836 (55%)] Loss: 12262.409180\n",
      "Train Epoch: 2193 [98560/118836 (83%)] Loss: 12197.615234\n",
      "    epoch          : 2193\n",
      "    loss           : 12238.87420647229\n",
      "    val_loss       : 12240.306443512087\n",
      "    val_log_likelihood: -12151.344018655655\n",
      "    val_log_marginal: -12159.410505795593\n",
      "Train Epoch: 2194 [256/118836 (0%)] Loss: 12355.503906\n",
      "Train Epoch: 2194 [33024/118836 (28%)] Loss: 12251.906250\n",
      "Train Epoch: 2194 [65792/118836 (55%)] Loss: 12283.737305\n",
      "Train Epoch: 2194 [98560/118836 (83%)] Loss: 12312.148438\n",
      "    epoch          : 2194\n",
      "    loss           : 12240.85396424602\n",
      "    val_loss       : 12244.468772510128\n",
      "    val_log_likelihood: -12151.412403232267\n",
      "    val_log_marginal: -12159.533105975928\n",
      "Train Epoch: 2195 [256/118836 (0%)] Loss: 12303.888672\n",
      "Train Epoch: 2195 [33024/118836 (28%)] Loss: 12347.101562\n",
      "Train Epoch: 2195 [65792/118836 (55%)] Loss: 12304.313477\n",
      "Train Epoch: 2195 [98560/118836 (83%)] Loss: 12286.010742\n",
      "    epoch          : 2195\n",
      "    loss           : 12236.77053882987\n",
      "    val_loss       : 12241.90834404454\n",
      "    val_log_likelihood: -12152.887742969397\n",
      "    val_log_marginal: -12161.000503970332\n",
      "Train Epoch: 2196 [256/118836 (0%)] Loss: 12278.353516\n",
      "Train Epoch: 2196 [33024/118836 (28%)] Loss: 12359.055664\n",
      "Train Epoch: 2196 [65792/118836 (55%)] Loss: 12188.564453\n",
      "Train Epoch: 2196 [98560/118836 (83%)] Loss: 12324.566406\n",
      "    epoch          : 2196\n",
      "    loss           : 12242.644647726685\n",
      "    val_loss       : 12240.190456261384\n",
      "    val_log_likelihood: -12151.36390078965\n",
      "    val_log_marginal: -12159.610769071254\n",
      "Train Epoch: 2197 [256/118836 (0%)] Loss: 12428.030273\n",
      "Train Epoch: 2197 [33024/118836 (28%)] Loss: 12172.989258\n",
      "Train Epoch: 2197 [65792/118836 (55%)] Loss: 12273.968750\n",
      "Train Epoch: 2197 [98560/118836 (83%)] Loss: 12257.435547\n",
      "    epoch          : 2197\n",
      "    loss           : 12237.61877568626\n",
      "    val_loss       : 12242.473608631723\n",
      "    val_log_likelihood: -12150.968741437913\n",
      "    val_log_marginal: -12159.186287205139\n",
      "Train Epoch: 2198 [256/118836 (0%)] Loss: 12223.320312\n",
      "Train Epoch: 2198 [33024/118836 (28%)] Loss: 12273.551758\n",
      "Train Epoch: 2198 [65792/118836 (55%)] Loss: 12191.484375\n",
      "Train Epoch: 2198 [98560/118836 (83%)] Loss: 12246.530273\n",
      "    epoch          : 2198\n",
      "    loss           : 12242.784894540942\n",
      "    val_loss       : 12238.817537152143\n",
      "    val_log_likelihood: -12151.720844157102\n",
      "    val_log_marginal: -12159.785860076789\n",
      "Train Epoch: 2199 [256/118836 (0%)] Loss: 12125.242188\n",
      "Train Epoch: 2199 [33024/118836 (28%)] Loss: 12171.761719\n",
      "Train Epoch: 2199 [65792/118836 (55%)] Loss: 12213.262695\n",
      "Train Epoch: 2199 [98560/118836 (83%)] Loss: 12180.816406\n",
      "    epoch          : 2199\n",
      "    loss           : 12241.310753819014\n",
      "    val_loss       : 12241.35009815628\n",
      "    val_log_likelihood: -12154.19553204482\n",
      "    val_log_marginal: -12162.239395964518\n",
      "Train Epoch: 2200 [256/118836 (0%)] Loss: 12203.241211\n",
      "Train Epoch: 2200 [33024/118836 (28%)] Loss: 12397.950195\n",
      "Train Epoch: 2200 [65792/118836 (55%)] Loss: 12239.958984\n",
      "Train Epoch: 2200 [98560/118836 (83%)] Loss: 12341.046875\n",
      "    epoch          : 2200\n",
      "    loss           : 12241.401145219448\n",
      "    val_loss       : 12240.442079319011\n",
      "    val_log_likelihood: -12150.011404537581\n",
      "    val_log_marginal: -12158.209600982018\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch2200.pth ...\n",
      "Train Epoch: 2201 [256/118836 (0%)] Loss: 12186.826172\n",
      "Train Epoch: 2201 [33024/118836 (28%)] Loss: 12266.253906\n",
      "Train Epoch: 2201 [65792/118836 (55%)] Loss: 12300.410156\n",
      "Train Epoch: 2201 [98560/118836 (83%)] Loss: 12288.810547\n",
      "    epoch          : 2201\n",
      "    loss           : 12241.569455483612\n",
      "    val_loss       : 12243.324176553486\n",
      "    val_log_likelihood: -12152.714079139525\n",
      "    val_log_marginal: -12160.834827839066\n",
      "Train Epoch: 2202 [256/118836 (0%)] Loss: 12218.466797\n",
      "Train Epoch: 2202 [33024/118836 (28%)] Loss: 12201.156250\n",
      "Train Epoch: 2202 [65792/118836 (55%)] Loss: 12138.127930\n",
      "Train Epoch: 2202 [98560/118836 (83%)] Loss: 12263.216797\n",
      "    epoch          : 2202\n",
      "    loss           : 12241.792367465363\n",
      "    val_loss       : 12240.087276245553\n",
      "    val_log_likelihood: -12152.929570700217\n",
      "    val_log_marginal: -12161.045482095386\n",
      "Train Epoch: 2203 [256/118836 (0%)] Loss: 12267.421875\n",
      "Train Epoch: 2203 [33024/118836 (28%)] Loss: 12214.602539\n",
      "Train Epoch: 2203 [65792/118836 (55%)] Loss: 12138.735352\n",
      "Train Epoch: 2203 [98560/118836 (83%)] Loss: 12346.529297\n",
      "    epoch          : 2203\n",
      "    loss           : 12240.445358702957\n",
      "    val_loss       : 12237.641938107283\n",
      "    val_log_likelihood: -12152.037419548697\n",
      "    val_log_marginal: -12160.172043846746\n",
      "Train Epoch: 2204 [256/118836 (0%)] Loss: 12244.563477\n",
      "Train Epoch: 2204 [33024/118836 (28%)] Loss: 12269.395508\n",
      "Train Epoch: 2204 [65792/118836 (55%)] Loss: 12257.833008\n",
      "Train Epoch: 2204 [98560/118836 (83%)] Loss: 12245.077148\n",
      "    epoch          : 2204\n",
      "    loss           : 12237.95965512562\n",
      "    val_loss       : 12237.178283457264\n",
      "    val_log_likelihood: -12152.301948278535\n",
      "    val_log_marginal: -12160.442550315654\n",
      "Train Epoch: 2205 [256/118836 (0%)] Loss: 12240.529297\n",
      "Train Epoch: 2205 [33024/118836 (28%)] Loss: 12192.426758\n",
      "Train Epoch: 2205 [65792/118836 (55%)] Loss: 12188.048828\n",
      "Train Epoch: 2205 [98560/118836 (83%)] Loss: 12189.939453\n",
      "    epoch          : 2205\n",
      "    loss           : 12245.789445855304\n",
      "    val_loss       : 12238.47980093764\n",
      "    val_log_likelihood: -12152.555128689775\n",
      "    val_log_marginal: -12160.797549699793\n",
      "Train Epoch: 2206 [256/118836 (0%)] Loss: 12251.186523\n",
      "Train Epoch: 2206 [33024/118836 (28%)] Loss: 12263.100586\n",
      "Train Epoch: 2206 [65792/118836 (55%)] Loss: 12251.959961\n",
      "Train Epoch: 2206 [98560/118836 (83%)] Loss: 12174.531250\n",
      "    epoch          : 2206\n",
      "    loss           : 12237.58370150305\n",
      "    val_loss       : 12239.844290816127\n",
      "    val_log_likelihood: -12152.523004549215\n",
      "    val_log_marginal: -12160.681548920184\n",
      "Train Epoch: 2207 [256/118836 (0%)] Loss: 12303.429688\n",
      "Train Epoch: 2207 [33024/118836 (28%)] Loss: 12230.847656\n",
      "Train Epoch: 2207 [65792/118836 (55%)] Loss: 12345.773438\n",
      "Train Epoch: 2207 [98560/118836 (83%)] Loss: 12225.472656\n",
      "    epoch          : 2207\n",
      "    loss           : 12241.554413674783\n",
      "    val_loss       : 12240.941174663165\n",
      "    val_log_likelihood: -12149.4938645381\n",
      "    val_log_marginal: -12157.873737609001\n",
      "Train Epoch: 2208 [256/118836 (0%)] Loss: 12220.346680\n",
      "Train Epoch: 2208 [33024/118836 (28%)] Loss: 12163.361328\n",
      "Train Epoch: 2208 [65792/118836 (55%)] Loss: 12260.168945\n",
      "Train Epoch: 2208 [98560/118836 (83%)] Loss: 12289.945312\n",
      "    epoch          : 2208\n",
      "    loss           : 12241.456381500724\n",
      "    val_loss       : 12241.216345350771\n",
      "    val_log_likelihood: -12150.082391180727\n",
      "    val_log_marginal: -12158.353993455576\n",
      "Train Epoch: 2209 [256/118836 (0%)] Loss: 12300.462891\n",
      "Train Epoch: 2209 [33024/118836 (28%)] Loss: 12222.658203\n",
      "Train Epoch: 2209 [65792/118836 (55%)] Loss: 12201.208008\n",
      "Train Epoch: 2209 [98560/118836 (83%)] Loss: 12337.293945\n",
      "    epoch          : 2209\n",
      "    loss           : 12240.146475166719\n",
      "    val_loss       : 12240.547205834617\n",
      "    val_log_likelihood: -12152.16729250672\n",
      "    val_log_marginal: -12160.3395977783\n",
      "Train Epoch: 2210 [256/118836 (0%)] Loss: 12199.561523\n",
      "Train Epoch: 2210 [33024/118836 (28%)] Loss: 12252.808594\n",
      "Train Epoch: 2210 [65792/118836 (55%)] Loss: 12189.397461\n",
      "Train Epoch: 2210 [98560/118836 (83%)] Loss: 12263.948242\n",
      "    epoch          : 2210\n",
      "    loss           : 12242.073806962106\n",
      "    val_loss       : 12238.6836939364\n",
      "    val_log_likelihood: -12153.438502410309\n",
      "    val_log_marginal: -12161.561316785372\n",
      "Train Epoch: 2211 [256/118836 (0%)] Loss: 12164.703125\n",
      "Train Epoch: 2211 [33024/118836 (28%)] Loss: 12303.460938\n",
      "Train Epoch: 2211 [65792/118836 (55%)] Loss: 12207.053711\n",
      "Train Epoch: 2211 [98560/118836 (83%)] Loss: 12256.289062\n",
      "    epoch          : 2211\n",
      "    loss           : 12237.748717787168\n",
      "    val_loss       : 12237.348477005797\n",
      "    val_log_likelihood: -12151.319747240746\n",
      "    val_log_marginal: -12159.562749809433\n",
      "Train Epoch: 2212 [256/118836 (0%)] Loss: 12305.319336\n",
      "Train Epoch: 2212 [33024/118836 (28%)] Loss: 12332.824219\n",
      "Train Epoch: 2212 [65792/118836 (55%)] Loss: 12306.648438\n",
      "Train Epoch: 2212 [98560/118836 (83%)] Loss: 12144.368164\n",
      "    epoch          : 2212\n",
      "    loss           : 12245.057533343674\n",
      "    val_loss       : 12236.437550321923\n",
      "    val_log_likelihood: -12152.724288215984\n",
      "    val_log_marginal: -12161.09505621262\n",
      "Train Epoch: 2213 [256/118836 (0%)] Loss: 12242.242188\n",
      "Train Epoch: 2213 [33024/118836 (28%)] Loss: 12355.162109\n",
      "Train Epoch: 2213 [65792/118836 (55%)] Loss: 12272.450195\n",
      "Train Epoch: 2213 [98560/118836 (83%)] Loss: 12222.029297\n",
      "    epoch          : 2213\n",
      "    loss           : 12241.384775479477\n",
      "    val_loss       : 12243.334815749593\n",
      "    val_log_likelihood: -12152.470863381412\n",
      "    val_log_marginal: -12160.584370859346\n",
      "Train Epoch: 2214 [256/118836 (0%)] Loss: 12307.770508\n",
      "Train Epoch: 2214 [33024/118836 (28%)] Loss: 12361.595703\n",
      "Train Epoch: 2214 [65792/118836 (55%)] Loss: 12256.551758\n",
      "Train Epoch: 2214 [98560/118836 (83%)] Loss: 12312.874023\n",
      "    epoch          : 2214\n",
      "    loss           : 12235.916586376912\n",
      "    val_loss       : 12241.478975489976\n",
      "    val_log_likelihood: -12151.555619959678\n",
      "    val_log_marginal: -12159.491218802446\n",
      "Train Epoch: 2215 [256/118836 (0%)] Loss: 12305.198242\n",
      "Train Epoch: 2215 [33024/118836 (28%)] Loss: 12261.287109\n",
      "Train Epoch: 2215 [65792/118836 (55%)] Loss: 12173.884766\n",
      "Train Epoch: 2215 [98560/118836 (83%)] Loss: 12298.498047\n",
      "    epoch          : 2215\n",
      "    loss           : 12239.104836286446\n",
      "    val_loss       : 12236.819975729713\n",
      "    val_log_likelihood: -12153.680395891492\n",
      "    val_log_marginal: -12161.96262249945\n",
      "Train Epoch: 2216 [256/118836 (0%)] Loss: 12198.789062\n",
      "Train Epoch: 2216 [33024/118836 (28%)] Loss: 12335.814453\n",
      "Train Epoch: 2216 [65792/118836 (55%)] Loss: 12176.140625\n",
      "Train Epoch: 2216 [98560/118836 (83%)] Loss: 12224.370117\n",
      "    epoch          : 2216\n",
      "    loss           : 12244.225744093776\n",
      "    val_loss       : 12246.047116977186\n",
      "    val_log_likelihood: -12153.12373539599\n",
      "    val_log_marginal: -12161.312409618164\n",
      "Train Epoch: 2217 [256/118836 (0%)] Loss: 12186.635742\n",
      "Train Epoch: 2217 [33024/118836 (28%)] Loss: 12278.591797\n",
      "Train Epoch: 2217 [65792/118836 (55%)] Loss: 12191.587891\n",
      "Train Epoch: 2217 [98560/118836 (83%)] Loss: 12268.920898\n",
      "    epoch          : 2217\n",
      "    loss           : 12239.073085485163\n",
      "    val_loss       : 12236.619387206016\n",
      "    val_log_likelihood: -12151.554055521092\n",
      "    val_log_marginal: -12159.829691858093\n",
      "Train Epoch: 2218 [256/118836 (0%)] Loss: 12128.220703\n",
      "Train Epoch: 2218 [33024/118836 (28%)] Loss: 12221.085938\n",
      "Train Epoch: 2218 [65792/118836 (55%)] Loss: 12329.848633\n",
      "Train Epoch: 2218 [98560/118836 (83%)] Loss: 12216.839844\n",
      "    epoch          : 2218\n",
      "    loss           : 12247.569165665063\n",
      "    val_loss       : 12238.469243488753\n",
      "    val_log_likelihood: -12149.74652750853\n",
      "    val_log_marginal: -12157.788345099741\n",
      "Train Epoch: 2219 [256/118836 (0%)] Loss: 12299.839844\n",
      "Train Epoch: 2219 [33024/118836 (28%)] Loss: 12293.090820\n",
      "Train Epoch: 2219 [65792/118836 (55%)] Loss: 12260.121094\n",
      "Train Epoch: 2219 [98560/118836 (83%)] Loss: 12259.951172\n",
      "    epoch          : 2219\n",
      "    loss           : 12242.626521628154\n",
      "    val_loss       : 12239.281282437947\n",
      "    val_log_likelihood: -12151.099847174835\n",
      "    val_log_marginal: -12159.203240889685\n",
      "Train Epoch: 2220 [256/118836 (0%)] Loss: 12237.909180\n",
      "Train Epoch: 2220 [33024/118836 (28%)] Loss: 12255.458984\n",
      "Train Epoch: 2220 [65792/118836 (55%)] Loss: 12354.771484\n",
      "Train Epoch: 2220 [98560/118836 (83%)] Loss: 12232.666016\n",
      "    epoch          : 2220\n",
      "    loss           : 12241.79697984517\n",
      "    val_loss       : 12240.23599919718\n",
      "    val_log_likelihood: -12149.835054797353\n",
      "    val_log_marginal: -12157.898145849264\n",
      "Train Epoch: 2221 [256/118836 (0%)] Loss: 12209.823242\n",
      "Train Epoch: 2221 [33024/118836 (28%)] Loss: 12265.269531\n",
      "Train Epoch: 2221 [65792/118836 (55%)] Loss: 12278.151367\n",
      "Train Epoch: 2221 [98560/118836 (83%)] Loss: 12341.712891\n",
      "    epoch          : 2221\n",
      "    loss           : 12240.052704003825\n",
      "    val_loss       : 12242.788349504584\n",
      "    val_log_likelihood: -12152.75010226039\n",
      "    val_log_marginal: -12161.008635892278\n",
      "Train Epoch: 2222 [256/118836 (0%)] Loss: 12350.830078\n",
      "Train Epoch: 2222 [33024/118836 (28%)] Loss: 12259.103516\n",
      "Train Epoch: 2222 [65792/118836 (55%)] Loss: 12205.062500\n",
      "Train Epoch: 2222 [98560/118836 (83%)] Loss: 12377.010742\n",
      "    epoch          : 2222\n",
      "    loss           : 12245.446673225548\n",
      "    val_loss       : 12249.073059217451\n",
      "    val_log_likelihood: -12152.872913758789\n",
      "    val_log_marginal: -12161.603272522521\n",
      "Train Epoch: 2223 [256/118836 (0%)] Loss: 12198.695312\n",
      "Train Epoch: 2223 [33024/118836 (28%)] Loss: 12250.531250\n",
      "Train Epoch: 2223 [65792/118836 (55%)] Loss: 12272.394531\n",
      "Train Epoch: 2223 [98560/118836 (83%)] Loss: 12328.653320\n",
      "    epoch          : 2223\n",
      "    loss           : 12243.919597969654\n",
      "    val_loss       : 12247.19167669783\n",
      "    val_log_likelihood: -12151.880650977047\n",
      "    val_log_marginal: -12160.274994778381\n",
      "Train Epoch: 2224 [256/118836 (0%)] Loss: 12183.224609\n",
      "Train Epoch: 2224 [33024/118836 (28%)] Loss: 12205.963867\n",
      "Train Epoch: 2224 [65792/118836 (55%)] Loss: 12182.707031\n",
      "Train Epoch: 2224 [98560/118836 (83%)] Loss: 12190.245117\n",
      "    epoch          : 2224\n",
      "    loss           : 12238.978400602255\n",
      "    val_loss       : 12241.352492990012\n",
      "    val_log_likelihood: -12153.169758387612\n",
      "    val_log_marginal: -12161.712812295133\n",
      "Train Epoch: 2225 [256/118836 (0%)] Loss: 12230.621094\n",
      "Train Epoch: 2225 [33024/118836 (28%)] Loss: 12340.233398\n",
      "Train Epoch: 2225 [65792/118836 (55%)] Loss: 12331.501953\n",
      "Train Epoch: 2225 [98560/118836 (83%)] Loss: 12157.276367\n",
      "    epoch          : 2225\n",
      "    loss           : 12245.485193567773\n",
      "    val_loss       : 12243.134610395447\n",
      "    val_log_likelihood: -12149.87346593259\n",
      "    val_log_marginal: -12157.989475528228\n",
      "Train Epoch: 2226 [256/118836 (0%)] Loss: 12220.391602\n",
      "Train Epoch: 2226 [33024/118836 (28%)] Loss: 12175.323242\n",
      "Train Epoch: 2226 [65792/118836 (55%)] Loss: 12156.041016\n",
      "Train Epoch: 2226 [98560/118836 (83%)] Loss: 12220.997070\n",
      "    epoch          : 2226\n",
      "    loss           : 12241.250113568807\n",
      "    val_loss       : 12241.750313161376\n",
      "    val_log_likelihood: -12150.287161232165\n",
      "    val_log_marginal: -12158.505779879755\n",
      "Train Epoch: 2227 [256/118836 (0%)] Loss: 12196.177734\n",
      "Train Epoch: 2227 [33024/118836 (28%)] Loss: 12196.773438\n",
      "Train Epoch: 2227 [65792/118836 (55%)] Loss: 12285.872070\n",
      "Train Epoch: 2227 [98560/118836 (83%)] Loss: 12208.119141\n",
      "    epoch          : 2227\n",
      "    loss           : 12240.341122408758\n",
      "    val_loss       : 12243.040507896823\n",
      "    val_log_likelihood: -12152.903091882496\n",
      "    val_log_marginal: -12161.075987466655\n",
      "Train Epoch: 2228 [256/118836 (0%)] Loss: 12264.242188\n",
      "Train Epoch: 2228 [33024/118836 (28%)] Loss: 12212.051758\n",
      "Train Epoch: 2228 [65792/118836 (55%)] Loss: 12239.391602\n",
      "Train Epoch: 2228 [98560/118836 (83%)] Loss: 12259.626953\n",
      "    epoch          : 2228\n",
      "    loss           : 12241.30234778872\n",
      "    val_loss       : 12239.060382075171\n",
      "    val_log_likelihood: -12149.528235499381\n",
      "    val_log_marginal: -12157.710477597044\n",
      "Train Epoch: 2229 [256/118836 (0%)] Loss: 12328.105469\n",
      "Train Epoch: 2229 [33024/118836 (28%)] Loss: 12238.172852\n",
      "Train Epoch: 2229 [65792/118836 (55%)] Loss: 12281.336914\n",
      "Train Epoch: 2229 [98560/118836 (83%)] Loss: 12291.586914\n",
      "    epoch          : 2229\n",
      "    loss           : 12246.104173290167\n",
      "    val_loss       : 12239.736677408033\n",
      "    val_log_likelihood: -12150.3433256113\n",
      "    val_log_marginal: -12158.588826569807\n",
      "Train Epoch: 2230 [256/118836 (0%)] Loss: 12252.992188\n",
      "Train Epoch: 2230 [33024/118836 (28%)] Loss: 12298.573242\n",
      "Train Epoch: 2230 [65792/118836 (55%)] Loss: 12287.524414\n",
      "Train Epoch: 2230 [98560/118836 (83%)] Loss: 12142.298828\n",
      "    epoch          : 2230\n",
      "    loss           : 12244.705736274813\n",
      "    val_loss       : 12241.786063503807\n",
      "    val_log_likelihood: -12156.816824822943\n",
      "    val_log_marginal: -12164.998716589698\n",
      "Train Epoch: 2231 [256/118836 (0%)] Loss: 12197.963867\n",
      "Train Epoch: 2231 [33024/118836 (28%)] Loss: 12227.140625\n",
      "Train Epoch: 2231 [65792/118836 (55%)] Loss: 12300.097656\n",
      "Train Epoch: 2231 [98560/118836 (83%)] Loss: 12327.171875\n",
      "    epoch          : 2231\n",
      "    loss           : 12241.186308092949\n",
      "    val_loss       : 12238.359039461833\n",
      "    val_log_likelihood: -12151.63382137226\n",
      "    val_log_marginal: -12159.90102889009\n",
      "Train Epoch: 2232 [256/118836 (0%)] Loss: 12234.085938\n",
      "Train Epoch: 2232 [33024/118836 (28%)] Loss: 12255.882812\n",
      "Train Epoch: 2232 [65792/118836 (55%)] Loss: 12182.353516\n",
      "Train Epoch: 2232 [98560/118836 (83%)] Loss: 12297.257812\n",
      "    epoch          : 2232\n",
      "    loss           : 12240.717134188897\n",
      "    val_loss       : 12239.731685821\n",
      "    val_log_likelihood: -12152.88270216217\n",
      "    val_log_marginal: -12160.909776707083\n",
      "Train Epoch: 2233 [256/118836 (0%)] Loss: 12276.048828\n",
      "Train Epoch: 2233 [33024/118836 (28%)] Loss: 12206.705078\n",
      "Train Epoch: 2233 [65792/118836 (55%)] Loss: 12309.314453\n",
      "Train Epoch: 2233 [98560/118836 (83%)] Loss: 12211.274414\n",
      "    epoch          : 2233\n",
      "    loss           : 12236.229246794872\n",
      "    val_loss       : 12237.5976553693\n",
      "    val_log_likelihood: -12150.701739557486\n",
      "    val_log_marginal: -12158.899726628173\n",
      "Train Epoch: 2234 [256/118836 (0%)] Loss: 12367.611328\n",
      "Train Epoch: 2234 [33024/118836 (28%)] Loss: 12265.671875\n",
      "Train Epoch: 2234 [65792/118836 (55%)] Loss: 12335.132812\n",
      "Train Epoch: 2234 [98560/118836 (83%)] Loss: 12105.937500\n",
      "    epoch          : 2234\n",
      "    loss           : 12239.054217716088\n",
      "    val_loss       : 12244.508142879253\n",
      "    val_log_likelihood: -12151.100474791925\n",
      "    val_log_marginal: -12159.247473412577\n",
      "Train Epoch: 2235 [256/118836 (0%)] Loss: 12177.005859\n",
      "Train Epoch: 2235 [33024/118836 (28%)] Loss: 12235.476562\n",
      "Train Epoch: 2235 [65792/118836 (55%)] Loss: 12318.498047\n",
      "Train Epoch: 2235 [98560/118836 (83%)] Loss: 12349.023438\n",
      "    epoch          : 2235\n",
      "    loss           : 12234.427584780811\n",
      "    val_loss       : 12240.182387248478\n",
      "    val_log_likelihood: -12152.313381733355\n",
      "    val_log_marginal: -12160.563057202464\n",
      "Train Epoch: 2236 [256/118836 (0%)] Loss: 12244.125000\n",
      "Train Epoch: 2236 [33024/118836 (28%)] Loss: 12429.770508\n",
      "Train Epoch: 2236 [65792/118836 (55%)] Loss: 12229.248047\n",
      "Train Epoch: 2236 [98560/118836 (83%)] Loss: 12234.539062\n",
      "    epoch          : 2236\n",
      "    loss           : 12238.60434921681\n",
      "    val_loss       : 12240.639432185562\n",
      "    val_log_likelihood: -12149.438888996587\n",
      "    val_log_marginal: -12157.620864985745\n",
      "Train Epoch: 2237 [256/118836 (0%)] Loss: 12219.968750\n",
      "Train Epoch: 2237 [33024/118836 (28%)] Loss: 12405.464844\n",
      "Train Epoch: 2237 [65792/118836 (55%)] Loss: 12298.636719\n",
      "Train Epoch: 2237 [98560/118836 (83%)] Loss: 12212.625000\n",
      "    epoch          : 2237\n",
      "    loss           : 12241.125667842742\n",
      "    val_loss       : 12243.724369765836\n",
      "    val_log_likelihood: -12149.414234064825\n",
      "    val_log_marginal: -12157.543224008807\n",
      "Train Epoch: 2238 [256/118836 (0%)] Loss: 12235.181641\n",
      "Train Epoch: 2238 [33024/118836 (28%)] Loss: 12249.943359\n",
      "Train Epoch: 2238 [65792/118836 (55%)] Loss: 12305.322266\n",
      "Train Epoch: 2238 [98560/118836 (83%)] Loss: 12176.013672\n",
      "    epoch          : 2238\n",
      "    loss           : 12244.5876145381\n",
      "    val_loss       : 12240.937069570658\n",
      "    val_log_likelihood: -12147.601592386529\n",
      "    val_log_marginal: -12155.8365326011\n",
      "Train Epoch: 2239 [256/118836 (0%)] Loss: 12304.611328\n",
      "Train Epoch: 2239 [33024/118836 (28%)] Loss: 12179.873047\n",
      "Train Epoch: 2239 [65792/118836 (55%)] Loss: 12251.131836\n",
      "Train Epoch: 2239 [98560/118836 (83%)] Loss: 12273.908203\n",
      "    epoch          : 2239\n",
      "    loss           : 12236.904802684296\n",
      "    val_loss       : 12242.324506885972\n",
      "    val_log_likelihood: -12150.407297159327\n",
      "    val_log_marginal: -12158.619804347534\n",
      "Train Epoch: 2240 [256/118836 (0%)] Loss: 12232.195312\n",
      "Train Epoch: 2240 [33024/118836 (28%)] Loss: 12282.477539\n",
      "Train Epoch: 2240 [65792/118836 (55%)] Loss: 12222.392578\n",
      "Train Epoch: 2240 [98560/118836 (83%)] Loss: 12160.653320\n",
      "    epoch          : 2240\n",
      "    loss           : 12242.753433881308\n",
      "    val_loss       : 12241.857320808691\n",
      "    val_log_likelihood: -12150.421308771452\n",
      "    val_log_marginal: -12158.542858002196\n",
      "Train Epoch: 2241 [256/118836 (0%)] Loss: 12345.094727\n",
      "Train Epoch: 2241 [33024/118836 (28%)] Loss: 12265.155273\n",
      "Train Epoch: 2241 [65792/118836 (55%)] Loss: 12323.915039\n",
      "Train Epoch: 2241 [98560/118836 (83%)] Loss: 12225.419922\n",
      "    epoch          : 2241\n",
      "    loss           : 12243.992220940601\n",
      "    val_loss       : 12243.428502786488\n",
      "    val_log_likelihood: -12148.438722924422\n",
      "    val_log_marginal: -12156.745427460319\n",
      "Train Epoch: 2242 [256/118836 (0%)] Loss: 12229.657227\n",
      "Train Epoch: 2242 [33024/118836 (28%)] Loss: 12249.466797\n",
      "Train Epoch: 2242 [65792/118836 (55%)] Loss: 12210.681641\n",
      "Train Epoch: 2242 [98560/118836 (83%)] Loss: 12202.242188\n",
      "    epoch          : 2242\n",
      "    loss           : 12237.709535094862\n",
      "    val_loss       : 12237.775045739167\n",
      "    val_log_likelihood: -12150.549442979736\n",
      "    val_log_marginal: -12158.7228884491\n",
      "Train Epoch: 2243 [256/118836 (0%)] Loss: 12382.324219\n",
      "Train Epoch: 2243 [33024/118836 (28%)] Loss: 12330.235352\n",
      "Train Epoch: 2243 [65792/118836 (55%)] Loss: 12174.199219\n",
      "Train Epoch: 2243 [98560/118836 (83%)] Loss: 12213.322266\n",
      "    epoch          : 2243\n",
      "    loss           : 12240.062675603547\n",
      "    val_loss       : 12242.728542272143\n",
      "    val_log_likelihood: -12153.404760196961\n",
      "    val_log_marginal: -12161.764170252038\n",
      "Train Epoch: 2244 [256/118836 (0%)] Loss: 12244.054688\n",
      "Train Epoch: 2244 [33024/118836 (28%)] Loss: 12259.657227\n",
      "Train Epoch: 2244 [65792/118836 (55%)] Loss: 12202.833984\n",
      "Train Epoch: 2244 [98560/118836 (83%)] Loss: 12293.476562\n",
      "    epoch          : 2244\n",
      "    loss           : 12235.048808092948\n",
      "    val_loss       : 12237.572802098777\n",
      "    val_log_likelihood: -12151.565436795647\n",
      "    val_log_marginal: -12159.756983902205\n",
      "Train Epoch: 2245 [256/118836 (0%)] Loss: 12339.077148\n",
      "Train Epoch: 2245 [33024/118836 (28%)] Loss: 12266.833984\n",
      "Train Epoch: 2245 [65792/118836 (55%)] Loss: 12308.770508\n",
      "Train Epoch: 2245 [98560/118836 (83%)] Loss: 12318.589844\n",
      "    epoch          : 2245\n",
      "    loss           : 12242.293900240385\n",
      "    val_loss       : 12239.95783565198\n",
      "    val_log_likelihood: -12150.847955761477\n",
      "    val_log_marginal: -12159.075674330368\n",
      "Train Epoch: 2246 [256/118836 (0%)] Loss: 12323.487305\n",
      "Train Epoch: 2246 [33024/118836 (28%)] Loss: 12153.106445\n",
      "Train Epoch: 2246 [65792/118836 (55%)] Loss: 12382.677734\n",
      "Train Epoch: 2246 [98560/118836 (83%)] Loss: 12211.071289\n",
      "    epoch          : 2246\n",
      "    loss           : 12241.038299505013\n",
      "    val_loss       : 12237.893719880505\n",
      "    val_log_likelihood: -12152.964838741987\n",
      "    val_log_marginal: -12161.265617792487\n",
      "Train Epoch: 2247 [256/118836 (0%)] Loss: 12373.690430\n",
      "Train Epoch: 2247 [33024/118836 (28%)] Loss: 12263.774414\n",
      "Train Epoch: 2247 [65792/118836 (55%)] Loss: 12320.427734\n",
      "Train Epoch: 2247 [98560/118836 (83%)] Loss: 12304.870117\n",
      "    epoch          : 2247\n",
      "    loss           : 12242.399565918373\n",
      "    val_loss       : 12240.692382863055\n",
      "    val_log_likelihood: -12151.334173710193\n",
      "    val_log_marginal: -12159.47806813583\n",
      "Train Epoch: 2248 [256/118836 (0%)] Loss: 12210.898438\n",
      "Train Epoch: 2248 [33024/118836 (28%)] Loss: 12268.796875\n",
      "Train Epoch: 2248 [65792/118836 (55%)] Loss: 12204.601562\n",
      "Train Epoch: 2248 [98560/118836 (83%)] Loss: 12183.955078\n",
      "    epoch          : 2248\n",
      "    loss           : 12239.210619895059\n",
      "    val_loss       : 12241.174919318182\n",
      "    val_log_likelihood: -12152.730198801954\n",
      "    val_log_marginal: -12161.403777906457\n",
      "Train Epoch: 2249 [256/118836 (0%)] Loss: 12321.365234\n",
      "Train Epoch: 2249 [33024/118836 (28%)] Loss: 12274.386719\n",
      "Train Epoch: 2249 [65792/118836 (55%)] Loss: 12235.807617\n",
      "Train Epoch: 2249 [98560/118836 (83%)] Loss: 12215.309570\n",
      "    epoch          : 2249\n",
      "    loss           : 12240.781040632755\n",
      "    val_loss       : 12233.14489218013\n",
      "    val_log_likelihood: -12150.988180120452\n",
      "    val_log_marginal: -12159.355236513275\n",
      "Train Epoch: 2250 [256/118836 (0%)] Loss: 12290.114258\n",
      "Train Epoch: 2250 [33024/118836 (28%)] Loss: 12314.460938\n",
      "Train Epoch: 2250 [65792/118836 (55%)] Loss: 12210.089844\n",
      "Train Epoch: 2250 [98560/118836 (83%)] Loss: 12362.067383\n",
      "    epoch          : 2250\n",
      "    loss           : 12238.829666498656\n",
      "    val_loss       : 12239.638484740115\n",
      "    val_log_likelihood: -12148.373620211694\n",
      "    val_log_marginal: -12156.518094422538\n",
      "Train Epoch: 2251 [256/118836 (0%)] Loss: 12233.752930\n",
      "Train Epoch: 2251 [33024/118836 (28%)] Loss: 12313.291016\n",
      "Train Epoch: 2251 [65792/118836 (55%)] Loss: 12330.197266\n",
      "Train Epoch: 2251 [98560/118836 (83%)] Loss: 12163.642578\n",
      "    epoch          : 2251\n",
      "    loss           : 12240.67577866522\n",
      "    val_loss       : 12239.120432984724\n",
      "    val_log_likelihood: -12148.47656266155\n",
      "    val_log_marginal: -12156.696527002254\n",
      "Train Epoch: 2252 [256/118836 (0%)] Loss: 12247.694336\n",
      "Train Epoch: 2252 [33024/118836 (28%)] Loss: 12315.308594\n",
      "Train Epoch: 2252 [65792/118836 (55%)] Loss: 12385.197266\n",
      "Train Epoch: 2252 [98560/118836 (83%)] Loss: 12249.839844\n",
      "    epoch          : 2252\n",
      "    loss           : 12241.510544936415\n",
      "    val_loss       : 12237.770866972547\n",
      "    val_log_likelihood: -12151.531334166926\n",
      "    val_log_marginal: -12159.648924303532\n",
      "Train Epoch: 2253 [256/118836 (0%)] Loss: 12346.791016\n",
      "Train Epoch: 2253 [33024/118836 (28%)] Loss: 12266.910156\n",
      "Train Epoch: 2253 [65792/118836 (55%)] Loss: 12389.754883\n",
      "Train Epoch: 2253 [98560/118836 (83%)] Loss: 12312.349609\n",
      "    epoch          : 2253\n",
      "    loss           : 12235.517988297406\n",
      "    val_loss       : 12238.670539474922\n",
      "    val_log_likelihood: -12149.956123991935\n",
      "    val_log_marginal: -12158.12290882537\n",
      "Train Epoch: 2254 [256/118836 (0%)] Loss: 12294.714844\n",
      "Train Epoch: 2254 [33024/118836 (28%)] Loss: 12262.706055\n",
      "Train Epoch: 2254 [65792/118836 (55%)] Loss: 12174.001953\n",
      "Train Epoch: 2254 [98560/118836 (83%)] Loss: 12230.285156\n",
      "    epoch          : 2254\n",
      "    loss           : 12241.54947625879\n",
      "    val_loss       : 12240.555881263588\n",
      "    val_log_likelihood: -12151.027145852719\n",
      "    val_log_marginal: -12159.331289322983\n",
      "Train Epoch: 2255 [256/118836 (0%)] Loss: 12343.242188\n",
      "Train Epoch: 2255 [33024/118836 (28%)] Loss: 12327.174805\n",
      "Train Epoch: 2255 [65792/118836 (55%)] Loss: 12300.904297\n",
      "Train Epoch: 2255 [98560/118836 (83%)] Loss: 12342.605469\n",
      "    epoch          : 2255\n",
      "    loss           : 12239.065483967897\n",
      "    val_loss       : 12235.235440156617\n",
      "    val_log_likelihood: -12150.752217418838\n",
      "    val_log_marginal: -12158.897140453402\n",
      "Train Epoch: 2256 [256/118836 (0%)] Loss: 12234.474609\n",
      "Train Epoch: 2256 [33024/118836 (28%)] Loss: 12317.020508\n",
      "Train Epoch: 2256 [65792/118836 (55%)] Loss: 12259.339844\n",
      "Train Epoch: 2256 [98560/118836 (83%)] Loss: 12251.826172\n",
      "    epoch          : 2256\n",
      "    loss           : 12240.642082170183\n",
      "    val_loss       : 12237.168091975442\n",
      "    val_log_likelihood: -12146.282072768043\n",
      "    val_log_marginal: -12154.443887103891\n",
      "Train Epoch: 2257 [256/118836 (0%)] Loss: 12238.522461\n",
      "Train Epoch: 2257 [33024/118836 (28%)] Loss: 12281.832031\n",
      "Train Epoch: 2257 [65792/118836 (55%)] Loss: 12126.722656\n",
      "Train Epoch: 2257 [98560/118836 (83%)] Loss: 12333.258789\n",
      "    epoch          : 2257\n",
      "    loss           : 12236.966083152398\n",
      "    val_loss       : 12240.691177852354\n",
      "    val_log_likelihood: -12148.473279182177\n",
      "    val_log_marginal: -12156.906197705906\n",
      "Train Epoch: 2258 [256/118836 (0%)] Loss: 12385.264648\n",
      "Train Epoch: 2258 [33024/118836 (28%)] Loss: 12254.341797\n",
      "Train Epoch: 2258 [65792/118836 (55%)] Loss: 12233.044922\n",
      "Train Epoch: 2258 [98560/118836 (83%)] Loss: 12239.140625\n",
      "    epoch          : 2258\n",
      "    loss           : 12237.660023133789\n",
      "    val_loss       : 12235.4409489895\n",
      "    val_log_likelihood: -12147.24600069789\n",
      "    val_log_marginal: -12155.476009040809\n",
      "Train Epoch: 2259 [256/118836 (0%)] Loss: 12206.486328\n",
      "Train Epoch: 2259 [33024/118836 (28%)] Loss: 12211.365234\n",
      "Train Epoch: 2259 [65792/118836 (55%)] Loss: 12243.767578\n",
      "Train Epoch: 2259 [98560/118836 (83%)] Loss: 12149.084961\n",
      "    epoch          : 2259\n",
      "    loss           : 12242.068378599306\n",
      "    val_loss       : 12236.405227178118\n",
      "    val_log_likelihood: -12147.15299705335\n",
      "    val_log_marginal: -12155.335476644897\n",
      "Train Epoch: 2260 [256/118836 (0%)] Loss: 12310.167969\n",
      "Train Epoch: 2260 [33024/118836 (28%)] Loss: 12217.066406\n",
      "Train Epoch: 2260 [65792/118836 (55%)] Loss: 12199.716797\n",
      "Train Epoch: 2260 [98560/118836 (83%)] Loss: 12146.204102\n",
      "    epoch          : 2260\n",
      "    loss           : 12234.763905797663\n",
      "    val_loss       : 12236.060379208018\n",
      "    val_log_likelihood: -12147.824155261322\n",
      "    val_log_marginal: -12155.979167977495\n",
      "Train Epoch: 2261 [256/118836 (0%)] Loss: 12287.652344\n",
      "Train Epoch: 2261 [33024/118836 (28%)] Loss: 12198.678711\n",
      "Train Epoch: 2261 [65792/118836 (55%)] Loss: 12296.401367\n",
      "Train Epoch: 2261 [98560/118836 (83%)] Loss: 12254.873047\n",
      "    epoch          : 2261\n",
      "    loss           : 12237.77297821676\n",
      "    val_loss       : 12235.410336480023\n",
      "    val_log_likelihood: -12147.687313734234\n",
      "    val_log_marginal: -12155.802686334244\n",
      "Train Epoch: 2262 [256/118836 (0%)] Loss: 12270.867188\n",
      "Train Epoch: 2262 [33024/118836 (28%)] Loss: 12380.285156\n",
      "Train Epoch: 2262 [65792/118836 (55%)] Loss: 12402.379883\n",
      "Train Epoch: 2262 [98560/118836 (83%)] Loss: 12248.112305\n",
      "    epoch          : 2262\n",
      "    loss           : 12237.664947787427\n",
      "    val_loss       : 12237.391229326575\n",
      "    val_log_likelihood: -12147.647207467433\n",
      "    val_log_marginal: -12155.791929010491\n",
      "Train Epoch: 2263 [256/118836 (0%)] Loss: 12244.098633\n",
      "Train Epoch: 2263 [33024/118836 (28%)] Loss: 12274.946289\n",
      "Train Epoch: 2263 [65792/118836 (55%)] Loss: 12251.217773\n",
      "Train Epoch: 2263 [98560/118836 (83%)] Loss: 12225.278320\n",
      "    epoch          : 2263\n",
      "    loss           : 12239.08729547922\n",
      "    val_loss       : 12239.993140391189\n",
      "    val_log_likelihood: -12149.707529951147\n",
      "    val_log_marginal: -12157.840647339144\n",
      "Train Epoch: 2264 [256/118836 (0%)] Loss: 12300.772461\n",
      "Train Epoch: 2264 [33024/118836 (28%)] Loss: 12264.681641\n",
      "Train Epoch: 2264 [65792/118836 (55%)] Loss: 12262.378906\n",
      "Train Epoch: 2264 [98560/118836 (83%)] Loss: 12260.796875\n",
      "    epoch          : 2264\n",
      "    loss           : 12239.712888524866\n",
      "    val_loss       : 12240.640014889805\n",
      "    val_log_likelihood: -12152.14516048258\n",
      "    val_log_marginal: -12160.253679842954\n",
      "Train Epoch: 2265 [256/118836 (0%)] Loss: 12245.333984\n",
      "Train Epoch: 2265 [33024/118836 (28%)] Loss: 12355.993164\n",
      "Train Epoch: 2265 [65792/118836 (55%)] Loss: 12251.813477\n",
      "Train Epoch: 2265 [98560/118836 (83%)] Loss: 12286.922852\n",
      "    epoch          : 2265\n",
      "    loss           : 12239.070672753825\n",
      "    val_loss       : 12239.256439100567\n",
      "    val_log_likelihood: -12149.224379006411\n",
      "    val_log_marginal: -12157.303434881129\n",
      "Train Epoch: 2266 [256/118836 (0%)] Loss: 12268.504883\n",
      "Train Epoch: 2266 [33024/118836 (28%)] Loss: 12322.880859\n",
      "Train Epoch: 2266 [65792/118836 (55%)] Loss: 12179.437500\n",
      "Train Epoch: 2266 [98560/118836 (83%)] Loss: 12245.670898\n",
      "    epoch          : 2266\n",
      "    loss           : 12238.122979347601\n",
      "    val_loss       : 12234.688528713492\n",
      "    val_log_likelihood: -12150.906258400537\n",
      "    val_log_marginal: -12159.088085307114\n",
      "Train Epoch: 2267 [256/118836 (0%)] Loss: 12260.728516\n",
      "Train Epoch: 2267 [33024/118836 (28%)] Loss: 12271.514648\n",
      "Train Epoch: 2267 [65792/118836 (55%)] Loss: 12237.559570\n",
      "Train Epoch: 2267 [98560/118836 (83%)] Loss: 12209.505859\n",
      "    epoch          : 2267\n",
      "    loss           : 12240.629991050197\n",
      "    val_loss       : 12238.90262081054\n",
      "    val_log_likelihood: -12148.74055892654\n",
      "    val_log_marginal: -12156.823894037665\n",
      "Train Epoch: 2268 [256/118836 (0%)] Loss: 12295.691406\n",
      "Train Epoch: 2268 [33024/118836 (28%)] Loss: 12281.280273\n",
      "Train Epoch: 2268 [65792/118836 (55%)] Loss: 12264.927734\n",
      "Train Epoch: 2268 [98560/118836 (83%)] Loss: 12240.506836\n",
      "    epoch          : 2268\n",
      "    loss           : 12233.422945583901\n",
      "    val_loss       : 12240.73183004689\n",
      "    val_log_likelihood: -12152.65072196159\n",
      "    val_log_marginal: -12160.962527809312\n",
      "Train Epoch: 2269 [256/118836 (0%)] Loss: 12208.646484\n",
      "Train Epoch: 2269 [33024/118836 (28%)] Loss: 12170.011719\n",
      "Train Epoch: 2269 [65792/118836 (55%)] Loss: 12196.228516\n",
      "Train Epoch: 2269 [98560/118836 (83%)] Loss: 12263.882812\n",
      "    epoch          : 2269\n",
      "    loss           : 12235.65288251525\n",
      "    val_loss       : 12239.341594438176\n",
      "    val_log_likelihood: -12148.542086047353\n",
      "    val_log_marginal: -12156.646196833637\n",
      "Train Epoch: 2270 [256/118836 (0%)] Loss: 12246.234375\n",
      "Train Epoch: 2270 [33024/118836 (28%)] Loss: 12272.313477\n",
      "Train Epoch: 2270 [65792/118836 (55%)] Loss: 12341.025391\n",
      "Train Epoch: 2270 [98560/118836 (83%)] Loss: 12197.212891\n",
      "    epoch          : 2270\n",
      "    loss           : 12236.759512154933\n",
      "    val_loss       : 12234.56205926609\n",
      "    val_log_likelihood: -12150.141386056399\n",
      "    val_log_marginal: -12158.317062382357\n",
      "Train Epoch: 2271 [256/118836 (0%)] Loss: 12230.581055\n",
      "Train Epoch: 2271 [33024/118836 (28%)] Loss: 12246.565430\n",
      "Train Epoch: 2271 [65792/118836 (55%)] Loss: 12222.794922\n",
      "Train Epoch: 2271 [98560/118836 (83%)] Loss: 12278.961914\n",
      "    epoch          : 2271\n",
      "    loss           : 12236.555017705748\n",
      "    val_loss       : 12238.312997644452\n",
      "    val_log_likelihood: -12148.560038319376\n",
      "    val_log_marginal: -12156.59001996931\n",
      "Train Epoch: 2272 [256/118836 (0%)] Loss: 12163.611328\n",
      "Train Epoch: 2272 [33024/118836 (28%)] Loss: 12290.046875\n",
      "Train Epoch: 2272 [65792/118836 (55%)] Loss: 12280.212891\n",
      "Train Epoch: 2272 [98560/118836 (83%)] Loss: 12271.613281\n",
      "    epoch          : 2272\n",
      "    loss           : 12241.40492610758\n",
      "    val_loss       : 12236.82545906903\n",
      "    val_log_likelihood: -12147.346674679487\n",
      "    val_log_marginal: -12155.649156990601\n",
      "Train Epoch: 2273 [256/118836 (0%)] Loss: 12417.574219\n",
      "Train Epoch: 2273 [33024/118836 (28%)] Loss: 12250.626953\n",
      "Train Epoch: 2273 [65792/118836 (55%)] Loss: 12156.458008\n",
      "Train Epoch: 2273 [98560/118836 (83%)] Loss: 12137.306641\n",
      "    epoch          : 2273\n",
      "    loss           : 12235.804866980718\n",
      "    val_loss       : 12243.630269134608\n",
      "    val_log_likelihood: -12149.120719926075\n",
      "    val_log_marginal: -12157.233449378065\n",
      "Train Epoch: 2274 [256/118836 (0%)] Loss: 12325.502930\n",
      "Train Epoch: 2274 [33024/118836 (28%)] Loss: 12175.720703\n",
      "Train Epoch: 2274 [65792/118836 (55%)] Loss: 12196.657227\n",
      "Train Epoch: 2274 [98560/118836 (83%)] Loss: 12130.983398\n",
      "    epoch          : 2274\n",
      "    loss           : 12238.431750478185\n",
      "    val_loss       : 12238.101450867556\n",
      "    val_log_likelihood: -12147.988422120556\n",
      "    val_log_marginal: -12156.249501565837\n",
      "Train Epoch: 2275 [256/118836 (0%)] Loss: 12177.238281\n",
      "Train Epoch: 2275 [33024/118836 (28%)] Loss: 12173.084961\n",
      "Train Epoch: 2275 [65792/118836 (55%)] Loss: 12378.887695\n",
      "Train Epoch: 2275 [98560/118836 (83%)] Loss: 12228.127930\n",
      "    epoch          : 2275\n",
      "    loss           : 12239.575374631668\n",
      "    val_loss       : 12240.512432340587\n",
      "    val_log_likelihood: -12150.514709341398\n",
      "    val_log_marginal: -12158.897507557962\n",
      "Train Epoch: 2276 [256/118836 (0%)] Loss: 12205.437500\n",
      "Train Epoch: 2276 [33024/118836 (28%)] Loss: 12203.765625\n",
      "Train Epoch: 2276 [65792/118836 (55%)] Loss: 12223.564453\n",
      "Train Epoch: 2276 [98560/118836 (83%)] Loss: 12276.214844\n",
      "    epoch          : 2276\n",
      "    loss           : 12237.841766665375\n",
      "    val_loss       : 12239.347662308612\n",
      "    val_log_likelihood: -12149.792583940756\n",
      "    val_log_marginal: -12158.188721020635\n",
      "Train Epoch: 2277 [256/118836 (0%)] Loss: 12165.178711\n",
      "Train Epoch: 2277 [33024/118836 (28%)] Loss: 12219.347656\n",
      "Train Epoch: 2277 [65792/118836 (55%)] Loss: 12184.177734\n",
      "Train Epoch: 2277 [98560/118836 (83%)] Loss: 12224.630859\n",
      "    epoch          : 2277\n",
      "    loss           : 12238.136634098428\n",
      "    val_loss       : 12238.492742621065\n",
      "    val_log_likelihood: -12148.913996426541\n",
      "    val_log_marginal: -12157.278322295466\n",
      "Train Epoch: 2278 [256/118836 (0%)] Loss: 12202.971680\n",
      "Train Epoch: 2278 [33024/118836 (28%)] Loss: 12293.191406\n",
      "Train Epoch: 2278 [65792/118836 (55%)] Loss: 12148.351562\n",
      "Train Epoch: 2278 [98560/118836 (83%)] Loss: 12188.467773\n",
      "    epoch          : 2278\n",
      "    loss           : 12237.249584334937\n",
      "    val_loss       : 12237.938728464098\n",
      "    val_log_likelihood: -12148.848460278381\n",
      "    val_log_marginal: -12156.978719779323\n",
      "Train Epoch: 2279 [256/118836 (0%)] Loss: 12280.497070\n",
      "Train Epoch: 2279 [33024/118836 (28%)] Loss: 12401.551758\n",
      "Train Epoch: 2279 [65792/118836 (55%)] Loss: 12341.912109\n",
      "Train Epoch: 2279 [98560/118836 (83%)] Loss: 12164.041016\n",
      "    epoch          : 2279\n",
      "    loss           : 12236.848884667079\n",
      "    val_loss       : 12239.498358859624\n",
      "    val_log_likelihood: -12152.017297676282\n",
      "    val_log_marginal: -12160.369893942701\n",
      "Train Epoch: 2280 [256/118836 (0%)] Loss: 12238.344727\n",
      "Train Epoch: 2280 [33024/118836 (28%)] Loss: 12219.294922\n",
      "Train Epoch: 2280 [65792/118836 (55%)] Loss: 12186.410156\n",
      "Train Epoch: 2280 [98560/118836 (83%)] Loss: 12284.135742\n",
      "    epoch          : 2280\n",
      "    loss           : 12239.093723829095\n",
      "    val_loss       : 12238.838473947299\n",
      "    val_log_likelihood: -12149.320978727254\n",
      "    val_log_marginal: -12157.493380590255\n",
      "Train Epoch: 2281 [256/118836 (0%)] Loss: 12254.427734\n",
      "Train Epoch: 2281 [33024/118836 (28%)] Loss: 12276.805664\n",
      "Train Epoch: 2281 [65792/118836 (55%)] Loss: 12288.821289\n",
      "Train Epoch: 2281 [98560/118836 (83%)] Loss: 12301.247070\n",
      "    epoch          : 2281\n",
      "    loss           : 12237.008431231907\n",
      "    val_loss       : 12239.027800736694\n",
      "    val_log_likelihood: -12148.442724003567\n",
      "    val_log_marginal: -12156.58356739461\n",
      "Train Epoch: 2282 [256/118836 (0%)] Loss: 12331.201172\n",
      "Train Epoch: 2282 [33024/118836 (28%)] Loss: 12192.764648\n",
      "Train Epoch: 2282 [65792/118836 (55%)] Loss: 12186.951172\n",
      "Train Epoch: 2282 [98560/118836 (83%)] Loss: 12246.331055\n",
      "    epoch          : 2282\n",
      "    loss           : 12235.968184094552\n",
      "    val_loss       : 12243.012612378054\n",
      "    val_log_likelihood: -12146.6801522759\n",
      "    val_log_marginal: -12154.853450072771\n",
      "Train Epoch: 2283 [256/118836 (0%)] Loss: 12252.567383\n",
      "Train Epoch: 2283 [33024/118836 (28%)] Loss: 12207.350586\n",
      "Train Epoch: 2283 [65792/118836 (55%)] Loss: 12160.316406\n",
      "Train Epoch: 2283 [98560/118836 (83%)] Loss: 12339.276367\n",
      "    epoch          : 2283\n",
      "    loss           : 12235.93346968052\n",
      "    val_loss       : 12244.721907621295\n",
      "    val_log_likelihood: -12152.225971392938\n",
      "    val_log_marginal: -12160.349686995409\n",
      "Train Epoch: 2284 [256/118836 (0%)] Loss: 12276.171875\n",
      "Train Epoch: 2284 [33024/118836 (28%)] Loss: 12166.583984\n",
      "Train Epoch: 2284 [65792/118836 (55%)] Loss: 12196.769531\n",
      "Train Epoch: 2284 [98560/118836 (83%)] Loss: 12180.976562\n",
      "    epoch          : 2284\n",
      "    loss           : 12235.80017108018\n",
      "    val_loss       : 12231.512233461728\n",
      "    val_log_likelihood: -12147.614649083696\n",
      "    val_log_marginal: -12155.932814293832\n",
      "Train Epoch: 2285 [256/118836 (0%)] Loss: 12171.344727\n",
      "Train Epoch: 2285 [33024/118836 (28%)] Loss: 12247.034180\n",
      "Train Epoch: 2285 [65792/118836 (55%)] Loss: 12212.680664\n",
      "Train Epoch: 2285 [98560/118836 (83%)] Loss: 12279.906250\n",
      "    epoch          : 2285\n",
      "    loss           : 12239.010789521299\n",
      "    val_loss       : 12239.220038342972\n",
      "    val_log_likelihood: -12148.847767395575\n",
      "    val_log_marginal: -12157.126273081398\n",
      "Train Epoch: 2286 [256/118836 (0%)] Loss: 12164.635742\n",
      "Train Epoch: 2286 [33024/118836 (28%)] Loss: 12313.315430\n",
      "Train Epoch: 2286 [65792/118836 (55%)] Loss: 12162.983398\n",
      "Train Epoch: 2286 [98560/118836 (83%)] Loss: 12326.447266\n",
      "    epoch          : 2286\n",
      "    loss           : 12240.607259195358\n",
      "    val_loss       : 12234.77231303688\n",
      "    val_log_likelihood: -12148.017701386734\n",
      "    val_log_marginal: -12156.21200510771\n",
      "Train Epoch: 2287 [256/118836 (0%)] Loss: 12318.541016\n",
      "Train Epoch: 2287 [33024/118836 (28%)] Loss: 12319.271484\n",
      "Train Epoch: 2287 [65792/118836 (55%)] Loss: 12364.088867\n",
      "Train Epoch: 2287 [98560/118836 (83%)] Loss: 12149.852539\n",
      "    epoch          : 2287\n",
      "    loss           : 12242.267633213141\n",
      "    val_loss       : 12236.4540282716\n",
      "    val_log_likelihood: -12149.62602922741\n",
      "    val_log_marginal: -12157.827636075728\n",
      "Train Epoch: 2288 [256/118836 (0%)] Loss: 12293.752930\n",
      "Train Epoch: 2288 [33024/118836 (28%)] Loss: 12179.071289\n",
      "Train Epoch: 2288 [65792/118836 (55%)] Loss: 12238.860352\n",
      "Train Epoch: 2288 [98560/118836 (83%)] Loss: 12356.424805\n",
      "    epoch          : 2288\n",
      "    loss           : 12242.117218840467\n",
      "    val_loss       : 12237.646045392563\n",
      "    val_log_likelihood: -12148.412344428505\n",
      "    val_log_marginal: -12156.937866848146\n",
      "Train Epoch: 2289 [256/118836 (0%)] Loss: 12139.779297\n",
      "Train Epoch: 2289 [33024/118836 (28%)] Loss: 12150.639648\n",
      "Train Epoch: 2289 [65792/118836 (55%)] Loss: 12237.533203\n",
      "Train Epoch: 2289 [98560/118836 (83%)] Loss: 12170.713867\n",
      "    epoch          : 2289\n",
      "    loss           : 12234.625142324494\n",
      "    val_loss       : 12246.692985838445\n",
      "    val_log_likelihood: -12146.773268519955\n",
      "    val_log_marginal: -12154.958272839342\n",
      "Train Epoch: 2290 [256/118836 (0%)] Loss: 12316.977539\n",
      "Train Epoch: 2290 [33024/118836 (28%)] Loss: 12200.909180\n",
      "Train Epoch: 2290 [65792/118836 (55%)] Loss: 12331.160156\n",
      "Train Epoch: 2290 [98560/118836 (83%)] Loss: 12280.402344\n",
      "    epoch          : 2290\n",
      "    loss           : 12238.693126098531\n",
      "    val_loss       : 12239.026819116672\n",
      "    val_log_likelihood: -12149.200799181917\n",
      "    val_log_marginal: -12157.278809662706\n",
      "Train Epoch: 2291 [256/118836 (0%)] Loss: 12331.708008\n",
      "Train Epoch: 2291 [33024/118836 (28%)] Loss: 12224.375000\n",
      "Train Epoch: 2291 [65792/118836 (55%)] Loss: 12327.900391\n",
      "Train Epoch: 2291 [98560/118836 (83%)] Loss: 12287.133789\n",
      "    epoch          : 2291\n",
      "    loss           : 12239.997939445047\n",
      "    val_loss       : 12239.124740781179\n",
      "    val_log_likelihood: -12148.253023547353\n",
      "    val_log_marginal: -12156.323352749507\n",
      "Train Epoch: 2292 [256/118836 (0%)] Loss: 12214.611328\n",
      "Train Epoch: 2292 [33024/118836 (28%)] Loss: 12257.124023\n",
      "Train Epoch: 2292 [65792/118836 (55%)] Loss: 12097.136719\n",
      "Train Epoch: 2292 [98560/118836 (83%)] Loss: 12204.220703\n",
      "    epoch          : 2292\n",
      "    loss           : 12234.93420117349\n",
      "    val_loss       : 12238.4118148444\n",
      "    val_log_likelihood: -12147.792454701716\n",
      "    val_log_marginal: -12155.975797491776\n",
      "Train Epoch: 2293 [256/118836 (0%)] Loss: 12230.609375\n",
      "Train Epoch: 2293 [33024/118836 (28%)] Loss: 12182.666016\n",
      "Train Epoch: 2293 [65792/118836 (55%)] Loss: 12210.494141\n",
      "Train Epoch: 2293 [98560/118836 (83%)] Loss: 12250.321289\n",
      "    epoch          : 2293\n",
      "    loss           : 12238.457748203577\n",
      "    val_loss       : 12232.912171438695\n",
      "    val_log_likelihood: -12150.710259641231\n",
      "    val_log_marginal: -12158.95965591822\n",
      "Train Epoch: 2294 [256/118836 (0%)] Loss: 12386.837891\n",
      "Train Epoch: 2294 [33024/118836 (28%)] Loss: 12332.959961\n",
      "Train Epoch: 2294 [65792/118836 (55%)] Loss: 12261.464844\n",
      "Train Epoch: 2294 [98560/118836 (83%)] Loss: 12247.759766\n",
      "    epoch          : 2294\n",
      "    loss           : 12233.954185890972\n",
      "    val_loss       : 12235.155693089053\n",
      "    val_log_likelihood: -12146.203560858665\n",
      "    val_log_marginal: -12154.298510849752\n",
      "Train Epoch: 2295 [256/118836 (0%)] Loss: 12232.191406\n",
      "Train Epoch: 2295 [33024/118836 (28%)] Loss: 12199.132812\n",
      "Train Epoch: 2295 [65792/118836 (55%)] Loss: 12376.497070\n",
      "Train Epoch: 2295 [98560/118836 (83%)] Loss: 12204.921875\n",
      "    epoch          : 2295\n",
      "    loss           : 12234.90065071857\n",
      "    val_loss       : 12239.08265960114\n",
      "    val_log_likelihood: -12148.890843090881\n",
      "    val_log_marginal: -12157.18831916484\n",
      "Train Epoch: 2296 [256/118836 (0%)] Loss: 12239.832031\n",
      "Train Epoch: 2296 [33024/118836 (28%)] Loss: 12301.384766\n",
      "Train Epoch: 2296 [65792/118836 (55%)] Loss: 12299.140625\n",
      "Train Epoch: 2296 [98560/118836 (83%)] Loss: 12223.363281\n",
      "    epoch          : 2296\n",
      "    loss           : 12238.15989502559\n",
      "    val_loss       : 12237.177922362776\n",
      "    val_log_likelihood: -12149.816745987127\n",
      "    val_log_marginal: -12158.09042510559\n",
      "Train Epoch: 2297 [256/118836 (0%)] Loss: 12286.033203\n",
      "Train Epoch: 2297 [33024/118836 (28%)] Loss: 12194.808594\n",
      "Train Epoch: 2297 [65792/118836 (55%)] Loss: 12246.935547\n",
      "Train Epoch: 2297 [98560/118836 (83%)] Loss: 12226.183594\n",
      "    epoch          : 2297\n",
      "    loss           : 12234.764002080748\n",
      "    val_loss       : 12242.765110037593\n",
      "    val_log_likelihood: -12148.478700275278\n",
      "    val_log_marginal: -12156.507155977122\n",
      "Train Epoch: 2298 [256/118836 (0%)] Loss: 12215.744141\n",
      "Train Epoch: 2298 [33024/118836 (28%)] Loss: 12256.722656\n",
      "Train Epoch: 2298 [65792/118836 (55%)] Loss: 12266.909180\n",
      "Train Epoch: 2298 [98560/118836 (83%)] Loss: 12218.947266\n",
      "    epoch          : 2298\n",
      "    loss           : 12239.066977486558\n",
      "    val_loss       : 12238.19909195883\n",
      "    val_log_likelihood: -12148.962133284222\n",
      "    val_log_marginal: -12157.107806145674\n",
      "Train Epoch: 2299 [256/118836 (0%)] Loss: 12202.305664\n",
      "Train Epoch: 2299 [33024/118836 (28%)] Loss: 12202.863281\n",
      "Train Epoch: 2299 [65792/118836 (55%)] Loss: 12360.839844\n",
      "Train Epoch: 2299 [98560/118836 (83%)] Loss: 12317.602539\n",
      "    epoch          : 2299\n",
      "    loss           : 12237.345016704145\n",
      "    val_loss       : 12241.971853754681\n",
      "    val_log_likelihood: -12149.647629109802\n",
      "    val_log_marginal: -12157.888548680368\n",
      "Train Epoch: 2300 [256/118836 (0%)] Loss: 12192.263672\n",
      "Train Epoch: 2300 [33024/118836 (28%)] Loss: 12371.769531\n",
      "Train Epoch: 2300 [65792/118836 (55%)] Loss: 12258.874023\n",
      "Train Epoch: 2300 [98560/118836 (83%)] Loss: 12234.506836\n",
      "    epoch          : 2300\n",
      "    loss           : 12239.540409978546\n",
      "    val_loss       : 12236.508597105492\n",
      "    val_log_likelihood: -12147.729440168787\n",
      "    val_log_marginal: -12156.11721150176\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch2300.pth ...\n",
      "Train Epoch: 2301 [256/118836 (0%)] Loss: 12278.729492\n",
      "Train Epoch: 2301 [33024/118836 (28%)] Loss: 12155.824219\n",
      "Train Epoch: 2301 [65792/118836 (55%)] Loss: 12454.235352\n",
      "Train Epoch: 2301 [98560/118836 (83%)] Loss: 12195.903320\n",
      "    epoch          : 2301\n",
      "    loss           : 12239.613903051335\n",
      "    val_loss       : 12237.298221071758\n",
      "    val_log_likelihood: -12149.417616412065\n",
      "    val_log_marginal: -12157.568590371468\n",
      "Train Epoch: 2302 [256/118836 (0%)] Loss: 12247.696289\n",
      "Train Epoch: 2302 [33024/118836 (28%)] Loss: 12439.881836\n",
      "Train Epoch: 2302 [65792/118836 (55%)] Loss: 12312.860352\n",
      "Train Epoch: 2302 [98560/118836 (83%)] Loss: 12280.297852\n",
      "    epoch          : 2302\n",
      "    loss           : 12233.727013059606\n",
      "    val_loss       : 12234.429988578495\n",
      "    val_log_likelihood: -12147.217370373242\n",
      "    val_log_marginal: -12155.532300407023\n",
      "Train Epoch: 2303 [256/118836 (0%)] Loss: 12213.712891\n",
      "Train Epoch: 2303 [33024/118836 (28%)] Loss: 12374.365234\n",
      "Train Epoch: 2303 [65792/118836 (55%)] Loss: 12346.979492\n",
      "Train Epoch: 2303 [98560/118836 (83%)] Loss: 12179.369141\n",
      "    epoch          : 2303\n",
      "    loss           : 12241.292527721775\n",
      "    val_loss       : 12235.619429067414\n",
      "    val_log_likelihood: -12147.822924097913\n",
      "    val_log_marginal: -12155.96606455158\n",
      "Train Epoch: 2304 [256/118836 (0%)] Loss: 12224.218750\n",
      "Train Epoch: 2304 [33024/118836 (28%)] Loss: 12367.302734\n",
      "Train Epoch: 2304 [65792/118836 (55%)] Loss: 12240.283203\n",
      "Train Epoch: 2304 [98560/118836 (83%)] Loss: 12232.617188\n",
      "    epoch          : 2304\n",
      "    loss           : 12238.72544716708\n",
      "    val_loss       : 12236.010377800852\n",
      "    val_log_likelihood: -12149.400351207092\n",
      "    val_log_marginal: -12157.569172485952\n",
      "Train Epoch: 2305 [256/118836 (0%)] Loss: 12210.984375\n",
      "Train Epoch: 2305 [33024/118836 (28%)] Loss: 12360.331055\n",
      "Train Epoch: 2305 [65792/118836 (55%)] Loss: 12247.504883\n",
      "Train Epoch: 2305 [98560/118836 (83%)] Loss: 12202.361328\n",
      "    epoch          : 2305\n",
      "    loss           : 12240.344587953628\n",
      "    val_loss       : 12237.681196905382\n",
      "    val_log_likelihood: -12147.718349520524\n",
      "    val_log_marginal: -12155.889865140427\n",
      "Train Epoch: 2306 [256/118836 (0%)] Loss: 12222.958984\n",
      "Train Epoch: 2306 [33024/118836 (28%)] Loss: 12164.030273\n",
      "Train Epoch: 2306 [65792/118836 (55%)] Loss: 12141.039062\n",
      "Train Epoch: 2306 [98560/118836 (83%)] Loss: 12266.330078\n",
      "    epoch          : 2306\n",
      "    loss           : 12237.337076742142\n",
      "    val_loss       : 12233.275215337422\n",
      "    val_log_likelihood: -12147.979671022022\n",
      "    val_log_marginal: -12156.069950317647\n",
      "Train Epoch: 2307 [256/118836 (0%)] Loss: 12207.652344\n",
      "Train Epoch: 2307 [33024/118836 (28%)] Loss: 12230.808594\n",
      "Train Epoch: 2307 [65792/118836 (55%)] Loss: 12202.237305\n",
      "Train Epoch: 2307 [98560/118836 (83%)] Loss: 12240.125000\n",
      "    epoch          : 2307\n",
      "    loss           : 12235.96285266103\n",
      "    val_loss       : 12234.705368471876\n",
      "    val_log_likelihood: -12148.182294412996\n",
      "    val_log_marginal: -12156.24496867394\n",
      "Train Epoch: 2308 [256/118836 (0%)] Loss: 12261.062500\n",
      "Train Epoch: 2308 [33024/118836 (28%)] Loss: 12178.400391\n",
      "Train Epoch: 2308 [65792/118836 (55%)] Loss: 12257.720703\n",
      "Train Epoch: 2308 [98560/118836 (83%)] Loss: 12234.177734\n",
      "    epoch          : 2308\n",
      "    loss           : 12235.30745806193\n",
      "    val_loss       : 12236.875701193836\n",
      "    val_log_likelihood: -12148.861586441531\n",
      "    val_log_marginal: -12156.933330266234\n",
      "Train Epoch: 2309 [256/118836 (0%)] Loss: 12194.167969\n",
      "Train Epoch: 2309 [33024/118836 (28%)] Loss: 12204.722656\n",
      "Train Epoch: 2309 [65792/118836 (55%)] Loss: 12271.874023\n",
      "Train Epoch: 2309 [98560/118836 (83%)] Loss: 12259.832031\n",
      "    epoch          : 2309\n",
      "    loss           : 12233.31658928479\n",
      "    val_loss       : 12238.924186530152\n",
      "    val_log_likelihood: -12147.074426824856\n",
      "    val_log_marginal: -12155.340201356441\n",
      "Train Epoch: 2310 [256/118836 (0%)] Loss: 12254.134766\n",
      "Train Epoch: 2310 [33024/118836 (28%)] Loss: 12165.518555\n",
      "Train Epoch: 2310 [65792/118836 (55%)] Loss: 12272.777344\n",
      "Train Epoch: 2310 [98560/118836 (83%)] Loss: 12256.355469\n",
      "    epoch          : 2310\n",
      "    loss           : 12238.159890017576\n",
      "    val_loss       : 12241.857227541288\n",
      "    val_log_likelihood: -12150.786179015457\n",
      "    val_log_marginal: -12159.370600452648\n",
      "Train Epoch: 2311 [256/118836 (0%)] Loss: 12253.584961\n",
      "Train Epoch: 2311 [33024/118836 (28%)] Loss: 12249.357422\n",
      "Train Epoch: 2311 [65792/118836 (55%)] Loss: 12323.382812\n",
      "Train Epoch: 2311 [98560/118836 (83%)] Loss: 12314.602539\n",
      "    epoch          : 2311\n",
      "    loss           : 12238.868768578113\n",
      "    val_loss       : 12236.601930839992\n",
      "    val_log_likelihood: -12147.831727053608\n",
      "    val_log_marginal: -12156.119388341647\n",
      "Train Epoch: 2312 [256/118836 (0%)] Loss: 12283.915039\n",
      "Train Epoch: 2312 [33024/118836 (28%)] Loss: 12198.144531\n",
      "Train Epoch: 2312 [65792/118836 (55%)] Loss: 12252.001953\n",
      "Train Epoch: 2312 [98560/118836 (83%)] Loss: 12194.873047\n",
      "    epoch          : 2312\n",
      "    loss           : 12242.819561621174\n",
      "    val_loss       : 12234.317019995782\n",
      "    val_log_likelihood: -12147.740770717017\n",
      "    val_log_marginal: -12155.931544369749\n",
      "Train Epoch: 2313 [256/118836 (0%)] Loss: 12399.343750\n",
      "Train Epoch: 2313 [33024/118836 (28%)] Loss: 12206.670898\n",
      "Train Epoch: 2313 [65792/118836 (55%)] Loss: 12282.857422\n",
      "Train Epoch: 2313 [98560/118836 (83%)] Loss: 12188.890625\n",
      "    epoch          : 2313\n",
      "    loss           : 12238.507144011062\n",
      "    val_loss       : 12239.972033789922\n",
      "    val_log_likelihood: -12149.06863820823\n",
      "    val_log_marginal: -12157.155218716465\n",
      "Train Epoch: 2314 [256/118836 (0%)] Loss: 12193.155273\n",
      "Train Epoch: 2314 [33024/118836 (28%)] Loss: 12220.620117\n",
      "Train Epoch: 2314 [65792/118836 (55%)] Loss: 12241.001953\n",
      "Train Epoch: 2314 [98560/118836 (83%)] Loss: 12366.083984\n",
      "    epoch          : 2314\n",
      "    loss           : 12237.59482494572\n",
      "    val_loss       : 12237.436871091104\n",
      "    val_log_likelihood: -12148.335915690914\n",
      "    val_log_marginal: -12156.504916296979\n",
      "Train Epoch: 2315 [256/118836 (0%)] Loss: 12207.690430\n",
      "Train Epoch: 2315 [33024/118836 (28%)] Loss: 12307.447266\n",
      "Train Epoch: 2315 [65792/118836 (55%)] Loss: 12158.833984\n",
      "Train Epoch: 2315 [98560/118836 (83%)] Loss: 12280.386719\n",
      "    epoch          : 2315\n",
      "    loss           : 12240.769546435587\n",
      "    val_loss       : 12237.79545176634\n",
      "    val_log_likelihood: -12151.502209018301\n",
      "    val_log_marginal: -12159.617940456936\n",
      "Train Epoch: 2316 [256/118836 (0%)] Loss: 12345.955078\n",
      "Train Epoch: 2316 [33024/118836 (28%)] Loss: 12300.632812\n",
      "Train Epoch: 2316 [65792/118836 (55%)] Loss: 12230.554688\n",
      "Train Epoch: 2316 [98560/118836 (83%)] Loss: 12215.386719\n",
      "    epoch          : 2316\n",
      "    loss           : 12242.344039172352\n",
      "    val_loss       : 12236.765856422793\n",
      "    val_log_likelihood: -12149.618661471259\n",
      "    val_log_marginal: -12157.714567172761\n",
      "Train Epoch: 2317 [256/118836 (0%)] Loss: 12178.877930\n",
      "Train Epoch: 2317 [33024/118836 (28%)] Loss: 12212.602539\n",
      "Train Epoch: 2317 [65792/118836 (55%)] Loss: 12335.517578\n",
      "Train Epoch: 2317 [98560/118836 (83%)] Loss: 12227.505859\n",
      "    epoch          : 2317\n",
      "    loss           : 12235.604749050093\n",
      "    val_loss       : 12250.05656354507\n",
      "    val_log_likelihood: -12154.948638628257\n",
      "    val_log_marginal: -12163.394994967326\n",
      "Train Epoch: 2318 [256/118836 (0%)] Loss: 12315.924805\n",
      "Train Epoch: 2318 [33024/118836 (28%)] Loss: 12214.954102\n",
      "Train Epoch: 2318 [65792/118836 (55%)] Loss: 12195.008789\n",
      "Train Epoch: 2318 [98560/118836 (83%)] Loss: 12221.124023\n",
      "    epoch          : 2318\n",
      "    loss           : 12240.238412589175\n",
      "    val_loss       : 12240.846941846623\n",
      "    val_log_likelihood: -12147.939963296112\n",
      "    val_log_marginal: -12156.421762288588\n",
      "Train Epoch: 2319 [256/118836 (0%)] Loss: 12349.709961\n",
      "Train Epoch: 2319 [33024/118836 (28%)] Loss: 12182.274414\n",
      "Train Epoch: 2319 [65792/118836 (55%)] Loss: 12298.355469\n",
      "Train Epoch: 2319 [98560/118836 (83%)] Loss: 12258.280273\n",
      "    epoch          : 2319\n",
      "    loss           : 12235.591166188482\n",
      "    val_loss       : 12235.89225546125\n",
      "    val_log_likelihood: -12146.378442120296\n",
      "    val_log_marginal: -12154.573693407508\n",
      "Train Epoch: 2320 [256/118836 (0%)] Loss: 12219.973633\n",
      "Train Epoch: 2320 [33024/118836 (28%)] Loss: 12154.985352\n",
      "Train Epoch: 2320 [65792/118836 (55%)] Loss: 12209.522461\n",
      "Train Epoch: 2320 [98560/118836 (83%)] Loss: 12278.493164\n",
      "    epoch          : 2320\n",
      "    loss           : 12234.669069543526\n",
      "    val_loss       : 12239.798806189565\n",
      "    val_log_likelihood: -12149.034085504549\n",
      "    val_log_marginal: -12157.241295309794\n",
      "Train Epoch: 2321 [256/118836 (0%)] Loss: 12227.150391\n",
      "Train Epoch: 2321 [33024/118836 (28%)] Loss: 12174.440430\n",
      "Train Epoch: 2321 [65792/118836 (55%)] Loss: 12240.513672\n",
      "Train Epoch: 2321 [98560/118836 (83%)] Loss: 12183.904297\n",
      "    epoch          : 2321\n",
      "    loss           : 12236.16733305547\n",
      "    val_loss       : 12233.47528399557\n",
      "    val_log_likelihood: -12151.200607746587\n",
      "    val_log_marginal: -12159.496782771888\n",
      "Train Epoch: 2322 [256/118836 (0%)] Loss: 12324.809570\n",
      "Train Epoch: 2322 [33024/118836 (28%)] Loss: 12160.419922\n",
      "Train Epoch: 2322 [65792/118836 (55%)] Loss: 12316.531250\n",
      "Train Epoch: 2322 [98560/118836 (83%)] Loss: 12299.684570\n",
      "    epoch          : 2322\n",
      "    loss           : 12236.263369132548\n",
      "    val_loss       : 12240.15715699701\n",
      "    val_log_likelihood: -12147.626949894024\n",
      "    val_log_marginal: -12155.68436890267\n",
      "Train Epoch: 2323 [256/118836 (0%)] Loss: 12280.632812\n",
      "Train Epoch: 2323 [33024/118836 (28%)] Loss: 12260.963867\n",
      "Train Epoch: 2323 [65792/118836 (55%)] Loss: 12194.215820\n",
      "Train Epoch: 2323 [98560/118836 (83%)] Loss: 12234.550781\n",
      "    epoch          : 2323\n",
      "    loss           : 12234.34016830154\n",
      "    val_loss       : 12233.774803842027\n",
      "    val_log_likelihood: -12146.520692301232\n",
      "    val_log_marginal: -12154.801350277843\n",
      "Train Epoch: 2324 [256/118836 (0%)] Loss: 12274.046875\n",
      "Train Epoch: 2324 [33024/118836 (28%)] Loss: 12281.858398\n",
      "Train Epoch: 2324 [65792/118836 (55%)] Loss: 12232.670898\n",
      "Train Epoch: 2324 [98560/118836 (83%)] Loss: 12299.156250\n",
      "    epoch          : 2324\n",
      "    loss           : 12232.905496213296\n",
      "    val_loss       : 12239.449612405118\n",
      "    val_log_likelihood: -12148.270418799111\n",
      "    val_log_marginal: -12156.353408647607\n",
      "Train Epoch: 2325 [256/118836 (0%)] Loss: 12204.151367\n",
      "Train Epoch: 2325 [33024/118836 (28%)] Loss: 12214.543945\n",
      "Train Epoch: 2325 [65792/118836 (55%)] Loss: 12240.724609\n",
      "Train Epoch: 2325 [98560/118836 (83%)] Loss: 12439.849609\n",
      "    epoch          : 2325\n",
      "    loss           : 12237.64998368357\n",
      "    val_loss       : 12238.184525716277\n",
      "    val_log_likelihood: -12149.745173083385\n",
      "    val_log_marginal: -12158.055154710051\n",
      "Train Epoch: 2326 [256/118836 (0%)] Loss: 12314.600586\n",
      "Train Epoch: 2326 [33024/118836 (28%)] Loss: 12152.124023\n",
      "Train Epoch: 2326 [65792/118836 (55%)] Loss: 12320.854492\n",
      "Train Epoch: 2326 [98560/118836 (83%)] Loss: 12232.804688\n",
      "    epoch          : 2326\n",
      "    loss           : 12237.68149765431\n",
      "    val_loss       : 12240.931543985524\n",
      "    val_log_likelihood: -12148.959341559399\n",
      "    val_log_marginal: -12157.280087113411\n",
      "Train Epoch: 2327 [256/118836 (0%)] Loss: 12278.591797\n",
      "Train Epoch: 2327 [33024/118836 (28%)] Loss: 12275.151367\n",
      "Train Epoch: 2327 [65792/118836 (55%)] Loss: 12260.335938\n",
      "Train Epoch: 2327 [98560/118836 (83%)] Loss: 12197.205078\n",
      "    epoch          : 2327\n",
      "    loss           : 12237.089059430573\n",
      "    val_loss       : 12239.943138489956\n",
      "    val_log_likelihood: -12147.861101310484\n",
      "    val_log_marginal: -12156.200873620583\n",
      "Train Epoch: 2328 [256/118836 (0%)] Loss: 12271.720703\n",
      "Train Epoch: 2328 [33024/118836 (28%)] Loss: 12344.015625\n",
      "Train Epoch: 2328 [65792/118836 (55%)] Loss: 12214.777344\n",
      "Train Epoch: 2328 [98560/118836 (83%)] Loss: 12284.666016\n",
      "    epoch          : 2328\n",
      "    loss           : 12238.193327226789\n",
      "    val_loss       : 12245.780401706992\n",
      "    val_log_likelihood: -12148.42954194453\n",
      "    val_log_marginal: -12156.71096517225\n",
      "Train Epoch: 2329 [256/118836 (0%)] Loss: 12239.228516\n",
      "Train Epoch: 2329 [33024/118836 (28%)] Loss: 12329.274414\n",
      "Train Epoch: 2329 [65792/118836 (55%)] Loss: 12265.355469\n",
      "Train Epoch: 2329 [98560/118836 (83%)] Loss: 12278.890625\n",
      "    epoch          : 2329\n",
      "    loss           : 12244.405234988884\n",
      "    val_loss       : 12237.853621482578\n",
      "    val_log_likelihood: -12146.45743399116\n",
      "    val_log_marginal: -12154.615377283688\n",
      "Train Epoch: 2330 [256/118836 (0%)] Loss: 12277.951172\n",
      "Train Epoch: 2330 [33024/118836 (28%)] Loss: 12184.461914\n",
      "Train Epoch: 2330 [65792/118836 (55%)] Loss: 12277.901367\n",
      "Train Epoch: 2330 [98560/118836 (83%)] Loss: 12327.320312\n",
      "    epoch          : 2330\n",
      "    loss           : 12239.76588493202\n",
      "    val_loss       : 12237.12197655555\n",
      "    val_log_likelihood: -12150.285790005944\n",
      "    val_log_marginal: -12158.48116201243\n",
      "Train Epoch: 2331 [256/118836 (0%)] Loss: 12277.790039\n",
      "Train Epoch: 2331 [33024/118836 (28%)] Loss: 12382.243164\n",
      "Train Epoch: 2331 [65792/118836 (55%)] Loss: 12203.597656\n",
      "Train Epoch: 2331 [98560/118836 (83%)] Loss: 12249.733398\n",
      "    epoch          : 2331\n",
      "    loss           : 12236.294605562447\n",
      "    val_loss       : 12233.11033732372\n",
      "    val_log_likelihood: -12147.169548858818\n",
      "    val_log_marginal: -12155.326095263865\n",
      "Train Epoch: 2332 [256/118836 (0%)] Loss: 12254.666016\n",
      "Train Epoch: 2332 [33024/118836 (28%)] Loss: 12332.380859\n",
      "Train Epoch: 2332 [65792/118836 (55%)] Loss: 12337.741211\n",
      "Train Epoch: 2332 [98560/118836 (83%)] Loss: 12212.327148\n",
      "    epoch          : 2332\n",
      "    loss           : 12238.349662201457\n",
      "    val_loss       : 12234.46238079069\n",
      "    val_log_likelihood: -12147.983952226788\n",
      "    val_log_marginal: -12156.10746822727\n",
      "Train Epoch: 2333 [256/118836 (0%)] Loss: 12247.156250\n",
      "Train Epoch: 2333 [33024/118836 (28%)] Loss: 12170.553711\n",
      "Train Epoch: 2333 [65792/118836 (55%)] Loss: 12179.053711\n",
      "Train Epoch: 2333 [98560/118836 (83%)] Loss: 12180.353516\n",
      "    epoch          : 2333\n",
      "    loss           : 12235.6035695823\n",
      "    val_loss       : 12241.266990886745\n",
      "    val_log_likelihood: -12151.226449738937\n",
      "    val_log_marginal: -12159.666251310073\n",
      "Train Epoch: 2334 [256/118836 (0%)] Loss: 12247.866211\n",
      "Train Epoch: 2334 [33024/118836 (28%)] Loss: 12256.900391\n",
      "Train Epoch: 2334 [65792/118836 (55%)] Loss: 12305.962891\n",
      "Train Epoch: 2334 [98560/118836 (83%)] Loss: 12232.199219\n",
      "    epoch          : 2334\n",
      "    loss           : 12240.33115177833\n",
      "    val_loss       : 12240.259705603652\n",
      "    val_log_likelihood: -12147.55995415245\n",
      "    val_log_marginal: -12155.79844513975\n",
      "Train Epoch: 2335 [256/118836 (0%)] Loss: 12292.825195\n",
      "Train Epoch: 2335 [33024/118836 (28%)] Loss: 12203.113281\n",
      "Train Epoch: 2335 [65792/118836 (55%)] Loss: 12261.829102\n",
      "Train Epoch: 2335 [98560/118836 (83%)] Loss: 12311.242188\n",
      "    epoch          : 2335\n",
      "    loss           : 12235.398897106339\n",
      "    val_loss       : 12238.580337703408\n",
      "    val_log_likelihood: -12150.042677154413\n",
      "    val_log_marginal: -12158.195255925084\n",
      "Train Epoch: 2336 [256/118836 (0%)] Loss: 12201.183594\n",
      "Train Epoch: 2336 [33024/118836 (28%)] Loss: 12358.098633\n",
      "Train Epoch: 2336 [65792/118836 (55%)] Loss: 12269.788086\n",
      "Train Epoch: 2336 [98560/118836 (83%)] Loss: 12318.763672\n",
      "    epoch          : 2336\n",
      "    loss           : 12236.214107895214\n",
      "    val_loss       : 12238.293865245405\n",
      "    val_log_likelihood: -12148.214230187656\n",
      "    val_log_marginal: -12156.441181772947\n",
      "Train Epoch: 2337 [256/118836 (0%)] Loss: 12223.739258\n",
      "Train Epoch: 2337 [33024/118836 (28%)] Loss: 12219.620117\n",
      "Train Epoch: 2337 [65792/118836 (55%)] Loss: 12263.437500\n",
      "Train Epoch: 2337 [98560/118836 (83%)] Loss: 12238.020508\n",
      "    epoch          : 2337\n",
      "    loss           : 12235.59562364299\n",
      "    val_loss       : 12239.321037809927\n",
      "    val_log_likelihood: -12148.679139688016\n",
      "    val_log_marginal: -12156.836422258317\n",
      "Train Epoch: 2338 [256/118836 (0%)] Loss: 12187.779297\n",
      "Train Epoch: 2338 [33024/118836 (28%)] Loss: 12270.700195\n",
      "Train Epoch: 2338 [65792/118836 (55%)] Loss: 12197.298828\n",
      "Train Epoch: 2338 [98560/118836 (83%)] Loss: 12338.378906\n",
      "    epoch          : 2338\n",
      "    loss           : 12237.05528539211\n",
      "    val_loss       : 12234.056553222568\n",
      "    val_log_likelihood: -12147.631448705026\n",
      "    val_log_marginal: -12155.74280496465\n",
      "Train Epoch: 2339 [256/118836 (0%)] Loss: 12300.937500\n",
      "Train Epoch: 2339 [33024/118836 (28%)] Loss: 12189.138672\n",
      "Train Epoch: 2339 [65792/118836 (55%)] Loss: 12253.955078\n",
      "Train Epoch: 2339 [98560/118836 (83%)] Loss: 12313.785156\n",
      "    epoch          : 2339\n",
      "    loss           : 12238.025327620968\n",
      "    val_loss       : 12235.453038895594\n",
      "    val_log_likelihood: -12148.303380408655\n",
      "    val_log_marginal: -12156.471686894722\n",
      "Train Epoch: 2340 [256/118836 (0%)] Loss: 12188.529297\n",
      "Train Epoch: 2340 [33024/118836 (28%)] Loss: 12245.699219\n",
      "Train Epoch: 2340 [65792/118836 (55%)] Loss: 12245.460938\n",
      "Train Epoch: 2340 [98560/118836 (83%)] Loss: 12300.305664\n",
      "    epoch          : 2340\n",
      "    loss           : 12237.005719796836\n",
      "    val_loss       : 12239.564868499945\n",
      "    val_log_likelihood: -12148.969018332558\n",
      "    val_log_marginal: -12157.026710356538\n",
      "Train Epoch: 2341 [256/118836 (0%)] Loss: 12247.017578\n",
      "Train Epoch: 2341 [33024/118836 (28%)] Loss: 12222.491211\n",
      "Train Epoch: 2341 [65792/118836 (55%)] Loss: 12197.856445\n",
      "Train Epoch: 2341 [98560/118836 (83%)] Loss: 12175.714844\n",
      "    epoch          : 2341\n",
      "    loss           : 12233.728154401882\n",
      "    val_loss       : 12236.880753961967\n",
      "    val_log_likelihood: -12149.804789114194\n",
      "    val_log_marginal: -12158.144293048495\n",
      "Train Epoch: 2342 [256/118836 (0%)] Loss: 12323.360352\n",
      "Train Epoch: 2342 [33024/118836 (28%)] Loss: 12270.246094\n",
      "Train Epoch: 2342 [65792/118836 (55%)] Loss: 12321.585938\n",
      "Train Epoch: 2342 [98560/118836 (83%)] Loss: 12234.483398\n",
      "    epoch          : 2342\n",
      "    loss           : 12242.245212662841\n",
      "    val_loss       : 12239.780602730096\n",
      "    val_log_likelihood: -12146.82716733871\n",
      "    val_log_marginal: -12155.081467620763\n",
      "Train Epoch: 2343 [256/118836 (0%)] Loss: 12229.951172\n",
      "Train Epoch: 2343 [33024/118836 (28%)] Loss: 12262.845703\n",
      "Train Epoch: 2343 [65792/118836 (55%)] Loss: 12223.793945\n",
      "Train Epoch: 2343 [98560/118836 (83%)] Loss: 12310.947266\n",
      "    epoch          : 2343\n",
      "    loss           : 12231.1048559954\n",
      "    val_loss       : 12234.843552710947\n",
      "    val_log_likelihood: -12147.451422598737\n",
      "    val_log_marginal: -12155.652926885075\n",
      "Train Epoch: 2344 [256/118836 (0%)] Loss: 12326.816406\n",
      "Train Epoch: 2344 [33024/118836 (28%)] Loss: 12171.945312\n",
      "Train Epoch: 2344 [65792/118836 (55%)] Loss: 12173.218750\n",
      "Train Epoch: 2344 [98560/118836 (83%)] Loss: 12277.351562\n",
      "    epoch          : 2344\n",
      "    loss           : 12236.319963877688\n",
      "    val_loss       : 12236.578195778116\n",
      "    val_log_likelihood: -12148.786404214485\n",
      "    val_log_marginal: -12156.949087106572\n",
      "Train Epoch: 2345 [256/118836 (0%)] Loss: 12163.120117\n",
      "Train Epoch: 2345 [33024/118836 (28%)] Loss: 12252.195312\n",
      "Train Epoch: 2345 [65792/118836 (55%)] Loss: 12235.156250\n",
      "Train Epoch: 2345 [98560/118836 (83%)] Loss: 12201.720703\n",
      "    epoch          : 2345\n",
      "    loss           : 12238.320042390404\n",
      "    val_loss       : 12237.789867704822\n",
      "    val_log_likelihood: -12148.781412518092\n",
      "    val_log_marginal: -12157.001179939776\n",
      "Train Epoch: 2346 [256/118836 (0%)] Loss: 12303.222656\n",
      "Train Epoch: 2346 [33024/118836 (28%)] Loss: 12278.341797\n",
      "Train Epoch: 2346 [65792/118836 (55%)] Loss: 12327.212891\n",
      "Train Epoch: 2346 [98560/118836 (83%)] Loss: 12203.763672\n",
      "    epoch          : 2346\n",
      "    loss           : 12238.870949325372\n",
      "    val_loss       : 12237.414809211105\n",
      "    val_log_likelihood: -12148.7989224695\n",
      "    val_log_marginal: -12157.23486992523\n",
      "Train Epoch: 2347 [256/118836 (0%)] Loss: 12187.347656\n",
      "Train Epoch: 2347 [33024/118836 (28%)] Loss: 12165.077148\n",
      "Train Epoch: 2347 [65792/118836 (55%)] Loss: 12356.734375\n",
      "Train Epoch: 2347 [98560/118836 (83%)] Loss: 12317.404297\n",
      "    epoch          : 2347\n",
      "    loss           : 12238.012464782361\n",
      "    val_loss       : 12239.870769198063\n",
      "    val_log_likelihood: -12150.072770141904\n",
      "    val_log_marginal: -12158.29364804761\n",
      "Train Epoch: 2348 [256/118836 (0%)] Loss: 12217.911133\n",
      "Train Epoch: 2348 [33024/118836 (28%)] Loss: 12259.765625\n",
      "Train Epoch: 2348 [65792/118836 (55%)] Loss: 12345.952148\n",
      "Train Epoch: 2348 [98560/118836 (83%)] Loss: 12235.203125\n",
      "    epoch          : 2348\n",
      "    loss           : 12239.571901332454\n",
      "    val_loss       : 12239.4056754193\n",
      "    val_log_likelihood: -12147.703195273729\n",
      "    val_log_marginal: -12155.800572144346\n",
      "Train Epoch: 2349 [256/118836 (0%)] Loss: 12282.715820\n",
      "Train Epoch: 2349 [33024/118836 (28%)] Loss: 12305.071289\n",
      "Train Epoch: 2349 [65792/118836 (55%)] Loss: 12268.812500\n",
      "Train Epoch: 2349 [98560/118836 (83%)] Loss: 12269.930664\n",
      "    epoch          : 2349\n",
      "    loss           : 12237.938016471515\n",
      "    val_loss       : 12234.130770526617\n",
      "    val_log_likelihood: -12147.601923238471\n",
      "    val_log_marginal: -12155.779747679142\n",
      "Train Epoch: 2350 [256/118836 (0%)] Loss: 12213.070312\n",
      "Train Epoch: 2350 [33024/118836 (28%)] Loss: 12230.697266\n",
      "Train Epoch: 2350 [65792/118836 (55%)] Loss: 12257.697266\n",
      "Train Epoch: 2350 [98560/118836 (83%)] Loss: 12220.015625\n",
      "    epoch          : 2350\n",
      "    loss           : 12237.056282632859\n",
      "    val_loss       : 12239.050679360107\n",
      "    val_log_likelihood: -12147.422099552832\n",
      "    val_log_marginal: -12155.514051362234\n",
      "Train Epoch: 2351 [256/118836 (0%)] Loss: 12226.874023\n",
      "Train Epoch: 2351 [33024/118836 (28%)] Loss: 12270.755859\n",
      "Train Epoch: 2351 [65792/118836 (55%)] Loss: 12276.970703\n",
      "Train Epoch: 2351 [98560/118836 (83%)] Loss: 12325.961914\n",
      "    epoch          : 2351\n",
      "    loss           : 12239.337686427316\n",
      "    val_loss       : 12235.889723203905\n",
      "    val_log_likelihood: -12153.306134492608\n",
      "    val_log_marginal: -12161.541219238878\n",
      "Train Epoch: 2352 [256/118836 (0%)] Loss: 12266.977539\n",
      "Train Epoch: 2352 [33024/118836 (28%)] Loss: 12219.114258\n",
      "Train Epoch: 2352 [65792/118836 (55%)] Loss: 12308.535156\n",
      "Train Epoch: 2352 [98560/118836 (83%)] Loss: 12234.302734\n",
      "    epoch          : 2352\n",
      "    loss           : 12233.772461583696\n",
      "    val_loss       : 12237.28707692699\n",
      "    val_log_likelihood: -12150.536141536135\n",
      "    val_log_marginal: -12158.547807635272\n",
      "Train Epoch: 2353 [256/118836 (0%)] Loss: 12355.433594\n",
      "Train Epoch: 2353 [33024/118836 (28%)] Loss: 12273.119141\n",
      "Train Epoch: 2353 [65792/118836 (55%)] Loss: 12289.070312\n",
      "Train Epoch: 2353 [98560/118836 (83%)] Loss: 12306.607422\n",
      "    epoch          : 2353\n",
      "    loss           : 12237.35409897772\n",
      "    val_loss       : 12236.784171924184\n",
      "    val_log_likelihood: -12147.097873856235\n",
      "    val_log_marginal: -12155.116165299054\n",
      "Train Epoch: 2354 [256/118836 (0%)] Loss: 12409.464844\n",
      "Train Epoch: 2354 [33024/118836 (28%)] Loss: 12313.482422\n",
      "Train Epoch: 2354 [65792/118836 (55%)] Loss: 12266.152344\n",
      "Train Epoch: 2354 [98560/118836 (83%)] Loss: 12282.849609\n",
      "    epoch          : 2354\n",
      "    loss           : 12238.43941516103\n",
      "    val_loss       : 12229.15347886371\n",
      "    val_log_likelihood: -12148.029544044664\n",
      "    val_log_marginal: -12156.032919673575\n",
      "Train Epoch: 2355 [256/118836 (0%)] Loss: 12238.195312\n",
      "Train Epoch: 2355 [33024/118836 (28%)] Loss: 12260.220703\n",
      "Train Epoch: 2355 [65792/118836 (55%)] Loss: 12232.147461\n",
      "Train Epoch: 2355 [98560/118836 (83%)] Loss: 12231.358398\n",
      "    epoch          : 2355\n",
      "    loss           : 12233.768469712832\n",
      "    val_loss       : 12239.71371161351\n",
      "    val_log_likelihood: -12148.176230194118\n",
      "    val_log_marginal: -12156.580008725661\n",
      "Train Epoch: 2356 [256/118836 (0%)] Loss: 12249.039062\n",
      "Train Epoch: 2356 [33024/118836 (28%)] Loss: 12257.782227\n",
      "Train Epoch: 2356 [65792/118836 (55%)] Loss: 12241.508789\n",
      "Train Epoch: 2356 [98560/118836 (83%)] Loss: 12336.362305\n",
      "    epoch          : 2356\n",
      "    loss           : 12240.659678711747\n",
      "    val_loss       : 12237.111870150153\n",
      "    val_log_likelihood: -12146.280329818032\n",
      "    val_log_marginal: -12154.53539585855\n",
      "Train Epoch: 2357 [256/118836 (0%)] Loss: 12177.412109\n",
      "Train Epoch: 2357 [33024/118836 (28%)] Loss: 12266.929688\n",
      "Train Epoch: 2357 [65792/118836 (55%)] Loss: 12260.559570\n",
      "Train Epoch: 2357 [98560/118836 (83%)] Loss: 12240.087891\n",
      "    epoch          : 2357\n",
      "    loss           : 12238.17265075734\n",
      "    val_loss       : 12238.02934012245\n",
      "    val_log_likelihood: -12148.592250019386\n",
      "    val_log_marginal: -12156.760881840006\n",
      "Train Epoch: 2358 [256/118836 (0%)] Loss: 12237.263672\n",
      "Train Epoch: 2358 [33024/118836 (28%)] Loss: 12161.108398\n",
      "Train Epoch: 2358 [65792/118836 (55%)] Loss: 12272.675781\n",
      "Train Epoch: 2358 [98560/118836 (83%)] Loss: 12200.802734\n",
      "    epoch          : 2358\n",
      "    loss           : 12241.94354790245\n",
      "    val_loss       : 12240.935865812939\n",
      "    val_log_likelihood: -12149.494759841553\n",
      "    val_log_marginal: -12157.676135116757\n",
      "Train Epoch: 2359 [256/118836 (0%)] Loss: 12219.968750\n",
      "Train Epoch: 2359 [33024/118836 (28%)] Loss: 12298.583008\n",
      "Train Epoch: 2359 [65792/118836 (55%)] Loss: 12405.680664\n",
      "Train Epoch: 2359 [98560/118836 (83%)] Loss: 12392.168945\n",
      "    epoch          : 2359\n",
      "    loss           : 12235.415665064103\n",
      "    val_loss       : 12237.448427668034\n",
      "    val_log_likelihood: -12148.332691015301\n",
      "    val_log_marginal: -12156.427641722756\n",
      "Train Epoch: 2360 [256/118836 (0%)] Loss: 12204.467773\n",
      "Train Epoch: 2360 [33024/118836 (28%)] Loss: 12222.191406\n",
      "Train Epoch: 2360 [65792/118836 (55%)] Loss: 12151.049805\n",
      "Train Epoch: 2360 [98560/118836 (83%)] Loss: 12235.146484\n",
      "    epoch          : 2360\n",
      "    loss           : 12237.24171739299\n",
      "    val_loss       : 12235.861587010228\n",
      "    val_log_likelihood: -12146.912989815963\n",
      "    val_log_marginal: -12154.98837181343\n",
      "Train Epoch: 2361 [256/118836 (0%)] Loss: 12213.291016\n",
      "Train Epoch: 2361 [33024/118836 (28%)] Loss: 12173.592773\n",
      "Train Epoch: 2361 [65792/118836 (55%)] Loss: 12351.875000\n",
      "Train Epoch: 2361 [98560/118836 (83%)] Loss: 12270.204102\n",
      "    epoch          : 2361\n",
      "    loss           : 12238.08524784817\n",
      "    val_loss       : 12234.187611241614\n",
      "    val_log_likelihood: -12147.859552057485\n",
      "    val_log_marginal: -12155.968489973104\n",
      "Train Epoch: 2362 [256/118836 (0%)] Loss: 12175.287109\n",
      "Train Epoch: 2362 [33024/118836 (28%)] Loss: 12279.657227\n",
      "Train Epoch: 2362 [65792/118836 (55%)] Loss: 12212.859375\n",
      "Train Epoch: 2362 [98560/118836 (83%)] Loss: 12239.940430\n",
      "    epoch          : 2362\n",
      "    loss           : 12237.757481486506\n",
      "    val_loss       : 12237.669015571817\n",
      "    val_log_likelihood: -12149.469978417079\n",
      "    val_log_marginal: -12157.607963032886\n",
      "Train Epoch: 2363 [256/118836 (0%)] Loss: 12297.325195\n",
      "Train Epoch: 2363 [33024/118836 (28%)] Loss: 12205.699219\n",
      "Train Epoch: 2363 [65792/118836 (55%)] Loss: 12186.219727\n",
      "Train Epoch: 2363 [98560/118836 (83%)] Loss: 12182.046875\n",
      "    epoch          : 2363\n",
      "    loss           : 12235.50732478417\n",
      "    val_loss       : 12240.698431957766\n",
      "    val_log_likelihood: -12147.726974934087\n",
      "    val_log_marginal: -12155.755937195407\n",
      "Train Epoch: 2364 [256/118836 (0%)] Loss: 12294.997070\n",
      "Train Epoch: 2364 [33024/118836 (28%)] Loss: 12295.328125\n",
      "Train Epoch: 2364 [65792/118836 (55%)] Loss: 12293.560547\n",
      "Train Epoch: 2364 [98560/118836 (83%)] Loss: 12214.740234\n",
      "    epoch          : 2364\n",
      "    loss           : 12232.78291766827\n",
      "    val_loss       : 12233.670916453506\n",
      "    val_log_likelihood: -12147.79073808416\n",
      "    val_log_marginal: -12155.833929094806\n",
      "Train Epoch: 2365 [256/118836 (0%)] Loss: 12173.366211\n",
      "Train Epoch: 2365 [33024/118836 (28%)] Loss: 12171.496094\n",
      "Train Epoch: 2365 [65792/118836 (55%)] Loss: 12315.308594\n",
      "Train Epoch: 2365 [98560/118836 (83%)] Loss: 12233.852539\n",
      "    epoch          : 2365\n",
      "    loss           : 12234.400999663978\n",
      "    val_loss       : 12237.403788778654\n",
      "    val_log_likelihood: -12146.655679248088\n",
      "    val_log_marginal: -12154.760539747454\n",
      "Train Epoch: 2366 [256/118836 (0%)] Loss: 12253.866211\n",
      "Train Epoch: 2366 [33024/118836 (28%)] Loss: 12243.178711\n",
      "Train Epoch: 2366 [65792/118836 (55%)] Loss: 12273.686523\n",
      "Train Epoch: 2366 [98560/118836 (83%)] Loss: 12277.101562\n",
      "    epoch          : 2366\n",
      "    loss           : 12241.217441777813\n",
      "    val_loss       : 12236.62439508744\n",
      "    val_log_likelihood: -12148.001394166151\n",
      "    val_log_marginal: -12156.154889800702\n",
      "Train Epoch: 2367 [256/118836 (0%)] Loss: 12272.199219\n",
      "Train Epoch: 2367 [33024/118836 (28%)] Loss: 12158.592773\n",
      "Train Epoch: 2367 [65792/118836 (55%)] Loss: 12193.143555\n",
      "Train Epoch: 2367 [98560/118836 (83%)] Loss: 12316.163086\n",
      "    epoch          : 2367\n",
      "    loss           : 12238.814992536447\n",
      "    val_loss       : 12235.24251267326\n",
      "    val_log_likelihood: -12148.369333352719\n",
      "    val_log_marginal: -12156.53368150203\n",
      "Train Epoch: 2368 [256/118836 (0%)] Loss: 12245.275391\n",
      "Train Epoch: 2368 [33024/118836 (28%)] Loss: 12286.077148\n",
      "Train Epoch: 2368 [65792/118836 (55%)] Loss: 12232.931641\n",
      "Train Epoch: 2368 [98560/118836 (83%)] Loss: 12173.125000\n",
      "    epoch          : 2368\n",
      "    loss           : 12238.128775233923\n",
      "    val_loss       : 12236.606512359896\n",
      "    val_log_likelihood: -12146.654182983095\n",
      "    val_log_marginal: -12154.823461699987\n",
      "Train Epoch: 2369 [256/118836 (0%)] Loss: 12274.073242\n",
      "Train Epoch: 2369 [33024/118836 (28%)] Loss: 12268.250000\n",
      "Train Epoch: 2369 [65792/118836 (55%)] Loss: 12273.073242\n",
      "Train Epoch: 2369 [98560/118836 (83%)] Loss: 12222.634766\n",
      "    epoch          : 2369\n",
      "    loss           : 12234.606740300609\n",
      "    val_loss       : 12234.086945472964\n",
      "    val_log_likelihood: -12147.628741631774\n",
      "    val_log_marginal: -12155.68719222678\n",
      "Train Epoch: 2370 [256/118836 (0%)] Loss: 12253.708984\n",
      "Train Epoch: 2370 [33024/118836 (28%)] Loss: 12172.206055\n",
      "Train Epoch: 2370 [65792/118836 (55%)] Loss: 12182.039062\n",
      "Train Epoch: 2370 [98560/118836 (83%)] Loss: 12206.716797\n",
      "    epoch          : 2370\n",
      "    loss           : 12235.8590247622\n",
      "    val_loss       : 12238.60216498698\n",
      "    val_log_likelihood: -12149.146459334937\n",
      "    val_log_marginal: -12157.318463819372\n",
      "Train Epoch: 2371 [256/118836 (0%)] Loss: 12201.376953\n",
      "Train Epoch: 2371 [33024/118836 (28%)] Loss: 12412.816406\n",
      "Train Epoch: 2371 [65792/118836 (55%)] Loss: 12242.917969\n",
      "Train Epoch: 2371 [98560/118836 (83%)] Loss: 12240.857422\n",
      "    epoch          : 2371\n",
      "    loss           : 12235.985404712055\n",
      "    val_loss       : 12238.40911187981\n",
      "    val_log_likelihood: -12147.532270503772\n",
      "    val_log_marginal: -12155.72775875226\n",
      "Train Epoch: 2372 [256/118836 (0%)] Loss: 12237.544922\n",
      "Train Epoch: 2372 [33024/118836 (28%)] Loss: 12298.310547\n",
      "Train Epoch: 2372 [65792/118836 (55%)] Loss: 12262.892578\n",
      "Train Epoch: 2372 [98560/118836 (83%)] Loss: 12203.620117\n",
      "    epoch          : 2372\n",
      "    loss           : 12239.92962837314\n",
      "    val_loss       : 12236.153292989597\n",
      "    val_log_likelihood: -12149.731896679848\n",
      "    val_log_marginal: -12158.02151869993\n",
      "Train Epoch: 2373 [256/118836 (0%)] Loss: 12351.216797\n",
      "Train Epoch: 2373 [33024/118836 (28%)] Loss: 12267.482422\n",
      "Train Epoch: 2373 [65792/118836 (55%)] Loss: 12183.820312\n",
      "Train Epoch: 2373 [98560/118836 (83%)] Loss: 12207.781250\n",
      "    epoch          : 2373\n",
      "    loss           : 12234.279078299473\n",
      "    val_loss       : 12240.617508082347\n",
      "    val_log_likelihood: -12148.898251072684\n",
      "    val_log_marginal: -12157.326696791313\n",
      "Train Epoch: 2374 [256/118836 (0%)] Loss: 12264.568359\n",
      "Train Epoch: 2374 [33024/118836 (28%)] Loss: 12188.560547\n",
      "Train Epoch: 2374 [65792/118836 (55%)] Loss: 12307.410156\n",
      "Train Epoch: 2374 [98560/118836 (83%)] Loss: 12237.150391\n",
      "    epoch          : 2374\n",
      "    loss           : 12242.297375639733\n",
      "    val_loss       : 12235.332510387972\n",
      "    val_log_likelihood: -12149.04166488963\n",
      "    val_log_marginal: -12157.403442186524\n",
      "Train Epoch: 2375 [256/118836 (0%)] Loss: 12448.727539\n",
      "Train Epoch: 2375 [33024/118836 (28%)] Loss: 12395.658203\n",
      "Train Epoch: 2375 [65792/118836 (55%)] Loss: 12272.266602\n",
      "Train Epoch: 2375 [98560/118836 (83%)] Loss: 12186.599609\n",
      "    epoch          : 2375\n",
      "    loss           : 12239.031825436829\n",
      "    val_loss       : 12236.901352707318\n",
      "    val_log_likelihood: -12150.104587501291\n",
      "    val_log_marginal: -12158.33493967929\n",
      "Train Epoch: 2376 [256/118836 (0%)] Loss: 12208.919922\n",
      "Train Epoch: 2376 [33024/118836 (28%)] Loss: 12363.543945\n",
      "Train Epoch: 2376 [65792/118836 (55%)] Loss: 12293.605469\n",
      "Train Epoch: 2376 [98560/118836 (83%)] Loss: 12179.294922\n",
      "    epoch          : 2376\n",
      "    loss           : 12238.487879155036\n",
      "    val_loss       : 12235.53723115934\n",
      "    val_log_likelihood: -12148.808556755324\n",
      "    val_log_marginal: -12157.18311861535\n",
      "Train Epoch: 2377 [256/118836 (0%)] Loss: 12278.127930\n",
      "Train Epoch: 2377 [33024/118836 (28%)] Loss: 12180.206055\n",
      "Train Epoch: 2377 [65792/118836 (55%)] Loss: 12274.669922\n",
      "Train Epoch: 2377 [98560/118836 (83%)] Loss: 12352.684570\n",
      "    epoch          : 2377\n",
      "    loss           : 12237.030471980976\n",
      "    val_loss       : 12235.38476559809\n",
      "    val_log_likelihood: -12147.463179151157\n",
      "    val_log_marginal: -12155.633372264543\n",
      "Train Epoch: 2378 [256/118836 (0%)] Loss: 12245.008789\n",
      "Train Epoch: 2378 [33024/118836 (28%)] Loss: 12289.929688\n",
      "Train Epoch: 2378 [65792/118836 (55%)] Loss: 12289.486328\n",
      "Train Epoch: 2378 [98560/118836 (83%)] Loss: 12261.030273\n",
      "    epoch          : 2378\n",
      "    loss           : 12239.825378831936\n",
      "    val_loss       : 12241.187727728184\n",
      "    val_log_likelihood: -12152.717249050094\n",
      "    val_log_marginal: -12161.207386228522\n",
      "Train Epoch: 2379 [256/118836 (0%)] Loss: 12235.806641\n",
      "Train Epoch: 2379 [33024/118836 (28%)] Loss: 12263.551758\n",
      "Train Epoch: 2379 [65792/118836 (55%)] Loss: 12316.369141\n",
      "Train Epoch: 2379 [98560/118836 (83%)] Loss: 12286.218750\n",
      "    epoch          : 2379\n",
      "    loss           : 12240.782082299422\n",
      "    val_loss       : 12236.078223227902\n",
      "    val_log_likelihood: -12148.078531618332\n",
      "    val_log_marginal: -12156.162680038988\n",
      "Train Epoch: 2380 [256/118836 (0%)] Loss: 12232.060547\n",
      "Train Epoch: 2380 [33024/118836 (28%)] Loss: 12329.941406\n",
      "Train Epoch: 2380 [65792/118836 (55%)] Loss: 12314.088867\n",
      "Train Epoch: 2380 [98560/118836 (83%)] Loss: 12239.905273\n",
      "    epoch          : 2380\n",
      "    loss           : 12233.790046493745\n",
      "    val_loss       : 12239.04884652133\n",
      "    val_log_likelihood: -12148.914928240023\n",
      "    val_log_marginal: -12157.231341933651\n",
      "Train Epoch: 2381 [256/118836 (0%)] Loss: 12303.367188\n",
      "Train Epoch: 2381 [33024/118836 (28%)] Loss: 12251.236328\n",
      "Train Epoch: 2381 [65792/118836 (55%)] Loss: 12254.474609\n",
      "Train Epoch: 2381 [98560/118836 (83%)] Loss: 12249.236328\n",
      "    epoch          : 2381\n",
      "    loss           : 12239.089971050456\n",
      "    val_loss       : 12237.328283698633\n",
      "    val_log_likelihood: -12147.611797262716\n",
      "    val_log_marginal: -12155.93794546004\n",
      "Train Epoch: 2382 [256/118836 (0%)] Loss: 12182.777344\n",
      "Train Epoch: 2382 [33024/118836 (28%)] Loss: 12182.621094\n",
      "Train Epoch: 2382 [65792/118836 (55%)] Loss: 12232.529297\n",
      "Train Epoch: 2382 [98560/118836 (83%)] Loss: 12235.873047\n",
      "    epoch          : 2382\n",
      "    loss           : 12233.047244300558\n",
      "    val_loss       : 12236.249385427587\n",
      "    val_log_likelihood: -12147.464549084987\n",
      "    val_log_marginal: -12155.68172122966\n",
      "Train Epoch: 2383 [256/118836 (0%)] Loss: 12181.670898\n",
      "Train Epoch: 2383 [33024/118836 (28%)] Loss: 12265.141602\n",
      "Train Epoch: 2383 [65792/118836 (55%)] Loss: 12314.865234\n",
      "Train Epoch: 2383 [98560/118836 (83%)] Loss: 12291.507812\n",
      "    epoch          : 2383\n",
      "    loss           : 12237.656618815912\n",
      "    val_loss       : 12237.082913381575\n",
      "    val_log_likelihood: -12148.01644518326\n",
      "    val_log_marginal: -12156.147061138296\n",
      "Train Epoch: 2384 [256/118836 (0%)] Loss: 12205.471680\n",
      "Train Epoch: 2384 [33024/118836 (28%)] Loss: 12184.874023\n",
      "Train Epoch: 2384 [65792/118836 (55%)] Loss: 12232.411133\n",
      "Train Epoch: 2384 [98560/118836 (83%)] Loss: 12194.892578\n",
      "    epoch          : 2384\n",
      "    loss           : 12237.333482765973\n",
      "    val_loss       : 12231.945815565814\n",
      "    val_log_likelihood: -12146.688521149968\n",
      "    val_log_marginal: -12154.82188208643\n",
      "Train Epoch: 2385 [256/118836 (0%)] Loss: 12252.149414\n",
      "Train Epoch: 2385 [33024/118836 (28%)] Loss: 12224.601562\n",
      "Train Epoch: 2385 [65792/118836 (55%)] Loss: 12315.353516\n",
      "Train Epoch: 2385 [98560/118836 (83%)] Loss: 12270.022461\n",
      "    epoch          : 2385\n",
      "    loss           : 12236.072468045646\n",
      "    val_loss       : 12238.647324445745\n",
      "    val_log_likelihood: -12147.616183797301\n",
      "    val_log_marginal: -12155.759431853608\n",
      "Train Epoch: 2386 [256/118836 (0%)] Loss: 12231.443359\n",
      "Train Epoch: 2386 [33024/118836 (28%)] Loss: 12185.622070\n",
      "Train Epoch: 2386 [65792/118836 (55%)] Loss: 12286.623047\n",
      "Train Epoch: 2386 [98560/118836 (83%)] Loss: 12250.952148\n",
      "    epoch          : 2386\n",
      "    loss           : 12229.357586816326\n",
      "    val_loss       : 12236.28727636209\n",
      "    val_log_likelihood: -12147.494129639681\n",
      "    val_log_marginal: -12155.662066344326\n",
      "Train Epoch: 2387 [256/118836 (0%)] Loss: 12202.027344\n",
      "Train Epoch: 2387 [33024/118836 (28%)] Loss: 12302.265625\n",
      "Train Epoch: 2387 [65792/118836 (55%)] Loss: 12248.738281\n",
      "Train Epoch: 2387 [98560/118836 (83%)] Loss: 12222.160156\n",
      "    epoch          : 2387\n",
      "    loss           : 12232.841591546474\n",
      "    val_loss       : 12236.002281853085\n",
      "    val_log_likelihood: -12149.09397374509\n",
      "    val_log_marginal: -12157.181006082528\n",
      "Train Epoch: 2388 [256/118836 (0%)] Loss: 12250.116211\n",
      "Train Epoch: 2388 [33024/118836 (28%)] Loss: 12194.602539\n",
      "Train Epoch: 2388 [65792/118836 (55%)] Loss: 12268.057617\n",
      "Train Epoch: 2388 [98560/118836 (83%)] Loss: 12338.460938\n",
      "    epoch          : 2388\n",
      "    loss           : 12235.542912692566\n",
      "    val_loss       : 12237.037388547787\n",
      "    val_log_likelihood: -12147.25177655216\n",
      "    val_log_marginal: -12155.588797218614\n",
      "Train Epoch: 2389 [256/118836 (0%)] Loss: 12289.982422\n",
      "Train Epoch: 2389 [33024/118836 (28%)] Loss: 12287.708984\n",
      "Train Epoch: 2389 [65792/118836 (55%)] Loss: 12348.322266\n",
      "Train Epoch: 2389 [98560/118836 (83%)] Loss: 12278.023438\n",
      "    epoch          : 2389\n",
      "    loss           : 12234.138326483664\n",
      "    val_loss       : 12241.125143762174\n",
      "    val_log_likelihood: -12147.658117665685\n",
      "    val_log_marginal: -12155.715910112465\n",
      "Train Epoch: 2390 [256/118836 (0%)] Loss: 12238.079102\n",
      "Train Epoch: 2390 [33024/118836 (28%)] Loss: 12395.127930\n",
      "Train Epoch: 2390 [65792/118836 (55%)] Loss: 12225.550781\n",
      "Train Epoch: 2390 [98560/118836 (83%)] Loss: 12369.940430\n",
      "    epoch          : 2390\n",
      "    loss           : 12231.422903258117\n",
      "    val_loss       : 12241.06041348305\n",
      "    val_log_likelihood: -12148.043271654002\n",
      "    val_log_marginal: -12156.43262813935\n",
      "Train Epoch: 2391 [256/118836 (0%)] Loss: 12390.355469\n",
      "Train Epoch: 2391 [33024/118836 (28%)] Loss: 12279.211914\n",
      "Train Epoch: 2391 [65792/118836 (55%)] Loss: 12203.130859\n",
      "Train Epoch: 2391 [98560/118836 (83%)] Loss: 12338.071289\n",
      "    epoch          : 2391\n",
      "    loss           : 12237.873163997881\n",
      "    val_loss       : 12241.900685869403\n",
      "    val_log_likelihood: -12154.165679765045\n",
      "    val_log_marginal: -12162.438211844212\n",
      "Train Epoch: 2392 [256/118836 (0%)] Loss: 12204.888672\n",
      "Train Epoch: 2392 [33024/118836 (28%)] Loss: 12277.781250\n",
      "Train Epoch: 2392 [65792/118836 (55%)] Loss: 12153.627930\n",
      "Train Epoch: 2392 [98560/118836 (83%)] Loss: 12157.506836\n",
      "    epoch          : 2392\n",
      "    loss           : 12239.86688718078\n",
      "    val_loss       : 12240.097237210884\n",
      "    val_log_likelihood: -12148.106117529984\n",
      "    val_log_marginal: -12156.158594109744\n",
      "Train Epoch: 2393 [256/118836 (0%)] Loss: 12307.253906\n",
      "Train Epoch: 2393 [33024/118836 (28%)] Loss: 12420.324219\n",
      "Train Epoch: 2393 [65792/118836 (55%)] Loss: 12219.348633\n",
      "Train Epoch: 2393 [98560/118836 (83%)] Loss: 12330.946289\n",
      "    epoch          : 2393\n",
      "    loss           : 12238.560593077957\n",
      "    val_loss       : 12238.561127338122\n",
      "    val_log_likelihood: -12147.96618282801\n",
      "    val_log_marginal: -12156.04602180939\n",
      "Train Epoch: 2394 [256/118836 (0%)] Loss: 12392.734375\n",
      "Train Epoch: 2394 [33024/118836 (28%)] Loss: 12262.085938\n",
      "Train Epoch: 2394 [65792/118836 (55%)] Loss: 12290.897461\n",
      "Train Epoch: 2394 [98560/118836 (83%)] Loss: 12266.683594\n",
      "    epoch          : 2394\n",
      "    loss           : 12235.486932640613\n",
      "    val_loss       : 12242.071648580317\n",
      "    val_log_likelihood: -12150.587721806503\n",
      "    val_log_marginal: -12158.777876700082\n",
      "Train Epoch: 2395 [256/118836 (0%)] Loss: 12178.902344\n",
      "Train Epoch: 2395 [33024/118836 (28%)] Loss: 12226.569336\n",
      "Train Epoch: 2395 [65792/118836 (55%)] Loss: 12212.716797\n",
      "Train Epoch: 2395 [98560/118836 (83%)] Loss: 12249.987305\n",
      "    epoch          : 2395\n",
      "    loss           : 12235.311800816791\n",
      "    val_loss       : 12237.740912329955\n",
      "    val_log_likelihood: -12148.267844034326\n",
      "    val_log_marginal: -12156.489821976984\n",
      "Train Epoch: 2396 [256/118836 (0%)] Loss: 12323.411133\n",
      "Train Epoch: 2396 [33024/118836 (28%)] Loss: 12231.444336\n",
      "Train Epoch: 2396 [65792/118836 (55%)] Loss: 12312.376953\n",
      "Train Epoch: 2396 [98560/118836 (83%)] Loss: 12211.174805\n",
      "    epoch          : 2396\n",
      "    loss           : 12233.771719105407\n",
      "    val_loss       : 12233.774104195654\n",
      "    val_log_likelihood: -12146.63366240824\n",
      "    val_log_marginal: -12154.794959358076\n",
      "Train Epoch: 2397 [256/118836 (0%)] Loss: 12393.000000\n",
      "Train Epoch: 2397 [33024/118836 (28%)] Loss: 12222.958984\n",
      "Train Epoch: 2397 [65792/118836 (55%)] Loss: 12362.625977\n",
      "Train Epoch: 2397 [98560/118836 (83%)] Loss: 12265.669922\n",
      "    epoch          : 2397\n",
      "    loss           : 12242.561719396195\n",
      "    val_loss       : 12240.13542777667\n",
      "    val_log_likelihood: -12148.51061811802\n",
      "    val_log_marginal: -12156.505846290762\n",
      "Train Epoch: 2398 [256/118836 (0%)] Loss: 12201.412109\n",
      "Train Epoch: 2398 [33024/118836 (28%)] Loss: 12429.888672\n",
      "Train Epoch: 2398 [65792/118836 (55%)] Loss: 12192.315430\n",
      "Train Epoch: 2398 [98560/118836 (83%)] Loss: 12292.470703\n",
      "    epoch          : 2398\n",
      "    loss           : 12241.479956155656\n",
      "    val_loss       : 12237.435951149371\n",
      "    val_log_likelihood: -12147.931940621123\n",
      "    val_log_marginal: -12156.204754611721\n",
      "Train Epoch: 2399 [256/118836 (0%)] Loss: 12352.512695\n",
      "Train Epoch: 2399 [33024/118836 (28%)] Loss: 12243.457031\n",
      "Train Epoch: 2399 [65792/118836 (55%)] Loss: 12282.584961\n",
      "Train Epoch: 2399 [98560/118836 (83%)] Loss: 12257.510742\n",
      "    epoch          : 2399\n",
      "    loss           : 12239.589042791047\n",
      "    val_loss       : 12238.90767982782\n",
      "    val_log_likelihood: -12148.615691881205\n",
      "    val_log_marginal: -12156.878936874738\n",
      "Train Epoch: 2400 [256/118836 (0%)] Loss: 12443.713867\n",
      "Train Epoch: 2400 [33024/118836 (28%)] Loss: 12239.669922\n",
      "Train Epoch: 2400 [65792/118836 (55%)] Loss: 12345.941406\n",
      "Train Epoch: 2400 [98560/118836 (83%)] Loss: 12338.372070\n",
      "    epoch          : 2400\n",
      "    loss           : 12240.852885100032\n",
      "    val_loss       : 12240.217480369429\n",
      "    val_log_likelihood: -12146.953506901365\n",
      "    val_log_marginal: -12155.0804255035\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch2400.pth ...\n",
      "Train Epoch: 2401 [256/118836 (0%)] Loss: 12286.620117\n",
      "Train Epoch: 2401 [33024/118836 (28%)] Loss: 12313.035156\n",
      "Train Epoch: 2401 [65792/118836 (55%)] Loss: 12188.551758\n",
      "Train Epoch: 2401 [98560/118836 (83%)] Loss: 12193.038086\n",
      "    epoch          : 2401\n",
      "    loss           : 12235.392051960556\n",
      "    val_loss       : 12234.367164622668\n",
      "    val_log_likelihood: -12149.213724863006\n",
      "    val_log_marginal: -12157.347731613942\n",
      "Train Epoch: 2402 [256/118836 (0%)] Loss: 12210.714844\n",
      "Train Epoch: 2402 [33024/118836 (28%)] Loss: 12253.904297\n",
      "Train Epoch: 2402 [65792/118836 (55%)] Loss: 12308.897461\n",
      "Train Epoch: 2402 [98560/118836 (83%)] Loss: 12285.603516\n",
      "    epoch          : 2402\n",
      "    loss           : 12233.955013020834\n",
      "    val_loss       : 12234.652228477853\n",
      "    val_log_likelihood: -12147.996660301644\n",
      "    val_log_marginal: -12156.368280614648\n",
      "Train Epoch: 2403 [256/118836 (0%)] Loss: 12264.802734\n",
      "Train Epoch: 2403 [33024/118836 (28%)] Loss: 12188.944336\n",
      "Train Epoch: 2403 [65792/118836 (55%)] Loss: 12201.261719\n",
      "Train Epoch: 2403 [98560/118836 (83%)] Loss: 12340.063477\n",
      "    epoch          : 2403\n",
      "    loss           : 12241.09832570823\n",
      "    val_loss       : 12236.09573789177\n",
      "    val_log_likelihood: -12147.446534778226\n",
      "    val_log_marginal: -12155.618302251429\n",
      "Train Epoch: 2404 [256/118836 (0%)] Loss: 12255.461914\n",
      "Train Epoch: 2404 [33024/118836 (28%)] Loss: 12191.603516\n",
      "Train Epoch: 2404 [65792/118836 (55%)] Loss: 12207.702148\n",
      "Train Epoch: 2404 [98560/118836 (83%)] Loss: 12436.406250\n",
      "    epoch          : 2404\n",
      "    loss           : 12238.034189218879\n",
      "    val_loss       : 12238.295748049764\n",
      "    val_log_likelihood: -12147.161177076872\n",
      "    val_log_marginal: -12155.35018197659\n",
      "Train Epoch: 2405 [256/118836 (0%)] Loss: 12298.200195\n",
      "Train Epoch: 2405 [33024/118836 (28%)] Loss: 12194.940430\n",
      "Train Epoch: 2405 [65792/118836 (55%)] Loss: 12270.166016\n",
      "Train Epoch: 2405 [98560/118836 (83%)] Loss: 12274.201172\n",
      "    epoch          : 2405\n",
      "    loss           : 12234.430543062448\n",
      "    val_loss       : 12235.20597183742\n",
      "    val_log_likelihood: -12147.298367226273\n",
      "    val_log_marginal: -12155.467800486038\n",
      "Train Epoch: 2406 [256/118836 (0%)] Loss: 12317.636719\n",
      "Train Epoch: 2406 [33024/118836 (28%)] Loss: 12154.909180\n",
      "Train Epoch: 2406 [65792/118836 (55%)] Loss: 12258.690430\n",
      "Train Epoch: 2406 [98560/118836 (83%)] Loss: 12330.614258\n",
      "    epoch          : 2406\n",
      "    loss           : 12236.041102215157\n",
      "    val_loss       : 12238.467540005304\n",
      "    val_log_likelihood: -12150.721520077284\n",
      "    val_log_marginal: -12158.821046059411\n",
      "Train Epoch: 2407 [256/118836 (0%)] Loss: 12212.025391\n",
      "Train Epoch: 2407 [33024/118836 (28%)] Loss: 12197.597656\n",
      "Train Epoch: 2407 [65792/118836 (55%)] Loss: 12281.615234\n",
      "Train Epoch: 2407 [98560/118836 (83%)] Loss: 12213.621094\n",
      "    epoch          : 2407\n",
      "    loss           : 12236.126079630636\n",
      "    val_loss       : 12231.768263915308\n",
      "    val_log_likelihood: -12147.98843197503\n",
      "    val_log_marginal: -12156.083323113227\n",
      "Train Epoch: 2408 [256/118836 (0%)] Loss: 12225.419922\n",
      "Train Epoch: 2408 [33024/118836 (28%)] Loss: 12327.869141\n",
      "Train Epoch: 2408 [65792/118836 (55%)] Loss: 12155.337891\n",
      "Train Epoch: 2408 [98560/118836 (83%)] Loss: 12185.447266\n",
      "    epoch          : 2408\n",
      "    loss           : 12234.231106221569\n",
      "    val_loss       : 12235.469519343904\n",
      "    val_log_likelihood: -12149.965535178866\n",
      "    val_log_marginal: -12158.16005427607\n",
      "Train Epoch: 2409 [256/118836 (0%)] Loss: 12166.376953\n",
      "Train Epoch: 2409 [33024/118836 (28%)] Loss: 12356.173828\n",
      "Train Epoch: 2409 [65792/118836 (55%)] Loss: 12295.051758\n",
      "Train Epoch: 2409 [98560/118836 (83%)] Loss: 12357.153320\n",
      "    epoch          : 2409\n",
      "    loss           : 12238.662306787635\n",
      "    val_loss       : 12238.445085528489\n",
      "    val_log_likelihood: -12146.83262639578\n",
      "    val_log_marginal: -12154.956917887037\n",
      "Train Epoch: 2410 [256/118836 (0%)] Loss: 12343.474609\n",
      "Train Epoch: 2410 [33024/118836 (28%)] Loss: 12190.820312\n",
      "Train Epoch: 2410 [65792/118836 (55%)] Loss: 12333.724609\n",
      "Train Epoch: 2410 [98560/118836 (83%)] Loss: 12340.566406\n",
      "    epoch          : 2410\n",
      "    loss           : 12235.802395768715\n",
      "    val_loss       : 12233.726994542985\n",
      "    val_log_likelihood: -12147.883445125104\n",
      "    val_log_marginal: -12156.101399122808\n",
      "Train Epoch: 2411 [256/118836 (0%)] Loss: 12302.229492\n",
      "Train Epoch: 2411 [33024/118836 (28%)] Loss: 12302.414062\n",
      "Train Epoch: 2411 [65792/118836 (55%)] Loss: 12170.287109\n",
      "Train Epoch: 2411 [98560/118836 (83%)] Loss: 12286.878906\n",
      "    epoch          : 2411\n",
      "    loss           : 12232.75812768171\n",
      "    val_loss       : 12236.253095162165\n",
      "    val_log_likelihood: -12147.775271725082\n",
      "    val_log_marginal: -12155.886512494972\n",
      "Train Epoch: 2412 [256/118836 (0%)] Loss: 12274.984375\n",
      "Train Epoch: 2412 [33024/118836 (28%)] Loss: 12278.284180\n",
      "Train Epoch: 2412 [65792/118836 (55%)] Loss: 12237.559570\n",
      "Train Epoch: 2412 [98560/118836 (83%)] Loss: 12142.516602\n",
      "    epoch          : 2412\n",
      "    loss           : 12236.77806151132\n",
      "    val_loss       : 12234.934943541148\n",
      "    val_log_likelihood: -12147.72124334419\n",
      "    val_log_marginal: -12155.99109565022\n",
      "Train Epoch: 2413 [256/118836 (0%)] Loss: 12294.404297\n",
      "Train Epoch: 2413 [33024/118836 (28%)] Loss: 12209.972656\n",
      "Train Epoch: 2413 [65792/118836 (55%)] Loss: 12164.693359\n",
      "Train Epoch: 2413 [98560/118836 (83%)] Loss: 12277.583984\n",
      "    epoch          : 2413\n",
      "    loss           : 12235.426406282311\n",
      "    val_loss       : 12231.874680972935\n",
      "    val_log_likelihood: -12146.000285133634\n",
      "    val_log_marginal: -12154.109429676506\n",
      "Train Epoch: 2414 [256/118836 (0%)] Loss: 12286.965820\n",
      "Train Epoch: 2414 [33024/118836 (28%)] Loss: 12396.000000\n",
      "Train Epoch: 2414 [65792/118836 (55%)] Loss: 12310.560547\n",
      "Train Epoch: 2414 [98560/118836 (83%)] Loss: 12306.580078\n",
      "    epoch          : 2414\n",
      "    loss           : 12237.369179235162\n",
      "    val_loss       : 12236.6654075718\n",
      "    val_log_likelihood: -12148.061772707299\n",
      "    val_log_marginal: -12156.130880913897\n",
      "Train Epoch: 2415 [256/118836 (0%)] Loss: 12205.787109\n",
      "Train Epoch: 2415 [33024/118836 (28%)] Loss: 12191.642578\n",
      "Train Epoch: 2415 [65792/118836 (55%)] Loss: 12274.264648\n",
      "Train Epoch: 2415 [98560/118836 (83%)] Loss: 12251.700195\n",
      "    epoch          : 2415\n",
      "    loss           : 12236.705256474876\n",
      "    val_loss       : 12236.617160554075\n",
      "    val_log_likelihood: -12148.13306580852\n",
      "    val_log_marginal: -12156.289787560772\n",
      "Train Epoch: 2416 [256/118836 (0%)] Loss: 12309.005859\n",
      "Train Epoch: 2416 [33024/118836 (28%)] Loss: 12155.531250\n",
      "Train Epoch: 2416 [65792/118836 (55%)] Loss: 12313.940430\n",
      "Train Epoch: 2416 [98560/118836 (83%)] Loss: 12229.796875\n",
      "    epoch          : 2416\n",
      "    loss           : 12233.748894683105\n",
      "    val_loss       : 12236.397181471526\n",
      "    val_log_likelihood: -12151.260792429177\n",
      "    val_log_marginal: -12159.415597882147\n",
      "Train Epoch: 2417 [256/118836 (0%)] Loss: 12311.767578\n",
      "Train Epoch: 2417 [33024/118836 (28%)] Loss: 12405.386719\n",
      "Train Epoch: 2417 [65792/118836 (55%)] Loss: 12221.177734\n",
      "Train Epoch: 2417 [98560/118836 (83%)] Loss: 12278.313477\n",
      "    epoch          : 2417\n",
      "    loss           : 12237.131774548956\n",
      "    val_loss       : 12246.414944577687\n",
      "    val_log_likelihood: -12145.547038164288\n",
      "    val_log_marginal: -12153.79488472956\n",
      "Train Epoch: 2418 [256/118836 (0%)] Loss: 12322.689453\n",
      "Train Epoch: 2418 [33024/118836 (28%)] Loss: 12272.188477\n",
      "Train Epoch: 2418 [65792/118836 (55%)] Loss: 12265.990234\n",
      "Train Epoch: 2418 [98560/118836 (83%)] Loss: 12176.876953\n",
      "    epoch          : 2418\n",
      "    loss           : 12233.5140962637\n",
      "    val_loss       : 12236.61280852452\n",
      "    val_log_likelihood: -12146.35563288358\n",
      "    val_log_marginal: -12154.611096249118\n",
      "Train Epoch: 2419 [256/118836 (0%)] Loss: 12276.775391\n",
      "Train Epoch: 2419 [33024/118836 (28%)] Loss: 12263.505859\n",
      "Train Epoch: 2419 [65792/118836 (55%)] Loss: 12225.928711\n",
      "Train Epoch: 2419 [98560/118836 (83%)] Loss: 12314.334961\n",
      "    epoch          : 2419\n",
      "    loss           : 12234.199274322787\n",
      "    val_loss       : 12235.846184167945\n",
      "    val_log_likelihood: -12147.870275666874\n",
      "    val_log_marginal: -12156.163623925555\n",
      "Train Epoch: 2420 [256/118836 (0%)] Loss: 12197.956055\n",
      "Train Epoch: 2420 [33024/118836 (28%)] Loss: 12212.962891\n",
      "Train Epoch: 2420 [65792/118836 (55%)] Loss: 12246.310547\n",
      "Train Epoch: 2420 [98560/118836 (83%)] Loss: 12259.670898\n",
      "    epoch          : 2420\n",
      "    loss           : 12237.716733870968\n",
      "    val_loss       : 12246.83354799816\n",
      "    val_log_likelihood: -12149.503001576717\n",
      "    val_log_marginal: -12157.972280911592\n",
      "Train Epoch: 2421 [256/118836 (0%)] Loss: 12263.202148\n",
      "Train Epoch: 2421 [33024/118836 (28%)] Loss: 12173.128906\n",
      "Train Epoch: 2421 [65792/118836 (55%)] Loss: 12252.229492\n",
      "Train Epoch: 2421 [98560/118836 (83%)] Loss: 12223.168945\n",
      "    epoch          : 2421\n",
      "    loss           : 12237.276432614763\n",
      "    val_loss       : 12237.68721797508\n",
      "    val_log_likelihood: -12147.578521279207\n",
      "    val_log_marginal: -12155.934628890385\n",
      "Train Epoch: 2422 [256/118836 (0%)] Loss: 12299.586914\n",
      "Train Epoch: 2422 [33024/118836 (28%)] Loss: 12344.430664\n",
      "Train Epoch: 2422 [65792/118836 (55%)] Loss: 12353.575195\n",
      "Train Epoch: 2422 [98560/118836 (83%)] Loss: 12358.437500\n",
      "    epoch          : 2422\n",
      "    loss           : 12238.314535353338\n",
      "    val_loss       : 12236.29253029402\n",
      "    val_log_likelihood: -12149.1266435975\n",
      "    val_log_marginal: -12157.176466740984\n",
      "Train Epoch: 2423 [256/118836 (0%)] Loss: 12228.232422\n",
      "Train Epoch: 2423 [33024/118836 (28%)] Loss: 12230.325195\n",
      "Train Epoch: 2423 [65792/118836 (55%)] Loss: 12215.283203\n",
      "Train Epoch: 2423 [98560/118836 (83%)] Loss: 12279.921875\n",
      "    epoch          : 2423\n",
      "    loss           : 12235.373252849722\n",
      "    val_loss       : 12236.846396679497\n",
      "    val_log_likelihood: -12148.268555979892\n",
      "    val_log_marginal: -12156.282262043025\n",
      "Train Epoch: 2424 [256/118836 (0%)] Loss: 12247.992188\n",
      "Train Epoch: 2424 [33024/118836 (28%)] Loss: 12245.675781\n",
      "Train Epoch: 2424 [65792/118836 (55%)] Loss: 12204.210938\n",
      "Train Epoch: 2424 [98560/118836 (83%)] Loss: 12180.634766\n",
      "    epoch          : 2424\n",
      "    loss           : 12231.861112941999\n",
      "    val_loss       : 12236.107391722739\n",
      "    val_log_likelihood: -12147.883079701716\n",
      "    val_log_marginal: -12156.031220285124\n",
      "Train Epoch: 2425 [256/118836 (0%)] Loss: 12301.585938\n",
      "Train Epoch: 2425 [33024/118836 (28%)] Loss: 12270.849609\n",
      "Train Epoch: 2425 [65792/118836 (55%)] Loss: 12214.515625\n",
      "Train Epoch: 2425 [98560/118836 (83%)] Loss: 12295.871094\n",
      "    epoch          : 2425\n",
      "    loss           : 12236.179945331885\n",
      "    val_loss       : 12233.361664158398\n",
      "    val_log_likelihood: -12148.729969402657\n",
      "    val_log_marginal: -12156.807781268079\n",
      "Train Epoch: 2426 [256/118836 (0%)] Loss: 12274.487305\n",
      "Train Epoch: 2426 [33024/118836 (28%)] Loss: 12316.554688\n",
      "Train Epoch: 2426 [65792/118836 (55%)] Loss: 12401.326172\n",
      "Train Epoch: 2426 [98560/118836 (83%)] Loss: 12338.884766\n",
      "    epoch          : 2426\n",
      "    loss           : 12237.683193108975\n",
      "    val_loss       : 12237.178636062334\n",
      "    val_log_likelihood: -12148.426543437241\n",
      "    val_log_marginal: -12156.498111687173\n",
      "Train Epoch: 2427 [256/118836 (0%)] Loss: 12268.021484\n",
      "Train Epoch: 2427 [33024/118836 (28%)] Loss: 12331.904297\n",
      "Train Epoch: 2427 [65792/118836 (55%)] Loss: 12347.570312\n",
      "Train Epoch: 2427 [98560/118836 (83%)] Loss: 12271.162109\n",
      "    epoch          : 2427\n",
      "    loss           : 12237.34825139578\n",
      "    val_loss       : 12238.106624237455\n",
      "    val_log_likelihood: -12147.469925590623\n",
      "    val_log_marginal: -12155.536539303022\n",
      "Train Epoch: 2428 [256/118836 (0%)] Loss: 12404.240234\n",
      "Train Epoch: 2428 [33024/118836 (28%)] Loss: 12277.129883\n",
      "Train Epoch: 2428 [65792/118836 (55%)] Loss: 12275.538086\n",
      "Train Epoch: 2428 [98560/118836 (83%)] Loss: 12245.934570\n",
      "    epoch          : 2428\n",
      "    loss           : 12238.511642175868\n",
      "    val_loss       : 12255.160457236072\n",
      "    val_log_likelihood: -12154.484075326975\n",
      "    val_log_marginal: -12163.02819416316\n",
      "Train Epoch: 2429 [256/118836 (0%)] Loss: 12305.815430\n",
      "Train Epoch: 2429 [33024/118836 (28%)] Loss: 12211.911133\n",
      "Train Epoch: 2429 [65792/118836 (55%)] Loss: 12384.120117\n",
      "Train Epoch: 2429 [98560/118836 (83%)] Loss: 12211.562500\n",
      "    epoch          : 2429\n",
      "    loss           : 12235.982385526519\n",
      "    val_loss       : 12236.36520705309\n",
      "    val_log_likelihood: -12150.602725651366\n",
      "    val_log_marginal: -12159.024155617439\n",
      "Train Epoch: 2430 [256/118836 (0%)] Loss: 12226.224609\n",
      "Train Epoch: 2430 [33024/118836 (28%)] Loss: 12182.918945\n",
      "Train Epoch: 2430 [65792/118836 (55%)] Loss: 12401.601562\n",
      "Train Epoch: 2430 [98560/118836 (83%)] Loss: 12231.248047\n",
      "    epoch          : 2430\n",
      "    loss           : 12237.91833239635\n",
      "    val_loss       : 12233.903944395475\n",
      "    val_log_likelihood: -12148.16668217535\n",
      "    val_log_marginal: -12156.630473196597\n",
      "Train Epoch: 2431 [256/118836 (0%)] Loss: 12204.922852\n",
      "Train Epoch: 2431 [33024/118836 (28%)] Loss: 12360.862305\n",
      "Train Epoch: 2431 [65792/118836 (55%)] Loss: 12287.898438\n",
      "Train Epoch: 2431 [98560/118836 (83%)] Loss: 12205.679688\n",
      "    epoch          : 2431\n",
      "    loss           : 12236.47379048413\n",
      "    val_loss       : 12236.78055514929\n",
      "    val_log_likelihood: -12149.660070629136\n",
      "    val_log_marginal: -12157.953570693826\n",
      "Train Epoch: 2432 [256/118836 (0%)] Loss: 12327.826172\n",
      "Train Epoch: 2432 [33024/118836 (28%)] Loss: 12316.697266\n",
      "Train Epoch: 2432 [65792/118836 (55%)] Loss: 12265.621094\n",
      "Train Epoch: 2432 [98560/118836 (83%)] Loss: 12164.795898\n",
      "    epoch          : 2432\n",
      "    loss           : 12233.80698278536\n",
      "    val_loss       : 12239.885712039331\n",
      "    val_log_likelihood: -12147.209471121536\n",
      "    val_log_marginal: -12155.48947448353\n",
      "Train Epoch: 2433 [256/118836 (0%)] Loss: 12170.256836\n",
      "Train Epoch: 2433 [33024/118836 (28%)] Loss: 12244.404297\n",
      "Train Epoch: 2433 [65792/118836 (55%)] Loss: 12228.225586\n",
      "Train Epoch: 2433 [98560/118836 (83%)] Loss: 12353.133789\n",
      "    epoch          : 2433\n",
      "    loss           : 12240.457974533447\n",
      "    val_loss       : 12236.034417547828\n",
      "    val_log_likelihood: -12147.837166886373\n",
      "    val_log_marginal: -12155.967904462294\n",
      "Train Epoch: 2434 [256/118836 (0%)] Loss: 12209.919922\n",
      "Train Epoch: 2434 [33024/118836 (28%)] Loss: 12336.647461\n",
      "Train Epoch: 2434 [65792/118836 (55%)] Loss: 12330.220703\n",
      "Train Epoch: 2434 [98560/118836 (83%)] Loss: 12251.596680\n",
      "    epoch          : 2434\n",
      "    loss           : 12235.864103856493\n",
      "    val_loss       : 12239.62956178023\n",
      "    val_log_likelihood: -12148.414204339848\n",
      "    val_log_marginal: -12156.520569772098\n",
      "Train Epoch: 2435 [256/118836 (0%)] Loss: 12257.148438\n",
      "Train Epoch: 2435 [33024/118836 (28%)] Loss: 12348.671875\n",
      "Train Epoch: 2435 [65792/118836 (55%)] Loss: 12171.961914\n",
      "Train Epoch: 2435 [98560/118836 (83%)] Loss: 12176.841797\n",
      "    epoch          : 2435\n",
      "    loss           : 12233.085521188741\n",
      "    val_loss       : 12234.837512701715\n",
      "    val_log_likelihood: -12147.736791285412\n",
      "    val_log_marginal: -12155.954431542157\n",
      "Train Epoch: 2436 [256/118836 (0%)] Loss: 12314.831055\n",
      "Train Epoch: 2436 [33024/118836 (28%)] Loss: 12256.229492\n",
      "Train Epoch: 2436 [65792/118836 (55%)] Loss: 12288.326172\n",
      "Train Epoch: 2436 [98560/118836 (83%)] Loss: 12334.239258\n",
      "    epoch          : 2436\n",
      "    loss           : 12235.078460698409\n",
      "    val_loss       : 12232.866044363576\n",
      "    val_log_likelihood: -12149.014977997052\n",
      "    val_log_marginal: -12157.082258485589\n",
      "Train Epoch: 2437 [256/118836 (0%)] Loss: 12266.886719\n",
      "Train Epoch: 2437 [33024/118836 (28%)] Loss: 12247.499023\n",
      "Train Epoch: 2437 [65792/118836 (55%)] Loss: 12168.896484\n",
      "Train Epoch: 2437 [98560/118836 (83%)] Loss: 12194.949219\n",
      "    epoch          : 2437\n",
      "    loss           : 12236.470040128723\n",
      "    val_loss       : 12238.014215253319\n",
      "    val_log_likelihood: -12149.875834884202\n",
      "    val_log_marginal: -12158.088159562438\n",
      "Train Epoch: 2438 [256/118836 (0%)] Loss: 12321.514648\n",
      "Train Epoch: 2438 [33024/118836 (28%)] Loss: 12246.990234\n",
      "Train Epoch: 2438 [65792/118836 (55%)] Loss: 12355.582031\n",
      "Train Epoch: 2438 [98560/118836 (83%)] Loss: 12286.429688\n",
      "    epoch          : 2438\n",
      "    loss           : 12235.580288784637\n",
      "    val_loss       : 12233.727916793017\n",
      "    val_log_likelihood: -12148.155774723427\n",
      "    val_log_marginal: -12156.206481901805\n",
      "Train Epoch: 2439 [256/118836 (0%)] Loss: 12248.410156\n",
      "Train Epoch: 2439 [33024/118836 (28%)] Loss: 12242.555664\n",
      "Train Epoch: 2439 [65792/118836 (55%)] Loss: 12324.219727\n",
      "Train Epoch: 2439 [98560/118836 (83%)] Loss: 12349.632812\n",
      "    epoch          : 2439\n",
      "    loss           : 12238.833721212004\n",
      "    val_loss       : 12231.294992215338\n",
      "    val_log_likelihood: -12148.927045369364\n",
      "    val_log_marginal: -12157.04909451329\n",
      "Train Epoch: 2440 [256/118836 (0%)] Loss: 12226.964844\n",
      "Train Epoch: 2440 [33024/118836 (28%)] Loss: 12266.803711\n",
      "Train Epoch: 2440 [65792/118836 (55%)] Loss: 12406.679688\n",
      "Train Epoch: 2440 [98560/118836 (83%)] Loss: 12249.975586\n",
      "    epoch          : 2440\n",
      "    loss           : 12235.163456045802\n",
      "    val_loss       : 12237.179058205998\n",
      "    val_log_likelihood: -12147.844755479735\n",
      "    val_log_marginal: -12155.954583246717\n",
      "Train Epoch: 2441 [256/118836 (0%)] Loss: 12233.060547\n",
      "Train Epoch: 2441 [33024/118836 (28%)] Loss: 12246.815430\n",
      "Train Epoch: 2441 [65792/118836 (55%)] Loss: 12234.896484\n",
      "Train Epoch: 2441 [98560/118836 (83%)] Loss: 12341.261719\n",
      "    epoch          : 2441\n",
      "    loss           : 12238.096609090673\n",
      "    val_loss       : 12238.898584153761\n",
      "    val_log_likelihood: -12147.645698440085\n",
      "    val_log_marginal: -12155.664911087713\n",
      "Train Epoch: 2442 [256/118836 (0%)] Loss: 12269.171875\n",
      "Train Epoch: 2442 [33024/118836 (28%)] Loss: 12262.782227\n",
      "Train Epoch: 2442 [65792/118836 (55%)] Loss: 12257.261719\n",
      "Train Epoch: 2442 [98560/118836 (83%)] Loss: 12278.822266\n",
      "    epoch          : 2442\n",
      "    loss           : 12231.82653293657\n",
      "    val_loss       : 12234.934248888501\n",
      "    val_log_likelihood: -12146.374652993176\n",
      "    val_log_marginal: -12154.499377932201\n",
      "Train Epoch: 2443 [256/118836 (0%)] Loss: 12287.669922\n",
      "Train Epoch: 2443 [33024/118836 (28%)] Loss: 12224.048828\n",
      "Train Epoch: 2443 [65792/118836 (55%)] Loss: 12224.710938\n",
      "Train Epoch: 2443 [98560/118836 (83%)] Loss: 12262.692383\n",
      "    epoch          : 2443\n",
      "    loss           : 12236.152131797975\n",
      "    val_loss       : 12238.797585928305\n",
      "    val_log_likelihood: -12148.124460265457\n",
      "    val_log_marginal: -12156.223653571658\n",
      "Train Epoch: 2444 [256/118836 (0%)] Loss: 12295.667969\n",
      "Train Epoch: 2444 [33024/118836 (28%)] Loss: 12263.535156\n",
      "Train Epoch: 2444 [65792/118836 (55%)] Loss: 12361.730469\n",
      "Train Epoch: 2444 [98560/118836 (83%)] Loss: 12275.267578\n",
      "    epoch          : 2444\n",
      "    loss           : 12236.879656159534\n",
      "    val_loss       : 12236.95408195777\n",
      "    val_log_likelihood: -12148.862510985318\n",
      "    val_log_marginal: -12157.117622170521\n",
      "Train Epoch: 2445 [256/118836 (0%)] Loss: 12366.587891\n",
      "Train Epoch: 2445 [33024/118836 (28%)] Loss: 12160.292969\n",
      "Train Epoch: 2445 [65792/118836 (55%)] Loss: 12200.268555\n",
      "Train Epoch: 2445 [98560/118836 (83%)] Loss: 12305.519531\n",
      "    epoch          : 2445\n",
      "    loss           : 12237.20849682072\n",
      "    val_loss       : 12238.5777035897\n",
      "    val_log_likelihood: -12149.584782102978\n",
      "    val_log_marginal: -12157.950914277953\n",
      "Train Epoch: 2446 [256/118836 (0%)] Loss: 12308.796875\n",
      "Train Epoch: 2446 [33024/118836 (28%)] Loss: 12209.500000\n",
      "Train Epoch: 2446 [65792/118836 (55%)] Loss: 12234.669922\n",
      "Train Epoch: 2446 [98560/118836 (83%)] Loss: 12323.883789\n",
      "    epoch          : 2446\n",
      "    loss           : 12239.77103187681\n",
      "    val_loss       : 12237.124053825664\n",
      "    val_log_likelihood: -12147.907379387665\n",
      "    val_log_marginal: -12156.108147275401\n",
      "Train Epoch: 2447 [256/118836 (0%)] Loss: 12175.732422\n",
      "Train Epoch: 2447 [33024/118836 (28%)] Loss: 12245.305664\n",
      "Train Epoch: 2447 [65792/118836 (55%)] Loss: 12304.062500\n",
      "Train Epoch: 2447 [98560/118836 (83%)] Loss: 12251.042969\n",
      "    epoch          : 2447\n",
      "    loss           : 12240.466801236818\n",
      "    val_loss       : 12239.295035030573\n",
      "    val_log_likelihood: -12148.549410185327\n",
      "    val_log_marginal: -12156.701795193543\n",
      "Train Epoch: 2448 [256/118836 (0%)] Loss: 12219.143555\n",
      "Train Epoch: 2448 [33024/118836 (28%)] Loss: 12241.120117\n",
      "Train Epoch: 2448 [65792/118836 (55%)] Loss: 12164.865234\n",
      "Train Epoch: 2448 [98560/118836 (83%)] Loss: 12221.033203\n",
      "    epoch          : 2448\n",
      "    loss           : 12230.475104360525\n",
      "    val_loss       : 12236.560845958335\n",
      "    val_log_likelihood: -12148.188327614507\n",
      "    val_log_marginal: -12156.336899052732\n",
      "Train Epoch: 2449 [256/118836 (0%)] Loss: 12230.284180\n",
      "Train Epoch: 2449 [33024/118836 (28%)] Loss: 12142.450195\n",
      "Train Epoch: 2449 [65792/118836 (55%)] Loss: 12180.970703\n",
      "Train Epoch: 2449 [98560/118836 (83%)] Loss: 12336.823242\n",
      "    epoch          : 2449\n",
      "    loss           : 12236.443341281534\n",
      "    val_loss       : 12233.521119157549\n",
      "    val_log_likelihood: -12147.403826121796\n",
      "    val_log_marginal: -12155.446856428645\n",
      "Train Epoch: 2450 [256/118836 (0%)] Loss: 12282.752930\n",
      "Train Epoch: 2450 [33024/118836 (28%)] Loss: 12268.788086\n",
      "Train Epoch: 2450 [65792/118836 (55%)] Loss: 12283.651367\n",
      "Train Epoch: 2450 [98560/118836 (83%)] Loss: 12373.774414\n",
      "    epoch          : 2450\n",
      "    loss           : 12240.540250368333\n",
      "    val_loss       : 12238.685874044857\n",
      "    val_log_likelihood: -12147.422935729426\n",
      "    val_log_marginal: -12155.527660805035\n",
      "Train Epoch: 2451 [256/118836 (0%)] Loss: 12275.158203\n",
      "Train Epoch: 2451 [33024/118836 (28%)] Loss: 12315.047852\n",
      "Train Epoch: 2451 [65792/118836 (55%)] Loss: 12288.796875\n",
      "Train Epoch: 2451 [98560/118836 (83%)] Loss: 12232.542969\n",
      "    epoch          : 2451\n",
      "    loss           : 12233.783145775174\n",
      "    val_loss       : 12232.045852242993\n",
      "    val_log_likelihood: -12147.258683248036\n",
      "    val_log_marginal: -12155.510502663825\n",
      "Train Epoch: 2452 [256/118836 (0%)] Loss: 12202.462891\n",
      "Train Epoch: 2452 [33024/118836 (28%)] Loss: 12193.039062\n",
      "Train Epoch: 2452 [65792/118836 (55%)] Loss: 12280.723633\n",
      "Train Epoch: 2452 [98560/118836 (83%)] Loss: 12290.656250\n",
      "    epoch          : 2452\n",
      "    loss           : 12240.780461318755\n",
      "    val_loss       : 12239.502020335618\n",
      "    val_log_likelihood: -12148.383270652397\n",
      "    val_log_marginal: -12156.709332360891\n",
      "Train Epoch: 2453 [256/118836 (0%)] Loss: 12212.520508\n",
      "Train Epoch: 2453 [33024/118836 (28%)] Loss: 12244.921875\n",
      "Train Epoch: 2453 [65792/118836 (55%)] Loss: 12264.677734\n",
      "Train Epoch: 2453 [98560/118836 (83%)] Loss: 12194.189453\n",
      "    epoch          : 2453\n",
      "    loss           : 12239.701659267732\n",
      "    val_loss       : 12235.14400231176\n",
      "    val_log_likelihood: -12149.7139479619\n",
      "    val_log_marginal: -12157.729292913396\n",
      "Train Epoch: 2454 [256/118836 (0%)] Loss: 12320.030273\n",
      "Train Epoch: 2454 [33024/118836 (28%)] Loss: 12248.259766\n",
      "Train Epoch: 2454 [65792/118836 (55%)] Loss: 12221.836914\n",
      "Train Epoch: 2454 [98560/118836 (83%)] Loss: 12259.752930\n",
      "    epoch          : 2454\n",
      "    loss           : 12241.073900337313\n",
      "    val_loss       : 12237.779003022635\n",
      "    val_log_likelihood: -12148.317210278381\n",
      "    val_log_marginal: -12156.47733606479\n",
      "Train Epoch: 2455 [256/118836 (0%)] Loss: 12339.241211\n",
      "Train Epoch: 2455 [33024/118836 (28%)] Loss: 12154.752930\n",
      "Train Epoch: 2455 [65792/118836 (55%)] Loss: 12320.398438\n",
      "Train Epoch: 2455 [98560/118836 (83%)] Loss: 12284.148438\n",
      "    epoch          : 2455\n",
      "    loss           : 12238.582699092742\n",
      "    val_loss       : 12234.130515491843\n",
      "    val_log_likelihood: -12146.83779482656\n",
      "    val_log_marginal: -12154.853807674695\n",
      "Train Epoch: 2456 [256/118836 (0%)] Loss: 12200.695312\n",
      "Train Epoch: 2456 [33024/118836 (28%)] Loss: 12215.083984\n",
      "Train Epoch: 2456 [65792/118836 (55%)] Loss: 12257.060547\n",
      "Train Epoch: 2456 [98560/118836 (83%)] Loss: 12230.638672\n",
      "    epoch          : 2456\n",
      "    loss           : 12233.768863891904\n",
      "    val_loss       : 12235.2555127022\n",
      "    val_log_likelihood: -12147.217597510857\n",
      "    val_log_marginal: -12155.341092650833\n",
      "Train Epoch: 2457 [256/118836 (0%)] Loss: 12151.298828\n",
      "Train Epoch: 2457 [33024/118836 (28%)] Loss: 12279.660156\n",
      "Train Epoch: 2457 [65792/118836 (55%)] Loss: 12320.490234\n",
      "Train Epoch: 2457 [98560/118836 (83%)] Loss: 12278.328125\n",
      "    epoch          : 2457\n",
      "    loss           : 12242.05074102435\n",
      "    val_loss       : 12235.386860253355\n",
      "    val_log_likelihood: -12148.100655403485\n",
      "    val_log_marginal: -12156.349979476528\n",
      "Train Epoch: 2458 [256/118836 (0%)] Loss: 12391.467773\n",
      "Train Epoch: 2458 [33024/118836 (28%)] Loss: 12268.002930\n",
      "Train Epoch: 2458 [65792/118836 (55%)] Loss: 12165.528320\n",
      "Train Epoch: 2458 [98560/118836 (83%)] Loss: 12252.704102\n",
      "    epoch          : 2458\n",
      "    loss           : 12241.099283692618\n",
      "    val_loss       : 12232.649253151672\n",
      "    val_log_likelihood: -12147.417251473325\n",
      "    val_log_marginal: -12155.545107280697\n",
      "Train Epoch: 2459 [256/118836 (0%)] Loss: 12192.335938\n",
      "Train Epoch: 2459 [33024/118836 (28%)] Loss: 12324.881836\n",
      "Train Epoch: 2459 [65792/118836 (55%)] Loss: 12276.030273\n",
      "Train Epoch: 2459 [98560/118836 (83%)] Loss: 12314.883789\n",
      "    epoch          : 2459\n",
      "    loss           : 12240.31558735913\n",
      "    val_loss       : 12238.89615331404\n",
      "    val_log_likelihood: -12148.956830929488\n",
      "    val_log_marginal: -12157.494408084729\n",
      "Train Epoch: 2460 [256/118836 (0%)] Loss: 12364.692383\n",
      "Train Epoch: 2460 [33024/118836 (28%)] Loss: 12241.029297\n",
      "Train Epoch: 2460 [65792/118836 (55%)] Loss: 12212.170898\n",
      "Train Epoch: 2460 [98560/118836 (83%)] Loss: 12284.684570\n",
      "    epoch          : 2460\n",
      "    loss           : 12244.87303072012\n",
      "    val_loss       : 12234.976777252527\n",
      "    val_log_likelihood: -12147.689951987697\n",
      "    val_log_marginal: -12155.942163807804\n",
      "Train Epoch: 2461 [256/118836 (0%)] Loss: 12244.767578\n",
      "Train Epoch: 2461 [33024/118836 (28%)] Loss: 12253.165039\n",
      "Train Epoch: 2461 [65792/118836 (55%)] Loss: 12244.647461\n",
      "Train Epoch: 2461 [98560/118836 (83%)] Loss: 12336.392578\n",
      "    epoch          : 2461\n",
      "    loss           : 12243.781253392526\n",
      "    val_loss       : 12237.638223876289\n",
      "    val_log_likelihood: -12147.060426682692\n",
      "    val_log_marginal: -12155.157864922305\n",
      "Train Epoch: 2462 [256/118836 (0%)] Loss: 12326.447266\n",
      "Train Epoch: 2462 [33024/118836 (28%)] Loss: 12212.291992\n",
      "Train Epoch: 2462 [65792/118836 (55%)] Loss: 12161.973633\n",
      "Train Epoch: 2462 [98560/118836 (83%)] Loss: 12174.777344\n",
      "    epoch          : 2462\n",
      "    loss           : 12235.36193845637\n",
      "    val_loss       : 12237.138547633187\n",
      "    val_log_likelihood: -12147.478845669199\n",
      "    val_log_marginal: -12155.674059838218\n",
      "Train Epoch: 2463 [256/118836 (0%)] Loss: 12219.844727\n",
      "Train Epoch: 2463 [33024/118836 (28%)] Loss: 12257.615234\n",
      "Train Epoch: 2463 [65792/118836 (55%)] Loss: 12346.169922\n",
      "Train Epoch: 2463 [98560/118836 (83%)] Loss: 12211.062500\n",
      "    epoch          : 2463\n",
      "    loss           : 12236.342774891438\n",
      "    val_loss       : 12235.950624253781\n",
      "    val_log_likelihood: -12147.657283427678\n",
      "    val_log_marginal: -12155.74754842055\n",
      "Train Epoch: 2464 [256/118836 (0%)] Loss: 12318.896484\n",
      "Train Epoch: 2464 [33024/118836 (28%)] Loss: 12216.529297\n",
      "Train Epoch: 2464 [65792/118836 (55%)] Loss: 12263.828125\n",
      "Train Epoch: 2464 [98560/118836 (83%)] Loss: 12189.322266\n",
      "    epoch          : 2464\n",
      "    loss           : 12232.80150757341\n",
      "    val_loss       : 12237.488683417852\n",
      "    val_log_likelihood: -12147.917961318755\n",
      "    val_log_marginal: -12155.960391002773\n",
      "Train Epoch: 2465 [256/118836 (0%)] Loss: 12479.640625\n",
      "Train Epoch: 2465 [33024/118836 (28%)] Loss: 12341.941406\n",
      "Train Epoch: 2465 [65792/118836 (55%)] Loss: 12254.177734\n",
      "Train Epoch: 2465 [98560/118836 (83%)] Loss: 12270.779297\n",
      "    epoch          : 2465\n",
      "    loss           : 12236.121305702025\n",
      "    val_loss       : 12238.62160845903\n",
      "    val_log_likelihood: -12148.010976271711\n",
      "    val_log_marginal: -12156.251630821114\n",
      "Train Epoch: 2466 [256/118836 (0%)] Loss: 12261.322266\n",
      "Train Epoch: 2466 [33024/118836 (28%)] Loss: 12205.273438\n",
      "Train Epoch: 2466 [65792/118836 (55%)] Loss: 12169.199219\n",
      "Train Epoch: 2466 [98560/118836 (83%)] Loss: 12282.045898\n",
      "    epoch          : 2466\n",
      "    loss           : 12242.030162615023\n",
      "    val_loss       : 12236.84088286539\n",
      "    val_log_likelihood: -12147.385901474618\n",
      "    val_log_marginal: -12155.486871062163\n",
      "Train Epoch: 2467 [256/118836 (0%)] Loss: 12230.451172\n",
      "Train Epoch: 2467 [33024/118836 (28%)] Loss: 12376.574219\n",
      "Train Epoch: 2467 [65792/118836 (55%)] Loss: 12253.455078\n",
      "Train Epoch: 2467 [98560/118836 (83%)] Loss: 12210.689453\n",
      "    epoch          : 2467\n",
      "    loss           : 12234.8016095107\n",
      "    val_loss       : 12237.672672864506\n",
      "    val_log_likelihood: -12148.192512374637\n",
      "    val_log_marginal: -12156.39466326263\n",
      "Train Epoch: 2468 [256/118836 (0%)] Loss: 12287.282227\n",
      "Train Epoch: 2468 [33024/118836 (28%)] Loss: 12232.284180\n",
      "Train Epoch: 2468 [65792/118836 (55%)] Loss: 12313.447266\n",
      "Train Epoch: 2468 [98560/118836 (83%)] Loss: 12385.701172\n",
      "    epoch          : 2468\n",
      "    loss           : 12238.306924466242\n",
      "    val_loss       : 12238.476461473789\n",
      "    val_log_likelihood: -12147.869034972084\n",
      "    val_log_marginal: -12155.912831161164\n",
      "Train Epoch: 2469 [256/118836 (0%)] Loss: 12322.919922\n",
      "Train Epoch: 2469 [33024/118836 (28%)] Loss: 12280.446289\n",
      "Train Epoch: 2469 [65792/118836 (55%)] Loss: 12190.059570\n",
      "Train Epoch: 2469 [98560/118836 (83%)] Loss: 12400.376953\n",
      "    epoch          : 2469\n",
      "    loss           : 12238.102815957145\n",
      "    val_loss       : 12240.157528702803\n",
      "    val_log_likelihood: -12148.759605207042\n",
      "    val_log_marginal: -12156.983227208557\n",
      "Train Epoch: 2470 [256/118836 (0%)] Loss: 12194.027344\n",
      "Train Epoch: 2470 [33024/118836 (28%)] Loss: 12166.017578\n",
      "Train Epoch: 2470 [65792/118836 (55%)] Loss: 12331.405273\n",
      "Train Epoch: 2470 [98560/118836 (83%)] Loss: 12364.206055\n",
      "    epoch          : 2470\n",
      "    loss           : 12235.5263536174\n",
      "    val_loss       : 12239.10745530991\n",
      "    val_log_likelihood: -12148.496234943652\n",
      "    val_log_marginal: -12156.602630258334\n",
      "Train Epoch: 2471 [256/118836 (0%)] Loss: 12191.737305\n",
      "Train Epoch: 2471 [33024/118836 (28%)] Loss: 12259.712891\n",
      "Train Epoch: 2471 [65792/118836 (55%)] Loss: 12243.582031\n",
      "Train Epoch: 2471 [98560/118836 (83%)] Loss: 12214.986328\n",
      "    epoch          : 2471\n",
      "    loss           : 12241.007271957715\n",
      "    val_loss       : 12236.695784543936\n",
      "    val_log_likelihood: -12150.94782716863\n",
      "    val_log_marginal: -12159.015908394927\n",
      "Train Epoch: 2472 [256/118836 (0%)] Loss: 12373.260742\n",
      "Train Epoch: 2472 [33024/118836 (28%)] Loss: 12333.650391\n",
      "Train Epoch: 2472 [65792/118836 (55%)] Loss: 12201.449219\n",
      "Train Epoch: 2472 [98560/118836 (83%)] Loss: 12202.927734\n",
      "    epoch          : 2472\n",
      "    loss           : 12238.35722381617\n",
      "    val_loss       : 12239.142927815696\n",
      "    val_log_likelihood: -12148.067271020731\n",
      "    val_log_marginal: -12156.155630114774\n",
      "Train Epoch: 2473 [256/118836 (0%)] Loss: 12206.568359\n",
      "Train Epoch: 2473 [33024/118836 (28%)] Loss: 12391.871094\n",
      "Train Epoch: 2473 [65792/118836 (55%)] Loss: 12152.208984\n",
      "Train Epoch: 2473 [98560/118836 (83%)] Loss: 12344.101562\n",
      "    epoch          : 2473\n",
      "    loss           : 12234.095958695203\n",
      "    val_loss       : 12232.992110852203\n",
      "    val_log_likelihood: -12149.753807220586\n",
      "    val_log_marginal: -12157.767463097382\n",
      "Train Epoch: 2474 [256/118836 (0%)] Loss: 12287.050781\n",
      "Train Epoch: 2474 [33024/118836 (28%)] Loss: 12321.657227\n",
      "Train Epoch: 2474 [65792/118836 (55%)] Loss: 12397.005859\n",
      "Train Epoch: 2474 [98560/118836 (83%)] Loss: 12212.048828\n",
      "    epoch          : 2474\n",
      "    loss           : 12237.712541033396\n",
      "    val_loss       : 12236.113433580496\n",
      "    val_log_likelihood: -12149.213174950888\n",
      "    val_log_marginal: -12157.607414637947\n",
      "Train Epoch: 2475 [256/118836 (0%)] Loss: 12241.056641\n",
      "Train Epoch: 2475 [33024/118836 (28%)] Loss: 12167.155273\n",
      "Train Epoch: 2475 [65792/118836 (55%)] Loss: 12177.511719\n",
      "Train Epoch: 2475 [98560/118836 (83%)] Loss: 12192.168945\n",
      "    epoch          : 2475\n",
      "    loss           : 12237.886998067876\n",
      "    val_loss       : 12232.794791739387\n",
      "    val_log_likelihood: -12146.55007673568\n",
      "    val_log_marginal: -12154.659372707227\n",
      "Train Epoch: 2476 [256/118836 (0%)] Loss: 12341.254883\n",
      "Train Epoch: 2476 [33024/118836 (28%)] Loss: 12287.966797\n",
      "Train Epoch: 2476 [65792/118836 (55%)] Loss: 12199.488281\n",
      "Train Epoch: 2476 [98560/118836 (83%)] Loss: 12285.781250\n",
      "    epoch          : 2476\n",
      "    loss           : 12241.219218329974\n",
      "    val_loss       : 12234.507917837092\n",
      "    val_log_likelihood: -12148.62013204999\n",
      "    val_log_marginal: -12156.656721450594\n",
      "Train Epoch: 2477 [256/118836 (0%)] Loss: 12258.931641\n",
      "Train Epoch: 2477 [33024/118836 (28%)] Loss: 12221.913086\n",
      "Train Epoch: 2477 [65792/118836 (55%)] Loss: 12241.190430\n",
      "Train Epoch: 2477 [98560/118836 (83%)] Loss: 12327.350586\n",
      "    epoch          : 2477\n",
      "    loss           : 12234.20856289418\n",
      "    val_loss       : 12236.654134339085\n",
      "    val_log_likelihood: -12147.389731150484\n",
      "    val_log_marginal: -12155.427877866628\n",
      "Train Epoch: 2478 [256/118836 (0%)] Loss: 12132.817383\n",
      "Train Epoch: 2478 [33024/118836 (28%)] Loss: 12212.418945\n",
      "Train Epoch: 2478 [65792/118836 (55%)] Loss: 12178.441406\n",
      "Train Epoch: 2478 [98560/118836 (83%)] Loss: 12284.564453\n",
      "    epoch          : 2478\n",
      "    loss           : 12243.886631675197\n",
      "    val_loss       : 12237.243907159316\n",
      "    val_log_likelihood: -12150.001642628204\n",
      "    val_log_marginal: -12158.238445367542\n",
      "Train Epoch: 2479 [256/118836 (0%)] Loss: 12240.932617\n",
      "Train Epoch: 2479 [33024/118836 (28%)] Loss: 12217.943359\n",
      "Train Epoch: 2479 [65792/118836 (55%)] Loss: 12329.430664\n",
      "Train Epoch: 2479 [98560/118836 (83%)] Loss: 12258.949219\n",
      "    epoch          : 2479\n",
      "    loss           : 12238.66198288229\n",
      "    val_loss       : 12236.578278179139\n",
      "    val_log_likelihood: -12147.872602938896\n",
      "    val_log_marginal: -12156.069882388496\n",
      "Train Epoch: 2480 [256/118836 (0%)] Loss: 12305.576172\n",
      "Train Epoch: 2480 [33024/118836 (28%)] Loss: 12227.224609\n",
      "Train Epoch: 2480 [65792/118836 (55%)] Loss: 12215.197266\n",
      "Train Epoch: 2480 [98560/118836 (83%)] Loss: 12337.701172\n",
      "    epoch          : 2480\n",
      "    loss           : 12235.333954165375\n",
      "    val_loss       : 12238.674677863142\n",
      "    val_log_likelihood: -12149.434445435278\n",
      "    val_log_marginal: -12157.548143725366\n",
      "Train Epoch: 2481 [256/118836 (0%)] Loss: 12263.583984\n",
      "Train Epoch: 2481 [33024/118836 (28%)] Loss: 12192.125000\n",
      "Train Epoch: 2481 [65792/118836 (55%)] Loss: 12259.121094\n",
      "Train Epoch: 2481 [98560/118836 (83%)] Loss: 12265.290039\n",
      "    epoch          : 2481\n",
      "    loss           : 12231.871416847602\n",
      "    val_loss       : 12236.909765688159\n",
      "    val_log_likelihood: -12145.778477176385\n",
      "    val_log_marginal: -12153.981880364841\n",
      "Train Epoch: 2482 [256/118836 (0%)] Loss: 12295.476562\n",
      "Train Epoch: 2482 [33024/118836 (28%)] Loss: 12345.098633\n",
      "Train Epoch: 2482 [65792/118836 (55%)] Loss: 12171.115234\n",
      "Train Epoch: 2482 [98560/118836 (83%)] Loss: 12192.006836\n",
      "    epoch          : 2482\n",
      "    loss           : 12239.768683280345\n",
      "    val_loss       : 12235.862612367286\n",
      "    val_log_likelihood: -12146.770132696183\n",
      "    val_log_marginal: -12154.83879980681\n",
      "Train Epoch: 2483 [256/118836 (0%)] Loss: 12359.925781\n",
      "Train Epoch: 2483 [33024/118836 (28%)] Loss: 12260.280273\n",
      "Train Epoch: 2483 [65792/118836 (55%)] Loss: 12339.039062\n",
      "Train Epoch: 2483 [98560/118836 (83%)] Loss: 12233.003906\n",
      "    epoch          : 2483\n",
      "    loss           : 12240.340426294973\n",
      "    val_loss       : 12230.48422277156\n",
      "    val_log_likelihood: -12150.889682524297\n",
      "    val_log_marginal: -12158.989948025024\n",
      "Train Epoch: 2484 [256/118836 (0%)] Loss: 12266.214844\n",
      "Train Epoch: 2484 [33024/118836 (28%)] Loss: 12264.268555\n",
      "Train Epoch: 2484 [65792/118836 (55%)] Loss: 12166.952148\n",
      "Train Epoch: 2484 [98560/118836 (83%)] Loss: 12302.838867\n",
      "    epoch          : 2484\n",
      "    loss           : 12235.003695105717\n",
      "    val_loss       : 12236.712290747644\n",
      "    val_log_likelihood: -12148.699143791357\n",
      "    val_log_marginal: -12157.074784806004\n",
      "Train Epoch: 2485 [256/118836 (0%)] Loss: 12165.884766\n",
      "Train Epoch: 2485 [33024/118836 (28%)] Loss: 12312.372070\n",
      "Train Epoch: 2485 [65792/118836 (55%)] Loss: 12273.892578\n",
      "Train Epoch: 2485 [98560/118836 (83%)] Loss: 12170.126953\n",
      "    epoch          : 2485\n",
      "    loss           : 12236.980914786238\n",
      "    val_loss       : 12232.705522833396\n",
      "    val_log_likelihood: -12148.015442126756\n",
      "    val_log_marginal: -12156.075415093364\n",
      "Train Epoch: 2486 [256/118836 (0%)] Loss: 12256.660156\n",
      "Train Epoch: 2486 [33024/118836 (28%)] Loss: 12245.971680\n",
      "Train Epoch: 2486 [65792/118836 (55%)] Loss: 12291.012695\n",
      "Train Epoch: 2486 [98560/118836 (83%)] Loss: 12291.247070\n",
      "    epoch          : 2486\n",
      "    loss           : 12238.374007605717\n",
      "    val_loss       : 12238.731148046107\n",
      "    val_log_likelihood: -12148.025685128463\n",
      "    val_log_marginal: -12156.222193812511\n",
      "Train Epoch: 2487 [256/118836 (0%)] Loss: 12357.343750\n",
      "Train Epoch: 2487 [33024/118836 (28%)] Loss: 12169.343750\n",
      "Train Epoch: 2487 [65792/118836 (55%)] Loss: 12439.193359\n",
      "Train Epoch: 2487 [98560/118836 (83%)] Loss: 12234.190430\n",
      "    epoch          : 2487\n",
      "    loss           : 12237.789704171837\n",
      "    val_loss       : 12236.938605917692\n",
      "    val_log_likelihood: -12147.559890179125\n",
      "    val_log_marginal: -12155.56923151982\n",
      "Train Epoch: 2488 [256/118836 (0%)] Loss: 12351.108398\n",
      "Train Epoch: 2488 [33024/118836 (28%)] Loss: 12179.961914\n",
      "Train Epoch: 2488 [65792/118836 (55%)] Loss: 12363.539062\n",
      "Train Epoch: 2488 [98560/118836 (83%)] Loss: 12251.148438\n",
      "    epoch          : 2488\n",
      "    loss           : 12238.279366179435\n",
      "    val_loss       : 12241.966787373636\n",
      "    val_log_likelihood: -12147.280929648728\n",
      "    val_log_marginal: -12155.250091900873\n",
      "Train Epoch: 2489 [256/118836 (0%)] Loss: 12260.259766\n",
      "Train Epoch: 2489 [33024/118836 (28%)] Loss: 12273.320312\n",
      "Train Epoch: 2489 [65792/118836 (55%)] Loss: 12326.912109\n",
      "Train Epoch: 2489 [98560/118836 (83%)] Loss: 12243.895508\n",
      "    epoch          : 2489\n",
      "    loss           : 12233.014741651157\n",
      "    val_loss       : 12231.945275592409\n",
      "    val_log_likelihood: -12148.962711790478\n",
      "    val_log_marginal: -12156.997127124867\n",
      "Train Epoch: 2490 [256/118836 (0%)] Loss: 12295.738281\n",
      "Train Epoch: 2490 [33024/118836 (28%)] Loss: 12194.593750\n",
      "Train Epoch: 2490 [65792/118836 (55%)] Loss: 12361.046875\n",
      "Train Epoch: 2490 [98560/118836 (83%)] Loss: 12263.585938\n",
      "    epoch          : 2490\n",
      "    loss           : 12238.469355323356\n",
      "    val_loss       : 12236.1948984124\n",
      "    val_log_likelihood: -12145.604460847033\n",
      "    val_log_marginal: -12153.661218482273\n",
      "Train Epoch: 2491 [256/118836 (0%)] Loss: 12285.906250\n",
      "Train Epoch: 2491 [33024/118836 (28%)] Loss: 12302.087891\n",
      "Train Epoch: 2491 [65792/118836 (55%)] Loss: 12262.068359\n",
      "Train Epoch: 2491 [98560/118836 (83%)] Loss: 12274.603516\n",
      "    epoch          : 2491\n",
      "    loss           : 12235.154686530708\n",
      "    val_loss       : 12236.370504737637\n",
      "    val_log_likelihood: -12148.002263944893\n",
      "    val_log_marginal: -12156.124737117958\n",
      "Train Epoch: 2492 [256/118836 (0%)] Loss: 12240.304688\n",
      "Train Epoch: 2492 [33024/118836 (28%)] Loss: 12181.492188\n",
      "Train Epoch: 2492 [65792/118836 (55%)] Loss: 12201.523438\n",
      "Train Epoch: 2492 [98560/118836 (83%)] Loss: 12236.936523\n",
      "    epoch          : 2492\n",
      "    loss           : 12235.284105698149\n",
      "    val_loss       : 12234.142178986996\n",
      "    val_log_likelihood: -12146.540436149453\n",
      "    val_log_marginal: -12154.986925292536\n",
      "Train Epoch: 2493 [256/118836 (0%)] Loss: 12278.714844\n",
      "Train Epoch: 2493 [33024/118836 (28%)] Loss: 12218.273438\n",
      "Train Epoch: 2493 [65792/118836 (55%)] Loss: 12253.220703\n",
      "Train Epoch: 2493 [98560/118836 (83%)] Loss: 12209.914062\n",
      "    epoch          : 2493\n",
      "    loss           : 12236.046534939775\n",
      "    val_loss       : 12238.7302941388\n",
      "    val_log_likelihood: -12147.30546325734\n",
      "    val_log_marginal: -12155.481845135957\n",
      "Train Epoch: 2494 [256/118836 (0%)] Loss: 12254.748047\n",
      "Train Epoch: 2494 [33024/118836 (28%)] Loss: 12412.390625\n",
      "Train Epoch: 2494 [65792/118836 (55%)] Loss: 12350.948242\n",
      "Train Epoch: 2494 [98560/118836 (83%)] Loss: 12261.503906\n",
      "    epoch          : 2494\n",
      "    loss           : 12239.427561356235\n",
      "    val_loss       : 12233.239379387374\n",
      "    val_log_likelihood: -12147.858666123862\n",
      "    val_log_marginal: -12155.896444302592\n",
      "Train Epoch: 2495 [256/118836 (0%)] Loss: 12253.142578\n",
      "Train Epoch: 2495 [33024/118836 (28%)] Loss: 12208.136719\n",
      "Train Epoch: 2495 [65792/118836 (55%)] Loss: 12220.427734\n",
      "Train Epoch: 2495 [98560/118836 (83%)] Loss: 12277.474609\n",
      "    epoch          : 2495\n",
      "    loss           : 12231.614746336074\n",
      "    val_loss       : 12240.472511999249\n",
      "    val_log_likelihood: -12148.413828092689\n",
      "    val_log_marginal: -12156.531640670593\n",
      "Train Epoch: 2496 [256/118836 (0%)] Loss: 12192.641602\n",
      "Train Epoch: 2496 [33024/118836 (28%)] Loss: 12190.513672\n",
      "Train Epoch: 2496 [65792/118836 (55%)] Loss: 12207.594727\n",
      "Train Epoch: 2496 [98560/118836 (83%)] Loss: 12350.003906\n",
      "    epoch          : 2496\n",
      "    loss           : 12233.41654889759\n",
      "    val_loss       : 12237.906550913269\n",
      "    val_log_likelihood: -12146.782480355667\n",
      "    val_log_marginal: -12154.956060592876\n",
      "Train Epoch: 2497 [256/118836 (0%)] Loss: 12390.741211\n",
      "Train Epoch: 2497 [33024/118836 (28%)] Loss: 12215.857422\n",
      "Train Epoch: 2497 [65792/118836 (55%)] Loss: 12119.107422\n",
      "Train Epoch: 2497 [98560/118836 (83%)] Loss: 12266.350586\n",
      "    epoch          : 2497\n",
      "    loss           : 12236.987252991883\n",
      "    val_loss       : 12235.722722286058\n",
      "    val_log_likelihood: -12146.010735402451\n",
      "    val_log_marginal: -12154.103327679868\n",
      "Train Epoch: 2498 [256/118836 (0%)] Loss: 12338.453125\n",
      "Train Epoch: 2498 [33024/118836 (28%)] Loss: 12308.235352\n",
      "Train Epoch: 2498 [65792/118836 (55%)] Loss: 12296.050781\n",
      "Train Epoch: 2498 [98560/118836 (83%)] Loss: 12251.708984\n",
      "    epoch          : 2498\n",
      "    loss           : 12234.609652056193\n",
      "    val_loss       : 12235.733845296001\n",
      "    val_log_likelihood: -12147.626299498552\n",
      "    val_log_marginal: -12155.957931468105\n",
      "Train Epoch: 2499 [256/118836 (0%)] Loss: 12312.612305\n",
      "Train Epoch: 2499 [33024/118836 (28%)] Loss: 12275.755859\n",
      "Train Epoch: 2499 [65792/118836 (55%)] Loss: 12342.673828\n",
      "Train Epoch: 2499 [98560/118836 (83%)] Loss: 12279.049805\n",
      "    epoch          : 2499\n",
      "    loss           : 12232.918832228339\n",
      "    val_loss       : 12238.46050149154\n",
      "    val_log_likelihood: -12148.052653923696\n",
      "    val_log_marginal: -12156.070026998093\n",
      "Train Epoch: 2500 [256/118836 (0%)] Loss: 12241.146484\n",
      "Train Epoch: 2500 [33024/118836 (28%)] Loss: 12219.757812\n",
      "Train Epoch: 2500 [65792/118836 (55%)] Loss: 12247.230469\n",
      "Train Epoch: 2500 [98560/118836 (83%)] Loss: 12214.081055\n",
      "    epoch          : 2500\n",
      "    loss           : 12236.401714032774\n",
      "    val_loss       : 12234.59582308475\n",
      "    val_log_likelihood: -12147.332532535927\n",
      "    val_log_marginal: -12155.549571291858\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch2500.pth ...\n",
      "Train Epoch: 2501 [256/118836 (0%)] Loss: 12303.575195\n",
      "Train Epoch: 2501 [33024/118836 (28%)] Loss: 12311.123047\n",
      "Train Epoch: 2501 [65792/118836 (55%)] Loss: 12213.194336\n",
      "Train Epoch: 2501 [98560/118836 (83%)] Loss: 12360.196289\n",
      "    epoch          : 2501\n",
      "    loss           : 12234.441332260649\n",
      "    val_loss       : 12237.606827394182\n",
      "    val_log_likelihood: -12147.286720203938\n",
      "    val_log_marginal: -12155.381395343065\n",
      "Train Epoch: 2502 [256/118836 (0%)] Loss: 12203.585938\n",
      "Train Epoch: 2502 [33024/118836 (28%)] Loss: 12157.861328\n",
      "Train Epoch: 2502 [65792/118836 (55%)] Loss: 12169.942383\n",
      "Train Epoch: 2502 [98560/118836 (83%)] Loss: 12240.784180\n",
      "    epoch          : 2502\n",
      "    loss           : 12239.106540464743\n",
      "    val_loss       : 12234.69809371772\n",
      "    val_log_likelihood: -12148.533818302833\n",
      "    val_log_marginal: -12156.767356682327\n",
      "Train Epoch: 2503 [256/118836 (0%)] Loss: 12171.847656\n",
      "Train Epoch: 2503 [33024/118836 (28%)] Loss: 12334.113281\n",
      "Train Epoch: 2503 [65792/118836 (55%)] Loss: 12219.711914\n",
      "Train Epoch: 2503 [98560/118836 (83%)] Loss: 12334.292969\n",
      "    epoch          : 2503\n",
      "    loss           : 12232.8681978262\n",
      "    val_loss       : 12240.399651493144\n",
      "    val_log_likelihood: -12148.036609866107\n",
      "    val_log_marginal: -12156.177122358338\n",
      "Train Epoch: 2504 [256/118836 (0%)] Loss: 12183.449219\n",
      "Train Epoch: 2504 [33024/118836 (28%)] Loss: 12164.281250\n",
      "Train Epoch: 2504 [65792/118836 (55%)] Loss: 12419.758789\n",
      "Train Epoch: 2504 [98560/118836 (83%)] Loss: 12239.320312\n",
      "    epoch          : 2504\n",
      "    loss           : 12238.746855775693\n",
      "    val_loss       : 12234.811935435717\n",
      "    val_log_likelihood: -12147.20474937319\n",
      "    val_log_marginal: -12155.376406102821\n",
      "Train Epoch: 2505 [256/118836 (0%)] Loss: 12175.449219\n",
      "Train Epoch: 2505 [33024/118836 (28%)] Loss: 12196.833008\n",
      "Train Epoch: 2505 [65792/118836 (55%)] Loss: 12230.053711\n",
      "Train Epoch: 2505 [98560/118836 (83%)] Loss: 12210.809570\n",
      "    epoch          : 2505\n",
      "    loss           : 12231.553331944015\n",
      "    val_loss       : 12235.082792489848\n",
      "    val_log_likelihood: -12149.826030842898\n",
      "    val_log_marginal: -12158.068267359984\n",
      "Train Epoch: 2506 [256/118836 (0%)] Loss: 12385.228516\n",
      "Train Epoch: 2506 [33024/118836 (28%)] Loss: 12249.996094\n",
      "Train Epoch: 2506 [65792/118836 (55%)] Loss: 12293.500977\n",
      "Train Epoch: 2506 [98560/118836 (83%)] Loss: 12313.838867\n",
      "    epoch          : 2506\n",
      "    loss           : 12237.20212711306\n",
      "    val_loss       : 12232.960183923331\n",
      "    val_log_likelihood: -12147.523481925919\n",
      "    val_log_marginal: -12155.584072098649\n",
      "Train Epoch: 2507 [256/118836 (0%)] Loss: 12163.455078\n",
      "Train Epoch: 2507 [33024/118836 (28%)] Loss: 12238.522461\n",
      "Train Epoch: 2507 [65792/118836 (55%)] Loss: 12266.145508\n",
      "Train Epoch: 2507 [98560/118836 (83%)] Loss: 12338.258789\n",
      "    epoch          : 2507\n",
      "    loss           : 12232.15090111921\n",
      "    val_loss       : 12233.057822791297\n",
      "    val_log_likelihood: -12147.221487282877\n",
      "    val_log_marginal: -12155.295077305622\n",
      "Train Epoch: 2508 [256/118836 (0%)] Loss: 12215.753906\n",
      "Train Epoch: 2508 [33024/118836 (28%)] Loss: 12214.846680\n",
      "Train Epoch: 2508 [65792/118836 (55%)] Loss: 12162.851562\n",
      "Train Epoch: 2508 [98560/118836 (83%)] Loss: 12222.991211\n",
      "    epoch          : 2508\n",
      "    loss           : 12235.409873701148\n",
      "    val_loss       : 12229.636091031523\n",
      "    val_log_likelihood: -12146.93196663048\n",
      "    val_log_marginal: -12155.069643050518\n",
      "Train Epoch: 2509 [256/118836 (0%)] Loss: 12183.445312\n",
      "Train Epoch: 2509 [33024/118836 (28%)] Loss: 12168.195312\n",
      "Train Epoch: 2509 [65792/118836 (55%)] Loss: 12112.550781\n",
      "Train Epoch: 2509 [98560/118836 (83%)] Loss: 12337.199219\n",
      "    epoch          : 2509\n",
      "    loss           : 12239.136396944788\n",
      "    val_loss       : 12235.858516473509\n",
      "    val_log_likelihood: -12148.051107901676\n",
      "    val_log_marginal: -12156.17947212915\n",
      "Train Epoch: 2510 [256/118836 (0%)] Loss: 12245.800781\n",
      "Train Epoch: 2510 [33024/118836 (28%)] Loss: 12318.876953\n",
      "Train Epoch: 2510 [65792/118836 (55%)] Loss: 12378.822266\n",
      "Train Epoch: 2510 [98560/118836 (83%)] Loss: 12320.537109\n",
      "    epoch          : 2510\n",
      "    loss           : 12237.613449260753\n",
      "    val_loss       : 12235.448047468954\n",
      "    val_log_likelihood: -12146.971091326768\n",
      "    val_log_marginal: -12155.02653472067\n",
      "Train Epoch: 2511 [256/118836 (0%)] Loss: 12192.664062\n",
      "Train Epoch: 2511 [33024/118836 (28%)] Loss: 12268.687500\n",
      "Train Epoch: 2511 [65792/118836 (55%)] Loss: 12203.657227\n",
      "Train Epoch: 2511 [98560/118836 (83%)] Loss: 12293.139648\n",
      "    epoch          : 2511\n",
      "    loss           : 12235.17270067592\n",
      "    val_loss       : 12235.084784729903\n",
      "    val_log_likelihood: -12148.328263124225\n",
      "    val_log_marginal: -12156.485971454998\n",
      "Train Epoch: 2512 [256/118836 (0%)] Loss: 12287.605469\n",
      "Train Epoch: 2512 [33024/118836 (28%)] Loss: 12289.434570\n",
      "Train Epoch: 2512 [65792/118836 (55%)] Loss: 12360.135742\n",
      "Train Epoch: 2512 [98560/118836 (83%)] Loss: 12351.291992\n",
      "    epoch          : 2512\n",
      "    loss           : 12235.388751518558\n",
      "    val_loss       : 12237.661250663874\n",
      "    val_log_likelihood: -12149.343248714073\n",
      "    val_log_marginal: -12157.669317647373\n",
      "Train Epoch: 2513 [256/118836 (0%)] Loss: 12330.683594\n",
      "Train Epoch: 2513 [33024/118836 (28%)] Loss: 12332.700195\n",
      "Train Epoch: 2513 [65792/118836 (55%)] Loss: 12272.450195\n",
      "Train Epoch: 2513 [98560/118836 (83%)] Loss: 12251.429688\n",
      "    epoch          : 2513\n",
      "    loss           : 12240.277356512355\n",
      "    val_loss       : 12232.62469763885\n",
      "    val_log_likelihood: -12148.931254361818\n",
      "    val_log_marginal: -12157.022368618836\n",
      "Train Epoch: 2514 [256/118836 (0%)] Loss: 12156.320312\n",
      "Train Epoch: 2514 [33024/118836 (28%)] Loss: 12297.538086\n",
      "Train Epoch: 2514 [65792/118836 (55%)] Loss: 12224.322266\n",
      "Train Epoch: 2514 [98560/118836 (83%)] Loss: 12310.333984\n",
      "    epoch          : 2514\n",
      "    loss           : 12238.180179900744\n",
      "    val_loss       : 12231.03661325189\n",
      "    val_log_likelihood: -12147.598179991212\n",
      "    val_log_marginal: -12155.644633942562\n",
      "Train Epoch: 2515 [256/118836 (0%)] Loss: 12272.521484\n",
      "Train Epoch: 2515 [33024/118836 (28%)] Loss: 12339.002930\n",
      "Train Epoch: 2515 [65792/118836 (55%)] Loss: 12391.822266\n",
      "Train Epoch: 2515 [98560/118836 (83%)] Loss: 12226.826172\n",
      "    epoch          : 2515\n",
      "    loss           : 12238.200919858871\n",
      "    val_loss       : 12236.136681971535\n",
      "    val_log_likelihood: -12146.270123487904\n",
      "    val_log_marginal: -12154.51806999796\n",
      "Train Epoch: 2516 [256/118836 (0%)] Loss: 12348.326172\n",
      "Train Epoch: 2516 [33024/118836 (28%)] Loss: 12313.729492\n",
      "Train Epoch: 2516 [65792/118836 (55%)] Loss: 12247.113281\n",
      "Train Epoch: 2516 [98560/118836 (83%)] Loss: 12215.009766\n",
      "    epoch          : 2516\n",
      "    loss           : 12232.6277274284\n",
      "    val_loss       : 12233.792834413558\n",
      "    val_log_likelihood: -12147.407896990022\n",
      "    val_log_marginal: -12155.48534776623\n",
      "Train Epoch: 2517 [256/118836 (0%)] Loss: 12309.552734\n",
      "Train Epoch: 2517 [33024/118836 (28%)] Loss: 12251.352539\n",
      "Train Epoch: 2517 [65792/118836 (55%)] Loss: 12187.687500\n",
      "Train Epoch: 2517 [98560/118836 (83%)] Loss: 12236.132812\n",
      "    epoch          : 2517\n",
      "    loss           : 12236.858982113316\n",
      "    val_loss       : 12239.46207738438\n",
      "    val_log_likelihood: -12147.923996943497\n",
      "    val_log_marginal: -12156.002332542559\n",
      "Train Epoch: 2518 [256/118836 (0%)] Loss: 12227.293945\n",
      "Train Epoch: 2518 [33024/118836 (28%)] Loss: 12229.035156\n",
      "Train Epoch: 2518 [65792/118836 (55%)] Loss: 12323.935547\n",
      "Train Epoch: 2518 [98560/118836 (83%)] Loss: 12286.572266\n",
      "    epoch          : 2518\n",
      "    loss           : 12242.071702788979\n",
      "    val_loss       : 12238.351537424622\n",
      "    val_log_likelihood: -12148.698012626654\n",
      "    val_log_marginal: -12156.792789222007\n",
      "Train Epoch: 2519 [256/118836 (0%)] Loss: 12270.291016\n",
      "Train Epoch: 2519 [33024/118836 (28%)] Loss: 12279.960938\n",
      "Train Epoch: 2519 [65792/118836 (55%)] Loss: 12178.259766\n",
      "Train Epoch: 2519 [98560/118836 (83%)] Loss: 12256.091797\n",
      "    epoch          : 2519\n",
      "    loss           : 12232.271294878257\n",
      "    val_loss       : 12240.836538057345\n",
      "    val_log_likelihood: -12146.3478585091\n",
      "    val_log_marginal: -12154.435175898807\n",
      "Train Epoch: 2520 [256/118836 (0%)] Loss: 12301.445312\n",
      "Train Epoch: 2520 [33024/118836 (28%)] Loss: 12210.937500\n",
      "Train Epoch: 2520 [65792/118836 (55%)] Loss: 12288.934570\n",
      "Train Epoch: 2520 [98560/118836 (83%)] Loss: 12368.163086\n",
      "    epoch          : 2520\n",
      "    loss           : 12236.375520510237\n",
      "    val_loss       : 12241.810872868607\n",
      "    val_log_likelihood: -12145.872111668992\n",
      "    val_log_marginal: -12154.049764070674\n",
      "Train Epoch: 2521 [256/118836 (0%)] Loss: 12238.328125\n",
      "Train Epoch: 2521 [33024/118836 (28%)] Loss: 12260.738281\n",
      "Train Epoch: 2521 [65792/118836 (55%)] Loss: 12245.289062\n",
      "Train Epoch: 2521 [98560/118836 (83%)] Loss: 12200.228516\n",
      "    epoch          : 2521\n",
      "    loss           : 12236.309184372414\n",
      "    val_loss       : 12236.62671569963\n",
      "    val_log_likelihood: -12147.1223060122\n",
      "    val_log_marginal: -12155.286125947983\n",
      "Train Epoch: 2522 [256/118836 (0%)] Loss: 12252.007812\n",
      "Train Epoch: 2522 [33024/118836 (28%)] Loss: 12236.555664\n",
      "Train Epoch: 2522 [65792/118836 (55%)] Loss: 12293.419922\n",
      "Train Epoch: 2522 [98560/118836 (83%)] Loss: 12264.739258\n",
      "    epoch          : 2522\n",
      "    loss           : 12237.87328935975\n",
      "    val_loss       : 12233.146030969141\n",
      "    val_log_likelihood: -12147.179252449081\n",
      "    val_log_marginal: -12155.278298241019\n",
      "Train Epoch: 2523 [256/118836 (0%)] Loss: 12310.707031\n",
      "Train Epoch: 2523 [33024/118836 (28%)] Loss: 12134.320312\n",
      "Train Epoch: 2523 [65792/118836 (55%)] Loss: 12398.759766\n",
      "Train Epoch: 2523 [98560/118836 (83%)] Loss: 12356.649414\n",
      "    epoch          : 2523\n",
      "    loss           : 12240.254487825681\n",
      "    val_loss       : 12236.53855325524\n",
      "    val_log_likelihood: -12148.84954071676\n",
      "    val_log_marginal: -12156.906607543415\n",
      "Train Epoch: 2524 [256/118836 (0%)] Loss: 12287.814453\n",
      "Train Epoch: 2524 [33024/118836 (28%)] Loss: 12289.474609\n",
      "Train Epoch: 2524 [65792/118836 (55%)] Loss: 12223.283203\n",
      "Train Epoch: 2524 [98560/118836 (83%)] Loss: 12297.298828\n",
      "    epoch          : 2524\n",
      "    loss           : 12237.867998798078\n",
      "    val_loss       : 12236.204703165095\n",
      "    val_log_likelihood: -12147.187108567257\n",
      "    val_log_marginal: -12155.315465027677\n",
      "Train Epoch: 2525 [256/118836 (0%)] Loss: 12226.058594\n",
      "Train Epoch: 2525 [33024/118836 (28%)] Loss: 12201.092773\n",
      "Train Epoch: 2525 [65792/118836 (55%)] Loss: 12187.088867\n",
      "Train Epoch: 2525 [98560/118836 (83%)] Loss: 12298.726562\n",
      "    epoch          : 2525\n",
      "    loss           : 12236.1861328125\n",
      "    val_loss       : 12237.460275547563\n",
      "    val_log_likelihood: -12146.934876285928\n",
      "    val_log_marginal: -12155.051383050688\n",
      "Train Epoch: 2526 [256/118836 (0%)] Loss: 12221.050781\n",
      "Train Epoch: 2526 [33024/118836 (28%)] Loss: 12187.144531\n",
      "Train Epoch: 2526 [65792/118836 (55%)] Loss: 12215.432617\n",
      "Train Epoch: 2526 [98560/118836 (83%)] Loss: 12307.040039\n",
      "    epoch          : 2526\n",
      "    loss           : 12236.189693994262\n",
      "    val_loss       : 12249.082150841587\n",
      "    val_log_likelihood: -12153.429559553351\n",
      "    val_log_marginal: -12161.586214703253\n",
      "Train Epoch: 2527 [256/118836 (0%)] Loss: 12276.348633\n",
      "Train Epoch: 2527 [33024/118836 (28%)] Loss: 12211.134766\n",
      "Train Epoch: 2527 [65792/118836 (55%)] Loss: 12272.917969\n",
      "Train Epoch: 2527 [98560/118836 (83%)] Loss: 12206.884766\n",
      "    epoch          : 2527\n",
      "    loss           : 12236.277479127895\n",
      "    val_loss       : 12233.169122838983\n",
      "    val_log_likelihood: -12145.54386986921\n",
      "    val_log_marginal: -12153.649881156405\n",
      "Train Epoch: 2528 [256/118836 (0%)] Loss: 12346.554688\n",
      "Train Epoch: 2528 [33024/118836 (28%)] Loss: 12262.869141\n",
      "Train Epoch: 2528 [65792/118836 (55%)] Loss: 12304.712891\n",
      "Train Epoch: 2528 [98560/118836 (83%)] Loss: 12163.349609\n",
      "    epoch          : 2528\n",
      "    loss           : 12231.234985492918\n",
      "    val_loss       : 12235.130269038627\n",
      "    val_log_likelihood: -12148.17589029544\n",
      "    val_log_marginal: -12156.195353903195\n",
      "Train Epoch: 2529 [256/118836 (0%)] Loss: 12193.011719\n",
      "Train Epoch: 2529 [33024/118836 (28%)] Loss: 12251.142578\n",
      "Train Epoch: 2529 [65792/118836 (55%)] Loss: 12332.130859\n",
      "Train Epoch: 2529 [98560/118836 (83%)] Loss: 12284.342773\n",
      "    epoch          : 2529\n",
      "    loss           : 12238.726289644077\n",
      "    val_loss       : 12232.641663689987\n",
      "    val_log_likelihood: -12145.794197813275\n",
      "    val_log_marginal: -12153.869632048565\n",
      "Train Epoch: 2530 [256/118836 (0%)] Loss: 12326.621094\n",
      "Train Epoch: 2530 [33024/118836 (28%)] Loss: 12339.456055\n",
      "Train Epoch: 2530 [65792/118836 (55%)] Loss: 12232.394531\n",
      "Train Epoch: 2530 [98560/118836 (83%)] Loss: 12264.910156\n",
      "    epoch          : 2530\n",
      "    loss           : 12242.14883781793\n",
      "    val_loss       : 12237.961577132912\n",
      "    val_log_likelihood: -12149.066727570564\n",
      "    val_log_marginal: -12157.047820340133\n",
      "Train Epoch: 2531 [256/118836 (0%)] Loss: 12265.619141\n",
      "Train Epoch: 2531 [33024/118836 (28%)] Loss: 12319.126953\n",
      "Train Epoch: 2531 [65792/118836 (55%)] Loss: 12262.881836\n",
      "Train Epoch: 2531 [98560/118836 (83%)] Loss: 12189.389648\n",
      "    epoch          : 2531\n",
      "    loss           : 12234.88248116341\n",
      "    val_loss       : 12238.689896456566\n",
      "    val_log_likelihood: -12146.248618596206\n",
      "    val_log_marginal: -12154.58077876456\n",
      "Train Epoch: 2532 [256/118836 (0%)] Loss: 12291.416016\n",
      "Train Epoch: 2532 [33024/118836 (28%)] Loss: 12186.056641\n",
      "Train Epoch: 2532 [65792/118836 (55%)] Loss: 12208.463867\n",
      "Train Epoch: 2532 [98560/118836 (83%)] Loss: 12331.058594\n",
      "    epoch          : 2532\n",
      "    loss           : 12241.384817966811\n",
      "    val_loss       : 12234.62277762543\n",
      "    val_log_likelihood: -12147.736879167958\n",
      "    val_log_marginal: -12155.841028422828\n",
      "Train Epoch: 2533 [256/118836 (0%)] Loss: 12158.758789\n",
      "Train Epoch: 2533 [33024/118836 (28%)] Loss: 12401.478516\n",
      "Train Epoch: 2533 [65792/118836 (55%)] Loss: 12267.096680\n",
      "Train Epoch: 2533 [98560/118836 (83%)] Loss: 12189.329102\n",
      "    epoch          : 2533\n",
      "    loss           : 12235.741033718466\n",
      "    val_loss       : 12229.58226901453\n",
      "    val_log_likelihood: -12148.322475961537\n",
      "    val_log_marginal: -12156.473580981565\n",
      "Train Epoch: 2534 [256/118836 (0%)] Loss: 12212.728516\n",
      "Train Epoch: 2534 [33024/118836 (28%)] Loss: 12446.539062\n",
      "Train Epoch: 2534 [65792/118836 (55%)] Loss: 12243.802734\n",
      "Train Epoch: 2534 [98560/118836 (83%)] Loss: 12336.741211\n",
      "    epoch          : 2534\n",
      "    loss           : 12234.7353985732\n",
      "    val_loss       : 12235.449037716011\n",
      "    val_log_likelihood: -12146.895010242195\n",
      "    val_log_marginal: -12154.954159131465\n",
      "Train Epoch: 2535 [256/118836 (0%)] Loss: 12309.550781\n",
      "Train Epoch: 2535 [33024/118836 (28%)] Loss: 12235.061523\n",
      "Train Epoch: 2535 [65792/118836 (55%)] Loss: 12253.941406\n",
      "Train Epoch: 2535 [98560/118836 (83%)] Loss: 12192.050781\n",
      "    epoch          : 2535\n",
      "    loss           : 12233.950037317773\n",
      "    val_loss       : 12239.258891184661\n",
      "    val_log_likelihood: -12147.224746206834\n",
      "    val_log_marginal: -12155.351220553204\n",
      "Train Epoch: 2536 [256/118836 (0%)] Loss: 12218.149414\n",
      "Train Epoch: 2536 [33024/118836 (28%)] Loss: 12189.011719\n",
      "Train Epoch: 2536 [65792/118836 (55%)] Loss: 12403.526367\n",
      "Train Epoch: 2536 [98560/118836 (83%)] Loss: 12183.156250\n",
      "    epoch          : 2536\n",
      "    loss           : 12234.96854790245\n",
      "    val_loss       : 12231.405936338875\n",
      "    val_log_likelihood: -12148.851701916614\n",
      "    val_log_marginal: -12156.877927437832\n",
      "Train Epoch: 2537 [256/118836 (0%)] Loss: 12305.250977\n",
      "Train Epoch: 2537 [33024/118836 (28%)] Loss: 12227.923828\n",
      "Train Epoch: 2537 [65792/118836 (55%)] Loss: 12163.672852\n",
      "Train Epoch: 2537 [98560/118836 (83%)] Loss: 12202.258789\n",
      "    epoch          : 2537\n",
      "    loss           : 12234.96705018352\n",
      "    val_loss       : 12237.924259986828\n",
      "    val_log_likelihood: -12148.386517137096\n",
      "    val_log_marginal: -12156.370196477385\n",
      "Train Epoch: 2538 [256/118836 (0%)] Loss: 12197.041016\n",
      "Train Epoch: 2538 [33024/118836 (28%)] Loss: 12196.055664\n",
      "Train Epoch: 2538 [65792/118836 (55%)] Loss: 12272.696289\n",
      "Train Epoch: 2538 [98560/118836 (83%)] Loss: 12308.964844\n",
      "    epoch          : 2538\n",
      "    loss           : 12233.999763977203\n",
      "    val_loss       : 12234.70534567023\n",
      "    val_log_likelihood: -12146.900083843828\n",
      "    val_log_marginal: -12154.985450271466\n",
      "Train Epoch: 2539 [256/118836 (0%)] Loss: 12178.447266\n",
      "Train Epoch: 2539 [33024/118836 (28%)] Loss: 12313.469727\n",
      "Train Epoch: 2539 [65792/118836 (55%)] Loss: 12200.463867\n",
      "Train Epoch: 2539 [98560/118836 (83%)] Loss: 12349.014648\n",
      "    epoch          : 2539\n",
      "    loss           : 12232.37328935975\n",
      "    val_loss       : 12236.210890994425\n",
      "    val_log_likelihood: -12147.457661774968\n",
      "    val_log_marginal: -12155.625379390502\n",
      "Train Epoch: 2540 [256/118836 (0%)] Loss: 12201.111328\n",
      "Train Epoch: 2540 [33024/118836 (28%)] Loss: 12222.285156\n",
      "Train Epoch: 2540 [65792/118836 (55%)] Loss: 12182.571289\n",
      "Train Epoch: 2540 [98560/118836 (83%)] Loss: 12357.303711\n",
      "    epoch          : 2540\n",
      "    loss           : 12233.00438314206\n",
      "    val_loss       : 12233.932855041006\n",
      "    val_log_likelihood: -12147.71890637924\n",
      "    val_log_marginal: -12155.785700562305\n",
      "Train Epoch: 2541 [256/118836 (0%)] Loss: 12319.957031\n",
      "Train Epoch: 2541 [33024/118836 (28%)] Loss: 12243.037109\n",
      "Train Epoch: 2541 [65792/118836 (55%)] Loss: 12271.789062\n",
      "Train Epoch: 2541 [98560/118836 (83%)] Loss: 12382.905273\n",
      "    epoch          : 2541\n",
      "    loss           : 12236.820546422663\n",
      "    val_loss       : 12241.274215042553\n",
      "    val_log_likelihood: -12146.524248798078\n",
      "    val_log_marginal: -12154.632389168743\n",
      "Train Epoch: 2542 [256/118836 (0%)] Loss: 12254.906250\n",
      "Train Epoch: 2542 [33024/118836 (28%)] Loss: 12316.613281\n",
      "Train Epoch: 2542 [65792/118836 (55%)] Loss: 12231.103516\n",
      "Train Epoch: 2542 [98560/118836 (83%)] Loss: 12355.186523\n",
      "    epoch          : 2542\n",
      "    loss           : 12235.321857068082\n",
      "    val_loss       : 12233.384217545818\n",
      "    val_log_likelihood: -12146.91725874302\n",
      "    val_log_marginal: -12155.16484526296\n",
      "Train Epoch: 2543 [256/118836 (0%)] Loss: 12270.460938\n",
      "Train Epoch: 2543 [33024/118836 (28%)] Loss: 12191.506836\n",
      "Train Epoch: 2543 [65792/118836 (55%)] Loss: 12198.074219\n",
      "Train Epoch: 2543 [98560/118836 (83%)] Loss: 12231.201172\n",
      "    epoch          : 2543\n",
      "    loss           : 12238.099669309606\n",
      "    val_loss       : 12232.339584120251\n",
      "    val_log_likelihood: -12146.099963005323\n",
      "    val_log_marginal: -12154.157623745736\n",
      "Train Epoch: 2544 [256/118836 (0%)] Loss: 12278.575195\n",
      "Train Epoch: 2544 [33024/118836 (28%)] Loss: 12201.136719\n",
      "Train Epoch: 2544 [65792/118836 (55%)] Loss: 12191.275391\n",
      "Train Epoch: 2544 [98560/118836 (83%)] Loss: 12309.621094\n",
      "    epoch          : 2544\n",
      "    loss           : 12235.911689832765\n",
      "    val_loss       : 12234.112007526765\n",
      "    val_log_likelihood: -12148.171533001188\n",
      "    val_log_marginal: -12156.22779423497\n",
      "Train Epoch: 2545 [256/118836 (0%)] Loss: 12158.238281\n",
      "Train Epoch: 2545 [33024/118836 (28%)] Loss: 12280.770508\n",
      "Train Epoch: 2545 [65792/118836 (55%)] Loss: 12166.268555\n",
      "Train Epoch: 2545 [98560/118836 (83%)] Loss: 12211.466797\n",
      "    epoch          : 2545\n",
      "    loss           : 12238.446260306813\n",
      "    val_loss       : 12232.373917372604\n",
      "    val_log_likelihood: -12146.452402069117\n",
      "    val_log_marginal: -12154.58869058635\n",
      "Train Epoch: 2546 [256/118836 (0%)] Loss: 12237.063477\n",
      "Train Epoch: 2546 [33024/118836 (28%)] Loss: 12243.526367\n",
      "Train Epoch: 2546 [65792/118836 (55%)] Loss: 12284.099609\n",
      "Train Epoch: 2546 [98560/118836 (83%)] Loss: 12256.983398\n",
      "    epoch          : 2546\n",
      "    loss           : 12236.276638266389\n",
      "    val_loss       : 12237.097975358001\n",
      "    val_log_likelihood: -12146.827344234645\n",
      "    val_log_marginal: -12154.989223759641\n",
      "Train Epoch: 2547 [256/118836 (0%)] Loss: 12314.544922\n",
      "Train Epoch: 2547 [33024/118836 (28%)] Loss: 12393.316406\n",
      "Train Epoch: 2547 [65792/118836 (55%)] Loss: 12236.404297\n",
      "Train Epoch: 2547 [98560/118836 (83%)] Loss: 12273.746094\n",
      "    epoch          : 2547\n",
      "    loss           : 12234.856598622311\n",
      "    val_loss       : 12231.415557387021\n",
      "    val_log_likelihood: -12150.55436294846\n",
      "    val_log_marginal: -12158.620159282333\n",
      "Train Epoch: 2548 [256/118836 (0%)] Loss: 12238.183594\n",
      "Train Epoch: 2548 [33024/118836 (28%)] Loss: 12215.996094\n",
      "Train Epoch: 2548 [65792/118836 (55%)] Loss: 12261.376953\n",
      "Train Epoch: 2548 [98560/118836 (83%)] Loss: 12208.541992\n",
      "    epoch          : 2548\n",
      "    loss           : 12238.667591210453\n",
      "    val_loss       : 12236.032166613362\n",
      "    val_log_likelihood: -12147.453419180367\n",
      "    val_log_marginal: -12155.562014752646\n",
      "Train Epoch: 2549 [256/118836 (0%)] Loss: 12287.736328\n",
      "Train Epoch: 2549 [33024/118836 (28%)] Loss: 12241.791016\n",
      "Train Epoch: 2549 [65792/118836 (55%)] Loss: 12422.408203\n",
      "Train Epoch: 2549 [98560/118836 (83%)] Loss: 12258.511719\n",
      "    epoch          : 2549\n",
      "    loss           : 12236.708689871537\n",
      "    val_loss       : 12234.047008183628\n",
      "    val_log_likelihood: -12147.734255776984\n",
      "    val_log_marginal: -12155.905841824753\n",
      "Train Epoch: 2550 [256/118836 (0%)] Loss: 12226.632812\n",
      "Train Epoch: 2550 [33024/118836 (28%)] Loss: 12344.128906\n",
      "Train Epoch: 2550 [65792/118836 (55%)] Loss: 12192.955078\n",
      "Train Epoch: 2550 [98560/118836 (83%)] Loss: 12285.946289\n",
      "    epoch          : 2550\n",
      "    loss           : 12232.11501596102\n",
      "    val_loss       : 12239.330084621473\n",
      "    val_log_likelihood: -12147.178212882549\n",
      "    val_log_marginal: -12155.24014153335\n",
      "Train Epoch: 2551 [256/118836 (0%)] Loss: 12245.685547\n",
      "Train Epoch: 2551 [33024/118836 (28%)] Loss: 12285.365234\n",
      "Train Epoch: 2551 [65792/118836 (55%)] Loss: 12260.806641\n",
      "Train Epoch: 2551 [98560/118836 (83%)] Loss: 12231.382812\n",
      "    epoch          : 2551\n",
      "    loss           : 12235.158436401469\n",
      "    val_loss       : 12236.364067675697\n",
      "    val_log_likelihood: -12148.323531359852\n",
      "    val_log_marginal: -12156.451743199339\n",
      "Train Epoch: 2552 [256/118836 (0%)] Loss: 12296.591797\n",
      "Train Epoch: 2552 [33024/118836 (28%)] Loss: 12278.591797\n",
      "Train Epoch: 2552 [65792/118836 (55%)] Loss: 12313.156250\n",
      "Train Epoch: 2552 [98560/118836 (83%)] Loss: 12257.898438\n",
      "    epoch          : 2552\n",
      "    loss           : 12236.941486216658\n",
      "    val_loss       : 12233.648331731623\n",
      "    val_log_likelihood: -12147.522572083075\n",
      "    val_log_marginal: -12155.640258967815\n",
      "Train Epoch: 2553 [256/118836 (0%)] Loss: 12307.056641\n",
      "Train Epoch: 2553 [33024/118836 (28%)] Loss: 12188.519531\n",
      "Train Epoch: 2553 [65792/118836 (55%)] Loss: 12163.050781\n",
      "Train Epoch: 2553 [98560/118836 (83%)] Loss: 12220.879883\n",
      "    epoch          : 2553\n",
      "    loss           : 12234.760009240592\n",
      "    val_loss       : 12239.707413865253\n",
      "    val_log_likelihood: -12146.958994229477\n",
      "    val_log_marginal: -12155.16041031164\n",
      "Train Epoch: 2554 [256/118836 (0%)] Loss: 12207.810547\n",
      "Train Epoch: 2554 [33024/118836 (28%)] Loss: 12216.203125\n",
      "Train Epoch: 2554 [65792/118836 (55%)] Loss: 12394.121094\n",
      "Train Epoch: 2554 [98560/118836 (83%)] Loss: 12248.251953\n",
      "    epoch          : 2554\n",
      "    loss           : 12235.097934921681\n",
      "    val_loss       : 12233.671340072886\n",
      "    val_log_likelihood: -12147.373231686828\n",
      "    val_log_marginal: -12155.498365608773\n",
      "Train Epoch: 2555 [256/118836 (0%)] Loss: 12232.525391\n",
      "Train Epoch: 2555 [33024/118836 (28%)] Loss: 12200.172852\n",
      "Train Epoch: 2555 [65792/118836 (55%)] Loss: 12155.389648\n",
      "Train Epoch: 2555 [98560/118836 (83%)] Loss: 12239.953125\n",
      "    epoch          : 2555\n",
      "    loss           : 12234.448700016801\n",
      "    val_loss       : 12235.946642168004\n",
      "    val_log_likelihood: -12146.225856854839\n",
      "    val_log_marginal: -12154.369964121212\n",
      "Train Epoch: 2556 [256/118836 (0%)] Loss: 12204.735352\n",
      "Train Epoch: 2556 [33024/118836 (28%)] Loss: 12249.250000\n",
      "Train Epoch: 2556 [65792/118836 (55%)] Loss: 12236.052734\n",
      "Train Epoch: 2556 [98560/118836 (83%)] Loss: 12281.530273\n",
      "    epoch          : 2556\n",
      "    loss           : 12236.179271027191\n",
      "    val_loss       : 12236.445792165645\n",
      "    val_log_likelihood: -12146.935954785722\n",
      "    val_log_marginal: -12155.034034453121\n",
      "Train Epoch: 2557 [256/118836 (0%)] Loss: 12272.717773\n",
      "Train Epoch: 2557 [33024/118836 (28%)] Loss: 12260.864258\n",
      "Train Epoch: 2557 [65792/118836 (55%)] Loss: 12301.274414\n",
      "Train Epoch: 2557 [98560/118836 (83%)] Loss: 12208.951172\n",
      "    epoch          : 2557\n",
      "    loss           : 12234.741503340829\n",
      "    val_loss       : 12236.192403894484\n",
      "    val_log_likelihood: -12148.911676101116\n",
      "    val_log_marginal: -12156.964627778849\n",
      "Train Epoch: 2558 [256/118836 (0%)] Loss: 12202.942383\n",
      "Train Epoch: 2558 [33024/118836 (28%)] Loss: 12308.521484\n",
      "Train Epoch: 2558 [65792/118836 (55%)] Loss: 12177.493164\n",
      "Train Epoch: 2558 [98560/118836 (83%)] Loss: 12289.930664\n",
      "    epoch          : 2558\n",
      "    loss           : 12239.368913487388\n",
      "    val_loss       : 12237.10720606734\n",
      "    val_log_likelihood: -12149.897866586538\n",
      "    val_log_marginal: -12157.883505135345\n",
      "Train Epoch: 2559 [256/118836 (0%)] Loss: 12237.642578\n",
      "Train Epoch: 2559 [33024/118836 (28%)] Loss: 12268.570312\n",
      "Train Epoch: 2559 [65792/118836 (55%)] Loss: 12274.552734\n",
      "Train Epoch: 2559 [98560/118836 (83%)] Loss: 12249.876953\n",
      "    epoch          : 2559\n",
      "    loss           : 12240.307031573097\n",
      "    val_loss       : 12234.50884344342\n",
      "    val_log_likelihood: -12146.423588709677\n",
      "    val_log_marginal: -12154.583293966994\n",
      "Train Epoch: 2560 [256/118836 (0%)] Loss: 12199.180664\n",
      "Train Epoch: 2560 [33024/118836 (28%)] Loss: 12251.000000\n",
      "Train Epoch: 2560 [65792/118836 (55%)] Loss: 12282.428711\n",
      "Train Epoch: 2560 [98560/118836 (83%)] Loss: 12208.966797\n",
      "    epoch          : 2560\n",
      "    loss           : 12234.124411800816\n",
      "    val_loss       : 12236.513788226257\n",
      "    val_log_likelihood: -12148.505939341658\n",
      "    val_log_marginal: -12156.6840749603\n",
      "Train Epoch: 2561 [256/118836 (0%)] Loss: 12180.105469\n",
      "Train Epoch: 2561 [33024/118836 (28%)] Loss: 12293.355469\n",
      "Train Epoch: 2561 [65792/118836 (55%)] Loss: 12235.560547\n",
      "Train Epoch: 2561 [98560/118836 (83%)] Loss: 12310.684570\n",
      "    epoch          : 2561\n",
      "    loss           : 12238.850276571546\n",
      "    val_loss       : 12232.582067522353\n",
      "    val_log_likelihood: -12146.316518849515\n",
      "    val_log_marginal: -12154.549473437766\n",
      "Train Epoch: 2562 [256/118836 (0%)] Loss: 12246.460938\n",
      "Train Epoch: 2562 [33024/118836 (28%)] Loss: 12198.037109\n",
      "Train Epoch: 2562 [65792/118836 (55%)] Loss: 12282.305664\n",
      "Train Epoch: 2562 [98560/118836 (83%)] Loss: 12219.000000\n",
      "    epoch          : 2562\n",
      "    loss           : 12235.806074558002\n",
      "    val_loss       : 12236.393770725277\n",
      "    val_log_likelihood: -12147.783790516438\n",
      "    val_log_marginal: -12155.976079469241\n",
      "Train Epoch: 2563 [256/118836 (0%)] Loss: 12281.326172\n",
      "Train Epoch: 2563 [33024/118836 (28%)] Loss: 12218.521484\n",
      "Train Epoch: 2563 [65792/118836 (55%)] Loss: 12209.200195\n",
      "Train Epoch: 2563 [98560/118836 (83%)] Loss: 12269.259766\n",
      "    epoch          : 2563\n",
      "    loss           : 12234.049699196134\n",
      "    val_loss       : 12231.421434971011\n",
      "    val_log_likelihood: -12147.747643326096\n",
      "    val_log_marginal: -12155.893709299691\n",
      "Train Epoch: 2564 [256/118836 (0%)] Loss: 12258.050781\n",
      "Train Epoch: 2564 [33024/118836 (28%)] Loss: 12296.637695\n",
      "Train Epoch: 2564 [65792/118836 (55%)] Loss: 12318.158203\n",
      "Train Epoch: 2564 [98560/118836 (83%)] Loss: 12182.422852\n",
      "    epoch          : 2564\n",
      "    loss           : 12239.834298910515\n",
      "    val_loss       : 12237.470253983292\n",
      "    val_log_likelihood: -12148.023663022124\n",
      "    val_log_marginal: -12156.228154618884\n",
      "Train Epoch: 2565 [256/118836 (0%)] Loss: 12288.296875\n",
      "Train Epoch: 2565 [33024/118836 (28%)] Loss: 12234.656250\n",
      "Train Epoch: 2565 [65792/118836 (55%)] Loss: 12285.238281\n",
      "Train Epoch: 2565 [98560/118836 (83%)] Loss: 12484.361328\n",
      "    epoch          : 2565\n",
      "    loss           : 12236.76676456524\n",
      "    val_loss       : 12232.811451035706\n",
      "    val_log_likelihood: -12148.538721470482\n",
      "    val_log_marginal: -12156.676211889811\n",
      "Train Epoch: 2566 [256/118836 (0%)] Loss: 12280.486328\n",
      "Train Epoch: 2566 [33024/118836 (28%)] Loss: 12177.925781\n",
      "Train Epoch: 2566 [65792/118836 (55%)] Loss: 12294.224609\n",
      "Train Epoch: 2566 [98560/118836 (83%)] Loss: 12299.078125\n",
      "    epoch          : 2566\n",
      "    loss           : 12238.191346153846\n",
      "    val_loss       : 12236.982259707147\n",
      "    val_log_likelihood: -12148.452410792752\n",
      "    val_log_marginal: -12156.684948606431\n",
      "Train Epoch: 2567 [256/118836 (0%)] Loss: 12205.521484\n",
      "Train Epoch: 2567 [33024/118836 (28%)] Loss: 12234.412109\n",
      "Train Epoch: 2567 [65792/118836 (55%)] Loss: 12178.708008\n",
      "Train Epoch: 2567 [98560/118836 (83%)] Loss: 12283.220703\n",
      "    epoch          : 2567\n",
      "    loss           : 12237.433949480459\n",
      "    val_loss       : 12237.119283409755\n",
      "    val_log_likelihood: -12147.933866444377\n",
      "    val_log_marginal: -12156.151994299305\n",
      "Train Epoch: 2568 [256/118836 (0%)] Loss: 12361.950195\n",
      "Train Epoch: 2568 [33024/118836 (28%)] Loss: 12209.289062\n",
      "Train Epoch: 2568 [65792/118836 (55%)] Loss: 12264.010742\n",
      "Train Epoch: 2568 [98560/118836 (83%)] Loss: 12340.055664\n",
      "    epoch          : 2568\n",
      "    loss           : 12234.637515185586\n",
      "    val_loss       : 12234.049289317392\n",
      "    val_log_likelihood: -12148.042366334521\n",
      "    val_log_marginal: -12156.250050800461\n",
      "Train Epoch: 2569 [256/118836 (0%)] Loss: 12407.689453\n",
      "Train Epoch: 2569 [33024/118836 (28%)] Loss: 12336.690430\n",
      "Train Epoch: 2569 [65792/118836 (55%)] Loss: 12227.211914\n",
      "Train Epoch: 2569 [98560/118836 (83%)] Loss: 12296.638672\n",
      "    epoch          : 2569\n",
      "    loss           : 12233.594064373965\n",
      "    val_loss       : 12240.25588927036\n",
      "    val_log_likelihood: -12148.082609917805\n",
      "    val_log_marginal: -12156.21272025768\n",
      "Train Epoch: 2570 [256/118836 (0%)] Loss: 12347.284180\n",
      "Train Epoch: 2570 [33024/118836 (28%)] Loss: 12342.914062\n",
      "Train Epoch: 2570 [65792/118836 (55%)] Loss: 12352.845703\n",
      "Train Epoch: 2570 [98560/118836 (83%)] Loss: 12246.660156\n",
      "    epoch          : 2570\n",
      "    loss           : 12237.599953473946\n",
      "    val_loss       : 12242.633169026463\n",
      "    val_log_likelihood: -12146.670151920493\n",
      "    val_log_marginal: -12154.849969107354\n",
      "Train Epoch: 2571 [256/118836 (0%)] Loss: 12251.310547\n",
      "Train Epoch: 2571 [33024/118836 (28%)] Loss: 12146.675781\n",
      "Train Epoch: 2571 [65792/118836 (55%)] Loss: 12379.830078\n",
      "Train Epoch: 2571 [98560/118836 (83%)] Loss: 12270.938477\n",
      "    epoch          : 2571\n",
      "    loss           : 12241.83634702621\n",
      "    val_loss       : 12235.913656012406\n",
      "    val_log_likelihood: -12146.672019909274\n",
      "    val_log_marginal: -12154.86524232681\n",
      "Train Epoch: 2572 [256/118836 (0%)] Loss: 12325.718750\n",
      "Train Epoch: 2572 [33024/118836 (28%)] Loss: 12259.595703\n",
      "Train Epoch: 2572 [65792/118836 (55%)] Loss: 12249.463867\n",
      "Train Epoch: 2572 [98560/118836 (83%)] Loss: 12195.507812\n",
      "    epoch          : 2572\n",
      "    loss           : 12240.530577633892\n",
      "    val_loss       : 12227.871498049288\n",
      "    val_log_likelihood: -12148.913146195202\n",
      "    val_log_marginal: -12157.021759142996\n",
      "Train Epoch: 2573 [256/118836 (0%)] Loss: 12322.370117\n",
      "Train Epoch: 2573 [33024/118836 (28%)] Loss: 12163.311523\n",
      "Train Epoch: 2573 [65792/118836 (55%)] Loss: 12251.815430\n",
      "Train Epoch: 2573 [98560/118836 (83%)] Loss: 12347.552734\n",
      "    epoch          : 2573\n",
      "    loss           : 12238.467117226272\n",
      "    val_loss       : 12235.648816428715\n",
      "    val_log_likelihood: -12148.544340299319\n",
      "    val_log_marginal: -12156.800770192947\n",
      "Train Epoch: 2574 [256/118836 (0%)] Loss: 12297.797852\n",
      "Train Epoch: 2574 [33024/118836 (28%)] Loss: 12202.975586\n",
      "Train Epoch: 2574 [65792/118836 (55%)] Loss: 12243.607422\n",
      "Train Epoch: 2574 [98560/118836 (83%)] Loss: 12182.647461\n",
      "    epoch          : 2574\n",
      "    loss           : 12232.593122382908\n",
      "    val_loss       : 12233.395431879686\n",
      "    val_log_likelihood: -12148.925598699856\n",
      "    val_log_marginal: -12157.174395899347\n",
      "Train Epoch: 2575 [256/118836 (0%)] Loss: 12402.551758\n",
      "Train Epoch: 2575 [33024/118836 (28%)] Loss: 12176.111328\n",
      "Train Epoch: 2575 [65792/118836 (55%)] Loss: 12190.449219\n",
      "Train Epoch: 2575 [98560/118836 (83%)] Loss: 12353.542969\n",
      "    epoch          : 2575\n",
      "    loss           : 12224.50846709574\n",
      "    val_loss       : 12226.022400332262\n",
      "    val_log_likelihood: -12147.244027863937\n",
      "    val_log_marginal: -12155.458984210874\n",
      "Train Epoch: 2576 [256/118836 (0%)] Loss: 12141.083008\n",
      "Train Epoch: 2576 [33024/118836 (28%)] Loss: 12231.348633\n",
      "Train Epoch: 2576 [65792/118836 (55%)] Loss: 12213.795898\n",
      "Train Epoch: 2576 [98560/118836 (83%)] Loss: 12315.533203\n",
      "    epoch          : 2576\n",
      "    loss           : 12221.801880266492\n",
      "    val_loss       : 12223.54005243913\n",
      "    val_log_likelihood: -12147.474028283965\n",
      "    val_log_marginal: -12155.726551385676\n",
      "Train Epoch: 2577 [256/118836 (0%)] Loss: 12306.198242\n",
      "Train Epoch: 2577 [33024/118836 (28%)] Loss: 12293.643555\n",
      "Train Epoch: 2577 [65792/118836 (55%)] Loss: 12355.052734\n",
      "Train Epoch: 2577 [98560/118836 (83%)] Loss: 12219.877930\n",
      "    epoch          : 2577\n",
      "    loss           : 12225.08954585401\n",
      "    val_loss       : 12221.377591708282\n",
      "    val_log_likelihood: -12146.34707305883\n",
      "    val_log_marginal: -12154.60653036987\n",
      "Train Epoch: 2578 [256/118836 (0%)] Loss: 12192.722656\n",
      "Train Epoch: 2578 [33024/118836 (28%)] Loss: 12167.223633\n",
      "Train Epoch: 2578 [65792/118836 (55%)] Loss: 12235.228516\n",
      "Train Epoch: 2578 [98560/118836 (83%)] Loss: 12329.124023\n",
      "    epoch          : 2578\n",
      "    loss           : 12224.775531980202\n",
      "    val_loss       : 12219.683355435509\n",
      "    val_log_likelihood: -12145.85768988446\n",
      "    val_log_marginal: -12153.90874452068\n",
      "Train Epoch: 2579 [256/118836 (0%)] Loss: 12232.078125\n",
      "Train Epoch: 2579 [33024/118836 (28%)] Loss: 12176.695312\n",
      "Train Epoch: 2579 [65792/118836 (55%)] Loss: 12182.888672\n",
      "Train Epoch: 2579 [98560/118836 (83%)] Loss: 12278.724609\n",
      "    epoch          : 2579\n",
      "    loss           : 12220.435365778794\n",
      "    val_loss       : 12220.304599316569\n",
      "    val_log_likelihood: -12145.495973557692\n",
      "    val_log_marginal: -12153.600138627073\n",
      "Train Epoch: 2580 [256/118836 (0%)] Loss: 12237.372070\n",
      "Train Epoch: 2580 [33024/118836 (28%)] Loss: 12313.682617\n",
      "Train Epoch: 2580 [65792/118836 (55%)] Loss: 12199.220703\n",
      "Train Epoch: 2580 [98560/118836 (83%)] Loss: 12273.819336\n",
      "    epoch          : 2580\n",
      "    loss           : 12226.056035463194\n",
      "    val_loss       : 12224.638900600863\n",
      "    val_log_likelihood: -12145.911775938275\n",
      "    val_log_marginal: -12154.04997310219\n",
      "Train Epoch: 2581 [256/118836 (0%)] Loss: 12265.000000\n",
      "Train Epoch: 2581 [33024/118836 (28%)] Loss: 12257.499023\n",
      "Train Epoch: 2581 [65792/118836 (55%)] Loss: 12276.833984\n",
      "Train Epoch: 2581 [98560/118836 (83%)] Loss: 12179.620117\n",
      "    epoch          : 2581\n",
      "    loss           : 12223.03007796345\n",
      "    val_loss       : 12225.221615082262\n",
      "    val_log_likelihood: -12147.768916556814\n",
      "    val_log_marginal: -12155.859033866778\n",
      "Train Epoch: 2582 [256/118836 (0%)] Loss: 12210.802734\n",
      "Train Epoch: 2582 [33024/118836 (28%)] Loss: 12257.003906\n",
      "Train Epoch: 2582 [65792/118836 (55%)] Loss: 12207.682617\n",
      "Train Epoch: 2582 [98560/118836 (83%)] Loss: 12301.890625\n",
      "    epoch          : 2582\n",
      "    loss           : 12220.319353707868\n",
      "    val_loss       : 12226.177088413095\n",
      "    val_log_likelihood: -12149.183670001034\n",
      "    val_log_marginal: -12157.452452600328\n",
      "Train Epoch: 2583 [256/118836 (0%)] Loss: 12251.966797\n",
      "Train Epoch: 2583 [33024/118836 (28%)] Loss: 12244.375000\n",
      "Train Epoch: 2583 [65792/118836 (55%)] Loss: 12209.140625\n",
      "Train Epoch: 2583 [98560/118836 (83%)] Loss: 12190.800781\n",
      "    epoch          : 2583\n",
      "    loss           : 12222.561468672457\n",
      "    val_loss       : 12225.273596598952\n",
      "    val_log_likelihood: -12147.787374476582\n",
      "    val_log_marginal: -12156.20426124119\n",
      "Train Epoch: 2584 [256/118836 (0%)] Loss: 12193.760742\n",
      "Train Epoch: 2584 [33024/118836 (28%)] Loss: 12204.429688\n",
      "Train Epoch: 2584 [65792/118836 (55%)] Loss: 12159.064453\n",
      "Train Epoch: 2584 [98560/118836 (83%)] Loss: 12206.721680\n",
      "    epoch          : 2584\n",
      "    loss           : 12225.165934204404\n",
      "    val_loss       : 12231.068149195245\n",
      "    val_log_likelihood: -12152.820212016646\n",
      "    val_log_marginal: -12161.419279867907\n",
      "Train Epoch: 2585 [256/118836 (0%)] Loss: 12197.072266\n",
      "Train Epoch: 2585 [33024/118836 (28%)] Loss: 12265.220703\n",
      "Train Epoch: 2585 [65792/118836 (55%)] Loss: 12287.090820\n",
      "Train Epoch: 2585 [98560/118836 (83%)] Loss: 12267.894531\n",
      "    epoch          : 2585\n",
      "    loss           : 12226.66547136709\n",
      "    val_loss       : 12222.678353060917\n",
      "    val_log_likelihood: -12145.498927315963\n",
      "    val_log_marginal: -12153.870354453771\n",
      "Train Epoch: 2586 [256/118836 (0%)] Loss: 12384.896484\n",
      "Train Epoch: 2586 [33024/118836 (28%)] Loss: 12180.453125\n",
      "Train Epoch: 2586 [65792/118836 (55%)] Loss: 12207.761719\n",
      "Train Epoch: 2586 [98560/118836 (83%)] Loss: 12146.144531\n",
      "    epoch          : 2586\n",
      "    loss           : 12227.481251615489\n",
      "    val_loss       : 12235.56091839974\n",
      "    val_log_likelihood: -12150.80629846464\n",
      "    val_log_marginal: -12159.433435131308\n",
      "Train Epoch: 2587 [256/118836 (0%)] Loss: 12305.037109\n",
      "Train Epoch: 2587 [33024/118836 (28%)] Loss: 12249.316406\n",
      "Train Epoch: 2587 [65792/118836 (55%)] Loss: 12234.496094\n",
      "Train Epoch: 2587 [98560/118836 (83%)] Loss: 12377.996094\n",
      "    epoch          : 2587\n",
      "    loss           : 12225.469486985628\n",
      "    val_loss       : 12225.798464866248\n",
      "    val_log_likelihood: -12147.966536458332\n",
      "    val_log_marginal: -12156.399388908632\n",
      "Train Epoch: 2588 [256/118836 (0%)] Loss: 12216.021484\n",
      "Train Epoch: 2588 [33024/118836 (28%)] Loss: 12192.602539\n",
      "Train Epoch: 2588 [65792/118836 (55%)] Loss: 12223.355469\n",
      "Train Epoch: 2588 [98560/118836 (83%)] Loss: 12325.135742\n",
      "    epoch          : 2588\n",
      "    loss           : 12221.815925642317\n",
      "    val_loss       : 12227.090741468537\n",
      "    val_log_likelihood: -12147.337843452751\n",
      "    val_log_marginal: -12155.596843160205\n",
      "Train Epoch: 2589 [256/118836 (0%)] Loss: 12193.659180\n",
      "Train Epoch: 2589 [33024/118836 (28%)] Loss: 12293.826172\n",
      "Train Epoch: 2589 [65792/118836 (55%)] Loss: 12379.684570\n",
      "Train Epoch: 2589 [98560/118836 (83%)] Loss: 12280.626953\n",
      "    epoch          : 2589\n",
      "    loss           : 12227.343276662015\n",
      "    val_loss       : 12244.53151189806\n",
      "    val_log_likelihood: -12152.578769741263\n",
      "    val_log_marginal: -12161.180076542634\n",
      "Train Epoch: 2590 [256/118836 (0%)] Loss: 12247.600586\n",
      "Train Epoch: 2590 [33024/118836 (28%)] Loss: 12171.878906\n",
      "Train Epoch: 2590 [65792/118836 (55%)] Loss: 12337.639648\n",
      "Train Epoch: 2590 [98560/118836 (83%)] Loss: 12216.037109\n",
      "    epoch          : 2590\n",
      "    loss           : 12226.59149106312\n",
      "    val_loss       : 12225.906620913114\n",
      "    val_log_likelihood: -12147.165480575371\n",
      "    val_log_marginal: -12155.64034598215\n",
      "Train Epoch: 2591 [256/118836 (0%)] Loss: 12227.714844\n",
      "Train Epoch: 2591 [33024/118836 (28%)] Loss: 12205.286133\n",
      "Train Epoch: 2591 [65792/118836 (55%)] Loss: 12178.630859\n",
      "Train Epoch: 2591 [98560/118836 (83%)] Loss: 12269.970703\n",
      "    epoch          : 2591\n",
      "    loss           : 12220.641720462418\n",
      "    val_loss       : 12223.704500691709\n",
      "    val_log_likelihood: -12145.206959037687\n",
      "    val_log_marginal: -12153.584214283592\n",
      "Train Epoch: 2592 [256/118836 (0%)] Loss: 12208.391602\n",
      "Train Epoch: 2592 [33024/118836 (28%)] Loss: 12175.048828\n",
      "Train Epoch: 2592 [65792/118836 (55%)] Loss: 12168.056641\n",
      "Train Epoch: 2592 [98560/118836 (83%)] Loss: 12318.306641\n",
      "    epoch          : 2592\n",
      "    loss           : 12222.896956905242\n",
      "    val_loss       : 12227.223748208975\n",
      "    val_log_likelihood: -12149.026148611973\n",
      "    val_log_marginal: -12157.615049526752\n",
      "Train Epoch: 2593 [256/118836 (0%)] Loss: 12193.835938\n",
      "Train Epoch: 2593 [33024/118836 (28%)] Loss: 12230.352539\n",
      "Train Epoch: 2593 [65792/118836 (55%)] Loss: 12249.824219\n",
      "Train Epoch: 2593 [98560/118836 (83%)] Loss: 12286.945312\n",
      "    epoch          : 2593\n",
      "    loss           : 12220.694538681246\n",
      "    val_loss       : 12226.128901081947\n",
      "    val_log_likelihood: -12146.722269986818\n",
      "    val_log_marginal: -12155.081952982326\n",
      "Train Epoch: 2594 [256/118836 (0%)] Loss: 12399.097656\n",
      "Train Epoch: 2594 [33024/118836 (28%)] Loss: 12250.978516\n",
      "Train Epoch: 2594 [65792/118836 (55%)] Loss: 12199.998047\n",
      "Train Epoch: 2594 [98560/118836 (83%)] Loss: 12211.115234\n",
      "    epoch          : 2594\n",
      "    loss           : 12225.58036374328\n",
      "    val_loss       : 12224.736292931195\n",
      "    val_log_likelihood: -12149.025821314102\n",
      "    val_log_marginal: -12157.662258616545\n",
      "Train Epoch: 2595 [256/118836 (0%)] Loss: 12229.355469\n",
      "Train Epoch: 2595 [33024/118836 (28%)] Loss: 12178.859375\n",
      "Train Epoch: 2595 [65792/118836 (55%)] Loss: 12250.123047\n",
      "Train Epoch: 2595 [98560/118836 (83%)] Loss: 12290.306641\n",
      "    epoch          : 2595\n",
      "    loss           : 12222.429945654983\n",
      "    val_loss       : 12224.399123888143\n",
      "    val_log_likelihood: -12144.356778103029\n",
      "    val_log_marginal: -12152.67676306565\n",
      "Train Epoch: 2596 [256/118836 (0%)] Loss: 12230.091797\n",
      "Train Epoch: 2596 [33024/118836 (28%)] Loss: 12230.577148\n",
      "Train Epoch: 2596 [65792/118836 (55%)] Loss: 12211.797852\n",
      "Train Epoch: 2596 [98560/118836 (83%)] Loss: 12202.425781\n",
      "    epoch          : 2596\n",
      "    loss           : 12224.306130776986\n",
      "    val_loss       : 12223.02999940786\n",
      "    val_log_likelihood: -12143.692235803092\n",
      "    val_log_marginal: -12151.86061621218\n",
      "Train Epoch: 2597 [256/118836 (0%)] Loss: 12186.777344\n",
      "Train Epoch: 2597 [33024/118836 (28%)] Loss: 12178.212891\n",
      "Train Epoch: 2597 [65792/118836 (55%)] Loss: 12235.704102\n",
      "Train Epoch: 2597 [98560/118836 (83%)] Loss: 12328.647461\n",
      "    epoch          : 2597\n",
      "    loss           : 12223.851356848378\n",
      "    val_loss       : 12231.103726471398\n",
      "    val_log_likelihood: -12152.74726966372\n",
      "    val_log_marginal: -12161.325456730974\n",
      "Train Epoch: 2598 [256/118836 (0%)] Loss: 12184.337891\n",
      "Train Epoch: 2598 [33024/118836 (28%)] Loss: 12196.875000\n",
      "Train Epoch: 2598 [65792/118836 (55%)] Loss: 12198.431641\n",
      "Train Epoch: 2598 [98560/118836 (83%)] Loss: 12280.342773\n",
      "    epoch          : 2598\n",
      "    loss           : 12218.639056038048\n",
      "    val_loss       : 12219.10679741856\n",
      "    val_log_likelihood: -12143.184943651779\n",
      "    val_log_marginal: -12151.697848046564\n",
      "Train Epoch: 2599 [256/118836 (0%)] Loss: 12143.082031\n",
      "Train Epoch: 2599 [33024/118836 (28%)] Loss: 12188.764648\n",
      "Train Epoch: 2599 [65792/118836 (55%)] Loss: 12227.654297\n",
      "Train Epoch: 2599 [98560/118836 (83%)] Loss: 12282.644531\n",
      "    epoch          : 2599\n",
      "    loss           : 12219.079886528123\n",
      "    val_loss       : 12224.190952202483\n",
      "    val_log_likelihood: -12150.780053408032\n",
      "    val_log_marginal: -12159.314379212767\n",
      "Train Epoch: 2600 [256/118836 (0%)] Loss: 12245.667969\n",
      "Train Epoch: 2600 [33024/118836 (28%)] Loss: 12197.482422\n",
      "Train Epoch: 2600 [65792/118836 (55%)] Loss: 12274.630859\n",
      "Train Epoch: 2600 [98560/118836 (83%)] Loss: 12263.032227\n",
      "    epoch          : 2600\n",
      "    loss           : 12223.640384453836\n",
      "    val_loss       : 12228.957944533315\n",
      "    val_log_likelihood: -12152.832533505221\n",
      "    val_log_marginal: -12161.669927426834\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch2600.pth ...\n",
      "Train Epoch: 2601 [256/118836 (0%)] Loss: 12352.329102\n",
      "Train Epoch: 2601 [33024/118836 (28%)] Loss: 12193.458984\n",
      "Train Epoch: 2601 [65792/118836 (55%)] Loss: 12251.188477\n",
      "Train Epoch: 2601 [98560/118836 (83%)] Loss: 12199.312500\n",
      "    epoch          : 2601\n",
      "    loss           : 12229.073823278535\n",
      "    val_loss       : 12222.377836388845\n",
      "    val_log_likelihood: -12144.658695848842\n",
      "    val_log_marginal: -12153.403047050526\n",
      "Train Epoch: 2602 [256/118836 (0%)] Loss: 12356.885742\n",
      "Train Epoch: 2602 [33024/118836 (28%)] Loss: 12245.726562\n",
      "Train Epoch: 2602 [65792/118836 (55%)] Loss: 12221.543945\n",
      "Train Epoch: 2602 [98560/118836 (83%)] Loss: 12239.945312\n",
      "    epoch          : 2602\n",
      "    loss           : 12222.102860383064\n",
      "    val_loss       : 12231.769970584426\n",
      "    val_log_likelihood: -12153.875785611819\n",
      "    val_log_marginal: -12162.332279285813\n",
      "Train Epoch: 2603 [256/118836 (0%)] Loss: 12275.370117\n",
      "Train Epoch: 2603 [33024/118836 (28%)] Loss: 12237.494141\n",
      "Train Epoch: 2603 [65792/118836 (55%)] Loss: 12246.799805\n",
      "Train Epoch: 2603 [98560/118836 (83%)] Loss: 12261.119141\n",
      "    epoch          : 2603\n",
      "    loss           : 12220.858004904623\n",
      "    val_loss       : 12220.862414989986\n",
      "    val_log_likelihood: -12140.834063533912\n",
      "    val_log_marginal: -12149.240330542498\n",
      "Train Epoch: 2604 [256/118836 (0%)] Loss: 12254.899414\n",
      "Train Epoch: 2604 [33024/118836 (28%)] Loss: 12243.678711\n",
      "Train Epoch: 2604 [65792/118836 (55%)] Loss: 12281.825195\n",
      "Train Epoch: 2604 [98560/118836 (83%)] Loss: 12167.327148\n",
      "    epoch          : 2604\n",
      "    loss           : 12234.893695558056\n",
      "    val_loss       : 12230.603540350301\n",
      "    val_log_likelihood: -12150.8423086616\n",
      "    val_log_marginal: -12159.576013224389\n",
      "Train Epoch: 2605 [256/118836 (0%)] Loss: 12170.429688\n",
      "Train Epoch: 2605 [33024/118836 (28%)] Loss: 12202.939453\n",
      "Train Epoch: 2605 [65792/118836 (55%)] Loss: 12268.468750\n",
      "Train Epoch: 2605 [98560/118836 (83%)] Loss: 12224.459961\n",
      "    epoch          : 2605\n",
      "    loss           : 12229.580498151881\n",
      "    val_loss       : 12230.73860918859\n",
      "    val_log_likelihood: -12151.94463641827\n",
      "    val_log_marginal: -12160.463840881448\n",
      "Train Epoch: 2606 [256/118836 (0%)] Loss: 12269.597656\n",
      "Train Epoch: 2606 [33024/118836 (28%)] Loss: 12271.631836\n",
      "Train Epoch: 2606 [65792/118836 (55%)] Loss: 12183.344727\n",
      "Train Epoch: 2606 [98560/118836 (83%)] Loss: 12216.452148\n",
      "    epoch          : 2606\n",
      "    loss           : 12224.286405506877\n",
      "    val_loss       : 12219.590184830502\n",
      "    val_log_likelihood: -12143.971625084005\n",
      "    val_log_marginal: -12152.280663288411\n",
      "Train Epoch: 2607 [256/118836 (0%)] Loss: 12272.644531\n",
      "Train Epoch: 2607 [33024/118836 (28%)] Loss: 12215.244141\n",
      "Train Epoch: 2607 [65792/118836 (55%)] Loss: 12215.945312\n",
      "Train Epoch: 2607 [98560/118836 (83%)] Loss: 12271.462891\n",
      "    epoch          : 2607\n",
      "    loss           : 12222.540849875932\n",
      "    val_loss       : 12226.624932327899\n",
      "    val_log_likelihood: -12145.135738471878\n",
      "    val_log_marginal: -12153.51763560884\n",
      "Train Epoch: 2608 [256/118836 (0%)] Loss: 12350.529297\n",
      "Train Epoch: 2608 [33024/118836 (28%)] Loss: 12258.953125\n",
      "Train Epoch: 2608 [65792/118836 (55%)] Loss: 12411.128906\n",
      "Train Epoch: 2608 [98560/118836 (83%)] Loss: 12242.110352\n",
      "    epoch          : 2608\n",
      "    loss           : 12219.212668172302\n",
      "    val_loss       : 12228.955377991344\n",
      "    val_log_likelihood: -12147.53326903691\n",
      "    val_log_marginal: -12155.950006341738\n",
      "Train Epoch: 2609 [256/118836 (0%)] Loss: 12255.152344\n",
      "Train Epoch: 2609 [33024/118836 (28%)] Loss: 12177.591797\n",
      "Train Epoch: 2609 [65792/118836 (55%)] Loss: 12258.065430\n",
      "Train Epoch: 2609 [98560/118836 (83%)] Loss: 12240.581055\n",
      "    epoch          : 2609\n",
      "    loss           : 12221.540999147022\n",
      "    val_loss       : 12221.155659995406\n",
      "    val_log_likelihood: -12142.444386179177\n",
      "    val_log_marginal: -12150.888940882036\n",
      "Train Epoch: 2610 [256/118836 (0%)] Loss: 12270.137695\n",
      "Train Epoch: 2610 [33024/118836 (28%)] Loss: 12191.091797\n",
      "Train Epoch: 2610 [65792/118836 (55%)] Loss: 12255.849609\n",
      "Train Epoch: 2610 [98560/118836 (83%)] Loss: 12167.235352\n",
      "    epoch          : 2610\n",
      "    loss           : 12222.734599229736\n",
      "    val_loss       : 12222.220447605223\n",
      "    val_log_likelihood: -12141.216512387562\n",
      "    val_log_marginal: -12149.767792464054\n",
      "Train Epoch: 2611 [256/118836 (0%)] Loss: 12307.060547\n",
      "Train Epoch: 2611 [33024/118836 (28%)] Loss: 12202.855469\n",
      "Train Epoch: 2611 [65792/118836 (55%)] Loss: 12157.959961\n",
      "Train Epoch: 2611 [98560/118836 (83%)] Loss: 12227.841797\n",
      "    epoch          : 2611\n",
      "    loss           : 12222.980256636425\n",
      "    val_loss       : 12220.26404963789\n",
      "    val_log_likelihood: -12141.197680805417\n",
      "    val_log_marginal: -12149.550927699727\n",
      "Train Epoch: 2612 [256/118836 (0%)] Loss: 12351.601562\n",
      "Train Epoch: 2612 [33024/118836 (28%)] Loss: 12221.583984\n",
      "Train Epoch: 2612 [65792/118836 (55%)] Loss: 12328.936523\n",
      "Train Epoch: 2612 [98560/118836 (83%)] Loss: 12227.954102\n",
      "    epoch          : 2612\n",
      "    loss           : 12221.775079320461\n",
      "    val_loss       : 12218.50009005894\n",
      "    val_log_likelihood: -12143.018184740748\n",
      "    val_log_marginal: -12151.320565600681\n",
      "Train Epoch: 2613 [256/118836 (0%)] Loss: 12272.044922\n",
      "Train Epoch: 2613 [33024/118836 (28%)] Loss: 12156.548828\n",
      "Train Epoch: 2613 [65792/118836 (55%)] Loss: 12245.877930\n",
      "Train Epoch: 2613 [98560/118836 (83%)] Loss: 12268.951172\n",
      "    epoch          : 2613\n",
      "    loss           : 12221.051114202079\n",
      "    val_loss       : 12227.427805323101\n",
      "    val_log_likelihood: -12145.419921067258\n",
      "    val_log_marginal: -12154.121850204076\n",
      "Train Epoch: 2614 [256/118836 (0%)] Loss: 12274.594727\n",
      "Train Epoch: 2614 [33024/118836 (28%)] Loss: 12196.120117\n",
      "Train Epoch: 2614 [65792/118836 (55%)] Loss: 12272.013672\n",
      "Train Epoch: 2614 [98560/118836 (83%)] Loss: 12208.269531\n",
      "    epoch          : 2614\n",
      "    loss           : 12223.453591876034\n",
      "    val_loss       : 12222.18445701262\n",
      "    val_log_likelihood: -12144.813497725394\n",
      "    val_log_marginal: -12153.278614598988\n",
      "Train Epoch: 2615 [256/118836 (0%)] Loss: 12306.434570\n",
      "Train Epoch: 2615 [33024/118836 (28%)] Loss: 12156.343750\n",
      "Train Epoch: 2615 [65792/118836 (55%)] Loss: 12164.981445\n",
      "Train Epoch: 2615 [98560/118836 (83%)] Loss: 12301.792969\n",
      "    epoch          : 2615\n",
      "    loss           : 12219.15985835401\n",
      "    val_loss       : 12226.596641781582\n",
      "    val_log_likelihood: -12143.278827414186\n",
      "    val_log_marginal: -12151.591853280532\n",
      "Train Epoch: 2616 [256/118836 (0%)] Loss: 12211.799805\n",
      "Train Epoch: 2616 [33024/118836 (28%)] Loss: 12296.805664\n",
      "Train Epoch: 2616 [65792/118836 (55%)] Loss: 12260.800781\n",
      "Train Epoch: 2616 [98560/118836 (83%)] Loss: 12260.959961\n",
      "    epoch          : 2616\n",
      "    loss           : 12219.45582318807\n",
      "    val_loss       : 12221.600698798276\n",
      "    val_log_likelihood: -12142.64360024426\n",
      "    val_log_marginal: -12151.000661466029\n",
      "Train Epoch: 2617 [256/118836 (0%)] Loss: 12143.499023\n",
      "Train Epoch: 2617 [33024/118836 (28%)] Loss: 12169.322266\n",
      "Train Epoch: 2617 [65792/118836 (55%)] Loss: 12264.820312\n",
      "Train Epoch: 2617 [98560/118836 (83%)] Loss: 12198.782227\n",
      "    epoch          : 2617\n",
      "    loss           : 12220.501274943135\n",
      "    val_loss       : 12223.921177571647\n",
      "    val_log_likelihood: -12143.41780154699\n",
      "    val_log_marginal: -12151.876572327435\n",
      "Train Epoch: 2618 [256/118836 (0%)] Loss: 12385.428711\n",
      "Train Epoch: 2618 [33024/118836 (28%)] Loss: 12281.688477\n",
      "Train Epoch: 2618 [65792/118836 (55%)] Loss: 12331.689453\n",
      "Train Epoch: 2618 [98560/118836 (83%)] Loss: 12292.669922\n",
      "    epoch          : 2618\n",
      "    loss           : 12218.49405468104\n",
      "    val_loss       : 12224.217476430911\n",
      "    val_log_likelihood: -12145.727410631203\n",
      "    val_log_marginal: -12154.19717256087\n",
      "Train Epoch: 2619 [256/118836 (0%)] Loss: 12213.690430\n",
      "Train Epoch: 2619 [33024/118836 (28%)] Loss: 12301.884766\n",
      "Train Epoch: 2619 [65792/118836 (55%)] Loss: 12159.257812\n",
      "Train Epoch: 2619 [98560/118836 (83%)] Loss: 12232.958984\n",
      "    epoch          : 2619\n",
      "    loss           : 12225.986529414806\n",
      "    val_loss       : 12217.63235292094\n",
      "    val_log_likelihood: -12142.01948003903\n",
      "    val_log_marginal: -12150.443064715719\n",
      "Train Epoch: 2620 [256/118836 (0%)] Loss: 12157.162109\n",
      "Train Epoch: 2620 [33024/118836 (28%)] Loss: 12240.650391\n",
      "Train Epoch: 2620 [65792/118836 (55%)] Loss: 12187.750000\n",
      "Train Epoch: 2620 [98560/118836 (83%)] Loss: 12194.693359\n",
      "    epoch          : 2620\n",
      "    loss           : 12221.223526028742\n",
      "    val_loss       : 12220.313890007974\n",
      "    val_log_likelihood: -12142.494845947065\n",
      "    val_log_marginal: -12150.937856180664\n",
      "Train Epoch: 2621 [256/118836 (0%)] Loss: 12350.819336\n",
      "Train Epoch: 2621 [33024/118836 (28%)] Loss: 12179.202148\n",
      "Train Epoch: 2621 [65792/118836 (55%)] Loss: 12222.818359\n",
      "Train Epoch: 2621 [98560/118836 (83%)] Loss: 12263.420898\n",
      "    epoch          : 2621\n",
      "    loss           : 12221.21563840855\n",
      "    val_loss       : 12224.114175714394\n",
      "    val_log_likelihood: -12143.693004290735\n",
      "    val_log_marginal: -12152.08075103551\n",
      "Train Epoch: 2622 [256/118836 (0%)] Loss: 12218.285156\n",
      "Train Epoch: 2622 [33024/118836 (28%)] Loss: 12285.832031\n",
      "Train Epoch: 2622 [65792/118836 (55%)] Loss: 12330.235352\n",
      "Train Epoch: 2622 [98560/118836 (83%)] Loss: 12244.936523\n",
      "    epoch          : 2622\n",
      "    loss           : 12217.229648082093\n",
      "    val_loss       : 12221.634411849669\n",
      "    val_log_likelihood: -12145.78909755609\n",
      "    val_log_marginal: -12154.191151998599\n",
      "Train Epoch: 2623 [256/118836 (0%)] Loss: 12275.809570\n",
      "Train Epoch: 2623 [33024/118836 (28%)] Loss: 12294.775391\n",
      "Train Epoch: 2623 [65792/118836 (55%)] Loss: 12221.292969\n",
      "Train Epoch: 2623 [98560/118836 (83%)] Loss: 12236.015625\n",
      "    epoch          : 2623\n",
      "    loss           : 12224.054824331834\n",
      "    val_loss       : 12218.153127306565\n",
      "    val_log_likelihood: -12142.138862017939\n",
      "    val_log_marginal: -12150.515040951808\n",
      "Train Epoch: 2624 [256/118836 (0%)] Loss: 12198.153320\n",
      "Train Epoch: 2624 [33024/118836 (28%)] Loss: 12321.413086\n",
      "Train Epoch: 2624 [65792/118836 (55%)] Loss: 12184.880859\n",
      "Train Epoch: 2624 [98560/118836 (83%)] Loss: 12210.125000\n",
      "    epoch          : 2624\n",
      "    loss           : 12222.037643455335\n",
      "    val_loss       : 12224.015206825974\n",
      "    val_log_likelihood: -12139.205902023883\n",
      "    val_log_marginal: -12147.758039817463\n",
      "Train Epoch: 2625 [256/118836 (0%)] Loss: 12226.737305\n",
      "Train Epoch: 2625 [33024/118836 (28%)] Loss: 12115.110352\n",
      "Train Epoch: 2625 [65792/118836 (55%)] Loss: 12224.687500\n",
      "Train Epoch: 2625 [98560/118836 (83%)] Loss: 12235.850586\n",
      "    epoch          : 2625\n",
      "    loss           : 12218.598576916615\n",
      "    val_loss       : 12220.851109252388\n",
      "    val_log_likelihood: -12142.820809747209\n",
      "    val_log_marginal: -12151.237220735095\n",
      "Train Epoch: 2626 [256/118836 (0%)] Loss: 12317.273438\n",
      "Train Epoch: 2626 [33024/118836 (28%)] Loss: 12221.941406\n",
      "Train Epoch: 2626 [65792/118836 (55%)] Loss: 12283.543945\n",
      "Train Epoch: 2626 [98560/118836 (83%)] Loss: 12303.619141\n",
      "    epoch          : 2626\n",
      "    loss           : 12218.858843989092\n",
      "    val_loss       : 12219.08993616748\n",
      "    val_log_likelihood: -12148.95862185949\n",
      "    val_log_marginal: -12157.41608050547\n",
      "Train Epoch: 2627 [256/118836 (0%)] Loss: 12347.730469\n",
      "Train Epoch: 2627 [33024/118836 (28%)] Loss: 12164.924805\n",
      "Train Epoch: 2627 [65792/118836 (55%)] Loss: 12333.704102\n",
      "Train Epoch: 2627 [98560/118836 (83%)] Loss: 12199.663086\n",
      "    epoch          : 2627\n",
      "    loss           : 12223.582038358147\n",
      "    val_loss       : 12217.895429871722\n",
      "    val_log_likelihood: -12142.75239383013\n",
      "    val_log_marginal: -12151.155823181669\n",
      "Train Epoch: 2628 [256/118836 (0%)] Loss: 12206.232422\n",
      "Train Epoch: 2628 [33024/118836 (28%)] Loss: 12348.475586\n",
      "Train Epoch: 2628 [65792/118836 (55%)] Loss: 12181.535156\n",
      "Train Epoch: 2628 [98560/118836 (83%)] Loss: 12301.535156\n",
      "    epoch          : 2628\n",
      "    loss           : 12219.01364764268\n",
      "    val_loss       : 12224.509543940918\n",
      "    val_log_likelihood: -12151.500466714486\n",
      "    val_log_marginal: -12160.096492242377\n",
      "Train Epoch: 2629 [256/118836 (0%)] Loss: 12246.927734\n",
      "Train Epoch: 2629 [33024/118836 (28%)] Loss: 12186.331055\n",
      "Train Epoch: 2629 [65792/118836 (55%)] Loss: 12279.963867\n",
      "Train Epoch: 2629 [98560/118836 (83%)] Loss: 12186.849609\n",
      "    epoch          : 2629\n",
      "    loss           : 12221.32258484543\n",
      "    val_loss       : 12229.243141408488\n",
      "    val_log_likelihood: -12151.222258355305\n",
      "    val_log_marginal: -12159.591329702766\n",
      "Train Epoch: 2630 [256/118836 (0%)] Loss: 12216.596680\n",
      "Train Epoch: 2630 [33024/118836 (28%)] Loss: 12236.897461\n",
      "Train Epoch: 2630 [65792/118836 (55%)] Loss: 12169.616211\n",
      "Train Epoch: 2630 [98560/118836 (83%)] Loss: 12161.818359\n",
      "    epoch          : 2630\n",
      "    loss           : 12222.507290535825\n",
      "    val_loss       : 12222.560334356838\n",
      "    val_log_likelihood: -12144.413781566636\n",
      "    val_log_marginal: -12152.937136077375\n",
      "Train Epoch: 2631 [256/118836 (0%)] Loss: 12270.430664\n",
      "Train Epoch: 2631 [33024/118836 (28%)] Loss: 12176.138672\n",
      "Train Epoch: 2631 [65792/118836 (55%)] Loss: 12259.982422\n",
      "Train Epoch: 2631 [98560/118836 (83%)] Loss: 12299.331055\n",
      "    epoch          : 2631\n",
      "    loss           : 12218.803316273781\n",
      "    val_loss       : 12220.163792631953\n",
      "    val_log_likelihood: -12139.408127197064\n",
      "    val_log_marginal: -12147.789622598977\n",
      "Train Epoch: 2632 [256/118836 (0%)] Loss: 12255.460938\n",
      "Train Epoch: 2632 [33024/118836 (28%)] Loss: 12269.037109\n",
      "Train Epoch: 2632 [65792/118836 (55%)] Loss: 12336.677734\n",
      "Train Epoch: 2632 [98560/118836 (83%)] Loss: 12182.713867\n",
      "    epoch          : 2632\n",
      "    loss           : 12224.700899503723\n",
      "    val_loss       : 12226.83786149393\n",
      "    val_log_likelihood: -12141.953871032361\n",
      "    val_log_marginal: -12150.426786173968\n",
      "Train Epoch: 2633 [256/118836 (0%)] Loss: 12285.211914\n",
      "Train Epoch: 2633 [33024/118836 (28%)] Loss: 12189.937500\n",
      "Train Epoch: 2633 [65792/118836 (55%)] Loss: 12304.729492\n",
      "Train Epoch: 2633 [98560/118836 (83%)] Loss: 12256.277344\n",
      "    epoch          : 2633\n",
      "    loss           : 12220.750440220483\n",
      "    val_loss       : 12219.18487098979\n",
      "    val_log_likelihood: -12141.049598712778\n",
      "    val_log_marginal: -12149.520331103096\n",
      "Train Epoch: 2634 [256/118836 (0%)] Loss: 12266.634766\n",
      "Train Epoch: 2634 [33024/118836 (28%)] Loss: 12168.774414\n",
      "Train Epoch: 2634 [65792/118836 (55%)] Loss: 12242.964844\n",
      "Train Epoch: 2634 [98560/118836 (83%)] Loss: 12381.889648\n",
      "    epoch          : 2634\n",
      "    loss           : 12223.251495295699\n",
      "    val_loss       : 12221.944509328901\n",
      "    val_log_likelihood: -12143.657563230201\n",
      "    val_log_marginal: -12152.19861123762\n",
      "Train Epoch: 2635 [256/118836 (0%)] Loss: 12262.393555\n",
      "Train Epoch: 2635 [33024/118836 (28%)] Loss: 12241.527344\n",
      "Train Epoch: 2635 [65792/118836 (55%)] Loss: 12253.482422\n",
      "Train Epoch: 2635 [98560/118836 (83%)] Loss: 12194.738281\n",
      "    epoch          : 2635\n",
      "    loss           : 12229.001946016853\n",
      "    val_loss       : 12224.813793149824\n",
      "    val_log_likelihood: -12145.81493680211\n",
      "    val_log_marginal: -12154.32733356241\n",
      "Train Epoch: 2636 [256/118836 (0%)] Loss: 12365.887695\n",
      "Train Epoch: 2636 [33024/118836 (28%)] Loss: 12193.714844\n",
      "Train Epoch: 2636 [65792/118836 (55%)] Loss: 12277.078125\n",
      "Train Epoch: 2636 [98560/118836 (83%)] Loss: 12258.007812\n",
      "    epoch          : 2636\n",
      "    loss           : 12226.495571624275\n",
      "    val_loss       : 12225.412203269072\n",
      "    val_log_likelihood: -12143.705634499069\n",
      "    val_log_marginal: -12152.317631188173\n",
      "Train Epoch: 2637 [256/118836 (0%)] Loss: 12310.412109\n",
      "Train Epoch: 2637 [33024/118836 (28%)] Loss: 12325.854492\n",
      "Train Epoch: 2637 [65792/118836 (55%)] Loss: 12270.214844\n",
      "Train Epoch: 2637 [98560/118836 (83%)] Loss: 12313.555664\n",
      "    epoch          : 2637\n",
      "    loss           : 12221.920399413253\n",
      "    val_loss       : 12223.01222674233\n",
      "    val_log_likelihood: -12142.838343930935\n",
      "    val_log_marginal: -12151.194528642083\n",
      "Train Epoch: 2638 [256/118836 (0%)] Loss: 12248.835938\n",
      "Train Epoch: 2638 [33024/118836 (28%)] Loss: 12320.186523\n",
      "Train Epoch: 2638 [65792/118836 (55%)] Loss: 12230.337891\n",
      "Train Epoch: 2638 [98560/118836 (83%)] Loss: 12241.971680\n",
      "    epoch          : 2638\n",
      "    loss           : 12222.366782658706\n",
      "    val_loss       : 12221.959833262405\n",
      "    val_log_likelihood: -12144.461515521609\n",
      "    val_log_marginal: -12152.940074992935\n",
      "Train Epoch: 2639 [256/118836 (0%)] Loss: 12245.516602\n",
      "Train Epoch: 2639 [33024/118836 (28%)] Loss: 12197.176758\n",
      "Train Epoch: 2639 [65792/118836 (55%)] Loss: 12180.622070\n",
      "Train Epoch: 2639 [98560/118836 (83%)] Loss: 12210.224609\n",
      "    epoch          : 2639\n",
      "    loss           : 12219.544399264629\n",
      "    val_loss       : 12220.266466151443\n",
      "    val_log_likelihood: -12143.181715260545\n",
      "    val_log_marginal: -12151.92240058162\n",
      "Train Epoch: 2640 [256/118836 (0%)] Loss: 12225.037109\n",
      "Train Epoch: 2640 [33024/118836 (28%)] Loss: 12273.106445\n",
      "Train Epoch: 2640 [65792/118836 (55%)] Loss: 12181.537109\n",
      "Train Epoch: 2640 [98560/118836 (83%)] Loss: 12211.617188\n",
      "    epoch          : 2640\n",
      "    loss           : 12222.382493441119\n",
      "    val_loss       : 12223.030761076901\n",
      "    val_log_likelihood: -12143.237220035928\n",
      "    val_log_marginal: -12152.011031091657\n",
      "Train Epoch: 2641 [256/118836 (0%)] Loss: 12158.496094\n",
      "Train Epoch: 2641 [33024/118836 (28%)] Loss: 12222.897461\n",
      "Train Epoch: 2641 [65792/118836 (55%)] Loss: 12263.400391\n",
      "Train Epoch: 2641 [98560/118836 (83%)] Loss: 12213.484375\n",
      "    epoch          : 2641\n",
      "    loss           : 12222.790751169614\n",
      "    val_loss       : 12214.974804191828\n",
      "    val_log_likelihood: -12141.038918883118\n",
      "    val_log_marginal: -12149.466251843154\n",
      "Train Epoch: 2642 [256/118836 (0%)] Loss: 12315.556641\n",
      "Train Epoch: 2642 [33024/118836 (28%)] Loss: 12284.898438\n",
      "Train Epoch: 2642 [65792/118836 (55%)] Loss: 12299.154297\n",
      "Train Epoch: 2642 [98560/118836 (83%)] Loss: 12202.164062\n",
      "    epoch          : 2642\n",
      "    loss           : 12218.783546416202\n",
      "    val_loss       : 12218.760161578743\n",
      "    val_log_likelihood: -12143.138032464847\n",
      "    val_log_marginal: -12151.538349738265\n",
      "Train Epoch: 2643 [256/118836 (0%)] Loss: 12177.462891\n",
      "Train Epoch: 2643 [33024/118836 (28%)] Loss: 12240.328125\n",
      "Train Epoch: 2643 [65792/118836 (55%)] Loss: 12298.917969\n",
      "Train Epoch: 2643 [98560/118836 (83%)] Loss: 12187.558594\n",
      "    epoch          : 2643\n",
      "    loss           : 12224.225062196288\n",
      "    val_loss       : 12219.805470417388\n",
      "    val_log_likelihood: -12142.035254471672\n",
      "    val_log_marginal: -12150.489038471622\n",
      "Train Epoch: 2644 [256/118836 (0%)] Loss: 12131.669922\n",
      "Train Epoch: 2644 [33024/118836 (28%)] Loss: 12190.757812\n",
      "Train Epoch: 2644 [65792/118836 (55%)] Loss: 12226.110352\n",
      "Train Epoch: 2644 [98560/118836 (83%)] Loss: 12417.324219\n",
      "    epoch          : 2644\n",
      "    loss           : 12217.120826386734\n",
      "    val_loss       : 12221.911765881448\n",
      "    val_log_likelihood: -12143.200015508684\n",
      "    val_log_marginal: -12151.523553967345\n",
      "Train Epoch: 2645 [256/118836 (0%)] Loss: 12272.662109\n",
      "Train Epoch: 2645 [33024/118836 (28%)] Loss: 12214.309570\n",
      "Train Epoch: 2645 [65792/118836 (55%)] Loss: 12217.698242\n",
      "Train Epoch: 2645 [98560/118836 (83%)] Loss: 12356.638672\n",
      "    epoch          : 2645\n",
      "    loss           : 12219.61677538901\n",
      "    val_loss       : 12221.481195080903\n",
      "    val_log_likelihood: -12142.194208637044\n",
      "    val_log_marginal: -12150.493256073563\n",
      "Train Epoch: 2646 [256/118836 (0%)] Loss: 12194.224609\n",
      "Train Epoch: 2646 [33024/118836 (28%)] Loss: 12212.994141\n",
      "Train Epoch: 2646 [65792/118836 (55%)] Loss: 12200.578125\n",
      "Train Epoch: 2646 [98560/118836 (83%)] Loss: 12262.275391\n",
      "    epoch          : 2646\n",
      "    loss           : 12222.464227925971\n",
      "    val_loss       : 12222.405314884993\n",
      "    val_log_likelihood: -12146.260942023366\n",
      "    val_log_marginal: -12154.810715550311\n",
      "Train Epoch: 2647 [256/118836 (0%)] Loss: 12336.298828\n",
      "Train Epoch: 2647 [33024/118836 (28%)] Loss: 12221.785156\n",
      "Train Epoch: 2647 [65792/118836 (55%)] Loss: 12237.655273\n",
      "Train Epoch: 2647 [98560/118836 (83%)] Loss: 12233.795898\n",
      "    epoch          : 2647\n",
      "    loss           : 12222.533702149247\n",
      "    val_loss       : 12225.114740868175\n",
      "    val_log_likelihood: -12145.50467263751\n",
      "    val_log_marginal: -12154.123933845169\n",
      "Train Epoch: 2648 [256/118836 (0%)] Loss: 12196.523438\n",
      "Train Epoch: 2648 [33024/118836 (28%)] Loss: 12323.037109\n",
      "Train Epoch: 2648 [65792/118836 (55%)] Loss: 12239.102539\n",
      "Train Epoch: 2648 [98560/118836 (83%)] Loss: 12206.356445\n",
      "    epoch          : 2648\n",
      "    loss           : 12218.33218213658\n",
      "    val_loss       : 12220.333033633595\n",
      "    val_log_likelihood: -12149.710702284947\n",
      "    val_log_marginal: -12158.162825642426\n",
      "Train Epoch: 2649 [256/118836 (0%)] Loss: 12205.125000\n",
      "Train Epoch: 2649 [33024/118836 (28%)] Loss: 12276.656250\n",
      "Train Epoch: 2649 [65792/118836 (55%)] Loss: 12292.353516\n",
      "Train Epoch: 2649 [98560/118836 (83%)] Loss: 12183.182617\n",
      "    epoch          : 2649\n",
      "    loss           : 12225.380814302885\n",
      "    val_loss       : 12221.389551760425\n",
      "    val_log_likelihood: -12143.595680992814\n",
      "    val_log_marginal: -12152.003065779334\n",
      "Train Epoch: 2650 [256/118836 (0%)] Loss: 12301.887695\n",
      "Train Epoch: 2650 [33024/118836 (28%)] Loss: 12260.967773\n",
      "Train Epoch: 2650 [65792/118836 (55%)] Loss: 12161.773438\n",
      "Train Epoch: 2650 [98560/118836 (83%)] Loss: 12349.365234\n",
      "    epoch          : 2650\n",
      "    loss           : 12225.696723628773\n",
      "    val_loss       : 12229.546718411468\n",
      "    val_log_likelihood: -12148.616635972394\n",
      "    val_log_marginal: -12157.369742816065\n",
      "Train Epoch: 2651 [256/118836 (0%)] Loss: 12180.732422\n",
      "Train Epoch: 2651 [33024/118836 (28%)] Loss: 12188.934570\n",
      "Train Epoch: 2651 [65792/118836 (55%)] Loss: 12224.607422\n",
      "Train Epoch: 2651 [98560/118836 (83%)] Loss: 12216.877930\n",
      "    epoch          : 2651\n",
      "    loss           : 12224.761085478702\n",
      "    val_loss       : 12225.866715138596\n",
      "    val_log_likelihood: -12142.07731144024\n",
      "    val_log_marginal: -12150.480530589068\n",
      "Train Epoch: 2652 [256/118836 (0%)] Loss: 12186.161133\n",
      "Train Epoch: 2652 [33024/118836 (28%)] Loss: 12250.623047\n",
      "Train Epoch: 2652 [65792/118836 (55%)] Loss: 12342.710938\n",
      "Train Epoch: 2652 [98560/118836 (83%)] Loss: 12312.810547\n",
      "    epoch          : 2652\n",
      "    loss           : 12226.426964594964\n",
      "    val_loss       : 12227.035391519932\n",
      "    val_log_likelihood: -12142.244360654468\n",
      "    val_log_marginal: -12150.560702603852\n",
      "Train Epoch: 2653 [256/118836 (0%)] Loss: 12363.973633\n",
      "Train Epoch: 2653 [33024/118836 (28%)] Loss: 12286.415039\n",
      "Train Epoch: 2653 [65792/118836 (55%)] Loss: 12293.196289\n",
      "Train Epoch: 2653 [98560/118836 (83%)] Loss: 12224.389648\n",
      "    epoch          : 2653\n",
      "    loss           : 12220.199519553868\n",
      "    val_loss       : 12221.04461944804\n",
      "    val_log_likelihood: -12145.387241845017\n",
      "    val_log_marginal: -12154.019031679105\n",
      "Train Epoch: 2654 [256/118836 (0%)] Loss: 12334.208984\n",
      "Train Epoch: 2654 [33024/118836 (28%)] Loss: 12217.044922\n",
      "Train Epoch: 2654 [65792/118836 (55%)] Loss: 12323.085938\n",
      "Train Epoch: 2654 [98560/118836 (83%)] Loss: 12225.129883\n",
      "    epoch          : 2654\n",
      "    loss           : 12222.881796034946\n",
      "    val_loss       : 12219.16695491167\n",
      "    val_log_likelihood: -12141.58559937836\n",
      "    val_log_marginal: -12149.871706683109\n",
      "Train Epoch: 2655 [256/118836 (0%)] Loss: 12204.213867\n",
      "Train Epoch: 2655 [33024/118836 (28%)] Loss: 12246.449219\n",
      "Train Epoch: 2655 [65792/118836 (55%)] Loss: 12187.374023\n",
      "Train Epoch: 2655 [98560/118836 (83%)] Loss: 12182.408203\n",
      "    epoch          : 2655\n",
      "    loss           : 12218.997903742764\n",
      "    val_loss       : 12225.49238853087\n",
      "    val_log_likelihood: -12145.35118253722\n",
      "    val_log_marginal: -12153.747582373597\n",
      "Train Epoch: 2656 [256/118836 (0%)] Loss: 12253.675781\n",
      "Train Epoch: 2656 [33024/118836 (28%)] Loss: 12235.908203\n",
      "Train Epoch: 2656 [65792/118836 (55%)] Loss: 12226.264648\n",
      "Train Epoch: 2656 [98560/118836 (83%)] Loss: 12272.746094\n",
      "    epoch          : 2656\n",
      "    loss           : 12223.380551301438\n",
      "    val_loss       : 12221.288406894468\n",
      "    val_log_likelihood: -12143.47787734569\n",
      "    val_log_marginal: -12151.673073436146\n",
      "Train Epoch: 2657 [256/118836 (0%)] Loss: 12196.521484\n",
      "Train Epoch: 2657 [33024/118836 (28%)] Loss: 12202.967773\n",
      "Train Epoch: 2657 [65792/118836 (55%)] Loss: 12209.712891\n",
      "Train Epoch: 2657 [98560/118836 (83%)] Loss: 12393.722656\n",
      "    epoch          : 2657\n",
      "    loss           : 12220.746427994467\n",
      "    val_loss       : 12224.510638085983\n",
      "    val_log_likelihood: -12145.218439664754\n",
      "    val_log_marginal: -12153.509608325547\n",
      "Train Epoch: 2658 [256/118836 (0%)] Loss: 12151.330078\n",
      "Train Epoch: 2658 [33024/118836 (28%)] Loss: 12264.619141\n",
      "Train Epoch: 2658 [65792/118836 (55%)] Loss: 12220.031250\n",
      "Train Epoch: 2658 [98560/118836 (83%)] Loss: 12244.594727\n",
      "    epoch          : 2658\n",
      "    loss           : 12224.781197173543\n",
      "    val_loss       : 12219.526621820658\n",
      "    val_log_likelihood: -12144.424399038462\n",
      "    val_log_marginal: -12152.877796549854\n",
      "Train Epoch: 2659 [256/118836 (0%)] Loss: 12151.473633\n",
      "Train Epoch: 2659 [33024/118836 (28%)] Loss: 12265.434570\n",
      "Train Epoch: 2659 [65792/118836 (55%)] Loss: 12238.498047\n",
      "Train Epoch: 2659 [98560/118836 (83%)] Loss: 12267.057617\n",
      "    epoch          : 2659\n",
      "    loss           : 12222.588340861506\n",
      "    val_loss       : 12229.329647551132\n",
      "    val_log_likelihood: -12143.639915639216\n",
      "    val_log_marginal: -12152.251478747607\n",
      "Train Epoch: 2660 [256/118836 (0%)] Loss: 12198.468750\n",
      "Train Epoch: 2660 [33024/118836 (28%)] Loss: 12197.625977\n",
      "Train Epoch: 2660 [65792/118836 (55%)] Loss: 12204.071289\n",
      "Train Epoch: 2660 [98560/118836 (83%)] Loss: 12225.646484\n",
      "    epoch          : 2660\n",
      "    loss           : 12223.678297049471\n",
      "    val_loss       : 12224.206051250943\n",
      "    val_log_likelihood: -12142.78522910851\n",
      "    val_log_marginal: -12151.266880018795\n",
      "Train Epoch: 2661 [256/118836 (0%)] Loss: 12375.705078\n",
      "Train Epoch: 2661 [33024/118836 (28%)] Loss: 12245.897461\n",
      "Train Epoch: 2661 [65792/118836 (55%)] Loss: 12236.471680\n",
      "Train Epoch: 2661 [98560/118836 (83%)] Loss: 12177.583008\n",
      "    epoch          : 2661\n",
      "    loss           : 12220.839720972912\n",
      "    val_loss       : 12226.644375539341\n",
      "    val_log_likelihood: -12145.80840247622\n",
      "    val_log_marginal: -12154.322472599868\n",
      "Train Epoch: 2662 [256/118836 (0%)] Loss: 12200.293945\n",
      "Train Epoch: 2662 [33024/118836 (28%)] Loss: 12277.391602\n",
      "Train Epoch: 2662 [65792/118836 (55%)] Loss: 12266.760742\n",
      "Train Epoch: 2662 [98560/118836 (83%)] Loss: 12207.106445\n",
      "    epoch          : 2662\n",
      "    loss           : 12219.525197089537\n",
      "    val_loss       : 12217.120533282246\n",
      "    val_log_likelihood: -12139.402410469655\n",
      "    val_log_marginal: -12147.734278604888\n",
      "Train Epoch: 2663 [256/118836 (0%)] Loss: 12270.287109\n",
      "Train Epoch: 2663 [33024/118836 (28%)] Loss: 12204.735352\n",
      "Train Epoch: 2663 [65792/118836 (55%)] Loss: 12191.781250\n",
      "Train Epoch: 2663 [98560/118836 (83%)] Loss: 12169.567383\n",
      "    epoch          : 2663\n",
      "    loss           : 12221.073183383736\n",
      "    val_loss       : 12218.973551253093\n",
      "    val_log_likelihood: -12141.181679073616\n",
      "    val_log_marginal: -12149.51589636298\n",
      "Train Epoch: 2664 [256/118836 (0%)] Loss: 12296.720703\n",
      "Train Epoch: 2664 [33024/118836 (28%)] Loss: 12267.140625\n",
      "Train Epoch: 2664 [65792/118836 (55%)] Loss: 12255.943359\n",
      "Train Epoch: 2664 [98560/118836 (83%)] Loss: 12201.699219\n",
      "    epoch          : 2664\n",
      "    loss           : 12218.396802464587\n",
      "    val_loss       : 12218.575201057654\n",
      "    val_log_likelihood: -12143.40479250672\n",
      "    val_log_marginal: -12151.977317563673\n",
      "Train Epoch: 2665 [256/118836 (0%)] Loss: 12179.564453\n",
      "Train Epoch: 2665 [33024/118836 (28%)] Loss: 12277.435547\n",
      "Train Epoch: 2665 [65792/118836 (55%)] Loss: 12252.476562\n",
      "Train Epoch: 2665 [98560/118836 (83%)] Loss: 12271.505859\n",
      "    epoch          : 2665\n",
      "    loss           : 12223.069780035154\n",
      "    val_loss       : 12222.173080500188\n",
      "    val_log_likelihood: -12142.899527954405\n",
      "    val_log_marginal: -12151.615370299965\n",
      "Train Epoch: 2666 [256/118836 (0%)] Loss: 12213.845703\n",
      "Train Epoch: 2666 [33024/118836 (28%)] Loss: 12228.461914\n",
      "Train Epoch: 2666 [65792/118836 (55%)] Loss: 12177.603516\n",
      "Train Epoch: 2666 [98560/118836 (83%)] Loss: 12279.212891\n",
      "    epoch          : 2666\n",
      "    loss           : 12223.818790710297\n",
      "    val_loss       : 12220.877715607308\n",
      "    val_log_likelihood: -12143.687068341604\n",
      "    val_log_marginal: -12152.160551450272\n",
      "Train Epoch: 2667 [256/118836 (0%)] Loss: 12302.708984\n",
      "Train Epoch: 2667 [33024/118836 (28%)] Loss: 12201.117188\n",
      "Train Epoch: 2667 [65792/118836 (55%)] Loss: 12258.034180\n",
      "Train Epoch: 2667 [98560/118836 (83%)] Loss: 12240.473633\n",
      "    epoch          : 2667\n",
      "    loss           : 12225.501089323563\n",
      "    val_loss       : 12216.14890611304\n",
      "    val_log_likelihood: -12142.610691622725\n",
      "    val_log_marginal: -12151.005953879863\n",
      "Train Epoch: 2668 [256/118836 (0%)] Loss: 12285.664062\n",
      "Train Epoch: 2668 [33024/118836 (28%)] Loss: 12212.811523\n",
      "Train Epoch: 2668 [65792/118836 (55%)] Loss: 12207.342773\n",
      "Train Epoch: 2668 [98560/118836 (83%)] Loss: 12184.843750\n",
      "    epoch          : 2668\n",
      "    loss           : 12218.674995153535\n",
      "    val_loss       : 12219.330228714654\n",
      "    val_log_likelihood: -12142.818688288358\n",
      "    val_log_marginal: -12151.39503757346\n",
      "Train Epoch: 2669 [256/118836 (0%)] Loss: 12192.416016\n",
      "Train Epoch: 2669 [33024/118836 (28%)] Loss: 12273.259766\n",
      "Train Epoch: 2669 [65792/118836 (55%)] Loss: 12281.745117\n",
      "Train Epoch: 2669 [98560/118836 (83%)] Loss: 12259.852539\n",
      "    epoch          : 2669\n",
      "    loss           : 12222.425222937343\n",
      "    val_loss       : 12223.346020786897\n",
      "    val_log_likelihood: -12144.087556865179\n",
      "    val_log_marginal: -12152.444144363082\n",
      "Train Epoch: 2670 [256/118836 (0%)] Loss: 12203.866211\n",
      "Train Epoch: 2670 [33024/118836 (28%)] Loss: 12186.669922\n",
      "Train Epoch: 2670 [65792/118836 (55%)] Loss: 12238.875977\n",
      "Train Epoch: 2670 [98560/118836 (83%)] Loss: 12214.037109\n",
      "    epoch          : 2670\n",
      "    loss           : 12221.303672165788\n",
      "    val_loss       : 12219.125045535095\n",
      "    val_log_likelihood: -12142.57491906405\n",
      "    val_log_marginal: -12150.89524021882\n",
      "Train Epoch: 2671 [256/118836 (0%)] Loss: 12224.714844\n",
      "Train Epoch: 2671 [33024/118836 (28%)] Loss: 12231.335938\n",
      "Train Epoch: 2671 [65792/118836 (55%)] Loss: 12275.402344\n",
      "Train Epoch: 2671 [98560/118836 (83%)] Loss: 12236.443359\n",
      "    epoch          : 2671\n",
      "    loss           : 12223.109800842638\n",
      "    val_loss       : 12221.193322887097\n",
      "    val_log_likelihood: -12142.839071546734\n",
      "    val_log_marginal: -12151.224106957425\n",
      "Train Epoch: 2672 [256/118836 (0%)] Loss: 12226.465820\n",
      "Train Epoch: 2672 [33024/118836 (28%)] Loss: 12187.146484\n",
      "Train Epoch: 2672 [65792/118836 (55%)] Loss: 12261.809570\n",
      "Train Epoch: 2672 [98560/118836 (83%)] Loss: 12312.386719\n",
      "    epoch          : 2672\n",
      "    loss           : 12221.17220892137\n",
      "    val_loss       : 12218.266802458398\n",
      "    val_log_likelihood: -12141.406249676904\n",
      "    val_log_marginal: -12149.82182980558\n",
      "Train Epoch: 2673 [256/118836 (0%)] Loss: 12176.821289\n",
      "Train Epoch: 2673 [33024/118836 (28%)] Loss: 12167.015625\n",
      "Train Epoch: 2673 [65792/118836 (55%)] Loss: 12425.281250\n",
      "Train Epoch: 2673 [98560/118836 (83%)] Loss: 12270.172852\n",
      "    epoch          : 2673\n",
      "    loss           : 12222.312690950681\n",
      "    val_loss       : 12225.030542248858\n",
      "    val_log_likelihood: -12142.826170259512\n",
      "    val_log_marginal: -12151.065559111888\n",
      "Train Epoch: 2674 [256/118836 (0%)] Loss: 12224.625000\n",
      "Train Epoch: 2674 [33024/118836 (28%)] Loss: 12151.331055\n",
      "Train Epoch: 2674 [65792/118836 (55%)] Loss: 12262.198242\n",
      "Train Epoch: 2674 [98560/118836 (83%)] Loss: 12204.677734\n",
      "    epoch          : 2674\n",
      "    loss           : 12221.125770749328\n",
      "    val_loss       : 12221.812525019506\n",
      "    val_log_likelihood: -12148.909747531534\n",
      "    val_log_marginal: -12157.34061332766\n",
      "Train Epoch: 2675 [256/118836 (0%)] Loss: 12212.402344\n",
      "Train Epoch: 2675 [33024/118836 (28%)] Loss: 12355.718750\n",
      "Train Epoch: 2675 [65792/118836 (55%)] Loss: 12237.719727\n",
      "Train Epoch: 2675 [98560/118836 (83%)] Loss: 12257.607422\n",
      "    epoch          : 2675\n",
      "    loss           : 12220.148598079508\n",
      "    val_loss       : 12221.131961318364\n",
      "    val_log_likelihood: -12140.125192727719\n",
      "    val_log_marginal: -12148.4113739902\n",
      "Train Epoch: 2676 [256/118836 (0%)] Loss: 12295.684570\n",
      "Train Epoch: 2676 [33024/118836 (28%)] Loss: 12245.680664\n",
      "Train Epoch: 2676 [65792/118836 (55%)] Loss: 12224.513672\n",
      "Train Epoch: 2676 [98560/118836 (83%)] Loss: 12191.083984\n",
      "    epoch          : 2676\n",
      "    loss           : 12223.797710368848\n",
      "    val_loss       : 12226.175978083202\n",
      "    val_log_likelihood: -12142.04895477926\n",
      "    val_log_marginal: -12150.357981247684\n",
      "Train Epoch: 2677 [256/118836 (0%)] Loss: 12170.033203\n",
      "Train Epoch: 2677 [33024/118836 (28%)] Loss: 12249.500000\n",
      "Train Epoch: 2677 [65792/118836 (55%)] Loss: 12244.499023\n",
      "Train Epoch: 2677 [98560/118836 (83%)] Loss: 12192.526367\n",
      "    epoch          : 2677\n",
      "    loss           : 12219.427148760597\n",
      "    val_loss       : 12222.629953834752\n",
      "    val_log_likelihood: -12147.889857158549\n",
      "    val_log_marginal: -12156.3939311675\n",
      "Train Epoch: 2678 [256/118836 (0%)] Loss: 12161.093750\n",
      "Train Epoch: 2678 [33024/118836 (28%)] Loss: 12243.117188\n",
      "Train Epoch: 2678 [65792/118836 (55%)] Loss: 12308.587891\n",
      "Train Epoch: 2678 [98560/118836 (83%)] Loss: 12257.824219\n",
      "    epoch          : 2678\n",
      "    loss           : 12221.92206627378\n",
      "    val_loss       : 12221.127121410887\n",
      "    val_log_likelihood: -12143.405021098275\n",
      "    val_log_marginal: -12151.865325391109\n",
      "Train Epoch: 2679 [256/118836 (0%)] Loss: 12186.898438\n",
      "Train Epoch: 2679 [33024/118836 (28%)] Loss: 12220.398438\n",
      "Train Epoch: 2679 [65792/118836 (55%)] Loss: 12267.503906\n",
      "Train Epoch: 2679 [98560/118836 (83%)] Loss: 12282.613281\n",
      "    epoch          : 2679\n",
      "    loss           : 12222.574578196081\n",
      "    val_loss       : 12217.18694482313\n",
      "    val_log_likelihood: -12139.681622046886\n",
      "    val_log_marginal: -12147.924156462514\n",
      "Train Epoch: 2680 [256/118836 (0%)] Loss: 12258.570312\n",
      "Train Epoch: 2680 [33024/118836 (28%)] Loss: 12157.769531\n",
      "Train Epoch: 2680 [65792/118836 (55%)] Loss: 12200.563477\n",
      "Train Epoch: 2680 [98560/118836 (83%)] Loss: 12254.140625\n",
      "    epoch          : 2680\n",
      "    loss           : 12220.651551514682\n",
      "    val_loss       : 12223.813660184851\n",
      "    val_log_likelihood: -12149.140938243123\n",
      "    val_log_marginal: -12157.687427489229\n",
      "Train Epoch: 2681 [256/118836 (0%)] Loss: 12292.045898\n",
      "Train Epoch: 2681 [33024/118836 (28%)] Loss: 12340.562500\n",
      "Train Epoch: 2681 [65792/118836 (55%)] Loss: 12175.722656\n",
      "Train Epoch: 2681 [98560/118836 (83%)] Loss: 12218.782227\n",
      "    epoch          : 2681\n",
      "    loss           : 12223.157347239454\n",
      "    val_loss       : 12224.539171680986\n",
      "    val_log_likelihood: -12145.938753134047\n",
      "    val_log_marginal: -12154.465494398934\n",
      "Train Epoch: 2682 [256/118836 (0%)] Loss: 12270.263672\n",
      "Train Epoch: 2682 [33024/118836 (28%)] Loss: 12223.915039\n",
      "Train Epoch: 2682 [65792/118836 (55%)] Loss: 12199.368164\n",
      "Train Epoch: 2682 [98560/118836 (83%)] Loss: 12283.046875\n",
      "    epoch          : 2682\n",
      "    loss           : 12222.955092502843\n",
      "    val_loss       : 12223.356608986802\n",
      "    val_log_likelihood: -12146.510719570668\n",
      "    val_log_marginal: -12155.335941521953\n",
      "Train Epoch: 2683 [256/118836 (0%)] Loss: 12268.060547\n",
      "Train Epoch: 2683 [33024/118836 (28%)] Loss: 12167.758789\n",
      "Train Epoch: 2683 [65792/118836 (55%)] Loss: 12277.357422\n",
      "Train Epoch: 2683 [98560/118836 (83%)] Loss: 12266.709961\n",
      "    epoch          : 2683\n",
      "    loss           : 12219.553996232682\n",
      "    val_loss       : 12220.846883631459\n",
      "    val_log_likelihood: -12143.286529737903\n",
      "    val_log_marginal: -12151.662581490807\n",
      "Train Epoch: 2684 [256/118836 (0%)] Loss: 12179.535156\n",
      "Train Epoch: 2684 [33024/118836 (28%)] Loss: 12206.246094\n",
      "Train Epoch: 2684 [65792/118836 (55%)] Loss: 12220.458008\n",
      "Train Epoch: 2684 [98560/118836 (83%)] Loss: 12311.592773\n",
      "    epoch          : 2684\n",
      "    loss           : 12217.54079591863\n",
      "    val_loss       : 12232.074914365676\n",
      "    val_log_likelihood: -12148.19271673387\n",
      "    val_log_marginal: -12156.8744922987\n",
      "Train Epoch: 2685 [256/118836 (0%)] Loss: 12229.220703\n",
      "Train Epoch: 2685 [33024/118836 (28%)] Loss: 12269.124023\n",
      "Train Epoch: 2685 [65792/118836 (55%)] Loss: 12196.579102\n",
      "Train Epoch: 2685 [98560/118836 (83%)] Loss: 12271.001953\n",
      "    epoch          : 2685\n",
      "    loss           : 12227.554607533342\n",
      "    val_loss       : 12220.994926376243\n",
      "    val_log_likelihood: -12144.516050034894\n",
      "    val_log_marginal: -12152.951096096038\n",
      "Train Epoch: 2686 [256/118836 (0%)] Loss: 12289.940430\n",
      "Train Epoch: 2686 [33024/118836 (28%)] Loss: 12354.841797\n",
      "Train Epoch: 2686 [65792/118836 (55%)] Loss: 12253.213867\n",
      "Train Epoch: 2686 [98560/118836 (83%)] Loss: 12156.221680\n",
      "    epoch          : 2686\n",
      "    loss           : 12222.203561827957\n",
      "    val_loss       : 12226.282609994827\n",
      "    val_log_likelihood: -12140.365995916047\n",
      "    val_log_marginal: -12148.840424927628\n",
      "Train Epoch: 2687 [256/118836 (0%)] Loss: 12430.999023\n",
      "Train Epoch: 2687 [33024/118836 (28%)] Loss: 12341.452148\n",
      "Train Epoch: 2687 [65792/118836 (55%)] Loss: 12269.366211\n",
      "Train Epoch: 2687 [98560/118836 (83%)] Loss: 12296.806641\n",
      "    epoch          : 2687\n",
      "    loss           : 12221.913636495812\n",
      "    val_loss       : 12221.977798328862\n",
      "    val_log_likelihood: -12142.229285081936\n",
      "    val_log_marginal: -12150.58385817531\n",
      "Train Epoch: 2688 [256/118836 (0%)] Loss: 12273.949219\n",
      "Train Epoch: 2688 [33024/118836 (28%)] Loss: 12181.874023\n",
      "Train Epoch: 2688 [65792/118836 (55%)] Loss: 12256.819336\n",
      "Train Epoch: 2688 [98560/118836 (83%)] Loss: 12207.448242\n",
      "    epoch          : 2688\n",
      "    loss           : 12223.08643297017\n",
      "    val_loss       : 12224.749356238905\n",
      "    val_log_likelihood: -12142.017571824597\n",
      "    val_log_marginal: -12150.560662008884\n",
      "Train Epoch: 2689 [256/118836 (0%)] Loss: 12334.836914\n",
      "Train Epoch: 2689 [33024/118836 (28%)] Loss: 12266.357422\n",
      "Train Epoch: 2689 [65792/118836 (55%)] Loss: 12274.952148\n",
      "Train Epoch: 2689 [98560/118836 (83%)] Loss: 12247.461914\n",
      "    epoch          : 2689\n",
      "    loss           : 12220.159527986714\n",
      "    val_loss       : 12220.26936656659\n",
      "    val_log_likelihood: -12140.774283854167\n",
      "    val_log_marginal: -12149.196050070936\n",
      "Train Epoch: 2690 [256/118836 (0%)] Loss: 12268.421875\n",
      "Train Epoch: 2690 [33024/118836 (28%)] Loss: 12159.228516\n",
      "Train Epoch: 2690 [65792/118836 (55%)] Loss: 12267.419922\n",
      "Train Epoch: 2690 [98560/118836 (83%)] Loss: 12244.419922\n",
      "    epoch          : 2690\n",
      "    loss           : 12221.368770355148\n",
      "    val_loss       : 12217.131152297516\n",
      "    val_log_likelihood: -12139.6145001357\n",
      "    val_log_marginal: -12147.893953530933\n",
      "Train Epoch: 2691 [256/118836 (0%)] Loss: 12314.612305\n",
      "Train Epoch: 2691 [33024/118836 (28%)] Loss: 12237.029297\n",
      "Train Epoch: 2691 [65792/118836 (55%)] Loss: 12187.285156\n",
      "Train Epoch: 2691 [98560/118836 (83%)] Loss: 12254.795898\n",
      "    epoch          : 2691\n",
      "    loss           : 12221.853773618435\n",
      "    val_loss       : 12220.499635696608\n",
      "    val_log_likelihood: -12140.80546729606\n",
      "    val_log_marginal: -12149.267512573895\n",
      "Train Epoch: 2692 [256/118836 (0%)] Loss: 12303.174805\n",
      "Train Epoch: 2692 [33024/118836 (28%)] Loss: 12248.334961\n",
      "Train Epoch: 2692 [65792/118836 (55%)] Loss: 12292.308594\n",
      "Train Epoch: 2692 [98560/118836 (83%)] Loss: 12198.513672\n",
      "    epoch          : 2692\n",
      "    loss           : 12220.054081853546\n",
      "    val_loss       : 12218.9571713197\n",
      "    val_log_likelihood: -12141.857526397074\n",
      "    val_log_marginal: -12150.224624439783\n",
      "Train Epoch: 2693 [256/118836 (0%)] Loss: 12320.661133\n",
      "Train Epoch: 2693 [33024/118836 (28%)] Loss: 12280.469727\n",
      "Train Epoch: 2693 [65792/118836 (55%)] Loss: 12240.713867\n",
      "Train Epoch: 2693 [98560/118836 (83%)] Loss: 12323.677734\n",
      "    epoch          : 2693\n",
      "    loss           : 12223.637542810431\n",
      "    val_loss       : 12220.868068478841\n",
      "    val_log_likelihood: -12144.102071217174\n",
      "    val_log_marginal: -12152.46437647194\n",
      "Train Epoch: 2694 [256/118836 (0%)] Loss: 12214.749023\n",
      "Train Epoch: 2694 [33024/118836 (28%)] Loss: 12159.063477\n",
      "Train Epoch: 2694 [65792/118836 (55%)] Loss: 12365.791016\n",
      "Train Epoch: 2694 [98560/118836 (83%)] Loss: 12259.526367\n",
      "    epoch          : 2694\n",
      "    loss           : 12224.722205367298\n",
      "    val_loss       : 12223.756824571914\n",
      "    val_log_likelihood: -12141.607071314103\n",
      "    val_log_marginal: -12150.156430599971\n",
      "Train Epoch: 2695 [256/118836 (0%)] Loss: 12341.498047\n",
      "Train Epoch: 2695 [33024/118836 (28%)] Loss: 12182.416992\n",
      "Train Epoch: 2695 [65792/118836 (55%)] Loss: 12290.977539\n",
      "Train Epoch: 2695 [98560/118836 (83%)] Loss: 12171.323242\n",
      "    epoch          : 2695\n",
      "    loss           : 12221.825267847911\n",
      "    val_loss       : 12221.822915918483\n",
      "    val_log_likelihood: -12140.775741832093\n",
      "    val_log_marginal: -12149.324433631904\n",
      "Train Epoch: 2696 [256/118836 (0%)] Loss: 12248.940430\n",
      "Train Epoch: 2696 [33024/118836 (28%)] Loss: 12293.405273\n",
      "Train Epoch: 2696 [65792/118836 (55%)] Loss: 12291.848633\n",
      "Train Epoch: 2696 [98560/118836 (83%)] Loss: 12229.310547\n",
      "    epoch          : 2696\n",
      "    loss           : 12220.262969945461\n",
      "    val_loss       : 12218.73928479293\n",
      "    val_log_likelihood: -12141.841905597343\n",
      "    val_log_marginal: -12150.27246027089\n",
      "Train Epoch: 2697 [256/118836 (0%)] Loss: 12287.486328\n",
      "Train Epoch: 2697 [33024/118836 (28%)] Loss: 12159.794922\n",
      "Train Epoch: 2697 [65792/118836 (55%)] Loss: 12278.882812\n",
      "Train Epoch: 2697 [98560/118836 (83%)] Loss: 12216.658203\n",
      "    epoch          : 2697\n",
      "    loss           : 12219.06380386037\n",
      "    val_loss       : 12223.426361187912\n",
      "    val_log_likelihood: -12140.835249625206\n",
      "    val_log_marginal: -12149.12924733161\n",
      "Train Epoch: 2698 [256/118836 (0%)] Loss: 12285.855469\n",
      "Train Epoch: 2698 [33024/118836 (28%)] Loss: 12196.974609\n",
      "Train Epoch: 2698 [65792/118836 (55%)] Loss: 12384.420898\n",
      "Train Epoch: 2698 [98560/118836 (83%)] Loss: 12428.454102\n",
      "    epoch          : 2698\n",
      "    loss           : 12222.090043424318\n",
      "    val_loss       : 12230.197369593574\n",
      "    val_log_likelihood: -12148.949531993123\n",
      "    val_log_marginal: -12157.59616940877\n",
      "Train Epoch: 2699 [256/118836 (0%)] Loss: 12255.934570\n",
      "Train Epoch: 2699 [33024/118836 (28%)] Loss: 12322.962891\n",
      "Train Epoch: 2699 [65792/118836 (55%)] Loss: 12132.222656\n",
      "Train Epoch: 2699 [98560/118836 (83%)] Loss: 12223.415039\n",
      "    epoch          : 2699\n",
      "    loss           : 12223.64389684786\n",
      "    val_loss       : 12220.369288764761\n",
      "    val_log_likelihood: -12141.873266904468\n",
      "    val_log_marginal: -12150.368738653637\n",
      "Train Epoch: 2700 [256/118836 (0%)] Loss: 12168.761719\n",
      "Train Epoch: 2700 [33024/118836 (28%)] Loss: 12178.646484\n",
      "Train Epoch: 2700 [65792/118836 (55%)] Loss: 12250.173828\n",
      "Train Epoch: 2700 [98560/118836 (83%)] Loss: 12184.583984\n",
      "    epoch          : 2700\n",
      "    loss           : 12225.590423871743\n",
      "    val_loss       : 12222.541458150543\n",
      "    val_log_likelihood: -12149.423153497208\n",
      "    val_log_marginal: -12157.818517570799\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch2700.pth ...\n",
      "Train Epoch: 2701 [256/118836 (0%)] Loss: 12359.584961\n",
      "Train Epoch: 2701 [33024/118836 (28%)] Loss: 12338.063477\n",
      "Train Epoch: 2701 [65792/118836 (55%)] Loss: 12267.073242\n",
      "Train Epoch: 2701 [98560/118836 (83%)] Loss: 12334.897461\n",
      "    epoch          : 2701\n",
      "    loss           : 12219.45237735215\n",
      "    val_loss       : 12221.171620093184\n",
      "    val_log_likelihood: -12142.743567288306\n",
      "    val_log_marginal: -12151.021128199915\n",
      "Train Epoch: 2702 [256/118836 (0%)] Loss: 12301.291016\n",
      "Train Epoch: 2702 [33024/118836 (28%)] Loss: 12211.333984\n",
      "Train Epoch: 2702 [65792/118836 (55%)] Loss: 12231.122070\n",
      "Train Epoch: 2702 [98560/118836 (83%)] Loss: 12254.826172\n",
      "    epoch          : 2702\n",
      "    loss           : 12221.441413842795\n",
      "    val_loss       : 12225.886601873595\n",
      "    val_log_likelihood: -12140.976701108872\n",
      "    val_log_marginal: -12149.13255584262\n",
      "Train Epoch: 2703 [256/118836 (0%)] Loss: 12239.452148\n",
      "Train Epoch: 2703 [33024/118836 (28%)] Loss: 12288.904297\n",
      "Train Epoch: 2703 [65792/118836 (55%)] Loss: 12257.482422\n",
      "Train Epoch: 2703 [98560/118836 (83%)] Loss: 12304.050781\n",
      "    epoch          : 2703\n",
      "    loss           : 12223.892084754963\n",
      "    val_loss       : 12214.51495381753\n",
      "    val_log_likelihood: -12144.601242471826\n",
      "    val_log_marginal: -12153.06543816743\n",
      "Train Epoch: 2704 [256/118836 (0%)] Loss: 12187.267578\n",
      "Train Epoch: 2704 [33024/118836 (28%)] Loss: 12216.264648\n",
      "Train Epoch: 2704 [65792/118836 (55%)] Loss: 12239.954102\n",
      "Train Epoch: 2704 [98560/118836 (83%)] Loss: 12259.441406\n",
      "    epoch          : 2704\n",
      "    loss           : 12221.62717396221\n",
      "    val_loss       : 12220.207281787789\n",
      "    val_log_likelihood: -12143.02020474695\n",
      "    val_log_marginal: -12151.528007517218\n",
      "Train Epoch: 2705 [256/118836 (0%)] Loss: 12411.464844\n",
      "Train Epoch: 2705 [33024/118836 (28%)] Loss: 12250.076172\n",
      "Train Epoch: 2705 [65792/118836 (55%)] Loss: 12335.361328\n",
      "Train Epoch: 2705 [98560/118836 (83%)] Loss: 12271.847656\n",
      "    epoch          : 2705\n",
      "    loss           : 12223.304982003463\n",
      "    val_loss       : 12221.18970175057\n",
      "    val_log_likelihood: -12144.924178685897\n",
      "    val_log_marginal: -12153.142753598242\n",
      "Train Epoch: 2706 [256/118836 (0%)] Loss: 12339.324219\n",
      "Train Epoch: 2706 [33024/118836 (28%)] Loss: 12221.826172\n",
      "Train Epoch: 2706 [65792/118836 (55%)] Loss: 12225.946289\n",
      "Train Epoch: 2706 [98560/118836 (83%)] Loss: 12206.965820\n",
      "    epoch          : 2706\n",
      "    loss           : 12221.30868938689\n",
      "    val_loss       : 12220.319106996\n",
      "    val_log_likelihood: -12143.662053156017\n",
      "    val_log_marginal: -12152.078322696141\n",
      "Train Epoch: 2707 [256/118836 (0%)] Loss: 12209.981445\n",
      "Train Epoch: 2707 [33024/118836 (28%)] Loss: 12142.883789\n",
      "Train Epoch: 2707 [65792/118836 (55%)] Loss: 12229.943359\n",
      "Train Epoch: 2707 [98560/118836 (83%)] Loss: 12196.541016\n",
      "    epoch          : 2707\n",
      "    loss           : 12219.046093103805\n",
      "    val_loss       : 12219.6760272793\n",
      "    val_log_likelihood: -12140.946364344241\n",
      "    val_log_marginal: -12149.327728562233\n",
      "Train Epoch: 2708 [256/118836 (0%)] Loss: 12206.007812\n",
      "Train Epoch: 2708 [33024/118836 (28%)] Loss: 12309.810547\n",
      "Train Epoch: 2708 [65792/118836 (55%)] Loss: 12299.146484\n",
      "Train Epoch: 2708 [98560/118836 (83%)] Loss: 12254.960938\n",
      "    epoch          : 2708\n",
      "    loss           : 12225.025130369882\n",
      "    val_loss       : 12220.70752095141\n",
      "    val_log_likelihood: -12143.744616870865\n",
      "    val_log_marginal: -12152.304419126145\n",
      "Train Epoch: 2709 [256/118836 (0%)] Loss: 12314.876953\n",
      "Train Epoch: 2709 [33024/118836 (28%)] Loss: 12278.560547\n",
      "Train Epoch: 2709 [65792/118836 (55%)] Loss: 12272.098633\n",
      "Train Epoch: 2709 [98560/118836 (83%)] Loss: 12285.209961\n",
      "    epoch          : 2709\n",
      "    loss           : 12221.046658524607\n",
      "    val_loss       : 12216.838898679927\n",
      "    val_log_likelihood: -12142.676242633373\n",
      "    val_log_marginal: -12150.943939194096\n",
      "Train Epoch: 2710 [256/118836 (0%)] Loss: 12149.652344\n",
      "Train Epoch: 2710 [33024/118836 (28%)] Loss: 12201.765625\n",
      "Train Epoch: 2710 [65792/118836 (55%)] Loss: 12264.625000\n",
      "Train Epoch: 2710 [98560/118836 (83%)] Loss: 12234.736328\n",
      "    epoch          : 2710\n",
      "    loss           : 12225.881688281896\n",
      "    val_loss       : 12215.700145332525\n",
      "    val_log_likelihood: -12140.814935994365\n",
      "    val_log_marginal: -12149.01738819386\n",
      "Train Epoch: 2711 [256/118836 (0%)] Loss: 12208.089844\n",
      "Train Epoch: 2711 [33024/118836 (28%)] Loss: 12202.019531\n",
      "Train Epoch: 2711 [65792/118836 (55%)] Loss: 12242.161133\n",
      "Train Epoch: 2711 [98560/118836 (83%)] Loss: 12240.432617\n",
      "    epoch          : 2711\n",
      "    loss           : 12218.443023999691\n",
      "    val_loss       : 12220.720880698604\n",
      "    val_log_likelihood: -12145.872262555573\n",
      "    val_log_marginal: -12154.17396603797\n",
      "Train Epoch: 2712 [256/118836 (0%)] Loss: 12227.955078\n",
      "Train Epoch: 2712 [33024/118836 (28%)] Loss: 12280.000000\n",
      "Train Epoch: 2712 [65792/118836 (55%)] Loss: 12324.641602\n",
      "Train Epoch: 2712 [98560/118836 (83%)] Loss: 12318.637695\n",
      "    epoch          : 2712\n",
      "    loss           : 12217.705658085193\n",
      "    val_loss       : 12223.080344663247\n",
      "    val_log_likelihood: -12151.65040936466\n",
      "    val_log_marginal: -12160.082979027164\n",
      "Train Epoch: 2713 [256/118836 (0%)] Loss: 12234.089844\n",
      "Train Epoch: 2713 [33024/118836 (28%)] Loss: 12288.176758\n",
      "Train Epoch: 2713 [65792/118836 (55%)] Loss: 12262.882812\n",
      "Train Epoch: 2713 [98560/118836 (83%)] Loss: 12348.270508\n",
      "    epoch          : 2713\n",
      "    loss           : 12220.32631743047\n",
      "    val_loss       : 12220.957496986848\n",
      "    val_log_likelihood: -12146.360132663875\n",
      "    val_log_marginal: -12154.928726686205\n",
      "Train Epoch: 2714 [256/118836 (0%)] Loss: 12265.135742\n",
      "Train Epoch: 2714 [33024/118836 (28%)] Loss: 12221.470703\n",
      "Train Epoch: 2714 [65792/118836 (55%)] Loss: 12236.732422\n",
      "Train Epoch: 2714 [98560/118836 (83%)] Loss: 12266.601562\n",
      "    epoch          : 2714\n",
      "    loss           : 12216.253132269696\n",
      "    val_loss       : 12220.491858578536\n",
      "    val_log_likelihood: -12142.418572457867\n",
      "    val_log_marginal: -12150.871320792572\n",
      "Train Epoch: 2715 [256/118836 (0%)] Loss: 12344.134766\n",
      "Train Epoch: 2715 [33024/118836 (28%)] Loss: 12199.417969\n",
      "Train Epoch: 2715 [65792/118836 (55%)] Loss: 12209.911133\n",
      "Train Epoch: 2715 [98560/118836 (83%)] Loss: 12237.048828\n",
      "    epoch          : 2715\n",
      "    loss           : 12216.071028161186\n",
      "    val_loss       : 12223.377954108608\n",
      "    val_log_likelihood: -12141.883369520265\n",
      "    val_log_marginal: -12150.182703465953\n",
      "Train Epoch: 2716 [256/118836 (0%)] Loss: 12133.870117\n",
      "Train Epoch: 2716 [33024/118836 (28%)] Loss: 12235.255859\n",
      "Train Epoch: 2716 [65792/118836 (55%)] Loss: 12225.743164\n",
      "Train Epoch: 2716 [98560/118836 (83%)] Loss: 12175.990234\n",
      "    epoch          : 2716\n",
      "    loss           : 12217.674443787479\n",
      "    val_loss       : 12220.07397266682\n",
      "    val_log_likelihood: -12141.713751033913\n",
      "    val_log_marginal: -12150.019233966823\n",
      "Train Epoch: 2717 [256/118836 (0%)] Loss: 12299.839844\n",
      "Train Epoch: 2717 [33024/118836 (28%)] Loss: 12197.355469\n",
      "Train Epoch: 2717 [65792/118836 (55%)] Loss: 12222.815430\n",
      "Train Epoch: 2717 [98560/118836 (83%)] Loss: 12167.027344\n",
      "    epoch          : 2717\n",
      "    loss           : 12217.623474171578\n",
      "    val_loss       : 12224.972100980218\n",
      "    val_log_likelihood: -12139.747058196339\n",
      "    val_log_marginal: -12148.111881923282\n",
      "Train Epoch: 2718 [256/118836 (0%)] Loss: 12262.220703\n",
      "Train Epoch: 2718 [33024/118836 (28%)] Loss: 12231.013672\n",
      "Train Epoch: 2718 [65792/118836 (55%)] Loss: 12182.631836\n",
      "Train Epoch: 2718 [98560/118836 (83%)] Loss: 12287.107422\n",
      "    epoch          : 2718\n",
      "    loss           : 12231.777516284119\n",
      "    val_loss       : 12222.877883005736\n",
      "    val_log_likelihood: -12146.269869210091\n",
      "    val_log_marginal: -12154.996691051087\n",
      "Train Epoch: 2719 [256/118836 (0%)] Loss: 12284.311523\n",
      "Train Epoch: 2719 [33024/118836 (28%)] Loss: 12322.319336\n",
      "Train Epoch: 2719 [65792/118836 (55%)] Loss: 12153.099609\n",
      "Train Epoch: 2719 [98560/118836 (83%)] Loss: 12126.225586\n",
      "    epoch          : 2719\n",
      "    loss           : 12217.072776603856\n",
      "    val_loss       : 12218.192878671498\n",
      "    val_log_likelihood: -12141.404133387612\n",
      "    val_log_marginal: -12149.770710538129\n",
      "Train Epoch: 2720 [256/118836 (0%)] Loss: 12193.425781\n",
      "Train Epoch: 2720 [33024/118836 (28%)] Loss: 12313.810547\n",
      "Train Epoch: 2720 [65792/118836 (55%)] Loss: 12209.986328\n",
      "Train Epoch: 2720 [98560/118836 (83%)] Loss: 12142.794922\n",
      "    epoch          : 2720\n",
      "    loss           : 12224.275084974668\n",
      "    val_loss       : 12222.227426389318\n",
      "    val_log_likelihood: -12144.978017570049\n",
      "    val_log_marginal: -12153.3547396343\n",
      "Train Epoch: 2721 [256/118836 (0%)] Loss: 12239.699219\n",
      "Train Epoch: 2721 [33024/118836 (28%)] Loss: 12204.476562\n",
      "Train Epoch: 2721 [65792/118836 (55%)] Loss: 12285.069336\n",
      "Train Epoch: 2721 [98560/118836 (83%)] Loss: 12295.831055\n",
      "    epoch          : 2721\n",
      "    loss           : 12217.178230814463\n",
      "    val_loss       : 12216.02559832911\n",
      "    val_log_likelihood: -12141.357299582558\n",
      "    val_log_marginal: -12149.655672098617\n",
      "Train Epoch: 2722 [256/118836 (0%)] Loss: 12181.102539\n",
      "Train Epoch: 2722 [33024/118836 (28%)] Loss: 12252.394531\n",
      "Train Epoch: 2722 [65792/118836 (55%)] Loss: 12235.402344\n",
      "Train Epoch: 2722 [98560/118836 (83%)] Loss: 12267.597656\n",
      "    epoch          : 2722\n",
      "    loss           : 12228.121509415063\n",
      "    val_loss       : 12222.613132237915\n",
      "    val_log_likelihood: -12141.851843594914\n",
      "    val_log_marginal: -12150.349849907763\n",
      "Train Epoch: 2723 [256/118836 (0%)] Loss: 12339.049805\n",
      "Train Epoch: 2723 [33024/118836 (28%)] Loss: 12190.416016\n",
      "Train Epoch: 2723 [65792/118836 (55%)] Loss: 12191.785156\n",
      "Train Epoch: 2723 [98560/118836 (83%)] Loss: 12223.027344\n",
      "    epoch          : 2723\n",
      "    loss           : 12223.191770542546\n",
      "    val_loss       : 12221.523581617112\n",
      "    val_log_likelihood: -12140.155923509874\n",
      "    val_log_marginal: -12148.493191641224\n",
      "Train Epoch: 2724 [256/118836 (0%)] Loss: 12263.615234\n",
      "Train Epoch: 2724 [33024/118836 (28%)] Loss: 12327.408203\n",
      "Train Epoch: 2724 [65792/118836 (55%)] Loss: 12290.726562\n",
      "Train Epoch: 2724 [98560/118836 (83%)] Loss: 12249.082031\n",
      "    epoch          : 2724\n",
      "    loss           : 12221.481896356752\n",
      "    val_loss       : 12222.503527238596\n",
      "    val_log_likelihood: -12138.678722730563\n",
      "    val_log_marginal: -12146.995562525368\n",
      "Train Epoch: 2725 [256/118836 (0%)] Loss: 12138.029297\n",
      "Train Epoch: 2725 [33024/118836 (28%)] Loss: 12280.869141\n",
      "Train Epoch: 2725 [65792/118836 (55%)] Loss: 12304.521484\n",
      "Train Epoch: 2725 [98560/118836 (83%)] Loss: 12219.590820\n",
      "    epoch          : 2725\n",
      "    loss           : 12220.155547262717\n",
      "    val_loss       : 12221.582184313525\n",
      "    val_log_likelihood: -12142.989859097137\n",
      "    val_log_marginal: -12151.621656486775\n",
      "Train Epoch: 2726 [256/118836 (0%)] Loss: 12142.069336\n",
      "Train Epoch: 2726 [33024/118836 (28%)] Loss: 12231.715820\n",
      "Train Epoch: 2726 [65792/118836 (55%)] Loss: 12250.525391\n",
      "Train Epoch: 2726 [98560/118836 (83%)] Loss: 12212.369141\n",
      "    epoch          : 2726\n",
      "    loss           : 12221.72644666951\n",
      "    val_loss       : 12243.883042374553\n",
      "    val_log_likelihood: -12150.355999760908\n",
      "    val_log_marginal: -12158.920202502246\n",
      "Train Epoch: 2727 [256/118836 (0%)] Loss: 12242.164062\n",
      "Train Epoch: 2727 [33024/118836 (28%)] Loss: 12246.924805\n",
      "Train Epoch: 2727 [65792/118836 (55%)] Loss: 12298.939453\n",
      "Train Epoch: 2727 [98560/118836 (83%)] Loss: 12224.092773\n",
      "    epoch          : 2727\n",
      "    loss           : 12221.877140036962\n",
      "    val_loss       : 12221.532039701979\n",
      "    val_log_likelihood: -12143.176352163462\n",
      "    val_log_marginal: -12151.656377125144\n",
      "Train Epoch: 2728 [256/118836 (0%)] Loss: 12276.000000\n",
      "Train Epoch: 2728 [33024/118836 (28%)] Loss: 12244.126953\n",
      "Train Epoch: 2728 [65792/118836 (55%)] Loss: 12267.142578\n",
      "Train Epoch: 2728 [98560/118836 (83%)] Loss: 12172.104492\n",
      "    epoch          : 2728\n",
      "    loss           : 12226.26550513079\n",
      "    val_loss       : 12226.316701847672\n",
      "    val_log_likelihood: -12148.033119281174\n",
      "    val_log_marginal: -12156.40538722159\n",
      "Train Epoch: 2729 [256/118836 (0%)] Loss: 12209.029297\n",
      "Train Epoch: 2729 [33024/118836 (28%)] Loss: 12211.049805\n",
      "Train Epoch: 2729 [65792/118836 (55%)] Loss: 12260.816406\n",
      "Train Epoch: 2729 [98560/118836 (83%)] Loss: 12221.220703\n",
      "    epoch          : 2729\n",
      "    loss           : 12220.670478572169\n",
      "    val_loss       : 12218.875016771084\n",
      "    val_log_likelihood: -12144.963586092586\n",
      "    val_log_marginal: -12153.350172407985\n",
      "Train Epoch: 2730 [256/118836 (0%)] Loss: 12252.628906\n",
      "Train Epoch: 2730 [33024/118836 (28%)] Loss: 12337.662109\n",
      "Train Epoch: 2730 [65792/118836 (55%)] Loss: 12197.483398\n",
      "Train Epoch: 2730 [98560/118836 (83%)] Loss: 12410.551758\n",
      "    epoch          : 2730\n",
      "    loss           : 12221.197927328889\n",
      "    val_loss       : 12221.747261495297\n",
      "    val_log_likelihood: -12140.414279621587\n",
      "    val_log_marginal: -12148.838924718675\n",
      "Train Epoch: 2731 [256/118836 (0%)] Loss: 12404.157227\n",
      "Train Epoch: 2731 [33024/118836 (28%)] Loss: 12218.698242\n",
      "Train Epoch: 2731 [65792/118836 (55%)] Loss: 12257.060547\n",
      "Train Epoch: 2731 [98560/118836 (83%)] Loss: 12152.123047\n",
      "    epoch          : 2731\n",
      "    loss           : 12224.85892767137\n",
      "    val_loss       : 12220.630767023247\n",
      "    val_log_likelihood: -12141.62143332558\n",
      "    val_log_marginal: -12149.903028813838\n",
      "Train Epoch: 2732 [256/118836 (0%)] Loss: 12163.770508\n",
      "Train Epoch: 2732 [33024/118836 (28%)] Loss: 12184.057617\n",
      "Train Epoch: 2732 [65792/118836 (55%)] Loss: 12267.728516\n",
      "Train Epoch: 2732 [98560/118836 (83%)] Loss: 12255.004883\n",
      "    epoch          : 2732\n",
      "    loss           : 12220.804727079458\n",
      "    val_loss       : 12225.32018295167\n",
      "    val_log_likelihood: -12146.548748481442\n",
      "    val_log_marginal: -12155.164368469379\n",
      "Train Epoch: 2733 [256/118836 (0%)] Loss: 12188.251953\n",
      "Train Epoch: 2733 [33024/118836 (28%)] Loss: 12336.976562\n",
      "Train Epoch: 2733 [65792/118836 (55%)] Loss: 12265.609375\n",
      "Train Epoch: 2733 [98560/118836 (83%)] Loss: 12253.947266\n",
      "    epoch          : 2733\n",
      "    loss           : 12221.682675991264\n",
      "    val_loss       : 12219.883617507683\n",
      "    val_log_likelihood: -12138.842060684192\n",
      "    val_log_marginal: -12147.196663915282\n",
      "Train Epoch: 2734 [256/118836 (0%)] Loss: 12216.442383\n",
      "Train Epoch: 2734 [33024/118836 (28%)] Loss: 12246.013672\n",
      "Train Epoch: 2734 [65792/118836 (55%)] Loss: 12183.593750\n",
      "Train Epoch: 2734 [98560/118836 (83%)] Loss: 12305.521484\n",
      "    epoch          : 2734\n",
      "    loss           : 12219.487088858303\n",
      "    val_loss       : 12221.337323558186\n",
      "    val_log_likelihood: -12141.496862560743\n",
      "    val_log_marginal: -12149.868398392733\n",
      "Train Epoch: 2735 [256/118836 (0%)] Loss: 12283.251953\n",
      "Train Epoch: 2735 [33024/118836 (28%)] Loss: 12198.073242\n",
      "Train Epoch: 2735 [65792/118836 (55%)] Loss: 12147.349609\n",
      "Train Epoch: 2735 [98560/118836 (83%)] Loss: 12298.992188\n",
      "    epoch          : 2735\n",
      "    loss           : 12219.919273418114\n",
      "    val_loss       : 12223.935572665801\n",
      "    val_log_likelihood: -12141.48158796009\n",
      "    val_log_marginal: -12149.804908224878\n",
      "Train Epoch: 2736 [256/118836 (0%)] Loss: 12300.339844\n",
      "Train Epoch: 2736 [33024/118836 (28%)] Loss: 12331.803711\n",
      "Train Epoch: 2736 [65792/118836 (55%)] Loss: 12210.759766\n",
      "Train Epoch: 2736 [98560/118836 (83%)] Loss: 12282.767578\n",
      "    epoch          : 2736\n",
      "    loss           : 12222.505871006513\n",
      "    val_loss       : 12219.466397288117\n",
      "    val_log_likelihood: -12141.091911090001\n",
      "    val_log_marginal: -12149.522680374457\n",
      "Train Epoch: 2737 [256/118836 (0%)] Loss: 12330.015625\n",
      "Train Epoch: 2737 [33024/118836 (28%)] Loss: 12178.357422\n",
      "Train Epoch: 2737 [65792/118836 (55%)] Loss: 12286.753906\n",
      "Train Epoch: 2737 [98560/118836 (83%)] Loss: 12263.361328\n",
      "    epoch          : 2737\n",
      "    loss           : 12220.244492639838\n",
      "    val_loss       : 12225.55253064708\n",
      "    val_log_likelihood: -12148.233612974307\n",
      "    val_log_marginal: -12156.737238553489\n",
      "Train Epoch: 2738 [256/118836 (0%)] Loss: 12202.472656\n",
      "Train Epoch: 2738 [33024/118836 (28%)] Loss: 12190.026367\n",
      "Train Epoch: 2738 [65792/118836 (55%)] Loss: 12192.753906\n",
      "Train Epoch: 2738 [98560/118836 (83%)] Loss: 12295.622070\n",
      "    epoch          : 2738\n",
      "    loss           : 12219.39683170492\n",
      "    val_loss       : 12217.954232708214\n",
      "    val_log_likelihood: -12141.654289605305\n",
      "    val_log_marginal: -12149.907920387146\n",
      "Train Epoch: 2739 [256/118836 (0%)] Loss: 12183.145508\n",
      "Train Epoch: 2739 [33024/118836 (28%)] Loss: 12213.242188\n",
      "Train Epoch: 2739 [65792/118836 (55%)] Loss: 12156.958008\n",
      "Train Epoch: 2739 [98560/118836 (83%)] Loss: 12198.719727\n",
      "    epoch          : 2739\n",
      "    loss           : 12220.575547327338\n",
      "    val_loss       : 12218.653687076428\n",
      "    val_log_likelihood: -12141.193347258839\n",
      "    val_log_marginal: -12149.544111501154\n",
      "Train Epoch: 2740 [256/118836 (0%)] Loss: 12226.283203\n",
      "Train Epoch: 2740 [33024/118836 (28%)] Loss: 12196.990234\n",
      "Train Epoch: 2740 [65792/118836 (55%)] Loss: 12181.966797\n",
      "Train Epoch: 2740 [98560/118836 (83%)] Loss: 12143.441406\n",
      "    epoch          : 2740\n",
      "    loss           : 12219.098824247829\n",
      "    val_loss       : 12219.555556258461\n",
      "    val_log_likelihood: -12142.425615016286\n",
      "    val_log_marginal: -12150.708104049145\n",
      "Train Epoch: 2741 [256/118836 (0%)] Loss: 12226.826172\n",
      "Train Epoch: 2741 [33024/118836 (28%)] Loss: 12228.556641\n",
      "Train Epoch: 2741 [65792/118836 (55%)] Loss: 12160.590820\n",
      "Train Epoch: 2741 [98560/118836 (83%)] Loss: 12353.975586\n",
      "    epoch          : 2741\n",
      "    loss           : 12214.074897901157\n",
      "    val_loss       : 12221.78094632717\n",
      "    val_log_likelihood: -12141.185884350443\n",
      "    val_log_marginal: -12149.314914609586\n",
      "Train Epoch: 2742 [256/118836 (0%)] Loss: 12167.248047\n",
      "Train Epoch: 2742 [33024/118836 (28%)] Loss: 12241.760742\n",
      "Train Epoch: 2742 [65792/118836 (55%)] Loss: 12279.921875\n",
      "Train Epoch: 2742 [98560/118836 (83%)] Loss: 12368.392578\n",
      "    epoch          : 2742\n",
      "    loss           : 12222.40426585763\n",
      "    val_loss       : 12215.9548400572\n",
      "    val_log_likelihood: -12140.525386747828\n",
      "    val_log_marginal: -12148.782881828136\n",
      "Train Epoch: 2743 [256/118836 (0%)] Loss: 12198.697266\n",
      "Train Epoch: 2743 [33024/118836 (28%)] Loss: 12213.511719\n",
      "Train Epoch: 2743 [65792/118836 (55%)] Loss: 12261.205078\n",
      "Train Epoch: 2743 [98560/118836 (83%)] Loss: 12296.705078\n",
      "    epoch          : 2743\n",
      "    loss           : 12220.597064335194\n",
      "    val_loss       : 12219.315899408022\n",
      "    val_log_likelihood: -12140.686827472344\n",
      "    val_log_marginal: -12149.061678351156\n",
      "Train Epoch: 2744 [256/118836 (0%)] Loss: 12263.929688\n",
      "Train Epoch: 2744 [33024/118836 (28%)] Loss: 12199.731445\n",
      "Train Epoch: 2744 [65792/118836 (55%)] Loss: 12204.786133\n",
      "Train Epoch: 2744 [98560/118836 (83%)] Loss: 12277.938477\n",
      "    epoch          : 2744\n",
      "    loss           : 12221.695195861766\n",
      "    val_loss       : 12236.59563908382\n",
      "    val_log_likelihood: -12151.887639093517\n",
      "    val_log_marginal: -12160.480531093048\n",
      "Train Epoch: 2745 [256/118836 (0%)] Loss: 12348.169922\n",
      "Train Epoch: 2745 [33024/118836 (28%)] Loss: 12232.404297\n",
      "Train Epoch: 2745 [65792/118836 (55%)] Loss: 12177.276367\n",
      "Train Epoch: 2745 [98560/118836 (83%)] Loss: 12338.733398\n",
      "    epoch          : 2745\n",
      "    loss           : 12219.8778893003\n",
      "    val_loss       : 12218.465007534238\n",
      "    val_log_likelihood: -12146.526916453422\n",
      "    val_log_marginal: -12154.950606110775\n",
      "Train Epoch: 2746 [256/118836 (0%)] Loss: 12309.493164\n",
      "Train Epoch: 2746 [33024/118836 (28%)] Loss: 12285.748047\n",
      "Train Epoch: 2746 [65792/118836 (55%)] Loss: 12185.462891\n",
      "Train Epoch: 2746 [98560/118836 (83%)] Loss: 12295.969727\n",
      "    epoch          : 2746\n",
      "    loss           : 12223.019224630376\n",
      "    val_loss       : 12224.162684570183\n",
      "    val_log_likelihood: -12148.700660734596\n",
      "    val_log_marginal: -12157.079015417055\n",
      "Train Epoch: 2747 [256/118836 (0%)] Loss: 12269.028320\n",
      "Train Epoch: 2747 [33024/118836 (28%)] Loss: 12189.167969\n",
      "Train Epoch: 2747 [65792/118836 (55%)] Loss: 12343.379883\n",
      "Train Epoch: 2747 [98560/118836 (83%)] Loss: 12240.663086\n",
      "    epoch          : 2747\n",
      "    loss           : 12219.21958649969\n",
      "    val_loss       : 12227.422014662658\n",
      "    val_log_likelihood: -12143.59121772255\n",
      "    val_log_marginal: -12152.06075463917\n",
      "Train Epoch: 2748 [256/118836 (0%)] Loss: 12194.224609\n",
      "Train Epoch: 2748 [33024/118836 (28%)] Loss: 12231.927734\n",
      "Train Epoch: 2748 [65792/118836 (55%)] Loss: 12175.917969\n",
      "Train Epoch: 2748 [98560/118836 (83%)] Loss: 12286.253906\n",
      "    epoch          : 2748\n",
      "    loss           : 12217.20122163203\n",
      "    val_loss       : 12222.934602358484\n",
      "    val_log_likelihood: -12143.17469418812\n",
      "    val_log_marginal: -12151.49529633305\n",
      "Train Epoch: 2749 [256/118836 (0%)] Loss: 12195.178711\n",
      "Train Epoch: 2749 [33024/118836 (28%)] Loss: 12288.588867\n",
      "Train Epoch: 2749 [65792/118836 (55%)] Loss: 12180.266602\n",
      "Train Epoch: 2749 [98560/118836 (83%)] Loss: 12343.238281\n",
      "    epoch          : 2749\n",
      "    loss           : 12219.33018717044\n",
      "    val_loss       : 12221.178879783098\n",
      "    val_log_likelihood: -12142.029109155295\n",
      "    val_log_marginal: -12150.4106859121\n",
      "Train Epoch: 2750 [256/118836 (0%)] Loss: 12201.743164\n",
      "Train Epoch: 2750 [33024/118836 (28%)] Loss: 12214.625977\n",
      "Train Epoch: 2750 [65792/118836 (55%)] Loss: 12310.345703\n",
      "Train Epoch: 2750 [98560/118836 (83%)] Loss: 12241.092773\n",
      "    epoch          : 2750\n",
      "    loss           : 12218.819880680056\n",
      "    val_loss       : 12217.081556062141\n",
      "    val_log_likelihood: -12142.606619462107\n",
      "    val_log_marginal: -12150.87188497716\n",
      "Train Epoch: 2751 [256/118836 (0%)] Loss: 12237.210938\n",
      "Train Epoch: 2751 [33024/118836 (28%)] Loss: 12164.519531\n",
      "Train Epoch: 2751 [65792/118836 (55%)] Loss: 12146.895508\n",
      "Train Epoch: 2751 [98560/118836 (83%)] Loss: 12323.106445\n",
      "    epoch          : 2751\n",
      "    loss           : 12217.278593168423\n",
      "    val_loss       : 12220.982230124388\n",
      "    val_log_likelihood: -12142.382416220793\n",
      "    val_log_marginal: -12150.672064428205\n",
      "Train Epoch: 2752 [256/118836 (0%)] Loss: 12275.371094\n",
      "Train Epoch: 2752 [33024/118836 (28%)] Loss: 12171.794922\n",
      "Train Epoch: 2752 [65792/118836 (55%)] Loss: 12291.631836\n",
      "Train Epoch: 2752 [98560/118836 (83%)] Loss: 12169.040039\n",
      "    epoch          : 2752\n",
      "    loss           : 12219.890307879705\n",
      "    val_loss       : 12215.736132572903\n",
      "    val_log_likelihood: -12141.297807782774\n",
      "    val_log_marginal: -12149.577044264943\n",
      "Train Epoch: 2753 [256/118836 (0%)] Loss: 12267.310547\n",
      "Train Epoch: 2753 [33024/118836 (28%)] Loss: 12216.206055\n",
      "Train Epoch: 2753 [65792/118836 (55%)] Loss: 12201.765625\n",
      "Train Epoch: 2753 [98560/118836 (83%)] Loss: 12213.433594\n",
      "    epoch          : 2753\n",
      "    loss           : 12224.282883258375\n",
      "    val_loss       : 12230.616170461426\n",
      "    val_log_likelihood: -12148.210141872156\n",
      "    val_log_marginal: -12156.779556457976\n",
      "Train Epoch: 2754 [256/118836 (0%)] Loss: 12279.586914\n",
      "Train Epoch: 2754 [33024/118836 (28%)] Loss: 12231.041016\n",
      "Train Epoch: 2754 [65792/118836 (55%)] Loss: 12295.555664\n",
      "Train Epoch: 2754 [98560/118836 (83%)] Loss: 12297.589844\n",
      "    epoch          : 2754\n",
      "    loss           : 12222.881672773212\n",
      "    val_loss       : 12224.215513350242\n",
      "    val_log_likelihood: -12149.743537078682\n",
      "    val_log_marginal: -12158.370747079836\n",
      "Train Epoch: 2755 [256/118836 (0%)] Loss: 12214.262695\n",
      "Train Epoch: 2755 [33024/118836 (28%)] Loss: 12323.991211\n",
      "Train Epoch: 2755 [65792/118836 (55%)] Loss: 12191.350586\n",
      "Train Epoch: 2755 [98560/118836 (83%)] Loss: 12231.969727\n",
      "    epoch          : 2755\n",
      "    loss           : 12218.022444297974\n",
      "    val_loss       : 12220.88306033784\n",
      "    val_log_likelihood: -12144.547437512925\n",
      "    val_log_marginal: -12152.751289065725\n",
      "Train Epoch: 2756 [256/118836 (0%)] Loss: 12256.162109\n",
      "Train Epoch: 2756 [33024/118836 (28%)] Loss: 12301.645508\n",
      "Train Epoch: 2756 [65792/118836 (55%)] Loss: 12314.353516\n",
      "Train Epoch: 2756 [98560/118836 (83%)] Loss: 12251.709961\n",
      "    epoch          : 2756\n",
      "    loss           : 12219.698102447786\n",
      "    val_loss       : 12221.661087617449\n",
      "    val_log_likelihood: -12141.925528426127\n",
      "    val_log_marginal: -12150.304596352275\n",
      "Train Epoch: 2757 [256/118836 (0%)] Loss: 12268.551758\n",
      "Train Epoch: 2757 [33024/118836 (28%)] Loss: 12289.233398\n",
      "Train Epoch: 2757 [65792/118836 (55%)] Loss: 12234.990234\n",
      "Train Epoch: 2757 [98560/118836 (83%)] Loss: 12272.445312\n",
      "    epoch          : 2757\n",
      "    loss           : 12220.679219008478\n",
      "    val_loss       : 12218.910807260469\n",
      "    val_log_likelihood: -12144.300628101737\n",
      "    val_log_marginal: -12152.797650454688\n",
      "Train Epoch: 2758 [256/118836 (0%)] Loss: 12234.076172\n",
      "Train Epoch: 2758 [33024/118836 (28%)] Loss: 12249.330078\n",
      "Train Epoch: 2758 [65792/118836 (55%)] Loss: 12203.724609\n",
      "Train Epoch: 2758 [98560/118836 (83%)] Loss: 12150.511719\n",
      "    epoch          : 2758\n",
      "    loss           : 12221.64472236223\n",
      "    val_loss       : 12217.737161614394\n",
      "    val_log_likelihood: -12141.474345565808\n",
      "    val_log_marginal: -12149.668846484381\n",
      "Train Epoch: 2759 [256/118836 (0%)] Loss: 12209.299805\n",
      "Train Epoch: 2759 [33024/118836 (28%)] Loss: 12188.462891\n",
      "Train Epoch: 2759 [65792/118836 (55%)] Loss: 12175.598633\n",
      "Train Epoch: 2759 [98560/118836 (83%)] Loss: 12216.368164\n",
      "    epoch          : 2759\n",
      "    loss           : 12218.094240623708\n",
      "    val_loss       : 12219.18414503824\n",
      "    val_log_likelihood: -12144.11294296681\n",
      "    val_log_marginal: -12152.329790372323\n",
      "Train Epoch: 2760 [256/118836 (0%)] Loss: 12352.056641\n",
      "Train Epoch: 2760 [33024/118836 (28%)] Loss: 12212.998047\n",
      "Train Epoch: 2760 [65792/118836 (55%)] Loss: 12249.372070\n",
      "Train Epoch: 2760 [98560/118836 (83%)] Loss: 12315.226562\n",
      "    epoch          : 2760\n",
      "    loss           : 12217.538774620036\n",
      "    val_loss       : 12217.564888793553\n",
      "    val_log_likelihood: -12142.265243260184\n",
      "    val_log_marginal: -12150.631314679607\n",
      "Train Epoch: 2761 [256/118836 (0%)] Loss: 12262.227539\n",
      "Train Epoch: 2761 [33024/118836 (28%)] Loss: 12190.542969\n",
      "Train Epoch: 2761 [65792/118836 (55%)] Loss: 12179.041016\n",
      "Train Epoch: 2761 [98560/118836 (83%)] Loss: 12207.601562\n",
      "    epoch          : 2761\n",
      "    loss           : 12219.41519414935\n",
      "    val_loss       : 12220.476554235578\n",
      "    val_log_likelihood: -12143.626666860524\n",
      "    val_log_marginal: -12151.990582944594\n",
      "Train Epoch: 2762 [256/118836 (0%)] Loss: 12170.858398\n",
      "Train Epoch: 2762 [33024/118836 (28%)] Loss: 12280.474609\n",
      "Train Epoch: 2762 [65792/118836 (55%)] Loss: 12283.569336\n",
      "Train Epoch: 2762 [98560/118836 (83%)] Loss: 12251.092773\n",
      "    epoch          : 2762\n",
      "    loss           : 12220.318123190653\n",
      "    val_loss       : 12220.44967435027\n",
      "    val_log_likelihood: -12142.485212145886\n",
      "    val_log_marginal: -12150.780563368395\n",
      "Train Epoch: 2763 [256/118836 (0%)] Loss: 12287.467773\n",
      "Train Epoch: 2763 [33024/118836 (28%)] Loss: 12339.083984\n",
      "Train Epoch: 2763 [65792/118836 (55%)] Loss: 12225.261719\n",
      "Train Epoch: 2763 [98560/118836 (83%)] Loss: 12253.988281\n",
      "    epoch          : 2763\n",
      "    loss           : 12218.415176055883\n",
      "    val_loss       : 12215.646549568588\n",
      "    val_log_likelihood: -12140.703119345793\n",
      "    val_log_marginal: -12149.072107643598\n",
      "Train Epoch: 2764 [256/118836 (0%)] Loss: 12196.337891\n",
      "Train Epoch: 2764 [33024/118836 (28%)] Loss: 12186.455078\n",
      "Train Epoch: 2764 [65792/118836 (55%)] Loss: 12333.980469\n",
      "Train Epoch: 2764 [98560/118836 (83%)] Loss: 12246.379883\n",
      "    epoch          : 2764\n",
      "    loss           : 12216.848624250413\n",
      "    val_loss       : 12218.634465706098\n",
      "    val_log_likelihood: -12141.98651665245\n",
      "    val_log_marginal: -12150.382778978561\n",
      "Train Epoch: 2765 [256/118836 (0%)] Loss: 12262.904297\n",
      "Train Epoch: 2765 [33024/118836 (28%)] Loss: 12174.188477\n",
      "Train Epoch: 2765 [65792/118836 (55%)] Loss: 12284.319336\n",
      "Train Epoch: 2765 [98560/118836 (83%)] Loss: 12233.221680\n",
      "    epoch          : 2765\n",
      "    loss           : 12221.16583775977\n",
      "    val_loss       : 12221.243590394484\n",
      "    val_log_likelihood: -12142.361337979477\n",
      "    val_log_marginal: -12150.813430919472\n",
      "Train Epoch: 2766 [256/118836 (0%)] Loss: 12185.628906\n",
      "Train Epoch: 2766 [33024/118836 (28%)] Loss: 12215.258789\n",
      "Train Epoch: 2766 [65792/118836 (55%)] Loss: 12248.214844\n",
      "Train Epoch: 2766 [98560/118836 (83%)] Loss: 12146.937500\n",
      "    epoch          : 2766\n",
      "    loss           : 12220.675232468724\n",
      "    val_loss       : 12217.396159924032\n",
      "    val_log_likelihood: -12140.92185060613\n",
      "    val_log_marginal: -12149.113084499979\n",
      "Train Epoch: 2767 [256/118836 (0%)] Loss: 12395.107422\n",
      "Train Epoch: 2767 [33024/118836 (28%)] Loss: 12224.189453\n",
      "Train Epoch: 2767 [65792/118836 (55%)] Loss: 12211.171875\n",
      "Train Epoch: 2767 [98560/118836 (83%)] Loss: 12300.070312\n",
      "    epoch          : 2767\n",
      "    loss           : 12223.517808655139\n",
      "    val_loss       : 12219.724994280608\n",
      "    val_log_likelihood: -12145.040247945099\n",
      "    val_log_marginal: -12153.687089995328\n",
      "Train Epoch: 2768 [256/118836 (0%)] Loss: 12213.736328\n",
      "Train Epoch: 2768 [33024/118836 (28%)] Loss: 12279.876953\n",
      "Train Epoch: 2768 [65792/118836 (55%)] Loss: 12336.848633\n",
      "Train Epoch: 2768 [98560/118836 (83%)] Loss: 12185.573242\n",
      "    epoch          : 2768\n",
      "    loss           : 12223.410090499638\n",
      "    val_loss       : 12213.62702885037\n",
      "    val_log_likelihood: -12140.33849804849\n",
      "    val_log_marginal: -12148.734149335587\n",
      "Train Epoch: 2769 [256/118836 (0%)] Loss: 12180.175781\n",
      "Train Epoch: 2769 [33024/118836 (28%)] Loss: 12304.146484\n",
      "Train Epoch: 2769 [65792/118836 (55%)] Loss: 12226.093750\n",
      "Train Epoch: 2769 [98560/118836 (83%)] Loss: 12266.811523\n",
      "    epoch          : 2769\n",
      "    loss           : 12222.601939877997\n",
      "    val_loss       : 12222.013223262471\n",
      "    val_log_likelihood: -12142.272159810536\n",
      "    val_log_marginal: -12150.650159850557\n",
      "Train Epoch: 2770 [256/118836 (0%)] Loss: 12178.733398\n",
      "Train Epoch: 2770 [33024/118836 (28%)] Loss: 12264.723633\n",
      "Train Epoch: 2770 [65792/118836 (55%)] Loss: 12139.978516\n",
      "Train Epoch: 2770 [98560/118836 (83%)] Loss: 12231.227539\n",
      "    epoch          : 2770\n",
      "    loss           : 12222.45164343595\n",
      "    val_loss       : 12220.016127530742\n",
      "    val_log_likelihood: -12143.175312920026\n",
      "    val_log_marginal: -12151.499891066258\n",
      "Train Epoch: 2771 [256/118836 (0%)] Loss: 12295.941406\n",
      "Train Epoch: 2771 [33024/118836 (28%)] Loss: 12222.236328\n",
      "Train Epoch: 2771 [65792/118836 (55%)] Loss: 12296.612305\n",
      "Train Epoch: 2771 [98560/118836 (83%)] Loss: 12393.947266\n",
      "    epoch          : 2771\n",
      "    loss           : 12223.55851539883\n",
      "    val_loss       : 12218.005796188594\n",
      "    val_log_likelihood: -12141.914986236043\n",
      "    val_log_marginal: -12150.371306438938\n",
      "Train Epoch: 2772 [256/118836 (0%)] Loss: 12285.227539\n",
      "Train Epoch: 2772 [33024/118836 (28%)] Loss: 12284.912109\n",
      "Train Epoch: 2772 [65792/118836 (55%)] Loss: 12304.389648\n",
      "Train Epoch: 2772 [98560/118836 (83%)] Loss: 12188.621094\n",
      "    epoch          : 2772\n",
      "    loss           : 12217.382069375517\n",
      "    val_loss       : 12220.206652346485\n",
      "    val_log_likelihood: -12144.743537078682\n",
      "    val_log_marginal: -12153.15141607673\n",
      "Train Epoch: 2773 [256/118836 (0%)] Loss: 12228.009766\n",
      "Train Epoch: 2773 [33024/118836 (28%)] Loss: 12276.111328\n",
      "Train Epoch: 2773 [65792/118836 (55%)] Loss: 12182.024414\n",
      "Train Epoch: 2773 [98560/118836 (83%)] Loss: 12186.008789\n",
      "    epoch          : 2773\n",
      "    loss           : 12218.493202672662\n",
      "    val_loss       : 12221.134975366847\n",
      "    val_log_likelihood: -12142.55666760365\n",
      "    val_log_marginal: -12150.88282294888\n",
      "Train Epoch: 2774 [256/118836 (0%)] Loss: 12259.426758\n",
      "Train Epoch: 2774 [33024/118836 (28%)] Loss: 12225.747070\n",
      "Train Epoch: 2774 [65792/118836 (55%)] Loss: 12319.207031\n",
      "Train Epoch: 2774 [98560/118836 (83%)] Loss: 12208.509766\n",
      "    epoch          : 2774\n",
      "    loss           : 12221.956782464847\n",
      "    val_loss       : 12218.399930266112\n",
      "    val_log_likelihood: -12142.037460905189\n",
      "    val_log_marginal: -12150.425209820607\n",
      "Train Epoch: 2775 [256/118836 (0%)] Loss: 12242.154297\n",
      "Train Epoch: 2775 [33024/118836 (28%)] Loss: 12233.291016\n",
      "Train Epoch: 2775 [65792/118836 (55%)] Loss: 12224.283203\n",
      "Train Epoch: 2775 [98560/118836 (83%)] Loss: 12147.705078\n",
      "    epoch          : 2775\n",
      "    loss           : 12227.342872143818\n",
      "    val_loss       : 12220.099090911352\n",
      "    val_log_likelihood: -12144.621787279\n",
      "    val_log_marginal: -12153.032834833053\n",
      "Train Epoch: 2776 [256/118836 (0%)] Loss: 12283.168945\n",
      "Train Epoch: 2776 [33024/118836 (28%)] Loss: 12222.221680\n",
      "Train Epoch: 2776 [65792/118836 (55%)] Loss: 12220.749023\n",
      "Train Epoch: 2776 [98560/118836 (83%)] Loss: 12226.320312\n",
      "    epoch          : 2776\n",
      "    loss           : 12217.382545136734\n",
      "    val_loss       : 12219.973893334774\n",
      "    val_log_likelihood: -12141.733126066223\n",
      "    val_log_marginal: -12150.086751858327\n",
      "Train Epoch: 2777 [256/118836 (0%)] Loss: 12166.083008\n",
      "Train Epoch: 2777 [33024/118836 (28%)] Loss: 12233.388672\n",
      "Train Epoch: 2777 [65792/118836 (55%)] Loss: 12226.909180\n",
      "Train Epoch: 2777 [98560/118836 (83%)] Loss: 12193.267578\n",
      "    epoch          : 2777\n",
      "    loss           : 12219.46649510184\n",
      "    val_loss       : 12218.562695397735\n",
      "    val_log_likelihood: -12140.535966578786\n",
      "    val_log_marginal: -12148.766970149216\n",
      "Train Epoch: 2778 [256/118836 (0%)] Loss: 12190.607422\n",
      "Train Epoch: 2778 [33024/118836 (28%)] Loss: 12302.191406\n",
      "Train Epoch: 2778 [65792/118836 (55%)] Loss: 12261.526367\n",
      "Train Epoch: 2778 [98560/118836 (83%)] Loss: 12226.984375\n",
      "    epoch          : 2778\n",
      "    loss           : 12219.873944763234\n",
      "    val_loss       : 12218.156878221665\n",
      "    val_log_likelihood: -12142.099295485681\n",
      "    val_log_marginal: -12150.485340610683\n",
      "Train Epoch: 2779 [256/118836 (0%)] Loss: 12192.053711\n",
      "Train Epoch: 2779 [33024/118836 (28%)] Loss: 12200.046875\n",
      "Train Epoch: 2779 [65792/118836 (55%)] Loss: 12284.614258\n",
      "Train Epoch: 2779 [98560/118836 (83%)] Loss: 12295.595703\n",
      "    epoch          : 2779\n",
      "    loss           : 12220.473648805832\n",
      "    val_loss       : 12218.125282343723\n",
      "    val_log_likelihood: -12143.292746943496\n",
      "    val_log_marginal: -12151.432283852188\n",
      "Train Epoch: 2780 [256/118836 (0%)] Loss: 12299.159180\n",
      "Train Epoch: 2780 [33024/118836 (28%)] Loss: 12300.796875\n",
      "Train Epoch: 2780 [65792/118836 (55%)] Loss: 12216.956055\n",
      "Train Epoch: 2780 [98560/118836 (83%)] Loss: 12255.304688\n",
      "    epoch          : 2780\n",
      "    loss           : 12221.429097685328\n",
      "    val_loss       : 12222.00811945615\n",
      "    val_log_likelihood: -12143.678475560897\n",
      "    val_log_marginal: -12152.127860103657\n",
      "Train Epoch: 2781 [256/118836 (0%)] Loss: 12225.844727\n",
      "Train Epoch: 2781 [33024/118836 (28%)] Loss: 12271.703125\n",
      "Train Epoch: 2781 [65792/118836 (55%)] Loss: 12237.917969\n",
      "Train Epoch: 2781 [98560/118836 (83%)] Loss: 12260.345703\n",
      "    epoch          : 2781\n",
      "    loss           : 12220.36551191584\n",
      "    val_loss       : 12217.885144566897\n",
      "    val_log_likelihood: -12143.073468840466\n",
      "    val_log_marginal: -12151.347237394342\n",
      "Train Epoch: 2782 [256/118836 (0%)] Loss: 12226.563477\n",
      "Train Epoch: 2782 [33024/118836 (28%)] Loss: 12246.135742\n",
      "Train Epoch: 2782 [65792/118836 (55%)] Loss: 12256.197266\n",
      "Train Epoch: 2782 [98560/118836 (83%)] Loss: 12323.834961\n",
      "    epoch          : 2782\n",
      "    loss           : 12222.336522468207\n",
      "    val_loss       : 12221.923046443871\n",
      "    val_log_likelihood: -12144.300193050816\n",
      "    val_log_marginal: -12152.632336036919\n",
      "Train Epoch: 2783 [256/118836 (0%)] Loss: 12215.594727\n",
      "Train Epoch: 2783 [33024/118836 (28%)] Loss: 12221.284180\n",
      "Train Epoch: 2783 [65792/118836 (55%)] Loss: 12183.376953\n",
      "Train Epoch: 2783 [98560/118836 (83%)] Loss: 12160.069336\n",
      "    epoch          : 2783\n",
      "    loss           : 12223.506585213763\n",
      "    val_loss       : 12217.892609016502\n",
      "    val_log_likelihood: -12143.278200120192\n",
      "    val_log_marginal: -12151.757631783235\n",
      "Train Epoch: 2784 [256/118836 (0%)] Loss: 12291.267578\n",
      "Train Epoch: 2784 [33024/118836 (28%)] Loss: 12297.187500\n",
      "Train Epoch: 2784 [65792/118836 (55%)] Loss: 12267.623047\n",
      "Train Epoch: 2784 [98560/118836 (83%)] Loss: 12261.984375\n",
      "    epoch          : 2784\n",
      "    loss           : 12222.629849856545\n",
      "    val_loss       : 12219.939074116945\n",
      "    val_log_likelihood: -12142.812017776829\n",
      "    val_log_marginal: -12151.284547216197\n",
      "Train Epoch: 2785 [256/118836 (0%)] Loss: 12170.488281\n",
      "Train Epoch: 2785 [33024/118836 (28%)] Loss: 12346.852539\n",
      "Train Epoch: 2785 [65792/118836 (55%)] Loss: 12240.262695\n",
      "Train Epoch: 2785 [98560/118836 (83%)] Loss: 12250.953125\n",
      "    epoch          : 2785\n",
      "    loss           : 12222.670512497414\n",
      "    val_loss       : 12219.623516046227\n",
      "    val_log_likelihood: -12139.609072419096\n",
      "    val_log_marginal: -12147.943429327179\n",
      "Train Epoch: 2786 [256/118836 (0%)] Loss: 12228.630859\n",
      "Train Epoch: 2786 [33024/118836 (28%)] Loss: 12274.878906\n",
      "Train Epoch: 2786 [65792/118836 (55%)] Loss: 12256.335938\n",
      "Train Epoch: 2786 [98560/118836 (83%)] Loss: 12207.761719\n",
      "    epoch          : 2786\n",
      "    loss           : 12224.257492471826\n",
      "    val_loss       : 12220.261601610764\n",
      "    val_log_likelihood: -12142.408869029157\n",
      "    val_log_marginal: -12150.789285590916\n",
      "Train Epoch: 2787 [256/118836 (0%)] Loss: 12178.110352\n",
      "Train Epoch: 2787 [33024/118836 (28%)] Loss: 12332.433594\n",
      "Train Epoch: 2787 [65792/118836 (55%)] Loss: 12175.804688\n",
      "Train Epoch: 2787 [98560/118836 (83%)] Loss: 12181.187500\n",
      "    epoch          : 2787\n",
      "    loss           : 12219.231538041511\n",
      "    val_loss       : 12226.445943956855\n",
      "    val_log_likelihood: -12141.125627617092\n",
      "    val_log_marginal: -12149.349060226541\n",
      "Train Epoch: 2788 [256/118836 (0%)] Loss: 12241.298828\n",
      "Train Epoch: 2788 [33024/118836 (28%)] Loss: 12332.101562\n",
      "Train Epoch: 2788 [65792/118836 (55%)] Loss: 12125.872070\n",
      "Train Epoch: 2788 [98560/118836 (83%)] Loss: 12123.369141\n",
      "    epoch          : 2788\n",
      "    loss           : 12220.872525880117\n",
      "    val_loss       : 12222.34650328877\n",
      "    val_log_likelihood: -12140.149683041254\n",
      "    val_log_marginal: -12148.398697392251\n",
      "Train Epoch: 2789 [256/118836 (0%)] Loss: 12239.338867\n",
      "Train Epoch: 2789 [33024/118836 (28%)] Loss: 12207.736328\n",
      "Train Epoch: 2789 [65792/118836 (55%)] Loss: 12281.875000\n",
      "Train Epoch: 2789 [98560/118836 (83%)] Loss: 12270.837891\n",
      "    epoch          : 2789\n",
      "    loss           : 12220.895800215829\n",
      "    val_loss       : 12219.775793044528\n",
      "    val_log_likelihood: -12142.764388020832\n",
      "    val_log_marginal: -12151.162833647117\n",
      "Train Epoch: 2790 [256/118836 (0%)] Loss: 12166.753906\n",
      "Train Epoch: 2790 [33024/118836 (28%)] Loss: 12192.339844\n",
      "Train Epoch: 2790 [65792/118836 (55%)] Loss: 12184.887695\n",
      "Train Epoch: 2790 [98560/118836 (83%)] Loss: 12177.626953\n",
      "    epoch          : 2790\n",
      "    loss           : 12217.309323465932\n",
      "    val_loss       : 12218.126454772868\n",
      "    val_log_likelihood: -12140.04187361068\n",
      "    val_log_marginal: -12148.31006266446\n",
      "Train Epoch: 2791 [256/118836 (0%)] Loss: 12230.082031\n",
      "Train Epoch: 2791 [33024/118836 (28%)] Loss: 12258.171875\n",
      "Train Epoch: 2791 [65792/118836 (55%)] Loss: 12267.169922\n",
      "Train Epoch: 2791 [98560/118836 (83%)] Loss: 12265.746094\n",
      "    epoch          : 2791\n",
      "    loss           : 12221.133391167805\n",
      "    val_loss       : 12216.2671109039\n",
      "    val_log_likelihood: -12141.098392750982\n",
      "    val_log_marginal: -12149.353688812917\n",
      "Train Epoch: 2792 [256/118836 (0%)] Loss: 12143.848633\n",
      "Train Epoch: 2792 [33024/118836 (28%)] Loss: 12210.891602\n",
      "Train Epoch: 2792 [65792/118836 (55%)] Loss: 12243.835938\n",
      "Train Epoch: 2792 [98560/118836 (83%)] Loss: 12226.670898\n",
      "    epoch          : 2792\n",
      "    loss           : 12221.282895536084\n",
      "    val_loss       : 12218.518442815817\n",
      "    val_log_likelihood: -12144.053833068392\n",
      "    val_log_marginal: -12152.428483709193\n",
      "Train Epoch: 2793 [256/118836 (0%)] Loss: 12269.875000\n",
      "Train Epoch: 2793 [33024/118836 (28%)] Loss: 12216.626953\n",
      "Train Epoch: 2793 [65792/118836 (55%)] Loss: 12154.312500\n",
      "Train Epoch: 2793 [98560/118836 (83%)] Loss: 12234.202148\n",
      "    epoch          : 2793\n",
      "    loss           : 12221.338530035153\n",
      "    val_loss       : 12219.789522003925\n",
      "    val_log_likelihood: -12140.264199170286\n",
      "    val_log_marginal: -12148.462862609354\n",
      "Train Epoch: 2794 [256/118836 (0%)] Loss: 12159.482422\n",
      "Train Epoch: 2794 [33024/118836 (28%)] Loss: 12215.194336\n",
      "Train Epoch: 2794 [65792/118836 (55%)] Loss: 12235.817383\n",
      "Train Epoch: 2794 [98560/118836 (83%)] Loss: 12264.508789\n",
      "    epoch          : 2794\n",
      "    loss           : 12218.113894327698\n",
      "    val_loss       : 12220.808911905267\n",
      "    val_log_likelihood: -12142.46557653536\n",
      "    val_log_marginal: -12150.794837850903\n",
      "Train Epoch: 2795 [256/118836 (0%)] Loss: 12298.792969\n",
      "Train Epoch: 2795 [33024/118836 (28%)] Loss: 12190.460938\n",
      "Train Epoch: 2795 [65792/118836 (55%)] Loss: 12261.650391\n",
      "Train Epoch: 2795 [98560/118836 (83%)] Loss: 12275.253906\n",
      "    epoch          : 2795\n",
      "    loss           : 12220.645299576097\n",
      "    val_loss       : 12217.155798001459\n",
      "    val_log_likelihood: -12140.991349869468\n",
      "    val_log_marginal: -12149.495458756166\n",
      "Train Epoch: 2796 [256/118836 (0%)] Loss: 12280.060547\n",
      "Train Epoch: 2796 [33024/118836 (28%)] Loss: 12255.817383\n",
      "Train Epoch: 2796 [65792/118836 (55%)] Loss: 12194.569336\n",
      "Train Epoch: 2796 [98560/118836 (83%)] Loss: 12306.816406\n",
      "    epoch          : 2796\n",
      "    loss           : 12220.529196714744\n",
      "    val_loss       : 12218.600663285926\n",
      "    val_log_likelihood: -12143.18904634512\n",
      "    val_log_marginal: -12151.425947572965\n",
      "Train Epoch: 2797 [256/118836 (0%)] Loss: 12185.796875\n",
      "Train Epoch: 2797 [33024/118836 (28%)] Loss: 12273.569336\n",
      "Train Epoch: 2797 [65792/118836 (55%)] Loss: 12167.383789\n",
      "Train Epoch: 2797 [98560/118836 (83%)] Loss: 12185.998047\n",
      "    epoch          : 2797\n",
      "    loss           : 12219.35497974178\n",
      "    val_loss       : 12217.740786158614\n",
      "    val_log_likelihood: -12141.922078228392\n",
      "    val_log_marginal: -12150.138776686259\n",
      "Train Epoch: 2798 [256/118836 (0%)] Loss: 12262.977539\n",
      "Train Epoch: 2798 [33024/118836 (28%)] Loss: 12245.162109\n",
      "Train Epoch: 2798 [65792/118836 (55%)] Loss: 12280.878906\n",
      "Train Epoch: 2798 [98560/118836 (83%)] Loss: 12215.918945\n",
      "    epoch          : 2798\n",
      "    loss           : 12220.888673652038\n",
      "    val_loss       : 12224.150778461182\n",
      "    val_log_likelihood: -12142.137237160103\n",
      "    val_log_marginal: -12150.414225089917\n",
      "Train Epoch: 2799 [256/118836 (0%)] Loss: 12279.464844\n",
      "Train Epoch: 2799 [33024/118836 (28%)] Loss: 12185.599609\n",
      "Train Epoch: 2799 [65792/118836 (55%)] Loss: 12301.817383\n",
      "Train Epoch: 2799 [98560/118836 (83%)] Loss: 12191.869141\n",
      "    epoch          : 2799\n",
      "    loss           : 12219.139788823408\n",
      "    val_loss       : 12218.84612472197\n",
      "    val_log_likelihood: -12142.861668023677\n",
      "    val_log_marginal: -12151.236452299381\n",
      "Train Epoch: 2800 [256/118836 (0%)] Loss: 12200.653320\n",
      "Train Epoch: 2800 [33024/118836 (28%)] Loss: 12279.458008\n",
      "Train Epoch: 2800 [65792/118836 (55%)] Loss: 12278.875977\n",
      "Train Epoch: 2800 [98560/118836 (83%)] Loss: 12323.144531\n",
      "    epoch          : 2800\n",
      "    loss           : 12219.796137206627\n",
      "    val_loss       : 12218.614465552026\n",
      "    val_log_likelihood: -12141.889164114195\n",
      "    val_log_marginal: -12150.188057903773\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch2800.pth ...\n",
      "Train Epoch: 2801 [256/118836 (0%)] Loss: 12241.704102\n",
      "Train Epoch: 2801 [33024/118836 (28%)] Loss: 12310.583984\n",
      "Train Epoch: 2801 [65792/118836 (55%)] Loss: 12240.841797\n",
      "Train Epoch: 2801 [98560/118836 (83%)] Loss: 12201.748047\n",
      "    epoch          : 2801\n",
      "    loss           : 12221.287777379292\n",
      "    val_loss       : 12220.613484996189\n",
      "    val_log_likelihood: -12143.073537498709\n",
      "    val_log_marginal: -12151.441877020437\n",
      "Train Epoch: 2802 [256/118836 (0%)] Loss: 12315.839844\n",
      "Train Epoch: 2802 [33024/118836 (28%)] Loss: 12250.912109\n",
      "Train Epoch: 2802 [65792/118836 (55%)] Loss: 12232.524414\n",
      "Train Epoch: 2802 [98560/118836 (83%)] Loss: 12292.912109\n",
      "    epoch          : 2802\n",
      "    loss           : 12220.129831439981\n",
      "    val_loss       : 12220.197715718872\n",
      "    val_log_likelihood: -12141.274986106802\n",
      "    val_log_marginal: -12149.740855855733\n",
      "Train Epoch: 2803 [256/118836 (0%)] Loss: 12190.098633\n",
      "Train Epoch: 2803 [33024/118836 (28%)] Loss: 12172.361328\n",
      "Train Epoch: 2803 [65792/118836 (55%)] Loss: 12262.550781\n",
      "Train Epoch: 2803 [98560/118836 (83%)] Loss: 12217.858398\n",
      "    epoch          : 2803\n",
      "    loss           : 12219.127120812656\n",
      "    val_loss       : 12222.56213409148\n",
      "    val_log_likelihood: -12140.909119106698\n",
      "    val_log_marginal: -12149.206091221062\n",
      "Train Epoch: 2804 [256/118836 (0%)] Loss: 12346.218750\n",
      "Train Epoch: 2804 [33024/118836 (28%)] Loss: 12189.544922\n",
      "Train Epoch: 2804 [65792/118836 (55%)] Loss: 12217.925781\n",
      "Train Epoch: 2804 [98560/118836 (83%)] Loss: 12203.801758\n",
      "    epoch          : 2804\n",
      "    loss           : 12217.379023857527\n",
      "    val_loss       : 12217.866817607426\n",
      "    val_log_likelihood: -12142.898237179488\n",
      "    val_log_marginal: -12151.394957289507\n",
      "Train Epoch: 2805 [256/118836 (0%)] Loss: 12159.751953\n",
      "Train Epoch: 2805 [33024/118836 (28%)] Loss: 12191.447266\n",
      "Train Epoch: 2805 [65792/118836 (55%)] Loss: 12275.787109\n",
      "Train Epoch: 2805 [98560/118836 (83%)] Loss: 12278.640625\n",
      "    epoch          : 2805\n",
      "    loss           : 12218.569450314051\n",
      "    val_loss       : 12224.096569755162\n",
      "    val_log_likelihood: -12141.248422637509\n",
      "    val_log_marginal: -12149.61190464586\n",
      "Train Epoch: 2806 [256/118836 (0%)] Loss: 12209.626953\n",
      "Train Epoch: 2806 [33024/118836 (28%)] Loss: 12294.105469\n",
      "Train Epoch: 2806 [65792/118836 (55%)] Loss: 12172.349609\n",
      "Train Epoch: 2806 [98560/118836 (83%)] Loss: 12206.648438\n",
      "    epoch          : 2806\n",
      "    loss           : 12218.064543753877\n",
      "    val_loss       : 12219.7038377586\n",
      "    val_log_likelihood: -12140.746878554073\n",
      "    val_log_marginal: -12149.036869721196\n",
      "Train Epoch: 2807 [256/118836 (0%)] Loss: 12279.593750\n",
      "Train Epoch: 2807 [33024/118836 (28%)] Loss: 12218.216797\n",
      "Train Epoch: 2807 [65792/118836 (55%)] Loss: 12209.724609\n",
      "Train Epoch: 2807 [98560/118836 (83%)] Loss: 12215.858398\n",
      "    epoch          : 2807\n",
      "    loss           : 12219.18811953965\n",
      "    val_loss       : 12217.499384289325\n",
      "    val_log_likelihood: -12142.42732210246\n",
      "    val_log_marginal: -12150.762825407317\n",
      "Train Epoch: 2808 [256/118836 (0%)] Loss: 12179.847656\n",
      "Train Epoch: 2808 [33024/118836 (28%)] Loss: 12235.949219\n",
      "Train Epoch: 2808 [65792/118836 (55%)] Loss: 12396.855469\n",
      "Train Epoch: 2808 [98560/118836 (83%)] Loss: 12337.044922\n",
      "    epoch          : 2808\n",
      "    loss           : 12221.416033718466\n",
      "    val_loss       : 12219.549272698056\n",
      "    val_log_likelihood: -12142.008229780551\n",
      "    val_log_marginal: -12150.288960596054\n",
      "Train Epoch: 2809 [256/118836 (0%)] Loss: 12239.278320\n",
      "Train Epoch: 2809 [33024/118836 (28%)] Loss: 12209.838867\n",
      "Train Epoch: 2809 [65792/118836 (55%)] Loss: 12202.751953\n",
      "Train Epoch: 2809 [98560/118836 (83%)] Loss: 12175.839844\n",
      "    epoch          : 2809\n",
      "    loss           : 12220.160721832352\n",
      "    val_loss       : 12216.368380105165\n",
      "    val_log_likelihood: -12140.783198116987\n",
      "    val_log_marginal: -12149.189113558941\n",
      "Train Epoch: 2810 [256/118836 (0%)] Loss: 12257.794922\n",
      "Train Epoch: 2810 [33024/118836 (28%)] Loss: 12240.186523\n",
      "Train Epoch: 2810 [65792/118836 (55%)] Loss: 12223.005859\n",
      "Train Epoch: 2810 [98560/118836 (83%)] Loss: 12313.187500\n",
      "    epoch          : 2810\n",
      "    loss           : 12220.068373752843\n",
      "    val_loss       : 12220.515718166134\n",
      "    val_log_likelihood: -12144.509699228442\n",
      "    val_log_marginal: -12153.032261905853\n",
      "Train Epoch: 2811 [256/118836 (0%)] Loss: 12208.404297\n",
      "Train Epoch: 2811 [33024/118836 (28%)] Loss: 12231.533203\n",
      "Train Epoch: 2811 [65792/118836 (55%)] Loss: 12324.143555\n",
      "Train Epoch: 2811 [98560/118836 (83%)] Loss: 12281.379883\n",
      "    epoch          : 2811\n",
      "    loss           : 12217.273875620349\n",
      "    val_loss       : 12220.908117941248\n",
      "    val_log_likelihood: -12140.183541246639\n",
      "    val_log_marginal: -12148.503984185329\n",
      "Train Epoch: 2812 [256/118836 (0%)] Loss: 12252.126953\n",
      "Train Epoch: 2812 [33024/118836 (28%)] Loss: 12301.016602\n",
      "Train Epoch: 2812 [65792/118836 (55%)] Loss: 12213.135742\n",
      "Train Epoch: 2812 [98560/118836 (83%)] Loss: 12317.329102\n",
      "    epoch          : 2812\n",
      "    loss           : 12219.140823705025\n",
      "    val_loss       : 12221.697873320245\n",
      "    val_log_likelihood: -12140.34321414263\n",
      "    val_log_marginal: -12148.71960911311\n",
      "Train Epoch: 2813 [256/118836 (0%)] Loss: 12142.621094\n",
      "Train Epoch: 2813 [33024/118836 (28%)] Loss: 12264.675781\n",
      "Train Epoch: 2813 [65792/118836 (55%)] Loss: 12242.814453\n",
      "Train Epoch: 2813 [98560/118836 (83%)] Loss: 12216.309570\n",
      "    epoch          : 2813\n",
      "    loss           : 12222.582913952647\n",
      "    val_loss       : 12220.956329787126\n",
      "    val_log_likelihood: -12142.93748126034\n",
      "    val_log_marginal: -12151.498637021137\n",
      "Train Epoch: 2814 [256/118836 (0%)] Loss: 12398.031250\n",
      "Train Epoch: 2814 [33024/118836 (28%)] Loss: 12236.767578\n",
      "Train Epoch: 2814 [65792/118836 (55%)] Loss: 12166.919922\n",
      "Train Epoch: 2814 [98560/118836 (83%)] Loss: 12249.263672\n",
      "    epoch          : 2814\n",
      "    loss           : 12224.204550345068\n",
      "    val_loss       : 12217.95442150669\n",
      "    val_log_likelihood: -12138.976334393092\n",
      "    val_log_marginal: -12147.402934875327\n",
      "Train Epoch: 2815 [256/118836 (0%)] Loss: 12271.625977\n",
      "Train Epoch: 2815 [33024/118836 (28%)] Loss: 12152.375000\n",
      "Train Epoch: 2815 [65792/118836 (55%)] Loss: 12391.267578\n",
      "Train Epoch: 2815 [98560/118836 (83%)] Loss: 12315.186523\n",
      "    epoch          : 2815\n",
      "    loss           : 12224.720080677473\n",
      "    val_loss       : 12220.349108631239\n",
      "    val_log_likelihood: -12149.742038390457\n",
      "    val_log_marginal: -12158.367291451075\n",
      "Train Epoch: 2816 [256/118836 (0%)] Loss: 12244.142578\n",
      "Train Epoch: 2816 [33024/118836 (28%)] Loss: 12261.775391\n",
      "Train Epoch: 2816 [65792/118836 (55%)] Loss: 12252.914062\n",
      "Train Epoch: 2816 [98560/118836 (83%)] Loss: 12150.752930\n",
      "    epoch          : 2816\n",
      "    loss           : 12216.837856376655\n",
      "    val_loss       : 12221.022100385238\n",
      "    val_log_likelihood: -12141.031840622414\n",
      "    val_log_marginal: -12149.26139182156\n",
      "Train Epoch: 2817 [256/118836 (0%)] Loss: 12405.696289\n",
      "Train Epoch: 2817 [33024/118836 (28%)] Loss: 12209.343750\n",
      "Train Epoch: 2817 [65792/118836 (55%)] Loss: 12239.019531\n",
      "Train Epoch: 2817 [98560/118836 (83%)] Loss: 12168.183594\n",
      "    epoch          : 2817\n",
      "    loss           : 12221.548609549473\n",
      "    val_loss       : 12215.437879772211\n",
      "    val_log_likelihood: -12141.318887801128\n",
      "    val_log_marginal: -12149.724775554783\n",
      "Train Epoch: 2818 [256/118836 (0%)] Loss: 12180.912109\n",
      "Train Epoch: 2818 [33024/118836 (28%)] Loss: 12170.082031\n",
      "Train Epoch: 2818 [65792/118836 (55%)] Loss: 12216.669922\n",
      "Train Epoch: 2818 [98560/118836 (83%)] Loss: 12257.596680\n",
      "    epoch          : 2818\n",
      "    loss           : 12215.364237457352\n",
      "    val_loss       : 12219.701548101426\n",
      "    val_log_likelihood: -12140.577632760805\n",
      "    val_log_marginal: -12148.772400259639\n",
      "Train Epoch: 2819 [256/118836 (0%)] Loss: 12219.130859\n",
      "Train Epoch: 2819 [33024/118836 (28%)] Loss: 12183.328125\n",
      "Train Epoch: 2819 [65792/118836 (55%)] Loss: 12185.735352\n",
      "Train Epoch: 2819 [98560/118836 (83%)] Loss: 12288.483398\n",
      "    epoch          : 2819\n",
      "    loss           : 12220.848962695152\n",
      "    val_loss       : 12219.607236983755\n",
      "    val_log_likelihood: -12143.664868466967\n",
      "    val_log_marginal: -12152.190518472189\n",
      "Train Epoch: 2820 [256/118836 (0%)] Loss: 12258.224609\n",
      "Train Epoch: 2820 [33024/118836 (28%)] Loss: 12197.751953\n",
      "Train Epoch: 2820 [65792/118836 (55%)] Loss: 12278.510742\n",
      "Train Epoch: 2820 [98560/118836 (83%)] Loss: 12295.805664\n",
      "    epoch          : 2820\n",
      "    loss           : 12221.01530028691\n",
      "    val_loss       : 12221.362567313665\n",
      "    val_log_likelihood: -12142.515012406948\n",
      "    val_log_marginal: -12150.993804861213\n",
      "Train Epoch: 2821 [256/118836 (0%)] Loss: 12203.451172\n",
      "Train Epoch: 2821 [33024/118836 (28%)] Loss: 12270.199219\n",
      "Train Epoch: 2821 [65792/118836 (55%)] Loss: 12210.291992\n",
      "Train Epoch: 2821 [98560/118836 (83%)] Loss: 12247.619141\n",
      "    epoch          : 2821\n",
      "    loss           : 12221.814863297406\n",
      "    val_loss       : 12220.630036070917\n",
      "    val_log_likelihood: -12139.750478345999\n",
      "    val_log_marginal: -12147.980127248842\n",
      "Train Epoch: 2822 [256/118836 (0%)] Loss: 12186.601562\n",
      "Train Epoch: 2822 [33024/118836 (28%)] Loss: 12200.274414\n",
      "Train Epoch: 2822 [65792/118836 (55%)] Loss: 12219.449219\n",
      "Train Epoch: 2822 [98560/118836 (83%)] Loss: 12181.800781\n",
      "    epoch          : 2822\n",
      "    loss           : 12214.618017376188\n",
      "    val_loss       : 12227.30115695066\n",
      "    val_log_likelihood: -12140.988921790995\n",
      "    val_log_marginal: -12149.151875426118\n",
      "Train Epoch: 2823 [256/118836 (0%)] Loss: 12196.435547\n",
      "Train Epoch: 2823 [33024/118836 (28%)] Loss: 12213.042969\n",
      "Train Epoch: 2823 [65792/118836 (55%)] Loss: 12187.398438\n",
      "Train Epoch: 2823 [98560/118836 (83%)] Loss: 12206.855469\n",
      "    epoch          : 2823\n",
      "    loss           : 12220.40787582713\n",
      "    val_loss       : 12219.25034703507\n",
      "    val_log_likelihood: -12144.312815020161\n",
      "    val_log_marginal: -12152.726192314018\n",
      "Train Epoch: 2824 [256/118836 (0%)] Loss: 12283.750000\n",
      "Train Epoch: 2824 [33024/118836 (28%)] Loss: 12265.934570\n",
      "Train Epoch: 2824 [65792/118836 (55%)] Loss: 12203.583008\n",
      "Train Epoch: 2824 [98560/118836 (83%)] Loss: 12316.017578\n",
      "    epoch          : 2824\n",
      "    loss           : 12224.325134731698\n",
      "    val_loss       : 12219.056109543102\n",
      "    val_log_likelihood: -12139.53815055702\n",
      "    val_log_marginal: -12147.856581039154\n",
      "Train Epoch: 2825 [256/118836 (0%)] Loss: 12277.050781\n",
      "Train Epoch: 2825 [33024/118836 (28%)] Loss: 12248.169922\n",
      "Train Epoch: 2825 [65792/118836 (55%)] Loss: 12159.624023\n",
      "Train Epoch: 2825 [98560/118836 (83%)] Loss: 12211.195312\n",
      "    epoch          : 2825\n",
      "    loss           : 12219.089205793785\n",
      "    val_loss       : 12219.163088668181\n",
      "    val_log_likelihood: -12140.406216882495\n",
      "    val_log_marginal: -12148.707294160078\n",
      "Train Epoch: 2826 [256/118836 (0%)] Loss: 12119.708008\n",
      "Train Epoch: 2826 [33024/118836 (28%)] Loss: 12121.898438\n",
      "Train Epoch: 2826 [65792/118836 (55%)] Loss: 12173.891602\n",
      "Train Epoch: 2826 [98560/118836 (83%)] Loss: 12229.028320\n",
      "    epoch          : 2826\n",
      "    loss           : 12218.537458966604\n",
      "    val_loss       : 12223.684645178244\n",
      "    val_log_likelihood: -12139.742005757598\n",
      "    val_log_marginal: -12148.046515466367\n",
      "Train Epoch: 2827 [256/118836 (0%)] Loss: 12228.865234\n",
      "Train Epoch: 2827 [33024/118836 (28%)] Loss: 12306.257812\n",
      "Train Epoch: 2827 [65792/118836 (55%)] Loss: 12254.235352\n",
      "Train Epoch: 2827 [98560/118836 (83%)] Loss: 12196.472656\n",
      "    epoch          : 2827\n",
      "    loss           : 12221.623561892577\n",
      "    val_loss       : 12215.279230493814\n",
      "    val_log_likelihood: -12142.57261570125\n",
      "    val_log_marginal: -12151.021967417595\n",
      "Train Epoch: 2828 [256/118836 (0%)] Loss: 12318.445312\n",
      "Train Epoch: 2828 [33024/118836 (28%)] Loss: 12372.935547\n",
      "Train Epoch: 2828 [65792/118836 (55%)] Loss: 12276.134766\n",
      "Train Epoch: 2828 [98560/118836 (83%)] Loss: 12311.007812\n",
      "    epoch          : 2828\n",
      "    loss           : 12217.50802816765\n",
      "    val_loss       : 12216.44592985662\n",
      "    val_log_likelihood: -12141.155411723274\n",
      "    val_log_marginal: -12149.688670508009\n",
      "Train Epoch: 2829 [256/118836 (0%)] Loss: 12189.901367\n",
      "Train Epoch: 2829 [33024/118836 (28%)] Loss: 12385.276367\n",
      "Train Epoch: 2829 [65792/118836 (55%)] Loss: 12252.735352\n",
      "Train Epoch: 2829 [98560/118836 (83%)] Loss: 12255.710938\n",
      "    epoch          : 2829\n",
      "    loss           : 12230.427088341345\n",
      "    val_loss       : 12219.013343984823\n",
      "    val_log_likelihood: -12147.31624857837\n",
      "    val_log_marginal: -12155.849400436999\n",
      "Train Epoch: 2830 [256/118836 (0%)] Loss: 12295.939453\n",
      "Train Epoch: 2830 [33024/118836 (28%)] Loss: 12173.115234\n",
      "Train Epoch: 2830 [65792/118836 (55%)] Loss: 12182.748047\n",
      "Train Epoch: 2830 [98560/118836 (83%)] Loss: 12211.912109\n",
      "    epoch          : 2830\n",
      "    loss           : 12220.569639326148\n",
      "    val_loss       : 12225.766638671623\n",
      "    val_log_likelihood: -12145.583554008996\n",
      "    val_log_marginal: -12153.995165479273\n",
      "Train Epoch: 2831 [256/118836 (0%)] Loss: 12305.530273\n",
      "Train Epoch: 2831 [33024/118836 (28%)] Loss: 12162.140625\n",
      "Train Epoch: 2831 [65792/118836 (55%)] Loss: 12290.687500\n",
      "Train Epoch: 2831 [98560/118836 (83%)] Loss: 12207.883789\n",
      "    epoch          : 2831\n",
      "    loss           : 12216.379169574544\n",
      "    val_loss       : 12218.20670868203\n",
      "    val_log_likelihood: -12144.700553466191\n",
      "    val_log_marginal: -12153.202663597485\n",
      "Train Epoch: 2832 [256/118836 (0%)] Loss: 12161.885742\n",
      "Train Epoch: 2832 [33024/118836 (28%)] Loss: 12209.323242\n",
      "Train Epoch: 2832 [65792/118836 (55%)] Loss: 12216.229492\n",
      "Train Epoch: 2832 [98560/118836 (83%)] Loss: 12263.736328\n",
      "    epoch          : 2832\n",
      "    loss           : 12217.123928769903\n",
      "    val_loss       : 12220.104668740987\n",
      "    val_log_likelihood: -12145.210517957765\n",
      "    val_log_marginal: -12153.415232170257\n",
      "Train Epoch: 2833 [256/118836 (0%)] Loss: 12286.460938\n",
      "Train Epoch: 2833 [33024/118836 (28%)] Loss: 12289.640625\n",
      "Train Epoch: 2833 [65792/118836 (55%)] Loss: 12251.826172\n",
      "Train Epoch: 2833 [98560/118836 (83%)] Loss: 12307.273438\n",
      "    epoch          : 2833\n",
      "    loss           : 12220.253548742505\n",
      "    val_loss       : 12219.29133462787\n",
      "    val_log_likelihood: -12141.921986307123\n",
      "    val_log_marginal: -12150.407824462602\n",
      "Train Epoch: 2834 [256/118836 (0%)] Loss: 12284.585938\n",
      "Train Epoch: 2834 [33024/118836 (28%)] Loss: 12211.235352\n",
      "Train Epoch: 2834 [65792/118836 (55%)] Loss: 12268.804688\n",
      "Train Epoch: 2834 [98560/118836 (83%)] Loss: 12269.295898\n",
      "    epoch          : 2834\n",
      "    loss           : 12219.901649413254\n",
      "    val_loss       : 12218.608838494627\n",
      "    val_log_likelihood: -12142.450101452647\n",
      "    val_log_marginal: -12150.722611947163\n",
      "Train Epoch: 2835 [256/118836 (0%)] Loss: 12144.429688\n",
      "Train Epoch: 2835 [33024/118836 (28%)] Loss: 12265.736328\n",
      "Train Epoch: 2835 [65792/118836 (55%)] Loss: 12193.164062\n",
      "Train Epoch: 2835 [98560/118836 (83%)] Loss: 12194.558594\n",
      "    epoch          : 2835\n",
      "    loss           : 12220.661133458696\n",
      "    val_loss       : 12220.553740753097\n",
      "    val_log_likelihood: -12142.425943768092\n",
      "    val_log_marginal: -12150.731336062085\n",
      "Train Epoch: 2836 [256/118836 (0%)] Loss: 12240.875977\n",
      "Train Epoch: 2836 [33024/118836 (28%)] Loss: 12323.714844\n",
      "Train Epoch: 2836 [65792/118836 (55%)] Loss: 12150.969727\n",
      "Train Epoch: 2836 [98560/118836 (83%)] Loss: 12211.268555\n",
      "    epoch          : 2836\n",
      "    loss           : 12218.414712087728\n",
      "    val_loss       : 12219.680213984286\n",
      "    val_log_likelihood: -12143.37206094267\n",
      "    val_log_marginal: -12151.530271650465\n",
      "Train Epoch: 2837 [256/118836 (0%)] Loss: 12285.190430\n",
      "Train Epoch: 2837 [33024/118836 (28%)] Loss: 12235.751953\n",
      "Train Epoch: 2837 [65792/118836 (55%)] Loss: 12297.315430\n",
      "Train Epoch: 2837 [98560/118836 (83%)] Loss: 12339.071289\n",
      "    epoch          : 2837\n",
      "    loss           : 12216.53996765793\n",
      "    val_loss       : 12226.224645358372\n",
      "    val_log_likelihood: -12146.266980717535\n",
      "    val_log_marginal: -12154.754706477741\n",
      "Train Epoch: 2838 [256/118836 (0%)] Loss: 12260.462891\n",
      "Train Epoch: 2838 [33024/118836 (28%)] Loss: 12225.559570\n",
      "Train Epoch: 2838 [65792/118836 (55%)] Loss: 12228.418945\n",
      "Train Epoch: 2838 [98560/118836 (83%)] Loss: 12179.667969\n",
      "    epoch          : 2838\n",
      "    loss           : 12220.501212908395\n",
      "    val_loss       : 12218.671118586757\n",
      "    val_log_likelihood: -12146.403773295337\n",
      "    val_log_marginal: -12154.93656854428\n",
      "Train Epoch: 2839 [256/118836 (0%)] Loss: 12262.835938\n",
      "Train Epoch: 2839 [33024/118836 (28%)] Loss: 12173.696289\n",
      "Train Epoch: 2839 [65792/118836 (55%)] Loss: 12239.804688\n",
      "Train Epoch: 2839 [98560/118836 (83%)] Loss: 12254.449219\n",
      "    epoch          : 2839\n",
      "    loss           : 12224.618059540426\n",
      "    val_loss       : 12224.13732679072\n",
      "    val_log_likelihood: -12146.232428013855\n",
      "    val_log_marginal: -12154.686023094162\n",
      "Train Epoch: 2840 [256/118836 (0%)] Loss: 12266.207031\n",
      "Train Epoch: 2840 [33024/118836 (28%)] Loss: 12175.805664\n",
      "Train Epoch: 2840 [65792/118836 (55%)] Loss: 12346.929688\n",
      "Train Epoch: 2840 [98560/118836 (83%)] Loss: 12348.671875\n",
      "    epoch          : 2840\n",
      "    loss           : 12226.117847103753\n",
      "    val_loss       : 12217.11600914148\n",
      "    val_log_likelihood: -12141.523372072737\n",
      "    val_log_marginal: -12149.87345839123\n",
      "Train Epoch: 2841 [256/118836 (0%)] Loss: 12279.485352\n",
      "Train Epoch: 2841 [33024/118836 (28%)] Loss: 12148.675781\n",
      "Train Epoch: 2841 [65792/118836 (55%)] Loss: 12273.380859\n",
      "Train Epoch: 2841 [98560/118836 (83%)] Loss: 12214.080078\n",
      "    epoch          : 2841\n",
      "    loss           : 12220.954961002119\n",
      "    val_loss       : 12219.20052202019\n",
      "    val_log_likelihood: -12143.618687642163\n",
      "    val_log_marginal: -12152.088197786534\n",
      "Train Epoch: 2842 [256/118836 (0%)] Loss: 12193.079102\n",
      "Train Epoch: 2842 [33024/118836 (28%)] Loss: 12202.398438\n",
      "Train Epoch: 2842 [65792/118836 (55%)] Loss: 12228.705078\n",
      "Train Epoch: 2842 [98560/118836 (83%)] Loss: 12269.504883\n",
      "    epoch          : 2842\n",
      "    loss           : 12215.206643694426\n",
      "    val_loss       : 12227.37038486605\n",
      "    val_log_likelihood: -12143.173621019436\n",
      "    val_log_marginal: -12151.59860008723\n",
      "Train Epoch: 2843 [256/118836 (0%)] Loss: 12310.340820\n",
      "Train Epoch: 2843 [33024/118836 (28%)] Loss: 12157.814453\n",
      "Train Epoch: 2843 [65792/118836 (55%)] Loss: 12197.257812\n",
      "Train Epoch: 2843 [98560/118836 (83%)] Loss: 12263.423828\n",
      "    epoch          : 2843\n",
      "    loss           : 12219.275651203216\n",
      "    val_loss       : 12219.678479436583\n",
      "    val_log_likelihood: -12140.213492071183\n",
      "    val_log_marginal: -12148.64977087394\n",
      "Train Epoch: 2844 [256/118836 (0%)] Loss: 12160.961914\n",
      "Train Epoch: 2844 [33024/118836 (28%)] Loss: 12250.451172\n",
      "Train Epoch: 2844 [65792/118836 (55%)] Loss: 12345.271484\n",
      "Train Epoch: 2844 [98560/118836 (83%)] Loss: 12225.490234\n",
      "    epoch          : 2844\n",
      "    loss           : 12223.432044173904\n",
      "    val_loss       : 12215.617883934718\n",
      "    val_log_likelihood: -12141.86112085789\n",
      "    val_log_marginal: -12150.087746813211\n",
      "Train Epoch: 2845 [256/118836 (0%)] Loss: 12262.783203\n",
      "Train Epoch: 2845 [33024/118836 (28%)] Loss: 12357.358398\n",
      "Train Epoch: 2845 [65792/118836 (55%)] Loss: 12175.242188\n",
      "Train Epoch: 2845 [98560/118836 (83%)] Loss: 12292.301758\n",
      "    epoch          : 2845\n",
      "    loss           : 12216.394149671733\n",
      "    val_loss       : 12218.8109367778\n",
      "    val_log_likelihood: -12140.84009980485\n",
      "    val_log_marginal: -12149.293981326295\n",
      "Train Epoch: 2846 [256/118836 (0%)] Loss: 12322.537109\n",
      "Train Epoch: 2846 [33024/118836 (28%)] Loss: 12227.643555\n",
      "Train Epoch: 2846 [65792/118836 (55%)] Loss: 12214.070312\n",
      "Train Epoch: 2846 [98560/118836 (83%)] Loss: 12267.906250\n",
      "    epoch          : 2846\n",
      "    loss           : 12219.002488174629\n",
      "    val_loss       : 12219.64749117627\n",
      "    val_log_likelihood: -12139.218100896918\n",
      "    val_log_marginal: -12147.470601763538\n",
      "Train Epoch: 2847 [256/118836 (0%)] Loss: 12185.159180\n",
      "Train Epoch: 2847 [33024/118836 (28%)] Loss: 12312.142578\n",
      "Train Epoch: 2847 [65792/118836 (55%)] Loss: 12216.346680\n",
      "Train Epoch: 2847 [98560/118836 (83%)] Loss: 12248.763672\n",
      "    epoch          : 2847\n",
      "    loss           : 12218.460051727925\n",
      "    val_loss       : 12218.182203682634\n",
      "    val_log_likelihood: -12139.996968698306\n",
      "    val_log_marginal: -12148.37792520185\n",
      "Train Epoch: 2848 [256/118836 (0%)] Loss: 12325.337891\n",
      "Train Epoch: 2848 [33024/118836 (28%)] Loss: 12335.298828\n",
      "Train Epoch: 2848 [65792/118836 (55%)] Loss: 12352.578125\n",
      "Train Epoch: 2848 [98560/118836 (83%)] Loss: 12328.006836\n",
      "    epoch          : 2848\n",
      "    loss           : 12225.021333003773\n",
      "    val_loss       : 12223.954234512517\n",
      "    val_log_likelihood: -12146.52192087986\n",
      "    val_log_marginal: -12155.140417606754\n",
      "Train Epoch: 2849 [256/118836 (0%)] Loss: 12196.039062\n",
      "Train Epoch: 2849 [33024/118836 (28%)] Loss: 12249.948242\n",
      "Train Epoch: 2849 [65792/118836 (55%)] Loss: 12213.744141\n",
      "Train Epoch: 2849 [98560/118836 (83%)] Loss: 12160.634766\n",
      "    epoch          : 2849\n",
      "    loss           : 12221.308134951407\n",
      "    val_loss       : 12220.61529945216\n",
      "    val_log_likelihood: -12142.018349358974\n",
      "    val_log_marginal: -12150.358294824699\n",
      "Train Epoch: 2850 [256/118836 (0%)] Loss: 12254.954102\n",
      "Train Epoch: 2850 [33024/118836 (28%)] Loss: 12299.549805\n",
      "Train Epoch: 2850 [65792/118836 (55%)] Loss: 12198.888672\n",
      "Train Epoch: 2850 [98560/118836 (83%)] Loss: 12206.591797\n",
      "    epoch          : 2850\n",
      "    loss           : 12215.065328073306\n",
      "    val_loss       : 12220.100162765348\n",
      "    val_log_likelihood: -12140.220181160825\n",
      "    val_log_marginal: -12148.61203353245\n",
      "Train Epoch: 2851 [256/118836 (0%)] Loss: 12301.105469\n",
      "Train Epoch: 2851 [33024/118836 (28%)] Loss: 12267.891602\n",
      "Train Epoch: 2851 [65792/118836 (55%)] Loss: 12241.492188\n",
      "Train Epoch: 2851 [98560/118836 (83%)] Loss: 12318.230469\n",
      "    epoch          : 2851\n",
      "    loss           : 12220.811471580335\n",
      "    val_loss       : 12218.531803100432\n",
      "    val_log_likelihood: -12139.447471115076\n",
      "    val_log_marginal: -12147.853440749974\n",
      "Train Epoch: 2852 [256/118836 (0%)] Loss: 12262.932617\n",
      "Train Epoch: 2852 [33024/118836 (28%)] Loss: 12255.722656\n",
      "Train Epoch: 2852 [65792/118836 (55%)] Loss: 12226.131836\n",
      "Train Epoch: 2852 [98560/118836 (83%)] Loss: 12194.703125\n",
      "    epoch          : 2852\n",
      "    loss           : 12219.937922288565\n",
      "    val_loss       : 12221.257268632145\n",
      "    val_log_likelihood: -12149.313211299368\n",
      "    val_log_marginal: -12157.614219517929\n",
      "Train Epoch: 2853 [256/118836 (0%)] Loss: 12208.115234\n",
      "Train Epoch: 2853 [33024/118836 (28%)] Loss: 12240.761719\n",
      "Train Epoch: 2853 [65792/118836 (55%)] Loss: 12256.633789\n",
      "Train Epoch: 2853 [98560/118836 (83%)] Loss: 12294.357422\n",
      "    epoch          : 2853\n",
      "    loss           : 12225.699457842225\n",
      "    val_loss       : 12220.399850211339\n",
      "    val_log_likelihood: -12141.52415800765\n",
      "    val_log_marginal: -12149.779326544525\n",
      "Train Epoch: 2854 [256/118836 (0%)] Loss: 12235.467773\n",
      "Train Epoch: 2854 [33024/118836 (28%)] Loss: 12320.004883\n",
      "Train Epoch: 2854 [65792/118836 (55%)] Loss: 12217.494141\n",
      "Train Epoch: 2854 [98560/118836 (83%)] Loss: 12244.330078\n",
      "    epoch          : 2854\n",
      "    loss           : 12218.613564768146\n",
      "    val_loss       : 12220.12574722\n",
      "    val_log_likelihood: -12141.001770736404\n",
      "    val_log_marginal: -12149.334997477643\n",
      "Train Epoch: 2855 [256/118836 (0%)] Loss: 12263.341797\n",
      "Train Epoch: 2855 [33024/118836 (28%)] Loss: 12286.750977\n",
      "Train Epoch: 2855 [65792/118836 (55%)] Loss: 12167.344727\n",
      "Train Epoch: 2855 [98560/118836 (83%)] Loss: 12258.066406\n",
      "    epoch          : 2855\n",
      "    loss           : 12222.306878101737\n",
      "    val_loss       : 12224.174250328324\n",
      "    val_log_likelihood: -12143.303532264526\n",
      "    val_log_marginal: -12151.806914560535\n",
      "Train Epoch: 2856 [256/118836 (0%)] Loss: 12145.254883\n",
      "Train Epoch: 2856 [33024/118836 (28%)] Loss: 12199.705078\n",
      "Train Epoch: 2856 [65792/118836 (55%)] Loss: 12222.008789\n",
      "Train Epoch: 2856 [98560/118836 (83%)] Loss: 12187.332031\n",
      "    epoch          : 2856\n",
      "    loss           : 12222.872957053867\n",
      "    val_loss       : 12220.98793921051\n",
      "    val_log_likelihood: -12144.373700501448\n",
      "    val_log_marginal: -12152.821109426499\n",
      "Train Epoch: 2857 [256/118836 (0%)] Loss: 12192.195312\n",
      "Train Epoch: 2857 [33024/118836 (28%)] Loss: 12222.886719\n",
      "Train Epoch: 2857 [65792/118836 (55%)] Loss: 12157.507812\n",
      "Train Epoch: 2857 [98560/118836 (83%)] Loss: 12184.841797\n",
      "    epoch          : 2857\n",
      "    loss           : 12222.51728717561\n",
      "    val_loss       : 12221.718041414626\n",
      "    val_log_likelihood: -12142.368215758117\n",
      "    val_log_marginal: -12150.745700136176\n",
      "Train Epoch: 2858 [256/118836 (0%)] Loss: 12169.831055\n",
      "Train Epoch: 2858 [33024/118836 (28%)] Loss: 12246.685547\n",
      "Train Epoch: 2858 [65792/118836 (55%)] Loss: 12234.794922\n",
      "Train Epoch: 2858 [98560/118836 (83%)] Loss: 12362.354492\n",
      "    epoch          : 2858\n",
      "    loss           : 12220.34664059269\n",
      "    val_loss       : 12220.880120384572\n",
      "    val_log_likelihood: -12141.184864331317\n",
      "    val_log_marginal: -12149.439035429235\n",
      "Train Epoch: 2859 [256/118836 (0%)] Loss: 12267.313477\n",
      "Train Epoch: 2859 [33024/118836 (28%)] Loss: 12226.472656\n",
      "Train Epoch: 2859 [65792/118836 (55%)] Loss: 12267.286133\n",
      "Train Epoch: 2859 [98560/118836 (83%)] Loss: 12225.229492\n",
      "    epoch          : 2859\n",
      "    loss           : 12215.059586952026\n",
      "    val_loss       : 12220.478071655782\n",
      "    val_log_likelihood: -12140.523548484027\n",
      "    val_log_marginal: -12148.73457176969\n",
      "Train Epoch: 2860 [256/118836 (0%)] Loss: 12221.330078\n",
      "Train Epoch: 2860 [33024/118836 (28%)] Loss: 12197.259766\n",
      "Train Epoch: 2860 [65792/118836 (55%)] Loss: 12305.049805\n",
      "Train Epoch: 2860 [98560/118836 (83%)] Loss: 12308.000000\n",
      "    epoch          : 2860\n",
      "    loss           : 12219.223375142163\n",
      "    val_loss       : 12220.493456368058\n",
      "    val_log_likelihood: -12145.528406094914\n",
      "    val_log_marginal: -12154.065254983447\n",
      "Train Epoch: 2861 [256/118836 (0%)] Loss: 12271.300781\n",
      "Train Epoch: 2861 [33024/118836 (28%)] Loss: 12164.085938\n",
      "Train Epoch: 2861 [65792/118836 (55%)] Loss: 12254.496094\n",
      "Train Epoch: 2861 [98560/118836 (83%)] Loss: 12148.822266\n",
      "    epoch          : 2861\n",
      "    loss           : 12217.000437312603\n",
      "    val_loss       : 12222.278276594961\n",
      "    val_log_likelihood: -12142.920065814982\n",
      "    val_log_marginal: -12151.444576925467\n",
      "Train Epoch: 2862 [256/118836 (0%)] Loss: 12209.868164\n",
      "Train Epoch: 2862 [33024/118836 (28%)] Loss: 12248.791992\n",
      "Train Epoch: 2862 [65792/118836 (55%)] Loss: 12163.237305\n",
      "Train Epoch: 2862 [98560/118836 (83%)] Loss: 12218.448242\n",
      "    epoch          : 2862\n",
      "    loss           : 12220.876156043218\n",
      "    val_loss       : 12224.395942234973\n",
      "    val_log_likelihood: -12142.006419626241\n",
      "    val_log_marginal: -12150.70382058263\n",
      "Train Epoch: 2863 [256/118836 (0%)] Loss: 12310.597656\n",
      "Train Epoch: 2863 [33024/118836 (28%)] Loss: 12222.179688\n",
      "Train Epoch: 2863 [65792/118836 (55%)] Loss: 12360.730469\n",
      "Train Epoch: 2863 [98560/118836 (83%)] Loss: 12262.175781\n",
      "    epoch          : 2863\n",
      "    loss           : 12219.890539379136\n",
      "    val_loss       : 12221.205356049462\n",
      "    val_log_likelihood: -12140.558637691272\n",
      "    val_log_marginal: -12149.220646688895\n",
      "Train Epoch: 2864 [256/118836 (0%)] Loss: 12184.243164\n",
      "Train Epoch: 2864 [33024/118836 (28%)] Loss: 12235.126953\n",
      "Train Epoch: 2864 [65792/118836 (55%)] Loss: 12354.275391\n",
      "Train Epoch: 2864 [98560/118836 (83%)] Loss: 12171.712891\n",
      "    epoch          : 2864\n",
      "    loss           : 12221.192184269024\n",
      "    val_loss       : 12216.867578331705\n",
      "    val_log_likelihood: -12141.609089704818\n",
      "    val_log_marginal: -12150.041227807502\n",
      "Train Epoch: 2865 [256/118836 (0%)] Loss: 12201.022461\n",
      "Train Epoch: 2865 [33024/118836 (28%)] Loss: 12257.045898\n",
      "Train Epoch: 2865 [65792/118836 (55%)] Loss: 12188.630859\n",
      "Train Epoch: 2865 [98560/118836 (83%)] Loss: 12206.767578\n",
      "    epoch          : 2865\n",
      "    loss           : 12219.527954242918\n",
      "    val_loss       : 12217.314277184525\n",
      "    val_log_likelihood: -12141.894057750465\n",
      "    val_log_marginal: -12150.229122615921\n",
      "Train Epoch: 2866 [256/118836 (0%)] Loss: 12187.561523\n",
      "Train Epoch: 2866 [33024/118836 (28%)] Loss: 12332.676758\n",
      "Train Epoch: 2866 [65792/118836 (55%)] Loss: 12200.105469\n",
      "Train Epoch: 2866 [98560/118836 (83%)] Loss: 12274.593750\n",
      "    epoch          : 2866\n",
      "    loss           : 12220.764006765663\n",
      "    val_loss       : 12220.5122215992\n",
      "    val_log_likelihood: -12144.288160249947\n",
      "    val_log_marginal: -12152.805376416334\n",
      "Train Epoch: 2867 [256/118836 (0%)] Loss: 12272.363281\n",
      "Train Epoch: 2867 [33024/118836 (28%)] Loss: 12242.489258\n",
      "Train Epoch: 2867 [65792/118836 (55%)] Loss: 12290.607422\n",
      "Train Epoch: 2867 [98560/118836 (83%)] Loss: 12176.439453\n",
      "    epoch          : 2867\n",
      "    loss           : 12219.107120586486\n",
      "    val_loss       : 12222.585086143079\n",
      "    val_log_likelihood: -12142.113629387666\n",
      "    val_log_marginal: -12150.60993755605\n",
      "Train Epoch: 2868 [256/118836 (0%)] Loss: 12316.155273\n",
      "Train Epoch: 2868 [33024/118836 (28%)] Loss: 12301.811523\n",
      "Train Epoch: 2868 [65792/118836 (55%)] Loss: 12138.875000\n",
      "Train Epoch: 2868 [98560/118836 (83%)] Loss: 12262.665039\n",
      "    epoch          : 2868\n",
      "    loss           : 12221.645304745658\n",
      "    val_loss       : 12221.577870760793\n",
      "    val_log_likelihood: -12140.275682220587\n",
      "    val_log_marginal: -12148.615262690962\n",
      "Train Epoch: 2869 [256/118836 (0%)] Loss: 12191.458984\n",
      "Train Epoch: 2869 [33024/118836 (28%)] Loss: 12357.379883\n",
      "Train Epoch: 2869 [65792/118836 (55%)] Loss: 12261.516602\n",
      "Train Epoch: 2869 [98560/118836 (83%)] Loss: 12208.097656\n",
      "    epoch          : 2869\n",
      "    loss           : 12220.608183092949\n",
      "    val_loss       : 12221.91001896185\n",
      "    val_log_likelihood: -12141.947159972084\n",
      "    val_log_marginal: -12150.398815572438\n",
      "Train Epoch: 2870 [256/118836 (0%)] Loss: 12249.917969\n",
      "Train Epoch: 2870 [33024/118836 (28%)] Loss: 12253.595703\n",
      "Train Epoch: 2870 [65792/118836 (55%)] Loss: 12187.547852\n",
      "Train Epoch: 2870 [98560/118836 (83%)] Loss: 12175.275391\n",
      "    epoch          : 2870\n",
      "    loss           : 12222.471360951717\n",
      "    val_loss       : 12220.847885055315\n",
      "    val_log_likelihood: -12144.141370870811\n",
      "    val_log_marginal: -12152.461914818181\n",
      "Train Epoch: 2871 [256/118836 (0%)] Loss: 12178.563477\n",
      "Train Epoch: 2871 [33024/118836 (28%)] Loss: 12191.847656\n",
      "Train Epoch: 2871 [65792/118836 (55%)] Loss: 12292.898438\n",
      "Train Epoch: 2871 [98560/118836 (83%)] Loss: 12242.803711\n",
      "    epoch          : 2871\n",
      "    loss           : 12220.656043055986\n",
      "    val_loss       : 12219.931224443775\n",
      "    val_log_likelihood: -12141.953501570255\n",
      "    val_log_marginal: -12150.212482826952\n",
      "Train Epoch: 2872 [256/118836 (0%)] Loss: 12300.250000\n",
      "Train Epoch: 2872 [33024/118836 (28%)] Loss: 12228.439453\n",
      "Train Epoch: 2872 [65792/118836 (55%)] Loss: 12252.194336\n",
      "Train Epoch: 2872 [98560/118836 (83%)] Loss: 12228.603516\n",
      "    epoch          : 2872\n",
      "    loss           : 12223.940528878464\n",
      "    val_loss       : 12220.143654572023\n",
      "    val_log_likelihood: -12142.270288590778\n",
      "    val_log_marginal: -12150.46296822215\n",
      "Train Epoch: 2873 [256/118836 (0%)] Loss: 12207.312500\n",
      "Train Epoch: 2873 [33024/118836 (28%)] Loss: 12187.157227\n",
      "Train Epoch: 2873 [65792/118836 (55%)] Loss: 12206.952148\n",
      "Train Epoch: 2873 [98560/118836 (83%)] Loss: 12158.574219\n",
      "    epoch          : 2873\n",
      "    loss           : 12224.150165264422\n",
      "    val_loss       : 12219.747210278532\n",
      "    val_log_likelihood: -12143.696090034377\n",
      "    val_log_marginal: -12152.006203314308\n",
      "Train Epoch: 2874 [256/118836 (0%)] Loss: 12245.924805\n",
      "Train Epoch: 2874 [33024/118836 (28%)] Loss: 12216.959961\n",
      "Train Epoch: 2874 [65792/118836 (55%)] Loss: 12258.878906\n",
      "Train Epoch: 2874 [98560/118836 (83%)] Loss: 12256.660156\n",
      "    epoch          : 2874\n",
      "    loss           : 12221.531182149503\n",
      "    val_loss       : 12221.67742619223\n",
      "    val_log_likelihood: -12142.57721434941\n",
      "    val_log_marginal: -12150.921076828554\n",
      "Train Epoch: 2875 [256/118836 (0%)] Loss: 12355.558594\n",
      "Train Epoch: 2875 [33024/118836 (28%)] Loss: 12261.442383\n",
      "Train Epoch: 2875 [65792/118836 (55%)] Loss: 12214.162109\n",
      "Train Epoch: 2875 [98560/118836 (83%)] Loss: 12251.400391\n",
      "    epoch          : 2875\n",
      "    loss           : 12222.396435425713\n",
      "    val_loss       : 12219.248570719054\n",
      "    val_log_likelihood: -12142.448221024606\n",
      "    val_log_marginal: -12150.739755502076\n",
      "Train Epoch: 2876 [256/118836 (0%)] Loss: 12199.347656\n",
      "Train Epoch: 2876 [33024/118836 (28%)] Loss: 12287.542969\n",
      "Train Epoch: 2876 [65792/118836 (55%)] Loss: 12359.771484\n",
      "Train Epoch: 2876 [98560/118836 (83%)] Loss: 12192.285156\n",
      "    epoch          : 2876\n",
      "    loss           : 12219.939587048955\n",
      "    val_loss       : 12219.615798775674\n",
      "    val_log_likelihood: -12141.985945738988\n",
      "    val_log_marginal: -12150.285181015817\n",
      "Train Epoch: 2877 [256/118836 (0%)] Loss: 12181.535156\n",
      "Train Epoch: 2877 [33024/118836 (28%)] Loss: 12313.187500\n",
      "Train Epoch: 2877 [65792/118836 (55%)] Loss: 12333.341797\n",
      "Train Epoch: 2877 [98560/118836 (83%)] Loss: 12204.201172\n",
      "    epoch          : 2877\n",
      "    loss           : 12223.836805178607\n",
      "    val_loss       : 12219.501411953015\n",
      "    val_log_likelihood: -12139.973091624019\n",
      "    val_log_marginal: -12148.330427880797\n",
      "Train Epoch: 2878 [256/118836 (0%)] Loss: 12169.619141\n",
      "Train Epoch: 2878 [33024/118836 (28%)] Loss: 12289.547852\n",
      "Train Epoch: 2878 [65792/118836 (55%)] Loss: 12239.793945\n",
      "Train Epoch: 2878 [98560/118836 (83%)] Loss: 12269.465820\n",
      "    epoch          : 2878\n",
      "    loss           : 12220.17329194453\n",
      "    val_loss       : 12218.425814520286\n",
      "    val_log_likelihood: -12141.265035346878\n",
      "    val_log_marginal: -12149.725112781449\n",
      "Train Epoch: 2879 [256/118836 (0%)] Loss: 12201.279297\n",
      "Train Epoch: 2879 [33024/118836 (28%)] Loss: 12176.735352\n",
      "Train Epoch: 2879 [65792/118836 (55%)] Loss: 12210.483398\n",
      "Train Epoch: 2879 [98560/118836 (83%)] Loss: 12227.581055\n",
      "    epoch          : 2879\n",
      "    loss           : 12222.032236416977\n",
      "    val_loss       : 12218.46261551608\n",
      "    val_log_likelihood: -12140.887224236198\n",
      "    val_log_marginal: -12149.064925703971\n",
      "Train Epoch: 2880 [256/118836 (0%)] Loss: 12220.821289\n",
      "Train Epoch: 2880 [33024/118836 (28%)] Loss: 12277.011719\n",
      "Train Epoch: 2880 [65792/118836 (55%)] Loss: 12266.667969\n",
      "Train Epoch: 2880 [98560/118836 (83%)] Loss: 12277.923828\n",
      "    epoch          : 2880\n",
      "    loss           : 12224.10607084238\n",
      "    val_loss       : 12219.421053844719\n",
      "    val_log_likelihood: -12146.538641826923\n",
      "    val_log_marginal: -12154.827525377072\n",
      "Train Epoch: 2881 [256/118836 (0%)] Loss: 12210.003906\n",
      "Train Epoch: 2881 [33024/118836 (28%)] Loss: 12274.534180\n",
      "Train Epoch: 2881 [65792/118836 (55%)] Loss: 12306.930664\n",
      "Train Epoch: 2881 [98560/118836 (83%)] Loss: 12164.819336\n",
      "    epoch          : 2881\n",
      "    loss           : 12218.293026584472\n",
      "    val_loss       : 12219.833570229543\n",
      "    val_log_likelihood: -12143.561133458696\n",
      "    val_log_marginal: -12151.920872714749\n",
      "Train Epoch: 2882 [256/118836 (0%)] Loss: 12179.951172\n",
      "Train Epoch: 2882 [33024/118836 (28%)] Loss: 12172.807617\n",
      "Train Epoch: 2882 [65792/118836 (55%)] Loss: 12202.799805\n",
      "Train Epoch: 2882 [98560/118836 (83%)] Loss: 12184.189453\n",
      "    epoch          : 2882\n",
      "    loss           : 12217.613558144645\n",
      "    val_loss       : 12222.959378456864\n",
      "    val_log_likelihood: -12140.553907542391\n",
      "    val_log_marginal: -12148.716245840977\n",
      "Train Epoch: 2883 [256/118836 (0%)] Loss: 12181.984375\n",
      "Train Epoch: 2883 [33024/118836 (28%)] Loss: 12231.549805\n",
      "Train Epoch: 2883 [65792/118836 (55%)] Loss: 12181.548828\n",
      "Train Epoch: 2883 [98560/118836 (83%)] Loss: 12201.070312\n",
      "    epoch          : 2883\n",
      "    loss           : 12217.62260503903\n",
      "    val_loss       : 12218.966696241429\n",
      "    val_log_likelihood: -12142.197928621277\n",
      "    val_log_marginal: -12150.357535399875\n",
      "Train Epoch: 2884 [256/118836 (0%)] Loss: 12216.566406\n",
      "Train Epoch: 2884 [33024/118836 (28%)] Loss: 12198.305664\n",
      "Train Epoch: 2884 [65792/118836 (55%)] Loss: 12252.619141\n",
      "Train Epoch: 2884 [98560/118836 (83%)] Loss: 12205.757812\n",
      "    epoch          : 2884\n",
      "    loss           : 12220.500324874638\n",
      "    val_loss       : 12222.078666746811\n",
      "    val_log_likelihood: -12142.404778290425\n",
      "    val_log_marginal: -12150.759291166805\n",
      "Train Epoch: 2885 [256/118836 (0%)] Loss: 12252.580078\n",
      "Train Epoch: 2885 [33024/118836 (28%)] Loss: 12217.893555\n",
      "Train Epoch: 2885 [65792/118836 (55%)] Loss: 12338.099609\n",
      "Train Epoch: 2885 [98560/118836 (83%)] Loss: 12331.875977\n",
      "    epoch          : 2885\n",
      "    loss           : 12221.888288681244\n",
      "    val_loss       : 12219.91220111513\n",
      "    val_log_likelihood: -12140.067741450837\n",
      "    val_log_marginal: -12148.412424907343\n",
      "Train Epoch: 2886 [256/118836 (0%)] Loss: 12213.158203\n",
      "Train Epoch: 2886 [33024/118836 (28%)] Loss: 12263.669922\n",
      "Train Epoch: 2886 [65792/118836 (55%)] Loss: 12224.635742\n",
      "Train Epoch: 2886 [98560/118836 (83%)] Loss: 12385.979492\n",
      "    epoch          : 2886\n",
      "    loss           : 12218.104150673335\n",
      "    val_loss       : 12222.851418062965\n",
      "    val_log_likelihood: -12143.192768267938\n",
      "    val_log_marginal: -12151.45982859395\n",
      "Train Epoch: 2887 [256/118836 (0%)] Loss: 12265.049805\n",
      "Train Epoch: 2887 [33024/118836 (28%)] Loss: 12284.627930\n",
      "Train Epoch: 2887 [65792/118836 (55%)] Loss: 12237.445312\n",
      "Train Epoch: 2887 [98560/118836 (83%)] Loss: 12224.489258\n",
      "    epoch          : 2887\n",
      "    loss           : 12218.172931690704\n",
      "    val_loss       : 12219.042467317975\n",
      "    val_log_likelihood: -12142.789475903382\n",
      "    val_log_marginal: -12150.922297745768\n",
      "Train Epoch: 2888 [256/118836 (0%)] Loss: 12227.412109\n",
      "Train Epoch: 2888 [33024/118836 (28%)] Loss: 12226.706055\n",
      "Train Epoch: 2888 [65792/118836 (55%)] Loss: 12356.271484\n",
      "Train Epoch: 2888 [98560/118836 (83%)] Loss: 12411.958008\n",
      "    epoch          : 2888\n",
      "    loss           : 12221.307691176851\n",
      "    val_loss       : 12222.617000408487\n",
      "    val_log_likelihood: -12141.945303291719\n",
      "    val_log_marginal: -12150.23194604212\n",
      "Train Epoch: 2889 [256/118836 (0%)] Loss: 12236.857422\n",
      "Train Epoch: 2889 [33024/118836 (28%)] Loss: 12205.566406\n",
      "Train Epoch: 2889 [65792/118836 (55%)] Loss: 12179.452148\n",
      "Train Epoch: 2889 [98560/118836 (83%)] Loss: 12290.515625\n",
      "    epoch          : 2889\n",
      "    loss           : 12222.331942075063\n",
      "    val_loss       : 12221.498672032752\n",
      "    val_log_likelihood: -12145.79749857837\n",
      "    val_log_marginal: -12154.239158271455\n",
      "Train Epoch: 2890 [256/118836 (0%)] Loss: 12207.667969\n",
      "Train Epoch: 2890 [33024/118836 (28%)] Loss: 12273.523438\n",
      "Train Epoch: 2890 [65792/118836 (55%)] Loss: 12278.198242\n",
      "Train Epoch: 2890 [98560/118836 (83%)] Loss: 12332.852539\n",
      "    epoch          : 2890\n",
      "    loss           : 12223.508880176023\n",
      "    val_loss       : 12219.235780318017\n",
      "    val_log_likelihood: -12140.12871222989\n",
      "    val_log_marginal: -12148.397024001457\n",
      "Train Epoch: 2891 [256/118836 (0%)] Loss: 12246.820312\n",
      "Train Epoch: 2891 [33024/118836 (28%)] Loss: 12238.973633\n",
      "Train Epoch: 2891 [65792/118836 (55%)] Loss: 12310.910156\n",
      "Train Epoch: 2891 [98560/118836 (83%)] Loss: 12263.236328\n",
      "    epoch          : 2891\n",
      "    loss           : 12218.939869436259\n",
      "    val_loss       : 12218.588725594687\n",
      "    val_log_likelihood: -12142.489824687242\n",
      "    val_log_marginal: -12150.651455129762\n",
      "Train Epoch: 2892 [256/118836 (0%)] Loss: 12247.387695\n",
      "Train Epoch: 2892 [33024/118836 (28%)] Loss: 12183.710938\n",
      "Train Epoch: 2892 [65792/118836 (55%)] Loss: 12256.323242\n",
      "Train Epoch: 2892 [98560/118836 (83%)] Loss: 12216.430664\n",
      "    epoch          : 2892\n",
      "    loss           : 12217.349640553919\n",
      "    val_loss       : 12217.542809301636\n",
      "    val_log_likelihood: -12142.48488420182\n",
      "    val_log_marginal: -12151.003426670428\n",
      "Train Epoch: 2893 [256/118836 (0%)] Loss: 12251.494141\n",
      "Train Epoch: 2893 [33024/118836 (28%)] Loss: 12176.664062\n",
      "Train Epoch: 2893 [65792/118836 (55%)] Loss: 12277.284180\n",
      "Train Epoch: 2893 [98560/118836 (83%)] Loss: 12359.868164\n",
      "    epoch          : 2893\n",
      "    loss           : 12220.421171131877\n",
      "    val_loss       : 12222.680945357683\n",
      "    val_log_likelihood: -12141.22406915581\n",
      "    val_log_marginal: -12149.490298641476\n",
      "Train Epoch: 2894 [256/118836 (0%)] Loss: 12248.451172\n",
      "Train Epoch: 2894 [33024/118836 (28%)] Loss: 12188.551758\n",
      "Train Epoch: 2894 [65792/118836 (55%)] Loss: 12210.982422\n",
      "Train Epoch: 2894 [98560/118836 (83%)] Loss: 12255.760742\n",
      "    epoch          : 2894\n",
      "    loss           : 12222.807222846877\n",
      "    val_loss       : 12221.742424016124\n",
      "    val_log_likelihood: -12140.635191790736\n",
      "    val_log_marginal: -12148.962628010277\n",
      "Train Epoch: 2895 [256/118836 (0%)] Loss: 12200.995117\n",
      "Train Epoch: 2895 [33024/118836 (28%)] Loss: 12240.656250\n",
      "Train Epoch: 2895 [65792/118836 (55%)] Loss: 12220.930664\n",
      "Train Epoch: 2895 [98560/118836 (83%)] Loss: 12363.135742\n",
      "    epoch          : 2895\n",
      "    loss           : 12220.415977176384\n",
      "    val_loss       : 12214.374340977065\n",
      "    val_log_likelihood: -12144.218777301747\n",
      "    val_log_marginal: -12152.592274516765\n",
      "Train Epoch: 2896 [256/118836 (0%)] Loss: 12223.834961\n",
      "Train Epoch: 2896 [33024/118836 (28%)] Loss: 12221.464844\n",
      "Train Epoch: 2896 [65792/118836 (55%)] Loss: 12326.734375\n",
      "Train Epoch: 2896 [98560/118836 (83%)] Loss: 12176.537109\n",
      "    epoch          : 2896\n",
      "    loss           : 12221.840005137252\n",
      "    val_loss       : 12222.4249514665\n",
      "    val_log_likelihood: -12144.225563159118\n",
      "    val_log_marginal: -12152.778187192582\n",
      "Train Epoch: 2897 [256/118836 (0%)] Loss: 12233.019531\n",
      "Train Epoch: 2897 [33024/118836 (28%)] Loss: 12203.685547\n",
      "Train Epoch: 2897 [65792/118836 (55%)] Loss: 12253.944336\n",
      "Train Epoch: 2897 [98560/118836 (83%)] Loss: 12178.003906\n",
      "    epoch          : 2897\n",
      "    loss           : 12219.876701916613\n",
      "    val_loss       : 12218.099929643651\n",
      "    val_log_likelihood: -12142.563074790633\n",
      "    val_log_marginal: -12150.97092268719\n",
      "Train Epoch: 2898 [256/118836 (0%)] Loss: 12356.439453\n",
      "Train Epoch: 2898 [33024/118836 (28%)] Loss: 12278.273438\n",
      "Train Epoch: 2898 [65792/118836 (55%)] Loss: 12265.869141\n",
      "Train Epoch: 2898 [98560/118836 (83%)] Loss: 12391.375000\n",
      "    epoch          : 2898\n",
      "    loss           : 12225.08514688017\n",
      "    val_loss       : 12222.518670780846\n",
      "    val_log_likelihood: -12142.649452995762\n",
      "    val_log_marginal: -12150.818971933677\n",
      "Train Epoch: 2899 [256/118836 (0%)] Loss: 12220.498047\n",
      "Train Epoch: 2899 [33024/118836 (28%)] Loss: 12342.228516\n",
      "Train Epoch: 2899 [65792/118836 (55%)] Loss: 12237.683594\n",
      "Train Epoch: 2899 [98560/118836 (83%)] Loss: 12194.594727\n",
      "    epoch          : 2899\n",
      "    loss           : 12221.583322671111\n",
      "    val_loss       : 12220.928652043078\n",
      "    val_log_likelihood: -12142.207173574494\n",
      "    val_log_marginal: -12150.702459773862\n",
      "Train Epoch: 2900 [256/118836 (0%)] Loss: 12240.650391\n",
      "Train Epoch: 2900 [33024/118836 (28%)] Loss: 12281.272461\n",
      "Train Epoch: 2900 [65792/118836 (55%)] Loss: 12353.186523\n",
      "Train Epoch: 2900 [98560/118836 (83%)] Loss: 12244.750977\n",
      "    epoch          : 2900\n",
      "    loss           : 12222.756489576872\n",
      "    val_loss       : 12220.115488458894\n",
      "    val_log_likelihood: -12141.855604289443\n",
      "    val_log_marginal: -12150.160378691657\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch2900.pth ...\n",
      "Train Epoch: 2901 [256/118836 (0%)] Loss: 12245.892578\n",
      "Train Epoch: 2901 [33024/118836 (28%)] Loss: 12349.438477\n",
      "Train Epoch: 2901 [65792/118836 (55%)] Loss: 12351.740234\n",
      "Train Epoch: 2901 [98560/118836 (83%)] Loss: 12244.684570\n",
      "    epoch          : 2901\n",
      "    loss           : 12225.97752032284\n",
      "    val_loss       : 12220.287550842933\n",
      "    val_log_likelihood: -12142.321815226944\n",
      "    val_log_marginal: -12150.697701902609\n",
      "Train Epoch: 2902 [256/118836 (0%)] Loss: 12204.364258\n",
      "Train Epoch: 2902 [33024/118836 (28%)] Loss: 12318.700195\n",
      "Train Epoch: 2902 [65792/118836 (55%)] Loss: 12297.416992\n",
      "Train Epoch: 2902 [98560/118836 (83%)] Loss: 12193.529297\n",
      "    epoch          : 2902\n",
      "    loss           : 12220.433385675144\n",
      "    val_loss       : 12221.761666281036\n",
      "    val_log_likelihood: -12144.454601556039\n",
      "    val_log_marginal: -12152.792304369297\n",
      "Train Epoch: 2903 [256/118836 (0%)] Loss: 12178.080078\n",
      "Train Epoch: 2903 [33024/118836 (28%)] Loss: 12264.320312\n",
      "Train Epoch: 2903 [65792/118836 (55%)] Loss: 12256.189453\n",
      "Train Epoch: 2903 [98560/118836 (83%)] Loss: 12271.298828\n",
      "    epoch          : 2903\n",
      "    loss           : 12224.963058958849\n",
      "    val_loss       : 12222.298178278046\n",
      "    val_log_likelihood: -12140.611024736352\n",
      "    val_log_marginal: -12148.89772425343\n",
      "Train Epoch: 2904 [256/118836 (0%)] Loss: 12171.160156\n",
      "Train Epoch: 2904 [33024/118836 (28%)] Loss: 12239.922852\n",
      "Train Epoch: 2904 [65792/118836 (55%)] Loss: 12217.398438\n",
      "Train Epoch: 2904 [98560/118836 (83%)] Loss: 12277.648438\n",
      "    epoch          : 2904\n",
      "    loss           : 12222.177464750053\n",
      "    val_loss       : 12216.159036721643\n",
      "    val_log_likelihood: -12141.42816280242\n",
      "    val_log_marginal: -12149.704331794577\n",
      "Train Epoch: 2905 [256/118836 (0%)] Loss: 12209.430664\n",
      "Train Epoch: 2905 [33024/118836 (28%)] Loss: 12194.197266\n",
      "Train Epoch: 2905 [65792/118836 (55%)] Loss: 12244.006836\n",
      "Train Epoch: 2905 [98560/118836 (83%)] Loss: 12241.315430\n",
      "    epoch          : 2905\n",
      "    loss           : 12216.173978365385\n",
      "    val_loss       : 12217.556161005967\n",
      "    val_log_likelihood: -12142.002081394749\n",
      "    val_log_marginal: -12150.232937028903\n",
      "Train Epoch: 2906 [256/118836 (0%)] Loss: 12257.468750\n",
      "Train Epoch: 2906 [33024/118836 (28%)] Loss: 12229.342773\n",
      "Train Epoch: 2906 [65792/118836 (55%)] Loss: 12193.126953\n",
      "Train Epoch: 2906 [98560/118836 (83%)] Loss: 12220.427734\n",
      "    epoch          : 2906\n",
      "    loss           : 12220.150143778432\n",
      "    val_loss       : 12219.478933095696\n",
      "    val_log_likelihood: -12140.562808235112\n",
      "    val_log_marginal: -12148.89820463234\n",
      "Train Epoch: 2907 [256/118836 (0%)] Loss: 12342.020508\n",
      "Train Epoch: 2907 [33024/118836 (28%)] Loss: 12251.243164\n",
      "Train Epoch: 2907 [65792/118836 (55%)] Loss: 12275.094727\n",
      "Train Epoch: 2907 [98560/118836 (83%)] Loss: 12250.396484\n",
      "    epoch          : 2907\n",
      "    loss           : 12223.483421538978\n",
      "    val_loss       : 12218.850612357974\n",
      "    val_log_likelihood: -12143.397478707868\n",
      "    val_log_marginal: -12151.688612581585\n",
      "Train Epoch: 2908 [256/118836 (0%)] Loss: 12284.306641\n",
      "Train Epoch: 2908 [33024/118836 (28%)] Loss: 12277.313477\n",
      "Train Epoch: 2908 [65792/118836 (55%)] Loss: 12298.438477\n",
      "Train Epoch: 2908 [98560/118836 (83%)] Loss: 12211.421875\n",
      "    epoch          : 2908\n",
      "    loss           : 12220.555329656483\n",
      "    val_loss       : 12220.361762812961\n",
      "    val_log_likelihood: -12140.250985286135\n",
      "    val_log_marginal: -12148.428839581755\n",
      "Train Epoch: 2909 [256/118836 (0%)] Loss: 12175.544922\n",
      "Train Epoch: 2909 [33024/118836 (28%)] Loss: 12249.222656\n",
      "Train Epoch: 2909 [65792/118836 (55%)] Loss: 12248.199219\n",
      "Train Epoch: 2909 [98560/118836 (83%)] Loss: 12164.258789\n",
      "    epoch          : 2909\n",
      "    loss           : 12220.958569033033\n",
      "    val_loss       : 12223.784222273758\n",
      "    val_log_likelihood: -12141.612110990489\n",
      "    val_log_marginal: -12149.831595411935\n",
      "Train Epoch: 2910 [256/118836 (0%)] Loss: 12271.869141\n",
      "Train Epoch: 2910 [33024/118836 (28%)] Loss: 12211.250000\n",
      "Train Epoch: 2910 [65792/118836 (55%)] Loss: 12422.425781\n",
      "Train Epoch: 2910 [98560/118836 (83%)] Loss: 12176.428711\n",
      "    epoch          : 2910\n",
      "    loss           : 12220.199198071752\n",
      "    val_loss       : 12219.73307723176\n",
      "    val_log_likelihood: -12139.525187719706\n",
      "    val_log_marginal: -12147.731092222502\n",
      "Train Epoch: 2911 [256/118836 (0%)] Loss: 12277.990234\n",
      "Train Epoch: 2911 [33024/118836 (28%)] Loss: 12239.601562\n",
      "Train Epoch: 2911 [65792/118836 (55%)] Loss: 12187.673828\n",
      "Train Epoch: 2911 [98560/118836 (83%)] Loss: 12213.821289\n",
      "    epoch          : 2911\n",
      "    loss           : 12220.02387400486\n",
      "    val_loss       : 12213.639462758643\n",
      "    val_log_likelihood: -12140.443124967689\n",
      "    val_log_marginal: -12148.639529233105\n",
      "Train Epoch: 2912 [256/118836 (0%)] Loss: 12227.938477\n",
      "Train Epoch: 2912 [33024/118836 (28%)] Loss: 12237.194336\n",
      "Train Epoch: 2912 [65792/118836 (55%)] Loss: 12201.628906\n",
      "Train Epoch: 2912 [98560/118836 (83%)] Loss: 12221.832031\n",
      "    epoch          : 2912\n",
      "    loss           : 12216.387448142836\n",
      "    val_loss       : 12219.355930559223\n",
      "    val_log_likelihood: -12140.872748332817\n",
      "    val_log_marginal: -12149.074826831858\n",
      "Train Epoch: 2913 [256/118836 (0%)] Loss: 12161.302734\n",
      "Train Epoch: 2913 [33024/118836 (28%)] Loss: 12160.775391\n",
      "Train Epoch: 2913 [65792/118836 (55%)] Loss: 12187.976562\n",
      "Train Epoch: 2913 [98560/118836 (83%)] Loss: 12183.500000\n",
      "    epoch          : 2913\n",
      "    loss           : 12218.69326632289\n",
      "    val_loss       : 12213.094562695584\n",
      "    val_log_likelihood: -12142.347948976427\n",
      "    val_log_marginal: -12150.485060776738\n",
      "Train Epoch: 2914 [256/118836 (0%)] Loss: 12271.435547\n",
      "Train Epoch: 2914 [33024/118836 (28%)] Loss: 12247.109375\n",
      "Train Epoch: 2914 [65792/118836 (55%)] Loss: 12240.062500\n",
      "Train Epoch: 2914 [98560/118836 (83%)] Loss: 12213.173828\n",
      "    epoch          : 2914\n",
      "    loss           : 12221.083770322839\n",
      "    val_loss       : 12219.50104842028\n",
      "    val_log_likelihood: -12140.48928479115\n",
      "    val_log_marginal: -12148.814010674292\n",
      "Train Epoch: 2915 [256/118836 (0%)] Loss: 12246.765625\n",
      "Train Epoch: 2915 [33024/118836 (28%)] Loss: 12238.925781\n",
      "Train Epoch: 2915 [65792/118836 (55%)] Loss: 12293.125000\n",
      "Train Epoch: 2915 [98560/118836 (83%)] Loss: 12232.683594\n",
      "    epoch          : 2915\n",
      "    loss           : 12218.544249508892\n",
      "    val_loss       : 12219.32201524665\n",
      "    val_log_likelihood: -12139.9395070823\n",
      "    val_log_marginal: -12148.242465538538\n",
      "Train Epoch: 2916 [256/118836 (0%)] Loss: 12301.658203\n",
      "Train Epoch: 2916 [33024/118836 (28%)] Loss: 12415.401367\n",
      "Train Epoch: 2916 [65792/118836 (55%)] Loss: 12258.216797\n",
      "Train Epoch: 2916 [98560/118836 (83%)] Loss: 12174.714844\n",
      "    epoch          : 2916\n",
      "    loss           : 12224.13869336099\n",
      "    val_loss       : 12219.133797137902\n",
      "    val_log_likelihood: -12142.316043249844\n",
      "    val_log_marginal: -12150.672182935947\n",
      "Train Epoch: 2917 [256/118836 (0%)] Loss: 12223.113281\n",
      "Train Epoch: 2917 [33024/118836 (28%)] Loss: 12357.322266\n",
      "Train Epoch: 2917 [65792/118836 (55%)] Loss: 12269.864258\n",
      "Train Epoch: 2917 [98560/118836 (83%)] Loss: 12220.343750\n",
      "    epoch          : 2917\n",
      "    loss           : 12220.265407555315\n",
      "    val_loss       : 12217.937696741015\n",
      "    val_log_likelihood: -12143.986006481338\n",
      "    val_log_marginal: -12152.530502568383\n",
      "Train Epoch: 2918 [256/118836 (0%)] Loss: 12257.991211\n",
      "Train Epoch: 2918 [33024/118836 (28%)] Loss: 12287.844727\n",
      "Train Epoch: 2918 [65792/118836 (55%)] Loss: 12171.018555\n",
      "Train Epoch: 2918 [98560/118836 (83%)] Loss: 12243.566406\n",
      "    epoch          : 2918\n",
      "    loss           : 12222.403016762304\n",
      "    val_loss       : 12218.515466002385\n",
      "    val_log_likelihood: -12141.955899277555\n",
      "    val_log_marginal: -12150.30456027238\n",
      "Train Epoch: 2919 [256/118836 (0%)] Loss: 12206.827148\n",
      "Train Epoch: 2919 [33024/118836 (28%)] Loss: 12251.772461\n",
      "Train Epoch: 2919 [65792/118836 (55%)] Loss: 12179.834961\n",
      "Train Epoch: 2919 [98560/118836 (83%)] Loss: 12207.612305\n",
      "    epoch          : 2919\n",
      "    loss           : 12220.610799052678\n",
      "    val_loss       : 12225.25479237633\n",
      "    val_log_likelihood: -12143.727521292132\n",
      "    val_log_marginal: -12152.186680598017\n",
      "Train Epoch: 2920 [256/118836 (0%)] Loss: 12202.869141\n",
      "Train Epoch: 2920 [33024/118836 (28%)] Loss: 12311.531250\n",
      "Train Epoch: 2920 [65792/118836 (55%)] Loss: 12233.548828\n",
      "Train Epoch: 2920 [98560/118836 (83%)] Loss: 12216.176758\n",
      "    epoch          : 2920\n",
      "    loss           : 12222.853467806555\n",
      "    val_loss       : 12222.508880308429\n",
      "    val_log_likelihood: -12144.607397965778\n",
      "    val_log_marginal: -12152.988626562543\n",
      "Train Epoch: 2921 [256/118836 (0%)] Loss: 12190.108398\n",
      "Train Epoch: 2921 [33024/118836 (28%)] Loss: 12367.052734\n",
      "Train Epoch: 2921 [65792/118836 (55%)] Loss: 12205.786133\n",
      "Train Epoch: 2921 [98560/118836 (83%)] Loss: 12206.185547\n",
      "    epoch          : 2921\n",
      "    loss           : 12217.834552219034\n",
      "    val_loss       : 12218.799814637561\n",
      "    val_log_likelihood: -12141.386475942152\n",
      "    val_log_marginal: -12149.650177469042\n",
      "Train Epoch: 2922 [256/118836 (0%)] Loss: 12248.784180\n",
      "Train Epoch: 2922 [33024/118836 (28%)] Loss: 12256.269531\n",
      "Train Epoch: 2922 [65792/118836 (55%)] Loss: 12225.822266\n",
      "Train Epoch: 2922 [98560/118836 (83%)] Loss: 12336.821289\n",
      "    epoch          : 2922\n",
      "    loss           : 12218.537302748915\n",
      "    val_loss       : 12219.90608549043\n",
      "    val_log_likelihood: -12139.874412447012\n",
      "    val_log_marginal: -12148.46413594929\n",
      "Train Epoch: 2923 [256/118836 (0%)] Loss: 12235.331055\n",
      "Train Epoch: 2923 [33024/118836 (28%)] Loss: 12198.113281\n",
      "Train Epoch: 2923 [65792/118836 (55%)] Loss: 12262.836914\n",
      "Train Epoch: 2923 [98560/118836 (83%)] Loss: 12217.064453\n",
      "    epoch          : 2923\n",
      "    loss           : 12222.39979515612\n",
      "    val_loss       : 12223.387537927698\n",
      "    val_log_likelihood: -12143.787715667648\n",
      "    val_log_marginal: -12152.24655381705\n",
      "Train Epoch: 2924 [256/118836 (0%)] Loss: 12238.564453\n",
      "Train Epoch: 2924 [33024/118836 (28%)] Loss: 12276.998047\n",
      "Train Epoch: 2924 [65792/118836 (55%)] Loss: 12335.729492\n",
      "Train Epoch: 2924 [98560/118836 (83%)] Loss: 12160.357422\n",
      "    epoch          : 2924\n",
      "    loss           : 12221.633112496123\n",
      "    val_loss       : 12225.833770870928\n",
      "    val_log_likelihood: -12141.6726896906\n",
      "    val_log_marginal: -12150.013240147786\n",
      "Train Epoch: 2925 [256/118836 (0%)] Loss: 12292.452148\n",
      "Train Epoch: 2925 [33024/118836 (28%)] Loss: 12217.968750\n",
      "Train Epoch: 2925 [65792/118836 (55%)] Loss: 12204.437500\n",
      "Train Epoch: 2925 [98560/118836 (83%)] Loss: 12196.708008\n",
      "    epoch          : 2925\n",
      "    loss           : 12217.482329307537\n",
      "    val_loss       : 12224.273662145097\n",
      "    val_log_likelihood: -12141.368348551232\n",
      "    val_log_marginal: -12150.007458875632\n",
      "Train Epoch: 2926 [256/118836 (0%)] Loss: 12271.592773\n",
      "Train Epoch: 2926 [33024/118836 (28%)] Loss: 12262.246094\n",
      "Train Epoch: 2926 [65792/118836 (55%)] Loss: 12311.600586\n",
      "Train Epoch: 2926 [98560/118836 (83%)] Loss: 12300.359375\n",
      "    epoch          : 2926\n",
      "    loss           : 12218.120764028898\n",
      "    val_loss       : 12216.793725882373\n",
      "    val_log_likelihood: -12142.740974753153\n",
      "    val_log_marginal: -12151.158445620416\n",
      "Train Epoch: 2927 [256/118836 (0%)] Loss: 12192.060547\n",
      "Train Epoch: 2927 [33024/118836 (28%)] Loss: 12257.477539\n",
      "Train Epoch: 2927 [65792/118836 (55%)] Loss: 12234.467773\n",
      "Train Epoch: 2927 [98560/118836 (83%)] Loss: 12245.931641\n",
      "    epoch          : 2927\n",
      "    loss           : 12217.262605006721\n",
      "    val_loss       : 12217.135842747166\n",
      "    val_log_likelihood: -12139.9125807744\n",
      "    val_log_marginal: -12148.35012580178\n",
      "Train Epoch: 2928 [256/118836 (0%)] Loss: 12298.236328\n",
      "Train Epoch: 2928 [33024/118836 (28%)] Loss: 12142.003906\n",
      "Train Epoch: 2928 [65792/118836 (55%)] Loss: 12266.004883\n",
      "Train Epoch: 2928 [98560/118836 (83%)] Loss: 12302.967773\n",
      "    epoch          : 2928\n",
      "    loss           : 12219.832264364919\n",
      "    val_loss       : 12218.493903736759\n",
      "    val_log_likelihood: -12142.10474000336\n",
      "    val_log_marginal: -12150.549523708645\n",
      "Train Epoch: 2929 [256/118836 (0%)] Loss: 12247.313477\n",
      "Train Epoch: 2929 [33024/118836 (28%)] Loss: 12148.171875\n",
      "Train Epoch: 2929 [65792/118836 (55%)] Loss: 12354.513672\n",
      "Train Epoch: 2929 [98560/118836 (83%)] Loss: 12344.863281\n",
      "    epoch          : 2929\n",
      "    loss           : 12218.962011961074\n",
      "    val_loss       : 12221.72698199129\n",
      "    val_log_likelihood: -12140.301054105923\n",
      "    val_log_marginal: -12148.547488445636\n",
      "Train Epoch: 2930 [256/118836 (0%)] Loss: 12185.458008\n",
      "Train Epoch: 2930 [33024/118836 (28%)] Loss: 12295.657227\n",
      "Train Epoch: 2930 [65792/118836 (55%)] Loss: 12266.772461\n",
      "Train Epoch: 2930 [98560/118836 (83%)] Loss: 12224.484375\n",
      "    epoch          : 2930\n",
      "    loss           : 12219.672405203164\n",
      "    val_loss       : 12218.616546400033\n",
      "    val_log_likelihood: -12142.201715486715\n",
      "    val_log_marginal: -12150.580067816718\n",
      "Train Epoch: 2931 [256/118836 (0%)] Loss: 12265.877930\n",
      "Train Epoch: 2931 [33024/118836 (28%)] Loss: 12221.816406\n",
      "Train Epoch: 2931 [65792/118836 (55%)] Loss: 12182.082031\n",
      "Train Epoch: 2931 [98560/118836 (83%)] Loss: 12289.283203\n",
      "    epoch          : 2931\n",
      "    loss           : 12221.99420282129\n",
      "    val_loss       : 12221.587390198356\n",
      "    val_log_likelihood: -12142.244632218\n",
      "    val_log_marginal: -12150.430931741932\n",
      "Train Epoch: 2932 [256/118836 (0%)] Loss: 12241.403320\n",
      "Train Epoch: 2932 [33024/118836 (28%)] Loss: 12203.609375\n",
      "Train Epoch: 2932 [65792/118836 (55%)] Loss: 12236.483398\n",
      "Train Epoch: 2932 [98560/118836 (83%)] Loss: 12196.881836\n",
      "    epoch          : 2932\n",
      "    loss           : 12222.976526474618\n",
      "    val_loss       : 12218.54595416729\n",
      "    val_log_likelihood: -12140.977678640664\n",
      "    val_log_marginal: -12149.450875690807\n",
      "Train Epoch: 2933 [256/118836 (0%)] Loss: 12282.200195\n",
      "Train Epoch: 2933 [33024/118836 (28%)] Loss: 12228.687500\n",
      "Train Epoch: 2933 [65792/118836 (55%)] Loss: 12252.934570\n",
      "Train Epoch: 2933 [98560/118836 (83%)] Loss: 12205.839844\n",
      "    epoch          : 2933\n",
      "    loss           : 12216.393835297768\n",
      "    val_loss       : 12216.689767680413\n",
      "    val_log_likelihood: -12139.938275272694\n",
      "    val_log_marginal: -12148.217344272305\n",
      "Train Epoch: 2934 [256/118836 (0%)] Loss: 12275.746094\n",
      "Train Epoch: 2934 [33024/118836 (28%)] Loss: 12218.657227\n",
      "Train Epoch: 2934 [65792/118836 (55%)] Loss: 12283.326172\n",
      "Train Epoch: 2934 [98560/118836 (83%)] Loss: 12249.048828\n",
      "    epoch          : 2934\n",
      "    loss           : 12220.291928537272\n",
      "    val_loss       : 12217.332580963975\n",
      "    val_log_likelihood: -12141.415866353907\n",
      "    val_log_marginal: -12149.777563132326\n",
      "Train Epoch: 2935 [256/118836 (0%)] Loss: 12197.238281\n",
      "Train Epoch: 2935 [33024/118836 (28%)] Loss: 12229.421875\n",
      "Train Epoch: 2935 [65792/118836 (55%)] Loss: 12278.989258\n",
      "Train Epoch: 2935 [98560/118836 (83%)] Loss: 12174.672852\n",
      "    epoch          : 2935\n",
      "    loss           : 12218.969380848066\n",
      "    val_loss       : 12218.586140274909\n",
      "    val_log_likelihood: -12139.2707769528\n",
      "    val_log_marginal: -12147.627929925085\n",
      "Train Epoch: 2936 [256/118836 (0%)] Loss: 12204.330078\n",
      "Train Epoch: 2936 [33024/118836 (28%)] Loss: 12207.501953\n",
      "Train Epoch: 2936 [65792/118836 (55%)] Loss: 12259.510742\n",
      "Train Epoch: 2936 [98560/118836 (83%)] Loss: 12191.396484\n",
      "    epoch          : 2936\n",
      "    loss           : 12220.35764545854\n",
      "    val_loss       : 12221.362855341953\n",
      "    val_log_likelihood: -12142.23114353934\n",
      "    val_log_marginal: -12150.588471179404\n",
      "Train Epoch: 2937 [256/118836 (0%)] Loss: 12343.118164\n",
      "Train Epoch: 2937 [33024/118836 (28%)] Loss: 12226.634766\n",
      "Train Epoch: 2937 [65792/118836 (55%)] Loss: 12152.649414\n",
      "Train Epoch: 2937 [98560/118836 (83%)] Loss: 12258.767578\n",
      "    epoch          : 2937\n",
      "    loss           : 12218.224568018508\n",
      "    val_loss       : 12217.632898910315\n",
      "    val_log_likelihood: -12140.096769993279\n",
      "    val_log_marginal: -12148.371493963305\n",
      "Train Epoch: 2938 [256/118836 (0%)] Loss: 12238.510742\n",
      "Train Epoch: 2938 [33024/118836 (28%)] Loss: 12229.701172\n",
      "Train Epoch: 2938 [65792/118836 (55%)] Loss: 12224.106445\n",
      "Train Epoch: 2938 [98560/118836 (83%)] Loss: 12209.327148\n",
      "    epoch          : 2938\n",
      "    loss           : 12224.220182130117\n",
      "    val_loss       : 12218.552443966491\n",
      "    val_log_likelihood: -12141.570001033911\n",
      "    val_log_marginal: -12149.903611834296\n",
      "Train Epoch: 2939 [256/118836 (0%)] Loss: 12193.658203\n",
      "Train Epoch: 2939 [33024/118836 (28%)] Loss: 12233.546875\n",
      "Train Epoch: 2939 [65792/118836 (55%)] Loss: 12287.542969\n",
      "Train Epoch: 2939 [98560/118836 (83%)] Loss: 12235.692383\n",
      "    epoch          : 2939\n",
      "    loss           : 12219.235615371692\n",
      "    val_loss       : 12218.923451755983\n",
      "    val_log_likelihood: -12139.949574642007\n",
      "    val_log_marginal: -12148.198018322242\n",
      "Train Epoch: 2940 [256/118836 (0%)] Loss: 12356.646484\n",
      "Train Epoch: 2940 [33024/118836 (28%)] Loss: 12211.928711\n",
      "Train Epoch: 2940 [65792/118836 (55%)] Loss: 12255.185547\n",
      "Train Epoch: 2940 [98560/118836 (83%)] Loss: 12254.564453\n",
      "    epoch          : 2940\n",
      "    loss           : 12219.888232785359\n",
      "    val_loss       : 12221.59743108812\n",
      "    val_log_likelihood: -12140.024348796784\n",
      "    val_log_marginal: -12148.381610800558\n",
      "Train Epoch: 2941 [256/118836 (0%)] Loss: 12222.522461\n",
      "Train Epoch: 2941 [33024/118836 (28%)] Loss: 12322.619141\n",
      "Train Epoch: 2941 [65792/118836 (55%)] Loss: 12342.145508\n",
      "Train Epoch: 2941 [98560/118836 (83%)] Loss: 12133.033203\n",
      "    epoch          : 2941\n",
      "    loss           : 12215.540277993176\n",
      "    val_loss       : 12220.870699813155\n",
      "    val_log_likelihood: -12140.660092115126\n",
      "    val_log_marginal: -12148.953297651158\n",
      "Train Epoch: 2942 [256/118836 (0%)] Loss: 12309.010742\n",
      "Train Epoch: 2942 [33024/118836 (28%)] Loss: 12229.007812\n",
      "Train Epoch: 2942 [65792/118836 (55%)] Loss: 12170.375000\n",
      "Train Epoch: 2942 [98560/118836 (83%)] Loss: 12180.359375\n",
      "    epoch          : 2942\n",
      "    loss           : 12221.12243541279\n",
      "    val_loss       : 12218.318348742296\n",
      "    val_log_likelihood: -12139.447607462262\n",
      "    val_log_marginal: -12147.662199194114\n",
      "Train Epoch: 2943 [256/118836 (0%)] Loss: 12244.678711\n",
      "Train Epoch: 2943 [33024/118836 (28%)] Loss: 12232.382812\n",
      "Train Epoch: 2943 [65792/118836 (55%)] Loss: 12184.276367\n",
      "Train Epoch: 2943 [98560/118836 (83%)] Loss: 12237.083984\n",
      "    epoch          : 2943\n",
      "    loss           : 12219.38986943626\n",
      "    val_loss       : 12219.721400956743\n",
      "    val_log_likelihood: -12139.916184605045\n",
      "    val_log_marginal: -12148.302577425406\n",
      "Train Epoch: 2944 [256/118836 (0%)] Loss: 12231.774414\n",
      "Train Epoch: 2944 [33024/118836 (28%)] Loss: 12273.943359\n",
      "Train Epoch: 2944 [65792/118836 (55%)] Loss: 12184.125977\n",
      "Train Epoch: 2944 [98560/118836 (83%)] Loss: 12443.516602\n",
      "    epoch          : 2944\n",
      "    loss           : 12218.794189574288\n",
      "    val_loss       : 12219.181792466414\n",
      "    val_log_likelihood: -12143.09188039573\n",
      "    val_log_marginal: -12151.307549543353\n",
      "Train Epoch: 2945 [256/118836 (0%)] Loss: 12247.341797\n",
      "Train Epoch: 2945 [33024/118836 (28%)] Loss: 12183.166016\n",
      "Train Epoch: 2945 [65792/118836 (55%)] Loss: 12235.521484\n",
      "Train Epoch: 2945 [98560/118836 (83%)] Loss: 12334.914062\n",
      "    epoch          : 2945\n",
      "    loss           : 12216.581544665012\n",
      "    val_loss       : 12219.854530500088\n",
      "    val_log_likelihood: -12140.338133594396\n",
      "    val_log_marginal: -12148.590290994804\n",
      "Train Epoch: 2946 [256/118836 (0%)] Loss: 12242.943359\n",
      "Train Epoch: 2946 [33024/118836 (28%)] Loss: 12186.687500\n",
      "Train Epoch: 2946 [65792/118836 (55%)] Loss: 12220.906250\n",
      "Train Epoch: 2946 [98560/118836 (83%)] Loss: 12230.196289\n",
      "    epoch          : 2946\n",
      "    loss           : 12218.051388027294\n",
      "    val_loss       : 12216.13231807977\n",
      "    val_log_likelihood: -12141.264697386787\n",
      "    val_log_marginal: -12149.73729371939\n",
      "Train Epoch: 2947 [256/118836 (0%)] Loss: 12191.108398\n",
      "Train Epoch: 2947 [33024/118836 (28%)] Loss: 12177.640625\n",
      "Train Epoch: 2947 [65792/118836 (55%)] Loss: 12190.042969\n",
      "Train Epoch: 2947 [98560/118836 (83%)] Loss: 12350.980469\n",
      "    epoch          : 2947\n",
      "    loss           : 12220.833019444013\n",
      "    val_loss       : 12221.071016223688\n",
      "    val_log_likelihood: -12139.027974274968\n",
      "    val_log_marginal: -12147.293767280373\n",
      "Train Epoch: 2948 [256/118836 (0%)] Loss: 12232.274414\n",
      "Train Epoch: 2948 [33024/118836 (28%)] Loss: 12164.033203\n",
      "Train Epoch: 2948 [65792/118836 (55%)] Loss: 12221.481445\n",
      "Train Epoch: 2948 [98560/118836 (83%)] Loss: 12210.726562\n",
      "    epoch          : 2948\n",
      "    loss           : 12221.222439936157\n",
      "    val_loss       : 12222.474600118943\n",
      "    val_log_likelihood: -12140.807063882858\n",
      "    val_log_marginal: -12148.986915171772\n",
      "Train Epoch: 2949 [256/118836 (0%)] Loss: 12243.427734\n",
      "Train Epoch: 2949 [33024/118836 (28%)] Loss: 12248.447266\n",
      "Train Epoch: 2949 [65792/118836 (55%)] Loss: 12166.407227\n",
      "Train Epoch: 2949 [98560/118836 (83%)] Loss: 12260.305664\n",
      "    epoch          : 2949\n",
      "    loss           : 12217.753320958696\n",
      "    val_loss       : 12214.320858160954\n",
      "    val_log_likelihood: -12137.89788661859\n",
      "    val_log_marginal: -12146.14819731627\n",
      "Train Epoch: 2950 [256/118836 (0%)] Loss: 12226.403320\n",
      "Train Epoch: 2950 [33024/118836 (28%)] Loss: 12248.714844\n",
      "Train Epoch: 2950 [65792/118836 (55%)] Loss: 12316.548828\n",
      "Train Epoch: 2950 [98560/118836 (83%)] Loss: 12325.558594\n",
      "    epoch          : 2950\n",
      "    loss           : 12220.6564790762\n",
      "    val_loss       : 12224.149671613783\n",
      "    val_log_likelihood: -12146.08559130092\n",
      "    val_log_marginal: -12154.736138490849\n",
      "Train Epoch: 2951 [256/118836 (0%)] Loss: 12181.353516\n",
      "Train Epoch: 2951 [33024/118836 (28%)] Loss: 12182.707031\n",
      "Train Epoch: 2951 [65792/118836 (55%)] Loss: 12159.234375\n",
      "Train Epoch: 2951 [98560/118836 (83%)] Loss: 12179.694336\n",
      "    epoch          : 2951\n",
      "    loss           : 12217.727785585968\n",
      "    val_loss       : 12219.037994098373\n",
      "    val_log_likelihood: -12141.896381629964\n",
      "    val_log_marginal: -12150.14151752229\n",
      "Train Epoch: 2952 [256/118836 (0%)] Loss: 12422.486328\n",
      "Train Epoch: 2952 [33024/118836 (28%)] Loss: 12271.619141\n",
      "Train Epoch: 2952 [65792/118836 (55%)] Loss: 12225.769531\n",
      "Train Epoch: 2952 [98560/118836 (83%)] Loss: 12235.790039\n",
      "    epoch          : 2952\n",
      "    loss           : 12218.945509751085\n",
      "    val_loss       : 12220.42850982444\n",
      "    val_log_likelihood: -12139.75150999664\n",
      "    val_log_marginal: -12148.19185691148\n",
      "Train Epoch: 2953 [256/118836 (0%)] Loss: 12211.107422\n",
      "Train Epoch: 2953 [33024/118836 (28%)] Loss: 12270.078125\n",
      "Train Epoch: 2953 [65792/118836 (55%)] Loss: 12209.098633\n",
      "Train Epoch: 2953 [98560/118836 (83%)] Loss: 12256.753906\n",
      "    epoch          : 2953\n",
      "    loss           : 12218.020158705542\n",
      "    val_loss       : 12222.47428956999\n",
      "    val_log_likelihood: -12141.567530629653\n",
      "    val_log_marginal: -12150.20463157387\n",
      "Train Epoch: 2954 [256/118836 (0%)] Loss: 12294.613281\n",
      "Train Epoch: 2954 [33024/118836 (28%)] Loss: 12209.248047\n",
      "Train Epoch: 2954 [65792/118836 (55%)] Loss: 12161.575195\n",
      "Train Epoch: 2954 [98560/118836 (83%)] Loss: 12246.883789\n",
      "    epoch          : 2954\n",
      "    loss           : 12220.804606079404\n",
      "    val_loss       : 12218.15519930174\n",
      "    val_log_likelihood: -12140.709323304383\n",
      "    val_log_marginal: -12149.126759392624\n",
      "Train Epoch: 2955 [256/118836 (0%)] Loss: 12194.074219\n",
      "Train Epoch: 2955 [33024/118836 (28%)] Loss: 12299.806641\n",
      "Train Epoch: 2955 [65792/118836 (55%)] Loss: 12221.486328\n",
      "Train Epoch: 2955 [98560/118836 (83%)] Loss: 12230.002930\n",
      "    epoch          : 2955\n",
      "    loss           : 12218.696690672818\n",
      "    val_loss       : 12221.42004806168\n",
      "    val_log_likelihood: -12140.48495835272\n",
      "    val_log_marginal: -12148.835078712436\n",
      "Train Epoch: 2956 [256/118836 (0%)] Loss: 12165.667969\n",
      "Train Epoch: 2956 [33024/118836 (28%)] Loss: 12254.480469\n",
      "Train Epoch: 2956 [65792/118836 (55%)] Loss: 12279.328125\n",
      "Train Epoch: 2956 [98560/118836 (83%)] Loss: 12319.868164\n",
      "    epoch          : 2956\n",
      "    loss           : 12221.041478300765\n",
      "    val_loss       : 12214.661948365927\n",
      "    val_log_likelihood: -12140.839550700475\n",
      "    val_log_marginal: -12149.040310358354\n",
      "Train Epoch: 2957 [256/118836 (0%)] Loss: 12206.959961\n",
      "Train Epoch: 2957 [33024/118836 (28%)] Loss: 12228.928711\n",
      "Train Epoch: 2957 [65792/118836 (55%)] Loss: 12270.707031\n",
      "Train Epoch: 2957 [98560/118836 (83%)] Loss: 12180.775391\n",
      "    epoch          : 2957\n",
      "    loss           : 12222.291528380892\n",
      "    val_loss       : 12226.235158212714\n",
      "    val_log_likelihood: -12145.221516846308\n",
      "    val_log_marginal: -12153.988585321214\n",
      "Train Epoch: 2958 [256/118836 (0%)] Loss: 12344.079102\n",
      "Train Epoch: 2958 [33024/118836 (28%)] Loss: 12314.163086\n",
      "Train Epoch: 2958 [65792/118836 (55%)] Loss: 12243.750000\n",
      "Train Epoch: 2958 [98560/118836 (83%)] Loss: 12274.677734\n",
      "    epoch          : 2958\n",
      "    loss           : 12221.020229625465\n",
      "    val_loss       : 12217.104555582828\n",
      "    val_log_likelihood: -12141.196255460349\n",
      "    val_log_marginal: -12149.499409173257\n",
      "Train Epoch: 2959 [256/118836 (0%)] Loss: 12263.755859\n",
      "Train Epoch: 2959 [33024/118836 (28%)] Loss: 12268.648438\n",
      "Train Epoch: 2959 [65792/118836 (55%)] Loss: 12246.652344\n",
      "Train Epoch: 2959 [98560/118836 (83%)] Loss: 12269.576172\n",
      "    epoch          : 2959\n",
      "    loss           : 12218.448734588244\n",
      "    val_loss       : 12219.993343253129\n",
      "    val_log_likelihood: -12139.934147539288\n",
      "    val_log_marginal: -12148.165598497199\n",
      "Train Epoch: 2960 [256/118836 (0%)] Loss: 12178.109375\n",
      "Train Epoch: 2960 [33024/118836 (28%)] Loss: 12224.687500\n",
      "Train Epoch: 2960 [65792/118836 (55%)] Loss: 12189.068359\n",
      "Train Epoch: 2960 [98560/118836 (83%)] Loss: 12199.851562\n",
      "    epoch          : 2960\n",
      "    loss           : 12217.586917939672\n",
      "    val_loss       : 12214.542969040382\n",
      "    val_log_likelihood: -12141.953154724979\n",
      "    val_log_marginal: -12150.346876872818\n",
      "Train Epoch: 2961 [256/118836 (0%)] Loss: 12210.539062\n",
      "Train Epoch: 2961 [33024/118836 (28%)] Loss: 12262.417969\n",
      "Train Epoch: 2961 [65792/118836 (55%)] Loss: 12214.333984\n",
      "Train Epoch: 2961 [98560/118836 (83%)] Loss: 12158.150391\n",
      "    epoch          : 2961\n",
      "    loss           : 12220.04373529906\n",
      "    val_loss       : 12223.538943875481\n",
      "    val_log_likelihood: -12142.455828034534\n",
      "    val_log_marginal: -12150.89092934466\n",
      "Train Epoch: 2962 [256/118836 (0%)] Loss: 12257.642578\n",
      "Train Epoch: 2962 [33024/118836 (28%)] Loss: 12214.265625\n",
      "Train Epoch: 2962 [65792/118836 (55%)] Loss: 12263.343750\n",
      "Train Epoch: 2962 [98560/118836 (83%)] Loss: 12269.805664\n",
      "    epoch          : 2962\n",
      "    loss           : 12217.966454391542\n",
      "    val_loss       : 12224.340101951635\n",
      "    val_log_likelihood: -12142.337710659636\n",
      "    val_log_marginal: -12150.685063618948\n",
      "Train Epoch: 2963 [256/118836 (0%)] Loss: 12237.233398\n",
      "Train Epoch: 2963 [33024/118836 (28%)] Loss: 12388.144531\n",
      "Train Epoch: 2963 [65792/118836 (55%)] Loss: 12132.919922\n",
      "Train Epoch: 2963 [98560/118836 (83%)] Loss: 12267.265625\n",
      "    epoch          : 2963\n",
      "    loss           : 12223.00752138906\n",
      "    val_loss       : 12220.838231383315\n",
      "    val_log_likelihood: -12143.072978378308\n",
      "    val_log_marginal: -12151.375620899402\n",
      "Train Epoch: 2964 [256/118836 (0%)] Loss: 12211.488281\n",
      "Train Epoch: 2964 [33024/118836 (28%)] Loss: 12222.437500\n",
      "Train Epoch: 2964 [65792/118836 (55%)] Loss: 12321.084961\n",
      "Train Epoch: 2964 [98560/118836 (83%)] Loss: 12185.531250\n",
      "    epoch          : 2964\n",
      "    loss           : 12220.142758413462\n",
      "    val_loss       : 12223.030824637606\n",
      "    val_log_likelihood: -12142.584862392732\n",
      "    val_log_marginal: -12151.03016520707\n",
      "Train Epoch: 2965 [256/118836 (0%)] Loss: 12197.633789\n",
      "Train Epoch: 2965 [33024/118836 (28%)] Loss: 12213.804688\n",
      "Train Epoch: 2965 [65792/118836 (55%)] Loss: 12234.634766\n",
      "Train Epoch: 2965 [98560/118836 (83%)] Loss: 12298.674805\n",
      "    epoch          : 2965\n",
      "    loss           : 12223.335308105874\n",
      "    val_loss       : 12217.10768705776\n",
      "    val_log_likelihood: -12143.580584742038\n",
      "    val_log_marginal: -12152.196956624073\n",
      "Train Epoch: 2966 [256/118836 (0%)] Loss: 12237.770508\n",
      "Train Epoch: 2966 [33024/118836 (28%)] Loss: 12297.436523\n",
      "Train Epoch: 2966 [65792/118836 (55%)] Loss: 12197.545898\n",
      "Train Epoch: 2966 [98560/118836 (83%)] Loss: 12321.338867\n",
      "    epoch          : 2966\n",
      "    loss           : 12226.421480336281\n",
      "    val_loss       : 12223.762929558497\n",
      "    val_log_likelihood: -12144.91679267473\n",
      "    val_log_marginal: -12153.591308025663\n",
      "Train Epoch: 2967 [256/118836 (0%)] Loss: 12304.321289\n",
      "Train Epoch: 2967 [33024/118836 (28%)] Loss: 12364.159180\n",
      "Train Epoch: 2967 [65792/118836 (55%)] Loss: 12294.507812\n",
      "Train Epoch: 2967 [98560/118836 (83%)] Loss: 12448.593750\n",
      "    epoch          : 2967\n",
      "    loss           : 12220.558618305417\n",
      "    val_loss       : 12222.601627158256\n",
      "    val_log_likelihood: -12144.376832932692\n",
      "    val_log_marginal: -12152.985348920423\n",
      "Train Epoch: 2968 [256/118836 (0%)] Loss: 12231.513672\n",
      "Train Epoch: 2968 [33024/118836 (28%)] Loss: 12172.451172\n",
      "Train Epoch: 2968 [65792/118836 (55%)] Loss: 12169.139648\n",
      "Train Epoch: 2968 [98560/118836 (83%)] Loss: 12183.079102\n",
      "    epoch          : 2968\n",
      "    loss           : 12222.106466475394\n",
      "    val_loss       : 12217.15545279978\n",
      "    val_log_likelihood: -12141.063680113988\n",
      "    val_log_marginal: -12149.533358426032\n",
      "Train Epoch: 2969 [256/118836 (0%)] Loss: 12216.620117\n",
      "Train Epoch: 2969 [33024/118836 (28%)] Loss: 12303.849609\n",
      "Train Epoch: 2969 [65792/118836 (55%)] Loss: 12165.119141\n",
      "Train Epoch: 2969 [98560/118836 (83%)] Loss: 12280.958984\n",
      "    epoch          : 2969\n",
      "    loss           : 12219.340492368436\n",
      "    val_loss       : 12223.955445301812\n",
      "    val_log_likelihood: -12139.852928556658\n",
      "    val_log_marginal: -12148.206264999279\n",
      "Train Epoch: 2970 [256/118836 (0%)] Loss: 12178.951172\n",
      "Train Epoch: 2970 [33024/118836 (28%)] Loss: 12323.876953\n",
      "Train Epoch: 2970 [65792/118836 (55%)] Loss: 12288.608398\n",
      "Train Epoch: 2970 [98560/118836 (83%)] Loss: 12385.881836\n",
      "    epoch          : 2970\n",
      "    loss           : 12218.380706711383\n",
      "    val_loss       : 12216.162028836276\n",
      "    val_log_likelihood: -12140.730801055883\n",
      "    val_log_marginal: -12148.995664778531\n",
      "Train Epoch: 2971 [256/118836 (0%)] Loss: 12168.871094\n",
      "Train Epoch: 2971 [33024/118836 (28%)] Loss: 12345.990234\n",
      "Train Epoch: 2971 [65792/118836 (55%)] Loss: 12338.502930\n",
      "Train Epoch: 2971 [98560/118836 (83%)] Loss: 12201.783203\n",
      "    epoch          : 2971\n",
      "    loss           : 12221.943166324183\n",
      "    val_loss       : 12220.372309913844\n",
      "    val_log_likelihood: -12139.66851413875\n",
      "    val_log_marginal: -12148.011761019983\n",
      "Train Epoch: 2972 [256/118836 (0%)] Loss: 12204.513672\n",
      "Train Epoch: 2972 [33024/118836 (28%)] Loss: 12196.273438\n",
      "Train Epoch: 2972 [65792/118836 (55%)] Loss: 12201.900391\n",
      "Train Epoch: 2972 [98560/118836 (83%)] Loss: 12252.935547\n",
      "    epoch          : 2972\n",
      "    loss           : 12218.61875533111\n",
      "    val_loss       : 12222.768567100335\n",
      "    val_log_likelihood: -12148.989812086435\n",
      "    val_log_marginal: -12157.24484323071\n",
      "Train Epoch: 2973 [256/118836 (0%)] Loss: 12309.369141\n",
      "Train Epoch: 2973 [33024/118836 (28%)] Loss: 12200.964844\n",
      "Train Epoch: 2973 [65792/118836 (55%)] Loss: 12110.343750\n",
      "Train Epoch: 2973 [98560/118836 (83%)] Loss: 12242.612305\n",
      "    epoch          : 2973\n",
      "    loss           : 12219.17504587986\n",
      "    val_loss       : 12214.814135386023\n",
      "    val_log_likelihood: -12140.652998830386\n",
      "    val_log_marginal: -12149.029433018713\n",
      "Train Epoch: 2974 [256/118836 (0%)] Loss: 12250.161133\n",
      "Train Epoch: 2974 [33024/118836 (28%)] Loss: 12280.624023\n",
      "Train Epoch: 2974 [65792/118836 (55%)] Loss: 12281.983398\n",
      "Train Epoch: 2974 [98560/118836 (83%)] Loss: 12227.526367\n",
      "    epoch          : 2974\n",
      "    loss           : 12223.010474501138\n",
      "    val_loss       : 12226.351926892245\n",
      "    val_log_likelihood: -12139.196716359078\n",
      "    val_log_marginal: -12147.39654050083\n",
      "Train Epoch: 2975 [256/118836 (0%)] Loss: 12247.303711\n",
      "Train Epoch: 2975 [33024/118836 (28%)] Loss: 12212.436523\n",
      "Train Epoch: 2975 [65792/118836 (55%)] Loss: 12351.721680\n",
      "Train Epoch: 2975 [98560/118836 (83%)] Loss: 12155.187500\n",
      "    epoch          : 2975\n",
      "    loss           : 12221.023716010133\n",
      "    val_loss       : 12217.780905189198\n",
      "    val_log_likelihood: -12143.27544474385\n",
      "    val_log_marginal: -12151.72714259729\n",
      "Train Epoch: 2976 [256/118836 (0%)] Loss: 12259.830078\n",
      "Train Epoch: 2976 [33024/118836 (28%)] Loss: 12218.707031\n",
      "Train Epoch: 2976 [65792/118836 (55%)] Loss: 12255.430664\n",
      "Train Epoch: 2976 [98560/118836 (83%)] Loss: 12221.566406\n",
      "    epoch          : 2976\n",
      "    loss           : 12220.888416627895\n",
      "    val_loss       : 12217.075746682847\n",
      "    val_log_likelihood: -12138.699160592432\n",
      "    val_log_marginal: -12147.128755748794\n",
      "Train Epoch: 2977 [256/118836 (0%)] Loss: 12320.761719\n",
      "Train Epoch: 2977 [33024/118836 (28%)] Loss: 12274.687500\n",
      "Train Epoch: 2977 [65792/118836 (55%)] Loss: 12272.652344\n",
      "Train Epoch: 2977 [98560/118836 (83%)] Loss: 12307.470703\n",
      "    epoch          : 2977\n",
      "    loss           : 12220.557028180574\n",
      "    val_loss       : 12221.795281781064\n",
      "    val_log_likelihood: -12140.552033737851\n",
      "    val_log_marginal: -12148.990068216415\n",
      "Train Epoch: 2978 [256/118836 (0%)] Loss: 12320.871094\n",
      "Train Epoch: 2978 [33024/118836 (28%)] Loss: 12297.412109\n",
      "Train Epoch: 2978 [65792/118836 (55%)] Loss: 12193.578125\n",
      "Train Epoch: 2978 [98560/118836 (83%)] Loss: 12356.863281\n",
      "    epoch          : 2978\n",
      "    loss           : 12219.497248500827\n",
      "    val_loss       : 12219.36234660094\n",
      "    val_log_likelihood: -12142.067950333436\n",
      "    val_log_marginal: -12150.467745073589\n",
      "Train Epoch: 2979 [256/118836 (0%)] Loss: 12350.636719\n",
      "Train Epoch: 2979 [33024/118836 (28%)] Loss: 12252.438477\n",
      "Train Epoch: 2979 [65792/118836 (55%)] Loss: 12232.217773\n",
      "Train Epoch: 2979 [98560/118836 (83%)] Loss: 12206.201172\n",
      "    epoch          : 2979\n",
      "    loss           : 12220.269352415478\n",
      "    val_loss       : 12221.951597387317\n",
      "    val_log_likelihood: -12141.932124786756\n",
      "    val_log_marginal: -12150.103195624006\n",
      "Train Epoch: 2980 [256/118836 (0%)] Loss: 12304.546875\n",
      "Train Epoch: 2980 [33024/118836 (28%)] Loss: 12287.064453\n",
      "Train Epoch: 2980 [65792/118836 (55%)] Loss: 12237.243164\n",
      "Train Epoch: 2980 [98560/118836 (83%)] Loss: 12352.525391\n",
      "    epoch          : 2980\n",
      "    loss           : 12222.70379429668\n",
      "    val_loss       : 12221.874772318239\n",
      "    val_log_likelihood: -12144.17409613446\n",
      "    val_log_marginal: -12152.771340736135\n",
      "Train Epoch: 2981 [256/118836 (0%)] Loss: 12237.212891\n",
      "Train Epoch: 2981 [33024/118836 (28%)] Loss: 12277.927734\n",
      "Train Epoch: 2981 [65792/118836 (55%)] Loss: 12276.951172\n",
      "Train Epoch: 2981 [98560/118836 (83%)] Loss: 12264.191406\n",
      "    epoch          : 2981\n",
      "    loss           : 12216.634044148055\n",
      "    val_loss       : 12219.38946963689\n",
      "    val_log_likelihood: -12140.359712636993\n",
      "    val_log_marginal: -12148.622577949132\n",
      "Train Epoch: 2982 [256/118836 (0%)] Loss: 12292.588867\n",
      "Train Epoch: 2982 [33024/118836 (28%)] Loss: 12233.794922\n",
      "Train Epoch: 2982 [65792/118836 (55%)] Loss: 12299.191406\n",
      "Train Epoch: 2982 [98560/118836 (83%)] Loss: 12201.863281\n",
      "    epoch          : 2982\n",
      "    loss           : 12220.033969350963\n",
      "    val_loss       : 12218.514607851272\n",
      "    val_log_likelihood: -12139.650770749327\n",
      "    val_log_marginal: -12147.97529218056\n",
      "Train Epoch: 2983 [256/118836 (0%)] Loss: 12332.673828\n",
      "Train Epoch: 2983 [33024/118836 (28%)] Loss: 12155.205078\n",
      "Train Epoch: 2983 [65792/118836 (55%)] Loss: 12209.130859\n",
      "Train Epoch: 2983 [98560/118836 (83%)] Loss: 12194.110352\n",
      "    epoch          : 2983\n",
      "    loss           : 12216.951211939104\n",
      "    val_loss       : 12218.314988125168\n",
      "    val_log_likelihood: -12139.774102757961\n",
      "    val_log_marginal: -12147.96227013789\n",
      "Train Epoch: 2984 [256/118836 (0%)] Loss: 12192.257812\n",
      "Train Epoch: 2984 [33024/118836 (28%)] Loss: 12353.765625\n",
      "Train Epoch: 2984 [65792/118836 (55%)] Loss: 12204.966797\n",
      "Train Epoch: 2984 [98560/118836 (83%)] Loss: 12206.707031\n",
      "    epoch          : 2984\n",
      "    loss           : 12219.757147242039\n",
      "    val_loss       : 12218.423786817628\n",
      "    val_log_likelihood: -12139.407431891026\n",
      "    val_log_marginal: -12147.89232284915\n",
      "Train Epoch: 2985 [256/118836 (0%)] Loss: 12163.054688\n",
      "Train Epoch: 2985 [33024/118836 (28%)] Loss: 12182.368164\n",
      "Train Epoch: 2985 [65792/118836 (55%)] Loss: 12257.814453\n",
      "Train Epoch: 2985 [98560/118836 (83%)] Loss: 12175.696289\n",
      "    epoch          : 2985\n",
      "    loss           : 12219.297029117555\n",
      "    val_loss       : 12220.146470564601\n",
      "    val_log_likelihood: -12140.74946123475\n",
      "    val_log_marginal: -12148.97544384134\n",
      "Train Epoch: 2986 [256/118836 (0%)] Loss: 12270.922852\n",
      "Train Epoch: 2986 [33024/118836 (28%)] Loss: 12274.979492\n",
      "Train Epoch: 2986 [65792/118836 (55%)] Loss: 12260.866211\n",
      "Train Epoch: 2986 [98560/118836 (83%)] Loss: 12300.894531\n",
      "    epoch          : 2986\n",
      "    loss           : 12221.10760490979\n",
      "    val_loss       : 12221.32019807176\n",
      "    val_log_likelihood: -12140.725570751913\n",
      "    val_log_marginal: -12148.868020014603\n",
      "Train Epoch: 2987 [256/118836 (0%)] Loss: 12170.878906\n",
      "Train Epoch: 2987 [33024/118836 (28%)] Loss: 12184.195312\n",
      "Train Epoch: 2987 [65792/118836 (55%)] Loss: 12272.113281\n",
      "Train Epoch: 2987 [98560/118836 (83%)] Loss: 12166.600586\n",
      "    epoch          : 2987\n",
      "    loss           : 12222.13980481674\n",
      "    val_loss       : 12217.414112006169\n",
      "    val_log_likelihood: -12140.99663251525\n",
      "    val_log_marginal: -12149.312205244469\n",
      "Train Epoch: 2988 [256/118836 (0%)] Loss: 12222.140625\n",
      "Train Epoch: 2988 [33024/118836 (28%)] Loss: 12284.357422\n",
      "Train Epoch: 2988 [65792/118836 (55%)] Loss: 12160.366211\n",
      "Train Epoch: 2988 [98560/118836 (83%)] Loss: 12201.680664\n",
      "    epoch          : 2988\n",
      "    loss           : 12223.249406631256\n",
      "    val_loss       : 12219.407925417625\n",
      "    val_log_likelihood: -12142.124165761994\n",
      "    val_log_marginal: -12150.378601192162\n",
      "Train Epoch: 2989 [256/118836 (0%)] Loss: 12202.438477\n",
      "Train Epoch: 2989 [33024/118836 (28%)] Loss: 12162.713867\n",
      "Train Epoch: 2989 [65792/118836 (55%)] Loss: 12214.966797\n",
      "Train Epoch: 2989 [98560/118836 (83%)] Loss: 12202.044922\n",
      "    epoch          : 2989\n",
      "    loss           : 12220.894033033499\n",
      "    val_loss       : 12219.455289204818\n",
      "    val_log_likelihood: -12140.337356060018\n",
      "    val_log_marginal: -12148.571007735627\n",
      "Train Epoch: 2990 [256/118836 (0%)] Loss: 12185.839844\n",
      "Train Epoch: 2990 [33024/118836 (28%)] Loss: 12236.318359\n",
      "Train Epoch: 2990 [65792/118836 (55%)] Loss: 12303.258789\n",
      "Train Epoch: 2990 [98560/118836 (83%)] Loss: 12275.683594\n",
      "    epoch          : 2990\n",
      "    loss           : 12223.309224759616\n",
      "    val_loss       : 12218.430960259717\n",
      "    val_log_likelihood: -12140.343808965314\n",
      "    val_log_marginal: -12148.69998072386\n",
      "Train Epoch: 2991 [256/118836 (0%)] Loss: 12321.886719\n",
      "Train Epoch: 2991 [33024/118836 (28%)] Loss: 12274.022461\n",
      "Train Epoch: 2991 [65792/118836 (55%)] Loss: 12201.779297\n",
      "Train Epoch: 2991 [98560/118836 (83%)] Loss: 12200.924805\n",
      "    epoch          : 2991\n",
      "    loss           : 12216.40685225393\n",
      "    val_loss       : 12218.171133851241\n",
      "    val_log_likelihood: -12139.660863833746\n",
      "    val_log_marginal: -12147.909979207701\n",
      "Train Epoch: 2992 [256/118836 (0%)] Loss: 12243.252930\n",
      "Train Epoch: 2992 [33024/118836 (28%)] Loss: 12295.186523\n",
      "Train Epoch: 2992 [65792/118836 (55%)] Loss: 12325.663086\n",
      "Train Epoch: 2992 [98560/118836 (83%)] Loss: 12192.956055\n",
      "    epoch          : 2992\n",
      "    loss           : 12220.42310487102\n",
      "    val_loss       : 12217.789482698443\n",
      "    val_log_likelihood: -12141.205565517732\n",
      "    val_log_marginal: -12149.506754732605\n",
      "Train Epoch: 2993 [256/118836 (0%)] Loss: 12337.225586\n",
      "Train Epoch: 2993 [33024/118836 (28%)] Loss: 12194.749023\n",
      "Train Epoch: 2993 [65792/118836 (55%)] Loss: 12255.757812\n",
      "Train Epoch: 2993 [98560/118836 (83%)] Loss: 12278.424805\n",
      "    epoch          : 2993\n",
      "    loss           : 12219.4560278704\n",
      "    val_loss       : 12217.032326226692\n",
      "    val_log_likelihood: -12139.604681845793\n",
      "    val_log_marginal: -12147.825131457996\n",
      "Train Epoch: 2994 [256/118836 (0%)] Loss: 12335.971680\n",
      "Train Epoch: 2994 [33024/118836 (28%)] Loss: 12283.374023\n",
      "Train Epoch: 2994 [65792/118836 (55%)] Loss: 12261.330078\n",
      "Train Epoch: 2994 [98560/118836 (83%)] Loss: 12240.586914\n",
      "    epoch          : 2994\n",
      "    loss           : 12219.419895865643\n",
      "    val_loss       : 12222.54679710227\n",
      "    val_log_likelihood: -12140.75446246252\n",
      "    val_log_marginal: -12149.067184503843\n",
      "Train Epoch: 2995 [256/118836 (0%)] Loss: 12166.960938\n",
      "Train Epoch: 2995 [33024/118836 (28%)] Loss: 12195.583984\n",
      "Train Epoch: 2995 [65792/118836 (55%)] Loss: 12298.728516\n",
      "Train Epoch: 2995 [98560/118836 (83%)] Loss: 12259.508789\n",
      "    epoch          : 2995\n",
      "    loss           : 12217.174512445721\n",
      "    val_loss       : 12216.410895012907\n",
      "    val_log_likelihood: -12138.214060238317\n",
      "    val_log_marginal: -12146.787348700325\n",
      "Train Epoch: 2996 [256/118836 (0%)] Loss: 12168.720703\n",
      "Train Epoch: 2996 [33024/118836 (28%)] Loss: 12277.589844\n",
      "Train Epoch: 2996 [65792/118836 (55%)] Loss: 12261.477539\n",
      "Train Epoch: 2996 [98560/118836 (83%)] Loss: 12252.236328\n",
      "    epoch          : 2996\n",
      "    loss           : 12222.051457170182\n",
      "    val_loss       : 12220.746960194556\n",
      "    val_log_likelihood: -12143.907191829507\n",
      "    val_log_marginal: -12152.429233149807\n",
      "Train Epoch: 2997 [256/118836 (0%)] Loss: 12269.536133\n",
      "Train Epoch: 2997 [33024/118836 (28%)] Loss: 12284.276367\n",
      "Train Epoch: 2997 [65792/118836 (55%)] Loss: 12219.439453\n",
      "Train Epoch: 2997 [98560/118836 (83%)] Loss: 12179.493164\n",
      "    epoch          : 2997\n",
      "    loss           : 12222.834701490126\n",
      "    val_loss       : 12220.516548553634\n",
      "    val_log_likelihood: -12142.015369591345\n",
      "    val_log_marginal: -12150.51843372905\n",
      "Train Epoch: 2998 [256/118836 (0%)] Loss: 12193.789062\n",
      "Train Epoch: 2998 [33024/118836 (28%)] Loss: 12349.862305\n",
      "Train Epoch: 2998 [65792/118836 (55%)] Loss: 12151.417969\n",
      "Train Epoch: 2998 [98560/118836 (83%)] Loss: 12309.070312\n",
      "    epoch          : 2998\n",
      "    loss           : 12219.453034532671\n",
      "    val_loss       : 12216.099660741484\n",
      "    val_log_likelihood: -12140.61268723506\n",
      "    val_log_marginal: -12149.018658563211\n",
      "Train Epoch: 2999 [256/118836 (0%)] Loss: 12276.554688\n",
      "Train Epoch: 2999 [33024/118836 (28%)] Loss: 12256.620117\n",
      "Train Epoch: 2999 [65792/118836 (55%)] Loss: 12320.064453\n",
      "Train Epoch: 2999 [98560/118836 (83%)] Loss: 12188.663086\n",
      "    epoch          : 2999\n",
      "    loss           : 12215.764305630944\n",
      "    val_loss       : 12220.05970736564\n",
      "    val_log_likelihood: -12141.585491463762\n",
      "    val_log_marginal: -12150.043731312502\n",
      "Train Epoch: 3000 [256/118836 (0%)] Loss: 12264.803711\n",
      "Train Epoch: 3000 [33024/118836 (28%)] Loss: 12167.109375\n",
      "Train Epoch: 3000 [65792/118836 (55%)] Loss: 12303.623047\n",
      "Train Epoch: 3000 [98560/118836 (83%)] Loss: 12210.327148\n",
      "    epoch          : 3000\n",
      "    loss           : 12219.387407432536\n",
      "    val_loss       : 12219.243139953922\n",
      "    val_log_likelihood: -12140.909606337882\n",
      "    val_log_marginal: -12149.28429800681\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch3000.pth ...\n",
      "Train Epoch: 3001 [256/118836 (0%)] Loss: 12273.267578\n",
      "Train Epoch: 3001 [33024/118836 (28%)] Loss: 12194.501953\n",
      "Train Epoch: 3001 [65792/118836 (55%)] Loss: 12309.064453\n",
      "Train Epoch: 3001 [98560/118836 (83%)] Loss: 12430.832031\n",
      "    epoch          : 3001\n",
      "    loss           : 12224.0310143003\n",
      "    val_loss       : 12216.696640289103\n",
      "    val_log_likelihood: -12141.834005214794\n",
      "    val_log_marginal: -12150.17967215898\n",
      "Train Epoch: 3002 [256/118836 (0%)] Loss: 12281.575195\n",
      "Train Epoch: 3002 [33024/118836 (28%)] Loss: 12174.142578\n",
      "Train Epoch: 3002 [65792/118836 (55%)] Loss: 12281.328125\n",
      "Train Epoch: 3002 [98560/118836 (83%)] Loss: 12158.931641\n",
      "    epoch          : 3002\n",
      "    loss           : 12223.322127016128\n",
      "    val_loss       : 12220.941065290996\n",
      "    val_log_likelihood: -12140.242898960918\n",
      "    val_log_marginal: -12148.868294103746\n",
      "Train Epoch: 3003 [256/118836 (0%)] Loss: 12209.948242\n",
      "Train Epoch: 3003 [33024/118836 (28%)] Loss: 12315.638672\n",
      "Train Epoch: 3003 [65792/118836 (55%)] Loss: 12261.709961\n",
      "Train Epoch: 3003 [98560/118836 (83%)] Loss: 12208.186523\n",
      "    epoch          : 3003\n",
      "    loss           : 12218.306211066738\n",
      "    val_loss       : 12222.839863544923\n",
      "    val_log_likelihood: -12141.407798283706\n",
      "    val_log_marginal: -12149.846905956712\n",
      "Train Epoch: 3004 [256/118836 (0%)] Loss: 12163.132812\n",
      "Train Epoch: 3004 [33024/118836 (28%)] Loss: 12215.429688\n",
      "Train Epoch: 3004 [65792/118836 (55%)] Loss: 12147.675781\n",
      "Train Epoch: 3004 [98560/118836 (83%)] Loss: 12320.319336\n",
      "    epoch          : 3004\n",
      "    loss           : 12220.53224804849\n",
      "    val_loss       : 12232.935143861934\n",
      "    val_log_likelihood: -12150.803486707764\n",
      "    val_log_marginal: -12159.232753320472\n",
      "Train Epoch: 3005 [256/118836 (0%)] Loss: 12306.353516\n",
      "Train Epoch: 3005 [33024/118836 (28%)] Loss: 12187.780273\n",
      "Train Epoch: 3005 [65792/118836 (55%)] Loss: 12279.922852\n",
      "Train Epoch: 3005 [98560/118836 (83%)] Loss: 12220.346680\n",
      "    epoch          : 3005\n",
      "    loss           : 12223.466133070977\n",
      "    val_loss       : 12222.552005648438\n",
      "    val_log_likelihood: -12140.253233883892\n",
      "    val_log_marginal: -12148.742064217819\n",
      "Train Epoch: 3006 [256/118836 (0%)] Loss: 12296.837891\n",
      "Train Epoch: 3006 [33024/118836 (28%)] Loss: 12220.743164\n",
      "Train Epoch: 3006 [65792/118836 (55%)] Loss: 12195.851562\n",
      "Train Epoch: 3006 [98560/118836 (83%)] Loss: 12240.029297\n",
      "    epoch          : 3006\n",
      "    loss           : 12220.465721121536\n",
      "    val_loss       : 12222.825053956729\n",
      "    val_log_likelihood: -12146.85160821831\n",
      "    val_log_marginal: -12155.392041124027\n",
      "Train Epoch: 3007 [256/118836 (0%)] Loss: 12234.125977\n",
      "Train Epoch: 3007 [33024/118836 (28%)] Loss: 12245.852539\n",
      "Train Epoch: 3007 [65792/118836 (55%)] Loss: 12212.937500\n",
      "Train Epoch: 3007 [98560/118836 (83%)] Loss: 12342.985352\n",
      "    epoch          : 3007\n",
      "    loss           : 12224.064806755323\n",
      "    val_loss       : 12225.349486832105\n",
      "    val_log_likelihood: -12143.31545666615\n",
      "    val_log_marginal: -12152.18595210824\n",
      "Train Epoch: 3008 [256/118836 (0%)] Loss: 12326.896484\n",
      "Train Epoch: 3008 [33024/118836 (28%)] Loss: 12332.320312\n",
      "Train Epoch: 3008 [65792/118836 (55%)] Loss: 12178.336914\n",
      "Train Epoch: 3008 [98560/118836 (83%)] Loss: 12222.511719\n",
      "    epoch          : 3008\n",
      "    loss           : 12223.633000058158\n",
      "    val_loss       : 12218.743392060227\n",
      "    val_log_likelihood: -12140.730815110628\n",
      "    val_log_marginal: -12149.08261311455\n",
      "Train Epoch: 3009 [256/118836 (0%)] Loss: 12175.738281\n",
      "Train Epoch: 3009 [33024/118836 (28%)] Loss: 12265.862305\n",
      "Train Epoch: 3009 [65792/118836 (55%)] Loss: 12183.445312\n",
      "Train Epoch: 3009 [98560/118836 (83%)] Loss: 12269.500000\n",
      "    epoch          : 3009\n",
      "    loss           : 12220.907898282414\n",
      "    val_loss       : 12219.952399937645\n",
      "    val_log_likelihood: -12140.631457105563\n",
      "    val_log_marginal: -12149.086530040911\n",
      "Train Epoch: 3010 [256/118836 (0%)] Loss: 12250.168945\n",
      "Train Epoch: 3010 [33024/118836 (28%)] Loss: 12153.458984\n",
      "Train Epoch: 3010 [65792/118836 (55%)] Loss: 12260.004883\n",
      "Train Epoch: 3010 [98560/118836 (83%)] Loss: 12289.332031\n",
      "    epoch          : 3010\n",
      "    loss           : 12219.901241664082\n",
      "    val_loss       : 12225.403662620794\n",
      "    val_log_likelihood: -12142.624500652657\n",
      "    val_log_marginal: -12150.957431702882\n",
      "Train Epoch: 3011 [256/118836 (0%)] Loss: 12350.887695\n",
      "Train Epoch: 3011 [33024/118836 (28%)] Loss: 12154.273438\n",
      "Train Epoch: 3011 [65792/118836 (55%)] Loss: 12251.400391\n",
      "Train Epoch: 3011 [98560/118836 (83%)] Loss: 12260.712891\n",
      "    epoch          : 3011\n",
      "    loss           : 12221.477374444274\n",
      "    val_loss       : 12223.540360677805\n",
      "    val_log_likelihood: -12144.009336066738\n",
      "    val_log_marginal: -12152.660464085546\n",
      "Train Epoch: 3012 [256/118836 (0%)] Loss: 12196.223633\n",
      "Train Epoch: 3012 [33024/118836 (28%)] Loss: 12305.889648\n",
      "Train Epoch: 3012 [65792/118836 (55%)] Loss: 12260.346680\n",
      "Train Epoch: 3012 [98560/118836 (83%)] Loss: 12181.963867\n",
      "    epoch          : 3012\n",
      "    loss           : 12223.966529673284\n",
      "    val_loss       : 12222.336224896868\n",
      "    val_log_likelihood: -12142.805902185431\n",
      "    val_log_marginal: -12151.322086361313\n",
      "Train Epoch: 3013 [256/118836 (0%)] Loss: 12156.966797\n",
      "Train Epoch: 3013 [33024/118836 (28%)] Loss: 12270.400391\n",
      "Train Epoch: 3013 [65792/118836 (55%)] Loss: 12283.980469\n",
      "Train Epoch: 3013 [98560/118836 (83%)] Loss: 12337.166992\n",
      "    epoch          : 3013\n",
      "    loss           : 12222.29102354089\n",
      "    val_loss       : 12221.725668885025\n",
      "    val_log_likelihood: -12142.823313107423\n",
      "    val_log_marginal: -12151.379605476135\n",
      "Train Epoch: 3014 [256/118836 (0%)] Loss: 12276.066406\n",
      "Train Epoch: 3014 [33024/118836 (28%)] Loss: 12168.941406\n",
      "Train Epoch: 3014 [65792/118836 (55%)] Loss: 12370.167969\n",
      "Train Epoch: 3014 [98560/118836 (83%)] Loss: 12236.236328\n",
      "    epoch          : 3014\n",
      "    loss           : 12217.848907445461\n",
      "    val_loss       : 12219.185294790741\n",
      "    val_log_likelihood: -12141.350983186\n",
      "    val_log_marginal: -12149.676767419061\n",
      "Train Epoch: 3015 [256/118836 (0%)] Loss: 12230.341797\n",
      "Train Epoch: 3015 [33024/118836 (28%)] Loss: 12180.472656\n",
      "Train Epoch: 3015 [65792/118836 (55%)] Loss: 12193.196289\n",
      "Train Epoch: 3015 [98560/118836 (83%)] Loss: 12162.743164\n",
      "    epoch          : 3015\n",
      "    loss           : 12218.88637788203\n",
      "    val_loss       : 12215.64685896987\n",
      "    val_log_likelihood: -12138.423735557537\n",
      "    val_log_marginal: -12146.690252175456\n",
      "Train Epoch: 3016 [256/118836 (0%)] Loss: 12240.034180\n",
      "Train Epoch: 3016 [33024/118836 (28%)] Loss: 12153.110352\n",
      "Train Epoch: 3016 [65792/118836 (55%)] Loss: 12255.527344\n",
      "Train Epoch: 3016 [98560/118836 (83%)] Loss: 12190.995117\n",
      "    epoch          : 3016\n",
      "    loss           : 12213.247202782517\n",
      "    val_loss       : 12214.609235025711\n",
      "    val_log_likelihood: -12143.342266497364\n",
      "    val_log_marginal: -12151.698255435382\n",
      "Train Epoch: 3017 [256/118836 (0%)] Loss: 12173.666016\n",
      "Train Epoch: 3017 [33024/118836 (28%)] Loss: 12238.232422\n",
      "Train Epoch: 3017 [65792/118836 (55%)] Loss: 12275.132812\n",
      "Train Epoch: 3017 [98560/118836 (83%)] Loss: 12271.572266\n",
      "    epoch          : 3017\n",
      "    loss           : 12219.998457370502\n",
      "    val_loss       : 12221.705065620932\n",
      "    val_log_likelihood: -12141.562206627379\n",
      "    val_log_marginal: -12149.82808516846\n",
      "Train Epoch: 3018 [256/118836 (0%)] Loss: 12204.796875\n",
      "Train Epoch: 3018 [33024/118836 (28%)] Loss: 12285.833008\n",
      "Train Epoch: 3018 [65792/118836 (55%)] Loss: 12368.545898\n",
      "Train Epoch: 3018 [98560/118836 (83%)] Loss: 12316.242188\n",
      "    epoch          : 3018\n",
      "    loss           : 12220.392392666976\n",
      "    val_loss       : 12220.148997036687\n",
      "    val_log_likelihood: -12139.37421196495\n",
      "    val_log_marginal: -12147.588121502471\n",
      "Train Epoch: 3019 [256/118836 (0%)] Loss: 12277.507812\n",
      "Train Epoch: 3019 [33024/118836 (28%)] Loss: 12271.977539\n",
      "Train Epoch: 3019 [65792/118836 (55%)] Loss: 12334.393555\n",
      "Train Epoch: 3019 [98560/118836 (83%)] Loss: 12163.711914\n",
      "    epoch          : 3019\n",
      "    loss           : 12216.493092819479\n",
      "    val_loss       : 12217.354517547634\n",
      "    val_log_likelihood: -12139.423935554953\n",
      "    val_log_marginal: -12147.598514308147\n",
      "Train Epoch: 3020 [256/118836 (0%)] Loss: 12262.921875\n",
      "Train Epoch: 3020 [33024/118836 (28%)] Loss: 12361.732422\n",
      "Train Epoch: 3020 [65792/118836 (55%)] Loss: 12156.108398\n",
      "Train Epoch: 3020 [98560/118836 (83%)] Loss: 12238.844727\n",
      "    epoch          : 3020\n",
      "    loss           : 12215.881629316586\n",
      "    val_loss       : 12219.979860633242\n",
      "    val_log_likelihood: -12140.5853111753\n",
      "    val_log_marginal: -12148.828192931258\n",
      "Train Epoch: 3021 [256/118836 (0%)] Loss: 12243.220703\n",
      "Train Epoch: 3021 [33024/118836 (28%)] Loss: 12183.535156\n",
      "Train Epoch: 3021 [65792/118836 (55%)] Loss: 12183.451172\n",
      "Train Epoch: 3021 [98560/118836 (83%)] Loss: 12177.232422\n",
      "    epoch          : 3021\n",
      "    loss           : 12222.784730084264\n",
      "    val_loss       : 12219.775881209029\n",
      "    val_log_likelihood: -12140.888768319634\n",
      "    val_log_marginal: -12149.416560896903\n",
      "Train Epoch: 3022 [256/118836 (0%)] Loss: 12218.208984\n",
      "Train Epoch: 3022 [33024/118836 (28%)] Loss: 12264.302734\n",
      "Train Epoch: 3022 [65792/118836 (55%)] Loss: 12212.078125\n",
      "Train Epoch: 3022 [98560/118836 (83%)] Loss: 12259.338867\n",
      "    epoch          : 3022\n",
      "    loss           : 12218.613083837365\n",
      "    val_loss       : 12218.513400556647\n",
      "    val_log_likelihood: -12139.477327595121\n",
      "    val_log_marginal: -12147.787484942419\n",
      "Train Epoch: 3023 [256/118836 (0%)] Loss: 12180.646484\n",
      "Train Epoch: 3023 [33024/118836 (28%)] Loss: 12221.009766\n",
      "Train Epoch: 3023 [65792/118836 (55%)] Loss: 12180.837891\n",
      "Train Epoch: 3023 [98560/118836 (83%)] Loss: 12188.058594\n",
      "    epoch          : 3023\n",
      "    loss           : 12219.370341094138\n",
      "    val_loss       : 12224.105820715766\n",
      "    val_log_likelihood: -12140.163758303608\n",
      "    val_log_marginal: -12148.449728422072\n",
      "Train Epoch: 3024 [256/118836 (0%)] Loss: 12281.937500\n",
      "Train Epoch: 3024 [33024/118836 (28%)] Loss: 12139.903320\n",
      "Train Epoch: 3024 [65792/118836 (55%)] Loss: 12246.631836\n",
      "Train Epoch: 3024 [98560/118836 (83%)] Loss: 12267.013672\n",
      "    epoch          : 3024\n",
      "    loss           : 12216.228287356545\n",
      "    val_loss       : 12220.31185209462\n",
      "    val_log_likelihood: -12141.411778523056\n",
      "    val_log_marginal: -12149.607636729132\n",
      "Train Epoch: 3025 [256/118836 (0%)] Loss: 12245.155273\n",
      "Train Epoch: 3025 [33024/118836 (28%)] Loss: 12287.747070\n",
      "Train Epoch: 3025 [65792/118836 (55%)] Loss: 12300.791016\n",
      "Train Epoch: 3025 [98560/118836 (83%)] Loss: 12222.689453\n",
      "    epoch          : 3025\n",
      "    loss           : 12217.47653826768\n",
      "    val_loss       : 12218.847882981609\n",
      "    val_log_likelihood: -12139.334168217536\n",
      "    val_log_marginal: -12147.666453000731\n",
      "Train Epoch: 3026 [256/118836 (0%)] Loss: 12267.843750\n",
      "Train Epoch: 3026 [33024/118836 (28%)] Loss: 12201.494141\n",
      "Train Epoch: 3026 [65792/118836 (55%)] Loss: 12202.736328\n",
      "Train Epoch: 3026 [98560/118836 (83%)] Loss: 12175.311523\n",
      "    epoch          : 3026\n",
      "    loss           : 12219.625822444945\n",
      "    val_loss       : 12220.232146957018\n",
      "    val_log_likelihood: -12141.230773592586\n",
      "    val_log_marginal: -12149.427758255277\n",
      "Train Epoch: 3027 [256/118836 (0%)] Loss: 12302.430664\n",
      "Train Epoch: 3027 [33024/118836 (28%)] Loss: 12257.781250\n",
      "Train Epoch: 3027 [65792/118836 (55%)] Loss: 12144.621094\n",
      "Train Epoch: 3027 [98560/118836 (83%)] Loss: 12278.004883\n",
      "    epoch          : 3027\n",
      "    loss           : 12218.469730116572\n",
      "    val_loss       : 12219.596807010552\n",
      "    val_log_likelihood: -12140.192595733819\n",
      "    val_log_marginal: -12148.326833217483\n",
      "Train Epoch: 3028 [256/118836 (0%)] Loss: 12178.442383\n",
      "Train Epoch: 3028 [33024/118836 (28%)] Loss: 12183.522461\n",
      "Train Epoch: 3028 [65792/118836 (55%)] Loss: 12261.212891\n",
      "Train Epoch: 3028 [98560/118836 (83%)] Loss: 12233.651367\n",
      "    epoch          : 3028\n",
      "    loss           : 12220.257984226375\n",
      "    val_loss       : 12219.424117431812\n",
      "    val_log_likelihood: -12141.558307808624\n",
      "    val_log_marginal: -12149.759946112286\n",
      "Train Epoch: 3029 [256/118836 (0%)] Loss: 12275.924805\n",
      "Train Epoch: 3029 [33024/118836 (28%)] Loss: 12236.122070\n",
      "Train Epoch: 3029 [65792/118836 (55%)] Loss: 12205.292969\n",
      "Train Epoch: 3029 [98560/118836 (83%)] Loss: 12227.847656\n",
      "    epoch          : 3029\n",
      "    loss           : 12227.861773515044\n",
      "    val_loss       : 12327.463103964252\n",
      "    val_log_likelihood: -12147.751434553351\n",
      "    val_log_marginal: -12156.275698952099\n",
      "Train Epoch: 3030 [256/118836 (0%)] Loss: 12862.350586\n",
      "Train Epoch: 3030 [33024/118836 (28%)] Loss: 12449.597656\n",
      "Train Epoch: 3030 [65792/118836 (55%)] Loss: 12252.758789\n",
      "Train Epoch: 3030 [98560/118836 (83%)] Loss: 12284.053711\n",
      "    epoch          : 3030\n",
      "    loss           : 12256.881836745242\n",
      "    val_loss       : 12223.741364972448\n",
      "    val_log_likelihood: -12148.2094289573\n",
      "    val_log_marginal: -12157.00104704537\n",
      "Train Epoch: 3031 [256/118836 (0%)] Loss: 12251.987305\n",
      "Train Epoch: 3031 [33024/118836 (28%)] Loss: 12211.356445\n",
      "Train Epoch: 3031 [65792/118836 (55%)] Loss: 12213.495117\n",
      "Train Epoch: 3031 [98560/118836 (83%)] Loss: 12167.919922\n",
      "    epoch          : 3031\n",
      "    loss           : 12221.58929028381\n",
      "    val_loss       : 12218.894691238878\n",
      "    val_log_likelihood: -12147.468885862541\n",
      "    val_log_marginal: -12155.871338833636\n",
      "Train Epoch: 3032 [256/118836 (0%)] Loss: 12299.657227\n",
      "Train Epoch: 3032 [33024/118836 (28%)] Loss: 12314.462891\n",
      "Train Epoch: 3032 [65792/118836 (55%)] Loss: 12327.158203\n",
      "Train Epoch: 3032 [98560/118836 (83%)] Loss: 12274.950195\n",
      "    epoch          : 3032\n",
      "    loss           : 12223.552304493642\n",
      "    val_loss       : 12225.887929607656\n",
      "    val_log_likelihood: -12146.50950423904\n",
      "    val_log_marginal: -12154.96772226986\n",
      "Train Epoch: 3033 [256/118836 (0%)] Loss: 12178.142578\n",
      "Train Epoch: 3033 [33024/118836 (28%)] Loss: 12298.325195\n",
      "Train Epoch: 3033 [65792/118836 (55%)] Loss: 12254.365234\n",
      "Train Epoch: 3033 [98560/118836 (83%)] Loss: 12281.841797\n",
      "    epoch          : 3033\n",
      "    loss           : 12227.601500949906\n",
      "    val_loss       : 12218.762712191634\n",
      "    val_log_likelihood: -12147.105783123965\n",
      "    val_log_marginal: -12155.432387578956\n",
      "Train Epoch: 3034 [256/118836 (0%)] Loss: 12290.840820\n",
      "Train Epoch: 3034 [33024/118836 (28%)] Loss: 12193.530273\n",
      "Train Epoch: 3034 [65792/118836 (55%)] Loss: 12155.872070\n",
      "Train Epoch: 3034 [98560/118836 (83%)] Loss: 12274.899414\n",
      "    epoch          : 3034\n",
      "    loss           : 12219.149298555107\n",
      "    val_loss       : 12222.217550205167\n",
      "    val_log_likelihood: -12148.110711977875\n",
      "    val_log_marginal: -12156.472185638875\n",
      "Train Epoch: 3035 [256/118836 (0%)] Loss: 12249.144531\n",
      "Train Epoch: 3035 [33024/118836 (28%)] Loss: 12205.575195\n",
      "Train Epoch: 3035 [65792/118836 (55%)] Loss: 12192.031250\n",
      "Train Epoch: 3035 [98560/118836 (83%)] Loss: 12281.751953\n",
      "    epoch          : 3035\n",
      "    loss           : 12220.691404311416\n",
      "    val_loss       : 12221.604877886635\n",
      "    val_log_likelihood: -12145.596415878308\n",
      "    val_log_marginal: -12153.94785328489\n",
      "Train Epoch: 3036 [256/118836 (0%)] Loss: 12250.128906\n",
      "Train Epoch: 3036 [33024/118836 (28%)] Loss: 12295.827148\n",
      "Train Epoch: 3036 [65792/118836 (55%)] Loss: 12260.923828\n",
      "Train Epoch: 3036 [98560/118836 (83%)] Loss: 12224.279297\n",
      "    epoch          : 3036\n",
      "    loss           : 12221.970114764268\n",
      "    val_loss       : 12220.447815827334\n",
      "    val_log_likelihood: -12143.691313682537\n",
      "    val_log_marginal: -12151.973542674754\n",
      "Train Epoch: 3037 [256/118836 (0%)] Loss: 12196.716797\n",
      "Train Epoch: 3037 [33024/118836 (28%)] Loss: 12394.083008\n",
      "Train Epoch: 3037 [65792/118836 (55%)] Loss: 12143.021484\n",
      "Train Epoch: 3037 [98560/118836 (83%)] Loss: 12241.625977\n",
      "    epoch          : 3037\n",
      "    loss           : 12216.609034616677\n",
      "    val_loss       : 12218.04576275338\n",
      "    val_log_likelihood: -12145.481642886683\n",
      "    val_log_marginal: -12153.62701072532\n",
      "Train Epoch: 3038 [256/118836 (0%)] Loss: 12307.474609\n",
      "Train Epoch: 3038 [33024/118836 (28%)] Loss: 12134.838867\n",
      "Train Epoch: 3038 [65792/118836 (55%)] Loss: 12146.518555\n",
      "Train Epoch: 3038 [98560/118836 (83%)] Loss: 12173.685547\n",
      "    epoch          : 3038\n",
      "    loss           : 12221.726583662894\n",
      "    val_loss       : 12224.083499972912\n",
      "    val_log_likelihood: -12143.424882392474\n",
      "    val_log_marginal: -12151.70100414884\n",
      "Train Epoch: 3039 [256/118836 (0%)] Loss: 12248.083984\n",
      "Train Epoch: 3039 [33024/118836 (28%)] Loss: 12171.759766\n",
      "Train Epoch: 3039 [65792/118836 (55%)] Loss: 12256.402344\n",
      "Train Epoch: 3039 [98560/118836 (83%)] Loss: 12189.625000\n",
      "    epoch          : 3039\n",
      "    loss           : 12215.589142143559\n",
      "    val_loss       : 12218.231246111098\n",
      "    val_log_likelihood: -12143.52639416615\n",
      "    val_log_marginal: -12151.714652731493\n",
      "Train Epoch: 3040 [256/118836 (0%)] Loss: 12271.952148\n",
      "Train Epoch: 3040 [33024/118836 (28%)] Loss: 12272.093750\n",
      "Train Epoch: 3040 [65792/118836 (55%)] Loss: 12290.568359\n",
      "Train Epoch: 3040 [98560/118836 (83%)] Loss: 12313.939453\n",
      "    epoch          : 3040\n",
      "    loss           : 12219.540151985111\n",
      "    val_loss       : 12220.814382666065\n",
      "    val_log_likelihood: -12144.851589963297\n",
      "    val_log_marginal: -12153.000081212558\n",
      "Train Epoch: 3041 [256/118836 (0%)] Loss: 12285.586914\n",
      "Train Epoch: 3041 [33024/118836 (28%)] Loss: 12260.023438\n",
      "Train Epoch: 3041 [65792/118836 (55%)] Loss: 12228.799805\n",
      "Train Epoch: 3041 [98560/118836 (83%)] Loss: 12173.392578\n",
      "    epoch          : 3041\n",
      "    loss           : 12223.186886437656\n",
      "    val_loss       : 12218.291660547104\n",
      "    val_log_likelihood: -12144.650132146919\n",
      "    val_log_marginal: -12153.260819343755\n",
      "Train Epoch: 3042 [256/118836 (0%)] Loss: 12245.789062\n",
      "Train Epoch: 3042 [33024/118836 (28%)] Loss: 12253.456055\n",
      "Train Epoch: 3042 [65792/118836 (55%)] Loss: 12228.154297\n",
      "Train Epoch: 3042 [98560/118836 (83%)] Loss: 12261.127930\n",
      "    epoch          : 3042\n",
      "    loss           : 12223.247150763804\n",
      "    val_loss       : 12226.164648391807\n",
      "    val_log_likelihood: -12144.64110609233\n",
      "    val_log_marginal: -12153.098091363923\n",
      "Train Epoch: 3043 [256/118836 (0%)] Loss: 12227.568359\n",
      "Train Epoch: 3043 [33024/118836 (28%)] Loss: 12283.933594\n",
      "Train Epoch: 3043 [65792/118836 (55%)] Loss: 12203.334961\n",
      "Train Epoch: 3043 [98560/118836 (83%)] Loss: 12302.744141\n",
      "    epoch          : 3043\n",
      "    loss           : 12218.979276196753\n",
      "    val_loss       : 12218.121928634308\n",
      "    val_log_likelihood: -12144.208921855614\n",
      "    val_log_marginal: -12152.43513756644\n",
      "Train Epoch: 3044 [256/118836 (0%)] Loss: 12321.552734\n",
      "Train Epoch: 3044 [33024/118836 (28%)] Loss: 12182.206055\n",
      "Train Epoch: 3044 [65792/118836 (55%)] Loss: 12270.392578\n",
      "Train Epoch: 3044 [98560/118836 (83%)] Loss: 12185.872070\n",
      "    epoch          : 3044\n",
      "    loss           : 12224.479505111403\n",
      "    val_loss       : 12218.68040030487\n",
      "    val_log_likelihood: -12145.810783059347\n",
      "    val_log_marginal: -12154.068894428432\n",
      "Train Epoch: 3045 [256/118836 (0%)] Loss: 12259.162109\n",
      "Train Epoch: 3045 [33024/118836 (28%)] Loss: 12190.220703\n",
      "Train Epoch: 3045 [65792/118836 (55%)] Loss: 12183.868164\n",
      "Train Epoch: 3045 [98560/118836 (83%)] Loss: 12327.781250\n",
      "    epoch          : 3045\n",
      "    loss           : 12221.036731997003\n",
      "    val_loss       : 12222.063481333125\n",
      "    val_log_likelihood: -12145.983695202647\n",
      "    val_log_marginal: -12154.398279609082\n",
      "Train Epoch: 3046 [256/118836 (0%)] Loss: 12237.232422\n",
      "Train Epoch: 3046 [33024/118836 (28%)] Loss: 12202.359375\n",
      "Train Epoch: 3046 [65792/118836 (55%)] Loss: 12210.377930\n",
      "Train Epoch: 3046 [98560/118836 (83%)] Loss: 12201.108398\n",
      "    epoch          : 3046\n",
      "    loss           : 12220.63101704663\n",
      "    val_loss       : 12219.443863830958\n",
      "    val_log_likelihood: -12142.715344551281\n",
      "    val_log_marginal: -12151.067834235853\n",
      "Train Epoch: 3047 [256/118836 (0%)] Loss: 12179.021484\n",
      "Train Epoch: 3047 [33024/118836 (28%)] Loss: 12190.242188\n",
      "Train Epoch: 3047 [65792/118836 (55%)] Loss: 12184.593750\n",
      "Train Epoch: 3047 [98560/118836 (83%)] Loss: 12157.054688\n",
      "    epoch          : 3047\n",
      "    loss           : 12220.120819601685\n",
      "    val_loss       : 12219.473383208779\n",
      "    val_log_likelihood: -12143.629278458437\n",
      "    val_log_marginal: -12151.81912995933\n",
      "Train Epoch: 3048 [256/118836 (0%)] Loss: 12237.046875\n",
      "Train Epoch: 3048 [33024/118836 (28%)] Loss: 12152.232422\n",
      "Train Epoch: 3048 [65792/118836 (55%)] Loss: 12311.085938\n",
      "Train Epoch: 3048 [98560/118836 (83%)] Loss: 12240.953125\n",
      "    epoch          : 3048\n",
      "    loss           : 12223.349094195873\n",
      "    val_loss       : 12215.467401161219\n",
      "    val_log_likelihood: -12143.027201587056\n",
      "    val_log_marginal: -12151.231988716463\n",
      "Train Epoch: 3049 [256/118836 (0%)] Loss: 12204.771484\n",
      "Train Epoch: 3049 [33024/118836 (28%)] Loss: 12290.210938\n",
      "Train Epoch: 3049 [65792/118836 (55%)] Loss: 12256.586914\n",
      "Train Epoch: 3049 [98560/118836 (83%)] Loss: 12161.418945\n",
      "    epoch          : 3049\n",
      "    loss           : 12219.103575236506\n",
      "    val_loss       : 12224.472102140362\n",
      "    val_log_likelihood: -12144.782994403951\n",
      "    val_log_marginal: -12153.148762038221\n",
      "Train Epoch: 3050 [256/118836 (0%)] Loss: 12238.849609\n",
      "Train Epoch: 3050 [33024/118836 (28%)] Loss: 12323.238281\n",
      "Train Epoch: 3050 [65792/118836 (55%)] Loss: 12166.933594\n",
      "Train Epoch: 3050 [98560/118836 (83%)] Loss: 12149.368164\n",
      "    epoch          : 3050\n",
      "    loss           : 12217.618127067824\n",
      "    val_loss       : 12220.545532762533\n",
      "    val_log_likelihood: -12142.996354651314\n",
      "    val_log_marginal: -12151.161460856267\n",
      "Train Epoch: 3051 [256/118836 (0%)] Loss: 12277.761719\n",
      "Train Epoch: 3051 [33024/118836 (28%)] Loss: 12272.017578\n",
      "Train Epoch: 3051 [65792/118836 (55%)] Loss: 12201.246094\n",
      "Train Epoch: 3051 [98560/118836 (83%)] Loss: 12251.188477\n",
      "    epoch          : 3051\n",
      "    loss           : 12224.354649697581\n",
      "    val_loss       : 12223.40618017056\n",
      "    val_log_likelihood: -12143.117812209211\n",
      "    val_log_marginal: -12151.517252512685\n",
      "Train Epoch: 3052 [256/118836 (0%)] Loss: 12289.888672\n",
      "Train Epoch: 3052 [33024/118836 (28%)] Loss: 12198.187500\n",
      "Train Epoch: 3052 [65792/118836 (55%)] Loss: 12266.970703\n",
      "Train Epoch: 3052 [98560/118836 (83%)] Loss: 12198.243164\n",
      "    epoch          : 3052\n",
      "    loss           : 12223.803262639578\n",
      "    val_loss       : 12222.482676141839\n",
      "    val_log_likelihood: -12145.263349423594\n",
      "    val_log_marginal: -12153.715339328399\n",
      "Train Epoch: 3053 [256/118836 (0%)] Loss: 12283.665039\n",
      "Train Epoch: 3053 [33024/118836 (28%)] Loss: 12249.886719\n",
      "Train Epoch: 3053 [65792/118836 (55%)] Loss: 12225.022461\n",
      "Train Epoch: 3053 [98560/118836 (83%)] Loss: 12218.561523\n",
      "    epoch          : 3053\n",
      "    loss           : 12218.330459703267\n",
      "    val_loss       : 12222.253739362264\n",
      "    val_log_likelihood: -12145.812113736818\n",
      "    val_log_marginal: -12154.078975362268\n",
      "Train Epoch: 3054 [256/118836 (0%)] Loss: 12268.908203\n",
      "Train Epoch: 3054 [33024/118836 (28%)] Loss: 12294.679688\n",
      "Train Epoch: 3054 [65792/118836 (55%)] Loss: 12143.307617\n",
      "Train Epoch: 3054 [98560/118836 (83%)] Loss: 12196.779297\n",
      "    epoch          : 3054\n",
      "    loss           : 12220.36082021557\n",
      "    val_loss       : 12220.603207992242\n",
      "    val_log_likelihood: -12143.352459257392\n",
      "    val_log_marginal: -12151.579556771727\n",
      "Train Epoch: 3055 [256/118836 (0%)] Loss: 12343.658203\n",
      "Train Epoch: 3055 [33024/118836 (28%)] Loss: 12339.927734\n",
      "Train Epoch: 3055 [65792/118836 (55%)] Loss: 12238.692383\n",
      "Train Epoch: 3055 [98560/118836 (83%)] Loss: 12250.043945\n",
      "    epoch          : 3055\n",
      "    loss           : 12219.671253844861\n",
      "    val_loss       : 12220.917565384163\n",
      "    val_log_likelihood: -12144.08523847834\n",
      "    val_log_marginal: -12152.313961135653\n",
      "Train Epoch: 3056 [256/118836 (0%)] Loss: 12249.705078\n",
      "Train Epoch: 3056 [33024/118836 (28%)] Loss: 12308.468750\n",
      "Train Epoch: 3056 [65792/118836 (55%)] Loss: 12258.881836\n",
      "Train Epoch: 3056 [98560/118836 (83%)] Loss: 12149.734375\n",
      "    epoch          : 3056\n",
      "    loss           : 12222.528795265973\n",
      "    val_loss       : 12219.957602478615\n",
      "    val_log_likelihood: -12144.31750009693\n",
      "    val_log_marginal: -12152.533359202414\n",
      "Train Epoch: 3057 [256/118836 (0%)] Loss: 12248.210938\n",
      "Train Epoch: 3057 [33024/118836 (28%)] Loss: 12415.603516\n",
      "Train Epoch: 3057 [65792/118836 (55%)] Loss: 12178.421875\n",
      "Train Epoch: 3057 [98560/118836 (83%)] Loss: 12261.345703\n",
      "    epoch          : 3057\n",
      "    loss           : 12221.779440814982\n",
      "    val_loss       : 12219.210771545848\n",
      "    val_log_likelihood: -12145.340492529984\n",
      "    val_log_marginal: -12153.59158584768\n",
      "Train Epoch: 3058 [256/118836 (0%)] Loss: 12210.989258\n",
      "Train Epoch: 3058 [33024/118836 (28%)] Loss: 12215.772461\n",
      "Train Epoch: 3058 [65792/118836 (55%)] Loss: 12187.220703\n",
      "Train Epoch: 3058 [98560/118836 (83%)] Loss: 12242.046875\n",
      "    epoch          : 3058\n",
      "    loss           : 12224.84127022591\n",
      "    val_loss       : 12224.00013970589\n",
      "    val_log_likelihood: -12145.112045724773\n",
      "    val_log_marginal: -12153.400809363995\n",
      "Train Epoch: 3059 [256/118836 (0%)] Loss: 12191.960938\n",
      "Train Epoch: 3059 [33024/118836 (28%)] Loss: 12208.930664\n",
      "Train Epoch: 3059 [65792/118836 (55%)] Loss: 12253.229492\n",
      "Train Epoch: 3059 [98560/118836 (83%)] Loss: 12336.931641\n",
      "    epoch          : 3059\n",
      "    loss           : 12219.464945364194\n",
      "    val_loss       : 12219.4624867294\n",
      "    val_log_likelihood: -12144.749842166822\n",
      "    val_log_marginal: -12153.037411874213\n",
      "Train Epoch: 3060 [256/118836 (0%)] Loss: 12336.757812\n",
      "Train Epoch: 3060 [33024/118836 (28%)] Loss: 12164.282227\n",
      "Train Epoch: 3060 [65792/118836 (55%)] Loss: 12181.381836\n",
      "Train Epoch: 3060 [98560/118836 (83%)] Loss: 12223.327148\n",
      "    epoch          : 3060\n",
      "    loss           : 12222.223301799006\n",
      "    val_loss       : 12223.52069484584\n",
      "    val_log_likelihood: -12145.309084373708\n",
      "    val_log_marginal: -12153.56152222812\n",
      "Train Epoch: 3061 [256/118836 (0%)] Loss: 12245.572266\n",
      "Train Epoch: 3061 [33024/118836 (28%)] Loss: 12333.438477\n",
      "Train Epoch: 3061 [65792/118836 (55%)] Loss: 12328.397461\n",
      "Train Epoch: 3061 [98560/118836 (83%)] Loss: 12224.679688\n",
      "    epoch          : 3061\n",
      "    loss           : 12223.66771544148\n",
      "    val_loss       : 12221.808078003196\n",
      "    val_log_likelihood: -12142.613825831007\n",
      "    val_log_marginal: -12150.912386824144\n",
      "Train Epoch: 3062 [256/118836 (0%)] Loss: 12202.586914\n",
      "Train Epoch: 3062 [33024/118836 (28%)] Loss: 12229.661133\n",
      "Train Epoch: 3062 [65792/118836 (55%)] Loss: 12230.451172\n",
      "Train Epoch: 3062 [98560/118836 (83%)] Loss: 12163.396484\n",
      "    epoch          : 3062\n",
      "    loss           : 12217.000204359234\n",
      "    val_loss       : 12223.886647734846\n",
      "    val_log_likelihood: -12143.139512090313\n",
      "    val_log_marginal: -12151.270396300164\n",
      "Train Epoch: 3063 [256/118836 (0%)] Loss: 12189.077148\n",
      "Train Epoch: 3063 [33024/118836 (28%)] Loss: 12329.782227\n",
      "Train Epoch: 3063 [65792/118836 (55%)] Loss: 12267.752930\n",
      "Train Epoch: 3063 [98560/118836 (83%)] Loss: 12160.652344\n",
      "    epoch          : 3063\n",
      "    loss           : 12221.335459638647\n",
      "    val_loss       : 12219.86874226809\n",
      "    val_log_likelihood: -12144.106978908188\n",
      "    val_log_marginal: -12152.317182574963\n",
      "Train Epoch: 3064 [256/118836 (0%)] Loss: 12267.039062\n",
      "Train Epoch: 3064 [33024/118836 (28%)] Loss: 12323.166992\n",
      "Train Epoch: 3064 [65792/118836 (55%)] Loss: 12210.544922\n",
      "Train Epoch: 3064 [98560/118836 (83%)] Loss: 12213.240234\n",
      "    epoch          : 3064\n",
      "    loss           : 12221.920055152761\n",
      "    val_loss       : 12220.959760211073\n",
      "    val_log_likelihood: -12144.471948666254\n",
      "    val_log_marginal: -12152.710696448918\n",
      "Train Epoch: 3065 [256/118836 (0%)] Loss: 12283.832031\n",
      "Train Epoch: 3065 [33024/118836 (28%)] Loss: 12111.674805\n",
      "Train Epoch: 3065 [65792/118836 (55%)] Loss: 12290.607422\n",
      "Train Epoch: 3065 [98560/118836 (83%)] Loss: 12269.924805\n",
      "    epoch          : 3065\n",
      "    loss           : 12221.362976245864\n",
      "    val_loss       : 12222.004617042014\n",
      "    val_log_likelihood: -12143.429444045958\n",
      "    val_log_marginal: -12151.685065525546\n",
      "Train Epoch: 3066 [256/118836 (0%)] Loss: 12231.894531\n",
      "Train Epoch: 3066 [33024/118836 (28%)] Loss: 12262.289062\n",
      "Train Epoch: 3066 [65792/118836 (55%)] Loss: 12272.816406\n",
      "Train Epoch: 3066 [98560/118836 (83%)] Loss: 12185.186523\n",
      "    epoch          : 3066\n",
      "    loss           : 12220.856616069583\n",
      "    val_loss       : 12219.456772301572\n",
      "    val_log_likelihood: -12142.980077317257\n",
      "    val_log_marginal: -12151.120398468564\n",
      "Train Epoch: 3067 [256/118836 (0%)] Loss: 12171.785156\n",
      "Train Epoch: 3067 [33024/118836 (28%)] Loss: 12190.626953\n",
      "Train Epoch: 3067 [65792/118836 (55%)] Loss: 12267.288086\n",
      "Train Epoch: 3067 [98560/118836 (83%)] Loss: 12271.569336\n",
      "    epoch          : 3067\n",
      "    loss           : 12222.863264125828\n",
      "    val_loss       : 12220.089408148848\n",
      "    val_log_likelihood: -12142.900629878774\n",
      "    val_log_marginal: -12151.07007445649\n",
      "Train Epoch: 3068 [256/118836 (0%)] Loss: 12255.302734\n",
      "Train Epoch: 3068 [33024/118836 (28%)] Loss: 12136.173828\n",
      "Train Epoch: 3068 [65792/118836 (55%)] Loss: 12189.447266\n",
      "Train Epoch: 3068 [98560/118836 (83%)] Loss: 12154.412109\n",
      "    epoch          : 3068\n",
      "    loss           : 12218.959706659687\n",
      "    val_loss       : 12224.357063460131\n",
      "    val_log_likelihood: -12144.555567940963\n",
      "    val_log_marginal: -12152.968912867831\n",
      "Train Epoch: 3069 [256/118836 (0%)] Loss: 12219.749023\n",
      "Train Epoch: 3069 [33024/118836 (28%)] Loss: 12218.300781\n",
      "Train Epoch: 3069 [65792/118836 (55%)] Loss: 12250.354492\n",
      "Train Epoch: 3069 [98560/118836 (83%)] Loss: 12343.520508\n",
      "    epoch          : 3069\n",
      "    loss           : 12219.295303776365\n",
      "    val_loss       : 12220.804542499343\n",
      "    val_log_likelihood: -12145.3977304009\n",
      "    val_log_marginal: -12153.614611384735\n",
      "Train Epoch: 3070 [256/118836 (0%)] Loss: 12237.742188\n",
      "Train Epoch: 3070 [33024/118836 (28%)] Loss: 12241.027344\n",
      "Train Epoch: 3070 [65792/118836 (55%)] Loss: 12232.061523\n",
      "Train Epoch: 3070 [98560/118836 (83%)] Loss: 12233.486328\n",
      "    epoch          : 3070\n",
      "    loss           : 12221.97609384693\n",
      "    val_loss       : 12221.03677250395\n",
      "    val_log_likelihood: -12142.927651985112\n",
      "    val_log_marginal: -12151.09289710231\n",
      "Train Epoch: 3071 [256/118836 (0%)] Loss: 12292.085938\n",
      "Train Epoch: 3071 [33024/118836 (28%)] Loss: 12250.987305\n",
      "Train Epoch: 3071 [65792/118836 (55%)] Loss: 12250.365234\n",
      "Train Epoch: 3071 [98560/118836 (83%)] Loss: 12269.100586\n",
      "    epoch          : 3071\n",
      "    loss           : 12220.351219208798\n",
      "    val_loss       : 12219.739371976017\n",
      "    val_log_likelihood: -12142.888068490229\n",
      "    val_log_marginal: -12151.026826491721\n",
      "Train Epoch: 3072 [256/118836 (0%)] Loss: 12167.896484\n",
      "Train Epoch: 3072 [33024/118836 (28%)] Loss: 12211.809570\n",
      "Train Epoch: 3072 [65792/118836 (55%)] Loss: 12205.812500\n",
      "Train Epoch: 3072 [98560/118836 (83%)] Loss: 12251.666016\n",
      "    epoch          : 3072\n",
      "    loss           : 12221.124613898366\n",
      "    val_loss       : 12221.49600640962\n",
      "    val_log_likelihood: -12145.061510352047\n",
      "    val_log_marginal: -12153.17851390727\n",
      "Train Epoch: 3073 [256/118836 (0%)] Loss: 12232.700195\n",
      "Train Epoch: 3073 [33024/118836 (28%)] Loss: 12251.593750\n",
      "Train Epoch: 3073 [65792/118836 (55%)] Loss: 12200.023438\n",
      "Train Epoch: 3073 [98560/118836 (83%)] Loss: 12277.663086\n",
      "    epoch          : 3073\n",
      "    loss           : 12220.681588444737\n",
      "    val_loss       : 12217.81799527535\n",
      "    val_log_likelihood: -12144.888107746587\n",
      "    val_log_marginal: -12153.142401957437\n",
      "Train Epoch: 3074 [256/118836 (0%)] Loss: 12179.571289\n",
      "Train Epoch: 3074 [33024/118836 (28%)] Loss: 12216.415039\n",
      "Train Epoch: 3074 [65792/118836 (55%)] Loss: 12233.345703\n",
      "Train Epoch: 3074 [98560/118836 (83%)] Loss: 12211.552734\n",
      "    epoch          : 3074\n",
      "    loss           : 12219.455424000982\n",
      "    val_loss       : 12219.489619150132\n",
      "    val_log_likelihood: -12142.380319802007\n",
      "    val_log_marginal: -12150.53749542307\n",
      "Train Epoch: 3075 [256/118836 (0%)] Loss: 12239.137695\n",
      "Train Epoch: 3075 [33024/118836 (28%)] Loss: 12258.997070\n",
      "Train Epoch: 3075 [65792/118836 (55%)] Loss: 12223.460938\n",
      "Train Epoch: 3075 [98560/118836 (83%)] Loss: 12344.845703\n",
      "    epoch          : 3075\n",
      "    loss           : 12224.934054648727\n",
      "    val_loss       : 12219.684215313931\n",
      "    val_log_likelihood: -12142.523907122364\n",
      "    val_log_marginal: -12150.81102033875\n",
      "Train Epoch: 3076 [256/118836 (0%)] Loss: 12207.796875\n",
      "Train Epoch: 3076 [33024/118836 (28%)] Loss: 12309.599609\n",
      "Train Epoch: 3076 [65792/118836 (55%)] Loss: 12267.602539\n",
      "Train Epoch: 3076 [98560/118836 (83%)] Loss: 12255.736328\n",
      "    epoch          : 3076\n",
      "    loss           : 12221.495180191532\n",
      "    val_loss       : 12222.038778405113\n",
      "    val_log_likelihood: -12143.261855420285\n",
      "    val_log_marginal: -12151.558191673934\n",
      "Train Epoch: 3077 [256/118836 (0%)] Loss: 12266.575195\n",
      "Train Epoch: 3077 [33024/118836 (28%)] Loss: 12293.746094\n",
      "Train Epoch: 3077 [65792/118836 (55%)] Loss: 12323.375977\n",
      "Train Epoch: 3077 [98560/118836 (83%)] Loss: 12234.924805\n",
      "    epoch          : 3077\n",
      "    loss           : 12222.877865552626\n",
      "    val_loss       : 12224.941188173028\n",
      "    val_log_likelihood: -12143.340888486093\n",
      "    val_log_marginal: -12151.589734328027\n",
      "Train Epoch: 3078 [256/118836 (0%)] Loss: 12206.156250\n",
      "Train Epoch: 3078 [33024/118836 (28%)] Loss: 12151.247070\n",
      "Train Epoch: 3078 [65792/118836 (55%)] Loss: 12182.234375\n",
      "Train Epoch: 3078 [98560/118836 (83%)] Loss: 12346.500000\n",
      "    epoch          : 3078\n",
      "    loss           : 12221.309128961177\n",
      "    val_loss       : 12219.07616300246\n",
      "    val_log_likelihood: -12144.025534080336\n",
      "    val_log_marginal: -12152.354387296427\n",
      "Train Epoch: 3079 [256/118836 (0%)] Loss: 12290.279297\n",
      "Train Epoch: 3079 [33024/118836 (28%)] Loss: 12256.254883\n",
      "Train Epoch: 3079 [65792/118836 (55%)] Loss: 12213.643555\n",
      "Train Epoch: 3079 [98560/118836 (83%)] Loss: 12260.625000\n",
      "    epoch          : 3079\n",
      "    loss           : 12218.994578099153\n",
      "    val_loss       : 12223.25331137944\n",
      "    val_log_likelihood: -12144.089982197322\n",
      "    val_log_marginal: -12152.317845383413\n",
      "Train Epoch: 3080 [256/118836 (0%)] Loss: 12152.291992\n",
      "Train Epoch: 3080 [33024/118836 (28%)] Loss: 12208.429688\n",
      "Train Epoch: 3080 [65792/118836 (55%)] Loss: 12196.964844\n",
      "Train Epoch: 3080 [98560/118836 (83%)] Loss: 12157.459961\n",
      "    epoch          : 3080\n",
      "    loss           : 12220.69381461952\n",
      "    val_loss       : 12224.297899421494\n",
      "    val_log_likelihood: -12145.229399620037\n",
      "    val_log_marginal: -12153.671267761714\n",
      "Train Epoch: 3081 [256/118836 (0%)] Loss: 12306.248047\n",
      "Train Epoch: 3081 [33024/118836 (28%)] Loss: 12203.775391\n",
      "Train Epoch: 3081 [65792/118836 (55%)] Loss: 12200.716797\n",
      "Train Epoch: 3081 [98560/118836 (83%)] Loss: 12366.000000\n",
      "    epoch          : 3081\n",
      "    loss           : 12220.082089892214\n",
      "    val_loss       : 12225.656119009189\n",
      "    val_log_likelihood: -12143.113586738782\n",
      "    val_log_marginal: -12151.254968453492\n",
      "Train Epoch: 3082 [256/118836 (0%)] Loss: 12270.270508\n",
      "Train Epoch: 3082 [33024/118836 (28%)] Loss: 12343.482422\n",
      "Train Epoch: 3082 [65792/118836 (55%)] Loss: 12284.796875\n",
      "Train Epoch: 3082 [98560/118836 (83%)] Loss: 12204.905273\n",
      "    epoch          : 3082\n",
      "    loss           : 12221.175527456833\n",
      "    val_loss       : 12218.86329825122\n",
      "    val_log_likelihood: -12143.733553847445\n",
      "    val_log_marginal: -12151.980206359478\n",
      "Train Epoch: 3083 [256/118836 (0%)] Loss: 12180.488281\n",
      "Train Epoch: 3083 [33024/118836 (28%)] Loss: 12200.716797\n",
      "Train Epoch: 3083 [65792/118836 (55%)] Loss: 12258.332031\n",
      "Train Epoch: 3083 [98560/118836 (83%)] Loss: 12227.244141\n",
      "    epoch          : 3083\n",
      "    loss           : 12220.113672844293\n",
      "    val_loss       : 12222.190259135903\n",
      "    val_log_likelihood: -12143.939662815344\n",
      "    val_log_marginal: -12152.252624515972\n",
      "Train Epoch: 3084 [256/118836 (0%)] Loss: 12265.237305\n",
      "Train Epoch: 3084 [33024/118836 (28%)] Loss: 12314.006836\n",
      "Train Epoch: 3084 [65792/118836 (55%)] Loss: 12155.375000\n",
      "Train Epoch: 3084 [98560/118836 (83%)] Loss: 12235.753906\n",
      "    epoch          : 3084\n",
      "    loss           : 12227.344369862747\n",
      "    val_loss       : 12217.267063428362\n",
      "    val_log_likelihood: -12145.327429855512\n",
      "    val_log_marginal: -12153.55929427919\n",
      "Train Epoch: 3085 [256/118836 (0%)] Loss: 12222.079102\n",
      "Train Epoch: 3085 [33024/118836 (28%)] Loss: 12170.725586\n",
      "Train Epoch: 3085 [65792/118836 (55%)] Loss: 12199.934570\n",
      "Train Epoch: 3085 [98560/118836 (83%)] Loss: 12181.710938\n",
      "    epoch          : 3085\n",
      "    loss           : 12218.780401707249\n",
      "    val_loss       : 12223.732731093363\n",
      "    val_log_likelihood: -12143.659004245503\n",
      "    val_log_marginal: -12152.13866212018\n",
      "Train Epoch: 3086 [256/118836 (0%)] Loss: 12241.481445\n",
      "Train Epoch: 3086 [33024/118836 (28%)] Loss: 12163.074219\n",
      "Train Epoch: 3086 [65792/118836 (55%)] Loss: 12389.244141\n",
      "Train Epoch: 3086 [98560/118836 (83%)] Loss: 12312.083008\n",
      "    epoch          : 3086\n",
      "    loss           : 12224.361562370761\n",
      "    val_loss       : 12218.021307792502\n",
      "    val_log_likelihood: -12144.758534300041\n",
      "    val_log_marginal: -12152.968988531642\n",
      "Train Epoch: 3087 [256/118836 (0%)] Loss: 12278.176758\n",
      "Train Epoch: 3087 [33024/118836 (28%)] Loss: 12207.523438\n",
      "Train Epoch: 3087 [65792/118836 (55%)] Loss: 12194.593750\n",
      "Train Epoch: 3087 [98560/118836 (83%)] Loss: 12156.342773\n",
      "    epoch          : 3087\n",
      "    loss           : 12224.049617129342\n",
      "    val_loss       : 12222.478040640517\n",
      "    val_log_likelihood: -12142.309549472706\n",
      "    val_log_marginal: -12150.465381049082\n",
      "Train Epoch: 3088 [256/118836 (0%)] Loss: 12246.050781\n",
      "Train Epoch: 3088 [33024/118836 (28%)] Loss: 12266.450195\n",
      "Train Epoch: 3088 [65792/118836 (55%)] Loss: 12205.541992\n",
      "Train Epoch: 3088 [98560/118836 (83%)] Loss: 12148.354492\n",
      "    epoch          : 3088\n",
      "    loss           : 12222.758985182743\n",
      "    val_loss       : 12223.52581119524\n",
      "    val_log_likelihood: -12143.008144159687\n",
      "    val_log_marginal: -12151.38068627128\n",
      "Train Epoch: 3089 [256/118836 (0%)] Loss: 12170.088867\n",
      "Train Epoch: 3089 [33024/118836 (28%)] Loss: 12184.556641\n",
      "Train Epoch: 3089 [65792/118836 (55%)] Loss: 12319.695312\n",
      "Train Epoch: 3089 [98560/118836 (83%)] Loss: 12222.067383\n",
      "    epoch          : 3089\n",
      "    loss           : 12216.911729896869\n",
      "    val_loss       : 12223.18380907519\n",
      "    val_log_likelihood: -12143.534650602254\n",
      "    val_log_marginal: -12151.726946653569\n",
      "Train Epoch: 3090 [256/118836 (0%)] Loss: 12240.879883\n",
      "Train Epoch: 3090 [33024/118836 (28%)] Loss: 12304.832031\n",
      "Train Epoch: 3090 [65792/118836 (55%)] Loss: 12246.655273\n",
      "Train Epoch: 3090 [98560/118836 (83%)] Loss: 12285.539062\n",
      "    epoch          : 3090\n",
      "    loss           : 12221.375118253722\n",
      "    val_loss       : 12220.102652870777\n",
      "    val_log_likelihood: -12144.079256810897\n",
      "    val_log_marginal: -12152.43784908904\n",
      "Train Epoch: 3091 [256/118836 (0%)] Loss: 12219.750977\n",
      "Train Epoch: 3091 [33024/118836 (28%)] Loss: 12211.005859\n",
      "Train Epoch: 3091 [65792/118836 (55%)] Loss: 12246.435547\n",
      "Train Epoch: 3091 [98560/118836 (83%)] Loss: 12313.693359\n",
      "    epoch          : 3091\n",
      "    loss           : 12222.249461396299\n",
      "    val_loss       : 12224.761749134936\n",
      "    val_log_likelihood: -12143.316824499843\n",
      "    val_log_marginal: -12151.56750804975\n",
      "Train Epoch: 3092 [256/118836 (0%)] Loss: 12189.045898\n",
      "Train Epoch: 3092 [33024/118836 (28%)] Loss: 12207.692383\n",
      "Train Epoch: 3092 [65792/118836 (55%)] Loss: 12236.804688\n",
      "Train Epoch: 3092 [98560/118836 (83%)] Loss: 12295.804688\n",
      "    epoch          : 3092\n",
      "    loss           : 12222.048235564\n",
      "    val_loss       : 12222.992353814889\n",
      "    val_log_likelihood: -12142.837155577958\n",
      "    val_log_marginal: -12151.167281116117\n",
      "Train Epoch: 3093 [256/118836 (0%)] Loss: 12196.187500\n",
      "Train Epoch: 3093 [33024/118836 (28%)] Loss: 12167.427734\n",
      "Train Epoch: 3093 [65792/118836 (55%)] Loss: 12317.529297\n",
      "Train Epoch: 3093 [98560/118836 (83%)] Loss: 12263.546875\n",
      "    epoch          : 3093\n",
      "    loss           : 12222.657011379499\n",
      "    val_loss       : 12219.268542134845\n",
      "    val_log_likelihood: -12142.36187900641\n",
      "    val_log_marginal: -12150.612355884545\n",
      "Train Epoch: 3094 [256/118836 (0%)] Loss: 12308.903320\n",
      "Train Epoch: 3094 [33024/118836 (28%)] Loss: 12243.576172\n",
      "Train Epoch: 3094 [65792/118836 (55%)] Loss: 12185.902344\n",
      "Train Epoch: 3094 [98560/118836 (83%)] Loss: 12333.728516\n",
      "    epoch          : 3094\n",
      "    loss           : 12218.82660821831\n",
      "    val_loss       : 12217.562683324706\n",
      "    val_log_likelihood: -12143.15822735732\n",
      "    val_log_marginal: -12151.408690544911\n",
      "Train Epoch: 3095 [256/118836 (0%)] Loss: 12313.676758\n",
      "Train Epoch: 3095 [33024/118836 (28%)] Loss: 12248.404297\n",
      "Train Epoch: 3095 [65792/118836 (55%)] Loss: 12344.077148\n",
      "Train Epoch: 3095 [98560/118836 (83%)] Loss: 12228.113281\n",
      "    epoch          : 3095\n",
      "    loss           : 12223.883900046527\n",
      "    val_loss       : 12219.370251437027\n",
      "    val_log_likelihood: -12143.348765282517\n",
      "    val_log_marginal: -12151.744007386224\n",
      "Train Epoch: 3096 [256/118836 (0%)] Loss: 12180.086914\n",
      "Train Epoch: 3096 [33024/118836 (28%)] Loss: 12211.478516\n",
      "Train Epoch: 3096 [65792/118836 (55%)] Loss: 12182.181641\n",
      "Train Epoch: 3096 [98560/118836 (83%)] Loss: 12237.761719\n",
      "    epoch          : 3096\n",
      "    loss           : 12220.847690982993\n",
      "    val_loss       : 12220.792481186398\n",
      "    val_log_likelihood: -12143.31359643171\n",
      "    val_log_marginal: -12151.700162921175\n",
      "Train Epoch: 3097 [256/118836 (0%)] Loss: 12329.044922\n",
      "Train Epoch: 3097 [33024/118836 (28%)] Loss: 12283.095703\n",
      "Train Epoch: 3097 [65792/118836 (55%)] Loss: 12202.643555\n",
      "Train Epoch: 3097 [98560/118836 (83%)] Loss: 12197.004883\n",
      "    epoch          : 3097\n",
      "    loss           : 12221.645026235525\n",
      "    val_loss       : 12221.305013401401\n",
      "    val_log_likelihood: -12142.841369739972\n",
      "    val_log_marginal: -12151.170520477279\n",
      "Train Epoch: 3098 [256/118836 (0%)] Loss: 12218.386719\n",
      "Train Epoch: 3098 [33024/118836 (28%)] Loss: 12258.169922\n",
      "Train Epoch: 3098 [65792/118836 (55%)] Loss: 12214.112305\n",
      "Train Epoch: 3098 [98560/118836 (83%)] Loss: 12228.365234\n",
      "    epoch          : 3098\n",
      "    loss           : 12218.364593187809\n",
      "    val_loss       : 12222.143217053304\n",
      "    val_log_likelihood: -12143.372850431659\n",
      "    val_log_marginal: -12151.728215554645\n",
      "Train Epoch: 3099 [256/118836 (0%)] Loss: 12190.003906\n",
      "Train Epoch: 3099 [33024/118836 (28%)] Loss: 12373.037109\n",
      "Train Epoch: 3099 [65792/118836 (55%)] Loss: 12316.745117\n",
      "Train Epoch: 3099 [98560/118836 (83%)] Loss: 12366.146484\n",
      "    epoch          : 3099\n",
      "    loss           : 12216.812579805108\n",
      "    val_loss       : 12219.01683168018\n",
      "    val_log_likelihood: -12142.351800461383\n",
      "    val_log_marginal: -12150.619309393047\n",
      "Train Epoch: 3100 [256/118836 (0%)] Loss: 12353.891602\n",
      "Train Epoch: 3100 [33024/118836 (28%)] Loss: 12311.976562\n",
      "Train Epoch: 3100 [65792/118836 (55%)] Loss: 12249.336914\n",
      "Train Epoch: 3100 [98560/118836 (83%)] Loss: 12202.473633\n",
      "    epoch          : 3100\n",
      "    loss           : 12218.980071986147\n",
      "    val_loss       : 12220.625996700677\n",
      "    val_log_likelihood: -12141.50372579999\n",
      "    val_log_marginal: -12149.926188171119\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch3100.pth ...\n",
      "Train Epoch: 3101 [256/118836 (0%)] Loss: 12250.304688\n",
      "Train Epoch: 3101 [33024/118836 (28%)] Loss: 12155.284180\n",
      "Train Epoch: 3101 [65792/118836 (55%)] Loss: 12264.844727\n",
      "Train Epoch: 3101 [98560/118836 (83%)] Loss: 12325.684570\n",
      "    epoch          : 3101\n",
      "    loss           : 12220.565439380429\n",
      "    val_loss       : 12222.801672611142\n",
      "    val_log_likelihood: -12142.767080554695\n",
      "    val_log_marginal: -12151.03797027179\n",
      "Train Epoch: 3102 [256/118836 (0%)] Loss: 12231.921875\n",
      "Train Epoch: 3102 [33024/118836 (28%)] Loss: 12244.275391\n",
      "Train Epoch: 3102 [65792/118836 (55%)] Loss: 12222.616211\n",
      "Train Epoch: 3102 [98560/118836 (83%)] Loss: 12207.107422\n",
      "    epoch          : 3102\n",
      "    loss           : 12217.974931664858\n",
      "    val_loss       : 12217.170108722758\n",
      "    val_log_likelihood: -12144.88133335918\n",
      "    val_log_marginal: -12153.03540961361\n",
      "Train Epoch: 3103 [256/118836 (0%)] Loss: 12251.781250\n",
      "Train Epoch: 3103 [33024/118836 (28%)] Loss: 12209.924805\n",
      "Train Epoch: 3103 [65792/118836 (55%)] Loss: 12318.683594\n",
      "Train Epoch: 3103 [98560/118836 (83%)] Loss: 12181.194336\n",
      "    epoch          : 3103\n",
      "    loss           : 12219.683167907362\n",
      "    val_loss       : 12219.581632440799\n",
      "    val_log_likelihood: -12144.89936139759\n",
      "    val_log_marginal: -12153.08156455167\n",
      "Train Epoch: 3104 [256/118836 (0%)] Loss: 12234.724609\n",
      "Train Epoch: 3104 [33024/118836 (28%)] Loss: 12183.378906\n",
      "Train Epoch: 3104 [65792/118836 (55%)] Loss: 12257.618164\n",
      "Train Epoch: 3104 [98560/118836 (83%)] Loss: 12300.051758\n",
      "    epoch          : 3104\n",
      "    loss           : 12219.14611798232\n",
      "    val_loss       : 12217.541180519702\n",
      "    val_log_likelihood: -12142.204705755013\n",
      "    val_log_marginal: -12150.423840199042\n",
      "Train Epoch: 3105 [256/118836 (0%)] Loss: 12248.124023\n",
      "Train Epoch: 3105 [33024/118836 (28%)] Loss: 12209.672852\n",
      "Train Epoch: 3105 [65792/118836 (55%)] Loss: 12255.147461\n",
      "Train Epoch: 3105 [98560/118836 (83%)] Loss: 12281.019531\n",
      "    epoch          : 3105\n",
      "    loss           : 12221.78904359879\n",
      "    val_loss       : 12220.378200682047\n",
      "    val_log_likelihood: -12142.3481074558\n",
      "    val_log_marginal: -12150.572957755025\n",
      "Train Epoch: 3106 [256/118836 (0%)] Loss: 12260.505859\n",
      "Train Epoch: 3106 [33024/118836 (28%)] Loss: 12219.845703\n",
      "Train Epoch: 3106 [65792/118836 (55%)] Loss: 12264.605469\n",
      "Train Epoch: 3106 [98560/118836 (83%)] Loss: 12237.681641\n",
      "    epoch          : 3106\n",
      "    loss           : 12220.116440659893\n",
      "    val_loss       : 12223.319027303025\n",
      "    val_log_likelihood: -12142.048252041976\n",
      "    val_log_marginal: -12150.295268905827\n",
      "Train Epoch: 3107 [256/118836 (0%)] Loss: 12193.280273\n",
      "Train Epoch: 3107 [33024/118836 (28%)] Loss: 12150.795898\n",
      "Train Epoch: 3107 [65792/118836 (55%)] Loss: 12148.224609\n",
      "Train Epoch: 3107 [98560/118836 (83%)] Loss: 12429.746094\n",
      "    epoch          : 3107\n",
      "    loss           : 12219.803188004033\n",
      "    val_loss       : 12220.51651247152\n",
      "    val_log_likelihood: -12144.504779582818\n",
      "    val_log_marginal: -12152.955722034349\n",
      "Train Epoch: 3108 [256/118836 (0%)] Loss: 12244.802734\n",
      "Train Epoch: 3108 [33024/118836 (28%)] Loss: 12250.153320\n",
      "Train Epoch: 3108 [65792/118836 (55%)] Loss: 12157.976562\n",
      "Train Epoch: 3108 [98560/118836 (83%)] Loss: 12343.373047\n",
      "    epoch          : 3108\n",
      "    loss           : 12219.42547673051\n",
      "    val_loss       : 12217.438714302629\n",
      "    val_log_likelihood: -12144.07458498113\n",
      "    val_log_marginal: -12152.280566871528\n",
      "Train Epoch: 3109 [256/118836 (0%)] Loss: 12320.551758\n",
      "Train Epoch: 3109 [33024/118836 (28%)] Loss: 12185.766602\n",
      "Train Epoch: 3109 [65792/118836 (55%)] Loss: 12290.867188\n",
      "Train Epoch: 3109 [98560/118836 (83%)] Loss: 12293.382812\n",
      "    epoch          : 3109\n",
      "    loss           : 12220.659327504653\n",
      "    val_loss       : 12222.234956406224\n",
      "    val_log_likelihood: -12143.365281547249\n",
      "    val_log_marginal: -12151.771447092433\n",
      "Train Epoch: 3110 [256/118836 (0%)] Loss: 12324.791016\n",
      "Train Epoch: 3110 [33024/118836 (28%)] Loss: 12267.327148\n",
      "Train Epoch: 3110 [65792/118836 (55%)] Loss: 12228.030273\n",
      "Train Epoch: 3110 [98560/118836 (83%)] Loss: 12244.818359\n",
      "    epoch          : 3110\n",
      "    loss           : 12217.937339743588\n",
      "    val_loss       : 12218.743126603342\n",
      "    val_log_likelihood: -12144.021496491161\n",
      "    val_log_marginal: -12152.307538756964\n",
      "Train Epoch: 3111 [256/118836 (0%)] Loss: 12188.846680\n",
      "Train Epoch: 3111 [33024/118836 (28%)] Loss: 12210.054688\n",
      "Train Epoch: 3111 [65792/118836 (55%)] Loss: 12147.497070\n",
      "Train Epoch: 3111 [98560/118836 (83%)] Loss: 12182.395508\n",
      "    epoch          : 3111\n",
      "    loss           : 12221.008585834108\n",
      "    val_loss       : 12221.685119532014\n",
      "    val_log_likelihood: -12145.615958436725\n",
      "    val_log_marginal: -12154.028168208342\n",
      "Train Epoch: 3112 [256/118836 (0%)] Loss: 12254.498047\n",
      "Train Epoch: 3112 [33024/118836 (28%)] Loss: 12177.264648\n",
      "Train Epoch: 3112 [65792/118836 (55%)] Loss: 12216.302734\n",
      "Train Epoch: 3112 [98560/118836 (83%)] Loss: 12239.476562\n",
      "    epoch          : 3112\n",
      "    loss           : 12220.276648605512\n",
      "    val_loss       : 12221.515496472965\n",
      "    val_log_likelihood: -12142.665887032155\n",
      "    val_log_marginal: -12150.91887798437\n",
      "Train Epoch: 3113 [256/118836 (0%)] Loss: 12283.187500\n",
      "Train Epoch: 3113 [33024/118836 (28%)] Loss: 12090.460938\n",
      "Train Epoch: 3113 [65792/118836 (55%)] Loss: 12230.704102\n",
      "Train Epoch: 3113 [98560/118836 (83%)] Loss: 12225.271484\n",
      "    epoch          : 3113\n",
      "    loss           : 12217.947004885236\n",
      "    val_loss       : 12221.283971219542\n",
      "    val_log_likelihood: -12143.883695848843\n",
      "    val_log_marginal: -12152.056462335811\n",
      "Train Epoch: 3114 [256/118836 (0%)] Loss: 12179.897461\n",
      "Train Epoch: 3114 [33024/118836 (28%)] Loss: 12272.845703\n",
      "Train Epoch: 3114 [65792/118836 (55%)] Loss: 12310.601562\n",
      "Train Epoch: 3114 [98560/118836 (83%)] Loss: 12292.579102\n",
      "    epoch          : 3114\n",
      "    loss           : 12216.655300739247\n",
      "    val_loss       : 12216.712372696846\n",
      "    val_log_likelihood: -12142.315829197683\n",
      "    val_log_marginal: -12150.851647657139\n",
      "Train Epoch: 3115 [256/118836 (0%)] Loss: 12230.562500\n",
      "Train Epoch: 3115 [33024/118836 (28%)] Loss: 12216.363281\n",
      "Train Epoch: 3115 [65792/118836 (55%)] Loss: 12197.166992\n",
      "Train Epoch: 3115 [98560/118836 (83%)] Loss: 12170.439453\n",
      "    epoch          : 3115\n",
      "    loss           : 12223.031432227048\n",
      "    val_loss       : 12222.432835285501\n",
      "    val_log_likelihood: -12143.95467409145\n",
      "    val_log_marginal: -12152.34141093839\n",
      "Train Epoch: 3116 [256/118836 (0%)] Loss: 12255.142578\n",
      "Train Epoch: 3116 [33024/118836 (28%)] Loss: 12201.212891\n",
      "Train Epoch: 3116 [65792/118836 (55%)] Loss: 12201.661133\n",
      "Train Epoch: 3116 [98560/118836 (83%)] Loss: 12245.058594\n",
      "    epoch          : 3116\n",
      "    loss           : 12221.237128437759\n",
      "    val_loss       : 12221.408957087671\n",
      "    val_log_likelihood: -12143.599396130583\n",
      "    val_log_marginal: -12151.864688428077\n",
      "Train Epoch: 3117 [256/118836 (0%)] Loss: 12226.880859\n",
      "Train Epoch: 3117 [33024/118836 (28%)] Loss: 12323.851562\n",
      "Train Epoch: 3117 [65792/118836 (55%)] Loss: 12201.021484\n",
      "Train Epoch: 3117 [98560/118836 (83%)] Loss: 12173.868164\n",
      "    epoch          : 3117\n",
      "    loss           : 12221.62257806038\n",
      "    val_loss       : 12221.308340651083\n",
      "    val_log_likelihood: -12144.438599178038\n",
      "    val_log_marginal: -12152.95065706798\n",
      "Train Epoch: 3118 [256/118836 (0%)] Loss: 12219.437500\n",
      "Train Epoch: 3118 [33024/118836 (28%)] Loss: 12170.315430\n",
      "Train Epoch: 3118 [65792/118836 (55%)] Loss: 12256.105469\n",
      "Train Epoch: 3118 [98560/118836 (83%)] Loss: 12204.203125\n",
      "    epoch          : 3118\n",
      "    loss           : 12218.464373481442\n",
      "    val_loss       : 12221.397409669627\n",
      "    val_log_likelihood: -12141.846950443289\n",
      "    val_log_marginal: -12150.18991610703\n",
      "Train Epoch: 3119 [256/118836 (0%)] Loss: 12201.648438\n",
      "Train Epoch: 3119 [33024/118836 (28%)] Loss: 12174.419922\n",
      "Train Epoch: 3119 [65792/118836 (55%)] Loss: 12213.783203\n",
      "Train Epoch: 3119 [98560/118836 (83%)] Loss: 12207.309570\n",
      "    epoch          : 3119\n",
      "    loss           : 12218.20315052471\n",
      "    val_loss       : 12216.108075896405\n",
      "    val_log_likelihood: -12144.291797036549\n",
      "    val_log_marginal: -12152.50558246275\n",
      "Train Epoch: 3120 [256/118836 (0%)] Loss: 12290.782227\n",
      "Train Epoch: 3120 [33024/118836 (28%)] Loss: 12188.158203\n",
      "Train Epoch: 3120 [65792/118836 (55%)] Loss: 12260.742188\n",
      "Train Epoch: 3120 [98560/118836 (83%)] Loss: 12209.472656\n",
      "    epoch          : 3120\n",
      "    loss           : 12218.936721980976\n",
      "    val_loss       : 12218.898162865762\n",
      "    val_log_likelihood: -12144.180144036909\n",
      "    val_log_marginal: -12152.468464146545\n",
      "Train Epoch: 3121 [256/118836 (0%)] Loss: 12159.678711\n",
      "Train Epoch: 3121 [33024/118836 (28%)] Loss: 12215.440430\n",
      "Train Epoch: 3121 [65792/118836 (55%)] Loss: 12301.837891\n",
      "Train Epoch: 3121 [98560/118836 (83%)] Loss: 12237.689453\n",
      "    epoch          : 3121\n",
      "    loss           : 12220.26073701794\n",
      "    val_loss       : 12218.968948353398\n",
      "    val_log_likelihood: -12145.232582292958\n",
      "    val_log_marginal: -12153.484497409216\n",
      "Train Epoch: 3122 [256/118836 (0%)] Loss: 12241.990234\n",
      "Train Epoch: 3122 [33024/118836 (28%)] Loss: 12154.918945\n",
      "Train Epoch: 3122 [65792/118836 (55%)] Loss: 12236.660156\n",
      "Train Epoch: 3122 [98560/118836 (83%)] Loss: 12241.887695\n",
      "    epoch          : 3122\n",
      "    loss           : 12217.582410081937\n",
      "    val_loss       : 12222.017588752607\n",
      "    val_log_likelihood: -12141.28975344422\n",
      "    val_log_marginal: -12149.844833083775\n",
      "Train Epoch: 3123 [256/118836 (0%)] Loss: 12293.072266\n",
      "Train Epoch: 3123 [33024/118836 (28%)] Loss: 12223.037109\n",
      "Train Epoch: 3123 [65792/118836 (55%)] Loss: 12391.415039\n",
      "Train Epoch: 3123 [98560/118836 (83%)] Loss: 12216.640625\n",
      "    epoch          : 3123\n",
      "    loss           : 12221.092550338606\n",
      "    val_loss       : 12220.800787312372\n",
      "    val_log_likelihood: -12145.92559272255\n",
      "    val_log_marginal: -12154.234257766033\n",
      "Train Epoch: 3124 [256/118836 (0%)] Loss: 12272.934570\n",
      "Train Epoch: 3124 [33024/118836 (28%)] Loss: 12244.219727\n",
      "Train Epoch: 3124 [65792/118836 (55%)] Loss: 12197.185547\n",
      "Train Epoch: 3124 [98560/118836 (83%)] Loss: 12328.431641\n",
      "    epoch          : 3124\n",
      "    loss           : 12222.371988730354\n",
      "    val_loss       : 12214.264252901743\n",
      "    val_log_likelihood: -12143.936626990282\n",
      "    val_log_marginal: -12152.214656820694\n",
      "Train Epoch: 3125 [256/118836 (0%)] Loss: 12193.168945\n",
      "Train Epoch: 3125 [33024/118836 (28%)] Loss: 12220.797852\n",
      "Train Epoch: 3125 [65792/118836 (55%)] Loss: 12273.554688\n",
      "Train Epoch: 3125 [98560/118836 (83%)] Loss: 12173.359375\n",
      "    epoch          : 3125\n",
      "    loss           : 12223.820524613575\n",
      "    val_loss       : 12218.758997895977\n",
      "    val_log_likelihood: -12145.079523527967\n",
      "    val_log_marginal: -12153.2588489322\n",
      "Train Epoch: 3126 [256/118836 (0%)] Loss: 12151.293945\n",
      "Train Epoch: 3126 [33024/118836 (28%)] Loss: 12224.320312\n",
      "Train Epoch: 3126 [65792/118836 (55%)] Loss: 12243.364258\n",
      "Train Epoch: 3126 [98560/118836 (83%)] Loss: 12158.930664\n",
      "    epoch          : 3126\n",
      "    loss           : 12222.095479056814\n",
      "    val_loss       : 12220.064113269771\n",
      "    val_log_likelihood: -12142.648872227823\n",
      "    val_log_marginal: -12150.91331668\n",
      "Train Epoch: 3127 [256/118836 (0%)] Loss: 12259.279297\n",
      "Train Epoch: 3127 [33024/118836 (28%)] Loss: 12189.158203\n",
      "Train Epoch: 3127 [65792/118836 (55%)] Loss: 12232.644531\n",
      "Train Epoch: 3127 [98560/118836 (83%)] Loss: 12162.358398\n",
      "    epoch          : 3127\n",
      "    loss           : 12216.479313514526\n",
      "    val_loss       : 12218.651121119938\n",
      "    val_log_likelihood: -12144.304225147333\n",
      "    val_log_marginal: -12152.543451415218\n",
      "Train Epoch: 3128 [256/118836 (0%)] Loss: 12225.767578\n",
      "Train Epoch: 3128 [33024/118836 (28%)] Loss: 12309.722656\n",
      "Train Epoch: 3128 [65792/118836 (55%)] Loss: 12254.161133\n",
      "Train Epoch: 3128 [98560/118836 (83%)] Loss: 12256.163086\n",
      "    epoch          : 3128\n",
      "    loss           : 12220.996445118642\n",
      "    val_loss       : 12222.857801480564\n",
      "    val_log_likelihood: -12143.08557126887\n",
      "    val_log_marginal: -12151.234560143834\n",
      "Train Epoch: 3129 [256/118836 (0%)] Loss: 12163.779297\n",
      "Train Epoch: 3129 [33024/118836 (28%)] Loss: 12178.364258\n",
      "Train Epoch: 3129 [65792/118836 (55%)] Loss: 12223.896484\n",
      "Train Epoch: 3129 [98560/118836 (83%)] Loss: 12333.644531\n",
      "    epoch          : 3129\n",
      "    loss           : 12219.025547488885\n",
      "    val_loss       : 12221.443337757233\n",
      "    val_log_likelihood: -12141.271367575217\n",
      "    val_log_marginal: -12149.512717418802\n",
      "Train Epoch: 3130 [256/118836 (0%)] Loss: 12356.996094\n",
      "Train Epoch: 3130 [33024/118836 (28%)] Loss: 12267.674805\n",
      "Train Epoch: 3130 [65792/118836 (55%)] Loss: 12267.775391\n",
      "Train Epoch: 3130 [98560/118836 (83%)] Loss: 12147.283203\n",
      "    epoch          : 3130\n",
      "    loss           : 12218.111391936774\n",
      "    val_loss       : 12220.319400197128\n",
      "    val_log_likelihood: -12143.764586887406\n",
      "    val_log_marginal: -12151.952417084278\n",
      "Train Epoch: 3131 [256/118836 (0%)] Loss: 12326.359375\n",
      "Train Epoch: 3131 [33024/118836 (28%)] Loss: 12245.045898\n",
      "Train Epoch: 3131 [65792/118836 (55%)] Loss: 12168.642578\n",
      "Train Epoch: 3131 [98560/118836 (83%)] Loss: 12222.105469\n",
      "    epoch          : 3131\n",
      "    loss           : 12226.133182769852\n",
      "    val_loss       : 12222.620552964312\n",
      "    val_log_likelihood: -12143.077042623036\n",
      "    val_log_marginal: -12151.620107926306\n",
      "Train Epoch: 3132 [256/118836 (0%)] Loss: 12278.912109\n",
      "Train Epoch: 3132 [33024/118836 (28%)] Loss: 12237.667969\n",
      "Train Epoch: 3132 [65792/118836 (55%)] Loss: 12234.173828\n",
      "Train Epoch: 3132 [98560/118836 (83%)] Loss: 12201.722656\n",
      "    epoch          : 3132\n",
      "    loss           : 12219.243014952957\n",
      "    val_loss       : 12219.309881260333\n",
      "    val_log_likelihood: -12146.258572910205\n",
      "    val_log_marginal: -12154.677882764146\n",
      "Train Epoch: 3133 [256/118836 (0%)] Loss: 12249.204102\n",
      "Train Epoch: 3133 [33024/118836 (28%)] Loss: 12426.414062\n",
      "Train Epoch: 3133 [65792/118836 (55%)] Loss: 12298.944336\n",
      "Train Epoch: 3133 [98560/118836 (83%)] Loss: 12203.716797\n",
      "    epoch          : 3133\n",
      "    loss           : 12221.525180934656\n",
      "    val_loss       : 12219.129394466789\n",
      "    val_log_likelihood: -12143.69312771402\n",
      "    val_log_marginal: -12152.08858683141\n",
      "Train Epoch: 3134 [256/118836 (0%)] Loss: 12267.431641\n",
      "Train Epoch: 3134 [33024/118836 (28%)] Loss: 12233.705078\n",
      "Train Epoch: 3134 [65792/118836 (55%)] Loss: 12313.462891\n",
      "Train Epoch: 3134 [98560/118836 (83%)] Loss: 12290.314453\n",
      "    epoch          : 3134\n",
      "    loss           : 12223.329618195565\n",
      "    val_loss       : 12227.011475571726\n",
      "    val_log_likelihood: -12142.41916873449\n",
      "    val_log_marginal: -12150.97585677983\n",
      "Train Epoch: 3135 [256/118836 (0%)] Loss: 12298.307617\n",
      "Train Epoch: 3135 [33024/118836 (28%)] Loss: 12196.619141\n",
      "Train Epoch: 3135 [65792/118836 (55%)] Loss: 12166.839844\n",
      "Train Epoch: 3135 [98560/118836 (83%)] Loss: 12285.129883\n",
      "    epoch          : 3135\n",
      "    loss           : 12217.522886941688\n",
      "    val_loss       : 12219.876779285023\n",
      "    val_log_likelihood: -12143.099460427007\n",
      "    val_log_marginal: -12151.401886766496\n",
      "Train Epoch: 3136 [256/118836 (0%)] Loss: 12301.241211\n",
      "Train Epoch: 3136 [33024/118836 (28%)] Loss: 12311.082031\n",
      "Train Epoch: 3136 [65792/118836 (55%)] Loss: 12166.210938\n",
      "Train Epoch: 3136 [98560/118836 (83%)] Loss: 12361.322266\n",
      "    epoch          : 3136\n",
      "    loss           : 12223.85157590855\n",
      "    val_loss       : 12221.490259196207\n",
      "    val_log_likelihood: -12143.495344809759\n",
      "    val_log_marginal: -12151.908666759931\n",
      "Train Epoch: 3137 [256/118836 (0%)] Loss: 12312.233398\n",
      "Train Epoch: 3137 [33024/118836 (28%)] Loss: 12274.038086\n",
      "Train Epoch: 3137 [65792/118836 (55%)] Loss: 12170.337891\n",
      "Train Epoch: 3137 [98560/118836 (83%)] Loss: 12389.634766\n",
      "    epoch          : 3137\n",
      "    loss           : 12222.909700036187\n",
      "    val_loss       : 12220.726208278478\n",
      "    val_log_likelihood: -12142.615671849153\n",
      "    val_log_marginal: -12150.852428010854\n",
      "Train Epoch: 3138 [256/118836 (0%)] Loss: 12262.427734\n",
      "Train Epoch: 3138 [33024/118836 (28%)] Loss: 12117.157227\n",
      "Train Epoch: 3138 [65792/118836 (55%)] Loss: 12157.836914\n",
      "Train Epoch: 3138 [98560/118836 (83%)] Loss: 12176.764648\n",
      "    epoch          : 3138\n",
      "    loss           : 12222.894708307485\n",
      "    val_loss       : 12219.484482727878\n",
      "    val_log_likelihood: -12143.901821139629\n",
      "    val_log_marginal: -12152.130664567083\n",
      "Train Epoch: 3139 [256/118836 (0%)] Loss: 12230.178711\n",
      "Train Epoch: 3139 [33024/118836 (28%)] Loss: 12257.250000\n",
      "Train Epoch: 3139 [65792/118836 (55%)] Loss: 12207.474609\n",
      "Train Epoch: 3139 [98560/118836 (83%)] Loss: 12302.005859\n",
      "    epoch          : 3139\n",
      "    loss           : 12221.941042280552\n",
      "    val_loss       : 12220.184519509481\n",
      "    val_log_likelihood: -12142.882384395678\n",
      "    val_log_marginal: -12151.045204105287\n",
      "Train Epoch: 3140 [256/118836 (0%)] Loss: 12239.954102\n",
      "Train Epoch: 3140 [33024/118836 (28%)] Loss: 12252.660156\n",
      "Train Epoch: 3140 [65792/118836 (55%)] Loss: 12167.058594\n",
      "Train Epoch: 3140 [98560/118836 (83%)] Loss: 12250.299805\n",
      "    epoch          : 3140\n",
      "    loss           : 12224.052648592586\n",
      "    val_loss       : 12216.724017358702\n",
      "    val_log_likelihood: -12142.402347142526\n",
      "    val_log_marginal: -12150.645263187891\n",
      "Train Epoch: 3141 [256/118836 (0%)] Loss: 12266.531250\n",
      "Train Epoch: 3141 [33024/118836 (28%)] Loss: 12224.257812\n",
      "Train Epoch: 3141 [65792/118836 (55%)] Loss: 12223.848633\n",
      "Train Epoch: 3141 [98560/118836 (83%)] Loss: 12264.373047\n",
      "    epoch          : 3141\n",
      "    loss           : 12218.789289960712\n",
      "    val_loss       : 12219.149752594289\n",
      "    val_log_likelihood: -12143.717328532102\n",
      "    val_log_marginal: -12152.046016635359\n",
      "Train Epoch: 3142 [256/118836 (0%)] Loss: 12166.175781\n",
      "Train Epoch: 3142 [33024/118836 (28%)] Loss: 12174.177734\n",
      "Train Epoch: 3142 [65792/118836 (55%)] Loss: 12297.236328\n",
      "Train Epoch: 3142 [98560/118836 (83%)] Loss: 12214.688477\n",
      "    epoch          : 3142\n",
      "    loss           : 12222.213484963038\n",
      "    val_loss       : 12222.699011581224\n",
      "    val_log_likelihood: -12141.633421215882\n",
      "    val_log_marginal: -12149.828717723753\n",
      "Train Epoch: 3143 [256/118836 (0%)] Loss: 12251.727539\n",
      "Train Epoch: 3143 [33024/118836 (28%)] Loss: 12272.597656\n",
      "Train Epoch: 3143 [65792/118836 (55%)] Loss: 12300.868164\n",
      "Train Epoch: 3143 [98560/118836 (83%)] Loss: 12144.888672\n",
      "    epoch          : 3143\n",
      "    loss           : 12216.67829898806\n",
      "    val_loss       : 12220.563820012092\n",
      "    val_log_likelihood: -12143.501792707042\n",
      "    val_log_marginal: -12151.66488114717\n",
      "Train Epoch: 3144 [256/118836 (0%)] Loss: 12389.030273\n",
      "Train Epoch: 3144 [33024/118836 (28%)] Loss: 12216.386719\n",
      "Train Epoch: 3144 [65792/118836 (55%)] Loss: 12247.048828\n",
      "Train Epoch: 3144 [98560/118836 (83%)] Loss: 12170.875977\n",
      "    epoch          : 3144\n",
      "    loss           : 12214.402132605717\n",
      "    val_loss       : 12222.021282652024\n",
      "    val_log_likelihood: -12141.401207900382\n",
      "    val_log_marginal: -12149.710611705894\n",
      "Train Epoch: 3145 [256/118836 (0%)] Loss: 12196.378906\n",
      "Train Epoch: 3145 [33024/118836 (28%)] Loss: 12239.311523\n",
      "Train Epoch: 3145 [65792/118836 (55%)] Loss: 12176.087891\n",
      "Train Epoch: 3145 [98560/118836 (83%)] Loss: 12160.757812\n",
      "    epoch          : 3145\n",
      "    loss           : 12217.274773993227\n",
      "    val_loss       : 12217.279605286116\n",
      "    val_log_likelihood: -12142.67172944453\n",
      "    val_log_marginal: -12150.912247870348\n",
      "Train Epoch: 3146 [256/118836 (0%)] Loss: 12341.698242\n",
      "Train Epoch: 3146 [33024/118836 (28%)] Loss: 12319.238281\n",
      "Train Epoch: 3146 [65792/118836 (55%)] Loss: 12278.414062\n",
      "Train Epoch: 3146 [98560/118836 (83%)] Loss: 12223.220703\n",
      "    epoch          : 3146\n",
      "    loss           : 12222.598278212881\n",
      "    val_loss       : 12223.224572785748\n",
      "    val_log_likelihood: -12141.455133859337\n",
      "    val_log_marginal: -12149.767942391809\n",
      "Train Epoch: 3147 [256/118836 (0%)] Loss: 12250.175781\n",
      "Train Epoch: 3147 [33024/118836 (28%)] Loss: 12243.636719\n",
      "Train Epoch: 3147 [65792/118836 (55%)] Loss: 12234.618164\n",
      "Train Epoch: 3147 [98560/118836 (83%)] Loss: 12175.720703\n",
      "    epoch          : 3147\n",
      "    loss           : 12219.526962010184\n",
      "    val_loss       : 12216.196743632876\n",
      "    val_log_likelihood: -12143.029229832247\n",
      "    val_log_marginal: -12151.284743005634\n",
      "Train Epoch: 3148 [256/118836 (0%)] Loss: 12320.137695\n",
      "Train Epoch: 3148 [33024/118836 (28%)] Loss: 12287.101562\n",
      "Train Epoch: 3148 [65792/118836 (55%)] Loss: 12191.311523\n",
      "Train Epoch: 3148 [98560/118836 (83%)] Loss: 12341.396484\n",
      "    epoch          : 3148\n",
      "    loss           : 12220.737322780966\n",
      "    val_loss       : 12218.884417074893\n",
      "    val_log_likelihood: -12143.693487321649\n",
      "    val_log_marginal: -12151.954264172206\n",
      "Train Epoch: 3149 [256/118836 (0%)] Loss: 12360.513672\n",
      "Train Epoch: 3149 [33024/118836 (28%)] Loss: 12172.864258\n",
      "Train Epoch: 3149 [65792/118836 (55%)] Loss: 12302.933594\n",
      "Train Epoch: 3149 [98560/118836 (83%)] Loss: 12243.433594\n",
      "    epoch          : 3149\n",
      "    loss           : 12223.240467651469\n",
      "    val_loss       : 12217.706503047262\n",
      "    val_log_likelihood: -12142.247273379342\n",
      "    val_log_marginal: -12150.42312169701\n",
      "Train Epoch: 3150 [256/118836 (0%)] Loss: 12163.041992\n",
      "Train Epoch: 3150 [33024/118836 (28%)] Loss: 12195.381836\n",
      "Train Epoch: 3150 [65792/118836 (55%)] Loss: 12169.214844\n",
      "Train Epoch: 3150 [98560/118836 (83%)] Loss: 12177.615234\n",
      "    epoch          : 3150\n",
      "    loss           : 12217.070115895109\n",
      "    val_loss       : 12221.836470280336\n",
      "    val_log_likelihood: -12142.175603546319\n",
      "    val_log_marginal: -12150.34000494428\n",
      "Train Epoch: 3151 [256/118836 (0%)] Loss: 12206.541016\n",
      "Train Epoch: 3151 [33024/118836 (28%)] Loss: 12185.987305\n",
      "Train Epoch: 3151 [65792/118836 (55%)] Loss: 12150.658203\n",
      "Train Epoch: 3151 [98560/118836 (83%)] Loss: 12156.380859\n",
      "    epoch          : 3151\n",
      "    loss           : 12219.65757276158\n",
      "    val_loss       : 12220.07545401723\n",
      "    val_log_likelihood: -12143.849796125412\n",
      "    val_log_marginal: -12152.207158724712\n",
      "Train Epoch: 3152 [256/118836 (0%)] Loss: 12188.358398\n",
      "Train Epoch: 3152 [33024/118836 (28%)] Loss: 12242.655273\n",
      "Train Epoch: 3152 [65792/118836 (55%)] Loss: 12285.730469\n",
      "Train Epoch: 3152 [98560/118836 (83%)] Loss: 12282.804688\n",
      "    epoch          : 3152\n",
      "    loss           : 12223.227781062604\n",
      "    val_loss       : 12222.76063075494\n",
      "    val_log_likelihood: -12141.388603539857\n",
      "    val_log_marginal: -12149.577470354348\n",
      "Train Epoch: 3153 [256/118836 (0%)] Loss: 12257.633789\n",
      "Train Epoch: 3153 [33024/118836 (28%)] Loss: 12180.936523\n",
      "Train Epoch: 3153 [65792/118836 (55%)] Loss: 12222.617188\n",
      "Train Epoch: 3153 [98560/118836 (83%)] Loss: 12177.550781\n",
      "    epoch          : 3153\n",
      "    loss           : 12219.89662443781\n",
      "    val_loss       : 12222.543406120734\n",
      "    val_log_likelihood: -12142.27082040943\n",
      "    val_log_marginal: -12150.431096388776\n",
      "Train Epoch: 3154 [256/118836 (0%)] Loss: 12248.543945\n",
      "Train Epoch: 3154 [33024/118836 (28%)] Loss: 12240.050781\n",
      "Train Epoch: 3154 [65792/118836 (55%)] Loss: 12235.719727\n",
      "Train Epoch: 3154 [98560/118836 (83%)] Loss: 12218.156250\n",
      "    epoch          : 3154\n",
      "    loss           : 12219.432259518455\n",
      "    val_loss       : 12222.757446092499\n",
      "    val_log_likelihood: -12141.835505841604\n",
      "    val_log_marginal: -12150.154827468428\n",
      "Train Epoch: 3155 [256/118836 (0%)] Loss: 12222.158203\n",
      "Train Epoch: 3155 [33024/118836 (28%)] Loss: 12193.957031\n",
      "Train Epoch: 3155 [65792/118836 (55%)] Loss: 12193.136719\n",
      "Train Epoch: 3155 [98560/118836 (83%)] Loss: 12169.274414\n",
      "    epoch          : 3155\n",
      "    loss           : 12220.781001861043\n",
      "    val_loss       : 12216.410758264243\n",
      "    val_log_likelihood: -12141.449262206626\n",
      "    val_log_marginal: -12149.733675781035\n",
      "Train Epoch: 3156 [256/118836 (0%)] Loss: 12326.203125\n",
      "Train Epoch: 3156 [33024/118836 (28%)] Loss: 12209.993164\n",
      "Train Epoch: 3156 [65792/118836 (55%)] Loss: 12280.247070\n",
      "Train Epoch: 3156 [98560/118836 (83%)] Loss: 12314.632812\n",
      "    epoch          : 3156\n",
      "    loss           : 12218.628743085712\n",
      "    val_loss       : 12219.639639970545\n",
      "    val_log_likelihood: -12143.436984013131\n",
      "    val_log_marginal: -12151.71514982715\n",
      "Train Epoch: 3157 [256/118836 (0%)] Loss: 12200.310547\n",
      "Train Epoch: 3157 [33024/118836 (28%)] Loss: 12302.673828\n",
      "Train Epoch: 3157 [65792/118836 (55%)] Loss: 12167.085938\n",
      "Train Epoch: 3157 [98560/118836 (83%)] Loss: 12209.706055\n",
      "    epoch          : 3157\n",
      "    loss           : 12215.070463063481\n",
      "    val_loss       : 12216.941473139788\n",
      "    val_log_likelihood: -12143.201587540063\n",
      "    val_log_marginal: -12151.52571694751\n",
      "Train Epoch: 3158 [256/118836 (0%)] Loss: 12273.437500\n",
      "Train Epoch: 3158 [33024/118836 (28%)] Loss: 12164.799805\n",
      "Train Epoch: 3158 [65792/118836 (55%)] Loss: 12265.412109\n",
      "Train Epoch: 3158 [98560/118836 (83%)] Loss: 12287.652344\n",
      "    epoch          : 3158\n",
      "    loss           : 12219.756740946805\n",
      "    val_loss       : 12219.759395506733\n",
      "    val_log_likelihood: -12142.811300177056\n",
      "    val_log_marginal: -12151.340082683713\n",
      "Train Epoch: 3159 [256/118836 (0%)] Loss: 12178.006836\n",
      "Train Epoch: 3159 [33024/118836 (28%)] Loss: 12184.197266\n",
      "Train Epoch: 3159 [65792/118836 (55%)] Loss: 12182.390625\n",
      "Train Epoch: 3159 [98560/118836 (83%)] Loss: 12303.212891\n",
      "    epoch          : 3159\n",
      "    loss           : 12219.496796648831\n",
      "    val_loss       : 12217.226603877973\n",
      "    val_log_likelihood: -12142.066854063276\n",
      "    val_log_marginal: -12150.397879746874\n",
      "Train Epoch: 3160 [256/118836 (0%)] Loss: 12264.649414\n",
      "Train Epoch: 3160 [33024/118836 (28%)] Loss: 12194.366211\n",
      "Train Epoch: 3160 [65792/118836 (55%)] Loss: 12269.151367\n",
      "Train Epoch: 3160 [98560/118836 (83%)] Loss: 12309.447266\n",
      "    epoch          : 3160\n",
      "    loss           : 12222.832563553298\n",
      "    val_loss       : 12221.652473874525\n",
      "    val_log_likelihood: -12141.434301333746\n",
      "    val_log_marginal: -12149.635387997861\n",
      "Train Epoch: 3161 [256/118836 (0%)] Loss: 12202.830078\n",
      "Train Epoch: 3161 [33024/118836 (28%)] Loss: 12246.339844\n",
      "Train Epoch: 3161 [65792/118836 (55%)] Loss: 12288.356445\n",
      "Train Epoch: 3161 [98560/118836 (83%)] Loss: 12243.955078\n",
      "    epoch          : 3161\n",
      "    loss           : 12220.089990274762\n",
      "    val_loss       : 12222.177850279524\n",
      "    val_log_likelihood: -12141.93158650615\n",
      "    val_log_marginal: -12150.04632495586\n",
      "Train Epoch: 3162 [256/118836 (0%)] Loss: 12208.240234\n",
      "Train Epoch: 3162 [33024/118836 (28%)] Loss: 12223.687500\n",
      "Train Epoch: 3162 [65792/118836 (55%)] Loss: 12209.027344\n",
      "Train Epoch: 3162 [98560/118836 (83%)] Loss: 12139.389648\n",
      "    epoch          : 3162\n",
      "    loss           : 12218.35210821185\n",
      "    val_loss       : 12216.643111862646\n",
      "    val_log_likelihood: -12144.544162110991\n",
      "    val_log_marginal: -12152.814751901346\n",
      "Train Epoch: 3163 [256/118836 (0%)] Loss: 12197.798828\n",
      "Train Epoch: 3163 [33024/118836 (28%)] Loss: 12299.418945\n",
      "Train Epoch: 3163 [65792/118836 (55%)] Loss: 12190.058594\n",
      "Train Epoch: 3163 [98560/118836 (83%)] Loss: 12246.421875\n",
      "    epoch          : 3163\n",
      "    loss           : 12216.636738135856\n",
      "    val_loss       : 12218.118776150446\n",
      "    val_log_likelihood: -12141.008777430987\n",
      "    val_log_marginal: -12149.142226440923\n",
      "Train Epoch: 3164 [256/118836 (0%)] Loss: 12231.259766\n",
      "Train Epoch: 3164 [33024/118836 (28%)] Loss: 12345.580078\n",
      "Train Epoch: 3164 [65792/118836 (55%)] Loss: 12180.714844\n",
      "Train Epoch: 3164 [98560/118836 (83%)] Loss: 12239.790039\n",
      "    epoch          : 3164\n",
      "    loss           : 12223.949239266698\n",
      "    val_loss       : 12229.57245993292\n",
      "    val_log_likelihood: -12143.647081944015\n",
      "    val_log_marginal: -12151.963421777084\n",
      "Train Epoch: 3165 [256/118836 (0%)] Loss: 12257.776367\n",
      "Train Epoch: 3165 [33024/118836 (28%)] Loss: 12276.488281\n",
      "Train Epoch: 3165 [65792/118836 (55%)] Loss: 12264.849609\n",
      "Train Epoch: 3165 [98560/118836 (83%)] Loss: 12267.572266\n",
      "    epoch          : 3165\n",
      "    loss           : 12217.443457919768\n",
      "    val_loss       : 12222.18046559054\n",
      "    val_log_likelihood: -12141.602662162688\n",
      "    val_log_marginal: -12149.761054634568\n",
      "Train Epoch: 3166 [256/118836 (0%)] Loss: 12255.509766\n",
      "Train Epoch: 3166 [33024/118836 (28%)] Loss: 12373.532227\n",
      "Train Epoch: 3166 [65792/118836 (55%)] Loss: 12207.178711\n",
      "Train Epoch: 3166 [98560/118836 (83%)] Loss: 12233.287109\n",
      "    epoch          : 3166\n",
      "    loss           : 12218.685293081833\n",
      "    val_loss       : 12223.014061083719\n",
      "    val_log_likelihood: -12142.372778380892\n",
      "    val_log_marginal: -12150.68867949152\n",
      "Train Epoch: 3167 [256/118836 (0%)] Loss: 12192.205078\n",
      "Train Epoch: 3167 [33024/118836 (28%)] Loss: 12289.679688\n",
      "Train Epoch: 3167 [65792/118836 (55%)] Loss: 12265.072266\n",
      "Train Epoch: 3167 [98560/118836 (83%)] Loss: 12250.050781\n",
      "    epoch          : 3167\n",
      "    loss           : 12223.500022132186\n",
      "    val_loss       : 12219.247540539674\n",
      "    val_log_likelihood: -12141.745447393248\n",
      "    val_log_marginal: -12150.011177978378\n",
      "Train Epoch: 3168 [256/118836 (0%)] Loss: 12234.669922\n",
      "Train Epoch: 3168 [33024/118836 (28%)] Loss: 12234.546875\n",
      "Train Epoch: 3168 [65792/118836 (55%)] Loss: 12222.566406\n",
      "Train Epoch: 3168 [98560/118836 (83%)] Loss: 12170.975586\n",
      "    epoch          : 3168\n",
      "    loss           : 12221.727399645886\n",
      "    val_loss       : 12221.9386861329\n",
      "    val_log_likelihood: -12143.004756965986\n",
      "    val_log_marginal: -12151.150953427657\n",
      "Train Epoch: 3169 [256/118836 (0%)] Loss: 12180.078125\n",
      "Train Epoch: 3169 [33024/118836 (28%)] Loss: 12172.929688\n",
      "Train Epoch: 3169 [65792/118836 (55%)] Loss: 12273.987305\n",
      "Train Epoch: 3169 [98560/118836 (83%)] Loss: 12293.118164\n",
      "    epoch          : 3169\n",
      "    loss           : 12220.231124961228\n",
      "    val_loss       : 12221.92730588158\n",
      "    val_log_likelihood: -12143.136122311827\n",
      "    val_log_marginal: -12151.338597270289\n",
      "Train Epoch: 3170 [256/118836 (0%)] Loss: 12235.853516\n",
      "Train Epoch: 3170 [33024/118836 (28%)] Loss: 12200.493164\n",
      "Train Epoch: 3170 [65792/118836 (55%)] Loss: 12269.250000\n",
      "Train Epoch: 3170 [98560/118836 (83%)] Loss: 12276.767578\n",
      "    epoch          : 3170\n",
      "    loss           : 12221.102341488317\n",
      "    val_loss       : 12220.217904476369\n",
      "    val_log_likelihood: -12141.355219641748\n",
      "    val_log_marginal: -12149.500517824703\n",
      "Train Epoch: 3171 [256/118836 (0%)] Loss: 12248.827148\n",
      "Train Epoch: 3171 [33024/118836 (28%)] Loss: 12188.722656\n",
      "Train Epoch: 3171 [65792/118836 (55%)] Loss: 12310.314453\n",
      "Train Epoch: 3171 [98560/118836 (83%)] Loss: 12137.599609\n",
      "    epoch          : 3171\n",
      "    loss           : 12219.244534157877\n",
      "    val_loss       : 12220.612383988098\n",
      "    val_log_likelihood: -12143.880277476219\n",
      "    val_log_marginal: -12152.27582256766\n",
      "Train Epoch: 3172 [256/118836 (0%)] Loss: 12169.563477\n",
      "Train Epoch: 3172 [33024/118836 (28%)] Loss: 12222.144531\n",
      "Train Epoch: 3172 [65792/118836 (55%)] Loss: 12256.958984\n",
      "Train Epoch: 3172 [98560/118836 (83%)] Loss: 12250.643555\n",
      "    epoch          : 3172\n",
      "    loss           : 12220.893540309657\n",
      "    val_loss       : 12226.395763778808\n",
      "    val_log_likelihood: -12143.344416388803\n",
      "    val_log_marginal: -12151.896439121536\n",
      "Train Epoch: 3173 [256/118836 (0%)] Loss: 12222.042969\n",
      "Train Epoch: 3173 [33024/118836 (28%)] Loss: 12194.771484\n",
      "Train Epoch: 3173 [65792/118836 (55%)] Loss: 12236.150391\n",
      "Train Epoch: 3173 [98560/118836 (83%)] Loss: 12239.835938\n",
      "    epoch          : 3173\n",
      "    loss           : 12218.884915219189\n",
      "    val_loss       : 12219.015093269256\n",
      "    val_log_likelihood: -12144.708350942152\n",
      "    val_log_marginal: -12153.128740239987\n",
      "Train Epoch: 3174 [256/118836 (0%)] Loss: 12219.489258\n",
      "Train Epoch: 3174 [33024/118836 (28%)] Loss: 12272.931641\n",
      "Train Epoch: 3174 [65792/118836 (55%)] Loss: 12226.595703\n",
      "Train Epoch: 3174 [98560/118836 (83%)] Loss: 12164.931641\n",
      "    epoch          : 3174\n",
      "    loss           : 12216.81125688198\n",
      "    val_loss       : 12219.38842985874\n",
      "    val_log_likelihood: -12144.653888479632\n",
      "    val_log_marginal: -12152.936009940953\n",
      "Train Epoch: 3175 [256/118836 (0%)] Loss: 12236.975586\n",
      "Train Epoch: 3175 [33024/118836 (28%)] Loss: 12182.068359\n",
      "Train Epoch: 3175 [65792/118836 (55%)] Loss: 12200.499023\n",
      "Train Epoch: 3175 [98560/118836 (83%)] Loss: 12181.298828\n",
      "    epoch          : 3175\n",
      "    loss           : 12221.0058123643\n",
      "    val_loss       : 12220.028851469078\n",
      "    val_log_likelihood: -12142.325879148573\n",
      "    val_log_marginal: -12150.46975677155\n",
      "Train Epoch: 3176 [256/118836 (0%)] Loss: 12252.877930\n",
      "Train Epoch: 3176 [33024/118836 (28%)] Loss: 12188.013672\n",
      "Train Epoch: 3176 [65792/118836 (55%)] Loss: 12280.559570\n",
      "Train Epoch: 3176 [98560/118836 (83%)] Loss: 12288.235352\n",
      "    epoch          : 3176\n",
      "    loss           : 12220.054196391646\n",
      "    val_loss       : 12224.347353264062\n",
      "    val_log_likelihood: -12143.376585763028\n",
      "    val_log_marginal: -12151.657344933283\n",
      "Train Epoch: 3177 [256/118836 (0%)] Loss: 12336.888672\n",
      "Train Epoch: 3177 [33024/118836 (28%)] Loss: 12292.079102\n",
      "Train Epoch: 3177 [65792/118836 (55%)] Loss: 12277.999023\n",
      "Train Epoch: 3177 [98560/118836 (83%)] Loss: 12270.671875\n",
      "    epoch          : 3177\n",
      "    loss           : 12221.87002413539\n",
      "    val_loss       : 12220.251179122612\n",
      "    val_log_likelihood: -12144.173709063534\n",
      "    val_log_marginal: -12152.406534560549\n",
      "Train Epoch: 3178 [256/118836 (0%)] Loss: 12190.436523\n",
      "Train Epoch: 3178 [33024/118836 (28%)] Loss: 12164.599609\n",
      "Train Epoch: 3178 [65792/118836 (55%)] Loss: 12319.371094\n",
      "Train Epoch: 3178 [98560/118836 (83%)] Loss: 12314.973633\n",
      "    epoch          : 3178\n",
      "    loss           : 12225.733489551023\n",
      "    val_loss       : 12215.369714729257\n",
      "    val_log_likelihood: -12142.812703066842\n",
      "    val_log_marginal: -12151.112750171736\n",
      "Train Epoch: 3179 [256/118836 (0%)] Loss: 12239.947266\n",
      "Train Epoch: 3179 [33024/118836 (28%)] Loss: 12262.723633\n",
      "Train Epoch: 3179 [65792/118836 (55%)] Loss: 12208.812500\n",
      "Train Epoch: 3179 [98560/118836 (83%)] Loss: 12200.576172\n",
      "    epoch          : 3179\n",
      "    loss           : 12218.179974087572\n",
      "    val_loss       : 12224.692014898299\n",
      "    val_log_likelihood: -12144.630932071961\n",
      "    val_log_marginal: -12152.891435812842\n",
      "Train Epoch: 3180 [256/118836 (0%)] Loss: 12338.052734\n",
      "Train Epoch: 3180 [33024/118836 (28%)] Loss: 12214.054688\n",
      "Train Epoch: 3180 [65792/118836 (55%)] Loss: 12196.066406\n",
      "Train Epoch: 3180 [98560/118836 (83%)] Loss: 12234.769531\n",
      "    epoch          : 3180\n",
      "    loss           : 12221.620620735112\n",
      "    val_loss       : 12220.139948899712\n",
      "    val_log_likelihood: -12142.585696146092\n",
      "    val_log_marginal: -12150.808872328987\n",
      "Train Epoch: 3181 [256/118836 (0%)] Loss: 12281.359375\n",
      "Train Epoch: 3181 [33024/118836 (28%)] Loss: 12253.744141\n",
      "Train Epoch: 3181 [65792/118836 (55%)] Loss: 12200.636719\n",
      "Train Epoch: 3181 [98560/118836 (83%)] Loss: 12282.705078\n",
      "    epoch          : 3181\n",
      "    loss           : 12221.81270759021\n",
      "    val_loss       : 12217.215252684622\n",
      "    val_log_likelihood: -12142.56893852745\n",
      "    val_log_marginal: -12150.836933326926\n",
      "Train Epoch: 3182 [256/118836 (0%)] Loss: 12217.853516\n",
      "Train Epoch: 3182 [33024/118836 (28%)] Loss: 12273.866211\n",
      "Train Epoch: 3182 [65792/118836 (55%)] Loss: 12303.341797\n",
      "Train Epoch: 3182 [98560/118836 (83%)] Loss: 12187.044922\n",
      "    epoch          : 3182\n",
      "    loss           : 12219.80034781457\n",
      "    val_loss       : 12221.374331508681\n",
      "    val_log_likelihood: -12142.741406896195\n",
      "    val_log_marginal: -12151.070748939619\n",
      "Train Epoch: 3183 [256/118836 (0%)] Loss: 12240.986328\n",
      "Train Epoch: 3183 [33024/118836 (28%)] Loss: 12257.383789\n",
      "Train Epoch: 3183 [65792/118836 (55%)] Loss: 12169.335938\n",
      "Train Epoch: 3183 [98560/118836 (83%)] Loss: 12199.583984\n",
      "    epoch          : 3183\n",
      "    loss           : 12216.926766697683\n",
      "    val_loss       : 12221.368714186237\n",
      "    val_log_likelihood: -12142.84425920182\n",
      "    val_log_marginal: -12151.182028904961\n",
      "Train Epoch: 3184 [256/118836 (0%)] Loss: 12187.806641\n",
      "Train Epoch: 3184 [33024/118836 (28%)] Loss: 12228.583008\n",
      "Train Epoch: 3184 [65792/118836 (55%)] Loss: 12287.427734\n",
      "Train Epoch: 3184 [98560/118836 (83%)] Loss: 12276.841797\n",
      "    epoch          : 3184\n",
      "    loss           : 12218.23775056219\n",
      "    val_loss       : 12215.810666041583\n",
      "    val_log_likelihood: -12141.302383329457\n",
      "    val_log_marginal: -12149.574477433202\n",
      "Train Epoch: 3185 [256/118836 (0%)] Loss: 12133.010742\n",
      "Train Epoch: 3185 [33024/118836 (28%)] Loss: 12266.583984\n",
      "Train Epoch: 3185 [65792/118836 (55%)] Loss: 12284.806641\n",
      "Train Epoch: 3185 [98560/118836 (83%)] Loss: 12221.493164\n",
      "    epoch          : 3185\n",
      "    loss           : 12219.334883394076\n",
      "    val_loss       : 12219.576507158834\n",
      "    val_log_likelihood: -12141.536104379911\n",
      "    val_log_marginal: -12149.679668414472\n",
      "Train Epoch: 3186 [256/118836 (0%)] Loss: 12186.650391\n",
      "Train Epoch: 3186 [33024/118836 (28%)] Loss: 12234.831055\n",
      "Train Epoch: 3186 [65792/118836 (55%)] Loss: 12238.634766\n",
      "Train Epoch: 3186 [98560/118836 (83%)] Loss: 12202.736328\n",
      "    epoch          : 3186\n",
      "    loss           : 12222.401823078217\n",
      "    val_loss       : 12219.916946105795\n",
      "    val_log_likelihood: -12142.054492995243\n",
      "    val_log_marginal: -12150.38906086854\n",
      "Train Epoch: 3187 [256/118836 (0%)] Loss: 12284.546875\n",
      "Train Epoch: 3187 [33024/118836 (28%)] Loss: 12253.254883\n",
      "Train Epoch: 3187 [65792/118836 (55%)] Loss: 12214.781250\n",
      "Train Epoch: 3187 [98560/118836 (83%)] Loss: 12162.753906\n",
      "    epoch          : 3187\n",
      "    loss           : 12220.236704695255\n",
      "    val_loss       : 12218.556897806726\n",
      "    val_log_likelihood: -12142.486901138596\n",
      "    val_log_marginal: -12150.764454132437\n",
      "Train Epoch: 3188 [256/118836 (0%)] Loss: 12234.398438\n",
      "Train Epoch: 3188 [33024/118836 (28%)] Loss: 12292.917969\n",
      "Train Epoch: 3188 [65792/118836 (55%)] Loss: 12244.864258\n",
      "Train Epoch: 3188 [98560/118836 (83%)] Loss: 12273.916016\n",
      "    epoch          : 3188\n",
      "    loss           : 12221.118746445927\n",
      "    val_loss       : 12224.107489255657\n",
      "    val_log_likelihood: -12142.714008704248\n",
      "    val_log_marginal: -12151.038106153494\n",
      "Train Epoch: 3189 [256/118836 (0%)] Loss: 12230.944336\n",
      "Train Epoch: 3189 [33024/118836 (28%)] Loss: 12165.400391\n",
      "Train Epoch: 3189 [65792/118836 (55%)] Loss: 12318.397461\n",
      "Train Epoch: 3189 [98560/118836 (83%)] Loss: 12208.543945\n",
      "    epoch          : 3189\n",
      "    loss           : 12216.333504575063\n",
      "    val_loss       : 12219.248970526703\n",
      "    val_log_likelihood: -12142.988846670802\n",
      "    val_log_marginal: -12151.244363706644\n",
      "Train Epoch: 3190 [256/118836 (0%)] Loss: 12220.930664\n",
      "Train Epoch: 3190 [33024/118836 (28%)] Loss: 12363.934570\n",
      "Train Epoch: 3190 [65792/118836 (55%)] Loss: 12187.547852\n",
      "Train Epoch: 3190 [98560/118836 (83%)] Loss: 12226.469727\n",
      "    epoch          : 3190\n",
      "    loss           : 12221.861655584418\n",
      "    val_loss       : 12219.380436405672\n",
      "    val_log_likelihood: -12144.392875536343\n",
      "    val_log_marginal: -12152.851652118283\n",
      "Train Epoch: 3191 [256/118836 (0%)] Loss: 12203.560547\n",
      "Train Epoch: 3191 [33024/118836 (28%)] Loss: 12215.015625\n",
      "Train Epoch: 3191 [65792/118836 (55%)] Loss: 12204.646484\n",
      "Train Epoch: 3191 [98560/118836 (83%)] Loss: 12158.028320\n",
      "    epoch          : 3191\n",
      "    loss           : 12222.45732914599\n",
      "    val_loss       : 12221.195836410847\n",
      "    val_log_likelihood: -12142.273927154414\n",
      "    val_log_marginal: -12150.810699038562\n",
      "Train Epoch: 3192 [256/118836 (0%)] Loss: 12206.926758\n",
      "Train Epoch: 3192 [33024/118836 (28%)] Loss: 12138.569336\n",
      "Train Epoch: 3192 [65792/118836 (55%)] Loss: 12282.463867\n",
      "Train Epoch: 3192 [98560/118836 (83%)] Loss: 12210.148438\n",
      "    epoch          : 3192\n",
      "    loss           : 12219.458689386891\n",
      "    val_loss       : 12216.65805850202\n",
      "    val_log_likelihood: -12144.62221586797\n",
      "    val_log_marginal: -12152.862932338807\n",
      "Train Epoch: 3193 [256/118836 (0%)] Loss: 12241.733398\n",
      "Train Epoch: 3193 [33024/118836 (28%)] Loss: 12299.293945\n",
      "Train Epoch: 3193 [65792/118836 (55%)] Loss: 12202.033203\n",
      "Train Epoch: 3193 [98560/118836 (83%)] Loss: 12264.298828\n",
      "    epoch          : 3193\n",
      "    loss           : 12217.53760904544\n",
      "    val_loss       : 12218.800158201582\n",
      "    val_log_likelihood: -12141.24478859724\n",
      "    val_log_marginal: -12149.557891698836\n",
      "Train Epoch: 3194 [256/118836 (0%)] Loss: 12357.250977\n",
      "Train Epoch: 3194 [33024/118836 (28%)] Loss: 12226.282227\n",
      "Train Epoch: 3194 [65792/118836 (55%)] Loss: 12282.651367\n",
      "Train Epoch: 3194 [98560/118836 (83%)] Loss: 12287.141602\n",
      "    epoch          : 3194\n",
      "    loss           : 12218.68804829663\n",
      "    val_loss       : 12221.592346883644\n",
      "    val_log_likelihood: -12143.156109452544\n",
      "    val_log_marginal: -12151.37664462938\n",
      "Train Epoch: 3195 [256/118836 (0%)] Loss: 12289.999023\n",
      "Train Epoch: 3195 [33024/118836 (28%)] Loss: 12313.827148\n",
      "Train Epoch: 3195 [65792/118836 (55%)] Loss: 12256.244141\n",
      "Train Epoch: 3195 [98560/118836 (83%)] Loss: 12143.517578\n",
      "    epoch          : 3195\n",
      "    loss           : 12221.687084496483\n",
      "    val_loss       : 12220.180443312853\n",
      "    val_log_likelihood: -12144.870397636218\n",
      "    val_log_marginal: -12153.141955703564\n",
      "Train Epoch: 3196 [256/118836 (0%)] Loss: 12258.423828\n",
      "Train Epoch: 3196 [33024/118836 (28%)] Loss: 12236.565430\n",
      "Train Epoch: 3196 [65792/118836 (55%)] Loss: 12238.925781\n",
      "Train Epoch: 3196 [98560/118836 (83%)] Loss: 12285.165039\n",
      "    epoch          : 3196\n",
      "    loss           : 12219.335389849566\n",
      "    val_loss       : 12219.59188684956\n",
      "    val_log_likelihood: -12141.811972704714\n",
      "    val_log_marginal: -12150.081601842861\n",
      "Train Epoch: 3197 [256/118836 (0%)] Loss: 12260.730469\n",
      "Train Epoch: 3197 [33024/118836 (28%)] Loss: 12250.360352\n",
      "Train Epoch: 3197 [65792/118836 (55%)] Loss: 12272.319336\n",
      "Train Epoch: 3197 [98560/118836 (83%)] Loss: 12175.996094\n",
      "    epoch          : 3197\n",
      "    loss           : 12218.837099520524\n",
      "    val_loss       : 12217.989258701542\n",
      "    val_log_likelihood: -12141.543111074494\n",
      "    val_log_marginal: -12149.712615452012\n",
      "Train Epoch: 3198 [256/118836 (0%)] Loss: 12227.788086\n",
      "Train Epoch: 3198 [33024/118836 (28%)] Loss: 12323.251953\n",
      "Train Epoch: 3198 [65792/118836 (55%)] Loss: 12233.692383\n",
      "Train Epoch: 3198 [98560/118836 (83%)] Loss: 12229.849609\n",
      "    epoch          : 3198\n",
      "    loss           : 12220.225233922662\n",
      "    val_loss       : 12219.185851203312\n",
      "    val_log_likelihood: -12141.10634046733\n",
      "    val_log_marginal: -12149.311414762507\n",
      "Train Epoch: 3199 [256/118836 (0%)] Loss: 12278.553711\n",
      "Train Epoch: 3199 [33024/118836 (28%)] Loss: 12270.954102\n",
      "Train Epoch: 3199 [65792/118836 (55%)] Loss: 12298.452148\n",
      "Train Epoch: 3199 [98560/118836 (83%)] Loss: 12225.475586\n",
      "    epoch          : 3199\n",
      "    loss           : 12216.83170249819\n",
      "    val_loss       : 12217.558091656194\n",
      "    val_log_likelihood: -12142.091821430417\n",
      "    val_log_marginal: -12150.326232917128\n",
      "Train Epoch: 3200 [256/118836 (0%)] Loss: 12224.890625\n",
      "Train Epoch: 3200 [33024/118836 (28%)] Loss: 12352.324219\n",
      "Train Epoch: 3200 [65792/118836 (55%)] Loss: 12179.919922\n",
      "Train Epoch: 3200 [98560/118836 (83%)] Loss: 12175.076172\n",
      "    epoch          : 3200\n",
      "    loss           : 12216.988719693445\n",
      "    val_loss       : 12221.578502373724\n",
      "    val_log_likelihood: -12143.626875743124\n",
      "    val_log_marginal: -12151.892282903667\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch3200.pth ...\n",
      "Train Epoch: 3201 [256/118836 (0%)] Loss: 12356.570312\n",
      "Train Epoch: 3201 [33024/118836 (28%)] Loss: 12189.888672\n",
      "Train Epoch: 3201 [65792/118836 (55%)] Loss: 12227.001953\n",
      "Train Epoch: 3201 [98560/118836 (83%)] Loss: 12193.138672\n",
      "    epoch          : 3201\n",
      "    loss           : 12216.038202737283\n",
      "    val_loss       : 12221.540304218359\n",
      "    val_log_likelihood: -12141.127838735525\n",
      "    val_log_marginal: -12149.464297299091\n",
      "Train Epoch: 3202 [256/118836 (0%)] Loss: 12270.583984\n",
      "Train Epoch: 3202 [33024/118836 (28%)] Loss: 12246.367188\n",
      "Train Epoch: 3202 [65792/118836 (55%)] Loss: 12236.386719\n",
      "Train Epoch: 3202 [98560/118836 (83%)] Loss: 12226.970703\n",
      "    epoch          : 3202\n",
      "    loss           : 12220.307851110163\n",
      "    val_loss       : 12221.42572262578\n",
      "    val_log_likelihood: -12141.987650401934\n",
      "    val_log_marginal: -12150.240641576016\n",
      "Train Epoch: 3203 [256/118836 (0%)] Loss: 12262.148438\n",
      "Train Epoch: 3203 [33024/118836 (28%)] Loss: 12242.892578\n",
      "Train Epoch: 3203 [65792/118836 (55%)] Loss: 12294.891602\n",
      "Train Epoch: 3203 [98560/118836 (83%)] Loss: 12199.955078\n",
      "    epoch          : 3203\n",
      "    loss           : 12219.279795737697\n",
      "    val_loss       : 12222.14738081826\n",
      "    val_log_likelihood: -12144.015367975859\n",
      "    val_log_marginal: -12152.234293746928\n",
      "Train Epoch: 3204 [256/118836 (0%)] Loss: 12268.787109\n",
      "Train Epoch: 3204 [33024/118836 (28%)] Loss: 12279.410156\n",
      "Train Epoch: 3204 [65792/118836 (55%)] Loss: 12223.111328\n",
      "Train Epoch: 3204 [98560/118836 (83%)] Loss: 12298.953125\n",
      "    epoch          : 3204\n",
      "    loss           : 12219.449171416201\n",
      "    val_loss       : 12219.266088726676\n",
      "    val_log_likelihood: -12142.05215522255\n",
      "    val_log_marginal: -12150.291413831657\n",
      "Train Epoch: 3205 [256/118836 (0%)] Loss: 12277.976562\n",
      "Train Epoch: 3205 [33024/118836 (28%)] Loss: 12283.912109\n",
      "Train Epoch: 3205 [65792/118836 (55%)] Loss: 12191.219727\n",
      "Train Epoch: 3205 [98560/118836 (83%)] Loss: 12205.492188\n",
      "    epoch          : 3205\n",
      "    loss           : 12219.503252623552\n",
      "    val_loss       : 12216.344326682432\n",
      "    val_log_likelihood: -12144.57937377223\n",
      "    val_log_marginal: -12152.909065745826\n",
      "Train Epoch: 3206 [256/118836 (0%)] Loss: 12314.721680\n",
      "Train Epoch: 3206 [33024/118836 (28%)] Loss: 12225.916016\n",
      "Train Epoch: 3206 [65792/118836 (55%)] Loss: 12214.888672\n",
      "Train Epoch: 3206 [98560/118836 (83%)] Loss: 12153.720703\n",
      "    epoch          : 3206\n",
      "    loss           : 12221.354473609386\n",
      "    val_loss       : 12221.103413896291\n",
      "    val_log_likelihood: -12143.271975483354\n",
      "    val_log_marginal: -12151.476736191014\n",
      "Train Epoch: 3207 [256/118836 (0%)] Loss: 12254.514648\n",
      "Train Epoch: 3207 [33024/118836 (28%)] Loss: 12249.995117\n",
      "Train Epoch: 3207 [65792/118836 (55%)] Loss: 12334.537109\n",
      "Train Epoch: 3207 [98560/118836 (83%)] Loss: 12232.097656\n",
      "    epoch          : 3207\n",
      "    loss           : 12220.884955283293\n",
      "    val_loss       : 12218.837574689845\n",
      "    val_log_likelihood: -12142.07075982863\n",
      "    val_log_marginal: -12150.15724166085\n",
      "Train Epoch: 3208 [256/118836 (0%)] Loss: 12235.902344\n",
      "Train Epoch: 3208 [33024/118836 (28%)] Loss: 12267.148438\n",
      "Train Epoch: 3208 [65792/118836 (55%)] Loss: 12350.648438\n",
      "Train Epoch: 3208 [98560/118836 (83%)] Loss: 12194.051758\n",
      "    epoch          : 3208\n",
      "    loss           : 12217.534981938845\n",
      "    val_loss       : 12220.714329476838\n",
      "    val_log_likelihood: -12142.526463147487\n",
      "    val_log_marginal: -12150.888928107623\n",
      "Train Epoch: 3209 [256/118836 (0%)] Loss: 12191.692383\n",
      "Train Epoch: 3209 [33024/118836 (28%)] Loss: 12224.011719\n",
      "Train Epoch: 3209 [65792/118836 (55%)] Loss: 12182.299805\n",
      "Train Epoch: 3209 [98560/118836 (83%)] Loss: 12184.462891\n",
      "    epoch          : 3209\n",
      "    loss           : 12218.934595029466\n",
      "    val_loss       : 12221.311474142893\n",
      "    val_log_likelihood: -12143.752521938328\n",
      "    val_log_marginal: -12152.065657908466\n",
      "Train Epoch: 3210 [256/118836 (0%)] Loss: 12315.494141\n",
      "Train Epoch: 3210 [33024/118836 (28%)] Loss: 12235.815430\n",
      "Train Epoch: 3210 [65792/118836 (55%)] Loss: 12275.495117\n",
      "Train Epoch: 3210 [98560/118836 (83%)] Loss: 12173.384766\n",
      "    epoch          : 3210\n",
      "    loss           : 12220.671121051748\n",
      "    val_loss       : 12218.77860342711\n",
      "    val_log_likelihood: -12141.736018920596\n",
      "    val_log_marginal: -12150.015324840566\n",
      "Train Epoch: 3211 [256/118836 (0%)] Loss: 12303.435547\n",
      "Train Epoch: 3211 [33024/118836 (28%)] Loss: 12360.026367\n",
      "Train Epoch: 3211 [65792/118836 (55%)] Loss: 12193.046875\n",
      "Train Epoch: 3211 [98560/118836 (83%)] Loss: 12245.718750\n",
      "    epoch          : 3211\n",
      "    loss           : 12219.103625478185\n",
      "    val_loss       : 12219.84226011442\n",
      "    val_log_likelihood: -12140.477925487232\n",
      "    val_log_marginal: -12148.693218783443\n",
      "Train Epoch: 3212 [256/118836 (0%)] Loss: 12324.676758\n",
      "Train Epoch: 3212 [33024/118836 (28%)] Loss: 12217.533203\n",
      "Train Epoch: 3212 [65792/118836 (55%)] Loss: 12334.879883\n",
      "Train Epoch: 3212 [98560/118836 (83%)] Loss: 12359.047852\n",
      "    epoch          : 3212\n",
      "    loss           : 12221.836176753774\n",
      "    val_loss       : 12220.287125708748\n",
      "    val_log_likelihood: -12146.234649956059\n",
      "    val_log_marginal: -12154.401661194835\n",
      "Train Epoch: 3213 [256/118836 (0%)] Loss: 12158.890625\n",
      "Train Epoch: 3213 [33024/118836 (28%)] Loss: 12133.795898\n",
      "Train Epoch: 3213 [65792/118836 (55%)] Loss: 12201.980469\n",
      "Train Epoch: 3213 [98560/118836 (83%)] Loss: 12383.763672\n",
      "    epoch          : 3213\n",
      "    loss           : 12217.192626266542\n",
      "    val_loss       : 12222.260160262771\n",
      "    val_log_likelihood: -12141.370798438791\n",
      "    val_log_marginal: -12149.641560507005\n",
      "Train Epoch: 3214 [256/118836 (0%)] Loss: 12213.417969\n",
      "Train Epoch: 3214 [33024/118836 (28%)] Loss: 12233.089844\n",
      "Train Epoch: 3214 [65792/118836 (55%)] Loss: 12259.313477\n",
      "Train Epoch: 3214 [98560/118836 (83%)] Loss: 12268.694336\n",
      "    epoch          : 3214\n",
      "    loss           : 12219.758903439051\n",
      "    val_loss       : 12217.388675931725\n",
      "    val_log_likelihood: -12144.14596854968\n",
      "    val_log_marginal: -12152.440508732516\n",
      "Train Epoch: 3215 [256/118836 (0%)] Loss: 12214.908203\n",
      "Train Epoch: 3215 [33024/118836 (28%)] Loss: 12264.048828\n",
      "Train Epoch: 3215 [65792/118836 (55%)] Loss: 12221.573242\n",
      "Train Epoch: 3215 [98560/118836 (83%)] Loss: 12215.916016\n",
      "    epoch          : 3215\n",
      "    loss           : 12217.988045065653\n",
      "    val_loss       : 12216.70070486339\n",
      "    val_log_likelihood: -12142.886437008892\n",
      "    val_log_marginal: -12151.140134768048\n",
      "Train Epoch: 3216 [256/118836 (0%)] Loss: 12148.299805\n",
      "Train Epoch: 3216 [33024/118836 (28%)] Loss: 12168.552734\n",
      "Train Epoch: 3216 [65792/118836 (55%)] Loss: 12261.605469\n",
      "Train Epoch: 3216 [98560/118836 (83%)] Loss: 12272.510742\n",
      "    epoch          : 3216\n",
      "    loss           : 12219.656437396608\n",
      "    val_loss       : 12220.541868639419\n",
      "    val_log_likelihood: -12141.765813204354\n",
      "    val_log_marginal: -12150.067611091175\n",
      "Train Epoch: 3217 [256/118836 (0%)] Loss: 12303.056641\n",
      "Train Epoch: 3217 [33024/118836 (28%)] Loss: 12287.272461\n",
      "Train Epoch: 3217 [65792/118836 (55%)] Loss: 12176.992188\n",
      "Train Epoch: 3217 [98560/118836 (83%)] Loss: 12329.242188\n",
      "    epoch          : 3217\n",
      "    loss           : 12218.953541149709\n",
      "    val_loss       : 12219.508754720046\n",
      "    val_log_likelihood: -12143.975256701044\n",
      "    val_log_marginal: -12152.215591905197\n",
      "Train Epoch: 3218 [256/118836 (0%)] Loss: 12193.668945\n",
      "Train Epoch: 3218 [33024/118836 (28%)] Loss: 12178.847656\n",
      "Train Epoch: 3218 [65792/118836 (55%)] Loss: 12206.402344\n",
      "Train Epoch: 3218 [98560/118836 (83%)] Loss: 12210.175781\n",
      "    epoch          : 3218\n",
      "    loss           : 12220.737492407205\n",
      "    val_loss       : 12217.931007572573\n",
      "    val_log_likelihood: -12142.680479573768\n",
      "    val_log_marginal: -12150.818914928943\n",
      "Train Epoch: 3219 [256/118836 (0%)] Loss: 12326.914062\n",
      "Train Epoch: 3219 [33024/118836 (28%)] Loss: 12226.374023\n",
      "Train Epoch: 3219 [65792/118836 (55%)] Loss: 12270.534180\n",
      "Train Epoch: 3219 [98560/118836 (83%)] Loss: 12179.179688\n",
      "    epoch          : 3219\n",
      "    loss           : 12219.35481011554\n",
      "    val_loss       : 12220.783631074513\n",
      "    val_log_likelihood: -12141.986894999742\n",
      "    val_log_marginal: -12150.19582546582\n",
      "Train Epoch: 3220 [256/118836 (0%)] Loss: 12276.280273\n",
      "Train Epoch: 3220 [33024/118836 (28%)] Loss: 12124.574219\n",
      "Train Epoch: 3220 [65792/118836 (55%)] Loss: 12330.090820\n",
      "Train Epoch: 3220 [98560/118836 (83%)] Loss: 12250.670898\n",
      "    epoch          : 3220\n",
      "    loss           : 12218.412038778173\n",
      "    val_loss       : 12218.719505689704\n",
      "    val_log_likelihood: -12142.227156676488\n",
      "    val_log_marginal: -12150.349389807148\n",
      "Train Epoch: 3221 [256/118836 (0%)] Loss: 12287.389648\n",
      "Train Epoch: 3221 [33024/118836 (28%)] Loss: 12233.972656\n",
      "Train Epoch: 3221 [65792/118836 (55%)] Loss: 12244.634766\n",
      "Train Epoch: 3221 [98560/118836 (83%)] Loss: 12253.650391\n",
      "    epoch          : 3221\n",
      "    loss           : 12221.725127946649\n",
      "    val_loss       : 12217.345755893404\n",
      "    val_log_likelihood: -12142.990399639422\n",
      "    val_log_marginal: -12151.246665294037\n",
      "Train Epoch: 3222 [256/118836 (0%)] Loss: 12181.423828\n",
      "Train Epoch: 3222 [33024/118836 (28%)] Loss: 12178.176758\n",
      "Train Epoch: 3222 [65792/118836 (55%)] Loss: 12243.441406\n",
      "Train Epoch: 3222 [98560/118836 (83%)] Loss: 12202.879883\n",
      "    epoch          : 3222\n",
      "    loss           : 12221.410109885494\n",
      "    val_loss       : 12218.293387603198\n",
      "    val_log_likelihood: -12143.063674459781\n",
      "    val_log_marginal: -12151.359878015483\n",
      "Train Epoch: 3223 [256/118836 (0%)] Loss: 12314.175781\n",
      "Train Epoch: 3223 [33024/118836 (28%)] Loss: 12276.611328\n",
      "Train Epoch: 3223 [65792/118836 (55%)] Loss: 12193.446289\n",
      "Train Epoch: 3223 [98560/118836 (83%)] Loss: 12209.443359\n",
      "    epoch          : 3223\n",
      "    loss           : 12223.536084024763\n",
      "    val_loss       : 12219.961164179389\n",
      "    val_log_likelihood: -12142.277351988989\n",
      "    val_log_marginal: -12150.61765997218\n",
      "Train Epoch: 3224 [256/118836 (0%)] Loss: 12268.649414\n",
      "Train Epoch: 3224 [33024/118836 (28%)] Loss: 12359.630859\n",
      "Train Epoch: 3224 [65792/118836 (55%)] Loss: 12230.200195\n",
      "Train Epoch: 3224 [98560/118836 (83%)] Loss: 12211.236328\n",
      "    epoch          : 3224\n",
      "    loss           : 12218.684987916151\n",
      "    val_loss       : 12220.856469066193\n",
      "    val_log_likelihood: -12142.815838405966\n",
      "    val_log_marginal: -12151.134092812143\n",
      "Train Epoch: 3225 [256/118836 (0%)] Loss: 12274.369141\n",
      "Train Epoch: 3225 [33024/118836 (28%)] Loss: 12172.921875\n",
      "Train Epoch: 3225 [65792/118836 (55%)] Loss: 12159.403320\n",
      "Train Epoch: 3225 [98560/118836 (83%)] Loss: 12183.492188\n",
      "    epoch          : 3225\n",
      "    loss           : 12218.291412227307\n",
      "    val_loss       : 12218.292176949615\n",
      "    val_log_likelihood: -12143.454414644077\n",
      "    val_log_marginal: -12151.618174071338\n",
      "Train Epoch: 3226 [256/118836 (0%)] Loss: 12232.017578\n",
      "Train Epoch: 3226 [33024/118836 (28%)] Loss: 12175.072266\n",
      "Train Epoch: 3226 [65792/118836 (55%)] Loss: 12252.126953\n",
      "Train Epoch: 3226 [98560/118836 (83%)] Loss: 12196.512695\n",
      "    epoch          : 3226\n",
      "    loss           : 12219.861971089225\n",
      "    val_loss       : 12218.94255327298\n",
      "    val_log_likelihood: -12142.529813184969\n",
      "    val_log_marginal: -12150.646852458909\n",
      "Train Epoch: 3227 [256/118836 (0%)] Loss: 12265.296875\n",
      "Train Epoch: 3227 [33024/118836 (28%)] Loss: 12349.709961\n",
      "Train Epoch: 3227 [65792/118836 (55%)] Loss: 12210.464844\n",
      "Train Epoch: 3227 [98560/118836 (83%)] Loss: 12170.965820\n",
      "    epoch          : 3227\n",
      "    loss           : 12224.387402101425\n",
      "    val_loss       : 12219.49745693275\n",
      "    val_log_likelihood: -12142.1569100884\n",
      "    val_log_marginal: -12150.467989927387\n",
      "Train Epoch: 3228 [256/118836 (0%)] Loss: 12309.482422\n",
      "Train Epoch: 3228 [33024/118836 (28%)] Loss: 12293.235352\n",
      "Train Epoch: 3228 [65792/118836 (55%)] Loss: 12229.142578\n",
      "Train Epoch: 3228 [98560/118836 (83%)] Loss: 12333.072266\n",
      "    epoch          : 3228\n",
      "    loss           : 12218.901387865748\n",
      "    val_loss       : 12218.84810560171\n",
      "    val_log_likelihood: -12142.149527954405\n",
      "    val_log_marginal: -12150.291537710786\n",
      "Train Epoch: 3229 [256/118836 (0%)] Loss: 12142.376953\n",
      "Train Epoch: 3229 [33024/118836 (28%)] Loss: 12284.492188\n",
      "Train Epoch: 3229 [65792/118836 (55%)] Loss: 12228.067383\n",
      "Train Epoch: 3229 [98560/118836 (83%)] Loss: 12180.836914\n",
      "    epoch          : 3229\n",
      "    loss           : 12217.09508213141\n",
      "    val_loss       : 12218.100284082795\n",
      "    val_log_likelihood: -12142.063370909584\n",
      "    val_log_marginal: -12150.363482862327\n",
      "Train Epoch: 3230 [256/118836 (0%)] Loss: 12256.779297\n",
      "Train Epoch: 3230 [33024/118836 (28%)] Loss: 12220.906250\n",
      "Train Epoch: 3230 [65792/118836 (55%)] Loss: 12267.972656\n",
      "Train Epoch: 3230 [98560/118836 (83%)] Loss: 12221.683594\n",
      "    epoch          : 3230\n",
      "    loss           : 12223.565032439\n",
      "    val_loss       : 12219.247896871508\n",
      "    val_log_likelihood: -12142.84202336642\n",
      "    val_log_marginal: -12151.150393225113\n",
      "Train Epoch: 3231 [256/118836 (0%)] Loss: 12242.426758\n",
      "Train Epoch: 3231 [33024/118836 (28%)] Loss: 12279.342773\n",
      "Train Epoch: 3231 [65792/118836 (55%)] Loss: 12214.281250\n",
      "Train Epoch: 3231 [98560/118836 (83%)] Loss: 12269.359375\n",
      "    epoch          : 3231\n",
      "    loss           : 12220.06905839666\n",
      "    val_loss       : 12221.921692807273\n",
      "    val_log_likelihood: -12141.841828538565\n",
      "    val_log_marginal: -12150.283195963078\n",
      "Train Epoch: 3232 [256/118836 (0%)] Loss: 12232.376953\n",
      "Train Epoch: 3232 [33024/118836 (28%)] Loss: 12256.552734\n",
      "Train Epoch: 3232 [65792/118836 (55%)] Loss: 12219.669922\n",
      "Train Epoch: 3232 [98560/118836 (83%)] Loss: 12197.230469\n",
      "    epoch          : 3232\n",
      "    loss           : 12219.463300797406\n",
      "    val_loss       : 12217.305009266953\n",
      "    val_log_likelihood: -12143.375225037478\n",
      "    val_log_marginal: -12151.585423295006\n",
      "Train Epoch: 3233 [256/118836 (0%)] Loss: 12204.338867\n",
      "Train Epoch: 3233 [33024/118836 (28%)] Loss: 12284.878906\n",
      "Train Epoch: 3233 [65792/118836 (55%)] Loss: 12190.311523\n",
      "Train Epoch: 3233 [98560/118836 (83%)] Loss: 12212.734375\n",
      "    epoch          : 3233\n",
      "    loss           : 12218.96861122958\n",
      "    val_loss       : 12216.275508936356\n",
      "    val_log_likelihood: -12140.543521893092\n",
      "    val_log_marginal: -12148.863029967279\n",
      "Train Epoch: 3234 [256/118836 (0%)] Loss: 12190.111328\n",
      "Train Epoch: 3234 [33024/118836 (28%)] Loss: 12237.242188\n",
      "Train Epoch: 3234 [65792/118836 (55%)] Loss: 12200.814453\n",
      "Train Epoch: 3234 [98560/118836 (83%)] Loss: 12250.388672\n",
      "    epoch          : 3234\n",
      "    loss           : 12218.210585323615\n",
      "    val_loss       : 12221.741584353902\n",
      "    val_log_likelihood: -12141.95507570177\n",
      "    val_log_marginal: -12150.157577714848\n",
      "Train Epoch: 3235 [256/118836 (0%)] Loss: 12311.688477\n",
      "Train Epoch: 3235 [33024/118836 (28%)] Loss: 12279.114258\n",
      "Train Epoch: 3235 [65792/118836 (55%)] Loss: 12295.000977\n",
      "Train Epoch: 3235 [98560/118836 (83%)] Loss: 12152.240234\n",
      "    epoch          : 3235\n",
      "    loss           : 12217.116394941584\n",
      "    val_loss       : 12220.17924696516\n",
      "    val_log_likelihood: -12143.039648760598\n",
      "    val_log_marginal: -12151.327342188188\n",
      "Train Epoch: 3236 [256/118836 (0%)] Loss: 12171.335938\n",
      "Train Epoch: 3236 [33024/118836 (28%)] Loss: 12147.013672\n",
      "Train Epoch: 3236 [65792/118836 (55%)] Loss: 12230.089844\n",
      "Train Epoch: 3236 [98560/118836 (83%)] Loss: 12356.356445\n",
      "    epoch          : 3236\n",
      "    loss           : 12220.758969997156\n",
      "    val_loss       : 12222.072651801178\n",
      "    val_log_likelihood: -12144.215430010598\n",
      "    val_log_marginal: -12152.884131409592\n",
      "Train Epoch: 3237 [256/118836 (0%)] Loss: 12283.710938\n",
      "Train Epoch: 3237 [33024/118836 (28%)] Loss: 12235.995117\n",
      "Train Epoch: 3237 [65792/118836 (55%)] Loss: 12264.250000\n",
      "Train Epoch: 3237 [98560/118836 (83%)] Loss: 12217.071289\n",
      "    epoch          : 3237\n",
      "    loss           : 12218.083396660462\n",
      "    val_loss       : 12219.88896467653\n",
      "    val_log_likelihood: -12142.85145765483\n",
      "    val_log_marginal: -12151.053336409796\n",
      "Train Epoch: 3238 [256/118836 (0%)] Loss: 12228.318359\n",
      "Train Epoch: 3238 [33024/118836 (28%)] Loss: 12174.250000\n",
      "Train Epoch: 3238 [65792/118836 (55%)] Loss: 12241.455078\n",
      "Train Epoch: 3238 [98560/118836 (83%)] Loss: 12276.667969\n",
      "    epoch          : 3238\n",
      "    loss           : 12223.623347840416\n",
      "    val_loss       : 12222.848252464224\n",
      "    val_log_likelihood: -12143.395196023315\n",
      "    val_log_marginal: -12151.582124451643\n",
      "Train Epoch: 3239 [256/118836 (0%)] Loss: 12142.357422\n",
      "Train Epoch: 3239 [33024/118836 (28%)] Loss: 12330.158203\n",
      "Train Epoch: 3239 [65792/118836 (55%)] Loss: 12282.066406\n",
      "Train Epoch: 3239 [98560/118836 (83%)] Loss: 12275.128906\n",
      "    epoch          : 3239\n",
      "    loss           : 12219.83799563172\n",
      "    val_loss       : 12225.009495827962\n",
      "    val_log_likelihood: -12142.982280681348\n",
      "    val_log_marginal: -12151.253803579124\n",
      "Train Epoch: 3240 [256/118836 (0%)] Loss: 12290.013672\n",
      "Train Epoch: 3240 [33024/118836 (28%)] Loss: 12297.720703\n",
      "Train Epoch: 3240 [65792/118836 (55%)] Loss: 12238.884766\n",
      "Train Epoch: 3240 [98560/118836 (83%)] Loss: 12211.728516\n",
      "    epoch          : 3240\n",
      "    loss           : 12220.428966669251\n",
      "    val_loss       : 12219.991079766005\n",
      "    val_log_likelihood: -12143.419170511528\n",
      "    val_log_marginal: -12151.574147474154\n",
      "Train Epoch: 3241 [256/118836 (0%)] Loss: 12271.766602\n",
      "Train Epoch: 3241 [33024/118836 (28%)] Loss: 12341.931641\n",
      "Train Epoch: 3241 [65792/118836 (55%)] Loss: 12184.617188\n",
      "Train Epoch: 3241 [98560/118836 (83%)] Loss: 12341.535156\n",
      "    epoch          : 3241\n",
      "    loss           : 12224.433871129291\n",
      "    val_loss       : 12220.81941407248\n",
      "    val_log_likelihood: -12144.177517091863\n",
      "    val_log_marginal: -12152.409259152379\n",
      "Train Epoch: 3242 [256/118836 (0%)] Loss: 12235.224609\n",
      "Train Epoch: 3242 [33024/118836 (28%)] Loss: 12198.675781\n",
      "Train Epoch: 3242 [65792/118836 (55%)] Loss: 12295.324219\n",
      "Train Epoch: 3242 [98560/118836 (83%)] Loss: 12218.990234\n",
      "    epoch          : 3242\n",
      "    loss           : 12220.53239748113\n",
      "    val_loss       : 12217.990476105588\n",
      "    val_log_likelihood: -12144.18991515457\n",
      "    val_log_marginal: -12152.425667870784\n",
      "Train Epoch: 3243 [256/118836 (0%)] Loss: 12276.113281\n",
      "Train Epoch: 3243 [33024/118836 (28%)] Loss: 12143.459961\n",
      "Train Epoch: 3243 [65792/118836 (55%)] Loss: 12133.246094\n",
      "Train Epoch: 3243 [98560/118836 (83%)] Loss: 12254.640625\n",
      "    epoch          : 3243\n",
      "    loss           : 12216.517863097084\n",
      "    val_loss       : 12220.778050991985\n",
      "    val_log_likelihood: -12142.431106706214\n",
      "    val_log_marginal: -12150.613105147044\n",
      "Train Epoch: 3244 [256/118836 (0%)] Loss: 12301.150391\n",
      "Train Epoch: 3244 [33024/118836 (28%)] Loss: 12277.759766\n",
      "Train Epoch: 3244 [65792/118836 (55%)] Loss: 12250.968750\n",
      "Train Epoch: 3244 [98560/118836 (83%)] Loss: 12241.380859\n",
      "    epoch          : 3244\n",
      "    loss           : 12217.045818147746\n",
      "    val_loss       : 12224.746020932362\n",
      "    val_log_likelihood: -12142.798019250156\n",
      "    val_log_marginal: -12151.193737226833\n",
      "Train Epoch: 3245 [256/118836 (0%)] Loss: 12290.757812\n",
      "Train Epoch: 3245 [33024/118836 (28%)] Loss: 12202.717773\n",
      "Train Epoch: 3245 [65792/118836 (55%)] Loss: 12259.905273\n",
      "Train Epoch: 3245 [98560/118836 (83%)] Loss: 12140.014648\n",
      "    epoch          : 3245\n",
      "    loss           : 12220.77272781612\n",
      "    val_loss       : 12221.781260100803\n",
      "    val_log_likelihood: -12143.068790871846\n",
      "    val_log_marginal: -12151.327987087436\n",
      "Train Epoch: 3246 [256/118836 (0%)] Loss: 12242.791016\n",
      "Train Epoch: 3246 [33024/118836 (28%)] Loss: 12278.097656\n",
      "Train Epoch: 3246 [65792/118836 (55%)] Loss: 12236.299805\n",
      "Train Epoch: 3246 [98560/118836 (83%)] Loss: 12293.793945\n",
      "    epoch          : 3246\n",
      "    loss           : 12223.423028781534\n",
      "    val_loss       : 12226.060921672657\n",
      "    val_log_likelihood: -12144.123088554592\n",
      "    val_log_marginal: -12152.42162328372\n",
      "Train Epoch: 3247 [256/118836 (0%)] Loss: 12242.104492\n",
      "Train Epoch: 3247 [33024/118836 (28%)] Loss: 12157.538086\n",
      "Train Epoch: 3247 [65792/118836 (55%)] Loss: 12172.717773\n",
      "Train Epoch: 3247 [98560/118836 (83%)] Loss: 12168.227539\n",
      "    epoch          : 3247\n",
      "    loss           : 12221.989424207764\n",
      "    val_loss       : 12218.1253800849\n",
      "    val_log_likelihood: -12141.368557918477\n",
      "    val_log_marginal: -12149.575370249993\n",
      "Train Epoch: 3248 [256/118836 (0%)] Loss: 12181.539062\n",
      "Train Epoch: 3248 [33024/118836 (28%)] Loss: 12284.178711\n",
      "Train Epoch: 3248 [65792/118836 (55%)] Loss: 12332.761719\n",
      "Train Epoch: 3248 [98560/118836 (83%)] Loss: 12234.755859\n",
      "    epoch          : 3248\n",
      "    loss           : 12221.406422534119\n",
      "    val_loss       : 12218.002453014768\n",
      "    val_log_likelihood: -12141.147494539651\n",
      "    val_log_marginal: -12149.268390638954\n",
      "Train Epoch: 3249 [256/118836 (0%)] Loss: 12191.508789\n",
      "Train Epoch: 3249 [33024/118836 (28%)] Loss: 12298.266602\n",
      "Train Epoch: 3249 [65792/118836 (55%)] Loss: 12218.793945\n",
      "Train Epoch: 3249 [98560/118836 (83%)] Loss: 12198.798828\n",
      "    epoch          : 3249\n",
      "    loss           : 12216.86381193781\n",
      "    val_loss       : 12220.224807519164\n",
      "    val_log_likelihood: -12142.82583229942\n",
      "    val_log_marginal: -12151.077895353412\n",
      "Train Epoch: 3250 [256/118836 (0%)] Loss: 12171.300781\n",
      "Train Epoch: 3250 [33024/118836 (28%)] Loss: 12251.210938\n",
      "Train Epoch: 3250 [65792/118836 (55%)] Loss: 12170.804688\n",
      "Train Epoch: 3250 [98560/118836 (83%)] Loss: 12160.383789\n",
      "    epoch          : 3250\n",
      "    loss           : 12217.671848667545\n",
      "    val_loss       : 12223.334950990253\n",
      "    val_log_likelihood: -12142.310263356854\n",
      "    val_log_marginal: -12150.64583732509\n",
      "Train Epoch: 3251 [256/118836 (0%)] Loss: 12283.885742\n",
      "Train Epoch: 3251 [33024/118836 (28%)] Loss: 12405.267578\n",
      "Train Epoch: 3251 [65792/118836 (55%)] Loss: 12267.084961\n",
      "Train Epoch: 3251 [98560/118836 (83%)] Loss: 12213.509766\n",
      "    epoch          : 3251\n",
      "    loss           : 12219.171458688741\n",
      "    val_loss       : 12220.115794126836\n",
      "    val_log_likelihood: -12143.099305986352\n",
      "    val_log_marginal: -12151.42146304982\n",
      "Train Epoch: 3252 [256/118836 (0%)] Loss: 12299.022461\n",
      "Train Epoch: 3252 [33024/118836 (28%)] Loss: 12256.964844\n",
      "Train Epoch: 3252 [65792/118836 (55%)] Loss: 12159.246094\n",
      "Train Epoch: 3252 [98560/118836 (83%)] Loss: 12334.987305\n",
      "    epoch          : 3252\n",
      "    loss           : 12219.406989731959\n",
      "    val_loss       : 12219.764625959959\n",
      "    val_log_likelihood: -12141.87289049576\n",
      "    val_log_marginal: -12150.022608315152\n",
      "Train Epoch: 3253 [256/118836 (0%)] Loss: 12188.899414\n",
      "Train Epoch: 3253 [33024/118836 (28%)] Loss: 12158.541992\n",
      "Train Epoch: 3253 [65792/118836 (55%)] Loss: 12277.763672\n",
      "Train Epoch: 3253 [98560/118836 (83%)] Loss: 12259.602539\n",
      "    epoch          : 3253\n",
      "    loss           : 12216.502498998398\n",
      "    val_loss       : 12217.762209527584\n",
      "    val_log_likelihood: -12143.973010203423\n",
      "    val_log_marginal: -12152.17530686312\n",
      "Train Epoch: 3254 [256/118836 (0%)] Loss: 12156.576172\n",
      "Train Epoch: 3254 [33024/118836 (28%)] Loss: 12234.251953\n",
      "Train Epoch: 3254 [65792/118836 (55%)] Loss: 12266.660156\n",
      "Train Epoch: 3254 [98560/118836 (83%)] Loss: 12156.938477\n",
      "    epoch          : 3254\n",
      "    loss           : 12218.51201228417\n",
      "    val_loss       : 12222.700076382393\n",
      "    val_log_likelihood: -12142.055644676644\n",
      "    val_log_marginal: -12150.336663749056\n",
      "Train Epoch: 3255 [256/118836 (0%)] Loss: 12213.292969\n",
      "Train Epoch: 3255 [33024/118836 (28%)] Loss: 12248.812500\n",
      "Train Epoch: 3255 [65792/118836 (55%)] Loss: 12243.733398\n",
      "Train Epoch: 3255 [98560/118836 (83%)] Loss: 12189.705078\n",
      "    epoch          : 3255\n",
      "    loss           : 12215.054717548077\n",
      "    val_loss       : 12217.364787274111\n",
      "    val_log_likelihood: -12142.01090519024\n",
      "    val_log_marginal: -12150.222332432251\n",
      "Train Epoch: 3256 [256/118836 (0%)] Loss: 12272.790039\n",
      "Train Epoch: 3256 [33024/118836 (28%)] Loss: 12181.838867\n",
      "Train Epoch: 3256 [65792/118836 (55%)] Loss: 12261.762695\n",
      "Train Epoch: 3256 [98560/118836 (83%)] Loss: 12233.636719\n",
      "    epoch          : 3256\n",
      "    loss           : 12219.600826322116\n",
      "    val_loss       : 12216.5788066319\n",
      "    val_log_likelihood: -12144.163795298284\n",
      "    val_log_marginal: -12152.720133123958\n",
      "Train Epoch: 3257 [256/118836 (0%)] Loss: 12236.042969\n",
      "Train Epoch: 3257 [33024/118836 (28%)] Loss: 12485.445312\n",
      "Train Epoch: 3257 [65792/118836 (55%)] Loss: 12278.960938\n",
      "Train Epoch: 3257 [98560/118836 (83%)] Loss: 12297.694336\n",
      "    epoch          : 3257\n",
      "    loss           : 12220.375729231286\n",
      "    val_loss       : 12221.69502096016\n",
      "    val_log_likelihood: -12143.782146111196\n",
      "    val_log_marginal: -12152.182446166265\n",
      "Train Epoch: 3258 [256/118836 (0%)] Loss: 12219.318359\n",
      "Train Epoch: 3258 [33024/118836 (28%)] Loss: 12198.099609\n",
      "Train Epoch: 3258 [65792/118836 (55%)] Loss: 12261.653320\n",
      "Train Epoch: 3258 [98560/118836 (83%)] Loss: 12273.187500\n",
      "    epoch          : 3258\n",
      "    loss           : 12218.921279208023\n",
      "    val_loss       : 12220.973594701167\n",
      "    val_log_likelihood: -12142.162976245865\n",
      "    val_log_marginal: -12150.31800344034\n",
      "Train Epoch: 3259 [256/118836 (0%)] Loss: 12217.035156\n",
      "Train Epoch: 3259 [33024/118836 (28%)] Loss: 12302.043945\n",
      "Train Epoch: 3259 [65792/118836 (55%)] Loss: 12277.714844\n",
      "Train Epoch: 3259 [98560/118836 (83%)] Loss: 12231.892578\n",
      "    epoch          : 3259\n",
      "    loss           : 12218.78856008323\n",
      "    val_loss       : 12216.300688967656\n",
      "    val_log_likelihood: -12143.04423093078\n",
      "    val_log_marginal: -12151.36475050281\n",
      "Train Epoch: 3260 [256/118836 (0%)] Loss: 12269.185547\n",
      "Train Epoch: 3260 [33024/118836 (28%)] Loss: 12244.550781\n",
      "Train Epoch: 3260 [65792/118836 (55%)] Loss: 12218.582031\n",
      "Train Epoch: 3260 [98560/118836 (83%)] Loss: 12276.722656\n",
      "    epoch          : 3260\n",
      "    loss           : 12218.806508478081\n",
      "    val_loss       : 12223.357136811419\n",
      "    val_log_likelihood: -12143.550102745037\n",
      "    val_log_marginal: -12151.874394943943\n",
      "Train Epoch: 3261 [256/118836 (0%)] Loss: 12282.851562\n",
      "Train Epoch: 3261 [33024/118836 (28%)] Loss: 12224.481445\n",
      "Train Epoch: 3261 [65792/118836 (55%)] Loss: 12239.447266\n",
      "Train Epoch: 3261 [98560/118836 (83%)] Loss: 12240.625977\n",
      "    epoch          : 3261\n",
      "    loss           : 12215.810572238162\n",
      "    val_loss       : 12221.444094454026\n",
      "    val_log_likelihood: -12141.744766465054\n",
      "    val_log_marginal: -12149.901967144433\n",
      "Train Epoch: 3262 [256/118836 (0%)] Loss: 12217.962891\n",
      "Train Epoch: 3262 [33024/118836 (28%)] Loss: 12323.919922\n",
      "Train Epoch: 3262 [65792/118836 (55%)] Loss: 12245.158203\n",
      "Train Epoch: 3262 [98560/118836 (83%)] Loss: 12282.727539\n",
      "    epoch          : 3262\n",
      "    loss           : 12221.680581349516\n",
      "    val_loss       : 12218.164578749613\n",
      "    val_log_likelihood: -12143.03295288591\n",
      "    val_log_marginal: -12151.240688582211\n",
      "Train Epoch: 3263 [256/118836 (0%)] Loss: 12166.156250\n",
      "Train Epoch: 3263 [33024/118836 (28%)] Loss: 12307.516602\n",
      "Train Epoch: 3263 [65792/118836 (55%)] Loss: 12247.719727\n",
      "Train Epoch: 3263 [98560/118836 (83%)] Loss: 12280.784180\n",
      "    epoch          : 3263\n",
      "    loss           : 12221.521888731648\n",
      "    val_loss       : 12219.817472991692\n",
      "    val_log_likelihood: -12141.808420731235\n",
      "    val_log_marginal: -12150.05540250278\n",
      "Train Epoch: 3264 [256/118836 (0%)] Loss: 12245.365234\n",
      "Train Epoch: 3264 [33024/118836 (28%)] Loss: 12262.324219\n",
      "Train Epoch: 3264 [65792/118836 (55%)] Loss: 12247.678711\n",
      "Train Epoch: 3264 [98560/118836 (83%)] Loss: 12274.238281\n",
      "    epoch          : 3264\n",
      "    loss           : 12217.204509957868\n",
      "    val_loss       : 12217.080589006713\n",
      "    val_log_likelihood: -12143.170546584211\n",
      "    val_log_marginal: -12151.486890397508\n",
      "Train Epoch: 3265 [256/118836 (0%)] Loss: 12138.542969\n",
      "Train Epoch: 3265 [33024/118836 (28%)] Loss: 12284.057617\n",
      "Train Epoch: 3265 [65792/118836 (55%)] Loss: 12250.963867\n",
      "Train Epoch: 3265 [98560/118836 (83%)] Loss: 12218.214844\n",
      "    epoch          : 3265\n",
      "    loss           : 12220.74609391155\n",
      "    val_loss       : 12218.08159029468\n",
      "    val_log_likelihood: -12144.7433391814\n",
      "    val_log_marginal: -12153.108221822658\n",
      "Train Epoch: 3266 [256/118836 (0%)] Loss: 12286.369141\n",
      "Train Epoch: 3266 [33024/118836 (28%)] Loss: 12167.352539\n",
      "Train Epoch: 3266 [65792/118836 (55%)] Loss: 12321.275391\n",
      "Train Epoch: 3266 [98560/118836 (83%)] Loss: 12187.320312\n",
      "    epoch          : 3266\n",
      "    loss           : 12219.22891158111\n",
      "    val_loss       : 12218.377395833993\n",
      "    val_log_likelihood: -12143.39471170001\n",
      "    val_log_marginal: -12151.628443840122\n",
      "Train Epoch: 3267 [256/118836 (0%)] Loss: 12162.113281\n",
      "Train Epoch: 3267 [33024/118836 (28%)] Loss: 12304.064453\n",
      "Train Epoch: 3267 [65792/118836 (55%)] Loss: 12281.419922\n",
      "Train Epoch: 3267 [98560/118836 (83%)] Loss: 12298.438477\n",
      "    epoch          : 3267\n",
      "    loss           : 12221.817060361094\n",
      "    val_loss       : 12219.797824000594\n",
      "    val_log_likelihood: -12142.690438249587\n",
      "    val_log_marginal: -12150.889870952476\n",
      "Train Epoch: 3268 [256/118836 (0%)] Loss: 12203.257812\n",
      "Train Epoch: 3268 [33024/118836 (28%)] Loss: 12240.436523\n",
      "Train Epoch: 3268 [65792/118836 (55%)] Loss: 12188.593750\n",
      "Train Epoch: 3268 [98560/118836 (83%)] Loss: 12328.757812\n",
      "    epoch          : 3268\n",
      "    loss           : 12221.631293618177\n",
      "    val_loss       : 12217.653405294575\n",
      "    val_log_likelihood: -12143.15772219422\n",
      "    val_log_marginal: -12151.327758531674\n",
      "Train Epoch: 3269 [256/118836 (0%)] Loss: 12229.968750\n",
      "Train Epoch: 3269 [33024/118836 (28%)] Loss: 12273.458984\n",
      "Train Epoch: 3269 [65792/118836 (55%)] Loss: 12207.083984\n",
      "Train Epoch: 3269 [98560/118836 (83%)] Loss: 12262.689453\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 03269: reducing learning rate of group 0 to 1.0000e-04.\n",
      "    epoch          : 3269\n",
      "    loss           : 12220.604210446392\n",
      "    val_loss       : 12219.866983371396\n",
      "    val_log_likelihood: -12141.223766574907\n",
      "    val_log_marginal: -12149.379312043942\n",
      "Train Epoch: 3270 [256/118836 (0%)] Loss: 12204.395508\n",
      "Train Epoch: 3270 [33024/118836 (28%)] Loss: 12212.056641\n",
      "Train Epoch: 3270 [65792/118836 (55%)] Loss: 12178.478516\n",
      "Train Epoch: 3270 [98560/118836 (83%)] Loss: 12275.676758\n",
      "    epoch          : 3270\n",
      "    loss           : 12221.548198246226\n",
      "    val_loss       : 12221.854748904123\n",
      "    val_log_likelihood: -12144.380386037015\n",
      "    val_log_marginal: -12152.50293266331\n",
      "Train Epoch: 3271 [256/118836 (0%)] Loss: 12208.055664\n",
      "Train Epoch: 3271 [33024/118836 (28%)] Loss: 12338.313477\n",
      "Train Epoch: 3271 [65792/118836 (55%)] Loss: 12229.239258\n",
      "Train Epoch: 3271 [98560/118836 (83%)] Loss: 12234.268555\n",
      "    epoch          : 3271\n",
      "    loss           : 12221.289303853908\n",
      "    val_loss       : 12216.751719760927\n",
      "    val_log_likelihood: -12139.72304945978\n",
      "    val_log_marginal: -12147.793702512023\n",
      "Train Epoch: 3272 [256/118836 (0%)] Loss: 12291.140625\n",
      "Train Epoch: 3272 [33024/118836 (28%)] Loss: 12251.826172\n",
      "Train Epoch: 3272 [65792/118836 (55%)] Loss: 12214.431641\n",
      "Train Epoch: 3272 [98560/118836 (83%)] Loss: 12212.022461\n",
      "    epoch          : 3272\n",
      "    loss           : 12220.87423361249\n",
      "    val_loss       : 12219.629794544127\n",
      "    val_log_likelihood: -12143.19436824726\n",
      "    val_log_marginal: -12151.234388734914\n",
      "Train Epoch: 3273 [256/118836 (0%)] Loss: 12248.358398\n",
      "Train Epoch: 3273 [33024/118836 (28%)] Loss: 12234.379883\n",
      "Train Epoch: 3273 [65792/118836 (55%)] Loss: 12285.376953\n",
      "Train Epoch: 3273 [98560/118836 (83%)] Loss: 12320.674805\n",
      "    epoch          : 3273\n",
      "    loss           : 12218.400169303144\n",
      "    val_loss       : 12220.623752241472\n",
      "    val_log_likelihood: -12141.419864202078\n",
      "    val_log_marginal: -12149.444223986464\n",
      "Train Epoch: 3274 [256/118836 (0%)] Loss: 12199.256836\n",
      "Train Epoch: 3274 [33024/118836 (28%)] Loss: 12166.238281\n",
      "Train Epoch: 3274 [65792/118836 (55%)] Loss: 12401.539062\n",
      "Train Epoch: 3274 [98560/118836 (83%)] Loss: 12257.372070\n",
      "    epoch          : 3274\n",
      "    loss           : 12215.812653956007\n",
      "    val_loss       : 12214.785147031762\n",
      "    val_log_likelihood: -12141.741722562552\n",
      "    val_log_marginal: -12149.732187980735\n",
      "Train Epoch: 3275 [256/118836 (0%)] Loss: 12345.916016\n",
      "Train Epoch: 3275 [33024/118836 (28%)] Loss: 12187.375000\n",
      "Train Epoch: 3275 [65792/118836 (55%)] Loss: 12218.547852\n",
      "Train Epoch: 3275 [98560/118836 (83%)] Loss: 12167.959961\n",
      "    epoch          : 3275\n",
      "    loss           : 12219.404757773727\n",
      "    val_loss       : 12216.939322678618\n",
      "    val_log_likelihood: -12142.61121907956\n",
      "    val_log_marginal: -12150.61072677324\n",
      "Train Epoch: 3276 [256/118836 (0%)] Loss: 12323.517578\n",
      "Train Epoch: 3276 [33024/118836 (28%)] Loss: 12196.087891\n",
      "Train Epoch: 3276 [65792/118836 (55%)] Loss: 12254.889648\n",
      "Train Epoch: 3276 [98560/118836 (83%)] Loss: 12232.256836\n",
      "    epoch          : 3276\n",
      "    loss           : 12217.68563023418\n",
      "    val_loss       : 12219.244700238762\n",
      "    val_log_likelihood: -12141.750951683984\n",
      "    val_log_marginal: -12149.730083685481\n",
      "Train Epoch: 3277 [256/118836 (0%)] Loss: 12155.160156\n",
      "Train Epoch: 3277 [33024/118836 (28%)] Loss: 12259.242188\n",
      "Train Epoch: 3277 [65792/118836 (55%)] Loss: 12262.089844\n",
      "Train Epoch: 3277 [98560/118836 (83%)] Loss: 12246.791016\n",
      "    epoch          : 3277\n",
      "    loss           : 12218.16877584781\n",
      "    val_loss       : 12218.877301214121\n",
      "    val_log_likelihood: -12142.637880285878\n",
      "    val_log_marginal: -12150.610762072463\n",
      "Train Epoch: 3278 [256/118836 (0%)] Loss: 12285.826172\n",
      "Train Epoch: 3278 [33024/118836 (28%)] Loss: 12248.953125\n",
      "Train Epoch: 3278 [65792/118836 (55%)] Loss: 12262.377930\n",
      "Train Epoch: 3278 [98560/118836 (83%)] Loss: 12315.851562\n",
      "    epoch          : 3278\n",
      "    loss           : 12220.68655041615\n",
      "    val_loss       : 12220.665558459626\n",
      "    val_log_likelihood: -12141.7882881966\n",
      "    val_log_marginal: -12149.769693888185\n",
      "Train Epoch: 3279 [256/118836 (0%)] Loss: 12237.232422\n",
      "Train Epoch: 3279 [33024/118836 (28%)] Loss: 12295.363281\n",
      "Train Epoch: 3279 [65792/118836 (55%)] Loss: 12278.578125\n",
      "Train Epoch: 3279 [98560/118836 (83%)] Loss: 12206.646484\n",
      "    epoch          : 3279\n",
      "    loss           : 12217.299518907672\n",
      "    val_loss       : 12215.121084558257\n",
      "    val_log_likelihood: -12143.346862560744\n",
      "    val_log_marginal: -12151.31109230338\n",
      "Train Epoch: 3280 [256/118836 (0%)] Loss: 12179.099609\n",
      "Train Epoch: 3280 [33024/118836 (28%)] Loss: 12211.852539\n",
      "Train Epoch: 3280 [65792/118836 (55%)] Loss: 12336.593750\n",
      "Train Epoch: 3280 [98560/118836 (83%)] Loss: 12387.458984\n",
      "    epoch          : 3280\n",
      "    loss           : 12217.149526985111\n",
      "    val_loss       : 12220.488975689084\n",
      "    val_log_likelihood: -12141.170845126397\n",
      "    val_log_marginal: -12149.108905547859\n",
      "Train Epoch: 3281 [256/118836 (0%)] Loss: 12174.311523\n",
      "Train Epoch: 3281 [33024/118836 (28%)] Loss: 12279.303711\n",
      "Train Epoch: 3281 [65792/118836 (55%)] Loss: 12184.935547\n",
      "Train Epoch: 3281 [98560/118836 (83%)] Loss: 12174.936523\n",
      "    epoch          : 3281\n",
      "    loss           : 12217.927009182433\n",
      "    val_loss       : 12213.303656920802\n",
      "    val_log_likelihood: -12142.369039818548\n",
      "    val_log_marginal: -12150.325579062692\n",
      "Train Epoch: 3282 [256/118836 (0%)] Loss: 12177.515625\n",
      "Train Epoch: 3282 [33024/118836 (28%)] Loss: 12226.881836\n",
      "Train Epoch: 3282 [65792/118836 (55%)] Loss: 12191.794922\n",
      "Train Epoch: 3282 [98560/118836 (83%)] Loss: 12207.531250\n",
      "    epoch          : 3282\n",
      "    loss           : 12217.19523964149\n",
      "    val_loss       : 12218.466134950368\n",
      "    val_log_likelihood: -12142.438915329041\n",
      "    val_log_marginal: -12150.386969933086\n",
      "Train Epoch: 3283 [256/118836 (0%)] Loss: 12270.700195\n",
      "Train Epoch: 3283 [33024/118836 (28%)] Loss: 12223.143555\n",
      "Train Epoch: 3283 [65792/118836 (55%)] Loss: 12167.753906\n",
      "Train Epoch: 3283 [98560/118836 (83%)] Loss: 12204.545898\n",
      "    epoch          : 3283\n",
      "    loss           : 12217.072593730614\n",
      "    val_loss       : 12223.071066077911\n",
      "    val_log_likelihood: -12143.050746840107\n",
      "    val_log_marginal: -12151.014864313625\n",
      "Train Epoch: 3284 [256/118836 (0%)] Loss: 12318.157227\n",
      "Train Epoch: 3284 [33024/118836 (28%)] Loss: 12265.644531\n",
      "Train Epoch: 3284 [65792/118836 (55%)] Loss: 12271.693359\n",
      "Train Epoch: 3284 [98560/118836 (83%)] Loss: 12196.881836\n",
      "    epoch          : 3284\n",
      "    loss           : 12219.931565989455\n",
      "    val_loss       : 12220.81708645811\n",
      "    val_log_likelihood: -12141.207014610474\n",
      "    val_log_marginal: -12149.168587344771\n",
      "Train Epoch: 3285 [256/118836 (0%)] Loss: 12261.855469\n",
      "Train Epoch: 3285 [33024/118836 (28%)] Loss: 12269.466797\n",
      "Train Epoch: 3285 [65792/118836 (55%)] Loss: 12217.574219\n",
      "Train Epoch: 3285 [98560/118836 (83%)] Loss: 12188.858398\n",
      "    epoch          : 3285\n",
      "    loss           : 12218.616469415581\n",
      "    val_loss       : 12219.947880725276\n",
      "    val_log_likelihood: -12143.285237509046\n",
      "    val_log_marginal: -12151.24467857884\n",
      "Train Epoch: 3286 [256/118836 (0%)] Loss: 12183.621094\n",
      "Train Epoch: 3286 [33024/118836 (28%)] Loss: 12291.821289\n",
      "Train Epoch: 3286 [65792/118836 (55%)] Loss: 12281.108398\n",
      "Train Epoch: 3286 [98560/118836 (83%)] Loss: 12239.931641\n",
      "    epoch          : 3286\n",
      "    loss           : 12216.864197231698\n",
      "    val_loss       : 12221.846109353455\n",
      "    val_log_likelihood: -12141.27784358199\n",
      "    val_log_marginal: -12149.254890117061\n",
      "Train Epoch: 3287 [256/118836 (0%)] Loss: 12285.041992\n",
      "Train Epoch: 3287 [33024/118836 (28%)] Loss: 12260.736328\n",
      "Train Epoch: 3287 [65792/118836 (55%)] Loss: 12222.334961\n",
      "Train Epoch: 3287 [98560/118836 (83%)] Loss: 12261.728516\n",
      "    epoch          : 3287\n",
      "    loss           : 12222.554208346259\n",
      "    val_loss       : 12218.695600313102\n",
      "    val_log_likelihood: -12142.02517495735\n",
      "    val_log_marginal: -12149.982010835462\n",
      "Train Epoch: 3288 [256/118836 (0%)] Loss: 12295.592773\n",
      "Train Epoch: 3288 [33024/118836 (28%)] Loss: 12140.571289\n",
      "Train Epoch: 3288 [65792/118836 (55%)] Loss: 12264.781250\n",
      "Train Epoch: 3288 [98560/118836 (83%)] Loss: 12354.493164\n",
      "    epoch          : 3288\n",
      "    loss           : 12219.465875885287\n",
      "    val_loss       : 12219.894989866474\n",
      "    val_log_likelihood: -12140.90859391155\n",
      "    val_log_marginal: -12148.854909229294\n",
      "Train Epoch: 3289 [256/118836 (0%)] Loss: 12254.053711\n",
      "Train Epoch: 3289 [33024/118836 (28%)] Loss: 12213.346680\n",
      "Train Epoch: 3289 [65792/118836 (55%)] Loss: 12294.468750\n",
      "Train Epoch: 3289 [98560/118836 (83%)] Loss: 12295.339844\n",
      "    epoch          : 3289\n",
      "    loss           : 12219.241304958901\n",
      "    val_loss       : 12214.154074456132\n",
      "    val_log_likelihood: -12142.13888172689\n",
      "    val_log_marginal: -12150.101773492008\n",
      "Train Epoch: 3290 [256/118836 (0%)] Loss: 12281.745117\n",
      "Train Epoch: 3290 [33024/118836 (28%)] Loss: 12215.580078\n",
      "Train Epoch: 3290 [65792/118836 (55%)] Loss: 12205.811523\n",
      "Train Epoch: 3290 [98560/118836 (83%)] Loss: 12261.201172\n",
      "    epoch          : 3290\n",
      "    loss           : 12216.276581724307\n",
      "    val_loss       : 12219.522751253764\n",
      "    val_log_likelihood: -12143.156511062862\n",
      "    val_log_marginal: -12151.12066369466\n",
      "Train Epoch: 3291 [256/118836 (0%)] Loss: 12304.269531\n",
      "Train Epoch: 3291 [33024/118836 (28%)] Loss: 12211.550781\n",
      "Train Epoch: 3291 [65792/118836 (55%)] Loss: 12288.773438\n",
      "Train Epoch: 3291 [98560/118836 (83%)] Loss: 12202.968750\n",
      "    epoch          : 3291\n",
      "    loss           : 12216.284900518249\n",
      "    val_loss       : 12212.005823038135\n",
      "    val_log_likelihood: -12142.670821055624\n",
      "    val_log_marginal: -12150.615885472675\n",
      "Train Epoch: 3292 [256/118836 (0%)] Loss: 12127.541992\n",
      "Train Epoch: 3292 [33024/118836 (28%)] Loss: 12180.818359\n",
      "Train Epoch: 3292 [65792/118836 (55%)] Loss: 12259.195312\n",
      "Train Epoch: 3292 [98560/118836 (83%)] Loss: 12238.385742\n",
      "    epoch          : 3292\n",
      "    loss           : 12215.371706343052\n",
      "    val_loss       : 12216.184835417025\n",
      "    val_log_likelihood: -12142.741177819995\n",
      "    val_log_marginal: -12150.695005201198\n",
      "Train Epoch: 3293 [256/118836 (0%)] Loss: 12242.960938\n",
      "Train Epoch: 3293 [33024/118836 (28%)] Loss: 12234.532227\n",
      "Train Epoch: 3293 [65792/118836 (55%)] Loss: 12199.624023\n",
      "Train Epoch: 3293 [98560/118836 (83%)] Loss: 12372.915039\n",
      "    epoch          : 3293\n",
      "    loss           : 12219.930754368279\n",
      "    val_loss       : 12218.284755369952\n",
      "    val_log_likelihood: -12142.312705005428\n",
      "    val_log_marginal: -12150.257694218624\n",
      "Train Epoch: 3294 [256/118836 (0%)] Loss: 12288.705078\n",
      "Train Epoch: 3294 [33024/118836 (28%)] Loss: 12175.601562\n",
      "Train Epoch: 3294 [65792/118836 (55%)] Loss: 12332.548828\n",
      "Train Epoch: 3294 [98560/118836 (83%)] Loss: 12248.576172\n",
      "    epoch          : 3294\n",
      "    loss           : 12220.755315601738\n",
      "    val_loss       : 12222.809565682912\n",
      "    val_log_likelihood: -12142.036846212004\n",
      "    val_log_marginal: -12149.984244186886\n",
      "Train Epoch: 3295 [256/118836 (0%)] Loss: 12122.764648\n",
      "Train Epoch: 3295 [33024/118836 (28%)] Loss: 12191.994141\n",
      "Train Epoch: 3295 [65792/118836 (55%)] Loss: 12165.158203\n",
      "Train Epoch: 3295 [98560/118836 (83%)] Loss: 12213.183594\n",
      "    epoch          : 3295\n",
      "    loss           : 12217.052371051748\n",
      "    val_loss       : 12216.966794895543\n",
      "    val_log_likelihood: -12142.84735835401\n",
      "    val_log_marginal: -12150.78232049456\n",
      "Train Epoch: 3296 [256/118836 (0%)] Loss: 12335.462891\n",
      "Train Epoch: 3296 [33024/118836 (28%)] Loss: 12151.029297\n",
      "Train Epoch: 3296 [65792/118836 (55%)] Loss: 12148.193359\n",
      "Train Epoch: 3296 [98560/118836 (83%)] Loss: 12303.012695\n",
      "    epoch          : 3296\n",
      "    loss           : 12218.681080212209\n",
      "    val_loss       : 12217.733553371781\n",
      "    val_log_likelihood: -12141.460364809502\n",
      "    val_log_marginal: -12149.416194466405\n",
      "Train Epoch: 3297 [256/118836 (0%)] Loss: 12262.427734\n",
      "Train Epoch: 3297 [33024/118836 (28%)] Loss: 12302.515625\n",
      "Train Epoch: 3297 [65792/118836 (55%)] Loss: 12158.864258\n",
      "Train Epoch: 3297 [98560/118836 (83%)] Loss: 12286.008789\n",
      "    epoch          : 3297\n",
      "    loss           : 12218.579956155654\n",
      "    val_loss       : 12218.959481245052\n",
      "    val_log_likelihood: -12143.255516245348\n",
      "    val_log_marginal: -12151.194360954752\n",
      "Train Epoch: 3298 [256/118836 (0%)] Loss: 12287.892578\n",
      "Train Epoch: 3298 [33024/118836 (28%)] Loss: 12352.682617\n",
      "Train Epoch: 3298 [65792/118836 (55%)] Loss: 12263.406250\n",
      "Train Epoch: 3298 [98560/118836 (83%)] Loss: 12165.939453\n",
      "    epoch          : 3298\n",
      "    loss           : 12217.550209528794\n",
      "    val_loss       : 12215.722483951653\n",
      "    val_log_likelihood: -12143.453219182951\n",
      "    val_log_marginal: -12151.385955828817\n",
      "Train Epoch: 3299 [256/118836 (0%)] Loss: 12265.146484\n",
      "Train Epoch: 3299 [33024/118836 (28%)] Loss: 12329.746094\n",
      "Train Epoch: 3299 [65792/118836 (55%)] Loss: 12261.135742\n",
      "Train Epoch: 3299 [98560/118836 (83%)] Loss: 12145.099609\n",
      "    epoch          : 3299\n",
      "    loss           : 12220.578890418217\n",
      "    val_loss       : 12219.28562861932\n",
      "    val_log_likelihood: -12142.655984252222\n",
      "    val_log_marginal: -12150.574854243647\n",
      "Train Epoch: 3300 [256/118836 (0%)] Loss: 12255.880859\n",
      "Train Epoch: 3300 [33024/118836 (28%)] Loss: 12134.697266\n",
      "Train Epoch: 3300 [65792/118836 (55%)] Loss: 12273.867188\n",
      "Train Epoch: 3300 [98560/118836 (83%)] Loss: 12236.273438\n",
      "    epoch          : 3300\n",
      "    loss           : 12219.756641594293\n",
      "    val_loss       : 12216.511084893265\n",
      "    val_log_likelihood: -12143.352166207867\n",
      "    val_log_marginal: -12151.261267968328\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch3300.pth ...\n",
      "Train Epoch: 3301 [256/118836 (0%)] Loss: 12266.269531\n",
      "Train Epoch: 3301 [33024/118836 (28%)] Loss: 12277.078125\n",
      "Train Epoch: 3301 [65792/118836 (55%)] Loss: 12305.878906\n",
      "Train Epoch: 3301 [98560/118836 (83%)] Loss: 12286.916016\n",
      "    epoch          : 3301\n",
      "    loss           : 12219.420211693548\n",
      "    val_loss       : 12216.799230528899\n",
      "    val_log_likelihood: -12142.680294438844\n",
      "    val_log_marginal: -12150.603682786243\n",
      "Train Epoch: 3302 [256/118836 (0%)] Loss: 12254.149414\n",
      "Train Epoch: 3302 [33024/118836 (28%)] Loss: 12265.376953\n",
      "Train Epoch: 3302 [65792/118836 (55%)] Loss: 12329.253906\n",
      "Train Epoch: 3302 [98560/118836 (83%)] Loss: 12307.778320\n",
      "    epoch          : 3302\n",
      "    loss           : 12216.575733431555\n",
      "    val_loss       : 12222.954621208171\n",
      "    val_log_likelihood: -12142.635338638596\n",
      "    val_log_marginal: -12150.55033372065\n",
      "Train Epoch: 3303 [256/118836 (0%)] Loss: 12168.950195\n",
      "Train Epoch: 3303 [33024/118836 (28%)] Loss: 12189.926758\n",
      "Train Epoch: 3303 [65792/118836 (55%)] Loss: 12303.607422\n",
      "Train Epoch: 3303 [98560/118836 (83%)] Loss: 12299.086914\n",
      "    epoch          : 3303\n",
      "    loss           : 12218.926876066222\n",
      "    val_loss       : 12216.963144335066\n",
      "    val_log_likelihood: -12141.95463499664\n",
      "    val_log_marginal: -12149.888603905934\n",
      "Train Epoch: 3304 [256/118836 (0%)] Loss: 12229.604492\n",
      "Train Epoch: 3304 [33024/118836 (28%)] Loss: 12177.168945\n",
      "Train Epoch: 3304 [65792/118836 (55%)] Loss: 12166.306641\n",
      "Train Epoch: 3304 [98560/118836 (83%)] Loss: 12247.427734\n",
      "    epoch          : 3304\n",
      "    loss           : 12217.801091908343\n",
      "    val_loss       : 12221.119258103918\n",
      "    val_log_likelihood: -12141.219770503772\n",
      "    val_log_marginal: -12149.158442088845\n",
      "Train Epoch: 3305 [256/118836 (0%)] Loss: 12302.057617\n",
      "Train Epoch: 3305 [33024/118836 (28%)] Loss: 12233.923828\n",
      "Train Epoch: 3305 [65792/118836 (55%)] Loss: 12173.386719\n",
      "Train Epoch: 3305 [98560/118836 (83%)] Loss: 12300.359375\n",
      "    epoch          : 3305\n",
      "    loss           : 12220.20123778691\n",
      "    val_loss       : 12220.925452210446\n",
      "    val_log_likelihood: -12142.249447987748\n",
      "    val_log_marginal: -12150.193305293858\n",
      "Train Epoch: 3306 [256/118836 (0%)] Loss: 12205.795898\n",
      "Train Epoch: 3306 [33024/118836 (28%)] Loss: 12234.323242\n",
      "Train Epoch: 3306 [65792/118836 (55%)] Loss: 12207.783203\n",
      "Train Epoch: 3306 [98560/118836 (83%)] Loss: 12343.855469\n",
      "    epoch          : 3306\n",
      "    loss           : 12218.547449144437\n",
      "    val_loss       : 12217.054565166618\n",
      "    val_log_likelihood: -12141.651313714847\n",
      "    val_log_marginal: -12149.626311778184\n",
      "Train Epoch: 3307 [256/118836 (0%)] Loss: 12180.702148\n",
      "Train Epoch: 3307 [33024/118836 (28%)] Loss: 12207.094727\n",
      "Train Epoch: 3307 [65792/118836 (55%)] Loss: 12150.213867\n",
      "Train Epoch: 3307 [98560/118836 (83%)] Loss: 12215.861328\n",
      "    epoch          : 3307\n",
      "    loss           : 12221.380209302626\n",
      "    val_loss       : 12219.131578880428\n",
      "    val_log_likelihood: -12143.901161374328\n",
      "    val_log_marginal: -12151.862031459801\n",
      "Train Epoch: 3308 [256/118836 (0%)] Loss: 12386.595703\n",
      "Train Epoch: 3308 [33024/118836 (28%)] Loss: 12298.154297\n",
      "Train Epoch: 3308 [65792/118836 (55%)] Loss: 12327.301758\n",
      "Train Epoch: 3308 [98560/118836 (83%)] Loss: 12302.303711\n",
      "    epoch          : 3308\n",
      "    loss           : 12220.345408944633\n",
      "    val_loss       : 12217.881113239859\n",
      "    val_log_likelihood: -12142.709458036084\n",
      "    val_log_marginal: -12150.668172535572\n",
      "Train Epoch: 3309 [256/118836 (0%)] Loss: 12189.103516\n",
      "Train Epoch: 3309 [33024/118836 (28%)] Loss: 12181.322266\n",
      "Train Epoch: 3309 [65792/118836 (55%)] Loss: 12288.477539\n",
      "Train Epoch: 3309 [98560/118836 (83%)] Loss: 12193.872070\n",
      "    epoch          : 3309\n",
      "    loss           : 12219.107123171269\n",
      "    val_loss       : 12218.407753741822\n",
      "    val_log_likelihood: -12142.294444659843\n",
      "    val_log_marginal: -12150.258037062717\n",
      "Train Epoch: 3310 [256/118836 (0%)] Loss: 12268.549805\n",
      "Train Epoch: 3310 [33024/118836 (28%)] Loss: 12426.933594\n",
      "Train Epoch: 3310 [65792/118836 (55%)] Loss: 12264.167969\n",
      "Train Epoch: 3310 [98560/118836 (83%)] Loss: 12178.330078\n",
      "    epoch          : 3310\n",
      "    loss           : 12218.327311763338\n",
      "    val_loss       : 12218.255108769832\n",
      "    val_log_likelihood: -12142.648750581577\n",
      "    val_log_marginal: -12150.606657348986\n",
      "Train Epoch: 3311 [256/118836 (0%)] Loss: 12268.657227\n",
      "Train Epoch: 3311 [33024/118836 (28%)] Loss: 12302.415039\n",
      "Train Epoch: 3311 [65792/118836 (55%)] Loss: 12191.820312\n",
      "Train Epoch: 3311 [98560/118836 (83%)] Loss: 12147.182617\n",
      "    epoch          : 3311\n",
      "    loss           : 12217.926461531999\n",
      "    val_loss       : 12220.416952215988\n",
      "    val_log_likelihood: -12141.282206207352\n",
      "    val_log_marginal: -12149.228076471636\n",
      "Train Epoch: 3312 [256/118836 (0%)] Loss: 12182.514648\n",
      "Train Epoch: 3312 [33024/118836 (28%)] Loss: 12323.266602\n",
      "Train Epoch: 3312 [65792/118836 (55%)] Loss: 12168.332031\n",
      "Train Epoch: 3312 [98560/118836 (83%)] Loss: 12218.250000\n",
      "    epoch          : 3312\n",
      "    loss           : 12216.174878515301\n",
      "    val_loss       : 12219.679920705728\n",
      "    val_log_likelihood: -12142.170646421371\n",
      "    val_log_marginal: -12150.166919308234\n",
      "Train Epoch: 3313 [256/118836 (0%)] Loss: 12169.320312\n",
      "Train Epoch: 3313 [33024/118836 (28%)] Loss: 12348.199219\n",
      "Train Epoch: 3313 [65792/118836 (55%)] Loss: 12266.386719\n",
      "Train Epoch: 3313 [98560/118836 (83%)] Loss: 12330.348633\n",
      "    epoch          : 3313\n",
      "    loss           : 12220.715432595378\n",
      "    val_loss       : 12219.275309624125\n",
      "    val_log_likelihood: -12140.957080522385\n",
      "    val_log_marginal: -12148.922649896795\n",
      "Train Epoch: 3314 [256/118836 (0%)] Loss: 12343.099609\n",
      "Train Epoch: 3314 [33024/118836 (28%)] Loss: 12244.421875\n",
      "Train Epoch: 3314 [65792/118836 (55%)] Loss: 12175.750000\n",
      "Train Epoch: 3314 [98560/118836 (83%)] Loss: 12173.023438\n",
      "    epoch          : 3314\n",
      "    loss           : 12216.537511954612\n",
      "    val_loss       : 12219.066059670959\n",
      "    val_log_likelihood: -12142.540630654208\n",
      "    val_log_marginal: -12150.466803274578\n",
      "Train Epoch: 3315 [256/118836 (0%)] Loss: 12288.665039\n",
      "Train Epoch: 3315 [33024/118836 (28%)] Loss: 12219.934570\n",
      "Train Epoch: 3315 [65792/118836 (55%)] Loss: 12295.347656\n",
      "Train Epoch: 3315 [98560/118836 (83%)] Loss: 12211.652344\n",
      "    epoch          : 3315\n",
      "    loss           : 12219.978798981596\n",
      "    val_loss       : 12216.112217543883\n",
      "    val_log_likelihood: -12140.057148211332\n",
      "    val_log_marginal: -12147.995954091319\n",
      "Train Epoch: 3316 [256/118836 (0%)] Loss: 12211.747070\n",
      "Train Epoch: 3316 [33024/118836 (28%)] Loss: 12157.822266\n",
      "Train Epoch: 3316 [65792/118836 (55%)] Loss: 12270.447266\n",
      "Train Epoch: 3316 [98560/118836 (83%)] Loss: 12213.887695\n",
      "    epoch          : 3316\n",
      "    loss           : 12221.288514849566\n",
      "    val_loss       : 12216.04842346915\n",
      "    val_log_likelihood: -12142.222196159015\n",
      "    val_log_marginal: -12150.182014258891\n",
      "Train Epoch: 3317 [256/118836 (0%)] Loss: 12216.991211\n",
      "Train Epoch: 3317 [33024/118836 (28%)] Loss: 12264.106445\n",
      "Train Epoch: 3317 [65792/118836 (55%)] Loss: 12192.012695\n",
      "Train Epoch: 3317 [98560/118836 (83%)] Loss: 12276.064453\n",
      "    epoch          : 3317\n",
      "    loss           : 12220.696434779516\n",
      "    val_loss       : 12215.35951685569\n",
      "    val_log_likelihood: -12141.253080897177\n",
      "    val_log_marginal: -12149.191732657124\n",
      "Train Epoch: 3318 [256/118836 (0%)] Loss: 12207.483398\n",
      "Train Epoch: 3318 [33024/118836 (28%)] Loss: 12251.029297\n",
      "Train Epoch: 3318 [65792/118836 (55%)] Loss: 12254.777344\n",
      "Train Epoch: 3318 [98560/118836 (83%)] Loss: 12224.504883\n",
      "    epoch          : 3318\n",
      "    loss           : 12216.208767253413\n",
      "    val_loss       : 12220.005614120744\n",
      "    val_log_likelihood: -12141.831703305936\n",
      "    val_log_marginal: -12149.769851241586\n",
      "Train Epoch: 3319 [256/118836 (0%)] Loss: 12253.814453\n",
      "Train Epoch: 3319 [33024/118836 (28%)] Loss: 12298.710938\n",
      "Train Epoch: 3319 [65792/118836 (55%)] Loss: 12284.824219\n",
      "Train Epoch: 3319 [98560/118836 (83%)] Loss: 12205.405273\n",
      "    epoch          : 3319\n",
      "    loss           : 12218.337415186881\n",
      "    val_loss       : 12220.718526723105\n",
      "    val_log_likelihood: -12143.193490229529\n",
      "    val_log_marginal: -12151.124131729966\n",
      "Train Epoch: 3320 [256/118836 (0%)] Loss: 12169.988281\n",
      "Train Epoch: 3320 [33024/118836 (28%)] Loss: 12174.817383\n",
      "Train Epoch: 3320 [65792/118836 (55%)] Loss: 12238.688477\n",
      "Train Epoch: 3320 [98560/118836 (83%)] Loss: 12256.992188\n",
      "    epoch          : 3320\n",
      "    loss           : 12219.343652747622\n",
      "    val_loss       : 12218.212631799634\n",
      "    val_log_likelihood: -12142.80160272565\n",
      "    val_log_marginal: -12150.740322863736\n",
      "Train Epoch: 3321 [256/118836 (0%)] Loss: 12249.822266\n",
      "Train Epoch: 3321 [33024/118836 (28%)] Loss: 12258.886719\n",
      "Train Epoch: 3321 [65792/118836 (55%)] Loss: 12177.212891\n",
      "Train Epoch: 3321 [98560/118836 (83%)] Loss: 12154.671875\n",
      "    epoch          : 3321\n",
      "    loss           : 12217.915164101272\n",
      "    val_loss       : 12217.603246546643\n",
      "    val_log_likelihood: -12142.154060044459\n",
      "    val_log_marginal: -12150.08161952209\n",
      "Train Epoch: 3322 [256/118836 (0%)] Loss: 12207.602539\n",
      "Train Epoch: 3322 [33024/118836 (28%)] Loss: 12277.935547\n",
      "Train Epoch: 3322 [65792/118836 (55%)] Loss: 12361.697266\n",
      "Train Epoch: 3322 [98560/118836 (83%)] Loss: 12267.272461\n",
      "    epoch          : 3322\n",
      "    loss           : 12218.569275518248\n",
      "    val_loss       : 12215.956019465071\n",
      "    val_log_likelihood: -12143.059412802419\n",
      "    val_log_marginal: -12150.98890239331\n",
      "Train Epoch: 3323 [256/118836 (0%)] Loss: 12319.213867\n",
      "Train Epoch: 3323 [33024/118836 (28%)] Loss: 12187.665039\n",
      "Train Epoch: 3323 [65792/118836 (55%)] Loss: 12250.986328\n",
      "Train Epoch: 3323 [98560/118836 (83%)] Loss: 12172.034180\n",
      "    epoch          : 3323\n",
      "    loss           : 12220.908015243745\n",
      "    val_loss       : 12221.372710796812\n",
      "    val_log_likelihood: -12144.021890993332\n",
      "    val_log_marginal: -12151.964738833136\n",
      "Train Epoch: 3324 [256/118836 (0%)] Loss: 12199.909180\n",
      "Train Epoch: 3324 [33024/118836 (28%)] Loss: 12284.763672\n",
      "Train Epoch: 3324 [65792/118836 (55%)] Loss: 12238.650391\n",
      "Train Epoch: 3324 [98560/118836 (83%)] Loss: 12262.258789\n",
      "    epoch          : 3324\n",
      "    loss           : 12217.937743615592\n",
      "    val_loss       : 12217.900191306735\n",
      "    val_log_likelihood: -12143.017838703216\n",
      "    val_log_marginal: -12150.949617471322\n",
      "Train Epoch: 3325 [256/118836 (0%)] Loss: 12185.965820\n",
      "Train Epoch: 3325 [33024/118836 (28%)] Loss: 12299.583008\n",
      "Train Epoch: 3325 [65792/118836 (55%)] Loss: 12277.335938\n",
      "Train Epoch: 3325 [98560/118836 (83%)] Loss: 12251.900391\n",
      "    epoch          : 3325\n",
      "    loss           : 12218.820255473274\n",
      "    val_loss       : 12219.362085361045\n",
      "    val_log_likelihood: -12140.71516571676\n",
      "    val_log_marginal: -12148.637340088166\n",
      "Train Epoch: 3326 [256/118836 (0%)] Loss: 12230.908203\n",
      "Train Epoch: 3326 [33024/118836 (28%)] Loss: 12186.440430\n",
      "Train Epoch: 3326 [65792/118836 (55%)] Loss: 12190.189453\n",
      "Train Epoch: 3326 [98560/118836 (83%)] Loss: 12226.444336\n",
      "    epoch          : 3326\n",
      "    loss           : 12213.645753366676\n",
      "    val_loss       : 12220.232617007687\n",
      "    val_log_likelihood: -12142.749626983818\n",
      "    val_log_marginal: -12150.704537124278\n",
      "Train Epoch: 3327 [256/118836 (0%)] Loss: 12234.861328\n",
      "Train Epoch: 3327 [33024/118836 (28%)] Loss: 12220.439453\n",
      "Train Epoch: 3327 [65792/118836 (55%)] Loss: 12277.063477\n",
      "Train Epoch: 3327 [98560/118836 (83%)] Loss: 12154.751953\n",
      "    epoch          : 3327\n",
      "    loss           : 12219.3148033628\n",
      "    val_loss       : 12217.446096171243\n",
      "    val_log_likelihood: -12142.522824260752\n",
      "    val_log_marginal: -12150.481429530493\n",
      "Train Epoch: 3328 [256/118836 (0%)] Loss: 12171.609375\n",
      "Train Epoch: 3328 [33024/118836 (28%)] Loss: 12242.500977\n",
      "Train Epoch: 3328 [65792/118836 (55%)] Loss: 12207.408203\n",
      "Train Epoch: 3328 [98560/118836 (83%)] Loss: 12370.166992\n",
      "    epoch          : 3328\n",
      "    loss           : 12214.413758465156\n",
      "    val_loss       : 12217.902848629774\n",
      "    val_log_likelihood: -12141.718315918373\n",
      "    val_log_marginal: -12149.664129419221\n",
      "Train Epoch: 3329 [256/118836 (0%)] Loss: 12197.836914\n",
      "Train Epoch: 3329 [33024/118836 (28%)] Loss: 12370.146484\n",
      "Train Epoch: 3329 [65792/118836 (55%)] Loss: 12172.393555\n",
      "Train Epoch: 3329 [98560/118836 (83%)] Loss: 12201.402344\n",
      "    epoch          : 3329\n",
      "    loss           : 12221.174181916873\n",
      "    val_loss       : 12219.69590293346\n",
      "    val_log_likelihood: -12142.09992181038\n",
      "    val_log_marginal: -12150.049533298792\n",
      "Train Epoch: 3330 [256/118836 (0%)] Loss: 12331.210938\n",
      "Train Epoch: 3330 [33024/118836 (28%)] Loss: 12189.099609\n",
      "Train Epoch: 3330 [65792/118836 (55%)] Loss: 12229.508789\n",
      "Train Epoch: 3330 [98560/118836 (83%)] Loss: 12176.291016\n",
      "    epoch          : 3330\n",
      "    loss           : 12220.20886046707\n",
      "    val_loss       : 12217.261640475104\n",
      "    val_log_likelihood: -12142.152745844964\n",
      "    val_log_marginal: -12150.09598070417\n",
      "Train Epoch: 3331 [256/118836 (0%)] Loss: 12221.236328\n",
      "Train Epoch: 3331 [33024/118836 (28%)] Loss: 12266.353516\n",
      "Train Epoch: 3331 [65792/118836 (55%)] Loss: 12178.917969\n",
      "Train Epoch: 3331 [98560/118836 (83%)] Loss: 12229.594727\n",
      "    epoch          : 3331\n",
      "    loss           : 12219.25391561983\n",
      "    val_loss       : 12215.273564789853\n",
      "    val_log_likelihood: -12142.604174421009\n",
      "    val_log_marginal: -12150.527764651692\n",
      "Train Epoch: 3332 [256/118836 (0%)] Loss: 12220.613281\n",
      "Train Epoch: 3332 [33024/118836 (28%)] Loss: 12265.291016\n",
      "Train Epoch: 3332 [65792/118836 (55%)] Loss: 12250.357422\n",
      "Train Epoch: 3332 [98560/118836 (83%)] Loss: 12189.087891\n",
      "    epoch          : 3332\n",
      "    loss           : 12217.237804681039\n",
      "    val_loss       : 12219.477248390953\n",
      "    val_log_likelihood: -12141.559690989454\n",
      "    val_log_marginal: -12149.483549046347\n",
      "Train Epoch: 3333 [256/118836 (0%)] Loss: 12339.759766\n",
      "Train Epoch: 3333 [33024/118836 (28%)] Loss: 12276.126953\n",
      "Train Epoch: 3333 [65792/118836 (55%)] Loss: 12338.057617\n",
      "Train Epoch: 3333 [98560/118836 (83%)] Loss: 12171.321289\n",
      "    epoch          : 3333\n",
      "    loss           : 12216.764782684553\n",
      "    val_loss       : 12217.439860869421\n",
      "    val_log_likelihood: -12142.040393015923\n",
      "    val_log_marginal: -12149.958887972665\n",
      "Train Epoch: 3334 [256/118836 (0%)] Loss: 12177.724609\n",
      "Train Epoch: 3334 [33024/118836 (28%)] Loss: 12208.876953\n",
      "Train Epoch: 3334 [65792/118836 (55%)] Loss: 12261.129883\n",
      "Train Epoch: 3334 [98560/118836 (83%)] Loss: 12297.026367\n",
      "    epoch          : 3334\n",
      "    loss           : 12218.024955250981\n",
      "    val_loss       : 12217.998746485588\n",
      "    val_log_likelihood: -12140.751576070099\n",
      "    val_log_marginal: -12148.68200493797\n",
      "Train Epoch: 3335 [256/118836 (0%)] Loss: 12217.807617\n",
      "Train Epoch: 3335 [33024/118836 (28%)] Loss: 12207.023438\n",
      "Train Epoch: 3335 [65792/118836 (55%)] Loss: 12142.494141\n",
      "Train Epoch: 3335 [98560/118836 (83%)] Loss: 12241.945312\n",
      "    epoch          : 3335\n",
      "    loss           : 12219.377594796837\n",
      "    val_loss       : 12217.967098830655\n",
      "    val_log_likelihood: -12142.257467270214\n",
      "    val_log_marginal: -12150.172745717246\n",
      "Train Epoch: 3336 [256/118836 (0%)] Loss: 12172.430664\n",
      "Train Epoch: 3336 [33024/118836 (28%)] Loss: 12195.767578\n",
      "Train Epoch: 3336 [65792/118836 (55%)] Loss: 12180.535156\n",
      "Train Epoch: 3336 [98560/118836 (83%)] Loss: 12232.208008\n",
      "    epoch          : 3336\n",
      "    loss           : 12221.398037666719\n",
      "    val_loss       : 12219.96108241421\n",
      "    val_log_likelihood: -12141.672970947064\n",
      "    val_log_marginal: -12149.595419808513\n",
      "Train Epoch: 3337 [256/118836 (0%)] Loss: 12251.819336\n",
      "Train Epoch: 3337 [33024/118836 (28%)] Loss: 12134.383789\n",
      "Train Epoch: 3337 [65792/118836 (55%)] Loss: 12169.892578\n",
      "Train Epoch: 3337 [98560/118836 (83%)] Loss: 12236.396484\n",
      "    epoch          : 3337\n",
      "    loss           : 12215.94624996769\n",
      "    val_loss       : 12219.911964467592\n",
      "    val_log_likelihood: -12142.835182905552\n",
      "    val_log_marginal: -12150.76016046967\n",
      "Train Epoch: 3338 [256/118836 (0%)] Loss: 12266.164062\n",
      "Train Epoch: 3338 [33024/118836 (28%)] Loss: 12350.753906\n",
      "Train Epoch: 3338 [65792/118836 (55%)] Loss: 12304.922852\n",
      "Train Epoch: 3338 [98560/118836 (83%)] Loss: 12202.441406\n",
      "    epoch          : 3338\n",
      "    loss           : 12217.733388744571\n",
      "    val_loss       : 12215.997415203621\n",
      "    val_log_likelihood: -12142.216975386425\n",
      "    val_log_marginal: -12150.147993351133\n",
      "Train Epoch: 3339 [256/118836 (0%)] Loss: 12274.424805\n",
      "Train Epoch: 3339 [33024/118836 (28%)] Loss: 12306.920898\n",
      "Train Epoch: 3339 [65792/118836 (55%)] Loss: 12225.810547\n",
      "Train Epoch: 3339 [98560/118836 (83%)] Loss: 12204.199219\n",
      "    epoch          : 3339\n",
      "    loss           : 12213.527115966192\n",
      "    val_loss       : 12220.582250196963\n",
      "    val_log_likelihood: -12141.845790038255\n",
      "    val_log_marginal: -12149.767105629953\n",
      "Train Epoch: 3340 [256/118836 (0%)] Loss: 12180.350586\n",
      "Train Epoch: 3340 [33024/118836 (28%)] Loss: 12167.310547\n",
      "Train Epoch: 3340 [65792/118836 (55%)] Loss: 12197.483398\n",
      "Train Epoch: 3340 [98560/118836 (83%)] Loss: 12190.861328\n",
      "    epoch          : 3340\n",
      "    loss           : 12219.159864654413\n",
      "    val_loss       : 12223.20245270663\n",
      "    val_log_likelihood: -12141.880832880996\n",
      "    val_log_marginal: -12149.798247722832\n",
      "Train Epoch: 3341 [256/118836 (0%)] Loss: 12190.165039\n",
      "Train Epoch: 3341 [33024/118836 (28%)] Loss: 12171.638672\n",
      "Train Epoch: 3341 [65792/118836 (55%)] Loss: 12309.054688\n",
      "Train Epoch: 3341 [98560/118836 (83%)] Loss: 12236.238281\n",
      "    epoch          : 3341\n",
      "    loss           : 12214.433225257186\n",
      "    val_loss       : 12212.68022426047\n",
      "    val_log_likelihood: -12142.997724585142\n",
      "    val_log_marginal: -12150.917904132864\n",
      "Train Epoch: 3342 [256/118836 (0%)] Loss: 12291.266602\n",
      "Train Epoch: 3342 [33024/118836 (28%)] Loss: 12334.487305\n",
      "Train Epoch: 3342 [65792/118836 (55%)] Loss: 12187.589844\n",
      "Train Epoch: 3342 [98560/118836 (83%)] Loss: 12141.006836\n",
      "    epoch          : 3342\n",
      "    loss           : 12221.814341656327\n",
      "    val_loss       : 12219.766406298117\n",
      "    val_log_likelihood: -12142.358000058159\n",
      "    val_log_marginal: -12150.28894321577\n",
      "Train Epoch: 3343 [256/118836 (0%)] Loss: 12236.173828\n",
      "Train Epoch: 3343 [33024/118836 (28%)] Loss: 12171.634766\n",
      "Train Epoch: 3343 [65792/118836 (55%)] Loss: 12193.340820\n",
      "Train Epoch: 3343 [98560/118836 (83%)] Loss: 12253.866211\n",
      "    epoch          : 3343\n",
      "    loss           : 12217.668890385909\n",
      "    val_loss       : 12219.041465630717\n",
      "    val_log_likelihood: -12142.635395665322\n",
      "    val_log_marginal: -12150.55791690061\n",
      "Train Epoch: 3344 [256/118836 (0%)] Loss: 12183.101562\n",
      "Train Epoch: 3344 [33024/118836 (28%)] Loss: 12274.708984\n",
      "Train Epoch: 3344 [65792/118836 (55%)] Loss: 12272.900391\n",
      "Train Epoch: 3344 [98560/118836 (83%)] Loss: 12225.660156\n",
      "    epoch          : 3344\n",
      "    loss           : 12219.732317352926\n",
      "    val_loss       : 12219.44955013574\n",
      "    val_log_likelihood: -12140.381267608818\n",
      "    val_log_marginal: -12148.302797769955\n",
      "Train Epoch: 3345 [256/118836 (0%)] Loss: 12213.344727\n",
      "Train Epoch: 3345 [33024/118836 (28%)] Loss: 12174.381836\n",
      "Train Epoch: 3345 [65792/118836 (55%)] Loss: 12264.860352\n",
      "Train Epoch: 3345 [98560/118836 (83%)] Loss: 12168.488281\n",
      "    epoch          : 3345\n",
      "    loss           : 12218.035618118021\n",
      "    val_loss       : 12221.165088577889\n",
      "    val_log_likelihood: -12141.58526674938\n",
      "    val_log_marginal: -12149.511908738652\n",
      "Train Epoch: 3346 [256/118836 (0%)] Loss: 12233.615234\n",
      "Train Epoch: 3346 [33024/118836 (28%)] Loss: 12257.757812\n",
      "Train Epoch: 3346 [65792/118836 (55%)] Loss: 12213.256836\n",
      "Train Epoch: 3346 [98560/118836 (83%)] Loss: 12204.034180\n",
      "    epoch          : 3346\n",
      "    loss           : 12218.381117368435\n",
      "    val_loss       : 12215.945468259206\n",
      "    val_log_likelihood: -12141.138382864196\n",
      "    val_log_marginal: -12149.052240105284\n",
      "Train Epoch: 3347 [256/118836 (0%)] Loss: 12326.900391\n",
      "Train Epoch: 3347 [33024/118836 (28%)] Loss: 12234.799805\n",
      "Train Epoch: 3347 [65792/118836 (55%)] Loss: 12263.730469\n",
      "Train Epoch: 3347 [98560/118836 (83%)] Loss: 12287.676758\n",
      "    epoch          : 3347\n",
      "    loss           : 12220.603850031017\n",
      "    val_loss       : 12220.627694911778\n",
      "    val_log_likelihood: -12142.52867071185\n",
      "    val_log_marginal: -12150.435504356024\n",
      "Train Epoch: 3348 [256/118836 (0%)] Loss: 12370.321289\n",
      "Train Epoch: 3348 [33024/118836 (28%)] Loss: 12196.557617\n",
      "Train Epoch: 3348 [65792/118836 (55%)] Loss: 12132.333008\n",
      "Train Epoch: 3348 [98560/118836 (83%)] Loss: 12147.275391\n",
      "    epoch          : 3348\n",
      "    loss           : 12221.465536794354\n",
      "    val_loss       : 12219.721589730996\n",
      "    val_log_likelihood: -12141.581137400484\n",
      "    val_log_marginal: -12149.484112083568\n",
      "Train Epoch: 3349 [256/118836 (0%)] Loss: 12150.264648\n",
      "Train Epoch: 3349 [33024/118836 (28%)] Loss: 12281.042969\n",
      "Train Epoch: 3349 [65792/118836 (55%)] Loss: 12245.360352\n",
      "Train Epoch: 3349 [98560/118836 (83%)] Loss: 12334.328125\n",
      "    epoch          : 3349\n",
      "    loss           : 12221.840872492763\n",
      "    val_loss       : 12220.541849629157\n",
      "    val_log_likelihood: -12142.353186065448\n",
      "    val_log_marginal: -12150.276127588235\n",
      "Train Epoch: 3350 [256/118836 (0%)] Loss: 12287.608398\n",
      "Train Epoch: 3350 [33024/118836 (28%)] Loss: 12245.807617\n",
      "Train Epoch: 3350 [65792/118836 (55%)] Loss: 12276.702148\n",
      "Train Epoch: 3350 [98560/118836 (83%)] Loss: 12154.239258\n",
      "    epoch          : 3350\n",
      "    loss           : 12220.131499592897\n",
      "    val_loss       : 12215.087130838969\n",
      "    val_log_likelihood: -12141.173106809605\n",
      "    val_log_marginal: -12149.085785008972\n",
      "Train Epoch: 3351 [256/118836 (0%)] Loss: 12167.038086\n",
      "Train Epoch: 3351 [33024/118836 (28%)] Loss: 12269.194336\n",
      "Train Epoch: 3351 [65792/118836 (55%)] Loss: 12173.322266\n",
      "Train Epoch: 3351 [98560/118836 (83%)] Loss: 12150.558594\n",
      "    epoch          : 3351\n",
      "    loss           : 12221.204977641646\n",
      "    val_loss       : 12220.559252954537\n",
      "    val_log_likelihood: -12142.767119649503\n",
      "    val_log_marginal: -12150.68590001947\n",
      "Train Epoch: 3352 [256/118836 (0%)] Loss: 12223.002930\n",
      "Train Epoch: 3352 [33024/118836 (28%)] Loss: 12222.719727\n",
      "Train Epoch: 3352 [65792/118836 (55%)] Loss: 12270.106445\n",
      "Train Epoch: 3352 [98560/118836 (83%)] Loss: 12344.693359\n",
      "    epoch          : 3352\n",
      "    loss           : 12216.68212963322\n",
      "    val_loss       : 12218.151838362819\n",
      "    val_log_likelihood: -12142.687317126756\n",
      "    val_log_marginal: -12150.60587101623\n",
      "Train Epoch: 3353 [256/118836 (0%)] Loss: 12220.533203\n",
      "Train Epoch: 3353 [33024/118836 (28%)] Loss: 12204.851562\n",
      "Train Epoch: 3353 [65792/118836 (55%)] Loss: 12255.214844\n",
      "Train Epoch: 3353 [98560/118836 (83%)] Loss: 12228.182617\n",
      "    epoch          : 3353\n",
      "    loss           : 12216.836640398831\n",
      "    val_loss       : 12218.114338432877\n",
      "    val_log_likelihood: -12141.85196540271\n",
      "    val_log_marginal: -12149.7632363972\n",
      "Train Epoch: 3354 [256/118836 (0%)] Loss: 12221.953125\n",
      "Train Epoch: 3354 [33024/118836 (28%)] Loss: 12179.793945\n",
      "Train Epoch: 3354 [65792/118836 (55%)] Loss: 12279.534180\n",
      "Train Epoch: 3354 [98560/118836 (83%)] Loss: 12251.224609\n",
      "    epoch          : 3354\n",
      "    loss           : 12217.594602008374\n",
      "    val_loss       : 12220.41976868199\n",
      "    val_log_likelihood: -12141.908383251914\n",
      "    val_log_marginal: -12149.828113433823\n",
      "Train Epoch: 3355 [256/118836 (0%)] Loss: 12201.361328\n",
      "Train Epoch: 3355 [33024/118836 (28%)] Loss: 12348.544922\n",
      "Train Epoch: 3355 [65792/118836 (55%)] Loss: 12273.992188\n",
      "Train Epoch: 3355 [98560/118836 (83%)] Loss: 12218.407227\n",
      "    epoch          : 3355\n",
      "    loss           : 12217.975006461953\n",
      "    val_loss       : 12221.158974169155\n",
      "    val_log_likelihood: -12142.368874877222\n",
      "    val_log_marginal: -12150.277306441974\n",
      "Train Epoch: 3356 [256/118836 (0%)] Loss: 12286.064453\n",
      "Train Epoch: 3356 [33024/118836 (28%)] Loss: 12184.238281\n",
      "Train Epoch: 3356 [65792/118836 (55%)] Loss: 12167.269531\n",
      "Train Epoch: 3356 [98560/118836 (83%)] Loss: 12256.756836\n",
      "    epoch          : 3356\n",
      "    loss           : 12218.465023230718\n",
      "    val_loss       : 12220.668915466922\n",
      "    val_log_likelihood: -12142.509825721154\n",
      "    val_log_marginal: -12150.412062882246\n",
      "Train Epoch: 3357 [256/118836 (0%)] Loss: 12234.469727\n",
      "Train Epoch: 3357 [33024/118836 (28%)] Loss: 12309.606445\n",
      "Train Epoch: 3357 [65792/118836 (55%)] Loss: 12239.998047\n",
      "Train Epoch: 3357 [98560/118836 (83%)] Loss: 12160.085938\n",
      "    epoch          : 3357\n",
      "    loss           : 12216.216711254136\n",
      "    val_loss       : 12213.495517861598\n",
      "    val_log_likelihood: -12141.980532723324\n",
      "    val_log_marginal: -12149.90567735858\n",
      "Train Epoch: 3358 [256/118836 (0%)] Loss: 12262.149414\n",
      "Train Epoch: 3358 [33024/118836 (28%)] Loss: 12215.760742\n",
      "Train Epoch: 3358 [65792/118836 (55%)] Loss: 12278.707031\n",
      "Train Epoch: 3358 [98560/118836 (83%)] Loss: 12252.824219\n",
      "    epoch          : 3358\n",
      "    loss           : 12217.799834089381\n",
      "    val_loss       : 12219.359550484498\n",
      "    val_log_likelihood: -12141.756384247054\n",
      "    val_log_marginal: -12149.66991707191\n",
      "Train Epoch: 3359 [256/118836 (0%)] Loss: 12178.048828\n",
      "Train Epoch: 3359 [33024/118836 (28%)] Loss: 12211.844727\n",
      "Train Epoch: 3359 [65792/118836 (55%)] Loss: 12320.908203\n",
      "Train Epoch: 3359 [98560/118836 (83%)] Loss: 12182.219727\n",
      "    epoch          : 3359\n",
      "    loss           : 12216.971685987903\n",
      "    val_loss       : 12219.610162099607\n",
      "    val_log_likelihood: -12142.54737047017\n",
      "    val_log_marginal: -12150.46266234302\n",
      "Train Epoch: 3360 [256/118836 (0%)] Loss: 12262.683594\n",
      "Train Epoch: 3360 [33024/118836 (28%)] Loss: 12363.389648\n",
      "Train Epoch: 3360 [65792/118836 (55%)] Loss: 12258.412109\n",
      "Train Epoch: 3360 [98560/118836 (83%)] Loss: 12338.436523\n",
      "    epoch          : 3360\n",
      "    loss           : 12218.826584793735\n",
      "    val_loss       : 12219.680246970256\n",
      "    val_log_likelihood: -12142.068300571236\n",
      "    val_log_marginal: -12149.984636720905\n",
      "Train Epoch: 3361 [256/118836 (0%)] Loss: 12271.691406\n",
      "Train Epoch: 3361 [33024/118836 (28%)] Loss: 12266.719727\n",
      "Train Epoch: 3361 [65792/118836 (55%)] Loss: 12163.481445\n",
      "Train Epoch: 3361 [98560/118836 (83%)] Loss: 12237.639648\n",
      "    epoch          : 3361\n",
      "    loss           : 12218.691857132702\n",
      "    val_loss       : 12217.28507496864\n",
      "    val_log_likelihood: -12142.256639817257\n",
      "    val_log_marginal: -12150.165869998706\n",
      "Train Epoch: 3362 [256/118836 (0%)] Loss: 12253.170898\n",
      "Train Epoch: 3362 [33024/118836 (28%)] Loss: 12176.298828\n",
      "Train Epoch: 3362 [65792/118836 (55%)] Loss: 12172.114258\n",
      "Train Epoch: 3362 [98560/118836 (83%)] Loss: 12411.281250\n",
      "    epoch          : 3362\n",
      "    loss           : 12220.354562622777\n",
      "    val_loss       : 12217.552714651709\n",
      "    val_log_likelihood: -12142.074408085193\n",
      "    val_log_marginal: -12149.981048447216\n",
      "Train Epoch: 3363 [256/118836 (0%)] Loss: 12295.839844\n",
      "Train Epoch: 3363 [33024/118836 (28%)] Loss: 12153.923828\n",
      "Train Epoch: 3363 [65792/118836 (55%)] Loss: 12208.251953\n",
      "Train Epoch: 3363 [98560/118836 (83%)] Loss: 12223.625000\n",
      "    epoch          : 3363\n",
      "    loss           : 12219.939175907259\n",
      "    val_loss       : 12220.987547527408\n",
      "    val_log_likelihood: -12141.470553046165\n",
      "    val_log_marginal: -12149.368605438678\n",
      "Train Epoch: 3364 [256/118836 (0%)] Loss: 12256.691406\n",
      "Train Epoch: 3364 [33024/118836 (28%)] Loss: 12259.125977\n",
      "Train Epoch: 3364 [65792/118836 (55%)] Loss: 12281.443359\n",
      "Train Epoch: 3364 [98560/118836 (83%)] Loss: 12208.744141\n",
      "    epoch          : 3364\n",
      "    loss           : 12216.770541091553\n",
      "    val_loss       : 12219.957589778911\n",
      "    val_log_likelihood: -12142.258431070359\n",
      "    val_log_marginal: -12150.158798780809\n",
      "Train Epoch: 3365 [256/118836 (0%)] Loss: 12117.109375\n",
      "Train Epoch: 3365 [33024/118836 (28%)] Loss: 12297.246094\n",
      "Train Epoch: 3365 [65792/118836 (55%)] Loss: 12229.386719\n",
      "Train Epoch: 3365 [98560/118836 (83%)] Loss: 12271.580078\n",
      "    epoch          : 3365\n",
      "    loss           : 12217.764180430624\n",
      "    val_loss       : 12216.27440672092\n",
      "    val_log_likelihood: -12142.434326212262\n",
      "    val_log_marginal: -12150.334484508827\n",
      "Train Epoch: 3366 [256/118836 (0%)] Loss: 12173.139648\n",
      "Train Epoch: 3366 [33024/118836 (28%)] Loss: 12203.090820\n",
      "Train Epoch: 3366 [65792/118836 (55%)] Loss: 12276.715820\n",
      "Train Epoch: 3366 [98560/118836 (83%)] Loss: 12183.470703\n",
      "    epoch          : 3366\n",
      "    loss           : 12217.6235990488\n",
      "    val_loss       : 12220.073703809787\n",
      "    val_log_likelihood: -12142.36406670027\n",
      "    val_log_marginal: -12150.285391809291\n",
      "Train Epoch: 3367 [256/118836 (0%)] Loss: 12244.455078\n",
      "Train Epoch: 3367 [33024/118836 (28%)] Loss: 12238.695312\n",
      "Train Epoch: 3367 [65792/118836 (55%)] Loss: 12334.697266\n",
      "Train Epoch: 3367 [98560/118836 (83%)] Loss: 12249.815430\n",
      "    epoch          : 3367\n",
      "    loss           : 12219.475345552884\n",
      "    val_loss       : 12217.320645105385\n",
      "    val_log_likelihood: -12141.991358108458\n",
      "    val_log_marginal: -12149.921685315518\n",
      "Train Epoch: 3368 [256/118836 (0%)] Loss: 12248.955078\n",
      "Train Epoch: 3368 [33024/118836 (28%)] Loss: 12210.498047\n",
      "Train Epoch: 3368 [65792/118836 (55%)] Loss: 12238.369141\n",
      "Train Epoch: 3368 [98560/118836 (83%)] Loss: 12199.594727\n",
      "    epoch          : 3368\n",
      "    loss           : 12223.689097717639\n",
      "    val_loss       : 12217.878120024834\n",
      "    val_log_likelihood: -12142.416195105718\n",
      "    val_log_marginal: -12150.326487675366\n",
      "Train Epoch: 3369 [256/118836 (0%)] Loss: 12274.197266\n",
      "Train Epoch: 3369 [33024/118836 (28%)] Loss: 12330.726562\n",
      "Train Epoch: 3369 [65792/118836 (55%)] Loss: 12252.534180\n",
      "Train Epoch: 3369 [98560/118836 (83%)] Loss: 12216.197266\n",
      "    epoch          : 3369\n",
      "    loss           : 12217.141018532877\n",
      "    val_loss       : 12215.332403427203\n",
      "    val_log_likelihood: -12139.878049395162\n",
      "    val_log_marginal: -12147.804705012528\n",
      "Train Epoch: 3370 [256/118836 (0%)] Loss: 12355.368164\n",
      "Train Epoch: 3370 [33024/118836 (28%)] Loss: 12272.722656\n",
      "Train Epoch: 3370 [65792/118836 (55%)] Loss: 12153.281250\n",
      "Train Epoch: 3370 [98560/118836 (83%)] Loss: 12226.814453\n",
      "    epoch          : 3370\n",
      "    loss           : 12221.270551269126\n",
      "    val_loss       : 12217.020033711304\n",
      "    val_log_likelihood: -12141.94865688327\n",
      "    val_log_marginal: -12149.86473367576\n",
      "Train Epoch: 3371 [256/118836 (0%)] Loss: 12156.333008\n",
      "Train Epoch: 3371 [33024/118836 (28%)] Loss: 12179.618164\n",
      "Train Epoch: 3371 [65792/118836 (55%)] Loss: 12192.662109\n",
      "Train Epoch: 3371 [98560/118836 (83%)] Loss: 12211.914062\n",
      "    epoch          : 3371\n",
      "    loss           : 12219.621699234905\n",
      "    val_loss       : 12220.103439366527\n",
      "    val_log_likelihood: -12142.366836292907\n",
      "    val_log_marginal: -12150.294146973521\n",
      "Train Epoch: 3372 [256/118836 (0%)] Loss: 12482.955078\n",
      "Train Epoch: 3372 [33024/118836 (28%)] Loss: 12226.242188\n",
      "Train Epoch: 3372 [65792/118836 (55%)] Loss: 12189.035156\n",
      "Train Epoch: 3372 [98560/118836 (83%)] Loss: 12247.600586\n",
      "    epoch          : 3372\n",
      "    loss           : 12220.013609355614\n",
      "    val_loss       : 12220.068894764892\n",
      "    val_log_likelihood: -12142.790218058572\n",
      "    val_log_marginal: -12150.70688783953\n",
      "Train Epoch: 3373 [256/118836 (0%)] Loss: 12201.992188\n",
      "Train Epoch: 3373 [33024/118836 (28%)] Loss: 12350.710938\n",
      "Train Epoch: 3373 [65792/118836 (55%)] Loss: 12219.130859\n",
      "Train Epoch: 3373 [98560/118836 (83%)] Loss: 12199.881836\n",
      "    epoch          : 3373\n",
      "    loss           : 12216.408627998346\n",
      "    val_loss       : 12218.923513473275\n",
      "    val_log_likelihood: -12143.413954100753\n",
      "    val_log_marginal: -12151.340665547508\n",
      "Train Epoch: 3374 [256/118836 (0%)] Loss: 12190.629883\n",
      "Train Epoch: 3374 [33024/118836 (28%)] Loss: 12259.974609\n",
      "Train Epoch: 3374 [65792/118836 (55%)] Loss: 12174.658203\n",
      "Train Epoch: 3374 [98560/118836 (83%)] Loss: 12151.518555\n",
      "    epoch          : 3374\n",
      "    loss           : 12217.150356699753\n",
      "    val_loss       : 12221.119671175204\n",
      "    val_log_likelihood: -12141.41436168838\n",
      "    val_log_marginal: -12149.355182462305\n",
      "Train Epoch: 3375 [256/118836 (0%)] Loss: 12231.938477\n",
      "Train Epoch: 3375 [33024/118836 (28%)] Loss: 12323.732422\n",
      "Train Epoch: 3375 [65792/118836 (55%)] Loss: 12218.829102\n",
      "Train Epoch: 3375 [98560/118836 (83%)] Loss: 12204.055664\n",
      "    epoch          : 3375\n",
      "    loss           : 12217.969500878826\n",
      "    val_loss       : 12223.651626855679\n",
      "    val_log_likelihood: -12140.904983457402\n",
      "    val_log_marginal: -12148.8313071549\n",
      "Train Epoch: 3376 [256/118836 (0%)] Loss: 12273.548828\n",
      "Train Epoch: 3376 [33024/118836 (28%)] Loss: 12252.978516\n",
      "Train Epoch: 3376 [65792/118836 (55%)] Loss: 12166.373047\n",
      "Train Epoch: 3376 [98560/118836 (83%)] Loss: 12247.958984\n",
      "    epoch          : 3376\n",
      "    loss           : 12225.415568619468\n",
      "    val_loss       : 12218.709628424127\n",
      "    val_log_likelihood: -12142.741094783913\n",
      "    val_log_marginal: -12150.654718424876\n",
      "Train Epoch: 3377 [256/118836 (0%)] Loss: 12249.427734\n",
      "Train Epoch: 3377 [33024/118836 (28%)] Loss: 12257.708984\n",
      "Train Epoch: 3377 [65792/118836 (55%)] Loss: 12290.269531\n",
      "Train Epoch: 3377 [98560/118836 (83%)] Loss: 12245.819336\n",
      "    epoch          : 3377\n",
      "    loss           : 12215.769102014838\n",
      "    val_loss       : 12220.406403255249\n",
      "    val_log_likelihood: -12143.338540535824\n",
      "    val_log_marginal: -12151.247952848807\n",
      "Train Epoch: 3378 [256/118836 (0%)] Loss: 12228.506836\n",
      "Train Epoch: 3378 [33024/118836 (28%)] Loss: 12295.292969\n",
      "Train Epoch: 3378 [65792/118836 (55%)] Loss: 12201.255859\n",
      "Train Epoch: 3378 [98560/118836 (83%)] Loss: 12172.629883\n",
      "    epoch          : 3378\n",
      "    loss           : 12219.53374738291\n",
      "    val_loss       : 12219.14371027172\n",
      "    val_log_likelihood: -12141.62241796552\n",
      "    val_log_marginal: -12149.569414439993\n",
      "Train Epoch: 3379 [256/118836 (0%)] Loss: 12246.017578\n",
      "Train Epoch: 3379 [33024/118836 (28%)] Loss: 12177.355469\n",
      "Train Epoch: 3379 [65792/118836 (55%)] Loss: 12320.421875\n",
      "Train Epoch: 3379 [98560/118836 (83%)] Loss: 12209.732422\n",
      "    epoch          : 3379\n",
      "    loss           : 12223.320162905811\n",
      "    val_loss       : 12221.266772163315\n",
      "    val_log_likelihood: -12142.859196003928\n",
      "    val_log_marginal: -12150.785110211944\n",
      "Train Epoch: 3380 [256/118836 (0%)] Loss: 12299.519531\n",
      "Train Epoch: 3380 [33024/118836 (28%)] Loss: 12219.750000\n",
      "Train Epoch: 3380 [65792/118836 (55%)] Loss: 12227.251953\n",
      "Train Epoch: 3380 [98560/118836 (83%)] Loss: 12333.839844\n",
      "    epoch          : 3380\n",
      "    loss           : 12219.76614227926\n",
      "    val_loss       : 12216.6503765459\n",
      "    val_log_likelihood: -12143.288623248813\n",
      "    val_log_marginal: -12151.208004041475\n",
      "Train Epoch: 3381 [256/118836 (0%)] Loss: 12274.814453\n",
      "Train Epoch: 3381 [33024/118836 (28%)] Loss: 12232.027344\n",
      "Train Epoch: 3381 [65792/118836 (55%)] Loss: 12215.991211\n",
      "Train Epoch: 3381 [98560/118836 (83%)] Loss: 12252.803711\n",
      "    epoch          : 3381\n",
      "    loss           : 12214.34144130609\n",
      "    val_loss       : 12220.60934050585\n",
      "    val_log_likelihood: -12140.417969880842\n",
      "    val_log_marginal: -12148.320423673522\n",
      "Train Epoch: 3382 [256/118836 (0%)] Loss: 12275.650391\n",
      "Train Epoch: 3382 [33024/118836 (28%)] Loss: 12299.965820\n",
      "Train Epoch: 3382 [65792/118836 (55%)] Loss: 12338.986328\n",
      "Train Epoch: 3382 [98560/118836 (83%)] Loss: 12223.355469\n",
      "    epoch          : 3382\n",
      "    loss           : 12218.612290632755\n",
      "    val_loss       : 12217.020321721307\n",
      "    val_log_likelihood: -12142.748446223635\n",
      "    val_log_marginal: -12150.657341047368\n",
      "Train Epoch: 3383 [256/118836 (0%)] Loss: 12245.512695\n",
      "Train Epoch: 3383 [33024/118836 (28%)] Loss: 12237.040039\n",
      "Train Epoch: 3383 [65792/118836 (55%)] Loss: 12276.850586\n",
      "Train Epoch: 3383 [98560/118836 (83%)] Loss: 12235.598633\n",
      "    epoch          : 3383\n",
      "    loss           : 12220.6523573201\n",
      "    val_loss       : 12214.872909803089\n",
      "    val_log_likelihood: -12141.716547605201\n",
      "    val_log_marginal: -12149.628206348321\n",
      "Train Epoch: 3384 [256/118836 (0%)] Loss: 12219.462891\n",
      "Train Epoch: 3384 [33024/118836 (28%)] Loss: 12223.688477\n",
      "Train Epoch: 3384 [65792/118836 (55%)] Loss: 12271.487305\n",
      "Train Epoch: 3384 [98560/118836 (83%)] Loss: 12188.603516\n",
      "    epoch          : 3384\n",
      "    loss           : 12219.02234252223\n",
      "    val_loss       : 12217.469554105193\n",
      "    val_log_likelihood: -12142.38785702285\n",
      "    val_log_marginal: -12150.309864074916\n",
      "Train Epoch: 3385 [256/118836 (0%)] Loss: 12159.066406\n",
      "Train Epoch: 3385 [33024/118836 (28%)] Loss: 12319.987305\n",
      "Train Epoch: 3385 [65792/118836 (55%)] Loss: 12271.824219\n",
      "Train Epoch: 3385 [98560/118836 (83%)] Loss: 12192.254883\n",
      "    epoch          : 3385\n",
      "    loss           : 12223.49371882108\n",
      "    val_loss       : 12217.65152454937\n",
      "    val_log_likelihood: -12140.524618098634\n",
      "    val_log_marginal: -12148.474685351706\n",
      "Train Epoch: 3386 [256/118836 (0%)] Loss: 12186.436523\n",
      "Train Epoch: 3386 [33024/118836 (28%)] Loss: 12234.922852\n",
      "Train Epoch: 3386 [65792/118836 (55%)] Loss: 12239.238281\n",
      "Train Epoch: 3386 [98560/118836 (83%)] Loss: 12199.764648\n",
      "    epoch          : 3386\n",
      "    loss           : 12216.859579036134\n",
      "    val_loss       : 12219.037704581686\n",
      "    val_log_likelihood: -12144.64647920544\n",
      "    val_log_marginal: -12152.580495510274\n",
      "Train Epoch: 3387 [256/118836 (0%)] Loss: 12203.395508\n",
      "Train Epoch: 3387 [33024/118836 (28%)] Loss: 12177.724609\n",
      "Train Epoch: 3387 [65792/118836 (55%)] Loss: 12129.501953\n",
      "Train Epoch: 3387 [98560/118836 (83%)] Loss: 12226.037109\n",
      "    epoch          : 3387\n",
      "    loss           : 12220.150698860112\n",
      "    val_loss       : 12220.650499949925\n",
      "    val_log_likelihood: -12139.544104761166\n",
      "    val_log_marginal: -12147.461565116388\n",
      "Train Epoch: 3388 [256/118836 (0%)] Loss: 12270.512695\n",
      "Train Epoch: 3388 [33024/118836 (28%)] Loss: 12144.228516\n",
      "Train Epoch: 3388 [65792/118836 (55%)] Loss: 12268.003906\n",
      "Train Epoch: 3388 [98560/118836 (83%)] Loss: 12154.021484\n",
      "    epoch          : 3388\n",
      "    loss           : 12219.031742723842\n",
      "    val_loss       : 12222.357357501343\n",
      "    val_log_likelihood: -12141.365143423025\n",
      "    val_log_marginal: -12149.269842838317\n",
      "Train Epoch: 3389 [256/118836 (0%)] Loss: 12174.332031\n",
      "Train Epoch: 3389 [33024/118836 (28%)] Loss: 12195.945312\n",
      "Train Epoch: 3389 [65792/118836 (55%)] Loss: 12208.418945\n",
      "Train Epoch: 3389 [98560/118836 (83%)] Loss: 12227.054688\n",
      "    epoch          : 3389\n",
      "    loss           : 12217.359837998862\n",
      "    val_loss       : 12220.57603522816\n",
      "    val_log_likelihood: -12141.386011004704\n",
      "    val_log_marginal: -12149.289325186553\n",
      "Train Epoch: 3390 [256/118836 (0%)] Loss: 12192.890625\n",
      "Train Epoch: 3390 [33024/118836 (28%)] Loss: 12156.255859\n",
      "Train Epoch: 3390 [65792/118836 (55%)] Loss: 12289.233398\n",
      "Train Epoch: 3390 [98560/118836 (83%)] Loss: 12287.657227\n",
      "    epoch          : 3390\n",
      "    loss           : 12216.877194478908\n",
      "    val_loss       : 12219.261851055988\n",
      "    val_log_likelihood: -12143.651123733458\n",
      "    val_log_marginal: -12151.550917950222\n",
      "Train Epoch: 3391 [256/118836 (0%)] Loss: 12252.815430\n",
      "Train Epoch: 3391 [33024/118836 (28%)] Loss: 12400.693359\n",
      "Train Epoch: 3391 [65792/118836 (55%)] Loss: 12270.968750\n",
      "Train Epoch: 3391 [98560/118836 (83%)] Loss: 12219.635742\n",
      "    epoch          : 3391\n",
      "    loss           : 12222.039147636218\n",
      "    val_loss       : 12215.620195496444\n",
      "    val_log_likelihood: -12142.329342916408\n",
      "    val_log_marginal: -12150.237398893767\n",
      "Train Epoch: 3392 [256/118836 (0%)] Loss: 12178.922852\n",
      "Train Epoch: 3392 [33024/118836 (28%)] Loss: 12238.671875\n",
      "Train Epoch: 3392 [65792/118836 (55%)] Loss: 12271.415039\n",
      "Train Epoch: 3392 [98560/118836 (83%)] Loss: 12256.775391\n",
      "    epoch          : 3392\n",
      "    loss           : 12217.339528245193\n",
      "    val_loss       : 12218.07044337478\n",
      "    val_log_likelihood: -12141.747338160412\n",
      "    val_log_marginal: -12149.66722872865\n",
      "Train Epoch: 3393 [256/118836 (0%)] Loss: 12197.502930\n",
      "Train Epoch: 3393 [33024/118836 (28%)] Loss: 12334.121094\n",
      "Train Epoch: 3393 [65792/118836 (55%)] Loss: 12250.685547\n",
      "Train Epoch: 3393 [98560/118836 (83%)] Loss: 12297.518555\n",
      "    epoch          : 3393\n",
      "    loss           : 12218.35480219965\n",
      "    val_loss       : 12216.192417269536\n",
      "    val_log_likelihood: -12140.15907451923\n",
      "    val_log_marginal: -12148.072411553247\n",
      "Train Epoch: 3394 [256/118836 (0%)] Loss: 12194.388672\n",
      "Train Epoch: 3394 [33024/118836 (28%)] Loss: 12257.048828\n",
      "Train Epoch: 3394 [65792/118836 (55%)] Loss: 12194.882812\n",
      "Train Epoch: 3394 [98560/118836 (83%)] Loss: 12146.101562\n",
      "    epoch          : 3394\n",
      "    loss           : 12217.756881171164\n",
      "    val_loss       : 12215.724174665147\n",
      "    val_log_likelihood: -12139.798902598997\n",
      "    val_log_marginal: -12147.716188754626\n",
      "Train Epoch: 3395 [256/118836 (0%)] Loss: 12158.808594\n",
      "Train Epoch: 3395 [33024/118836 (28%)] Loss: 12199.661133\n",
      "Train Epoch: 3395 [65792/118836 (55%)] Loss: 12185.353516\n",
      "Train Epoch: 3395 [98560/118836 (83%)] Loss: 12145.650391\n",
      "    epoch          : 3395\n",
      "    loss           : 12219.035800506617\n",
      "    val_loss       : 12215.850727577657\n",
      "    val_log_likelihood: -12142.910817469241\n",
      "    val_log_marginal: -12150.816606241298\n",
      "Train Epoch: 3396 [256/118836 (0%)] Loss: 12212.501953\n",
      "Train Epoch: 3396 [33024/118836 (28%)] Loss: 12251.530273\n",
      "Train Epoch: 3396 [65792/118836 (55%)] Loss: 12288.304688\n",
      "Train Epoch: 3396 [98560/118836 (83%)] Loss: 12139.964844\n",
      "    epoch          : 3396\n",
      "    loss           : 12219.535641542598\n",
      "    val_loss       : 12221.190775841967\n",
      "    val_log_likelihood: -12141.427019844656\n",
      "    val_log_marginal: -12149.337614651567\n",
      "Train Epoch: 3397 [256/118836 (0%)] Loss: 12194.843750\n",
      "Train Epoch: 3397 [33024/118836 (28%)] Loss: 12255.910156\n",
      "Train Epoch: 3397 [65792/118836 (55%)] Loss: 12163.824219\n",
      "Train Epoch: 3397 [98560/118836 (83%)] Loss: 12221.990234\n",
      "    epoch          : 3397\n",
      "    loss           : 12219.035106008323\n",
      "    val_loss       : 12216.323779313052\n",
      "    val_log_likelihood: -12142.40717002042\n",
      "    val_log_marginal: -12150.313823734035\n",
      "Train Epoch: 3398 [256/118836 (0%)] Loss: 12274.225586\n",
      "Train Epoch: 3398 [33024/118836 (28%)] Loss: 12218.360352\n",
      "Train Epoch: 3398 [65792/118836 (55%)] Loss: 12199.328125\n",
      "Train Epoch: 3398 [98560/118836 (83%)] Loss: 12201.578125\n",
      "    epoch          : 3398\n",
      "    loss           : 12215.640075895626\n",
      "    val_loss       : 12217.783754088834\n",
      "    val_log_likelihood: -12142.881102829042\n",
      "    val_log_marginal: -12150.792008149212\n",
      "Train Epoch: 3399 [256/118836 (0%)] Loss: 12291.899414\n",
      "Train Epoch: 3399 [33024/118836 (28%)] Loss: 12242.089844\n",
      "Train Epoch: 3399 [65792/118836 (55%)] Loss: 12321.258789\n",
      "Train Epoch: 3399 [98560/118836 (83%)] Loss: 12251.246094\n",
      "    epoch          : 3399\n",
      "    loss           : 12219.082134479682\n",
      "    val_loss       : 12218.73843973124\n",
      "    val_log_likelihood: -12142.780810264165\n",
      "    val_log_marginal: -12150.689796221812\n",
      "Train Epoch: 3400 [256/118836 (0%)] Loss: 12260.126953\n",
      "Train Epoch: 3400 [33024/118836 (28%)] Loss: 12237.033203\n",
      "Train Epoch: 3400 [65792/118836 (55%)] Loss: 12332.646484\n",
      "Train Epoch: 3400 [98560/118836 (83%)] Loss: 12274.564453\n",
      "    epoch          : 3400\n",
      "    loss           : 12222.444720423646\n",
      "    val_loss       : 12216.05903016908\n",
      "    val_log_likelihood: -12141.73405981829\n",
      "    val_log_marginal: -12149.642299880084\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch3400.pth ...\n",
      "Train Epoch: 3401 [256/118836 (0%)] Loss: 12236.575195\n",
      "Train Epoch: 3401 [33024/118836 (28%)] Loss: 12235.167969\n",
      "Train Epoch: 3401 [65792/118836 (55%)] Loss: 12195.771484\n",
      "Train Epoch: 3401 [98560/118836 (83%)] Loss: 12211.832031\n",
      "    epoch          : 3401\n",
      "    loss           : 12219.040255053247\n",
      "    val_loss       : 12214.26425515009\n",
      "    val_log_likelihood: -12142.430774077233\n",
      "    val_log_marginal: -12150.337755924704\n",
      "Train Epoch: 3402 [256/118836 (0%)] Loss: 12208.115234\n",
      "Train Epoch: 3402 [33024/118836 (28%)] Loss: 12245.148438\n",
      "Train Epoch: 3402 [65792/118836 (55%)] Loss: 12257.434570\n",
      "Train Epoch: 3402 [98560/118836 (83%)] Loss: 12189.541016\n",
      "    epoch          : 3402\n",
      "    loss           : 12216.862650078836\n",
      "    val_loss       : 12218.189759355619\n",
      "    val_log_likelihood: -12143.07861513906\n",
      "    val_log_marginal: -12150.992920704184\n",
      "Train Epoch: 3403 [256/118836 (0%)] Loss: 12187.733398\n",
      "Train Epoch: 3403 [33024/118836 (28%)] Loss: 12420.102539\n",
      "Train Epoch: 3403 [65792/118836 (55%)] Loss: 12149.695312\n",
      "Train Epoch: 3403 [98560/118836 (83%)] Loss: 12288.095703\n",
      "    epoch          : 3403\n",
      "    loss           : 12218.574160915528\n",
      "    val_loss       : 12220.014619036889\n",
      "    val_log_likelihood: -12141.41532985034\n",
      "    val_log_marginal: -12149.32269758669\n",
      "Train Epoch: 3404 [256/118836 (0%)] Loss: 12210.606445\n",
      "Train Epoch: 3404 [33024/118836 (28%)] Loss: 12244.314453\n",
      "Train Epoch: 3404 [65792/118836 (55%)] Loss: 12266.084961\n",
      "Train Epoch: 3404 [98560/118836 (83%)] Loss: 12185.707031\n",
      "    epoch          : 3404\n",
      "    loss           : 12214.65860990488\n",
      "    val_loss       : 12214.503069185084\n",
      "    val_log_likelihood: -12141.867575701768\n",
      "    val_log_marginal: -12149.764387419573\n",
      "Train Epoch: 3405 [256/118836 (0%)] Loss: 12343.617188\n",
      "Train Epoch: 3405 [33024/118836 (28%)] Loss: 12237.273438\n",
      "Train Epoch: 3405 [65792/118836 (55%)] Loss: 12251.101562\n",
      "Train Epoch: 3405 [98560/118836 (83%)] Loss: 12161.664062\n",
      "    epoch          : 3405\n",
      "    loss           : 12218.70337540064\n",
      "    val_loss       : 12217.762245972013\n",
      "    val_log_likelihood: -12141.819839162015\n",
      "    val_log_marginal: -12149.71989102439\n",
      "Train Epoch: 3406 [256/118836 (0%)] Loss: 12156.791016\n",
      "Train Epoch: 3406 [33024/118836 (28%)] Loss: 12171.047852\n",
      "Train Epoch: 3406 [65792/118836 (55%)] Loss: 12248.652344\n",
      "Train Epoch: 3406 [98560/118836 (83%)] Loss: 12327.119141\n",
      "    epoch          : 3406\n",
      "    loss           : 12217.579458908447\n",
      "    val_loss       : 12218.301772109357\n",
      "    val_log_likelihood: -12141.329139849566\n",
      "    val_log_marginal: -12149.241348208114\n",
      "Train Epoch: 3407 [256/118836 (0%)] Loss: 12173.141602\n",
      "Train Epoch: 3407 [33024/118836 (28%)] Loss: 12237.753906\n",
      "Train Epoch: 3407 [65792/118836 (55%)] Loss: 12143.470703\n",
      "Train Epoch: 3407 [98560/118836 (83%)] Loss: 12331.605469\n",
      "    epoch          : 3407\n",
      "    loss           : 12214.861915354892\n",
      "    val_loss       : 12218.3923785672\n",
      "    val_log_likelihood: -12141.657468724152\n",
      "    val_log_marginal: -12149.600876516268\n",
      "Train Epoch: 3408 [256/118836 (0%)] Loss: 12223.914062\n",
      "Train Epoch: 3408 [33024/118836 (28%)] Loss: 12315.670898\n",
      "Train Epoch: 3408 [65792/118836 (55%)] Loss: 12194.684570\n",
      "Train Epoch: 3408 [98560/118836 (83%)] Loss: 12184.360352\n",
      "    epoch          : 3408\n",
      "    loss           : 12217.487450566066\n",
      "    val_loss       : 12221.226493410237\n",
      "    val_log_likelihood: -12143.517441454715\n",
      "    val_log_marginal: -12151.444494068146\n",
      "Train Epoch: 3409 [256/118836 (0%)] Loss: 12221.084961\n",
      "Train Epoch: 3409 [33024/118836 (28%)] Loss: 12303.236328\n",
      "Train Epoch: 3409 [65792/118836 (55%)] Loss: 12304.837891\n",
      "Train Epoch: 3409 [98560/118836 (83%)] Loss: 12224.986328\n",
      "    epoch          : 3409\n",
      "    loss           : 12219.31953125\n",
      "    val_loss       : 12217.892973531552\n",
      "    val_log_likelihood: -12141.867616896712\n",
      "    val_log_marginal: -12149.79519817465\n",
      "Train Epoch: 3410 [256/118836 (0%)] Loss: 12188.573242\n",
      "Train Epoch: 3410 [33024/118836 (28%)] Loss: 12259.481445\n",
      "Train Epoch: 3410 [65792/118836 (55%)] Loss: 12277.057617\n",
      "Train Epoch: 3410 [98560/118836 (83%)] Loss: 12245.332031\n",
      "    epoch          : 3410\n",
      "    loss           : 12221.937926650382\n",
      "    val_loss       : 12217.688241471924\n",
      "    val_log_likelihood: -12140.794336422146\n",
      "    val_log_marginal: -12148.713816526875\n",
      "Train Epoch: 3411 [256/118836 (0%)] Loss: 12239.445312\n",
      "Train Epoch: 3411 [33024/118836 (28%)] Loss: 12251.207031\n",
      "Train Epoch: 3411 [65792/118836 (55%)] Loss: 12193.905273\n",
      "Train Epoch: 3411 [98560/118836 (83%)] Loss: 12269.714844\n",
      "    epoch          : 3411\n",
      "    loss           : 12216.059863362025\n",
      "    val_loss       : 12218.933224147439\n",
      "    val_log_likelihood: -12143.318696688897\n",
      "    val_log_marginal: -12151.24505444571\n",
      "Train Epoch: 3412 [256/118836 (0%)] Loss: 12303.556641\n",
      "Train Epoch: 3412 [33024/118836 (28%)] Loss: 12215.222656\n",
      "Train Epoch: 3412 [65792/118836 (55%)] Loss: 12254.292969\n",
      "Train Epoch: 3412 [98560/118836 (83%)] Loss: 12290.689453\n",
      "    epoch          : 3412\n",
      "    loss           : 12217.12276804177\n",
      "    val_loss       : 12219.0885198983\n",
      "    val_log_likelihood: -12142.302763292235\n",
      "    val_log_marginal: -12150.225162491452\n",
      "Train Epoch: 3413 [256/118836 (0%)] Loss: 12234.392578\n",
      "Train Epoch: 3413 [33024/118836 (28%)] Loss: 12258.900391\n",
      "Train Epoch: 3413 [65792/118836 (55%)] Loss: 12239.311523\n",
      "Train Epoch: 3413 [98560/118836 (83%)] Loss: 12155.448242\n",
      "    epoch          : 3413\n",
      "    loss           : 12216.857116870864\n",
      "    val_loss       : 12219.907657606878\n",
      "    val_log_likelihood: -12142.394166957454\n",
      "    val_log_marginal: -12150.310006311374\n",
      "Train Epoch: 3414 [256/118836 (0%)] Loss: 12336.104492\n",
      "Train Epoch: 3414 [33024/118836 (28%)] Loss: 12180.882812\n",
      "Train Epoch: 3414 [65792/118836 (55%)] Loss: 12303.375977\n",
      "Train Epoch: 3414 [98560/118836 (83%)] Loss: 12271.094727\n",
      "    epoch          : 3414\n",
      "    loss           : 12217.83622812629\n",
      "    val_loss       : 12214.101250875237\n",
      "    val_log_likelihood: -12143.704964071547\n",
      "    val_log_marginal: -12151.619423964077\n",
      "Train Epoch: 3415 [256/118836 (0%)] Loss: 12242.230469\n",
      "Train Epoch: 3415 [33024/118836 (28%)] Loss: 12217.944336\n",
      "Train Epoch: 3415 [65792/118836 (55%)] Loss: 12248.861328\n",
      "Train Epoch: 3415 [98560/118836 (83%)] Loss: 12152.476562\n",
      "    epoch          : 3415\n",
      "    loss           : 12216.773003256823\n",
      "    val_loss       : 12215.7861544892\n",
      "    val_log_likelihood: -12142.579262626654\n",
      "    val_log_marginal: -12150.498976757639\n",
      "Train Epoch: 3416 [256/118836 (0%)] Loss: 12226.333984\n",
      "Train Epoch: 3416 [33024/118836 (28%)] Loss: 12247.388672\n",
      "Train Epoch: 3416 [65792/118836 (55%)] Loss: 12219.342773\n",
      "Train Epoch: 3416 [98560/118836 (83%)] Loss: 12249.322266\n",
      "    epoch          : 3416\n",
      "    loss           : 12216.411621174524\n",
      "    val_loss       : 12220.364643014182\n",
      "    val_log_likelihood: -12143.742935794045\n",
      "    val_log_marginal: -12151.673547189737\n",
      "Train Epoch: 3417 [256/118836 (0%)] Loss: 12238.884766\n",
      "Train Epoch: 3417 [33024/118836 (28%)] Loss: 12248.423828\n",
      "Train Epoch: 3417 [65792/118836 (55%)] Loss: 12305.279297\n",
      "Train Epoch: 3417 [98560/118836 (83%)] Loss: 12335.020508\n",
      "    epoch          : 3417\n",
      "    loss           : 12221.699607759512\n",
      "    val_loss       : 12221.000851034942\n",
      "    val_log_likelihood: -12142.431356137562\n",
      "    val_log_marginal: -12150.358659566768\n",
      "Train Epoch: 3418 [256/118836 (0%)] Loss: 12323.619141\n",
      "Train Epoch: 3418 [33024/118836 (28%)] Loss: 12203.773438\n",
      "Train Epoch: 3418 [65792/118836 (55%)] Loss: 12224.137695\n",
      "Train Epoch: 3418 [98560/118836 (83%)] Loss: 12209.799805\n",
      "    epoch          : 3418\n",
      "    loss           : 12219.997531372777\n",
      "    val_loss       : 12215.682086467743\n",
      "    val_log_likelihood: -12142.698379988627\n",
      "    val_log_marginal: -12150.605442052745\n",
      "Train Epoch: 3419 [256/118836 (0%)] Loss: 12249.244141\n",
      "Train Epoch: 3419 [33024/118836 (28%)] Loss: 12276.065430\n",
      "Train Epoch: 3419 [65792/118836 (55%)] Loss: 12217.437500\n",
      "Train Epoch: 3419 [98560/118836 (83%)] Loss: 12325.237305\n",
      "    epoch          : 3419\n",
      "    loss           : 12219.014816286703\n",
      "    val_loss       : 12219.374212576038\n",
      "    val_log_likelihood: -12142.012483522021\n",
      "    val_log_marginal: -12149.938784495163\n",
      "Train Epoch: 3420 [256/118836 (0%)] Loss: 12211.942383\n",
      "Train Epoch: 3420 [33024/118836 (28%)] Loss: 12301.726562\n",
      "Train Epoch: 3420 [65792/118836 (55%)] Loss: 12233.154297\n",
      "Train Epoch: 3420 [98560/118836 (83%)] Loss: 12271.568359\n",
      "    epoch          : 3420\n",
      "    loss           : 12213.321574842328\n",
      "    val_loss       : 12223.617525177408\n",
      "    val_log_likelihood: -12142.731711544924\n",
      "    val_log_marginal: -12150.649410281372\n",
      "Train Epoch: 3421 [256/118836 (0%)] Loss: 12300.831055\n",
      "Train Epoch: 3421 [33024/118836 (28%)] Loss: 12213.128906\n",
      "Train Epoch: 3421 [65792/118836 (55%)] Loss: 12220.369141\n",
      "Train Epoch: 3421 [98560/118836 (83%)] Loss: 12251.949219\n",
      "    epoch          : 3421\n",
      "    loss           : 12222.01990555857\n",
      "    val_loss       : 12220.023048267418\n",
      "    val_log_likelihood: -12141.251748765766\n",
      "    val_log_marginal: -12149.168135368474\n",
      "Train Epoch: 3422 [256/118836 (0%)] Loss: 12201.535156\n",
      "Train Epoch: 3422 [33024/118836 (28%)] Loss: 12219.263672\n",
      "Train Epoch: 3422 [65792/118836 (55%)] Loss: 12323.580078\n",
      "Train Epoch: 3422 [98560/118836 (83%)] Loss: 12273.179688\n",
      "    epoch          : 3422\n",
      "    loss           : 12221.293144191997\n",
      "    val_loss       : 12218.177563557181\n",
      "    val_log_likelihood: -12140.903190588813\n",
      "    val_log_marginal: -12148.820585915191\n",
      "Train Epoch: 3423 [256/118836 (0%)] Loss: 12284.519531\n",
      "Train Epoch: 3423 [33024/118836 (28%)] Loss: 12268.375000\n",
      "Train Epoch: 3423 [65792/118836 (55%)] Loss: 12271.481445\n",
      "Train Epoch: 3423 [98560/118836 (83%)] Loss: 12239.791016\n",
      "    epoch          : 3423\n",
      "    loss           : 12216.770831556296\n",
      "    val_loss       : 12217.322185827812\n",
      "    val_log_likelihood: -12141.699446372259\n",
      "    val_log_marginal: -12149.639827095982\n",
      "Train Epoch: 3424 [256/118836 (0%)] Loss: 12210.696289\n",
      "Train Epoch: 3424 [33024/118836 (28%)] Loss: 12193.107422\n",
      "Train Epoch: 3424 [65792/118836 (55%)] Loss: 12163.596680\n",
      "Train Epoch: 3424 [98560/118836 (83%)] Loss: 12222.727539\n",
      "    epoch          : 3424\n",
      "    loss           : 12219.651854257134\n",
      "    val_loss       : 12219.083947772531\n",
      "    val_log_likelihood: -12141.550944091192\n",
      "    val_log_marginal: -12149.48825371028\n",
      "Train Epoch: 3425 [256/118836 (0%)] Loss: 12282.419922\n",
      "Train Epoch: 3425 [33024/118836 (28%)] Loss: 12327.274414\n",
      "Train Epoch: 3425 [65792/118836 (55%)] Loss: 12273.939453\n",
      "Train Epoch: 3425 [98560/118836 (83%)] Loss: 12197.468750\n",
      "    epoch          : 3425\n",
      "    loss           : 12216.801979295906\n",
      "    val_loss       : 12222.487293788003\n",
      "    val_log_likelihood: -12140.888539404983\n",
      "    val_log_marginal: -12148.811532643627\n",
      "Train Epoch: 3426 [256/118836 (0%)] Loss: 12267.988281\n",
      "Train Epoch: 3426 [33024/118836 (28%)] Loss: 12340.944336\n",
      "Train Epoch: 3426 [65792/118836 (55%)] Loss: 12352.059570\n",
      "Train Epoch: 3426 [98560/118836 (83%)] Loss: 12253.106445\n",
      "    epoch          : 3426\n",
      "    loss           : 12221.133515075735\n",
      "    val_loss       : 12214.570747634001\n",
      "    val_log_likelihood: -12142.94650860732\n",
      "    val_log_marginal: -12150.865464617873\n",
      "Train Epoch: 3427 [256/118836 (0%)] Loss: 12338.991211\n",
      "Train Epoch: 3427 [33024/118836 (28%)] Loss: 12195.669922\n",
      "Train Epoch: 3427 [65792/118836 (55%)] Loss: 12156.957031\n",
      "Train Epoch: 3427 [98560/118836 (83%)] Loss: 12226.701172\n",
      "    epoch          : 3427\n",
      "    loss           : 12221.693632069377\n",
      "    val_loss       : 12217.845425141948\n",
      "    val_log_likelihood: -12140.28221428479\n",
      "    val_log_marginal: -12148.212762597366\n",
      "Train Epoch: 3428 [256/118836 (0%)] Loss: 12360.206055\n",
      "Train Epoch: 3428 [33024/118836 (28%)] Loss: 12190.847656\n",
      "Train Epoch: 3428 [65792/118836 (55%)] Loss: 12364.515625\n",
      "Train Epoch: 3428 [98560/118836 (83%)] Loss: 12160.190430\n",
      "    epoch          : 3428\n",
      "    loss           : 12218.373259473221\n",
      "    val_loss       : 12220.634124727287\n",
      "    val_log_likelihood: -12142.995048044613\n",
      "    val_log_marginal: -12150.900692880175\n",
      "Train Epoch: 3429 [256/118836 (0%)] Loss: 12177.495117\n",
      "Train Epoch: 3429 [33024/118836 (28%)] Loss: 12255.426758\n",
      "Train Epoch: 3429 [65792/118836 (55%)] Loss: 12155.090820\n",
      "Train Epoch: 3429 [98560/118836 (83%)] Loss: 12164.578125\n",
      "    epoch          : 3429\n",
      "    loss           : 12218.805596535101\n",
      "    val_loss       : 12216.317129608165\n",
      "    val_log_likelihood: -12140.969122854633\n",
      "    val_log_marginal: -12148.900161006526\n",
      "Train Epoch: 3430 [256/118836 (0%)] Loss: 12182.177734\n",
      "Train Epoch: 3430 [33024/118836 (28%)] Loss: 12160.791992\n",
      "Train Epoch: 3430 [65792/118836 (55%)] Loss: 12228.678711\n",
      "Train Epoch: 3430 [98560/118836 (83%)] Loss: 12141.828125\n",
      "    epoch          : 3430\n",
      "    loss           : 12217.37799963813\n",
      "    val_loss       : 12218.72739459841\n",
      "    val_log_likelihood: -12141.545129626757\n",
      "    val_log_marginal: -12149.466135816661\n",
      "Train Epoch: 3431 [256/118836 (0%)] Loss: 12210.286133\n",
      "Train Epoch: 3431 [33024/118836 (28%)] Loss: 12359.851562\n",
      "Train Epoch: 3431 [65792/118836 (55%)] Loss: 12197.561523\n",
      "Train Epoch: 3431 [98560/118836 (83%)] Loss: 12278.297852\n",
      "    epoch          : 3431\n",
      "    loss           : 12218.118348712778\n",
      "    val_loss       : 12222.684811674282\n",
      "    val_log_likelihood: -12143.549842813016\n",
      "    val_log_marginal: -12151.470129434192\n",
      "Train Epoch: 3432 [256/118836 (0%)] Loss: 12414.187500\n",
      "Train Epoch: 3432 [33024/118836 (28%)] Loss: 12273.921875\n",
      "Train Epoch: 3432 [65792/118836 (55%)] Loss: 12164.630859\n",
      "Train Epoch: 3432 [98560/118836 (83%)] Loss: 12224.333984\n",
      "    epoch          : 3432\n",
      "    loss           : 12222.41678750517\n",
      "    val_loss       : 12219.654685546166\n",
      "    val_log_likelihood: -12141.51907616703\n",
      "    val_log_marginal: -12149.445424858539\n",
      "Train Epoch: 3433 [256/118836 (0%)] Loss: 12265.555664\n",
      "Train Epoch: 3433 [33024/118836 (28%)] Loss: 12272.302734\n",
      "Train Epoch: 3433 [65792/118836 (55%)] Loss: 12221.468750\n",
      "Train Epoch: 3433 [98560/118836 (83%)] Loss: 12270.264648\n",
      "    epoch          : 3433\n",
      "    loss           : 12215.469027702387\n",
      "    val_loss       : 12221.301511582855\n",
      "    val_log_likelihood: -12142.709404886527\n",
      "    val_log_marginal: -12150.610525831555\n",
      "Train Epoch: 3434 [256/118836 (0%)] Loss: 12299.877930\n",
      "Train Epoch: 3434 [33024/118836 (28%)] Loss: 12131.291016\n",
      "Train Epoch: 3434 [65792/118836 (55%)] Loss: 12195.690430\n",
      "Train Epoch: 3434 [98560/118836 (83%)] Loss: 12292.691406\n",
      "    epoch          : 3434\n",
      "    loss           : 12216.764188184969\n",
      "    val_loss       : 12221.331690639616\n",
      "    val_log_likelihood: -12143.21974821004\n",
      "    val_log_marginal: -12151.12927653625\n",
      "Train Epoch: 3435 [256/118836 (0%)] Loss: 12309.605469\n",
      "Train Epoch: 3435 [33024/118836 (28%)] Loss: 12205.709961\n",
      "Train Epoch: 3435 [65792/118836 (55%)] Loss: 12203.302734\n",
      "Train Epoch: 3435 [98560/118836 (83%)] Loss: 12198.269531\n",
      "    epoch          : 3435\n",
      "    loss           : 12213.724229735319\n",
      "    val_loss       : 12216.714238067909\n",
      "    val_log_likelihood: -12141.906328512716\n",
      "    val_log_marginal: -12149.826572811138\n",
      "Train Epoch: 3436 [256/118836 (0%)] Loss: 12343.921875\n",
      "Train Epoch: 3436 [33024/118836 (28%)] Loss: 12207.092773\n",
      "Train Epoch: 3436 [65792/118836 (55%)] Loss: 12209.992188\n",
      "Train Epoch: 3436 [98560/118836 (83%)] Loss: 12189.000000\n",
      "    epoch          : 3436\n",
      "    loss           : 12216.783316532257\n",
      "    val_loss       : 12220.085553546453\n",
      "    val_log_likelihood: -12142.042547592277\n",
      "    val_log_marginal: -12149.951883363097\n",
      "Train Epoch: 3437 [256/118836 (0%)] Loss: 12307.597656\n",
      "Train Epoch: 3437 [33024/118836 (28%)] Loss: 12355.147461\n",
      "Train Epoch: 3437 [65792/118836 (55%)] Loss: 12279.191406\n",
      "Train Epoch: 3437 [98560/118836 (83%)] Loss: 12278.099609\n",
      "    epoch          : 3437\n",
      "    loss           : 12215.80997321521\n",
      "    val_loss       : 12219.547499422744\n",
      "    val_log_likelihood: -12143.379851633581\n",
      "    val_log_marginal: -12151.28375674257\n",
      "Train Epoch: 3438 [256/118836 (0%)] Loss: 12220.527344\n",
      "Train Epoch: 3438 [33024/118836 (28%)] Loss: 12177.963867\n",
      "Train Epoch: 3438 [65792/118836 (55%)] Loss: 12338.339844\n",
      "Train Epoch: 3438 [98560/118836 (83%)] Loss: 12296.487305\n",
      "    epoch          : 3438\n",
      "    loss           : 12216.995611042183\n",
      "    val_loss       : 12216.388709077131\n",
      "    val_log_likelihood: -12141.073295498603\n",
      "    val_log_marginal: -12149.001313204977\n",
      "Train Epoch: 3439 [256/118836 (0%)] Loss: 12227.287109\n",
      "Train Epoch: 3439 [33024/118836 (28%)] Loss: 12246.928711\n",
      "Train Epoch: 3439 [65792/118836 (55%)] Loss: 12332.281250\n",
      "Train Epoch: 3439 [98560/118836 (83%)] Loss: 12163.549805\n",
      "    epoch          : 3439\n",
      "    loss           : 12216.635751395781\n",
      "    val_loss       : 12221.511540246653\n",
      "    val_log_likelihood: -12141.22241505764\n",
      "    val_log_marginal: -12149.158744005617\n",
      "Train Epoch: 3440 [256/118836 (0%)] Loss: 12293.072266\n",
      "Train Epoch: 3440 [33024/118836 (28%)] Loss: 12291.198242\n",
      "Train Epoch: 3440 [65792/118836 (55%)] Loss: 12262.385742\n",
      "Train Epoch: 3440 [98560/118836 (83%)] Loss: 12234.255859\n",
      "    epoch          : 3440\n",
      "    loss           : 12216.507282135288\n",
      "    val_loss       : 12220.160958387381\n",
      "    val_log_likelihood: -12143.074578196081\n",
      "    val_log_marginal: -12151.000907147149\n",
      "Train Epoch: 3441 [256/118836 (0%)] Loss: 12241.102539\n",
      "Train Epoch: 3441 [33024/118836 (28%)] Loss: 12156.925781\n",
      "Train Epoch: 3441 [65792/118836 (55%)] Loss: 12217.974609\n",
      "Train Epoch: 3441 [98560/118836 (83%)] Loss: 12214.889648\n",
      "    epoch          : 3441\n",
      "    loss           : 12218.099243305418\n",
      "    val_loss       : 12214.912705888251\n",
      "    val_log_likelihood: -12142.358961758167\n",
      "    val_log_marginal: -12150.283836832827\n",
      "Train Epoch: 3442 [256/118836 (0%)] Loss: 12275.847656\n",
      "Train Epoch: 3442 [33024/118836 (28%)] Loss: 12334.849609\n",
      "Train Epoch: 3442 [65792/118836 (55%)] Loss: 12345.737305\n",
      "Train Epoch: 3442 [98560/118836 (83%)] Loss: 12220.925781\n",
      "    epoch          : 3442\n",
      "    loss           : 12220.158370812656\n",
      "    val_loss       : 12218.231356970762\n",
      "    val_log_likelihood: -12141.552550532464\n",
      "    val_log_marginal: -12149.474213087136\n",
      "Train Epoch: 3443 [256/118836 (0%)] Loss: 12238.599609\n",
      "Train Epoch: 3443 [33024/118836 (28%)] Loss: 12250.647461\n",
      "Train Epoch: 3443 [65792/118836 (55%)] Loss: 12192.638672\n",
      "Train Epoch: 3443 [98560/118836 (83%)] Loss: 12238.140625\n",
      "    epoch          : 3443\n",
      "    loss           : 12219.069307181813\n",
      "    val_loss       : 12222.628530350743\n",
      "    val_log_likelihood: -12142.075101775745\n",
      "    val_log_marginal: -12149.995667923755\n",
      "Train Epoch: 3444 [256/118836 (0%)] Loss: 12198.161133\n",
      "Train Epoch: 3444 [33024/118836 (28%)] Loss: 12179.673828\n",
      "Train Epoch: 3444 [65792/118836 (55%)] Loss: 12237.089844\n",
      "Train Epoch: 3444 [98560/118836 (83%)] Loss: 12215.339844\n",
      "    epoch          : 3444\n",
      "    loss           : 12215.13780015767\n",
      "    val_loss       : 12218.989970078888\n",
      "    val_log_likelihood: -12141.295773398728\n",
      "    val_log_marginal: -12149.203099835802\n",
      "Train Epoch: 3445 [256/118836 (0%)] Loss: 12219.208008\n",
      "Train Epoch: 3445 [33024/118836 (28%)] Loss: 12224.820312\n",
      "Train Epoch: 3445 [65792/118836 (55%)] Loss: 12347.924805\n",
      "Train Epoch: 3445 [98560/118836 (83%)] Loss: 12175.285156\n",
      "    epoch          : 3445\n",
      "    loss           : 12216.132603294302\n",
      "    val_loss       : 12215.48515443765\n",
      "    val_log_likelihood: -12142.622797443652\n",
      "    val_log_marginal: -12150.546746514743\n",
      "Train Epoch: 3446 [256/118836 (0%)] Loss: 12185.433594\n",
      "Train Epoch: 3446 [33024/118836 (28%)] Loss: 12289.006836\n",
      "Train Epoch: 3446 [65792/118836 (55%)] Loss: 12224.297852\n",
      "Train Epoch: 3446 [98560/118836 (83%)] Loss: 12225.626953\n",
      "    epoch          : 3446\n",
      "    loss           : 12219.264247150279\n",
      "    val_loss       : 12220.475314810352\n",
      "    val_log_likelihood: -12143.210639927109\n",
      "    val_log_marginal: -12151.111611693887\n",
      "Train Epoch: 3447 [256/118836 (0%)] Loss: 12297.814453\n",
      "Train Epoch: 3447 [33024/118836 (28%)] Loss: 12295.724609\n",
      "Train Epoch: 3447 [65792/118836 (55%)] Loss: 12257.830078\n",
      "Train Epoch: 3447 [98560/118836 (83%)] Loss: 12201.042969\n",
      "    epoch          : 3447\n",
      "    loss           : 12219.437377546008\n",
      "    val_loss       : 12216.018204793176\n",
      "    val_log_likelihood: -12141.821362728753\n",
      "    val_log_marginal: -12149.721252695557\n",
      "Train Epoch: 3448 [256/118836 (0%)] Loss: 12291.343750\n",
      "Train Epoch: 3448 [33024/118836 (28%)] Loss: 12230.453125\n",
      "Train Epoch: 3448 [65792/118836 (55%)] Loss: 12139.500000\n",
      "Train Epoch: 3448 [98560/118836 (83%)] Loss: 12252.062500\n",
      "    epoch          : 3448\n",
      "    loss           : 12215.114670731235\n",
      "    val_loss       : 12218.168228561166\n",
      "    val_log_likelihood: -12143.655554370864\n",
      "    val_log_marginal: -12151.559252965966\n",
      "Train Epoch: 3449 [256/118836 (0%)] Loss: 12248.093750\n",
      "Train Epoch: 3449 [33024/118836 (28%)] Loss: 12296.655273\n",
      "Train Epoch: 3449 [65792/118836 (55%)] Loss: 12311.613281\n",
      "Train Epoch: 3449 [98560/118836 (83%)] Loss: 12263.006836\n",
      "    epoch          : 3449\n",
      "    loss           : 12218.12578803505\n",
      "    val_loss       : 12221.477470360724\n",
      "    val_log_likelihood: -12143.18993680211\n",
      "    val_log_marginal: -12151.121974929772\n",
      "Train Epoch: 3450 [256/118836 (0%)] Loss: 12301.937500\n",
      "Train Epoch: 3450 [33024/118836 (28%)] Loss: 12272.422852\n",
      "Train Epoch: 3450 [65792/118836 (55%)] Loss: 12249.933594\n",
      "Train Epoch: 3450 [98560/118836 (83%)] Loss: 12287.406250\n",
      "    epoch          : 3450\n",
      "    loss           : 12218.223211654777\n",
      "    val_loss       : 12218.7852879714\n",
      "    val_log_likelihood: -12142.95440462805\n",
      "    val_log_marginal: -12150.861444760953\n",
      "Train Epoch: 3451 [256/118836 (0%)] Loss: 12287.171875\n",
      "Train Epoch: 3451 [33024/118836 (28%)] Loss: 12255.150391\n",
      "Train Epoch: 3451 [65792/118836 (55%)] Loss: 12210.487305\n",
      "Train Epoch: 3451 [98560/118836 (83%)] Loss: 12271.035156\n",
      "    epoch          : 3451\n",
      "    loss           : 12217.91574874638\n",
      "    val_loss       : 12220.60799397908\n",
      "    val_log_likelihood: -12144.160880957921\n",
      "    val_log_marginal: -12152.076284073426\n",
      "Train Epoch: 3452 [256/118836 (0%)] Loss: 12167.509766\n",
      "Train Epoch: 3452 [33024/118836 (28%)] Loss: 12261.360352\n",
      "Train Epoch: 3452 [65792/118836 (55%)] Loss: 12257.968750\n",
      "Train Epoch: 3452 [98560/118836 (83%)] Loss: 12192.448242\n",
      "    epoch          : 3452\n",
      "    loss           : 12220.798652198357\n",
      "    val_loss       : 12216.173940743716\n",
      "    val_log_likelihood: -12141.491586861559\n",
      "    val_log_marginal: -12149.404243151641\n",
      "Train Epoch: 3453 [256/118836 (0%)] Loss: 12276.211914\n",
      "Train Epoch: 3453 [33024/118836 (28%)] Loss: 12189.828125\n",
      "Train Epoch: 3453 [65792/118836 (55%)] Loss: 12227.361328\n",
      "Train Epoch: 3453 [98560/118836 (83%)] Loss: 12336.218750\n",
      "    epoch          : 3453\n",
      "    loss           : 12222.964487050247\n",
      "    val_loss       : 12217.545773280937\n",
      "    val_log_likelihood: -12143.085094699907\n",
      "    val_log_marginal: -12151.00637749081\n",
      "Train Epoch: 3454 [256/118836 (0%)] Loss: 12254.262695\n",
      "Train Epoch: 3454 [33024/118836 (28%)] Loss: 12124.779297\n",
      "Train Epoch: 3454 [65792/118836 (55%)] Loss: 12222.940430\n",
      "Train Epoch: 3454 [98560/118836 (83%)] Loss: 12183.458008\n",
      "    epoch          : 3454\n",
      "    loss           : 12218.33165952621\n",
      "    val_loss       : 12215.865308141536\n",
      "    val_log_likelihood: -12141.359628470067\n",
      "    val_log_marginal: -12149.263918954332\n",
      "Train Epoch: 3455 [256/118836 (0%)] Loss: 12279.204102\n",
      "Train Epoch: 3455 [33024/118836 (28%)] Loss: 12179.856445\n",
      "Train Epoch: 3455 [65792/118836 (55%)] Loss: 12215.125000\n",
      "Train Epoch: 3455 [98560/118836 (83%)] Loss: 12271.363281\n",
      "    epoch          : 3455\n",
      "    loss           : 12219.231520432693\n",
      "    val_loss       : 12220.483766897285\n",
      "    val_log_likelihood: -12142.42275172534\n",
      "    val_log_marginal: -12150.316756600747\n",
      "Train Epoch: 3456 [256/118836 (0%)] Loss: 12249.927734\n",
      "Train Epoch: 3456 [33024/118836 (28%)] Loss: 12305.160156\n",
      "Train Epoch: 3456 [65792/118836 (55%)] Loss: 12324.042969\n",
      "Train Epoch: 3456 [98560/118836 (83%)] Loss: 12347.136719\n",
      "    epoch          : 3456\n",
      "    loss           : 12217.195808454817\n",
      "    val_loss       : 12219.159055238246\n",
      "    val_log_likelihood: -12141.287448304383\n",
      "    val_log_marginal: -12149.184917863993\n",
      "Train Epoch: 3457 [256/118836 (0%)] Loss: 12217.896484\n",
      "Train Epoch: 3457 [33024/118836 (28%)] Loss: 12380.213867\n",
      "Train Epoch: 3457 [65792/118836 (55%)] Loss: 12196.007812\n",
      "Train Epoch: 3457 [98560/118836 (83%)] Loss: 12236.653320\n",
      "    epoch          : 3457\n",
      "    loss           : 12217.528629355354\n",
      "    val_loss       : 12217.82094514545\n",
      "    val_log_likelihood: -12140.502648431038\n",
      "    val_log_marginal: -12148.416604121427\n",
      "Train Epoch: 3458 [256/118836 (0%)] Loss: 12241.201172\n",
      "Train Epoch: 3458 [33024/118836 (28%)] Loss: 12222.562500\n",
      "Train Epoch: 3458 [65792/118836 (55%)] Loss: 12278.024414\n",
      "Train Epoch: 3458 [98560/118836 (83%)] Loss: 12241.375000\n",
      "    epoch          : 3458\n",
      "    loss           : 12215.293985053504\n",
      "    val_loss       : 12215.467806861669\n",
      "    val_log_likelihood: -12143.127297547044\n",
      "    val_log_marginal: -12151.043513991388\n",
      "Train Epoch: 3459 [256/118836 (0%)] Loss: 12207.765625\n",
      "Train Epoch: 3459 [33024/118836 (28%)] Loss: 12214.016602\n",
      "Train Epoch: 3459 [65792/118836 (55%)] Loss: 12204.647461\n",
      "Train Epoch: 3459 [98560/118836 (83%)] Loss: 12233.933594\n",
      "    epoch          : 3459\n",
      "    loss           : 12215.088852001913\n",
      "    val_loss       : 12218.272583063906\n",
      "    val_log_likelihood: -12140.109499554126\n",
      "    val_log_marginal: -12148.014461762974\n",
      "Train Epoch: 3460 [256/118836 (0%)] Loss: 12120.177734\n",
      "Train Epoch: 3460 [33024/118836 (28%)] Loss: 12325.132812\n",
      "Train Epoch: 3460 [65792/118836 (55%)] Loss: 12192.553711\n",
      "Train Epoch: 3460 [98560/118836 (83%)] Loss: 12265.234375\n",
      "    epoch          : 3460\n",
      "    loss           : 12217.33372444298\n",
      "    val_loss       : 12217.843048029586\n",
      "    val_log_likelihood: -12143.453578790582\n",
      "    val_log_marginal: -12151.376066231947\n",
      "Train Epoch: 3461 [256/118836 (0%)] Loss: 12342.109375\n",
      "Train Epoch: 3461 [33024/118836 (28%)] Loss: 12333.113281\n",
      "Train Epoch: 3461 [65792/118836 (55%)] Loss: 12423.894531\n",
      "Train Epoch: 3461 [98560/118836 (83%)] Loss: 12320.204102\n",
      "    epoch          : 3461\n",
      "    loss           : 12220.87242588141\n",
      "    val_loss       : 12217.87794509238\n",
      "    val_log_likelihood: -12141.382969525434\n",
      "    val_log_marginal: -12149.293554674814\n",
      "Train Epoch: 3462 [256/118836 (0%)] Loss: 12205.429688\n",
      "Train Epoch: 3462 [33024/118836 (28%)] Loss: 12217.401367\n",
      "Train Epoch: 3462 [65792/118836 (55%)] Loss: 12282.736328\n",
      "Train Epoch: 3462 [98560/118836 (83%)] Loss: 12192.499023\n",
      "    epoch          : 3462\n",
      "    loss           : 12216.28791065705\n",
      "    val_loss       : 12218.723227819159\n",
      "    val_log_likelihood: -12142.285229431607\n",
      "    val_log_marginal: -12150.196686985957\n",
      "Train Epoch: 3463 [256/118836 (0%)] Loss: 12206.489258\n",
      "Train Epoch: 3463 [33024/118836 (28%)] Loss: 12188.375977\n",
      "Train Epoch: 3463 [65792/118836 (55%)] Loss: 12207.968750\n",
      "Train Epoch: 3463 [98560/118836 (83%)] Loss: 12233.395508\n",
      "    epoch          : 3463\n",
      "    loss           : 12219.326416136788\n",
      "    val_loss       : 12221.694252476756\n",
      "    val_log_likelihood: -12142.457950785774\n",
      "    val_log_marginal: -12150.356487667055\n",
      "Train Epoch: 3464 [256/118836 (0%)] Loss: 12225.547852\n",
      "Train Epoch: 3464 [33024/118836 (28%)] Loss: 12154.631836\n",
      "Train Epoch: 3464 [65792/118836 (55%)] Loss: 12306.971680\n",
      "Train Epoch: 3464 [98560/118836 (83%)] Loss: 12205.050781\n",
      "    epoch          : 3464\n",
      "    loss           : 12221.370687939412\n",
      "    val_loss       : 12218.128398605695\n",
      "    val_log_likelihood: -12142.228862793374\n",
      "    val_log_marginal: -12150.157935994175\n",
      "Train Epoch: 3465 [256/118836 (0%)] Loss: 12200.323242\n",
      "Train Epoch: 3465 [33024/118836 (28%)] Loss: 12164.964844\n",
      "Train Epoch: 3465 [65792/118836 (55%)] Loss: 12223.931641\n",
      "Train Epoch: 3465 [98560/118836 (83%)] Loss: 12272.551758\n",
      "    epoch          : 3465\n",
      "    loss           : 12218.39764009512\n",
      "    val_loss       : 12215.896293357311\n",
      "    val_log_likelihood: -12141.934018623346\n",
      "    val_log_marginal: -12149.8459142527\n",
      "Train Epoch: 3466 [256/118836 (0%)] Loss: 12230.126953\n",
      "Train Epoch: 3466 [33024/118836 (28%)] Loss: 12333.617188\n",
      "Train Epoch: 3466 [65792/118836 (55%)] Loss: 12223.392578\n",
      "Train Epoch: 3466 [98560/118836 (83%)] Loss: 12188.546875\n",
      "    epoch          : 3466\n",
      "    loss           : 12218.535053504964\n",
      "    val_loss       : 12217.118060176657\n",
      "    val_log_likelihood: -12143.490818697011\n",
      "    val_log_marginal: -12151.410998396525\n",
      "Train Epoch: 3467 [256/118836 (0%)] Loss: 12224.130859\n",
      "Train Epoch: 3467 [33024/118836 (28%)] Loss: 12239.756836\n",
      "Train Epoch: 3467 [65792/118836 (55%)] Loss: 12189.612305\n",
      "Train Epoch: 3467 [98560/118836 (83%)] Loss: 12191.841797\n",
      "    epoch          : 3467\n",
      "    loss           : 12217.484409248345\n",
      "    val_loss       : 12216.29545550472\n",
      "    val_log_likelihood: -12142.674826981236\n",
      "    val_log_marginal: -12150.584592269552\n",
      "Train Epoch: 3468 [256/118836 (0%)] Loss: 12170.663086\n",
      "Train Epoch: 3468 [33024/118836 (28%)] Loss: 12260.158203\n",
      "Train Epoch: 3468 [65792/118836 (55%)] Loss: 12213.275391\n",
      "Train Epoch: 3468 [98560/118836 (83%)] Loss: 12188.577148\n",
      "    epoch          : 3468\n",
      "    loss           : 12216.64131610577\n",
      "    val_loss       : 12218.902894674678\n",
      "    val_log_likelihood: -12141.06935467716\n",
      "    val_log_marginal: -12148.993481879492\n",
      "Train Epoch: 3469 [256/118836 (0%)] Loss: 12278.367188\n",
      "Train Epoch: 3469 [33024/118836 (28%)] Loss: 12174.890625\n",
      "Train Epoch: 3469 [65792/118836 (55%)] Loss: 12236.304688\n",
      "Train Epoch: 3469 [98560/118836 (83%)] Loss: 12177.850586\n",
      "    epoch          : 3469\n",
      "    loss           : 12214.090870231079\n",
      "    val_loss       : 12216.514873306767\n",
      "    val_log_likelihood: -12141.003758432847\n",
      "    val_log_marginal: -12148.921787705474\n",
      "Train Epoch: 3470 [256/118836 (0%)] Loss: 12227.518555\n",
      "Train Epoch: 3470 [33024/118836 (28%)] Loss: 12407.353516\n",
      "Train Epoch: 3470 [65792/118836 (55%)] Loss: 12255.342773\n",
      "Train Epoch: 3470 [98560/118836 (83%)] Loss: 12200.895508\n",
      "    epoch          : 3470\n",
      "    loss           : 12220.489040206265\n",
      "    val_loss       : 12219.067727202944\n",
      "    val_log_likelihood: -12141.81972640095\n",
      "    val_log_marginal: -12149.7335310689\n",
      "Train Epoch: 3471 [256/118836 (0%)] Loss: 12212.677734\n",
      "Train Epoch: 3471 [33024/118836 (28%)] Loss: 12343.737305\n",
      "Train Epoch: 3471 [65792/118836 (55%)] Loss: 12190.396484\n",
      "Train Epoch: 3471 [98560/118836 (83%)] Loss: 12236.533203\n",
      "    epoch          : 3471\n",
      "    loss           : 12221.646055624484\n",
      "    val_loss       : 12219.6431745579\n",
      "    val_log_likelihood: -12142.11329352771\n",
      "    val_log_marginal: -12150.031304758839\n",
      "Train Epoch: 3472 [256/118836 (0%)] Loss: 12156.878906\n",
      "Train Epoch: 3472 [33024/118836 (28%)] Loss: 12200.836914\n",
      "Train Epoch: 3472 [65792/118836 (55%)] Loss: 12182.847656\n",
      "Train Epoch: 3472 [98560/118836 (83%)] Loss: 12223.812500\n",
      "    epoch          : 3472\n",
      "    loss           : 12219.555721573872\n",
      "    val_loss       : 12216.93521769765\n",
      "    val_log_likelihood: -12141.249166569736\n",
      "    val_log_marginal: -12149.170844475357\n",
      "Train Epoch: 3473 [256/118836 (0%)] Loss: 12309.622070\n",
      "Train Epoch: 3473 [33024/118836 (28%)] Loss: 12207.680664\n",
      "Train Epoch: 3473 [65792/118836 (55%)] Loss: 12280.936523\n",
      "Train Epoch: 3473 [98560/118836 (83%)] Loss: 12315.365234\n",
      "    epoch          : 3473\n",
      "    loss           : 12223.327229858096\n",
      "    val_loss       : 12219.61312709577\n",
      "    val_log_likelihood: -12141.662394347084\n",
      "    val_log_marginal: -12149.578791006208\n",
      "Train Epoch: 3474 [256/118836 (0%)] Loss: 12215.071289\n",
      "Train Epoch: 3474 [33024/118836 (28%)] Loss: 12239.765625\n",
      "Train Epoch: 3474 [65792/118836 (55%)] Loss: 12176.852539\n",
      "Train Epoch: 3474 [98560/118836 (83%)] Loss: 12181.584961\n",
      "    epoch          : 3474\n",
      "    loss           : 12220.237861223119\n",
      "    val_loss       : 12215.457059520862\n",
      "    val_log_likelihood: -12142.13027602228\n",
      "    val_log_marginal: -12150.026913048681\n",
      "Train Epoch: 3475 [256/118836 (0%)] Loss: 12260.576172\n",
      "Train Epoch: 3475 [33024/118836 (28%)] Loss: 12290.769531\n",
      "Train Epoch: 3475 [65792/118836 (55%)] Loss: 12287.608398\n",
      "Train Epoch: 3475 [98560/118836 (83%)] Loss: 12243.568359\n",
      "    epoch          : 3475\n",
      "    loss           : 12213.957029796062\n",
      "    val_loss       : 12221.338926230615\n",
      "    val_log_likelihood: -12142.10145733173\n",
      "    val_log_marginal: -12150.021840921969\n",
      "Train Epoch: 3476 [256/118836 (0%)] Loss: 12241.169922\n",
      "Train Epoch: 3476 [33024/118836 (28%)] Loss: 12384.446289\n",
      "Train Epoch: 3476 [65792/118836 (55%)] Loss: 12238.002930\n",
      "Train Epoch: 3476 [98560/118836 (83%)] Loss: 12195.576172\n",
      "    epoch          : 3476\n",
      "    loss           : 12220.234559650278\n",
      "    val_loss       : 12224.97852746772\n",
      "    val_log_likelihood: -12141.856837552989\n",
      "    val_log_marginal: -12149.750955866226\n",
      "Train Epoch: 3477 [256/118836 (0%)] Loss: 12161.710938\n",
      "Train Epoch: 3477 [33024/118836 (28%)] Loss: 12273.136719\n",
      "Train Epoch: 3477 [65792/118836 (55%)] Loss: 12197.595703\n",
      "Train Epoch: 3477 [98560/118836 (83%)] Loss: 12230.034180\n",
      "    epoch          : 3477\n",
      "    loss           : 12217.045969034327\n",
      "    val_loss       : 12220.40135658921\n",
      "    val_log_likelihood: -12140.825436020214\n",
      "    val_log_marginal: -12148.73051402179\n",
      "Train Epoch: 3478 [256/118836 (0%)] Loss: 12314.047852\n",
      "Train Epoch: 3478 [33024/118836 (28%)] Loss: 12338.371094\n",
      "Train Epoch: 3478 [65792/118836 (55%)] Loss: 12276.254883\n",
      "Train Epoch: 3478 [98560/118836 (83%)] Loss: 12252.727539\n",
      "    epoch          : 3478\n",
      "    loss           : 12219.633894553866\n",
      "    val_loss       : 12218.082747749695\n",
      "    val_log_likelihood: -12142.146086641853\n",
      "    val_log_marginal: -12150.06300059646\n",
      "Train Epoch: 3479 [256/118836 (0%)] Loss: 12220.409180\n",
      "Train Epoch: 3479 [33024/118836 (28%)] Loss: 12302.209961\n",
      "Train Epoch: 3479 [65792/118836 (55%)] Loss: 12211.798828\n",
      "Train Epoch: 3479 [98560/118836 (83%)] Loss: 12175.475586\n",
      "    epoch          : 3479\n",
      "    loss           : 12215.844101530189\n",
      "    val_loss       : 12220.614066978223\n",
      "    val_log_likelihood: -12140.93472798413\n",
      "    val_log_marginal: -12148.853599163682\n",
      "Train Epoch: 3480 [256/118836 (0%)] Loss: 12328.894531\n",
      "Train Epoch: 3480 [33024/118836 (28%)] Loss: 12327.786133\n",
      "Train Epoch: 3480 [65792/118836 (55%)] Loss: 12210.526367\n",
      "Train Epoch: 3480 [98560/118836 (83%)] Loss: 12176.341797\n",
      "    epoch          : 3480\n",
      "    loss           : 12223.193178440344\n",
      "    val_loss       : 12220.017494836846\n",
      "    val_log_likelihood: -12143.640681865178\n",
      "    val_log_marginal: -12151.579403910691\n",
      "Train Epoch: 3481 [256/118836 (0%)] Loss: 12229.372070\n",
      "Train Epoch: 3481 [33024/118836 (28%)] Loss: 12229.035156\n",
      "Train Epoch: 3481 [65792/118836 (55%)] Loss: 12300.789062\n",
      "Train Epoch: 3481 [98560/118836 (83%)] Loss: 12217.683594\n",
      "    epoch          : 3481\n",
      "    loss           : 12220.453085097446\n",
      "    val_loss       : 12218.696057664423\n",
      "    val_log_likelihood: -12143.420775821962\n",
      "    val_log_marginal: -12151.358656787948\n",
      "Train Epoch: 3482 [256/118836 (0%)] Loss: 12214.345703\n",
      "Train Epoch: 3482 [33024/118836 (28%)] Loss: 12203.152344\n",
      "Train Epoch: 3482 [65792/118836 (55%)] Loss: 12258.365234\n",
      "Train Epoch: 3482 [98560/118836 (83%)] Loss: 12162.423828\n",
      "    epoch          : 3482\n",
      "    loss           : 12217.087060102616\n",
      "    val_loss       : 12221.6191137719\n",
      "    val_log_likelihood: -12141.280192340002\n",
      "    val_log_marginal: -12149.191804417407\n",
      "Train Epoch: 3483 [256/118836 (0%)] Loss: 12152.606445\n",
      "Train Epoch: 3483 [33024/118836 (28%)] Loss: 12229.019531\n",
      "Train Epoch: 3483 [65792/118836 (55%)] Loss: 12197.106445\n",
      "Train Epoch: 3483 [98560/118836 (83%)] Loss: 12257.886719\n",
      "    epoch          : 3483\n",
      "    loss           : 12218.669939645368\n",
      "    val_loss       : 12218.130479523106\n",
      "    val_log_likelihood: -12142.705518022383\n",
      "    val_log_marginal: -12150.63194129279\n",
      "Train Epoch: 3484 [256/118836 (0%)] Loss: 12208.091797\n",
      "Train Epoch: 3484 [33024/118836 (28%)] Loss: 12302.312500\n",
      "Train Epoch: 3484 [65792/118836 (55%)] Loss: 12268.581055\n",
      "Train Epoch: 3484 [98560/118836 (83%)] Loss: 12238.761719\n",
      "    epoch          : 3484\n",
      "    loss           : 12218.904994765819\n",
      "    val_loss       : 12217.702947697833\n",
      "    val_log_likelihood: -12143.39483851582\n",
      "    val_log_marginal: -12151.297741484701\n",
      "Train Epoch: 3485 [256/118836 (0%)] Loss: 12159.666016\n",
      "Train Epoch: 3485 [33024/118836 (28%)] Loss: 12217.392578\n",
      "Train Epoch: 3485 [65792/118836 (55%)] Loss: 12230.255859\n",
      "Train Epoch: 3485 [98560/118836 (83%)] Loss: 12195.766602\n",
      "    epoch          : 3485\n",
      "    loss           : 12213.205971489866\n",
      "    val_loss       : 12217.14631568241\n",
      "    val_log_likelihood: -12143.119185374018\n",
      "    val_log_marginal: -12151.039945460037\n",
      "Train Epoch: 3486 [256/118836 (0%)] Loss: 12184.482422\n",
      "Train Epoch: 3486 [33024/118836 (28%)] Loss: 12275.319336\n",
      "Train Epoch: 3486 [65792/118836 (55%)] Loss: 12393.614258\n",
      "Train Epoch: 3486 [98560/118836 (83%)] Loss: 12259.228516\n",
      "    epoch          : 3486\n",
      "    loss           : 12219.878664249896\n",
      "    val_loss       : 12219.069900694498\n",
      "    val_log_likelihood: -12142.06028565059\n",
      "    val_log_marginal: -12149.996119429947\n",
      "Train Epoch: 3487 [256/118836 (0%)] Loss: 12245.084961\n",
      "Train Epoch: 3487 [33024/118836 (28%)] Loss: 12105.765625\n",
      "Train Epoch: 3487 [65792/118836 (55%)] Loss: 12225.807617\n",
      "Train Epoch: 3487 [98560/118836 (83%)] Loss: 12216.781250\n",
      "    epoch          : 3487\n",
      "    loss           : 12221.696703273625\n",
      "    val_loss       : 12219.065314353698\n",
      "    val_log_likelihood: -12141.206412356545\n",
      "    val_log_marginal: -12149.133001991793\n",
      "Train Epoch: 3488 [256/118836 (0%)] Loss: 12230.424805\n",
      "Train Epoch: 3488 [33024/118836 (28%)] Loss: 12245.453125\n",
      "Train Epoch: 3488 [65792/118836 (55%)] Loss: 12173.199219\n",
      "Train Epoch: 3488 [98560/118836 (83%)] Loss: 12220.127930\n",
      "    epoch          : 3488\n",
      "    loss           : 12218.486475457505\n",
      "    val_loss       : 12218.55360078085\n",
      "    val_log_likelihood: -12143.305822703422\n",
      "    val_log_marginal: -12151.224892064574\n",
      "Train Epoch: 3489 [256/118836 (0%)] Loss: 12179.365234\n",
      "Train Epoch: 3489 [33024/118836 (28%)] Loss: 12220.958984\n",
      "Train Epoch: 3489 [65792/118836 (55%)] Loss: 12158.314453\n",
      "Train Epoch: 3489 [98560/118836 (83%)] Loss: 12162.572266\n",
      "    epoch          : 3489\n",
      "    loss           : 12221.857016872156\n",
      "    val_loss       : 12217.006005230276\n",
      "    val_log_likelihood: -12141.52889284145\n",
      "    val_log_marginal: -12149.44595423784\n",
      "Train Epoch: 3490 [256/118836 (0%)] Loss: 12330.231445\n",
      "Train Epoch: 3490 [33024/118836 (28%)] Loss: 12418.837891\n",
      "Train Epoch: 3490 [65792/118836 (55%)] Loss: 12418.548828\n",
      "Train Epoch: 3490 [98560/118836 (83%)] Loss: 12248.817383\n",
      "    epoch          : 3490\n",
      "    loss           : 12223.69176876551\n",
      "    val_loss       : 12218.433377565778\n",
      "    val_log_likelihood: -12142.834613122932\n",
      "    val_log_marginal: -12150.738971867713\n",
      "Train Epoch: 3491 [256/118836 (0%)] Loss: 12273.190430\n",
      "Train Epoch: 3491 [33024/118836 (28%)] Loss: 12210.243164\n",
      "Train Epoch: 3491 [65792/118836 (55%)] Loss: 12128.227539\n",
      "Train Epoch: 3491 [98560/118836 (83%)] Loss: 12306.826172\n",
      "    epoch          : 3491\n",
      "    loss           : 12219.280015120969\n",
      "    val_loss       : 12220.586747696481\n",
      "    val_log_likelihood: -12143.08621504084\n",
      "    val_log_marginal: -12151.036471790656\n",
      "Train Epoch: 3492 [256/118836 (0%)] Loss: 12183.867188\n",
      "Train Epoch: 3492 [33024/118836 (28%)] Loss: 12170.400391\n",
      "Train Epoch: 3492 [65792/118836 (55%)] Loss: 12174.435547\n",
      "Train Epoch: 3492 [98560/118836 (83%)] Loss: 12259.701172\n",
      "    epoch          : 3492\n",
      "    loss           : 12220.681224475291\n",
      "    val_loss       : 12218.497433051945\n",
      "    val_log_likelihood: -12142.50721848506\n",
      "    val_log_marginal: -12150.441811835917\n",
      "Train Epoch: 3493 [256/118836 (0%)] Loss: 12267.939453\n",
      "Train Epoch: 3493 [33024/118836 (28%)] Loss: 12339.444336\n",
      "Train Epoch: 3493 [65792/118836 (55%)] Loss: 12332.417969\n",
      "Train Epoch: 3493 [98560/118836 (83%)] Loss: 12205.402344\n",
      "    epoch          : 3493\n",
      "    loss           : 12215.344879710763\n",
      "    val_loss       : 12221.472229323545\n",
      "    val_log_likelihood: -12141.493580212209\n",
      "    val_log_marginal: -12149.439206751116\n",
      "Train Epoch: 3494 [256/118836 (0%)] Loss: 12187.565430\n",
      "Train Epoch: 3494 [33024/118836 (28%)] Loss: 12166.060547\n",
      "Train Epoch: 3494 [65792/118836 (55%)] Loss: 12234.365234\n",
      "Train Epoch: 3494 [98560/118836 (83%)] Loss: 12232.204102\n",
      "    epoch          : 3494\n",
      "    loss           : 12216.717143397178\n",
      "    val_loss       : 12223.907765884433\n",
      "    val_log_likelihood: -12142.371884046734\n",
      "    val_log_marginal: -12150.282951127665\n",
      "Train Epoch: 3495 [256/118836 (0%)] Loss: 12291.320312\n",
      "Train Epoch: 3495 [33024/118836 (28%)] Loss: 12266.439453\n",
      "Train Epoch: 3495 [65792/118836 (55%)] Loss: 12258.209961\n",
      "Train Epoch: 3495 [98560/118836 (83%)] Loss: 12322.943359\n",
      "    epoch          : 3495\n",
      "    loss           : 12212.59649632961\n",
      "    val_loss       : 12215.00658993663\n",
      "    val_log_likelihood: -12142.897508109749\n",
      "    val_log_marginal: -12150.80223684161\n",
      "Train Epoch: 3496 [256/118836 (0%)] Loss: 12147.481445\n",
      "Train Epoch: 3496 [33024/118836 (28%)] Loss: 12314.497070\n",
      "Train Epoch: 3496 [65792/118836 (55%)] Loss: 12253.175781\n",
      "Train Epoch: 3496 [98560/118836 (83%)] Loss: 12154.699219\n",
      "    epoch          : 3496\n",
      "    loss           : 12222.18062900641\n",
      "    val_loss       : 12217.485714932729\n",
      "    val_log_likelihood: -12142.548274820358\n",
      "    val_log_marginal: -12150.457183898508\n",
      "Train Epoch: 3497 [256/118836 (0%)] Loss: 12187.770508\n",
      "Train Epoch: 3497 [33024/118836 (28%)] Loss: 12191.127930\n",
      "Train Epoch: 3497 [65792/118836 (55%)] Loss: 12146.849609\n",
      "Train Epoch: 3497 [98560/118836 (83%)] Loss: 12321.598633\n",
      "    epoch          : 3497\n",
      "    loss           : 12217.986419238523\n",
      "    val_loss       : 12218.32675422582\n",
      "    val_log_likelihood: -12143.09052257806\n",
      "    val_log_marginal: -12151.010831125868\n",
      "Train Epoch: 3498 [256/118836 (0%)] Loss: 12298.910156\n",
      "Train Epoch: 3498 [33024/118836 (28%)] Loss: 12161.919922\n",
      "Train Epoch: 3498 [65792/118836 (55%)] Loss: 12310.404297\n",
      "Train Epoch: 3498 [98560/118836 (83%)] Loss: 12199.077148\n",
      "    epoch          : 3498\n",
      "    loss           : 12216.668891032103\n",
      "    val_loss       : 12218.385271305762\n",
      "    val_log_likelihood: -12142.612257515251\n",
      "    val_log_marginal: -12150.529872573588\n",
      "Train Epoch: 3499 [256/118836 (0%)] Loss: 12290.399414\n",
      "Train Epoch: 3499 [33024/118836 (28%)] Loss: 12262.570312\n",
      "Train Epoch: 3499 [65792/118836 (55%)] Loss: 12187.321289\n",
      "Train Epoch: 3499 [98560/118836 (83%)] Loss: 12189.991211\n",
      "    epoch          : 3499\n",
      "    loss           : 12219.07757815731\n",
      "    val_loss       : 12220.050147119508\n",
      "    val_log_likelihood: -12142.090979922716\n",
      "    val_log_marginal: -12149.989168898293\n",
      "Train Epoch: 3500 [256/118836 (0%)] Loss: 12190.685547\n",
      "Train Epoch: 3500 [33024/118836 (28%)] Loss: 12234.579102\n",
      "Train Epoch: 3500 [65792/118836 (55%)] Loss: 12262.050781\n",
      "Train Epoch: 3500 [98560/118836 (83%)] Loss: 12215.257812\n",
      "    epoch          : 3500\n",
      "    loss           : 12221.086183215726\n",
      "    val_loss       : 12221.950243237958\n",
      "    val_log_likelihood: -12141.873162220843\n",
      "    val_log_marginal: -12149.761700171652\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch3500.pth ...\n",
      "Train Epoch: 3501 [256/118836 (0%)] Loss: 12209.035156\n",
      "Train Epoch: 3501 [33024/118836 (28%)] Loss: 12280.395508\n",
      "Train Epoch: 3501 [65792/118836 (55%)] Loss: 12229.712891\n",
      "Train Epoch: 3501 [98560/118836 (83%)] Loss: 12245.250977\n",
      "    epoch          : 3501\n",
      "    loss           : 12220.57958507806\n",
      "    val_loss       : 12223.158693113926\n",
      "    val_log_likelihood: -12142.949871407154\n",
      "    val_log_marginal: -12150.862995711152\n",
      "Train Epoch: 3502 [256/118836 (0%)] Loss: 12153.658203\n",
      "Train Epoch: 3502 [33024/118836 (28%)] Loss: 12252.681641\n",
      "Train Epoch: 3502 [65792/118836 (55%)] Loss: 12199.010742\n",
      "Train Epoch: 3502 [98560/118836 (83%)] Loss: 12186.875000\n",
      "    epoch          : 3502\n",
      "    loss           : 12221.642187176903\n",
      "    val_loss       : 12215.15390861955\n",
      "    val_log_likelihood: -12141.391363278019\n",
      "    val_log_marginal: -12149.302788320356\n",
      "Train Epoch: 3503 [256/118836 (0%)] Loss: 12246.980469\n",
      "Train Epoch: 3503 [33024/118836 (28%)] Loss: 12177.592773\n",
      "Train Epoch: 3503 [65792/118836 (55%)] Loss: 12191.073242\n",
      "Train Epoch: 3503 [98560/118836 (83%)] Loss: 12274.849609\n",
      "    epoch          : 3503\n",
      "    loss           : 12216.276095785515\n",
      "    val_loss       : 12218.375903971752\n",
      "    val_log_likelihood: -12141.767483134305\n",
      "    val_log_marginal: -12149.65939565258\n",
      "Train Epoch: 3504 [256/118836 (0%)] Loss: 12177.816406\n",
      "Train Epoch: 3504 [33024/118836 (28%)] Loss: 12168.054688\n",
      "Train Epoch: 3504 [65792/118836 (55%)] Loss: 12292.807617\n",
      "Train Epoch: 3504 [98560/118836 (83%)] Loss: 12193.960938\n",
      "    epoch          : 3504\n",
      "    loss           : 12220.635872880479\n",
      "    val_loss       : 12214.858064974216\n",
      "    val_log_likelihood: -12144.472042364558\n",
      "    val_log_marginal: -12152.372273399542\n",
      "Train Epoch: 3505 [256/118836 (0%)] Loss: 12263.644531\n",
      "Train Epoch: 3505 [33024/118836 (28%)] Loss: 12295.749023\n",
      "Train Epoch: 3505 [65792/118836 (55%)] Loss: 12205.940430\n",
      "Train Epoch: 3505 [98560/118836 (83%)] Loss: 12241.859375\n",
      "    epoch          : 3505\n",
      "    loss           : 12216.170890683157\n",
      "    val_loss       : 12220.872329110149\n",
      "    val_log_likelihood: -12142.07036080309\n",
      "    val_log_marginal: -12149.969722431677\n",
      "Train Epoch: 3506 [256/118836 (0%)] Loss: 12225.265625\n",
      "Train Epoch: 3506 [33024/118836 (28%)] Loss: 12195.351562\n",
      "Train Epoch: 3506 [65792/118836 (55%)] Loss: 12197.546875\n",
      "Train Epoch: 3506 [98560/118836 (83%)] Loss: 12278.730469\n",
      "    epoch          : 3506\n",
      "    loss           : 12218.046650608716\n",
      "    val_loss       : 12220.332832712007\n",
      "    val_log_likelihood: -12142.552362328113\n",
      "    val_log_marginal: -12150.444030671593\n",
      "Train Epoch: 3507 [256/118836 (0%)] Loss: 12228.136719\n",
      "Train Epoch: 3507 [33024/118836 (28%)] Loss: 12173.280273\n",
      "Train Epoch: 3507 [65792/118836 (55%)] Loss: 12243.045898\n",
      "Train Epoch: 3507 [98560/118836 (83%)] Loss: 12314.980469\n",
      "    epoch          : 3507\n",
      "    loss           : 12221.199419393612\n",
      "    val_loss       : 12218.911599871657\n",
      "    val_log_likelihood: -12141.674649116005\n",
      "    val_log_marginal: -12149.59111795095\n",
      "Train Epoch: 3508 [256/118836 (0%)] Loss: 12278.345703\n",
      "Train Epoch: 3508 [33024/118836 (28%)] Loss: 12278.413086\n",
      "Train Epoch: 3508 [65792/118836 (55%)] Loss: 12308.734375\n",
      "Train Epoch: 3508 [98560/118836 (83%)] Loss: 12382.560547\n",
      "    epoch          : 3508\n",
      "    loss           : 12220.15502578319\n",
      "    val_loss       : 12220.337213533754\n",
      "    val_log_likelihood: -12142.447960931038\n",
      "    val_log_marginal: -12150.359634242468\n",
      "Train Epoch: 3509 [256/118836 (0%)] Loss: 12221.175781\n",
      "Train Epoch: 3509 [33024/118836 (28%)] Loss: 12213.904297\n",
      "Train Epoch: 3509 [65792/118836 (55%)] Loss: 12163.003906\n",
      "Train Epoch: 3509 [98560/118836 (83%)] Loss: 12293.345703\n",
      "    epoch          : 3509\n",
      "    loss           : 12217.94550377378\n",
      "    val_loss       : 12217.811688886599\n",
      "    val_log_likelihood: -12141.57938847317\n",
      "    val_log_marginal: -12149.475076247629\n",
      "Train Epoch: 3510 [256/118836 (0%)] Loss: 12260.496094\n",
      "Train Epoch: 3510 [33024/118836 (28%)] Loss: 12329.826172\n",
      "Train Epoch: 3510 [65792/118836 (55%)] Loss: 12204.308594\n",
      "Train Epoch: 3510 [98560/118836 (83%)] Loss: 12213.408203\n",
      "    epoch          : 3510\n",
      "    loss           : 12219.206949021662\n",
      "    val_loss       : 12216.669400363175\n",
      "    val_log_likelihood: -12144.292527721775\n",
      "    val_log_marginal: -12152.197955585132\n",
      "Train Epoch: 3511 [256/118836 (0%)] Loss: 12196.833984\n",
      "Train Epoch: 3511 [33024/118836 (28%)] Loss: 12243.814453\n",
      "Train Epoch: 3511 [65792/118836 (55%)] Loss: 12149.354492\n",
      "Train Epoch: 3511 [98560/118836 (83%)] Loss: 12348.836914\n",
      "    epoch          : 3511\n",
      "    loss           : 12219.638368324804\n",
      "    val_loss       : 12216.036042465987\n",
      "    val_log_likelihood: -12142.42676023573\n",
      "    val_log_marginal: -12150.325932437561\n",
      "Train Epoch: 3512 [256/118836 (0%)] Loss: 12229.041016\n",
      "Train Epoch: 3512 [33024/118836 (28%)] Loss: 12202.533203\n",
      "Train Epoch: 3512 [65792/118836 (55%)] Loss: 12289.093750\n",
      "Train Epoch: 3512 [98560/118836 (83%)] Loss: 12210.104492\n",
      "    epoch          : 3512\n",
      "    loss           : 12218.023536529414\n",
      "    val_loss       : 12216.539143078528\n",
      "    val_log_likelihood: -12141.167078777657\n",
      "    val_log_marginal: -12149.077722308948\n",
      "Train Epoch: 3513 [256/118836 (0%)] Loss: 12242.232422\n",
      "Train Epoch: 3513 [33024/118836 (28%)] Loss: 12235.365234\n",
      "Train Epoch: 3513 [65792/118836 (55%)] Loss: 12265.802734\n",
      "Train Epoch: 3513 [98560/118836 (83%)] Loss: 12190.756836\n",
      "    epoch          : 3513\n",
      "    loss           : 12215.253237922612\n",
      "    val_loss       : 12215.498476011166\n",
      "    val_log_likelihood: -12142.133656754033\n",
      "    val_log_marginal: -12150.048407398699\n",
      "Train Epoch: 3514 [256/118836 (0%)] Loss: 12231.541992\n",
      "Train Epoch: 3514 [33024/118836 (28%)] Loss: 12219.166016\n",
      "Train Epoch: 3514 [65792/118836 (55%)] Loss: 12180.019531\n",
      "Train Epoch: 3514 [98560/118836 (83%)] Loss: 12283.610352\n",
      "    epoch          : 3514\n",
      "    loss           : 12217.544760487748\n",
      "    val_loss       : 12217.590642283369\n",
      "    val_log_likelihood: -12141.217810109078\n",
      "    val_log_marginal: -12149.12737631815\n",
      "Train Epoch: 3515 [256/118836 (0%)] Loss: 12244.582031\n",
      "Train Epoch: 3515 [33024/118836 (28%)] Loss: 12226.559570\n",
      "Train Epoch: 3515 [65792/118836 (55%)] Loss: 12274.960938\n",
      "Train Epoch: 3515 [98560/118836 (83%)] Loss: 12174.337891\n",
      "    epoch          : 3515\n",
      "    loss           : 12215.995433015405\n",
      "    val_loss       : 12218.387076638872\n",
      "    val_log_likelihood: -12141.753476691738\n",
      "    val_log_marginal: -12149.674225346462\n",
      "Train Epoch: 3516 [256/118836 (0%)] Loss: 12260.476562\n",
      "Train Epoch: 3516 [33024/118836 (28%)] Loss: 12218.657227\n",
      "Train Epoch: 3516 [65792/118836 (55%)] Loss: 12286.277344\n",
      "Train Epoch: 3516 [98560/118836 (83%)] Loss: 12184.500000\n",
      "    epoch          : 3516\n",
      "    loss           : 12217.185241224668\n",
      "    val_loss       : 12218.579952686689\n",
      "    val_log_likelihood: -12142.52470097317\n",
      "    val_log_marginal: -12150.433711545238\n",
      "Train Epoch: 3517 [256/118836 (0%)] Loss: 12260.683594\n",
      "Train Epoch: 3517 [33024/118836 (28%)] Loss: 12216.685547\n",
      "Train Epoch: 3517 [65792/118836 (55%)] Loss: 12160.136719\n",
      "Train Epoch: 3517 [98560/118836 (83%)] Loss: 12206.615234\n",
      "    epoch          : 3517\n",
      "    loss           : 12223.423571262407\n",
      "    val_loss       : 12215.835081421415\n",
      "    val_log_likelihood: -12141.955577149247\n",
      "    val_log_marginal: -12149.859349022383\n",
      "Train Epoch: 3518 [256/118836 (0%)] Loss: 12181.386719\n",
      "Train Epoch: 3518 [33024/118836 (28%)] Loss: 12114.466797\n",
      "Train Epoch: 3518 [65792/118836 (55%)] Loss: 12236.771484\n",
      "Train Epoch: 3518 [98560/118836 (83%)] Loss: 12118.285156\n",
      "    epoch          : 3518\n",
      "    loss           : 12215.207586816327\n",
      "    val_loss       : 12221.064198799855\n",
      "    val_log_likelihood: -12142.199020368073\n",
      "    val_log_marginal: -12150.106590473288\n",
      "Train Epoch: 3519 [256/118836 (0%)] Loss: 12237.908203\n",
      "Train Epoch: 3519 [33024/118836 (28%)] Loss: 12195.229492\n",
      "Train Epoch: 3519 [65792/118836 (55%)] Loss: 12209.792969\n",
      "Train Epoch: 3519 [98560/118836 (83%)] Loss: 12281.057617\n",
      "    epoch          : 3519\n",
      "    loss           : 12220.624018914134\n",
      "    val_loss       : 12218.609890516529\n",
      "    val_log_likelihood: -12141.904684592122\n",
      "    val_log_marginal: -12149.810430829599\n",
      "Train Epoch: 3520 [256/118836 (0%)] Loss: 12204.747070\n",
      "Train Epoch: 3520 [33024/118836 (28%)] Loss: 12276.012695\n",
      "Train Epoch: 3520 [65792/118836 (55%)] Loss: 12309.258789\n",
      "Train Epoch: 3520 [98560/118836 (83%)] Loss: 12197.825195\n",
      "    epoch          : 3520\n",
      "    loss           : 12218.643050655242\n",
      "    val_loss       : 12217.66860818974\n",
      "    val_log_likelihood: -12141.595455632236\n",
      "    val_log_marginal: -12149.494842972119\n",
      "Train Epoch: 3521 [256/118836 (0%)] Loss: 12413.307617\n",
      "Train Epoch: 3521 [33024/118836 (28%)] Loss: 12161.856445\n",
      "Train Epoch: 3521 [65792/118836 (55%)] Loss: 12180.066406\n",
      "Train Epoch: 3521 [98560/118836 (83%)] Loss: 12234.460938\n",
      "    epoch          : 3521\n",
      "    loss           : 12219.907899736352\n",
      "    val_loss       : 12217.253185363095\n",
      "    val_log_likelihood: -12143.465859245762\n",
      "    val_log_marginal: -12151.356492651556\n",
      "Train Epoch: 3522 [256/118836 (0%)] Loss: 12258.792969\n",
      "Train Epoch: 3522 [33024/118836 (28%)] Loss: 12261.939453\n",
      "Train Epoch: 3522 [65792/118836 (55%)] Loss: 12307.227539\n",
      "Train Epoch: 3522 [98560/118836 (83%)] Loss: 12222.183594\n",
      "    epoch          : 3522\n",
      "    loss           : 12218.98331249354\n",
      "    val_loss       : 12223.409382267579\n",
      "    val_log_likelihood: -12144.00009983716\n",
      "    val_log_marginal: -12151.93191979625\n",
      "Train Epoch: 3523 [256/118836 (0%)] Loss: 12386.792969\n",
      "Train Epoch: 3523 [33024/118836 (28%)] Loss: 12208.658203\n",
      "Train Epoch: 3523 [65792/118836 (55%)] Loss: 12358.156250\n",
      "Train Epoch: 3523 [98560/118836 (83%)] Loss: 12224.056641\n",
      "    epoch          : 3523\n",
      "    loss           : 12218.04297149633\n",
      "    val_loss       : 12219.287339166574\n",
      "    val_log_likelihood: -12140.955086848635\n",
      "    val_log_marginal: -12148.867358427744\n",
      "Train Epoch: 3524 [256/118836 (0%)] Loss: 12280.827148\n",
      "Train Epoch: 3524 [33024/118836 (28%)] Loss: 12301.981445\n",
      "Train Epoch: 3524 [65792/118836 (55%)] Loss: 12200.173828\n",
      "Train Epoch: 3524 [98560/118836 (83%)] Loss: 12338.111328\n",
      "    epoch          : 3524\n",
      "    loss           : 12221.722060134925\n",
      "    val_loss       : 12219.531002929058\n",
      "    val_log_likelihood: -12141.648010203422\n",
      "    val_log_marginal: -12149.550125183212\n",
      "Train Epoch: 3525 [256/118836 (0%)] Loss: 12212.872070\n",
      "Train Epoch: 3525 [33024/118836 (28%)] Loss: 12206.563477\n",
      "Train Epoch: 3525 [65792/118836 (55%)] Loss: 12261.165039\n",
      "Train Epoch: 3525 [98560/118836 (83%)] Loss: 12119.875000\n",
      "    epoch          : 3525\n",
      "    loss           : 12219.657003625156\n",
      "    val_loss       : 12216.935935895719\n",
      "    val_log_likelihood: -12141.249135229269\n",
      "    val_log_marginal: -12149.150928934423\n",
      "Train Epoch: 3526 [256/118836 (0%)] Loss: 12206.406250\n",
      "Train Epoch: 3526 [33024/118836 (28%)] Loss: 12237.318359\n",
      "Train Epoch: 3526 [65792/118836 (55%)] Loss: 12308.033203\n",
      "Train Epoch: 3526 [98560/118836 (83%)] Loss: 12211.486328\n",
      "    epoch          : 3526\n",
      "    loss           : 12218.640765385908\n",
      "    val_loss       : 12221.620248550656\n",
      "    val_log_likelihood: -12144.155837081265\n",
      "    val_log_marginal: -12152.072603410577\n",
      "Train Epoch: 3527 [256/118836 (0%)] Loss: 12158.248047\n",
      "Train Epoch: 3527 [33024/118836 (28%)] Loss: 12281.047852\n",
      "Train Epoch: 3527 [65792/118836 (55%)] Loss: 12163.032227\n",
      "Train Epoch: 3527 [98560/118836 (83%)] Loss: 12252.544922\n",
      "    epoch          : 3527\n",
      "    loss           : 12220.772692760029\n",
      "    val_loss       : 12223.955249963436\n",
      "    val_log_likelihood: -12141.93278665219\n",
      "    val_log_marginal: -12149.850440197042\n",
      "Train Epoch: 3528 [256/118836 (0%)] Loss: 12175.489258\n",
      "Train Epoch: 3528 [33024/118836 (28%)] Loss: 12233.354492\n",
      "Train Epoch: 3528 [65792/118836 (55%)] Loss: 12252.539062\n",
      "Train Epoch: 3528 [98560/118836 (83%)] Loss: 12163.492188\n",
      "    epoch          : 3528\n",
      "    loss           : 12216.866769573253\n",
      "    val_loss       : 12221.867674925072\n",
      "    val_log_likelihood: -12142.742775537634\n",
      "    val_log_marginal: -12150.667161223051\n",
      "Train Epoch: 3529 [256/118836 (0%)] Loss: 12187.253906\n",
      "Train Epoch: 3529 [33024/118836 (28%)] Loss: 12282.957031\n",
      "Train Epoch: 3529 [65792/118836 (55%)] Loss: 12159.359375\n",
      "Train Epoch: 3529 [98560/118836 (83%)] Loss: 12186.312500\n",
      "    epoch          : 3529\n",
      "    loss           : 12217.24334192773\n",
      "    val_loss       : 12216.476270489584\n",
      "    val_log_likelihood: -12141.47429936285\n",
      "    val_log_marginal: -12149.389614280353\n",
      "Train Epoch: 3530 [256/118836 (0%)] Loss: 12195.367188\n",
      "Train Epoch: 3530 [33024/118836 (28%)] Loss: 12301.069336\n",
      "Train Epoch: 3530 [65792/118836 (55%)] Loss: 12271.530273\n",
      "Train Epoch: 3530 [98560/118836 (83%)] Loss: 12280.609375\n",
      "    epoch          : 3530\n",
      "    loss           : 12215.63424414547\n",
      "    val_loss       : 12216.912473381813\n",
      "    val_log_likelihood: -12142.21072748656\n",
      "    val_log_marginal: -12150.132935229416\n",
      "Train Epoch: 3531 [256/118836 (0%)] Loss: 12173.767578\n",
      "Train Epoch: 3531 [33024/118836 (28%)] Loss: 12279.424805\n",
      "Train Epoch: 3531 [65792/118836 (55%)] Loss: 12283.164062\n",
      "Train Epoch: 3531 [98560/118836 (83%)] Loss: 12217.298828\n",
      "    epoch          : 3531\n",
      "    loss           : 12222.140513531327\n",
      "    val_loss       : 12216.590116897325\n",
      "    val_log_likelihood: -12142.020417183623\n",
      "    val_log_marginal: -12149.959198484461\n",
      "Train Epoch: 3532 [256/118836 (0%)] Loss: 12262.814453\n",
      "Train Epoch: 3532 [33024/118836 (28%)] Loss: 12290.659180\n",
      "Train Epoch: 3532 [65792/118836 (55%)] Loss: 12212.353516\n",
      "Train Epoch: 3532 [98560/118836 (83%)] Loss: 12186.859375\n",
      "    epoch          : 3532\n",
      "    loss           : 12220.97841982656\n",
      "    val_loss       : 12218.543972756415\n",
      "    val_log_likelihood: -12142.757643519955\n",
      "    val_log_marginal: -12150.675837013308\n",
      "Train Epoch: 3533 [256/118836 (0%)] Loss: 12252.074219\n",
      "Train Epoch: 3533 [33024/118836 (28%)] Loss: 12196.046875\n",
      "Train Epoch: 3533 [65792/118836 (55%)] Loss: 12259.000000\n",
      "Train Epoch: 3533 [98560/118836 (83%)] Loss: 12282.628906\n",
      "    epoch          : 3533\n",
      "    loss           : 12214.514971535102\n",
      "    val_loss       : 12215.695892892365\n",
      "    val_log_likelihood: -12141.585532981802\n",
      "    val_log_marginal: -12149.50197766127\n",
      "Train Epoch: 3534 [256/118836 (0%)] Loss: 12212.763672\n",
      "Train Epoch: 3534 [33024/118836 (28%)] Loss: 12271.591797\n",
      "Train Epoch: 3534 [65792/118836 (55%)] Loss: 12280.578125\n",
      "Train Epoch: 3534 [98560/118836 (83%)] Loss: 12242.273438\n",
      "    epoch          : 3534\n",
      "    loss           : 12218.68275999664\n",
      "    val_loss       : 12217.118483295766\n",
      "    val_log_likelihood: -12142.286171745762\n",
      "    val_log_marginal: -12150.196000642381\n",
      "Train Epoch: 3535 [256/118836 (0%)] Loss: 12160.731445\n",
      "Train Epoch: 3535 [33024/118836 (28%)] Loss: 12248.339844\n",
      "Train Epoch: 3535 [65792/118836 (55%)] Loss: 12301.950195\n",
      "Train Epoch: 3535 [98560/118836 (83%)] Loss: 12188.322266\n",
      "    epoch          : 3535\n",
      "    loss           : 12223.947417480873\n",
      "    val_loss       : 12220.851837421575\n",
      "    val_log_likelihood: -12143.553847607785\n",
      "    val_log_marginal: -12151.476117851935\n",
      "Train Epoch: 3536 [256/118836 (0%)] Loss: 12211.072266\n",
      "Train Epoch: 3536 [33024/118836 (28%)] Loss: 12249.041016\n",
      "Train Epoch: 3536 [65792/118836 (55%)] Loss: 12273.967773\n",
      "Train Epoch: 3536 [98560/118836 (83%)] Loss: 12245.142578\n",
      "    epoch          : 3536\n",
      "    loss           : 12218.73791469577\n",
      "    val_loss       : 12221.397015026885\n",
      "    val_log_likelihood: -12141.935473693393\n",
      "    val_log_marginal: -12149.86780836278\n",
      "Train Epoch: 3537 [256/118836 (0%)] Loss: 12312.985352\n",
      "Train Epoch: 3537 [33024/118836 (28%)] Loss: 12246.816406\n",
      "Train Epoch: 3537 [65792/118836 (55%)] Loss: 12223.064453\n",
      "Train Epoch: 3537 [98560/118836 (83%)] Loss: 12209.961914\n",
      "    epoch          : 3537\n",
      "    loss           : 12215.935662382393\n",
      "    val_loss       : 12216.373331958632\n",
      "    val_log_likelihood: -12142.047127500775\n",
      "    val_log_marginal: -12149.954597632046\n",
      "Train Epoch: 3538 [256/118836 (0%)] Loss: 12379.035156\n",
      "Train Epoch: 3538 [33024/118836 (28%)] Loss: 12195.118164\n",
      "Train Epoch: 3538 [65792/118836 (55%)] Loss: 12252.064453\n",
      "Train Epoch: 3538 [98560/118836 (83%)] Loss: 12293.255859\n",
      "    epoch          : 3538\n",
      "    loss           : 12222.283242866004\n",
      "    val_loss       : 12218.05391284217\n",
      "    val_log_likelihood: -12143.383087617607\n",
      "    val_log_marginal: -12151.298595301796\n",
      "Train Epoch: 3539 [256/118836 (0%)] Loss: 12207.761719\n",
      "Train Epoch: 3539 [33024/118836 (28%)] Loss: 12222.617188\n",
      "Train Epoch: 3539 [65792/118836 (55%)] Loss: 12280.947266\n",
      "Train Epoch: 3539 [98560/118836 (83%)] Loss: 12279.802734\n",
      "    epoch          : 3539\n",
      "    loss           : 12222.412510016025\n",
      "    val_loss       : 12217.187224806052\n",
      "    val_log_likelihood: -12142.25135781767\n",
      "    val_log_marginal: -12150.176847323668\n",
      "Train Epoch: 3540 [256/118836 (0%)] Loss: 12179.064453\n",
      "Train Epoch: 3540 [33024/118836 (28%)] Loss: 12172.107422\n",
      "Train Epoch: 3540 [65792/118836 (55%)] Loss: 12269.035156\n",
      "Train Epoch: 3540 [98560/118836 (83%)] Loss: 12184.981445\n",
      "    epoch          : 3540\n",
      "    loss           : 12220.85932233509\n",
      "    val_loss       : 12218.372458449025\n",
      "    val_log_likelihood: -12142.377220165166\n",
      "    val_log_marginal: -12150.296988944261\n",
      "Train Epoch: 3541 [256/118836 (0%)] Loss: 12241.600586\n",
      "Train Epoch: 3541 [33024/118836 (28%)] Loss: 12278.109375\n",
      "Train Epoch: 3541 [65792/118836 (55%)] Loss: 12207.154297\n",
      "Train Epoch: 3541 [98560/118836 (83%)] Loss: 12198.437500\n",
      "    epoch          : 3541\n",
      "    loss           : 12215.698323608096\n",
      "    val_loss       : 12218.044489070398\n",
      "    val_log_likelihood: -12141.521187448305\n",
      "    val_log_marginal: -12149.438078004034\n",
      "Train Epoch: 3542 [256/118836 (0%)] Loss: 12320.061523\n",
      "Train Epoch: 3542 [33024/118836 (28%)] Loss: 12225.076172\n",
      "Train Epoch: 3542 [65792/118836 (55%)] Loss: 12288.768555\n",
      "Train Epoch: 3542 [98560/118836 (83%)] Loss: 12349.904297\n",
      "    epoch          : 3542\n",
      "    loss           : 12215.447574667854\n",
      "    val_loss       : 12216.867387572978\n",
      "    val_log_likelihood: -12141.891651481079\n",
      "    val_log_marginal: -12149.8000893404\n",
      "Train Epoch: 3543 [256/118836 (0%)] Loss: 12199.235352\n",
      "Train Epoch: 3543 [33024/118836 (28%)] Loss: 12255.855469\n",
      "Train Epoch: 3543 [65792/118836 (55%)] Loss: 12301.241211\n",
      "Train Epoch: 3543 [98560/118836 (83%)] Loss: 12216.218750\n",
      "    epoch          : 3543\n",
      "    loss           : 12219.227922094706\n",
      "    val_loss       : 12222.251818180544\n",
      "    val_log_likelihood: -12142.71531240307\n",
      "    val_log_marginal: -12150.618139022074\n",
      "Train Epoch: 3544 [256/118836 (0%)] Loss: 12265.940430\n",
      "Train Epoch: 3544 [33024/118836 (28%)] Loss: 12264.015625\n",
      "Train Epoch: 3544 [65792/118836 (55%)] Loss: 12304.944336\n",
      "Train Epoch: 3544 [98560/118836 (83%)] Loss: 12219.931641\n",
      "    epoch          : 3544\n",
      "    loss           : 12217.866782174058\n",
      "    val_loss       : 12214.634517717157\n",
      "    val_log_likelihood: -12141.091009647693\n",
      "    val_log_marginal: -12148.988511164613\n",
      "Train Epoch: 3545 [256/118836 (0%)] Loss: 12210.958984\n",
      "Train Epoch: 3545 [33024/118836 (28%)] Loss: 12238.230469\n",
      "Train Epoch: 3545 [65792/118836 (55%)] Loss: 12195.882812\n",
      "Train Epoch: 3545 [98560/118836 (83%)] Loss: 12215.643555\n",
      "    epoch          : 3545\n",
      "    loss           : 12219.00763204999\n",
      "    val_loss       : 12218.25219037724\n",
      "    val_log_likelihood: -12142.19020626551\n",
      "    val_log_marginal: -12150.100769075847\n",
      "Train Epoch: 3546 [256/118836 (0%)] Loss: 12234.754883\n",
      "Train Epoch: 3546 [33024/118836 (28%)] Loss: 12163.574219\n",
      "Train Epoch: 3546 [65792/118836 (55%)] Loss: 12161.851562\n",
      "Train Epoch: 3546 [98560/118836 (83%)] Loss: 12181.018555\n",
      "    epoch          : 3546\n",
      "    loss           : 12218.649089187862\n",
      "    val_loss       : 12219.592626098252\n",
      "    val_log_likelihood: -12142.154678453267\n",
      "    val_log_marginal: -12150.056872605817\n",
      "Train Epoch: 3547 [256/118836 (0%)] Loss: 12238.369141\n",
      "Train Epoch: 3547 [33024/118836 (28%)] Loss: 12216.781250\n",
      "Train Epoch: 3547 [65792/118836 (55%)] Loss: 12273.996094\n",
      "Train Epoch: 3547 [98560/118836 (83%)] Loss: 12335.123047\n",
      "    epoch          : 3547\n",
      "    loss           : 12217.906055495243\n",
      "    val_loss       : 12218.030485208044\n",
      "    val_log_likelihood: -12143.631288933262\n",
      "    val_log_marginal: -12151.534802140573\n",
      "Train Epoch: 3548 [256/118836 (0%)] Loss: 12255.721680\n",
      "Train Epoch: 3548 [33024/118836 (28%)] Loss: 12279.980469\n",
      "Train Epoch: 3548 [65792/118836 (55%)] Loss: 12272.503906\n",
      "Train Epoch: 3548 [98560/118836 (83%)] Loss: 12275.999023\n",
      "    epoch          : 3548\n",
      "    loss           : 12222.25264132289\n",
      "    val_loss       : 12220.979612701005\n",
      "    val_log_likelihood: -12141.811535069013\n",
      "    val_log_marginal: -12149.724201567193\n",
      "Train Epoch: 3549 [256/118836 (0%)] Loss: 12216.490234\n",
      "Train Epoch: 3549 [33024/118836 (28%)] Loss: 12244.061523\n",
      "Train Epoch: 3549 [65792/118836 (55%)] Loss: 12187.593750\n",
      "Train Epoch: 3549 [98560/118836 (83%)] Loss: 12237.515625\n",
      "    epoch          : 3549\n",
      "    loss           : 12215.917645006204\n",
      "    val_loss       : 12221.560496099852\n",
      "    val_log_likelihood: -12143.326343439825\n",
      "    val_log_marginal: -12151.237851160367\n",
      "Train Epoch: 3550 [256/118836 (0%)] Loss: 12307.159180\n",
      "Train Epoch: 3550 [33024/118836 (28%)] Loss: 12238.149414\n",
      "Train Epoch: 3550 [65792/118836 (55%)] Loss: 12288.275391\n",
      "Train Epoch: 3550 [98560/118836 (83%)] Loss: 12194.562500\n",
      "    epoch          : 3550\n",
      "    loss           : 12219.482358224772\n",
      "    val_loss       : 12218.701319639504\n",
      "    val_log_likelihood: -12143.414127765716\n",
      "    val_log_marginal: -12151.344399503882\n",
      "Train Epoch: 3551 [256/118836 (0%)] Loss: 12292.113281\n",
      "Train Epoch: 3551 [33024/118836 (28%)] Loss: 12204.625000\n",
      "Train Epoch: 3551 [65792/118836 (55%)] Loss: 12222.760742\n",
      "Train Epoch: 3551 [98560/118836 (83%)] Loss: 12203.003906\n",
      "    epoch          : 3551\n",
      "    loss           : 12217.599516645989\n",
      "    val_loss       : 12218.7723664573\n",
      "    val_log_likelihood: -12142.144497809399\n",
      "    val_log_marginal: -12150.063771317635\n",
      "Train Epoch: 3552 [256/118836 (0%)] Loss: 12178.619141\n",
      "Train Epoch: 3552 [33024/118836 (28%)] Loss: 12205.750000\n",
      "Train Epoch: 3552 [65792/118836 (55%)] Loss: 12208.847656\n",
      "Train Epoch: 3552 [98560/118836 (83%)] Loss: 12272.963867\n",
      "    epoch          : 3552\n",
      "    loss           : 12217.369009124275\n",
      "    val_loss       : 12221.544969060107\n",
      "    val_log_likelihood: -12142.186516006255\n",
      "    val_log_marginal: -12150.102047834851\n",
      "Train Epoch: 3553 [256/118836 (0%)] Loss: 12297.718750\n",
      "Train Epoch: 3553 [33024/118836 (28%)] Loss: 12426.140625\n",
      "Train Epoch: 3553 [65792/118836 (55%)] Loss: 12286.608398\n",
      "Train Epoch: 3553 [98560/118836 (83%)] Loss: 12219.677734\n",
      "    epoch          : 3553\n",
      "    loss           : 12220.686436847343\n",
      "    val_loss       : 12219.604702550612\n",
      "    val_log_likelihood: -12142.727369113163\n",
      "    val_log_marginal: -12150.638636050388\n",
      "Train Epoch: 3554 [256/118836 (0%)] Loss: 12243.825195\n",
      "Train Epoch: 3554 [33024/118836 (28%)] Loss: 12269.828125\n",
      "Train Epoch: 3554 [65792/118836 (55%)] Loss: 12245.863281\n",
      "Train Epoch: 3554 [98560/118836 (83%)] Loss: 12323.234375\n",
      "    epoch          : 3554\n",
      "    loss           : 12216.744207183105\n",
      "    val_loss       : 12218.698588394513\n",
      "    val_log_likelihood: -12141.810244455644\n",
      "    val_log_marginal: -12149.716293996258\n",
      "Train Epoch: 3555 [256/118836 (0%)] Loss: 12311.635742\n",
      "Train Epoch: 3555 [33024/118836 (28%)] Loss: 12335.501953\n",
      "Train Epoch: 3555 [65792/118836 (55%)] Loss: 12207.417969\n",
      "Train Epoch: 3555 [98560/118836 (83%)] Loss: 12209.580078\n",
      "    epoch          : 3555\n",
      "    loss           : 12217.679178944376\n",
      "    val_loss       : 12217.123383519836\n",
      "    val_log_likelihood: -12142.157836409222\n",
      "    val_log_marginal: -12150.058167613764\n",
      "Train Epoch: 3556 [256/118836 (0%)] Loss: 12285.598633\n",
      "Train Epoch: 3556 [33024/118836 (28%)] Loss: 12254.473633\n",
      "Train Epoch: 3556 [65792/118836 (55%)] Loss: 12184.526367\n",
      "Train Epoch: 3556 [98560/118836 (83%)] Loss: 12225.116211\n",
      "    epoch          : 3556\n",
      "    loss           : 12218.187875924059\n",
      "    val_loss       : 12218.470888821645\n",
      "    val_log_likelihood: -12143.600676566377\n",
      "    val_log_marginal: -12151.496522577207\n",
      "Train Epoch: 3557 [256/118836 (0%)] Loss: 12263.836914\n",
      "Train Epoch: 3557 [33024/118836 (28%)] Loss: 12265.400391\n",
      "Train Epoch: 3557 [65792/118836 (55%)] Loss: 12271.474609\n",
      "Train Epoch: 3557 [98560/118836 (83%)] Loss: 12189.281250\n",
      "    epoch          : 3557\n",
      "    loss           : 12216.081016077338\n",
      "    val_loss       : 12219.042494679088\n",
      "    val_log_likelihood: -12141.689670569685\n",
      "    val_log_marginal: -12149.599090847012\n",
      "Train Epoch: 3558 [256/118836 (0%)] Loss: 12286.384766\n",
      "Train Epoch: 3558 [33024/118836 (28%)] Loss: 12267.819336\n",
      "Train Epoch: 3558 [65792/118836 (55%)] Loss: 12188.992188\n",
      "Train Epoch: 3558 [98560/118836 (83%)] Loss: 12303.919922\n",
      "    epoch          : 3558\n",
      "    loss           : 12214.631939328732\n",
      "    val_loss       : 12220.502798958745\n",
      "    val_log_likelihood: -12142.220882121072\n",
      "    val_log_marginal: -12150.127431995808\n",
      "Train Epoch: 3559 [256/118836 (0%)] Loss: 12215.029297\n",
      "Train Epoch: 3559 [33024/118836 (28%)] Loss: 12192.846680\n",
      "Train Epoch: 3559 [65792/118836 (55%)] Loss: 12278.134766\n",
      "Train Epoch: 3559 [98560/118836 (83%)] Loss: 12189.455078\n",
      "    epoch          : 3559\n",
      "    loss           : 12219.358124935381\n",
      "    val_loss       : 12216.172125183684\n",
      "    val_log_likelihood: -12142.775728423543\n",
      "    val_log_marginal: -12150.685955091632\n",
      "Train Epoch: 3560 [256/118836 (0%)] Loss: 12230.289062\n",
      "Train Epoch: 3560 [33024/118836 (28%)] Loss: 12324.700195\n",
      "Train Epoch: 3560 [65792/118836 (55%)] Loss: 12313.158203\n",
      "Train Epoch: 3560 [98560/118836 (83%)] Loss: 12279.448242\n",
      "    epoch          : 3560\n",
      "    loss           : 12215.895628327904\n",
      "    val_loss       : 12218.548333540328\n",
      "    val_log_likelihood: -12141.222162233767\n",
      "    val_log_marginal: -12149.137040816908\n",
      "Train Epoch: 3561 [256/118836 (0%)] Loss: 12290.972656\n",
      "Train Epoch: 3561 [33024/118836 (28%)] Loss: 12294.702148\n",
      "Train Epoch: 3561 [65792/118836 (55%)] Loss: 12291.117188\n",
      "Train Epoch: 3561 [98560/118836 (83%)] Loss: 12231.743164\n",
      "    epoch          : 3561\n",
      "    loss           : 12217.819779712056\n",
      "    val_loss       : 12218.300033661735\n",
      "    val_log_likelihood: -12143.054539844396\n",
      "    val_log_marginal: -12150.966812728937\n",
      "Train Epoch: 3562 [256/118836 (0%)] Loss: 12391.854492\n",
      "Train Epoch: 3562 [33024/118836 (28%)] Loss: 12317.510742\n",
      "Train Epoch: 3562 [65792/118836 (55%)] Loss: 12240.978516\n",
      "Train Epoch: 3562 [98560/118836 (83%)] Loss: 12253.574219\n",
      "    epoch          : 3562\n",
      "    loss           : 12216.828335659637\n",
      "    val_loss       : 12220.736516014522\n",
      "    val_log_likelihood: -12141.628937752017\n",
      "    val_log_marginal: -12149.537652642855\n",
      "Train Epoch: 3563 [256/118836 (0%)] Loss: 12144.985352\n",
      "Train Epoch: 3563 [33024/118836 (28%)] Loss: 12250.525391\n",
      "Train Epoch: 3563 [65792/118836 (55%)] Loss: 12228.943359\n",
      "Train Epoch: 3563 [98560/118836 (83%)] Loss: 12195.062500\n",
      "    epoch          : 3563\n",
      "    loss           : 12219.651177852307\n",
      "    val_loss       : 12216.030702439873\n",
      "    val_log_likelihood: -12141.335710039288\n",
      "    val_log_marginal: -12149.227047405862\n",
      "Train Epoch: 3564 [256/118836 (0%)] Loss: 12299.208984\n",
      "Train Epoch: 3564 [33024/118836 (28%)] Loss: 12300.425781\n",
      "Train Epoch: 3564 [65792/118836 (55%)] Loss: 12358.457031\n",
      "Train Epoch: 3564 [98560/118836 (83%)] Loss: 12255.765625\n",
      "    epoch          : 3564\n",
      "    loss           : 12220.168835782413\n",
      "    val_loss       : 12219.409583294731\n",
      "    val_log_likelihood: -12142.08308584057\n",
      "    val_log_marginal: -12149.978611103952\n",
      "Train Epoch: 3565 [256/118836 (0%)] Loss: 12252.596680\n",
      "Train Epoch: 3565 [33024/118836 (28%)] Loss: 12118.817383\n",
      "Train Epoch: 3565 [65792/118836 (55%)] Loss: 12213.646484\n",
      "Train Epoch: 3565 [98560/118836 (83%)] Loss: 12165.796875\n",
      "    epoch          : 3565\n",
      "    loss           : 12218.825460898728\n",
      "    val_loss       : 12220.069455733665\n",
      "    val_log_likelihood: -12143.57919025279\n",
      "    val_log_marginal: -12151.466272931058\n",
      "Train Epoch: 3566 [256/118836 (0%)] Loss: 12355.278320\n",
      "Train Epoch: 3566 [33024/118836 (28%)] Loss: 12186.839844\n",
      "Train Epoch: 3566 [65792/118836 (55%)] Loss: 12190.126953\n",
      "Train Epoch: 3566 [98560/118836 (83%)] Loss: 12266.507812\n",
      "    epoch          : 3566\n",
      "    loss           : 12214.873245095378\n",
      "    val_loss       : 12217.027863725223\n",
      "    val_log_likelihood: -12142.122077582195\n",
      "    val_log_marginal: -12150.036607114476\n",
      "Train Epoch: 3567 [256/118836 (0%)] Loss: 12333.060547\n",
      "Train Epoch: 3567 [33024/118836 (28%)] Loss: 12211.002930\n",
      "Train Epoch: 3567 [65792/118836 (55%)] Loss: 12189.702148\n",
      "Train Epoch: 3567 [98560/118836 (83%)] Loss: 12189.000000\n",
      "    epoch          : 3567\n",
      "    loss           : 12212.598528290426\n",
      "    val_loss       : 12216.179146862038\n",
      "    val_log_likelihood: -12143.362934566274\n",
      "    val_log_marginal: -12151.27488620657\n",
      "Train Epoch: 3568 [256/118836 (0%)] Loss: 12224.882812\n",
      "Train Epoch: 3568 [33024/118836 (28%)] Loss: 12224.894531\n",
      "Train Epoch: 3568 [65792/118836 (55%)] Loss: 12247.150391\n",
      "Train Epoch: 3568 [98560/118836 (83%)] Loss: 12209.916016\n",
      "    epoch          : 3568\n",
      "    loss           : 12218.41817213994\n",
      "    val_loss       : 12218.025232882792\n",
      "    val_log_likelihood: -12143.790074764785\n",
      "    val_log_marginal: -12151.70108468938\n",
      "Train Epoch: 3569 [256/118836 (0%)] Loss: 12214.070312\n",
      "Train Epoch: 3569 [33024/118836 (28%)] Loss: 12186.300781\n",
      "Train Epoch: 3569 [65792/118836 (55%)] Loss: 12242.900391\n",
      "Train Epoch: 3569 [98560/118836 (83%)] Loss: 12169.704102\n",
      "    epoch          : 3569\n",
      "    loss           : 12222.409567404622\n",
      "    val_loss       : 12218.634972865622\n",
      "    val_log_likelihood: -12141.828840661186\n",
      "    val_log_marginal: -12149.73668251987\n",
      "Train Epoch: 3570 [256/118836 (0%)] Loss: 12160.452148\n",
      "Train Epoch: 3570 [33024/118836 (28%)] Loss: 12242.941406\n",
      "Train Epoch: 3570 [65792/118836 (55%)] Loss: 12242.650391\n",
      "Train Epoch: 3570 [98560/118836 (83%)] Loss: 12267.625000\n",
      "    epoch          : 3570\n",
      "    loss           : 12219.261025382548\n",
      "    val_loss       : 12220.062347841398\n",
      "    val_log_likelihood: -12142.249890146815\n",
      "    val_log_marginal: -12150.181260267058\n",
      "Train Epoch: 3571 [256/118836 (0%)] Loss: 12321.525391\n",
      "Train Epoch: 3571 [33024/118836 (28%)] Loss: 12188.310547\n",
      "Train Epoch: 3571 [65792/118836 (55%)] Loss: 12207.465820\n",
      "Train Epoch: 3571 [98560/118836 (83%)] Loss: 12251.970703\n",
      "    epoch          : 3571\n",
      "    loss           : 12219.189828564413\n",
      "    val_loss       : 12216.423380780907\n",
      "    val_log_likelihood: -12141.926533744314\n",
      "    val_log_marginal: -12149.847622531803\n",
      "Train Epoch: 3572 [256/118836 (0%)] Loss: 12258.613281\n",
      "Train Epoch: 3572 [33024/118836 (28%)] Loss: 12154.684570\n",
      "Train Epoch: 3572 [65792/118836 (55%)] Loss: 12221.291992\n",
      "Train Epoch: 3572 [98560/118836 (83%)] Loss: 12209.589844\n",
      "    epoch          : 3572\n",
      "    loss           : 12217.879674091451\n",
      "    val_loss       : 12214.305707675954\n",
      "    val_log_likelihood: -12142.72105077802\n",
      "    val_log_marginal: -12150.663681976263\n",
      "Train Epoch: 3573 [256/118836 (0%)] Loss: 12261.563477\n",
      "Train Epoch: 3573 [33024/118836 (28%)] Loss: 12238.455078\n",
      "Train Epoch: 3573 [65792/118836 (55%)] Loss: 12336.180664\n",
      "Train Epoch: 3573 [98560/118836 (83%)] Loss: 12282.469727\n",
      "    epoch          : 3573\n",
      "    loss           : 12221.61745680185\n",
      "    val_loss       : 12218.517874602141\n",
      "    val_log_likelihood: -12143.021048677885\n",
      "    val_log_marginal: -12150.94161235293\n",
      "Train Epoch: 3574 [256/118836 (0%)] Loss: 12244.072266\n",
      "Train Epoch: 3574 [33024/118836 (28%)] Loss: 12347.264648\n",
      "Train Epoch: 3574 [65792/118836 (55%)] Loss: 12257.222656\n",
      "Train Epoch: 3574 [98560/118836 (83%)] Loss: 12237.433594\n",
      "    epoch          : 3574\n",
      "    loss           : 12221.072995340932\n",
      "    val_loss       : 12218.117765840914\n",
      "    val_log_likelihood: -12144.298287744261\n",
      "    val_log_marginal: -12152.208373137975\n",
      "Train Epoch: 3575 [256/118836 (0%)] Loss: 12209.164062\n",
      "Train Epoch: 3575 [33024/118836 (28%)] Loss: 12309.212891\n",
      "Train Epoch: 3575 [65792/118836 (55%)] Loss: 12261.201172\n",
      "Train Epoch: 3575 [98560/118836 (83%)] Loss: 12183.631836\n",
      "    epoch          : 3575\n",
      "    loss           : 12218.543273431038\n",
      "    val_loss       : 12215.541043824245\n",
      "    val_log_likelihood: -12141.133885022487\n",
      "    val_log_marginal: -12149.036972957429\n",
      "Train Epoch: 3576 [256/118836 (0%)] Loss: 12221.722656\n",
      "Train Epoch: 3576 [33024/118836 (28%)] Loss: 12209.325195\n",
      "Train Epoch: 3576 [65792/118836 (55%)] Loss: 12316.960938\n",
      "Train Epoch: 3576 [98560/118836 (83%)] Loss: 12204.455078\n",
      "    epoch          : 3576\n",
      "    loss           : 12217.67175109207\n",
      "    val_loss       : 12219.514362440148\n",
      "    val_log_likelihood: -12142.493899109542\n",
      "    val_log_marginal: -12150.393624857361\n",
      "Train Epoch: 3577 [256/118836 (0%)] Loss: 12166.697266\n",
      "Train Epoch: 3577 [33024/118836 (28%)] Loss: 12139.155273\n",
      "Train Epoch: 3577 [65792/118836 (55%)] Loss: 12203.983398\n",
      "Train Epoch: 3577 [98560/118836 (83%)] Loss: 12183.308594\n",
      "    epoch          : 3577\n",
      "    loss           : 12219.216894935122\n",
      "    val_loss       : 12215.910448740684\n",
      "    val_log_likelihood: -12142.0435546875\n",
      "    val_log_marginal: -12149.963028353824\n",
      "Train Epoch: 3578 [256/118836 (0%)] Loss: 12177.289062\n",
      "Train Epoch: 3578 [33024/118836 (28%)] Loss: 12260.690430\n",
      "Train Epoch: 3578 [65792/118836 (55%)] Loss: 12349.662109\n",
      "Train Epoch: 3578 [98560/118836 (83%)] Loss: 12185.997070\n",
      "    epoch          : 3578\n",
      "    loss           : 12221.621298916976\n",
      "    val_loss       : 12219.824271453372\n",
      "    val_log_likelihood: -12142.376489803039\n",
      "    val_log_marginal: -12150.263377278985\n",
      "Train Epoch: 3579 [256/118836 (0%)] Loss: 12276.805664\n",
      "Train Epoch: 3579 [33024/118836 (28%)] Loss: 12178.433594\n",
      "Train Epoch: 3579 [65792/118836 (55%)] Loss: 12274.189453\n",
      "Train Epoch: 3579 [98560/118836 (83%)] Loss: 12254.978516\n",
      "    epoch          : 3579\n",
      "    loss           : 12217.15650750879\n",
      "    val_loss       : 12216.976132360689\n",
      "    val_log_likelihood: -12140.793694427213\n",
      "    val_log_marginal: -12148.681128007625\n",
      "Train Epoch: 3580 [256/118836 (0%)] Loss: 12273.624023\n",
      "Train Epoch: 3580 [33024/118836 (28%)] Loss: 12279.009766\n",
      "Train Epoch: 3580 [65792/118836 (55%)] Loss: 12345.620117\n",
      "Train Epoch: 3580 [98560/118836 (83%)] Loss: 12167.758789\n",
      "    epoch          : 3580\n",
      "    loss           : 12223.058855459058\n",
      "    val_loss       : 12218.24953367678\n",
      "    val_log_likelihood: -12140.884196811672\n",
      "    val_log_marginal: -12148.79342120637\n",
      "Train Epoch: 3581 [256/118836 (0%)] Loss: 12168.895508\n",
      "Train Epoch: 3581 [33024/118836 (28%)] Loss: 12266.031250\n",
      "Train Epoch: 3581 [65792/118836 (55%)] Loss: 12162.310547\n",
      "Train Epoch: 3581 [98560/118836 (83%)] Loss: 12219.439453\n",
      "    epoch          : 3581\n",
      "    loss           : 12215.813475108562\n",
      "    val_loss       : 12220.86270719507\n",
      "    val_log_likelihood: -12142.204788468001\n",
      "    val_log_marginal: -12150.12585516866\n",
      "Train Epoch: 3582 [256/118836 (0%)] Loss: 12287.990234\n",
      "Train Epoch: 3582 [33024/118836 (28%)] Loss: 12365.067383\n",
      "Train Epoch: 3582 [65792/118836 (55%)] Loss: 12291.449219\n",
      "Train Epoch: 3582 [98560/118836 (83%)] Loss: 12286.594727\n",
      "    epoch          : 3582\n",
      "    loss           : 12218.585769327698\n",
      "    val_loss       : 12221.991586236676\n",
      "    val_log_likelihood: -12142.419563236663\n",
      "    val_log_marginal: -12150.344356615902\n",
      "Train Epoch: 3583 [256/118836 (0%)] Loss: 12168.048828\n",
      "Train Epoch: 3583 [33024/118836 (28%)] Loss: 12304.976562\n",
      "Train Epoch: 3583 [65792/118836 (55%)] Loss: 12221.156250\n",
      "Train Epoch: 3583 [98560/118836 (83%)] Loss: 12238.235352\n",
      "    epoch          : 3583\n",
      "    loss           : 12214.84819226892\n",
      "    val_loss       : 12217.015890397533\n",
      "    val_log_likelihood: -12142.78992581679\n",
      "    val_log_marginal: -12150.694760806093\n",
      "Train Epoch: 3584 [256/118836 (0%)] Loss: 12319.977539\n",
      "Train Epoch: 3584 [33024/118836 (28%)] Loss: 12158.997070\n",
      "Train Epoch: 3584 [65792/118836 (55%)] Loss: 12299.583984\n",
      "Train Epoch: 3584 [98560/118836 (83%)] Loss: 12332.761719\n",
      "    epoch          : 3584\n",
      "    loss           : 12219.820038674783\n",
      "    val_loss       : 12215.163667192362\n",
      "    val_log_likelihood: -12142.05476585117\n",
      "    val_log_marginal: -12149.952911920964\n",
      "Train Epoch: 3585 [256/118836 (0%)] Loss: 12172.222656\n",
      "Train Epoch: 3585 [33024/118836 (28%)] Loss: 12171.541992\n",
      "Train Epoch: 3585 [65792/118836 (55%)] Loss: 12250.909180\n",
      "Train Epoch: 3585 [98560/118836 (83%)] Loss: 12238.839844\n",
      "    epoch          : 3585\n",
      "    loss           : 12219.104024988368\n",
      "    val_loss       : 12214.926700617429\n",
      "    val_log_likelihood: -12141.872841061828\n",
      "    val_log_marginal: -12149.770967431365\n",
      "Train Epoch: 3586 [256/118836 (0%)] Loss: 12220.115234\n",
      "Train Epoch: 3586 [33024/118836 (28%)] Loss: 12289.077148\n",
      "Train Epoch: 3586 [65792/118836 (55%)] Loss: 12163.505859\n",
      "Train Epoch: 3586 [98560/118836 (83%)] Loss: 12259.839844\n",
      "    epoch          : 3586\n",
      "    loss           : 12215.17859623785\n",
      "    val_loss       : 12217.45225307555\n",
      "    val_log_likelihood: -12139.969749340882\n",
      "    val_log_marginal: -12147.872497203542\n",
      "Train Epoch: 3587 [256/118836 (0%)] Loss: 12263.687500\n",
      "Train Epoch: 3587 [33024/118836 (28%)] Loss: 12304.367188\n",
      "Train Epoch: 3587 [65792/118836 (55%)] Loss: 12141.013672\n",
      "Train Epoch: 3587 [98560/118836 (83%)] Loss: 12114.705078\n",
      "    epoch          : 3587\n",
      "    loss           : 12217.60306054203\n",
      "    val_loss       : 12218.98827510501\n",
      "    val_log_likelihood: -12141.044951599979\n",
      "    val_log_marginal: -12148.953099758693\n",
      "Train Epoch: 3588 [256/118836 (0%)] Loss: 12242.287109\n",
      "Train Epoch: 3588 [33024/118836 (28%)] Loss: 12206.949219\n",
      "Train Epoch: 3588 [65792/118836 (55%)] Loss: 12235.683594\n",
      "Train Epoch: 3588 [98560/118836 (83%)] Loss: 12167.111328\n",
      "    epoch          : 3588\n",
      "    loss           : 12219.692090732267\n",
      "    val_loss       : 12222.108147617662\n",
      "    val_log_likelihood: -12140.89360024426\n",
      "    val_log_marginal: -12148.799052565479\n",
      "Train Epoch: 3589 [256/118836 (0%)] Loss: 12234.592773\n",
      "Train Epoch: 3589 [33024/118836 (28%)] Loss: 12171.279297\n",
      "Train Epoch: 3589 [65792/118836 (55%)] Loss: 12250.373047\n",
      "Train Epoch: 3589 [98560/118836 (83%)] Loss: 12324.573242\n",
      "    epoch          : 3589\n",
      "    loss           : 12217.786393875362\n",
      "    val_loss       : 12218.661436286386\n",
      "    val_log_likelihood: -12142.706627539548\n",
      "    val_log_marginal: -12150.607589875577\n",
      "Train Epoch: 3590 [256/118836 (0%)] Loss: 12289.836914\n",
      "Train Epoch: 3590 [33024/118836 (28%)] Loss: 12220.111328\n",
      "Train Epoch: 3590 [65792/118836 (55%)] Loss: 12224.368164\n",
      "Train Epoch: 3590 [98560/118836 (83%)] Loss: 12344.818359\n",
      "    epoch          : 3590\n",
      "    loss           : 12216.826542467948\n",
      "    val_loss       : 12217.9940113024\n",
      "    val_log_likelihood: -12141.613357016387\n",
      "    val_log_marginal: -12149.513487446757\n",
      "Train Epoch: 3591 [256/118836 (0%)] Loss: 12362.886719\n",
      "Train Epoch: 3591 [33024/118836 (28%)] Loss: 12231.341797\n",
      "Train Epoch: 3591 [65792/118836 (55%)] Loss: 12114.853516\n",
      "Train Epoch: 3591 [98560/118836 (83%)] Loss: 12290.985352\n",
      "    epoch          : 3591\n",
      "    loss           : 12218.978541957455\n",
      "    val_loss       : 12215.779874431675\n",
      "    val_log_likelihood: -12142.13798351556\n",
      "    val_log_marginal: -12150.059026187011\n",
      "Train Epoch: 3592 [256/118836 (0%)] Loss: 12146.289062\n",
      "Train Epoch: 3592 [33024/118836 (28%)] Loss: 12275.705078\n",
      "Train Epoch: 3592 [65792/118836 (55%)] Loss: 12288.712891\n",
      "Train Epoch: 3592 [98560/118836 (83%)] Loss: 12119.080078\n",
      "    epoch          : 3592\n",
      "    loss           : 12219.359466921267\n",
      "    val_loss       : 12217.589735656642\n",
      "    val_log_likelihood: -12141.059998093724\n",
      "    val_log_marginal: -12148.968321980374\n",
      "Train Epoch: 3593 [256/118836 (0%)] Loss: 12268.447266\n",
      "Train Epoch: 3593 [33024/118836 (28%)] Loss: 12288.725586\n",
      "Train Epoch: 3593 [65792/118836 (55%)] Loss: 12342.375977\n",
      "Train Epoch: 3593 [98560/118836 (83%)] Loss: 12236.901367\n",
      "    epoch          : 3593\n",
      "    loss           : 12218.87265075734\n",
      "    val_loss       : 12219.740959400204\n",
      "    val_log_likelihood: -12142.588554267473\n",
      "    val_log_marginal: -12150.48259827743\n",
      "Train Epoch: 3594 [256/118836 (0%)] Loss: 12221.672852\n",
      "Train Epoch: 3594 [33024/118836 (28%)] Loss: 12185.731445\n",
      "Train Epoch: 3594 [65792/118836 (55%)] Loss: 12282.408203\n",
      "Train Epoch: 3594 [98560/118836 (83%)] Loss: 12191.009766\n",
      "    epoch          : 3594\n",
      "    loss           : 12219.811666892836\n",
      "    val_loss       : 12219.277869851818\n",
      "    val_log_likelihood: -12142.383071947374\n",
      "    val_log_marginal: -12150.282568913324\n",
      "Train Epoch: 3595 [256/118836 (0%)] Loss: 12160.197266\n",
      "Train Epoch: 3595 [33024/118836 (28%)] Loss: 12199.316406\n",
      "Train Epoch: 3595 [65792/118836 (55%)] Loss: 12373.286133\n",
      "Train Epoch: 3595 [98560/118836 (83%)] Loss: 12279.090820\n",
      "    epoch          : 3595\n",
      "    loss           : 12222.324008413463\n",
      "    val_loss       : 12219.777322040283\n",
      "    val_log_likelihood: -12141.5207838994\n",
      "    val_log_marginal: -12149.402570574402\n",
      "Train Epoch: 3596 [256/118836 (0%)] Loss: 12313.291016\n",
      "Train Epoch: 3596 [33024/118836 (28%)] Loss: 12298.632812\n",
      "Train Epoch: 3596 [65792/118836 (55%)] Loss: 12274.244141\n",
      "Train Epoch: 3596 [98560/118836 (83%)] Loss: 12194.544922\n",
      "    epoch          : 3596\n",
      "    loss           : 12215.538778820306\n",
      "    val_loss       : 12216.220152474472\n",
      "    val_log_likelihood: -12142.148878528225\n",
      "    val_log_marginal: -12150.05235403425\n",
      "Train Epoch: 3597 [256/118836 (0%)] Loss: 12140.929688\n",
      "Train Epoch: 3597 [33024/118836 (28%)] Loss: 12291.080078\n",
      "Train Epoch: 3597 [65792/118836 (55%)] Loss: 12298.627930\n",
      "Train Epoch: 3597 [98560/118836 (83%)] Loss: 12152.860352\n",
      "    epoch          : 3597\n",
      "    loss           : 12216.569351607734\n",
      "    val_loss       : 12219.900140050206\n",
      "    val_log_likelihood: -12140.211079178298\n",
      "    val_log_marginal: -12148.122796166994\n",
      "Train Epoch: 3598 [256/118836 (0%)] Loss: 12263.171875\n",
      "Train Epoch: 3598 [33024/118836 (28%)] Loss: 12293.609375\n",
      "Train Epoch: 3598 [65792/118836 (55%)] Loss: 12243.527344\n",
      "Train Epoch: 3598 [98560/118836 (83%)] Loss: 12375.119141\n",
      "    epoch          : 3598\n",
      "    loss           : 12220.180922540581\n",
      "    val_loss       : 12215.81719093373\n",
      "    val_log_likelihood: -12142.107296351582\n",
      "    val_log_marginal: -12150.007469243872\n",
      "Train Epoch: 3599 [256/118836 (0%)] Loss: 12268.826172\n",
      "Train Epoch: 3599 [33024/118836 (28%)] Loss: 12411.687500\n",
      "Train Epoch: 3599 [65792/118836 (55%)] Loss: 12181.247070\n",
      "Train Epoch: 3599 [98560/118836 (83%)] Loss: 12160.077148\n",
      "    epoch          : 3599\n",
      "    loss           : 12219.48535689361\n",
      "    val_loss       : 12218.832957303215\n",
      "    val_log_likelihood: -12142.019061627636\n",
      "    val_log_marginal: -12149.92315881035\n",
      "Train Epoch: 3600 [256/118836 (0%)] Loss: 12289.771484\n",
      "Train Epoch: 3600 [33024/118836 (28%)] Loss: 12323.194336\n",
      "Train Epoch: 3600 [65792/118836 (55%)] Loss: 12188.271484\n",
      "Train Epoch: 3600 [98560/118836 (83%)] Loss: 12238.102539\n",
      "    epoch          : 3600\n",
      "    loss           : 12215.597763195305\n",
      "    val_loss       : 12216.972554270296\n",
      "    val_log_likelihood: -12143.700852331473\n",
      "    val_log_marginal: -12151.59755992761\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch3600.pth ...\n",
      "Train Epoch: 3601 [256/118836 (0%)] Loss: 12148.488281\n",
      "Train Epoch: 3601 [33024/118836 (28%)] Loss: 12307.193359\n",
      "Train Epoch: 3601 [65792/118836 (55%)] Loss: 12300.638672\n",
      "Train Epoch: 3601 [98560/118836 (83%)] Loss: 12173.306641\n",
      "    epoch          : 3601\n",
      "    loss           : 12217.211913416304\n",
      "    val_loss       : 12215.575072055215\n",
      "    val_log_likelihood: -12142.520520897953\n",
      "    val_log_marginal: -12150.42976553575\n",
      "Train Epoch: 3602 [256/118836 (0%)] Loss: 12232.867188\n",
      "Train Epoch: 3602 [33024/118836 (28%)] Loss: 12296.385742\n",
      "Train Epoch: 3602 [65792/118836 (55%)] Loss: 12271.805664\n",
      "Train Epoch: 3602 [98560/118836 (83%)] Loss: 12179.578125\n",
      "    epoch          : 3602\n",
      "    loss           : 12216.978005615436\n",
      "    val_loss       : 12218.25475192895\n",
      "    val_log_likelihood: -12142.755405907517\n",
      "    val_log_marginal: -12150.647794962388\n",
      "Train Epoch: 3603 [256/118836 (0%)] Loss: 12185.580078\n",
      "Train Epoch: 3603 [33024/118836 (28%)] Loss: 12157.522461\n",
      "Train Epoch: 3603 [65792/118836 (55%)] Loss: 12332.084961\n",
      "Train Epoch: 3603 [98560/118836 (83%)] Loss: 12217.041992\n",
      "    epoch          : 3603\n",
      "    loss           : 12218.185393403639\n",
      "    val_loss       : 12218.951725087889\n",
      "    val_log_likelihood: -12141.615085427007\n",
      "    val_log_marginal: -12149.519382528419\n",
      "Train Epoch: 3604 [256/118836 (0%)] Loss: 12276.078125\n",
      "Train Epoch: 3604 [33024/118836 (28%)] Loss: 12301.201172\n",
      "Train Epoch: 3604 [65792/118836 (55%)] Loss: 12167.361328\n",
      "Train Epoch: 3604 [98560/118836 (83%)] Loss: 12338.633789\n",
      "    epoch          : 3604\n",
      "    loss           : 12223.349320202648\n",
      "    val_loss       : 12216.034891930052\n",
      "    val_log_likelihood: -12141.598212624069\n",
      "    val_log_marginal: -12149.49209532261\n",
      "Train Epoch: 3605 [256/118836 (0%)] Loss: 12257.341797\n",
      "Train Epoch: 3605 [33024/118836 (28%)] Loss: 12185.258789\n",
      "Train Epoch: 3605 [65792/118836 (55%)] Loss: 12168.500000\n",
      "Train Epoch: 3605 [98560/118836 (83%)] Loss: 12178.102539\n",
      "    epoch          : 3605\n",
      "    loss           : 12213.065519670183\n",
      "    val_loss       : 12218.937108394397\n",
      "    val_log_likelihood: -12141.526682046111\n",
      "    val_log_marginal: -12149.420793130295\n",
      "Train Epoch: 3606 [256/118836 (0%)] Loss: 12216.533203\n",
      "Train Epoch: 3606 [33024/118836 (28%)] Loss: 12251.675781\n",
      "Train Epoch: 3606 [65792/118836 (55%)] Loss: 12264.568359\n",
      "Train Epoch: 3606 [98560/118836 (83%)] Loss: 12252.564453\n",
      "    epoch          : 3606\n",
      "    loss           : 12214.815960213762\n",
      "    val_loss       : 12218.039986313546\n",
      "    val_log_likelihood: -12141.276443600082\n",
      "    val_log_marginal: -12149.187371433\n",
      "Train Epoch: 3607 [256/118836 (0%)] Loss: 12224.277344\n",
      "Train Epoch: 3607 [33024/118836 (28%)] Loss: 12232.573242\n",
      "Train Epoch: 3607 [65792/118836 (55%)] Loss: 12252.052734\n",
      "Train Epoch: 3607 [98560/118836 (83%)] Loss: 12180.453125\n",
      "    epoch          : 3607\n",
      "    loss           : 12219.402070732527\n",
      "    val_loss       : 12220.798517424828\n",
      "    val_log_likelihood: -12142.805819310897\n",
      "    val_log_marginal: -12150.726311794753\n",
      "Train Epoch: 3608 [256/118836 (0%)] Loss: 12280.854492\n",
      "Train Epoch: 3608 [33024/118836 (28%)] Loss: 12282.166992\n",
      "Train Epoch: 3608 [65792/118836 (55%)] Loss: 12251.779297\n",
      "Train Epoch: 3608 [98560/118836 (83%)] Loss: 12269.243164\n",
      "    epoch          : 3608\n",
      "    loss           : 12217.151270419768\n",
      "    val_loss       : 12217.260932601455\n",
      "    val_log_likelihood: -12139.90301030035\n",
      "    val_log_marginal: -12147.83319056446\n",
      "Train Epoch: 3609 [256/118836 (0%)] Loss: 12203.850586\n",
      "Train Epoch: 3609 [33024/118836 (28%)] Loss: 12153.385742\n",
      "Train Epoch: 3609 [65792/118836 (55%)] Loss: 12278.733398\n",
      "Train Epoch: 3609 [98560/118836 (83%)] Loss: 12294.618164\n",
      "    epoch          : 3609\n",
      "    loss           : 12216.95445454663\n",
      "    val_loss       : 12217.015846613333\n",
      "    val_log_likelihood: -12140.112858638338\n",
      "    val_log_marginal: -12148.035603859751\n",
      "Train Epoch: 3610 [256/118836 (0%)] Loss: 12334.458984\n",
      "Train Epoch: 3610 [33024/118836 (28%)] Loss: 12191.787109\n",
      "Train Epoch: 3610 [65792/118836 (55%)] Loss: 12200.563477\n",
      "Train Epoch: 3610 [98560/118836 (83%)] Loss: 12285.341797\n",
      "    epoch          : 3610\n",
      "    loss           : 12216.755898308262\n",
      "    val_loss       : 12218.991453607296\n",
      "    val_log_likelihood: -12139.910726194168\n",
      "    val_log_marginal: -12147.838806557114\n",
      "Train Epoch: 3611 [256/118836 (0%)] Loss: 12194.542969\n",
      "Train Epoch: 3611 [33024/118836 (28%)] Loss: 12373.115234\n",
      "Train Epoch: 3611 [65792/118836 (55%)] Loss: 12361.826172\n",
      "Train Epoch: 3611 [98560/118836 (83%)] Loss: 12252.081055\n",
      "    epoch          : 3611\n",
      "    loss           : 12220.478976523727\n",
      "    val_loss       : 12214.881017543745\n",
      "    val_log_likelihood: -12140.228404964071\n",
      "    val_log_marginal: -12148.147354214278\n",
      "Train Epoch: 3612 [256/118836 (0%)] Loss: 12186.232422\n",
      "Train Epoch: 3612 [33024/118836 (28%)] Loss: 12222.494141\n",
      "Train Epoch: 3612 [65792/118836 (55%)] Loss: 12285.978516\n",
      "Train Epoch: 3612 [98560/118836 (83%)] Loss: 12146.859375\n",
      "    epoch          : 3612\n",
      "    loss           : 12220.031638524866\n",
      "    val_loss       : 12219.691253763907\n",
      "    val_log_likelihood: -12141.089793508323\n",
      "    val_log_marginal: -12149.011285381612\n",
      "Train Epoch: 3613 [256/118836 (0%)] Loss: 12244.055664\n",
      "Train Epoch: 3613 [33024/118836 (28%)] Loss: 12179.037109\n",
      "Train Epoch: 3613 [65792/118836 (55%)] Loss: 12170.333008\n",
      "Train Epoch: 3613 [98560/118836 (83%)] Loss: 12269.178711\n",
      "    epoch          : 3613\n",
      "    loss           : 12220.156740623708\n",
      "    val_loss       : 12219.71701386448\n",
      "    val_log_likelihood: -12139.592789107734\n",
      "    val_log_marginal: -12147.529776392585\n",
      "Train Epoch: 3614 [256/118836 (0%)] Loss: 12206.437500\n",
      "Train Epoch: 3614 [33024/118836 (28%)] Loss: 12178.116211\n",
      "Train Epoch: 3614 [65792/118836 (55%)] Loss: 12265.739258\n",
      "Train Epoch: 3614 [98560/118836 (83%)] Loss: 12255.283203\n",
      "    epoch          : 3614\n",
      "    loss           : 12215.092197516025\n",
      "    val_loss       : 12215.02713213978\n",
      "    val_log_likelihood: -12138.519237392733\n",
      "    val_log_marginal: -12146.437516892383\n",
      "Train Epoch: 3615 [256/118836 (0%)] Loss: 12174.259766\n",
      "Train Epoch: 3615 [33024/118836 (28%)] Loss: 12319.602539\n",
      "Train Epoch: 3615 [65792/118836 (55%)] Loss: 12290.242188\n",
      "Train Epoch: 3615 [98560/118836 (83%)] Loss: 12171.097656\n",
      "    epoch          : 3615\n",
      "    loss           : 12221.151993027554\n",
      "    val_loss       : 12216.613023724665\n",
      "    val_log_likelihood: -12140.28204336616\n",
      "    val_log_marginal: -12148.201325516096\n",
      "Train Epoch: 3616 [256/118836 (0%)] Loss: 12304.435547\n",
      "Train Epoch: 3616 [33024/118836 (28%)] Loss: 12165.795898\n",
      "Train Epoch: 3616 [65792/118836 (55%)] Loss: 12208.061523\n",
      "Train Epoch: 3616 [98560/118836 (83%)] Loss: 12190.816406\n",
      "    epoch          : 3616\n",
      "    loss           : 12219.607073575786\n",
      "    val_loss       : 12214.85474173813\n",
      "    val_log_likelihood: -12140.010242193961\n",
      "    val_log_marginal: -12147.940985061648\n",
      "Train Epoch: 3617 [256/118836 (0%)] Loss: 12285.130859\n",
      "Train Epoch: 3617 [33024/118836 (28%)] Loss: 12268.634766\n",
      "Train Epoch: 3617 [65792/118836 (55%)] Loss: 12259.289062\n",
      "Train Epoch: 3617 [98560/118836 (83%)] Loss: 12233.500977\n",
      "    epoch          : 3617\n",
      "    loss           : 12219.391594454353\n",
      "    val_loss       : 12217.577136165735\n",
      "    val_log_likelihood: -12139.051920169046\n",
      "    val_log_marginal: -12146.959753792744\n",
      "Train Epoch: 3618 [256/118836 (0%)] Loss: 12164.542969\n",
      "Train Epoch: 3618 [33024/118836 (28%)] Loss: 12248.589844\n",
      "Train Epoch: 3618 [65792/118836 (55%)] Loss: 12177.755859\n",
      "Train Epoch: 3618 [98560/118836 (83%)] Loss: 12167.545898\n",
      "    epoch          : 3618\n",
      "    loss           : 12219.093922695667\n",
      "    val_loss       : 12218.869194024584\n",
      "    val_log_likelihood: -12139.600799181917\n",
      "    val_log_marginal: -12147.487791581329\n",
      "Train Epoch: 3619 [256/118836 (0%)] Loss: 12194.805664\n",
      "Train Epoch: 3619 [33024/118836 (28%)] Loss: 12205.105469\n",
      "Train Epoch: 3619 [65792/118836 (55%)] Loss: 12191.316406\n",
      "Train Epoch: 3619 [98560/118836 (83%)] Loss: 12292.656250\n",
      "    epoch          : 3619\n",
      "    loss           : 12217.106773256564\n",
      "    val_loss       : 12220.740328947586\n",
      "    val_log_likelihood: -12139.510863995296\n",
      "    val_log_marginal: -12147.394372630886\n",
      "Train Epoch: 3620 [256/118836 (0%)] Loss: 12319.876953\n",
      "Train Epoch: 3620 [33024/118836 (28%)] Loss: 12188.601562\n",
      "Train Epoch: 3620 [65792/118836 (55%)] Loss: 12329.115234\n",
      "Train Epoch: 3620 [98560/118836 (83%)] Loss: 12207.010742\n",
      "    epoch          : 3620\n",
      "    loss           : 12220.632189406277\n",
      "    val_loss       : 12214.509285110105\n",
      "    val_log_likelihood: -12139.827599481752\n",
      "    val_log_marginal: -12147.741627042818\n",
      "Train Epoch: 3621 [256/118836 (0%)] Loss: 12179.463867\n",
      "Train Epoch: 3621 [33024/118836 (28%)] Loss: 12253.826172\n",
      "Train Epoch: 3621 [65792/118836 (55%)] Loss: 12252.557617\n",
      "Train Epoch: 3621 [98560/118836 (83%)] Loss: 12178.947266\n",
      "    epoch          : 3621\n",
      "    loss           : 12221.21928585737\n",
      "    val_loss       : 12214.97252286797\n",
      "    val_log_likelihood: -12140.120183907155\n",
      "    val_log_marginal: -12148.037572782947\n",
      "Train Epoch: 3622 [256/118836 (0%)] Loss: 12204.383789\n",
      "Train Epoch: 3622 [33024/118836 (28%)] Loss: 12246.083984\n",
      "Train Epoch: 3622 [65792/118836 (55%)] Loss: 12168.539062\n",
      "Train Epoch: 3622 [98560/118836 (83%)] Loss: 12145.264648\n",
      "    epoch          : 3622\n",
      "    loss           : 12217.407047727978\n",
      "    val_loss       : 12219.845771060385\n",
      "    val_log_likelihood: -12140.359522655604\n",
      "    val_log_marginal: -12148.276182820782\n",
      "Train Epoch: 3623 [256/118836 (0%)] Loss: 12241.161133\n",
      "Train Epoch: 3623 [33024/118836 (28%)] Loss: 12169.527344\n",
      "Train Epoch: 3623 [65792/118836 (55%)] Loss: 12279.196289\n",
      "Train Epoch: 3623 [98560/118836 (83%)] Loss: 12266.560547\n",
      "    epoch          : 3623\n",
      "    loss           : 12215.594195066946\n",
      "    val_loss       : 12215.91737395291\n",
      "    val_log_likelihood: -12139.385875465261\n",
      "    val_log_marginal: -12147.29138118005\n",
      "Train Epoch: 3624 [256/118836 (0%)] Loss: 12323.900391\n",
      "Train Epoch: 3624 [33024/118836 (28%)] Loss: 12161.629883\n",
      "Train Epoch: 3624 [65792/118836 (55%)] Loss: 12268.259766\n",
      "Train Epoch: 3624 [98560/118836 (83%)] Loss: 12188.911133\n",
      "    epoch          : 3624\n",
      "    loss           : 12215.597351568962\n",
      "    val_loss       : 12216.808553400744\n",
      "    val_log_likelihood: -12139.619299427472\n",
      "    val_log_marginal: -12147.522291030697\n",
      "Train Epoch: 3625 [256/118836 (0%)] Loss: 12119.510742\n",
      "Train Epoch: 3625 [33024/118836 (28%)] Loss: 12326.454102\n",
      "Train Epoch: 3625 [65792/118836 (55%)] Loss: 12188.817383\n",
      "Train Epoch: 3625 [98560/118836 (83%)] Loss: 12168.675781\n",
      "    epoch          : 3625\n",
      "    loss           : 12223.386417299938\n",
      "    val_loss       : 12221.164074350452\n",
      "    val_log_likelihood: -12140.792746135752\n",
      "    val_log_marginal: -12148.69895046753\n",
      "Train Epoch: 3626 [256/118836 (0%)] Loss: 12234.358398\n",
      "Train Epoch: 3626 [33024/118836 (28%)] Loss: 12202.738281\n",
      "Train Epoch: 3626 [65792/118836 (55%)] Loss: 12227.929688\n",
      "Train Epoch: 3626 [98560/118836 (83%)] Loss: 12203.175781\n",
      "    epoch          : 3626\n",
      "    loss           : 12218.37678640664\n",
      "    val_loss       : 12217.767054005224\n",
      "    val_log_likelihood: -12138.68107358871\n",
      "    val_log_marginal: -12146.577303588343\n",
      "Train Epoch: 3627 [256/118836 (0%)] Loss: 12230.526367\n",
      "Train Epoch: 3627 [33024/118836 (28%)] Loss: 12233.773438\n",
      "Train Epoch: 3627 [65792/118836 (55%)] Loss: 12194.111328\n",
      "Train Epoch: 3627 [98560/118836 (83%)] Loss: 12456.908203\n",
      "    epoch          : 3627\n",
      "    loss           : 12221.039548600342\n",
      "    val_loss       : 12220.645804296995\n",
      "    val_log_likelihood: -12138.880243712521\n",
      "    val_log_marginal: -12146.77521700817\n",
      "Train Epoch: 3628 [256/118836 (0%)] Loss: 12361.755859\n",
      "Train Epoch: 3628 [33024/118836 (28%)] Loss: 12204.142578\n",
      "Train Epoch: 3628 [65792/118836 (55%)] Loss: 12313.035156\n",
      "Train Epoch: 3628 [98560/118836 (83%)] Loss: 12295.609375\n",
      "    epoch          : 3628\n",
      "    loss           : 12219.435061420854\n",
      "    val_loss       : 12217.074188969613\n",
      "    val_log_likelihood: -12140.456851930832\n",
      "    val_log_marginal: -12148.352953173135\n",
      "Train Epoch: 3629 [256/118836 (0%)] Loss: 12328.755859\n",
      "Train Epoch: 3629 [33024/118836 (28%)] Loss: 12280.040039\n",
      "Train Epoch: 3629 [65792/118836 (55%)] Loss: 12399.230469\n",
      "Train Epoch: 3629 [98560/118836 (83%)] Loss: 12201.723633\n",
      "    epoch          : 3629\n",
      "    loss           : 12219.044830599927\n",
      "    val_loss       : 12219.990754673956\n",
      "    val_log_likelihood: -12139.359888725186\n",
      "    val_log_marginal: -12147.259711651106\n",
      "Train Epoch: 3630 [256/118836 (0%)] Loss: 12225.776367\n",
      "Train Epoch: 3630 [33024/118836 (28%)] Loss: 12230.569336\n",
      "Train Epoch: 3630 [65792/118836 (55%)] Loss: 12273.922852\n",
      "Train Epoch: 3630 [98560/118836 (83%)] Loss: 12165.365234\n",
      "    epoch          : 3630\n",
      "    loss           : 12219.671959328472\n",
      "    val_loss       : 12219.769000532207\n",
      "    val_log_likelihood: -12140.533985505843\n",
      "    val_log_marginal: -12148.448950589143\n",
      "Train Epoch: 3631 [256/118836 (0%)] Loss: 12245.941406\n",
      "Train Epoch: 3631 [33024/118836 (28%)] Loss: 12203.546875\n",
      "Train Epoch: 3631 [65792/118836 (55%)] Loss: 12252.871094\n",
      "Train Epoch: 3631 [98560/118836 (83%)] Loss: 12165.292969\n",
      "    epoch          : 3631\n",
      "    loss           : 12214.745806516232\n",
      "    val_loss       : 12217.785519586423\n",
      "    val_log_likelihood: -12139.508948834264\n",
      "    val_log_marginal: -12147.424585962994\n",
      "Train Epoch: 3632 [256/118836 (0%)] Loss: 12180.412109\n",
      "Train Epoch: 3632 [33024/118836 (28%)] Loss: 12237.462891\n",
      "Train Epoch: 3632 [65792/118836 (55%)] Loss: 12243.269531\n",
      "Train Epoch: 3632 [98560/118836 (83%)] Loss: 12187.626953\n",
      "    epoch          : 3632\n",
      "    loss           : 12217.067150182227\n",
      "    val_loss       : 12218.521704127283\n",
      "    val_log_likelihood: -12138.763824700165\n",
      "    val_log_marginal: -12146.668661039803\n",
      "Train Epoch: 3633 [256/118836 (0%)] Loss: 12173.183594\n",
      "Train Epoch: 3633 [33024/118836 (28%)] Loss: 12163.916992\n",
      "Train Epoch: 3633 [65792/118836 (55%)] Loss: 12157.032227\n",
      "Train Epoch: 3633 [98560/118836 (83%)] Loss: 12227.013672\n",
      "    epoch          : 3633\n",
      "    loss           : 12217.225347976117\n",
      "    val_loss       : 12218.879040558155\n",
      "    val_log_likelihood: -12139.554314968467\n",
      "    val_log_marginal: -12147.471429940084\n",
      "Train Epoch: 3634 [256/118836 (0%)] Loss: 12470.680664\n",
      "Train Epoch: 3634 [33024/118836 (28%)] Loss: 12261.773438\n",
      "Train Epoch: 3634 [65792/118836 (55%)] Loss: 12259.099609\n",
      "Train Epoch: 3634 [98560/118836 (83%)] Loss: 12202.561523\n",
      "    epoch          : 3634\n",
      "    loss           : 12221.103710129757\n",
      "    val_loss       : 12218.19688050608\n",
      "    val_log_likelihood: -12140.200342968104\n",
      "    val_log_marginal: -12148.120020577846\n",
      "Train Epoch: 3635 [256/118836 (0%)] Loss: 12213.869141\n",
      "Train Epoch: 3635 [33024/118836 (28%)] Loss: 12240.150391\n",
      "Train Epoch: 3635 [65792/118836 (55%)] Loss: 12227.579102\n",
      "Train Epoch: 3635 [98560/118836 (83%)] Loss: 12201.786133\n",
      "    epoch          : 3635\n",
      "    loss           : 12219.42460921345\n",
      "    val_loss       : 12212.313694497176\n",
      "    val_log_likelihood: -12140.312846360628\n",
      "    val_log_marginal: -12148.23363376015\n",
      "Train Epoch: 3636 [256/118836 (0%)] Loss: 12207.169922\n",
      "Train Epoch: 3636 [33024/118836 (28%)] Loss: 12340.377930\n",
      "Train Epoch: 3636 [65792/118836 (55%)] Loss: 12338.511719\n",
      "Train Epoch: 3636 [98560/118836 (83%)] Loss: 12173.904297\n",
      "    epoch          : 3636\n",
      "    loss           : 12215.911648314723\n",
      "    val_loss       : 12218.276506373963\n",
      "    val_log_likelihood: -12139.86623226194\n",
      "    val_log_marginal: -12147.794632932286\n",
      "Train Epoch: 3637 [256/118836 (0%)] Loss: 12304.856445\n",
      "Train Epoch: 3637 [33024/118836 (28%)] Loss: 12176.944336\n",
      "Train Epoch: 3637 [65792/118836 (55%)] Loss: 12204.998047\n",
      "Train Epoch: 3637 [98560/118836 (83%)] Loss: 12141.591797\n",
      "    epoch          : 3637\n",
      "    loss           : 12215.664596580335\n",
      "    val_loss       : 12215.848241453738\n",
      "    val_log_likelihood: -12138.588177697218\n",
      "    val_log_marginal: -12146.513301762956\n",
      "Train Epoch: 3638 [256/118836 (0%)] Loss: 12252.339844\n",
      "Train Epoch: 3638 [33024/118836 (28%)] Loss: 12250.116211\n",
      "Train Epoch: 3638 [65792/118836 (55%)] Loss: 12172.298828\n",
      "Train Epoch: 3638 [98560/118836 (83%)] Loss: 12283.337891\n",
      "    epoch          : 3638\n",
      "    loss           : 12219.040706743692\n",
      "    val_loss       : 12215.756435295918\n",
      "    val_log_likelihood: -12139.105337733923\n",
      "    val_log_marginal: -12147.036212107023\n",
      "Train Epoch: 3639 [256/118836 (0%)] Loss: 12193.757812\n",
      "Train Epoch: 3639 [33024/118836 (28%)] Loss: 12340.078125\n",
      "Train Epoch: 3639 [65792/118836 (55%)] Loss: 12266.360352\n",
      "Train Epoch: 3639 [98560/118836 (83%)] Loss: 12245.019531\n",
      "    epoch          : 3639\n",
      "    loss           : 12216.424806626084\n",
      "    val_loss       : 12215.388316049635\n",
      "    val_log_likelihood: -12139.767192184914\n",
      "    val_log_marginal: -12147.688623958038\n",
      "Train Epoch: 3640 [256/118836 (0%)] Loss: 12209.091797\n",
      "Train Epoch: 3640 [33024/118836 (28%)] Loss: 12170.110352\n",
      "Train Epoch: 3640 [65792/118836 (55%)] Loss: 12242.107422\n",
      "Train Epoch: 3640 [98560/118836 (83%)] Loss: 12318.896484\n",
      "    epoch          : 3640\n",
      "    loss           : 12217.67158388906\n",
      "    val_loss       : 12216.392194607946\n",
      "    val_log_likelihood: -12139.062066887665\n",
      "    val_log_marginal: -12146.993904780571\n",
      "Train Epoch: 3641 [256/118836 (0%)] Loss: 12216.697266\n",
      "Train Epoch: 3641 [33024/118836 (28%)] Loss: 12209.456055\n",
      "Train Epoch: 3641 [65792/118836 (55%)] Loss: 12174.428711\n",
      "Train Epoch: 3641 [98560/118836 (83%)] Loss: 12181.624023\n",
      "    epoch          : 3641\n",
      "    loss           : 12220.253186388543\n",
      "    val_loss       : 12213.389125527245\n",
      "    val_log_likelihood: -12140.731180857114\n",
      "    val_log_marginal: -12148.658305517156\n",
      "Train Epoch: 3642 [256/118836 (0%)] Loss: 12182.273438\n",
      "Train Epoch: 3642 [33024/118836 (28%)] Loss: 12261.488281\n",
      "Train Epoch: 3642 [65792/118836 (55%)] Loss: 12238.548828\n",
      "Train Epoch: 3642 [98560/118836 (83%)] Loss: 12305.345703\n",
      "    epoch          : 3642\n",
      "    loss           : 12219.945298283705\n",
      "    val_loss       : 12219.634739658848\n",
      "    val_log_likelihood: -12138.959228636788\n",
      "    val_log_marginal: -12146.870534394599\n",
      "Train Epoch: 3643 [256/118836 (0%)] Loss: 12200.708008\n",
      "Train Epoch: 3643 [33024/118836 (28%)] Loss: 12237.090820\n",
      "Train Epoch: 3643 [65792/118836 (55%)] Loss: 12199.000977\n",
      "Train Epoch: 3643 [98560/118836 (83%)] Loss: 12354.951172\n",
      "    epoch          : 3643\n",
      "    loss           : 12219.609547211021\n",
      "    val_loss       : 12215.88465151545\n",
      "    val_log_likelihood: -12140.786543146452\n",
      "    val_log_marginal: -12148.720636598335\n",
      "Train Epoch: 3644 [256/118836 (0%)] Loss: 12260.265625\n",
      "Train Epoch: 3644 [33024/118836 (28%)] Loss: 12206.156250\n",
      "Train Epoch: 3644 [65792/118836 (55%)] Loss: 12250.724609\n",
      "Train Epoch: 3644 [98560/118836 (83%)] Loss: 12268.990234\n",
      "    epoch          : 3644\n",
      "    loss           : 12220.685077575734\n",
      "    val_loss       : 12215.75191918077\n",
      "    val_log_likelihood: -12140.38790807227\n",
      "    val_log_marginal: -12148.310111095236\n",
      "Train Epoch: 3645 [256/118836 (0%)] Loss: 12273.489258\n",
      "Train Epoch: 3645 [33024/118836 (28%)] Loss: 12238.895508\n",
      "Train Epoch: 3645 [65792/118836 (55%)] Loss: 12343.164062\n",
      "Train Epoch: 3645 [98560/118836 (83%)] Loss: 12311.109375\n",
      "    epoch          : 3645\n",
      "    loss           : 12220.41170259512\n",
      "    val_loss       : 12215.851846981272\n",
      "    val_log_likelihood: -12140.608650615179\n",
      "    val_log_marginal: -12148.532491047492\n",
      "Train Epoch: 3646 [256/118836 (0%)] Loss: 12216.634766\n",
      "Train Epoch: 3646 [33024/118836 (28%)] Loss: 12183.928711\n",
      "Train Epoch: 3646 [65792/118836 (55%)] Loss: 12134.606445\n",
      "Train Epoch: 3646 [98560/118836 (83%)] Loss: 12312.227539\n",
      "    epoch          : 3646\n",
      "    loss           : 12213.785690330335\n",
      "    val_loss       : 12216.318901942872\n",
      "    val_log_likelihood: -12139.487947651725\n",
      "    val_log_marginal: -12147.420141659994\n",
      "Train Epoch: 3647 [256/118836 (0%)] Loss: 12268.671875\n",
      "Train Epoch: 3647 [33024/118836 (28%)] Loss: 12301.284180\n",
      "Train Epoch: 3647 [65792/118836 (55%)] Loss: 12250.035156\n",
      "Train Epoch: 3647 [98560/118836 (83%)] Loss: 12228.053711\n",
      "    epoch          : 3647\n",
      "    loss           : 12218.481514778485\n",
      "    val_loss       : 12212.302454679371\n",
      "    val_log_likelihood: -12138.74755786678\n",
      "    val_log_marginal: -12146.676998975152\n",
      "Train Epoch: 3648 [256/118836 (0%)] Loss: 12148.540039\n",
      "Train Epoch: 3648 [33024/118836 (28%)] Loss: 12177.477539\n",
      "Train Epoch: 3648 [65792/118836 (55%)] Loss: 12236.208984\n",
      "Train Epoch: 3648 [98560/118836 (83%)] Loss: 12334.723633\n",
      "    epoch          : 3648\n",
      "    loss           : 12219.619519941585\n",
      "    val_loss       : 12217.439199910832\n",
      "    val_log_likelihood: -12139.436527153122\n",
      "    val_log_marginal: -12147.35656477114\n",
      "Train Epoch: 3649 [256/118836 (0%)] Loss: 12187.921875\n",
      "Train Epoch: 3649 [33024/118836 (28%)] Loss: 12203.451172\n",
      "Train Epoch: 3649 [65792/118836 (55%)] Loss: 12167.749023\n",
      "Train Epoch: 3649 [98560/118836 (83%)] Loss: 12213.355469\n",
      "    epoch          : 3649\n",
      "    loss           : 12222.533491812706\n",
      "    val_loss       : 12216.040279833527\n",
      "    val_log_likelihood: -12139.697839769437\n",
      "    val_log_marginal: -12147.620726535157\n",
      "Train Epoch: 3650 [256/118836 (0%)] Loss: 12244.375977\n",
      "Train Epoch: 3650 [33024/118836 (28%)] Loss: 12201.736328\n",
      "Train Epoch: 3650 [65792/118836 (55%)] Loss: 12261.837891\n",
      "Train Epoch: 3650 [98560/118836 (83%)] Loss: 12231.312500\n",
      "    epoch          : 3650\n",
      "    loss           : 12220.733973389682\n",
      "    val_loss       : 12218.272103366506\n",
      "    val_log_likelihood: -12142.257765812397\n",
      "    val_log_marginal: -12150.188576716546\n",
      "Train Epoch: 3651 [256/118836 (0%)] Loss: 12230.212891\n",
      "Train Epoch: 3651 [33024/118836 (28%)] Loss: 12236.220703\n",
      "Train Epoch: 3651 [65792/118836 (55%)] Loss: 12207.234375\n",
      "Train Epoch: 3651 [98560/118836 (83%)] Loss: 12302.565430\n",
      "    epoch          : 3651\n",
      "    loss           : 12218.643623022643\n",
      "    val_loss       : 12218.51253160279\n",
      "    val_log_likelihood: -12140.125646679848\n",
      "    val_log_marginal: -12148.024443346902\n",
      "Train Epoch: 3652 [256/118836 (0%)] Loss: 12213.687500\n",
      "Train Epoch: 3652 [33024/118836 (28%)] Loss: 12256.644531\n",
      "Train Epoch: 3652 [65792/118836 (55%)] Loss: 12247.775391\n",
      "Train Epoch: 3652 [98560/118836 (83%)] Loss: 12296.345703\n",
      "    epoch          : 3652\n",
      "    loss           : 12218.688203383477\n",
      "    val_loss       : 12216.73156898717\n",
      "    val_log_likelihood: -12140.19158540762\n",
      "    val_log_marginal: -12148.098900454726\n",
      "Train Epoch: 3653 [256/118836 (0%)] Loss: 12274.349609\n",
      "Train Epoch: 3653 [33024/118836 (28%)] Loss: 12169.416016\n",
      "Train Epoch: 3653 [65792/118836 (55%)] Loss: 12214.029297\n",
      "Train Epoch: 3653 [98560/118836 (83%)] Loss: 12267.546875\n",
      "    epoch          : 3653\n",
      "    loss           : 12219.468494429797\n",
      "    val_loss       : 12216.874137022589\n",
      "    val_log_likelihood: -12138.575103714329\n",
      "    val_log_marginal: -12146.49008684881\n",
      "Train Epoch: 3654 [256/118836 (0%)] Loss: 12251.228516\n",
      "Train Epoch: 3654 [33024/118836 (28%)] Loss: 12253.890625\n",
      "Train Epoch: 3654 [65792/118836 (55%)] Loss: 12237.776367\n",
      "Train Epoch: 3654 [98560/118836 (83%)] Loss: 12297.650391\n",
      "    epoch          : 3654\n",
      "    loss           : 12219.335538797559\n",
      "    val_loss       : 12216.488427067234\n",
      "    val_log_likelihood: -12140.46127836797\n",
      "    val_log_marginal: -12148.377717638614\n",
      "Train Epoch: 3655 [256/118836 (0%)] Loss: 12315.698242\n",
      "Train Epoch: 3655 [33024/118836 (28%)] Loss: 12324.549805\n",
      "Train Epoch: 3655 [65792/118836 (55%)] Loss: 12197.457031\n",
      "Train Epoch: 3655 [98560/118836 (83%)] Loss: 12261.601562\n",
      "    epoch          : 3655\n",
      "    loss           : 12216.68873536368\n",
      "    val_loss       : 12215.080424431282\n",
      "    val_log_likelihood: -12138.777029376035\n",
      "    val_log_marginal: -12146.684310654393\n",
      "Train Epoch: 3656 [256/118836 (0%)] Loss: 12236.845703\n",
      "Train Epoch: 3656 [33024/118836 (28%)] Loss: 12291.851562\n",
      "Train Epoch: 3656 [65792/118836 (55%)] Loss: 12252.465820\n",
      "Train Epoch: 3656 [98560/118836 (83%)] Loss: 12308.525391\n",
      "    epoch          : 3656\n",
      "    loss           : 12216.923029750827\n",
      "    val_loss       : 12219.449793679241\n",
      "    val_log_likelihood: -12140.490417086694\n",
      "    val_log_marginal: -12148.408293352237\n",
      "Train Epoch: 3657 [256/118836 (0%)] Loss: 12187.191406\n",
      "Train Epoch: 3657 [33024/118836 (28%)] Loss: 12183.366211\n",
      "Train Epoch: 3657 [65792/118836 (55%)] Loss: 12374.587891\n",
      "Train Epoch: 3657 [98560/118836 (83%)] Loss: 12169.296875\n",
      "    epoch          : 3657\n",
      "    loss           : 12216.905506390869\n",
      "    val_loss       : 12219.096978280482\n",
      "    val_log_likelihood: -12138.604653090106\n",
      "    val_log_marginal: -12146.52472912202\n",
      "Train Epoch: 3658 [256/118836 (0%)] Loss: 12344.966797\n",
      "Train Epoch: 3658 [33024/118836 (28%)] Loss: 12198.218750\n",
      "Train Epoch: 3658 [65792/118836 (55%)] Loss: 12249.995117\n",
      "Train Epoch: 3658 [98560/118836 (83%)] Loss: 12197.385742\n",
      "    epoch          : 3658\n",
      "    loss           : 12219.643561311\n",
      "    val_loss       : 12213.395364902242\n",
      "    val_log_likelihood: -12139.20066267318\n",
      "    val_log_marginal: -12147.11799954314\n",
      "Train Epoch: 3659 [256/118836 (0%)] Loss: 12275.061523\n",
      "Train Epoch: 3659 [33024/118836 (28%)] Loss: 12187.531250\n",
      "Train Epoch: 3659 [65792/118836 (55%)] Loss: 12283.208008\n",
      "Train Epoch: 3659 [98560/118836 (83%)] Loss: 12253.130859\n",
      "    epoch          : 3659\n",
      "    loss           : 12219.2991379756\n",
      "    val_loss       : 12216.006339207857\n",
      "    val_log_likelihood: -12139.931127222912\n",
      "    val_log_marginal: -12147.82494550483\n",
      "Train Epoch: 3660 [256/118836 (0%)] Loss: 12278.011719\n",
      "Train Epoch: 3660 [33024/118836 (28%)] Loss: 12163.250977\n",
      "Train Epoch: 3660 [65792/118836 (55%)] Loss: 12340.028320\n",
      "Train Epoch: 3660 [98560/118836 (83%)] Loss: 12218.500000\n",
      "    epoch          : 3660\n",
      "    loss           : 12217.713394980356\n",
      "    val_loss       : 12218.985074891756\n",
      "    val_log_likelihood: -12139.099814541976\n",
      "    val_log_marginal: -12146.999807994052\n",
      "Train Epoch: 3661 [256/118836 (0%)] Loss: 12231.542969\n",
      "Train Epoch: 3661 [33024/118836 (28%)] Loss: 12206.929688\n",
      "Train Epoch: 3661 [65792/118836 (55%)] Loss: 12180.198242\n",
      "Train Epoch: 3661 [98560/118836 (83%)] Loss: 12277.356445\n",
      "    epoch          : 3661\n",
      "    loss           : 12216.707817669561\n",
      "    val_loss       : 12216.709012684349\n",
      "    val_log_likelihood: -12140.748239764269\n",
      "    val_log_marginal: -12148.660308985045\n",
      "Train Epoch: 3662 [256/118836 (0%)] Loss: 12288.119141\n",
      "Train Epoch: 3662 [33024/118836 (28%)] Loss: 12204.809570\n",
      "Train Epoch: 3662 [65792/118836 (55%)] Loss: 12265.158203\n",
      "Train Epoch: 3662 [98560/118836 (83%)] Loss: 12250.021484\n",
      "    epoch          : 3662\n",
      "    loss           : 12216.363567676022\n",
      "    val_loss       : 12218.401617479984\n",
      "    val_log_likelihood: -12139.56709460944\n",
      "    val_log_marginal: -12147.472459542347\n",
      "Train Epoch: 3663 [256/118836 (0%)] Loss: 12252.273438\n",
      "Train Epoch: 3663 [33024/118836 (28%)] Loss: 12211.105469\n",
      "Train Epoch: 3663 [65792/118836 (55%)] Loss: 12153.560547\n",
      "Train Epoch: 3663 [98560/118836 (83%)] Loss: 12175.705078\n",
      "    epoch          : 3663\n",
      "    loss           : 12215.1463360732\n",
      "    val_loss       : 12214.996257851615\n",
      "    val_log_likelihood: -12140.088355885546\n",
      "    val_log_marginal: -12148.005196684559\n",
      "Train Epoch: 3664 [256/118836 (0%)] Loss: 12253.800781\n",
      "Train Epoch: 3664 [33024/118836 (28%)] Loss: 12179.452148\n",
      "Train Epoch: 3664 [65792/118836 (55%)] Loss: 12148.297852\n",
      "Train Epoch: 3664 [98560/118836 (83%)] Loss: 12346.043945\n",
      "    epoch          : 3664\n",
      "    loss           : 12219.099532800868\n",
      "    val_loss       : 12219.146489076073\n",
      "    val_log_likelihood: -12141.081735131049\n",
      "    val_log_marginal: -12148.989529636665\n",
      "Train Epoch: 3665 [256/118836 (0%)] Loss: 12190.314453\n",
      "Train Epoch: 3665 [33024/118836 (28%)] Loss: 12175.029297\n",
      "Train Epoch: 3665 [65792/118836 (55%)] Loss: 12257.208008\n",
      "Train Epoch: 3665 [98560/118836 (83%)] Loss: 12248.939453\n",
      "    epoch          : 3665\n",
      "    loss           : 12216.02872838477\n",
      "    val_loss       : 12216.1909658846\n",
      "    val_log_likelihood: -12140.17996762562\n",
      "    val_log_marginal: -12148.073432432691\n",
      "Train Epoch: 3666 [256/118836 (0%)] Loss: 12146.900391\n",
      "Train Epoch: 3666 [33024/118836 (28%)] Loss: 12269.500000\n",
      "Train Epoch: 3666 [65792/118836 (55%)] Loss: 12280.726562\n",
      "Train Epoch: 3666 [98560/118836 (83%)] Loss: 12151.610352\n",
      "    epoch          : 3666\n",
      "    loss           : 12216.4149920518\n",
      "    val_loss       : 12218.145421040044\n",
      "    val_log_likelihood: -12140.70689377197\n",
      "    val_log_marginal: -12148.604438086932\n",
      "Train Epoch: 3667 [256/118836 (0%)] Loss: 12176.826172\n",
      "Train Epoch: 3667 [33024/118836 (28%)] Loss: 12214.708984\n",
      "Train Epoch: 3667 [65792/118836 (55%)] Loss: 12188.453125\n",
      "Train Epoch: 3667 [98560/118836 (83%)] Loss: 12242.232422\n",
      "    epoch          : 3667\n",
      "    loss           : 12217.702717412376\n",
      "    val_loss       : 12219.986577810289\n",
      "    val_log_likelihood: -12139.270815562966\n",
      "    val_log_marginal: -12147.157461556311\n",
      "Train Epoch: 3668 [256/118836 (0%)] Loss: 12188.229492\n",
      "Train Epoch: 3668 [33024/118836 (28%)] Loss: 12342.828125\n",
      "Train Epoch: 3668 [65792/118836 (55%)] Loss: 12172.263672\n",
      "Train Epoch: 3668 [98560/118836 (83%)] Loss: 12281.259766\n",
      "    epoch          : 3668\n",
      "    loss           : 12214.423224578682\n",
      "    val_loss       : 12214.981332508583\n",
      "    val_log_likelihood: -12138.739273644283\n",
      "    val_log_marginal: -12146.64547278477\n",
      "Train Epoch: 3669 [256/118836 (0%)] Loss: 12278.065430\n",
      "Train Epoch: 3669 [33024/118836 (28%)] Loss: 12232.352539\n",
      "Train Epoch: 3669 [65792/118836 (55%)] Loss: 12242.075195\n",
      "Train Epoch: 3669 [98560/118836 (83%)] Loss: 12206.687500\n",
      "    epoch          : 3669\n",
      "    loss           : 12218.96817359388\n",
      "    val_loss       : 12217.143482753736\n",
      "    val_log_likelihood: -12140.168488614041\n",
      "    val_log_marginal: -12148.077067557493\n",
      "Train Epoch: 3670 [256/118836 (0%)] Loss: 12242.470703\n",
      "Train Epoch: 3670 [33024/118836 (28%)] Loss: 12204.331055\n",
      "Train Epoch: 3670 [65792/118836 (55%)] Loss: 12280.133789\n",
      "Train Epoch: 3670 [98560/118836 (83%)] Loss: 12210.038086\n",
      "    epoch          : 3670\n",
      "    loss           : 12215.09262400486\n",
      "    val_loss       : 12215.188657957407\n",
      "    val_log_likelihood: -12140.413867510597\n",
      "    val_log_marginal: -12148.30338440005\n",
      "Train Epoch: 3671 [256/118836 (0%)] Loss: 12336.046875\n",
      "Train Epoch: 3671 [33024/118836 (28%)] Loss: 12206.305664\n",
      "Train Epoch: 3671 [65792/118836 (55%)] Loss: 12191.085938\n",
      "Train Epoch: 3671 [98560/118836 (83%)] Loss: 12263.685547\n",
      "    epoch          : 3671\n",
      "    loss           : 12214.153120476634\n",
      "    val_loss       : 12220.595472319927\n",
      "    val_log_likelihood: -12139.477970720896\n",
      "    val_log_marginal: -12147.372798705788\n",
      "Train Epoch: 3672 [256/118836 (0%)] Loss: 12223.474609\n",
      "Train Epoch: 3672 [33024/118836 (28%)] Loss: 12125.896484\n",
      "Train Epoch: 3672 [65792/118836 (55%)] Loss: 12179.215820\n",
      "Train Epoch: 3672 [98560/118836 (83%)] Loss: 12289.619141\n",
      "    epoch          : 3672\n",
      "    loss           : 12217.286832641905\n",
      "    val_loss       : 12219.898201544445\n",
      "    val_log_likelihood: -12139.363630195408\n",
      "    val_log_marginal: -12147.267022333685\n",
      "Train Epoch: 3673 [256/118836 (0%)] Loss: 12226.043945\n",
      "Train Epoch: 3673 [33024/118836 (28%)] Loss: 12381.604492\n",
      "Train Epoch: 3673 [65792/118836 (55%)] Loss: 12281.274414\n",
      "Train Epoch: 3673 [98560/118836 (83%)] Loss: 12373.173828\n",
      "    epoch          : 3673\n",
      "    loss           : 12220.679872634924\n",
      "    val_loss       : 12214.626315082229\n",
      "    val_log_likelihood: -12139.220177606752\n",
      "    val_log_marginal: -12147.1320510431\n",
      "Train Epoch: 3674 [256/118836 (0%)] Loss: 12253.768555\n",
      "Train Epoch: 3674 [33024/118836 (28%)] Loss: 12274.312500\n",
      "Train Epoch: 3674 [65792/118836 (55%)] Loss: 12194.375000\n",
      "Train Epoch: 3674 [98560/118836 (83%)] Loss: 12266.280273\n",
      "    epoch          : 3674\n",
      "    loss           : 12215.394103630324\n",
      "    val_loss       : 12221.537811353834\n",
      "    val_log_likelihood: -12137.960433790839\n",
      "    val_log_marginal: -12145.872963750118\n",
      "Train Epoch: 3675 [256/118836 (0%)] Loss: 12157.358398\n",
      "Train Epoch: 3675 [33024/118836 (28%)] Loss: 12184.879883\n",
      "Train Epoch: 3675 [65792/118836 (55%)] Loss: 12319.379883\n",
      "Train Epoch: 3675 [98560/118836 (83%)] Loss: 12275.576172\n",
      "    epoch          : 3675\n",
      "    loss           : 12219.168866315136\n",
      "    val_loss       : 12217.832570301129\n",
      "    val_log_likelihood: -12139.500733593104\n",
      "    val_log_marginal: -12147.436586704915\n",
      "Train Epoch: 3676 [256/118836 (0%)] Loss: 12237.821289\n",
      "Train Epoch: 3676 [33024/118836 (28%)] Loss: 12291.396484\n",
      "Train Epoch: 3676 [65792/118836 (55%)] Loss: 12283.531250\n",
      "Train Epoch: 3676 [98560/118836 (83%)] Loss: 12321.762695\n",
      "    epoch          : 3676\n",
      "    loss           : 12220.027628560536\n",
      "    val_loss       : 12214.763429885212\n",
      "    val_log_likelihood: -12139.801014526469\n",
      "    val_log_marginal: -12147.716528755976\n",
      "Train Epoch: 3677 [256/118836 (0%)] Loss: 12251.406250\n",
      "Train Epoch: 3677 [33024/118836 (28%)] Loss: 12407.014648\n",
      "Train Epoch: 3677 [65792/118836 (55%)] Loss: 12260.921875\n",
      "Train Epoch: 3677 [98560/118836 (83%)] Loss: 12259.563477\n",
      "    epoch          : 3677\n",
      "    loss           : 12219.226083346259\n",
      "    val_loss       : 12218.487531383604\n",
      "    val_log_likelihood: -12140.318413978495\n",
      "    val_log_marginal: -12148.225782096963\n",
      "Train Epoch: 3678 [256/118836 (0%)] Loss: 12234.469727\n",
      "Train Epoch: 3678 [33024/118836 (28%)] Loss: 12398.134766\n",
      "Train Epoch: 3678 [65792/118836 (55%)] Loss: 12249.811523\n",
      "Train Epoch: 3678 [98560/118836 (83%)] Loss: 12231.459961\n",
      "    epoch          : 3678\n",
      "    loss           : 12221.680190724515\n",
      "    val_loss       : 12217.592906080808\n",
      "    val_log_likelihood: -12140.145901830027\n",
      "    val_log_marginal: -12148.046607203152\n",
      "Train Epoch: 3679 [256/118836 (0%)] Loss: 12178.207031\n",
      "Train Epoch: 3679 [33024/118836 (28%)] Loss: 12223.823242\n",
      "Train Epoch: 3679 [65792/118836 (55%)] Loss: 12267.761719\n",
      "Train Epoch: 3679 [98560/118836 (83%)] Loss: 12262.033203\n",
      "    epoch          : 3679\n",
      "    loss           : 12217.442154544045\n",
      "    val_loss       : 12222.953418469666\n",
      "    val_log_likelihood: -12138.5876479787\n",
      "    val_log_marginal: -12146.472788035966\n",
      "Train Epoch: 3680 [256/118836 (0%)] Loss: 12272.834961\n",
      "Train Epoch: 3680 [33024/118836 (28%)] Loss: 12253.541992\n",
      "Train Epoch: 3680 [65792/118836 (55%)] Loss: 12158.246094\n",
      "Train Epoch: 3680 [98560/118836 (83%)] Loss: 12174.005859\n",
      "    epoch          : 3680\n",
      "    loss           : 12215.829840325165\n",
      "    val_loss       : 12217.128723275557\n",
      "    val_log_likelihood: -12140.414630667132\n",
      "    val_log_marginal: -12148.319296069823\n",
      "Train Epoch: 3681 [256/118836 (0%)] Loss: 12233.314453\n",
      "Train Epoch: 3681 [33024/118836 (28%)] Loss: 12166.216797\n",
      "Train Epoch: 3681 [65792/118836 (55%)] Loss: 12193.980469\n",
      "Train Epoch: 3681 [98560/118836 (83%)] Loss: 12226.419922\n",
      "    epoch          : 3681\n",
      "    loss           : 12217.14962165271\n",
      "    val_loss       : 12217.365532055972\n",
      "    val_log_likelihood: -12140.16747861094\n",
      "    val_log_marginal: -12148.076594745567\n",
      "Train Epoch: 3682 [256/118836 (0%)] Loss: 12320.586914\n",
      "Train Epoch: 3682 [33024/118836 (28%)] Loss: 12357.402344\n",
      "Train Epoch: 3682 [65792/118836 (55%)] Loss: 12244.294922\n",
      "Train Epoch: 3682 [98560/118836 (83%)] Loss: 12225.773438\n",
      "    epoch          : 3682\n",
      "    loss           : 12218.385331045802\n",
      "    val_loss       : 12215.475203497945\n",
      "    val_log_likelihood: -12140.07504587986\n",
      "    val_log_marginal: -12147.980759854901\n",
      "Train Epoch: 3683 [256/118836 (0%)] Loss: 12216.186523\n",
      "Train Epoch: 3683 [33024/118836 (28%)] Loss: 12337.393555\n",
      "Train Epoch: 3683 [65792/118836 (55%)] Loss: 12213.384766\n",
      "Train Epoch: 3683 [98560/118836 (83%)] Loss: 12294.259766\n",
      "    epoch          : 3683\n",
      "    loss           : 12218.011124896608\n",
      "    val_loss       : 12216.585166091847\n",
      "    val_log_likelihood: -12139.279276681398\n",
      "    val_log_marginal: -12147.17453724289\n",
      "Train Epoch: 3684 [256/118836 (0%)] Loss: 12307.509766\n",
      "Train Epoch: 3684 [33024/118836 (28%)] Loss: 12263.160156\n",
      "Train Epoch: 3684 [65792/118836 (55%)] Loss: 12274.359375\n",
      "Train Epoch: 3684 [98560/118836 (83%)] Loss: 12284.189453\n",
      "    epoch          : 3684\n",
      "    loss           : 12221.157043043064\n",
      "    val_loss       : 12216.990830523873\n",
      "    val_log_likelihood: -12139.858526868797\n",
      "    val_log_marginal: -12147.760275191566\n",
      "Train Epoch: 3685 [256/118836 (0%)] Loss: 12296.966797\n",
      "Train Epoch: 3685 [33024/118836 (28%)] Loss: 12322.007812\n",
      "Train Epoch: 3685 [65792/118836 (55%)] Loss: 12211.095703\n",
      "Train Epoch: 3685 [98560/118836 (83%)] Loss: 12209.918945\n",
      "    epoch          : 3685\n",
      "    loss           : 12219.236307931398\n",
      "    val_loss       : 12212.468878545316\n",
      "    val_log_likelihood: -12138.599486274812\n",
      "    val_log_marginal: -12146.497894164375\n",
      "Train Epoch: 3686 [256/118836 (0%)] Loss: 12187.639648\n",
      "Train Epoch: 3686 [33024/118836 (28%)] Loss: 12154.515625\n",
      "Train Epoch: 3686 [65792/118836 (55%)] Loss: 12242.496094\n",
      "Train Epoch: 3686 [98560/118836 (83%)] Loss: 12298.580078\n",
      "    epoch          : 3686\n",
      "    loss           : 12218.17168905733\n",
      "    val_loss       : 12218.655848378015\n",
      "    val_log_likelihood: -12139.3234040594\n",
      "    val_log_marginal: -12147.229042377969\n",
      "Train Epoch: 3687 [256/118836 (0%)] Loss: 12235.461914\n",
      "Train Epoch: 3687 [33024/118836 (28%)] Loss: 12249.594727\n",
      "Train Epoch: 3687 [65792/118836 (55%)] Loss: 12279.660156\n",
      "Train Epoch: 3687 [98560/118836 (83%)] Loss: 12181.625000\n",
      "    epoch          : 3687\n",
      "    loss           : 12220.605592011734\n",
      "    val_loss       : 12217.753527710449\n",
      "    val_log_likelihood: -12140.02796054332\n",
      "    val_log_marginal: -12147.930698785773\n",
      "Train Epoch: 3688 [256/118836 (0%)] Loss: 12274.806641\n",
      "Train Epoch: 3688 [33024/118836 (28%)] Loss: 12160.306641\n",
      "Train Epoch: 3688 [65792/118836 (55%)] Loss: 12239.537109\n",
      "Train Epoch: 3688 [98560/118836 (83%)] Loss: 12340.647461\n",
      "    epoch          : 3688\n",
      "    loss           : 12220.490776048127\n",
      "    val_loss       : 12219.923423405457\n",
      "    val_log_likelihood: -12138.92616654389\n",
      "    val_log_marginal: -12146.836334582393\n",
      "Train Epoch: 3689 [256/118836 (0%)] Loss: 12219.761719\n",
      "Train Epoch: 3689 [33024/118836 (28%)] Loss: 12377.850586\n",
      "Train Epoch: 3689 [65792/118836 (55%)] Loss: 12239.775391\n",
      "Train Epoch: 3689 [98560/118836 (83%)] Loss: 12277.097656\n",
      "    epoch          : 3689\n",
      "    loss           : 12218.84609358845\n",
      "    val_loss       : 12214.550295961966\n",
      "    val_log_likelihood: -12138.728307550144\n",
      "    val_log_marginal: -12146.632573379133\n",
      "Train Epoch: 3690 [256/118836 (0%)] Loss: 12242.806641\n",
      "Train Epoch: 3690 [33024/118836 (28%)] Loss: 12276.181641\n",
      "Train Epoch: 3690 [65792/118836 (55%)] Loss: 12269.867188\n",
      "Train Epoch: 3690 [98560/118836 (83%)] Loss: 12182.519531\n",
      "    epoch          : 3690\n",
      "    loss           : 12215.992246465314\n",
      "    val_loss       : 12214.35244792844\n",
      "    val_log_likelihood: -12140.310713108714\n",
      "    val_log_marginal: -12148.218975190475\n",
      "Train Epoch: 3691 [256/118836 (0%)] Loss: 12262.576172\n",
      "Train Epoch: 3691 [33024/118836 (28%)] Loss: 12175.849609\n",
      "Train Epoch: 3691 [65792/118836 (55%)] Loss: 12331.443359\n",
      "Train Epoch: 3691 [98560/118836 (83%)] Loss: 12280.776367\n",
      "    epoch          : 3691\n",
      "    loss           : 12216.739224210349\n",
      "    val_loss       : 12218.218233064794\n",
      "    val_log_likelihood: -12138.122671112491\n",
      "    val_log_marginal: -12146.035012404796\n",
      "Train Epoch: 3692 [256/118836 (0%)] Loss: 12238.804688\n",
      "Train Epoch: 3692 [33024/118836 (28%)] Loss: 12317.000977\n",
      "Train Epoch: 3692 [65792/118836 (55%)] Loss: 12317.736328\n",
      "Train Epoch: 3692 [98560/118836 (83%)] Loss: 12213.351562\n",
      "    epoch          : 3692\n",
      "    loss           : 12216.096049485628\n",
      "    val_loss       : 12218.315233903495\n",
      "    val_log_likelihood: -12140.38765734853\n",
      "    val_log_marginal: -12148.309018554593\n",
      "Train Epoch: 3693 [256/118836 (0%)] Loss: 12259.685547\n",
      "Train Epoch: 3693 [33024/118836 (28%)] Loss: 12388.359375\n",
      "Train Epoch: 3693 [65792/118836 (55%)] Loss: 12196.504883\n",
      "Train Epoch: 3693 [98560/118836 (83%)] Loss: 12279.765625\n",
      "    epoch          : 3693\n",
      "    loss           : 12220.402263460246\n",
      "    val_loss       : 12216.183244161433\n",
      "    val_log_likelihood: -12140.451264119365\n",
      "    val_log_marginal: -12148.382842166817\n",
      "Train Epoch: 3694 [256/118836 (0%)] Loss: 12262.716797\n",
      "Train Epoch: 3694 [33024/118836 (28%)] Loss: 12129.683594\n",
      "Train Epoch: 3694 [65792/118836 (55%)] Loss: 12196.225586\n",
      "Train Epoch: 3694 [98560/118836 (83%)] Loss: 12230.393555\n",
      "    epoch          : 3694\n",
      "    loss           : 12218.665379122725\n",
      "    val_loss       : 12221.207264282053\n",
      "    val_log_likelihood: -12139.295075023263\n",
      "    val_log_marginal: -12147.23414749649\n",
      "Train Epoch: 3695 [256/118836 (0%)] Loss: 12240.851562\n",
      "Train Epoch: 3695 [33024/118836 (28%)] Loss: 12160.421875\n",
      "Train Epoch: 3695 [65792/118836 (55%)] Loss: 12157.236328\n",
      "Train Epoch: 3695 [98560/118836 (83%)] Loss: 12324.958984\n",
      "    epoch          : 3695\n",
      "    loss           : 12219.251306606699\n",
      "    val_loss       : 12223.816164727037\n",
      "    val_log_likelihood: -12141.086071585505\n",
      "    val_log_marginal: -12149.006296115465\n",
      "Train Epoch: 3696 [256/118836 (0%)] Loss: 12197.128906\n",
      "Train Epoch: 3696 [33024/118836 (28%)] Loss: 12262.335938\n",
      "Train Epoch: 3696 [65792/118836 (55%)] Loss: 12358.289062\n",
      "Train Epoch: 3696 [98560/118836 (83%)] Loss: 12298.166016\n",
      "    epoch          : 3696\n",
      "    loss           : 12219.103529518197\n",
      "    val_loss       : 12217.684764515849\n",
      "    val_log_likelihood: -12138.773219893765\n",
      "    val_log_marginal: -12146.703286290245\n",
      "Train Epoch: 3697 [256/118836 (0%)] Loss: 12272.294922\n",
      "Train Epoch: 3697 [33024/118836 (28%)] Loss: 12262.955078\n",
      "Train Epoch: 3697 [65792/118836 (55%)] Loss: 12147.810547\n",
      "Train Epoch: 3697 [98560/118836 (83%)] Loss: 12202.456055\n",
      "    epoch          : 3697\n",
      "    loss           : 12218.35897468207\n",
      "    val_loss       : 12217.909296188252\n",
      "    val_log_likelihood: -12139.849577549887\n",
      "    val_log_marginal: -12147.778307925406\n",
      "Train Epoch: 3698 [256/118836 (0%)] Loss: 12418.313477\n",
      "Train Epoch: 3698 [33024/118836 (28%)] Loss: 12213.539062\n",
      "Train Epoch: 3698 [65792/118836 (55%)] Loss: 12315.811523\n",
      "Train Epoch: 3698 [98560/118836 (83%)] Loss: 12243.217773\n",
      "    epoch          : 3698\n",
      "    loss           : 12216.97337821159\n",
      "    val_loss       : 12220.614433156641\n",
      "    val_log_likelihood: -12138.717449370606\n",
      "    val_log_marginal: -12146.61488609489\n",
      "Train Epoch: 3699 [256/118836 (0%)] Loss: 12296.546875\n",
      "Train Epoch: 3699 [33024/118836 (28%)] Loss: 12204.126953\n",
      "Train Epoch: 3699 [65792/118836 (55%)] Loss: 12179.706055\n",
      "Train Epoch: 3699 [98560/118836 (83%)] Loss: 12202.089844\n",
      "    epoch          : 3699\n",
      "    loss           : 12215.240546164185\n",
      "    val_loss       : 12219.167175275012\n",
      "    val_log_likelihood: -12139.63922421035\n",
      "    val_log_marginal: -12147.545687008535\n",
      "Train Epoch: 3700 [256/118836 (0%)] Loss: 12175.262695\n",
      "Train Epoch: 3700 [33024/118836 (28%)] Loss: 12181.878906\n",
      "Train Epoch: 3700 [65792/118836 (55%)] Loss: 12354.255859\n",
      "Train Epoch: 3700 [98560/118836 (83%)] Loss: 12161.038086\n",
      "    epoch          : 3700\n",
      "    loss           : 12218.364989790116\n",
      "    val_loss       : 12218.570181700652\n",
      "    val_log_likelihood: -12141.206921558365\n",
      "    val_log_marginal: -12149.113941993683\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch3700.pth ...\n",
      "Train Epoch: 3701 [256/118836 (0%)] Loss: 12136.588867\n",
      "Train Epoch: 3701 [33024/118836 (28%)] Loss: 12231.448242\n",
      "Train Epoch: 3701 [65792/118836 (55%)] Loss: 12220.031250\n",
      "Train Epoch: 3701 [98560/118836 (83%)] Loss: 12222.271484\n",
      "    epoch          : 3701\n",
      "    loss           : 12220.400737147178\n",
      "    val_loss       : 12220.281594599272\n",
      "    val_log_likelihood: -12139.12990833721\n",
      "    val_log_marginal: -12147.051814619435\n",
      "Train Epoch: 3702 [256/118836 (0%)] Loss: 12149.458008\n",
      "Train Epoch: 3702 [33024/118836 (28%)] Loss: 12223.303711\n",
      "Train Epoch: 3702 [65792/118836 (55%)] Loss: 12142.807617\n",
      "Train Epoch: 3702 [98560/118836 (83%)] Loss: 12235.995117\n",
      "    epoch          : 3702\n",
      "    loss           : 12215.906179403173\n",
      "    val_loss       : 12214.950388246367\n",
      "    val_log_likelihood: -12141.13743134176\n",
      "    val_log_marginal: -12149.0567746434\n",
      "Train Epoch: 3703 [256/118836 (0%)] Loss: 12237.590820\n",
      "Train Epoch: 3703 [33024/118836 (28%)] Loss: 12324.902344\n",
      "Train Epoch: 3703 [65792/118836 (55%)] Loss: 12268.691406\n",
      "Train Epoch: 3703 [98560/118836 (83%)] Loss: 12295.438477\n",
      "    epoch          : 3703\n",
      "    loss           : 12214.589781069064\n",
      "    val_loss       : 12214.567013664831\n",
      "    val_log_likelihood: -12138.284379846464\n",
      "    val_log_marginal: -12146.207761581672\n",
      "Train Epoch: 3704 [256/118836 (0%)] Loss: 12225.974609\n",
      "Train Epoch: 3704 [33024/118836 (28%)] Loss: 12292.500000\n",
      "Train Epoch: 3704 [65792/118836 (55%)] Loss: 12219.828125\n",
      "Train Epoch: 3704 [98560/118836 (83%)] Loss: 12234.653320\n",
      "    epoch          : 3704\n",
      "    loss           : 12218.606331259047\n",
      "    val_loss       : 12218.147428525977\n",
      "    val_log_likelihood: -12137.289445532206\n",
      "    val_log_marginal: -12145.193069688112\n",
      "Train Epoch: 3705 [256/118836 (0%)] Loss: 12309.880859\n",
      "Train Epoch: 3705 [33024/118836 (28%)] Loss: 12222.071289\n",
      "Train Epoch: 3705 [65792/118836 (55%)] Loss: 12221.102539\n",
      "Train Epoch: 3705 [98560/118836 (83%)] Loss: 12325.470703\n",
      "    epoch          : 3705\n",
      "    loss           : 12218.165183164032\n",
      "    val_loss       : 12222.226390331298\n",
      "    val_log_likelihood: -12138.499023922146\n",
      "    val_log_marginal: -12146.427350076865\n",
      "Train Epoch: 3706 [256/118836 (0%)] Loss: 12210.322266\n",
      "Train Epoch: 3706 [33024/118836 (28%)] Loss: 12257.851562\n",
      "Train Epoch: 3706 [65792/118836 (55%)] Loss: 12157.510742\n",
      "Train Epoch: 3706 [98560/118836 (83%)] Loss: 12147.859375\n",
      "    epoch          : 3706\n",
      "    loss           : 12219.333695202648\n",
      "    val_loss       : 12219.026336422668\n",
      "    val_log_likelihood: -12140.275401287221\n",
      "    val_log_marginal: -12148.20100529154\n",
      "Train Epoch: 3707 [256/118836 (0%)] Loss: 12227.027344\n",
      "Train Epoch: 3707 [33024/118836 (28%)] Loss: 12243.874023\n",
      "Train Epoch: 3707 [65792/118836 (55%)] Loss: 12267.146484\n",
      "Train Epoch: 3707 [98560/118836 (83%)] Loss: 12252.811523\n",
      "    epoch          : 3707\n",
      "    loss           : 12217.124732475188\n",
      "    val_loss       : 12219.775915860633\n",
      "    val_log_likelihood: -12139.951331162118\n",
      "    val_log_marginal: -12147.874653689245\n",
      "Train Epoch: 3708 [256/118836 (0%)] Loss: 12297.224609\n",
      "Train Epoch: 3708 [33024/118836 (28%)] Loss: 12164.017578\n",
      "Train Epoch: 3708 [65792/118836 (55%)] Loss: 12280.797852\n",
      "Train Epoch: 3708 [98560/118836 (83%)] Loss: 12374.932617\n",
      "    epoch          : 3708\n",
      "    loss           : 12220.063788351685\n",
      "    val_loss       : 12218.932389299423\n",
      "    val_log_likelihood: -12142.092366334522\n",
      "    val_log_marginal: -12150.011204304843\n",
      "Train Epoch: 3709 [256/118836 (0%)] Loss: 12174.937500\n",
      "Train Epoch: 3709 [33024/118836 (28%)] Loss: 12254.459961\n",
      "Train Epoch: 3709 [65792/118836 (55%)] Loss: 12233.939453\n",
      "Train Epoch: 3709 [98560/118836 (83%)] Loss: 12154.435547\n",
      "    epoch          : 3709\n",
      "    loss           : 12217.177349888854\n",
      "    val_loss       : 12224.76655011313\n",
      "    val_log_likelihood: -12138.756658233819\n",
      "    val_log_marginal: -12146.686403861097\n",
      "Train Epoch: 3710 [256/118836 (0%)] Loss: 12287.609375\n",
      "Train Epoch: 3710 [33024/118836 (28%)] Loss: 12180.734375\n",
      "Train Epoch: 3710 [65792/118836 (55%)] Loss: 12218.314453\n",
      "Train Epoch: 3710 [98560/118836 (83%)] Loss: 12251.921875\n",
      "    epoch          : 3710\n",
      "    loss           : 12220.357785844448\n",
      "    val_loss       : 12217.983470564524\n",
      "    val_log_likelihood: -12140.285150111145\n",
      "    val_log_marginal: -12148.214700734694\n",
      "Train Epoch: 3711 [256/118836 (0%)] Loss: 12240.037109\n",
      "Train Epoch: 3711 [33024/118836 (28%)] Loss: 12328.194336\n",
      "Train Epoch: 3711 [65792/118836 (55%)] Loss: 12195.330078\n",
      "Train Epoch: 3711 [98560/118836 (83%)] Loss: 12297.921875\n",
      "    epoch          : 3711\n",
      "    loss           : 12217.87718462443\n",
      "    val_loss       : 12221.17482295527\n",
      "    val_log_likelihood: -12138.828813682536\n",
      "    val_log_marginal: -12146.744994507811\n",
      "Train Epoch: 3712 [256/118836 (0%)] Loss: 12161.396484\n",
      "Train Epoch: 3712 [33024/118836 (28%)] Loss: 12268.282227\n",
      "Train Epoch: 3712 [65792/118836 (55%)] Loss: 12243.890625\n",
      "Train Epoch: 3712 [98560/118836 (83%)] Loss: 12369.760742\n",
      "    epoch          : 3712\n",
      "    loss           : 12218.992708171783\n",
      "    val_loss       : 12218.959338344695\n",
      "    val_log_likelihood: -12140.659475806451\n",
      "    val_log_marginal: -12148.576858158533\n",
      "Train Epoch: 3713 [256/118836 (0%)] Loss: 12218.804688\n",
      "Train Epoch: 3713 [33024/118836 (28%)] Loss: 12207.656250\n",
      "Train Epoch: 3713 [65792/118836 (55%)] Loss: 12265.861328\n",
      "Train Epoch: 3713 [98560/118836 (83%)] Loss: 12424.396484\n",
      "    epoch          : 3713\n",
      "    loss           : 12218.511566571031\n",
      "    val_loss       : 12217.65767279953\n",
      "    val_log_likelihood: -12138.492288468002\n",
      "    val_log_marginal: -12146.406291822204\n",
      "Train Epoch: 3714 [256/118836 (0%)] Loss: 12179.224609\n",
      "Train Epoch: 3714 [33024/118836 (28%)] Loss: 12200.855469\n",
      "Train Epoch: 3714 [65792/118836 (55%)] Loss: 12220.636719\n",
      "Train Epoch: 3714 [98560/118836 (83%)] Loss: 12233.434570\n",
      "    epoch          : 3714\n",
      "    loss           : 12214.273144773573\n",
      "    val_loss       : 12217.673338180108\n",
      "    val_log_likelihood: -12140.253383962729\n",
      "    val_log_marginal: -12148.160081557355\n",
      "Train Epoch: 3715 [256/118836 (0%)] Loss: 12279.198242\n",
      "Train Epoch: 3715 [33024/118836 (28%)] Loss: 12222.793945\n",
      "Train Epoch: 3715 [65792/118836 (55%)] Loss: 12238.705078\n",
      "Train Epoch: 3715 [98560/118836 (83%)] Loss: 12243.927734\n",
      "    epoch          : 3715\n",
      "    loss           : 12221.356632386012\n",
      "    val_loss       : 12219.73979074648\n",
      "    val_log_likelihood: -12140.285709231544\n",
      "    val_log_marginal: -12148.203277997243\n",
      "Train Epoch: 3716 [256/118836 (0%)] Loss: 12217.721680\n",
      "Train Epoch: 3716 [33024/118836 (28%)] Loss: 12283.271484\n",
      "Train Epoch: 3716 [65792/118836 (55%)] Loss: 12198.403320\n",
      "Train Epoch: 3716 [98560/118836 (83%)] Loss: 12193.171875\n",
      "    epoch          : 3716\n",
      "    loss           : 12220.215782025434\n",
      "    val_loss       : 12221.420173391223\n",
      "    val_log_likelihood: -12139.708454010288\n",
      "    val_log_marginal: -12147.620327145145\n",
      "Train Epoch: 3717 [256/118836 (0%)] Loss: 12297.798828\n",
      "Train Epoch: 3717 [33024/118836 (28%)] Loss: 12164.645508\n",
      "Train Epoch: 3717 [65792/118836 (55%)] Loss: 12235.641602\n",
      "Train Epoch: 3717 [98560/118836 (83%)] Loss: 12307.093750\n",
      "    epoch          : 3717\n",
      "    loss           : 12222.205257121072\n",
      "    val_loss       : 12218.116149034986\n",
      "    val_log_likelihood: -12140.736934417648\n",
      "    val_log_marginal: -12148.6414679821\n",
      "Train Epoch: 3718 [256/118836 (0%)] Loss: 12190.413086\n",
      "Train Epoch: 3718 [33024/118836 (28%)] Loss: 12131.774414\n",
      "Train Epoch: 3718 [65792/118836 (55%)] Loss: 12283.898438\n",
      "Train Epoch: 3718 [98560/118836 (83%)] Loss: 12286.230469\n",
      "    epoch          : 3718\n",
      "    loss           : 12215.986068516077\n",
      "    val_loss       : 12218.62062241709\n",
      "    val_log_likelihood: -12139.93855345973\n",
      "    val_log_marginal: -12147.843018911542\n",
      "Train Epoch: 3719 [256/118836 (0%)] Loss: 12326.798828\n",
      "Train Epoch: 3719 [33024/118836 (28%)] Loss: 12255.736328\n",
      "Train Epoch: 3719 [65792/118836 (55%)] Loss: 12247.716797\n",
      "Train Epoch: 3719 [98560/118836 (83%)] Loss: 12277.017578\n",
      "    epoch          : 3719\n",
      "    loss           : 12213.748695978082\n",
      "    val_loss       : 12218.836409980298\n",
      "    val_log_likelihood: -12142.243125775434\n",
      "    val_log_marginal: -12150.157234910097\n",
      "Train Epoch: 3720 [256/118836 (0%)] Loss: 12312.921875\n",
      "Train Epoch: 3720 [33024/118836 (28%)] Loss: 12202.419922\n",
      "Train Epoch: 3720 [65792/118836 (55%)] Loss: 12229.494141\n",
      "Train Epoch: 3720 [98560/118836 (83%)] Loss: 12214.705078\n",
      "    epoch          : 3720\n",
      "    loss           : 12218.4213228262\n",
      "    val_loss       : 12212.304413437472\n",
      "    val_log_likelihood: -12140.034413287065\n",
      "    val_log_marginal: -12147.924638423985\n",
      "Train Epoch: 3721 [256/118836 (0%)] Loss: 12182.737305\n",
      "Train Epoch: 3721 [33024/118836 (28%)] Loss: 12228.107422\n",
      "Train Epoch: 3721 [65792/118836 (55%)] Loss: 12199.975586\n",
      "Train Epoch: 3721 [98560/118836 (83%)] Loss: 12262.500000\n",
      "    epoch          : 3721\n",
      "    loss           : 12218.832541259564\n",
      "    val_loss       : 12216.86148938983\n",
      "    val_log_likelihood: -12139.432637865748\n",
      "    val_log_marginal: -12147.357017320435\n",
      "Train Epoch: 3722 [256/118836 (0%)] Loss: 12209.019531\n",
      "Train Epoch: 3722 [33024/118836 (28%)] Loss: 12216.142578\n",
      "Train Epoch: 3722 [65792/118836 (55%)] Loss: 12262.611328\n",
      "Train Epoch: 3722 [98560/118836 (83%)] Loss: 12195.845703\n",
      "    epoch          : 3722\n",
      "    loss           : 12224.564280913979\n",
      "    val_loss       : 12216.958502248557\n",
      "    val_log_likelihood: -12139.885670944479\n",
      "    val_log_marginal: -12147.804958987423\n",
      "Train Epoch: 3723 [256/118836 (0%)] Loss: 12267.412109\n",
      "Train Epoch: 3723 [33024/118836 (28%)] Loss: 12260.263672\n",
      "Train Epoch: 3723 [65792/118836 (55%)] Loss: 12184.261719\n",
      "Train Epoch: 3723 [98560/118836 (83%)] Loss: 12185.478516\n",
      "    epoch          : 3723\n",
      "    loss           : 12219.547049957351\n",
      "    val_loss       : 12213.906567673657\n",
      "    val_log_likelihood: -12140.747618286032\n",
      "    val_log_marginal: -12148.664404069417\n",
      "Train Epoch: 3724 [256/118836 (0%)] Loss: 12205.536133\n",
      "Train Epoch: 3724 [33024/118836 (28%)] Loss: 12135.158203\n",
      "Train Epoch: 3724 [65792/118836 (55%)] Loss: 12207.058594\n",
      "Train Epoch: 3724 [98560/118836 (83%)] Loss: 12220.407227\n",
      "    epoch          : 3724\n",
      "    loss           : 12216.244622201975\n",
      "    val_loss       : 12214.131375759898\n",
      "    val_log_likelihood: -12140.601498849772\n",
      "    val_log_marginal: -12148.523208325792\n",
      "Train Epoch: 3725 [256/118836 (0%)] Loss: 12328.206055\n",
      "Train Epoch: 3725 [33024/118836 (28%)] Loss: 12183.723633\n",
      "Train Epoch: 3725 [65792/118836 (55%)] Loss: 12206.761719\n",
      "Train Epoch: 3725 [98560/118836 (83%)] Loss: 12310.862305\n",
      "    epoch          : 3725\n",
      "    loss           : 12212.848469002016\n",
      "    val_loss       : 12213.986162876847\n",
      "    val_log_likelihood: -12140.112628269748\n",
      "    val_log_marginal: -12148.02953485799\n",
      "Train Epoch: 3726 [256/118836 (0%)] Loss: 12283.338867\n",
      "Train Epoch: 3726 [33024/118836 (28%)] Loss: 12253.930664\n",
      "Train Epoch: 3726 [65792/118836 (55%)] Loss: 12194.922852\n",
      "Train Epoch: 3726 [98560/118836 (83%)] Loss: 12255.213867\n",
      "    epoch          : 3726\n",
      "    loss           : 12219.21014138751\n",
      "    val_loss       : 12217.761066947069\n",
      "    val_log_likelihood: -12140.285010209884\n",
      "    val_log_marginal: -12148.188445981932\n",
      "Train Epoch: 3727 [256/118836 (0%)] Loss: 12186.815430\n",
      "Train Epoch: 3727 [33024/118836 (28%)] Loss: 12151.919922\n",
      "Train Epoch: 3727 [65792/118836 (55%)] Loss: 12179.533203\n",
      "Train Epoch: 3727 [98560/118836 (83%)] Loss: 12294.494141\n",
      "    epoch          : 3727\n",
      "    loss           : 12220.148239602719\n",
      "    val_loss       : 12214.942338007193\n",
      "    val_log_likelihood: -12140.017242749691\n",
      "    val_log_marginal: -12147.917140350522\n",
      "Train Epoch: 3728 [256/118836 (0%)] Loss: 12262.996094\n",
      "Train Epoch: 3728 [33024/118836 (28%)] Loss: 12234.407227\n",
      "Train Epoch: 3728 [65792/118836 (55%)] Loss: 12192.619141\n",
      "Train Epoch: 3728 [98560/118836 (83%)] Loss: 12265.172852\n",
      "    epoch          : 3728\n",
      "    loss           : 12217.890759570151\n",
      "    val_loss       : 12221.204143408251\n",
      "    val_log_likelihood: -12139.280568102513\n",
      "    val_log_marginal: -12147.201952595415\n",
      "Train Epoch: 3729 [256/118836 (0%)] Loss: 12241.239258\n",
      "Train Epoch: 3729 [33024/118836 (28%)] Loss: 12202.641602\n",
      "Train Epoch: 3729 [65792/118836 (55%)] Loss: 12218.585938\n",
      "Train Epoch: 3729 [98560/118836 (83%)] Loss: 12256.471680\n",
      "    epoch          : 3729\n",
      "    loss           : 12222.016192682486\n",
      "    val_loss       : 12214.986209759563\n",
      "    val_log_likelihood: -12139.481292325785\n",
      "    val_log_marginal: -12147.393746467502\n",
      "Train Epoch: 3730 [256/118836 (0%)] Loss: 12249.078125\n",
      "Train Epoch: 3730 [33024/118836 (28%)] Loss: 12241.859375\n",
      "Train Epoch: 3730 [65792/118836 (55%)] Loss: 12197.451172\n",
      "Train Epoch: 3730 [98560/118836 (83%)] Loss: 12162.861328\n",
      "    epoch          : 3730\n",
      "    loss           : 12216.265367006565\n",
      "    val_loss       : 12220.895356595413\n",
      "    val_log_likelihood: -12140.766451645213\n",
      "    val_log_marginal: -12148.672763247083\n",
      "Train Epoch: 3731 [256/118836 (0%)] Loss: 12237.587891\n",
      "Train Epoch: 3731 [33024/118836 (28%)] Loss: 12250.301758\n",
      "Train Epoch: 3731 [65792/118836 (55%)] Loss: 12263.808594\n",
      "Train Epoch: 3731 [98560/118836 (83%)] Loss: 12250.174805\n",
      "    epoch          : 3731\n",
      "    loss           : 12216.079695092794\n",
      "    val_loss       : 12216.636340249037\n",
      "    val_log_likelihood: -12141.063000155085\n",
      "    val_log_marginal: -12148.960686529934\n",
      "Train Epoch: 3732 [256/118836 (0%)] Loss: 12213.788086\n",
      "Train Epoch: 3732 [33024/118836 (28%)] Loss: 12248.983398\n",
      "Train Epoch: 3732 [65792/118836 (55%)] Loss: 12221.027344\n",
      "Train Epoch: 3732 [98560/118836 (83%)] Loss: 12315.794922\n",
      "    epoch          : 3732\n",
      "    loss           : 12220.336461887406\n",
      "    val_loss       : 12216.089833283968\n",
      "    val_log_likelihood: -12140.596664178816\n",
      "    val_log_marginal: -12148.486987018041\n",
      "Train Epoch: 3733 [256/118836 (0%)] Loss: 12300.679688\n",
      "Train Epoch: 3733 [33024/118836 (28%)] Loss: 12170.285156\n",
      "Train Epoch: 3733 [65792/118836 (55%)] Loss: 12227.564453\n",
      "Train Epoch: 3733 [98560/118836 (83%)] Loss: 12200.505859\n",
      "    epoch          : 3733\n",
      "    loss           : 12218.773807446752\n",
      "    val_loss       : 12218.569187316918\n",
      "    val_log_likelihood: -12139.70627568626\n",
      "    val_log_marginal: -12147.60148626075\n",
      "Train Epoch: 3734 [256/118836 (0%)] Loss: 12255.093750\n",
      "Train Epoch: 3734 [33024/118836 (28%)] Loss: 12187.492188\n",
      "Train Epoch: 3734 [65792/118836 (55%)] Loss: 12242.908203\n",
      "Train Epoch: 3734 [98560/118836 (83%)] Loss: 12226.934570\n",
      "    epoch          : 3734\n",
      "    loss           : 12220.513414689309\n",
      "    val_loss       : 12221.523436999545\n",
      "    val_log_likelihood: -12142.092627397384\n",
      "    val_log_marginal: -12149.991134314938\n",
      "Train Epoch: 3735 [256/118836 (0%)] Loss: 12227.255859\n",
      "Train Epoch: 3735 [33024/118836 (28%)] Loss: 12356.309570\n",
      "Train Epoch: 3735 [65792/118836 (55%)] Loss: 12188.498047\n",
      "Train Epoch: 3735 [98560/118836 (83%)] Loss: 12344.581055\n",
      "    epoch          : 3735\n",
      "    loss           : 12219.743253398987\n",
      "    val_loss       : 12214.417065846668\n",
      "    val_log_likelihood: -12138.835820054022\n",
      "    val_log_marginal: -12146.741450841771\n",
      "Train Epoch: 3736 [256/118836 (0%)] Loss: 12273.572266\n",
      "Train Epoch: 3736 [33024/118836 (28%)] Loss: 12256.898438\n",
      "Train Epoch: 3736 [65792/118836 (55%)] Loss: 12281.550781\n",
      "Train Epoch: 3736 [98560/118836 (83%)] Loss: 12155.499023\n",
      "    epoch          : 3736\n",
      "    loss           : 12218.76454860034\n",
      "    val_loss       : 12218.010576119414\n",
      "    val_log_likelihood: -12139.92169164211\n",
      "    val_log_marginal: -12147.853060170319\n",
      "Train Epoch: 3737 [256/118836 (0%)] Loss: 12235.763672\n",
      "Train Epoch: 3737 [33024/118836 (28%)] Loss: 12256.661133\n",
      "Train Epoch: 3737 [65792/118836 (55%)] Loss: 12206.616211\n",
      "Train Epoch: 3737 [98560/118836 (83%)] Loss: 12180.939453\n",
      "    epoch          : 3737\n",
      "    loss           : 12214.190709328474\n",
      "    val_loss       : 12220.972108932156\n",
      "    val_log_likelihood: -12140.926684630893\n",
      "    val_log_marginal: -12148.84537046008\n",
      "Train Epoch: 3738 [256/118836 (0%)] Loss: 12219.482422\n",
      "Train Epoch: 3738 [33024/118836 (28%)] Loss: 12190.442383\n",
      "Train Epoch: 3738 [65792/118836 (55%)] Loss: 12275.066406\n",
      "Train Epoch: 3738 [98560/118836 (83%)] Loss: 12228.139648\n",
      "    epoch          : 3738\n",
      "    loss           : 12219.011462210505\n",
      "    val_loss       : 12221.324707329892\n",
      "    val_log_likelihood: -12140.96070600057\n",
      "    val_log_marginal: -12148.906832772742\n",
      "Train Epoch: 3739 [256/118836 (0%)] Loss: 12200.416992\n",
      "Train Epoch: 3739 [33024/118836 (28%)] Loss: 12247.524414\n",
      "Train Epoch: 3739 [65792/118836 (55%)] Loss: 12172.916016\n",
      "Train Epoch: 3739 [98560/118836 (83%)] Loss: 12214.019531\n",
      "    epoch          : 3739\n",
      "    loss           : 12219.141917390405\n",
      "    val_loss       : 12214.735243393612\n",
      "    val_log_likelihood: -12140.147694213969\n",
      "    val_log_marginal: -12148.095710951358\n",
      "Train Epoch: 3740 [256/118836 (0%)] Loss: 12193.676758\n",
      "Train Epoch: 3740 [33024/118836 (28%)] Loss: 12237.542969\n",
      "Train Epoch: 3740 [65792/118836 (55%)] Loss: 12339.599609\n",
      "Train Epoch: 3740 [98560/118836 (83%)] Loss: 12313.216797\n",
      "    epoch          : 3740\n",
      "    loss           : 12216.31296736068\n",
      "    val_loss       : 12218.264761050306\n",
      "    val_log_likelihood: -12138.899285469655\n",
      "    val_log_marginal: -12146.819077254311\n",
      "Train Epoch: 3741 [256/118836 (0%)] Loss: 12191.705078\n",
      "Train Epoch: 3741 [33024/118836 (28%)] Loss: 12241.466797\n",
      "Train Epoch: 3741 [65792/118836 (55%)] Loss: 12225.504883\n",
      "Train Epoch: 3741 [98560/118836 (83%)] Loss: 12282.991211\n",
      "    epoch          : 3741\n",
      "    loss           : 12216.801667183623\n",
      "    val_loss       : 12220.368157174298\n",
      "    val_log_likelihood: -12140.727063785927\n",
      "    val_log_marginal: -12148.636384342428\n",
      "Train Epoch: 3742 [256/118836 (0%)] Loss: 12141.628906\n",
      "Train Epoch: 3742 [33024/118836 (28%)] Loss: 12343.034180\n",
      "Train Epoch: 3742 [65792/118836 (55%)] Loss: 12363.347656\n",
      "Train Epoch: 3742 [98560/118836 (83%)] Loss: 12228.696289\n",
      "    epoch          : 3742\n",
      "    loss           : 12218.427838897074\n",
      "    val_loss       : 12219.038024425538\n",
      "    val_log_likelihood: -12139.767029505274\n",
      "    val_log_marginal: -12147.662894379622\n",
      "Train Epoch: 3743 [256/118836 (0%)] Loss: 12278.785156\n",
      "Train Epoch: 3743 [33024/118836 (28%)] Loss: 12290.945312\n",
      "Train Epoch: 3743 [65792/118836 (55%)] Loss: 12223.285156\n",
      "Train Epoch: 3743 [98560/118836 (83%)] Loss: 12287.556641\n",
      "    epoch          : 3743\n",
      "    loss           : 12217.592185722962\n",
      "    val_loss       : 12220.751576226407\n",
      "    val_log_likelihood: -12138.768167616574\n",
      "    val_log_marginal: -12146.67764239264\n",
      "Train Epoch: 3744 [256/118836 (0%)] Loss: 12265.830078\n",
      "Train Epoch: 3744 [33024/118836 (28%)] Loss: 12219.612305\n",
      "Train Epoch: 3744 [65792/118836 (55%)] Loss: 12237.796875\n",
      "Train Epoch: 3744 [98560/118836 (83%)] Loss: 12248.833984\n",
      "    epoch          : 3744\n",
      "    loss           : 12218.452497221362\n",
      "    val_loss       : 12216.76150108307\n",
      "    val_log_likelihood: -12140.942061330386\n",
      "    val_log_marginal: -12148.850477202082\n",
      "Train Epoch: 3745 [256/118836 (0%)] Loss: 12245.668945\n",
      "Train Epoch: 3745 [33024/118836 (28%)] Loss: 12181.329102\n",
      "Train Epoch: 3745 [65792/118836 (55%)] Loss: 12179.621094\n",
      "Train Epoch: 3745 [98560/118836 (83%)] Loss: 12215.557617\n",
      "    epoch          : 3745\n",
      "    loss           : 12219.85536907439\n",
      "    val_loss       : 12213.276261476844\n",
      "    val_log_likelihood: -12139.58896298594\n",
      "    val_log_marginal: -12147.50107788895\n",
      "Train Epoch: 3746 [256/118836 (0%)] Loss: 12146.717773\n",
      "Train Epoch: 3746 [33024/118836 (28%)] Loss: 12235.777344\n",
      "Train Epoch: 3746 [65792/118836 (55%)] Loss: 12285.796875\n",
      "Train Epoch: 3746 [98560/118836 (83%)] Loss: 12311.578125\n",
      "    epoch          : 3746\n",
      "    loss           : 12217.333482442877\n",
      "    val_loss       : 12220.440012228768\n",
      "    val_log_likelihood: -12139.036677555056\n",
      "    val_log_marginal: -12146.92825749971\n",
      "Train Epoch: 3747 [256/118836 (0%)] Loss: 12248.636719\n",
      "Train Epoch: 3747 [33024/118836 (28%)] Loss: 12229.033203\n",
      "Train Epoch: 3747 [65792/118836 (55%)] Loss: 12199.091797\n",
      "Train Epoch: 3747 [98560/118836 (83%)] Loss: 12365.784180\n",
      "    epoch          : 3747\n",
      "    loss           : 12220.342210763029\n",
      "    val_loss       : 12217.729266191165\n",
      "    val_log_likelihood: -12138.445444162273\n",
      "    val_log_marginal: -12146.366350496806\n",
      "Train Epoch: 3748 [256/118836 (0%)] Loss: 12225.814453\n",
      "Train Epoch: 3748 [33024/118836 (28%)] Loss: 12188.386719\n",
      "Train Epoch: 3748 [65792/118836 (55%)] Loss: 12182.427734\n",
      "Train Epoch: 3748 [98560/118836 (83%)] Loss: 12213.228516\n",
      "    epoch          : 3748\n",
      "    loss           : 12216.67020603934\n",
      "    val_loss       : 12220.899288739738\n",
      "    val_log_likelihood: -12140.995472110215\n",
      "    val_log_marginal: -12148.911090872049\n",
      "Train Epoch: 3749 [256/118836 (0%)] Loss: 12349.106445\n",
      "Train Epoch: 3749 [33024/118836 (28%)] Loss: 12213.470703\n",
      "Train Epoch: 3749 [65792/118836 (55%)] Loss: 12240.233398\n",
      "Train Epoch: 3749 [98560/118836 (83%)] Loss: 12227.986328\n",
      "    epoch          : 3749\n",
      "    loss           : 12218.334401332455\n",
      "    val_loss       : 12218.287954087722\n",
      "    val_log_likelihood: -12140.759782910722\n",
      "    val_log_marginal: -12148.681739118561\n",
      "Train Epoch: 3750 [256/118836 (0%)] Loss: 12192.776367\n",
      "Train Epoch: 3750 [33024/118836 (28%)] Loss: 12203.384766\n",
      "Train Epoch: 3750 [65792/118836 (55%)] Loss: 12174.243164\n",
      "Train Epoch: 3750 [98560/118836 (83%)] Loss: 12329.215820\n",
      "    epoch          : 3750\n",
      "    loss           : 12217.183017020781\n",
      "    val_loss       : 12219.800319188607\n",
      "    val_log_likelihood: -12138.883935748812\n",
      "    val_log_marginal: -12146.801570961483\n",
      "Train Epoch: 3751 [256/118836 (0%)] Loss: 12198.491211\n",
      "Train Epoch: 3751 [33024/118836 (28%)] Loss: 12230.751953\n",
      "Train Epoch: 3751 [65792/118836 (55%)] Loss: 12198.692383\n",
      "Train Epoch: 3751 [98560/118836 (83%)] Loss: 12153.763672\n",
      "    epoch          : 3751\n",
      "    loss           : 12216.330819957093\n",
      "    val_loss       : 12220.733913605103\n",
      "    val_log_likelihood: -12139.730096056917\n",
      "    val_log_marginal: -12147.639798758131\n",
      "Train Epoch: 3752 [256/118836 (0%)] Loss: 12246.095703\n",
      "Train Epoch: 3752 [33024/118836 (28%)] Loss: 12203.873047\n",
      "Train Epoch: 3752 [65792/118836 (55%)] Loss: 12219.255859\n",
      "Train Epoch: 3752 [98560/118836 (83%)] Loss: 12224.188477\n",
      "    epoch          : 3752\n",
      "    loss           : 12217.167884259978\n",
      "    val_loss       : 12219.350636591555\n",
      "    val_log_likelihood: -12139.45698827802\n",
      "    val_log_marginal: -12147.370053926574\n",
      "Train Epoch: 3753 [256/118836 (0%)] Loss: 12156.529297\n",
      "Train Epoch: 3753 [33024/118836 (28%)] Loss: 12174.219727\n",
      "Train Epoch: 3753 [65792/118836 (55%)] Loss: 12218.572266\n",
      "Train Epoch: 3753 [98560/118836 (83%)] Loss: 12222.489258\n",
      "    epoch          : 3753\n",
      "    loss           : 12219.176525666873\n",
      "    val_loss       : 12221.359174350873\n",
      "    val_log_likelihood: -12140.45862331343\n",
      "    val_log_marginal: -12148.37833256959\n",
      "Train Epoch: 3754 [256/118836 (0%)] Loss: 12273.035156\n",
      "Train Epoch: 3754 [33024/118836 (28%)] Loss: 12219.052734\n",
      "Train Epoch: 3754 [65792/118836 (55%)] Loss: 12257.232422\n",
      "Train Epoch: 3754 [98560/118836 (83%)] Loss: 12279.538086\n",
      "    epoch          : 3754\n",
      "    loss           : 12216.187484491316\n",
      "    val_loss       : 12217.469674528658\n",
      "    val_log_likelihood: -12139.940946159015\n",
      "    val_log_marginal: -12147.871394958465\n",
      "Train Epoch: 3755 [256/118836 (0%)] Loss: 12161.571289\n",
      "Train Epoch: 3755 [33024/118836 (28%)] Loss: 12251.061523\n",
      "Train Epoch: 3755 [65792/118836 (55%)] Loss: 12234.346680\n",
      "Train Epoch: 3755 [98560/118836 (83%)] Loss: 12220.710938\n",
      "    epoch          : 3755\n",
      "    loss           : 12220.727796894385\n",
      "    val_loss       : 12218.685596475552\n",
      "    val_log_likelihood: -12139.451297721514\n",
      "    val_log_marginal: -12147.371701376507\n",
      "Train Epoch: 3756 [256/118836 (0%)] Loss: 12203.285156\n",
      "Train Epoch: 3756 [33024/118836 (28%)] Loss: 12327.833008\n",
      "Train Epoch: 3756 [65792/118836 (55%)] Loss: 12244.234375\n",
      "Train Epoch: 3756 [98560/118836 (83%)] Loss: 12161.927734\n",
      "    epoch          : 3756\n",
      "    loss           : 12218.165788164288\n",
      "    val_loss       : 12216.118057214175\n",
      "    val_log_likelihood: -12139.692988943601\n",
      "    val_log_marginal: -12147.602090377868\n",
      "Train Epoch: 3757 [256/118836 (0%)] Loss: 12229.048828\n",
      "Train Epoch: 3757 [33024/118836 (28%)] Loss: 12226.427734\n",
      "Train Epoch: 3757 [65792/118836 (55%)] Loss: 12455.644531\n",
      "Train Epoch: 3757 [98560/118836 (83%)] Loss: 12234.892578\n",
      "    epoch          : 3757\n",
      "    loss           : 12221.251074622622\n",
      "    val_loss       : 12212.437063903602\n",
      "    val_log_likelihood: -12139.93129717225\n",
      "    val_log_marginal: -12147.828772006342\n",
      "Train Epoch: 3758 [256/118836 (0%)] Loss: 12212.151367\n",
      "Train Epoch: 3758 [33024/118836 (28%)] Loss: 12183.515625\n",
      "Train Epoch: 3758 [65792/118836 (55%)] Loss: 12188.247070\n",
      "Train Epoch: 3758 [98560/118836 (83%)] Loss: 12152.464844\n",
      "    epoch          : 3758\n",
      "    loss           : 12214.851629219655\n",
      "    val_loss       : 12218.334462845642\n",
      "    val_log_likelihood: -12140.299921164185\n",
      "    val_log_marginal: -12148.235306484525\n",
      "Train Epoch: 3759 [256/118836 (0%)] Loss: 12225.950195\n",
      "Train Epoch: 3759 [33024/118836 (28%)] Loss: 12230.244141\n",
      "Train Epoch: 3759 [65792/118836 (55%)] Loss: 12187.595703\n",
      "Train Epoch: 3759 [98560/118836 (83%)] Loss: 12155.127930\n",
      "    epoch          : 3759\n",
      "    loss           : 12214.890881216397\n",
      "    val_loss       : 12216.973738000876\n",
      "    val_log_likelihood: -12138.991194944168\n",
      "    val_log_marginal: -12146.920356444365\n",
      "Train Epoch: 3760 [256/118836 (0%)] Loss: 12238.292969\n",
      "Train Epoch: 3760 [33024/118836 (28%)] Loss: 12256.418945\n",
      "Train Epoch: 3760 [65792/118836 (55%)] Loss: 12192.451172\n",
      "Train Epoch: 3760 [98560/118836 (83%)] Loss: 12203.079102\n",
      "    epoch          : 3760\n",
      "    loss           : 12223.270683900693\n",
      "    val_loss       : 12219.427605277493\n",
      "    val_log_likelihood: -12140.776294652089\n",
      "    val_log_marginal: -12148.707459493131\n",
      "Train Epoch: 3761 [256/118836 (0%)] Loss: 12256.923828\n",
      "Train Epoch: 3761 [33024/118836 (28%)] Loss: 12270.972656\n",
      "Train Epoch: 3761 [65792/118836 (55%)] Loss: 12201.876953\n",
      "Train Epoch: 3761 [98560/118836 (83%)] Loss: 12311.668945\n",
      "    epoch          : 3761\n",
      "    loss           : 12218.370030597343\n",
      "    val_loss       : 12219.449230638067\n",
      "    val_log_likelihood: -12138.98213496433\n",
      "    val_log_marginal: -12146.900326310948\n",
      "Train Epoch: 3762 [256/118836 (0%)] Loss: 12249.776367\n",
      "Train Epoch: 3762 [33024/118836 (28%)] Loss: 12187.076172\n",
      "Train Epoch: 3762 [65792/118836 (55%)] Loss: 12174.309570\n",
      "Train Epoch: 3762 [98560/118836 (83%)] Loss: 12170.390625\n",
      "    epoch          : 3762\n",
      "    loss           : 12215.293857429951\n",
      "    val_loss       : 12222.394291414776\n",
      "    val_log_likelihood: -12139.253078473947\n",
      "    val_log_marginal: -12147.169418894593\n",
      "Train Epoch: 3763 [256/118836 (0%)] Loss: 12307.763672\n",
      "Train Epoch: 3763 [33024/118836 (28%)] Loss: 12392.713867\n",
      "Train Epoch: 3763 [65792/118836 (55%)] Loss: 12192.181641\n",
      "Train Epoch: 3763 [98560/118836 (83%)] Loss: 12211.662109\n",
      "    epoch          : 3763\n",
      "    loss           : 12221.78621633323\n",
      "    val_loss       : 12217.365435792772\n",
      "    val_log_likelihood: -12137.940205780862\n",
      "    val_log_marginal: -12145.84766972223\n",
      "Train Epoch: 3764 [256/118836 (0%)] Loss: 12202.678711\n",
      "Train Epoch: 3764 [33024/118836 (28%)] Loss: 12240.054688\n",
      "Train Epoch: 3764 [65792/118836 (55%)] Loss: 12186.713867\n",
      "Train Epoch: 3764 [98560/118836 (83%)] Loss: 12215.669922\n",
      "    epoch          : 3764\n",
      "    loss           : 12218.604955509461\n",
      "    val_loss       : 12215.106355502741\n",
      "    val_log_likelihood: -12139.001428576044\n",
      "    val_log_marginal: -12146.900117377072\n",
      "Train Epoch: 3765 [256/118836 (0%)] Loss: 12184.868164\n",
      "Train Epoch: 3765 [33024/118836 (28%)] Loss: 12326.267578\n",
      "Train Epoch: 3765 [65792/118836 (55%)] Loss: 12333.595703\n",
      "Train Epoch: 3765 [98560/118836 (83%)] Loss: 12257.066406\n",
      "    epoch          : 3765\n",
      "    loss           : 12221.857204430315\n",
      "    val_loss       : 12218.706666965138\n",
      "    val_log_likelihood: -12140.182107501034\n",
      "    val_log_marginal: -12148.076606837323\n",
      "Train Epoch: 3766 [256/118836 (0%)] Loss: 12170.943359\n",
      "Train Epoch: 3766 [33024/118836 (28%)] Loss: 12149.057617\n",
      "Train Epoch: 3766 [65792/118836 (55%)] Loss: 12245.746094\n",
      "Train Epoch: 3766 [98560/118836 (83%)] Loss: 12191.287109\n",
      "    epoch          : 3766\n",
      "    loss           : 12214.186918262769\n",
      "    val_loss       : 12221.43669295581\n",
      "    val_log_likelihood: -12139.73002287531\n",
      "    val_log_marginal: -12147.628344605018\n",
      "Train Epoch: 3767 [256/118836 (0%)] Loss: 12201.250000\n",
      "Train Epoch: 3767 [33024/118836 (28%)] Loss: 12198.637695\n",
      "Train Epoch: 3767 [65792/118836 (55%)] Loss: 12340.777344\n",
      "Train Epoch: 3767 [98560/118836 (83%)] Loss: 12211.767578\n",
      "    epoch          : 3767\n",
      "    loss           : 12216.415304648728\n",
      "    val_loss       : 12220.465178723223\n",
      "    val_log_likelihood: -12140.312046370967\n",
      "    val_log_marginal: -12148.18342107852\n",
      "Train Epoch: 3768 [256/118836 (0%)] Loss: 12255.273438\n",
      "Train Epoch: 3768 [33024/118836 (28%)] Loss: 12162.932617\n",
      "Train Epoch: 3768 [65792/118836 (55%)] Loss: 12234.043945\n",
      "Train Epoch: 3768 [98560/118836 (83%)] Loss: 12252.721680\n",
      "    epoch          : 3768\n",
      "    loss           : 12224.271493260183\n",
      "    val_loss       : 12218.870480296397\n",
      "    val_log_likelihood: -12138.53807656767\n",
      "    val_log_marginal: -12146.430220672202\n",
      "Train Epoch: 3769 [256/118836 (0%)] Loss: 12203.402344\n",
      "Train Epoch: 3769 [33024/118836 (28%)] Loss: 12205.556641\n",
      "Train Epoch: 3769 [65792/118836 (55%)] Loss: 12293.221680\n",
      "Train Epoch: 3769 [98560/118836 (83%)] Loss: 12199.916016\n",
      "    epoch          : 3769\n",
      "    loss           : 12213.665717567463\n",
      "    val_loss       : 12218.081242004615\n",
      "    val_log_likelihood: -12139.611436039599\n",
      "    val_log_marginal: -12147.53714648833\n",
      "Train Epoch: 3770 [256/118836 (0%)] Loss: 12185.757812\n",
      "Train Epoch: 3770 [33024/118836 (28%)] Loss: 12261.830078\n",
      "Train Epoch: 3770 [65792/118836 (55%)] Loss: 12199.900391\n",
      "Train Epoch: 3770 [98560/118836 (83%)] Loss: 12165.458984\n",
      "    epoch          : 3770\n",
      "    loss           : 12216.683988252173\n",
      "    val_loss       : 12218.809559224916\n",
      "    val_log_likelihood: -12140.783443509616\n",
      "    val_log_marginal: -12148.697507776395\n",
      "Train Epoch: 3771 [256/118836 (0%)] Loss: 12271.593750\n",
      "Train Epoch: 3771 [33024/118836 (28%)] Loss: 12267.109375\n",
      "Train Epoch: 3771 [65792/118836 (55%)] Loss: 12290.796875\n",
      "Train Epoch: 3771 [98560/118836 (83%)] Loss: 12335.076172\n",
      "    epoch          : 3771\n",
      "    loss           : 12220.461049937965\n",
      "    val_loss       : 12216.082841161\n",
      "    val_log_likelihood: -12139.815293986507\n",
      "    val_log_marginal: -12147.709811684665\n",
      "Train Epoch: 3772 [256/118836 (0%)] Loss: 12188.399414\n",
      "Train Epoch: 3772 [33024/118836 (28%)] Loss: 12308.830078\n",
      "Train Epoch: 3772 [65792/118836 (55%)] Loss: 12239.903320\n",
      "Train Epoch: 3772 [98560/118836 (83%)] Loss: 12251.262695\n",
      "    epoch          : 3772\n",
      "    loss           : 12221.079592993952\n",
      "    val_loss       : 12215.46770603153\n",
      "    val_log_likelihood: -12141.664734381462\n",
      "    val_log_marginal: -12149.564558858843\n",
      "Train Epoch: 3773 [256/118836 (0%)] Loss: 12275.004883\n",
      "Train Epoch: 3773 [33024/118836 (28%)] Loss: 12203.182617\n",
      "Train Epoch: 3773 [65792/118836 (55%)] Loss: 12203.296875\n",
      "Train Epoch: 3773 [98560/118836 (83%)] Loss: 12317.389648\n",
      "    epoch          : 3773\n",
      "    loss           : 12216.444200721155\n",
      "    val_loss       : 12220.24090106544\n",
      "    val_log_likelihood: -12139.987563650227\n",
      "    val_log_marginal: -12147.871997324433\n",
      "Train Epoch: 3774 [256/118836 (0%)] Loss: 12230.214844\n",
      "Train Epoch: 3774 [33024/118836 (28%)] Loss: 12149.798828\n",
      "Train Epoch: 3774 [65792/118836 (55%)] Loss: 12266.832031\n",
      "Train Epoch: 3774 [98560/118836 (83%)] Loss: 12269.776367\n",
      "    epoch          : 3774\n",
      "    loss           : 12216.433187293218\n",
      "    val_loss       : 12216.031609904829\n",
      "    val_log_likelihood: -12140.104214969759\n",
      "    val_log_marginal: -12148.00395347487\n",
      "Train Epoch: 3775 [256/118836 (0%)] Loss: 12162.056641\n",
      "Train Epoch: 3775 [33024/118836 (28%)] Loss: 12324.591797\n",
      "Train Epoch: 3775 [65792/118836 (55%)] Loss: 12277.001953\n",
      "Train Epoch: 3775 [98560/118836 (83%)] Loss: 12325.505859\n",
      "    epoch          : 3775\n",
      "    loss           : 12219.913064936156\n",
      "    val_loss       : 12220.14771293001\n",
      "    val_log_likelihood: -12139.147324590313\n",
      "    val_log_marginal: -12147.042968233865\n",
      "Train Epoch: 3776 [256/118836 (0%)] Loss: 12303.985352\n",
      "Train Epoch: 3776 [33024/118836 (28%)] Loss: 12231.792969\n",
      "Train Epoch: 3776 [65792/118836 (55%)] Loss: 12298.146484\n",
      "Train Epoch: 3776 [98560/118836 (83%)] Loss: 12225.736328\n",
      "    epoch          : 3776\n",
      "    loss           : 12221.173934424112\n",
      "    val_loss       : 12217.424833397301\n",
      "    val_log_likelihood: -12140.582267595895\n",
      "    val_log_marginal: -12148.480040830307\n",
      "Train Epoch: 3777 [256/118836 (0%)] Loss: 12288.400391\n",
      "Train Epoch: 3777 [33024/118836 (28%)] Loss: 12189.496094\n",
      "Train Epoch: 3777 [65792/118836 (55%)] Loss: 12193.227539\n",
      "Train Epoch: 3777 [98560/118836 (83%)] Loss: 12209.995117\n",
      "    epoch          : 3777\n",
      "    loss           : 12220.677469596516\n",
      "    val_loss       : 12214.76412643738\n",
      "    val_log_likelihood: -12141.10062923258\n",
      "    val_log_marginal: -12149.026644796359\n",
      "Train Epoch: 3778 [256/118836 (0%)] Loss: 12258.547852\n",
      "Train Epoch: 3778 [33024/118836 (28%)] Loss: 12294.527344\n",
      "Train Epoch: 3778 [65792/118836 (55%)] Loss: 12191.953125\n",
      "Train Epoch: 3778 [98560/118836 (83%)] Loss: 12168.892578\n",
      "    epoch          : 3778\n",
      "    loss           : 12214.807588270265\n",
      "    val_loss       : 12216.628644861274\n",
      "    val_log_likelihood: -12137.794356292648\n",
      "    val_log_marginal: -12145.704243166345\n",
      "Train Epoch: 3779 [256/118836 (0%)] Loss: 12146.717773\n",
      "Train Epoch: 3779 [33024/118836 (28%)] Loss: 12221.125977\n",
      "Train Epoch: 3779 [65792/118836 (55%)] Loss: 12267.406250\n",
      "Train Epoch: 3779 [98560/118836 (83%)] Loss: 12226.605469\n",
      "    epoch          : 3779\n",
      "    loss           : 12218.529786206318\n",
      "    val_loss       : 12220.70961248338\n",
      "    val_log_likelihood: -12139.952533246744\n",
      "    val_log_marginal: -12147.862967344994\n",
      "Train Epoch: 3780 [256/118836 (0%)] Loss: 12174.137695\n",
      "Train Epoch: 3780 [33024/118836 (28%)] Loss: 12344.171875\n",
      "Train Epoch: 3780 [65792/118836 (55%)] Loss: 12283.406250\n",
      "Train Epoch: 3780 [98560/118836 (83%)] Loss: 12244.553711\n",
      "    epoch          : 3780\n",
      "    loss           : 12216.719777611921\n",
      "    val_loss       : 12216.326216295089\n",
      "    val_log_likelihood: -12139.400660896143\n",
      "    val_log_marginal: -12147.306837319538\n",
      "Train Epoch: 3781 [256/118836 (0%)] Loss: 12225.851562\n",
      "Train Epoch: 3781 [33024/118836 (28%)] Loss: 12262.845703\n",
      "Train Epoch: 3781 [65792/118836 (55%)] Loss: 12229.041016\n",
      "Train Epoch: 3781 [98560/118836 (83%)] Loss: 12169.369141\n",
      "    epoch          : 3781\n",
      "    loss           : 12219.476072522488\n",
      "    val_loss       : 12217.75321501539\n",
      "    val_log_likelihood: -12139.456570997469\n",
      "    val_log_marginal: -12147.372178830892\n",
      "Train Epoch: 3782 [256/118836 (0%)] Loss: 12235.797852\n",
      "Train Epoch: 3782 [33024/118836 (28%)] Loss: 12251.254883\n",
      "Train Epoch: 3782 [65792/118836 (55%)] Loss: 12298.945312\n",
      "Train Epoch: 3782 [98560/118836 (83%)] Loss: 12184.266602\n",
      "    epoch          : 3782\n",
      "    loss           : 12222.672585168528\n",
      "    val_loss       : 12220.580532982462\n",
      "    val_log_likelihood: -12138.91636053169\n",
      "    val_log_marginal: -12146.817600640254\n",
      "Train Epoch: 3783 [256/118836 (0%)] Loss: 12201.429688\n",
      "Train Epoch: 3783 [33024/118836 (28%)] Loss: 12220.989258\n",
      "Train Epoch: 3783 [65792/118836 (55%)] Loss: 12284.574219\n",
      "Train Epoch: 3783 [98560/118836 (83%)] Loss: 12240.019531\n",
      "    epoch          : 3783\n",
      "    loss           : 12211.58555624483\n",
      "    val_loss       : 12217.747111244387\n",
      "    val_log_likelihood: -12139.975016801076\n",
      "    val_log_marginal: -12147.889315628649\n",
      "Train Epoch: 3784 [256/118836 (0%)] Loss: 12286.011719\n",
      "Train Epoch: 3784 [33024/118836 (28%)] Loss: 12250.492188\n",
      "Train Epoch: 3784 [65792/118836 (55%)] Loss: 12260.613281\n",
      "Train Epoch: 3784 [98560/118836 (83%)] Loss: 12176.585938\n",
      "    epoch          : 3784\n",
      "    loss           : 12216.113663636012\n",
      "    val_loss       : 12215.224500458338\n",
      "    val_log_likelihood: -12139.615750846515\n",
      "    val_log_marginal: -12147.517551872932\n",
      "Train Epoch: 3785 [256/118836 (0%)] Loss: 12178.605469\n",
      "Train Epoch: 3785 [33024/118836 (28%)] Loss: 12221.204102\n",
      "Train Epoch: 3785 [65792/118836 (55%)] Loss: 12328.246094\n",
      "Train Epoch: 3785 [98560/118836 (83%)] Loss: 12257.281250\n",
      "    epoch          : 3785\n",
      "    loss           : 12216.67919736094\n",
      "    val_loss       : 12213.101083476025\n",
      "    val_log_likelihood: -12140.38374883685\n",
      "    val_log_marginal: -12148.296225772208\n",
      "Train Epoch: 3786 [256/118836 (0%)] Loss: 12207.349609\n",
      "Train Epoch: 3786 [33024/118836 (28%)] Loss: 12305.167969\n",
      "Train Epoch: 3786 [65792/118836 (55%)] Loss: 12264.599609\n",
      "Train Epoch: 3786 [98560/118836 (83%)] Loss: 12143.083008\n",
      "    epoch          : 3786\n",
      "    loss           : 12216.072066758426\n",
      "    val_loss       : 12217.556241562945\n",
      "    val_log_likelihood: -12137.186438624381\n",
      "    val_log_marginal: -12145.095665859437\n",
      "Train Epoch: 3787 [256/118836 (0%)] Loss: 12301.785156\n",
      "Train Epoch: 3787 [33024/118836 (28%)] Loss: 12269.861328\n",
      "Train Epoch: 3787 [65792/118836 (55%)] Loss: 12175.155273\n",
      "Train Epoch: 3787 [98560/118836 (83%)] Loss: 12286.962891\n",
      "    epoch          : 3787\n",
      "    loss           : 12214.432305075215\n",
      "    val_loss       : 12223.79799438761\n",
      "    val_log_likelihood: -12138.341320790685\n",
      "    val_log_marginal: -12146.261763567842\n",
      "Train Epoch: 3788 [256/118836 (0%)] Loss: 12303.439453\n",
      "Train Epoch: 3788 [33024/118836 (28%)] Loss: 12252.537109\n",
      "Train Epoch: 3788 [65792/118836 (55%)] Loss: 12235.591797\n",
      "Train Epoch: 3788 [98560/118836 (83%)] Loss: 12171.595703\n",
      "    epoch          : 3788\n",
      "    loss           : 12213.159154970534\n",
      "    val_loss       : 12217.948649147898\n",
      "    val_log_likelihood: -12140.109303272333\n",
      "    val_log_marginal: -12148.030199875435\n",
      "Train Epoch: 3789 [256/118836 (0%)] Loss: 12165.867188\n",
      "Train Epoch: 3789 [33024/118836 (28%)] Loss: 12319.295898\n",
      "Train Epoch: 3789 [65792/118836 (55%)] Loss: 12236.387695\n",
      "Train Epoch: 3789 [98560/118836 (83%)] Loss: 12256.791016\n",
      "    epoch          : 3789\n",
      "    loss           : 12214.704680553401\n",
      "    val_loss       : 12213.414780772862\n",
      "    val_log_likelihood: -12138.927800610008\n",
      "    val_log_marginal: -12146.829122868527\n",
      "Train Epoch: 3790 [256/118836 (0%)] Loss: 12298.529297\n",
      "Train Epoch: 3790 [33024/118836 (28%)] Loss: 12270.456055\n",
      "Train Epoch: 3790 [65792/118836 (55%)] Loss: 12213.906250\n",
      "Train Epoch: 3790 [98560/118836 (83%)] Loss: 12256.078125\n",
      "    epoch          : 3790\n",
      "    loss           : 12213.628660372724\n",
      "    val_loss       : 12216.50683049785\n",
      "    val_log_likelihood: -12141.687165755533\n",
      "    val_log_marginal: -12149.593477975395\n",
      "Train Epoch: 3791 [256/118836 (0%)] Loss: 12260.048828\n",
      "Train Epoch: 3791 [33024/118836 (28%)] Loss: 12166.451172\n",
      "Train Epoch: 3791 [65792/118836 (55%)] Loss: 12285.000977\n",
      "Train Epoch: 3791 [98560/118836 (83%)] Loss: 12232.251953\n",
      "    epoch          : 3791\n",
      "    loss           : 12214.225265424679\n",
      "    val_loss       : 12217.039695344683\n",
      "    val_log_likelihood: -12139.164115649555\n",
      "    val_log_marginal: -12147.075176402746\n",
      "Train Epoch: 3792 [256/118836 (0%)] Loss: 12230.820312\n",
      "Train Epoch: 3792 [33024/118836 (28%)] Loss: 12141.531250\n",
      "Train Epoch: 3792 [65792/118836 (55%)] Loss: 12280.931641\n",
      "Train Epoch: 3792 [98560/118836 (83%)] Loss: 12239.604492\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 03792: reducing learning rate of group 0 to 1.0000e-05.\n",
      "    epoch          : 3792\n",
      "    loss           : 12217.476643759046\n",
      "    val_loss       : 12219.408331138078\n",
      "    val_log_likelihood: -12139.231428672974\n",
      "    val_log_marginal: -12147.147373926433\n",
      "Train Epoch: 3793 [256/118836 (0%)] Loss: 12215.517578\n",
      "Train Epoch: 3793 [33024/118836 (28%)] Loss: 12272.503906\n",
      "Train Epoch: 3793 [65792/118836 (55%)] Loss: 12233.101562\n",
      "Train Epoch: 3793 [98560/118836 (83%)] Loss: 12159.418945\n",
      "    epoch          : 3793\n",
      "    loss           : 12221.721345281483\n",
      "    val_loss       : 12214.349407485091\n",
      "    val_log_likelihood: -12140.477277514992\n",
      "    val_log_marginal: -12148.384094595342\n",
      "Train Epoch: 3794 [256/118836 (0%)] Loss: 12304.519531\n",
      "Train Epoch: 3794 [33024/118836 (28%)] Loss: 12253.839844\n",
      "Train Epoch: 3794 [65792/118836 (55%)] Loss: 12188.716797\n",
      "Train Epoch: 3794 [98560/118836 (83%)] Loss: 12169.185547\n",
      "    epoch          : 3794\n",
      "    loss           : 12218.249759776934\n",
      "    val_loss       : 12219.106528999358\n",
      "    val_log_likelihood: -12139.84860050274\n",
      "    val_log_marginal: -12147.75641826939\n",
      "Train Epoch: 3795 [256/118836 (0%)] Loss: 12179.439453\n",
      "Train Epoch: 3795 [33024/118836 (28%)] Loss: 12279.933594\n",
      "Train Epoch: 3795 [65792/118836 (55%)] Loss: 12263.800781\n",
      "Train Epoch: 3795 [98560/118836 (83%)] Loss: 12168.479492\n",
      "    epoch          : 3795\n",
      "    loss           : 12218.273853649709\n",
      "    val_loss       : 12212.726140178049\n",
      "    val_log_likelihood: -12138.49790955852\n",
      "    val_log_marginal: -12146.40568111189\n",
      "Train Epoch: 3796 [256/118836 (0%)] Loss: 12150.498047\n",
      "Train Epoch: 3796 [33024/118836 (28%)] Loss: 12181.641602\n",
      "Train Epoch: 3796 [65792/118836 (55%)] Loss: 12311.947266\n",
      "Train Epoch: 3796 [98560/118836 (83%)] Loss: 12258.053711\n",
      "    epoch          : 3796\n",
      "    loss           : 12220.226580431918\n",
      "    val_loss       : 12215.525619767637\n",
      "    val_log_likelihood: -12139.108393429487\n",
      "    val_log_marginal: -12147.02547909181\n",
      "Train Epoch: 3797 [256/118836 (0%)] Loss: 12236.941406\n",
      "Train Epoch: 3797 [33024/118836 (28%)] Loss: 12354.884766\n",
      "Train Epoch: 3797 [65792/118836 (55%)] Loss: 12290.797852\n",
      "Train Epoch: 3797 [98560/118836 (83%)] Loss: 12251.708984\n",
      "    epoch          : 3797\n",
      "    loss           : 12220.943268099927\n",
      "    val_loss       : 12221.192597173349\n",
      "    val_log_likelihood: -12139.595110725548\n",
      "    val_log_marginal: -12147.502587302628\n",
      "Train Epoch: 3798 [256/118836 (0%)] Loss: 12220.759766\n",
      "Train Epoch: 3798 [33024/118836 (28%)] Loss: 12222.710938\n",
      "Train Epoch: 3798 [65792/118836 (55%)] Loss: 12180.723633\n",
      "Train Epoch: 3798 [98560/118836 (83%)] Loss: 12229.279297\n",
      "    epoch          : 3798\n",
      "    loss           : 12217.759387439257\n",
      "    val_loss       : 12218.531642961665\n",
      "    val_log_likelihood: -12139.55466843724\n",
      "    val_log_marginal: -12147.458040346864\n",
      "Train Epoch: 3799 [256/118836 (0%)] Loss: 12236.528320\n",
      "Train Epoch: 3799 [33024/118836 (28%)] Loss: 12153.828125\n",
      "Train Epoch: 3799 [65792/118836 (55%)] Loss: 12295.617188\n",
      "Train Epoch: 3799 [98560/118836 (83%)] Loss: 12299.872070\n",
      "    epoch          : 3799\n",
      "    loss           : 12214.57079892344\n",
      "    val_loss       : 12212.595094855847\n",
      "    val_log_likelihood: -12139.458095371949\n",
      "    val_log_marginal: -12147.362253539226\n",
      "Train Epoch: 3800 [256/118836 (0%)] Loss: 12186.906250\n",
      "Train Epoch: 3800 [33024/118836 (28%)] Loss: 12224.320312\n",
      "Train Epoch: 3800 [65792/118836 (55%)] Loss: 12276.871094\n",
      "Train Epoch: 3800 [98560/118836 (83%)] Loss: 12263.810547\n",
      "    epoch          : 3800\n",
      "    loss           : 12220.937661225704\n",
      "    val_loss       : 12218.63081350511\n",
      "    val_log_likelihood: -12139.316365055054\n",
      "    val_log_marginal: -12147.22754385024\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch3800.pth ...\n",
      "Train Epoch: 3801 [256/118836 (0%)] Loss: 12159.742188\n",
      "Train Epoch: 3801 [33024/118836 (28%)] Loss: 12229.511719\n",
      "Train Epoch: 3801 [65792/118836 (55%)] Loss: 12232.673828\n",
      "Train Epoch: 3801 [98560/118836 (83%)] Loss: 12357.902344\n",
      "    epoch          : 3801\n",
      "    loss           : 12217.982811853804\n",
      "    val_loss       : 12214.33410665681\n",
      "    val_log_likelihood: -12139.374502752791\n",
      "    val_log_marginal: -12147.280150704839\n",
      "Train Epoch: 3802 [256/118836 (0%)] Loss: 12245.281250\n",
      "Train Epoch: 3802 [33024/118836 (28%)] Loss: 12283.550781\n",
      "Train Epoch: 3802 [65792/118836 (55%)] Loss: 12297.930664\n",
      "Train Epoch: 3802 [98560/118836 (83%)] Loss: 12251.185547\n",
      "    epoch          : 3802\n",
      "    loss           : 12218.13688837624\n",
      "    val_loss       : 12221.436702551118\n",
      "    val_log_likelihood: -12138.49551023573\n",
      "    val_log_marginal: -12146.403986160205\n",
      "Train Epoch: 3803 [256/118836 (0%)] Loss: 12250.038086\n",
      "Train Epoch: 3803 [33024/118836 (28%)] Loss: 12203.375977\n",
      "Train Epoch: 3803 [65792/118836 (55%)] Loss: 12328.510742\n",
      "Train Epoch: 3803 [98560/118836 (83%)] Loss: 12319.685547\n",
      "    epoch          : 3803\n",
      "    loss           : 12215.920544645627\n",
      "    val_loss       : 12217.457696812642\n",
      "    val_log_likelihood: -12141.716303504963\n",
      "    val_log_marginal: -12149.628727533505\n",
      "Train Epoch: 3804 [256/118836 (0%)] Loss: 12262.681641\n",
      "Train Epoch: 3804 [33024/118836 (28%)] Loss: 12216.317383\n",
      "Train Epoch: 3804 [65792/118836 (55%)] Loss: 12182.740234\n",
      "Train Epoch: 3804 [98560/118836 (83%)] Loss: 12279.716797\n",
      "    epoch          : 3804\n",
      "    loss           : 12218.426149258168\n",
      "    val_loss       : 12213.658821933015\n",
      "    val_log_likelihood: -12139.2598397759\n",
      "    val_log_marginal: -12147.167444376291\n",
      "Train Epoch: 3805 [256/118836 (0%)] Loss: 12259.939453\n",
      "Train Epoch: 3805 [33024/118836 (28%)] Loss: 12220.400391\n",
      "Train Epoch: 3805 [65792/118836 (55%)] Loss: 12310.336914\n",
      "Train Epoch: 3805 [98560/118836 (83%)] Loss: 12187.936523\n",
      "    epoch          : 3805\n",
      "    loss           : 12213.430262452182\n",
      "    val_loss       : 12217.773585825398\n",
      "    val_log_likelihood: -12137.3257272927\n",
      "    val_log_marginal: -12145.237958570693\n",
      "Train Epoch: 3806 [256/118836 (0%)] Loss: 12291.330078\n",
      "Train Epoch: 3806 [33024/118836 (28%)] Loss: 12328.348633\n",
      "Train Epoch: 3806 [65792/118836 (55%)] Loss: 12198.086914\n",
      "Train Epoch: 3806 [98560/118836 (83%)] Loss: 12227.049805\n",
      "    epoch          : 3806\n",
      "    loss           : 12214.246283892939\n",
      "    val_loss       : 12216.716899426172\n",
      "    val_log_likelihood: -12141.489536968827\n",
      "    val_log_marginal: -12149.405923979239\n",
      "Train Epoch: 3807 [256/118836 (0%)] Loss: 12208.902344\n",
      "Train Epoch: 3807 [33024/118836 (28%)] Loss: 12273.187500\n",
      "Train Epoch: 3807 [65792/118836 (55%)] Loss: 12232.994141\n",
      "Train Epoch: 3807 [98560/118836 (83%)] Loss: 12169.708008\n",
      "    epoch          : 3807\n",
      "    loss           : 12216.602822580646\n",
      "    val_loss       : 12220.506675179004\n",
      "    val_log_likelihood: -12140.539693832714\n",
      "    val_log_marginal: -12148.451033399588\n",
      "Train Epoch: 3808 [256/118836 (0%)] Loss: 12247.478516\n",
      "Train Epoch: 3808 [33024/118836 (28%)] Loss: 12185.015625\n",
      "Train Epoch: 3808 [65792/118836 (55%)] Loss: 12206.596680\n",
      "Train Epoch: 3808 [98560/118836 (83%)] Loss: 12257.135742\n",
      "    epoch          : 3808\n",
      "    loss           : 12222.891298820048\n",
      "    val_loss       : 12216.389366100202\n",
      "    val_log_likelihood: -12140.70020904415\n",
      "    val_log_marginal: -12148.608168769359\n",
      "Train Epoch: 3809 [256/118836 (0%)] Loss: 12290.575195\n",
      "Train Epoch: 3809 [33024/118836 (28%)] Loss: 12281.789062\n",
      "Train Epoch: 3809 [65792/118836 (55%)] Loss: 12277.660156\n",
      "Train Epoch: 3809 [98560/118836 (83%)] Loss: 12366.787109\n",
      "    epoch          : 3809\n",
      "    loss           : 12217.087887717122\n",
      "    val_loss       : 12217.713428678757\n",
      "    val_log_likelihood: -12138.637013899659\n",
      "    val_log_marginal: -12146.554260283287\n",
      "Train Epoch: 3810 [256/118836 (0%)] Loss: 12228.554688\n",
      "Train Epoch: 3810 [33024/118836 (28%)] Loss: 12356.480469\n",
      "Train Epoch: 3810 [65792/118836 (55%)] Loss: 12153.831055\n",
      "Train Epoch: 3810 [98560/118836 (83%)] Loss: 12264.097656\n",
      "    epoch          : 3810\n",
      "    loss           : 12218.416488478339\n",
      "    val_loss       : 12219.725083363717\n",
      "    val_log_likelihood: -12140.54981551127\n",
      "    val_log_marginal: -12148.461680061295\n",
      "Train Epoch: 3811 [256/118836 (0%)] Loss: 12163.422852\n",
      "Train Epoch: 3811 [33024/118836 (28%)] Loss: 12282.427734\n",
      "Train Epoch: 3811 [65792/118836 (55%)] Loss: 12203.531250\n",
      "Train Epoch: 3811 [98560/118836 (83%)] Loss: 12150.819336\n",
      "    epoch          : 3811\n",
      "    loss           : 12217.928792358096\n",
      "    val_loss       : 12219.262939649725\n",
      "    val_log_likelihood: -12138.692310923283\n",
      "    val_log_marginal: -12146.595025735714\n",
      "Train Epoch: 3812 [256/118836 (0%)] Loss: 12299.392578\n",
      "Train Epoch: 3812 [33024/118836 (28%)] Loss: 12125.077148\n",
      "Train Epoch: 3812 [65792/118836 (55%)] Loss: 12268.457031\n",
      "Train Epoch: 3812 [98560/118836 (83%)] Loss: 12220.871094\n",
      "    epoch          : 3812\n",
      "    loss           : 12215.937314541976\n",
      "    val_loss       : 12216.949846757718\n",
      "    val_log_likelihood: -12139.317651791254\n",
      "    val_log_marginal: -12147.22309712436\n",
      "Train Epoch: 3813 [256/118836 (0%)] Loss: 12189.411133\n",
      "Train Epoch: 3813 [33024/118836 (28%)] Loss: 12185.874023\n",
      "Train Epoch: 3813 [65792/118836 (55%)] Loss: 12202.479492\n",
      "Train Epoch: 3813 [98560/118836 (83%)] Loss: 12265.181641\n",
      "    epoch          : 3813\n",
      "    loss           : 12220.262381423181\n",
      "    val_loss       : 12216.058694681667\n",
      "    val_log_likelihood: -12140.711063669614\n",
      "    val_log_marginal: -12148.612805576346\n",
      "Train Epoch: 3814 [256/118836 (0%)] Loss: 12183.419922\n",
      "Train Epoch: 3814 [33024/118836 (28%)] Loss: 12241.664062\n",
      "Train Epoch: 3814 [65792/118836 (55%)] Loss: 12282.166016\n",
      "Train Epoch: 3814 [98560/118836 (83%)] Loss: 12267.962891\n",
      "    epoch          : 3814\n",
      "    loss           : 12216.120633820565\n",
      "    val_loss       : 12220.346798202047\n",
      "    val_log_likelihood: -12140.346092134512\n",
      "    val_log_marginal: -12148.25533350563\n",
      "Train Epoch: 3815 [256/118836 (0%)] Loss: 12237.004883\n",
      "Train Epoch: 3815 [33024/118836 (28%)] Loss: 12294.049805\n",
      "Train Epoch: 3815 [65792/118836 (55%)] Loss: 12186.132812\n",
      "Train Epoch: 3815 [98560/118836 (83%)] Loss: 12153.537109\n",
      "    epoch          : 3815\n",
      "    loss           : 12216.096004575062\n",
      "    val_loss       : 12217.784418553645\n",
      "    val_log_likelihood: -12140.989224856545\n",
      "    val_log_marginal: -12148.89345754096\n",
      "Train Epoch: 3816 [256/118836 (0%)] Loss: 12227.786133\n",
      "Train Epoch: 3816 [33024/118836 (28%)] Loss: 12199.723633\n",
      "Train Epoch: 3816 [65792/118836 (55%)] Loss: 12269.804688\n",
      "Train Epoch: 3816 [98560/118836 (83%)] Loss: 12342.506836\n",
      "    epoch          : 3816\n",
      "    loss           : 12216.098512135546\n",
      "    val_loss       : 12217.171215061368\n",
      "    val_log_likelihood: -12140.049459134616\n",
      "    val_log_marginal: -12147.94847302558\n",
      "Train Epoch: 3817 [256/118836 (0%)] Loss: 12275.030273\n",
      "Train Epoch: 3817 [33024/118836 (28%)] Loss: 12270.755859\n",
      "Train Epoch: 3817 [65792/118836 (55%)] Loss: 12282.386719\n",
      "Train Epoch: 3817 [98560/118836 (83%)] Loss: 12248.161133\n",
      "    epoch          : 3817\n",
      "    loss           : 12218.373297760287\n",
      "    val_loss       : 12220.31967708525\n",
      "    val_log_likelihood: -12141.010257541098\n",
      "    val_log_marginal: -12148.905243148001\n",
      "Train Epoch: 3818 [256/118836 (0%)] Loss: 12243.202148\n",
      "Train Epoch: 3818 [33024/118836 (28%)] Loss: 12224.968750\n",
      "Train Epoch: 3818 [65792/118836 (55%)] Loss: 12142.240234\n",
      "Train Epoch: 3818 [98560/118836 (83%)] Loss: 12233.978516\n",
      "    epoch          : 3818\n",
      "    loss           : 12217.769245147074\n",
      "    val_loss       : 12216.223960526691\n",
      "    val_log_likelihood: -12138.874378037119\n",
      "    val_log_marginal: -12146.778801160233\n",
      "Train Epoch: 3819 [256/118836 (0%)] Loss: 12214.358398\n",
      "Train Epoch: 3819 [33024/118836 (28%)] Loss: 12136.054688\n",
      "Train Epoch: 3819 [65792/118836 (55%)] Loss: 12406.044922\n",
      "Train Epoch: 3819 [98560/118836 (83%)] Loss: 12127.787109\n",
      "    epoch          : 3819\n",
      "    loss           : 12219.546103442928\n",
      "    val_loss       : 12220.459816743156\n",
      "    val_log_likelihood: -12140.96208934295\n",
      "    val_log_marginal: -12148.862472787316\n",
      "Train Epoch: 3820 [256/118836 (0%)] Loss: 12174.385742\n",
      "Train Epoch: 3820 [33024/118836 (28%)] Loss: 12340.243164\n",
      "Train Epoch: 3820 [65792/118836 (55%)] Loss: 12255.545898\n",
      "Train Epoch: 3820 [98560/118836 (83%)] Loss: 12230.584961\n",
      "    epoch          : 3820\n",
      "    loss           : 12215.113380440964\n",
      "    val_loss       : 12215.303304140187\n",
      "    val_log_likelihood: -12138.600586422146\n",
      "    val_log_marginal: -12146.496917597482\n",
      "Train Epoch: 3821 [256/118836 (0%)] Loss: 12208.506836\n",
      "Train Epoch: 3821 [33024/118836 (28%)] Loss: 12242.000000\n",
      "Train Epoch: 3821 [65792/118836 (55%)] Loss: 12279.545898\n",
      "Train Epoch: 3821 [98560/118836 (83%)] Loss: 12154.658203\n",
      "    epoch          : 3821\n",
      "    loss           : 12214.894802651985\n",
      "    val_loss       : 12215.746732217027\n",
      "    val_log_likelihood: -12140.655431755324\n",
      "    val_log_marginal: -12148.556027345696\n",
      "Train Epoch: 3822 [256/118836 (0%)] Loss: 12257.317383\n",
      "Train Epoch: 3822 [33024/118836 (28%)] Loss: 12186.993164\n",
      "Train Epoch: 3822 [65792/118836 (55%)] Loss: 12337.767578\n",
      "Train Epoch: 3822 [98560/118836 (83%)] Loss: 12241.046875\n",
      "    epoch          : 3822\n",
      "    loss           : 12218.323998397436\n",
      "    val_loss       : 12219.486167641575\n",
      "    val_log_likelihood: -12140.126922592277\n",
      "    val_log_marginal: -12148.030613439601\n",
      "Train Epoch: 3823 [256/118836 (0%)] Loss: 12336.235352\n",
      "Train Epoch: 3823 [33024/118836 (28%)] Loss: 12213.246094\n",
      "Train Epoch: 3823 [65792/118836 (55%)] Loss: 12184.221680\n",
      "Train Epoch: 3823 [98560/118836 (83%)] Loss: 12177.502930\n",
      "    epoch          : 3823\n",
      "    loss           : 12217.38211218595\n",
      "    val_loss       : 12218.531740890221\n",
      "    val_log_likelihood: -12139.736974643301\n",
      "    val_log_marginal: -12147.640924231608\n",
      "Train Epoch: 3824 [256/118836 (0%)] Loss: 12222.965820\n",
      "Train Epoch: 3824 [33024/118836 (28%)] Loss: 12238.314453\n",
      "Train Epoch: 3824 [65792/118836 (55%)] Loss: 12323.859375\n",
      "Train Epoch: 3824 [98560/118836 (83%)] Loss: 12238.865234\n",
      "    epoch          : 3824\n",
      "    loss           : 12216.758997783549\n",
      "    val_loss       : 12219.945115559207\n",
      "    val_log_likelihood: -12139.729424337003\n",
      "    val_log_marginal: -12147.63249170491\n",
      "Train Epoch: 3825 [256/118836 (0%)] Loss: 12222.977539\n",
      "Train Epoch: 3825 [33024/118836 (28%)] Loss: 12220.328125\n",
      "Train Epoch: 3825 [65792/118836 (55%)] Loss: 12219.212891\n",
      "Train Epoch: 3825 [98560/118836 (83%)] Loss: 12282.947266\n",
      "    epoch          : 3825\n",
      "    loss           : 12219.273175952492\n",
      "    val_loss       : 12216.200530159282\n",
      "    val_log_likelihood: -12139.437968491522\n",
      "    val_log_marginal: -12147.338096457757\n",
      "Train Epoch: 3826 [256/118836 (0%)] Loss: 12259.193359\n",
      "Train Epoch: 3826 [33024/118836 (28%)] Loss: 12253.331055\n",
      "Train Epoch: 3826 [65792/118836 (55%)] Loss: 12267.332031\n",
      "Train Epoch: 3826 [98560/118836 (83%)] Loss: 12214.305664\n",
      "    epoch          : 3826\n",
      "    loss           : 12215.642705263906\n",
      "    val_loss       : 12218.336025307999\n",
      "    val_log_likelihood: -12139.15391545828\n",
      "    val_log_marginal: -12147.064779227914\n",
      "Train Epoch: 3827 [256/118836 (0%)] Loss: 12225.374023\n",
      "Train Epoch: 3827 [33024/118836 (28%)] Loss: 12298.890625\n",
      "Train Epoch: 3827 [65792/118836 (55%)] Loss: 12182.896484\n",
      "Train Epoch: 3827 [98560/118836 (83%)] Loss: 12168.652344\n",
      "    epoch          : 3827\n",
      "    loss           : 12220.351294975188\n",
      "    val_loss       : 12215.322426110379\n",
      "    val_log_likelihood: -12139.672159325888\n",
      "    val_log_marginal: -12147.57922557081\n",
      "Train Epoch: 3828 [256/118836 (0%)] Loss: 12186.087891\n",
      "Train Epoch: 3828 [33024/118836 (28%)] Loss: 12264.723633\n",
      "Train Epoch: 3828 [65792/118836 (55%)] Loss: 12200.898438\n",
      "Train Epoch: 3828 [98560/118836 (83%)] Loss: 12221.246094\n",
      "    epoch          : 3828\n",
      "    loss           : 12220.055436278692\n",
      "    val_loss       : 12219.10482475528\n",
      "    val_log_likelihood: -12140.50719699907\n",
      "    val_log_marginal: -12148.404710478208\n",
      "Train Epoch: 3829 [256/118836 (0%)] Loss: 12221.941406\n",
      "Train Epoch: 3829 [33024/118836 (28%)] Loss: 12274.064453\n",
      "Train Epoch: 3829 [65792/118836 (55%)] Loss: 12224.480469\n",
      "Train Epoch: 3829 [98560/118836 (83%)] Loss: 12257.221680\n",
      "    epoch          : 3829\n",
      "    loss           : 12216.864541492194\n",
      "    val_loss       : 12218.24084520028\n",
      "    val_log_likelihood: -12140.840141161343\n",
      "    val_log_marginal: -12148.743495660021\n",
      "Train Epoch: 3830 [256/118836 (0%)] Loss: 12175.216797\n",
      "Train Epoch: 3830 [33024/118836 (28%)] Loss: 12209.060547\n",
      "Train Epoch: 3830 [65792/118836 (55%)] Loss: 12301.796875\n",
      "Train Epoch: 3830 [98560/118836 (83%)] Loss: 12207.423828\n",
      "    epoch          : 3830\n",
      "    loss           : 12214.49839032775\n",
      "    val_loss       : 12220.988352070472\n",
      "    val_log_likelihood: -12137.980663254757\n",
      "    val_log_marginal: -12145.888885196151\n",
      "Train Epoch: 3831 [256/118836 (0%)] Loss: 12273.673828\n",
      "Train Epoch: 3831 [33024/118836 (28%)] Loss: 12177.164062\n",
      "Train Epoch: 3831 [65792/118836 (55%)] Loss: 12204.205078\n",
      "Train Epoch: 3831 [98560/118836 (83%)] Loss: 12261.684570\n",
      "    epoch          : 3831\n",
      "    loss           : 12217.851258788254\n",
      "    val_loss       : 12216.418088447663\n",
      "    val_log_likelihood: -12139.52411358173\n",
      "    val_log_marginal: -12147.42335615303\n",
      "Train Epoch: 3832 [256/118836 (0%)] Loss: 12316.295898\n",
      "Train Epoch: 3832 [33024/118836 (28%)] Loss: 12247.112305\n",
      "Train Epoch: 3832 [65792/118836 (55%)] Loss: 12210.387695\n",
      "Train Epoch: 3832 [98560/118836 (83%)] Loss: 12287.542969\n",
      "    epoch          : 3832\n",
      "    loss           : 12215.065418379085\n",
      "    val_loss       : 12216.39652353143\n",
      "    val_log_likelihood: -12138.150324551541\n",
      "    val_log_marginal: -12146.040824110994\n",
      "Train Epoch: 3833 [256/118836 (0%)] Loss: 12212.291016\n",
      "Train Epoch: 3833 [33024/118836 (28%)] Loss: 12262.394531\n",
      "Train Epoch: 3833 [65792/118836 (55%)] Loss: 12286.019531\n",
      "Train Epoch: 3833 [98560/118836 (83%)] Loss: 12319.855469\n",
      "    epoch          : 3833\n",
      "    loss           : 12216.046335911653\n",
      "    val_loss       : 12216.273121358057\n",
      "    val_log_likelihood: -12139.26163926799\n",
      "    val_log_marginal: -12147.163181735026\n",
      "Train Epoch: 3834 [256/118836 (0%)] Loss: 12194.041016\n",
      "Train Epoch: 3834 [33024/118836 (28%)] Loss: 12276.548828\n",
      "Train Epoch: 3834 [65792/118836 (55%)] Loss: 12254.995117\n",
      "Train Epoch: 3834 [98560/118836 (83%)] Loss: 12233.163086\n",
      "    epoch          : 3834\n",
      "    loss           : 12220.513253786703\n",
      "    val_loss       : 12217.015766764656\n",
      "    val_log_likelihood: -12140.065472174834\n",
      "    val_log_marginal: -12147.96412367719\n",
      "Train Epoch: 3835 [256/118836 (0%)] Loss: 12214.180664\n",
      "Train Epoch: 3835 [33024/118836 (28%)] Loss: 12248.084961\n",
      "Train Epoch: 3835 [65792/118836 (55%)] Loss: 12190.976562\n",
      "Train Epoch: 3835 [98560/118836 (83%)] Loss: 12375.733398\n",
      "    epoch          : 3835\n",
      "    loss           : 12216.945981312034\n",
      "    val_loss       : 12215.05154316474\n",
      "    val_log_likelihood: -12140.006177787687\n",
      "    val_log_marginal: -12147.904408985016\n",
      "Train Epoch: 3836 [256/118836 (0%)] Loss: 12176.384766\n",
      "Train Epoch: 3836 [33024/118836 (28%)] Loss: 12249.602539\n",
      "Train Epoch: 3836 [65792/118836 (55%)] Loss: 12146.196289\n",
      "Train Epoch: 3836 [98560/118836 (83%)] Loss: 12244.181641\n",
      "    epoch          : 3836\n",
      "    loss           : 12217.54665125491\n",
      "    val_loss       : 12218.584823263809\n",
      "    val_log_likelihood: -12139.708323640405\n",
      "    val_log_marginal: -12147.607932848456\n",
      "Train Epoch: 3837 [256/118836 (0%)] Loss: 12337.800781\n",
      "Train Epoch: 3837 [33024/118836 (28%)] Loss: 12276.646484\n",
      "Train Epoch: 3837 [65792/118836 (55%)] Loss: 12345.469727\n",
      "Train Epoch: 3837 [98560/118836 (83%)] Loss: 12251.662109\n",
      "    epoch          : 3837\n",
      "    loss           : 12215.125945060483\n",
      "    val_loss       : 12214.015917050587\n",
      "    val_log_likelihood: -12138.816708507806\n",
      "    val_log_marginal: -12146.715550412073\n",
      "Train Epoch: 3838 [256/118836 (0%)] Loss: 12323.847656\n",
      "Train Epoch: 3838 [33024/118836 (28%)] Loss: 12325.120117\n",
      "Train Epoch: 3838 [65792/118836 (55%)] Loss: 12183.024414\n",
      "Train Epoch: 3838 [98560/118836 (83%)] Loss: 12302.852539\n",
      "    epoch          : 3838\n",
      "    loss           : 12215.467281521402\n",
      "    val_loss       : 12218.481375352436\n",
      "    val_log_likelihood: -12139.978673619727\n",
      "    val_log_marginal: -12147.882589246237\n",
      "Train Epoch: 3839 [256/118836 (0%)] Loss: 12237.083984\n",
      "Train Epoch: 3839 [33024/118836 (28%)] Loss: 12199.925781\n",
      "Train Epoch: 3839 [65792/118836 (55%)] Loss: 12154.658203\n",
      "Train Epoch: 3839 [98560/118836 (83%)] Loss: 12278.659180\n",
      "    epoch          : 3839\n",
      "    loss           : 12220.538021802626\n",
      "    val_loss       : 12218.24294991235\n",
      "    val_log_likelihood: -12140.56540836306\n",
      "    val_log_marginal: -12148.465114736986\n",
      "Train Epoch: 3840 [256/118836 (0%)] Loss: 12209.201172\n",
      "Train Epoch: 3840 [33024/118836 (28%)] Loss: 12189.283203\n",
      "Train Epoch: 3840 [65792/118836 (55%)] Loss: 12232.916992\n",
      "Train Epoch: 3840 [98560/118836 (83%)] Loss: 12233.071289\n",
      "    epoch          : 3840\n",
      "    loss           : 12215.40739360396\n",
      "    val_loss       : 12214.870220142313\n",
      "    val_log_likelihood: -12139.3477370244\n",
      "    val_log_marginal: -12147.252372005105\n",
      "Train Epoch: 3841 [256/118836 (0%)] Loss: 12183.145508\n",
      "Train Epoch: 3841 [33024/118836 (28%)] Loss: 12315.721680\n",
      "Train Epoch: 3841 [65792/118836 (55%)] Loss: 12292.994141\n",
      "Train Epoch: 3841 [98560/118836 (83%)] Loss: 12315.358398\n",
      "    epoch          : 3841\n",
      "    loss           : 12218.292781838038\n",
      "    val_loss       : 12217.930437428242\n",
      "    val_log_likelihood: -12139.602560063844\n",
      "    val_log_marginal: -12147.501730359905\n",
      "Train Epoch: 3842 [256/118836 (0%)] Loss: 12205.949219\n",
      "Train Epoch: 3842 [33024/118836 (28%)] Loss: 12221.698242\n",
      "Train Epoch: 3842 [65792/118836 (55%)] Loss: 12193.434570\n",
      "Train Epoch: 3842 [98560/118836 (83%)] Loss: 12162.727539\n",
      "    epoch          : 3842\n",
      "    loss           : 12218.47562115514\n",
      "    val_loss       : 12218.003008269927\n",
      "    val_log_likelihood: -12139.706499108252\n",
      "    val_log_marginal: -12147.60582854661\n",
      "Train Epoch: 3843 [256/118836 (0%)] Loss: 12160.995117\n",
      "Train Epoch: 3843 [33024/118836 (28%)] Loss: 12276.436523\n",
      "Train Epoch: 3843 [65792/118836 (55%)] Loss: 12382.827148\n",
      "Train Epoch: 3843 [98560/118836 (83%)] Loss: 12188.491211\n",
      "    epoch          : 3843\n",
      "    loss           : 12220.711121019436\n",
      "    val_loss       : 12217.119675276655\n",
      "    val_log_likelihood: -12140.979549537326\n",
      "    val_log_marginal: -12148.87167101099\n",
      "Train Epoch: 3844 [256/118836 (0%)] Loss: 12312.016602\n",
      "Train Epoch: 3844 [33024/118836 (28%)] Loss: 12193.807617\n",
      "Train Epoch: 3844 [65792/118836 (55%)] Loss: 12215.707031\n",
      "Train Epoch: 3844 [98560/118836 (83%)] Loss: 12175.423828\n",
      "    epoch          : 3844\n",
      "    loss           : 12219.597967231442\n",
      "    val_loss       : 12215.15315456102\n",
      "    val_log_likelihood: -12139.087815343259\n",
      "    val_log_marginal: -12146.984904353736\n",
      "Train Epoch: 3845 [256/118836 (0%)] Loss: 12337.340820\n",
      "Train Epoch: 3845 [33024/118836 (28%)] Loss: 12242.162109\n",
      "Train Epoch: 3845 [65792/118836 (55%)] Loss: 12155.988281\n",
      "Train Epoch: 3845 [98560/118836 (83%)] Loss: 12200.007812\n",
      "    epoch          : 3845\n",
      "    loss           : 12217.949489344242\n",
      "    val_loss       : 12221.699018475705\n",
      "    val_log_likelihood: -12139.902464103858\n",
      "    val_log_marginal: -12147.801209354848\n",
      "Train Epoch: 3846 [256/118836 (0%)] Loss: 12221.509766\n",
      "Train Epoch: 3846 [33024/118836 (28%)] Loss: 12291.578125\n",
      "Train Epoch: 3846 [65792/118836 (55%)] Loss: 12348.356445\n",
      "Train Epoch: 3846 [98560/118836 (83%)] Loss: 12186.416016\n",
      "    epoch          : 3846\n",
      "    loss           : 12218.39165002714\n",
      "    val_loss       : 12219.391656672062\n",
      "    val_log_likelihood: -12139.995224779\n",
      "    val_log_marginal: -12147.894176657586\n",
      "Train Epoch: 3847 [256/118836 (0%)] Loss: 12313.339844\n",
      "Train Epoch: 3847 [33024/118836 (28%)] Loss: 12202.792969\n",
      "Train Epoch: 3847 [65792/118836 (55%)] Loss: 12229.185547\n",
      "Train Epoch: 3847 [98560/118836 (83%)] Loss: 12179.556641\n",
      "    epoch          : 3847\n",
      "    loss           : 12216.418623184192\n",
      "    val_loss       : 12218.882105009181\n",
      "    val_log_likelihood: -12139.78491118047\n",
      "    val_log_marginal: -12147.68060807062\n",
      "Train Epoch: 3848 [256/118836 (0%)] Loss: 12410.208008\n",
      "Train Epoch: 3848 [33024/118836 (28%)] Loss: 12169.683594\n",
      "Train Epoch: 3848 [65792/118836 (55%)] Loss: 12201.226562\n",
      "Train Epoch: 3848 [98560/118836 (83%)] Loss: 12232.574219\n",
      "    epoch          : 3848\n",
      "    loss           : 12220.126358786962\n",
      "    val_loss       : 12219.858523782443\n",
      "    val_log_likelihood: -12139.774292416254\n",
      "    val_log_marginal: -12147.674626582699\n",
      "Train Epoch: 3849 [256/118836 (0%)] Loss: 12288.223633\n",
      "Train Epoch: 3849 [33024/118836 (28%)] Loss: 12189.183594\n",
      "Train Epoch: 3849 [65792/118836 (55%)] Loss: 12167.796875\n",
      "Train Epoch: 3849 [98560/118836 (83%)] Loss: 12269.570312\n",
      "    epoch          : 3849\n",
      "    loss           : 12219.05322726039\n",
      "    val_loss       : 12217.387449133419\n",
      "    val_log_likelihood: -12139.012787718413\n",
      "    val_log_marginal: -12146.90762514753\n",
      "Train Epoch: 3850 [256/118836 (0%)] Loss: 12169.225586\n",
      "Train Epoch: 3850 [33024/118836 (28%)] Loss: 12254.386719\n",
      "Train Epoch: 3850 [65792/118836 (55%)] Loss: 12188.432617\n",
      "Train Epoch: 3850 [98560/118836 (83%)] Loss: 12238.394531\n",
      "    epoch          : 3850\n",
      "    loss           : 12217.516643726738\n",
      "    val_loss       : 12218.212688149171\n",
      "    val_log_likelihood: -12139.374829404467\n",
      "    val_log_marginal: -12147.2723282504\n",
      "Train Epoch: 3851 [256/118836 (0%)] Loss: 12143.279297\n",
      "Train Epoch: 3851 [33024/118836 (28%)] Loss: 12305.917969\n",
      "Train Epoch: 3851 [65792/118836 (55%)] Loss: 12249.440430\n",
      "Train Epoch: 3851 [98560/118836 (83%)] Loss: 12210.554688\n",
      "    epoch          : 3851\n",
      "    loss           : 12219.196554810278\n",
      "    val_loss       : 12217.719569649562\n",
      "    val_log_likelihood: -12140.839621297302\n",
      "    val_log_marginal: -12148.7324602547\n",
      "Train Epoch: 3852 [256/118836 (0%)] Loss: 12380.269531\n",
      "Train Epoch: 3852 [33024/118836 (28%)] Loss: 12270.734375\n",
      "Train Epoch: 3852 [65792/118836 (55%)] Loss: 12195.597656\n",
      "Train Epoch: 3852 [98560/118836 (83%)] Loss: 12192.330078\n",
      "    epoch          : 3852\n",
      "    loss           : 12213.540310626035\n",
      "    val_loss       : 12212.61647014049\n",
      "    val_log_likelihood: -12141.702924517938\n",
      "    val_log_marginal: -12149.596089356228\n",
      "Train Epoch: 3853 [256/118836 (0%)] Loss: 12152.258789\n",
      "Train Epoch: 3853 [33024/118836 (28%)] Loss: 12221.333984\n",
      "Train Epoch: 3853 [65792/118836 (55%)] Loss: 12204.473633\n",
      "Train Epoch: 3853 [98560/118836 (83%)] Loss: 12178.208008\n",
      "    epoch          : 3853\n",
      "    loss           : 12218.83270200062\n",
      "    val_loss       : 12216.99359285192\n",
      "    val_log_likelihood: -12139.810982410565\n",
      "    val_log_marginal: -12147.703166776932\n",
      "Train Epoch: 3854 [256/118836 (0%)] Loss: 12239.513672\n",
      "Train Epoch: 3854 [33024/118836 (28%)] Loss: 12250.488281\n",
      "Train Epoch: 3854 [65792/118836 (55%)] Loss: 12212.731445\n",
      "Train Epoch: 3854 [98560/118836 (83%)] Loss: 12229.330078\n",
      "    epoch          : 3854\n",
      "    loss           : 12215.573001156688\n",
      "    val_loss       : 12220.001769028491\n",
      "    val_log_likelihood: -12139.551071714744\n",
      "    val_log_marginal: -12147.445227657801\n",
      "Train Epoch: 3855 [256/118836 (0%)] Loss: 12367.514648\n",
      "Train Epoch: 3855 [33024/118836 (28%)] Loss: 12308.584961\n",
      "Train Epoch: 3855 [65792/118836 (55%)] Loss: 12319.149414\n",
      "Train Epoch: 3855 [98560/118836 (83%)] Loss: 12197.555664\n",
      "    epoch          : 3855\n",
      "    loss           : 12215.859422172249\n",
      "    val_loss       : 12220.84044137688\n",
      "    val_log_likelihood: -12140.014566532258\n",
      "    val_log_marginal: -12147.916678552428\n",
      "Train Epoch: 3856 [256/118836 (0%)] Loss: 12250.330078\n",
      "Train Epoch: 3856 [33024/118836 (28%)] Loss: 12281.916016\n",
      "Train Epoch: 3856 [65792/118836 (55%)] Loss: 12204.151367\n",
      "Train Epoch: 3856 [98560/118836 (83%)] Loss: 12243.916992\n",
      "    epoch          : 3856\n",
      "    loss           : 12218.189226472034\n",
      "    val_loss       : 12218.225394363202\n",
      "    val_log_likelihood: -12139.831607507496\n",
      "    val_log_marginal: -12147.725540396405\n",
      "Train Epoch: 3857 [256/118836 (0%)] Loss: 12271.490234\n",
      "Train Epoch: 3857 [33024/118836 (28%)] Loss: 12185.107422\n",
      "Train Epoch: 3857 [65792/118836 (55%)] Loss: 12201.699219\n",
      "Train Epoch: 3857 [98560/118836 (83%)] Loss: 12285.413086\n",
      "    epoch          : 3857\n",
      "    loss           : 12217.38228827414\n",
      "    val_loss       : 12221.553054102887\n",
      "    val_log_likelihood: -12139.123938624381\n",
      "    val_log_marginal: -12147.019861891573\n",
      "Train Epoch: 3858 [256/118836 (0%)] Loss: 12152.379883\n",
      "Train Epoch: 3858 [33024/118836 (28%)] Loss: 12271.636719\n",
      "Train Epoch: 3858 [65792/118836 (55%)] Loss: 12173.052734\n",
      "Train Epoch: 3858 [98560/118836 (83%)] Loss: 12206.995117\n",
      "    epoch          : 3858\n",
      "    loss           : 12218.305699926334\n",
      "    val_loss       : 12219.957323336015\n",
      "    val_log_likelihood: -12142.349764784945\n",
      "    val_log_marginal: -12150.237641020802\n",
      "Train Epoch: 3859 [256/118836 (0%)] Loss: 12241.680664\n",
      "Train Epoch: 3859 [33024/118836 (28%)] Loss: 12166.232422\n",
      "Train Epoch: 3859 [65792/118836 (55%)] Loss: 12232.473633\n",
      "Train Epoch: 3859 [98560/118836 (83%)] Loss: 12209.958008\n",
      "    epoch          : 3859\n",
      "    loss           : 12216.682025595792\n",
      "    val_loss       : 12216.277207885036\n",
      "    val_log_likelihood: -12139.163297889525\n",
      "    val_log_marginal: -12147.061061729752\n",
      "Train Epoch: 3860 [256/118836 (0%)] Loss: 12223.194336\n",
      "Train Epoch: 3860 [33024/118836 (28%)] Loss: 12235.546875\n",
      "Train Epoch: 3860 [65792/118836 (55%)] Loss: 12183.918945\n",
      "Train Epoch: 3860 [98560/118836 (83%)] Loss: 12242.321289\n",
      "    epoch          : 3860\n",
      "    loss           : 12220.94139397229\n",
      "    val_loss       : 12219.044644408506\n",
      "    val_log_likelihood: -12139.961468510908\n",
      "    val_log_marginal: -12147.853758149766\n",
      "Train Epoch: 3861 [256/118836 (0%)] Loss: 12252.737305\n",
      "Train Epoch: 3861 [33024/118836 (28%)] Loss: 12276.977539\n",
      "Train Epoch: 3861 [65792/118836 (55%)] Loss: 12189.898438\n",
      "Train Epoch: 3861 [98560/118836 (83%)] Loss: 12147.835938\n",
      "    epoch          : 3861\n",
      "    loss           : 12216.944518326096\n",
      "    val_loss       : 12218.954055887176\n",
      "    val_log_likelihood: -12139.706895387459\n",
      "    val_log_marginal: -12147.60157983984\n",
      "Train Epoch: 3862 [256/118836 (0%)] Loss: 12346.899414\n",
      "Train Epoch: 3862 [33024/118836 (28%)] Loss: 12243.146484\n",
      "Train Epoch: 3862 [65792/118836 (55%)] Loss: 12212.761719\n",
      "Train Epoch: 3862 [98560/118836 (83%)] Loss: 12172.103516\n",
      "    epoch          : 3862\n",
      "    loss           : 12218.306180534015\n",
      "    val_loss       : 12216.635876981456\n",
      "    val_log_likelihood: -12141.258795201355\n",
      "    val_log_marginal: -12149.149642400345\n",
      "Train Epoch: 3863 [256/118836 (0%)] Loss: 12246.599609\n",
      "Train Epoch: 3863 [33024/118836 (28%)] Loss: 12383.431641\n",
      "Train Epoch: 3863 [65792/118836 (55%)] Loss: 12228.219727\n",
      "Train Epoch: 3863 [98560/118836 (83%)] Loss: 12250.148438\n",
      "    epoch          : 3863\n",
      "    loss           : 12221.341423535721\n",
      "    val_loss       : 12212.687614944081\n",
      "    val_log_likelihood: -12139.17157322684\n",
      "    val_log_marginal: -12147.059525692037\n",
      "Train Epoch: 3864 [256/118836 (0%)] Loss: 12214.419922\n",
      "Train Epoch: 3864 [33024/118836 (28%)] Loss: 12279.300781\n",
      "Train Epoch: 3864 [65792/118836 (55%)] Loss: 12182.807617\n",
      "Train Epoch: 3864 [98560/118836 (83%)] Loss: 12168.981445\n",
      "    epoch          : 3864\n",
      "    loss           : 12221.037656540788\n",
      "    val_loss       : 12222.688827975311\n",
      "    val_log_likelihood: -12140.338028749225\n",
      "    val_log_marginal: -12148.233900435034\n",
      "Train Epoch: 3865 [256/118836 (0%)] Loss: 12209.313477\n",
      "Train Epoch: 3865 [33024/118836 (28%)] Loss: 12305.201172\n",
      "Train Epoch: 3865 [65792/118836 (55%)] Loss: 12204.370117\n",
      "Train Epoch: 3865 [98560/118836 (83%)] Loss: 12184.098633\n",
      "    epoch          : 3865\n",
      "    loss           : 12219.871774031999\n",
      "    val_loss       : 12219.417643743394\n",
      "    val_log_likelihood: -12139.327030668424\n",
      "    val_log_marginal: -12147.218421210086\n",
      "Train Epoch: 3866 [256/118836 (0%)] Loss: 12214.896484\n",
      "Train Epoch: 3866 [33024/118836 (28%)] Loss: 12256.921875\n",
      "Train Epoch: 3866 [65792/118836 (55%)] Loss: 12296.234375\n",
      "Train Epoch: 3866 [98560/118836 (83%)] Loss: 12274.575195\n",
      "    epoch          : 3866\n",
      "    loss           : 12219.577395284068\n",
      "    val_loss       : 12216.2307208111\n",
      "    val_log_likelihood: -12140.597991625309\n",
      "    val_log_marginal: -12148.49255779528\n",
      "Train Epoch: 3867 [256/118836 (0%)] Loss: 12275.746094\n",
      "Train Epoch: 3867 [33024/118836 (28%)] Loss: 12180.911133\n",
      "Train Epoch: 3867 [65792/118836 (55%)] Loss: 12292.733398\n",
      "Train Epoch: 3867 [98560/118836 (83%)] Loss: 12203.623047\n",
      "    epoch          : 3867\n",
      "    loss           : 12212.597260616989\n",
      "    val_loss       : 12218.315211004736\n",
      "    val_log_likelihood: -12141.85247040426\n",
      "    val_log_marginal: -12149.741305240186\n",
      "Train Epoch: 3868 [256/118836 (0%)] Loss: 12282.603516\n",
      "Train Epoch: 3868 [33024/118836 (28%)] Loss: 12189.989258\n",
      "Train Epoch: 3868 [65792/118836 (55%)] Loss: 12263.966797\n",
      "Train Epoch: 3868 [98560/118836 (83%)] Loss: 12309.242188\n",
      "    epoch          : 3868\n",
      "    loss           : 12220.85044716708\n",
      "    val_loss       : 12213.8828290992\n",
      "    val_log_likelihood: -12139.709204889114\n",
      "    val_log_marginal: -12147.605803381379\n",
      "Train Epoch: 3869 [256/118836 (0%)] Loss: 12241.063477\n",
      "Train Epoch: 3869 [33024/118836 (28%)] Loss: 12173.488281\n",
      "Train Epoch: 3869 [65792/118836 (55%)] Loss: 12322.919922\n",
      "Train Epoch: 3869 [98560/118836 (83%)] Loss: 12201.509766\n",
      "    epoch          : 3869\n",
      "    loss           : 12214.533761922303\n",
      "    val_loss       : 12218.878444050992\n",
      "    val_log_likelihood: -12139.512324073356\n",
      "    val_log_marginal: -12147.409865173964\n",
      "Train Epoch: 3870 [256/118836 (0%)] Loss: 12246.879883\n",
      "Train Epoch: 3870 [33024/118836 (28%)] Loss: 12226.454102\n",
      "Train Epoch: 3870 [65792/118836 (55%)] Loss: 12305.048828\n",
      "Train Epoch: 3870 [98560/118836 (83%)] Loss: 12158.865234\n",
      "    epoch          : 3870\n",
      "    loss           : 12218.25441545182\n",
      "    val_loss       : 12221.25239088368\n",
      "    val_log_likelihood: -12140.155483774037\n",
      "    val_log_marginal: -12148.048106845297\n",
      "Train Epoch: 3871 [256/118836 (0%)] Loss: 12249.583984\n",
      "Train Epoch: 3871 [33024/118836 (28%)] Loss: 12212.089844\n",
      "Train Epoch: 3871 [65792/118836 (55%)] Loss: 12151.115234\n",
      "Train Epoch: 3871 [98560/118836 (83%)] Loss: 12309.331055\n",
      "    epoch          : 3871\n",
      "    loss           : 12216.099229089123\n",
      "    val_loss       : 12219.422087319343\n",
      "    val_log_likelihood: -12141.218041123862\n",
      "    val_log_marginal: -12149.10975911415\n",
      "Train Epoch: 3872 [256/118836 (0%)] Loss: 12265.381836\n",
      "Train Epoch: 3872 [33024/118836 (28%)] Loss: 12304.162109\n",
      "Train Epoch: 3872 [65792/118836 (55%)] Loss: 12327.139648\n",
      "Train Epoch: 3872 [98560/118836 (83%)] Loss: 12292.697266\n",
      "    epoch          : 3872\n",
      "    loss           : 12218.816700107267\n",
      "    val_loss       : 12216.191083789416\n",
      "    val_log_likelihood: -12139.005572302782\n",
      "    val_log_marginal: -12146.896464092039\n",
      "Train Epoch: 3873 [256/118836 (0%)] Loss: 12245.250000\n",
      "Train Epoch: 3873 [33024/118836 (28%)] Loss: 12211.236328\n",
      "Train Epoch: 3873 [65792/118836 (55%)] Loss: 12170.324219\n",
      "Train Epoch: 3873 [98560/118836 (83%)] Loss: 12189.282227\n",
      "    epoch          : 3873\n",
      "    loss           : 12216.519147894696\n",
      "    val_loss       : 12219.427152333896\n",
      "    val_log_likelihood: -12138.795033666769\n",
      "    val_log_marginal: -12146.683819001892\n",
      "Train Epoch: 3874 [256/118836 (0%)] Loss: 12263.824219\n",
      "Train Epoch: 3874 [33024/118836 (28%)] Loss: 12252.619141\n",
      "Train Epoch: 3874 [65792/118836 (55%)] Loss: 12238.819336\n",
      "Train Epoch: 3874 [98560/118836 (83%)] Loss: 12186.268555\n",
      "    epoch          : 3874\n",
      "    loss           : 12214.491813676075\n",
      "    val_loss       : 12215.584748987685\n",
      "    val_log_likelihood: -12138.321464989143\n",
      "    val_log_marginal: -12146.216110811663\n",
      "Train Epoch: 3875 [256/118836 (0%)] Loss: 12252.535156\n",
      "Train Epoch: 3875 [33024/118836 (28%)] Loss: 12164.590820\n",
      "Train Epoch: 3875 [65792/118836 (55%)] Loss: 12306.617188\n",
      "Train Epoch: 3875 [98560/118836 (83%)] Loss: 12286.090820\n",
      "    epoch          : 3875\n",
      "    loss           : 12219.214295291822\n",
      "    val_loss       : 12218.956931389519\n",
      "    val_log_likelihood: -12138.61265201742\n",
      "    val_log_marginal: -12146.506287070613\n",
      "Train Epoch: 3876 [256/118836 (0%)] Loss: 12198.687500\n",
      "Train Epoch: 3876 [33024/118836 (28%)] Loss: 12218.019531\n",
      "Train Epoch: 3876 [65792/118836 (55%)] Loss: 12177.434570\n",
      "Train Epoch: 3876 [98560/118836 (83%)] Loss: 12168.418945\n",
      "    epoch          : 3876\n",
      "    loss           : 12217.15398960918\n",
      "    val_loss       : 12220.270522783652\n",
      "    val_log_likelihood: -12138.572050280449\n",
      "    val_log_marginal: -12146.46506405822\n",
      "Train Epoch: 3877 [256/118836 (0%)] Loss: 12207.309570\n",
      "Train Epoch: 3877 [33024/118836 (28%)] Loss: 12318.191406\n",
      "Train Epoch: 3877 [65792/118836 (55%)] Loss: 12257.703125\n",
      "Train Epoch: 3877 [98560/118836 (83%)] Loss: 12228.972656\n",
      "    epoch          : 3877\n",
      "    loss           : 12223.876382211538\n",
      "    val_loss       : 12220.37310233883\n",
      "    val_log_likelihood: -12140.42105481674\n",
      "    val_log_marginal: -12148.315768268552\n",
      "Train Epoch: 3878 [256/118836 (0%)] Loss: 12203.074219\n",
      "Train Epoch: 3878 [33024/118836 (28%)] Loss: 12282.540039\n",
      "Train Epoch: 3878 [65792/118836 (55%)] Loss: 12187.250977\n",
      "Train Epoch: 3878 [98560/118836 (83%)] Loss: 12242.437500\n",
      "    epoch          : 3878\n",
      "    loss           : 12217.335160611818\n",
      "    val_loss       : 12219.70281917223\n",
      "    val_log_likelihood: -12139.499544432383\n",
      "    val_log_marginal: -12147.395321758377\n",
      "Train Epoch: 3879 [256/118836 (0%)] Loss: 12160.796875\n",
      "Train Epoch: 3879 [33024/118836 (28%)] Loss: 12209.359375\n",
      "Train Epoch: 3879 [65792/118836 (55%)] Loss: 12165.400391\n",
      "Train Epoch: 3879 [98560/118836 (83%)] Loss: 12271.696289\n",
      "    epoch          : 3879\n",
      "    loss           : 12217.09494610732\n",
      "    val_loss       : 12220.206314062976\n",
      "    val_log_likelihood: -12141.860770943185\n",
      "    val_log_marginal: -12149.752579278042\n",
      "Train Epoch: 3880 [256/118836 (0%)] Loss: 12175.377930\n",
      "Train Epoch: 3880 [33024/118836 (28%)] Loss: 12251.817383\n",
      "Train Epoch: 3880 [65792/118836 (55%)] Loss: 12281.286133\n",
      "Train Epoch: 3880 [98560/118836 (83%)] Loss: 12199.056641\n",
      "    epoch          : 3880\n",
      "    loss           : 12216.295423807123\n",
      "    val_loss       : 12215.958363098413\n",
      "    val_log_likelihood: -12138.441740494467\n",
      "    val_log_marginal: -12146.332495020994\n",
      "Train Epoch: 3881 [256/118836 (0%)] Loss: 12247.151367\n",
      "Train Epoch: 3881 [33024/118836 (28%)] Loss: 12225.072266\n",
      "Train Epoch: 3881 [65792/118836 (55%)] Loss: 12194.773438\n",
      "Train Epoch: 3881 [98560/118836 (83%)] Loss: 12297.214844\n",
      "    epoch          : 3881\n",
      "    loss           : 12217.981022054642\n",
      "    val_loss       : 12213.655826012786\n",
      "    val_log_likelihood: -12139.241701884306\n",
      "    val_log_marginal: -12147.138304793478\n",
      "Train Epoch: 3882 [256/118836 (0%)] Loss: 12224.419922\n",
      "Train Epoch: 3882 [33024/118836 (28%)] Loss: 12217.302734\n",
      "Train Epoch: 3882 [65792/118836 (55%)] Loss: 12323.427734\n",
      "Train Epoch: 3882 [98560/118836 (83%)] Loss: 12334.232422\n",
      "    epoch          : 3882\n",
      "    loss           : 12215.244055811881\n",
      "    val_loss       : 12214.595573890758\n",
      "    val_log_likelihood: -12140.18160039935\n",
      "    val_log_marginal: -12148.07388142295\n",
      "Train Epoch: 3883 [256/118836 (0%)] Loss: 12208.589844\n",
      "Train Epoch: 3883 [33024/118836 (28%)] Loss: 12389.482422\n",
      "Train Epoch: 3883 [65792/118836 (55%)] Loss: 12337.798828\n",
      "Train Epoch: 3883 [98560/118836 (83%)] Loss: 12178.066406\n",
      "    epoch          : 3883\n",
      "    loss           : 12219.284850599668\n",
      "    val_loss       : 12217.66246524051\n",
      "    val_log_likelihood: -12139.312442650174\n",
      "    val_log_marginal: -12147.209647856258\n",
      "Train Epoch: 3884 [256/118836 (0%)] Loss: 12313.343750\n",
      "Train Epoch: 3884 [33024/118836 (28%)] Loss: 12231.157227\n",
      "Train Epoch: 3884 [65792/118836 (55%)] Loss: 12236.870117\n",
      "Train Epoch: 3884 [98560/118836 (83%)] Loss: 12239.301758\n",
      "    epoch          : 3884\n",
      "    loss           : 12213.167860027657\n",
      "    val_loss       : 12218.693709709254\n",
      "    val_log_likelihood: -12140.164048283705\n",
      "    val_log_marginal: -12148.05948156454\n",
      "Train Epoch: 3885 [256/118836 (0%)] Loss: 12275.888672\n",
      "Train Epoch: 3885 [33024/118836 (28%)] Loss: 12445.207031\n",
      "Train Epoch: 3885 [65792/118836 (55%)] Loss: 12215.164062\n",
      "Train Epoch: 3885 [98560/118836 (83%)] Loss: 12157.210938\n",
      "    epoch          : 3885\n",
      "    loss           : 12216.891508671939\n",
      "    val_loss       : 12216.88627343545\n",
      "    val_log_likelihood: -12139.788360893559\n",
      "    val_log_marginal: -12147.6863179134\n",
      "Train Epoch: 3886 [256/118836 (0%)] Loss: 12226.941406\n",
      "Train Epoch: 3886 [33024/118836 (28%)] Loss: 12287.648438\n",
      "Train Epoch: 3886 [65792/118836 (55%)] Loss: 12175.917969\n",
      "Train Epoch: 3886 [98560/118836 (83%)] Loss: 12273.615234\n",
      "    epoch          : 3886\n",
      "    loss           : 12215.47197871433\n",
      "    val_loss       : 12215.415590032431\n",
      "    val_log_likelihood: -12140.784316680883\n",
      "    val_log_marginal: -12148.679333508473\n",
      "Train Epoch: 3887 [256/118836 (0%)] Loss: 12224.685547\n",
      "Train Epoch: 3887 [33024/118836 (28%)] Loss: 12124.499023\n",
      "Train Epoch: 3887 [65792/118836 (55%)] Loss: 12247.134766\n",
      "Train Epoch: 3887 [98560/118836 (83%)] Loss: 12297.978516\n",
      "    epoch          : 3887\n",
      "    loss           : 12218.592398644283\n",
      "    val_loss       : 12217.3452113614\n",
      "    val_log_likelihood: -12141.079858095532\n",
      "    val_log_marginal: -12148.971630053184\n",
      "Train Epoch: 3888 [256/118836 (0%)] Loss: 12257.351562\n",
      "Train Epoch: 3888 [33024/118836 (28%)] Loss: 12186.804688\n",
      "Train Epoch: 3888 [65792/118836 (55%)] Loss: 12210.280273\n",
      "Train Epoch: 3888 [98560/118836 (83%)] Loss: 12268.085938\n",
      "    epoch          : 3888\n",
      "    loss           : 12217.849787886425\n",
      "    val_loss       : 12211.983891970063\n",
      "    val_log_likelihood: -12139.203423219085\n",
      "    val_log_marginal: -12147.097193432572\n",
      "Train Epoch: 3889 [256/118836 (0%)] Loss: 12200.805664\n",
      "Train Epoch: 3889 [33024/118836 (28%)] Loss: 12233.439453\n",
      "Train Epoch: 3889 [65792/118836 (55%)] Loss: 12236.312500\n",
      "Train Epoch: 3889 [98560/118836 (83%)] Loss: 12246.701172\n",
      "    epoch          : 3889\n",
      "    loss           : 12220.015064425663\n",
      "    val_loss       : 12221.874787991452\n",
      "    val_log_likelihood: -12140.768808965313\n",
      "    val_log_marginal: -12148.668271558423\n",
      "Train Epoch: 3890 [256/118836 (0%)] Loss: 12254.880859\n",
      "Train Epoch: 3890 [33024/118836 (28%)] Loss: 12183.310547\n",
      "Train Epoch: 3890 [65792/118836 (55%)] Loss: 12295.742188\n",
      "Train Epoch: 3890 [98560/118836 (83%)] Loss: 12259.711914\n",
      "    epoch          : 3890\n",
      "    loss           : 12216.90263437629\n",
      "    val_loss       : 12219.290533640851\n",
      "    val_log_likelihood: -12137.932001040375\n",
      "    val_log_marginal: -12145.826907925206\n",
      "Train Epoch: 3891 [256/118836 (0%)] Loss: 12417.515625\n",
      "Train Epoch: 3891 [33024/118836 (28%)] Loss: 12233.568359\n",
      "Train Epoch: 3891 [65792/118836 (55%)] Loss: 12156.801758\n",
      "Train Epoch: 3891 [98560/118836 (83%)] Loss: 12232.184570\n",
      "    epoch          : 3891\n",
      "    loss           : 12220.657216223377\n",
      "    val_loss       : 12217.825377207508\n",
      "    val_log_likelihood: -12140.00603594784\n",
      "    val_log_marginal: -12147.896992325825\n",
      "Train Epoch: 3892 [256/118836 (0%)] Loss: 12166.509766\n",
      "Train Epoch: 3892 [33024/118836 (28%)] Loss: 12256.799805\n",
      "Train Epoch: 3892 [65792/118836 (55%)] Loss: 12307.289062\n",
      "Train Epoch: 3892 [98560/118836 (83%)] Loss: 12220.760742\n",
      "    epoch          : 3892\n",
      "    loss           : 12218.504636773678\n",
      "    val_loss       : 12216.932412450824\n",
      "    val_log_likelihood: -12142.26359901649\n",
      "    val_log_marginal: -12150.151629521386\n",
      "Train Epoch: 3893 [256/118836 (0%)] Loss: 12184.278320\n",
      "Train Epoch: 3893 [33024/118836 (28%)] Loss: 12223.584961\n",
      "Train Epoch: 3893 [65792/118836 (55%)] Loss: 12217.929688\n",
      "Train Epoch: 3893 [98560/118836 (83%)] Loss: 12143.071289\n",
      "    epoch          : 3893\n",
      "    loss           : 12216.819460814722\n",
      "    val_loss       : 12218.835386629382\n",
      "    val_log_likelihood: -12140.959150285618\n",
      "    val_log_marginal: -12148.846020574956\n",
      "Train Epoch: 3894 [256/118836 (0%)] Loss: 12274.573242\n",
      "Train Epoch: 3894 [33024/118836 (28%)] Loss: 12302.546875\n",
      "Train Epoch: 3894 [65792/118836 (55%)] Loss: 12248.578125\n",
      "Train Epoch: 3894 [98560/118836 (83%)] Loss: 12195.901367\n",
      "    epoch          : 3894\n",
      "    loss           : 12216.491535165942\n",
      "    val_loss       : 12220.43223197612\n",
      "    val_log_likelihood: -12138.841587346205\n",
      "    val_log_marginal: -12146.732740193727\n",
      "Train Epoch: 3895 [256/118836 (0%)] Loss: 12181.384766\n",
      "Train Epoch: 3895 [33024/118836 (28%)] Loss: 12194.281250\n",
      "Train Epoch: 3895 [65792/118836 (55%)] Loss: 12220.522461\n",
      "Train Epoch: 3895 [98560/118836 (83%)] Loss: 12211.406250\n",
      "    epoch          : 3895\n",
      "    loss           : 12216.180500575112\n",
      "    val_loss       : 12219.22491979299\n",
      "    val_log_likelihood: -12140.830628198666\n",
      "    val_log_marginal: -12148.716685891692\n",
      "Train Epoch: 3896 [256/118836 (0%)] Loss: 12278.085938\n",
      "Train Epoch: 3896 [33024/118836 (28%)] Loss: 12154.597656\n",
      "Train Epoch: 3896 [65792/118836 (55%)] Loss: 12180.731445\n",
      "Train Epoch: 3896 [98560/118836 (83%)] Loss: 12317.867188\n",
      "    epoch          : 3896\n",
      "    loss           : 12217.53741906405\n",
      "    val_loss       : 12219.7620185518\n",
      "    val_log_likelihood: -12138.884789372674\n",
      "    val_log_marginal: -12146.77982278772\n",
      "Train Epoch: 3897 [256/118836 (0%)] Loss: 12326.607422\n",
      "Train Epoch: 3897 [33024/118836 (28%)] Loss: 12180.513672\n",
      "Train Epoch: 3897 [65792/118836 (55%)] Loss: 12293.705078\n",
      "Train Epoch: 3897 [98560/118836 (83%)] Loss: 12188.401367\n",
      "    epoch          : 3897\n",
      "    loss           : 12215.905210756564\n",
      "    val_loss       : 12221.605418793642\n",
      "    val_log_likelihood: -12140.149006959522\n",
      "    val_log_marginal: -12148.03703534102\n",
      "Train Epoch: 3898 [256/118836 (0%)] Loss: 12298.477539\n",
      "Train Epoch: 3898 [33024/118836 (28%)] Loss: 12197.774414\n",
      "Train Epoch: 3898 [65792/118836 (55%)] Loss: 12208.211914\n",
      "Train Epoch: 3898 [98560/118836 (83%)] Loss: 12175.466797\n",
      "    epoch          : 3898\n",
      "    loss           : 12218.657153380893\n",
      "    val_loss       : 12217.877862030846\n",
      "    val_log_likelihood: -12139.576577524038\n",
      "    val_log_marginal: -12147.46626292307\n",
      "Train Epoch: 3899 [256/118836 (0%)] Loss: 12227.318359\n",
      "Train Epoch: 3899 [33024/118836 (28%)] Loss: 12269.228516\n",
      "Train Epoch: 3899 [65792/118836 (55%)] Loss: 12225.751953\n",
      "Train Epoch: 3899 [98560/118836 (83%)] Loss: 12267.806641\n",
      "    epoch          : 3899\n",
      "    loss           : 12217.526709024762\n",
      "    val_loss       : 12219.214861496062\n",
      "    val_log_likelihood: -12139.612951528898\n",
      "    val_log_marginal: -12147.500145431175\n",
      "Train Epoch: 3900 [256/118836 (0%)] Loss: 12225.071289\n",
      "Train Epoch: 3900 [33024/118836 (28%)] Loss: 12332.103516\n",
      "Train Epoch: 3900 [65792/118836 (55%)] Loss: 12127.492188\n",
      "Train Epoch: 3900 [98560/118836 (83%)] Loss: 12257.256836\n",
      "    epoch          : 3900\n",
      "    loss           : 12215.783715396246\n",
      "    val_loss       : 12216.658843355539\n",
      "    val_log_likelihood: -12139.986188869934\n",
      "    val_log_marginal: -12147.873021575124\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch3900.pth ...\n",
      "Train Epoch: 3901 [256/118836 (0%)] Loss: 12287.687500\n",
      "Train Epoch: 3901 [33024/118836 (28%)] Loss: 12189.328125\n",
      "Train Epoch: 3901 [65792/118836 (55%)] Loss: 12288.281250\n",
      "Train Epoch: 3901 [98560/118836 (83%)] Loss: 12215.328125\n",
      "    epoch          : 3901\n",
      "    loss           : 12217.205560025071\n",
      "    val_loss       : 12217.737271336193\n",
      "    val_log_likelihood: -12138.7612943613\n",
      "    val_log_marginal: -12146.64978459128\n",
      "Train Epoch: 3902 [256/118836 (0%)] Loss: 12352.056641\n",
      "Train Epoch: 3902 [33024/118836 (28%)] Loss: 12196.308594\n",
      "Train Epoch: 3902 [65792/118836 (55%)] Loss: 12199.974609\n",
      "Train Epoch: 3902 [98560/118836 (83%)] Loss: 12201.267578\n",
      "    epoch          : 3902\n",
      "    loss           : 12218.566787182071\n",
      "    val_loss       : 12212.915997263477\n",
      "    val_log_likelihood: -12138.900625032311\n",
      "    val_log_marginal: -12146.787528290157\n",
      "Train Epoch: 3903 [256/118836 (0%)] Loss: 12193.164062\n",
      "Train Epoch: 3903 [33024/118836 (28%)] Loss: 12291.893555\n",
      "Train Epoch: 3903 [65792/118836 (55%)] Loss: 12262.735352\n",
      "Train Epoch: 3903 [98560/118836 (83%)] Loss: 12205.931641\n",
      "    epoch          : 3903\n",
      "    loss           : 12220.42626751189\n",
      "    val_loss       : 12221.806622194063\n",
      "    val_log_likelihood: -12142.071309256102\n",
      "    val_log_marginal: -12149.95985952814\n",
      "Train Epoch: 3904 [256/118836 (0%)] Loss: 12235.091797\n",
      "Train Epoch: 3904 [33024/118836 (28%)] Loss: 12355.773438\n",
      "Train Epoch: 3904 [65792/118836 (55%)] Loss: 12324.910156\n",
      "Train Epoch: 3904 [98560/118836 (83%)] Loss: 12185.984375\n",
      "    epoch          : 3904\n",
      "    loss           : 12216.627771854322\n",
      "    val_loss       : 12220.48999508208\n",
      "    val_log_likelihood: -12139.364950856854\n",
      "    val_log_marginal: -12147.248217661401\n",
      "Train Epoch: 3905 [256/118836 (0%)] Loss: 12201.794922\n",
      "Train Epoch: 3905 [33024/118836 (28%)] Loss: 12244.140625\n",
      "Train Epoch: 3905 [65792/118836 (55%)] Loss: 12274.793945\n",
      "Train Epoch: 3905 [98560/118836 (83%)] Loss: 12272.322266\n",
      "    epoch          : 3905\n",
      "    loss           : 12219.678850192566\n",
      "    val_loss       : 12216.770105864676\n",
      "    val_log_likelihood: -12140.23993518662\n",
      "    val_log_marginal: -12148.123631581917\n",
      "Train Epoch: 3906 [256/118836 (0%)] Loss: 12180.196289\n",
      "Train Epoch: 3906 [33024/118836 (28%)] Loss: 12146.435547\n",
      "Train Epoch: 3906 [65792/118836 (55%)] Loss: 12284.115234\n",
      "Train Epoch: 3906 [98560/118836 (83%)] Loss: 12200.390625\n",
      "    epoch          : 3906\n",
      "    loss           : 12218.037992077647\n",
      "    val_loss       : 12216.438519652482\n",
      "    val_log_likelihood: -12138.858691648575\n",
      "    val_log_marginal: -12146.742919542465\n",
      "Train Epoch: 3907 [256/118836 (0%)] Loss: 12265.884766\n",
      "Train Epoch: 3907 [33024/118836 (28%)] Loss: 12207.515625\n",
      "Train Epoch: 3907 [65792/118836 (55%)] Loss: 12142.262695\n",
      "Train Epoch: 3907 [98560/118836 (83%)] Loss: 12294.799805\n",
      "    epoch          : 3907\n",
      "    loss           : 12219.392084916512\n",
      "    val_loss       : 12222.51365229788\n",
      "    val_log_likelihood: -12141.996122021039\n",
      "    val_log_marginal: -12149.879273039393\n",
      "Train Epoch: 3908 [256/118836 (0%)] Loss: 12212.480469\n",
      "Train Epoch: 3908 [33024/118836 (28%)] Loss: 12159.333984\n",
      "Train Epoch: 3908 [65792/118836 (55%)] Loss: 12179.906250\n",
      "Train Epoch: 3908 [98560/118836 (83%)] Loss: 12260.955078\n",
      "    epoch          : 3908\n",
      "    loss           : 12222.557034319427\n",
      "    val_loss       : 12212.347117697298\n",
      "    val_log_likelihood: -12138.47690869908\n",
      "    val_log_marginal: -12146.361514912885\n",
      "Train Epoch: 3909 [256/118836 (0%)] Loss: 12209.816406\n",
      "Train Epoch: 3909 [33024/118836 (28%)] Loss: 12215.577148\n",
      "Train Epoch: 3909 [65792/118836 (55%)] Loss: 12166.425781\n",
      "Train Epoch: 3909 [98560/118836 (83%)] Loss: 12220.592773\n",
      "    epoch          : 3909\n",
      "    loss           : 12219.55134311673\n",
      "    val_loss       : 12217.783076294578\n",
      "    val_log_likelihood: -12140.145905384099\n",
      "    val_log_marginal: -12148.030781417569\n",
      "Train Epoch: 3910 [256/118836 (0%)] Loss: 12132.759766\n",
      "Train Epoch: 3910 [33024/118836 (28%)] Loss: 12308.125977\n",
      "Train Epoch: 3910 [65792/118836 (55%)] Loss: 12242.849609\n",
      "Train Epoch: 3910 [98560/118836 (83%)] Loss: 12296.969727\n",
      "    epoch          : 3910\n",
      "    loss           : 12218.813382702647\n",
      "    val_loss       : 12215.261933962302\n",
      "    val_log_likelihood: -12141.181994739973\n",
      "    val_log_marginal: -12149.067645260231\n",
      "Train Epoch: 3911 [256/118836 (0%)] Loss: 12274.561523\n",
      "Train Epoch: 3911 [33024/118836 (28%)] Loss: 12209.496094\n",
      "Train Epoch: 3911 [65792/118836 (55%)] Loss: 12225.725586\n",
      "Train Epoch: 3911 [98560/118836 (83%)] Loss: 12281.045898\n",
      "    epoch          : 3911\n",
      "    loss           : 12215.618506707506\n",
      "    val_loss       : 12218.79697839666\n",
      "    val_log_likelihood: -12138.786556231908\n",
      "    val_log_marginal: -12146.676622685209\n",
      "Train Epoch: 3912 [256/118836 (0%)] Loss: 12318.228516\n",
      "Train Epoch: 3912 [33024/118836 (28%)] Loss: 12222.364258\n",
      "Train Epoch: 3912 [65792/118836 (55%)] Loss: 12199.277344\n",
      "Train Epoch: 3912 [98560/118836 (83%)] Loss: 12222.651367\n",
      "    epoch          : 3912\n",
      "    loss           : 12220.221745437862\n",
      "    val_loss       : 12218.001709750482\n",
      "    val_log_likelihood: -12139.93928333721\n",
      "    val_log_marginal: -12147.827333827916\n",
      "Train Epoch: 3913 [256/118836 (0%)] Loss: 12339.192383\n",
      "Train Epoch: 3913 [33024/118836 (28%)] Loss: 12194.022461\n",
      "Train Epoch: 3913 [65792/118836 (55%)] Loss: 12221.437500\n",
      "Train Epoch: 3913 [98560/118836 (83%)] Loss: 12381.423828\n",
      "    epoch          : 3913\n",
      "    loss           : 12216.888599501137\n",
      "    val_loss       : 12220.683146595626\n",
      "    val_log_likelihood: -12142.587612437965\n",
      "    val_log_marginal: -12150.476121403311\n",
      "Train Epoch: 3914 [256/118836 (0%)] Loss: 12281.983398\n",
      "Train Epoch: 3914 [33024/118836 (28%)] Loss: 12274.615234\n",
      "Train Epoch: 3914 [65792/118836 (55%)] Loss: 12321.113281\n",
      "Train Epoch: 3914 [98560/118836 (83%)] Loss: 12189.958984\n",
      "    epoch          : 3914\n",
      "    loss           : 12217.863816622725\n",
      "    val_loss       : 12216.460595382114\n",
      "    val_log_likelihood: -12139.794840292854\n",
      "    val_log_marginal: -12147.687316705413\n",
      "Train Epoch: 3915 [256/118836 (0%)] Loss: 12286.246094\n",
      "Train Epoch: 3915 [33024/118836 (28%)] Loss: 12252.435547\n",
      "Train Epoch: 3915 [65792/118836 (55%)] Loss: 12190.382812\n",
      "Train Epoch: 3915 [98560/118836 (83%)] Loss: 12190.722656\n",
      "    epoch          : 3915\n",
      "    loss           : 12220.112497576769\n",
      "    val_loss       : 12217.965025844002\n",
      "    val_log_likelihood: -12140.444156456782\n",
      "    val_log_marginal: -12148.336944936324\n",
      "Train Epoch: 3916 [256/118836 (0%)] Loss: 12165.189453\n",
      "Train Epoch: 3916 [33024/118836 (28%)] Loss: 12287.720703\n",
      "Train Epoch: 3916 [65792/118836 (55%)] Loss: 12216.154297\n",
      "Train Epoch: 3916 [98560/118836 (83%)] Loss: 12307.768555\n",
      "    epoch          : 3916\n",
      "    loss           : 12216.987460258995\n",
      "    val_loss       : 12216.467032126144\n",
      "    val_log_likelihood: -12140.42449095973\n",
      "    val_log_marginal: -12148.310598705511\n",
      "Train Epoch: 3917 [256/118836 (0%)] Loss: 12329.283203\n",
      "Train Epoch: 3917 [33024/118836 (28%)] Loss: 12201.673828\n",
      "Train Epoch: 3917 [65792/118836 (55%)] Loss: 12198.223633\n",
      "Train Epoch: 3917 [98560/118836 (83%)] Loss: 12273.476562\n",
      "    epoch          : 3917\n",
      "    loss           : 12219.185387587882\n",
      "    val_loss       : 12220.123033920847\n",
      "    val_log_likelihood: -12141.99551023573\n",
      "    val_log_marginal: -12149.885819357603\n",
      "Train Epoch: 3918 [256/118836 (0%)] Loss: 12204.074219\n",
      "Train Epoch: 3918 [33024/118836 (28%)] Loss: 12270.366211\n",
      "Train Epoch: 3918 [65792/118836 (55%)] Loss: 12220.584961\n",
      "Train Epoch: 3918 [98560/118836 (83%)] Loss: 12234.811523\n",
      "    epoch          : 3918\n",
      "    loss           : 12217.075908873552\n",
      "    val_loss       : 12218.10548622524\n",
      "    val_log_likelihood: -12139.174768177472\n",
      "    val_log_marginal: -12147.063086920472\n",
      "Train Epoch: 3919 [256/118836 (0%)] Loss: 12402.902344\n",
      "Train Epoch: 3919 [33024/118836 (28%)] Loss: 12294.121094\n",
      "Train Epoch: 3919 [65792/118836 (55%)] Loss: 12142.999023\n",
      "Train Epoch: 3919 [98560/118836 (83%)] Loss: 12394.624023\n",
      "    epoch          : 3919\n",
      "    loss           : 12218.177500452337\n",
      "    val_loss       : 12217.184849414045\n",
      "    val_log_likelihood: -12140.302311601788\n",
      "    val_log_marginal: -12148.189382788772\n",
      "Train Epoch: 3920 [256/118836 (0%)] Loss: 12189.468750\n",
      "Train Epoch: 3920 [33024/118836 (28%)] Loss: 12300.457031\n",
      "Train Epoch: 3920 [65792/118836 (55%)] Loss: 12194.235352\n",
      "Train Epoch: 3920 [98560/118836 (83%)] Loss: 12186.677734\n",
      "    epoch          : 3920\n",
      "    loss           : 12219.519189412738\n",
      "    val_loss       : 12221.0398154197\n",
      "    val_log_likelihood: -12139.192611242504\n",
      "    val_log_marginal: -12147.084472803035\n",
      "Train Epoch: 3921 [256/118836 (0%)] Loss: 12203.977539\n",
      "Train Epoch: 3921 [33024/118836 (28%)] Loss: 12261.196289\n",
      "Train Epoch: 3921 [65792/118836 (55%)] Loss: 12285.283203\n",
      "Train Epoch: 3921 [98560/118836 (83%)] Loss: 12191.121094\n",
      "    epoch          : 3921\n",
      "    loss           : 12216.559065957144\n",
      "    val_loss       : 12218.691374541024\n",
      "    val_log_likelihood: -12141.029530151469\n",
      "    val_log_marginal: -12148.920436187174\n",
      "Train Epoch: 3922 [256/118836 (0%)] Loss: 12404.556641\n",
      "Train Epoch: 3922 [33024/118836 (28%)] Loss: 12256.466797\n",
      "Train Epoch: 3922 [65792/118836 (55%)] Loss: 12147.021484\n",
      "Train Epoch: 3922 [98560/118836 (83%)] Loss: 12298.003906\n",
      "    epoch          : 3922\n",
      "    loss           : 12220.923876751189\n",
      "    val_loss       : 12220.22286990243\n",
      "    val_log_likelihood: -12140.031995386165\n",
      "    val_log_marginal: -12147.927626925348\n",
      "Train Epoch: 3923 [256/118836 (0%)] Loss: 12185.339844\n",
      "Train Epoch: 3923 [33024/118836 (28%)] Loss: 12223.256836\n",
      "Train Epoch: 3923 [65792/118836 (55%)] Loss: 12214.180664\n",
      "Train Epoch: 3923 [98560/118836 (83%)] Loss: 12268.843750\n",
      "    epoch          : 3923\n",
      "    loss           : 12213.495543514786\n",
      "    val_loss       : 12223.968811197885\n",
      "    val_log_likelihood: -12140.506299595483\n",
      "    val_log_marginal: -12148.391407623076\n",
      "Train Epoch: 3924 [256/118836 (0%)] Loss: 12286.439453\n",
      "Train Epoch: 3924 [33024/118836 (28%)] Loss: 12245.196289\n",
      "Train Epoch: 3924 [65792/118836 (55%)] Loss: 12415.268555\n",
      "Train Epoch: 3924 [98560/118836 (83%)] Loss: 12258.057617\n",
      "    epoch          : 3924\n",
      "    loss           : 12217.55205958566\n",
      "    val_loss       : 12218.758654690604\n",
      "    val_log_likelihood: -12139.903109491315\n",
      "    val_log_marginal: -12147.785383867635\n",
      "Train Epoch: 3925 [256/118836 (0%)] Loss: 12262.249023\n",
      "Train Epoch: 3925 [33024/118836 (28%)] Loss: 12193.466797\n",
      "Train Epoch: 3925 [65792/118836 (55%)] Loss: 12225.927734\n",
      "Train Epoch: 3925 [98560/118836 (83%)] Loss: 12173.140625\n",
      "    epoch          : 3925\n",
      "    loss           : 12213.642813178505\n",
      "    val_loss       : 12217.686585415118\n",
      "    val_log_likelihood: -12140.090859084212\n",
      "    val_log_marginal: -12147.977596228966\n",
      "Train Epoch: 3926 [256/118836 (0%)] Loss: 12321.257812\n",
      "Train Epoch: 3926 [33024/118836 (28%)] Loss: 12362.120117\n",
      "Train Epoch: 3926 [65792/118836 (55%)] Loss: 12202.933594\n",
      "Train Epoch: 3926 [98560/118836 (83%)] Loss: 12246.708984\n",
      "    epoch          : 3926\n",
      "    loss           : 12215.797275641025\n",
      "    val_loss       : 12220.046460259831\n",
      "    val_log_likelihood: -12138.382951755066\n",
      "    val_log_marginal: -12146.276745389236\n",
      "Train Epoch: 3927 [256/118836 (0%)] Loss: 12248.738281\n",
      "Train Epoch: 3927 [33024/118836 (28%)] Loss: 12209.511719\n",
      "Train Epoch: 3927 [65792/118836 (55%)] Loss: 12265.738281\n",
      "Train Epoch: 3927 [98560/118836 (83%)] Loss: 12181.910156\n",
      "    epoch          : 3927\n",
      "    loss           : 12216.813525350237\n",
      "    val_loss       : 12219.839118869588\n",
      "    val_log_likelihood: -12139.239728404156\n",
      "    val_log_marginal: -12147.125154881789\n",
      "Train Epoch: 3928 [256/118836 (0%)] Loss: 12214.240234\n",
      "Train Epoch: 3928 [33024/118836 (28%)] Loss: 12212.408203\n",
      "Train Epoch: 3928 [65792/118836 (55%)] Loss: 12188.753906\n",
      "Train Epoch: 3928 [98560/118836 (83%)] Loss: 12149.662109\n",
      "    epoch          : 3928\n",
      "    loss           : 12215.806163571393\n",
      "    val_loss       : 12219.803816776528\n",
      "    val_log_likelihood: -12142.178425157672\n",
      "    val_log_marginal: -12150.070411305398\n",
      "Train Epoch: 3929 [256/118836 (0%)] Loss: 12173.182617\n",
      "Train Epoch: 3929 [33024/118836 (28%)] Loss: 12226.685547\n",
      "Train Epoch: 3929 [65792/118836 (55%)] Loss: 12158.162109\n",
      "Train Epoch: 3929 [98560/118836 (83%)] Loss: 12217.449219\n",
      "    epoch          : 3929\n",
      "    loss           : 12216.761031844499\n",
      "    val_loss       : 12215.969922051809\n",
      "    val_log_likelihood: -12140.851241340984\n",
      "    val_log_marginal: -12148.739567223342\n",
      "Train Epoch: 3930 [256/118836 (0%)] Loss: 12270.190430\n",
      "Train Epoch: 3930 [33024/118836 (28%)] Loss: 12146.250000\n",
      "Train Epoch: 3930 [65792/118836 (55%)] Loss: 12170.871094\n",
      "Train Epoch: 3930 [98560/118836 (83%)] Loss: 12195.617188\n",
      "    epoch          : 3930\n",
      "    loss           : 12220.401436168839\n",
      "    val_loss       : 12219.044883474266\n",
      "    val_log_likelihood: -12137.910931845792\n",
      "    val_log_marginal: -12145.80596877637\n",
      "Train Epoch: 3931 [256/118836 (0%)] Loss: 12177.824219\n",
      "Train Epoch: 3931 [33024/118836 (28%)] Loss: 12240.065430\n",
      "Train Epoch: 3931 [65792/118836 (55%)] Loss: 12217.674805\n",
      "Train Epoch: 3931 [98560/118836 (83%)] Loss: 12195.832031\n",
      "    epoch          : 3931\n",
      "    loss           : 12218.535188882857\n",
      "    val_loss       : 12221.069852995435\n",
      "    val_log_likelihood: -12139.825925351532\n",
      "    val_log_marginal: -12147.717619632396\n",
      "Train Epoch: 3932 [256/118836 (0%)] Loss: 12164.386719\n",
      "Train Epoch: 3932 [33024/118836 (28%)] Loss: 12325.295898\n",
      "Train Epoch: 3932 [65792/118836 (55%)] Loss: 12252.358398\n",
      "Train Epoch: 3932 [98560/118836 (83%)] Loss: 12260.792969\n",
      "    epoch          : 3932\n",
      "    loss           : 12220.40675241677\n",
      "    val_loss       : 12218.2948361458\n",
      "    val_log_likelihood: -12138.890526293682\n",
      "    val_log_marginal: -12146.777258753415\n",
      "Train Epoch: 3933 [256/118836 (0%)] Loss: 12289.167969\n",
      "Train Epoch: 3933 [33024/118836 (28%)] Loss: 12189.991211\n",
      "Train Epoch: 3933 [65792/118836 (55%)] Loss: 12184.171875\n",
      "Train Epoch: 3933 [98560/118836 (83%)] Loss: 12275.035156\n",
      "    epoch          : 3933\n",
      "    loss           : 12215.739492542907\n",
      "    val_loss       : 12222.480662097063\n",
      "    val_log_likelihood: -12138.673514881877\n",
      "    val_log_marginal: -12146.56111257258\n",
      "Train Epoch: 3934 [256/118836 (0%)] Loss: 12313.965820\n",
      "Train Epoch: 3934 [33024/118836 (28%)] Loss: 12232.687500\n",
      "Train Epoch: 3934 [65792/118836 (55%)] Loss: 12341.183594\n",
      "Train Epoch: 3934 [98560/118836 (83%)] Loss: 12257.980469\n",
      "    epoch          : 3934\n",
      "    loss           : 12214.923136534584\n",
      "    val_loss       : 12219.509394876759\n",
      "    val_log_likelihood: -12139.649916963916\n",
      "    val_log_marginal: -12147.529064089107\n",
      "Train Epoch: 3935 [256/118836 (0%)] Loss: 12270.935547\n",
      "Train Epoch: 3935 [33024/118836 (28%)] Loss: 12237.187500\n",
      "Train Epoch: 3935 [65792/118836 (55%)] Loss: 12184.114258\n",
      "Train Epoch: 3935 [98560/118836 (83%)] Loss: 12186.162109\n",
      "    epoch          : 3935\n",
      "    loss           : 12214.085513434398\n",
      "    val_loss       : 12218.081751941465\n",
      "    val_log_likelihood: -12138.514750213244\n",
      "    val_log_marginal: -12146.403601684291\n",
      "Train Epoch: 3936 [256/118836 (0%)] Loss: 12189.608398\n",
      "Train Epoch: 3936 [33024/118836 (28%)] Loss: 12255.929688\n",
      "Train Epoch: 3936 [65792/118836 (55%)] Loss: 12353.815430\n",
      "Train Epoch: 3936 [98560/118836 (83%)] Loss: 12223.375977\n",
      "    epoch          : 3936\n",
      "    loss           : 12217.825073020058\n",
      "    val_loss       : 12214.10333115156\n",
      "    val_log_likelihood: -12138.88737593052\n",
      "    val_log_marginal: -12146.77709479024\n",
      "Train Epoch: 3937 [256/118836 (0%)] Loss: 12211.550781\n",
      "Train Epoch: 3937 [33024/118836 (28%)] Loss: 12244.693359\n",
      "Train Epoch: 3937 [65792/118836 (55%)] Loss: 12248.757812\n",
      "Train Epoch: 3937 [98560/118836 (83%)] Loss: 12199.208984\n",
      "    epoch          : 3937\n",
      "    loss           : 12217.933347872724\n",
      "    val_loss       : 12224.059763512681\n",
      "    val_log_likelihood: -12139.59747434605\n",
      "    val_log_marginal: -12147.485315424394\n",
      "Train Epoch: 3938 [256/118836 (0%)] Loss: 12276.713867\n",
      "Train Epoch: 3938 [33024/118836 (28%)] Loss: 12307.585938\n",
      "Train Epoch: 3938 [65792/118836 (55%)] Loss: 12284.778320\n",
      "Train Epoch: 3938 [98560/118836 (83%)] Loss: 12326.708984\n",
      "    epoch          : 3938\n",
      "    loss           : 12223.141861332972\n",
      "    val_loss       : 12216.634509142228\n",
      "    val_log_likelihood: -12140.213448614557\n",
      "    val_log_marginal: -12148.100593072179\n",
      "Train Epoch: 3939 [256/118836 (0%)] Loss: 12260.919922\n",
      "Train Epoch: 3939 [33024/118836 (28%)] Loss: 12188.007812\n",
      "Train Epoch: 3939 [65792/118836 (55%)] Loss: 12178.422852\n",
      "Train Epoch: 3939 [98560/118836 (83%)] Loss: 12266.734375\n",
      "    epoch          : 3939\n",
      "    loss           : 12218.410310206007\n",
      "    val_loss       : 12217.607098156925\n",
      "    val_log_likelihood: -12138.848105678764\n",
      "    val_log_marginal: -12146.730462739477\n",
      "Train Epoch: 3940 [256/118836 (0%)] Loss: 12230.703125\n",
      "Train Epoch: 3940 [33024/118836 (28%)] Loss: 12187.500000\n",
      "Train Epoch: 3940 [65792/118836 (55%)] Loss: 12253.269531\n",
      "Train Epoch: 3940 [98560/118836 (83%)] Loss: 12314.546875\n",
      "    epoch          : 3940\n",
      "    loss           : 12221.68982484879\n",
      "    val_loss       : 12217.73713688189\n",
      "    val_log_likelihood: -12140.66824774478\n",
      "    val_log_marginal: -12148.551204782896\n",
      "Train Epoch: 3941 [256/118836 (0%)] Loss: 12266.035156\n",
      "Train Epoch: 3941 [33024/118836 (28%)] Loss: 12345.160156\n",
      "Train Epoch: 3941 [65792/118836 (55%)] Loss: 12231.558594\n",
      "Train Epoch: 3941 [98560/118836 (83%)] Loss: 12404.621094\n",
      "    epoch          : 3941\n",
      "    loss           : 12216.013330360835\n",
      "    val_loss       : 12217.602893686362\n",
      "    val_log_likelihood: -12137.40329478779\n",
      "    val_log_marginal: -12145.293833570651\n",
      "Train Epoch: 3942 [256/118836 (0%)] Loss: 12093.035156\n",
      "Train Epoch: 3942 [33024/118836 (28%)] Loss: 12238.951172\n",
      "Train Epoch: 3942 [65792/118836 (55%)] Loss: 12209.748047\n",
      "Train Epoch: 3942 [98560/118836 (83%)] Loss: 12200.825195\n",
      "    epoch          : 3942\n",
      "    loss           : 12215.999001143764\n",
      "    val_loss       : 12214.89402917303\n",
      "    val_log_likelihood: -12141.341779266182\n",
      "    val_log_marginal: -12149.22682756936\n",
      "Train Epoch: 3943 [256/118836 (0%)] Loss: 12229.123047\n",
      "Train Epoch: 3943 [33024/118836 (28%)] Loss: 12253.201172\n",
      "Train Epoch: 3943 [65792/118836 (55%)] Loss: 12259.707031\n",
      "Train Epoch: 3943 [98560/118836 (83%)] Loss: 12237.722656\n",
      "    epoch          : 3943\n",
      "    loss           : 12219.856085058675\n",
      "    val_loss       : 12219.729566958393\n",
      "    val_log_likelihood: -12140.074008090363\n",
      "    val_log_marginal: -12147.955460341533\n",
      "Train Epoch: 3944 [256/118836 (0%)] Loss: 12173.896484\n",
      "Train Epoch: 3944 [33024/118836 (28%)] Loss: 12167.054688\n",
      "Train Epoch: 3944 [65792/118836 (55%)] Loss: 12186.584961\n",
      "Train Epoch: 3944 [98560/118836 (83%)] Loss: 12169.698242\n",
      "    epoch          : 3944\n",
      "    loss           : 12216.396040600444\n",
      "    val_loss       : 12214.9029698918\n",
      "    val_log_likelihood: -12139.82893435949\n",
      "    val_log_marginal: -12147.718160175746\n",
      "Train Epoch: 3945 [256/118836 (0%)] Loss: 12224.859375\n",
      "Train Epoch: 3945 [33024/118836 (28%)] Loss: 12248.640625\n",
      "Train Epoch: 3945 [65792/118836 (55%)] Loss: 12249.236328\n",
      "Train Epoch: 3945 [98560/118836 (83%)] Loss: 12228.203125\n",
      "    epoch          : 3945\n",
      "    loss           : 12214.540710782414\n",
      "    val_loss       : 12219.33819600119\n",
      "    val_log_likelihood: -12142.029638873812\n",
      "    val_log_marginal: -12149.923516416366\n",
      "Train Epoch: 3946 [256/118836 (0%)] Loss: 12186.482422\n",
      "Train Epoch: 3946 [33024/118836 (28%)] Loss: 12353.158203\n",
      "Train Epoch: 3946 [65792/118836 (55%)] Loss: 12332.376953\n",
      "Train Epoch: 3946 [98560/118836 (83%)] Loss: 12271.650391\n",
      "    epoch          : 3946\n",
      "    loss           : 12218.208292946134\n",
      "    val_loss       : 12217.093744175128\n",
      "    val_log_likelihood: -12140.993727867814\n",
      "    val_log_marginal: -12148.886573464522\n",
      "Train Epoch: 3947 [256/118836 (0%)] Loss: 12203.488281\n",
      "Train Epoch: 3947 [33024/118836 (28%)] Loss: 12217.724609\n",
      "Train Epoch: 3947 [65792/118836 (55%)] Loss: 12391.511719\n",
      "Train Epoch: 3947 [98560/118836 (83%)] Loss: 12206.311523\n",
      "    epoch          : 3947\n",
      "    loss           : 12216.774296616522\n",
      "    val_loss       : 12217.15620394642\n",
      "    val_log_likelihood: -12138.401296267577\n",
      "    val_log_marginal: -12146.285085960617\n",
      "Train Epoch: 3948 [256/118836 (0%)] Loss: 12305.807617\n",
      "Train Epoch: 3948 [33024/118836 (28%)] Loss: 12202.746094\n",
      "Train Epoch: 3948 [65792/118836 (55%)] Loss: 12242.974609\n",
      "Train Epoch: 3948 [98560/118836 (83%)] Loss: 12226.143555\n",
      "    epoch          : 3948\n",
      "    loss           : 12217.34984782103\n",
      "    val_loss       : 12217.245344394365\n",
      "    val_log_likelihood: -12139.235632657414\n",
      "    val_log_marginal: -12147.123413436953\n",
      "Train Epoch: 3949 [256/118836 (0%)] Loss: 12295.093750\n",
      "Train Epoch: 3949 [33024/118836 (28%)] Loss: 12223.523438\n",
      "Train Epoch: 3949 [65792/118836 (55%)] Loss: 12238.272461\n",
      "Train Epoch: 3949 [98560/118836 (83%)] Loss: 12243.867188\n",
      "    epoch          : 3949\n",
      "    loss           : 12214.519836254136\n",
      "    val_loss       : 12217.535865558448\n",
      "    val_log_likelihood: -12139.980079094292\n",
      "    val_log_marginal: -12147.867904851437\n",
      "Train Epoch: 3950 [256/118836 (0%)] Loss: 12216.577148\n",
      "Train Epoch: 3950 [33024/118836 (28%)] Loss: 12140.529297\n",
      "Train Epoch: 3950 [65792/118836 (55%)] Loss: 12251.448242\n",
      "Train Epoch: 3950 [98560/118836 (83%)] Loss: 12262.134766\n",
      "    epoch          : 3950\n",
      "    loss           : 12217.57986811156\n",
      "    val_loss       : 12214.60032279645\n",
      "    val_log_likelihood: -12138.835660605355\n",
      "    val_log_marginal: -12146.723009382657\n",
      "Train Epoch: 3951 [256/118836 (0%)] Loss: 12294.970703\n",
      "Train Epoch: 3951 [33024/118836 (28%)] Loss: 12227.750977\n",
      "Train Epoch: 3951 [65792/118836 (55%)] Loss: 12276.367188\n",
      "Train Epoch: 3951 [98560/118836 (83%)] Loss: 12235.961914\n",
      "    epoch          : 3951\n",
      "    loss           : 12219.534300526004\n",
      "    val_loss       : 12214.290147619846\n",
      "    val_log_likelihood: -12139.574456872931\n",
      "    val_log_marginal: -12147.462333584539\n",
      "Train Epoch: 3952 [256/118836 (0%)] Loss: 12138.574219\n",
      "Train Epoch: 3952 [33024/118836 (28%)] Loss: 12304.757812\n",
      "Train Epoch: 3952 [65792/118836 (55%)] Loss: 12216.722656\n",
      "Train Epoch: 3952 [98560/118836 (83%)] Loss: 12168.389648\n",
      "    epoch          : 3952\n",
      "    loss           : 12219.512654279104\n",
      "    val_loss       : 12212.998133140118\n",
      "    val_log_likelihood: -12138.743647901158\n",
      "    val_log_marginal: -12146.634110359246\n",
      "Train Epoch: 3953 [256/118836 (0%)] Loss: 12231.100586\n",
      "Train Epoch: 3953 [33024/118836 (28%)] Loss: 12245.034180\n",
      "Train Epoch: 3953 [65792/118836 (55%)] Loss: 12261.375000\n",
      "Train Epoch: 3953 [98560/118836 (83%)] Loss: 12264.734375\n",
      "    epoch          : 3953\n",
      "    loss           : 12216.374697419096\n",
      "    val_loss       : 12220.033943791716\n",
      "    val_log_likelihood: -12140.696306671318\n",
      "    val_log_marginal: -12148.583919425184\n",
      "Train Epoch: 3954 [256/118836 (0%)] Loss: 12262.683594\n",
      "Train Epoch: 3954 [33024/118836 (28%)] Loss: 12257.378906\n",
      "Train Epoch: 3954 [65792/118836 (55%)] Loss: 12196.505859\n",
      "Train Epoch: 3954 [98560/118836 (83%)] Loss: 12264.367188\n",
      "    epoch          : 3954\n",
      "    loss           : 12216.939047152864\n",
      "    val_loss       : 12217.606503413665\n",
      "    val_log_likelihood: -12139.486887568495\n",
      "    val_log_marginal: -12147.372277876579\n",
      "Train Epoch: 3955 [256/118836 (0%)] Loss: 12263.355469\n",
      "Train Epoch: 3955 [33024/118836 (28%)] Loss: 12244.847656\n",
      "Train Epoch: 3955 [65792/118836 (55%)] Loss: 12199.216797\n",
      "Train Epoch: 3955 [98560/118836 (83%)] Loss: 12301.277344\n",
      "    epoch          : 3955\n",
      "    loss           : 12219.509992601066\n",
      "    val_loss       : 12216.918956798978\n",
      "    val_log_likelihood: -12139.321002474928\n",
      "    val_log_marginal: -12147.215844810302\n",
      "Train Epoch: 3956 [256/118836 (0%)] Loss: 12338.065430\n",
      "Train Epoch: 3956 [33024/118836 (28%)] Loss: 12326.703125\n",
      "Train Epoch: 3956 [65792/118836 (55%)] Loss: 12270.804688\n",
      "Train Epoch: 3956 [98560/118836 (83%)] Loss: 12258.207031\n",
      "    epoch          : 3956\n",
      "    loss           : 12217.815987515509\n",
      "    val_loss       : 12217.802348686506\n",
      "    val_log_likelihood: -12140.357000878827\n",
      "    val_log_marginal: -12148.246174278624\n",
      "Train Epoch: 3957 [256/118836 (0%)] Loss: 12203.808594\n",
      "Train Epoch: 3957 [33024/118836 (28%)] Loss: 12243.284180\n",
      "Train Epoch: 3957 [65792/118836 (55%)] Loss: 12245.434570\n",
      "Train Epoch: 3957 [98560/118836 (83%)] Loss: 12266.310547\n",
      "    epoch          : 3957\n",
      "    loss           : 12219.58834296164\n",
      "    val_loss       : 12215.866479237533\n",
      "    val_log_likelihood: -12139.075497570306\n",
      "    val_log_marginal: -12146.962412152676\n",
      "Train Epoch: 3958 [256/118836 (0%)] Loss: 12275.347656\n",
      "Train Epoch: 3958 [33024/118836 (28%)] Loss: 12239.289062\n",
      "Train Epoch: 3958 [65792/118836 (55%)] Loss: 12258.541016\n",
      "Train Epoch: 3958 [98560/118836 (83%)] Loss: 12249.186523\n",
      "    epoch          : 3958\n",
      "    loss           : 12218.729195583903\n",
      "    val_loss       : 12217.115600900679\n",
      "    val_log_likelihood: -12139.47806975031\n",
      "    val_log_marginal: -12147.366069833806\n",
      "Train Epoch: 3959 [256/118836 (0%)] Loss: 12195.075195\n",
      "Train Epoch: 3959 [33024/118836 (28%)] Loss: 12159.089844\n",
      "Train Epoch: 3959 [65792/118836 (55%)] Loss: 12242.416992\n",
      "Train Epoch: 3959 [98560/118836 (83%)] Loss: 12247.315430\n",
      "    epoch          : 3959\n",
      "    loss           : 12215.006049356389\n",
      "    val_loss       : 12219.492674109782\n",
      "    val_log_likelihood: -12139.822938475754\n",
      "    val_log_marginal: -12147.709708756396\n",
      "Train Epoch: 3960 [256/118836 (0%)] Loss: 12274.526367\n",
      "Train Epoch: 3960 [33024/118836 (28%)] Loss: 12191.824219\n",
      "Train Epoch: 3960 [65792/118836 (55%)] Loss: 12227.023438\n",
      "Train Epoch: 3960 [98560/118836 (83%)] Loss: 12228.296875\n",
      "    epoch          : 3960\n",
      "    loss           : 12217.509836868021\n",
      "    val_loss       : 12216.768545325269\n",
      "    val_log_likelihood: -12138.111030875207\n",
      "    val_log_marginal: -12146.008550962231\n",
      "Train Epoch: 3961 [256/118836 (0%)] Loss: 12295.782227\n",
      "Train Epoch: 3961 [33024/118836 (28%)] Loss: 12180.933594\n",
      "Train Epoch: 3961 [65792/118836 (55%)] Loss: 12251.568359\n",
      "Train Epoch: 3961 [98560/118836 (83%)] Loss: 12336.054688\n",
      "    epoch          : 3961\n",
      "    loss           : 12218.490926934708\n",
      "    val_loss       : 12215.87351648507\n",
      "    val_log_likelihood: -12139.543484413773\n",
      "    val_log_marginal: -12147.430383695371\n",
      "Train Epoch: 3962 [256/118836 (0%)] Loss: 12179.440430\n",
      "Train Epoch: 3962 [33024/118836 (28%)] Loss: 12253.185547\n",
      "Train Epoch: 3962 [65792/118836 (55%)] Loss: 12253.499023\n",
      "Train Epoch: 3962 [98560/118836 (83%)] Loss: 12222.912109\n",
      "    epoch          : 3962\n",
      "    loss           : 12217.585783705543\n",
      "    val_loss       : 12218.3912372832\n",
      "    val_log_likelihood: -12140.930071016854\n",
      "    val_log_marginal: -12148.819064557012\n",
      "Train Epoch: 3963 [256/118836 (0%)] Loss: 12359.890625\n",
      "Train Epoch: 3963 [33024/118836 (28%)] Loss: 12185.470703\n",
      "Train Epoch: 3963 [65792/118836 (55%)] Loss: 12279.431641\n",
      "Train Epoch: 3963 [98560/118836 (83%)] Loss: 12322.580078\n",
      "    epoch          : 3963\n",
      "    loss           : 12218.09445467587\n",
      "    val_loss       : 12215.620942665972\n",
      "    val_log_likelihood: -12140.4058659985\n",
      "    val_log_marginal: -12148.292485282078\n",
      "Train Epoch: 3964 [256/118836 (0%)] Loss: 12293.408203\n",
      "Train Epoch: 3964 [33024/118836 (28%)] Loss: 12284.216797\n",
      "Train Epoch: 3964 [65792/118836 (55%)] Loss: 12266.968750\n",
      "Train Epoch: 3964 [98560/118836 (83%)] Loss: 12318.905273\n",
      "    epoch          : 3964\n",
      "    loss           : 12217.073162220842\n",
      "    val_loss       : 12218.908202865421\n",
      "    val_log_likelihood: -12139.586960265457\n",
      "    val_log_marginal: -12147.474942411061\n",
      "Train Epoch: 3965 [256/118836 (0%)] Loss: 12158.057617\n",
      "Train Epoch: 3965 [33024/118836 (28%)] Loss: 12212.173828\n",
      "Train Epoch: 3965 [65792/118836 (55%)] Loss: 12224.279297\n",
      "Train Epoch: 3965 [98560/118836 (83%)] Loss: 12225.682617\n",
      "    epoch          : 3965\n",
      "    loss           : 12219.260150434244\n",
      "    val_loss       : 12217.423302380123\n",
      "    val_log_likelihood: -12137.26383907801\n",
      "    val_log_marginal: -12145.150896816653\n",
      "Train Epoch: 3966 [256/118836 (0%)] Loss: 12272.353516\n",
      "Train Epoch: 3966 [33024/118836 (28%)] Loss: 12182.749023\n",
      "Train Epoch: 3966 [65792/118836 (55%)] Loss: 12188.866211\n",
      "Train Epoch: 3966 [98560/118836 (83%)] Loss: 12220.585938\n",
      "    epoch          : 3966\n",
      "    loss           : 12218.27892256643\n",
      "    val_loss       : 12219.316017660929\n",
      "    val_log_likelihood: -12139.612036193392\n",
      "    val_log_marginal: -12147.502402106702\n",
      "Train Epoch: 3967 [256/118836 (0%)] Loss: 12256.882812\n",
      "Train Epoch: 3967 [33024/118836 (28%)] Loss: 12310.975586\n",
      "Train Epoch: 3967 [65792/118836 (55%)] Loss: 12233.777344\n",
      "Train Epoch: 3967 [98560/118836 (83%)] Loss: 12433.264648\n",
      "    epoch          : 3967\n",
      "    loss           : 12215.967454701717\n",
      "    val_loss       : 12216.297715853163\n",
      "    val_log_likelihood: -12138.810652366366\n",
      "    val_log_marginal: -12146.698702305312\n",
      "Train Epoch: 3968 [256/118836 (0%)] Loss: 12264.233398\n",
      "Train Epoch: 3968 [33024/118836 (28%)] Loss: 12283.555664\n",
      "Train Epoch: 3968 [65792/118836 (55%)] Loss: 12203.341797\n",
      "Train Epoch: 3968 [98560/118836 (83%)] Loss: 12173.226562\n",
      "    epoch          : 3968\n",
      "    loss           : 12220.67325963477\n",
      "    val_loss       : 12216.642404472334\n",
      "    val_log_likelihood: -12139.424827627428\n",
      "    val_log_marginal: -12147.318761740984\n",
      "Train Epoch: 3969 [256/118836 (0%)] Loss: 12313.498047\n",
      "Train Epoch: 3969 [33024/118836 (28%)] Loss: 12333.267578\n",
      "Train Epoch: 3969 [65792/118836 (55%)] Loss: 12203.261719\n",
      "Train Epoch: 3969 [98560/118836 (83%)] Loss: 12225.244141\n",
      "    epoch          : 3969\n",
      "    loss           : 12219.507473732165\n",
      "    val_loss       : 12217.971793268534\n",
      "    val_log_likelihood: -12141.18888883504\n",
      "    val_log_marginal: -12149.087465087176\n",
      "Train Epoch: 3970 [256/118836 (0%)] Loss: 12292.863281\n",
      "Train Epoch: 3970 [33024/118836 (28%)] Loss: 12237.684570\n",
      "Train Epoch: 3970 [65792/118836 (55%)] Loss: 12284.770508\n",
      "Train Epoch: 3970 [98560/118836 (83%)] Loss: 12181.092773\n",
      "    epoch          : 3970\n",
      "    loss           : 12219.032736733612\n",
      "    val_loss       : 12215.289310604592\n",
      "    val_log_likelihood: -12139.09339023082\n",
      "    val_log_marginal: -12146.988616066046\n",
      "Train Epoch: 3971 [256/118836 (0%)] Loss: 12307.770508\n",
      "Train Epoch: 3971 [33024/118836 (28%)] Loss: 12245.216797\n",
      "Train Epoch: 3971 [65792/118836 (55%)] Loss: 12183.711914\n",
      "Train Epoch: 3971 [98560/118836 (83%)] Loss: 12249.556641\n",
      "    epoch          : 3971\n",
      "    loss           : 12218.181820590364\n",
      "    val_loss       : 12222.701988750556\n",
      "    val_log_likelihood: -12141.790159093258\n",
      "    val_log_marginal: -12149.68654754442\n",
      "Train Epoch: 3972 [256/118836 (0%)] Loss: 12208.392578\n",
      "Train Epoch: 3972 [33024/118836 (28%)] Loss: 12179.665039\n",
      "Train Epoch: 3972 [65792/118836 (55%)] Loss: 12203.634766\n",
      "Train Epoch: 3972 [98560/118836 (83%)] Loss: 12407.983398\n",
      "    epoch          : 3972\n",
      "    loss           : 12218.72837039263\n",
      "    val_loss       : 12218.314978711647\n",
      "    val_log_likelihood: -12140.695351917908\n",
      "    val_log_marginal: -12148.589244470977\n",
      "Train Epoch: 3973 [256/118836 (0%)] Loss: 12135.779297\n",
      "Train Epoch: 3973 [33024/118836 (28%)] Loss: 12270.179688\n",
      "Train Epoch: 3973 [65792/118836 (55%)] Loss: 12247.539062\n",
      "Train Epoch: 3973 [98560/118836 (83%)] Loss: 12248.165039\n",
      "    epoch          : 3973\n",
      "    loss           : 12219.3202787363\n",
      "    val_loss       : 12216.643722555866\n",
      "    val_log_likelihood: -12139.259953829354\n",
      "    val_log_marginal: -12147.147377742538\n",
      "Train Epoch: 3974 [256/118836 (0%)] Loss: 12185.435547\n",
      "Train Epoch: 3974 [33024/118836 (28%)] Loss: 12198.404297\n",
      "Train Epoch: 3974 [65792/118836 (55%)] Loss: 12238.799805\n",
      "Train Epoch: 3974 [98560/118836 (83%)] Loss: 12261.610352\n",
      "    epoch          : 3974\n",
      "    loss           : 12220.652112896765\n",
      "    val_loss       : 12219.655971868535\n",
      "    val_log_likelihood: -12140.200052988006\n",
      "    val_log_marginal: -12148.08851660798\n",
      "Train Epoch: 3975 [256/118836 (0%)] Loss: 12376.813477\n",
      "Train Epoch: 3975 [33024/118836 (28%)] Loss: 12205.898438\n",
      "Train Epoch: 3975 [65792/118836 (55%)] Loss: 12375.194336\n",
      "Train Epoch: 3975 [98560/118836 (83%)] Loss: 12251.521484\n",
      "    epoch          : 3975\n",
      "    loss           : 12218.77245286006\n",
      "    val_loss       : 12218.579578203331\n",
      "    val_log_likelihood: -12139.620436246381\n",
      "    val_log_marginal: -12147.508124979622\n",
      "Train Epoch: 3976 [256/118836 (0%)] Loss: 12190.234375\n",
      "Train Epoch: 3976 [33024/118836 (28%)] Loss: 12269.173828\n",
      "Train Epoch: 3976 [65792/118836 (55%)] Loss: 12234.888672\n",
      "Train Epoch: 3976 [98560/118836 (83%)] Loss: 12267.474609\n",
      "    epoch          : 3976\n",
      "    loss           : 12220.638389326148\n",
      "    val_loss       : 12217.426276916325\n",
      "    val_log_likelihood: -12139.498625542805\n",
      "    val_log_marginal: -12147.38681755168\n",
      "Train Epoch: 3977 [256/118836 (0%)] Loss: 12191.364258\n",
      "Train Epoch: 3977 [33024/118836 (28%)] Loss: 12210.048828\n",
      "Train Epoch: 3977 [65792/118836 (55%)] Loss: 12293.053711\n",
      "Train Epoch: 3977 [98560/118836 (83%)] Loss: 12379.440430\n",
      "    epoch          : 3977\n",
      "    loss           : 12216.862590467328\n",
      "    val_loss       : 12218.462721160013\n",
      "    val_log_likelihood: -12140.64211884176\n",
      "    val_log_marginal: -12148.533497588827\n",
      "Train Epoch: 3978 [256/118836 (0%)] Loss: 12310.486328\n",
      "Train Epoch: 3978 [33024/118836 (28%)] Loss: 12296.054688\n",
      "Train Epoch: 3978 [65792/118836 (55%)] Loss: 12314.719727\n",
      "Train Epoch: 3978 [98560/118836 (83%)] Loss: 12143.771484\n",
      "    epoch          : 3978\n",
      "    loss           : 12220.327657154674\n",
      "    val_loss       : 12212.286791630619\n",
      "    val_log_likelihood: -12139.9273371265\n",
      "    val_log_marginal: -12147.819668469721\n",
      "Train Epoch: 3979 [256/118836 (0%)] Loss: 12328.692383\n",
      "Train Epoch: 3979 [33024/118836 (28%)] Loss: 12302.327148\n",
      "Train Epoch: 3979 [65792/118836 (55%)] Loss: 12280.096680\n",
      "Train Epoch: 3979 [98560/118836 (83%)] Loss: 12227.513672\n",
      "    epoch          : 3979\n",
      "    loss           : 12218.976992381358\n",
      "    val_loss       : 12218.488779510382\n",
      "    val_log_likelihood: -12139.452565233405\n",
      "    val_log_marginal: -12147.345852966238\n",
      "Train Epoch: 3980 [256/118836 (0%)] Loss: 12179.132812\n",
      "Train Epoch: 3980 [33024/118836 (28%)] Loss: 12211.353516\n",
      "Train Epoch: 3980 [65792/118836 (55%)] Loss: 12329.589844\n",
      "Train Epoch: 3980 [98560/118836 (83%)] Loss: 12215.250977\n",
      "    epoch          : 3980\n",
      "    loss           : 12218.722422973531\n",
      "    val_loss       : 12219.298175866668\n",
      "    val_log_likelihood: -12140.986813579146\n",
      "    val_log_marginal: -12148.879775384974\n",
      "Train Epoch: 3981 [256/118836 (0%)] Loss: 12243.412109\n",
      "Train Epoch: 3981 [33024/118836 (28%)] Loss: 12200.201172\n",
      "Train Epoch: 3981 [65792/118836 (55%)] Loss: 12310.796875\n",
      "Train Epoch: 3981 [98560/118836 (83%)] Loss: 12364.381836\n",
      "    epoch          : 3981\n",
      "    loss           : 12216.732970656276\n",
      "    val_loss       : 12218.514849123758\n",
      "    val_log_likelihood: -12140.322524910825\n",
      "    val_log_marginal: -12148.217444059108\n",
      "Train Epoch: 3982 [256/118836 (0%)] Loss: 12296.449219\n",
      "Train Epoch: 3982 [33024/118836 (28%)] Loss: 12244.608398\n",
      "Train Epoch: 3982 [65792/118836 (55%)] Loss: 12185.119141\n",
      "Train Epoch: 3982 [98560/118836 (83%)] Loss: 12123.817383\n",
      "    epoch          : 3982\n",
      "    loss           : 12218.812363652813\n",
      "    val_loss       : 12221.312526015765\n",
      "    val_log_likelihood: -12140.604084761424\n",
      "    val_log_marginal: -12148.493175476493\n",
      "Train Epoch: 3983 [256/118836 (0%)] Loss: 12324.660156\n",
      "Train Epoch: 3983 [33024/118836 (28%)] Loss: 12170.944336\n",
      "Train Epoch: 3983 [65792/118836 (55%)] Loss: 12209.445312\n",
      "Train Epoch: 3983 [98560/118836 (83%)] Loss: 12251.275391\n",
      "    epoch          : 3983\n",
      "    loss           : 12218.961883368227\n",
      "    val_loss       : 12214.797955869839\n",
      "    val_log_likelihood: -12138.14493673749\n",
      "    val_log_marginal: -12146.034191259523\n",
      "Train Epoch: 3984 [256/118836 (0%)] Loss: 12221.563477\n",
      "Train Epoch: 3984 [33024/118836 (28%)] Loss: 12246.583008\n",
      "Train Epoch: 3984 [65792/118836 (55%)] Loss: 12294.777344\n",
      "Train Epoch: 3984 [98560/118836 (83%)] Loss: 12269.925781\n",
      "    epoch          : 3984\n",
      "    loss           : 12217.956047094705\n",
      "    val_loss       : 12215.584132561084\n",
      "    val_log_likelihood: -12140.608099572219\n",
      "    val_log_marginal: -12148.492414204753\n",
      "Train Epoch: 3985 [256/118836 (0%)] Loss: 12207.521484\n",
      "Train Epoch: 3985 [33024/118836 (28%)] Loss: 12260.917969\n",
      "Train Epoch: 3985 [65792/118836 (55%)] Loss: 12199.314453\n",
      "Train Epoch: 3985 [98560/118836 (83%)] Loss: 12178.700195\n",
      "    epoch          : 3985\n",
      "    loss           : 12217.514446016854\n",
      "    val_loss       : 12222.687803873063\n",
      "    val_log_likelihood: -12139.891947115384\n",
      "    val_log_marginal: -12147.781892795687\n",
      "Train Epoch: 3986 [256/118836 (0%)] Loss: 12297.218750\n",
      "Train Epoch: 3986 [33024/118836 (28%)] Loss: 12252.944336\n",
      "Train Epoch: 3986 [65792/118836 (55%)] Loss: 12270.230469\n",
      "Train Epoch: 3986 [98560/118836 (83%)] Loss: 12122.415039\n",
      "    epoch          : 3986\n",
      "    loss           : 12218.042368757755\n",
      "    val_loss       : 12223.037102491697\n",
      "    val_log_likelihood: -12138.526361533293\n",
      "    val_log_marginal: -12146.417888625581\n",
      "Train Epoch: 3987 [256/118836 (0%)] Loss: 12152.249023\n",
      "Train Epoch: 3987 [33024/118836 (28%)] Loss: 12271.628906\n",
      "Train Epoch: 3987 [65792/118836 (55%)] Loss: 12204.184570\n",
      "Train Epoch: 3987 [98560/118836 (83%)] Loss: 12224.050781\n",
      "    epoch          : 3987\n",
      "    loss           : 12219.421475974463\n",
      "    val_loss       : 12212.76871553485\n",
      "    val_log_likelihood: -12139.446986307124\n",
      "    val_log_marginal: -12147.346617011646\n",
      "Train Epoch: 3988 [256/118836 (0%)] Loss: 12199.416992\n",
      "Train Epoch: 3988 [33024/118836 (28%)] Loss: 12241.351562\n",
      "Train Epoch: 3988 [65792/118836 (55%)] Loss: 12311.919922\n",
      "Train Epoch: 3988 [98560/118836 (83%)] Loss: 12147.977539\n",
      "    epoch          : 3988\n",
      "    loss           : 12221.324127313379\n",
      "    val_loss       : 12218.704293270182\n",
      "    val_log_likelihood: -12141.355629652606\n",
      "    val_log_marginal: -12149.241317447833\n",
      "Train Epoch: 3989 [256/118836 (0%)] Loss: 12187.519531\n",
      "Train Epoch: 3989 [33024/118836 (28%)] Loss: 12182.963867\n",
      "Train Epoch: 3989 [65792/118836 (55%)] Loss: 12283.783203\n",
      "Train Epoch: 3989 [98560/118836 (83%)] Loss: 12274.124023\n",
      "    epoch          : 3989\n",
      "    loss           : 12219.036864628566\n",
      "    val_loss       : 12219.257228360602\n",
      "    val_log_likelihood: -12139.751556038047\n",
      "    val_log_marginal: -12147.642843411639\n",
      "Train Epoch: 3990 [256/118836 (0%)] Loss: 12226.748047\n",
      "Train Epoch: 3990 [33024/118836 (28%)] Loss: 12377.416016\n",
      "Train Epoch: 3990 [65792/118836 (55%)] Loss: 12261.139648\n",
      "Train Epoch: 3990 [98560/118836 (83%)] Loss: 12274.286133\n",
      "    epoch          : 3990\n",
      "    loss           : 12218.222589691895\n",
      "    val_loss       : 12222.124720156304\n",
      "    val_log_likelihood: -12140.628366515457\n",
      "    val_log_marginal: -12148.522220449217\n",
      "Train Epoch: 3991 [256/118836 (0%)] Loss: 12171.621094\n",
      "Train Epoch: 3991 [33024/118836 (28%)] Loss: 12205.493164\n",
      "Train Epoch: 3991 [65792/118836 (55%)] Loss: 12147.386719\n",
      "Train Epoch: 3991 [98560/118836 (83%)] Loss: 12195.384766\n",
      "    epoch          : 3991\n",
      "    loss           : 12218.66812787557\n",
      "    val_loss       : 12217.185179596967\n",
      "    val_log_likelihood: -12138.577198840725\n",
      "    val_log_marginal: -12146.474963112012\n",
      "Train Epoch: 3992 [256/118836 (0%)] Loss: 12185.559570\n",
      "Train Epoch: 3992 [33024/118836 (28%)] Loss: 12148.041992\n",
      "Train Epoch: 3992 [65792/118836 (55%)] Loss: 12165.165039\n",
      "Train Epoch: 3992 [98560/118836 (83%)] Loss: 12220.897461\n",
      "    epoch          : 3992\n",
      "    loss           : 12218.509284371123\n",
      "    val_loss       : 12218.598687533193\n",
      "    val_log_likelihood: -12139.432168243382\n",
      "    val_log_marginal: -12147.331848001597\n",
      "Train Epoch: 3993 [256/118836 (0%)] Loss: 12352.458008\n",
      "Train Epoch: 3993 [33024/118836 (28%)] Loss: 12286.258789\n",
      "Train Epoch: 3993 [65792/118836 (55%)] Loss: 12300.629883\n",
      "Train Epoch: 3993 [98560/118836 (83%)] Loss: 12129.760742\n",
      "    epoch          : 3993\n",
      "    loss           : 12222.656343698305\n",
      "    val_loss       : 12219.59352888034\n",
      "    val_log_likelihood: -12140.284106505893\n",
      "    val_log_marginal: -12148.175857614724\n",
      "Train Epoch: 3994 [256/118836 (0%)] Loss: 12219.915039\n",
      "Train Epoch: 3994 [33024/118836 (28%)] Loss: 12328.646484\n",
      "Train Epoch: 3994 [65792/118836 (55%)] Loss: 12302.099609\n",
      "Train Epoch: 3994 [98560/118836 (83%)] Loss: 12200.068359\n",
      "    epoch          : 3994\n",
      "    loss           : 12216.345814432123\n",
      "    val_loss       : 12219.780061183637\n",
      "    val_log_likelihood: -12138.7135169497\n",
      "    val_log_marginal: -12146.605858388795\n",
      "Train Epoch: 3995 [256/118836 (0%)] Loss: 12227.296875\n",
      "Train Epoch: 3995 [33024/118836 (28%)] Loss: 12188.390625\n",
      "Train Epoch: 3995 [65792/118836 (55%)] Loss: 12230.320312\n",
      "Train Epoch: 3995 [98560/118836 (83%)] Loss: 12145.792969\n",
      "    epoch          : 3995\n",
      "    loss           : 12216.702403523055\n",
      "    val_loss       : 12215.067879880724\n",
      "    val_log_likelihood: -12139.724621006513\n",
      "    val_log_marginal: -12147.612679835409\n",
      "Train Epoch: 3996 [256/118836 (0%)] Loss: 12346.057617\n",
      "Train Epoch: 3996 [33024/118836 (28%)] Loss: 12202.296875\n",
      "Train Epoch: 3996 [65792/118836 (55%)] Loss: 12139.474609\n",
      "Train Epoch: 3996 [98560/118836 (83%)] Loss: 12144.815430\n",
      "    epoch          : 3996\n",
      "    loss           : 12217.605742413669\n",
      "    val_loss       : 12214.360094167185\n",
      "    val_log_likelihood: -12138.80206362438\n",
      "    val_log_marginal: -12146.691353814014\n",
      "Train Epoch: 3997 [256/118836 (0%)] Loss: 12239.082031\n",
      "Train Epoch: 3997 [33024/118836 (28%)] Loss: 12271.656250\n",
      "Train Epoch: 3997 [65792/118836 (55%)] Loss: 12182.283203\n",
      "Train Epoch: 3997 [98560/118836 (83%)] Loss: 12243.074219\n",
      "    epoch          : 3997\n",
      "    loss           : 12221.463468646609\n",
      "    val_loss       : 12215.14345195219\n",
      "    val_log_likelihood: -12139.71725631979\n",
      "    val_log_marginal: -12147.616091643811\n",
      "Train Epoch: 3998 [256/118836 (0%)] Loss: 12186.053711\n",
      "Train Epoch: 3998 [33024/118836 (28%)] Loss: 12235.689453\n",
      "Train Epoch: 3998 [65792/118836 (55%)] Loss: 12252.477539\n",
      "Train Epoch: 3998 [98560/118836 (83%)] Loss: 12284.184570\n",
      "    epoch          : 3998\n",
      "    loss           : 12216.95626890121\n",
      "    val_loss       : 12216.326628099627\n",
      "    val_log_likelihood: -12139.722778380892\n",
      "    val_log_marginal: -12147.621420747697\n",
      "Train Epoch: 3999 [256/118836 (0%)] Loss: 12214.173828\n",
      "Train Epoch: 3999 [33024/118836 (28%)] Loss: 12259.640625\n",
      "Train Epoch: 3999 [65792/118836 (55%)] Loss: 12327.858398\n",
      "Train Epoch: 3999 [98560/118836 (83%)] Loss: 12200.013672\n",
      "    epoch          : 3999\n",
      "    loss           : 12220.793965667648\n",
      "    val_loss       : 12222.292191399545\n",
      "    val_log_likelihood: -12140.146988730356\n",
      "    val_log_marginal: -12148.044013581753\n",
      "Train Epoch: 4000 [256/118836 (0%)] Loss: 12176.971680\n",
      "Train Epoch: 4000 [33024/118836 (28%)] Loss: 12249.046875\n",
      "Train Epoch: 4000 [65792/118836 (55%)] Loss: 12169.723633\n",
      "Train Epoch: 4000 [98560/118836 (83%)] Loss: 12176.013672\n",
      "    epoch          : 4000\n",
      "    loss           : 12214.460215215313\n",
      "    val_loss       : 12217.304781267118\n",
      "    val_log_likelihood: -12139.25997014578\n",
      "    val_log_marginal: -12147.153841529442\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch4000.pth ...\n",
      "Train Epoch: 4001 [256/118836 (0%)] Loss: 12263.998047\n",
      "Train Epoch: 4001 [33024/118836 (28%)] Loss: 12139.356445\n",
      "Train Epoch: 4001 [65792/118836 (55%)] Loss: 12181.142578\n",
      "Train Epoch: 4001 [98560/118836 (83%)] Loss: 12340.653320\n",
      "    epoch          : 4001\n",
      "    loss           : 12219.616244378101\n",
      "    val_loss       : 12220.521559391564\n",
      "    val_log_likelihood: -12139.598590971362\n",
      "    val_log_marginal: -12147.490136337283\n",
      "Train Epoch: 4002 [256/118836 (0%)] Loss: 12214.947266\n",
      "Train Epoch: 4002 [33024/118836 (28%)] Loss: 12312.987305\n",
      "Train Epoch: 4002 [65792/118836 (55%)] Loss: 12175.292969\n",
      "Train Epoch: 4002 [98560/118836 (83%)] Loss: 12268.552734\n",
      "    epoch          : 4002\n",
      "    loss           : 12215.567576186415\n",
      "    val_loss       : 12217.795237807872\n",
      "    val_log_likelihood: -12139.826175913719\n",
      "    val_log_marginal: -12147.71945316885\n",
      "Train Epoch: 4003 [256/118836 (0%)] Loss: 12140.401367\n",
      "Train Epoch: 4003 [33024/118836 (28%)] Loss: 12184.068359\n",
      "Train Epoch: 4003 [65792/118836 (55%)] Loss: 12222.513672\n",
      "Train Epoch: 4003 [98560/118836 (83%)] Loss: 12213.015625\n",
      "    epoch          : 4003\n",
      "    loss           : 12216.081460821184\n",
      "    val_loss       : 12216.382675250108\n",
      "    val_log_likelihood: -12140.22292845973\n",
      "    val_log_marginal: -12148.121702449702\n",
      "Train Epoch: 4004 [256/118836 (0%)] Loss: 12248.451172\n",
      "Train Epoch: 4004 [33024/118836 (28%)] Loss: 12172.131836\n",
      "Train Epoch: 4004 [65792/118836 (55%)] Loss: 12203.568359\n",
      "Train Epoch: 4004 [98560/118836 (83%)] Loss: 12231.086914\n",
      "    epoch          : 4004\n",
      "    loss           : 12214.661718265354\n",
      "    val_loss       : 12214.303634365284\n",
      "    val_log_likelihood: -12139.53592813017\n",
      "    val_log_marginal: -12147.431430373108\n",
      "Train Epoch: 4005 [256/118836 (0%)] Loss: 12312.110352\n",
      "Train Epoch: 4005 [33024/118836 (28%)] Loss: 12270.830078\n",
      "Train Epoch: 4005 [65792/118836 (55%)] Loss: 12188.623047\n",
      "Train Epoch: 4005 [98560/118836 (83%)] Loss: 12301.799805\n",
      "    epoch          : 4005\n",
      "    loss           : 12217.79847255609\n",
      "    val_loss       : 12223.333505046648\n",
      "    val_log_likelihood: -12140.644864848275\n",
      "    val_log_marginal: -12148.539394926962\n",
      "Train Epoch: 4006 [256/118836 (0%)] Loss: 12189.515625\n",
      "Train Epoch: 4006 [33024/118836 (28%)] Loss: 12193.404297\n",
      "Train Epoch: 4006 [65792/118836 (55%)] Loss: 12208.090820\n",
      "Train Epoch: 4006 [98560/118836 (83%)] Loss: 12271.722656\n",
      "    epoch          : 4006\n",
      "    loss           : 12218.068243221413\n",
      "    val_loss       : 12213.525889541685\n",
      "    val_log_likelihood: -12140.58817898961\n",
      "    val_log_marginal: -12148.481233091208\n",
      "Train Epoch: 4007 [256/118836 (0%)] Loss: 12243.126953\n",
      "Train Epoch: 4007 [33024/118836 (28%)] Loss: 12290.887695\n",
      "Train Epoch: 4007 [65792/118836 (55%)] Loss: 12141.662109\n",
      "Train Epoch: 4007 [98560/118836 (83%)] Loss: 12285.124023\n",
      "    epoch          : 4007\n",
      "    loss           : 12219.55736921009\n",
      "    val_loss       : 12219.17011203685\n",
      "    val_log_likelihood: -12140.497590822737\n",
      "    val_log_marginal: -12148.39051350362\n",
      "Train Epoch: 4008 [256/118836 (0%)] Loss: 12287.691406\n",
      "Train Epoch: 4008 [33024/118836 (28%)] Loss: 12175.816406\n",
      "Train Epoch: 4008 [65792/118836 (55%)] Loss: 12267.174805\n",
      "Train Epoch: 4008 [98560/118836 (83%)] Loss: 12299.727539\n",
      "    epoch          : 4008\n",
      "    loss           : 12220.683231719138\n",
      "    val_loss       : 12219.198709497678\n",
      "    val_log_likelihood: -12140.06343375207\n",
      "    val_log_marginal: -12147.958836425889\n",
      "Train Epoch: 4009 [256/118836 (0%)] Loss: 12215.206055\n",
      "Train Epoch: 4009 [33024/118836 (28%)] Loss: 12325.720703\n",
      "Train Epoch: 4009 [65792/118836 (55%)] Loss: 12279.646484\n",
      "Train Epoch: 4009 [98560/118836 (83%)] Loss: 12334.017578\n",
      "    epoch          : 4009\n",
      "    loss           : 12216.87529062629\n",
      "    val_loss       : 12219.039014170916\n",
      "    val_log_likelihood: -12138.813732132703\n",
      "    val_log_marginal: -12146.707159080606\n",
      "Train Epoch: 4010 [256/118836 (0%)] Loss: 12286.501953\n",
      "Train Epoch: 4010 [33024/118836 (28%)] Loss: 12193.671875\n",
      "Train Epoch: 4010 [65792/118836 (55%)] Loss: 12269.700195\n",
      "Train Epoch: 4010 [98560/118836 (83%)] Loss: 12218.091797\n",
      "    epoch          : 4010\n",
      "    loss           : 12221.597601484957\n",
      "    val_loss       : 12217.779568832475\n",
      "    val_log_likelihood: -12141.252076063638\n",
      "    val_log_marginal: -12149.137224624135\n",
      "Train Epoch: 4011 [256/118836 (0%)] Loss: 12270.447266\n",
      "Train Epoch: 4011 [33024/118836 (28%)] Loss: 12206.478516\n",
      "Train Epoch: 4011 [65792/118836 (55%)] Loss: 12175.449219\n",
      "Train Epoch: 4011 [98560/118836 (83%)] Loss: 12314.831055\n",
      "    epoch          : 4011\n",
      "    loss           : 12217.445304907205\n",
      "    val_loss       : 12214.327384487275\n",
      "    val_log_likelihood: -12140.583651746021\n",
      "    val_log_marginal: -12148.47028910676\n",
      "Train Epoch: 4012 [256/118836 (0%)] Loss: 12159.499023\n",
      "Train Epoch: 4012 [33024/118836 (28%)] Loss: 12193.727539\n",
      "Train Epoch: 4012 [65792/118836 (55%)] Loss: 12219.964844\n",
      "Train Epoch: 4012 [98560/118836 (83%)] Loss: 12192.814453\n",
      "    epoch          : 4012\n",
      "    loss           : 12220.749858483252\n",
      "    val_loss       : 12215.519571078174\n",
      "    val_log_likelihood: -12137.913132786653\n",
      "    val_log_marginal: -12145.806301437575\n",
      "Train Epoch: 4013 [256/118836 (0%)] Loss: 12123.824219\n",
      "Train Epoch: 4013 [33024/118836 (28%)] Loss: 12191.846680\n",
      "Train Epoch: 4013 [65792/118836 (55%)] Loss: 12340.269531\n",
      "Train Epoch: 4013 [98560/118836 (83%)] Loss: 12259.205078\n",
      "    epoch          : 4013\n",
      "    loss           : 12219.33847930883\n",
      "    val_loss       : 12216.758371490969\n",
      "    val_log_likelihood: -12138.705360835402\n",
      "    val_log_marginal: -12146.596115419952\n",
      "Train Epoch: 4014 [256/118836 (0%)] Loss: 12208.558594\n",
      "Train Epoch: 4014 [33024/118836 (28%)] Loss: 12248.520508\n",
      "Train Epoch: 4014 [65792/118836 (55%)] Loss: 12253.461914\n",
      "Train Epoch: 4014 [98560/118836 (83%)] Loss: 12124.294922\n",
      "    epoch          : 4014\n",
      "    loss           : 12215.955206879395\n",
      "    val_loss       : 12218.875154780875\n",
      "    val_log_likelihood: -12138.789745205233\n",
      "    val_log_marginal: -12146.681102090282\n",
      "Train Epoch: 4015 [256/118836 (0%)] Loss: 12212.658203\n",
      "Train Epoch: 4015 [33024/118836 (28%)] Loss: 12307.226562\n",
      "Train Epoch: 4015 [65792/118836 (55%)] Loss: 12343.398438\n",
      "Train Epoch: 4015 [98560/118836 (83%)] Loss: 12132.371094\n",
      "    epoch          : 4015\n",
      "    loss           : 12219.134723299216\n",
      "    val_loss       : 12214.986685122241\n",
      "    val_log_likelihood: -12140.071594712832\n",
      "    val_log_marginal: -12147.960825944278\n",
      "Train Epoch: 4016 [256/118836 (0%)] Loss: 12255.892578\n",
      "Train Epoch: 4016 [33024/118836 (28%)] Loss: 12247.976562\n",
      "Train Epoch: 4016 [65792/118836 (55%)] Loss: 12368.667969\n",
      "Train Epoch: 4016 [98560/118836 (83%)] Loss: 12150.801758\n",
      "    epoch          : 4016\n",
      "    loss           : 12216.482311052523\n",
      "    val_loss       : 12216.51779474838\n",
      "    val_log_likelihood: -12141.65724029415\n",
      "    val_log_marginal: -12149.545619138333\n",
      "Train Epoch: 4017 [256/118836 (0%)] Loss: 12280.460938\n",
      "Train Epoch: 4017 [33024/118836 (28%)] Loss: 12267.375000\n",
      "Train Epoch: 4017 [65792/118836 (55%)] Loss: 12235.252930\n",
      "Train Epoch: 4017 [98560/118836 (83%)] Loss: 12326.802734\n",
      "    epoch          : 4017\n",
      "    loss           : 12218.041364408862\n",
      "    val_loss       : 12216.125147726578\n",
      "    val_log_likelihood: -12141.81119759357\n",
      "    val_log_marginal: -12149.70265223656\n",
      "Train Epoch: 4018 [256/118836 (0%)] Loss: 12157.616211\n",
      "Train Epoch: 4018 [33024/118836 (28%)] Loss: 12219.055664\n",
      "Train Epoch: 4018 [65792/118836 (55%)] Loss: 12249.861328\n",
      "Train Epoch: 4018 [98560/118836 (83%)] Loss: 12294.242188\n",
      "    epoch          : 4018\n",
      "    loss           : 12218.826629381205\n",
      "    val_loss       : 12216.694358844805\n",
      "    val_log_likelihood: -12141.43252138906\n",
      "    val_log_marginal: -12149.327628801537\n",
      "Train Epoch: 4019 [256/118836 (0%)] Loss: 12202.317383\n",
      "Train Epoch: 4019 [33024/118836 (28%)] Loss: 12281.452148\n",
      "Train Epoch: 4019 [65792/118836 (55%)] Loss: 12244.221680\n",
      "Train Epoch: 4019 [98560/118836 (83%)] Loss: 12281.658203\n",
      "    epoch          : 4019\n",
      "    loss           : 12217.8040677988\n",
      "    val_loss       : 12216.222617789355\n",
      "    val_log_likelihood: -12141.166904304951\n",
      "    val_log_marginal: -12149.055761891235\n",
      "Train Epoch: 4020 [256/118836 (0%)] Loss: 12162.138672\n",
      "Train Epoch: 4020 [33024/118836 (28%)] Loss: 12269.367188\n",
      "Train Epoch: 4020 [65792/118836 (55%)] Loss: 12273.876953\n",
      "Train Epoch: 4020 [98560/118836 (83%)] Loss: 12322.054688\n",
      "    epoch          : 4020\n",
      "    loss           : 12221.857489887045\n",
      "    val_loss       : 12216.742295924567\n",
      "    val_log_likelihood: -12139.28100218414\n",
      "    val_log_marginal: -12147.17436736105\n",
      "Train Epoch: 4021 [256/118836 (0%)] Loss: 12255.506836\n",
      "Train Epoch: 4021 [33024/118836 (28%)] Loss: 12304.373047\n",
      "Train Epoch: 4021 [65792/118836 (55%)] Loss: 12227.171875\n",
      "Train Epoch: 4021 [98560/118836 (83%)] Loss: 12169.619141\n",
      "    epoch          : 4021\n",
      "    loss           : 12217.190609652864\n",
      "    val_loss       : 12219.118763180913\n",
      "    val_log_likelihood: -12140.4876145381\n",
      "    val_log_marginal: -12148.37706455886\n",
      "Train Epoch: 4022 [256/118836 (0%)] Loss: 12349.665039\n",
      "Train Epoch: 4022 [33024/118836 (28%)] Loss: 12242.811523\n",
      "Train Epoch: 4022 [65792/118836 (55%)] Loss: 12213.156250\n",
      "Train Epoch: 4022 [98560/118836 (83%)] Loss: 12257.681641\n",
      "    epoch          : 4022\n",
      "    loss           : 12219.632316383633\n",
      "    val_loss       : 12215.088783817999\n",
      "    val_log_likelihood: -12140.419937060587\n",
      "    val_log_marginal: -12148.311393705262\n",
      "Train Epoch: 4023 [256/118836 (0%)] Loss: 12322.816406\n",
      "Train Epoch: 4023 [33024/118836 (28%)] Loss: 12301.518555\n",
      "Train Epoch: 4023 [65792/118836 (55%)] Loss: 12249.152344\n",
      "Train Epoch: 4023 [98560/118836 (83%)] Loss: 12185.455078\n",
      "    epoch          : 4023\n",
      "    loss           : 12221.442865843414\n",
      "    val_loss       : 12213.333157864166\n",
      "    val_log_likelihood: -12139.265623546062\n",
      "    val_log_marginal: -12147.152738253444\n",
      "Train Epoch: 4024 [256/118836 (0%)] Loss: 12298.757812\n",
      "Train Epoch: 4024 [33024/118836 (28%)] Loss: 12370.608398\n",
      "Train Epoch: 4024 [65792/118836 (55%)] Loss: 12172.416992\n",
      "Train Epoch: 4024 [98560/118836 (83%)] Loss: 12180.752930\n",
      "    epoch          : 4024\n",
      "    loss           : 12215.112577058777\n",
      "    val_loss       : 12221.143440227086\n",
      "    val_log_likelihood: -12139.101305960505\n",
      "    val_log_marginal: -12146.988484070893\n",
      "Train Epoch: 4025 [256/118836 (0%)] Loss: 12151.481445\n",
      "Train Epoch: 4025 [33024/118836 (28%)] Loss: 12240.453125\n",
      "Train Epoch: 4025 [65792/118836 (55%)] Loss: 12151.830078\n",
      "Train Epoch: 4025 [98560/118836 (83%)] Loss: 12274.964844\n",
      "    epoch          : 4025\n",
      "    loss           : 12217.72530581188\n",
      "    val_loss       : 12220.661075989561\n",
      "    val_log_likelihood: -12139.960716178144\n",
      "    val_log_marginal: -12147.85003496054\n",
      "Train Epoch: 4026 [256/118836 (0%)] Loss: 12278.436523\n",
      "Train Epoch: 4026 [33024/118836 (28%)] Loss: 12387.755859\n",
      "Train Epoch: 4026 [65792/118836 (55%)] Loss: 12269.392578\n",
      "Train Epoch: 4026 [98560/118836 (83%)] Loss: 12211.319336\n",
      "    epoch          : 4026\n",
      "    loss           : 12218.564219686983\n",
      "    val_loss       : 12223.50689032133\n",
      "    val_log_likelihood: -12137.500693044354\n",
      "    val_log_marginal: -12145.39120803627\n",
      "Train Epoch: 4027 [256/118836 (0%)] Loss: 12154.115234\n",
      "Train Epoch: 4027 [33024/118836 (28%)] Loss: 12223.551758\n",
      "Train Epoch: 4027 [65792/118836 (55%)] Loss: 12159.989258\n",
      "Train Epoch: 4027 [98560/118836 (83%)] Loss: 12252.777344\n",
      "    epoch          : 4027\n",
      "    loss           : 12215.23542959057\n",
      "    val_loss       : 12219.390092616706\n",
      "    val_log_likelihood: -12140.52783405061\n",
      "    val_log_marginal: -12148.42040966806\n",
      "Train Epoch: 4028 [256/118836 (0%)] Loss: 12138.209961\n",
      "Train Epoch: 4028 [33024/118836 (28%)] Loss: 12179.059570\n",
      "Train Epoch: 4028 [65792/118836 (55%)] Loss: 12197.049805\n",
      "Train Epoch: 4028 [98560/118836 (83%)] Loss: 12188.639648\n",
      "    epoch          : 4028\n",
      "    loss           : 12225.104559876447\n",
      "    val_loss       : 12221.49954204674\n",
      "    val_log_likelihood: -12140.971174039754\n",
      "    val_log_marginal: -12148.854246315022\n",
      "Train Epoch: 4029 [256/118836 (0%)] Loss: 12274.970703\n",
      "Train Epoch: 4029 [33024/118836 (28%)] Loss: 12166.889648\n",
      "Train Epoch: 4029 [65792/118836 (55%)] Loss: 12340.970703\n",
      "Train Epoch: 4029 [98560/118836 (83%)] Loss: 12188.875977\n",
      "    epoch          : 4029\n",
      "    loss           : 12214.91712708075\n",
      "    val_loss       : 12218.11895383445\n",
      "    val_log_likelihood: -12140.747504717227\n",
      "    val_log_marginal: -12148.637957297029\n",
      "Train Epoch: 4030 [256/118836 (0%)] Loss: 12324.304688\n",
      "Train Epoch: 4030 [33024/118836 (28%)] Loss: 12312.806641\n",
      "Train Epoch: 4030 [65792/118836 (55%)] Loss: 12267.398438\n",
      "Train Epoch: 4030 [98560/118836 (83%)] Loss: 12272.828125\n",
      "    epoch          : 4030\n",
      "    loss           : 12218.050054441945\n",
      "    val_loss       : 12215.501281130453\n",
      "    val_log_likelihood: -12139.034306826406\n",
      "    val_log_marginal: -12146.924985468126\n",
      "Train Epoch: 4031 [256/118836 (0%)] Loss: 12279.428711\n",
      "Train Epoch: 4031 [33024/118836 (28%)] Loss: 12288.780273\n",
      "Train Epoch: 4031 [65792/118836 (55%)] Loss: 12142.892578\n",
      "Train Epoch: 4031 [98560/118836 (83%)] Loss: 12182.915039\n",
      "    epoch          : 4031\n",
      "    loss           : 12216.772184042855\n",
      "    val_loss       : 12220.80863309054\n",
      "    val_log_likelihood: -12137.695648844603\n",
      "    val_log_marginal: -12145.581368949266\n",
      "Train Epoch: 4032 [256/118836 (0%)] Loss: 12145.915039\n",
      "Train Epoch: 4032 [33024/118836 (28%)] Loss: 12300.191406\n",
      "Train Epoch: 4032 [65792/118836 (55%)] Loss: 12228.617188\n",
      "Train Epoch: 4032 [98560/118836 (83%)] Loss: 12226.945312\n",
      "    epoch          : 4032\n",
      "    loss           : 12220.443630292339\n",
      "    val_loss       : 12219.500641507997\n",
      "    val_log_likelihood: -12140.055787324234\n",
      "    val_log_marginal: -12147.94857233209\n",
      "Train Epoch: 4033 [256/118836 (0%)] Loss: 12241.218750\n",
      "Train Epoch: 4033 [33024/118836 (28%)] Loss: 12268.029297\n",
      "Train Epoch: 4033 [65792/118836 (55%)] Loss: 12164.008789\n",
      "Train Epoch: 4033 [98560/118836 (83%)] Loss: 12271.779297\n",
      "    epoch          : 4033\n",
      "    loss           : 12221.305139351995\n",
      "    val_loss       : 12220.796121335372\n",
      "    val_log_likelihood: -12139.382212184655\n",
      "    val_log_marginal: -12147.261976687929\n",
      "Train Epoch: 4034 [256/118836 (0%)] Loss: 12293.529297\n",
      "Train Epoch: 4034 [33024/118836 (28%)] Loss: 12247.257812\n",
      "Train Epoch: 4034 [65792/118836 (55%)] Loss: 12300.558594\n",
      "Train Epoch: 4034 [98560/118836 (83%)] Loss: 12281.033203\n",
      "    epoch          : 4034\n",
      "    loss           : 12217.907169051128\n",
      "    val_loss       : 12215.749287770512\n",
      "    val_log_likelihood: -12138.446217819479\n",
      "    val_log_marginal: -12146.331391758744\n",
      "Train Epoch: 4035 [256/118836 (0%)] Loss: 12168.981445\n",
      "Train Epoch: 4035 [33024/118836 (28%)] Loss: 12192.056641\n",
      "Train Epoch: 4035 [65792/118836 (55%)] Loss: 12302.451172\n",
      "Train Epoch: 4035 [98560/118836 (83%)] Loss: 12215.125000\n",
      "    epoch          : 4035\n",
      "    loss           : 12220.461474165117\n",
      "    val_loss       : 12219.277347203626\n",
      "    val_log_likelihood: -12138.715048109234\n",
      "    val_log_marginal: -12146.600160419643\n",
      "Train Epoch: 4036 [256/118836 (0%)] Loss: 12260.582031\n",
      "Train Epoch: 4036 [33024/118836 (28%)] Loss: 12182.405273\n",
      "Train Epoch: 4036 [65792/118836 (55%)] Loss: 12240.657227\n",
      "Train Epoch: 4036 [98560/118836 (83%)] Loss: 12228.180664\n",
      "    epoch          : 4036\n",
      "    loss           : 12215.988970094086\n",
      "    val_loss       : 12220.71682952409\n",
      "    val_log_likelihood: -12140.64718921242\n",
      "    val_log_marginal: -12148.531969873813\n",
      "Train Epoch: 4037 [256/118836 (0%)] Loss: 12209.727539\n",
      "Train Epoch: 4037 [33024/118836 (28%)] Loss: 12291.074219\n",
      "Train Epoch: 4037 [65792/118836 (55%)] Loss: 12157.500977\n",
      "Train Epoch: 4037 [98560/118836 (83%)] Loss: 12197.429688\n",
      "    epoch          : 4037\n",
      "    loss           : 12217.829237263493\n",
      "    val_loss       : 12217.639087037793\n",
      "    val_log_likelihood: -12141.771152715313\n",
      "    val_log_marginal: -12149.656790652356\n",
      "Train Epoch: 4038 [256/118836 (0%)] Loss: 12239.525391\n",
      "Train Epoch: 4038 [33024/118836 (28%)] Loss: 12237.090820\n",
      "Train Epoch: 4038 [65792/118836 (55%)] Loss: 12157.814453\n",
      "Train Epoch: 4038 [98560/118836 (83%)] Loss: 12210.697266\n",
      "    epoch          : 4038\n",
      "    loss           : 12217.02773986766\n",
      "    val_loss       : 12221.134295322636\n",
      "    val_log_likelihood: -12138.113447322168\n",
      "    val_log_marginal: -12146.000036123805\n",
      "Train Epoch: 4039 [256/118836 (0%)] Loss: 12156.188477\n",
      "Train Epoch: 4039 [33024/118836 (28%)] Loss: 12289.856445\n",
      "Train Epoch: 4039 [65792/118836 (55%)] Loss: 12306.016602\n",
      "Train Epoch: 4039 [98560/118836 (83%)] Loss: 12332.429688\n",
      "    epoch          : 4039\n",
      "    loss           : 12216.169895704095\n",
      "    val_loss       : 12220.50686799701\n",
      "    val_log_likelihood: -12141.488012432796\n",
      "    val_log_marginal: -12149.37718275046\n",
      "Train Epoch: 4040 [256/118836 (0%)] Loss: 12175.508789\n",
      "Train Epoch: 4040 [33024/118836 (28%)] Loss: 12210.753906\n",
      "Train Epoch: 4040 [65792/118836 (55%)] Loss: 12192.241211\n",
      "Train Epoch: 4040 [98560/118836 (83%)] Loss: 12242.194336\n",
      "    epoch          : 4040\n",
      "    loss           : 12218.069756125931\n",
      "    val_loss       : 12217.818816227276\n",
      "    val_log_likelihood: -12139.462853307226\n",
      "    val_log_marginal: -12147.357925351202\n",
      "Train Epoch: 4041 [256/118836 (0%)] Loss: 12348.145508\n",
      "Train Epoch: 4041 [33024/118836 (28%)] Loss: 12327.411133\n",
      "Train Epoch: 4041 [65792/118836 (55%)] Loss: 12344.641602\n",
      "Train Epoch: 4041 [98560/118836 (83%)] Loss: 12256.732422\n",
      "    epoch          : 4041\n",
      "    loss           : 12221.470446100859\n",
      "    val_loss       : 12219.147913325165\n",
      "    val_log_likelihood: -12137.975753948253\n",
      "    val_log_marginal: -12145.865424804055\n",
      "Train Epoch: 4042 [256/118836 (0%)] Loss: 12156.923828\n",
      "Train Epoch: 4042 [33024/118836 (28%)] Loss: 12230.622070\n",
      "Train Epoch: 4042 [65792/118836 (55%)] Loss: 12173.837891\n",
      "Train Epoch: 4042 [98560/118836 (83%)] Loss: 12277.369141\n",
      "    epoch          : 4042\n",
      "    loss           : 12218.643667287015\n",
      "    val_loss       : 12217.881454000213\n",
      "    val_log_likelihood: -12139.329021111196\n",
      "    val_log_marginal: -12147.213896676349\n",
      "Train Epoch: 4043 [256/118836 (0%)] Loss: 12182.798828\n",
      "Train Epoch: 4043 [33024/118836 (28%)] Loss: 12280.664062\n",
      "Train Epoch: 4043 [65792/118836 (55%)] Loss: 12143.407227\n",
      "Train Epoch: 4043 [98560/118836 (83%)] Loss: 12196.612305\n",
      "    epoch          : 4043\n",
      "    loss           : 12214.293931419303\n",
      "    val_loss       : 12216.129851685602\n",
      "    val_log_likelihood: -12138.93115420156\n",
      "    val_log_marginal: -12146.813438345793\n",
      "Train Epoch: 4044 [256/118836 (0%)] Loss: 12255.941406\n",
      "Train Epoch: 4044 [33024/118836 (28%)] Loss: 12230.663086\n",
      "Train Epoch: 4044 [65792/118836 (55%)] Loss: 12298.836914\n",
      "Train Epoch: 4044 [98560/118836 (83%)] Loss: 12239.997070\n",
      "    epoch          : 4044\n",
      "    loss           : 12214.969654188637\n",
      "    val_loss       : 12219.818885477145\n",
      "    val_log_likelihood: -12141.213089653123\n",
      "    val_log_marginal: -12149.094153897742\n",
      "Train Epoch: 4045 [256/118836 (0%)] Loss: 12164.042969\n",
      "Train Epoch: 4045 [33024/118836 (28%)] Loss: 12283.895508\n",
      "Train Epoch: 4045 [65792/118836 (55%)] Loss: 12172.247070\n",
      "Train Epoch: 4045 [98560/118836 (83%)] Loss: 12253.371094\n",
      "    epoch          : 4045\n",
      "    loss           : 12216.721417655344\n",
      "    val_loss       : 12221.69395477243\n",
      "    val_log_likelihood: -12139.507065336797\n",
      "    val_log_marginal: -12147.395395791513\n",
      "Train Epoch: 4046 [256/118836 (0%)] Loss: 12295.065430\n",
      "Train Epoch: 4046 [33024/118836 (28%)] Loss: 12146.375000\n",
      "Train Epoch: 4046 [65792/118836 (55%)] Loss: 12340.550781\n",
      "Train Epoch: 4046 [98560/118836 (83%)] Loss: 12227.060547\n",
      "    epoch          : 4046\n",
      "    loss           : 12218.605500575113\n",
      "    val_loss       : 12218.45584367472\n",
      "    val_log_likelihood: -12139.81204750181\n",
      "    val_log_marginal: -12147.694064227255\n",
      "Train Epoch: 4047 [256/118836 (0%)] Loss: 12254.673828\n",
      "Train Epoch: 4047 [33024/118836 (28%)] Loss: 12203.706055\n",
      "Train Epoch: 4047 [65792/118836 (55%)] Loss: 12139.613281\n",
      "Train Epoch: 4047 [98560/118836 (83%)] Loss: 12264.017578\n",
      "    epoch          : 4047\n",
      "    loss           : 12216.194301204507\n",
      "    val_loss       : 12216.882896610276\n",
      "    val_log_likelihood: -12140.126104509149\n",
      "    val_log_marginal: -12148.013687277662\n",
      "Train Epoch: 4048 [256/118836 (0%)] Loss: 12219.107422\n",
      "Train Epoch: 4048 [33024/118836 (28%)] Loss: 12224.069336\n",
      "Train Epoch: 4048 [65792/118836 (55%)] Loss: 12366.045898\n",
      "Train Epoch: 4048 [98560/118836 (83%)] Loss: 12174.386719\n",
      "    epoch          : 4048\n",
      "    loss           : 12219.056873578369\n",
      "    val_loss       : 12215.781187707582\n",
      "    val_log_likelihood: -12139.144786335555\n",
      "    val_log_marginal: -12147.029267263386\n",
      "Train Epoch: 4049 [256/118836 (0%)] Loss: 12165.283203\n",
      "Train Epoch: 4049 [33024/118836 (28%)] Loss: 12173.287109\n",
      "Train Epoch: 4049 [65792/118836 (55%)] Loss: 12336.800781\n",
      "Train Epoch: 4049 [98560/118836 (83%)] Loss: 12142.280273\n",
      "    epoch          : 4049\n",
      "    loss           : 12218.988656689413\n",
      "    val_loss       : 12217.61647200763\n",
      "    val_log_likelihood: -12138.585262387562\n",
      "    val_log_marginal: -12146.467461113116\n",
      "Train Epoch: 4050 [256/118836 (0%)] Loss: 12319.916016\n",
      "Train Epoch: 4050 [33024/118836 (28%)] Loss: 12248.340820\n",
      "Train Epoch: 4050 [65792/118836 (55%)] Loss: 12216.171875\n",
      "Train Epoch: 4050 [98560/118836 (83%)] Loss: 12305.458008\n",
      "    epoch          : 4050\n",
      "    loss           : 12219.468988122931\n",
      "    val_loss       : 12216.769783383705\n",
      "    val_log_likelihood: -12138.782466947116\n",
      "    val_log_marginal: -12146.661276085752\n",
      "Train Epoch: 4051 [256/118836 (0%)] Loss: 12178.593750\n",
      "Train Epoch: 4051 [33024/118836 (28%)] Loss: 12257.750000\n",
      "Train Epoch: 4051 [65792/118836 (55%)] Loss: 12191.830078\n",
      "Train Epoch: 4051 [98560/118836 (83%)] Loss: 12217.080078\n",
      "    epoch          : 4051\n",
      "    loss           : 12213.262970107011\n",
      "    val_loss       : 12220.198819950027\n",
      "    val_log_likelihood: -12141.178938236662\n",
      "    val_log_marginal: -12149.071317390255\n",
      "Train Epoch: 4052 [256/118836 (0%)] Loss: 12245.977539\n",
      "Train Epoch: 4052 [33024/118836 (28%)] Loss: 12266.038086\n",
      "Train Epoch: 4052 [65792/118836 (55%)] Loss: 12222.070312\n",
      "Train Epoch: 4052 [98560/118836 (83%)] Loss: 12337.193359\n",
      "    epoch          : 4052\n",
      "    loss           : 12220.53851436492\n",
      "    val_loss       : 12216.166478085615\n",
      "    val_log_likelihood: -12138.977717573925\n",
      "    val_log_marginal: -12146.861964661446\n",
      "Train Epoch: 4053 [256/118836 (0%)] Loss: 12240.245117\n",
      "Train Epoch: 4053 [33024/118836 (28%)] Loss: 12215.911133\n",
      "Train Epoch: 4053 [65792/118836 (55%)] Loss: 12303.772461\n",
      "Train Epoch: 4053 [98560/118836 (83%)] Loss: 12169.239258\n",
      "    epoch          : 4053\n",
      "    loss           : 12220.23111946857\n",
      "    val_loss       : 12220.409957183385\n",
      "    val_log_likelihood: -12139.042799769955\n",
      "    val_log_marginal: -12146.934964434568\n",
      "Train Epoch: 4054 [256/118836 (0%)] Loss: 12246.134766\n",
      "Train Epoch: 4054 [33024/118836 (28%)] Loss: 12246.361328\n",
      "Train Epoch: 4054 [65792/118836 (55%)] Loss: 12166.517578\n",
      "Train Epoch: 4054 [98560/118836 (83%)] Loss: 12299.985352\n",
      "    epoch          : 4054\n",
      "    loss           : 12218.442862612437\n",
      "    val_loss       : 12221.074907527658\n",
      "    val_log_likelihood: -12138.99367310277\n",
      "    val_log_marginal: -12146.885107954768\n",
      "Train Epoch: 4055 [256/118836 (0%)] Loss: 12318.047852\n",
      "Train Epoch: 4055 [33024/118836 (28%)] Loss: 12223.261719\n",
      "Train Epoch: 4055 [65792/118836 (55%)] Loss: 12160.273438\n",
      "Train Epoch: 4055 [98560/118836 (83%)] Loss: 12182.193359\n",
      "    epoch          : 4055\n",
      "    loss           : 12221.262736830542\n",
      "    val_loss       : 12216.1727819931\n",
      "    val_log_likelihood: -12138.866088806608\n",
      "    val_log_marginal: -12146.75131409038\n",
      "Train Epoch: 4056 [256/118836 (0%)] Loss: 12188.493164\n",
      "Train Epoch: 4056 [33024/118836 (28%)] Loss: 12231.910156\n",
      "Train Epoch: 4056 [65792/118836 (55%)] Loss: 12171.247070\n",
      "Train Epoch: 4056 [98560/118836 (83%)] Loss: 12276.328125\n",
      "    epoch          : 4056\n",
      "    loss           : 12216.20094037557\n",
      "    val_loss       : 12218.259881624317\n",
      "    val_log_likelihood: -12141.157827200941\n",
      "    val_log_marginal: -12149.04784693645\n",
      "Train Epoch: 4057 [256/118836 (0%)] Loss: 12271.721680\n",
      "Train Epoch: 4057 [33024/118836 (28%)] Loss: 12328.240234\n",
      "Train Epoch: 4057 [65792/118836 (55%)] Loss: 12182.684570\n",
      "Train Epoch: 4057 [98560/118836 (83%)] Loss: 12302.253906\n",
      "    epoch          : 4057\n",
      "    loss           : 12217.291326121796\n",
      "    val_loss       : 12220.844061156975\n",
      "    val_log_likelihood: -12139.314545207817\n",
      "    val_log_marginal: -12147.202962989093\n",
      "Train Epoch: 4058 [256/118836 (0%)] Loss: 12143.526367\n",
      "Train Epoch: 4058 [33024/118836 (28%)] Loss: 12210.075195\n",
      "Train Epoch: 4058 [65792/118836 (55%)] Loss: 12232.282227\n",
      "Train Epoch: 4058 [98560/118836 (83%)] Loss: 12215.716797\n",
      "    epoch          : 4058\n",
      "    loss           : 12215.893291201406\n",
      "    val_loss       : 12216.492741107239\n",
      "    val_log_likelihood: -12139.954792183624\n",
      "    val_log_marginal: -12147.842758630622\n",
      "Train Epoch: 4059 [256/118836 (0%)] Loss: 12199.025391\n",
      "Train Epoch: 4059 [33024/118836 (28%)] Loss: 12159.735352\n",
      "Train Epoch: 4059 [65792/118836 (55%)] Loss: 12307.690430\n",
      "Train Epoch: 4059 [98560/118836 (83%)] Loss: 12293.156250\n",
      "    epoch          : 4059\n",
      "    loss           : 12219.274078364091\n",
      "    val_loss       : 12217.519769592103\n",
      "    val_log_likelihood: -12140.432461939103\n",
      "    val_log_marginal: -12148.323293265623\n",
      "Train Epoch: 4060 [256/118836 (0%)] Loss: 12202.330078\n",
      "Train Epoch: 4060 [33024/118836 (28%)] Loss: 12202.622070\n",
      "Train Epoch: 4060 [65792/118836 (55%)] Loss: 12235.015625\n",
      "Train Epoch: 4060 [98560/118836 (83%)] Loss: 12187.220703\n",
      "    epoch          : 4060\n",
      "    loss           : 12215.559641555521\n",
      "    val_loss       : 12218.502341597457\n",
      "    val_log_likelihood: -12138.703097859801\n",
      "    val_log_marginal: -12146.597156371863\n",
      "Train Epoch: 4061 [256/118836 (0%)] Loss: 12205.294922\n",
      "Train Epoch: 4061 [33024/118836 (28%)] Loss: 12359.341797\n",
      "Train Epoch: 4061 [65792/118836 (55%)] Loss: 12259.575195\n",
      "Train Epoch: 4061 [98560/118836 (83%)] Loss: 12210.476562\n",
      "    epoch          : 4061\n",
      "    loss           : 12220.82800771557\n",
      "    val_loss       : 12214.694697820978\n",
      "    val_log_likelihood: -12139.876363471878\n",
      "    val_log_marginal: -12147.774119059057\n",
      "Train Epoch: 4062 [256/118836 (0%)] Loss: 12221.057617\n",
      "Train Epoch: 4062 [33024/118836 (28%)] Loss: 12317.466797\n",
      "Train Epoch: 4062 [65792/118836 (55%)] Loss: 12212.048828\n",
      "Train Epoch: 4062 [98560/118836 (83%)] Loss: 12201.498047\n",
      "    epoch          : 4062\n",
      "    loss           : 12217.206704113678\n",
      "    val_loss       : 12215.466741582124\n",
      "    val_log_likelihood: -12137.234217813017\n",
      "    val_log_marginal: -12145.13046099292\n",
      "Train Epoch: 4063 [256/118836 (0%)] Loss: 12244.191406\n",
      "Train Epoch: 4063 [33024/118836 (28%)] Loss: 12397.824219\n",
      "Train Epoch: 4063 [65792/118836 (55%)] Loss: 12206.556641\n",
      "Train Epoch: 4063 [98560/118836 (83%)] Loss: 12193.210938\n",
      "    epoch          : 4063\n",
      "    loss           : 12218.220664837934\n",
      "    val_loss       : 12220.334078754906\n",
      "    val_log_likelihood: -12138.3161371097\n",
      "    val_log_marginal: -12146.20445189169\n",
      "Train Epoch: 4064 [256/118836 (0%)] Loss: 12260.445312\n",
      "Train Epoch: 4064 [33024/118836 (28%)] Loss: 12184.733398\n",
      "Train Epoch: 4064 [65792/118836 (55%)] Loss: 12344.587891\n",
      "Train Epoch: 4064 [98560/118836 (83%)] Loss: 12177.791016\n",
      "    epoch          : 4064\n",
      "    loss           : 12222.906496361922\n",
      "    val_loss       : 12213.34087865913\n",
      "    val_log_likelihood: -12139.908014920646\n",
      "    val_log_marginal: -12147.800562569044\n",
      "Train Epoch: 4065 [256/118836 (0%)] Loss: 12174.494141\n",
      "Train Epoch: 4065 [33024/118836 (28%)] Loss: 12189.494141\n",
      "Train Epoch: 4065 [65792/118836 (55%)] Loss: 12212.744141\n",
      "Train Epoch: 4065 [98560/118836 (83%)] Loss: 12270.212891\n",
      "    epoch          : 4065\n",
      "    loss           : 12219.159944459521\n",
      "    val_loss       : 12219.61385211035\n",
      "    val_log_likelihood: -12140.613327291407\n",
      "    val_log_marginal: -12148.507687417145\n",
      "Train Epoch: 4066 [256/118836 (0%)] Loss: 12185.710938\n",
      "Train Epoch: 4066 [33024/118836 (28%)] Loss: 12231.366211\n",
      "Train Epoch: 4066 [65792/118836 (55%)] Loss: 12189.041992\n",
      "Train Epoch: 4066 [98560/118836 (83%)] Loss: 12275.649414\n",
      "    epoch          : 4066\n",
      "    loss           : 12216.759180818342\n",
      "    val_loss       : 12217.609146167913\n",
      "    val_log_likelihood: -12139.620396828474\n",
      "    val_log_marginal: -12147.510896751808\n",
      "Train Epoch: 4067 [256/118836 (0%)] Loss: 12256.249023\n",
      "Train Epoch: 4067 [33024/118836 (28%)] Loss: 12202.928711\n",
      "Train Epoch: 4067 [65792/118836 (55%)] Loss: 12156.083984\n",
      "Train Epoch: 4067 [98560/118836 (83%)] Loss: 12231.541992\n",
      "    epoch          : 4067\n",
      "    loss           : 12218.306086027967\n",
      "    val_loss       : 12218.707805855856\n",
      "    val_log_likelihood: -12138.927009343983\n",
      "    val_log_marginal: -12146.816953182\n",
      "Train Epoch: 4068 [256/118836 (0%)] Loss: 12181.686523\n",
      "Train Epoch: 4068 [33024/118836 (28%)] Loss: 12268.563477\n",
      "Train Epoch: 4068 [65792/118836 (55%)] Loss: 12215.617188\n",
      "Train Epoch: 4068 [98560/118836 (83%)] Loss: 12152.348633\n",
      "    epoch          : 4068\n",
      "    loss           : 12213.787155901055\n",
      "    val_loss       : 12219.832804180407\n",
      "    val_log_likelihood: -12141.054278943082\n",
      "    val_log_marginal: -12148.944885387866\n",
      "Train Epoch: 4069 [256/118836 (0%)] Loss: 12264.015625\n",
      "Train Epoch: 4069 [33024/118836 (28%)] Loss: 12183.530273\n",
      "Train Epoch: 4069 [65792/118836 (55%)] Loss: 12152.613281\n",
      "Train Epoch: 4069 [98560/118836 (83%)] Loss: 12212.521484\n",
      "    epoch          : 4069\n",
      "    loss           : 12218.164263628258\n",
      "    val_loss       : 12216.767049114876\n",
      "    val_log_likelihood: -12139.487315349721\n",
      "    val_log_marginal: -12147.382941062584\n",
      "Train Epoch: 4070 [256/118836 (0%)] Loss: 12182.314453\n",
      "Train Epoch: 4070 [33024/118836 (28%)] Loss: 12255.620117\n",
      "Train Epoch: 4070 [65792/118836 (55%)] Loss: 12256.436523\n",
      "Train Epoch: 4070 [98560/118836 (83%)] Loss: 12244.994141\n",
      "    epoch          : 4070\n",
      "    loss           : 12223.85331756617\n",
      "    val_loss       : 12215.8514065387\n",
      "    val_log_likelihood: -12137.743539501913\n",
      "    val_log_marginal: -12145.631095653373\n",
      "Train Epoch: 4071 [256/118836 (0%)] Loss: 12191.182617\n",
      "Train Epoch: 4071 [33024/118836 (28%)] Loss: 12193.925781\n",
      "Train Epoch: 4071 [65792/118836 (55%)] Loss: 12239.984375\n",
      "Train Epoch: 4071 [98560/118836 (83%)] Loss: 12181.706055\n",
      "    epoch          : 4071\n",
      "    loss           : 12217.535687422456\n",
      "    val_loss       : 12216.407324455988\n",
      "    val_log_likelihood: -12140.273209069996\n",
      "    val_log_marginal: -12148.170981281193\n",
      "Train Epoch: 4072 [256/118836 (0%)] Loss: 12199.084961\n",
      "Train Epoch: 4072 [33024/118836 (28%)] Loss: 12109.773438\n",
      "Train Epoch: 4072 [65792/118836 (55%)] Loss: 12167.457031\n",
      "Train Epoch: 4072 [98560/118836 (83%)] Loss: 12217.595703\n",
      "    epoch          : 4072\n",
      "    loss           : 12218.867509789858\n",
      "    val_loss       : 12216.198024897756\n",
      "    val_log_likelihood: -12141.4881332713\n",
      "    val_log_marginal: -12149.382950194478\n",
      "Train Epoch: 4073 [256/118836 (0%)] Loss: 12185.443359\n",
      "Train Epoch: 4073 [33024/118836 (28%)] Loss: 12182.337891\n",
      "Train Epoch: 4073 [65792/118836 (55%)] Loss: 12299.855469\n",
      "Train Epoch: 4073 [98560/118836 (83%)] Loss: 12237.715820\n",
      "    epoch          : 4073\n",
      "    loss           : 12218.129822393248\n",
      "    val_loss       : 12215.540342285218\n",
      "    val_log_likelihood: -12139.205222388078\n",
      "    val_log_marginal: -12147.097112924981\n",
      "Train Epoch: 4074 [256/118836 (0%)] Loss: 12189.623047\n",
      "Train Epoch: 4074 [33024/118836 (28%)] Loss: 12245.962891\n",
      "Train Epoch: 4074 [65792/118836 (55%)] Loss: 12191.660156\n",
      "Train Epoch: 4074 [98560/118836 (83%)] Loss: 12271.271484\n",
      "    epoch          : 4074\n",
      "    loss           : 12217.231046125413\n",
      "    val_loss       : 12220.530805799854\n",
      "    val_log_likelihood: -12139.227968459212\n",
      "    val_log_marginal: -12147.122246270095\n",
      "Train Epoch: 4075 [256/118836 (0%)] Loss: 12259.067383\n",
      "Train Epoch: 4075 [33024/118836 (28%)] Loss: 12176.360352\n",
      "Train Epoch: 4075 [65792/118836 (55%)] Loss: 12249.677734\n",
      "Train Epoch: 4075 [98560/118836 (83%)] Loss: 12270.604492\n",
      "    epoch          : 4075\n",
      "    loss           : 12216.47601953448\n",
      "    val_loss       : 12219.116010405398\n",
      "    val_log_likelihood: -12138.969424466242\n",
      "    val_log_marginal: -12146.863669295293\n",
      "Train Epoch: 4076 [256/118836 (0%)] Loss: 12249.854492\n",
      "Train Epoch: 4076 [33024/118836 (28%)] Loss: 12334.599609\n",
      "Train Epoch: 4076 [65792/118836 (55%)] Loss: 12205.625977\n",
      "Train Epoch: 4076 [98560/118836 (83%)] Loss: 12201.091797\n",
      "    epoch          : 4076\n",
      "    loss           : 12221.974924072065\n",
      "    val_loss       : 12219.694564137746\n",
      "    val_log_likelihood: -12139.139440685743\n",
      "    val_log_marginal: -12147.028440133938\n",
      "Train Epoch: 4077 [256/118836 (0%)] Loss: 12286.681641\n",
      "Train Epoch: 4077 [33024/118836 (28%)] Loss: 12263.932617\n",
      "Train Epoch: 4077 [65792/118836 (55%)] Loss: 12284.435547\n",
      "Train Epoch: 4077 [98560/118836 (83%)] Loss: 12305.106445\n",
      "    epoch          : 4077\n",
      "    loss           : 12217.658818949029\n",
      "    val_loss       : 12216.56101777044\n",
      "    val_log_likelihood: -12140.253006261632\n",
      "    val_log_marginal: -12148.146676107332\n",
      "Train Epoch: 4078 [256/118836 (0%)] Loss: 12168.046875\n",
      "Train Epoch: 4078 [33024/118836 (28%)] Loss: 12189.875000\n",
      "Train Epoch: 4078 [65792/118836 (55%)] Loss: 12200.945312\n",
      "Train Epoch: 4078 [98560/118836 (83%)] Loss: 12216.416016\n",
      "    epoch          : 4078\n",
      "    loss           : 12219.349161077078\n",
      "    val_loss       : 12216.28524822647\n",
      "    val_log_likelihood: -12140.483809417648\n",
      "    val_log_marginal: -12148.373494012056\n",
      "Train Epoch: 4079 [256/118836 (0%)] Loss: 12336.409180\n",
      "Train Epoch: 4079 [33024/118836 (28%)] Loss: 12229.756836\n",
      "Train Epoch: 4079 [65792/118836 (55%)] Loss: 12174.249023\n",
      "Train Epoch: 4079 [98560/118836 (83%)] Loss: 12242.997070\n",
      "    epoch          : 4079\n",
      "    loss           : 12217.533480988937\n",
      "    val_loss       : 12219.845961199178\n",
      "    val_log_likelihood: -12140.306604761166\n",
      "    val_log_marginal: -12148.204416604396\n",
      "Train Epoch: 4080 [256/118836 (0%)] Loss: 12112.024414\n",
      "Train Epoch: 4080 [33024/118836 (28%)] Loss: 12292.826172\n",
      "Train Epoch: 4080 [65792/118836 (55%)] Loss: 12272.437500\n",
      "Train Epoch: 4080 [98560/118836 (83%)] Loss: 12203.088867\n",
      "    epoch          : 4080\n",
      "    loss           : 12215.367520128979\n",
      "    val_loss       : 12220.056025913042\n",
      "    val_log_likelihood: -12140.346094719293\n",
      "    val_log_marginal: -12148.23911178091\n",
      "Train Epoch: 4081 [256/118836 (0%)] Loss: 12170.750977\n",
      "Train Epoch: 4081 [33024/118836 (28%)] Loss: 12140.259766\n",
      "Train Epoch: 4081 [65792/118836 (55%)] Loss: 12180.938477\n",
      "Train Epoch: 4081 [98560/118836 (83%)] Loss: 12233.263672\n",
      "    epoch          : 4081\n",
      "    loss           : 12218.030447910205\n",
      "    val_loss       : 12218.12178949919\n",
      "    val_log_likelihood: -12140.694747079198\n",
      "    val_log_marginal: -12148.58977738192\n",
      "Train Epoch: 4082 [256/118836 (0%)] Loss: 12295.826172\n",
      "Train Epoch: 4082 [33024/118836 (28%)] Loss: 12176.512695\n",
      "Train Epoch: 4082 [65792/118836 (55%)] Loss: 12215.958984\n",
      "Train Epoch: 4082 [98560/118836 (83%)] Loss: 12214.527344\n",
      "    epoch          : 4082\n",
      "    loss           : 12213.677437609853\n",
      "    val_loss       : 12214.247732982712\n",
      "    val_log_likelihood: -12142.82127420001\n",
      "    val_log_marginal: -12150.711358430179\n",
      "Train Epoch: 4083 [256/118836 (0%)] Loss: 12136.990234\n",
      "Train Epoch: 4083 [33024/118836 (28%)] Loss: 12260.773438\n",
      "Train Epoch: 4083 [65792/118836 (55%)] Loss: 12208.505859\n",
      "Train Epoch: 4083 [98560/118836 (83%)] Loss: 12133.454102\n",
      "    epoch          : 4083\n",
      "    loss           : 12220.304966010133\n",
      "    val_loss       : 12220.922817925906\n",
      "    val_log_likelihood: -12139.66431710091\n",
      "    val_log_marginal: -12147.561346658562\n",
      "Train Epoch: 4084 [256/118836 (0%)] Loss: 12114.062500\n",
      "Train Epoch: 4084 [33024/118836 (28%)] Loss: 12166.136719\n",
      "Train Epoch: 4084 [65792/118836 (55%)] Loss: 12210.471680\n",
      "Train Epoch: 4084 [98560/118836 (83%)] Loss: 12229.587891\n",
      "    epoch          : 4084\n",
      "    loss           : 12210.0769935122\n",
      "    val_loss       : 12216.31310391964\n",
      "    val_log_likelihood: -12140.13878237438\n",
      "    val_log_marginal: -12148.033759756237\n",
      "Train Epoch: 4085 [256/118836 (0%)] Loss: 12217.039062\n",
      "Train Epoch: 4085 [33024/118836 (28%)] Loss: 12279.958984\n",
      "Train Epoch: 4085 [65792/118836 (55%)] Loss: 12216.497070\n",
      "Train Epoch: 4085 [98560/118836 (83%)] Loss: 12210.104492\n",
      "    epoch          : 4085\n",
      "    loss           : 12213.563815168785\n",
      "    val_loss       : 12217.448305295151\n",
      "    val_log_likelihood: -12138.978749224565\n",
      "    val_log_marginal: -12146.865903300479\n",
      "Train Epoch: 4086 [256/118836 (0%)] Loss: 12182.345703\n",
      "Train Epoch: 4086 [33024/118836 (28%)] Loss: 12332.388672\n",
      "Train Epoch: 4086 [65792/118836 (55%)] Loss: 12257.537109\n",
      "Train Epoch: 4086 [98560/118836 (83%)] Loss: 12239.009766\n",
      "    epoch          : 4086\n",
      "    loss           : 12217.794670505067\n",
      "    val_loss       : 12216.51907571355\n",
      "    val_log_likelihood: -12138.284671603598\n",
      "    val_log_marginal: -12146.181916555828\n",
      "Train Epoch: 4087 [256/118836 (0%)] Loss: 12196.250000\n",
      "Train Epoch: 4087 [33024/118836 (28%)] Loss: 12166.509766\n",
      "Train Epoch: 4087 [65792/118836 (55%)] Loss: 12232.037109\n",
      "Train Epoch: 4087 [98560/118836 (83%)] Loss: 12168.483398\n",
      "    epoch          : 4087\n",
      "    loss           : 12214.85814109026\n",
      "    val_loss       : 12220.195009284951\n",
      "    val_log_likelihood: -12138.315705128205\n",
      "    val_log_marginal: -12146.210519827677\n",
      "Train Epoch: 4088 [256/118836 (0%)] Loss: 12208.922852\n",
      "Train Epoch: 4088 [33024/118836 (28%)] Loss: 12162.716797\n",
      "Train Epoch: 4088 [65792/118836 (55%)] Loss: 12277.625000\n",
      "Train Epoch: 4088 [98560/118836 (83%)] Loss: 12259.315430\n",
      "    epoch          : 4088\n",
      "    loss           : 12216.624064309346\n",
      "    val_loss       : 12215.496339992873\n",
      "    val_log_likelihood: -12140.379004310122\n",
      "    val_log_marginal: -12148.276398487329\n",
      "Train Epoch: 4089 [256/118836 (0%)] Loss: 12199.138672\n",
      "Train Epoch: 4089 [33024/118836 (28%)] Loss: 12206.007812\n",
      "Train Epoch: 4089 [65792/118836 (55%)] Loss: 12286.329102\n",
      "Train Epoch: 4089 [98560/118836 (83%)] Loss: 12131.783203\n",
      "    epoch          : 4089\n",
      "    loss           : 12219.47340648263\n",
      "    val_loss       : 12218.025058635467\n",
      "    val_log_likelihood: -12140.691050034893\n",
      "    val_log_marginal: -12148.583175325472\n",
      "Train Epoch: 4090 [256/118836 (0%)] Loss: 12253.875000\n",
      "Train Epoch: 4090 [33024/118836 (28%)] Loss: 12199.789062\n",
      "Train Epoch: 4090 [65792/118836 (55%)] Loss: 12187.024414\n",
      "Train Epoch: 4090 [98560/118836 (83%)] Loss: 12305.978516\n",
      "    epoch          : 4090\n",
      "    loss           : 12212.446261114557\n",
      "    val_loss       : 12215.183898289835\n",
      "    val_log_likelihood: -12140.171798587418\n",
      "    val_log_marginal: -12148.061089839373\n",
      "Train Epoch: 4091 [256/118836 (0%)] Loss: 12252.271484\n",
      "Train Epoch: 4091 [33024/118836 (28%)] Loss: 12261.437500\n",
      "Train Epoch: 4091 [65792/118836 (55%)] Loss: 12216.618164\n",
      "Train Epoch: 4091 [98560/118836 (83%)] Loss: 12287.955078\n",
      "    epoch          : 4091\n",
      "    loss           : 12215.804165374275\n",
      "    val_loss       : 12220.678539863631\n",
      "    val_log_likelihood: -12140.158386644436\n",
      "    val_log_marginal: -12148.050409172114\n",
      "Train Epoch: 4092 [256/118836 (0%)] Loss: 12361.459961\n",
      "Train Epoch: 4092 [33024/118836 (28%)] Loss: 12216.963867\n",
      "Train Epoch: 4092 [65792/118836 (55%)] Loss: 12270.029297\n",
      "Train Epoch: 4092 [98560/118836 (83%)] Loss: 12256.066406\n",
      "    epoch          : 4092\n",
      "    loss           : 12212.777078163772\n",
      "    val_loss       : 12214.157603493677\n",
      "    val_log_likelihood: -12139.185203745348\n",
      "    val_log_marginal: -12147.073710972636\n",
      "Train Epoch: 4093 [256/118836 (0%)] Loss: 12210.667969\n",
      "Train Epoch: 4093 [33024/118836 (28%)] Loss: 12299.325195\n",
      "Train Epoch: 4093 [65792/118836 (55%)] Loss: 12275.945312\n",
      "Train Epoch: 4093 [98560/118836 (83%)] Loss: 12200.484375\n",
      "    epoch          : 4093\n",
      "    loss           : 12217.921529931762\n",
      "    val_loss       : 12223.106780272155\n",
      "    val_log_likelihood: -12138.213228423543\n",
      "    val_log_marginal: -12146.114306375835\n",
      "Train Epoch: 4094 [256/118836 (0%)] Loss: 12303.013672\n",
      "Train Epoch: 4094 [33024/118836 (28%)] Loss: 12194.721680\n",
      "Train Epoch: 4094 [65792/118836 (55%)] Loss: 12224.135742\n",
      "Train Epoch: 4094 [98560/118836 (83%)] Loss: 12268.237305\n",
      "    epoch          : 4094\n",
      "    loss           : 12217.233848835556\n",
      "    val_loss       : 12222.09056357072\n",
      "    val_log_likelihood: -12139.379060852203\n",
      "    val_log_marginal: -12147.264250879676\n",
      "Train Epoch: 4095 [256/118836 (0%)] Loss: 12266.726562\n",
      "Train Epoch: 4095 [33024/118836 (28%)] Loss: 12265.307617\n",
      "Train Epoch: 4095 [65792/118836 (55%)] Loss: 12251.591797\n",
      "Train Epoch: 4095 [98560/118836 (83%)] Loss: 12190.968750\n",
      "    epoch          : 4095\n",
      "    loss           : 12215.09311155914\n",
      "    val_loss       : 12220.294185479015\n",
      "    val_log_likelihood: -12140.954786852513\n",
      "    val_log_marginal: -12148.849279237618\n",
      "Train Epoch: 4096 [256/118836 (0%)] Loss: 12254.252930\n",
      "Train Epoch: 4096 [33024/118836 (28%)] Loss: 12236.916016\n",
      "Train Epoch: 4096 [65792/118836 (55%)] Loss: 12252.881836\n",
      "Train Epoch: 4096 [98560/118836 (83%)] Loss: 12192.181641\n",
      "    epoch          : 4096\n",
      "    loss           : 12215.227162492245\n",
      "    val_loss       : 12217.95473922743\n",
      "    val_log_likelihood: -12138.012644909275\n",
      "    val_log_marginal: -12145.907849239273\n",
      "Train Epoch: 4097 [256/118836 (0%)] Loss: 12277.372070\n",
      "Train Epoch: 4097 [33024/118836 (28%)] Loss: 12279.121094\n",
      "Train Epoch: 4097 [65792/118836 (55%)] Loss: 12223.182617\n",
      "Train Epoch: 4097 [98560/118836 (83%)] Loss: 12175.707031\n",
      "    epoch          : 4097\n",
      "    loss           : 12215.224598389681\n",
      "    val_loss       : 12216.454707955823\n",
      "    val_log_likelihood: -12139.752486720688\n",
      "    val_log_marginal: -12147.645931503459\n",
      "Train Epoch: 4098 [256/118836 (0%)] Loss: 12203.900391\n",
      "Train Epoch: 4098 [33024/118836 (28%)] Loss: 12211.172852\n",
      "Train Epoch: 4098 [65792/118836 (55%)] Loss: 12160.170898\n",
      "Train Epoch: 4098 [98560/118836 (83%)] Loss: 12389.965820\n",
      "    epoch          : 4098\n",
      "    loss           : 12217.094571798749\n",
      "    val_loss       : 12218.474855846505\n",
      "    val_log_likelihood: -12140.191105607682\n",
      "    val_log_marginal: -12148.078197782264\n",
      "Train Epoch: 4099 [256/118836 (0%)] Loss: 12193.667969\n",
      "Train Epoch: 4099 [33024/118836 (28%)] Loss: 12248.138672\n",
      "Train Epoch: 4099 [65792/118836 (55%)] Loss: 12158.042969\n",
      "Train Epoch: 4099 [98560/118836 (83%)] Loss: 12263.262695\n",
      "    epoch          : 4099\n",
      "    loss           : 12224.396972898574\n",
      "    val_loss       : 12220.867107686798\n",
      "    val_log_likelihood: -12137.922393733199\n",
      "    val_log_marginal: -12145.81161893808\n",
      "Train Epoch: 4100 [256/118836 (0%)] Loss: 12194.147461\n",
      "Train Epoch: 4100 [33024/118836 (28%)] Loss: 12317.430664\n",
      "Train Epoch: 4100 [65792/118836 (55%)] Loss: 12276.798828\n",
      "Train Epoch: 4100 [98560/118836 (83%)] Loss: 12269.779297\n",
      "    epoch          : 4100\n",
      "    loss           : 12212.26157594086\n",
      "    val_loss       : 12219.372766780722\n",
      "    val_log_likelihood: -12140.436915031793\n",
      "    val_log_marginal: -12148.333188465731\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch4100.pth ...\n",
      "Train Epoch: 4101 [256/118836 (0%)] Loss: 12248.025391\n",
      "Train Epoch: 4101 [33024/118836 (28%)] Loss: 12222.812500\n",
      "Train Epoch: 4101 [65792/118836 (55%)] Loss: 12281.941406\n",
      "Train Epoch: 4101 [98560/118836 (83%)] Loss: 12280.625000\n",
      "    epoch          : 4101\n",
      "    loss           : 12216.664828564413\n",
      "    val_loss       : 12217.922350095705\n",
      "    val_log_likelihood: -12140.486108257082\n",
      "    val_log_marginal: -12148.375330900375\n",
      "Train Epoch: 4102 [256/118836 (0%)] Loss: 12301.785156\n",
      "Train Epoch: 4102 [33024/118836 (28%)] Loss: 12167.619141\n",
      "Train Epoch: 4102 [65792/118836 (55%)] Loss: 12233.816406\n",
      "Train Epoch: 4102 [98560/118836 (83%)] Loss: 12283.061523\n",
      "    epoch          : 4102\n",
      "    loss           : 12218.80200304358\n",
      "    val_loss       : 12216.22590913074\n",
      "    val_log_likelihood: -12140.29142515121\n",
      "    val_log_marginal: -12148.181362881713\n",
      "Train Epoch: 4103 [256/118836 (0%)] Loss: 12383.517578\n",
      "Train Epoch: 4103 [33024/118836 (28%)] Loss: 12241.809570\n",
      "Train Epoch: 4103 [65792/118836 (55%)] Loss: 12328.835938\n",
      "Train Epoch: 4103 [98560/118836 (83%)] Loss: 12371.350586\n",
      "    epoch          : 4103\n",
      "    loss           : 12217.879140657311\n",
      "    val_loss       : 12214.650624512054\n",
      "    val_log_likelihood: -12139.062289501913\n",
      "    val_log_marginal: -12146.953316123876\n",
      "Train Epoch: 4104 [256/118836 (0%)] Loss: 12239.213867\n",
      "Train Epoch: 4104 [33024/118836 (28%)] Loss: 12194.953125\n",
      "Train Epoch: 4104 [65792/118836 (55%)] Loss: 12157.758789\n",
      "Train Epoch: 4104 [98560/118836 (83%)] Loss: 12317.178711\n",
      "    epoch          : 4104\n",
      "    loss           : 12217.627514183983\n",
      "    val_loss       : 12220.987855566387\n",
      "    val_log_likelihood: -12141.827363943601\n",
      "    val_log_marginal: -12149.714162069895\n",
      "Train Epoch: 4105 [256/118836 (0%)] Loss: 12289.937500\n",
      "Train Epoch: 4105 [33024/118836 (28%)] Loss: 12230.910156\n",
      "Train Epoch: 4105 [65792/118836 (55%)] Loss: 12239.311523\n",
      "Train Epoch: 4105 [98560/118836 (83%)] Loss: 12252.063477\n",
      "    epoch          : 4105\n",
      "    loss           : 12221.941584922974\n",
      "    val_loss       : 12220.07561305336\n",
      "    val_log_likelihood: -12139.504969241108\n",
      "    val_log_marginal: -12147.397976559221\n",
      "Train Epoch: 4106 [256/118836 (0%)] Loss: 12185.732422\n",
      "Train Epoch: 4106 [33024/118836 (28%)] Loss: 12226.255859\n",
      "Train Epoch: 4106 [65792/118836 (55%)] Loss: 12251.654297\n",
      "Train Epoch: 4106 [98560/118836 (83%)] Loss: 12250.861328\n",
      "    epoch          : 4106\n",
      "    loss           : 12216.616995580025\n",
      "    val_loss       : 12222.303151967622\n",
      "    val_log_likelihood: -12140.195106363732\n",
      "    val_log_marginal: -12148.088588072495\n",
      "Train Epoch: 4107 [256/118836 (0%)] Loss: 12262.392578\n",
      "Train Epoch: 4107 [33024/118836 (28%)] Loss: 12361.731445\n",
      "Train Epoch: 4107 [65792/118836 (55%)] Loss: 12275.705078\n",
      "Train Epoch: 4107 [98560/118836 (83%)] Loss: 12278.808594\n",
      "    epoch          : 4107\n",
      "    loss           : 12219.20956902011\n",
      "    val_loss       : 12216.788256857568\n",
      "    val_log_likelihood: -12139.085565291563\n",
      "    val_log_marginal: -12146.97077211337\n",
      "Train Epoch: 4108 [256/118836 (0%)] Loss: 12176.931641\n",
      "Train Epoch: 4108 [33024/118836 (28%)] Loss: 12194.553711\n",
      "Train Epoch: 4108 [65792/118836 (55%)] Loss: 12200.554688\n",
      "Train Epoch: 4108 [98560/118836 (83%)] Loss: 12205.867188\n",
      "    epoch          : 4108\n",
      "    loss           : 12217.642714472187\n",
      "    val_loss       : 12214.781966299168\n",
      "    val_log_likelihood: -12139.9132408628\n",
      "    val_log_marginal: -12147.807043855853\n",
      "Train Epoch: 4109 [256/118836 (0%)] Loss: 12210.347656\n",
      "Train Epoch: 4109 [33024/118836 (28%)] Loss: 12150.671875\n",
      "Train Epoch: 4109 [65792/118836 (55%)] Loss: 12258.660156\n",
      "Train Epoch: 4109 [98560/118836 (83%)] Loss: 12189.595703\n",
      "    epoch          : 4109\n",
      "    loss           : 12218.964217748397\n",
      "    val_loss       : 12213.039570282568\n",
      "    val_log_likelihood: -12141.842396382599\n",
      "    val_log_marginal: -12149.733285099612\n",
      "Train Epoch: 4110 [256/118836 (0%)] Loss: 12262.243164\n",
      "Train Epoch: 4110 [33024/118836 (28%)] Loss: 12275.757812\n",
      "Train Epoch: 4110 [65792/118836 (55%)] Loss: 12314.795898\n",
      "Train Epoch: 4110 [98560/118836 (83%)] Loss: 12260.616211\n",
      "    epoch          : 4110\n",
      "    loss           : 12214.685524904364\n",
      "    val_loss       : 12217.08197897023\n",
      "    val_log_likelihood: -12137.209922327338\n",
      "    val_log_marginal: -12145.09544416953\n",
      "Train Epoch: 4111 [256/118836 (0%)] Loss: 12230.544922\n",
      "Train Epoch: 4111 [33024/118836 (28%)] Loss: 12134.851562\n",
      "Train Epoch: 4111 [65792/118836 (55%)] Loss: 12272.066406\n",
      "Train Epoch: 4111 [98560/118836 (83%)] Loss: 12215.973633\n",
      "    epoch          : 4111\n",
      "    loss           : 12219.207873242349\n",
      "    val_loss       : 12219.715569692491\n",
      "    val_log_likelihood: -12140.656341436621\n",
      "    val_log_marginal: -12148.547390072692\n",
      "Train Epoch: 4112 [256/118836 (0%)] Loss: 12169.636719\n",
      "Train Epoch: 4112 [33024/118836 (28%)] Loss: 12260.317383\n",
      "Train Epoch: 4112 [65792/118836 (55%)] Loss: 12225.556641\n",
      "Train Epoch: 4112 [98560/118836 (83%)] Loss: 12267.379883\n",
      "    epoch          : 4112\n",
      "    loss           : 12216.160235570462\n",
      "    val_loss       : 12214.584397924815\n",
      "    val_log_likelihood: -12137.922412795959\n",
      "    val_log_marginal: -12145.812453191596\n",
      "Train Epoch: 4113 [256/118836 (0%)] Loss: 12242.218750\n",
      "Train Epoch: 4113 [33024/118836 (28%)] Loss: 12290.021484\n",
      "Train Epoch: 4113 [65792/118836 (55%)] Loss: 12190.407227\n",
      "Train Epoch: 4113 [98560/118836 (83%)] Loss: 12232.712891\n",
      "    epoch          : 4113\n",
      "    loss           : 12218.438094661135\n",
      "    val_loss       : 12216.004457882018\n",
      "    val_log_likelihood: -12139.819150641026\n",
      "    val_log_marginal: -12147.715167557975\n",
      "Train Epoch: 4114 [256/118836 (0%)] Loss: 12236.958008\n",
      "Train Epoch: 4114 [33024/118836 (28%)] Loss: 12289.799805\n",
      "Train Epoch: 4114 [65792/118836 (55%)] Loss: 12198.734375\n",
      "Train Epoch: 4114 [98560/118836 (83%)] Loss: 12170.108398\n",
      "    epoch          : 4114\n",
      "    loss           : 12213.62500791589\n",
      "    val_loss       : 12217.75251120597\n",
      "    val_log_likelihood: -12139.0016839847\n",
      "    val_log_marginal: -12146.88649460853\n",
      "Train Epoch: 4115 [256/118836 (0%)] Loss: 12320.680664\n",
      "Train Epoch: 4115 [33024/118836 (28%)] Loss: 12202.425781\n",
      "Train Epoch: 4115 [65792/118836 (55%)] Loss: 12196.830078\n",
      "Train Epoch: 4115 [98560/118836 (83%)] Loss: 12235.546875\n",
      "    epoch          : 4115\n",
      "    loss           : 12216.310873526676\n",
      "    val_loss       : 12215.731614284037\n",
      "    val_log_likelihood: -12140.404140172663\n",
      "    val_log_marginal: -12148.299262861756\n",
      "Train Epoch: 4116 [256/118836 (0%)] Loss: 12161.744141\n",
      "Train Epoch: 4116 [33024/118836 (28%)] Loss: 12145.077148\n",
      "Train Epoch: 4116 [65792/118836 (55%)] Loss: 12290.873047\n",
      "Train Epoch: 4116 [98560/118836 (83%)] Loss: 12236.314453\n",
      "    epoch          : 4116\n",
      "    loss           : 12214.081294264372\n",
      "    val_loss       : 12220.704283064402\n",
      "    val_log_likelihood: -12140.609940420802\n",
      "    val_log_marginal: -12148.500748461624\n",
      "Train Epoch: 4117 [256/118836 (0%)] Loss: 12277.849609\n",
      "Train Epoch: 4117 [33024/118836 (28%)] Loss: 12197.025391\n",
      "Train Epoch: 4117 [65792/118836 (55%)] Loss: 12275.962891\n",
      "Train Epoch: 4117 [98560/118836 (83%)] Loss: 12119.251953\n",
      "    epoch          : 4117\n",
      "    loss           : 12220.912977215157\n",
      "    val_loss       : 12217.05876202041\n",
      "    val_log_likelihood: -12140.509744785206\n",
      "    val_log_marginal: -12148.40944577102\n",
      "Train Epoch: 4118 [256/118836 (0%)] Loss: 12159.637695\n",
      "Train Epoch: 4118 [33024/118836 (28%)] Loss: 12289.091797\n",
      "Train Epoch: 4118 [65792/118836 (55%)] Loss: 12257.173828\n",
      "Train Epoch: 4118 [98560/118836 (83%)] Loss: 12143.667969\n",
      "    epoch          : 4118\n",
      "    loss           : 12219.689141658911\n",
      "    val_loss       : 12217.754028245858\n",
      "    val_log_likelihood: -12139.662875600963\n",
      "    val_log_marginal: -12147.555246377633\n",
      "Train Epoch: 4119 [256/118836 (0%)] Loss: 12233.472656\n",
      "Train Epoch: 4119 [33024/118836 (28%)] Loss: 12195.556641\n",
      "Train Epoch: 4119 [65792/118836 (55%)] Loss: 12158.966797\n",
      "Train Epoch: 4119 [98560/118836 (83%)] Loss: 12362.884766\n",
      "    epoch          : 4119\n",
      "    loss           : 12220.967268597498\n",
      "    val_loss       : 12217.339419809185\n",
      "    val_log_likelihood: -12140.074615352307\n",
      "    val_log_marginal: -12147.965578951265\n",
      "Train Epoch: 4120 [256/118836 (0%)] Loss: 12302.537109\n",
      "Train Epoch: 4120 [33024/118836 (28%)] Loss: 12228.091797\n",
      "Train Epoch: 4120 [65792/118836 (55%)] Loss: 12324.254883\n",
      "Train Epoch: 4120 [98560/118836 (83%)] Loss: 12204.536133\n",
      "    epoch          : 4120\n",
      "    loss           : 12216.141057789238\n",
      "    val_loss       : 12222.394966530432\n",
      "    val_log_likelihood: -12138.832952078163\n",
      "    val_log_marginal: -12146.723119674154\n",
      "Train Epoch: 4121 [256/118836 (0%)] Loss: 12197.500000\n",
      "Train Epoch: 4121 [33024/118836 (28%)] Loss: 12201.009766\n",
      "Train Epoch: 4121 [65792/118836 (55%)] Loss: 12398.092773\n",
      "Train Epoch: 4121 [98560/118836 (83%)] Loss: 12168.527344\n",
      "    epoch          : 4121\n",
      "    loss           : 12218.146944789081\n",
      "    val_loss       : 12219.375986858353\n",
      "    val_log_likelihood: -12140.373825701769\n",
      "    val_log_marginal: -12148.265991470542\n",
      "Train Epoch: 4122 [256/118836 (0%)] Loss: 12204.666992\n",
      "Train Epoch: 4122 [33024/118836 (28%)] Loss: 12181.343750\n",
      "Train Epoch: 4122 [65792/118836 (55%)] Loss: 12204.753906\n",
      "Train Epoch: 4122 [98560/118836 (83%)] Loss: 12333.005859\n",
      "    epoch          : 4122\n",
      "    loss           : 12220.150602253929\n",
      "    val_loss       : 12218.826425075837\n",
      "    val_log_likelihood: -12141.306737877378\n",
      "    val_log_marginal: -12149.192358249773\n",
      "Train Epoch: 4123 [256/118836 (0%)] Loss: 12174.011719\n",
      "Train Epoch: 4123 [33024/118836 (28%)] Loss: 12282.585938\n",
      "Train Epoch: 4123 [65792/118836 (55%)] Loss: 12200.115234\n",
      "Train Epoch: 4123 [98560/118836 (83%)] Loss: 12228.470703\n",
      "    epoch          : 4123\n",
      "    loss           : 12217.614946979684\n",
      "    val_loss       : 12212.623897554988\n",
      "    val_log_likelihood: -12139.717143558726\n",
      "    val_log_marginal: -12147.613107551284\n",
      "Train Epoch: 4124 [256/118836 (0%)] Loss: 12420.885742\n",
      "Train Epoch: 4124 [33024/118836 (28%)] Loss: 12199.134766\n",
      "Train Epoch: 4124 [65792/118836 (55%)] Loss: 12237.481445\n",
      "Train Epoch: 4124 [98560/118836 (83%)] Loss: 12151.945312\n",
      "    epoch          : 4124\n",
      "    loss           : 12218.08018587805\n",
      "    val_loss       : 12219.801998367026\n",
      "    val_log_likelihood: -12140.421020406846\n",
      "    val_log_marginal: -12148.307152436384\n",
      "Train Epoch: 4125 [256/118836 (0%)] Loss: 12243.036133\n",
      "Train Epoch: 4125 [33024/118836 (28%)] Loss: 12238.828125\n",
      "Train Epoch: 4125 [65792/118836 (55%)] Loss: 12215.774414\n",
      "Train Epoch: 4125 [98560/118836 (83%)] Loss: 12157.392578\n",
      "    epoch          : 4125\n",
      "    loss           : 12221.761120211693\n",
      "    val_loss       : 12217.646639920664\n",
      "    val_log_likelihood: -12142.06210388234\n",
      "    val_log_marginal: -12149.949711025616\n",
      "Train Epoch: 4126 [256/118836 (0%)] Loss: 12324.058594\n",
      "Train Epoch: 4126 [33024/118836 (28%)] Loss: 12162.610352\n",
      "Train Epoch: 4126 [65792/118836 (55%)] Loss: 12200.460938\n",
      "Train Epoch: 4126 [98560/118836 (83%)] Loss: 12312.675781\n",
      "    epoch          : 4126\n",
      "    loss           : 12221.152176385443\n",
      "    val_loss       : 12221.664879465345\n",
      "    val_log_likelihood: -12137.688375109852\n",
      "    val_log_marginal: -12145.577061883661\n",
      "Train Epoch: 4127 [256/118836 (0%)] Loss: 12278.910156\n",
      "Train Epoch: 4127 [33024/118836 (28%)] Loss: 12299.086914\n",
      "Train Epoch: 4127 [65792/118836 (55%)] Loss: 12197.058594\n",
      "Train Epoch: 4127 [98560/118836 (83%)] Loss: 12221.384766\n",
      "    epoch          : 4127\n",
      "    loss           : 12216.502557640613\n",
      "    val_loss       : 12219.620869117653\n",
      "    val_log_likelihood: -12138.766094783912\n",
      "    val_log_marginal: -12146.65648015135\n",
      "Train Epoch: 4128 [256/118836 (0%)] Loss: 12333.985352\n",
      "Train Epoch: 4128 [33024/118836 (28%)] Loss: 12327.697266\n",
      "Train Epoch: 4128 [65792/118836 (55%)] Loss: 12184.113281\n",
      "Train Epoch: 4128 [98560/118836 (83%)] Loss: 12204.687500\n",
      "    epoch          : 4128\n",
      "    loss           : 12219.39727790271\n",
      "    val_loss       : 12215.847058472582\n",
      "    val_log_likelihood: -12139.373248164806\n",
      "    val_log_marginal: -12147.264653234603\n",
      "Train Epoch: 4129 [256/118836 (0%)] Loss: 12188.245117\n",
      "Train Epoch: 4129 [33024/118836 (28%)] Loss: 12139.830078\n",
      "Train Epoch: 4129 [65792/118836 (55%)] Loss: 12262.830078\n",
      "Train Epoch: 4129 [98560/118836 (83%)] Loss: 12217.675781\n",
      "    epoch          : 4129\n",
      "    loss           : 12217.503102544717\n",
      "    val_loss       : 12216.693586312811\n",
      "    val_log_likelihood: -12138.981858554333\n",
      "    val_log_marginal: -12146.873962546635\n",
      "Train Epoch: 4130 [256/118836 (0%)] Loss: 12290.213867\n",
      "Train Epoch: 4130 [33024/118836 (28%)] Loss: 12266.586914\n",
      "Train Epoch: 4130 [65792/118836 (55%)] Loss: 12200.084961\n",
      "Train Epoch: 4130 [98560/118836 (83%)] Loss: 12262.378906\n",
      "    epoch          : 4130\n",
      "    loss           : 12219.951451031327\n",
      "    val_loss       : 12217.50045076605\n",
      "    val_log_likelihood: -12138.682689399813\n",
      "    val_log_marginal: -12146.573087877681\n",
      "Train Epoch: 4131 [256/118836 (0%)] Loss: 12156.200195\n",
      "Train Epoch: 4131 [33024/118836 (28%)] Loss: 12171.811523\n",
      "Train Epoch: 4131 [65792/118836 (55%)] Loss: 12263.264648\n",
      "Train Epoch: 4131 [98560/118836 (83%)] Loss: 12220.030273\n",
      "    epoch          : 4131\n",
      "    loss           : 12216.22388741341\n",
      "    val_loss       : 12213.545744365332\n",
      "    val_log_likelihood: -12139.672939283499\n",
      "    val_log_marginal: -12147.572241747224\n",
      "Train Epoch: 4132 [256/118836 (0%)] Loss: 12172.085938\n",
      "Train Epoch: 4132 [33024/118836 (28%)] Loss: 12181.288086\n",
      "Train Epoch: 4132 [65792/118836 (55%)] Loss: 12220.257812\n",
      "Train Epoch: 4132 [98560/118836 (83%)] Loss: 12232.232422\n",
      "    epoch          : 4132\n",
      "    loss           : 12214.758846089226\n",
      "    val_loss       : 12217.725734082756\n",
      "    val_log_likelihood: -12138.40035007625\n",
      "    val_log_marginal: -12146.30084060983\n",
      "Train Epoch: 4133 [256/118836 (0%)] Loss: 12342.414062\n",
      "Train Epoch: 4133 [33024/118836 (28%)] Loss: 12136.179688\n",
      "Train Epoch: 4133 [65792/118836 (55%)] Loss: 12192.587891\n",
      "Train Epoch: 4133 [98560/118836 (83%)] Loss: 12202.528320\n",
      "    epoch          : 4133\n",
      "    loss           : 12219.972400356699\n",
      "    val_loss       : 12219.802128850852\n",
      "    val_log_likelihood: -12139.413262994985\n",
      "    val_log_marginal: -12147.29997588309\n",
      "Train Epoch: 4134 [256/118836 (0%)] Loss: 12253.415039\n",
      "Train Epoch: 4134 [33024/118836 (28%)] Loss: 12253.697266\n",
      "Train Epoch: 4134 [65792/118836 (55%)] Loss: 12226.941406\n",
      "Train Epoch: 4134 [98560/118836 (83%)] Loss: 12217.584961\n",
      "    epoch          : 4134\n",
      "    loss           : 12216.692801062343\n",
      "    val_loss       : 12221.852018272864\n",
      "    val_log_likelihood: -12141.075083359181\n",
      "    val_log_marginal: -12148.965737179911\n",
      "Train Epoch: 4135 [256/118836 (0%)] Loss: 12205.144531\n",
      "Train Epoch: 4135 [33024/118836 (28%)] Loss: 12183.304688\n",
      "Train Epoch: 4135 [65792/118836 (55%)] Loss: 12287.551758\n",
      "Train Epoch: 4135 [98560/118836 (83%)] Loss: 12155.295898\n",
      "    epoch          : 4135\n",
      "    loss           : 12214.333416369418\n",
      "    val_loss       : 12215.810831191624\n",
      "    val_log_likelihood: -12141.546322018456\n",
      "    val_log_marginal: -12149.441140439158\n",
      "Train Epoch: 4136 [256/118836 (0%)] Loss: 12178.539062\n",
      "Train Epoch: 4136 [33024/118836 (28%)] Loss: 12209.369141\n",
      "Train Epoch: 4136 [65792/118836 (55%)] Loss: 12335.507812\n",
      "Train Epoch: 4136 [98560/118836 (83%)] Loss: 12311.378906\n",
      "    epoch          : 4136\n",
      "    loss           : 12220.011370450786\n",
      "    val_loss       : 12219.513232467662\n",
      "    val_log_likelihood: -12140.942414152967\n",
      "    val_log_marginal: -12148.837460181274\n",
      "Train Epoch: 4137 [256/118836 (0%)] Loss: 12220.167969\n",
      "Train Epoch: 4137 [33024/118836 (28%)] Loss: 12298.159180\n",
      "Train Epoch: 4137 [65792/118836 (55%)] Loss: 12252.354492\n",
      "Train Epoch: 4137 [98560/118836 (83%)] Loss: 12191.524414\n",
      "    epoch          : 4137\n",
      "    loss           : 12213.826198045905\n",
      "    val_loss       : 12217.241586093003\n",
      "    val_log_likelihood: -12139.37140214666\n",
      "    val_log_marginal: -12147.263367556103\n",
      "Train Epoch: 4138 [256/118836 (0%)] Loss: 12272.027344\n",
      "Train Epoch: 4138 [33024/118836 (28%)] Loss: 12253.963867\n",
      "Train Epoch: 4138 [65792/118836 (55%)] Loss: 12223.391602\n",
      "Train Epoch: 4138 [98560/118836 (83%)] Loss: 12268.201172\n",
      "    epoch          : 4138\n",
      "    loss           : 12221.247716346155\n",
      "    val_loss       : 12215.614394866057\n",
      "    val_log_likelihood: -12137.952806587313\n",
      "    val_log_marginal: -12145.835832804341\n",
      "Train Epoch: 4139 [256/118836 (0%)] Loss: 12203.667969\n",
      "Train Epoch: 4139 [33024/118836 (28%)] Loss: 12288.355469\n",
      "Train Epoch: 4139 [65792/118836 (55%)] Loss: 12243.159180\n",
      "Train Epoch: 4139 [98560/118836 (83%)] Loss: 12255.722656\n",
      "    epoch          : 4139\n",
      "    loss           : 12220.359850922767\n",
      "    val_loss       : 12216.388276399171\n",
      "    val_log_likelihood: -12139.28949625853\n",
      "    val_log_marginal: -12147.174409622437\n",
      "Train Epoch: 4140 [256/118836 (0%)] Loss: 12398.482422\n",
      "Train Epoch: 4140 [33024/118836 (28%)] Loss: 12283.691406\n",
      "Train Epoch: 4140 [65792/118836 (55%)] Loss: 12169.205078\n",
      "Train Epoch: 4140 [98560/118836 (83%)] Loss: 12168.980469\n",
      "    epoch          : 4140\n",
      "    loss           : 12217.31949360913\n",
      "    val_loss       : 12216.37156790031\n",
      "    val_log_likelihood: -12141.035018448873\n",
      "    val_log_marginal: -12148.926158525686\n",
      "Train Epoch: 4141 [256/118836 (0%)] Loss: 12142.136719\n",
      "Train Epoch: 4141 [33024/118836 (28%)] Loss: 12196.650391\n",
      "Train Epoch: 4141 [65792/118836 (55%)] Loss: 12250.550781\n",
      "Train Epoch: 4141 [98560/118836 (83%)] Loss: 12303.749023\n",
      "    epoch          : 4141\n",
      "    loss           : 12219.407842548077\n",
      "    val_loss       : 12216.854104577444\n",
      "    val_log_likelihood: -12141.539247311828\n",
      "    val_log_marginal: -12149.427944348407\n",
      "Train Epoch: 4142 [256/118836 (0%)] Loss: 12186.024414\n",
      "Train Epoch: 4142 [33024/118836 (28%)] Loss: 12317.372070\n",
      "Train Epoch: 4142 [65792/118836 (55%)] Loss: 12260.134766\n",
      "Train Epoch: 4142 [98560/118836 (83%)] Loss: 12203.906250\n",
      "    epoch          : 4142\n",
      "    loss           : 12215.95096186156\n",
      "    val_loss       : 12215.214653461193\n",
      "    val_log_likelihood: -12140.59042726427\n",
      "    val_log_marginal: -12148.480332703813\n",
      "Train Epoch: 4143 [256/118836 (0%)] Loss: 12194.385742\n",
      "Train Epoch: 4143 [33024/118836 (28%)] Loss: 12175.963867\n",
      "Train Epoch: 4143 [65792/118836 (55%)] Loss: 12187.400391\n",
      "Train Epoch: 4143 [98560/118836 (83%)] Loss: 12249.042969\n",
      "    epoch          : 4143\n",
      "    loss           : 12213.763413396919\n",
      "    val_loss       : 12222.552094505168\n",
      "    val_log_likelihood: -12140.417580063586\n",
      "    val_log_marginal: -12148.307438408545\n",
      "Train Epoch: 4144 [256/118836 (0%)] Loss: 12166.996094\n",
      "Train Epoch: 4144 [33024/118836 (28%)] Loss: 12169.468750\n",
      "Train Epoch: 4144 [65792/118836 (55%)] Loss: 12186.322266\n",
      "Train Epoch: 4144 [98560/118836 (83%)] Loss: 12256.833008\n",
      "    epoch          : 4144\n",
      "    loss           : 12216.750218090881\n",
      "    val_loss       : 12219.971802073023\n",
      "    val_log_likelihood: -12138.999078364091\n",
      "    val_log_marginal: -12146.890352698018\n",
      "Train Epoch: 4145 [256/118836 (0%)] Loss: 12274.650391\n",
      "Train Epoch: 4145 [33024/118836 (28%)] Loss: 12176.585938\n",
      "Train Epoch: 4145 [65792/118836 (55%)] Loss: 12329.242188\n",
      "Train Epoch: 4145 [98560/118836 (83%)] Loss: 12202.575195\n",
      "    epoch          : 4145\n",
      "    loss           : 12217.597679836124\n",
      "    val_loss       : 12215.97940578032\n",
      "    val_log_likelihood: -12140.37436430547\n",
      "    val_log_marginal: -12148.261614632549\n",
      "Train Epoch: 4146 [256/118836 (0%)] Loss: 12240.142578\n",
      "Train Epoch: 4146 [33024/118836 (28%)] Loss: 12283.980469\n",
      "Train Epoch: 4146 [65792/118836 (55%)] Loss: 12201.679688\n",
      "Train Epoch: 4146 [98560/118836 (83%)] Loss: 12244.779297\n",
      "    epoch          : 4146\n",
      "    loss           : 12219.150690782672\n",
      "    val_loss       : 12217.59040417466\n",
      "    val_log_likelihood: -12139.54658663539\n",
      "    val_log_marginal: -12147.434036424815\n",
      "Train Epoch: 4147 [256/118836 (0%)] Loss: 12272.468750\n",
      "Train Epoch: 4147 [33024/118836 (28%)] Loss: 12193.602539\n",
      "Train Epoch: 4147 [65792/118836 (55%)] Loss: 12259.449219\n",
      "Train Epoch: 4147 [98560/118836 (83%)] Loss: 12190.727539\n",
      "    epoch          : 4147\n",
      "    loss           : 12217.207443845637\n",
      "    val_loss       : 12218.253000593884\n",
      "    val_log_likelihood: -12137.974580134667\n",
      "    val_log_marginal: -12145.866507129891\n",
      "Train Epoch: 4148 [256/118836 (0%)] Loss: 12257.394531\n",
      "Train Epoch: 4148 [33024/118836 (28%)] Loss: 12240.726562\n",
      "Train Epoch: 4148 [65792/118836 (55%)] Loss: 12299.380859\n",
      "Train Epoch: 4148 [98560/118836 (83%)] Loss: 12323.207031\n",
      "    epoch          : 4148\n",
      "    loss           : 12219.59402899478\n",
      "    val_loss       : 12217.844344148116\n",
      "    val_log_likelihood: -12141.408347226532\n",
      "    val_log_marginal: -12149.30118644019\n",
      "Train Epoch: 4149 [256/118836 (0%)] Loss: 12241.252930\n",
      "Train Epoch: 4149 [33024/118836 (28%)] Loss: 12159.607422\n",
      "Train Epoch: 4149 [65792/118836 (55%)] Loss: 12224.172852\n",
      "Train Epoch: 4149 [98560/118836 (83%)] Loss: 12153.042969\n",
      "    epoch          : 4149\n",
      "    loss           : 12221.380765838245\n",
      "    val_loss       : 12218.757249123688\n",
      "    val_log_likelihood: -12138.691105284584\n",
      "    val_log_marginal: -12146.576655203164\n",
      "Train Epoch: 4150 [256/118836 (0%)] Loss: 12189.640625\n",
      "Train Epoch: 4150 [33024/118836 (28%)] Loss: 12268.710938\n",
      "Train Epoch: 4150 [65792/118836 (55%)] Loss: 12161.275391\n",
      "Train Epoch: 4150 [98560/118836 (83%)] Loss: 12208.742188\n",
      "    epoch          : 4150\n",
      "    loss           : 12215.779815769749\n",
      "    val_loss       : 12211.525370410725\n",
      "    val_log_likelihood: -12139.045036897745\n",
      "    val_log_marginal: -12146.927163331433\n",
      "Train Epoch: 4151 [256/118836 (0%)] Loss: 12378.482422\n",
      "Train Epoch: 4151 [33024/118836 (28%)] Loss: 12181.125977\n",
      "Train Epoch: 4151 [65792/118836 (55%)] Loss: 12344.787109\n",
      "Train Epoch: 4151 [98560/118836 (83%)] Loss: 12218.285156\n",
      "    epoch          : 4151\n",
      "    loss           : 12218.321572903742\n",
      "    val_loss       : 12219.474126298694\n",
      "    val_log_likelihood: -12141.037081103961\n",
      "    val_log_marginal: -12148.92592084645\n",
      "Train Epoch: 4152 [256/118836 (0%)] Loss: 12259.822266\n",
      "Train Epoch: 4152 [33024/118836 (28%)] Loss: 12210.824219\n",
      "Train Epoch: 4152 [65792/118836 (55%)] Loss: 12252.011719\n",
      "Train Epoch: 4152 [98560/118836 (83%)] Loss: 12285.320312\n",
      "    epoch          : 4152\n",
      "    loss           : 12216.368151623243\n",
      "    val_loss       : 12213.726089827953\n",
      "    val_log_likelihood: -12140.330470850135\n",
      "    val_log_marginal: -12148.223788754813\n",
      "Train Epoch: 4153 [256/118836 (0%)] Loss: 12133.192383\n",
      "Train Epoch: 4153 [33024/118836 (28%)] Loss: 12168.205078\n",
      "Train Epoch: 4153 [65792/118836 (55%)] Loss: 12179.955078\n",
      "Train Epoch: 4153 [98560/118836 (83%)] Loss: 12185.551758\n",
      "    epoch          : 4153\n",
      "    loss           : 12214.191113200475\n",
      "    val_loss       : 12215.373408029205\n",
      "    val_log_likelihood: -12140.292874567049\n",
      "    val_log_marginal: -12148.181107662242\n",
      "Train Epoch: 4154 [256/118836 (0%)] Loss: 12193.077148\n",
      "Train Epoch: 4154 [33024/118836 (28%)] Loss: 12287.675781\n",
      "Train Epoch: 4154 [65792/118836 (55%)] Loss: 12203.673828\n",
      "Train Epoch: 4154 [98560/118836 (83%)] Loss: 12256.031250\n",
      "    epoch          : 4154\n",
      "    loss           : 12217.864516129031\n",
      "    val_loss       : 12217.339962919028\n",
      "    val_log_likelihood: -12139.686177076872\n",
      "    val_log_marginal: -12147.577472714609\n",
      "Train Epoch: 4155 [256/118836 (0%)] Loss: 12331.457031\n",
      "Train Epoch: 4155 [33024/118836 (28%)] Loss: 12262.884766\n",
      "Train Epoch: 4155 [65792/118836 (55%)] Loss: 12221.088867\n",
      "Train Epoch: 4155 [98560/118836 (83%)] Loss: 12217.398438\n",
      "    epoch          : 4155\n",
      "    loss           : 12217.678798496949\n",
      "    val_loss       : 12216.980528212007\n",
      "    val_log_likelihood: -12139.092465202388\n",
      "    val_log_marginal: -12146.982719279451\n",
      "Train Epoch: 4156 [256/118836 (0%)] Loss: 12180.100586\n",
      "Train Epoch: 4156 [33024/118836 (28%)] Loss: 12259.222656\n",
      "Train Epoch: 4156 [65792/118836 (55%)] Loss: 12231.907227\n",
      "Train Epoch: 4156 [98560/118836 (83%)] Loss: 12151.973633\n",
      "    epoch          : 4156\n",
      "    loss           : 12216.389023566739\n",
      "    val_loss       : 12216.75812699556\n",
      "    val_log_likelihood: -12141.480805740797\n",
      "    val_log_marginal: -12149.370085859839\n",
      "Train Epoch: 4157 [256/118836 (0%)] Loss: 12382.400391\n",
      "Train Epoch: 4157 [33024/118836 (28%)] Loss: 12199.050781\n",
      "Train Epoch: 4157 [65792/118836 (55%)] Loss: 12296.783203\n",
      "Train Epoch: 4157 [98560/118836 (83%)] Loss: 12199.236328\n",
      "    epoch          : 4157\n",
      "    loss           : 12220.622197612956\n",
      "    val_loss       : 12213.325404850611\n",
      "    val_log_likelihood: -12141.544137232475\n",
      "    val_log_marginal: -12149.428626408644\n",
      "Train Epoch: 4158 [256/118836 (0%)] Loss: 12414.353516\n",
      "Train Epoch: 4158 [33024/118836 (28%)] Loss: 12160.154297\n",
      "Train Epoch: 4158 [65792/118836 (55%)] Loss: 12212.027344\n",
      "Train Epoch: 4158 [98560/118836 (83%)] Loss: 12352.381836\n",
      "    epoch          : 4158\n",
      "    loss           : 12221.05982782129\n",
      "    val_loss       : 12216.7339091773\n",
      "    val_log_likelihood: -12138.947347045596\n",
      "    val_log_marginal: -12146.835048752793\n",
      "Train Epoch: 4159 [256/118836 (0%)] Loss: 12201.867188\n",
      "Train Epoch: 4159 [33024/118836 (28%)] Loss: 12271.407227\n",
      "Train Epoch: 4159 [65792/118836 (55%)] Loss: 12163.531250\n",
      "Train Epoch: 4159 [98560/118836 (83%)] Loss: 12179.497070\n",
      "    epoch          : 4159\n",
      "    loss           : 12216.132001363472\n",
      "    val_loss       : 12214.026418788471\n",
      "    val_log_likelihood: -12138.02609917804\n",
      "    val_log_marginal: -12145.91323144987\n",
      "Train Epoch: 4160 [256/118836 (0%)] Loss: 12286.216797\n",
      "Train Epoch: 4160 [33024/118836 (28%)] Loss: 12227.953125\n",
      "Train Epoch: 4160 [65792/118836 (55%)] Loss: 12224.886719\n",
      "Train Epoch: 4160 [98560/118836 (83%)] Loss: 12197.928711\n",
      "    epoch          : 4160\n",
      "    loss           : 12219.051808054177\n",
      "    val_loss       : 12217.930697134447\n",
      "    val_log_likelihood: -12138.52395203293\n",
      "    val_log_marginal: -12146.413288121787\n",
      "Train Epoch: 4161 [256/118836 (0%)] Loss: 12158.966797\n",
      "Train Epoch: 4161 [33024/118836 (28%)] Loss: 12242.804688\n",
      "Train Epoch: 4161 [65792/118836 (55%)] Loss: 12223.045898\n",
      "Train Epoch: 4161 [98560/118836 (83%)] Loss: 12188.098633\n",
      "    epoch          : 4161\n",
      "    loss           : 12219.361965919665\n",
      "    val_loss       : 12215.064261299784\n",
      "    val_log_likelihood: -12139.418564703526\n",
      "    val_log_marginal: -12147.304877196158\n",
      "Train Epoch: 4162 [256/118836 (0%)] Loss: 12262.562500\n",
      "Train Epoch: 4162 [33024/118836 (28%)] Loss: 12233.910156\n",
      "Train Epoch: 4162 [65792/118836 (55%)] Loss: 12176.633789\n",
      "Train Epoch: 4162 [98560/118836 (83%)] Loss: 12245.677734\n",
      "    epoch          : 4162\n",
      "    loss           : 12219.881069065344\n",
      "    val_loss       : 12220.288775912459\n",
      "    val_log_likelihood: -12139.512226820978\n",
      "    val_log_marginal: -12147.402339274162\n",
      "Train Epoch: 4163 [256/118836 (0%)] Loss: 12290.869141\n",
      "Train Epoch: 4163 [33024/118836 (28%)] Loss: 12171.933594\n",
      "Train Epoch: 4163 [65792/118836 (55%)] Loss: 12213.457031\n",
      "Train Epoch: 4163 [98560/118836 (83%)] Loss: 12195.250977\n",
      "    epoch          : 4163\n",
      "    loss           : 12219.666235008272\n",
      "    val_loss       : 12218.695256544499\n",
      "    val_log_likelihood: -12139.958980336281\n",
      "    val_log_marginal: -12147.852506603227\n",
      "Train Epoch: 4164 [256/118836 (0%)] Loss: 12200.074219\n",
      "Train Epoch: 4164 [33024/118836 (28%)] Loss: 12284.610352\n",
      "Train Epoch: 4164 [65792/118836 (55%)] Loss: 12220.376953\n",
      "Train Epoch: 4164 [98560/118836 (83%)] Loss: 12149.085938\n",
      "    epoch          : 4164\n",
      "    loss           : 12219.06422937991\n",
      "    val_loss       : 12221.328724853178\n",
      "    val_log_likelihood: -12139.61388334238\n",
      "    val_log_marginal: -12147.501235558078\n",
      "Train Epoch: 4165 [256/118836 (0%)] Loss: 12178.407227\n",
      "Train Epoch: 4165 [33024/118836 (28%)] Loss: 12357.086914\n",
      "Train Epoch: 4165 [65792/118836 (55%)] Loss: 12196.846680\n",
      "Train Epoch: 4165 [98560/118836 (83%)] Loss: 12170.171875\n",
      "    epoch          : 4165\n",
      "    loss           : 12215.244961454457\n",
      "    val_loss       : 12216.415410214324\n",
      "    val_log_likelihood: -12140.118140476376\n",
      "    val_log_marginal: -12148.01121648738\n",
      "Train Epoch: 4166 [256/118836 (0%)] Loss: 12168.070312\n",
      "Train Epoch: 4166 [33024/118836 (28%)] Loss: 12244.411133\n",
      "Train Epoch: 4166 [65792/118836 (55%)] Loss: 12241.464844\n",
      "Train Epoch: 4166 [98560/118836 (83%)] Loss: 12293.852539\n",
      "    epoch          : 4166\n",
      "    loss           : 12217.02966456007\n",
      "    val_loss       : 12220.760030036745\n",
      "    val_log_likelihood: -12141.663345546423\n",
      "    val_log_marginal: -12149.551129942163\n",
      "Train Epoch: 4167 [256/118836 (0%)] Loss: 12220.215820\n",
      "Train Epoch: 4167 [33024/118836 (28%)] Loss: 12159.378906\n",
      "Train Epoch: 4167 [65792/118836 (55%)] Loss: 12282.170898\n",
      "Train Epoch: 4167 [98560/118836 (83%)] Loss: 12266.660156\n",
      "    epoch          : 4167\n",
      "    loss           : 12221.726190291563\n",
      "    val_loss       : 12215.062173057564\n",
      "    val_log_likelihood: -12140.488852971206\n",
      "    val_log_marginal: -12148.380419755495\n",
      "Train Epoch: 4168 [256/118836 (0%)] Loss: 12281.037109\n",
      "Train Epoch: 4168 [33024/118836 (28%)] Loss: 12302.163086\n",
      "Train Epoch: 4168 [65792/118836 (55%)] Loss: 12316.931641\n",
      "Train Epoch: 4168 [98560/118836 (83%)] Loss: 12237.162109\n",
      "    epoch          : 4168\n",
      "    loss           : 12217.141423212624\n",
      "    val_loss       : 12220.561759723045\n",
      "    val_log_likelihood: -12141.24900970585\n",
      "    val_log_marginal: -12149.143264751305\n",
      "Train Epoch: 4169 [256/118836 (0%)] Loss: 12215.175781\n",
      "Train Epoch: 4169 [33024/118836 (28%)] Loss: 12199.008789\n",
      "Train Epoch: 4169 [65792/118836 (55%)] Loss: 12209.957031\n",
      "Train Epoch: 4169 [98560/118836 (83%)] Loss: 12259.633789\n",
      "    epoch          : 4169\n",
      "    loss           : 12226.377239066376\n",
      "    val_loss       : 12214.141538231808\n",
      "    val_log_likelihood: -12139.217440970067\n",
      "    val_log_marginal: -12147.107016376514\n",
      "Train Epoch: 4170 [256/118836 (0%)] Loss: 12178.486328\n",
      "Train Epoch: 4170 [33024/118836 (28%)] Loss: 12286.508789\n",
      "Train Epoch: 4170 [65792/118836 (55%)] Loss: 12147.115234\n",
      "Train Epoch: 4170 [98560/118836 (83%)] Loss: 12267.467773\n",
      "    epoch          : 4170\n",
      "    loss           : 12216.637624554125\n",
      "    val_loss       : 12211.91159552857\n",
      "    val_log_likelihood: -12139.887869300559\n",
      "    val_log_marginal: -12147.78027185546\n",
      "Train Epoch: 4171 [256/118836 (0%)] Loss: 12192.250000\n",
      "Train Epoch: 4171 [33024/118836 (28%)] Loss: 12162.132812\n",
      "Train Epoch: 4171 [65792/118836 (55%)] Loss: 12330.030273\n",
      "Train Epoch: 4171 [98560/118836 (83%)] Loss: 12358.134766\n",
      "    epoch          : 4171\n",
      "    loss           : 12218.153519502172\n",
      "    val_loss       : 12218.98658751551\n",
      "    val_log_likelihood: -12139.525510009564\n",
      "    val_log_marginal: -12147.416114334843\n",
      "Train Epoch: 4172 [256/118836 (0%)] Loss: 12266.118164\n",
      "Train Epoch: 4172 [33024/118836 (28%)] Loss: 12208.968750\n",
      "Train Epoch: 4172 [65792/118836 (55%)] Loss: 12306.055664\n",
      "Train Epoch: 4172 [98560/118836 (83%)] Loss: 12345.107422\n",
      "    epoch          : 4172\n",
      "    loss           : 12216.36514423077\n",
      "    val_loss       : 12218.405705907759\n",
      "    val_log_likelihood: -12139.271750445874\n",
      "    val_log_marginal: -12147.16538806297\n",
      "Train Epoch: 4173 [256/118836 (0%)] Loss: 12236.323242\n",
      "Train Epoch: 4173 [33024/118836 (28%)] Loss: 12214.349609\n",
      "Train Epoch: 4173 [65792/118836 (55%)] Loss: 12182.908203\n",
      "Train Epoch: 4173 [98560/118836 (83%)] Loss: 12212.970703\n",
      "    epoch          : 4173\n",
      "    loss           : 12220.874742006565\n",
      "    val_loss       : 12219.359671035294\n",
      "    val_log_likelihood: -12138.997217483458\n",
      "    val_log_marginal: -12146.890387083175\n",
      "Train Epoch: 4174 [256/118836 (0%)] Loss: 12203.701172\n",
      "Train Epoch: 4174 [33024/118836 (28%)] Loss: 12228.745117\n",
      "Train Epoch: 4174 [65792/118836 (55%)] Loss: 12273.242188\n",
      "Train Epoch: 4174 [98560/118836 (83%)] Loss: 12230.412109\n",
      "    epoch          : 4174\n",
      "    loss           : 12215.668330296216\n",
      "    val_loss       : 12212.36404044528\n",
      "    val_log_likelihood: -12139.093528516594\n",
      "    val_log_marginal: -12146.990483751744\n",
      "Train Epoch: 4175 [256/118836 (0%)] Loss: 12264.722656\n",
      "Train Epoch: 4175 [33024/118836 (28%)] Loss: 12251.767578\n",
      "Train Epoch: 4175 [65792/118836 (55%)] Loss: 12172.035156\n",
      "Train Epoch: 4175 [98560/118836 (83%)] Loss: 12263.285156\n",
      "    epoch          : 4175\n",
      "    loss           : 12218.778352460711\n",
      "    val_loss       : 12221.986239440139\n",
      "    val_log_likelihood: -12139.917558416048\n",
      "    val_log_marginal: -12147.810649714846\n",
      "Train Epoch: 4176 [256/118836 (0%)] Loss: 12241.283203\n",
      "Train Epoch: 4176 [33024/118836 (28%)] Loss: 12213.971680\n",
      "Train Epoch: 4176 [65792/118836 (55%)] Loss: 12217.316406\n",
      "Train Epoch: 4176 [98560/118836 (83%)] Loss: 12333.675781\n",
      "    epoch          : 4176\n",
      "    loss           : 12217.946113458953\n",
      "    val_loss       : 12216.781128971443\n",
      "    val_log_likelihood: -12141.844711215363\n",
      "    val_log_marginal: -12149.734633230824\n",
      "Train Epoch: 4177 [256/118836 (0%)] Loss: 12186.563477\n",
      "Train Epoch: 4177 [33024/118836 (28%)] Loss: 12272.005859\n",
      "Train Epoch: 4177 [65792/118836 (55%)] Loss: 12402.773438\n",
      "Train Epoch: 4177 [98560/118836 (83%)] Loss: 12275.669922\n",
      "    epoch          : 4177\n",
      "    loss           : 12215.208795201353\n",
      "    val_loss       : 12216.66346179451\n",
      "    val_log_likelihood: -12141.083766607217\n",
      "    val_log_marginal: -12148.978985481677\n",
      "Train Epoch: 4178 [256/118836 (0%)] Loss: 12290.198242\n",
      "Train Epoch: 4178 [33024/118836 (28%)] Loss: 12277.199219\n",
      "Train Epoch: 4178 [65792/118836 (55%)] Loss: 12262.588867\n",
      "Train Epoch: 4178 [98560/118836 (83%)] Loss: 12167.174805\n",
      "    epoch          : 4178\n",
      "    loss           : 12212.98024258168\n",
      "    val_loss       : 12212.759277363468\n",
      "    val_log_likelihood: -12140.20043117375\n",
      "    val_log_marginal: -12148.08518455902\n",
      "Train Epoch: 4179 [256/118836 (0%)] Loss: 12243.869141\n",
      "Train Epoch: 4179 [33024/118836 (28%)] Loss: 12304.831055\n",
      "Train Epoch: 4179 [65792/118836 (55%)] Loss: 12307.580078\n",
      "Train Epoch: 4179 [98560/118836 (83%)] Loss: 12221.173828\n",
      "    epoch          : 4179\n",
      "    loss           : 12217.705397991625\n",
      "    val_loss       : 12219.435597910177\n",
      "    val_log_likelihood: -12138.723935231856\n",
      "    val_log_marginal: -12146.617597665792\n",
      "Train Epoch: 4180 [256/118836 (0%)] Loss: 12201.904297\n",
      "Train Epoch: 4180 [33024/118836 (28%)] Loss: 12196.511719\n",
      "Train Epoch: 4180 [65792/118836 (55%)] Loss: 12311.176758\n",
      "Train Epoch: 4180 [98560/118836 (83%)] Loss: 12220.188477\n",
      "    epoch          : 4180\n",
      "    loss           : 12216.851468478599\n",
      "    val_loss       : 12218.602735112368\n",
      "    val_log_likelihood: -12139.15162857346\n",
      "    val_log_marginal: -12147.042357763848\n",
      "Train Epoch: 4181 [256/118836 (0%)] Loss: 12236.832031\n",
      "Train Epoch: 4181 [33024/118836 (28%)] Loss: 12245.772461\n",
      "Train Epoch: 4181 [65792/118836 (55%)] Loss: 12298.836914\n",
      "Train Epoch: 4181 [98560/118836 (83%)] Loss: 12238.379883\n",
      "    epoch          : 4181\n",
      "    loss           : 12218.518067456318\n",
      "    val_loss       : 12218.21821499401\n",
      "    val_log_likelihood: -12139.981230291047\n",
      "    val_log_marginal: -12147.865671417332\n",
      "Train Epoch: 4182 [256/118836 (0%)] Loss: 12276.262695\n",
      "Train Epoch: 4182 [33024/118836 (28%)] Loss: 12180.772461\n",
      "Train Epoch: 4182 [65792/118836 (55%)] Loss: 12260.076172\n",
      "Train Epoch: 4182 [98560/118836 (83%)] Loss: 12242.589844\n",
      "    epoch          : 4182\n",
      "    loss           : 12216.745368234337\n",
      "    val_loss       : 12218.000141698014\n",
      "    val_log_likelihood: -12138.244276487541\n",
      "    val_log_marginal: -12146.132871511678\n",
      "Train Epoch: 4183 [256/118836 (0%)] Loss: 12279.860352\n",
      "Train Epoch: 4183 [33024/118836 (28%)] Loss: 12244.166016\n",
      "Train Epoch: 4183 [65792/118836 (55%)] Loss: 12209.051758\n",
      "Train Epoch: 4183 [98560/118836 (83%)] Loss: 12209.453125\n",
      "    epoch          : 4183\n",
      "    loss           : 12216.339442785877\n",
      "    val_loss       : 12218.525761225397\n",
      "    val_log_likelihood: -12140.057464039237\n",
      "    val_log_marginal: -12147.94612087517\n",
      "Train Epoch: 4184 [256/118836 (0%)] Loss: 12207.405273\n",
      "Train Epoch: 4184 [33024/118836 (28%)] Loss: 12363.701172\n",
      "Train Epoch: 4184 [65792/118836 (55%)] Loss: 12230.257812\n",
      "Train Epoch: 4184 [98560/118836 (83%)] Loss: 12172.786133\n",
      "    epoch          : 4184\n",
      "    loss           : 12215.933277760547\n",
      "    val_loss       : 12215.35890559145\n",
      "    val_log_likelihood: -12138.369835446392\n",
      "    val_log_marginal: -12146.260402762906\n",
      "Train Epoch: 4185 [256/118836 (0%)] Loss: 12153.073242\n",
      "Train Epoch: 4185 [33024/118836 (28%)] Loss: 12230.062500\n",
      "Train Epoch: 4185 [65792/118836 (55%)] Loss: 12267.480469\n",
      "Train Epoch: 4185 [98560/118836 (83%)] Loss: 12192.000000\n",
      "    epoch          : 4185\n",
      "    loss           : 12219.091387833436\n",
      "    val_loss       : 12215.502841074333\n",
      "    val_log_likelihood: -12137.454800099515\n",
      "    val_log_marginal: -12145.345759349535\n",
      "Train Epoch: 4186 [256/118836 (0%)] Loss: 12358.302734\n",
      "Train Epoch: 4186 [33024/118836 (28%)] Loss: 12151.591797\n",
      "Train Epoch: 4186 [65792/118836 (55%)] Loss: 12322.834961\n",
      "Train Epoch: 4186 [98560/118836 (83%)] Loss: 12156.921875\n",
      "    epoch          : 4186\n",
      "    loss           : 12220.200705645162\n",
      "    val_loss       : 12216.996144509829\n",
      "    val_log_likelihood: -12141.087595798439\n",
      "    val_log_marginal: -12148.972986177756\n",
      "Train Epoch: 4187 [256/118836 (0%)] Loss: 12193.137695\n",
      "Train Epoch: 4187 [33024/118836 (28%)] Loss: 12163.785156\n",
      "Train Epoch: 4187 [65792/118836 (55%)] Loss: 12188.087891\n",
      "Train Epoch: 4187 [98560/118836 (83%)] Loss: 12242.388672\n",
      "    epoch          : 4187\n",
      "    loss           : 12218.342554054228\n",
      "    val_loss       : 12219.322939900883\n",
      "    val_log_likelihood: -12138.648628289135\n",
      "    val_log_marginal: -12146.53593274268\n",
      "Train Epoch: 4188 [256/118836 (0%)] Loss: 12248.248047\n",
      "Train Epoch: 4188 [33024/118836 (28%)] Loss: 12237.549805\n",
      "Train Epoch: 4188 [65792/118836 (55%)] Loss: 12233.767578\n",
      "Train Epoch: 4188 [98560/118836 (83%)] Loss: 12267.941406\n",
      "    epoch          : 4188\n",
      "    loss           : 12214.757438999171\n",
      "    val_loss       : 12219.642508648598\n",
      "    val_log_likelihood: -12139.36929005764\n",
      "    val_log_marginal: -12147.258847039062\n",
      "Train Epoch: 4189 [256/118836 (0%)] Loss: 12348.296875\n",
      "Train Epoch: 4189 [33024/118836 (28%)] Loss: 12255.919922\n",
      "Train Epoch: 4189 [65792/118836 (55%)] Loss: 12222.757812\n",
      "Train Epoch: 4189 [98560/118836 (83%)] Loss: 12196.450195\n",
      "    epoch          : 4189\n",
      "    loss           : 12220.147755764061\n",
      "    val_loss       : 12214.727342739912\n",
      "    val_log_likelihood: -12138.999431186672\n",
      "    val_log_marginal: -12146.886682215183\n",
      "Train Epoch: 4190 [256/118836 (0%)] Loss: 12184.370117\n",
      "Train Epoch: 4190 [33024/118836 (28%)] Loss: 12211.013672\n",
      "Train Epoch: 4190 [65792/118836 (55%)] Loss: 12300.805664\n",
      "Train Epoch: 4190 [98560/118836 (83%)] Loss: 12186.135742\n",
      "    epoch          : 4190\n",
      "    loss           : 12220.017951141182\n",
      "    val_loss       : 12215.730668159058\n",
      "    val_log_likelihood: -12142.136341533549\n",
      "    val_log_marginal: -12150.028963081122\n",
      "Train Epoch: 4191 [256/118836 (0%)] Loss: 12158.239258\n",
      "Train Epoch: 4191 [33024/118836 (28%)] Loss: 12197.841797\n",
      "Train Epoch: 4191 [65792/118836 (55%)] Loss: 12213.807617\n",
      "Train Epoch: 4191 [98560/118836 (83%)] Loss: 12242.887695\n",
      "    epoch          : 4191\n",
      "    loss           : 12217.798446223635\n",
      "    val_loss       : 12218.332665557216\n",
      "    val_log_likelihood: -12138.916171034945\n",
      "    val_log_marginal: -12146.805564158254\n",
      "Train Epoch: 4192 [256/118836 (0%)] Loss: 12254.450195\n",
      "Train Epoch: 4192 [33024/118836 (28%)] Loss: 12183.940430\n",
      "Train Epoch: 4192 [65792/118836 (55%)] Loss: 12201.997070\n",
      "Train Epoch: 4192 [98560/118836 (83%)] Loss: 12189.008789\n",
      "    epoch          : 4192\n",
      "    loss           : 12213.032208953682\n",
      "    val_loss       : 12222.807415661855\n",
      "    val_log_likelihood: -12139.740868615592\n",
      "    val_log_marginal: -12147.628162986615\n",
      "Train Epoch: 4193 [256/118836 (0%)] Loss: 12189.154297\n",
      "Train Epoch: 4193 [33024/118836 (28%)] Loss: 12294.220703\n",
      "Train Epoch: 4193 [65792/118836 (55%)] Loss: 12232.119141\n",
      "Train Epoch: 4193 [98560/118836 (83%)] Loss: 12164.858398\n",
      "    epoch          : 4193\n",
      "    loss           : 12217.737144754188\n",
      "    val_loss       : 12221.282079594168\n",
      "    val_log_likelihood: -12140.65898857527\n",
      "    val_log_marginal: -12148.547511040424\n",
      "Train Epoch: 4194 [256/118836 (0%)] Loss: 12226.995117\n",
      "Train Epoch: 4194 [33024/118836 (28%)] Loss: 12202.428711\n",
      "Train Epoch: 4194 [65792/118836 (55%)] Loss: 12184.748047\n",
      "Train Epoch: 4194 [98560/118836 (83%)] Loss: 12231.926758\n",
      "    epoch          : 4194\n",
      "    loss           : 12217.934973699856\n",
      "    val_loss       : 12217.390831237446\n",
      "    val_log_likelihood: -12139.330846612644\n",
      "    val_log_marginal: -12147.217863637425\n",
      "Train Epoch: 4195 [256/118836 (0%)] Loss: 12339.994141\n",
      "Train Epoch: 4195 [33024/118836 (28%)] Loss: 12327.945312\n",
      "Train Epoch: 4195 [65792/118836 (55%)] Loss: 12332.166016\n",
      "Train Epoch: 4195 [98560/118836 (83%)] Loss: 12364.274414\n",
      "    epoch          : 4195\n",
      "    loss           : 12217.243322057227\n",
      "    val_loss       : 12218.76263595998\n",
      "    val_log_likelihood: -12139.50812218905\n",
      "    val_log_marginal: -12147.401604231793\n",
      "Train Epoch: 4196 [256/118836 (0%)] Loss: 12306.565430\n",
      "Train Epoch: 4196 [33024/118836 (28%)] Loss: 12355.655273\n",
      "Train Epoch: 4196 [65792/118836 (55%)] Loss: 12235.650391\n",
      "Train Epoch: 4196 [98560/118836 (83%)] Loss: 12151.672852\n",
      "    epoch          : 4196\n",
      "    loss           : 12215.279007218001\n",
      "    val_loss       : 12220.42622435709\n",
      "    val_log_likelihood: -12138.871928311104\n",
      "    val_log_marginal: -12146.767947071428\n",
      "Train Epoch: 4197 [256/118836 (0%)] Loss: 12207.515625\n",
      "Train Epoch: 4197 [33024/118836 (28%)] Loss: 12211.703125\n",
      "Train Epoch: 4197 [65792/118836 (55%)] Loss: 12188.935547\n",
      "Train Epoch: 4197 [98560/118836 (83%)] Loss: 12221.755859\n",
      "    epoch          : 4197\n",
      "    loss           : 12214.955780700735\n",
      "    val_loss       : 12217.207648516434\n",
      "    val_log_likelihood: -12140.549559941066\n",
      "    val_log_marginal: -12148.439773768463\n",
      "Train Epoch: 4198 [256/118836 (0%)] Loss: 12204.787109\n",
      "Train Epoch: 4198 [33024/118836 (28%)] Loss: 12202.978516\n",
      "Train Epoch: 4198 [65792/118836 (55%)] Loss: 12156.590820\n",
      "Train Epoch: 4198 [98560/118836 (83%)] Loss: 12330.946289\n",
      "    epoch          : 4198\n",
      "    loss           : 12216.884642847912\n",
      "    val_loss       : 12222.137690327254\n",
      "    val_log_likelihood: -12140.885037996277\n",
      "    val_log_marginal: -12148.779853540209\n",
      "Train Epoch: 4199 [256/118836 (0%)] Loss: 12205.322266\n",
      "Train Epoch: 4199 [33024/118836 (28%)] Loss: 12299.285156\n",
      "Train Epoch: 4199 [65792/118836 (55%)] Loss: 12204.293945\n",
      "Train Epoch: 4199 [98560/118836 (83%)] Loss: 12297.125000\n",
      "    epoch          : 4199\n",
      "    loss           : 12215.9501281082\n",
      "    val_loss       : 12218.878252403676\n",
      "    val_log_likelihood: -12140.816609316842\n",
      "    val_log_marginal: -12148.713933475316\n",
      "Train Epoch: 4200 [256/118836 (0%)] Loss: 12196.953125\n",
      "Train Epoch: 4200 [33024/118836 (28%)] Loss: 12169.085938\n",
      "Train Epoch: 4200 [65792/118836 (55%)] Loss: 12189.189453\n",
      "Train Epoch: 4200 [98560/118836 (83%)] Loss: 12180.118164\n",
      "    epoch          : 4200\n",
      "    loss           : 12216.449258167908\n",
      "    val_loss       : 12221.0913964261\n",
      "    val_log_likelihood: -12140.003159732993\n",
      "    val_log_marginal: -12147.895319848105\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch4200.pth ...\n",
      "Train Epoch: 4201 [256/118836 (0%)] Loss: 12208.067383\n",
      "Train Epoch: 4201 [33024/118836 (28%)] Loss: 12280.558594\n",
      "Train Epoch: 4201 [65792/118836 (55%)] Loss: 12241.599609\n",
      "Train Epoch: 4201 [98560/118836 (83%)] Loss: 12374.881836\n",
      "    epoch          : 4201\n",
      "    loss           : 12220.152856990539\n",
      "    val_loss       : 12218.91103281352\n",
      "    val_log_likelihood: -12141.618920757082\n",
      "    val_log_marginal: -12149.5178124012\n",
      "Train Epoch: 4202 [256/118836 (0%)] Loss: 12209.652344\n",
      "Train Epoch: 4202 [33024/118836 (28%)] Loss: 12220.660156\n",
      "Train Epoch: 4202 [65792/118836 (55%)] Loss: 12263.201172\n",
      "Train Epoch: 4202 [98560/118836 (83%)] Loss: 12287.486328\n",
      "    epoch          : 4202\n",
      "    loss           : 12218.876660398573\n",
      "    val_loss       : 12220.233027212838\n",
      "    val_log_likelihood: -12137.940979438068\n",
      "    val_log_marginal: -12145.836949368457\n",
      "Train Epoch: 4203 [256/118836 (0%)] Loss: 12233.322266\n",
      "Train Epoch: 4203 [33024/118836 (28%)] Loss: 12244.737305\n",
      "Train Epoch: 4203 [65792/118836 (55%)] Loss: 12120.833984\n",
      "Train Epoch: 4203 [98560/118836 (83%)] Loss: 12195.333008\n",
      "    epoch          : 4203\n",
      "    loss           : 12219.26162424395\n",
      "    val_loss       : 12216.829827009062\n",
      "    val_log_likelihood: -12140.001392227563\n",
      "    val_log_marginal: -12147.894975185323\n",
      "Train Epoch: 4204 [256/118836 (0%)] Loss: 12218.727539\n",
      "Train Epoch: 4204 [33024/118836 (28%)] Loss: 12248.169922\n",
      "Train Epoch: 4204 [65792/118836 (55%)] Loss: 12229.220703\n",
      "Train Epoch: 4204 [98560/118836 (83%)] Loss: 12232.138672\n",
      "    epoch          : 4204\n",
      "    loss           : 12217.972584037687\n",
      "    val_loss       : 12218.280560205414\n",
      "    val_log_likelihood: -12139.19712281586\n",
      "    val_log_marginal: -12147.092618259\n",
      "Train Epoch: 4205 [256/118836 (0%)] Loss: 12184.674805\n",
      "Train Epoch: 4205 [33024/118836 (28%)] Loss: 12235.464844\n",
      "Train Epoch: 4205 [65792/118836 (55%)] Loss: 12265.277344\n",
      "Train Epoch: 4205 [98560/118836 (83%)] Loss: 12356.555664\n",
      "    epoch          : 4205\n",
      "    loss           : 12217.081564212418\n",
      "    val_loss       : 12217.160083467668\n",
      "    val_log_likelihood: -12140.571144960968\n",
      "    val_log_marginal: -12148.449343348495\n",
      "Train Epoch: 4206 [256/118836 (0%)] Loss: 12288.241211\n",
      "Train Epoch: 4206 [33024/118836 (28%)] Loss: 12174.436523\n",
      "Train Epoch: 4206 [65792/118836 (55%)] Loss: 12147.070312\n",
      "Train Epoch: 4206 [98560/118836 (83%)] Loss: 12234.774414\n",
      "    epoch          : 4206\n",
      "    loss           : 12216.017173768352\n",
      "    val_loss       : 12217.104231981308\n",
      "    val_log_likelihood: -12137.892428207713\n",
      "    val_log_marginal: -12145.785846224251\n",
      "Train Epoch: 4207 [256/118836 (0%)] Loss: 12161.501953\n",
      "Train Epoch: 4207 [33024/118836 (28%)] Loss: 12325.382812\n",
      "Train Epoch: 4207 [65792/118836 (55%)] Loss: 12331.187500\n",
      "Train Epoch: 4207 [98560/118836 (83%)] Loss: 12305.980469\n",
      "    epoch          : 4207\n",
      "    loss           : 12219.309832344656\n",
      "    val_loss       : 12220.481213730807\n",
      "    val_log_likelihood: -12138.200430689103\n",
      "    val_log_marginal: -12146.08841232182\n",
      "Train Epoch: 4208 [256/118836 (0%)] Loss: 12299.123047\n",
      "Train Epoch: 4208 [33024/118836 (28%)] Loss: 12204.394531\n",
      "Train Epoch: 4208 [65792/118836 (55%)] Loss: 12253.751953\n",
      "Train Epoch: 4208 [98560/118836 (83%)] Loss: 12158.522461\n",
      "    epoch          : 4208\n",
      "    loss           : 12219.543272946392\n",
      "    val_loss       : 12214.083355639164\n",
      "    val_log_likelihood: -12140.427102880738\n",
      "    val_log_marginal: -12148.31094301461\n",
      "Train Epoch: 4209 [256/118836 (0%)] Loss: 12344.651367\n",
      "Train Epoch: 4209 [33024/118836 (28%)] Loss: 12218.382812\n",
      "Train Epoch: 4209 [65792/118836 (55%)] Loss: 12189.802734\n",
      "Train Epoch: 4209 [98560/118836 (83%)] Loss: 12176.911133\n",
      "    epoch          : 4209\n",
      "    loss           : 12215.544676482372\n",
      "    val_loss       : 12218.43832813354\n",
      "    val_log_likelihood: -12140.738849417132\n",
      "    val_log_marginal: -12148.62019070476\n",
      "Train Epoch: 4210 [256/118836 (0%)] Loss: 12260.214844\n",
      "Train Epoch: 4210 [33024/118836 (28%)] Loss: 12196.001953\n",
      "Train Epoch: 4210 [65792/118836 (55%)] Loss: 12221.944336\n",
      "Train Epoch: 4210 [98560/118836 (83%)] Loss: 12179.334961\n",
      "    epoch          : 4210\n",
      "    loss           : 12219.858410876757\n",
      "    val_loss       : 12213.771905307089\n",
      "    val_log_likelihood: -12139.474627145368\n",
      "    val_log_marginal: -12147.366827953554\n",
      "Train Epoch: 4211 [256/118836 (0%)] Loss: 12166.220703\n",
      "Train Epoch: 4211 [33024/118836 (28%)] Loss: 12245.122070\n",
      "Train Epoch: 4211 [65792/118836 (55%)] Loss: 12237.940430\n",
      "Train Epoch: 4211 [98560/118836 (83%)] Loss: 12279.451172\n",
      "    epoch          : 4211\n",
      "    loss           : 12216.34756061311\n",
      "    val_loss       : 12216.850749707059\n",
      "    val_log_likelihood: -12140.63549841036\n",
      "    val_log_marginal: -12148.526142068002\n",
      "Train Epoch: 4212 [256/118836 (0%)] Loss: 12281.806641\n",
      "Train Epoch: 4212 [33024/118836 (28%)] Loss: 12339.517578\n",
      "Train Epoch: 4212 [65792/118836 (55%)] Loss: 12292.003906\n",
      "Train Epoch: 4212 [98560/118836 (83%)] Loss: 12199.978516\n",
      "    epoch          : 4212\n",
      "    loss           : 12223.851824209058\n",
      "    val_loss       : 12219.120179379443\n",
      "    val_log_likelihood: -12141.45316797198\n",
      "    val_log_marginal: -12149.344954702665\n",
      "Train Epoch: 4213 [256/118836 (0%)] Loss: 12187.969727\n",
      "Train Epoch: 4213 [33024/118836 (28%)] Loss: 12264.875000\n",
      "Train Epoch: 4213 [65792/118836 (55%)] Loss: 12236.906250\n",
      "Train Epoch: 4213 [98560/118836 (83%)] Loss: 12193.685547\n",
      "    epoch          : 4213\n",
      "    loss           : 12215.116667635959\n",
      "    val_loss       : 12219.15084154905\n",
      "    val_log_likelihood: -12140.355040161032\n",
      "    val_log_marginal: -12148.250034914168\n",
      "Train Epoch: 4214 [256/118836 (0%)] Loss: 12257.481445\n",
      "Train Epoch: 4214 [33024/118836 (28%)] Loss: 12313.005859\n",
      "Train Epoch: 4214 [65792/118836 (55%)] Loss: 12176.933594\n",
      "Train Epoch: 4214 [98560/118836 (83%)] Loss: 12270.962891\n",
      "    epoch          : 4214\n",
      "    loss           : 12215.063813553297\n",
      "    val_loss       : 12215.3346482336\n",
      "    val_log_likelihood: -12140.087660094861\n",
      "    val_log_marginal: -12147.980854131836\n",
      "Train Epoch: 4215 [256/118836 (0%)] Loss: 12278.409180\n",
      "Train Epoch: 4215 [33024/118836 (28%)] Loss: 12265.678711\n",
      "Train Epoch: 4215 [65792/118836 (55%)] Loss: 12231.321289\n",
      "Train Epoch: 4215 [98560/118836 (83%)] Loss: 12143.378906\n",
      "    epoch          : 4215\n",
      "    loss           : 12221.774103081058\n",
      "    val_loss       : 12216.444072891574\n",
      "    val_log_likelihood: -12138.74512655733\n",
      "    val_log_marginal: -12146.639073511224\n",
      "Train Epoch: 4216 [256/118836 (0%)] Loss: 12179.355469\n",
      "Train Epoch: 4216 [33024/118836 (28%)] Loss: 12144.449219\n",
      "Train Epoch: 4216 [65792/118836 (55%)] Loss: 12186.982422\n",
      "Train Epoch: 4216 [98560/118836 (83%)] Loss: 12173.318359\n",
      "    epoch          : 4216\n",
      "    loss           : 12213.54728452621\n",
      "    val_loss       : 12215.467948176529\n",
      "    val_log_likelihood: -12141.023708417339\n",
      "    val_log_marginal: -12148.913895746698\n",
      "Train Epoch: 4217 [256/118836 (0%)] Loss: 12270.560547\n",
      "Train Epoch: 4217 [33024/118836 (28%)] Loss: 12207.773438\n",
      "Train Epoch: 4217 [65792/118836 (55%)] Loss: 12212.402344\n",
      "Train Epoch: 4217 [98560/118836 (83%)] Loss: 12215.667969\n",
      "    epoch          : 4217\n",
      "    loss           : 12219.513786251551\n",
      "    val_loss       : 12215.906312783462\n",
      "    val_log_likelihood: -12141.602275414856\n",
      "    val_log_marginal: -12149.491004143152\n",
      "Train Epoch: 4218 [256/118836 (0%)] Loss: 12202.196289\n",
      "Train Epoch: 4218 [33024/118836 (28%)] Loss: 12254.598633\n",
      "Train Epoch: 4218 [65792/118836 (55%)] Loss: 12249.201172\n",
      "Train Epoch: 4218 [98560/118836 (83%)] Loss: 12127.327148\n",
      "    epoch          : 4218\n",
      "    loss           : 12215.071697619418\n",
      "    val_loss       : 12215.718607017883\n",
      "    val_log_likelihood: -12139.042022558675\n",
      "    val_log_marginal: -12146.929006846021\n",
      "Train Epoch: 4219 [256/118836 (0%)] Loss: 12174.564453\n",
      "Train Epoch: 4219 [33024/118836 (28%)] Loss: 12234.301758\n",
      "Train Epoch: 4219 [65792/118836 (55%)] Loss: 12219.655273\n",
      "Train Epoch: 4219 [98560/118836 (83%)] Loss: 12204.861328\n",
      "    epoch          : 4219\n",
      "    loss           : 12213.999831989247\n",
      "    val_loss       : 12216.334097567273\n",
      "    val_log_likelihood: -12139.901564761683\n",
      "    val_log_marginal: -12147.78932302576\n",
      "Train Epoch: 4220 [256/118836 (0%)] Loss: 12194.978516\n",
      "Train Epoch: 4220 [33024/118836 (28%)] Loss: 12184.552734\n",
      "Train Epoch: 4220 [65792/118836 (55%)] Loss: 12250.985352\n",
      "Train Epoch: 4220 [98560/118836 (83%)] Loss: 12237.894531\n",
      "    epoch          : 4220\n",
      "    loss           : 12216.649600812912\n",
      "    val_loss       : 12217.668594670913\n",
      "    val_log_likelihood: -12142.135010048334\n",
      "    val_log_marginal: -12150.028333390406\n",
      "Train Epoch: 4221 [256/118836 (0%)] Loss: 12253.310547\n",
      "Train Epoch: 4221 [33024/118836 (28%)] Loss: 12275.864258\n",
      "Train Epoch: 4221 [65792/118836 (55%)] Loss: 12185.096680\n",
      "Train Epoch: 4221 [98560/118836 (83%)] Loss: 12231.426758\n",
      "    epoch          : 4221\n",
      "    loss           : 12215.7900658796\n",
      "    val_loss       : 12217.482412772275\n",
      "    val_log_likelihood: -12138.632271149969\n",
      "    val_log_marginal: -12146.520986907106\n",
      "Train Epoch: 4222 [256/118836 (0%)] Loss: 12154.863281\n",
      "Train Epoch: 4222 [33024/118836 (28%)] Loss: 12274.753906\n",
      "Train Epoch: 4222 [65792/118836 (55%)] Loss: 12287.802734\n",
      "Train Epoch: 4222 [98560/118836 (83%)] Loss: 12231.189453\n",
      "    epoch          : 4222\n",
      "    loss           : 12218.484058202801\n",
      "    val_loss       : 12217.694548148902\n",
      "    val_log_likelihood: -12138.62403959238\n",
      "    val_log_marginal: -12146.516369382143\n",
      "Train Epoch: 4223 [256/118836 (0%)] Loss: 12306.737305\n",
      "Train Epoch: 4223 [33024/118836 (28%)] Loss: 12263.092773\n",
      "Train Epoch: 4223 [65792/118836 (55%)] Loss: 12313.307617\n",
      "Train Epoch: 4223 [98560/118836 (83%)] Loss: 12266.183594\n",
      "    epoch          : 4223\n",
      "    loss           : 12221.687000814207\n",
      "    val_loss       : 12217.854381121642\n",
      "    val_log_likelihood: -12142.686444278588\n",
      "    val_log_marginal: -12150.57842021962\n",
      "Train Epoch: 4224 [256/118836 (0%)] Loss: 12262.292969\n",
      "Train Epoch: 4224 [33024/118836 (28%)] Loss: 12307.898438\n",
      "Train Epoch: 4224 [65792/118836 (55%)] Loss: 12260.365234\n",
      "Train Epoch: 4224 [98560/118836 (83%)] Loss: 12425.421875\n",
      "    epoch          : 4224\n",
      "    loss           : 12223.867406560174\n",
      "    val_loss       : 12214.935811927688\n",
      "    val_log_likelihood: -12139.970338832454\n",
      "    val_log_marginal: -12147.850302147834\n",
      "Train Epoch: 4225 [256/118836 (0%)] Loss: 12201.970703\n",
      "Train Epoch: 4225 [33024/118836 (28%)] Loss: 12244.426758\n",
      "Train Epoch: 4225 [65792/118836 (55%)] Loss: 12309.663086\n",
      "Train Epoch: 4225 [98560/118836 (83%)] Loss: 12199.076172\n",
      "    epoch          : 4225\n",
      "    loss           : 12217.486797101168\n",
      "    val_loss       : 12214.760716319965\n",
      "    val_log_likelihood: -12139.4199622622\n",
      "    val_log_marginal: -12147.309275949152\n",
      "Train Epoch: 4226 [256/118836 (0%)] Loss: 12176.826172\n",
      "Train Epoch: 4226 [33024/118836 (28%)] Loss: 12238.654297\n",
      "Train Epoch: 4226 [65792/118836 (55%)] Loss: 12206.372070\n",
      "Train Epoch: 4226 [98560/118836 (83%)] Loss: 12280.991211\n",
      "    epoch          : 4226\n",
      "    loss           : 12217.944395872106\n",
      "    val_loss       : 12218.185014737805\n",
      "    val_log_likelihood: -12140.44482236094\n",
      "    val_log_marginal: -12148.339837705416\n",
      "Train Epoch: 4227 [256/118836 (0%)] Loss: 12177.691406\n",
      "Train Epoch: 4227 [33024/118836 (28%)] Loss: 12159.449219\n",
      "Train Epoch: 4227 [65792/118836 (55%)] Loss: 12224.993164\n",
      "Train Epoch: 4227 [98560/118836 (83%)] Loss: 12213.568359\n",
      "    epoch          : 4227\n",
      "    loss           : 12214.973230555986\n",
      "    val_loss       : 12217.886581670695\n",
      "    val_log_likelihood: -12138.770051760237\n",
      "    val_log_marginal: -12146.656045166947\n",
      "Train Epoch: 4228 [256/118836 (0%)] Loss: 12280.855469\n",
      "Train Epoch: 4228 [33024/118836 (28%)] Loss: 12332.750000\n",
      "Train Epoch: 4228 [65792/118836 (55%)] Loss: 12209.078125\n",
      "Train Epoch: 4228 [98560/118836 (83%)] Loss: 12171.381836\n",
      "    epoch          : 4228\n",
      "    loss           : 12220.308347872724\n",
      "    val_loss       : 12219.741752747756\n",
      "    val_log_likelihood: -12140.909191157465\n",
      "    val_log_marginal: -12148.796703252301\n",
      "Train Epoch: 4229 [256/118836 (0%)] Loss: 12174.814453\n",
      "Train Epoch: 4229 [33024/118836 (28%)] Loss: 12270.268555\n",
      "Train Epoch: 4229 [65792/118836 (55%)] Loss: 12216.126953\n",
      "Train Epoch: 4229 [98560/118836 (83%)] Loss: 12193.152344\n",
      "    epoch          : 4229\n",
      "    loss           : 12217.789480911393\n",
      "    val_loss       : 12215.030456591081\n",
      "    val_log_likelihood: -12139.080128689775\n",
      "    val_log_marginal: -12146.974930567801\n",
      "Train Epoch: 4230 [256/118836 (0%)] Loss: 12180.179688\n",
      "Train Epoch: 4230 [33024/118836 (28%)] Loss: 12170.398438\n",
      "Train Epoch: 4230 [65792/118836 (55%)] Loss: 12209.165039\n",
      "Train Epoch: 4230 [98560/118836 (83%)] Loss: 12205.438477\n",
      "    epoch          : 4230\n",
      "    loss           : 12219.659409571443\n",
      "    val_loss       : 12216.925485008693\n",
      "    val_log_likelihood: -12139.12265608845\n",
      "    val_log_marginal: -12147.01844031711\n",
      "Train Epoch: 4231 [256/118836 (0%)] Loss: 12257.835938\n",
      "Train Epoch: 4231 [33024/118836 (28%)] Loss: 12218.310547\n",
      "Train Epoch: 4231 [65792/118836 (55%)] Loss: 12257.283203\n",
      "Train Epoch: 4231 [98560/118836 (83%)] Loss: 12360.613281\n",
      "    epoch          : 4231\n",
      "    loss           : 12214.365663125516\n",
      "    val_loss       : 12216.732815691275\n",
      "    val_log_likelihood: -12138.89497712469\n",
      "    val_log_marginal: -12146.783771031112\n",
      "Train Epoch: 4232 [256/118836 (0%)] Loss: 12242.363281\n",
      "Train Epoch: 4232 [33024/118836 (28%)] Loss: 12179.018555\n",
      "Train Epoch: 4232 [65792/118836 (55%)] Loss: 12357.148438\n",
      "Train Epoch: 4232 [98560/118836 (83%)] Loss: 12276.481445\n",
      "    epoch          : 4232\n",
      "    loss           : 12216.973550907258\n",
      "    val_loss       : 12217.797877101597\n",
      "    val_log_likelihood: -12139.09820971619\n",
      "    val_log_marginal: -12146.993135673576\n",
      "Train Epoch: 4233 [256/118836 (0%)] Loss: 12200.171875\n",
      "Train Epoch: 4233 [33024/118836 (28%)] Loss: 12251.363281\n",
      "Train Epoch: 4233 [65792/118836 (55%)] Loss: 12161.462891\n",
      "Train Epoch: 4233 [98560/118836 (83%)] Loss: 12269.817383\n",
      "    epoch          : 4233\n",
      "    loss           : 12216.957989719034\n",
      "    val_loss       : 12218.889451148096\n",
      "    val_log_likelihood: -12139.02591178143\n",
      "    val_log_marginal: -12146.911768128284\n",
      "Train Epoch: 4234 [256/118836 (0%)] Loss: 12210.575195\n",
      "Train Epoch: 4234 [33024/118836 (28%)] Loss: 12272.496094\n",
      "Train Epoch: 4234 [65792/118836 (55%)] Loss: 12193.786133\n",
      "Train Epoch: 4234 [98560/118836 (83%)] Loss: 12209.421875\n",
      "    epoch          : 4234\n",
      "    loss           : 12214.090522093415\n",
      "    val_loss       : 12215.569773577454\n",
      "    val_log_likelihood: -12140.156766956161\n",
      "    val_log_marginal: -12148.04437408103\n",
      "Train Epoch: 4235 [256/118836 (0%)] Loss: 12272.318359\n",
      "Train Epoch: 4235 [33024/118836 (28%)] Loss: 12237.557617\n",
      "Train Epoch: 4235 [65792/118836 (55%)] Loss: 12209.642578\n",
      "Train Epoch: 4235 [98560/118836 (83%)] Loss: 12267.135742\n",
      "    epoch          : 4235\n",
      "    loss           : 12215.374777870398\n",
      "    val_loss       : 12217.484097930646\n",
      "    val_log_likelihood: -12141.527578480407\n",
      "    val_log_marginal: -12149.418417094525\n",
      "Train Epoch: 4236 [256/118836 (0%)] Loss: 12165.231445\n",
      "Train Epoch: 4236 [33024/118836 (28%)] Loss: 12254.814453\n",
      "Train Epoch: 4236 [65792/118836 (55%)] Loss: 12135.019531\n",
      "Train Epoch: 4236 [98560/118836 (83%)] Loss: 12207.855469\n",
      "    epoch          : 4236\n",
      "    loss           : 12214.323730226426\n",
      "    val_loss       : 12216.858909099536\n",
      "    val_log_likelihood: -12140.88439390121\n",
      "    val_log_marginal: -12148.771359962035\n",
      "Train Epoch: 4237 [256/118836 (0%)] Loss: 12260.398438\n",
      "Train Epoch: 4237 [33024/118836 (28%)] Loss: 12253.353516\n",
      "Train Epoch: 4237 [65792/118836 (55%)] Loss: 12229.102539\n",
      "Train Epoch: 4237 [98560/118836 (83%)] Loss: 12224.664062\n",
      "    epoch          : 4237\n",
      "    loss           : 12216.762659610215\n",
      "    val_loss       : 12218.81502130091\n",
      "    val_log_likelihood: -12139.784815705128\n",
      "    val_log_marginal: -12147.679304688207\n",
      "Train Epoch: 4238 [256/118836 (0%)] Loss: 12182.777344\n",
      "Train Epoch: 4238 [33024/118836 (28%)] Loss: 12241.376953\n",
      "Train Epoch: 4238 [65792/118836 (55%)] Loss: 12169.488281\n",
      "Train Epoch: 4238 [98560/118836 (83%)] Loss: 12285.661133\n",
      "    epoch          : 4238\n",
      "    loss           : 12218.339565239867\n",
      "    val_loss       : 12220.088318833454\n",
      "    val_log_likelihood: -12138.820344648211\n",
      "    val_log_marginal: -12146.709661453277\n",
      "Train Epoch: 4239 [256/118836 (0%)] Loss: 12273.384766\n",
      "Train Epoch: 4239 [33024/118836 (28%)] Loss: 12203.012695\n",
      "Train Epoch: 4239 [65792/118836 (55%)] Loss: 12195.036133\n",
      "Train Epoch: 4239 [98560/118836 (83%)] Loss: 12381.581055\n",
      "    epoch          : 4239\n",
      "    loss           : 12217.147781127223\n",
      "    val_loss       : 12215.118433661792\n",
      "    val_log_likelihood: -12140.033313785929\n",
      "    val_log_marginal: -12147.92172164584\n",
      "Train Epoch: 4240 [256/118836 (0%)] Loss: 12205.189453\n",
      "Train Epoch: 4240 [33024/118836 (28%)] Loss: 12299.092773\n",
      "Train Epoch: 4240 [65792/118836 (55%)] Loss: 12417.572266\n",
      "Train Epoch: 4240 [98560/118836 (83%)] Loss: 12353.549805\n",
      "    epoch          : 4240\n",
      "    loss           : 12215.4380117866\n",
      "    val_loss       : 12219.365427919809\n",
      "    val_log_likelihood: -12140.969340622414\n",
      "    val_log_marginal: -12148.857813005545\n",
      "Train Epoch: 4241 [256/118836 (0%)] Loss: 12271.883789\n",
      "Train Epoch: 4241 [33024/118836 (28%)] Loss: 12223.527344\n",
      "Train Epoch: 4241 [65792/118836 (55%)] Loss: 12234.656250\n",
      "Train Epoch: 4241 [98560/118836 (83%)] Loss: 12234.379883\n",
      "    epoch          : 4241\n",
      "    loss           : 12218.810166750673\n",
      "    val_loss       : 12215.839162597036\n",
      "    val_log_likelihood: -12141.981580851943\n",
      "    val_log_marginal: -12149.872054521617\n",
      "Train Epoch: 4242 [256/118836 (0%)] Loss: 12214.749023\n",
      "Train Epoch: 4242 [33024/118836 (28%)] Loss: 12171.098633\n",
      "Train Epoch: 4242 [65792/118836 (55%)] Loss: 12191.158203\n",
      "Train Epoch: 4242 [98560/118836 (83%)] Loss: 12255.733398\n",
      "    epoch          : 4242\n",
      "    loss           : 12214.839575902088\n",
      "    val_loss       : 12218.508360626802\n",
      "    val_log_likelihood: -12139.234753185743\n",
      "    val_log_marginal: -12147.126987096404\n",
      "Train Epoch: 4243 [256/118836 (0%)] Loss: 12254.438477\n",
      "Train Epoch: 4243 [33024/118836 (28%)] Loss: 12262.641602\n",
      "Train Epoch: 4243 [65792/118836 (55%)] Loss: 12203.104492\n",
      "Train Epoch: 4243 [98560/118836 (83%)] Loss: 12267.265625\n",
      "    epoch          : 4243\n",
      "    loss           : 12217.263771873708\n",
      "    val_loss       : 12218.642956825368\n",
      "    val_log_likelihood: -12139.743997492764\n",
      "    val_log_marginal: -12147.632981341278\n",
      "Train Epoch: 4244 [256/118836 (0%)] Loss: 12192.609375\n",
      "Train Epoch: 4244 [33024/118836 (28%)] Loss: 12198.875000\n",
      "Train Epoch: 4244 [65792/118836 (55%)] Loss: 12294.298828\n",
      "Train Epoch: 4244 [98560/118836 (83%)] Loss: 12209.045898\n",
      "    epoch          : 4244\n",
      "    loss           : 12218.346857229631\n",
      "    val_loss       : 12220.26722341144\n",
      "    val_log_likelihood: -12140.04231011554\n",
      "    val_log_marginal: -12147.923436529398\n",
      "Train Epoch: 4245 [256/118836 (0%)] Loss: 12151.824219\n",
      "Train Epoch: 4245 [33024/118836 (28%)] Loss: 12280.043945\n",
      "Train Epoch: 4245 [65792/118836 (55%)] Loss: 12156.024414\n",
      "Train Epoch: 4245 [98560/118836 (83%)] Loss: 12290.771484\n",
      "    epoch          : 4245\n",
      "    loss           : 12221.460874334418\n",
      "    val_loss       : 12219.001146304123\n",
      "    val_log_likelihood: -12140.355584257393\n",
      "    val_log_marginal: -12148.237344704632\n",
      "Train Epoch: 4246 [256/118836 (0%)] Loss: 12208.498047\n",
      "Train Epoch: 4246 [33024/118836 (28%)] Loss: 12202.277344\n",
      "Train Epoch: 4246 [65792/118836 (55%)] Loss: 12152.048828\n",
      "Train Epoch: 4246 [98560/118836 (83%)] Loss: 12243.487305\n",
      "    epoch          : 4246\n",
      "    loss           : 12217.275193697013\n",
      "    val_loss       : 12215.150245612997\n",
      "    val_log_likelihood: -12139.098863665735\n",
      "    val_log_marginal: -12146.986522148934\n",
      "Train Epoch: 4247 [256/118836 (0%)] Loss: 12169.917969\n",
      "Train Epoch: 4247 [33024/118836 (28%)] Loss: 12133.255859\n",
      "Train Epoch: 4247 [65792/118836 (55%)] Loss: 12263.184570\n",
      "Train Epoch: 4247 [98560/118836 (83%)] Loss: 12216.947266\n",
      "    epoch          : 4247\n",
      "    loss           : 12214.552490759408\n",
      "    val_loss       : 12219.054826872813\n",
      "    val_log_likelihood: -12140.957065821443\n",
      "    val_log_marginal: -12148.836184565518\n",
      "Train Epoch: 4248 [256/118836 (0%)] Loss: 12224.678711\n",
      "Train Epoch: 4248 [33024/118836 (28%)] Loss: 12273.347656\n",
      "Train Epoch: 4248 [65792/118836 (55%)] Loss: 12210.214844\n",
      "Train Epoch: 4248 [98560/118836 (83%)] Loss: 12232.501953\n",
      "    epoch          : 4248\n",
      "    loss           : 12222.900756209936\n",
      "    val_loss       : 12219.260901787275\n",
      "    val_log_likelihood: -12139.986611320048\n",
      "    val_log_marginal: -12147.880399767178\n",
      "Train Epoch: 4249 [256/118836 (0%)] Loss: 12315.889648\n",
      "Train Epoch: 4249 [33024/118836 (28%)] Loss: 12150.459961\n",
      "Train Epoch: 4249 [65792/118836 (55%)] Loss: 12304.855469\n",
      "Train Epoch: 4249 [98560/118836 (83%)] Loss: 12348.369141\n",
      "    epoch          : 4249\n",
      "    loss           : 12214.193790871846\n",
      "    val_loss       : 12217.074422452624\n",
      "    val_log_likelihood: -12138.451323407775\n",
      "    val_log_marginal: -12146.333938098727\n",
      "Train Epoch: 4250 [256/118836 (0%)] Loss: 12204.658203\n",
      "Train Epoch: 4250 [33024/118836 (28%)] Loss: 12215.558594\n",
      "Train Epoch: 4250 [65792/118836 (55%)] Loss: 12323.026367\n",
      "Train Epoch: 4250 [98560/118836 (83%)] Loss: 12170.857422\n",
      "    epoch          : 4250\n",
      "    loss           : 12217.384024116005\n",
      "    val_loss       : 12214.754075702755\n",
      "    val_log_likelihood: -12141.360969002017\n",
      "    val_log_marginal: -12149.250805687358\n",
      "Train Epoch: 4251 [256/118836 (0%)] Loss: 12316.714844\n",
      "Train Epoch: 4251 [33024/118836 (28%)] Loss: 12340.410156\n",
      "Train Epoch: 4251 [65792/118836 (55%)] Loss: 12283.472656\n",
      "Train Epoch: 4251 [98560/118836 (83%)] Loss: 12152.464844\n",
      "    epoch          : 4251\n",
      "    loss           : 12215.803765379445\n",
      "    val_loss       : 12215.330286516131\n",
      "    val_log_likelihood: -12138.505523515045\n",
      "    val_log_marginal: -12146.393373858933\n",
      "Train Epoch: 4252 [256/118836 (0%)] Loss: 12162.076172\n",
      "Train Epoch: 4252 [33024/118836 (28%)] Loss: 12284.436523\n",
      "Train Epoch: 4252 [65792/118836 (55%)] Loss: 12208.740234\n",
      "Train Epoch: 4252 [98560/118836 (83%)] Loss: 12158.305664\n",
      "    epoch          : 4252\n",
      "    loss           : 12219.08955958566\n",
      "    val_loss       : 12218.320858462323\n",
      "    val_log_likelihood: -12137.249534254808\n",
      "    val_log_marginal: -12145.13873567476\n",
      "Train Epoch: 4253 [256/118836 (0%)] Loss: 12303.243164\n",
      "Train Epoch: 4253 [33024/118836 (28%)] Loss: 12305.188477\n",
      "Train Epoch: 4253 [65792/118836 (55%)] Loss: 12342.902344\n",
      "Train Epoch: 4253 [98560/118836 (83%)] Loss: 12193.343750\n",
      "    epoch          : 4253\n",
      "    loss           : 12219.225389494159\n",
      "    val_loss       : 12220.391005833084\n",
      "    val_log_likelihood: -12138.454748726994\n",
      "    val_log_marginal: -12146.342463827259\n",
      "Train Epoch: 4254 [256/118836 (0%)] Loss: 12170.998047\n",
      "Train Epoch: 4254 [33024/118836 (28%)] Loss: 12291.668945\n",
      "Train Epoch: 4254 [65792/118836 (55%)] Loss: 12294.315430\n",
      "Train Epoch: 4254 [98560/118836 (83%)] Loss: 12263.568359\n",
      "    epoch          : 4254\n",
      "    loss           : 12215.837556380533\n",
      "    val_loss       : 12219.221707782557\n",
      "    val_log_likelihood: -12140.69198685639\n",
      "    val_log_marginal: -12148.578240788644\n",
      "Train Epoch: 4255 [256/118836 (0%)] Loss: 12200.902344\n",
      "Train Epoch: 4255 [33024/118836 (28%)] Loss: 12183.872070\n",
      "Train Epoch: 4255 [65792/118836 (55%)] Loss: 12202.313477\n",
      "Train Epoch: 4255 [98560/118836 (83%)] Loss: 12270.854492\n",
      "    epoch          : 4255\n",
      "    loss           : 12221.0171875\n",
      "    val_loss       : 12214.435187050949\n",
      "    val_log_likelihood: -12137.725213890613\n",
      "    val_log_marginal: -12145.616361161872\n",
      "Train Epoch: 4256 [256/118836 (0%)] Loss: 12189.561523\n",
      "Train Epoch: 4256 [33024/118836 (28%)] Loss: 12314.253906\n",
      "Train Epoch: 4256 [65792/118836 (55%)] Loss: 12228.240234\n",
      "Train Epoch: 4256 [98560/118836 (83%)] Loss: 12307.595703\n",
      "    epoch          : 4256\n",
      "    loss           : 12217.113800306295\n",
      "    val_loss       : 12217.595553320423\n",
      "    val_log_likelihood: -12139.40438055728\n",
      "    val_log_marginal: -12147.287141275658\n",
      "Train Epoch: 4257 [256/118836 (0%)] Loss: 12337.664062\n",
      "Train Epoch: 4257 [33024/118836 (28%)] Loss: 12220.335938\n",
      "Train Epoch: 4257 [65792/118836 (55%)] Loss: 12224.337891\n",
      "Train Epoch: 4257 [98560/118836 (83%)] Loss: 12299.665039\n",
      "    epoch          : 4257\n",
      "    loss           : 12220.402542455025\n",
      "    val_loss       : 12218.66261322616\n",
      "    val_log_likelihood: -12139.85299979968\n",
      "    val_log_marginal: -12147.74177864489\n",
      "Train Epoch: 4258 [256/118836 (0%)] Loss: 12162.048828\n",
      "Train Epoch: 4258 [33024/118836 (28%)] Loss: 12186.227539\n",
      "Train Epoch: 4258 [65792/118836 (55%)] Loss: 12177.068359\n",
      "Train Epoch: 4258 [98560/118836 (83%)] Loss: 12164.238281\n",
      "    epoch          : 4258\n",
      "    loss           : 12220.98218326742\n",
      "    val_loss       : 12216.284553757825\n",
      "    val_log_likelihood: -12138.280013828577\n",
      "    val_log_marginal: -12146.167260987475\n",
      "Train Epoch: 4259 [256/118836 (0%)] Loss: 12214.722656\n",
      "Train Epoch: 4259 [33024/118836 (28%)] Loss: 12235.717773\n",
      "Train Epoch: 4259 [65792/118836 (55%)] Loss: 12233.712891\n",
      "Train Epoch: 4259 [98560/118836 (83%)] Loss: 12186.661133\n",
      "    epoch          : 4259\n",
      "    loss           : 12217.628037925197\n",
      "    val_loss       : 12221.903890668906\n",
      "    val_log_likelihood: -12139.649973505995\n",
      "    val_log_marginal: -12147.529957236753\n",
      "Train Epoch: 4260 [256/118836 (0%)] Loss: 12307.827148\n",
      "Train Epoch: 4260 [33024/118836 (28%)] Loss: 12333.000977\n",
      "Train Epoch: 4260 [65792/118836 (55%)] Loss: 12203.239258\n",
      "Train Epoch: 4260 [98560/118836 (83%)] Loss: 12282.823242\n",
      "    epoch          : 4260\n",
      "    loss           : 12218.644649180624\n",
      "    val_loss       : 12218.31411569746\n",
      "    val_log_likelihood: -12140.460368363576\n",
      "    val_log_marginal: -12148.347617038029\n",
      "Train Epoch: 4261 [256/118836 (0%)] Loss: 12189.070312\n",
      "Train Epoch: 4261 [33024/118836 (28%)] Loss: 12210.079102\n",
      "Train Epoch: 4261 [65792/118836 (55%)] Loss: 12204.714844\n",
      "Train Epoch: 4261 [98560/118836 (83%)] Loss: 12268.438477\n",
      "    epoch          : 4261\n",
      "    loss           : 12222.094585045752\n",
      "    val_loss       : 12220.771033531135\n",
      "    val_log_likelihood: -12139.217605426747\n",
      "    val_log_marginal: -12147.107401319543\n",
      "Train Epoch: 4262 [256/118836 (0%)] Loss: 12290.650391\n",
      "Train Epoch: 4262 [33024/118836 (28%)] Loss: 12257.355469\n",
      "Train Epoch: 4262 [65792/118836 (55%)] Loss: 12186.954102\n",
      "Train Epoch: 4262 [98560/118836 (83%)] Loss: 12179.628906\n",
      "    epoch          : 4262\n",
      "    loss           : 12215.432282781483\n",
      "    val_loss       : 12218.403394387347\n",
      "    val_log_likelihood: -12140.945286329095\n",
      "    val_log_marginal: -12148.828165582323\n",
      "Train Epoch: 4263 [256/118836 (0%)] Loss: 12251.802734\n",
      "Train Epoch: 4263 [33024/118836 (28%)] Loss: 12332.030273\n",
      "Train Epoch: 4263 [65792/118836 (55%)] Loss: 12188.162109\n",
      "Train Epoch: 4263 [98560/118836 (83%)] Loss: 12194.750000\n",
      "    epoch          : 4263\n",
      "    loss           : 12217.213109846722\n",
      "    val_loss       : 12222.345033169378\n",
      "    val_log_likelihood: -12141.49401930185\n",
      "    val_log_marginal: -12149.371328357118\n",
      "Train Epoch: 4264 [256/118836 (0%)] Loss: 12361.576172\n",
      "Train Epoch: 4264 [33024/118836 (28%)] Loss: 12258.521484\n",
      "Train Epoch: 4264 [65792/118836 (55%)] Loss: 12306.562500\n",
      "Train Epoch: 4264 [98560/118836 (83%)] Loss: 12346.468750\n",
      "    epoch          : 4264\n",
      "    loss           : 12218.735641542598\n",
      "    val_loss       : 12216.585967487035\n",
      "    val_log_likelihood: -12140.726144573253\n",
      "    val_log_marginal: -12148.610561020647\n",
      "Train Epoch: 4265 [256/118836 (0%)] Loss: 12178.437500\n",
      "Train Epoch: 4265 [33024/118836 (28%)] Loss: 12307.640625\n",
      "Train Epoch: 4265 [65792/118836 (55%)] Loss: 12298.500977\n",
      "Train Epoch: 4265 [98560/118836 (83%)] Loss: 12370.723633\n",
      "    epoch          : 4265\n",
      "    loss           : 12222.535187913565\n",
      "    val_loss       : 12218.23138520167\n",
      "    val_log_likelihood: -12140.332147565136\n",
      "    val_log_marginal: -12148.2266239203\n",
      "Train Epoch: 4266 [256/118836 (0%)] Loss: 12233.498047\n",
      "Train Epoch: 4266 [33024/118836 (28%)] Loss: 12369.125000\n",
      "Train Epoch: 4266 [65792/118836 (55%)] Loss: 12253.621094\n",
      "Train Epoch: 4266 [98560/118836 (83%)] Loss: 12277.509766\n",
      "    epoch          : 4266\n",
      "    loss           : 12221.325436343312\n",
      "    val_loss       : 12220.061236519765\n",
      "    val_log_likelihood: -12141.043936750413\n",
      "    val_log_marginal: -12148.933588693484\n",
      "Train Epoch: 4267 [256/118836 (0%)] Loss: 12149.203125\n",
      "Train Epoch: 4267 [33024/118836 (28%)] Loss: 12188.017578\n",
      "Train Epoch: 4267 [65792/118836 (55%)] Loss: 12332.713867\n",
      "Train Epoch: 4267 [98560/118836 (83%)] Loss: 12309.869141\n",
      "    epoch          : 4267\n",
      "    loss           : 12218.317971980976\n",
      "    val_loss       : 12213.885618970182\n",
      "    val_log_likelihood: -12138.634644948046\n",
      "    val_log_marginal: -12146.522845070833\n",
      "Train Epoch: 4268 [256/118836 (0%)] Loss: 12202.819336\n",
      "Train Epoch: 4268 [33024/118836 (28%)] Loss: 12316.237305\n",
      "Train Epoch: 4268 [65792/118836 (55%)] Loss: 12279.598633\n",
      "Train Epoch: 4268 [98560/118836 (83%)] Loss: 12216.363281\n",
      "    epoch          : 4268\n",
      "    loss           : 12213.608910547198\n",
      "    val_loss       : 12219.374770465412\n",
      "    val_log_likelihood: -12139.383201347962\n",
      "    val_log_marginal: -12147.271422302529\n",
      "Train Epoch: 4269 [256/118836 (0%)] Loss: 12193.755859\n",
      "Train Epoch: 4269 [33024/118836 (28%)] Loss: 12379.756836\n",
      "Train Epoch: 4269 [65792/118836 (55%)] Loss: 12199.611328\n",
      "Train Epoch: 4269 [98560/118836 (83%)] Loss: 12224.094727\n",
      "    epoch          : 4269\n",
      "    loss           : 12218.489133096828\n",
      "    val_loss       : 12216.983240448079\n",
      "    val_log_likelihood: -12140.27000814206\n",
      "    val_log_marginal: -12148.158845735345\n",
      "Train Epoch: 4270 [256/118836 (0%)] Loss: 12181.453125\n",
      "Train Epoch: 4270 [33024/118836 (28%)] Loss: 12246.643555\n",
      "Train Epoch: 4270 [65792/118836 (55%)] Loss: 12238.573242\n",
      "Train Epoch: 4270 [98560/118836 (83%)] Loss: 12200.231445\n",
      "    epoch          : 4270\n",
      "    loss           : 12218.237868815913\n",
      "    val_loss       : 12218.552593665174\n",
      "    val_log_likelihood: -12139.62019666951\n",
      "    val_log_marginal: -12147.50805056793\n",
      "Train Epoch: 4271 [256/118836 (0%)] Loss: 12238.300781\n",
      "Train Epoch: 4271 [33024/118836 (28%)] Loss: 12390.678711\n",
      "Train Epoch: 4271 [65792/118836 (55%)] Loss: 12338.214844\n",
      "Train Epoch: 4271 [98560/118836 (83%)] Loss: 12256.630859\n",
      "    epoch          : 4271\n",
      "    loss           : 12218.43973599695\n",
      "    val_loss       : 12217.856759978007\n",
      "    val_log_likelihood: -12142.16185332015\n",
      "    val_log_marginal: -12150.043653103803\n",
      "Train Epoch: 4272 [256/118836 (0%)] Loss: 12220.642578\n",
      "Train Epoch: 4272 [33024/118836 (28%)] Loss: 12323.717773\n",
      "Train Epoch: 4272 [65792/118836 (55%)] Loss: 12198.913086\n",
      "Train Epoch: 4272 [98560/118836 (83%)] Loss: 12195.980469\n",
      "    epoch          : 4272\n",
      "    loss           : 12218.461149452025\n",
      "    val_loss       : 12218.221123842446\n",
      "    val_log_likelihood: -12138.3553945991\n",
      "    val_log_marginal: -12146.242366900377\n",
      "Train Epoch: 4273 [256/118836 (0%)] Loss: 12260.628906\n",
      "Train Epoch: 4273 [33024/118836 (28%)] Loss: 12245.687500\n",
      "Train Epoch: 4273 [65792/118836 (55%)] Loss: 12245.945312\n",
      "Train Epoch: 4273 [98560/118836 (83%)] Loss: 12196.715820\n",
      "    epoch          : 4273\n",
      "    loss           : 12214.601399820358\n",
      "    val_loss       : 12215.6684547287\n",
      "    val_log_likelihood: -12137.436916001085\n",
      "    val_log_marginal: -12145.325718137308\n",
      "Train Epoch: 4274 [256/118836 (0%)] Loss: 12280.372070\n",
      "Train Epoch: 4274 [33024/118836 (28%)] Loss: 12209.064453\n",
      "Train Epoch: 4274 [65792/118836 (55%)] Loss: 12299.650391\n",
      "Train Epoch: 4274 [98560/118836 (83%)] Loss: 12183.619141\n",
      "    epoch          : 4274\n",
      "    loss           : 12214.794807983095\n",
      "    val_loss       : 12215.25556917487\n",
      "    val_log_likelihood: -12137.597759641232\n",
      "    val_log_marginal: -12145.480098091553\n",
      "Train Epoch: 4275 [256/118836 (0%)] Loss: 12320.833008\n",
      "Train Epoch: 4275 [33024/118836 (28%)] Loss: 12323.975586\n",
      "Train Epoch: 4275 [65792/118836 (55%)] Loss: 12275.263672\n",
      "Train Epoch: 4275 [98560/118836 (83%)] Loss: 12160.177734\n",
      "    epoch          : 4275\n",
      "    loss           : 12217.687299517938\n",
      "    val_loss       : 12214.565637163725\n",
      "    val_log_likelihood: -12139.017217548077\n",
      "    val_log_marginal: -12146.901815106265\n",
      "Train Epoch: 4276 [256/118836 (0%)] Loss: 12192.892578\n",
      "Train Epoch: 4276 [33024/118836 (28%)] Loss: 12276.576172\n",
      "Train Epoch: 4276 [65792/118836 (55%)] Loss: 12302.809570\n",
      "Train Epoch: 4276 [98560/118836 (83%)] Loss: 12170.008789\n",
      "    epoch          : 4276\n",
      "    loss           : 12216.921554487179\n",
      "    val_loss       : 12219.493528515728\n",
      "    val_log_likelihood: -12142.436018920596\n",
      "    val_log_marginal: -12150.327219276429\n",
      "Train Epoch: 4277 [256/118836 (0%)] Loss: 12195.922852\n",
      "Train Epoch: 4277 [33024/118836 (28%)] Loss: 12283.060547\n",
      "Train Epoch: 4277 [65792/118836 (55%)] Loss: 12262.853516\n",
      "Train Epoch: 4277 [98560/118836 (83%)] Loss: 12236.561523\n",
      "    epoch          : 4277\n",
      "    loss           : 12220.859922004238\n",
      "    val_loss       : 12214.790041310647\n",
      "    val_log_likelihood: -12139.65886741367\n",
      "    val_log_marginal: -12147.558045596612\n",
      "Train Epoch: 4278 [256/118836 (0%)] Loss: 12187.083984\n",
      "Train Epoch: 4278 [33024/118836 (28%)] Loss: 12293.006836\n",
      "Train Epoch: 4278 [65792/118836 (55%)] Loss: 12189.273438\n",
      "Train Epoch: 4278 [98560/118836 (83%)] Loss: 12358.107422\n",
      "    epoch          : 4278\n",
      "    loss           : 12218.454208830904\n",
      "    val_loss       : 12218.015688947904\n",
      "    val_log_likelihood: -12140.218285062552\n",
      "    val_log_marginal: -12148.10516057224\n",
      "Train Epoch: 4279 [256/118836 (0%)] Loss: 12189.253906\n",
      "Train Epoch: 4279 [33024/118836 (28%)] Loss: 12308.888672\n",
      "Train Epoch: 4279 [65792/118836 (55%)] Loss: 12278.068359\n",
      "Train Epoch: 4279 [98560/118836 (83%)] Loss: 12261.903320\n",
      "    epoch          : 4279\n",
      "    loss           : 12217.791306897487\n",
      "    val_loss       : 12217.442422513152\n",
      "    val_log_likelihood: -12141.758062415995\n",
      "    val_log_marginal: -12149.646763147264\n",
      "Train Epoch: 4280 [256/118836 (0%)] Loss: 12154.894531\n",
      "Train Epoch: 4280 [33024/118836 (28%)] Loss: 12271.677734\n",
      "Train Epoch: 4280 [65792/118836 (55%)] Loss: 12286.983398\n",
      "Train Epoch: 4280 [98560/118836 (83%)] Loss: 12239.009766\n",
      "    epoch          : 4280\n",
      "    loss           : 12218.225835368848\n",
      "    val_loss       : 12219.003220200031\n",
      "    val_log_likelihood: -12139.080327394799\n",
      "    val_log_marginal: -12146.966539434225\n",
      "Train Epoch: 4281 [256/118836 (0%)] Loss: 12171.398438\n",
      "Train Epoch: 4281 [33024/118836 (28%)] Loss: 12188.553711\n",
      "Train Epoch: 4281 [65792/118836 (55%)] Loss: 12119.738281\n",
      "Train Epoch: 4281 [98560/118836 (83%)] Loss: 12269.308594\n",
      "    epoch          : 4281\n",
      "    loss           : 12216.055347749947\n",
      "    val_loss       : 12215.682531474708\n",
      "    val_log_likelihood: -12140.772855439673\n",
      "    val_log_marginal: -12148.658577028993\n",
      "Train Epoch: 4282 [256/118836 (0%)] Loss: 12228.921875\n",
      "Train Epoch: 4282 [33024/118836 (28%)] Loss: 12310.388672\n",
      "Train Epoch: 4282 [65792/118836 (55%)] Loss: 12397.302734\n",
      "Train Epoch: 4282 [98560/118836 (83%)] Loss: 12222.543945\n",
      "    epoch          : 4282\n",
      "    loss           : 12219.242831271971\n",
      "    val_loss       : 12216.237611668514\n",
      "    val_log_likelihood: -12139.395531398624\n",
      "    val_log_marginal: -12147.282692662506\n",
      "Train Epoch: 4283 [256/118836 (0%)] Loss: 12169.121094\n",
      "Train Epoch: 4283 [33024/118836 (28%)] Loss: 12200.772461\n",
      "Train Epoch: 4283 [65792/118836 (55%)] Loss: 12213.881836\n",
      "Train Epoch: 4283 [98560/118836 (83%)] Loss: 12199.134766\n",
      "    epoch          : 4283\n",
      "    loss           : 12218.073975295958\n",
      "    val_loss       : 12218.898867549166\n",
      "    val_log_likelihood: -12140.479080722705\n",
      "    val_log_marginal: -12148.361717835096\n",
      "Train Epoch: 4284 [256/118836 (0%)] Loss: 12260.891602\n",
      "Train Epoch: 4284 [33024/118836 (28%)] Loss: 12195.818359\n",
      "Train Epoch: 4284 [65792/118836 (55%)] Loss: 12230.670898\n",
      "Train Epoch: 4284 [98560/118836 (83%)] Loss: 12218.203125\n",
      "    epoch          : 4284\n",
      "    loss           : 12217.098552684296\n",
      "    val_loss       : 12215.961016255018\n",
      "    val_log_likelihood: -12138.83923358018\n",
      "    val_log_marginal: -12146.720948590752\n",
      "Train Epoch: 4285 [256/118836 (0%)] Loss: 12193.886719\n",
      "Train Epoch: 4285 [33024/118836 (28%)] Loss: 12215.794922\n",
      "Train Epoch: 4285 [65792/118836 (55%)] Loss: 12161.652344\n",
      "Train Epoch: 4285 [98560/118836 (83%)] Loss: 12237.346680\n",
      "    epoch          : 4285\n",
      "    loss           : 12215.75944365824\n",
      "    val_loss       : 12216.031217616197\n",
      "    val_log_likelihood: -12139.762559773057\n",
      "    val_log_marginal: -12147.647615441323\n",
      "Train Epoch: 4286 [256/118836 (0%)] Loss: 12283.681641\n",
      "Train Epoch: 4286 [33024/118836 (28%)] Loss: 12224.665039\n",
      "Train Epoch: 4286 [65792/118836 (55%)] Loss: 12194.143555\n",
      "Train Epoch: 4286 [98560/118836 (83%)] Loss: 12227.062500\n",
      "    epoch          : 4286\n",
      "    loss           : 12218.11313181736\n",
      "    val_loss       : 12219.003718759708\n",
      "    val_log_likelihood: -12137.276016303504\n",
      "    val_log_marginal: -12145.162751025784\n",
      "Train Epoch: 4287 [256/118836 (0%)] Loss: 12216.431641\n",
      "Train Epoch: 4287 [33024/118836 (28%)] Loss: 12325.423828\n",
      "Train Epoch: 4287 [65792/118836 (55%)] Loss: 12169.832031\n",
      "Train Epoch: 4287 [98560/118836 (83%)] Loss: 12355.615234\n",
      "    epoch          : 4287\n",
      "    loss           : 12219.081930443548\n",
      "    val_loss       : 12216.03610247484\n",
      "    val_log_likelihood: -12139.187250568652\n",
      "    val_log_marginal: -12147.071194289018\n",
      "Train Epoch: 4288 [256/118836 (0%)] Loss: 12154.804688\n",
      "Train Epoch: 4288 [33024/118836 (28%)] Loss: 12250.153320\n",
      "Train Epoch: 4288 [65792/118836 (55%)] Loss: 12213.972656\n",
      "Train Epoch: 4288 [98560/118836 (83%)] Loss: 12212.455078\n",
      "    epoch          : 4288\n",
      "    loss           : 12215.84132014449\n",
      "    val_loss       : 12213.58102178969\n",
      "    val_log_likelihood: -12140.581253715623\n",
      "    val_log_marginal: -12148.467143465796\n",
      "Train Epoch: 4289 [256/118836 (0%)] Loss: 12335.293945\n",
      "Train Epoch: 4289 [33024/118836 (28%)] Loss: 12241.257812\n",
      "Train Epoch: 4289 [65792/118836 (55%)] Loss: 12207.691406\n",
      "Train Epoch: 4289 [98560/118836 (83%)] Loss: 12325.566406\n",
      "    epoch          : 4289\n",
      "    loss           : 12218.533459826043\n",
      "    val_loss       : 12223.045886370626\n",
      "    val_log_likelihood: -12139.03089960065\n",
      "    val_log_marginal: -12146.919781445544\n",
      "Train Epoch: 4290 [256/118836 (0%)] Loss: 12423.068359\n",
      "Train Epoch: 4290 [33024/118836 (28%)] Loss: 12287.026367\n",
      "Train Epoch: 4290 [65792/118836 (55%)] Loss: 12328.534180\n",
      "Train Epoch: 4290 [98560/118836 (83%)] Loss: 12232.133789\n",
      "    epoch          : 4290\n",
      "    loss           : 12216.416654550507\n",
      "    val_loss       : 12216.614274389613\n",
      "    val_log_likelihood: -12140.226392066015\n",
      "    val_log_marginal: -12148.117878608778\n",
      "Train Epoch: 4291 [256/118836 (0%)] Loss: 12392.556641\n",
      "Train Epoch: 4291 [33024/118836 (28%)] Loss: 12162.239258\n",
      "Train Epoch: 4291 [65792/118836 (55%)] Loss: 12277.235352\n",
      "Train Epoch: 4291 [98560/118836 (83%)] Loss: 12212.083984\n",
      "    epoch          : 4291\n",
      "    loss           : 12216.435309236715\n",
      "    val_loss       : 12212.667943945316\n",
      "    val_log_likelihood: -12140.35766435975\n",
      "    val_log_marginal: -12148.233788512567\n",
      "Train Epoch: 4292 [256/118836 (0%)] Loss: 12253.828125\n",
      "Train Epoch: 4292 [33024/118836 (28%)] Loss: 12277.638672\n",
      "Train Epoch: 4292 [65792/118836 (55%)] Loss: 12284.173828\n",
      "Train Epoch: 4292 [98560/118836 (83%)] Loss: 12245.182617\n",
      "    epoch          : 4292\n",
      "    loss           : 12214.844395549006\n",
      "    val_loss       : 12214.541108549318\n",
      "    val_log_likelihood: -12140.769798613266\n",
      "    val_log_marginal: -12148.645844363844\n",
      "Train Epoch: 4293 [256/118836 (0%)] Loss: 12236.330078\n",
      "Train Epoch: 4293 [33024/118836 (28%)] Loss: 12272.370117\n",
      "Train Epoch: 4293 [65792/118836 (55%)] Loss: 12268.788086\n",
      "Train Epoch: 4293 [98560/118836 (83%)] Loss: 12285.912109\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 04293: reducing learning rate of group 0 to 1.0000e-06.\n",
      "    epoch          : 4293\n",
      "    loss           : 12215.66252407077\n",
      "    val_loss       : 12215.979194056403\n",
      "    val_log_likelihood: -12139.719844654674\n",
      "    val_log_marginal: -12147.599255502922\n",
      "Train Epoch: 4294 [256/118836 (0%)] Loss: 12234.619141\n",
      "Train Epoch: 4294 [33024/118836 (28%)] Loss: 12252.519531\n",
      "Train Epoch: 4294 [65792/118836 (55%)] Loss: 12280.562500\n",
      "Train Epoch: 4294 [98560/118836 (83%)] Loss: 12222.085938\n",
      "    epoch          : 4294\n",
      "    loss           : 12222.470488588193\n",
      "    val_loss       : 12217.897981049673\n",
      "    val_log_likelihood: -12140.160763188844\n",
      "    val_log_marginal: -12148.039327641663\n",
      "Train Epoch: 4295 [256/118836 (0%)] Loss: 12222.142578\n",
      "Train Epoch: 4295 [33024/118836 (28%)] Loss: 12326.903320\n",
      "Train Epoch: 4295 [65792/118836 (55%)] Loss: 12315.928711\n",
      "Train Epoch: 4295 [98560/118836 (83%)] Loss: 12198.975586\n",
      "    epoch          : 4295\n",
      "    loss           : 12218.965172824906\n",
      "    val_loss       : 12217.487515511764\n",
      "    val_log_likelihood: -12139.461175138285\n",
      "    val_log_marginal: -12147.344963314425\n",
      "Train Epoch: 4296 [256/118836 (0%)] Loss: 12173.979492\n",
      "Train Epoch: 4296 [33024/118836 (28%)] Loss: 12138.567383\n",
      "Train Epoch: 4296 [65792/118836 (55%)] Loss: 12411.814453\n",
      "Train Epoch: 4296 [98560/118836 (83%)] Loss: 12277.302734\n",
      "    epoch          : 4296\n",
      "    loss           : 12217.000590299318\n",
      "    val_loss       : 12218.763141811032\n",
      "    val_log_likelihood: -12139.795622996795\n",
      "    val_log_marginal: -12147.675356990936\n",
      "Train Epoch: 4297 [256/118836 (0%)] Loss: 12269.190430\n",
      "Train Epoch: 4297 [33024/118836 (28%)] Loss: 12228.097656\n",
      "Train Epoch: 4297 [65792/118836 (55%)] Loss: 12340.583984\n",
      "Train Epoch: 4297 [98560/118836 (83%)] Loss: 12308.042969\n",
      "    epoch          : 4297\n",
      "    loss           : 12216.356112845067\n",
      "    val_loss       : 12213.22397905858\n",
      "    val_log_likelihood: -12139.827402230667\n",
      "    val_log_marginal: -12147.700422623575\n",
      "Train Epoch: 4298 [256/118836 (0%)] Loss: 12247.033203\n",
      "Train Epoch: 4298 [33024/118836 (28%)] Loss: 12173.637695\n",
      "Train Epoch: 4298 [65792/118836 (55%)] Loss: 12195.473633\n",
      "Train Epoch: 4298 [98560/118836 (83%)] Loss: 12270.734375\n",
      "    epoch          : 4298\n",
      "    loss           : 12219.496442210762\n",
      "    val_loss       : 12220.036847410007\n",
      "    val_log_likelihood: -12137.29093743538\n",
      "    val_log_marginal: -12145.173875746654\n",
      "Train Epoch: 4299 [256/118836 (0%)] Loss: 12311.158203\n",
      "Train Epoch: 4299 [33024/118836 (28%)] Loss: 12285.773438\n",
      "Train Epoch: 4299 [65792/118836 (55%)] Loss: 12356.405273\n",
      "Train Epoch: 4299 [98560/118836 (83%)] Loss: 12139.236328\n",
      "    epoch          : 4299\n",
      "    loss           : 12217.711479173127\n",
      "    val_loss       : 12213.294007062183\n",
      "    val_log_likelihood: -12139.92781773418\n",
      "    val_log_marginal: -12147.812068014136\n",
      "Train Epoch: 4300 [256/118836 (0%)] Loss: 12283.445312\n",
      "Train Epoch: 4300 [33024/118836 (28%)] Loss: 12178.745117\n",
      "Train Epoch: 4300 [65792/118836 (55%)] Loss: 12209.253906\n",
      "Train Epoch: 4300 [98560/118836 (83%)] Loss: 12261.693359\n",
      "    epoch          : 4300\n",
      "    loss           : 12217.297721354167\n",
      "    val_loss       : 12222.865589500836\n",
      "    val_log_likelihood: -12140.681738362025\n",
      "    val_log_marginal: -12148.5606312257\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch4300.pth ...\n",
      "Train Epoch: 4301 [256/118836 (0%)] Loss: 12201.683594\n",
      "Train Epoch: 4301 [33024/118836 (28%)] Loss: 12305.140625\n",
      "Train Epoch: 4301 [65792/118836 (55%)] Loss: 12182.338867\n",
      "Train Epoch: 4301 [98560/118836 (83%)] Loss: 12212.350586\n",
      "    epoch          : 4301\n",
      "    loss           : 12215.594331898781\n",
      "    val_loss       : 12220.268868350871\n",
      "    val_log_likelihood: -12140.556707183105\n",
      "    val_log_marginal: -12148.430709784636\n",
      "Train Epoch: 4302 [256/118836 (0%)] Loss: 12267.289062\n",
      "Train Epoch: 4302 [33024/118836 (28%)] Loss: 12304.769531\n",
      "Train Epoch: 4302 [65792/118836 (55%)] Loss: 12196.580078\n",
      "Train Epoch: 4302 [98560/118836 (83%)] Loss: 12196.277344\n",
      "    epoch          : 4302\n",
      "    loss           : 12219.63866929022\n",
      "    val_loss       : 12216.73544370661\n",
      "    val_log_likelihood: -12139.792249211641\n",
      "    val_log_marginal: -12147.674515578045\n",
      "Train Epoch: 4303 [256/118836 (0%)] Loss: 12203.492188\n",
      "Train Epoch: 4303 [33024/118836 (28%)] Loss: 12236.458984\n",
      "Train Epoch: 4303 [65792/118836 (55%)] Loss: 12260.654297\n",
      "Train Epoch: 4303 [98560/118836 (83%)] Loss: 12271.825195\n",
      "    epoch          : 4303\n",
      "    loss           : 12217.961271744467\n",
      "    val_loss       : 12216.097570037635\n",
      "    val_log_likelihood: -12139.709292287014\n",
      "    val_log_marginal: -12147.588681646304\n",
      "Train Epoch: 4304 [256/118836 (0%)] Loss: 12306.613281\n",
      "Train Epoch: 4304 [33024/118836 (28%)] Loss: 12222.663086\n",
      "Train Epoch: 4304 [65792/118836 (55%)] Loss: 12206.837891\n",
      "Train Epoch: 4304 [98560/118836 (83%)] Loss: 12269.491211\n",
      "    epoch          : 4304\n",
      "    loss           : 12217.899049608406\n",
      "    val_loss       : 12216.827468053803\n",
      "    val_log_likelihood: -12139.65712397901\n",
      "    val_log_marginal: -12147.537750745132\n",
      "Train Epoch: 4305 [256/118836 (0%)] Loss: 12275.363281\n",
      "Train Epoch: 4305 [33024/118836 (28%)] Loss: 12249.523438\n",
      "Train Epoch: 4305 [65792/118836 (55%)] Loss: 12303.058594\n",
      "Train Epoch: 4305 [98560/118836 (83%)] Loss: 12218.258789\n",
      "    epoch          : 4305\n",
      "    loss           : 12217.530071824596\n",
      "    val_loss       : 12215.09988259179\n",
      "    val_log_likelihood: -12141.400701444893\n",
      "    val_log_marginal: -12149.28037545565\n",
      "Train Epoch: 4306 [256/118836 (0%)] Loss: 12209.238281\n",
      "Train Epoch: 4306 [33024/118836 (28%)] Loss: 12231.279297\n",
      "Train Epoch: 4306 [65792/118836 (55%)] Loss: 12246.787109\n",
      "Train Epoch: 4306 [98560/118836 (83%)] Loss: 12155.291016\n",
      "    epoch          : 4306\n",
      "    loss           : 12218.157467593312\n",
      "    val_loss       : 12217.403278683192\n",
      "    val_log_likelihood: -12141.562423910515\n",
      "    val_log_marginal: -12149.447831221049\n",
      "Train Epoch: 4307 [256/118836 (0%)] Loss: 12191.591797\n",
      "Train Epoch: 4307 [33024/118836 (28%)] Loss: 12302.839844\n",
      "Train Epoch: 4307 [65792/118836 (55%)] Loss: 12276.490234\n",
      "Train Epoch: 4307 [98560/118836 (83%)] Loss: 12290.827148\n",
      "    epoch          : 4307\n",
      "    loss           : 12217.529758096827\n",
      "    val_loss       : 12217.987620760818\n",
      "    val_log_likelihood: -12138.702814018558\n",
      "    val_log_marginal: -12146.584344213592\n",
      "Train Epoch: 4308 [256/118836 (0%)] Loss: 12298.887695\n",
      "Train Epoch: 4308 [33024/118836 (28%)] Loss: 12161.041992\n",
      "Train Epoch: 4308 [65792/118836 (55%)] Loss: 12259.593750\n",
      "Train Epoch: 4308 [98560/118836 (83%)] Loss: 12156.734375\n",
      "    epoch          : 4308\n",
      "    loss           : 12223.469643526416\n",
      "    val_loss       : 12220.855833527303\n",
      "    val_log_likelihood: -12141.075409203113\n",
      "    val_log_marginal: -12148.96072530005\n",
      "Train Epoch: 4309 [256/118836 (0%)] Loss: 12277.580078\n",
      "Train Epoch: 4309 [33024/118836 (28%)] Loss: 12279.445312\n",
      "Train Epoch: 4309 [65792/118836 (55%)] Loss: 12211.277344\n",
      "Train Epoch: 4309 [98560/118836 (83%)] Loss: 12181.193359\n",
      "    epoch          : 4309\n",
      "    loss           : 12217.704855349204\n",
      "    val_loss       : 12218.892094919423\n",
      "    val_log_likelihood: -12139.742825294665\n",
      "    val_log_marginal: -12147.629371920084\n",
      "Train Epoch: 4310 [256/118836 (0%)] Loss: 12195.606445\n",
      "Train Epoch: 4310 [33024/118836 (28%)] Loss: 12110.275391\n",
      "Train Epoch: 4310 [65792/118836 (55%)] Loss: 12236.162109\n",
      "Train Epoch: 4310 [98560/118836 (83%)] Loss: 12252.830078\n",
      "    epoch          : 4310\n",
      "    loss           : 12220.171162731338\n",
      "    val_loss       : 12219.282078873497\n",
      "    val_log_likelihood: -12139.185876434552\n",
      "    val_log_marginal: -12147.067903519475\n",
      "Train Epoch: 4311 [256/118836 (0%)] Loss: 12177.528320\n",
      "Train Epoch: 4311 [33024/118836 (28%)] Loss: 12167.137695\n",
      "Train Epoch: 4311 [65792/118836 (55%)] Loss: 12233.683594\n",
      "Train Epoch: 4311 [98560/118836 (83%)] Loss: 12232.161133\n",
      "    epoch          : 4311\n",
      "    loss           : 12217.391545181968\n",
      "    val_loss       : 12216.479270545366\n",
      "    val_log_likelihood: -12138.286689509669\n",
      "    val_log_marginal: -12146.173923162603\n",
      "Train Epoch: 4312 [256/118836 (0%)] Loss: 12138.724609\n",
      "Train Epoch: 4312 [33024/118836 (28%)] Loss: 12364.125977\n",
      "Train Epoch: 4312 [65792/118836 (55%)] Loss: 12201.903320\n",
      "Train Epoch: 4312 [98560/118836 (83%)] Loss: 12214.874023\n",
      "    epoch          : 4312\n",
      "    loss           : 12218.207470016541\n",
      "    val_loss       : 12213.621882969075\n",
      "    val_log_likelihood: -12138.797223460762\n",
      "    val_log_marginal: -12146.678994351892\n",
      "Train Epoch: 4313 [256/118836 (0%)] Loss: 12197.203125\n",
      "Train Epoch: 4313 [33024/118836 (28%)] Loss: 12208.242188\n",
      "Train Epoch: 4313 [65792/118836 (55%)] Loss: 12222.849609\n",
      "Train Epoch: 4313 [98560/118836 (83%)] Loss: 12201.157227\n",
      "    epoch          : 4313\n",
      "    loss           : 12218.952138906121\n",
      "    val_loss       : 12217.683884855485\n",
      "    val_log_likelihood: -12140.267824002276\n",
      "    val_log_marginal: -12148.147450105755\n",
      "Train Epoch: 4314 [256/118836 (0%)] Loss: 12181.460938\n",
      "Train Epoch: 4314 [33024/118836 (28%)] Loss: 12225.847656\n",
      "Train Epoch: 4314 [65792/118836 (55%)] Loss: 12191.175781\n",
      "Train Epoch: 4314 [98560/118836 (83%)] Loss: 12168.347656\n",
      "    epoch          : 4314\n",
      "    loss           : 12216.124495967742\n",
      "    val_loss       : 12215.596207569159\n",
      "    val_log_likelihood: -12138.405527553763\n",
      "    val_log_marginal: -12146.292655414069\n",
      "Train Epoch: 4315 [256/118836 (0%)] Loss: 12214.667969\n",
      "Train Epoch: 4315 [33024/118836 (28%)] Loss: 12262.311523\n",
      "Train Epoch: 4315 [65792/118836 (55%)] Loss: 12223.308594\n",
      "Train Epoch: 4315 [98560/118836 (83%)] Loss: 12163.740234\n",
      "    epoch          : 4315\n",
      "    loss           : 12216.453617562294\n",
      "    val_loss       : 12211.397946986282\n",
      "    val_log_likelihood: -12138.833204417391\n",
      "    val_log_marginal: -12146.719127578664\n",
      "Train Epoch: 4316 [256/118836 (0%)] Loss: 12283.078125\n",
      "Train Epoch: 4316 [33024/118836 (28%)] Loss: 12216.148438\n",
      "Train Epoch: 4316 [65792/118836 (55%)] Loss: 12211.606445\n",
      "Train Epoch: 4316 [98560/118836 (83%)] Loss: 12289.062500\n",
      "    epoch          : 4316\n",
      "    loss           : 12217.135653012563\n",
      "    val_loss       : 12213.875250879859\n",
      "    val_log_likelihood: -12140.980955496536\n",
      "    val_log_marginal: -12148.861223841766\n",
      "Train Epoch: 4317 [256/118836 (0%)] Loss: 12306.977539\n",
      "Train Epoch: 4317 [33024/118836 (28%)] Loss: 12222.440430\n",
      "Train Epoch: 4317 [65792/118836 (55%)] Loss: 12256.643555\n",
      "Train Epoch: 4317 [98560/118836 (83%)] Loss: 12389.116211\n",
      "    epoch          : 4317\n",
      "    loss           : 12218.805026429383\n",
      "    val_loss       : 12218.499052472054\n",
      "    val_log_likelihood: -12139.005279253257\n",
      "    val_log_marginal: -12146.88237574916\n",
      "Train Epoch: 4318 [256/118836 (0%)] Loss: 12284.291016\n",
      "Train Epoch: 4318 [33024/118836 (28%)] Loss: 12280.561523\n",
      "Train Epoch: 4318 [65792/118836 (55%)] Loss: 12212.023438\n",
      "Train Epoch: 4318 [98560/118836 (83%)] Loss: 12255.087891\n",
      "    epoch          : 4318\n",
      "    loss           : 12215.953200120193\n",
      "    val_loss       : 12216.95661707632\n",
      "    val_log_likelihood: -12138.517251796424\n",
      "    val_log_marginal: -12146.399349093483\n",
      "Train Epoch: 4319 [256/118836 (0%)] Loss: 12187.051758\n",
      "Train Epoch: 4319 [33024/118836 (28%)] Loss: 12291.320312\n",
      "Train Epoch: 4319 [65792/118836 (55%)] Loss: 12279.451172\n",
      "Train Epoch: 4319 [98560/118836 (83%)] Loss: 12235.994141\n",
      "    epoch          : 4319\n",
      "    loss           : 12219.445769036909\n",
      "    val_loss       : 12218.940541120784\n",
      "    val_log_likelihood: -12139.663695622674\n",
      "    val_log_marginal: -12147.548462028473\n",
      "Train Epoch: 4320 [256/118836 (0%)] Loss: 12223.859375\n",
      "Train Epoch: 4320 [33024/118836 (28%)] Loss: 12267.812500\n",
      "Train Epoch: 4320 [65792/118836 (55%)] Loss: 12213.032227\n",
      "Train Epoch: 4320 [98560/118836 (83%)] Loss: 12235.469727\n",
      "    epoch          : 4320\n",
      "    loss           : 12215.686939587209\n",
      "    val_loss       : 12219.996047915482\n",
      "    val_log_likelihood: -12139.425618893454\n",
      "    val_log_marginal: -12147.303399296332\n",
      "Train Epoch: 4321 [256/118836 (0%)] Loss: 12208.093750\n",
      "Train Epoch: 4321 [33024/118836 (28%)] Loss: 12171.572266\n",
      "Train Epoch: 4321 [65792/118836 (55%)] Loss: 12245.962891\n",
      "Train Epoch: 4321 [98560/118836 (83%)] Loss: 12231.403320\n",
      "    epoch          : 4321\n",
      "    loss           : 12216.925126008064\n",
      "    val_loss       : 12219.769798941972\n",
      "    val_log_likelihood: -12138.728110299058\n",
      "    val_log_marginal: -12146.616409540768\n",
      "Train Epoch: 4322 [256/118836 (0%)] Loss: 12161.042969\n",
      "Train Epoch: 4322 [33024/118836 (28%)] Loss: 12149.509766\n",
      "Train Epoch: 4322 [65792/118836 (55%)] Loss: 12256.919922\n",
      "Train Epoch: 4322 [98560/118836 (83%)] Loss: 12259.328125\n",
      "    epoch          : 4322\n",
      "    loss           : 12219.514788338762\n",
      "    val_loss       : 12213.893754363371\n",
      "    val_log_likelihood: -12139.854044374224\n",
      "    val_log_marginal: -12147.744874423091\n",
      "Train Epoch: 4323 [256/118836 (0%)] Loss: 12199.909180\n",
      "Train Epoch: 4323 [33024/118836 (28%)] Loss: 12246.169922\n",
      "Train Epoch: 4323 [65792/118836 (55%)] Loss: 12160.145508\n",
      "Train Epoch: 4323 [98560/118836 (83%)] Loss: 12224.619141\n",
      "    epoch          : 4323\n",
      "    loss           : 12213.934711990798\n",
      "    val_loss       : 12215.987268455465\n",
      "    val_log_likelihood: -12139.55940860215\n",
      "    val_log_marginal: -12147.443907519226\n",
      "Train Epoch: 4324 [256/118836 (0%)] Loss: 12186.203125\n",
      "Train Epoch: 4324 [33024/118836 (28%)] Loss: 12134.670898\n",
      "Train Epoch: 4324 [65792/118836 (55%)] Loss: 12144.296875\n",
      "Train Epoch: 4324 [98560/118836 (83%)] Loss: 12288.770508\n",
      "    epoch          : 4324\n",
      "    loss           : 12221.059142692824\n",
      "    val_loss       : 12216.32140133871\n",
      "    val_log_likelihood: -12139.514900776725\n",
      "    val_log_marginal: -12147.400086970387\n",
      "Train Epoch: 4325 [256/118836 (0%)] Loss: 12156.080078\n",
      "Train Epoch: 4325 [33024/118836 (28%)] Loss: 12197.417969\n",
      "Train Epoch: 4325 [65792/118836 (55%)] Loss: 12215.170898\n",
      "Train Epoch: 4325 [98560/118836 (83%)] Loss: 12232.348633\n",
      "    epoch          : 4325\n",
      "    loss           : 12222.623482087469\n",
      "    val_loss       : 12214.980596766913\n",
      "    val_log_likelihood: -12140.156848376757\n",
      "    val_log_marginal: -12148.04441581136\n",
      "Train Epoch: 4326 [256/118836 (0%)] Loss: 12303.932617\n",
      "Train Epoch: 4326 [33024/118836 (28%)] Loss: 12141.419922\n",
      "Train Epoch: 4326 [65792/118836 (55%)] Loss: 12219.989258\n",
      "Train Epoch: 4326 [98560/118836 (83%)] Loss: 12292.906250\n",
      "    epoch          : 4326\n",
      "    loss           : 12218.585400188687\n",
      "    val_loss       : 12213.832685764894\n",
      "    val_log_likelihood: -12141.559432188275\n",
      "    val_log_marginal: -12149.442354199638\n",
      "Train Epoch: 4327 [256/118836 (0%)] Loss: 12269.960938\n",
      "Train Epoch: 4327 [33024/118836 (28%)] Loss: 12293.790039\n",
      "Train Epoch: 4327 [65792/118836 (55%)] Loss: 12261.605469\n",
      "Train Epoch: 4327 [98560/118836 (83%)] Loss: 12296.603516\n",
      "    epoch          : 4327\n",
      "    loss           : 12221.889708049006\n",
      "    val_loss       : 12220.961576326652\n",
      "    val_log_likelihood: -12140.241966339692\n",
      "    val_log_marginal: -12148.13170560703\n",
      "Train Epoch: 4328 [256/118836 (0%)] Loss: 12150.000000\n",
      "Train Epoch: 4328 [33024/118836 (28%)] Loss: 12195.000000\n",
      "Train Epoch: 4328 [65792/118836 (55%)] Loss: 12217.902344\n",
      "Train Epoch: 4328 [98560/118836 (83%)] Loss: 12227.060547\n",
      "    epoch          : 4328\n",
      "    loss           : 12219.745202969914\n",
      "    val_loss       : 12217.846246435953\n",
      "    val_log_likelihood: -12140.21293117375\n",
      "    val_log_marginal: -12148.09668447556\n",
      "Train Epoch: 4329 [256/118836 (0%)] Loss: 12208.361328\n",
      "Train Epoch: 4329 [33024/118836 (28%)] Loss: 12253.451172\n",
      "Train Epoch: 4329 [65792/118836 (55%)] Loss: 12161.979492\n",
      "Train Epoch: 4329 [98560/118836 (83%)] Loss: 12182.769531\n",
      "    epoch          : 4329\n",
      "    loss           : 12219.22588625672\n",
      "    val_loss       : 12213.637727236293\n",
      "    val_log_likelihood: -12140.77825827776\n",
      "    val_log_marginal: -12148.662004993186\n",
      "Train Epoch: 4330 [256/118836 (0%)] Loss: 12340.604492\n",
      "Train Epoch: 4330 [33024/118836 (28%)] Loss: 12301.754883\n",
      "Train Epoch: 4330 [65792/118836 (55%)] Loss: 12306.228516\n",
      "Train Epoch: 4330 [98560/118836 (83%)] Loss: 12291.900391\n",
      "    epoch          : 4330\n",
      "    loss           : 12217.715955528845\n",
      "    val_loss       : 12218.264599132794\n",
      "    val_log_likelihood: -12139.320372111506\n",
      "    val_log_marginal: -12147.204117488272\n",
      "Train Epoch: 4331 [256/118836 (0%)] Loss: 12290.507812\n",
      "Train Epoch: 4331 [33024/118836 (28%)] Loss: 12257.558594\n",
      "Train Epoch: 4331 [65792/118836 (55%)] Loss: 12287.771484\n",
      "Train Epoch: 4331 [98560/118836 (83%)] Loss: 12137.222656\n",
      "    epoch          : 4331\n",
      "    loss           : 12223.331593291201\n",
      "    val_loss       : 12218.365139220774\n",
      "    val_log_likelihood: -12139.397641225962\n",
      "    val_log_marginal: -12147.286305352598\n",
      "Train Epoch: 4332 [256/118836 (0%)] Loss: 12161.496094\n",
      "Train Epoch: 4332 [33024/118836 (28%)] Loss: 12345.818359\n",
      "Train Epoch: 4332 [65792/118836 (55%)] Loss: 12241.673828\n",
      "Train Epoch: 4332 [98560/118836 (83%)] Loss: 12258.566406\n",
      "    epoch          : 4332\n",
      "    loss           : 12220.742271343826\n",
      "    val_loss       : 12215.760375320851\n",
      "    val_log_likelihood: -12139.162620030758\n",
      "    val_log_marginal: -12147.052275263308\n",
      "Train Epoch: 4333 [256/118836 (0%)] Loss: 12211.863281\n",
      "Train Epoch: 4333 [33024/118836 (28%)] Loss: 12184.813477\n",
      "Train Epoch: 4333 [65792/118836 (55%)] Loss: 12171.648438\n",
      "Train Epoch: 4333 [98560/118836 (83%)] Loss: 12300.284180\n",
      "    epoch          : 4333\n",
      "    loss           : 12218.257758865799\n",
      "    val_loss       : 12217.276319851193\n",
      "    val_log_likelihood: -12140.074909048024\n",
      "    val_log_marginal: -12147.96412532416\n",
      "Train Epoch: 4334 [256/118836 (0%)] Loss: 12208.398438\n",
      "Train Epoch: 4334 [33024/118836 (28%)] Loss: 12192.683594\n",
      "Train Epoch: 4334 [65792/118836 (55%)] Loss: 12199.860352\n",
      "Train Epoch: 4334 [98560/118836 (83%)] Loss: 12279.080078\n",
      "    epoch          : 4334\n",
      "    loss           : 12218.243804765045\n",
      "    val_loss       : 12220.131773866427\n",
      "    val_log_likelihood: -12139.334803750517\n",
      "    val_log_marginal: -12147.227045432563\n",
      "Train Epoch: 4335 [256/118836 (0%)] Loss: 12286.498047\n",
      "Train Epoch: 4335 [33024/118836 (28%)] Loss: 12238.626953\n",
      "Train Epoch: 4335 [65792/118836 (55%)] Loss: 12286.604492\n",
      "Train Epoch: 4335 [98560/118836 (83%)] Loss: 12336.341797\n",
      "    epoch          : 4335\n",
      "    loss           : 12222.068894747725\n",
      "    val_loss       : 12220.257933034034\n",
      "    val_log_likelihood: -12141.733628967639\n",
      "    val_log_marginal: -12149.619044128567\n",
      "Train Epoch: 4336 [256/118836 (0%)] Loss: 12269.006836\n",
      "Train Epoch: 4336 [33024/118836 (28%)] Loss: 12186.441406\n",
      "Train Epoch: 4336 [65792/118836 (55%)] Loss: 12203.958984\n",
      "Train Epoch: 4336 [98560/118836 (83%)] Loss: 12209.348633\n",
      "    epoch          : 4336\n",
      "    loss           : 12214.63520212986\n",
      "    val_loss       : 12212.883294931058\n",
      "    val_log_likelihood: -12139.815753431296\n",
      "    val_log_marginal: -12147.707552050777\n",
      "Train Epoch: 4337 [256/118836 (0%)] Loss: 12254.804688\n",
      "Train Epoch: 4337 [33024/118836 (28%)] Loss: 12207.703125\n",
      "Train Epoch: 4337 [65792/118836 (55%)] Loss: 12157.695312\n",
      "Train Epoch: 4337 [98560/118836 (83%)] Loss: 12284.702148\n",
      "    epoch          : 4337\n",
      "    loss           : 12216.479024019076\n",
      "    val_loss       : 12216.352245895705\n",
      "    val_log_likelihood: -12140.678396886631\n",
      "    val_log_marginal: -12148.566713729255\n",
      "Train Epoch: 4338 [256/118836 (0%)] Loss: 12230.970703\n",
      "Train Epoch: 4338 [33024/118836 (28%)] Loss: 12163.216797\n",
      "Train Epoch: 4338 [65792/118836 (55%)] Loss: 12312.347656\n",
      "Train Epoch: 4338 [98560/118836 (83%)] Loss: 12290.734375\n",
      "    epoch          : 4338\n",
      "    loss           : 12219.580008497467\n",
      "    val_loss       : 12216.217363983833\n",
      "    val_log_likelihood: -12138.479887820511\n",
      "    val_log_marginal: -12146.36455161724\n",
      "Train Epoch: 4339 [256/118836 (0%)] Loss: 12152.820312\n",
      "Train Epoch: 4339 [33024/118836 (28%)] Loss: 12224.510742\n",
      "Train Epoch: 4339 [65792/118836 (55%)] Loss: 12201.320312\n",
      "Train Epoch: 4339 [98560/118836 (83%)] Loss: 12333.326172\n",
      "    epoch          : 4339\n",
      "    loss           : 12216.086975451044\n",
      "    val_loss       : 12217.366834179473\n",
      "    val_log_likelihood: -12138.388527288822\n",
      "    val_log_marginal: -12146.274589519973\n",
      "Train Epoch: 4340 [256/118836 (0%)] Loss: 12306.636719\n",
      "Train Epoch: 4340 [33024/118836 (28%)] Loss: 12287.328125\n",
      "Train Epoch: 4340 [65792/118836 (55%)] Loss: 12258.851562\n",
      "Train Epoch: 4340 [98560/118836 (83%)] Loss: 12273.677734\n",
      "    epoch          : 4340\n",
      "    loss           : 12217.142043560018\n",
      "    val_loss       : 12217.642314862878\n",
      "    val_log_likelihood: -12140.373181122053\n",
      "    val_log_marginal: -12148.254994093897\n",
      "Train Epoch: 4341 [256/118836 (0%)] Loss: 12284.469727\n",
      "Train Epoch: 4341 [33024/118836 (28%)] Loss: 12280.581055\n",
      "Train Epoch: 4341 [65792/118836 (55%)] Loss: 12276.474609\n",
      "Train Epoch: 4341 [98560/118836 (83%)] Loss: 12207.687500\n",
      "    epoch          : 4341\n",
      "    loss           : 12216.922452375413\n",
      "    val_loss       : 12219.184464593407\n",
      "    val_log_likelihood: -12139.105885707455\n",
      "    val_log_marginal: -12146.998804425362\n",
      "Train Epoch: 4342 [256/118836 (0%)] Loss: 12199.068359\n",
      "Train Epoch: 4342 [33024/118836 (28%)] Loss: 12213.218750\n",
      "Train Epoch: 4342 [65792/118836 (55%)] Loss: 12136.701172\n",
      "Train Epoch: 4342 [98560/118836 (83%)] Loss: 12278.125000\n",
      "    epoch          : 4342\n",
      "    loss           : 12219.45188834393\n",
      "    val_loss       : 12217.161128048298\n",
      "    val_log_likelihood: -12141.628135177576\n",
      "    val_log_marginal: -12149.510603621966\n",
      "Train Epoch: 4343 [256/118836 (0%)] Loss: 12238.687500\n",
      "Train Epoch: 4343 [33024/118836 (28%)] Loss: 12242.195312\n",
      "Train Epoch: 4343 [65792/118836 (55%)] Loss: 12216.503906\n",
      "Train Epoch: 4343 [98560/118836 (83%)] Loss: 12348.464844\n",
      "    epoch          : 4343\n",
      "    loss           : 12218.12097388079\n",
      "    val_loss       : 12214.397921883\n",
      "    val_log_likelihood: -12136.908389875412\n",
      "    val_log_marginal: -12144.789032930916\n",
      "Train Epoch: 4344 [256/118836 (0%)] Loss: 12375.221680\n",
      "Train Epoch: 4344 [33024/118836 (28%)] Loss: 12231.682617\n",
      "Train Epoch: 4344 [65792/118836 (55%)] Loss: 12176.262695\n",
      "Train Epoch: 4344 [98560/118836 (83%)] Loss: 12281.390625\n",
      "    epoch          : 4344\n",
      "    loss           : 12215.544389571702\n",
      "    val_loss       : 12222.502649093507\n",
      "    val_log_likelihood: -12139.90063585608\n",
      "    val_log_marginal: -12147.780231622484\n",
      "Train Epoch: 4345 [256/118836 (0%)] Loss: 12217.554688\n",
      "Train Epoch: 4345 [33024/118836 (28%)] Loss: 12181.333008\n",
      "Train Epoch: 4345 [65792/118836 (55%)] Loss: 12218.019531\n",
      "Train Epoch: 4345 [98560/118836 (83%)] Loss: 12182.424805\n",
      "    epoch          : 4345\n",
      "    loss           : 12221.321298593879\n",
      "    val_loss       : 12223.092265983245\n",
      "    val_log_likelihood: -12140.862262523262\n",
      "    val_log_marginal: -12148.750879482612\n",
      "Train Epoch: 4346 [256/118836 (0%)] Loss: 12224.917969\n",
      "Train Epoch: 4346 [33024/118836 (28%)] Loss: 12333.113281\n",
      "Train Epoch: 4346 [65792/118836 (55%)] Loss: 12233.806641\n",
      "Train Epoch: 4346 [98560/118836 (83%)] Loss: 12182.022461\n",
      "    epoch          : 4346\n",
      "    loss           : 12216.013735686774\n",
      "    val_loss       : 12213.326084919505\n",
      "    val_log_likelihood: -12138.959937997572\n",
      "    val_log_marginal: -12146.853914969186\n",
      "Train Epoch: 4347 [256/118836 (0%)] Loss: 12262.967773\n",
      "Train Epoch: 4347 [33024/118836 (28%)] Loss: 12314.273438\n",
      "Train Epoch: 4347 [65792/118836 (55%)] Loss: 12255.300781\n",
      "Train Epoch: 4347 [98560/118836 (83%)] Loss: 12217.958984\n",
      "    epoch          : 4347\n",
      "    loss           : 12219.326401274297\n",
      "    val_loss       : 12217.042167361482\n",
      "    val_log_likelihood: -12139.592952918218\n",
      "    val_log_marginal: -12147.474401641084\n",
      "Train Epoch: 4348 [256/118836 (0%)] Loss: 12235.205078\n",
      "Train Epoch: 4348 [33024/118836 (28%)] Loss: 12250.610352\n",
      "Train Epoch: 4348 [65792/118836 (55%)] Loss: 12337.377930\n",
      "Train Epoch: 4348 [98560/118836 (83%)] Loss: 12259.331055\n",
      "    epoch          : 4348\n",
      "    loss           : 12215.614876544407\n",
      "    val_loss       : 12216.030403479883\n",
      "    val_log_likelihood: -12139.761760914238\n",
      "    val_log_marginal: -12147.646642387901\n",
      "Train Epoch: 4349 [256/118836 (0%)] Loss: 12226.894531\n",
      "Train Epoch: 4349 [33024/118836 (28%)] Loss: 12242.547852\n",
      "Train Epoch: 4349 [65792/118836 (55%)] Loss: 12286.411133\n",
      "Train Epoch: 4349 [98560/118836 (83%)] Loss: 12180.708008\n",
      "    epoch          : 4349\n",
      "    loss           : 12214.90621235913\n",
      "    val_loss       : 12220.597763562193\n",
      "    val_log_likelihood: -12140.342072800351\n",
      "    val_log_marginal: -12148.23169181448\n",
      "Train Epoch: 4350 [256/118836 (0%)] Loss: 12293.619141\n",
      "Train Epoch: 4350 [33024/118836 (28%)] Loss: 12276.348633\n",
      "Train Epoch: 4350 [65792/118836 (55%)] Loss: 12335.229492\n",
      "Train Epoch: 4350 [98560/118836 (83%)] Loss: 12243.286133\n",
      "    epoch          : 4350\n",
      "    loss           : 12219.506978423544\n",
      "    val_loss       : 12210.167278033503\n",
      "    val_log_likelihood: -12139.305364712573\n",
      "    val_log_marginal: -12147.19378265191\n",
      "Train Epoch: 4351 [256/118836 (0%)] Loss: 12286.179688\n",
      "Train Epoch: 4351 [33024/118836 (28%)] Loss: 12166.430664\n",
      "Train Epoch: 4351 [65792/118836 (55%)] Loss: 12244.919922\n",
      "Train Epoch: 4351 [98560/118836 (83%)] Loss: 12259.453125\n",
      "    epoch          : 4351\n",
      "    loss           : 12219.110485486455\n",
      "    val_loss       : 12214.26081167961\n",
      "    val_log_likelihood: -12136.810914883168\n",
      "    val_log_marginal: -12144.698242542567\n",
      "Train Epoch: 4352 [256/118836 (0%)] Loss: 12283.884766\n",
      "Train Epoch: 4352 [33024/118836 (28%)] Loss: 12335.725586\n",
      "Train Epoch: 4352 [65792/118836 (55%)] Loss: 12235.404297\n",
      "Train Epoch: 4352 [98560/118836 (83%)] Loss: 12155.334961\n",
      "    epoch          : 4352\n",
      "    loss           : 12216.896643823666\n",
      "    val_loss       : 12220.069560036161\n",
      "    val_log_likelihood: -12138.324949919872\n",
      "    val_log_marginal: -12146.207638765889\n",
      "Train Epoch: 4353 [256/118836 (0%)] Loss: 12243.355469\n",
      "Train Epoch: 4353 [33024/118836 (28%)] Loss: 12239.200195\n",
      "Train Epoch: 4353 [65792/118836 (55%)] Loss: 12256.452148\n",
      "Train Epoch: 4353 [98560/118836 (83%)] Loss: 12203.705078\n",
      "    epoch          : 4353\n",
      "    loss           : 12219.982769043374\n",
      "    val_loss       : 12214.326363362712\n",
      "    val_log_likelihood: -12141.272049149606\n",
      "    val_log_marginal: -12149.157796320254\n",
      "Train Epoch: 4354 [256/118836 (0%)] Loss: 12266.888672\n",
      "Train Epoch: 4354 [33024/118836 (28%)] Loss: 12163.199219\n",
      "Train Epoch: 4354 [65792/118836 (55%)] Loss: 12306.074219\n",
      "Train Epoch: 4354 [98560/118836 (83%)] Loss: 12197.781250\n",
      "    epoch          : 4354\n",
      "    loss           : 12220.744782619933\n",
      "    val_loss       : 12219.12881883595\n",
      "    val_log_likelihood: -12140.016581368898\n",
      "    val_log_marginal: -12147.900562759118\n",
      "Train Epoch: 4355 [256/118836 (0%)] Loss: 12156.675781\n",
      "Train Epoch: 4355 [33024/118836 (28%)] Loss: 12326.769531\n",
      "Train Epoch: 4355 [65792/118836 (55%)] Loss: 12269.419922\n",
      "Train Epoch: 4355 [98560/118836 (83%)] Loss: 12201.575195\n",
      "    epoch          : 4355\n",
      "    loss           : 12218.124104696546\n",
      "    val_loss       : 12214.92651556871\n",
      "    val_log_likelihood: -12140.140344389732\n",
      "    val_log_marginal: -12148.03084108152\n",
      "Train Epoch: 4356 [256/118836 (0%)] Loss: 12276.814453\n",
      "Train Epoch: 4356 [33024/118836 (28%)] Loss: 12188.537109\n",
      "Train Epoch: 4356 [65792/118836 (55%)] Loss: 12335.759766\n",
      "Train Epoch: 4356 [98560/118836 (83%)] Loss: 12246.536133\n",
      "    epoch          : 4356\n",
      "    loss           : 12218.96581223506\n",
      "    val_loss       : 12222.940458633968\n",
      "    val_log_likelihood: -12140.309007153382\n",
      "    val_log_marginal: -12148.193624399719\n",
      "Train Epoch: 4357 [256/118836 (0%)] Loss: 12218.223633\n",
      "Train Epoch: 4357 [33024/118836 (28%)] Loss: 12109.974609\n",
      "Train Epoch: 4357 [65792/118836 (55%)] Loss: 12259.208008\n",
      "Train Epoch: 4357 [98560/118836 (83%)] Loss: 12214.615234\n",
      "    epoch          : 4357\n",
      "    loss           : 12220.718001221308\n",
      "    val_loss       : 12217.97755905972\n",
      "    val_log_likelihood: -12139.187931496846\n",
      "    val_log_marginal: -12147.078137010853\n",
      "Train Epoch: 4358 [256/118836 (0%)] Loss: 12184.682617\n",
      "Train Epoch: 4358 [33024/118836 (28%)] Loss: 12239.281250\n",
      "Train Epoch: 4358 [65792/118836 (55%)] Loss: 12349.919922\n",
      "Train Epoch: 4358 [98560/118836 (83%)] Loss: 12236.334961\n",
      "    epoch          : 4358\n",
      "    loss           : 12219.468308971775\n",
      "    val_loss       : 12214.26443099061\n",
      "    val_log_likelihood: -12139.3085129756\n",
      "    val_log_marginal: -12147.195523860533\n",
      "Train Epoch: 4359 [256/118836 (0%)] Loss: 12193.570312\n",
      "Train Epoch: 4359 [33024/118836 (28%)] Loss: 12227.437500\n",
      "Train Epoch: 4359 [65792/118836 (55%)] Loss: 12185.421875\n",
      "Train Epoch: 4359 [98560/118836 (83%)] Loss: 12253.279297\n",
      "    epoch          : 4359\n",
      "    loss           : 12213.20173341863\n",
      "    val_loss       : 12221.306368140482\n",
      "    val_log_likelihood: -12139.545333662894\n",
      "    val_log_marginal: -12147.429610278039\n",
      "Train Epoch: 4360 [256/118836 (0%)] Loss: 12213.247070\n",
      "Train Epoch: 4360 [33024/118836 (28%)] Loss: 12218.299805\n",
      "Train Epoch: 4360 [65792/118836 (55%)] Loss: 12186.159180\n",
      "Train Epoch: 4360 [98560/118836 (83%)] Loss: 12221.109375\n",
      "    epoch          : 4360\n",
      "    loss           : 12217.093627707558\n",
      "    val_loss       : 12216.677882098493\n",
      "    val_log_likelihood: -12139.60124182563\n",
      "    val_log_marginal: -12147.488946369202\n",
      "Train Epoch: 4361 [256/118836 (0%)] Loss: 12282.873047\n",
      "Train Epoch: 4361 [33024/118836 (28%)] Loss: 12114.435547\n",
      "Train Epoch: 4361 [65792/118836 (55%)] Loss: 12294.207031\n",
      "Train Epoch: 4361 [98560/118836 (83%)] Loss: 12268.429688\n",
      "    epoch          : 4361\n",
      "    loss           : 12214.547528464898\n",
      "    val_loss       : 12221.264848698864\n",
      "    val_log_likelihood: -12137.982130279413\n",
      "    val_log_marginal: -12145.869501697707\n",
      "Train Epoch: 4362 [256/118836 (0%)] Loss: 12222.480469\n",
      "Train Epoch: 4362 [33024/118836 (28%)] Loss: 12254.262695\n",
      "Train Epoch: 4362 [65792/118836 (55%)] Loss: 12291.331055\n",
      "Train Epoch: 4362 [98560/118836 (83%)] Loss: 12272.036133\n",
      "    epoch          : 4362\n",
      "    loss           : 12219.962787556864\n",
      "    val_loss       : 12216.477861114106\n",
      "    val_log_likelihood: -12138.698872066274\n",
      "    val_log_marginal: -12146.583159400936\n",
      "Train Epoch: 4363 [256/118836 (0%)] Loss: 12260.824219\n",
      "Train Epoch: 4363 [33024/118836 (28%)] Loss: 12304.445312\n",
      "Train Epoch: 4363 [65792/118836 (55%)] Loss: 12245.386719\n",
      "Train Epoch: 4363 [98560/118836 (83%)] Loss: 12277.668945\n",
      "    epoch          : 4363\n",
      "    loss           : 12216.74683170492\n",
      "    val_loss       : 12219.821902084625\n",
      "    val_log_likelihood: -12139.527126628413\n",
      "    val_log_marginal: -12147.410845169003\n",
      "Train Epoch: 4364 [256/118836 (0%)] Loss: 12311.877930\n",
      "Train Epoch: 4364 [33024/118836 (28%)] Loss: 12196.722656\n",
      "Train Epoch: 4364 [65792/118836 (55%)] Loss: 12237.958008\n",
      "Train Epoch: 4364 [98560/118836 (83%)] Loss: 12187.423828\n",
      "    epoch          : 4364\n",
      "    loss           : 12219.519217037581\n",
      "    val_loss       : 12216.720809812063\n",
      "    val_log_likelihood: -12139.680567617866\n",
      "    val_log_marginal: -12147.567191548464\n",
      "Train Epoch: 4365 [256/118836 (0%)] Loss: 12246.532227\n",
      "Train Epoch: 4365 [33024/118836 (28%)] Loss: 12235.513672\n",
      "Train Epoch: 4365 [65792/118836 (55%)] Loss: 12235.580078\n",
      "Train Epoch: 4365 [98560/118836 (83%)] Loss: 12255.084961\n",
      "    epoch          : 4365\n",
      "    loss           : 12218.437600160256\n",
      "    val_loss       : 12220.455291410317\n",
      "    val_log_likelihood: -12138.568012368176\n",
      "    val_log_marginal: -12146.451933476019\n",
      "Train Epoch: 4366 [256/118836 (0%)] Loss: 12297.281250\n",
      "Train Epoch: 4366 [33024/118836 (28%)] Loss: 12194.009766\n",
      "Train Epoch: 4366 [65792/118836 (55%)] Loss: 12312.941406\n",
      "Train Epoch: 4366 [98560/118836 (83%)] Loss: 12131.977539\n",
      "    epoch          : 4366\n",
      "    loss           : 12214.866011747828\n",
      "    val_loss       : 12214.693278790266\n",
      "    val_log_likelihood: -12140.497857539807\n",
      "    val_log_marginal: -12148.384359502152\n",
      "Train Epoch: 4367 [256/118836 (0%)] Loss: 12371.786133\n",
      "Train Epoch: 4367 [33024/118836 (28%)] Loss: 12211.742188\n",
      "Train Epoch: 4367 [65792/118836 (55%)] Loss: 12251.752930\n",
      "Train Epoch: 4367 [98560/118836 (83%)] Loss: 12273.180664\n",
      "    epoch          : 4367\n",
      "    loss           : 12216.028608192464\n",
      "    val_loss       : 12217.601759055462\n",
      "    val_log_likelihood: -12140.683473234596\n",
      "    val_log_marginal: -12148.569409498901\n",
      "Train Epoch: 4368 [256/118836 (0%)] Loss: 12242.351562\n",
      "Train Epoch: 4368 [33024/118836 (28%)] Loss: 12258.583008\n",
      "Train Epoch: 4368 [65792/118836 (55%)] Loss: 12279.492188\n",
      "Train Epoch: 4368 [98560/118836 (83%)] Loss: 12284.409180\n",
      "    epoch          : 4368\n",
      "    loss           : 12214.643053724669\n",
      "    val_loss       : 12218.255916469996\n",
      "    val_log_likelihood: -12141.986237657671\n",
      "    val_log_marginal: -12149.880300592802\n",
      "Train Epoch: 4369 [256/118836 (0%)] Loss: 12199.188477\n",
      "Train Epoch: 4369 [33024/118836 (28%)] Loss: 12164.783203\n",
      "Train Epoch: 4369 [65792/118836 (55%)] Loss: 12275.473633\n",
      "Train Epoch: 4369 [98560/118836 (83%)] Loss: 12324.208008\n",
      "    epoch          : 4369\n",
      "    loss           : 12222.82149649116\n",
      "    val_loss       : 12215.092918669812\n",
      "    val_log_likelihood: -12139.507527689464\n",
      "    val_log_marginal: -12147.39784074059\n",
      "Train Epoch: 4370 [256/118836 (0%)] Loss: 12306.902344\n",
      "Train Epoch: 4370 [33024/118836 (28%)] Loss: 12218.340820\n",
      "Train Epoch: 4370 [65792/118836 (55%)] Loss: 12192.380859\n",
      "Train Epoch: 4370 [98560/118836 (83%)] Loss: 12343.605469\n",
      "    epoch          : 4370\n",
      "    loss           : 12215.441611255428\n",
      "    val_loss       : 12221.022704908286\n",
      "    val_log_likelihood: -12140.282845455955\n",
      "    val_log_marginal: -12148.167182799965\n",
      "Train Epoch: 4371 [256/118836 (0%)] Loss: 12117.885742\n",
      "Train Epoch: 4371 [33024/118836 (28%)] Loss: 12291.187500\n",
      "Train Epoch: 4371 [65792/118836 (55%)] Loss: 12163.653320\n",
      "Train Epoch: 4371 [98560/118836 (83%)] Loss: 12202.945312\n",
      "    epoch          : 4371\n",
      "    loss           : 12216.459479198977\n",
      "    val_loss       : 12221.867610369676\n",
      "    val_log_likelihood: -12139.510487909687\n",
      "    val_log_marginal: -12147.403614676321\n",
      "Train Epoch: 4372 [256/118836 (0%)] Loss: 12273.934570\n",
      "Train Epoch: 4372 [33024/118836 (28%)] Loss: 12196.722656\n",
      "Train Epoch: 4372 [65792/118836 (55%)] Loss: 12131.250977\n",
      "Train Epoch: 4372 [98560/118836 (83%)] Loss: 12254.691406\n",
      "    epoch          : 4372\n",
      "    loss           : 12222.961947503101\n",
      "    val_loss       : 12217.936823385486\n",
      "    val_log_likelihood: -12139.301716617554\n",
      "    val_log_marginal: -12147.186394964623\n",
      "Train Epoch: 4373 [256/118836 (0%)] Loss: 12322.722656\n",
      "Train Epoch: 4373 [33024/118836 (28%)] Loss: 12227.774414\n",
      "Train Epoch: 4373 [65792/118836 (55%)] Loss: 12147.961914\n",
      "Train Epoch: 4373 [98560/118836 (83%)] Loss: 12204.902344\n",
      "    epoch          : 4373\n",
      "    loss           : 12219.114360880636\n",
      "    val_loss       : 12213.370857923233\n",
      "    val_log_likelihood: -12139.32292280552\n",
      "    val_log_marginal: -12147.209726030622\n",
      "Train Epoch: 4374 [256/118836 (0%)] Loss: 12245.372070\n",
      "Train Epoch: 4374 [33024/118836 (28%)] Loss: 12306.019531\n",
      "Train Epoch: 4374 [65792/118836 (55%)] Loss: 12180.954102\n",
      "Train Epoch: 4374 [98560/118836 (83%)] Loss: 12330.150391\n",
      "    epoch          : 4374\n",
      "    loss           : 12214.670888744571\n",
      "    val_loss       : 12220.05225312736\n",
      "    val_log_likelihood: -12137.346166446961\n",
      "    val_log_marginal: -12145.237104829248\n",
      "Train Epoch: 4375 [256/118836 (0%)] Loss: 12261.999023\n",
      "Train Epoch: 4375 [33024/118836 (28%)] Loss: 12242.350586\n",
      "Train Epoch: 4375 [65792/118836 (55%)] Loss: 12222.437500\n",
      "Train Epoch: 4375 [98560/118836 (83%)] Loss: 12270.154297\n",
      "    epoch          : 4375\n",
      "    loss           : 12215.94102111766\n",
      "    val_loss       : 12217.207339995904\n",
      "    val_log_likelihood: -12140.222929752119\n",
      "    val_log_marginal: -12148.10773680721\n",
      "Train Epoch: 4376 [256/118836 (0%)] Loss: 12191.580078\n",
      "Train Epoch: 4376 [33024/118836 (28%)] Loss: 12223.952148\n",
      "Train Epoch: 4376 [65792/118836 (55%)] Loss: 12249.981445\n",
      "Train Epoch: 4376 [98560/118836 (83%)] Loss: 12203.517578\n",
      "    epoch          : 4376\n",
      "    loss           : 12218.786748959625\n",
      "    val_loss       : 12218.379089252267\n",
      "    val_log_likelihood: -12138.405780054538\n",
      "    val_log_marginal: -12146.290699924113\n",
      "Train Epoch: 4377 [256/118836 (0%)] Loss: 12282.245117\n",
      "Train Epoch: 4377 [33024/118836 (28%)] Loss: 12375.527344\n",
      "Train Epoch: 4377 [65792/118836 (55%)] Loss: 12224.994141\n",
      "Train Epoch: 4377 [98560/118836 (83%)] Loss: 12307.905273\n",
      "    epoch          : 4377\n",
      "    loss           : 12215.737338774297\n",
      "    val_loss       : 12216.247611818908\n",
      "    val_log_likelihood: -12138.698614557487\n",
      "    val_log_marginal: -12146.57929976796\n",
      "Train Epoch: 4378 [256/118836 (0%)] Loss: 12257.591797\n",
      "Train Epoch: 4378 [33024/118836 (28%)] Loss: 12192.048828\n",
      "Train Epoch: 4378 [65792/118836 (55%)] Loss: 12221.344727\n",
      "Train Epoch: 4378 [98560/118836 (83%)] Loss: 12348.362305\n",
      "    epoch          : 4378\n",
      "    loss           : 12217.859146408446\n",
      "    val_loss       : 12220.885019576312\n",
      "    val_log_likelihood: -12140.47638721955\n",
      "    val_log_marginal: -12148.358551656938\n",
      "Train Epoch: 4379 [256/118836 (0%)] Loss: 12198.148438\n",
      "Train Epoch: 4379 [33024/118836 (28%)] Loss: 12173.797852\n",
      "Train Epoch: 4379 [65792/118836 (55%)] Loss: 12225.889648\n",
      "Train Epoch: 4379 [98560/118836 (83%)] Loss: 12215.066406\n",
      "    epoch          : 4379\n",
      "    loss           : 12215.396309740747\n",
      "    val_loss       : 12219.640193454383\n",
      "    val_log_likelihood: -12139.509441558106\n",
      "    val_log_marginal: -12147.39798782742\n",
      "Train Epoch: 4380 [256/118836 (0%)] Loss: 12286.872070\n",
      "Train Epoch: 4380 [33024/118836 (28%)] Loss: 12188.164062\n",
      "Train Epoch: 4380 [65792/118836 (55%)] Loss: 12194.920898\n",
      "Train Epoch: 4380 [98560/118836 (83%)] Loss: 12197.620117\n",
      "    epoch          : 4380\n",
      "    loss           : 12216.289586241212\n",
      "    val_loss       : 12220.025457070684\n",
      "    val_log_likelihood: -12138.96063847317\n",
      "    val_log_marginal: -12146.847740994957\n",
      "Train Epoch: 4381 [256/118836 (0%)] Loss: 12183.889648\n",
      "Train Epoch: 4381 [33024/118836 (28%)] Loss: 12165.683594\n",
      "Train Epoch: 4381 [65792/118836 (55%)] Loss: 12224.483398\n",
      "Train Epoch: 4381 [98560/118836 (83%)] Loss: 12195.691406\n",
      "    epoch          : 4381\n",
      "    loss           : 12216.238508387612\n",
      "    val_loss       : 12217.324659012593\n",
      "    val_log_likelihood: -12139.674507437707\n",
      "    val_log_marginal: -12147.562404622267\n",
      "Train Epoch: 4382 [256/118836 (0%)] Loss: 12203.897461\n",
      "Train Epoch: 4382 [33024/118836 (28%)] Loss: 12258.791016\n",
      "Train Epoch: 4382 [65792/118836 (55%)] Loss: 12228.343750\n",
      "Train Epoch: 4382 [98560/118836 (83%)] Loss: 12160.501953\n",
      "    epoch          : 4382\n",
      "    loss           : 12214.41194524142\n",
      "    val_loss       : 12222.261158520176\n",
      "    val_log_likelihood: -12140.512405332402\n",
      "    val_log_marginal: -12148.396925318013\n",
      "Train Epoch: 4383 [256/118836 (0%)] Loss: 12182.826172\n",
      "Train Epoch: 4383 [33024/118836 (28%)] Loss: 12262.744141\n",
      "Train Epoch: 4383 [65792/118836 (55%)] Loss: 12392.363281\n",
      "Train Epoch: 4383 [98560/118836 (83%)] Loss: 12196.314453\n",
      "    epoch          : 4383\n",
      "    loss           : 12215.920963380117\n",
      "    val_loss       : 12214.755699284178\n",
      "    val_log_likelihood: -12142.430061647021\n",
      "    val_log_marginal: -12150.315420454235\n",
      "Train Epoch: 4384 [256/118836 (0%)] Loss: 12237.571289\n",
      "Train Epoch: 4384 [33024/118836 (28%)] Loss: 12372.765625\n",
      "Train Epoch: 4384 [65792/118836 (55%)] Loss: 12328.807617\n",
      "Train Epoch: 4384 [98560/118836 (83%)] Loss: 12241.546875\n",
      "    epoch          : 4384\n",
      "    loss           : 12220.772042526107\n",
      "    val_loss       : 12219.555907358032\n",
      "    val_log_likelihood: -12139.595836079663\n",
      "    val_log_marginal: -12147.486622764873\n",
      "Train Epoch: 4385 [256/118836 (0%)] Loss: 12332.230469\n",
      "Train Epoch: 4385 [33024/118836 (28%)] Loss: 12144.461914\n",
      "Train Epoch: 4385 [65792/118836 (55%)] Loss: 12292.694336\n",
      "Train Epoch: 4385 [98560/118836 (83%)] Loss: 12205.035156\n",
      "    epoch          : 4385\n",
      "    loss           : 12213.950927936312\n",
      "    val_loss       : 12219.931048843971\n",
      "    val_log_likelihood: -12138.596419270832\n",
      "    val_log_marginal: -12146.481285717466\n",
      "Train Epoch: 4386 [256/118836 (0%)] Loss: 12306.693359\n",
      "Train Epoch: 4386 [33024/118836 (28%)] Loss: 12258.175781\n",
      "Train Epoch: 4386 [65792/118836 (55%)] Loss: 12196.464844\n",
      "Train Epoch: 4386 [98560/118836 (83%)] Loss: 12211.367188\n",
      "    epoch          : 4386\n",
      "    loss           : 12220.514365727098\n",
      "    val_loss       : 12214.761233722664\n",
      "    val_log_likelihood: -12139.73012125853\n",
      "    val_log_marginal: -12147.61583092445\n",
      "Train Epoch: 4387 [256/118836 (0%)] Loss: 12173.439453\n",
      "Train Epoch: 4387 [33024/118836 (28%)] Loss: 12200.913086\n",
      "Train Epoch: 4387 [65792/118836 (55%)] Loss: 12180.128906\n",
      "Train Epoch: 4387 [98560/118836 (83%)] Loss: 12370.318359\n",
      "    epoch          : 4387\n",
      "    loss           : 12219.257301521144\n",
      "    val_loss       : 12214.87173108947\n",
      "    val_log_likelihood: -12139.064468472137\n",
      "    val_log_marginal: -12146.955283605635\n",
      "Train Epoch: 4388 [256/118836 (0%)] Loss: 12237.186523\n",
      "Train Epoch: 4388 [33024/118836 (28%)] Loss: 12171.644531\n",
      "Train Epoch: 4388 [65792/118836 (55%)] Loss: 12188.056641\n",
      "Train Epoch: 4388 [98560/118836 (83%)] Loss: 12178.625000\n",
      "    epoch          : 4388\n",
      "    loss           : 12216.312199196132\n",
      "    val_loss       : 12219.89481056134\n",
      "    val_log_likelihood: -12139.720924931504\n",
      "    val_log_marginal: -12147.61683310809\n",
      "Train Epoch: 4389 [256/118836 (0%)] Loss: 12189.871094\n",
      "Train Epoch: 4389 [33024/118836 (28%)] Loss: 12284.686523\n",
      "Train Epoch: 4389 [65792/118836 (55%)] Loss: 12184.732422\n",
      "Train Epoch: 4389 [98560/118836 (83%)] Loss: 12176.376953\n",
      "    epoch          : 4389\n",
      "    loss           : 12215.701885436052\n",
      "    val_loss       : 12216.16380124362\n",
      "    val_log_likelihood: -12138.897247208437\n",
      "    val_log_marginal: -12146.784185535837\n",
      "Train Epoch: 4390 [256/118836 (0%)] Loss: 12304.038086\n",
      "Train Epoch: 4390 [33024/118836 (28%)] Loss: 12249.770508\n",
      "Train Epoch: 4390 [65792/118836 (55%)] Loss: 12249.111328\n",
      "Train Epoch: 4390 [98560/118836 (83%)] Loss: 12236.636719\n",
      "    epoch          : 4390\n",
      "    loss           : 12219.537791434037\n",
      "    val_loss       : 12220.512038748115\n",
      "    val_log_likelihood: -12141.168820758374\n",
      "    val_log_marginal: -12149.05465619285\n",
      "Train Epoch: 4391 [256/118836 (0%)] Loss: 12205.867188\n",
      "Train Epoch: 4391 [33024/118836 (28%)] Loss: 12206.368164\n",
      "Train Epoch: 4391 [65792/118836 (55%)] Loss: 12255.541992\n",
      "Train Epoch: 4391 [98560/118836 (83%)] Loss: 12248.606445\n",
      "    epoch          : 4391\n",
      "    loss           : 12222.112375445875\n",
      "    val_loss       : 12216.453417605751\n",
      "    val_log_likelihood: -12140.914007734957\n",
      "    val_log_marginal: -12148.80741114338\n",
      "Train Epoch: 4392 [256/118836 (0%)] Loss: 12157.503906\n",
      "Train Epoch: 4392 [33024/118836 (28%)] Loss: 12262.530273\n",
      "Train Epoch: 4392 [65792/118836 (55%)] Loss: 12294.715820\n",
      "Train Epoch: 4392 [98560/118836 (83%)] Loss: 12200.291016\n",
      "    epoch          : 4392\n",
      "    loss           : 12216.81681819944\n",
      "    val_loss       : 12218.537817941382\n",
      "    val_log_likelihood: -12139.17777750853\n",
      "    val_log_marginal: -12147.071965285215\n",
      "Train Epoch: 4393 [256/118836 (0%)] Loss: 12220.507812\n",
      "Train Epoch: 4393 [33024/118836 (28%)] Loss: 12191.726562\n",
      "Train Epoch: 4393 [65792/118836 (55%)] Loss: 12295.280273\n",
      "Train Epoch: 4393 [98560/118836 (83%)] Loss: 12211.137695\n",
      "    epoch          : 4393\n",
      "    loss           : 12219.747670950941\n",
      "    val_loss       : 12215.061307073505\n",
      "    val_log_likelihood: -12143.38102657801\n",
      "    val_log_marginal: -12151.266842360053\n",
      "Train Epoch: 4394 [256/118836 (0%)] Loss: 12190.928711\n",
      "Train Epoch: 4394 [33024/118836 (28%)] Loss: 12262.632812\n",
      "Train Epoch: 4394 [65792/118836 (55%)] Loss: 12202.224609\n",
      "Train Epoch: 4394 [98560/118836 (83%)] Loss: 12258.220703\n",
      "    epoch          : 4394\n",
      "    loss           : 12216.504552929848\n",
      "    val_loss       : 12218.128928402695\n",
      "    val_log_likelihood: -12140.001433745605\n",
      "    val_log_marginal: -12147.888431581214\n",
      "Train Epoch: 4395 [256/118836 (0%)] Loss: 12189.860352\n",
      "Train Epoch: 4395 [33024/118836 (28%)] Loss: 12304.873047\n",
      "Train Epoch: 4395 [65792/118836 (55%)] Loss: 12163.459961\n",
      "Train Epoch: 4395 [98560/118836 (83%)] Loss: 12322.085938\n",
      "    epoch          : 4395\n",
      "    loss           : 12214.99592428531\n",
      "    val_loss       : 12216.633172531774\n",
      "    val_log_likelihood: -12138.331596683727\n",
      "    val_log_marginal: -12146.217184138439\n",
      "Train Epoch: 4396 [256/118836 (0%)] Loss: 12402.021484\n",
      "Train Epoch: 4396 [33024/118836 (28%)] Loss: 12246.420898\n",
      "Train Epoch: 4396 [65792/118836 (55%)] Loss: 12285.234375\n",
      "Train Epoch: 4396 [98560/118836 (83%)] Loss: 12269.042969\n",
      "    epoch          : 4396\n",
      "    loss           : 12216.278419018818\n",
      "    val_loss       : 12219.430323849761\n",
      "    val_log_likelihood: -12139.319088929384\n",
      "    val_log_marginal: -12147.20494647252\n",
      "Train Epoch: 4397 [256/118836 (0%)] Loss: 12444.115234\n",
      "Train Epoch: 4397 [33024/118836 (28%)] Loss: 12248.929688\n",
      "Train Epoch: 4397 [65792/118836 (55%)] Loss: 12219.324219\n",
      "Train Epoch: 4397 [98560/118836 (83%)] Loss: 12285.298828\n",
      "    epoch          : 4397\n",
      "    loss           : 12218.898255111404\n",
      "    val_loss       : 12217.23066651719\n",
      "    val_log_likelihood: -12137.62348321831\n",
      "    val_log_marginal: -12145.51612184094\n",
      "Train Epoch: 4398 [256/118836 (0%)] Loss: 12204.888672\n",
      "Train Epoch: 4398 [33024/118836 (28%)] Loss: 12273.103516\n",
      "Train Epoch: 4398 [65792/118836 (55%)] Loss: 12290.196289\n",
      "Train Epoch: 4398 [98560/118836 (83%)] Loss: 12231.941406\n",
      "    epoch          : 4398\n",
      "    loss           : 12218.096588089329\n",
      "    val_loss       : 12219.721756620711\n",
      "    val_log_likelihood: -12140.553804151157\n",
      "    val_log_marginal: -12148.443656562042\n",
      "Train Epoch: 4399 [256/118836 (0%)] Loss: 12203.005859\n",
      "Train Epoch: 4399 [33024/118836 (28%)] Loss: 12205.869141\n",
      "Train Epoch: 4399 [65792/118836 (55%)] Loss: 12139.451172\n",
      "Train Epoch: 4399 [98560/118836 (83%)] Loss: 12237.517578\n",
      "    epoch          : 4399\n",
      "    loss           : 12215.689906430935\n",
      "    val_loss       : 12220.30028356017\n",
      "    val_log_likelihood: -12139.46846842044\n",
      "    val_log_marginal: -12147.356144861793\n",
      "Train Epoch: 4400 [256/118836 (0%)] Loss: 12200.901367\n",
      "Train Epoch: 4400 [33024/118836 (28%)] Loss: 12176.517578\n",
      "Train Epoch: 4400 [65792/118836 (55%)] Loss: 12197.386719\n",
      "Train Epoch: 4400 [98560/118836 (83%)] Loss: 12188.148438\n",
      "    epoch          : 4400\n",
      "    loss           : 12215.402987198871\n",
      "    val_loss       : 12218.084826749477\n",
      "    val_log_likelihood: -12139.887226820978\n",
      "    val_log_marginal: -12147.779260833215\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch4400.pth ...\n",
      "Train Epoch: 4401 [256/118836 (0%)] Loss: 12248.601562\n",
      "Train Epoch: 4401 [33024/118836 (28%)] Loss: 12206.958984\n",
      "Train Epoch: 4401 [65792/118836 (55%)] Loss: 12271.366211\n",
      "Train Epoch: 4401 [98560/118836 (83%)] Loss: 12195.415039\n",
      "    epoch          : 4401\n",
      "    loss           : 12219.471006998294\n",
      "    val_loss       : 12215.127297190043\n",
      "    val_log_likelihood: -12139.481619946753\n",
      "    val_log_marginal: -12147.370391519487\n",
      "Train Epoch: 4402 [256/118836 (0%)] Loss: 12234.495117\n",
      "Train Epoch: 4402 [33024/118836 (28%)] Loss: 12239.957031\n",
      "Train Epoch: 4402 [65792/118836 (55%)] Loss: 12197.935547\n",
      "Train Epoch: 4402 [98560/118836 (83%)] Loss: 12210.526367\n",
      "    epoch          : 4402\n",
      "    loss           : 12219.390060871587\n",
      "    val_loss       : 12219.693696478747\n",
      "    val_log_likelihood: -12139.890688488678\n",
      "    val_log_marginal: -12147.780889409416\n",
      "Train Epoch: 4403 [256/118836 (0%)] Loss: 12283.620117\n",
      "Train Epoch: 4403 [33024/118836 (28%)] Loss: 12257.858398\n",
      "Train Epoch: 4403 [65792/118836 (55%)] Loss: 12229.581055\n",
      "Train Epoch: 4403 [98560/118836 (83%)] Loss: 12370.398438\n",
      "    epoch          : 4403\n",
      "    loss           : 12220.295896822012\n",
      "    val_loss       : 12218.210485855956\n",
      "    val_log_likelihood: -12140.38988009848\n",
      "    val_log_marginal: -12148.28159462019\n",
      "Train Epoch: 4404 [256/118836 (0%)] Loss: 12366.198242\n",
      "Train Epoch: 4404 [33024/118836 (28%)] Loss: 12339.419922\n",
      "Train Epoch: 4404 [65792/118836 (55%)] Loss: 12182.264648\n",
      "Train Epoch: 4404 [98560/118836 (83%)] Loss: 12235.351562\n",
      "    epoch          : 4404\n",
      "    loss           : 12218.945993266645\n",
      "    val_loss       : 12214.6639511912\n",
      "    val_log_likelihood: -12138.82452310794\n",
      "    val_log_marginal: -12146.71394971942\n",
      "Train Epoch: 4405 [256/118836 (0%)] Loss: 12133.851562\n",
      "Train Epoch: 4405 [33024/118836 (28%)] Loss: 12304.805664\n",
      "Train Epoch: 4405 [65792/118836 (55%)] Loss: 12323.613281\n",
      "Train Epoch: 4405 [98560/118836 (83%)] Loss: 12253.279297\n",
      "    epoch          : 4405\n",
      "    loss           : 12216.841113685121\n",
      "    val_loss       : 12219.029198986325\n",
      "    val_log_likelihood: -12139.688536820202\n",
      "    val_log_marginal: -12147.577485215961\n",
      "Train Epoch: 4406 [256/118836 (0%)] Loss: 12322.645508\n",
      "Train Epoch: 4406 [33024/118836 (28%)] Loss: 12149.706055\n",
      "Train Epoch: 4406 [65792/118836 (55%)] Loss: 12171.727539\n",
      "Train Epoch: 4406 [98560/118836 (83%)] Loss: 12183.603516\n",
      "    epoch          : 4406\n",
      "    loss           : 12213.910764804332\n",
      "    val_loss       : 12216.912256246887\n",
      "    val_log_likelihood: -12141.5901536006\n",
      "    val_log_marginal: -12149.473788239664\n",
      "Train Epoch: 4407 [256/118836 (0%)] Loss: 12265.593750\n",
      "Train Epoch: 4407 [33024/118836 (28%)] Loss: 12169.464844\n",
      "Train Epoch: 4407 [65792/118836 (55%)] Loss: 12168.233398\n",
      "Train Epoch: 4407 [98560/118836 (83%)] Loss: 12210.947266\n",
      "    epoch          : 4407\n",
      "    loss           : 12215.028553750517\n",
      "    val_loss       : 12213.846469134367\n",
      "    val_log_likelihood: -12139.956765502222\n",
      "    val_log_marginal: -12147.843048470344\n",
      "Train Epoch: 4408 [256/118836 (0%)] Loss: 12394.716797\n",
      "Train Epoch: 4408 [33024/118836 (28%)] Loss: 12152.589844\n",
      "Train Epoch: 4408 [65792/118836 (55%)] Loss: 12162.119141\n",
      "Train Epoch: 4408 [98560/118836 (83%)] Loss: 12166.755859\n",
      "    epoch          : 4408\n",
      "    loss           : 12215.466988633427\n",
      "    val_loss       : 12213.280342281556\n",
      "    val_log_likelihood: -12140.097348499534\n",
      "    val_log_marginal: -12147.98359976444\n",
      "Train Epoch: 4409 [256/118836 (0%)] Loss: 12154.009766\n",
      "Train Epoch: 4409 [33024/118836 (28%)] Loss: 12350.767578\n",
      "Train Epoch: 4409 [65792/118836 (55%)] Loss: 12166.490234\n",
      "Train Epoch: 4409 [98560/118836 (83%)] Loss: 12263.367188\n",
      "    epoch          : 4409\n",
      "    loss           : 12219.343500245555\n",
      "    val_loss       : 12215.780051934837\n",
      "    val_log_likelihood: -12139.668951451355\n",
      "    val_log_marginal: -12147.561247404388\n",
      "Train Epoch: 4410 [256/118836 (0%)] Loss: 12242.546875\n",
      "Train Epoch: 4410 [33024/118836 (28%)] Loss: 12268.688477\n",
      "Train Epoch: 4410 [65792/118836 (55%)] Loss: 12266.442383\n",
      "Train Epoch: 4410 [98560/118836 (83%)] Loss: 12457.278320\n",
      "    epoch          : 4410\n",
      "    loss           : 12219.673740404001\n",
      "    val_loss       : 12217.289854119217\n",
      "    val_log_likelihood: -12140.27211877714\n",
      "    val_log_marginal: -12148.158771232305\n",
      "Train Epoch: 4411 [256/118836 (0%)] Loss: 12194.190430\n",
      "Train Epoch: 4411 [33024/118836 (28%)] Loss: 12194.845703\n",
      "Train Epoch: 4411 [65792/118836 (55%)] Loss: 12191.741211\n",
      "Train Epoch: 4411 [98560/118836 (83%)] Loss: 12225.698242\n",
      "    epoch          : 4411\n",
      "    loss           : 12219.423885959472\n",
      "    val_loss       : 12219.074515031924\n",
      "    val_log_likelihood: -12140.07247305366\n",
      "    val_log_marginal: -12147.959243335696\n",
      "Train Epoch: 4412 [256/118836 (0%)] Loss: 12243.776367\n",
      "Train Epoch: 4412 [33024/118836 (28%)] Loss: 12269.913086\n",
      "Train Epoch: 4412 [65792/118836 (55%)] Loss: 12239.177734\n",
      "Train Epoch: 4412 [98560/118836 (83%)] Loss: 12249.379883\n",
      "    epoch          : 4412\n",
      "    loss           : 12219.977480420286\n",
      "    val_loss       : 12217.894813014509\n",
      "    val_log_likelihood: -12139.474097265302\n",
      "    val_log_marginal: -12147.367665776448\n",
      "Train Epoch: 4413 [256/118836 (0%)] Loss: 12214.398438\n",
      "Train Epoch: 4413 [33024/118836 (28%)] Loss: 12325.904297\n",
      "Train Epoch: 4413 [65792/118836 (55%)] Loss: 12243.125000\n",
      "Train Epoch: 4413 [98560/118836 (83%)] Loss: 12202.175781\n",
      "    epoch          : 4413\n",
      "    loss           : 12218.473613103548\n",
      "    val_loss       : 12216.415135058085\n",
      "    val_log_likelihood: -12138.893941435328\n",
      "    val_log_marginal: -12146.783225353925\n",
      "Train Epoch: 4414 [256/118836 (0%)] Loss: 12206.443359\n",
      "Train Epoch: 4414 [33024/118836 (28%)] Loss: 12232.741211\n",
      "Train Epoch: 4414 [65792/118836 (55%)] Loss: 12208.834961\n",
      "Train Epoch: 4414 [98560/118836 (83%)] Loss: 12239.613281\n",
      "    epoch          : 4414\n",
      "    loss           : 12217.907687945873\n",
      "    val_loss       : 12217.797746088283\n",
      "    val_log_likelihood: -12140.134964814672\n",
      "    val_log_marginal: -12148.022783715212\n",
      "Train Epoch: 4415 [256/118836 (0%)] Loss: 12345.946289\n",
      "Train Epoch: 4415 [33024/118836 (28%)] Loss: 12265.148438\n",
      "Train Epoch: 4415 [65792/118836 (55%)] Loss: 12300.064453\n",
      "Train Epoch: 4415 [98560/118836 (83%)] Loss: 12374.253906\n",
      "    epoch          : 4415\n",
      "    loss           : 12214.95731024478\n",
      "    val_loss       : 12222.922974188758\n",
      "    val_log_likelihood: -12139.582884873862\n",
      "    val_log_marginal: -12147.467978791276\n",
      "Train Epoch: 4416 [256/118836 (0%)] Loss: 12140.204102\n",
      "Train Epoch: 4416 [33024/118836 (28%)] Loss: 12161.000000\n",
      "Train Epoch: 4416 [65792/118836 (55%)] Loss: 12178.850586\n",
      "Train Epoch: 4416 [98560/118836 (83%)] Loss: 12198.956055\n",
      "    epoch          : 4416\n",
      "    loss           : 12217.443586027966\n",
      "    val_loss       : 12221.841198252858\n",
      "    val_log_likelihood: -12139.524071417494\n",
      "    val_log_marginal: -12147.413450019023\n",
      "Train Epoch: 4417 [256/118836 (0%)] Loss: 12287.031250\n",
      "Train Epoch: 4417 [33024/118836 (28%)] Loss: 12206.004883\n",
      "Train Epoch: 4417 [65792/118836 (55%)] Loss: 12149.583984\n",
      "Train Epoch: 4417 [98560/118836 (83%)] Loss: 12172.507812\n",
      "    epoch          : 4417\n",
      "    loss           : 12217.284374030707\n",
      "    val_loss       : 12218.443220742614\n",
      "    val_log_likelihood: -12141.091470546424\n",
      "    val_log_marginal: -12148.977755372634\n",
      "Train Epoch: 4418 [256/118836 (0%)] Loss: 12217.422852\n",
      "Train Epoch: 4418 [33024/118836 (28%)] Loss: 12180.099609\n",
      "Train Epoch: 4418 [65792/118836 (55%)] Loss: 12176.697266\n",
      "Train Epoch: 4418 [98560/118836 (83%)] Loss: 12223.516602\n",
      "    epoch          : 4418\n",
      "    loss           : 12217.158311201147\n",
      "    val_loss       : 12216.8556297166\n",
      "    val_log_likelihood: -12140.923474010027\n",
      "    val_log_marginal: -12148.81079100249\n",
      "Train Epoch: 4419 [256/118836 (0%)] Loss: 12295.127930\n",
      "Train Epoch: 4419 [33024/118836 (28%)] Loss: 12169.094727\n",
      "Train Epoch: 4419 [65792/118836 (55%)] Loss: 12200.785156\n",
      "Train Epoch: 4419 [98560/118836 (83%)] Loss: 12166.364258\n",
      "    epoch          : 4419\n",
      "    loss           : 12215.077870237541\n",
      "    val_loss       : 12216.03734022715\n",
      "    val_log_likelihood: -12138.772831853546\n",
      "    val_log_marginal: -12146.665222505608\n",
      "Train Epoch: 4420 [256/118836 (0%)] Loss: 12236.302734\n",
      "Train Epoch: 4420 [33024/118836 (28%)] Loss: 12285.406250\n",
      "Train Epoch: 4420 [65792/118836 (55%)] Loss: 12130.502930\n",
      "Train Epoch: 4420 [98560/118836 (83%)] Loss: 12259.551758\n",
      "    epoch          : 4420\n",
      "    loss           : 12220.424046700527\n",
      "    val_loss       : 12213.73186548744\n",
      "    val_log_likelihood: -12139.826588509355\n",
      "    val_log_marginal: -12147.71721954063\n",
      "Train Epoch: 4421 [256/118836 (0%)] Loss: 12240.846680\n",
      "Train Epoch: 4421 [33024/118836 (28%)] Loss: 12155.190430\n",
      "Train Epoch: 4421 [65792/118836 (55%)] Loss: 12214.456055\n",
      "Train Epoch: 4421 [98560/118836 (83%)] Loss: 12151.380859\n",
      "    epoch          : 4421\n",
      "    loss           : 12213.918852745037\n",
      "    val_loss       : 12213.80292178201\n",
      "    val_log_likelihood: -12139.675182227047\n",
      "    val_log_marginal: -12147.561314903673\n",
      "Train Epoch: 4422 [256/118836 (0%)] Loss: 12273.154297\n",
      "Train Epoch: 4422 [33024/118836 (28%)] Loss: 12285.206055\n",
      "Train Epoch: 4422 [65792/118836 (55%)] Loss: 12201.716797\n",
      "Train Epoch: 4422 [98560/118836 (83%)] Loss: 12265.615234\n",
      "    epoch          : 4422\n",
      "    loss           : 12218.701889959419\n",
      "    val_loss       : 12217.361580161569\n",
      "    val_log_likelihood: -12142.337234090674\n",
      "    val_log_marginal: -12150.221749180395\n",
      "Train Epoch: 4423 [256/118836 (0%)] Loss: 12236.252930\n",
      "Train Epoch: 4423 [33024/118836 (28%)] Loss: 12207.175781\n",
      "Train Epoch: 4423 [65792/118836 (55%)] Loss: 12196.407227\n",
      "Train Epoch: 4423 [98560/118836 (83%)] Loss: 12267.055664\n",
      "    epoch          : 4423\n",
      "    loss           : 12217.99787724876\n",
      "    val_loss       : 12218.441565711784\n",
      "    val_log_likelihood: -12139.994795382288\n",
      "    val_log_marginal: -12147.885033757293\n",
      "Train Epoch: 4424 [256/118836 (0%)] Loss: 12171.729492\n",
      "Train Epoch: 4424 [33024/118836 (28%)] Loss: 12264.653320\n",
      "Train Epoch: 4424 [65792/118836 (55%)] Loss: 12196.285156\n",
      "Train Epoch: 4424 [98560/118836 (83%)] Loss: 12180.894531\n",
      "    epoch          : 4424\n",
      "    loss           : 12216.070895044973\n",
      "    val_loss       : 12213.864950782598\n",
      "    val_log_likelihood: -12141.198415206265\n",
      "    val_log_marginal: -12149.084393860008\n",
      "Train Epoch: 4425 [256/118836 (0%)] Loss: 12198.601562\n",
      "Train Epoch: 4425 [33024/118836 (28%)] Loss: 12284.440430\n",
      "Train Epoch: 4425 [65792/118836 (55%)] Loss: 12212.514648\n",
      "Train Epoch: 4425 [98560/118836 (83%)] Loss: 12145.721680\n",
      "    epoch          : 4425\n",
      "    loss           : 12218.955163584315\n",
      "    val_loss       : 12215.048654409575\n",
      "    val_log_likelihood: -12140.222221683727\n",
      "    val_log_marginal: -12148.1134601474\n",
      "Train Epoch: 4426 [256/118836 (0%)] Loss: 12238.530273\n",
      "Train Epoch: 4426 [33024/118836 (28%)] Loss: 12231.789062\n",
      "Train Epoch: 4426 [65792/118836 (55%)] Loss: 12179.812500\n",
      "Train Epoch: 4426 [98560/118836 (83%)] Loss: 12209.787109\n",
      "    epoch          : 4426\n",
      "    loss           : 12216.968687157518\n",
      "    val_loss       : 12219.282958612326\n",
      "    val_log_likelihood: -12140.002789786238\n",
      "    val_log_marginal: -12147.896318667745\n",
      "Train Epoch: 4427 [256/118836 (0%)] Loss: 12211.857422\n",
      "Train Epoch: 4427 [33024/118836 (28%)] Loss: 12186.675781\n",
      "Train Epoch: 4427 [65792/118836 (55%)] Loss: 12238.254883\n",
      "Train Epoch: 4427 [98560/118836 (83%)] Loss: 12256.376953\n",
      "    epoch          : 4427\n",
      "    loss           : 12217.077062655086\n",
      "    val_loss       : 12216.07935680799\n",
      "    val_log_likelihood: -12139.515395116057\n",
      "    val_log_marginal: -12147.40209933896\n",
      "Train Epoch: 4428 [256/118836 (0%)] Loss: 12235.910156\n",
      "Train Epoch: 4428 [33024/118836 (28%)] Loss: 12324.856445\n",
      "Train Epoch: 4428 [65792/118836 (55%)] Loss: 12153.585938\n",
      "Train Epoch: 4428 [98560/118836 (83%)] Loss: 12154.074219\n",
      "    epoch          : 4428\n",
      "    loss           : 12219.080190401417\n",
      "    val_loss       : 12217.661733052873\n",
      "    val_log_likelihood: -12140.103883956264\n",
      "    val_log_marginal: -12147.990215894648\n",
      "Train Epoch: 4429 [256/118836 (0%)] Loss: 12254.808594\n",
      "Train Epoch: 4429 [33024/118836 (28%)] Loss: 12165.632812\n",
      "Train Epoch: 4429 [65792/118836 (55%)] Loss: 12169.072266\n",
      "Train Epoch: 4429 [98560/118836 (83%)] Loss: 12202.764648\n",
      "    epoch          : 4429\n",
      "    loss           : 12216.287851207093\n",
      "    val_loss       : 12218.031594407244\n",
      "    val_log_likelihood: -12139.112354767629\n",
      "    val_log_marginal: -12146.997520094372\n",
      "Train Epoch: 4430 [256/118836 (0%)] Loss: 12328.438477\n",
      "Train Epoch: 4430 [33024/118836 (28%)] Loss: 12206.965820\n",
      "Train Epoch: 4430 [65792/118836 (55%)] Loss: 12192.835938\n",
      "Train Epoch: 4430 [98560/118836 (83%)] Loss: 12245.984375\n",
      "    epoch          : 4430\n",
      "    loss           : 12217.303321443342\n",
      "    val_loss       : 12218.88364796178\n",
      "    val_log_likelihood: -12139.606888763958\n",
      "    val_log_marginal: -12147.500363721525\n",
      "Train Epoch: 4431 [256/118836 (0%)] Loss: 12195.654297\n",
      "Train Epoch: 4431 [33024/118836 (28%)] Loss: 12253.962891\n",
      "Train Epoch: 4431 [65792/118836 (55%)] Loss: 12265.628906\n",
      "Train Epoch: 4431 [98560/118836 (83%)] Loss: 12132.060547\n",
      "    epoch          : 4431\n",
      "    loss           : 12220.219328344705\n",
      "    val_loss       : 12216.561315573852\n",
      "    val_log_likelihood: -12139.679129025797\n",
      "    val_log_marginal: -12147.572224176381\n",
      "Train Epoch: 4432 [256/118836 (0%)] Loss: 12191.429688\n",
      "Train Epoch: 4432 [33024/118836 (28%)] Loss: 12161.312500\n",
      "Train Epoch: 4432 [65792/118836 (55%)] Loss: 12242.940430\n",
      "Train Epoch: 4432 [98560/118836 (83%)] Loss: 12247.111328\n",
      "    epoch          : 4432\n",
      "    loss           : 12217.852616767474\n",
      "    val_loss       : 12215.989121360632\n",
      "    val_log_likelihood: -12140.843610583386\n",
      "    val_log_marginal: -12148.737234653707\n",
      "Train Epoch: 4433 [256/118836 (0%)] Loss: 12277.758789\n",
      "Train Epoch: 4433 [33024/118836 (28%)] Loss: 12160.555664\n",
      "Train Epoch: 4433 [65792/118836 (55%)] Loss: 12155.527344\n",
      "Train Epoch: 4433 [98560/118836 (83%)] Loss: 12141.990234\n",
      "    epoch          : 4433\n",
      "    loss           : 12214.106942559709\n",
      "    val_loss       : 12217.408349259616\n",
      "    val_log_likelihood: -12141.158394883427\n",
      "    val_log_marginal: -12149.04814380956\n",
      "Train Epoch: 4434 [256/118836 (0%)] Loss: 12360.154297\n",
      "Train Epoch: 4434 [33024/118836 (28%)] Loss: 12216.689453\n",
      "Train Epoch: 4434 [65792/118836 (55%)] Loss: 12177.261719\n",
      "Train Epoch: 4434 [98560/118836 (83%)] Loss: 12297.795898\n",
      "    epoch          : 4434\n",
      "    loss           : 12216.482812176902\n",
      "    val_loss       : 12219.35712442434\n",
      "    val_log_likelihood: -12141.140738568807\n",
      "    val_log_marginal: -12149.023947842496\n",
      "Train Epoch: 4435 [256/118836 (0%)] Loss: 12311.582031\n",
      "Train Epoch: 4435 [33024/118836 (28%)] Loss: 12143.953125\n",
      "Train Epoch: 4435 [65792/118836 (55%)] Loss: 12209.936523\n",
      "Train Epoch: 4435 [98560/118836 (83%)] Loss: 12206.762695\n",
      "    epoch          : 4435\n",
      "    loss           : 12217.501468478598\n",
      "    val_loss       : 12215.008349289446\n",
      "    val_log_likelihood: -12139.259407955955\n",
      "    val_log_marginal: -12147.144818127577\n",
      "Train Epoch: 4436 [256/118836 (0%)] Loss: 12179.015625\n",
      "Train Epoch: 4436 [33024/118836 (28%)] Loss: 12346.599609\n",
      "Train Epoch: 4436 [65792/118836 (55%)] Loss: 12274.190430\n",
      "Train Epoch: 4436 [98560/118836 (83%)] Loss: 12279.256836\n",
      "    epoch          : 4436\n",
      "    loss           : 12218.427449887562\n",
      "    val_loss       : 12218.740404597722\n",
      "    val_log_likelihood: -12138.86969812991\n",
      "    val_log_marginal: -12146.757670262463\n",
      "Train Epoch: 4437 [256/118836 (0%)] Loss: 12178.901367\n",
      "Train Epoch: 4437 [33024/118836 (28%)] Loss: 12173.113281\n",
      "Train Epoch: 4437 [65792/118836 (55%)] Loss: 12138.543945\n",
      "Train Epoch: 4437 [98560/118836 (83%)] Loss: 12165.216797\n",
      "    epoch          : 4437\n",
      "    loss           : 12218.571714097394\n",
      "    val_loss       : 12217.376762843454\n",
      "    val_log_likelihood: -12141.317214317101\n",
      "    val_log_marginal: -12149.20820766901\n",
      "Train Epoch: 4438 [256/118836 (0%)] Loss: 12157.263672\n",
      "Train Epoch: 4438 [33024/118836 (28%)] Loss: 12214.267578\n",
      "Train Epoch: 4438 [65792/118836 (55%)] Loss: 12285.455078\n",
      "Train Epoch: 4438 [98560/118836 (83%)] Loss: 12124.597656\n",
      "    epoch          : 4438\n",
      "    loss           : 12220.007892628204\n",
      "    val_loss       : 12213.490799673438\n",
      "    val_log_likelihood: -12140.364320978082\n",
      "    val_log_marginal: -12148.246322256491\n",
      "Train Epoch: 4439 [256/118836 (0%)] Loss: 12197.351562\n",
      "Train Epoch: 4439 [33024/118836 (28%)] Loss: 12287.875000\n",
      "Train Epoch: 4439 [65792/118836 (55%)] Loss: 12204.027344\n",
      "Train Epoch: 4439 [98560/118836 (83%)] Loss: 12185.834961\n",
      "    epoch          : 4439\n",
      "    loss           : 12212.745705548232\n",
      "    val_loss       : 12214.309979498381\n",
      "    val_log_likelihood: -12137.062944259202\n",
      "    val_log_marginal: -12144.947427765934\n",
      "Train Epoch: 4440 [256/118836 (0%)] Loss: 12318.385742\n",
      "Train Epoch: 4440 [33024/118836 (28%)] Loss: 12278.624023\n",
      "Train Epoch: 4440 [65792/118836 (55%)] Loss: 12231.443359\n",
      "Train Epoch: 4440 [98560/118836 (83%)] Loss: 12179.508789\n",
      "    epoch          : 4440\n",
      "    loss           : 12215.1944908628\n",
      "    val_loss       : 12215.775190900222\n",
      "    val_log_likelihood: -12142.167598964794\n",
      "    val_log_marginal: -12150.05596595914\n",
      "Train Epoch: 4441 [256/118836 (0%)] Loss: 12209.065430\n",
      "Train Epoch: 4441 [33024/118836 (28%)] Loss: 12187.462891\n",
      "Train Epoch: 4441 [65792/118836 (55%)] Loss: 12242.063477\n",
      "Train Epoch: 4441 [98560/118836 (83%)] Loss: 12330.312500\n",
      "    epoch          : 4441\n",
      "    loss           : 12218.516297850754\n",
      "    val_loss       : 12220.159109652597\n",
      "    val_log_likelihood: -12139.101924046216\n",
      "    val_log_marginal: -12146.986590841989\n",
      "Train Epoch: 4442 [256/118836 (0%)] Loss: 12241.824219\n",
      "Train Epoch: 4442 [33024/118836 (28%)] Loss: 12255.772461\n",
      "Train Epoch: 4442 [65792/118836 (55%)] Loss: 12194.355469\n",
      "Train Epoch: 4442 [98560/118836 (83%)] Loss: 12149.822266\n",
      "    epoch          : 4442\n",
      "    loss           : 12217.654129510443\n",
      "    val_loss       : 12219.888463972997\n",
      "    val_log_likelihood: -12139.819076813224\n",
      "    val_log_marginal: -12147.704489021187\n",
      "Train Epoch: 4443 [256/118836 (0%)] Loss: 12259.228516\n",
      "Train Epoch: 4443 [33024/118836 (28%)] Loss: 12269.298828\n",
      "Train Epoch: 4443 [65792/118836 (55%)] Loss: 12385.410156\n",
      "Train Epoch: 4443 [98560/118836 (83%)] Loss: 12234.738281\n",
      "    epoch          : 4443\n",
      "    loss           : 12213.757695377119\n",
      "    val_loss       : 12218.742657754972\n",
      "    val_log_likelihood: -12140.939100787065\n",
      "    val_log_marginal: -12148.831637789848\n",
      "Train Epoch: 4444 [256/118836 (0%)] Loss: 12227.475586\n",
      "Train Epoch: 4444 [33024/118836 (28%)] Loss: 12202.362305\n",
      "Train Epoch: 4444 [65792/118836 (55%)] Loss: 12248.608398\n",
      "Train Epoch: 4444 [98560/118836 (83%)] Loss: 12246.224609\n",
      "    epoch          : 4444\n",
      "    loss           : 12216.513797236868\n",
      "    val_loss       : 12218.908781938399\n",
      "    val_log_likelihood: -12140.319348053661\n",
      "    val_log_marginal: -12148.20658346987\n",
      "Train Epoch: 4445 [256/118836 (0%)] Loss: 12214.525391\n",
      "Train Epoch: 4445 [33024/118836 (28%)] Loss: 12346.230469\n",
      "Train Epoch: 4445 [65792/118836 (55%)] Loss: 12310.669922\n",
      "Train Epoch: 4445 [98560/118836 (83%)] Loss: 12274.234375\n",
      "    epoch          : 4445\n",
      "    loss           : 12218.663599985784\n",
      "    val_loss       : 12217.621588010223\n",
      "    val_log_likelihood: -12139.39488827285\n",
      "    val_log_marginal: -12147.283903202026\n",
      "Train Epoch: 4446 [256/118836 (0%)] Loss: 12277.890625\n",
      "Train Epoch: 4446 [33024/118836 (28%)] Loss: 12260.246094\n",
      "Train Epoch: 4446 [65792/118836 (55%)] Loss: 12182.190430\n",
      "Train Epoch: 4446 [98560/118836 (83%)] Loss: 12284.593750\n",
      "    epoch          : 4446\n",
      "    loss           : 12222.442803647125\n",
      "    val_loss       : 12218.203068299354\n",
      "    val_log_likelihood: -12142.444097006824\n",
      "    val_log_marginal: -12150.323105068474\n",
      "Train Epoch: 4447 [256/118836 (0%)] Loss: 12240.155273\n",
      "Train Epoch: 4447 [33024/118836 (28%)] Loss: 12287.708984\n",
      "Train Epoch: 4447 [65792/118836 (55%)] Loss: 12283.404297\n",
      "Train Epoch: 4447 [98560/118836 (83%)] Loss: 12165.666992\n",
      "    epoch          : 4447\n",
      "    loss           : 12214.099953473946\n",
      "    val_loss       : 12217.446971139005\n",
      "    val_log_likelihood: -12138.865221451097\n",
      "    val_log_marginal: -12146.750329270102\n",
      "Train Epoch: 4448 [256/118836 (0%)] Loss: 12268.979492\n",
      "Train Epoch: 4448 [33024/118836 (28%)] Loss: 12228.243164\n",
      "Train Epoch: 4448 [65792/118836 (55%)] Loss: 12214.051758\n",
      "Train Epoch: 4448 [98560/118836 (83%)] Loss: 12275.370117\n",
      "    epoch          : 4448\n",
      "    loss           : 12218.315007075838\n",
      "    val_loss       : 12226.112755976328\n",
      "    val_log_likelihood: -12140.33119345792\n",
      "    val_log_marginal: -12148.223259779541\n",
      "Train Epoch: 4449 [256/118836 (0%)] Loss: 12208.001953\n",
      "Train Epoch: 4449 [33024/118836 (28%)] Loss: 12353.045898\n",
      "Train Epoch: 4449 [65792/118836 (55%)] Loss: 12293.448242\n",
      "Train Epoch: 4449 [98560/118836 (83%)] Loss: 12169.611328\n",
      "    epoch          : 4449\n",
      "    loss           : 12217.019965493177\n",
      "    val_loss       : 12213.385591625898\n",
      "    val_log_likelihood: -12139.462411794355\n",
      "    val_log_marginal: -12147.348700588602\n",
      "Train Epoch: 4450 [256/118836 (0%)] Loss: 12219.744141\n",
      "Train Epoch: 4450 [33024/118836 (28%)] Loss: 12162.330078\n",
      "Train Epoch: 4450 [65792/118836 (55%)] Loss: 12230.758789\n",
      "Train Epoch: 4450 [98560/118836 (83%)] Loss: 12300.057617\n",
      "    epoch          : 4450\n",
      "    loss           : 12213.933883245452\n",
      "    val_loss       : 12215.756359351797\n",
      "    val_log_likelihood: -12140.571498268198\n",
      "    val_log_marginal: -12148.460077932517\n",
      "Train Epoch: 4451 [256/118836 (0%)] Loss: 12189.097656\n",
      "Train Epoch: 4451 [33024/118836 (28%)] Loss: 12207.791992\n",
      "Train Epoch: 4451 [65792/118836 (55%)] Loss: 12280.648438\n",
      "Train Epoch: 4451 [98560/118836 (83%)] Loss: 12278.391602\n",
      "    epoch          : 4451\n",
      "    loss           : 12215.354645335763\n",
      "    val_loss       : 12215.442842094477\n",
      "    val_log_likelihood: -12137.694604754704\n",
      "    val_log_marginal: -12145.578460558714\n",
      "Train Epoch: 4452 [256/118836 (0%)] Loss: 12282.219727\n",
      "Train Epoch: 4452 [33024/118836 (28%)] Loss: 12174.112305\n",
      "Train Epoch: 4452 [65792/118836 (55%)] Loss: 12291.904297\n",
      "Train Epoch: 4452 [98560/118836 (83%)] Loss: 12313.041016\n",
      "    epoch          : 4452\n",
      "    loss           : 12218.960240255376\n",
      "    val_loss       : 12212.445775918866\n",
      "    val_log_likelihood: -12140.142324493383\n",
      "    val_log_marginal: -12148.023204089584\n",
      "Train Epoch: 4453 [256/118836 (0%)] Loss: 12174.907227\n",
      "Train Epoch: 4453 [33024/118836 (28%)] Loss: 12357.298828\n",
      "Train Epoch: 4453 [65792/118836 (55%)] Loss: 12239.029297\n",
      "Train Epoch: 4453 [98560/118836 (83%)] Loss: 12300.109375\n",
      "    epoch          : 4453\n",
      "    loss           : 12216.41774645885\n",
      "    val_loss       : 12218.038166315428\n",
      "    val_log_likelihood: -12139.551536652192\n",
      "    val_log_marginal: -12147.440021450726\n",
      "Train Epoch: 4454 [256/118836 (0%)] Loss: 12182.396484\n",
      "Train Epoch: 4454 [33024/118836 (28%)] Loss: 12251.746094\n",
      "Train Epoch: 4454 [65792/118836 (55%)] Loss: 12241.840820\n",
      "Train Epoch: 4454 [98560/118836 (83%)] Loss: 12231.925781\n",
      "    epoch          : 4454\n",
      "    loss           : 12217.743241444376\n",
      "    val_loss       : 12217.711325765451\n",
      "    val_log_likelihood: -12139.163874618745\n",
      "    val_log_marginal: -12147.057476263768\n",
      "Train Epoch: 4455 [256/118836 (0%)] Loss: 12215.372070\n",
      "Train Epoch: 4455 [33024/118836 (28%)] Loss: 12190.434570\n",
      "Train Epoch: 4455 [65792/118836 (55%)] Loss: 12327.903320\n",
      "Train Epoch: 4455 [98560/118836 (83%)] Loss: 12190.904297\n",
      "    epoch          : 4455\n",
      "    loss           : 12220.428864408861\n",
      "    val_loss       : 12219.442057387309\n",
      "    val_log_likelihood: -12139.67591727409\n",
      "    val_log_marginal: -12147.565838258222\n",
      "Train Epoch: 4456 [256/118836 (0%)] Loss: 12255.283203\n",
      "Train Epoch: 4456 [33024/118836 (28%)] Loss: 12202.005859\n",
      "Train Epoch: 4456 [65792/118836 (55%)] Loss: 12294.682617\n",
      "Train Epoch: 4456 [98560/118836 (83%)] Loss: 12236.906250\n",
      "    epoch          : 4456\n",
      "    loss           : 12220.08416094784\n",
      "    val_loss       : 12218.349693939776\n",
      "    val_log_likelihood: -12139.898829094293\n",
      "    val_log_marginal: -12147.781444862261\n",
      "Train Epoch: 4457 [256/118836 (0%)] Loss: 12190.819336\n",
      "Train Epoch: 4457 [33024/118836 (28%)] Loss: 12167.860352\n",
      "Train Epoch: 4457 [65792/118836 (55%)] Loss: 12271.790039\n",
      "Train Epoch: 4457 [98560/118836 (83%)] Loss: 12267.476562\n",
      "    epoch          : 4457\n",
      "    loss           : 12214.711014235681\n",
      "    val_loss       : 12218.559661872434\n",
      "    val_log_likelihood: -12141.10712817928\n",
      "    val_log_marginal: -12148.996374555545\n",
      "Train Epoch: 4458 [256/118836 (0%)] Loss: 12278.246094\n",
      "Train Epoch: 4458 [33024/118836 (28%)] Loss: 12211.010742\n",
      "Train Epoch: 4458 [65792/118836 (55%)] Loss: 12204.267578\n",
      "Train Epoch: 4458 [98560/118836 (83%)] Loss: 12327.626953\n",
      "    epoch          : 4458\n",
      "    loss           : 12219.541111100341\n",
      "    val_loss       : 12219.96444985114\n",
      "    val_log_likelihood: -12139.986595165166\n",
      "    val_log_marginal: -12147.88338357072\n",
      "Train Epoch: 4459 [256/118836 (0%)] Loss: 12260.994141\n",
      "Train Epoch: 4459 [33024/118836 (28%)] Loss: 12154.138672\n",
      "Train Epoch: 4459 [65792/118836 (55%)] Loss: 12191.866211\n",
      "Train Epoch: 4459 [98560/118836 (83%)] Loss: 12177.000977\n",
      "    epoch          : 4459\n",
      "    loss           : 12219.785097769334\n",
      "    val_loss       : 12215.037047320791\n",
      "    val_log_likelihood: -12140.724592735474\n",
      "    val_log_marginal: -12148.61278727323\n",
      "Train Epoch: 4460 [256/118836 (0%)] Loss: 12193.109375\n",
      "Train Epoch: 4460 [33024/118836 (28%)] Loss: 12198.123047\n",
      "Train Epoch: 4460 [65792/118836 (55%)] Loss: 12243.099609\n",
      "Train Epoch: 4460 [98560/118836 (83%)] Loss: 12169.375977\n",
      "    epoch          : 4460\n",
      "    loss           : 12219.686047676281\n",
      "    val_loss       : 12219.249727002694\n",
      "    val_log_likelihood: -12139.661384828629\n",
      "    val_log_marginal: -12147.553795285927\n",
      "Train Epoch: 4461 [256/118836 (0%)] Loss: 12166.014648\n",
      "Train Epoch: 4461 [33024/118836 (28%)] Loss: 12315.495117\n",
      "Train Epoch: 4461 [65792/118836 (55%)] Loss: 12296.951172\n",
      "Train Epoch: 4461 [98560/118836 (83%)] Loss: 12199.536133\n",
      "    epoch          : 4461\n",
      "    loss           : 12222.65835675791\n",
      "    val_loss       : 12217.429122326916\n",
      "    val_log_likelihood: -12137.043464704817\n",
      "    val_log_marginal: -12144.92368270468\n",
      "Train Epoch: 4462 [256/118836 (0%)] Loss: 12206.767578\n",
      "Train Epoch: 4462 [33024/118836 (28%)] Loss: 12203.190430\n",
      "Train Epoch: 4462 [65792/118836 (55%)] Loss: 12152.928711\n",
      "Train Epoch: 4462 [98560/118836 (83%)] Loss: 12156.109375\n",
      "    epoch          : 4462\n",
      "    loss           : 12219.434618292495\n",
      "    val_loss       : 12220.435075857797\n",
      "    val_log_likelihood: -12140.842384266438\n",
      "    val_log_marginal: -12148.72979744979\n",
      "Train Epoch: 4463 [256/118836 (0%)] Loss: 12160.496094\n",
      "Train Epoch: 4463 [33024/118836 (28%)] Loss: 12173.478516\n",
      "Train Epoch: 4463 [65792/118836 (55%)] Loss: 12129.854492\n",
      "Train Epoch: 4463 [98560/118836 (83%)] Loss: 12232.713867\n",
      "    epoch          : 4463\n",
      "    loss           : 12215.935658989867\n",
      "    val_loss       : 12217.37942604882\n",
      "    val_log_likelihood: -12139.88281233845\n",
      "    val_log_marginal: -12147.77535468528\n",
      "Train Epoch: 4464 [256/118836 (0%)] Loss: 12201.041992\n",
      "Train Epoch: 4464 [33024/118836 (28%)] Loss: 12282.708984\n",
      "Train Epoch: 4464 [65792/118836 (55%)] Loss: 12208.160156\n",
      "Train Epoch: 4464 [98560/118836 (83%)] Loss: 12279.895508\n",
      "    epoch          : 4464\n",
      "    loss           : 12218.590178317567\n",
      "    val_loss       : 12219.547847206764\n",
      "    val_log_likelihood: -12140.893965667648\n",
      "    val_log_marginal: -12148.77992681287\n",
      "Train Epoch: 4465 [256/118836 (0%)] Loss: 12187.529297\n",
      "Train Epoch: 4465 [33024/118836 (28%)] Loss: 12339.179688\n",
      "Train Epoch: 4465 [65792/118836 (55%)] Loss: 12168.524414\n",
      "Train Epoch: 4465 [98560/118836 (83%)] Loss: 12326.332031\n",
      "    epoch          : 4465\n",
      "    loss           : 12217.572269017526\n",
      "    val_loss       : 12218.872746630279\n",
      "    val_log_likelihood: -12138.771656424473\n",
      "    val_log_marginal: -12146.662569556594\n",
      "Train Epoch: 4466 [256/118836 (0%)] Loss: 12224.361328\n",
      "Train Epoch: 4466 [33024/118836 (28%)] Loss: 12281.957031\n",
      "Train Epoch: 4466 [65792/118836 (55%)] Loss: 12220.384766\n",
      "Train Epoch: 4466 [98560/118836 (83%)] Loss: 12190.519531\n",
      "    epoch          : 4466\n",
      "    loss           : 12214.269457583747\n",
      "    val_loss       : 12219.437051314995\n",
      "    val_log_likelihood: -12139.418012206626\n",
      "    val_log_marginal: -12147.30456680817\n",
      "Train Epoch: 4467 [256/118836 (0%)] Loss: 12207.698242\n",
      "Train Epoch: 4467 [33024/118836 (28%)] Loss: 12284.308594\n",
      "Train Epoch: 4467 [65792/118836 (55%)] Loss: 12187.279297\n",
      "Train Epoch: 4467 [98560/118836 (83%)] Loss: 12254.451172\n",
      "    epoch          : 4467\n",
      "    loss           : 12220.210317152607\n",
      "    val_loss       : 12218.768436555552\n",
      "    val_log_likelihood: -12140.886004381204\n",
      "    val_log_marginal: -12148.777287730903\n",
      "Train Epoch: 4468 [256/118836 (0%)] Loss: 12232.745117\n",
      "Train Epoch: 4468 [33024/118836 (28%)] Loss: 12162.062500\n",
      "Train Epoch: 4468 [65792/118836 (55%)] Loss: 12197.853516\n",
      "Train Epoch: 4468 [98560/118836 (83%)] Loss: 12247.167969\n",
      "    epoch          : 4468\n",
      "    loss           : 12217.317801223893\n",
      "    val_loss       : 12214.73310250367\n",
      "    val_log_likelihood: -12139.789093678919\n",
      "    val_log_marginal: -12147.674749354794\n",
      "Train Epoch: 4469 [256/118836 (0%)] Loss: 12169.634766\n",
      "Train Epoch: 4469 [33024/118836 (28%)] Loss: 12213.386719\n",
      "Train Epoch: 4469 [65792/118836 (55%)] Loss: 12273.684570\n",
      "Train Epoch: 4469 [98560/118836 (83%)] Loss: 12167.871094\n",
      "    epoch          : 4469\n",
      "    loss           : 12217.571606828991\n",
      "    val_loss       : 12221.974229142435\n",
      "    val_log_likelihood: -12139.709782587624\n",
      "    val_log_marginal: -12147.602103114006\n",
      "Train Epoch: 4470 [256/118836 (0%)] Loss: 12279.503906\n",
      "Train Epoch: 4470 [33024/118836 (28%)] Loss: 12359.495117\n",
      "Train Epoch: 4470 [65792/118836 (55%)] Loss: 12191.636719\n",
      "Train Epoch: 4470 [98560/118836 (83%)] Loss: 12267.466797\n",
      "    epoch          : 4470\n",
      "    loss           : 12213.18417984905\n",
      "    val_loss       : 12218.000048314078\n",
      "    val_log_likelihood: -12138.771108127845\n",
      "    val_log_marginal: -12146.663136576559\n",
      "Train Epoch: 4471 [256/118836 (0%)] Loss: 12260.832031\n",
      "Train Epoch: 4471 [33024/118836 (28%)] Loss: 12155.552734\n",
      "Train Epoch: 4471 [65792/118836 (55%)] Loss: 12275.593750\n",
      "Train Epoch: 4471 [98560/118836 (83%)] Loss: 12200.099609\n",
      "    epoch          : 4471\n",
      "    loss           : 12216.355260675144\n",
      "    val_loss       : 12217.200429738921\n",
      "    val_log_likelihood: -12140.235154957609\n",
      "    val_log_marginal: -12148.125768236008\n",
      "Train Epoch: 4472 [256/118836 (0%)] Loss: 12220.929688\n",
      "Train Epoch: 4472 [33024/118836 (28%)] Loss: 12191.691406\n",
      "Train Epoch: 4472 [65792/118836 (55%)] Loss: 12262.096680\n",
      "Train Epoch: 4472 [98560/118836 (83%)] Loss: 12294.091797\n",
      "    epoch          : 4472\n",
      "    loss           : 12219.220245618795\n",
      "    val_loss       : 12214.11102863372\n",
      "    val_log_likelihood: -12140.309383077441\n",
      "    val_log_marginal: -12148.201819727825\n",
      "Train Epoch: 4473 [256/118836 (0%)] Loss: 12185.140625\n",
      "Train Epoch: 4473 [33024/118836 (28%)] Loss: 12348.904297\n",
      "Train Epoch: 4473 [65792/118836 (55%)] Loss: 12184.607422\n",
      "Train Epoch: 4473 [98560/118836 (83%)] Loss: 12352.535156\n",
      "    epoch          : 4473\n",
      "    loss           : 12214.786169160981\n",
      "    val_loss       : 12220.042174292603\n",
      "    val_log_likelihood: -12140.59750116315\n",
      "    val_log_marginal: -12148.489674981074\n",
      "Train Epoch: 4474 [256/118836 (0%)] Loss: 12200.906250\n",
      "Train Epoch: 4474 [33024/118836 (28%)] Loss: 12272.278320\n",
      "Train Epoch: 4474 [65792/118836 (55%)] Loss: 12194.706055\n",
      "Train Epoch: 4474 [98560/118836 (83%)] Loss: 12330.056641\n",
      "    epoch          : 4474\n",
      "    loss           : 12216.881372938638\n",
      "    val_loss       : 12218.390308275304\n",
      "    val_log_likelihood: -12139.858300700475\n",
      "    val_log_marginal: -12147.748547068142\n",
      "Train Epoch: 4475 [256/118836 (0%)] Loss: 12301.929688\n",
      "Train Epoch: 4475 [33024/118836 (28%)] Loss: 12213.982422\n",
      "Train Epoch: 4475 [65792/118836 (55%)] Loss: 12238.434570\n",
      "Train Epoch: 4475 [98560/118836 (83%)] Loss: 12229.500000\n",
      "    epoch          : 4475\n",
      "    loss           : 12215.73028571521\n",
      "    val_loss       : 12217.099377610999\n",
      "    val_log_likelihood: -12137.410411981751\n",
      "    val_log_marginal: -12145.295593967312\n",
      "Train Epoch: 4476 [256/118836 (0%)] Loss: 12210.933594\n",
      "Train Epoch: 4476 [33024/118836 (28%)] Loss: 12228.042969\n",
      "Train Epoch: 4476 [65792/118836 (55%)] Loss: 12134.761719\n",
      "Train Epoch: 4476 [98560/118836 (83%)] Loss: 12252.000000\n",
      "    epoch          : 4476\n",
      "    loss           : 12218.14163952647\n",
      "    val_loss       : 12224.65496843039\n",
      "    val_log_likelihood: -12139.333739790116\n",
      "    val_log_marginal: -12147.221490133374\n",
      "Train Epoch: 4477 [256/118836 (0%)] Loss: 12240.958008\n",
      "Train Epoch: 4477 [33024/118836 (28%)] Loss: 12179.827148\n",
      "Train Epoch: 4477 [65792/118836 (55%)] Loss: 12175.539062\n",
      "Train Epoch: 4477 [98560/118836 (83%)] Loss: 12233.511719\n",
      "    epoch          : 4477\n",
      "    loss           : 12216.392501550868\n",
      "    val_loss       : 12219.800593683673\n",
      "    val_log_likelihood: -12138.259728468776\n",
      "    val_log_marginal: -12146.154381674392\n",
      "Train Epoch: 4478 [256/118836 (0%)] Loss: 12151.349609\n",
      "Train Epoch: 4478 [33024/118836 (28%)] Loss: 12269.612305\n",
      "Train Epoch: 4478 [65792/118836 (55%)] Loss: 12243.129883\n",
      "Train Epoch: 4478 [98560/118836 (83%)] Loss: 12194.183594\n",
      "    epoch          : 4478\n",
      "    loss           : 12219.758943180055\n",
      "    val_loss       : 12215.749778835676\n",
      "    val_log_likelihood: -12140.187825036188\n",
      "    val_log_marginal: -12148.078951307069\n",
      "Train Epoch: 4479 [256/118836 (0%)] Loss: 12258.412109\n",
      "Train Epoch: 4479 [33024/118836 (28%)] Loss: 12236.302734\n",
      "Train Epoch: 4479 [65792/118836 (55%)] Loss: 12183.369141\n",
      "Train Epoch: 4479 [98560/118836 (83%)] Loss: 12172.916992\n",
      "    epoch          : 4479\n",
      "    loss           : 12217.809187603392\n",
      "    val_loss       : 12217.733276185692\n",
      "    val_log_likelihood: -12141.440730814464\n",
      "    val_log_marginal: -12149.335540354043\n",
      "Train Epoch: 4480 [256/118836 (0%)] Loss: 12206.929688\n",
      "Train Epoch: 4480 [33024/118836 (28%)] Loss: 12161.918945\n",
      "Train Epoch: 4480 [65792/118836 (55%)] Loss: 12216.154297\n",
      "Train Epoch: 4480 [98560/118836 (83%)] Loss: 12276.417969\n",
      "    epoch          : 4480\n",
      "    loss           : 12218.904130318186\n",
      "    val_loss       : 12219.468217128357\n",
      "    val_log_likelihood: -12139.920698601634\n",
      "    val_log_marginal: -12147.809387401774\n",
      "Train Epoch: 4481 [256/118836 (0%)] Loss: 12221.519531\n",
      "Train Epoch: 4481 [33024/118836 (28%)] Loss: 12232.173828\n",
      "Train Epoch: 4481 [65792/118836 (55%)] Loss: 12263.707031\n",
      "Train Epoch: 4481 [98560/118836 (83%)] Loss: 12267.773438\n",
      "    epoch          : 4481\n",
      "    loss           : 12217.557218323513\n",
      "    val_loss       : 12215.440952080635\n",
      "    val_log_likelihood: -12139.47967813017\n",
      "    val_log_marginal: -12147.371654480123\n",
      "Train Epoch: 4482 [256/118836 (0%)] Loss: 12265.832031\n",
      "Train Epoch: 4482 [33024/118836 (28%)] Loss: 12264.507812\n",
      "Train Epoch: 4482 [65792/118836 (55%)] Loss: 12308.518555\n",
      "Train Epoch: 4482 [98560/118836 (83%)] Loss: 12198.088867\n",
      "    epoch          : 4482\n",
      "    loss           : 12222.206532548853\n",
      "    val_loss       : 12213.55185527789\n",
      "    val_log_likelihood: -12138.790365714176\n",
      "    val_log_marginal: -12146.677214491881\n",
      "Train Epoch: 4483 [256/118836 (0%)] Loss: 12244.103516\n",
      "Train Epoch: 4483 [33024/118836 (28%)] Loss: 12308.267578\n",
      "Train Epoch: 4483 [65792/118836 (55%)] Loss: 12352.659180\n",
      "Train Epoch: 4483 [98560/118836 (83%)] Loss: 12221.464844\n",
      "    epoch          : 4483\n",
      "    loss           : 12219.860625549265\n",
      "    val_loss       : 12216.964231267111\n",
      "    val_log_likelihood: -12140.850641025641\n",
      "    val_log_marginal: -12148.746387027413\n",
      "Train Epoch: 4484 [256/118836 (0%)] Loss: 12290.076172\n",
      "Train Epoch: 4484 [33024/118836 (28%)] Loss: 12189.782227\n",
      "Train Epoch: 4484 [65792/118836 (55%)] Loss: 12260.659180\n",
      "Train Epoch: 4484 [98560/118836 (83%)] Loss: 12181.788086\n",
      "    epoch          : 4484\n",
      "    loss           : 12217.51455393145\n",
      "    val_loss       : 12215.79693320018\n",
      "    val_log_likelihood: -12139.138892066016\n",
      "    val_log_marginal: -12147.025611544064\n",
      "Train Epoch: 4485 [256/118836 (0%)] Loss: 12262.654297\n",
      "Train Epoch: 4485 [33024/118836 (28%)] Loss: 12234.981445\n",
      "Train Epoch: 4485 [65792/118836 (55%)] Loss: 12158.609375\n",
      "Train Epoch: 4485 [98560/118836 (83%)] Loss: 12381.729492\n",
      "    epoch          : 4485\n",
      "    loss           : 12219.684975315344\n",
      "    val_loss       : 12219.417031373936\n",
      "    val_log_likelihood: -12138.338592716089\n",
      "    val_log_marginal: -12146.233154786942\n",
      "Train Epoch: 4486 [256/118836 (0%)] Loss: 12239.263672\n",
      "Train Epoch: 4486 [33024/118836 (28%)] Loss: 12163.105469\n",
      "Train Epoch: 4486 [65792/118836 (55%)] Loss: 12256.008789\n",
      "Train Epoch: 4486 [98560/118836 (83%)] Loss: 12157.036133\n",
      "    epoch          : 4486\n",
      "    loss           : 12218.475442482164\n",
      "    val_loss       : 12219.211722594093\n",
      "    val_log_likelihood: -12139.037710821183\n",
      "    val_log_marginal: -12146.928078710514\n",
      "Train Epoch: 4487 [256/118836 (0%)] Loss: 12307.097656\n",
      "Train Epoch: 4487 [33024/118836 (28%)] Loss: 12267.226562\n",
      "Train Epoch: 4487 [65792/118836 (55%)] Loss: 12266.709961\n",
      "Train Epoch: 4487 [98560/118836 (83%)] Loss: 12209.027344\n",
      "    epoch          : 4487\n",
      "    loss           : 12219.475904673283\n",
      "    val_loss       : 12218.383471159721\n",
      "    val_log_likelihood: -12141.487051863627\n",
      "    val_log_marginal: -12149.376315251897\n",
      "Train Epoch: 4488 [256/118836 (0%)] Loss: 12305.328125\n",
      "Train Epoch: 4488 [33024/118836 (28%)] Loss: 12190.625000\n",
      "Train Epoch: 4488 [65792/118836 (55%)] Loss: 12231.794922\n",
      "Train Epoch: 4488 [98560/118836 (83%)] Loss: 12324.630859\n",
      "    epoch          : 4488\n",
      "    loss           : 12213.261688863471\n",
      "    val_loss       : 12217.343803048538\n",
      "    val_log_likelihood: -12139.9858633491\n",
      "    val_log_marginal: -12147.88256928698\n",
      "Train Epoch: 4489 [256/118836 (0%)] Loss: 12215.013672\n",
      "Train Epoch: 4489 [33024/118836 (28%)] Loss: 12216.455078\n",
      "Train Epoch: 4489 [65792/118836 (55%)] Loss: 12297.553711\n",
      "Train Epoch: 4489 [98560/118836 (83%)] Loss: 12287.561523\n",
      "    epoch          : 4489\n",
      "    loss           : 12213.844859355613\n",
      "    val_loss       : 12218.686943115623\n",
      "    val_log_likelihood: -12138.182146918942\n",
      "    val_log_marginal: -12146.074683710016\n",
      "Train Epoch: 4490 [256/118836 (0%)] Loss: 12183.501953\n",
      "Train Epoch: 4490 [33024/118836 (28%)] Loss: 12209.441406\n",
      "Train Epoch: 4490 [65792/118836 (55%)] Loss: 12289.662109\n",
      "Train Epoch: 4490 [98560/118836 (83%)] Loss: 12232.855469\n",
      "    epoch          : 4490\n",
      "    loss           : 12217.068226581887\n",
      "    val_loss       : 12218.989260681275\n",
      "    val_log_likelihood: -12140.245887775278\n",
      "    val_log_marginal: -12148.137779239867\n",
      "Train Epoch: 4491 [256/118836 (0%)] Loss: 12207.907227\n",
      "Train Epoch: 4491 [33024/118836 (28%)] Loss: 12252.234375\n",
      "Train Epoch: 4491 [65792/118836 (55%)] Loss: 12352.875000\n",
      "Train Epoch: 4491 [98560/118836 (83%)] Loss: 12320.265625\n",
      "    epoch          : 4491\n",
      "    loss           : 12221.853652456834\n",
      "    val_loss       : 12214.784690102484\n",
      "    val_log_likelihood: -12139.584683719759\n",
      "    val_log_marginal: -12147.474934672502\n",
      "Train Epoch: 4492 [256/118836 (0%)] Loss: 12274.103516\n",
      "Train Epoch: 4492 [33024/118836 (28%)] Loss: 12130.408203\n",
      "Train Epoch: 4492 [65792/118836 (55%)] Loss: 12190.993164\n",
      "Train Epoch: 4492 [98560/118836 (83%)] Loss: 12246.214844\n",
      "    epoch          : 4492\n",
      "    loss           : 12219.010702608042\n",
      "    val_loss       : 12219.780320114316\n",
      "    val_log_likelihood: -12139.530350657828\n",
      "    val_log_marginal: -12147.421695214503\n",
      "Train Epoch: 4493 [256/118836 (0%)] Loss: 12193.083008\n",
      "Train Epoch: 4493 [33024/118836 (28%)] Loss: 12273.818359\n",
      "Train Epoch: 4493 [65792/118836 (55%)] Loss: 12170.297852\n",
      "Train Epoch: 4493 [98560/118836 (83%)] Loss: 12181.416992\n",
      "    epoch          : 4493\n",
      "    loss           : 12216.934533479372\n",
      "    val_loss       : 12218.217856217096\n",
      "    val_log_likelihood: -12138.535470947065\n",
      "    val_log_marginal: -12146.422271054087\n",
      "Train Epoch: 4494 [256/118836 (0%)] Loss: 12276.248047\n",
      "Train Epoch: 4494 [33024/118836 (28%)] Loss: 12226.793945\n",
      "Train Epoch: 4494 [65792/118836 (55%)] Loss: 12222.916992\n",
      "Train Epoch: 4494 [98560/118836 (83%)] Loss: 12270.574219\n",
      "    epoch          : 4494\n",
      "    loss           : 12220.220861119727\n",
      "    val_loss       : 12218.921913644455\n",
      "    val_log_likelihood: -12138.017160844449\n",
      "    val_log_marginal: -12145.900042449934\n",
      "Train Epoch: 4495 [256/118836 (0%)] Loss: 12240.851562\n",
      "Train Epoch: 4495 [33024/118836 (28%)] Loss: 12199.864258\n",
      "Train Epoch: 4495 [65792/118836 (55%)] Loss: 12227.559570\n",
      "Train Epoch: 4495 [98560/118836 (83%)] Loss: 12212.193359\n",
      "    epoch          : 4495\n",
      "    loss           : 12219.003216759718\n",
      "    val_loss       : 12216.988533171805\n",
      "    val_log_likelihood: -12138.281975031017\n",
      "    val_log_marginal: -12146.175933142255\n",
      "Train Epoch: 4496 [256/118836 (0%)] Loss: 12159.528320\n",
      "Train Epoch: 4496 [33024/118836 (28%)] Loss: 12173.720703\n",
      "Train Epoch: 4496 [65792/118836 (55%)] Loss: 12209.151367\n",
      "Train Epoch: 4496 [98560/118836 (83%)] Loss: 12187.317383\n",
      "    epoch          : 4496\n",
      "    loss           : 12223.395477279777\n",
      "    val_loss       : 12216.979224705656\n",
      "    val_log_likelihood: -12139.185174505013\n",
      "    val_log_marginal: -12147.074401338747\n",
      "Train Epoch: 4497 [256/118836 (0%)] Loss: 12167.056641\n",
      "Train Epoch: 4497 [33024/118836 (28%)] Loss: 12355.195312\n",
      "Train Epoch: 4497 [65792/118836 (55%)] Loss: 12183.628906\n",
      "Train Epoch: 4497 [98560/118836 (83%)] Loss: 12244.804688\n",
      "    epoch          : 4497\n",
      "    loss           : 12220.511797908912\n",
      "    val_loss       : 12216.425754818836\n",
      "    val_log_likelihood: -12140.558701179953\n",
      "    val_log_marginal: -12148.448522028368\n",
      "Train Epoch: 4498 [256/118836 (0%)] Loss: 12313.082031\n",
      "Train Epoch: 4498 [33024/118836 (28%)] Loss: 12241.239258\n",
      "Train Epoch: 4498 [65792/118836 (55%)] Loss: 12243.667969\n",
      "Train Epoch: 4498 [98560/118836 (83%)] Loss: 12261.653320\n",
      "    epoch          : 4498\n",
      "    loss           : 12216.692254219655\n",
      "    val_loss       : 12216.778249319757\n",
      "    val_log_likelihood: -12138.753621601012\n",
      "    val_log_marginal: -12146.646352575333\n",
      "Train Epoch: 4499 [256/118836 (0%)] Loss: 12166.554688\n",
      "Train Epoch: 4499 [33024/118836 (28%)] Loss: 12191.205078\n",
      "Train Epoch: 4499 [65792/118836 (55%)] Loss: 12298.869141\n",
      "Train Epoch: 4499 [98560/118836 (83%)] Loss: 12173.568359\n",
      "    epoch          : 4499\n",
      "    loss           : 12221.022896149969\n",
      "    val_loss       : 12217.043558970545\n",
      "    val_log_likelihood: -12140.73126340855\n",
      "    val_log_marginal: -12148.624810589741\n",
      "Train Epoch: 4500 [256/118836 (0%)] Loss: 12250.177734\n",
      "Train Epoch: 4500 [33024/118836 (28%)] Loss: 12288.197266\n",
      "Train Epoch: 4500 [65792/118836 (55%)] Loss: 12292.406250\n",
      "Train Epoch: 4500 [98560/118836 (83%)] Loss: 12255.874023\n",
      "    epoch          : 4500\n",
      "    loss           : 12218.478171849152\n",
      "    val_loss       : 12220.494742919675\n",
      "    val_log_likelihood: -12139.70803204482\n",
      "    val_log_marginal: -12147.601880532433\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch4500.pth ...\n",
      "Train Epoch: 4501 [256/118836 (0%)] Loss: 12247.716797\n",
      "Train Epoch: 4501 [33024/118836 (28%)] Loss: 12290.738281\n",
      "Train Epoch: 4501 [65792/118836 (55%)] Loss: 12274.545898\n",
      "Train Epoch: 4501 [98560/118836 (83%)] Loss: 12166.687500\n",
      "    epoch          : 4501\n",
      "    loss           : 12216.212675765095\n",
      "    val_loss       : 12217.033496479777\n",
      "    val_log_likelihood: -12141.376278174112\n",
      "    val_log_marginal: -12149.262892221504\n",
      "Train Epoch: 4502 [256/118836 (0%)] Loss: 12333.953125\n",
      "Train Epoch: 4502 [33024/118836 (28%)] Loss: 12265.303711\n",
      "Train Epoch: 4502 [65792/118836 (55%)] Loss: 12205.077148\n",
      "Train Epoch: 4502 [98560/118836 (83%)] Loss: 12237.979492\n",
      "    epoch          : 4502\n",
      "    loss           : 12217.775636825372\n",
      "    val_loss       : 12216.312371647733\n",
      "    val_log_likelihood: -12140.247786619882\n",
      "    val_log_marginal: -12148.142269863125\n",
      "Train Epoch: 4503 [256/118836 (0%)] Loss: 12291.541992\n",
      "Train Epoch: 4503 [33024/118836 (28%)] Loss: 12185.068359\n",
      "Train Epoch: 4503 [65792/118836 (55%)] Loss: 12164.092773\n",
      "Train Epoch: 4503 [98560/118836 (83%)] Loss: 12267.621094\n",
      "    epoch          : 4503\n",
      "    loss           : 12218.617601872675\n",
      "    val_loss       : 12221.295831647714\n",
      "    val_log_likelihood: -12139.926078499793\n",
      "    val_log_marginal: -12147.815335139887\n",
      "Train Epoch: 4504 [256/118836 (0%)] Loss: 12263.494141\n",
      "Train Epoch: 4504 [33024/118836 (28%)] Loss: 12172.615234\n",
      "Train Epoch: 4504 [65792/118836 (55%)] Loss: 12148.810547\n",
      "Train Epoch: 4504 [98560/118836 (83%)] Loss: 12211.461914\n",
      "    epoch          : 4504\n",
      "    loss           : 12217.623808900693\n",
      "    val_loss       : 12218.192691645998\n",
      "    val_log_likelihood: -12139.322522810691\n",
      "    val_log_marginal: -12147.211744234031\n",
      "Train Epoch: 4505 [256/118836 (0%)] Loss: 12192.493164\n",
      "Train Epoch: 4505 [33024/118836 (28%)] Loss: 12148.009766\n",
      "Train Epoch: 4505 [65792/118836 (55%)] Loss: 12171.914062\n",
      "Train Epoch: 4505 [98560/118836 (83%)] Loss: 12235.323242\n",
      "    epoch          : 4505\n",
      "    loss           : 12213.89360024426\n",
      "    val_loss       : 12222.992090524293\n",
      "    val_log_likelihood: -12138.243100896918\n",
      "    val_log_marginal: -12146.137599210368\n",
      "Train Epoch: 4506 [256/118836 (0%)] Loss: 12179.716797\n",
      "Train Epoch: 4506 [33024/118836 (28%)] Loss: 12145.499023\n",
      "Train Epoch: 4506 [65792/118836 (55%)] Loss: 12263.265625\n",
      "Train Epoch: 4506 [98560/118836 (83%)] Loss: 12314.164062\n",
      "    epoch          : 4506\n",
      "    loss           : 12217.569482300712\n",
      "    val_loss       : 12216.557595459646\n",
      "    val_log_likelihood: -12138.511873190653\n",
      "    val_log_marginal: -12146.401928351088\n",
      "Train Epoch: 4507 [256/118836 (0%)] Loss: 12266.320312\n",
      "Train Epoch: 4507 [33024/118836 (28%)] Loss: 12280.326172\n",
      "Train Epoch: 4507 [65792/118836 (55%)] Loss: 12349.799805\n",
      "Train Epoch: 4507 [98560/118836 (83%)] Loss: 12207.310547\n",
      "    epoch          : 4507\n",
      "    loss           : 12216.557301036497\n",
      "    val_loss       : 12218.29084095806\n",
      "    val_log_likelihood: -12138.922078551488\n",
      "    val_log_marginal: -12146.813625544306\n",
      "Train Epoch: 4508 [256/118836 (0%)] Loss: 12179.417969\n",
      "Train Epoch: 4508 [33024/118836 (28%)] Loss: 12192.464844\n",
      "Train Epoch: 4508 [65792/118836 (55%)] Loss: 12299.577148\n",
      "Train Epoch: 4508 [98560/118836 (83%)] Loss: 12234.347656\n",
      "    epoch          : 4508\n",
      "    loss           : 12216.038220992297\n",
      "    val_loss       : 12216.489709113228\n",
      "    val_log_likelihood: -12141.011627959575\n",
      "    val_log_marginal: -12148.899927603941\n",
      "Train Epoch: 4509 [256/118836 (0%)] Loss: 12186.893555\n",
      "Train Epoch: 4509 [33024/118836 (28%)] Loss: 12190.776367\n",
      "Train Epoch: 4509 [65792/118836 (55%)] Loss: 12369.359375\n",
      "Train Epoch: 4509 [98560/118836 (83%)] Loss: 12267.778320\n",
      "    epoch          : 4509\n",
      "    loss           : 12220.3384159817\n",
      "    val_loss       : 12219.301000310674\n",
      "    val_log_likelihood: -12139.274523269489\n",
      "    val_log_marginal: -12147.163893370056\n",
      "Train Epoch: 4510 [256/118836 (0%)] Loss: 12260.138672\n",
      "Train Epoch: 4510 [33024/118836 (28%)] Loss: 12169.360352\n",
      "Train Epoch: 4510 [65792/118836 (55%)] Loss: 12307.943359\n",
      "Train Epoch: 4510 [98560/118836 (83%)] Loss: 12190.059570\n",
      "    epoch          : 4510\n",
      "    loss           : 12215.183099895317\n",
      "    val_loss       : 12217.573231307388\n",
      "    val_log_likelihood: -12141.165199480458\n",
      "    val_log_marginal: -12149.05544963962\n",
      "Train Epoch: 4511 [256/118836 (0%)] Loss: 12275.712891\n",
      "Train Epoch: 4511 [33024/118836 (28%)] Loss: 12321.583008\n",
      "Train Epoch: 4511 [65792/118836 (55%)] Loss: 12227.025391\n",
      "Train Epoch: 4511 [98560/118836 (83%)] Loss: 12195.189453\n",
      "    epoch          : 4511\n",
      "    loss           : 12217.749094518973\n",
      "    val_loss       : 12215.759465956664\n",
      "    val_log_likelihood: -12142.880408976944\n",
      "    val_log_marginal: -12150.767039573935\n",
      "Train Epoch: 4512 [256/118836 (0%)] Loss: 12357.945312\n",
      "Train Epoch: 4512 [33024/118836 (28%)] Loss: 12214.877930\n",
      "Train Epoch: 4512 [65792/118836 (55%)] Loss: 12170.180664\n",
      "Train Epoch: 4512 [98560/118836 (83%)] Loss: 12349.375000\n",
      "    epoch          : 4512\n",
      "    loss           : 12219.261956872931\n",
      "    val_loss       : 12216.39675219768\n",
      "    val_log_likelihood: -12138.832250633272\n",
      "    val_log_marginal: -12146.716473242725\n",
      "Train Epoch: 4513 [256/118836 (0%)] Loss: 12228.917969\n",
      "Train Epoch: 4513 [33024/118836 (28%)] Loss: 12202.918945\n",
      "Train Epoch: 4513 [65792/118836 (55%)] Loss: 12226.224609\n",
      "Train Epoch: 4513 [98560/118836 (83%)] Loss: 12223.074219\n",
      "    epoch          : 4513\n",
      "    loss           : 12218.975623093724\n",
      "    val_loss       : 12215.41568905425\n",
      "    val_log_likelihood: -12139.831308803763\n",
      "    val_log_marginal: -12147.722218403664\n",
      "Train Epoch: 4514 [256/118836 (0%)] Loss: 12213.419922\n",
      "Train Epoch: 4514 [33024/118836 (28%)] Loss: 12337.917969\n",
      "Train Epoch: 4514 [65792/118836 (55%)] Loss: 12184.975586\n",
      "Train Epoch: 4514 [98560/118836 (83%)] Loss: 12173.461914\n",
      "    epoch          : 4514\n",
      "    loss           : 12216.160747841708\n",
      "    val_loss       : 12221.24024874483\n",
      "    val_log_likelihood: -12138.880395406844\n",
      "    val_log_marginal: -12146.765770309854\n",
      "Train Epoch: 4515 [256/118836 (0%)] Loss: 12206.654297\n",
      "Train Epoch: 4515 [33024/118836 (28%)] Loss: 12283.862305\n",
      "Train Epoch: 4515 [65792/118836 (55%)] Loss: 12192.750000\n",
      "Train Epoch: 4515 [98560/118836 (83%)] Loss: 12294.124023\n",
      "    epoch          : 4515\n",
      "    loss           : 12215.703354883943\n",
      "    val_loss       : 12219.195081777114\n",
      "    val_log_likelihood: -12139.598453816428\n",
      "    val_log_marginal: -12147.490721187896\n",
      "Train Epoch: 4516 [256/118836 (0%)] Loss: 12166.842773\n",
      "Train Epoch: 4516 [33024/118836 (28%)] Loss: 12465.051758\n",
      "Train Epoch: 4516 [65792/118836 (55%)] Loss: 12187.643555\n",
      "Train Epoch: 4516 [98560/118836 (83%)] Loss: 12300.772461\n",
      "    epoch          : 4516\n",
      "    loss           : 12220.29964297715\n",
      "    val_loss       : 12218.058226158546\n",
      "    val_log_likelihood: -12139.881203150848\n",
      "    val_log_marginal: -12147.769284419584\n",
      "Train Epoch: 4517 [256/118836 (0%)] Loss: 12397.102539\n",
      "Train Epoch: 4517 [33024/118836 (28%)] Loss: 12285.525391\n",
      "Train Epoch: 4517 [65792/118836 (55%)] Loss: 12300.528320\n",
      "Train Epoch: 4517 [98560/118836 (83%)] Loss: 12210.652344\n",
      "    epoch          : 4517\n",
      "    loss           : 12214.048492749691\n",
      "    val_loss       : 12222.436671090663\n",
      "    val_log_likelihood: -12140.032015256667\n",
      "    val_log_marginal: -12147.931425200768\n",
      "Train Epoch: 4518 [256/118836 (0%)] Loss: 12286.429688\n",
      "Train Epoch: 4518 [33024/118836 (28%)] Loss: 12185.190430\n",
      "Train Epoch: 4518 [65792/118836 (55%)] Loss: 12249.355469\n",
      "Train Epoch: 4518 [98560/118836 (83%)] Loss: 12137.257812\n",
      "    epoch          : 4518\n",
      "    loss           : 12217.070440285102\n",
      "    val_loss       : 12216.627519802969\n",
      "    val_log_likelihood: -12139.503496400694\n",
      "    val_log_marginal: -12147.397555307982\n",
      "Train Epoch: 4519 [256/118836 (0%)] Loss: 12170.040039\n",
      "Train Epoch: 4519 [33024/118836 (28%)] Loss: 12210.419922\n",
      "Train Epoch: 4519 [65792/118836 (55%)] Loss: 12178.164062\n",
      "Train Epoch: 4519 [98560/118836 (83%)] Loss: 12453.535156\n",
      "    epoch          : 4519\n",
      "    loss           : 12219.495615727097\n",
      "    val_loss       : 12215.670377636934\n",
      "    val_log_likelihood: -12138.723631358562\n",
      "    val_log_marginal: -12146.611610336095\n",
      "Train Epoch: 4520 [256/118836 (0%)] Loss: 12231.209961\n",
      "Train Epoch: 4520 [33024/118836 (28%)] Loss: 12293.913086\n",
      "Train Epoch: 4520 [65792/118836 (55%)] Loss: 12311.688477\n",
      "Train Epoch: 4520 [98560/118836 (83%)] Loss: 12151.735352\n",
      "    epoch          : 4520\n",
      "    loss           : 12218.591555844188\n",
      "    val_loss       : 12218.135923620084\n",
      "    val_log_likelihood: -12140.143610260287\n",
      "    val_log_marginal: -12148.03296340877\n",
      "Train Epoch: 4521 [256/118836 (0%)] Loss: 12255.810547\n",
      "Train Epoch: 4521 [33024/118836 (28%)] Loss: 12281.792969\n",
      "Train Epoch: 4521 [65792/118836 (55%)] Loss: 12188.424805\n",
      "Train Epoch: 4521 [98560/118836 (83%)] Loss: 12321.806641\n",
      "    epoch          : 4521\n",
      "    loss           : 12217.071295847549\n",
      "    val_loss       : 12218.286141926574\n",
      "    val_log_likelihood: -12140.52533117504\n",
      "    val_log_marginal: -12148.416516045003\n",
      "Train Epoch: 4522 [256/118836 (0%)] Loss: 12284.898438\n",
      "Train Epoch: 4522 [33024/118836 (28%)] Loss: 12273.341797\n",
      "Train Epoch: 4522 [65792/118836 (55%)] Loss: 12199.110352\n",
      "Train Epoch: 4522 [98560/118836 (83%)] Loss: 12213.021484\n",
      "    epoch          : 4522\n",
      "    loss           : 12215.617195092795\n",
      "    val_loss       : 12219.746876064275\n",
      "    val_log_likelihood: -12137.62618673749\n",
      "    val_log_marginal: -12145.512443593267\n",
      "Train Epoch: 4523 [256/118836 (0%)] Loss: 12236.783203\n",
      "Train Epoch: 4523 [33024/118836 (28%)] Loss: 12322.941406\n",
      "Train Epoch: 4523 [65792/118836 (55%)] Loss: 12321.095703\n",
      "Train Epoch: 4523 [98560/118836 (83%)] Loss: 12190.864258\n",
      "    epoch          : 4523\n",
      "    loss           : 12213.713034080336\n",
      "    val_loss       : 12217.952128257048\n",
      "    val_log_likelihood: -12141.386384343983\n",
      "    val_log_marginal: -12149.27927739663\n",
      "Train Epoch: 4524 [256/118836 (0%)] Loss: 12268.734375\n",
      "Train Epoch: 4524 [33024/118836 (28%)] Loss: 12253.764648\n",
      "Train Epoch: 4524 [65792/118836 (55%)] Loss: 12197.656250\n",
      "Train Epoch: 4524 [98560/118836 (83%)] Loss: 12187.498047\n",
      "    epoch          : 4524\n",
      "    loss           : 12216.562918572943\n",
      "    val_loss       : 12217.440214778217\n",
      "    val_log_likelihood: -12140.86513857656\n",
      "    val_log_marginal: -12148.752164816686\n",
      "Train Epoch: 4525 [256/118836 (0%)] Loss: 12176.707031\n",
      "Train Epoch: 4525 [33024/118836 (28%)] Loss: 12197.682617\n",
      "Train Epoch: 4525 [65792/118836 (55%)] Loss: 12257.037109\n",
      "Train Epoch: 4525 [98560/118836 (83%)] Loss: 12290.523438\n",
      "    epoch          : 4525\n",
      "    loss           : 12213.082132864194\n",
      "    val_loss       : 12215.341923097869\n",
      "    val_log_likelihood: -12141.462997247208\n",
      "    val_log_marginal: -12149.358940988132\n",
      "Train Epoch: 4526 [256/118836 (0%)] Loss: 12410.652344\n",
      "Train Epoch: 4526 [33024/118836 (28%)] Loss: 12296.710938\n",
      "Train Epoch: 4526 [65792/118836 (55%)] Loss: 12133.892578\n",
      "Train Epoch: 4526 [98560/118836 (83%)] Loss: 12339.626953\n",
      "    epoch          : 4526\n",
      "    loss           : 12219.503184449959\n",
      "    val_loss       : 12218.074088938261\n",
      "    val_log_likelihood: -12138.920268397178\n",
      "    val_log_marginal: -12146.808204736326\n",
      "Train Epoch: 4527 [256/118836 (0%)] Loss: 12204.054688\n",
      "Train Epoch: 4527 [33024/118836 (28%)] Loss: 12272.101562\n",
      "Train Epoch: 4527 [65792/118836 (55%)] Loss: 12287.339844\n",
      "Train Epoch: 4527 [98560/118836 (83%)] Loss: 12243.361328\n",
      "    epoch          : 4527\n",
      "    loss           : 12214.28506061311\n",
      "    val_loss       : 12214.911951236232\n",
      "    val_log_likelihood: -12139.453363769127\n",
      "    val_log_marginal: -12147.34618290924\n",
      "Train Epoch: 4528 [256/118836 (0%)] Loss: 12388.315430\n",
      "Train Epoch: 4528 [33024/118836 (28%)] Loss: 12249.009766\n",
      "Train Epoch: 4528 [65792/118836 (55%)] Loss: 12178.833984\n",
      "Train Epoch: 4528 [98560/118836 (83%)] Loss: 12375.110352\n",
      "    epoch          : 4528\n",
      "    loss           : 12212.663799983198\n",
      "    val_loss       : 12216.75029997302\n",
      "    val_log_likelihood: -12138.980093310587\n",
      "    val_log_marginal: -12146.868739173065\n",
      "Train Epoch: 4529 [256/118836 (0%)] Loss: 12221.357422\n",
      "Train Epoch: 4529 [33024/118836 (28%)] Loss: 12146.650391\n",
      "Train Epoch: 4529 [65792/118836 (55%)] Loss: 12260.044922\n",
      "Train Epoch: 4529 [98560/118836 (83%)] Loss: 12190.078125\n",
      "    epoch          : 4529\n",
      "    loss           : 12216.281952252635\n",
      "    val_loss       : 12218.733887884848\n",
      "    val_log_likelihood: -12140.38592780707\n",
      "    val_log_marginal: -12148.278795199794\n",
      "Train Epoch: 4530 [256/118836 (0%)] Loss: 12218.648438\n",
      "Train Epoch: 4530 [33024/118836 (28%)] Loss: 12455.527344\n",
      "Train Epoch: 4530 [65792/118836 (55%)] Loss: 12330.654297\n",
      "Train Epoch: 4530 [98560/118836 (83%)] Loss: 12188.730469\n",
      "    epoch          : 4530\n",
      "    loss           : 12215.662407917183\n",
      "    val_loss       : 12222.072831267727\n",
      "    val_log_likelihood: -12140.573265773624\n",
      "    val_log_marginal: -12148.46028418105\n",
      "Train Epoch: 4531 [256/118836 (0%)] Loss: 12199.580078\n",
      "Train Epoch: 4531 [33024/118836 (28%)] Loss: 12283.982422\n",
      "Train Epoch: 4531 [65792/118836 (55%)] Loss: 12243.916016\n",
      "Train Epoch: 4531 [98560/118836 (83%)] Loss: 12133.627930\n",
      "    epoch          : 4531\n",
      "    loss           : 12217.296951089485\n",
      "    val_loss       : 12217.32733806642\n",
      "    val_log_likelihood: -12139.20584709729\n",
      "    val_log_marginal: -12147.092848947272\n",
      "Train Epoch: 4532 [256/118836 (0%)] Loss: 12152.098633\n",
      "Train Epoch: 4532 [33024/118836 (28%)] Loss: 12239.079102\n",
      "Train Epoch: 4532 [65792/118836 (55%)] Loss: 12280.742188\n",
      "Train Epoch: 4532 [98560/118836 (83%)] Loss: 12213.236328\n",
      "    epoch          : 4532\n",
      "    loss           : 12213.195134796319\n",
      "    val_loss       : 12220.838039309407\n",
      "    val_log_likelihood: -12138.589413707092\n",
      "    val_log_marginal: -12146.472607111713\n",
      "Train Epoch: 4533 [256/118836 (0%)] Loss: 12151.726562\n",
      "Train Epoch: 4533 [33024/118836 (28%)] Loss: 12225.941406\n",
      "Train Epoch: 4533 [65792/118836 (55%)] Loss: 12268.839844\n",
      "Train Epoch: 4533 [98560/118836 (83%)] Loss: 12341.818359\n",
      "    epoch          : 4533\n",
      "    loss           : 12220.932784713606\n",
      "    val_loss       : 12218.191947976393\n",
      "    val_log_likelihood: -12140.921218465674\n",
      "    val_log_marginal: -12148.809079523646\n",
      "Train Epoch: 4534 [256/118836 (0%)] Loss: 12223.736328\n",
      "Train Epoch: 4534 [33024/118836 (28%)] Loss: 12281.578125\n",
      "Train Epoch: 4534 [65792/118836 (55%)] Loss: 12238.581055\n",
      "Train Epoch: 4534 [98560/118836 (83%)] Loss: 12215.602539\n",
      "    epoch          : 4534\n",
      "    loss           : 12219.863559437035\n",
      "    val_loss       : 12218.013672017772\n",
      "    val_log_likelihood: -12138.01475102099\n",
      "    val_log_marginal: -12145.903593726245\n",
      "Train Epoch: 4535 [256/118836 (0%)] Loss: 12193.751953\n",
      "Train Epoch: 4535 [33024/118836 (28%)] Loss: 12418.443359\n",
      "Train Epoch: 4535 [65792/118836 (55%)] Loss: 12271.319336\n",
      "Train Epoch: 4535 [98560/118836 (83%)] Loss: 12201.526367\n",
      "    epoch          : 4535\n",
      "    loss           : 12220.988137471568\n",
      "    val_loss       : 12215.02369433343\n",
      "    val_log_likelihood: -12138.536066254395\n",
      "    val_log_marginal: -12146.421322434237\n",
      "Train Epoch: 4536 [256/118836 (0%)] Loss: 12308.704102\n",
      "Train Epoch: 4536 [33024/118836 (28%)] Loss: 12209.399414\n",
      "Train Epoch: 4536 [65792/118836 (55%)] Loss: 12190.304688\n",
      "Train Epoch: 4536 [98560/118836 (83%)] Loss: 12226.759766\n",
      "    epoch          : 4536\n",
      "    loss           : 12218.622238000156\n",
      "    val_loss       : 12215.25931026424\n",
      "    val_log_likelihood: -12140.949501460402\n",
      "    val_log_marginal: -12148.839178901279\n",
      "Train Epoch: 4537 [256/118836 (0%)] Loss: 12204.556641\n",
      "Train Epoch: 4537 [33024/118836 (28%)] Loss: 12343.965820\n",
      "Train Epoch: 4537 [65792/118836 (55%)] Loss: 12228.253906\n",
      "Train Epoch: 4537 [98560/118836 (83%)] Loss: 12196.480469\n",
      "    epoch          : 4537\n",
      "    loss           : 12217.20622140586\n",
      "    val_loss       : 12219.510444742213\n",
      "    val_log_likelihood: -12139.044570021711\n",
      "    val_log_marginal: -12146.930816625994\n",
      "Train Epoch: 4538 [256/118836 (0%)] Loss: 12217.819336\n",
      "Train Epoch: 4538 [33024/118836 (28%)] Loss: 12190.242188\n",
      "Train Epoch: 4538 [65792/118836 (55%)] Loss: 12154.141602\n",
      "Train Epoch: 4538 [98560/118836 (83%)] Loss: 12243.388672\n",
      "    epoch          : 4538\n",
      "    loss           : 12218.106112521971\n",
      "    val_loss       : 12216.919292518956\n",
      "    val_log_likelihood: -12139.070827356027\n",
      "    val_log_marginal: -12146.957751595488\n",
      "Train Epoch: 4539 [256/118836 (0%)] Loss: 12290.213867\n",
      "Train Epoch: 4539 [33024/118836 (28%)] Loss: 12191.313477\n",
      "Train Epoch: 4539 [65792/118836 (55%)] Loss: 12181.830078\n",
      "Train Epoch: 4539 [98560/118836 (83%)] Loss: 12204.515625\n",
      "    epoch          : 4539\n",
      "    loss           : 12214.40905594112\n",
      "    val_loss       : 12221.432332372528\n",
      "    val_log_likelihood: -12139.004747111507\n",
      "    val_log_marginal: -12146.895585089234\n",
      "Train Epoch: 4540 [256/118836 (0%)] Loss: 12275.064453\n",
      "Train Epoch: 4540 [33024/118836 (28%)] Loss: 12197.117188\n",
      "Train Epoch: 4540 [65792/118836 (55%)] Loss: 12182.707031\n",
      "Train Epoch: 4540 [98560/118836 (83%)] Loss: 12196.050781\n",
      "    epoch          : 4540\n",
      "    loss           : 12217.444955315601\n",
      "    val_loss       : 12217.818283719707\n",
      "    val_log_likelihood: -12141.379911245089\n",
      "    val_log_marginal: -12149.271014075468\n",
      "Train Epoch: 4541 [256/118836 (0%)] Loss: 12218.343750\n",
      "Train Epoch: 4541 [33024/118836 (28%)] Loss: 12255.428711\n",
      "Train Epoch: 4541 [65792/118836 (55%)] Loss: 12195.134766\n",
      "Train Epoch: 4541 [98560/118836 (83%)] Loss: 12309.812500\n",
      "    epoch          : 4541\n",
      "    loss           : 12220.17273169329\n",
      "    val_loss       : 12219.509814931514\n",
      "    val_log_likelihood: -12140.76911913901\n",
      "    val_log_marginal: -12148.661118672682\n",
      "Train Epoch: 4542 [256/118836 (0%)] Loss: 12138.459961\n",
      "Train Epoch: 4542 [33024/118836 (28%)] Loss: 12232.449219\n",
      "Train Epoch: 4542 [65792/118836 (55%)] Loss: 12253.849609\n",
      "Train Epoch: 4542 [98560/118836 (83%)] Loss: 12233.723633\n",
      "    epoch          : 4542\n",
      "    loss           : 12216.336216656327\n",
      "    val_loss       : 12216.954606559122\n",
      "    val_log_likelihood: -12138.459871116367\n",
      "    val_log_marginal: -12146.346869049243\n",
      "Train Epoch: 4543 [256/118836 (0%)] Loss: 12153.352539\n",
      "Train Epoch: 4543 [33024/118836 (28%)] Loss: 12200.872070\n",
      "Train Epoch: 4543 [65792/118836 (55%)] Loss: 12261.748047\n",
      "Train Epoch: 4543 [98560/118836 (83%)] Loss: 12189.363281\n",
      "    epoch          : 4543\n",
      "    loss           : 12217.651556199597\n",
      "    val_loss       : 12217.213762266681\n",
      "    val_log_likelihood: -12140.70731008323\n",
      "    val_log_marginal: -12148.594017252779\n",
      "Train Epoch: 4544 [256/118836 (0%)] Loss: 12202.037109\n",
      "Train Epoch: 4544 [33024/118836 (28%)] Loss: 12203.145508\n",
      "Train Epoch: 4544 [65792/118836 (55%)] Loss: 12315.484375\n",
      "Train Epoch: 4544 [98560/118836 (83%)] Loss: 12362.378906\n",
      "    epoch          : 4544\n",
      "    loss           : 12216.909229929177\n",
      "    val_loss       : 12219.371857165486\n",
      "    val_log_likelihood: -12139.704298813585\n",
      "    val_log_marginal: -12147.597014904633\n",
      "Train Epoch: 4545 [256/118836 (0%)] Loss: 12202.919922\n",
      "Train Epoch: 4545 [33024/118836 (28%)] Loss: 12143.549805\n",
      "Train Epoch: 4545 [65792/118836 (55%)] Loss: 12268.047852\n",
      "Train Epoch: 4545 [98560/118836 (83%)] Loss: 12202.679688\n",
      "    epoch          : 4545\n",
      "    loss           : 12217.280161645731\n",
      "    val_loss       : 12216.512206071711\n",
      "    val_log_likelihood: -12140.078318373915\n",
      "    val_log_marginal: -12147.971625176098\n",
      "Train Epoch: 4546 [256/118836 (0%)] Loss: 12265.849609\n",
      "Train Epoch: 4546 [33024/118836 (28%)] Loss: 12274.074219\n",
      "Train Epoch: 4546 [65792/118836 (55%)] Loss: 12211.991211\n",
      "Train Epoch: 4546 [98560/118836 (83%)] Loss: 12197.061523\n",
      "    epoch          : 4546\n",
      "    loss           : 12213.765809488732\n",
      "    val_loss       : 12216.91377509754\n",
      "    val_log_likelihood: -12138.534286794355\n",
      "    val_log_marginal: -12146.42657012836\n",
      "Train Epoch: 4547 [256/118836 (0%)] Loss: 12187.568359\n",
      "Train Epoch: 4547 [33024/118836 (28%)] Loss: 12230.166016\n",
      "Train Epoch: 4547 [65792/118836 (55%)] Loss: 12200.084961\n",
      "Train Epoch: 4547 [98560/118836 (83%)] Loss: 12290.610352\n",
      "    epoch          : 4547\n",
      "    loss           : 12212.373224255583\n",
      "    val_loss       : 12217.188937447429\n",
      "    val_log_likelihood: -12139.191691383634\n",
      "    val_log_marginal: -12147.080002413959\n",
      "Train Epoch: 4548 [256/118836 (0%)] Loss: 12253.406250\n",
      "Train Epoch: 4548 [33024/118836 (28%)] Loss: 12284.507812\n",
      "Train Epoch: 4548 [65792/118836 (55%)] Loss: 12302.764648\n",
      "Train Epoch: 4548 [98560/118836 (83%)] Loss: 12320.395508\n",
      "    epoch          : 4548\n",
      "    loss           : 12216.697441390095\n",
      "    val_loss       : 12214.091290075325\n",
      "    val_log_likelihood: -12139.164263466708\n",
      "    val_log_marginal: -12147.056768224871\n",
      "Train Epoch: 4549 [256/118836 (0%)] Loss: 12286.845703\n",
      "Train Epoch: 4549 [33024/118836 (28%)] Loss: 12214.114258\n",
      "Train Epoch: 4549 [65792/118836 (55%)] Loss: 12316.207031\n",
      "Train Epoch: 4549 [98560/118836 (83%)] Loss: 12258.036133\n",
      "    epoch          : 4549\n",
      "    loss           : 12215.976430029983\n",
      "    val_loss       : 12217.115002597104\n",
      "    val_log_likelihood: -12139.388051366057\n",
      "    val_log_marginal: -12147.269282871255\n",
      "Train Epoch: 4550 [256/118836 (0%)] Loss: 12268.224609\n",
      "Train Epoch: 4550 [33024/118836 (28%)] Loss: 12285.973633\n",
      "Train Epoch: 4550 [65792/118836 (55%)] Loss: 12174.430664\n",
      "Train Epoch: 4550 [98560/118836 (83%)] Loss: 12212.440430\n",
      "    epoch          : 4550\n",
      "    loss           : 12214.997948007134\n",
      "    val_loss       : 12217.258100070378\n",
      "    val_log_likelihood: -12139.487074803557\n",
      "    val_log_marginal: -12147.382522496933\n",
      "Train Epoch: 4551 [256/118836 (0%)] Loss: 12173.751953\n",
      "Train Epoch: 4551 [33024/118836 (28%)] Loss: 12275.662109\n",
      "Train Epoch: 4551 [65792/118836 (55%)] Loss: 12297.683594\n",
      "Train Epoch: 4551 [98560/118836 (83%)] Loss: 12306.892578\n",
      "    epoch          : 4551\n",
      "    loss           : 12217.74494077621\n",
      "    val_loss       : 12220.233123851445\n",
      "    val_log_likelihood: -12141.562699028123\n",
      "    val_log_marginal: -12149.454524890343\n",
      "Train Epoch: 4552 [256/118836 (0%)] Loss: 12161.367188\n",
      "Train Epoch: 4552 [33024/118836 (28%)] Loss: 12354.821289\n",
      "Train Epoch: 4552 [65792/118836 (55%)] Loss: 12161.176758\n",
      "Train Epoch: 4552 [98560/118836 (83%)] Loss: 12300.245117\n",
      "    epoch          : 4552\n",
      "    loss           : 12216.529943554848\n",
      "    val_loss       : 12222.217049513027\n",
      "    val_log_likelihood: -12138.678058603442\n",
      "    val_log_marginal: -12146.571577674205\n",
      "Train Epoch: 4553 [256/118836 (0%)] Loss: 12188.588867\n",
      "Train Epoch: 4553 [33024/118836 (28%)] Loss: 12184.141602\n",
      "Train Epoch: 4553 [65792/118836 (55%)] Loss: 12240.365234\n",
      "Train Epoch: 4553 [98560/118836 (83%)] Loss: 12236.094727\n",
      "    epoch          : 4553\n",
      "    loss           : 12219.442083139475\n",
      "    val_loss       : 12220.832461040805\n",
      "    val_log_likelihood: -12141.235312629238\n",
      "    val_log_marginal: -12149.125689481842\n",
      "Train Epoch: 4554 [256/118836 (0%)] Loss: 12255.486328\n",
      "Train Epoch: 4554 [33024/118836 (28%)] Loss: 12235.786133\n",
      "Train Epoch: 4554 [65792/118836 (55%)] Loss: 12346.791016\n",
      "Train Epoch: 4554 [98560/118836 (83%)] Loss: 12231.714844\n",
      "    epoch          : 4554\n",
      "    loss           : 12213.967999444272\n",
      "    val_loss       : 12215.625763641336\n",
      "    val_log_likelihood: -12138.099480620605\n",
      "    val_log_marginal: -12145.990686748453\n",
      "Train Epoch: 4555 [256/118836 (0%)] Loss: 12203.434570\n",
      "Train Epoch: 4555 [33024/118836 (28%)] Loss: 12154.373047\n",
      "Train Epoch: 4555 [65792/118836 (55%)] Loss: 12173.285156\n",
      "Train Epoch: 4555 [98560/118836 (83%)] Loss: 12268.978516\n",
      "    epoch          : 4555\n",
      "    loss           : 12216.119306858714\n",
      "    val_loss       : 12213.376305695017\n",
      "    val_log_likelihood: -12141.37203008685\n",
      "    val_log_marginal: -12149.258166028203\n",
      "Train Epoch: 4556 [256/118836 (0%)] Loss: 12167.108398\n",
      "Train Epoch: 4556 [33024/118836 (28%)] Loss: 12144.155273\n",
      "Train Epoch: 4556 [65792/118836 (55%)] Loss: 12281.251953\n",
      "Train Epoch: 4556 [98560/118836 (83%)] Loss: 12288.892578\n",
      "    epoch          : 4556\n",
      "    loss           : 12215.374304532414\n",
      "    val_loss       : 12215.016633157733\n",
      "    val_log_likelihood: -12139.849838612747\n",
      "    val_log_marginal: -12147.748879282493\n",
      "Train Epoch: 4557 [256/118836 (0%)] Loss: 12238.870117\n",
      "Train Epoch: 4557 [33024/118836 (28%)] Loss: 12189.400391\n",
      "Train Epoch: 4557 [65792/118836 (55%)] Loss: 12242.549805\n",
      "Train Epoch: 4557 [98560/118836 (83%)] Loss: 12242.111328\n",
      "    epoch          : 4557\n",
      "    loss           : 12215.28873100186\n",
      "    val_loss       : 12219.34256309476\n",
      "    val_log_likelihood: -12141.04012274478\n",
      "    val_log_marginal: -12148.928194548771\n",
      "Train Epoch: 4558 [256/118836 (0%)] Loss: 12166.117188\n",
      "Train Epoch: 4558 [33024/118836 (28%)] Loss: 12359.484375\n",
      "Train Epoch: 4558 [65792/118836 (55%)] Loss: 12288.672852\n",
      "Train Epoch: 4558 [98560/118836 (83%)] Loss: 12145.789062\n",
      "    epoch          : 4558\n",
      "    loss           : 12216.981767925456\n",
      "    val_loss       : 12219.164339209296\n",
      "    val_log_likelihood: -12140.450699829404\n",
      "    val_log_marginal: -12148.343554426498\n",
      "Train Epoch: 4559 [256/118836 (0%)] Loss: 12265.837891\n",
      "Train Epoch: 4559 [33024/118836 (28%)] Loss: 12338.391602\n",
      "Train Epoch: 4559 [65792/118836 (55%)] Loss: 12228.776367\n",
      "Train Epoch: 4559 [98560/118836 (83%)] Loss: 12267.376953\n",
      "    epoch          : 4559\n",
      "    loss           : 12213.356786503567\n",
      "    val_loss       : 12221.196234147532\n",
      "    val_log_likelihood: -12139.491099145731\n",
      "    val_log_marginal: -12147.378422802083\n",
      "Train Epoch: 4560 [256/118836 (0%)] Loss: 12370.123047\n",
      "Train Epoch: 4560 [33024/118836 (28%)] Loss: 12345.003906\n",
      "Train Epoch: 4560 [65792/118836 (55%)] Loss: 12199.333984\n",
      "Train Epoch: 4560 [98560/118836 (83%)] Loss: 12316.389648\n",
      "    epoch          : 4560\n",
      "    loss           : 12213.416534519749\n",
      "    val_loss       : 12213.803615382461\n",
      "    val_log_likelihood: -12140.193649839744\n",
      "    val_log_marginal: -12148.084238773243\n",
      "Train Epoch: 4561 [256/118836 (0%)] Loss: 12206.550781\n",
      "Train Epoch: 4561 [33024/118836 (28%)] Loss: 12200.185547\n",
      "Train Epoch: 4561 [65792/118836 (55%)] Loss: 12249.833984\n",
      "Train Epoch: 4561 [98560/118836 (83%)] Loss: 12179.320312\n",
      "    epoch          : 4561\n",
      "    loss           : 12216.83989221464\n",
      "    val_loss       : 12213.569031171837\n",
      "    val_log_likelihood: -12139.098869319945\n",
      "    val_log_marginal: -12146.988131581047\n",
      "Train Epoch: 4562 [256/118836 (0%)] Loss: 12155.722656\n",
      "Train Epoch: 4562 [33024/118836 (28%)] Loss: 12235.076172\n",
      "Train Epoch: 4562 [65792/118836 (55%)] Loss: 12280.119141\n",
      "Train Epoch: 4562 [98560/118836 (83%)] Loss: 12195.710938\n",
      "    epoch          : 4562\n",
      "    loss           : 12221.935319414288\n",
      "    val_loss       : 12222.901470107407\n",
      "    val_log_likelihood: -12139.458598111818\n",
      "    val_log_marginal: -12147.348204435608\n",
      "Train Epoch: 4563 [256/118836 (0%)] Loss: 12333.103516\n",
      "Train Epoch: 4563 [33024/118836 (28%)] Loss: 12169.738281\n",
      "Train Epoch: 4563 [65792/118836 (55%)] Loss: 12290.754883\n",
      "Train Epoch: 4563 [98560/118836 (83%)] Loss: 12367.746094\n",
      "    epoch          : 4563\n",
      "    loss           : 12218.919019624947\n",
      "    val_loss       : 12215.67450275524\n",
      "    val_log_likelihood: -12141.252184139785\n",
      "    val_log_marginal: -12149.147506478188\n",
      "Train Epoch: 4564 [256/118836 (0%)] Loss: 12359.181641\n",
      "Train Epoch: 4564 [33024/118836 (28%)] Loss: 12211.304688\n",
      "Train Epoch: 4564 [65792/118836 (55%)] Loss: 12389.183594\n",
      "Train Epoch: 4564 [98560/118836 (83%)] Loss: 12259.288086\n",
      "    epoch          : 4564\n",
      "    loss           : 12216.445275505324\n",
      "    val_loss       : 12217.1885559521\n",
      "    val_log_likelihood: -12139.22511324571\n",
      "    val_log_marginal: -12147.119225203296\n",
      "Train Epoch: 4565 [256/118836 (0%)] Loss: 12210.047852\n",
      "Train Epoch: 4565 [33024/118836 (28%)] Loss: 12397.241211\n",
      "Train Epoch: 4565 [65792/118836 (55%)] Loss: 12222.293945\n",
      "Train Epoch: 4565 [98560/118836 (83%)] Loss: 12223.246094\n",
      "    epoch          : 4565\n",
      "    loss           : 12223.657919122208\n",
      "    val_loss       : 12217.381154417168\n",
      "    val_log_likelihood: -12140.956730607682\n",
      "    val_log_marginal: -12148.84709223306\n",
      "Train Epoch: 4566 [256/118836 (0%)] Loss: 12213.730469\n",
      "Train Epoch: 4566 [33024/118836 (28%)] Loss: 12236.345703\n",
      "Train Epoch: 4566 [65792/118836 (55%)] Loss: 12138.730469\n",
      "Train Epoch: 4566 [98560/118836 (83%)] Loss: 12209.621094\n",
      "    epoch          : 4566\n",
      "    loss           : 12219.550480123036\n",
      "    val_loss       : 12214.303446975826\n",
      "    val_log_likelihood: -12140.107233186\n",
      "    val_log_marginal: -12148.00391356612\n",
      "Train Epoch: 4567 [256/118836 (0%)] Loss: 12180.859375\n",
      "Train Epoch: 4567 [33024/118836 (28%)] Loss: 12158.088867\n",
      "Train Epoch: 4567 [65792/118836 (55%)] Loss: 12261.177734\n",
      "Train Epoch: 4567 [98560/118836 (83%)] Loss: 12202.680664\n",
      "    epoch          : 4567\n",
      "    loss           : 12216.251083669355\n",
      "    val_loss       : 12220.72640775513\n",
      "    val_log_likelihood: -12140.228558273884\n",
      "    val_log_marginal: -12148.12089530831\n",
      "Train Epoch: 4568 [256/118836 (0%)] Loss: 12119.563477\n",
      "Train Epoch: 4568 [33024/118836 (28%)] Loss: 12268.148438\n",
      "Train Epoch: 4568 [65792/118836 (55%)] Loss: 12249.483398\n",
      "Train Epoch: 4568 [98560/118836 (83%)] Loss: 12286.931641\n",
      "    epoch          : 4568\n",
      "    loss           : 12220.43497709238\n",
      "    val_loss       : 12221.18938725994\n",
      "    val_log_likelihood: -12139.928396401985\n",
      "    val_log_marginal: -12147.820549364622\n",
      "Train Epoch: 4569 [256/118836 (0%)] Loss: 12202.689453\n",
      "Train Epoch: 4569 [33024/118836 (28%)] Loss: 12321.123047\n",
      "Train Epoch: 4569 [65792/118836 (55%)] Loss: 12281.750000\n",
      "Train Epoch: 4569 [98560/118836 (83%)] Loss: 12276.629883\n",
      "    epoch          : 4569\n",
      "    loss           : 12216.57937377223\n",
      "    val_loss       : 12221.797572191173\n",
      "    val_log_likelihood: -12140.336417138387\n",
      "    val_log_marginal: -12148.227734040662\n",
      "Train Epoch: 4570 [256/118836 (0%)] Loss: 12205.925781\n",
      "Train Epoch: 4570 [33024/118836 (28%)] Loss: 12212.496094\n",
      "Train Epoch: 4570 [65792/118836 (55%)] Loss: 12186.452148\n",
      "Train Epoch: 4570 [98560/118836 (83%)] Loss: 12179.091797\n",
      "    epoch          : 4570\n",
      "    loss           : 12217.758638660565\n",
      "    val_loss       : 12216.93977384022\n",
      "    val_log_likelihood: -12139.752469919615\n",
      "    val_log_marginal: -12147.64857672157\n",
      "Train Epoch: 4571 [256/118836 (0%)] Loss: 12218.383789\n",
      "Train Epoch: 4571 [33024/118836 (28%)] Loss: 12251.353516\n",
      "Train Epoch: 4571 [65792/118836 (55%)] Loss: 12381.732422\n",
      "Train Epoch: 4571 [98560/118836 (83%)] Loss: 12208.902344\n",
      "    epoch          : 4571\n",
      "    loss           : 12215.457058874845\n",
      "    val_loss       : 12222.613252079254\n",
      "    val_log_likelihood: -12140.655070370658\n",
      "    val_log_marginal: -12148.54191069646\n",
      "Train Epoch: 4572 [256/118836 (0%)] Loss: 12286.000000\n",
      "Train Epoch: 4572 [33024/118836 (28%)] Loss: 12291.793945\n",
      "Train Epoch: 4572 [65792/118836 (55%)] Loss: 12192.813477\n",
      "Train Epoch: 4572 [98560/118836 (83%)] Loss: 12279.344727\n",
      "    epoch          : 4572\n",
      "    loss           : 12222.65257541098\n",
      "    val_loss       : 12223.057635206684\n",
      "    val_log_likelihood: -12139.500027140199\n",
      "    val_log_marginal: -12147.39459854067\n",
      "Train Epoch: 4573 [256/118836 (0%)] Loss: 12247.295898\n",
      "Train Epoch: 4573 [33024/118836 (28%)] Loss: 12213.506836\n",
      "Train Epoch: 4573 [65792/118836 (55%)] Loss: 12246.773438\n",
      "Train Epoch: 4573 [98560/118836 (83%)] Loss: 12225.527344\n",
      "    epoch          : 4573\n",
      "    loss           : 12216.61909490669\n",
      "    val_loss       : 12220.473625746301\n",
      "    val_log_likelihood: -12140.00697341553\n",
      "    val_log_marginal: -12147.895332196445\n",
      "Train Epoch: 4574 [256/118836 (0%)] Loss: 12333.308594\n",
      "Train Epoch: 4574 [33024/118836 (28%)] Loss: 12259.311523\n",
      "Train Epoch: 4574 [65792/118836 (55%)] Loss: 12202.060547\n",
      "Train Epoch: 4574 [98560/118836 (83%)] Loss: 12149.810547\n",
      "    epoch          : 4574\n",
      "    loss           : 12217.438466869571\n",
      "    val_loss       : 12214.184373855815\n",
      "    val_log_likelihood: -12139.927357481649\n",
      "    val_log_marginal: -12147.818845521855\n",
      "Train Epoch: 4575 [256/118836 (0%)] Loss: 12216.741211\n",
      "Train Epoch: 4575 [33024/118836 (28%)] Loss: 12125.677734\n",
      "Train Epoch: 4575 [65792/118836 (55%)] Loss: 12166.505859\n",
      "Train Epoch: 4575 [98560/118836 (83%)] Loss: 12153.925781\n",
      "    epoch          : 4575\n",
      "    loss           : 12214.79753783473\n",
      "    val_loss       : 12211.411412597747\n",
      "    val_log_likelihood: -12139.921784371123\n",
      "    val_log_marginal: -12147.814570559085\n",
      "Train Epoch: 4576 [256/118836 (0%)] Loss: 12262.411133\n",
      "Train Epoch: 4576 [33024/118836 (28%)] Loss: 12289.310547\n",
      "Train Epoch: 4576 [65792/118836 (55%)] Loss: 12220.412109\n",
      "Train Epoch: 4576 [98560/118836 (83%)] Loss: 12202.196289\n",
      "    epoch          : 4576\n",
      "    loss           : 12218.237357998603\n",
      "    val_loss       : 12216.3352168627\n",
      "    val_log_likelihood: -12139.716271841398\n",
      "    val_log_marginal: -12147.61092602359\n",
      "Train Epoch: 4577 [256/118836 (0%)] Loss: 12156.499023\n",
      "Train Epoch: 4577 [33024/118836 (28%)] Loss: 12221.275391\n",
      "Train Epoch: 4577 [65792/118836 (55%)] Loss: 12301.521484\n",
      "Train Epoch: 4577 [98560/118836 (83%)] Loss: 12278.588867\n",
      "    epoch          : 4577\n",
      "    loss           : 12215.27510646066\n",
      "    val_loss       : 12218.06093828186\n",
      "    val_log_likelihood: -12137.709627985421\n",
      "    val_log_marginal: -12145.595553110574\n",
      "Train Epoch: 4578 [256/118836 (0%)] Loss: 12353.773438\n",
      "Train Epoch: 4578 [33024/118836 (28%)] Loss: 12242.184570\n",
      "Train Epoch: 4578 [65792/118836 (55%)] Loss: 12186.805664\n",
      "Train Epoch: 4578 [98560/118836 (83%)] Loss: 12255.949219\n",
      "    epoch          : 4578\n",
      "    loss           : 12222.348693231752\n",
      "    val_loss       : 12221.039357288475\n",
      "    val_log_likelihood: -12138.988976717586\n",
      "    val_log_marginal: -12146.883697089588\n",
      "Train Epoch: 4579 [256/118836 (0%)] Loss: 12180.666016\n",
      "Train Epoch: 4579 [33024/118836 (28%)] Loss: 12337.671875\n",
      "Train Epoch: 4579 [65792/118836 (55%)] Loss: 12249.615234\n",
      "Train Epoch: 4579 [98560/118836 (83%)] Loss: 12183.427734\n",
      "    epoch          : 4579\n",
      "    loss           : 12216.342758251913\n",
      "    val_loss       : 12216.980466881829\n",
      "    val_log_likelihood: -12140.64109381462\n",
      "    val_log_marginal: -12148.532045624239\n",
      "Train Epoch: 4580 [256/118836 (0%)] Loss: 12188.189453\n",
      "Train Epoch: 4580 [33024/118836 (28%)] Loss: 12197.177734\n",
      "Train Epoch: 4580 [65792/118836 (55%)] Loss: 12330.301758\n",
      "Train Epoch: 4580 [98560/118836 (83%)] Loss: 12221.232422\n",
      "    epoch          : 4580\n",
      "    loss           : 12215.711898392266\n",
      "    val_loss       : 12218.090107141381\n",
      "    val_log_likelihood: -12142.092907361455\n",
      "    val_log_marginal: -12149.980681691497\n",
      "Train Epoch: 4581 [256/118836 (0%)] Loss: 12287.478516\n",
      "Train Epoch: 4581 [33024/118836 (28%)] Loss: 12240.355469\n",
      "Train Epoch: 4581 [65792/118836 (55%)] Loss: 12309.558594\n",
      "Train Epoch: 4581 [98560/118836 (83%)] Loss: 12165.174805\n",
      "    epoch          : 4581\n",
      "    loss           : 12217.81465247622\n",
      "    val_loss       : 12215.655197044109\n",
      "    val_log_likelihood: -12139.838803052626\n",
      "    val_log_marginal: -12147.73754106587\n",
      "Train Epoch: 4582 [256/118836 (0%)] Loss: 12188.645508\n",
      "Train Epoch: 4582 [33024/118836 (28%)] Loss: 12203.890625\n",
      "Train Epoch: 4582 [65792/118836 (55%)] Loss: 12169.338867\n",
      "Train Epoch: 4582 [98560/118836 (83%)] Loss: 12228.698242\n",
      "    epoch          : 4582\n",
      "    loss           : 12212.670314277038\n",
      "    val_loss       : 12222.486625803533\n",
      "    val_log_likelihood: -12140.113764119365\n",
      "    val_log_marginal: -12148.00132994207\n",
      "Train Epoch: 4583 [256/118836 (0%)] Loss: 12205.110352\n",
      "Train Epoch: 4583 [33024/118836 (28%)] Loss: 12170.811523\n",
      "Train Epoch: 4583 [65792/118836 (55%)] Loss: 12192.611328\n",
      "Train Epoch: 4583 [98560/118836 (83%)] Loss: 12158.509766\n",
      "    epoch          : 4583\n",
      "    loss           : 12216.53172398418\n",
      "    val_loss       : 12219.684666974388\n",
      "    val_log_likelihood: -12140.033624444273\n",
      "    val_log_marginal: -12147.92269876693\n",
      "Train Epoch: 4584 [256/118836 (0%)] Loss: 12313.470703\n",
      "Train Epoch: 4584 [33024/118836 (28%)] Loss: 12326.950195\n",
      "Train Epoch: 4584 [65792/118836 (55%)] Loss: 12290.093750\n",
      "Train Epoch: 4584 [98560/118836 (83%)] Loss: 12251.980469\n",
      "    epoch          : 4584\n",
      "    loss           : 12222.123843956782\n",
      "    val_loss       : 12219.905005789284\n",
      "    val_log_likelihood: -12141.820269043374\n",
      "    val_log_marginal: -12149.710882611713\n",
      "Train Epoch: 4585 [256/118836 (0%)] Loss: 12261.654297\n",
      "Train Epoch: 4585 [33024/118836 (28%)] Loss: 12202.782227\n",
      "Train Epoch: 4585 [65792/118836 (55%)] Loss: 12172.538086\n",
      "Train Epoch: 4585 [98560/118836 (83%)] Loss: 12273.398438\n",
      "    epoch          : 4585\n",
      "    loss           : 12217.697173057537\n",
      "    val_loss       : 12218.200877669084\n",
      "    val_log_likelihood: -12139.349737160102\n",
      "    val_log_marginal: -12147.238418893528\n",
      "Train Epoch: 4586 [256/118836 (0%)] Loss: 12153.332031\n",
      "Train Epoch: 4586 [33024/118836 (28%)] Loss: 12194.849609\n",
      "Train Epoch: 4586 [65792/118836 (55%)] Loss: 12255.983398\n",
      "Train Epoch: 4586 [98560/118836 (83%)] Loss: 12286.573242\n",
      "    epoch          : 4586\n",
      "    loss           : 12218.386406960815\n",
      "    val_loss       : 12215.65433150545\n",
      "    val_log_likelihood: -12140.43295110887\n",
      "    val_log_marginal: -12148.327055204523\n",
      "Train Epoch: 4587 [256/118836 (0%)] Loss: 12289.620117\n",
      "Train Epoch: 4587 [33024/118836 (28%)] Loss: 12227.743164\n",
      "Train Epoch: 4587 [65792/118836 (55%)] Loss: 12251.501953\n",
      "Train Epoch: 4587 [98560/118836 (83%)] Loss: 12238.155273\n",
      "    epoch          : 4587\n",
      "    loss           : 12214.530314309346\n",
      "    val_loss       : 12212.600733597077\n",
      "    val_log_likelihood: -12139.236474326666\n",
      "    val_log_marginal: -12147.127285692233\n",
      "Train Epoch: 4588 [256/118836 (0%)] Loss: 12255.579102\n",
      "Train Epoch: 4588 [33024/118836 (28%)] Loss: 12194.733398\n",
      "Train Epoch: 4588 [65792/118836 (55%)] Loss: 12233.258789\n",
      "Train Epoch: 4588 [98560/118836 (83%)] Loss: 12254.801758\n",
      "    epoch          : 4588\n",
      "    loss           : 12217.683275660413\n",
      "    val_loss       : 12220.779866630222\n",
      "    val_log_likelihood: -12139.003621601012\n",
      "    val_log_marginal: -12146.897415335194\n",
      "Train Epoch: 4589 [256/118836 (0%)] Loss: 12250.310547\n",
      "Train Epoch: 4589 [33024/118836 (28%)] Loss: 12207.744141\n",
      "Train Epoch: 4589 [65792/118836 (55%)] Loss: 12359.024414\n",
      "Train Epoch: 4589 [98560/118836 (83%)] Loss: 12286.756836\n",
      "    epoch          : 4589\n",
      "    loss           : 12218.826776229063\n",
      "    val_loss       : 12220.507247753902\n",
      "    val_log_likelihood: -12141.119920905707\n",
      "    val_log_marginal: -12149.016828775595\n",
      "Train Epoch: 4590 [256/118836 (0%)] Loss: 12188.444336\n",
      "Train Epoch: 4590 [33024/118836 (28%)] Loss: 12161.649414\n",
      "Train Epoch: 4590 [65792/118836 (55%)] Loss: 12209.801758\n",
      "Train Epoch: 4590 [98560/118836 (83%)] Loss: 12304.856445\n",
      "    epoch          : 4590\n",
      "    loss           : 12216.01618686673\n",
      "    val_loss       : 12223.462168046715\n",
      "    val_log_likelihood: -12140.294287957506\n",
      "    val_log_marginal: -12148.187166240808\n",
      "Train Epoch: 4591 [256/118836 (0%)] Loss: 12232.522461\n",
      "Train Epoch: 4591 [33024/118836 (28%)] Loss: 12212.662109\n",
      "Train Epoch: 4591 [65792/118836 (55%)] Loss: 12239.083008\n",
      "Train Epoch: 4591 [98560/118836 (83%)] Loss: 12273.663086\n",
      "    epoch          : 4591\n",
      "    loss           : 12219.58750258478\n",
      "    val_loss       : 12219.008071447784\n",
      "    val_log_likelihood: -12138.948241218208\n",
      "    val_log_marginal: -12146.839918149815\n",
      "Train Epoch: 4592 [256/118836 (0%)] Loss: 12338.048828\n",
      "Train Epoch: 4592 [33024/118836 (28%)] Loss: 12252.513672\n",
      "Train Epoch: 4592 [65792/118836 (55%)] Loss: 12238.267578\n",
      "Train Epoch: 4592 [98560/118836 (83%)] Loss: 12241.558594\n",
      "    epoch          : 4592\n",
      "    loss           : 12214.898671584211\n",
      "    val_loss       : 12217.833890958173\n",
      "    val_log_likelihood: -12137.94305760184\n",
      "    val_log_marginal: -12145.834916122869\n",
      "Train Epoch: 4593 [256/118836 (0%)] Loss: 12267.406250\n",
      "Train Epoch: 4593 [33024/118836 (28%)] Loss: 12264.385742\n",
      "Train Epoch: 4593 [65792/118836 (55%)] Loss: 12311.068359\n",
      "Train Epoch: 4593 [98560/118836 (83%)] Loss: 12175.342773\n",
      "    epoch          : 4593\n",
      "    loss           : 12216.365353921112\n",
      "    val_loss       : 12216.703276858512\n",
      "    val_log_likelihood: -12140.201794968723\n",
      "    val_log_marginal: -12148.09488786417\n",
      "Train Epoch: 4594 [256/118836 (0%)] Loss: 12182.775391\n",
      "Train Epoch: 4594 [33024/118836 (28%)] Loss: 12317.208984\n",
      "Train Epoch: 4594 [65792/118836 (55%)] Loss: 12299.410156\n",
      "Train Epoch: 4594 [98560/118836 (83%)] Loss: 12187.544922\n",
      "    epoch          : 4594\n",
      "    loss           : 12217.712441519334\n",
      "    val_loss       : 12217.601759706691\n",
      "    val_log_likelihood: -12141.101306929797\n",
      "    val_log_marginal: -12148.998797810651\n",
      "Train Epoch: 4595 [256/118836 (0%)] Loss: 12254.353516\n",
      "Train Epoch: 4595 [33024/118836 (28%)] Loss: 12298.483398\n",
      "Train Epoch: 4595 [65792/118836 (55%)] Loss: 12252.416016\n",
      "Train Epoch: 4595 [98560/118836 (83%)] Loss: 12200.479492\n",
      "    epoch          : 4595\n",
      "    loss           : 12217.142232087468\n",
      "    val_loss       : 12217.071131169303\n",
      "    val_log_likelihood: -12140.088445222033\n",
      "    val_log_marginal: -12147.97360615492\n",
      "Train Epoch: 4596 [256/118836 (0%)] Loss: 12183.196289\n",
      "Train Epoch: 4596 [33024/118836 (28%)] Loss: 12303.412109\n",
      "Train Epoch: 4596 [65792/118836 (55%)] Loss: 12296.983398\n",
      "Train Epoch: 4596 [98560/118836 (83%)] Loss: 12220.040039\n",
      "    epoch          : 4596\n",
      "    loss           : 12219.006243861146\n",
      "    val_loss       : 12217.254633483954\n",
      "    val_log_likelihood: -12139.787205819634\n",
      "    val_log_marginal: -12147.680565140474\n",
      "Train Epoch: 4597 [256/118836 (0%)] Loss: 12236.728516\n",
      "Train Epoch: 4597 [33024/118836 (28%)] Loss: 12288.059570\n",
      "Train Epoch: 4597 [65792/118836 (55%)] Loss: 12242.145508\n",
      "Train Epoch: 4597 [98560/118836 (83%)] Loss: 12227.171875\n",
      "    epoch          : 4597\n",
      "    loss           : 12217.468981822529\n",
      "    val_loss       : 12217.424116896726\n",
      "    val_log_likelihood: -12140.359293094758\n",
      "    val_log_marginal: -12148.248685056154\n",
      "Train Epoch: 4598 [256/118836 (0%)] Loss: 12252.634766\n",
      "Train Epoch: 4598 [33024/118836 (28%)] Loss: 12269.537109\n",
      "Train Epoch: 4598 [65792/118836 (55%)] Loss: 12262.425781\n",
      "Train Epoch: 4598 [98560/118836 (83%)] Loss: 12214.699219\n",
      "    epoch          : 4598\n",
      "    loss           : 12218.115195118642\n",
      "    val_loss       : 12215.894921078794\n",
      "    val_log_likelihood: -12139.962931981492\n",
      "    val_log_marginal: -12147.854626046505\n",
      "Train Epoch: 4599 [256/118836 (0%)] Loss: 12199.074219\n",
      "Train Epoch: 4599 [33024/118836 (28%)] Loss: 12191.144531\n",
      "Train Epoch: 4599 [65792/118836 (55%)] Loss: 12164.105469\n",
      "Train Epoch: 4599 [98560/118836 (83%)] Loss: 12343.951172\n",
      "    epoch          : 4599\n",
      "    loss           : 12214.617800900796\n",
      "    val_loss       : 12218.687329588338\n",
      "    val_log_likelihood: -12140.336694194582\n",
      "    val_log_marginal: -12148.226107138045\n",
      "Train Epoch: 4600 [256/118836 (0%)] Loss: 12203.720703\n",
      "Train Epoch: 4600 [33024/118836 (28%)] Loss: 12246.496094\n",
      "Train Epoch: 4600 [65792/118836 (55%)] Loss: 12204.710938\n",
      "Train Epoch: 4600 [98560/118836 (83%)] Loss: 12186.104492\n",
      "    epoch          : 4600\n",
      "    loss           : 12216.416165542287\n",
      "    val_loss       : 12213.951455837076\n",
      "    val_log_likelihood: -12141.479989273159\n",
      "    val_log_marginal: -12149.371871212426\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch4600.pth ...\n",
      "Train Epoch: 4601 [256/118836 (0%)] Loss: 12202.421875\n",
      "Train Epoch: 4601 [33024/118836 (28%)] Loss: 12161.380859\n",
      "Train Epoch: 4601 [65792/118836 (55%)] Loss: 12225.007812\n",
      "Train Epoch: 4601 [98560/118836 (83%)] Loss: 12248.593750\n",
      "    epoch          : 4601\n",
      "    loss           : 12220.555712688689\n",
      "    val_loss       : 12219.584699702531\n",
      "    val_log_likelihood: -12140.31963900305\n",
      "    val_log_marginal: -12148.21025020029\n",
      "Train Epoch: 4602 [256/118836 (0%)] Loss: 12158.365234\n",
      "Train Epoch: 4602 [33024/118836 (28%)] Loss: 12181.690430\n",
      "Train Epoch: 4602 [65792/118836 (55%)] Loss: 12204.673828\n",
      "Train Epoch: 4602 [98560/118836 (83%)] Loss: 12274.441406\n",
      "    epoch          : 4602\n",
      "    loss           : 12214.343266161342\n",
      "    val_loss       : 12219.669772438014\n",
      "    val_log_likelihood: -12140.416870379704\n",
      "    val_log_marginal: -12148.306764745681\n",
      "Train Epoch: 4603 [256/118836 (0%)] Loss: 12220.888672\n",
      "Train Epoch: 4603 [33024/118836 (28%)] Loss: 12229.080078\n",
      "Train Epoch: 4603 [65792/118836 (55%)] Loss: 12211.212891\n",
      "Train Epoch: 4603 [98560/118836 (83%)] Loss: 12286.669922\n",
      "    epoch          : 4603\n",
      "    loss           : 12215.619464207248\n",
      "    val_loss       : 12222.08294506104\n",
      "    val_log_likelihood: -12138.567911400176\n",
      "    val_log_marginal: -12146.455322218417\n",
      "Train Epoch: 4604 [256/118836 (0%)] Loss: 12242.869141\n",
      "Train Epoch: 4604 [33024/118836 (28%)] Loss: 12332.064453\n",
      "Train Epoch: 4604 [65792/118836 (55%)] Loss: 12209.166016\n",
      "Train Epoch: 4604 [98560/118836 (83%)] Loss: 12126.171875\n",
      "    epoch          : 4604\n",
      "    loss           : 12218.992551469448\n",
      "    val_loss       : 12217.026188760237\n",
      "    val_log_likelihood: -12138.545233825735\n",
      "    val_log_marginal: -12146.434312296898\n",
      "Train Epoch: 4605 [256/118836 (0%)] Loss: 12265.649414\n",
      "Train Epoch: 4605 [33024/118836 (28%)] Loss: 12393.804688\n",
      "Train Epoch: 4605 [65792/118836 (55%)] Loss: 12145.070312\n",
      "Train Epoch: 4605 [98560/118836 (83%)] Loss: 12335.826172\n",
      "    epoch          : 4605\n",
      "    loss           : 12220.688149426178\n",
      "    val_loss       : 12219.063796660977\n",
      "    val_log_likelihood: -12140.71691529027\n",
      "    val_log_marginal: -12148.612212522876\n",
      "Train Epoch: 4606 [256/118836 (0%)] Loss: 12143.742188\n",
      "Train Epoch: 4606 [33024/118836 (28%)] Loss: 12192.369141\n",
      "Train Epoch: 4606 [65792/118836 (55%)] Loss: 12267.250000\n",
      "Train Epoch: 4606 [98560/118836 (83%)] Loss: 12220.484375\n",
      "    epoch          : 4606\n",
      "    loss           : 12214.8224205503\n",
      "    val_loss       : 12224.521860638511\n",
      "    val_log_likelihood: -12140.50148560277\n",
      "    val_log_marginal: -12148.39440317036\n",
      "Train Epoch: 4607 [256/118836 (0%)] Loss: 12339.237305\n",
      "Train Epoch: 4607 [33024/118836 (28%)] Loss: 12358.213867\n",
      "Train Epoch: 4607 [65792/118836 (55%)] Loss: 12208.474609\n",
      "Train Epoch: 4607 [98560/118836 (83%)] Loss: 12209.045898\n",
      "    epoch          : 4607\n",
      "    loss           : 12216.921377591241\n",
      "    val_loss       : 12214.554346239333\n",
      "    val_log_likelihood: -12140.383259990178\n",
      "    val_log_marginal: -12148.271027643483\n",
      "Train Epoch: 4608 [256/118836 (0%)] Loss: 12271.107422\n",
      "Train Epoch: 4608 [33024/118836 (28%)] Loss: 12183.521484\n",
      "Train Epoch: 4608 [65792/118836 (55%)] Loss: 12243.041016\n",
      "Train Epoch: 4608 [98560/118836 (83%)] Loss: 12225.237305\n",
      "    epoch          : 4608\n",
      "    loss           : 12218.749533608612\n",
      "    val_loss       : 12214.313889033752\n",
      "    val_log_likelihood: -12139.558705864867\n",
      "    val_log_marginal: -12147.45184274581\n",
      "Train Epoch: 4609 [256/118836 (0%)] Loss: 12188.310547\n",
      "Train Epoch: 4609 [33024/118836 (28%)] Loss: 12178.365234\n",
      "Train Epoch: 4609 [65792/118836 (55%)] Loss: 12206.568359\n",
      "Train Epoch: 4609 [98560/118836 (83%)] Loss: 12196.654297\n",
      "    epoch          : 4609\n",
      "    loss           : 12221.141177012252\n",
      "    val_loss       : 12219.25346566254\n",
      "    val_log_likelihood: -12141.528929997674\n",
      "    val_log_marginal: -12149.4219658166\n",
      "Train Epoch: 4610 [256/118836 (0%)] Loss: 12189.144531\n",
      "Train Epoch: 4610 [33024/118836 (28%)] Loss: 12339.341797\n",
      "Train Epoch: 4610 [65792/118836 (55%)] Loss: 12325.011719\n",
      "Train Epoch: 4610 [98560/118836 (83%)] Loss: 12179.061523\n",
      "    epoch          : 4610\n",
      "    loss           : 12216.41807908783\n",
      "    val_loss       : 12216.70248696998\n",
      "    val_log_likelihood: -12140.02682663229\n",
      "    val_log_marginal: -12147.913934067492\n",
      "Train Epoch: 4611 [256/118836 (0%)] Loss: 12201.805664\n",
      "Train Epoch: 4611 [33024/118836 (28%)] Loss: 12231.971680\n",
      "Train Epoch: 4611 [65792/118836 (55%)] Loss: 12190.280273\n",
      "Train Epoch: 4611 [98560/118836 (83%)] Loss: 12253.035156\n",
      "    epoch          : 4611\n",
      "    loss           : 12222.13436563017\n",
      "    val_loss       : 12220.27713773019\n",
      "    val_log_likelihood: -12139.89396922172\n",
      "    val_log_marginal: -12147.783742672642\n",
      "Train Epoch: 4612 [256/118836 (0%)] Loss: 12343.492188\n",
      "Train Epoch: 4612 [33024/118836 (28%)] Loss: 12316.577148\n",
      "Train Epoch: 4612 [65792/118836 (55%)] Loss: 12285.863281\n",
      "Train Epoch: 4612 [98560/118836 (83%)] Loss: 12198.180664\n",
      "    epoch          : 4612\n",
      "    loss           : 12212.361759460297\n",
      "    val_loss       : 12213.921391205462\n",
      "    val_log_likelihood: -12139.019278426127\n",
      "    val_log_marginal: -12146.905089217827\n",
      "Train Epoch: 4613 [256/118836 (0%)] Loss: 12201.418945\n",
      "Train Epoch: 4613 [33024/118836 (28%)] Loss: 12295.642578\n",
      "Train Epoch: 4613 [65792/118836 (55%)] Loss: 12357.795898\n",
      "Train Epoch: 4613 [98560/118836 (83%)] Loss: 12172.488281\n",
      "    epoch          : 4613\n",
      "    loss           : 12215.21289611766\n",
      "    val_loss       : 12218.452571702888\n",
      "    val_log_likelihood: -12140.164969758065\n",
      "    val_log_marginal: -12148.062785325339\n",
      "Train Epoch: 4614 [256/118836 (0%)] Loss: 12249.447266\n",
      "Train Epoch: 4614 [33024/118836 (28%)] Loss: 12181.248047\n",
      "Train Epoch: 4614 [65792/118836 (55%)] Loss: 12322.756836\n",
      "Train Epoch: 4614 [98560/118836 (83%)] Loss: 12161.783203\n",
      "    epoch          : 4614\n",
      "    loss           : 12216.295294083437\n",
      "    val_loss       : 12214.554063595333\n",
      "    val_log_likelihood: -12138.699936188224\n",
      "    val_log_marginal: -12146.59200366616\n",
      "Train Epoch: 4615 [256/118836 (0%)] Loss: 12238.562500\n",
      "Train Epoch: 4615 [33024/118836 (28%)] Loss: 12183.734375\n",
      "Train Epoch: 4615 [65792/118836 (55%)] Loss: 12284.139648\n",
      "Train Epoch: 4615 [98560/118836 (83%)] Loss: 12308.980469\n",
      "    epoch          : 4615\n",
      "    loss           : 12217.82531001215\n",
      "    val_loss       : 12222.48097766036\n",
      "    val_log_likelihood: -12140.589070900538\n",
      "    val_log_marginal: -12148.477366336681\n",
      "Train Epoch: 4616 [256/118836 (0%)] Loss: 12153.514648\n",
      "Train Epoch: 4616 [33024/118836 (28%)] Loss: 12233.531250\n",
      "Train Epoch: 4616 [65792/118836 (55%)] Loss: 12216.089844\n",
      "Train Epoch: 4616 [98560/118836 (83%)] Loss: 12167.571289\n",
      "    epoch          : 4616\n",
      "    loss           : 12214.983361119726\n",
      "    val_loss       : 12212.436738265365\n",
      "    val_log_likelihood: -12140.485215861507\n",
      "    val_log_marginal: -12148.384567627982\n",
      "Train Epoch: 4617 [256/118836 (0%)] Loss: 12211.615234\n",
      "Train Epoch: 4617 [33024/118836 (28%)] Loss: 12186.853516\n",
      "Train Epoch: 4617 [65792/118836 (55%)] Loss: 12314.015625\n",
      "Train Epoch: 4617 [98560/118836 (83%)] Loss: 12280.046875\n",
      "    epoch          : 4617\n",
      "    loss           : 12217.597943322218\n",
      "    val_loss       : 12214.943062909819\n",
      "    val_log_likelihood: -12137.230870683416\n",
      "    val_log_marginal: -12145.11962484946\n",
      "Train Epoch: 4618 [256/118836 (0%)] Loss: 12240.123047\n",
      "Train Epoch: 4618 [33024/118836 (28%)] Loss: 12219.814453\n",
      "Train Epoch: 4618 [65792/118836 (55%)] Loss: 12200.719727\n",
      "Train Epoch: 4618 [98560/118836 (83%)] Loss: 12184.108398\n",
      "    epoch          : 4618\n",
      "    loss           : 12216.738542797508\n",
      "    val_loss       : 12218.155745429349\n",
      "    val_log_likelihood: -12139.603116276365\n",
      "    val_log_marginal: -12147.49325762003\n",
      "Train Epoch: 4619 [256/118836 (0%)] Loss: 12236.708984\n",
      "Train Epoch: 4619 [33024/118836 (28%)] Loss: 12254.158203\n",
      "Train Epoch: 4619 [65792/118836 (55%)] Loss: 12263.728516\n",
      "Train Epoch: 4619 [98560/118836 (83%)] Loss: 12171.284180\n",
      "    epoch          : 4619\n",
      "    loss           : 12219.272546719914\n",
      "    val_loss       : 12215.069468460546\n",
      "    val_log_likelihood: -12139.304006733355\n",
      "    val_log_marginal: -12147.195631506675\n",
      "Train Epoch: 4620 [256/118836 (0%)] Loss: 12311.308594\n",
      "Train Epoch: 4620 [33024/118836 (28%)] Loss: 12220.042969\n",
      "Train Epoch: 4620 [65792/118836 (55%)] Loss: 12168.617188\n",
      "Train Epoch: 4620 [98560/118836 (83%)] Loss: 12200.365234\n",
      "    epoch          : 4620\n",
      "    loss           : 12218.16940459574\n",
      "    val_loss       : 12217.828469843818\n",
      "    val_log_likelihood: -12141.536482242554\n",
      "    val_log_marginal: -12149.426061353037\n",
      "Train Epoch: 4621 [256/118836 (0%)] Loss: 12217.513672\n",
      "Train Epoch: 4621 [33024/118836 (28%)] Loss: 12186.307617\n",
      "Train Epoch: 4621 [65792/118836 (55%)] Loss: 12223.415039\n",
      "Train Epoch: 4621 [98560/118836 (83%)] Loss: 12292.082031\n",
      "    epoch          : 4621\n",
      "    loss           : 12213.01564907077\n",
      "    val_loss       : 12219.570690134962\n",
      "    val_log_likelihood: -12138.604210446392\n",
      "    val_log_marginal: -12146.493932688629\n",
      "Train Epoch: 4622 [256/118836 (0%)] Loss: 12247.568359\n",
      "Train Epoch: 4622 [33024/118836 (28%)] Loss: 12284.985352\n",
      "Train Epoch: 4622 [65792/118836 (55%)] Loss: 12216.052734\n",
      "Train Epoch: 4622 [98560/118836 (83%)] Loss: 12238.625000\n",
      "    epoch          : 4622\n",
      "    loss           : 12218.054320138028\n",
      "    val_loss       : 12218.310039645836\n",
      "    val_log_likelihood: -12141.780864059914\n",
      "    val_log_marginal: -12149.668101980962\n",
      "Train Epoch: 4623 [256/118836 (0%)] Loss: 12294.215820\n",
      "Train Epoch: 4623 [33024/118836 (28%)] Loss: 12169.753906\n",
      "Train Epoch: 4623 [65792/118836 (55%)] Loss: 12259.874023\n",
      "Train Epoch: 4623 [98560/118836 (83%)] Loss: 12192.202148\n",
      "    epoch          : 4623\n",
      "    loss           : 12217.738577368951\n",
      "    val_loss       : 12214.667487613804\n",
      "    val_log_likelihood: -12139.372507302007\n",
      "    val_log_marginal: -12147.26651301819\n",
      "Train Epoch: 4624 [256/118836 (0%)] Loss: 12283.551758\n",
      "Train Epoch: 4624 [33024/118836 (28%)] Loss: 12230.675781\n",
      "Train Epoch: 4624 [65792/118836 (55%)] Loss: 12250.476562\n",
      "Train Epoch: 4624 [98560/118836 (83%)] Loss: 12204.294922\n",
      "    epoch          : 4624\n",
      "    loss           : 12217.685067882805\n",
      "    val_loss       : 12216.838720513777\n",
      "    val_log_likelihood: -12140.87611387898\n",
      "    val_log_marginal: -12148.764645070238\n",
      "Train Epoch: 4625 [256/118836 (0%)] Loss: 12178.674805\n",
      "Train Epoch: 4625 [33024/118836 (28%)] Loss: 12229.022461\n",
      "Train Epoch: 4625 [65792/118836 (55%)] Loss: 12350.099609\n",
      "Train Epoch: 4625 [98560/118836 (83%)] Loss: 12284.017578\n",
      "    epoch          : 4625\n",
      "    loss           : 12218.22358160153\n",
      "    val_loss       : 12220.337580128748\n",
      "    val_log_likelihood: -12139.750314697065\n",
      "    val_log_marginal: -12147.64354437007\n",
      "Train Epoch: 4626 [256/118836 (0%)] Loss: 12155.481445\n",
      "Train Epoch: 4626 [33024/118836 (28%)] Loss: 12329.258789\n",
      "Train Epoch: 4626 [65792/118836 (55%)] Loss: 12206.962891\n",
      "Train Epoch: 4626 [98560/118836 (83%)] Loss: 12196.034180\n",
      "    epoch          : 4626\n",
      "    loss           : 12216.104913183673\n",
      "    val_loss       : 12213.651446909302\n",
      "    val_log_likelihood: -12142.532151442307\n",
      "    val_log_marginal: -12150.426978100288\n",
      "Train Epoch: 4627 [256/118836 (0%)] Loss: 12155.093750\n",
      "Train Epoch: 4627 [33024/118836 (28%)] Loss: 12138.752930\n",
      "Train Epoch: 4627 [65792/118836 (55%)] Loss: 12310.486328\n",
      "Train Epoch: 4627 [98560/118836 (83%)] Loss: 12322.090820\n",
      "    epoch          : 4627\n",
      "    loss           : 12217.771842528691\n",
      "    val_loss       : 12215.704253638374\n",
      "    val_log_likelihood: -12139.205485874172\n",
      "    val_log_marginal: -12147.100244818786\n",
      "Train Epoch: 4628 [256/118836 (0%)] Loss: 12192.319336\n",
      "Train Epoch: 4628 [33024/118836 (28%)] Loss: 12222.395508\n",
      "Train Epoch: 4628 [65792/118836 (55%)] Loss: 12196.628906\n",
      "Train Epoch: 4628 [98560/118836 (83%)] Loss: 12261.275391\n",
      "    epoch          : 4628\n",
      "    loss           : 12220.127473635237\n",
      "    val_loss       : 12218.420649017467\n",
      "    val_log_likelihood: -12140.057892305107\n",
      "    val_log_marginal: -12147.949681802633\n",
      "Train Epoch: 4629 [256/118836 (0%)] Loss: 12234.031250\n",
      "Train Epoch: 4629 [33024/118836 (28%)] Loss: 12193.870117\n",
      "Train Epoch: 4629 [65792/118836 (55%)] Loss: 12230.150391\n",
      "Train Epoch: 4629 [98560/118836 (83%)] Loss: 12248.776367\n",
      "    epoch          : 4629\n",
      "    loss           : 12218.218281023832\n",
      "    val_loss       : 12220.360438969363\n",
      "    val_log_likelihood: -12139.050827452957\n",
      "    val_log_marginal: -12146.941954665306\n",
      "Train Epoch: 4630 [256/118836 (0%)] Loss: 12190.939453\n",
      "Train Epoch: 4630 [33024/118836 (28%)] Loss: 12235.949219\n",
      "Train Epoch: 4630 [65792/118836 (55%)] Loss: 12242.552734\n",
      "Train Epoch: 4630 [98560/118836 (83%)] Loss: 12264.054688\n",
      "    epoch          : 4630\n",
      "    loss           : 12218.200557020264\n",
      "    val_loss       : 12217.66603279868\n",
      "    val_log_likelihood: -12140.25670686001\n",
      "    val_log_marginal: -12148.155109968162\n",
      "Train Epoch: 4631 [256/118836 (0%)] Loss: 12296.835938\n",
      "Train Epoch: 4631 [33024/118836 (28%)] Loss: 12271.730469\n",
      "Train Epoch: 4631 [65792/118836 (55%)] Loss: 12261.464844\n",
      "Train Epoch: 4631 [98560/118836 (83%)] Loss: 12268.517578\n",
      "    epoch          : 4631\n",
      "    loss           : 12218.742049375775\n",
      "    val_loss       : 12219.432608292345\n",
      "    val_log_likelihood: -12140.52201377042\n",
      "    val_log_marginal: -12148.409557064719\n",
      "Train Epoch: 4632 [256/118836 (0%)] Loss: 12292.172852\n",
      "Train Epoch: 4632 [33024/118836 (28%)] Loss: 12318.272461\n",
      "Train Epoch: 4632 [65792/118836 (55%)] Loss: 12215.513672\n",
      "Train Epoch: 4632 [98560/118836 (83%)] Loss: 12298.894531\n",
      "    epoch          : 4632\n",
      "    loss           : 12220.055563740694\n",
      "    val_loss       : 12218.836766698545\n",
      "    val_log_likelihood: -12139.900594176488\n",
      "    val_log_marginal: -12147.792970704966\n",
      "Train Epoch: 4633 [256/118836 (0%)] Loss: 12218.651367\n",
      "Train Epoch: 4633 [33024/118836 (28%)] Loss: 12186.715820\n",
      "Train Epoch: 4633 [65792/118836 (55%)] Loss: 12270.175781\n",
      "Train Epoch: 4633 [98560/118836 (83%)] Loss: 12332.779297\n",
      "    epoch          : 4633\n",
      "    loss           : 12217.220234956576\n",
      "    val_loss       : 12222.627371973984\n",
      "    val_log_likelihood: -12139.182456607992\n",
      "    val_log_marginal: -12147.077309233335\n",
      "Train Epoch: 4634 [256/118836 (0%)] Loss: 12158.384766\n",
      "Train Epoch: 4634 [33024/118836 (28%)] Loss: 12203.292969\n",
      "Train Epoch: 4634 [65792/118836 (55%)] Loss: 12288.424805\n",
      "Train Epoch: 4634 [98560/118836 (83%)] Loss: 12268.222656\n",
      "    epoch          : 4634\n",
      "    loss           : 12215.815889293839\n",
      "    val_loss       : 12214.73587610307\n",
      "    val_log_likelihood: -12138.841096722499\n",
      "    val_log_marginal: -12146.735389995556\n",
      "Train Epoch: 4635 [256/118836 (0%)] Loss: 12244.845703\n",
      "Train Epoch: 4635 [33024/118836 (28%)] Loss: 12310.017578\n",
      "Train Epoch: 4635 [65792/118836 (55%)] Loss: 12300.752930\n",
      "Train Epoch: 4635 [98560/118836 (83%)] Loss: 12277.262695\n",
      "    epoch          : 4635\n",
      "    loss           : 12218.780693625931\n",
      "    val_loss       : 12219.156464201773\n",
      "    val_log_likelihood: -12139.60259576613\n",
      "    val_log_marginal: -12147.497477295386\n",
      "Train Epoch: 4636 [256/118836 (0%)] Loss: 12168.937500\n",
      "Train Epoch: 4636 [33024/118836 (28%)] Loss: 12250.038086\n",
      "Train Epoch: 4636 [65792/118836 (55%)] Loss: 12179.327148\n",
      "Train Epoch: 4636 [98560/118836 (83%)] Loss: 12203.328125\n",
      "    epoch          : 4636\n",
      "    loss           : 12218.94607630273\n",
      "    val_loss       : 12220.202799600484\n",
      "    val_log_likelihood: -12139.675169464692\n",
      "    val_log_marginal: -12147.561636910634\n",
      "Train Epoch: 4637 [256/118836 (0%)] Loss: 12284.285156\n",
      "Train Epoch: 4637 [33024/118836 (28%)] Loss: 12187.914062\n",
      "Train Epoch: 4637 [65792/118836 (55%)] Loss: 12174.193359\n",
      "Train Epoch: 4637 [98560/118836 (83%)] Loss: 12315.148438\n",
      "    epoch          : 4637\n",
      "    loss           : 12217.998661245088\n",
      "    val_loss       : 12215.228493598652\n",
      "    val_log_likelihood: -12140.674584496484\n",
      "    val_log_marginal: -12148.569270751093\n",
      "Train Epoch: 4638 [256/118836 (0%)] Loss: 12164.180664\n",
      "Train Epoch: 4638 [33024/118836 (28%)] Loss: 12183.250977\n",
      "Train Epoch: 4638 [65792/118836 (55%)] Loss: 12367.636719\n",
      "Train Epoch: 4638 [98560/118836 (83%)] Loss: 12172.317383\n",
      "    epoch          : 4638\n",
      "    loss           : 12219.304994281172\n",
      "    val_loss       : 12219.61849914124\n",
      "    val_log_likelihood: -12139.779420136734\n",
      "    val_log_marginal: -12147.67615984093\n",
      "Train Epoch: 4639 [256/118836 (0%)] Loss: 12283.150391\n",
      "Train Epoch: 4639 [33024/118836 (28%)] Loss: 12192.001953\n",
      "Train Epoch: 4639 [65792/118836 (55%)] Loss: 12169.044922\n",
      "Train Epoch: 4639 [98560/118836 (83%)] Loss: 12198.429688\n",
      "    epoch          : 4639\n",
      "    loss           : 12219.067058584058\n",
      "    val_loss       : 12218.186377345955\n",
      "    val_log_likelihood: -12139.78701244572\n",
      "    val_log_marginal: -12147.680745747015\n",
      "Train Epoch: 4640 [256/118836 (0%)] Loss: 12226.471680\n",
      "Train Epoch: 4640 [33024/118836 (28%)] Loss: 12241.434570\n",
      "Train Epoch: 4640 [65792/118836 (55%)] Loss: 12165.710938\n",
      "Train Epoch: 4640 [98560/118836 (83%)] Loss: 12195.050781\n",
      "    epoch          : 4640\n",
      "    loss           : 12218.82510662221\n",
      "    val_loss       : 12216.89319689156\n",
      "    val_log_likelihood: -12140.295713787222\n",
      "    val_log_marginal: -12148.184708884495\n",
      "Train Epoch: 4641 [256/118836 (0%)] Loss: 12263.867188\n",
      "Train Epoch: 4641 [33024/118836 (28%)] Loss: 12333.478516\n",
      "Train Epoch: 4641 [65792/118836 (55%)] Loss: 12203.918945\n",
      "Train Epoch: 4641 [98560/118836 (83%)] Loss: 12252.809570\n",
      "    epoch          : 4641\n",
      "    loss           : 12215.078538241833\n",
      "    val_loss       : 12216.886581120985\n",
      "    val_log_likelihood: -12139.72840528717\n",
      "    val_log_marginal: -12147.616370227612\n",
      "Train Epoch: 4642 [256/118836 (0%)] Loss: 12281.567383\n",
      "Train Epoch: 4642 [33024/118836 (28%)] Loss: 12179.796875\n",
      "Train Epoch: 4642 [65792/118836 (55%)] Loss: 12294.915039\n",
      "Train Epoch: 4642 [98560/118836 (83%)] Loss: 12244.157227\n",
      "    epoch          : 4642\n",
      "    loss           : 12215.48560180159\n",
      "    val_loss       : 12217.876320455469\n",
      "    val_log_likelihood: -12140.415024846205\n",
      "    val_log_marginal: -12148.306863447684\n",
      "Train Epoch: 4643 [256/118836 (0%)] Loss: 12211.369141\n",
      "Train Epoch: 4643 [33024/118836 (28%)] Loss: 12214.385742\n",
      "Train Epoch: 4643 [65792/118836 (55%)] Loss: 12193.025391\n",
      "Train Epoch: 4643 [98560/118836 (83%)] Loss: 12139.996094\n",
      "    epoch          : 4643\n",
      "    loss           : 12220.151398043321\n",
      "    val_loss       : 12219.942734588654\n",
      "    val_log_likelihood: -12142.680534177263\n",
      "    val_log_marginal: -12150.577829093434\n",
      "Train Epoch: 4644 [256/118836 (0%)] Loss: 12228.851562\n",
      "Train Epoch: 4644 [33024/118836 (28%)] Loss: 12253.833984\n",
      "Train Epoch: 4644 [65792/118836 (55%)] Loss: 12391.082031\n",
      "Train Epoch: 4644 [98560/118836 (83%)] Loss: 12269.190430\n",
      "    epoch          : 4644\n",
      "    loss           : 12218.866233231236\n",
      "    val_loss       : 12213.687332080694\n",
      "    val_log_likelihood: -12140.8585871265\n",
      "    val_log_marginal: -12148.751291515278\n",
      "Train Epoch: 4645 [256/118836 (0%)] Loss: 12259.535156\n",
      "Train Epoch: 4645 [33024/118836 (28%)] Loss: 12281.639648\n",
      "Train Epoch: 4645 [65792/118836 (55%)] Loss: 12262.908203\n",
      "Train Epoch: 4645 [98560/118836 (83%)] Loss: 12165.987305\n",
      "    epoch          : 4645\n",
      "    loss           : 12218.788909351737\n",
      "    val_loss       : 12215.62102823324\n",
      "    val_log_likelihood: -12140.392677154414\n",
      "    val_log_marginal: -12148.284019295319\n",
      "Train Epoch: 4646 [256/118836 (0%)] Loss: 12226.526367\n",
      "Train Epoch: 4646 [33024/118836 (28%)] Loss: 12310.952148\n",
      "Train Epoch: 4646 [65792/118836 (55%)] Loss: 12298.789062\n",
      "Train Epoch: 4646 [98560/118836 (83%)] Loss: 12208.092773\n",
      "    epoch          : 4646\n",
      "    loss           : 12221.327618867608\n",
      "    val_loss       : 12217.728628367478\n",
      "    val_log_likelihood: -12139.136745890199\n",
      "    val_log_marginal: -12147.034938303213\n",
      "Train Epoch: 4647 [256/118836 (0%)] Loss: 12242.416016\n",
      "Train Epoch: 4647 [33024/118836 (28%)] Loss: 12301.578125\n",
      "Train Epoch: 4647 [65792/118836 (55%)] Loss: 12222.637695\n",
      "Train Epoch: 4647 [98560/118836 (83%)] Loss: 12247.805664\n",
      "    epoch          : 4647\n",
      "    loss           : 12217.217287821806\n",
      "    val_loss       : 12214.966019265807\n",
      "    val_log_likelihood: -12141.655842897022\n",
      "    val_log_marginal: -12149.546428757012\n",
      "Train Epoch: 4648 [256/118836 (0%)] Loss: 12264.443359\n",
      "Train Epoch: 4648 [33024/118836 (28%)] Loss: 12214.805664\n",
      "Train Epoch: 4648 [65792/118836 (55%)] Loss: 12253.904297\n",
      "Train Epoch: 4648 [98560/118836 (83%)] Loss: 12221.487305\n",
      "    epoch          : 4648\n",
      "    loss           : 12222.71816648573\n",
      "    val_loss       : 12221.244556499272\n",
      "    val_log_likelihood: -12139.00139141982\n",
      "    val_log_marginal: -12146.896980828016\n",
      "Train Epoch: 4649 [256/118836 (0%)] Loss: 12287.136719\n",
      "Train Epoch: 4649 [33024/118836 (28%)] Loss: 12352.046875\n",
      "Train Epoch: 4649 [65792/118836 (55%)] Loss: 12190.002930\n",
      "Train Epoch: 4649 [98560/118836 (83%)] Loss: 12184.466797\n",
      "    epoch          : 4649\n",
      "    loss           : 12218.724163338762\n",
      "    val_loss       : 12218.487775025611\n",
      "    val_log_likelihood: -12137.655474404208\n",
      "    val_log_marginal: -12145.55445391118\n",
      "Train Epoch: 4650 [256/118836 (0%)] Loss: 12232.659180\n",
      "Train Epoch: 4650 [33024/118836 (28%)] Loss: 12344.352539\n",
      "Train Epoch: 4650 [65792/118836 (55%)] Loss: 12191.757812\n",
      "Train Epoch: 4650 [98560/118836 (83%)] Loss: 12316.361328\n",
      "    epoch          : 4650\n",
      "    loss           : 12218.917112702904\n",
      "    val_loss       : 12215.782255928629\n",
      "    val_log_likelihood: -12139.210465454404\n",
      "    val_log_marginal: -12147.106314933955\n",
      "Train Epoch: 4651 [256/118836 (0%)] Loss: 12216.220703\n",
      "Train Epoch: 4651 [33024/118836 (28%)] Loss: 12162.949219\n",
      "Train Epoch: 4651 [65792/118836 (55%)] Loss: 12173.434570\n",
      "Train Epoch: 4651 [98560/118836 (83%)] Loss: 12289.512695\n",
      "    epoch          : 4651\n",
      "    loss           : 12214.258765961022\n",
      "    val_loss       : 12219.044742296965\n",
      "    val_log_likelihood: -12138.290287686103\n",
      "    val_log_marginal: -12146.179203372723\n",
      "Train Epoch: 4652 [256/118836 (0%)] Loss: 12275.898438\n",
      "Train Epoch: 4652 [33024/118836 (28%)] Loss: 12327.424805\n",
      "Train Epoch: 4652 [65792/118836 (55%)] Loss: 12209.454102\n",
      "Train Epoch: 4652 [98560/118836 (83%)] Loss: 12256.579102\n",
      "    epoch          : 4652\n",
      "    loss           : 12218.156260016025\n",
      "    val_loss       : 12216.577109878775\n",
      "    val_log_likelihood: -12140.060311498397\n",
      "    val_log_marginal: -12147.948722610769\n",
      "Train Epoch: 4653 [256/118836 (0%)] Loss: 12366.730469\n",
      "Train Epoch: 4653 [33024/118836 (28%)] Loss: 12200.645508\n",
      "Train Epoch: 4653 [65792/118836 (55%)] Loss: 12252.705078\n",
      "Train Epoch: 4653 [98560/118836 (83%)] Loss: 12362.623047\n",
      "    epoch          : 4653\n",
      "    loss           : 12221.227033414754\n",
      "    val_loss       : 12216.871216876854\n",
      "    val_log_likelihood: -12139.487437480613\n",
      "    val_log_marginal: -12147.384930644292\n",
      "Train Epoch: 4654 [256/118836 (0%)] Loss: 12214.880859\n",
      "Train Epoch: 4654 [33024/118836 (28%)] Loss: 12187.960938\n",
      "Train Epoch: 4654 [65792/118836 (55%)] Loss: 12190.119141\n",
      "Train Epoch: 4654 [98560/118836 (83%)] Loss: 12250.525391\n",
      "    epoch          : 4654\n",
      "    loss           : 12218.422347530242\n",
      "    val_loss       : 12219.410887693868\n",
      "    val_log_likelihood: -12139.591626602563\n",
      "    val_log_marginal: -12147.486043661745\n",
      "Train Epoch: 4655 [256/118836 (0%)] Loss: 12242.883789\n",
      "Train Epoch: 4655 [33024/118836 (28%)] Loss: 12304.296875\n",
      "Train Epoch: 4655 [65792/118836 (55%)] Loss: 12238.896484\n",
      "Train Epoch: 4655 [98560/118836 (83%)] Loss: 12196.787109\n",
      "    epoch          : 4655\n",
      "    loss           : 12220.119165665063\n",
      "    val_loss       : 12219.267668821263\n",
      "    val_log_likelihood: -12139.440923057537\n",
      "    val_log_marginal: -12147.341132479798\n",
      "Train Epoch: 4656 [256/118836 (0%)] Loss: 12242.030273\n",
      "Train Epoch: 4656 [33024/118836 (28%)] Loss: 12284.359375\n",
      "Train Epoch: 4656 [65792/118836 (55%)] Loss: 12195.559570\n",
      "Train Epoch: 4656 [98560/118836 (83%)] Loss: 12194.142578\n",
      "    epoch          : 4656\n",
      "    loss           : 12215.700717761321\n",
      "    val_loss       : 12213.603770232607\n",
      "    val_log_likelihood: -12139.636192424006\n",
      "    val_log_marginal: -12147.533457991785\n",
      "Train Epoch: 4657 [256/118836 (0%)] Loss: 12402.753906\n",
      "Train Epoch: 4657 [33024/118836 (28%)] Loss: 12205.566406\n",
      "Train Epoch: 4657 [65792/118836 (55%)] Loss: 12148.426758\n",
      "Train Epoch: 4657 [98560/118836 (83%)] Loss: 12168.871094\n",
      "    epoch          : 4657\n",
      "    loss           : 12217.09693299602\n",
      "    val_loss       : 12216.513524072423\n",
      "    val_log_likelihood: -12140.930546293424\n",
      "    val_log_marginal: -12148.82280788144\n",
      "Train Epoch: 4658 [256/118836 (0%)] Loss: 12238.796875\n",
      "Train Epoch: 4658 [33024/118836 (28%)] Loss: 12224.605469\n",
      "Train Epoch: 4658 [65792/118836 (55%)] Loss: 12272.018555\n",
      "Train Epoch: 4658 [98560/118836 (83%)] Loss: 12186.412109\n",
      "    epoch          : 4658\n",
      "    loss           : 12211.275968000413\n",
      "    val_loss       : 12214.370344482606\n",
      "    val_log_likelihood: -12139.757737864455\n",
      "    val_log_marginal: -12147.656194930505\n",
      "Train Epoch: 4659 [256/118836 (0%)] Loss: 12204.224609\n",
      "Train Epoch: 4659 [33024/118836 (28%)] Loss: 12273.340820\n",
      "Train Epoch: 4659 [65792/118836 (55%)] Loss: 12172.960938\n",
      "Train Epoch: 4659 [98560/118836 (83%)] Loss: 12255.444336\n",
      "    epoch          : 4659\n",
      "    loss           : 12213.667788461538\n",
      "    val_loss       : 12222.517813058446\n",
      "    val_log_likelihood: -12138.726904821908\n",
      "    val_log_marginal: -12146.619625586933\n",
      "Train Epoch: 4660 [256/118836 (0%)] Loss: 12188.970703\n",
      "Train Epoch: 4660 [33024/118836 (28%)] Loss: 12194.835938\n",
      "Train Epoch: 4660 [65792/118836 (55%)] Loss: 12284.437500\n",
      "Train Epoch: 4660 [98560/118836 (83%)] Loss: 12209.199219\n",
      "    epoch          : 4660\n",
      "    loss           : 12219.124422786135\n",
      "    val_loss       : 12217.247490347261\n",
      "    val_log_likelihood: -12139.8876413552\n",
      "    val_log_marginal: -12147.779610035048\n",
      "Train Epoch: 4661 [256/118836 (0%)] Loss: 12205.309570\n",
      "Train Epoch: 4661 [33024/118836 (28%)] Loss: 12200.327148\n",
      "Train Epoch: 4661 [65792/118836 (55%)] Loss: 12193.701172\n",
      "Train Epoch: 4661 [98560/118836 (83%)] Loss: 12288.053711\n",
      "    epoch          : 4661\n",
      "    loss           : 12220.080897500517\n",
      "    val_loss       : 12219.11334766792\n",
      "    val_log_likelihood: -12141.576962010184\n",
      "    val_log_marginal: -12149.472664661165\n",
      "Train Epoch: 4662 [256/118836 (0%)] Loss: 12126.411133\n",
      "Train Epoch: 4662 [33024/118836 (28%)] Loss: 12192.048828\n",
      "Train Epoch: 4662 [65792/118836 (55%)] Loss: 12314.531250\n",
      "Train Epoch: 4662 [98560/118836 (83%)] Loss: 12201.480469\n",
      "    epoch          : 4662\n",
      "    loss           : 12219.2379376357\n",
      "    val_loss       : 12217.879655447616\n",
      "    val_log_likelihood: -12139.530465195927\n",
      "    val_log_marginal: -12147.426451180829\n",
      "Train Epoch: 4663 [256/118836 (0%)] Loss: 12159.690430\n",
      "Train Epoch: 4663 [33024/118836 (28%)] Loss: 12242.618164\n",
      "Train Epoch: 4663 [65792/118836 (55%)] Loss: 12171.125977\n",
      "Train Epoch: 4663 [98560/118836 (83%)] Loss: 12222.107422\n",
      "    epoch          : 4663\n",
      "    loss           : 12215.29337552988\n",
      "    val_loss       : 12218.838498163657\n",
      "    val_log_likelihood: -12140.04802684295\n",
      "    val_log_marginal: -12147.937513016901\n",
      "Train Epoch: 4664 [256/118836 (0%)] Loss: 12230.392578\n",
      "Train Epoch: 4664 [33024/118836 (28%)] Loss: 12192.771484\n",
      "Train Epoch: 4664 [65792/118836 (55%)] Loss: 12157.042969\n",
      "Train Epoch: 4664 [98560/118836 (83%)] Loss: 12231.852539\n",
      "    epoch          : 4664\n",
      "    loss           : 12218.802704973119\n",
      "    val_loss       : 12217.38144804414\n",
      "    val_log_likelihood: -12139.002499483044\n",
      "    val_log_marginal: -12146.893481720444\n",
      "Train Epoch: 4665 [256/118836 (0%)] Loss: 12131.844727\n",
      "Train Epoch: 4665 [33024/118836 (28%)] Loss: 12170.646484\n",
      "Train Epoch: 4665 [65792/118836 (55%)] Loss: 12225.098633\n",
      "Train Epoch: 4665 [98560/118836 (83%)] Loss: 12358.243164\n",
      "    epoch          : 4665\n",
      "    loss           : 12216.248339924525\n",
      "    val_loss       : 12217.88122873913\n",
      "    val_log_likelihood: -12139.473408582764\n",
      "    val_log_marginal: -12147.362725982599\n",
      "Train Epoch: 4666 [256/118836 (0%)] Loss: 12319.044922\n",
      "Train Epoch: 4666 [33024/118836 (28%)] Loss: 12194.614258\n",
      "Train Epoch: 4666 [65792/118836 (55%)] Loss: 12202.056641\n",
      "Train Epoch: 4666 [98560/118836 (83%)] Loss: 12226.756836\n",
      "    epoch          : 4666\n",
      "    loss           : 12217.539213063481\n",
      "    val_loss       : 12214.511007130144\n",
      "    val_log_likelihood: -12140.1800039741\n",
      "    val_log_marginal: -12148.077469871887\n",
      "Train Epoch: 4667 [256/118836 (0%)] Loss: 12206.329102\n",
      "Train Epoch: 4667 [33024/118836 (28%)] Loss: 12294.795898\n",
      "Train Epoch: 4667 [65792/118836 (55%)] Loss: 12197.621094\n",
      "Train Epoch: 4667 [98560/118836 (83%)] Loss: 12315.468750\n",
      "    epoch          : 4667\n",
      "    loss           : 12214.812393216242\n",
      "    val_loss       : 12222.020792127214\n",
      "    val_log_likelihood: -12139.396700527295\n",
      "    val_log_marginal: -12147.28680318627\n",
      "Train Epoch: 4668 [256/118836 (0%)] Loss: 12241.244141\n",
      "Train Epoch: 4668 [33024/118836 (28%)] Loss: 12273.380859\n",
      "Train Epoch: 4668 [65792/118836 (55%)] Loss: 12305.058594\n",
      "Train Epoch: 4668 [98560/118836 (83%)] Loss: 12169.063477\n",
      "    epoch          : 4668\n",
      "    loss           : 12218.884436550094\n",
      "    val_loss       : 12220.130416158614\n",
      "    val_log_likelihood: -12136.775249431348\n",
      "    val_log_marginal: -12144.6635872571\n",
      "Train Epoch: 4669 [256/118836 (0%)] Loss: 12247.941406\n",
      "Train Epoch: 4669 [33024/118836 (28%)] Loss: 12287.687500\n",
      "Train Epoch: 4669 [65792/118836 (55%)] Loss: 12209.935547\n",
      "Train Epoch: 4669 [98560/118836 (83%)] Loss: 12247.589844\n",
      "    epoch          : 4669\n",
      "    loss           : 12218.0543306387\n",
      "    val_loss       : 12214.707454408437\n",
      "    val_log_likelihood: -12139.336905661963\n",
      "    val_log_marginal: -12147.23293133969\n",
      "Train Epoch: 4670 [256/118836 (0%)] Loss: 12271.812500\n",
      "Train Epoch: 4670 [33024/118836 (28%)] Loss: 12182.381836\n",
      "Train Epoch: 4670 [65792/118836 (55%)] Loss: 12164.410156\n",
      "Train Epoch: 4670 [98560/118836 (83%)] Loss: 12150.716797\n",
      "    epoch          : 4670\n",
      "    loss           : 12219.553525802576\n",
      "    val_loss       : 12215.382168087943\n",
      "    val_log_likelihood: -12141.326700785774\n",
      "    val_log_marginal: -12149.21983609493\n",
      "Train Epoch: 4671 [256/118836 (0%)] Loss: 12099.131836\n",
      "Train Epoch: 4671 [33024/118836 (28%)] Loss: 12215.365234\n",
      "Train Epoch: 4671 [65792/118836 (55%)] Loss: 12162.131836\n",
      "Train Epoch: 4671 [98560/118836 (83%)] Loss: 12233.895508\n",
      "    epoch          : 4671\n",
      "    loss           : 12216.09957157258\n",
      "    val_loss       : 12220.759026985877\n",
      "    val_log_likelihood: -12142.03432912014\n",
      "    val_log_marginal: -12149.924064204326\n",
      "Train Epoch: 4672 [256/118836 (0%)] Loss: 12183.627930\n",
      "Train Epoch: 4672 [33024/118836 (28%)] Loss: 12172.531250\n",
      "Train Epoch: 4672 [65792/118836 (55%)] Loss: 12258.646484\n",
      "Train Epoch: 4672 [98560/118836 (83%)] Loss: 12248.488281\n",
      "    epoch          : 4672\n",
      "    loss           : 12217.256624631667\n",
      "    val_loss       : 12219.777660201687\n",
      "    val_log_likelihood: -12140.029704785722\n",
      "    val_log_marginal: -12147.917581852947\n",
      "Train Epoch: 4673 [256/118836 (0%)] Loss: 12178.910156\n",
      "Train Epoch: 4673 [33024/118836 (28%)] Loss: 12175.722656\n",
      "Train Epoch: 4673 [65792/118836 (55%)] Loss: 12208.546875\n",
      "Train Epoch: 4673 [98560/118836 (83%)] Loss: 12163.532227\n",
      "    epoch          : 4673\n",
      "    loss           : 12213.25627148599\n",
      "    val_loss       : 12222.31362340026\n",
      "    val_log_likelihood: -12140.737057679384\n",
      "    val_log_marginal: -12148.633474635713\n",
      "Train Epoch: 4674 [256/118836 (0%)] Loss: 12295.409180\n",
      "Train Epoch: 4674 [33024/118836 (28%)] Loss: 12166.333984\n",
      "Train Epoch: 4674 [65792/118836 (55%)] Loss: 12176.990234\n",
      "Train Epoch: 4674 [98560/118836 (83%)] Loss: 12286.125977\n",
      "    epoch          : 4674\n",
      "    loss           : 12220.614269767111\n",
      "    val_loss       : 12219.77190771841\n",
      "    val_log_likelihood: -12140.998120056607\n",
      "    val_log_marginal: -12148.893205335187\n",
      "Train Epoch: 4675 [256/118836 (0%)] Loss: 12283.369141\n",
      "Train Epoch: 4675 [33024/118836 (28%)] Loss: 12178.071289\n",
      "Train Epoch: 4675 [65792/118836 (55%)] Loss: 12191.367188\n",
      "Train Epoch: 4675 [98560/118836 (83%)] Loss: 12264.490234\n",
      "    epoch          : 4675\n",
      "    loss           : 12218.914638259925\n",
      "    val_loss       : 12222.589779319425\n",
      "    val_log_likelihood: -12139.736610673852\n",
      "    val_log_marginal: -12147.632631835235\n",
      "Train Epoch: 4676 [256/118836 (0%)] Loss: 12223.181641\n",
      "Train Epoch: 4676 [33024/118836 (28%)] Loss: 12300.601562\n",
      "Train Epoch: 4676 [65792/118836 (55%)] Loss: 12230.301758\n",
      "Train Epoch: 4676 [98560/118836 (83%)] Loss: 12214.470703\n",
      "    epoch          : 4676\n",
      "    loss           : 12218.242035644127\n",
      "    val_loss       : 12215.925940708372\n",
      "    val_log_likelihood: -12142.11870702802\n",
      "    val_log_marginal: -12150.016386016274\n",
      "Train Epoch: 4677 [256/118836 (0%)] Loss: 12162.673828\n",
      "Train Epoch: 4677 [33024/118836 (28%)] Loss: 12195.166016\n",
      "Train Epoch: 4677 [65792/118836 (55%)] Loss: 12200.804688\n",
      "Train Epoch: 4677 [98560/118836 (83%)] Loss: 12217.188477\n",
      "    epoch          : 4677\n",
      "    loss           : 12217.089441655036\n",
      "    val_loss       : 12216.044424738557\n",
      "    val_log_likelihood: -12140.362503877172\n",
      "    val_log_marginal: -12148.256011960097\n",
      "Train Epoch: 4678 [256/118836 (0%)] Loss: 12189.208008\n",
      "Train Epoch: 4678 [33024/118836 (28%)] Loss: 12288.439453\n",
      "Train Epoch: 4678 [65792/118836 (55%)] Loss: 12234.904297\n",
      "Train Epoch: 4678 [98560/118836 (83%)] Loss: 12268.459961\n",
      "    epoch          : 4678\n",
      "    loss           : 12219.471320241419\n",
      "    val_loss       : 12216.83445350039\n",
      "    val_log_likelihood: -12137.850532626397\n",
      "    val_log_marginal: -12145.741616474837\n",
      "Train Epoch: 4679 [256/118836 (0%)] Loss: 12192.858398\n",
      "Train Epoch: 4679 [33024/118836 (28%)] Loss: 12227.830078\n",
      "Train Epoch: 4679 [65792/118836 (55%)] Loss: 12229.634766\n",
      "Train Epoch: 4679 [98560/118836 (83%)] Loss: 12264.200195\n",
      "    epoch          : 4679\n",
      "    loss           : 12220.31101035851\n",
      "    val_loss       : 12215.915255086322\n",
      "    val_log_likelihood: -12139.231589737128\n",
      "    val_log_marginal: -12147.121129057854\n",
      "Train Epoch: 4680 [256/118836 (0%)] Loss: 12254.624023\n",
      "Train Epoch: 4680 [33024/118836 (28%)] Loss: 12166.818359\n",
      "Train Epoch: 4680 [65792/118836 (55%)] Loss: 12311.609375\n",
      "Train Epoch: 4680 [98560/118836 (83%)] Loss: 12200.210938\n",
      "    epoch          : 4680\n",
      "    loss           : 12220.823155758892\n",
      "    val_loss       : 12213.86837749721\n",
      "    val_log_likelihood: -12139.734236068032\n",
      "    val_log_marginal: -12147.621014118706\n",
      "Train Epoch: 4681 [256/118836 (0%)] Loss: 12261.835938\n",
      "Train Epoch: 4681 [33024/118836 (28%)] Loss: 12322.672852\n",
      "Train Epoch: 4681 [65792/118836 (55%)] Loss: 12219.574219\n",
      "Train Epoch: 4681 [98560/118836 (83%)] Loss: 12221.700195\n",
      "    epoch          : 4681\n",
      "    loss           : 12219.952287369468\n",
      "    val_loss       : 12216.59058709201\n",
      "    val_log_likelihood: -12140.090077834211\n",
      "    val_log_marginal: -12147.980125093965\n",
      "Train Epoch: 4682 [256/118836 (0%)] Loss: 12165.558594\n",
      "Train Epoch: 4682 [33024/118836 (28%)] Loss: 12226.917969\n",
      "Train Epoch: 4682 [65792/118836 (55%)] Loss: 12186.885742\n",
      "Train Epoch: 4682 [98560/118836 (83%)] Loss: 12185.490234\n",
      "    epoch          : 4682\n",
      "    loss           : 12223.72081217044\n",
      "    val_loss       : 12220.470492792845\n",
      "    val_log_likelihood: -12140.1696457881\n",
      "    val_log_marginal: -12148.066848162933\n",
      "Train Epoch: 4683 [256/118836 (0%)] Loss: 12260.004883\n",
      "Train Epoch: 4683 [33024/118836 (28%)] Loss: 12183.792969\n",
      "Train Epoch: 4683 [65792/118836 (55%)] Loss: 12198.992188\n",
      "Train Epoch: 4683 [98560/118836 (83%)] Loss: 12256.996094\n",
      "    epoch          : 4683\n",
      "    loss           : 12215.890068625931\n",
      "    val_loss       : 12220.002281893998\n",
      "    val_log_likelihood: -12139.136184992763\n",
      "    val_log_marginal: -12147.028191592646\n",
      "Train Epoch: 4684 [256/118836 (0%)] Loss: 12156.841797\n",
      "Train Epoch: 4684 [33024/118836 (28%)] Loss: 12270.232422\n",
      "Train Epoch: 4684 [65792/118836 (55%)] Loss: 12313.064453\n",
      "Train Epoch: 4684 [98560/118836 (83%)] Loss: 12286.231445\n",
      "    epoch          : 4684\n",
      "    loss           : 12221.956728830644\n",
      "    val_loss       : 12212.433353692753\n",
      "    val_log_likelihood: -12138.681985854786\n",
      "    val_log_marginal: -12146.578358937015\n",
      "Train Epoch: 4685 [256/118836 (0%)] Loss: 12246.697266\n",
      "Train Epoch: 4685 [33024/118836 (28%)] Loss: 12328.558594\n",
      "Train Epoch: 4685 [65792/118836 (55%)] Loss: 12151.202148\n",
      "Train Epoch: 4685 [98560/118836 (83%)] Loss: 12229.431641\n",
      "    epoch          : 4685\n",
      "    loss           : 12217.272527010959\n",
      "    val_loss       : 12217.383544489721\n",
      "    val_log_likelihood: -12137.860699861714\n",
      "    val_log_marginal: -12145.75166934907\n",
      "Train Epoch: 4686 [256/118836 (0%)] Loss: 12261.608398\n",
      "Train Epoch: 4686 [33024/118836 (28%)] Loss: 12261.872070\n",
      "Train Epoch: 4686 [65792/118836 (55%)] Loss: 12230.865234\n",
      "Train Epoch: 4686 [98560/118836 (83%)] Loss: 12186.407227\n",
      "    epoch          : 4686\n",
      "    loss           : 12211.896841882495\n",
      "    val_loss       : 12222.872902023784\n",
      "    val_log_likelihood: -12140.784082112024\n",
      "    val_log_marginal: -12148.67822199909\n",
      "Train Epoch: 4687 [256/118836 (0%)] Loss: 12304.582031\n",
      "Train Epoch: 4687 [33024/118836 (28%)] Loss: 12210.977539\n",
      "Train Epoch: 4687 [65792/118836 (55%)] Loss: 12227.234375\n",
      "Train Epoch: 4687 [98560/118836 (83%)] Loss: 12239.868164\n",
      "    epoch          : 4687\n",
      "    loss           : 12215.049193386838\n",
      "    val_loss       : 12217.00653132449\n",
      "    val_log_likelihood: -12141.154693315757\n",
      "    val_log_marginal: -12149.050121718214\n",
      "Train Epoch: 4688 [256/118836 (0%)] Loss: 12172.673828\n",
      "Train Epoch: 4688 [33024/118836 (28%)] Loss: 12209.860352\n",
      "Train Epoch: 4688 [65792/118836 (55%)] Loss: 12261.195312\n",
      "Train Epoch: 4688 [98560/118836 (83%)] Loss: 12216.601562\n",
      "    epoch          : 4688\n",
      "    loss           : 12218.414548277244\n",
      "    val_loss       : 12218.052349661462\n",
      "    val_log_likelihood: -12138.885593239507\n",
      "    val_log_marginal: -12146.780583533206\n",
      "Train Epoch: 4689 [256/118836 (0%)] Loss: 12130.688477\n",
      "Train Epoch: 4689 [33024/118836 (28%)] Loss: 12203.105469\n",
      "Train Epoch: 4689 [65792/118836 (55%)] Loss: 12289.930664\n",
      "Train Epoch: 4689 [98560/118836 (83%)] Loss: 12172.211914\n",
      "    epoch          : 4689\n",
      "    loss           : 12217.664790600444\n",
      "    val_loss       : 12221.125039924531\n",
      "    val_log_likelihood: -12140.984857546267\n",
      "    val_log_marginal: -12148.874906573561\n",
      "Train Epoch: 4690 [256/118836 (0%)] Loss: 12232.417969\n",
      "Train Epoch: 4690 [33024/118836 (28%)] Loss: 12210.324219\n",
      "Train Epoch: 4690 [65792/118836 (55%)] Loss: 12223.575195\n",
      "Train Epoch: 4690 [98560/118836 (83%)] Loss: 12168.346680\n",
      "    epoch          : 4690\n",
      "    loss           : 12215.790134537841\n",
      "    val_loss       : 12217.84665059165\n",
      "    val_log_likelihood: -12139.023296790994\n",
      "    val_log_marginal: -12146.917029724476\n",
      "Train Epoch: 4691 [256/118836 (0%)] Loss: 12252.158203\n",
      "Train Epoch: 4691 [33024/118836 (28%)] Loss: 12189.827148\n",
      "Train Epoch: 4691 [65792/118836 (55%)] Loss: 12166.341797\n",
      "Train Epoch: 4691 [98560/118836 (83%)] Loss: 12196.402344\n",
      "    epoch          : 4691\n",
      "    loss           : 12219.878741147124\n",
      "    val_loss       : 12218.533377819667\n",
      "    val_log_likelihood: -12140.663230685224\n",
      "    val_log_marginal: -12148.557558651162\n",
      "Train Epoch: 4692 [256/118836 (0%)] Loss: 12212.025391\n",
      "Train Epoch: 4692 [33024/118836 (28%)] Loss: 12241.148438\n",
      "Train Epoch: 4692 [65792/118836 (55%)] Loss: 12299.164062\n",
      "Train Epoch: 4692 [98560/118836 (83%)] Loss: 12166.709961\n",
      "    epoch          : 4692\n",
      "    loss           : 12219.478559081626\n",
      "    val_loss       : 12217.843972051272\n",
      "    val_log_likelihood: -12140.840189302884\n",
      "    val_log_marginal: -12148.73328169927\n",
      "Train Epoch: 4693 [256/118836 (0%)] Loss: 12219.714844\n",
      "Train Epoch: 4693 [33024/118836 (28%)] Loss: 12221.200195\n",
      "Train Epoch: 4693 [65792/118836 (55%)] Loss: 12198.254883\n",
      "Train Epoch: 4693 [98560/118836 (83%)] Loss: 12256.588867\n",
      "    epoch          : 4693\n",
      "    loss           : 12216.97364912893\n",
      "    val_loss       : 12217.811222796776\n",
      "    val_log_likelihood: -12139.769723816169\n",
      "    val_log_marginal: -12147.66274113675\n",
      "Train Epoch: 4694 [256/118836 (0%)] Loss: 12286.488281\n",
      "Train Epoch: 4694 [33024/118836 (28%)] Loss: 12164.380859\n",
      "Train Epoch: 4694 [65792/118836 (55%)] Loss: 12247.518555\n",
      "Train Epoch: 4694 [98560/118836 (83%)] Loss: 12235.534180\n",
      "    epoch          : 4694\n",
      "    loss           : 12219.229236778845\n",
      "    val_loss       : 12219.060902867703\n",
      "    val_log_likelihood: -12139.725555889423\n",
      "    val_log_marginal: -12147.617785540742\n",
      "Train Epoch: 4695 [256/118836 (0%)] Loss: 12311.807617\n",
      "Train Epoch: 4695 [33024/118836 (28%)] Loss: 12314.491211\n",
      "Train Epoch: 4695 [65792/118836 (55%)] Loss: 12220.207031\n",
      "Train Epoch: 4695 [98560/118836 (83%)] Loss: 12281.115234\n",
      "    epoch          : 4695\n",
      "    loss           : 12221.112330535309\n",
      "    val_loss       : 12219.631974414157\n",
      "    val_log_likelihood: -12141.497638964278\n",
      "    val_log_marginal: -12149.389569788538\n",
      "Train Epoch: 4696 [256/118836 (0%)] Loss: 12213.960938\n",
      "Train Epoch: 4696 [33024/118836 (28%)] Loss: 12361.042969\n",
      "Train Epoch: 4696 [65792/118836 (55%)] Loss: 12273.880859\n",
      "Train Epoch: 4696 [98560/118836 (83%)] Loss: 12333.929688\n",
      "    epoch          : 4696\n",
      "    loss           : 12217.990726614195\n",
      "    val_loss       : 12220.177961971875\n",
      "    val_log_likelihood: -12140.527090118383\n",
      "    val_log_marginal: -12148.42355911176\n",
      "Train Epoch: 4697 [256/118836 (0%)] Loss: 12239.359375\n",
      "Train Epoch: 4697 [33024/118836 (28%)] Loss: 12195.242188\n",
      "Train Epoch: 4697 [65792/118836 (55%)] Loss: 12193.424805\n",
      "Train Epoch: 4697 [98560/118836 (83%)] Loss: 12307.882812\n",
      "    epoch          : 4697\n",
      "    loss           : 12218.82038939723\n",
      "    val_loss       : 12217.139092382007\n",
      "    val_log_likelihood: -12140.314670892783\n",
      "    val_log_marginal: -12148.208120586401\n",
      "Train Epoch: 4698 [256/118836 (0%)] Loss: 12291.070312\n",
      "Train Epoch: 4698 [33024/118836 (28%)] Loss: 12235.420898\n",
      "Train Epoch: 4698 [65792/118836 (55%)] Loss: 12154.027344\n",
      "Train Epoch: 4698 [98560/118836 (83%)] Loss: 12253.404297\n",
      "    epoch          : 4698\n",
      "    loss           : 12221.56422695668\n",
      "    val_loss       : 12218.906756935534\n",
      "    val_log_likelihood: -12140.871145607165\n",
      "    val_log_marginal: -12148.762872711462\n",
      "Train Epoch: 4699 [256/118836 (0%)] Loss: 12231.245117\n",
      "Train Epoch: 4699 [33024/118836 (28%)] Loss: 12210.818359\n",
      "Train Epoch: 4699 [65792/118836 (55%)] Loss: 12255.878906\n",
      "Train Epoch: 4699 [98560/118836 (83%)] Loss: 12184.251953\n",
      "    epoch          : 4699\n",
      "    loss           : 12217.332990526778\n",
      "    val_loss       : 12222.286284282985\n",
      "    val_log_likelihood: -12139.30338897074\n",
      "    val_log_marginal: -12147.194823802387\n",
      "Train Epoch: 4700 [256/118836 (0%)] Loss: 12280.243164\n",
      "Train Epoch: 4700 [33024/118836 (28%)] Loss: 12211.925781\n",
      "Train Epoch: 4700 [65792/118836 (55%)] Loss: 12162.352539\n",
      "Train Epoch: 4700 [98560/118836 (83%)] Loss: 12282.393555\n",
      "    epoch          : 4700\n",
      "    loss           : 12216.518915264423\n",
      "    val_loss       : 12217.132022038535\n",
      "    val_log_likelihood: -12139.31220646583\n",
      "    val_log_marginal: -12147.200496580554\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch4700.pth ...\n",
      "Train Epoch: 4701 [256/118836 (0%)] Loss: 12270.444336\n",
      "Train Epoch: 4701 [33024/118836 (28%)] Loss: 12205.134766\n",
      "Train Epoch: 4701 [65792/118836 (55%)] Loss: 12184.129883\n",
      "Train Epoch: 4701 [98560/118836 (83%)] Loss: 12244.265625\n",
      "    epoch          : 4701\n",
      "    loss           : 12217.584469667598\n",
      "    val_loss       : 12221.226881308441\n",
      "    val_log_likelihood: -12140.937066726116\n",
      "    val_log_marginal: -12148.826493053626\n",
      "Train Epoch: 4702 [256/118836 (0%)] Loss: 12195.230469\n",
      "Train Epoch: 4702 [33024/118836 (28%)] Loss: 12285.574219\n",
      "Train Epoch: 4702 [65792/118836 (55%)] Loss: 12279.126953\n",
      "Train Epoch: 4702 [98560/118836 (83%)] Loss: 12191.652344\n",
      "    epoch          : 4702\n",
      "    loss           : 12217.466377171215\n",
      "    val_loss       : 12218.442300144507\n",
      "    val_log_likelihood: -12140.762933435431\n",
      "    val_log_marginal: -12148.653020963438\n",
      "Train Epoch: 4703 [256/118836 (0%)] Loss: 12213.431641\n",
      "Train Epoch: 4703 [33024/118836 (28%)] Loss: 12154.265625\n",
      "Train Epoch: 4703 [65792/118836 (55%)] Loss: 12162.162109\n",
      "Train Epoch: 4703 [98560/118836 (83%)] Loss: 12308.460938\n",
      "    epoch          : 4703\n",
      "    loss           : 12217.899931826407\n",
      "    val_loss       : 12220.464254104994\n",
      "    val_log_likelihood: -12139.8622239131\n",
      "    val_log_marginal: -12147.75239148901\n",
      "Train Epoch: 4704 [256/118836 (0%)] Loss: 12199.318359\n",
      "Train Epoch: 4704 [33024/118836 (28%)] Loss: 12139.607422\n",
      "Train Epoch: 4704 [65792/118836 (55%)] Loss: 12196.026367\n",
      "Train Epoch: 4704 [98560/118836 (83%)] Loss: 12267.494141\n",
      "    epoch          : 4704\n",
      "    loss           : 12220.407424298231\n",
      "    val_loss       : 12214.667990890372\n",
      "    val_log_likelihood: -12138.926060729425\n",
      "    val_log_marginal: -12146.82305525978\n",
      "Train Epoch: 4705 [256/118836 (0%)] Loss: 12280.065430\n",
      "Train Epoch: 4705 [33024/118836 (28%)] Loss: 12342.457031\n",
      "Train Epoch: 4705 [65792/118836 (55%)] Loss: 12345.121094\n",
      "Train Epoch: 4705 [98560/118836 (83%)] Loss: 12299.347656\n",
      "    epoch          : 4705\n",
      "    loss           : 12217.884965622416\n",
      "    val_loss       : 12220.482930282302\n",
      "    val_log_likelihood: -12140.335901959264\n",
      "    val_log_marginal: -12148.226958358799\n",
      "Train Epoch: 4706 [256/118836 (0%)] Loss: 12161.498047\n",
      "Train Epoch: 4706 [33024/118836 (28%)] Loss: 12377.470703\n",
      "Train Epoch: 4706 [65792/118836 (55%)] Loss: 12312.107422\n",
      "Train Epoch: 4706 [98560/118836 (83%)] Loss: 12314.043945\n",
      "    epoch          : 4706\n",
      "    loss           : 12218.813608709419\n",
      "    val_loss       : 12214.8949457236\n",
      "    val_log_likelihood: -12139.69117975858\n",
      "    val_log_marginal: -12147.586729889941\n",
      "Train Epoch: 4707 [256/118836 (0%)] Loss: 12226.031250\n",
      "Train Epoch: 4707 [33024/118836 (28%)] Loss: 12176.730469\n",
      "Train Epoch: 4707 [65792/118836 (55%)] Loss: 12201.840820\n",
      "Train Epoch: 4707 [98560/118836 (83%)] Loss: 12198.476562\n",
      "    epoch          : 4707\n",
      "    loss           : 12217.128580567618\n",
      "    val_loss       : 12213.555930896851\n",
      "    val_log_likelihood: -12139.874390153278\n",
      "    val_log_marginal: -12147.77566430728\n",
      "Train Epoch: 4708 [256/118836 (0%)] Loss: 12219.933594\n",
      "Train Epoch: 4708 [33024/118836 (28%)] Loss: 12241.863281\n",
      "Train Epoch: 4708 [65792/118836 (55%)] Loss: 12204.597656\n",
      "Train Epoch: 4708 [98560/118836 (83%)] Loss: 12224.970703\n",
      "    epoch          : 4708\n",
      "    loss           : 12214.118408162738\n",
      "    val_loss       : 12221.881149092713\n",
      "    val_log_likelihood: -12141.4221848506\n",
      "    val_log_marginal: -12149.315874283262\n",
      "Train Epoch: 4709 [256/118836 (0%)] Loss: 12231.125977\n",
      "Train Epoch: 4709 [33024/118836 (28%)] Loss: 12235.639648\n",
      "Train Epoch: 4709 [65792/118836 (55%)] Loss: 12350.347656\n",
      "Train Epoch: 4709 [98560/118836 (83%)] Loss: 12181.205078\n",
      "    epoch          : 4709\n",
      "    loss           : 12223.188899335712\n",
      "    val_loss       : 12215.127870459768\n",
      "    val_log_likelihood: -12139.974742652761\n",
      "    val_log_marginal: -12147.859190803094\n",
      "Train Epoch: 4710 [256/118836 (0%)] Loss: 12173.053711\n",
      "Train Epoch: 4710 [33024/118836 (28%)] Loss: 12217.551758\n",
      "Train Epoch: 4710 [65792/118836 (55%)] Loss: 12262.813477\n",
      "Train Epoch: 4710 [98560/118836 (83%)] Loss: 12225.327148\n",
      "    epoch          : 4710\n",
      "    loss           : 12215.25689781069\n",
      "    val_loss       : 12214.138994634215\n",
      "    val_log_likelihood: -12140.408623636527\n",
      "    val_log_marginal: -12148.296451673663\n",
      "Train Epoch: 4711 [256/118836 (0%)] Loss: 12202.203125\n",
      "Train Epoch: 4711 [33024/118836 (28%)] Loss: 12200.785156\n",
      "Train Epoch: 4711 [65792/118836 (55%)] Loss: 12186.309570\n",
      "Train Epoch: 4711 [98560/118836 (83%)] Loss: 12211.504883\n",
      "    epoch          : 4711\n",
      "    loss           : 12217.020205070048\n",
      "    val_loss       : 12217.520724171054\n",
      "    val_log_likelihood: -12139.329361332972\n",
      "    val_log_marginal: -12147.22273631451\n",
      "Train Epoch: 4712 [256/118836 (0%)] Loss: 12160.269531\n",
      "Train Epoch: 4712 [33024/118836 (28%)] Loss: 12227.757812\n",
      "Train Epoch: 4712 [65792/118836 (55%)] Loss: 12253.402344\n",
      "Train Epoch: 4712 [98560/118836 (83%)] Loss: 12201.695312\n",
      "    epoch          : 4712\n",
      "    loss           : 12217.778132754342\n",
      "    val_loss       : 12215.09118028367\n",
      "    val_log_likelihood: -12137.946172262717\n",
      "    val_log_marginal: -12145.845580470152\n",
      "Train Epoch: 4713 [256/118836 (0%)] Loss: 12277.689453\n",
      "Train Epoch: 4713 [33024/118836 (28%)] Loss: 12240.274414\n",
      "Train Epoch: 4713 [65792/118836 (55%)] Loss: 12306.806641\n",
      "Train Epoch: 4713 [98560/118836 (83%)] Loss: 12275.244141\n",
      "    epoch          : 4713\n",
      "    loss           : 12211.5363211784\n",
      "    val_loss       : 12218.031222304247\n",
      "    val_log_likelihood: -12140.894994087314\n",
      "    val_log_marginal: -12148.79268387427\n",
      "Train Epoch: 4714 [256/118836 (0%)] Loss: 12194.009766\n",
      "Train Epoch: 4714 [33024/118836 (28%)] Loss: 12152.977539\n",
      "Train Epoch: 4714 [65792/118836 (55%)] Loss: 12192.026367\n",
      "Train Epoch: 4714 [98560/118836 (83%)] Loss: 12277.712891\n",
      "    epoch          : 4714\n",
      "    loss           : 12216.4435881281\n",
      "    val_loss       : 12219.398334921714\n",
      "    val_log_likelihood: -12138.493239182693\n",
      "    val_log_marginal: -12146.391439288767\n",
      "Train Epoch: 4715 [256/118836 (0%)] Loss: 12177.380859\n",
      "Train Epoch: 4715 [33024/118836 (28%)] Loss: 12155.567383\n",
      "Train Epoch: 4715 [65792/118836 (55%)] Loss: 12280.291992\n",
      "Train Epoch: 4715 [98560/118836 (83%)] Loss: 12200.517578\n",
      "    epoch          : 4715\n",
      "    loss           : 12217.617961318754\n",
      "    val_loss       : 12219.228401593284\n",
      "    val_log_likelihood: -12139.691744694737\n",
      "    val_log_marginal: -12147.585003908867\n",
      "Train Epoch: 4716 [256/118836 (0%)] Loss: 12256.382812\n",
      "Train Epoch: 4716 [33024/118836 (28%)] Loss: 12285.792969\n",
      "Train Epoch: 4716 [65792/118836 (55%)] Loss: 12228.536133\n",
      "Train Epoch: 4716 [98560/118836 (83%)] Loss: 12219.736328\n",
      "    epoch          : 4716\n",
      "    loss           : 12218.077456187966\n",
      "    val_loss       : 12216.229198884123\n",
      "    val_log_likelihood: -12139.89453641956\n",
      "    val_log_marginal: -12147.792967883746\n",
      "Train Epoch: 4717 [256/118836 (0%)] Loss: 12166.187500\n",
      "Train Epoch: 4717 [33024/118836 (28%)] Loss: 12247.445312\n",
      "Train Epoch: 4717 [65792/118836 (55%)] Loss: 12150.092773\n",
      "Train Epoch: 4717 [98560/118836 (83%)] Loss: 12206.945312\n",
      "    epoch          : 4717\n",
      "    loss           : 12216.104937739092\n",
      "    val_loss       : 12224.115651861906\n",
      "    val_log_likelihood: -12139.585204068446\n",
      "    val_log_marginal: -12147.47470803581\n",
      "Train Epoch: 4718 [256/118836 (0%)] Loss: 12219.548828\n",
      "Train Epoch: 4718 [33024/118836 (28%)] Loss: 12316.025391\n",
      "Train Epoch: 4718 [65792/118836 (55%)] Loss: 12298.298828\n",
      "Train Epoch: 4718 [98560/118836 (83%)] Loss: 12189.395508\n",
      "    epoch          : 4718\n",
      "    loss           : 12216.979192837573\n",
      "    val_loss       : 12220.07130579648\n",
      "    val_log_likelihood: -12138.48372993564\n",
      "    val_log_marginal: -12146.374648336105\n",
      "Train Epoch: 4719 [256/118836 (0%)] Loss: 12361.341797\n",
      "Train Epoch: 4719 [33024/118836 (28%)] Loss: 12231.724609\n",
      "Train Epoch: 4719 [65792/118836 (55%)] Loss: 12194.707031\n",
      "Train Epoch: 4719 [98560/118836 (83%)] Loss: 12252.632812\n",
      "    epoch          : 4719\n",
      "    loss           : 12217.833380828682\n",
      "    val_loss       : 12216.803210975764\n",
      "    val_log_likelihood: -12141.802971205541\n",
      "    val_log_marginal: -12149.693889288905\n",
      "Train Epoch: 4720 [256/118836 (0%)] Loss: 12180.585938\n",
      "Train Epoch: 4720 [33024/118836 (28%)] Loss: 12378.858398\n",
      "Train Epoch: 4720 [65792/118836 (55%)] Loss: 12226.134766\n",
      "Train Epoch: 4720 [98560/118836 (83%)] Loss: 12325.707031\n",
      "    epoch          : 4720\n",
      "    loss           : 12218.699554125309\n",
      "    val_loss       : 12220.427892522383\n",
      "    val_log_likelihood: -12139.563040703835\n",
      "    val_log_marginal: -12147.45403362419\n",
      "Train Epoch: 4721 [256/118836 (0%)] Loss: 12326.607422\n",
      "Train Epoch: 4721 [33024/118836 (28%)] Loss: 12217.510742\n",
      "Train Epoch: 4721 [65792/118836 (55%)] Loss: 12295.263672\n",
      "Train Epoch: 4721 [98560/118836 (83%)] Loss: 12176.582031\n",
      "    epoch          : 4721\n",
      "    loss           : 12218.064692217225\n",
      "    val_loss       : 12216.343437853275\n",
      "    val_log_likelihood: -12140.522834438329\n",
      "    val_log_marginal: -12148.41693501559\n",
      "Train Epoch: 4722 [256/118836 (0%)] Loss: 12339.638672\n",
      "Train Epoch: 4722 [33024/118836 (28%)] Loss: 12273.967773\n",
      "Train Epoch: 4722 [65792/118836 (55%)] Loss: 12229.565430\n",
      "Train Epoch: 4722 [98560/118836 (83%)] Loss: 12360.885742\n",
      "    epoch          : 4722\n",
      "    loss           : 12217.633908447064\n",
      "    val_loss       : 12215.649595332892\n",
      "    val_log_likelihood: -12139.070776145058\n",
      "    val_log_marginal: -12146.962257452044\n",
      "Train Epoch: 4723 [256/118836 (0%)] Loss: 12190.829102\n",
      "Train Epoch: 4723 [33024/118836 (28%)] Loss: 12171.690430\n",
      "Train Epoch: 4723 [65792/118836 (55%)] Loss: 12278.281250\n",
      "Train Epoch: 4723 [98560/118836 (83%)] Loss: 12199.944336\n",
      "    epoch          : 4723\n",
      "    loss           : 12217.008047715053\n",
      "    val_loss       : 12222.889956670599\n",
      "    val_log_likelihood: -12139.582451438431\n",
      "    val_log_marginal: -12147.470277317416\n",
      "Train Epoch: 4724 [256/118836 (0%)] Loss: 12199.167969\n",
      "Train Epoch: 4724 [33024/118836 (28%)] Loss: 12262.500000\n",
      "Train Epoch: 4724 [65792/118836 (55%)] Loss: 12206.401367\n",
      "Train Epoch: 4724 [98560/118836 (83%)] Loss: 12210.950195\n",
      "    epoch          : 4724\n",
      "    loss           : 12217.751843110267\n",
      "    val_loss       : 12216.479565707854\n",
      "    val_log_likelihood: -12138.186918262769\n",
      "    val_log_marginal: -12146.07983671772\n",
      "Train Epoch: 4725 [256/118836 (0%)] Loss: 12330.177734\n",
      "Train Epoch: 4725 [33024/118836 (28%)] Loss: 12189.925781\n",
      "Train Epoch: 4725 [65792/118836 (55%)] Loss: 12171.924805\n",
      "Train Epoch: 4725 [98560/118836 (83%)] Loss: 12211.479492\n",
      "    epoch          : 4725\n",
      "    loss           : 12219.864216132908\n",
      "    val_loss       : 12221.301935836946\n",
      "    val_log_likelihood: -12139.78773036859\n",
      "    val_log_marginal: -12147.679970420702\n",
      "Train Epoch: 4726 [256/118836 (0%)] Loss: 12237.587891\n",
      "Train Epoch: 4726 [33024/118836 (28%)] Loss: 12267.348633\n",
      "Train Epoch: 4726 [65792/118836 (55%)] Loss: 12192.538086\n",
      "Train Epoch: 4726 [98560/118836 (83%)] Loss: 12242.103516\n",
      "    epoch          : 4726\n",
      "    loss           : 12214.594122369987\n",
      "    val_loss       : 12218.240100998168\n",
      "    val_log_likelihood: -12139.198109232837\n",
      "    val_log_marginal: -12147.095282071416\n",
      "Train Epoch: 4727 [256/118836 (0%)] Loss: 12261.166016\n",
      "Train Epoch: 4727 [33024/118836 (28%)] Loss: 12237.220703\n",
      "Train Epoch: 4727 [65792/118836 (55%)] Loss: 12179.519531\n",
      "Train Epoch: 4727 [98560/118836 (83%)] Loss: 12231.207031\n",
      "    epoch          : 4727\n",
      "    loss           : 12217.058903115954\n",
      "    val_loss       : 12216.786684023065\n",
      "    val_log_likelihood: -12140.953462475443\n",
      "    val_log_marginal: -12148.843899919277\n",
      "Train Epoch: 4728 [256/118836 (0%)] Loss: 12317.501953\n",
      "Train Epoch: 4728 [33024/118836 (28%)] Loss: 12188.172852\n",
      "Train Epoch: 4728 [65792/118836 (55%)] Loss: 12174.062500\n",
      "Train Epoch: 4728 [98560/118836 (83%)] Loss: 12226.422852\n",
      "    epoch          : 4728\n",
      "    loss           : 12216.607584716192\n",
      "    val_loss       : 12218.779918534568\n",
      "    val_log_likelihood: -12139.487978669096\n",
      "    val_log_marginal: -12147.37801067481\n",
      "Train Epoch: 4729 [256/118836 (0%)] Loss: 12248.312500\n",
      "Train Epoch: 4729 [33024/118836 (28%)] Loss: 12158.374023\n",
      "Train Epoch: 4729 [65792/118836 (55%)] Loss: 12182.730469\n",
      "Train Epoch: 4729 [98560/118836 (83%)] Loss: 12195.524414\n",
      "    epoch          : 4729\n",
      "    loss           : 12216.903372654311\n",
      "    val_loss       : 12217.737694824447\n",
      "    val_log_likelihood: -12139.35170789392\n",
      "    val_log_marginal: -12147.238610992452\n",
      "Train Epoch: 4730 [256/118836 (0%)] Loss: 12235.432617\n",
      "Train Epoch: 4730 [33024/118836 (28%)] Loss: 12268.208008\n",
      "Train Epoch: 4730 [65792/118836 (55%)] Loss: 12166.844727\n",
      "Train Epoch: 4730 [98560/118836 (83%)] Loss: 12237.159180\n",
      "    epoch          : 4730\n",
      "    loss           : 12214.466048742504\n",
      "    val_loss       : 12219.152657220013\n",
      "    val_log_likelihood: -12142.04558600212\n",
      "    val_log_marginal: -12149.934049741598\n",
      "Train Epoch: 4731 [256/118836 (0%)] Loss: 12326.330078\n",
      "Train Epoch: 4731 [33024/118836 (28%)] Loss: 12260.226562\n",
      "Train Epoch: 4731 [65792/118836 (55%)] Loss: 12187.899414\n",
      "Train Epoch: 4731 [98560/118836 (83%)] Loss: 12193.051758\n",
      "    epoch          : 4731\n",
      "    loss           : 12222.42879688146\n",
      "    val_loss       : 12222.523064504263\n",
      "    val_log_likelihood: -12138.829093161963\n",
      "    val_log_marginal: -12146.723966778145\n",
      "Train Epoch: 4732 [256/118836 (0%)] Loss: 12172.233398\n",
      "Train Epoch: 4732 [33024/118836 (28%)] Loss: 12370.138672\n",
      "Train Epoch: 4732 [65792/118836 (55%)] Loss: 12252.482422\n",
      "Train Epoch: 4732 [98560/118836 (83%)] Loss: 12211.247070\n",
      "    epoch          : 4732\n",
      "    loss           : 12220.427949396453\n",
      "    val_loss       : 12215.336166244211\n",
      "    val_log_likelihood: -12140.435755919149\n",
      "    val_log_marginal: -12148.327749924101\n",
      "Train Epoch: 4733 [256/118836 (0%)] Loss: 12187.875977\n",
      "Train Epoch: 4733 [33024/118836 (28%)] Loss: 12349.382812\n",
      "Train Epoch: 4733 [65792/118836 (55%)] Loss: 12190.310547\n",
      "Train Epoch: 4733 [98560/118836 (83%)] Loss: 12291.013672\n",
      "    epoch          : 4733\n",
      "    loss           : 12216.475398379342\n",
      "    val_loss       : 12221.194511100126\n",
      "    val_log_likelihood: -12139.97502843259\n",
      "    val_log_marginal: -12147.869389322317\n",
      "Train Epoch: 4734 [256/118836 (0%)] Loss: 12250.378906\n",
      "Train Epoch: 4734 [33024/118836 (28%)] Loss: 12260.359375\n",
      "Train Epoch: 4734 [65792/118836 (55%)] Loss: 12191.313477\n",
      "Train Epoch: 4734 [98560/118836 (83%)] Loss: 12220.116211\n",
      "    epoch          : 4734\n",
      "    loss           : 12216.341444860162\n",
      "    val_loss       : 12218.799528220316\n",
      "    val_log_likelihood: -12139.849185470946\n",
      "    val_log_marginal: -12147.734740832118\n",
      "Train Epoch: 4735 [256/118836 (0%)] Loss: 12232.326172\n",
      "Train Epoch: 4735 [33024/118836 (28%)] Loss: 12226.703125\n",
      "Train Epoch: 4735 [65792/118836 (55%)] Loss: 12253.932617\n",
      "Train Epoch: 4735 [98560/118836 (83%)] Loss: 12236.954102\n",
      "    epoch          : 4735\n",
      "    loss           : 12217.25354114971\n",
      "    val_loss       : 12222.996599097818\n",
      "    val_log_likelihood: -12140.577430824804\n",
      "    val_log_marginal: -12148.478988200499\n",
      "Train Epoch: 4736 [256/118836 (0%)] Loss: 12205.728516\n",
      "Train Epoch: 4736 [33024/118836 (28%)] Loss: 12283.074219\n",
      "Train Epoch: 4736 [65792/118836 (55%)] Loss: 12220.070312\n",
      "Train Epoch: 4736 [98560/118836 (83%)] Loss: 12233.214844\n",
      "    epoch          : 4736\n",
      "    loss           : 12218.550411626344\n",
      "    val_loss       : 12217.260618012186\n",
      "    val_log_likelihood: -12140.577055546939\n",
      "    val_log_marginal: -12148.471281171307\n",
      "Train Epoch: 4737 [256/118836 (0%)] Loss: 12199.961914\n",
      "Train Epoch: 4737 [33024/118836 (28%)] Loss: 12222.921875\n",
      "Train Epoch: 4737 [65792/118836 (55%)] Loss: 12208.184570\n",
      "Train Epoch: 4737 [98560/118836 (83%)] Loss: 12256.999023\n",
      "    epoch          : 4737\n",
      "    loss           : 12224.205938533913\n",
      "    val_loss       : 12218.002839530027\n",
      "    val_log_likelihood: -12141.234710536859\n",
      "    val_log_marginal: -12149.134213858346\n",
      "Train Epoch: 4738 [256/118836 (0%)] Loss: 12184.528320\n",
      "Train Epoch: 4738 [33024/118836 (28%)] Loss: 12274.038086\n",
      "Train Epoch: 4738 [65792/118836 (55%)] Loss: 12195.162109\n",
      "Train Epoch: 4738 [98560/118836 (83%)] Loss: 12254.586914\n",
      "    epoch          : 4738\n",
      "    loss           : 12216.760264810793\n",
      "    val_loss       : 12214.720685958167\n",
      "    val_log_likelihood: -12137.817629982164\n",
      "    val_log_marginal: -12145.71602185621\n",
      "Train Epoch: 4739 [256/118836 (0%)] Loss: 12185.928711\n",
      "Train Epoch: 4739 [33024/118836 (28%)] Loss: 12259.612305\n",
      "Train Epoch: 4739 [65792/118836 (55%)] Loss: 12193.473633\n",
      "Train Epoch: 4739 [98560/118836 (83%)] Loss: 12163.796875\n",
      "    epoch          : 4739\n",
      "    loss           : 12217.618558403123\n",
      "    val_loss       : 12214.040025221271\n",
      "    val_log_likelihood: -12140.510765127428\n",
      "    val_log_marginal: -12148.401117569707\n",
      "Train Epoch: 4740 [256/118836 (0%)] Loss: 12158.771484\n",
      "Train Epoch: 4740 [33024/118836 (28%)] Loss: 12180.390625\n",
      "Train Epoch: 4740 [65792/118836 (55%)] Loss: 12190.559570\n",
      "Train Epoch: 4740 [98560/118836 (83%)] Loss: 12330.798828\n",
      "    epoch          : 4740\n",
      "    loss           : 12216.8684805366\n",
      "    val_loss       : 12218.32150003381\n",
      "    val_log_likelihood: -12139.036225218413\n",
      "    val_log_marginal: -12146.927774943568\n",
      "Train Epoch: 4741 [256/118836 (0%)] Loss: 12281.711914\n",
      "Train Epoch: 4741 [33024/118836 (28%)] Loss: 12212.941406\n",
      "Train Epoch: 4741 [65792/118836 (55%)] Loss: 12265.024414\n",
      "Train Epoch: 4741 [98560/118836 (83%)] Loss: 12179.212891\n",
      "    epoch          : 4741\n",
      "    loss           : 12219.614018397177\n",
      "    val_loss       : 12216.518227507735\n",
      "    val_log_likelihood: -12138.644522849463\n",
      "    val_log_marginal: -12146.541128706607\n",
      "Train Epoch: 4742 [256/118836 (0%)] Loss: 12251.085938\n",
      "Train Epoch: 4742 [33024/118836 (28%)] Loss: 12290.611328\n",
      "Train Epoch: 4742 [65792/118836 (55%)] Loss: 12259.753906\n",
      "Train Epoch: 4742 [98560/118836 (83%)] Loss: 12281.746094\n",
      "    epoch          : 4742\n",
      "    loss           : 12220.603492685072\n",
      "    val_loss       : 12212.150078139292\n",
      "    val_log_likelihood: -12138.672842838865\n",
      "    val_log_marginal: -12146.560250939832\n",
      "Train Epoch: 4743 [256/118836 (0%)] Loss: 12188.968750\n",
      "Train Epoch: 4743 [33024/118836 (28%)] Loss: 12147.844727\n",
      "Train Epoch: 4743 [65792/118836 (55%)] Loss: 12320.089844\n",
      "Train Epoch: 4743 [98560/118836 (83%)] Loss: 12218.932617\n",
      "    epoch          : 4743\n",
      "    loss           : 12218.243652101426\n",
      "    val_loss       : 12219.733984074608\n",
      "    val_log_likelihood: -12140.277523069168\n",
      "    val_log_marginal: -12148.166177590652\n",
      "Train Epoch: 4744 [256/118836 (0%)] Loss: 12251.683594\n",
      "Train Epoch: 4744 [33024/118836 (28%)] Loss: 12344.912109\n",
      "Train Epoch: 4744 [65792/118836 (55%)] Loss: 12286.900391\n",
      "Train Epoch: 4744 [98560/118836 (83%)] Loss: 12222.507812\n",
      "    epoch          : 4744\n",
      "    loss           : 12219.748476271712\n",
      "    val_loss       : 12221.584204508677\n",
      "    val_log_likelihood: -12137.54663639242\n",
      "    val_log_marginal: -12145.442714494693\n",
      "Train Epoch: 4745 [256/118836 (0%)] Loss: 12186.355469\n",
      "Train Epoch: 4745 [33024/118836 (28%)] Loss: 12199.195312\n",
      "Train Epoch: 4745 [65792/118836 (55%)] Loss: 12195.631836\n",
      "Train Epoch: 4745 [98560/118836 (83%)] Loss: 12216.060547\n",
      "    epoch          : 4745\n",
      "    loss           : 12220.554159235422\n",
      "    val_loss       : 12215.548010044862\n",
      "    val_log_likelihood: -12139.4519865656\n",
      "    val_log_marginal: -12147.342832428356\n",
      "Train Epoch: 4746 [256/118836 (0%)] Loss: 12202.097656\n",
      "Train Epoch: 4746 [33024/118836 (28%)] Loss: 12159.681641\n",
      "Train Epoch: 4746 [65792/118836 (55%)] Loss: 12305.772461\n",
      "Train Epoch: 4746 [98560/118836 (83%)] Loss: 12180.990234\n",
      "    epoch          : 4746\n",
      "    loss           : 12219.55434533964\n",
      "    val_loss       : 12217.911194985469\n",
      "    val_log_likelihood: -12141.367415283808\n",
      "    val_log_marginal: -12149.254471566734\n",
      "Train Epoch: 4747 [256/118836 (0%)] Loss: 12236.038086\n",
      "Train Epoch: 4747 [33024/118836 (28%)] Loss: 12206.353516\n",
      "Train Epoch: 4747 [65792/118836 (55%)] Loss: 12225.461914\n",
      "Train Epoch: 4747 [98560/118836 (83%)] Loss: 12226.351562\n",
      "    epoch          : 4747\n",
      "    loss           : 12216.30728035825\n",
      "    val_loss       : 12215.730287453736\n",
      "    val_log_likelihood: -12138.203970869521\n",
      "    val_log_marginal: -12146.09693938216\n",
      "Train Epoch: 4748 [256/118836 (0%)] Loss: 12157.056641\n",
      "Train Epoch: 4748 [33024/118836 (28%)] Loss: 12112.703125\n",
      "Train Epoch: 4748 [65792/118836 (55%)] Loss: 12169.101562\n",
      "Train Epoch: 4748 [98560/118836 (83%)] Loss: 12194.721680\n",
      "    epoch          : 4748\n",
      "    loss           : 12215.227018875363\n",
      "    val_loss       : 12217.584942093932\n",
      "    val_log_likelihood: -12140.454261495812\n",
      "    val_log_marginal: -12148.34807461333\n",
      "Train Epoch: 4749 [256/118836 (0%)] Loss: 12275.146484\n",
      "Train Epoch: 4749 [33024/118836 (28%)] Loss: 12244.884766\n",
      "Train Epoch: 4749 [65792/118836 (55%)] Loss: 12350.219727\n",
      "Train Epoch: 4749 [98560/118836 (83%)] Loss: 12146.712891\n",
      "    epoch          : 4749\n",
      "    loss           : 12218.329028865539\n",
      "    val_loss       : 12220.73943118181\n",
      "    val_log_likelihood: -12138.248839271868\n",
      "    val_log_marginal: -12146.144598165902\n",
      "Train Epoch: 4750 [256/118836 (0%)] Loss: 12185.841797\n",
      "Train Epoch: 4750 [33024/118836 (28%)] Loss: 12294.998047\n",
      "Train Epoch: 4750 [65792/118836 (55%)] Loss: 12255.043945\n",
      "Train Epoch: 4750 [98560/118836 (83%)] Loss: 12305.843750\n",
      "    epoch          : 4750\n",
      "    loss           : 12216.762901610318\n",
      "    val_loss       : 12214.044626079056\n",
      "    val_log_likelihood: -12139.058497790013\n",
      "    val_log_marginal: -12146.952260838521\n",
      "Train Epoch: 4751 [256/118836 (0%)] Loss: 12180.472656\n",
      "Train Epoch: 4751 [33024/118836 (28%)] Loss: 12204.919922\n",
      "Train Epoch: 4751 [65792/118836 (55%)] Loss: 12249.010742\n",
      "Train Epoch: 4751 [98560/118836 (83%)] Loss: 12207.967773\n",
      "    epoch          : 4751\n",
      "    loss           : 12218.920713948768\n",
      "    val_loss       : 12222.599879186884\n",
      "    val_log_likelihood: -12139.42040942928\n",
      "    val_log_marginal: -12147.312296488297\n",
      "Train Epoch: 4752 [256/118836 (0%)] Loss: 12218.695312\n",
      "Train Epoch: 4752 [33024/118836 (28%)] Loss: 12234.789062\n",
      "Train Epoch: 4752 [65792/118836 (55%)] Loss: 12289.571289\n",
      "Train Epoch: 4752 [98560/118836 (83%)] Loss: 12236.285156\n",
      "    epoch          : 4752\n",
      "    loss           : 12218.818388292235\n",
      "    val_loss       : 12218.18789180879\n",
      "    val_log_likelihood: -12138.320913946183\n",
      "    val_log_marginal: -12146.214432398248\n",
      "Train Epoch: 4753 [256/118836 (0%)] Loss: 12161.991211\n",
      "Train Epoch: 4753 [33024/118836 (28%)] Loss: 12215.708984\n",
      "Train Epoch: 4753 [65792/118836 (55%)] Loss: 12192.508789\n",
      "Train Epoch: 4753 [98560/118836 (83%)] Loss: 12257.080078\n",
      "    epoch          : 4753\n",
      "    loss           : 12217.776348609386\n",
      "    val_loss       : 12216.860953663087\n",
      "    val_log_likelihood: -12138.774005667132\n",
      "    val_log_marginal: -12146.66750664068\n",
      "Train Epoch: 4754 [256/118836 (0%)] Loss: 12162.278320\n",
      "Train Epoch: 4754 [33024/118836 (28%)] Loss: 12221.199219\n",
      "Train Epoch: 4754 [65792/118836 (55%)] Loss: 12159.113281\n",
      "Train Epoch: 4754 [98560/118836 (83%)] Loss: 12238.364258\n",
      "    epoch          : 4754\n",
      "    loss           : 12216.058547223945\n",
      "    val_loss       : 12216.932301236104\n",
      "    val_log_likelihood: -12139.563241347447\n",
      "    val_log_marginal: -12147.45322432195\n",
      "Train Epoch: 4755 [256/118836 (0%)] Loss: 12207.476562\n",
      "Train Epoch: 4755 [33024/118836 (28%)] Loss: 12196.245117\n",
      "Train Epoch: 4755 [65792/118836 (55%)] Loss: 12239.590820\n",
      "Train Epoch: 4755 [98560/118836 (83%)] Loss: 12215.017578\n",
      "    epoch          : 4755\n",
      "    loss           : 12217.709924750568\n",
      "    val_loss       : 12219.618253293218\n",
      "    val_log_likelihood: -12139.88650599023\n",
      "    val_log_marginal: -12147.78351633402\n",
      "Train Epoch: 4756 [256/118836 (0%)] Loss: 12233.076172\n",
      "Train Epoch: 4756 [33024/118836 (28%)] Loss: 12224.125000\n",
      "Train Epoch: 4756 [65792/118836 (55%)] Loss: 12229.818359\n",
      "Train Epoch: 4756 [98560/118836 (83%)] Loss: 12256.517578\n",
      "    epoch          : 4756\n",
      "    loss           : 12216.278232106855\n",
      "    val_loss       : 12218.64495145775\n",
      "    val_log_likelihood: -12139.12061298077\n",
      "    val_log_marginal: -12147.01391563372\n",
      "Train Epoch: 4757 [256/118836 (0%)] Loss: 12230.519531\n",
      "Train Epoch: 4757 [33024/118836 (28%)] Loss: 12229.491211\n",
      "Train Epoch: 4757 [65792/118836 (55%)] Loss: 12278.449219\n",
      "Train Epoch: 4757 [98560/118836 (83%)] Loss: 12286.513672\n",
      "    epoch          : 4757\n",
      "    loss           : 12217.310902282361\n",
      "    val_loss       : 12218.376803533556\n",
      "    val_log_likelihood: -12141.048046228805\n",
      "    val_log_marginal: -12148.939969777528\n",
      "Train Epoch: 4758 [256/118836 (0%)] Loss: 12290.376953\n",
      "Train Epoch: 4758 [33024/118836 (28%)] Loss: 12180.460938\n",
      "Train Epoch: 4758 [65792/118836 (55%)] Loss: 12202.290039\n",
      "Train Epoch: 4758 [98560/118836 (83%)] Loss: 12174.026367\n",
      "    epoch          : 4758\n",
      "    loss           : 12219.104383303607\n",
      "    val_loss       : 12214.41135431492\n",
      "    val_log_likelihood: -12137.852494151934\n",
      "    val_log_marginal: -12145.739783705478\n",
      "Train Epoch: 4759 [256/118836 (0%)] Loss: 12226.546875\n",
      "Train Epoch: 4759 [33024/118836 (28%)] Loss: 12217.131836\n",
      "Train Epoch: 4759 [65792/118836 (55%)] Loss: 12177.935547\n",
      "Train Epoch: 4759 [98560/118836 (83%)] Loss: 12257.139648\n",
      "    epoch          : 4759\n",
      "    loss           : 12224.73875281095\n",
      "    val_loss       : 12220.366926145198\n",
      "    val_log_likelihood: -12139.531030132082\n",
      "    val_log_marginal: -12147.423387457571\n",
      "Train Epoch: 4760 [256/118836 (0%)] Loss: 12141.084961\n",
      "Train Epoch: 4760 [33024/118836 (28%)] Loss: 12251.466797\n",
      "Train Epoch: 4760 [65792/118836 (55%)] Loss: 12165.060547\n",
      "Train Epoch: 4760 [98560/118836 (83%)] Loss: 12258.613281\n",
      "    epoch          : 4760\n",
      "    loss           : 12215.981258885184\n",
      "    val_loss       : 12217.857263662243\n",
      "    val_log_likelihood: -12139.388154272643\n",
      "    val_log_marginal: -12147.284494181706\n",
      "Train Epoch: 4761 [256/118836 (0%)] Loss: 12246.912109\n",
      "Train Epoch: 4761 [33024/118836 (28%)] Loss: 12217.479492\n",
      "Train Epoch: 4761 [65792/118836 (55%)] Loss: 12225.430664\n",
      "Train Epoch: 4761 [98560/118836 (83%)] Loss: 12219.416992\n",
      "    epoch          : 4761\n",
      "    loss           : 12216.870441415942\n",
      "    val_loss       : 12219.310361753785\n",
      "    val_log_likelihood: -12140.196014914185\n",
      "    val_log_marginal: -12148.0898403554\n",
      "Train Epoch: 4762 [256/118836 (0%)] Loss: 12252.082031\n",
      "Train Epoch: 4762 [33024/118836 (28%)] Loss: 12323.849609\n",
      "Train Epoch: 4762 [65792/118836 (55%)] Loss: 12298.972656\n",
      "Train Epoch: 4762 [98560/118836 (83%)] Loss: 12272.821289\n",
      "    epoch          : 4762\n",
      "    loss           : 12211.843503961178\n",
      "    val_loss       : 12216.310969916667\n",
      "    val_log_likelihood: -12139.36155170854\n",
      "    val_log_marginal: -12147.249773459991\n",
      "Train Epoch: 4763 [256/118836 (0%)] Loss: 12230.518555\n",
      "Train Epoch: 4763 [33024/118836 (28%)] Loss: 12223.558594\n",
      "Train Epoch: 4763 [65792/118836 (55%)] Loss: 12291.181641\n",
      "Train Epoch: 4763 [98560/118836 (83%)] Loss: 12230.989258\n",
      "    epoch          : 4763\n",
      "    loss           : 12215.949209864817\n",
      "    val_loss       : 12220.782936539497\n",
      "    val_log_likelihood: -12138.884721683728\n",
      "    val_log_marginal: -12146.776578956049\n",
      "Train Epoch: 4764 [256/118836 (0%)] Loss: 12172.405273\n",
      "Train Epoch: 4764 [33024/118836 (28%)] Loss: 12206.822266\n",
      "Train Epoch: 4764 [65792/118836 (55%)] Loss: 12222.830078\n",
      "Train Epoch: 4764 [98560/118836 (83%)] Loss: 12246.009766\n",
      "    epoch          : 4764\n",
      "    loss           : 12219.497461422146\n",
      "    val_loss       : 12216.536753701144\n",
      "    val_log_likelihood: -12139.75507828655\n",
      "    val_log_marginal: -12147.645063565173\n",
      "Train Epoch: 4765 [256/118836 (0%)] Loss: 12279.435547\n",
      "Train Epoch: 4765 [33024/118836 (28%)] Loss: 12243.883789\n",
      "Train Epoch: 4765 [65792/118836 (55%)] Loss: 12160.787109\n",
      "Train Epoch: 4765 [98560/118836 (83%)] Loss: 12217.659180\n",
      "    epoch          : 4765\n",
      "    loss           : 12218.368116244055\n",
      "    val_loss       : 12220.933619209345\n",
      "    val_log_likelihood: -12139.765115475082\n",
      "    val_log_marginal: -12147.659528255934\n",
      "Train Epoch: 4766 [256/118836 (0%)] Loss: 12185.477539\n",
      "Train Epoch: 4766 [33024/118836 (28%)] Loss: 12250.884766\n",
      "Train Epoch: 4766 [65792/118836 (55%)] Loss: 12178.568359\n",
      "Train Epoch: 4766 [98560/118836 (83%)] Loss: 12387.712891\n",
      "    epoch          : 4766\n",
      "    loss           : 12218.008909093258\n",
      "    val_loss       : 12218.818208060382\n",
      "    val_log_likelihood: -12140.437308564671\n",
      "    val_log_marginal: -12148.326818191234\n",
      "Train Epoch: 4767 [256/118836 (0%)] Loss: 12277.972656\n",
      "Train Epoch: 4767 [33024/118836 (28%)] Loss: 12299.866211\n",
      "Train Epoch: 4767 [65792/118836 (55%)] Loss: 12335.867188\n",
      "Train Epoch: 4767 [98560/118836 (83%)] Loss: 12131.198242\n",
      "    epoch          : 4767\n",
      "    loss           : 12216.25478701406\n",
      "    val_loss       : 12219.065805324693\n",
      "    val_log_likelihood: -12138.861668346774\n",
      "    val_log_marginal: -12146.747366949958\n",
      "Train Epoch: 4768 [256/118836 (0%)] Loss: 12184.768555\n",
      "Train Epoch: 4768 [33024/118836 (28%)] Loss: 12273.546875\n",
      "Train Epoch: 4768 [65792/118836 (55%)] Loss: 12187.857422\n",
      "Train Epoch: 4768 [98560/118836 (83%)] Loss: 12151.628906\n",
      "    epoch          : 4768\n",
      "    loss           : 12214.600606615748\n",
      "    val_loss       : 12218.675272533223\n",
      "    val_log_likelihood: -12140.229366987178\n",
      "    val_log_marginal: -12148.115069407539\n",
      "Train Epoch: 4769 [256/118836 (0%)] Loss: 12268.941406\n",
      "Train Epoch: 4769 [33024/118836 (28%)] Loss: 12239.829102\n",
      "Train Epoch: 4769 [65792/118836 (55%)] Loss: 12193.484375\n",
      "Train Epoch: 4769 [98560/118836 (83%)] Loss: 12270.388672\n",
      "    epoch          : 4769\n",
      "    loss           : 12218.342332086177\n",
      "    val_loss       : 12217.759529464609\n",
      "    val_log_likelihood: -12138.912054771505\n",
      "    val_log_marginal: -12146.804322281392\n",
      "Train Epoch: 4770 [256/118836 (0%)] Loss: 12290.861328\n",
      "Train Epoch: 4770 [33024/118836 (28%)] Loss: 12258.429688\n",
      "Train Epoch: 4770 [65792/118836 (55%)] Loss: 12236.384766\n",
      "Train Epoch: 4770 [98560/118836 (83%)] Loss: 12257.357422\n",
      "    epoch          : 4770\n",
      "    loss           : 12217.025097898573\n",
      "    val_loss       : 12213.025798772109\n",
      "    val_log_likelihood: -12140.228781049678\n",
      "    val_log_marginal: -12148.117761858952\n",
      "Train Epoch: 4771 [256/118836 (0%)] Loss: 12235.209961\n",
      "Train Epoch: 4771 [33024/118836 (28%)] Loss: 12218.537109\n",
      "Train Epoch: 4771 [65792/118836 (55%)] Loss: 12307.411133\n",
      "Train Epoch: 4771 [98560/118836 (83%)] Loss: 12275.409180\n",
      "    epoch          : 4771\n",
      "    loss           : 12219.247766910928\n",
      "    val_loss       : 12214.781561046355\n",
      "    val_log_likelihood: -12139.329443884408\n",
      "    val_log_marginal: -12147.216219953754\n",
      "Train Epoch: 4772 [256/118836 (0%)] Loss: 12241.533203\n",
      "Train Epoch: 4772 [33024/118836 (28%)] Loss: 12427.918945\n",
      "Train Epoch: 4772 [65792/118836 (55%)] Loss: 12233.033203\n",
      "Train Epoch: 4772 [98560/118836 (83%)] Loss: 12212.281250\n",
      "    epoch          : 4772\n",
      "    loss           : 12211.10468329973\n",
      "    val_loss       : 12221.018744162755\n",
      "    val_log_likelihood: -12138.50089045699\n",
      "    val_log_marginal: -12146.391011470247\n",
      "Train Epoch: 4773 [256/118836 (0%)] Loss: 12244.909180\n",
      "Train Epoch: 4773 [33024/118836 (28%)] Loss: 12248.672852\n",
      "Train Epoch: 4773 [65792/118836 (55%)] Loss: 12185.307617\n",
      "Train Epoch: 4773 [98560/118836 (83%)] Loss: 12181.949219\n",
      "    epoch          : 4773\n",
      "    loss           : 12217.62808041253\n",
      "    val_loss       : 12217.519074613594\n",
      "    val_log_likelihood: -12139.684112644749\n",
      "    val_log_marginal: -12147.5707503429\n",
      "Train Epoch: 4774 [256/118836 (0%)] Loss: 12218.636719\n",
      "Train Epoch: 4774 [33024/118836 (28%)] Loss: 12269.046875\n",
      "Train Epoch: 4774 [65792/118836 (55%)] Loss: 12215.795898\n",
      "Train Epoch: 4774 [98560/118836 (83%)] Loss: 12240.234375\n",
      "    epoch          : 4774\n",
      "    loss           : 12221.990511592741\n",
      "    val_loss       : 12220.7334697557\n",
      "    val_log_likelihood: -12137.703728707867\n",
      "    val_log_marginal: -12145.591602231416\n",
      "Train Epoch: 4775 [256/118836 (0%)] Loss: 12207.494141\n",
      "Train Epoch: 4775 [33024/118836 (28%)] Loss: 12257.825195\n",
      "Train Epoch: 4775 [65792/118836 (55%)] Loss: 12199.664062\n",
      "Train Epoch: 4775 [98560/118836 (83%)] Loss: 12282.660156\n",
      "    epoch          : 4775\n",
      "    loss           : 12212.985650427781\n",
      "    val_loss       : 12216.851799956004\n",
      "    val_log_likelihood: -12139.858030590882\n",
      "    val_log_marginal: -12147.746685200256\n",
      "Train Epoch: 4776 [256/118836 (0%)] Loss: 12166.721680\n",
      "Train Epoch: 4776 [33024/118836 (28%)] Loss: 12274.181641\n",
      "Train Epoch: 4776 [65792/118836 (55%)] Loss: 12208.105469\n",
      "Train Epoch: 4776 [98560/118836 (83%)] Loss: 12277.545898\n",
      "    epoch          : 4776\n",
      "    loss           : 12213.248832002171\n",
      "    val_loss       : 12215.892194491891\n",
      "    val_log_likelihood: -12140.32853727254\n",
      "    val_log_marginal: -12148.215956643227\n",
      "Train Epoch: 4777 [256/118836 (0%)] Loss: 12283.452148\n",
      "Train Epoch: 4777 [33024/118836 (28%)] Loss: 12241.416016\n",
      "Train Epoch: 4777 [65792/118836 (55%)] Loss: 12297.334961\n",
      "Train Epoch: 4777 [98560/118836 (83%)] Loss: 12251.873047\n",
      "    epoch          : 4777\n",
      "    loss           : 12216.758973551232\n",
      "    val_loss       : 12221.790012390802\n",
      "    val_log_likelihood: -12138.98015082196\n",
      "    val_log_marginal: -12146.87214870008\n",
      "Train Epoch: 4778 [256/118836 (0%)] Loss: 12163.814453\n",
      "Train Epoch: 4778 [33024/118836 (28%)] Loss: 12137.109375\n",
      "Train Epoch: 4778 [65792/118836 (55%)] Loss: 12180.391602\n",
      "Train Epoch: 4778 [98560/118836 (83%)] Loss: 12353.031250\n",
      "    epoch          : 4778\n",
      "    loss           : 12219.80391836616\n",
      "    val_loss       : 12217.417767203806\n",
      "    val_log_likelihood: -12141.44406792804\n",
      "    val_log_marginal: -12149.334699850195\n",
      "Train Epoch: 4779 [256/118836 (0%)] Loss: 12150.970703\n",
      "Train Epoch: 4779 [33024/118836 (28%)] Loss: 12259.487305\n",
      "Train Epoch: 4779 [65792/118836 (55%)] Loss: 12190.949219\n",
      "Train Epoch: 4779 [98560/118836 (83%)] Loss: 12245.048828\n",
      "    epoch          : 4779\n",
      "    loss           : 12213.306378592844\n",
      "    val_loss       : 12218.752573948992\n",
      "    val_log_likelihood: -12139.662851045543\n",
      "    val_log_marginal: -12147.558383906004\n",
      "Train Epoch: 4780 [256/118836 (0%)] Loss: 12216.933594\n",
      "Train Epoch: 4780 [33024/118836 (28%)] Loss: 12172.273438\n",
      "Train Epoch: 4780 [65792/118836 (55%)] Loss: 12266.266602\n",
      "Train Epoch: 4780 [98560/118836 (83%)] Loss: 12275.579102\n",
      "    epoch          : 4780\n",
      "    loss           : 12218.623146873708\n",
      "    val_loss       : 12216.66991288787\n",
      "    val_log_likelihood: -12140.611480303969\n",
      "    val_log_marginal: -12148.498692434585\n",
      "Train Epoch: 4781 [256/118836 (0%)] Loss: 12159.253906\n",
      "Train Epoch: 4781 [33024/118836 (28%)] Loss: 12245.728516\n",
      "Train Epoch: 4781 [65792/118836 (55%)] Loss: 12269.500000\n",
      "Train Epoch: 4781 [98560/118836 (83%)] Loss: 12240.038086\n",
      "    epoch          : 4781\n",
      "    loss           : 12213.689514028898\n",
      "    val_loss       : 12216.119602477951\n",
      "    val_log_likelihood: -12138.622072412634\n",
      "    val_log_marginal: -12146.512996398606\n",
      "Train Epoch: 4782 [256/118836 (0%)] Loss: 12232.717773\n",
      "Train Epoch: 4782 [33024/118836 (28%)] Loss: 12252.995117\n",
      "Train Epoch: 4782 [65792/118836 (55%)] Loss: 12185.416992\n",
      "Train Epoch: 4782 [98560/118836 (83%)] Loss: 12225.535156\n",
      "    epoch          : 4782\n",
      "    loss           : 12220.409688404674\n",
      "    val_loss       : 12218.071409214741\n",
      "    val_log_likelihood: -12139.842405429332\n",
      "    val_log_marginal: -12147.733197232277\n",
      "Train Epoch: 4783 [256/118836 (0%)] Loss: 12319.152344\n",
      "Train Epoch: 4783 [33024/118836 (28%)] Loss: 12230.106445\n",
      "Train Epoch: 4783 [65792/118836 (55%)] Loss: 12262.915039\n",
      "Train Epoch: 4783 [98560/118836 (83%)] Loss: 12342.145508\n",
      "    epoch          : 4783\n",
      "    loss           : 12217.610357539807\n",
      "    val_loss       : 12219.4189643967\n",
      "    val_log_likelihood: -12138.978476368642\n",
      "    val_log_marginal: -12146.866832927142\n",
      "Train Epoch: 4784 [256/118836 (0%)] Loss: 12240.811523\n",
      "Train Epoch: 4784 [33024/118836 (28%)] Loss: 12135.644531\n",
      "Train Epoch: 4784 [65792/118836 (55%)] Loss: 12227.244141\n",
      "Train Epoch: 4784 [98560/118836 (83%)] Loss: 12238.924805\n",
      "    epoch          : 4784\n",
      "    loss           : 12220.083927186724\n",
      "    val_loss       : 12216.799335541975\n",
      "    val_log_likelihood: -12139.250523741212\n",
      "    val_log_marginal: -12147.141539587807\n",
      "Train Epoch: 4785 [256/118836 (0%)] Loss: 12177.228516\n",
      "Train Epoch: 4785 [33024/118836 (28%)] Loss: 12260.993164\n",
      "Train Epoch: 4785 [65792/118836 (55%)] Loss: 12233.759766\n",
      "Train Epoch: 4785 [98560/118836 (83%)] Loss: 12197.919922\n",
      "    epoch          : 4785\n",
      "    loss           : 12219.492548400021\n",
      "    val_loss       : 12216.903052368332\n",
      "    val_log_likelihood: -12139.301914837934\n",
      "    val_log_marginal: -12147.203083685303\n",
      "Train Epoch: 4786 [256/118836 (0%)] Loss: 12204.378906\n",
      "Train Epoch: 4786 [33024/118836 (28%)] Loss: 12339.300781\n",
      "Train Epoch: 4786 [65792/118836 (55%)] Loss: 12304.980469\n",
      "Train Epoch: 4786 [98560/118836 (83%)] Loss: 12303.671875\n",
      "    epoch          : 4786\n",
      "    loss           : 12217.063493686672\n",
      "    val_loss       : 12216.37053007983\n",
      "    val_log_likelihood: -12139.31121100212\n",
      "    val_log_marginal: -12147.203412824987\n",
      "Train Epoch: 4787 [256/118836 (0%)] Loss: 12240.535156\n",
      "Train Epoch: 4787 [33024/118836 (28%)] Loss: 12260.541016\n",
      "Train Epoch: 4787 [65792/118836 (55%)] Loss: 12199.268555\n",
      "Train Epoch: 4787 [98560/118836 (83%)] Loss: 12298.727539\n",
      "    epoch          : 4787\n",
      "    loss           : 12214.734033324286\n",
      "    val_loss       : 12218.712717017253\n",
      "    val_log_likelihood: -12139.840202065241\n",
      "    val_log_marginal: -12147.728697963234\n",
      "Train Epoch: 4788 [256/118836 (0%)] Loss: 12282.634766\n",
      "Train Epoch: 4788 [33024/118836 (28%)] Loss: 12225.000000\n",
      "Train Epoch: 4788 [65792/118836 (55%)] Loss: 12303.627930\n",
      "Train Epoch: 4788 [98560/118836 (83%)] Loss: 12323.329102\n",
      "    epoch          : 4788\n",
      "    loss           : 12217.278330974721\n",
      "    val_loss       : 12216.489773668307\n",
      "    val_log_likelihood: -12141.003129684916\n",
      "    val_log_marginal: -12148.894736875754\n",
      "Train Epoch: 4789 [256/118836 (0%)] Loss: 12206.774414\n",
      "Train Epoch: 4789 [33024/118836 (28%)] Loss: 12244.242188\n",
      "Train Epoch: 4789 [65792/118836 (55%)] Loss: 12278.359375\n",
      "Train Epoch: 4789 [98560/118836 (83%)] Loss: 12192.285156\n",
      "    epoch          : 4789\n",
      "    loss           : 12216.312066079921\n",
      "    val_loss       : 12218.765774774074\n",
      "    val_log_likelihood: -12141.596047547044\n",
      "    val_log_marginal: -12149.484346829075\n",
      "Train Epoch: 4790 [256/118836 (0%)] Loss: 12172.199219\n",
      "Train Epoch: 4790 [33024/118836 (28%)] Loss: 12242.873047\n",
      "Train Epoch: 4790 [65792/118836 (55%)] Loss: 12280.468750\n",
      "Train Epoch: 4790 [98560/118836 (83%)] Loss: 12176.012695\n",
      "    epoch          : 4790\n",
      "    loss           : 12215.454322884356\n",
      "    val_loss       : 12214.716556365776\n",
      "    val_log_likelihood: -12140.280329494934\n",
      "    val_log_marginal: -12148.168946997164\n",
      "Train Epoch: 4791 [256/118836 (0%)] Loss: 12406.666992\n",
      "Train Epoch: 4791 [33024/118836 (28%)] Loss: 12132.572266\n",
      "Train Epoch: 4791 [65792/118836 (55%)] Loss: 12195.535156\n",
      "Train Epoch: 4791 [98560/118836 (83%)] Loss: 12296.077148\n",
      "    epoch          : 4791\n",
      "    loss           : 12216.555843704768\n",
      "    val_loss       : 12219.001589683401\n",
      "    val_log_likelihood: -12141.183808933001\n",
      "    val_log_marginal: -12149.074160832388\n",
      "Train Epoch: 4792 [256/118836 (0%)] Loss: 12341.378906\n",
      "Train Epoch: 4792 [33024/118836 (28%)] Loss: 12336.483398\n",
      "Train Epoch: 4792 [65792/118836 (55%)] Loss: 12227.848633\n",
      "Train Epoch: 4792 [98560/118836 (83%)] Loss: 12171.771484\n",
      "    epoch          : 4792\n",
      "    loss           : 12219.341320952233\n",
      "    val_loss       : 12223.312587380235\n",
      "    val_log_likelihood: -12139.422657380843\n",
      "    val_log_marginal: -12147.307706049223\n",
      "Train Epoch: 4793 [256/118836 (0%)] Loss: 12248.259766\n",
      "Train Epoch: 4793 [33024/118836 (28%)] Loss: 12161.515625\n",
      "Train Epoch: 4793 [65792/118836 (55%)] Loss: 12188.859375\n",
      "Train Epoch: 4793 [98560/118836 (83%)] Loss: 12261.242188\n",
      "    epoch          : 4793\n",
      "    loss           : 12214.937757670337\n",
      "    val_loss       : 12221.796596139868\n",
      "    val_log_likelihood: -12140.692056322374\n",
      "    val_log_marginal: -12148.58211630028\n",
      "Train Epoch: 4794 [256/118836 (0%)] Loss: 12350.184570\n",
      "Train Epoch: 4794 [33024/118836 (28%)] Loss: 12209.453125\n",
      "Train Epoch: 4794 [65792/118836 (55%)] Loss: 12190.243164\n",
      "Train Epoch: 4794 [98560/118836 (83%)] Loss: 12333.536133\n",
      "    epoch          : 4794\n",
      "    loss           : 12217.72141297043\n",
      "    val_loss       : 12213.695531495985\n",
      "    val_log_likelihood: -12139.145213955233\n",
      "    val_log_marginal: -12147.035164847477\n",
      "Train Epoch: 4795 [256/118836 (0%)] Loss: 12228.511719\n",
      "Train Epoch: 4795 [33024/118836 (28%)] Loss: 12311.297852\n",
      "Train Epoch: 4795 [65792/118836 (55%)] Loss: 12169.431641\n",
      "Train Epoch: 4795 [98560/118836 (83%)] Loss: 12302.706055\n",
      "    epoch          : 4795\n",
      "    loss           : 12221.466408027038\n",
      "    val_loss       : 12213.469135428539\n",
      "    val_log_likelihood: -12141.514027282361\n",
      "    val_log_marginal: -12149.408571338681\n",
      "Train Epoch: 4796 [256/118836 (0%)] Loss: 12274.051758\n",
      "Train Epoch: 4796 [33024/118836 (28%)] Loss: 12325.054688\n",
      "Train Epoch: 4796 [65792/118836 (55%)] Loss: 12275.244141\n",
      "Train Epoch: 4796 [98560/118836 (83%)] Loss: 12246.504883\n",
      "    epoch          : 4796\n",
      "    loss           : 12219.776287867038\n",
      "    val_loss       : 12214.996834170734\n",
      "    val_log_likelihood: -12139.59976446185\n",
      "    val_log_marginal: -12147.49422817544\n",
      "Train Epoch: 4797 [256/118836 (0%)] Loss: 12227.745117\n",
      "Train Epoch: 4797 [33024/118836 (28%)] Loss: 12183.916992\n",
      "Train Epoch: 4797 [65792/118836 (55%)] Loss: 12245.902344\n",
      "Train Epoch: 4797 [98560/118836 (83%)] Loss: 12272.869141\n",
      "    epoch          : 4797\n",
      "    loss           : 12217.137241683467\n",
      "    val_loss       : 12218.465755501444\n",
      "    val_log_likelihood: -12140.164994151934\n",
      "    val_log_marginal: -12148.056769736488\n",
      "Train Epoch: 4798 [256/118836 (0%)] Loss: 12302.167969\n",
      "Train Epoch: 4798 [33024/118836 (28%)] Loss: 12249.120117\n",
      "Train Epoch: 4798 [65792/118836 (55%)] Loss: 12321.166016\n",
      "Train Epoch: 4798 [98560/118836 (83%)] Loss: 12349.198242\n",
      "    epoch          : 4798\n",
      "    loss           : 12219.813920983252\n",
      "    val_loss       : 12219.61978548204\n",
      "    val_log_likelihood: -12139.495178576044\n",
      "    val_log_marginal: -12147.38099414673\n",
      "Train Epoch: 4799 [256/118836 (0%)] Loss: 12302.679688\n",
      "Train Epoch: 4799 [33024/118836 (28%)] Loss: 12144.547852\n",
      "Train Epoch: 4799 [65792/118836 (55%)] Loss: 12217.374023\n",
      "Train Epoch: 4799 [98560/118836 (83%)] Loss: 12162.113281\n",
      "    epoch          : 4799\n",
      "    loss           : 12219.89725883995\n",
      "    val_loss       : 12212.450685251666\n",
      "    val_log_likelihood: -12138.419569537065\n",
      "    val_log_marginal: -12146.311309539646\n",
      "Train Epoch: 4800 [256/118836 (0%)] Loss: 12280.464844\n",
      "Train Epoch: 4800 [33024/118836 (28%)] Loss: 12328.221680\n",
      "Train Epoch: 4800 [65792/118836 (55%)] Loss: 12084.014648\n",
      "Train Epoch: 4800 [98560/118836 (83%)] Loss: 12182.097656\n",
      "    epoch          : 4800\n",
      "    loss           : 12218.549307601841\n",
      "    val_loss       : 12223.149831861549\n",
      "    val_log_likelihood: -12139.702751660721\n",
      "    val_log_marginal: -12147.594909781157\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch4800.pth ...\n",
      "Train Epoch: 4801 [256/118836 (0%)] Loss: 12172.817383\n",
      "Train Epoch: 4801 [33024/118836 (28%)] Loss: 12294.789062\n",
      "Train Epoch: 4801 [65792/118836 (55%)] Loss: 12153.351562\n",
      "Train Epoch: 4801 [98560/118836 (83%)] Loss: 12249.439453\n",
      "    epoch          : 4801\n",
      "    loss           : 12216.770644321237\n",
      "    val_loss       : 12221.076797314643\n",
      "    val_log_likelihood: -12138.100875432952\n",
      "    val_log_marginal: -12145.996794461464\n",
      "Train Epoch: 4802 [256/118836 (0%)] Loss: 12213.438477\n",
      "Train Epoch: 4802 [33024/118836 (28%)] Loss: 12279.021484\n",
      "Train Epoch: 4802 [65792/118836 (55%)] Loss: 12263.800781\n",
      "Train Epoch: 4802 [98560/118836 (83%)] Loss: 12229.983398\n",
      "    epoch          : 4802\n",
      "    loss           : 12213.424644915736\n",
      "    val_loss       : 12217.10878414656\n",
      "    val_log_likelihood: -12139.69393045001\n",
      "    val_log_marginal: -12147.587304935512\n",
      "Train Epoch: 4803 [256/118836 (0%)] Loss: 12260.921875\n",
      "Train Epoch: 4803 [33024/118836 (28%)] Loss: 12188.239258\n",
      "Train Epoch: 4803 [65792/118836 (55%)] Loss: 12181.398438\n",
      "Train Epoch: 4803 [98560/118836 (83%)] Loss: 12308.286133\n",
      "    epoch          : 4803\n",
      "    loss           : 12217.371593097343\n",
      "    val_loss       : 12214.963467135145\n",
      "    val_log_likelihood: -12138.594472123139\n",
      "    val_log_marginal: -12146.48085425021\n",
      "Train Epoch: 4804 [256/118836 (0%)] Loss: 12268.130859\n",
      "Train Epoch: 4804 [33024/118836 (28%)] Loss: 12102.678711\n",
      "Train Epoch: 4804 [65792/118836 (55%)] Loss: 12311.436523\n",
      "Train Epoch: 4804 [98560/118836 (83%)] Loss: 12296.503906\n",
      "    epoch          : 4804\n",
      "    loss           : 12220.534088735525\n",
      "    val_loss       : 12219.265995628728\n",
      "    val_log_likelihood: -12140.551423083385\n",
      "    val_log_marginal: -12148.444151615115\n",
      "Train Epoch: 4805 [256/118836 (0%)] Loss: 12259.927734\n",
      "Train Epoch: 4805 [33024/118836 (28%)] Loss: 12226.145508\n",
      "Train Epoch: 4805 [65792/118836 (55%)] Loss: 12269.913086\n",
      "Train Epoch: 4805 [98560/118836 (83%)] Loss: 12255.337891\n",
      "    epoch          : 4805\n",
      "    loss           : 12213.964306115591\n",
      "    val_loss       : 12217.802272426195\n",
      "    val_log_likelihood: -12139.181901526314\n",
      "    val_log_marginal: -12147.068031804441\n",
      "Train Epoch: 4806 [256/118836 (0%)] Loss: 12201.523438\n",
      "Train Epoch: 4806 [33024/118836 (28%)] Loss: 12155.140625\n",
      "Train Epoch: 4806 [65792/118836 (55%)] Loss: 12168.167969\n",
      "Train Epoch: 4806 [98560/118836 (83%)] Loss: 12163.079102\n",
      "    epoch          : 4806\n",
      "    loss           : 12216.697044464692\n",
      "    val_loss       : 12217.888289054958\n",
      "    val_log_likelihood: -12141.070151597394\n",
      "    val_log_marginal: -12148.959890753498\n",
      "Train Epoch: 4807 [256/118836 (0%)] Loss: 12279.449219\n",
      "Train Epoch: 4807 [33024/118836 (28%)] Loss: 12281.714844\n",
      "Train Epoch: 4807 [65792/118836 (55%)] Loss: 12311.559570\n",
      "Train Epoch: 4807 [98560/118836 (83%)] Loss: 12235.018555\n",
      "    epoch          : 4807\n",
      "    loss           : 12215.879359232837\n",
      "    val_loss       : 12218.441886022038\n",
      "    val_log_likelihood: -12139.817018519954\n",
      "    val_log_marginal: -12147.705767617006\n",
      "Train Epoch: 4808 [256/118836 (0%)] Loss: 12231.282227\n",
      "Train Epoch: 4808 [33024/118836 (28%)] Loss: 12188.375000\n",
      "Train Epoch: 4808 [65792/118836 (55%)] Loss: 12209.675781\n",
      "Train Epoch: 4808 [98560/118836 (83%)] Loss: 12308.581055\n",
      "    epoch          : 4808\n",
      "    loss           : 12216.626063798853\n",
      "    val_loss       : 12216.92228004367\n",
      "    val_log_likelihood: -12139.717005434502\n",
      "    val_log_marginal: -12147.608557549\n",
      "Train Epoch: 4809 [256/118836 (0%)] Loss: 12113.203125\n",
      "Train Epoch: 4809 [33024/118836 (28%)] Loss: 12240.600586\n",
      "Train Epoch: 4809 [65792/118836 (55%)] Loss: 12157.197266\n",
      "Train Epoch: 4809 [98560/118836 (83%)] Loss: 12246.148438\n",
      "    epoch          : 4809\n",
      "    loss           : 12213.451861365282\n",
      "    val_loss       : 12221.733689491526\n",
      "    val_log_likelihood: -12138.917791530966\n",
      "    val_log_marginal: -12146.813314606443\n",
      "Train Epoch: 4810 [256/118836 (0%)] Loss: 12241.697266\n",
      "Train Epoch: 4810 [33024/118836 (28%)] Loss: 12290.171875\n",
      "Train Epoch: 4810 [65792/118836 (55%)] Loss: 12211.978516\n",
      "Train Epoch: 4810 [98560/118836 (83%)] Loss: 12205.602539\n",
      "    epoch          : 4810\n",
      "    loss           : 12219.132138195306\n",
      "    val_loss       : 12219.696709429203\n",
      "    val_log_likelihood: -12139.20217541615\n",
      "    val_log_marginal: -12147.088890165407\n",
      "Train Epoch: 4811 [256/118836 (0%)] Loss: 12317.638672\n",
      "Train Epoch: 4811 [33024/118836 (28%)] Loss: 12157.988281\n",
      "Train Epoch: 4811 [65792/118836 (55%)] Loss: 12197.371094\n",
      "Train Epoch: 4811 [98560/118836 (83%)] Loss: 12268.051758\n",
      "    epoch          : 4811\n",
      "    loss           : 12221.172152702387\n",
      "    val_loss       : 12218.169755970379\n",
      "    val_log_likelihood: -12138.455935626034\n",
      "    val_log_marginal: -12146.346816151543\n",
      "Train Epoch: 4812 [256/118836 (0%)] Loss: 12267.871094\n",
      "Train Epoch: 4812 [33024/118836 (28%)] Loss: 12336.435547\n",
      "Train Epoch: 4812 [65792/118836 (55%)] Loss: 12354.238281\n",
      "Train Epoch: 4812 [98560/118836 (83%)] Loss: 12213.925781\n",
      "    epoch          : 4812\n",
      "    loss           : 12215.887510823768\n",
      "    val_loss       : 12218.365533149106\n",
      "    val_log_likelihood: -12140.934684689051\n",
      "    val_log_marginal: -12148.824111564345\n",
      "Train Epoch: 4813 [256/118836 (0%)] Loss: 12206.256836\n",
      "Train Epoch: 4813 [33024/118836 (28%)] Loss: 12302.871094\n",
      "Train Epoch: 4813 [65792/118836 (55%)] Loss: 12212.537109\n",
      "Train Epoch: 4813 [98560/118836 (83%)] Loss: 12232.950195\n",
      "    epoch          : 4813\n",
      "    loss           : 12215.082497964486\n",
      "    val_loss       : 12215.39831644498\n",
      "    val_log_likelihood: -12140.22674311156\n",
      "    val_log_marginal: -12148.116828241722\n",
      "Train Epoch: 4814 [256/118836 (0%)] Loss: 12447.934570\n",
      "Train Epoch: 4814 [33024/118836 (28%)] Loss: 12310.142578\n",
      "Train Epoch: 4814 [65792/118836 (55%)] Loss: 12219.966797\n",
      "Train Epoch: 4814 [98560/118836 (83%)] Loss: 12158.744141\n",
      "    epoch          : 4814\n",
      "    loss           : 12210.969753864247\n",
      "    val_loss       : 12219.618154447011\n",
      "    val_log_likelihood: -12139.66143183933\n",
      "    val_log_marginal: -12147.546724302925\n",
      "Train Epoch: 4815 [256/118836 (0%)] Loss: 12397.648438\n",
      "Train Epoch: 4815 [33024/118836 (28%)] Loss: 12317.642578\n",
      "Train Epoch: 4815 [65792/118836 (55%)] Loss: 12241.992188\n",
      "Train Epoch: 4815 [98560/118836 (83%)] Loss: 12242.337891\n",
      "    epoch          : 4815\n",
      "    loss           : 12219.324785301644\n",
      "    val_loss       : 12217.925568927518\n",
      "    val_log_likelihood: -12138.867097678867\n",
      "    val_log_marginal: -12146.762523299545\n",
      "Train Epoch: 4816 [256/118836 (0%)] Loss: 12303.088867\n",
      "Train Epoch: 4816 [33024/118836 (28%)] Loss: 12275.724609\n",
      "Train Epoch: 4816 [65792/118836 (55%)] Loss: 12212.804688\n",
      "Train Epoch: 4816 [98560/118836 (83%)] Loss: 12240.748047\n",
      "    epoch          : 4816\n",
      "    loss           : 12220.483933487129\n",
      "    val_loss       : 12218.57741166562\n",
      "    val_log_likelihood: -12139.374462365591\n",
      "    val_log_marginal: -12147.26611851423\n",
      "Train Epoch: 4817 [256/118836 (0%)] Loss: 12260.572266\n",
      "Train Epoch: 4817 [33024/118836 (28%)] Loss: 12326.727539\n",
      "Train Epoch: 4817 [65792/118836 (55%)] Loss: 12278.615234\n",
      "Train Epoch: 4817 [98560/118836 (83%)] Loss: 12222.761719\n",
      "    epoch          : 4817\n",
      "    loss           : 12221.938478501084\n",
      "    val_loss       : 12216.539508503543\n",
      "    val_log_likelihood: -12139.781972930883\n",
      "    val_log_marginal: -12147.672597988845\n",
      "Train Epoch: 4818 [256/118836 (0%)] Loss: 12436.448242\n",
      "Train Epoch: 4818 [33024/118836 (28%)] Loss: 12266.917969\n",
      "Train Epoch: 4818 [65792/118836 (55%)] Loss: 12215.844727\n",
      "Train Epoch: 4818 [98560/118836 (83%)] Loss: 12160.624023\n",
      "    epoch          : 4818\n",
      "    loss           : 12216.358569840779\n",
      "    val_loss       : 12215.509601072054\n",
      "    val_log_likelihood: -12139.457594732217\n",
      "    val_log_marginal: -12147.352673570995\n",
      "Train Epoch: 4819 [256/118836 (0%)] Loss: 12172.107422\n",
      "Train Epoch: 4819 [33024/118836 (28%)] Loss: 12204.416016\n",
      "Train Epoch: 4819 [65792/118836 (55%)] Loss: 12306.538086\n",
      "Train Epoch: 4819 [98560/118836 (83%)] Loss: 12181.316406\n",
      "    epoch          : 4819\n",
      "    loss           : 12218.55707034481\n",
      "    val_loss       : 12221.423435589246\n",
      "    val_log_likelihood: -12140.684367730304\n",
      "    val_log_marginal: -12148.581658622767\n",
      "Train Epoch: 4820 [256/118836 (0%)] Loss: 12341.225586\n",
      "Train Epoch: 4820 [33024/118836 (28%)] Loss: 12338.253906\n",
      "Train Epoch: 4820 [65792/118836 (55%)] Loss: 12133.485352\n",
      "Train Epoch: 4820 [98560/118836 (83%)] Loss: 12337.900391\n",
      "    epoch          : 4820\n",
      "    loss           : 12220.153703344706\n",
      "    val_loss       : 12218.681247917151\n",
      "    val_log_likelihood: -12138.936701948925\n",
      "    val_log_marginal: -12146.83140261909\n",
      "Train Epoch: 4821 [256/118836 (0%)] Loss: 12200.465820\n",
      "Train Epoch: 4821 [33024/118836 (28%)] Loss: 12271.864258\n",
      "Train Epoch: 4821 [65792/118836 (55%)] Loss: 12191.246094\n",
      "Train Epoch: 4821 [98560/118836 (83%)] Loss: 12245.627930\n",
      "    epoch          : 4821\n",
      "    loss           : 12220.537418094758\n",
      "    val_loss       : 12217.546975997571\n",
      "    val_log_likelihood: -12140.68037844422\n",
      "    val_log_marginal: -12148.57039574648\n",
      "Train Epoch: 4822 [256/118836 (0%)] Loss: 12224.847656\n",
      "Train Epoch: 4822 [33024/118836 (28%)] Loss: 12194.324219\n",
      "Train Epoch: 4822 [65792/118836 (55%)] Loss: 12272.111328\n",
      "Train Epoch: 4822 [98560/118836 (83%)] Loss: 12236.492188\n",
      "    epoch          : 4822\n",
      "    loss           : 12218.90594822684\n",
      "    val_loss       : 12219.916743780908\n",
      "    val_log_likelihood: -12141.12332845456\n",
      "    val_log_marginal: -12149.018854938964\n",
      "Train Epoch: 4823 [256/118836 (0%)] Loss: 12223.536133\n",
      "Train Epoch: 4823 [33024/118836 (28%)] Loss: 12373.106445\n",
      "Train Epoch: 4823 [65792/118836 (55%)] Loss: 12401.095703\n",
      "Train Epoch: 4823 [98560/118836 (83%)] Loss: 12181.202148\n",
      "    epoch          : 4823\n",
      "    loss           : 12215.75844157103\n",
      "    val_loss       : 12219.188600071822\n",
      "    val_log_likelihood: -12138.848442508011\n",
      "    val_log_marginal: -12146.738948778717\n",
      "Train Epoch: 4824 [256/118836 (0%)] Loss: 12260.763672\n",
      "Train Epoch: 4824 [33024/118836 (28%)] Loss: 12198.359375\n",
      "Train Epoch: 4824 [65792/118836 (55%)] Loss: 12265.183594\n",
      "Train Epoch: 4824 [98560/118836 (83%)] Loss: 12233.164062\n",
      "    epoch          : 4824\n",
      "    loss           : 12218.7642174253\n",
      "    val_loss       : 12220.224690619538\n",
      "    val_log_likelihood: -12139.98252155061\n",
      "    val_log_marginal: -12147.870689976347\n",
      "Train Epoch: 4825 [256/118836 (0%)] Loss: 12209.893555\n",
      "Train Epoch: 4825 [33024/118836 (28%)] Loss: 12242.449219\n",
      "Train Epoch: 4825 [65792/118836 (55%)] Loss: 12365.851562\n",
      "Train Epoch: 4825 [98560/118836 (83%)] Loss: 12261.134766\n",
      "    epoch          : 4825\n",
      "    loss           : 12218.594629794768\n",
      "    val_loss       : 12221.304332078786\n",
      "    val_log_likelihood: -12142.243381184087\n",
      "    val_log_marginal: -12150.132719397596\n",
      "Train Epoch: 4826 [256/118836 (0%)] Loss: 12182.123047\n",
      "Train Epoch: 4826 [33024/118836 (28%)] Loss: 12180.149414\n",
      "Train Epoch: 4826 [65792/118836 (55%)] Loss: 12199.771484\n",
      "Train Epoch: 4826 [98560/118836 (83%)] Loss: 12155.273438\n",
      "    epoch          : 4826\n",
      "    loss           : 12215.719410411497\n",
      "    val_loss       : 12221.583978591758\n",
      "    val_log_likelihood: -12141.17302538901\n",
      "    val_log_marginal: -12149.062531829448\n",
      "Train Epoch: 4827 [256/118836 (0%)] Loss: 12284.992188\n",
      "Train Epoch: 4827 [33024/118836 (28%)] Loss: 12221.445312\n",
      "Train Epoch: 4827 [65792/118836 (55%)] Loss: 12264.726562\n",
      "Train Epoch: 4827 [98560/118836 (83%)] Loss: 12246.046875\n",
      "    epoch          : 4827\n",
      "    loss           : 12218.795011857683\n",
      "    val_loss       : 12223.01307020853\n",
      "    val_log_likelihood: -12139.576666375879\n",
      "    val_log_marginal: -12147.472938311677\n",
      "Train Epoch: 4828 [256/118836 (0%)] Loss: 12166.629883\n",
      "Train Epoch: 4828 [33024/118836 (28%)] Loss: 12154.798828\n",
      "Train Epoch: 4828 [65792/118836 (55%)] Loss: 12221.599609\n",
      "Train Epoch: 4828 [98560/118836 (83%)] Loss: 12335.652344\n",
      "    epoch          : 4828\n",
      "    loss           : 12215.940843413979\n",
      "    val_loss       : 12217.816290108423\n",
      "    val_log_likelihood: -12139.989057491988\n",
      "    val_log_marginal: -12147.883232900027\n",
      "Train Epoch: 4829 [256/118836 (0%)] Loss: 12309.458984\n",
      "Train Epoch: 4829 [33024/118836 (28%)] Loss: 12220.657227\n",
      "Train Epoch: 4829 [65792/118836 (55%)] Loss: 12367.132812\n",
      "Train Epoch: 4829 [98560/118836 (83%)] Loss: 12263.957031\n",
      "    epoch          : 4829\n",
      "    loss           : 12218.74435063844\n",
      "    val_loss       : 12216.05596079512\n",
      "    val_log_likelihood: -12140.637171894386\n",
      "    val_log_marginal: -12148.526655028152\n",
      "Train Epoch: 4830 [256/118836 (0%)] Loss: 12226.920898\n",
      "Train Epoch: 4830 [33024/118836 (28%)] Loss: 12233.926758\n",
      "Train Epoch: 4830 [65792/118836 (55%)] Loss: 12357.830078\n",
      "Train Epoch: 4830 [98560/118836 (83%)] Loss: 12216.379883\n",
      "    epoch          : 4830\n",
      "    loss           : 12218.396552225497\n",
      "    val_loss       : 12218.506239083055\n",
      "    val_log_likelihood: -12139.504879904622\n",
      "    val_log_marginal: -12147.399746132063\n",
      "Train Epoch: 4831 [256/118836 (0%)] Loss: 12221.880859\n",
      "Train Epoch: 4831 [33024/118836 (28%)] Loss: 12164.228516\n",
      "Train Epoch: 4831 [65792/118836 (55%)] Loss: 12145.897461\n",
      "Train Epoch: 4831 [98560/118836 (83%)] Loss: 12176.080078\n",
      "    epoch          : 4831\n",
      "    loss           : 12217.78058038022\n",
      "    val_loss       : 12215.064517233159\n",
      "    val_log_likelihood: -12139.571245444324\n",
      "    val_log_marginal: -12147.461299274415\n",
      "Train Epoch: 4832 [256/118836 (0%)] Loss: 12202.537109\n",
      "Train Epoch: 4832 [33024/118836 (28%)] Loss: 12243.269531\n",
      "Train Epoch: 4832 [65792/118836 (55%)] Loss: 12343.443359\n",
      "Train Epoch: 4832 [98560/118836 (83%)] Loss: 12275.280273\n",
      "    epoch          : 4832\n",
      "    loss           : 12212.751365572012\n",
      "    val_loss       : 12220.717326799815\n",
      "    val_log_likelihood: -12137.300330690394\n",
      "    val_log_marginal: -12145.193444120498\n",
      "Train Epoch: 4833 [256/118836 (0%)] Loss: 12430.970703\n",
      "Train Epoch: 4833 [33024/118836 (28%)] Loss: 12189.832031\n",
      "Train Epoch: 4833 [65792/118836 (55%)] Loss: 12282.912109\n",
      "Train Epoch: 4833 [98560/118836 (83%)] Loss: 12191.197266\n",
      "    epoch          : 4833\n",
      "    loss           : 12213.815639054745\n",
      "    val_loss       : 12217.764992569057\n",
      "    val_log_likelihood: -12139.096136237335\n",
      "    val_log_marginal: -12146.985844446202\n",
      "Train Epoch: 4834 [256/118836 (0%)] Loss: 12174.535156\n",
      "Train Epoch: 4834 [33024/118836 (28%)] Loss: 12249.233398\n",
      "Train Epoch: 4834 [65792/118836 (55%)] Loss: 12158.539062\n",
      "Train Epoch: 4834 [98560/118836 (83%)] Loss: 12153.014648\n",
      "    epoch          : 4834\n",
      "    loss           : 12219.802324041048\n",
      "    val_loss       : 12216.30007593827\n",
      "    val_log_likelihood: -12141.203803020317\n",
      "    val_log_marginal: -12149.095676326384\n",
      "Train Epoch: 4835 [256/118836 (0%)] Loss: 12313.220703\n",
      "Train Epoch: 4835 [33024/118836 (28%)] Loss: 12362.812500\n",
      "Train Epoch: 4835 [65792/118836 (55%)] Loss: 12281.206055\n",
      "Train Epoch: 4835 [98560/118836 (83%)] Loss: 12289.058594\n",
      "    epoch          : 4835\n",
      "    loss           : 12222.313870580027\n",
      "    val_loss       : 12218.984543395738\n",
      "    val_log_likelihood: -12138.618831743692\n",
      "    val_log_marginal: -12146.51710386921\n",
      "Train Epoch: 4836 [256/118836 (0%)] Loss: 12274.157227\n",
      "Train Epoch: 4836 [33024/118836 (28%)] Loss: 12270.238281\n",
      "Train Epoch: 4836 [65792/118836 (55%)] Loss: 12204.944336\n",
      "Train Epoch: 4836 [98560/118836 (83%)] Loss: 12223.673828\n",
      "    epoch          : 4836\n",
      "    loss           : 12218.064841326768\n",
      "    val_loss       : 12217.52360668509\n",
      "    val_log_likelihood: -12141.895749489506\n",
      "    val_log_marginal: -12149.791784256027\n",
      "Train Epoch: 4837 [256/118836 (0%)] Loss: 12349.578125\n",
      "Train Epoch: 4837 [33024/118836 (28%)] Loss: 12278.420898\n",
      "Train Epoch: 4837 [65792/118836 (55%)] Loss: 12310.220703\n",
      "Train Epoch: 4837 [98560/118836 (83%)] Loss: 12198.189453\n",
      "    epoch          : 4837\n",
      "    loss           : 12218.05962830852\n",
      "    val_loss       : 12216.508833638443\n",
      "    val_log_likelihood: -12141.003265870555\n",
      "    val_log_marginal: -12148.895849994307\n",
      "Train Epoch: 4838 [256/118836 (0%)] Loss: 12265.815430\n",
      "Train Epoch: 4838 [33024/118836 (28%)] Loss: 12197.946289\n",
      "Train Epoch: 4838 [65792/118836 (55%)] Loss: 12300.042969\n",
      "Train Epoch: 4838 [98560/118836 (83%)] Loss: 12225.669922\n",
      "    epoch          : 4838\n",
      "    loss           : 12218.31389949726\n",
      "    val_loss       : 12215.835865321154\n",
      "    val_log_likelihood: -12139.44324338296\n",
      "    val_log_marginal: -12147.340419186225\n",
      "Train Epoch: 4839 [256/118836 (0%)] Loss: 12241.109375\n",
      "Train Epoch: 4839 [33024/118836 (28%)] Loss: 12204.852539\n",
      "Train Epoch: 4839 [65792/118836 (55%)] Loss: 12218.022461\n",
      "Train Epoch: 4839 [98560/118836 (83%)] Loss: 12226.624023\n",
      "    epoch          : 4839\n",
      "    loss           : 12211.693929965364\n",
      "    val_loss       : 12217.829715981024\n",
      "    val_log_likelihood: -12139.938353785412\n",
      "    val_log_marginal: -12147.82453853956\n",
      "Train Epoch: 4840 [256/118836 (0%)] Loss: 12309.959961\n",
      "Train Epoch: 4840 [33024/118836 (28%)] Loss: 12206.691406\n",
      "Train Epoch: 4840 [65792/118836 (55%)] Loss: 12238.845703\n",
      "Train Epoch: 4840 [98560/118836 (83%)] Loss: 12159.848633\n",
      "    epoch          : 4840\n",
      "    loss           : 12217.626849249122\n",
      "    val_loss       : 12220.379550881631\n",
      "    val_log_likelihood: -12140.658604896867\n",
      "    val_log_marginal: -12148.54894176095\n",
      "Train Epoch: 4841 [256/118836 (0%)] Loss: 12373.004883\n",
      "Train Epoch: 4841 [33024/118836 (28%)] Loss: 12366.181641\n",
      "Train Epoch: 4841 [65792/118836 (55%)] Loss: 12291.604492\n",
      "Train Epoch: 4841 [98560/118836 (83%)] Loss: 12215.892578\n",
      "    epoch          : 4841\n",
      "    loss           : 12220.689876382858\n",
      "    val_loss       : 12216.80643173774\n",
      "    val_log_likelihood: -12137.340889940033\n",
      "    val_log_marginal: -12145.238947207788\n",
      "Train Epoch: 4842 [256/118836 (0%)] Loss: 12387.353516\n",
      "Train Epoch: 4842 [33024/118836 (28%)] Loss: 12196.942383\n",
      "Train Epoch: 4842 [65792/118836 (55%)] Loss: 12324.556641\n",
      "Train Epoch: 4842 [98560/118836 (83%)] Loss: 12224.845703\n",
      "    epoch          : 4842\n",
      "    loss           : 12218.216235654467\n",
      "    val_loss       : 12221.123649830537\n",
      "    val_log_likelihood: -12138.474697580645\n",
      "    val_log_marginal: -12146.370844966497\n",
      "Train Epoch: 4843 [256/118836 (0%)] Loss: 12224.960938\n",
      "Train Epoch: 4843 [33024/118836 (28%)] Loss: 12280.993164\n",
      "Train Epoch: 4843 [65792/118836 (55%)] Loss: 12249.951172\n",
      "Train Epoch: 4843 [98560/118836 (83%)] Loss: 12280.910156\n",
      "    epoch          : 4843\n",
      "    loss           : 12217.70948000672\n",
      "    val_loss       : 12214.468107977573\n",
      "    val_log_likelihood: -12138.466174427473\n",
      "    val_log_marginal: -12146.362407674585\n",
      "Train Epoch: 4844 [256/118836 (0%)] Loss: 12376.027344\n",
      "Train Epoch: 4844 [33024/118836 (28%)] Loss: 12363.892578\n",
      "Train Epoch: 4844 [65792/118836 (55%)] Loss: 12269.159180\n",
      "Train Epoch: 4844 [98560/118836 (83%)] Loss: 12294.154297\n",
      "    epoch          : 4844\n",
      "    loss           : 12221.835871103442\n",
      "    val_loss       : 12218.722638489051\n",
      "    val_log_likelihood: -12138.718712843776\n",
      "    val_log_marginal: -12146.615064323274\n",
      "Train Epoch: 4845 [256/118836 (0%)] Loss: 12257.189453\n",
      "Train Epoch: 4845 [33024/118836 (28%)] Loss: 12310.081055\n",
      "Train Epoch: 4845 [65792/118836 (55%)] Loss: 12263.712891\n",
      "Train Epoch: 4845 [98560/118836 (83%)] Loss: 12215.368164\n",
      "    epoch          : 4845\n",
      "    loss           : 12215.040287847653\n",
      "    val_loss       : 12216.683118111494\n",
      "    val_log_likelihood: -12140.109804881358\n",
      "    val_log_marginal: -12148.008781667595\n",
      "Train Epoch: 4846 [256/118836 (0%)] Loss: 12214.367188\n",
      "Train Epoch: 4846 [33024/118836 (28%)] Loss: 12197.916992\n",
      "Train Epoch: 4846 [65792/118836 (55%)] Loss: 12216.324219\n",
      "Train Epoch: 4846 [98560/118836 (83%)] Loss: 12292.186523\n",
      "    epoch          : 4846\n",
      "    loss           : 12215.375867032413\n",
      "    val_loss       : 12215.557696020056\n",
      "    val_log_likelihood: -12140.005423839435\n",
      "    val_log_marginal: -12147.898951550329\n",
      "Train Epoch: 4847 [256/118836 (0%)] Loss: 12336.545898\n",
      "Train Epoch: 4847 [33024/118836 (28%)] Loss: 12266.160156\n",
      "Train Epoch: 4847 [65792/118836 (55%)] Loss: 12250.934570\n",
      "Train Epoch: 4847 [98560/118836 (83%)] Loss: 12202.505859\n",
      "    epoch          : 4847\n",
      "    loss           : 12219.21451532129\n",
      "    val_loss       : 12217.20369942237\n",
      "    val_log_likelihood: -12139.260322645265\n",
      "    val_log_marginal: -12147.14906049941\n",
      "Train Epoch: 4848 [256/118836 (0%)] Loss: 12259.353516\n",
      "Train Epoch: 4848 [33024/118836 (28%)] Loss: 12163.705078\n",
      "Train Epoch: 4848 [65792/118836 (55%)] Loss: 12340.784180\n",
      "Train Epoch: 4848 [98560/118836 (83%)] Loss: 12288.085938\n",
      "    epoch          : 4848\n",
      "    loss           : 12216.71606538203\n",
      "    val_loss       : 12219.619199547331\n",
      "    val_log_likelihood: -12140.057532697478\n",
      "    val_log_marginal: -12147.941923549271\n",
      "Train Epoch: 4849 [256/118836 (0%)] Loss: 12286.616211\n",
      "Train Epoch: 4849 [33024/118836 (28%)] Loss: 12213.359375\n",
      "Train Epoch: 4849 [65792/118836 (55%)] Loss: 12303.985352\n",
      "Train Epoch: 4849 [98560/118836 (83%)] Loss: 12325.082031\n",
      "    epoch          : 4849\n",
      "    loss           : 12216.733397952854\n",
      "    val_loss       : 12218.052408616983\n",
      "    val_log_likelihood: -12140.026439238265\n",
      "    val_log_marginal: -12147.915794716617\n",
      "Train Epoch: 4850 [256/118836 (0%)] Loss: 12312.015625\n",
      "Train Epoch: 4850 [33024/118836 (28%)] Loss: 12326.650391\n",
      "Train Epoch: 4850 [65792/118836 (55%)] Loss: 12306.310547\n",
      "Train Epoch: 4850 [98560/118836 (83%)] Loss: 12125.805664\n",
      "    epoch          : 4850\n",
      "    loss           : 12213.517343879239\n",
      "    val_loss       : 12218.911672045642\n",
      "    val_log_likelihood: -12139.683294238523\n",
      "    val_log_marginal: -12147.576504453593\n",
      "Train Epoch: 4851 [256/118836 (0%)] Loss: 12234.022461\n",
      "Train Epoch: 4851 [33024/118836 (28%)] Loss: 12300.802734\n",
      "Train Epoch: 4851 [65792/118836 (55%)] Loss: 12294.100586\n",
      "Train Epoch: 4851 [98560/118836 (83%)] Loss: 12439.611328\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 04851: reducing learning rate of group 0 to 1.0000e-07.\n",
      "    epoch          : 4851\n",
      "    loss           : 12218.552535831524\n",
      "    val_loss       : 12216.90706171917\n",
      "    val_log_likelihood: -12138.118866315135\n",
      "    val_log_marginal: -12146.007457262367\n",
      "Train Epoch: 4852 [256/118836 (0%)] Loss: 12197.739258\n",
      "Train Epoch: 4852 [33024/118836 (28%)] Loss: 12187.871094\n",
      "Train Epoch: 4852 [65792/118836 (55%)] Loss: 12296.587891\n",
      "Train Epoch: 4852 [98560/118836 (83%)] Loss: 12190.925781\n",
      "    epoch          : 4852\n",
      "    loss           : 12215.473707609595\n",
      "    val_loss       : 12218.351804220249\n",
      "    val_log_likelihood: -12138.42622179358\n",
      "    val_log_marginal: -12146.321973951164\n",
      "Train Epoch: 4853 [256/118836 (0%)] Loss: 12199.169922\n",
      "Train Epoch: 4853 [33024/118836 (28%)] Loss: 12237.218750\n",
      "Train Epoch: 4853 [65792/118836 (55%)] Loss: 12296.299805\n",
      "Train Epoch: 4853 [98560/118836 (83%)] Loss: 12198.509766\n",
      "    epoch          : 4853\n",
      "    loss           : 12217.848224255582\n",
      "    val_loss       : 12218.422557336555\n",
      "    val_log_likelihood: -12141.197529111094\n",
      "    val_log_marginal: -12149.087140129386\n",
      "Train Epoch: 4854 [256/118836 (0%)] Loss: 12200.817383\n",
      "Train Epoch: 4854 [33024/118836 (28%)] Loss: 12189.451172\n",
      "Train Epoch: 4854 [65792/118836 (55%)] Loss: 12168.638672\n",
      "Train Epoch: 4854 [98560/118836 (83%)] Loss: 12195.036133\n",
      "    epoch          : 4854\n",
      "    loss           : 12218.541872479838\n",
      "    val_loss       : 12214.128918923181\n",
      "    val_log_likelihood: -12139.886020051437\n",
      "    val_log_marginal: -12147.772993274735\n",
      "Train Epoch: 4855 [256/118836 (0%)] Loss: 12248.525391\n",
      "Train Epoch: 4855 [33024/118836 (28%)] Loss: 12255.225586\n",
      "Train Epoch: 4855 [65792/118836 (55%)] Loss: 12273.647461\n",
      "Train Epoch: 4855 [98560/118836 (83%)] Loss: 12290.890625\n",
      "    epoch          : 4855\n",
      "    loss           : 12219.736370289238\n",
      "    val_loss       : 12218.208457099096\n",
      "    val_log_likelihood: -12139.029001886891\n",
      "    val_log_marginal: -12146.923241883153\n",
      "Train Epoch: 4856 [256/118836 (0%)] Loss: 12220.769531\n",
      "Train Epoch: 4856 [33024/118836 (28%)] Loss: 12183.046875\n",
      "Train Epoch: 4856 [65792/118836 (55%)] Loss: 12213.479492\n",
      "Train Epoch: 4856 [98560/118836 (83%)] Loss: 12206.136719\n",
      "    epoch          : 4856\n",
      "    loss           : 12212.303945506357\n",
      "    val_loss       : 12219.644043342218\n",
      "    val_log_likelihood: -12140.370518797818\n",
      "    val_log_marginal: -12148.264010560668\n",
      "Train Epoch: 4857 [256/118836 (0%)] Loss: 12139.030273\n",
      "Train Epoch: 4857 [33024/118836 (28%)] Loss: 12177.897461\n",
      "Train Epoch: 4857 [65792/118836 (55%)] Loss: 12219.833984\n",
      "Train Epoch: 4857 [98560/118836 (83%)] Loss: 12132.201172\n",
      "    epoch          : 4857\n",
      "    loss           : 12216.55970520575\n",
      "    val_loss       : 12219.496687541465\n",
      "    val_log_likelihood: -12139.480957112024\n",
      "    val_log_marginal: -12147.380236075764\n",
      "Train Epoch: 4858 [256/118836 (0%)] Loss: 12125.383789\n",
      "Train Epoch: 4858 [33024/118836 (28%)] Loss: 12179.125000\n",
      "Train Epoch: 4858 [65792/118836 (55%)] Loss: 12166.489258\n",
      "Train Epoch: 4858 [98560/118836 (83%)] Loss: 12236.345703\n",
      "    epoch          : 4858\n",
      "    loss           : 12217.202937118745\n",
      "    val_loss       : 12221.711214055113\n",
      "    val_log_likelihood: -12140.112056386994\n",
      "    val_log_marginal: -12148.00712721032\n",
      "Train Epoch: 4859 [256/118836 (0%)] Loss: 12266.416016\n",
      "Train Epoch: 4859 [33024/118836 (28%)] Loss: 12259.741211\n",
      "Train Epoch: 4859 [65792/118836 (55%)] Loss: 12164.222656\n",
      "Train Epoch: 4859 [98560/118836 (83%)] Loss: 12243.521484\n",
      "    epoch          : 4859\n",
      "    loss           : 12216.814297391957\n",
      "    val_loss       : 12219.10908812818\n",
      "    val_log_likelihood: -12140.611886114557\n",
      "    val_log_marginal: -12148.499388317585\n",
      "Train Epoch: 4860 [256/118836 (0%)] Loss: 12207.588867\n",
      "Train Epoch: 4860 [33024/118836 (28%)] Loss: 12288.014648\n",
      "Train Epoch: 4860 [65792/118836 (55%)] Loss: 12273.933594\n",
      "Train Epoch: 4860 [98560/118836 (83%)] Loss: 12256.616211\n",
      "    epoch          : 4860\n",
      "    loss           : 12216.01205008659\n",
      "    val_loss       : 12217.78159974803\n",
      "    val_log_likelihood: -12141.844410573047\n",
      "    val_log_marginal: -12149.741685539699\n",
      "Train Epoch: 4861 [256/118836 (0%)] Loss: 12311.125977\n",
      "Train Epoch: 4861 [33024/118836 (28%)] Loss: 12222.117188\n",
      "Train Epoch: 4861 [65792/118836 (55%)] Loss: 12170.062500\n",
      "Train Epoch: 4861 [98560/118836 (83%)] Loss: 12239.289062\n",
      "    epoch          : 4861\n",
      "    loss           : 12217.13577029699\n",
      "    val_loss       : 12221.355108065518\n",
      "    val_log_likelihood: -12140.137895633012\n",
      "    val_log_marginal: -12148.032588205671\n",
      "Train Epoch: 4862 [256/118836 (0%)] Loss: 12279.116211\n",
      "Train Epoch: 4862 [33024/118836 (28%)] Loss: 12248.222656\n",
      "Train Epoch: 4862 [65792/118836 (55%)] Loss: 12286.134766\n",
      "Train Epoch: 4862 [98560/118836 (83%)] Loss: 12328.584961\n",
      "    epoch          : 4862\n",
      "    loss           : 12216.908261444118\n",
      "    val_loss       : 12215.01665164509\n",
      "    val_log_likelihood: -12141.541178789288\n",
      "    val_log_marginal: -12149.431580699864\n",
      "Train Epoch: 4863 [256/118836 (0%)] Loss: 12224.492188\n",
      "Train Epoch: 4863 [33024/118836 (28%)] Loss: 12231.369141\n",
      "Train Epoch: 4863 [65792/118836 (55%)] Loss: 12314.549805\n",
      "Train Epoch: 4863 [98560/118836 (83%)] Loss: 12157.222656\n",
      "    epoch          : 4863\n",
      "    loss           : 12220.317033382446\n",
      "    val_loss       : 12218.186807279864\n",
      "    val_log_likelihood: -12140.32128663927\n",
      "    val_log_marginal: -12148.214447249717\n",
      "Train Epoch: 4864 [256/118836 (0%)] Loss: 12212.284180\n",
      "Train Epoch: 4864 [33024/118836 (28%)] Loss: 12330.918945\n",
      "Train Epoch: 4864 [65792/118836 (55%)] Loss: 12216.441406\n",
      "Train Epoch: 4864 [98560/118836 (83%)] Loss: 12197.828125\n",
      "    epoch          : 4864\n",
      "    loss           : 12219.51389804332\n",
      "    val_loss       : 12218.292313155405\n",
      "    val_log_likelihood: -12138.454603494623\n",
      "    val_log_marginal: -12146.351920750274\n",
      "Train Epoch: 4865 [256/118836 (0%)] Loss: 12248.074219\n",
      "Train Epoch: 4865 [33024/118836 (28%)] Loss: 12229.398438\n",
      "Train Epoch: 4865 [65792/118836 (55%)] Loss: 12298.708984\n",
      "Train Epoch: 4865 [98560/118836 (83%)] Loss: 12191.082031\n",
      "    epoch          : 4865\n",
      "    loss           : 12219.899051870088\n",
      "    val_loss       : 12216.657722197124\n",
      "    val_log_likelihood: -12139.782239486403\n",
      "    val_log_marginal: -12147.678199250087\n",
      "Train Epoch: 4866 [256/118836 (0%)] Loss: 12152.520508\n",
      "Train Epoch: 4866 [33024/118836 (28%)] Loss: 12278.290039\n",
      "Train Epoch: 4866 [65792/118836 (55%)] Loss: 12225.144531\n",
      "Train Epoch: 4866 [98560/118836 (83%)] Loss: 12232.494141\n",
      "    epoch          : 4866\n",
      "    loss           : 12219.826473163514\n",
      "    val_loss       : 12214.772168058118\n",
      "    val_log_likelihood: -12137.931123022643\n",
      "    val_log_marginal: -12145.825295173714\n",
      "Train Epoch: 4867 [256/118836 (0%)] Loss: 12210.369141\n",
      "Train Epoch: 4867 [33024/118836 (28%)] Loss: 12261.886719\n",
      "Train Epoch: 4867 [65792/118836 (55%)] Loss: 12418.829102\n",
      "Train Epoch: 4867 [98560/118836 (83%)] Loss: 12292.481445\n",
      "    epoch          : 4867\n",
      "    loss           : 12216.913923568032\n",
      "    val_loss       : 12215.62841688344\n",
      "    val_log_likelihood: -12139.60706226737\n",
      "    val_log_marginal: -12147.494023957588\n",
      "Train Epoch: 4868 [256/118836 (0%)] Loss: 12263.347656\n",
      "Train Epoch: 4868 [33024/118836 (28%)] Loss: 12237.574219\n",
      "Train Epoch: 4868 [65792/118836 (55%)] Loss: 12400.692383\n",
      "Train Epoch: 4868 [98560/118836 (83%)] Loss: 12280.777344\n",
      "    epoch          : 4868\n",
      "    loss           : 12220.065422902451\n",
      "    val_loss       : 12212.936677308195\n",
      "    val_log_likelihood: -12140.168955490075\n",
      "    val_log_marginal: -12148.061376152009\n",
      "Train Epoch: 4869 [256/118836 (0%)] Loss: 12203.262695\n",
      "Train Epoch: 4869 [33024/118836 (28%)] Loss: 12334.457031\n",
      "Train Epoch: 4869 [65792/118836 (55%)] Loss: 12208.708984\n",
      "Train Epoch: 4869 [98560/118836 (83%)] Loss: 12237.516602\n",
      "    epoch          : 4869\n",
      "    loss           : 12218.420427361198\n",
      "    val_loss       : 12220.811478492538\n",
      "    val_log_likelihood: -12141.140900117607\n",
      "    val_log_marginal: -12149.04021365929\n",
      "Train Epoch: 4870 [256/118836 (0%)] Loss: 12184.733398\n",
      "Train Epoch: 4870 [33024/118836 (28%)] Loss: 12218.453125\n",
      "Train Epoch: 4870 [65792/118836 (55%)] Loss: 12249.257812\n",
      "Train Epoch: 4870 [98560/118836 (83%)] Loss: 12227.988281\n",
      "    epoch          : 4870\n",
      "    loss           : 12222.343690065394\n",
      "    val_loss       : 12217.838605060819\n",
      "    val_log_likelihood: -12139.905700895626\n",
      "    val_log_marginal: -12147.803738650215\n",
      "Train Epoch: 4871 [256/118836 (0%)] Loss: 12195.310547\n",
      "Train Epoch: 4871 [33024/118836 (28%)] Loss: 12212.238281\n",
      "Train Epoch: 4871 [65792/118836 (55%)] Loss: 12228.406250\n",
      "Train Epoch: 4871 [98560/118836 (83%)] Loss: 12280.530273\n",
      "    epoch          : 4871\n",
      "    loss           : 12216.679797514733\n",
      "    val_loss       : 12213.799267356148\n",
      "    val_log_likelihood: -12140.13037182072\n",
      "    val_log_marginal: -12148.023668243544\n",
      "Train Epoch: 4872 [256/118836 (0%)] Loss: 12172.653320\n",
      "Train Epoch: 4872 [33024/118836 (28%)] Loss: 12188.712891\n",
      "Train Epoch: 4872 [65792/118836 (55%)] Loss: 12207.426758\n",
      "Train Epoch: 4872 [98560/118836 (83%)] Loss: 12222.449219\n",
      "    epoch          : 4872\n",
      "    loss           : 12215.912612276416\n",
      "    val_loss       : 12214.676344362606\n",
      "    val_log_likelihood: -12140.33848609388\n",
      "    val_log_marginal: -12148.230344624026\n",
      "Train Epoch: 4873 [256/118836 (0%)] Loss: 12243.608398\n",
      "Train Epoch: 4873 [33024/118836 (28%)] Loss: 12185.277344\n",
      "Train Epoch: 4873 [65792/118836 (55%)] Loss: 12185.675781\n",
      "Train Epoch: 4873 [98560/118836 (83%)] Loss: 12319.435547\n",
      "    epoch          : 4873\n",
      "    loss           : 12220.176869281173\n",
      "    val_loss       : 12216.979686590372\n",
      "    val_log_likelihood: -12139.133235757858\n",
      "    val_log_marginal: -12147.02646251915\n",
      "Train Epoch: 4874 [256/118836 (0%)] Loss: 12321.651367\n",
      "Train Epoch: 4874 [33024/118836 (28%)] Loss: 12186.652344\n",
      "Train Epoch: 4874 [65792/118836 (55%)] Loss: 12193.055664\n",
      "Train Epoch: 4874 [98560/118836 (83%)] Loss: 12289.100586\n",
      "    epoch          : 4874\n",
      "    loss           : 12218.039423400021\n",
      "    val_loss       : 12219.540498143648\n",
      "    val_log_likelihood: -12138.212323911806\n",
      "    val_log_marginal: -12146.110500045781\n",
      "Train Epoch: 4875 [256/118836 (0%)] Loss: 12200.269531\n",
      "Train Epoch: 4875 [33024/118836 (28%)] Loss: 12179.801758\n",
      "Train Epoch: 4875 [65792/118836 (55%)] Loss: 12227.802734\n",
      "Train Epoch: 4875 [98560/118836 (83%)] Loss: 12207.041992\n",
      "    epoch          : 4875\n",
      "    loss           : 12220.065454404466\n",
      "    val_loss       : 12216.444535775385\n",
      "    val_log_likelihood: -12140.925037317773\n",
      "    val_log_marginal: -12148.819730373874\n",
      "Train Epoch: 4876 [256/118836 (0%)] Loss: 12181.659180\n",
      "Train Epoch: 4876 [33024/118836 (28%)] Loss: 12225.435547\n",
      "Train Epoch: 4876 [65792/118836 (55%)] Loss: 12168.054688\n",
      "Train Epoch: 4876 [98560/118836 (83%)] Loss: 12262.863281\n",
      "    epoch          : 4876\n",
      "    loss           : 12217.972614247312\n",
      "    val_loss       : 12217.529777476042\n",
      "    val_log_likelihood: -12138.900572367402\n",
      "    val_log_marginal: -12146.7850607875\n",
      "Train Epoch: 4877 [256/118836 (0%)] Loss: 12257.385742\n",
      "Train Epoch: 4877 [33024/118836 (28%)] Loss: 12200.191406\n",
      "Train Epoch: 4877 [65792/118836 (55%)] Loss: 12313.449219\n",
      "Train Epoch: 4877 [98560/118836 (83%)] Loss: 12171.054688\n",
      "    epoch          : 4877\n",
      "    loss           : 12221.79279217716\n",
      "    val_loss       : 12215.192519226368\n",
      "    val_log_likelihood: -12138.873657691016\n",
      "    val_log_marginal: -12146.771314940748\n",
      "Train Epoch: 4878 [256/118836 (0%)] Loss: 12193.013672\n",
      "Train Epoch: 4878 [33024/118836 (28%)] Loss: 12297.401367\n",
      "Train Epoch: 4878 [65792/118836 (55%)] Loss: 12376.515625\n",
      "Train Epoch: 4878 [98560/118836 (83%)] Loss: 12178.929688\n",
      "    epoch          : 4878\n",
      "    loss           : 12217.252624360268\n",
      "    val_loss       : 12217.75111336608\n",
      "    val_log_likelihood: -12142.5518319634\n",
      "    val_log_marginal: -12150.446469235754\n",
      "Train Epoch: 4879 [256/118836 (0%)] Loss: 12328.174805\n",
      "Train Epoch: 4879 [33024/118836 (28%)] Loss: 12337.731445\n",
      "Train Epoch: 4879 [65792/118836 (55%)] Loss: 12208.612305\n",
      "Train Epoch: 4879 [98560/118836 (83%)] Loss: 12223.928711\n",
      "    epoch          : 4879\n",
      "    loss           : 12216.843210265457\n",
      "    val_loss       : 12214.614362406814\n",
      "    val_log_likelihood: -12140.559636709058\n",
      "    val_log_marginal: -12148.446364731572\n",
      "Train Epoch: 4880 [256/118836 (0%)] Loss: 12256.811523\n",
      "Train Epoch: 4880 [33024/118836 (28%)] Loss: 12237.242188\n",
      "Train Epoch: 4880 [65792/118836 (55%)] Loss: 12330.452148\n",
      "Train Epoch: 4880 [98560/118836 (83%)] Loss: 12211.041992\n",
      "    epoch          : 4880\n",
      "    loss           : 12212.523779821908\n",
      "    val_loss       : 12222.211184832988\n",
      "    val_log_likelihood: -12139.383704572478\n",
      "    val_log_marginal: -12147.271071598892\n",
      "Train Epoch: 4881 [256/118836 (0%)] Loss: 12288.705078\n",
      "Train Epoch: 4881 [33024/118836 (28%)] Loss: 12231.334961\n",
      "Train Epoch: 4881 [65792/118836 (55%)] Loss: 12230.901367\n",
      "Train Epoch: 4881 [98560/118836 (83%)] Loss: 12182.796875\n",
      "    epoch          : 4881\n",
      "    loss           : 12217.278864893507\n",
      "    val_loss       : 12216.438434166761\n",
      "    val_log_likelihood: -12138.928948575785\n",
      "    val_log_marginal: -12146.818104344771\n",
      "Train Epoch: 4882 [256/118836 (0%)] Loss: 12320.498047\n",
      "Train Epoch: 4882 [33024/118836 (28%)] Loss: 12247.618164\n",
      "Train Epoch: 4882 [65792/118836 (55%)] Loss: 12233.451172\n",
      "Train Epoch: 4882 [98560/118836 (83%)] Loss: 12398.958984\n",
      "    epoch          : 4882\n",
      "    loss           : 12217.06357219939\n",
      "    val_loss       : 12219.485984853754\n",
      "    val_log_likelihood: -12139.832669367763\n",
      "    val_log_marginal: -12147.726382707175\n",
      "Train Epoch: 4883 [256/118836 (0%)] Loss: 12227.125000\n",
      "Train Epoch: 4883 [33024/118836 (28%)] Loss: 12371.775391\n",
      "Train Epoch: 4883 [65792/118836 (55%)] Loss: 12176.402344\n",
      "Train Epoch: 4883 [98560/118836 (83%)] Loss: 12173.375000\n",
      "    epoch          : 4883\n",
      "    loss           : 12214.677825003877\n",
      "    val_loss       : 12215.576433209728\n",
      "    val_log_likelihood: -12138.578103998656\n",
      "    val_log_marginal: -12146.47330970769\n",
      "Train Epoch: 4884 [256/118836 (0%)] Loss: 12195.802734\n",
      "Train Epoch: 4884 [33024/118836 (28%)] Loss: 12199.519531\n",
      "Train Epoch: 4884 [65792/118836 (55%)] Loss: 12336.112305\n",
      "Train Epoch: 4884 [98560/118836 (83%)] Loss: 12162.143555\n",
      "    epoch          : 4884\n",
      "    loss           : 12218.178150524709\n",
      "    val_loss       : 12216.218773209646\n",
      "    val_log_likelihood: -12137.827034384047\n",
      "    val_log_marginal: -12145.716367279692\n",
      "Train Epoch: 4885 [256/118836 (0%)] Loss: 12416.479492\n",
      "Train Epoch: 4885 [33024/118836 (28%)] Loss: 12276.251953\n",
      "Train Epoch: 4885 [65792/118836 (55%)] Loss: 12181.724609\n",
      "Train Epoch: 4885 [98560/118836 (83%)] Loss: 12206.502930\n",
      "    epoch          : 4885\n",
      "    loss           : 12221.989177684296\n",
      "    val_loss       : 12214.531636184882\n",
      "    val_log_likelihood: -12138.670515082196\n",
      "    val_log_marginal: -12146.570808228797\n",
      "Train Epoch: 4886 [256/118836 (0%)] Loss: 12357.400391\n",
      "Train Epoch: 4886 [33024/118836 (28%)] Loss: 12191.141602\n",
      "Train Epoch: 4886 [65792/118836 (55%)] Loss: 12271.717773\n",
      "Train Epoch: 4886 [98560/118836 (83%)] Loss: 12205.410156\n",
      "    epoch          : 4886\n",
      "    loss           : 12216.064399975445\n",
      "    val_loss       : 12220.399244745067\n",
      "    val_log_likelihood: -12138.499691118692\n",
      "    val_log_marginal: -12146.386483008579\n",
      "Train Epoch: 4887 [256/118836 (0%)] Loss: 12171.121094\n",
      "Train Epoch: 4887 [33024/118836 (28%)] Loss: 12170.676758\n",
      "Train Epoch: 4887 [65792/118836 (55%)] Loss: 12252.693359\n",
      "Train Epoch: 4887 [98560/118836 (83%)] Loss: 12185.338867\n",
      "    epoch          : 4887\n",
      "    loss           : 12219.5100013247\n",
      "    val_loss       : 12218.808836034477\n",
      "    val_log_likelihood: -12139.809941228546\n",
      "    val_log_marginal: -12147.698818593062\n",
      "Train Epoch: 4888 [256/118836 (0%)] Loss: 12187.015625\n",
      "Train Epoch: 4888 [33024/118836 (28%)] Loss: 12213.413086\n",
      "Train Epoch: 4888 [65792/118836 (55%)] Loss: 12214.501953\n",
      "Train Epoch: 4888 [98560/118836 (83%)] Loss: 12199.136719\n",
      "    epoch          : 4888\n",
      "    loss           : 12216.984624269799\n",
      "    val_loss       : 12217.682981625676\n",
      "    val_log_likelihood: -12140.275979955026\n",
      "    val_log_marginal: -12148.1644647077\n",
      "Train Epoch: 4889 [256/118836 (0%)] Loss: 12248.213867\n",
      "Train Epoch: 4889 [33024/118836 (28%)] Loss: 12163.220703\n",
      "Train Epoch: 4889 [65792/118836 (55%)] Loss: 12293.900391\n",
      "Train Epoch: 4889 [98560/118836 (83%)] Loss: 12390.475586\n",
      "    epoch          : 4889\n",
      "    loss           : 12220.480109950113\n",
      "    val_loss       : 12219.14943757713\n",
      "    val_log_likelihood: -12139.645233017989\n",
      "    val_log_marginal: -12147.53332466177\n",
      "Train Epoch: 4890 [256/118836 (0%)] Loss: 12186.546875\n",
      "Train Epoch: 4890 [33024/118836 (28%)] Loss: 12252.353516\n",
      "Train Epoch: 4890 [65792/118836 (55%)] Loss: 12257.138672\n",
      "Train Epoch: 4890 [98560/118836 (83%)] Loss: 12264.163086\n",
      "    epoch          : 4890\n",
      "    loss           : 12217.017725457505\n",
      "    val_loss       : 12213.311671747464\n",
      "    val_log_likelihood: -12139.214927593826\n",
      "    val_log_marginal: -12147.104936468962\n",
      "Train Epoch: 4891 [256/118836 (0%)] Loss: 12139.771484\n",
      "Train Epoch: 4891 [33024/118836 (28%)] Loss: 12166.304688\n",
      "Train Epoch: 4891 [65792/118836 (55%)] Loss: 12276.069336\n",
      "Train Epoch: 4891 [98560/118836 (83%)] Loss: 12302.354492\n",
      "    epoch          : 4891\n",
      "    loss           : 12215.043249198718\n",
      "    val_loss       : 12213.819296809148\n",
      "    val_log_likelihood: -12138.475733431555\n",
      "    val_log_marginal: -12146.36838385538\n",
      "Train Epoch: 4892 [256/118836 (0%)] Loss: 12434.425781\n",
      "Train Epoch: 4892 [33024/118836 (28%)] Loss: 12172.913086\n",
      "Train Epoch: 4892 [65792/118836 (55%)] Loss: 12243.833008\n",
      "Train Epoch: 4892 [98560/118836 (83%)] Loss: 12405.330078\n",
      "    epoch          : 4892\n",
      "    loss           : 12220.655840312242\n",
      "    val_loss       : 12218.469473965031\n",
      "    val_log_likelihood: -12138.481614454095\n",
      "    val_log_marginal: -12146.37364953081\n",
      "Train Epoch: 4893 [256/118836 (0%)] Loss: 12240.223633\n",
      "Train Epoch: 4893 [33024/118836 (28%)] Loss: 12216.169922\n",
      "Train Epoch: 4893 [65792/118836 (55%)] Loss: 12264.009766\n",
      "Train Epoch: 4893 [98560/118836 (83%)] Loss: 12183.696289\n",
      "    epoch          : 4893\n",
      "    loss           : 12216.836973997106\n",
      "    val_loss       : 12218.849871681428\n",
      "    val_log_likelihood: -12138.984174679488\n",
      "    val_log_marginal: -12146.875617582231\n",
      "Train Epoch: 4894 [256/118836 (0%)] Loss: 12212.000977\n",
      "Train Epoch: 4894 [33024/118836 (28%)] Loss: 12167.849609\n",
      "Train Epoch: 4894 [65792/118836 (55%)] Loss: 12183.009766\n",
      "Train Epoch: 4894 [98560/118836 (83%)] Loss: 12163.714844\n",
      "    epoch          : 4894\n",
      "    loss           : 12216.523834263853\n",
      "    val_loss       : 12219.814718447946\n",
      "    val_log_likelihood: -12140.704033712003\n",
      "    val_log_marginal: -12148.595224199655\n",
      "Train Epoch: 4895 [256/118836 (0%)] Loss: 12210.482422\n",
      "Train Epoch: 4895 [33024/118836 (28%)] Loss: 12258.712891\n",
      "Train Epoch: 4895 [65792/118836 (55%)] Loss: 12174.046875\n",
      "Train Epoch: 4895 [98560/118836 (83%)] Loss: 12216.616211\n",
      "    epoch          : 4895\n",
      "    loss           : 12213.002071217174\n",
      "    val_loss       : 12220.752251181106\n",
      "    val_log_likelihood: -12138.181580205748\n",
      "    val_log_marginal: -12146.074017983214\n",
      "Train Epoch: 4896 [256/118836 (0%)] Loss: 12274.471680\n",
      "Train Epoch: 4896 [33024/118836 (28%)] Loss: 12178.120117\n",
      "Train Epoch: 4896 [65792/118836 (55%)] Loss: 12300.897461\n",
      "Train Epoch: 4896 [98560/118836 (83%)] Loss: 12190.278320\n",
      "    epoch          : 4896\n",
      "    loss           : 12216.20452304332\n",
      "    val_loss       : 12217.446898335782\n",
      "    val_log_likelihood: -12139.241367962934\n",
      "    val_log_marginal: -12147.133376576028\n",
      "Train Epoch: 4897 [256/118836 (0%)] Loss: 12215.710938\n",
      "Train Epoch: 4897 [33024/118836 (28%)] Loss: 12175.070312\n",
      "Train Epoch: 4897 [65792/118836 (55%)] Loss: 12263.989258\n",
      "Train Epoch: 4897 [98560/118836 (83%)] Loss: 12261.637695\n",
      "    epoch          : 4897\n",
      "    loss           : 12218.29311139759\n",
      "    val_loss       : 12219.391258310443\n",
      "    val_log_likelihood: -12138.244762587881\n",
      "    val_log_marginal: -12146.136206987228\n",
      "Train Epoch: 4898 [256/118836 (0%)] Loss: 12270.031250\n",
      "Train Epoch: 4898 [33024/118836 (28%)] Loss: 12241.490234\n",
      "Train Epoch: 4898 [65792/118836 (55%)] Loss: 12331.212891\n",
      "Train Epoch: 4898 [98560/118836 (83%)] Loss: 12138.352539\n",
      "    epoch          : 4898\n",
      "    loss           : 12215.950835853495\n",
      "    val_loss       : 12213.402112971582\n",
      "    val_log_likelihood: -12140.838650550559\n",
      "    val_log_marginal: -12148.736358563649\n",
      "Train Epoch: 4899 [256/118836 (0%)] Loss: 12169.613281\n",
      "Train Epoch: 4899 [33024/118836 (28%)] Loss: 12255.961914\n",
      "Train Epoch: 4899 [65792/118836 (55%)] Loss: 12247.746094\n",
      "Train Epoch: 4899 [98560/118836 (83%)] Loss: 12286.915039\n",
      "    epoch          : 4899\n",
      "    loss           : 12216.98187890948\n",
      "    val_loss       : 12215.59105824469\n",
      "    val_log_likelihood: -12140.60859795027\n",
      "    val_log_marginal: -12148.501156099224\n",
      "Train Epoch: 4900 [256/118836 (0%)] Loss: 12184.619141\n",
      "Train Epoch: 4900 [33024/118836 (28%)] Loss: 12311.266602\n",
      "Train Epoch: 4900 [65792/118836 (55%)] Loss: 12315.612305\n",
      "Train Epoch: 4900 [98560/118836 (83%)] Loss: 12121.085938\n",
      "    epoch          : 4900\n",
      "    loss           : 12217.910474339587\n",
      "    val_loss       : 12220.279489359757\n",
      "    val_log_likelihood: -12140.183459664495\n",
      "    val_log_marginal: -12148.075254529438\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch4900.pth ...\n",
      "Train Epoch: 4901 [256/118836 (0%)] Loss: 12284.343750\n",
      "Train Epoch: 4901 [33024/118836 (28%)] Loss: 12357.507812\n",
      "Train Epoch: 4901 [65792/118836 (55%)] Loss: 12208.832031\n",
      "Train Epoch: 4901 [98560/118836 (83%)] Loss: 12165.189453\n",
      "    epoch          : 4901\n",
      "    loss           : 12220.13711195978\n",
      "    val_loss       : 12215.96266277622\n",
      "    val_log_likelihood: -12139.715065394956\n",
      "    val_log_marginal: -12147.609484126182\n",
      "Train Epoch: 4902 [256/118836 (0%)] Loss: 12233.240234\n",
      "Train Epoch: 4902 [33024/118836 (28%)] Loss: 12181.007812\n",
      "Train Epoch: 4902 [65792/118836 (55%)] Loss: 12190.592773\n",
      "Train Epoch: 4902 [98560/118836 (83%)] Loss: 12209.640625\n",
      "    epoch          : 4902\n",
      "    loss           : 12220.954598971259\n",
      "    val_loss       : 12222.140733898268\n",
      "    val_log_likelihood: -12140.250817921578\n",
      "    val_log_marginal: -12148.145406580226\n",
      "Train Epoch: 4903 [256/118836 (0%)] Loss: 12236.273438\n",
      "Train Epoch: 4903 [33024/118836 (28%)] Loss: 12246.790039\n",
      "Train Epoch: 4903 [65792/118836 (55%)] Loss: 12209.048828\n",
      "Train Epoch: 4903 [98560/118836 (83%)] Loss: 12362.420898\n",
      "    epoch          : 4903\n",
      "    loss           : 12216.45966320306\n",
      "    val_loss       : 12218.41624266162\n",
      "    val_log_likelihood: -12139.291758264837\n",
      "    val_log_marginal: -12147.180522803885\n",
      "Train Epoch: 4904 [256/118836 (0%)] Loss: 12277.541016\n",
      "Train Epoch: 4904 [33024/118836 (28%)] Loss: 12165.836914\n",
      "Train Epoch: 4904 [65792/118836 (55%)] Loss: 12279.954102\n",
      "Train Epoch: 4904 [98560/118836 (83%)] Loss: 12264.198242\n",
      "    epoch          : 4904\n",
      "    loss           : 12213.26787165917\n",
      "    val_loss       : 12216.668587554987\n",
      "    val_log_likelihood: -12141.556879717225\n",
      "    val_log_marginal: -12149.45107411753\n",
      "Train Epoch: 4905 [256/118836 (0%)] Loss: 12321.134766\n",
      "Train Epoch: 4905 [33024/118836 (28%)] Loss: 12226.484375\n",
      "Train Epoch: 4905 [65792/118836 (55%)] Loss: 12212.588867\n",
      "Train Epoch: 4905 [98560/118836 (83%)] Loss: 12210.522461\n",
      "    epoch          : 4905\n",
      "    loss           : 12221.003419665012\n",
      "    val_loss       : 12213.869481535598\n",
      "    val_log_likelihood: -12140.357799737643\n",
      "    val_log_marginal: -12148.253672078079\n",
      "Train Epoch: 4906 [256/118836 (0%)] Loss: 12248.069336\n",
      "Train Epoch: 4906 [33024/118836 (28%)] Loss: 12172.777344\n",
      "Train Epoch: 4906 [65792/118836 (55%)] Loss: 12224.299805\n",
      "Train Epoch: 4906 [98560/118836 (83%)] Loss: 12272.524414\n",
      "    epoch          : 4906\n",
      "    loss           : 12218.12126903045\n",
      "    val_loss       : 12217.454402683981\n",
      "    val_log_likelihood: -12140.507934146246\n",
      "    val_log_marginal: -12148.401962885155\n",
      "Train Epoch: 4907 [256/118836 (0%)] Loss: 12344.702148\n",
      "Train Epoch: 4907 [33024/118836 (28%)] Loss: 12178.463867\n",
      "Train Epoch: 4907 [65792/118836 (55%)] Loss: 12240.476562\n",
      "Train Epoch: 4907 [98560/118836 (83%)] Loss: 12247.204102\n",
      "    epoch          : 4907\n",
      "    loss           : 12220.210824577389\n",
      "    val_loss       : 12221.386005621232\n",
      "    val_log_likelihood: -12140.605378767317\n",
      "    val_log_marginal: -12148.49690243001\n",
      "Train Epoch: 4908 [256/118836 (0%)] Loss: 12214.166016\n",
      "Train Epoch: 4908 [33024/118836 (28%)] Loss: 12182.913086\n",
      "Train Epoch: 4908 [65792/118836 (55%)] Loss: 12421.991211\n",
      "Train Epoch: 4908 [98560/118836 (83%)] Loss: 12276.197266\n",
      "    epoch          : 4908\n",
      "    loss           : 12219.132298451716\n",
      "    val_loss       : 12217.494785638974\n",
      "    val_log_likelihood: -12138.862300971878\n",
      "    val_log_marginal: -12146.749737228682\n",
      "Train Epoch: 4909 [256/118836 (0%)] Loss: 12287.918945\n",
      "Train Epoch: 4909 [33024/118836 (28%)] Loss: 12327.620117\n",
      "Train Epoch: 4909 [65792/118836 (55%)] Loss: 12236.506836\n",
      "Train Epoch: 4909 [98560/118836 (83%)] Loss: 12167.962891\n",
      "    epoch          : 4909\n",
      "    loss           : 12216.774660585968\n",
      "    val_loss       : 12220.552329217939\n",
      "    val_log_likelihood: -12139.309644140301\n",
      "    val_log_marginal: -12147.203887651558\n",
      "Train Epoch: 4910 [256/118836 (0%)] Loss: 12292.247070\n",
      "Train Epoch: 4910 [33024/118836 (28%)] Loss: 12244.917969\n",
      "Train Epoch: 4910 [65792/118836 (55%)] Loss: 12365.894531\n",
      "Train Epoch: 4910 [98560/118836 (83%)] Loss: 12182.104492\n",
      "    epoch          : 4910\n",
      "    loss           : 12218.92338225031\n",
      "    val_loss       : 12218.08884895135\n",
      "    val_log_likelihood: -12139.314775414858\n",
      "    val_log_marginal: -12147.211597299356\n",
      "Train Epoch: 4911 [256/118836 (0%)] Loss: 12210.432617\n",
      "Train Epoch: 4911 [33024/118836 (28%)] Loss: 12266.665039\n",
      "Train Epoch: 4911 [65792/118836 (55%)] Loss: 12151.261719\n",
      "Train Epoch: 4911 [98560/118836 (83%)] Loss: 12182.363281\n",
      "    epoch          : 4911\n",
      "    loss           : 12218.75467813017\n",
      "    val_loss       : 12217.047865343615\n",
      "    val_log_likelihood: -12140.713650712107\n",
      "    val_log_marginal: -12148.612105252118\n",
      "Train Epoch: 4912 [256/118836 (0%)] Loss: 12211.001953\n",
      "Train Epoch: 4912 [33024/118836 (28%)] Loss: 12192.857422\n",
      "Train Epoch: 4912 [65792/118836 (55%)] Loss: 12257.227539\n",
      "Train Epoch: 4912 [98560/118836 (83%)] Loss: 12260.548828\n",
      "    epoch          : 4912\n",
      "    loss           : 12219.360768519955\n",
      "    val_loss       : 12218.175977025525\n",
      "    val_log_likelihood: -12139.870198446546\n",
      "    val_log_marginal: -12147.759309362673\n",
      "Train Epoch: 4913 [256/118836 (0%)] Loss: 12218.116211\n",
      "Train Epoch: 4913 [33024/118836 (28%)] Loss: 12243.501953\n",
      "Train Epoch: 4913 [65792/118836 (55%)] Loss: 12248.207031\n",
      "Train Epoch: 4913 [98560/118836 (83%)] Loss: 12179.481445\n",
      "    epoch          : 4913\n",
      "    loss           : 12219.99264015974\n",
      "    val_loss       : 12216.874467810552\n",
      "    val_log_likelihood: -12139.5192621097\n",
      "    val_log_marginal: -12147.416650085946\n",
      "Train Epoch: 4914 [256/118836 (0%)] Loss: 12261.891602\n",
      "Train Epoch: 4914 [33024/118836 (28%)] Loss: 12297.035156\n",
      "Train Epoch: 4914 [65792/118836 (55%)] Loss: 12170.111328\n",
      "Train Epoch: 4914 [98560/118836 (83%)] Loss: 12213.387695\n",
      "    epoch          : 4914\n",
      "    loss           : 12213.068998946703\n",
      "    val_loss       : 12220.900279715752\n",
      "    val_log_likelihood: -12140.86106932382\n",
      "    val_log_marginal: -12148.757791017126\n",
      "Train Epoch: 4915 [256/118836 (0%)] Loss: 12267.757812\n",
      "Train Epoch: 4915 [33024/118836 (28%)] Loss: 12179.329102\n",
      "Train Epoch: 4915 [65792/118836 (55%)] Loss: 12159.507812\n",
      "Train Epoch: 4915 [98560/118836 (83%)] Loss: 12240.085938\n",
      "    epoch          : 4915\n",
      "    loss           : 12220.647590661187\n",
      "    val_loss       : 12220.592965669493\n",
      "    val_log_likelihood: -12141.109316519334\n",
      "    val_log_marginal: -12148.99720326063\n",
      "Train Epoch: 4916 [256/118836 (0%)] Loss: 12228.437500\n",
      "Train Epoch: 4916 [33024/118836 (28%)] Loss: 12256.179688\n",
      "Train Epoch: 4916 [65792/118836 (55%)] Loss: 12358.204102\n",
      "Train Epoch: 4916 [98560/118836 (83%)] Loss: 12244.650391\n",
      "    epoch          : 4916\n",
      "    loss           : 12218.036008096828\n",
      "    val_loss       : 12214.63136850226\n",
      "    val_log_likelihood: -12140.37247046888\n",
      "    val_log_marginal: -12148.264048200135\n",
      "Train Epoch: 4917 [256/118836 (0%)] Loss: 12239.673828\n",
      "Train Epoch: 4917 [33024/118836 (28%)] Loss: 12131.661133\n",
      "Train Epoch: 4917 [65792/118836 (55%)] Loss: 12289.721680\n",
      "Train Epoch: 4917 [98560/118836 (83%)] Loss: 12189.115234\n",
      "    epoch          : 4917\n",
      "    loss           : 12216.253771841399\n",
      "    val_loss       : 12217.510059155251\n",
      "    val_log_likelihood: -12139.581283279054\n",
      "    val_log_marginal: -12147.471746227988\n",
      "Train Epoch: 4918 [256/118836 (0%)] Loss: 12256.754883\n",
      "Train Epoch: 4918 [33024/118836 (28%)] Loss: 12176.351562\n",
      "Train Epoch: 4918 [65792/118836 (55%)] Loss: 12403.331055\n",
      "Train Epoch: 4918 [98560/118836 (83%)] Loss: 12207.537109\n",
      "    epoch          : 4918\n",
      "    loss           : 12224.270992297354\n",
      "    val_loss       : 12217.783096801666\n",
      "    val_log_likelihood: -12138.691262471568\n",
      "    val_log_marginal: -12146.580124486502\n",
      "Train Epoch: 4919 [256/118836 (0%)] Loss: 12329.826172\n",
      "Train Epoch: 4919 [33024/118836 (28%)] Loss: 12212.919922\n",
      "Train Epoch: 4919 [65792/118836 (55%)] Loss: 12339.107422\n",
      "Train Epoch: 4919 [98560/118836 (83%)] Loss: 12267.984375\n",
      "    epoch          : 4919\n",
      "    loss           : 12215.117762290633\n",
      "    val_loss       : 12217.721694149115\n",
      "    val_log_likelihood: -12139.652306432226\n",
      "    val_log_marginal: -12147.541163018921\n",
      "Train Epoch: 4920 [256/118836 (0%)] Loss: 12271.898438\n",
      "Train Epoch: 4920 [33024/118836 (28%)] Loss: 12245.558594\n",
      "Train Epoch: 4920 [65792/118836 (55%)] Loss: 12152.708984\n",
      "Train Epoch: 4920 [98560/118836 (83%)] Loss: 12180.857422\n",
      "    epoch          : 4920\n",
      "    loss           : 12221.22496235913\n",
      "    val_loss       : 12216.809089280425\n",
      "    val_log_likelihood: -12139.137617930624\n",
      "    val_log_marginal: -12147.02652333394\n",
      "Train Epoch: 4921 [256/118836 (0%)] Loss: 12270.370117\n",
      "Train Epoch: 4921 [33024/118836 (28%)] Loss: 12254.774414\n",
      "Train Epoch: 4921 [65792/118836 (55%)] Loss: 12212.491211\n",
      "Train Epoch: 4921 [98560/118836 (83%)] Loss: 12139.285156\n",
      "    epoch          : 4921\n",
      "    loss           : 12224.814147636218\n",
      "    val_loss       : 12220.769289319396\n",
      "    val_log_likelihood: -12139.794571314103\n",
      "    val_log_marginal: -12147.688786104323\n",
      "Train Epoch: 4922 [256/118836 (0%)] Loss: 12282.155273\n",
      "Train Epoch: 4922 [33024/118836 (28%)] Loss: 12178.261719\n",
      "Train Epoch: 4922 [65792/118836 (55%)] Loss: 12293.221680\n",
      "Train Epoch: 4922 [98560/118836 (83%)] Loss: 12187.173828\n",
      "    epoch          : 4922\n",
      "    loss           : 12219.13165677988\n",
      "    val_loss       : 12219.214738045077\n",
      "    val_log_likelihood: -12139.66082021557\n",
      "    val_log_marginal: -12147.555319541412\n",
      "Train Epoch: 4923 [256/118836 (0%)] Loss: 12219.262695\n",
      "Train Epoch: 4923 [33024/118836 (28%)] Loss: 12222.482422\n",
      "Train Epoch: 4923 [65792/118836 (55%)] Loss: 12179.077148\n",
      "Train Epoch: 4923 [98560/118836 (83%)] Loss: 12177.105469\n",
      "    epoch          : 4923\n",
      "    loss           : 12220.713683344964\n",
      "    val_loss       : 12219.4641982943\n",
      "    val_log_likelihood: -12140.437244268249\n",
      "    val_log_marginal: -12148.330175085877\n",
      "Train Epoch: 4924 [256/118836 (0%)] Loss: 12211.880859\n",
      "Train Epoch: 4924 [33024/118836 (28%)] Loss: 12178.842773\n",
      "Train Epoch: 4924 [65792/118836 (55%)] Loss: 12205.648438\n",
      "Train Epoch: 4924 [98560/118836 (83%)] Loss: 12143.899414\n",
      "    epoch          : 4924\n",
      "    loss           : 12218.815924511477\n",
      "    val_loss       : 12217.23700409411\n",
      "    val_log_likelihood: -12139.30302257806\n",
      "    val_log_marginal: -12147.191764398209\n",
      "Train Epoch: 4925 [256/118836 (0%)] Loss: 12217.830078\n",
      "Train Epoch: 4925 [33024/118836 (28%)] Loss: 12232.691406\n",
      "Train Epoch: 4925 [65792/118836 (55%)] Loss: 12285.156250\n",
      "Train Epoch: 4925 [98560/118836 (83%)] Loss: 12217.286133\n",
      "    epoch          : 4925\n",
      "    loss           : 12218.346501983819\n",
      "    val_loss       : 12217.464884861027\n",
      "    val_log_likelihood: -12139.64690327104\n",
      "    val_log_marginal: -12147.538630463829\n",
      "Train Epoch: 4926 [256/118836 (0%)] Loss: 12233.067383\n",
      "Train Epoch: 4926 [33024/118836 (28%)] Loss: 12304.122070\n",
      "Train Epoch: 4926 [65792/118836 (55%)] Loss: 12242.150391\n",
      "Train Epoch: 4926 [98560/118836 (83%)] Loss: 12322.879883\n",
      "    epoch          : 4926\n",
      "    loss           : 12220.577620644644\n",
      "    val_loss       : 12215.060901170313\n",
      "    val_log_likelihood: -12139.28287437319\n",
      "    val_log_marginal: -12147.180634355027\n",
      "Train Epoch: 4927 [256/118836 (0%)] Loss: 12203.125000\n",
      "Train Epoch: 4927 [33024/118836 (28%)] Loss: 12172.250977\n",
      "Train Epoch: 4927 [65792/118836 (55%)] Loss: 12275.901367\n",
      "Train Epoch: 4927 [98560/118836 (83%)] Loss: 12311.464844\n",
      "    epoch          : 4927\n",
      "    loss           : 12218.942074900486\n",
      "    val_loss       : 12222.64684830637\n",
      "    val_log_likelihood: -12139.003309811827\n",
      "    val_log_marginal: -12146.887374536842\n",
      "Train Epoch: 4928 [256/118836 (0%)] Loss: 12130.386719\n",
      "Train Epoch: 4928 [33024/118836 (28%)] Loss: 12199.395508\n",
      "Train Epoch: 4928 [65792/118836 (55%)] Loss: 12148.673828\n",
      "Train Epoch: 4928 [98560/118836 (83%)] Loss: 12313.611328\n",
      "    epoch          : 4928\n",
      "    loss           : 12219.278946152554\n",
      "    val_loss       : 12215.382042019204\n",
      "    val_log_likelihood: -12138.089374450734\n",
      "    val_log_marginal: -12145.986734023647\n",
      "Train Epoch: 4929 [256/118836 (0%)] Loss: 12197.343750\n",
      "Train Epoch: 4929 [33024/118836 (28%)] Loss: 12219.137695\n",
      "Train Epoch: 4929 [65792/118836 (55%)] Loss: 12202.417969\n",
      "Train Epoch: 4929 [98560/118836 (83%)] Loss: 12304.265625\n",
      "    epoch          : 4929\n",
      "    loss           : 12218.786378205128\n",
      "    val_loss       : 12219.79909608811\n",
      "    val_log_likelihood: -12139.623651390613\n",
      "    val_log_marginal: -12147.524069247149\n",
      "Train Epoch: 4930 [256/118836 (0%)] Loss: 12214.961914\n",
      "Train Epoch: 4930 [33024/118836 (28%)] Loss: 12260.558594\n",
      "Train Epoch: 4930 [65792/118836 (55%)] Loss: 12166.273438\n",
      "Train Epoch: 4930 [98560/118836 (83%)] Loss: 12163.681641\n",
      "    epoch          : 4930\n",
      "    loss           : 12216.925346199081\n",
      "    val_loss       : 12218.842452230276\n",
      "    val_log_likelihood: -12138.914309023468\n",
      "    val_log_marginal: -12146.81034006796\n",
      "Train Epoch: 4931 [256/118836 (0%)] Loss: 12189.178711\n",
      "Train Epoch: 4931 [33024/118836 (28%)] Loss: 12309.525391\n",
      "Train Epoch: 4931 [65792/118836 (55%)] Loss: 12227.732422\n",
      "Train Epoch: 4931 [98560/118836 (83%)] Loss: 12167.203125\n",
      "    epoch          : 4931\n",
      "    loss           : 12215.143489744883\n",
      "    val_loss       : 12218.788287558571\n",
      "    val_log_likelihood: -12139.943566319014\n",
      "    val_log_marginal: -12147.840095761774\n",
      "Train Epoch: 4932 [256/118836 (0%)] Loss: 12254.482422\n",
      "Train Epoch: 4932 [33024/118836 (28%)] Loss: 12313.884766\n",
      "Train Epoch: 4932 [65792/118836 (55%)] Loss: 12205.962891\n",
      "Train Epoch: 4932 [98560/118836 (83%)] Loss: 12311.322266\n",
      "    epoch          : 4932\n",
      "    loss           : 12220.903319666304\n",
      "    val_loss       : 12220.849998293199\n",
      "    val_log_likelihood: -12140.902979282982\n",
      "    val_log_marginal: -12148.79586639496\n",
      "Train Epoch: 4933 [256/118836 (0%)] Loss: 12348.094727\n",
      "Train Epoch: 4933 [33024/118836 (28%)] Loss: 12182.172852\n",
      "Train Epoch: 4933 [65792/118836 (55%)] Loss: 12140.767578\n",
      "Train Epoch: 4933 [98560/118836 (83%)] Loss: 12304.499023\n",
      "    epoch          : 4933\n",
      "    loss           : 12216.689778484284\n",
      "    val_loss       : 12216.85455985141\n",
      "    val_log_likelihood: -12139.632510888388\n",
      "    val_log_marginal: -12147.522469538266\n",
      "Train Epoch: 4934 [256/118836 (0%)] Loss: 12197.033203\n",
      "Train Epoch: 4934 [33024/118836 (28%)] Loss: 12219.279297\n",
      "Train Epoch: 4934 [65792/118836 (55%)] Loss: 12232.300781\n",
      "Train Epoch: 4934 [98560/118836 (83%)] Loss: 12238.192383\n",
      "    epoch          : 4934\n",
      "    loss           : 12220.096167254704\n",
      "    val_loss       : 12216.836187300814\n",
      "    val_log_likelihood: -12139.266530804125\n",
      "    val_log_marginal: -12147.162322448528\n",
      "Train Epoch: 4935 [256/118836 (0%)] Loss: 12228.664062\n",
      "Train Epoch: 4935 [33024/118836 (28%)] Loss: 12150.307617\n",
      "Train Epoch: 4935 [65792/118836 (55%)] Loss: 12152.849609\n",
      "Train Epoch: 4935 [98560/118836 (83%)] Loss: 12205.753906\n",
      "    epoch          : 4935\n",
      "    loss           : 12219.456698297921\n",
      "    val_loss       : 12220.032827484145\n",
      "    val_log_likelihood: -12140.138065743899\n",
      "    val_log_marginal: -12148.036218516807\n",
      "Train Epoch: 4936 [256/118836 (0%)] Loss: 12160.576172\n",
      "Train Epoch: 4936 [33024/118836 (28%)] Loss: 12214.276367\n",
      "Train Epoch: 4936 [65792/118836 (55%)] Loss: 12205.873047\n",
      "Train Epoch: 4936 [98560/118836 (83%)] Loss: 12404.960938\n",
      "    epoch          : 4936\n",
      "    loss           : 12217.502831142732\n",
      "    val_loss       : 12215.716684191502\n",
      "    val_log_likelihood: -12139.694810244779\n",
      "    val_log_marginal: -12147.587457359157\n",
      "Train Epoch: 4937 [256/118836 (0%)] Loss: 12317.042969\n",
      "Train Epoch: 4937 [33024/118836 (28%)] Loss: 12288.394531\n",
      "Train Epoch: 4937 [65792/118836 (55%)] Loss: 12307.056641\n",
      "Train Epoch: 4937 [98560/118836 (83%)] Loss: 12255.912109\n",
      "    epoch          : 4937\n",
      "    loss           : 12212.638969447891\n",
      "    val_loss       : 12220.717513509375\n",
      "    val_log_likelihood: -12138.072659804073\n",
      "    val_log_marginal: -12145.96608758024\n",
      "Train Epoch: 4938 [256/118836 (0%)] Loss: 12160.976562\n",
      "Train Epoch: 4938 [33024/118836 (28%)] Loss: 12184.076172\n",
      "Train Epoch: 4938 [65792/118836 (55%)] Loss: 12169.425781\n",
      "Train Epoch: 4938 [98560/118836 (83%)] Loss: 12311.367188\n",
      "    epoch          : 4938\n",
      "    loss           : 12218.105294277295\n",
      "    val_loss       : 12212.416499764602\n",
      "    val_log_likelihood: -12139.660288558467\n",
      "    val_log_marginal: -12147.555073631767\n",
      "Train Epoch: 4939 [256/118836 (0%)] Loss: 12212.189453\n",
      "Train Epoch: 4939 [33024/118836 (28%)] Loss: 12307.628906\n",
      "Train Epoch: 4939 [65792/118836 (55%)] Loss: 12245.256836\n",
      "Train Epoch: 4939 [98560/118836 (83%)] Loss: 12222.249023\n",
      "    epoch          : 4939\n",
      "    loss           : 12213.969467599773\n",
      "    val_loss       : 12214.280588670847\n",
      "    val_log_likelihood: -12138.272661258012\n",
      "    val_log_marginal: -12146.172252391558\n",
      "Train Epoch: 4940 [256/118836 (0%)] Loss: 12323.542969\n",
      "Train Epoch: 4940 [33024/118836 (28%)] Loss: 12309.575195\n",
      "Train Epoch: 4940 [65792/118836 (55%)] Loss: 12359.030273\n",
      "Train Epoch: 4940 [98560/118836 (83%)] Loss: 12188.799805\n",
      "    epoch          : 4940\n",
      "    loss           : 12218.287004852926\n",
      "    val_loss       : 12216.9185700876\n",
      "    val_log_likelihood: -12140.424959128153\n",
      "    val_log_marginal: -12148.32023629912\n",
      "Train Epoch: 4941 [256/118836 (0%)] Loss: 12171.520508\n",
      "Train Epoch: 4941 [33024/118836 (28%)] Loss: 12161.298828\n",
      "Train Epoch: 4941 [65792/118836 (55%)] Loss: 12252.287109\n",
      "Train Epoch: 4941 [98560/118836 (83%)] Loss: 12133.269531\n",
      "    epoch          : 4941\n",
      "    loss           : 12211.697531049678\n",
      "    val_loss       : 12217.806766453412\n",
      "    val_log_likelihood: -12140.284589698356\n",
      "    val_log_marginal: -12148.178019883817\n",
      "Train Epoch: 4942 [256/118836 (0%)] Loss: 12276.734375\n",
      "Train Epoch: 4942 [33024/118836 (28%)] Loss: 12232.859375\n",
      "Train Epoch: 4942 [65792/118836 (55%)] Loss: 12216.110352\n",
      "Train Epoch: 4942 [98560/118836 (83%)] Loss: 12192.260742\n",
      "    epoch          : 4942\n",
      "    loss           : 12217.959124114714\n",
      "    val_loss       : 12218.825035315716\n",
      "    val_log_likelihood: -12140.834516032102\n",
      "    val_log_marginal: -12148.73134288541\n",
      "Train Epoch: 4943 [256/118836 (0%)] Loss: 12314.977539\n",
      "Train Epoch: 4943 [33024/118836 (28%)] Loss: 12240.833984\n",
      "Train Epoch: 4943 [65792/118836 (55%)] Loss: 12308.767578\n",
      "Train Epoch: 4943 [98560/118836 (83%)] Loss: 12165.000000\n",
      "    epoch          : 4943\n",
      "    loss           : 12218.943778594139\n",
      "    val_loss       : 12217.10005344179\n",
      "    val_log_likelihood: -12139.126141665376\n",
      "    val_log_marginal: -12147.022208771179\n",
      "Train Epoch: 4944 [256/118836 (0%)] Loss: 12222.425781\n",
      "Train Epoch: 4944 [33024/118836 (28%)] Loss: 12245.173828\n",
      "Train Epoch: 4944 [65792/118836 (55%)] Loss: 12273.749023\n",
      "Train Epoch: 4944 [98560/118836 (83%)] Loss: 12365.369141\n",
      "    epoch          : 4944\n",
      "    loss           : 12216.763739563947\n",
      "    val_loss       : 12216.371380234976\n",
      "    val_log_likelihood: -12139.645788261218\n",
      "    val_log_marginal: -12147.539668001475\n",
      "Train Epoch: 4945 [256/118836 (0%)] Loss: 12160.540039\n",
      "Train Epoch: 4945 [33024/118836 (28%)] Loss: 12256.310547\n",
      "Train Epoch: 4945 [65792/118836 (55%)] Loss: 12265.505859\n",
      "Train Epoch: 4945 [98560/118836 (83%)] Loss: 12272.683594\n",
      "    epoch          : 4945\n",
      "    loss           : 12220.667884583074\n",
      "    val_loss       : 12215.400318852839\n",
      "    val_log_likelihood: -12138.692161329094\n",
      "    val_log_marginal: -12146.583669147622\n",
      "Train Epoch: 4946 [256/118836 (0%)] Loss: 12280.020508\n",
      "Train Epoch: 4946 [33024/118836 (28%)] Loss: 12175.480469\n",
      "Train Epoch: 4946 [65792/118836 (55%)] Loss: 12255.982422\n",
      "Train Epoch: 4946 [98560/118836 (83%)] Loss: 12166.987305\n",
      "    epoch          : 4946\n",
      "    loss           : 12217.407010410205\n",
      "    val_loss       : 12218.370756986813\n",
      "    val_log_likelihood: -12138.893978591554\n",
      "    val_log_marginal: -12146.78367288125\n",
      "Train Epoch: 4947 [256/118836 (0%)] Loss: 12351.710938\n",
      "Train Epoch: 4947 [33024/118836 (28%)] Loss: 12266.178711\n",
      "Train Epoch: 4947 [65792/118836 (55%)] Loss: 12171.485352\n",
      "Train Epoch: 4947 [98560/118836 (83%)] Loss: 12181.589844\n",
      "    epoch          : 4947\n",
      "    loss           : 12218.362484491316\n",
      "    val_loss       : 12220.438568123704\n",
      "    val_log_likelihood: -12139.10452740514\n",
      "    val_log_marginal: -12146.997462779102\n",
      "Train Epoch: 4948 [256/118836 (0%)] Loss: 12302.000000\n",
      "Train Epoch: 4948 [33024/118836 (28%)] Loss: 12194.261719\n",
      "Train Epoch: 4948 [65792/118836 (55%)] Loss: 12327.866211\n",
      "Train Epoch: 4948 [98560/118836 (83%)] Loss: 12329.196289\n",
      "    epoch          : 4948\n",
      "    loss           : 12216.892696055624\n",
      "    val_loss       : 12214.356000181353\n",
      "    val_log_likelihood: -12139.696581465829\n",
      "    val_log_marginal: -12147.587876148586\n",
      "Train Epoch: 4949 [256/118836 (0%)] Loss: 12295.305664\n",
      "Train Epoch: 4949 [33024/118836 (28%)] Loss: 12278.741211\n",
      "Train Epoch: 4949 [65792/118836 (55%)] Loss: 12197.493164\n",
      "Train Epoch: 4949 [98560/118836 (83%)] Loss: 12204.751953\n",
      "    epoch          : 4949\n",
      "    loss           : 12220.50872993564\n",
      "    val_loss       : 12217.221830129161\n",
      "    val_log_likelihood: -12138.904545337056\n",
      "    val_log_marginal: -12146.802094597513\n",
      "Train Epoch: 4950 [256/118836 (0%)] Loss: 12154.031250\n",
      "Train Epoch: 4950 [33024/118836 (28%)] Loss: 12177.830078\n",
      "Train Epoch: 4950 [65792/118836 (55%)] Loss: 12274.172852\n",
      "Train Epoch: 4950 [98560/118836 (83%)] Loss: 12212.076172\n",
      "    epoch          : 4950\n",
      "    loss           : 12222.431323343155\n",
      "    val_loss       : 12218.12385948219\n",
      "    val_log_likelihood: -12140.138710162066\n",
      "    val_log_marginal: -12148.02981176962\n",
      "Train Epoch: 4951 [256/118836 (0%)] Loss: 12234.612305\n",
      "Train Epoch: 4951 [33024/118836 (28%)] Loss: 12181.349609\n",
      "Train Epoch: 4951 [65792/118836 (55%)] Loss: 12208.166992\n",
      "Train Epoch: 4951 [98560/118836 (83%)] Loss: 12326.626953\n",
      "    epoch          : 4951\n",
      "    loss           : 12210.427467011734\n",
      "    val_loss       : 12221.100969621692\n",
      "    val_log_likelihood: -12140.243577627429\n",
      "    val_log_marginal: -12148.133770407374\n",
      "Train Epoch: 4952 [256/118836 (0%)] Loss: 12301.272461\n",
      "Train Epoch: 4952 [33024/118836 (28%)] Loss: 12192.143555\n",
      "Train Epoch: 4952 [65792/118836 (55%)] Loss: 12172.675781\n",
      "Train Epoch: 4952 [98560/118836 (83%)] Loss: 12163.262695\n",
      "    epoch          : 4952\n",
      "    loss           : 12217.160710200838\n",
      "    val_loss       : 12215.759400840354\n",
      "    val_log_likelihood: -12138.562426333747\n",
      "    val_log_marginal: -12146.457260963658\n",
      "Train Epoch: 4953 [256/118836 (0%)] Loss: 12257.178711\n",
      "Train Epoch: 4953 [33024/118836 (28%)] Loss: 12280.870117\n",
      "Train Epoch: 4953 [65792/118836 (55%)] Loss: 12154.013672\n",
      "Train Epoch: 4953 [98560/118836 (83%)] Loss: 12271.458984\n",
      "    epoch          : 4953\n",
      "    loss           : 12222.189471218466\n",
      "    val_loss       : 12219.07672935603\n",
      "    val_log_likelihood: -12139.596992607527\n",
      "    val_log_marginal: -12147.49441810451\n",
      "Train Epoch: 4954 [256/118836 (0%)] Loss: 12219.592773\n",
      "Train Epoch: 4954 [33024/118836 (28%)] Loss: 12268.346680\n",
      "Train Epoch: 4954 [65792/118836 (55%)] Loss: 12275.041016\n",
      "Train Epoch: 4954 [98560/118836 (83%)] Loss: 12254.916992\n",
      "    epoch          : 4954\n",
      "    loss           : 12215.257895697634\n",
      "    val_loss       : 12217.657302613927\n",
      "    val_log_likelihood: -12140.312119229477\n",
      "    val_log_marginal: -12148.204527405165\n",
      "Train Epoch: 4955 [256/118836 (0%)] Loss: 12136.915039\n",
      "Train Epoch: 4955 [33024/118836 (28%)] Loss: 12286.615234\n",
      "Train Epoch: 4955 [65792/118836 (55%)] Loss: 12338.856445\n",
      "Train Epoch: 4955 [98560/118836 (83%)] Loss: 12190.169922\n",
      "    epoch          : 4955\n",
      "    loss           : 12218.932147565136\n",
      "    val_loss       : 12217.671589051111\n",
      "    val_log_likelihood: -12140.357835763029\n",
      "    val_log_marginal: -12148.2538899106\n",
      "Train Epoch: 4956 [256/118836 (0%)] Loss: 12255.169922\n",
      "Train Epoch: 4956 [33024/118836 (28%)] Loss: 12171.511719\n",
      "Train Epoch: 4956 [65792/118836 (55%)] Loss: 12219.208984\n",
      "Train Epoch: 4956 [98560/118836 (83%)] Loss: 12223.310547\n",
      "    epoch          : 4956\n",
      "    loss           : 12216.89656240307\n",
      "    val_loss       : 12218.73957268156\n",
      "    val_log_likelihood: -12139.155546131877\n",
      "    val_log_marginal: -12147.048662663034\n",
      "Train Epoch: 4957 [256/118836 (0%)] Loss: 12176.011719\n",
      "Train Epoch: 4957 [33024/118836 (28%)] Loss: 12271.396484\n",
      "Train Epoch: 4957 [65792/118836 (55%)] Loss: 12217.741211\n",
      "Train Epoch: 4957 [98560/118836 (83%)] Loss: 12296.188477\n",
      "    epoch          : 4957\n",
      "    loss           : 12217.76432113963\n",
      "    val_loss       : 12217.8004588635\n",
      "    val_log_likelihood: -12139.622845100548\n",
      "    val_log_marginal: -12147.518526581962\n",
      "Train Epoch: 4958 [256/118836 (0%)] Loss: 12253.194336\n",
      "Train Epoch: 4958 [33024/118836 (28%)] Loss: 12306.619141\n",
      "Train Epoch: 4958 [65792/118836 (55%)] Loss: 12240.259766\n",
      "Train Epoch: 4958 [98560/118836 (83%)] Loss: 12274.622070\n",
      "    epoch          : 4958\n",
      "    loss           : 12216.094270671783\n",
      "    val_loss       : 12219.26918603347\n",
      "    val_log_likelihood: -12138.57302135029\n",
      "    val_log_marginal: -12146.46654005805\n",
      "Train Epoch: 4959 [256/118836 (0%)] Loss: 12219.722656\n",
      "Train Epoch: 4959 [33024/118836 (28%)] Loss: 12199.470703\n",
      "Train Epoch: 4959 [65792/118836 (55%)] Loss: 12281.739258\n",
      "Train Epoch: 4959 [98560/118836 (83%)] Loss: 12339.210938\n",
      "    epoch          : 4959\n",
      "    loss           : 12219.692875374794\n",
      "    val_loss       : 12215.456246325328\n",
      "    val_log_likelihood: -12139.904402527916\n",
      "    val_log_marginal: -12147.794739248604\n",
      "Train Epoch: 4960 [256/118836 (0%)] Loss: 12178.572266\n",
      "Train Epoch: 4960 [33024/118836 (28%)] Loss: 12198.644531\n",
      "Train Epoch: 4960 [65792/118836 (55%)] Loss: 12294.935547\n",
      "Train Epoch: 4960 [98560/118836 (83%)] Loss: 12228.098633\n",
      "    epoch          : 4960\n",
      "    loss           : 12220.954310122002\n",
      "    val_loss       : 12221.072919147886\n",
      "    val_log_likelihood: -12138.72773712133\n",
      "    val_log_marginal: -12146.619008526084\n",
      "Train Epoch: 4961 [256/118836 (0%)] Loss: 12268.856445\n",
      "Train Epoch: 4961 [33024/118836 (28%)] Loss: 12324.996094\n",
      "Train Epoch: 4961 [65792/118836 (55%)] Loss: 12168.793945\n",
      "Train Epoch: 4961 [98560/118836 (83%)] Loss: 12223.527344\n",
      "    epoch          : 4961\n",
      "    loss           : 12217.990928711746\n",
      "    val_loss       : 12216.000428459487\n",
      "    val_log_likelihood: -12139.518708481957\n",
      "    val_log_marginal: -12147.414980245429\n",
      "Train Epoch: 4962 [256/118836 (0%)] Loss: 12166.373047\n",
      "Train Epoch: 4962 [33024/118836 (28%)] Loss: 12172.111328\n",
      "Train Epoch: 4962 [65792/118836 (55%)] Loss: 12232.212891\n",
      "Train Epoch: 4962 [98560/118836 (83%)] Loss: 12266.349609\n",
      "    epoch          : 4962\n",
      "    loss           : 12218.342727234543\n",
      "    val_loss       : 12213.28396508537\n",
      "    val_log_likelihood: -12139.351019857579\n",
      "    val_log_marginal: -12147.24725765201\n",
      "Train Epoch: 4963 [256/118836 (0%)] Loss: 12181.240234\n",
      "Train Epoch: 4963 [33024/118836 (28%)] Loss: 12282.359375\n",
      "Train Epoch: 4963 [65792/118836 (55%)] Loss: 12216.554688\n",
      "Train Epoch: 4963 [98560/118836 (83%)] Loss: 12269.309570\n",
      "    epoch          : 4963\n",
      "    loss           : 12219.50723867866\n",
      "    val_loss       : 12214.443392664569\n",
      "    val_log_likelihood: -12141.394921551902\n",
      "    val_log_marginal: -12149.284620883278\n",
      "Train Epoch: 4964 [256/118836 (0%)] Loss: 12222.105469\n",
      "Train Epoch: 4964 [33024/118836 (28%)] Loss: 12209.072266\n",
      "Train Epoch: 4964 [65792/118836 (55%)] Loss: 12194.567383\n",
      "Train Epoch: 4964 [98560/118836 (83%)] Loss: 12206.170898\n",
      "    epoch          : 4964\n",
      "    loss           : 12219.050206782464\n",
      "    val_loss       : 12222.287247533786\n",
      "    val_log_likelihood: -12140.047480969552\n",
      "    val_log_marginal: -12147.940790783496\n",
      "Train Epoch: 4965 [256/118836 (0%)] Loss: 12159.478516\n",
      "Train Epoch: 4965 [33024/118836 (28%)] Loss: 12214.900391\n",
      "Train Epoch: 4965 [65792/118836 (55%)] Loss: 12211.248047\n",
      "Train Epoch: 4965 [98560/118836 (83%)] Loss: 12246.427734\n",
      "    epoch          : 4965\n",
      "    loss           : 12218.166176204508\n",
      "    val_loss       : 12217.169900095012\n",
      "    val_log_likelihood: -12139.424876576717\n",
      "    val_log_marginal: -12147.319801859985\n",
      "Train Epoch: 4966 [256/118836 (0%)] Loss: 12143.921875\n",
      "Train Epoch: 4966 [33024/118836 (28%)] Loss: 12261.419922\n",
      "Train Epoch: 4966 [65792/118836 (55%)] Loss: 12223.698242\n",
      "Train Epoch: 4966 [98560/118836 (83%)] Loss: 12238.325195\n",
      "    epoch          : 4966\n",
      "    loss           : 12218.596093265354\n",
      "    val_loss       : 12220.698167779145\n",
      "    val_log_likelihood: -12140.667169729631\n",
      "    val_log_marginal: -12148.557897749566\n",
      "Train Epoch: 4967 [256/118836 (0%)] Loss: 12253.809570\n",
      "Train Epoch: 4967 [33024/118836 (28%)] Loss: 12198.010742\n",
      "Train Epoch: 4967 [65792/118836 (55%)] Loss: 12197.561523\n",
      "Train Epoch: 4967 [98560/118836 (83%)] Loss: 12313.489258\n",
      "    epoch          : 4967\n",
      "    loss           : 12216.792320131564\n",
      "    val_loss       : 12215.805065257964\n",
      "    val_log_likelihood: -12139.737445234956\n",
      "    val_log_marginal: -12147.632176062087\n",
      "Train Epoch: 4968 [256/118836 (0%)] Loss: 12194.230469\n",
      "Train Epoch: 4968 [33024/118836 (28%)] Loss: 12192.484375\n",
      "Train Epoch: 4968 [65792/118836 (55%)] Loss: 12314.984375\n",
      "Train Epoch: 4968 [98560/118836 (83%)] Loss: 12180.870117\n",
      "    epoch          : 4968\n",
      "    loss           : 12219.633024452025\n",
      "    val_loss       : 12219.563608187085\n",
      "    val_log_likelihood: -12136.679311252845\n",
      "    val_log_marginal: -12144.575294343502\n",
      "Train Epoch: 4969 [256/118836 (0%)] Loss: 12214.152344\n",
      "Train Epoch: 4969 [33024/118836 (28%)] Loss: 12242.228516\n",
      "Train Epoch: 4969 [65792/118836 (55%)] Loss: 12216.722656\n",
      "Train Epoch: 4969 [98560/118836 (83%)] Loss: 12137.425781\n",
      "    epoch          : 4969\n",
      "    loss           : 12219.997115546164\n",
      "    val_loss       : 12214.56495762537\n",
      "    val_log_likelihood: -12141.19567226918\n",
      "    val_log_marginal: -12149.085833288515\n",
      "Train Epoch: 4970 [256/118836 (0%)] Loss: 12258.736328\n",
      "Train Epoch: 4970 [33024/118836 (28%)] Loss: 12213.560547\n",
      "Train Epoch: 4970 [65792/118836 (55%)] Loss: 12216.414062\n",
      "Train Epoch: 4970 [98560/118836 (83%)] Loss: 12207.508789\n",
      "    epoch          : 4970\n",
      "    loss           : 12217.1701645213\n",
      "    val_loss       : 12218.249522819342\n",
      "    val_log_likelihood: -12139.144435774659\n",
      "    val_log_marginal: -12147.046307147248\n",
      "Train Epoch: 4971 [256/118836 (0%)] Loss: 12225.178711\n",
      "Train Epoch: 4971 [33024/118836 (28%)] Loss: 12225.231445\n",
      "Train Epoch: 4971 [65792/118836 (55%)] Loss: 12215.744141\n",
      "Train Epoch: 4971 [98560/118836 (83%)] Loss: 12170.737305\n",
      "    epoch          : 4971\n",
      "    loss           : 12219.765746969346\n",
      "    val_loss       : 12216.830537154066\n",
      "    val_log_likelihood: -12140.934807304591\n",
      "    val_log_marginal: -12148.826816961775\n",
      "Train Epoch: 4972 [256/118836 (0%)] Loss: 12208.390625\n",
      "Train Epoch: 4972 [33024/118836 (28%)] Loss: 12212.784180\n",
      "Train Epoch: 4972 [65792/118836 (55%)] Loss: 12231.197266\n",
      "Train Epoch: 4972 [98560/118836 (83%)] Loss: 12168.923828\n",
      "    epoch          : 4972\n",
      "    loss           : 12216.056412356545\n",
      "    val_loss       : 12215.178007258413\n",
      "    val_log_likelihood: -12141.208787770109\n",
      "    val_log_marginal: -12149.100604599207\n",
      "Train Epoch: 4973 [256/118836 (0%)] Loss: 12124.958008\n",
      "Train Epoch: 4973 [33024/118836 (28%)] Loss: 12195.728516\n",
      "Train Epoch: 4973 [65792/118836 (55%)] Loss: 12275.691406\n",
      "Train Epoch: 4973 [98560/118836 (83%)] Loss: 12219.601562\n",
      "    epoch          : 4973\n",
      "    loss           : 12218.046750122778\n",
      "    val_loss       : 12219.218703550852\n",
      "    val_log_likelihood: -12140.475071727667\n",
      "    val_log_marginal: -12148.37080185718\n",
      "Train Epoch: 4974 [256/118836 (0%)] Loss: 12212.525391\n",
      "Train Epoch: 4974 [33024/118836 (28%)] Loss: 12278.695312\n",
      "Train Epoch: 4974 [65792/118836 (55%)] Loss: 12272.181641\n",
      "Train Epoch: 4974 [98560/118836 (83%)] Loss: 12241.605469\n",
      "    epoch          : 4974\n",
      "    loss           : 12219.727386237335\n",
      "    val_loss       : 12217.477686324193\n",
      "    val_log_likelihood: -12140.21561821495\n",
      "    val_log_marginal: -12148.110151772868\n",
      "Train Epoch: 4975 [256/118836 (0%)] Loss: 12153.548828\n",
      "Train Epoch: 4975 [33024/118836 (28%)] Loss: 12120.228516\n",
      "Train Epoch: 4975 [65792/118836 (55%)] Loss: 12171.980469\n",
      "Train Epoch: 4975 [98560/118836 (83%)] Loss: 12207.660156\n",
      "    epoch          : 4975\n",
      "    loss           : 12218.743039346828\n",
      "    val_loss       : 12215.651874673315\n",
      "    val_log_likelihood: -12139.069858224771\n",
      "    val_log_marginal: -12146.966251390406\n",
      "Train Epoch: 4976 [256/118836 (0%)] Loss: 12289.100586\n",
      "Train Epoch: 4976 [33024/118836 (28%)] Loss: 12204.166016\n",
      "Train Epoch: 4976 [65792/118836 (55%)] Loss: 12210.431641\n",
      "Train Epoch: 4976 [98560/118836 (83%)] Loss: 12286.595703\n",
      "    epoch          : 4976\n",
      "    loss           : 12223.146063863473\n",
      "    val_loss       : 12218.727597540774\n",
      "    val_log_likelihood: -12141.097282426075\n",
      "    val_log_marginal: -12148.99280555532\n",
      "Train Epoch: 4977 [256/118836 (0%)] Loss: 12370.140625\n",
      "Train Epoch: 4977 [33024/118836 (28%)] Loss: 12228.792969\n",
      "Train Epoch: 4977 [65792/118836 (55%)] Loss: 12161.445312\n",
      "Train Epoch: 4977 [98560/118836 (83%)] Loss: 12203.681641\n",
      "    epoch          : 4977\n",
      "    loss           : 12217.494366147124\n",
      "    val_loss       : 12217.415114735175\n",
      "    val_log_likelihood: -12138.86349821004\n",
      "    val_log_marginal: -12146.758522322454\n",
      "Train Epoch: 4978 [256/118836 (0%)] Loss: 12295.476562\n",
      "Train Epoch: 4978 [33024/118836 (28%)] Loss: 12174.055664\n",
      "Train Epoch: 4978 [65792/118836 (55%)] Loss: 12241.875000\n",
      "Train Epoch: 4978 [98560/118836 (83%)] Loss: 12269.341797\n",
      "    epoch          : 4978\n",
      "    loss           : 12220.82246901494\n",
      "    val_loss       : 12213.856860380212\n",
      "    val_log_likelihood: -12138.917334186312\n",
      "    val_log_marginal: -12146.810714450388\n",
      "Train Epoch: 4979 [256/118836 (0%)] Loss: 12385.668945\n",
      "Train Epoch: 4979 [33024/118836 (28%)] Loss: 12140.711914\n",
      "Train Epoch: 4979 [65792/118836 (55%)] Loss: 12191.051758\n",
      "Train Epoch: 4979 [98560/118836 (83%)] Loss: 12216.503906\n",
      "    epoch          : 4979\n",
      "    loss           : 12220.12493828836\n",
      "    val_loss       : 12216.513361697127\n",
      "    val_log_likelihood: -12139.528700759925\n",
      "    val_log_marginal: -12147.429041184392\n",
      "Train Epoch: 4980 [256/118836 (0%)] Loss: 12232.682617\n",
      "Train Epoch: 4980 [33024/118836 (28%)] Loss: 12360.083984\n",
      "Train Epoch: 4980 [65792/118836 (55%)] Loss: 12277.623047\n",
      "Train Epoch: 4980 [98560/118836 (83%)] Loss: 12271.073242\n",
      "    epoch          : 4980\n",
      "    loss           : 12216.53031107837\n",
      "    val_loss       : 12216.850072832778\n",
      "    val_log_likelihood: -12138.366402372829\n",
      "    val_log_marginal: -12146.265596049896\n",
      "Train Epoch: 4981 [256/118836 (0%)] Loss: 12308.609375\n",
      "Train Epoch: 4981 [33024/118836 (28%)] Loss: 12224.754883\n",
      "Train Epoch: 4981 [65792/118836 (55%)] Loss: 12313.193359\n",
      "Train Epoch: 4981 [98560/118836 (83%)] Loss: 12251.166016\n",
      "    epoch          : 4981\n",
      "    loss           : 12217.152676217433\n",
      "    val_loss       : 12215.284449117655\n",
      "    val_log_likelihood: -12140.065135184037\n",
      "    val_log_marginal: -12147.963521493864\n",
      "Train Epoch: 4982 [256/118836 (0%)] Loss: 12291.026367\n",
      "Train Epoch: 4982 [33024/118836 (28%)] Loss: 12281.030273\n",
      "Train Epoch: 4982 [65792/118836 (55%)] Loss: 12189.166016\n",
      "Train Epoch: 4982 [98560/118836 (83%)] Loss: 12247.988281\n",
      "    epoch          : 4982\n",
      "    loss           : 12217.192391051489\n",
      "    val_loss       : 12217.40067539127\n",
      "    val_log_likelihood: -12139.886857358872\n",
      "    val_log_marginal: -12147.77702044353\n",
      "Train Epoch: 4983 [256/118836 (0%)] Loss: 12253.336914\n",
      "Train Epoch: 4983 [33024/118836 (28%)] Loss: 12344.200195\n",
      "Train Epoch: 4983 [65792/118836 (55%)] Loss: 12311.201172\n",
      "Train Epoch: 4983 [98560/118836 (83%)] Loss: 12235.204102\n",
      "    epoch          : 4983\n",
      "    loss           : 12220.256443212365\n",
      "    val_loss       : 12215.884424047448\n",
      "    val_log_likelihood: -12139.939531637716\n",
      "    val_log_marginal: -12147.829939587687\n",
      "Train Epoch: 4984 [256/118836 (0%)] Loss: 12281.572266\n",
      "Train Epoch: 4984 [33024/118836 (28%)] Loss: 12324.501953\n",
      "Train Epoch: 4984 [65792/118836 (55%)] Loss: 12182.728516\n",
      "Train Epoch: 4984 [98560/118836 (83%)] Loss: 12232.577148\n",
      "    epoch          : 4984\n",
      "    loss           : 12217.831110098738\n",
      "    val_loss       : 12217.458582880721\n",
      "    val_log_likelihood: -12140.129508180833\n",
      "    val_log_marginal: -12148.029872819114\n",
      "Train Epoch: 4985 [256/118836 (0%)] Loss: 12175.208984\n",
      "Train Epoch: 4985 [33024/118836 (28%)] Loss: 12215.119141\n",
      "Train Epoch: 4985 [65792/118836 (55%)] Loss: 12177.202148\n",
      "Train Epoch: 4985 [98560/118836 (83%)] Loss: 12165.271484\n",
      "    epoch          : 4985\n",
      "    loss           : 12220.065597375156\n",
      "    val_loss       : 12216.428072261964\n",
      "    val_log_likelihood: -12139.590857468724\n",
      "    val_log_marginal: -12147.481568939209\n",
      "Train Epoch: 4986 [256/118836 (0%)] Loss: 12248.229492\n",
      "Train Epoch: 4986 [33024/118836 (28%)] Loss: 12320.570312\n",
      "Train Epoch: 4986 [65792/118836 (55%)] Loss: 12167.948242\n",
      "Train Epoch: 4986 [98560/118836 (83%)] Loss: 12271.143555\n",
      "    epoch          : 4986\n",
      "    loss           : 12218.840592205592\n",
      "    val_loss       : 12212.525311004447\n",
      "    val_log_likelihood: -12139.276509188896\n",
      "    val_log_marginal: -12147.16988431251\n",
      "Train Epoch: 4987 [256/118836 (0%)] Loss: 12213.357422\n",
      "Train Epoch: 4987 [33024/118836 (28%)] Loss: 12330.894531\n",
      "Train Epoch: 4987 [65792/118836 (55%)] Loss: 12234.541016\n",
      "Train Epoch: 4987 [98560/118836 (83%)] Loss: 12258.916016\n",
      "    epoch          : 4987\n",
      "    loss           : 12218.168272300196\n",
      "    val_loss       : 12217.00040360951\n",
      "    val_log_likelihood: -12139.913941823046\n",
      "    val_log_marginal: -12147.816062131786\n",
      "Train Epoch: 4988 [256/118836 (0%)] Loss: 12226.652344\n",
      "Train Epoch: 4988 [33024/118836 (28%)] Loss: 12267.109375\n",
      "Train Epoch: 4988 [65792/118836 (55%)] Loss: 12274.920898\n",
      "Train Epoch: 4988 [98560/118836 (83%)] Loss: 12188.599609\n",
      "    epoch          : 4988\n",
      "    loss           : 12213.320120095377\n",
      "    val_loss       : 12215.72883471571\n",
      "    val_log_likelihood: -12141.298118925766\n",
      "    val_log_marginal: -12149.192974892694\n",
      "Train Epoch: 4989 [256/118836 (0%)] Loss: 12333.520508\n",
      "Train Epoch: 4989 [33024/118836 (28%)] Loss: 12225.437500\n",
      "Train Epoch: 4989 [65792/118836 (55%)] Loss: 12220.394531\n",
      "Train Epoch: 4989 [98560/118836 (83%)] Loss: 12241.845703\n",
      "    epoch          : 4989\n",
      "    loss           : 12214.54134033809\n",
      "    val_loss       : 12213.262777808895\n",
      "    val_log_likelihood: -12141.57667445332\n",
      "    val_log_marginal: -12149.474703689595\n",
      "Train Epoch: 4990 [256/118836 (0%)] Loss: 12404.320312\n",
      "Train Epoch: 4990 [33024/118836 (28%)] Loss: 12174.213867\n",
      "Train Epoch: 4990 [65792/118836 (55%)] Loss: 12229.782227\n",
      "Train Epoch: 4990 [98560/118836 (83%)] Loss: 12199.504883\n",
      "    epoch          : 4990\n",
      "    loss           : 12219.775308235112\n",
      "    val_loss       : 12215.812357499473\n",
      "    val_log_likelihood: -12141.075020355149\n",
      "    val_log_marginal: -12148.96213459548\n",
      "Train Epoch: 4991 [256/118836 (0%)] Loss: 12220.431641\n",
      "Train Epoch: 4991 [33024/118836 (28%)] Loss: 12185.677734\n",
      "Train Epoch: 4991 [65792/118836 (55%)] Loss: 12218.546875\n",
      "Train Epoch: 4991 [98560/118836 (83%)] Loss: 12248.688477\n",
      "    epoch          : 4991\n",
      "    loss           : 12218.729581685537\n",
      "    val_loss       : 12220.110121192463\n",
      "    val_log_likelihood: -12138.847601808055\n",
      "    val_log_marginal: -12146.739857275103\n",
      "Train Epoch: 4992 [256/118836 (0%)] Loss: 12229.503906\n",
      "Train Epoch: 4992 [33024/118836 (28%)] Loss: 12257.376953\n",
      "Train Epoch: 4992 [65792/118836 (55%)] Loss: 12283.960938\n",
      "Train Epoch: 4992 [98560/118836 (83%)] Loss: 12169.975586\n",
      "    epoch          : 4992\n",
      "    loss           : 12213.249551055882\n",
      "    val_loss       : 12213.884706077635\n",
      "    val_log_likelihood: -12139.052837927782\n",
      "    val_log_marginal: -12146.946896398924\n",
      "Train Epoch: 4993 [256/118836 (0%)] Loss: 12304.798828\n",
      "Train Epoch: 4993 [33024/118836 (28%)] Loss: 12169.322266\n",
      "Train Epoch: 4993 [65792/118836 (55%)] Loss: 12159.388672\n",
      "Train Epoch: 4993 [98560/118836 (83%)] Loss: 12241.270508\n",
      "    epoch          : 4993\n",
      "    loss           : 12218.656398463347\n",
      "    val_loss       : 12216.147461998198\n",
      "    val_log_likelihood: -12140.13070428815\n",
      "    val_log_marginal: -12148.017780091173\n",
      "Train Epoch: 4994 [256/118836 (0%)] Loss: 12258.551758\n",
      "Train Epoch: 4994 [33024/118836 (28%)] Loss: 12270.425781\n",
      "Train Epoch: 4994 [65792/118836 (55%)] Loss: 12129.304688\n",
      "Train Epoch: 4994 [98560/118836 (83%)] Loss: 12210.221680\n",
      "    epoch          : 4994\n",
      "    loss           : 12218.019638356855\n",
      "    val_loss       : 12221.37222651771\n",
      "    val_log_likelihood: -12138.762426656845\n",
      "    val_log_marginal: -12146.663892628978\n",
      "Train Epoch: 4995 [256/118836 (0%)] Loss: 12306.849609\n",
      "Train Epoch: 4995 [33024/118836 (28%)] Loss: 12160.072266\n",
      "Train Epoch: 4995 [65792/118836 (55%)] Loss: 12192.330078\n",
      "Train Epoch: 4995 [98560/118836 (83%)] Loss: 12203.678711\n",
      "    epoch          : 4995\n",
      "    loss           : 12215.986727635183\n",
      "    val_loss       : 12222.788765263775\n",
      "    val_log_likelihood: -12140.69625142163\n",
      "    val_log_marginal: -12148.591316124654\n",
      "Train Epoch: 4996 [256/118836 (0%)] Loss: 12190.275391\n",
      "Train Epoch: 4996 [33024/118836 (28%)] Loss: 12200.005859\n",
      "Train Epoch: 4996 [65792/118836 (55%)] Loss: 12200.310547\n",
      "Train Epoch: 4996 [98560/118836 (83%)] Loss: 12190.390625\n",
      "    epoch          : 4996\n",
      "    loss           : 12215.5197738963\n",
      "    val_loss       : 12215.986362938404\n",
      "    val_log_likelihood: -12140.425506617039\n",
      "    val_log_marginal: -12148.327373895412\n",
      "Train Epoch: 4997 [256/118836 (0%)] Loss: 12240.888672\n",
      "Train Epoch: 4997 [33024/118836 (28%)] Loss: 12153.957031\n",
      "Train Epoch: 4997 [65792/118836 (55%)] Loss: 12296.873047\n",
      "Train Epoch: 4997 [98560/118836 (83%)] Loss: 12203.275391\n",
      "    epoch          : 4997\n",
      "    loss           : 12216.559733961434\n",
      "    val_loss       : 12216.14326436107\n",
      "    val_log_likelihood: -12139.824226504343\n",
      "    val_log_marginal: -12147.722829480015\n",
      "Train Epoch: 4998 [256/118836 (0%)] Loss: 12318.559570\n",
      "Train Epoch: 4998 [33024/118836 (28%)] Loss: 12188.132812\n",
      "Train Epoch: 4998 [65792/118836 (55%)] Loss: 12367.994141\n",
      "Train Epoch: 4998 [98560/118836 (83%)] Loss: 12187.371094\n",
      "    epoch          : 4998\n",
      "    loss           : 12219.353954553091\n",
      "    val_loss       : 12215.16462351678\n",
      "    val_log_likelihood: -12140.541141471514\n",
      "    val_log_marginal: -12148.431150929844\n",
      "Train Epoch: 4999 [256/118836 (0%)] Loss: 12279.259766\n",
      "Train Epoch: 4999 [33024/118836 (28%)] Loss: 12188.664062\n",
      "Train Epoch: 4999 [65792/118836 (55%)] Loss: 12155.029297\n",
      "Train Epoch: 4999 [98560/118836 (83%)] Loss: 12155.646484\n",
      "    epoch          : 4999\n",
      "    loss           : 12212.605049530861\n",
      "    val_loss       : 12220.875992386422\n",
      "    val_log_likelihood: -12138.08398453655\n",
      "    val_log_marginal: -12145.977708894325\n",
      "Train Epoch: 5000 [256/118836 (0%)] Loss: 12364.619141\n",
      "Train Epoch: 5000 [33024/118836 (28%)] Loss: 12211.826172\n",
      "Train Epoch: 5000 [65792/118836 (55%)] Loss: 12209.015625\n",
      "Train Epoch: 5000 [98560/118836 (83%)] Loss: 12186.674805\n",
      "    epoch          : 5000\n",
      "    loss           : 12216.776378657465\n",
      "    val_loss       : 12213.81408161664\n",
      "    val_log_likelihood: -12140.814206924628\n",
      "    val_log_marginal: -12148.712223712584\n",
      "Saving checkpoint: saved/models/SelfiesAutoencodingOperad/0619_140910/checkpoint-epoch5000.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfiesAutoencodingModel(\n",
       "  (_operad): FreeOperad(\n",
       "    (generator_0): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 64, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 64, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 64, num_layers=2, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 64, num_layers=2, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 64, num_layers=4, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 64, num_layers=4, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 100, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=100, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 100, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=100, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 100, num_layers=2, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=200, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 100, num_layers=2, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=200, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_10): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 100, num_layers=4, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=400, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_11): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 100, num_layers=4, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=400, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_12): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 128, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_13): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 128, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_14): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 128, num_layers=2, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_15): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 128, num_layers=2, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_16): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 128, num_layers=4, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_17): RecurrentDecoder(\n",
       "      (recurrence): GRU(12, 128, num_layers=4, batch_first=True)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=18, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=378, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=378, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=18, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (latent_prior): StandardNormal()\n",
       "  (encoder): StringEncoder(\n",
       "    (conv_layers): Sequential(\n",
       "      (0): Conv1d(21, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): Conv1d(32, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (3): ReLU()\n",
       "      (4): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (dense): Sequential(\n",
       "      (0): Linear(in_features=320, out_features=435, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=435, out_features=24, bias=True)\n",
       "    )\n",
       "    (distribution): DiagonalGaussian()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:categorical_bpl] *",
   "language": "python",
   "name": "conda-env-categorical_bpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
