{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eli/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='chemical_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 5,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer, log_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [256/225000 (0%)] Loss: 109643.046875\n",
      "Train Epoch: 1 [4352/225000 (2%)] Loss: 108109.851562\n",
      "Train Epoch: 1 [8448/225000 (4%)] Loss: 107512.031250\n",
      "Train Epoch: 1 [12544/225000 (6%)] Loss: 106638.468750\n",
      "Train Epoch: 1 [16640/225000 (7%)] Loss: 104553.070312\n",
      "Train Epoch: 1 [20736/225000 (9%)] Loss: 105544.554688\n",
      "Train Epoch: 1 [24832/225000 (11%)] Loss: 104594.851562\n",
      "Train Epoch: 1 [28928/225000 (13%)] Loss: 106521.421875\n",
      "Train Epoch: 1 [33024/225000 (15%)] Loss: 100556.359375\n",
      "Train Epoch: 1 [37120/225000 (16%)] Loss: 92882.890625\n",
      "Train Epoch: 1 [41216/225000 (18%)] Loss: 88755.765625\n",
      "Train Epoch: 1 [45312/225000 (20%)] Loss: 94441.632812\n",
      "Train Epoch: 1 [49408/225000 (22%)] Loss: 90899.078125\n",
      "Train Epoch: 1 [53504/225000 (24%)] Loss: 66947.195312\n",
      "Train Epoch: 1 [57600/225000 (26%)] Loss: 97453.015625\n",
      "Train Epoch: 1 [61696/225000 (27%)] Loss: 85326.437500\n",
      "Train Epoch: 1 [65792/225000 (29%)] Loss: 68704.023438\n",
      "Train Epoch: 1 [69888/225000 (31%)] Loss: 94781.937500\n",
      "Train Epoch: 1 [73984/225000 (33%)] Loss: 89350.101562\n",
      "Train Epoch: 1 [78080/225000 (35%)] Loss: 79653.250000\n",
      "Train Epoch: 1 [82176/225000 (37%)] Loss: 84318.078125\n",
      "Train Epoch: 1 [86272/225000 (38%)] Loss: 98451.187500\n",
      "Train Epoch: 1 [90368/225000 (40%)] Loss: 73830.390625\n",
      "Train Epoch: 1 [94464/225000 (42%)] Loss: 60392.773438\n",
      "Train Epoch: 1 [98560/225000 (44%)] Loss: 55585.746094\n",
      "Train Epoch: 1 [102656/225000 (46%)] Loss: 66486.875000\n",
      "Train Epoch: 1 [106752/225000 (47%)] Loss: 67201.648438\n",
      "Train Epoch: 1 [110848/225000 (49%)] Loss: 54218.070312\n",
      "Train Epoch: 1 [114944/225000 (51%)] Loss: 51073.050781\n",
      "Train Epoch: 1 [119040/225000 (53%)] Loss: 45163.207031\n",
      "Train Epoch: 1 [123136/225000 (55%)] Loss: 50335.394531\n",
      "Train Epoch: 1 [127232/225000 (57%)] Loss: 41183.046875\n",
      "Train Epoch: 1 [131328/225000 (58%)] Loss: 36103.816406\n",
      "Train Epoch: 1 [135424/225000 (60%)] Loss: 35899.671875\n",
      "Train Epoch: 1 [139520/225000 (62%)] Loss: 34173.617188\n",
      "Train Epoch: 1 [143616/225000 (64%)] Loss: 33172.804688\n",
      "Train Epoch: 1 [147712/225000 (66%)] Loss: 32413.843750\n",
      "Train Epoch: 1 [151808/225000 (67%)] Loss: 31505.457031\n",
      "Train Epoch: 1 [155904/225000 (69%)] Loss: 31466.507812\n",
      "Train Epoch: 1 [160000/225000 (71%)] Loss: 32193.671875\n",
      "Train Epoch: 1 [164096/225000 (73%)] Loss: 31284.869141\n",
      "Train Epoch: 1 [168192/225000 (75%)] Loss: 31727.998047\n",
      "Train Epoch: 1 [172288/225000 (77%)] Loss: 30965.783203\n",
      "Train Epoch: 1 [176384/225000 (78%)] Loss: 30898.902344\n",
      "Train Epoch: 1 [180480/225000 (80%)] Loss: 30242.449219\n",
      "Train Epoch: 1 [184576/225000 (82%)] Loss: 30217.451172\n",
      "Train Epoch: 1 [188672/225000 (84%)] Loss: 30015.349609\n",
      "Train Epoch: 1 [192768/225000 (86%)] Loss: 30335.140625\n",
      "Train Epoch: 1 [196864/225000 (87%)] Loss: 29886.175781\n",
      "Train Epoch: 1 [200960/225000 (89%)] Loss: 29680.068359\n",
      "Train Epoch: 1 [205056/225000 (91%)] Loss: 30119.269531\n",
      "Train Epoch: 1 [209152/225000 (93%)] Loss: 29961.140625\n",
      "Train Epoch: 1 [213248/225000 (95%)] Loss: 29930.417969\n",
      "Train Epoch: 1 [217344/225000 (97%)] Loss: 29617.371094\n",
      "Train Epoch: 1 [221440/225000 (98%)] Loss: 29416.242188\n",
      "    epoch          : 1\n",
      "    loss           : 59610.15647442051\n",
      "    val_loss       : 29540.532146309375\n",
      "Train Epoch: 2 [256/225000 (0%)] Loss: 29320.462891\n",
      "Train Epoch: 2 [4352/225000 (2%)] Loss: 29825.810547\n",
      "Train Epoch: 2 [8448/225000 (4%)] Loss: 30200.568359\n",
      "Train Epoch: 2 [12544/225000 (6%)] Loss: 29317.687500\n",
      "Train Epoch: 2 [16640/225000 (7%)] Loss: 29344.066406\n",
      "Train Epoch: 2 [20736/225000 (9%)] Loss: 29313.375000\n",
      "Train Epoch: 2 [24832/225000 (11%)] Loss: 28919.699219\n",
      "Train Epoch: 2 [28928/225000 (13%)] Loss: 28785.152344\n",
      "Train Epoch: 2 [33024/225000 (15%)] Loss: 28475.554688\n",
      "Train Epoch: 2 [37120/225000 (16%)] Loss: 28961.291016\n",
      "Train Epoch: 2 [41216/225000 (18%)] Loss: 27912.662109\n",
      "Train Epoch: 2 [45312/225000 (20%)] Loss: 28090.460938\n",
      "Train Epoch: 2 [49408/225000 (22%)] Loss: 31763.218750\n",
      "Train Epoch: 2 [53504/225000 (24%)] Loss: 28457.244141\n",
      "Train Epoch: 2 [57600/225000 (26%)] Loss: 27553.175781\n",
      "Train Epoch: 2 [61696/225000 (27%)] Loss: 28200.837891\n",
      "Train Epoch: 2 [65792/225000 (29%)] Loss: 27676.281250\n",
      "Train Epoch: 2 [69888/225000 (31%)] Loss: 27496.027344\n",
      "Train Epoch: 2 [73984/225000 (33%)] Loss: 27983.755859\n",
      "Train Epoch: 2 [78080/225000 (35%)] Loss: 27566.654297\n",
      "Train Epoch: 2 [82176/225000 (37%)] Loss: 26563.361328\n",
      "Train Epoch: 2 [86272/225000 (38%)] Loss: 26432.185547\n",
      "Train Epoch: 2 [90368/225000 (40%)] Loss: 26607.908203\n",
      "Train Epoch: 2 [94464/225000 (42%)] Loss: 26031.677734\n",
      "Train Epoch: 2 [98560/225000 (44%)] Loss: 27576.080078\n",
      "Train Epoch: 2 [102656/225000 (46%)] Loss: 26497.873047\n",
      "Train Epoch: 2 [106752/225000 (47%)] Loss: 26020.496094\n",
      "Train Epoch: 2 [110848/225000 (49%)] Loss: 25365.687500\n",
      "Train Epoch: 2 [114944/225000 (51%)] Loss: 29905.519531\n",
      "Train Epoch: 2 [119040/225000 (53%)] Loss: 24721.164062\n",
      "Train Epoch: 2 [123136/225000 (55%)] Loss: 24339.419922\n",
      "Train Epoch: 2 [127232/225000 (57%)] Loss: 24496.621094\n",
      "Train Epoch: 2 [131328/225000 (58%)] Loss: 23801.589844\n",
      "Train Epoch: 2 [135424/225000 (60%)] Loss: 24013.146484\n",
      "Train Epoch: 2 [139520/225000 (62%)] Loss: 23120.234375\n",
      "Train Epoch: 2 [143616/225000 (64%)] Loss: 23108.060547\n",
      "Train Epoch: 2 [147712/225000 (66%)] Loss: 22491.830078\n",
      "Train Epoch: 2 [151808/225000 (67%)] Loss: 22372.195312\n",
      "Train Epoch: 2 [155904/225000 (69%)] Loss: 22269.146484\n",
      "Train Epoch: 2 [160000/225000 (71%)] Loss: 22287.248047\n",
      "Train Epoch: 2 [164096/225000 (73%)] Loss: 21352.560547\n",
      "Train Epoch: 2 [168192/225000 (75%)] Loss: 21477.951172\n",
      "Train Epoch: 2 [172288/225000 (77%)] Loss: 20699.156250\n",
      "Train Epoch: 2 [176384/225000 (78%)] Loss: 21352.076172\n",
      "Train Epoch: 2 [180480/225000 (80%)] Loss: 20552.425781\n",
      "Train Epoch: 2 [184576/225000 (82%)] Loss: 19917.462891\n",
      "Train Epoch: 2 [188672/225000 (84%)] Loss: 19815.628906\n",
      "Train Epoch: 2 [192768/225000 (86%)] Loss: 19829.353516\n",
      "Train Epoch: 2 [196864/225000 (87%)] Loss: 18833.480469\n",
      "Train Epoch: 2 [200960/225000 (89%)] Loss: 19285.328125\n",
      "Train Epoch: 2 [205056/225000 (91%)] Loss: 18728.222656\n",
      "Train Epoch: 2 [209152/225000 (93%)] Loss: 18576.572266\n",
      "Train Epoch: 2 [213248/225000 (95%)] Loss: 18919.632812\n",
      "Train Epoch: 2 [217344/225000 (97%)] Loss: 18101.091797\n",
      "Train Epoch: 2 [221440/225000 (98%)] Loss: 18294.054688\n",
      "    epoch          : 2\n",
      "    loss           : 24987.04930140785\n",
      "    val_loss       : 18361.310375298774\n",
      "Train Epoch: 3 [256/225000 (0%)] Loss: 23649.769531\n",
      "Train Epoch: 3 [4352/225000 (2%)] Loss: 17549.162109\n",
      "Train Epoch: 3 [8448/225000 (4%)] Loss: 17328.072266\n",
      "Train Epoch: 3 [12544/225000 (6%)] Loss: 17384.066406\n",
      "Train Epoch: 3 [16640/225000 (7%)] Loss: 17255.728516\n",
      "Train Epoch: 3 [20736/225000 (9%)] Loss: 17309.093750\n",
      "Train Epoch: 3 [24832/225000 (11%)] Loss: 17431.099609\n",
      "Train Epoch: 3 [28928/225000 (13%)] Loss: 16922.179688\n",
      "Train Epoch: 3 [33024/225000 (15%)] Loss: 17037.439453\n",
      "Train Epoch: 3 [37120/225000 (16%)] Loss: 17018.556641\n",
      "Train Epoch: 3 [41216/225000 (18%)] Loss: 16862.636719\n",
      "Train Epoch: 3 [45312/225000 (20%)] Loss: 16525.919922\n",
      "Train Epoch: 3 [49408/225000 (22%)] Loss: 16469.849609\n",
      "Train Epoch: 3 [53504/225000 (24%)] Loss: 21906.755859\n",
      "Train Epoch: 3 [57600/225000 (26%)] Loss: 16393.955078\n",
      "Train Epoch: 3 [61696/225000 (27%)] Loss: 16027.708984\n",
      "Train Epoch: 3 [65792/225000 (29%)] Loss: 16137.263672\n",
      "Train Epoch: 3 [69888/225000 (31%)] Loss: 16343.666016\n",
      "Train Epoch: 3 [73984/225000 (33%)] Loss: 16180.111328\n",
      "Train Epoch: 3 [78080/225000 (35%)] Loss: 16262.187500\n",
      "Train Epoch: 3 [82176/225000 (37%)] Loss: 16034.039062\n",
      "Train Epoch: 3 [86272/225000 (38%)] Loss: 15837.615234\n",
      "Train Epoch: 3 [90368/225000 (40%)] Loss: 16191.001953\n",
      "Train Epoch: 3 [94464/225000 (42%)] Loss: 15677.458984\n",
      "Train Epoch: 3 [98560/225000 (44%)] Loss: 15668.275391\n",
      "Train Epoch: 3 [102656/225000 (46%)] Loss: 15589.328125\n",
      "Train Epoch: 3 [106752/225000 (47%)] Loss: 15166.869141\n",
      "Train Epoch: 3 [110848/225000 (49%)] Loss: 15577.132812\n",
      "Train Epoch: 3 [114944/225000 (51%)] Loss: 20999.371094\n",
      "Train Epoch: 3 [119040/225000 (53%)] Loss: 15440.220703\n",
      "Train Epoch: 3 [123136/225000 (55%)] Loss: 14959.832031\n",
      "Train Epoch: 3 [127232/225000 (57%)] Loss: 21260.230469\n",
      "Train Epoch: 3 [131328/225000 (58%)] Loss: 22778.236328\n",
      "Train Epoch: 3 [135424/225000 (60%)] Loss: 14958.347656\n",
      "Train Epoch: 3 [139520/225000 (62%)] Loss: 14991.771484\n",
      "Train Epoch: 3 [143616/225000 (64%)] Loss: 14516.847656\n",
      "Train Epoch: 3 [147712/225000 (66%)] Loss: 14961.855469\n",
      "Train Epoch: 3 [151808/225000 (67%)] Loss: 14878.990234\n",
      "Train Epoch: 3 [155904/225000 (69%)] Loss: 20452.687500\n",
      "Train Epoch: 3 [160000/225000 (71%)] Loss: 14411.289062\n",
      "Train Epoch: 3 [164096/225000 (73%)] Loss: 14495.134766\n",
      "Train Epoch: 3 [168192/225000 (75%)] Loss: 14710.339844\n",
      "Train Epoch: 3 [172288/225000 (77%)] Loss: 14478.177734\n",
      "Train Epoch: 3 [176384/225000 (78%)] Loss: 14514.691406\n",
      "Train Epoch: 3 [180480/225000 (80%)] Loss: 14305.042969\n",
      "Train Epoch: 3 [184576/225000 (82%)] Loss: 14058.000000\n",
      "Train Epoch: 3 [188672/225000 (84%)] Loss: 14501.587891\n",
      "Train Epoch: 3 [192768/225000 (86%)] Loss: 14533.470703\n",
      "Train Epoch: 3 [196864/225000 (87%)] Loss: 14466.195312\n",
      "Train Epoch: 3 [200960/225000 (89%)] Loss: 19674.466797\n",
      "Train Epoch: 3 [205056/225000 (91%)] Loss: 14396.138672\n",
      "Train Epoch: 3 [209152/225000 (93%)] Loss: 14369.378906\n",
      "Train Epoch: 3 [213248/225000 (95%)] Loss: 14227.314453\n",
      "Train Epoch: 3 [217344/225000 (97%)] Loss: 14153.289062\n",
      "Train Epoch: 3 [221440/225000 (98%)] Loss: 14203.927734\n",
      "    epoch          : 3\n",
      "    loss           : 16661.594989867746\n",
      "    val_loss       : 15367.864824351635\n",
      "Train Epoch: 4 [256/225000 (0%)] Loss: 14094.199219\n",
      "Train Epoch: 4 [4352/225000 (2%)] Loss: 13956.177734\n",
      "Train Epoch: 4 [8448/225000 (4%)] Loss: 14154.359375\n",
      "Train Epoch: 4 [12544/225000 (6%)] Loss: 19562.787109\n",
      "Train Epoch: 4 [16640/225000 (7%)] Loss: 13790.958984\n",
      "Train Epoch: 4 [20736/225000 (9%)] Loss: 13909.451172\n",
      "Train Epoch: 4 [24832/225000 (11%)] Loss: 13659.453125\n",
      "Train Epoch: 4 [28928/225000 (13%)] Loss: 13801.599609\n",
      "Train Epoch: 4 [33024/225000 (15%)] Loss: 13736.677734\n",
      "Train Epoch: 4 [37120/225000 (16%)] Loss: 13842.083984\n",
      "Train Epoch: 4 [41216/225000 (18%)] Loss: 18797.142578\n",
      "Train Epoch: 4 [45312/225000 (20%)] Loss: 24984.947266\n",
      "Train Epoch: 4 [49408/225000 (22%)] Loss: 13683.566406\n",
      "Train Epoch: 4 [53504/225000 (24%)] Loss: 13777.912109\n",
      "Train Epoch: 4 [57600/225000 (26%)] Loss: 19009.873047\n",
      "Train Epoch: 4 [61696/225000 (27%)] Loss: 13596.644531\n",
      "Train Epoch: 4 [65792/225000 (29%)] Loss: 13151.710938\n",
      "Train Epoch: 4 [69888/225000 (31%)] Loss: 13408.558594\n",
      "Train Epoch: 4 [73984/225000 (33%)] Loss: 13420.769531\n",
      "Train Epoch: 4 [78080/225000 (35%)] Loss: 18761.173828\n",
      "Train Epoch: 4 [82176/225000 (37%)] Loss: 13436.183594\n",
      "Train Epoch: 4 [86272/225000 (38%)] Loss: 13450.994141\n",
      "Train Epoch: 4 [90368/225000 (40%)] Loss: 13421.941406\n",
      "Train Epoch: 4 [94464/225000 (42%)] Loss: 13458.958984\n",
      "Train Epoch: 4 [98560/225000 (44%)] Loss: 18733.716797\n",
      "Train Epoch: 4 [102656/225000 (46%)] Loss: 13349.732422\n",
      "Train Epoch: 4 [106752/225000 (47%)] Loss: 13208.634766\n",
      "Train Epoch: 4 [110848/225000 (49%)] Loss: 13329.677734\n",
      "Train Epoch: 4 [114944/225000 (51%)] Loss: 13690.240234\n",
      "Train Epoch: 4 [119040/225000 (53%)] Loss: 28575.541016\n",
      "Train Epoch: 4 [123136/225000 (55%)] Loss: 13416.035156\n",
      "Train Epoch: 4 [127232/225000 (57%)] Loss: 13729.699219\n",
      "Train Epoch: 4 [131328/225000 (58%)] Loss: 18480.580078\n",
      "Train Epoch: 4 [135424/225000 (60%)] Loss: 17969.451172\n",
      "Train Epoch: 4 [139520/225000 (62%)] Loss: 13095.162109\n",
      "Train Epoch: 4 [143616/225000 (64%)] Loss: 13303.865234\n",
      "Train Epoch: 4 [147712/225000 (66%)] Loss: 12921.794922\n",
      "Train Epoch: 4 [151808/225000 (67%)] Loss: 12857.332031\n",
      "Train Epoch: 4 [155904/225000 (69%)] Loss: 13214.763672\n",
      "Train Epoch: 4 [160000/225000 (71%)] Loss: 13113.919922\n",
      "Train Epoch: 4 [164096/225000 (73%)] Loss: 18743.460938\n",
      "Train Epoch: 4 [168192/225000 (75%)] Loss: 18573.703125\n",
      "Train Epoch: 4 [172288/225000 (77%)] Loss: 13000.396484\n",
      "Train Epoch: 4 [176384/225000 (78%)] Loss: 12649.708984\n",
      "Train Epoch: 4 [180480/225000 (80%)] Loss: 12950.324219\n",
      "Train Epoch: 4 [184576/225000 (82%)] Loss: 12638.236328\n",
      "Train Epoch: 4 [188672/225000 (84%)] Loss: 12858.492188\n",
      "Train Epoch: 4 [192768/225000 (86%)] Loss: 12775.464844\n",
      "Train Epoch: 4 [196864/225000 (87%)] Loss: 12705.707031\n",
      "Train Epoch: 4 [200960/225000 (89%)] Loss: 12910.466797\n",
      "Train Epoch: 4 [205056/225000 (91%)] Loss: 12664.900391\n",
      "Train Epoch: 4 [209152/225000 (93%)] Loss: 12937.748047\n",
      "Train Epoch: 4 [213248/225000 (95%)] Loss: 12965.066406\n",
      "Train Epoch: 4 [217344/225000 (97%)] Loss: 13087.216797\n",
      "Train Epoch: 4 [221440/225000 (98%)] Loss: 12747.468750\n",
      "    epoch          : 4\n",
      "    loss           : 14878.382201454067\n",
      "    val_loss       : 14075.20717163447\n",
      "Train Epoch: 5 [256/225000 (0%)] Loss: 12961.976562\n",
      "Train Epoch: 5 [4352/225000 (2%)] Loss: 12826.705078\n",
      "Train Epoch: 5 [8448/225000 (4%)] Loss: 25197.544922\n",
      "Train Epoch: 5 [12544/225000 (6%)] Loss: 12452.080078\n",
      "Train Epoch: 5 [16640/225000 (7%)] Loss: 12711.775391\n",
      "Train Epoch: 5 [20736/225000 (9%)] Loss: 12668.726562\n",
      "Train Epoch: 5 [24832/225000 (11%)] Loss: 12446.185547\n",
      "Train Epoch: 5 [28928/225000 (13%)] Loss: 13000.914062\n",
      "Train Epoch: 5 [33024/225000 (15%)] Loss: 12740.646484\n",
      "Train Epoch: 5 [37120/225000 (16%)] Loss: 12720.134766\n",
      "Train Epoch: 5 [41216/225000 (18%)] Loss: 12736.820312\n",
      "Train Epoch: 5 [45312/225000 (20%)] Loss: 12683.111328\n",
      "Train Epoch: 5 [49408/225000 (22%)] Loss: 24201.486328\n",
      "Train Epoch: 5 [53504/225000 (24%)] Loss: 17411.675781\n",
      "Train Epoch: 5 [57600/225000 (26%)] Loss: 18035.564453\n",
      "Train Epoch: 5 [61696/225000 (27%)] Loss: 12107.195312\n",
      "Train Epoch: 5 [65792/225000 (29%)] Loss: 12557.121094\n",
      "Train Epoch: 5 [69888/225000 (31%)] Loss: 12227.779297\n",
      "Train Epoch: 5 [73984/225000 (33%)] Loss: 12210.310547\n",
      "Train Epoch: 5 [78080/225000 (35%)] Loss: 12340.957031\n",
      "Train Epoch: 5 [82176/225000 (37%)] Loss: 12612.195312\n",
      "Train Epoch: 5 [86272/225000 (38%)] Loss: 12640.726562\n",
      "Train Epoch: 5 [90368/225000 (40%)] Loss: 12318.755859\n",
      "Train Epoch: 5 [94464/225000 (42%)] Loss: 22066.005859\n",
      "Train Epoch: 5 [98560/225000 (44%)] Loss: 13308.363281\n",
      "Train Epoch: 5 [102656/225000 (46%)] Loss: 12610.464844\n",
      "Train Epoch: 5 [106752/225000 (47%)] Loss: 18020.037109\n",
      "Train Epoch: 5 [110848/225000 (49%)] Loss: 12013.304688\n",
      "Train Epoch: 5 [114944/225000 (51%)] Loss: 12256.080078\n",
      "Train Epoch: 5 [119040/225000 (53%)] Loss: 17046.523438\n",
      "Train Epoch: 5 [123136/225000 (55%)] Loss: 12382.847656\n",
      "Train Epoch: 5 [127232/225000 (57%)] Loss: 12159.177734\n",
      "Train Epoch: 5 [131328/225000 (58%)] Loss: 12237.091797\n",
      "Train Epoch: 5 [135424/225000 (60%)] Loss: 12126.376953\n",
      "Train Epoch: 5 [139520/225000 (62%)] Loss: 12087.857422\n",
      "Train Epoch: 5 [143616/225000 (64%)] Loss: 11849.460938\n",
      "Train Epoch: 5 [147712/225000 (66%)] Loss: 19545.250000\n",
      "Train Epoch: 5 [151808/225000 (67%)] Loss: 12079.308594\n",
      "Train Epoch: 5 [155904/225000 (69%)] Loss: 12234.806641\n",
      "Train Epoch: 5 [160000/225000 (71%)] Loss: 12075.650391\n",
      "Train Epoch: 5 [164096/225000 (73%)] Loss: 19652.023438\n",
      "Train Epoch: 5 [168192/225000 (75%)] Loss: 12153.156250\n",
      "Train Epoch: 5 [172288/225000 (77%)] Loss: 12246.130859\n",
      "Train Epoch: 5 [176384/225000 (78%)] Loss: 28563.570312\n",
      "Train Epoch: 5 [180480/225000 (80%)] Loss: 11882.708984\n",
      "Train Epoch: 5 [184576/225000 (82%)] Loss: 12126.990234\n",
      "Train Epoch: 5 [188672/225000 (84%)] Loss: 12196.597656\n",
      "Train Epoch: 5 [192768/225000 (86%)] Loss: 17920.566406\n",
      "Train Epoch: 5 [196864/225000 (87%)] Loss: 12313.427734\n",
      "Train Epoch: 5 [200960/225000 (89%)] Loss: 11809.826172\n",
      "Train Epoch: 5 [205056/225000 (91%)] Loss: 12216.060547\n",
      "Train Epoch: 5 [209152/225000 (93%)] Loss: 21227.220703\n",
      "Train Epoch: 5 [213248/225000 (95%)] Loss: 12171.736328\n",
      "Train Epoch: 5 [217344/225000 (97%)] Loss: 19334.144531\n",
      "Train Epoch: 5 [221440/225000 (98%)] Loss: 12033.597656\n",
      "    epoch          : 5\n",
      "    loss           : 14300.013958511092\n",
      "    val_loss       : 13858.643186481631\n",
      "Train Epoch: 6 [256/225000 (0%)] Loss: 12115.916016\n",
      "Train Epoch: 6 [4352/225000 (2%)] Loss: 19278.712891\n",
      "Train Epoch: 6 [8448/225000 (4%)] Loss: 16704.437500\n",
      "Train Epoch: 6 [12544/225000 (6%)] Loss: 12068.416016\n",
      "Train Epoch: 6 [16640/225000 (7%)] Loss: 16902.732422\n",
      "Train Epoch: 6 [20736/225000 (9%)] Loss: 11715.986328\n",
      "Train Epoch: 6 [24832/225000 (11%)] Loss: 11827.001953\n",
      "Train Epoch: 6 [28928/225000 (13%)] Loss: 11544.740234\n",
      "Train Epoch: 6 [33024/225000 (15%)] Loss: 26589.125000\n",
      "Train Epoch: 6 [37120/225000 (16%)] Loss: 16745.828125\n",
      "Train Epoch: 6 [41216/225000 (18%)] Loss: 11691.970703\n",
      "Train Epoch: 6 [45312/225000 (20%)] Loss: 11686.431641\n",
      "Train Epoch: 6 [49408/225000 (22%)] Loss: 11614.853516\n",
      "Train Epoch: 6 [53504/225000 (24%)] Loss: 11898.066406\n",
      "Train Epoch: 6 [57600/225000 (26%)] Loss: 11444.113281\n",
      "Train Epoch: 6 [61696/225000 (27%)] Loss: 16821.490234\n",
      "Train Epoch: 6 [65792/225000 (29%)] Loss: 16215.726562\n",
      "Train Epoch: 6 [69888/225000 (31%)] Loss: 12069.101562\n",
      "Train Epoch: 6 [73984/225000 (33%)] Loss: 21372.275391\n",
      "Train Epoch: 6 [78080/225000 (35%)] Loss: 16580.007812\n",
      "Train Epoch: 6 [82176/225000 (37%)] Loss: 16178.796875\n",
      "Train Epoch: 6 [86272/225000 (38%)] Loss: 20855.773438\n",
      "Train Epoch: 6 [90368/225000 (40%)] Loss: 16312.287109\n",
      "Train Epoch: 6 [94464/225000 (42%)] Loss: 16728.400391\n",
      "Train Epoch: 6 [98560/225000 (44%)] Loss: 11572.148438\n",
      "Train Epoch: 6 [102656/225000 (46%)] Loss: 18505.000000\n",
      "Train Epoch: 6 [106752/225000 (47%)] Loss: 11745.298828\n",
      "Train Epoch: 6 [110848/225000 (49%)] Loss: 11844.328125\n",
      "Train Epoch: 6 [114944/225000 (51%)] Loss: 11682.021484\n",
      "Train Epoch: 6 [119040/225000 (53%)] Loss: 11499.796875\n",
      "Train Epoch: 6 [123136/225000 (55%)] Loss: 16799.353516\n",
      "Train Epoch: 6 [127232/225000 (57%)] Loss: 11486.195312\n",
      "Train Epoch: 6 [131328/225000 (58%)] Loss: 11939.945312\n",
      "Train Epoch: 6 [135424/225000 (60%)] Loss: 11719.707031\n",
      "Train Epoch: 6 [139520/225000 (62%)] Loss: 17057.285156\n",
      "Train Epoch: 6 [143616/225000 (64%)] Loss: 11601.435547\n",
      "Train Epoch: 6 [147712/225000 (66%)] Loss: 11601.779297\n",
      "Train Epoch: 6 [151808/225000 (67%)] Loss: 11505.476562\n",
      "Train Epoch: 6 [155904/225000 (69%)] Loss: 11545.195312\n",
      "Train Epoch: 6 [160000/225000 (71%)] Loss: 11588.066406\n",
      "Train Epoch: 6 [164096/225000 (73%)] Loss: 11349.414062\n",
      "Train Epoch: 6 [168192/225000 (75%)] Loss: 11463.015625\n",
      "Train Epoch: 6 [172288/225000 (77%)] Loss: 11260.816406\n",
      "Train Epoch: 6 [176384/225000 (78%)] Loss: 11543.583984\n",
      "Train Epoch: 6 [180480/225000 (80%)] Loss: 11540.367188\n",
      "Train Epoch: 6 [184576/225000 (82%)] Loss: 11254.275391\n",
      "Train Epoch: 6 [188672/225000 (84%)] Loss: 16457.152344\n",
      "Train Epoch: 6 [192768/225000 (86%)] Loss: 11100.646484\n",
      "Train Epoch: 6 [196864/225000 (87%)] Loss: 11654.037109\n",
      "Train Epoch: 6 [200960/225000 (89%)] Loss: 11290.484375\n",
      "Train Epoch: 6 [205056/225000 (91%)] Loss: 11367.652344\n",
      "Train Epoch: 6 [209152/225000 (93%)] Loss: 11500.335938\n",
      "Train Epoch: 6 [213248/225000 (95%)] Loss: 11400.076172\n",
      "Train Epoch: 6 [217344/225000 (97%)] Loss: 11168.294922\n",
      "Train Epoch: 6 [221440/225000 (98%)] Loss: 11323.042969\n",
      "    epoch          : 6\n",
      "    loss           : 14135.414915742322\n",
      "    val_loss       : 13357.040671203818\n",
      "Train Epoch: 7 [256/225000 (0%)] Loss: 11242.587891\n",
      "Train Epoch: 7 [4352/225000 (2%)] Loss: 11202.511719\n",
      "Train Epoch: 7 [8448/225000 (4%)] Loss: 16026.302734\n",
      "Train Epoch: 7 [12544/225000 (6%)] Loss: 15853.746094\n",
      "Train Epoch: 7 [16640/225000 (7%)] Loss: 11316.113281\n",
      "Train Epoch: 7 [20736/225000 (9%)] Loss: 11386.433594\n",
      "Train Epoch: 7 [24832/225000 (11%)] Loss: 11151.398438\n",
      "Train Epoch: 7 [28928/225000 (13%)] Loss: 11440.486328\n",
      "Train Epoch: 7 [33024/225000 (15%)] Loss: 16479.042969\n",
      "Train Epoch: 7 [37120/225000 (16%)] Loss: 18230.826172\n",
      "Train Epoch: 7 [41216/225000 (18%)] Loss: 11353.542969\n",
      "Train Epoch: 7 [45312/225000 (20%)] Loss: 30531.351562\n",
      "Train Epoch: 7 [49408/225000 (22%)] Loss: 11122.761719\n",
      "Train Epoch: 7 [53504/225000 (24%)] Loss: 11322.523438\n",
      "Train Epoch: 7 [57600/225000 (26%)] Loss: 16167.072266\n",
      "Train Epoch: 7 [61696/225000 (27%)] Loss: 11238.855469\n",
      "Train Epoch: 7 [65792/225000 (29%)] Loss: 21158.542969\n",
      "Train Epoch: 7 [69888/225000 (31%)] Loss: 28106.119141\n",
      "Train Epoch: 7 [73984/225000 (33%)] Loss: 11257.539062\n",
      "Train Epoch: 7 [78080/225000 (35%)] Loss: 11173.730469\n",
      "Train Epoch: 7 [82176/225000 (37%)] Loss: 11278.533203\n",
      "Train Epoch: 7 [86272/225000 (38%)] Loss: 11135.726562\n",
      "Train Epoch: 7 [90368/225000 (40%)] Loss: 10939.623047\n",
      "Train Epoch: 7 [94464/225000 (42%)] Loss: 11115.953125\n",
      "Train Epoch: 7 [98560/225000 (44%)] Loss: 11149.941406\n",
      "Train Epoch: 7 [102656/225000 (46%)] Loss: 21067.664062\n",
      "Train Epoch: 7 [106752/225000 (47%)] Loss: 11224.064453\n",
      "Train Epoch: 7 [110848/225000 (49%)] Loss: 11222.634766\n",
      "Train Epoch: 7 [114944/225000 (51%)] Loss: 30294.876953\n",
      "Train Epoch: 7 [119040/225000 (53%)] Loss: 11072.628906\n",
      "Train Epoch: 7 [123136/225000 (55%)] Loss: 11012.003906\n",
      "Train Epoch: 7 [127232/225000 (57%)] Loss: 15766.037109\n",
      "Train Epoch: 7 [131328/225000 (58%)] Loss: 11090.410156\n",
      "Train Epoch: 7 [135424/225000 (60%)] Loss: 10984.669922\n",
      "Train Epoch: 7 [139520/225000 (62%)] Loss: 15798.802734\n",
      "Train Epoch: 7 [143616/225000 (64%)] Loss: 10911.101562\n",
      "Train Epoch: 7 [147712/225000 (66%)] Loss: 10778.119141\n",
      "Train Epoch: 7 [151808/225000 (67%)] Loss: 15820.128906\n",
      "Train Epoch: 7 [155904/225000 (69%)] Loss: 11168.472656\n",
      "Train Epoch: 7 [160000/225000 (71%)] Loss: 11431.343750\n",
      "Train Epoch: 7 [164096/225000 (73%)] Loss: 11293.142578\n",
      "Train Epoch: 7 [168192/225000 (75%)] Loss: 11168.501953\n",
      "Train Epoch: 7 [172288/225000 (77%)] Loss: 15875.300781\n",
      "Train Epoch: 7 [176384/225000 (78%)] Loss: 11065.357422\n",
      "Train Epoch: 7 [180480/225000 (80%)] Loss: 10908.029297\n",
      "Train Epoch: 7 [184576/225000 (82%)] Loss: 10929.695312\n",
      "Train Epoch: 7 [188672/225000 (84%)] Loss: 20570.730469\n",
      "Train Epoch: 7 [192768/225000 (86%)] Loss: 10685.765625\n",
      "Train Epoch: 7 [196864/225000 (87%)] Loss: 10970.822266\n",
      "Train Epoch: 7 [200960/225000 (89%)] Loss: 10916.386719\n",
      "Train Epoch: 7 [205056/225000 (91%)] Loss: 10922.158203\n",
      "Train Epoch: 7 [209152/225000 (93%)] Loss: 10935.802734\n",
      "Train Epoch: 7 [213248/225000 (95%)] Loss: 11007.894531\n",
      "Train Epoch: 7 [217344/225000 (97%)] Loss: 15648.199219\n",
      "Train Epoch: 7 [221440/225000 (98%)] Loss: 11252.486328\n",
      "    epoch          : 7\n",
      "    loss           : 13753.11090594781\n",
      "    val_loss       : 14594.07436124646\n",
      "Train Epoch: 8 [256/225000 (0%)] Loss: 11051.802734\n",
      "Train Epoch: 8 [4352/225000 (2%)] Loss: 10828.849609\n",
      "Train Epoch: 8 [8448/225000 (4%)] Loss: 15457.431641\n",
      "Train Epoch: 8 [12544/225000 (6%)] Loss: 24493.509766\n",
      "Train Epoch: 8 [16640/225000 (7%)] Loss: 10868.277344\n",
      "Train Epoch: 8 [20736/225000 (9%)] Loss: 15911.759766\n",
      "Train Epoch: 8 [24832/225000 (11%)] Loss: 10986.648438\n",
      "Train Epoch: 8 [28928/225000 (13%)] Loss: 11029.058594\n",
      "Train Epoch: 8 [33024/225000 (15%)] Loss: 15337.816406\n",
      "Train Epoch: 8 [37120/225000 (16%)] Loss: 15790.853516\n",
      "Train Epoch: 8 [41216/225000 (18%)] Loss: 10866.726562\n",
      "Train Epoch: 8 [45312/225000 (20%)] Loss: 19759.542969\n",
      "Train Epoch: 8 [49408/225000 (22%)] Loss: 26598.324219\n",
      "Train Epoch: 8 [53504/225000 (24%)] Loss: 10884.957031\n",
      "Train Epoch: 8 [57600/225000 (26%)] Loss: 23720.171875\n",
      "Train Epoch: 8 [61696/225000 (27%)] Loss: 10768.794922\n",
      "Train Epoch: 8 [65792/225000 (29%)] Loss: 18491.669922\n",
      "Train Epoch: 8 [69888/225000 (31%)] Loss: 10699.576172\n",
      "Train Epoch: 8 [73984/225000 (33%)] Loss: 19306.109375\n",
      "Train Epoch: 8 [78080/225000 (35%)] Loss: 15505.125000\n",
      "Train Epoch: 8 [82176/225000 (37%)] Loss: 20323.349609\n",
      "Train Epoch: 8 [86272/225000 (38%)] Loss: 10767.970703\n",
      "Train Epoch: 8 [90368/225000 (40%)] Loss: 10804.914062\n",
      "Train Epoch: 8 [94464/225000 (42%)] Loss: 10984.906250\n",
      "Train Epoch: 8 [98560/225000 (44%)] Loss: 17480.388672\n",
      "Train Epoch: 8 [102656/225000 (46%)] Loss: 23322.843750\n",
      "Train Epoch: 8 [106752/225000 (47%)] Loss: 11133.351562\n",
      "Train Epoch: 8 [110848/225000 (49%)] Loss: 10979.304688\n",
      "Train Epoch: 8 [114944/225000 (51%)] Loss: 10677.392578\n",
      "Train Epoch: 8 [119040/225000 (53%)] Loss: 10642.291016\n",
      "Train Epoch: 8 [123136/225000 (55%)] Loss: 11043.535156\n",
      "Train Epoch: 8 [127232/225000 (57%)] Loss: 10812.382812\n",
      "Train Epoch: 8 [131328/225000 (58%)] Loss: 10720.328125\n",
      "Train Epoch: 8 [135424/225000 (60%)] Loss: 10695.939453\n",
      "Train Epoch: 8 [139520/225000 (62%)] Loss: 15570.794922\n",
      "Train Epoch: 8 [143616/225000 (64%)] Loss: 10574.093750\n",
      "Train Epoch: 8 [147712/225000 (66%)] Loss: 15518.861328\n",
      "Train Epoch: 8 [151808/225000 (67%)] Loss: 10542.015625\n",
      "Train Epoch: 8 [155904/225000 (69%)] Loss: 10896.750000\n",
      "Train Epoch: 8 [160000/225000 (71%)] Loss: 15464.083984\n",
      "Train Epoch: 8 [164096/225000 (73%)] Loss: 15239.054688\n",
      "Train Epoch: 8 [168192/225000 (75%)] Loss: 10534.960938\n",
      "Train Epoch: 8 [172288/225000 (77%)] Loss: 18648.056641\n",
      "Train Epoch: 8 [176384/225000 (78%)] Loss: 17855.189453\n",
      "Train Epoch: 8 [180480/225000 (80%)] Loss: 15637.515625\n",
      "Train Epoch: 8 [184576/225000 (82%)] Loss: 18845.472656\n",
      "Train Epoch: 8 [188672/225000 (84%)] Loss: 18814.179688\n",
      "Train Epoch: 8 [192768/225000 (86%)] Loss: 15506.058594\n",
      "Train Epoch: 8 [196864/225000 (87%)] Loss: 10995.240234\n",
      "Train Epoch: 8 [200960/225000 (89%)] Loss: 17602.103516\n",
      "Train Epoch: 8 [205056/225000 (91%)] Loss: 15184.906250\n",
      "Train Epoch: 8 [209152/225000 (93%)] Loss: 20017.169922\n",
      "Train Epoch: 8 [213248/225000 (95%)] Loss: 10606.773438\n",
      "Train Epoch: 8 [217344/225000 (97%)] Loss: 15080.193359\n",
      "Train Epoch: 8 [221440/225000 (98%)] Loss: 15270.769531\n",
      "    epoch          : 8\n",
      "    loss           : 14078.343145620023\n",
      "    val_loss       : 13992.593780232859\n",
      "Train Epoch: 9 [256/225000 (0%)] Loss: 10629.074219\n",
      "Train Epoch: 9 [4352/225000 (2%)] Loss: 10532.699219\n",
      "Train Epoch: 9 [8448/225000 (4%)] Loss: 17965.662109\n",
      "Train Epoch: 9 [12544/225000 (6%)] Loss: 10467.935547\n",
      "Train Epoch: 9 [16640/225000 (7%)] Loss: 15142.843750\n",
      "Train Epoch: 9 [20736/225000 (9%)] Loss: 10325.875000\n",
      "Train Epoch: 9 [24832/225000 (11%)] Loss: 10420.138672\n",
      "Train Epoch: 9 [28928/225000 (13%)] Loss: 10352.541016\n",
      "Train Epoch: 9 [33024/225000 (15%)] Loss: 15233.849609\n",
      "Train Epoch: 9 [37120/225000 (16%)] Loss: 10471.984375\n",
      "Train Epoch: 9 [41216/225000 (18%)] Loss: 15185.320312\n",
      "Train Epoch: 9 [45312/225000 (20%)] Loss: 10361.382812\n",
      "Train Epoch: 9 [49408/225000 (22%)] Loss: 15497.296875\n",
      "Train Epoch: 9 [53504/225000 (24%)] Loss: 10373.535156\n",
      "Train Epoch: 9 [57600/225000 (26%)] Loss: 10648.988281\n",
      "Train Epoch: 9 [61696/225000 (27%)] Loss: 10430.294922\n",
      "Train Epoch: 9 [65792/225000 (29%)] Loss: 10447.966797\n",
      "Train Epoch: 9 [69888/225000 (31%)] Loss: 18220.119141\n",
      "Train Epoch: 9 [73984/225000 (33%)] Loss: 30089.103516\n",
      "Train Epoch: 9 [78080/225000 (35%)] Loss: 14955.193359\n",
      "Train Epoch: 9 [82176/225000 (37%)] Loss: 10359.785156\n",
      "Train Epoch: 9 [86272/225000 (38%)] Loss: 18226.033203\n",
      "Train Epoch: 9 [90368/225000 (40%)] Loss: 15207.203125\n",
      "Train Epoch: 9 [94464/225000 (42%)] Loss: 22491.560547\n",
      "Train Epoch: 9 [98560/225000 (44%)] Loss: 10511.324219\n",
      "Train Epoch: 9 [102656/225000 (46%)] Loss: 14814.742188\n",
      "Train Epoch: 9 [106752/225000 (47%)] Loss: 10625.628906\n",
      "Train Epoch: 9 [110848/225000 (49%)] Loss: 14945.810547\n",
      "Train Epoch: 9 [114944/225000 (51%)] Loss: 10077.855469\n",
      "Train Epoch: 9 [119040/225000 (53%)] Loss: 15931.207031\n",
      "Train Epoch: 9 [123136/225000 (55%)] Loss: 18007.429688\n",
      "Train Epoch: 9 [127232/225000 (57%)] Loss: 22499.080078\n",
      "Train Epoch: 9 [131328/225000 (58%)] Loss: 17894.423828\n",
      "Train Epoch: 9 [135424/225000 (60%)] Loss: 27877.726562\n",
      "Train Epoch: 9 [139520/225000 (62%)] Loss: 10418.505859\n",
      "Train Epoch: 9 [143616/225000 (64%)] Loss: 28244.476562\n",
      "Train Epoch: 9 [147712/225000 (66%)] Loss: 10804.925781\n",
      "Train Epoch: 9 [151808/225000 (67%)] Loss: 10584.644531\n",
      "Train Epoch: 9 [155904/225000 (69%)] Loss: 15376.271484\n",
      "Train Epoch: 9 [160000/225000 (71%)] Loss: 17523.109375\n",
      "Train Epoch: 9 [164096/225000 (73%)] Loss: 10312.009766\n",
      "Train Epoch: 9 [168192/225000 (75%)] Loss: 19753.126953\n",
      "Train Epoch: 9 [172288/225000 (77%)] Loss: 10428.066406\n",
      "Train Epoch: 9 [176384/225000 (78%)] Loss: 10440.628906\n",
      "Train Epoch: 9 [180480/225000 (80%)] Loss: 17538.775391\n",
      "Train Epoch: 9 [184576/225000 (82%)] Loss: 14900.376953\n",
      "Train Epoch: 9 [188672/225000 (84%)] Loss: 10279.656250\n",
      "Train Epoch: 9 [192768/225000 (86%)] Loss: 10356.251953\n",
      "Train Epoch: 9 [196864/225000 (87%)] Loss: 15046.521484\n",
      "Train Epoch: 9 [200960/225000 (89%)] Loss: 10423.078125\n",
      "Train Epoch: 9 [205056/225000 (91%)] Loss: 17472.136719\n",
      "Train Epoch: 9 [209152/225000 (93%)] Loss: 24912.458984\n",
      "Train Epoch: 9 [213248/225000 (95%)] Loss: 15194.310547\n",
      "Train Epoch: 9 [217344/225000 (97%)] Loss: 15339.916016\n",
      "Train Epoch: 9 [221440/225000 (98%)] Loss: 10147.365234\n",
      "    epoch          : 9\n",
      "    loss           : 14754.08815504124\n",
      "    val_loss       : 16101.55443220529\n",
      "Train Epoch: 10 [256/225000 (0%)] Loss: 14872.900391\n",
      "Train Epoch: 10 [4352/225000 (2%)] Loss: 19492.722656\n",
      "Train Epoch: 10 [8448/225000 (4%)] Loss: 15009.644531\n",
      "Train Epoch: 10 [12544/225000 (6%)] Loss: 14978.867188\n",
      "Train Epoch: 10 [16640/225000 (7%)] Loss: 14878.904297\n",
      "Train Epoch: 10 [20736/225000 (9%)] Loss: 10371.964844\n",
      "Train Epoch: 10 [24832/225000 (11%)] Loss: 14950.943359\n",
      "Train Epoch: 10 [28928/225000 (13%)] Loss: 10185.767578\n",
      "Train Epoch: 10 [33024/225000 (15%)] Loss: 10204.298828\n",
      "Train Epoch: 10 [37120/225000 (16%)] Loss: 15815.406250\n",
      "Train Epoch: 10 [41216/225000 (18%)] Loss: 19539.246094\n",
      "Train Epoch: 10 [45312/225000 (20%)] Loss: 14779.933594\n",
      "Train Epoch: 10 [49408/225000 (22%)] Loss: 21926.390625\n",
      "Train Epoch: 10 [53504/225000 (24%)] Loss: 10145.859375\n",
      "Train Epoch: 10 [57600/225000 (26%)] Loss: 19449.490234\n",
      "Train Epoch: 10 [61696/225000 (27%)] Loss: 19238.152344\n",
      "Train Epoch: 10 [65792/225000 (29%)] Loss: 14743.837891\n",
      "Train Epoch: 10 [69888/225000 (31%)] Loss: 10140.271484\n",
      "Train Epoch: 10 [73984/225000 (33%)] Loss: 14891.164062\n",
      "Train Epoch: 10 [78080/225000 (35%)] Loss: 14477.898438\n",
      "Train Epoch: 10 [82176/225000 (37%)] Loss: 10281.546875\n",
      "Train Epoch: 10 [86272/225000 (38%)] Loss: 14593.759766\n",
      "Train Epoch: 10 [90368/225000 (40%)] Loss: 14714.335938\n",
      "Train Epoch: 10 [94464/225000 (42%)] Loss: 16551.933594\n",
      "Train Epoch: 10 [98560/225000 (44%)] Loss: 14693.527344\n",
      "Train Epoch: 10 [102656/225000 (46%)] Loss: 21660.648438\n",
      "Train Epoch: 10 [106752/225000 (47%)] Loss: 25717.392578\n",
      "Train Epoch: 10 [110848/225000 (49%)] Loss: 14726.777344\n",
      "Train Epoch: 10 [114944/225000 (51%)] Loss: 14678.423828\n",
      "Train Epoch: 10 [119040/225000 (53%)] Loss: 10577.724609\n",
      "Train Epoch: 10 [123136/225000 (55%)] Loss: 19348.349609\n",
      "Train Epoch: 10 [127232/225000 (57%)] Loss: 10954.154297\n",
      "Train Epoch: 10 [131328/225000 (58%)] Loss: 10304.031250\n",
      "Train Epoch: 10 [135424/225000 (60%)] Loss: 14547.287109\n",
      "Train Epoch: 10 [139520/225000 (62%)] Loss: 10051.074219\n",
      "Train Epoch: 10 [143616/225000 (64%)] Loss: 14711.673828\n",
      "Train Epoch: 10 [147712/225000 (66%)] Loss: 18807.882812\n",
      "Train Epoch: 10 [151808/225000 (67%)] Loss: 17135.867188\n",
      "Train Epoch: 10 [155904/225000 (69%)] Loss: 14725.140625\n",
      "Train Epoch: 10 [160000/225000 (71%)] Loss: 17087.437500\n",
      "Train Epoch: 10 [164096/225000 (73%)] Loss: 9941.900391\n",
      "Train Epoch: 10 [168192/225000 (75%)] Loss: 10234.849609\n",
      "Train Epoch: 10 [172288/225000 (77%)] Loss: 14812.060547\n",
      "Train Epoch: 10 [176384/225000 (78%)] Loss: 14423.248047\n",
      "Train Epoch: 10 [180480/225000 (80%)] Loss: 10115.505859\n",
      "Train Epoch: 10 [184576/225000 (82%)] Loss: 18904.023438\n",
      "Train Epoch: 10 [188672/225000 (84%)] Loss: 16947.175781\n",
      "Train Epoch: 10 [192768/225000 (86%)] Loss: 10083.857422\n",
      "Train Epoch: 10 [196864/225000 (87%)] Loss: 10247.232422\n",
      "Train Epoch: 10 [200960/225000 (89%)] Loss: 14104.314453\n",
      "Train Epoch: 10 [205056/225000 (91%)] Loss: 15407.365234\n",
      "Train Epoch: 10 [209152/225000 (93%)] Loss: 10073.621094\n",
      "Train Epoch: 10 [213248/225000 (95%)] Loss: 14247.113281\n",
      "Train Epoch: 10 [217344/225000 (97%)] Loss: 10078.771484\n",
      "Train Epoch: 10 [221440/225000 (98%)] Loss: 10203.675781\n",
      "    epoch          : 10\n",
      "    loss           : 14478.094203284983\n",
      "    val_loss       : 13096.713926447805\n",
      "Train Epoch: 11 [256/225000 (0%)] Loss: 10052.251953\n",
      "Train Epoch: 11 [4352/225000 (2%)] Loss: 14266.320312\n",
      "Train Epoch: 11 [8448/225000 (4%)] Loss: 14377.542969\n",
      "Train Epoch: 11 [12544/225000 (6%)] Loss: 14044.199219\n",
      "Train Epoch: 11 [16640/225000 (7%)] Loss: 18327.851562\n",
      "Train Epoch: 11 [20736/225000 (9%)] Loss: 10081.789062\n",
      "Train Epoch: 11 [24832/225000 (11%)] Loss: 9947.644531\n",
      "Train Epoch: 11 [28928/225000 (13%)] Loss: 14079.603516\n",
      "Train Epoch: 11 [33024/225000 (15%)] Loss: 9916.035156\n",
      "Train Epoch: 11 [37120/225000 (16%)] Loss: 9965.705078\n",
      "Train Epoch: 11 [41216/225000 (18%)] Loss: 14047.476562\n",
      "Train Epoch: 11 [45312/225000 (20%)] Loss: 18189.457031\n",
      "Train Epoch: 11 [49408/225000 (22%)] Loss: 9820.167969\n",
      "Train Epoch: 11 [53504/225000 (24%)] Loss: 10250.431641\n",
      "Train Epoch: 11 [57600/225000 (26%)] Loss: 9966.716797\n",
      "Train Epoch: 11 [61696/225000 (27%)] Loss: 13813.755859\n",
      "Train Epoch: 11 [65792/225000 (29%)] Loss: 10138.126953\n",
      "Train Epoch: 11 [69888/225000 (31%)] Loss: 13699.904297\n",
      "Train Epoch: 11 [73984/225000 (33%)] Loss: 10182.021484\n",
      "Train Epoch: 11 [78080/225000 (35%)] Loss: 10028.759766\n",
      "Train Epoch: 11 [82176/225000 (37%)] Loss: 13477.017578\n",
      "Train Epoch: 11 [86272/225000 (38%)] Loss: 17051.552734\n",
      "Train Epoch: 11 [90368/225000 (40%)] Loss: 20396.339844\n",
      "Train Epoch: 11 [94464/225000 (42%)] Loss: 10005.894531\n",
      "Train Epoch: 11 [98560/225000 (44%)] Loss: 14300.156250\n",
      "Train Epoch: 11 [102656/225000 (46%)] Loss: 10142.019531\n",
      "Train Epoch: 11 [106752/225000 (47%)] Loss: 16420.128906\n",
      "Train Epoch: 11 [110848/225000 (49%)] Loss: 9938.183594\n",
      "Train Epoch: 11 [114944/225000 (51%)] Loss: 18982.150391\n",
      "Train Epoch: 11 [119040/225000 (53%)] Loss: 13536.525391\n",
      "Train Epoch: 11 [123136/225000 (55%)] Loss: 9996.666016\n",
      "Train Epoch: 11 [127232/225000 (57%)] Loss: 13332.111328\n",
      "Train Epoch: 11 [131328/225000 (58%)] Loss: 9939.273438\n",
      "Train Epoch: 11 [135424/225000 (60%)] Loss: 13300.365234\n",
      "Train Epoch: 11 [139520/225000 (62%)] Loss: 10096.589844\n",
      "Train Epoch: 11 [143616/225000 (64%)] Loss: 13179.148438\n",
      "Train Epoch: 11 [147712/225000 (66%)] Loss: 13208.000000\n",
      "Train Epoch: 11 [151808/225000 (67%)] Loss: 16186.318359\n",
      "Train Epoch: 11 [155904/225000 (69%)] Loss: 13129.181641\n",
      "Train Epoch: 11 [160000/225000 (71%)] Loss: 19932.771484\n",
      "Train Epoch: 11 [164096/225000 (73%)] Loss: 9949.736328\n",
      "Train Epoch: 11 [168192/225000 (75%)] Loss: 10019.248047\n",
      "Train Epoch: 11 [172288/225000 (77%)] Loss: 10052.507812\n",
      "Train Epoch: 11 [176384/225000 (78%)] Loss: 19448.357422\n",
      "Train Epoch: 11 [180480/225000 (80%)] Loss: 12875.773438\n",
      "Train Epoch: 11 [184576/225000 (82%)] Loss: 9659.503906\n",
      "Train Epoch: 11 [188672/225000 (84%)] Loss: 9892.546875\n",
      "Train Epoch: 11 [192768/225000 (86%)] Loss: 10137.173828\n",
      "Train Epoch: 11 [196864/225000 (87%)] Loss: 12705.933594\n",
      "Train Epoch: 11 [200960/225000 (89%)] Loss: 16765.619141\n",
      "Train Epoch: 11 [205056/225000 (91%)] Loss: 9765.878906\n",
      "Train Epoch: 11 [209152/225000 (93%)] Loss: 12473.632812\n",
      "Train Epoch: 11 [213248/225000 (95%)] Loss: 9983.183594\n",
      "Train Epoch: 11 [217344/225000 (97%)] Loss: 9890.632812\n",
      "Train Epoch: 11 [221440/225000 (98%)] Loss: 12535.078125\n",
      "    epoch          : 11\n",
      "    loss           : 13560.115947632252\n",
      "    val_loss       : 12180.620698590668\n",
      "Train Epoch: 12 [256/225000 (0%)] Loss: 9625.246094\n",
      "Train Epoch: 12 [4352/225000 (2%)] Loss: 16190.699219\n",
      "Train Epoch: 12 [8448/225000 (4%)] Loss: 15938.064453\n",
      "Train Epoch: 12 [12544/225000 (6%)] Loss: 9815.259766\n",
      "Train Epoch: 12 [16640/225000 (7%)] Loss: 9889.058594\n",
      "Train Epoch: 12 [20736/225000 (9%)] Loss: 9645.072266\n",
      "Train Epoch: 12 [24832/225000 (11%)] Loss: 12296.916016\n",
      "Train Epoch: 12 [28928/225000 (13%)] Loss: 9816.718750\n",
      "Train Epoch: 12 [33024/225000 (15%)] Loss: 12511.699219\n",
      "Train Epoch: 12 [37120/225000 (16%)] Loss: 14746.152344\n",
      "Train Epoch: 12 [41216/225000 (18%)] Loss: 9958.435547\n",
      "Train Epoch: 12 [45312/225000 (20%)] Loss: 14281.068359\n",
      "Train Epoch: 12 [49408/225000 (22%)] Loss: 12268.777344\n",
      "Train Epoch: 12 [53504/225000 (24%)] Loss: 9818.490234\n",
      "Train Epoch: 12 [57600/225000 (26%)] Loss: 28716.150391\n",
      "Train Epoch: 12 [61696/225000 (27%)] Loss: 9877.923828\n",
      "Train Epoch: 12 [65792/225000 (29%)] Loss: 16267.365234\n",
      "Train Epoch: 12 [69888/225000 (31%)] Loss: 9834.033203\n",
      "Train Epoch: 12 [73984/225000 (33%)] Loss: 12146.574219\n",
      "Train Epoch: 12 [78080/225000 (35%)] Loss: 9778.197266\n",
      "Train Epoch: 12 [82176/225000 (37%)] Loss: 16338.205078\n",
      "Train Epoch: 12 [86272/225000 (38%)] Loss: 9697.673828\n",
      "Train Epoch: 12 [90368/225000 (40%)] Loss: 22860.779297\n",
      "Train Epoch: 12 [94464/225000 (42%)] Loss: 9673.576172\n",
      "Train Epoch: 12 [98560/225000 (44%)] Loss: 9728.779297\n",
      "Train Epoch: 12 [102656/225000 (46%)] Loss: 12040.093750\n",
      "Train Epoch: 12 [106752/225000 (47%)] Loss: 9856.880859\n",
      "Train Epoch: 12 [110848/225000 (49%)] Loss: 19862.734375\n",
      "Train Epoch: 12 [114944/225000 (51%)] Loss: 11816.212891\n",
      "Train Epoch: 12 [119040/225000 (53%)] Loss: 9873.132812\n",
      "Train Epoch: 12 [123136/225000 (55%)] Loss: 9605.757812\n",
      "Train Epoch: 12 [127232/225000 (57%)] Loss: 11828.708984\n",
      "Train Epoch: 12 [131328/225000 (58%)] Loss: 16129.587891\n",
      "Train Epoch: 12 [135424/225000 (60%)] Loss: 22888.619141\n",
      "Train Epoch: 12 [139520/225000 (62%)] Loss: 16366.949219\n",
      "Train Epoch: 12 [143616/225000 (64%)] Loss: 10022.021484\n",
      "Train Epoch: 12 [147712/225000 (66%)] Loss: 16283.876953\n",
      "Train Epoch: 12 [151808/225000 (67%)] Loss: 18499.005859\n",
      "Train Epoch: 12 [155904/225000 (69%)] Loss: 9696.041016\n",
      "Train Epoch: 12 [160000/225000 (71%)] Loss: 10073.169922\n",
      "Train Epoch: 12 [164096/225000 (73%)] Loss: 16103.025391\n",
      "Train Epoch: 12 [168192/225000 (75%)] Loss: 10446.083984\n",
      "Train Epoch: 12 [172288/225000 (77%)] Loss: 11744.855469\n",
      "Train Epoch: 12 [176384/225000 (78%)] Loss: 9779.123047\n",
      "Train Epoch: 12 [180480/225000 (80%)] Loss: 18135.480469\n",
      "Train Epoch: 12 [184576/225000 (82%)] Loss: 9723.833984\n",
      "Train Epoch: 12 [188672/225000 (84%)] Loss: 9900.748047\n",
      "Train Epoch: 12 [192768/225000 (86%)] Loss: 10088.611328\n",
      "Train Epoch: 12 [196864/225000 (87%)] Loss: 16361.427734\n",
      "Train Epoch: 12 [200960/225000 (89%)] Loss: 11163.099609\n",
      "Train Epoch: 12 [205056/225000 (91%)] Loss: 9621.847656\n",
      "Train Epoch: 12 [209152/225000 (93%)] Loss: 9788.484375\n",
      "Train Epoch: 12 [213248/225000 (95%)] Loss: 9776.634766\n",
      "Train Epoch: 12 [217344/225000 (97%)] Loss: 17876.542969\n",
      "Train Epoch: 12 [221440/225000 (98%)] Loss: 9793.003906\n",
      "    epoch          : 12\n",
      "    loss           : 13071.341359143913\n",
      "    val_loss       : 12947.41690335225\n",
      "Train Epoch: 13 [256/225000 (0%)] Loss: 9615.675781\n",
      "Train Epoch: 13 [4352/225000 (2%)] Loss: 9827.005859\n",
      "Train Epoch: 13 [8448/225000 (4%)] Loss: 9559.785156\n",
      "Train Epoch: 13 [12544/225000 (6%)] Loss: 9676.269531\n",
      "Train Epoch: 13 [16640/225000 (7%)] Loss: 9910.335938\n",
      "Train Epoch: 13 [20736/225000 (9%)] Loss: 9895.402344\n",
      "Train Epoch: 13 [24832/225000 (11%)] Loss: 9895.236328\n",
      "Train Epoch: 13 [28928/225000 (13%)] Loss: 12994.394531\n",
      "Train Epoch: 13 [33024/225000 (15%)] Loss: 29890.716797\n",
      "Train Epoch: 13 [37120/225000 (16%)] Loss: 9622.685547\n",
      "Train Epoch: 13 [41216/225000 (18%)] Loss: 15827.154297\n",
      "Train Epoch: 13 [45312/225000 (20%)] Loss: 11482.859375\n",
      "Train Epoch: 13 [49408/225000 (22%)] Loss: 9606.380859\n",
      "Train Epoch: 13 [53504/225000 (24%)] Loss: 12824.330078\n",
      "Train Epoch: 13 [57600/225000 (26%)] Loss: 17714.052734\n",
      "Train Epoch: 13 [61696/225000 (27%)] Loss: 9780.087891\n",
      "Train Epoch: 13 [65792/225000 (29%)] Loss: 9618.128906\n",
      "Train Epoch: 13 [69888/225000 (31%)] Loss: 15935.412109\n",
      "Train Epoch: 13 [73984/225000 (33%)] Loss: 15922.562500\n",
      "Train Epoch: 13 [78080/225000 (35%)] Loss: 14040.585938\n",
      "Train Epoch: 13 [82176/225000 (37%)] Loss: 18876.667969\n",
      "Train Epoch: 13 [86272/225000 (38%)] Loss: 18071.578125\n",
      "Train Epoch: 13 [90368/225000 (40%)] Loss: 9765.697266\n",
      "Train Epoch: 13 [94464/225000 (42%)] Loss: 9645.330078\n",
      "Train Epoch: 13 [98560/225000 (44%)] Loss: 9986.876953\n",
      "Train Epoch: 13 [102656/225000 (46%)] Loss: 12192.736328\n",
      "Train Epoch: 13 [106752/225000 (47%)] Loss: 11354.488281\n",
      "Train Epoch: 13 [110848/225000 (49%)] Loss: 15045.820312\n",
      "Train Epoch: 13 [114944/225000 (51%)] Loss: 11092.447266\n",
      "Train Epoch: 13 [119040/225000 (53%)] Loss: 9249.828125\n",
      "Train Epoch: 13 [123136/225000 (55%)] Loss: 9878.582031\n",
      "Train Epoch: 13 [127232/225000 (57%)] Loss: 9531.773438\n",
      "Train Epoch: 13 [131328/225000 (58%)] Loss: 15743.796875\n",
      "Train Epoch: 13 [135424/225000 (60%)] Loss: 13830.886719\n",
      "Train Epoch: 13 [139520/225000 (62%)] Loss: 9667.466797\n",
      "Train Epoch: 13 [143616/225000 (64%)] Loss: 9577.433594\n",
      "Train Epoch: 13 [147712/225000 (66%)] Loss: 14981.859375\n",
      "Train Epoch: 13 [151808/225000 (67%)] Loss: 11211.300781\n",
      "Train Epoch: 13 [155904/225000 (69%)] Loss: 9285.632812\n",
      "Train Epoch: 13 [160000/225000 (71%)] Loss: 9436.193359\n",
      "Train Epoch: 13 [164096/225000 (73%)] Loss: 9667.167969\n",
      "Train Epoch: 13 [168192/225000 (75%)] Loss: 12274.777344\n",
      "Train Epoch: 13 [172288/225000 (77%)] Loss: 13990.646484\n",
      "Train Epoch: 13 [176384/225000 (78%)] Loss: 11233.585938\n",
      "Train Epoch: 13 [180480/225000 (80%)] Loss: 11130.544922\n",
      "Train Epoch: 13 [184576/225000 (82%)] Loss: 16264.917969\n",
      "Train Epoch: 13 [188672/225000 (84%)] Loss: 11128.816406\n",
      "Train Epoch: 13 [192768/225000 (86%)] Loss: 9674.574219\n",
      "Train Epoch: 13 [196864/225000 (87%)] Loss: 11260.894531\n",
      "Train Epoch: 13 [200960/225000 (89%)] Loss: 9653.632812\n",
      "Train Epoch: 13 [205056/225000 (91%)] Loss: 9686.968750\n",
      "Train Epoch: 13 [209152/225000 (93%)] Loss: 10994.335938\n",
      "Train Epoch: 13 [213248/225000 (95%)] Loss: 9656.464844\n",
      "Train Epoch: 13 [217344/225000 (97%)] Loss: 9769.419922\n",
      "Train Epoch: 13 [221440/225000 (98%)] Loss: 24750.054688\n",
      "    epoch          : 13\n",
      "    loss           : 12714.326934015928\n",
      "    val_loss       : 12070.889742751511\n",
      "Train Epoch: 14 [256/225000 (0%)] Loss: 9617.523438\n",
      "Train Epoch: 14 [4352/225000 (2%)] Loss: 17343.884766\n",
      "Train Epoch: 14 [8448/225000 (4%)] Loss: 10888.263672\n",
      "Train Epoch: 14 [12544/225000 (6%)] Loss: 11087.226562\n",
      "Train Epoch: 14 [16640/225000 (7%)] Loss: 9538.009766\n",
      "Train Epoch: 14 [20736/225000 (9%)] Loss: 13874.802734\n",
      "Train Epoch: 14 [24832/225000 (11%)] Loss: 9565.896484\n",
      "Train Epoch: 14 [28928/225000 (13%)] Loss: 16598.929688\n",
      "Train Epoch: 14 [33024/225000 (15%)] Loss: 10912.917969\n",
      "Train Epoch: 14 [37120/225000 (16%)] Loss: 9609.566406\n",
      "Train Epoch: 14 [41216/225000 (18%)] Loss: 9423.513672\n",
      "Train Epoch: 14 [45312/225000 (20%)] Loss: 17273.306641\n",
      "Train Epoch: 14 [49408/225000 (22%)] Loss: 10808.236328\n",
      "Train Epoch: 14 [53504/225000 (24%)] Loss: 9456.244141\n",
      "Train Epoch: 14 [57600/225000 (26%)] Loss: 15975.574219\n",
      "Train Epoch: 14 [61696/225000 (27%)] Loss: 9310.716797\n",
      "Train Epoch: 14 [65792/225000 (29%)] Loss: 11058.431641\n",
      "Train Epoch: 14 [69888/225000 (31%)] Loss: 9691.673828\n",
      "Train Epoch: 14 [73984/225000 (33%)] Loss: 15172.583984\n",
      "Train Epoch: 14 [78080/225000 (35%)] Loss: 15139.544922\n",
      "Train Epoch: 14 [82176/225000 (37%)] Loss: 9610.455078\n",
      "Train Epoch: 14 [86272/225000 (38%)] Loss: 16113.509766\n",
      "Train Epoch: 14 [90368/225000 (40%)] Loss: 15576.919922\n",
      "Train Epoch: 14 [94464/225000 (42%)] Loss: 14747.771484\n",
      "Train Epoch: 14 [98560/225000 (44%)] Loss: 10818.998047\n",
      "Train Epoch: 14 [102656/225000 (46%)] Loss: 16744.962891\n",
      "Train Epoch: 14 [106752/225000 (47%)] Loss: 15492.962891\n",
      "Train Epoch: 14 [110848/225000 (49%)] Loss: 9574.972656\n",
      "Train Epoch: 14 [114944/225000 (51%)] Loss: 9593.833984\n",
      "Train Epoch: 14 [119040/225000 (53%)] Loss: 9792.939453\n",
      "Train Epoch: 14 [123136/225000 (55%)] Loss: 18183.777344\n",
      "Train Epoch: 14 [127232/225000 (57%)] Loss: 9405.679688\n",
      "Train Epoch: 14 [131328/225000 (58%)] Loss: 13739.197266\n",
      "Train Epoch: 14 [135424/225000 (60%)] Loss: 9515.289062\n",
      "Train Epoch: 14 [139520/225000 (62%)] Loss: 16451.736328\n",
      "Train Epoch: 14 [143616/225000 (64%)] Loss: 9489.976562\n",
      "Train Epoch: 14 [147712/225000 (66%)] Loss: 20975.656250\n",
      "Train Epoch: 14 [151808/225000 (67%)] Loss: 10524.533203\n",
      "Train Epoch: 14 [155904/225000 (69%)] Loss: 15789.923828\n",
      "Train Epoch: 14 [160000/225000 (71%)] Loss: 9397.089844\n",
      "Train Epoch: 14 [164096/225000 (73%)] Loss: 9766.818359\n",
      "Train Epoch: 14 [168192/225000 (75%)] Loss: 9678.412109\n",
      "Train Epoch: 14 [172288/225000 (77%)] Loss: 9510.957031\n",
      "Train Epoch: 14 [176384/225000 (78%)] Loss: 9444.402344\n",
      "Train Epoch: 14 [180480/225000 (80%)] Loss: 10902.105469\n",
      "Train Epoch: 14 [184576/225000 (82%)] Loss: 9470.310547\n",
      "Train Epoch: 14 [188672/225000 (84%)] Loss: 9554.119141\n",
      "Train Epoch: 14 [192768/225000 (86%)] Loss: 10673.300781\n",
      "Train Epoch: 14 [196864/225000 (87%)] Loss: 9621.326172\n",
      "Train Epoch: 14 [200960/225000 (89%)] Loss: 9795.191406\n",
      "Train Epoch: 14 [205056/225000 (91%)] Loss: 13632.658203\n",
      "Train Epoch: 14 [209152/225000 (93%)] Loss: 9574.572266\n",
      "Train Epoch: 14 [213248/225000 (95%)] Loss: 16841.906250\n",
      "Train Epoch: 14 [217344/225000 (97%)] Loss: 9390.554688\n",
      "Train Epoch: 14 [221440/225000 (98%)] Loss: 23027.105469\n",
      "    epoch          : 14\n",
      "    loss           : 12816.120800447952\n",
      "    val_loss       : 12671.741404243878\n",
      "Train Epoch: 15 [256/225000 (0%)] Loss: 10786.759766\n",
      "Train Epoch: 15 [4352/225000 (2%)] Loss: 9251.310547\n",
      "Train Epoch: 15 [8448/225000 (4%)] Loss: 21348.908203\n",
      "Train Epoch: 15 [12544/225000 (6%)] Loss: 9329.302734\n",
      "Train Epoch: 15 [16640/225000 (7%)] Loss: 10529.316406\n",
      "Train Epoch: 15 [20736/225000 (9%)] Loss: 18896.500000\n",
      "Train Epoch: 15 [24832/225000 (11%)] Loss: 14832.472656\n",
      "Train Epoch: 15 [28928/225000 (13%)] Loss: 11586.275391\n",
      "Train Epoch: 15 [33024/225000 (15%)] Loss: 9310.435547\n",
      "Train Epoch: 15 [37120/225000 (16%)] Loss: 10315.628906\n",
      "Train Epoch: 15 [41216/225000 (18%)] Loss: 16546.710938\n",
      "Train Epoch: 15 [45312/225000 (20%)] Loss: 9447.546875\n",
      "Train Epoch: 15 [49408/225000 (22%)] Loss: 9472.013672\n",
      "Train Epoch: 15 [53504/225000 (24%)] Loss: 18706.816406\n",
      "Train Epoch: 15 [57600/225000 (26%)] Loss: 14936.167969\n",
      "Train Epoch: 15 [61696/225000 (27%)] Loss: 13787.000000\n",
      "Train Epoch: 15 [65792/225000 (29%)] Loss: 9354.648438\n",
      "Train Epoch: 15 [69888/225000 (31%)] Loss: 13675.281250\n",
      "Train Epoch: 15 [73984/225000 (33%)] Loss: 9337.453125\n",
      "Train Epoch: 15 [78080/225000 (35%)] Loss: 10617.156250\n",
      "Train Epoch: 15 [82176/225000 (37%)] Loss: 9380.390625\n",
      "Train Epoch: 15 [86272/225000 (38%)] Loss: 9041.929688\n",
      "Train Epoch: 15 [90368/225000 (40%)] Loss: 9332.679688\n",
      "Train Epoch: 15 [94464/225000 (42%)] Loss: 14709.412109\n",
      "Train Epoch: 15 [98560/225000 (44%)] Loss: 13545.445312\n",
      "Train Epoch: 15 [102656/225000 (46%)] Loss: 14564.958984\n",
      "Train Epoch: 15 [106752/225000 (47%)] Loss: 9333.779297\n",
      "Train Epoch: 15 [110848/225000 (49%)] Loss: 15470.736328\n",
      "Train Epoch: 15 [114944/225000 (51%)] Loss: 18279.289062\n",
      "Train Epoch: 15 [119040/225000 (53%)] Loss: 14428.808594\n",
      "Train Epoch: 15 [123136/225000 (55%)] Loss: 15443.771484\n",
      "Train Epoch: 15 [127232/225000 (57%)] Loss: 9425.017578\n",
      "Train Epoch: 15 [131328/225000 (58%)] Loss: 17375.943359\n",
      "Train Epoch: 15 [135424/225000 (60%)] Loss: 13386.421875\n",
      "Train Epoch: 15 [139520/225000 (62%)] Loss: 9279.630859\n",
      "Train Epoch: 15 [143616/225000 (64%)] Loss: 19476.652344\n",
      "Train Epoch: 15 [147712/225000 (66%)] Loss: 10518.861328\n",
      "Train Epoch: 15 [151808/225000 (67%)] Loss: 9216.445312\n",
      "Train Epoch: 15 [155904/225000 (69%)] Loss: 11494.611328\n",
      "Train Epoch: 15 [160000/225000 (71%)] Loss: 9452.535156\n",
      "Train Epoch: 15 [164096/225000 (73%)] Loss: 9397.593750\n",
      "Train Epoch: 15 [168192/225000 (75%)] Loss: 10454.380859\n",
      "Train Epoch: 15 [172288/225000 (77%)] Loss: 16917.369141\n",
      "Train Epoch: 15 [176384/225000 (78%)] Loss: 10435.189453\n",
      "Train Epoch: 15 [180480/225000 (80%)] Loss: 11438.066406\n",
      "Train Epoch: 15 [184576/225000 (82%)] Loss: 9584.458984\n",
      "Train Epoch: 15 [188672/225000 (84%)] Loss: 9461.880859\n",
      "Train Epoch: 15 [192768/225000 (86%)] Loss: 9668.353516\n",
      "Train Epoch: 15 [196864/225000 (87%)] Loss: 20337.238281\n",
      "Train Epoch: 15 [200960/225000 (89%)] Loss: 14739.703125\n",
      "Train Epoch: 15 [205056/225000 (91%)] Loss: 9390.339844\n",
      "Train Epoch: 15 [209152/225000 (93%)] Loss: 10159.001953\n",
      "Train Epoch: 15 [213248/225000 (95%)] Loss: 15048.654297\n",
      "Train Epoch: 15 [217344/225000 (97%)] Loss: 10468.128906\n",
      "Train Epoch: 15 [221440/225000 (98%)] Loss: 9502.046875\n",
      "    epoch          : 15\n",
      "    loss           : 12980.485122698023\n",
      "    val_loss       : 12701.72980719683\n",
      "Train Epoch: 16 [256/225000 (0%)] Loss: 14328.083984\n",
      "Train Epoch: 16 [4352/225000 (2%)] Loss: 10280.853516\n",
      "Train Epoch: 16 [8448/225000 (4%)] Loss: 10177.257812\n",
      "Train Epoch: 16 [12544/225000 (6%)] Loss: 9386.267578\n",
      "Train Epoch: 16 [16640/225000 (7%)] Loss: 9087.847656\n",
      "Train Epoch: 16 [20736/225000 (9%)] Loss: 17110.060547\n",
      "Train Epoch: 16 [24832/225000 (11%)] Loss: 9949.593750\n",
      "Train Epoch: 16 [28928/225000 (13%)] Loss: 16977.390625\n",
      "Train Epoch: 16 [33024/225000 (15%)] Loss: 9642.697266\n",
      "Train Epoch: 16 [37120/225000 (16%)] Loss: 9372.585938\n",
      "Train Epoch: 16 [41216/225000 (18%)] Loss: 13160.136719\n",
      "Train Epoch: 16 [45312/225000 (20%)] Loss: 13100.103516\n",
      "Train Epoch: 16 [49408/225000 (22%)] Loss: 9180.103516\n",
      "Train Epoch: 16 [53504/225000 (24%)] Loss: 17070.283203\n",
      "Train Epoch: 16 [57600/225000 (26%)] Loss: 19567.490234\n",
      "Train Epoch: 16 [61696/225000 (27%)] Loss: 9433.236328\n",
      "Train Epoch: 16 [65792/225000 (29%)] Loss: 14620.109375\n",
      "Train Epoch: 16 [69888/225000 (31%)] Loss: 9374.611328\n",
      "Train Epoch: 16 [73984/225000 (33%)] Loss: 16758.265625\n",
      "Train Epoch: 16 [78080/225000 (35%)] Loss: 15028.136719\n",
      "Train Epoch: 16 [82176/225000 (37%)] Loss: 15891.601562\n",
      "Train Epoch: 16 [86272/225000 (38%)] Loss: 18878.935547\n",
      "Train Epoch: 16 [90368/225000 (40%)] Loss: 15094.968750\n",
      "Train Epoch: 16 [94464/225000 (42%)] Loss: 11165.966797\n",
      "Train Epoch: 16 [98560/225000 (44%)] Loss: 21675.720703\n",
      "Train Epoch: 16 [102656/225000 (46%)] Loss: 19412.695312\n",
      "Train Epoch: 16 [106752/225000 (47%)] Loss: 9471.349609\n",
      "Train Epoch: 16 [110848/225000 (49%)] Loss: 10187.880859\n",
      "Train Epoch: 16 [114944/225000 (51%)] Loss: 9474.175781\n",
      "Train Epoch: 16 [119040/225000 (53%)] Loss: 16583.681641\n",
      "Train Epoch: 16 [123136/225000 (55%)] Loss: 9236.867188\n",
      "Train Epoch: 16 [127232/225000 (57%)] Loss: 22304.390625\n",
      "Train Epoch: 16 [131328/225000 (58%)] Loss: 10164.126953\n",
      "Train Epoch: 16 [135424/225000 (60%)] Loss: 9177.654297\n",
      "Train Epoch: 16 [139520/225000 (62%)] Loss: 9125.037109\n",
      "Train Epoch: 16 [143616/225000 (64%)] Loss: 14257.664062\n",
      "Train Epoch: 16 [147712/225000 (66%)] Loss: 9292.585938\n",
      "Train Epoch: 16 [151808/225000 (67%)] Loss: 15677.488281\n",
      "Train Epoch: 16 [155904/225000 (69%)] Loss: 9969.578125\n",
      "Train Epoch: 16 [160000/225000 (71%)] Loss: 14835.044922\n",
      "Train Epoch: 16 [164096/225000 (73%)] Loss: 16658.226562\n",
      "Train Epoch: 16 [168192/225000 (75%)] Loss: 15374.964844\n",
      "Train Epoch: 16 [172288/225000 (77%)] Loss: 10845.587891\n",
      "Train Epoch: 16 [176384/225000 (78%)] Loss: 9522.750000\n",
      "Train Epoch: 16 [180480/225000 (80%)] Loss: 9201.158203\n",
      "Train Epoch: 16 [184576/225000 (82%)] Loss: 9160.005859\n",
      "Train Epoch: 16 [188672/225000 (84%)] Loss: 9520.380859\n",
      "Train Epoch: 16 [192768/225000 (86%)] Loss: 24490.933594\n",
      "Train Epoch: 16 [196864/225000 (87%)] Loss: 9229.263672\n",
      "Train Epoch: 16 [200960/225000 (89%)] Loss: 21273.943359\n",
      "Train Epoch: 16 [205056/225000 (91%)] Loss: 9555.451172\n",
      "Train Epoch: 16 [209152/225000 (93%)] Loss: 13658.060547\n",
      "Train Epoch: 16 [213248/225000 (95%)] Loss: 9572.263672\n",
      "Train Epoch: 16 [217344/225000 (97%)] Loss: 15787.730469\n",
      "Train Epoch: 16 [221440/225000 (98%)] Loss: 9074.339844\n",
      "    epoch          : 16\n",
      "    loss           : 12778.624856681954\n",
      "    val_loss       : 13413.110779003222\n",
      "Train Epoch: 17 [256/225000 (0%)] Loss: 9135.232422\n",
      "Train Epoch: 17 [4352/225000 (2%)] Loss: 10510.984375\n",
      "Train Epoch: 17 [8448/225000 (4%)] Loss: 10132.281250\n",
      "Train Epoch: 17 [12544/225000 (6%)] Loss: 20131.154297\n",
      "Train Epoch: 17 [16640/225000 (7%)] Loss: 10349.726562\n",
      "Train Epoch: 17 [20736/225000 (9%)] Loss: 21026.082031\n",
      "Train Epoch: 17 [24832/225000 (11%)] Loss: 10093.476562\n",
      "Train Epoch: 17 [28928/225000 (13%)] Loss: 20397.462891\n",
      "Train Epoch: 17 [33024/225000 (15%)] Loss: 11013.230469\n",
      "Train Epoch: 17 [37120/225000 (16%)] Loss: 15442.402344\n",
      "Train Epoch: 17 [41216/225000 (18%)] Loss: 14421.562500\n",
      "Train Epoch: 17 [45312/225000 (20%)] Loss: 10099.900391\n",
      "Train Epoch: 17 [49408/225000 (22%)] Loss: 10973.041016\n",
      "Train Epoch: 17 [53504/225000 (24%)] Loss: 14463.990234\n",
      "Train Epoch: 17 [57600/225000 (26%)] Loss: 14271.482422\n",
      "Train Epoch: 17 [61696/225000 (27%)] Loss: 9264.898438\n",
      "Train Epoch: 17 [65792/225000 (29%)] Loss: 15296.087891\n",
      "Train Epoch: 17 [69888/225000 (31%)] Loss: 9256.664062\n",
      "Train Epoch: 17 [73984/225000 (33%)] Loss: 10390.460938\n",
      "Train Epoch: 17 [78080/225000 (35%)] Loss: 11219.564453\n",
      "Train Epoch: 17 [82176/225000 (37%)] Loss: 9419.703125\n",
      "Train Epoch: 17 [86272/225000 (38%)] Loss: 16573.152344\n",
      "Train Epoch: 17 [90368/225000 (40%)] Loss: 9272.886719\n",
      "Train Epoch: 17 [94464/225000 (42%)] Loss: 10011.703125\n",
      "Train Epoch: 17 [98560/225000 (44%)] Loss: 12903.894531\n",
      "Train Epoch: 17 [102656/225000 (46%)] Loss: 10123.775391\n",
      "Train Epoch: 17 [106752/225000 (47%)] Loss: 15233.533203\n",
      "Train Epoch: 17 [110848/225000 (49%)] Loss: 14662.822266\n",
      "Train Epoch: 17 [114944/225000 (51%)] Loss: 9115.740234\n",
      "Train Epoch: 17 [119040/225000 (53%)] Loss: 10233.705078\n",
      "Train Epoch: 17 [123136/225000 (55%)] Loss: 9907.203125\n",
      "Train Epoch: 17 [127232/225000 (57%)] Loss: 12739.468750\n",
      "Train Epoch: 17 [131328/225000 (58%)] Loss: 21868.205078\n",
      "Train Epoch: 17 [135424/225000 (60%)] Loss: 9202.892578\n",
      "Train Epoch: 17 [139520/225000 (62%)] Loss: 16897.244141\n",
      "Train Epoch: 17 [143616/225000 (64%)] Loss: 10083.601562\n",
      "Train Epoch: 17 [147712/225000 (66%)] Loss: 9328.767578\n",
      "Train Epoch: 17 [151808/225000 (67%)] Loss: 9839.257812\n",
      "Train Epoch: 17 [155904/225000 (69%)] Loss: 15929.154297\n",
      "Train Epoch: 17 [160000/225000 (71%)] Loss: 10916.414062\n",
      "Train Epoch: 17 [164096/225000 (73%)] Loss: 8942.746094\n",
      "Train Epoch: 17 [168192/225000 (75%)] Loss: 10791.222656\n",
      "Train Epoch: 17 [172288/225000 (77%)] Loss: 15333.654297\n",
      "Train Epoch: 17 [176384/225000 (78%)] Loss: 15623.718750\n",
      "Train Epoch: 17 [180480/225000 (80%)] Loss: 9967.279297\n",
      "Train Epoch: 17 [184576/225000 (82%)] Loss: 15291.628906\n",
      "Train Epoch: 17 [188672/225000 (84%)] Loss: 9400.066406\n",
      "Train Epoch: 17 [192768/225000 (86%)] Loss: 14589.330078\n",
      "Train Epoch: 17 [196864/225000 (87%)] Loss: 21864.095703\n",
      "Train Epoch: 17 [200960/225000 (89%)] Loss: 13196.494141\n",
      "Train Epoch: 17 [205056/225000 (91%)] Loss: 16406.195312\n",
      "Train Epoch: 17 [209152/225000 (93%)] Loss: 19676.900391\n",
      "Train Epoch: 17 [213248/225000 (95%)] Loss: 10609.587891\n",
      "Train Epoch: 17 [217344/225000 (97%)] Loss: 9387.277344\n",
      "Train Epoch: 17 [221440/225000 (98%)] Loss: 9241.205078\n",
      "    epoch          : 17\n",
      "    loss           : 13776.451765145051\n",
      "    val_loss       : 12507.084263553308\n",
      "Train Epoch: 18 [256/225000 (0%)] Loss: 9070.650391\n",
      "Train Epoch: 18 [4352/225000 (2%)] Loss: 9074.433594\n",
      "Train Epoch: 18 [8448/225000 (4%)] Loss: 15962.638672\n",
      "Train Epoch: 18 [12544/225000 (6%)] Loss: 18195.908203\n",
      "Train Epoch: 18 [16640/225000 (7%)] Loss: 9126.789062\n",
      "Train Epoch: 18 [20736/225000 (9%)] Loss: 10636.542969\n",
      "Train Epoch: 18 [24832/225000 (11%)] Loss: 10035.941406\n",
      "Train Epoch: 18 [28928/225000 (13%)] Loss: 14426.117188\n",
      "Train Epoch: 18 [33024/225000 (15%)] Loss: 10729.863281\n",
      "Train Epoch: 18 [37120/225000 (16%)] Loss: 22244.927734\n",
      "Train Epoch: 18 [41216/225000 (18%)] Loss: 9992.046875\n",
      "Train Epoch: 18 [45312/225000 (20%)] Loss: 22019.150391\n",
      "Train Epoch: 18 [49408/225000 (22%)] Loss: 9148.269531\n",
      "Train Epoch: 18 [53504/225000 (24%)] Loss: 9406.074219\n",
      "Train Epoch: 18 [57600/225000 (26%)] Loss: 16331.296875\n",
      "Train Epoch: 18 [61696/225000 (27%)] Loss: 9995.302734\n",
      "Train Epoch: 18 [65792/225000 (29%)] Loss: 20388.906250\n",
      "Train Epoch: 18 [69888/225000 (31%)] Loss: 15333.373047\n",
      "Train Epoch: 18 [73984/225000 (33%)] Loss: 10575.714844\n",
      "Train Epoch: 18 [78080/225000 (35%)] Loss: 9206.050781\n",
      "Train Epoch: 18 [82176/225000 (37%)] Loss: 9185.171875\n",
      "Train Epoch: 18 [86272/225000 (38%)] Loss: 9777.365234\n",
      "Train Epoch: 18 [90368/225000 (40%)] Loss: 21610.037109\n",
      "Train Epoch: 18 [94464/225000 (42%)] Loss: 9953.472656\n",
      "Train Epoch: 18 [98560/225000 (44%)] Loss: 9225.949219\n",
      "Train Epoch: 18 [102656/225000 (46%)] Loss: 14562.261719\n",
      "Train Epoch: 18 [106752/225000 (47%)] Loss: 9704.414062\n",
      "Train Epoch: 18 [110848/225000 (49%)] Loss: 9790.734375\n",
      "Train Epoch: 18 [114944/225000 (51%)] Loss: 17237.658203\n",
      "Train Epoch: 18 [119040/225000 (53%)] Loss: 15023.853516\n",
      "Train Epoch: 18 [123136/225000 (55%)] Loss: 9177.787109\n",
      "Train Epoch: 18 [127232/225000 (57%)] Loss: 17610.431641\n",
      "Train Epoch: 18 [131328/225000 (58%)] Loss: 9018.521484\n",
      "Train Epoch: 18 [135424/225000 (60%)] Loss: 9242.275391\n",
      "Train Epoch: 18 [139520/225000 (62%)] Loss: 9939.890625\n",
      "Train Epoch: 18 [143616/225000 (64%)] Loss: 9259.505859\n",
      "Train Epoch: 18 [147712/225000 (66%)] Loss: 9882.892578\n",
      "Train Epoch: 18 [151808/225000 (67%)] Loss: 14840.394531\n",
      "Train Epoch: 18 [155904/225000 (69%)] Loss: 9681.091797\n",
      "Train Epoch: 18 [160000/225000 (71%)] Loss: 9699.867188\n",
      "Train Epoch: 18 [164096/225000 (73%)] Loss: 9677.808594\n",
      "Train Epoch: 18 [168192/225000 (75%)] Loss: 10562.828125\n",
      "Train Epoch: 18 [172288/225000 (77%)] Loss: 14482.130859\n",
      "Train Epoch: 18 [176384/225000 (78%)] Loss: 9757.359375\n",
      "Train Epoch: 18 [180480/225000 (80%)] Loss: 9131.789062\n",
      "Train Epoch: 18 [184576/225000 (82%)] Loss: 13001.939453\n",
      "Train Epoch: 18 [188672/225000 (84%)] Loss: 9586.041016\n",
      "Train Epoch: 18 [192768/225000 (86%)] Loss: 8798.849609\n",
      "Train Epoch: 18 [196864/225000 (87%)] Loss: 9655.910156\n",
      "Train Epoch: 18 [200960/225000 (89%)] Loss: 10067.869141\n",
      "Train Epoch: 18 [205056/225000 (91%)] Loss: 14222.242188\n",
      "Train Epoch: 18 [209152/225000 (93%)] Loss: 14773.007812\n",
      "Train Epoch: 18 [213248/225000 (95%)] Loss: 9030.609375\n",
      "Train Epoch: 18 [217344/225000 (97%)] Loss: 9275.822266\n",
      "Train Epoch: 18 [221440/225000 (98%)] Loss: 9094.775391\n",
      "    epoch          : 18\n",
      "    loss           : 12377.540229042235\n",
      "    val_loss       : 12862.461971589497\n",
      "Train Epoch: 19 [256/225000 (0%)] Loss: 10601.464844\n",
      "Train Epoch: 19 [4352/225000 (2%)] Loss: 14643.656250\n",
      "Train Epoch: 19 [8448/225000 (4%)] Loss: 9884.236328\n",
      "Train Epoch: 19 [12544/225000 (6%)] Loss: 9798.435547\n",
      "Train Epoch: 19 [16640/225000 (7%)] Loss: 14127.539062\n",
      "Train Epoch: 19 [20736/225000 (9%)] Loss: 8980.570312\n",
      "Train Epoch: 19 [24832/225000 (11%)] Loss: 14828.625000\n",
      "Train Epoch: 19 [28928/225000 (13%)] Loss: 8961.681641\n",
      "Train Epoch: 19 [33024/225000 (15%)] Loss: 8985.646484\n",
      "Train Epoch: 19 [37120/225000 (16%)] Loss: 9600.302734\n",
      "Train Epoch: 19 [41216/225000 (18%)] Loss: 15107.970703\n",
      "Train Epoch: 19 [45312/225000 (20%)] Loss: 15510.628906\n",
      "Train Epoch: 19 [49408/225000 (22%)] Loss: 22885.669922\n",
      "Train Epoch: 19 [53504/225000 (24%)] Loss: 12827.548828\n",
      "Train Epoch: 19 [57600/225000 (26%)] Loss: 25331.216797\n",
      "Train Epoch: 19 [61696/225000 (27%)] Loss: 17141.685547\n",
      "Train Epoch: 19 [65792/225000 (29%)] Loss: 15813.671875\n",
      "Train Epoch: 19 [69888/225000 (31%)] Loss: 9077.060547\n",
      "Train Epoch: 19 [73984/225000 (33%)] Loss: 16713.357422\n",
      "Train Epoch: 19 [78080/225000 (35%)] Loss: 9655.005859\n",
      "Train Epoch: 19 [82176/225000 (37%)] Loss: 9051.009766\n",
      "Train Epoch: 19 [86272/225000 (38%)] Loss: 9673.927734\n",
      "Train Epoch: 19 [90368/225000 (40%)] Loss: 9136.611328\n",
      "Train Epoch: 19 [94464/225000 (42%)] Loss: 9047.642578\n",
      "Train Epoch: 19 [98560/225000 (44%)] Loss: 9833.283203\n",
      "Train Epoch: 19 [102656/225000 (46%)] Loss: 9664.111328\n",
      "Train Epoch: 19 [106752/225000 (47%)] Loss: 9194.751953\n",
      "Train Epoch: 19 [110848/225000 (49%)] Loss: 9017.671875\n",
      "Train Epoch: 19 [114944/225000 (51%)] Loss: 15485.039062\n",
      "Train Epoch: 19 [119040/225000 (53%)] Loss: 9096.091797\n",
      "Train Epoch: 19 [123136/225000 (55%)] Loss: 9641.205078\n",
      "Train Epoch: 19 [127232/225000 (57%)] Loss: 14030.429688\n",
      "Train Epoch: 19 [131328/225000 (58%)] Loss: 9410.039062\n",
      "Train Epoch: 19 [135424/225000 (60%)] Loss: 8902.119141\n",
      "Train Epoch: 19 [139520/225000 (62%)] Loss: 14920.791016\n",
      "Train Epoch: 19 [143616/225000 (64%)] Loss: 9066.589844\n",
      "Train Epoch: 19 [147712/225000 (66%)] Loss: 10192.589844\n",
      "Train Epoch: 19 [151808/225000 (67%)] Loss: 9848.189453\n",
      "Train Epoch: 19 [155904/225000 (69%)] Loss: 21756.117188\n",
      "Train Epoch: 19 [160000/225000 (71%)] Loss: 9201.824219\n",
      "Train Epoch: 19 [164096/225000 (73%)] Loss: 15618.822266\n",
      "Train Epoch: 19 [168192/225000 (75%)] Loss: 9584.412109\n",
      "Train Epoch: 19 [172288/225000 (77%)] Loss: 9083.486328\n",
      "Train Epoch: 19 [176384/225000 (78%)] Loss: 14532.998047\n",
      "Train Epoch: 19 [180480/225000 (80%)] Loss: 9021.962891\n",
      "Train Epoch: 19 [184576/225000 (82%)] Loss: 17254.736328\n",
      "Train Epoch: 19 [188672/225000 (84%)] Loss: 9509.406250\n",
      "Train Epoch: 19 [192768/225000 (86%)] Loss: 10420.138672\n",
      "Train Epoch: 19 [196864/225000 (87%)] Loss: 8958.646484\n",
      "Train Epoch: 19 [200960/225000 (89%)] Loss: 14257.101562\n",
      "Train Epoch: 19 [205056/225000 (91%)] Loss: 8719.343750\n",
      "Train Epoch: 19 [209152/225000 (93%)] Loss: 9651.638672\n",
      "Train Epoch: 19 [213248/225000 (95%)] Loss: 9114.027344\n",
      "Train Epoch: 19 [217344/225000 (97%)] Loss: 8932.787109\n",
      "Train Epoch: 19 [221440/225000 (98%)] Loss: 9082.000000\n",
      "    epoch          : 19\n",
      "    loss           : 11984.71828116112\n",
      "    val_loss       : 11032.350326367787\n",
      "Train Epoch: 20 [256/225000 (0%)] Loss: 9502.601562\n",
      "Train Epoch: 20 [4352/225000 (2%)] Loss: 9192.167969\n",
      "Train Epoch: 20 [8448/225000 (4%)] Loss: 8781.832031\n",
      "Train Epoch: 20 [12544/225000 (6%)] Loss: 9793.769531\n",
      "Train Epoch: 20 [16640/225000 (7%)] Loss: 8943.972656\n",
      "Train Epoch: 20 [20736/225000 (9%)] Loss: 9130.642578\n",
      "Train Epoch: 20 [24832/225000 (11%)] Loss: 15416.933594\n",
      "Train Epoch: 20 [28928/225000 (13%)] Loss: 17170.519531\n",
      "Train Epoch: 20 [33024/225000 (15%)] Loss: 9276.230469\n",
      "Train Epoch: 20 [37120/225000 (16%)] Loss: 9622.226562\n",
      "Train Epoch: 20 [41216/225000 (18%)] Loss: 9193.847656\n",
      "Train Epoch: 20 [45312/225000 (20%)] Loss: 8943.503906\n",
      "Train Epoch: 20 [49408/225000 (22%)] Loss: 9093.333984\n",
      "Train Epoch: 20 [53504/225000 (24%)] Loss: 21339.464844\n",
      "Train Epoch: 20 [57600/225000 (26%)] Loss: 8965.869141\n",
      "Train Epoch: 20 [61696/225000 (27%)] Loss: 15277.207031\n",
      "Train Epoch: 20 [65792/225000 (29%)] Loss: 8929.951172\n",
      "Train Epoch: 20 [69888/225000 (31%)] Loss: 9569.320312\n",
      "Train Epoch: 20 [73984/225000 (33%)] Loss: 16218.798828\n",
      "Train Epoch: 20 [78080/225000 (35%)] Loss: 21733.173828\n",
      "Train Epoch: 20 [82176/225000 (37%)] Loss: 8836.734375\n",
      "Train Epoch: 20 [86272/225000 (38%)] Loss: 8975.087891\n",
      "Train Epoch: 20 [90368/225000 (40%)] Loss: 8911.757812\n",
      "Train Epoch: 20 [94464/225000 (42%)] Loss: 9535.710938\n",
      "Train Epoch: 20 [98560/225000 (44%)] Loss: 9930.707031\n",
      "Train Epoch: 20 [102656/225000 (46%)] Loss: 9599.755859\n",
      "Train Epoch: 20 [106752/225000 (47%)] Loss: 9039.177734\n",
      "Train Epoch: 20 [110848/225000 (49%)] Loss: 8968.750000\n",
      "Train Epoch: 20 [114944/225000 (51%)] Loss: 8773.183594\n",
      "Train Epoch: 20 [119040/225000 (53%)] Loss: 9309.322266\n",
      "Train Epoch: 20 [123136/225000 (55%)] Loss: 9120.222656\n",
      "Train Epoch: 20 [127232/225000 (57%)] Loss: 9019.093750\n",
      "Train Epoch: 20 [131328/225000 (58%)] Loss: 8735.023438\n",
      "Train Epoch: 20 [135424/225000 (60%)] Loss: 14281.761719\n",
      "Train Epoch: 20 [139520/225000 (62%)] Loss: 9752.976562\n",
      "Train Epoch: 20 [143616/225000 (64%)] Loss: 8863.419922\n",
      "Train Epoch: 20 [147712/225000 (66%)] Loss: 9612.779297\n",
      "Train Epoch: 20 [151808/225000 (67%)] Loss: 9081.923828\n",
      "Train Epoch: 20 [155904/225000 (69%)] Loss: 9586.814453\n",
      "Train Epoch: 20 [160000/225000 (71%)] Loss: 8954.628906\n",
      "Train Epoch: 20 [164096/225000 (73%)] Loss: 20734.521484\n",
      "Train Epoch: 20 [168192/225000 (75%)] Loss: 14223.375000\n",
      "Train Epoch: 20 [172288/225000 (77%)] Loss: 14300.914062\n",
      "Train Epoch: 20 [176384/225000 (78%)] Loss: 13993.240234\n",
      "Train Epoch: 20 [180480/225000 (80%)] Loss: 22958.923828\n",
      "Train Epoch: 20 [184576/225000 (82%)] Loss: 14203.228516\n",
      "Train Epoch: 20 [188672/225000 (84%)] Loss: 9155.921875\n",
      "Train Epoch: 20 [192768/225000 (86%)] Loss: 9050.734375\n",
      "Train Epoch: 20 [196864/225000 (87%)] Loss: 9446.289062\n",
      "Train Epoch: 20 [200960/225000 (89%)] Loss: 13872.492188\n",
      "Train Epoch: 20 [205056/225000 (91%)] Loss: 10062.146484\n",
      "Train Epoch: 20 [209152/225000 (93%)] Loss: 9049.683594\n",
      "Train Epoch: 20 [213248/225000 (95%)] Loss: 8922.984375\n",
      "Train Epoch: 20 [217344/225000 (97%)] Loss: 9021.062500\n",
      "Train Epoch: 20 [221440/225000 (98%)] Loss: 9427.214844\n",
      "    epoch          : 20\n",
      "    loss           : 11228.943941535124\n",
      "    val_loss       : 11259.641258497628\n",
      "Train Epoch: 21 [256/225000 (0%)] Loss: 14034.644531\n",
      "Train Epoch: 21 [4352/225000 (2%)] Loss: 9117.902344\n",
      "Train Epoch: 21 [8448/225000 (4%)] Loss: 9607.654297\n",
      "Train Epoch: 21 [12544/225000 (6%)] Loss: 20255.425781\n",
      "Train Epoch: 21 [16640/225000 (7%)] Loss: 9128.025391\n",
      "Train Epoch: 21 [20736/225000 (9%)] Loss: 9517.962891\n",
      "Train Epoch: 21 [24832/225000 (11%)] Loss: 9532.488281\n",
      "Train Epoch: 21 [28928/225000 (13%)] Loss: 9058.435547\n",
      "Train Epoch: 21 [33024/225000 (15%)] Loss: 8859.181641\n",
      "Train Epoch: 21 [37120/225000 (16%)] Loss: 16691.958984\n",
      "Train Epoch: 21 [41216/225000 (18%)] Loss: 9503.031250\n",
      "Train Epoch: 21 [45312/225000 (20%)] Loss: 13970.554688\n",
      "Train Epoch: 21 [49408/225000 (22%)] Loss: 14190.271484\n",
      "Train Epoch: 21 [53504/225000 (24%)] Loss: 12270.123047\n",
      "Train Epoch: 21 [57600/225000 (26%)] Loss: 9577.476562\n",
      "Train Epoch: 21 [61696/225000 (27%)] Loss: 17130.937500\n",
      "Train Epoch: 21 [65792/225000 (29%)] Loss: 8914.843750\n",
      "Train Epoch: 21 [69888/225000 (31%)] Loss: 9059.882812\n",
      "Train Epoch: 21 [73984/225000 (33%)] Loss: 19353.085938\n",
      "Train Epoch: 21 [78080/225000 (35%)] Loss: 9586.138672\n",
      "Train Epoch: 21 [82176/225000 (37%)] Loss: 9041.208984\n",
      "Train Epoch: 21 [86272/225000 (38%)] Loss: 9310.710938\n",
      "Train Epoch: 21 [90368/225000 (40%)] Loss: 8888.853516\n",
      "Train Epoch: 21 [94464/225000 (42%)] Loss: 14186.734375\n",
      "Train Epoch: 21 [98560/225000 (44%)] Loss: 9679.564453\n",
      "Train Epoch: 21 [102656/225000 (46%)] Loss: 8907.748047\n",
      "Train Epoch: 21 [106752/225000 (47%)] Loss: 14021.054688\n",
      "Train Epoch: 21 [110848/225000 (49%)] Loss: 8802.539062\n",
      "Train Epoch: 21 [114944/225000 (51%)] Loss: 9744.755859\n",
      "Train Epoch: 21 [119040/225000 (53%)] Loss: 8996.398438\n",
      "Train Epoch: 21 [123136/225000 (55%)] Loss: 9852.902344\n",
      "Train Epoch: 21 [127232/225000 (57%)] Loss: 8924.902344\n",
      "Train Epoch: 21 [131328/225000 (58%)] Loss: 8868.550781\n",
      "Train Epoch: 21 [135424/225000 (60%)] Loss: 9050.917969\n",
      "Train Epoch: 21 [139520/225000 (62%)] Loss: 8957.144531\n",
      "Train Epoch: 21 [143616/225000 (64%)] Loss: 8874.699219\n",
      "Train Epoch: 21 [147712/225000 (66%)] Loss: 16630.003906\n",
      "Train Epoch: 21 [151808/225000 (67%)] Loss: 9130.091797\n",
      "Train Epoch: 21 [155904/225000 (69%)] Loss: 8954.394531\n",
      "Train Epoch: 21 [160000/225000 (71%)] Loss: 9613.177734\n",
      "Train Epoch: 21 [164096/225000 (73%)] Loss: 8702.398438\n",
      "Train Epoch: 21 [168192/225000 (75%)] Loss: 8955.531250\n",
      "Train Epoch: 21 [172288/225000 (77%)] Loss: 9167.900391\n",
      "Train Epoch: 21 [176384/225000 (78%)] Loss: 8779.677734\n",
      "Train Epoch: 21 [180480/225000 (80%)] Loss: 9496.488281\n",
      "Train Epoch: 21 [184576/225000 (82%)] Loss: 14229.296875\n",
      "Train Epoch: 21 [188672/225000 (84%)] Loss: 9058.347656\n",
      "Train Epoch: 21 [192768/225000 (86%)] Loss: 12397.615234\n",
      "Train Epoch: 21 [196864/225000 (87%)] Loss: 8978.929688\n",
      "Train Epoch: 21 [200960/225000 (89%)] Loss: 9142.689453\n",
      "Train Epoch: 21 [205056/225000 (91%)] Loss: 8838.705078\n",
      "Train Epoch: 21 [209152/225000 (93%)] Loss: 13794.353516\n",
      "Train Epoch: 21 [213248/225000 (95%)] Loss: 8955.429688\n",
      "Train Epoch: 21 [217344/225000 (97%)] Loss: 13892.984375\n",
      "Train Epoch: 21 [221440/225000 (98%)] Loss: 8913.244141\n",
      "    epoch          : 21\n",
      "    loss           : 11041.198001102104\n",
      "    val_loss       : 11126.044828373559\n",
      "Train Epoch: 22 [256/225000 (0%)] Loss: 8899.556641\n",
      "Train Epoch: 22 [4352/225000 (2%)] Loss: 9533.330078\n",
      "Train Epoch: 22 [8448/225000 (4%)] Loss: 8823.166016\n",
      "Train Epoch: 22 [12544/225000 (6%)] Loss: 9066.951172\n",
      "Train Epoch: 22 [16640/225000 (7%)] Loss: 9301.968750\n",
      "Train Epoch: 22 [20736/225000 (9%)] Loss: 16521.722656\n",
      "Train Epoch: 22 [24832/225000 (11%)] Loss: 9107.753906\n",
      "Train Epoch: 22 [28928/225000 (13%)] Loss: 13968.929688\n",
      "Train Epoch: 22 [33024/225000 (15%)] Loss: 12035.310547\n",
      "Train Epoch: 22 [37120/225000 (16%)] Loss: 8861.232422\n",
      "Train Epoch: 22 [41216/225000 (18%)] Loss: 8787.472656\n",
      "Train Epoch: 22 [45312/225000 (20%)] Loss: 14851.343750\n",
      "Train Epoch: 22 [49408/225000 (22%)] Loss: 8788.398438\n",
      "Train Epoch: 22 [53504/225000 (24%)] Loss: 20345.384766\n",
      "Train Epoch: 22 [57600/225000 (26%)] Loss: 8697.687500\n",
      "Train Epoch: 22 [61696/225000 (27%)] Loss: 13905.486328\n",
      "Train Epoch: 22 [65792/225000 (29%)] Loss: 11794.291016\n",
      "Train Epoch: 22 [69888/225000 (31%)] Loss: 14364.984375\n",
      "Train Epoch: 22 [73984/225000 (33%)] Loss: 9224.728516\n",
      "Train Epoch: 22 [78080/225000 (35%)] Loss: 8895.218750\n",
      "Train Epoch: 22 [82176/225000 (37%)] Loss: 8954.029297\n",
      "Train Epoch: 22 [86272/225000 (38%)] Loss: 28506.394531\n",
      "Train Epoch: 22 [90368/225000 (40%)] Loss: 15556.806641\n",
      "Train Epoch: 22 [94464/225000 (42%)] Loss: 8990.628906\n",
      "Train Epoch: 22 [98560/225000 (44%)] Loss: 9991.867188\n",
      "Train Epoch: 22 [102656/225000 (46%)] Loss: 15094.314453\n",
      "Train Epoch: 22 [106752/225000 (47%)] Loss: 9031.001953\n",
      "Train Epoch: 22 [110848/225000 (49%)] Loss: 9368.027344\n",
      "Train Epoch: 22 [114944/225000 (51%)] Loss: 8875.062500\n",
      "Train Epoch: 22 [119040/225000 (53%)] Loss: 8887.925781\n",
      "Train Epoch: 22 [123136/225000 (55%)] Loss: 8931.984375\n",
      "Train Epoch: 22 [127232/225000 (57%)] Loss: 8956.632812\n",
      "Train Epoch: 22 [131328/225000 (58%)] Loss: 8817.164062\n",
      "Train Epoch: 22 [135424/225000 (60%)] Loss: 13980.888672\n",
      "Train Epoch: 22 [139520/225000 (62%)] Loss: 20630.771484\n",
      "Train Epoch: 22 [143616/225000 (64%)] Loss: 8843.396484\n",
      "Train Epoch: 22 [147712/225000 (66%)] Loss: 13779.777344\n",
      "Train Epoch: 22 [151808/225000 (67%)] Loss: 8817.085938\n",
      "Train Epoch: 22 [155904/225000 (69%)] Loss: 13713.291016\n",
      "Train Epoch: 22 [160000/225000 (71%)] Loss: 9083.861328\n",
      "Train Epoch: 22 [164096/225000 (73%)] Loss: 8872.576172\n",
      "Train Epoch: 22 [168192/225000 (75%)] Loss: 8809.281250\n",
      "Train Epoch: 22 [172288/225000 (77%)] Loss: 9021.738281\n",
      "Train Epoch: 22 [176384/225000 (78%)] Loss: 8696.539062\n",
      "Train Epoch: 22 [180480/225000 (80%)] Loss: 20241.058594\n",
      "Train Epoch: 22 [184576/225000 (82%)] Loss: 8779.664062\n",
      "Train Epoch: 22 [188672/225000 (84%)] Loss: 14560.787109\n",
      "Train Epoch: 22 [192768/225000 (86%)] Loss: 8950.646484\n",
      "Train Epoch: 22 [196864/225000 (87%)] Loss: 9290.650391\n",
      "Train Epoch: 22 [200960/225000 (89%)] Loss: 14887.939453\n",
      "Train Epoch: 22 [205056/225000 (91%)] Loss: 8727.886719\n",
      "Train Epoch: 22 [209152/225000 (93%)] Loss: 8877.007812\n",
      "Train Epoch: 22 [213248/225000 (95%)] Loss: 12365.205078\n",
      "Train Epoch: 22 [217344/225000 (97%)] Loss: 9312.332031\n",
      "Train Epoch: 22 [221440/225000 (98%)] Loss: 8886.011719\n",
      "    epoch          : 22\n",
      "    loss           : 11095.399152979238\n",
      "    val_loss       : 11051.494591571847\n",
      "Train Epoch: 23 [256/225000 (0%)] Loss: 9040.298828\n",
      "Train Epoch: 23 [4352/225000 (2%)] Loss: 9090.683594\n",
      "Train Epoch: 23 [8448/225000 (4%)] Loss: 8953.216797\n",
      "Train Epoch: 23 [12544/225000 (6%)] Loss: 11742.384766\n",
      "Train Epoch: 23 [16640/225000 (7%)] Loss: 8656.521484\n",
      "Train Epoch: 23 [20736/225000 (9%)] Loss: 8772.876953\n",
      "Train Epoch: 23 [24832/225000 (11%)] Loss: 15408.425781\n",
      "Train Epoch: 23 [28928/225000 (13%)] Loss: 9260.996094\n",
      "Train Epoch: 23 [33024/225000 (15%)] Loss: 8815.880859\n",
      "Train Epoch: 23 [37120/225000 (16%)] Loss: 8987.257812\n",
      "Train Epoch: 23 [41216/225000 (18%)] Loss: 16489.455078\n",
      "Train Epoch: 23 [45312/225000 (20%)] Loss: 9080.855469\n",
      "Train Epoch: 23 [49408/225000 (22%)] Loss: 8700.910156\n",
      "Train Epoch: 23 [53504/225000 (24%)] Loss: 13989.746094\n",
      "Train Epoch: 23 [57600/225000 (26%)] Loss: 9343.542969\n",
      "Train Epoch: 23 [61696/225000 (27%)] Loss: 15602.242188\n",
      "Train Epoch: 23 [65792/225000 (29%)] Loss: 14405.455078\n",
      "Train Epoch: 23 [69888/225000 (31%)] Loss: 8627.812500\n",
      "Train Epoch: 23 [73984/225000 (33%)] Loss: 9410.794922\n",
      "Train Epoch: 23 [78080/225000 (35%)] Loss: 8791.818359\n",
      "Train Epoch: 23 [82176/225000 (37%)] Loss: 9352.298828\n",
      "Train Epoch: 23 [86272/225000 (38%)] Loss: 8994.046875\n",
      "Train Epoch: 23 [90368/225000 (40%)] Loss: 8747.105469\n",
      "Train Epoch: 23 [94464/225000 (42%)] Loss: 8915.734375\n",
      "Train Epoch: 23 [98560/225000 (44%)] Loss: 9176.701172\n",
      "Train Epoch: 23 [102656/225000 (46%)] Loss: 8831.980469\n",
      "Train Epoch: 23 [106752/225000 (47%)] Loss: 8766.041016\n",
      "Train Epoch: 23 [110848/225000 (49%)] Loss: 14916.791016\n",
      "Train Epoch: 23 [114944/225000 (51%)] Loss: 8809.943359\n",
      "Train Epoch: 23 [119040/225000 (53%)] Loss: 13725.640625\n",
      "Train Epoch: 23 [123136/225000 (55%)] Loss: 8774.257812\n",
      "Train Epoch: 23 [127232/225000 (57%)] Loss: 9358.410156\n",
      "Train Epoch: 23 [131328/225000 (58%)] Loss: 8687.363281\n",
      "Train Epoch: 23 [135424/225000 (60%)] Loss: 8729.669922\n",
      "Train Epoch: 23 [139520/225000 (62%)] Loss: 8766.925781\n",
      "Train Epoch: 23 [143616/225000 (64%)] Loss: 14675.318359\n",
      "Train Epoch: 23 [147712/225000 (66%)] Loss: 9028.558594\n",
      "Train Epoch: 23 [151808/225000 (67%)] Loss: 14580.244141\n",
      "Train Epoch: 23 [155904/225000 (69%)] Loss: 8904.308594\n",
      "Train Epoch: 23 [160000/225000 (71%)] Loss: 16224.970703\n",
      "Train Epoch: 23 [164096/225000 (73%)] Loss: 14076.445312\n",
      "Train Epoch: 23 [168192/225000 (75%)] Loss: 8890.207031\n",
      "Train Epoch: 23 [172288/225000 (77%)] Loss: 16661.962891\n",
      "Train Epoch: 23 [176384/225000 (78%)] Loss: 9247.998047\n",
      "Train Epoch: 23 [180480/225000 (80%)] Loss: 8823.734375\n",
      "Train Epoch: 23 [184576/225000 (82%)] Loss: 11569.917969\n",
      "Train Epoch: 23 [188672/225000 (84%)] Loss: 9145.833984\n",
      "Train Epoch: 23 [192768/225000 (86%)] Loss: 28357.566406\n",
      "Train Epoch: 23 [196864/225000 (87%)] Loss: 8931.453125\n",
      "Train Epoch: 23 [200960/225000 (89%)] Loss: 16228.730469\n",
      "Train Epoch: 23 [205056/225000 (91%)] Loss: 14010.755859\n",
      "Train Epoch: 23 [209152/225000 (93%)] Loss: 12204.851562\n",
      "Train Epoch: 23 [213248/225000 (95%)] Loss: 9215.064453\n",
      "Train Epoch: 23 [217344/225000 (97%)] Loss: 11718.072266\n",
      "Train Epoch: 23 [221440/225000 (98%)] Loss: 9351.601562\n",
      "    epoch          : 23\n",
      "    loss           : 11044.58998817904\n",
      "    val_loss       : 10860.738689923774\n",
      "Train Epoch: 24 [256/225000 (0%)] Loss: 8803.177734\n",
      "Train Epoch: 24 [4352/225000 (2%)] Loss: 8564.728516\n",
      "Train Epoch: 24 [8448/225000 (4%)] Loss: 16071.164062\n",
      "Train Epoch: 24 [12544/225000 (6%)] Loss: 14191.189453\n",
      "Train Epoch: 24 [16640/225000 (7%)] Loss: 8938.925781\n",
      "Train Epoch: 24 [20736/225000 (9%)] Loss: 13586.986328\n",
      "Train Epoch: 24 [24832/225000 (11%)] Loss: 9400.488281\n",
      "Train Epoch: 24 [28928/225000 (13%)] Loss: 8925.609375\n",
      "Train Epoch: 24 [33024/225000 (15%)] Loss: 9110.255859\n",
      "Train Epoch: 24 [37120/225000 (16%)] Loss: 14092.910156\n",
      "Train Epoch: 24 [41216/225000 (18%)] Loss: 10386.099609\n",
      "Train Epoch: 24 [45312/225000 (20%)] Loss: 8650.195312\n",
      "Train Epoch: 24 [49408/225000 (22%)] Loss: 8801.388672\n",
      "Train Epoch: 24 [53504/225000 (24%)] Loss: 8927.187500\n",
      "Train Epoch: 24 [57600/225000 (26%)] Loss: 9245.724609\n",
      "Train Epoch: 24 [61696/225000 (27%)] Loss: 8796.275391\n",
      "Train Epoch: 24 [65792/225000 (29%)] Loss: 8830.414062\n",
      "Train Epoch: 24 [69888/225000 (31%)] Loss: 9219.796875\n",
      "Train Epoch: 24 [73984/225000 (33%)] Loss: 9100.527344\n",
      "Train Epoch: 24 [78080/225000 (35%)] Loss: 13609.263672\n",
      "Train Epoch: 24 [82176/225000 (37%)] Loss: 14065.222656\n",
      "Train Epoch: 24 [86272/225000 (38%)] Loss: 9219.958984\n",
      "Train Epoch: 24 [90368/225000 (40%)] Loss: 8690.974609\n",
      "Train Epoch: 24 [94464/225000 (42%)] Loss: 14203.880859\n",
      "Train Epoch: 24 [98560/225000 (44%)] Loss: 8946.910156\n",
      "Train Epoch: 24 [102656/225000 (46%)] Loss: 8757.791016\n",
      "Train Epoch: 24 [106752/225000 (47%)] Loss: 8616.236328\n",
      "Train Epoch: 24 [110848/225000 (49%)] Loss: 8552.261719\n",
      "Train Epoch: 24 [114944/225000 (51%)] Loss: 13953.130859\n",
      "Train Epoch: 24 [119040/225000 (53%)] Loss: 8651.449219\n",
      "Train Epoch: 24 [123136/225000 (55%)] Loss: 10085.361328\n",
      "Train Epoch: 24 [127232/225000 (57%)] Loss: 13638.990234\n",
      "Train Epoch: 24 [131328/225000 (58%)] Loss: 14003.503906\n",
      "Train Epoch: 24 [135424/225000 (60%)] Loss: 11298.962891\n",
      "Train Epoch: 24 [139520/225000 (62%)] Loss: 15379.941406\n",
      "Train Epoch: 24 [143616/225000 (64%)] Loss: 14930.966797\n",
      "Train Epoch: 24 [147712/225000 (66%)] Loss: 8747.015625\n",
      "Train Epoch: 24 [151808/225000 (67%)] Loss: 8883.435547\n",
      "Train Epoch: 24 [155904/225000 (69%)] Loss: 13599.710938\n",
      "Train Epoch: 24 [160000/225000 (71%)] Loss: 8700.855469\n",
      "Train Epoch: 24 [164096/225000 (73%)] Loss: 13936.833984\n",
      "Train Epoch: 24 [168192/225000 (75%)] Loss: 13724.894531\n",
      "Train Epoch: 24 [172288/225000 (77%)] Loss: 9298.187500\n",
      "Train Epoch: 24 [176384/225000 (78%)] Loss: 9602.640625\n",
      "Train Epoch: 24 [180480/225000 (80%)] Loss: 14760.023438\n",
      "Train Epoch: 24 [184576/225000 (82%)] Loss: 8729.992188\n",
      "Train Epoch: 24 [188672/225000 (84%)] Loss: 8718.494141\n",
      "Train Epoch: 24 [192768/225000 (86%)] Loss: 14040.757812\n",
      "Train Epoch: 24 [196864/225000 (87%)] Loss: 9491.082031\n",
      "Train Epoch: 24 [200960/225000 (89%)] Loss: 8634.712891\n",
      "Train Epoch: 24 [205056/225000 (91%)] Loss: 13747.197266\n",
      "Train Epoch: 24 [209152/225000 (93%)] Loss: 9111.220703\n",
      "Train Epoch: 24 [213248/225000 (95%)] Loss: 16737.707031\n",
      "Train Epoch: 24 [217344/225000 (97%)] Loss: 14092.353516\n",
      "Train Epoch: 24 [221440/225000 (98%)] Loss: 13660.265625\n",
      "    epoch          : 24\n",
      "    loss           : 11400.596327502844\n",
      "    val_loss       : 11066.825565014567\n",
      "Train Epoch: 25 [256/225000 (0%)] Loss: 9069.556641\n",
      "Train Epoch: 25 [4352/225000 (2%)] Loss: 8852.402344\n",
      "Train Epoch: 25 [8448/225000 (4%)] Loss: 8606.968750\n",
      "Train Epoch: 25 [12544/225000 (6%)] Loss: 8751.054688\n",
      "Train Epoch: 25 [16640/225000 (7%)] Loss: 8543.656250\n",
      "Train Epoch: 25 [20736/225000 (9%)] Loss: 8755.623047\n",
      "Train Epoch: 25 [24832/225000 (11%)] Loss: 24593.046875\n",
      "Train Epoch: 25 [28928/225000 (13%)] Loss: 13658.833984\n",
      "Train Epoch: 25 [33024/225000 (15%)] Loss: 8966.181641\n",
      "Train Epoch: 25 [37120/225000 (16%)] Loss: 8666.011719\n",
      "Train Epoch: 25 [41216/225000 (18%)] Loss: 9043.025391\n",
      "Train Epoch: 25 [45312/225000 (20%)] Loss: 8669.449219\n",
      "Train Epoch: 25 [49408/225000 (22%)] Loss: 14388.900391\n",
      "Train Epoch: 25 [53504/225000 (24%)] Loss: 8832.583984\n",
      "Train Epoch: 25 [57600/225000 (26%)] Loss: 9463.226562\n",
      "Train Epoch: 25 [61696/225000 (27%)] Loss: 8987.169922\n",
      "Train Epoch: 25 [65792/225000 (29%)] Loss: 9450.111328\n",
      "Train Epoch: 25 [69888/225000 (31%)] Loss: 14347.125000\n",
      "Train Epoch: 25 [73984/225000 (33%)] Loss: 8695.416016\n",
      "Train Epoch: 25 [78080/225000 (35%)] Loss: 14073.472656\n",
      "Train Epoch: 25 [82176/225000 (37%)] Loss: 8766.195312\n",
      "Train Epoch: 25 [86272/225000 (38%)] Loss: 8694.708984\n",
      "Train Epoch: 25 [90368/225000 (40%)] Loss: 8929.033203\n",
      "Train Epoch: 25 [94464/225000 (42%)] Loss: 8900.794922\n",
      "Train Epoch: 25 [98560/225000 (44%)] Loss: 11682.531250\n",
      "Train Epoch: 25 [102656/225000 (46%)] Loss: 15944.453125\n",
      "Train Epoch: 25 [106752/225000 (47%)] Loss: 8889.347656\n",
      "Train Epoch: 25 [110848/225000 (49%)] Loss: 8911.158203\n",
      "Train Epoch: 25 [114944/225000 (51%)] Loss: 13817.929688\n",
      "Train Epoch: 25 [119040/225000 (53%)] Loss: 8532.683594\n",
      "Train Epoch: 25 [123136/225000 (55%)] Loss: 11927.826172\n",
      "Train Epoch: 25 [127232/225000 (57%)] Loss: 8994.300781\n",
      "Train Epoch: 25 [131328/225000 (58%)] Loss: 19307.001953\n",
      "Train Epoch: 25 [135424/225000 (60%)] Loss: 9166.980469\n",
      "Train Epoch: 25 [139520/225000 (62%)] Loss: 14501.882812\n",
      "Train Epoch: 25 [143616/225000 (64%)] Loss: 8912.365234\n",
      "Train Epoch: 25 [147712/225000 (66%)] Loss: 8800.347656\n",
      "Train Epoch: 25 [151808/225000 (67%)] Loss: 8759.457031\n",
      "Train Epoch: 25 [155904/225000 (69%)] Loss: 8739.951172\n",
      "Train Epoch: 25 [160000/225000 (71%)] Loss: 11401.029297\n",
      "Train Epoch: 25 [164096/225000 (73%)] Loss: 8631.664062\n",
      "Train Epoch: 25 [168192/225000 (75%)] Loss: 8872.632812\n",
      "Train Epoch: 25 [172288/225000 (77%)] Loss: 8611.515625\n",
      "Train Epoch: 25 [176384/225000 (78%)] Loss: 13709.396484\n",
      "Train Epoch: 25 [180480/225000 (80%)] Loss: 8599.164062\n",
      "Train Epoch: 25 [184576/225000 (82%)] Loss: 14008.070312\n",
      "Train Epoch: 25 [188672/225000 (84%)] Loss: 8754.664062\n",
      "Train Epoch: 25 [192768/225000 (86%)] Loss: 8912.812500\n",
      "Train Epoch: 25 [196864/225000 (87%)] Loss: 18929.212891\n",
      "Train Epoch: 25 [200960/225000 (89%)] Loss: 8442.910156\n",
      "Train Epoch: 25 [205056/225000 (91%)] Loss: 18791.462891\n",
      "Train Epoch: 25 [209152/225000 (93%)] Loss: 8651.376953\n",
      "Train Epoch: 25 [213248/225000 (95%)] Loss: 8821.875000\n",
      "Train Epoch: 25 [217344/225000 (97%)] Loss: 8690.382812\n",
      "Train Epoch: 25 [221440/225000 (98%)] Loss: 11346.789062\n",
      "    epoch          : 25\n",
      "    loss           : 11136.583313335466\n",
      "    val_loss       : 11307.705803187526\n",
      "Train Epoch: 26 [256/225000 (0%)] Loss: 8583.851562\n",
      "Train Epoch: 26 [4352/225000 (2%)] Loss: 8691.175781\n",
      "Train Epoch: 26 [8448/225000 (4%)] Loss: 8795.410156\n",
      "Train Epoch: 26 [12544/225000 (6%)] Loss: 8609.835938\n",
      "Train Epoch: 26 [16640/225000 (7%)] Loss: 13413.759766\n",
      "Train Epoch: 26 [20736/225000 (9%)] Loss: 8652.224609\n",
      "Train Epoch: 26 [24832/225000 (11%)] Loss: 8415.810547\n",
      "Train Epoch: 26 [28928/225000 (13%)] Loss: 14031.347656\n",
      "Train Epoch: 26 [33024/225000 (15%)] Loss: 8922.326172\n",
      "Train Epoch: 26 [37120/225000 (16%)] Loss: 8759.232422\n",
      "Train Epoch: 26 [41216/225000 (18%)] Loss: 8878.300781\n",
      "Train Epoch: 26 [45312/225000 (20%)] Loss: 11277.677734\n",
      "Train Epoch: 26 [49408/225000 (22%)] Loss: 8824.318359\n",
      "Train Epoch: 26 [53504/225000 (24%)] Loss: 8730.470703\n",
      "Train Epoch: 26 [57600/225000 (26%)] Loss: 13710.148438\n",
      "Train Epoch: 26 [61696/225000 (27%)] Loss: 8905.359375\n",
      "Train Epoch: 26 [65792/225000 (29%)] Loss: 8879.458984\n",
      "Train Epoch: 26 [69888/225000 (31%)] Loss: 8527.056641\n",
      "Train Epoch: 26 [73984/225000 (33%)] Loss: 8603.691406\n",
      "Train Epoch: 26 [78080/225000 (35%)] Loss: 8986.984375\n",
      "Train Epoch: 26 [82176/225000 (37%)] Loss: 8600.191406\n",
      "Train Epoch: 26 [86272/225000 (38%)] Loss: 8608.375000\n",
      "Train Epoch: 26 [90368/225000 (40%)] Loss: 8974.527344\n",
      "Train Epoch: 26 [94464/225000 (42%)] Loss: 8522.662109\n",
      "Train Epoch: 26 [98560/225000 (44%)] Loss: 13453.601562\n",
      "Train Epoch: 26 [102656/225000 (46%)] Loss: 13421.785156\n",
      "Train Epoch: 26 [106752/225000 (47%)] Loss: 8730.228516\n",
      "Train Epoch: 26 [110848/225000 (49%)] Loss: 8829.332031\n",
      "Train Epoch: 26 [114944/225000 (51%)] Loss: 8585.599609\n",
      "Train Epoch: 26 [119040/225000 (53%)] Loss: 13752.615234\n",
      "Train Epoch: 26 [123136/225000 (55%)] Loss: 16374.107422\n",
      "Train Epoch: 26 [127232/225000 (57%)] Loss: 18688.937500\n",
      "Train Epoch: 26 [131328/225000 (58%)] Loss: 13633.566406\n",
      "Train Epoch: 26 [135424/225000 (60%)] Loss: 15751.515625\n",
      "Train Epoch: 26 [139520/225000 (62%)] Loss: 13760.160156\n",
      "Train Epoch: 26 [143616/225000 (64%)] Loss: 8860.781250\n",
      "Train Epoch: 26 [147712/225000 (66%)] Loss: 8685.132812\n",
      "Train Epoch: 26 [151808/225000 (67%)] Loss: 8635.572266\n",
      "Train Epoch: 26 [155904/225000 (69%)] Loss: 8587.869141\n",
      "Train Epoch: 26 [160000/225000 (71%)] Loss: 14181.052734\n",
      "Train Epoch: 26 [164096/225000 (73%)] Loss: 15811.294922\n",
      "Train Epoch: 26 [168192/225000 (75%)] Loss: 8742.527344\n",
      "Train Epoch: 26 [172288/225000 (77%)] Loss: 8681.085938\n",
      "Train Epoch: 26 [176384/225000 (78%)] Loss: 15510.734375\n",
      "Train Epoch: 26 [180480/225000 (80%)] Loss: 8792.273438\n",
      "Train Epoch: 26 [184576/225000 (82%)] Loss: 8515.302734\n",
      "Train Epoch: 26 [188672/225000 (84%)] Loss: 8862.507812\n",
      "Train Epoch: 26 [192768/225000 (86%)] Loss: 13481.933594\n",
      "Train Epoch: 26 [196864/225000 (87%)] Loss: 13627.332031\n",
      "Train Epoch: 26 [200960/225000 (89%)] Loss: 8873.748047\n",
      "Train Epoch: 26 [205056/225000 (91%)] Loss: 8649.943359\n",
      "Train Epoch: 26 [209152/225000 (93%)] Loss: 8464.943359\n",
      "Train Epoch: 26 [213248/225000 (95%)] Loss: 13753.056641\n",
      "Train Epoch: 26 [217344/225000 (97%)] Loss: 8808.341797\n",
      "Train Epoch: 26 [221440/225000 (98%)] Loss: 13523.255859\n",
      "    epoch          : 26\n",
      "    loss           : 11009.17575569717\n",
      "    val_loss       : 11014.753496778254\n",
      "Train Epoch: 27 [256/225000 (0%)] Loss: 8689.847656\n",
      "Train Epoch: 27 [4352/225000 (2%)] Loss: 8654.923828\n",
      "Train Epoch: 27 [8448/225000 (4%)] Loss: 8711.332031\n",
      "Train Epoch: 27 [12544/225000 (6%)] Loss: 8707.445312\n",
      "Train Epoch: 27 [16640/225000 (7%)] Loss: 8634.392578\n",
      "Train Epoch: 27 [20736/225000 (9%)] Loss: 8566.277344\n",
      "Train Epoch: 27 [24832/225000 (11%)] Loss: 8731.083984\n",
      "Train Epoch: 27 [28928/225000 (13%)] Loss: 8717.224609\n",
      "Train Epoch: 27 [33024/225000 (15%)] Loss: 8478.197266\n",
      "Train Epoch: 27 [37120/225000 (16%)] Loss: 13502.419922\n",
      "Train Epoch: 27 [41216/225000 (18%)] Loss: 13222.898438\n",
      "Train Epoch: 27 [45312/225000 (20%)] Loss: 11322.884766\n",
      "Train Epoch: 27 [49408/225000 (22%)] Loss: 13531.150391\n",
      "Train Epoch: 27 [53504/225000 (24%)] Loss: 13454.949219\n",
      "Train Epoch: 27 [57600/225000 (26%)] Loss: 8485.828125\n",
      "Train Epoch: 27 [61696/225000 (27%)] Loss: 8597.041016\n",
      "Train Epoch: 27 [65792/225000 (29%)] Loss: 8556.580078\n",
      "Train Epoch: 27 [69888/225000 (31%)] Loss: 8737.718750\n",
      "Train Epoch: 27 [73984/225000 (33%)] Loss: 8352.244141\n",
      "Train Epoch: 27 [78080/225000 (35%)] Loss: 13378.732422\n",
      "Train Epoch: 27 [82176/225000 (37%)] Loss: 16110.312500\n",
      "Train Epoch: 27 [86272/225000 (38%)] Loss: 8600.369141\n",
      "Train Epoch: 27 [90368/225000 (40%)] Loss: 13535.042969\n",
      "Train Epoch: 27 [94464/225000 (42%)] Loss: 8807.886719\n",
      "Train Epoch: 27 [98560/225000 (44%)] Loss: 8743.027344\n",
      "Train Epoch: 27 [102656/225000 (46%)] Loss: 13410.753906\n",
      "Train Epoch: 27 [106752/225000 (47%)] Loss: 20895.035156\n",
      "Train Epoch: 27 [110848/225000 (49%)] Loss: 8592.460938\n",
      "Train Epoch: 27 [114944/225000 (51%)] Loss: 8508.359375\n",
      "Train Epoch: 27 [119040/225000 (53%)] Loss: 8409.613281\n",
      "Train Epoch: 27 [123136/225000 (55%)] Loss: 8636.136719\n",
      "Train Epoch: 27 [127232/225000 (57%)] Loss: 8529.613281\n",
      "Train Epoch: 27 [131328/225000 (58%)] Loss: 8778.511719\n",
      "Train Epoch: 27 [135424/225000 (60%)] Loss: 8502.886719\n",
      "Train Epoch: 27 [139520/225000 (62%)] Loss: 11014.189453\n",
      "Train Epoch: 27 [143616/225000 (64%)] Loss: 8687.095703\n",
      "Train Epoch: 27 [147712/225000 (66%)] Loss: 18022.781250\n",
      "Train Epoch: 27 [151808/225000 (67%)] Loss: 8494.291016\n",
      "Train Epoch: 27 [155904/225000 (69%)] Loss: 9216.169922\n",
      "Train Epoch: 27 [160000/225000 (71%)] Loss: 11288.144531\n",
      "Train Epoch: 27 [164096/225000 (73%)] Loss: 8696.572266\n",
      "Train Epoch: 27 [168192/225000 (75%)] Loss: 8583.837891\n",
      "Train Epoch: 27 [172288/225000 (77%)] Loss: 8549.980469\n",
      "Train Epoch: 27 [176384/225000 (78%)] Loss: 10941.833984\n",
      "Train Epoch: 27 [180480/225000 (80%)] Loss: 8692.373047\n",
      "Train Epoch: 27 [184576/225000 (82%)] Loss: 20386.355469\n",
      "Train Epoch: 27 [188672/225000 (84%)] Loss: 8473.343750\n",
      "Train Epoch: 27 [192768/225000 (86%)] Loss: 8513.216797\n",
      "Train Epoch: 27 [196864/225000 (87%)] Loss: 13318.644531\n",
      "Train Epoch: 27 [200960/225000 (89%)] Loss: 8720.644531\n",
      "Train Epoch: 27 [205056/225000 (91%)] Loss: 8638.208984\n",
      "Train Epoch: 27 [209152/225000 (93%)] Loss: 13301.287109\n",
      "Train Epoch: 27 [213248/225000 (95%)] Loss: 15962.009766\n",
      "Train Epoch: 27 [217344/225000 (97%)] Loss: 8630.042969\n",
      "Train Epoch: 27 [221440/225000 (98%)] Loss: 8639.218750\n",
      "    epoch          : 27\n",
      "    loss           : 10517.548412613765\n",
      "    val_loss       : 9904.257575769814\n",
      "Train Epoch: 28 [256/225000 (0%)] Loss: 8725.425781\n",
      "Train Epoch: 28 [4352/225000 (2%)] Loss: 8633.234375\n",
      "Train Epoch: 28 [8448/225000 (4%)] Loss: 8688.500000\n",
      "Train Epoch: 28 [12544/225000 (6%)] Loss: 8723.474609\n",
      "Train Epoch: 28 [16640/225000 (7%)] Loss: 8633.056641\n",
      "Train Epoch: 28 [20736/225000 (9%)] Loss: 10966.822266\n",
      "Train Epoch: 28 [24832/225000 (11%)] Loss: 13585.806641\n",
      "Train Epoch: 28 [28928/225000 (13%)] Loss: 10992.656250\n",
      "Train Epoch: 28 [33024/225000 (15%)] Loss: 8842.302734\n",
      "Train Epoch: 28 [37120/225000 (16%)] Loss: 8842.431641\n",
      "Train Epoch: 28 [41216/225000 (18%)] Loss: 11054.451172\n",
      "Train Epoch: 28 [45312/225000 (20%)] Loss: 8402.488281\n",
      "Train Epoch: 28 [49408/225000 (22%)] Loss: 8720.488281\n",
      "Train Epoch: 28 [53504/225000 (24%)] Loss: 8702.349609\n",
      "Train Epoch: 28 [57600/225000 (26%)] Loss: 8632.835938\n",
      "Train Epoch: 28 [61696/225000 (27%)] Loss: 13776.257812\n",
      "Train Epoch: 28 [65792/225000 (29%)] Loss: 13452.777344\n",
      "Train Epoch: 28 [69888/225000 (31%)] Loss: 10743.363281\n",
      "Train Epoch: 28 [73984/225000 (33%)] Loss: 8964.742188\n",
      "Train Epoch: 28 [78080/225000 (35%)] Loss: 8756.525391\n",
      "Train Epoch: 28 [82176/225000 (37%)] Loss: 8609.365234\n",
      "Train Epoch: 28 [86272/225000 (38%)] Loss: 8759.628906\n",
      "Train Epoch: 28 [90368/225000 (40%)] Loss: 8685.742188\n",
      "Train Epoch: 28 [94464/225000 (42%)] Loss: 8731.878906\n",
      "Train Epoch: 28 [98560/225000 (44%)] Loss: 16127.042969\n",
      "Train Epoch: 28 [102656/225000 (46%)] Loss: 12958.146484\n",
      "Train Epoch: 28 [106752/225000 (47%)] Loss: 8518.927734\n",
      "Train Epoch: 28 [110848/225000 (49%)] Loss: 10823.496094\n",
      "Train Epoch: 28 [114944/225000 (51%)] Loss: 8366.425781\n",
      "Train Epoch: 28 [119040/225000 (53%)] Loss: 8575.191406\n",
      "Train Epoch: 28 [123136/225000 (55%)] Loss: 10906.974609\n",
      "Train Epoch: 28 [127232/225000 (57%)] Loss: 8737.917969\n",
      "Train Epoch: 28 [131328/225000 (58%)] Loss: 8388.617188\n",
      "Train Epoch: 28 [135424/225000 (60%)] Loss: 13659.863281\n",
      "Train Epoch: 28 [139520/225000 (62%)] Loss: 8627.621094\n",
      "Train Epoch: 28 [143616/225000 (64%)] Loss: 13767.384766\n",
      "Train Epoch: 28 [147712/225000 (66%)] Loss: 8779.556641\n",
      "Train Epoch: 28 [151808/225000 (67%)] Loss: 8523.591797\n",
      "Train Epoch: 28 [155904/225000 (69%)] Loss: 8542.251953\n",
      "Train Epoch: 28 [160000/225000 (71%)] Loss: 8612.767578\n",
      "Train Epoch: 28 [164096/225000 (73%)] Loss: 15443.115234\n",
      "Train Epoch: 28 [168192/225000 (75%)] Loss: 8638.667969\n",
      "Train Epoch: 28 [172288/225000 (77%)] Loss: 8547.062500\n",
      "Train Epoch: 28 [176384/225000 (78%)] Loss: 13062.558594\n",
      "Train Epoch: 28 [180480/225000 (80%)] Loss: 8725.611328\n",
      "Train Epoch: 28 [184576/225000 (82%)] Loss: 8550.162109\n",
      "Train Epoch: 28 [188672/225000 (84%)] Loss: 8597.814453\n",
      "Train Epoch: 28 [192768/225000 (86%)] Loss: 8718.304688\n",
      "Train Epoch: 28 [196864/225000 (87%)] Loss: 8421.650391\n",
      "Train Epoch: 28 [200960/225000 (89%)] Loss: 8622.896484\n",
      "Train Epoch: 28 [205056/225000 (91%)] Loss: 8565.744141\n",
      "Train Epoch: 28 [209152/225000 (93%)] Loss: 8587.935547\n",
      "Train Epoch: 28 [213248/225000 (95%)] Loss: 13222.376953\n",
      "Train Epoch: 28 [217344/225000 (97%)] Loss: 8453.417969\n",
      "Train Epoch: 28 [221440/225000 (98%)] Loss: 8536.958984\n",
      "    epoch          : 28\n",
      "    loss           : 10186.823678807594\n",
      "    val_loss       : 10255.086649826595\n",
      "Train Epoch: 29 [256/225000 (0%)] Loss: 8672.835938\n",
      "Train Epoch: 29 [4352/225000 (2%)] Loss: 8572.121094\n",
      "Train Epoch: 29 [8448/225000 (4%)] Loss: 8496.519531\n",
      "Train Epoch: 29 [12544/225000 (6%)] Loss: 8504.345703\n",
      "Train Epoch: 29 [16640/225000 (7%)] Loss: 13246.162109\n",
      "Train Epoch: 29 [20736/225000 (9%)] Loss: 17957.107422\n",
      "Train Epoch: 29 [24832/225000 (11%)] Loss: 12914.234375\n",
      "Train Epoch: 29 [28928/225000 (13%)] Loss: 10831.900391\n",
      "Train Epoch: 29 [33024/225000 (15%)] Loss: 8691.621094\n",
      "Train Epoch: 29 [37120/225000 (16%)] Loss: 10677.074219\n",
      "Train Epoch: 29 [41216/225000 (18%)] Loss: 8846.292969\n",
      "Train Epoch: 29 [45312/225000 (20%)] Loss: 12986.216797\n",
      "Train Epoch: 29 [49408/225000 (22%)] Loss: 8522.435547\n",
      "Train Epoch: 29 [53504/225000 (24%)] Loss: 8583.980469\n",
      "Train Epoch: 29 [57600/225000 (26%)] Loss: 8457.904297\n",
      "Train Epoch: 29 [61696/225000 (27%)] Loss: 22783.460938\n",
      "Train Epoch: 29 [65792/225000 (29%)] Loss: 8519.955078\n",
      "Train Epoch: 29 [69888/225000 (31%)] Loss: 8540.847656\n",
      "Train Epoch: 29 [73984/225000 (33%)] Loss: 8566.386719\n",
      "Train Epoch: 29 [78080/225000 (35%)] Loss: 13472.529297\n",
      "Train Epoch: 29 [82176/225000 (37%)] Loss: 13087.287109\n",
      "Train Epoch: 29 [86272/225000 (38%)] Loss: 8546.867188\n",
      "Train Epoch: 29 [90368/225000 (40%)] Loss: 10801.355469\n",
      "Train Epoch: 29 [94464/225000 (42%)] Loss: 8379.650391\n",
      "Train Epoch: 29 [98560/225000 (44%)] Loss: 8464.863281\n",
      "Train Epoch: 29 [102656/225000 (46%)] Loss: 12938.230469\n",
      "Train Epoch: 29 [106752/225000 (47%)] Loss: 8567.589844\n",
      "Train Epoch: 29 [110848/225000 (49%)] Loss: 12973.414062\n",
      "Train Epoch: 29 [114944/225000 (51%)] Loss: 8495.642578\n",
      "Train Epoch: 29 [119040/225000 (53%)] Loss: 8589.984375\n",
      "Train Epoch: 29 [123136/225000 (55%)] Loss: 8432.876953\n",
      "Train Epoch: 29 [127232/225000 (57%)] Loss: 8474.746094\n",
      "Train Epoch: 29 [131328/225000 (58%)] Loss: 8386.867188\n",
      "Train Epoch: 29 [135424/225000 (60%)] Loss: 8594.558594\n",
      "Train Epoch: 29 [139520/225000 (62%)] Loss: 15358.962891\n",
      "Train Epoch: 29 [143616/225000 (64%)] Loss: 8484.111328\n",
      "Train Epoch: 29 [147712/225000 (66%)] Loss: 12861.886719\n",
      "Train Epoch: 29 [151808/225000 (67%)] Loss: 8391.855469\n",
      "Train Epoch: 29 [155904/225000 (69%)] Loss: 8469.369141\n",
      "Train Epoch: 29 [160000/225000 (71%)] Loss: 8561.884766\n",
      "Train Epoch: 29 [164096/225000 (73%)] Loss: 8245.621094\n",
      "Train Epoch: 29 [168192/225000 (75%)] Loss: 8912.300781\n",
      "Train Epoch: 29 [172288/225000 (77%)] Loss: 8645.445312\n",
      "Train Epoch: 29 [176384/225000 (78%)] Loss: 8203.974609\n",
      "Train Epoch: 29 [180480/225000 (80%)] Loss: 15381.162109\n",
      "Train Epoch: 29 [184576/225000 (82%)] Loss: 8475.125000\n",
      "Train Epoch: 29 [188672/225000 (84%)] Loss: 8310.822266\n",
      "Train Epoch: 29 [192768/225000 (86%)] Loss: 10913.451172\n",
      "Train Epoch: 29 [196864/225000 (87%)] Loss: 12815.828125\n",
      "Train Epoch: 29 [200960/225000 (89%)] Loss: 15197.826172\n",
      "Train Epoch: 29 [205056/225000 (91%)] Loss: 8529.134766\n",
      "Train Epoch: 29 [209152/225000 (93%)] Loss: 15170.839844\n",
      "Train Epoch: 29 [213248/225000 (95%)] Loss: 8431.123047\n",
      "Train Epoch: 29 [217344/225000 (97%)] Loss: 8659.990234\n",
      "Train Epoch: 29 [221440/225000 (98%)] Loss: 8606.894531\n",
      "    epoch          : 29\n",
      "    loss           : 10471.14077054003\n",
      "    val_loss       : 9958.478934852445\n",
      "Train Epoch: 30 [256/225000 (0%)] Loss: 13059.816406\n",
      "Train Epoch: 30 [4352/225000 (2%)] Loss: 12924.595703\n",
      "Train Epoch: 30 [8448/225000 (4%)] Loss: 8577.939453\n",
      "Train Epoch: 30 [12544/225000 (6%)] Loss: 8563.507812\n",
      "Train Epoch: 30 [16640/225000 (7%)] Loss: 8590.433594\n",
      "Train Epoch: 30 [20736/225000 (9%)] Loss: 8577.185547\n",
      "Train Epoch: 30 [24832/225000 (11%)] Loss: 8673.425781\n",
      "Train Epoch: 30 [28928/225000 (13%)] Loss: 8499.365234\n",
      "Train Epoch: 30 [33024/225000 (15%)] Loss: 8531.591797\n",
      "Train Epoch: 30 [37120/225000 (16%)] Loss: 8453.046875\n",
      "Train Epoch: 30 [41216/225000 (18%)] Loss: 8574.429688\n",
      "Train Epoch: 30 [45312/225000 (20%)] Loss: 8646.789062\n",
      "Train Epoch: 30 [49408/225000 (22%)] Loss: 8426.687500\n",
      "Train Epoch: 30 [53504/225000 (24%)] Loss: 8469.345703\n",
      "Train Epoch: 30 [57600/225000 (26%)] Loss: 8339.458984\n",
      "Train Epoch: 30 [61696/225000 (27%)] Loss: 12293.332031\n",
      "Train Epoch: 30 [65792/225000 (29%)] Loss: 8322.884766\n",
      "Train Epoch: 30 [69888/225000 (31%)] Loss: 8416.607422\n",
      "Train Epoch: 30 [73984/225000 (33%)] Loss: 8453.496094\n",
      "Train Epoch: 30 [78080/225000 (35%)] Loss: 8299.939453\n",
      "Train Epoch: 30 [82176/225000 (37%)] Loss: 13011.498047\n",
      "Train Epoch: 30 [86272/225000 (38%)] Loss: 8289.773438\n",
      "Train Epoch: 30 [90368/225000 (40%)] Loss: 8228.935547\n",
      "Train Epoch: 30 [94464/225000 (42%)] Loss: 8136.791016\n",
      "Train Epoch: 30 [98560/225000 (44%)] Loss: 15614.828125\n",
      "Train Epoch: 30 [102656/225000 (46%)] Loss: 8509.582031\n",
      "Train Epoch: 30 [106752/225000 (47%)] Loss: 17107.478516\n",
      "Train Epoch: 30 [110848/225000 (49%)] Loss: 9111.023438\n",
      "Train Epoch: 30 [114944/225000 (51%)] Loss: 8423.171875\n",
      "Train Epoch: 30 [119040/225000 (53%)] Loss: 8475.302734\n",
      "Train Epoch: 30 [123136/225000 (55%)] Loss: 8514.357422\n",
      "Train Epoch: 30 [127232/225000 (57%)] Loss: 8647.529297\n",
      "Train Epoch: 30 [131328/225000 (58%)] Loss: 8454.410156\n",
      "Train Epoch: 30 [135424/225000 (60%)] Loss: 8296.779297\n",
      "Train Epoch: 30 [139520/225000 (62%)] Loss: 8421.037109\n",
      "Train Epoch: 30 [143616/225000 (64%)] Loss: 8319.417969\n",
      "Train Epoch: 30 [147712/225000 (66%)] Loss: 8552.328125\n",
      "Train Epoch: 30 [151808/225000 (67%)] Loss: 8430.416016\n",
      "Train Epoch: 30 [155904/225000 (69%)] Loss: 10604.568359\n",
      "Train Epoch: 30 [160000/225000 (71%)] Loss: 8324.843750\n",
      "Train Epoch: 30 [164096/225000 (73%)] Loss: 10748.134766\n",
      "Train Epoch: 30 [168192/225000 (75%)] Loss: 8546.605469\n",
      "Train Epoch: 30 [172288/225000 (77%)] Loss: 8552.183594\n",
      "Train Epoch: 30 [176384/225000 (78%)] Loss: 8488.667969\n",
      "Train Epoch: 30 [180480/225000 (80%)] Loss: 8726.619141\n",
      "Train Epoch: 30 [184576/225000 (82%)] Loss: 12047.796875\n",
      "Train Epoch: 30 [188672/225000 (84%)] Loss: 10662.429688\n",
      "Train Epoch: 30 [192768/225000 (86%)] Loss: 15075.777344\n",
      "Train Epoch: 30 [196864/225000 (87%)] Loss: 8368.857422\n",
      "Train Epoch: 30 [200960/225000 (89%)] Loss: 8503.087891\n",
      "Train Epoch: 30 [205056/225000 (91%)] Loss: 8353.242188\n",
      "Train Epoch: 30 [209152/225000 (93%)] Loss: 14034.087891\n",
      "Train Epoch: 30 [213248/225000 (95%)] Loss: 12712.341797\n",
      "Train Epoch: 30 [217344/225000 (97%)] Loss: 12714.777344\n",
      "Train Epoch: 30 [221440/225000 (98%)] Loss: 8210.591797\n",
      "    epoch          : 30\n",
      "    loss           : 10170.514913964733\n",
      "    val_loss       : 10358.180267526179\n",
      "Train Epoch: 31 [256/225000 (0%)] Loss: 8421.732422\n",
      "Train Epoch: 31 [4352/225000 (2%)] Loss: 8439.126953\n",
      "Train Epoch: 31 [8448/225000 (4%)] Loss: 8462.451172\n",
      "Train Epoch: 31 [12544/225000 (6%)] Loss: 8567.566406\n",
      "Train Epoch: 31 [16640/225000 (7%)] Loss: 8823.134766\n",
      "Train Epoch: 31 [20736/225000 (9%)] Loss: 12113.712891\n",
      "Train Epoch: 31 [24832/225000 (11%)] Loss: 8528.613281\n",
      "Train Epoch: 31 [28928/225000 (13%)] Loss: 18730.777344\n",
      "Train Epoch: 31 [33024/225000 (15%)] Loss: 14437.689453\n",
      "Train Epoch: 31 [37120/225000 (16%)] Loss: 13267.755859\n",
      "Train Epoch: 31 [41216/225000 (18%)] Loss: 23966.781250\n",
      "Train Epoch: 31 [45312/225000 (20%)] Loss: 14747.136719\n",
      "Train Epoch: 31 [49408/225000 (22%)] Loss: 8437.652344\n",
      "Train Epoch: 31 [53504/225000 (24%)] Loss: 8353.837891\n",
      "Train Epoch: 31 [57600/225000 (26%)] Loss: 12438.912109\n",
      "Train Epoch: 31 [61696/225000 (27%)] Loss: 8575.894531\n",
      "Train Epoch: 31 [65792/225000 (29%)] Loss: 12365.833984\n",
      "Train Epoch: 31 [69888/225000 (31%)] Loss: 12747.167969\n",
      "Train Epoch: 31 [73984/225000 (33%)] Loss: 8572.742188\n",
      "Train Epoch: 31 [78080/225000 (35%)] Loss: 8145.552734\n",
      "Train Epoch: 31 [82176/225000 (37%)] Loss: 12517.082031\n",
      "Train Epoch: 31 [86272/225000 (38%)] Loss: 8526.888672\n",
      "Train Epoch: 31 [90368/225000 (40%)] Loss: 11693.164062\n",
      "Train Epoch: 31 [94464/225000 (42%)] Loss: 8305.224609\n",
      "Train Epoch: 31 [98560/225000 (44%)] Loss: 8493.333984\n",
      "Train Epoch: 31 [102656/225000 (46%)] Loss: 8587.759766\n",
      "Train Epoch: 31 [106752/225000 (47%)] Loss: 10460.292969\n",
      "Train Epoch: 31 [110848/225000 (49%)] Loss: 10653.843750\n",
      "Train Epoch: 31 [114944/225000 (51%)] Loss: 11710.779297\n",
      "Train Epoch: 31 [119040/225000 (53%)] Loss: 12418.738281\n",
      "Train Epoch: 31 [123136/225000 (55%)] Loss: 8143.150391\n",
      "Train Epoch: 31 [127232/225000 (57%)] Loss: 11944.519531\n",
      "Train Epoch: 31 [131328/225000 (58%)] Loss: 8232.798828\n",
      "Train Epoch: 31 [135424/225000 (60%)] Loss: 10606.152344\n",
      "Train Epoch: 31 [139520/225000 (62%)] Loss: 8450.703125\n",
      "Train Epoch: 31 [143616/225000 (64%)] Loss: 11725.648438\n",
      "Train Epoch: 31 [147712/225000 (66%)] Loss: 8509.867188\n",
      "Train Epoch: 31 [151808/225000 (67%)] Loss: 8204.541016\n",
      "Train Epoch: 31 [155904/225000 (69%)] Loss: 8475.683594\n",
      "Train Epoch: 31 [160000/225000 (71%)] Loss: 14758.693359\n",
      "Train Epoch: 31 [164096/225000 (73%)] Loss: 8378.878906\n",
      "Train Epoch: 31 [168192/225000 (75%)] Loss: 8674.369141\n",
      "Train Epoch: 31 [172288/225000 (77%)] Loss: 8243.125000\n",
      "Train Epoch: 31 [176384/225000 (78%)] Loss: 13226.748047\n",
      "Train Epoch: 31 [180480/225000 (80%)] Loss: 8239.132812\n",
      "Train Epoch: 31 [184576/225000 (82%)] Loss: 11746.326172\n",
      "Train Epoch: 31 [188672/225000 (84%)] Loss: 12322.105469\n",
      "Train Epoch: 31 [192768/225000 (86%)] Loss: 8585.439453\n",
      "Train Epoch: 31 [196864/225000 (87%)] Loss: 8580.101562\n",
      "Train Epoch: 31 [200960/225000 (89%)] Loss: 8407.921875\n",
      "Train Epoch: 31 [205056/225000 (91%)] Loss: 8311.798828\n",
      "Train Epoch: 31 [209152/225000 (93%)] Loss: 12313.806641\n",
      "Train Epoch: 31 [213248/225000 (95%)] Loss: 10450.726562\n",
      "Train Epoch: 31 [217344/225000 (97%)] Loss: 8423.628906\n",
      "Train Epoch: 31 [221440/225000 (98%)] Loss: 8357.902344\n",
      "    epoch          : 31\n",
      "    loss           : 10127.714977069112\n",
      "    val_loss       : 9603.943732823476\n",
      "Train Epoch: 32 [256/225000 (0%)] Loss: 8428.328125\n",
      "Train Epoch: 32 [4352/225000 (2%)] Loss: 11329.533203\n",
      "Train Epoch: 32 [8448/225000 (4%)] Loss: 10509.464844\n",
      "Train Epoch: 32 [12544/225000 (6%)] Loss: 12161.123047\n",
      "Train Epoch: 32 [16640/225000 (7%)] Loss: 8364.814453\n",
      "Train Epoch: 32 [20736/225000 (9%)] Loss: 14577.908203\n",
      "Train Epoch: 32 [24832/225000 (11%)] Loss: 12278.800781\n",
      "Train Epoch: 32 [28928/225000 (13%)] Loss: 8553.376953\n",
      "Train Epoch: 32 [33024/225000 (15%)] Loss: 8285.117188\n",
      "Train Epoch: 32 [37120/225000 (16%)] Loss: 8205.431641\n",
      "Train Epoch: 32 [41216/225000 (18%)] Loss: 8387.414062\n",
      "Train Epoch: 32 [45312/225000 (20%)] Loss: 12361.191406\n",
      "Train Epoch: 32 [49408/225000 (22%)] Loss: 8306.150391\n",
      "Train Epoch: 32 [53504/225000 (24%)] Loss: 8497.316406\n",
      "Train Epoch: 32 [57600/225000 (26%)] Loss: 10440.583984\n",
      "Train Epoch: 32 [61696/225000 (27%)] Loss: 8406.382812\n",
      "Train Epoch: 32 [65792/225000 (29%)] Loss: 8143.990234\n",
      "Train Epoch: 32 [69888/225000 (31%)] Loss: 11349.029297\n",
      "Train Epoch: 32 [73984/225000 (33%)] Loss: 8301.841797\n",
      "Train Epoch: 32 [78080/225000 (35%)] Loss: 11309.939453\n",
      "Train Epoch: 32 [82176/225000 (37%)] Loss: 8411.558594\n",
      "Train Epoch: 32 [86272/225000 (38%)] Loss: 8185.132812\n",
      "Train Epoch: 32 [90368/225000 (40%)] Loss: 13406.652344\n",
      "Train Epoch: 32 [94464/225000 (42%)] Loss: 8950.412109\n",
      "Train Epoch: 32 [98560/225000 (44%)] Loss: 14463.646484\n",
      "Train Epoch: 32 [102656/225000 (46%)] Loss: 8421.732422\n",
      "Train Epoch: 32 [106752/225000 (47%)] Loss: 8438.894531\n",
      "Train Epoch: 32 [110848/225000 (49%)] Loss: 8462.998047\n",
      "Train Epoch: 32 [114944/225000 (51%)] Loss: 8297.093750\n",
      "Train Epoch: 32 [119040/225000 (53%)] Loss: 8457.613281\n",
      "Train Epoch: 32 [123136/225000 (55%)] Loss: 8356.066406\n",
      "Train Epoch: 32 [127232/225000 (57%)] Loss: 8514.154297\n",
      "Train Epoch: 32 [131328/225000 (58%)] Loss: 8455.617188\n",
      "Train Epoch: 32 [135424/225000 (60%)] Loss: 10471.443359\n",
      "Train Epoch: 32 [139520/225000 (62%)] Loss: 8439.240234\n",
      "Train Epoch: 32 [143616/225000 (64%)] Loss: 8415.509766\n",
      "Train Epoch: 32 [147712/225000 (66%)] Loss: 8307.025391\n",
      "Train Epoch: 32 [151808/225000 (67%)] Loss: 8281.345703\n",
      "Train Epoch: 32 [155904/225000 (69%)] Loss: 14142.408203\n",
      "Train Epoch: 32 [160000/225000 (71%)] Loss: 8114.332031\n",
      "Train Epoch: 32 [164096/225000 (73%)] Loss: 8319.751953\n",
      "Train Epoch: 32 [168192/225000 (75%)] Loss: 10529.943359\n",
      "Train Epoch: 32 [172288/225000 (77%)] Loss: 8387.576172\n",
      "Train Epoch: 32 [176384/225000 (78%)] Loss: 8258.843750\n",
      "Train Epoch: 32 [180480/225000 (80%)] Loss: 8484.587891\n",
      "Train Epoch: 32 [184576/225000 (82%)] Loss: 8419.480469\n",
      "Train Epoch: 32 [188672/225000 (84%)] Loss: 12230.242188\n",
      "Train Epoch: 32 [192768/225000 (86%)] Loss: 8424.183594\n",
      "Train Epoch: 32 [196864/225000 (87%)] Loss: 8261.019531\n",
      "Train Epoch: 32 [200960/225000 (89%)] Loss: 8389.125000\n",
      "Train Epoch: 32 [205056/225000 (91%)] Loss: 13295.066406\n",
      "Train Epoch: 32 [209152/225000 (93%)] Loss: 13970.482422\n",
      "Train Epoch: 32 [213248/225000 (95%)] Loss: 8131.400391\n",
      "Train Epoch: 32 [217344/225000 (97%)] Loss: 14036.492188\n",
      "Train Epoch: 32 [221440/225000 (98%)] Loss: 8337.855469\n",
      "    epoch          : 32\n",
      "    loss           : 10107.597236294796\n",
      "    val_loss       : 10292.748608438336\n",
      "Train Epoch: 33 [256/225000 (0%)] Loss: 11996.148438\n",
      "Train Epoch: 33 [4352/225000 (2%)] Loss: 13286.623047\n",
      "Train Epoch: 33 [8448/225000 (4%)] Loss: 8451.451172\n",
      "Train Epoch: 33 [12544/225000 (6%)] Loss: 8446.875000\n",
      "Train Epoch: 33 [16640/225000 (7%)] Loss: 8417.951172\n",
      "Train Epoch: 33 [20736/225000 (9%)] Loss: 11226.806641\n",
      "Train Epoch: 33 [24832/225000 (11%)] Loss: 13400.166016\n",
      "Train Epoch: 33 [28928/225000 (13%)] Loss: 8217.488281\n",
      "Train Epoch: 33 [33024/225000 (15%)] Loss: 8346.695312\n",
      "Train Epoch: 33 [37120/225000 (16%)] Loss: 8325.634766\n",
      "Train Epoch: 33 [41216/225000 (18%)] Loss: 8288.085938\n",
      "Train Epoch: 33 [45312/225000 (20%)] Loss: 8448.048828\n",
      "Train Epoch: 33 [49408/225000 (22%)] Loss: 12057.554688\n",
      "Train Epoch: 33 [53504/225000 (24%)] Loss: 8373.027344\n",
      "Train Epoch: 33 [57600/225000 (26%)] Loss: 8515.582031\n",
      "Train Epoch: 33 [61696/225000 (27%)] Loss: 20244.550781\n",
      "Train Epoch: 33 [65792/225000 (29%)] Loss: 8197.562500\n",
      "Train Epoch: 33 [69888/225000 (31%)] Loss: 10790.601562\n",
      "Train Epoch: 33 [73984/225000 (33%)] Loss: 10929.697266\n",
      "Train Epoch: 33 [78080/225000 (35%)] Loss: 8402.126953\n",
      "Train Epoch: 33 [82176/225000 (37%)] Loss: 10692.578125\n",
      "Train Epoch: 33 [86272/225000 (38%)] Loss: 13547.623047\n",
      "Train Epoch: 33 [90368/225000 (40%)] Loss: 14713.625000\n",
      "Train Epoch: 33 [94464/225000 (42%)] Loss: 8344.505859\n",
      "Train Epoch: 33 [98560/225000 (44%)] Loss: 10997.082031\n",
      "Train Epoch: 33 [102656/225000 (46%)] Loss: 8208.027344\n",
      "Train Epoch: 33 [106752/225000 (47%)] Loss: 8384.914062\n",
      "Train Epoch: 33 [110848/225000 (49%)] Loss: 8349.992188\n",
      "Train Epoch: 33 [114944/225000 (51%)] Loss: 8060.052734\n",
      "Train Epoch: 33 [119040/225000 (53%)] Loss: 10818.179688\n",
      "Train Epoch: 33 [123136/225000 (55%)] Loss: 8266.257812\n",
      "Train Epoch: 33 [127232/225000 (57%)] Loss: 8510.171875\n",
      "Train Epoch: 33 [131328/225000 (58%)] Loss: 13340.562500\n",
      "Train Epoch: 33 [135424/225000 (60%)] Loss: 8427.818359\n",
      "Train Epoch: 33 [139520/225000 (62%)] Loss: 8195.306641\n",
      "Train Epoch: 33 [143616/225000 (64%)] Loss: 12015.505859\n",
      "Train Epoch: 33 [147712/225000 (66%)] Loss: 10243.142578\n",
      "Train Epoch: 33 [151808/225000 (67%)] Loss: 16712.841797\n",
      "Train Epoch: 33 [155904/225000 (69%)] Loss: 8393.246094\n",
      "Train Epoch: 33 [160000/225000 (71%)] Loss: 8273.312500\n",
      "Train Epoch: 33 [164096/225000 (73%)] Loss: 8195.781250\n",
      "Train Epoch: 33 [168192/225000 (75%)] Loss: 10388.984375\n",
      "Train Epoch: 33 [172288/225000 (77%)] Loss: 11796.873047\n",
      "Train Epoch: 33 [176384/225000 (78%)] Loss: 11933.041016\n",
      "Train Epoch: 33 [180480/225000 (80%)] Loss: 14361.775391\n",
      "Train Epoch: 33 [184576/225000 (82%)] Loss: 11962.281250\n",
      "Train Epoch: 33 [188672/225000 (84%)] Loss: 12077.470703\n",
      "Train Epoch: 33 [192768/225000 (86%)] Loss: 8375.240234\n",
      "Train Epoch: 33 [196864/225000 (87%)] Loss: 8408.603516\n",
      "Train Epoch: 33 [200960/225000 (89%)] Loss: 13473.726562\n",
      "Train Epoch: 33 [205056/225000 (91%)] Loss: 8343.054688\n",
      "Train Epoch: 33 [209152/225000 (93%)] Loss: 8265.958984\n",
      "Train Epoch: 33 [213248/225000 (95%)] Loss: 8330.628906\n",
      "Train Epoch: 33 [217344/225000 (97%)] Loss: 10934.779297\n",
      "Train Epoch: 33 [221440/225000 (98%)] Loss: 17282.560547\n",
      "    epoch          : 33\n",
      "    loss           : 9958.409019704564\n",
      "    val_loss       : 9787.91924987034\n",
      "Train Epoch: 34 [256/225000 (0%)] Loss: 8425.369141\n",
      "Train Epoch: 34 [4352/225000 (2%)] Loss: 13255.593750\n",
      "Train Epoch: 34 [8448/225000 (4%)] Loss: 8303.810547\n",
      "Train Epoch: 34 [12544/225000 (6%)] Loss: 12306.712891\n",
      "Train Epoch: 34 [16640/225000 (7%)] Loss: 14297.906250\n",
      "Train Epoch: 34 [20736/225000 (9%)] Loss: 8250.175781\n",
      "Train Epoch: 34 [24832/225000 (11%)] Loss: 8279.402344\n",
      "Train Epoch: 34 [28928/225000 (13%)] Loss: 8357.886719\n",
      "Train Epoch: 34 [33024/225000 (15%)] Loss: 8333.300781\n",
      "Train Epoch: 34 [37120/225000 (16%)] Loss: 8188.314453\n",
      "Train Epoch: 34 [41216/225000 (18%)] Loss: 10814.333984\n",
      "Train Epoch: 34 [45312/225000 (20%)] Loss: 14477.753906\n",
      "Train Epoch: 34 [49408/225000 (22%)] Loss: 15315.080078\n",
      "Train Epoch: 34 [53504/225000 (24%)] Loss: 11712.535156\n",
      "Train Epoch: 34 [57600/225000 (26%)] Loss: 8454.845703\n",
      "Train Epoch: 34 [61696/225000 (27%)] Loss: 10502.576172\n",
      "Train Epoch: 34 [65792/225000 (29%)] Loss: 11629.343750\n",
      "Train Epoch: 34 [69888/225000 (31%)] Loss: 15073.390625\n",
      "Train Epoch: 34 [73984/225000 (33%)] Loss: 10310.541016\n",
      "Train Epoch: 34 [78080/225000 (35%)] Loss: 8284.753906\n",
      "Train Epoch: 34 [82176/225000 (37%)] Loss: 8222.339844\n",
      "Train Epoch: 34 [86272/225000 (38%)] Loss: 8312.906250\n",
      "Train Epoch: 34 [90368/225000 (40%)] Loss: 11793.529297\n",
      "Train Epoch: 34 [94464/225000 (42%)] Loss: 8310.302734\n",
      "Train Epoch: 34 [98560/225000 (44%)] Loss: 8411.724609\n",
      "Train Epoch: 34 [102656/225000 (46%)] Loss: 18804.818359\n",
      "Train Epoch: 34 [106752/225000 (47%)] Loss: 8388.796875\n",
      "Train Epoch: 34 [110848/225000 (49%)] Loss: 13100.648438\n",
      "Train Epoch: 34 [114944/225000 (51%)] Loss: 10426.458984\n",
      "Train Epoch: 34 [119040/225000 (53%)] Loss: 11630.326172\n",
      "Train Epoch: 34 [123136/225000 (55%)] Loss: 14941.265625\n",
      "Train Epoch: 34 [127232/225000 (57%)] Loss: 15454.367188\n",
      "Train Epoch: 34 [131328/225000 (58%)] Loss: 15150.199219\n",
      "Train Epoch: 34 [135424/225000 (60%)] Loss: 11539.888672\n",
      "Train Epoch: 34 [139520/225000 (62%)] Loss: 17686.371094\n",
      "Train Epoch: 34 [143616/225000 (64%)] Loss: 8535.455078\n",
      "Train Epoch: 34 [147712/225000 (66%)] Loss: 19766.593750\n",
      "Train Epoch: 34 [151808/225000 (67%)] Loss: 10136.306641\n",
      "Train Epoch: 34 [155904/225000 (69%)] Loss: 10491.810547\n",
      "Train Epoch: 34 [160000/225000 (71%)] Loss: 8409.238281\n",
      "Train Epoch: 34 [164096/225000 (73%)] Loss: 8468.078125\n",
      "Train Epoch: 34 [168192/225000 (75%)] Loss: 14280.265625\n",
      "Train Epoch: 34 [172288/225000 (77%)] Loss: 8238.382812\n",
      "Train Epoch: 34 [176384/225000 (78%)] Loss: 11431.712891\n",
      "Train Epoch: 34 [180480/225000 (80%)] Loss: 8207.078125\n",
      "Train Epoch: 34 [184576/225000 (82%)] Loss: 11522.947266\n",
      "Train Epoch: 34 [188672/225000 (84%)] Loss: 8393.628906\n",
      "Train Epoch: 34 [192768/225000 (86%)] Loss: 8072.125000\n",
      "Train Epoch: 34 [196864/225000 (87%)] Loss: 8142.462891\n",
      "Train Epoch: 34 [200960/225000 (89%)] Loss: 10580.808594\n",
      "Train Epoch: 34 [205056/225000 (91%)] Loss: 11431.546875\n",
      "Train Epoch: 34 [209152/225000 (93%)] Loss: 8066.371094\n",
      "Train Epoch: 34 [213248/225000 (95%)] Loss: 8170.070312\n",
      "Train Epoch: 34 [217344/225000 (97%)] Loss: 10225.320312\n",
      "Train Epoch: 34 [221440/225000 (98%)] Loss: 10182.095703\n",
      "    epoch          : 34\n",
      "    loss           : 10391.972831786832\n",
      "    val_loss       : 10209.458400648467\n",
      "Train Epoch: 35 [256/225000 (0%)] Loss: 11475.662109\n",
      "Train Epoch: 35 [4352/225000 (2%)] Loss: 8257.304688\n",
      "Train Epoch: 35 [8448/225000 (4%)] Loss: 8397.669922\n",
      "Train Epoch: 35 [12544/225000 (6%)] Loss: 10408.757812\n",
      "Train Epoch: 35 [16640/225000 (7%)] Loss: 8218.501953\n",
      "Train Epoch: 35 [20736/225000 (9%)] Loss: 11315.646484\n",
      "Train Epoch: 35 [24832/225000 (11%)] Loss: 8187.367188\n",
      "Train Epoch: 35 [28928/225000 (13%)] Loss: 10411.798828\n",
      "Train Epoch: 35 [33024/225000 (15%)] Loss: 8206.609375\n",
      "Train Epoch: 35 [37120/225000 (16%)] Loss: 10147.949219\n",
      "Train Epoch: 35 [41216/225000 (18%)] Loss: 10286.974609\n",
      "Train Epoch: 35 [45312/225000 (20%)] Loss: 10182.328125\n",
      "Train Epoch: 35 [49408/225000 (22%)] Loss: 10342.982422\n",
      "Train Epoch: 35 [53504/225000 (24%)] Loss: 8215.257812\n",
      "Train Epoch: 35 [57600/225000 (26%)] Loss: 8264.626953\n",
      "Train Epoch: 35 [61696/225000 (27%)] Loss: 13048.166016\n",
      "Train Epoch: 35 [65792/225000 (29%)] Loss: 8301.503906\n",
      "Train Epoch: 35 [69888/225000 (31%)] Loss: 8278.505859\n",
      "Train Epoch: 35 [73984/225000 (33%)] Loss: 14339.101562\n",
      "Train Epoch: 35 [78080/225000 (35%)] Loss: 15760.601562\n",
      "Train Epoch: 35 [82176/225000 (37%)] Loss: 10413.568359\n",
      "Train Epoch: 35 [86272/225000 (38%)] Loss: 8248.609375\n",
      "Train Epoch: 35 [90368/225000 (40%)] Loss: 10533.136719\n",
      "Train Epoch: 35 [94464/225000 (42%)] Loss: 8150.589844\n",
      "Train Epoch: 35 [98560/225000 (44%)] Loss: 8225.046875\n",
      "Train Epoch: 35 [102656/225000 (46%)] Loss: 14170.986328\n",
      "Train Epoch: 35 [106752/225000 (47%)] Loss: 8336.058594\n",
      "Train Epoch: 35 [110848/225000 (49%)] Loss: 11232.933594\n",
      "Train Epoch: 35 [114944/225000 (51%)] Loss: 11705.373047\n",
      "Train Epoch: 35 [119040/225000 (53%)] Loss: 8003.056641\n",
      "Train Epoch: 35 [123136/225000 (55%)] Loss: 10458.386719\n",
      "Train Epoch: 35 [127232/225000 (57%)] Loss: 13037.882812\n",
      "Train Epoch: 35 [131328/225000 (58%)] Loss: 8475.398438\n",
      "Train Epoch: 35 [135424/225000 (60%)] Loss: 16174.287109\n",
      "Train Epoch: 35 [139520/225000 (62%)] Loss: 8304.519531\n",
      "Train Epoch: 35 [143616/225000 (64%)] Loss: 14145.968750\n",
      "Train Epoch: 35 [147712/225000 (66%)] Loss: 10066.392578\n",
      "Train Epoch: 35 [151808/225000 (67%)] Loss: 10016.259766\n",
      "Train Epoch: 35 [155904/225000 (69%)] Loss: 8301.574219\n",
      "Train Epoch: 35 [160000/225000 (71%)] Loss: 20229.529297\n",
      "Train Epoch: 35 [164096/225000 (73%)] Loss: 8281.667969\n",
      "Train Epoch: 35 [168192/225000 (75%)] Loss: 8223.962891\n",
      "Train Epoch: 35 [172288/225000 (77%)] Loss: 8298.343750\n",
      "Train Epoch: 35 [176384/225000 (78%)] Loss: 8234.802734\n",
      "Train Epoch: 35 [180480/225000 (80%)] Loss: 8211.662109\n",
      "Train Epoch: 35 [184576/225000 (82%)] Loss: 8374.939453\n",
      "Train Epoch: 35 [188672/225000 (84%)] Loss: 10344.300781\n",
      "Train Epoch: 35 [192768/225000 (86%)] Loss: 8247.662109\n",
      "Train Epoch: 35 [196864/225000 (87%)] Loss: 8338.453125\n",
      "Train Epoch: 35 [200960/225000 (89%)] Loss: 8005.716797\n",
      "Train Epoch: 35 [205056/225000 (91%)] Loss: 12257.421875\n",
      "Train Epoch: 35 [209152/225000 (93%)] Loss: 8438.804688\n",
      "Train Epoch: 35 [213248/225000 (95%)] Loss: 8436.742188\n",
      "Train Epoch: 35 [217344/225000 (97%)] Loss: 13923.974609\n",
      "Train Epoch: 35 [221440/225000 (98%)] Loss: 13344.363281\n",
      "    epoch          : 35\n",
      "    loss           : 10331.899148535267\n",
      "    val_loss       : 10087.119752192984\n",
      "Train Epoch: 36 [256/225000 (0%)] Loss: 8148.884766\n",
      "Train Epoch: 36 [4352/225000 (2%)] Loss: 8240.478516\n",
      "Train Epoch: 36 [8448/225000 (4%)] Loss: 8472.712891\n",
      "Train Epoch: 36 [12544/225000 (6%)] Loss: 8303.330078\n",
      "Train Epoch: 36 [16640/225000 (7%)] Loss: 8144.994141\n",
      "Train Epoch: 36 [20736/225000 (9%)] Loss: 10203.384766\n",
      "Train Epoch: 36 [24832/225000 (11%)] Loss: 8208.595703\n",
      "Train Epoch: 36 [28928/225000 (13%)] Loss: 8215.722656\n",
      "Train Epoch: 36 [33024/225000 (15%)] Loss: 8163.380859\n",
      "Train Epoch: 36 [37120/225000 (16%)] Loss: 8193.718750\n",
      "Train Epoch: 36 [41216/225000 (18%)] Loss: 8258.035156\n",
      "Train Epoch: 36 [45312/225000 (20%)] Loss: 8163.509766\n",
      "Train Epoch: 36 [49408/225000 (22%)] Loss: 10259.599609\n",
      "Train Epoch: 36 [53504/225000 (24%)] Loss: 8080.320312\n",
      "Train Epoch: 36 [57600/225000 (26%)] Loss: 8254.128906\n",
      "Train Epoch: 36 [61696/225000 (27%)] Loss: 8326.341797\n",
      "Train Epoch: 36 [65792/225000 (29%)] Loss: 8055.880859\n",
      "Train Epoch: 36 [69888/225000 (31%)] Loss: 10206.822266\n",
      "Train Epoch: 36 [73984/225000 (33%)] Loss: 11108.640625\n",
      "Train Epoch: 36 [78080/225000 (35%)] Loss: 8407.402344\n",
      "Train Epoch: 36 [82176/225000 (37%)] Loss: 8156.591797\n",
      "Train Epoch: 36 [86272/225000 (38%)] Loss: 8081.318359\n",
      "Train Epoch: 36 [90368/225000 (40%)] Loss: 8172.513672\n",
      "Train Epoch: 36 [94464/225000 (42%)] Loss: 8137.736328\n",
      "Train Epoch: 36 [98560/225000 (44%)] Loss: 8408.464844\n",
      "Train Epoch: 36 [102656/225000 (46%)] Loss: 20040.125000\n",
      "Train Epoch: 36 [106752/225000 (47%)] Loss: 14590.478516\n",
      "Train Epoch: 36 [110848/225000 (49%)] Loss: 8094.296875\n",
      "Train Epoch: 36 [114944/225000 (51%)] Loss: 8205.193359\n",
      "Train Epoch: 36 [119040/225000 (53%)] Loss: 10210.730469\n",
      "Train Epoch: 36 [123136/225000 (55%)] Loss: 8247.714844\n",
      "Train Epoch: 36 [127232/225000 (57%)] Loss: 9964.888672\n",
      "Train Epoch: 36 [131328/225000 (58%)] Loss: 8376.292969\n",
      "Train Epoch: 36 [135424/225000 (60%)] Loss: 13141.054688\n",
      "Train Epoch: 36 [139520/225000 (62%)] Loss: 9791.814453\n",
      "Train Epoch: 36 [143616/225000 (64%)] Loss: 8186.818359\n",
      "Train Epoch: 36 [147712/225000 (66%)] Loss: 10132.187500\n",
      "Train Epoch: 36 [151808/225000 (67%)] Loss: 11660.644531\n",
      "Train Epoch: 36 [155904/225000 (69%)] Loss: 8340.974609\n",
      "Train Epoch: 36 [160000/225000 (71%)] Loss: 16607.578125\n",
      "Train Epoch: 36 [164096/225000 (73%)] Loss: 9966.564453\n",
      "Train Epoch: 36 [168192/225000 (75%)] Loss: 11787.535156\n",
      "Train Epoch: 36 [172288/225000 (77%)] Loss: 8606.937500\n",
      "Train Epoch: 36 [176384/225000 (78%)] Loss: 14285.673828\n",
      "Train Epoch: 36 [180480/225000 (80%)] Loss: 8348.701172\n",
      "Train Epoch: 36 [184576/225000 (82%)] Loss: 8127.144531\n",
      "Train Epoch: 36 [188672/225000 (84%)] Loss: 14745.720703\n",
      "Train Epoch: 36 [192768/225000 (86%)] Loss: 8097.841797\n",
      "Train Epoch: 36 [196864/225000 (87%)] Loss: 8350.472656\n",
      "Train Epoch: 36 [200960/225000 (89%)] Loss: 16329.828125\n",
      "Train Epoch: 36 [205056/225000 (91%)] Loss: 8149.248047\n",
      "Train Epoch: 36 [209152/225000 (93%)] Loss: 9961.308594\n",
      "Train Epoch: 36 [213248/225000 (95%)] Loss: 8214.486328\n",
      "Train Epoch: 36 [217344/225000 (97%)] Loss: 9820.333984\n",
      "Train Epoch: 36 [221440/225000 (98%)] Loss: 12817.294922\n",
      "    epoch          : 36\n",
      "    loss           : 10189.9599909343\n",
      "    val_loss       : 9904.57885673338\n",
      "Train Epoch: 37 [256/225000 (0%)] Loss: 11076.876953\n",
      "Train Epoch: 37 [4352/225000 (2%)] Loss: 10054.974609\n",
      "Train Epoch: 37 [8448/225000 (4%)] Loss: 8216.076172\n",
      "Train Epoch: 37 [12544/225000 (6%)] Loss: 12745.234375\n",
      "Train Epoch: 37 [16640/225000 (7%)] Loss: 11862.113281\n",
      "Train Epoch: 37 [20736/225000 (9%)] Loss: 12078.978516\n",
      "Train Epoch: 37 [24832/225000 (11%)] Loss: 9852.205078\n",
      "Train Epoch: 37 [28928/225000 (13%)] Loss: 10876.009766\n",
      "Train Epoch: 37 [33024/225000 (15%)] Loss: 8205.279297\n",
      "Train Epoch: 37 [37120/225000 (16%)] Loss: 10762.158203\n",
      "Train Epoch: 37 [41216/225000 (18%)] Loss: 8240.328125\n",
      "Train Epoch: 37 [45312/225000 (20%)] Loss: 8156.597656\n",
      "Train Epoch: 37 [49408/225000 (22%)] Loss: 8307.964844\n",
      "Train Epoch: 37 [53504/225000 (24%)] Loss: 10996.550781\n",
      "Train Epoch: 37 [57600/225000 (26%)] Loss: 8394.523438\n",
      "Train Epoch: 37 [61696/225000 (27%)] Loss: 8183.412109\n",
      "Train Epoch: 37 [65792/225000 (29%)] Loss: 8372.400391\n",
      "Train Epoch: 37 [69888/225000 (31%)] Loss: 8119.796875\n",
      "Train Epoch: 37 [73984/225000 (33%)] Loss: 8070.291016\n",
      "Train Epoch: 37 [78080/225000 (35%)] Loss: 8204.587891\n",
      "Train Epoch: 37 [82176/225000 (37%)] Loss: 13609.691406\n",
      "Train Epoch: 37 [86272/225000 (38%)] Loss: 8383.326172\n",
      "Train Epoch: 37 [90368/225000 (40%)] Loss: 10746.687500\n",
      "Train Epoch: 37 [94464/225000 (42%)] Loss: 9763.626953\n",
      "Train Epoch: 37 [98560/225000 (44%)] Loss: 9927.546875\n",
      "Train Epoch: 37 [102656/225000 (46%)] Loss: 8031.546875\n",
      "Train Epoch: 37 [106752/225000 (47%)] Loss: 9834.810547\n",
      "Train Epoch: 37 [110848/225000 (49%)] Loss: 8256.685547\n",
      "Train Epoch: 37 [114944/225000 (51%)] Loss: 12858.425781\n",
      "Train Epoch: 37 [119040/225000 (53%)] Loss: 8514.501953\n",
      "Train Epoch: 37 [123136/225000 (55%)] Loss: 10951.886719\n",
      "Train Epoch: 37 [127232/225000 (57%)] Loss: 9913.941406\n",
      "Train Epoch: 37 [131328/225000 (58%)] Loss: 9890.919922\n",
      "Train Epoch: 37 [135424/225000 (60%)] Loss: 8064.638672\n",
      "Train Epoch: 37 [139520/225000 (62%)] Loss: 8461.867188\n",
      "Train Epoch: 37 [143616/225000 (64%)] Loss: 10960.697266\n",
      "Train Epoch: 37 [147712/225000 (66%)] Loss: 13839.394531\n",
      "Train Epoch: 37 [151808/225000 (67%)] Loss: 9864.082031\n",
      "Train Epoch: 37 [155904/225000 (69%)] Loss: 8311.607422\n",
      "Train Epoch: 37 [160000/225000 (71%)] Loss: 8273.773438\n",
      "Train Epoch: 37 [164096/225000 (73%)] Loss: 8323.384766\n",
      "Train Epoch: 37 [168192/225000 (75%)] Loss: 8199.945312\n",
      "Train Epoch: 37 [172288/225000 (77%)] Loss: 13915.853516\n",
      "Train Epoch: 37 [176384/225000 (78%)] Loss: 13786.722656\n",
      "Train Epoch: 37 [180480/225000 (80%)] Loss: 10982.470703\n",
      "Train Epoch: 37 [184576/225000 (82%)] Loss: 9927.140625\n",
      "Train Epoch: 37 [188672/225000 (84%)] Loss: 8085.035156\n",
      "Train Epoch: 37 [192768/225000 (86%)] Loss: 8213.750000\n",
      "Train Epoch: 37 [196864/225000 (87%)] Loss: 8098.832031\n",
      "Train Epoch: 37 [200960/225000 (89%)] Loss: 8127.164062\n",
      "Train Epoch: 37 [205056/225000 (91%)] Loss: 8228.166016\n",
      "Train Epoch: 37 [209152/225000 (93%)] Loss: 8174.041016\n",
      "Train Epoch: 37 [213248/225000 (95%)] Loss: 8175.816406\n",
      "Train Epoch: 37 [217344/225000 (97%)] Loss: 9916.052734\n",
      "Train Epoch: 37 [221440/225000 (98%)] Loss: 8339.791016\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "    epoch          : 37\n",
      "    loss           : 10228.801227869028\n",
      "    val_loss       : 9903.383289273295\n",
      "Train Epoch: 38 [256/225000 (0%)] Loss: 8379.417969\n",
      "Train Epoch: 38 [4352/225000 (2%)] Loss: 9912.744141\n",
      "Train Epoch: 38 [8448/225000 (4%)] Loss: 8182.757812\n",
      "Train Epoch: 38 [12544/225000 (6%)] Loss: 11040.339844\n",
      "Train Epoch: 38 [16640/225000 (7%)] Loss: 8071.871094\n",
      "Train Epoch: 38 [20736/225000 (9%)] Loss: 9604.896484\n",
      "Train Epoch: 38 [24832/225000 (11%)] Loss: 8177.591797\n",
      "Train Epoch: 38 [28928/225000 (13%)] Loss: 8123.976562\n",
      "Train Epoch: 38 [33024/225000 (15%)] Loss: 10631.705078\n",
      "Train Epoch: 38 [37120/225000 (16%)] Loss: 10765.257812\n",
      "Train Epoch: 38 [41216/225000 (18%)] Loss: 8189.927734\n",
      "Train Epoch: 38 [45312/225000 (20%)] Loss: 8142.218750\n",
      "Train Epoch: 38 [49408/225000 (22%)] Loss: 9790.039062\n",
      "Train Epoch: 38 [53504/225000 (24%)] Loss: 8139.496094\n",
      "Train Epoch: 38 [57600/225000 (26%)] Loss: 10935.654297\n",
      "Train Epoch: 38 [61696/225000 (27%)] Loss: 8048.121094\n",
      "Train Epoch: 38 [65792/225000 (29%)] Loss: 8153.648438\n",
      "Train Epoch: 38 [69888/225000 (31%)] Loss: 8098.416016\n",
      "Train Epoch: 38 [73984/225000 (33%)] Loss: 8170.964844\n",
      "Train Epoch: 38 [78080/225000 (35%)] Loss: 13247.363281\n",
      "Train Epoch: 38 [82176/225000 (37%)] Loss: 8050.412109\n",
      "Train Epoch: 38 [86272/225000 (38%)] Loss: 9900.773438\n",
      "Train Epoch: 38 [90368/225000 (40%)] Loss: 8042.054688\n",
      "Train Epoch: 38 [94464/225000 (42%)] Loss: 14241.689453\n",
      "Train Epoch: 38 [98560/225000 (44%)] Loss: 12714.884766\n",
      "Train Epoch: 38 [102656/225000 (46%)] Loss: 9962.404297\n",
      "Train Epoch: 38 [106752/225000 (47%)] Loss: 8083.710938\n",
      "Train Epoch: 38 [110848/225000 (49%)] Loss: 9663.193359\n",
      "Train Epoch: 38 [114944/225000 (51%)] Loss: 9747.476562\n",
      "Train Epoch: 38 [119040/225000 (53%)] Loss: 9779.632812\n",
      "Train Epoch: 38 [123136/225000 (55%)] Loss: 8003.304688\n",
      "Train Epoch: 38 [127232/225000 (57%)] Loss: 9845.701172\n",
      "Train Epoch: 38 [131328/225000 (58%)] Loss: 8167.937500\n",
      "Train Epoch: 38 [135424/225000 (60%)] Loss: 8230.500000\n",
      "Train Epoch: 38 [139520/225000 (62%)] Loss: 8122.767578\n",
      "Train Epoch: 38 [143616/225000 (64%)] Loss: 8072.667969\n",
      "Train Epoch: 38 [147712/225000 (66%)] Loss: 9716.699219\n",
      "Train Epoch: 38 [151808/225000 (67%)] Loss: 9797.164062\n",
      "Train Epoch: 38 [155904/225000 (69%)] Loss: 13764.121094\n",
      "Train Epoch: 38 [160000/225000 (71%)] Loss: 8235.841797\n",
      "Train Epoch: 38 [164096/225000 (73%)] Loss: 12845.480469\n",
      "Train Epoch: 38 [168192/225000 (75%)] Loss: 9855.871094\n",
      "Train Epoch: 38 [172288/225000 (77%)] Loss: 12790.953125\n",
      "Train Epoch: 38 [176384/225000 (78%)] Loss: 8114.056641\n",
      "Train Epoch: 38 [180480/225000 (80%)] Loss: 8120.787109\n",
      "Train Epoch: 38 [184576/225000 (82%)] Loss: 9690.126953\n",
      "Train Epoch: 38 [188672/225000 (84%)] Loss: 9907.748047\n",
      "Train Epoch: 38 [192768/225000 (86%)] Loss: 8112.398438\n",
      "Train Epoch: 38 [196864/225000 (87%)] Loss: 8073.839844\n",
      "Train Epoch: 38 [200960/225000 (89%)] Loss: 8055.609375\n",
      "Train Epoch: 38 [205056/225000 (91%)] Loss: 9878.464844\n",
      "Train Epoch: 38 [209152/225000 (93%)] Loss: 15374.876953\n",
      "Train Epoch: 38 [213248/225000 (95%)] Loss: 9726.960938\n",
      "Train Epoch: 38 [217344/225000 (97%)] Loss: 9668.921875\n",
      "Train Epoch: 38 [221440/225000 (98%)] Loss: 10039.664062\n",
      "    epoch          : 38\n",
      "    loss           : 9870.844006639292\n",
      "    val_loss       : 9958.49597494578\n",
      "Train Epoch: 39 [256/225000 (0%)] Loss: 8276.257812\n",
      "Train Epoch: 39 [4352/225000 (2%)] Loss: 7975.060547\n",
      "Train Epoch: 39 [8448/225000 (4%)] Loss: 8022.689453\n",
      "Train Epoch: 39 [12544/225000 (6%)] Loss: 9660.289062\n",
      "Train Epoch: 39 [16640/225000 (7%)] Loss: 15972.003906\n",
      "Train Epoch: 39 [20736/225000 (9%)] Loss: 8171.234375\n",
      "Train Epoch: 39 [24832/225000 (11%)] Loss: 8127.921875\n",
      "Train Epoch: 39 [28928/225000 (13%)] Loss: 8161.179688\n",
      "Train Epoch: 39 [33024/225000 (15%)] Loss: 8310.328125\n",
      "Train Epoch: 39 [37120/225000 (16%)] Loss: 12249.083984\n",
      "Train Epoch: 39 [41216/225000 (18%)] Loss: 18345.972656\n",
      "Train Epoch: 39 [45312/225000 (20%)] Loss: 9970.371094\n",
      "Train Epoch: 39 [49408/225000 (22%)] Loss: 12535.875000\n",
      "Train Epoch: 39 [53504/225000 (24%)] Loss: 10844.763672\n",
      "Train Epoch: 39 [57600/225000 (26%)] Loss: 9852.537109\n",
      "Train Epoch: 39 [61696/225000 (27%)] Loss: 7944.187500\n",
      "Train Epoch: 39 [65792/225000 (29%)] Loss: 18431.255859\n",
      "Train Epoch: 39 [69888/225000 (31%)] Loss: 10871.607422\n",
      "Train Epoch: 39 [73984/225000 (33%)] Loss: 9766.236328\n",
      "Train Epoch: 39 [78080/225000 (35%)] Loss: 7958.552734\n",
      "Train Epoch: 39 [82176/225000 (37%)] Loss: 8114.574219\n",
      "Train Epoch: 39 [86272/225000 (38%)] Loss: 8079.216797\n",
      "Train Epoch: 39 [90368/225000 (40%)] Loss: 9823.503906\n",
      "Train Epoch: 39 [94464/225000 (42%)] Loss: 15439.160156\n",
      "Train Epoch: 39 [98560/225000 (44%)] Loss: 8167.027344\n",
      "Train Epoch: 39 [102656/225000 (46%)] Loss: 12494.705078\n",
      "Train Epoch: 39 [106752/225000 (47%)] Loss: 8358.634766\n",
      "Train Epoch: 39 [110848/225000 (49%)] Loss: 8133.753906\n",
      "Train Epoch: 39 [114944/225000 (51%)] Loss: 9651.218750\n",
      "Train Epoch: 39 [119040/225000 (53%)] Loss: 7945.068359\n",
      "Train Epoch: 39 [123136/225000 (55%)] Loss: 12792.300781\n",
      "Train Epoch: 39 [127232/225000 (57%)] Loss: 8165.656250\n",
      "Train Epoch: 39 [131328/225000 (58%)] Loss: 7944.257812\n",
      "Train Epoch: 39 [135424/225000 (60%)] Loss: 8251.082031\n",
      "Train Epoch: 39 [139520/225000 (62%)] Loss: 13525.986328\n",
      "Train Epoch: 39 [143616/225000 (64%)] Loss: 9627.802734\n",
      "Train Epoch: 39 [147712/225000 (66%)] Loss: 7979.115234\n",
      "Train Epoch: 39 [151808/225000 (67%)] Loss: 8481.589844\n",
      "Train Epoch: 39 [155904/225000 (69%)] Loss: 8100.570312\n",
      "Train Epoch: 39 [160000/225000 (71%)] Loss: 8024.041016\n",
      "Train Epoch: 39 [164096/225000 (73%)] Loss: 8108.048828\n",
      "Train Epoch: 39 [168192/225000 (75%)] Loss: 12492.220703\n",
      "Train Epoch: 39 [172288/225000 (77%)] Loss: 8004.101562\n",
      "Train Epoch: 39 [176384/225000 (78%)] Loss: 11499.332031\n",
      "Train Epoch: 39 [180480/225000 (80%)] Loss: 16533.851562\n",
      "Train Epoch: 39 [184576/225000 (82%)] Loss: 10802.144531\n",
      "Train Epoch: 39 [188672/225000 (84%)] Loss: 8134.720703\n",
      "Train Epoch: 39 [192768/225000 (86%)] Loss: 8235.697266\n",
      "Train Epoch: 39 [196864/225000 (87%)] Loss: 8419.144531\n",
      "Train Epoch: 39 [200960/225000 (89%)] Loss: 8233.617188\n",
      "Train Epoch: 39 [205056/225000 (91%)] Loss: 8438.625000\n",
      "Train Epoch: 39 [209152/225000 (93%)] Loss: 8406.228516\n",
      "Train Epoch: 39 [213248/225000 (95%)] Loss: 8049.363281\n",
      "Train Epoch: 39 [217344/225000 (97%)] Loss: 8202.091797\n",
      "Train Epoch: 39 [221440/225000 (98%)] Loss: 14109.796875\n",
      "    epoch          : 39\n",
      "    loss           : 10134.478900028442\n",
      "    val_loss       : 10150.160638430898\n",
      "Train Epoch: 40 [256/225000 (0%)] Loss: 8082.271484\n",
      "Train Epoch: 40 [4352/225000 (2%)] Loss: 8106.919922\n",
      "Train Epoch: 40 [8448/225000 (4%)] Loss: 9740.582031\n",
      "Train Epoch: 40 [12544/225000 (6%)] Loss: 18997.750000\n",
      "Train Epoch: 40 [16640/225000 (7%)] Loss: 8141.154297\n",
      "Train Epoch: 40 [20736/225000 (9%)] Loss: 9758.625000\n",
      "Train Epoch: 40 [24832/225000 (11%)] Loss: 8145.894531\n",
      "Train Epoch: 40 [28928/225000 (13%)] Loss: 7950.537109\n",
      "Train Epoch: 40 [33024/225000 (15%)] Loss: 9687.574219\n",
      "Train Epoch: 40 [37120/225000 (16%)] Loss: 10910.763672\n",
      "Train Epoch: 40 [41216/225000 (18%)] Loss: 14934.986328\n",
      "Train Epoch: 40 [45312/225000 (20%)] Loss: 9791.351562\n",
      "Train Epoch: 40 [49408/225000 (22%)] Loss: 11612.572266\n",
      "Train Epoch: 40 [53504/225000 (24%)] Loss: 10583.205078\n",
      "Train Epoch: 40 [57600/225000 (26%)] Loss: 8262.755859\n",
      "Train Epoch: 40 [61696/225000 (27%)] Loss: 15465.066406\n",
      "Train Epoch: 40 [65792/225000 (29%)] Loss: 8061.646484\n",
      "Train Epoch: 40 [69888/225000 (31%)] Loss: 9785.994141\n",
      "Train Epoch: 40 [73984/225000 (33%)] Loss: 8069.683594\n",
      "Train Epoch: 40 [78080/225000 (35%)] Loss: 12396.134766\n",
      "Train Epoch: 40 [82176/225000 (37%)] Loss: 10026.593750\n",
      "Train Epoch: 40 [86272/225000 (38%)] Loss: 8035.683594\n",
      "Train Epoch: 40 [90368/225000 (40%)] Loss: 8144.109375\n",
      "Train Epoch: 40 [94464/225000 (42%)] Loss: 8253.341797\n",
      "Train Epoch: 40 [98560/225000 (44%)] Loss: 8077.738281\n",
      "Train Epoch: 40 [102656/225000 (46%)] Loss: 12643.673828\n",
      "Train Epoch: 40 [106752/225000 (47%)] Loss: 9674.072266\n",
      "Train Epoch: 40 [110848/225000 (49%)] Loss: 12523.343750\n",
      "Train Epoch: 40 [114944/225000 (51%)] Loss: 8007.359375\n",
      "Train Epoch: 40 [119040/225000 (53%)] Loss: 8231.050781\n",
      "Train Epoch: 40 [123136/225000 (55%)] Loss: 10743.443359\n",
      "Train Epoch: 40 [127232/225000 (57%)] Loss: 8114.402344\n",
      "Train Epoch: 40 [131328/225000 (58%)] Loss: 7964.490234\n",
      "Train Epoch: 40 [135424/225000 (60%)] Loss: 8219.029297\n",
      "Train Epoch: 40 [139520/225000 (62%)] Loss: 8129.802734\n",
      "Train Epoch: 40 [143616/225000 (64%)] Loss: 7995.923828\n",
      "Train Epoch: 40 [147712/225000 (66%)] Loss: 8060.162109\n",
      "Train Epoch: 40 [151808/225000 (67%)] Loss: 8102.316406\n",
      "Train Epoch: 40 [155904/225000 (69%)] Loss: 12561.794922\n",
      "Train Epoch: 40 [160000/225000 (71%)] Loss: 11042.681641\n",
      "Train Epoch: 40 [164096/225000 (73%)] Loss: 8323.306641\n",
      "Train Epoch: 40 [168192/225000 (75%)] Loss: 8275.546875\n",
      "Train Epoch: 40 [172288/225000 (77%)] Loss: 12494.062500\n",
      "Train Epoch: 40 [176384/225000 (78%)] Loss: 10895.121094\n",
      "Train Epoch: 40 [180480/225000 (80%)] Loss: 7974.093750\n",
      "Train Epoch: 40 [184576/225000 (82%)] Loss: 8165.871094\n",
      "Train Epoch: 40 [188672/225000 (84%)] Loss: 9634.964844\n",
      "Train Epoch: 40 [192768/225000 (86%)] Loss: 9736.964844\n",
      "Train Epoch: 40 [196864/225000 (87%)] Loss: 9884.345703\n",
      "Train Epoch: 40 [200960/225000 (89%)] Loss: 9886.208984\n",
      "Train Epoch: 40 [205056/225000 (91%)] Loss: 9844.279297\n",
      "Train Epoch: 40 [209152/225000 (93%)] Loss: 8104.546875\n",
      "Train Epoch: 40 [213248/225000 (95%)] Loss: 8055.667969\n",
      "Train Epoch: 40 [217344/225000 (97%)] Loss: 16368.089844\n",
      "Train Epoch: 40 [221440/225000 (98%)] Loss: 8145.750000\n",
      "    epoch          : 40\n",
      "    loss           : 9865.077442850541\n",
      "    val_loss       : 10786.876799837668\n",
      "Train Epoch: 41 [256/225000 (0%)] Loss: 8035.986328\n",
      "Train Epoch: 41 [4352/225000 (2%)] Loss: 8132.568359\n",
      "Train Epoch: 41 [8448/225000 (4%)] Loss: 8121.986328\n",
      "Train Epoch: 41 [12544/225000 (6%)] Loss: 8060.580078\n",
      "Train Epoch: 41 [16640/225000 (7%)] Loss: 10895.416016\n",
      "Train Epoch: 41 [20736/225000 (9%)] Loss: 15613.998047\n",
      "Train Epoch: 41 [24832/225000 (11%)] Loss: 18281.876953\n",
      "Train Epoch: 41 [28928/225000 (13%)] Loss: 9697.373047\n",
      "Train Epoch: 41 [33024/225000 (15%)] Loss: 14291.763672\n",
      "Train Epoch: 41 [37120/225000 (16%)] Loss: 9919.382812\n",
      "Train Epoch: 41 [41216/225000 (18%)] Loss: 12644.890625\n",
      "Train Epoch: 41 [45312/225000 (20%)] Loss: 8251.369141\n",
      "Train Epoch: 41 [49408/225000 (22%)] Loss: 8223.205078\n",
      "Train Epoch: 41 [53504/225000 (24%)] Loss: 8139.875000\n",
      "Train Epoch: 41 [57600/225000 (26%)] Loss: 17390.759766\n",
      "Train Epoch: 41 [61696/225000 (27%)] Loss: 8257.056641\n",
      "Train Epoch: 41 [65792/225000 (29%)] Loss: 9862.134766\n",
      "Train Epoch: 41 [69888/225000 (31%)] Loss: 7955.042969\n",
      "Train Epoch: 41 [73984/225000 (33%)] Loss: 8082.755859\n",
      "Train Epoch: 41 [78080/225000 (35%)] Loss: 12640.494141\n",
      "Train Epoch: 41 [82176/225000 (37%)] Loss: 8154.660156\n",
      "Train Epoch: 41 [86272/225000 (38%)] Loss: 9669.935547\n",
      "Train Epoch: 41 [90368/225000 (40%)] Loss: 13183.625000\n",
      "Train Epoch: 41 [94464/225000 (42%)] Loss: 10828.687500\n",
      "Train Epoch: 41 [98560/225000 (44%)] Loss: 9906.251953\n",
      "Train Epoch: 41 [102656/225000 (46%)] Loss: 8020.693359\n",
      "Train Epoch: 41 [106752/225000 (47%)] Loss: 9561.279297\n",
      "Train Epoch: 41 [110848/225000 (49%)] Loss: 9943.562500\n",
      "Train Epoch: 41 [114944/225000 (51%)] Loss: 11527.289062\n",
      "Train Epoch: 41 [119040/225000 (53%)] Loss: 10706.341797\n",
      "Train Epoch: 41 [123136/225000 (55%)] Loss: 10748.292969\n",
      "Train Epoch: 41 [127232/225000 (57%)] Loss: 8003.687500\n",
      "Train Epoch: 41 [131328/225000 (58%)] Loss: 8108.761719\n",
      "Train Epoch: 41 [135424/225000 (60%)] Loss: 16335.300781\n",
      "Train Epoch: 41 [139520/225000 (62%)] Loss: 8314.144531\n",
      "Train Epoch: 41 [143616/225000 (64%)] Loss: 8022.988281\n",
      "Train Epoch: 41 [147712/225000 (66%)] Loss: 8122.113281\n",
      "Train Epoch: 41 [151808/225000 (67%)] Loss: 8225.208984\n",
      "Train Epoch: 41 [155904/225000 (69%)] Loss: 9776.640625\n",
      "Train Epoch: 41 [160000/225000 (71%)] Loss: 7966.119141\n",
      "Train Epoch: 41 [164096/225000 (73%)] Loss: 8199.966797\n",
      "Train Epoch: 41 [168192/225000 (75%)] Loss: 8136.294922\n",
      "Train Epoch: 41 [172288/225000 (77%)] Loss: 13375.080078\n",
      "Train Epoch: 41 [176384/225000 (78%)] Loss: 9798.146484\n",
      "Train Epoch: 41 [180480/225000 (80%)] Loss: 8009.730469\n",
      "Train Epoch: 41 [184576/225000 (82%)] Loss: 8231.611328\n",
      "Train Epoch: 41 [188672/225000 (84%)] Loss: 7964.914062\n",
      "Train Epoch: 41 [192768/225000 (86%)] Loss: 16484.167969\n",
      "Train Epoch: 41 [196864/225000 (87%)] Loss: 8289.238281\n",
      "Train Epoch: 41 [200960/225000 (89%)] Loss: 9645.386719\n",
      "Train Epoch: 41 [205056/225000 (91%)] Loss: 8127.125000\n",
      "Train Epoch: 41 [209152/225000 (93%)] Loss: 8330.273438\n",
      "Train Epoch: 41 [213248/225000 (95%)] Loss: 8064.531250\n",
      "Train Epoch: 41 [217344/225000 (97%)] Loss: 14579.505859\n",
      "Train Epoch: 41 [221440/225000 (98%)] Loss: 8000.980469\n",
      "    epoch          : 41\n",
      "    loss           : 10025.000604379977\n",
      "    val_loss       : 10344.464950172269\n",
      "Train Epoch: 42 [256/225000 (0%)] Loss: 7940.519531\n",
      "Train Epoch: 42 [4352/225000 (2%)] Loss: 12450.820312\n",
      "Train Epoch: 42 [8448/225000 (4%)] Loss: 10912.841797\n",
      "Train Epoch: 42 [12544/225000 (6%)] Loss: 11029.291016\n",
      "Train Epoch: 42 [16640/225000 (7%)] Loss: 9771.964844\n",
      "Train Epoch: 42 [20736/225000 (9%)] Loss: 9727.623047\n",
      "Train Epoch: 42 [24832/225000 (11%)] Loss: 14378.152344\n",
      "Train Epoch: 42 [28928/225000 (13%)] Loss: 16586.044922\n",
      "Train Epoch: 42 [33024/225000 (15%)] Loss: 14370.589844\n",
      "Train Epoch: 42 [37120/225000 (16%)] Loss: 9859.392578\n",
      "Train Epoch: 42 [41216/225000 (18%)] Loss: 8108.503906\n",
      "Train Epoch: 42 [45312/225000 (20%)] Loss: 8038.591797\n",
      "Train Epoch: 42 [49408/225000 (22%)] Loss: 8103.234375\n",
      "Train Epoch: 42 [53504/225000 (24%)] Loss: 12372.826172\n",
      "Train Epoch: 42 [57600/225000 (26%)] Loss: 8013.714844\n",
      "Train Epoch: 42 [61696/225000 (27%)] Loss: 7970.636719\n",
      "Train Epoch: 42 [65792/225000 (29%)] Loss: 9495.837891\n",
      "Train Epoch: 42 [69888/225000 (31%)] Loss: 11134.517578\n",
      "Train Epoch: 42 [73984/225000 (33%)] Loss: 9905.041016\n",
      "Train Epoch: 42 [78080/225000 (35%)] Loss: 12565.099609\n",
      "Train Epoch: 42 [82176/225000 (37%)] Loss: 7915.865234\n",
      "Train Epoch: 42 [86272/225000 (38%)] Loss: 10662.613281\n",
      "Train Epoch: 42 [90368/225000 (40%)] Loss: 10960.082031\n",
      "Train Epoch: 42 [94464/225000 (42%)] Loss: 10409.380859\n",
      "Train Epoch: 42 [98560/225000 (44%)] Loss: 9920.601562\n",
      "Train Epoch: 42 [102656/225000 (46%)] Loss: 10026.003906\n",
      "Train Epoch: 42 [106752/225000 (47%)] Loss: 12620.308594\n",
      "Train Epoch: 42 [110848/225000 (49%)] Loss: 8097.265625\n",
      "Train Epoch: 42 [114944/225000 (51%)] Loss: 9870.394531\n",
      "Train Epoch: 42 [119040/225000 (53%)] Loss: 10651.820312\n",
      "Train Epoch: 42 [123136/225000 (55%)] Loss: 9789.431641\n",
      "Train Epoch: 42 [127232/225000 (57%)] Loss: 13981.134766\n",
      "Train Epoch: 42 [131328/225000 (58%)] Loss: 12834.501953\n",
      "Train Epoch: 42 [135424/225000 (60%)] Loss: 8230.621094\n",
      "Train Epoch: 42 [139520/225000 (62%)] Loss: 10002.187500\n",
      "Train Epoch: 42 [143616/225000 (64%)] Loss: 15564.085938\n",
      "Train Epoch: 42 [147712/225000 (66%)] Loss: 8223.214844\n",
      "Train Epoch: 42 [151808/225000 (67%)] Loss: 8053.644531\n",
      "Train Epoch: 42 [155904/225000 (69%)] Loss: 8059.275391\n",
      "Train Epoch: 42 [160000/225000 (71%)] Loss: 8045.218750\n",
      "Train Epoch: 42 [164096/225000 (73%)] Loss: 9802.267578\n",
      "Train Epoch: 42 [168192/225000 (75%)] Loss: 8135.470703\n",
      "Train Epoch: 42 [172288/225000 (77%)] Loss: 9908.714844\n",
      "Train Epoch: 42 [176384/225000 (78%)] Loss: 9721.765625\n",
      "Train Epoch: 42 [180480/225000 (80%)] Loss: 8113.427734\n",
      "Train Epoch: 42 [184576/225000 (82%)] Loss: 9791.789062\n",
      "Train Epoch: 42 [188672/225000 (84%)] Loss: 8106.162109\n",
      "Train Epoch: 42 [192768/225000 (86%)] Loss: 8398.191406\n",
      "Train Epoch: 42 [196864/225000 (87%)] Loss: 8205.507812\n",
      "Train Epoch: 42 [200960/225000 (89%)] Loss: 12775.167969\n",
      "Train Epoch: 42 [205056/225000 (91%)] Loss: 10681.343750\n",
      "Train Epoch: 42 [209152/225000 (93%)] Loss: 10520.843750\n",
      "Train Epoch: 42 [213248/225000 (95%)] Loss: 13690.277344\n",
      "Train Epoch: 42 [217344/225000 (97%)] Loss: 13685.296875\n",
      "Train Epoch: 42 [221440/225000 (98%)] Loss: 9943.179688\n",
      "    epoch          : 42\n",
      "    loss           : 9930.474842683447\n",
      "    val_loss       : 9389.761010812254\n",
      "Train Epoch: 43 [256/225000 (0%)] Loss: 8141.281250\n",
      "Train Epoch: 43 [4352/225000 (2%)] Loss: 8158.755859\n",
      "Train Epoch: 43 [8448/225000 (4%)] Loss: 8137.324219\n",
      "Train Epoch: 43 [12544/225000 (6%)] Loss: 10776.339844\n",
      "Train Epoch: 43 [16640/225000 (7%)] Loss: 8298.666016\n",
      "Train Epoch: 43 [20736/225000 (9%)] Loss: 7944.826172\n",
      "Train Epoch: 43 [24832/225000 (11%)] Loss: 12783.355469\n",
      "Train Epoch: 43 [28928/225000 (13%)] Loss: 7961.658203\n",
      "Train Epoch: 43 [33024/225000 (15%)] Loss: 12333.130859\n",
      "Train Epoch: 43 [37120/225000 (16%)] Loss: 10919.322266\n",
      "Train Epoch: 43 [41216/225000 (18%)] Loss: 8270.332031\n",
      "Train Epoch: 43 [45312/225000 (20%)] Loss: 8118.857422\n",
      "Train Epoch: 43 [49408/225000 (22%)] Loss: 9635.572266\n",
      "Train Epoch: 43 [53504/225000 (24%)] Loss: 9523.878906\n",
      "Train Epoch: 43 [57600/225000 (26%)] Loss: 8103.435547\n",
      "Train Epoch: 43 [61696/225000 (27%)] Loss: 13843.810547\n",
      "Train Epoch: 43 [65792/225000 (29%)] Loss: 8131.433594\n",
      "Train Epoch: 43 [69888/225000 (31%)] Loss: 8115.923828\n",
      "Train Epoch: 43 [73984/225000 (33%)] Loss: 9795.234375\n",
      "Train Epoch: 43 [78080/225000 (35%)] Loss: 13872.962891\n",
      "Train Epoch: 43 [82176/225000 (37%)] Loss: 16219.839844\n",
      "Train Epoch: 43 [86272/225000 (38%)] Loss: 18366.050781\n",
      "Train Epoch: 43 [90368/225000 (40%)] Loss: 15027.628906\n",
      "Train Epoch: 43 [94464/225000 (42%)] Loss: 13575.035156\n",
      "Train Epoch: 43 [98560/225000 (44%)] Loss: 8109.281250\n",
      "Train Epoch: 43 [102656/225000 (46%)] Loss: 9749.527344\n",
      "Train Epoch: 43 [106752/225000 (47%)] Loss: 8098.306641\n",
      "Train Epoch: 43 [110848/225000 (49%)] Loss: 8073.898438\n",
      "Train Epoch: 43 [114944/225000 (51%)] Loss: 8144.574219\n",
      "Train Epoch: 43 [119040/225000 (53%)] Loss: 7996.933594\n",
      "Train Epoch: 43 [123136/225000 (55%)] Loss: 10866.232422\n",
      "Train Epoch: 43 [127232/225000 (57%)] Loss: 8318.958984\n",
      "Train Epoch: 43 [131328/225000 (58%)] Loss: 12443.410156\n",
      "Train Epoch: 43 [135424/225000 (60%)] Loss: 14076.136719\n",
      "Train Epoch: 43 [139520/225000 (62%)] Loss: 8009.753906\n",
      "Train Epoch: 43 [143616/225000 (64%)] Loss: 8148.427734\n",
      "Train Epoch: 43 [147712/225000 (66%)] Loss: 10713.734375\n",
      "Train Epoch: 43 [151808/225000 (67%)] Loss: 8143.246094\n",
      "Train Epoch: 43 [155904/225000 (69%)] Loss: 8004.568359\n",
      "Train Epoch: 43 [160000/225000 (71%)] Loss: 8111.326172\n",
      "Train Epoch: 43 [164096/225000 (73%)] Loss: 8124.916016\n",
      "Train Epoch: 43 [168192/225000 (75%)] Loss: 8028.382812\n",
      "Train Epoch: 43 [172288/225000 (77%)] Loss: 8028.136719\n",
      "Train Epoch: 43 [176384/225000 (78%)] Loss: 9850.746094\n",
      "Train Epoch: 43 [180480/225000 (80%)] Loss: 8226.929688\n",
      "Train Epoch: 43 [184576/225000 (82%)] Loss: 8165.390625\n",
      "Train Epoch: 43 [188672/225000 (84%)] Loss: 21339.666016\n",
      "Train Epoch: 43 [192768/225000 (86%)] Loss: 8107.810547\n",
      "Train Epoch: 43 [196864/225000 (87%)] Loss: 10493.054688\n",
      "Train Epoch: 43 [200960/225000 (89%)] Loss: 8052.462891\n",
      "Train Epoch: 43 [205056/225000 (91%)] Loss: 8195.785156\n",
      "Train Epoch: 43 [209152/225000 (93%)] Loss: 8189.304688\n",
      "Train Epoch: 43 [213248/225000 (95%)] Loss: 8207.285156\n",
      "Train Epoch: 43 [217344/225000 (97%)] Loss: 15107.027344\n",
      "Train Epoch: 43 [221440/225000 (98%)] Loss: 8238.431641\n",
      "    epoch          : 43\n",
      "    loss           : 9820.058827058447\n",
      "    val_loss       : 9458.952509381334\n",
      "Train Epoch: 44 [256/225000 (0%)] Loss: 8137.431641\n",
      "Train Epoch: 44 [4352/225000 (2%)] Loss: 12838.955078\n",
      "Train Epoch: 44 [8448/225000 (4%)] Loss: 8163.087891\n",
      "Train Epoch: 44 [12544/225000 (6%)] Loss: 9819.273438\n",
      "Train Epoch: 44 [16640/225000 (7%)] Loss: 9697.884766\n",
      "Train Epoch: 44 [20736/225000 (9%)] Loss: 10681.716797\n",
      "Train Epoch: 44 [24832/225000 (11%)] Loss: 8378.701172\n",
      "Train Epoch: 44 [28928/225000 (13%)] Loss: 8292.046875\n",
      "Train Epoch: 44 [33024/225000 (15%)] Loss: 8065.111328\n",
      "Train Epoch: 44 [37120/225000 (16%)] Loss: 7914.722656\n",
      "Train Epoch: 44 [41216/225000 (18%)] Loss: 10555.724609\n",
      "Train Epoch: 44 [45312/225000 (20%)] Loss: 9711.666016\n",
      "Train Epoch: 44 [49408/225000 (22%)] Loss: 8065.431641\n",
      "Train Epoch: 44 [53504/225000 (24%)] Loss: 8002.458984\n",
      "Train Epoch: 44 [57600/225000 (26%)] Loss: 12240.285156\n",
      "Train Epoch: 44 [61696/225000 (27%)] Loss: 8149.470703\n",
      "Train Epoch: 44 [65792/225000 (29%)] Loss: 8119.533203\n",
      "Train Epoch: 44 [69888/225000 (31%)] Loss: 8024.179688\n",
      "Train Epoch: 44 [73984/225000 (33%)] Loss: 13619.806641\n",
      "Train Epoch: 44 [78080/225000 (35%)] Loss: 9649.460938\n",
      "Train Epoch: 44 [82176/225000 (37%)] Loss: 11556.921875\n",
      "Train Epoch: 44 [86272/225000 (38%)] Loss: 9697.935547\n",
      "Train Epoch: 44 [90368/225000 (40%)] Loss: 8166.169922\n",
      "Train Epoch: 44 [94464/225000 (42%)] Loss: 8120.470703\n",
      "Train Epoch: 44 [98560/225000 (44%)] Loss: 8128.550781\n",
      "Train Epoch: 44 [102656/225000 (46%)] Loss: 8020.884766\n",
      "Train Epoch: 44 [106752/225000 (47%)] Loss: 7888.623047\n",
      "Train Epoch: 44 [110848/225000 (49%)] Loss: 7966.486328\n",
      "Train Epoch: 44 [114944/225000 (51%)] Loss: 8282.224609\n",
      "Train Epoch: 44 [119040/225000 (53%)] Loss: 8012.960938\n",
      "Train Epoch: 44 [123136/225000 (55%)] Loss: 8127.564453\n",
      "Train Epoch: 44 [127232/225000 (57%)] Loss: 8107.789062\n",
      "Train Epoch: 44 [131328/225000 (58%)] Loss: 8064.892578\n",
      "Train Epoch: 44 [135424/225000 (60%)] Loss: 8006.140625\n",
      "Train Epoch: 44 [139520/225000 (62%)] Loss: 8214.466797\n",
      "Train Epoch: 44 [143616/225000 (64%)] Loss: 7871.621094\n",
      "Train Epoch: 44 [147712/225000 (66%)] Loss: 8288.169922\n",
      "Train Epoch: 44 [151808/225000 (67%)] Loss: 9899.394531\n",
      "Train Epoch: 44 [155904/225000 (69%)] Loss: 10731.648438\n",
      "Train Epoch: 44 [160000/225000 (71%)] Loss: 12473.570312\n",
      "Train Epoch: 44 [164096/225000 (73%)] Loss: 10580.380859\n",
      "Train Epoch: 44 [168192/225000 (75%)] Loss: 8121.728516\n",
      "Train Epoch: 44 [172288/225000 (77%)] Loss: 8130.703125\n",
      "Train Epoch: 44 [176384/225000 (78%)] Loss: 7895.197266\n",
      "Train Epoch: 44 [180480/225000 (80%)] Loss: 7973.173828\n",
      "Train Epoch: 44 [184576/225000 (82%)] Loss: 8085.537109\n",
      "Train Epoch: 44 [188672/225000 (84%)] Loss: 8013.214844\n",
      "Train Epoch: 44 [192768/225000 (86%)] Loss: 10680.484375\n",
      "Train Epoch: 44 [196864/225000 (87%)] Loss: 8167.664062\n",
      "Train Epoch: 44 [200960/225000 (89%)] Loss: 12361.544922\n",
      "Train Epoch: 44 [205056/225000 (91%)] Loss: 8032.585938\n",
      "Train Epoch: 44 [209152/225000 (93%)] Loss: 8107.185547\n",
      "Train Epoch: 44 [213248/225000 (95%)] Loss: 8110.015625\n",
      "Train Epoch: 44 [217344/225000 (97%)] Loss: 15092.750000\n",
      "Train Epoch: 44 [221440/225000 (98%)] Loss: 15731.572266\n",
      "    epoch          : 44\n",
      "    loss           : 9749.948706582409\n",
      "    val_loss       : 9489.375695194516\n",
      "Train Epoch: 45 [256/225000 (0%)] Loss: 8035.865234\n",
      "Train Epoch: 45 [4352/225000 (2%)] Loss: 8382.855469\n",
      "Train Epoch: 45 [8448/225000 (4%)] Loss: 8037.166016\n",
      "Train Epoch: 45 [12544/225000 (6%)] Loss: 9427.826172\n",
      "Train Epoch: 45 [16640/225000 (7%)] Loss: 12980.527344\n",
      "Train Epoch: 45 [20736/225000 (9%)] Loss: 17589.437500\n",
      "Train Epoch: 45 [24832/225000 (11%)] Loss: 8050.041016\n",
      "Train Epoch: 45 [28928/225000 (13%)] Loss: 16112.611328\n",
      "Train Epoch: 45 [33024/225000 (15%)] Loss: 8081.964844\n",
      "Train Epoch: 45 [37120/225000 (16%)] Loss: 8256.660156\n",
      "Train Epoch: 45 [41216/225000 (18%)] Loss: 8084.951172\n",
      "Train Epoch: 45 [45312/225000 (20%)] Loss: 9571.441406\n",
      "Train Epoch: 45 [49408/225000 (22%)] Loss: 13892.830078\n",
      "Train Epoch: 45 [53504/225000 (24%)] Loss: 8139.466797\n",
      "Train Epoch: 45 [57600/225000 (26%)] Loss: 16500.792969\n",
      "Train Epoch: 45 [61696/225000 (27%)] Loss: 9677.007812\n",
      "Train Epoch: 45 [65792/225000 (29%)] Loss: 8339.740234\n",
      "Train Epoch: 45 [69888/225000 (31%)] Loss: 8024.951172\n",
      "Train Epoch: 45 [73984/225000 (33%)] Loss: 8171.433594\n",
      "Train Epoch: 45 [78080/225000 (35%)] Loss: 8035.767578\n",
      "Train Epoch: 45 [82176/225000 (37%)] Loss: 8158.878906\n",
      "Train Epoch: 45 [86272/225000 (38%)] Loss: 8157.318359\n",
      "Train Epoch: 45 [90368/225000 (40%)] Loss: 8085.484375\n",
      "Train Epoch: 45 [94464/225000 (42%)] Loss: 8251.906250\n",
      "Train Epoch: 45 [98560/225000 (44%)] Loss: 8082.076172\n",
      "Train Epoch: 45 [102656/225000 (46%)] Loss: 8111.628906\n",
      "Train Epoch: 45 [106752/225000 (47%)] Loss: 8023.771484\n",
      "Train Epoch: 45 [110848/225000 (49%)] Loss: 8202.283203\n",
      "Train Epoch: 45 [114944/225000 (51%)] Loss: 8017.878906\n",
      "Train Epoch: 45 [119040/225000 (53%)] Loss: 8174.394531\n",
      "Train Epoch: 45 [123136/225000 (55%)] Loss: 8083.552734\n",
      "Train Epoch: 45 [127232/225000 (57%)] Loss: 10744.667969\n",
      "Train Epoch: 45 [131328/225000 (58%)] Loss: 7917.888672\n",
      "Train Epoch: 45 [135424/225000 (60%)] Loss: 8128.689453\n",
      "Train Epoch: 45 [139520/225000 (62%)] Loss: 12257.925781\n",
      "Train Epoch: 45 [143616/225000 (64%)] Loss: 12549.603516\n",
      "Train Epoch: 45 [147712/225000 (66%)] Loss: 9559.433594\n",
      "Train Epoch: 45 [151808/225000 (67%)] Loss: 8291.511719\n",
      "Train Epoch: 45 [155904/225000 (69%)] Loss: 10618.984375\n",
      "Train Epoch: 45 [160000/225000 (71%)] Loss: 8109.542969\n",
      "Train Epoch: 45 [164096/225000 (73%)] Loss: 8159.279297\n",
      "Train Epoch: 45 [168192/225000 (75%)] Loss: 7903.957031\n",
      "Train Epoch: 45 [172288/225000 (77%)] Loss: 8135.980469\n",
      "Train Epoch: 45 [176384/225000 (78%)] Loss: 8295.021484\n",
      "Train Epoch: 45 [180480/225000 (80%)] Loss: 8172.027344\n",
      "Train Epoch: 45 [184576/225000 (82%)] Loss: 8094.949219\n",
      "Train Epoch: 45 [188672/225000 (84%)] Loss: 12514.148438\n",
      "Train Epoch: 45 [192768/225000 (86%)] Loss: 9621.587891\n",
      "Train Epoch: 45 [196864/225000 (87%)] Loss: 13049.980469\n",
      "Train Epoch: 45 [200960/225000 (89%)] Loss: 11018.847656\n",
      "Train Epoch: 45 [205056/225000 (91%)] Loss: 8209.517578\n",
      "Train Epoch: 45 [209152/225000 (93%)] Loss: 10697.720703\n",
      "Train Epoch: 45 [213248/225000 (95%)] Loss: 8203.087891\n",
      "Train Epoch: 45 [217344/225000 (97%)] Loss: 8172.568359\n",
      "Train Epoch: 45 [221440/225000 (98%)] Loss: 8248.033203\n",
      "    epoch          : 45\n",
      "    loss           : 9594.247476935794\n",
      "    val_loss       : 9365.297324034633\n",
      "Train Epoch: 46 [256/225000 (0%)] Loss: 8283.417969\n",
      "Train Epoch: 46 [4352/225000 (2%)] Loss: 8108.429688\n",
      "Train Epoch: 46 [8448/225000 (4%)] Loss: 8342.884766\n",
      "Train Epoch: 46 [12544/225000 (6%)] Loss: 11324.029297\n",
      "Train Epoch: 46 [16640/225000 (7%)] Loss: 7988.632812\n",
      "Train Epoch: 46 [20736/225000 (9%)] Loss: 8003.707031\n",
      "Train Epoch: 46 [24832/225000 (11%)] Loss: 8202.042969\n",
      "Train Epoch: 46 [28928/225000 (13%)] Loss: 10653.445312\n",
      "Train Epoch: 46 [33024/225000 (15%)] Loss: 8062.625000\n",
      "Train Epoch: 46 [37120/225000 (16%)] Loss: 14089.265625\n",
      "Train Epoch: 46 [41216/225000 (18%)] Loss: 8235.082031\n",
      "Train Epoch: 46 [45312/225000 (20%)] Loss: 14857.265625\n",
      "Train Epoch: 46 [49408/225000 (22%)] Loss: 15359.378906\n",
      "Train Epoch: 46 [53504/225000 (24%)] Loss: 8106.439453\n",
      "Train Epoch: 46 [57600/225000 (26%)] Loss: 9780.970703\n",
      "Train Epoch: 46 [61696/225000 (27%)] Loss: 8073.958984\n",
      "Train Epoch: 46 [65792/225000 (29%)] Loss: 8270.496094\n",
      "Train Epoch: 46 [69888/225000 (31%)] Loss: 8263.392578\n",
      "Train Epoch: 46 [73984/225000 (33%)] Loss: 7947.074219\n",
      "Train Epoch: 46 [78080/225000 (35%)] Loss: 15346.412109\n",
      "Train Epoch: 46 [82176/225000 (37%)] Loss: 8011.748047\n",
      "Train Epoch: 46 [86272/225000 (38%)] Loss: 14077.488281\n",
      "Train Epoch: 46 [90368/225000 (40%)] Loss: 10854.167969\n",
      "Train Epoch: 46 [94464/225000 (42%)] Loss: 9594.773438\n",
      "Train Epoch: 46 [98560/225000 (44%)] Loss: 10604.613281\n",
      "Train Epoch: 46 [102656/225000 (46%)] Loss: 9667.705078\n",
      "Train Epoch: 46 [106752/225000 (47%)] Loss: 13621.941406\n",
      "Train Epoch: 46 [110848/225000 (49%)] Loss: 15793.894531\n",
      "Train Epoch: 46 [114944/225000 (51%)] Loss: 8035.375000\n",
      "Train Epoch: 46 [119040/225000 (53%)] Loss: 9744.017578\n",
      "Train Epoch: 46 [123136/225000 (55%)] Loss: 7991.656250\n",
      "Train Epoch: 46 [127232/225000 (57%)] Loss: 8094.011719\n",
      "Train Epoch: 46 [131328/225000 (58%)] Loss: 7918.875000\n",
      "Train Epoch: 46 [135424/225000 (60%)] Loss: 12505.078125\n",
      "Train Epoch: 46 [139520/225000 (62%)] Loss: 7743.136719\n",
      "Train Epoch: 46 [143616/225000 (64%)] Loss: 10650.062500\n",
      "Train Epoch: 46 [147712/225000 (66%)] Loss: 7979.167969\n",
      "Train Epoch: 46 [151808/225000 (67%)] Loss: 8419.056641\n",
      "Train Epoch: 46 [155904/225000 (69%)] Loss: 9577.658203\n",
      "Train Epoch: 46 [160000/225000 (71%)] Loss: 8132.527344\n",
      "Train Epoch: 46 [164096/225000 (73%)] Loss: 7974.976562\n",
      "Train Epoch: 46 [168192/225000 (75%)] Loss: 7994.716797\n",
      "Train Epoch: 46 [172288/225000 (77%)] Loss: 8212.517578\n",
      "Train Epoch: 46 [176384/225000 (78%)] Loss: 15373.369141\n",
      "Train Epoch: 46 [180480/225000 (80%)] Loss: 8244.421875\n",
      "Train Epoch: 46 [184576/225000 (82%)] Loss: 7991.550781\n",
      "Train Epoch: 46 [188672/225000 (84%)] Loss: 8238.435547\n",
      "Train Epoch: 46 [192768/225000 (86%)] Loss: 9591.017578\n",
      "Train Epoch: 46 [196864/225000 (87%)] Loss: 8238.388672\n",
      "Train Epoch: 46 [200960/225000 (89%)] Loss: 8172.583984\n",
      "Train Epoch: 46 [205056/225000 (91%)] Loss: 8062.828125\n",
      "Train Epoch: 46 [209152/225000 (93%)] Loss: 8070.162109\n",
      "Train Epoch: 46 [213248/225000 (95%)] Loss: 8208.652344\n",
      "Train Epoch: 46 [217344/225000 (97%)] Loss: 8155.021484\n",
      "Train Epoch: 46 [221440/225000 (98%)] Loss: 7993.222656\n",
      "    epoch          : 46\n",
      "    loss           : 9560.783743067406\n",
      "    val_loss       : 9748.249485395392\n",
      "Train Epoch: 47 [256/225000 (0%)] Loss: 9604.955078\n",
      "Train Epoch: 47 [4352/225000 (2%)] Loss: 9897.103516\n",
      "Train Epoch: 47 [8448/225000 (4%)] Loss: 9557.380859\n",
      "Train Epoch: 47 [12544/225000 (6%)] Loss: 8196.681641\n",
      "Train Epoch: 47 [16640/225000 (7%)] Loss: 8165.728516\n",
      "Train Epoch: 47 [20736/225000 (9%)] Loss: 8132.740234\n",
      "Train Epoch: 47 [24832/225000 (11%)] Loss: 10436.982422\n",
      "Train Epoch: 47 [28928/225000 (13%)] Loss: 9694.375000\n",
      "Train Epoch: 47 [33024/225000 (15%)] Loss: 8222.031250\n",
      "Train Epoch: 47 [37120/225000 (16%)] Loss: 8175.066406\n",
      "Train Epoch: 47 [41216/225000 (18%)] Loss: 9561.982422\n",
      "Train Epoch: 47 [45312/225000 (20%)] Loss: 8261.488281\n",
      "Train Epoch: 47 [49408/225000 (22%)] Loss: 8006.564453\n",
      "Train Epoch: 47 [53504/225000 (24%)] Loss: 9806.527344\n",
      "Train Epoch: 47 [57600/225000 (26%)] Loss: 8142.638672\n",
      "Train Epoch: 47 [61696/225000 (27%)] Loss: 7896.923828\n",
      "Train Epoch: 47 [65792/225000 (29%)] Loss: 10524.089844\n",
      "Train Epoch: 47 [69888/225000 (31%)] Loss: 8235.197266\n",
      "Train Epoch: 47 [73984/225000 (33%)] Loss: 7899.677734\n",
      "Train Epoch: 47 [78080/225000 (35%)] Loss: 8143.742188\n",
      "Train Epoch: 47 [82176/225000 (37%)] Loss: 8051.566406\n",
      "Train Epoch: 47 [86272/225000 (38%)] Loss: 7961.292969\n",
      "Train Epoch: 47 [90368/225000 (40%)] Loss: 8164.105469\n",
      "Train Epoch: 47 [94464/225000 (42%)] Loss: 9623.271484\n",
      "Train Epoch: 47 [98560/225000 (44%)] Loss: 16049.087891\n",
      "Train Epoch: 47 [102656/225000 (46%)] Loss: 8062.296875\n",
      "Train Epoch: 47 [106752/225000 (47%)] Loss: 8030.802734\n",
      "Train Epoch: 47 [110848/225000 (49%)] Loss: 8092.640625\n",
      "Train Epoch: 47 [114944/225000 (51%)] Loss: 7994.402344\n",
      "Train Epoch: 47 [119040/225000 (53%)] Loss: 8048.437500\n",
      "Train Epoch: 47 [123136/225000 (55%)] Loss: 8252.050781\n",
      "Train Epoch: 47 [127232/225000 (57%)] Loss: 8061.761719\n",
      "Train Epoch: 47 [131328/225000 (58%)] Loss: 8019.527344\n",
      "Train Epoch: 47 [135424/225000 (60%)] Loss: 15480.960938\n",
      "Train Epoch: 47 [139520/225000 (62%)] Loss: 8144.500000\n",
      "Train Epoch: 47 [143616/225000 (64%)] Loss: 8189.453125\n",
      "Train Epoch: 47 [147712/225000 (66%)] Loss: 8083.400391\n",
      "Train Epoch: 47 [151808/225000 (67%)] Loss: 8047.818359\n",
      "Train Epoch: 47 [155904/225000 (69%)] Loss: 8034.072266\n",
      "Train Epoch: 47 [160000/225000 (71%)] Loss: 10774.746094\n",
      "Train Epoch: 47 [164096/225000 (73%)] Loss: 8074.910156\n",
      "Train Epoch: 47 [168192/225000 (75%)] Loss: 9662.908203\n",
      "Train Epoch: 47 [172288/225000 (77%)] Loss: 9629.585938\n",
      "Train Epoch: 47 [176384/225000 (78%)] Loss: 8158.003906\n",
      "Train Epoch: 47 [180480/225000 (80%)] Loss: 9694.212891\n",
      "Train Epoch: 47 [184576/225000 (82%)] Loss: 10764.562500\n",
      "Train Epoch: 47 [188672/225000 (84%)] Loss: 9947.736328\n",
      "Train Epoch: 47 [192768/225000 (86%)] Loss: 9746.863281\n",
      "Train Epoch: 47 [196864/225000 (87%)] Loss: 8257.318359\n",
      "Train Epoch: 47 [200960/225000 (89%)] Loss: 7963.472656\n",
      "Train Epoch: 47 [205056/225000 (91%)] Loss: 10460.861328\n",
      "Train Epoch: 47 [209152/225000 (93%)] Loss: 8137.277344\n",
      "Train Epoch: 47 [213248/225000 (95%)] Loss: 9676.902344\n",
      "Train Epoch: 47 [217344/225000 (97%)] Loss: 8119.798828\n",
      "Train Epoch: 47 [221440/225000 (98%)] Loss: 8082.736328\n",
      "    epoch          : 47\n",
      "    loss           : 9599.84896944326\n",
      "    val_loss       : 9391.74419969442\n",
      "Train Epoch: 48 [256/225000 (0%)] Loss: 8245.804688\n",
      "Train Epoch: 48 [4352/225000 (2%)] Loss: 8141.335938\n",
      "Train Epoch: 48 [8448/225000 (4%)] Loss: 8003.044922\n",
      "Train Epoch: 48 [12544/225000 (6%)] Loss: 9622.330078\n",
      "Train Epoch: 48 [16640/225000 (7%)] Loss: 14257.285156\n",
      "Train Epoch: 48 [20736/225000 (9%)] Loss: 9804.611328\n",
      "Train Epoch: 48 [24832/225000 (11%)] Loss: 8031.257812\n",
      "Train Epoch: 48 [28928/225000 (13%)] Loss: 10448.228516\n",
      "Train Epoch: 48 [33024/225000 (15%)] Loss: 8148.769531\n",
      "Train Epoch: 48 [37120/225000 (16%)] Loss: 8181.394531\n",
      "Train Epoch: 48 [41216/225000 (18%)] Loss: 10616.773438\n",
      "Train Epoch: 48 [45312/225000 (20%)] Loss: 10702.013672\n",
      "Train Epoch: 48 [49408/225000 (22%)] Loss: 8145.828125\n",
      "Train Epoch: 48 [53504/225000 (24%)] Loss: 8122.558594\n",
      "Train Epoch: 48 [57600/225000 (26%)] Loss: 9666.085938\n",
      "Train Epoch: 48 [61696/225000 (27%)] Loss: 9747.408203\n",
      "Train Epoch: 48 [65792/225000 (29%)] Loss: 8122.474609\n",
      "Train Epoch: 48 [69888/225000 (31%)] Loss: 9612.707031\n",
      "Train Epoch: 48 [73984/225000 (33%)] Loss: 8039.619141\n",
      "Train Epoch: 48 [78080/225000 (35%)] Loss: 9619.876953\n",
      "Train Epoch: 48 [82176/225000 (37%)] Loss: 10559.917969\n",
      "Train Epoch: 48 [86272/225000 (38%)] Loss: 12484.523438\n",
      "Train Epoch: 48 [90368/225000 (40%)] Loss: 9736.117188\n",
      "Train Epoch: 48 [94464/225000 (42%)] Loss: 17821.902344\n",
      "Train Epoch: 48 [98560/225000 (44%)] Loss: 8127.312500\n",
      "Train Epoch: 48 [102656/225000 (46%)] Loss: 13715.917969\n",
      "Train Epoch: 48 [106752/225000 (47%)] Loss: 8174.029297\n",
      "Train Epoch: 48 [110848/225000 (49%)] Loss: 8232.609375\n",
      "Train Epoch: 48 [114944/225000 (51%)] Loss: 8355.607422\n",
      "Train Epoch: 48 [119040/225000 (53%)] Loss: 9762.271484\n",
      "Train Epoch: 48 [123136/225000 (55%)] Loss: 9725.445312\n",
      "Train Epoch: 48 [127232/225000 (57%)] Loss: 8055.214844\n",
      "Train Epoch: 48 [131328/225000 (58%)] Loss: 10420.224609\n",
      "Train Epoch: 48 [135424/225000 (60%)] Loss: 8025.230469\n",
      "Train Epoch: 48 [139520/225000 (62%)] Loss: 15635.753906\n",
      "Train Epoch: 48 [143616/225000 (64%)] Loss: 8313.396484\n",
      "Train Epoch: 48 [147712/225000 (66%)] Loss: 7998.607422\n",
      "Train Epoch: 48 [151808/225000 (67%)] Loss: 8131.058594\n",
      "Train Epoch: 48 [155904/225000 (69%)] Loss: 8250.279297\n",
      "Train Epoch: 48 [160000/225000 (71%)] Loss: 12505.166016\n",
      "Train Epoch: 48 [164096/225000 (73%)] Loss: 8251.175781\n",
      "Train Epoch: 48 [168192/225000 (75%)] Loss: 8287.394531\n",
      "Train Epoch: 48 [172288/225000 (77%)] Loss: 9856.060547\n",
      "Train Epoch: 48 [176384/225000 (78%)] Loss: 10588.246094\n",
      "Train Epoch: 48 [180480/225000 (80%)] Loss: 7907.597656\n",
      "Train Epoch: 48 [184576/225000 (82%)] Loss: 7815.052734\n",
      "Train Epoch: 48 [188672/225000 (84%)] Loss: 8224.593750\n",
      "Train Epoch: 48 [192768/225000 (86%)] Loss: 9689.646484\n",
      "Train Epoch: 48 [196864/225000 (87%)] Loss: 10788.279297\n",
      "Train Epoch: 48 [200960/225000 (89%)] Loss: 8199.210938\n",
      "Train Epoch: 48 [205056/225000 (91%)] Loss: 10430.335938\n",
      "Train Epoch: 48 [209152/225000 (93%)] Loss: 8195.300781\n",
      "Train Epoch: 48 [213248/225000 (95%)] Loss: 8183.423828\n",
      "Train Epoch: 48 [217344/225000 (97%)] Loss: 13224.386719\n",
      "Train Epoch: 48 [221440/225000 (98%)] Loss: 15449.166016\n",
      "    epoch          : 48\n",
      "    loss           : 9576.069720340942\n",
      "    val_loss       : 9653.011402280963\n",
      "Train Epoch: 49 [256/225000 (0%)] Loss: 8208.197266\n",
      "Train Epoch: 49 [4352/225000 (2%)] Loss: 10542.806641\n",
      "Train Epoch: 49 [8448/225000 (4%)] Loss: 10615.222656\n",
      "Train Epoch: 49 [12544/225000 (6%)] Loss: 8241.453125\n",
      "Train Epoch: 49 [16640/225000 (7%)] Loss: 16330.917969\n",
      "Train Epoch: 49 [20736/225000 (9%)] Loss: 8071.015625\n",
      "Train Epoch: 49 [24832/225000 (11%)] Loss: 7980.080078\n",
      "Train Epoch: 49 [28928/225000 (13%)] Loss: 8036.226562\n",
      "Train Epoch: 49 [33024/225000 (15%)] Loss: 12369.810547\n",
      "Train Epoch: 49 [37120/225000 (16%)] Loss: 8253.882812\n",
      "Train Epoch: 49 [41216/225000 (18%)] Loss: 8029.871094\n",
      "Train Epoch: 49 [45312/225000 (20%)] Loss: 11050.337891\n",
      "Train Epoch: 49 [49408/225000 (22%)] Loss: 7998.214844\n",
      "Train Epoch: 49 [53504/225000 (24%)] Loss: 10564.093750\n",
      "Train Epoch: 49 [57600/225000 (26%)] Loss: 8073.273438\n",
      "Train Epoch: 49 [61696/225000 (27%)] Loss: 9845.173828\n",
      "Train Epoch: 49 [65792/225000 (29%)] Loss: 13864.451172\n",
      "Train Epoch: 49 [69888/225000 (31%)] Loss: 8006.501953\n",
      "Train Epoch: 49 [73984/225000 (33%)] Loss: 9703.845703\n",
      "Train Epoch: 49 [78080/225000 (35%)] Loss: 8126.580078\n",
      "Train Epoch: 49 [82176/225000 (37%)] Loss: 7926.035156\n",
      "Train Epoch: 49 [86272/225000 (38%)] Loss: 19572.511719\n",
      "Train Epoch: 49 [90368/225000 (40%)] Loss: 10561.156250\n",
      "Train Epoch: 49 [94464/225000 (42%)] Loss: 9771.513672\n",
      "Train Epoch: 49 [98560/225000 (44%)] Loss: 8114.345703\n",
      "Train Epoch: 49 [102656/225000 (46%)] Loss: 8049.119141\n",
      "Train Epoch: 49 [106752/225000 (47%)] Loss: 8127.187500\n",
      "Train Epoch: 49 [110848/225000 (49%)] Loss: 14166.064453\n",
      "Train Epoch: 49 [114944/225000 (51%)] Loss: 10090.835938\n",
      "Train Epoch: 49 [119040/225000 (53%)] Loss: 10649.541016\n",
      "Train Epoch: 49 [123136/225000 (55%)] Loss: 10677.437500\n",
      "Train Epoch: 49 [127232/225000 (57%)] Loss: 8075.363281\n",
      "Train Epoch: 49 [131328/225000 (58%)] Loss: 10759.789062\n",
      "Train Epoch: 49 [135424/225000 (60%)] Loss: 8053.089844\n",
      "Train Epoch: 49 [139520/225000 (62%)] Loss: 10534.921875\n",
      "Train Epoch: 49 [143616/225000 (64%)] Loss: 8029.248047\n",
      "Train Epoch: 49 [147712/225000 (66%)] Loss: 8098.970703\n",
      "Train Epoch: 49 [151808/225000 (67%)] Loss: 14131.779297\n",
      "Train Epoch: 49 [155904/225000 (69%)] Loss: 8061.228516\n",
      "Train Epoch: 49 [160000/225000 (71%)] Loss: 8029.300781\n",
      "Train Epoch: 49 [164096/225000 (73%)] Loss: 11441.158203\n",
      "Train Epoch: 49 [168192/225000 (75%)] Loss: 8006.707031\n",
      "Train Epoch: 49 [172288/225000 (77%)] Loss: 12432.142578\n",
      "Train Epoch: 49 [176384/225000 (78%)] Loss: 10577.730469\n",
      "Train Epoch: 49 [180480/225000 (80%)] Loss: 8229.351562\n",
      "Train Epoch: 49 [184576/225000 (82%)] Loss: 9617.302734\n",
      "Train Epoch: 49 [188672/225000 (84%)] Loss: 8019.115234\n",
      "Train Epoch: 49 [192768/225000 (86%)] Loss: 8118.332031\n",
      "Train Epoch: 49 [196864/225000 (87%)] Loss: 9718.445312\n",
      "Train Epoch: 49 [200960/225000 (89%)] Loss: 8008.445312\n",
      "Train Epoch: 49 [205056/225000 (91%)] Loss: 8148.281250\n",
      "Train Epoch: 49 [209152/225000 (93%)] Loss: 8062.455078\n",
      "Train Epoch: 49 [213248/225000 (95%)] Loss: 8109.878906\n",
      "Train Epoch: 49 [217344/225000 (97%)] Loss: 8129.552734\n",
      "Train Epoch: 49 [221440/225000 (98%)] Loss: 10564.255859\n",
      "    epoch          : 49\n",
      "    loss           : 9508.372009207907\n",
      "    val_loss       : 9791.646771031155\n",
      "Train Epoch: 50 [256/225000 (0%)] Loss: 8149.951172\n",
      "Train Epoch: 50 [4352/225000 (2%)] Loss: 8348.808594\n",
      "Train Epoch: 50 [8448/225000 (4%)] Loss: 8166.849609\n",
      "Train Epoch: 50 [12544/225000 (6%)] Loss: 10575.468750\n",
      "Train Epoch: 50 [16640/225000 (7%)] Loss: 8210.392578\n",
      "Train Epoch: 50 [20736/225000 (9%)] Loss: 8132.091797\n",
      "Train Epoch: 50 [24832/225000 (11%)] Loss: 13658.212891\n",
      "Train Epoch: 50 [28928/225000 (13%)] Loss: 14150.076172\n",
      "Train Epoch: 50 [33024/225000 (15%)] Loss: 8047.595703\n",
      "Train Epoch: 50 [37120/225000 (16%)] Loss: 8086.513672\n",
      "Train Epoch: 50 [41216/225000 (18%)] Loss: 12531.753906\n",
      "Train Epoch: 50 [45312/225000 (20%)] Loss: 18297.935547\n",
      "Train Epoch: 50 [49408/225000 (22%)] Loss: 12672.826172\n",
      "Train Epoch: 50 [53504/225000 (24%)] Loss: 12070.445312\n",
      "Train Epoch: 50 [57600/225000 (26%)] Loss: 9814.570312\n",
      "Train Epoch: 50 [61696/225000 (27%)] Loss: 9604.625000\n",
      "Train Epoch: 50 [65792/225000 (29%)] Loss: 8039.916016\n",
      "Train Epoch: 50 [69888/225000 (31%)] Loss: 8179.455078\n",
      "Train Epoch: 50 [73984/225000 (33%)] Loss: 8019.900391\n",
      "Train Epoch: 50 [78080/225000 (35%)] Loss: 7854.259766\n",
      "Train Epoch: 50 [82176/225000 (37%)] Loss: 8202.589844\n",
      "Train Epoch: 50 [86272/225000 (38%)] Loss: 8077.724609\n",
      "Train Epoch: 50 [90368/225000 (40%)] Loss: 12596.873047\n",
      "Train Epoch: 50 [94464/225000 (42%)] Loss: 8012.220703\n",
      "Train Epoch: 50 [98560/225000 (44%)] Loss: 12267.181641\n",
      "Train Epoch: 50 [102656/225000 (46%)] Loss: 7987.019531\n",
      "Train Epoch: 50 [106752/225000 (47%)] Loss: 8111.251953\n",
      "Train Epoch: 50 [110848/225000 (49%)] Loss: 8190.146484\n",
      "Train Epoch: 50 [114944/225000 (51%)] Loss: 8011.613281\n",
      "Train Epoch: 50 [119040/225000 (53%)] Loss: 9733.033203\n",
      "Train Epoch: 50 [123136/225000 (55%)] Loss: 16355.794922\n",
      "Train Epoch: 50 [127232/225000 (57%)] Loss: 11999.271484\n",
      "Train Epoch: 50 [131328/225000 (58%)] Loss: 13902.529297\n",
      "Train Epoch: 50 [135424/225000 (60%)] Loss: 8075.542969\n",
      "Train Epoch: 50 [139520/225000 (62%)] Loss: 8100.613281\n",
      "Train Epoch: 50 [143616/225000 (64%)] Loss: 8084.974609\n",
      "Train Epoch: 50 [147712/225000 (66%)] Loss: 8204.968750\n",
      "Train Epoch: 50 [151808/225000 (67%)] Loss: 8159.162109\n",
      "Train Epoch: 50 [155904/225000 (69%)] Loss: 8184.291016\n",
      "Train Epoch: 50 [160000/225000 (71%)] Loss: 8219.208984\n",
      "Train Epoch: 50 [164096/225000 (73%)] Loss: 8197.496094\n",
      "Train Epoch: 50 [168192/225000 (75%)] Loss: 8110.882812\n",
      "Train Epoch: 50 [172288/225000 (77%)] Loss: 13913.205078\n",
      "Train Epoch: 50 [176384/225000 (78%)] Loss: 7982.138672\n",
      "Train Epoch: 50 [180480/225000 (80%)] Loss: 7868.109375\n",
      "Train Epoch: 50 [184576/225000 (82%)] Loss: 8198.398438\n",
      "Train Epoch: 50 [188672/225000 (84%)] Loss: 8299.716797\n",
      "Train Epoch: 50 [192768/225000 (86%)] Loss: 8098.841797\n",
      "Train Epoch: 50 [196864/225000 (87%)] Loss: 8219.826172\n",
      "Train Epoch: 50 [200960/225000 (89%)] Loss: 8083.621094\n",
      "Train Epoch: 50 [205056/225000 (91%)] Loss: 9684.093750\n",
      "Train Epoch: 50 [209152/225000 (93%)] Loss: 8304.689453\n",
      "Train Epoch: 50 [213248/225000 (95%)] Loss: 8223.533203\n",
      "Train Epoch: 50 [217344/225000 (97%)] Loss: 8124.751953\n",
      "Train Epoch: 50 [221440/225000 (98%)] Loss: 8187.187500\n",
      "    epoch          : 50\n",
      "    loss           : 9613.760837732863\n",
      "    val_loss       : 9146.205085289721\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0103_171400/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [256/225000 (0%)] Loss: 8285.650391\n",
      "Train Epoch: 51 [4352/225000 (2%)] Loss: 7967.759766\n",
      "Train Epoch: 51 [8448/225000 (4%)] Loss: 9776.171875\n",
      "Train Epoch: 51 [12544/225000 (6%)] Loss: 8028.283203\n",
      "Train Epoch: 51 [16640/225000 (7%)] Loss: 12240.554688\n",
      "Train Epoch: 51 [20736/225000 (9%)] Loss: 8095.203125\n",
      "Train Epoch: 51 [24832/225000 (11%)] Loss: 8037.740234\n",
      "Train Epoch: 51 [28928/225000 (13%)] Loss: 7978.513672\n",
      "Train Epoch: 51 [33024/225000 (15%)] Loss: 9782.179688\n",
      "Train Epoch: 51 [37120/225000 (16%)] Loss: 9597.359375\n",
      "Train Epoch: 51 [41216/225000 (18%)] Loss: 8194.585938\n",
      "Train Epoch: 51 [45312/225000 (20%)] Loss: 12582.201172\n",
      "Train Epoch: 51 [49408/225000 (22%)] Loss: 8076.279297\n",
      "Train Epoch: 51 [53504/225000 (24%)] Loss: 10730.697266\n",
      "Train Epoch: 51 [57600/225000 (26%)] Loss: 8106.634766\n",
      "Train Epoch: 51 [61696/225000 (27%)] Loss: 7911.345703\n",
      "Train Epoch: 51 [65792/225000 (29%)] Loss: 8358.574219\n",
      "Train Epoch: 51 [69888/225000 (31%)] Loss: 7968.152344\n",
      "Train Epoch: 51 [73984/225000 (33%)] Loss: 9837.212891\n",
      "Train Epoch: 51 [78080/225000 (35%)] Loss: 19323.244141\n",
      "Train Epoch: 51 [82176/225000 (37%)] Loss: 7889.304688\n",
      "Train Epoch: 51 [86272/225000 (38%)] Loss: 8150.767578\n",
      "Train Epoch: 51 [90368/225000 (40%)] Loss: 8204.251953\n",
      "Train Epoch: 51 [94464/225000 (42%)] Loss: 13505.083984\n",
      "Train Epoch: 51 [98560/225000 (44%)] Loss: 10022.585938\n",
      "Train Epoch: 51 [102656/225000 (46%)] Loss: 10645.556641\n",
      "Train Epoch: 51 [106752/225000 (47%)] Loss: 8096.505859\n",
      "Train Epoch: 51 [110848/225000 (49%)] Loss: 8035.708984\n",
      "Train Epoch: 51 [114944/225000 (51%)] Loss: 7793.193359\n",
      "Train Epoch: 51 [119040/225000 (53%)] Loss: 7979.582031\n",
      "Train Epoch: 51 [123136/225000 (55%)] Loss: 8066.478516\n",
      "Train Epoch: 51 [127232/225000 (57%)] Loss: 8029.736328\n",
      "Train Epoch: 51 [131328/225000 (58%)] Loss: 8000.619141\n",
      "Train Epoch: 51 [135424/225000 (60%)] Loss: 8142.105469\n",
      "Train Epoch: 51 [139520/225000 (62%)] Loss: 8236.646484\n",
      "Train Epoch: 51 [143616/225000 (64%)] Loss: 10544.152344\n",
      "Train Epoch: 51 [147712/225000 (66%)] Loss: 8099.490234\n",
      "Train Epoch: 51 [151808/225000 (67%)] Loss: 8005.666016\n",
      "Train Epoch: 51 [155904/225000 (69%)] Loss: 8350.746094\n",
      "Train Epoch: 51 [160000/225000 (71%)] Loss: 8338.894531\n",
      "Train Epoch: 51 [164096/225000 (73%)] Loss: 10575.449219\n",
      "Train Epoch: 51 [168192/225000 (75%)] Loss: 8298.355469\n",
      "Train Epoch: 51 [172288/225000 (77%)] Loss: 10685.541016\n",
      "Train Epoch: 51 [176384/225000 (78%)] Loss: 8145.125000\n",
      "Train Epoch: 51 [180480/225000 (80%)] Loss: 8354.031250\n",
      "Train Epoch: 51 [184576/225000 (82%)] Loss: 13836.964844\n",
      "Train Epoch: 51 [188672/225000 (84%)] Loss: 7991.318359\n",
      "Train Epoch: 51 [192768/225000 (86%)] Loss: 10502.890625\n",
      "Train Epoch: 51 [196864/225000 (87%)] Loss: 9597.878906\n",
      "Train Epoch: 51 [200960/225000 (89%)] Loss: 8061.742188\n",
      "Train Epoch: 51 [205056/225000 (91%)] Loss: 8095.833984\n",
      "Train Epoch: 51 [209152/225000 (93%)] Loss: 8064.716797\n",
      "Train Epoch: 51 [213248/225000 (95%)] Loss: 8041.583984\n",
      "Train Epoch: 51 [217344/225000 (97%)] Loss: 8038.208984\n",
      "Train Epoch: 51 [221440/225000 (98%)] Loss: 10576.376953\n",
      "    epoch          : 51\n",
      "    loss           : 9603.26791919973\n",
      "    val_loss       : 9576.792398953925\n",
      "Train Epoch: 52 [256/225000 (0%)] Loss: 12625.261719\n",
      "Train Epoch: 52 [4352/225000 (2%)] Loss: 8027.908203\n",
      "Train Epoch: 52 [8448/225000 (4%)] Loss: 8080.072266\n",
      "Train Epoch: 52 [12544/225000 (6%)] Loss: 7931.775391\n",
      "Train Epoch: 52 [16640/225000 (7%)] Loss: 8024.685547\n",
      "Train Epoch: 52 [20736/225000 (9%)] Loss: 8359.447266\n",
      "Train Epoch: 52 [24832/225000 (11%)] Loss: 8085.775391\n",
      "Train Epoch: 52 [28928/225000 (13%)] Loss: 15071.939453\n",
      "Train Epoch: 52 [33024/225000 (15%)] Loss: 8049.316406\n",
      "Train Epoch: 52 [37120/225000 (16%)] Loss: 8015.859375\n",
      "Train Epoch: 52 [41216/225000 (18%)] Loss: 9576.537109\n",
      "Train Epoch: 52 [45312/225000 (20%)] Loss: 11038.986328\n",
      "Train Epoch: 52 [49408/225000 (22%)] Loss: 8094.166016\n",
      "Train Epoch: 52 [53504/225000 (24%)] Loss: 12054.744141\n",
      "Train Epoch: 52 [57600/225000 (26%)] Loss: 8008.787109\n",
      "Train Epoch: 52 [61696/225000 (27%)] Loss: 8038.683594\n",
      "Train Epoch: 52 [65792/225000 (29%)] Loss: 8112.714844\n",
      "Train Epoch: 52 [69888/225000 (31%)] Loss: 7911.087891\n",
      "Train Epoch: 52 [73984/225000 (33%)] Loss: 8066.072266\n",
      "Train Epoch: 52 [78080/225000 (35%)] Loss: 8015.417969\n",
      "Train Epoch: 52 [82176/225000 (37%)] Loss: 13972.054688\n",
      "Train Epoch: 52 [86272/225000 (38%)] Loss: 8151.574219\n",
      "Train Epoch: 52 [90368/225000 (40%)] Loss: 8046.894531\n",
      "Train Epoch: 52 [94464/225000 (42%)] Loss: 13031.367188\n",
      "Train Epoch: 52 [98560/225000 (44%)] Loss: 12382.519531\n",
      "Train Epoch: 52 [102656/225000 (46%)] Loss: 8336.958984\n",
      "Train Epoch: 52 [106752/225000 (47%)] Loss: 8106.873047\n",
      "Train Epoch: 52 [110848/225000 (49%)] Loss: 8004.371094\n",
      "Train Epoch: 52 [114944/225000 (51%)] Loss: 10511.291016\n",
      "Train Epoch: 52 [119040/225000 (53%)] Loss: 17115.421875\n",
      "Train Epoch: 52 [123136/225000 (55%)] Loss: 13733.337891\n",
      "Train Epoch: 52 [127232/225000 (57%)] Loss: 10570.533203\n",
      "Train Epoch: 52 [131328/225000 (58%)] Loss: 7780.060547\n",
      "Train Epoch: 52 [135424/225000 (60%)] Loss: 8069.015625\n",
      "Train Epoch: 52 [139520/225000 (62%)] Loss: 8287.923828\n",
      "Train Epoch: 52 [143616/225000 (64%)] Loss: 8153.265625\n",
      "Train Epoch: 52 [147712/225000 (66%)] Loss: 12723.402344\n",
      "Train Epoch: 52 [151808/225000 (67%)] Loss: 20436.294922\n",
      "Train Epoch: 52 [155904/225000 (69%)] Loss: 8000.947266\n",
      "Train Epoch: 52 [160000/225000 (71%)] Loss: 8061.841797\n",
      "Train Epoch: 52 [164096/225000 (73%)] Loss: 14079.609375\n",
      "Train Epoch: 52 [168192/225000 (75%)] Loss: 8045.333984\n",
      "Train Epoch: 52 [172288/225000 (77%)] Loss: 7941.992188\n",
      "Train Epoch: 52 [176384/225000 (78%)] Loss: 8009.097656\n",
      "Train Epoch: 52 [180480/225000 (80%)] Loss: 8119.917969\n",
      "Train Epoch: 52 [184576/225000 (82%)] Loss: 8004.960938\n",
      "Train Epoch: 52 [188672/225000 (84%)] Loss: 12134.904297\n",
      "Train Epoch: 52 [192768/225000 (86%)] Loss: 13434.839844\n",
      "Train Epoch: 52 [196864/225000 (87%)] Loss: 8093.660156\n",
      "Train Epoch: 52 [200960/225000 (89%)] Loss: 9732.646484\n",
      "Train Epoch: 52 [205056/225000 (91%)] Loss: 13490.933594\n",
      "Train Epoch: 52 [209152/225000 (93%)] Loss: 9477.955078\n",
      "Train Epoch: 52 [213248/225000 (95%)] Loss: 8082.503906\n",
      "Train Epoch: 52 [217344/225000 (97%)] Loss: 8121.849609\n",
      "Train Epoch: 52 [221440/225000 (98%)] Loss: 12348.425781\n",
      "    epoch          : 52\n",
      "    loss           : 9666.47264847305\n",
      "    val_loss       : 9616.967554443952\n",
      "Train Epoch: 53 [256/225000 (0%)] Loss: 13648.156250\n",
      "Train Epoch: 53 [4352/225000 (2%)] Loss: 8124.939453\n",
      "Train Epoch: 53 [8448/225000 (4%)] Loss: 8060.890625\n",
      "Train Epoch: 53 [12544/225000 (6%)] Loss: 7985.716797\n",
      "Train Epoch: 53 [16640/225000 (7%)] Loss: 14727.968750\n",
      "Train Epoch: 53 [20736/225000 (9%)] Loss: 13272.421875\n",
      "Train Epoch: 53 [24832/225000 (11%)] Loss: 8124.501953\n",
      "Train Epoch: 53 [28928/225000 (13%)] Loss: 8121.052734\n",
      "Train Epoch: 53 [33024/225000 (15%)] Loss: 8104.316406\n",
      "Train Epoch: 53 [37120/225000 (16%)] Loss: 7978.218750\n",
      "Train Epoch: 53 [41216/225000 (18%)] Loss: 8101.050781\n",
      "Train Epoch: 53 [45312/225000 (20%)] Loss: 10473.679688\n",
      "Train Epoch: 53 [49408/225000 (22%)] Loss: 7803.783203\n",
      "Train Epoch: 53 [53504/225000 (24%)] Loss: 8145.906250\n",
      "Train Epoch: 53 [57600/225000 (26%)] Loss: 8019.970703\n",
      "Train Epoch: 53 [61696/225000 (27%)] Loss: 14015.835938\n",
      "Train Epoch: 53 [65792/225000 (29%)] Loss: 12060.582031\n",
      "Train Epoch: 53 [69888/225000 (31%)] Loss: 8089.353516\n",
      "Train Epoch: 53 [73984/225000 (33%)] Loss: 8122.683594\n",
      "Train Epoch: 53 [78080/225000 (35%)] Loss: 8202.544922\n",
      "Train Epoch: 53 [82176/225000 (37%)] Loss: 8031.539062\n",
      "Train Epoch: 53 [86272/225000 (38%)] Loss: 10640.087891\n",
      "Train Epoch: 53 [90368/225000 (40%)] Loss: 8043.330078\n",
      "Train Epoch: 53 [94464/225000 (42%)] Loss: 8181.216797\n",
      "Train Epoch: 53 [98560/225000 (44%)] Loss: 8015.410156\n",
      "Train Epoch: 53 [102656/225000 (46%)] Loss: 7922.267578\n",
      "Train Epoch: 53 [106752/225000 (47%)] Loss: 8200.796875\n",
      "Train Epoch: 53 [110848/225000 (49%)] Loss: 16147.789062\n",
      "Train Epoch: 53 [114944/225000 (51%)] Loss: 8252.689453\n",
      "Train Epoch: 53 [119040/225000 (53%)] Loss: 8087.859375\n",
      "Train Epoch: 53 [123136/225000 (55%)] Loss: 8031.292969\n",
      "Train Epoch: 53 [127232/225000 (57%)] Loss: 8163.408203\n",
      "Train Epoch: 53 [131328/225000 (58%)] Loss: 9678.791016\n",
      "Train Epoch: 53 [135424/225000 (60%)] Loss: 8316.031250\n",
      "Train Epoch: 53 [139520/225000 (62%)] Loss: 10444.134766\n",
      "Train Epoch: 53 [143616/225000 (64%)] Loss: 14298.705078\n",
      "Train Epoch: 53 [147712/225000 (66%)] Loss: 8098.468750\n",
      "Train Epoch: 53 [151808/225000 (67%)] Loss: 7965.679688\n",
      "Train Epoch: 53 [155904/225000 (69%)] Loss: 11293.386719\n",
      "Train Epoch: 53 [160000/225000 (71%)] Loss: 9520.400391\n",
      "Train Epoch: 53 [164096/225000 (73%)] Loss: 9669.486328\n",
      "Train Epoch: 53 [168192/225000 (75%)] Loss: 9709.324219\n",
      "Train Epoch: 53 [172288/225000 (77%)] Loss: 9949.482422\n",
      "Train Epoch: 53 [176384/225000 (78%)] Loss: 9635.476562\n",
      "Train Epoch: 53 [180480/225000 (80%)] Loss: 8304.234375\n",
      "Train Epoch: 53 [184576/225000 (82%)] Loss: 8269.089844\n",
      "Train Epoch: 53 [188672/225000 (84%)] Loss: 9720.830078\n",
      "Train Epoch: 53 [192768/225000 (86%)] Loss: 12267.759766\n",
      "Train Epoch: 53 [196864/225000 (87%)] Loss: 9573.070312\n",
      "Train Epoch: 53 [200960/225000 (89%)] Loss: 14896.498047\n",
      "Train Epoch: 53 [205056/225000 (91%)] Loss: 8063.425781\n",
      "Train Epoch: 53 [209152/225000 (93%)] Loss: 9639.607422\n",
      "Train Epoch: 53 [213248/225000 (95%)] Loss: 8125.673828\n",
      "Train Epoch: 53 [217344/225000 (97%)] Loss: 9616.189453\n",
      "Train Epoch: 53 [221440/225000 (98%)] Loss: 8250.744141\n",
      "    epoch          : 53\n",
      "    loss           : 9557.080782494311\n",
      "    val_loss       : 10029.5620987318\n",
      "Train Epoch: 54 [256/225000 (0%)] Loss: 9554.814453\n",
      "Train Epoch: 54 [4352/225000 (2%)] Loss: 9561.927734\n",
      "Train Epoch: 54 [8448/225000 (4%)] Loss: 10508.132812\n",
      "Train Epoch: 54 [12544/225000 (6%)] Loss: 10934.376953\n",
      "Train Epoch: 54 [16640/225000 (7%)] Loss: 9607.857422\n",
      "Train Epoch: 54 [20736/225000 (9%)] Loss: 8219.992188\n",
      "Train Epoch: 54 [24832/225000 (11%)] Loss: 9724.378906\n",
      "Train Epoch: 54 [28928/225000 (13%)] Loss: 7812.900391\n",
      "Train Epoch: 54 [33024/225000 (15%)] Loss: 13764.738281\n",
      "Train Epoch: 54 [37120/225000 (16%)] Loss: 8054.080078\n",
      "Train Epoch: 54 [41216/225000 (18%)] Loss: 8084.392578\n",
      "Train Epoch: 54 [45312/225000 (20%)] Loss: 13556.519531\n",
      "Train Epoch: 54 [49408/225000 (22%)] Loss: 9674.642578\n",
      "Train Epoch: 54 [53504/225000 (24%)] Loss: 7914.966797\n",
      "Train Epoch: 54 [57600/225000 (26%)] Loss: 9611.720703\n",
      "Train Epoch: 54 [61696/225000 (27%)] Loss: 8176.746094\n",
      "Train Epoch: 54 [65792/225000 (29%)] Loss: 10674.796875\n",
      "Train Epoch: 54 [69888/225000 (31%)] Loss: 8048.935547\n",
      "Train Epoch: 54 [73984/225000 (33%)] Loss: 8073.343750\n",
      "Train Epoch: 54 [78080/225000 (35%)] Loss: 10564.501953\n",
      "Train Epoch: 54 [82176/225000 (37%)] Loss: 7980.193359\n",
      "Train Epoch: 54 [86272/225000 (38%)] Loss: 8024.427734\n",
      "Train Epoch: 54 [90368/225000 (40%)] Loss: 7970.109375\n",
      "Train Epoch: 54 [94464/225000 (42%)] Loss: 10495.876953\n",
      "Train Epoch: 54 [98560/225000 (44%)] Loss: 8108.386719\n",
      "Train Epoch: 54 [102656/225000 (46%)] Loss: 8089.199219\n",
      "Train Epoch: 54 [106752/225000 (47%)] Loss: 8033.066406\n",
      "Train Epoch: 54 [110848/225000 (49%)] Loss: 7996.126953\n",
      "Train Epoch: 54 [114944/225000 (51%)] Loss: 9707.402344\n",
      "Train Epoch: 54 [119040/225000 (53%)] Loss: 7890.390625\n",
      "Train Epoch: 54 [123136/225000 (55%)] Loss: 7914.138672\n",
      "Train Epoch: 54 [127232/225000 (57%)] Loss: 8027.623047\n",
      "Train Epoch: 54 [131328/225000 (58%)] Loss: 7804.560547\n",
      "Train Epoch: 54 [135424/225000 (60%)] Loss: 8120.107422\n",
      "Train Epoch: 54 [139520/225000 (62%)] Loss: 10494.958984\n",
      "Train Epoch: 54 [143616/225000 (64%)] Loss: 8174.472656\n",
      "Train Epoch: 54 [147712/225000 (66%)] Loss: 7918.667969\n",
      "Train Epoch: 54 [151808/225000 (67%)] Loss: 8134.291016\n",
      "Train Epoch: 54 [155904/225000 (69%)] Loss: 8302.949219\n",
      "Train Epoch: 54 [160000/225000 (71%)] Loss: 8291.962891\n",
      "Train Epoch: 54 [164096/225000 (73%)] Loss: 16363.570312\n",
      "Train Epoch: 54 [168192/225000 (75%)] Loss: 12317.539062\n",
      "Train Epoch: 54 [172288/225000 (77%)] Loss: 8118.244141\n",
      "Train Epoch: 54 [176384/225000 (78%)] Loss: 7969.111328\n",
      "Train Epoch: 54 [180480/225000 (80%)] Loss: 8109.412109\n",
      "Train Epoch: 54 [184576/225000 (82%)] Loss: 8097.587891\n",
      "Train Epoch: 54 [188672/225000 (84%)] Loss: 8131.146484\n",
      "Train Epoch: 54 [192768/225000 (86%)] Loss: 8059.605469\n",
      "Train Epoch: 54 [196864/225000 (87%)] Loss: 10452.070312\n",
      "Train Epoch: 54 [200960/225000 (89%)] Loss: 9787.238281\n",
      "Train Epoch: 54 [205056/225000 (91%)] Loss: 10654.255859\n",
      "Train Epoch: 54 [209152/225000 (93%)] Loss: 8135.601562\n",
      "Train Epoch: 54 [213248/225000 (95%)] Loss: 7930.017578\n",
      "Train Epoch: 54 [217344/225000 (97%)] Loss: 7993.394531\n",
      "Train Epoch: 54 [221440/225000 (98%)] Loss: 8014.455078\n",
      "    epoch          : 54\n",
      "    loss           : 9526.582364547781\n",
      "    val_loss       : 9729.320331591733\n",
      "Train Epoch: 55 [256/225000 (0%)] Loss: 8046.347656\n",
      "Train Epoch: 55 [4352/225000 (2%)] Loss: 14016.205078\n",
      "Train Epoch: 55 [8448/225000 (4%)] Loss: 8107.771484\n",
      "Train Epoch: 55 [12544/225000 (6%)] Loss: 8043.105469\n",
      "Train Epoch: 55 [16640/225000 (7%)] Loss: 9785.380859\n",
      "Train Epoch: 55 [20736/225000 (9%)] Loss: 13730.583984\n",
      "Train Epoch: 55 [24832/225000 (11%)] Loss: 12304.980469\n",
      "Train Epoch: 55 [28928/225000 (13%)] Loss: 9781.468750\n",
      "Train Epoch: 55 [33024/225000 (15%)] Loss: 8159.580078\n",
      "Train Epoch: 55 [37120/225000 (16%)] Loss: 8022.560547\n",
      "Train Epoch: 55 [41216/225000 (18%)] Loss: 8132.277344\n",
      "Train Epoch: 55 [45312/225000 (20%)] Loss: 8167.132812\n",
      "Train Epoch: 55 [49408/225000 (22%)] Loss: 8017.695312\n",
      "Train Epoch: 55 [53504/225000 (24%)] Loss: 7921.281250\n",
      "Train Epoch: 55 [57600/225000 (26%)] Loss: 10751.367188\n",
      "Train Epoch: 55 [61696/225000 (27%)] Loss: 8005.398438\n",
      "Train Epoch: 55 [65792/225000 (29%)] Loss: 8117.462891\n",
      "Train Epoch: 55 [69888/225000 (31%)] Loss: 8092.638672\n",
      "Train Epoch: 55 [73984/225000 (33%)] Loss: 7973.173828\n",
      "Train Epoch: 55 [78080/225000 (35%)] Loss: 8244.052734\n",
      "Train Epoch: 55 [82176/225000 (37%)] Loss: 8023.843750\n",
      "Train Epoch: 55 [86272/225000 (38%)] Loss: 17911.005859\n",
      "Train Epoch: 55 [90368/225000 (40%)] Loss: 8015.228516\n",
      "Train Epoch: 55 [94464/225000 (42%)] Loss: 8312.802734\n",
      "Train Epoch: 55 [98560/225000 (44%)] Loss: 7980.375000\n",
      "Train Epoch: 55 [102656/225000 (46%)] Loss: 9768.773438\n",
      "Train Epoch: 55 [106752/225000 (47%)] Loss: 9686.767578\n",
      "Train Epoch: 55 [110848/225000 (49%)] Loss: 8133.380859\n",
      "Train Epoch: 55 [114944/225000 (51%)] Loss: 9538.525391\n",
      "Train Epoch: 55 [119040/225000 (53%)] Loss: 8074.490234\n",
      "Train Epoch: 55 [123136/225000 (55%)] Loss: 10688.615234\n",
      "Train Epoch: 55 [127232/225000 (57%)] Loss: 8140.185547\n",
      "Train Epoch: 55 [131328/225000 (58%)] Loss: 8107.224609\n",
      "Train Epoch: 55 [135424/225000 (60%)] Loss: 8195.468750\n",
      "Train Epoch: 55 [139520/225000 (62%)] Loss: 7961.978516\n",
      "Train Epoch: 55 [143616/225000 (64%)] Loss: 7888.867188\n",
      "Train Epoch: 55 [147712/225000 (66%)] Loss: 8098.744141\n",
      "Train Epoch: 55 [151808/225000 (67%)] Loss: 8122.271484\n",
      "Train Epoch: 55 [155904/225000 (69%)] Loss: 12083.511719\n",
      "Train Epoch: 55 [160000/225000 (71%)] Loss: 8144.931641\n",
      "Train Epoch: 55 [164096/225000 (73%)] Loss: 10471.203125\n",
      "Train Epoch: 55 [168192/225000 (75%)] Loss: 12335.267578\n",
      "Train Epoch: 55 [172288/225000 (77%)] Loss: 7901.863281\n",
      "Train Epoch: 55 [176384/225000 (78%)] Loss: 8108.468750\n",
      "Train Epoch: 55 [180480/225000 (80%)] Loss: 8062.955078\n",
      "Train Epoch: 55 [184576/225000 (82%)] Loss: 8027.429688\n",
      "Train Epoch: 55 [188672/225000 (84%)] Loss: 13467.771484\n",
      "Train Epoch: 55 [192768/225000 (86%)] Loss: 16838.492188\n",
      "Train Epoch: 55 [196864/225000 (87%)] Loss: 8000.101562\n",
      "Train Epoch: 55 [200960/225000 (89%)] Loss: 8214.789062\n",
      "Train Epoch: 55 [205056/225000 (91%)] Loss: 10708.078125\n",
      "Train Epoch: 55 [209152/225000 (93%)] Loss: 8052.273438\n",
      "Train Epoch: 55 [213248/225000 (95%)] Loss: 14091.623047\n",
      "Train Epoch: 55 [217344/225000 (97%)] Loss: 9630.140625\n",
      "Train Epoch: 55 [221440/225000 (98%)] Loss: 13595.011719\n",
      "    epoch          : 55\n",
      "    loss           : 9506.85699969781\n",
      "    val_loss       : 9520.073148683627\n",
      "Train Epoch: 56 [256/225000 (0%)] Loss: 9683.884766\n",
      "Train Epoch: 56 [4352/225000 (2%)] Loss: 10423.775391\n",
      "Train Epoch: 56 [8448/225000 (4%)] Loss: 9679.355469\n",
      "Train Epoch: 56 [12544/225000 (6%)] Loss: 12176.300781\n",
      "Train Epoch: 56 [16640/225000 (7%)] Loss: 8111.634766\n",
      "Train Epoch: 56 [20736/225000 (9%)] Loss: 8136.476562\n",
      "Train Epoch: 56 [24832/225000 (11%)] Loss: 8010.087891\n",
      "Train Epoch: 56 [28928/225000 (13%)] Loss: 9815.921875\n",
      "Train Epoch: 56 [33024/225000 (15%)] Loss: 8085.169922\n",
      "Train Epoch: 56 [37120/225000 (16%)] Loss: 16084.779297\n",
      "Train Epoch: 56 [41216/225000 (18%)] Loss: 14202.109375\n",
      "Train Epoch: 56 [45312/225000 (20%)] Loss: 7975.531250\n",
      "Train Epoch: 56 [49408/225000 (22%)] Loss: 13744.429688\n",
      "Train Epoch: 56 [53504/225000 (24%)] Loss: 7968.417969\n",
      "Train Epoch: 56 [57600/225000 (26%)] Loss: 7970.957031\n",
      "Train Epoch: 56 [61696/225000 (27%)] Loss: 7889.496094\n",
      "Train Epoch: 56 [65792/225000 (29%)] Loss: 13662.449219\n",
      "Train Epoch: 56 [69888/225000 (31%)] Loss: 9439.230469\n",
      "Train Epoch: 56 [73984/225000 (33%)] Loss: 8018.505859\n",
      "Train Epoch: 56 [78080/225000 (35%)] Loss: 7995.115234\n",
      "Train Epoch: 56 [82176/225000 (37%)] Loss: 8001.095703\n",
      "Train Epoch: 56 [86272/225000 (38%)] Loss: 13685.972656\n",
      "Train Epoch: 56 [90368/225000 (40%)] Loss: 8350.625000\n",
      "Train Epoch: 56 [94464/225000 (42%)] Loss: 10423.619141\n",
      "Train Epoch: 56 [98560/225000 (44%)] Loss: 13918.875000\n",
      "Train Epoch: 56 [102656/225000 (46%)] Loss: 10571.455078\n",
      "Train Epoch: 56 [106752/225000 (47%)] Loss: 8159.914062\n",
      "Train Epoch: 56 [110848/225000 (49%)] Loss: 8035.251953\n",
      "Train Epoch: 56 [114944/225000 (51%)] Loss: 8023.667969\n",
      "Train Epoch: 56 [119040/225000 (53%)] Loss: 10651.197266\n",
      "Train Epoch: 56 [123136/225000 (55%)] Loss: 8031.367188\n",
      "Train Epoch: 56 [127232/225000 (57%)] Loss: 8250.601562\n",
      "Train Epoch: 56 [131328/225000 (58%)] Loss: 9728.853516\n",
      "Train Epoch: 56 [135424/225000 (60%)] Loss: 9680.992188\n",
      "Train Epoch: 56 [139520/225000 (62%)] Loss: 9537.470703\n",
      "Train Epoch: 56 [143616/225000 (64%)] Loss: 11115.804688\n",
      "Train Epoch: 56 [147712/225000 (66%)] Loss: 9610.376953\n",
      "Train Epoch: 56 [151808/225000 (67%)] Loss: 8076.435547\n",
      "Train Epoch: 56 [155904/225000 (69%)] Loss: 13989.406250\n",
      "Train Epoch: 56 [160000/225000 (71%)] Loss: 8116.777344\n",
      "Train Epoch: 56 [164096/225000 (73%)] Loss: 8125.447266\n",
      "Train Epoch: 56 [168192/225000 (75%)] Loss: 7985.533203\n",
      "Train Epoch: 56 [172288/225000 (77%)] Loss: 10552.726562\n",
      "Train Epoch: 56 [176384/225000 (78%)] Loss: 8105.031250\n",
      "Train Epoch: 56 [180480/225000 (80%)] Loss: 8056.804688\n",
      "Train Epoch: 56 [184576/225000 (82%)] Loss: 12900.187500\n",
      "Train Epoch: 56 [188672/225000 (84%)] Loss: 10495.318359\n",
      "Train Epoch: 56 [192768/225000 (86%)] Loss: 10454.806641\n",
      "Train Epoch: 56 [196864/225000 (87%)] Loss: 10519.333984\n",
      "Train Epoch: 56 [200960/225000 (89%)] Loss: 7987.460938\n",
      "Train Epoch: 56 [205056/225000 (91%)] Loss: 9514.148438\n",
      "Train Epoch: 56 [209152/225000 (93%)] Loss: 7856.365234\n",
      "Train Epoch: 56 [213248/225000 (95%)] Loss: 8170.599609\n",
      "Train Epoch: 56 [217344/225000 (97%)] Loss: 8089.771484\n",
      "Train Epoch: 56 [221440/225000 (98%)] Loss: 9460.027344\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "    epoch          : 56\n",
      "    loss           : 9547.180920701792\n",
      "    val_loss       : 9314.786711464123\n",
      "Train Epoch: 57 [256/225000 (0%)] Loss: 8157.517578\n",
      "Train Epoch: 57 [4352/225000 (2%)] Loss: 8049.210938\n",
      "Train Epoch: 57 [8448/225000 (4%)] Loss: 7922.513672\n",
      "Train Epoch: 57 [12544/225000 (6%)] Loss: 8100.839844\n",
      "Train Epoch: 57 [16640/225000 (7%)] Loss: 8124.814453\n",
      "Train Epoch: 57 [20736/225000 (9%)] Loss: 8023.808594\n",
      "Train Epoch: 57 [24832/225000 (11%)] Loss: 8084.064453\n",
      "Train Epoch: 57 [28928/225000 (13%)] Loss: 8016.714844\n",
      "Train Epoch: 57 [33024/225000 (15%)] Loss: 7947.439453\n",
      "Train Epoch: 57 [37120/225000 (16%)] Loss: 8121.208984\n",
      "Train Epoch: 57 [41216/225000 (18%)] Loss: 13894.683594\n",
      "Train Epoch: 57 [45312/225000 (20%)] Loss: 8168.029297\n",
      "Train Epoch: 57 [49408/225000 (22%)] Loss: 10725.992188\n",
      "Train Epoch: 57 [53504/225000 (24%)] Loss: 9636.507812\n",
      "Train Epoch: 57 [57600/225000 (26%)] Loss: 10289.269531\n",
      "Train Epoch: 57 [61696/225000 (27%)] Loss: 8238.048828\n",
      "Train Epoch: 57 [65792/225000 (29%)] Loss: 9816.548828\n",
      "Train Epoch: 57 [69888/225000 (31%)] Loss: 8074.234375\n",
      "Train Epoch: 57 [73984/225000 (33%)] Loss: 7960.255859\n",
      "Train Epoch: 57 [78080/225000 (35%)] Loss: 7890.523438\n",
      "Train Epoch: 57 [82176/225000 (37%)] Loss: 8042.378906\n",
      "Train Epoch: 57 [86272/225000 (38%)] Loss: 7987.943359\n",
      "Train Epoch: 57 [90368/225000 (40%)] Loss: 13011.486328\n",
      "Train Epoch: 57 [94464/225000 (42%)] Loss: 8258.867188\n",
      "Train Epoch: 57 [98560/225000 (44%)] Loss: 8039.443359\n",
      "Train Epoch: 57 [102656/225000 (46%)] Loss: 10759.109375\n",
      "Train Epoch: 57 [106752/225000 (47%)] Loss: 13525.167969\n",
      "Train Epoch: 57 [110848/225000 (49%)] Loss: 7896.962891\n",
      "Train Epoch: 57 [114944/225000 (51%)] Loss: 7945.914062\n",
      "Train Epoch: 57 [119040/225000 (53%)] Loss: 7944.750000\n",
      "Train Epoch: 57 [123136/225000 (55%)] Loss: 9505.386719\n",
      "Train Epoch: 57 [127232/225000 (57%)] Loss: 8105.970703\n",
      "Train Epoch: 57 [131328/225000 (58%)] Loss: 13492.390625\n",
      "Train Epoch: 57 [135424/225000 (60%)] Loss: 13533.716797\n",
      "Train Epoch: 57 [139520/225000 (62%)] Loss: 8153.927734\n",
      "Train Epoch: 57 [143616/225000 (64%)] Loss: 9462.828125\n",
      "Train Epoch: 57 [147712/225000 (66%)] Loss: 9560.822266\n",
      "Train Epoch: 57 [151808/225000 (67%)] Loss: 13649.556641\n",
      "Train Epoch: 57 [155904/225000 (69%)] Loss: 8319.240234\n",
      "Train Epoch: 57 [160000/225000 (71%)] Loss: 7915.837891\n",
      "Train Epoch: 57 [164096/225000 (73%)] Loss: 9632.623047\n",
      "Train Epoch: 57 [168192/225000 (75%)] Loss: 8138.802734\n",
      "Train Epoch: 57 [172288/225000 (77%)] Loss: 13648.671875\n",
      "Train Epoch: 57 [176384/225000 (78%)] Loss: 19830.265625\n",
      "Train Epoch: 57 [180480/225000 (80%)] Loss: 8194.234375\n",
      "Train Epoch: 57 [184576/225000 (82%)] Loss: 8163.671875\n",
      "Train Epoch: 57 [188672/225000 (84%)] Loss: 8012.771484\n",
      "Train Epoch: 57 [192768/225000 (86%)] Loss: 12247.482422\n",
      "Train Epoch: 57 [196864/225000 (87%)] Loss: 8105.457031\n",
      "Train Epoch: 57 [200960/225000 (89%)] Loss: 7964.056641\n",
      "Train Epoch: 57 [205056/225000 (91%)] Loss: 16230.376953\n",
      "Train Epoch: 57 [209152/225000 (93%)] Loss: 13599.597656\n",
      "Train Epoch: 57 [213248/225000 (95%)] Loss: 8257.388672\n",
      "Train Epoch: 57 [217344/225000 (97%)] Loss: 7966.234375\n",
      "Train Epoch: 57 [221440/225000 (98%)] Loss: 15385.896484\n",
      "    epoch          : 57\n",
      "    loss           : 9543.722574036547\n",
      "    val_loss       : 9342.508018020464\n",
      "Train Epoch: 58 [256/225000 (0%)] Loss: 7903.001953\n",
      "Train Epoch: 58 [4352/225000 (2%)] Loss: 13685.888672\n",
      "Train Epoch: 58 [8448/225000 (4%)] Loss: 8020.029297\n",
      "Train Epoch: 58 [12544/225000 (6%)] Loss: 8174.359375\n",
      "Train Epoch: 58 [16640/225000 (7%)] Loss: 9792.173828\n",
      "Train Epoch: 58 [20736/225000 (9%)] Loss: 13912.412109\n",
      "Train Epoch: 58 [24832/225000 (11%)] Loss: 12288.716797\n",
      "Train Epoch: 58 [28928/225000 (13%)] Loss: 8048.314453\n",
      "Train Epoch: 58 [33024/225000 (15%)] Loss: 7998.148438\n",
      "Train Epoch: 58 [37120/225000 (16%)] Loss: 12333.619141\n",
      "Train Epoch: 58 [41216/225000 (18%)] Loss: 7927.773438\n",
      "Train Epoch: 58 [45312/225000 (20%)] Loss: 8026.439453\n",
      "Train Epoch: 58 [49408/225000 (22%)] Loss: 10652.693359\n",
      "Train Epoch: 58 [53504/225000 (24%)] Loss: 16452.556641\n",
      "Train Epoch: 58 [57600/225000 (26%)] Loss: 8357.841797\n",
      "Train Epoch: 58 [61696/225000 (27%)] Loss: 9673.669922\n",
      "Train Epoch: 58 [65792/225000 (29%)] Loss: 13686.808594\n",
      "Train Epoch: 58 [69888/225000 (31%)] Loss: 7957.505859\n",
      "Train Epoch: 58 [73984/225000 (33%)] Loss: 7947.007812\n",
      "Train Epoch: 58 [78080/225000 (35%)] Loss: 9605.943359\n",
      "Train Epoch: 58 [82176/225000 (37%)] Loss: 8144.435547\n",
      "Train Epoch: 58 [86272/225000 (38%)] Loss: 9865.083984\n",
      "Train Epoch: 58 [90368/225000 (40%)] Loss: 8133.810547\n",
      "Train Epoch: 58 [94464/225000 (42%)] Loss: 9546.671875\n",
      "Train Epoch: 58 [98560/225000 (44%)] Loss: 7984.781250\n",
      "Train Epoch: 58 [102656/225000 (46%)] Loss: 7963.501953\n",
      "Train Epoch: 58 [106752/225000 (47%)] Loss: 13279.330078\n",
      "Train Epoch: 58 [110848/225000 (49%)] Loss: 10419.537109\n",
      "Train Epoch: 58 [114944/225000 (51%)] Loss: 10415.732422\n",
      "Train Epoch: 58 [119040/225000 (53%)] Loss: 8111.806641\n",
      "Train Epoch: 58 [123136/225000 (55%)] Loss: 8016.847656\n",
      "Train Epoch: 58 [127232/225000 (57%)] Loss: 9561.031250\n",
      "Train Epoch: 58 [131328/225000 (58%)] Loss: 8032.201172\n",
      "Train Epoch: 58 [135424/225000 (60%)] Loss: 7865.148438\n",
      "Train Epoch: 58 [139520/225000 (62%)] Loss: 9660.054688\n",
      "Train Epoch: 58 [143616/225000 (64%)] Loss: 11486.998047\n",
      "Train Epoch: 58 [147712/225000 (66%)] Loss: 8076.035156\n",
      "Train Epoch: 58 [151808/225000 (67%)] Loss: 10432.019531\n",
      "Train Epoch: 58 [155904/225000 (69%)] Loss: 9544.599609\n",
      "Train Epoch: 58 [160000/225000 (71%)] Loss: 12939.652344\n",
      "Train Epoch: 58 [164096/225000 (73%)] Loss: 13890.001953\n",
      "Train Epoch: 58 [168192/225000 (75%)] Loss: 9531.455078\n",
      "Train Epoch: 58 [172288/225000 (77%)] Loss: 9667.904297\n",
      "Train Epoch: 58 [176384/225000 (78%)] Loss: 8320.767578\n",
      "Train Epoch: 58 [180480/225000 (80%)] Loss: 8238.160156\n",
      "Train Epoch: 58 [184576/225000 (82%)] Loss: 7870.828125\n",
      "Train Epoch: 58 [188672/225000 (84%)] Loss: 8126.859375\n",
      "Train Epoch: 58 [192768/225000 (86%)] Loss: 8119.988281\n",
      "Train Epoch: 58 [196864/225000 (87%)] Loss: 13865.785156\n",
      "Train Epoch: 58 [200960/225000 (89%)] Loss: 7930.236328\n",
      "Train Epoch: 58 [205056/225000 (91%)] Loss: 8029.582031\n",
      "Train Epoch: 58 [209152/225000 (93%)] Loss: 10447.087891\n",
      "Train Epoch: 58 [213248/225000 (95%)] Loss: 12490.517578\n",
      "Train Epoch: 58 [217344/225000 (97%)] Loss: 8150.521484\n",
      "Train Epoch: 58 [221440/225000 (98%)] Loss: 8018.707031\n",
      "    epoch          : 58\n",
      "    loss           : 9641.558140465017\n",
      "    val_loss       : 9122.7755372135\n",
      "Train Epoch: 59 [256/225000 (0%)] Loss: 8059.054688\n",
      "Train Epoch: 59 [4352/225000 (2%)] Loss: 9618.136719\n",
      "Train Epoch: 59 [8448/225000 (4%)] Loss: 8035.458984\n",
      "Train Epoch: 59 [12544/225000 (6%)] Loss: 13628.681641\n",
      "Train Epoch: 59 [16640/225000 (7%)] Loss: 8036.103516\n",
      "Train Epoch: 59 [20736/225000 (9%)] Loss: 8042.326172\n",
      "Train Epoch: 59 [24832/225000 (11%)] Loss: 9795.857422\n",
      "Train Epoch: 59 [28928/225000 (13%)] Loss: 7995.117188\n",
      "Train Epoch: 59 [33024/225000 (15%)] Loss: 9779.664062\n",
      "Train Epoch: 59 [37120/225000 (16%)] Loss: 10318.042969\n",
      "Train Epoch: 59 [41216/225000 (18%)] Loss: 8168.488281\n",
      "Train Epoch: 59 [45312/225000 (20%)] Loss: 7906.035156\n",
      "Train Epoch: 59 [49408/225000 (22%)] Loss: 8234.988281\n",
      "Train Epoch: 59 [53504/225000 (24%)] Loss: 7988.607422\n",
      "Train Epoch: 59 [57600/225000 (26%)] Loss: 8086.083984\n",
      "Train Epoch: 59 [61696/225000 (27%)] Loss: 7924.296875\n",
      "Train Epoch: 59 [65792/225000 (29%)] Loss: 8168.632812\n",
      "Train Epoch: 59 [69888/225000 (31%)] Loss: 12171.632812\n",
      "Train Epoch: 59 [73984/225000 (33%)] Loss: 9571.572266\n",
      "Train Epoch: 59 [78080/225000 (35%)] Loss: 10524.707031\n",
      "Train Epoch: 59 [82176/225000 (37%)] Loss: 9696.488281\n",
      "Train Epoch: 59 [86272/225000 (38%)] Loss: 8017.982422\n",
      "Train Epoch: 59 [90368/225000 (40%)] Loss: 9625.716797\n",
      "Train Epoch: 59 [94464/225000 (42%)] Loss: 13784.142578\n",
      "Train Epoch: 59 [98560/225000 (44%)] Loss: 12581.587891\n",
      "Train Epoch: 59 [102656/225000 (46%)] Loss: 8087.972656\n",
      "Train Epoch: 59 [106752/225000 (47%)] Loss: 8157.123047\n",
      "Train Epoch: 59 [110848/225000 (49%)] Loss: 8173.070312\n",
      "Train Epoch: 59 [114944/225000 (51%)] Loss: 8181.380859\n",
      "Train Epoch: 59 [119040/225000 (53%)] Loss: 8128.074219\n",
      "Train Epoch: 59 [123136/225000 (55%)] Loss: 8262.265625\n",
      "Train Epoch: 59 [127232/225000 (57%)] Loss: 15364.824219\n",
      "Train Epoch: 59 [131328/225000 (58%)] Loss: 7975.486328\n",
      "Train Epoch: 59 [135424/225000 (60%)] Loss: 16422.861328\n",
      "Train Epoch: 59 [139520/225000 (62%)] Loss: 8065.638672\n",
      "Train Epoch: 59 [143616/225000 (64%)] Loss: 9728.689453\n",
      "Train Epoch: 59 [147712/225000 (66%)] Loss: 9771.554688\n",
      "Train Epoch: 59 [151808/225000 (67%)] Loss: 8079.285156\n",
      "Train Epoch: 59 [155904/225000 (69%)] Loss: 9688.134766\n",
      "Train Epoch: 59 [160000/225000 (71%)] Loss: 8079.322266\n",
      "Train Epoch: 59 [164096/225000 (73%)] Loss: 12110.515625\n",
      "Train Epoch: 59 [168192/225000 (75%)] Loss: 7947.466797\n",
      "Train Epoch: 59 [172288/225000 (77%)] Loss: 8092.021484\n",
      "Train Epoch: 59 [176384/225000 (78%)] Loss: 13990.863281\n",
      "Train Epoch: 59 [180480/225000 (80%)] Loss: 10539.861328\n",
      "Train Epoch: 59 [184576/225000 (82%)] Loss: 15985.064453\n",
      "Train Epoch: 59 [188672/225000 (84%)] Loss: 8154.845703\n",
      "Train Epoch: 59 [192768/225000 (86%)] Loss: 7942.658203\n",
      "Train Epoch: 59 [196864/225000 (87%)] Loss: 12173.830078\n",
      "Train Epoch: 59 [200960/225000 (89%)] Loss: 8008.039062\n",
      "Train Epoch: 59 [205056/225000 (91%)] Loss: 8300.072266\n",
      "Train Epoch: 59 [209152/225000 (93%)] Loss: 7943.330078\n",
      "Train Epoch: 59 [213248/225000 (95%)] Loss: 8192.673828\n",
      "Train Epoch: 59 [217344/225000 (97%)] Loss: 8125.152344\n",
      "Train Epoch: 59 [221440/225000 (98%)] Loss: 8052.914062\n",
      "    epoch          : 59\n",
      "    loss           : 9568.171265065059\n",
      "    val_loss       : 9371.073219849139\n",
      "Train Epoch: 60 [256/225000 (0%)] Loss: 13694.236328\n",
      "Train Epoch: 60 [4352/225000 (2%)] Loss: 9437.613281\n",
      "Train Epoch: 60 [8448/225000 (4%)] Loss: 10549.238281\n",
      "Train Epoch: 60 [12544/225000 (6%)] Loss: 9589.062500\n",
      "Train Epoch: 60 [16640/225000 (7%)] Loss: 7991.078125\n",
      "Train Epoch: 60 [20736/225000 (9%)] Loss: 7981.359375\n",
      "Train Epoch: 60 [24832/225000 (11%)] Loss: 12974.716797\n",
      "Train Epoch: 60 [28928/225000 (13%)] Loss: 10469.863281\n",
      "Train Epoch: 60 [33024/225000 (15%)] Loss: 8053.761719\n",
      "Train Epoch: 60 [37120/225000 (16%)] Loss: 7958.050781\n",
      "Train Epoch: 60 [41216/225000 (18%)] Loss: 7996.531250\n",
      "Train Epoch: 60 [45312/225000 (20%)] Loss: 7969.982422\n",
      "Train Epoch: 60 [49408/225000 (22%)] Loss: 10418.357422\n",
      "Train Epoch: 60 [53504/225000 (24%)] Loss: 8142.160156\n",
      "Train Epoch: 60 [57600/225000 (26%)] Loss: 7978.250000\n",
      "Train Epoch: 60 [61696/225000 (27%)] Loss: 8301.113281\n",
      "Train Epoch: 60 [65792/225000 (29%)] Loss: 7973.070312\n",
      "Train Epoch: 60 [69888/225000 (31%)] Loss: 9541.458984\n",
      "Train Epoch: 60 [73984/225000 (33%)] Loss: 8088.800781\n",
      "Train Epoch: 60 [78080/225000 (35%)] Loss: 9757.789062\n",
      "Train Epoch: 60 [82176/225000 (37%)] Loss: 8063.546875\n",
      "Train Epoch: 60 [86272/225000 (38%)] Loss: 8023.759766\n",
      "Train Epoch: 60 [90368/225000 (40%)] Loss: 8077.976562\n",
      "Train Epoch: 60 [94464/225000 (42%)] Loss: 7966.027344\n",
      "Train Epoch: 60 [98560/225000 (44%)] Loss: 9557.003906\n",
      "Train Epoch: 60 [102656/225000 (46%)] Loss: 7955.304688\n",
      "Train Epoch: 60 [106752/225000 (47%)] Loss: 10655.126953\n",
      "Train Epoch: 60 [110848/225000 (49%)] Loss: 7995.414062\n",
      "Train Epoch: 60 [114944/225000 (51%)] Loss: 8347.310547\n",
      "Train Epoch: 60 [119040/225000 (53%)] Loss: 8184.140625\n",
      "Train Epoch: 60 [123136/225000 (55%)] Loss: 9698.248047\n",
      "Train Epoch: 60 [127232/225000 (57%)] Loss: 12235.130859\n",
      "Train Epoch: 60 [131328/225000 (58%)] Loss: 7998.021484\n",
      "Train Epoch: 60 [135424/225000 (60%)] Loss: 9750.042969\n",
      "Train Epoch: 60 [139520/225000 (62%)] Loss: 9636.837891\n",
      "Train Epoch: 60 [143616/225000 (64%)] Loss: 11996.597656\n",
      "Train Epoch: 60 [147712/225000 (66%)] Loss: 8042.570312\n",
      "Train Epoch: 60 [151808/225000 (67%)] Loss: 8000.376953\n",
      "Train Epoch: 60 [155904/225000 (69%)] Loss: 7892.580078\n",
      "Train Epoch: 60 [160000/225000 (71%)] Loss: 7972.796875\n",
      "Train Epoch: 60 [164096/225000 (73%)] Loss: 7899.078125\n",
      "Train Epoch: 60 [168192/225000 (75%)] Loss: 9665.695312\n",
      "Train Epoch: 60 [172288/225000 (77%)] Loss: 10251.113281\n",
      "Train Epoch: 60 [176384/225000 (78%)] Loss: 8125.515625\n",
      "Train Epoch: 60 [180480/225000 (80%)] Loss: 9582.013672\n",
      "Train Epoch: 60 [184576/225000 (82%)] Loss: 9571.083984\n",
      "Train Epoch: 60 [188672/225000 (84%)] Loss: 7970.525391\n",
      "Train Epoch: 60 [192768/225000 (86%)] Loss: 13566.828125\n",
      "Train Epoch: 60 [196864/225000 (87%)] Loss: 13774.357422\n",
      "Train Epoch: 60 [200960/225000 (89%)] Loss: 8017.972656\n",
      "Train Epoch: 60 [205056/225000 (91%)] Loss: 8001.931641\n",
      "Train Epoch: 60 [209152/225000 (93%)] Loss: 7893.978516\n",
      "Train Epoch: 60 [213248/225000 (95%)] Loss: 13658.353516\n",
      "Train Epoch: 60 [217344/225000 (97%)] Loss: 8151.044922\n",
      "Train Epoch: 60 [221440/225000 (98%)] Loss: 13555.365234\n",
      "    epoch          : 60\n",
      "    loss           : 9441.694751448735\n",
      "    val_loss       : 9402.934704700296\n",
      "Train Epoch: 61 [256/225000 (0%)] Loss: 8182.337891\n",
      "Train Epoch: 61 [4352/225000 (2%)] Loss: 12173.824219\n",
      "Train Epoch: 61 [8448/225000 (4%)] Loss: 7901.056641\n",
      "Train Epoch: 61 [12544/225000 (6%)] Loss: 12498.937500\n",
      "Train Epoch: 61 [16640/225000 (7%)] Loss: 9677.267578\n",
      "Train Epoch: 61 [20736/225000 (9%)] Loss: 8119.753906\n",
      "Train Epoch: 61 [24832/225000 (11%)] Loss: 10579.865234\n",
      "Train Epoch: 61 [28928/225000 (13%)] Loss: 7754.740234\n",
      "Train Epoch: 61 [33024/225000 (15%)] Loss: 8049.843750\n",
      "Train Epoch: 61 [37120/225000 (16%)] Loss: 7927.597656\n",
      "Train Epoch: 61 [41216/225000 (18%)] Loss: 8034.169922\n",
      "Train Epoch: 61 [45312/225000 (20%)] Loss: 8024.914062\n",
      "Train Epoch: 61 [49408/225000 (22%)] Loss: 12233.167969\n",
      "Train Epoch: 61 [53504/225000 (24%)] Loss: 15008.912109\n",
      "Train Epoch: 61 [57600/225000 (26%)] Loss: 7813.001953\n",
      "Train Epoch: 61 [61696/225000 (27%)] Loss: 8028.287109\n",
      "Train Epoch: 61 [65792/225000 (29%)] Loss: 7943.228516\n",
      "Train Epoch: 61 [69888/225000 (31%)] Loss: 8207.144531\n",
      "Train Epoch: 61 [73984/225000 (33%)] Loss: 8280.917969\n",
      "Train Epoch: 61 [78080/225000 (35%)] Loss: 7978.828125\n",
      "Train Epoch: 61 [82176/225000 (37%)] Loss: 8035.113281\n",
      "Train Epoch: 61 [86272/225000 (38%)] Loss: 8254.253906\n",
      "Train Epoch: 61 [90368/225000 (40%)] Loss: 13734.613281\n",
      "Train Epoch: 61 [94464/225000 (42%)] Loss: 8053.128906\n",
      "Train Epoch: 61 [98560/225000 (44%)] Loss: 12586.210938\n",
      "Train Epoch: 61 [102656/225000 (46%)] Loss: 8127.109375\n",
      "Train Epoch: 61 [106752/225000 (47%)] Loss: 8029.714844\n",
      "Train Epoch: 61 [110848/225000 (49%)] Loss: 8287.025391\n",
      "Train Epoch: 61 [114944/225000 (51%)] Loss: 12327.447266\n",
      "Train Epoch: 61 [119040/225000 (53%)] Loss: 8160.494141\n",
      "Train Epoch: 61 [123136/225000 (55%)] Loss: 8017.937500\n",
      "Train Epoch: 61 [127232/225000 (57%)] Loss: 13039.693359\n",
      "Train Epoch: 61 [131328/225000 (58%)] Loss: 16187.533203\n",
      "Train Epoch: 61 [135424/225000 (60%)] Loss: 8127.140625\n",
      "Train Epoch: 61 [139520/225000 (62%)] Loss: 9515.316406\n",
      "Train Epoch: 61 [143616/225000 (64%)] Loss: 7886.697266\n",
      "Train Epoch: 61 [147712/225000 (66%)] Loss: 8171.066406\n",
      "Train Epoch: 61 [151808/225000 (67%)] Loss: 8001.009766\n",
      "Train Epoch: 61 [155904/225000 (69%)] Loss: 9588.515625\n",
      "Train Epoch: 61 [160000/225000 (71%)] Loss: 9703.824219\n",
      "Train Epoch: 61 [164096/225000 (73%)] Loss: 7985.912109\n",
      "Train Epoch: 61 [168192/225000 (75%)] Loss: 9634.222656\n",
      "Train Epoch: 61 [172288/225000 (77%)] Loss: 8070.292969\n",
      "Train Epoch: 61 [176384/225000 (78%)] Loss: 9565.224609\n",
      "Train Epoch: 61 [180480/225000 (80%)] Loss: 9509.960938\n",
      "Train Epoch: 61 [184576/225000 (82%)] Loss: 9767.394531\n",
      "Train Epoch: 61 [188672/225000 (84%)] Loss: 8099.578125\n",
      "Train Epoch: 61 [192768/225000 (86%)] Loss: 8124.341797\n",
      "Train Epoch: 61 [196864/225000 (87%)] Loss: 9617.500000\n",
      "Train Epoch: 61 [200960/225000 (89%)] Loss: 9546.818359\n",
      "Train Epoch: 61 [205056/225000 (91%)] Loss: 7945.042969\n",
      "Train Epoch: 61 [209152/225000 (93%)] Loss: 7907.029297\n",
      "Train Epoch: 61 [213248/225000 (95%)] Loss: 7972.890625\n",
      "Train Epoch: 61 [217344/225000 (97%)] Loss: 9397.867188\n",
      "Train Epoch: 61 [221440/225000 (98%)] Loss: 12958.675781\n",
      "    epoch          : 61\n",
      "    loss           : 9478.321051310082\n",
      "    val_loss       : 9395.187122298747\n",
      "Train Epoch: 62 [256/225000 (0%)] Loss: 10648.210938\n",
      "Train Epoch: 62 [4352/225000 (2%)] Loss: 10764.957031\n",
      "Train Epoch: 62 [8448/225000 (4%)] Loss: 8120.740234\n",
      "Train Epoch: 62 [12544/225000 (6%)] Loss: 10586.339844\n",
      "Train Epoch: 62 [16640/225000 (7%)] Loss: 12554.746094\n",
      "Train Epoch: 62 [20736/225000 (9%)] Loss: 8293.609375\n",
      "Train Epoch: 62 [24832/225000 (11%)] Loss: 9532.023438\n",
      "Train Epoch: 62 [28928/225000 (13%)] Loss: 8063.812500\n",
      "Train Epoch: 62 [33024/225000 (15%)] Loss: 8063.677734\n",
      "Train Epoch: 62 [37120/225000 (16%)] Loss: 13652.822266\n",
      "Train Epoch: 62 [41216/225000 (18%)] Loss: 7963.439453\n",
      "Train Epoch: 62 [45312/225000 (20%)] Loss: 13589.591797\n",
      "Train Epoch: 62 [49408/225000 (22%)] Loss: 8132.617188\n",
      "Train Epoch: 62 [53504/225000 (24%)] Loss: 8279.037109\n",
      "Train Epoch: 62 [57600/225000 (26%)] Loss: 8097.595703\n",
      "Train Epoch: 62 [61696/225000 (27%)] Loss: 8174.208984\n",
      "Train Epoch: 62 [65792/225000 (29%)] Loss: 8097.978516\n",
      "Train Epoch: 62 [69888/225000 (31%)] Loss: 7906.322266\n",
      "Train Epoch: 62 [73984/225000 (33%)] Loss: 8132.195312\n",
      "Train Epoch: 62 [78080/225000 (35%)] Loss: 8225.568359\n",
      "Train Epoch: 62 [82176/225000 (37%)] Loss: 8305.130859\n",
      "Train Epoch: 62 [86272/225000 (38%)] Loss: 7733.203125\n",
      "Train Epoch: 62 [90368/225000 (40%)] Loss: 8080.894531\n",
      "Train Epoch: 62 [94464/225000 (42%)] Loss: 8105.001953\n",
      "Train Epoch: 62 [98560/225000 (44%)] Loss: 8160.099609\n",
      "Train Epoch: 62 [102656/225000 (46%)] Loss: 8098.244141\n",
      "Train Epoch: 62 [106752/225000 (47%)] Loss: 8007.205078\n",
      "Train Epoch: 62 [110848/225000 (49%)] Loss: 10372.814453\n",
      "Train Epoch: 62 [114944/225000 (51%)] Loss: 10408.011719\n",
      "Train Epoch: 62 [119040/225000 (53%)] Loss: 9654.486328\n",
      "Train Epoch: 62 [123136/225000 (55%)] Loss: 8102.031250\n",
      "Train Epoch: 62 [127232/225000 (57%)] Loss: 8325.572266\n",
      "Train Epoch: 62 [131328/225000 (58%)] Loss: 10534.271484\n",
      "Train Epoch: 62 [135424/225000 (60%)] Loss: 10524.544922\n",
      "Train Epoch: 62 [139520/225000 (62%)] Loss: 8128.511719\n",
      "Train Epoch: 62 [143616/225000 (64%)] Loss: 7918.472656\n",
      "Train Epoch: 62 [147712/225000 (66%)] Loss: 15032.554688\n",
      "Train Epoch: 62 [151808/225000 (67%)] Loss: 9576.072266\n",
      "Train Epoch: 62 [155904/225000 (69%)] Loss: 8102.830078\n",
      "Train Epoch: 62 [160000/225000 (71%)] Loss: 8103.054688\n",
      "Train Epoch: 62 [164096/225000 (73%)] Loss: 7980.472656\n",
      "Train Epoch: 62 [168192/225000 (75%)] Loss: 9525.369141\n",
      "Train Epoch: 62 [172288/225000 (77%)] Loss: 9551.457031\n",
      "Train Epoch: 62 [176384/225000 (78%)] Loss: 8108.574219\n",
      "Train Epoch: 62 [180480/225000 (80%)] Loss: 10699.972656\n",
      "Train Epoch: 62 [184576/225000 (82%)] Loss: 10474.408203\n",
      "Train Epoch: 62 [188672/225000 (84%)] Loss: 8100.146484\n",
      "Train Epoch: 62 [192768/225000 (86%)] Loss: 8100.757812\n",
      "Train Epoch: 62 [196864/225000 (87%)] Loss: 8067.158203\n",
      "Train Epoch: 62 [200960/225000 (89%)] Loss: 7945.533203\n",
      "Train Epoch: 62 [205056/225000 (91%)] Loss: 8092.697266\n",
      "Train Epoch: 62 [209152/225000 (93%)] Loss: 10545.816406\n",
      "Train Epoch: 62 [213248/225000 (95%)] Loss: 8206.515625\n",
      "Train Epoch: 62 [217344/225000 (97%)] Loss: 9593.402344\n",
      "Train Epoch: 62 [221440/225000 (98%)] Loss: 8082.294922\n",
      "    epoch          : 62\n",
      "    loss           : 9542.10958831058\n",
      "    val_loss       : 10180.141157933644\n",
      "Train Epoch: 63 [256/225000 (0%)] Loss: 8192.060547\n",
      "Train Epoch: 63 [4352/225000 (2%)] Loss: 13842.654297\n",
      "Train Epoch: 63 [8448/225000 (4%)] Loss: 8096.439453\n",
      "Train Epoch: 63 [12544/225000 (6%)] Loss: 8171.294922\n",
      "Train Epoch: 63 [16640/225000 (7%)] Loss: 8249.671875\n",
      "Train Epoch: 63 [20736/225000 (9%)] Loss: 7918.121094\n",
      "Train Epoch: 63 [24832/225000 (11%)] Loss: 7934.613281\n",
      "Train Epoch: 63 [28928/225000 (13%)] Loss: 9625.353516\n",
      "Train Epoch: 63 [33024/225000 (15%)] Loss: 10819.199219\n",
      "Train Epoch: 63 [37120/225000 (16%)] Loss: 8016.335938\n",
      "Train Epoch: 63 [41216/225000 (18%)] Loss: 7933.828125\n",
      "Train Epoch: 63 [45312/225000 (20%)] Loss: 8251.171875\n",
      "Train Epoch: 63 [49408/225000 (22%)] Loss: 8124.031250\n",
      "Train Epoch: 63 [53504/225000 (24%)] Loss: 8167.921875\n",
      "Train Epoch: 63 [57600/225000 (26%)] Loss: 13515.082031\n",
      "Train Epoch: 63 [61696/225000 (27%)] Loss: 7980.531250\n",
      "Train Epoch: 63 [65792/225000 (29%)] Loss: 8082.935547\n",
      "Train Epoch: 63 [69888/225000 (31%)] Loss: 8072.656250\n",
      "Train Epoch: 63 [73984/225000 (33%)] Loss: 10344.568359\n",
      "Train Epoch: 63 [78080/225000 (35%)] Loss: 7917.666016\n",
      "Train Epoch: 63 [82176/225000 (37%)] Loss: 8249.785156\n",
      "Train Epoch: 63 [86272/225000 (38%)] Loss: 8122.660156\n",
      "Train Epoch: 63 [90368/225000 (40%)] Loss: 13978.117188\n",
      "Train Epoch: 63 [94464/225000 (42%)] Loss: 7945.753906\n",
      "Train Epoch: 63 [98560/225000 (44%)] Loss: 8131.302734\n",
      "Train Epoch: 63 [102656/225000 (46%)] Loss: 8004.361328\n",
      "Train Epoch: 63 [106752/225000 (47%)] Loss: 8028.800781\n",
      "Train Epoch: 63 [110848/225000 (49%)] Loss: 8121.306641\n",
      "Train Epoch: 63 [114944/225000 (51%)] Loss: 13827.039062\n",
      "Train Epoch: 63 [119040/225000 (53%)] Loss: 8134.152344\n",
      "Train Epoch: 63 [123136/225000 (55%)] Loss: 8132.892578\n",
      "Train Epoch: 63 [127232/225000 (57%)] Loss: 7933.841797\n",
      "Train Epoch: 63 [131328/225000 (58%)] Loss: 13009.117188\n",
      "Train Epoch: 63 [135424/225000 (60%)] Loss: 8169.087891\n",
      "Train Epoch: 63 [139520/225000 (62%)] Loss: 10497.080078\n",
      "Train Epoch: 63 [143616/225000 (64%)] Loss: 8021.130859\n",
      "Train Epoch: 63 [147712/225000 (66%)] Loss: 9757.472656\n",
      "Train Epoch: 63 [151808/225000 (67%)] Loss: 10500.013672\n",
      "Train Epoch: 63 [155904/225000 (69%)] Loss: 8158.369141\n",
      "Train Epoch: 63 [160000/225000 (71%)] Loss: 7963.468750\n",
      "Train Epoch: 63 [164096/225000 (73%)] Loss: 8144.611328\n",
      "Train Epoch: 63 [168192/225000 (75%)] Loss: 8024.773438\n",
      "Train Epoch: 63 [172288/225000 (77%)] Loss: 7988.001953\n",
      "Train Epoch: 63 [176384/225000 (78%)] Loss: 10395.236328\n",
      "Train Epoch: 63 [180480/225000 (80%)] Loss: 9633.009766\n",
      "Train Epoch: 63 [184576/225000 (82%)] Loss: 12317.724609\n",
      "Train Epoch: 63 [188672/225000 (84%)] Loss: 8061.136719\n",
      "Train Epoch: 63 [192768/225000 (86%)] Loss: 9550.312500\n",
      "Train Epoch: 63 [196864/225000 (87%)] Loss: 8191.984375\n",
      "Train Epoch: 63 [200960/225000 (89%)] Loss: 7981.185547\n",
      "Train Epoch: 63 [205056/225000 (91%)] Loss: 10374.035156\n",
      "Train Epoch: 63 [209152/225000 (93%)] Loss: 8009.701172\n",
      "Train Epoch: 63 [213248/225000 (95%)] Loss: 8355.507812\n",
      "Train Epoch: 63 [217344/225000 (97%)] Loss: 7859.435547\n",
      "Train Epoch: 63 [221440/225000 (98%)] Loss: 8151.802734\n",
      "    epoch          : 63\n",
      "    loss           : 9632.018015856087\n",
      "    val_loss       : 9812.472596971356\n",
      "Train Epoch: 64 [256/225000 (0%)] Loss: 8174.707031\n",
      "Train Epoch: 64 [4352/225000 (2%)] Loss: 8051.714844\n",
      "Train Epoch: 64 [8448/225000 (4%)] Loss: 13721.453125\n",
      "Train Epoch: 64 [12544/225000 (6%)] Loss: 15445.607422\n",
      "Train Epoch: 64 [16640/225000 (7%)] Loss: 7884.529297\n",
      "Train Epoch: 64 [20736/225000 (9%)] Loss: 7924.287109\n",
      "Train Epoch: 64 [24832/225000 (11%)] Loss: 7892.353516\n",
      "Train Epoch: 64 [28928/225000 (13%)] Loss: 9586.408203\n",
      "Train Epoch: 64 [33024/225000 (15%)] Loss: 9515.980469\n",
      "Train Epoch: 64 [37120/225000 (16%)] Loss: 18509.154297\n",
      "Train Epoch: 64 [41216/225000 (18%)] Loss: 8241.890625\n",
      "Train Epoch: 64 [45312/225000 (20%)] Loss: 9607.197266\n",
      "Train Epoch: 64 [49408/225000 (22%)] Loss: 8087.250000\n",
      "Train Epoch: 64 [53504/225000 (24%)] Loss: 8095.951172\n",
      "Train Epoch: 64 [57600/225000 (26%)] Loss: 13796.589844\n",
      "Train Epoch: 64 [61696/225000 (27%)] Loss: 8160.724609\n",
      "Train Epoch: 64 [65792/225000 (29%)] Loss: 7891.433594\n",
      "Train Epoch: 64 [69888/225000 (31%)] Loss: 9685.529297\n",
      "Train Epoch: 64 [73984/225000 (33%)] Loss: 12953.701172\n",
      "Train Epoch: 64 [78080/225000 (35%)] Loss: 8166.621094\n",
      "Train Epoch: 64 [82176/225000 (37%)] Loss: 8157.898438\n",
      "Train Epoch: 64 [86272/225000 (38%)] Loss: 8204.167969\n",
      "Train Epoch: 64 [90368/225000 (40%)] Loss: 8087.533203\n",
      "Train Epoch: 64 [94464/225000 (42%)] Loss: 12382.941406\n",
      "Train Epoch: 64 [98560/225000 (44%)] Loss: 8185.857422\n",
      "Train Epoch: 64 [102656/225000 (46%)] Loss: 8187.921875\n",
      "Train Epoch: 64 [106752/225000 (47%)] Loss: 8005.941406\n",
      "Train Epoch: 64 [110848/225000 (49%)] Loss: 9788.677734\n",
      "Train Epoch: 64 [114944/225000 (51%)] Loss: 12579.412109\n",
      "Train Epoch: 64 [119040/225000 (53%)] Loss: 12016.720703\n",
      "Train Epoch: 64 [123136/225000 (55%)] Loss: 8129.185547\n",
      "Train Epoch: 64 [127232/225000 (57%)] Loss: 8295.445312\n",
      "Train Epoch: 64 [131328/225000 (58%)] Loss: 7997.148438\n",
      "Train Epoch: 64 [135424/225000 (60%)] Loss: 8011.595703\n",
      "Train Epoch: 64 [139520/225000 (62%)] Loss: 8006.035156\n",
      "Train Epoch: 64 [143616/225000 (64%)] Loss: 13768.158203\n",
      "Train Epoch: 64 [147712/225000 (66%)] Loss: 18626.728516\n",
      "Train Epoch: 64 [151808/225000 (67%)] Loss: 8153.076172\n",
      "Train Epoch: 64 [155904/225000 (69%)] Loss: 13748.853516\n",
      "Train Epoch: 64 [160000/225000 (71%)] Loss: 8132.529297\n",
      "Train Epoch: 64 [164096/225000 (73%)] Loss: 8003.496094\n",
      "Train Epoch: 64 [168192/225000 (75%)] Loss: 7954.945312\n",
      "Train Epoch: 64 [172288/225000 (77%)] Loss: 8040.794922\n",
      "Train Epoch: 64 [176384/225000 (78%)] Loss: 8036.886719\n",
      "Train Epoch: 64 [180480/225000 (80%)] Loss: 7901.251953\n",
      "Train Epoch: 64 [184576/225000 (82%)] Loss: 12051.878906\n",
      "Train Epoch: 64 [188672/225000 (84%)] Loss: 14777.693359\n",
      "Train Epoch: 64 [192768/225000 (86%)] Loss: 7979.671875\n",
      "Train Epoch: 64 [196864/225000 (87%)] Loss: 8074.894531\n",
      "Train Epoch: 64 [200960/225000 (89%)] Loss: 7975.667969\n",
      "Train Epoch: 64 [205056/225000 (91%)] Loss: 8003.361328\n",
      "Train Epoch: 64 [209152/225000 (93%)] Loss: 7977.289062\n",
      "Train Epoch: 64 [213248/225000 (95%)] Loss: 8041.958984\n",
      "Train Epoch: 64 [217344/225000 (97%)] Loss: 8153.447266\n",
      "Train Epoch: 64 [221440/225000 (98%)] Loss: 12363.296875\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "    epoch          : 64\n",
      "    loss           : 9495.247589146047\n",
      "    val_loss       : 9371.203420327634\n",
      "Train Epoch: 65 [256/225000 (0%)] Loss: 10599.101562\n",
      "Train Epoch: 65 [4352/225000 (2%)] Loss: 7953.363281\n",
      "Train Epoch: 65 [8448/225000 (4%)] Loss: 8048.929688\n",
      "Train Epoch: 65 [12544/225000 (6%)] Loss: 12509.111328\n",
      "Train Epoch: 65 [16640/225000 (7%)] Loss: 8143.046875\n",
      "Train Epoch: 65 [20736/225000 (9%)] Loss: 8308.220703\n",
      "Train Epoch: 65 [24832/225000 (11%)] Loss: 8062.869141\n",
      "Train Epoch: 65 [28928/225000 (13%)] Loss: 13418.220703\n",
      "Train Epoch: 65 [33024/225000 (15%)] Loss: 13962.113281\n",
      "Train Epoch: 65 [37120/225000 (16%)] Loss: 8170.677734\n",
      "Train Epoch: 65 [41216/225000 (18%)] Loss: 13423.417969\n",
      "Train Epoch: 65 [45312/225000 (20%)] Loss: 8189.341797\n",
      "Train Epoch: 65 [49408/225000 (22%)] Loss: 12228.679688\n",
      "Train Epoch: 65 [53504/225000 (24%)] Loss: 8071.304688\n",
      "Train Epoch: 65 [57600/225000 (26%)] Loss: 8095.498047\n",
      "Train Epoch: 65 [61696/225000 (27%)] Loss: 13542.441406\n",
      "Train Epoch: 65 [65792/225000 (29%)] Loss: 14796.962891\n",
      "Train Epoch: 65 [69888/225000 (31%)] Loss: 8300.835938\n",
      "Train Epoch: 65 [73984/225000 (33%)] Loss: 8106.089844\n",
      "Train Epoch: 65 [78080/225000 (35%)] Loss: 7941.583984\n",
      "Train Epoch: 65 [82176/225000 (37%)] Loss: 11084.193359\n",
      "Train Epoch: 65 [86272/225000 (38%)] Loss: 10442.529297\n",
      "Train Epoch: 65 [90368/225000 (40%)] Loss: 8067.943359\n",
      "Train Epoch: 65 [94464/225000 (42%)] Loss: 8181.976562\n",
      "Train Epoch: 65 [98560/225000 (44%)] Loss: 10497.011719\n",
      "Train Epoch: 65 [102656/225000 (46%)] Loss: 8092.181641\n",
      "Train Epoch: 65 [106752/225000 (47%)] Loss: 13728.308594\n",
      "Train Epoch: 65 [110848/225000 (49%)] Loss: 12777.222656\n",
      "Train Epoch: 65 [114944/225000 (51%)] Loss: 8121.267578\n",
      "Train Epoch: 65 [119040/225000 (53%)] Loss: 8154.291016\n",
      "Train Epoch: 65 [123136/225000 (55%)] Loss: 9778.425781\n",
      "Train Epoch: 65 [127232/225000 (57%)] Loss: 9537.683594\n",
      "Train Epoch: 65 [131328/225000 (58%)] Loss: 8078.302734\n",
      "Train Epoch: 65 [135424/225000 (60%)] Loss: 8025.044922\n",
      "Train Epoch: 65 [139520/225000 (62%)] Loss: 12815.019531\n",
      "Train Epoch: 65 [143616/225000 (64%)] Loss: 7921.148438\n",
      "Train Epoch: 65 [147712/225000 (66%)] Loss: 13668.417969\n",
      "Train Epoch: 65 [151808/225000 (67%)] Loss: 7999.628906\n",
      "Train Epoch: 65 [155904/225000 (69%)] Loss: 10427.390625\n",
      "Train Epoch: 65 [160000/225000 (71%)] Loss: 8019.845703\n",
      "Train Epoch: 65 [164096/225000 (73%)] Loss: 9733.730469\n",
      "Train Epoch: 65 [168192/225000 (75%)] Loss: 8083.904297\n",
      "Train Epoch: 65 [172288/225000 (77%)] Loss: 11976.300781\n",
      "Train Epoch: 65 [176384/225000 (78%)] Loss: 8042.066406\n",
      "Train Epoch: 65 [180480/225000 (80%)] Loss: 9590.656250\n",
      "Train Epoch: 65 [184576/225000 (82%)] Loss: 15663.917969\n",
      "Train Epoch: 65 [188672/225000 (84%)] Loss: 8131.291016\n",
      "Train Epoch: 65 [192768/225000 (86%)] Loss: 7964.638672\n",
      "Train Epoch: 65 [196864/225000 (87%)] Loss: 8021.966797\n",
      "Train Epoch: 65 [200960/225000 (89%)] Loss: 10604.179688\n",
      "Train Epoch: 65 [205056/225000 (91%)] Loss: 9731.152344\n",
      "Train Epoch: 65 [209152/225000 (93%)] Loss: 7995.103516\n",
      "Train Epoch: 65 [213248/225000 (95%)] Loss: 16211.080078\n",
      "Train Epoch: 65 [217344/225000 (97%)] Loss: 9758.166016\n",
      "Train Epoch: 65 [221440/225000 (98%)] Loss: 7956.490234\n",
      "    epoch          : 65\n",
      "    loss           : 9637.176093438922\n",
      "    val_loss       : 9160.346010091353\n",
      "Train Epoch: 66 [256/225000 (0%)] Loss: 8203.800781\n",
      "Train Epoch: 66 [4352/225000 (2%)] Loss: 9767.300781\n",
      "Train Epoch: 66 [8448/225000 (4%)] Loss: 10371.458984\n",
      "Train Epoch: 66 [12544/225000 (6%)] Loss: 8067.349609\n",
      "Train Epoch: 66 [16640/225000 (7%)] Loss: 8324.710938\n",
      "Train Epoch: 66 [20736/225000 (9%)] Loss: 14627.386719\n",
      "Train Epoch: 66 [24832/225000 (11%)] Loss: 9519.390625\n",
      "Train Epoch: 66 [28928/225000 (13%)] Loss: 8301.277344\n",
      "Train Epoch: 66 [33024/225000 (15%)] Loss: 8121.496094\n",
      "Train Epoch: 66 [37120/225000 (16%)] Loss: 7993.390625\n",
      "Train Epoch: 66 [41216/225000 (18%)] Loss: 8205.228516\n",
      "Train Epoch: 66 [45312/225000 (20%)] Loss: 8187.892578\n",
      "Train Epoch: 66 [49408/225000 (22%)] Loss: 9415.156250\n",
      "Train Epoch: 66 [53504/225000 (24%)] Loss: 8121.125000\n",
      "Train Epoch: 66 [57600/225000 (26%)] Loss: 8093.119141\n",
      "Train Epoch: 66 [61696/225000 (27%)] Loss: 7918.462891\n",
      "Train Epoch: 66 [65792/225000 (29%)] Loss: 9634.355469\n",
      "Train Epoch: 66 [69888/225000 (31%)] Loss: 12002.781250\n",
      "Train Epoch: 66 [73984/225000 (33%)] Loss: 13517.916016\n",
      "Train Epoch: 66 [78080/225000 (35%)] Loss: 8037.369141\n",
      "Train Epoch: 66 [82176/225000 (37%)] Loss: 10786.314453\n",
      "Train Epoch: 66 [86272/225000 (38%)] Loss: 7965.869141\n",
      "Train Epoch: 66 [90368/225000 (40%)] Loss: 9590.298828\n",
      "Train Epoch: 66 [94464/225000 (42%)] Loss: 8150.703125\n",
      "Train Epoch: 66 [98560/225000 (44%)] Loss: 8209.832031\n",
      "Train Epoch: 66 [102656/225000 (46%)] Loss: 9581.845703\n",
      "Train Epoch: 66 [106752/225000 (47%)] Loss: 13723.146484\n",
      "Train Epoch: 66 [110848/225000 (49%)] Loss: 8114.486328\n",
      "Train Epoch: 66 [114944/225000 (51%)] Loss: 18062.695312\n",
      "Train Epoch: 66 [119040/225000 (53%)] Loss: 15245.908203\n",
      "Train Epoch: 66 [123136/225000 (55%)] Loss: 8168.523438\n",
      "Train Epoch: 66 [127232/225000 (57%)] Loss: 8067.742188\n",
      "Train Epoch: 66 [131328/225000 (58%)] Loss: 9670.212891\n",
      "Train Epoch: 66 [135424/225000 (60%)] Loss: 8045.763672\n",
      "Train Epoch: 66 [139520/225000 (62%)] Loss: 12043.183594\n",
      "Train Epoch: 66 [143616/225000 (64%)] Loss: 9737.101562\n",
      "Train Epoch: 66 [147712/225000 (66%)] Loss: 10490.962891\n",
      "Train Epoch: 66 [151808/225000 (67%)] Loss: 8040.642578\n",
      "Train Epoch: 66 [155904/225000 (69%)] Loss: 7936.107422\n",
      "Train Epoch: 66 [160000/225000 (71%)] Loss: 12813.814453\n",
      "Train Epoch: 66 [164096/225000 (73%)] Loss: 8079.279297\n",
      "Train Epoch: 66 [168192/225000 (75%)] Loss: 14768.855469\n",
      "Train Epoch: 66 [172288/225000 (77%)] Loss: 13693.353516\n",
      "Train Epoch: 66 [176384/225000 (78%)] Loss: 8081.951172\n",
      "Train Epoch: 66 [180480/225000 (80%)] Loss: 9783.037109\n",
      "Train Epoch: 66 [184576/225000 (82%)] Loss: 12550.619141\n",
      "Train Epoch: 66 [188672/225000 (84%)] Loss: 8182.867188\n",
      "Train Epoch: 66 [192768/225000 (86%)] Loss: 10446.300781\n",
      "Train Epoch: 66 [196864/225000 (87%)] Loss: 7967.109375\n",
      "Train Epoch: 66 [200960/225000 (89%)] Loss: 8152.408203\n",
      "Train Epoch: 66 [205056/225000 (91%)] Loss: 12179.224609\n",
      "Train Epoch: 66 [209152/225000 (93%)] Loss: 8090.708984\n",
      "Train Epoch: 66 [213248/225000 (95%)] Loss: 8229.886719\n",
      "Train Epoch: 66 [217344/225000 (97%)] Loss: 13849.562500\n",
      "Train Epoch: 66 [221440/225000 (98%)] Loss: 11074.574219\n",
      "    epoch          : 66\n",
      "    loss           : 9546.273155307877\n",
      "    val_loss       : 9118.8423749768\n",
      "Train Epoch: 67 [256/225000 (0%)] Loss: 11117.851562\n",
      "Train Epoch: 67 [4352/225000 (2%)] Loss: 8127.210938\n",
      "Train Epoch: 67 [8448/225000 (4%)] Loss: 7965.427734\n",
      "Train Epoch: 67 [12544/225000 (6%)] Loss: 10600.015625\n",
      "Train Epoch: 67 [16640/225000 (7%)] Loss: 8091.185547\n",
      "Train Epoch: 67 [20736/225000 (9%)] Loss: 7991.966797\n",
      "Train Epoch: 67 [24832/225000 (11%)] Loss: 8087.728516\n",
      "Train Epoch: 67 [28928/225000 (13%)] Loss: 8062.166016\n",
      "Train Epoch: 67 [33024/225000 (15%)] Loss: 8114.818359\n",
      "Train Epoch: 67 [37120/225000 (16%)] Loss: 9428.718750\n",
      "Train Epoch: 67 [41216/225000 (18%)] Loss: 8043.001953\n",
      "Train Epoch: 67 [45312/225000 (20%)] Loss: 8137.625000\n",
      "Train Epoch: 67 [49408/225000 (22%)] Loss: 7953.513672\n",
      "Train Epoch: 67 [53504/225000 (24%)] Loss: 11967.695312\n",
      "Train Epoch: 67 [57600/225000 (26%)] Loss: 8186.773438\n",
      "Train Epoch: 67 [61696/225000 (27%)] Loss: 15560.904297\n",
      "Train Epoch: 67 [65792/225000 (29%)] Loss: 9466.277344\n",
      "Train Epoch: 67 [69888/225000 (31%)] Loss: 7802.925781\n",
      "Train Epoch: 67 [73984/225000 (33%)] Loss: 7982.845703\n",
      "Train Epoch: 67 [78080/225000 (35%)] Loss: 9418.859375\n",
      "Train Epoch: 67 [82176/225000 (37%)] Loss: 13100.541016\n",
      "Train Epoch: 67 [86272/225000 (38%)] Loss: 10467.500000\n",
      "Train Epoch: 67 [90368/225000 (40%)] Loss: 8053.587891\n",
      "Train Epoch: 67 [94464/225000 (42%)] Loss: 13514.738281\n",
      "Train Epoch: 67 [98560/225000 (44%)] Loss: 8066.568359\n",
      "Train Epoch: 67 [102656/225000 (46%)] Loss: 13904.388672\n",
      "Train Epoch: 67 [106752/225000 (47%)] Loss: 8080.027344\n",
      "Train Epoch: 67 [110848/225000 (49%)] Loss: 9443.980469\n",
      "Train Epoch: 67 [114944/225000 (51%)] Loss: 7950.496094\n",
      "Train Epoch: 67 [119040/225000 (53%)] Loss: 8111.417969\n",
      "Train Epoch: 67 [123136/225000 (55%)] Loss: 8158.384766\n",
      "Train Epoch: 67 [127232/225000 (57%)] Loss: 8108.230469\n",
      "Train Epoch: 67 [131328/225000 (58%)] Loss: 10616.322266\n",
      "Train Epoch: 67 [135424/225000 (60%)] Loss: 7926.990234\n",
      "Train Epoch: 67 [139520/225000 (62%)] Loss: 10595.957031\n",
      "Train Epoch: 67 [143616/225000 (64%)] Loss: 8111.449219\n",
      "Train Epoch: 67 [147712/225000 (66%)] Loss: 12050.671875\n",
      "Train Epoch: 67 [151808/225000 (67%)] Loss: 8233.621094\n",
      "Train Epoch: 67 [155904/225000 (69%)] Loss: 8015.560547\n",
      "Train Epoch: 67 [160000/225000 (71%)] Loss: 10523.603516\n",
      "Train Epoch: 67 [164096/225000 (73%)] Loss: 7958.787109\n",
      "Train Epoch: 67 [168192/225000 (75%)] Loss: 9887.074219\n",
      "Train Epoch: 67 [172288/225000 (77%)] Loss: 7939.185547\n",
      "Train Epoch: 67 [176384/225000 (78%)] Loss: 7844.324219\n",
      "Train Epoch: 67 [180480/225000 (80%)] Loss: 9692.068359\n",
      "Train Epoch: 67 [184576/225000 (82%)] Loss: 8053.687500\n",
      "Train Epoch: 67 [188672/225000 (84%)] Loss: 9636.138672\n",
      "Train Epoch: 67 [192768/225000 (86%)] Loss: 9552.464844\n",
      "Train Epoch: 67 [196864/225000 (87%)] Loss: 13067.962891\n",
      "Train Epoch: 67 [200960/225000 (89%)] Loss: 7877.562500\n",
      "Train Epoch: 67 [205056/225000 (91%)] Loss: 9642.039062\n",
      "Train Epoch: 67 [209152/225000 (93%)] Loss: 10640.625000\n",
      "Train Epoch: 67 [213248/225000 (95%)] Loss: 8218.960938\n",
      "Train Epoch: 67 [217344/225000 (97%)] Loss: 9673.046875\n",
      "Train Epoch: 67 [221440/225000 (98%)] Loss: 7899.218750\n",
      "    epoch          : 67\n",
      "    loss           : 9549.49208973265\n",
      "    val_loss       : 9657.765760027632\n",
      "Train Epoch: 68 [256/225000 (0%)] Loss: 9689.753906\n",
      "Train Epoch: 68 [4352/225000 (2%)] Loss: 13855.406250\n",
      "Train Epoch: 68 [8448/225000 (4%)] Loss: 7862.328125\n",
      "Train Epoch: 68 [12544/225000 (6%)] Loss: 8144.017578\n",
      "Train Epoch: 68 [16640/225000 (7%)] Loss: 8249.181641\n",
      "Train Epoch: 68 [20736/225000 (9%)] Loss: 12431.140625\n",
      "Train Epoch: 68 [24832/225000 (11%)] Loss: 7998.744141\n",
      "Train Epoch: 68 [28928/225000 (13%)] Loss: 8099.351562\n",
      "Train Epoch: 68 [33024/225000 (15%)] Loss: 8229.544922\n",
      "Train Epoch: 68 [37120/225000 (16%)] Loss: 12345.818359\n",
      "Train Epoch: 68 [41216/225000 (18%)] Loss: 8224.068359\n",
      "Train Epoch: 68 [45312/225000 (20%)] Loss: 8115.994141\n",
      "Train Epoch: 68 [49408/225000 (22%)] Loss: 8259.951172\n",
      "Train Epoch: 68 [53504/225000 (24%)] Loss: 8213.119141\n",
      "Train Epoch: 68 [57600/225000 (26%)] Loss: 10851.806641\n",
      "Train Epoch: 68 [61696/225000 (27%)] Loss: 10516.931641\n",
      "Train Epoch: 68 [65792/225000 (29%)] Loss: 12403.720703\n",
      "Train Epoch: 68 [69888/225000 (31%)] Loss: 9615.078125\n",
      "Train Epoch: 68 [73984/225000 (33%)] Loss: 10658.470703\n",
      "Train Epoch: 68 [78080/225000 (35%)] Loss: 9620.000000\n",
      "Train Epoch: 68 [82176/225000 (37%)] Loss: 9756.470703\n",
      "Train Epoch: 68 [86272/225000 (38%)] Loss: 12867.099609\n",
      "Train Epoch: 68 [90368/225000 (40%)] Loss: 8046.341797\n",
      "Train Epoch: 68 [94464/225000 (42%)] Loss: 8193.197266\n",
      "Train Epoch: 68 [98560/225000 (44%)] Loss: 9678.646484\n",
      "Train Epoch: 68 [102656/225000 (46%)] Loss: 8236.558594\n",
      "Train Epoch: 68 [106752/225000 (47%)] Loss: 7947.285156\n",
      "Train Epoch: 68 [110848/225000 (49%)] Loss: 8148.611328\n",
      "Train Epoch: 68 [114944/225000 (51%)] Loss: 7942.378906\n",
      "Train Epoch: 68 [119040/225000 (53%)] Loss: 7870.341797\n",
      "Train Epoch: 68 [123136/225000 (55%)] Loss: 10561.621094\n",
      "Train Epoch: 68 [127232/225000 (57%)] Loss: 8113.724609\n",
      "Train Epoch: 68 [131328/225000 (58%)] Loss: 9781.041016\n",
      "Train Epoch: 68 [135424/225000 (60%)] Loss: 10349.869141\n",
      "Train Epoch: 68 [139520/225000 (62%)] Loss: 7978.750000\n",
      "Train Epoch: 68 [143616/225000 (64%)] Loss: 9587.464844\n",
      "Train Epoch: 68 [147712/225000 (66%)] Loss: 7911.203125\n",
      "Train Epoch: 68 [151808/225000 (67%)] Loss: 8055.652344\n",
      "Train Epoch: 68 [155904/225000 (69%)] Loss: 7947.591797\n",
      "Train Epoch: 68 [160000/225000 (71%)] Loss: 8248.841797\n",
      "Train Epoch: 68 [164096/225000 (73%)] Loss: 8282.593750\n",
      "Train Epoch: 68 [168192/225000 (75%)] Loss: 9720.927734\n",
      "Train Epoch: 68 [172288/225000 (77%)] Loss: 8045.121094\n",
      "Train Epoch: 68 [176384/225000 (78%)] Loss: 9661.669922\n",
      "Train Epoch: 68 [180480/225000 (80%)] Loss: 8078.763672\n",
      "Train Epoch: 68 [184576/225000 (82%)] Loss: 12399.443359\n",
      "Train Epoch: 68 [188672/225000 (84%)] Loss: 8044.908203\n",
      "Train Epoch: 68 [192768/225000 (86%)] Loss: 11785.898438\n",
      "Train Epoch: 68 [196864/225000 (87%)] Loss: 8115.111328\n",
      "Train Epoch: 68 [200960/225000 (89%)] Loss: 7988.386719\n",
      "Train Epoch: 68 [205056/225000 (91%)] Loss: 8162.443359\n",
      "Train Epoch: 68 [209152/225000 (93%)] Loss: 12143.919922\n",
      "Train Epoch: 68 [213248/225000 (95%)] Loss: 8126.564453\n",
      "Train Epoch: 68 [217344/225000 (97%)] Loss: 8149.291016\n",
      "Train Epoch: 68 [221440/225000 (98%)] Loss: 8047.384766\n",
      "    epoch          : 68\n",
      "    loss           : 9479.892825876352\n",
      "    val_loss       : 9622.731379937153\n",
      "Train Epoch: 69 [256/225000 (0%)] Loss: 7846.908203\n",
      "Train Epoch: 69 [4352/225000 (2%)] Loss: 8020.925781\n",
      "Train Epoch: 69 [8448/225000 (4%)] Loss: 13509.445312\n",
      "Train Epoch: 69 [12544/225000 (6%)] Loss: 13923.599609\n",
      "Train Epoch: 69 [16640/225000 (7%)] Loss: 7902.406250\n",
      "Train Epoch: 69 [20736/225000 (9%)] Loss: 8017.687500\n",
      "Train Epoch: 69 [24832/225000 (11%)] Loss: 8109.628906\n",
      "Train Epoch: 69 [28928/225000 (13%)] Loss: 10625.234375\n",
      "Train Epoch: 69 [33024/225000 (15%)] Loss: 7940.912109\n",
      "Train Epoch: 69 [37120/225000 (16%)] Loss: 13629.015625\n",
      "Train Epoch: 69 [41216/225000 (18%)] Loss: 10632.447266\n",
      "Train Epoch: 69 [45312/225000 (20%)] Loss: 8119.470703\n",
      "Train Epoch: 69 [49408/225000 (22%)] Loss: 8096.226562\n",
      "Train Epoch: 69 [53504/225000 (24%)] Loss: 7849.083984\n",
      "Train Epoch: 69 [57600/225000 (26%)] Loss: 7935.322266\n",
      "Train Epoch: 69 [61696/225000 (27%)] Loss: 8005.701172\n",
      "Train Epoch: 69 [65792/225000 (29%)] Loss: 8149.269531\n",
      "Train Epoch: 69 [69888/225000 (31%)] Loss: 16064.326172\n",
      "Train Epoch: 69 [73984/225000 (33%)] Loss: 9727.236328\n",
      "Train Epoch: 69 [78080/225000 (35%)] Loss: 13377.830078\n",
      "Train Epoch: 69 [82176/225000 (37%)] Loss: 9754.410156\n",
      "Train Epoch: 69 [86272/225000 (38%)] Loss: 10423.710938\n",
      "Train Epoch: 69 [90368/225000 (40%)] Loss: 7956.580078\n",
      "Train Epoch: 69 [94464/225000 (42%)] Loss: 7886.804688\n",
      "Train Epoch: 69 [98560/225000 (44%)] Loss: 10627.792969\n",
      "Train Epoch: 69 [102656/225000 (46%)] Loss: 8004.789062\n",
      "Train Epoch: 69 [106752/225000 (47%)] Loss: 8125.955078\n",
      "Train Epoch: 69 [110848/225000 (49%)] Loss: 12645.117188\n",
      "Train Epoch: 69 [114944/225000 (51%)] Loss: 9718.216797\n",
      "Train Epoch: 69 [119040/225000 (53%)] Loss: 7950.304688\n",
      "Train Epoch: 69 [123136/225000 (55%)] Loss: 8186.132812\n",
      "Train Epoch: 69 [127232/225000 (57%)] Loss: 13708.945312\n",
      "Train Epoch: 69 [131328/225000 (58%)] Loss: 8042.371094\n",
      "Train Epoch: 69 [135424/225000 (60%)] Loss: 8074.974609\n",
      "Train Epoch: 69 [139520/225000 (62%)] Loss: 9599.902344\n",
      "Train Epoch: 69 [143616/225000 (64%)] Loss: 7946.882812\n",
      "Train Epoch: 69 [147712/225000 (66%)] Loss: 8141.876953\n",
      "Train Epoch: 69 [151808/225000 (67%)] Loss: 9518.947266\n",
      "Train Epoch: 69 [155904/225000 (69%)] Loss: 8042.351562\n",
      "Train Epoch: 69 [160000/225000 (71%)] Loss: 9791.419922\n",
      "Train Epoch: 69 [164096/225000 (73%)] Loss: 7923.708984\n",
      "Train Epoch: 69 [168192/225000 (75%)] Loss: 9487.199219\n",
      "Train Epoch: 69 [172288/225000 (77%)] Loss: 8267.601562\n",
      "Train Epoch: 69 [176384/225000 (78%)] Loss: 8022.056641\n",
      "Train Epoch: 69 [180480/225000 (80%)] Loss: 9496.021484\n",
      "Train Epoch: 69 [184576/225000 (82%)] Loss: 8068.716797\n",
      "Train Epoch: 69 [188672/225000 (84%)] Loss: 13727.433594\n",
      "Train Epoch: 69 [192768/225000 (86%)] Loss: 13825.203125\n",
      "Train Epoch: 69 [196864/225000 (87%)] Loss: 13597.056641\n",
      "Train Epoch: 69 [200960/225000 (89%)] Loss: 9608.666016\n",
      "Train Epoch: 69 [205056/225000 (91%)] Loss: 9802.535156\n",
      "Train Epoch: 69 [209152/225000 (93%)] Loss: 8219.541016\n",
      "Train Epoch: 69 [213248/225000 (95%)] Loss: 7811.349609\n",
      "Train Epoch: 69 [217344/225000 (97%)] Loss: 9561.916016\n",
      "Train Epoch: 69 [221440/225000 (98%)] Loss: 8289.294922\n",
      "    epoch          : 69\n",
      "    loss           : 9606.360881505972\n",
      "    val_loss       : 9447.533928922245\n",
      "Train Epoch: 70 [256/225000 (0%)] Loss: 8024.011719\n",
      "Train Epoch: 70 [4352/225000 (2%)] Loss: 14032.912109\n",
      "Train Epoch: 70 [8448/225000 (4%)] Loss: 10564.734375\n",
      "Train Epoch: 70 [12544/225000 (6%)] Loss: 7903.931641\n",
      "Train Epoch: 70 [16640/225000 (7%)] Loss: 8148.195312\n",
      "Train Epoch: 70 [20736/225000 (9%)] Loss: 12474.164062\n",
      "Train Epoch: 70 [24832/225000 (11%)] Loss: 8015.837891\n",
      "Train Epoch: 70 [28928/225000 (13%)] Loss: 10640.402344\n",
      "Train Epoch: 70 [33024/225000 (15%)] Loss: 9502.615234\n",
      "Train Epoch: 70 [37120/225000 (16%)] Loss: 13313.363281\n",
      "Train Epoch: 70 [41216/225000 (18%)] Loss: 8147.199219\n",
      "Train Epoch: 70 [45312/225000 (20%)] Loss: 8047.292969\n",
      "Train Epoch: 70 [49408/225000 (22%)] Loss: 7915.423828\n",
      "Train Epoch: 70 [53504/225000 (24%)] Loss: 13793.591797\n",
      "Train Epoch: 70 [57600/225000 (26%)] Loss: 15443.949219\n",
      "Train Epoch: 70 [61696/225000 (27%)] Loss: 8210.986328\n",
      "Train Epoch: 70 [65792/225000 (29%)] Loss: 8062.103516\n",
      "Train Epoch: 70 [69888/225000 (31%)] Loss: 8071.166016\n",
      "Train Epoch: 70 [73984/225000 (33%)] Loss: 8034.189453\n",
      "Train Epoch: 70 [78080/225000 (35%)] Loss: 8027.873047\n",
      "Train Epoch: 70 [82176/225000 (37%)] Loss: 9518.318359\n",
      "Train Epoch: 70 [86272/225000 (38%)] Loss: 10348.433594\n",
      "Train Epoch: 70 [90368/225000 (40%)] Loss: 8141.275391\n",
      "Train Epoch: 70 [94464/225000 (42%)] Loss: 9842.886719\n",
      "Train Epoch: 70 [98560/225000 (44%)] Loss: 8244.675781\n",
      "Train Epoch: 70 [102656/225000 (46%)] Loss: 8051.302734\n",
      "Train Epoch: 70 [106752/225000 (47%)] Loss: 16226.216797\n",
      "Train Epoch: 70 [110848/225000 (49%)] Loss: 7914.945312\n",
      "Train Epoch: 70 [114944/225000 (51%)] Loss: 7968.298828\n",
      "Train Epoch: 70 [119040/225000 (53%)] Loss: 8155.251953\n",
      "Train Epoch: 70 [123136/225000 (55%)] Loss: 12447.181641\n",
      "Train Epoch: 70 [127232/225000 (57%)] Loss: 8258.730469\n",
      "Train Epoch: 70 [131328/225000 (58%)] Loss: 14034.507812\n",
      "Train Epoch: 70 [135424/225000 (60%)] Loss: 7852.527344\n",
      "Train Epoch: 70 [139520/225000 (62%)] Loss: 9717.304688\n",
      "Train Epoch: 70 [143616/225000 (64%)] Loss: 13462.162109\n",
      "Train Epoch: 70 [147712/225000 (66%)] Loss: 8026.466797\n",
      "Train Epoch: 70 [151808/225000 (67%)] Loss: 8074.421875\n",
      "Train Epoch: 70 [155904/225000 (69%)] Loss: 8062.695312\n",
      "Train Epoch: 70 [160000/225000 (71%)] Loss: 8040.357422\n",
      "Train Epoch: 70 [164096/225000 (73%)] Loss: 9465.710938\n",
      "Train Epoch: 70 [168192/225000 (75%)] Loss: 12172.777344\n",
      "Train Epoch: 70 [172288/225000 (77%)] Loss: 10655.099609\n",
      "Train Epoch: 70 [176384/225000 (78%)] Loss: 7802.908203\n",
      "Train Epoch: 70 [180480/225000 (80%)] Loss: 8074.046875\n",
      "Train Epoch: 70 [184576/225000 (82%)] Loss: 12940.876953\n",
      "Train Epoch: 70 [188672/225000 (84%)] Loss: 10279.421875\n",
      "Train Epoch: 70 [192768/225000 (86%)] Loss: 8076.615234\n",
      "Train Epoch: 70 [196864/225000 (87%)] Loss: 8040.316406\n",
      "Train Epoch: 70 [200960/225000 (89%)] Loss: 8090.695312\n",
      "Train Epoch: 70 [205056/225000 (91%)] Loss: 8051.966797\n",
      "Train Epoch: 70 [209152/225000 (93%)] Loss: 9637.085938\n",
      "Train Epoch: 70 [213248/225000 (95%)] Loss: 8212.525391\n",
      "Train Epoch: 70 [217344/225000 (97%)] Loss: 9532.119141\n",
      "Train Epoch: 70 [221440/225000 (98%)] Loss: 8105.742188\n",
      "    epoch          : 70\n",
      "    loss           : 9583.695846887444\n",
      "    val_loss       : 9520.254367696996\n",
      "Train Epoch: 71 [256/225000 (0%)] Loss: 7976.861328\n",
      "Train Epoch: 71 [4352/225000 (2%)] Loss: 13834.343750\n",
      "Train Epoch: 71 [8448/225000 (4%)] Loss: 13532.748047\n",
      "Train Epoch: 71 [12544/225000 (6%)] Loss: 8111.800781\n",
      "Train Epoch: 71 [16640/225000 (7%)] Loss: 8137.189453\n",
      "Train Epoch: 71 [20736/225000 (9%)] Loss: 8203.556641\n",
      "Train Epoch: 71 [24832/225000 (11%)] Loss: 11808.257812\n",
      "Train Epoch: 71 [28928/225000 (13%)] Loss: 7903.115234\n",
      "Train Epoch: 71 [33024/225000 (15%)] Loss: 9633.171875\n",
      "Train Epoch: 71 [37120/225000 (16%)] Loss: 8017.449219\n",
      "Train Epoch: 71 [41216/225000 (18%)] Loss: 8135.158203\n",
      "Train Epoch: 71 [45312/225000 (20%)] Loss: 8286.886719\n",
      "Train Epoch: 71 [49408/225000 (22%)] Loss: 8155.593750\n",
      "Train Epoch: 71 [53504/225000 (24%)] Loss: 7968.224609\n",
      "Train Epoch: 71 [57600/225000 (26%)] Loss: 9558.802734\n",
      "Train Epoch: 71 [61696/225000 (27%)] Loss: 9491.263672\n",
      "Train Epoch: 71 [65792/225000 (29%)] Loss: 9772.751953\n",
      "Train Epoch: 71 [69888/225000 (31%)] Loss: 8158.476562\n",
      "Train Epoch: 71 [73984/225000 (33%)] Loss: 8006.593750\n",
      "Train Epoch: 71 [78080/225000 (35%)] Loss: 7933.376953\n",
      "Train Epoch: 71 [82176/225000 (37%)] Loss: 11289.251953\n",
      "Train Epoch: 71 [86272/225000 (38%)] Loss: 8224.257812\n",
      "Train Epoch: 71 [90368/225000 (40%)] Loss: 7854.710938\n",
      "Train Epoch: 71 [94464/225000 (42%)] Loss: 8107.281250\n",
      "Train Epoch: 71 [98560/225000 (44%)] Loss: 7902.339844\n",
      "Train Epoch: 71 [102656/225000 (46%)] Loss: 12273.716797\n",
      "Train Epoch: 71 [106752/225000 (47%)] Loss: 7902.941406\n",
      "Train Epoch: 71 [110848/225000 (49%)] Loss: 9375.517578\n",
      "Train Epoch: 71 [114944/225000 (51%)] Loss: 8084.314453\n",
      "Train Epoch: 71 [119040/225000 (53%)] Loss: 8022.542969\n",
      "Train Epoch: 71 [123136/225000 (55%)] Loss: 7880.890625\n",
      "Train Epoch: 71 [127232/225000 (57%)] Loss: 10637.808594\n",
      "Train Epoch: 71 [131328/225000 (58%)] Loss: 10716.591797\n",
      "Train Epoch: 71 [135424/225000 (60%)] Loss: 8008.412109\n",
      "Train Epoch: 71 [139520/225000 (62%)] Loss: 8066.136719\n",
      "Train Epoch: 71 [143616/225000 (64%)] Loss: 9620.218750\n",
      "Train Epoch: 71 [147712/225000 (66%)] Loss: 8064.064453\n",
      "Train Epoch: 71 [151808/225000 (67%)] Loss: 10434.330078\n",
      "Train Epoch: 71 [155904/225000 (69%)] Loss: 9612.517578\n",
      "Train Epoch: 71 [160000/225000 (71%)] Loss: 8015.955078\n",
      "Train Epoch: 71 [164096/225000 (73%)] Loss: 8236.242188\n",
      "Train Epoch: 71 [168192/225000 (75%)] Loss: 9825.513672\n",
      "Train Epoch: 71 [172288/225000 (77%)] Loss: 8175.164062\n",
      "Train Epoch: 71 [176384/225000 (78%)] Loss: 8203.082031\n",
      "Train Epoch: 71 [180480/225000 (80%)] Loss: 17245.175781\n",
      "Train Epoch: 71 [184576/225000 (82%)] Loss: 8169.472656\n",
      "Train Epoch: 71 [188672/225000 (84%)] Loss: 8067.353516\n",
      "Train Epoch: 71 [192768/225000 (86%)] Loss: 9832.324219\n",
      "Train Epoch: 71 [196864/225000 (87%)] Loss: 7936.070312\n",
      "Train Epoch: 71 [200960/225000 (89%)] Loss: 8146.441406\n",
      "Train Epoch: 71 [205056/225000 (91%)] Loss: 9714.031250\n",
      "Train Epoch: 71 [209152/225000 (93%)] Loss: 7935.175781\n",
      "Train Epoch: 71 [213248/225000 (95%)] Loss: 9681.664062\n",
      "Train Epoch: 71 [217344/225000 (97%)] Loss: 8207.968750\n",
      "Train Epoch: 71 [221440/225000 (98%)] Loss: 8011.574219\n",
      "    epoch          : 71\n",
      "    loss           : 9381.24515162827\n",
      "    val_loss       : 9623.219992097544\n",
      "Train Epoch: 72 [256/225000 (0%)] Loss: 8203.027344\n",
      "Train Epoch: 72 [4352/225000 (2%)] Loss: 13966.060547\n",
      "Train Epoch: 72 [8448/225000 (4%)] Loss: 8010.744141\n",
      "Train Epoch: 72 [12544/225000 (6%)] Loss: 10499.750000\n",
      "Train Epoch: 72 [16640/225000 (7%)] Loss: 16255.166016\n",
      "Train Epoch: 72 [20736/225000 (9%)] Loss: 12431.001953\n",
      "Train Epoch: 72 [24832/225000 (11%)] Loss: 13747.078125\n",
      "Train Epoch: 72 [28928/225000 (13%)] Loss: 12534.929688\n",
      "Train Epoch: 72 [33024/225000 (15%)] Loss: 8006.539062\n",
      "Train Epoch: 72 [37120/225000 (16%)] Loss: 7961.232422\n",
      "Train Epoch: 72 [41216/225000 (18%)] Loss: 15502.792969\n",
      "Train Epoch: 72 [45312/225000 (20%)] Loss: 8002.294922\n",
      "Train Epoch: 72 [49408/225000 (22%)] Loss: 8176.820312\n",
      "Train Epoch: 72 [53504/225000 (24%)] Loss: 12586.759766\n",
      "Train Epoch: 72 [57600/225000 (26%)] Loss: 8143.738281\n",
      "Train Epoch: 72 [61696/225000 (27%)] Loss: 8066.134766\n",
      "Train Epoch: 72 [65792/225000 (29%)] Loss: 7985.589844\n",
      "Train Epoch: 72 [69888/225000 (31%)] Loss: 10592.642578\n",
      "Train Epoch: 72 [73984/225000 (33%)] Loss: 7898.126953\n",
      "Train Epoch: 72 [78080/225000 (35%)] Loss: 8113.166016\n",
      "Train Epoch: 72 [82176/225000 (37%)] Loss: 13765.019531\n",
      "Train Epoch: 72 [86272/225000 (38%)] Loss: 8235.261719\n",
      "Train Epoch: 72 [90368/225000 (40%)] Loss: 13704.804688\n",
      "Train Epoch: 72 [94464/225000 (42%)] Loss: 9634.117188\n",
      "Train Epoch: 72 [98560/225000 (44%)] Loss: 7951.486328\n",
      "Train Epoch: 72 [102656/225000 (46%)] Loss: 9617.937500\n",
      "Train Epoch: 72 [106752/225000 (47%)] Loss: 7942.771484\n",
      "Train Epoch: 72 [110848/225000 (49%)] Loss: 8051.683594\n",
      "Train Epoch: 72 [114944/225000 (51%)] Loss: 9891.916016\n",
      "Train Epoch: 72 [119040/225000 (53%)] Loss: 8048.048828\n",
      "Train Epoch: 72 [123136/225000 (55%)] Loss: 8127.587891\n",
      "Train Epoch: 72 [127232/225000 (57%)] Loss: 8126.607422\n",
      "Train Epoch: 72 [131328/225000 (58%)] Loss: 19054.658203\n",
      "Train Epoch: 72 [135424/225000 (60%)] Loss: 8123.083984\n",
      "Train Epoch: 72 [139520/225000 (62%)] Loss: 8145.972656\n",
      "Train Epoch: 72 [143616/225000 (64%)] Loss: 12458.439453\n",
      "Train Epoch: 72 [147712/225000 (66%)] Loss: 8142.833984\n",
      "Train Epoch: 72 [151808/225000 (67%)] Loss: 10542.642578\n",
      "Train Epoch: 72 [155904/225000 (69%)] Loss: 13726.652344\n",
      "Train Epoch: 72 [160000/225000 (71%)] Loss: 9619.802734\n",
      "Train Epoch: 72 [164096/225000 (73%)] Loss: 8020.445312\n",
      "Train Epoch: 72 [168192/225000 (75%)] Loss: 7971.908203\n",
      "Train Epoch: 72 [172288/225000 (77%)] Loss: 7968.066406\n",
      "Train Epoch: 72 [176384/225000 (78%)] Loss: 7978.507812\n",
      "Train Epoch: 72 [180480/225000 (80%)] Loss: 8101.611328\n",
      "Train Epoch: 72 [184576/225000 (82%)] Loss: 8135.396484\n",
      "Train Epoch: 72 [188672/225000 (84%)] Loss: 8032.017578\n",
      "Train Epoch: 72 [192768/225000 (86%)] Loss: 7988.240234\n",
      "Train Epoch: 72 [196864/225000 (87%)] Loss: 9629.921875\n",
      "Train Epoch: 72 [200960/225000 (89%)] Loss: 7870.576172\n",
      "Train Epoch: 72 [205056/225000 (91%)] Loss: 8073.222656\n",
      "Train Epoch: 72 [209152/225000 (93%)] Loss: 14029.146484\n",
      "Train Epoch: 72 [213248/225000 (95%)] Loss: 8257.724609\n",
      "Train Epoch: 72 [217344/225000 (97%)] Loss: 12781.658203\n",
      "Train Epoch: 72 [221440/225000 (98%)] Loss: 9740.310547\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "    epoch          : 72\n",
      "    loss           : 9585.560140251706\n",
      "    val_loss       : 9393.464239458648\n",
      "Train Epoch: 73 [256/225000 (0%)] Loss: 8001.105469\n",
      "Train Epoch: 73 [4352/225000 (2%)] Loss: 7880.048828\n",
      "Train Epoch: 73 [8448/225000 (4%)] Loss: 8027.162109\n",
      "Train Epoch: 73 [12544/225000 (6%)] Loss: 12848.064453\n",
      "Train Epoch: 73 [16640/225000 (7%)] Loss: 7925.962891\n",
      "Train Epoch: 73 [20736/225000 (9%)] Loss: 13865.130859\n",
      "Train Epoch: 73 [24832/225000 (11%)] Loss: 10509.205078\n",
      "Train Epoch: 73 [28928/225000 (13%)] Loss: 13575.005859\n",
      "Train Epoch: 73 [33024/225000 (15%)] Loss: 7823.158203\n",
      "Train Epoch: 73 [37120/225000 (16%)] Loss: 8138.199219\n",
      "Train Epoch: 73 [41216/225000 (18%)] Loss: 9752.878906\n",
      "Train Epoch: 73 [45312/225000 (20%)] Loss: 8271.896484\n",
      "Train Epoch: 73 [49408/225000 (22%)] Loss: 10580.445312\n",
      "Train Epoch: 73 [53504/225000 (24%)] Loss: 8157.492188\n",
      "Train Epoch: 73 [57600/225000 (26%)] Loss: 7930.976562\n",
      "Train Epoch: 73 [61696/225000 (27%)] Loss: 7958.283203\n",
      "Train Epoch: 73 [65792/225000 (29%)] Loss: 9631.626953\n",
      "Train Epoch: 73 [69888/225000 (31%)] Loss: 10473.132812\n",
      "Train Epoch: 73 [73984/225000 (33%)] Loss: 7965.355469\n",
      "Train Epoch: 73 [78080/225000 (35%)] Loss: 8153.119141\n",
      "Train Epoch: 73 [82176/225000 (37%)] Loss: 10388.097656\n",
      "Train Epoch: 73 [86272/225000 (38%)] Loss: 8013.648438\n",
      "Train Epoch: 73 [90368/225000 (40%)] Loss: 7866.132812\n",
      "Train Epoch: 73 [94464/225000 (42%)] Loss: 12426.792969\n",
      "Train Epoch: 73 [98560/225000 (44%)] Loss: 8117.792969\n",
      "Train Epoch: 73 [102656/225000 (46%)] Loss: 7920.492188\n",
      "Train Epoch: 73 [106752/225000 (47%)] Loss: 19380.257812\n",
      "Train Epoch: 73 [110848/225000 (49%)] Loss: 8170.488281\n",
      "Train Epoch: 73 [114944/225000 (51%)] Loss: 9796.962891\n",
      "Train Epoch: 73 [119040/225000 (53%)] Loss: 9627.744141\n",
      "Train Epoch: 73 [123136/225000 (55%)] Loss: 8142.146484\n",
      "Train Epoch: 73 [127232/225000 (57%)] Loss: 7828.169922\n",
      "Train Epoch: 73 [131328/225000 (58%)] Loss: 12348.812500\n",
      "Train Epoch: 73 [135424/225000 (60%)] Loss: 8179.070312\n",
      "Train Epoch: 73 [139520/225000 (62%)] Loss: 7979.000000\n",
      "Train Epoch: 73 [143616/225000 (64%)] Loss: 10516.126953\n",
      "Train Epoch: 73 [147712/225000 (66%)] Loss: 8252.703125\n",
      "Train Epoch: 73 [151808/225000 (67%)] Loss: 12320.236328\n",
      "Train Epoch: 73 [155904/225000 (69%)] Loss: 14974.521484\n",
      "Train Epoch: 73 [160000/225000 (71%)] Loss: 8158.294922\n",
      "Train Epoch: 73 [164096/225000 (73%)] Loss: 8176.958984\n",
      "Train Epoch: 73 [168192/225000 (75%)] Loss: 7988.488281\n",
      "Train Epoch: 73 [172288/225000 (77%)] Loss: 8019.892578\n",
      "Train Epoch: 73 [176384/225000 (78%)] Loss: 8124.695312\n",
      "Train Epoch: 73 [180480/225000 (80%)] Loss: 8129.169922\n",
      "Train Epoch: 73 [184576/225000 (82%)] Loss: 15198.095703\n",
      "Train Epoch: 73 [188672/225000 (84%)] Loss: 11229.863281\n",
      "Train Epoch: 73 [192768/225000 (86%)] Loss: 8025.066406\n",
      "Train Epoch: 73 [196864/225000 (87%)] Loss: 7968.017578\n",
      "Train Epoch: 73 [200960/225000 (89%)] Loss: 12366.923828\n",
      "Train Epoch: 73 [205056/225000 (91%)] Loss: 7885.720703\n",
      "Train Epoch: 73 [209152/225000 (93%)] Loss: 10585.580078\n",
      "Train Epoch: 73 [213248/225000 (95%)] Loss: 8112.753906\n",
      "Train Epoch: 73 [217344/225000 (97%)] Loss: 9724.449219\n",
      "Train Epoch: 73 [221440/225000 (98%)] Loss: 8221.443359\n",
      "    epoch          : 73\n",
      "    loss           : 9547.2057391656\n",
      "    val_loss       : 9636.004650926103\n",
      "Train Epoch: 74 [256/225000 (0%)] Loss: 9608.257812\n",
      "Train Epoch: 74 [4352/225000 (2%)] Loss: 10443.687500\n",
      "Train Epoch: 74 [8448/225000 (4%)] Loss: 10616.201172\n",
      "Train Epoch: 74 [12544/225000 (6%)] Loss: 7854.609375\n",
      "Train Epoch: 74 [16640/225000 (7%)] Loss: 8074.318359\n",
      "Train Epoch: 74 [20736/225000 (9%)] Loss: 7990.201172\n",
      "Train Epoch: 74 [24832/225000 (11%)] Loss: 9512.826172\n",
      "Train Epoch: 74 [28928/225000 (13%)] Loss: 7993.363281\n",
      "Train Epoch: 74 [33024/225000 (15%)] Loss: 7938.425781\n",
      "Train Epoch: 74 [37120/225000 (16%)] Loss: 8047.472656\n",
      "Train Epoch: 74 [41216/225000 (18%)] Loss: 8143.212891\n",
      "Train Epoch: 74 [45312/225000 (20%)] Loss: 8059.826172\n",
      "Train Epoch: 74 [49408/225000 (22%)] Loss: 12208.652344\n",
      "Train Epoch: 74 [53504/225000 (24%)] Loss: 9954.324219\n",
      "Train Epoch: 74 [57600/225000 (26%)] Loss: 7850.625000\n",
      "Train Epoch: 74 [61696/225000 (27%)] Loss: 13526.666016\n",
      "Train Epoch: 74 [65792/225000 (29%)] Loss: 8083.605469\n",
      "Train Epoch: 74 [69888/225000 (31%)] Loss: 8101.861328\n",
      "Train Epoch: 74 [73984/225000 (33%)] Loss: 19346.869141\n",
      "Train Epoch: 74 [78080/225000 (35%)] Loss: 9517.166016\n",
      "Train Epoch: 74 [82176/225000 (37%)] Loss: 7993.828125\n",
      "Train Epoch: 74 [86272/225000 (38%)] Loss: 9697.251953\n",
      "Train Epoch: 74 [90368/225000 (40%)] Loss: 12361.410156\n",
      "Train Epoch: 74 [94464/225000 (42%)] Loss: 10557.121094\n",
      "Train Epoch: 74 [98560/225000 (44%)] Loss: 8244.787109\n",
      "Train Epoch: 74 [102656/225000 (46%)] Loss: 9699.103516\n",
      "Train Epoch: 74 [106752/225000 (47%)] Loss: 8025.101562\n",
      "Train Epoch: 74 [110848/225000 (49%)] Loss: 9786.806641\n",
      "Train Epoch: 74 [114944/225000 (51%)] Loss: 7939.027344\n",
      "Train Epoch: 74 [119040/225000 (53%)] Loss: 7923.955078\n",
      "Train Epoch: 74 [123136/225000 (55%)] Loss: 8227.623047\n",
      "Train Epoch: 74 [127232/225000 (57%)] Loss: 8106.792969\n",
      "Train Epoch: 74 [131328/225000 (58%)] Loss: 7952.371094\n",
      "Train Epoch: 74 [135424/225000 (60%)] Loss: 9731.380859\n",
      "Train Epoch: 74 [139520/225000 (62%)] Loss: 12089.640625\n",
      "Train Epoch: 74 [143616/225000 (64%)] Loss: 15069.951172\n",
      "Train Epoch: 74 [147712/225000 (66%)] Loss: 8074.531250\n",
      "Train Epoch: 74 [151808/225000 (67%)] Loss: 7963.208984\n",
      "Train Epoch: 74 [155904/225000 (69%)] Loss: 7909.781250\n",
      "Train Epoch: 74 [160000/225000 (71%)] Loss: 8160.048828\n",
      "Train Epoch: 74 [164096/225000 (73%)] Loss: 9579.724609\n",
      "Train Epoch: 74 [168192/225000 (75%)] Loss: 8219.882812\n",
      "Train Epoch: 74 [172288/225000 (77%)] Loss: 8183.943359\n",
      "Train Epoch: 74 [176384/225000 (78%)] Loss: 8204.685547\n",
      "Train Epoch: 74 [180480/225000 (80%)] Loss: 8011.222656\n",
      "Train Epoch: 74 [184576/225000 (82%)] Loss: 8141.226562\n",
      "Train Epoch: 74 [188672/225000 (84%)] Loss: 10512.425781\n",
      "Train Epoch: 74 [192768/225000 (86%)] Loss: 10575.755859\n",
      "Train Epoch: 74 [196864/225000 (87%)] Loss: 8234.599609\n",
      "Train Epoch: 74 [200960/225000 (89%)] Loss: 8119.488281\n",
      "Train Epoch: 74 [205056/225000 (91%)] Loss: 13877.109375\n",
      "Train Epoch: 74 [209152/225000 (93%)] Loss: 8006.685547\n",
      "Train Epoch: 74 [213248/225000 (95%)] Loss: 9611.095703\n",
      "Train Epoch: 74 [217344/225000 (97%)] Loss: 12634.964844\n",
      "Train Epoch: 74 [221440/225000 (98%)] Loss: 7851.371094\n",
      "    epoch          : 74\n",
      "    loss           : 9629.73109090586\n",
      "    val_loss       : 9342.957620266749\n",
      "Train Epoch: 75 [256/225000 (0%)] Loss: 9445.634766\n",
      "Train Epoch: 75 [4352/225000 (2%)] Loss: 15353.595703\n",
      "Train Epoch: 75 [8448/225000 (4%)] Loss: 13594.783203\n",
      "Train Epoch: 75 [12544/225000 (6%)] Loss: 8159.218750\n",
      "Train Epoch: 75 [16640/225000 (7%)] Loss: 8278.072266\n",
      "Train Epoch: 75 [20736/225000 (9%)] Loss: 10455.679688\n",
      "Train Epoch: 75 [24832/225000 (11%)] Loss: 7938.912109\n",
      "Train Epoch: 75 [28928/225000 (13%)] Loss: 7975.361328\n",
      "Train Epoch: 75 [33024/225000 (15%)] Loss: 7955.796875\n",
      "Train Epoch: 75 [37120/225000 (16%)] Loss: 13134.455078\n",
      "Train Epoch: 75 [41216/225000 (18%)] Loss: 7961.701172\n",
      "Train Epoch: 75 [45312/225000 (20%)] Loss: 10523.968750\n",
      "Train Epoch: 75 [49408/225000 (22%)] Loss: 8132.400391\n",
      "Train Epoch: 75 [53504/225000 (24%)] Loss: 8079.013672\n",
      "Train Epoch: 75 [57600/225000 (26%)] Loss: 10831.197266\n",
      "Train Epoch: 75 [61696/225000 (27%)] Loss: 8238.060547\n",
      "Train Epoch: 75 [65792/225000 (29%)] Loss: 8179.083984\n",
      "Train Epoch: 75 [69888/225000 (31%)] Loss: 9401.074219\n",
      "Train Epoch: 75 [73984/225000 (33%)] Loss: 12729.765625\n",
      "Train Epoch: 75 [78080/225000 (35%)] Loss: 9565.716797\n",
      "Train Epoch: 75 [82176/225000 (37%)] Loss: 7871.695312\n",
      "Train Epoch: 75 [86272/225000 (38%)] Loss: 9537.017578\n",
      "Train Epoch: 75 [90368/225000 (40%)] Loss: 10681.896484\n",
      "Train Epoch: 75 [94464/225000 (42%)] Loss: 7870.359375\n",
      "Train Epoch: 75 [98560/225000 (44%)] Loss: 8309.980469\n",
      "Train Epoch: 75 [102656/225000 (46%)] Loss: 7987.349609\n",
      "Train Epoch: 75 [106752/225000 (47%)] Loss: 9794.025391\n",
      "Train Epoch: 75 [110848/225000 (49%)] Loss: 8013.908203\n",
      "Train Epoch: 75 [114944/225000 (51%)] Loss: 8047.279297\n",
      "Train Epoch: 75 [119040/225000 (53%)] Loss: 9647.089844\n",
      "Train Epoch: 75 [123136/225000 (55%)] Loss: 16021.494141\n",
      "Train Epoch: 75 [127232/225000 (57%)] Loss: 10253.363281\n",
      "Train Epoch: 75 [131328/225000 (58%)] Loss: 9429.576172\n",
      "Train Epoch: 75 [135424/225000 (60%)] Loss: 9616.705078\n",
      "Train Epoch: 75 [139520/225000 (62%)] Loss: 8051.105469\n",
      "Train Epoch: 75 [143616/225000 (64%)] Loss: 16372.646484\n",
      "Train Epoch: 75 [147712/225000 (66%)] Loss: 8059.419922\n",
      "Train Epoch: 75 [151808/225000 (67%)] Loss: 9604.703125\n",
      "Train Epoch: 75 [155904/225000 (69%)] Loss: 8105.408203\n",
      "Train Epoch: 75 [160000/225000 (71%)] Loss: 7986.548828\n",
      "Train Epoch: 75 [164096/225000 (73%)] Loss: 7949.519531\n",
      "Train Epoch: 75 [168192/225000 (75%)] Loss: 9766.408203\n",
      "Train Epoch: 75 [172288/225000 (77%)] Loss: 8102.796875\n",
      "Train Epoch: 75 [176384/225000 (78%)] Loss: 7996.789062\n",
      "Train Epoch: 75 [180480/225000 (80%)] Loss: 8105.347656\n",
      "Train Epoch: 75 [184576/225000 (82%)] Loss: 10449.232422\n",
      "Train Epoch: 75 [188672/225000 (84%)] Loss: 8054.335938\n",
      "Train Epoch: 75 [192768/225000 (86%)] Loss: 8010.951172\n",
      "Train Epoch: 75 [196864/225000 (87%)] Loss: 8035.572266\n",
      "Train Epoch: 75 [200960/225000 (89%)] Loss: 12355.759766\n",
      "Train Epoch: 75 [205056/225000 (91%)] Loss: 8085.476562\n",
      "Train Epoch: 75 [209152/225000 (93%)] Loss: 7956.007812\n",
      "Train Epoch: 75 [213248/225000 (95%)] Loss: 8170.687500\n",
      "Train Epoch: 75 [217344/225000 (97%)] Loss: 9648.429688\n",
      "Train Epoch: 75 [221440/225000 (98%)] Loss: 8131.248047\n",
      "    epoch          : 75\n",
      "    loss           : 9560.79791600007\n",
      "    val_loss       : 9694.55536211875\n",
      "Train Epoch: 76 [256/225000 (0%)] Loss: 8229.705078\n",
      "Train Epoch: 76 [4352/225000 (2%)] Loss: 8107.699219\n",
      "Train Epoch: 76 [8448/225000 (4%)] Loss: 8088.316406\n",
      "Train Epoch: 76 [12544/225000 (6%)] Loss: 8119.078125\n",
      "Train Epoch: 76 [16640/225000 (7%)] Loss: 8000.994141\n",
      "Train Epoch: 76 [20736/225000 (9%)] Loss: 7994.304688\n",
      "Train Epoch: 76 [24832/225000 (11%)] Loss: 8211.085938\n",
      "Train Epoch: 76 [28928/225000 (13%)] Loss: 10464.451172\n",
      "Train Epoch: 76 [33024/225000 (15%)] Loss: 8179.886719\n",
      "Train Epoch: 76 [37120/225000 (16%)] Loss: 10533.365234\n",
      "Train Epoch: 76 [41216/225000 (18%)] Loss: 8127.191406\n",
      "Train Epoch: 76 [45312/225000 (20%)] Loss: 8172.906250\n",
      "Train Epoch: 76 [49408/225000 (22%)] Loss: 8127.103516\n",
      "Train Epoch: 76 [53504/225000 (24%)] Loss: 7894.761719\n",
      "Train Epoch: 76 [57600/225000 (26%)] Loss: 8238.625000\n",
      "Train Epoch: 76 [61696/225000 (27%)] Loss: 7996.236328\n",
      "Train Epoch: 76 [65792/225000 (29%)] Loss: 8048.423828\n",
      "Train Epoch: 76 [69888/225000 (31%)] Loss: 8068.830078\n",
      "Train Epoch: 76 [73984/225000 (33%)] Loss: 8109.566406\n",
      "Train Epoch: 76 [78080/225000 (35%)] Loss: 8161.253906\n",
      "Train Epoch: 76 [82176/225000 (37%)] Loss: 13037.185547\n",
      "Train Epoch: 76 [86272/225000 (38%)] Loss: 7930.050781\n",
      "Train Epoch: 76 [90368/225000 (40%)] Loss: 9467.632812\n",
      "Train Epoch: 76 [94464/225000 (42%)] Loss: 8066.533203\n",
      "Train Epoch: 76 [98560/225000 (44%)] Loss: 10527.904297\n",
      "Train Epoch: 76 [102656/225000 (46%)] Loss: 8187.904297\n",
      "Train Epoch: 76 [106752/225000 (47%)] Loss: 22034.333984\n",
      "Train Epoch: 76 [110848/225000 (49%)] Loss: 9568.339844\n",
      "Train Epoch: 76 [114944/225000 (51%)] Loss: 8178.080078\n",
      "Train Epoch: 76 [119040/225000 (53%)] Loss: 8113.724609\n",
      "Train Epoch: 76 [123136/225000 (55%)] Loss: 10589.578125\n",
      "Train Epoch: 76 [127232/225000 (57%)] Loss: 8099.464844\n",
      "Train Epoch: 76 [131328/225000 (58%)] Loss: 8119.994141\n",
      "Train Epoch: 76 [135424/225000 (60%)] Loss: 7923.804688\n",
      "Train Epoch: 76 [139520/225000 (62%)] Loss: 12356.505859\n",
      "Train Epoch: 76 [143616/225000 (64%)] Loss: 13778.642578\n",
      "Train Epoch: 76 [147712/225000 (66%)] Loss: 8058.871094\n",
      "Train Epoch: 76 [151808/225000 (67%)] Loss: 8054.083984\n",
      "Train Epoch: 76 [155904/225000 (69%)] Loss: 7974.986328\n",
      "Train Epoch: 76 [160000/225000 (71%)] Loss: 8174.044922\n",
      "Train Epoch: 76 [164096/225000 (73%)] Loss: 10597.890625\n",
      "Train Epoch: 76 [168192/225000 (75%)] Loss: 8197.267578\n",
      "Train Epoch: 76 [172288/225000 (77%)] Loss: 8186.015625\n",
      "Train Epoch: 76 [176384/225000 (78%)] Loss: 10573.738281\n",
      "Train Epoch: 76 [180480/225000 (80%)] Loss: 8076.332031\n",
      "Train Epoch: 76 [184576/225000 (82%)] Loss: 8154.558594\n",
      "Train Epoch: 76 [188672/225000 (84%)] Loss: 9542.398438\n",
      "Train Epoch: 76 [192768/225000 (86%)] Loss: 8034.984375\n",
      "Train Epoch: 76 [196864/225000 (87%)] Loss: 7867.800781\n",
      "Train Epoch: 76 [200960/225000 (89%)] Loss: 7963.123047\n",
      "Train Epoch: 76 [205056/225000 (91%)] Loss: 8053.753906\n",
      "Train Epoch: 76 [209152/225000 (93%)] Loss: 9652.416016\n",
      "Train Epoch: 76 [213248/225000 (95%)] Loss: 7923.408203\n",
      "Train Epoch: 76 [217344/225000 (97%)] Loss: 8160.621094\n",
      "Train Epoch: 76 [221440/225000 (98%)] Loss: 7737.400391\n",
      "    epoch          : 76\n",
      "    loss           : 9553.530107899602\n",
      "    val_loss       : 9645.04407499761\n",
      "Train Epoch: 77 [256/225000 (0%)] Loss: 10643.511719\n",
      "Train Epoch: 77 [4352/225000 (2%)] Loss: 8117.541016\n",
      "Train Epoch: 77 [8448/225000 (4%)] Loss: 7942.160156\n",
      "Train Epoch: 77 [12544/225000 (6%)] Loss: 7954.472656\n",
      "Train Epoch: 77 [16640/225000 (7%)] Loss: 7919.820312\n",
      "Train Epoch: 77 [20736/225000 (9%)] Loss: 8182.517578\n",
      "Train Epoch: 77 [24832/225000 (11%)] Loss: 9684.642578\n",
      "Train Epoch: 77 [28928/225000 (13%)] Loss: 7986.656250\n",
      "Train Epoch: 77 [33024/225000 (15%)] Loss: 8168.650391\n",
      "Train Epoch: 77 [37120/225000 (16%)] Loss: 7956.125000\n",
      "Train Epoch: 77 [41216/225000 (18%)] Loss: 7992.521484\n",
      "Train Epoch: 77 [45312/225000 (20%)] Loss: 12269.285156\n",
      "Train Epoch: 77 [49408/225000 (22%)] Loss: 10425.066406\n",
      "Train Epoch: 77 [53504/225000 (24%)] Loss: 13833.464844\n",
      "Train Epoch: 77 [57600/225000 (26%)] Loss: 12229.220703\n",
      "Train Epoch: 77 [61696/225000 (27%)] Loss: 8038.585938\n",
      "Train Epoch: 77 [65792/225000 (29%)] Loss: 8070.138672\n",
      "Train Epoch: 77 [69888/225000 (31%)] Loss: 8017.539062\n",
      "Train Epoch: 77 [73984/225000 (33%)] Loss: 7923.931641\n",
      "Train Epoch: 77 [78080/225000 (35%)] Loss: 13799.304688\n",
      "Train Epoch: 77 [82176/225000 (37%)] Loss: 10513.953125\n",
      "Train Epoch: 77 [86272/225000 (38%)] Loss: 8237.357422\n",
      "Train Epoch: 77 [90368/225000 (40%)] Loss: 8138.822266\n",
      "Train Epoch: 77 [94464/225000 (42%)] Loss: 10615.556641\n",
      "Train Epoch: 77 [98560/225000 (44%)] Loss: 8067.236328\n",
      "Train Epoch: 77 [102656/225000 (46%)] Loss: 8052.451172\n",
      "Train Epoch: 77 [106752/225000 (47%)] Loss: 9897.078125\n",
      "Train Epoch: 77 [110848/225000 (49%)] Loss: 10733.761719\n",
      "Train Epoch: 77 [114944/225000 (51%)] Loss: 7990.468750\n",
      "Train Epoch: 77 [119040/225000 (53%)] Loss: 7967.785156\n",
      "Train Epoch: 77 [123136/225000 (55%)] Loss: 12167.718750\n",
      "Train Epoch: 77 [127232/225000 (57%)] Loss: 10697.511719\n",
      "Train Epoch: 77 [131328/225000 (58%)] Loss: 16069.517578\n",
      "Train Epoch: 77 [135424/225000 (60%)] Loss: 7989.156250\n",
      "Train Epoch: 77 [139520/225000 (62%)] Loss: 10331.210938\n",
      "Train Epoch: 77 [143616/225000 (64%)] Loss: 7908.835938\n",
      "Train Epoch: 77 [147712/225000 (66%)] Loss: 8115.220703\n",
      "Train Epoch: 77 [151808/225000 (67%)] Loss: 13533.941406\n",
      "Train Epoch: 77 [155904/225000 (69%)] Loss: 8010.628906\n",
      "Train Epoch: 77 [160000/225000 (71%)] Loss: 7982.455078\n",
      "Train Epoch: 77 [164096/225000 (73%)] Loss: 8077.695312\n",
      "Train Epoch: 77 [168192/225000 (75%)] Loss: 10466.019531\n",
      "Train Epoch: 77 [172288/225000 (77%)] Loss: 8145.398438\n",
      "Train Epoch: 77 [176384/225000 (78%)] Loss: 13348.041016\n",
      "Train Epoch: 77 [180480/225000 (80%)] Loss: 13926.296875\n",
      "Train Epoch: 77 [184576/225000 (82%)] Loss: 8196.013672\n",
      "Train Epoch: 77 [188672/225000 (84%)] Loss: 9633.322266\n",
      "Train Epoch: 77 [192768/225000 (86%)] Loss: 9781.501953\n",
      "Train Epoch: 77 [196864/225000 (87%)] Loss: 9487.320312\n",
      "Train Epoch: 77 [200960/225000 (89%)] Loss: 7963.005859\n",
      "Train Epoch: 77 [205056/225000 (91%)] Loss: 7859.544922\n",
      "Train Epoch: 77 [209152/225000 (93%)] Loss: 12069.203125\n",
      "Train Epoch: 77 [213248/225000 (95%)] Loss: 8376.199219\n",
      "Train Epoch: 77 [217344/225000 (97%)] Loss: 7922.515625\n",
      "Train Epoch: 77 [221440/225000 (98%)] Loss: 8090.357422\n",
      "    epoch          : 77\n",
      "    loss           : 9590.017263714093\n",
      "    val_loss       : 9553.31552656816\n",
      "Train Epoch: 78 [256/225000 (0%)] Loss: 9529.648438\n",
      "Train Epoch: 78 [4352/225000 (2%)] Loss: 8001.767578\n",
      "Train Epoch: 78 [8448/225000 (4%)] Loss: 10589.517578\n",
      "Train Epoch: 78 [12544/225000 (6%)] Loss: 10417.912109\n",
      "Train Epoch: 78 [16640/225000 (7%)] Loss: 10545.652344\n",
      "Train Epoch: 78 [20736/225000 (9%)] Loss: 8039.646484\n",
      "Train Epoch: 78 [24832/225000 (11%)] Loss: 7943.710938\n",
      "Train Epoch: 78 [28928/225000 (13%)] Loss: 7969.267578\n",
      "Train Epoch: 78 [33024/225000 (15%)] Loss: 7935.958984\n",
      "Train Epoch: 78 [37120/225000 (16%)] Loss: 7980.146484\n",
      "Train Epoch: 78 [41216/225000 (18%)] Loss: 8180.238281\n",
      "Train Epoch: 78 [45312/225000 (20%)] Loss: 10487.484375\n",
      "Train Epoch: 78 [49408/225000 (22%)] Loss: 8041.425781\n",
      "Train Epoch: 78 [53504/225000 (24%)] Loss: 10525.515625\n",
      "Train Epoch: 78 [57600/225000 (26%)] Loss: 7912.345703\n",
      "Train Epoch: 78 [61696/225000 (27%)] Loss: 10408.097656\n",
      "Train Epoch: 78 [65792/225000 (29%)] Loss: 10341.119141\n",
      "Train Epoch: 78 [69888/225000 (31%)] Loss: 8279.673828\n",
      "Train Epoch: 78 [73984/225000 (33%)] Loss: 14077.623047\n",
      "Train Epoch: 78 [78080/225000 (35%)] Loss: 8043.968750\n",
      "Train Epoch: 78 [82176/225000 (37%)] Loss: 7992.935547\n",
      "Train Epoch: 78 [86272/225000 (38%)] Loss: 12800.615234\n",
      "Train Epoch: 78 [90368/225000 (40%)] Loss: 8341.833984\n",
      "Train Epoch: 78 [94464/225000 (42%)] Loss: 8115.322266\n",
      "Train Epoch: 78 [98560/225000 (44%)] Loss: 7796.390625\n",
      "Train Epoch: 78 [102656/225000 (46%)] Loss: 10374.683594\n",
      "Train Epoch: 78 [106752/225000 (47%)] Loss: 12425.480469\n",
      "Train Epoch: 78 [110848/225000 (49%)] Loss: 14786.751953\n",
      "Train Epoch: 78 [114944/225000 (51%)] Loss: 8079.062500\n",
      "Train Epoch: 78 [119040/225000 (53%)] Loss: 8014.302734\n",
      "Train Epoch: 78 [123136/225000 (55%)] Loss: 8190.351562\n",
      "Train Epoch: 78 [127232/225000 (57%)] Loss: 12293.113281\n",
      "Train Epoch: 78 [131328/225000 (58%)] Loss: 15998.458984\n",
      "Train Epoch: 78 [135424/225000 (60%)] Loss: 7979.939453\n",
      "Train Epoch: 78 [139520/225000 (62%)] Loss: 13710.681641\n",
      "Train Epoch: 78 [143616/225000 (64%)] Loss: 7967.933594\n",
      "Train Epoch: 78 [147712/225000 (66%)] Loss: 7982.351562\n",
      "Train Epoch: 78 [151808/225000 (67%)] Loss: 9759.166016\n",
      "Train Epoch: 78 [155904/225000 (69%)] Loss: 8175.259766\n",
      "Train Epoch: 78 [160000/225000 (71%)] Loss: 7945.529297\n",
      "Train Epoch: 78 [164096/225000 (73%)] Loss: 13695.978516\n",
      "Train Epoch: 78 [168192/225000 (75%)] Loss: 8068.765625\n",
      "Train Epoch: 78 [172288/225000 (77%)] Loss: 7913.650391\n",
      "Train Epoch: 78 [176384/225000 (78%)] Loss: 8015.289062\n",
      "Train Epoch: 78 [180480/225000 (80%)] Loss: 14055.513672\n",
      "Train Epoch: 78 [184576/225000 (82%)] Loss: 12464.310547\n",
      "Train Epoch: 78 [188672/225000 (84%)] Loss: 18063.337891\n",
      "Train Epoch: 78 [192768/225000 (86%)] Loss: 7929.882812\n",
      "Train Epoch: 78 [196864/225000 (87%)] Loss: 8046.300781\n",
      "Train Epoch: 78 [200960/225000 (89%)] Loss: 10590.185547\n",
      "Train Epoch: 78 [205056/225000 (91%)] Loss: 7989.492188\n",
      "Train Epoch: 78 [209152/225000 (93%)] Loss: 8023.123047\n",
      "Train Epoch: 78 [213248/225000 (95%)] Loss: 7985.630859\n",
      "Train Epoch: 78 [217344/225000 (97%)] Loss: 9689.449219\n",
      "Train Epoch: 78 [221440/225000 (98%)] Loss: 8043.029297\n",
      "    epoch          : 78\n",
      "    loss           : 9731.708675519056\n",
      "    val_loss       : 9288.898429722201\n",
      "Train Epoch: 79 [256/225000 (0%)] Loss: 13579.910156\n",
      "Train Epoch: 79 [4352/225000 (2%)] Loss: 11155.830078\n",
      "Train Epoch: 79 [8448/225000 (4%)] Loss: 9558.054688\n",
      "Train Epoch: 79 [12544/225000 (6%)] Loss: 8007.658203\n",
      "Train Epoch: 79 [16640/225000 (7%)] Loss: 10455.345703\n",
      "Train Epoch: 79 [20736/225000 (9%)] Loss: 8061.458984\n",
      "Train Epoch: 79 [24832/225000 (11%)] Loss: 8090.798828\n",
      "Train Epoch: 79 [28928/225000 (13%)] Loss: 9656.871094\n",
      "Train Epoch: 79 [33024/225000 (15%)] Loss: 8077.246094\n",
      "Train Epoch: 79 [37120/225000 (16%)] Loss: 18733.947266\n",
      "Train Epoch: 79 [41216/225000 (18%)] Loss: 9691.751953\n",
      "Train Epoch: 79 [45312/225000 (20%)] Loss: 8204.562500\n",
      "Train Epoch: 79 [49408/225000 (22%)] Loss: 8318.521484\n",
      "Train Epoch: 79 [53504/225000 (24%)] Loss: 14074.998047\n",
      "Train Epoch: 79 [57600/225000 (26%)] Loss: 8157.123047\n",
      "Train Epoch: 79 [61696/225000 (27%)] Loss: 8063.156250\n",
      "Train Epoch: 79 [65792/225000 (29%)] Loss: 8346.753906\n",
      "Train Epoch: 79 [69888/225000 (31%)] Loss: 8131.660156\n",
      "Train Epoch: 79 [73984/225000 (33%)] Loss: 8207.289062\n",
      "Train Epoch: 79 [78080/225000 (35%)] Loss: 9610.894531\n",
      "Train Epoch: 79 [82176/225000 (37%)] Loss: 8130.734375\n",
      "Train Epoch: 79 [86272/225000 (38%)] Loss: 10660.023438\n",
      "Train Epoch: 79 [90368/225000 (40%)] Loss: 8136.515625\n",
      "Train Epoch: 79 [94464/225000 (42%)] Loss: 14634.593750\n",
      "Train Epoch: 79 [98560/225000 (44%)] Loss: 8099.894531\n",
      "Train Epoch: 79 [102656/225000 (46%)] Loss: 16119.351562\n",
      "Train Epoch: 79 [106752/225000 (47%)] Loss: 9685.816406\n",
      "Train Epoch: 79 [110848/225000 (49%)] Loss: 8154.939453\n",
      "Train Epoch: 79 [114944/225000 (51%)] Loss: 8024.089844\n",
      "Train Epoch: 79 [119040/225000 (53%)] Loss: 8223.722656\n",
      "Train Epoch: 79 [123136/225000 (55%)] Loss: 7985.927734\n",
      "Train Epoch: 79 [127232/225000 (57%)] Loss: 7995.556641\n",
      "Train Epoch: 79 [131328/225000 (58%)] Loss: 11996.998047\n",
      "Train Epoch: 79 [135424/225000 (60%)] Loss: 7858.994141\n",
      "Train Epoch: 79 [139520/225000 (62%)] Loss: 8087.154297\n",
      "Train Epoch: 79 [143616/225000 (64%)] Loss: 7964.572266\n",
      "Train Epoch: 79 [147712/225000 (66%)] Loss: 8135.898438\n",
      "Train Epoch: 79 [151808/225000 (67%)] Loss: 8090.916016\n",
      "Train Epoch: 79 [155904/225000 (69%)] Loss: 12284.330078\n",
      "Train Epoch: 79 [160000/225000 (71%)] Loss: 8128.404297\n",
      "Train Epoch: 79 [164096/225000 (73%)] Loss: 9794.871094\n",
      "Train Epoch: 79 [168192/225000 (75%)] Loss: 10522.777344\n",
      "Train Epoch: 79 [172288/225000 (77%)] Loss: 16360.638672\n",
      "Train Epoch: 79 [176384/225000 (78%)] Loss: 7977.355469\n",
      "Train Epoch: 79 [180480/225000 (80%)] Loss: 8067.779297\n",
      "Train Epoch: 79 [184576/225000 (82%)] Loss: 8124.572266\n",
      "Train Epoch: 79 [188672/225000 (84%)] Loss: 10517.058594\n",
      "Train Epoch: 79 [192768/225000 (86%)] Loss: 13794.828125\n",
      "Train Epoch: 79 [196864/225000 (87%)] Loss: 7823.072266\n",
      "Train Epoch: 79 [200960/225000 (89%)] Loss: 10609.412109\n",
      "Train Epoch: 79 [205056/225000 (91%)] Loss: 9554.646484\n",
      "Train Epoch: 79 [209152/225000 (93%)] Loss: 8089.529297\n",
      "Train Epoch: 79 [213248/225000 (95%)] Loss: 8060.523438\n",
      "Train Epoch: 79 [217344/225000 (97%)] Loss: 7957.121094\n",
      "Train Epoch: 79 [221440/225000 (98%)] Loss: 9751.080078\n",
      "    epoch          : 79\n",
      "    loss           : 9635.818731557523\n",
      "    val_loss       : 9306.762864924207\n",
      "Train Epoch: 80 [256/225000 (0%)] Loss: 10589.519531\n",
      "Train Epoch: 80 [4352/225000 (2%)] Loss: 9808.748047\n",
      "Train Epoch: 80 [8448/225000 (4%)] Loss: 8171.759766\n",
      "Train Epoch: 80 [12544/225000 (6%)] Loss: 8109.291016\n",
      "Train Epoch: 80 [16640/225000 (7%)] Loss: 8057.179688\n",
      "Train Epoch: 80 [20736/225000 (9%)] Loss: 8156.796875\n",
      "Train Epoch: 80 [24832/225000 (11%)] Loss: 19989.751953\n",
      "Train Epoch: 80 [28928/225000 (13%)] Loss: 8128.632812\n",
      "Train Epoch: 80 [33024/225000 (15%)] Loss: 12725.681641\n",
      "Train Epoch: 80 [37120/225000 (16%)] Loss: 8155.708984\n",
      "Train Epoch: 80 [41216/225000 (18%)] Loss: 9510.041016\n",
      "Train Epoch: 80 [45312/225000 (20%)] Loss: 8175.162109\n",
      "Train Epoch: 80 [49408/225000 (22%)] Loss: 15336.015625\n",
      "Train Epoch: 80 [53504/225000 (24%)] Loss: 7986.308594\n",
      "Train Epoch: 80 [57600/225000 (26%)] Loss: 9549.007812\n",
      "Train Epoch: 80 [61696/225000 (27%)] Loss: 8424.070312\n",
      "Train Epoch: 80 [65792/225000 (29%)] Loss: 9641.777344\n",
      "Train Epoch: 80 [69888/225000 (31%)] Loss: 7968.193359\n",
      "Train Epoch: 80 [73984/225000 (33%)] Loss: 16299.613281\n",
      "Train Epoch: 80 [78080/225000 (35%)] Loss: 7975.009766\n",
      "Train Epoch: 80 [82176/225000 (37%)] Loss: 7931.962891\n",
      "Train Epoch: 80 [86272/225000 (38%)] Loss: 10532.542969\n",
      "Train Epoch: 80 [90368/225000 (40%)] Loss: 9454.310547\n",
      "Train Epoch: 80 [94464/225000 (42%)] Loss: 7927.556641\n",
      "Train Epoch: 80 [98560/225000 (44%)] Loss: 8226.132812\n",
      "Train Epoch: 80 [102656/225000 (46%)] Loss: 12863.136719\n",
      "Train Epoch: 80 [106752/225000 (47%)] Loss: 10612.464844\n",
      "Train Epoch: 80 [110848/225000 (49%)] Loss: 9535.605469\n",
      "Train Epoch: 80 [114944/225000 (51%)] Loss: 8129.433594\n",
      "Train Epoch: 80 [119040/225000 (53%)] Loss: 10456.353516\n",
      "Train Epoch: 80 [123136/225000 (55%)] Loss: 8175.583984\n",
      "Train Epoch: 80 [127232/225000 (57%)] Loss: 7938.583984\n",
      "Train Epoch: 80 [131328/225000 (58%)] Loss: 8005.992188\n",
      "Train Epoch: 80 [135424/225000 (60%)] Loss: 8082.181641\n",
      "Train Epoch: 80 [139520/225000 (62%)] Loss: 8230.189453\n",
      "Train Epoch: 80 [143616/225000 (64%)] Loss: 8114.246094\n",
      "Train Epoch: 80 [147712/225000 (66%)] Loss: 9898.462891\n",
      "Train Epoch: 80 [151808/225000 (67%)] Loss: 16348.621094\n",
      "Train Epoch: 80 [155904/225000 (69%)] Loss: 7847.419922\n",
      "Train Epoch: 80 [160000/225000 (71%)] Loss: 13885.119141\n",
      "Train Epoch: 80 [164096/225000 (73%)] Loss: 7897.736328\n",
      "Train Epoch: 80 [168192/225000 (75%)] Loss: 12715.294922\n",
      "Train Epoch: 80 [172288/225000 (77%)] Loss: 8072.107422\n",
      "Train Epoch: 80 [176384/225000 (78%)] Loss: 10619.005859\n",
      "Train Epoch: 80 [180480/225000 (80%)] Loss: 8091.634766\n",
      "Train Epoch: 80 [184576/225000 (82%)] Loss: 7951.232422\n",
      "Train Epoch: 80 [188672/225000 (84%)] Loss: 10378.078125\n",
      "Train Epoch: 80 [192768/225000 (86%)] Loss: 8063.539062\n",
      "Train Epoch: 80 [196864/225000 (87%)] Loss: 10578.955078\n",
      "Train Epoch: 80 [200960/225000 (89%)] Loss: 8103.242188\n",
      "Train Epoch: 80 [205056/225000 (91%)] Loss: 9639.357422\n",
      "Train Epoch: 80 [209152/225000 (93%)] Loss: 9729.498047\n",
      "Train Epoch: 80 [213248/225000 (95%)] Loss: 8358.951172\n",
      "Train Epoch: 80 [217344/225000 (97%)] Loss: 7983.859375\n",
      "Train Epoch: 80 [221440/225000 (98%)] Loss: 7866.251953\n",
      "    epoch          : 80\n",
      "    loss           : 9602.145837777303\n",
      "    val_loss       : 9530.091327676968\n",
      "Train Epoch: 81 [256/225000 (0%)] Loss: 8065.000000\n",
      "Train Epoch: 81 [4352/225000 (2%)] Loss: 8138.324219\n",
      "Train Epoch: 81 [8448/225000 (4%)] Loss: 8121.636719\n",
      "Train Epoch: 81 [12544/225000 (6%)] Loss: 7995.597656\n",
      "Train Epoch: 81 [16640/225000 (7%)] Loss: 9592.000000\n",
      "Train Epoch: 81 [20736/225000 (9%)] Loss: 8055.949219\n",
      "Train Epoch: 81 [24832/225000 (11%)] Loss: 9498.810547\n",
      "Train Epoch: 81 [28928/225000 (13%)] Loss: 12077.546875\n",
      "Train Epoch: 81 [33024/225000 (15%)] Loss: 8048.947266\n",
      "Train Epoch: 81 [37120/225000 (16%)] Loss: 7925.523438\n",
      "Train Epoch: 81 [41216/225000 (18%)] Loss: 13712.156250\n",
      "Train Epoch: 81 [45312/225000 (20%)] Loss: 8279.218750\n",
      "Train Epoch: 81 [49408/225000 (22%)] Loss: 10510.730469\n",
      "Train Epoch: 81 [53504/225000 (24%)] Loss: 7915.865234\n",
      "Train Epoch: 81 [57600/225000 (26%)] Loss: 18148.642578\n",
      "Train Epoch: 81 [61696/225000 (27%)] Loss: 9443.378906\n",
      "Train Epoch: 81 [65792/225000 (29%)] Loss: 11687.812500\n",
      "Train Epoch: 81 [69888/225000 (31%)] Loss: 7965.330078\n",
      "Train Epoch: 81 [73984/225000 (33%)] Loss: 10563.128906\n",
      "Train Epoch: 81 [78080/225000 (35%)] Loss: 7990.843750\n",
      "Train Epoch: 81 [82176/225000 (37%)] Loss: 8049.175781\n",
      "Train Epoch: 81 [86272/225000 (38%)] Loss: 8109.607422\n",
      "Train Epoch: 81 [90368/225000 (40%)] Loss: 8029.423828\n",
      "Train Epoch: 81 [94464/225000 (42%)] Loss: 9616.302734\n",
      "Train Epoch: 81 [98560/225000 (44%)] Loss: 7957.429688\n",
      "Train Epoch: 81 [102656/225000 (46%)] Loss: 13165.667969\n",
      "Train Epoch: 81 [106752/225000 (47%)] Loss: 9709.656250\n",
      "Train Epoch: 81 [110848/225000 (49%)] Loss: 8191.882812\n",
      "Train Epoch: 81 [114944/225000 (51%)] Loss: 10616.396484\n",
      "Train Epoch: 81 [119040/225000 (53%)] Loss: 7822.816406\n",
      "Train Epoch: 81 [123136/225000 (55%)] Loss: 8092.144531\n",
      "Train Epoch: 81 [127232/225000 (57%)] Loss: 12218.572266\n",
      "Train Epoch: 81 [131328/225000 (58%)] Loss: 7908.974609\n",
      "Train Epoch: 81 [135424/225000 (60%)] Loss: 8122.531250\n",
      "Train Epoch: 81 [139520/225000 (62%)] Loss: 12317.830078\n",
      "Train Epoch: 81 [143616/225000 (64%)] Loss: 8361.783203\n",
      "Train Epoch: 81 [147712/225000 (66%)] Loss: 14052.988281\n",
      "Train Epoch: 81 [151808/225000 (67%)] Loss: 10580.451172\n",
      "Train Epoch: 81 [155904/225000 (69%)] Loss: 13257.652344\n",
      "Train Epoch: 81 [160000/225000 (71%)] Loss: 8112.058594\n",
      "Train Epoch: 81 [164096/225000 (73%)] Loss: 7974.595703\n",
      "Train Epoch: 81 [168192/225000 (75%)] Loss: 8117.728516\n",
      "Train Epoch: 81 [172288/225000 (77%)] Loss: 8313.847656\n",
      "Train Epoch: 81 [176384/225000 (78%)] Loss: 10715.269531\n",
      "Train Epoch: 81 [180480/225000 (80%)] Loss: 8126.412109\n",
      "Train Epoch: 81 [184576/225000 (82%)] Loss: 9611.197266\n",
      "Train Epoch: 81 [188672/225000 (84%)] Loss: 7908.599609\n",
      "Train Epoch: 81 [192768/225000 (86%)] Loss: 8212.349609\n",
      "Train Epoch: 81 [196864/225000 (87%)] Loss: 8081.001953\n",
      "Train Epoch: 81 [200960/225000 (89%)] Loss: 8174.796875\n",
      "Train Epoch: 81 [205056/225000 (91%)] Loss: 8058.583984\n",
      "Train Epoch: 81 [209152/225000 (93%)] Loss: 12464.855469\n",
      "Train Epoch: 81 [213248/225000 (95%)] Loss: 8171.253906\n",
      "Train Epoch: 81 [217344/225000 (97%)] Loss: 14059.574219\n",
      "Train Epoch: 81 [221440/225000 (98%)] Loss: 7908.458984\n",
      "    epoch          : 81\n",
      "    loss           : 9537.860625977673\n",
      "    val_loss       : 9365.897462696445\n",
      "Train Epoch: 82 [256/225000 (0%)] Loss: 7994.943359\n",
      "Train Epoch: 82 [4352/225000 (2%)] Loss: 8232.179688\n",
      "Train Epoch: 82 [8448/225000 (4%)] Loss: 8089.753906\n",
      "Train Epoch: 82 [12544/225000 (6%)] Loss: 11072.291016\n",
      "Train Epoch: 82 [16640/225000 (7%)] Loss: 8089.212891\n",
      "Train Epoch: 82 [20736/225000 (9%)] Loss: 7934.222656\n",
      "Train Epoch: 82 [24832/225000 (11%)] Loss: 8292.878906\n",
      "Train Epoch: 82 [28928/225000 (13%)] Loss: 8122.185547\n",
      "Train Epoch: 82 [33024/225000 (15%)] Loss: 8059.175781\n",
      "Train Epoch: 82 [37120/225000 (16%)] Loss: 12223.185547\n",
      "Train Epoch: 82 [41216/225000 (18%)] Loss: 8077.054688\n",
      "Train Epoch: 82 [45312/225000 (20%)] Loss: 7859.101562\n",
      "Train Epoch: 82 [49408/225000 (22%)] Loss: 8112.976562\n",
      "Train Epoch: 82 [53504/225000 (24%)] Loss: 10570.962891\n",
      "Train Epoch: 82 [57600/225000 (26%)] Loss: 7928.126953\n",
      "Train Epoch: 82 [61696/225000 (27%)] Loss: 7964.412109\n",
      "Train Epoch: 82 [65792/225000 (29%)] Loss: 8289.423828\n",
      "Train Epoch: 82 [69888/225000 (31%)] Loss: 7842.724609\n",
      "Train Epoch: 82 [73984/225000 (33%)] Loss: 7959.660156\n",
      "Train Epoch: 82 [78080/225000 (35%)] Loss: 10400.894531\n",
      "Train Epoch: 82 [82176/225000 (37%)] Loss: 15178.503906\n",
      "Train Epoch: 82 [86272/225000 (38%)] Loss: 8011.486328\n",
      "Train Epoch: 82 [90368/225000 (40%)] Loss: 8066.285156\n",
      "Train Epoch: 82 [94464/225000 (42%)] Loss: 13623.888672\n",
      "Train Epoch: 82 [98560/225000 (44%)] Loss: 8010.246094\n",
      "Train Epoch: 82 [102656/225000 (46%)] Loss: 8018.486328\n",
      "Train Epoch: 82 [106752/225000 (47%)] Loss: 10224.699219\n",
      "Train Epoch: 82 [110848/225000 (49%)] Loss: 8347.173828\n",
      "Train Epoch: 82 [114944/225000 (51%)] Loss: 8245.630859\n",
      "Train Epoch: 82 [119040/225000 (53%)] Loss: 8252.478516\n",
      "Train Epoch: 82 [123136/225000 (55%)] Loss: 10431.201172\n",
      "Train Epoch: 82 [127232/225000 (57%)] Loss: 16235.156250\n",
      "Train Epoch: 82 [131328/225000 (58%)] Loss: 7811.972656\n",
      "Train Epoch: 82 [135424/225000 (60%)] Loss: 8036.904297\n",
      "Train Epoch: 82 [139520/225000 (62%)] Loss: 9516.371094\n",
      "Train Epoch: 82 [143616/225000 (64%)] Loss: 13683.447266\n",
      "Train Epoch: 82 [147712/225000 (66%)] Loss: 7913.876953\n",
      "Train Epoch: 82 [151808/225000 (67%)] Loss: 9678.218750\n",
      "Train Epoch: 82 [155904/225000 (69%)] Loss: 7959.611328\n",
      "Train Epoch: 82 [160000/225000 (71%)] Loss: 8107.285156\n",
      "Train Epoch: 82 [164096/225000 (73%)] Loss: 9565.531250\n",
      "Train Epoch: 82 [168192/225000 (75%)] Loss: 10627.210938\n",
      "Train Epoch: 82 [172288/225000 (77%)] Loss: 8001.007812\n",
      "Train Epoch: 82 [176384/225000 (78%)] Loss: 13683.634766\n",
      "Train Epoch: 82 [180480/225000 (80%)] Loss: 8021.404297\n",
      "Train Epoch: 82 [184576/225000 (82%)] Loss: 8034.103516\n",
      "Train Epoch: 82 [188672/225000 (84%)] Loss: 8205.333984\n",
      "Train Epoch: 82 [192768/225000 (86%)] Loss: 12244.441406\n",
      "Train Epoch: 82 [196864/225000 (87%)] Loss: 8127.791016\n",
      "Train Epoch: 82 [200960/225000 (89%)] Loss: 8098.091797\n",
      "Train Epoch: 82 [205056/225000 (91%)] Loss: 8019.320312\n",
      "Train Epoch: 82 [209152/225000 (93%)] Loss: 7928.773438\n",
      "Train Epoch: 82 [213248/225000 (95%)] Loss: 8188.177734\n",
      "Train Epoch: 82 [217344/225000 (97%)] Loss: 10515.554688\n",
      "Train Epoch: 82 [221440/225000 (98%)] Loss: 12665.570312\n",
      "    epoch          : 82\n",
      "    loss           : 9613.331614627774\n",
      "    val_loss       : 9320.426132844419\n",
      "Train Epoch: 83 [256/225000 (0%)] Loss: 8148.898438\n",
      "Train Epoch: 83 [4352/225000 (2%)] Loss: 8206.964844\n",
      "Train Epoch: 83 [8448/225000 (4%)] Loss: 8128.294922\n",
      "Train Epoch: 83 [12544/225000 (6%)] Loss: 9543.166016\n",
      "Train Epoch: 83 [16640/225000 (7%)] Loss: 10416.990234\n",
      "Train Epoch: 83 [20736/225000 (9%)] Loss: 8199.253906\n",
      "Train Epoch: 83 [24832/225000 (11%)] Loss: 12240.898438\n",
      "Train Epoch: 83 [28928/225000 (13%)] Loss: 8195.880859\n",
      "Train Epoch: 83 [33024/225000 (15%)] Loss: 8106.898438\n",
      "Train Epoch: 83 [37120/225000 (16%)] Loss: 9742.353516\n",
      "Train Epoch: 83 [41216/225000 (18%)] Loss: 9693.015625\n",
      "Train Epoch: 83 [45312/225000 (20%)] Loss: 12322.708984\n",
      "Train Epoch: 83 [49408/225000 (22%)] Loss: 8117.148438\n",
      "Train Epoch: 83 [53504/225000 (24%)] Loss: 15043.144531\n",
      "Train Epoch: 83 [57600/225000 (26%)] Loss: 12219.578125\n",
      "Train Epoch: 83 [61696/225000 (27%)] Loss: 10645.751953\n",
      "Train Epoch: 83 [65792/225000 (29%)] Loss: 8019.021484\n",
      "Train Epoch: 83 [69888/225000 (31%)] Loss: 10476.355469\n",
      "Train Epoch: 83 [73984/225000 (33%)] Loss: 10535.255859\n",
      "Train Epoch: 83 [78080/225000 (35%)] Loss: 14822.996094\n",
      "Train Epoch: 83 [82176/225000 (37%)] Loss: 14579.626953\n",
      "Train Epoch: 83 [86272/225000 (38%)] Loss: 10460.337891\n",
      "Train Epoch: 83 [90368/225000 (40%)] Loss: 8144.251953\n",
      "Train Epoch: 83 [94464/225000 (42%)] Loss: 9664.082031\n",
      "Train Epoch: 83 [98560/225000 (44%)] Loss: 10601.449219\n",
      "Train Epoch: 83 [102656/225000 (46%)] Loss: 12182.964844\n",
      "Train Epoch: 83 [106752/225000 (47%)] Loss: 7853.521484\n",
      "Train Epoch: 83 [110848/225000 (49%)] Loss: 9596.960938\n",
      "Train Epoch: 83 [114944/225000 (51%)] Loss: 8017.882812\n",
      "Train Epoch: 83 [119040/225000 (53%)] Loss: 8157.671875\n",
      "Train Epoch: 83 [123136/225000 (55%)] Loss: 9617.437500\n",
      "Train Epoch: 83 [127232/225000 (57%)] Loss: 7999.246094\n",
      "Train Epoch: 83 [131328/225000 (58%)] Loss: 10526.214844\n",
      "Train Epoch: 83 [135424/225000 (60%)] Loss: 8135.517578\n",
      "Train Epoch: 83 [139520/225000 (62%)] Loss: 10615.939453\n",
      "Train Epoch: 83 [143616/225000 (64%)] Loss: 8073.660156\n",
      "Train Epoch: 83 [147712/225000 (66%)] Loss: 7960.548828\n",
      "Train Epoch: 83 [151808/225000 (67%)] Loss: 8138.308594\n",
      "Train Epoch: 83 [155904/225000 (69%)] Loss: 8121.101562\n",
      "Train Epoch: 83 [160000/225000 (71%)] Loss: 12668.179688\n",
      "Train Epoch: 83 [164096/225000 (73%)] Loss: 8051.347656\n",
      "Train Epoch: 83 [168192/225000 (75%)] Loss: 10686.824219\n",
      "Train Epoch: 83 [172288/225000 (77%)] Loss: 10433.765625\n",
      "Train Epoch: 83 [176384/225000 (78%)] Loss: 14064.263672\n",
      "Train Epoch: 83 [180480/225000 (80%)] Loss: 8073.064453\n",
      "Train Epoch: 83 [184576/225000 (82%)] Loss: 8094.353516\n",
      "Train Epoch: 83 [188672/225000 (84%)] Loss: 11028.363281\n",
      "Train Epoch: 83 [192768/225000 (86%)] Loss: 8053.941406\n",
      "Train Epoch: 83 [196864/225000 (87%)] Loss: 7880.595703\n",
      "Train Epoch: 83 [200960/225000 (89%)] Loss: 10477.916016\n",
      "Train Epoch: 83 [205056/225000 (91%)] Loss: 7839.232422\n",
      "Train Epoch: 83 [209152/225000 (93%)] Loss: 8042.027344\n",
      "Train Epoch: 83 [213248/225000 (95%)] Loss: 8214.792969\n",
      "Train Epoch: 83 [217344/225000 (97%)] Loss: 8026.587891\n",
      "Train Epoch: 83 [221440/225000 (98%)] Loss: 9834.517578\n",
      "    epoch          : 83\n",
      "    loss           : 9505.133692406143\n",
      "    val_loss       : 9569.522334142606\n",
      "Train Epoch: 84 [256/225000 (0%)] Loss: 13594.587891\n",
      "Train Epoch: 84 [4352/225000 (2%)] Loss: 7903.087891\n",
      "Train Epoch: 84 [8448/225000 (4%)] Loss: 9429.121094\n",
      "Train Epoch: 84 [12544/225000 (6%)] Loss: 8208.498047\n",
      "Train Epoch: 84 [16640/225000 (7%)] Loss: 12975.447266\n",
      "Train Epoch: 84 [20736/225000 (9%)] Loss: 9685.607422\n",
      "Train Epoch: 84 [24832/225000 (11%)] Loss: 9588.605469\n",
      "Train Epoch: 84 [28928/225000 (13%)] Loss: 12027.087891\n",
      "Train Epoch: 84 [33024/225000 (15%)] Loss: 8176.828125\n",
      "Train Epoch: 84 [37120/225000 (16%)] Loss: 9532.382812\n",
      "Train Epoch: 84 [41216/225000 (18%)] Loss: 9614.095703\n",
      "Train Epoch: 84 [45312/225000 (20%)] Loss: 7981.751953\n",
      "Train Epoch: 84 [49408/225000 (22%)] Loss: 8096.541016\n",
      "Train Epoch: 84 [53504/225000 (24%)] Loss: 7917.392578\n",
      "Train Epoch: 84 [57600/225000 (26%)] Loss: 8167.693359\n",
      "Train Epoch: 84 [61696/225000 (27%)] Loss: 8159.949219\n",
      "Train Epoch: 84 [65792/225000 (29%)] Loss: 8165.734375\n",
      "Train Epoch: 84 [69888/225000 (31%)] Loss: 7924.175781\n",
      "Train Epoch: 84 [73984/225000 (33%)] Loss: 9716.367188\n",
      "Train Epoch: 84 [78080/225000 (35%)] Loss: 7953.486328\n",
      "Train Epoch: 84 [82176/225000 (37%)] Loss: 8084.292969\n",
      "Train Epoch: 84 [86272/225000 (38%)] Loss: 13963.306641\n",
      "Train Epoch: 84 [90368/225000 (40%)] Loss: 8092.187500\n",
      "Train Epoch: 84 [94464/225000 (42%)] Loss: 8223.857422\n",
      "Train Epoch: 84 [98560/225000 (44%)] Loss: 8139.494141\n",
      "Train Epoch: 84 [102656/225000 (46%)] Loss: 8219.027344\n",
      "Train Epoch: 84 [106752/225000 (47%)] Loss: 9591.115234\n",
      "Train Epoch: 84 [110848/225000 (49%)] Loss: 9590.281250\n",
      "Train Epoch: 84 [114944/225000 (51%)] Loss: 20311.525391\n",
      "Train Epoch: 84 [119040/225000 (53%)] Loss: 7949.876953\n",
      "Train Epoch: 84 [123136/225000 (55%)] Loss: 8055.953125\n",
      "Train Epoch: 84 [127232/225000 (57%)] Loss: 12758.447266\n",
      "Train Epoch: 84 [131328/225000 (58%)] Loss: 8038.964844\n",
      "Train Epoch: 84 [135424/225000 (60%)] Loss: 8137.451172\n",
      "Train Epoch: 84 [139520/225000 (62%)] Loss: 10543.882812\n",
      "Train Epoch: 84 [143616/225000 (64%)] Loss: 8059.230469\n",
      "Train Epoch: 84 [147712/225000 (66%)] Loss: 8355.076172\n",
      "Train Epoch: 84 [151808/225000 (67%)] Loss: 10604.880859\n",
      "Train Epoch: 84 [155904/225000 (69%)] Loss: 7946.484375\n",
      "Train Epoch: 84 [160000/225000 (71%)] Loss: 13454.871094\n",
      "Train Epoch: 84 [164096/225000 (73%)] Loss: 7940.916016\n",
      "Train Epoch: 84 [168192/225000 (75%)] Loss: 10413.636719\n",
      "Train Epoch: 84 [172288/225000 (77%)] Loss: 8157.927734\n",
      "Train Epoch: 84 [176384/225000 (78%)] Loss: 8032.310547\n",
      "Train Epoch: 84 [180480/225000 (80%)] Loss: 14665.226562\n",
      "Train Epoch: 84 [184576/225000 (82%)] Loss: 9530.150391\n",
      "Train Epoch: 84 [188672/225000 (84%)] Loss: 10481.978516\n",
      "Train Epoch: 84 [192768/225000 (86%)] Loss: 9540.119141\n",
      "Train Epoch: 84 [196864/225000 (87%)] Loss: 10453.873047\n",
      "Train Epoch: 84 [200960/225000 (89%)] Loss: 8135.988281\n",
      "Train Epoch: 84 [205056/225000 (91%)] Loss: 13468.898438\n",
      "Train Epoch: 84 [209152/225000 (93%)] Loss: 8018.416016\n",
      "Train Epoch: 84 [213248/225000 (95%)] Loss: 12113.757812\n",
      "Train Epoch: 84 [217344/225000 (97%)] Loss: 8180.937500\n",
      "Train Epoch: 84 [221440/225000 (98%)] Loss: 8157.246094\n",
      "    epoch          : 84\n",
      "    loss           : 9632.75042662116\n",
      "    val_loss       : 9404.857263063897\n",
      "Train Epoch: 85 [256/225000 (0%)] Loss: 12623.554688\n",
      "Train Epoch: 85 [4352/225000 (2%)] Loss: 10416.691406\n",
      "Train Epoch: 85 [8448/225000 (4%)] Loss: 15134.578125\n",
      "Train Epoch: 85 [12544/225000 (6%)] Loss: 8112.468750\n",
      "Train Epoch: 85 [16640/225000 (7%)] Loss: 9820.332031\n",
      "Train Epoch: 85 [20736/225000 (9%)] Loss: 8072.091797\n",
      "Train Epoch: 85 [24832/225000 (11%)] Loss: 12146.138672\n",
      "Train Epoch: 85 [28928/225000 (13%)] Loss: 8368.714844\n",
      "Train Epoch: 85 [33024/225000 (15%)] Loss: 8043.455078\n",
      "Train Epoch: 85 [37120/225000 (16%)] Loss: 8128.050781\n",
      "Train Epoch: 85 [41216/225000 (18%)] Loss: 10442.103516\n",
      "Train Epoch: 85 [45312/225000 (20%)] Loss: 9682.427734\n",
      "Train Epoch: 85 [49408/225000 (22%)] Loss: 8048.533203\n",
      "Train Epoch: 85 [53504/225000 (24%)] Loss: 8126.244141\n",
      "Train Epoch: 85 [57600/225000 (26%)] Loss: 7962.324219\n",
      "Train Epoch: 85 [61696/225000 (27%)] Loss: 12255.714844\n",
      "Train Epoch: 85 [65792/225000 (29%)] Loss: 8182.087891\n",
      "Train Epoch: 85 [69888/225000 (31%)] Loss: 7858.359375\n",
      "Train Epoch: 85 [73984/225000 (33%)] Loss: 10582.607422\n",
      "Train Epoch: 85 [78080/225000 (35%)] Loss: 13789.671875\n",
      "Train Epoch: 85 [82176/225000 (37%)] Loss: 8117.304688\n",
      "Train Epoch: 85 [86272/225000 (38%)] Loss: 7997.796875\n",
      "Train Epoch: 85 [90368/225000 (40%)] Loss: 15373.175781\n",
      "Train Epoch: 85 [94464/225000 (42%)] Loss: 10607.863281\n",
      "Train Epoch: 85 [98560/225000 (44%)] Loss: 13901.076172\n",
      "Train Epoch: 85 [102656/225000 (46%)] Loss: 8119.560547\n",
      "Train Epoch: 85 [106752/225000 (47%)] Loss: 8049.712891\n",
      "Train Epoch: 85 [110848/225000 (49%)] Loss: 7903.544922\n",
      "Train Epoch: 85 [114944/225000 (51%)] Loss: 12457.166016\n",
      "Train Epoch: 85 [119040/225000 (53%)] Loss: 8164.082031\n",
      "Train Epoch: 85 [123136/225000 (55%)] Loss: 10544.847656\n",
      "Train Epoch: 85 [127232/225000 (57%)] Loss: 7900.755859\n",
      "Train Epoch: 85 [131328/225000 (58%)] Loss: 7956.001953\n",
      "Train Epoch: 85 [135424/225000 (60%)] Loss: 7926.195312\n",
      "Train Epoch: 85 [139520/225000 (62%)] Loss: 13847.111328\n",
      "Train Epoch: 85 [143616/225000 (64%)] Loss: 7929.248047\n",
      "Train Epoch: 85 [147712/225000 (66%)] Loss: 8193.615234\n",
      "Train Epoch: 85 [151808/225000 (67%)] Loss: 10692.400391\n",
      "Train Epoch: 85 [155904/225000 (69%)] Loss: 12210.880859\n",
      "Train Epoch: 85 [160000/225000 (71%)] Loss: 7970.681641\n",
      "Train Epoch: 85 [164096/225000 (73%)] Loss: 8062.617188\n",
      "Train Epoch: 85 [168192/225000 (75%)] Loss: 10629.970703\n",
      "Train Epoch: 85 [172288/225000 (77%)] Loss: 8244.742188\n",
      "Train Epoch: 85 [176384/225000 (78%)] Loss: 7953.144531\n",
      "Train Epoch: 85 [180480/225000 (80%)] Loss: 8193.027344\n",
      "Train Epoch: 85 [184576/225000 (82%)] Loss: 7864.849609\n",
      "Train Epoch: 85 [188672/225000 (84%)] Loss: 9613.371094\n",
      "Train Epoch: 85 [192768/225000 (86%)] Loss: 8026.273438\n",
      "Train Epoch: 85 [196864/225000 (87%)] Loss: 8215.910156\n",
      "Train Epoch: 85 [200960/225000 (89%)] Loss: 8190.716797\n",
      "Train Epoch: 85 [205056/225000 (91%)] Loss: 10746.460938\n",
      "Train Epoch: 85 [209152/225000 (93%)] Loss: 9583.847656\n",
      "Train Epoch: 85 [213248/225000 (95%)] Loss: 15394.304688\n",
      "Train Epoch: 85 [217344/225000 (97%)] Loss: 7926.519531\n",
      "Train Epoch: 85 [221440/225000 (98%)] Loss: 8204.773438\n",
      "    epoch          : 85\n",
      "    loss           : 9659.158529756825\n",
      "    val_loss       : 9379.552607891512\n",
      "Train Epoch: 86 [256/225000 (0%)] Loss: 7911.058594\n",
      "Train Epoch: 86 [4352/225000 (2%)] Loss: 7869.933594\n",
      "Train Epoch: 86 [8448/225000 (4%)] Loss: 8025.525391\n",
      "Train Epoch: 86 [12544/225000 (6%)] Loss: 8097.001953\n",
      "Train Epoch: 86 [16640/225000 (7%)] Loss: 7991.845703\n",
      "Train Epoch: 86 [20736/225000 (9%)] Loss: 10463.775391\n",
      "Train Epoch: 86 [24832/225000 (11%)] Loss: 10554.158203\n",
      "Train Epoch: 86 [28928/225000 (13%)] Loss: 8257.250000\n",
      "Train Epoch: 86 [33024/225000 (15%)] Loss: 8206.904297\n",
      "Train Epoch: 86 [37120/225000 (16%)] Loss: 7941.857422\n",
      "Train Epoch: 86 [41216/225000 (18%)] Loss: 10409.320312\n",
      "Train Epoch: 86 [45312/225000 (20%)] Loss: 8056.746094\n",
      "Train Epoch: 86 [49408/225000 (22%)] Loss: 7918.261719\n",
      "Train Epoch: 86 [53504/225000 (24%)] Loss: 9745.732422\n",
      "Train Epoch: 86 [57600/225000 (26%)] Loss: 10410.490234\n",
      "Train Epoch: 86 [61696/225000 (27%)] Loss: 9614.214844\n",
      "Train Epoch: 86 [65792/225000 (29%)] Loss: 7897.937500\n",
      "Train Epoch: 86 [69888/225000 (31%)] Loss: 7818.113281\n",
      "Train Epoch: 86 [73984/225000 (33%)] Loss: 9641.988281\n",
      "Train Epoch: 86 [78080/225000 (35%)] Loss: 10670.023438\n",
      "Train Epoch: 86 [82176/225000 (37%)] Loss: 8080.302734\n",
      "Train Epoch: 86 [86272/225000 (38%)] Loss: 8065.601562\n",
      "Train Epoch: 86 [90368/225000 (40%)] Loss: 7967.843750\n",
      "Train Epoch: 86 [94464/225000 (42%)] Loss: 8004.195312\n",
      "Train Epoch: 86 [98560/225000 (44%)] Loss: 10351.841797\n",
      "Train Epoch: 86 [102656/225000 (46%)] Loss: 8230.310547\n",
      "Train Epoch: 86 [106752/225000 (47%)] Loss: 8235.976562\n",
      "Train Epoch: 86 [110848/225000 (49%)] Loss: 8098.330078\n",
      "Train Epoch: 86 [114944/225000 (51%)] Loss: 8144.041016\n",
      "Train Epoch: 86 [119040/225000 (53%)] Loss: 9547.781250\n",
      "Train Epoch: 86 [123136/225000 (55%)] Loss: 8022.408203\n",
      "Train Epoch: 86 [127232/225000 (57%)] Loss: 15962.560547\n",
      "Train Epoch: 86 [131328/225000 (58%)] Loss: 7949.728516\n",
      "Train Epoch: 86 [135424/225000 (60%)] Loss: 8175.341797\n",
      "Train Epoch: 86 [139520/225000 (62%)] Loss: 15972.646484\n",
      "Train Epoch: 86 [143616/225000 (64%)] Loss: 8244.902344\n",
      "Train Epoch: 86 [147712/225000 (66%)] Loss: 8031.269531\n",
      "Train Epoch: 86 [151808/225000 (67%)] Loss: 12847.121094\n",
      "Train Epoch: 86 [155904/225000 (69%)] Loss: 8078.267578\n",
      "Train Epoch: 86 [160000/225000 (71%)] Loss: 7990.248047\n",
      "Train Epoch: 86 [164096/225000 (73%)] Loss: 9534.894531\n",
      "Train Epoch: 86 [168192/225000 (75%)] Loss: 9856.582031\n",
      "Train Epoch: 86 [172288/225000 (77%)] Loss: 8033.583984\n",
      "Train Epoch: 86 [176384/225000 (78%)] Loss: 8052.544922\n",
      "Train Epoch: 86 [180480/225000 (80%)] Loss: 9575.365234\n",
      "Train Epoch: 86 [184576/225000 (82%)] Loss: 9592.816406\n",
      "Train Epoch: 86 [188672/225000 (84%)] Loss: 7914.523438\n",
      "Train Epoch: 86 [192768/225000 (86%)] Loss: 8257.365234\n",
      "Train Epoch: 86 [196864/225000 (87%)] Loss: 7965.031250\n",
      "Train Epoch: 86 [200960/225000 (89%)] Loss: 8076.580078\n",
      "Train Epoch: 86 [205056/225000 (91%)] Loss: 9555.277344\n",
      "Train Epoch: 86 [209152/225000 (93%)] Loss: 13947.658203\n",
      "Train Epoch: 86 [213248/225000 (95%)] Loss: 12560.339844\n",
      "Train Epoch: 86 [217344/225000 (97%)] Loss: 13096.355469\n",
      "Train Epoch: 86 [221440/225000 (98%)] Loss: 21069.570312\n",
      "    epoch          : 86\n",
      "    loss           : 9571.238558998151\n",
      "    val_loss       : 9845.357426745551\n",
      "Train Epoch: 87 [256/225000 (0%)] Loss: 9499.316406\n",
      "Train Epoch: 87 [4352/225000 (2%)] Loss: 8117.902344\n",
      "Train Epoch: 87 [8448/225000 (4%)] Loss: 7887.027344\n",
      "Train Epoch: 87 [12544/225000 (6%)] Loss: 8038.154297\n",
      "Train Epoch: 87 [16640/225000 (7%)] Loss: 8002.291016\n",
      "Train Epoch: 87 [20736/225000 (9%)] Loss: 7828.984375\n",
      "Train Epoch: 87 [24832/225000 (11%)] Loss: 9606.707031\n",
      "Train Epoch: 87 [28928/225000 (13%)] Loss: 19842.250000\n",
      "Train Epoch: 87 [33024/225000 (15%)] Loss: 7943.675781\n",
      "Train Epoch: 87 [37120/225000 (16%)] Loss: 8211.953125\n",
      "Train Epoch: 87 [41216/225000 (18%)] Loss: 13986.710938\n",
      "Train Epoch: 87 [45312/225000 (20%)] Loss: 12441.093750\n",
      "Train Epoch: 87 [49408/225000 (22%)] Loss: 9390.613281\n",
      "Train Epoch: 87 [53504/225000 (24%)] Loss: 9546.273438\n",
      "Train Epoch: 87 [57600/225000 (26%)] Loss: 8124.386719\n",
      "Train Epoch: 87 [61696/225000 (27%)] Loss: 8054.888672\n",
      "Train Epoch: 87 [65792/225000 (29%)] Loss: 7973.521484\n",
      "Train Epoch: 87 [69888/225000 (31%)] Loss: 8030.835938\n",
      "Train Epoch: 87 [73984/225000 (33%)] Loss: 8241.470703\n",
      "Train Epoch: 87 [78080/225000 (35%)] Loss: 8226.000000\n",
      "Train Epoch: 87 [82176/225000 (37%)] Loss: 8016.685547\n",
      "Train Epoch: 87 [86272/225000 (38%)] Loss: 8115.007812\n",
      "Train Epoch: 87 [90368/225000 (40%)] Loss: 8020.119141\n",
      "Train Epoch: 87 [94464/225000 (42%)] Loss: 9447.666016\n",
      "Train Epoch: 87 [98560/225000 (44%)] Loss: 9537.292969\n",
      "Train Epoch: 87 [102656/225000 (46%)] Loss: 10614.691406\n",
      "Train Epoch: 87 [106752/225000 (47%)] Loss: 7885.724609\n",
      "Train Epoch: 87 [110848/225000 (49%)] Loss: 13722.701172\n",
      "Train Epoch: 87 [114944/225000 (51%)] Loss: 8104.138672\n",
      "Train Epoch: 87 [119040/225000 (53%)] Loss: 8008.654297\n",
      "Train Epoch: 87 [123136/225000 (55%)] Loss: 8073.431641\n",
      "Train Epoch: 87 [127232/225000 (57%)] Loss: 10590.275391\n",
      "Train Epoch: 87 [131328/225000 (58%)] Loss: 15081.574219\n",
      "Train Epoch: 87 [135424/225000 (60%)] Loss: 7862.011719\n",
      "Train Epoch: 87 [139520/225000 (62%)] Loss: 8020.820312\n",
      "Train Epoch: 87 [143616/225000 (64%)] Loss: 7948.544922\n",
      "Train Epoch: 87 [147712/225000 (66%)] Loss: 7952.515625\n",
      "Train Epoch: 87 [151808/225000 (67%)] Loss: 7789.839844\n",
      "Train Epoch: 87 [155904/225000 (69%)] Loss: 16247.046875\n",
      "Train Epoch: 87 [160000/225000 (71%)] Loss: 8011.335938\n",
      "Train Epoch: 87 [164096/225000 (73%)] Loss: 8047.019531\n",
      "Train Epoch: 87 [168192/225000 (75%)] Loss: 9553.976562\n",
      "Train Epoch: 87 [172288/225000 (77%)] Loss: 8147.820312\n",
      "Train Epoch: 87 [176384/225000 (78%)] Loss: 10490.712891\n",
      "Train Epoch: 87 [180480/225000 (80%)] Loss: 8036.359375\n",
      "Train Epoch: 87 [184576/225000 (82%)] Loss: 7952.513672\n",
      "Train Epoch: 87 [188672/225000 (84%)] Loss: 8072.929688\n",
      "Train Epoch: 87 [192768/225000 (86%)] Loss: 13815.363281\n",
      "Train Epoch: 87 [196864/225000 (87%)] Loss: 12550.357422\n",
      "Train Epoch: 87 [200960/225000 (89%)] Loss: 10570.300781\n",
      "Train Epoch: 87 [205056/225000 (91%)] Loss: 9656.718750\n",
      "Train Epoch: 87 [209152/225000 (93%)] Loss: 12296.498047\n",
      "Train Epoch: 87 [213248/225000 (95%)] Loss: 14806.656250\n",
      "Train Epoch: 87 [217344/225000 (97%)] Loss: 10524.611328\n",
      "Train Epoch: 87 [221440/225000 (98%)] Loss: 12411.613281\n",
      "    epoch          : 87\n",
      "    loss           : 9383.118409591865\n",
      "    val_loss       : 9504.357997359062\n",
      "Train Epoch: 88 [256/225000 (0%)] Loss: 7964.443359\n",
      "Train Epoch: 88 [4352/225000 (2%)] Loss: 7969.460938\n",
      "Train Epoch: 88 [8448/225000 (4%)] Loss: 8054.507812\n",
      "Train Epoch: 88 [12544/225000 (6%)] Loss: 9699.193359\n",
      "Train Epoch: 88 [16640/225000 (7%)] Loss: 8095.535156\n",
      "Train Epoch: 88 [20736/225000 (9%)] Loss: 7914.378906\n",
      "Train Epoch: 88 [24832/225000 (11%)] Loss: 8060.078125\n",
      "Train Epoch: 88 [28928/225000 (13%)] Loss: 8084.818359\n",
      "Train Epoch: 88 [33024/225000 (15%)] Loss: 7907.320312\n",
      "Train Epoch: 88 [37120/225000 (16%)] Loss: 9599.925781\n",
      "Train Epoch: 88 [41216/225000 (18%)] Loss: 8062.289062\n",
      "Train Epoch: 88 [45312/225000 (20%)] Loss: 8202.439453\n",
      "Train Epoch: 88 [49408/225000 (22%)] Loss: 10635.025391\n",
      "Train Epoch: 88 [53504/225000 (24%)] Loss: 7847.349609\n",
      "Train Epoch: 88 [57600/225000 (26%)] Loss: 8089.183594\n",
      "Train Epoch: 88 [61696/225000 (27%)] Loss: 13458.019531\n",
      "Train Epoch: 88 [65792/225000 (29%)] Loss: 8014.537109\n",
      "Train Epoch: 88 [69888/225000 (31%)] Loss: 9705.880859\n",
      "Train Epoch: 88 [73984/225000 (33%)] Loss: 8117.705078\n",
      "Train Epoch: 88 [78080/225000 (35%)] Loss: 8207.478516\n",
      "Train Epoch: 88 [82176/225000 (37%)] Loss: 8027.591797\n",
      "Train Epoch: 88 [86272/225000 (38%)] Loss: 7995.447266\n",
      "Train Epoch: 88 [90368/225000 (40%)] Loss: 8112.519531\n",
      "Train Epoch: 88 [94464/225000 (42%)] Loss: 9675.875000\n",
      "Train Epoch: 88 [98560/225000 (44%)] Loss: 7981.287109\n",
      "Train Epoch: 88 [102656/225000 (46%)] Loss: 12815.060547\n",
      "Train Epoch: 88 [106752/225000 (47%)] Loss: 8271.654297\n",
      "Train Epoch: 88 [110848/225000 (49%)] Loss: 9694.650391\n",
      "Train Epoch: 88 [114944/225000 (51%)] Loss: 9637.414062\n",
      "Train Epoch: 88 [119040/225000 (53%)] Loss: 12712.619141\n",
      "Train Epoch: 88 [123136/225000 (55%)] Loss: 10386.027344\n",
      "Train Epoch: 88 [127232/225000 (57%)] Loss: 9520.072266\n",
      "Train Epoch: 88 [131328/225000 (58%)] Loss: 8053.730469\n",
      "Train Epoch: 88 [135424/225000 (60%)] Loss: 8211.132812\n",
      "Train Epoch: 88 [139520/225000 (62%)] Loss: 14130.867188\n",
      "Train Epoch: 88 [143616/225000 (64%)] Loss: 7997.289062\n",
      "Train Epoch: 88 [147712/225000 (66%)] Loss: 13170.871094\n",
      "Train Epoch: 88 [151808/225000 (67%)] Loss: 8154.019531\n",
      "Train Epoch: 88 [155904/225000 (69%)] Loss: 8055.976562\n",
      "Train Epoch: 88 [160000/225000 (71%)] Loss: 7894.669922\n",
      "Train Epoch: 88 [164096/225000 (73%)] Loss: 9838.603516\n",
      "Train Epoch: 88 [168192/225000 (75%)] Loss: 9372.285156\n",
      "Train Epoch: 88 [172288/225000 (77%)] Loss: 12163.417969\n",
      "Train Epoch: 88 [176384/225000 (78%)] Loss: 7931.718750\n",
      "Train Epoch: 88 [180480/225000 (80%)] Loss: 14280.074219\n",
      "Train Epoch: 88 [184576/225000 (82%)] Loss: 12362.162109\n",
      "Train Epoch: 88 [188672/225000 (84%)] Loss: 8161.710938\n",
      "Train Epoch: 88 [192768/225000 (86%)] Loss: 11015.863281\n",
      "Train Epoch: 88 [196864/225000 (87%)] Loss: 9785.283203\n",
      "Train Epoch: 88 [200960/225000 (89%)] Loss: 8220.279297\n",
      "Train Epoch: 88 [205056/225000 (91%)] Loss: 8172.398438\n",
      "Train Epoch: 88 [209152/225000 (93%)] Loss: 8155.904297\n",
      "Train Epoch: 88 [213248/225000 (95%)] Loss: 8301.085938\n",
      "Train Epoch: 88 [217344/225000 (97%)] Loss: 7997.699219\n",
      "Train Epoch: 88 [221440/225000 (98%)] Loss: 8056.253906\n",
      "    epoch          : 88\n",
      "    loss           : 9467.327129550626\n",
      "    val_loss       : 9604.670673524846\n",
      "Train Epoch: 89 [256/225000 (0%)] Loss: 7889.181641\n",
      "Train Epoch: 89 [4352/225000 (2%)] Loss: 12416.328125\n",
      "Train Epoch: 89 [8448/225000 (4%)] Loss: 10475.951172\n",
      "Train Epoch: 89 [12544/225000 (6%)] Loss: 8100.673828\n",
      "Train Epoch: 89 [16640/225000 (7%)] Loss: 11945.705078\n",
      "Train Epoch: 89 [20736/225000 (9%)] Loss: 9587.015625\n",
      "Train Epoch: 89 [24832/225000 (11%)] Loss: 8101.898438\n",
      "Train Epoch: 89 [28928/225000 (13%)] Loss: 10538.191406\n",
      "Train Epoch: 89 [33024/225000 (15%)] Loss: 8018.257812\n",
      "Train Epoch: 89 [37120/225000 (16%)] Loss: 8120.917969\n",
      "Train Epoch: 89 [41216/225000 (18%)] Loss: 8281.437500\n",
      "Train Epoch: 89 [45312/225000 (20%)] Loss: 8155.957031\n",
      "Train Epoch: 89 [49408/225000 (22%)] Loss: 13313.037109\n",
      "Train Epoch: 89 [53504/225000 (24%)] Loss: 7800.365234\n",
      "Train Epoch: 89 [57600/225000 (26%)] Loss: 8086.025391\n",
      "Train Epoch: 89 [61696/225000 (27%)] Loss: 8189.261719\n",
      "Train Epoch: 89 [65792/225000 (29%)] Loss: 11098.947266\n",
      "Train Epoch: 89 [69888/225000 (31%)] Loss: 9723.892578\n",
      "Train Epoch: 89 [73984/225000 (33%)] Loss: 10662.060547\n",
      "Train Epoch: 89 [78080/225000 (35%)] Loss: 7752.689453\n",
      "Train Epoch: 89 [82176/225000 (37%)] Loss: 9506.117188\n",
      "Train Epoch: 89 [86272/225000 (38%)] Loss: 13583.736328\n",
      "Train Epoch: 89 [90368/225000 (40%)] Loss: 7927.458984\n",
      "Train Epoch: 89 [94464/225000 (42%)] Loss: 13928.107422\n",
      "Train Epoch: 89 [98560/225000 (44%)] Loss: 9608.412109\n",
      "Train Epoch: 89 [102656/225000 (46%)] Loss: 7892.605469\n",
      "Train Epoch: 89 [106752/225000 (47%)] Loss: 12245.564453\n",
      "Train Epoch: 89 [110848/225000 (49%)] Loss: 8044.429688\n",
      "Train Epoch: 89 [114944/225000 (51%)] Loss: 8026.443359\n",
      "Train Epoch: 89 [119040/225000 (53%)] Loss: 8186.607422\n",
      "Train Epoch: 89 [123136/225000 (55%)] Loss: 9635.103516\n",
      "Train Epoch: 89 [127232/225000 (57%)] Loss: 8134.523438\n",
      "Train Epoch: 89 [131328/225000 (58%)] Loss: 7857.005859\n",
      "Train Epoch: 89 [135424/225000 (60%)] Loss: 8077.076172\n",
      "Train Epoch: 89 [139520/225000 (62%)] Loss: 9606.541016\n",
      "Train Epoch: 89 [143616/225000 (64%)] Loss: 9545.076172\n",
      "Train Epoch: 89 [147712/225000 (66%)] Loss: 10621.027344\n",
      "Train Epoch: 89 [151808/225000 (67%)] Loss: 9400.822266\n",
      "Train Epoch: 89 [155904/225000 (69%)] Loss: 8143.728516\n",
      "Train Epoch: 89 [160000/225000 (71%)] Loss: 12464.953125\n",
      "Train Epoch: 89 [164096/225000 (73%)] Loss: 8332.460938\n",
      "Train Epoch: 89 [168192/225000 (75%)] Loss: 8026.830078\n",
      "Train Epoch: 89 [172288/225000 (77%)] Loss: 10684.968750\n",
      "Train Epoch: 89 [176384/225000 (78%)] Loss: 8277.681641\n",
      "Train Epoch: 89 [180480/225000 (80%)] Loss: 9731.966797\n",
      "Train Epoch: 89 [184576/225000 (82%)] Loss: 8000.558594\n",
      "Train Epoch: 89 [188672/225000 (84%)] Loss: 15658.781250\n",
      "Train Epoch: 89 [192768/225000 (86%)] Loss: 8225.779297\n",
      "Train Epoch: 89 [196864/225000 (87%)] Loss: 16380.558594\n",
      "Train Epoch: 89 [200960/225000 (89%)] Loss: 12367.140625\n",
      "Train Epoch: 89 [205056/225000 (91%)] Loss: 8251.958984\n",
      "Train Epoch: 89 [209152/225000 (93%)] Loss: 8028.625000\n",
      "Train Epoch: 89 [213248/225000 (95%)] Loss: 7944.593750\n",
      "Train Epoch: 89 [217344/225000 (97%)] Loss: 10714.527344\n",
      "Train Epoch: 89 [221440/225000 (98%)] Loss: 13855.513672\n",
      "    epoch          : 89\n",
      "    loss           : 9476.68384483433\n",
      "    val_loss       : 9628.336418171319\n",
      "Train Epoch: 90 [256/225000 (0%)] Loss: 8061.019531\n",
      "Train Epoch: 90 [4352/225000 (2%)] Loss: 8304.976562\n",
      "Train Epoch: 90 [8448/225000 (4%)] Loss: 8141.347656\n",
      "Train Epoch: 90 [12544/225000 (6%)] Loss: 7959.154297\n",
      "Train Epoch: 90 [16640/225000 (7%)] Loss: 9463.167969\n",
      "Train Epoch: 90 [20736/225000 (9%)] Loss: 8108.546875\n",
      "Train Epoch: 90 [24832/225000 (11%)] Loss: 8343.464844\n",
      "Train Epoch: 90 [28928/225000 (13%)] Loss: 8053.857422\n",
      "Train Epoch: 90 [33024/225000 (15%)] Loss: 7975.873047\n",
      "Train Epoch: 90 [37120/225000 (16%)] Loss: 9655.029297\n",
      "Train Epoch: 90 [41216/225000 (18%)] Loss: 8129.330078\n",
      "Train Epoch: 90 [45312/225000 (20%)] Loss: 8187.619141\n",
      "Train Epoch: 90 [49408/225000 (22%)] Loss: 16929.148438\n",
      "Train Epoch: 90 [53504/225000 (24%)] Loss: 9639.517578\n",
      "Train Epoch: 90 [57600/225000 (26%)] Loss: 8164.660156\n",
      "Train Epoch: 90 [61696/225000 (27%)] Loss: 10603.986328\n",
      "Train Epoch: 90 [65792/225000 (29%)] Loss: 9530.111328\n",
      "Train Epoch: 90 [69888/225000 (31%)] Loss: 8094.195312\n",
      "Train Epoch: 90 [73984/225000 (33%)] Loss: 7989.464844\n",
      "Train Epoch: 90 [78080/225000 (35%)] Loss: 8059.865234\n",
      "Train Epoch: 90 [82176/225000 (37%)] Loss: 7932.445312\n",
      "Train Epoch: 90 [86272/225000 (38%)] Loss: 8097.841797\n",
      "Train Epoch: 90 [90368/225000 (40%)] Loss: 8187.804688\n",
      "Train Epoch: 90 [94464/225000 (42%)] Loss: 8123.796875\n",
      "Train Epoch: 90 [98560/225000 (44%)] Loss: 10456.302734\n",
      "Train Epoch: 90 [102656/225000 (46%)] Loss: 7954.847656\n",
      "Train Epoch: 90 [106752/225000 (47%)] Loss: 7958.287109\n",
      "Train Epoch: 90 [110848/225000 (49%)] Loss: 7916.773438\n",
      "Train Epoch: 90 [114944/225000 (51%)] Loss: 8030.236328\n",
      "Train Epoch: 90 [119040/225000 (53%)] Loss: 8298.380859\n",
      "Train Epoch: 90 [123136/225000 (55%)] Loss: 8113.464844\n",
      "Train Epoch: 90 [127232/225000 (57%)] Loss: 12635.365234\n",
      "Train Epoch: 90 [131328/225000 (58%)] Loss: 7946.984375\n",
      "Train Epoch: 90 [135424/225000 (60%)] Loss: 8061.207031\n",
      "Train Epoch: 90 [139520/225000 (62%)] Loss: 8008.369141\n",
      "Train Epoch: 90 [143616/225000 (64%)] Loss: 8043.111328\n",
      "Train Epoch: 90 [147712/225000 (66%)] Loss: 12475.201172\n",
      "Train Epoch: 90 [151808/225000 (67%)] Loss: 7939.005859\n",
      "Train Epoch: 90 [155904/225000 (69%)] Loss: 8189.652344\n",
      "Train Epoch: 90 [160000/225000 (71%)] Loss: 7905.849609\n",
      "Train Epoch: 90 [164096/225000 (73%)] Loss: 8091.287109\n",
      "Train Epoch: 90 [168192/225000 (75%)] Loss: 8192.207031\n",
      "Train Epoch: 90 [172288/225000 (77%)] Loss: 9611.388672\n",
      "Train Epoch: 90 [176384/225000 (78%)] Loss: 8055.363281\n",
      "Train Epoch: 90 [180480/225000 (80%)] Loss: 8071.740234\n",
      "Train Epoch: 90 [184576/225000 (82%)] Loss: 7953.552734\n",
      "Train Epoch: 90 [188672/225000 (84%)] Loss: 8279.806641\n",
      "Train Epoch: 90 [192768/225000 (86%)] Loss: 9534.287109\n",
      "Train Epoch: 90 [196864/225000 (87%)] Loss: 16276.763672\n",
      "Train Epoch: 90 [200960/225000 (89%)] Loss: 8009.906250\n",
      "Train Epoch: 90 [205056/225000 (91%)] Loss: 16378.761719\n",
      "Train Epoch: 90 [209152/225000 (93%)] Loss: 14045.701172\n",
      "Train Epoch: 90 [213248/225000 (95%)] Loss: 7885.322266\n",
      "Train Epoch: 90 [217344/225000 (97%)] Loss: 8206.662109\n",
      "Train Epoch: 90 [221440/225000 (98%)] Loss: 12452.523438\n",
      "    epoch          : 90\n",
      "    loss           : 9516.163936957835\n",
      "    val_loss       : 9399.002516834104\n",
      "Train Epoch: 91 [256/225000 (0%)] Loss: 14728.929688\n",
      "Train Epoch: 91 [4352/225000 (2%)] Loss: 11946.968750\n",
      "Train Epoch: 91 [8448/225000 (4%)] Loss: 10376.695312\n",
      "Train Epoch: 91 [12544/225000 (6%)] Loss: 8022.210938\n",
      "Train Epoch: 91 [16640/225000 (7%)] Loss: 8095.951172\n",
      "Train Epoch: 91 [20736/225000 (9%)] Loss: 8035.537109\n",
      "Train Epoch: 91 [24832/225000 (11%)] Loss: 8163.876953\n",
      "Train Epoch: 91 [28928/225000 (13%)] Loss: 7852.099609\n",
      "Train Epoch: 91 [33024/225000 (15%)] Loss: 8105.867188\n",
      "Train Epoch: 91 [37120/225000 (16%)] Loss: 8029.230469\n",
      "Train Epoch: 91 [41216/225000 (18%)] Loss: 13816.156250\n",
      "Train Epoch: 91 [45312/225000 (20%)] Loss: 8076.152344\n",
      "Train Epoch: 91 [49408/225000 (22%)] Loss: 12212.300781\n",
      "Train Epoch: 91 [53504/225000 (24%)] Loss: 8083.173828\n",
      "Train Epoch: 91 [57600/225000 (26%)] Loss: 7909.199219\n",
      "Train Epoch: 91 [61696/225000 (27%)] Loss: 8096.427734\n",
      "Train Epoch: 91 [65792/225000 (29%)] Loss: 18340.812500\n",
      "Train Epoch: 91 [69888/225000 (31%)] Loss: 7976.968750\n",
      "Train Epoch: 91 [73984/225000 (33%)] Loss: 8014.070312\n",
      "Train Epoch: 91 [78080/225000 (35%)] Loss: 13305.789062\n",
      "Train Epoch: 91 [82176/225000 (37%)] Loss: 9898.763672\n",
      "Train Epoch: 91 [86272/225000 (38%)] Loss: 9543.021484\n",
      "Train Epoch: 91 [90368/225000 (40%)] Loss: 12305.943359\n",
      "Train Epoch: 91 [94464/225000 (42%)] Loss: 9586.150391\n",
      "Train Epoch: 91 [98560/225000 (44%)] Loss: 8293.007812\n",
      "Train Epoch: 91 [102656/225000 (46%)] Loss: 8247.048828\n",
      "Train Epoch: 91 [106752/225000 (47%)] Loss: 10386.951172\n",
      "Train Epoch: 91 [110848/225000 (49%)] Loss: 7996.404297\n",
      "Train Epoch: 91 [114944/225000 (51%)] Loss: 9566.378906\n",
      "Train Epoch: 91 [119040/225000 (53%)] Loss: 13575.585938\n",
      "Train Epoch: 91 [123136/225000 (55%)] Loss: 8022.558594\n",
      "Train Epoch: 91 [127232/225000 (57%)] Loss: 8027.998047\n",
      "Train Epoch: 91 [131328/225000 (58%)] Loss: 10796.464844\n",
      "Train Epoch: 91 [135424/225000 (60%)] Loss: 13694.378906\n",
      "Train Epoch: 91 [139520/225000 (62%)] Loss: 10404.462891\n",
      "Train Epoch: 91 [143616/225000 (64%)] Loss: 13783.542969\n",
      "Train Epoch: 91 [147712/225000 (66%)] Loss: 8147.513672\n",
      "Train Epoch: 91 [151808/225000 (67%)] Loss: 8110.634766\n",
      "Train Epoch: 91 [155904/225000 (69%)] Loss: 8149.921875\n",
      "Train Epoch: 91 [160000/225000 (71%)] Loss: 8273.970703\n",
      "Train Epoch: 91 [164096/225000 (73%)] Loss: 8313.976562\n",
      "Train Epoch: 91 [168192/225000 (75%)] Loss: 9540.802734\n",
      "Train Epoch: 91 [172288/225000 (77%)] Loss: 8159.496094\n",
      "Train Epoch: 91 [176384/225000 (78%)] Loss: 12710.218750\n",
      "Train Epoch: 91 [180480/225000 (80%)] Loss: 8130.183594\n",
      "Train Epoch: 91 [184576/225000 (82%)] Loss: 12096.593750\n",
      "Train Epoch: 91 [188672/225000 (84%)] Loss: 7963.412109\n",
      "Train Epoch: 91 [192768/225000 (86%)] Loss: 8274.677734\n",
      "Train Epoch: 91 [196864/225000 (87%)] Loss: 8017.876953\n",
      "Train Epoch: 91 [200960/225000 (89%)] Loss: 18023.333984\n",
      "Train Epoch: 91 [205056/225000 (91%)] Loss: 10377.812500\n",
      "Train Epoch: 91 [209152/225000 (93%)] Loss: 8110.748047\n",
      "Train Epoch: 91 [213248/225000 (95%)] Loss: 7985.093750\n",
      "Train Epoch: 91 [217344/225000 (97%)] Loss: 7968.771484\n",
      "Train Epoch: 91 [221440/225000 (98%)] Loss: 13049.894531\n",
      "    epoch          : 91\n",
      "    loss           : 9667.36867956307\n",
      "    val_loss       : 9653.42913902049\n",
      "Train Epoch: 92 [256/225000 (0%)] Loss: 14024.039062\n",
      "Train Epoch: 92 [4352/225000 (2%)] Loss: 8191.464844\n",
      "Train Epoch: 92 [8448/225000 (4%)] Loss: 8012.486328\n",
      "Train Epoch: 92 [12544/225000 (6%)] Loss: 8063.503906\n",
      "Train Epoch: 92 [16640/225000 (7%)] Loss: 7944.080078\n",
      "Train Epoch: 92 [20736/225000 (9%)] Loss: 10395.138672\n",
      "Train Epoch: 92 [24832/225000 (11%)] Loss: 8022.859375\n",
      "Train Epoch: 92 [28928/225000 (13%)] Loss: 13983.757812\n",
      "Train Epoch: 92 [33024/225000 (15%)] Loss: 8271.271484\n",
      "Train Epoch: 92 [37120/225000 (16%)] Loss: 8185.097656\n",
      "Train Epoch: 92 [41216/225000 (18%)] Loss: 7913.156250\n",
      "Train Epoch: 92 [45312/225000 (20%)] Loss: 9619.255859\n",
      "Train Epoch: 92 [49408/225000 (22%)] Loss: 8100.189453\n",
      "Train Epoch: 92 [53504/225000 (24%)] Loss: 8022.121094\n",
      "Train Epoch: 92 [57600/225000 (26%)] Loss: 8117.990234\n",
      "Train Epoch: 92 [61696/225000 (27%)] Loss: 8084.484375\n",
      "Train Epoch: 92 [65792/225000 (29%)] Loss: 9686.660156\n",
      "Train Epoch: 92 [69888/225000 (31%)] Loss: 9598.968750\n",
      "Train Epoch: 92 [73984/225000 (33%)] Loss: 8107.031250\n",
      "Train Epoch: 92 [78080/225000 (35%)] Loss: 8187.576172\n",
      "Train Epoch: 92 [82176/225000 (37%)] Loss: 11968.136719\n",
      "Train Epoch: 92 [86272/225000 (38%)] Loss: 7994.939453\n",
      "Train Epoch: 92 [90368/225000 (40%)] Loss: 10701.201172\n",
      "Train Epoch: 92 [94464/225000 (42%)] Loss: 7996.277344\n",
      "Train Epoch: 92 [98560/225000 (44%)] Loss: 13671.878906\n",
      "Train Epoch: 92 [102656/225000 (46%)] Loss: 9719.972656\n",
      "Train Epoch: 92 [106752/225000 (47%)] Loss: 8022.582031\n",
      "Train Epoch: 92 [110848/225000 (49%)] Loss: 8103.730469\n",
      "Train Epoch: 92 [114944/225000 (51%)] Loss: 8140.570312\n",
      "Train Epoch: 92 [119040/225000 (53%)] Loss: 10631.667969\n",
      "Train Epoch: 92 [123136/225000 (55%)] Loss: 9696.189453\n",
      "Train Epoch: 92 [127232/225000 (57%)] Loss: 9932.427734\n",
      "Train Epoch: 92 [131328/225000 (58%)] Loss: 9537.531250\n",
      "Train Epoch: 92 [135424/225000 (60%)] Loss: 8142.720703\n",
      "Train Epoch: 92 [139520/225000 (62%)] Loss: 8121.542969\n",
      "Train Epoch: 92 [143616/225000 (64%)] Loss: 13871.708984\n",
      "Train Epoch: 92 [147712/225000 (66%)] Loss: 8320.957031\n",
      "Train Epoch: 92 [151808/225000 (67%)] Loss: 9802.791016\n",
      "Train Epoch: 92 [155904/225000 (69%)] Loss: 8043.091797\n",
      "Train Epoch: 92 [160000/225000 (71%)] Loss: 8324.183594\n",
      "Train Epoch: 92 [164096/225000 (73%)] Loss: 7986.777344\n",
      "Train Epoch: 92 [168192/225000 (75%)] Loss: 10388.814453\n",
      "Train Epoch: 92 [172288/225000 (77%)] Loss: 9511.943359\n",
      "Train Epoch: 92 [176384/225000 (78%)] Loss: 12814.253906\n",
      "Train Epoch: 92 [180480/225000 (80%)] Loss: 13868.615234\n",
      "Train Epoch: 92 [184576/225000 (82%)] Loss: 7894.843750\n",
      "Train Epoch: 92 [188672/225000 (84%)] Loss: 9715.306641\n",
      "Train Epoch: 92 [192768/225000 (86%)] Loss: 9650.763672\n",
      "Train Epoch: 92 [196864/225000 (87%)] Loss: 14142.693359\n",
      "Train Epoch: 92 [200960/225000 (89%)] Loss: 12825.068359\n",
      "Train Epoch: 92 [205056/225000 (91%)] Loss: 11083.175781\n",
      "Train Epoch: 92 [209152/225000 (93%)] Loss: 8209.195312\n",
      "Train Epoch: 92 [213248/225000 (95%)] Loss: 9579.496094\n",
      "Train Epoch: 92 [217344/225000 (97%)] Loss: 11118.224609\n",
      "Train Epoch: 92 [221440/225000 (98%)] Loss: 12404.667969\n",
      "    epoch          : 92\n",
      "    loss           : 9621.248510158917\n",
      "    val_loss       : 9507.323381801041\n",
      "Train Epoch: 93 [256/225000 (0%)] Loss: 7932.925781\n",
      "Train Epoch: 93 [4352/225000 (2%)] Loss: 7957.845703\n",
      "Train Epoch: 93 [8448/225000 (4%)] Loss: 8155.054688\n",
      "Train Epoch: 93 [12544/225000 (6%)] Loss: 7958.464844\n",
      "Train Epoch: 93 [16640/225000 (7%)] Loss: 12345.652344\n",
      "Train Epoch: 93 [20736/225000 (9%)] Loss: 11190.171875\n",
      "Train Epoch: 93 [24832/225000 (11%)] Loss: 7914.726562\n",
      "Train Epoch: 93 [28928/225000 (13%)] Loss: 12353.181641\n",
      "Train Epoch: 93 [33024/225000 (15%)] Loss: 7806.908203\n",
      "Train Epoch: 93 [37120/225000 (16%)] Loss: 8078.560547\n",
      "Train Epoch: 93 [41216/225000 (18%)] Loss: 7953.966797\n",
      "Train Epoch: 93 [45312/225000 (20%)] Loss: 8110.201172\n",
      "Train Epoch: 93 [49408/225000 (22%)] Loss: 8051.037109\n",
      "Train Epoch: 93 [53504/225000 (24%)] Loss: 9786.828125\n",
      "Train Epoch: 93 [57600/225000 (26%)] Loss: 10523.076172\n",
      "Train Epoch: 93 [61696/225000 (27%)] Loss: 8066.554688\n",
      "Train Epoch: 93 [65792/225000 (29%)] Loss: 10567.160156\n",
      "Train Epoch: 93 [69888/225000 (31%)] Loss: 8140.669922\n",
      "Train Epoch: 93 [73984/225000 (33%)] Loss: 10717.613281\n",
      "Train Epoch: 93 [78080/225000 (35%)] Loss: 8059.064453\n",
      "Train Epoch: 93 [82176/225000 (37%)] Loss: 12132.517578\n",
      "Train Epoch: 93 [86272/225000 (38%)] Loss: 14962.187500\n",
      "Train Epoch: 93 [90368/225000 (40%)] Loss: 9620.177734\n",
      "Train Epoch: 93 [94464/225000 (42%)] Loss: 9625.441406\n",
      "Train Epoch: 93 [98560/225000 (44%)] Loss: 8141.380859\n",
      "Train Epoch: 93 [102656/225000 (46%)] Loss: 8038.289062\n",
      "Train Epoch: 93 [106752/225000 (47%)] Loss: 7892.531250\n",
      "Train Epoch: 93 [110848/225000 (49%)] Loss: 9649.308594\n",
      "Train Epoch: 93 [114944/225000 (51%)] Loss: 8147.000000\n",
      "Train Epoch: 93 [119040/225000 (53%)] Loss: 12910.455078\n",
      "Train Epoch: 93 [123136/225000 (55%)] Loss: 7994.412109\n",
      "Train Epoch: 93 [127232/225000 (57%)] Loss: 14104.083984\n",
      "Train Epoch: 93 [131328/225000 (58%)] Loss: 8136.363281\n",
      "Train Epoch: 93 [135424/225000 (60%)] Loss: 8174.570312\n",
      "Train Epoch: 93 [139520/225000 (62%)] Loss: 7990.373047\n",
      "Train Epoch: 93 [143616/225000 (64%)] Loss: 12364.160156\n",
      "Train Epoch: 93 [147712/225000 (66%)] Loss: 7865.419922\n",
      "Train Epoch: 93 [151808/225000 (67%)] Loss: 8007.205078\n",
      "Train Epoch: 93 [155904/225000 (69%)] Loss: 12176.732422\n",
      "Train Epoch: 93 [160000/225000 (71%)] Loss: 8003.476562\n",
      "Train Epoch: 93 [164096/225000 (73%)] Loss: 7854.111328\n",
      "Train Epoch: 93 [168192/225000 (75%)] Loss: 12327.861328\n",
      "Train Epoch: 93 [172288/225000 (77%)] Loss: 16058.367188\n",
      "Train Epoch: 93 [176384/225000 (78%)] Loss: 9471.869141\n",
      "Train Epoch: 93 [180480/225000 (80%)] Loss: 13712.769531\n",
      "Train Epoch: 93 [184576/225000 (82%)] Loss: 8013.781250\n",
      "Train Epoch: 93 [188672/225000 (84%)] Loss: 15451.726562\n",
      "Train Epoch: 93 [192768/225000 (86%)] Loss: 7964.732422\n",
      "Train Epoch: 93 [196864/225000 (87%)] Loss: 9574.835938\n",
      "Train Epoch: 93 [200960/225000 (89%)] Loss: 9602.507812\n",
      "Train Epoch: 93 [205056/225000 (91%)] Loss: 16284.384766\n",
      "Train Epoch: 93 [209152/225000 (93%)] Loss: 7986.523438\n",
      "Train Epoch: 93 [213248/225000 (95%)] Loss: 10469.023438\n",
      "Train Epoch: 93 [217344/225000 (97%)] Loss: 8060.755859\n",
      "Train Epoch: 93 [221440/225000 (98%)] Loss: 7893.468750\n",
      "    epoch          : 93\n",
      "    loss           : 9598.142260381115\n",
      "    val_loss       : 9332.249731942098\n",
      "Train Epoch: 94 [256/225000 (0%)] Loss: 10470.917969\n",
      "Train Epoch: 94 [4352/225000 (2%)] Loss: 8099.195312\n",
      "Train Epoch: 94 [8448/225000 (4%)] Loss: 8136.103516\n",
      "Train Epoch: 94 [12544/225000 (6%)] Loss: 7966.277344\n",
      "Train Epoch: 94 [16640/225000 (7%)] Loss: 8051.310547\n",
      "Train Epoch: 94 [20736/225000 (9%)] Loss: 18429.609375\n",
      "Train Epoch: 94 [24832/225000 (11%)] Loss: 7911.289062\n",
      "Train Epoch: 94 [28928/225000 (13%)] Loss: 9652.109375\n",
      "Train Epoch: 94 [33024/225000 (15%)] Loss: 9442.818359\n",
      "Train Epoch: 94 [37120/225000 (16%)] Loss: 8118.158203\n",
      "Train Epoch: 94 [41216/225000 (18%)] Loss: 10535.763672\n",
      "Train Epoch: 94 [45312/225000 (20%)] Loss: 8002.431641\n",
      "Train Epoch: 94 [49408/225000 (22%)] Loss: 8098.068359\n",
      "Train Epoch: 94 [53504/225000 (24%)] Loss: 12247.533203\n",
      "Train Epoch: 94 [57600/225000 (26%)] Loss: 13599.171875\n",
      "Train Epoch: 94 [61696/225000 (27%)] Loss: 12487.875000\n",
      "Train Epoch: 94 [65792/225000 (29%)] Loss: 8083.695312\n",
      "Train Epoch: 94 [69888/225000 (31%)] Loss: 10548.642578\n",
      "Train Epoch: 94 [73984/225000 (33%)] Loss: 10627.701172\n",
      "Train Epoch: 94 [78080/225000 (35%)] Loss: 8089.302734\n",
      "Train Epoch: 94 [82176/225000 (37%)] Loss: 15061.402344\n",
      "Train Epoch: 94 [86272/225000 (38%)] Loss: 8007.367188\n",
      "Train Epoch: 94 [90368/225000 (40%)] Loss: 7973.199219\n",
      "Train Epoch: 94 [94464/225000 (42%)] Loss: 7947.031250\n",
      "Train Epoch: 94 [98560/225000 (44%)] Loss: 9434.427734\n",
      "Train Epoch: 94 [102656/225000 (46%)] Loss: 7844.439453\n",
      "Train Epoch: 94 [106752/225000 (47%)] Loss: 7942.060547\n",
      "Train Epoch: 94 [110848/225000 (49%)] Loss: 9627.923828\n",
      "Train Epoch: 94 [114944/225000 (51%)] Loss: 8162.517578\n",
      "Train Epoch: 94 [119040/225000 (53%)] Loss: 8134.496094\n",
      "Train Epoch: 94 [123136/225000 (55%)] Loss: 8046.558594\n",
      "Train Epoch: 94 [127232/225000 (57%)] Loss: 8037.144531\n",
      "Train Epoch: 94 [131328/225000 (58%)] Loss: 8212.169922\n",
      "Train Epoch: 94 [135424/225000 (60%)] Loss: 8036.978516\n",
      "Train Epoch: 94 [139520/225000 (62%)] Loss: 8165.460938\n",
      "Train Epoch: 94 [143616/225000 (64%)] Loss: 13865.662109\n",
      "Train Epoch: 94 [147712/225000 (66%)] Loss: 12999.492188\n",
      "Train Epoch: 94 [151808/225000 (67%)] Loss: 8029.458984\n",
      "Train Epoch: 94 [155904/225000 (69%)] Loss: 9458.781250\n",
      "Train Epoch: 94 [160000/225000 (71%)] Loss: 8191.951172\n",
      "Train Epoch: 94 [164096/225000 (73%)] Loss: 12283.152344\n",
      "Train Epoch: 94 [168192/225000 (75%)] Loss: 9695.119141\n",
      "Train Epoch: 94 [172288/225000 (77%)] Loss: 13512.619141\n",
      "Train Epoch: 94 [176384/225000 (78%)] Loss: 8143.746094\n",
      "Train Epoch: 94 [180480/225000 (80%)] Loss: 13771.765625\n",
      "Train Epoch: 94 [184576/225000 (82%)] Loss: 10602.695312\n",
      "Train Epoch: 94 [188672/225000 (84%)] Loss: 8080.921875\n",
      "Train Epoch: 94 [192768/225000 (86%)] Loss: 7999.478516\n",
      "Train Epoch: 94 [196864/225000 (87%)] Loss: 8139.906250\n",
      "Train Epoch: 94 [200960/225000 (89%)] Loss: 12291.052734\n",
      "Train Epoch: 94 [205056/225000 (91%)] Loss: 8044.708984\n",
      "Train Epoch: 94 [209152/225000 (93%)] Loss: 8185.789062\n",
      "Train Epoch: 94 [213248/225000 (95%)] Loss: 10517.800781\n",
      "Train Epoch: 94 [217344/225000 (97%)] Loss: 12392.089844\n",
      "Train Epoch: 94 [221440/225000 (98%)] Loss: 8207.011719\n",
      "    epoch          : 94\n",
      "    loss           : 9537.90365694326\n",
      "    val_loss       : 9451.932182433653\n",
      "Train Epoch: 95 [256/225000 (0%)] Loss: 8147.447266\n",
      "Train Epoch: 95 [4352/225000 (2%)] Loss: 7963.255859\n",
      "Train Epoch: 95 [8448/225000 (4%)] Loss: 12102.636719\n",
      "Train Epoch: 95 [12544/225000 (6%)] Loss: 8018.150391\n",
      "Train Epoch: 95 [16640/225000 (7%)] Loss: 10429.558594\n",
      "Train Epoch: 95 [20736/225000 (9%)] Loss: 9496.125000\n",
      "Train Epoch: 95 [24832/225000 (11%)] Loss: 9523.958984\n",
      "Train Epoch: 95 [28928/225000 (13%)] Loss: 9765.972656\n",
      "Train Epoch: 95 [33024/225000 (15%)] Loss: 8205.349609\n",
      "Train Epoch: 95 [37120/225000 (16%)] Loss: 8214.082031\n",
      "Train Epoch: 95 [41216/225000 (18%)] Loss: 8173.611328\n",
      "Train Epoch: 95 [45312/225000 (20%)] Loss: 8110.796875\n",
      "Train Epoch: 95 [49408/225000 (22%)] Loss: 8079.273438\n",
      "Train Epoch: 95 [53504/225000 (24%)] Loss: 8113.484375\n",
      "Train Epoch: 95 [57600/225000 (26%)] Loss: 8198.906250\n",
      "Train Epoch: 95 [61696/225000 (27%)] Loss: 12329.376953\n",
      "Train Epoch: 95 [65792/225000 (29%)] Loss: 8039.556641\n",
      "Train Epoch: 95 [69888/225000 (31%)] Loss: 12242.886719\n",
      "Train Epoch: 95 [73984/225000 (33%)] Loss: 12572.599609\n",
      "Train Epoch: 95 [78080/225000 (35%)] Loss: 8079.347656\n",
      "Train Epoch: 95 [82176/225000 (37%)] Loss: 8024.437500\n",
      "Train Epoch: 95 [86272/225000 (38%)] Loss: 7997.558594\n",
      "Train Epoch: 95 [90368/225000 (40%)] Loss: 8058.451172\n",
      "Train Epoch: 95 [94464/225000 (42%)] Loss: 8050.728516\n",
      "Train Epoch: 95 [98560/225000 (44%)] Loss: 8103.427734\n",
      "Train Epoch: 95 [102656/225000 (46%)] Loss: 7969.166016\n",
      "Train Epoch: 95 [106752/225000 (47%)] Loss: 8227.937500\n",
      "Train Epoch: 95 [110848/225000 (49%)] Loss: 11857.152344\n",
      "Train Epoch: 95 [114944/225000 (51%)] Loss: 7952.765625\n",
      "Train Epoch: 95 [119040/225000 (53%)] Loss: 7931.751953\n",
      "Train Epoch: 95 [123136/225000 (55%)] Loss: 8219.492188\n",
      "Train Epoch: 95 [127232/225000 (57%)] Loss: 7991.396484\n",
      "Train Epoch: 95 [131328/225000 (58%)] Loss: 9578.771484\n",
      "Train Epoch: 95 [135424/225000 (60%)] Loss: 7962.494141\n",
      "Train Epoch: 95 [139520/225000 (62%)] Loss: 8088.603516\n",
      "Train Epoch: 95 [143616/225000 (64%)] Loss: 9545.394531\n",
      "Train Epoch: 95 [147712/225000 (66%)] Loss: 10574.763672\n",
      "Train Epoch: 95 [151808/225000 (67%)] Loss: 8199.451172\n",
      "Train Epoch: 95 [155904/225000 (69%)] Loss: 8255.707031\n",
      "Train Epoch: 95 [160000/225000 (71%)] Loss: 8106.792969\n",
      "Train Epoch: 95 [164096/225000 (73%)] Loss: 8015.146484\n",
      "Train Epoch: 95 [168192/225000 (75%)] Loss: 7970.230469\n",
      "Train Epoch: 95 [172288/225000 (77%)] Loss: 8141.185547\n",
      "Train Epoch: 95 [176384/225000 (78%)] Loss: 9568.386719\n",
      "Train Epoch: 95 [180480/225000 (80%)] Loss: 9612.804688\n",
      "Train Epoch: 95 [184576/225000 (82%)] Loss: 8118.626953\n",
      "Train Epoch: 95 [188672/225000 (84%)] Loss: 8100.033203\n",
      "Train Epoch: 95 [192768/225000 (86%)] Loss: 13910.353516\n",
      "Train Epoch: 95 [196864/225000 (87%)] Loss: 8129.878906\n",
      "Train Epoch: 95 [200960/225000 (89%)] Loss: 10422.281250\n",
      "Train Epoch: 95 [205056/225000 (91%)] Loss: 8138.521484\n",
      "Train Epoch: 95 [209152/225000 (93%)] Loss: 8115.296875\n",
      "Train Epoch: 95 [213248/225000 (95%)] Loss: 8034.398438\n",
      "Train Epoch: 95 [217344/225000 (97%)] Loss: 8120.851562\n",
      "Train Epoch: 95 [221440/225000 (98%)] Loss: 8107.726562\n",
      "    epoch          : 95\n",
      "    loss           : 9484.925923457053\n",
      "    val_loss       : 9654.524190092574\n",
      "Train Epoch: 96 [256/225000 (0%)] Loss: 8142.375000\n",
      "Train Epoch: 96 [4352/225000 (2%)] Loss: 7998.650391\n",
      "Train Epoch: 96 [8448/225000 (4%)] Loss: 10495.511719\n",
      "Train Epoch: 96 [12544/225000 (6%)] Loss: 9601.072266\n",
      "Train Epoch: 96 [16640/225000 (7%)] Loss: 8044.394531\n",
      "Train Epoch: 96 [20736/225000 (9%)] Loss: 9597.687500\n",
      "Train Epoch: 96 [24832/225000 (11%)] Loss: 12381.214844\n",
      "Train Epoch: 96 [28928/225000 (13%)] Loss: 15073.109375\n",
      "Train Epoch: 96 [33024/225000 (15%)] Loss: 13090.818359\n",
      "Train Epoch: 96 [37120/225000 (16%)] Loss: 8013.175781\n",
      "Train Epoch: 96 [41216/225000 (18%)] Loss: 9663.685547\n",
      "Train Epoch: 96 [45312/225000 (20%)] Loss: 14752.871094\n",
      "Train Epoch: 96 [49408/225000 (22%)] Loss: 8064.539062\n",
      "Train Epoch: 96 [53504/225000 (24%)] Loss: 10402.623047\n",
      "Train Epoch: 96 [57600/225000 (26%)] Loss: 8024.296875\n",
      "Train Epoch: 96 [61696/225000 (27%)] Loss: 10504.779297\n",
      "Train Epoch: 96 [65792/225000 (29%)] Loss: 12545.591797\n",
      "Train Epoch: 96 [69888/225000 (31%)] Loss: 8059.910156\n",
      "Train Epoch: 96 [73984/225000 (33%)] Loss: 10348.681641\n",
      "Train Epoch: 96 [78080/225000 (35%)] Loss: 13996.451172\n",
      "Train Epoch: 96 [82176/225000 (37%)] Loss: 8231.011719\n",
      "Train Epoch: 96 [86272/225000 (38%)] Loss: 10471.951172\n",
      "Train Epoch: 96 [90368/225000 (40%)] Loss: 8044.802734\n",
      "Train Epoch: 96 [94464/225000 (42%)] Loss: 8014.830078\n",
      "Train Epoch: 96 [98560/225000 (44%)] Loss: 8081.580078\n",
      "Train Epoch: 96 [102656/225000 (46%)] Loss: 9769.673828\n",
      "Train Epoch: 96 [106752/225000 (47%)] Loss: 7930.470703\n",
      "Train Epoch: 96 [110848/225000 (49%)] Loss: 7881.361328\n",
      "Train Epoch: 96 [114944/225000 (51%)] Loss: 16350.699219\n",
      "Train Epoch: 96 [119040/225000 (53%)] Loss: 8079.425781\n",
      "Train Epoch: 96 [123136/225000 (55%)] Loss: 8305.935547\n",
      "Train Epoch: 96 [127232/225000 (57%)] Loss: 8121.529297\n",
      "Train Epoch: 96 [131328/225000 (58%)] Loss: 8067.849609\n",
      "Train Epoch: 96 [135424/225000 (60%)] Loss: 8055.833984\n",
      "Train Epoch: 96 [139520/225000 (62%)] Loss: 7894.664062\n",
      "Train Epoch: 96 [143616/225000 (64%)] Loss: 8119.398438\n",
      "Train Epoch: 96 [147712/225000 (66%)] Loss: 12339.134766\n",
      "Train Epoch: 96 [151808/225000 (67%)] Loss: 12328.414062\n",
      "Train Epoch: 96 [155904/225000 (69%)] Loss: 8170.468750\n",
      "Train Epoch: 96 [160000/225000 (71%)] Loss: 8099.191406\n",
      "Train Epoch: 96 [164096/225000 (73%)] Loss: 8140.662109\n",
      "Train Epoch: 96 [168192/225000 (75%)] Loss: 13173.806641\n",
      "Train Epoch: 96 [172288/225000 (77%)] Loss: 7980.318359\n",
      "Train Epoch: 96 [176384/225000 (78%)] Loss: 7940.490234\n",
      "Train Epoch: 96 [180480/225000 (80%)] Loss: 8336.835938\n",
      "Train Epoch: 96 [184576/225000 (82%)] Loss: 13847.859375\n",
      "Train Epoch: 96 [188672/225000 (84%)] Loss: 14714.187500\n",
      "Train Epoch: 96 [192768/225000 (86%)] Loss: 10282.953125\n",
      "Train Epoch: 96 [196864/225000 (87%)] Loss: 7980.029297\n",
      "Train Epoch: 96 [200960/225000 (89%)] Loss: 7962.109375\n",
      "Train Epoch: 96 [205056/225000 (91%)] Loss: 8015.505859\n",
      "Train Epoch: 96 [209152/225000 (93%)] Loss: 8085.148438\n",
      "Train Epoch: 96 [213248/225000 (95%)] Loss: 9844.496094\n",
      "Train Epoch: 96 [217344/225000 (97%)] Loss: 11986.167969\n",
      "Train Epoch: 96 [221440/225000 (98%)] Loss: 9653.632812\n",
      "    epoch          : 96\n",
      "    loss           : 9603.886045488482\n",
      "    val_loss       : 9613.059784159368\n",
      "Train Epoch: 97 [256/225000 (0%)] Loss: 13814.742188\n",
      "Train Epoch: 97 [4352/225000 (2%)] Loss: 8021.623047\n",
      "Train Epoch: 97 [8448/225000 (4%)] Loss: 8149.021484\n",
      "Train Epoch: 97 [12544/225000 (6%)] Loss: 8045.953125\n",
      "Train Epoch: 97 [16640/225000 (7%)] Loss: 8128.871094\n",
      "Train Epoch: 97 [20736/225000 (9%)] Loss: 7954.531250\n",
      "Train Epoch: 97 [24832/225000 (11%)] Loss: 8163.806641\n",
      "Train Epoch: 97 [28928/225000 (13%)] Loss: 7926.986328\n",
      "Train Epoch: 97 [33024/225000 (15%)] Loss: 10607.400391\n",
      "Train Epoch: 97 [37120/225000 (16%)] Loss: 8032.156250\n",
      "Train Epoch: 97 [41216/225000 (18%)] Loss: 8102.228516\n",
      "Train Epoch: 97 [45312/225000 (20%)] Loss: 9597.257812\n",
      "Train Epoch: 97 [49408/225000 (22%)] Loss: 13839.595703\n",
      "Train Epoch: 97 [53504/225000 (24%)] Loss: 9688.892578\n",
      "Train Epoch: 97 [57600/225000 (26%)] Loss: 8209.628906\n",
      "Train Epoch: 97 [61696/225000 (27%)] Loss: 8216.601562\n",
      "Train Epoch: 97 [65792/225000 (29%)] Loss: 8186.955078\n",
      "Train Epoch: 97 [69888/225000 (31%)] Loss: 8073.375000\n",
      "Train Epoch: 97 [73984/225000 (33%)] Loss: 8053.974609\n",
      "Train Epoch: 97 [78080/225000 (35%)] Loss: 7753.380859\n",
      "Train Epoch: 97 [82176/225000 (37%)] Loss: 8162.876953\n",
      "Train Epoch: 97 [86272/225000 (38%)] Loss: 12044.906250\n",
      "Train Epoch: 97 [90368/225000 (40%)] Loss: 8059.484375\n",
      "Train Epoch: 97 [94464/225000 (42%)] Loss: 7952.544922\n",
      "Train Epoch: 97 [98560/225000 (44%)] Loss: 9919.912109\n",
      "Train Epoch: 97 [102656/225000 (46%)] Loss: 10510.023438\n",
      "Train Epoch: 97 [106752/225000 (47%)] Loss: 9472.818359\n",
      "Train Epoch: 97 [110848/225000 (49%)] Loss: 9473.875000\n",
      "Train Epoch: 97 [114944/225000 (51%)] Loss: 7927.611328\n",
      "Train Epoch: 97 [119040/225000 (53%)] Loss: 8181.234375\n",
      "Train Epoch: 97 [123136/225000 (55%)] Loss: 11058.617188\n",
      "Train Epoch: 97 [127232/225000 (57%)] Loss: 8331.005859\n",
      "Train Epoch: 97 [131328/225000 (58%)] Loss: 9640.041016\n",
      "Train Epoch: 97 [135424/225000 (60%)] Loss: 13628.121094\n",
      "Train Epoch: 97 [139520/225000 (62%)] Loss: 12058.349609\n",
      "Train Epoch: 97 [143616/225000 (64%)] Loss: 13519.419922\n",
      "Train Epoch: 97 [147712/225000 (66%)] Loss: 8161.927734\n",
      "Train Epoch: 97 [151808/225000 (67%)] Loss: 8122.183594\n",
      "Train Epoch: 97 [155904/225000 (69%)] Loss: 10458.832031\n",
      "Train Epoch: 97 [160000/225000 (71%)] Loss: 10458.443359\n",
      "Train Epoch: 97 [164096/225000 (73%)] Loss: 7922.585938\n",
      "Train Epoch: 97 [168192/225000 (75%)] Loss: 8054.253906\n",
      "Train Epoch: 97 [172288/225000 (77%)] Loss: 12461.736328\n",
      "Train Epoch: 97 [176384/225000 (78%)] Loss: 13826.005859\n",
      "Train Epoch: 97 [180480/225000 (80%)] Loss: 9616.630859\n",
      "Train Epoch: 97 [184576/225000 (82%)] Loss: 11972.046875\n",
      "Train Epoch: 97 [188672/225000 (84%)] Loss: 8195.222656\n",
      "Train Epoch: 97 [192768/225000 (86%)] Loss: 8159.720703\n",
      "Train Epoch: 97 [196864/225000 (87%)] Loss: 7989.017578\n",
      "Train Epoch: 97 [200960/225000 (89%)] Loss: 8222.669922\n",
      "Train Epoch: 97 [205056/225000 (91%)] Loss: 10528.146484\n",
      "Train Epoch: 97 [209152/225000 (93%)] Loss: 11380.005859\n",
      "Train Epoch: 97 [213248/225000 (95%)] Loss: 7926.013672\n",
      "Train Epoch: 97 [217344/225000 (97%)] Loss: 8102.019531\n",
      "Train Epoch: 97 [221440/225000 (98%)] Loss: 7986.560547\n",
      "    epoch          : 97\n",
      "    loss           : 9487.8447565593\n",
      "    val_loss       : 9749.609247090864\n",
      "Train Epoch: 98 [256/225000 (0%)] Loss: 8141.109375\n",
      "Train Epoch: 98 [4352/225000 (2%)] Loss: 9589.824219\n",
      "Train Epoch: 98 [8448/225000 (4%)] Loss: 11308.726562\n",
      "Train Epoch: 98 [12544/225000 (6%)] Loss: 12544.843750\n",
      "Train Epoch: 98 [16640/225000 (7%)] Loss: 12536.865234\n",
      "Train Epoch: 98 [20736/225000 (9%)] Loss: 8094.818359\n",
      "Train Epoch: 98 [24832/225000 (11%)] Loss: 8049.835938\n",
      "Train Epoch: 98 [28928/225000 (13%)] Loss: 15315.628906\n",
      "Train Epoch: 98 [33024/225000 (15%)] Loss: 9650.884766\n",
      "Train Epoch: 98 [37120/225000 (16%)] Loss: 15337.226562\n",
      "Train Epoch: 98 [41216/225000 (18%)] Loss: 12852.816406\n",
      "Train Epoch: 98 [45312/225000 (20%)] Loss: 9609.357422\n",
      "Train Epoch: 98 [49408/225000 (22%)] Loss: 7944.656250\n",
      "Train Epoch: 98 [53504/225000 (24%)] Loss: 11306.251953\n",
      "Train Epoch: 98 [57600/225000 (26%)] Loss: 7816.304688\n",
      "Train Epoch: 98 [61696/225000 (27%)] Loss: 8125.724609\n",
      "Train Epoch: 98 [65792/225000 (29%)] Loss: 12309.503906\n",
      "Train Epoch: 98 [69888/225000 (31%)] Loss: 7904.533203\n",
      "Train Epoch: 98 [73984/225000 (33%)] Loss: 8050.355469\n",
      "Train Epoch: 98 [78080/225000 (35%)] Loss: 8056.654297\n",
      "Train Epoch: 98 [82176/225000 (37%)] Loss: 8050.302734\n",
      "Train Epoch: 98 [86272/225000 (38%)] Loss: 7975.544922\n",
      "Train Epoch: 98 [90368/225000 (40%)] Loss: 8076.777344\n",
      "Train Epoch: 98 [94464/225000 (42%)] Loss: 15905.595703\n",
      "Train Epoch: 98 [98560/225000 (44%)] Loss: 8302.500000\n",
      "Train Epoch: 98 [102656/225000 (46%)] Loss: 7968.031250\n",
      "Train Epoch: 98 [106752/225000 (47%)] Loss: 10587.888672\n",
      "Train Epoch: 98 [110848/225000 (49%)] Loss: 10603.121094\n",
      "Train Epoch: 98 [114944/225000 (51%)] Loss: 7930.226562\n",
      "Train Epoch: 98 [119040/225000 (53%)] Loss: 8285.671875\n",
      "Train Epoch: 98 [123136/225000 (55%)] Loss: 13807.896484\n",
      "Train Epoch: 98 [127232/225000 (57%)] Loss: 8176.728516\n",
      "Train Epoch: 98 [131328/225000 (58%)] Loss: 8133.787109\n",
      "Train Epoch: 98 [135424/225000 (60%)] Loss: 8133.498047\n",
      "Train Epoch: 98 [139520/225000 (62%)] Loss: 8022.845703\n",
      "Train Epoch: 98 [143616/225000 (64%)] Loss: 7889.382812\n",
      "Train Epoch: 98 [147712/225000 (66%)] Loss: 9727.804688\n",
      "Train Epoch: 98 [151808/225000 (67%)] Loss: 12253.216797\n",
      "Train Epoch: 98 [155904/225000 (69%)] Loss: 9666.972656\n",
      "Train Epoch: 98 [160000/225000 (71%)] Loss: 7888.232422\n",
      "Train Epoch: 98 [164096/225000 (73%)] Loss: 8092.425781\n",
      "Train Epoch: 98 [168192/225000 (75%)] Loss: 9670.822266\n",
      "Train Epoch: 98 [172288/225000 (77%)] Loss: 9292.388672\n",
      "Train Epoch: 98 [176384/225000 (78%)] Loss: 16383.494141\n",
      "Train Epoch: 98 [180480/225000 (80%)] Loss: 8180.177734\n",
      "Train Epoch: 98 [184576/225000 (82%)] Loss: 8058.986328\n",
      "Train Epoch: 98 [188672/225000 (84%)] Loss: 7978.097656\n",
      "Train Epoch: 98 [192768/225000 (86%)] Loss: 9639.296875\n",
      "Train Epoch: 98 [196864/225000 (87%)] Loss: 9433.757812\n",
      "Train Epoch: 98 [200960/225000 (89%)] Loss: 8283.257812\n",
      "Train Epoch: 98 [205056/225000 (91%)] Loss: 8101.582031\n",
      "Train Epoch: 98 [209152/225000 (93%)] Loss: 8177.175781\n",
      "Train Epoch: 98 [213248/225000 (95%)] Loss: 8252.443359\n",
      "Train Epoch: 98 [217344/225000 (97%)] Loss: 8146.896484\n",
      "Train Epoch: 98 [221440/225000 (98%)] Loss: 13703.693359\n",
      "    epoch          : 98\n",
      "    loss           : 9535.534662969283\n",
      "    val_loss       : 9838.558801728852\n",
      "Train Epoch: 99 [256/225000 (0%)] Loss: 7916.316406\n",
      "Train Epoch: 99 [4352/225000 (2%)] Loss: 8052.478516\n",
      "Train Epoch: 99 [8448/225000 (4%)] Loss: 7944.689453\n",
      "Train Epoch: 99 [12544/225000 (6%)] Loss: 9496.353516\n",
      "Train Epoch: 99 [16640/225000 (7%)] Loss: 8099.195312\n",
      "Train Epoch: 99 [20736/225000 (9%)] Loss: 13670.671875\n",
      "Train Epoch: 99 [24832/225000 (11%)] Loss: 8010.167969\n",
      "Train Epoch: 99 [28928/225000 (13%)] Loss: 9613.750000\n",
      "Train Epoch: 99 [33024/225000 (15%)] Loss: 8241.826172\n",
      "Train Epoch: 99 [37120/225000 (16%)] Loss: 13703.558594\n",
      "Train Epoch: 99 [41216/225000 (18%)] Loss: 10756.859375\n",
      "Train Epoch: 99 [45312/225000 (20%)] Loss: 13811.841797\n",
      "Train Epoch: 99 [49408/225000 (22%)] Loss: 9466.105469\n",
      "Train Epoch: 99 [53504/225000 (24%)] Loss: 9505.291016\n",
      "Train Epoch: 99 [57600/225000 (26%)] Loss: 8110.816406\n",
      "Train Epoch: 99 [61696/225000 (27%)] Loss: 7917.595703\n",
      "Train Epoch: 99 [65792/225000 (29%)] Loss: 12337.453125\n",
      "Train Epoch: 99 [69888/225000 (31%)] Loss: 7921.355469\n",
      "Train Epoch: 99 [73984/225000 (33%)] Loss: 11211.982422\n",
      "Train Epoch: 99 [78080/225000 (35%)] Loss: 10632.884766\n",
      "Train Epoch: 99 [82176/225000 (37%)] Loss: 8138.984375\n",
      "Train Epoch: 99 [86272/225000 (38%)] Loss: 9888.453125\n",
      "Train Epoch: 99 [90368/225000 (40%)] Loss: 8203.705078\n",
      "Train Epoch: 99 [94464/225000 (42%)] Loss: 8112.617188\n",
      "Train Epoch: 99 [98560/225000 (44%)] Loss: 14043.199219\n",
      "Train Epoch: 99 [102656/225000 (46%)] Loss: 8080.640625\n",
      "Train Epoch: 99 [106752/225000 (47%)] Loss: 9573.648438\n",
      "Train Epoch: 99 [110848/225000 (49%)] Loss: 7983.888672\n",
      "Train Epoch: 99 [114944/225000 (51%)] Loss: 8013.226562\n",
      "Train Epoch: 99 [119040/225000 (53%)] Loss: 10411.728516\n",
      "Train Epoch: 99 [123136/225000 (55%)] Loss: 10974.505859\n",
      "Train Epoch: 99 [127232/225000 (57%)] Loss: 10685.111328\n",
      "Train Epoch: 99 [131328/225000 (58%)] Loss: 8029.583984\n",
      "Train Epoch: 99 [135424/225000 (60%)] Loss: 10524.976562\n",
      "Train Epoch: 99 [139520/225000 (62%)] Loss: 7926.396484\n",
      "Train Epoch: 99 [143616/225000 (64%)] Loss: 13877.150391\n",
      "Train Epoch: 99 [147712/225000 (66%)] Loss: 8184.238281\n",
      "Train Epoch: 99 [151808/225000 (67%)] Loss: 8032.669922\n",
      "Train Epoch: 99 [155904/225000 (69%)] Loss: 8035.972656\n",
      "Train Epoch: 99 [160000/225000 (71%)] Loss: 8147.236328\n",
      "Train Epoch: 99 [164096/225000 (73%)] Loss: 8025.935547\n",
      "Train Epoch: 99 [168192/225000 (75%)] Loss: 8128.941406\n",
      "Train Epoch: 99 [172288/225000 (77%)] Loss: 10600.232422\n",
      "Train Epoch: 99 [176384/225000 (78%)] Loss: 12234.980469\n",
      "Train Epoch: 99 [180480/225000 (80%)] Loss: 11004.544922\n",
      "Train Epoch: 99 [184576/225000 (82%)] Loss: 8268.554688\n",
      "Train Epoch: 99 [188672/225000 (84%)] Loss: 10514.031250\n",
      "Train Epoch: 99 [192768/225000 (86%)] Loss: 8086.601562\n",
      "Train Epoch: 99 [196864/225000 (87%)] Loss: 7991.494141\n",
      "Train Epoch: 99 [200960/225000 (89%)] Loss: 9558.234375\n",
      "Train Epoch: 99 [205056/225000 (91%)] Loss: 8070.708984\n",
      "Train Epoch: 99 [209152/225000 (93%)] Loss: 12768.544922\n",
      "Train Epoch: 99 [213248/225000 (95%)] Loss: 8116.298828\n",
      "Train Epoch: 99 [217344/225000 (97%)] Loss: 8243.525391\n",
      "Train Epoch: 99 [221440/225000 (98%)] Loss: 7983.240234\n",
      "    epoch          : 99\n",
      "    loss           : 9654.526774921786\n",
      "    val_loss       : 9440.615355277549\n",
      "Train Epoch: 100 [256/225000 (0%)] Loss: 8229.316406\n",
      "Train Epoch: 100 [4352/225000 (2%)] Loss: 8092.343750\n",
      "Train Epoch: 100 [8448/225000 (4%)] Loss: 7957.207031\n",
      "Train Epoch: 100 [12544/225000 (6%)] Loss: 9726.896484\n",
      "Train Epoch: 100 [16640/225000 (7%)] Loss: 9760.353516\n",
      "Train Epoch: 100 [20736/225000 (9%)] Loss: 7927.025391\n",
      "Train Epoch: 100 [24832/225000 (11%)] Loss: 7894.066406\n",
      "Train Epoch: 100 [28928/225000 (13%)] Loss: 14554.898438\n",
      "Train Epoch: 100 [33024/225000 (15%)] Loss: 15490.013672\n",
      "Train Epoch: 100 [37120/225000 (16%)] Loss: 10341.220703\n",
      "Train Epoch: 100 [41216/225000 (18%)] Loss: 8459.148438\n",
      "Train Epoch: 100 [45312/225000 (20%)] Loss: 9471.447266\n",
      "Train Epoch: 100 [49408/225000 (22%)] Loss: 8073.474609\n",
      "Train Epoch: 100 [53504/225000 (24%)] Loss: 8273.488281\n",
      "Train Epoch: 100 [57600/225000 (26%)] Loss: 13415.927734\n",
      "Train Epoch: 100 [61696/225000 (27%)] Loss: 13466.609375\n",
      "Train Epoch: 100 [65792/225000 (29%)] Loss: 10625.759766\n",
      "Train Epoch: 100 [69888/225000 (31%)] Loss: 8347.765625\n",
      "Train Epoch: 100 [73984/225000 (33%)] Loss: 9595.650391\n",
      "Train Epoch: 100 [78080/225000 (35%)] Loss: 13738.398438\n",
      "Train Epoch: 100 [82176/225000 (37%)] Loss: 8142.091797\n",
      "Train Epoch: 100 [86272/225000 (38%)] Loss: 8207.599609\n",
      "Train Epoch: 100 [90368/225000 (40%)] Loss: 8017.277344\n",
      "Train Epoch: 100 [94464/225000 (42%)] Loss: 15078.298828\n",
      "Train Epoch: 100 [98560/225000 (44%)] Loss: 10306.722656\n",
      "Train Epoch: 100 [102656/225000 (46%)] Loss: 9772.914062\n",
      "Train Epoch: 100 [106752/225000 (47%)] Loss: 7905.369141\n",
      "Train Epoch: 100 [110848/225000 (49%)] Loss: 8013.826172\n",
      "Train Epoch: 100 [114944/225000 (51%)] Loss: 10445.552734\n",
      "Train Epoch: 100 [119040/225000 (53%)] Loss: 8169.433594\n",
      "Train Epoch: 100 [123136/225000 (55%)] Loss: 9709.279297\n",
      "Train Epoch: 100 [127232/225000 (57%)] Loss: 12351.164062\n",
      "Train Epoch: 100 [131328/225000 (58%)] Loss: 8277.886719\n",
      "Train Epoch: 100 [135424/225000 (60%)] Loss: 10529.990234\n",
      "Train Epoch: 100 [139520/225000 (62%)] Loss: 8166.695312\n",
      "Train Epoch: 100 [143616/225000 (64%)] Loss: 8010.099609\n",
      "Train Epoch: 100 [147712/225000 (66%)] Loss: 13861.037109\n",
      "Train Epoch: 100 [151808/225000 (67%)] Loss: 8161.833984\n",
      "Train Epoch: 100 [155904/225000 (69%)] Loss: 8051.072266\n",
      "Train Epoch: 100 [160000/225000 (71%)] Loss: 10490.943359\n",
      "Train Epoch: 100 [164096/225000 (73%)] Loss: 8069.605469\n",
      "Train Epoch: 100 [168192/225000 (75%)] Loss: 8120.357422\n",
      "Train Epoch: 100 [172288/225000 (77%)] Loss: 13807.386719\n",
      "Train Epoch: 100 [176384/225000 (78%)] Loss: 8312.470703\n",
      "Train Epoch: 100 [180480/225000 (80%)] Loss: 8145.222656\n",
      "Train Epoch: 100 [184576/225000 (82%)] Loss: 8104.392578\n",
      "Train Epoch: 100 [188672/225000 (84%)] Loss: 8289.595703\n",
      "Train Epoch: 100 [192768/225000 (86%)] Loss: 8047.966797\n",
      "Train Epoch: 100 [196864/225000 (87%)] Loss: 9607.046875\n",
      "Train Epoch: 100 [200960/225000 (89%)] Loss: 8032.064453\n",
      "Train Epoch: 100 [205056/225000 (91%)] Loss: 12724.462891\n",
      "Train Epoch: 100 [209152/225000 (93%)] Loss: 14356.652344\n",
      "Train Epoch: 100 [213248/225000 (95%)] Loss: 8296.070312\n",
      "Train Epoch: 100 [217344/225000 (97%)] Loss: 8215.732422\n",
      "Train Epoch: 100 [221440/225000 (98%)] Loss: 9614.281250\n",
      "    epoch          : 100\n",
      "    loss           : 9642.109604975469\n",
      "    val_loss       : 9090.134711078235\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0103_171400/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [256/225000 (0%)] Loss: 13548.636719\n",
      "Train Epoch: 101 [4352/225000 (2%)] Loss: 10611.859375\n",
      "Train Epoch: 101 [8448/225000 (4%)] Loss: 14842.224609\n",
      "Train Epoch: 101 [12544/225000 (6%)] Loss: 9432.671875\n",
      "Train Epoch: 101 [16640/225000 (7%)] Loss: 13767.867188\n",
      "Train Epoch: 101 [20736/225000 (9%)] Loss: 9587.470703\n",
      "Train Epoch: 101 [24832/225000 (11%)] Loss: 12252.976562\n",
      "Train Epoch: 101 [28928/225000 (13%)] Loss: 8061.820312\n",
      "Train Epoch: 101 [33024/225000 (15%)] Loss: 8123.855469\n",
      "Train Epoch: 101 [37120/225000 (16%)] Loss: 8187.548828\n",
      "Train Epoch: 101 [41216/225000 (18%)] Loss: 9745.298828\n",
      "Train Epoch: 101 [45312/225000 (20%)] Loss: 8046.240234\n",
      "Train Epoch: 101 [49408/225000 (22%)] Loss: 14912.328125\n",
      "Train Epoch: 101 [53504/225000 (24%)] Loss: 11026.212891\n",
      "Train Epoch: 101 [57600/225000 (26%)] Loss: 7914.117188\n",
      "Train Epoch: 101 [61696/225000 (27%)] Loss: 8084.343750\n",
      "Train Epoch: 101 [65792/225000 (29%)] Loss: 7988.724609\n",
      "Train Epoch: 101 [69888/225000 (31%)] Loss: 10505.171875\n",
      "Train Epoch: 101 [73984/225000 (33%)] Loss: 8038.736328\n",
      "Train Epoch: 101 [78080/225000 (35%)] Loss: 17852.613281\n",
      "Train Epoch: 101 [82176/225000 (37%)] Loss: 10433.539062\n",
      "Train Epoch: 101 [86272/225000 (38%)] Loss: 7831.027344\n",
      "Train Epoch: 101 [90368/225000 (40%)] Loss: 19463.468750\n",
      "Train Epoch: 101 [94464/225000 (42%)] Loss: 7948.058594\n",
      "Train Epoch: 101 [98560/225000 (44%)] Loss: 12878.466797\n",
      "Train Epoch: 101 [102656/225000 (46%)] Loss: 9642.216797\n",
      "Train Epoch: 101 [106752/225000 (47%)] Loss: 12518.851562\n",
      "Train Epoch: 101 [110848/225000 (49%)] Loss: 7920.787109\n",
      "Train Epoch: 101 [114944/225000 (51%)] Loss: 8014.617188\n",
      "Train Epoch: 101 [119040/225000 (53%)] Loss: 8023.695312\n",
      "Train Epoch: 101 [123136/225000 (55%)] Loss: 8126.136719\n",
      "Train Epoch: 101 [127232/225000 (57%)] Loss: 10596.525391\n",
      "Train Epoch: 101 [131328/225000 (58%)] Loss: 7859.187500\n",
      "Train Epoch: 101 [135424/225000 (60%)] Loss: 13681.005859\n",
      "Train Epoch: 101 [139520/225000 (62%)] Loss: 8133.162109\n",
      "Train Epoch: 101 [143616/225000 (64%)] Loss: 10646.189453\n",
      "Train Epoch: 101 [147712/225000 (66%)] Loss: 10723.570312\n",
      "Train Epoch: 101 [151808/225000 (67%)] Loss: 9503.539062\n",
      "Train Epoch: 101 [155904/225000 (69%)] Loss: 10609.914062\n",
      "Train Epoch: 101 [160000/225000 (71%)] Loss: 12201.154297\n",
      "Train Epoch: 101 [164096/225000 (73%)] Loss: 8174.980469\n",
      "Train Epoch: 101 [168192/225000 (75%)] Loss: 9553.917969\n",
      "Train Epoch: 101 [172288/225000 (77%)] Loss: 13934.705078\n",
      "Train Epoch: 101 [176384/225000 (78%)] Loss: 7969.181641\n",
      "Train Epoch: 101 [180480/225000 (80%)] Loss: 9505.451172\n",
      "Train Epoch: 101 [184576/225000 (82%)] Loss: 8010.164062\n",
      "Train Epoch: 101 [188672/225000 (84%)] Loss: 8124.445312\n",
      "Train Epoch: 101 [192768/225000 (86%)] Loss: 13954.599609\n",
      "Train Epoch: 101 [196864/225000 (87%)] Loss: 10590.607422\n",
      "Train Epoch: 101 [200960/225000 (89%)] Loss: 12446.578125\n",
      "Train Epoch: 101 [205056/225000 (91%)] Loss: 7906.783203\n",
      "Train Epoch: 101 [209152/225000 (93%)] Loss: 18654.738281\n",
      "Train Epoch: 101 [213248/225000 (95%)] Loss: 12383.037109\n",
      "Train Epoch: 101 [217344/225000 (97%)] Loss: 8150.238281\n",
      "Train Epoch: 101 [221440/225000 (98%)] Loss: 8128.902344\n",
      "    epoch          : 101\n",
      "    loss           : 9533.943388260808\n",
      "    val_loss       : 9572.07655749029\n",
      "Train Epoch: 102 [256/225000 (0%)] Loss: 14692.761719\n",
      "Train Epoch: 102 [4352/225000 (2%)] Loss: 7955.767578\n",
      "Train Epoch: 102 [8448/225000 (4%)] Loss: 8001.048828\n",
      "Train Epoch: 102 [12544/225000 (6%)] Loss: 10575.160156\n",
      "Train Epoch: 102 [16640/225000 (7%)] Loss: 10581.355469\n",
      "Train Epoch: 102 [20736/225000 (9%)] Loss: 9524.060547\n",
      "Train Epoch: 102 [24832/225000 (11%)] Loss: 7944.656250\n",
      "Train Epoch: 102 [28928/225000 (13%)] Loss: 8131.683594\n",
      "Train Epoch: 102 [33024/225000 (15%)] Loss: 8005.490234\n",
      "Train Epoch: 102 [37120/225000 (16%)] Loss: 10268.130859\n",
      "Train Epoch: 102 [41216/225000 (18%)] Loss: 14072.347656\n",
      "Train Epoch: 102 [45312/225000 (20%)] Loss: 12430.339844\n",
      "Train Epoch: 102 [49408/225000 (22%)] Loss: 8065.402344\n",
      "Train Epoch: 102 [53504/225000 (24%)] Loss: 8114.228516\n",
      "Train Epoch: 102 [57600/225000 (26%)] Loss: 8199.041016\n",
      "Train Epoch: 102 [61696/225000 (27%)] Loss: 8058.501953\n",
      "Train Epoch: 102 [65792/225000 (29%)] Loss: 7865.427734\n",
      "Train Epoch: 102 [69888/225000 (31%)] Loss: 7828.074219\n",
      "Train Epoch: 102 [73984/225000 (33%)] Loss: 8452.414062\n",
      "Train Epoch: 102 [78080/225000 (35%)] Loss: 13808.093750\n",
      "Train Epoch: 102 [82176/225000 (37%)] Loss: 8093.083984\n",
      "Train Epoch: 102 [86272/225000 (38%)] Loss: 8069.238281\n",
      "Train Epoch: 102 [90368/225000 (40%)] Loss: 8148.166016\n",
      "Train Epoch: 102 [94464/225000 (42%)] Loss: 8132.937500\n",
      "Train Epoch: 102 [98560/225000 (44%)] Loss: 10528.916016\n",
      "Train Epoch: 102 [102656/225000 (46%)] Loss: 8021.806641\n",
      "Train Epoch: 102 [106752/225000 (47%)] Loss: 9742.701172\n",
      "Train Epoch: 102 [110848/225000 (49%)] Loss: 10562.265625\n",
      "Train Epoch: 102 [114944/225000 (51%)] Loss: 13082.310547\n",
      "Train Epoch: 102 [119040/225000 (53%)] Loss: 10600.125000\n",
      "Train Epoch: 102 [123136/225000 (55%)] Loss: 10527.728516\n",
      "Train Epoch: 102 [127232/225000 (57%)] Loss: 10587.322266\n",
      "Train Epoch: 102 [131328/225000 (58%)] Loss: 8099.556641\n",
      "Train Epoch: 102 [135424/225000 (60%)] Loss: 10541.808594\n",
      "Train Epoch: 102 [139520/225000 (62%)] Loss: 9495.500000\n",
      "Train Epoch: 102 [143616/225000 (64%)] Loss: 8014.785156\n",
      "Train Epoch: 102 [147712/225000 (66%)] Loss: 7988.208984\n",
      "Train Epoch: 102 [151808/225000 (67%)] Loss: 8262.322266\n",
      "Train Epoch: 102 [155904/225000 (69%)] Loss: 8342.900391\n",
      "Train Epoch: 102 [160000/225000 (71%)] Loss: 15430.773438\n",
      "Train Epoch: 102 [164096/225000 (73%)] Loss: 16169.056641\n",
      "Train Epoch: 102 [168192/225000 (75%)] Loss: 8042.943359\n",
      "Train Epoch: 102 [172288/225000 (77%)] Loss: 8129.289062\n",
      "Train Epoch: 102 [176384/225000 (78%)] Loss: 7908.410156\n",
      "Train Epoch: 102 [180480/225000 (80%)] Loss: 8195.746094\n",
      "Train Epoch: 102 [184576/225000 (82%)] Loss: 13665.093750\n",
      "Train Epoch: 102 [188672/225000 (84%)] Loss: 8293.478516\n",
      "Train Epoch: 102 [192768/225000 (86%)] Loss: 8094.050781\n",
      "Train Epoch: 102 [196864/225000 (87%)] Loss: 10556.949219\n",
      "Train Epoch: 102 [200960/225000 (89%)] Loss: 13611.443359\n",
      "Train Epoch: 102 [205056/225000 (91%)] Loss: 9629.183594\n",
      "Train Epoch: 102 [209152/225000 (93%)] Loss: 9558.984375\n",
      "Train Epoch: 102 [213248/225000 (95%)] Loss: 8242.734375\n",
      "Train Epoch: 102 [217344/225000 (97%)] Loss: 10986.201172\n",
      "Train Epoch: 102 [221440/225000 (98%)] Loss: 8224.515625\n",
      "    epoch          : 102\n",
      "    loss           : 9531.21706573521\n",
      "    val_loss       : 9271.004019129034\n",
      "Train Epoch: 103 [256/225000 (0%)] Loss: 8116.730469\n",
      "Train Epoch: 103 [4352/225000 (2%)] Loss: 8192.201172\n",
      "Train Epoch: 103 [8448/225000 (4%)] Loss: 8107.830078\n",
      "Train Epoch: 103 [12544/225000 (6%)] Loss: 12176.769531\n",
      "Train Epoch: 103 [16640/225000 (7%)] Loss: 7982.972656\n",
      "Train Epoch: 103 [20736/225000 (9%)] Loss: 8028.187500\n",
      "Train Epoch: 103 [24832/225000 (11%)] Loss: 8040.255859\n",
      "Train Epoch: 103 [28928/225000 (13%)] Loss: 13951.421875\n",
      "Train Epoch: 103 [33024/225000 (15%)] Loss: 13512.236328\n",
      "Train Epoch: 103 [37120/225000 (16%)] Loss: 10344.019531\n",
      "Train Epoch: 103 [41216/225000 (18%)] Loss: 13739.519531\n",
      "Train Epoch: 103 [45312/225000 (20%)] Loss: 9567.308594\n",
      "Train Epoch: 103 [49408/225000 (22%)] Loss: 8123.929688\n",
      "Train Epoch: 103 [53504/225000 (24%)] Loss: 8119.302734\n",
      "Train Epoch: 103 [57600/225000 (26%)] Loss: 13124.828125\n",
      "Train Epoch: 103 [61696/225000 (27%)] Loss: 9780.318359\n",
      "Train Epoch: 103 [65792/225000 (29%)] Loss: 10620.015625\n",
      "Train Epoch: 103 [69888/225000 (31%)] Loss: 9489.916016\n",
      "Train Epoch: 103 [73984/225000 (33%)] Loss: 8090.216797\n",
      "Train Epoch: 103 [78080/225000 (35%)] Loss: 8154.263672\n",
      "Train Epoch: 103 [82176/225000 (37%)] Loss: 7914.341797\n",
      "Train Epoch: 103 [86272/225000 (38%)] Loss: 8035.617188\n",
      "Train Epoch: 103 [90368/225000 (40%)] Loss: 9755.623047\n",
      "Train Epoch: 103 [94464/225000 (42%)] Loss: 8054.171875\n",
      "Train Epoch: 103 [98560/225000 (44%)] Loss: 7980.716797\n",
      "Train Epoch: 103 [102656/225000 (46%)] Loss: 11406.175781\n",
      "Train Epoch: 103 [106752/225000 (47%)] Loss: 13088.558594\n",
      "Train Epoch: 103 [110848/225000 (49%)] Loss: 13262.183594\n",
      "Train Epoch: 103 [114944/225000 (51%)] Loss: 8586.164062\n",
      "Train Epoch: 103 [119040/225000 (53%)] Loss: 8075.916016\n",
      "Train Epoch: 103 [123136/225000 (55%)] Loss: 8227.521484\n",
      "Train Epoch: 103 [127232/225000 (57%)] Loss: 8155.230469\n",
      "Train Epoch: 103 [131328/225000 (58%)] Loss: 8091.904297\n",
      "Train Epoch: 103 [135424/225000 (60%)] Loss: 8308.742188\n",
      "Train Epoch: 103 [139520/225000 (62%)] Loss: 8084.400391\n",
      "Train Epoch: 103 [143616/225000 (64%)] Loss: 9752.259766\n",
      "Train Epoch: 103 [147712/225000 (66%)] Loss: 10406.257812\n",
      "Train Epoch: 103 [151808/225000 (67%)] Loss: 7895.404297\n",
      "Train Epoch: 103 [155904/225000 (69%)] Loss: 8026.578125\n",
      "Train Epoch: 103 [160000/225000 (71%)] Loss: 7993.128906\n",
      "Train Epoch: 103 [164096/225000 (73%)] Loss: 10529.482422\n",
      "Train Epoch: 103 [168192/225000 (75%)] Loss: 8103.755859\n",
      "Train Epoch: 103 [172288/225000 (77%)] Loss: 8055.171875\n",
      "Train Epoch: 103 [176384/225000 (78%)] Loss: 8002.255859\n",
      "Train Epoch: 103 [180480/225000 (80%)] Loss: 8209.066406\n",
      "Train Epoch: 103 [184576/225000 (82%)] Loss: 10381.078125\n",
      "Train Epoch: 103 [188672/225000 (84%)] Loss: 8180.294922\n",
      "Train Epoch: 103 [192768/225000 (86%)] Loss: 7985.675781\n",
      "Train Epoch: 103 [196864/225000 (87%)] Loss: 8142.871094\n",
      "Train Epoch: 103 [200960/225000 (89%)] Loss: 10276.519531\n",
      "Train Epoch: 103 [205056/225000 (91%)] Loss: 9614.642578\n",
      "Train Epoch: 103 [209152/225000 (93%)] Loss: 8157.642578\n",
      "Train Epoch: 103 [213248/225000 (95%)] Loss: 17630.134766\n",
      "Train Epoch: 103 [217344/225000 (97%)] Loss: 7981.925781\n",
      "Train Epoch: 103 [221440/225000 (98%)] Loss: 10473.765625\n",
      "    epoch          : 103\n",
      "    loss           : 9648.23913004835\n",
      "    val_loss       : 9259.712359795765\n",
      "Train Epoch: 104 [256/225000 (0%)] Loss: 13544.058594\n",
      "Train Epoch: 104 [4352/225000 (2%)] Loss: 7980.232422\n",
      "Train Epoch: 104 [8448/225000 (4%)] Loss: 8116.566406\n",
      "Train Epoch: 104 [12544/225000 (6%)] Loss: 8048.476562\n",
      "Train Epoch: 104 [16640/225000 (7%)] Loss: 7932.222656\n",
      "Train Epoch: 104 [20736/225000 (9%)] Loss: 8249.335938\n",
      "Train Epoch: 104 [24832/225000 (11%)] Loss: 14732.580078\n",
      "Train Epoch: 104 [28928/225000 (13%)] Loss: 8167.742188\n",
      "Train Epoch: 104 [33024/225000 (15%)] Loss: 8163.523438\n",
      "Train Epoch: 104 [37120/225000 (16%)] Loss: 9561.261719\n",
      "Train Epoch: 104 [41216/225000 (18%)] Loss: 13230.792969\n",
      "Train Epoch: 104 [45312/225000 (20%)] Loss: 12929.000000\n",
      "Train Epoch: 104 [49408/225000 (22%)] Loss: 8205.468750\n",
      "Train Epoch: 104 [53504/225000 (24%)] Loss: 9400.308594\n",
      "Train Epoch: 104 [57600/225000 (26%)] Loss: 13747.384766\n",
      "Train Epoch: 104 [61696/225000 (27%)] Loss: 12767.281250\n",
      "Train Epoch: 104 [65792/225000 (29%)] Loss: 7867.806641\n",
      "Train Epoch: 104 [69888/225000 (31%)] Loss: 8155.978516\n",
      "Train Epoch: 104 [73984/225000 (33%)] Loss: 10558.687500\n",
      "Train Epoch: 104 [78080/225000 (35%)] Loss: 8036.216797\n",
      "Train Epoch: 104 [82176/225000 (37%)] Loss: 7974.724609\n",
      "Train Epoch: 104 [86272/225000 (38%)] Loss: 13502.556641\n",
      "Train Epoch: 104 [90368/225000 (40%)] Loss: 8117.919922\n",
      "Train Epoch: 104 [94464/225000 (42%)] Loss: 8225.552734\n",
      "Train Epoch: 104 [98560/225000 (44%)] Loss: 7957.621094\n",
      "Train Epoch: 104 [102656/225000 (46%)] Loss: 7801.185547\n",
      "Train Epoch: 104 [106752/225000 (47%)] Loss: 10824.998047\n",
      "Train Epoch: 104 [110848/225000 (49%)] Loss: 13719.099609\n",
      "Train Epoch: 104 [114944/225000 (51%)] Loss: 7879.097656\n",
      "Train Epoch: 104 [119040/225000 (53%)] Loss: 9733.316406\n",
      "Train Epoch: 104 [123136/225000 (55%)] Loss: 8194.878906\n",
      "Train Epoch: 104 [127232/225000 (57%)] Loss: 7999.162109\n",
      "Train Epoch: 104 [131328/225000 (58%)] Loss: 9885.390625\n",
      "Train Epoch: 104 [135424/225000 (60%)] Loss: 8003.630859\n",
      "Train Epoch: 104 [139520/225000 (62%)] Loss: 7936.175781\n",
      "Train Epoch: 104 [143616/225000 (64%)] Loss: 7906.412109\n",
      "Train Epoch: 104 [147712/225000 (66%)] Loss: 9642.136719\n",
      "Train Epoch: 104 [151808/225000 (67%)] Loss: 11890.648438\n",
      "Train Epoch: 104 [155904/225000 (69%)] Loss: 13698.863281\n",
      "Train Epoch: 104 [160000/225000 (71%)] Loss: 8134.652344\n",
      "Train Epoch: 104 [164096/225000 (73%)] Loss: 8177.826172\n",
      "Train Epoch: 104 [168192/225000 (75%)] Loss: 9572.419922\n",
      "Train Epoch: 104 [172288/225000 (77%)] Loss: 10564.988281\n",
      "Train Epoch: 104 [176384/225000 (78%)] Loss: 8293.498047\n",
      "Train Epoch: 104 [180480/225000 (80%)] Loss: 19171.476562\n",
      "Train Epoch: 104 [184576/225000 (82%)] Loss: 7987.949219\n",
      "Train Epoch: 104 [188672/225000 (84%)] Loss: 8064.230469\n",
      "Train Epoch: 104 [192768/225000 (86%)] Loss: 7942.669922\n",
      "Train Epoch: 104 [196864/225000 (87%)] Loss: 7985.804688\n",
      "Train Epoch: 104 [200960/225000 (89%)] Loss: 9459.267578\n",
      "Train Epoch: 104 [205056/225000 (91%)] Loss: 8047.572266\n",
      "Train Epoch: 104 [209152/225000 (93%)] Loss: 8003.156250\n",
      "Train Epoch: 104 [213248/225000 (95%)] Loss: 9626.580078\n",
      "Train Epoch: 104 [217344/225000 (97%)] Loss: 13827.417969\n",
      "Train Epoch: 104 [221440/225000 (98%)] Loss: 8229.843750\n",
      "    epoch          : 104\n",
      "    loss           : 9658.459890944965\n",
      "    val_loss       : 9851.42992094585\n",
      "Train Epoch: 105 [256/225000 (0%)] Loss: 8022.728516\n",
      "Train Epoch: 105 [4352/225000 (2%)] Loss: 7983.044922\n",
      "Train Epoch: 105 [8448/225000 (4%)] Loss: 9651.662109\n",
      "Train Epoch: 105 [12544/225000 (6%)] Loss: 9601.646484\n",
      "Train Epoch: 105 [16640/225000 (7%)] Loss: 9673.748047\n",
      "Train Epoch: 105 [20736/225000 (9%)] Loss: 8167.527344\n",
      "Train Epoch: 105 [24832/225000 (11%)] Loss: 8117.990234\n",
      "Train Epoch: 105 [28928/225000 (13%)] Loss: 8127.050781\n",
      "Train Epoch: 105 [33024/225000 (15%)] Loss: 8172.908203\n",
      "Train Epoch: 105 [37120/225000 (16%)] Loss: 8142.117188\n",
      "Train Epoch: 105 [41216/225000 (18%)] Loss: 8142.437500\n",
      "Train Epoch: 105 [45312/225000 (20%)] Loss: 10415.556641\n",
      "Train Epoch: 105 [49408/225000 (22%)] Loss: 8110.808594\n",
      "Train Epoch: 105 [53504/225000 (24%)] Loss: 8106.589844\n",
      "Train Epoch: 105 [57600/225000 (26%)] Loss: 8065.587891\n",
      "Train Epoch: 105 [61696/225000 (27%)] Loss: 8042.523438\n",
      "Train Epoch: 105 [65792/225000 (29%)] Loss: 9533.939453\n",
      "Train Epoch: 105 [69888/225000 (31%)] Loss: 8143.919922\n",
      "Train Epoch: 105 [73984/225000 (33%)] Loss: 8121.521484\n",
      "Train Epoch: 105 [78080/225000 (35%)] Loss: 12320.017578\n",
      "Train Epoch: 105 [82176/225000 (37%)] Loss: 7799.906250\n",
      "Train Epoch: 105 [86272/225000 (38%)] Loss: 10579.716797\n",
      "Train Epoch: 105 [90368/225000 (40%)] Loss: 12104.218750\n",
      "Train Epoch: 105 [94464/225000 (42%)] Loss: 7899.642578\n",
      "Train Epoch: 105 [98560/225000 (44%)] Loss: 8042.515625\n",
      "Train Epoch: 105 [102656/225000 (46%)] Loss: 9821.857422\n",
      "Train Epoch: 105 [106752/225000 (47%)] Loss: 8089.664062\n",
      "Train Epoch: 105 [110848/225000 (49%)] Loss: 7985.886719\n",
      "Train Epoch: 105 [114944/225000 (51%)] Loss: 8033.941406\n",
      "Train Epoch: 105 [119040/225000 (53%)] Loss: 7792.832031\n",
      "Train Epoch: 105 [123136/225000 (55%)] Loss: 10345.281250\n",
      "Train Epoch: 105 [127232/225000 (57%)] Loss: 8280.187500\n",
      "Train Epoch: 105 [131328/225000 (58%)] Loss: 7953.287109\n",
      "Train Epoch: 105 [135424/225000 (60%)] Loss: 7992.537109\n",
      "Train Epoch: 105 [139520/225000 (62%)] Loss: 13650.261719\n",
      "Train Epoch: 105 [143616/225000 (64%)] Loss: 13651.226562\n",
      "Train Epoch: 105 [147712/225000 (66%)] Loss: 10573.751953\n",
      "Train Epoch: 105 [151808/225000 (67%)] Loss: 8003.814453\n",
      "Train Epoch: 105 [155904/225000 (69%)] Loss: 8080.439453\n",
      "Train Epoch: 105 [160000/225000 (71%)] Loss: 8089.955078\n",
      "Train Epoch: 105 [164096/225000 (73%)] Loss: 13047.183594\n",
      "Train Epoch: 105 [168192/225000 (75%)] Loss: 7984.169922\n",
      "Train Epoch: 105 [172288/225000 (77%)] Loss: 12148.117188\n",
      "Train Epoch: 105 [176384/225000 (78%)] Loss: 8049.031250\n",
      "Train Epoch: 105 [180480/225000 (80%)] Loss: 7906.328125\n",
      "Train Epoch: 105 [184576/225000 (82%)] Loss: 8094.796875\n",
      "Train Epoch: 105 [188672/225000 (84%)] Loss: 8140.154297\n",
      "Train Epoch: 105 [192768/225000 (86%)] Loss: 8078.410156\n",
      "Train Epoch: 105 [196864/225000 (87%)] Loss: 9594.912109\n",
      "Train Epoch: 105 [200960/225000 (89%)] Loss: 8092.183594\n",
      "Train Epoch: 105 [205056/225000 (91%)] Loss: 7966.892578\n",
      "Train Epoch: 105 [209152/225000 (93%)] Loss: 10714.728516\n",
      "Train Epoch: 105 [213248/225000 (95%)] Loss: 8070.625000\n",
      "Train Epoch: 105 [217344/225000 (97%)] Loss: 9664.613281\n",
      "Train Epoch: 105 [221440/225000 (98%)] Loss: 8061.773438\n",
      "    epoch          : 105\n",
      "    loss           : 9454.929966359145\n",
      "    val_loss       : 9370.768507260449\n",
      "Train Epoch: 106 [256/225000 (0%)] Loss: 7981.947266\n",
      "Train Epoch: 106 [4352/225000 (2%)] Loss: 10451.466797\n",
      "Train Epoch: 106 [8448/225000 (4%)] Loss: 8100.851562\n",
      "Train Epoch: 106 [12544/225000 (6%)] Loss: 17731.878906\n",
      "Train Epoch: 106 [16640/225000 (7%)] Loss: 8074.412109\n",
      "Train Epoch: 106 [20736/225000 (9%)] Loss: 15094.423828\n",
      "Train Epoch: 106 [24832/225000 (11%)] Loss: 8168.109375\n",
      "Train Epoch: 106 [28928/225000 (13%)] Loss: 9812.027344\n",
      "Train Epoch: 106 [33024/225000 (15%)] Loss: 8168.792969\n",
      "Train Epoch: 106 [37120/225000 (16%)] Loss: 8139.498047\n",
      "Train Epoch: 106 [41216/225000 (18%)] Loss: 7855.619141\n",
      "Train Epoch: 106 [45312/225000 (20%)] Loss: 12492.343750\n",
      "Train Epoch: 106 [49408/225000 (22%)] Loss: 8149.406250\n",
      "Train Epoch: 106 [53504/225000 (24%)] Loss: 13529.101562\n",
      "Train Epoch: 106 [57600/225000 (26%)] Loss: 18334.382812\n",
      "Train Epoch: 106 [61696/225000 (27%)] Loss: 8174.566406\n",
      "Train Epoch: 106 [65792/225000 (29%)] Loss: 8260.462891\n",
      "Train Epoch: 106 [69888/225000 (31%)] Loss: 8157.464844\n",
      "Train Epoch: 106 [73984/225000 (33%)] Loss: 13561.101562\n",
      "Train Epoch: 106 [78080/225000 (35%)] Loss: 8071.587891\n",
      "Train Epoch: 106 [82176/225000 (37%)] Loss: 10281.974609\n",
      "Train Epoch: 106 [86272/225000 (38%)] Loss: 7857.958984\n",
      "Train Epoch: 106 [90368/225000 (40%)] Loss: 8127.097656\n",
      "Train Epoch: 106 [94464/225000 (42%)] Loss: 8217.587891\n",
      "Train Epoch: 106 [98560/225000 (44%)] Loss: 13843.927734\n",
      "Train Epoch: 106 [102656/225000 (46%)] Loss: 8314.740234\n",
      "Train Epoch: 106 [106752/225000 (47%)] Loss: 9598.355469\n",
      "Train Epoch: 106 [110848/225000 (49%)] Loss: 7895.546875\n",
      "Train Epoch: 106 [114944/225000 (51%)] Loss: 7766.728516\n",
      "Train Epoch: 106 [119040/225000 (53%)] Loss: 8131.447266\n",
      "Train Epoch: 106 [123136/225000 (55%)] Loss: 7893.683594\n",
      "Train Epoch: 106 [127232/225000 (57%)] Loss: 12173.089844\n",
      "Train Epoch: 106 [131328/225000 (58%)] Loss: 12363.914062\n",
      "Train Epoch: 106 [135424/225000 (60%)] Loss: 8095.679688\n",
      "Train Epoch: 106 [139520/225000 (62%)] Loss: 10303.613281\n",
      "Train Epoch: 106 [143616/225000 (64%)] Loss: 12022.685547\n",
      "Train Epoch: 106 [147712/225000 (66%)] Loss: 8007.468750\n",
      "Train Epoch: 106 [151808/225000 (67%)] Loss: 12895.558594\n",
      "Train Epoch: 106 [155904/225000 (69%)] Loss: 10248.939453\n",
      "Train Epoch: 106 [160000/225000 (71%)] Loss: 12100.869141\n",
      "Train Epoch: 106 [164096/225000 (73%)] Loss: 8035.603516\n",
      "Train Epoch: 106 [168192/225000 (75%)] Loss: 11218.980469\n",
      "Train Epoch: 106 [172288/225000 (77%)] Loss: 8052.650391\n",
      "Train Epoch: 106 [176384/225000 (78%)] Loss: 7934.636719\n",
      "Train Epoch: 106 [180480/225000 (80%)] Loss: 8083.943359\n",
      "Train Epoch: 106 [184576/225000 (82%)] Loss: 9609.748047\n",
      "Train Epoch: 106 [188672/225000 (84%)] Loss: 9610.345703\n",
      "Train Epoch: 106 [192768/225000 (86%)] Loss: 8142.583984\n",
      "Train Epoch: 106 [196864/225000 (87%)] Loss: 7892.238281\n",
      "Train Epoch: 106 [200960/225000 (89%)] Loss: 8063.300781\n",
      "Train Epoch: 106 [205056/225000 (91%)] Loss: 10348.017578\n",
      "Train Epoch: 106 [209152/225000 (93%)] Loss: 7944.482422\n",
      "Train Epoch: 106 [213248/225000 (95%)] Loss: 9448.544922\n",
      "Train Epoch: 106 [217344/225000 (97%)] Loss: 13006.253906\n",
      "Train Epoch: 106 [221440/225000 (98%)] Loss: 8310.388672\n",
      "    epoch          : 106\n",
      "    loss           : 9557.321530147896\n",
      "    val_loss       : 9150.457746985006\n",
      "Train Epoch: 107 [256/225000 (0%)] Loss: 12509.630859\n",
      "Train Epoch: 107 [4352/225000 (2%)] Loss: 8178.255859\n",
      "Train Epoch: 107 [8448/225000 (4%)] Loss: 10515.257812\n",
      "Train Epoch: 107 [12544/225000 (6%)] Loss: 8320.832031\n",
      "Train Epoch: 107 [16640/225000 (7%)] Loss: 12667.589844\n",
      "Train Epoch: 107 [20736/225000 (9%)] Loss: 8399.945312\n",
      "Train Epoch: 107 [24832/225000 (11%)] Loss: 7910.437500\n",
      "Train Epoch: 107 [28928/225000 (13%)] Loss: 13651.201172\n",
      "Train Epoch: 107 [33024/225000 (15%)] Loss: 9818.919922\n",
      "Train Epoch: 107 [37120/225000 (16%)] Loss: 8078.181641\n",
      "Train Epoch: 107 [41216/225000 (18%)] Loss: 8159.126953\n",
      "Train Epoch: 107 [45312/225000 (20%)] Loss: 13670.062500\n",
      "Train Epoch: 107 [49408/225000 (22%)] Loss: 10349.458984\n",
      "Train Epoch: 107 [53504/225000 (24%)] Loss: 8107.025391\n",
      "Train Epoch: 107 [57600/225000 (26%)] Loss: 8053.179688\n",
      "Train Epoch: 107 [61696/225000 (27%)] Loss: 7998.357422\n",
      "Train Epoch: 107 [65792/225000 (29%)] Loss: 8175.515625\n",
      "Train Epoch: 107 [69888/225000 (31%)] Loss: 8064.974609\n",
      "Train Epoch: 107 [73984/225000 (33%)] Loss: 7896.812500\n",
      "Train Epoch: 107 [78080/225000 (35%)] Loss: 10372.568359\n",
      "Train Epoch: 107 [82176/225000 (37%)] Loss: 8035.449219\n",
      "Train Epoch: 107 [86272/225000 (38%)] Loss: 10412.833984\n",
      "Train Epoch: 107 [90368/225000 (40%)] Loss: 10473.095703\n",
      "Train Epoch: 107 [94464/225000 (42%)] Loss: 7921.599609\n",
      "Train Epoch: 107 [98560/225000 (44%)] Loss: 8071.300781\n",
      "Train Epoch: 107 [102656/225000 (46%)] Loss: 10576.937500\n",
      "Train Epoch: 107 [106752/225000 (47%)] Loss: 7896.521484\n",
      "Train Epoch: 107 [110848/225000 (49%)] Loss: 8073.744141\n",
      "Train Epoch: 107 [114944/225000 (51%)] Loss: 13832.369141\n",
      "Train Epoch: 107 [119040/225000 (53%)] Loss: 8239.246094\n",
      "Train Epoch: 107 [123136/225000 (55%)] Loss: 10386.660156\n",
      "Train Epoch: 107 [127232/225000 (57%)] Loss: 9686.072266\n",
      "Train Epoch: 107 [131328/225000 (58%)] Loss: 7961.386719\n",
      "Train Epoch: 107 [135424/225000 (60%)] Loss: 7939.447266\n",
      "Train Epoch: 107 [139520/225000 (62%)] Loss: 8073.072266\n",
      "Train Epoch: 107 [143616/225000 (64%)] Loss: 13985.501953\n",
      "Train Epoch: 107 [147712/225000 (66%)] Loss: 8084.951172\n",
      "Train Epoch: 107 [151808/225000 (67%)] Loss: 8109.656250\n",
      "Train Epoch: 107 [155904/225000 (69%)] Loss: 12912.902344\n",
      "Train Epoch: 107 [160000/225000 (71%)] Loss: 8244.261719\n",
      "Train Epoch: 107 [164096/225000 (73%)] Loss: 8080.849609\n",
      "Train Epoch: 107 [168192/225000 (75%)] Loss: 7987.134766\n",
      "Train Epoch: 107 [172288/225000 (77%)] Loss: 8171.162109\n",
      "Train Epoch: 107 [176384/225000 (78%)] Loss: 7908.025391\n",
      "Train Epoch: 107 [180480/225000 (80%)] Loss: 9808.283203\n",
      "Train Epoch: 107 [184576/225000 (82%)] Loss: 8188.062500\n",
      "Train Epoch: 107 [188672/225000 (84%)] Loss: 9731.466797\n",
      "Train Epoch: 107 [192768/225000 (86%)] Loss: 7950.289062\n",
      "Train Epoch: 107 [196864/225000 (87%)] Loss: 8278.132812\n",
      "Train Epoch: 107 [200960/225000 (89%)] Loss: 13713.460938\n",
      "Train Epoch: 107 [205056/225000 (91%)] Loss: 9707.962891\n",
      "Train Epoch: 107 [209152/225000 (93%)] Loss: 8126.347656\n",
      "Train Epoch: 107 [213248/225000 (95%)] Loss: 8004.554688\n",
      "Train Epoch: 107 [217344/225000 (97%)] Loss: 7968.519531\n",
      "Train Epoch: 107 [221440/225000 (98%)] Loss: 7937.117188\n",
      "    epoch          : 107\n",
      "    loss           : 9543.587424008107\n",
      "    val_loss       : 9411.954021580364\n",
      "Train Epoch: 108 [256/225000 (0%)] Loss: 8081.703125\n",
      "Train Epoch: 108 [4352/225000 (2%)] Loss: 9567.416016\n",
      "Train Epoch: 108 [8448/225000 (4%)] Loss: 7986.230469\n",
      "Train Epoch: 108 [12544/225000 (6%)] Loss: 8051.011719\n",
      "Train Epoch: 108 [16640/225000 (7%)] Loss: 12393.994141\n",
      "Train Epoch: 108 [20736/225000 (9%)] Loss: 8100.818359\n",
      "Train Epoch: 108 [24832/225000 (11%)] Loss: 8070.441406\n",
      "Train Epoch: 108 [28928/225000 (13%)] Loss: 7844.087891\n",
      "Train Epoch: 108 [33024/225000 (15%)] Loss: 8101.234375\n",
      "Train Epoch: 108 [37120/225000 (16%)] Loss: 8037.435547\n",
      "Train Epoch: 108 [41216/225000 (18%)] Loss: 11190.048828\n",
      "Train Epoch: 108 [45312/225000 (20%)] Loss: 8099.974609\n",
      "Train Epoch: 108 [49408/225000 (22%)] Loss: 8289.628906\n",
      "Train Epoch: 108 [53504/225000 (24%)] Loss: 7961.945312\n",
      "Train Epoch: 108 [57600/225000 (26%)] Loss: 7999.189453\n",
      "Train Epoch: 108 [61696/225000 (27%)] Loss: 7982.835938\n",
      "Train Epoch: 108 [65792/225000 (29%)] Loss: 14771.787109\n",
      "Train Epoch: 108 [69888/225000 (31%)] Loss: 15067.515625\n",
      "Train Epoch: 108 [73984/225000 (33%)] Loss: 8162.160156\n",
      "Train Epoch: 108 [78080/225000 (35%)] Loss: 7968.988281\n",
      "Train Epoch: 108 [82176/225000 (37%)] Loss: 8036.945312\n",
      "Train Epoch: 108 [86272/225000 (38%)] Loss: 7965.015625\n",
      "Train Epoch: 108 [90368/225000 (40%)] Loss: 8149.203125\n",
      "Train Epoch: 108 [94464/225000 (42%)] Loss: 7902.468750\n",
      "Train Epoch: 108 [98560/225000 (44%)] Loss: 15996.351562\n",
      "Train Epoch: 108 [102656/225000 (46%)] Loss: 9799.056641\n",
      "Train Epoch: 108 [106752/225000 (47%)] Loss: 10461.203125\n",
      "Train Epoch: 108 [110848/225000 (49%)] Loss: 8038.474609\n",
      "Train Epoch: 108 [114944/225000 (51%)] Loss: 8026.462891\n",
      "Train Epoch: 108 [119040/225000 (53%)] Loss: 8044.318359\n",
      "Train Epoch: 108 [123136/225000 (55%)] Loss: 8199.306641\n",
      "Train Epoch: 108 [127232/225000 (57%)] Loss: 7923.835938\n",
      "Train Epoch: 108 [131328/225000 (58%)] Loss: 8107.984375\n",
      "Train Epoch: 108 [135424/225000 (60%)] Loss: 8067.746094\n",
      "Train Epoch: 108 [139520/225000 (62%)] Loss: 9665.462891\n",
      "Train Epoch: 108 [143616/225000 (64%)] Loss: 8148.826172\n",
      "Train Epoch: 108 [147712/225000 (66%)] Loss: 8072.341797\n",
      "Train Epoch: 108 [151808/225000 (67%)] Loss: 8242.175781\n",
      "Train Epoch: 108 [155904/225000 (69%)] Loss: 8021.683594\n",
      "Train Epoch: 108 [160000/225000 (71%)] Loss: 8176.943359\n",
      "Train Epoch: 108 [164096/225000 (73%)] Loss: 8216.214844\n",
      "Train Epoch: 108 [168192/225000 (75%)] Loss: 8156.386719\n",
      "Train Epoch: 108 [172288/225000 (77%)] Loss: 9368.347656\n",
      "Train Epoch: 108 [176384/225000 (78%)] Loss: 7908.253906\n",
      "Train Epoch: 108 [180480/225000 (80%)] Loss: 8319.449219\n",
      "Train Epoch: 108 [184576/225000 (82%)] Loss: 13553.851562\n",
      "Train Epoch: 108 [188672/225000 (84%)] Loss: 11341.980469\n",
      "Train Epoch: 108 [192768/225000 (86%)] Loss: 8039.304688\n",
      "Train Epoch: 108 [196864/225000 (87%)] Loss: 10583.015625\n",
      "Train Epoch: 108 [200960/225000 (89%)] Loss: 8207.748047\n",
      "Train Epoch: 108 [205056/225000 (91%)] Loss: 8078.111328\n",
      "Train Epoch: 108 [209152/225000 (93%)] Loss: 16161.148438\n",
      "Train Epoch: 108 [213248/225000 (95%)] Loss: 9612.255859\n",
      "Train Epoch: 108 [217344/225000 (97%)] Loss: 14030.347656\n",
      "Train Epoch: 108 [221440/225000 (98%)] Loss: 7991.103516\n",
      "    epoch          : 108\n",
      "    loss           : 9550.605988694539\n",
      "    val_loss       : 9577.511528499272\n",
      "Train Epoch: 109 [256/225000 (0%)] Loss: 10512.447266\n",
      "Train Epoch: 109 [4352/225000 (2%)] Loss: 7809.003906\n",
      "Train Epoch: 109 [8448/225000 (4%)] Loss: 7922.896484\n",
      "Train Epoch: 109 [12544/225000 (6%)] Loss: 8172.236328\n",
      "Train Epoch: 109 [16640/225000 (7%)] Loss: 18490.582031\n",
      "Train Epoch: 109 [20736/225000 (9%)] Loss: 11152.361328\n",
      "Train Epoch: 109 [24832/225000 (11%)] Loss: 8061.406250\n",
      "Train Epoch: 109 [28928/225000 (13%)] Loss: 8348.921875\n",
      "Train Epoch: 109 [33024/225000 (15%)] Loss: 8035.632812\n",
      "Train Epoch: 109 [37120/225000 (16%)] Loss: 10687.085938\n",
      "Train Epoch: 109 [41216/225000 (18%)] Loss: 10499.242188\n",
      "Train Epoch: 109 [45312/225000 (20%)] Loss: 8156.408203\n",
      "Train Epoch: 109 [49408/225000 (22%)] Loss: 12526.027344\n",
      "Train Epoch: 109 [53504/225000 (24%)] Loss: 7983.996094\n",
      "Train Epoch: 109 [57600/225000 (26%)] Loss: 10510.230469\n",
      "Train Epoch: 109 [61696/225000 (27%)] Loss: 9717.660156\n",
      "Train Epoch: 109 [65792/225000 (29%)] Loss: 9558.666016\n",
      "Train Epoch: 109 [69888/225000 (31%)] Loss: 8094.060547\n",
      "Train Epoch: 109 [73984/225000 (33%)] Loss: 9768.199219\n",
      "Train Epoch: 109 [78080/225000 (35%)] Loss: 7967.972656\n",
      "Train Epoch: 109 [82176/225000 (37%)] Loss: 8207.265625\n",
      "Train Epoch: 109 [86272/225000 (38%)] Loss: 8151.193359\n",
      "Train Epoch: 109 [90368/225000 (40%)] Loss: 7908.250000\n",
      "Train Epoch: 109 [94464/225000 (42%)] Loss: 7945.007812\n",
      "Train Epoch: 109 [98560/225000 (44%)] Loss: 8149.693359\n",
      "Train Epoch: 109 [102656/225000 (46%)] Loss: 8156.441406\n",
      "Train Epoch: 109 [106752/225000 (47%)] Loss: 8083.285156\n",
      "Train Epoch: 109 [110848/225000 (49%)] Loss: 8098.017578\n",
      "Train Epoch: 109 [114944/225000 (51%)] Loss: 8115.615234\n",
      "Train Epoch: 109 [119040/225000 (53%)] Loss: 7921.267578\n",
      "Train Epoch: 109 [123136/225000 (55%)] Loss: 8086.328125\n",
      "Train Epoch: 109 [127232/225000 (57%)] Loss: 8164.619141\n",
      "Train Epoch: 109 [131328/225000 (58%)] Loss: 8010.365234\n",
      "Train Epoch: 109 [135424/225000 (60%)] Loss: 13559.636719\n",
      "Train Epoch: 109 [139520/225000 (62%)] Loss: 12336.476562\n",
      "Train Epoch: 109 [143616/225000 (64%)] Loss: 8216.892578\n",
      "Train Epoch: 109 [147712/225000 (66%)] Loss: 8048.634766\n",
      "Train Epoch: 109 [151808/225000 (67%)] Loss: 9687.820312\n",
      "Train Epoch: 109 [155904/225000 (69%)] Loss: 7973.730469\n",
      "Train Epoch: 109 [160000/225000 (71%)] Loss: 9755.912109\n",
      "Train Epoch: 109 [164096/225000 (73%)] Loss: 8148.708984\n",
      "Train Epoch: 109 [168192/225000 (75%)] Loss: 9581.486328\n",
      "Train Epoch: 109 [172288/225000 (77%)] Loss: 7994.751953\n",
      "Train Epoch: 109 [176384/225000 (78%)] Loss: 10420.144531\n",
      "Train Epoch: 109 [180480/225000 (80%)] Loss: 8082.695312\n",
      "Train Epoch: 109 [184576/225000 (82%)] Loss: 10604.806641\n",
      "Train Epoch: 109 [188672/225000 (84%)] Loss: 8090.365234\n",
      "Train Epoch: 109 [192768/225000 (86%)] Loss: 8131.960938\n",
      "Train Epoch: 109 [196864/225000 (87%)] Loss: 8262.314453\n",
      "Train Epoch: 109 [200960/225000 (89%)] Loss: 8225.351562\n",
      "Train Epoch: 109 [205056/225000 (91%)] Loss: 9681.875000\n",
      "Train Epoch: 109 [209152/225000 (93%)] Loss: 7965.847656\n",
      "Train Epoch: 109 [213248/225000 (95%)] Loss: 8244.277344\n",
      "Train Epoch: 109 [217344/225000 (97%)] Loss: 10632.445312\n",
      "Train Epoch: 109 [221440/225000 (98%)] Loss: 8121.722656\n",
      "    epoch          : 109\n",
      "    loss           : 9436.78489183376\n",
      "    val_loss       : 9896.079770942124\n",
      "Train Epoch: 110 [256/225000 (0%)] Loss: 11205.300781\n",
      "Train Epoch: 110 [4352/225000 (2%)] Loss: 9713.033203\n",
      "Train Epoch: 110 [8448/225000 (4%)] Loss: 8154.613281\n",
      "Train Epoch: 110 [12544/225000 (6%)] Loss: 15459.875000\n",
      "Train Epoch: 110 [16640/225000 (7%)] Loss: 12050.546875\n",
      "Train Epoch: 110 [20736/225000 (9%)] Loss: 8304.343750\n",
      "Train Epoch: 110 [24832/225000 (11%)] Loss: 8060.890625\n",
      "Train Epoch: 110 [28928/225000 (13%)] Loss: 8003.957031\n",
      "Train Epoch: 110 [33024/225000 (15%)] Loss: 8057.042969\n",
      "Train Epoch: 110 [37120/225000 (16%)] Loss: 16305.464844\n",
      "Train Epoch: 110 [41216/225000 (18%)] Loss: 8054.738281\n",
      "Train Epoch: 110 [45312/225000 (20%)] Loss: 8074.691406\n",
      "Train Epoch: 110 [49408/225000 (22%)] Loss: 7738.873047\n",
      "Train Epoch: 110 [53504/225000 (24%)] Loss: 9666.068359\n",
      "Train Epoch: 110 [57600/225000 (26%)] Loss: 8097.087891\n",
      "Train Epoch: 110 [61696/225000 (27%)] Loss: 13856.615234\n",
      "Train Epoch: 110 [65792/225000 (29%)] Loss: 8141.832031\n",
      "Train Epoch: 110 [69888/225000 (31%)] Loss: 8196.500000\n",
      "Train Epoch: 110 [73984/225000 (33%)] Loss: 11105.759766\n",
      "Train Epoch: 110 [78080/225000 (35%)] Loss: 8065.900391\n",
      "Train Epoch: 110 [82176/225000 (37%)] Loss: 7890.777344\n",
      "Train Epoch: 110 [86272/225000 (38%)] Loss: 8098.322266\n",
      "Train Epoch: 110 [90368/225000 (40%)] Loss: 8002.568359\n",
      "Train Epoch: 110 [94464/225000 (42%)] Loss: 10734.201172\n",
      "Train Epoch: 110 [98560/225000 (44%)] Loss: 8145.273438\n",
      "Train Epoch: 110 [102656/225000 (46%)] Loss: 14496.623047\n",
      "Train Epoch: 110 [106752/225000 (47%)] Loss: 8015.804688\n",
      "Train Epoch: 110 [110848/225000 (49%)] Loss: 10701.449219\n",
      "Train Epoch: 110 [114944/225000 (51%)] Loss: 8181.916016\n",
      "Train Epoch: 110 [119040/225000 (53%)] Loss: 10632.123047\n",
      "Train Epoch: 110 [123136/225000 (55%)] Loss: 7927.867188\n",
      "Train Epoch: 110 [127232/225000 (57%)] Loss: 9567.064453\n",
      "Train Epoch: 110 [131328/225000 (58%)] Loss: 9583.621094\n",
      "Train Epoch: 110 [135424/225000 (60%)] Loss: 13484.269531\n",
      "Train Epoch: 110 [139520/225000 (62%)] Loss: 9694.662109\n",
      "Train Epoch: 110 [143616/225000 (64%)] Loss: 10369.187500\n",
      "Train Epoch: 110 [147712/225000 (66%)] Loss: 19678.353516\n",
      "Train Epoch: 110 [151808/225000 (67%)] Loss: 10549.234375\n",
      "Train Epoch: 110 [155904/225000 (69%)] Loss: 8159.023438\n",
      "Train Epoch: 110 [160000/225000 (71%)] Loss: 7919.751953\n",
      "Train Epoch: 110 [164096/225000 (73%)] Loss: 8084.839844\n",
      "Train Epoch: 110 [168192/225000 (75%)] Loss: 8238.683594\n",
      "Train Epoch: 110 [172288/225000 (77%)] Loss: 8085.033203\n",
      "Train Epoch: 110 [176384/225000 (78%)] Loss: 7886.308594\n",
      "Train Epoch: 110 [180480/225000 (80%)] Loss: 8114.111328\n",
      "Train Epoch: 110 [184576/225000 (82%)] Loss: 13731.152344\n",
      "Train Epoch: 110 [188672/225000 (84%)] Loss: 10455.111328\n",
      "Train Epoch: 110 [192768/225000 (86%)] Loss: 8083.425781\n",
      "Train Epoch: 110 [196864/225000 (87%)] Loss: 8042.746094\n",
      "Train Epoch: 110 [200960/225000 (89%)] Loss: 8049.775391\n",
      "Train Epoch: 110 [205056/225000 (91%)] Loss: 10549.568359\n",
      "Train Epoch: 110 [209152/225000 (93%)] Loss: 9615.298828\n",
      "Train Epoch: 110 [213248/225000 (95%)] Loss: 8004.470703\n",
      "Train Epoch: 110 [217344/225000 (97%)] Loss: 12764.730469\n",
      "Train Epoch: 110 [221440/225000 (98%)] Loss: 8204.406250\n",
      "    epoch          : 110\n",
      "    loss           : 9525.222840674773\n",
      "    val_loss       : 9748.280228807002\n",
      "Train Epoch: 111 [256/225000 (0%)] Loss: 8251.080078\n",
      "Train Epoch: 111 [4352/225000 (2%)] Loss: 10491.226562\n",
      "Train Epoch: 111 [8448/225000 (4%)] Loss: 10740.785156\n",
      "Train Epoch: 111 [12544/225000 (6%)] Loss: 8167.126953\n",
      "Train Epoch: 111 [16640/225000 (7%)] Loss: 7919.408203\n",
      "Train Epoch: 111 [20736/225000 (9%)] Loss: 7968.314453\n",
      "Train Epoch: 111 [24832/225000 (11%)] Loss: 8053.113281\n",
      "Train Epoch: 111 [28928/225000 (13%)] Loss: 12367.072266\n",
      "Train Epoch: 111 [33024/225000 (15%)] Loss: 8130.009766\n",
      "Train Epoch: 111 [37120/225000 (16%)] Loss: 8260.416016\n",
      "Train Epoch: 111 [41216/225000 (18%)] Loss: 8036.320312\n",
      "Train Epoch: 111 [45312/225000 (20%)] Loss: 8070.960938\n",
      "Train Epoch: 111 [49408/225000 (22%)] Loss: 8018.824219\n",
      "Train Epoch: 111 [53504/225000 (24%)] Loss: 8202.439453\n",
      "Train Epoch: 111 [57600/225000 (26%)] Loss: 9595.550781\n",
      "Train Epoch: 111 [61696/225000 (27%)] Loss: 13816.750000\n",
      "Train Epoch: 111 [65792/225000 (29%)] Loss: 12299.683594\n",
      "Train Epoch: 111 [69888/225000 (31%)] Loss: 7936.166016\n",
      "Train Epoch: 111 [73984/225000 (33%)] Loss: 8049.986328\n",
      "Train Epoch: 111 [78080/225000 (35%)] Loss: 8113.855469\n",
      "Train Epoch: 111 [82176/225000 (37%)] Loss: 8249.828125\n",
      "Train Epoch: 111 [86272/225000 (38%)] Loss: 18487.126953\n",
      "Train Epoch: 111 [90368/225000 (40%)] Loss: 10431.517578\n",
      "Train Epoch: 111 [94464/225000 (42%)] Loss: 10637.302734\n",
      "Train Epoch: 111 [98560/225000 (44%)] Loss: 8174.718750\n",
      "Train Epoch: 111 [102656/225000 (46%)] Loss: 8098.140625\n",
      "Train Epoch: 111 [106752/225000 (47%)] Loss: 12131.007812\n",
      "Train Epoch: 111 [110848/225000 (49%)] Loss: 8058.779297\n",
      "Train Epoch: 111 [114944/225000 (51%)] Loss: 10496.330078\n",
      "Train Epoch: 111 [119040/225000 (53%)] Loss: 7811.935547\n",
      "Train Epoch: 111 [123136/225000 (55%)] Loss: 8075.085938\n",
      "Train Epoch: 111 [127232/225000 (57%)] Loss: 10492.000000\n",
      "Train Epoch: 111 [131328/225000 (58%)] Loss: 7934.417969\n",
      "Train Epoch: 111 [135424/225000 (60%)] Loss: 8149.490234\n",
      "Train Epoch: 111 [139520/225000 (62%)] Loss: 8021.404297\n",
      "Train Epoch: 111 [143616/225000 (64%)] Loss: 11384.101562\n",
      "Train Epoch: 111 [147712/225000 (66%)] Loss: 8086.642578\n",
      "Train Epoch: 111 [151808/225000 (67%)] Loss: 8147.183594\n",
      "Train Epoch: 111 [155904/225000 (69%)] Loss: 8057.916016\n",
      "Train Epoch: 111 [160000/225000 (71%)] Loss: 7966.152344\n",
      "Train Epoch: 111 [164096/225000 (73%)] Loss: 7866.242188\n",
      "Train Epoch: 111 [168192/225000 (75%)] Loss: 12147.062500\n",
      "Train Epoch: 111 [172288/225000 (77%)] Loss: 8134.853516\n",
      "Train Epoch: 111 [176384/225000 (78%)] Loss: 8075.951172\n",
      "Train Epoch: 111 [180480/225000 (80%)] Loss: 8212.914062\n",
      "Train Epoch: 111 [184576/225000 (82%)] Loss: 7995.369141\n",
      "Train Epoch: 111 [188672/225000 (84%)] Loss: 8085.775391\n",
      "Train Epoch: 111 [192768/225000 (86%)] Loss: 8138.890625\n",
      "Train Epoch: 111 [196864/225000 (87%)] Loss: 10399.906250\n",
      "Train Epoch: 111 [200960/225000 (89%)] Loss: 7989.146484\n",
      "Train Epoch: 111 [205056/225000 (91%)] Loss: 12359.808594\n",
      "Train Epoch: 111 [209152/225000 (93%)] Loss: 10602.232422\n",
      "Train Epoch: 111 [213248/225000 (95%)] Loss: 11950.386719\n",
      "Train Epoch: 111 [217344/225000 (97%)] Loss: 10464.531250\n",
      "Train Epoch: 111 [221440/225000 (98%)] Loss: 8151.777344\n",
      "    epoch          : 111\n",
      "    loss           : 9554.855562073379\n",
      "    val_loss       : 9574.565875654318\n",
      "Train Epoch: 112 [256/225000 (0%)] Loss: 8061.734375\n",
      "Train Epoch: 112 [4352/225000 (2%)] Loss: 7960.017578\n",
      "Train Epoch: 112 [8448/225000 (4%)] Loss: 13745.544922\n",
      "Train Epoch: 112 [12544/225000 (6%)] Loss: 9743.916016\n",
      "Train Epoch: 112 [16640/225000 (7%)] Loss: 8288.423828\n",
      "Train Epoch: 112 [20736/225000 (9%)] Loss: 8038.900391\n",
      "Train Epoch: 112 [24832/225000 (11%)] Loss: 9505.533203\n",
      "Train Epoch: 112 [28928/225000 (13%)] Loss: 9642.861328\n",
      "Train Epoch: 112 [33024/225000 (15%)] Loss: 8227.642578\n",
      "Train Epoch: 112 [37120/225000 (16%)] Loss: 7910.183594\n",
      "Train Epoch: 112 [41216/225000 (18%)] Loss: 8057.912109\n",
      "Train Epoch: 112 [45312/225000 (20%)] Loss: 10307.308594\n",
      "Train Epoch: 112 [49408/225000 (22%)] Loss: 8035.044922\n",
      "Train Epoch: 112 [53504/225000 (24%)] Loss: 10626.566406\n",
      "Train Epoch: 112 [57600/225000 (26%)] Loss: 8235.175781\n",
      "Train Epoch: 112 [61696/225000 (27%)] Loss: 13879.574219\n",
      "Train Epoch: 112 [65792/225000 (29%)] Loss: 7849.472656\n",
      "Train Epoch: 112 [69888/225000 (31%)] Loss: 7986.169922\n",
      "Train Epoch: 112 [73984/225000 (33%)] Loss: 7828.675781\n",
      "Train Epoch: 112 [78080/225000 (35%)] Loss: 8169.347656\n",
      "Train Epoch: 112 [82176/225000 (37%)] Loss: 8208.439453\n",
      "Train Epoch: 112 [86272/225000 (38%)] Loss: 7987.894531\n",
      "Train Epoch: 112 [90368/225000 (40%)] Loss: 8331.716797\n",
      "Train Epoch: 112 [94464/225000 (42%)] Loss: 7924.927734\n",
      "Train Epoch: 112 [98560/225000 (44%)] Loss: 8060.923828\n",
      "Train Epoch: 112 [102656/225000 (46%)] Loss: 8245.826172\n",
      "Train Epoch: 112 [106752/225000 (47%)] Loss: 8095.132812\n",
      "Train Epoch: 112 [110848/225000 (49%)] Loss: 8036.947266\n",
      "Train Epoch: 112 [114944/225000 (51%)] Loss: 7890.250000\n",
      "Train Epoch: 112 [119040/225000 (53%)] Loss: 7995.382812\n",
      "Train Epoch: 112 [123136/225000 (55%)] Loss: 8078.052734\n",
      "Train Epoch: 112 [127232/225000 (57%)] Loss: 13260.740234\n",
      "Train Epoch: 112 [131328/225000 (58%)] Loss: 8000.271484\n",
      "Train Epoch: 112 [135424/225000 (60%)] Loss: 9729.476562\n",
      "Train Epoch: 112 [139520/225000 (62%)] Loss: 8164.824219\n",
      "Train Epoch: 112 [143616/225000 (64%)] Loss: 8171.628906\n",
      "Train Epoch: 112 [147712/225000 (66%)] Loss: 8064.248047\n",
      "Train Epoch: 112 [151808/225000 (67%)] Loss: 9605.212891\n",
      "Train Epoch: 112 [155904/225000 (69%)] Loss: 7949.279297\n",
      "Train Epoch: 112 [160000/225000 (71%)] Loss: 7920.398438\n",
      "Train Epoch: 112 [164096/225000 (73%)] Loss: 8099.888672\n",
      "Train Epoch: 112 [168192/225000 (75%)] Loss: 8271.460938\n",
      "Train Epoch: 112 [172288/225000 (77%)] Loss: 8038.753906\n",
      "Train Epoch: 112 [176384/225000 (78%)] Loss: 12459.519531\n",
      "Train Epoch: 112 [180480/225000 (80%)] Loss: 8194.771484\n",
      "Train Epoch: 112 [184576/225000 (82%)] Loss: 8196.175781\n",
      "Train Epoch: 112 [188672/225000 (84%)] Loss: 9590.650391\n",
      "Train Epoch: 112 [192768/225000 (86%)] Loss: 8178.136719\n",
      "Train Epoch: 112 [196864/225000 (87%)] Loss: 8226.011719\n",
      "Train Epoch: 112 [200960/225000 (89%)] Loss: 8221.031250\n",
      "Train Epoch: 112 [205056/225000 (91%)] Loss: 9590.093750\n",
      "Train Epoch: 112 [209152/225000 (93%)] Loss: 9479.357422\n",
      "Train Epoch: 112 [213248/225000 (95%)] Loss: 9693.466797\n",
      "Train Epoch: 112 [217344/225000 (97%)] Loss: 8255.166016\n",
      "Train Epoch: 112 [221440/225000 (98%)] Loss: 8172.830078\n",
      "    epoch          : 112\n",
      "    loss           : 9556.880486081485\n",
      "    val_loss       : 9446.92799529859\n",
      "Train Epoch: 113 [256/225000 (0%)] Loss: 7979.982422\n",
      "Train Epoch: 113 [4352/225000 (2%)] Loss: 8152.658203\n",
      "Train Epoch: 113 [8448/225000 (4%)] Loss: 8032.107422\n",
      "Train Epoch: 113 [12544/225000 (6%)] Loss: 8192.466797\n",
      "Train Epoch: 113 [16640/225000 (7%)] Loss: 8140.541016\n",
      "Train Epoch: 113 [20736/225000 (9%)] Loss: 9421.695312\n",
      "Train Epoch: 113 [24832/225000 (11%)] Loss: 8020.199219\n",
      "Train Epoch: 113 [28928/225000 (13%)] Loss: 13220.814453\n",
      "Train Epoch: 113 [33024/225000 (15%)] Loss: 8049.902344\n",
      "Train Epoch: 113 [37120/225000 (16%)] Loss: 8276.197266\n",
      "Train Epoch: 113 [41216/225000 (18%)] Loss: 8126.843750\n",
      "Train Epoch: 113 [45312/225000 (20%)] Loss: 7977.796875\n",
      "Train Epoch: 113 [49408/225000 (22%)] Loss: 8078.619141\n",
      "Train Epoch: 113 [53504/225000 (24%)] Loss: 10763.884766\n",
      "Train Epoch: 113 [57600/225000 (26%)] Loss: 8076.568359\n",
      "Train Epoch: 113 [61696/225000 (27%)] Loss: 8083.955078\n",
      "Train Epoch: 113 [65792/225000 (29%)] Loss: 8534.130859\n",
      "Train Epoch: 113 [69888/225000 (31%)] Loss: 19418.076172\n",
      "Train Epoch: 113 [73984/225000 (33%)] Loss: 12895.912109\n",
      "Train Epoch: 113 [78080/225000 (35%)] Loss: 8065.931641\n",
      "Train Epoch: 113 [82176/225000 (37%)] Loss: 8142.011719\n",
      "Train Epoch: 113 [86272/225000 (38%)] Loss: 9595.371094\n",
      "Train Epoch: 113 [90368/225000 (40%)] Loss: 8210.089844\n",
      "Train Epoch: 113 [94464/225000 (42%)] Loss: 7931.835938\n",
      "Train Epoch: 113 [98560/225000 (44%)] Loss: 13853.007812\n",
      "Train Epoch: 113 [102656/225000 (46%)] Loss: 15467.882812\n",
      "Train Epoch: 113 [106752/225000 (47%)] Loss: 11213.687500\n",
      "Train Epoch: 113 [110848/225000 (49%)] Loss: 8238.007812\n",
      "Train Epoch: 113 [114944/225000 (51%)] Loss: 7911.496094\n",
      "Train Epoch: 113 [119040/225000 (53%)] Loss: 8119.212891\n",
      "Train Epoch: 113 [123136/225000 (55%)] Loss: 10536.382812\n",
      "Train Epoch: 113 [127232/225000 (57%)] Loss: 8078.763672\n",
      "Train Epoch: 113 [131328/225000 (58%)] Loss: 8048.464844\n",
      "Train Epoch: 113 [135424/225000 (60%)] Loss: 11247.169922\n",
      "Train Epoch: 113 [139520/225000 (62%)] Loss: 9595.121094\n",
      "Train Epoch: 113 [143616/225000 (64%)] Loss: 8108.382812\n",
      "Train Epoch: 113 [147712/225000 (66%)] Loss: 8087.859375\n",
      "Train Epoch: 113 [151808/225000 (67%)] Loss: 7936.248047\n",
      "Train Epoch: 113 [155904/225000 (69%)] Loss: 8100.945312\n",
      "Train Epoch: 113 [160000/225000 (71%)] Loss: 7938.892578\n",
      "Train Epoch: 113 [164096/225000 (73%)] Loss: 7946.494141\n",
      "Train Epoch: 113 [168192/225000 (75%)] Loss: 8225.416016\n",
      "Train Epoch: 113 [172288/225000 (77%)] Loss: 9799.105469\n",
      "Train Epoch: 113 [176384/225000 (78%)] Loss: 7882.478516\n",
      "Train Epoch: 113 [180480/225000 (80%)] Loss: 8101.146484\n",
      "Train Epoch: 113 [184576/225000 (82%)] Loss: 14080.244141\n",
      "Train Epoch: 113 [188672/225000 (84%)] Loss: 8284.457031\n",
      "Train Epoch: 113 [192768/225000 (86%)] Loss: 9535.539062\n",
      "Train Epoch: 113 [196864/225000 (87%)] Loss: 12374.162109\n",
      "Train Epoch: 113 [200960/225000 (89%)] Loss: 9646.406250\n",
      "Train Epoch: 113 [205056/225000 (91%)] Loss: 8065.746094\n",
      "Train Epoch: 113 [209152/225000 (93%)] Loss: 8065.638672\n",
      "Train Epoch: 113 [213248/225000 (95%)] Loss: 10517.884766\n",
      "Train Epoch: 113 [217344/225000 (97%)] Loss: 16222.865234\n",
      "Train Epoch: 113 [221440/225000 (98%)] Loss: 8036.781250\n",
      "    epoch          : 113\n",
      "    loss           : 9559.04283209791\n",
      "    val_loss       : 9442.044777376312\n",
      "Train Epoch: 114 [256/225000 (0%)] Loss: 10464.552734\n",
      "Train Epoch: 114 [4352/225000 (2%)] Loss: 12532.070312\n",
      "Train Epoch: 114 [8448/225000 (4%)] Loss: 9642.507812\n",
      "Train Epoch: 114 [12544/225000 (6%)] Loss: 7884.417969\n",
      "Train Epoch: 114 [16640/225000 (7%)] Loss: 7806.380859\n",
      "Train Epoch: 114 [20736/225000 (9%)] Loss: 10670.095703\n",
      "Train Epoch: 114 [24832/225000 (11%)] Loss: 12226.597656\n",
      "Train Epoch: 114 [28928/225000 (13%)] Loss: 9705.832031\n",
      "Train Epoch: 114 [33024/225000 (15%)] Loss: 8153.265625\n",
      "Train Epoch: 114 [37120/225000 (16%)] Loss: 8131.369141\n",
      "Train Epoch: 114 [41216/225000 (18%)] Loss: 8245.568359\n",
      "Train Epoch: 114 [45312/225000 (20%)] Loss: 8184.484375\n",
      "Train Epoch: 114 [49408/225000 (22%)] Loss: 12745.939453\n",
      "Train Epoch: 114 [53504/225000 (24%)] Loss: 8157.398438\n",
      "Train Epoch: 114 [57600/225000 (26%)] Loss: 7933.296875\n",
      "Train Epoch: 114 [61696/225000 (27%)] Loss: 7942.708984\n",
      "Train Epoch: 114 [65792/225000 (29%)] Loss: 8052.000000\n",
      "Train Epoch: 114 [69888/225000 (31%)] Loss: 10608.378906\n",
      "Train Epoch: 114 [73984/225000 (33%)] Loss: 9666.916016\n",
      "Train Epoch: 114 [78080/225000 (35%)] Loss: 8012.078125\n",
      "Train Epoch: 114 [82176/225000 (37%)] Loss: 13607.416016\n",
      "Train Epoch: 114 [86272/225000 (38%)] Loss: 9643.011719\n",
      "Train Epoch: 114 [90368/225000 (40%)] Loss: 9676.042969\n",
      "Train Epoch: 114 [94464/225000 (42%)] Loss: 10648.386719\n",
      "Train Epoch: 114 [98560/225000 (44%)] Loss: 9761.542969\n",
      "Train Epoch: 114 [102656/225000 (46%)] Loss: 8026.669922\n",
      "Train Epoch: 114 [106752/225000 (47%)] Loss: 7995.339844\n",
      "Train Epoch: 114 [110848/225000 (49%)] Loss: 8186.808594\n",
      "Train Epoch: 114 [114944/225000 (51%)] Loss: 10673.562500\n",
      "Train Epoch: 114 [119040/225000 (53%)] Loss: 13662.203125\n",
      "Train Epoch: 114 [123136/225000 (55%)] Loss: 8113.691406\n",
      "Train Epoch: 114 [127232/225000 (57%)] Loss: 16390.855469\n",
      "Train Epoch: 114 [131328/225000 (58%)] Loss: 8373.371094\n",
      "Train Epoch: 114 [135424/225000 (60%)] Loss: 10455.240234\n",
      "Train Epoch: 114 [139520/225000 (62%)] Loss: 7959.255859\n",
      "Train Epoch: 114 [143616/225000 (64%)] Loss: 8060.433594\n",
      "Train Epoch: 114 [147712/225000 (66%)] Loss: 8109.390625\n",
      "Train Epoch: 114 [151808/225000 (67%)] Loss: 10402.921875\n",
      "Train Epoch: 114 [155904/225000 (69%)] Loss: 8071.269531\n",
      "Train Epoch: 114 [160000/225000 (71%)] Loss: 15496.695312\n",
      "Train Epoch: 114 [164096/225000 (73%)] Loss: 8350.703125\n",
      "Train Epoch: 114 [168192/225000 (75%)] Loss: 12167.085938\n",
      "Train Epoch: 114 [172288/225000 (77%)] Loss: 7996.427734\n",
      "Train Epoch: 114 [176384/225000 (78%)] Loss: 9561.589844\n",
      "Train Epoch: 114 [180480/225000 (80%)] Loss: 7850.519531\n",
      "Train Epoch: 114 [184576/225000 (82%)] Loss: 17724.869141\n",
      "Train Epoch: 114 [188672/225000 (84%)] Loss: 9493.841797\n",
      "Train Epoch: 114 [192768/225000 (86%)] Loss: 7921.726562\n",
      "Train Epoch: 114 [196864/225000 (87%)] Loss: 10506.011719\n",
      "Train Epoch: 114 [200960/225000 (89%)] Loss: 8106.041016\n",
      "Train Epoch: 114 [205056/225000 (91%)] Loss: 14062.419922\n",
      "Train Epoch: 114 [209152/225000 (93%)] Loss: 8120.384766\n",
      "Train Epoch: 114 [213248/225000 (95%)] Loss: 13014.636719\n",
      "Train Epoch: 114 [217344/225000 (97%)] Loss: 12494.558594\n",
      "Train Epoch: 114 [221440/225000 (98%)] Loss: 10365.021484\n",
      "    epoch          : 114\n",
      "    loss           : 9548.860312677758\n",
      "    val_loss       : 9540.80029055537\n",
      "Train Epoch: 115 [256/225000 (0%)] Loss: 11193.564453\n",
      "Train Epoch: 115 [4352/225000 (2%)] Loss: 15150.033203\n",
      "Train Epoch: 115 [8448/225000 (4%)] Loss: 7950.373047\n",
      "Train Epoch: 115 [12544/225000 (6%)] Loss: 16466.003906\n",
      "Train Epoch: 115 [16640/225000 (7%)] Loss: 9842.160156\n",
      "Train Epoch: 115 [20736/225000 (9%)] Loss: 8256.720703\n",
      "Train Epoch: 115 [24832/225000 (11%)] Loss: 8014.138672\n",
      "Train Epoch: 115 [28928/225000 (13%)] Loss: 8192.328125\n",
      "Train Epoch: 115 [33024/225000 (15%)] Loss: 8153.017578\n",
      "Train Epoch: 115 [37120/225000 (16%)] Loss: 8069.523438\n",
      "Train Epoch: 115 [41216/225000 (18%)] Loss: 7860.269531\n",
      "Train Epoch: 115 [45312/225000 (20%)] Loss: 7944.718750\n",
      "Train Epoch: 115 [49408/225000 (22%)] Loss: 12040.445312\n",
      "Train Epoch: 115 [53504/225000 (24%)] Loss: 8161.197266\n",
      "Train Epoch: 115 [57600/225000 (26%)] Loss: 8122.839844\n",
      "Train Epoch: 115 [61696/225000 (27%)] Loss: 10572.113281\n",
      "Train Epoch: 115 [65792/225000 (29%)] Loss: 9443.966797\n",
      "Train Epoch: 115 [69888/225000 (31%)] Loss: 9443.669922\n",
      "Train Epoch: 115 [73984/225000 (33%)] Loss: 8295.496094\n",
      "Train Epoch: 115 [78080/225000 (35%)] Loss: 12434.080078\n",
      "Train Epoch: 115 [82176/225000 (37%)] Loss: 9717.861328\n",
      "Train Epoch: 115 [86272/225000 (38%)] Loss: 7953.628906\n",
      "Train Epoch: 115 [90368/225000 (40%)] Loss: 8187.173828\n",
      "Train Epoch: 115 [94464/225000 (42%)] Loss: 8065.857422\n",
      "Train Epoch: 115 [98560/225000 (44%)] Loss: 14393.060547\n",
      "Train Epoch: 115 [102656/225000 (46%)] Loss: 8029.015625\n",
      "Train Epoch: 115 [106752/225000 (47%)] Loss: 7959.666016\n",
      "Train Epoch: 115 [110848/225000 (49%)] Loss: 8075.246094\n",
      "Train Epoch: 115 [114944/225000 (51%)] Loss: 7907.167969\n",
      "Train Epoch: 115 [119040/225000 (53%)] Loss: 7907.572266\n",
      "Train Epoch: 115 [123136/225000 (55%)] Loss: 8174.382812\n",
      "Train Epoch: 115 [127232/225000 (57%)] Loss: 8140.716797\n",
      "Train Epoch: 115 [131328/225000 (58%)] Loss: 10464.998047\n",
      "Train Epoch: 115 [135424/225000 (60%)] Loss: 8197.996094\n",
      "Train Epoch: 115 [139520/225000 (62%)] Loss: 10396.314453\n",
      "Train Epoch: 115 [143616/225000 (64%)] Loss: 8205.498047\n",
      "Train Epoch: 115 [147712/225000 (66%)] Loss: 13884.294922\n",
      "Train Epoch: 115 [151808/225000 (67%)] Loss: 13471.031250\n",
      "Train Epoch: 115 [155904/225000 (69%)] Loss: 8148.865234\n",
      "Train Epoch: 115 [160000/225000 (71%)] Loss: 8153.320312\n",
      "Train Epoch: 115 [164096/225000 (73%)] Loss: 8040.445312\n",
      "Train Epoch: 115 [168192/225000 (75%)] Loss: 7877.265625\n",
      "Train Epoch: 115 [172288/225000 (77%)] Loss: 8160.792969\n",
      "Train Epoch: 115 [176384/225000 (78%)] Loss: 13838.095703\n",
      "Train Epoch: 115 [180480/225000 (80%)] Loss: 8037.273438\n",
      "Train Epoch: 115 [184576/225000 (82%)] Loss: 8214.892578\n",
      "Train Epoch: 115 [188672/225000 (84%)] Loss: 12117.423828\n",
      "Train Epoch: 115 [192768/225000 (86%)] Loss: 9632.703125\n",
      "Train Epoch: 115 [196864/225000 (87%)] Loss: 7925.978516\n",
      "Train Epoch: 115 [200960/225000 (89%)] Loss: 10618.560547\n",
      "Train Epoch: 115 [205056/225000 (91%)] Loss: 8131.585938\n",
      "Train Epoch: 115 [209152/225000 (93%)] Loss: 8089.488281\n",
      "Train Epoch: 115 [213248/225000 (95%)] Loss: 12425.755859\n",
      "Train Epoch: 115 [217344/225000 (97%)] Loss: 8138.292969\n",
      "Train Epoch: 115 [221440/225000 (98%)] Loss: 10466.912109\n",
      "    epoch          : 115\n",
      "    loss           : 9434.29669168622\n",
      "    val_loss       : 9465.531309916048\n",
      "Train Epoch: 116 [256/225000 (0%)] Loss: 9335.683594\n",
      "Train Epoch: 116 [4352/225000 (2%)] Loss: 8088.687500\n",
      "Train Epoch: 116 [8448/225000 (4%)] Loss: 12357.748047\n",
      "Train Epoch: 116 [12544/225000 (6%)] Loss: 9566.923828\n",
      "Train Epoch: 116 [16640/225000 (7%)] Loss: 20158.072266\n",
      "Train Epoch: 116 [20736/225000 (9%)] Loss: 10339.369141\n",
      "Train Epoch: 116 [24832/225000 (11%)] Loss: 8001.132812\n",
      "Train Epoch: 116 [28928/225000 (13%)] Loss: 8167.648438\n",
      "Train Epoch: 116 [33024/225000 (15%)] Loss: 14928.582031\n",
      "Train Epoch: 116 [37120/225000 (16%)] Loss: 9521.083984\n",
      "Train Epoch: 116 [41216/225000 (18%)] Loss: 8090.748047\n",
      "Train Epoch: 116 [45312/225000 (20%)] Loss: 8025.255859\n",
      "Train Epoch: 116 [49408/225000 (22%)] Loss: 9964.218750\n",
      "Train Epoch: 116 [53504/225000 (24%)] Loss: 8073.093750\n",
      "Train Epoch: 116 [57600/225000 (26%)] Loss: 12184.472656\n",
      "Train Epoch: 116 [61696/225000 (27%)] Loss: 8120.986328\n",
      "Train Epoch: 116 [65792/225000 (29%)] Loss: 8058.982422\n",
      "Train Epoch: 116 [69888/225000 (31%)] Loss: 9520.179688\n",
      "Train Epoch: 116 [73984/225000 (33%)] Loss: 9633.945312\n",
      "Train Epoch: 116 [78080/225000 (35%)] Loss: 14052.525391\n",
      "Train Epoch: 116 [82176/225000 (37%)] Loss: 8013.470703\n",
      "Train Epoch: 116 [86272/225000 (38%)] Loss: 10500.972656\n",
      "Train Epoch: 116 [90368/225000 (40%)] Loss: 8023.910156\n",
      "Train Epoch: 116 [94464/225000 (42%)] Loss: 13891.275391\n",
      "Train Epoch: 116 [98560/225000 (44%)] Loss: 8152.933594\n",
      "Train Epoch: 116 [102656/225000 (46%)] Loss: 8149.193359\n",
      "Train Epoch: 116 [106752/225000 (47%)] Loss: 8057.312500\n",
      "Train Epoch: 116 [110848/225000 (49%)] Loss: 8204.406250\n",
      "Train Epoch: 116 [114944/225000 (51%)] Loss: 13720.513672\n",
      "Train Epoch: 116 [119040/225000 (53%)] Loss: 7948.328125\n",
      "Train Epoch: 116 [123136/225000 (55%)] Loss: 8144.669922\n",
      "Train Epoch: 116 [127232/225000 (57%)] Loss: 8047.257812\n",
      "Train Epoch: 116 [131328/225000 (58%)] Loss: 8207.958984\n",
      "Train Epoch: 116 [135424/225000 (60%)] Loss: 8117.035156\n",
      "Train Epoch: 116 [139520/225000 (62%)] Loss: 8134.562500\n",
      "Train Epoch: 116 [143616/225000 (64%)] Loss: 8125.212891\n",
      "Train Epoch: 116 [147712/225000 (66%)] Loss: 7872.384766\n",
      "Train Epoch: 116 [151808/225000 (67%)] Loss: 11122.726562\n",
      "Train Epoch: 116 [155904/225000 (69%)] Loss: 8198.726562\n",
      "Train Epoch: 116 [160000/225000 (71%)] Loss: 8041.763672\n",
      "Train Epoch: 116 [164096/225000 (73%)] Loss: 8196.156250\n",
      "Train Epoch: 116 [168192/225000 (75%)] Loss: 13690.431641\n",
      "Train Epoch: 116 [172288/225000 (77%)] Loss: 8057.404297\n",
      "Train Epoch: 116 [176384/225000 (78%)] Loss: 8064.917969\n",
      "Train Epoch: 116 [180480/225000 (80%)] Loss: 13460.480469\n",
      "Train Epoch: 116 [184576/225000 (82%)] Loss: 15295.519531\n",
      "Train Epoch: 116 [188672/225000 (84%)] Loss: 7799.007812\n",
      "Train Epoch: 116 [192768/225000 (86%)] Loss: 10687.796875\n",
      "Train Epoch: 116 [196864/225000 (87%)] Loss: 10723.488281\n",
      "Train Epoch: 116 [200960/225000 (89%)] Loss: 7985.761719\n",
      "Train Epoch: 116 [205056/225000 (91%)] Loss: 9630.648438\n",
      "Train Epoch: 116 [209152/225000 (93%)] Loss: 7843.025391\n",
      "Train Epoch: 116 [213248/225000 (95%)] Loss: 8097.183594\n",
      "Train Epoch: 116 [217344/225000 (97%)] Loss: 8027.109375\n",
      "Train Epoch: 116 [221440/225000 (98%)] Loss: 13692.816406\n",
      "    epoch          : 116\n",
      "    loss           : 9424.676990009955\n",
      "    val_loss       : 10031.880019392285\n",
      "Train Epoch: 117 [256/225000 (0%)] Loss: 8162.746094\n",
      "Train Epoch: 117 [4352/225000 (2%)] Loss: 9675.783203\n",
      "Train Epoch: 117 [8448/225000 (4%)] Loss: 7970.857422\n",
      "Train Epoch: 117 [12544/225000 (6%)] Loss: 8160.445312\n",
      "Train Epoch: 117 [16640/225000 (7%)] Loss: 9605.332031\n",
      "Train Epoch: 117 [20736/225000 (9%)] Loss: 10647.494141\n",
      "Train Epoch: 117 [24832/225000 (11%)] Loss: 7923.501953\n",
      "Train Epoch: 117 [28928/225000 (13%)] Loss: 7738.519531\n",
      "Train Epoch: 117 [33024/225000 (15%)] Loss: 9507.142578\n",
      "Train Epoch: 117 [37120/225000 (16%)] Loss: 7866.421875\n",
      "Train Epoch: 117 [41216/225000 (18%)] Loss: 7828.281250\n",
      "Train Epoch: 117 [45312/225000 (20%)] Loss: 9716.212891\n",
      "Train Epoch: 117 [49408/225000 (22%)] Loss: 8044.121094\n",
      "Train Epoch: 117 [53504/225000 (24%)] Loss: 10647.695312\n",
      "Train Epoch: 117 [57600/225000 (26%)] Loss: 9622.773438\n",
      "Train Epoch: 117 [61696/225000 (27%)] Loss: 8141.939453\n",
      "Train Epoch: 117 [65792/225000 (29%)] Loss: 7973.826172\n",
      "Train Epoch: 117 [69888/225000 (31%)] Loss: 8150.703125\n",
      "Train Epoch: 117 [73984/225000 (33%)] Loss: 7992.871094\n",
      "Train Epoch: 117 [78080/225000 (35%)] Loss: 9866.568359\n",
      "Train Epoch: 117 [82176/225000 (37%)] Loss: 8022.017578\n",
      "Train Epoch: 117 [86272/225000 (38%)] Loss: 7974.871094\n",
      "Train Epoch: 117 [90368/225000 (40%)] Loss: 8097.972656\n",
      "Train Epoch: 117 [94464/225000 (42%)] Loss: 15373.439453\n",
      "Train Epoch: 117 [98560/225000 (44%)] Loss: 8218.048828\n",
      "Train Epoch: 117 [102656/225000 (46%)] Loss: 9675.138672\n",
      "Train Epoch: 117 [106752/225000 (47%)] Loss: 8152.986328\n",
      "Train Epoch: 117 [110848/225000 (49%)] Loss: 8035.019531\n",
      "Train Epoch: 117 [114944/225000 (51%)] Loss: 8121.851562\n",
      "Train Epoch: 117 [119040/225000 (53%)] Loss: 7958.966797\n",
      "Train Epoch: 117 [123136/225000 (55%)] Loss: 12320.603516\n",
      "Train Epoch: 117 [127232/225000 (57%)] Loss: 8236.207031\n",
      "Train Epoch: 117 [131328/225000 (58%)] Loss: 8012.876953\n",
      "Train Epoch: 117 [135424/225000 (60%)] Loss: 9806.572266\n",
      "Train Epoch: 117 [139520/225000 (62%)] Loss: 9854.373047\n",
      "Train Epoch: 117 [143616/225000 (64%)] Loss: 14645.085938\n",
      "Train Epoch: 117 [147712/225000 (66%)] Loss: 9379.691406\n",
      "Train Epoch: 117 [151808/225000 (67%)] Loss: 9666.880859\n",
      "Train Epoch: 117 [155904/225000 (69%)] Loss: 18056.236328\n",
      "Train Epoch: 117 [160000/225000 (71%)] Loss: 8185.841797\n",
      "Train Epoch: 117 [164096/225000 (73%)] Loss: 8223.587891\n",
      "Train Epoch: 117 [168192/225000 (75%)] Loss: 7992.980469\n",
      "Train Epoch: 117 [172288/225000 (77%)] Loss: 8209.388672\n",
      "Train Epoch: 117 [176384/225000 (78%)] Loss: 8104.660156\n",
      "Train Epoch: 117 [180480/225000 (80%)] Loss: 8091.687500\n",
      "Train Epoch: 117 [184576/225000 (82%)] Loss: 9697.701172\n",
      "Train Epoch: 117 [188672/225000 (84%)] Loss: 7989.085938\n",
      "Train Epoch: 117 [192768/225000 (86%)] Loss: 9519.658203\n",
      "Train Epoch: 117 [196864/225000 (87%)] Loss: 8037.744141\n",
      "Train Epoch: 117 [200960/225000 (89%)] Loss: 7977.439453\n",
      "Train Epoch: 117 [205056/225000 (91%)] Loss: 7943.958984\n",
      "Train Epoch: 117 [209152/225000 (93%)] Loss: 8260.630859\n",
      "Train Epoch: 117 [213248/225000 (95%)] Loss: 8045.951172\n",
      "Train Epoch: 117 [217344/225000 (97%)] Loss: 8152.609375\n",
      "Train Epoch: 117 [221440/225000 (98%)] Loss: 9725.691406\n",
      "    epoch          : 117\n",
      "    loss           : 9447.714803754267\n",
      "    val_loss       : 9188.166152589176\n",
      "Train Epoch: 118 [256/225000 (0%)] Loss: 8110.617188\n",
      "Train Epoch: 118 [4352/225000 (2%)] Loss: 7993.896484\n",
      "Train Epoch: 118 [8448/225000 (4%)] Loss: 8077.695312\n",
      "Train Epoch: 118 [12544/225000 (6%)] Loss: 10544.880859\n",
      "Train Epoch: 118 [16640/225000 (7%)] Loss: 8081.539062\n",
      "Train Epoch: 118 [20736/225000 (9%)] Loss: 8044.111328\n",
      "Train Epoch: 118 [24832/225000 (11%)] Loss: 12847.130859\n",
      "Train Epoch: 118 [28928/225000 (13%)] Loss: 10375.123047\n",
      "Train Epoch: 118 [33024/225000 (15%)] Loss: 8049.222656\n",
      "Train Epoch: 118 [37120/225000 (16%)] Loss: 8247.869141\n",
      "Train Epoch: 118 [41216/225000 (18%)] Loss: 7991.701172\n",
      "Train Epoch: 118 [45312/225000 (20%)] Loss: 7952.699219\n",
      "Train Epoch: 118 [49408/225000 (22%)] Loss: 8117.166016\n",
      "Train Epoch: 118 [53504/225000 (24%)] Loss: 8097.617188\n",
      "Train Epoch: 118 [57600/225000 (26%)] Loss: 9715.212891\n",
      "Train Epoch: 118 [61696/225000 (27%)] Loss: 8032.691406\n",
      "Train Epoch: 118 [65792/225000 (29%)] Loss: 12190.167969\n",
      "Train Epoch: 118 [69888/225000 (31%)] Loss: 10514.953125\n",
      "Train Epoch: 118 [73984/225000 (33%)] Loss: 8036.304688\n",
      "Train Epoch: 118 [78080/225000 (35%)] Loss: 12686.164062\n",
      "Train Epoch: 118 [82176/225000 (37%)] Loss: 7956.998047\n",
      "Train Epoch: 118 [86272/225000 (38%)] Loss: 13945.380859\n",
      "Train Epoch: 118 [90368/225000 (40%)] Loss: 9574.048828\n",
      "Train Epoch: 118 [94464/225000 (42%)] Loss: 8126.363281\n",
      "Train Epoch: 118 [98560/225000 (44%)] Loss: 9748.562500\n",
      "Train Epoch: 118 [102656/225000 (46%)] Loss: 8086.234375\n",
      "Train Epoch: 118 [106752/225000 (47%)] Loss: 7896.921875\n",
      "Train Epoch: 118 [110848/225000 (49%)] Loss: 8147.365234\n",
      "Train Epoch: 118 [114944/225000 (51%)] Loss: 9704.566406\n",
      "Train Epoch: 118 [119040/225000 (53%)] Loss: 8061.625000\n",
      "Train Epoch: 118 [123136/225000 (55%)] Loss: 8216.378906\n",
      "Train Epoch: 118 [127232/225000 (57%)] Loss: 8038.568359\n",
      "Train Epoch: 118 [131328/225000 (58%)] Loss: 8283.037109\n",
      "Train Epoch: 118 [135424/225000 (60%)] Loss: 8132.513672\n",
      "Train Epoch: 118 [139520/225000 (62%)] Loss: 7924.968750\n",
      "Train Epoch: 118 [143616/225000 (64%)] Loss: 9647.437500\n",
      "Train Epoch: 118 [147712/225000 (66%)] Loss: 8054.259766\n",
      "Train Epoch: 118 [151808/225000 (67%)] Loss: 8091.457031\n",
      "Train Epoch: 118 [155904/225000 (69%)] Loss: 12225.695312\n",
      "Train Epoch: 118 [160000/225000 (71%)] Loss: 13788.671875\n",
      "Train Epoch: 118 [164096/225000 (73%)] Loss: 8003.642578\n",
      "Train Epoch: 118 [168192/225000 (75%)] Loss: 8033.636719\n",
      "Train Epoch: 118 [172288/225000 (77%)] Loss: 7829.888672\n",
      "Train Epoch: 118 [176384/225000 (78%)] Loss: 10529.982422\n",
      "Train Epoch: 118 [180480/225000 (80%)] Loss: 8078.781250\n",
      "Train Epoch: 118 [184576/225000 (82%)] Loss: 16608.652344\n",
      "Train Epoch: 118 [188672/225000 (84%)] Loss: 8053.939453\n",
      "Train Epoch: 118 [192768/225000 (86%)] Loss: 8068.667969\n",
      "Train Epoch: 118 [196864/225000 (87%)] Loss: 7980.177734\n",
      "Train Epoch: 118 [200960/225000 (89%)] Loss: 12314.003906\n",
      "Train Epoch: 118 [205056/225000 (91%)] Loss: 8075.746094\n",
      "Train Epoch: 118 [209152/225000 (93%)] Loss: 8163.634766\n",
      "Train Epoch: 118 [213248/225000 (95%)] Loss: 13137.310547\n",
      "Train Epoch: 118 [217344/225000 (97%)] Loss: 10525.203125\n",
      "Train Epoch: 118 [221440/225000 (98%)] Loss: 8247.533203\n",
      "    epoch          : 118\n",
      "    loss           : 9524.468956644625\n",
      "    val_loss       : 9543.076138087681\n",
      "Train Epoch: 119 [256/225000 (0%)] Loss: 7967.195312\n",
      "Train Epoch: 119 [4352/225000 (2%)] Loss: 12573.474609\n",
      "Train Epoch: 119 [8448/225000 (4%)] Loss: 7915.923828\n",
      "Train Epoch: 119 [12544/225000 (6%)] Loss: 14936.052734\n",
      "Train Epoch: 119 [16640/225000 (7%)] Loss: 13040.728516\n",
      "Train Epoch: 119 [20736/225000 (9%)] Loss: 9360.945312\n",
      "Train Epoch: 119 [24832/225000 (11%)] Loss: 10414.775391\n",
      "Train Epoch: 119 [28928/225000 (13%)] Loss: 13886.019531\n",
      "Train Epoch: 119 [33024/225000 (15%)] Loss: 8251.185547\n",
      "Train Epoch: 119 [37120/225000 (16%)] Loss: 13924.820312\n",
      "Train Epoch: 119 [41216/225000 (18%)] Loss: 8208.156250\n",
      "Train Epoch: 119 [45312/225000 (20%)] Loss: 7993.335938\n",
      "Train Epoch: 119 [49408/225000 (22%)] Loss: 15755.375000\n",
      "Train Epoch: 119 [53504/225000 (24%)] Loss: 8051.441406\n",
      "Train Epoch: 119 [57600/225000 (26%)] Loss: 9709.960938\n",
      "Train Epoch: 119 [61696/225000 (27%)] Loss: 9598.039062\n",
      "Train Epoch: 119 [65792/225000 (29%)] Loss: 8105.091797\n",
      "Train Epoch: 119 [69888/225000 (31%)] Loss: 9811.308594\n",
      "Train Epoch: 119 [73984/225000 (33%)] Loss: 9638.431641\n",
      "Train Epoch: 119 [78080/225000 (35%)] Loss: 7847.306641\n",
      "Train Epoch: 119 [82176/225000 (37%)] Loss: 12288.494141\n",
      "Train Epoch: 119 [86272/225000 (38%)] Loss: 7971.707031\n",
      "Train Epoch: 119 [90368/225000 (40%)] Loss: 10427.630859\n",
      "Train Epoch: 119 [94464/225000 (42%)] Loss: 8113.939453\n",
      "Train Epoch: 119 [98560/225000 (44%)] Loss: 8225.720703\n",
      "Train Epoch: 119 [102656/225000 (46%)] Loss: 12038.175781\n",
      "Train Epoch: 119 [106752/225000 (47%)] Loss: 8121.101562\n",
      "Train Epoch: 119 [110848/225000 (49%)] Loss: 10509.933594\n",
      "Train Epoch: 119 [114944/225000 (51%)] Loss: 13650.251953\n",
      "Train Epoch: 119 [119040/225000 (53%)] Loss: 7904.298828\n",
      "Train Epoch: 119 [123136/225000 (55%)] Loss: 8094.564453\n",
      "Train Epoch: 119 [127232/225000 (57%)] Loss: 10312.232422\n",
      "Train Epoch: 119 [131328/225000 (58%)] Loss: 8269.269531\n",
      "Train Epoch: 119 [135424/225000 (60%)] Loss: 10521.822266\n",
      "Train Epoch: 119 [139520/225000 (62%)] Loss: 8075.009766\n",
      "Train Epoch: 119 [143616/225000 (64%)] Loss: 9792.599609\n",
      "Train Epoch: 119 [147712/225000 (66%)] Loss: 7894.484375\n",
      "Train Epoch: 119 [151808/225000 (67%)] Loss: 15402.666016\n",
      "Train Epoch: 119 [155904/225000 (69%)] Loss: 9657.306641\n",
      "Train Epoch: 119 [160000/225000 (71%)] Loss: 8156.013672\n",
      "Train Epoch: 119 [164096/225000 (73%)] Loss: 7977.259766\n",
      "Train Epoch: 119 [168192/225000 (75%)] Loss: 11106.675781\n",
      "Train Epoch: 119 [172288/225000 (77%)] Loss: 9620.798828\n",
      "Train Epoch: 119 [176384/225000 (78%)] Loss: 8114.046875\n",
      "Train Epoch: 119 [180480/225000 (80%)] Loss: 8040.460938\n",
      "Train Epoch: 119 [184576/225000 (82%)] Loss: 8075.976562\n",
      "Train Epoch: 119 [188672/225000 (84%)] Loss: 7922.916016\n",
      "Train Epoch: 119 [192768/225000 (86%)] Loss: 10451.589844\n",
      "Train Epoch: 119 [196864/225000 (87%)] Loss: 9644.472656\n",
      "Train Epoch: 119 [200960/225000 (89%)] Loss: 8114.484375\n",
      "Train Epoch: 119 [205056/225000 (91%)] Loss: 8115.068359\n",
      "Train Epoch: 119 [209152/225000 (93%)] Loss: 10583.808594\n",
      "Train Epoch: 119 [213248/225000 (95%)] Loss: 8044.441406\n",
      "Train Epoch: 119 [217344/225000 (97%)] Loss: 7847.439453\n",
      "Train Epoch: 119 [221440/225000 (98%)] Loss: 9542.626953\n",
      "    epoch          : 119\n",
      "    loss           : 9561.033323112202\n",
      "    val_loss       : 9366.286078963962\n",
      "Train Epoch: 120 [256/225000 (0%)] Loss: 10510.914062\n",
      "Train Epoch: 120 [4352/225000 (2%)] Loss: 8115.654297\n",
      "Train Epoch: 120 [8448/225000 (4%)] Loss: 8108.367188\n",
      "Train Epoch: 120 [12544/225000 (6%)] Loss: 10492.621094\n",
      "Train Epoch: 120 [16640/225000 (7%)] Loss: 16084.564453\n",
      "Train Epoch: 120 [20736/225000 (9%)] Loss: 16570.886719\n",
      "Train Epoch: 120 [24832/225000 (11%)] Loss: 10603.681641\n",
      "Train Epoch: 120 [28928/225000 (13%)] Loss: 13973.923828\n",
      "Train Epoch: 120 [33024/225000 (15%)] Loss: 7991.371094\n",
      "Train Epoch: 120 [37120/225000 (16%)] Loss: 7981.798828\n",
      "Train Epoch: 120 [41216/225000 (18%)] Loss: 9727.683594\n",
      "Train Epoch: 120 [45312/225000 (20%)] Loss: 7966.929688\n",
      "Train Epoch: 120 [49408/225000 (22%)] Loss: 10517.373047\n",
      "Train Epoch: 120 [53504/225000 (24%)] Loss: 9344.457031\n",
      "Train Epoch: 120 [57600/225000 (26%)] Loss: 16221.218750\n",
      "Train Epoch: 120 [61696/225000 (27%)] Loss: 8038.142578\n",
      "Train Epoch: 120 [65792/225000 (29%)] Loss: 8034.005859\n",
      "Train Epoch: 120 [69888/225000 (31%)] Loss: 12684.671875\n",
      "Train Epoch: 120 [73984/225000 (33%)] Loss: 8096.197266\n",
      "Train Epoch: 120 [78080/225000 (35%)] Loss: 9590.900391\n",
      "Train Epoch: 120 [82176/225000 (37%)] Loss: 7932.091797\n",
      "Train Epoch: 120 [86272/225000 (38%)] Loss: 9599.546875\n",
      "Train Epoch: 120 [90368/225000 (40%)] Loss: 7945.443359\n",
      "Train Epoch: 120 [94464/225000 (42%)] Loss: 8074.160156\n",
      "Train Epoch: 120 [98560/225000 (44%)] Loss: 9648.146484\n",
      "Train Epoch: 120 [102656/225000 (46%)] Loss: 8311.212891\n",
      "Train Epoch: 120 [106752/225000 (47%)] Loss: 8030.062500\n",
      "Train Epoch: 120 [110848/225000 (49%)] Loss: 8106.339844\n",
      "Train Epoch: 120 [114944/225000 (51%)] Loss: 7871.181641\n",
      "Train Epoch: 120 [119040/225000 (53%)] Loss: 8028.148438\n",
      "Train Epoch: 120 [123136/225000 (55%)] Loss: 14071.179688\n",
      "Train Epoch: 120 [127232/225000 (57%)] Loss: 7945.626953\n",
      "Train Epoch: 120 [131328/225000 (58%)] Loss: 10424.777344\n",
      "Train Epoch: 120 [135424/225000 (60%)] Loss: 16522.980469\n",
      "Train Epoch: 120 [139520/225000 (62%)] Loss: 10567.132812\n",
      "Train Epoch: 120 [143616/225000 (64%)] Loss: 7867.478516\n",
      "Train Epoch: 120 [147712/225000 (66%)] Loss: 7981.591797\n",
      "Train Epoch: 120 [151808/225000 (67%)] Loss: 8053.156250\n",
      "Train Epoch: 120 [155904/225000 (69%)] Loss: 10478.945312\n",
      "Train Epoch: 120 [160000/225000 (71%)] Loss: 8084.195312\n",
      "Train Epoch: 120 [164096/225000 (73%)] Loss: 8071.593750\n",
      "Train Epoch: 120 [168192/225000 (75%)] Loss: 12958.306641\n",
      "Train Epoch: 120 [172288/225000 (77%)] Loss: 7939.324219\n",
      "Train Epoch: 120 [176384/225000 (78%)] Loss: 8165.652344\n",
      "Train Epoch: 120 [180480/225000 (80%)] Loss: 7986.503906\n",
      "Train Epoch: 120 [184576/225000 (82%)] Loss: 11966.234375\n",
      "Train Epoch: 120 [188672/225000 (84%)] Loss: 11012.943359\n",
      "Train Epoch: 120 [192768/225000 (86%)] Loss: 7990.378906\n",
      "Train Epoch: 120 [196864/225000 (87%)] Loss: 8030.464844\n",
      "Train Epoch: 120 [200960/225000 (89%)] Loss: 10690.917969\n",
      "Train Epoch: 120 [205056/225000 (91%)] Loss: 8148.265625\n",
      "Train Epoch: 120 [209152/225000 (93%)] Loss: 8277.158203\n",
      "Train Epoch: 120 [213248/225000 (95%)] Loss: 7990.197266\n",
      "Train Epoch: 120 [217344/225000 (97%)] Loss: 7946.470703\n",
      "Train Epoch: 120 [221440/225000 (98%)] Loss: 8071.871094\n",
      "    epoch          : 120\n",
      "    loss           : 9513.373173528156\n",
      "    val_loss       : 9530.403806876164\n",
      "Train Epoch: 121 [256/225000 (0%)] Loss: 8061.972656\n",
      "Train Epoch: 121 [4352/225000 (2%)] Loss: 8112.994141\n",
      "Train Epoch: 121 [8448/225000 (4%)] Loss: 8075.072266\n",
      "Train Epoch: 121 [12544/225000 (6%)] Loss: 8041.785156\n",
      "Train Epoch: 121 [16640/225000 (7%)] Loss: 8131.753906\n",
      "Train Epoch: 121 [20736/225000 (9%)] Loss: 10383.417969\n",
      "Train Epoch: 121 [24832/225000 (11%)] Loss: 8095.230469\n",
      "Train Epoch: 121 [28928/225000 (13%)] Loss: 9528.691406\n",
      "Train Epoch: 121 [33024/225000 (15%)] Loss: 7961.417969\n",
      "Train Epoch: 121 [37120/225000 (16%)] Loss: 8068.746094\n",
      "Train Epoch: 121 [41216/225000 (18%)] Loss: 11246.333984\n",
      "Train Epoch: 121 [45312/225000 (20%)] Loss: 14239.851562\n",
      "Train Epoch: 121 [49408/225000 (22%)] Loss: 7994.160156\n",
      "Train Epoch: 121 [53504/225000 (24%)] Loss: 14030.824219\n",
      "Train Epoch: 121 [57600/225000 (26%)] Loss: 8015.589844\n",
      "Train Epoch: 121 [61696/225000 (27%)] Loss: 10391.416016\n",
      "Train Epoch: 121 [65792/225000 (29%)] Loss: 14563.513672\n",
      "Train Epoch: 121 [69888/225000 (31%)] Loss: 10540.226562\n",
      "Train Epoch: 121 [73984/225000 (33%)] Loss: 8106.949219\n",
      "Train Epoch: 121 [78080/225000 (35%)] Loss: 7800.580078\n",
      "Train Epoch: 121 [82176/225000 (37%)] Loss: 8111.337891\n",
      "Train Epoch: 121 [86272/225000 (38%)] Loss: 13703.257812\n",
      "Train Epoch: 121 [90368/225000 (40%)] Loss: 8082.183594\n",
      "Train Epoch: 121 [94464/225000 (42%)] Loss: 12814.431641\n",
      "Train Epoch: 121 [98560/225000 (44%)] Loss: 12196.253906\n",
      "Train Epoch: 121 [102656/225000 (46%)] Loss: 9609.273438\n",
      "Train Epoch: 121 [106752/225000 (47%)] Loss: 8004.011719\n",
      "Train Epoch: 121 [110848/225000 (49%)] Loss: 11160.490234\n",
      "Train Epoch: 121 [114944/225000 (51%)] Loss: 8213.550781\n",
      "Train Epoch: 121 [119040/225000 (53%)] Loss: 8084.917969\n",
      "Train Epoch: 121 [123136/225000 (55%)] Loss: 8177.734375\n",
      "Train Epoch: 121 [127232/225000 (57%)] Loss: 8002.800781\n",
      "Train Epoch: 121 [131328/225000 (58%)] Loss: 8032.277344\n",
      "Train Epoch: 121 [135424/225000 (60%)] Loss: 8100.417969\n",
      "Train Epoch: 121 [139520/225000 (62%)] Loss: 9743.039062\n",
      "Train Epoch: 121 [143616/225000 (64%)] Loss: 15199.648438\n",
      "Train Epoch: 121 [147712/225000 (66%)] Loss: 12071.992188\n",
      "Train Epoch: 121 [151808/225000 (67%)] Loss: 7960.001953\n",
      "Train Epoch: 121 [155904/225000 (69%)] Loss: 8038.519531\n",
      "Train Epoch: 121 [160000/225000 (71%)] Loss: 12123.851562\n",
      "Train Epoch: 121 [164096/225000 (73%)] Loss: 12566.853516\n",
      "Train Epoch: 121 [168192/225000 (75%)] Loss: 9572.023438\n",
      "Train Epoch: 121 [172288/225000 (77%)] Loss: 9451.003906\n",
      "Train Epoch: 121 [176384/225000 (78%)] Loss: 9682.570312\n",
      "Train Epoch: 121 [180480/225000 (80%)] Loss: 9701.136719\n",
      "Train Epoch: 121 [184576/225000 (82%)] Loss: 8038.269531\n",
      "Train Epoch: 121 [188672/225000 (84%)] Loss: 12152.472656\n",
      "Train Epoch: 121 [192768/225000 (86%)] Loss: 7954.232422\n",
      "Train Epoch: 121 [196864/225000 (87%)] Loss: 7984.605469\n",
      "Train Epoch: 121 [200960/225000 (89%)] Loss: 9553.755859\n",
      "Train Epoch: 121 [205056/225000 (91%)] Loss: 8117.474609\n",
      "Train Epoch: 121 [209152/225000 (93%)] Loss: 8055.248047\n",
      "Train Epoch: 121 [213248/225000 (95%)] Loss: 9542.449219\n",
      "Train Epoch: 121 [217344/225000 (97%)] Loss: 9465.015625\n",
      "Train Epoch: 121 [221440/225000 (98%)] Loss: 8179.097656\n",
      "    epoch          : 121\n",
      "    loss           : 9539.876248755689\n",
      "    val_loss       : 9424.433630600268\n",
      "Train Epoch: 122 [256/225000 (0%)] Loss: 8051.144531\n",
      "Train Epoch: 122 [4352/225000 (2%)] Loss: 8376.601562\n",
      "Train Epoch: 122 [8448/225000 (4%)] Loss: 7989.396484\n",
      "Train Epoch: 122 [12544/225000 (6%)] Loss: 8037.828125\n",
      "Train Epoch: 122 [16640/225000 (7%)] Loss: 12270.759766\n",
      "Train Epoch: 122 [20736/225000 (9%)] Loss: 7887.076172\n",
      "Train Epoch: 122 [24832/225000 (11%)] Loss: 8025.353516\n",
      "Train Epoch: 122 [28928/225000 (13%)] Loss: 8166.382812\n",
      "Train Epoch: 122 [33024/225000 (15%)] Loss: 10727.435547\n",
      "Train Epoch: 122 [37120/225000 (16%)] Loss: 12339.847656\n",
      "Train Epoch: 122 [41216/225000 (18%)] Loss: 9714.929688\n",
      "Train Epoch: 122 [45312/225000 (20%)] Loss: 9816.982422\n",
      "Train Epoch: 122 [49408/225000 (22%)] Loss: 8048.855469\n",
      "Train Epoch: 122 [53504/225000 (24%)] Loss: 8170.250000\n",
      "Train Epoch: 122 [57600/225000 (26%)] Loss: 7962.406250\n",
      "Train Epoch: 122 [61696/225000 (27%)] Loss: 8170.646484\n",
      "Train Epoch: 122 [65792/225000 (29%)] Loss: 15405.896484\n",
      "Train Epoch: 122 [69888/225000 (31%)] Loss: 7852.968750\n",
      "Train Epoch: 122 [73984/225000 (33%)] Loss: 8234.708984\n",
      "Train Epoch: 122 [78080/225000 (35%)] Loss: 8080.130859\n",
      "Train Epoch: 122 [82176/225000 (37%)] Loss: 10526.666016\n",
      "Train Epoch: 122 [86272/225000 (38%)] Loss: 8025.355469\n",
      "Train Epoch: 122 [90368/225000 (40%)] Loss: 7956.621094\n",
      "Train Epoch: 122 [94464/225000 (42%)] Loss: 8102.027344\n",
      "Train Epoch: 122 [98560/225000 (44%)] Loss: 9512.396484\n",
      "Train Epoch: 122 [102656/225000 (46%)] Loss: 7891.751953\n",
      "Train Epoch: 122 [106752/225000 (47%)] Loss: 8094.363281\n",
      "Train Epoch: 122 [110848/225000 (49%)] Loss: 10470.019531\n",
      "Train Epoch: 122 [114944/225000 (51%)] Loss: 9704.156250\n",
      "Train Epoch: 122 [119040/225000 (53%)] Loss: 9737.242188\n",
      "Train Epoch: 122 [123136/225000 (55%)] Loss: 8287.910156\n",
      "Train Epoch: 122 [127232/225000 (57%)] Loss: 7953.521484\n",
      "Train Epoch: 122 [131328/225000 (58%)] Loss: 8221.445312\n",
      "Train Epoch: 122 [135424/225000 (60%)] Loss: 10404.761719\n",
      "Train Epoch: 122 [139520/225000 (62%)] Loss: 8119.457031\n",
      "Train Epoch: 122 [143616/225000 (64%)] Loss: 8498.060547\n",
      "Train Epoch: 122 [147712/225000 (66%)] Loss: 8006.636719\n",
      "Train Epoch: 122 [151808/225000 (67%)] Loss: 7968.525391\n",
      "Train Epoch: 122 [155904/225000 (69%)] Loss: 7810.269531\n",
      "Train Epoch: 122 [160000/225000 (71%)] Loss: 8064.863281\n",
      "Train Epoch: 122 [164096/225000 (73%)] Loss: 8131.236328\n",
      "Train Epoch: 122 [168192/225000 (75%)] Loss: 8047.847656\n",
      "Train Epoch: 122 [172288/225000 (77%)] Loss: 8171.390625\n",
      "Train Epoch: 122 [176384/225000 (78%)] Loss: 12745.318359\n",
      "Train Epoch: 122 [180480/225000 (80%)] Loss: 8055.542969\n",
      "Train Epoch: 122 [184576/225000 (82%)] Loss: 10425.474609\n",
      "Train Epoch: 122 [188672/225000 (84%)] Loss: 7886.923828\n",
      "Train Epoch: 122 [192768/225000 (86%)] Loss: 8098.400391\n",
      "Train Epoch: 122 [196864/225000 (87%)] Loss: 16206.376953\n",
      "Train Epoch: 122 [200960/225000 (89%)] Loss: 8149.914062\n",
      "Train Epoch: 122 [205056/225000 (91%)] Loss: 7977.398438\n",
      "Train Epoch: 122 [209152/225000 (93%)] Loss: 7858.283203\n",
      "Train Epoch: 122 [213248/225000 (95%)] Loss: 7983.367188\n",
      "Train Epoch: 122 [217344/225000 (97%)] Loss: 8074.597656\n",
      "Train Epoch: 122 [221440/225000 (98%)] Loss: 8080.591797\n",
      "    epoch          : 122\n",
      "    loss           : 9509.482033027589\n",
      "    val_loss       : 9605.04294003759\n",
      "Train Epoch: 123 [256/225000 (0%)] Loss: 8050.130859\n",
      "Train Epoch: 123 [4352/225000 (2%)] Loss: 9598.583984\n",
      "Train Epoch: 123 [8448/225000 (4%)] Loss: 8016.414062\n",
      "Train Epoch: 123 [12544/225000 (6%)] Loss: 8096.894531\n",
      "Train Epoch: 123 [16640/225000 (7%)] Loss: 10501.578125\n",
      "Train Epoch: 123 [20736/225000 (9%)] Loss: 13680.263672\n",
      "Train Epoch: 123 [24832/225000 (11%)] Loss: 7968.220703\n",
      "Train Epoch: 123 [28928/225000 (13%)] Loss: 8088.496094\n",
      "Train Epoch: 123 [33024/225000 (15%)] Loss: 7871.339844\n",
      "Train Epoch: 123 [37120/225000 (16%)] Loss: 13554.828125\n",
      "Train Epoch: 123 [41216/225000 (18%)] Loss: 7984.527344\n",
      "Train Epoch: 123 [45312/225000 (20%)] Loss: 13236.787109\n",
      "Train Epoch: 123 [49408/225000 (22%)] Loss: 7966.527344\n",
      "Train Epoch: 123 [53504/225000 (24%)] Loss: 10420.935547\n",
      "Train Epoch: 123 [57600/225000 (26%)] Loss: 8029.095703\n",
      "Train Epoch: 123 [61696/225000 (27%)] Loss: 10608.849609\n",
      "Train Epoch: 123 [65792/225000 (29%)] Loss: 7996.587891\n",
      "Train Epoch: 123 [69888/225000 (31%)] Loss: 10757.695312\n",
      "Train Epoch: 123 [73984/225000 (33%)] Loss: 8220.962891\n",
      "Train Epoch: 123 [78080/225000 (35%)] Loss: 8157.117188\n",
      "Train Epoch: 123 [82176/225000 (37%)] Loss: 8192.183594\n",
      "Train Epoch: 123 [86272/225000 (38%)] Loss: 9597.720703\n",
      "Train Epoch: 123 [90368/225000 (40%)] Loss: 7934.773438\n",
      "Train Epoch: 123 [94464/225000 (42%)] Loss: 7894.800781\n",
      "Train Epoch: 123 [98560/225000 (44%)] Loss: 8088.009766\n",
      "Train Epoch: 123 [102656/225000 (46%)] Loss: 8203.486328\n",
      "Train Epoch: 123 [106752/225000 (47%)] Loss: 8045.980469\n",
      "Train Epoch: 123 [110848/225000 (49%)] Loss: 8254.607422\n",
      "Train Epoch: 123 [114944/225000 (51%)] Loss: 10541.400391\n",
      "Train Epoch: 123 [119040/225000 (53%)] Loss: 13955.564453\n",
      "Train Epoch: 123 [123136/225000 (55%)] Loss: 11088.583984\n",
      "Train Epoch: 123 [127232/225000 (57%)] Loss: 8151.412109\n",
      "Train Epoch: 123 [131328/225000 (58%)] Loss: 8481.660156\n",
      "Train Epoch: 123 [135424/225000 (60%)] Loss: 8017.248047\n",
      "Train Epoch: 123 [139520/225000 (62%)] Loss: 8104.304688\n",
      "Train Epoch: 123 [143616/225000 (64%)] Loss: 8065.650391\n",
      "Train Epoch: 123 [147712/225000 (66%)] Loss: 9640.265625\n",
      "Train Epoch: 123 [151808/225000 (67%)] Loss: 8153.431641\n",
      "Train Epoch: 123 [155904/225000 (69%)] Loss: 8042.103516\n",
      "Train Epoch: 123 [160000/225000 (71%)] Loss: 8033.964844\n",
      "Train Epoch: 123 [164096/225000 (73%)] Loss: 8144.390625\n",
      "Train Epoch: 123 [168192/225000 (75%)] Loss: 8190.550781\n",
      "Train Epoch: 123 [172288/225000 (77%)] Loss: 9595.705078\n",
      "Train Epoch: 123 [176384/225000 (78%)] Loss: 8076.736328\n",
      "Train Epoch: 123 [180480/225000 (80%)] Loss: 9647.632812\n",
      "Train Epoch: 123 [184576/225000 (82%)] Loss: 8098.144531\n",
      "Train Epoch: 123 [188672/225000 (84%)] Loss: 9307.583984\n",
      "Train Epoch: 123 [192768/225000 (86%)] Loss: 8095.880859\n",
      "Train Epoch: 123 [196864/225000 (87%)] Loss: 10599.816406\n",
      "Train Epoch: 123 [200960/225000 (89%)] Loss: 13752.058594\n",
      "Train Epoch: 123 [205056/225000 (91%)] Loss: 13919.519531\n",
      "Train Epoch: 123 [209152/225000 (93%)] Loss: 7963.994141\n",
      "Train Epoch: 123 [213248/225000 (95%)] Loss: 8019.564453\n",
      "Train Epoch: 123 [217344/225000 (97%)] Loss: 7915.083984\n",
      "Train Epoch: 123 [221440/225000 (98%)] Loss: 8013.048828\n",
      "    epoch          : 123\n",
      "    loss           : 9413.585388669653\n",
      "    val_loss       : 9410.813034944389\n",
      "Train Epoch: 124 [256/225000 (0%)] Loss: 9668.423828\n",
      "Train Epoch: 124 [4352/225000 (2%)] Loss: 8062.685547\n",
      "Train Epoch: 124 [8448/225000 (4%)] Loss: 9600.021484\n",
      "Train Epoch: 124 [12544/225000 (6%)] Loss: 9590.986328\n",
      "Train Epoch: 124 [16640/225000 (7%)] Loss: 8066.521484\n",
      "Train Epoch: 124 [20736/225000 (9%)] Loss: 7971.572266\n",
      "Train Epoch: 124 [24832/225000 (11%)] Loss: 7957.771484\n",
      "Train Epoch: 124 [28928/225000 (13%)] Loss: 8304.955078\n",
      "Train Epoch: 124 [33024/225000 (15%)] Loss: 10628.292969\n",
      "Train Epoch: 124 [37120/225000 (16%)] Loss: 8216.423828\n",
      "Train Epoch: 124 [41216/225000 (18%)] Loss: 8000.667969\n",
      "Train Epoch: 124 [45312/225000 (20%)] Loss: 9828.933594\n",
      "Train Epoch: 124 [49408/225000 (22%)] Loss: 9806.212891\n",
      "Train Epoch: 124 [53504/225000 (24%)] Loss: 7912.224609\n",
      "Train Epoch: 124 [57600/225000 (26%)] Loss: 9708.705078\n",
      "Train Epoch: 124 [61696/225000 (27%)] Loss: 8047.808594\n",
      "Train Epoch: 124 [65792/225000 (29%)] Loss: 8087.628906\n",
      "Train Epoch: 124 [69888/225000 (31%)] Loss: 7849.167969\n",
      "Train Epoch: 124 [73984/225000 (33%)] Loss: 7962.607422\n",
      "Train Epoch: 124 [78080/225000 (35%)] Loss: 8117.154297\n",
      "Train Epoch: 124 [82176/225000 (37%)] Loss: 8214.058594\n",
      "Train Epoch: 124 [86272/225000 (38%)] Loss: 7925.662109\n",
      "Train Epoch: 124 [90368/225000 (40%)] Loss: 8216.652344\n",
      "Train Epoch: 124 [94464/225000 (42%)] Loss: 12856.722656\n",
      "Train Epoch: 124 [98560/225000 (44%)] Loss: 8079.660156\n",
      "Train Epoch: 124 [102656/225000 (46%)] Loss: 8161.806641\n",
      "Train Epoch: 124 [106752/225000 (47%)] Loss: 7990.298828\n",
      "Train Epoch: 124 [110848/225000 (49%)] Loss: 8071.666016\n",
      "Train Epoch: 124 [114944/225000 (51%)] Loss: 8059.603516\n",
      "Train Epoch: 124 [119040/225000 (53%)] Loss: 15418.703125\n",
      "Train Epoch: 124 [123136/225000 (55%)] Loss: 15416.105469\n",
      "Train Epoch: 124 [127232/225000 (57%)] Loss: 9740.453125\n",
      "Train Epoch: 124 [131328/225000 (58%)] Loss: 7974.748047\n",
      "Train Epoch: 124 [135424/225000 (60%)] Loss: 8048.136719\n",
      "Train Epoch: 124 [139520/225000 (62%)] Loss: 7915.064453\n",
      "Train Epoch: 124 [143616/225000 (64%)] Loss: 13550.191406\n",
      "Train Epoch: 124 [147712/225000 (66%)] Loss: 8127.427734\n",
      "Train Epoch: 124 [151808/225000 (67%)] Loss: 8008.921875\n",
      "Train Epoch: 124 [155904/225000 (69%)] Loss: 10349.039062\n",
      "Train Epoch: 124 [160000/225000 (71%)] Loss: 20706.580078\n",
      "Train Epoch: 124 [164096/225000 (73%)] Loss: 12926.574219\n",
      "Train Epoch: 124 [168192/225000 (75%)] Loss: 10633.023438\n",
      "Train Epoch: 124 [172288/225000 (77%)] Loss: 10717.402344\n",
      "Train Epoch: 124 [176384/225000 (78%)] Loss: 13660.554688\n",
      "Train Epoch: 124 [180480/225000 (80%)] Loss: 15984.755859\n",
      "Train Epoch: 124 [184576/225000 (82%)] Loss: 8117.775391\n",
      "Train Epoch: 124 [188672/225000 (84%)] Loss: 8065.431641\n",
      "Train Epoch: 124 [192768/225000 (86%)] Loss: 10581.052734\n",
      "Train Epoch: 124 [196864/225000 (87%)] Loss: 9682.160156\n",
      "Train Epoch: 124 [200960/225000 (89%)] Loss: 8088.695312\n",
      "Train Epoch: 124 [205056/225000 (91%)] Loss: 12350.380859\n",
      "Train Epoch: 124 [209152/225000 (93%)] Loss: 7861.541016\n",
      "Train Epoch: 124 [213248/225000 (95%)] Loss: 8201.847656\n",
      "Train Epoch: 124 [217344/225000 (97%)] Loss: 8088.337891\n",
      "Train Epoch: 124 [221440/225000 (98%)] Loss: 9547.697266\n",
      "    epoch          : 124\n",
      "    loss           : 9652.67571681243\n",
      "    val_loss       : 9235.488318061341\n",
      "Train Epoch: 125 [256/225000 (0%)] Loss: 7953.291016\n",
      "Train Epoch: 125 [4352/225000 (2%)] Loss: 10452.349609\n",
      "Train Epoch: 125 [8448/225000 (4%)] Loss: 8104.132812\n",
      "Train Epoch: 125 [12544/225000 (6%)] Loss: 7896.062500\n",
      "Train Epoch: 125 [16640/225000 (7%)] Loss: 9728.207031\n",
      "Train Epoch: 125 [20736/225000 (9%)] Loss: 7977.146484\n",
      "Train Epoch: 125 [24832/225000 (11%)] Loss: 12202.968750\n",
      "Train Epoch: 125 [28928/225000 (13%)] Loss: 8005.013672\n",
      "Train Epoch: 125 [33024/225000 (15%)] Loss: 8002.003906\n",
      "Train Epoch: 125 [37120/225000 (16%)] Loss: 8212.220703\n",
      "Train Epoch: 125 [41216/225000 (18%)] Loss: 7983.308594\n",
      "Train Epoch: 125 [45312/225000 (20%)] Loss: 10657.119141\n",
      "Train Epoch: 125 [49408/225000 (22%)] Loss: 8074.023438\n",
      "Train Epoch: 125 [53504/225000 (24%)] Loss: 10448.089844\n",
      "Train Epoch: 125 [57600/225000 (26%)] Loss: 14105.998047\n",
      "Train Epoch: 125 [61696/225000 (27%)] Loss: 13596.695312\n",
      "Train Epoch: 125 [65792/225000 (29%)] Loss: 7987.841797\n",
      "Train Epoch: 125 [69888/225000 (31%)] Loss: 7994.291016\n",
      "Train Epoch: 125 [73984/225000 (33%)] Loss: 8208.371094\n",
      "Train Epoch: 125 [78080/225000 (35%)] Loss: 9782.523438\n",
      "Train Epoch: 125 [82176/225000 (37%)] Loss: 7971.408203\n",
      "Train Epoch: 125 [86272/225000 (38%)] Loss: 8086.105469\n",
      "Train Epoch: 125 [90368/225000 (40%)] Loss: 10587.507812\n",
      "Train Epoch: 125 [94464/225000 (42%)] Loss: 8225.941406\n",
      "Train Epoch: 125 [98560/225000 (44%)] Loss: 7963.173828\n",
      "Train Epoch: 125 [102656/225000 (46%)] Loss: 12069.433594\n",
      "Train Epoch: 125 [106752/225000 (47%)] Loss: 7843.798828\n",
      "Train Epoch: 125 [110848/225000 (49%)] Loss: 7844.611328\n",
      "Train Epoch: 125 [114944/225000 (51%)] Loss: 13642.394531\n",
      "Train Epoch: 125 [119040/225000 (53%)] Loss: 8017.955078\n",
      "Train Epoch: 125 [123136/225000 (55%)] Loss: 8181.218750\n",
      "Train Epoch: 125 [127232/225000 (57%)] Loss: 12252.458984\n",
      "Train Epoch: 125 [131328/225000 (58%)] Loss: 8068.123047\n",
      "Train Epoch: 125 [135424/225000 (60%)] Loss: 8077.125000\n",
      "Train Epoch: 125 [139520/225000 (62%)] Loss: 8171.814453\n",
      "Train Epoch: 125 [143616/225000 (64%)] Loss: 7997.906250\n",
      "Train Epoch: 125 [147712/225000 (66%)] Loss: 13861.755859\n",
      "Train Epoch: 125 [151808/225000 (67%)] Loss: 10514.480469\n",
      "Train Epoch: 125 [155904/225000 (69%)] Loss: 17173.074219\n",
      "Train Epoch: 125 [160000/225000 (71%)] Loss: 7908.490234\n",
      "Train Epoch: 125 [164096/225000 (73%)] Loss: 8084.369141\n",
      "Train Epoch: 125 [168192/225000 (75%)] Loss: 13721.933594\n",
      "Train Epoch: 125 [172288/225000 (77%)] Loss: 8136.005859\n",
      "Train Epoch: 125 [176384/225000 (78%)] Loss: 7954.375000\n",
      "Train Epoch: 125 [180480/225000 (80%)] Loss: 9717.654297\n",
      "Train Epoch: 125 [184576/225000 (82%)] Loss: 8080.314453\n",
      "Train Epoch: 125 [188672/225000 (84%)] Loss: 8064.380859\n",
      "Train Epoch: 125 [192768/225000 (86%)] Loss: 13795.417969\n",
      "Train Epoch: 125 [196864/225000 (87%)] Loss: 8022.412109\n",
      "Train Epoch: 125 [200960/225000 (89%)] Loss: 13855.367188\n",
      "Train Epoch: 125 [205056/225000 (91%)] Loss: 8290.708984\n",
      "Train Epoch: 125 [209152/225000 (93%)] Loss: 7894.888672\n",
      "Train Epoch: 125 [213248/225000 (95%)] Loss: 7959.816406\n",
      "Train Epoch: 125 [217344/225000 (97%)] Loss: 8259.326172\n",
      "Train Epoch: 125 [221440/225000 (98%)] Loss: 8314.611328\n",
      "    epoch          : 125\n",
      "    loss           : 9488.245564917519\n",
      "    val_loss       : 9442.7949557833\n",
      "Train Epoch: 126 [256/225000 (0%)] Loss: 8175.755859\n",
      "Train Epoch: 126 [4352/225000 (2%)] Loss: 8026.126953\n",
      "Train Epoch: 126 [8448/225000 (4%)] Loss: 8072.958984\n",
      "Train Epoch: 126 [12544/225000 (6%)] Loss: 8114.312500\n",
      "Train Epoch: 126 [16640/225000 (7%)] Loss: 8060.474609\n",
      "Train Epoch: 126 [20736/225000 (9%)] Loss: 8129.021484\n",
      "Train Epoch: 126 [24832/225000 (11%)] Loss: 8181.703125\n",
      "Train Epoch: 126 [28928/225000 (13%)] Loss: 8103.039062\n",
      "Train Epoch: 126 [33024/225000 (15%)] Loss: 8408.126953\n",
      "Train Epoch: 126 [37120/225000 (16%)] Loss: 10542.750000\n",
      "Train Epoch: 126 [41216/225000 (18%)] Loss: 13710.519531\n",
      "Train Epoch: 126 [45312/225000 (20%)] Loss: 8201.640625\n",
      "Train Epoch: 126 [49408/225000 (22%)] Loss: 7977.111328\n",
      "Train Epoch: 126 [53504/225000 (24%)] Loss: 13763.500000\n",
      "Train Epoch: 126 [57600/225000 (26%)] Loss: 8014.769531\n",
      "Train Epoch: 126 [61696/225000 (27%)] Loss: 8136.294922\n",
      "Train Epoch: 126 [65792/225000 (29%)] Loss: 9630.009766\n",
      "Train Epoch: 126 [69888/225000 (31%)] Loss: 10464.759766\n",
      "Train Epoch: 126 [73984/225000 (33%)] Loss: 8208.726562\n",
      "Train Epoch: 126 [78080/225000 (35%)] Loss: 8019.718750\n",
      "Train Epoch: 126 [82176/225000 (37%)] Loss: 12338.431641\n",
      "Train Epoch: 126 [86272/225000 (38%)] Loss: 7849.380859\n",
      "Train Epoch: 126 [90368/225000 (40%)] Loss: 10632.525391\n",
      "Train Epoch: 126 [94464/225000 (42%)] Loss: 8042.378906\n",
      "Train Epoch: 126 [98560/225000 (44%)] Loss: 9413.574219\n",
      "Train Epoch: 126 [102656/225000 (46%)] Loss: 9796.333984\n",
      "Train Epoch: 126 [106752/225000 (47%)] Loss: 8108.498047\n",
      "Train Epoch: 126 [110848/225000 (49%)] Loss: 8040.894531\n",
      "Train Epoch: 126 [114944/225000 (51%)] Loss: 12789.277344\n",
      "Train Epoch: 126 [119040/225000 (53%)] Loss: 11929.652344\n",
      "Train Epoch: 126 [123136/225000 (55%)] Loss: 9664.488281\n",
      "Train Epoch: 126 [127232/225000 (57%)] Loss: 16149.412109\n",
      "Train Epoch: 126 [131328/225000 (58%)] Loss: 9622.281250\n",
      "Train Epoch: 126 [135424/225000 (60%)] Loss: 10754.558594\n",
      "Train Epoch: 126 [139520/225000 (62%)] Loss: 7952.460938\n",
      "Train Epoch: 126 [143616/225000 (64%)] Loss: 7998.947266\n",
      "Train Epoch: 126 [147712/225000 (66%)] Loss: 12254.513672\n",
      "Train Epoch: 126 [151808/225000 (67%)] Loss: 8024.136719\n",
      "Train Epoch: 126 [155904/225000 (69%)] Loss: 10543.361328\n",
      "Train Epoch: 126 [160000/225000 (71%)] Loss: 18069.361328\n",
      "Train Epoch: 126 [164096/225000 (73%)] Loss: 9715.373047\n",
      "Train Epoch: 126 [168192/225000 (75%)] Loss: 8160.343750\n",
      "Train Epoch: 126 [172288/225000 (77%)] Loss: 10623.205078\n",
      "Train Epoch: 126 [176384/225000 (78%)] Loss: 7971.207031\n",
      "Train Epoch: 126 [180480/225000 (80%)] Loss: 7893.191406\n",
      "Train Epoch: 126 [184576/225000 (82%)] Loss: 10593.427734\n",
      "Train Epoch: 126 [188672/225000 (84%)] Loss: 8040.654297\n",
      "Train Epoch: 126 [192768/225000 (86%)] Loss: 7943.617188\n",
      "Train Epoch: 126 [196864/225000 (87%)] Loss: 8125.101562\n",
      "Train Epoch: 126 [200960/225000 (89%)] Loss: 8146.458984\n",
      "Train Epoch: 126 [205056/225000 (91%)] Loss: 9633.082031\n",
      "Train Epoch: 126 [209152/225000 (93%)] Loss: 7924.068359\n",
      "Train Epoch: 126 [213248/225000 (95%)] Loss: 8033.589844\n",
      "Train Epoch: 126 [217344/225000 (97%)] Loss: 8025.425781\n",
      "Train Epoch: 126 [221440/225000 (98%)] Loss: 7987.132812\n",
      "    epoch          : 126\n",
      "    loss           : 9452.621807007252\n",
      "    val_loss       : 9480.94918967753\n",
      "Train Epoch: 127 [256/225000 (0%)] Loss: 10273.638672\n",
      "Train Epoch: 127 [4352/225000 (2%)] Loss: 8055.375000\n",
      "Train Epoch: 127 [8448/225000 (4%)] Loss: 9578.761719\n",
      "Train Epoch: 127 [12544/225000 (6%)] Loss: 11446.667969\n",
      "Train Epoch: 127 [16640/225000 (7%)] Loss: 7877.566406\n",
      "Train Epoch: 127 [20736/225000 (9%)] Loss: 12365.566406\n",
      "Train Epoch: 127 [24832/225000 (11%)] Loss: 8152.433594\n",
      "Train Epoch: 127 [28928/225000 (13%)] Loss: 8189.787109\n",
      "Train Epoch: 127 [33024/225000 (15%)] Loss: 7994.560547\n",
      "Train Epoch: 127 [37120/225000 (16%)] Loss: 8194.748047\n",
      "Train Epoch: 127 [41216/225000 (18%)] Loss: 8131.136719\n",
      "Train Epoch: 127 [45312/225000 (20%)] Loss: 7951.064453\n",
      "Train Epoch: 127 [49408/225000 (22%)] Loss: 8303.220703\n",
      "Train Epoch: 127 [53504/225000 (24%)] Loss: 8094.140625\n",
      "Train Epoch: 127 [57600/225000 (26%)] Loss: 8147.761719\n",
      "Train Epoch: 127 [61696/225000 (27%)] Loss: 13632.060547\n",
      "Train Epoch: 127 [65792/225000 (29%)] Loss: 9526.156250\n",
      "Train Epoch: 127 [69888/225000 (31%)] Loss: 8272.667969\n",
      "Train Epoch: 127 [73984/225000 (33%)] Loss: 12207.365234\n",
      "Train Epoch: 127 [78080/225000 (35%)] Loss: 8154.980469\n",
      "Train Epoch: 127 [82176/225000 (37%)] Loss: 8064.164062\n",
      "Train Epoch: 127 [86272/225000 (38%)] Loss: 8099.025391\n",
      "Train Epoch: 127 [90368/225000 (40%)] Loss: 10547.378906\n",
      "Train Epoch: 127 [94464/225000 (42%)] Loss: 8093.935547\n",
      "Train Epoch: 127 [98560/225000 (44%)] Loss: 8029.324219\n",
      "Train Epoch: 127 [102656/225000 (46%)] Loss: 12048.876953\n",
      "Train Epoch: 127 [106752/225000 (47%)] Loss: 8135.400391\n",
      "Train Epoch: 127 [110848/225000 (49%)] Loss: 7986.507812\n",
      "Train Epoch: 127 [114944/225000 (51%)] Loss: 8217.548828\n",
      "Train Epoch: 127 [119040/225000 (53%)] Loss: 8185.121094\n",
      "Train Epoch: 127 [123136/225000 (55%)] Loss: 8141.660156\n",
      "Train Epoch: 127 [127232/225000 (57%)] Loss: 7995.748047\n",
      "Train Epoch: 127 [131328/225000 (58%)] Loss: 12307.826172\n",
      "Train Epoch: 127 [135424/225000 (60%)] Loss: 11282.410156\n",
      "Train Epoch: 127 [139520/225000 (62%)] Loss: 9647.734375\n",
      "Train Epoch: 127 [143616/225000 (64%)] Loss: 7852.742188\n",
      "Train Epoch: 127 [147712/225000 (66%)] Loss: 8228.183594\n",
      "Train Epoch: 127 [151808/225000 (67%)] Loss: 8015.232422\n",
      "Train Epoch: 127 [155904/225000 (69%)] Loss: 7872.730469\n",
      "Train Epoch: 127 [160000/225000 (71%)] Loss: 10512.093750\n",
      "Train Epoch: 127 [164096/225000 (73%)] Loss: 8236.082031\n",
      "Train Epoch: 127 [168192/225000 (75%)] Loss: 16386.339844\n",
      "Train Epoch: 127 [172288/225000 (77%)] Loss: 9678.250000\n",
      "Train Epoch: 127 [176384/225000 (78%)] Loss: 8066.005859\n",
      "Train Epoch: 127 [180480/225000 (80%)] Loss: 7826.738281\n",
      "Train Epoch: 127 [184576/225000 (82%)] Loss: 10666.078125\n",
      "Train Epoch: 127 [188672/225000 (84%)] Loss: 10464.134766\n",
      "Train Epoch: 127 [192768/225000 (86%)] Loss: 9673.742188\n",
      "Train Epoch: 127 [196864/225000 (87%)] Loss: 9574.203125\n",
      "Train Epoch: 127 [200960/225000 (89%)] Loss: 8103.287109\n",
      "Train Epoch: 127 [205056/225000 (91%)] Loss: 9409.914062\n",
      "Train Epoch: 127 [209152/225000 (93%)] Loss: 7979.136719\n",
      "Train Epoch: 127 [213248/225000 (95%)] Loss: 8067.839844\n",
      "Train Epoch: 127 [217344/225000 (97%)] Loss: 8282.251953\n",
      "Train Epoch: 127 [221440/225000 (98%)] Loss: 8058.437500\n",
      "    epoch          : 127\n",
      "    loss           : 9339.82185233575\n",
      "    val_loss       : 9540.593389423526\n",
      "Train Epoch: 128 [256/225000 (0%)] Loss: 10467.824219\n",
      "Train Epoch: 128 [4352/225000 (2%)] Loss: 8046.222656\n",
      "Train Epoch: 128 [8448/225000 (4%)] Loss: 9596.833984\n",
      "Train Epoch: 128 [12544/225000 (6%)] Loss: 7940.103516\n",
      "Train Epoch: 128 [16640/225000 (7%)] Loss: 9661.431641\n",
      "Train Epoch: 128 [20736/225000 (9%)] Loss: 8035.906250\n",
      "Train Epoch: 128 [24832/225000 (11%)] Loss: 8055.376953\n",
      "Train Epoch: 128 [28928/225000 (13%)] Loss: 8145.281250\n",
      "Train Epoch: 128 [33024/225000 (15%)] Loss: 9582.425781\n",
      "Train Epoch: 128 [37120/225000 (16%)] Loss: 12811.302734\n",
      "Train Epoch: 128 [41216/225000 (18%)] Loss: 9572.482422\n",
      "Train Epoch: 128 [45312/225000 (20%)] Loss: 7948.462891\n",
      "Train Epoch: 128 [49408/225000 (22%)] Loss: 17017.873047\n",
      "Train Epoch: 128 [53504/225000 (24%)] Loss: 10541.181641\n",
      "Train Epoch: 128 [57600/225000 (26%)] Loss: 8108.566406\n",
      "Train Epoch: 128 [61696/225000 (27%)] Loss: 12167.062500\n",
      "Train Epoch: 128 [65792/225000 (29%)] Loss: 9715.076172\n",
      "Train Epoch: 128 [69888/225000 (31%)] Loss: 13862.886719\n",
      "Train Epoch: 128 [73984/225000 (33%)] Loss: 8191.310547\n",
      "Train Epoch: 128 [78080/225000 (35%)] Loss: 8229.902344\n",
      "Train Epoch: 128 [82176/225000 (37%)] Loss: 9748.630859\n",
      "Train Epoch: 128 [86272/225000 (38%)] Loss: 7878.832031\n",
      "Train Epoch: 128 [90368/225000 (40%)] Loss: 13023.611328\n",
      "Train Epoch: 128 [94464/225000 (42%)] Loss: 13676.042969\n",
      "Train Epoch: 128 [98560/225000 (44%)] Loss: 8039.023438\n",
      "Train Epoch: 128 [102656/225000 (46%)] Loss: 8063.355469\n",
      "Train Epoch: 128 [106752/225000 (47%)] Loss: 7937.158203\n",
      "Train Epoch: 128 [110848/225000 (49%)] Loss: 8193.642578\n",
      "Train Epoch: 128 [114944/225000 (51%)] Loss: 8056.406250\n",
      "Train Epoch: 128 [119040/225000 (53%)] Loss: 8243.193359\n",
      "Train Epoch: 128 [123136/225000 (55%)] Loss: 8205.539062\n",
      "Train Epoch: 128 [127232/225000 (57%)] Loss: 9731.783203\n",
      "Train Epoch: 128 [131328/225000 (58%)] Loss: 8096.525391\n",
      "Train Epoch: 128 [135424/225000 (60%)] Loss: 9721.835938\n",
      "Train Epoch: 128 [139520/225000 (62%)] Loss: 9674.466797\n",
      "Train Epoch: 128 [143616/225000 (64%)] Loss: 10525.519531\n",
      "Train Epoch: 128 [147712/225000 (66%)] Loss: 9693.955078\n",
      "Train Epoch: 128 [151808/225000 (67%)] Loss: 7922.845703\n",
      "Train Epoch: 128 [155904/225000 (69%)] Loss: 8024.179688\n",
      "Train Epoch: 128 [160000/225000 (71%)] Loss: 14942.935547\n",
      "Train Epoch: 128 [164096/225000 (73%)] Loss: 12416.166016\n",
      "Train Epoch: 128 [168192/225000 (75%)] Loss: 8155.515625\n",
      "Train Epoch: 128 [172288/225000 (77%)] Loss: 9672.605469\n",
      "Train Epoch: 128 [176384/225000 (78%)] Loss: 7920.484375\n",
      "Train Epoch: 128 [180480/225000 (80%)] Loss: 8177.080078\n",
      "Train Epoch: 128 [184576/225000 (82%)] Loss: 13237.205078\n",
      "Train Epoch: 128 [188672/225000 (84%)] Loss: 13540.494141\n",
      "Train Epoch: 128 [192768/225000 (86%)] Loss: 8265.076172\n",
      "Train Epoch: 128 [196864/225000 (87%)] Loss: 10609.117188\n",
      "Train Epoch: 128 [200960/225000 (89%)] Loss: 7931.982422\n",
      "Train Epoch: 128 [205056/225000 (91%)] Loss: 9741.347656\n",
      "Train Epoch: 128 [209152/225000 (93%)] Loss: 7908.492188\n",
      "Train Epoch: 128 [213248/225000 (95%)] Loss: 8075.744141\n",
      "Train Epoch: 128 [217344/225000 (97%)] Loss: 7890.775391\n",
      "Train Epoch: 128 [221440/225000 (98%)] Loss: 7965.322266\n",
      "    epoch          : 128\n",
      "    loss           : 9555.00759918942\n",
      "    val_loss       : 9261.571596288486\n",
      "Train Epoch: 129 [256/225000 (0%)] Loss: 8004.644531\n",
      "Train Epoch: 129 [4352/225000 (2%)] Loss: 7935.091797\n",
      "Train Epoch: 129 [8448/225000 (4%)] Loss: 13809.011719\n",
      "Train Epoch: 129 [12544/225000 (6%)] Loss: 12822.257812\n",
      "Train Epoch: 129 [16640/225000 (7%)] Loss: 12152.466797\n",
      "Train Epoch: 129 [20736/225000 (9%)] Loss: 10693.681641\n",
      "Train Epoch: 129 [24832/225000 (11%)] Loss: 8154.662109\n",
      "Train Epoch: 129 [28928/225000 (13%)] Loss: 8114.029297\n",
      "Train Epoch: 129 [33024/225000 (15%)] Loss: 9707.839844\n",
      "Train Epoch: 129 [37120/225000 (16%)] Loss: 13717.347656\n",
      "Train Epoch: 129 [41216/225000 (18%)] Loss: 8230.369141\n",
      "Train Epoch: 129 [45312/225000 (20%)] Loss: 8084.060547\n",
      "Train Epoch: 129 [49408/225000 (22%)] Loss: 10405.656250\n",
      "Train Epoch: 129 [53504/225000 (24%)] Loss: 9683.818359\n",
      "Train Epoch: 129 [57600/225000 (26%)] Loss: 8042.427734\n",
      "Train Epoch: 129 [61696/225000 (27%)] Loss: 8088.134766\n",
      "Train Epoch: 129 [65792/225000 (29%)] Loss: 8085.833984\n",
      "Train Epoch: 129 [69888/225000 (31%)] Loss: 13677.025391\n",
      "Train Epoch: 129 [73984/225000 (33%)] Loss: 10201.158203\n",
      "Train Epoch: 129 [78080/225000 (35%)] Loss: 8098.585938\n",
      "Train Epoch: 129 [82176/225000 (37%)] Loss: 8079.447266\n",
      "Train Epoch: 129 [86272/225000 (38%)] Loss: 9675.468750\n",
      "Train Epoch: 129 [90368/225000 (40%)] Loss: 8051.958984\n",
      "Train Epoch: 129 [94464/225000 (42%)] Loss: 7990.544922\n",
      "Train Epoch: 129 [98560/225000 (44%)] Loss: 17981.816406\n",
      "Train Epoch: 129 [102656/225000 (46%)] Loss: 8226.244141\n",
      "Train Epoch: 129 [106752/225000 (47%)] Loss: 10622.048828\n",
      "Train Epoch: 129 [110848/225000 (49%)] Loss: 8120.958984\n",
      "Train Epoch: 129 [114944/225000 (51%)] Loss: 8338.187500\n",
      "Train Epoch: 129 [119040/225000 (53%)] Loss: 10548.884766\n",
      "Train Epoch: 129 [123136/225000 (55%)] Loss: 8280.515625\n",
      "Train Epoch: 129 [127232/225000 (57%)] Loss: 10625.097656\n",
      "Train Epoch: 129 [131328/225000 (58%)] Loss: 8130.507812\n",
      "Train Epoch: 129 [135424/225000 (60%)] Loss: 9448.183594\n",
      "Train Epoch: 129 [139520/225000 (62%)] Loss: 8082.070312\n",
      "Train Epoch: 129 [143616/225000 (64%)] Loss: 8034.412109\n",
      "Train Epoch: 129 [147712/225000 (66%)] Loss: 8185.062500\n",
      "Train Epoch: 129 [151808/225000 (67%)] Loss: 8013.558594\n",
      "Train Epoch: 129 [155904/225000 (69%)] Loss: 8160.835938\n",
      "Train Epoch: 129 [160000/225000 (71%)] Loss: 9542.919922\n",
      "Train Epoch: 129 [164096/225000 (73%)] Loss: 10611.888672\n",
      "Train Epoch: 129 [168192/225000 (75%)] Loss: 12018.525391\n",
      "Train Epoch: 129 [172288/225000 (77%)] Loss: 8001.650391\n",
      "Train Epoch: 129 [176384/225000 (78%)] Loss: 8294.263672\n",
      "Train Epoch: 129 [180480/225000 (80%)] Loss: 8165.851562\n",
      "Train Epoch: 129 [184576/225000 (82%)] Loss: 7996.074219\n",
      "Train Epoch: 129 [188672/225000 (84%)] Loss: 20569.609375\n",
      "Train Epoch: 129 [192768/225000 (86%)] Loss: 8158.464844\n",
      "Train Epoch: 129 [196864/225000 (87%)] Loss: 8091.843750\n",
      "Train Epoch: 129 [200960/225000 (89%)] Loss: 8058.519531\n",
      "Train Epoch: 129 [205056/225000 (91%)] Loss: 8239.683594\n",
      "Train Epoch: 129 [209152/225000 (93%)] Loss: 12391.816406\n",
      "Train Epoch: 129 [213248/225000 (95%)] Loss: 7942.669922\n",
      "Train Epoch: 129 [217344/225000 (97%)] Loss: 8018.351562\n",
      "Train Epoch: 129 [221440/225000 (98%)] Loss: 8183.505859\n",
      "    epoch          : 129\n",
      "    loss           : 9664.529034680745\n",
      "    val_loss       : 10167.725711172941\n",
      "Train Epoch: 130 [256/225000 (0%)] Loss: 15546.537109\n",
      "Train Epoch: 130 [4352/225000 (2%)] Loss: 8094.505859\n",
      "Train Epoch: 130 [8448/225000 (4%)] Loss: 13791.894531\n",
      "Train Epoch: 130 [12544/225000 (6%)] Loss: 8254.205078\n",
      "Train Epoch: 130 [16640/225000 (7%)] Loss: 8088.179688\n",
      "Train Epoch: 130 [20736/225000 (9%)] Loss: 8020.730469\n",
      "Train Epoch: 130 [24832/225000 (11%)] Loss: 8113.710938\n",
      "Train Epoch: 130 [28928/225000 (13%)] Loss: 8213.646484\n",
      "Train Epoch: 130 [33024/225000 (15%)] Loss: 9679.843750\n",
      "Train Epoch: 130 [37120/225000 (16%)] Loss: 8186.910156\n",
      "Train Epoch: 130 [41216/225000 (18%)] Loss: 8008.908203\n",
      "Train Epoch: 130 [45312/225000 (20%)] Loss: 19309.212891\n",
      "Train Epoch: 130 [49408/225000 (22%)] Loss: 10440.992188\n",
      "Train Epoch: 130 [53504/225000 (24%)] Loss: 8123.085938\n",
      "Train Epoch: 130 [57600/225000 (26%)] Loss: 9756.351562\n",
      "Train Epoch: 130 [61696/225000 (27%)] Loss: 12308.785156\n",
      "Train Epoch: 130 [65792/225000 (29%)] Loss: 9650.285156\n",
      "Train Epoch: 130 [69888/225000 (31%)] Loss: 8158.667969\n",
      "Train Epoch: 130 [73984/225000 (33%)] Loss: 7861.619141\n",
      "Train Epoch: 130 [78080/225000 (35%)] Loss: 8223.699219\n",
      "Train Epoch: 130 [82176/225000 (37%)] Loss: 9721.845703\n",
      "Train Epoch: 130 [86272/225000 (38%)] Loss: 8002.980469\n",
      "Train Epoch: 130 [90368/225000 (40%)] Loss: 8008.675781\n",
      "Train Epoch: 130 [94464/225000 (42%)] Loss: 15257.507812\n",
      "Train Epoch: 130 [98560/225000 (44%)] Loss: 8091.160156\n",
      "Train Epoch: 130 [102656/225000 (46%)] Loss: 8032.419922\n",
      "Train Epoch: 130 [106752/225000 (47%)] Loss: 7924.552734\n",
      "Train Epoch: 130 [110848/225000 (49%)] Loss: 10587.382812\n",
      "Train Epoch: 130 [114944/225000 (51%)] Loss: 8333.396484\n",
      "Train Epoch: 130 [119040/225000 (53%)] Loss: 8035.814453\n",
      "Train Epoch: 130 [123136/225000 (55%)] Loss: 12096.750000\n",
      "Train Epoch: 130 [127232/225000 (57%)] Loss: 9582.136719\n",
      "Train Epoch: 130 [131328/225000 (58%)] Loss: 7951.544922\n",
      "Train Epoch: 130 [135424/225000 (60%)] Loss: 8068.718750\n",
      "Train Epoch: 130 [139520/225000 (62%)] Loss: 8071.042969\n",
      "Train Epoch: 130 [143616/225000 (64%)] Loss: 8037.421875\n",
      "Train Epoch: 130 [147712/225000 (66%)] Loss: 7741.177734\n",
      "Train Epoch: 130 [151808/225000 (67%)] Loss: 10515.679688\n",
      "Train Epoch: 130 [155904/225000 (69%)] Loss: 8078.312500\n",
      "Train Epoch: 130 [160000/225000 (71%)] Loss: 8181.566406\n",
      "Train Epoch: 130 [164096/225000 (73%)] Loss: 9740.667969\n",
      "Train Epoch: 130 [168192/225000 (75%)] Loss: 8226.630859\n",
      "Train Epoch: 130 [172288/225000 (77%)] Loss: 12367.132812\n",
      "Train Epoch: 130 [176384/225000 (78%)] Loss: 8120.802734\n",
      "Train Epoch: 130 [180480/225000 (80%)] Loss: 7981.779297\n",
      "Train Epoch: 130 [184576/225000 (82%)] Loss: 7998.146484\n",
      "Train Epoch: 130 [188672/225000 (84%)] Loss: 7922.906250\n",
      "Train Epoch: 130 [192768/225000 (86%)] Loss: 8133.466797\n",
      "Train Epoch: 130 [196864/225000 (87%)] Loss: 7962.875000\n",
      "Train Epoch: 130 [200960/225000 (89%)] Loss: 15153.330078\n",
      "Train Epoch: 130 [205056/225000 (91%)] Loss: 7987.128906\n",
      "Train Epoch: 130 [209152/225000 (93%)] Loss: 8097.394531\n",
      "Train Epoch: 130 [213248/225000 (95%)] Loss: 8192.652344\n",
      "Train Epoch: 130 [217344/225000 (97%)] Loss: 7900.328125\n",
      "Train Epoch: 130 [221440/225000 (98%)] Loss: 7905.615234\n",
      "    epoch          : 130\n",
      "    loss           : 9392.200783027589\n",
      "    val_loss       : 9491.86795840458\n",
      "Train Epoch: 131 [256/225000 (0%)] Loss: 8165.832031\n",
      "Train Epoch: 131 [4352/225000 (2%)] Loss: 8059.982422\n",
      "Train Epoch: 131 [8448/225000 (4%)] Loss: 8042.693359\n",
      "Train Epoch: 131 [12544/225000 (6%)] Loss: 9795.904297\n",
      "Train Epoch: 131 [16640/225000 (7%)] Loss: 8197.617188\n",
      "Train Epoch: 131 [20736/225000 (9%)] Loss: 7876.968750\n",
      "Train Epoch: 131 [24832/225000 (11%)] Loss: 7963.425781\n",
      "Train Epoch: 131 [28928/225000 (13%)] Loss: 7937.015625\n",
      "Train Epoch: 131 [33024/225000 (15%)] Loss: 9590.195312\n",
      "Train Epoch: 131 [37120/225000 (16%)] Loss: 8238.617188\n",
      "Train Epoch: 131 [41216/225000 (18%)] Loss: 12649.441406\n",
      "Train Epoch: 131 [45312/225000 (20%)] Loss: 9628.863281\n",
      "Train Epoch: 131 [49408/225000 (22%)] Loss: 7983.382812\n",
      "Train Epoch: 131 [53504/225000 (24%)] Loss: 8153.525391\n",
      "Train Epoch: 131 [57600/225000 (26%)] Loss: 12800.203125\n",
      "Train Epoch: 131 [61696/225000 (27%)] Loss: 9522.259766\n",
      "Train Epoch: 131 [65792/225000 (29%)] Loss: 13819.691406\n",
      "Train Epoch: 131 [69888/225000 (31%)] Loss: 8126.207031\n",
      "Train Epoch: 131 [73984/225000 (33%)] Loss: 19631.751953\n",
      "Train Epoch: 131 [78080/225000 (35%)] Loss: 8028.845703\n",
      "Train Epoch: 131 [82176/225000 (37%)] Loss: 8223.472656\n",
      "Train Epoch: 131 [86272/225000 (38%)] Loss: 8035.207031\n",
      "Train Epoch: 131 [90368/225000 (40%)] Loss: 8078.349609\n",
      "Train Epoch: 131 [94464/225000 (42%)] Loss: 11267.730469\n",
      "Train Epoch: 131 [98560/225000 (44%)] Loss: 8185.792969\n",
      "Train Epoch: 131 [102656/225000 (46%)] Loss: 8136.878906\n",
      "Train Epoch: 131 [106752/225000 (47%)] Loss: 8075.240234\n",
      "Train Epoch: 131 [110848/225000 (49%)] Loss: 8096.775391\n",
      "Train Epoch: 131 [114944/225000 (51%)] Loss: 8002.832031\n",
      "Train Epoch: 131 [119040/225000 (53%)] Loss: 10473.705078\n",
      "Train Epoch: 131 [123136/225000 (55%)] Loss: 8287.250000\n",
      "Train Epoch: 131 [127232/225000 (57%)] Loss: 8091.845703\n",
      "Train Epoch: 131 [131328/225000 (58%)] Loss: 13174.472656\n",
      "Train Epoch: 131 [135424/225000 (60%)] Loss: 11095.113281\n",
      "Train Epoch: 131 [139520/225000 (62%)] Loss: 12419.849609\n",
      "Train Epoch: 131 [143616/225000 (64%)] Loss: 8177.687500\n",
      "Train Epoch: 131 [147712/225000 (66%)] Loss: 10565.642578\n",
      "Train Epoch: 131 [151808/225000 (67%)] Loss: 8129.439453\n",
      "Train Epoch: 131 [155904/225000 (69%)] Loss: 13951.451172\n",
      "Train Epoch: 131 [160000/225000 (71%)] Loss: 7969.357422\n",
      "Train Epoch: 131 [164096/225000 (73%)] Loss: 9655.451172\n",
      "Train Epoch: 131 [168192/225000 (75%)] Loss: 8186.003906\n",
      "Train Epoch: 131 [172288/225000 (77%)] Loss: 9807.093750\n",
      "Train Epoch: 131 [176384/225000 (78%)] Loss: 18437.451172\n",
      "Train Epoch: 131 [180480/225000 (80%)] Loss: 8230.072266\n",
      "Train Epoch: 131 [184576/225000 (82%)] Loss: 12897.541016\n",
      "Train Epoch: 131 [188672/225000 (84%)] Loss: 11330.220703\n",
      "Train Epoch: 131 [192768/225000 (86%)] Loss: 13680.845703\n",
      "Train Epoch: 131 [196864/225000 (87%)] Loss: 8224.513672\n",
      "Train Epoch: 131 [200960/225000 (89%)] Loss: 7976.800781\n",
      "Train Epoch: 131 [205056/225000 (91%)] Loss: 8064.115234\n",
      "Train Epoch: 131 [209152/225000 (93%)] Loss: 7902.857422\n",
      "Train Epoch: 131 [213248/225000 (95%)] Loss: 8043.748047\n",
      "Train Epoch: 131 [217344/225000 (97%)] Loss: 7908.435547\n",
      "Train Epoch: 131 [221440/225000 (98%)] Loss: 7948.451172\n",
      "    epoch          : 131\n",
      "    loss           : 9506.174004772824\n",
      "    val_loss       : 9674.371023791176\n",
      "Train Epoch: 132 [256/225000 (0%)] Loss: 7959.583984\n",
      "Train Epoch: 132 [4352/225000 (2%)] Loss: 8116.480469\n",
      "Train Epoch: 132 [8448/225000 (4%)] Loss: 8080.195312\n",
      "Train Epoch: 132 [12544/225000 (6%)] Loss: 8047.783203\n",
      "Train Epoch: 132 [16640/225000 (7%)] Loss: 16403.232422\n",
      "Train Epoch: 132 [20736/225000 (9%)] Loss: 9421.335938\n",
      "Train Epoch: 132 [24832/225000 (11%)] Loss: 8020.601562\n",
      "Train Epoch: 132 [28928/225000 (13%)] Loss: 8008.152344\n",
      "Train Epoch: 132 [33024/225000 (15%)] Loss: 8108.117188\n",
      "Train Epoch: 132 [37120/225000 (16%)] Loss: 8139.000000\n",
      "Train Epoch: 132 [41216/225000 (18%)] Loss: 8028.253906\n",
      "Train Epoch: 132 [45312/225000 (20%)] Loss: 8690.162109\n",
      "Train Epoch: 132 [49408/225000 (22%)] Loss: 8175.537109\n",
      "Train Epoch: 132 [53504/225000 (24%)] Loss: 8156.923828\n",
      "Train Epoch: 132 [57600/225000 (26%)] Loss: 8062.042969\n",
      "Train Epoch: 132 [61696/225000 (27%)] Loss: 12148.464844\n",
      "Train Epoch: 132 [65792/225000 (29%)] Loss: 7831.015625\n",
      "Train Epoch: 132 [69888/225000 (31%)] Loss: 10583.861328\n",
      "Train Epoch: 132 [73984/225000 (33%)] Loss: 9594.158203\n",
      "Train Epoch: 132 [78080/225000 (35%)] Loss: 8010.742188\n",
      "Train Epoch: 132 [82176/225000 (37%)] Loss: 12269.269531\n",
      "Train Epoch: 132 [86272/225000 (38%)] Loss: 9652.687500\n",
      "Train Epoch: 132 [90368/225000 (40%)] Loss: 8187.656250\n",
      "Train Epoch: 132 [94464/225000 (42%)] Loss: 8019.683594\n",
      "Train Epoch: 132 [98560/225000 (44%)] Loss: 8132.308594\n",
      "Train Epoch: 132 [102656/225000 (46%)] Loss: 8128.046875\n",
      "Train Epoch: 132 [106752/225000 (47%)] Loss: 9585.132812\n",
      "Train Epoch: 132 [110848/225000 (49%)] Loss: 8308.988281\n",
      "Train Epoch: 132 [114944/225000 (51%)] Loss: 8070.250000\n",
      "Train Epoch: 132 [119040/225000 (53%)] Loss: 10543.535156\n",
      "Train Epoch: 132 [123136/225000 (55%)] Loss: 13283.394531\n",
      "Train Epoch: 132 [127232/225000 (57%)] Loss: 8096.878906\n",
      "Train Epoch: 132 [131328/225000 (58%)] Loss: 9566.982422\n",
      "Train Epoch: 132 [135424/225000 (60%)] Loss: 7922.251953\n",
      "Train Epoch: 132 [139520/225000 (62%)] Loss: 8043.023438\n",
      "Train Epoch: 132 [143616/225000 (64%)] Loss: 8248.419922\n",
      "Train Epoch: 132 [147712/225000 (66%)] Loss: 12475.394531\n",
      "Train Epoch: 132 [151808/225000 (67%)] Loss: 9466.634766\n",
      "Train Epoch: 132 [155904/225000 (69%)] Loss: 12471.230469\n",
      "Train Epoch: 132 [160000/225000 (71%)] Loss: 8043.646484\n",
      "Train Epoch: 132 [164096/225000 (73%)] Loss: 8238.218750\n",
      "Train Epoch: 132 [168192/225000 (75%)] Loss: 7907.158203\n",
      "Train Epoch: 132 [172288/225000 (77%)] Loss: 7928.060547\n",
      "Train Epoch: 132 [176384/225000 (78%)] Loss: 10305.519531\n",
      "Train Epoch: 132 [180480/225000 (80%)] Loss: 21604.917969\n",
      "Train Epoch: 132 [184576/225000 (82%)] Loss: 7977.974609\n",
      "Train Epoch: 132 [188672/225000 (84%)] Loss: 13921.683594\n",
      "Train Epoch: 132 [192768/225000 (86%)] Loss: 8045.597656\n",
      "Train Epoch: 132 [196864/225000 (87%)] Loss: 8257.271484\n",
      "Train Epoch: 132 [200960/225000 (89%)] Loss: 10651.527344\n",
      "Train Epoch: 132 [205056/225000 (91%)] Loss: 7941.992188\n",
      "Train Epoch: 132 [209152/225000 (93%)] Loss: 12708.701172\n",
      "Train Epoch: 132 [213248/225000 (95%)] Loss: 10479.058594\n",
      "Train Epoch: 132 [217344/225000 (97%)] Loss: 8202.355469\n",
      "Train Epoch: 132 [221440/225000 (98%)] Loss: 7987.089844\n",
      "    epoch          : 132\n",
      "    loss           : 9636.65816868423\n",
      "    val_loss       : 9381.544892384081\n",
      "Train Epoch: 133 [256/225000 (0%)] Loss: 10432.558594\n",
      "Train Epoch: 133 [4352/225000 (2%)] Loss: 10698.017578\n",
      "Train Epoch: 133 [8448/225000 (4%)] Loss: 8289.716797\n",
      "Train Epoch: 133 [12544/225000 (6%)] Loss: 7950.427734\n",
      "Train Epoch: 133 [16640/225000 (7%)] Loss: 8014.871094\n",
      "Train Epoch: 133 [20736/225000 (9%)] Loss: 7943.177734\n",
      "Train Epoch: 133 [24832/225000 (11%)] Loss: 8095.375000\n",
      "Train Epoch: 133 [28928/225000 (13%)] Loss: 8029.470703\n",
      "Train Epoch: 133 [33024/225000 (15%)] Loss: 9796.414062\n",
      "Train Epoch: 133 [37120/225000 (16%)] Loss: 9432.435547\n",
      "Train Epoch: 133 [41216/225000 (18%)] Loss: 10428.648438\n",
      "Train Epoch: 133 [45312/225000 (20%)] Loss: 13345.808594\n",
      "Train Epoch: 133 [49408/225000 (22%)] Loss: 8241.882812\n",
      "Train Epoch: 133 [53504/225000 (24%)] Loss: 8100.910156\n",
      "Train Epoch: 133 [57600/225000 (26%)] Loss: 11263.460938\n",
      "Train Epoch: 133 [61696/225000 (27%)] Loss: 8081.816406\n",
      "Train Epoch: 133 [65792/225000 (29%)] Loss: 12314.355469\n",
      "Train Epoch: 133 [69888/225000 (31%)] Loss: 8143.281250\n",
      "Train Epoch: 133 [73984/225000 (33%)] Loss: 7908.904297\n",
      "Train Epoch: 133 [78080/225000 (35%)] Loss: 7952.050781\n",
      "Train Epoch: 133 [82176/225000 (37%)] Loss: 9778.333984\n",
      "Train Epoch: 133 [86272/225000 (38%)] Loss: 13599.507812\n",
      "Train Epoch: 133 [90368/225000 (40%)] Loss: 9512.763672\n",
      "Train Epoch: 133 [94464/225000 (42%)] Loss: 8025.250000\n",
      "Train Epoch: 133 [98560/225000 (44%)] Loss: 11947.306641\n",
      "Train Epoch: 133 [102656/225000 (46%)] Loss: 11249.494141\n",
      "Train Epoch: 133 [106752/225000 (47%)] Loss: 7960.261719\n",
      "Train Epoch: 133 [110848/225000 (49%)] Loss: 8201.250000\n",
      "Train Epoch: 133 [114944/225000 (51%)] Loss: 8029.683594\n",
      "Train Epoch: 133 [119040/225000 (53%)] Loss: 8000.281250\n",
      "Train Epoch: 133 [123136/225000 (55%)] Loss: 9493.208984\n",
      "Train Epoch: 133 [127232/225000 (57%)] Loss: 15280.265625\n",
      "Train Epoch: 133 [131328/225000 (58%)] Loss: 15204.289062\n",
      "Train Epoch: 133 [135424/225000 (60%)] Loss: 9576.451172\n",
      "Train Epoch: 133 [139520/225000 (62%)] Loss: 15284.255859\n",
      "Train Epoch: 133 [143616/225000 (64%)] Loss: 8240.576172\n",
      "Train Epoch: 133 [147712/225000 (66%)] Loss: 11215.726562\n",
      "Train Epoch: 133 [151808/225000 (67%)] Loss: 7952.542969\n",
      "Train Epoch: 133 [155904/225000 (69%)] Loss: 7928.632812\n",
      "Train Epoch: 133 [160000/225000 (71%)] Loss: 15356.109375\n",
      "Train Epoch: 133 [164096/225000 (73%)] Loss: 9694.363281\n",
      "Train Epoch: 133 [168192/225000 (75%)] Loss: 14928.757812\n",
      "Train Epoch: 133 [172288/225000 (77%)] Loss: 8037.900391\n",
      "Train Epoch: 133 [176384/225000 (78%)] Loss: 8076.078125\n",
      "Train Epoch: 133 [180480/225000 (80%)] Loss: 8092.660156\n",
      "Train Epoch: 133 [184576/225000 (82%)] Loss: 8025.158203\n",
      "Train Epoch: 133 [188672/225000 (84%)] Loss: 12714.943359\n",
      "Train Epoch: 133 [192768/225000 (86%)] Loss: 8210.964844\n",
      "Train Epoch: 133 [196864/225000 (87%)] Loss: 9755.140625\n",
      "Train Epoch: 133 [200960/225000 (89%)] Loss: 9845.441406\n",
      "Train Epoch: 133 [205056/225000 (91%)] Loss: 8029.373047\n",
      "Train Epoch: 133 [209152/225000 (93%)] Loss: 8060.208984\n",
      "Train Epoch: 133 [213248/225000 (95%)] Loss: 8020.863281\n",
      "Train Epoch: 133 [217344/225000 (97%)] Loss: 9650.722656\n",
      "Train Epoch: 133 [221440/225000 (98%)] Loss: 7909.025391\n",
      "    epoch          : 133\n",
      "    loss           : 9632.053292093287\n",
      "    val_loss       : 9561.639317410332\n",
      "Train Epoch: 134 [256/225000 (0%)] Loss: 9575.195312\n",
      "Train Epoch: 134 [4352/225000 (2%)] Loss: 8033.878906\n",
      "Train Epoch: 134 [8448/225000 (4%)] Loss: 8006.972656\n",
      "Train Epoch: 134 [12544/225000 (6%)] Loss: 8216.251953\n",
      "Train Epoch: 134 [16640/225000 (7%)] Loss: 9605.431641\n",
      "Train Epoch: 134 [20736/225000 (9%)] Loss: 13443.517578\n",
      "Train Epoch: 134 [24832/225000 (11%)] Loss: 8019.724609\n",
      "Train Epoch: 134 [28928/225000 (13%)] Loss: 9542.841797\n",
      "Train Epoch: 134 [33024/225000 (15%)] Loss: 11220.148438\n",
      "Train Epoch: 134 [37120/225000 (16%)] Loss: 7830.447266\n",
      "Train Epoch: 134 [41216/225000 (18%)] Loss: 8133.082031\n",
      "Train Epoch: 134 [45312/225000 (20%)] Loss: 10581.201172\n",
      "Train Epoch: 134 [49408/225000 (22%)] Loss: 8013.158203\n",
      "Train Epoch: 134 [53504/225000 (24%)] Loss: 7977.437500\n",
      "Train Epoch: 134 [57600/225000 (26%)] Loss: 8051.837891\n",
      "Train Epoch: 134 [61696/225000 (27%)] Loss: 7949.208984\n",
      "Train Epoch: 134 [65792/225000 (29%)] Loss: 9273.703125\n",
      "Train Epoch: 134 [69888/225000 (31%)] Loss: 8037.466797\n",
      "Train Epoch: 134 [73984/225000 (33%)] Loss: 10474.564453\n",
      "Train Epoch: 134 [78080/225000 (35%)] Loss: 8144.613281\n",
      "Train Epoch: 134 [82176/225000 (37%)] Loss: 13950.300781\n",
      "Train Epoch: 134 [86272/225000 (38%)] Loss: 12816.644531\n",
      "Train Epoch: 134 [90368/225000 (40%)] Loss: 8014.896484\n",
      "Train Epoch: 134 [94464/225000 (42%)] Loss: 8043.669922\n",
      "Train Epoch: 134 [98560/225000 (44%)] Loss: 8144.775391\n",
      "Train Epoch: 134 [102656/225000 (46%)] Loss: 7929.410156\n",
      "Train Epoch: 134 [106752/225000 (47%)] Loss: 8111.427734\n",
      "Train Epoch: 134 [110848/225000 (49%)] Loss: 17396.244141\n",
      "Train Epoch: 134 [114944/225000 (51%)] Loss: 8062.312500\n",
      "Train Epoch: 134 [119040/225000 (53%)] Loss: 15120.937500\n",
      "Train Epoch: 134 [123136/225000 (55%)] Loss: 8243.732422\n",
      "Train Epoch: 134 [127232/225000 (57%)] Loss: 7870.082031\n",
      "Train Epoch: 134 [131328/225000 (58%)] Loss: 9620.441406\n",
      "Train Epoch: 134 [135424/225000 (60%)] Loss: 9590.582031\n",
      "Train Epoch: 134 [139520/225000 (62%)] Loss: 8062.414062\n",
      "Train Epoch: 134 [143616/225000 (64%)] Loss: 17918.199219\n",
      "Train Epoch: 134 [147712/225000 (66%)] Loss: 13496.792969\n",
      "Train Epoch: 134 [151808/225000 (67%)] Loss: 7974.376953\n",
      "Train Epoch: 134 [155904/225000 (69%)] Loss: 13028.525391\n",
      "Train Epoch: 134 [160000/225000 (71%)] Loss: 8325.648438\n",
      "Train Epoch: 134 [164096/225000 (73%)] Loss: 8235.134766\n",
      "Train Epoch: 134 [168192/225000 (75%)] Loss: 15681.884766\n",
      "Train Epoch: 134 [172288/225000 (77%)] Loss: 8311.414062\n",
      "Train Epoch: 134 [176384/225000 (78%)] Loss: 10310.369141\n",
      "Train Epoch: 134 [180480/225000 (80%)] Loss: 9644.416016\n",
      "Train Epoch: 134 [184576/225000 (82%)] Loss: 10487.974609\n",
      "Train Epoch: 134 [188672/225000 (84%)] Loss: 9563.029297\n",
      "Train Epoch: 134 [192768/225000 (86%)] Loss: 9474.582031\n",
      "Train Epoch: 134 [196864/225000 (87%)] Loss: 7932.410156\n",
      "Train Epoch: 134 [200960/225000 (89%)] Loss: 12210.707031\n",
      "Train Epoch: 134 [205056/225000 (91%)] Loss: 8207.912109\n",
      "Train Epoch: 134 [209152/225000 (93%)] Loss: 9540.343750\n",
      "Train Epoch: 134 [213248/225000 (95%)] Loss: 8066.591797\n",
      "Train Epoch: 134 [217344/225000 (97%)] Loss: 9545.966797\n",
      "Train Epoch: 134 [221440/225000 (98%)] Loss: 7931.449219\n",
      "    epoch          : 134\n",
      "    loss           : 9590.597260736633\n",
      "    val_loss       : 9395.225794106114\n",
      "Train Epoch: 135 [256/225000 (0%)] Loss: 7882.259766\n",
      "Train Epoch: 135 [4352/225000 (2%)] Loss: 8079.107422\n",
      "Train Epoch: 135 [8448/225000 (4%)] Loss: 14177.203125\n",
      "Train Epoch: 135 [12544/225000 (6%)] Loss: 7971.832031\n",
      "Train Epoch: 135 [16640/225000 (7%)] Loss: 8152.378906\n",
      "Train Epoch: 135 [20736/225000 (9%)] Loss: 8115.505859\n",
      "Train Epoch: 135 [24832/225000 (11%)] Loss: 8035.259766\n",
      "Train Epoch: 135 [28928/225000 (13%)] Loss: 8165.433594\n",
      "Train Epoch: 135 [33024/225000 (15%)] Loss: 16208.839844\n",
      "Train Epoch: 135 [37120/225000 (16%)] Loss: 8056.039062\n",
      "Train Epoch: 135 [41216/225000 (18%)] Loss: 10513.365234\n",
      "Train Epoch: 135 [45312/225000 (20%)] Loss: 8248.414062\n",
      "Train Epoch: 135 [49408/225000 (22%)] Loss: 9702.078125\n",
      "Train Epoch: 135 [53504/225000 (24%)] Loss: 8059.755859\n",
      "Train Epoch: 135 [57600/225000 (26%)] Loss: 16340.515625\n",
      "Train Epoch: 135 [61696/225000 (27%)] Loss: 7811.130859\n",
      "Train Epoch: 135 [65792/225000 (29%)] Loss: 9852.656250\n",
      "Train Epoch: 135 [69888/225000 (31%)] Loss: 8032.876953\n",
      "Train Epoch: 135 [73984/225000 (33%)] Loss: 7959.937500\n",
      "Train Epoch: 135 [78080/225000 (35%)] Loss: 13547.699219\n",
      "Train Epoch: 135 [82176/225000 (37%)] Loss: 8111.728516\n",
      "Train Epoch: 135 [86272/225000 (38%)] Loss: 8133.902344\n",
      "Train Epoch: 135 [90368/225000 (40%)] Loss: 8015.617188\n",
      "Train Epoch: 135 [94464/225000 (42%)] Loss: 8045.755859\n",
      "Train Epoch: 135 [98560/225000 (44%)] Loss: 8072.527344\n",
      "Train Epoch: 135 [102656/225000 (46%)] Loss: 10663.443359\n",
      "Train Epoch: 135 [106752/225000 (47%)] Loss: 10429.767578\n",
      "Train Epoch: 135 [110848/225000 (49%)] Loss: 9832.001953\n",
      "Train Epoch: 135 [114944/225000 (51%)] Loss: 7764.667969\n",
      "Train Epoch: 135 [119040/225000 (53%)] Loss: 10432.164062\n",
      "Train Epoch: 135 [123136/225000 (55%)] Loss: 8194.185547\n",
      "Train Epoch: 135 [127232/225000 (57%)] Loss: 7931.718750\n",
      "Train Epoch: 135 [131328/225000 (58%)] Loss: 7950.082031\n",
      "Train Epoch: 135 [135424/225000 (60%)] Loss: 12307.923828\n",
      "Train Epoch: 135 [139520/225000 (62%)] Loss: 9660.423828\n",
      "Train Epoch: 135 [143616/225000 (64%)] Loss: 13650.978516\n",
      "Train Epoch: 135 [147712/225000 (66%)] Loss: 8239.949219\n",
      "Train Epoch: 135 [151808/225000 (67%)] Loss: 10483.431641\n",
      "Train Epoch: 135 [155904/225000 (69%)] Loss: 7994.468750\n",
      "Train Epoch: 135 [160000/225000 (71%)] Loss: 9504.535156\n",
      "Train Epoch: 135 [164096/225000 (73%)] Loss: 8065.966797\n",
      "Train Epoch: 135 [168192/225000 (75%)] Loss: 9604.695312\n",
      "Train Epoch: 135 [172288/225000 (77%)] Loss: 9751.824219\n",
      "Train Epoch: 135 [176384/225000 (78%)] Loss: 7916.253906\n",
      "Train Epoch: 135 [180480/225000 (80%)] Loss: 8063.544922\n",
      "Train Epoch: 135 [184576/225000 (82%)] Loss: 8169.527344\n",
      "Train Epoch: 135 [188672/225000 (84%)] Loss: 8136.337891\n",
      "Train Epoch: 135 [192768/225000 (86%)] Loss: 8168.398438\n",
      "Train Epoch: 135 [196864/225000 (87%)] Loss: 7881.511719\n",
      "Train Epoch: 135 [200960/225000 (89%)] Loss: 17730.570312\n",
      "Train Epoch: 135 [205056/225000 (91%)] Loss: 7878.859375\n",
      "Train Epoch: 135 [209152/225000 (93%)] Loss: 7994.925781\n",
      "Train Epoch: 135 [213248/225000 (95%)] Loss: 8100.197266\n",
      "Train Epoch: 135 [217344/225000 (97%)] Loss: 13639.289062\n",
      "Train Epoch: 135 [221440/225000 (98%)] Loss: 9387.437500\n",
      "    epoch          : 135\n",
      "    loss           : 9626.697358948379\n",
      "    val_loss       : 9708.503090289174\n",
      "Train Epoch: 136 [256/225000 (0%)] Loss: 9806.601562\n",
      "Train Epoch: 136 [4352/225000 (2%)] Loss: 14828.347656\n",
      "Train Epoch: 136 [8448/225000 (4%)] Loss: 7998.076172\n",
      "Train Epoch: 136 [12544/225000 (6%)] Loss: 13905.867188\n",
      "Train Epoch: 136 [16640/225000 (7%)] Loss: 7818.933594\n",
      "Train Epoch: 136 [20736/225000 (9%)] Loss: 8019.060547\n",
      "Train Epoch: 136 [24832/225000 (11%)] Loss: 7980.923828\n",
      "Train Epoch: 136 [28928/225000 (13%)] Loss: 8002.484375\n",
      "Train Epoch: 136 [33024/225000 (15%)] Loss: 7939.054688\n",
      "Train Epoch: 136 [37120/225000 (16%)] Loss: 15988.802734\n",
      "Train Epoch: 136 [41216/225000 (18%)] Loss: 15618.076172\n",
      "Train Epoch: 136 [45312/225000 (20%)] Loss: 13031.076172\n",
      "Train Epoch: 136 [49408/225000 (22%)] Loss: 8069.771484\n",
      "Train Epoch: 136 [53504/225000 (24%)] Loss: 13838.337891\n",
      "Train Epoch: 136 [57600/225000 (26%)] Loss: 8008.064453\n",
      "Train Epoch: 136 [61696/225000 (27%)] Loss: 7981.345703\n",
      "Train Epoch: 136 [65792/225000 (29%)] Loss: 12900.939453\n",
      "Train Epoch: 136 [69888/225000 (31%)] Loss: 8067.058594\n",
      "Train Epoch: 136 [73984/225000 (33%)] Loss: 8158.521484\n",
      "Train Epoch: 136 [78080/225000 (35%)] Loss: 9548.449219\n",
      "Train Epoch: 136 [82176/225000 (37%)] Loss: 15469.894531\n",
      "Train Epoch: 136 [86272/225000 (38%)] Loss: 13695.529297\n",
      "Train Epoch: 136 [90368/225000 (40%)] Loss: 8012.707031\n",
      "Train Epoch: 136 [94464/225000 (42%)] Loss: 8137.501953\n",
      "Train Epoch: 136 [98560/225000 (44%)] Loss: 8015.816406\n",
      "Train Epoch: 136 [102656/225000 (46%)] Loss: 8067.269531\n",
      "Train Epoch: 136 [106752/225000 (47%)] Loss: 13719.601562\n",
      "Train Epoch: 136 [110848/225000 (49%)] Loss: 9417.181641\n",
      "Train Epoch: 136 [114944/225000 (51%)] Loss: 10549.923828\n",
      "Train Epoch: 136 [119040/225000 (53%)] Loss: 13709.632812\n",
      "Train Epoch: 136 [123136/225000 (55%)] Loss: 13156.697266\n",
      "Train Epoch: 136 [127232/225000 (57%)] Loss: 8111.527344\n",
      "Train Epoch: 136 [131328/225000 (58%)] Loss: 9727.908203\n",
      "Train Epoch: 136 [135424/225000 (60%)] Loss: 8283.238281\n",
      "Train Epoch: 136 [139520/225000 (62%)] Loss: 9554.757812\n",
      "Train Epoch: 136 [143616/225000 (64%)] Loss: 8106.431641\n",
      "Train Epoch: 136 [147712/225000 (66%)] Loss: 13877.597656\n",
      "Train Epoch: 136 [151808/225000 (67%)] Loss: 7952.769531\n",
      "Train Epoch: 136 [155904/225000 (69%)] Loss: 8235.804688\n",
      "Train Epoch: 136 [160000/225000 (71%)] Loss: 9600.574219\n",
      "Train Epoch: 136 [164096/225000 (73%)] Loss: 8187.023438\n",
      "Train Epoch: 136 [168192/225000 (75%)] Loss: 8093.529297\n",
      "Train Epoch: 136 [172288/225000 (77%)] Loss: 7989.031250\n",
      "Train Epoch: 136 [176384/225000 (78%)] Loss: 11903.123047\n",
      "Train Epoch: 136 [180480/225000 (80%)] Loss: 15198.054688\n",
      "Train Epoch: 136 [184576/225000 (82%)] Loss: 12409.103516\n",
      "Train Epoch: 136 [188672/225000 (84%)] Loss: 8045.259766\n",
      "Train Epoch: 136 [192768/225000 (86%)] Loss: 9749.662109\n",
      "Train Epoch: 136 [196864/225000 (87%)] Loss: 8336.578125\n",
      "Train Epoch: 136 [200960/225000 (89%)] Loss: 9667.324219\n",
      "Train Epoch: 136 [205056/225000 (91%)] Loss: 8046.833984\n",
      "Train Epoch: 136 [209152/225000 (93%)] Loss: 10399.128906\n",
      "Train Epoch: 136 [213248/225000 (95%)] Loss: 8192.595703\n",
      "Train Epoch: 136 [217344/225000 (97%)] Loss: 10582.197266\n",
      "Train Epoch: 136 [221440/225000 (98%)] Loss: 10671.753906\n",
      "    epoch          : 136\n",
      "    loss           : 9498.395971096417\n",
      "    val_loss       : 9249.579419775884\n",
      "Train Epoch: 137 [256/225000 (0%)] Loss: 8065.271484\n",
      "Train Epoch: 137 [4352/225000 (2%)] Loss: 8107.925781\n",
      "Train Epoch: 137 [8448/225000 (4%)] Loss: 8229.373047\n",
      "Train Epoch: 137 [12544/225000 (6%)] Loss: 7999.357422\n",
      "Train Epoch: 137 [16640/225000 (7%)] Loss: 8230.875000\n",
      "Train Epoch: 137 [20736/225000 (9%)] Loss: 8000.494141\n",
      "Train Epoch: 137 [24832/225000 (11%)] Loss: 8001.324219\n",
      "Train Epoch: 137 [28928/225000 (13%)] Loss: 8109.101562\n",
      "Train Epoch: 137 [33024/225000 (15%)] Loss: 8010.548828\n",
      "Train Epoch: 137 [37120/225000 (16%)] Loss: 8038.000000\n",
      "Train Epoch: 137 [41216/225000 (18%)] Loss: 7954.339844\n",
      "Train Epoch: 137 [45312/225000 (20%)] Loss: 9736.363281\n",
      "Train Epoch: 137 [49408/225000 (22%)] Loss: 9684.638672\n",
      "Train Epoch: 137 [53504/225000 (24%)] Loss: 7904.498047\n",
      "Train Epoch: 137 [57600/225000 (26%)] Loss: 12924.316406\n",
      "Train Epoch: 137 [61696/225000 (27%)] Loss: 8161.253906\n",
      "Train Epoch: 137 [65792/225000 (29%)] Loss: 11226.191406\n",
      "Train Epoch: 137 [69888/225000 (31%)] Loss: 8153.580078\n",
      "Train Epoch: 137 [73984/225000 (33%)] Loss: 8058.195312\n",
      "Train Epoch: 137 [78080/225000 (35%)] Loss: 8018.904297\n",
      "Train Epoch: 137 [82176/225000 (37%)] Loss: 8068.509766\n",
      "Train Epoch: 137 [86272/225000 (38%)] Loss: 12171.046875\n",
      "Train Epoch: 137 [90368/225000 (40%)] Loss: 8141.009766\n",
      "Train Epoch: 137 [94464/225000 (42%)] Loss: 13797.816406\n",
      "Train Epoch: 137 [98560/225000 (44%)] Loss: 8244.761719\n",
      "Train Epoch: 137 [102656/225000 (46%)] Loss: 10625.490234\n",
      "Train Epoch: 137 [106752/225000 (47%)] Loss: 8207.375000\n",
      "Train Epoch: 137 [110848/225000 (49%)] Loss: 11272.146484\n",
      "Train Epoch: 137 [114944/225000 (51%)] Loss: 7896.496094\n",
      "Train Epoch: 137 [119040/225000 (53%)] Loss: 8105.099609\n",
      "Train Epoch: 137 [123136/225000 (55%)] Loss: 11637.595703\n",
      "Train Epoch: 137 [127232/225000 (57%)] Loss: 16135.789062\n",
      "Train Epoch: 137 [131328/225000 (58%)] Loss: 12143.669922\n",
      "Train Epoch: 137 [135424/225000 (60%)] Loss: 14812.503906\n",
      "Train Epoch: 137 [139520/225000 (62%)] Loss: 8043.052734\n",
      "Train Epoch: 137 [143616/225000 (64%)] Loss: 8012.449219\n",
      "Train Epoch: 137 [147712/225000 (66%)] Loss: 9535.035156\n",
      "Train Epoch: 137 [151808/225000 (67%)] Loss: 7993.015625\n",
      "Train Epoch: 137 [155904/225000 (69%)] Loss: 13904.605469\n",
      "Train Epoch: 137 [160000/225000 (71%)] Loss: 9788.796875\n",
      "Train Epoch: 137 [164096/225000 (73%)] Loss: 12576.224609\n",
      "Train Epoch: 137 [168192/225000 (75%)] Loss: 8153.802734\n",
      "Train Epoch: 137 [172288/225000 (77%)] Loss: 8071.492188\n",
      "Train Epoch: 137 [176384/225000 (78%)] Loss: 8157.736328\n",
      "Train Epoch: 137 [180480/225000 (80%)] Loss: 10608.939453\n",
      "Train Epoch: 137 [184576/225000 (82%)] Loss: 10673.195312\n",
      "Train Epoch: 137 [188672/225000 (84%)] Loss: 8092.746094\n",
      "Train Epoch: 137 [192768/225000 (86%)] Loss: 10489.648438\n",
      "Train Epoch: 137 [196864/225000 (87%)] Loss: 8010.890625\n",
      "Train Epoch: 137 [200960/225000 (89%)] Loss: 10480.273438\n",
      "Train Epoch: 137 [205056/225000 (91%)] Loss: 8150.597656\n",
      "Train Epoch: 137 [209152/225000 (93%)] Loss: 12878.107422\n",
      "Train Epoch: 137 [213248/225000 (95%)] Loss: 8153.322266\n",
      "Train Epoch: 137 [217344/225000 (97%)] Loss: 8074.103516\n",
      "Train Epoch: 137 [221440/225000 (98%)] Loss: 13027.123047\n",
      "    epoch          : 137\n",
      "    loss           : 9497.134521206626\n",
      "    val_loss       : 9418.684497580236\n",
      "Train Epoch: 138 [256/225000 (0%)] Loss: 8157.732422\n",
      "Train Epoch: 138 [4352/225000 (2%)] Loss: 8074.843750\n",
      "Train Epoch: 138 [8448/225000 (4%)] Loss: 7883.927734\n",
      "Train Epoch: 138 [12544/225000 (6%)] Loss: 13071.433594\n",
      "Train Epoch: 138 [16640/225000 (7%)] Loss: 8090.423828\n",
      "Train Epoch: 138 [20736/225000 (9%)] Loss: 8305.507812\n",
      "Train Epoch: 138 [24832/225000 (11%)] Loss: 11927.808594\n",
      "Train Epoch: 138 [28928/225000 (13%)] Loss: 9615.707031\n",
      "Train Epoch: 138 [33024/225000 (15%)] Loss: 15459.451172\n",
      "Train Epoch: 138 [37120/225000 (16%)] Loss: 8170.203125\n",
      "Train Epoch: 138 [41216/225000 (18%)] Loss: 13551.210938\n",
      "Train Epoch: 138 [45312/225000 (20%)] Loss: 12913.962891\n",
      "Train Epoch: 138 [49408/225000 (22%)] Loss: 8062.382812\n",
      "Train Epoch: 138 [53504/225000 (24%)] Loss: 8128.990234\n",
      "Train Epoch: 138 [57600/225000 (26%)] Loss: 8029.992188\n",
      "Train Epoch: 138 [61696/225000 (27%)] Loss: 10563.441406\n",
      "Train Epoch: 138 [65792/225000 (29%)] Loss: 8312.023438\n",
      "Train Epoch: 138 [69888/225000 (31%)] Loss: 12464.111328\n",
      "Train Epoch: 138 [73984/225000 (33%)] Loss: 12341.621094\n",
      "Train Epoch: 138 [78080/225000 (35%)] Loss: 9642.998047\n",
      "Train Epoch: 138 [82176/225000 (37%)] Loss: 8180.347656\n",
      "Train Epoch: 138 [86272/225000 (38%)] Loss: 12434.326172\n",
      "Train Epoch: 138 [90368/225000 (40%)] Loss: 7820.406250\n",
      "Train Epoch: 138 [94464/225000 (42%)] Loss: 7939.277344\n",
      "Train Epoch: 138 [98560/225000 (44%)] Loss: 11202.517578\n",
      "Train Epoch: 138 [102656/225000 (46%)] Loss: 14960.933594\n",
      "Train Epoch: 138 [106752/225000 (47%)] Loss: 11261.392578\n",
      "Train Epoch: 138 [110848/225000 (49%)] Loss: 8163.132812\n",
      "Train Epoch: 138 [114944/225000 (51%)] Loss: 8280.730469\n",
      "Train Epoch: 138 [119040/225000 (53%)] Loss: 8002.783203\n",
      "Train Epoch: 138 [123136/225000 (55%)] Loss: 7961.970703\n",
      "Train Epoch: 138 [127232/225000 (57%)] Loss: 8193.648438\n",
      "Train Epoch: 138 [131328/225000 (58%)] Loss: 8113.585938\n",
      "Train Epoch: 138 [135424/225000 (60%)] Loss: 8173.826172\n",
      "Train Epoch: 138 [139520/225000 (62%)] Loss: 7817.267578\n",
      "Train Epoch: 138 [143616/225000 (64%)] Loss: 8241.335938\n",
      "Train Epoch: 138 [147712/225000 (66%)] Loss: 8078.443359\n",
      "Train Epoch: 138 [151808/225000 (67%)] Loss: 8209.472656\n",
      "Train Epoch: 138 [155904/225000 (69%)] Loss: 8069.607422\n",
      "Train Epoch: 138 [160000/225000 (71%)] Loss: 8226.943359\n",
      "Train Epoch: 138 [164096/225000 (73%)] Loss: 8044.283203\n",
      "Train Epoch: 138 [168192/225000 (75%)] Loss: 14268.281250\n",
      "Train Epoch: 138 [172288/225000 (77%)] Loss: 10556.765625\n",
      "Train Epoch: 138 [176384/225000 (78%)] Loss: 15448.246094\n",
      "Train Epoch: 138 [180480/225000 (80%)] Loss: 8108.251953\n",
      "Train Epoch: 138 [184576/225000 (82%)] Loss: 8076.824219\n",
      "Train Epoch: 138 [188672/225000 (84%)] Loss: 9789.781250\n",
      "Train Epoch: 138 [192768/225000 (86%)] Loss: 7955.429688\n",
      "Train Epoch: 138 [196864/225000 (87%)] Loss: 7909.253906\n",
      "Train Epoch: 138 [200960/225000 (89%)] Loss: 10496.050781\n",
      "Train Epoch: 138 [205056/225000 (91%)] Loss: 8039.453125\n",
      "Train Epoch: 138 [209152/225000 (93%)] Loss: 8112.861328\n",
      "Train Epoch: 138 [213248/225000 (95%)] Loss: 7905.279297\n",
      "Train Epoch: 138 [217344/225000 (97%)] Loss: 7868.609375\n",
      "Train Epoch: 138 [221440/225000 (98%)] Loss: 9493.083984\n",
      "    epoch          : 138\n",
      "    loss           : 9437.758567974972\n",
      "    val_loss       : 9892.931304790536\n",
      "Train Epoch: 139 [256/225000 (0%)] Loss: 13582.679688\n",
      "Train Epoch: 139 [4352/225000 (2%)] Loss: 8033.326172\n",
      "Train Epoch: 139 [8448/225000 (4%)] Loss: 8044.400391\n",
      "Train Epoch: 139 [12544/225000 (6%)] Loss: 8268.527344\n",
      "Train Epoch: 139 [16640/225000 (7%)] Loss: 8186.416016\n",
      "Train Epoch: 139 [20736/225000 (9%)] Loss: 9419.875000\n",
      "Train Epoch: 139 [24832/225000 (11%)] Loss: 16269.675781\n",
      "Train Epoch: 139 [28928/225000 (13%)] Loss: 10737.894531\n",
      "Train Epoch: 139 [33024/225000 (15%)] Loss: 9518.066406\n",
      "Train Epoch: 139 [37120/225000 (16%)] Loss: 9952.619141\n",
      "Train Epoch: 139 [41216/225000 (18%)] Loss: 10659.455078\n",
      "Train Epoch: 139 [45312/225000 (20%)] Loss: 8129.492188\n",
      "Train Epoch: 139 [49408/225000 (22%)] Loss: 9627.964844\n",
      "Train Epoch: 139 [53504/225000 (24%)] Loss: 9532.935547\n",
      "Train Epoch: 139 [57600/225000 (26%)] Loss: 11987.082031\n",
      "Train Epoch: 139 [61696/225000 (27%)] Loss: 8047.966797\n",
      "Train Epoch: 139 [65792/225000 (29%)] Loss: 8290.945312\n",
      "Train Epoch: 139 [69888/225000 (31%)] Loss: 9679.343750\n",
      "Train Epoch: 139 [73984/225000 (33%)] Loss: 7921.628906\n",
      "Train Epoch: 139 [78080/225000 (35%)] Loss: 9442.599609\n",
      "Train Epoch: 139 [82176/225000 (37%)] Loss: 8022.109375\n",
      "Train Epoch: 139 [86272/225000 (38%)] Loss: 7933.847656\n",
      "Train Epoch: 139 [90368/225000 (40%)] Loss: 9577.724609\n",
      "Train Epoch: 139 [94464/225000 (42%)] Loss: 8050.603516\n",
      "Train Epoch: 139 [98560/225000 (44%)] Loss: 8042.744141\n",
      "Train Epoch: 139 [102656/225000 (46%)] Loss: 7997.197266\n",
      "Train Epoch: 139 [106752/225000 (47%)] Loss: 8070.066406\n",
      "Train Epoch: 139 [110848/225000 (49%)] Loss: 8049.697266\n",
      "Train Epoch: 139 [114944/225000 (51%)] Loss: 7858.529297\n",
      "Train Epoch: 139 [119040/225000 (53%)] Loss: 8124.062500\n",
      "Train Epoch: 139 [123136/225000 (55%)] Loss: 8092.107422\n",
      "Train Epoch: 139 [127232/225000 (57%)] Loss: 8078.681641\n",
      "Train Epoch: 139 [131328/225000 (58%)] Loss: 8065.457031\n",
      "Train Epoch: 139 [135424/225000 (60%)] Loss: 9753.894531\n",
      "Train Epoch: 139 [139520/225000 (62%)] Loss: 7945.373047\n",
      "Train Epoch: 139 [143616/225000 (64%)] Loss: 14003.398438\n",
      "Train Epoch: 139 [147712/225000 (66%)] Loss: 8105.871094\n",
      "Train Epoch: 139 [151808/225000 (67%)] Loss: 8019.091797\n",
      "Train Epoch: 139 [155904/225000 (69%)] Loss: 8027.814453\n",
      "Train Epoch: 139 [160000/225000 (71%)] Loss: 13905.763672\n",
      "Train Epoch: 139 [164096/225000 (73%)] Loss: 8089.115234\n",
      "Train Epoch: 139 [168192/225000 (75%)] Loss: 13614.160156\n",
      "Train Epoch: 139 [172288/225000 (77%)] Loss: 10460.781250\n",
      "Train Epoch: 139 [176384/225000 (78%)] Loss: 7834.843750\n",
      "Train Epoch: 139 [180480/225000 (80%)] Loss: 9619.031250\n",
      "Train Epoch: 139 [184576/225000 (82%)] Loss: 8067.871094\n",
      "Train Epoch: 139 [188672/225000 (84%)] Loss: 7991.095703\n",
      "Train Epoch: 139 [192768/225000 (86%)] Loss: 8129.605469\n",
      "Train Epoch: 139 [196864/225000 (87%)] Loss: 16318.849609\n",
      "Train Epoch: 139 [200960/225000 (89%)] Loss: 8010.718750\n",
      "Train Epoch: 139 [205056/225000 (91%)] Loss: 8058.972656\n",
      "Train Epoch: 139 [209152/225000 (93%)] Loss: 8225.332031\n",
      "Train Epoch: 139 [213248/225000 (95%)] Loss: 10476.912109\n",
      "Train Epoch: 139 [217344/225000 (97%)] Loss: 9703.591797\n",
      "Train Epoch: 139 [221440/225000 (98%)] Loss: 8045.998047\n",
      "    epoch          : 139\n",
      "    loss           : 9355.522278734712\n",
      "    val_loss       : 9227.661685131034\n",
      "Train Epoch: 140 [256/225000 (0%)] Loss: 9672.679688\n",
      "Train Epoch: 140 [4352/225000 (2%)] Loss: 7984.628906\n",
      "Train Epoch: 140 [8448/225000 (4%)] Loss: 7835.964844\n",
      "Train Epoch: 140 [12544/225000 (6%)] Loss: 13377.984375\n",
      "Train Epoch: 140 [16640/225000 (7%)] Loss: 8327.525391\n",
      "Train Epoch: 140 [20736/225000 (9%)] Loss: 13742.164062\n",
      "Train Epoch: 140 [24832/225000 (11%)] Loss: 12111.810547\n",
      "Train Epoch: 140 [28928/225000 (13%)] Loss: 7742.986328\n",
      "Train Epoch: 140 [33024/225000 (15%)] Loss: 9515.191406\n",
      "Train Epoch: 140 [37120/225000 (16%)] Loss: 9648.275391\n",
      "Train Epoch: 140 [41216/225000 (18%)] Loss: 8201.796875\n",
      "Train Epoch: 140 [45312/225000 (20%)] Loss: 11973.705078\n",
      "Train Epoch: 140 [49408/225000 (22%)] Loss: 8021.759766\n",
      "Train Epoch: 140 [53504/225000 (24%)] Loss: 8318.169922\n",
      "Train Epoch: 140 [57600/225000 (26%)] Loss: 8068.904297\n",
      "Train Epoch: 140 [61696/225000 (27%)] Loss: 10417.515625\n",
      "Train Epoch: 140 [65792/225000 (29%)] Loss: 8037.773438\n",
      "Train Epoch: 140 [69888/225000 (31%)] Loss: 8059.089844\n",
      "Train Epoch: 140 [73984/225000 (33%)] Loss: 9525.519531\n",
      "Train Epoch: 140 [78080/225000 (35%)] Loss: 8156.716797\n",
      "Train Epoch: 140 [82176/225000 (37%)] Loss: 7936.593750\n",
      "Train Epoch: 140 [86272/225000 (38%)] Loss: 8219.216797\n",
      "Train Epoch: 140 [90368/225000 (40%)] Loss: 8147.123047\n",
      "Train Epoch: 140 [94464/225000 (42%)] Loss: 8135.060547\n",
      "Train Epoch: 140 [98560/225000 (44%)] Loss: 8110.212891\n",
      "Train Epoch: 140 [102656/225000 (46%)] Loss: 8133.994141\n",
      "Train Epoch: 140 [106752/225000 (47%)] Loss: 10689.679688\n",
      "Train Epoch: 140 [110848/225000 (49%)] Loss: 7971.835938\n",
      "Train Epoch: 140 [114944/225000 (51%)] Loss: 12484.841797\n",
      "Train Epoch: 140 [119040/225000 (53%)] Loss: 7928.619141\n",
      "Train Epoch: 140 [123136/225000 (55%)] Loss: 11158.585938\n",
      "Train Epoch: 140 [127232/225000 (57%)] Loss: 10592.287109\n",
      "Train Epoch: 140 [131328/225000 (58%)] Loss: 9860.550781\n",
      "Train Epoch: 140 [135424/225000 (60%)] Loss: 8088.044922\n",
      "Train Epoch: 140 [139520/225000 (62%)] Loss: 8146.474609\n",
      "Train Epoch: 140 [143616/225000 (64%)] Loss: 8098.289062\n",
      "Train Epoch: 140 [147712/225000 (66%)] Loss: 7980.826172\n",
      "Train Epoch: 140 [151808/225000 (67%)] Loss: 8075.673828\n",
      "Train Epoch: 140 [155904/225000 (69%)] Loss: 10388.724609\n",
      "Train Epoch: 140 [160000/225000 (71%)] Loss: 7962.246094\n",
      "Train Epoch: 140 [164096/225000 (73%)] Loss: 8003.650391\n",
      "Train Epoch: 140 [168192/225000 (75%)] Loss: 8159.292969\n",
      "Train Epoch: 140 [172288/225000 (77%)] Loss: 8076.759766\n",
      "Train Epoch: 140 [176384/225000 (78%)] Loss: 7940.138672\n",
      "Train Epoch: 140 [180480/225000 (80%)] Loss: 10480.408203\n",
      "Train Epoch: 140 [184576/225000 (82%)] Loss: 10435.625000\n",
      "Train Epoch: 140 [188672/225000 (84%)] Loss: 8057.958984\n",
      "Train Epoch: 140 [192768/225000 (86%)] Loss: 7936.117188\n",
      "Train Epoch: 140 [196864/225000 (87%)] Loss: 8076.093750\n",
      "Train Epoch: 140 [200960/225000 (89%)] Loss: 13960.826172\n",
      "Train Epoch: 140 [205056/225000 (91%)] Loss: 8190.685547\n",
      "Train Epoch: 140 [209152/225000 (93%)] Loss: 10402.519531\n",
      "Train Epoch: 140 [213248/225000 (95%)] Loss: 9570.089844\n",
      "Train Epoch: 140 [217344/225000 (97%)] Loss: 9771.621094\n",
      "Train Epoch: 140 [221440/225000 (98%)] Loss: 8089.857422\n",
      "    epoch          : 140\n",
      "    loss           : 9455.094868769553\n",
      "    val_loss       : 9601.113327804876\n",
      "Train Epoch: 141 [256/225000 (0%)] Loss: 9582.443359\n",
      "Train Epoch: 141 [4352/225000 (2%)] Loss: 8157.769531\n",
      "Train Epoch: 141 [8448/225000 (4%)] Loss: 9528.986328\n",
      "Train Epoch: 141 [12544/225000 (6%)] Loss: 8048.140625\n",
      "Train Epoch: 141 [16640/225000 (7%)] Loss: 12111.054688\n",
      "Train Epoch: 141 [20736/225000 (9%)] Loss: 8203.558594\n",
      "Train Epoch: 141 [24832/225000 (11%)] Loss: 8153.183594\n",
      "Train Epoch: 141 [28928/225000 (13%)] Loss: 8016.511719\n",
      "Train Epoch: 141 [33024/225000 (15%)] Loss: 8018.431641\n",
      "Train Epoch: 141 [37120/225000 (16%)] Loss: 12471.921875\n",
      "Train Epoch: 141 [41216/225000 (18%)] Loss: 9657.742188\n",
      "Train Epoch: 141 [45312/225000 (20%)] Loss: 8248.572266\n",
      "Train Epoch: 141 [49408/225000 (22%)] Loss: 10427.437500\n",
      "Train Epoch: 141 [53504/225000 (24%)] Loss: 10497.371094\n",
      "Train Epoch: 141 [57600/225000 (26%)] Loss: 8038.152344\n",
      "Train Epoch: 141 [61696/225000 (27%)] Loss: 8050.832031\n",
      "Train Epoch: 141 [65792/225000 (29%)] Loss: 16421.585938\n",
      "Train Epoch: 141 [69888/225000 (31%)] Loss: 8139.996094\n",
      "Train Epoch: 141 [73984/225000 (33%)] Loss: 8190.955078\n",
      "Train Epoch: 141 [78080/225000 (35%)] Loss: 7879.144531\n",
      "Train Epoch: 141 [82176/225000 (37%)] Loss: 8200.080078\n",
      "Train Epoch: 141 [86272/225000 (38%)] Loss: 7897.648438\n",
      "Train Epoch: 141 [90368/225000 (40%)] Loss: 10748.859375\n",
      "Train Epoch: 141 [94464/225000 (42%)] Loss: 8076.794922\n",
      "Train Epoch: 141 [98560/225000 (44%)] Loss: 13603.597656\n",
      "Train Epoch: 141 [102656/225000 (46%)] Loss: 8037.763672\n",
      "Train Epoch: 141 [106752/225000 (47%)] Loss: 9614.419922\n",
      "Train Epoch: 141 [110848/225000 (49%)] Loss: 7952.900391\n",
      "Train Epoch: 141 [114944/225000 (51%)] Loss: 8009.865234\n",
      "Train Epoch: 141 [119040/225000 (53%)] Loss: 9617.148438\n",
      "Train Epoch: 141 [123136/225000 (55%)] Loss: 8101.230469\n",
      "Train Epoch: 141 [127232/225000 (57%)] Loss: 12299.902344\n",
      "Train Epoch: 141 [131328/225000 (58%)] Loss: 7955.779297\n",
      "Train Epoch: 141 [135424/225000 (60%)] Loss: 9953.820312\n",
      "Train Epoch: 141 [139520/225000 (62%)] Loss: 8065.402344\n",
      "Train Epoch: 141 [143616/225000 (64%)] Loss: 10308.636719\n",
      "Train Epoch: 141 [147712/225000 (66%)] Loss: 14044.990234\n",
      "Train Epoch: 141 [151808/225000 (67%)] Loss: 9712.460938\n",
      "Train Epoch: 141 [155904/225000 (69%)] Loss: 8040.367188\n",
      "Train Epoch: 141 [160000/225000 (71%)] Loss: 12347.835938\n",
      "Train Epoch: 141 [164096/225000 (73%)] Loss: 13744.607422\n",
      "Train Epoch: 141 [168192/225000 (75%)] Loss: 8160.875000\n",
      "Train Epoch: 141 [172288/225000 (77%)] Loss: 16157.240234\n",
      "Train Epoch: 141 [176384/225000 (78%)] Loss: 8235.623047\n",
      "Train Epoch: 141 [180480/225000 (80%)] Loss: 8107.164062\n",
      "Train Epoch: 141 [184576/225000 (82%)] Loss: 13668.742188\n",
      "Train Epoch: 141 [188672/225000 (84%)] Loss: 10007.578125\n",
      "Train Epoch: 141 [192768/225000 (86%)] Loss: 16453.152344\n",
      "Train Epoch: 141 [196864/225000 (87%)] Loss: 7985.865234\n",
      "Train Epoch: 141 [200960/225000 (89%)] Loss: 7881.695312\n",
      "Train Epoch: 141 [205056/225000 (91%)] Loss: 8165.640625\n",
      "Train Epoch: 141 [209152/225000 (93%)] Loss: 12067.390625\n",
      "Train Epoch: 141 [213248/225000 (95%)] Loss: 7892.105469\n",
      "Train Epoch: 141 [217344/225000 (97%)] Loss: 7977.917969\n",
      "Train Epoch: 141 [221440/225000 (98%)] Loss: 7986.667969\n",
      "    epoch          : 141\n",
      "    loss           : 9547.61031712173\n",
      "    val_loss       : 9442.862827253584\n",
      "Train Epoch: 142 [256/225000 (0%)] Loss: 8263.937500\n",
      "Train Epoch: 142 [4352/225000 (2%)] Loss: 9697.599609\n",
      "Train Epoch: 142 [8448/225000 (4%)] Loss: 8103.275391\n",
      "Train Epoch: 142 [12544/225000 (6%)] Loss: 7987.955078\n",
      "Train Epoch: 142 [16640/225000 (7%)] Loss: 8086.599609\n",
      "Train Epoch: 142 [20736/225000 (9%)] Loss: 8070.296875\n",
      "Train Epoch: 142 [24832/225000 (11%)] Loss: 7974.923828\n",
      "Train Epoch: 142 [28928/225000 (13%)] Loss: 8202.093750\n",
      "Train Epoch: 142 [33024/225000 (15%)] Loss: 10734.380859\n",
      "Train Epoch: 142 [37120/225000 (16%)] Loss: 7905.966797\n",
      "Train Epoch: 142 [41216/225000 (18%)] Loss: 12604.380859\n",
      "Train Epoch: 142 [45312/225000 (20%)] Loss: 9577.783203\n",
      "Train Epoch: 142 [49408/225000 (22%)] Loss: 7876.613281\n",
      "Train Epoch: 142 [53504/225000 (24%)] Loss: 11082.183594\n",
      "Train Epoch: 142 [57600/225000 (26%)] Loss: 18502.132812\n",
      "Train Epoch: 142 [61696/225000 (27%)] Loss: 9677.730469\n",
      "Train Epoch: 142 [65792/225000 (29%)] Loss: 8153.220703\n",
      "Train Epoch: 142 [69888/225000 (31%)] Loss: 8140.544922\n",
      "Train Epoch: 142 [73984/225000 (33%)] Loss: 8098.671875\n",
      "Train Epoch: 142 [78080/225000 (35%)] Loss: 8240.160156\n",
      "Train Epoch: 142 [82176/225000 (37%)] Loss: 15199.615234\n",
      "Train Epoch: 142 [86272/225000 (38%)] Loss: 7959.185547\n",
      "Train Epoch: 142 [90368/225000 (40%)] Loss: 7936.642578\n",
      "Train Epoch: 142 [94464/225000 (42%)] Loss: 8016.833984\n",
      "Train Epoch: 142 [98560/225000 (44%)] Loss: 10630.003906\n",
      "Train Epoch: 142 [102656/225000 (46%)] Loss: 7950.296875\n",
      "Train Epoch: 142 [106752/225000 (47%)] Loss: 9361.664062\n",
      "Train Epoch: 142 [110848/225000 (49%)] Loss: 10515.500000\n",
      "Train Epoch: 142 [114944/225000 (51%)] Loss: 8062.113281\n",
      "Train Epoch: 142 [119040/225000 (53%)] Loss: 10280.224609\n",
      "Train Epoch: 142 [123136/225000 (55%)] Loss: 7829.310547\n",
      "Train Epoch: 142 [127232/225000 (57%)] Loss: 8125.412109\n",
      "Train Epoch: 142 [131328/225000 (58%)] Loss: 9664.562500\n",
      "Train Epoch: 142 [135424/225000 (60%)] Loss: 7987.621094\n",
      "Train Epoch: 142 [139520/225000 (62%)] Loss: 12485.404297\n",
      "Train Epoch: 142 [143616/225000 (64%)] Loss: 10466.769531\n",
      "Train Epoch: 142 [147712/225000 (66%)] Loss: 10255.904297\n",
      "Train Epoch: 142 [151808/225000 (67%)] Loss: 8218.591797\n",
      "Train Epoch: 142 [155904/225000 (69%)] Loss: 9638.626953\n",
      "Train Epoch: 142 [160000/225000 (71%)] Loss: 9526.720703\n",
      "Train Epoch: 142 [164096/225000 (73%)] Loss: 8008.720703\n",
      "Train Epoch: 142 [168192/225000 (75%)] Loss: 10714.400391\n",
      "Train Epoch: 142 [172288/225000 (77%)] Loss: 8149.408203\n",
      "Train Epoch: 142 [176384/225000 (78%)] Loss: 9713.496094\n",
      "Train Epoch: 142 [180480/225000 (80%)] Loss: 13859.265625\n",
      "Train Epoch: 142 [184576/225000 (82%)] Loss: 8258.460938\n",
      "Train Epoch: 142 [188672/225000 (84%)] Loss: 8180.009766\n",
      "Train Epoch: 142 [192768/225000 (86%)] Loss: 13678.042969\n",
      "Train Epoch: 142 [196864/225000 (87%)] Loss: 8119.000000\n",
      "Train Epoch: 142 [200960/225000 (89%)] Loss: 15418.439453\n",
      "Train Epoch: 142 [205056/225000 (91%)] Loss: 9521.519531\n",
      "Train Epoch: 142 [209152/225000 (93%)] Loss: 9647.734375\n",
      "Train Epoch: 142 [213248/225000 (95%)] Loss: 8114.210938\n",
      "Train Epoch: 142 [217344/225000 (97%)] Loss: 8140.847656\n",
      "Train Epoch: 142 [221440/225000 (98%)] Loss: 8045.191406\n",
      "    epoch          : 142\n",
      "    loss           : 9664.13783196459\n",
      "    val_loss       : 9466.43294948826\n",
      "Train Epoch: 143 [256/225000 (0%)] Loss: 8064.750000\n",
      "Train Epoch: 143 [4352/225000 (2%)] Loss: 8064.453125\n",
      "Train Epoch: 143 [8448/225000 (4%)] Loss: 8040.619141\n",
      "Train Epoch: 143 [12544/225000 (6%)] Loss: 7962.867188\n",
      "Train Epoch: 143 [16640/225000 (7%)] Loss: 10387.732422\n",
      "Train Epoch: 143 [20736/225000 (9%)] Loss: 9724.449219\n",
      "Train Epoch: 143 [24832/225000 (11%)] Loss: 13530.921875\n",
      "Train Epoch: 143 [28928/225000 (13%)] Loss: 12917.457031\n",
      "Train Epoch: 143 [33024/225000 (15%)] Loss: 7966.787109\n",
      "Train Epoch: 143 [37120/225000 (16%)] Loss: 15450.214844\n",
      "Train Epoch: 143 [41216/225000 (18%)] Loss: 10416.675781\n",
      "Train Epoch: 143 [45312/225000 (20%)] Loss: 8072.888672\n",
      "Train Epoch: 143 [49408/225000 (22%)] Loss: 11113.390625\n",
      "Train Epoch: 143 [53504/225000 (24%)] Loss: 9653.482422\n",
      "Train Epoch: 143 [57600/225000 (26%)] Loss: 10555.347656\n",
      "Train Epoch: 143 [61696/225000 (27%)] Loss: 13784.720703\n",
      "Train Epoch: 143 [65792/225000 (29%)] Loss: 8180.001953\n",
      "Train Epoch: 143 [69888/225000 (31%)] Loss: 8151.542969\n",
      "Train Epoch: 143 [73984/225000 (33%)] Loss: 10444.974609\n",
      "Train Epoch: 143 [78080/225000 (35%)] Loss: 10531.148438\n",
      "Train Epoch: 143 [82176/225000 (37%)] Loss: 13782.431641\n",
      "Train Epoch: 143 [86272/225000 (38%)] Loss: 8002.623047\n",
      "Train Epoch: 143 [90368/225000 (40%)] Loss: 7966.816406\n",
      "Train Epoch: 143 [94464/225000 (42%)] Loss: 8154.126953\n",
      "Train Epoch: 143 [98560/225000 (44%)] Loss: 7951.500000\n",
      "Train Epoch: 143 [102656/225000 (46%)] Loss: 8002.498047\n",
      "Train Epoch: 143 [106752/225000 (47%)] Loss: 7889.109375\n",
      "Train Epoch: 143 [110848/225000 (49%)] Loss: 8285.164062\n",
      "Train Epoch: 143 [114944/225000 (51%)] Loss: 8076.189453\n",
      "Train Epoch: 143 [119040/225000 (53%)] Loss: 8209.935547\n",
      "Train Epoch: 143 [123136/225000 (55%)] Loss: 9503.017578\n",
      "Train Epoch: 143 [127232/225000 (57%)] Loss: 8199.617188\n",
      "Train Epoch: 143 [131328/225000 (58%)] Loss: 11942.345703\n",
      "Train Epoch: 143 [135424/225000 (60%)] Loss: 9522.501953\n",
      "Train Epoch: 143 [139520/225000 (62%)] Loss: 8153.943359\n",
      "Train Epoch: 143 [143616/225000 (64%)] Loss: 9648.035156\n",
      "Train Epoch: 143 [147712/225000 (66%)] Loss: 9734.544922\n",
      "Train Epoch: 143 [151808/225000 (67%)] Loss: 9685.490234\n",
      "Train Epoch: 143 [155904/225000 (69%)] Loss: 9743.148438\n",
      "Train Epoch: 143 [160000/225000 (71%)] Loss: 7925.455078\n",
      "Train Epoch: 143 [164096/225000 (73%)] Loss: 8276.830078\n",
      "Train Epoch: 143 [168192/225000 (75%)] Loss: 8102.339844\n",
      "Train Epoch: 143 [172288/225000 (77%)] Loss: 8032.148438\n",
      "Train Epoch: 143 [176384/225000 (78%)] Loss: 11098.041016\n",
      "Train Epoch: 143 [180480/225000 (80%)] Loss: 8099.808594\n",
      "Train Epoch: 143 [184576/225000 (82%)] Loss: 8202.345703\n",
      "Train Epoch: 143 [188672/225000 (84%)] Loss: 9683.628906\n",
      "Train Epoch: 143 [192768/225000 (86%)] Loss: 8075.560547\n",
      "Train Epoch: 143 [196864/225000 (87%)] Loss: 9549.605469\n",
      "Train Epoch: 143 [200960/225000 (89%)] Loss: 13878.644531\n",
      "Train Epoch: 143 [205056/225000 (91%)] Loss: 8107.886719\n",
      "Train Epoch: 143 [209152/225000 (93%)] Loss: 10447.775391\n",
      "Train Epoch: 143 [213248/225000 (95%)] Loss: 8009.492188\n",
      "Train Epoch: 143 [217344/225000 (97%)] Loss: 8219.707031\n",
      "Train Epoch: 143 [221440/225000 (98%)] Loss: 8259.216797\n",
      "    epoch          : 143\n",
      "    loss           : 9540.032425430176\n",
      "    val_loss       : 9575.501450949785\n",
      "Train Epoch: 144 [256/225000 (0%)] Loss: 12875.925781\n",
      "Train Epoch: 144 [4352/225000 (2%)] Loss: 8080.464844\n",
      "Train Epoch: 144 [8448/225000 (4%)] Loss: 8077.294922\n",
      "Train Epoch: 144 [12544/225000 (6%)] Loss: 12066.003906\n",
      "Train Epoch: 144 [16640/225000 (7%)] Loss: 7992.648438\n",
      "Train Epoch: 144 [20736/225000 (9%)] Loss: 7956.033203\n",
      "Train Epoch: 144 [24832/225000 (11%)] Loss: 8341.013672\n",
      "Train Epoch: 144 [28928/225000 (13%)] Loss: 8042.800781\n",
      "Train Epoch: 144 [33024/225000 (15%)] Loss: 8095.892578\n",
      "Train Epoch: 144 [37120/225000 (16%)] Loss: 7926.181641\n",
      "Train Epoch: 144 [41216/225000 (18%)] Loss: 8045.291016\n",
      "Train Epoch: 144 [45312/225000 (20%)] Loss: 10478.128906\n",
      "Train Epoch: 144 [49408/225000 (22%)] Loss: 8180.521484\n",
      "Train Epoch: 144 [53504/225000 (24%)] Loss: 13100.599609\n",
      "Train Epoch: 144 [57600/225000 (26%)] Loss: 7856.355469\n",
      "Train Epoch: 144 [61696/225000 (27%)] Loss: 7837.882812\n",
      "Train Epoch: 144 [65792/225000 (29%)] Loss: 13842.126953\n",
      "Train Epoch: 144 [69888/225000 (31%)] Loss: 8017.345703\n",
      "Train Epoch: 144 [73984/225000 (33%)] Loss: 13819.878906\n",
      "Train Epoch: 144 [78080/225000 (35%)] Loss: 19263.595703\n",
      "Train Epoch: 144 [82176/225000 (37%)] Loss: 11959.914062\n",
      "Train Epoch: 144 [86272/225000 (38%)] Loss: 8072.123047\n",
      "Train Epoch: 144 [90368/225000 (40%)] Loss: 8036.246094\n",
      "Train Epoch: 144 [94464/225000 (42%)] Loss: 8120.574219\n",
      "Train Epoch: 144 [98560/225000 (44%)] Loss: 7970.968750\n",
      "Train Epoch: 144 [102656/225000 (46%)] Loss: 7972.968750\n",
      "Train Epoch: 144 [106752/225000 (47%)] Loss: 16208.880859\n",
      "Train Epoch: 144 [110848/225000 (49%)] Loss: 12988.152344\n",
      "Train Epoch: 144 [114944/225000 (51%)] Loss: 7917.080078\n",
      "Train Epoch: 144 [119040/225000 (53%)] Loss: 8172.769531\n",
      "Train Epoch: 144 [123136/225000 (55%)] Loss: 8032.605469\n",
      "Train Epoch: 144 [127232/225000 (57%)] Loss: 10676.677734\n",
      "Train Epoch: 144 [131328/225000 (58%)] Loss: 8068.308594\n",
      "Train Epoch: 144 [135424/225000 (60%)] Loss: 8019.750000\n",
      "Train Epoch: 144 [139520/225000 (62%)] Loss: 8019.398438\n",
      "Train Epoch: 144 [143616/225000 (64%)] Loss: 7889.738281\n",
      "Train Epoch: 144 [147712/225000 (66%)] Loss: 9694.253906\n",
      "Train Epoch: 144 [151808/225000 (67%)] Loss: 8175.660156\n",
      "Train Epoch: 144 [155904/225000 (69%)] Loss: 12545.603516\n",
      "Train Epoch: 144 [160000/225000 (71%)] Loss: 9723.546875\n",
      "Train Epoch: 144 [164096/225000 (73%)] Loss: 7991.189453\n",
      "Train Epoch: 144 [168192/225000 (75%)] Loss: 15609.453125\n",
      "Train Epoch: 144 [172288/225000 (77%)] Loss: 9635.054688\n",
      "Train Epoch: 144 [176384/225000 (78%)] Loss: 9676.697266\n",
      "Train Epoch: 144 [180480/225000 (80%)] Loss: 8185.091797\n",
      "Train Epoch: 144 [184576/225000 (82%)] Loss: 9661.154297\n",
      "Train Epoch: 144 [188672/225000 (84%)] Loss: 10544.593750\n",
      "Train Epoch: 144 [192768/225000 (86%)] Loss: 8247.113281\n",
      "Train Epoch: 144 [196864/225000 (87%)] Loss: 8211.982422\n",
      "Train Epoch: 144 [200960/225000 (89%)] Loss: 10613.037109\n",
      "Train Epoch: 144 [205056/225000 (91%)] Loss: 9639.291016\n",
      "Train Epoch: 144 [209152/225000 (93%)] Loss: 7927.902344\n",
      "Train Epoch: 144 [213248/225000 (95%)] Loss: 15284.742188\n",
      "Train Epoch: 144 [217344/225000 (97%)] Loss: 8042.150391\n",
      "Train Epoch: 144 [221440/225000 (98%)] Loss: 8194.621094\n",
      "    epoch          : 144\n",
      "    loss           : 9494.461115258817\n",
      "    val_loss       : 9601.433129378727\n",
      "Train Epoch: 145 [256/225000 (0%)] Loss: 8325.148438\n",
      "Train Epoch: 145 [4352/225000 (2%)] Loss: 10629.875000\n",
      "Train Epoch: 145 [8448/225000 (4%)] Loss: 8284.333984\n",
      "Train Epoch: 145 [12544/225000 (6%)] Loss: 8195.560547\n",
      "Train Epoch: 145 [16640/225000 (7%)] Loss: 7983.849609\n",
      "Train Epoch: 145 [20736/225000 (9%)] Loss: 8003.921875\n",
      "Train Epoch: 145 [24832/225000 (11%)] Loss: 9661.904297\n",
      "Train Epoch: 145 [28928/225000 (13%)] Loss: 7966.605469\n",
      "Train Epoch: 145 [33024/225000 (15%)] Loss: 8148.011719\n",
      "Train Epoch: 145 [37120/225000 (16%)] Loss: 8090.095703\n",
      "Train Epoch: 145 [41216/225000 (18%)] Loss: 8131.826172\n",
      "Train Epoch: 145 [45312/225000 (20%)] Loss: 9688.365234\n",
      "Train Epoch: 145 [49408/225000 (22%)] Loss: 10505.236328\n",
      "Train Epoch: 145 [53504/225000 (24%)] Loss: 12376.570312\n",
      "Train Epoch: 145 [57600/225000 (26%)] Loss: 8066.357422\n",
      "Train Epoch: 145 [61696/225000 (27%)] Loss: 8003.978516\n",
      "Train Epoch: 145 [65792/225000 (29%)] Loss: 8211.501953\n",
      "Train Epoch: 145 [69888/225000 (31%)] Loss: 13615.968750\n",
      "Train Epoch: 145 [73984/225000 (33%)] Loss: 14660.572266\n",
      "Train Epoch: 145 [78080/225000 (35%)] Loss: 7916.285156\n",
      "Train Epoch: 145 [82176/225000 (37%)] Loss: 13444.435547\n",
      "Train Epoch: 145 [86272/225000 (38%)] Loss: 8138.763672\n",
      "Train Epoch: 145 [90368/225000 (40%)] Loss: 8042.021484\n",
      "Train Epoch: 145 [94464/225000 (42%)] Loss: 9478.945312\n",
      "Train Epoch: 145 [98560/225000 (44%)] Loss: 12441.298828\n",
      "Train Epoch: 145 [102656/225000 (46%)] Loss: 8005.818359\n",
      "Train Epoch: 145 [106752/225000 (47%)] Loss: 8077.708984\n",
      "Train Epoch: 145 [110848/225000 (49%)] Loss: 12471.611328\n",
      "Train Epoch: 145 [114944/225000 (51%)] Loss: 10444.267578\n",
      "Train Epoch: 145 [119040/225000 (53%)] Loss: 12239.183594\n",
      "Train Epoch: 145 [123136/225000 (55%)] Loss: 8063.304688\n",
      "Train Epoch: 145 [127232/225000 (57%)] Loss: 7973.529297\n",
      "Train Epoch: 145 [131328/225000 (58%)] Loss: 8017.431641\n",
      "Train Epoch: 145 [135424/225000 (60%)] Loss: 13920.218750\n",
      "Train Epoch: 145 [139520/225000 (62%)] Loss: 8099.996094\n",
      "Train Epoch: 145 [143616/225000 (64%)] Loss: 7994.630859\n",
      "Train Epoch: 145 [147712/225000 (66%)] Loss: 8199.894531\n",
      "Train Epoch: 145 [151808/225000 (67%)] Loss: 7876.156250\n",
      "Train Epoch: 145 [155904/225000 (69%)] Loss: 9588.546875\n",
      "Train Epoch: 145 [160000/225000 (71%)] Loss: 8398.292969\n",
      "Train Epoch: 145 [164096/225000 (73%)] Loss: 10527.701172\n",
      "Train Epoch: 145 [168192/225000 (75%)] Loss: 9605.044922\n",
      "Train Epoch: 145 [172288/225000 (77%)] Loss: 12898.093750\n",
      "Train Epoch: 145 [176384/225000 (78%)] Loss: 10466.820312\n",
      "Train Epoch: 145 [180480/225000 (80%)] Loss: 7938.148438\n",
      "Train Epoch: 145 [184576/225000 (82%)] Loss: 8088.914062\n",
      "Train Epoch: 145 [188672/225000 (84%)] Loss: 8086.513672\n",
      "Train Epoch: 145 [192768/225000 (86%)] Loss: 9478.548828\n",
      "Train Epoch: 145 [196864/225000 (87%)] Loss: 12367.857422\n",
      "Train Epoch: 145 [200960/225000 (89%)] Loss: 14110.957031\n",
      "Train Epoch: 145 [205056/225000 (91%)] Loss: 13240.707031\n",
      "Train Epoch: 145 [209152/225000 (93%)] Loss: 7930.337891\n",
      "Train Epoch: 145 [213248/225000 (95%)] Loss: 7956.777344\n",
      "Train Epoch: 145 [217344/225000 (97%)] Loss: 9654.017578\n",
      "Train Epoch: 145 [221440/225000 (98%)] Loss: 10324.664062\n",
      "    epoch          : 145\n",
      "    loss           : 9625.31239778868\n",
      "    val_loss       : 9586.261289693872\n",
      "Train Epoch: 146 [256/225000 (0%)] Loss: 9617.318359\n",
      "Train Epoch: 146 [4352/225000 (2%)] Loss: 13963.822266\n",
      "Train Epoch: 146 [8448/225000 (4%)] Loss: 12434.484375\n",
      "Train Epoch: 146 [12544/225000 (6%)] Loss: 13891.562500\n",
      "Train Epoch: 146 [16640/225000 (7%)] Loss: 10596.148438\n",
      "Train Epoch: 146 [20736/225000 (9%)] Loss: 8029.212891\n",
      "Train Epoch: 146 [24832/225000 (11%)] Loss: 13347.175781\n",
      "Train Epoch: 146 [28928/225000 (13%)] Loss: 7948.558594\n",
      "Train Epoch: 146 [33024/225000 (15%)] Loss: 8150.365234\n",
      "Train Epoch: 146 [37120/225000 (16%)] Loss: 13796.662109\n",
      "Train Epoch: 146 [41216/225000 (18%)] Loss: 13722.740234\n",
      "Train Epoch: 146 [45312/225000 (20%)] Loss: 8262.785156\n",
      "Train Epoch: 146 [49408/225000 (22%)] Loss: 7817.843750\n",
      "Train Epoch: 146 [53504/225000 (24%)] Loss: 10437.537109\n",
      "Train Epoch: 146 [57600/225000 (26%)] Loss: 13791.306641\n",
      "Train Epoch: 146 [61696/225000 (27%)] Loss: 8137.208984\n",
      "Train Epoch: 146 [65792/225000 (29%)] Loss: 10273.371094\n",
      "Train Epoch: 146 [69888/225000 (31%)] Loss: 13599.759766\n",
      "Train Epoch: 146 [73984/225000 (33%)] Loss: 7828.433594\n",
      "Train Epoch: 146 [78080/225000 (35%)] Loss: 10389.412109\n",
      "Train Epoch: 146 [82176/225000 (37%)] Loss: 13458.027344\n",
      "Train Epoch: 146 [86272/225000 (38%)] Loss: 8101.357422\n",
      "Train Epoch: 146 [90368/225000 (40%)] Loss: 11492.597656\n",
      "Train Epoch: 146 [94464/225000 (42%)] Loss: 8038.410156\n",
      "Train Epoch: 146 [98560/225000 (44%)] Loss: 8059.251953\n",
      "Train Epoch: 146 [102656/225000 (46%)] Loss: 8080.818359\n",
      "Train Epoch: 146 [106752/225000 (47%)] Loss: 8247.802734\n",
      "Train Epoch: 146 [110848/225000 (49%)] Loss: 7983.367188\n",
      "Train Epoch: 146 [114944/225000 (51%)] Loss: 9777.943359\n",
      "Train Epoch: 146 [119040/225000 (53%)] Loss: 13617.148438\n",
      "Train Epoch: 146 [123136/225000 (55%)] Loss: 8257.712891\n",
      "Train Epoch: 146 [127232/225000 (57%)] Loss: 8019.230469\n",
      "Train Epoch: 146 [131328/225000 (58%)] Loss: 8038.039062\n",
      "Train Epoch: 146 [135424/225000 (60%)] Loss: 8189.189453\n",
      "Train Epoch: 146 [139520/225000 (62%)] Loss: 8185.402344\n",
      "Train Epoch: 146 [143616/225000 (64%)] Loss: 7942.580078\n",
      "Train Epoch: 146 [147712/225000 (66%)] Loss: 8005.757812\n",
      "Train Epoch: 146 [151808/225000 (67%)] Loss: 8100.599609\n",
      "Train Epoch: 146 [155904/225000 (69%)] Loss: 13756.886719\n",
      "Train Epoch: 146 [160000/225000 (71%)] Loss: 8108.509766\n",
      "Train Epoch: 146 [164096/225000 (73%)] Loss: 7929.277344\n",
      "Train Epoch: 146 [168192/225000 (75%)] Loss: 10540.068359\n",
      "Train Epoch: 146 [172288/225000 (77%)] Loss: 8089.714844\n",
      "Train Epoch: 146 [176384/225000 (78%)] Loss: 10752.503906\n",
      "Train Epoch: 146 [180480/225000 (80%)] Loss: 8146.105469\n",
      "Train Epoch: 146 [184576/225000 (82%)] Loss: 9467.412109\n",
      "Train Epoch: 146 [188672/225000 (84%)] Loss: 8042.109375\n",
      "Train Epoch: 146 [192768/225000 (86%)] Loss: 9735.683594\n",
      "Train Epoch: 146 [196864/225000 (87%)] Loss: 10485.322266\n",
      "Train Epoch: 146 [200960/225000 (89%)] Loss: 8033.531250\n",
      "Train Epoch: 146 [205056/225000 (91%)] Loss: 8155.525391\n",
      "Train Epoch: 146 [209152/225000 (93%)] Loss: 7910.199219\n",
      "Train Epoch: 146 [213248/225000 (95%)] Loss: 9527.162109\n",
      "Train Epoch: 146 [217344/225000 (97%)] Loss: 9679.447266\n",
      "Train Epoch: 146 [221440/225000 (98%)] Loss: 10643.816406\n",
      "    epoch          : 146\n",
      "    loss           : 9616.389168488695\n",
      "    val_loss       : 9529.808866785497\n",
      "Train Epoch: 147 [256/225000 (0%)] Loss: 8246.298828\n",
      "Train Epoch: 147 [4352/225000 (2%)] Loss: 7941.394531\n",
      "Train Epoch: 147 [8448/225000 (4%)] Loss: 7994.201172\n",
      "Train Epoch: 147 [12544/225000 (6%)] Loss: 11137.990234\n",
      "Train Epoch: 147 [16640/225000 (7%)] Loss: 8026.970703\n",
      "Train Epoch: 147 [20736/225000 (9%)] Loss: 8006.279297\n",
      "Train Epoch: 147 [24832/225000 (11%)] Loss: 8030.453125\n",
      "Train Epoch: 147 [28928/225000 (13%)] Loss: 8050.308594\n",
      "Train Epoch: 147 [33024/225000 (15%)] Loss: 12899.082031\n",
      "Train Epoch: 147 [37120/225000 (16%)] Loss: 9562.955078\n",
      "Train Epoch: 147 [41216/225000 (18%)] Loss: 8015.521484\n",
      "Train Epoch: 147 [45312/225000 (20%)] Loss: 9719.849609\n",
      "Train Epoch: 147 [49408/225000 (22%)] Loss: 9519.609375\n",
      "Train Epoch: 147 [53504/225000 (24%)] Loss: 8151.523438\n",
      "Train Epoch: 147 [57600/225000 (26%)] Loss: 7988.943359\n",
      "Train Epoch: 147 [61696/225000 (27%)] Loss: 8002.376953\n",
      "Train Epoch: 147 [65792/225000 (29%)] Loss: 10526.759766\n",
      "Train Epoch: 147 [69888/225000 (31%)] Loss: 8027.677734\n",
      "Train Epoch: 147 [73984/225000 (33%)] Loss: 13687.806641\n",
      "Train Epoch: 147 [78080/225000 (35%)] Loss: 12231.365234\n",
      "Train Epoch: 147 [82176/225000 (37%)] Loss: 7859.115234\n",
      "Train Epoch: 147 [86272/225000 (38%)] Loss: 9456.443359\n",
      "Train Epoch: 147 [90368/225000 (40%)] Loss: 9443.525391\n",
      "Train Epoch: 147 [94464/225000 (42%)] Loss: 8095.585938\n",
      "Train Epoch: 147 [98560/225000 (44%)] Loss: 7777.396484\n",
      "Train Epoch: 147 [102656/225000 (46%)] Loss: 7993.957031\n",
      "Train Epoch: 147 [106752/225000 (47%)] Loss: 8214.259766\n",
      "Train Epoch: 147 [110848/225000 (49%)] Loss: 9683.453125\n",
      "Train Epoch: 147 [114944/225000 (51%)] Loss: 8034.619141\n",
      "Train Epoch: 147 [119040/225000 (53%)] Loss: 12134.421875\n",
      "Train Epoch: 147 [123136/225000 (55%)] Loss: 8140.660156\n",
      "Train Epoch: 147 [127232/225000 (57%)] Loss: 8124.548828\n",
      "Train Epoch: 147 [131328/225000 (58%)] Loss: 10454.447266\n",
      "Train Epoch: 147 [135424/225000 (60%)] Loss: 7914.025391\n",
      "Train Epoch: 147 [139520/225000 (62%)] Loss: 9628.695312\n",
      "Train Epoch: 147 [143616/225000 (64%)] Loss: 12019.046875\n",
      "Train Epoch: 147 [147712/225000 (66%)] Loss: 7972.705078\n",
      "Train Epoch: 147 [151808/225000 (67%)] Loss: 9532.707031\n",
      "Train Epoch: 147 [155904/225000 (69%)] Loss: 8050.705078\n",
      "Train Epoch: 147 [160000/225000 (71%)] Loss: 10683.738281\n",
      "Train Epoch: 147 [164096/225000 (73%)] Loss: 7984.275391\n",
      "Train Epoch: 147 [168192/225000 (75%)] Loss: 8060.320312\n",
      "Train Epoch: 147 [172288/225000 (77%)] Loss: 16330.587891\n",
      "Train Epoch: 147 [176384/225000 (78%)] Loss: 8108.753906\n",
      "Train Epoch: 147 [180480/225000 (80%)] Loss: 8097.203125\n",
      "Train Epoch: 147 [184576/225000 (82%)] Loss: 14007.623047\n",
      "Train Epoch: 147 [188672/225000 (84%)] Loss: 14767.054688\n",
      "Train Epoch: 147 [192768/225000 (86%)] Loss: 8064.462891\n",
      "Train Epoch: 147 [196864/225000 (87%)] Loss: 8070.810547\n",
      "Train Epoch: 147 [200960/225000 (89%)] Loss: 10616.347656\n",
      "Train Epoch: 147 [205056/225000 (91%)] Loss: 7956.585938\n",
      "Train Epoch: 147 [209152/225000 (93%)] Loss: 8067.947266\n",
      "Train Epoch: 147 [213248/225000 (95%)] Loss: 7946.835938\n",
      "Train Epoch: 147 [217344/225000 (97%)] Loss: 13641.566406\n",
      "Train Epoch: 147 [221440/225000 (98%)] Loss: 10374.671875\n",
      "    epoch          : 147\n",
      "    loss           : 9595.757532529864\n",
      "    val_loss       : 9847.69840059475\n",
      "Train Epoch: 148 [256/225000 (0%)] Loss: 7987.455078\n",
      "Train Epoch: 148 [4352/225000 (2%)] Loss: 8052.210938\n",
      "Train Epoch: 148 [8448/225000 (4%)] Loss: 17926.392578\n",
      "Train Epoch: 148 [12544/225000 (6%)] Loss: 9624.626953\n",
      "Train Epoch: 148 [16640/225000 (7%)] Loss: 10543.736328\n",
      "Train Epoch: 148 [20736/225000 (9%)] Loss: 8078.097656\n",
      "Train Epoch: 148 [24832/225000 (11%)] Loss: 8089.324219\n",
      "Train Epoch: 148 [28928/225000 (13%)] Loss: 8094.058594\n",
      "Train Epoch: 148 [33024/225000 (15%)] Loss: 8036.941406\n",
      "Train Epoch: 148 [37120/225000 (16%)] Loss: 8078.146484\n",
      "Train Epoch: 148 [41216/225000 (18%)] Loss: 7882.291016\n",
      "Train Epoch: 148 [45312/225000 (20%)] Loss: 8256.361328\n",
      "Train Epoch: 148 [49408/225000 (22%)] Loss: 8157.525391\n",
      "Train Epoch: 148 [53504/225000 (24%)] Loss: 13360.132812\n",
      "Train Epoch: 148 [57600/225000 (26%)] Loss: 8015.455078\n",
      "Train Epoch: 148 [61696/225000 (27%)] Loss: 9624.996094\n",
      "Train Epoch: 148 [65792/225000 (29%)] Loss: 10510.787109\n",
      "Train Epoch: 148 [69888/225000 (31%)] Loss: 8077.449219\n",
      "Train Epoch: 148 [73984/225000 (33%)] Loss: 7982.859375\n",
      "Train Epoch: 148 [78080/225000 (35%)] Loss: 9865.287109\n",
      "Train Epoch: 148 [82176/225000 (37%)] Loss: 9719.228516\n",
      "Train Epoch: 148 [86272/225000 (38%)] Loss: 8056.298828\n",
      "Train Epoch: 148 [90368/225000 (40%)] Loss: 9574.755859\n",
      "Train Epoch: 148 [94464/225000 (42%)] Loss: 7921.363281\n",
      "Train Epoch: 148 [98560/225000 (44%)] Loss: 9744.316406\n",
      "Train Epoch: 148 [102656/225000 (46%)] Loss: 8119.421875\n",
      "Train Epoch: 148 [106752/225000 (47%)] Loss: 8034.890625\n",
      "Train Epoch: 148 [110848/225000 (49%)] Loss: 8199.710938\n",
      "Train Epoch: 148 [114944/225000 (51%)] Loss: 7864.037109\n",
      "Train Epoch: 148 [119040/225000 (53%)] Loss: 8002.560547\n",
      "Train Epoch: 148 [123136/225000 (55%)] Loss: 8199.660156\n",
      "Train Epoch: 148 [127232/225000 (57%)] Loss: 8095.457031\n",
      "Train Epoch: 148 [131328/225000 (58%)] Loss: 8125.830078\n",
      "Train Epoch: 148 [135424/225000 (60%)] Loss: 7993.630859\n",
      "Train Epoch: 148 [139520/225000 (62%)] Loss: 8152.240234\n",
      "Train Epoch: 148 [143616/225000 (64%)] Loss: 18092.757812\n",
      "Train Epoch: 148 [147712/225000 (66%)] Loss: 8069.226562\n",
      "Train Epoch: 148 [151808/225000 (67%)] Loss: 10439.214844\n",
      "Train Epoch: 148 [155904/225000 (69%)] Loss: 9605.285156\n",
      "Train Epoch: 148 [160000/225000 (71%)] Loss: 7914.779297\n",
      "Train Epoch: 148 [164096/225000 (73%)] Loss: 9560.955078\n",
      "Train Epoch: 148 [168192/225000 (75%)] Loss: 8021.478516\n",
      "Train Epoch: 148 [172288/225000 (77%)] Loss: 14821.242188\n",
      "Train Epoch: 148 [176384/225000 (78%)] Loss: 9921.220703\n",
      "Train Epoch: 148 [180480/225000 (80%)] Loss: 9699.765625\n",
      "Train Epoch: 148 [184576/225000 (82%)] Loss: 7881.515625\n",
      "Train Epoch: 148 [188672/225000 (84%)] Loss: 9771.769531\n",
      "Train Epoch: 148 [192768/225000 (86%)] Loss: 13756.035156\n",
      "Train Epoch: 148 [196864/225000 (87%)] Loss: 8144.652344\n",
      "Train Epoch: 148 [200960/225000 (89%)] Loss: 8409.562500\n",
      "Train Epoch: 148 [205056/225000 (91%)] Loss: 7988.294922\n",
      "Train Epoch: 148 [209152/225000 (93%)] Loss: 10528.980469\n",
      "Train Epoch: 148 [213248/225000 (95%)] Loss: 9594.695312\n",
      "Train Epoch: 148 [217344/225000 (97%)] Loss: 8094.568359\n",
      "Train Epoch: 148 [221440/225000 (98%)] Loss: 9385.736328\n",
      "    epoch          : 148\n",
      "    loss           : 9646.886692086178\n",
      "    val_loss       : 9768.099054202741\n",
      "Train Epoch: 149 [256/225000 (0%)] Loss: 8066.974609\n",
      "Train Epoch: 149 [4352/225000 (2%)] Loss: 14854.960938\n",
      "Train Epoch: 149 [8448/225000 (4%)] Loss: 7977.658203\n",
      "Train Epoch: 149 [12544/225000 (6%)] Loss: 13612.255859\n",
      "Train Epoch: 149 [16640/225000 (7%)] Loss: 7888.486328\n",
      "Train Epoch: 149 [20736/225000 (9%)] Loss: 10292.011719\n",
      "Train Epoch: 149 [24832/225000 (11%)] Loss: 8117.968750\n",
      "Train Epoch: 149 [28928/225000 (13%)] Loss: 8098.431641\n",
      "Train Epoch: 149 [33024/225000 (15%)] Loss: 7871.183594\n",
      "Train Epoch: 149 [37120/225000 (16%)] Loss: 8200.791016\n",
      "Train Epoch: 149 [41216/225000 (18%)] Loss: 9439.300781\n",
      "Train Epoch: 149 [45312/225000 (20%)] Loss: 7944.468750\n",
      "Train Epoch: 149 [49408/225000 (22%)] Loss: 7948.560547\n",
      "Train Epoch: 149 [53504/225000 (24%)] Loss: 7979.535156\n",
      "Train Epoch: 149 [57600/225000 (26%)] Loss: 13717.435547\n",
      "Train Epoch: 149 [61696/225000 (27%)] Loss: 7951.541016\n",
      "Train Epoch: 149 [65792/225000 (29%)] Loss: 10480.291016\n",
      "Train Epoch: 149 [69888/225000 (31%)] Loss: 13804.189453\n",
      "Train Epoch: 149 [73984/225000 (33%)] Loss: 13757.691406\n",
      "Train Epoch: 149 [78080/225000 (35%)] Loss: 9734.660156\n",
      "Train Epoch: 149 [82176/225000 (37%)] Loss: 8022.628906\n",
      "Train Epoch: 149 [86272/225000 (38%)] Loss: 8337.560547\n",
      "Train Epoch: 149 [90368/225000 (40%)] Loss: 9398.468750\n",
      "Train Epoch: 149 [94464/225000 (42%)] Loss: 9565.539062\n",
      "Train Epoch: 149 [98560/225000 (44%)] Loss: 8052.673828\n",
      "Train Epoch: 149 [102656/225000 (46%)] Loss: 10427.544922\n",
      "Train Epoch: 149 [106752/225000 (47%)] Loss: 8048.648438\n",
      "Train Epoch: 149 [110848/225000 (49%)] Loss: 10572.281250\n",
      "Train Epoch: 149 [114944/225000 (51%)] Loss: 7877.480469\n",
      "Train Epoch: 149 [119040/225000 (53%)] Loss: 13629.693359\n",
      "Train Epoch: 149 [123136/225000 (55%)] Loss: 8035.417969\n",
      "Train Epoch: 149 [127232/225000 (57%)] Loss: 8091.019531\n",
      "Train Epoch: 149 [131328/225000 (58%)] Loss: 8105.367188\n",
      "Train Epoch: 149 [135424/225000 (60%)] Loss: 7949.769531\n",
      "Train Epoch: 149 [139520/225000 (62%)] Loss: 8183.826172\n",
      "Train Epoch: 149 [143616/225000 (64%)] Loss: 15185.181641\n",
      "Train Epoch: 149 [147712/225000 (66%)] Loss: 8159.183594\n",
      "Train Epoch: 149 [151808/225000 (67%)] Loss: 12288.273438\n",
      "Train Epoch: 149 [155904/225000 (69%)] Loss: 12141.583984\n",
      "Train Epoch: 149 [160000/225000 (71%)] Loss: 8066.306641\n",
      "Train Epoch: 149 [164096/225000 (73%)] Loss: 12746.431641\n",
      "Train Epoch: 149 [168192/225000 (75%)] Loss: 13478.146484\n",
      "Train Epoch: 149 [172288/225000 (77%)] Loss: 14724.640625\n",
      "Train Epoch: 149 [176384/225000 (78%)] Loss: 8278.113281\n",
      "Train Epoch: 149 [180480/225000 (80%)] Loss: 8058.183594\n",
      "Train Epoch: 149 [184576/225000 (82%)] Loss: 8080.105469\n",
      "Train Epoch: 149 [188672/225000 (84%)] Loss: 9439.914062\n",
      "Train Epoch: 149 [192768/225000 (86%)] Loss: 9525.158203\n",
      "Train Epoch: 149 [196864/225000 (87%)] Loss: 7972.210938\n",
      "Train Epoch: 149 [200960/225000 (89%)] Loss: 18609.328125\n",
      "Train Epoch: 149 [205056/225000 (91%)] Loss: 12110.425781\n",
      "Train Epoch: 149 [209152/225000 (93%)] Loss: 8080.658203\n",
      "Train Epoch: 149 [213248/225000 (95%)] Loss: 11116.117188\n",
      "Train Epoch: 149 [217344/225000 (97%)] Loss: 7943.308594\n",
      "Train Epoch: 149 [221440/225000 (98%)] Loss: 8042.859375\n",
      "    epoch          : 149\n",
      "    loss           : 9559.339576000782\n",
      "    val_loss       : 9487.572632489157\n",
      "Train Epoch: 150 [256/225000 (0%)] Loss: 10758.111328\n",
      "Train Epoch: 150 [4352/225000 (2%)] Loss: 8069.132812\n",
      "Train Epoch: 150 [8448/225000 (4%)] Loss: 8058.185547\n",
      "Train Epoch: 150 [12544/225000 (6%)] Loss: 13522.546875\n",
      "Train Epoch: 150 [16640/225000 (7%)] Loss: 8081.503906\n",
      "Train Epoch: 150 [20736/225000 (9%)] Loss: 12417.314453\n",
      "Train Epoch: 150 [24832/225000 (11%)] Loss: 7927.330078\n",
      "Train Epoch: 150 [28928/225000 (13%)] Loss: 8034.496094\n",
      "Train Epoch: 150 [33024/225000 (15%)] Loss: 7908.800781\n",
      "Train Epoch: 150 [37120/225000 (16%)] Loss: 8044.355469\n",
      "Train Epoch: 150 [41216/225000 (18%)] Loss: 8139.375000\n",
      "Train Epoch: 150 [45312/225000 (20%)] Loss: 7979.621094\n",
      "Train Epoch: 150 [49408/225000 (22%)] Loss: 11052.285156\n",
      "Train Epoch: 150 [53504/225000 (24%)] Loss: 12268.183594\n",
      "Train Epoch: 150 [57600/225000 (26%)] Loss: 8221.257812\n",
      "Train Epoch: 150 [61696/225000 (27%)] Loss: 8350.685547\n",
      "Train Epoch: 150 [65792/225000 (29%)] Loss: 8103.308594\n",
      "Train Epoch: 150 [69888/225000 (31%)] Loss: 15548.158203\n",
      "Train Epoch: 150 [73984/225000 (33%)] Loss: 13771.238281\n",
      "Train Epoch: 150 [78080/225000 (35%)] Loss: 8185.171875\n",
      "Train Epoch: 150 [82176/225000 (37%)] Loss: 8021.804688\n",
      "Train Epoch: 150 [86272/225000 (38%)] Loss: 13777.009766\n",
      "Train Epoch: 150 [90368/225000 (40%)] Loss: 8054.251953\n",
      "Train Epoch: 150 [94464/225000 (42%)] Loss: 10511.492188\n",
      "Train Epoch: 150 [98560/225000 (44%)] Loss: 9619.310547\n",
      "Train Epoch: 150 [102656/225000 (46%)] Loss: 12230.513672\n",
      "Train Epoch: 150 [106752/225000 (47%)] Loss: 8092.513672\n",
      "Train Epoch: 150 [110848/225000 (49%)] Loss: 7944.605469\n",
      "Train Epoch: 150 [114944/225000 (51%)] Loss: 8247.427734\n",
      "Train Epoch: 150 [119040/225000 (53%)] Loss: 9566.322266\n",
      "Train Epoch: 150 [123136/225000 (55%)] Loss: 8075.476562\n",
      "Train Epoch: 150 [127232/225000 (57%)] Loss: 8075.517578\n",
      "Train Epoch: 150 [131328/225000 (58%)] Loss: 13682.023438\n",
      "Train Epoch: 150 [135424/225000 (60%)] Loss: 9784.958984\n",
      "Train Epoch: 150 [139520/225000 (62%)] Loss: 9582.000000\n",
      "Train Epoch: 150 [143616/225000 (64%)] Loss: 8010.425781\n",
      "Train Epoch: 150 [147712/225000 (66%)] Loss: 7944.800781\n",
      "Train Epoch: 150 [151808/225000 (67%)] Loss: 7902.250000\n",
      "Train Epoch: 150 [155904/225000 (69%)] Loss: 7969.746094\n",
      "Train Epoch: 150 [160000/225000 (71%)] Loss: 8240.550781\n",
      "Train Epoch: 150 [164096/225000 (73%)] Loss: 7952.619141\n",
      "Train Epoch: 150 [168192/225000 (75%)] Loss: 12035.675781\n",
      "Train Epoch: 150 [172288/225000 (77%)] Loss: 7920.638672\n",
      "Train Epoch: 150 [176384/225000 (78%)] Loss: 12360.763672\n",
      "Train Epoch: 150 [180480/225000 (80%)] Loss: 8024.640625\n",
      "Train Epoch: 150 [184576/225000 (82%)] Loss: 8170.050781\n",
      "Train Epoch: 150 [188672/225000 (84%)] Loss: 16578.853516\n",
      "Train Epoch: 150 [192768/225000 (86%)] Loss: 10565.792969\n",
      "Train Epoch: 150 [196864/225000 (87%)] Loss: 8137.919922\n",
      "Train Epoch: 150 [200960/225000 (89%)] Loss: 7962.757812\n",
      "Train Epoch: 150 [205056/225000 (91%)] Loss: 8036.333984\n",
      "Train Epoch: 150 [209152/225000 (93%)] Loss: 7995.435547\n",
      "Train Epoch: 150 [213248/225000 (95%)] Loss: 8035.736328\n",
      "Train Epoch: 150 [217344/225000 (97%)] Loss: 7743.664062\n",
      "Train Epoch: 150 [221440/225000 (98%)] Loss: 8033.396484\n",
      "    epoch          : 150\n",
      "    loss           : 9580.328984908276\n",
      "    val_loss       : 9358.161897192196\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0103_171400/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [256/225000 (0%)] Loss: 8169.195312\n",
      "Train Epoch: 151 [4352/225000 (2%)] Loss: 7890.587891\n",
      "Train Epoch: 151 [8448/225000 (4%)] Loss: 7975.468750\n",
      "Train Epoch: 151 [12544/225000 (6%)] Loss: 8039.496094\n",
      "Train Epoch: 151 [16640/225000 (7%)] Loss: 10630.216797\n",
      "Train Epoch: 151 [20736/225000 (9%)] Loss: 13122.939453\n",
      "Train Epoch: 151 [24832/225000 (11%)] Loss: 8053.302734\n",
      "Train Epoch: 151 [28928/225000 (13%)] Loss: 7893.109375\n",
      "Train Epoch: 151 [33024/225000 (15%)] Loss: 7994.015625\n",
      "Train Epoch: 151 [37120/225000 (16%)] Loss: 13974.263672\n",
      "Train Epoch: 151 [41216/225000 (18%)] Loss: 9580.716797\n",
      "Train Epoch: 151 [45312/225000 (20%)] Loss: 8219.781250\n",
      "Train Epoch: 151 [49408/225000 (22%)] Loss: 8193.316406\n",
      "Train Epoch: 151 [53504/225000 (24%)] Loss: 7998.830078\n",
      "Train Epoch: 151 [57600/225000 (26%)] Loss: 16816.617188\n",
      "Train Epoch: 151 [61696/225000 (27%)] Loss: 10606.337891\n",
      "Train Epoch: 151 [65792/225000 (29%)] Loss: 12149.146484\n",
      "Train Epoch: 151 [69888/225000 (31%)] Loss: 8066.419922\n",
      "Train Epoch: 151 [73984/225000 (33%)] Loss: 10547.619141\n",
      "Train Epoch: 151 [78080/225000 (35%)] Loss: 8207.082031\n",
      "Train Epoch: 151 [82176/225000 (37%)] Loss: 9652.359375\n",
      "Train Epoch: 151 [86272/225000 (38%)] Loss: 8127.560547\n",
      "Train Epoch: 151 [90368/225000 (40%)] Loss: 11094.060547\n",
      "Train Epoch: 151 [94464/225000 (42%)] Loss: 9612.273438\n",
      "Train Epoch: 151 [98560/225000 (44%)] Loss: 13770.500000\n",
      "Train Epoch: 151 [102656/225000 (46%)] Loss: 13845.580078\n",
      "Train Epoch: 151 [106752/225000 (47%)] Loss: 9569.384766\n",
      "Train Epoch: 151 [110848/225000 (49%)] Loss: 9640.099609\n",
      "Train Epoch: 151 [114944/225000 (51%)] Loss: 8102.828125\n",
      "Train Epoch: 151 [119040/225000 (53%)] Loss: 7987.328125\n",
      "Train Epoch: 151 [123136/225000 (55%)] Loss: 9574.722656\n",
      "Train Epoch: 151 [127232/225000 (57%)] Loss: 8028.400391\n",
      "Train Epoch: 151 [131328/225000 (58%)] Loss: 8097.101562\n",
      "Train Epoch: 151 [135424/225000 (60%)] Loss: 9862.679688\n",
      "Train Epoch: 151 [139520/225000 (62%)] Loss: 10540.148438\n",
      "Train Epoch: 151 [143616/225000 (64%)] Loss: 8124.089844\n",
      "Train Epoch: 151 [147712/225000 (66%)] Loss: 7957.349609\n",
      "Train Epoch: 151 [151808/225000 (67%)] Loss: 10430.171875\n",
      "Train Epoch: 151 [155904/225000 (69%)] Loss: 8140.320312\n",
      "Train Epoch: 151 [160000/225000 (71%)] Loss: 9515.453125\n",
      "Train Epoch: 151 [164096/225000 (73%)] Loss: 8084.142578\n",
      "Train Epoch: 151 [168192/225000 (75%)] Loss: 12396.806641\n",
      "Train Epoch: 151 [172288/225000 (77%)] Loss: 10577.048828\n",
      "Train Epoch: 151 [176384/225000 (78%)] Loss: 8241.083984\n",
      "Train Epoch: 151 [180480/225000 (80%)] Loss: 9763.822266\n",
      "Train Epoch: 151 [184576/225000 (82%)] Loss: 7950.550781\n",
      "Train Epoch: 151 [188672/225000 (84%)] Loss: 8026.353516\n",
      "Train Epoch: 151 [192768/225000 (86%)] Loss: 9572.330078\n",
      "Train Epoch: 151 [196864/225000 (87%)] Loss: 9469.136719\n",
      "Train Epoch: 151 [200960/225000 (89%)] Loss: 9817.843750\n",
      "Train Epoch: 151 [205056/225000 (91%)] Loss: 12235.294922\n",
      "Train Epoch: 151 [209152/225000 (93%)] Loss: 7996.564453\n",
      "Train Epoch: 151 [213248/225000 (95%)] Loss: 7909.064453\n",
      "Train Epoch: 151 [217344/225000 (97%)] Loss: 8067.792969\n",
      "Train Epoch: 151 [221440/225000 (98%)] Loss: 8124.949219\n",
      "    epoch          : 151\n",
      "    loss           : 9477.431019580134\n",
      "    val_loss       : 9853.26888967047\n",
      "Train Epoch: 152 [256/225000 (0%)] Loss: 7998.607422\n",
      "Train Epoch: 152 [4352/225000 (2%)] Loss: 12234.222656\n",
      "Train Epoch: 152 [8448/225000 (4%)] Loss: 10535.916016\n",
      "Train Epoch: 152 [12544/225000 (6%)] Loss: 8029.892578\n",
      "Train Epoch: 152 [16640/225000 (7%)] Loss: 10493.074219\n",
      "Train Epoch: 152 [20736/225000 (9%)] Loss: 8085.195312\n",
      "Train Epoch: 152 [24832/225000 (11%)] Loss: 7974.880859\n",
      "Train Epoch: 152 [28928/225000 (13%)] Loss: 8247.716797\n",
      "Train Epoch: 152 [33024/225000 (15%)] Loss: 10680.851562\n",
      "Train Epoch: 152 [37120/225000 (16%)] Loss: 7884.585938\n",
      "Train Epoch: 152 [41216/225000 (18%)] Loss: 9700.953125\n",
      "Train Epoch: 152 [45312/225000 (20%)] Loss: 12480.876953\n",
      "Train Epoch: 152 [49408/225000 (22%)] Loss: 12192.800781\n",
      "Train Epoch: 152 [53504/225000 (24%)] Loss: 7976.890625\n",
      "Train Epoch: 152 [57600/225000 (26%)] Loss: 8063.556641\n",
      "Train Epoch: 152 [61696/225000 (27%)] Loss: 9575.183594\n",
      "Train Epoch: 152 [65792/225000 (29%)] Loss: 8087.105469\n",
      "Train Epoch: 152 [69888/225000 (31%)] Loss: 8117.732422\n",
      "Train Epoch: 152 [73984/225000 (33%)] Loss: 7945.648438\n",
      "Train Epoch: 152 [78080/225000 (35%)] Loss: 8088.232422\n",
      "Train Epoch: 152 [82176/225000 (37%)] Loss: 9558.396484\n",
      "Train Epoch: 152 [86272/225000 (38%)] Loss: 12733.267578\n",
      "Train Epoch: 152 [90368/225000 (40%)] Loss: 8282.767578\n",
      "Train Epoch: 152 [94464/225000 (42%)] Loss: 7854.238281\n",
      "Train Epoch: 152 [98560/225000 (44%)] Loss: 7955.011719\n",
      "Train Epoch: 152 [102656/225000 (46%)] Loss: 10609.304688\n",
      "Train Epoch: 152 [106752/225000 (47%)] Loss: 8279.923828\n",
      "Train Epoch: 152 [110848/225000 (49%)] Loss: 8003.177734\n",
      "Train Epoch: 152 [114944/225000 (51%)] Loss: 8238.212891\n",
      "Train Epoch: 152 [119040/225000 (53%)] Loss: 16292.617188\n",
      "Train Epoch: 152 [123136/225000 (55%)] Loss: 13663.800781\n",
      "Train Epoch: 152 [127232/225000 (57%)] Loss: 15919.824219\n",
      "Train Epoch: 152 [131328/225000 (58%)] Loss: 8015.351562\n",
      "Train Epoch: 152 [135424/225000 (60%)] Loss: 7986.103516\n",
      "Train Epoch: 152 [139520/225000 (62%)] Loss: 9567.714844\n",
      "Train Epoch: 152 [143616/225000 (64%)] Loss: 8112.589844\n",
      "Train Epoch: 152 [147712/225000 (66%)] Loss: 8159.669922\n",
      "Train Epoch: 152 [151808/225000 (67%)] Loss: 9552.080078\n",
      "Train Epoch: 152 [155904/225000 (69%)] Loss: 8001.099609\n",
      "Train Epoch: 152 [160000/225000 (71%)] Loss: 13791.447266\n",
      "Train Epoch: 152 [164096/225000 (73%)] Loss: 9522.427734\n",
      "Train Epoch: 152 [168192/225000 (75%)] Loss: 8029.750000\n",
      "Train Epoch: 152 [172288/225000 (77%)] Loss: 9571.921875\n",
      "Train Epoch: 152 [176384/225000 (78%)] Loss: 10578.259766\n",
      "Train Epoch: 152 [180480/225000 (80%)] Loss: 11215.666016\n",
      "Train Epoch: 152 [184576/225000 (82%)] Loss: 8016.095703\n",
      "Train Epoch: 152 [188672/225000 (84%)] Loss: 13659.658203\n",
      "Train Epoch: 152 [192768/225000 (86%)] Loss: 12442.953125\n",
      "Train Epoch: 152 [196864/225000 (87%)] Loss: 13582.128906\n",
      "Train Epoch: 152 [200960/225000 (89%)] Loss: 7948.140625\n",
      "Train Epoch: 152 [205056/225000 (91%)] Loss: 8180.851562\n",
      "Train Epoch: 152 [209152/225000 (93%)] Loss: 7920.750000\n",
      "Train Epoch: 152 [213248/225000 (95%)] Loss: 8117.060547\n",
      "Train Epoch: 152 [217344/225000 (97%)] Loss: 10551.041016\n",
      "Train Epoch: 152 [221440/225000 (98%)] Loss: 13880.097656\n",
      "    epoch          : 152\n",
      "    loss           : 9648.508242454138\n",
      "    val_loss       : 9713.127557384725\n",
      "Train Epoch: 153 [256/225000 (0%)] Loss: 12146.285156\n",
      "Train Epoch: 153 [4352/225000 (2%)] Loss: 8057.068359\n",
      "Train Epoch: 153 [8448/225000 (4%)] Loss: 8107.607422\n",
      "Train Epoch: 153 [12544/225000 (6%)] Loss: 10452.369141\n",
      "Train Epoch: 153 [16640/225000 (7%)] Loss: 12440.320312\n",
      "Train Epoch: 153 [20736/225000 (9%)] Loss: 8038.148438\n",
      "Train Epoch: 153 [24832/225000 (11%)] Loss: 8067.119141\n",
      "Train Epoch: 153 [28928/225000 (13%)] Loss: 12951.064453\n",
      "Train Epoch: 153 [33024/225000 (15%)] Loss: 13746.195312\n",
      "Train Epoch: 153 [37120/225000 (16%)] Loss: 8047.201172\n",
      "Train Epoch: 153 [41216/225000 (18%)] Loss: 8049.339844\n",
      "Train Epoch: 153 [45312/225000 (20%)] Loss: 8026.419922\n",
      "Train Epoch: 153 [49408/225000 (22%)] Loss: 13916.027344\n",
      "Train Epoch: 153 [53504/225000 (24%)] Loss: 9560.380859\n",
      "Train Epoch: 153 [57600/225000 (26%)] Loss: 7917.177734\n",
      "Train Epoch: 153 [61696/225000 (27%)] Loss: 7997.273438\n",
      "Train Epoch: 153 [65792/225000 (29%)] Loss: 7927.568359\n",
      "Train Epoch: 153 [69888/225000 (31%)] Loss: 13678.884766\n",
      "Train Epoch: 153 [73984/225000 (33%)] Loss: 15027.160156\n",
      "Train Epoch: 153 [78080/225000 (35%)] Loss: 10511.685547\n",
      "Train Epoch: 153 [82176/225000 (37%)] Loss: 9592.603516\n",
      "Train Epoch: 153 [86272/225000 (38%)] Loss: 7882.841797\n",
      "Train Epoch: 153 [90368/225000 (40%)] Loss: 9629.533203\n",
      "Train Epoch: 153 [94464/225000 (42%)] Loss: 7920.396484\n",
      "Train Epoch: 153 [98560/225000 (44%)] Loss: 8079.572266\n",
      "Train Epoch: 153 [102656/225000 (46%)] Loss: 7811.070312\n",
      "Train Epoch: 153 [106752/225000 (47%)] Loss: 8009.103516\n",
      "Train Epoch: 153 [110848/225000 (49%)] Loss: 12684.244141\n",
      "Train Epoch: 153 [114944/225000 (51%)] Loss: 15280.048828\n",
      "Train Epoch: 153 [119040/225000 (53%)] Loss: 8181.259766\n",
      "Train Epoch: 153 [123136/225000 (55%)] Loss: 8131.212891\n",
      "Train Epoch: 153 [127232/225000 (57%)] Loss: 10132.244141\n",
      "Train Epoch: 153 [131328/225000 (58%)] Loss: 11880.482422\n",
      "Train Epoch: 153 [135424/225000 (60%)] Loss: 8038.439453\n",
      "Train Epoch: 153 [139520/225000 (62%)] Loss: 8212.880859\n",
      "Train Epoch: 153 [143616/225000 (64%)] Loss: 8263.208984\n",
      "Train Epoch: 153 [147712/225000 (66%)] Loss: 7961.658203\n",
      "Train Epoch: 153 [151808/225000 (67%)] Loss: 9754.539062\n",
      "Train Epoch: 153 [155904/225000 (69%)] Loss: 8043.896484\n",
      "Train Epoch: 153 [160000/225000 (71%)] Loss: 13840.929688\n",
      "Train Epoch: 153 [164096/225000 (73%)] Loss: 13972.255859\n",
      "Train Epoch: 153 [168192/225000 (75%)] Loss: 7894.333984\n",
      "Train Epoch: 153 [172288/225000 (77%)] Loss: 7856.486328\n",
      "Train Epoch: 153 [176384/225000 (78%)] Loss: 9545.402344\n",
      "Train Epoch: 153 [180480/225000 (80%)] Loss: 8047.916016\n",
      "Train Epoch: 153 [184576/225000 (82%)] Loss: 9604.839844\n",
      "Train Epoch: 153 [188672/225000 (84%)] Loss: 8170.535156\n",
      "Train Epoch: 153 [192768/225000 (86%)] Loss: 8071.824219\n",
      "Train Epoch: 153 [196864/225000 (87%)] Loss: 8082.017578\n",
      "Train Epoch: 153 [200960/225000 (89%)] Loss: 12333.667969\n",
      "Train Epoch: 153 [205056/225000 (91%)] Loss: 8095.361328\n",
      "Train Epoch: 153 [209152/225000 (93%)] Loss: 7911.265625\n",
      "Train Epoch: 153 [213248/225000 (95%)] Loss: 9659.726562\n",
      "Train Epoch: 153 [217344/225000 (97%)] Loss: 8134.169922\n",
      "Train Epoch: 153 [221440/225000 (98%)] Loss: 7981.289062\n",
      "    epoch          : 153\n",
      "    loss           : 9502.727730153229\n",
      "    val_loss       : 9442.14467568209\n",
      "Train Epoch: 154 [256/225000 (0%)] Loss: 8105.027344\n",
      "Train Epoch: 154 [4352/225000 (2%)] Loss: 9552.648438\n",
      "Train Epoch: 154 [8448/225000 (4%)] Loss: 8022.634766\n",
      "Train Epoch: 154 [12544/225000 (6%)] Loss: 15043.560547\n",
      "Train Epoch: 154 [16640/225000 (7%)] Loss: 8013.257812\n",
      "Train Epoch: 154 [20736/225000 (9%)] Loss: 12914.146484\n",
      "Train Epoch: 154 [24832/225000 (11%)] Loss: 9775.296875\n",
      "Train Epoch: 154 [28928/225000 (13%)] Loss: 8020.152344\n",
      "Train Epoch: 154 [33024/225000 (15%)] Loss: 8199.105469\n",
      "Train Epoch: 154 [37120/225000 (16%)] Loss: 9604.406250\n",
      "Train Epoch: 154 [41216/225000 (18%)] Loss: 17954.675781\n",
      "Train Epoch: 154 [45312/225000 (20%)] Loss: 12549.353516\n",
      "Train Epoch: 154 [49408/225000 (22%)] Loss: 9543.490234\n",
      "Train Epoch: 154 [53504/225000 (24%)] Loss: 7958.472656\n",
      "Train Epoch: 154 [57600/225000 (26%)] Loss: 8162.404297\n",
      "Train Epoch: 154 [61696/225000 (27%)] Loss: 8153.873047\n",
      "Train Epoch: 154 [65792/225000 (29%)] Loss: 7919.896484\n",
      "Train Epoch: 154 [69888/225000 (31%)] Loss: 7963.988281\n",
      "Train Epoch: 154 [73984/225000 (33%)] Loss: 8149.859375\n",
      "Train Epoch: 154 [78080/225000 (35%)] Loss: 8049.921875\n",
      "Train Epoch: 154 [82176/225000 (37%)] Loss: 8249.000000\n",
      "Train Epoch: 154 [86272/225000 (38%)] Loss: 8248.408203\n",
      "Train Epoch: 154 [90368/225000 (40%)] Loss: 8256.558594\n",
      "Train Epoch: 154 [94464/225000 (42%)] Loss: 10563.355469\n",
      "Train Epoch: 154 [98560/225000 (44%)] Loss: 9629.644531\n",
      "Train Epoch: 154 [102656/225000 (46%)] Loss: 9425.640625\n",
      "Train Epoch: 154 [106752/225000 (47%)] Loss: 12863.521484\n",
      "Train Epoch: 154 [110848/225000 (49%)] Loss: 7941.597656\n",
      "Train Epoch: 154 [114944/225000 (51%)] Loss: 10617.781250\n",
      "Train Epoch: 154 [119040/225000 (53%)] Loss: 9640.041016\n",
      "Train Epoch: 154 [123136/225000 (55%)] Loss: 10500.583984\n",
      "Train Epoch: 154 [127232/225000 (57%)] Loss: 9779.318359\n",
      "Train Epoch: 154 [131328/225000 (58%)] Loss: 7931.775391\n",
      "Train Epoch: 154 [135424/225000 (60%)] Loss: 8202.060547\n",
      "Train Epoch: 154 [139520/225000 (62%)] Loss: 8060.970703\n",
      "Train Epoch: 154 [143616/225000 (64%)] Loss: 13710.146484\n",
      "Train Epoch: 154 [147712/225000 (66%)] Loss: 7867.562500\n",
      "Train Epoch: 154 [151808/225000 (67%)] Loss: 9529.302734\n",
      "Train Epoch: 154 [155904/225000 (69%)] Loss: 8112.207031\n",
      "Train Epoch: 154 [160000/225000 (71%)] Loss: 8221.230469\n",
      "Train Epoch: 154 [164096/225000 (73%)] Loss: 9623.451172\n",
      "Train Epoch: 154 [168192/225000 (75%)] Loss: 12834.617188\n",
      "Train Epoch: 154 [172288/225000 (77%)] Loss: 8053.878906\n",
      "Train Epoch: 154 [176384/225000 (78%)] Loss: 12210.019531\n",
      "Train Epoch: 154 [180480/225000 (80%)] Loss: 9547.505859\n",
      "Train Epoch: 154 [184576/225000 (82%)] Loss: 7928.564453\n",
      "Train Epoch: 154 [188672/225000 (84%)] Loss: 8193.472656\n",
      "Train Epoch: 154 [192768/225000 (86%)] Loss: 12452.964844\n",
      "Train Epoch: 154 [196864/225000 (87%)] Loss: 8073.556641\n",
      "Train Epoch: 154 [200960/225000 (89%)] Loss: 7958.255859\n",
      "Train Epoch: 154 [205056/225000 (91%)] Loss: 8145.154297\n",
      "Train Epoch: 154 [209152/225000 (93%)] Loss: 8086.556641\n",
      "Train Epoch: 154 [213248/225000 (95%)] Loss: 7932.515625\n",
      "Train Epoch: 154 [217344/225000 (97%)] Loss: 8207.839844\n",
      "Train Epoch: 154 [221440/225000 (98%)] Loss: 7992.496094\n",
      "    epoch          : 154\n",
      "    loss           : 9577.444778112556\n",
      "    val_loss       : 9464.311386517116\n",
      "Train Epoch: 155 [256/225000 (0%)] Loss: 8367.292969\n",
      "Train Epoch: 155 [4352/225000 (2%)] Loss: 8078.285156\n",
      "Train Epoch: 155 [8448/225000 (4%)] Loss: 9440.398438\n",
      "Train Epoch: 155 [12544/225000 (6%)] Loss: 9642.976562\n",
      "Train Epoch: 155 [16640/225000 (7%)] Loss: 10442.738281\n",
      "Train Epoch: 155 [20736/225000 (9%)] Loss: 8012.292969\n",
      "Train Epoch: 155 [24832/225000 (11%)] Loss: 8140.167969\n",
      "Train Epoch: 155 [28928/225000 (13%)] Loss: 8040.835938\n",
      "Train Epoch: 155 [33024/225000 (15%)] Loss: 8088.826172\n",
      "Train Epoch: 155 [37120/225000 (16%)] Loss: 7839.015625\n",
      "Train Epoch: 155 [41216/225000 (18%)] Loss: 8038.087891\n",
      "Train Epoch: 155 [45312/225000 (20%)] Loss: 8126.751953\n",
      "Train Epoch: 155 [49408/225000 (22%)] Loss: 8042.408203\n",
      "Train Epoch: 155 [53504/225000 (24%)] Loss: 8120.333984\n",
      "Train Epoch: 155 [57600/225000 (26%)] Loss: 8123.699219\n",
      "Train Epoch: 155 [61696/225000 (27%)] Loss: 13824.515625\n",
      "Train Epoch: 155 [65792/225000 (29%)] Loss: 9943.724609\n",
      "Train Epoch: 155 [69888/225000 (31%)] Loss: 11332.587891\n",
      "Train Epoch: 155 [73984/225000 (33%)] Loss: 8234.330078\n",
      "Train Epoch: 155 [78080/225000 (35%)] Loss: 13777.583984\n",
      "Train Epoch: 155 [82176/225000 (37%)] Loss: 8129.638672\n",
      "Train Epoch: 155 [86272/225000 (38%)] Loss: 8209.263672\n",
      "Train Epoch: 155 [90368/225000 (40%)] Loss: 14079.402344\n",
      "Train Epoch: 155 [94464/225000 (42%)] Loss: 8077.062500\n",
      "Train Epoch: 155 [98560/225000 (44%)] Loss: 9677.507812\n",
      "Train Epoch: 155 [102656/225000 (46%)] Loss: 7927.125000\n",
      "Train Epoch: 155 [106752/225000 (47%)] Loss: 9691.167969\n",
      "Train Epoch: 155 [110848/225000 (49%)] Loss: 9681.792969\n",
      "Train Epoch: 155 [114944/225000 (51%)] Loss: 7986.751953\n",
      "Train Epoch: 155 [119040/225000 (53%)] Loss: 10611.990234\n",
      "Train Epoch: 155 [123136/225000 (55%)] Loss: 8128.994141\n",
      "Train Epoch: 155 [127232/225000 (57%)] Loss: 10602.298828\n",
      "Train Epoch: 155 [131328/225000 (58%)] Loss: 7940.056641\n",
      "Train Epoch: 155 [135424/225000 (60%)] Loss: 12295.619141\n",
      "Train Epoch: 155 [139520/225000 (62%)] Loss: 7834.826172\n",
      "Train Epoch: 155 [143616/225000 (64%)] Loss: 12534.949219\n",
      "Train Epoch: 155 [147712/225000 (66%)] Loss: 8093.652344\n",
      "Train Epoch: 155 [151808/225000 (67%)] Loss: 8062.134766\n",
      "Train Epoch: 155 [155904/225000 (69%)] Loss: 10222.488281\n",
      "Train Epoch: 155 [160000/225000 (71%)] Loss: 8193.853516\n",
      "Train Epoch: 155 [164096/225000 (73%)] Loss: 8082.521484\n",
      "Train Epoch: 155 [168192/225000 (75%)] Loss: 8174.062500\n",
      "Train Epoch: 155 [172288/225000 (77%)] Loss: 9728.068359\n",
      "Train Epoch: 155 [176384/225000 (78%)] Loss: 12191.955078\n",
      "Train Epoch: 155 [180480/225000 (80%)] Loss: 7872.156250\n",
      "Train Epoch: 155 [184576/225000 (82%)] Loss: 8237.841797\n",
      "Train Epoch: 155 [188672/225000 (84%)] Loss: 8195.208984\n",
      "Train Epoch: 155 [192768/225000 (86%)] Loss: 9483.480469\n",
      "Train Epoch: 155 [196864/225000 (87%)] Loss: 7883.158203\n",
      "Train Epoch: 155 [200960/225000 (89%)] Loss: 7995.728516\n",
      "Train Epoch: 155 [205056/225000 (91%)] Loss: 13816.525391\n",
      "Train Epoch: 155 [209152/225000 (93%)] Loss: 8022.369141\n",
      "Train Epoch: 155 [213248/225000 (95%)] Loss: 7935.708984\n",
      "Train Epoch: 155 [217344/225000 (97%)] Loss: 8144.406250\n",
      "Train Epoch: 155 [221440/225000 (98%)] Loss: 8295.857422\n",
      "    epoch          : 155\n",
      "    loss           : 9562.395783338667\n",
      "    val_loss       : 9854.227640587456\n",
      "Train Epoch: 156 [256/225000 (0%)] Loss: 8077.617188\n",
      "Train Epoch: 156 [4352/225000 (2%)] Loss: 13837.091797\n",
      "Train Epoch: 156 [8448/225000 (4%)] Loss: 13829.970703\n",
      "Train Epoch: 156 [12544/225000 (6%)] Loss: 10584.083984\n",
      "Train Epoch: 156 [16640/225000 (7%)] Loss: 13236.421875\n",
      "Train Epoch: 156 [20736/225000 (9%)] Loss: 8245.277344\n",
      "Train Epoch: 156 [24832/225000 (11%)] Loss: 13599.439453\n",
      "Train Epoch: 156 [28928/225000 (13%)] Loss: 13633.919922\n",
      "Train Epoch: 156 [33024/225000 (15%)] Loss: 8119.988281\n",
      "Train Epoch: 156 [37120/225000 (16%)] Loss: 8185.839844\n",
      "Train Epoch: 156 [41216/225000 (18%)] Loss: 8204.414062\n",
      "Train Epoch: 156 [45312/225000 (20%)] Loss: 8274.839844\n",
      "Train Epoch: 156 [49408/225000 (22%)] Loss: 8103.861328\n",
      "Train Epoch: 156 [53504/225000 (24%)] Loss: 12257.585938\n",
      "Train Epoch: 156 [57600/225000 (26%)] Loss: 8052.187500\n",
      "Train Epoch: 156 [61696/225000 (27%)] Loss: 13618.216797\n",
      "Train Epoch: 156 [65792/225000 (29%)] Loss: 8106.691406\n",
      "Train Epoch: 156 [69888/225000 (31%)] Loss: 10739.105469\n",
      "Train Epoch: 156 [73984/225000 (33%)] Loss: 9514.236328\n",
      "Train Epoch: 156 [78080/225000 (35%)] Loss: 7988.589844\n",
      "Train Epoch: 156 [82176/225000 (37%)] Loss: 12337.638672\n",
      "Train Epoch: 156 [86272/225000 (38%)] Loss: 8072.658203\n",
      "Train Epoch: 156 [90368/225000 (40%)] Loss: 12347.914062\n",
      "Train Epoch: 156 [94464/225000 (42%)] Loss: 7987.908203\n",
      "Train Epoch: 156 [98560/225000 (44%)] Loss: 8134.117188\n",
      "Train Epoch: 156 [102656/225000 (46%)] Loss: 8297.193359\n",
      "Train Epoch: 156 [106752/225000 (47%)] Loss: 15554.402344\n",
      "Train Epoch: 156 [110848/225000 (49%)] Loss: 8153.363281\n",
      "Train Epoch: 156 [114944/225000 (51%)] Loss: 7939.255859\n",
      "Train Epoch: 156 [119040/225000 (53%)] Loss: 8163.695312\n",
      "Train Epoch: 156 [123136/225000 (55%)] Loss: 8145.492188\n",
      "Train Epoch: 156 [127232/225000 (57%)] Loss: 8036.136719\n",
      "Train Epoch: 156 [131328/225000 (58%)] Loss: 8105.505859\n",
      "Train Epoch: 156 [135424/225000 (60%)] Loss: 8063.484375\n",
      "Train Epoch: 156 [139520/225000 (62%)] Loss: 8158.431641\n",
      "Train Epoch: 156 [143616/225000 (64%)] Loss: 8174.722656\n",
      "Train Epoch: 156 [147712/225000 (66%)] Loss: 7996.439453\n",
      "Train Epoch: 156 [151808/225000 (67%)] Loss: 8137.921875\n",
      "Train Epoch: 156 [155904/225000 (69%)] Loss: 8285.560547\n",
      "Train Epoch: 156 [160000/225000 (71%)] Loss: 8173.554688\n",
      "Train Epoch: 156 [164096/225000 (73%)] Loss: 9371.638672\n",
      "Train Epoch: 156 [168192/225000 (75%)] Loss: 9649.710938\n",
      "Train Epoch: 156 [172288/225000 (77%)] Loss: 9732.375000\n",
      "Train Epoch: 156 [176384/225000 (78%)] Loss: 8119.455078\n",
      "Train Epoch: 156 [180480/225000 (80%)] Loss: 14325.486328\n",
      "Train Epoch: 156 [184576/225000 (82%)] Loss: 12836.714844\n",
      "Train Epoch: 156 [188672/225000 (84%)] Loss: 7945.919922\n",
      "Train Epoch: 156 [192768/225000 (86%)] Loss: 10602.476562\n",
      "Train Epoch: 156 [196864/225000 (87%)] Loss: 10413.484375\n",
      "Train Epoch: 156 [200960/225000 (89%)] Loss: 7985.277344\n",
      "Train Epoch: 156 [205056/225000 (91%)] Loss: 7941.496094\n",
      "Train Epoch: 156 [209152/225000 (93%)] Loss: 8160.191406\n",
      "Train Epoch: 156 [213248/225000 (95%)] Loss: 8151.769531\n",
      "Train Epoch: 156 [217344/225000 (97%)] Loss: 12446.861328\n",
      "Train Epoch: 156 [221440/225000 (98%)] Loss: 10522.244141\n",
      "    epoch          : 156\n",
      "    loss           : 9571.912493778442\n",
      "    val_loss       : 9503.103453830798\n",
      "Train Epoch: 157 [256/225000 (0%)] Loss: 10396.587891\n",
      "Train Epoch: 157 [4352/225000 (2%)] Loss: 9400.859375\n",
      "Train Epoch: 157 [8448/225000 (4%)] Loss: 8153.480469\n",
      "Train Epoch: 157 [12544/225000 (6%)] Loss: 8123.001953\n",
      "Train Epoch: 157 [16640/225000 (7%)] Loss: 8156.345703\n",
      "Train Epoch: 157 [20736/225000 (9%)] Loss: 7907.660156\n",
      "Train Epoch: 157 [24832/225000 (11%)] Loss: 9597.615234\n",
      "Train Epoch: 157 [28928/225000 (13%)] Loss: 9601.701172\n",
      "Train Epoch: 157 [33024/225000 (15%)] Loss: 13909.394531\n",
      "Train Epoch: 157 [37120/225000 (16%)] Loss: 9728.845703\n",
      "Train Epoch: 157 [41216/225000 (18%)] Loss: 15366.193359\n",
      "Train Epoch: 157 [45312/225000 (20%)] Loss: 8066.650391\n",
      "Train Epoch: 157 [49408/225000 (22%)] Loss: 13944.785156\n",
      "Train Epoch: 157 [53504/225000 (24%)] Loss: 12274.943359\n",
      "Train Epoch: 157 [57600/225000 (26%)] Loss: 8034.140625\n",
      "Train Epoch: 157 [61696/225000 (27%)] Loss: 8109.544922\n",
      "Train Epoch: 157 [65792/225000 (29%)] Loss: 7924.439453\n",
      "Train Epoch: 157 [69888/225000 (31%)] Loss: 8280.650391\n",
      "Train Epoch: 157 [73984/225000 (33%)] Loss: 9889.841797\n",
      "Train Epoch: 157 [78080/225000 (35%)] Loss: 7882.000000\n",
      "Train Epoch: 157 [82176/225000 (37%)] Loss: 9680.496094\n",
      "Train Epoch: 157 [86272/225000 (38%)] Loss: 8228.804688\n",
      "Train Epoch: 157 [90368/225000 (40%)] Loss: 8102.041016\n",
      "Train Epoch: 157 [94464/225000 (42%)] Loss: 7958.925781\n",
      "Train Epoch: 157 [98560/225000 (44%)] Loss: 13750.607422\n",
      "Train Epoch: 157 [102656/225000 (46%)] Loss: 8078.449219\n",
      "Train Epoch: 157 [106752/225000 (47%)] Loss: 9453.277344\n",
      "Train Epoch: 157 [110848/225000 (49%)] Loss: 8256.154297\n",
      "Train Epoch: 157 [114944/225000 (51%)] Loss: 9686.884766\n",
      "Train Epoch: 157 [119040/225000 (53%)] Loss: 7935.552734\n",
      "Train Epoch: 157 [123136/225000 (55%)] Loss: 8090.667969\n",
      "Train Epoch: 157 [127232/225000 (57%)] Loss: 11949.445312\n",
      "Train Epoch: 157 [131328/225000 (58%)] Loss: 8149.935547\n",
      "Train Epoch: 157 [135424/225000 (60%)] Loss: 8174.443359\n",
      "Train Epoch: 157 [139520/225000 (62%)] Loss: 13714.166016\n",
      "Train Epoch: 157 [143616/225000 (64%)] Loss: 8103.955078\n",
      "Train Epoch: 157 [147712/225000 (66%)] Loss: 12393.511719\n",
      "Train Epoch: 157 [151808/225000 (67%)] Loss: 10679.962891\n",
      "Train Epoch: 157 [155904/225000 (69%)] Loss: 12800.404297\n",
      "Train Epoch: 157 [160000/225000 (71%)] Loss: 8024.125000\n",
      "Train Epoch: 157 [164096/225000 (73%)] Loss: 9509.056641\n",
      "Train Epoch: 157 [168192/225000 (75%)] Loss: 9632.359375\n",
      "Train Epoch: 157 [172288/225000 (77%)] Loss: 9670.820312\n",
      "Train Epoch: 157 [176384/225000 (78%)] Loss: 12406.212891\n",
      "Train Epoch: 157 [180480/225000 (80%)] Loss: 7973.769531\n",
      "Train Epoch: 157 [184576/225000 (82%)] Loss: 7926.251953\n",
      "Train Epoch: 157 [188672/225000 (84%)] Loss: 8022.226562\n",
      "Train Epoch: 157 [192768/225000 (86%)] Loss: 10566.779297\n",
      "Train Epoch: 157 [196864/225000 (87%)] Loss: 8051.384766\n",
      "Train Epoch: 157 [200960/225000 (89%)] Loss: 7970.226562\n",
      "Train Epoch: 157 [205056/225000 (91%)] Loss: 12113.529297\n",
      "Train Epoch: 157 [209152/225000 (93%)] Loss: 7867.980469\n",
      "Train Epoch: 157 [213248/225000 (95%)] Loss: 12981.730469\n",
      "Train Epoch: 157 [217344/225000 (97%)] Loss: 7973.751953\n",
      "Train Epoch: 157 [221440/225000 (98%)] Loss: 13869.080078\n",
      "    epoch          : 157\n",
      "    loss           : 9436.098430611846\n",
      "    val_loss       : 9612.370551469374\n",
      "Train Epoch: 158 [256/225000 (0%)] Loss: 8176.253906\n",
      "Train Epoch: 158 [4352/225000 (2%)] Loss: 7895.742188\n",
      "Train Epoch: 158 [8448/225000 (4%)] Loss: 9823.515625\n",
      "Train Epoch: 158 [12544/225000 (6%)] Loss: 7958.279297\n",
      "Train Epoch: 158 [16640/225000 (7%)] Loss: 8127.476562\n",
      "Train Epoch: 158 [20736/225000 (9%)] Loss: 7907.582031\n",
      "Train Epoch: 158 [24832/225000 (11%)] Loss: 8299.632812\n",
      "Train Epoch: 158 [28928/225000 (13%)] Loss: 8134.792969\n",
      "Train Epoch: 158 [33024/225000 (15%)] Loss: 8071.503906\n",
      "Train Epoch: 158 [37120/225000 (16%)] Loss: 8028.816406\n",
      "Train Epoch: 158 [41216/225000 (18%)] Loss: 8024.207031\n",
      "Train Epoch: 158 [45312/225000 (20%)] Loss: 8276.828125\n",
      "Train Epoch: 158 [49408/225000 (22%)] Loss: 8128.283203\n",
      "Train Epoch: 158 [53504/225000 (24%)] Loss: 7952.986328\n",
      "Train Epoch: 158 [57600/225000 (26%)] Loss: 8105.644531\n",
      "Train Epoch: 158 [61696/225000 (27%)] Loss: 7950.087891\n",
      "Train Epoch: 158 [65792/225000 (29%)] Loss: 7969.068359\n",
      "Train Epoch: 158 [69888/225000 (31%)] Loss: 7996.984375\n",
      "Train Epoch: 158 [73984/225000 (33%)] Loss: 11458.451172\n",
      "Train Epoch: 158 [78080/225000 (35%)] Loss: 8105.402344\n",
      "Train Epoch: 158 [82176/225000 (37%)] Loss: 7995.890625\n",
      "Train Epoch: 158 [86272/225000 (38%)] Loss: 12482.669922\n",
      "Train Epoch: 158 [90368/225000 (40%)] Loss: 9518.859375\n",
      "Train Epoch: 158 [94464/225000 (42%)] Loss: 9838.621094\n",
      "Train Epoch: 158 [98560/225000 (44%)] Loss: 7875.832031\n",
      "Train Epoch: 158 [102656/225000 (46%)] Loss: 8068.353516\n",
      "Train Epoch: 158 [106752/225000 (47%)] Loss: 9450.853516\n",
      "Train Epoch: 158 [110848/225000 (49%)] Loss: 9508.835938\n",
      "Train Epoch: 158 [114944/225000 (51%)] Loss: 9661.937500\n",
      "Train Epoch: 158 [119040/225000 (53%)] Loss: 7931.167969\n",
      "Train Epoch: 158 [123136/225000 (55%)] Loss: 9716.628906\n",
      "Train Epoch: 158 [127232/225000 (57%)] Loss: 10643.447266\n",
      "Train Epoch: 158 [131328/225000 (58%)] Loss: 10291.662109\n",
      "Train Epoch: 158 [135424/225000 (60%)] Loss: 8077.359375\n",
      "Train Epoch: 158 [139520/225000 (62%)] Loss: 15183.765625\n",
      "Train Epoch: 158 [143616/225000 (64%)] Loss: 8244.564453\n",
      "Train Epoch: 158 [147712/225000 (66%)] Loss: 7821.281250\n",
      "Train Epoch: 158 [151808/225000 (67%)] Loss: 9462.589844\n",
      "Train Epoch: 158 [155904/225000 (69%)] Loss: 10468.228516\n",
      "Train Epoch: 158 [160000/225000 (71%)] Loss: 7847.414062\n",
      "Train Epoch: 158 [164096/225000 (73%)] Loss: 12821.646484\n",
      "Train Epoch: 158 [168192/225000 (75%)] Loss: 8000.253906\n",
      "Train Epoch: 158 [172288/225000 (77%)] Loss: 8162.007812\n",
      "Train Epoch: 158 [176384/225000 (78%)] Loss: 9553.964844\n",
      "Train Epoch: 158 [180480/225000 (80%)] Loss: 8010.339844\n",
      "Train Epoch: 158 [184576/225000 (82%)] Loss: 8124.794922\n",
      "Train Epoch: 158 [188672/225000 (84%)] Loss: 7884.371094\n",
      "Train Epoch: 158 [192768/225000 (86%)] Loss: 8178.931641\n",
      "Train Epoch: 158 [196864/225000 (87%)] Loss: 8067.263672\n",
      "Train Epoch: 158 [200960/225000 (89%)] Loss: 7903.718750\n",
      "Train Epoch: 158 [205056/225000 (91%)] Loss: 8036.453125\n",
      "Train Epoch: 158 [209152/225000 (93%)] Loss: 7951.009766\n",
      "Train Epoch: 158 [213248/225000 (95%)] Loss: 8016.320312\n",
      "Train Epoch: 158 [217344/225000 (97%)] Loss: 12061.304688\n",
      "Train Epoch: 158 [221440/225000 (98%)] Loss: 8080.113281\n",
      "    epoch          : 158\n",
      "    loss           : 9329.56524192975\n",
      "    val_loss       : 9009.224812385597\n",
      "Train Epoch: 159 [256/225000 (0%)] Loss: 9781.857422\n",
      "Train Epoch: 159 [4352/225000 (2%)] Loss: 13706.878906\n",
      "Train Epoch: 159 [8448/225000 (4%)] Loss: 10579.519531\n",
      "Train Epoch: 159 [12544/225000 (6%)] Loss: 15460.832031\n",
      "Train Epoch: 159 [16640/225000 (7%)] Loss: 13976.162109\n",
      "Train Epoch: 159 [20736/225000 (9%)] Loss: 11213.468750\n",
      "Train Epoch: 159 [24832/225000 (11%)] Loss: 9589.751953\n",
      "Train Epoch: 159 [28928/225000 (13%)] Loss: 7909.783203\n",
      "Train Epoch: 159 [33024/225000 (15%)] Loss: 12384.638672\n",
      "Train Epoch: 159 [37120/225000 (16%)] Loss: 8123.798828\n",
      "Train Epoch: 159 [41216/225000 (18%)] Loss: 8150.929688\n",
      "Train Epoch: 159 [45312/225000 (20%)] Loss: 9684.681641\n",
      "Train Epoch: 159 [49408/225000 (22%)] Loss: 8170.068359\n",
      "Train Epoch: 159 [53504/225000 (24%)] Loss: 8197.593750\n",
      "Train Epoch: 159 [57600/225000 (26%)] Loss: 9570.609375\n",
      "Train Epoch: 159 [61696/225000 (27%)] Loss: 10508.611328\n",
      "Train Epoch: 159 [65792/225000 (29%)] Loss: 12428.130859\n",
      "Train Epoch: 159 [69888/225000 (31%)] Loss: 9684.025391\n",
      "Train Epoch: 159 [73984/225000 (33%)] Loss: 8081.894531\n",
      "Train Epoch: 159 [78080/225000 (35%)] Loss: 13923.068359\n",
      "Train Epoch: 159 [82176/225000 (37%)] Loss: 13425.845703\n",
      "Train Epoch: 159 [86272/225000 (38%)] Loss: 12487.337891\n",
      "Train Epoch: 159 [90368/225000 (40%)] Loss: 13805.949219\n",
      "Train Epoch: 159 [94464/225000 (42%)] Loss: 8112.531250\n",
      "Train Epoch: 159 [98560/225000 (44%)] Loss: 7932.210938\n",
      "Train Epoch: 159 [102656/225000 (46%)] Loss: 12437.027344\n",
      "Train Epoch: 159 [106752/225000 (47%)] Loss: 7925.128906\n",
      "Train Epoch: 159 [110848/225000 (49%)] Loss: 10579.953125\n",
      "Train Epoch: 159 [114944/225000 (51%)] Loss: 10549.589844\n",
      "Train Epoch: 159 [119040/225000 (53%)] Loss: 7920.349609\n",
      "Train Epoch: 159 [123136/225000 (55%)] Loss: 9692.587891\n",
      "Train Epoch: 159 [127232/225000 (57%)] Loss: 8142.259766\n",
      "Train Epoch: 159 [131328/225000 (58%)] Loss: 8040.402344\n",
      "Train Epoch: 159 [135424/225000 (60%)] Loss: 8131.214844\n",
      "Train Epoch: 159 [139520/225000 (62%)] Loss: 10440.978516\n",
      "Train Epoch: 159 [143616/225000 (64%)] Loss: 10023.291016\n",
      "Train Epoch: 159 [147712/225000 (66%)] Loss: 18775.351562\n",
      "Train Epoch: 159 [151808/225000 (67%)] Loss: 8372.287109\n",
      "Train Epoch: 159 [155904/225000 (69%)] Loss: 7945.333984\n",
      "Train Epoch: 159 [160000/225000 (71%)] Loss: 7933.423828\n",
      "Train Epoch: 159 [164096/225000 (73%)] Loss: 8095.923828\n",
      "Train Epoch: 159 [168192/225000 (75%)] Loss: 7979.835938\n",
      "Train Epoch: 159 [172288/225000 (77%)] Loss: 10437.453125\n",
      "Train Epoch: 159 [176384/225000 (78%)] Loss: 8066.927734\n",
      "Train Epoch: 159 [180480/225000 (80%)] Loss: 8346.205078\n",
      "Train Epoch: 159 [184576/225000 (82%)] Loss: 8030.714844\n",
      "Train Epoch: 159 [188672/225000 (84%)] Loss: 7875.869141\n",
      "Train Epoch: 159 [192768/225000 (86%)] Loss: 9382.970703\n",
      "Train Epoch: 159 [196864/225000 (87%)] Loss: 9538.714844\n",
      "Train Epoch: 159 [200960/225000 (89%)] Loss: 10413.863281\n",
      "Train Epoch: 159 [205056/225000 (91%)] Loss: 7984.240234\n",
      "Train Epoch: 159 [209152/225000 (93%)] Loss: 24209.781250\n",
      "Train Epoch: 159 [213248/225000 (95%)] Loss: 8103.384766\n",
      "Train Epoch: 159 [217344/225000 (97%)] Loss: 8016.175781\n",
      "Train Epoch: 159 [221440/225000 (98%)] Loss: 8086.765625\n",
      "    epoch          : 159\n",
      "    loss           : 9483.854898810794\n",
      "    val_loss       : 9538.165280652289\n",
      "Train Epoch: 160 [256/225000 (0%)] Loss: 8126.697266\n",
      "Train Epoch: 160 [4352/225000 (2%)] Loss: 13938.398438\n",
      "Train Epoch: 160 [8448/225000 (4%)] Loss: 9724.904297\n",
      "Train Epoch: 160 [12544/225000 (6%)] Loss: 7906.716797\n",
      "Train Epoch: 160 [16640/225000 (7%)] Loss: 10558.919922\n",
      "Train Epoch: 160 [20736/225000 (9%)] Loss: 10641.873047\n",
      "Train Epoch: 160 [24832/225000 (11%)] Loss: 9819.626953\n",
      "Train Epoch: 160 [28928/225000 (13%)] Loss: 13625.152344\n",
      "Train Epoch: 160 [33024/225000 (15%)] Loss: 9631.058594\n",
      "Train Epoch: 160 [37120/225000 (16%)] Loss: 8010.902344\n",
      "Train Epoch: 160 [41216/225000 (18%)] Loss: 8044.851562\n",
      "Train Epoch: 160 [45312/225000 (20%)] Loss: 7987.449219\n",
      "Train Epoch: 160 [49408/225000 (22%)] Loss: 8114.230469\n",
      "Train Epoch: 160 [53504/225000 (24%)] Loss: 8051.498047\n",
      "Train Epoch: 160 [57600/225000 (26%)] Loss: 9473.662109\n",
      "Train Epoch: 160 [61696/225000 (27%)] Loss: 7978.314453\n",
      "Train Epoch: 160 [65792/225000 (29%)] Loss: 10659.171875\n",
      "Train Epoch: 160 [69888/225000 (31%)] Loss: 8084.208984\n",
      "Train Epoch: 160 [73984/225000 (33%)] Loss: 8210.599609\n",
      "Train Epoch: 160 [78080/225000 (35%)] Loss: 13970.433594\n",
      "Train Epoch: 160 [82176/225000 (37%)] Loss: 12922.517578\n",
      "Train Epoch: 160 [86272/225000 (38%)] Loss: 7974.652344\n",
      "Train Epoch: 160 [90368/225000 (40%)] Loss: 7974.873047\n",
      "Train Epoch: 160 [94464/225000 (42%)] Loss: 11233.677734\n",
      "Train Epoch: 160 [98560/225000 (44%)] Loss: 8087.029297\n",
      "Train Epoch: 160 [102656/225000 (46%)] Loss: 10448.783203\n",
      "Train Epoch: 160 [106752/225000 (47%)] Loss: 8022.603516\n",
      "Train Epoch: 160 [110848/225000 (49%)] Loss: 8176.019531\n",
      "Train Epoch: 160 [114944/225000 (51%)] Loss: 10793.009766\n",
      "Train Epoch: 160 [119040/225000 (53%)] Loss: 7935.074219\n",
      "Train Epoch: 160 [123136/225000 (55%)] Loss: 7861.501953\n",
      "Train Epoch: 160 [127232/225000 (57%)] Loss: 8107.589844\n",
      "Train Epoch: 160 [131328/225000 (58%)] Loss: 10659.755859\n",
      "Train Epoch: 160 [135424/225000 (60%)] Loss: 7969.484375\n",
      "Train Epoch: 160 [139520/225000 (62%)] Loss: 8243.123047\n",
      "Train Epoch: 160 [143616/225000 (64%)] Loss: 9688.638672\n",
      "Train Epoch: 160 [147712/225000 (66%)] Loss: 8206.718750\n",
      "Train Epoch: 160 [151808/225000 (67%)] Loss: 8029.845703\n",
      "Train Epoch: 160 [155904/225000 (69%)] Loss: 8169.525391\n",
      "Train Epoch: 160 [160000/225000 (71%)] Loss: 8134.617188\n",
      "Train Epoch: 160 [164096/225000 (73%)] Loss: 8143.363281\n",
      "Train Epoch: 160 [168192/225000 (75%)] Loss: 9763.238281\n",
      "Train Epoch: 160 [172288/225000 (77%)] Loss: 10574.425781\n",
      "Train Epoch: 160 [176384/225000 (78%)] Loss: 8100.984375\n",
      "Train Epoch: 160 [180480/225000 (80%)] Loss: 10651.048828\n",
      "Train Epoch: 160 [184576/225000 (82%)] Loss: 7993.220703\n",
      "Train Epoch: 160 [188672/225000 (84%)] Loss: 8026.986328\n",
      "Train Epoch: 160 [192768/225000 (86%)] Loss: 8115.380859\n",
      "Train Epoch: 160 [196864/225000 (87%)] Loss: 8033.007812\n",
      "Train Epoch: 160 [200960/225000 (89%)] Loss: 7959.150391\n",
      "Train Epoch: 160 [205056/225000 (91%)] Loss: 8156.166016\n",
      "Train Epoch: 160 [209152/225000 (93%)] Loss: 7916.396484\n",
      "Train Epoch: 160 [213248/225000 (95%)] Loss: 10624.257812\n",
      "Train Epoch: 160 [217344/225000 (97%)] Loss: 12466.390625\n",
      "Train Epoch: 160 [221440/225000 (98%)] Loss: 7910.494141\n",
      "    epoch          : 160\n",
      "    loss           : 9678.518555798493\n",
      "    val_loss       : 9454.632543091871\n",
      "Train Epoch: 161 [256/225000 (0%)] Loss: 9758.945312\n",
      "Train Epoch: 161 [4352/225000 (2%)] Loss: 7924.625000\n",
      "Train Epoch: 161 [8448/225000 (4%)] Loss: 8111.314453\n",
      "Train Epoch: 161 [12544/225000 (6%)] Loss: 9645.324219\n",
      "Train Epoch: 161 [16640/225000 (7%)] Loss: 8083.681641\n",
      "Train Epoch: 161 [20736/225000 (9%)] Loss: 10367.675781\n",
      "Train Epoch: 161 [24832/225000 (11%)] Loss: 8040.107422\n",
      "Train Epoch: 161 [28928/225000 (13%)] Loss: 8041.919922\n",
      "Train Epoch: 161 [33024/225000 (15%)] Loss: 9628.792969\n",
      "Train Epoch: 161 [37120/225000 (16%)] Loss: 10609.779297\n",
      "Train Epoch: 161 [41216/225000 (18%)] Loss: 7904.152344\n",
      "Train Epoch: 161 [45312/225000 (20%)] Loss: 13974.388672\n",
      "Train Epoch: 161 [49408/225000 (22%)] Loss: 9492.722656\n",
      "Train Epoch: 161 [53504/225000 (24%)] Loss: 9565.988281\n",
      "Train Epoch: 161 [57600/225000 (26%)] Loss: 8081.089844\n",
      "Train Epoch: 161 [61696/225000 (27%)] Loss: 10480.429688\n",
      "Train Epoch: 161 [65792/225000 (29%)] Loss: 9618.187500\n",
      "Train Epoch: 161 [69888/225000 (31%)] Loss: 16190.978516\n",
      "Train Epoch: 161 [73984/225000 (33%)] Loss: 8129.367188\n",
      "Train Epoch: 161 [78080/225000 (35%)] Loss: 14708.619141\n",
      "Train Epoch: 161 [82176/225000 (37%)] Loss: 8241.128906\n",
      "Train Epoch: 161 [86272/225000 (38%)] Loss: 8008.826172\n",
      "Train Epoch: 161 [90368/225000 (40%)] Loss: 8229.427734\n",
      "Train Epoch: 161 [94464/225000 (42%)] Loss: 7916.802734\n",
      "Train Epoch: 161 [98560/225000 (44%)] Loss: 8029.693359\n",
      "Train Epoch: 161 [102656/225000 (46%)] Loss: 8067.509766\n",
      "Train Epoch: 161 [106752/225000 (47%)] Loss: 9731.871094\n",
      "Train Epoch: 161 [110848/225000 (49%)] Loss: 8088.482422\n",
      "Train Epoch: 161 [114944/225000 (51%)] Loss: 8002.445312\n",
      "Train Epoch: 161 [119040/225000 (53%)] Loss: 8218.017578\n",
      "Train Epoch: 161 [123136/225000 (55%)] Loss: 7988.179688\n",
      "Train Epoch: 161 [127232/225000 (57%)] Loss: 8015.859375\n",
      "Train Epoch: 161 [131328/225000 (58%)] Loss: 8177.652344\n",
      "Train Epoch: 161 [135424/225000 (60%)] Loss: 9707.205078\n",
      "Train Epoch: 161 [139520/225000 (62%)] Loss: 8114.517578\n",
      "Train Epoch: 161 [143616/225000 (64%)] Loss: 8014.542969\n",
      "Train Epoch: 161 [147712/225000 (66%)] Loss: 8255.482422\n",
      "Train Epoch: 161 [151808/225000 (67%)] Loss: 9546.419922\n",
      "Train Epoch: 161 [155904/225000 (69%)] Loss: 7969.007812\n",
      "Train Epoch: 161 [160000/225000 (71%)] Loss: 8156.972656\n",
      "Train Epoch: 161 [164096/225000 (73%)] Loss: 8257.937500\n",
      "Train Epoch: 161 [168192/225000 (75%)] Loss: 7905.275391\n",
      "Train Epoch: 161 [172288/225000 (77%)] Loss: 8236.490234\n",
      "Train Epoch: 161 [176384/225000 (78%)] Loss: 8261.933594\n",
      "Train Epoch: 161 [180480/225000 (80%)] Loss: 8117.804688\n",
      "Train Epoch: 161 [184576/225000 (82%)] Loss: 10600.693359\n",
      "Train Epoch: 161 [188672/225000 (84%)] Loss: 8014.083984\n",
      "Train Epoch: 161 [192768/225000 (86%)] Loss: 13971.367188\n",
      "Train Epoch: 161 [196864/225000 (87%)] Loss: 10706.203125\n",
      "Train Epoch: 161 [200960/225000 (89%)] Loss: 8221.095703\n",
      "Train Epoch: 161 [205056/225000 (91%)] Loss: 9488.664062\n",
      "Train Epoch: 161 [209152/225000 (93%)] Loss: 9672.882812\n",
      "Train Epoch: 161 [213248/225000 (95%)] Loss: 13120.257812\n",
      "Train Epoch: 161 [217344/225000 (97%)] Loss: 9864.515625\n",
      "Train Epoch: 161 [221440/225000 (98%)] Loss: 9654.732422\n",
      "    epoch          : 161\n",
      "    loss           : 9480.691579564846\n",
      "    val_loss       : 9463.022620274096\n",
      "Train Epoch: 162 [256/225000 (0%)] Loss: 8115.798828\n",
      "Train Epoch: 162 [4352/225000 (2%)] Loss: 8236.900391\n",
      "Train Epoch: 162 [8448/225000 (4%)] Loss: 8172.441406\n",
      "Train Epoch: 162 [12544/225000 (6%)] Loss: 8152.775391\n",
      "Train Epoch: 162 [16640/225000 (7%)] Loss: 8194.140625\n",
      "Train Epoch: 162 [20736/225000 (9%)] Loss: 7940.380859\n",
      "Train Epoch: 162 [24832/225000 (11%)] Loss: 8085.164062\n",
      "Train Epoch: 162 [28928/225000 (13%)] Loss: 8131.414062\n",
      "Train Epoch: 162 [33024/225000 (15%)] Loss: 8023.066406\n",
      "Train Epoch: 162 [37120/225000 (16%)] Loss: 12186.867188\n",
      "Train Epoch: 162 [41216/225000 (18%)] Loss: 9622.494141\n",
      "Train Epoch: 162 [45312/225000 (20%)] Loss: 9852.851562\n",
      "Train Epoch: 162 [49408/225000 (22%)] Loss: 8163.792969\n",
      "Train Epoch: 162 [53504/225000 (24%)] Loss: 9658.576172\n",
      "Train Epoch: 162 [57600/225000 (26%)] Loss: 11226.218750\n",
      "Train Epoch: 162 [61696/225000 (27%)] Loss: 9607.658203\n",
      "Train Epoch: 162 [65792/225000 (29%)] Loss: 8164.718750\n",
      "Train Epoch: 162 [69888/225000 (31%)] Loss: 8194.146484\n",
      "Train Epoch: 162 [73984/225000 (33%)] Loss: 9588.845703\n",
      "Train Epoch: 162 [78080/225000 (35%)] Loss: 8226.089844\n",
      "Train Epoch: 162 [82176/225000 (37%)] Loss: 8065.025391\n",
      "Train Epoch: 162 [86272/225000 (38%)] Loss: 12919.564453\n",
      "Train Epoch: 162 [90368/225000 (40%)] Loss: 14038.775391\n",
      "Train Epoch: 162 [94464/225000 (42%)] Loss: 10764.224609\n",
      "Train Epoch: 162 [98560/225000 (44%)] Loss: 8249.314453\n",
      "Train Epoch: 162 [102656/225000 (46%)] Loss: 13436.746094\n",
      "Train Epoch: 162 [106752/225000 (47%)] Loss: 7879.498047\n",
      "Train Epoch: 162 [110848/225000 (49%)] Loss: 10358.816406\n",
      "Train Epoch: 162 [114944/225000 (51%)] Loss: 8176.048828\n",
      "Train Epoch: 162 [119040/225000 (53%)] Loss: 12476.650391\n",
      "Train Epoch: 162 [123136/225000 (55%)] Loss: 9660.048828\n",
      "Train Epoch: 162 [127232/225000 (57%)] Loss: 8025.771484\n",
      "Train Epoch: 162 [131328/225000 (58%)] Loss: 7979.652344\n",
      "Train Epoch: 162 [135424/225000 (60%)] Loss: 8319.646484\n",
      "Train Epoch: 162 [139520/225000 (62%)] Loss: 8118.921875\n",
      "Train Epoch: 162 [143616/225000 (64%)] Loss: 8106.929688\n",
      "Train Epoch: 162 [147712/225000 (66%)] Loss: 10640.271484\n",
      "Train Epoch: 162 [151808/225000 (67%)] Loss: 8157.587891\n",
      "Train Epoch: 162 [155904/225000 (69%)] Loss: 10466.589844\n",
      "Train Epoch: 162 [160000/225000 (71%)] Loss: 8234.574219\n",
      "Train Epoch: 162 [164096/225000 (73%)] Loss: 8080.673828\n",
      "Train Epoch: 162 [168192/225000 (75%)] Loss: 8160.244141\n",
      "Train Epoch: 162 [172288/225000 (77%)] Loss: 8145.066406\n",
      "Train Epoch: 162 [176384/225000 (78%)] Loss: 8104.779297\n",
      "Train Epoch: 162 [180480/225000 (80%)] Loss: 8077.587891\n",
      "Train Epoch: 162 [184576/225000 (82%)] Loss: 11176.949219\n",
      "Train Epoch: 162 [188672/225000 (84%)] Loss: 8225.617188\n",
      "Train Epoch: 162 [192768/225000 (86%)] Loss: 7943.611328\n",
      "Train Epoch: 162 [196864/225000 (87%)] Loss: 12311.222656\n",
      "Train Epoch: 162 [200960/225000 (89%)] Loss: 13992.812500\n",
      "Train Epoch: 162 [205056/225000 (91%)] Loss: 9507.289062\n",
      "Train Epoch: 162 [209152/225000 (93%)] Loss: 9665.947266\n",
      "Train Epoch: 162 [213248/225000 (95%)] Loss: 9550.716797\n",
      "Train Epoch: 162 [217344/225000 (97%)] Loss: 8138.744141\n",
      "Train Epoch: 162 [221440/225000 (98%)] Loss: 8225.503906\n",
      "    epoch          : 162\n",
      "    loss           : 9678.062070045862\n",
      "    val_loss       : 9516.98175037637\n",
      "Train Epoch: 163 [256/225000 (0%)] Loss: 15014.638672\n",
      "Train Epoch: 163 [4352/225000 (2%)] Loss: 8016.142578\n",
      "Train Epoch: 163 [8448/225000 (4%)] Loss: 9732.390625\n",
      "Train Epoch: 163 [12544/225000 (6%)] Loss: 8340.027344\n",
      "Train Epoch: 163 [16640/225000 (7%)] Loss: 10611.691406\n",
      "Train Epoch: 163 [20736/225000 (9%)] Loss: 10515.123047\n",
      "Train Epoch: 163 [24832/225000 (11%)] Loss: 18022.488281\n",
      "Train Epoch: 163 [28928/225000 (13%)] Loss: 12370.529297\n",
      "Train Epoch: 163 [33024/225000 (15%)] Loss: 8016.738281\n",
      "Train Epoch: 163 [37120/225000 (16%)] Loss: 7887.292969\n",
      "Train Epoch: 163 [41216/225000 (18%)] Loss: 8080.988281\n",
      "Train Epoch: 163 [45312/225000 (20%)] Loss: 8057.111328\n",
      "Train Epoch: 163 [49408/225000 (22%)] Loss: 12012.361328\n",
      "Train Epoch: 163 [53504/225000 (24%)] Loss: 8020.433594\n",
      "Train Epoch: 163 [57600/225000 (26%)] Loss: 13794.775391\n",
      "Train Epoch: 163 [61696/225000 (27%)] Loss: 9439.988281\n",
      "Train Epoch: 163 [65792/225000 (29%)] Loss: 9428.187500\n",
      "Train Epoch: 163 [69888/225000 (31%)] Loss: 7947.019531\n",
      "Train Epoch: 163 [73984/225000 (33%)] Loss: 8047.498047\n",
      "Train Epoch: 163 [78080/225000 (35%)] Loss: 8082.501953\n",
      "Train Epoch: 163 [82176/225000 (37%)] Loss: 7916.355469\n",
      "Train Epoch: 163 [86272/225000 (38%)] Loss: 7939.529297\n",
      "Train Epoch: 163 [90368/225000 (40%)] Loss: 8187.359375\n",
      "Train Epoch: 163 [94464/225000 (42%)] Loss: 7874.861328\n",
      "Train Epoch: 163 [98560/225000 (44%)] Loss: 8108.498047\n",
      "Train Epoch: 163 [102656/225000 (46%)] Loss: 7972.253906\n",
      "Train Epoch: 163 [106752/225000 (47%)] Loss: 10735.919922\n",
      "Train Epoch: 163 [110848/225000 (49%)] Loss: 8104.640625\n",
      "Train Epoch: 163 [114944/225000 (51%)] Loss: 8048.816406\n",
      "Train Epoch: 163 [119040/225000 (53%)] Loss: 8012.824219\n",
      "Train Epoch: 163 [123136/225000 (55%)] Loss: 8126.812500\n",
      "Train Epoch: 163 [127232/225000 (57%)] Loss: 11997.175781\n",
      "Train Epoch: 163 [131328/225000 (58%)] Loss: 13111.509766\n",
      "Train Epoch: 163 [135424/225000 (60%)] Loss: 8103.148438\n",
      "Train Epoch: 163 [139520/225000 (62%)] Loss: 9485.992188\n",
      "Train Epoch: 163 [143616/225000 (64%)] Loss: 9735.748047\n",
      "Train Epoch: 163 [147712/225000 (66%)] Loss: 8070.785156\n",
      "Train Epoch: 163 [151808/225000 (67%)] Loss: 9519.302734\n",
      "Train Epoch: 163 [155904/225000 (69%)] Loss: 7993.925781\n",
      "Train Epoch: 163 [160000/225000 (71%)] Loss: 9607.777344\n",
      "Train Epoch: 163 [164096/225000 (73%)] Loss: 8059.386719\n",
      "Train Epoch: 163 [168192/225000 (75%)] Loss: 8304.583984\n",
      "Train Epoch: 163 [172288/225000 (77%)] Loss: 12000.179688\n",
      "Train Epoch: 163 [176384/225000 (78%)] Loss: 8144.677734\n",
      "Train Epoch: 163 [180480/225000 (80%)] Loss: 8277.861328\n",
      "Train Epoch: 163 [184576/225000 (82%)] Loss: 8164.771484\n",
      "Train Epoch: 163 [188672/225000 (84%)] Loss: 8072.412109\n",
      "Train Epoch: 163 [192768/225000 (86%)] Loss: 7937.369141\n",
      "Train Epoch: 163 [196864/225000 (87%)] Loss: 7922.787109\n",
      "Train Epoch: 163 [200960/225000 (89%)] Loss: 8102.757812\n",
      "Train Epoch: 163 [205056/225000 (91%)] Loss: 10674.312500\n",
      "Train Epoch: 163 [209152/225000 (93%)] Loss: 12240.396484\n",
      "Train Epoch: 163 [213248/225000 (95%)] Loss: 10535.744141\n",
      "Train Epoch: 163 [217344/225000 (97%)] Loss: 8067.421875\n",
      "Train Epoch: 163 [221440/225000 (98%)] Loss: 8070.392578\n",
      "    epoch          : 163\n",
      "    loss           : 9609.975744809442\n",
      "    val_loss       : 9410.102036509883\n",
      "Train Epoch: 164 [256/225000 (0%)] Loss: 12541.683594\n",
      "Train Epoch: 164 [4352/225000 (2%)] Loss: 8124.566406\n",
      "Train Epoch: 164 [8448/225000 (4%)] Loss: 9585.130859\n",
      "Train Epoch: 164 [12544/225000 (6%)] Loss: 18042.685547\n",
      "Train Epoch: 164 [16640/225000 (7%)] Loss: 12290.982422\n",
      "Train Epoch: 164 [20736/225000 (9%)] Loss: 8104.904297\n",
      "Train Epoch: 164 [24832/225000 (11%)] Loss: 10437.312500\n",
      "Train Epoch: 164 [28928/225000 (13%)] Loss: 8084.933594\n",
      "Train Epoch: 164 [33024/225000 (15%)] Loss: 8179.005859\n",
      "Train Epoch: 164 [37120/225000 (16%)] Loss: 10525.728516\n",
      "Train Epoch: 164 [41216/225000 (18%)] Loss: 9691.314453\n",
      "Train Epoch: 164 [45312/225000 (20%)] Loss: 13594.761719\n",
      "Train Epoch: 164 [49408/225000 (22%)] Loss: 9674.281250\n",
      "Train Epoch: 164 [53504/225000 (24%)] Loss: 8097.142578\n",
      "Train Epoch: 164 [57600/225000 (26%)] Loss: 8027.052734\n",
      "Train Epoch: 164 [61696/225000 (27%)] Loss: 8141.609375\n",
      "Train Epoch: 164 [65792/225000 (29%)] Loss: 9656.160156\n",
      "Train Epoch: 164 [69888/225000 (31%)] Loss: 8120.371094\n",
      "Train Epoch: 164 [73984/225000 (33%)] Loss: 9583.281250\n",
      "Train Epoch: 164 [78080/225000 (35%)] Loss: 7876.705078\n",
      "Train Epoch: 164 [82176/225000 (37%)] Loss: 9619.562500\n",
      "Train Epoch: 164 [86272/225000 (38%)] Loss: 11337.595703\n",
      "Train Epoch: 164 [90368/225000 (40%)] Loss: 9805.666016\n",
      "Train Epoch: 164 [94464/225000 (42%)] Loss: 14897.451172\n",
      "Train Epoch: 164 [98560/225000 (44%)] Loss: 10377.433594\n",
      "Train Epoch: 164 [102656/225000 (46%)] Loss: 10501.716797\n",
      "Train Epoch: 164 [106752/225000 (47%)] Loss: 8331.482422\n",
      "Train Epoch: 164 [110848/225000 (49%)] Loss: 14864.937500\n",
      "Train Epoch: 164 [114944/225000 (51%)] Loss: 9639.738281\n",
      "Train Epoch: 164 [119040/225000 (53%)] Loss: 8102.882812\n",
      "Train Epoch: 164 [123136/225000 (55%)] Loss: 11990.839844\n",
      "Train Epoch: 164 [127232/225000 (57%)] Loss: 7922.025391\n",
      "Train Epoch: 164 [131328/225000 (58%)] Loss: 9815.667969\n",
      "Train Epoch: 164 [135424/225000 (60%)] Loss: 10530.220703\n",
      "Train Epoch: 164 [139520/225000 (62%)] Loss: 9562.107422\n",
      "Train Epoch: 164 [143616/225000 (64%)] Loss: 7973.324219\n",
      "Train Epoch: 164 [147712/225000 (66%)] Loss: 10531.621094\n",
      "Train Epoch: 164 [151808/225000 (67%)] Loss: 7973.009766\n",
      "Train Epoch: 164 [155904/225000 (69%)] Loss: 16354.300781\n",
      "Train Epoch: 164 [160000/225000 (71%)] Loss: 8031.113281\n",
      "Train Epoch: 164 [164096/225000 (73%)] Loss: 10504.052734\n",
      "Train Epoch: 164 [168192/225000 (75%)] Loss: 8018.892578\n",
      "Train Epoch: 164 [172288/225000 (77%)] Loss: 8233.785156\n",
      "Train Epoch: 164 [176384/225000 (78%)] Loss: 8038.964844\n",
      "Train Epoch: 164 [180480/225000 (80%)] Loss: 17945.181641\n",
      "Train Epoch: 164 [184576/225000 (82%)] Loss: 10552.724609\n",
      "Train Epoch: 164 [188672/225000 (84%)] Loss: 13630.515625\n",
      "Train Epoch: 164 [192768/225000 (86%)] Loss: 9618.839844\n",
      "Train Epoch: 164 [196864/225000 (87%)] Loss: 9560.960938\n",
      "Train Epoch: 164 [200960/225000 (89%)] Loss: 8164.142578\n",
      "Train Epoch: 164 [205056/225000 (91%)] Loss: 12741.207031\n",
      "Train Epoch: 164 [209152/225000 (93%)] Loss: 8193.085938\n",
      "Train Epoch: 164 [213248/225000 (95%)] Loss: 8173.890625\n",
      "Train Epoch: 164 [217344/225000 (97%)] Loss: 8177.771484\n",
      "Train Epoch: 164 [221440/225000 (98%)] Loss: 7916.316406\n",
      "    epoch          : 164\n",
      "    loss           : 9609.021179963027\n",
      "    val_loss       : 9910.77951161472\n",
      "Train Epoch: 165 [256/225000 (0%)] Loss: 9770.746094\n",
      "Train Epoch: 165 [4352/225000 (2%)] Loss: 16194.777344\n",
      "Train Epoch: 165 [8448/225000 (4%)] Loss: 8238.384766\n",
      "Train Epoch: 165 [12544/225000 (6%)] Loss: 15127.226562\n",
      "Train Epoch: 165 [16640/225000 (7%)] Loss: 12447.394531\n",
      "Train Epoch: 165 [20736/225000 (9%)] Loss: 8063.976562\n",
      "Train Epoch: 165 [24832/225000 (11%)] Loss: 8173.902344\n",
      "Train Epoch: 165 [28928/225000 (13%)] Loss: 9752.935547\n",
      "Train Epoch: 165 [33024/225000 (15%)] Loss: 9541.613281\n",
      "Train Epoch: 165 [37120/225000 (16%)] Loss: 8093.601562\n",
      "Train Epoch: 165 [41216/225000 (18%)] Loss: 9697.177734\n",
      "Train Epoch: 165 [45312/225000 (20%)] Loss: 9660.603516\n",
      "Train Epoch: 165 [49408/225000 (22%)] Loss: 9623.630859\n",
      "Train Epoch: 165 [53504/225000 (24%)] Loss: 15240.507812\n",
      "Train Epoch: 165 [57600/225000 (26%)] Loss: 8041.533203\n",
      "Train Epoch: 165 [61696/225000 (27%)] Loss: 8009.091797\n",
      "Train Epoch: 165 [65792/225000 (29%)] Loss: 9535.113281\n",
      "Train Epoch: 165 [69888/225000 (31%)] Loss: 7951.146484\n",
      "Train Epoch: 165 [73984/225000 (33%)] Loss: 14755.802734\n",
      "Train Epoch: 165 [78080/225000 (35%)] Loss: 8146.324219\n",
      "Train Epoch: 165 [82176/225000 (37%)] Loss: 8353.906250\n",
      "Train Epoch: 165 [86272/225000 (38%)] Loss: 10647.980469\n",
      "Train Epoch: 165 [90368/225000 (40%)] Loss: 11147.576172\n",
      "Train Epoch: 165 [94464/225000 (42%)] Loss: 8196.347656\n",
      "Train Epoch: 165 [98560/225000 (44%)] Loss: 16161.234375\n",
      "Train Epoch: 165 [102656/225000 (46%)] Loss: 14610.242188\n",
      "Train Epoch: 165 [106752/225000 (47%)] Loss: 9750.021484\n",
      "Train Epoch: 165 [110848/225000 (49%)] Loss: 8150.117188\n",
      "Train Epoch: 165 [114944/225000 (51%)] Loss: 9430.722656\n",
      "Train Epoch: 165 [119040/225000 (53%)] Loss: 16278.070312\n",
      "Train Epoch: 165 [123136/225000 (55%)] Loss: 8216.136719\n",
      "Train Epoch: 165 [127232/225000 (57%)] Loss: 9558.910156\n",
      "Train Epoch: 165 [131328/225000 (58%)] Loss: 8216.433594\n",
      "Train Epoch: 165 [135424/225000 (60%)] Loss: 17891.539062\n",
      "Train Epoch: 165 [139520/225000 (62%)] Loss: 7901.837891\n",
      "Train Epoch: 165 [143616/225000 (64%)] Loss: 7989.263672\n",
      "Train Epoch: 165 [147712/225000 (66%)] Loss: 8202.388672\n",
      "Train Epoch: 165 [151808/225000 (67%)] Loss: 7990.798828\n",
      "Train Epoch: 165 [155904/225000 (69%)] Loss: 9785.730469\n",
      "Train Epoch: 165 [160000/225000 (71%)] Loss: 9603.316406\n",
      "Train Epoch: 165 [164096/225000 (73%)] Loss: 7931.111328\n",
      "Train Epoch: 165 [168192/225000 (75%)] Loss: 9655.785156\n",
      "Train Epoch: 165 [172288/225000 (77%)] Loss: 8026.826172\n",
      "Train Epoch: 165 [176384/225000 (78%)] Loss: 7866.605469\n",
      "Train Epoch: 165 [180480/225000 (80%)] Loss: 7992.277344\n",
      "Train Epoch: 165 [184576/225000 (82%)] Loss: 8154.929688\n",
      "Train Epoch: 165 [188672/225000 (84%)] Loss: 8153.083984\n",
      "Train Epoch: 165 [192768/225000 (86%)] Loss: 9552.869141\n",
      "Train Epoch: 165 [196864/225000 (87%)] Loss: 7964.423828\n",
      "Train Epoch: 165 [200960/225000 (89%)] Loss: 8077.876953\n",
      "Train Epoch: 165 [205056/225000 (91%)] Loss: 9654.156250\n",
      "Train Epoch: 165 [209152/225000 (93%)] Loss: 8051.396484\n",
      "Train Epoch: 165 [213248/225000 (95%)] Loss: 12992.128906\n",
      "Train Epoch: 165 [217344/225000 (97%)] Loss: 9864.332031\n",
      "Train Epoch: 165 [221440/225000 (98%)] Loss: 8115.339844\n",
      "    epoch          : 165\n",
      "    loss           : 9663.24766691553\n",
      "    val_loss       : 9431.032212469043\n",
      "Train Epoch: 166 [256/225000 (0%)] Loss: 9731.544922\n",
      "Train Epoch: 166 [4352/225000 (2%)] Loss: 7893.843750\n",
      "Train Epoch: 166 [8448/225000 (4%)] Loss: 10518.619141\n",
      "Train Epoch: 166 [12544/225000 (6%)] Loss: 9558.847656\n",
      "Train Epoch: 166 [16640/225000 (7%)] Loss: 8055.046875\n",
      "Train Epoch: 166 [20736/225000 (9%)] Loss: 8031.253906\n",
      "Train Epoch: 166 [24832/225000 (11%)] Loss: 7947.212891\n",
      "Train Epoch: 166 [28928/225000 (13%)] Loss: 13530.076172\n",
      "Train Epoch: 166 [33024/225000 (15%)] Loss: 7880.455078\n",
      "Train Epoch: 166 [37120/225000 (16%)] Loss: 9673.451172\n",
      "Train Epoch: 166 [41216/225000 (18%)] Loss: 7872.308594\n",
      "Train Epoch: 166 [45312/225000 (20%)] Loss: 14737.763672\n",
      "Train Epoch: 166 [49408/225000 (22%)] Loss: 7894.455078\n",
      "Train Epoch: 166 [53504/225000 (24%)] Loss: 12656.220703\n",
      "Train Epoch: 166 [57600/225000 (26%)] Loss: 10521.857422\n",
      "Train Epoch: 166 [61696/225000 (27%)] Loss: 7926.449219\n",
      "Train Epoch: 166 [65792/225000 (29%)] Loss: 12212.408203\n",
      "Train Epoch: 166 [69888/225000 (31%)] Loss: 8044.621094\n",
      "Train Epoch: 166 [73984/225000 (33%)] Loss: 8185.677734\n",
      "Train Epoch: 166 [78080/225000 (35%)] Loss: 10600.781250\n",
      "Train Epoch: 166 [82176/225000 (37%)] Loss: 9809.091797\n",
      "Train Epoch: 166 [86272/225000 (38%)] Loss: 8047.087891\n",
      "Train Epoch: 166 [90368/225000 (40%)] Loss: 8063.357422\n",
      "Train Epoch: 166 [94464/225000 (42%)] Loss: 8384.246094\n",
      "Train Epoch: 166 [98560/225000 (44%)] Loss: 11005.033203\n",
      "Train Epoch: 166 [102656/225000 (46%)] Loss: 9669.621094\n",
      "Train Epoch: 166 [106752/225000 (47%)] Loss: 12331.078125\n",
      "Train Epoch: 166 [110848/225000 (49%)] Loss: 12041.091797\n",
      "Train Epoch: 166 [114944/225000 (51%)] Loss: 7923.042969\n",
      "Train Epoch: 166 [119040/225000 (53%)] Loss: 9712.753906\n",
      "Train Epoch: 166 [123136/225000 (55%)] Loss: 8156.753906\n",
      "Train Epoch: 166 [127232/225000 (57%)] Loss: 12829.720703\n",
      "Train Epoch: 166 [131328/225000 (58%)] Loss: 8135.244141\n",
      "Train Epoch: 166 [135424/225000 (60%)] Loss: 11134.154297\n",
      "Train Epoch: 166 [139520/225000 (62%)] Loss: 10752.353516\n",
      "Train Epoch: 166 [143616/225000 (64%)] Loss: 14636.884766\n",
      "Train Epoch: 166 [147712/225000 (66%)] Loss: 8154.697266\n",
      "Train Epoch: 166 [151808/225000 (67%)] Loss: 13616.517578\n",
      "Train Epoch: 166 [155904/225000 (69%)] Loss: 8029.294922\n",
      "Train Epoch: 166 [160000/225000 (71%)] Loss: 7994.068359\n",
      "Train Epoch: 166 [164096/225000 (73%)] Loss: 7943.197266\n",
      "Train Epoch: 166 [168192/225000 (75%)] Loss: 8245.509766\n",
      "Train Epoch: 166 [172288/225000 (77%)] Loss: 12206.160156\n",
      "Train Epoch: 166 [176384/225000 (78%)] Loss: 8138.386719\n",
      "Train Epoch: 166 [180480/225000 (80%)] Loss: 11095.232422\n",
      "Train Epoch: 166 [184576/225000 (82%)] Loss: 8034.132812\n",
      "Train Epoch: 166 [188672/225000 (84%)] Loss: 7876.046875\n",
      "Train Epoch: 166 [192768/225000 (86%)] Loss: 8170.296875\n",
      "Train Epoch: 166 [196864/225000 (87%)] Loss: 7974.373047\n",
      "Train Epoch: 166 [200960/225000 (89%)] Loss: 12322.800781\n",
      "Train Epoch: 166 [205056/225000 (91%)] Loss: 9439.787109\n",
      "Train Epoch: 166 [209152/225000 (93%)] Loss: 8222.384766\n",
      "Train Epoch: 166 [213248/225000 (95%)] Loss: 7979.851562\n",
      "Train Epoch: 166 [217344/225000 (97%)] Loss: 7922.259766\n",
      "Train Epoch: 166 [221440/225000 (98%)] Loss: 7993.025391\n",
      "    epoch          : 166\n",
      "    loss           : 9574.183337110708\n",
      "    val_loss       : 10009.462574494128\n",
      "Train Epoch: 167 [256/225000 (0%)] Loss: 8271.691406\n",
      "Train Epoch: 167 [4352/225000 (2%)] Loss: 8017.359375\n",
      "Train Epoch: 167 [8448/225000 (4%)] Loss: 8060.458984\n",
      "Train Epoch: 167 [12544/225000 (6%)] Loss: 8123.335938\n",
      "Train Epoch: 167 [16640/225000 (7%)] Loss: 7991.410156\n",
      "Train Epoch: 167 [20736/225000 (9%)] Loss: 9725.197266\n",
      "Train Epoch: 167 [24832/225000 (11%)] Loss: 7954.591797\n",
      "Train Epoch: 167 [28928/225000 (13%)] Loss: 12927.228516\n",
      "Train Epoch: 167 [33024/225000 (15%)] Loss: 8366.203125\n",
      "Train Epoch: 167 [37120/225000 (16%)] Loss: 8196.447266\n",
      "Train Epoch: 167 [41216/225000 (18%)] Loss: 8338.236328\n",
      "Train Epoch: 167 [45312/225000 (20%)] Loss: 7993.472656\n",
      "Train Epoch: 167 [49408/225000 (22%)] Loss: 8170.519531\n",
      "Train Epoch: 167 [53504/225000 (24%)] Loss: 8051.189453\n",
      "Train Epoch: 167 [57600/225000 (26%)] Loss: 8078.044922\n",
      "Train Epoch: 167 [61696/225000 (27%)] Loss: 9450.103516\n",
      "Train Epoch: 167 [65792/225000 (29%)] Loss: 7935.037109\n",
      "Train Epoch: 167 [69888/225000 (31%)] Loss: 8001.806641\n",
      "Train Epoch: 167 [73984/225000 (33%)] Loss: 7874.964844\n",
      "Train Epoch: 167 [78080/225000 (35%)] Loss: 8075.916016\n",
      "Train Epoch: 167 [82176/225000 (37%)] Loss: 13695.257812\n",
      "Train Epoch: 167 [86272/225000 (38%)] Loss: 9531.113281\n",
      "Train Epoch: 167 [90368/225000 (40%)] Loss: 7844.195312\n",
      "Train Epoch: 167 [94464/225000 (42%)] Loss: 8120.937500\n",
      "Train Epoch: 167 [98560/225000 (44%)] Loss: 9545.929688\n",
      "Train Epoch: 167 [102656/225000 (46%)] Loss: 9713.162109\n",
      "Train Epoch: 167 [106752/225000 (47%)] Loss: 8149.601562\n",
      "Train Epoch: 167 [110848/225000 (49%)] Loss: 8187.695312\n",
      "Train Epoch: 167 [114944/225000 (51%)] Loss: 8053.494141\n",
      "Train Epoch: 167 [119040/225000 (53%)] Loss: 8031.585938\n",
      "Train Epoch: 167 [123136/225000 (55%)] Loss: 10520.154297\n",
      "Train Epoch: 167 [127232/225000 (57%)] Loss: 11394.941406\n",
      "Train Epoch: 167 [131328/225000 (58%)] Loss: 11147.335938\n",
      "Train Epoch: 167 [135424/225000 (60%)] Loss: 7944.783203\n",
      "Train Epoch: 167 [139520/225000 (62%)] Loss: 13990.789062\n",
      "Train Epoch: 167 [143616/225000 (64%)] Loss: 10557.599609\n",
      "Train Epoch: 167 [147712/225000 (66%)] Loss: 8182.501953\n",
      "Train Epoch: 167 [151808/225000 (67%)] Loss: 8163.937500\n",
      "Train Epoch: 167 [155904/225000 (69%)] Loss: 10687.869141\n",
      "Train Epoch: 167 [160000/225000 (71%)] Loss: 7977.751953\n",
      "Train Epoch: 167 [164096/225000 (73%)] Loss: 13544.732422\n",
      "Train Epoch: 167 [168192/225000 (75%)] Loss: 12292.267578\n",
      "Train Epoch: 167 [172288/225000 (77%)] Loss: 8070.054688\n",
      "Train Epoch: 167 [176384/225000 (78%)] Loss: 8292.021484\n",
      "Train Epoch: 167 [180480/225000 (80%)] Loss: 8172.277344\n",
      "Train Epoch: 167 [184576/225000 (82%)] Loss: 9510.382812\n",
      "Train Epoch: 167 [188672/225000 (84%)] Loss: 12409.783203\n",
      "Train Epoch: 167 [192768/225000 (86%)] Loss: 8023.357422\n",
      "Train Epoch: 167 [196864/225000 (87%)] Loss: 8001.580078\n",
      "Train Epoch: 167 [200960/225000 (89%)] Loss: 8141.964844\n",
      "Train Epoch: 167 [205056/225000 (91%)] Loss: 12240.304688\n",
      "Train Epoch: 167 [209152/225000 (93%)] Loss: 9675.195312\n",
      "Train Epoch: 167 [213248/225000 (95%)] Loss: 8000.818359\n",
      "Train Epoch: 167 [217344/225000 (97%)] Loss: 8088.580078\n",
      "Train Epoch: 167 [221440/225000 (98%)] Loss: 9648.392578\n",
      "    epoch          : 167\n",
      "    loss           : 9329.104251102104\n",
      "    val_loss       : 9284.223826194297\n",
      "Train Epoch: 168 [256/225000 (0%)] Loss: 10522.498047\n",
      "Train Epoch: 168 [4352/225000 (2%)] Loss: 10514.177734\n",
      "Train Epoch: 168 [8448/225000 (4%)] Loss: 14800.619141\n",
      "Train Epoch: 168 [12544/225000 (6%)] Loss: 13911.130859\n",
      "Train Epoch: 168 [16640/225000 (7%)] Loss: 8080.068359\n",
      "Train Epoch: 168 [20736/225000 (9%)] Loss: 8032.027344\n",
      "Train Epoch: 168 [24832/225000 (11%)] Loss: 9736.093750\n",
      "Train Epoch: 168 [28928/225000 (13%)] Loss: 9473.115234\n",
      "Train Epoch: 168 [33024/225000 (15%)] Loss: 13605.462891\n",
      "Train Epoch: 168 [37120/225000 (16%)] Loss: 8106.851562\n",
      "Train Epoch: 168 [41216/225000 (18%)] Loss: 8110.060547\n",
      "Train Epoch: 168 [45312/225000 (20%)] Loss: 12475.714844\n",
      "Train Epoch: 168 [49408/225000 (22%)] Loss: 11277.609375\n",
      "Train Epoch: 168 [53504/225000 (24%)] Loss: 13798.658203\n",
      "Train Epoch: 168 [57600/225000 (26%)] Loss: 8145.332031\n",
      "Train Epoch: 168 [61696/225000 (27%)] Loss: 13844.548828\n",
      "Train Epoch: 168 [65792/225000 (29%)] Loss: 8086.185547\n",
      "Train Epoch: 168 [69888/225000 (31%)] Loss: 10446.138672\n",
      "Train Epoch: 168 [73984/225000 (33%)] Loss: 13726.439453\n",
      "Train Epoch: 168 [78080/225000 (35%)] Loss: 7989.164062\n",
      "Train Epoch: 168 [82176/225000 (37%)] Loss: 8017.417969\n",
      "Train Epoch: 168 [86272/225000 (38%)] Loss: 8173.355469\n",
      "Train Epoch: 168 [90368/225000 (40%)] Loss: 7961.824219\n",
      "Train Epoch: 168 [94464/225000 (42%)] Loss: 7956.994141\n",
      "Train Epoch: 168 [98560/225000 (44%)] Loss: 7852.207031\n",
      "Train Epoch: 168 [102656/225000 (46%)] Loss: 13775.595703\n",
      "Train Epoch: 168 [106752/225000 (47%)] Loss: 8112.214844\n",
      "Train Epoch: 168 [110848/225000 (49%)] Loss: 10728.339844\n",
      "Train Epoch: 168 [114944/225000 (51%)] Loss: 8327.148438\n",
      "Train Epoch: 168 [119040/225000 (53%)] Loss: 9599.007812\n",
      "Train Epoch: 168 [123136/225000 (55%)] Loss: 12291.621094\n",
      "Train Epoch: 168 [127232/225000 (57%)] Loss: 9416.607422\n",
      "Train Epoch: 168 [131328/225000 (58%)] Loss: 8024.414062\n",
      "Train Epoch: 168 [135424/225000 (60%)] Loss: 8168.052734\n",
      "Train Epoch: 168 [139520/225000 (62%)] Loss: 12219.298828\n",
      "Train Epoch: 168 [143616/225000 (64%)] Loss: 7955.154297\n",
      "Train Epoch: 168 [147712/225000 (66%)] Loss: 7885.562500\n",
      "Train Epoch: 168 [151808/225000 (67%)] Loss: 13833.642578\n",
      "Train Epoch: 168 [155904/225000 (69%)] Loss: 8219.535156\n",
      "Train Epoch: 168 [160000/225000 (71%)] Loss: 10635.480469\n",
      "Train Epoch: 168 [164096/225000 (73%)] Loss: 8003.962891\n",
      "Train Epoch: 168 [168192/225000 (75%)] Loss: 9536.390625\n",
      "Train Epoch: 168 [172288/225000 (77%)] Loss: 8257.416016\n",
      "Train Epoch: 168 [176384/225000 (78%)] Loss: 8150.423828\n",
      "Train Epoch: 168 [180480/225000 (80%)] Loss: 8730.093750\n",
      "Train Epoch: 168 [184576/225000 (82%)] Loss: 8287.849609\n",
      "Train Epoch: 168 [188672/225000 (84%)] Loss: 12343.681641\n",
      "Train Epoch: 168 [192768/225000 (86%)] Loss: 9556.935547\n",
      "Train Epoch: 168 [196864/225000 (87%)] Loss: 14084.230469\n",
      "Train Epoch: 168 [200960/225000 (89%)] Loss: 8099.144531\n",
      "Train Epoch: 168 [205056/225000 (91%)] Loss: 10718.324219\n",
      "Train Epoch: 168 [209152/225000 (93%)] Loss: 9562.425781\n",
      "Train Epoch: 168 [213248/225000 (95%)] Loss: 10524.115234\n",
      "Train Epoch: 168 [217344/225000 (97%)] Loss: 10605.556641\n",
      "Train Epoch: 168 [221440/225000 (98%)] Loss: 13033.361328\n",
      "    epoch          : 168\n",
      "    loss           : 9496.786848291738\n",
      "    val_loss       : 9481.487802310865\n",
      "Train Epoch: 169 [256/225000 (0%)] Loss: 8249.982422\n",
      "Train Epoch: 169 [4352/225000 (2%)] Loss: 13817.197266\n",
      "Train Epoch: 169 [8448/225000 (4%)] Loss: 8061.800781\n",
      "Train Epoch: 169 [12544/225000 (6%)] Loss: 8029.798828\n",
      "Train Epoch: 169 [16640/225000 (7%)] Loss: 7908.707031\n",
      "Train Epoch: 169 [20736/225000 (9%)] Loss: 9635.355469\n",
      "Train Epoch: 169 [24832/225000 (11%)] Loss: 8045.169922\n",
      "Train Epoch: 169 [28928/225000 (13%)] Loss: 8072.189453\n",
      "Train Epoch: 169 [33024/225000 (15%)] Loss: 10481.189453\n",
      "Train Epoch: 169 [37120/225000 (16%)] Loss: 8060.757812\n",
      "Train Epoch: 169 [41216/225000 (18%)] Loss: 13915.431641\n",
      "Train Epoch: 169 [45312/225000 (20%)] Loss: 8105.927734\n",
      "Train Epoch: 169 [49408/225000 (22%)] Loss: 8117.138672\n",
      "Train Epoch: 169 [53504/225000 (24%)] Loss: 8152.996094\n",
      "Train Epoch: 169 [57600/225000 (26%)] Loss: 9788.853516\n",
      "Train Epoch: 169 [61696/225000 (27%)] Loss: 14849.365234\n",
      "Train Epoch: 169 [65792/225000 (29%)] Loss: 8030.460938\n",
      "Train Epoch: 169 [69888/225000 (31%)] Loss: 8101.912109\n",
      "Train Epoch: 169 [73984/225000 (33%)] Loss: 8078.289062\n",
      "Train Epoch: 169 [78080/225000 (35%)] Loss: 8032.029297\n",
      "Train Epoch: 169 [82176/225000 (37%)] Loss: 8238.580078\n",
      "Train Epoch: 169 [86272/225000 (38%)] Loss: 10593.369141\n",
      "Train Epoch: 169 [90368/225000 (40%)] Loss: 7981.662109\n",
      "Train Epoch: 169 [94464/225000 (42%)] Loss: 15091.304688\n",
      "Train Epoch: 169 [98560/225000 (44%)] Loss: 9806.736328\n",
      "Train Epoch: 169 [102656/225000 (46%)] Loss: 8171.583984\n",
      "Train Epoch: 169 [106752/225000 (47%)] Loss: 13753.685547\n",
      "Train Epoch: 169 [110848/225000 (49%)] Loss: 8003.136719\n",
      "Train Epoch: 169 [114944/225000 (51%)] Loss: 7884.908203\n",
      "Train Epoch: 169 [119040/225000 (53%)] Loss: 8097.697266\n",
      "Train Epoch: 169 [123136/225000 (55%)] Loss: 8073.816406\n",
      "Train Epoch: 169 [127232/225000 (57%)] Loss: 13814.021484\n",
      "Train Epoch: 169 [131328/225000 (58%)] Loss: 12431.644531\n",
      "Train Epoch: 169 [135424/225000 (60%)] Loss: 9778.248047\n",
      "Train Epoch: 169 [139520/225000 (62%)] Loss: 8239.015625\n",
      "Train Epoch: 169 [143616/225000 (64%)] Loss: 8090.832031\n",
      "Train Epoch: 169 [147712/225000 (66%)] Loss: 7842.220703\n",
      "Train Epoch: 169 [151808/225000 (67%)] Loss: 8060.185547\n",
      "Train Epoch: 169 [155904/225000 (69%)] Loss: 13516.808594\n",
      "Train Epoch: 169 [160000/225000 (71%)] Loss: 10484.089844\n",
      "Train Epoch: 169 [164096/225000 (73%)] Loss: 8212.195312\n",
      "Train Epoch: 169 [168192/225000 (75%)] Loss: 8225.279297\n",
      "Train Epoch: 169 [172288/225000 (77%)] Loss: 7849.382812\n",
      "Train Epoch: 169 [176384/225000 (78%)] Loss: 9608.441406\n",
      "Train Epoch: 169 [180480/225000 (80%)] Loss: 8351.140625\n",
      "Train Epoch: 169 [184576/225000 (82%)] Loss: 8087.314453\n",
      "Train Epoch: 169 [188672/225000 (84%)] Loss: 8103.724609\n",
      "Train Epoch: 169 [192768/225000 (86%)] Loss: 13546.722656\n",
      "Train Epoch: 169 [196864/225000 (87%)] Loss: 10489.550781\n",
      "Train Epoch: 169 [200960/225000 (89%)] Loss: 7915.318359\n",
      "Train Epoch: 169 [205056/225000 (91%)] Loss: 13428.671875\n",
      "Train Epoch: 169 [209152/225000 (93%)] Loss: 10687.927734\n",
      "Train Epoch: 169 [213248/225000 (95%)] Loss: 7964.236328\n",
      "Train Epoch: 169 [217344/225000 (97%)] Loss: 10561.630859\n",
      "Train Epoch: 169 [221440/225000 (98%)] Loss: 13153.005859\n",
      "    epoch          : 169\n",
      "    loss           : 9538.726069219283\n",
      "    val_loss       : 9379.008075354051\n",
      "Train Epoch: 170 [256/225000 (0%)] Loss: 8106.154297\n",
      "Train Epoch: 170 [4352/225000 (2%)] Loss: 8032.097656\n",
      "Train Epoch: 170 [8448/225000 (4%)] Loss: 10338.646484\n",
      "Train Epoch: 170 [12544/225000 (6%)] Loss: 8071.886719\n",
      "Train Epoch: 170 [16640/225000 (7%)] Loss: 9682.716797\n",
      "Train Epoch: 170 [20736/225000 (9%)] Loss: 8028.316406\n",
      "Train Epoch: 170 [24832/225000 (11%)] Loss: 12131.578125\n",
      "Train Epoch: 170 [28928/225000 (13%)] Loss: 7927.878906\n",
      "Train Epoch: 170 [33024/225000 (15%)] Loss: 7782.191406\n",
      "Train Epoch: 170 [37120/225000 (16%)] Loss: 13652.376953\n",
      "Train Epoch: 170 [41216/225000 (18%)] Loss: 8238.642578\n",
      "Train Epoch: 170 [45312/225000 (20%)] Loss: 10770.902344\n",
      "Train Epoch: 170 [49408/225000 (22%)] Loss: 10406.476562\n",
      "Train Epoch: 170 [53504/225000 (24%)] Loss: 8016.847656\n",
      "Train Epoch: 170 [57600/225000 (26%)] Loss: 8168.318359\n",
      "Train Epoch: 170 [61696/225000 (27%)] Loss: 12953.375000\n",
      "Train Epoch: 170 [65792/225000 (29%)] Loss: 16065.677734\n",
      "Train Epoch: 170 [69888/225000 (31%)] Loss: 8075.853516\n",
      "Train Epoch: 170 [73984/225000 (33%)] Loss: 10365.599609\n",
      "Train Epoch: 170 [78080/225000 (35%)] Loss: 7915.939453\n",
      "Train Epoch: 170 [82176/225000 (37%)] Loss: 8126.455078\n",
      "Train Epoch: 170 [86272/225000 (38%)] Loss: 8055.082031\n",
      "Train Epoch: 170 [90368/225000 (40%)] Loss: 8121.070312\n",
      "Train Epoch: 170 [94464/225000 (42%)] Loss: 13679.349609\n",
      "Train Epoch: 170 [98560/225000 (44%)] Loss: 13201.703125\n",
      "Train Epoch: 170 [102656/225000 (46%)] Loss: 8060.953125\n",
      "Train Epoch: 170 [106752/225000 (47%)] Loss: 9661.451172\n",
      "Train Epoch: 170 [110848/225000 (49%)] Loss: 8026.607422\n",
      "Train Epoch: 170 [114944/225000 (51%)] Loss: 10463.525391\n",
      "Train Epoch: 170 [119040/225000 (53%)] Loss: 8037.445312\n",
      "Train Epoch: 170 [123136/225000 (55%)] Loss: 8008.462891\n",
      "Train Epoch: 170 [127232/225000 (57%)] Loss: 8026.138672\n",
      "Train Epoch: 170 [131328/225000 (58%)] Loss: 9294.085938\n",
      "Train Epoch: 170 [135424/225000 (60%)] Loss: 7932.902344\n",
      "Train Epoch: 170 [139520/225000 (62%)] Loss: 8047.042969\n",
      "Train Epoch: 170 [143616/225000 (64%)] Loss: 8055.376953\n",
      "Train Epoch: 170 [147712/225000 (66%)] Loss: 8277.808594\n",
      "Train Epoch: 170 [151808/225000 (67%)] Loss: 9841.246094\n",
      "Train Epoch: 170 [155904/225000 (69%)] Loss: 8144.888672\n",
      "Train Epoch: 170 [160000/225000 (71%)] Loss: 8114.357422\n",
      "Train Epoch: 170 [164096/225000 (73%)] Loss: 14614.695312\n",
      "Train Epoch: 170 [168192/225000 (75%)] Loss: 8111.724609\n",
      "Train Epoch: 170 [172288/225000 (77%)] Loss: 7992.666016\n",
      "Train Epoch: 170 [176384/225000 (78%)] Loss: 7999.822266\n",
      "Train Epoch: 170 [180480/225000 (80%)] Loss: 9737.757812\n",
      "Train Epoch: 170 [184576/225000 (82%)] Loss: 8199.035156\n",
      "Train Epoch: 170 [188672/225000 (84%)] Loss: 8019.503906\n",
      "Train Epoch: 170 [192768/225000 (86%)] Loss: 8189.048828\n",
      "Train Epoch: 170 [196864/225000 (87%)] Loss: 8092.472656\n",
      "Train Epoch: 170 [200960/225000 (89%)] Loss: 8034.185547\n",
      "Train Epoch: 170 [205056/225000 (91%)] Loss: 12156.572266\n",
      "Train Epoch: 170 [209152/225000 (93%)] Loss: 7919.429688\n",
      "Train Epoch: 170 [213248/225000 (95%)] Loss: 9514.685547\n",
      "Train Epoch: 170 [217344/225000 (97%)] Loss: 7839.666016\n",
      "Train Epoch: 170 [221440/225000 (98%)] Loss: 9747.562500\n",
      "    epoch          : 170\n",
      "    loss           : 9488.826240756542\n",
      "    val_loss       : 9500.538689783642\n",
      "Train Epoch: 171 [256/225000 (0%)] Loss: 13772.353516\n",
      "Train Epoch: 171 [4352/225000 (2%)] Loss: 8102.150391\n",
      "Train Epoch: 171 [8448/225000 (4%)] Loss: 7981.347656\n",
      "Train Epoch: 171 [12544/225000 (6%)] Loss: 8231.371094\n",
      "Train Epoch: 171 [16640/225000 (7%)] Loss: 7960.960938\n",
      "Train Epoch: 171 [20736/225000 (9%)] Loss: 13738.273438\n",
      "Train Epoch: 171 [24832/225000 (11%)] Loss: 9527.666016\n",
      "Train Epoch: 171 [28928/225000 (13%)] Loss: 8188.558594\n",
      "Train Epoch: 171 [33024/225000 (15%)] Loss: 9824.826172\n",
      "Train Epoch: 171 [37120/225000 (16%)] Loss: 8059.294922\n",
      "Train Epoch: 171 [41216/225000 (18%)] Loss: 9780.050781\n",
      "Train Epoch: 171 [45312/225000 (20%)] Loss: 9757.759766\n",
      "Train Epoch: 171 [49408/225000 (22%)] Loss: 10714.402344\n",
      "Train Epoch: 171 [53504/225000 (24%)] Loss: 10444.343750\n",
      "Train Epoch: 171 [57600/225000 (26%)] Loss: 8194.718750\n",
      "Train Epoch: 171 [61696/225000 (27%)] Loss: 10541.695312\n",
      "Train Epoch: 171 [65792/225000 (29%)] Loss: 9611.964844\n",
      "Train Epoch: 171 [69888/225000 (31%)] Loss: 9581.574219\n",
      "Train Epoch: 171 [73984/225000 (33%)] Loss: 8044.851562\n",
      "Train Epoch: 171 [78080/225000 (35%)] Loss: 8122.835938\n",
      "Train Epoch: 171 [82176/225000 (37%)] Loss: 8039.101562\n",
      "Train Epoch: 171 [86272/225000 (38%)] Loss: 13768.423828\n",
      "Train Epoch: 171 [90368/225000 (40%)] Loss: 9574.933594\n",
      "Train Epoch: 171 [94464/225000 (42%)] Loss: 9579.619141\n",
      "Train Epoch: 171 [98560/225000 (44%)] Loss: 13537.990234\n",
      "Train Epoch: 171 [102656/225000 (46%)] Loss: 7879.011719\n",
      "Train Epoch: 171 [106752/225000 (47%)] Loss: 8222.140625\n",
      "Train Epoch: 171 [110848/225000 (49%)] Loss: 8099.525391\n",
      "Train Epoch: 171 [114944/225000 (51%)] Loss: 7991.609375\n",
      "Train Epoch: 171 [119040/225000 (53%)] Loss: 10587.736328\n",
      "Train Epoch: 171 [123136/225000 (55%)] Loss: 8128.994141\n",
      "Train Epoch: 171 [127232/225000 (57%)] Loss: 10340.726562\n",
      "Train Epoch: 171 [131328/225000 (58%)] Loss: 16358.646484\n",
      "Train Epoch: 171 [135424/225000 (60%)] Loss: 10530.611328\n",
      "Train Epoch: 171 [139520/225000 (62%)] Loss: 8047.964844\n",
      "Train Epoch: 171 [143616/225000 (64%)] Loss: 8040.078125\n",
      "Train Epoch: 171 [147712/225000 (66%)] Loss: 12336.097656\n",
      "Train Epoch: 171 [151808/225000 (67%)] Loss: 10457.439453\n",
      "Train Epoch: 171 [155904/225000 (69%)] Loss: 9580.822266\n",
      "Train Epoch: 171 [160000/225000 (71%)] Loss: 14022.943359\n",
      "Train Epoch: 171 [164096/225000 (73%)] Loss: 8057.734375\n",
      "Train Epoch: 171 [168192/225000 (75%)] Loss: 8167.115234\n",
      "Train Epoch: 171 [172288/225000 (77%)] Loss: 8154.382812\n",
      "Train Epoch: 171 [176384/225000 (78%)] Loss: 12561.734375\n",
      "Train Epoch: 171 [180480/225000 (80%)] Loss: 10673.585938\n",
      "Train Epoch: 171 [184576/225000 (82%)] Loss: 8031.667969\n",
      "Train Epoch: 171 [188672/225000 (84%)] Loss: 10447.906250\n",
      "Train Epoch: 171 [192768/225000 (86%)] Loss: 9492.001953\n",
      "Train Epoch: 171 [196864/225000 (87%)] Loss: 8080.900391\n",
      "Train Epoch: 171 [200960/225000 (89%)] Loss: 7934.156250\n",
      "Train Epoch: 171 [205056/225000 (91%)] Loss: 9508.642578\n",
      "Train Epoch: 171 [209152/225000 (93%)] Loss: 8119.943359\n",
      "Train Epoch: 171 [213248/225000 (95%)] Loss: 10395.636719\n",
      "Train Epoch: 171 [217344/225000 (97%)] Loss: 8136.224609\n",
      "Train Epoch: 171 [221440/225000 (98%)] Loss: 17373.144531\n",
      "    epoch          : 171\n",
      "    loss           : 9659.647584257678\n",
      "    val_loss       : 9459.69177914639\n",
      "Train Epoch: 172 [256/225000 (0%)] Loss: 9479.121094\n",
      "Train Epoch: 172 [4352/225000 (2%)] Loss: 10654.218750\n",
      "Train Epoch: 172 [8448/225000 (4%)] Loss: 8045.824219\n",
      "Train Epoch: 172 [12544/225000 (6%)] Loss: 8058.707031\n",
      "Train Epoch: 172 [16640/225000 (7%)] Loss: 8116.376953\n",
      "Train Epoch: 172 [20736/225000 (9%)] Loss: 12330.636719\n",
      "Train Epoch: 172 [24832/225000 (11%)] Loss: 8250.378906\n",
      "Train Epoch: 172 [28928/225000 (13%)] Loss: 8134.244141\n",
      "Train Epoch: 172 [33024/225000 (15%)] Loss: 7997.718750\n",
      "Train Epoch: 172 [37120/225000 (16%)] Loss: 9647.652344\n",
      "Train Epoch: 172 [41216/225000 (18%)] Loss: 8118.031250\n",
      "Train Epoch: 172 [45312/225000 (20%)] Loss: 9771.970703\n",
      "Train Epoch: 172 [49408/225000 (22%)] Loss: 7959.097656\n",
      "Train Epoch: 172 [53504/225000 (24%)] Loss: 8003.884766\n",
      "Train Epoch: 172 [57600/225000 (26%)] Loss: 9345.033203\n",
      "Train Epoch: 172 [61696/225000 (27%)] Loss: 10540.703125\n",
      "Train Epoch: 172 [65792/225000 (29%)] Loss: 8143.263672\n",
      "Train Epoch: 172 [69888/225000 (31%)] Loss: 19369.548828\n",
      "Train Epoch: 172 [73984/225000 (33%)] Loss: 8012.562500\n",
      "Train Epoch: 172 [78080/225000 (35%)] Loss: 13625.361328\n",
      "Train Epoch: 172 [82176/225000 (37%)] Loss: 8038.306641\n",
      "Train Epoch: 172 [86272/225000 (38%)] Loss: 7932.871094\n",
      "Train Epoch: 172 [90368/225000 (40%)] Loss: 7926.314453\n",
      "Train Epoch: 172 [94464/225000 (42%)] Loss: 8151.361328\n",
      "Train Epoch: 172 [98560/225000 (44%)] Loss: 7951.724609\n",
      "Train Epoch: 172 [102656/225000 (46%)] Loss: 8025.529297\n",
      "Train Epoch: 172 [106752/225000 (47%)] Loss: 8105.271484\n",
      "Train Epoch: 172 [110848/225000 (49%)] Loss: 9701.363281\n",
      "Train Epoch: 172 [114944/225000 (51%)] Loss: 12531.388672\n",
      "Train Epoch: 172 [119040/225000 (53%)] Loss: 13436.083984\n",
      "Train Epoch: 172 [123136/225000 (55%)] Loss: 9505.978516\n",
      "Train Epoch: 172 [127232/225000 (57%)] Loss: 8475.449219\n",
      "Train Epoch: 172 [131328/225000 (58%)] Loss: 13467.515625\n",
      "Train Epoch: 172 [135424/225000 (60%)] Loss: 13820.050781\n",
      "Train Epoch: 172 [139520/225000 (62%)] Loss: 11150.441406\n",
      "Train Epoch: 172 [143616/225000 (64%)] Loss: 15285.626953\n",
      "Train Epoch: 172 [147712/225000 (66%)] Loss: 9465.501953\n",
      "Train Epoch: 172 [151808/225000 (67%)] Loss: 8093.994141\n",
      "Train Epoch: 172 [155904/225000 (69%)] Loss: 8011.708984\n",
      "Train Epoch: 172 [160000/225000 (71%)] Loss: 7999.414062\n",
      "Train Epoch: 172 [164096/225000 (73%)] Loss: 10614.691406\n",
      "Train Epoch: 172 [168192/225000 (75%)] Loss: 8144.189453\n",
      "Train Epoch: 172 [172288/225000 (77%)] Loss: 8066.275391\n",
      "Train Epoch: 172 [176384/225000 (78%)] Loss: 8021.048828\n",
      "Train Epoch: 172 [180480/225000 (80%)] Loss: 8239.314453\n",
      "Train Epoch: 172 [184576/225000 (82%)] Loss: 9669.859375\n",
      "Train Epoch: 172 [188672/225000 (84%)] Loss: 9650.523438\n",
      "Train Epoch: 172 [192768/225000 (86%)] Loss: 8176.609375\n",
      "Train Epoch: 172 [196864/225000 (87%)] Loss: 9780.726562\n",
      "Train Epoch: 172 [200960/225000 (89%)] Loss: 7875.677734\n",
      "Train Epoch: 172 [205056/225000 (91%)] Loss: 9686.464844\n",
      "Train Epoch: 172 [209152/225000 (93%)] Loss: 8061.699219\n",
      "Train Epoch: 172 [213248/225000 (95%)] Loss: 8173.294922\n",
      "Train Epoch: 172 [217344/225000 (97%)] Loss: 9746.500000\n",
      "Train Epoch: 172 [221440/225000 (98%)] Loss: 7999.458984\n",
      "    epoch          : 172\n",
      "    loss           : 9620.750711035267\n",
      "    val_loss       : 9625.204799764011\n",
      "Train Epoch: 173 [256/225000 (0%)] Loss: 8057.628906\n",
      "Train Epoch: 173 [4352/225000 (2%)] Loss: 12347.857422\n",
      "Train Epoch: 173 [8448/225000 (4%)] Loss: 7830.187500\n",
      "Train Epoch: 173 [12544/225000 (6%)] Loss: 8189.548828\n",
      "Train Epoch: 173 [16640/225000 (7%)] Loss: 8090.853516\n",
      "Train Epoch: 173 [20736/225000 (9%)] Loss: 8025.921875\n",
      "Train Epoch: 173 [24832/225000 (11%)] Loss: 10453.693359\n",
      "Train Epoch: 173 [28928/225000 (13%)] Loss: 8057.414062\n",
      "Train Epoch: 173 [33024/225000 (15%)] Loss: 9724.113281\n",
      "Train Epoch: 173 [37120/225000 (16%)] Loss: 8104.125000\n",
      "Train Epoch: 173 [41216/225000 (18%)] Loss: 8184.025391\n",
      "Train Epoch: 173 [45312/225000 (20%)] Loss: 11413.658203\n",
      "Train Epoch: 173 [49408/225000 (22%)] Loss: 8082.292969\n",
      "Train Epoch: 173 [53504/225000 (24%)] Loss: 8164.486328\n",
      "Train Epoch: 173 [57600/225000 (26%)] Loss: 7983.181641\n",
      "Train Epoch: 173 [61696/225000 (27%)] Loss: 8167.074219\n",
      "Train Epoch: 173 [65792/225000 (29%)] Loss: 10388.595703\n",
      "Train Epoch: 173 [69888/225000 (31%)] Loss: 8310.167969\n",
      "Train Epoch: 173 [73984/225000 (33%)] Loss: 8235.107422\n",
      "Train Epoch: 173 [78080/225000 (35%)] Loss: 9850.832031\n",
      "Train Epoch: 173 [82176/225000 (37%)] Loss: 9477.605469\n",
      "Train Epoch: 173 [86272/225000 (38%)] Loss: 13615.525391\n",
      "Train Epoch: 173 [90368/225000 (40%)] Loss: 8036.419922\n",
      "Train Epoch: 173 [94464/225000 (42%)] Loss: 12891.890625\n",
      "Train Epoch: 173 [98560/225000 (44%)] Loss: 12434.964844\n",
      "Train Epoch: 173 [102656/225000 (46%)] Loss: 13590.597656\n",
      "Train Epoch: 173 [106752/225000 (47%)] Loss: 8139.234375\n",
      "Train Epoch: 173 [110848/225000 (49%)] Loss: 8035.496094\n",
      "Train Epoch: 173 [114944/225000 (51%)] Loss: 7841.558594\n",
      "Train Epoch: 173 [119040/225000 (53%)] Loss: 7966.138672\n",
      "Train Epoch: 173 [123136/225000 (55%)] Loss: 8144.730469\n",
      "Train Epoch: 173 [127232/225000 (57%)] Loss: 7960.865234\n",
      "Train Epoch: 173 [131328/225000 (58%)] Loss: 8136.660156\n",
      "Train Epoch: 173 [135424/225000 (60%)] Loss: 7904.193359\n",
      "Train Epoch: 173 [139520/225000 (62%)] Loss: 8093.156250\n",
      "Train Epoch: 173 [143616/225000 (64%)] Loss: 10565.294922\n",
      "Train Epoch: 173 [147712/225000 (66%)] Loss: 9494.681641\n",
      "Train Epoch: 173 [151808/225000 (67%)] Loss: 8180.482422\n",
      "Train Epoch: 173 [155904/225000 (69%)] Loss: 8001.156250\n",
      "Train Epoch: 173 [160000/225000 (71%)] Loss: 9727.693359\n",
      "Train Epoch: 173 [164096/225000 (73%)] Loss: 8124.587891\n",
      "Train Epoch: 173 [168192/225000 (75%)] Loss: 9539.175781\n",
      "Train Epoch: 173 [172288/225000 (77%)] Loss: 7910.371094\n",
      "Train Epoch: 173 [176384/225000 (78%)] Loss: 9641.599609\n",
      "Train Epoch: 173 [180480/225000 (80%)] Loss: 9704.363281\n",
      "Train Epoch: 173 [184576/225000 (82%)] Loss: 8212.287109\n",
      "Train Epoch: 173 [188672/225000 (84%)] Loss: 8120.144531\n",
      "Train Epoch: 173 [192768/225000 (86%)] Loss: 11095.562500\n",
      "Train Epoch: 173 [196864/225000 (87%)] Loss: 10819.248047\n",
      "Train Epoch: 173 [200960/225000 (89%)] Loss: 8035.601562\n",
      "Train Epoch: 173 [205056/225000 (91%)] Loss: 9664.769531\n",
      "Train Epoch: 173 [209152/225000 (93%)] Loss: 16239.433594\n",
      "Train Epoch: 173 [213248/225000 (95%)] Loss: 7998.175781\n",
      "Train Epoch: 173 [217344/225000 (97%)] Loss: 8085.937500\n",
      "Train Epoch: 173 [221440/225000 (98%)] Loss: 8066.806641\n",
      "    epoch          : 173\n",
      "    loss           : 9494.197638918515\n",
      "    val_loss       : 9494.498150068886\n",
      "Train Epoch: 174 [256/225000 (0%)] Loss: 8073.742188\n",
      "Train Epoch: 174 [4352/225000 (2%)] Loss: 8106.955078\n",
      "Train Epoch: 174 [8448/225000 (4%)] Loss: 8025.302734\n",
      "Train Epoch: 174 [12544/225000 (6%)] Loss: 8094.767578\n",
      "Train Epoch: 174 [16640/225000 (7%)] Loss: 8154.470703\n",
      "Train Epoch: 174 [20736/225000 (9%)] Loss: 13710.947266\n",
      "Train Epoch: 174 [24832/225000 (11%)] Loss: 14705.933594\n",
      "Train Epoch: 174 [28928/225000 (13%)] Loss: 8181.679688\n",
      "Train Epoch: 174 [33024/225000 (15%)] Loss: 8407.779297\n",
      "Train Epoch: 174 [37120/225000 (16%)] Loss: 8359.998047\n",
      "Train Epoch: 174 [41216/225000 (18%)] Loss: 10478.226562\n",
      "Train Epoch: 174 [45312/225000 (20%)] Loss: 7865.699219\n",
      "Train Epoch: 174 [49408/225000 (22%)] Loss: 7914.000000\n",
      "Train Epoch: 174 [53504/225000 (24%)] Loss: 8166.542969\n",
      "Train Epoch: 174 [57600/225000 (26%)] Loss: 9571.019531\n",
      "Train Epoch: 174 [61696/225000 (27%)] Loss: 8030.382812\n",
      "Train Epoch: 174 [65792/225000 (29%)] Loss: 8190.818359\n",
      "Train Epoch: 174 [69888/225000 (31%)] Loss: 7965.726562\n",
      "Train Epoch: 174 [73984/225000 (33%)] Loss: 8073.482422\n",
      "Train Epoch: 174 [78080/225000 (35%)] Loss: 10476.142578\n",
      "Train Epoch: 174 [82176/225000 (37%)] Loss: 10958.111328\n",
      "Train Epoch: 174 [86272/225000 (38%)] Loss: 7892.128906\n",
      "Train Epoch: 174 [90368/225000 (40%)] Loss: 7954.406250\n",
      "Train Epoch: 174 [94464/225000 (42%)] Loss: 8106.154297\n",
      "Train Epoch: 174 [98560/225000 (44%)] Loss: 8097.484375\n",
      "Train Epoch: 174 [102656/225000 (46%)] Loss: 13662.330078\n",
      "Train Epoch: 174 [106752/225000 (47%)] Loss: 8016.636719\n",
      "Train Epoch: 174 [110848/225000 (49%)] Loss: 8266.941406\n",
      "Train Epoch: 174 [114944/225000 (51%)] Loss: 7946.460938\n",
      "Train Epoch: 174 [119040/225000 (53%)] Loss: 10605.941406\n",
      "Train Epoch: 174 [123136/225000 (55%)] Loss: 9743.544922\n",
      "Train Epoch: 174 [127232/225000 (57%)] Loss: 10449.339844\n",
      "Train Epoch: 174 [131328/225000 (58%)] Loss: 8073.087891\n",
      "Train Epoch: 174 [135424/225000 (60%)] Loss: 9471.724609\n",
      "Train Epoch: 174 [139520/225000 (62%)] Loss: 12566.820312\n",
      "Train Epoch: 174 [143616/225000 (64%)] Loss: 7910.093750\n",
      "Train Epoch: 174 [147712/225000 (66%)] Loss: 12981.011719\n",
      "Train Epoch: 174 [151808/225000 (67%)] Loss: 10264.984375\n",
      "Train Epoch: 174 [155904/225000 (69%)] Loss: 8119.210938\n",
      "Train Epoch: 174 [160000/225000 (71%)] Loss: 7997.238281\n",
      "Train Epoch: 174 [164096/225000 (73%)] Loss: 9675.357422\n",
      "Train Epoch: 174 [168192/225000 (75%)] Loss: 8144.107422\n",
      "Train Epoch: 174 [172288/225000 (77%)] Loss: 8000.277344\n",
      "Train Epoch: 174 [176384/225000 (78%)] Loss: 8139.753906\n",
      "Train Epoch: 174 [180480/225000 (80%)] Loss: 9625.931641\n",
      "Train Epoch: 174 [184576/225000 (82%)] Loss: 7955.443359\n",
      "Train Epoch: 174 [188672/225000 (84%)] Loss: 11131.925781\n",
      "Train Epoch: 174 [192768/225000 (86%)] Loss: 13886.062500\n",
      "Train Epoch: 174 [196864/225000 (87%)] Loss: 8048.195312\n",
      "Train Epoch: 174 [200960/225000 (89%)] Loss: 9628.669922\n",
      "Train Epoch: 174 [205056/225000 (91%)] Loss: 10436.652344\n",
      "Train Epoch: 174 [209152/225000 (93%)] Loss: 8153.699219\n",
      "Train Epoch: 174 [213248/225000 (95%)] Loss: 12259.812500\n",
      "Train Epoch: 174 [217344/225000 (97%)] Loss: 11166.412109\n",
      "Train Epoch: 174 [221440/225000 (98%)] Loss: 8261.736328\n",
      "    epoch          : 174\n",
      "    loss           : 9572.144422372725\n",
      "    val_loss       : 9539.698602878318\n",
      "Train Epoch: 175 [256/225000 (0%)] Loss: 10632.939453\n",
      "Train Epoch: 175 [4352/225000 (2%)] Loss: 10619.349609\n",
      "Train Epoch: 175 [8448/225000 (4%)] Loss: 10637.460938\n",
      "Train Epoch: 175 [12544/225000 (6%)] Loss: 8100.998047\n",
      "Train Epoch: 175 [16640/225000 (7%)] Loss: 9779.703125\n",
      "Train Epoch: 175 [20736/225000 (9%)] Loss: 8079.875000\n",
      "Train Epoch: 175 [24832/225000 (11%)] Loss: 8307.716797\n",
      "Train Epoch: 175 [28928/225000 (13%)] Loss: 8099.457031\n",
      "Train Epoch: 175 [33024/225000 (15%)] Loss: 7948.126953\n",
      "Train Epoch: 175 [37120/225000 (16%)] Loss: 7855.945312\n",
      "Train Epoch: 175 [41216/225000 (18%)] Loss: 15174.242188\n",
      "Train Epoch: 175 [45312/225000 (20%)] Loss: 10291.308594\n",
      "Train Epoch: 175 [49408/225000 (22%)] Loss: 8070.789062\n",
      "Train Epoch: 175 [53504/225000 (24%)] Loss: 7987.126953\n",
      "Train Epoch: 175 [57600/225000 (26%)] Loss: 7900.345703\n",
      "Train Epoch: 175 [61696/225000 (27%)] Loss: 10463.175781\n",
      "Train Epoch: 175 [65792/225000 (29%)] Loss: 7892.505859\n",
      "Train Epoch: 175 [69888/225000 (31%)] Loss: 8169.392578\n",
      "Train Epoch: 175 [73984/225000 (33%)] Loss: 8106.302734\n",
      "Train Epoch: 175 [78080/225000 (35%)] Loss: 10329.228516\n",
      "Train Epoch: 175 [82176/225000 (37%)] Loss: 8083.414062\n",
      "Train Epoch: 175 [86272/225000 (38%)] Loss: 8031.376953\n",
      "Train Epoch: 175 [90368/225000 (40%)] Loss: 8087.966797\n",
      "Train Epoch: 175 [94464/225000 (42%)] Loss: 8072.822266\n",
      "Train Epoch: 175 [98560/225000 (44%)] Loss: 9669.718750\n",
      "Train Epoch: 175 [102656/225000 (46%)] Loss: 8161.955078\n",
      "Train Epoch: 175 [106752/225000 (47%)] Loss: 11899.923828\n",
      "Train Epoch: 175 [110848/225000 (49%)] Loss: 9727.484375\n",
      "Train Epoch: 175 [114944/225000 (51%)] Loss: 7986.156250\n",
      "Train Epoch: 175 [119040/225000 (53%)] Loss: 9657.726562\n",
      "Train Epoch: 175 [123136/225000 (55%)] Loss: 13688.644531\n",
      "Train Epoch: 175 [127232/225000 (57%)] Loss: 17993.097656\n",
      "Train Epoch: 175 [131328/225000 (58%)] Loss: 17968.431641\n",
      "Train Epoch: 175 [135424/225000 (60%)] Loss: 8016.082031\n",
      "Train Epoch: 175 [139520/225000 (62%)] Loss: 8146.296875\n",
      "Train Epoch: 175 [143616/225000 (64%)] Loss: 11081.421875\n",
      "Train Epoch: 175 [147712/225000 (66%)] Loss: 12555.701172\n",
      "Train Epoch: 175 [151808/225000 (67%)] Loss: 8170.302734\n",
      "Train Epoch: 175 [155904/225000 (69%)] Loss: 12035.277344\n",
      "Train Epoch: 175 [160000/225000 (71%)] Loss: 8070.156250\n",
      "Train Epoch: 175 [164096/225000 (73%)] Loss: 8049.437500\n",
      "Train Epoch: 175 [168192/225000 (75%)] Loss: 9788.037109\n",
      "Train Epoch: 175 [172288/225000 (77%)] Loss: 8221.826172\n",
      "Train Epoch: 175 [176384/225000 (78%)] Loss: 7969.640625\n",
      "Train Epoch: 175 [180480/225000 (80%)] Loss: 8021.240234\n",
      "Train Epoch: 175 [184576/225000 (82%)] Loss: 8032.103516\n",
      "Train Epoch: 175 [188672/225000 (84%)] Loss: 8178.009766\n",
      "Train Epoch: 175 [192768/225000 (86%)] Loss: 8006.970703\n",
      "Train Epoch: 175 [196864/225000 (87%)] Loss: 7996.419922\n",
      "Train Epoch: 175 [200960/225000 (89%)] Loss: 10690.718750\n",
      "Train Epoch: 175 [205056/225000 (91%)] Loss: 9699.187500\n",
      "Train Epoch: 175 [209152/225000 (93%)] Loss: 8038.527344\n",
      "Train Epoch: 175 [213248/225000 (95%)] Loss: 9902.908203\n",
      "Train Epoch: 175 [217344/225000 (97%)] Loss: 13057.916016\n",
      "Train Epoch: 175 [221440/225000 (98%)] Loss: 8242.841797\n",
      "    epoch          : 175\n",
      "    loss           : 9445.452513954067\n",
      "    val_loss       : 9809.124215040889\n",
      "Train Epoch: 176 [256/225000 (0%)] Loss: 10559.669922\n",
      "Train Epoch: 176 [4352/225000 (2%)] Loss: 13908.042969\n",
      "Train Epoch: 176 [8448/225000 (4%)] Loss: 7940.902344\n",
      "Train Epoch: 176 [12544/225000 (6%)] Loss: 13516.529297\n",
      "Train Epoch: 176 [16640/225000 (7%)] Loss: 8147.611328\n",
      "Train Epoch: 176 [20736/225000 (9%)] Loss: 8233.482422\n",
      "Train Epoch: 176 [24832/225000 (11%)] Loss: 12504.923828\n",
      "Train Epoch: 176 [28928/225000 (13%)] Loss: 7892.062500\n",
      "Train Epoch: 176 [33024/225000 (15%)] Loss: 7955.490234\n",
      "Train Epoch: 176 [37120/225000 (16%)] Loss: 10567.673828\n",
      "Train Epoch: 176 [41216/225000 (18%)] Loss: 8058.140625\n",
      "Train Epoch: 176 [45312/225000 (20%)] Loss: 8029.628906\n",
      "Train Epoch: 176 [49408/225000 (22%)] Loss: 8133.109375\n",
      "Train Epoch: 176 [53504/225000 (24%)] Loss: 8046.169922\n",
      "Train Epoch: 176 [57600/225000 (26%)] Loss: 7923.519531\n",
      "Train Epoch: 176 [61696/225000 (27%)] Loss: 13758.074219\n",
      "Train Epoch: 176 [65792/225000 (29%)] Loss: 8053.101562\n",
      "Train Epoch: 176 [69888/225000 (31%)] Loss: 9541.302734\n",
      "Train Epoch: 176 [73984/225000 (33%)] Loss: 8250.916016\n",
      "Train Epoch: 176 [78080/225000 (35%)] Loss: 13723.332031\n",
      "Train Epoch: 176 [82176/225000 (37%)] Loss: 10628.634766\n",
      "Train Epoch: 176 [86272/225000 (38%)] Loss: 14508.312500\n",
      "Train Epoch: 176 [90368/225000 (40%)] Loss: 8237.423828\n",
      "Train Epoch: 176 [94464/225000 (42%)] Loss: 14948.857422\n",
      "Train Epoch: 176 [98560/225000 (44%)] Loss: 8277.962891\n",
      "Train Epoch: 176 [102656/225000 (46%)] Loss: 10564.906250\n",
      "Train Epoch: 176 [106752/225000 (47%)] Loss: 12160.726562\n",
      "Train Epoch: 176 [110848/225000 (49%)] Loss: 8005.751953\n",
      "Train Epoch: 176 [114944/225000 (51%)] Loss: 8182.429688\n",
      "Train Epoch: 176 [119040/225000 (53%)] Loss: 9580.244141\n",
      "Train Epoch: 176 [123136/225000 (55%)] Loss: 13844.419922\n",
      "Train Epoch: 176 [127232/225000 (57%)] Loss: 10742.830078\n",
      "Train Epoch: 176 [131328/225000 (58%)] Loss: 8409.919922\n",
      "Train Epoch: 176 [135424/225000 (60%)] Loss: 8205.185547\n",
      "Train Epoch: 176 [139520/225000 (62%)] Loss: 12183.234375\n",
      "Train Epoch: 176 [143616/225000 (64%)] Loss: 8004.679688\n",
      "Train Epoch: 176 [147712/225000 (66%)] Loss: 8056.808594\n",
      "Train Epoch: 176 [151808/225000 (67%)] Loss: 8094.972656\n",
      "Train Epoch: 176 [155904/225000 (69%)] Loss: 8089.767578\n",
      "Train Epoch: 176 [160000/225000 (71%)] Loss: 9544.382812\n",
      "Train Epoch: 176 [164096/225000 (73%)] Loss: 12448.179688\n",
      "Train Epoch: 176 [168192/225000 (75%)] Loss: 12402.500000\n",
      "Train Epoch: 176 [172288/225000 (77%)] Loss: 7968.804688\n",
      "Train Epoch: 176 [176384/225000 (78%)] Loss: 9634.419922\n",
      "Train Epoch: 176 [180480/225000 (80%)] Loss: 15049.037109\n",
      "Train Epoch: 176 [184576/225000 (82%)] Loss: 8113.207031\n",
      "Train Epoch: 176 [188672/225000 (84%)] Loss: 8028.197266\n",
      "Train Epoch: 176 [192768/225000 (86%)] Loss: 12377.802734\n",
      "Train Epoch: 176 [196864/225000 (87%)] Loss: 7940.763672\n",
      "Train Epoch: 176 [200960/225000 (89%)] Loss: 7916.714844\n",
      "Train Epoch: 176 [205056/225000 (91%)] Loss: 8159.753906\n",
      "Train Epoch: 176 [209152/225000 (93%)] Loss: 7918.240234\n",
      "Train Epoch: 176 [213248/225000 (95%)] Loss: 10669.806641\n",
      "Train Epoch: 176 [217344/225000 (97%)] Loss: 8092.154297\n",
      "Train Epoch: 176 [221440/225000 (98%)] Loss: 12085.070312\n",
      "    epoch          : 176\n",
      "    loss           : 9587.927173323735\n",
      "    val_loss       : 9504.714710486178\n",
      "Train Epoch: 177 [256/225000 (0%)] Loss: 7957.501953\n",
      "Train Epoch: 177 [4352/225000 (2%)] Loss: 8093.939453\n",
      "Train Epoch: 177 [8448/225000 (4%)] Loss: 8199.273438\n",
      "Train Epoch: 177 [12544/225000 (6%)] Loss: 8178.480469\n",
      "Train Epoch: 177 [16640/225000 (7%)] Loss: 7974.501953\n",
      "Train Epoch: 177 [20736/225000 (9%)] Loss: 9655.355469\n",
      "Train Epoch: 177 [24832/225000 (11%)] Loss: 15134.869141\n",
      "Train Epoch: 177 [28928/225000 (13%)] Loss: 7848.119141\n",
      "Train Epoch: 177 [33024/225000 (15%)] Loss: 9792.953125\n",
      "Train Epoch: 177 [37120/225000 (16%)] Loss: 8062.308594\n",
      "Train Epoch: 177 [41216/225000 (18%)] Loss: 13662.621094\n",
      "Train Epoch: 177 [45312/225000 (20%)] Loss: 8052.675781\n",
      "Train Epoch: 177 [49408/225000 (22%)] Loss: 7968.447266\n",
      "Train Epoch: 177 [53504/225000 (24%)] Loss: 8100.425781\n",
      "Train Epoch: 177 [57600/225000 (26%)] Loss: 11477.626953\n",
      "Train Epoch: 177 [61696/225000 (27%)] Loss: 12313.277344\n",
      "Train Epoch: 177 [65792/225000 (29%)] Loss: 8037.746094\n",
      "Train Epoch: 177 [69888/225000 (31%)] Loss: 10327.753906\n",
      "Train Epoch: 177 [73984/225000 (33%)] Loss: 8333.873047\n",
      "Train Epoch: 177 [78080/225000 (35%)] Loss: 8201.847656\n",
      "Train Epoch: 177 [82176/225000 (37%)] Loss: 8114.607422\n",
      "Train Epoch: 177 [86272/225000 (38%)] Loss: 18044.312500\n",
      "Train Epoch: 177 [90368/225000 (40%)] Loss: 8363.462891\n",
      "Train Epoch: 177 [94464/225000 (42%)] Loss: 8019.443359\n",
      "Train Epoch: 177 [98560/225000 (44%)] Loss: 8235.460938\n",
      "Train Epoch: 177 [102656/225000 (46%)] Loss: 8080.646484\n",
      "Train Epoch: 177 [106752/225000 (47%)] Loss: 12804.337891\n",
      "Train Epoch: 177 [110848/225000 (49%)] Loss: 8193.271484\n",
      "Train Epoch: 177 [114944/225000 (51%)] Loss: 8192.820312\n",
      "Train Epoch: 177 [119040/225000 (53%)] Loss: 7943.240234\n",
      "Train Epoch: 177 [123136/225000 (55%)] Loss: 7980.062500\n",
      "Train Epoch: 177 [127232/225000 (57%)] Loss: 7962.400391\n",
      "Train Epoch: 177 [131328/225000 (58%)] Loss: 12498.599609\n",
      "Train Epoch: 177 [135424/225000 (60%)] Loss: 7964.882812\n",
      "Train Epoch: 177 [139520/225000 (62%)] Loss: 10317.828125\n",
      "Train Epoch: 177 [143616/225000 (64%)] Loss: 19357.980469\n",
      "Train Epoch: 177 [147712/225000 (66%)] Loss: 9556.117188\n",
      "Train Epoch: 177 [151808/225000 (67%)] Loss: 7989.011719\n",
      "Train Epoch: 177 [155904/225000 (69%)] Loss: 8165.199219\n",
      "Train Epoch: 177 [160000/225000 (71%)] Loss: 8016.384766\n",
      "Train Epoch: 177 [164096/225000 (73%)] Loss: 8187.058594\n",
      "Train Epoch: 177 [168192/225000 (75%)] Loss: 10587.498047\n",
      "Train Epoch: 177 [172288/225000 (77%)] Loss: 8148.564453\n",
      "Train Epoch: 177 [176384/225000 (78%)] Loss: 8019.324219\n",
      "Train Epoch: 177 [180480/225000 (80%)] Loss: 7979.636719\n",
      "Train Epoch: 177 [184576/225000 (82%)] Loss: 8169.521484\n",
      "Train Epoch: 177 [188672/225000 (84%)] Loss: 7915.808594\n",
      "Train Epoch: 177 [192768/225000 (86%)] Loss: 8079.341797\n",
      "Train Epoch: 177 [196864/225000 (87%)] Loss: 13093.234375\n",
      "Train Epoch: 177 [200960/225000 (89%)] Loss: 8036.322266\n",
      "Train Epoch: 177 [205056/225000 (91%)] Loss: 8016.017578\n",
      "Train Epoch: 177 [209152/225000 (93%)] Loss: 8027.470703\n",
      "Train Epoch: 177 [213248/225000 (95%)] Loss: 8006.888672\n",
      "Train Epoch: 177 [217344/225000 (97%)] Loss: 7986.906250\n",
      "Train Epoch: 177 [221440/225000 (98%)] Loss: 8323.960938\n",
      "    epoch          : 177\n",
      "    loss           : 9462.514522895335\n",
      "    val_loss       : 9436.529218279586\n",
      "Train Epoch: 178 [256/225000 (0%)] Loss: 8150.279297\n",
      "Train Epoch: 178 [4352/225000 (2%)] Loss: 8184.326172\n",
      "Train Epoch: 178 [8448/225000 (4%)] Loss: 8063.468750\n",
      "Train Epoch: 178 [12544/225000 (6%)] Loss: 12183.796875\n",
      "Train Epoch: 178 [16640/225000 (7%)] Loss: 9697.824219\n",
      "Train Epoch: 178 [20736/225000 (9%)] Loss: 7955.837891\n",
      "Train Epoch: 178 [24832/225000 (11%)] Loss: 8076.650391\n",
      "Train Epoch: 178 [28928/225000 (13%)] Loss: 8044.935547\n",
      "Train Epoch: 178 [33024/225000 (15%)] Loss: 8195.556641\n",
      "Train Epoch: 178 [37120/225000 (16%)] Loss: 10584.148438\n",
      "Train Epoch: 178 [41216/225000 (18%)] Loss: 12102.250000\n",
      "Train Epoch: 178 [45312/225000 (20%)] Loss: 8016.867188\n",
      "Train Epoch: 178 [49408/225000 (22%)] Loss: 7993.705078\n",
      "Train Epoch: 178 [53504/225000 (24%)] Loss: 13027.642578\n",
      "Train Epoch: 178 [57600/225000 (26%)] Loss: 9722.595703\n",
      "Train Epoch: 178 [61696/225000 (27%)] Loss: 7986.318359\n",
      "Train Epoch: 178 [65792/225000 (29%)] Loss: 10272.613281\n",
      "Train Epoch: 178 [69888/225000 (31%)] Loss: 8046.035156\n",
      "Train Epoch: 178 [73984/225000 (33%)] Loss: 8049.343750\n",
      "Train Epoch: 178 [78080/225000 (35%)] Loss: 8277.646484\n",
      "Train Epoch: 178 [82176/225000 (37%)] Loss: 9555.960938\n",
      "Train Epoch: 178 [86272/225000 (38%)] Loss: 13956.414062\n",
      "Train Epoch: 178 [90368/225000 (40%)] Loss: 7951.431641\n",
      "Train Epoch: 178 [94464/225000 (42%)] Loss: 9676.275391\n",
      "Train Epoch: 178 [98560/225000 (44%)] Loss: 8087.425781\n",
      "Train Epoch: 178 [102656/225000 (46%)] Loss: 8098.191406\n",
      "Train Epoch: 178 [106752/225000 (47%)] Loss: 9418.095703\n",
      "Train Epoch: 178 [110848/225000 (49%)] Loss: 16872.953125\n",
      "Train Epoch: 178 [114944/225000 (51%)] Loss: 9703.669922\n",
      "Train Epoch: 178 [119040/225000 (53%)] Loss: 7978.564453\n",
      "Train Epoch: 178 [123136/225000 (55%)] Loss: 12624.666016\n",
      "Train Epoch: 178 [127232/225000 (57%)] Loss: 12410.533203\n",
      "Train Epoch: 178 [131328/225000 (58%)] Loss: 8059.880859\n",
      "Train Epoch: 178 [135424/225000 (60%)] Loss: 13861.455078\n",
      "Train Epoch: 178 [139520/225000 (62%)] Loss: 9571.933594\n",
      "Train Epoch: 178 [143616/225000 (64%)] Loss: 9459.777344\n",
      "Train Epoch: 178 [147712/225000 (66%)] Loss: 16290.210938\n",
      "Train Epoch: 178 [151808/225000 (67%)] Loss: 7932.564453\n",
      "Train Epoch: 178 [155904/225000 (69%)] Loss: 8130.148438\n",
      "Train Epoch: 178 [160000/225000 (71%)] Loss: 8103.937500\n",
      "Train Epoch: 178 [164096/225000 (73%)] Loss: 8051.103516\n",
      "Train Epoch: 178 [168192/225000 (75%)] Loss: 8254.832031\n",
      "Train Epoch: 178 [172288/225000 (77%)] Loss: 8032.958984\n",
      "Train Epoch: 178 [176384/225000 (78%)] Loss: 12195.230469\n",
      "Train Epoch: 178 [180480/225000 (80%)] Loss: 7931.394531\n",
      "Train Epoch: 178 [184576/225000 (82%)] Loss: 9552.710938\n",
      "Train Epoch: 178 [188672/225000 (84%)] Loss: 8054.558594\n",
      "Train Epoch: 178 [192768/225000 (86%)] Loss: 8193.726562\n",
      "Train Epoch: 178 [196864/225000 (87%)] Loss: 12285.861328\n",
      "Train Epoch: 178 [200960/225000 (89%)] Loss: 9318.824219\n",
      "Train Epoch: 178 [205056/225000 (91%)] Loss: 8121.875000\n",
      "Train Epoch: 178 [209152/225000 (93%)] Loss: 8205.158203\n",
      "Train Epoch: 178 [213248/225000 (95%)] Loss: 10584.927734\n",
      "Train Epoch: 178 [217344/225000 (97%)] Loss: 10375.744141\n",
      "Train Epoch: 178 [221440/225000 (98%)] Loss: 12326.548828\n",
      "    epoch          : 178\n",
      "    loss           : 9477.895662240473\n",
      "    val_loss       : 9037.03810866025\n",
      "Train Epoch: 179 [256/225000 (0%)] Loss: 8112.345703\n",
      "Train Epoch: 179 [4352/225000 (2%)] Loss: 8095.025391\n",
      "Train Epoch: 179 [8448/225000 (4%)] Loss: 10542.992188\n",
      "Train Epoch: 179 [12544/225000 (6%)] Loss: 8093.029297\n",
      "Train Epoch: 179 [16640/225000 (7%)] Loss: 8193.351562\n",
      "Train Epoch: 179 [20736/225000 (9%)] Loss: 15669.115234\n",
      "Train Epoch: 179 [24832/225000 (11%)] Loss: 9670.703125\n",
      "Train Epoch: 179 [28928/225000 (13%)] Loss: 8365.593750\n",
      "Train Epoch: 179 [33024/225000 (15%)] Loss: 12207.050781\n",
      "Train Epoch: 179 [37120/225000 (16%)] Loss: 9622.210938\n",
      "Train Epoch: 179 [41216/225000 (18%)] Loss: 8178.441406\n",
      "Train Epoch: 179 [45312/225000 (20%)] Loss: 8152.882812\n",
      "Train Epoch: 179 [49408/225000 (22%)] Loss: 8072.593750\n",
      "Train Epoch: 179 [53504/225000 (24%)] Loss: 9491.115234\n",
      "Train Epoch: 179 [57600/225000 (26%)] Loss: 9446.382812\n",
      "Train Epoch: 179 [61696/225000 (27%)] Loss: 13744.296875\n",
      "Train Epoch: 179 [65792/225000 (29%)] Loss: 12063.056641\n",
      "Train Epoch: 179 [69888/225000 (31%)] Loss: 8000.919922\n",
      "Train Epoch: 179 [73984/225000 (33%)] Loss: 12956.560547\n",
      "Train Epoch: 179 [78080/225000 (35%)] Loss: 12173.046875\n",
      "Train Epoch: 179 [82176/225000 (37%)] Loss: 10452.693359\n",
      "Train Epoch: 179 [86272/225000 (38%)] Loss: 7991.783203\n",
      "Train Epoch: 179 [90368/225000 (40%)] Loss: 9758.988281\n",
      "Train Epoch: 179 [94464/225000 (42%)] Loss: 8288.255859\n",
      "Train Epoch: 179 [98560/225000 (44%)] Loss: 7988.054688\n",
      "Train Epoch: 179 [102656/225000 (46%)] Loss: 9731.146484\n",
      "Train Epoch: 179 [106752/225000 (47%)] Loss: 8222.896484\n",
      "Train Epoch: 179 [110848/225000 (49%)] Loss: 10498.671875\n",
      "Train Epoch: 179 [114944/225000 (51%)] Loss: 12037.720703\n",
      "Train Epoch: 179 [119040/225000 (53%)] Loss: 7971.044922\n",
      "Train Epoch: 179 [123136/225000 (55%)] Loss: 11323.007812\n",
      "Train Epoch: 179 [127232/225000 (57%)] Loss: 8081.988281\n",
      "Train Epoch: 179 [131328/225000 (58%)] Loss: 9587.011719\n",
      "Train Epoch: 179 [135424/225000 (60%)] Loss: 8128.998047\n",
      "Train Epoch: 179 [139520/225000 (62%)] Loss: 8180.812500\n",
      "Train Epoch: 179 [143616/225000 (64%)] Loss: 12456.691406\n",
      "Train Epoch: 179 [147712/225000 (66%)] Loss: 7926.638672\n",
      "Train Epoch: 179 [151808/225000 (67%)] Loss: 12001.757812\n",
      "Train Epoch: 179 [155904/225000 (69%)] Loss: 8031.083984\n",
      "Train Epoch: 179 [160000/225000 (71%)] Loss: 7924.535156\n",
      "Train Epoch: 179 [164096/225000 (73%)] Loss: 8002.228516\n",
      "Train Epoch: 179 [168192/225000 (75%)] Loss: 15988.718750\n",
      "Train Epoch: 179 [172288/225000 (77%)] Loss: 12872.220703\n",
      "Train Epoch: 179 [176384/225000 (78%)] Loss: 7969.087891\n",
      "Train Epoch: 179 [180480/225000 (80%)] Loss: 13933.023438\n",
      "Train Epoch: 179 [184576/225000 (82%)] Loss: 8056.683594\n",
      "Train Epoch: 179 [188672/225000 (84%)] Loss: 9513.093750\n",
      "Train Epoch: 179 [192768/225000 (86%)] Loss: 8196.144531\n",
      "Train Epoch: 179 [196864/225000 (87%)] Loss: 7978.912109\n",
      "Train Epoch: 179 [200960/225000 (89%)] Loss: 8089.843750\n",
      "Train Epoch: 179 [205056/225000 (91%)] Loss: 7979.601562\n",
      "Train Epoch: 179 [209152/225000 (93%)] Loss: 12904.099609\n",
      "Train Epoch: 179 [213248/225000 (95%)] Loss: 8129.556641\n",
      "Train Epoch: 179 [217344/225000 (97%)] Loss: 8076.865234\n",
      "Train Epoch: 179 [221440/225000 (98%)] Loss: 8148.933594\n",
      "    epoch          : 179\n",
      "    loss           : 9499.418379817263\n",
      "    val_loss       : 9461.421849260525\n",
      "Train Epoch: 180 [256/225000 (0%)] Loss: 12704.052734\n",
      "Train Epoch: 180 [4352/225000 (2%)] Loss: 8215.187500\n",
      "Train Epoch: 180 [8448/225000 (4%)] Loss: 7959.982422\n",
      "Train Epoch: 180 [12544/225000 (6%)] Loss: 9810.427734\n",
      "Train Epoch: 180 [16640/225000 (7%)] Loss: 9740.787109\n",
      "Train Epoch: 180 [20736/225000 (9%)] Loss: 10434.708984\n",
      "Train Epoch: 180 [24832/225000 (11%)] Loss: 10522.621094\n",
      "Train Epoch: 180 [28928/225000 (13%)] Loss: 9424.044922\n",
      "Train Epoch: 180 [33024/225000 (15%)] Loss: 8223.873047\n",
      "Train Epoch: 180 [37120/225000 (16%)] Loss: 9556.337891\n",
      "Train Epoch: 180 [41216/225000 (18%)] Loss: 9785.671875\n",
      "Train Epoch: 180 [45312/225000 (20%)] Loss: 8058.582031\n",
      "Train Epoch: 180 [49408/225000 (22%)] Loss: 7997.984375\n",
      "Train Epoch: 180 [53504/225000 (24%)] Loss: 8063.111328\n",
      "Train Epoch: 180 [57600/225000 (26%)] Loss: 8352.781250\n",
      "Train Epoch: 180 [61696/225000 (27%)] Loss: 10506.451172\n",
      "Train Epoch: 180 [65792/225000 (29%)] Loss: 8158.865234\n",
      "Train Epoch: 180 [69888/225000 (31%)] Loss: 8229.576172\n",
      "Train Epoch: 180 [73984/225000 (33%)] Loss: 8234.466797\n",
      "Train Epoch: 180 [78080/225000 (35%)] Loss: 12765.207031\n",
      "Train Epoch: 180 [82176/225000 (37%)] Loss: 8143.714844\n",
      "Train Epoch: 180 [86272/225000 (38%)] Loss: 13716.888672\n",
      "Train Epoch: 180 [90368/225000 (40%)] Loss: 8146.259766\n",
      "Train Epoch: 180 [94464/225000 (42%)] Loss: 9689.593750\n",
      "Train Epoch: 180 [98560/225000 (44%)] Loss: 13988.599609\n",
      "Train Epoch: 180 [102656/225000 (46%)] Loss: 8127.326172\n",
      "Train Epoch: 180 [106752/225000 (47%)] Loss: 7865.837891\n",
      "Train Epoch: 180 [110848/225000 (49%)] Loss: 10478.857422\n",
      "Train Epoch: 180 [114944/225000 (51%)] Loss: 9636.955078\n",
      "Train Epoch: 180 [119040/225000 (53%)] Loss: 7913.906250\n",
      "Train Epoch: 180 [123136/225000 (55%)] Loss: 8271.376953\n",
      "Train Epoch: 180 [127232/225000 (57%)] Loss: 7922.458984\n",
      "Train Epoch: 180 [131328/225000 (58%)] Loss: 8090.273438\n",
      "Train Epoch: 180 [135424/225000 (60%)] Loss: 10471.556641\n",
      "Train Epoch: 180 [139520/225000 (62%)] Loss: 10686.349609\n",
      "Train Epoch: 180 [143616/225000 (64%)] Loss: 10500.875000\n",
      "Train Epoch: 180 [147712/225000 (66%)] Loss: 8104.148438\n",
      "Train Epoch: 180 [151808/225000 (67%)] Loss: 8015.843750\n",
      "Train Epoch: 180 [155904/225000 (69%)] Loss: 8074.501953\n",
      "Train Epoch: 180 [160000/225000 (71%)] Loss: 10390.835938\n",
      "Train Epoch: 180 [164096/225000 (73%)] Loss: 7984.666016\n",
      "Train Epoch: 180 [168192/225000 (75%)] Loss: 9641.609375\n",
      "Train Epoch: 180 [172288/225000 (77%)] Loss: 7981.710938\n",
      "Train Epoch: 180 [176384/225000 (78%)] Loss: 8036.326172\n",
      "Train Epoch: 180 [180480/225000 (80%)] Loss: 9701.472656\n",
      "Train Epoch: 180 [184576/225000 (82%)] Loss: 8188.199219\n",
      "Train Epoch: 180 [188672/225000 (84%)] Loss: 8110.939453\n",
      "Train Epoch: 180 [192768/225000 (86%)] Loss: 8252.585938\n",
      "Train Epoch: 180 [196864/225000 (87%)] Loss: 7971.419922\n",
      "Train Epoch: 180 [200960/225000 (89%)] Loss: 9695.707031\n",
      "Train Epoch: 180 [205056/225000 (91%)] Loss: 8235.347656\n",
      "Train Epoch: 180 [209152/225000 (93%)] Loss: 12543.705078\n",
      "Train Epoch: 180 [213248/225000 (95%)] Loss: 9747.476562\n",
      "Train Epoch: 180 [217344/225000 (97%)] Loss: 10518.910156\n",
      "Train Epoch: 180 [221440/225000 (98%)] Loss: 9760.597656\n",
      "    epoch          : 180\n",
      "    loss           : 9648.875809913608\n",
      "    val_loss       : 9236.815553066683\n",
      "Train Epoch: 181 [256/225000 (0%)] Loss: 7969.041016\n",
      "Train Epoch: 181 [4352/225000 (2%)] Loss: 8087.908203\n",
      "Train Epoch: 181 [8448/225000 (4%)] Loss: 9683.291016\n",
      "Train Epoch: 181 [12544/225000 (6%)] Loss: 8073.722656\n",
      "Train Epoch: 181 [16640/225000 (7%)] Loss: 10383.791016\n",
      "Train Epoch: 181 [20736/225000 (9%)] Loss: 10541.992188\n",
      "Train Epoch: 181 [24832/225000 (11%)] Loss: 9586.675781\n",
      "Train Epoch: 181 [28928/225000 (13%)] Loss: 8108.281250\n",
      "Train Epoch: 181 [33024/225000 (15%)] Loss: 8168.330078\n",
      "Train Epoch: 181 [37120/225000 (16%)] Loss: 8147.912109\n",
      "Train Epoch: 181 [41216/225000 (18%)] Loss: 8133.851562\n",
      "Train Epoch: 181 [45312/225000 (20%)] Loss: 8145.861328\n",
      "Train Epoch: 181 [49408/225000 (22%)] Loss: 9821.115234\n",
      "Train Epoch: 181 [53504/225000 (24%)] Loss: 9698.287109\n",
      "Train Epoch: 181 [57600/225000 (26%)] Loss: 10402.642578\n",
      "Train Epoch: 181 [61696/225000 (27%)] Loss: 7835.519531\n",
      "Train Epoch: 181 [65792/225000 (29%)] Loss: 12337.582031\n",
      "Train Epoch: 181 [69888/225000 (31%)] Loss: 8025.160156\n",
      "Train Epoch: 181 [73984/225000 (33%)] Loss: 8115.589844\n",
      "Train Epoch: 181 [78080/225000 (35%)] Loss: 8016.867188\n",
      "Train Epoch: 181 [82176/225000 (37%)] Loss: 8089.255859\n",
      "Train Epoch: 181 [86272/225000 (38%)] Loss: 8282.253906\n",
      "Train Epoch: 181 [90368/225000 (40%)] Loss: 8296.460938\n",
      "Train Epoch: 181 [94464/225000 (42%)] Loss: 9629.625000\n",
      "Train Epoch: 181 [98560/225000 (44%)] Loss: 8028.757812\n",
      "Train Epoch: 181 [102656/225000 (46%)] Loss: 8087.923828\n",
      "Train Epoch: 181 [106752/225000 (47%)] Loss: 8140.863281\n",
      "Train Epoch: 181 [110848/225000 (49%)] Loss: 8020.144531\n",
      "Train Epoch: 181 [114944/225000 (51%)] Loss: 8139.771484\n",
      "Train Epoch: 181 [119040/225000 (53%)] Loss: 9593.205078\n",
      "Train Epoch: 181 [123136/225000 (55%)] Loss: 8031.304688\n",
      "Train Epoch: 181 [127232/225000 (57%)] Loss: 13695.130859\n",
      "Train Epoch: 181 [131328/225000 (58%)] Loss: 12475.486328\n",
      "Train Epoch: 181 [135424/225000 (60%)] Loss: 13098.892578\n",
      "Train Epoch: 181 [139520/225000 (62%)] Loss: 8060.556641\n",
      "Train Epoch: 181 [143616/225000 (64%)] Loss: 8051.345703\n",
      "Train Epoch: 181 [147712/225000 (66%)] Loss: 8152.417969\n",
      "Train Epoch: 181 [151808/225000 (67%)] Loss: 8003.488281\n",
      "Train Epoch: 181 [155904/225000 (69%)] Loss: 7981.494141\n",
      "Train Epoch: 181 [160000/225000 (71%)] Loss: 7990.941406\n",
      "Train Epoch: 181 [164096/225000 (73%)] Loss: 8020.443359\n",
      "Train Epoch: 181 [168192/225000 (75%)] Loss: 8036.878906\n",
      "Train Epoch: 181 [172288/225000 (77%)] Loss: 7963.953125\n",
      "Train Epoch: 181 [176384/225000 (78%)] Loss: 7927.275391\n",
      "Train Epoch: 181 [180480/225000 (80%)] Loss: 10676.957031\n",
      "Train Epoch: 181 [184576/225000 (82%)] Loss: 12216.037109\n",
      "Train Epoch: 181 [188672/225000 (84%)] Loss: 7861.552734\n",
      "Train Epoch: 181 [192768/225000 (86%)] Loss: 8034.064453\n",
      "Train Epoch: 181 [196864/225000 (87%)] Loss: 8064.519531\n",
      "Train Epoch: 181 [200960/225000 (89%)] Loss: 8092.152344\n",
      "Train Epoch: 181 [205056/225000 (91%)] Loss: 7831.511719\n",
      "Train Epoch: 181 [209152/225000 (93%)] Loss: 12365.359375\n",
      "Train Epoch: 181 [213248/225000 (95%)] Loss: 7991.515625\n",
      "Train Epoch: 181 [217344/225000 (97%)] Loss: 9609.376953\n",
      "Train Epoch: 181 [221440/225000 (98%)] Loss: 9504.154297\n",
      "    epoch          : 181\n",
      "    loss           : 9500.486946947882\n",
      "    val_loss       : 9445.061352581393\n",
      "Train Epoch: 182 [256/225000 (0%)] Loss: 8030.310547\n",
      "Train Epoch: 182 [4352/225000 (2%)] Loss: 11903.769531\n",
      "Train Epoch: 182 [8448/225000 (4%)] Loss: 13579.896484\n",
      "Train Epoch: 182 [12544/225000 (6%)] Loss: 10469.958984\n",
      "Train Epoch: 182 [16640/225000 (7%)] Loss: 8047.841797\n",
      "Train Epoch: 182 [20736/225000 (9%)] Loss: 13779.076172\n",
      "Train Epoch: 182 [24832/225000 (11%)] Loss: 8088.312500\n",
      "Train Epoch: 182 [28928/225000 (13%)] Loss: 15497.625000\n",
      "Train Epoch: 182 [33024/225000 (15%)] Loss: 7892.458984\n",
      "Train Epoch: 182 [37120/225000 (16%)] Loss: 7884.335938\n",
      "Train Epoch: 182 [41216/225000 (18%)] Loss: 8116.623047\n",
      "Train Epoch: 182 [45312/225000 (20%)] Loss: 12881.023438\n",
      "Train Epoch: 182 [49408/225000 (22%)] Loss: 8116.210938\n",
      "Train Epoch: 182 [53504/225000 (24%)] Loss: 13968.582031\n",
      "Train Epoch: 182 [57600/225000 (26%)] Loss: 8017.148438\n",
      "Train Epoch: 182 [61696/225000 (27%)] Loss: 12339.779297\n",
      "Train Epoch: 182 [65792/225000 (29%)] Loss: 10388.818359\n",
      "Train Epoch: 182 [69888/225000 (31%)] Loss: 10560.494141\n",
      "Train Epoch: 182 [73984/225000 (33%)] Loss: 8064.970703\n",
      "Train Epoch: 182 [78080/225000 (35%)] Loss: 8057.996094\n",
      "Train Epoch: 182 [82176/225000 (37%)] Loss: 8184.253906\n",
      "Train Epoch: 182 [86272/225000 (38%)] Loss: 13578.863281\n",
      "Train Epoch: 182 [90368/225000 (40%)] Loss: 8016.574219\n",
      "Train Epoch: 182 [94464/225000 (42%)] Loss: 7993.771484\n",
      "Train Epoch: 182 [98560/225000 (44%)] Loss: 12443.265625\n",
      "Train Epoch: 182 [102656/225000 (46%)] Loss: 13980.593750\n",
      "Train Epoch: 182 [106752/225000 (47%)] Loss: 9369.339844\n",
      "Train Epoch: 182 [110848/225000 (49%)] Loss: 9620.437500\n",
      "Train Epoch: 182 [114944/225000 (51%)] Loss: 8014.425781\n",
      "Train Epoch: 182 [119040/225000 (53%)] Loss: 7855.039062\n",
      "Train Epoch: 182 [123136/225000 (55%)] Loss: 8124.478516\n",
      "Train Epoch: 182 [127232/225000 (57%)] Loss: 7944.746094\n",
      "Train Epoch: 182 [131328/225000 (58%)] Loss: 8140.394531\n",
      "Train Epoch: 182 [135424/225000 (60%)] Loss: 7865.593750\n",
      "Train Epoch: 182 [139520/225000 (62%)] Loss: 14205.283203\n",
      "Train Epoch: 182 [143616/225000 (64%)] Loss: 9723.619141\n",
      "Train Epoch: 182 [147712/225000 (66%)] Loss: 8005.998047\n",
      "Train Epoch: 182 [151808/225000 (67%)] Loss: 8227.718750\n",
      "Train Epoch: 182 [155904/225000 (69%)] Loss: 8055.162109\n",
      "Train Epoch: 182 [160000/225000 (71%)] Loss: 8080.111328\n",
      "Train Epoch: 182 [164096/225000 (73%)] Loss: 9863.867188\n",
      "Train Epoch: 182 [168192/225000 (75%)] Loss: 9469.863281\n",
      "Train Epoch: 182 [172288/225000 (77%)] Loss: 9523.914062\n",
      "Train Epoch: 182 [176384/225000 (78%)] Loss: 9864.931641\n",
      "Train Epoch: 182 [180480/225000 (80%)] Loss: 12260.558594\n",
      "Train Epoch: 182 [184576/225000 (82%)] Loss: 8138.589844\n",
      "Train Epoch: 182 [188672/225000 (84%)] Loss: 8062.806641\n",
      "Train Epoch: 182 [192768/225000 (86%)] Loss: 8254.035156\n",
      "Train Epoch: 182 [196864/225000 (87%)] Loss: 8073.060547\n",
      "Train Epoch: 182 [200960/225000 (89%)] Loss: 9656.289062\n",
      "Train Epoch: 182 [205056/225000 (91%)] Loss: 8026.734375\n",
      "Train Epoch: 182 [209152/225000 (93%)] Loss: 9570.865234\n",
      "Train Epoch: 182 [213248/225000 (95%)] Loss: 13144.099609\n",
      "Train Epoch: 182 [217344/225000 (97%)] Loss: 10582.175781\n",
      "Train Epoch: 182 [221440/225000 (98%)] Loss: 10749.662109\n",
      "    epoch          : 182\n",
      "    loss           : 9594.45675794582\n",
      "    val_loss       : 9410.325193996332\n",
      "Train Epoch: 183 [256/225000 (0%)] Loss: 8090.029297\n",
      "Train Epoch: 183 [4352/225000 (2%)] Loss: 12462.439453\n",
      "Train Epoch: 183 [8448/225000 (4%)] Loss: 7963.378906\n",
      "Train Epoch: 183 [12544/225000 (6%)] Loss: 12881.802734\n",
      "Train Epoch: 183 [16640/225000 (7%)] Loss: 8185.576172\n",
      "Train Epoch: 183 [20736/225000 (9%)] Loss: 12572.248047\n",
      "Train Epoch: 183 [24832/225000 (11%)] Loss: 8061.679688\n",
      "Train Epoch: 183 [28928/225000 (13%)] Loss: 10431.779297\n",
      "Train Epoch: 183 [33024/225000 (15%)] Loss: 8022.845703\n",
      "Train Epoch: 183 [37120/225000 (16%)] Loss: 8209.876953\n",
      "Train Epoch: 183 [41216/225000 (18%)] Loss: 8067.744141\n",
      "Train Epoch: 183 [45312/225000 (20%)] Loss: 10797.117188\n",
      "Train Epoch: 183 [49408/225000 (22%)] Loss: 14609.714844\n",
      "Train Epoch: 183 [53504/225000 (24%)] Loss: 13995.423828\n",
      "Train Epoch: 183 [57600/225000 (26%)] Loss: 8002.187500\n",
      "Train Epoch: 183 [61696/225000 (27%)] Loss: 9477.941406\n",
      "Train Epoch: 183 [65792/225000 (29%)] Loss: 9715.001953\n",
      "Train Epoch: 183 [69888/225000 (31%)] Loss: 9620.878906\n",
      "Train Epoch: 183 [73984/225000 (33%)] Loss: 12169.281250\n",
      "Train Epoch: 183 [78080/225000 (35%)] Loss: 9759.419922\n",
      "Train Epoch: 183 [82176/225000 (37%)] Loss: 8100.347656\n",
      "Train Epoch: 183 [86272/225000 (38%)] Loss: 7980.279297\n",
      "Train Epoch: 183 [90368/225000 (40%)] Loss: 8014.226562\n",
      "Train Epoch: 183 [94464/225000 (42%)] Loss: 12279.421875\n",
      "Train Epoch: 183 [98560/225000 (44%)] Loss: 7956.771484\n",
      "Train Epoch: 183 [102656/225000 (46%)] Loss: 12349.296875\n",
      "Train Epoch: 183 [106752/225000 (47%)] Loss: 9539.787109\n",
      "Train Epoch: 183 [110848/225000 (49%)] Loss: 9701.230469\n",
      "Train Epoch: 183 [114944/225000 (51%)] Loss: 8223.671875\n",
      "Train Epoch: 183 [119040/225000 (53%)] Loss: 8256.863281\n",
      "Train Epoch: 183 [123136/225000 (55%)] Loss: 9715.064453\n",
      "Train Epoch: 183 [127232/225000 (57%)] Loss: 8013.601562\n",
      "Train Epoch: 183 [131328/225000 (58%)] Loss: 7866.048828\n",
      "Train Epoch: 183 [135424/225000 (60%)] Loss: 7986.769531\n",
      "Train Epoch: 183 [139520/225000 (62%)] Loss: 7866.833984\n",
      "Train Epoch: 183 [143616/225000 (64%)] Loss: 9603.042969\n",
      "Train Epoch: 183 [147712/225000 (66%)] Loss: 10649.523438\n",
      "Train Epoch: 183 [151808/225000 (67%)] Loss: 8251.269531\n",
      "Train Epoch: 183 [155904/225000 (69%)] Loss: 8263.556641\n",
      "Train Epoch: 183 [160000/225000 (71%)] Loss: 8157.576172\n",
      "Train Epoch: 183 [164096/225000 (73%)] Loss: 8102.941406\n",
      "Train Epoch: 183 [168192/225000 (75%)] Loss: 8283.091797\n",
      "Train Epoch: 183 [172288/225000 (77%)] Loss: 9370.625000\n",
      "Train Epoch: 183 [176384/225000 (78%)] Loss: 10375.501953\n",
      "Train Epoch: 183 [180480/225000 (80%)] Loss: 11310.156250\n",
      "Train Epoch: 183 [184576/225000 (82%)] Loss: 8048.587891\n",
      "Train Epoch: 183 [188672/225000 (84%)] Loss: 8153.064453\n",
      "Train Epoch: 183 [192768/225000 (86%)] Loss: 8133.601562\n",
      "Train Epoch: 183 [196864/225000 (87%)] Loss: 7893.261719\n",
      "Train Epoch: 183 [200960/225000 (89%)] Loss: 7920.091797\n",
      "Train Epoch: 183 [205056/225000 (91%)] Loss: 8133.886719\n",
      "Train Epoch: 183 [209152/225000 (93%)] Loss: 9631.410156\n",
      "Train Epoch: 183 [213248/225000 (95%)] Loss: 12378.359375\n",
      "Train Epoch: 183 [217344/225000 (97%)] Loss: 8104.398438\n",
      "Train Epoch: 183 [221440/225000 (98%)] Loss: 8016.185547\n",
      "    epoch          : 183\n",
      "    loss           : 9471.923001546502\n",
      "    val_loss       : 9697.253715614432\n",
      "Train Epoch: 184 [256/225000 (0%)] Loss: 11312.589844\n",
      "Train Epoch: 184 [4352/225000 (2%)] Loss: 8001.644531\n",
      "Train Epoch: 184 [8448/225000 (4%)] Loss: 8144.205078\n",
      "Train Epoch: 184 [12544/225000 (6%)] Loss: 10471.291016\n",
      "Train Epoch: 184 [16640/225000 (7%)] Loss: 7945.695312\n",
      "Train Epoch: 184 [20736/225000 (9%)] Loss: 8149.574219\n",
      "Train Epoch: 184 [24832/225000 (11%)] Loss: 11946.488281\n",
      "Train Epoch: 184 [28928/225000 (13%)] Loss: 13603.011719\n",
      "Train Epoch: 184 [33024/225000 (15%)] Loss: 8006.476562\n",
      "Train Epoch: 184 [37120/225000 (16%)] Loss: 9610.853516\n",
      "Train Epoch: 184 [41216/225000 (18%)] Loss: 9806.679688\n",
      "Train Epoch: 184 [45312/225000 (20%)] Loss: 8107.615234\n",
      "Train Epoch: 184 [49408/225000 (22%)] Loss: 9656.906250\n",
      "Train Epoch: 184 [53504/225000 (24%)] Loss: 10492.720703\n",
      "Train Epoch: 184 [57600/225000 (26%)] Loss: 8021.291016\n",
      "Train Epoch: 184 [61696/225000 (27%)] Loss: 8148.347656\n",
      "Train Epoch: 184 [65792/225000 (29%)] Loss: 7991.541016\n",
      "Train Epoch: 184 [69888/225000 (31%)] Loss: 8294.550781\n",
      "Train Epoch: 184 [73984/225000 (33%)] Loss: 10402.421875\n",
      "Train Epoch: 184 [78080/225000 (35%)] Loss: 9561.427734\n",
      "Train Epoch: 184 [82176/225000 (37%)] Loss: 8032.195312\n",
      "Train Epoch: 184 [86272/225000 (38%)] Loss: 8218.355469\n",
      "Train Epoch: 184 [90368/225000 (40%)] Loss: 7996.265625\n",
      "Train Epoch: 184 [94464/225000 (42%)] Loss: 8130.267578\n",
      "Train Epoch: 184 [98560/225000 (44%)] Loss: 7979.691406\n",
      "Train Epoch: 184 [102656/225000 (46%)] Loss: 10505.753906\n",
      "Train Epoch: 184 [106752/225000 (47%)] Loss: 7902.757812\n",
      "Train Epoch: 184 [110848/225000 (49%)] Loss: 8095.884766\n",
      "Train Epoch: 184 [114944/225000 (51%)] Loss: 8217.046875\n",
      "Train Epoch: 184 [119040/225000 (53%)] Loss: 8105.685547\n",
      "Train Epoch: 184 [123136/225000 (55%)] Loss: 8114.814453\n",
      "Train Epoch: 184 [127232/225000 (57%)] Loss: 7930.910156\n",
      "Train Epoch: 184 [131328/225000 (58%)] Loss: 8062.046875\n",
      "Train Epoch: 184 [135424/225000 (60%)] Loss: 13914.394531\n",
      "Train Epoch: 184 [139520/225000 (62%)] Loss: 8062.035156\n",
      "Train Epoch: 184 [143616/225000 (64%)] Loss: 13293.070312\n",
      "Train Epoch: 184 [147712/225000 (66%)] Loss: 8170.373047\n",
      "Train Epoch: 184 [151808/225000 (67%)] Loss: 8043.001953\n",
      "Train Epoch: 184 [155904/225000 (69%)] Loss: 8159.251953\n",
      "Train Epoch: 184 [160000/225000 (71%)] Loss: 9696.279297\n",
      "Train Epoch: 184 [164096/225000 (73%)] Loss: 8019.404297\n",
      "Train Epoch: 184 [168192/225000 (75%)] Loss: 8113.765625\n",
      "Train Epoch: 184 [172288/225000 (77%)] Loss: 19849.539062\n",
      "Train Epoch: 184 [176384/225000 (78%)] Loss: 18065.416016\n",
      "Train Epoch: 184 [180480/225000 (80%)] Loss: 8021.576172\n",
      "Train Epoch: 184 [184576/225000 (82%)] Loss: 13530.595703\n",
      "Train Epoch: 184 [188672/225000 (84%)] Loss: 8034.427734\n",
      "Train Epoch: 184 [192768/225000 (86%)] Loss: 11200.308594\n",
      "Train Epoch: 184 [196864/225000 (87%)] Loss: 13009.447266\n",
      "Train Epoch: 184 [200960/225000 (89%)] Loss: 7965.818359\n",
      "Train Epoch: 184 [205056/225000 (91%)] Loss: 9760.833984\n",
      "Train Epoch: 184 [209152/225000 (93%)] Loss: 15313.054688\n",
      "Train Epoch: 184 [213248/225000 (95%)] Loss: 8004.406250\n",
      "Train Epoch: 184 [217344/225000 (97%)] Loss: 8023.230469\n",
      "Train Epoch: 184 [221440/225000 (98%)] Loss: 10681.621094\n",
      "    epoch          : 184\n",
      "    loss           : 9471.434622529152\n",
      "    val_loss       : 10096.200634095134\n",
      "Train Epoch: 185 [256/225000 (0%)] Loss: 7942.447266\n",
      "Train Epoch: 185 [4352/225000 (2%)] Loss: 9725.701172\n",
      "Train Epoch: 185 [8448/225000 (4%)] Loss: 8217.382812\n",
      "Train Epoch: 185 [12544/225000 (6%)] Loss: 7948.062500\n",
      "Train Epoch: 185 [16640/225000 (7%)] Loss: 8051.027344\n",
      "Train Epoch: 185 [20736/225000 (9%)] Loss: 15409.724609\n",
      "Train Epoch: 185 [24832/225000 (11%)] Loss: 9688.333984\n",
      "Train Epoch: 185 [28928/225000 (13%)] Loss: 8006.751953\n",
      "Train Epoch: 185 [33024/225000 (15%)] Loss: 9871.699219\n",
      "Train Epoch: 185 [37120/225000 (16%)] Loss: 18602.025391\n",
      "Train Epoch: 185 [41216/225000 (18%)] Loss: 8208.916016\n",
      "Train Epoch: 185 [45312/225000 (20%)] Loss: 12304.376953\n",
      "Train Epoch: 185 [49408/225000 (22%)] Loss: 7920.900391\n",
      "Train Epoch: 185 [53504/225000 (24%)] Loss: 11290.523438\n",
      "Train Epoch: 185 [57600/225000 (26%)] Loss: 10423.796875\n",
      "Train Epoch: 185 [61696/225000 (27%)] Loss: 11098.785156\n",
      "Train Epoch: 185 [65792/225000 (29%)] Loss: 11354.691406\n",
      "Train Epoch: 185 [69888/225000 (31%)] Loss: 8244.537109\n",
      "Train Epoch: 185 [73984/225000 (33%)] Loss: 7921.699219\n",
      "Train Epoch: 185 [78080/225000 (35%)] Loss: 9633.984375\n",
      "Train Epoch: 185 [82176/225000 (37%)] Loss: 8108.800781\n",
      "Train Epoch: 185 [86272/225000 (38%)] Loss: 7857.181641\n",
      "Train Epoch: 185 [90368/225000 (40%)] Loss: 8094.335938\n",
      "Train Epoch: 185 [94464/225000 (42%)] Loss: 8113.531250\n",
      "Train Epoch: 185 [98560/225000 (44%)] Loss: 12750.435547\n",
      "Train Epoch: 185 [102656/225000 (46%)] Loss: 13903.376953\n",
      "Train Epoch: 185 [106752/225000 (47%)] Loss: 8125.185547\n",
      "Train Epoch: 185 [110848/225000 (49%)] Loss: 10547.177734\n",
      "Train Epoch: 185 [114944/225000 (51%)] Loss: 10425.175781\n",
      "Train Epoch: 185 [119040/225000 (53%)] Loss: 9811.761719\n",
      "Train Epoch: 185 [123136/225000 (55%)] Loss: 11263.779297\n",
      "Train Epoch: 185 [127232/225000 (57%)] Loss: 12463.884766\n",
      "Train Epoch: 185 [131328/225000 (58%)] Loss: 7972.605469\n",
      "Train Epoch: 185 [135424/225000 (60%)] Loss: 8071.736328\n",
      "Train Epoch: 185 [139520/225000 (62%)] Loss: 7936.748047\n",
      "Train Epoch: 185 [143616/225000 (64%)] Loss: 8069.367188\n",
      "Train Epoch: 185 [147712/225000 (66%)] Loss: 9759.998047\n",
      "Train Epoch: 185 [151808/225000 (67%)] Loss: 13661.195312\n",
      "Train Epoch: 185 [155904/225000 (69%)] Loss: 12114.595703\n",
      "Train Epoch: 185 [160000/225000 (71%)] Loss: 9413.380859\n",
      "Train Epoch: 185 [164096/225000 (73%)] Loss: 8207.214844\n",
      "Train Epoch: 185 [168192/225000 (75%)] Loss: 12455.378906\n",
      "Train Epoch: 185 [172288/225000 (77%)] Loss: 10714.326172\n",
      "Train Epoch: 185 [176384/225000 (78%)] Loss: 8000.435547\n",
      "Train Epoch: 185 [180480/225000 (80%)] Loss: 8088.804688\n",
      "Train Epoch: 185 [184576/225000 (82%)] Loss: 12667.880859\n",
      "Train Epoch: 185 [188672/225000 (84%)] Loss: 9617.589844\n",
      "Train Epoch: 185 [192768/225000 (86%)] Loss: 7957.576172\n",
      "Train Epoch: 185 [196864/225000 (87%)] Loss: 7957.228516\n",
      "Train Epoch: 185 [200960/225000 (89%)] Loss: 8187.332031\n",
      "Train Epoch: 185 [205056/225000 (91%)] Loss: 9565.710938\n",
      "Train Epoch: 185 [209152/225000 (93%)] Loss: 8190.355469\n",
      "Train Epoch: 185 [213248/225000 (95%)] Loss: 9678.093750\n",
      "Train Epoch: 185 [217344/225000 (97%)] Loss: 8108.011719\n",
      "Train Epoch: 185 [221440/225000 (98%)] Loss: 7958.400391\n",
      "    epoch          : 185\n",
      "    loss           : 9622.292377701933\n",
      "    val_loss       : 9939.96534330261\n",
      "Train Epoch: 186 [256/225000 (0%)] Loss: 10628.044922\n",
      "Train Epoch: 186 [4352/225000 (2%)] Loss: 10546.208984\n",
      "Train Epoch: 186 [8448/225000 (4%)] Loss: 9558.951172\n",
      "Train Epoch: 186 [12544/225000 (6%)] Loss: 9680.710938\n",
      "Train Epoch: 186 [16640/225000 (7%)] Loss: 10308.984375\n",
      "Train Epoch: 186 [20736/225000 (9%)] Loss: 11902.324219\n",
      "Train Epoch: 186 [24832/225000 (11%)] Loss: 7907.765625\n",
      "Train Epoch: 186 [28928/225000 (13%)] Loss: 9663.720703\n",
      "Train Epoch: 186 [33024/225000 (15%)] Loss: 10471.027344\n",
      "Train Epoch: 186 [37120/225000 (16%)] Loss: 9644.472656\n",
      "Train Epoch: 186 [41216/225000 (18%)] Loss: 8140.337891\n",
      "Train Epoch: 186 [45312/225000 (20%)] Loss: 8069.974609\n",
      "Train Epoch: 186 [49408/225000 (22%)] Loss: 10234.894531\n",
      "Train Epoch: 186 [53504/225000 (24%)] Loss: 15317.107422\n",
      "Train Epoch: 186 [57600/225000 (26%)] Loss: 8232.638672\n",
      "Train Epoch: 186 [61696/225000 (27%)] Loss: 9714.722656\n",
      "Train Epoch: 186 [65792/225000 (29%)] Loss: 9806.312500\n",
      "Train Epoch: 186 [69888/225000 (31%)] Loss: 7978.300781\n",
      "Train Epoch: 186 [73984/225000 (33%)] Loss: 8200.000000\n",
      "Train Epoch: 186 [78080/225000 (35%)] Loss: 15163.074219\n",
      "Train Epoch: 186 [82176/225000 (37%)] Loss: 8058.654297\n",
      "Train Epoch: 186 [86272/225000 (38%)] Loss: 8028.173828\n",
      "Train Epoch: 186 [90368/225000 (40%)] Loss: 9604.144531\n",
      "Train Epoch: 186 [94464/225000 (42%)] Loss: 8471.019531\n",
      "Train Epoch: 186 [98560/225000 (44%)] Loss: 9500.119141\n",
      "Train Epoch: 186 [102656/225000 (46%)] Loss: 8129.908203\n",
      "Train Epoch: 186 [106752/225000 (47%)] Loss: 12826.970703\n",
      "Train Epoch: 186 [110848/225000 (49%)] Loss: 7683.302734\n",
      "Train Epoch: 186 [114944/225000 (51%)] Loss: 10427.695312\n",
      "Train Epoch: 186 [119040/225000 (53%)] Loss: 15334.515625\n",
      "Train Epoch: 186 [123136/225000 (55%)] Loss: 8241.150391\n",
      "Train Epoch: 186 [127232/225000 (57%)] Loss: 13619.080078\n",
      "Train Epoch: 186 [131328/225000 (58%)] Loss: 8162.410156\n",
      "Train Epoch: 186 [135424/225000 (60%)] Loss: 13596.371094\n",
      "Train Epoch: 186 [139520/225000 (62%)] Loss: 12417.275391\n",
      "Train Epoch: 186 [143616/225000 (64%)] Loss: 10724.240234\n",
      "Train Epoch: 186 [147712/225000 (66%)] Loss: 8136.292969\n",
      "Train Epoch: 186 [151808/225000 (67%)] Loss: 9537.753906\n",
      "Train Epoch: 186 [155904/225000 (69%)] Loss: 10460.310547\n",
      "Train Epoch: 186 [160000/225000 (71%)] Loss: 13012.251953\n",
      "Train Epoch: 186 [164096/225000 (73%)] Loss: 7909.833984\n",
      "Train Epoch: 186 [168192/225000 (75%)] Loss: 17288.751953\n",
      "Train Epoch: 186 [172288/225000 (77%)] Loss: 9643.238281\n",
      "Train Epoch: 186 [176384/225000 (78%)] Loss: 8029.437500\n",
      "Train Epoch: 186 [180480/225000 (80%)] Loss: 8169.386719\n",
      "Train Epoch: 186 [184576/225000 (82%)] Loss: 8131.636719\n",
      "Train Epoch: 186 [188672/225000 (84%)] Loss: 8042.718750\n",
      "Train Epoch: 186 [192768/225000 (86%)] Loss: 8053.406250\n",
      "Train Epoch: 186 [196864/225000 (87%)] Loss: 8174.087891\n",
      "Train Epoch: 186 [200960/225000 (89%)] Loss: 8025.962891\n",
      "Train Epoch: 186 [205056/225000 (91%)] Loss: 14933.308594\n",
      "Train Epoch: 186 [209152/225000 (93%)] Loss: 8074.378906\n",
      "Train Epoch: 186 [213248/225000 (95%)] Loss: 8185.396484\n",
      "Train Epoch: 186 [217344/225000 (97%)] Loss: 9530.193359\n",
      "Train Epoch: 186 [221440/225000 (98%)] Loss: 10478.230469\n",
      "    epoch          : 186\n",
      "    loss           : 9595.75147984215\n",
      "    val_loss       : 9669.210387087598\n",
      "Train Epoch: 187 [256/225000 (0%)] Loss: 8026.708984\n",
      "Train Epoch: 187 [4352/225000 (2%)] Loss: 12325.015625\n",
      "Train Epoch: 187 [8448/225000 (4%)] Loss: 15170.933594\n",
      "Train Epoch: 187 [12544/225000 (6%)] Loss: 8045.224609\n",
      "Train Epoch: 187 [16640/225000 (7%)] Loss: 8184.398438\n",
      "Train Epoch: 187 [20736/225000 (9%)] Loss: 8091.310547\n",
      "Train Epoch: 187 [24832/225000 (11%)] Loss: 7939.203125\n",
      "Train Epoch: 187 [28928/225000 (13%)] Loss: 10545.873047\n",
      "Train Epoch: 187 [33024/225000 (15%)] Loss: 13566.031250\n",
      "Train Epoch: 187 [37120/225000 (16%)] Loss: 7994.097656\n",
      "Train Epoch: 187 [41216/225000 (18%)] Loss: 8019.148438\n",
      "Train Epoch: 187 [45312/225000 (20%)] Loss: 12784.474609\n",
      "Train Epoch: 187 [49408/225000 (22%)] Loss: 7827.400391\n",
      "Train Epoch: 187 [53504/225000 (24%)] Loss: 8013.328125\n",
      "Train Epoch: 187 [57600/225000 (26%)] Loss: 9535.410156\n",
      "Train Epoch: 187 [61696/225000 (27%)] Loss: 13555.291016\n",
      "Train Epoch: 187 [65792/225000 (29%)] Loss: 9498.835938\n",
      "Train Epoch: 187 [69888/225000 (31%)] Loss: 8074.835938\n",
      "Train Epoch: 187 [73984/225000 (33%)] Loss: 13820.843750\n",
      "Train Epoch: 187 [78080/225000 (35%)] Loss: 9673.603516\n",
      "Train Epoch: 187 [82176/225000 (37%)] Loss: 8019.210938\n",
      "Train Epoch: 187 [86272/225000 (38%)] Loss: 7880.802734\n",
      "Train Epoch: 187 [90368/225000 (40%)] Loss: 8288.189453\n",
      "Train Epoch: 187 [94464/225000 (42%)] Loss: 8246.666016\n",
      "Train Epoch: 187 [98560/225000 (44%)] Loss: 8037.880859\n",
      "Train Epoch: 187 [102656/225000 (46%)] Loss: 8013.806641\n",
      "Train Epoch: 187 [106752/225000 (47%)] Loss: 7972.884766\n",
      "Train Epoch: 187 [110848/225000 (49%)] Loss: 9487.082031\n",
      "Train Epoch: 187 [114944/225000 (51%)] Loss: 8171.097656\n",
      "Train Epoch: 187 [119040/225000 (53%)] Loss: 8102.085938\n",
      "Train Epoch: 187 [123136/225000 (55%)] Loss: 7960.466797\n",
      "Train Epoch: 187 [127232/225000 (57%)] Loss: 11314.746094\n",
      "Train Epoch: 187 [131328/225000 (58%)] Loss: 8021.242188\n",
      "Train Epoch: 187 [135424/225000 (60%)] Loss: 7953.429688\n",
      "Train Epoch: 187 [139520/225000 (62%)] Loss: 10547.431641\n",
      "Train Epoch: 187 [143616/225000 (64%)] Loss: 9729.218750\n",
      "Train Epoch: 187 [147712/225000 (66%)] Loss: 8120.341797\n",
      "Train Epoch: 187 [151808/225000 (67%)] Loss: 10639.550781\n",
      "Train Epoch: 187 [155904/225000 (69%)] Loss: 8179.683594\n",
      "Train Epoch: 187 [160000/225000 (71%)] Loss: 10513.248047\n",
      "Train Epoch: 187 [164096/225000 (73%)] Loss: 8148.687500\n",
      "Train Epoch: 187 [168192/225000 (75%)] Loss: 8025.837891\n",
      "Train Epoch: 187 [172288/225000 (77%)] Loss: 8057.035156\n",
      "Train Epoch: 187 [176384/225000 (78%)] Loss: 8054.234375\n",
      "Train Epoch: 187 [180480/225000 (80%)] Loss: 10498.572266\n",
      "Train Epoch: 187 [184576/225000 (82%)] Loss: 8012.486328\n",
      "Train Epoch: 187 [188672/225000 (84%)] Loss: 12065.480469\n",
      "Train Epoch: 187 [192768/225000 (86%)] Loss: 8086.773438\n",
      "Train Epoch: 187 [196864/225000 (87%)] Loss: 8131.027344\n",
      "Train Epoch: 187 [200960/225000 (89%)] Loss: 7946.019531\n",
      "Train Epoch: 187 [205056/225000 (91%)] Loss: 9729.994141\n",
      "Train Epoch: 187 [209152/225000 (93%)] Loss: 13057.138672\n",
      "Train Epoch: 187 [213248/225000 (95%)] Loss: 8010.830078\n",
      "Train Epoch: 187 [217344/225000 (97%)] Loss: 9610.320312\n",
      "Train Epoch: 187 [221440/225000 (98%)] Loss: 7969.033203\n",
      "    epoch          : 187\n",
      "    loss           : 9523.221747458048\n",
      "    val_loss       : 9414.262044320301\n",
      "Train Epoch: 188 [256/225000 (0%)] Loss: 8135.060547\n",
      "Train Epoch: 188 [4352/225000 (2%)] Loss: 8026.814453\n",
      "Train Epoch: 188 [8448/225000 (4%)] Loss: 7872.658203\n",
      "Train Epoch: 188 [12544/225000 (6%)] Loss: 8180.019531\n",
      "Train Epoch: 188 [16640/225000 (7%)] Loss: 12892.142578\n",
      "Train Epoch: 188 [20736/225000 (9%)] Loss: 8102.283203\n",
      "Train Epoch: 188 [24832/225000 (11%)] Loss: 8113.574219\n",
      "Train Epoch: 188 [28928/225000 (13%)] Loss: 9592.601562\n",
      "Train Epoch: 188 [33024/225000 (15%)] Loss: 12358.863281\n",
      "Train Epoch: 188 [37120/225000 (16%)] Loss: 8334.248047\n",
      "Train Epoch: 188 [41216/225000 (18%)] Loss: 9682.804688\n",
      "Train Epoch: 188 [45312/225000 (20%)] Loss: 9593.550781\n",
      "Train Epoch: 188 [49408/225000 (22%)] Loss: 10451.542969\n",
      "Train Epoch: 188 [53504/225000 (24%)] Loss: 10577.193359\n",
      "Train Epoch: 188 [57600/225000 (26%)] Loss: 11136.537109\n",
      "Train Epoch: 188 [61696/225000 (27%)] Loss: 8226.066406\n",
      "Train Epoch: 188 [65792/225000 (29%)] Loss: 7915.968750\n",
      "Train Epoch: 188 [69888/225000 (31%)] Loss: 8142.744141\n",
      "Train Epoch: 188 [73984/225000 (33%)] Loss: 8086.009766\n",
      "Train Epoch: 188 [78080/225000 (35%)] Loss: 9572.988281\n",
      "Train Epoch: 188 [82176/225000 (37%)] Loss: 7966.019531\n",
      "Train Epoch: 188 [86272/225000 (38%)] Loss: 8182.527344\n",
      "Train Epoch: 188 [90368/225000 (40%)] Loss: 7976.037109\n",
      "Train Epoch: 188 [94464/225000 (42%)] Loss: 8046.916016\n",
      "Train Epoch: 188 [98560/225000 (44%)] Loss: 10481.810547\n",
      "Train Epoch: 188 [102656/225000 (46%)] Loss: 7724.126953\n",
      "Train Epoch: 188 [106752/225000 (47%)] Loss: 7926.640625\n",
      "Train Epoch: 188 [110848/225000 (49%)] Loss: 10521.626953\n",
      "Train Epoch: 188 [114944/225000 (51%)] Loss: 8040.638672\n",
      "Train Epoch: 188 [119040/225000 (53%)] Loss: 8084.781250\n",
      "Train Epoch: 188 [123136/225000 (55%)] Loss: 8243.349609\n",
      "Train Epoch: 188 [127232/225000 (57%)] Loss: 13554.759766\n",
      "Train Epoch: 188 [131328/225000 (58%)] Loss: 8075.400391\n",
      "Train Epoch: 188 [135424/225000 (60%)] Loss: 14603.943359\n",
      "Train Epoch: 188 [139520/225000 (62%)] Loss: 8014.230469\n",
      "Train Epoch: 188 [143616/225000 (64%)] Loss: 10528.259766\n",
      "Train Epoch: 188 [147712/225000 (66%)] Loss: 9628.458984\n",
      "Train Epoch: 188 [151808/225000 (67%)] Loss: 13377.490234\n",
      "Train Epoch: 188 [155904/225000 (69%)] Loss: 10539.060547\n",
      "Train Epoch: 188 [160000/225000 (71%)] Loss: 8063.433594\n",
      "Train Epoch: 188 [164096/225000 (73%)] Loss: 7776.716797\n",
      "Train Epoch: 188 [168192/225000 (75%)] Loss: 15617.517578\n",
      "Train Epoch: 188 [172288/225000 (77%)] Loss: 14827.189453\n",
      "Train Epoch: 188 [176384/225000 (78%)] Loss: 8133.847656\n",
      "Train Epoch: 188 [180480/225000 (80%)] Loss: 12148.730469\n",
      "Train Epoch: 188 [184576/225000 (82%)] Loss: 12625.132812\n",
      "Train Epoch: 188 [188672/225000 (84%)] Loss: 8057.388672\n",
      "Train Epoch: 188 [192768/225000 (86%)] Loss: 15484.445312\n",
      "Train Epoch: 188 [196864/225000 (87%)] Loss: 8159.208984\n",
      "Train Epoch: 188 [200960/225000 (89%)] Loss: 8159.257812\n",
      "Train Epoch: 188 [205056/225000 (91%)] Loss: 7925.208984\n",
      "Train Epoch: 188 [209152/225000 (93%)] Loss: 8071.791016\n",
      "Train Epoch: 188 [213248/225000 (95%)] Loss: 8016.152344\n",
      "Train Epoch: 188 [217344/225000 (97%)] Loss: 8160.794922\n",
      "Train Epoch: 188 [221440/225000 (98%)] Loss: 8036.960938\n",
      "    epoch          : 188\n",
      "    loss           : 9499.765707213453\n",
      "    val_loss       : 9635.90126474293\n",
      "Train Epoch: 189 [256/225000 (0%)] Loss: 7989.576172\n",
      "Train Epoch: 189 [4352/225000 (2%)] Loss: 8091.902344\n",
      "Train Epoch: 189 [8448/225000 (4%)] Loss: 13580.095703\n",
      "Train Epoch: 189 [12544/225000 (6%)] Loss: 8020.269531\n",
      "Train Epoch: 189 [16640/225000 (7%)] Loss: 9715.513672\n",
      "Train Epoch: 189 [20736/225000 (9%)] Loss: 9671.984375\n",
      "Train Epoch: 189 [24832/225000 (11%)] Loss: 7824.330078\n",
      "Train Epoch: 189 [28928/225000 (13%)] Loss: 7965.666016\n",
      "Train Epoch: 189 [33024/225000 (15%)] Loss: 8000.263672\n",
      "Train Epoch: 189 [37120/225000 (16%)] Loss: 9815.023438\n",
      "Train Epoch: 189 [41216/225000 (18%)] Loss: 8031.835938\n",
      "Train Epoch: 189 [45312/225000 (20%)] Loss: 12485.289062\n",
      "Train Epoch: 189 [49408/225000 (22%)] Loss: 8021.867188\n",
      "Train Epoch: 189 [53504/225000 (24%)] Loss: 8105.535156\n",
      "Train Epoch: 189 [57600/225000 (26%)] Loss: 7844.849609\n",
      "Train Epoch: 189 [61696/225000 (27%)] Loss: 7959.150391\n",
      "Train Epoch: 189 [65792/225000 (29%)] Loss: 7820.425781\n",
      "Train Epoch: 189 [69888/225000 (31%)] Loss: 7889.771484\n",
      "Train Epoch: 189 [73984/225000 (33%)] Loss: 9648.421875\n",
      "Train Epoch: 189 [78080/225000 (35%)] Loss: 9605.796875\n",
      "Train Epoch: 189 [82176/225000 (37%)] Loss: 7833.152344\n",
      "Train Epoch: 189 [86272/225000 (38%)] Loss: 8001.298828\n",
      "Train Epoch: 189 [90368/225000 (40%)] Loss: 7969.138672\n",
      "Train Epoch: 189 [94464/225000 (42%)] Loss: 8334.566406\n",
      "Train Epoch: 189 [98560/225000 (44%)] Loss: 10640.716797\n",
      "Train Epoch: 189 [102656/225000 (46%)] Loss: 10691.601562\n",
      "Train Epoch: 189 [106752/225000 (47%)] Loss: 8258.308594\n",
      "Train Epoch: 189 [110848/225000 (49%)] Loss: 11933.103516\n",
      "Train Epoch: 189 [114944/225000 (51%)] Loss: 8170.496094\n",
      "Train Epoch: 189 [119040/225000 (53%)] Loss: 8011.861328\n",
      "Train Epoch: 189 [123136/225000 (55%)] Loss: 9607.957031\n",
      "Train Epoch: 189 [127232/225000 (57%)] Loss: 8061.080078\n",
      "Train Epoch: 189 [131328/225000 (58%)] Loss: 9441.833984\n",
      "Train Epoch: 189 [135424/225000 (60%)] Loss: 8029.199219\n",
      "Train Epoch: 189 [139520/225000 (62%)] Loss: 9526.771484\n",
      "Train Epoch: 189 [143616/225000 (64%)] Loss: 8023.337891\n",
      "Train Epoch: 189 [147712/225000 (66%)] Loss: 7927.466797\n",
      "Train Epoch: 189 [151808/225000 (67%)] Loss: 8092.158203\n",
      "Train Epoch: 189 [155904/225000 (69%)] Loss: 12888.785156\n",
      "Train Epoch: 189 [160000/225000 (71%)] Loss: 10435.671875\n",
      "Train Epoch: 189 [164096/225000 (73%)] Loss: 7946.320312\n",
      "Train Epoch: 189 [168192/225000 (75%)] Loss: 10341.820312\n",
      "Train Epoch: 189 [172288/225000 (77%)] Loss: 8277.660156\n",
      "Train Epoch: 189 [176384/225000 (78%)] Loss: 8238.345703\n",
      "Train Epoch: 189 [180480/225000 (80%)] Loss: 8126.042969\n",
      "Train Epoch: 189 [184576/225000 (82%)] Loss: 7964.435547\n",
      "Train Epoch: 189 [188672/225000 (84%)] Loss: 8115.757812\n",
      "Train Epoch: 189 [192768/225000 (86%)] Loss: 9566.486328\n",
      "Train Epoch: 189 [196864/225000 (87%)] Loss: 7992.507812\n",
      "Train Epoch: 189 [200960/225000 (89%)] Loss: 7987.099609\n",
      "Train Epoch: 189 [205056/225000 (91%)] Loss: 7975.765625\n",
      "Train Epoch: 189 [209152/225000 (93%)] Loss: 8263.037109\n",
      "Train Epoch: 189 [213248/225000 (95%)] Loss: 14034.429688\n",
      "Train Epoch: 189 [217344/225000 (97%)] Loss: 7993.876953\n",
      "Train Epoch: 189 [221440/225000 (98%)] Loss: 8356.757812\n",
      "    epoch          : 189\n",
      "    loss           : 9536.356987476891\n",
      "    val_loss       : 9530.033771203489\n",
      "Train Epoch: 190 [256/225000 (0%)] Loss: 8175.916016\n",
      "Train Epoch: 190 [4352/225000 (2%)] Loss: 7996.175781\n",
      "Train Epoch: 190 [8448/225000 (4%)] Loss: 8269.279297\n",
      "Train Epoch: 190 [12544/225000 (6%)] Loss: 11756.863281\n",
      "Train Epoch: 190 [16640/225000 (7%)] Loss: 8051.191406\n",
      "Train Epoch: 190 [20736/225000 (9%)] Loss: 13979.794922\n",
      "Train Epoch: 190 [24832/225000 (11%)] Loss: 9513.767578\n",
      "Train Epoch: 190 [28928/225000 (13%)] Loss: 7955.158203\n",
      "Train Epoch: 190 [33024/225000 (15%)] Loss: 11431.667969\n",
      "Train Epoch: 190 [37120/225000 (16%)] Loss: 8090.152344\n",
      "Train Epoch: 190 [41216/225000 (18%)] Loss: 9681.941406\n",
      "Train Epoch: 190 [45312/225000 (20%)] Loss: 8045.080078\n",
      "Train Epoch: 190 [49408/225000 (22%)] Loss: 11048.199219\n",
      "Train Epoch: 190 [53504/225000 (24%)] Loss: 9813.939453\n",
      "Train Epoch: 190 [57600/225000 (26%)] Loss: 8059.351562\n",
      "Train Epoch: 190 [61696/225000 (27%)] Loss: 9693.578125\n",
      "Train Epoch: 190 [65792/225000 (29%)] Loss: 12154.699219\n",
      "Train Epoch: 190 [69888/225000 (31%)] Loss: 10523.183594\n",
      "Train Epoch: 190 [73984/225000 (33%)] Loss: 8159.699219\n",
      "Train Epoch: 190 [78080/225000 (35%)] Loss: 8118.648438\n",
      "Train Epoch: 190 [82176/225000 (37%)] Loss: 10565.933594\n",
      "Train Epoch: 190 [86272/225000 (38%)] Loss: 9502.460938\n",
      "Train Epoch: 190 [90368/225000 (40%)] Loss: 11175.535156\n",
      "Train Epoch: 190 [94464/225000 (42%)] Loss: 8197.705078\n",
      "Train Epoch: 190 [98560/225000 (44%)] Loss: 8026.097656\n",
      "Train Epoch: 190 [102656/225000 (46%)] Loss: 9530.878906\n",
      "Train Epoch: 190 [106752/225000 (47%)] Loss: 10656.460938\n",
      "Train Epoch: 190 [110848/225000 (49%)] Loss: 7954.117188\n",
      "Train Epoch: 190 [114944/225000 (51%)] Loss: 9858.996094\n",
      "Train Epoch: 190 [119040/225000 (53%)] Loss: 12386.767578\n",
      "Train Epoch: 190 [123136/225000 (55%)] Loss: 9814.054688\n",
      "Train Epoch: 190 [127232/225000 (57%)] Loss: 8164.287109\n",
      "Train Epoch: 190 [131328/225000 (58%)] Loss: 8020.560547\n",
      "Train Epoch: 190 [135424/225000 (60%)] Loss: 8026.767578\n",
      "Train Epoch: 190 [139520/225000 (62%)] Loss: 10474.751953\n",
      "Train Epoch: 190 [143616/225000 (64%)] Loss: 7955.267578\n",
      "Train Epoch: 190 [147712/225000 (66%)] Loss: 8075.843750\n",
      "Train Epoch: 190 [151808/225000 (67%)] Loss: 14978.025391\n",
      "Train Epoch: 190 [155904/225000 (69%)] Loss: 8150.152344\n",
      "Train Epoch: 190 [160000/225000 (71%)] Loss: 8175.050781\n",
      "Train Epoch: 190 [164096/225000 (73%)] Loss: 8057.623047\n",
      "Train Epoch: 190 [168192/225000 (75%)] Loss: 8103.216797\n",
      "Train Epoch: 190 [172288/225000 (77%)] Loss: 7959.431641\n",
      "Train Epoch: 190 [176384/225000 (78%)] Loss: 7861.447266\n",
      "Train Epoch: 190 [180480/225000 (80%)] Loss: 8109.730469\n",
      "Train Epoch: 190 [184576/225000 (82%)] Loss: 10683.628906\n",
      "Train Epoch: 190 [188672/225000 (84%)] Loss: 8054.310547\n",
      "Train Epoch: 190 [192768/225000 (86%)] Loss: 8020.070312\n",
      "Train Epoch: 190 [196864/225000 (87%)] Loss: 12411.445312\n",
      "Train Epoch: 190 [200960/225000 (89%)] Loss: 9621.253906\n",
      "Train Epoch: 190 [205056/225000 (91%)] Loss: 10569.291016\n",
      "Train Epoch: 190 [209152/225000 (93%)] Loss: 9475.314453\n",
      "Train Epoch: 190 [213248/225000 (95%)] Loss: 8019.257812\n",
      "Train Epoch: 190 [217344/225000 (97%)] Loss: 7975.822266\n",
      "Train Epoch: 190 [221440/225000 (98%)] Loss: 8159.052734\n",
      "    epoch          : 190\n",
      "    loss           : 9490.78737823521\n",
      "    val_loss       : 9696.90059244511\n",
      "Train Epoch: 191 [256/225000 (0%)] Loss: 8175.933594\n",
      "Train Epoch: 191 [4352/225000 (2%)] Loss: 14060.363281\n",
      "Train Epoch: 191 [8448/225000 (4%)] Loss: 9799.847656\n",
      "Train Epoch: 191 [12544/225000 (6%)] Loss: 20564.992188\n",
      "Train Epoch: 191 [16640/225000 (7%)] Loss: 8127.619141\n",
      "Train Epoch: 191 [20736/225000 (9%)] Loss: 8157.929688\n",
      "Train Epoch: 191 [24832/225000 (11%)] Loss: 8172.148438\n",
      "Train Epoch: 191 [28928/225000 (13%)] Loss: 8335.240234\n",
      "Train Epoch: 191 [33024/225000 (15%)] Loss: 9496.687500\n",
      "Train Epoch: 191 [37120/225000 (16%)] Loss: 8241.986328\n",
      "Train Epoch: 191 [41216/225000 (18%)] Loss: 7849.988281\n",
      "Train Epoch: 191 [45312/225000 (20%)] Loss: 8198.140625\n",
      "Train Epoch: 191 [49408/225000 (22%)] Loss: 9576.488281\n",
      "Train Epoch: 191 [53504/225000 (24%)] Loss: 9686.900391\n",
      "Train Epoch: 191 [57600/225000 (26%)] Loss: 13970.298828\n",
      "Train Epoch: 191 [61696/225000 (27%)] Loss: 7986.300781\n",
      "Train Epoch: 191 [65792/225000 (29%)] Loss: 14967.804688\n",
      "Train Epoch: 191 [69888/225000 (31%)] Loss: 8031.533203\n",
      "Train Epoch: 191 [73984/225000 (33%)] Loss: 8197.113281\n",
      "Train Epoch: 191 [78080/225000 (35%)] Loss: 9541.013672\n",
      "Train Epoch: 191 [82176/225000 (37%)] Loss: 10584.869141\n",
      "Train Epoch: 191 [86272/225000 (38%)] Loss: 8274.306641\n",
      "Train Epoch: 191 [90368/225000 (40%)] Loss: 12186.736328\n",
      "Train Epoch: 191 [94464/225000 (42%)] Loss: 9823.111328\n",
      "Train Epoch: 191 [98560/225000 (44%)] Loss: 8042.876953\n",
      "Train Epoch: 191 [102656/225000 (46%)] Loss: 7980.529297\n",
      "Train Epoch: 191 [106752/225000 (47%)] Loss: 9621.986328\n",
      "Train Epoch: 191 [110848/225000 (49%)] Loss: 13658.968750\n",
      "Train Epoch: 191 [114944/225000 (51%)] Loss: 8269.757812\n",
      "Train Epoch: 191 [119040/225000 (53%)] Loss: 13779.150391\n",
      "Train Epoch: 191 [123136/225000 (55%)] Loss: 7992.199219\n",
      "Train Epoch: 191 [127232/225000 (57%)] Loss: 9624.183594\n",
      "Train Epoch: 191 [131328/225000 (58%)] Loss: 13761.720703\n",
      "Train Epoch: 191 [135424/225000 (60%)] Loss: 7891.539062\n",
      "Train Epoch: 191 [139520/225000 (62%)] Loss: 8070.544922\n",
      "Train Epoch: 191 [143616/225000 (64%)] Loss: 13770.855469\n",
      "Train Epoch: 191 [147712/225000 (66%)] Loss: 7983.669922\n",
      "Train Epoch: 191 [151808/225000 (67%)] Loss: 9459.523438\n",
      "Train Epoch: 191 [155904/225000 (69%)] Loss: 9890.519531\n",
      "Train Epoch: 191 [160000/225000 (71%)] Loss: 9476.707031\n",
      "Train Epoch: 191 [164096/225000 (73%)] Loss: 8090.183594\n",
      "Train Epoch: 191 [168192/225000 (75%)] Loss: 8087.947266\n",
      "Train Epoch: 191 [172288/225000 (77%)] Loss: 7960.261719\n",
      "Train Epoch: 191 [176384/225000 (78%)] Loss: 8064.902344\n",
      "Train Epoch: 191 [180480/225000 (80%)] Loss: 8120.365234\n",
      "Train Epoch: 191 [184576/225000 (82%)] Loss: 10651.359375\n",
      "Train Epoch: 191 [188672/225000 (84%)] Loss: 7918.107422\n",
      "Train Epoch: 191 [192768/225000 (86%)] Loss: 12061.736328\n",
      "Train Epoch: 191 [196864/225000 (87%)] Loss: 8097.470703\n",
      "Train Epoch: 191 [200960/225000 (89%)] Loss: 8041.240234\n",
      "Train Epoch: 191 [205056/225000 (91%)] Loss: 8072.884766\n",
      "Train Epoch: 191 [209152/225000 (93%)] Loss: 19753.667969\n",
      "Train Epoch: 191 [213248/225000 (95%)] Loss: 7725.171875\n",
      "Train Epoch: 191 [217344/225000 (97%)] Loss: 9614.013672\n",
      "Train Epoch: 191 [221440/225000 (98%)] Loss: 8422.712891\n",
      "    epoch          : 191\n",
      "    loss           : 9600.798174861347\n",
      "    val_loss       : 9618.689610636964\n",
      "Train Epoch: 192 [256/225000 (0%)] Loss: 8103.691406\n",
      "Train Epoch: 192 [4352/225000 (2%)] Loss: 8122.484375\n",
      "Train Epoch: 192 [8448/225000 (4%)] Loss: 7907.921875\n",
      "Train Epoch: 192 [12544/225000 (6%)] Loss: 8287.230469\n",
      "Train Epoch: 192 [16640/225000 (7%)] Loss: 7945.070312\n",
      "Train Epoch: 192 [20736/225000 (9%)] Loss: 8047.535156\n",
      "Train Epoch: 192 [24832/225000 (11%)] Loss: 8186.021484\n",
      "Train Epoch: 192 [28928/225000 (13%)] Loss: 8159.283203\n",
      "Train Epoch: 192 [33024/225000 (15%)] Loss: 10526.187500\n",
      "Train Epoch: 192 [37120/225000 (16%)] Loss: 10709.796875\n",
      "Train Epoch: 192 [41216/225000 (18%)] Loss: 8168.478516\n",
      "Train Epoch: 192 [45312/225000 (20%)] Loss: 8075.484375\n",
      "Train Epoch: 192 [49408/225000 (22%)] Loss: 8214.167969\n",
      "Train Epoch: 192 [53504/225000 (24%)] Loss: 7890.162109\n",
      "Train Epoch: 192 [57600/225000 (26%)] Loss: 8181.625000\n",
      "Train Epoch: 192 [61696/225000 (27%)] Loss: 8203.636719\n",
      "Train Epoch: 192 [65792/225000 (29%)] Loss: 7901.794922\n",
      "Train Epoch: 192 [69888/225000 (31%)] Loss: 8116.861328\n",
      "Train Epoch: 192 [73984/225000 (33%)] Loss: 8074.527344\n",
      "Train Epoch: 192 [78080/225000 (35%)] Loss: 13771.109375\n",
      "Train Epoch: 192 [82176/225000 (37%)] Loss: 8007.613281\n",
      "Train Epoch: 192 [86272/225000 (38%)] Loss: 10554.333984\n",
      "Train Epoch: 192 [90368/225000 (40%)] Loss: 18545.871094\n",
      "Train Epoch: 192 [94464/225000 (42%)] Loss: 8210.244141\n",
      "Train Epoch: 192 [98560/225000 (44%)] Loss: 9669.007812\n",
      "Train Epoch: 192 [102656/225000 (46%)] Loss: 8020.273438\n",
      "Train Epoch: 192 [106752/225000 (47%)] Loss: 8027.576172\n",
      "Train Epoch: 192 [110848/225000 (49%)] Loss: 9755.597656\n",
      "Train Epoch: 192 [114944/225000 (51%)] Loss: 8206.761719\n",
      "Train Epoch: 192 [119040/225000 (53%)] Loss: 8108.582031\n",
      "Train Epoch: 192 [123136/225000 (55%)] Loss: 8018.583984\n",
      "Train Epoch: 192 [127232/225000 (57%)] Loss: 13691.537109\n",
      "Train Epoch: 192 [131328/225000 (58%)] Loss: 9587.951172\n",
      "Train Epoch: 192 [135424/225000 (60%)] Loss: 8099.578125\n",
      "Train Epoch: 192 [139520/225000 (62%)] Loss: 7944.359375\n",
      "Train Epoch: 192 [143616/225000 (64%)] Loss: 7980.902344\n",
      "Train Epoch: 192 [147712/225000 (66%)] Loss: 8034.792969\n",
      "Train Epoch: 192 [151808/225000 (67%)] Loss: 7919.837891\n",
      "Train Epoch: 192 [155904/225000 (69%)] Loss: 10766.515625\n",
      "Train Epoch: 192 [160000/225000 (71%)] Loss: 8069.542969\n",
      "Train Epoch: 192 [164096/225000 (73%)] Loss: 8145.000000\n",
      "Train Epoch: 192 [168192/225000 (75%)] Loss: 8208.003906\n",
      "Train Epoch: 192 [172288/225000 (77%)] Loss: 7978.326172\n",
      "Train Epoch: 192 [176384/225000 (78%)] Loss: 7946.146484\n",
      "Train Epoch: 192 [180480/225000 (80%)] Loss: 8175.953125\n",
      "Train Epoch: 192 [184576/225000 (82%)] Loss: 8135.167969\n",
      "Train Epoch: 192 [188672/225000 (84%)] Loss: 12434.068359\n",
      "Train Epoch: 192 [192768/225000 (86%)] Loss: 8043.232422\n",
      "Train Epoch: 192 [196864/225000 (87%)] Loss: 8037.322266\n",
      "Train Epoch: 192 [200960/225000 (89%)] Loss: 8030.582031\n",
      "Train Epoch: 192 [205056/225000 (91%)] Loss: 12183.679688\n",
      "Train Epoch: 192 [209152/225000 (93%)] Loss: 8275.011719\n",
      "Train Epoch: 192 [213248/225000 (95%)] Loss: 7916.576172\n",
      "Train Epoch: 192 [217344/225000 (97%)] Loss: 8128.833984\n",
      "Train Epoch: 192 [221440/225000 (98%)] Loss: 8028.224609\n",
      "    epoch          : 192\n",
      "    loss           : 9602.945345829778\n",
      "    val_loss       : 9485.289904596853\n",
      "Train Epoch: 193 [256/225000 (0%)] Loss: 8049.962891\n",
      "Train Epoch: 193 [4352/225000 (2%)] Loss: 8164.240234\n",
      "Train Epoch: 193 [8448/225000 (4%)] Loss: 8112.658203\n",
      "Train Epoch: 193 [12544/225000 (6%)] Loss: 8047.517578\n",
      "Train Epoch: 193 [16640/225000 (7%)] Loss: 12089.101562\n",
      "Train Epoch: 193 [20736/225000 (9%)] Loss: 7848.890625\n",
      "Train Epoch: 193 [24832/225000 (11%)] Loss: 8048.199219\n",
      "Train Epoch: 193 [28928/225000 (13%)] Loss: 13703.054688\n",
      "Train Epoch: 193 [33024/225000 (15%)] Loss: 10449.527344\n",
      "Train Epoch: 193 [37120/225000 (16%)] Loss: 12531.933594\n",
      "Train Epoch: 193 [41216/225000 (18%)] Loss: 12126.492188\n",
      "Train Epoch: 193 [45312/225000 (20%)] Loss: 8153.552734\n",
      "Train Epoch: 193 [49408/225000 (22%)] Loss: 13867.699219\n",
      "Train Epoch: 193 [53504/225000 (24%)] Loss: 8075.687500\n",
      "Train Epoch: 193 [57600/225000 (26%)] Loss: 7893.140625\n",
      "Train Epoch: 193 [61696/225000 (27%)] Loss: 12239.296875\n",
      "Train Epoch: 193 [65792/225000 (29%)] Loss: 10517.070312\n",
      "Train Epoch: 193 [69888/225000 (31%)] Loss: 10455.025391\n",
      "Train Epoch: 193 [73984/225000 (33%)] Loss: 8022.201172\n",
      "Train Epoch: 193 [78080/225000 (35%)] Loss: 7909.007812\n",
      "Train Epoch: 193 [82176/225000 (37%)] Loss: 13962.689453\n",
      "Train Epoch: 193 [86272/225000 (38%)] Loss: 8153.753906\n",
      "Train Epoch: 193 [90368/225000 (40%)] Loss: 8088.080078\n",
      "Train Epoch: 193 [94464/225000 (42%)] Loss: 8058.156250\n",
      "Train Epoch: 193 [98560/225000 (44%)] Loss: 10462.937500\n",
      "Train Epoch: 193 [102656/225000 (46%)] Loss: 8059.679688\n",
      "Train Epoch: 193 [106752/225000 (47%)] Loss: 8270.386719\n",
      "Train Epoch: 193 [110848/225000 (49%)] Loss: 8084.490234\n",
      "Train Epoch: 193 [114944/225000 (51%)] Loss: 8064.593750\n",
      "Train Epoch: 193 [119040/225000 (53%)] Loss: 9540.716797\n",
      "Train Epoch: 193 [123136/225000 (55%)] Loss: 9709.822266\n",
      "Train Epoch: 193 [127232/225000 (57%)] Loss: 8076.892578\n",
      "Train Epoch: 193 [131328/225000 (58%)] Loss: 9649.855469\n",
      "Train Epoch: 193 [135424/225000 (60%)] Loss: 7994.994141\n",
      "Train Epoch: 193 [139520/225000 (62%)] Loss: 8155.005859\n",
      "Train Epoch: 193 [143616/225000 (64%)] Loss: 7971.376953\n",
      "Train Epoch: 193 [147712/225000 (66%)] Loss: 8119.335938\n",
      "Train Epoch: 193 [151808/225000 (67%)] Loss: 8296.849609\n",
      "Train Epoch: 193 [155904/225000 (69%)] Loss: 12935.359375\n",
      "Train Epoch: 193 [160000/225000 (71%)] Loss: 12003.236328\n",
      "Train Epoch: 193 [164096/225000 (73%)] Loss: 10215.685547\n",
      "Train Epoch: 193 [168192/225000 (75%)] Loss: 8187.910156\n",
      "Train Epoch: 193 [172288/225000 (77%)] Loss: 14008.089844\n",
      "Train Epoch: 193 [176384/225000 (78%)] Loss: 8044.306641\n",
      "Train Epoch: 193 [180480/225000 (80%)] Loss: 12091.621094\n",
      "Train Epoch: 193 [184576/225000 (82%)] Loss: 8437.421875\n",
      "Train Epoch: 193 [188672/225000 (84%)] Loss: 9694.226562\n",
      "Train Epoch: 193 [192768/225000 (86%)] Loss: 8036.664062\n",
      "Train Epoch: 193 [196864/225000 (87%)] Loss: 8031.820312\n",
      "Train Epoch: 193 [200960/225000 (89%)] Loss: 8060.132812\n",
      "Train Epoch: 193 [205056/225000 (91%)] Loss: 8141.898438\n",
      "Train Epoch: 193 [209152/225000 (93%)] Loss: 8279.818359\n",
      "Train Epoch: 193 [213248/225000 (95%)] Loss: 9513.361328\n",
      "Train Epoch: 193 [217344/225000 (97%)] Loss: 8106.486328\n",
      "Train Epoch: 193 [221440/225000 (98%)] Loss: 7981.431641\n",
      "    epoch          : 193\n",
      "    loss           : 9418.776680487415\n",
      "    val_loss       : 9669.511680242967\n",
      "Train Epoch: 194 [256/225000 (0%)] Loss: 9477.136719\n",
      "Train Epoch: 194 [4352/225000 (2%)] Loss: 8056.146484\n",
      "Train Epoch: 194 [8448/225000 (4%)] Loss: 10352.718750\n",
      "Train Epoch: 194 [12544/225000 (6%)] Loss: 10336.880859\n",
      "Train Epoch: 194 [16640/225000 (7%)] Loss: 9735.296875\n",
      "Train Epoch: 194 [20736/225000 (9%)] Loss: 11047.996094\n",
      "Train Epoch: 194 [24832/225000 (11%)] Loss: 8116.566406\n",
      "Train Epoch: 194 [28928/225000 (13%)] Loss: 8022.132812\n",
      "Train Epoch: 194 [33024/225000 (15%)] Loss: 7919.103516\n",
      "Train Epoch: 194 [37120/225000 (16%)] Loss: 8063.611328\n",
      "Train Epoch: 194 [41216/225000 (18%)] Loss: 8235.685547\n",
      "Train Epoch: 194 [45312/225000 (20%)] Loss: 16023.138672\n",
      "Train Epoch: 194 [49408/225000 (22%)] Loss: 10582.539062\n",
      "Train Epoch: 194 [53504/225000 (24%)] Loss: 9559.417969\n",
      "Train Epoch: 194 [57600/225000 (26%)] Loss: 10476.925781\n",
      "Train Epoch: 194 [61696/225000 (27%)] Loss: 15075.351562\n",
      "Train Epoch: 194 [65792/225000 (29%)] Loss: 8122.753906\n",
      "Train Epoch: 194 [69888/225000 (31%)] Loss: 9589.710938\n",
      "Train Epoch: 194 [73984/225000 (33%)] Loss: 7843.716797\n",
      "Train Epoch: 194 [78080/225000 (35%)] Loss: 14747.437500\n",
      "Train Epoch: 194 [82176/225000 (37%)] Loss: 8081.439453\n",
      "Train Epoch: 194 [86272/225000 (38%)] Loss: 8197.662109\n",
      "Train Epoch: 194 [90368/225000 (40%)] Loss: 9764.455078\n",
      "Train Epoch: 194 [94464/225000 (42%)] Loss: 7994.958984\n",
      "Train Epoch: 194 [98560/225000 (44%)] Loss: 13617.509766\n",
      "Train Epoch: 194 [102656/225000 (46%)] Loss: 10898.130859\n",
      "Train Epoch: 194 [106752/225000 (47%)] Loss: 10542.125000\n",
      "Train Epoch: 194 [110848/225000 (49%)] Loss: 9697.595703\n",
      "Train Epoch: 194 [114944/225000 (51%)] Loss: 9709.451172\n",
      "Train Epoch: 194 [119040/225000 (53%)] Loss: 7969.537109\n",
      "Train Epoch: 194 [123136/225000 (55%)] Loss: 20474.923828\n",
      "Train Epoch: 194 [127232/225000 (57%)] Loss: 8221.953125\n",
      "Train Epoch: 194 [131328/225000 (58%)] Loss: 9540.707031\n",
      "Train Epoch: 194 [135424/225000 (60%)] Loss: 9556.353516\n",
      "Train Epoch: 194 [139520/225000 (62%)] Loss: 8072.564453\n",
      "Train Epoch: 194 [143616/225000 (64%)] Loss: 8203.005859\n",
      "Train Epoch: 194 [147712/225000 (66%)] Loss: 13608.439453\n",
      "Train Epoch: 194 [151808/225000 (67%)] Loss: 8197.753906\n",
      "Train Epoch: 194 [155904/225000 (69%)] Loss: 7990.457031\n",
      "Train Epoch: 194 [160000/225000 (71%)] Loss: 8207.232422\n",
      "Train Epoch: 194 [164096/225000 (73%)] Loss: 8205.357422\n",
      "Train Epoch: 194 [168192/225000 (75%)] Loss: 9723.988281\n",
      "Train Epoch: 194 [172288/225000 (77%)] Loss: 8135.574219\n",
      "Train Epoch: 194 [176384/225000 (78%)] Loss: 8081.042969\n",
      "Train Epoch: 194 [180480/225000 (80%)] Loss: 9439.845703\n",
      "Train Epoch: 194 [184576/225000 (82%)] Loss: 10505.263672\n",
      "Train Epoch: 194 [188672/225000 (84%)] Loss: 10356.773438\n",
      "Train Epoch: 194 [192768/225000 (86%)] Loss: 9484.992188\n",
      "Train Epoch: 194 [196864/225000 (87%)] Loss: 12271.021484\n",
      "Train Epoch: 194 [200960/225000 (89%)] Loss: 8177.367188\n",
      "Train Epoch: 194 [205056/225000 (91%)] Loss: 9611.037109\n",
      "Train Epoch: 194 [209152/225000 (93%)] Loss: 7994.470703\n",
      "Train Epoch: 194 [213248/225000 (95%)] Loss: 8051.232422\n",
      "Train Epoch: 194 [217344/225000 (97%)] Loss: 9555.837891\n",
      "Train Epoch: 194 [221440/225000 (98%)] Loss: 7978.386719\n",
      "    epoch          : 194\n",
      "    loss           : 9571.151279419084\n",
      "    val_loss       : 9359.477666511828\n",
      "Train Epoch: 195 [256/225000 (0%)] Loss: 7925.359375\n",
      "Train Epoch: 195 [4352/225000 (2%)] Loss: 8013.039062\n",
      "Train Epoch: 195 [8448/225000 (4%)] Loss: 10628.314453\n",
      "Train Epoch: 195 [12544/225000 (6%)] Loss: 8331.316406\n",
      "Train Epoch: 195 [16640/225000 (7%)] Loss: 8133.335938\n",
      "Train Epoch: 195 [20736/225000 (9%)] Loss: 7951.179688\n",
      "Train Epoch: 195 [24832/225000 (11%)] Loss: 9618.548828\n",
      "Train Epoch: 195 [28928/225000 (13%)] Loss: 8248.953125\n",
      "Train Epoch: 195 [33024/225000 (15%)] Loss: 7989.126953\n",
      "Train Epoch: 195 [37120/225000 (16%)] Loss: 7969.519531\n",
      "Train Epoch: 195 [41216/225000 (18%)] Loss: 15139.712891\n",
      "Train Epoch: 195 [45312/225000 (20%)] Loss: 7896.574219\n",
      "Train Epoch: 195 [49408/225000 (22%)] Loss: 12914.978516\n",
      "Train Epoch: 195 [53504/225000 (24%)] Loss: 9636.919922\n",
      "Train Epoch: 195 [57600/225000 (26%)] Loss: 8191.380859\n",
      "Train Epoch: 195 [61696/225000 (27%)] Loss: 8166.582031\n",
      "Train Epoch: 195 [65792/225000 (29%)] Loss: 7860.324219\n",
      "Train Epoch: 195 [69888/225000 (31%)] Loss: 8132.820312\n",
      "Train Epoch: 195 [73984/225000 (33%)] Loss: 8062.791016\n",
      "Train Epoch: 195 [78080/225000 (35%)] Loss: 11065.642578\n",
      "Train Epoch: 195 [82176/225000 (37%)] Loss: 8075.312500\n",
      "Train Epoch: 195 [86272/225000 (38%)] Loss: 9717.742188\n",
      "Train Epoch: 195 [90368/225000 (40%)] Loss: 8014.761719\n",
      "Train Epoch: 195 [94464/225000 (42%)] Loss: 8046.021484\n",
      "Train Epoch: 195 [98560/225000 (44%)] Loss: 7867.802734\n",
      "Train Epoch: 195 [102656/225000 (46%)] Loss: 10640.169922\n",
      "Train Epoch: 195 [106752/225000 (47%)] Loss: 8185.808594\n",
      "Train Epoch: 195 [110848/225000 (49%)] Loss: 8110.136719\n",
      "Train Epoch: 195 [114944/225000 (51%)] Loss: 7823.966797\n",
      "Train Epoch: 195 [119040/225000 (53%)] Loss: 8087.302734\n",
      "Train Epoch: 195 [123136/225000 (55%)] Loss: 8179.423828\n",
      "Train Epoch: 195 [127232/225000 (57%)] Loss: 7981.677734\n",
      "Train Epoch: 195 [131328/225000 (58%)] Loss: 10425.669922\n",
      "Train Epoch: 195 [135424/225000 (60%)] Loss: 7890.697266\n",
      "Train Epoch: 195 [139520/225000 (62%)] Loss: 12210.025391\n",
      "Train Epoch: 195 [143616/225000 (64%)] Loss: 8146.273438\n",
      "Train Epoch: 195 [147712/225000 (66%)] Loss: 10622.367188\n",
      "Train Epoch: 195 [151808/225000 (67%)] Loss: 8120.720703\n",
      "Train Epoch: 195 [155904/225000 (69%)] Loss: 8024.701172\n",
      "Train Epoch: 195 [160000/225000 (71%)] Loss: 17779.068359\n",
      "Train Epoch: 195 [164096/225000 (73%)] Loss: 8238.078125\n",
      "Train Epoch: 195 [168192/225000 (75%)] Loss: 8064.453125\n",
      "Train Epoch: 195 [172288/225000 (77%)] Loss: 8022.380859\n",
      "Train Epoch: 195 [176384/225000 (78%)] Loss: 7982.316406\n",
      "Train Epoch: 195 [180480/225000 (80%)] Loss: 8222.494141\n",
      "Train Epoch: 195 [184576/225000 (82%)] Loss: 9524.031250\n",
      "Train Epoch: 195 [188672/225000 (84%)] Loss: 10689.259766\n",
      "Train Epoch: 195 [192768/225000 (86%)] Loss: 8156.326172\n",
      "Train Epoch: 195 [196864/225000 (87%)] Loss: 10610.626953\n",
      "Train Epoch: 195 [200960/225000 (89%)] Loss: 8028.400391\n",
      "Train Epoch: 195 [205056/225000 (91%)] Loss: 8028.175781\n",
      "Train Epoch: 195 [209152/225000 (93%)] Loss: 13729.580078\n",
      "Train Epoch: 195 [213248/225000 (95%)] Loss: 7987.218750\n",
      "Train Epoch: 195 [217344/225000 (97%)] Loss: 8014.806641\n",
      "Train Epoch: 195 [221440/225000 (98%)] Loss: 7776.187500\n",
      "    epoch          : 195\n",
      "    loss           : 9559.816017402589\n",
      "    val_loss       : 9416.888000108758\n",
      "Train Epoch: 196 [256/225000 (0%)] Loss: 8153.642578\n",
      "Train Epoch: 196 [4352/225000 (2%)] Loss: 8074.089844\n",
      "Train Epoch: 196 [8448/225000 (4%)] Loss: 8113.605469\n",
      "Train Epoch: 196 [12544/225000 (6%)] Loss: 8150.763672\n",
      "Train Epoch: 196 [16640/225000 (7%)] Loss: 15590.835938\n",
      "Train Epoch: 196 [20736/225000 (9%)] Loss: 8127.101562\n",
      "Train Epoch: 196 [24832/225000 (11%)] Loss: 8025.708984\n",
      "Train Epoch: 196 [28928/225000 (13%)] Loss: 8078.269531\n",
      "Train Epoch: 196 [33024/225000 (15%)] Loss: 10593.892578\n",
      "Train Epoch: 196 [37120/225000 (16%)] Loss: 7954.343750\n",
      "Train Epoch: 196 [41216/225000 (18%)] Loss: 8046.558594\n",
      "Train Epoch: 196 [45312/225000 (20%)] Loss: 7972.576172\n",
      "Train Epoch: 196 [49408/225000 (22%)] Loss: 12312.886719\n",
      "Train Epoch: 196 [53504/225000 (24%)] Loss: 9655.578125\n",
      "Train Epoch: 196 [57600/225000 (26%)] Loss: 9667.931641\n",
      "Train Epoch: 196 [61696/225000 (27%)] Loss: 10506.583984\n",
      "Train Epoch: 196 [65792/225000 (29%)] Loss: 8107.390625\n",
      "Train Epoch: 196 [69888/225000 (31%)] Loss: 10578.818359\n",
      "Train Epoch: 196 [73984/225000 (33%)] Loss: 8111.208984\n",
      "Train Epoch: 196 [78080/225000 (35%)] Loss: 8135.593750\n",
      "Train Epoch: 196 [82176/225000 (37%)] Loss: 7853.904297\n",
      "Train Epoch: 196 [86272/225000 (38%)] Loss: 8183.767578\n",
      "Train Epoch: 196 [90368/225000 (40%)] Loss: 7922.146484\n",
      "Train Epoch: 196 [94464/225000 (42%)] Loss: 8112.884766\n",
      "Train Epoch: 196 [98560/225000 (44%)] Loss: 8106.746094\n",
      "Train Epoch: 196 [102656/225000 (46%)] Loss: 7911.064453\n",
      "Train Epoch: 196 [106752/225000 (47%)] Loss: 8065.697266\n",
      "Train Epoch: 196 [110848/225000 (49%)] Loss: 9485.195312\n",
      "Train Epoch: 196 [114944/225000 (51%)] Loss: 8094.947266\n",
      "Train Epoch: 196 [119040/225000 (53%)] Loss: 12399.742188\n",
      "Train Epoch: 196 [123136/225000 (55%)] Loss: 10600.652344\n",
      "Train Epoch: 196 [127232/225000 (57%)] Loss: 8118.156250\n",
      "Train Epoch: 196 [131328/225000 (58%)] Loss: 16269.798828\n",
      "Train Epoch: 196 [135424/225000 (60%)] Loss: 9438.947266\n",
      "Train Epoch: 196 [139520/225000 (62%)] Loss: 8106.382812\n",
      "Train Epoch: 196 [143616/225000 (64%)] Loss: 9480.878906\n",
      "Train Epoch: 196 [147712/225000 (66%)] Loss: 9685.896484\n",
      "Train Epoch: 196 [151808/225000 (67%)] Loss: 8051.972656\n",
      "Train Epoch: 196 [155904/225000 (69%)] Loss: 10574.947266\n",
      "Train Epoch: 196 [160000/225000 (71%)] Loss: 9776.593750\n",
      "Train Epoch: 196 [164096/225000 (73%)] Loss: 9773.236328\n",
      "Train Epoch: 196 [168192/225000 (75%)] Loss: 14027.117188\n",
      "Train Epoch: 196 [172288/225000 (77%)] Loss: 9705.144531\n",
      "Train Epoch: 196 [176384/225000 (78%)] Loss: 10490.060547\n",
      "Train Epoch: 196 [180480/225000 (80%)] Loss: 8047.722656\n",
      "Train Epoch: 196 [184576/225000 (82%)] Loss: 8234.820312\n",
      "Train Epoch: 196 [188672/225000 (84%)] Loss: 7999.812500\n",
      "Train Epoch: 196 [192768/225000 (86%)] Loss: 8102.236328\n",
      "Train Epoch: 196 [196864/225000 (87%)] Loss: 14049.480469\n",
      "Train Epoch: 196 [200960/225000 (89%)] Loss: 8286.609375\n",
      "Train Epoch: 196 [205056/225000 (91%)] Loss: 7968.857422\n",
      "Train Epoch: 196 [209152/225000 (93%)] Loss: 8126.794922\n",
      "Train Epoch: 196 [213248/225000 (95%)] Loss: 8003.785156\n",
      "Train Epoch: 196 [217344/225000 (97%)] Loss: 12385.556641\n",
      "Train Epoch: 196 [221440/225000 (98%)] Loss: 13763.978516\n",
      "    epoch          : 196\n",
      "    loss           : 9473.553005457195\n",
      "    val_loss       : 9237.277854827364\n",
      "Train Epoch: 197 [256/225000 (0%)] Loss: 7984.224609\n",
      "Train Epoch: 197 [4352/225000 (2%)] Loss: 7996.742188\n",
      "Train Epoch: 197 [8448/225000 (4%)] Loss: 9582.140625\n",
      "Train Epoch: 197 [12544/225000 (6%)] Loss: 8056.033203\n",
      "Train Epoch: 197 [16640/225000 (7%)] Loss: 9647.121094\n",
      "Train Epoch: 197 [20736/225000 (9%)] Loss: 9726.033203\n",
      "Train Epoch: 197 [24832/225000 (11%)] Loss: 12716.255859\n",
      "Train Epoch: 197 [28928/225000 (13%)] Loss: 13917.449219\n",
      "Train Epoch: 197 [33024/225000 (15%)] Loss: 8178.755859\n",
      "Train Epoch: 197 [37120/225000 (16%)] Loss: 9639.785156\n",
      "Train Epoch: 197 [41216/225000 (18%)] Loss: 8058.582031\n",
      "Train Epoch: 197 [45312/225000 (20%)] Loss: 8264.531250\n",
      "Train Epoch: 197 [49408/225000 (22%)] Loss: 10558.943359\n",
      "Train Epoch: 197 [53504/225000 (24%)] Loss: 8155.115234\n",
      "Train Epoch: 197 [57600/225000 (26%)] Loss: 13427.457031\n",
      "Train Epoch: 197 [61696/225000 (27%)] Loss: 15641.412109\n",
      "Train Epoch: 197 [65792/225000 (29%)] Loss: 8050.751953\n",
      "Train Epoch: 197 [69888/225000 (31%)] Loss: 8082.574219\n",
      "Train Epoch: 197 [73984/225000 (33%)] Loss: 8170.884766\n",
      "Train Epoch: 197 [78080/225000 (35%)] Loss: 8287.421875\n",
      "Train Epoch: 197 [82176/225000 (37%)] Loss: 8081.599609\n",
      "Train Epoch: 197 [86272/225000 (38%)] Loss: 9479.064453\n",
      "Train Epoch: 197 [90368/225000 (40%)] Loss: 8039.609375\n",
      "Train Epoch: 197 [94464/225000 (42%)] Loss: 8072.681641\n",
      "Train Epoch: 197 [98560/225000 (44%)] Loss: 10585.291016\n",
      "Train Epoch: 197 [102656/225000 (46%)] Loss: 7841.564453\n",
      "Train Epoch: 197 [106752/225000 (47%)] Loss: 10507.248047\n",
      "Train Epoch: 197 [110848/225000 (49%)] Loss: 10344.087891\n",
      "Train Epoch: 197 [114944/225000 (51%)] Loss: 8031.240234\n",
      "Train Epoch: 197 [119040/225000 (53%)] Loss: 12016.611328\n",
      "Train Epoch: 197 [123136/225000 (55%)] Loss: 8003.304688\n",
      "Train Epoch: 197 [127232/225000 (57%)] Loss: 8051.714844\n",
      "Train Epoch: 197 [131328/225000 (58%)] Loss: 8094.187500\n",
      "Train Epoch: 197 [135424/225000 (60%)] Loss: 13844.167969\n",
      "Train Epoch: 197 [139520/225000 (62%)] Loss: 8005.056641\n",
      "Train Epoch: 197 [143616/225000 (64%)] Loss: 8317.798828\n",
      "Train Epoch: 197 [147712/225000 (66%)] Loss: 7938.261719\n",
      "Train Epoch: 197 [151808/225000 (67%)] Loss: 13653.876953\n",
      "Train Epoch: 197 [155904/225000 (69%)] Loss: 8041.548828\n",
      "Train Epoch: 197 [160000/225000 (71%)] Loss: 8380.619141\n",
      "Train Epoch: 197 [164096/225000 (73%)] Loss: 8101.957031\n",
      "Train Epoch: 197 [168192/225000 (75%)] Loss: 13776.917969\n",
      "Train Epoch: 197 [172288/225000 (77%)] Loss: 8101.966797\n",
      "Train Epoch: 197 [176384/225000 (78%)] Loss: 8039.355469\n",
      "Train Epoch: 197 [180480/225000 (80%)] Loss: 8030.955078\n",
      "Train Epoch: 197 [184576/225000 (82%)] Loss: 9596.001953\n",
      "Train Epoch: 197 [188672/225000 (84%)] Loss: 8050.714844\n",
      "Train Epoch: 197 [192768/225000 (86%)] Loss: 10538.750000\n",
      "Train Epoch: 197 [196864/225000 (87%)] Loss: 8100.166016\n",
      "Train Epoch: 197 [200960/225000 (89%)] Loss: 8015.107422\n",
      "Train Epoch: 197 [205056/225000 (91%)] Loss: 9618.273438\n",
      "Train Epoch: 197 [209152/225000 (93%)] Loss: 8278.029297\n",
      "Train Epoch: 197 [213248/225000 (95%)] Loss: 10450.419922\n",
      "Train Epoch: 197 [217344/225000 (97%)] Loss: 8009.509766\n",
      "Train Epoch: 197 [221440/225000 (98%)] Loss: 13329.673828\n",
      "    epoch          : 197\n",
      "    loss           : 9478.55759163467\n",
      "    val_loss       : 9617.86555679477\n",
      "Train Epoch: 198 [256/225000 (0%)] Loss: 8307.789062\n",
      "Train Epoch: 198 [4352/225000 (2%)] Loss: 8115.910156\n",
      "Train Epoch: 198 [8448/225000 (4%)] Loss: 13612.083984\n",
      "Train Epoch: 198 [12544/225000 (6%)] Loss: 7995.570312\n",
      "Train Epoch: 198 [16640/225000 (7%)] Loss: 10656.810547\n",
      "Train Epoch: 198 [20736/225000 (9%)] Loss: 14123.089844\n",
      "Train Epoch: 198 [24832/225000 (11%)] Loss: 17660.673828\n",
      "Train Epoch: 198 [28928/225000 (13%)] Loss: 8105.304688\n",
      "Train Epoch: 198 [33024/225000 (15%)] Loss: 7887.708984\n",
      "Train Epoch: 198 [37120/225000 (16%)] Loss: 9490.226562\n",
      "Train Epoch: 198 [41216/225000 (18%)] Loss: 8094.539062\n",
      "Train Epoch: 198 [45312/225000 (20%)] Loss: 9531.539062\n",
      "Train Epoch: 198 [49408/225000 (22%)] Loss: 12295.289062\n",
      "Train Epoch: 198 [53504/225000 (24%)] Loss: 7939.021484\n",
      "Train Epoch: 198 [57600/225000 (26%)] Loss: 12297.431641\n",
      "Train Epoch: 198 [61696/225000 (27%)] Loss: 10790.324219\n",
      "Train Epoch: 198 [65792/225000 (29%)] Loss: 7985.593750\n",
      "Train Epoch: 198 [69888/225000 (31%)] Loss: 8280.392578\n",
      "Train Epoch: 198 [73984/225000 (33%)] Loss: 8019.353516\n",
      "Train Epoch: 198 [78080/225000 (35%)] Loss: 9586.089844\n",
      "Train Epoch: 198 [82176/225000 (37%)] Loss: 8106.246094\n",
      "Train Epoch: 198 [86272/225000 (38%)] Loss: 9729.685547\n",
      "Train Epoch: 198 [90368/225000 (40%)] Loss: 8236.011719\n",
      "Train Epoch: 198 [94464/225000 (42%)] Loss: 8225.974609\n",
      "Train Epoch: 198 [98560/225000 (44%)] Loss: 7998.425781\n",
      "Train Epoch: 198 [102656/225000 (46%)] Loss: 9518.046875\n",
      "Train Epoch: 198 [106752/225000 (47%)] Loss: 8074.240234\n",
      "Train Epoch: 198 [110848/225000 (49%)] Loss: 13978.443359\n",
      "Train Epoch: 198 [114944/225000 (51%)] Loss: 9535.646484\n",
      "Train Epoch: 198 [119040/225000 (53%)] Loss: 7973.001953\n",
      "Train Epoch: 198 [123136/225000 (55%)] Loss: 13036.353516\n",
      "Train Epoch: 198 [127232/225000 (57%)] Loss: 8170.156250\n",
      "Train Epoch: 198 [131328/225000 (58%)] Loss: 7948.667969\n",
      "Train Epoch: 198 [135424/225000 (60%)] Loss: 7944.503906\n",
      "Train Epoch: 198 [139520/225000 (62%)] Loss: 8222.597656\n",
      "Train Epoch: 198 [143616/225000 (64%)] Loss: 12317.066406\n",
      "Train Epoch: 198 [147712/225000 (66%)] Loss: 7978.986328\n",
      "Train Epoch: 198 [151808/225000 (67%)] Loss: 10376.494141\n",
      "Train Epoch: 198 [155904/225000 (69%)] Loss: 19989.878906\n",
      "Train Epoch: 198 [160000/225000 (71%)] Loss: 7808.287109\n",
      "Train Epoch: 198 [164096/225000 (73%)] Loss: 8068.783203\n",
      "Train Epoch: 198 [168192/225000 (75%)] Loss: 7879.759766\n",
      "Train Epoch: 198 [172288/225000 (77%)] Loss: 13782.642578\n",
      "Train Epoch: 198 [176384/225000 (78%)] Loss: 8061.455078\n",
      "Train Epoch: 198 [180480/225000 (80%)] Loss: 8070.998047\n",
      "Train Epoch: 198 [184576/225000 (82%)] Loss: 8029.480469\n",
      "Train Epoch: 198 [188672/225000 (84%)] Loss: 8084.394531\n",
      "Train Epoch: 198 [192768/225000 (86%)] Loss: 8153.455078\n",
      "Train Epoch: 198 [196864/225000 (87%)] Loss: 9514.304688\n",
      "Train Epoch: 198 [200960/225000 (89%)] Loss: 8038.652344\n",
      "Train Epoch: 198 [205056/225000 (91%)] Loss: 9618.607422\n",
      "Train Epoch: 198 [209152/225000 (93%)] Loss: 8148.609375\n",
      "Train Epoch: 198 [213248/225000 (95%)] Loss: 9666.869141\n",
      "Train Epoch: 198 [217344/225000 (97%)] Loss: 12838.601562\n",
      "Train Epoch: 198 [221440/225000 (98%)] Loss: 9486.480469\n",
      "    epoch          : 198\n",
      "    loss           : 9426.69611908063\n",
      "    val_loss       : 9590.100487529015\n",
      "Train Epoch: 199 [256/225000 (0%)] Loss: 8181.529297\n",
      "Train Epoch: 199 [4352/225000 (2%)] Loss: 13636.623047\n",
      "Train Epoch: 199 [8448/225000 (4%)] Loss: 12895.900391\n",
      "Train Epoch: 199 [12544/225000 (6%)] Loss: 8058.849609\n",
      "Train Epoch: 199 [16640/225000 (7%)] Loss: 10462.154297\n",
      "Train Epoch: 199 [20736/225000 (9%)] Loss: 9680.097656\n",
      "Train Epoch: 199 [24832/225000 (11%)] Loss: 12261.291016\n",
      "Train Epoch: 199 [28928/225000 (13%)] Loss: 11886.693359\n",
      "Train Epoch: 199 [33024/225000 (15%)] Loss: 9617.917969\n",
      "Train Epoch: 199 [37120/225000 (16%)] Loss: 8097.386719\n",
      "Train Epoch: 199 [41216/225000 (18%)] Loss: 8147.921875\n",
      "Train Epoch: 199 [45312/225000 (20%)] Loss: 7924.349609\n",
      "Train Epoch: 199 [49408/225000 (22%)] Loss: 8188.417969\n",
      "Train Epoch: 199 [53504/225000 (24%)] Loss: 8054.652344\n",
      "Train Epoch: 199 [57600/225000 (26%)] Loss: 7952.800781\n",
      "Train Epoch: 199 [61696/225000 (27%)] Loss: 8105.763672\n",
      "Train Epoch: 199 [65792/225000 (29%)] Loss: 9583.230469\n",
      "Train Epoch: 199 [69888/225000 (31%)] Loss: 7967.781250\n",
      "Train Epoch: 199 [73984/225000 (33%)] Loss: 10415.222656\n",
      "Train Epoch: 199 [78080/225000 (35%)] Loss: 13966.150391\n",
      "Train Epoch: 199 [82176/225000 (37%)] Loss: 7988.849609\n",
      "Train Epoch: 199 [86272/225000 (38%)] Loss: 8058.126953\n",
      "Train Epoch: 199 [90368/225000 (40%)] Loss: 7938.193359\n",
      "Train Epoch: 199 [94464/225000 (42%)] Loss: 7920.724609\n",
      "Train Epoch: 199 [98560/225000 (44%)] Loss: 8214.781250\n",
      "Train Epoch: 199 [102656/225000 (46%)] Loss: 7996.564453\n",
      "Train Epoch: 199 [106752/225000 (47%)] Loss: 8108.894531\n",
      "Train Epoch: 199 [110848/225000 (49%)] Loss: 12399.988281\n",
      "Train Epoch: 199 [114944/225000 (51%)] Loss: 8075.035156\n",
      "Train Epoch: 199 [119040/225000 (53%)] Loss: 9667.363281\n",
      "Train Epoch: 199 [123136/225000 (55%)] Loss: 8107.826172\n",
      "Train Epoch: 199 [127232/225000 (57%)] Loss: 10602.314453\n",
      "Train Epoch: 199 [131328/225000 (58%)] Loss: 8016.203125\n",
      "Train Epoch: 199 [135424/225000 (60%)] Loss: 13755.414062\n",
      "Train Epoch: 199 [139520/225000 (62%)] Loss: 8308.011719\n",
      "Train Epoch: 199 [143616/225000 (64%)] Loss: 8222.070312\n",
      "Train Epoch: 199 [147712/225000 (66%)] Loss: 13136.748047\n",
      "Train Epoch: 199 [151808/225000 (67%)] Loss: 10564.074219\n",
      "Train Epoch: 199 [155904/225000 (69%)] Loss: 15566.640625\n",
      "Train Epoch: 199 [160000/225000 (71%)] Loss: 12135.726562\n",
      "Train Epoch: 199 [164096/225000 (73%)] Loss: 8063.314453\n",
      "Train Epoch: 199 [168192/225000 (75%)] Loss: 10693.484375\n",
      "Train Epoch: 199 [172288/225000 (77%)] Loss: 9527.908203\n",
      "Train Epoch: 199 [176384/225000 (78%)] Loss: 10569.566406\n",
      "Train Epoch: 199 [180480/225000 (80%)] Loss: 8084.595703\n",
      "Train Epoch: 199 [184576/225000 (82%)] Loss: 9624.771484\n",
      "Train Epoch: 199 [188672/225000 (84%)] Loss: 13948.957031\n",
      "Train Epoch: 199 [192768/225000 (86%)] Loss: 7944.738281\n",
      "Train Epoch: 199 [196864/225000 (87%)] Loss: 9665.267578\n",
      "Train Epoch: 199 [200960/225000 (89%)] Loss: 8104.775391\n",
      "Train Epoch: 199 [205056/225000 (91%)] Loss: 8149.384766\n",
      "Train Epoch: 199 [209152/225000 (93%)] Loss: 12146.658203\n",
      "Train Epoch: 199 [213248/225000 (95%)] Loss: 12683.466797\n",
      "Train Epoch: 199 [217344/225000 (97%)] Loss: 10498.480469\n",
      "Train Epoch: 199 [221440/225000 (98%)] Loss: 8140.011719\n",
      "    epoch          : 199\n",
      "    loss           : 9326.067056180675\n",
      "    val_loss       : 9444.633067717145\n",
      "Train Epoch: 200 [256/225000 (0%)] Loss: 12365.375000\n",
      "Train Epoch: 200 [4352/225000 (2%)] Loss: 8104.533203\n",
      "Train Epoch: 200 [8448/225000 (4%)] Loss: 8042.785156\n",
      "Train Epoch: 200 [12544/225000 (6%)] Loss: 8145.416016\n",
      "Train Epoch: 200 [16640/225000 (7%)] Loss: 7961.583984\n",
      "Train Epoch: 200 [20736/225000 (9%)] Loss: 10376.474609\n",
      "Train Epoch: 200 [24832/225000 (11%)] Loss: 7986.283203\n",
      "Train Epoch: 200 [28928/225000 (13%)] Loss: 8109.642578\n",
      "Train Epoch: 200 [33024/225000 (15%)] Loss: 9649.076172\n",
      "Train Epoch: 200 [37120/225000 (16%)] Loss: 8118.636719\n",
      "Train Epoch: 200 [41216/225000 (18%)] Loss: 7984.027344\n",
      "Train Epoch: 200 [45312/225000 (20%)] Loss: 8122.333984\n",
      "Train Epoch: 200 [49408/225000 (22%)] Loss: 8022.246094\n",
      "Train Epoch: 200 [53504/225000 (24%)] Loss: 7872.728516\n",
      "Train Epoch: 200 [57600/225000 (26%)] Loss: 8319.183594\n",
      "Train Epoch: 200 [61696/225000 (27%)] Loss: 7944.390625\n",
      "Train Epoch: 200 [65792/225000 (29%)] Loss: 8187.166016\n",
      "Train Epoch: 200 [69888/225000 (31%)] Loss: 8086.509766\n",
      "Train Epoch: 200 [73984/225000 (33%)] Loss: 12463.214844\n",
      "Train Epoch: 200 [78080/225000 (35%)] Loss: 8006.363281\n",
      "Train Epoch: 200 [82176/225000 (37%)] Loss: 8108.062500\n",
      "Train Epoch: 200 [86272/225000 (38%)] Loss: 8092.333984\n",
      "Train Epoch: 200 [90368/225000 (40%)] Loss: 8112.730469\n",
      "Train Epoch: 200 [94464/225000 (42%)] Loss: 8144.712891\n",
      "Train Epoch: 200 [98560/225000 (44%)] Loss: 8393.208984\n",
      "Train Epoch: 200 [102656/225000 (46%)] Loss: 8078.359375\n",
      "Train Epoch: 200 [106752/225000 (47%)] Loss: 8213.003906\n",
      "Train Epoch: 200 [110848/225000 (49%)] Loss: 9521.783203\n",
      "Train Epoch: 200 [114944/225000 (51%)] Loss: 8278.089844\n",
      "Train Epoch: 200 [119040/225000 (53%)] Loss: 7998.089844\n",
      "Train Epoch: 200 [123136/225000 (55%)] Loss: 8224.650391\n",
      "Train Epoch: 200 [127232/225000 (57%)] Loss: 8185.271484\n",
      "Train Epoch: 200 [131328/225000 (58%)] Loss: 7992.494141\n",
      "Train Epoch: 200 [135424/225000 (60%)] Loss: 8247.134766\n",
      "Train Epoch: 200 [139520/225000 (62%)] Loss: 12422.208984\n",
      "Train Epoch: 200 [143616/225000 (64%)] Loss: 7931.683594\n",
      "Train Epoch: 200 [147712/225000 (66%)] Loss: 10410.378906\n",
      "Train Epoch: 200 [151808/225000 (67%)] Loss: 7968.169922\n",
      "Train Epoch: 200 [155904/225000 (69%)] Loss: 9825.769531\n",
      "Train Epoch: 200 [160000/225000 (71%)] Loss: 8116.591797\n",
      "Train Epoch: 200 [164096/225000 (73%)] Loss: 8206.099609\n",
      "Train Epoch: 200 [168192/225000 (75%)] Loss: 12410.498047\n",
      "Train Epoch: 200 [172288/225000 (77%)] Loss: 10477.822266\n",
      "Train Epoch: 200 [176384/225000 (78%)] Loss: 8105.771484\n",
      "Train Epoch: 200 [180480/225000 (80%)] Loss: 8202.390625\n",
      "Train Epoch: 200 [184576/225000 (82%)] Loss: 7992.886719\n",
      "Train Epoch: 200 [188672/225000 (84%)] Loss: 8221.812500\n",
      "Train Epoch: 200 [192768/225000 (86%)] Loss: 7977.490234\n",
      "Train Epoch: 200 [196864/225000 (87%)] Loss: 13566.855469\n",
      "Train Epoch: 200 [200960/225000 (89%)] Loss: 7843.861328\n",
      "Train Epoch: 200 [205056/225000 (91%)] Loss: 10591.617188\n",
      "Train Epoch: 200 [209152/225000 (93%)] Loss: 10524.294922\n",
      "Train Epoch: 200 [213248/225000 (95%)] Loss: 15131.164062\n",
      "Train Epoch: 200 [217344/225000 (97%)] Loss: 10552.521484\n",
      "Train Epoch: 200 [221440/225000 (98%)] Loss: 8095.646484\n",
      "    epoch          : 200\n",
      "    loss           : 9636.800471283063\n",
      "    val_loss       : 10006.73103204552\n",
      "Saving checkpoint: saved/models/Molecular_VaeCategory/0103_171400/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [256/225000 (0%)] Loss: 7831.373047\n",
      "Train Epoch: 201 [4352/225000 (2%)] Loss: 10635.464844\n",
      "Train Epoch: 201 [8448/225000 (4%)] Loss: 8182.732422\n",
      "Train Epoch: 201 [12544/225000 (6%)] Loss: 10724.191406\n",
      "Train Epoch: 201 [16640/225000 (7%)] Loss: 7960.472656\n",
      "Train Epoch: 201 [20736/225000 (9%)] Loss: 15361.732422\n",
      "Train Epoch: 201 [24832/225000 (11%)] Loss: 9536.937500\n",
      "Train Epoch: 201 [28928/225000 (13%)] Loss: 7840.943359\n",
      "Train Epoch: 201 [33024/225000 (15%)] Loss: 12586.283203\n",
      "Train Epoch: 201 [37120/225000 (16%)] Loss: 7915.228516\n",
      "Train Epoch: 201 [41216/225000 (18%)] Loss: 10401.939453\n",
      "Train Epoch: 201 [45312/225000 (20%)] Loss: 8041.775391\n",
      "Train Epoch: 201 [49408/225000 (22%)] Loss: 8190.533203\n",
      "Train Epoch: 201 [53504/225000 (24%)] Loss: 8102.863281\n",
      "Train Epoch: 201 [57600/225000 (26%)] Loss: 10454.628906\n",
      "Train Epoch: 201 [61696/225000 (27%)] Loss: 9421.833984\n",
      "Train Epoch: 201 [65792/225000 (29%)] Loss: 8122.929688\n",
      "Train Epoch: 201 [69888/225000 (31%)] Loss: 13722.503906\n",
      "Train Epoch: 201 [73984/225000 (33%)] Loss: 12089.576172\n",
      "Train Epoch: 201 [78080/225000 (35%)] Loss: 8096.927734\n",
      "Train Epoch: 201 [82176/225000 (37%)] Loss: 12085.322266\n",
      "Train Epoch: 201 [86272/225000 (38%)] Loss: 13739.734375\n",
      "Train Epoch: 201 [90368/225000 (40%)] Loss: 8085.220703\n",
      "Train Epoch: 201 [94464/225000 (42%)] Loss: 18857.960938\n",
      "Train Epoch: 201 [98560/225000 (44%)] Loss: 8063.457031\n",
      "Train Epoch: 201 [102656/225000 (46%)] Loss: 19639.056641\n",
      "Train Epoch: 201 [106752/225000 (47%)] Loss: 8161.423828\n",
      "Train Epoch: 201 [110848/225000 (49%)] Loss: 8197.935547\n",
      "Train Epoch: 201 [114944/225000 (51%)] Loss: 8044.267578\n",
      "Train Epoch: 201 [119040/225000 (53%)] Loss: 8043.242188\n",
      "Train Epoch: 201 [123136/225000 (55%)] Loss: 9572.535156\n",
      "Train Epoch: 201 [127232/225000 (57%)] Loss: 10458.462891\n",
      "Train Epoch: 201 [131328/225000 (58%)] Loss: 7997.564453\n",
      "Train Epoch: 201 [135424/225000 (60%)] Loss: 15249.835938\n",
      "Train Epoch: 201 [139520/225000 (62%)] Loss: 8098.521484\n",
      "Train Epoch: 201 [143616/225000 (64%)] Loss: 7952.369141\n",
      "Train Epoch: 201 [147712/225000 (66%)] Loss: 8082.416016\n",
      "Train Epoch: 201 [151808/225000 (67%)] Loss: 10363.085938\n",
      "Train Epoch: 201 [155904/225000 (69%)] Loss: 8208.312500\n",
      "Train Epoch: 201 [160000/225000 (71%)] Loss: 12956.871094\n",
      "Train Epoch: 201 [164096/225000 (73%)] Loss: 8052.992188\n",
      "Train Epoch: 201 [168192/225000 (75%)] Loss: 8064.462891\n",
      "Train Epoch: 201 [172288/225000 (77%)] Loss: 12168.189453\n",
      "Train Epoch: 201 [176384/225000 (78%)] Loss: 10549.404297\n",
      "Train Epoch: 201 [180480/225000 (80%)] Loss: 16185.089844\n",
      "Train Epoch: 201 [184576/225000 (82%)] Loss: 12364.068359\n",
      "Train Epoch: 201 [188672/225000 (84%)] Loss: 8229.861328\n",
      "Train Epoch: 201 [192768/225000 (86%)] Loss: 7989.181641\n",
      "Train Epoch: 201 [196864/225000 (87%)] Loss: 9777.656250\n",
      "Train Epoch: 201 [200960/225000 (89%)] Loss: 8032.869141\n",
      "Train Epoch: 201 [205056/225000 (91%)] Loss: 15346.486328\n",
      "Train Epoch: 201 [209152/225000 (93%)] Loss: 7995.246094\n",
      "Train Epoch: 201 [213248/225000 (95%)] Loss: 13569.173828\n",
      "Train Epoch: 201 [217344/225000 (97%)] Loss: 8085.865234\n",
      "Train Epoch: 201 [221440/225000 (98%)] Loss: 9699.005859\n",
      "    epoch          : 201\n",
      "    loss           : 9642.876784254124\n",
      "    val_loss       : 9641.592586091587\n",
      "Train Epoch: 202 [256/225000 (0%)] Loss: 8139.283203\n",
      "Train Epoch: 202 [4352/225000 (2%)] Loss: 8164.943359\n",
      "Train Epoch: 202 [8448/225000 (4%)] Loss: 9772.273438\n",
      "Train Epoch: 202 [12544/225000 (6%)] Loss: 8209.250000\n",
      "Train Epoch: 202 [16640/225000 (7%)] Loss: 10386.423828\n",
      "Train Epoch: 202 [20736/225000 (9%)] Loss: 10830.927734\n",
      "Train Epoch: 202 [24832/225000 (11%)] Loss: 8135.378906\n",
      "Train Epoch: 202 [28928/225000 (13%)] Loss: 7985.544922\n",
      "Train Epoch: 202 [33024/225000 (15%)] Loss: 9635.937500\n",
      "Train Epoch: 202 [37120/225000 (16%)] Loss: 8162.890625\n",
      "Train Epoch: 202 [41216/225000 (18%)] Loss: 10531.853516\n",
      "Train Epoch: 202 [45312/225000 (20%)] Loss: 10318.568359\n",
      "Train Epoch: 202 [49408/225000 (22%)] Loss: 7826.623047\n",
      "Train Epoch: 202 [53504/225000 (24%)] Loss: 7978.277344\n",
      "Train Epoch: 202 [57600/225000 (26%)] Loss: 8115.855469\n",
      "Train Epoch: 202 [61696/225000 (27%)] Loss: 8040.582031\n",
      "Train Epoch: 202 [65792/225000 (29%)] Loss: 7977.837891\n",
      "Train Epoch: 202 [69888/225000 (31%)] Loss: 8084.166016\n",
      "Train Epoch: 202 [73984/225000 (33%)] Loss: 8133.677734\n",
      "Train Epoch: 202 [78080/225000 (35%)] Loss: 9561.380859\n",
      "Train Epoch: 202 [82176/225000 (37%)] Loss: 14045.351562\n",
      "Train Epoch: 202 [86272/225000 (38%)] Loss: 8180.429688\n",
      "Train Epoch: 202 [90368/225000 (40%)] Loss: 8065.431641\n",
      "Train Epoch: 202 [94464/225000 (42%)] Loss: 8061.498047\n",
      "Train Epoch: 202 [98560/225000 (44%)] Loss: 8198.351562\n",
      "Train Epoch: 202 [102656/225000 (46%)] Loss: 12463.814453\n",
      "Train Epoch: 202 [106752/225000 (47%)] Loss: 8106.289062\n",
      "Train Epoch: 202 [110848/225000 (49%)] Loss: 7973.710938\n",
      "Train Epoch: 202 [114944/225000 (51%)] Loss: 8244.527344\n",
      "Train Epoch: 202 [119040/225000 (53%)] Loss: 8052.816406\n",
      "Train Epoch: 202 [123136/225000 (55%)] Loss: 9635.261719\n",
      "Train Epoch: 202 [127232/225000 (57%)] Loss: 8082.171875\n",
      "Train Epoch: 202 [131328/225000 (58%)] Loss: 7937.373047\n",
      "Train Epoch: 202 [135424/225000 (60%)] Loss: 9624.373047\n",
      "Train Epoch: 202 [139520/225000 (62%)] Loss: 7947.080078\n",
      "Train Epoch: 202 [143616/225000 (64%)] Loss: 7913.408203\n",
      "Train Epoch: 202 [147712/225000 (66%)] Loss: 8156.974609\n",
      "Train Epoch: 202 [151808/225000 (67%)] Loss: 19735.425781\n",
      "Train Epoch: 202 [155904/225000 (69%)] Loss: 7978.867188\n",
      "Train Epoch: 202 [160000/225000 (71%)] Loss: 8006.330078\n",
      "Train Epoch: 202 [164096/225000 (73%)] Loss: 7920.875000\n",
      "Train Epoch: 202 [168192/225000 (75%)] Loss: 10380.964844\n",
      "Train Epoch: 202 [172288/225000 (77%)] Loss: 8144.156250\n",
      "Train Epoch: 202 [176384/225000 (78%)] Loss: 8025.986328\n",
      "Train Epoch: 202 [180480/225000 (80%)] Loss: 8036.912109\n",
      "Train Epoch: 202 [184576/225000 (82%)] Loss: 8102.171875\n",
      "Train Epoch: 202 [188672/225000 (84%)] Loss: 9462.269531\n",
      "Train Epoch: 202 [192768/225000 (86%)] Loss: 10409.328125\n",
      "Train Epoch: 202 [196864/225000 (87%)] Loss: 8153.552734\n",
      "Train Epoch: 202 [200960/225000 (89%)] Loss: 7958.132812\n",
      "Train Epoch: 202 [205056/225000 (91%)] Loss: 9578.419922\n",
      "Train Epoch: 202 [209152/225000 (93%)] Loss: 9963.888672\n",
      "Train Epoch: 202 [213248/225000 (95%)] Loss: 9722.593750\n",
      "Train Epoch: 202 [217344/225000 (97%)] Loss: 8088.876953\n",
      "Train Epoch: 202 [221440/225000 (98%)] Loss: 13727.724609\n",
      "    epoch          : 202\n",
      "    loss           : 9603.223885007821\n",
      "    val_loss       : 9717.085683501497\n",
      "Train Epoch: 203 [256/225000 (0%)] Loss: 8035.720703\n",
      "Train Epoch: 203 [4352/225000 (2%)] Loss: 13684.880859\n",
      "Train Epoch: 203 [8448/225000 (4%)] Loss: 8130.539062\n",
      "Train Epoch: 203 [12544/225000 (6%)] Loss: 7938.513672\n",
      "Train Epoch: 203 [16640/225000 (7%)] Loss: 13812.205078\n",
      "Train Epoch: 203 [20736/225000 (9%)] Loss: 8085.201172\n",
      "Train Epoch: 203 [24832/225000 (11%)] Loss: 10849.601562\n",
      "Train Epoch: 203 [28928/225000 (13%)] Loss: 9475.740234\n",
      "Train Epoch: 203 [33024/225000 (15%)] Loss: 8142.464844\n",
      "Train Epoch: 203 [37120/225000 (16%)] Loss: 8045.537109\n",
      "Train Epoch: 203 [41216/225000 (18%)] Loss: 7880.728516\n",
      "Train Epoch: 203 [45312/225000 (20%)] Loss: 9820.892578\n",
      "Train Epoch: 203 [49408/225000 (22%)] Loss: 9633.152344\n",
      "Train Epoch: 203 [53504/225000 (24%)] Loss: 8143.439453\n",
      "Train Epoch: 203 [57600/225000 (26%)] Loss: 17749.628906\n",
      "Train Epoch: 203 [61696/225000 (27%)] Loss: 10266.314453\n",
      "Train Epoch: 203 [65792/225000 (29%)] Loss: 8219.251953\n",
      "Train Epoch: 203 [69888/225000 (31%)] Loss: 8107.900391\n",
      "Train Epoch: 203 [73984/225000 (33%)] Loss: 10556.816406\n",
      "Train Epoch: 203 [78080/225000 (35%)] Loss: 9453.843750\n",
      "Train Epoch: 203 [82176/225000 (37%)] Loss: 8057.126953\n",
      "Train Epoch: 203 [86272/225000 (38%)] Loss: 8041.773438\n",
      "Train Epoch: 203 [90368/225000 (40%)] Loss: 8164.236328\n",
      "Train Epoch: 203 [94464/225000 (42%)] Loss: 8147.185547\n",
      "Train Epoch: 203 [98560/225000 (44%)] Loss: 8027.923828\n",
      "Train Epoch: 203 [102656/225000 (46%)] Loss: 8304.636719\n",
      "Train Epoch: 203 [106752/225000 (47%)] Loss: 11031.197266\n",
      "Train Epoch: 203 [110848/225000 (49%)] Loss: 7986.875000\n",
      "Train Epoch: 203 [114944/225000 (51%)] Loss: 7967.166016\n",
      "Train Epoch: 203 [119040/225000 (53%)] Loss: 12611.041016\n",
      "Train Epoch: 203 [123136/225000 (55%)] Loss: 8180.291016\n",
      "Train Epoch: 203 [127232/225000 (57%)] Loss: 8024.767578\n",
      "Train Epoch: 203 [131328/225000 (58%)] Loss: 7924.691406\n",
      "Train Epoch: 203 [135424/225000 (60%)] Loss: 9489.144531\n",
      "Train Epoch: 203 [139520/225000 (62%)] Loss: 8185.832031\n",
      "Train Epoch: 203 [143616/225000 (64%)] Loss: 10554.333984\n",
      "Train Epoch: 203 [147712/225000 (66%)] Loss: 8057.134766\n",
      "Train Epoch: 203 [151808/225000 (67%)] Loss: 9619.441406\n",
      "Train Epoch: 203 [155904/225000 (69%)] Loss: 8050.033203\n",
      "Train Epoch: 203 [160000/225000 (71%)] Loss: 8318.541016\n",
      "Train Epoch: 203 [164096/225000 (73%)] Loss: 13452.267578\n",
      "Train Epoch: 203 [168192/225000 (75%)] Loss: 13544.150391\n",
      "Train Epoch: 203 [172288/225000 (77%)] Loss: 10659.113281\n",
      "Train Epoch: 203 [176384/225000 (78%)] Loss: 7907.726562\n",
      "Train Epoch: 203 [180480/225000 (80%)] Loss: 10403.205078\n",
      "Train Epoch: 203 [184576/225000 (82%)] Loss: 9414.130859\n",
      "Train Epoch: 203 [188672/225000 (84%)] Loss: 8014.957031\n",
      "Train Epoch: 203 [192768/225000 (86%)] Loss: 14197.140625\n",
      "Train Epoch: 203 [196864/225000 (87%)] Loss: 7920.292969\n",
      "Train Epoch: 203 [200960/225000 (89%)] Loss: 7875.658203\n",
      "Train Epoch: 203 [205056/225000 (91%)] Loss: 9712.353516\n",
      "Train Epoch: 203 [209152/225000 (93%)] Loss: 8103.568359\n",
      "Train Epoch: 203 [213248/225000 (95%)] Loss: 9587.314453\n",
      "Train Epoch: 203 [217344/225000 (97%)] Loss: 9612.625000\n",
      "Train Epoch: 203 [221440/225000 (98%)] Loss: 9632.017578\n",
      "    epoch          : 203\n",
      "    loss           : 9548.338146153299\n",
      "    val_loss       : 9228.544633184161\n",
      "Train Epoch: 204 [256/225000 (0%)] Loss: 8060.929688\n",
      "Train Epoch: 204 [4352/225000 (2%)] Loss: 9440.876953\n",
      "Train Epoch: 204 [8448/225000 (4%)] Loss: 7848.298828\n",
      "Train Epoch: 204 [12544/225000 (6%)] Loss: 8082.123047\n",
      "Train Epoch: 204 [16640/225000 (7%)] Loss: 10607.457031\n",
      "Train Epoch: 204 [20736/225000 (9%)] Loss: 10533.812500\n",
      "Train Epoch: 204 [24832/225000 (11%)] Loss: 10288.802734\n",
      "Train Epoch: 204 [28928/225000 (13%)] Loss: 8150.261719\n",
      "Train Epoch: 204 [33024/225000 (15%)] Loss: 8081.675781\n",
      "Train Epoch: 204 [37120/225000 (16%)] Loss: 7988.802734\n",
      "Train Epoch: 204 [41216/225000 (18%)] Loss: 7822.933594\n",
      "Train Epoch: 204 [45312/225000 (20%)] Loss: 9964.447266\n",
      "Train Epoch: 204 [49408/225000 (22%)] Loss: 11257.777344\n",
      "Train Epoch: 204 [53504/225000 (24%)] Loss: 7998.240234\n",
      "Train Epoch: 204 [57600/225000 (26%)] Loss: 10524.648438\n",
      "Train Epoch: 204 [61696/225000 (27%)] Loss: 8048.066406\n",
      "Train Epoch: 204 [65792/225000 (29%)] Loss: 10550.982422\n",
      "Train Epoch: 204 [69888/225000 (31%)] Loss: 9682.207031\n",
      "Train Epoch: 204 [73984/225000 (33%)] Loss: 8079.976562\n",
      "Train Epoch: 204 [78080/225000 (35%)] Loss: 7992.072266\n",
      "Train Epoch: 204 [82176/225000 (37%)] Loss: 8061.380859\n",
      "Train Epoch: 204 [86272/225000 (38%)] Loss: 10791.060547\n",
      "Train Epoch: 204 [90368/225000 (40%)] Loss: 10892.029297\n",
      "Train Epoch: 204 [94464/225000 (42%)] Loss: 13798.380859\n",
      "Train Epoch: 204 [98560/225000 (44%)] Loss: 13878.935547\n",
      "Train Epoch: 204 [102656/225000 (46%)] Loss: 7934.083984\n",
      "Train Epoch: 204 [106752/225000 (47%)] Loss: 9780.486328\n",
      "Train Epoch: 204 [110848/225000 (49%)] Loss: 8147.214844\n",
      "Train Epoch: 204 [114944/225000 (51%)] Loss: 8007.224609\n",
      "Train Epoch: 204 [119040/225000 (53%)] Loss: 12137.125000\n",
      "Train Epoch: 204 [123136/225000 (55%)] Loss: 8112.498047\n",
      "Train Epoch: 204 [127232/225000 (57%)] Loss: 13690.142578\n",
      "Train Epoch: 204 [131328/225000 (58%)] Loss: 9455.832031\n",
      "Train Epoch: 204 [135424/225000 (60%)] Loss: 8233.275391\n",
      "Train Epoch: 204 [139520/225000 (62%)] Loss: 9883.273438\n",
      "Train Epoch: 204 [143616/225000 (64%)] Loss: 8112.765625\n",
      "Train Epoch: 204 [147712/225000 (66%)] Loss: 9576.224609\n",
      "Train Epoch: 204 [151808/225000 (67%)] Loss: 9588.517578\n",
      "Train Epoch: 204 [155904/225000 (69%)] Loss: 12209.931641\n",
      "Train Epoch: 204 [160000/225000 (71%)] Loss: 8035.464844\n",
      "Train Epoch: 204 [164096/225000 (73%)] Loss: 12232.263672\n",
      "Train Epoch: 204 [168192/225000 (75%)] Loss: 7979.888672\n",
      "Train Epoch: 204 [172288/225000 (77%)] Loss: 8075.728516\n",
      "Train Epoch: 204 [176384/225000 (78%)] Loss: 7913.601562\n",
      "Train Epoch: 204 [180480/225000 (80%)] Loss: 8223.373047\n",
      "Train Epoch: 204 [184576/225000 (82%)] Loss: 7972.279297\n",
      "Train Epoch: 204 [188672/225000 (84%)] Loss: 8175.277344\n",
      "Train Epoch: 204 [192768/225000 (86%)] Loss: 8110.835938\n",
      "Train Epoch: 204 [196864/225000 (87%)] Loss: 7965.556641\n",
      "Train Epoch: 204 [200960/225000 (89%)] Loss: 10560.271484\n",
      "Train Epoch: 204 [205056/225000 (91%)] Loss: 10660.154297\n",
      "Train Epoch: 204 [209152/225000 (93%)] Loss: 7954.693359\n",
      "Train Epoch: 204 [213248/225000 (95%)] Loss: 9592.994141\n",
      "Train Epoch: 204 [217344/225000 (97%)] Loss: 8142.685547\n",
      "Train Epoch: 204 [221440/225000 (98%)] Loss: 8021.640625\n",
      "    epoch          : 204\n",
      "    loss           : 9488.876830915813\n",
      "    val_loss       : 9667.858660328146\n",
      "Train Epoch: 205 [256/225000 (0%)] Loss: 12398.330078\n",
      "Train Epoch: 205 [4352/225000 (2%)] Loss: 7976.226562\n",
      "Train Epoch: 205 [8448/225000 (4%)] Loss: 8161.318359\n",
      "Train Epoch: 205 [12544/225000 (6%)] Loss: 8079.439453\n",
      "Train Epoch: 205 [16640/225000 (7%)] Loss: 9609.964844\n",
      "Train Epoch: 205 [20736/225000 (9%)] Loss: 10466.412109\n",
      "Train Epoch: 205 [24832/225000 (11%)] Loss: 9820.435547\n",
      "Train Epoch: 205 [28928/225000 (13%)] Loss: 7941.511719\n",
      "Train Epoch: 205 [33024/225000 (15%)] Loss: 8198.544922\n",
      "Train Epoch: 205 [37120/225000 (16%)] Loss: 10441.335938\n",
      "Train Epoch: 205 [41216/225000 (18%)] Loss: 8062.492188\n",
      "Train Epoch: 205 [45312/225000 (20%)] Loss: 8099.541016\n",
      "Train Epoch: 205 [49408/225000 (22%)] Loss: 8288.898438\n",
      "Train Epoch: 205 [53504/225000 (24%)] Loss: 8166.070312\n",
      "Train Epoch: 205 [57600/225000 (26%)] Loss: 9674.744141\n",
      "Train Epoch: 205 [61696/225000 (27%)] Loss: 9394.107422\n",
      "Train Epoch: 205 [65792/225000 (29%)] Loss: 8074.162109\n",
      "Train Epoch: 205 [69888/225000 (31%)] Loss: 8124.695312\n",
      "Train Epoch: 205 [73984/225000 (33%)] Loss: 9673.041016\n",
      "Train Epoch: 205 [78080/225000 (35%)] Loss: 13816.197266\n",
      "Train Epoch: 205 [82176/225000 (37%)] Loss: 17778.412109\n",
      "Train Epoch: 205 [86272/225000 (38%)] Loss: 7888.587891\n",
      "Train Epoch: 205 [90368/225000 (40%)] Loss: 10447.693359\n",
      "Train Epoch: 205 [94464/225000 (42%)] Loss: 8247.908203\n",
      "Train Epoch: 205 [98560/225000 (44%)] Loss: 9573.009766\n",
      "Train Epoch: 205 [102656/225000 (46%)] Loss: 9611.078125\n",
      "Train Epoch: 205 [106752/225000 (47%)] Loss: 9640.429688\n",
      "Train Epoch: 205 [110848/225000 (49%)] Loss: 8126.833984\n",
      "Train Epoch: 205 [114944/225000 (51%)] Loss: 7913.343750\n",
      "Train Epoch: 205 [119040/225000 (53%)] Loss: 8172.707031\n",
      "Train Epoch: 205 [123136/225000 (55%)] Loss: 8005.972656\n",
      "Train Epoch: 205 [127232/225000 (57%)] Loss: 7987.892578\n",
      "Train Epoch: 205 [131328/225000 (58%)] Loss: 7929.660156\n",
      "Train Epoch: 205 [135424/225000 (60%)] Loss: 9640.109375\n",
      "Train Epoch: 205 [139520/225000 (62%)] Loss: 8097.937500\n",
      "Train Epoch: 205 [143616/225000 (64%)] Loss: 8223.869141\n",
      "Train Epoch: 205 [147712/225000 (66%)] Loss: 8080.441406\n",
      "Train Epoch: 205 [151808/225000 (67%)] Loss: 8036.640625\n",
      "Train Epoch: 205 [155904/225000 (69%)] Loss: 8050.679688\n",
      "Train Epoch: 205 [160000/225000 (71%)] Loss: 7993.595703\n",
      "Train Epoch: 205 [164096/225000 (73%)] Loss: 12745.998047\n",
      "Train Epoch: 205 [168192/225000 (75%)] Loss: 8014.224609\n",
      "Train Epoch: 205 [172288/225000 (77%)] Loss: 9585.785156\n",
      "Train Epoch: 205 [176384/225000 (78%)] Loss: 8016.750000\n",
      "Train Epoch: 205 [180480/225000 (80%)] Loss: 8204.527344\n",
      "Train Epoch: 205 [184576/225000 (82%)] Loss: 8107.423828\n",
      "Train Epoch: 205 [188672/225000 (84%)] Loss: 13709.095703\n",
      "Train Epoch: 205 [192768/225000 (86%)] Loss: 8102.488281\n",
      "Train Epoch: 205 [196864/225000 (87%)] Loss: 15652.046875\n",
      "Train Epoch: 205 [200960/225000 (89%)] Loss: 9575.009766\n",
      "Train Epoch: 205 [205056/225000 (91%)] Loss: 8148.103516\n",
      "Train Epoch: 205 [209152/225000 (93%)] Loss: 9526.173828\n",
      "Train Epoch: 205 [213248/225000 (95%)] Loss: 12437.919922\n",
      "Train Epoch: 205 [217344/225000 (97%)] Loss: 8122.568359\n",
      "Train Epoch: 205 [221440/225000 (98%)] Loss: 8227.234375\n",
      "    epoch          : 205\n",
      "    loss           : 9517.501237645762\n",
      "    val_loss       : 9836.38012042824\n",
      "Train Epoch: 206 [256/225000 (0%)] Loss: 8205.691406\n",
      "Train Epoch: 206 [4352/225000 (2%)] Loss: 8023.517578\n",
      "Train Epoch: 206 [8448/225000 (4%)] Loss: 8012.080078\n",
      "Train Epoch: 206 [12544/225000 (6%)] Loss: 8129.244141\n",
      "Train Epoch: 206 [16640/225000 (7%)] Loss: 9771.404297\n",
      "Train Epoch: 206 [20736/225000 (9%)] Loss: 9677.367188\n",
      "Train Epoch: 206 [24832/225000 (11%)] Loss: 10783.810547\n",
      "Train Epoch: 206 [28928/225000 (13%)] Loss: 9670.328125\n",
      "Train Epoch: 206 [33024/225000 (15%)] Loss: 8172.685547\n",
      "Train Epoch: 206 [37120/225000 (16%)] Loss: 9760.486328\n",
      "Train Epoch: 206 [41216/225000 (18%)] Loss: 8140.722656\n",
      "Train Epoch: 206 [45312/225000 (20%)] Loss: 16919.339844\n",
      "Train Epoch: 206 [49408/225000 (22%)] Loss: 8012.318359\n",
      "Train Epoch: 206 [53504/225000 (24%)] Loss: 10563.132812\n",
      "Train Epoch: 206 [57600/225000 (26%)] Loss: 10383.500000\n",
      "Train Epoch: 206 [61696/225000 (27%)] Loss: 12155.964844\n",
      "Train Epoch: 206 [65792/225000 (29%)] Loss: 7969.996094\n",
      "Train Epoch: 206 [69888/225000 (31%)] Loss: 8218.808594\n",
      "Train Epoch: 206 [73984/225000 (33%)] Loss: 8150.128906\n",
      "Train Epoch: 206 [78080/225000 (35%)] Loss: 8170.033203\n",
      "Train Epoch: 206 [82176/225000 (37%)] Loss: 9703.263672\n",
      "Train Epoch: 206 [86272/225000 (38%)] Loss: 7978.558594\n",
      "Train Epoch: 206 [90368/225000 (40%)] Loss: 7930.568359\n",
      "Train Epoch: 206 [94464/225000 (42%)] Loss: 12617.503906\n",
      "Train Epoch: 206 [98560/225000 (44%)] Loss: 12152.630859\n",
      "Train Epoch: 206 [102656/225000 (46%)] Loss: 10575.751953\n",
      "Train Epoch: 206 [106752/225000 (47%)] Loss: 7819.302734\n",
      "Train Epoch: 206 [110848/225000 (49%)] Loss: 8146.343750\n",
      "Train Epoch: 206 [114944/225000 (51%)] Loss: 7995.804688\n",
      "Train Epoch: 206 [119040/225000 (53%)] Loss: 8189.597656\n",
      "Train Epoch: 206 [123136/225000 (55%)] Loss: 9598.359375\n",
      "Train Epoch: 206 [127232/225000 (57%)] Loss: 8003.712891\n",
      "Train Epoch: 206 [131328/225000 (58%)] Loss: 8157.941406\n",
      "Train Epoch: 206 [135424/225000 (60%)] Loss: 7963.218750\n",
      "Train Epoch: 206 [139520/225000 (62%)] Loss: 8072.970703\n",
      "Train Epoch: 206 [143616/225000 (64%)] Loss: 10465.503906\n",
      "Train Epoch: 206 [147712/225000 (66%)] Loss: 8004.822266\n",
      "Train Epoch: 206 [151808/225000 (67%)] Loss: 7952.203125\n",
      "Train Epoch: 206 [155904/225000 (69%)] Loss: 8048.437500\n",
      "Train Epoch: 206 [160000/225000 (71%)] Loss: 12859.222656\n",
      "Train Epoch: 206 [164096/225000 (73%)] Loss: 12292.054688\n",
      "Train Epoch: 206 [168192/225000 (75%)] Loss: 9820.605469\n",
      "Train Epoch: 206 [172288/225000 (77%)] Loss: 13831.769531\n",
      "Train Epoch: 206 [176384/225000 (78%)] Loss: 9703.544922\n",
      "Train Epoch: 206 [180480/225000 (80%)] Loss: 13622.527344\n",
      "Train Epoch: 206 [184576/225000 (82%)] Loss: 8112.367188\n",
      "Train Epoch: 206 [188672/225000 (84%)] Loss: 8111.343750\n",
      "Train Epoch: 206 [192768/225000 (86%)] Loss: 8145.416016\n",
      "Train Epoch: 206 [196864/225000 (87%)] Loss: 10536.902344\n",
      "Train Epoch: 206 [200960/225000 (89%)] Loss: 13632.708984\n",
      "Train Epoch: 206 [205056/225000 (91%)] Loss: 9661.681641\n",
      "Train Epoch: 206 [209152/225000 (93%)] Loss: 10607.691406\n",
      "Train Epoch: 206 [213248/225000 (95%)] Loss: 7874.207031\n",
      "Train Epoch: 206 [217344/225000 (97%)] Loss: 7934.429688\n",
      "Train Epoch: 206 [221440/225000 (98%)] Loss: 13644.468750\n",
      "    epoch          : 206\n",
      "    loss           : 9459.86680865152\n",
      "    val_loss       : 9803.963448626655\n",
      "Train Epoch: 207 [256/225000 (0%)] Loss: 8188.181641\n",
      "Train Epoch: 207 [4352/225000 (2%)] Loss: 8124.708984\n",
      "Train Epoch: 207 [8448/225000 (4%)] Loss: 7978.244141\n",
      "Train Epoch: 207 [12544/225000 (6%)] Loss: 10798.185547\n",
      "Train Epoch: 207 [16640/225000 (7%)] Loss: 8238.728516\n",
      "Train Epoch: 207 [20736/225000 (9%)] Loss: 10444.326172\n",
      "Train Epoch: 207 [24832/225000 (11%)] Loss: 8041.253906\n",
      "Train Epoch: 207 [28928/225000 (13%)] Loss: 12761.488281\n",
      "Train Epoch: 207 [33024/225000 (15%)] Loss: 13693.271484\n",
      "Train Epoch: 207 [37120/225000 (16%)] Loss: 7825.117188\n",
      "Train Epoch: 207 [41216/225000 (18%)] Loss: 8107.527344\n",
      "Train Epoch: 207 [45312/225000 (20%)] Loss: 9729.871094\n",
      "Train Epoch: 207 [49408/225000 (22%)] Loss: 8333.302734\n",
      "Train Epoch: 207 [53504/225000 (24%)] Loss: 13421.003906\n",
      "Train Epoch: 207 [57600/225000 (26%)] Loss: 10478.273438\n",
      "Train Epoch: 207 [61696/225000 (27%)] Loss: 8183.751953\n",
      "Train Epoch: 207 [65792/225000 (29%)] Loss: 8032.990234\n",
      "Train Epoch: 207 [69888/225000 (31%)] Loss: 9752.292969\n",
      "Train Epoch: 207 [73984/225000 (33%)] Loss: 8074.609375\n",
      "Train Epoch: 207 [78080/225000 (35%)] Loss: 15499.289062\n",
      "Train Epoch: 207 [82176/225000 (37%)] Loss: 8011.019531\n",
      "Train Epoch: 207 [86272/225000 (38%)] Loss: 8003.126953\n",
      "Train Epoch: 207 [90368/225000 (40%)] Loss: 8138.044922\n",
      "Train Epoch: 207 [94464/225000 (42%)] Loss: 8129.931641\n",
      "Train Epoch: 207 [98560/225000 (44%)] Loss: 8087.294922\n",
      "Train Epoch: 207 [102656/225000 (46%)] Loss: 15396.167969\n",
      "Train Epoch: 207 [106752/225000 (47%)] Loss: 7999.640625\n",
      "Train Epoch: 207 [110848/225000 (49%)] Loss: 9486.166016\n",
      "Train Epoch: 207 [114944/225000 (51%)] Loss: 10582.404297\n",
      "Train Epoch: 207 [119040/225000 (53%)] Loss: 7989.593750\n",
      "Train Epoch: 207 [123136/225000 (55%)] Loss: 8019.488281\n",
      "Train Epoch: 207 [127232/225000 (57%)] Loss: 10513.294922\n",
      "Train Epoch: 207 [131328/225000 (58%)] Loss: 8066.439453\n",
      "Train Epoch: 207 [135424/225000 (60%)] Loss: 8062.957031\n",
      "Train Epoch: 207 [139520/225000 (62%)] Loss: 8163.458984\n",
      "Train Epoch: 207 [143616/225000 (64%)] Loss: 8192.724609\n",
      "Train Epoch: 207 [147712/225000 (66%)] Loss: 8138.541016\n",
      "Train Epoch: 207 [151808/225000 (67%)] Loss: 7842.501953\n",
      "Train Epoch: 207 [155904/225000 (69%)] Loss: 10288.794922\n",
      "Train Epoch: 207 [160000/225000 (71%)] Loss: 8241.685547\n",
      "Train Epoch: 207 [164096/225000 (73%)] Loss: 8018.484375\n",
      "Train Epoch: 207 [168192/225000 (75%)] Loss: 8048.521484\n",
      "Train Epoch: 207 [172288/225000 (77%)] Loss: 8016.708984\n",
      "Train Epoch: 207 [176384/225000 (78%)] Loss: 10543.388672\n",
      "Train Epoch: 207 [180480/225000 (80%)] Loss: 11881.828125\n",
      "Train Epoch: 207 [184576/225000 (82%)] Loss: 9787.966797\n",
      "Train Epoch: 207 [188672/225000 (84%)] Loss: 12281.771484\n",
      "Train Epoch: 207 [192768/225000 (86%)] Loss: 8145.507812\n",
      "Train Epoch: 207 [196864/225000 (87%)] Loss: 10402.193359\n",
      "Train Epoch: 207 [200960/225000 (89%)] Loss: 11214.341797\n",
      "Train Epoch: 207 [205056/225000 (91%)] Loss: 8048.693359\n",
      "Train Epoch: 207 [209152/225000 (93%)] Loss: 8039.755859\n",
      "Train Epoch: 207 [213248/225000 (95%)] Loss: 8060.484375\n",
      "Train Epoch: 207 [217344/225000 (97%)] Loss: 8033.875000\n",
      "Train Epoch: 207 [221440/225000 (98%)] Loss: 13398.515625\n",
      "    epoch          : 207\n",
      "    loss           : 9488.04674723585\n",
      "    val_loss       : 9444.050796805595\n",
      "Train Epoch: 208 [256/225000 (0%)] Loss: 8014.582031\n",
      "Train Epoch: 208 [4352/225000 (2%)] Loss: 16350.498047\n",
      "Train Epoch: 208 [8448/225000 (4%)] Loss: 7911.029297\n",
      "Train Epoch: 208 [12544/225000 (6%)] Loss: 8033.345703\n",
      "Train Epoch: 208 [16640/225000 (7%)] Loss: 8119.054688\n",
      "Train Epoch: 208 [20736/225000 (9%)] Loss: 8005.394531\n",
      "Train Epoch: 208 [24832/225000 (11%)] Loss: 8213.855469\n",
      "Train Epoch: 208 [28928/225000 (13%)] Loss: 10687.216797\n",
      "Train Epoch: 208 [33024/225000 (15%)] Loss: 10539.125000\n",
      "Train Epoch: 208 [37120/225000 (16%)] Loss: 7860.572266\n",
      "Train Epoch: 208 [41216/225000 (18%)] Loss: 8015.335938\n",
      "Train Epoch: 208 [45312/225000 (20%)] Loss: 9697.554688\n",
      "Train Epoch: 208 [49408/225000 (22%)] Loss: 9824.308594\n",
      "Train Epoch: 208 [53504/225000 (24%)] Loss: 8002.423828\n",
      "Train Epoch: 208 [57600/225000 (26%)] Loss: 10639.320312\n",
      "Train Epoch: 208 [61696/225000 (27%)] Loss: 12894.816406\n",
      "Train Epoch: 208 [65792/225000 (29%)] Loss: 11132.078125\n",
      "Train Epoch: 208 [69888/225000 (31%)] Loss: 8085.166016\n",
      "Train Epoch: 208 [73984/225000 (33%)] Loss: 8155.449219\n",
      "Train Epoch: 208 [78080/225000 (35%)] Loss: 7876.687500\n",
      "Train Epoch: 208 [82176/225000 (37%)] Loss: 7956.861328\n",
      "Train Epoch: 208 [86272/225000 (38%)] Loss: 8052.808594\n",
      "Train Epoch: 208 [90368/225000 (40%)] Loss: 8011.697266\n",
      "Train Epoch: 208 [94464/225000 (42%)] Loss: 8027.392578\n",
      "Train Epoch: 208 [98560/225000 (44%)] Loss: 8359.320312\n",
      "Train Epoch: 208 [102656/225000 (46%)] Loss: 10275.369141\n",
      "Train Epoch: 208 [106752/225000 (47%)] Loss: 8052.710938\n",
      "Train Epoch: 208 [110848/225000 (49%)] Loss: 8119.982422\n",
      "Train Epoch: 208 [114944/225000 (51%)] Loss: 10689.222656\n",
      "Train Epoch: 208 [119040/225000 (53%)] Loss: 7943.046875\n",
      "Train Epoch: 208 [123136/225000 (55%)] Loss: 8078.072266\n",
      "Train Epoch: 208 [127232/225000 (57%)] Loss: 8142.646484\n",
      "Train Epoch: 208 [131328/225000 (58%)] Loss: 8134.298828\n",
      "Train Epoch: 208 [135424/225000 (60%)] Loss: 16090.939453\n",
      "Train Epoch: 208 [139520/225000 (62%)] Loss: 9482.750000\n",
      "Train Epoch: 208 [143616/225000 (64%)] Loss: 10669.597656\n",
      "Train Epoch: 208 [147712/225000 (66%)] Loss: 13697.376953\n",
      "Train Epoch: 208 [151808/225000 (67%)] Loss: 9421.998047\n",
      "Train Epoch: 208 [155904/225000 (69%)] Loss: 8008.744141\n",
      "Train Epoch: 208 [160000/225000 (71%)] Loss: 16369.320312\n",
      "Train Epoch: 208 [164096/225000 (73%)] Loss: 12413.638672\n",
      "Train Epoch: 208 [168192/225000 (75%)] Loss: 7995.251953\n",
      "Train Epoch: 208 [172288/225000 (77%)] Loss: 8197.777344\n",
      "Train Epoch: 208 [176384/225000 (78%)] Loss: 9538.769531\n",
      "Train Epoch: 208 [180480/225000 (80%)] Loss: 13024.738281\n",
      "Train Epoch: 208 [184576/225000 (82%)] Loss: 11078.330078\n",
      "Train Epoch: 208 [188672/225000 (84%)] Loss: 8019.455078\n",
      "Train Epoch: 208 [192768/225000 (86%)] Loss: 8174.482422\n",
      "Train Epoch: 208 [196864/225000 (87%)] Loss: 7985.035156\n",
      "Train Epoch: 208 [200960/225000 (89%)] Loss: 8165.224609\n",
      "Train Epoch: 208 [205056/225000 (91%)] Loss: 7977.923828\n",
      "Train Epoch: 208 [209152/225000 (93%)] Loss: 14461.068359\n",
      "Train Epoch: 208 [213248/225000 (95%)] Loss: 8021.626953\n",
      "Train Epoch: 208 [217344/225000 (97%)] Loss: 7891.646484\n",
      "Train Epoch: 208 [221440/225000 (98%)] Loss: 8046.535156\n",
      "    epoch          : 208\n",
      "    loss           : 9455.239845527589\n",
      "    val_loss       : 9152.96070918745\n",
      "Train Epoch: 209 [256/225000 (0%)] Loss: 8073.746094\n",
      "Train Epoch: 209 [4352/225000 (2%)] Loss: 8055.523438\n",
      "Train Epoch: 209 [8448/225000 (4%)] Loss: 10437.814453\n",
      "Train Epoch: 209 [12544/225000 (6%)] Loss: 8093.242188\n",
      "Train Epoch: 209 [16640/225000 (7%)] Loss: 8103.263672\n",
      "Train Epoch: 209 [20736/225000 (9%)] Loss: 9656.708984\n",
      "Train Epoch: 209 [24832/225000 (11%)] Loss: 8003.574219\n",
      "Train Epoch: 209 [28928/225000 (13%)] Loss: 12430.427734\n",
      "Train Epoch: 209 [33024/225000 (15%)] Loss: 8067.423828\n",
      "Train Epoch: 209 [37120/225000 (16%)] Loss: 8138.791016\n",
      "Train Epoch: 209 [41216/225000 (18%)] Loss: 10991.992188\n",
      "Train Epoch: 209 [45312/225000 (20%)] Loss: 8072.162109\n",
      "Train Epoch: 209 [49408/225000 (22%)] Loss: 7902.068359\n",
      "Train Epoch: 209 [53504/225000 (24%)] Loss: 10507.398438\n",
      "Train Epoch: 209 [57600/225000 (26%)] Loss: 8161.269531\n",
      "Train Epoch: 209 [61696/225000 (27%)] Loss: 8163.271484\n",
      "Train Epoch: 209 [65792/225000 (29%)] Loss: 10412.552734\n",
      "Train Epoch: 209 [69888/225000 (31%)] Loss: 12152.093750\n",
      "Train Epoch: 209 [73984/225000 (33%)] Loss: 16257.001953\n",
      "Train Epoch: 209 [78080/225000 (35%)] Loss: 8429.632812\n",
      "Train Epoch: 209 [82176/225000 (37%)] Loss: 8049.675781\n",
      "Train Epoch: 209 [86272/225000 (38%)] Loss: 8069.492188\n",
      "Train Epoch: 209 [90368/225000 (40%)] Loss: 10639.990234\n",
      "Train Epoch: 209 [94464/225000 (42%)] Loss: 9751.179688\n",
      "Train Epoch: 209 [98560/225000 (44%)] Loss: 13725.328125\n",
      "Train Epoch: 209 [102656/225000 (46%)] Loss: 8201.136719\n",
      "Train Epoch: 209 [106752/225000 (47%)] Loss: 9759.314453\n",
      "Train Epoch: 209 [110848/225000 (49%)] Loss: 9480.500000\n",
      "Train Epoch: 209 [114944/225000 (51%)] Loss: 8359.830078\n",
      "Train Epoch: 209 [119040/225000 (53%)] Loss: 8030.189453\n",
      "Train Epoch: 209 [123136/225000 (55%)] Loss: 8166.458984\n",
      "Train Epoch: 209 [127232/225000 (57%)] Loss: 12210.613281\n",
      "Train Epoch: 209 [131328/225000 (58%)] Loss: 7841.859375\n",
      "Train Epoch: 209 [135424/225000 (60%)] Loss: 10232.845703\n",
      "Train Epoch: 209 [139520/225000 (62%)] Loss: 10622.072266\n",
      "Train Epoch: 209 [143616/225000 (64%)] Loss: 12275.070312\n",
      "Train Epoch: 209 [147712/225000 (66%)] Loss: 13537.582031\n",
      "Train Epoch: 209 [151808/225000 (67%)] Loss: 8047.048828\n",
      "Train Epoch: 209 [155904/225000 (69%)] Loss: 9588.068359\n",
      "Train Epoch: 209 [160000/225000 (71%)] Loss: 7992.101562\n",
      "Train Epoch: 209 [164096/225000 (73%)] Loss: 9649.384766\n",
      "Train Epoch: 209 [168192/225000 (75%)] Loss: 9520.845703\n",
      "Train Epoch: 209 [172288/225000 (77%)] Loss: 7975.626953\n",
      "Train Epoch: 209 [176384/225000 (78%)] Loss: 7945.238281\n",
      "Train Epoch: 209 [180480/225000 (80%)] Loss: 10542.726562\n",
      "Train Epoch: 209 [184576/225000 (82%)] Loss: 8150.824219\n",
      "Train Epoch: 209 [188672/225000 (84%)] Loss: 12511.201172\n",
      "Train Epoch: 209 [192768/225000 (86%)] Loss: 8024.816406\n",
      "Train Epoch: 209 [196864/225000 (87%)] Loss: 7841.609375\n",
      "Train Epoch: 209 [200960/225000 (89%)] Loss: 9743.972656\n",
      "Train Epoch: 209 [205056/225000 (91%)] Loss: 8308.277344\n",
      "Train Epoch: 209 [209152/225000 (93%)] Loss: 7892.240234\n",
      "Train Epoch: 209 [213248/225000 (95%)] Loss: 12157.308594\n",
      "Train Epoch: 209 [217344/225000 (97%)] Loss: 7921.242188\n",
      "Train Epoch: 209 [221440/225000 (98%)] Loss: 10648.630859\n",
      "    epoch          : 209\n",
      "    loss           : 9469.212108486206\n",
      "    val_loss       : 9813.929907793901\n",
      "Train Epoch: 210 [256/225000 (0%)] Loss: 7998.632812\n",
      "Train Epoch: 210 [4352/225000 (2%)] Loss: 10659.818359\n",
      "Train Epoch: 210 [8448/225000 (4%)] Loss: 8003.886719\n",
      "Train Epoch: 210 [12544/225000 (6%)] Loss: 13590.128906\n",
      "Train Epoch: 210 [16640/225000 (7%)] Loss: 10466.560547\n",
      "Train Epoch: 210 [20736/225000 (9%)] Loss: 9542.476562\n",
      "Train Epoch: 210 [24832/225000 (11%)] Loss: 9671.056641\n",
      "Train Epoch: 210 [28928/225000 (13%)] Loss: 9815.886719\n",
      "Train Epoch: 210 [33024/225000 (15%)] Loss: 8115.236328\n",
      "Train Epoch: 210 [37120/225000 (16%)] Loss: 8030.580078\n",
      "Train Epoch: 210 [41216/225000 (18%)] Loss: 8017.146484\n",
      "Train Epoch: 210 [45312/225000 (20%)] Loss: 7953.285156\n",
      "Train Epoch: 210 [49408/225000 (22%)] Loss: 9591.142578\n",
      "Train Epoch: 210 [53504/225000 (24%)] Loss: 13019.871094\n",
      "Train Epoch: 210 [57600/225000 (26%)] Loss: 8109.044922\n",
      "Train Epoch: 210 [61696/225000 (27%)] Loss: 10500.011719\n",
      "Train Epoch: 210 [65792/225000 (29%)] Loss: 10384.904297\n",
      "Train Epoch: 210 [69888/225000 (31%)] Loss: 10562.630859\n",
      "Train Epoch: 210 [73984/225000 (33%)] Loss: 7979.558594\n",
      "Train Epoch: 210 [78080/225000 (35%)] Loss: 9473.869141\n",
      "Train Epoch: 210 [82176/225000 (37%)] Loss: 10561.802734\n",
      "Train Epoch: 210 [86272/225000 (38%)] Loss: 16118.142578\n",
      "Train Epoch: 210 [90368/225000 (40%)] Loss: 7906.951172\n",
      "Train Epoch: 210 [94464/225000 (42%)] Loss: 12306.378906\n",
      "Train Epoch: 210 [98560/225000 (44%)] Loss: 8062.554688\n",
      "Train Epoch: 210 [102656/225000 (46%)] Loss: 8284.492188\n",
      "Train Epoch: 210 [106752/225000 (47%)] Loss: 15205.628906\n",
      "Train Epoch: 210 [110848/225000 (49%)] Loss: 8246.394531\n",
      "Train Epoch: 210 [114944/225000 (51%)] Loss: 8046.742188\n",
      "Train Epoch: 210 [119040/225000 (53%)] Loss: 8158.404297\n",
      "Train Epoch: 210 [123136/225000 (55%)] Loss: 10445.839844\n",
      "Train Epoch: 210 [127232/225000 (57%)] Loss: 8204.443359\n",
      "Train Epoch: 210 [131328/225000 (58%)] Loss: 13671.308594\n",
      "Train Epoch: 210 [135424/225000 (60%)] Loss: 13606.148438\n",
      "Train Epoch: 210 [139520/225000 (62%)] Loss: 8186.662109\n",
      "Train Epoch: 210 [143616/225000 (64%)] Loss: 8075.675781\n",
      "Train Epoch: 210 [147712/225000 (66%)] Loss: 7948.185547\n",
      "Train Epoch: 210 [151808/225000 (67%)] Loss: 7956.060547\n",
      "Train Epoch: 210 [155904/225000 (69%)] Loss: 8034.054688\n",
      "Train Epoch: 210 [160000/225000 (71%)] Loss: 8076.613281\n",
      "Train Epoch: 210 [164096/225000 (73%)] Loss: 8122.414062\n",
      "Train Epoch: 210 [168192/225000 (75%)] Loss: 7928.181641\n",
      "Train Epoch: 210 [172288/225000 (77%)] Loss: 8095.050781\n",
      "Train Epoch: 210 [176384/225000 (78%)] Loss: 8163.335938\n",
      "Train Epoch: 210 [180480/225000 (80%)] Loss: 10746.138672\n",
      "Train Epoch: 210 [184576/225000 (82%)] Loss: 8157.560547\n",
      "Train Epoch: 210 [188672/225000 (84%)] Loss: 8192.384766\n",
      "Train Epoch: 210 [192768/225000 (86%)] Loss: 8169.539062\n",
      "Train Epoch: 210 [196864/225000 (87%)] Loss: 8106.816406\n",
      "Train Epoch: 210 [200960/225000 (89%)] Loss: 13006.570312\n",
      "Train Epoch: 210 [205056/225000 (91%)] Loss: 8088.248047\n",
      "Train Epoch: 210 [209152/225000 (93%)] Loss: 7985.027344\n",
      "Train Epoch: 210 [213248/225000 (95%)] Loss: 9851.943359\n",
      "Train Epoch: 210 [217344/225000 (97%)] Loss: 7927.382812\n",
      "Train Epoch: 210 [221440/225000 (98%)] Loss: 8096.708984\n",
      "    epoch          : 210\n",
      "    loss           : 9540.37603211213\n",
      "    val_loss       : 9555.825259886225\n",
      "Train Epoch: 211 [256/225000 (0%)] Loss: 12866.498047\n",
      "Train Epoch: 211 [4352/225000 (2%)] Loss: 8061.650391\n",
      "Train Epoch: 211 [8448/225000 (4%)] Loss: 13769.599609\n",
      "Train Epoch: 211 [12544/225000 (6%)] Loss: 7957.246094\n",
      "Train Epoch: 211 [16640/225000 (7%)] Loss: 8187.361328\n",
      "Train Epoch: 211 [20736/225000 (9%)] Loss: 8128.566406\n",
      "Train Epoch: 211 [24832/225000 (11%)] Loss: 7905.859375\n",
      "Train Epoch: 211 [28928/225000 (13%)] Loss: 8141.205078\n",
      "Train Epoch: 211 [33024/225000 (15%)] Loss: 10541.345703\n",
      "Train Epoch: 211 [37120/225000 (16%)] Loss: 8158.591797\n",
      "Train Epoch: 211 [41216/225000 (18%)] Loss: 9486.498047\n",
      "Train Epoch: 211 [45312/225000 (20%)] Loss: 8154.746094\n",
      "Train Epoch: 211 [49408/225000 (22%)] Loss: 9609.486328\n",
      "Train Epoch: 211 [53504/225000 (24%)] Loss: 8008.462891\n",
      "Train Epoch: 211 [57600/225000 (26%)] Loss: 9863.527344\n",
      "Train Epoch: 211 [61696/225000 (27%)] Loss: 10652.570312\n",
      "Train Epoch: 211 [65792/225000 (29%)] Loss: 10618.402344\n",
      "Train Epoch: 211 [69888/225000 (31%)] Loss: 12771.910156\n",
      "Train Epoch: 211 [73984/225000 (33%)] Loss: 10696.949219\n",
      "Train Epoch: 211 [78080/225000 (35%)] Loss: 8058.173828\n",
      "Train Epoch: 211 [82176/225000 (37%)] Loss: 8118.146484\n",
      "Train Epoch: 211 [86272/225000 (38%)] Loss: 10349.433594\n",
      "Train Epoch: 211 [90368/225000 (40%)] Loss: 10482.349609\n",
      "Train Epoch: 211 [94464/225000 (42%)] Loss: 8084.619141\n",
      "Train Epoch: 211 [98560/225000 (44%)] Loss: 9600.333984\n",
      "Train Epoch: 211 [102656/225000 (46%)] Loss: 8185.218750\n",
      "Train Epoch: 211 [106752/225000 (47%)] Loss: 13652.603516\n",
      "Train Epoch: 211 [110848/225000 (49%)] Loss: 8206.150391\n",
      "Train Epoch: 211 [114944/225000 (51%)] Loss: 8116.431641\n",
      "Train Epoch: 211 [119040/225000 (53%)] Loss: 12272.216797\n",
      "Train Epoch: 211 [123136/225000 (55%)] Loss: 19146.875000\n",
      "Train Epoch: 211 [127232/225000 (57%)] Loss: 9440.394531\n",
      "Train Epoch: 211 [131328/225000 (58%)] Loss: 8025.792969\n",
      "Train Epoch: 211 [135424/225000 (60%)] Loss: 8131.910156\n",
      "Train Epoch: 211 [139520/225000 (62%)] Loss: 10587.296875\n",
      "Train Epoch: 211 [143616/225000 (64%)] Loss: 8239.826172\n",
      "Train Epoch: 211 [147712/225000 (66%)] Loss: 8176.724609\n",
      "Train Epoch: 211 [151808/225000 (67%)] Loss: 8002.818359\n",
      "Train Epoch: 211 [155904/225000 (69%)] Loss: 9767.664062\n",
      "Train Epoch: 211 [160000/225000 (71%)] Loss: 12815.830078\n",
      "Train Epoch: 211 [164096/225000 (73%)] Loss: 8312.708984\n",
      "Train Epoch: 211 [168192/225000 (75%)] Loss: 10386.333984\n",
      "Train Epoch: 211 [172288/225000 (77%)] Loss: 8309.351562\n",
      "Train Epoch: 211 [176384/225000 (78%)] Loss: 8028.564453\n",
      "Train Epoch: 211 [180480/225000 (80%)] Loss: 8114.171875\n",
      "Train Epoch: 211 [184576/225000 (82%)] Loss: 8016.691406\n",
      "Train Epoch: 211 [188672/225000 (84%)] Loss: 9639.779297\n",
      "Train Epoch: 211 [192768/225000 (86%)] Loss: 8287.496094\n",
      "Train Epoch: 211 [196864/225000 (87%)] Loss: 8225.066406\n",
      "Train Epoch: 211 [200960/225000 (89%)] Loss: 9662.146484\n",
      "Train Epoch: 211 [205056/225000 (91%)] Loss: 8183.416016\n",
      "Train Epoch: 211 [209152/225000 (93%)] Loss: 8216.591797\n",
      "Train Epoch: 211 [213248/225000 (95%)] Loss: 12679.792969\n",
      "Train Epoch: 211 [217344/225000 (97%)] Loss: 8122.099609\n",
      "Train Epoch: 211 [221440/225000 (98%)] Loss: 9608.130859\n",
      "    epoch          : 211\n",
      "    loss           : 9600.612470225398\n",
      "    val_loss       : 9534.422208267815\n",
      "Train Epoch: 212 [256/225000 (0%)] Loss: 8073.677734\n",
      "Train Epoch: 212 [4352/225000 (2%)] Loss: 8286.804688\n",
      "Train Epoch: 212 [8448/225000 (4%)] Loss: 9423.818359\n",
      "Train Epoch: 212 [12544/225000 (6%)] Loss: 19900.238281\n",
      "Train Epoch: 212 [16640/225000 (7%)] Loss: 16363.515625\n",
      "Train Epoch: 212 [20736/225000 (9%)] Loss: 7949.945312\n",
      "Train Epoch: 212 [24832/225000 (11%)] Loss: 10349.880859\n",
      "Train Epoch: 212 [28928/225000 (13%)] Loss: 8222.232422\n",
      "Train Epoch: 212 [33024/225000 (15%)] Loss: 7966.076172\n",
      "Train Epoch: 212 [37120/225000 (16%)] Loss: 10840.417969\n",
      "Train Epoch: 212 [41216/225000 (18%)] Loss: 11178.375000\n",
      "Train Epoch: 212 [45312/225000 (20%)] Loss: 13748.396484\n",
      "Train Epoch: 212 [49408/225000 (22%)] Loss: 13456.519531\n",
      "Train Epoch: 212 [53504/225000 (24%)] Loss: 9862.800781\n",
      "Train Epoch: 212 [57600/225000 (26%)] Loss: 12844.324219\n",
      "Train Epoch: 212 [61696/225000 (27%)] Loss: 12899.792969\n",
      "Train Epoch: 212 [65792/225000 (29%)] Loss: 9700.595703\n",
      "Train Epoch: 212 [69888/225000 (31%)] Loss: 12340.857422\n",
      "Train Epoch: 212 [73984/225000 (33%)] Loss: 8050.353516\n",
      "Train Epoch: 212 [78080/225000 (35%)] Loss: 10740.451172\n",
      "Train Epoch: 212 [82176/225000 (37%)] Loss: 13955.660156\n",
      "Train Epoch: 212 [86272/225000 (38%)] Loss: 8162.384766\n",
      "Train Epoch: 212 [90368/225000 (40%)] Loss: 13548.931641\n",
      "Train Epoch: 212 [94464/225000 (42%)] Loss: 7962.777344\n",
      "Train Epoch: 212 [98560/225000 (44%)] Loss: 13383.164062\n",
      "Train Epoch: 212 [102656/225000 (46%)] Loss: 7885.847656\n",
      "Train Epoch: 212 [106752/225000 (47%)] Loss: 8152.759766\n",
      "Train Epoch: 212 [110848/225000 (49%)] Loss: 9578.945312\n",
      "Train Epoch: 212 [114944/225000 (51%)] Loss: 8038.158203\n",
      "Train Epoch: 212 [119040/225000 (53%)] Loss: 9491.677734\n",
      "Train Epoch: 212 [123136/225000 (55%)] Loss: 12253.642578\n",
      "Train Epoch: 212 [127232/225000 (57%)] Loss: 10505.550781\n",
      "Train Epoch: 212 [131328/225000 (58%)] Loss: 9477.068359\n",
      "Train Epoch: 212 [135424/225000 (60%)] Loss: 8163.419922\n",
      "Train Epoch: 212 [139520/225000 (62%)] Loss: 10635.123047\n",
      "Train Epoch: 212 [143616/225000 (64%)] Loss: 9590.193359\n",
      "Train Epoch: 212 [147712/225000 (66%)] Loss: 15371.402344\n",
      "Train Epoch: 212 [151808/225000 (67%)] Loss: 9592.412109\n",
      "Train Epoch: 212 [155904/225000 (69%)] Loss: 12286.511719\n",
      "Train Epoch: 212 [160000/225000 (71%)] Loss: 9802.054688\n",
      "Train Epoch: 212 [164096/225000 (73%)] Loss: 8344.994141\n",
      "Train Epoch: 212 [168192/225000 (75%)] Loss: 13919.529297\n",
      "Train Epoch: 212 [172288/225000 (77%)] Loss: 7870.441406\n",
      "Train Epoch: 212 [176384/225000 (78%)] Loss: 9512.238281\n",
      "Train Epoch: 212 [180480/225000 (80%)] Loss: 7929.910156\n",
      "Train Epoch: 212 [184576/225000 (82%)] Loss: 8108.986328\n",
      "Train Epoch: 212 [188672/225000 (84%)] Loss: 7998.080078\n",
      "Train Epoch: 212 [192768/225000 (86%)] Loss: 9613.275391\n",
      "Train Epoch: 212 [196864/225000 (87%)] Loss: 13683.392578\n",
      "Train Epoch: 212 [200960/225000 (89%)] Loss: 7935.761719\n",
      "Train Epoch: 212 [205056/225000 (91%)] Loss: 8091.197266\n",
      "Train Epoch: 212 [209152/225000 (93%)] Loss: 7871.742188\n",
      "Train Epoch: 212 [213248/225000 (95%)] Loss: 8031.910156\n",
      "Train Epoch: 212 [217344/225000 (97%)] Loss: 9572.941406\n",
      "Train Epoch: 212 [221440/225000 (98%)] Loss: 12329.117188\n",
      "    epoch          : 212\n",
      "    loss           : 9601.607339661547\n",
      "    val_loss       : 9537.807000235636\n",
      "Train Epoch: 213 [256/225000 (0%)] Loss: 8216.548828\n",
      "Train Epoch: 213 [4352/225000 (2%)] Loss: 8197.253906\n",
      "Train Epoch: 213 [8448/225000 (4%)] Loss: 10606.564453\n",
      "Train Epoch: 213 [12544/225000 (6%)] Loss: 7949.767578\n",
      "Train Epoch: 213 [16640/225000 (7%)] Loss: 13906.640625\n",
      "Train Epoch: 213 [20736/225000 (9%)] Loss: 8117.484375\n",
      "Train Epoch: 213 [24832/225000 (11%)] Loss: 8042.078125\n",
      "Train Epoch: 213 [28928/225000 (13%)] Loss: 7981.349609\n",
      "Train Epoch: 213 [33024/225000 (15%)] Loss: 9545.255859\n",
      "Train Epoch: 213 [37120/225000 (16%)] Loss: 7815.085938\n",
      "Train Epoch: 213 [41216/225000 (18%)] Loss: 8169.814453\n",
      "Train Epoch: 213 [45312/225000 (20%)] Loss: 14921.189453\n",
      "Train Epoch: 213 [49408/225000 (22%)] Loss: 9575.705078\n",
      "Train Epoch: 213 [53504/225000 (24%)] Loss: 9745.048828\n",
      "Train Epoch: 213 [57600/225000 (26%)] Loss: 8154.914062\n",
      "Train Epoch: 213 [61696/225000 (27%)] Loss: 9736.851562\n",
      "Train Epoch: 213 [65792/225000 (29%)] Loss: 7994.724609\n",
      "Train Epoch: 213 [69888/225000 (31%)] Loss: 9525.023438\n",
      "Train Epoch: 213 [73984/225000 (33%)] Loss: 13923.650391\n",
      "Train Epoch: 213 [78080/225000 (35%)] Loss: 9576.326172\n",
      "Train Epoch: 213 [82176/225000 (37%)] Loss: 8092.267578\n",
      "Train Epoch: 213 [86272/225000 (38%)] Loss: 9686.949219\n",
      "Train Epoch: 213 [90368/225000 (40%)] Loss: 7958.425781\n",
      "Train Epoch: 213 [94464/225000 (42%)] Loss: 10353.314453\n",
      "Train Epoch: 213 [98560/225000 (44%)] Loss: 9834.962891\n",
      "Train Epoch: 213 [102656/225000 (46%)] Loss: 9572.744141\n",
      "Train Epoch: 213 [106752/225000 (47%)] Loss: 8059.625000\n",
      "Train Epoch: 213 [110848/225000 (49%)] Loss: 11159.808594\n",
      "Train Epoch: 213 [114944/225000 (51%)] Loss: 8113.078125\n",
      "Train Epoch: 213 [119040/225000 (53%)] Loss: 7999.035156\n",
      "Train Epoch: 213 [123136/225000 (55%)] Loss: 7995.052734\n",
      "Train Epoch: 213 [127232/225000 (57%)] Loss: 8018.611328\n",
      "Train Epoch: 213 [131328/225000 (58%)] Loss: 10647.621094\n",
      "Train Epoch: 213 [135424/225000 (60%)] Loss: 13650.736328\n",
      "Train Epoch: 213 [139520/225000 (62%)] Loss: 7888.457031\n",
      "Train Epoch: 213 [143616/225000 (64%)] Loss: 10615.716797\n",
      "Train Epoch: 213 [147712/225000 (66%)] Loss: 9725.863281\n",
      "Train Epoch: 213 [151808/225000 (67%)] Loss: 8070.513672\n",
      "Train Epoch: 213 [155904/225000 (69%)] Loss: 7964.753906\n",
      "Train Epoch: 213 [160000/225000 (71%)] Loss: 8138.943359\n",
      "Train Epoch: 213 [164096/225000 (73%)] Loss: 12255.765625\n",
      "Train Epoch: 213 [168192/225000 (75%)] Loss: 9857.890625\n",
      "Train Epoch: 213 [172288/225000 (77%)] Loss: 7891.800781\n",
      "Train Epoch: 213 [176384/225000 (78%)] Loss: 9507.906250\n",
      "Train Epoch: 213 [180480/225000 (80%)] Loss: 8201.343750\n",
      "Train Epoch: 213 [184576/225000 (82%)] Loss: 13904.826172\n",
      "Train Epoch: 213 [188672/225000 (84%)] Loss: 8028.666016\n",
      "Train Epoch: 213 [192768/225000 (86%)] Loss: 8055.457031\n",
      "Train Epoch: 213 [196864/225000 (87%)] Loss: 10404.310547\n",
      "Train Epoch: 213 [200960/225000 (89%)] Loss: 10315.798828\n",
      "Train Epoch: 213 [205056/225000 (91%)] Loss: 9499.453125\n",
      "Train Epoch: 213 [209152/225000 (93%)] Loss: 10513.199219\n",
      "Train Epoch: 213 [213248/225000 (95%)] Loss: 8161.621094\n",
      "Train Epoch: 213 [217344/225000 (97%)] Loss: 8186.224609\n",
      "Train Epoch: 213 [221440/225000 (98%)] Loss: 12078.664062\n",
      "    epoch          : 213\n",
      "    loss           : 9465.698798794796\n",
      "    val_loss       : 9410.982284446152\n",
      "Train Epoch: 214 [256/225000 (0%)] Loss: 8029.783203\n",
      "Train Epoch: 214 [4352/225000 (2%)] Loss: 9468.556641\n",
      "Train Epoch: 214 [8448/225000 (4%)] Loss: 16210.292969\n",
      "Train Epoch: 214 [12544/225000 (6%)] Loss: 13716.007812\n",
      "Train Epoch: 214 [16640/225000 (7%)] Loss: 7945.742188\n",
      "Train Epoch: 214 [20736/225000 (9%)] Loss: 8045.248047\n",
      "Train Epoch: 214 [24832/225000 (11%)] Loss: 7962.511719\n",
      "Train Epoch: 214 [28928/225000 (13%)] Loss: 9649.599609\n",
      "Train Epoch: 214 [33024/225000 (15%)] Loss: 10517.277344\n",
      "Train Epoch: 214 [37120/225000 (16%)] Loss: 8054.187500\n",
      "Train Epoch: 214 [41216/225000 (18%)] Loss: 8129.582031\n",
      "Train Epoch: 214 [45312/225000 (20%)] Loss: 9554.130859\n",
      "Train Epoch: 214 [49408/225000 (22%)] Loss: 12345.853516\n",
      "Train Epoch: 214 [53504/225000 (24%)] Loss: 8049.210938\n",
      "Train Epoch: 214 [57600/225000 (26%)] Loss: 8058.619141\n",
      "Train Epoch: 214 [61696/225000 (27%)] Loss: 9673.466797\n",
      "Train Epoch: 214 [65792/225000 (29%)] Loss: 10377.416016\n",
      "Train Epoch: 214 [69888/225000 (31%)] Loss: 7959.718750\n",
      "Train Epoch: 214 [73984/225000 (33%)] Loss: 9597.052734\n",
      "Train Epoch: 214 [78080/225000 (35%)] Loss: 8056.343750\n",
      "Train Epoch: 214 [82176/225000 (37%)] Loss: 10436.550781\n",
      "Train Epoch: 214 [86272/225000 (38%)] Loss: 7961.880859\n",
      "Train Epoch: 214 [90368/225000 (40%)] Loss: 10630.603516\n",
      "Train Epoch: 214 [94464/225000 (42%)] Loss: 8159.363281\n",
      "Train Epoch: 214 [98560/225000 (44%)] Loss: 9726.054688\n",
      "Train Epoch: 214 [102656/225000 (46%)] Loss: 8217.705078\n",
      "Train Epoch: 214 [106752/225000 (47%)] Loss: 9536.734375\n",
      "Train Epoch: 214 [110848/225000 (49%)] Loss: 11197.921875\n",
      "Train Epoch: 214 [114944/225000 (51%)] Loss: 7987.591797\n",
      "Train Epoch: 214 [119040/225000 (53%)] Loss: 13775.345703\n",
      "Train Epoch: 214 [123136/225000 (55%)] Loss: 8105.097656\n",
      "Train Epoch: 214 [127232/225000 (57%)] Loss: 8463.576172\n",
      "Train Epoch: 214 [131328/225000 (58%)] Loss: 10449.318359\n",
      "Train Epoch: 214 [135424/225000 (60%)] Loss: 8052.523438\n",
      "Train Epoch: 214 [139520/225000 (62%)] Loss: 7904.701172\n",
      "Train Epoch: 214 [143616/225000 (64%)] Loss: 8175.814453\n",
      "Train Epoch: 214 [147712/225000 (66%)] Loss: 8058.179688\n",
      "Train Epoch: 214 [151808/225000 (67%)] Loss: 8057.921875\n",
      "Train Epoch: 214 [155904/225000 (69%)] Loss: 13864.003906\n",
      "Train Epoch: 214 [160000/225000 (71%)] Loss: 12351.470703\n",
      "Train Epoch: 214 [164096/225000 (73%)] Loss: 8029.091797\n",
      "Train Epoch: 214 [168192/225000 (75%)] Loss: 15178.453125\n",
      "Train Epoch: 214 [172288/225000 (77%)] Loss: 8053.455078\n",
      "Train Epoch: 214 [176384/225000 (78%)] Loss: 9560.910156\n",
      "Train Epoch: 214 [180480/225000 (80%)] Loss: 10389.234375\n",
      "Train Epoch: 214 [184576/225000 (82%)] Loss: 8224.820312\n",
      "Train Epoch: 214 [188672/225000 (84%)] Loss: 10525.421875\n",
      "Train Epoch: 214 [192768/225000 (86%)] Loss: 9451.048828\n",
      "Train Epoch: 214 [196864/225000 (87%)] Loss: 8063.470703\n",
      "Train Epoch: 214 [200960/225000 (89%)] Loss: 10359.236328\n",
      "Train Epoch: 214 [205056/225000 (91%)] Loss: 12581.886719\n",
      "Train Epoch: 214 [209152/225000 (93%)] Loss: 8050.767578\n",
      "Train Epoch: 214 [213248/225000 (95%)] Loss: 10524.054688\n",
      "Train Epoch: 214 [217344/225000 (97%)] Loss: 7960.402344\n",
      "Train Epoch: 214 [221440/225000 (98%)] Loss: 12067.572266\n",
      "    epoch          : 214\n",
      "    loss           : 9454.255454973692\n",
      "    val_loss       : 9675.677416819699\n",
      "Train Epoch: 215 [256/225000 (0%)] Loss: 8147.593750\n",
      "Train Epoch: 215 [4352/225000 (2%)] Loss: 9568.333984\n",
      "Train Epoch: 215 [8448/225000 (4%)] Loss: 10369.892578\n",
      "Train Epoch: 215 [12544/225000 (6%)] Loss: 7970.357422\n",
      "Train Epoch: 215 [16640/225000 (7%)] Loss: 9674.173828\n",
      "Train Epoch: 215 [20736/225000 (9%)] Loss: 8216.291016\n",
      "Train Epoch: 215 [24832/225000 (11%)] Loss: 9633.615234\n",
      "Train Epoch: 215 [28928/225000 (13%)] Loss: 12336.199219\n",
      "Train Epoch: 215 [33024/225000 (15%)] Loss: 8244.535156\n",
      "Train Epoch: 215 [37120/225000 (16%)] Loss: 7861.402344\n",
      "Train Epoch: 215 [41216/225000 (18%)] Loss: 12459.937500\n",
      "Train Epoch: 215 [45312/225000 (20%)] Loss: 9836.125000\n",
      "Train Epoch: 215 [49408/225000 (22%)] Loss: 8119.183594\n",
      "Train Epoch: 215 [53504/225000 (24%)] Loss: 7953.037109\n",
      "Train Epoch: 215 [57600/225000 (26%)] Loss: 8172.845703\n",
      "Train Epoch: 215 [61696/225000 (27%)] Loss: 14343.230469\n",
      "Train Epoch: 215 [65792/225000 (29%)] Loss: 13097.914062\n",
      "Train Epoch: 215 [69888/225000 (31%)] Loss: 8009.349609\n",
      "Train Epoch: 215 [73984/225000 (33%)] Loss: 7965.425781\n",
      "Train Epoch: 215 [78080/225000 (35%)] Loss: 9694.623047\n",
      "Train Epoch: 215 [82176/225000 (37%)] Loss: 8012.117188\n",
      "Train Epoch: 215 [86272/225000 (38%)] Loss: 8192.972656\n",
      "Train Epoch: 215 [90368/225000 (40%)] Loss: 8026.824219\n",
      "Train Epoch: 215 [94464/225000 (42%)] Loss: 7918.789062\n",
      "Train Epoch: 215 [98560/225000 (44%)] Loss: 7946.921875\n",
      "Train Epoch: 215 [102656/225000 (46%)] Loss: 8097.839844\n",
      "Train Epoch: 215 [106752/225000 (47%)] Loss: 8209.986328\n",
      "Train Epoch: 215 [110848/225000 (49%)] Loss: 8280.246094\n",
      "Train Epoch: 215 [114944/225000 (51%)] Loss: 8023.189453\n",
      "Train Epoch: 215 [119040/225000 (53%)] Loss: 12809.429688\n",
      "Train Epoch: 215 [123136/225000 (55%)] Loss: 8001.447266\n",
      "Train Epoch: 215 [127232/225000 (57%)] Loss: 8076.119141\n",
      "Train Epoch: 215 [131328/225000 (58%)] Loss: 9696.548828\n",
      "Train Epoch: 215 [135424/225000 (60%)] Loss: 8173.201172\n",
      "Train Epoch: 215 [139520/225000 (62%)] Loss: 9662.667969\n",
      "Train Epoch: 215 [143616/225000 (64%)] Loss: 16827.486328\n",
      "Train Epoch: 215 [147712/225000 (66%)] Loss: 10533.623047\n",
      "Train Epoch: 215 [151808/225000 (67%)] Loss: 8025.703125\n",
      "Train Epoch: 215 [155904/225000 (69%)] Loss: 10606.919922\n",
      "Train Epoch: 215 [160000/225000 (71%)] Loss: 8274.273438\n",
      "Train Epoch: 215 [164096/225000 (73%)] Loss: 10657.458984\n",
      "Train Epoch: 215 [168192/225000 (75%)] Loss: 11959.837891\n",
      "Train Epoch: 215 [172288/225000 (77%)] Loss: 9632.839844\n",
      "Train Epoch: 215 [176384/225000 (78%)] Loss: 7953.218750\n",
      "Train Epoch: 215 [180480/225000 (80%)] Loss: 8202.554688\n",
      "Train Epoch: 215 [184576/225000 (82%)] Loss: 7868.605469\n",
      "Train Epoch: 215 [188672/225000 (84%)] Loss: 8157.806641\n",
      "Train Epoch: 215 [192768/225000 (86%)] Loss: 21538.375000\n",
      "Train Epoch: 215 [196864/225000 (87%)] Loss: 9699.125000\n",
      "Train Epoch: 215 [200960/225000 (89%)] Loss: 8130.095703\n",
      "Train Epoch: 215 [205056/225000 (91%)] Loss: 8097.275391\n",
      "Train Epoch: 215 [209152/225000 (93%)] Loss: 15565.017578\n",
      "Train Epoch: 215 [213248/225000 (95%)] Loss: 10390.947266\n",
      "Train Epoch: 215 [217344/225000 (97%)] Loss: 10431.244141\n",
      "Train Epoch: 215 [221440/225000 (98%)] Loss: 19686.421875\n",
      "    epoch          : 215\n",
      "    loss           : 9467.796386163254\n",
      "    val_loss       : 9305.115257632975\n",
      "Train Epoch: 216 [256/225000 (0%)] Loss: 8081.410156\n",
      "Train Epoch: 216 [4352/225000 (2%)] Loss: 15661.949219\n",
      "Train Epoch: 216 [8448/225000 (4%)] Loss: 9579.814453\n",
      "Train Epoch: 216 [12544/225000 (6%)] Loss: 7949.048828\n",
      "Train Epoch: 216 [16640/225000 (7%)] Loss: 9553.222656\n",
      "Train Epoch: 216 [20736/225000 (9%)] Loss: 12831.041016\n",
      "Train Epoch: 216 [24832/225000 (11%)] Loss: 8189.406250\n",
      "Train Epoch: 216 [28928/225000 (13%)] Loss: 10643.326172\n",
      "Train Epoch: 216 [33024/225000 (15%)] Loss: 7926.029297\n",
      "Train Epoch: 216 [37120/225000 (16%)] Loss: 10620.710938\n",
      "Train Epoch: 216 [41216/225000 (18%)] Loss: 16116.148438\n",
      "Train Epoch: 216 [45312/225000 (20%)] Loss: 13210.140625\n",
      "Train Epoch: 216 [49408/225000 (22%)] Loss: 10610.203125\n",
      "Train Epoch: 216 [53504/225000 (24%)] Loss: 10394.523438\n",
      "Train Epoch: 216 [57600/225000 (26%)] Loss: 12441.804688\n",
      "Train Epoch: 216 [61696/225000 (27%)] Loss: 8080.035156\n",
      "Train Epoch: 216 [65792/225000 (29%)] Loss: 7992.388672\n",
      "Train Epoch: 216 [69888/225000 (31%)] Loss: 8179.496094\n",
      "Train Epoch: 216 [73984/225000 (33%)] Loss: 8039.205078\n",
      "Train Epoch: 216 [78080/225000 (35%)] Loss: 9720.542969\n",
      "Train Epoch: 216 [82176/225000 (37%)] Loss: 14019.669922\n",
      "Train Epoch: 216 [86272/225000 (38%)] Loss: 10446.800781\n",
      "Train Epoch: 216 [90368/225000 (40%)] Loss: 13042.917969\n",
      "Train Epoch: 216 [94464/225000 (42%)] Loss: 8105.195312\n",
      "Train Epoch: 216 [98560/225000 (44%)] Loss: 8394.470703\n",
      "Train Epoch: 216 [102656/225000 (46%)] Loss: 8107.886719\n",
      "Train Epoch: 216 [106752/225000 (47%)] Loss: 12953.226562\n",
      "Train Epoch: 216 [110848/225000 (49%)] Loss: 7876.029297\n",
      "Train Epoch: 216 [114944/225000 (51%)] Loss: 13761.593750\n",
      "Train Epoch: 216 [119040/225000 (53%)] Loss: 10536.537109\n",
      "Train Epoch: 216 [123136/225000 (55%)] Loss: 12148.808594\n",
      "Train Epoch: 216 [127232/225000 (57%)] Loss: 7921.505859\n",
      "Train Epoch: 216 [131328/225000 (58%)] Loss: 8028.500000\n",
      "Train Epoch: 216 [135424/225000 (60%)] Loss: 13909.548828\n",
      "Train Epoch: 216 [139520/225000 (62%)] Loss: 9544.255859\n",
      "Train Epoch: 216 [143616/225000 (64%)] Loss: 9718.794922\n",
      "Train Epoch: 216 [147712/225000 (66%)] Loss: 13749.597656\n",
      "Train Epoch: 216 [151808/225000 (67%)] Loss: 10582.419922\n",
      "Train Epoch: 216 [155904/225000 (69%)] Loss: 12462.091797\n",
      "Train Epoch: 216 [160000/225000 (71%)] Loss: 8107.945312\n",
      "Train Epoch: 216 [164096/225000 (73%)] Loss: 9361.140625\n",
      "Train Epoch: 216 [168192/225000 (75%)] Loss: 7848.972656\n",
      "Train Epoch: 216 [172288/225000 (77%)] Loss: 10831.021484\n",
      "Train Epoch: 216 [176384/225000 (78%)] Loss: 8156.865234\n",
      "Train Epoch: 216 [180480/225000 (80%)] Loss: 10478.705078\n",
      "Train Epoch: 216 [184576/225000 (82%)] Loss: 9605.517578\n",
      "Train Epoch: 216 [188672/225000 (84%)] Loss: 13279.054688\n",
      "Train Epoch: 216 [192768/225000 (86%)] Loss: 12076.072266\n",
      "Train Epoch: 216 [196864/225000 (87%)] Loss: 10552.818359\n",
      "Train Epoch: 216 [200960/225000 (89%)] Loss: 9598.574219\n",
      "Train Epoch: 216 [205056/225000 (91%)] Loss: 7898.351562\n",
      "Train Epoch: 216 [209152/225000 (93%)] Loss: 10442.759766\n",
      "Train Epoch: 216 [213248/225000 (95%)] Loss: 8057.121094\n",
      "Train Epoch: 216 [217344/225000 (97%)] Loss: 7968.732422\n",
      "Train Epoch: 216 [221440/225000 (98%)] Loss: 8150.437500\n",
      "    epoch          : 216\n",
      "    loss           : 9621.01174430283\n",
      "    val_loss       : 9439.595211834323\n",
      "Train Epoch: 217 [256/225000 (0%)] Loss: 7974.136719\n",
      "Train Epoch: 217 [4352/225000 (2%)] Loss: 8012.644531\n",
      "Train Epoch: 217 [8448/225000 (4%)] Loss: 8140.652344\n",
      "Train Epoch: 217 [12544/225000 (6%)] Loss: 8132.951172\n",
      "Train Epoch: 217 [16640/225000 (7%)] Loss: 10747.564453\n",
      "Train Epoch: 217 [20736/225000 (9%)] Loss: 8275.369141\n",
      "Train Epoch: 217 [24832/225000 (11%)] Loss: 8087.121094\n",
      "Train Epoch: 217 [28928/225000 (13%)] Loss: 7972.341797\n",
      "Train Epoch: 217 [33024/225000 (15%)] Loss: 10394.431641\n",
      "Train Epoch: 217 [37120/225000 (16%)] Loss: 10560.103516\n",
      "Train Epoch: 217 [41216/225000 (18%)] Loss: 10547.068359\n",
      "Train Epoch: 217 [45312/225000 (20%)] Loss: 12720.945312\n",
      "Train Epoch: 217 [49408/225000 (22%)] Loss: 8182.263672\n",
      "Train Epoch: 217 [53504/225000 (24%)] Loss: 7984.503906\n",
      "Train Epoch: 217 [57600/225000 (26%)] Loss: 9672.507812\n",
      "Train Epoch: 217 [61696/225000 (27%)] Loss: 7973.769531\n",
      "Train Epoch: 217 [65792/225000 (29%)] Loss: 8066.759766\n",
      "Train Epoch: 217 [69888/225000 (31%)] Loss: 8034.603516\n",
      "Train Epoch: 217 [73984/225000 (33%)] Loss: 10451.287109\n",
      "Train Epoch: 217 [78080/225000 (35%)] Loss: 12690.181641\n",
      "Train Epoch: 217 [82176/225000 (37%)] Loss: 9501.669922\n",
      "Train Epoch: 217 [86272/225000 (38%)] Loss: 7916.142578\n",
      "Train Epoch: 217 [90368/225000 (40%)] Loss: 8128.978516\n",
      "Train Epoch: 217 [94464/225000 (42%)] Loss: 12779.460938\n",
      "Train Epoch: 217 [98560/225000 (44%)] Loss: 12250.810547\n",
      "Train Epoch: 217 [102656/225000 (46%)] Loss: 8071.265625\n",
      "Train Epoch: 217 [106752/225000 (47%)] Loss: 10663.039062\n",
      "Train Epoch: 217 [110848/225000 (49%)] Loss: 8030.669922\n",
      "Train Epoch: 217 [114944/225000 (51%)] Loss: 9546.359375\n",
      "Train Epoch: 217 [119040/225000 (53%)] Loss: 15041.800781\n",
      "Train Epoch: 217 [123136/225000 (55%)] Loss: 8021.609375\n",
      "Train Epoch: 217 [127232/225000 (57%)] Loss: 13837.455078\n",
      "Train Epoch: 217 [131328/225000 (58%)] Loss: 10370.144531\n",
      "Train Epoch: 217 [135424/225000 (60%)] Loss: 7893.642578\n",
      "Train Epoch: 217 [139520/225000 (62%)] Loss: 8167.005859\n",
      "Train Epoch: 217 [143616/225000 (64%)] Loss: 10434.363281\n",
      "Train Epoch: 217 [147712/225000 (66%)] Loss: 7937.767578\n",
      "Train Epoch: 217 [151808/225000 (67%)] Loss: 16327.710938\n",
      "Train Epoch: 217 [155904/225000 (69%)] Loss: 8111.707031\n",
      "Train Epoch: 217 [160000/225000 (71%)] Loss: 8086.650391\n",
      "Train Epoch: 217 [164096/225000 (73%)] Loss: 8218.117188\n",
      "Train Epoch: 217 [168192/225000 (75%)] Loss: 15336.044922\n",
      "Train Epoch: 217 [172288/225000 (77%)] Loss: 7930.486328\n",
      "Train Epoch: 217 [176384/225000 (78%)] Loss: 9462.451172\n",
      "Train Epoch: 217 [180480/225000 (80%)] Loss: 10470.484375\n",
      "Train Epoch: 217 [184576/225000 (82%)] Loss: 15326.406250\n",
      "Train Epoch: 217 [188672/225000 (84%)] Loss: 7856.695312\n",
      "Train Epoch: 217 [192768/225000 (86%)] Loss: 8082.507812\n",
      "Train Epoch: 217 [196864/225000 (87%)] Loss: 7866.541016\n",
      "Train Epoch: 217 [200960/225000 (89%)] Loss: 7973.410156\n",
      "Train Epoch: 217 [205056/225000 (91%)] Loss: 7986.484375\n",
      "Train Epoch: 217 [209152/225000 (93%)] Loss: 8167.576172\n",
      "Train Epoch: 217 [213248/225000 (95%)] Loss: 7986.132812\n",
      "Train Epoch: 217 [217344/225000 (97%)] Loss: 8098.593750\n",
      "Train Epoch: 217 [221440/225000 (98%)] Loss: 7915.847656\n",
      "    epoch          : 217\n",
      "    loss           : 9395.339197152303\n",
      "    val_loss       : 9428.518385328312\n",
      "Train Epoch: 218 [256/225000 (0%)] Loss: 8087.740234\n",
      "Train Epoch: 218 [4352/225000 (2%)] Loss: 9676.386719\n",
      "Train Epoch: 218 [8448/225000 (4%)] Loss: 8185.607422\n",
      "Train Epoch: 218 [12544/225000 (6%)] Loss: 9911.064453\n",
      "Train Epoch: 218 [16640/225000 (7%)] Loss: 12195.300781\n",
      "Train Epoch: 218 [20736/225000 (9%)] Loss: 12452.697266\n",
      "Train Epoch: 218 [24832/225000 (11%)] Loss: 10924.062500\n",
      "Train Epoch: 218 [28928/225000 (13%)] Loss: 7997.890625\n",
      "Train Epoch: 218 [33024/225000 (15%)] Loss: 10562.017578\n",
      "Train Epoch: 218 [37120/225000 (16%)] Loss: 9680.906250\n",
      "Train Epoch: 218 [41216/225000 (18%)] Loss: 8309.384766\n",
      "Train Epoch: 218 [45312/225000 (20%)] Loss: 8122.035156\n",
      "Train Epoch: 218 [49408/225000 (22%)] Loss: 8136.416016\n",
      "Train Epoch: 218 [53504/225000 (24%)] Loss: 7977.550781\n",
      "Train Epoch: 218 [57600/225000 (26%)] Loss: 14686.263672\n",
      "Train Epoch: 218 [61696/225000 (27%)] Loss: 12561.460938\n",
      "Train Epoch: 218 [65792/225000 (29%)] Loss: 10374.476562\n",
      "Train Epoch: 218 [69888/225000 (31%)] Loss: 12268.392578\n",
      "Train Epoch: 218 [73984/225000 (33%)] Loss: 9705.589844\n",
      "Train Epoch: 218 [78080/225000 (35%)] Loss: 7953.736328\n",
      "Train Epoch: 218 [82176/225000 (37%)] Loss: 8080.445312\n",
      "Train Epoch: 218 [86272/225000 (38%)] Loss: 12240.666016\n",
      "Train Epoch: 218 [90368/225000 (40%)] Loss: 10560.443359\n",
      "Train Epoch: 218 [94464/225000 (42%)] Loss: 8156.244141\n",
      "Train Epoch: 218 [98560/225000 (44%)] Loss: 8218.000000\n",
      "Train Epoch: 218 [102656/225000 (46%)] Loss: 8235.093750\n",
      "Train Epoch: 218 [106752/225000 (47%)] Loss: 9614.062500\n",
      "Train Epoch: 218 [110848/225000 (49%)] Loss: 8205.566406\n",
      "Train Epoch: 218 [114944/225000 (51%)] Loss: 9575.300781\n",
      "Train Epoch: 218 [119040/225000 (53%)] Loss: 8020.515625\n",
      "Train Epoch: 218 [123136/225000 (55%)] Loss: 9749.714844\n",
      "Train Epoch: 218 [127232/225000 (57%)] Loss: 9855.750000\n",
      "Train Epoch: 218 [131328/225000 (58%)] Loss: 7983.720703\n",
      "Train Epoch: 218 [135424/225000 (60%)] Loss: 9574.068359\n",
      "Train Epoch: 218 [139520/225000 (62%)] Loss: 8151.962891\n",
      "Train Epoch: 218 [143616/225000 (64%)] Loss: 9881.011719\n",
      "Train Epoch: 218 [147712/225000 (66%)] Loss: 8213.304688\n",
      "Train Epoch: 218 [151808/225000 (67%)] Loss: 8083.099609\n",
      "Train Epoch: 218 [155904/225000 (69%)] Loss: 7887.640625\n",
      "Train Epoch: 218 [160000/225000 (71%)] Loss: 8121.371094\n",
      "Train Epoch: 218 [164096/225000 (73%)] Loss: 8128.796875\n",
      "Train Epoch: 218 [168192/225000 (75%)] Loss: 9603.433594\n",
      "Train Epoch: 218 [172288/225000 (77%)] Loss: 13560.976562\n",
      "Train Epoch: 218 [176384/225000 (78%)] Loss: 8051.734375\n",
      "Train Epoch: 218 [180480/225000 (80%)] Loss: 18191.000000\n",
      "Train Epoch: 218 [184576/225000 (82%)] Loss: 10632.660156\n",
      "Train Epoch: 218 [188672/225000 (84%)] Loss: 7932.634766\n",
      "Train Epoch: 218 [192768/225000 (86%)] Loss: 8000.759766\n",
      "Train Epoch: 218 [196864/225000 (87%)] Loss: 11330.792969\n",
      "Train Epoch: 218 [200960/225000 (89%)] Loss: 9555.519531\n",
      "Train Epoch: 218 [205056/225000 (91%)] Loss: 8066.800781\n",
      "Train Epoch: 218 [209152/225000 (93%)] Loss: 8157.636719\n",
      "Train Epoch: 218 [213248/225000 (95%)] Loss: 9767.697266\n",
      "Train Epoch: 218 [217344/225000 (97%)] Loss: 8119.339844\n",
      "Train Epoch: 218 [221440/225000 (98%)] Loss: 13840.273438\n",
      "    epoch          : 218\n",
      "    loss           : 9527.947812233362\n",
      "    val_loss       : 9902.23528927443\n",
      "Train Epoch: 219 [256/225000 (0%)] Loss: 7888.710938\n",
      "Train Epoch: 219 [4352/225000 (2%)] Loss: 10597.451172\n",
      "Train Epoch: 219 [8448/225000 (4%)] Loss: 9531.119141\n",
      "Train Epoch: 219 [12544/225000 (6%)] Loss: 12191.167969\n",
      "Train Epoch: 219 [16640/225000 (7%)] Loss: 7897.859375\n",
      "Train Epoch: 219 [20736/225000 (9%)] Loss: 9550.535156\n",
      "Train Epoch: 219 [24832/225000 (11%)] Loss: 8179.529297\n",
      "Train Epoch: 219 [28928/225000 (13%)] Loss: 8004.367188\n",
      "Train Epoch: 219 [33024/225000 (15%)] Loss: 10600.332031\n",
      "Train Epoch: 219 [37120/225000 (16%)] Loss: 9565.779297\n",
      "Train Epoch: 219 [41216/225000 (18%)] Loss: 7914.558594\n",
      "Train Epoch: 219 [45312/225000 (20%)] Loss: 8052.722656\n",
      "Train Epoch: 219 [49408/225000 (22%)] Loss: 8117.275391\n",
      "Train Epoch: 219 [53504/225000 (24%)] Loss: 13897.564453\n",
      "Train Epoch: 219 [57600/225000 (26%)] Loss: 12432.125000\n",
      "Train Epoch: 219 [61696/225000 (27%)] Loss: 8110.406250\n",
      "Train Epoch: 219 [65792/225000 (29%)] Loss: 12936.478516\n",
      "Train Epoch: 219 [69888/225000 (31%)] Loss: 7818.005859\n",
      "Train Epoch: 219 [73984/225000 (33%)] Loss: 10487.000000\n",
      "Train Epoch: 219 [78080/225000 (35%)] Loss: 9545.458984\n",
      "Train Epoch: 219 [82176/225000 (37%)] Loss: 12293.775391\n",
      "Train Epoch: 219 [86272/225000 (38%)] Loss: 8346.828125\n",
      "Train Epoch: 219 [90368/225000 (40%)] Loss: 9614.580078\n",
      "Train Epoch: 219 [94464/225000 (42%)] Loss: 7980.490234\n",
      "Train Epoch: 219 [98560/225000 (44%)] Loss: 13693.763672\n",
      "Train Epoch: 219 [102656/225000 (46%)] Loss: 9654.908203\n",
      "Train Epoch: 219 [106752/225000 (47%)] Loss: 8098.046875\n",
      "Train Epoch: 219 [110848/225000 (49%)] Loss: 9524.615234\n",
      "Train Epoch: 219 [114944/225000 (51%)] Loss: 12056.912109\n",
      "Train Epoch: 219 [119040/225000 (53%)] Loss: 8177.253906\n",
      "Train Epoch: 219 [123136/225000 (55%)] Loss: 8209.281250\n",
      "Train Epoch: 219 [127232/225000 (57%)] Loss: 10445.162109\n",
      "Train Epoch: 219 [131328/225000 (58%)] Loss: 13673.882812\n",
      "Train Epoch: 219 [135424/225000 (60%)] Loss: 8201.304688\n",
      "Train Epoch: 219 [139520/225000 (62%)] Loss: 8071.607422\n",
      "Train Epoch: 219 [143616/225000 (64%)] Loss: 8097.361328\n",
      "Train Epoch: 219 [147712/225000 (66%)] Loss: 8187.298828\n",
      "Train Epoch: 219 [151808/225000 (67%)] Loss: 8052.263672\n",
      "Train Epoch: 219 [155904/225000 (69%)] Loss: 8287.441406\n",
      "Train Epoch: 219 [160000/225000 (71%)] Loss: 13947.488281\n",
      "Train Epoch: 219 [164096/225000 (73%)] Loss: 9749.753906\n",
      "Train Epoch: 219 [168192/225000 (75%)] Loss: 7956.478516\n",
      "Train Epoch: 219 [172288/225000 (77%)] Loss: 18999.000000\n",
      "Train Epoch: 219 [176384/225000 (78%)] Loss: 7987.031250\n",
      "Train Epoch: 219 [180480/225000 (80%)] Loss: 8075.804688\n",
      "Train Epoch: 219 [184576/225000 (82%)] Loss: 8173.208984\n",
      "Train Epoch: 219 [188672/225000 (84%)] Loss: 12146.136719\n",
      "Train Epoch: 219 [192768/225000 (86%)] Loss: 8311.978516\n",
      "Train Epoch: 219 [196864/225000 (87%)] Loss: 17938.056641\n",
      "Train Epoch: 219 [200960/225000 (89%)] Loss: 8229.316406\n",
      "Train Epoch: 219 [205056/225000 (91%)] Loss: 9793.720703\n",
      "Train Epoch: 219 [209152/225000 (93%)] Loss: 8300.185547\n",
      "Train Epoch: 219 [213248/225000 (95%)] Loss: 7871.867188\n",
      "Train Epoch: 219 [217344/225000 (97%)] Loss: 8137.298828\n",
      "Train Epoch: 219 [221440/225000 (98%)] Loss: 7952.412109\n",
      "    epoch          : 219\n",
      "    loss           : 9607.917445472483\n",
      "    val_loss       : 9232.167420544187\n",
      "Train Epoch: 220 [256/225000 (0%)] Loss: 8350.517578\n",
      "Train Epoch: 220 [4352/225000 (2%)] Loss: 8248.853516\n",
      "Train Epoch: 220 [8448/225000 (4%)] Loss: 9686.382812\n",
      "Train Epoch: 220 [12544/225000 (6%)] Loss: 8228.699219\n",
      "Train Epoch: 220 [16640/225000 (7%)] Loss: 13427.031250\n",
      "Train Epoch: 220 [20736/225000 (9%)] Loss: 7889.445312\n",
      "Train Epoch: 220 [24832/225000 (11%)] Loss: 8182.496094\n",
      "Train Epoch: 220 [28928/225000 (13%)] Loss: 12512.291016\n",
      "Train Epoch: 220 [33024/225000 (15%)] Loss: 8040.642578\n",
      "Train Epoch: 220 [37120/225000 (16%)] Loss: 8133.314453\n",
      "Train Epoch: 220 [41216/225000 (18%)] Loss: 8027.363281\n",
      "Train Epoch: 220 [45312/225000 (20%)] Loss: 9680.259766\n",
      "Train Epoch: 220 [49408/225000 (22%)] Loss: 7921.246094\n",
      "Train Epoch: 220 [53504/225000 (24%)] Loss: 10480.664062\n",
      "Train Epoch: 220 [57600/225000 (26%)] Loss: 8053.720703\n",
      "Train Epoch: 220 [61696/225000 (27%)] Loss: 8035.765625\n",
      "Train Epoch: 220 [65792/225000 (29%)] Loss: 8002.187500\n",
      "Train Epoch: 220 [69888/225000 (31%)] Loss: 16628.025391\n",
      "Train Epoch: 220 [73984/225000 (33%)] Loss: 8169.552734\n",
      "Train Epoch: 220 [78080/225000 (35%)] Loss: 9445.771484\n",
      "Train Epoch: 220 [82176/225000 (37%)] Loss: 8059.115234\n",
      "Train Epoch: 220 [86272/225000 (38%)] Loss: 8030.382812\n",
      "Train Epoch: 220 [90368/225000 (40%)] Loss: 8101.105469\n",
      "Train Epoch: 220 [94464/225000 (42%)] Loss: 9410.798828\n",
      "Train Epoch: 220 [98560/225000 (44%)] Loss: 8151.572266\n",
      "Train Epoch: 220 [102656/225000 (46%)] Loss: 7960.187500\n",
      "Train Epoch: 220 [106752/225000 (47%)] Loss: 9676.248047\n",
      "Train Epoch: 220 [110848/225000 (49%)] Loss: 8032.355469\n",
      "Train Epoch: 220 [114944/225000 (51%)] Loss: 8153.259766\n",
      "Train Epoch: 220 [119040/225000 (53%)] Loss: 8004.236328\n",
      "Train Epoch: 220 [123136/225000 (55%)] Loss: 8131.111328\n",
      "Train Epoch: 220 [127232/225000 (57%)] Loss: 8149.923828\n",
      "Train Epoch: 220 [131328/225000 (58%)] Loss: 7966.810547\n",
      "Train Epoch: 220 [135424/225000 (60%)] Loss: 7853.753906\n",
      "Train Epoch: 220 [139520/225000 (62%)] Loss: 13640.603516\n",
      "Train Epoch: 220 [143616/225000 (64%)] Loss: 13649.283203\n",
      "Train Epoch: 220 [147712/225000 (66%)] Loss: 12491.986328\n",
      "Train Epoch: 220 [151808/225000 (67%)] Loss: 8197.744141\n",
      "Train Epoch: 220 [155904/225000 (69%)] Loss: 8001.365234\n",
      "Train Epoch: 220 [160000/225000 (71%)] Loss: 8073.001953\n",
      "Train Epoch: 220 [164096/225000 (73%)] Loss: 7955.271484\n",
      "Train Epoch: 220 [168192/225000 (75%)] Loss: 8085.236328\n",
      "Train Epoch: 220 [172288/225000 (77%)] Loss: 8035.494141\n",
      "Train Epoch: 220 [176384/225000 (78%)] Loss: 7962.158203\n",
      "Train Epoch: 220 [180480/225000 (80%)] Loss: 7899.253906\n",
      "Train Epoch: 220 [184576/225000 (82%)] Loss: 7924.242188\n",
      "Train Epoch: 220 [188672/225000 (84%)] Loss: 16242.076172\n",
      "Train Epoch: 220 [192768/225000 (86%)] Loss: 8234.369141\n",
      "Train Epoch: 220 [196864/225000 (87%)] Loss: 10569.978516\n",
      "Train Epoch: 220 [200960/225000 (89%)] Loss: 9623.167969\n",
      "Train Epoch: 220 [205056/225000 (91%)] Loss: 8151.421875\n",
      "Train Epoch: 220 [209152/225000 (93%)] Loss: 8165.318359\n",
      "Train Epoch: 220 [213248/225000 (95%)] Loss: 8096.812500\n",
      "Train Epoch: 220 [217344/225000 (97%)] Loss: 10497.703125\n",
      "Train Epoch: 220 [221440/225000 (98%)] Loss: 7976.853516\n",
      "    epoch          : 220\n",
      "    loss           : 9524.110857064135\n",
      "    val_loss       : 9775.106016937567\n",
      "Train Epoch: 221 [256/225000 (0%)] Loss: 8182.158203\n",
      "Train Epoch: 221 [4352/225000 (2%)] Loss: 9681.353516\n",
      "Train Epoch: 221 [8448/225000 (4%)] Loss: 11860.302734\n",
      "Train Epoch: 221 [12544/225000 (6%)] Loss: 7908.296875\n",
      "Train Epoch: 221 [16640/225000 (7%)] Loss: 8031.751953\n",
      "Train Epoch: 221 [20736/225000 (9%)] Loss: 9806.177734\n",
      "Train Epoch: 221 [24832/225000 (11%)] Loss: 9691.886719\n",
      "Train Epoch: 221 [28928/225000 (13%)] Loss: 14168.478516\n",
      "Train Epoch: 221 [33024/225000 (15%)] Loss: 9603.521484\n",
      "Train Epoch: 221 [37120/225000 (16%)] Loss: 10492.740234\n",
      "Train Epoch: 221 [41216/225000 (18%)] Loss: 9513.070312\n",
      "Train Epoch: 221 [45312/225000 (20%)] Loss: 8134.845703\n",
      "Train Epoch: 221 [49408/225000 (22%)] Loss: 13791.699219\n",
      "Train Epoch: 221 [53504/225000 (24%)] Loss: 13766.406250\n",
      "Train Epoch: 221 [57600/225000 (26%)] Loss: 9419.199219\n",
      "Train Epoch: 221 [61696/225000 (27%)] Loss: 8152.601562\n",
      "Train Epoch: 221 [65792/225000 (29%)] Loss: 8324.117188\n",
      "Train Epoch: 221 [69888/225000 (31%)] Loss: 9626.082031\n",
      "Train Epoch: 221 [73984/225000 (33%)] Loss: 7960.943359\n",
      "Train Epoch: 221 [78080/225000 (35%)] Loss: 8196.191406\n",
      "Train Epoch: 221 [82176/225000 (37%)] Loss: 9869.335938\n",
      "Train Epoch: 221 [86272/225000 (38%)] Loss: 9557.753906\n",
      "Train Epoch: 221 [90368/225000 (40%)] Loss: 15338.158203\n",
      "Train Epoch: 221 [94464/225000 (42%)] Loss: 10352.519531\n",
      "Train Epoch: 221 [98560/225000 (44%)] Loss: 13769.878906\n",
      "Train Epoch: 221 [102656/225000 (46%)] Loss: 8082.908203\n",
      "Train Epoch: 221 [106752/225000 (47%)] Loss: 8280.710938\n",
      "Train Epoch: 221 [110848/225000 (49%)] Loss: 18576.794922\n",
      "Train Epoch: 221 [114944/225000 (51%)] Loss: 12033.308594\n",
      "Train Epoch: 221 [119040/225000 (53%)] Loss: 16272.595703\n",
      "Train Epoch: 221 [123136/225000 (55%)] Loss: 9558.662109\n",
      "Train Epoch: 221 [127232/225000 (57%)] Loss: 8106.839844\n",
      "Train Epoch: 221 [131328/225000 (58%)] Loss: 7933.394531\n",
      "Train Epoch: 221 [135424/225000 (60%)] Loss: 7971.966797\n",
      "Train Epoch: 221 [139520/225000 (62%)] Loss: 8029.193359\n",
      "Train Epoch: 221 [143616/225000 (64%)] Loss: 7968.318359\n",
      "Train Epoch: 221 [147712/225000 (66%)] Loss: 8069.986328\n",
      "Train Epoch: 221 [151808/225000 (67%)] Loss: 8199.859375\n",
      "Train Epoch: 221 [155904/225000 (69%)] Loss: 7968.570312\n",
      "Train Epoch: 221 [160000/225000 (71%)] Loss: 9629.794922\n",
      "Train Epoch: 221 [164096/225000 (73%)] Loss: 8287.693359\n",
      "Train Epoch: 221 [168192/225000 (75%)] Loss: 14771.630859\n",
      "Train Epoch: 221 [172288/225000 (77%)] Loss: 10519.201172\n",
      "Train Epoch: 221 [176384/225000 (78%)] Loss: 10599.806641\n",
      "Train Epoch: 221 [180480/225000 (80%)] Loss: 12797.298828\n",
      "Train Epoch: 221 [184576/225000 (82%)] Loss: 12274.496094\n",
      "Train Epoch: 221 [188672/225000 (84%)] Loss: 8084.062500\n",
      "Train Epoch: 221 [192768/225000 (86%)] Loss: 18510.082031\n",
      "Train Epoch: 221 [196864/225000 (87%)] Loss: 8029.337891\n",
      "Train Epoch: 221 [200960/225000 (89%)] Loss: 9649.552734\n",
      "Train Epoch: 221 [205056/225000 (91%)] Loss: 8186.392578\n",
      "Train Epoch: 221 [209152/225000 (93%)] Loss: 10495.824219\n",
      "Train Epoch: 221 [213248/225000 (95%)] Loss: 9708.478516\n",
      "Train Epoch: 221 [217344/225000 (97%)] Loss: 10401.441406\n",
      "Train Epoch: 221 [221440/225000 (98%)] Loss: 12644.513672\n",
      "    epoch          : 221\n",
      "    loss           : 9521.648965221488\n",
      "    val_loss       : 9425.371475643042\n",
      "Train Epoch: 222 [256/225000 (0%)] Loss: 8202.625000\n",
      "Train Epoch: 222 [4352/225000 (2%)] Loss: 9559.498047\n",
      "Train Epoch: 222 [8448/225000 (4%)] Loss: 7824.222656\n",
      "Train Epoch: 222 [12544/225000 (6%)] Loss: 9494.757812\n",
      "Train Epoch: 222 [16640/225000 (7%)] Loss: 7906.134766\n",
      "Train Epoch: 222 [20736/225000 (9%)] Loss: 11337.341797\n",
      "Train Epoch: 222 [24832/225000 (11%)] Loss: 10401.150391\n",
      "Train Epoch: 222 [28928/225000 (13%)] Loss: 7915.423828\n",
      "Train Epoch: 222 [33024/225000 (15%)] Loss: 8223.742188\n",
      "Train Epoch: 222 [37120/225000 (16%)] Loss: 10271.011719\n",
      "Train Epoch: 222 [41216/225000 (18%)] Loss: 8259.664062\n",
      "Train Epoch: 222 [45312/225000 (20%)] Loss: 8220.691406\n",
      "Train Epoch: 222 [49408/225000 (22%)] Loss: 13697.535156\n",
      "Train Epoch: 222 [53504/225000 (24%)] Loss: 8066.230469\n",
      "Train Epoch: 222 [57600/225000 (26%)] Loss: 8301.636719\n",
      "Train Epoch: 222 [61696/225000 (27%)] Loss: 8145.921875\n",
      "Train Epoch: 222 [65792/225000 (29%)] Loss: 7956.226562\n",
      "Train Epoch: 222 [69888/225000 (31%)] Loss: 8024.414062\n",
      "Train Epoch: 222 [73984/225000 (33%)] Loss: 19982.035156\n",
      "Train Epoch: 222 [78080/225000 (35%)] Loss: 8015.371094\n",
      "Train Epoch: 222 [82176/225000 (37%)] Loss: 8002.962891\n",
      "Train Epoch: 222 [86272/225000 (38%)] Loss: 8030.015625\n",
      "Train Epoch: 222 [90368/225000 (40%)] Loss: 9604.117188\n",
      "Train Epoch: 222 [94464/225000 (42%)] Loss: 7893.640625\n",
      "Train Epoch: 222 [98560/225000 (44%)] Loss: 8165.220703\n",
      "Train Epoch: 222 [102656/225000 (46%)] Loss: 8308.281250\n",
      "Train Epoch: 222 [106752/225000 (47%)] Loss: 8180.976562\n",
      "Train Epoch: 222 [110848/225000 (49%)] Loss: 8386.304688\n",
      "Train Epoch: 222 [114944/225000 (51%)] Loss: 8066.730469\n",
      "Train Epoch: 222 [119040/225000 (53%)] Loss: 12629.275391\n",
      "Train Epoch: 222 [123136/225000 (55%)] Loss: 7945.837891\n",
      "Train Epoch: 222 [127232/225000 (57%)] Loss: 7906.007812\n",
      "Train Epoch: 222 [131328/225000 (58%)] Loss: 13559.726562\n",
      "Train Epoch: 222 [135424/225000 (60%)] Loss: 8088.990234\n",
      "Train Epoch: 222 [139520/225000 (62%)] Loss: 14051.412109\n",
      "Train Epoch: 222 [143616/225000 (64%)] Loss: 8051.078125\n",
      "Train Epoch: 222 [147712/225000 (66%)] Loss: 11377.890625\n",
      "Train Epoch: 222 [151808/225000 (67%)] Loss: 7898.984375\n",
      "Train Epoch: 222 [155904/225000 (69%)] Loss: 8071.414062\n",
      "Train Epoch: 222 [160000/225000 (71%)] Loss: 8113.857422\n",
      "Train Epoch: 222 [164096/225000 (73%)] Loss: 8085.476562\n",
      "Train Epoch: 222 [168192/225000 (75%)] Loss: 8113.222656\n",
      "Train Epoch: 222 [172288/225000 (77%)] Loss: 12450.242188\n",
      "Train Epoch: 222 [176384/225000 (78%)] Loss: 13892.525391\n",
      "Train Epoch: 222 [180480/225000 (80%)] Loss: 12299.685547\n",
      "Train Epoch: 222 [184576/225000 (82%)] Loss: 8021.107422\n",
      "Train Epoch: 222 [188672/225000 (84%)] Loss: 9760.363281\n",
      "Train Epoch: 222 [192768/225000 (86%)] Loss: 9661.621094\n",
      "Train Epoch: 222 [196864/225000 (87%)] Loss: 8075.068359\n",
      "Train Epoch: 222 [200960/225000 (89%)] Loss: 9875.634766\n",
      "Train Epoch: 222 [205056/225000 (91%)] Loss: 8184.144531\n",
      "Train Epoch: 222 [209152/225000 (93%)] Loss: 8189.378906\n",
      "Train Epoch: 222 [213248/225000 (95%)] Loss: 7938.453125\n",
      "Train Epoch: 222 [217344/225000 (97%)] Loss: 10576.658203\n",
      "Train Epoch: 222 [221440/225000 (98%)] Loss: 8242.048828\n",
      "    epoch          : 222\n",
      "    loss           : 9454.16643335822\n",
      "    val_loss       : 9699.130714158622\n",
      "Train Epoch: 223 [256/225000 (0%)] Loss: 11260.777344\n",
      "Train Epoch: 223 [4352/225000 (2%)] Loss: 8161.904297\n",
      "Train Epoch: 223 [8448/225000 (4%)] Loss: 10612.548828\n",
      "Train Epoch: 223 [12544/225000 (6%)] Loss: 14160.447266\n",
      "Train Epoch: 223 [16640/225000 (7%)] Loss: 8148.167969\n",
      "Train Epoch: 223 [20736/225000 (9%)] Loss: 8098.939453\n",
      "Train Epoch: 223 [24832/225000 (11%)] Loss: 8185.820312\n",
      "Train Epoch: 223 [28928/225000 (13%)] Loss: 8219.626953\n",
      "Train Epoch: 223 [33024/225000 (15%)] Loss: 8150.613281\n",
      "Train Epoch: 223 [37120/225000 (16%)] Loss: 9560.685547\n",
      "Train Epoch: 223 [41216/225000 (18%)] Loss: 8126.570312\n",
      "Train Epoch: 223 [45312/225000 (20%)] Loss: 9543.503906\n",
      "Train Epoch: 223 [49408/225000 (22%)] Loss: 7922.556641\n",
      "Train Epoch: 223 [53504/225000 (24%)] Loss: 13903.800781\n",
      "Train Epoch: 223 [57600/225000 (26%)] Loss: 10435.230469\n",
      "Train Epoch: 223 [61696/225000 (27%)] Loss: 13538.906250\n",
      "Train Epoch: 223 [65792/225000 (29%)] Loss: 7884.080078\n",
      "Train Epoch: 223 [69888/225000 (31%)] Loss: 8066.267578\n",
      "Train Epoch: 223 [73984/225000 (33%)] Loss: 8074.517578\n",
      "Train Epoch: 223 [78080/225000 (35%)] Loss: 13814.029297\n",
      "Train Epoch: 223 [82176/225000 (37%)] Loss: 8129.324219\n",
      "Train Epoch: 223 [86272/225000 (38%)] Loss: 8052.804688\n",
      "Train Epoch: 223 [90368/225000 (40%)] Loss: 7985.259766\n",
      "Train Epoch: 223 [94464/225000 (42%)] Loss: 8191.996094\n",
      "Train Epoch: 223 [98560/225000 (44%)] Loss: 8221.691406\n",
      "Train Epoch: 223 [102656/225000 (46%)] Loss: 8179.750000\n",
      "Train Epoch: 223 [106752/225000 (47%)] Loss: 8194.365234\n",
      "Train Epoch: 223 [110848/225000 (49%)] Loss: 8199.908203\n",
      "Train Epoch: 223 [114944/225000 (51%)] Loss: 10511.814453\n",
      "Train Epoch: 223 [119040/225000 (53%)] Loss: 8074.931641\n",
      "Train Epoch: 223 [123136/225000 (55%)] Loss: 14780.859375\n",
      "Train Epoch: 223 [127232/225000 (57%)] Loss: 9499.845703\n",
      "Train Epoch: 223 [131328/225000 (58%)] Loss: 10555.425781\n",
      "Train Epoch: 223 [135424/225000 (60%)] Loss: 7868.582031\n",
      "Train Epoch: 223 [139520/225000 (62%)] Loss: 13177.007812\n",
      "Train Epoch: 223 [143616/225000 (64%)] Loss: 12317.777344\n",
      "Train Epoch: 223 [147712/225000 (66%)] Loss: 7985.185547\n",
      "Train Epoch: 223 [151808/225000 (67%)] Loss: 8128.505859\n",
      "Train Epoch: 223 [155904/225000 (69%)] Loss: 12168.921875\n",
      "Train Epoch: 223 [160000/225000 (71%)] Loss: 11219.310547\n",
      "Train Epoch: 223 [164096/225000 (73%)] Loss: 8219.132812\n",
      "Train Epoch: 223 [168192/225000 (75%)] Loss: 9681.146484\n",
      "Train Epoch: 223 [172288/225000 (77%)] Loss: 8085.968750\n",
      "Train Epoch: 223 [176384/225000 (78%)] Loss: 8270.162109\n",
      "Train Epoch: 223 [180480/225000 (80%)] Loss: 10541.109375\n",
      "Train Epoch: 223 [184576/225000 (82%)] Loss: 13895.335938\n",
      "Train Epoch: 223 [188672/225000 (84%)] Loss: 13624.207031\n",
      "Train Epoch: 223 [192768/225000 (86%)] Loss: 9576.748047\n",
      "Train Epoch: 223 [196864/225000 (87%)] Loss: 7875.351562\n",
      "Train Epoch: 223 [200960/225000 (89%)] Loss: 9669.337891\n",
      "Train Epoch: 223 [205056/225000 (91%)] Loss: 7909.701172\n",
      "Train Epoch: 223 [209152/225000 (93%)] Loss: 10747.644531\n",
      "Train Epoch: 223 [213248/225000 (95%)] Loss: 7886.912109\n",
      "Train Epoch: 223 [217344/225000 (97%)] Loss: 7884.613281\n",
      "Train Epoch: 223 [221440/225000 (98%)] Loss: 8033.611328\n",
      "    epoch          : 223\n",
      "    loss           : 9534.588039498009\n",
      "    val_loss       : 9412.873191780915\n",
      "Train Epoch: 224 [256/225000 (0%)] Loss: 8036.660156\n",
      "Train Epoch: 224 [4352/225000 (2%)] Loss: 10591.966797\n",
      "Train Epoch: 224 [8448/225000 (4%)] Loss: 7975.681641\n",
      "Train Epoch: 224 [12544/225000 (6%)] Loss: 8177.261719\n",
      "Train Epoch: 224 [16640/225000 (7%)] Loss: 8088.833984\n",
      "Train Epoch: 224 [20736/225000 (9%)] Loss: 8272.544922\n",
      "Train Epoch: 224 [24832/225000 (11%)] Loss: 10751.433594\n",
      "Train Epoch: 224 [28928/225000 (13%)] Loss: 8107.457031\n",
      "Train Epoch: 224 [33024/225000 (15%)] Loss: 15800.111328\n",
      "Train Epoch: 224 [37120/225000 (16%)] Loss: 8055.570312\n",
      "Train Epoch: 224 [41216/225000 (18%)] Loss: 7943.076172\n",
      "Train Epoch: 224 [45312/225000 (20%)] Loss: 9477.515625\n",
      "Train Epoch: 224 [49408/225000 (22%)] Loss: 7767.296875\n",
      "Train Epoch: 224 [53504/225000 (24%)] Loss: 8074.162109\n",
      "Train Epoch: 224 [57600/225000 (26%)] Loss: 10372.859375\n",
      "Train Epoch: 224 [61696/225000 (27%)] Loss: 20755.642578\n",
      "Train Epoch: 224 [65792/225000 (29%)] Loss: 8194.160156\n",
      "Train Epoch: 224 [69888/225000 (31%)] Loss: 8067.953125\n",
      "Train Epoch: 224 [73984/225000 (33%)] Loss: 7925.710938\n",
      "Train Epoch: 224 [78080/225000 (35%)] Loss: 9512.529297\n",
      "Train Epoch: 224 [82176/225000 (37%)] Loss: 9651.830078\n",
      "Train Epoch: 224 [86272/225000 (38%)] Loss: 8095.560547\n",
      "Train Epoch: 224 [90368/225000 (40%)] Loss: 7854.849609\n",
      "Train Epoch: 224 [94464/225000 (42%)] Loss: 9536.476562\n",
      "Train Epoch: 224 [98560/225000 (44%)] Loss: 8130.537109\n",
      "Train Epoch: 224 [102656/225000 (46%)] Loss: 8073.587891\n",
      "Train Epoch: 224 [106752/225000 (47%)] Loss: 12020.921875\n",
      "Train Epoch: 224 [110848/225000 (49%)] Loss: 8010.410156\n",
      "Train Epoch: 224 [114944/225000 (51%)] Loss: 13763.566406\n",
      "Train Epoch: 224 [119040/225000 (53%)] Loss: 8019.818359\n",
      "Train Epoch: 224 [123136/225000 (55%)] Loss: 14067.542969\n",
      "Train Epoch: 224 [127232/225000 (57%)] Loss: 9559.031250\n",
      "Train Epoch: 224 [131328/225000 (58%)] Loss: 7990.464844\n",
      "Train Epoch: 224 [135424/225000 (60%)] Loss: 9582.398438\n",
      "Train Epoch: 224 [139520/225000 (62%)] Loss: 8284.425781\n",
      "Train Epoch: 224 [143616/225000 (64%)] Loss: 7920.513672\n",
      "Train Epoch: 224 [147712/225000 (66%)] Loss: 13814.273438\n",
      "Train Epoch: 224 [151808/225000 (67%)] Loss: 8179.248047\n",
      "Train Epoch: 224 [155904/225000 (69%)] Loss: 7924.578125\n",
      "Train Epoch: 224 [160000/225000 (71%)] Loss: 9903.250000\n",
      "Train Epoch: 224 [164096/225000 (73%)] Loss: 7974.046875\n",
      "Train Epoch: 224 [168192/225000 (75%)] Loss: 8218.337891\n",
      "Train Epoch: 224 [172288/225000 (77%)] Loss: 9699.207031\n",
      "Train Epoch: 224 [176384/225000 (78%)] Loss: 9547.554688\n",
      "Train Epoch: 224 [180480/225000 (80%)] Loss: 14564.636719\n",
      "Train Epoch: 224 [184576/225000 (82%)] Loss: 12240.785156\n",
      "Train Epoch: 224 [188672/225000 (84%)] Loss: 10652.164062\n",
      "Train Epoch: 224 [192768/225000 (86%)] Loss: 8187.494141\n",
      "Train Epoch: 224 [196864/225000 (87%)] Loss: 13974.039062\n",
      "Train Epoch: 224 [200960/225000 (89%)] Loss: 9696.789062\n",
      "Train Epoch: 224 [205056/225000 (91%)] Loss: 9758.986328\n",
      "Train Epoch: 224 [209152/225000 (93%)] Loss: 13512.572266\n",
      "Train Epoch: 224 [213248/225000 (95%)] Loss: 8188.410156\n",
      "Train Epoch: 224 [217344/225000 (97%)] Loss: 10586.652344\n",
      "Train Epoch: 224 [221440/225000 (98%)] Loss: 9547.185547\n",
      "    epoch          : 224\n",
      "    loss           : 9680.860923723692\n",
      "    val_loss       : 9300.655414121491\n",
      "Train Epoch: 225 [256/225000 (0%)] Loss: 7869.671875\n",
      "Train Epoch: 225 [4352/225000 (2%)] Loss: 7877.373047\n",
      "Train Epoch: 225 [8448/225000 (4%)] Loss: 8151.572266\n",
      "Train Epoch: 225 [12544/225000 (6%)] Loss: 8037.667969\n",
      "Train Epoch: 225 [16640/225000 (7%)] Loss: 11057.130859\n",
      "Train Epoch: 225 [20736/225000 (9%)] Loss: 10554.914062\n",
      "Train Epoch: 225 [24832/225000 (11%)] Loss: 8084.068359\n",
      "Train Epoch: 225 [28928/225000 (13%)] Loss: 9662.273438\n",
      "Train Epoch: 225 [33024/225000 (15%)] Loss: 19565.695312\n",
      "Train Epoch: 225 [37120/225000 (16%)] Loss: 12262.996094\n",
      "Train Epoch: 225 [41216/225000 (18%)] Loss: 9471.242188\n",
      "Train Epoch: 225 [45312/225000 (20%)] Loss: 10340.896484\n",
      "Train Epoch: 225 [49408/225000 (22%)] Loss: 9529.179688\n",
      "Train Epoch: 225 [53504/225000 (24%)] Loss: 8001.472656\n",
      "Train Epoch: 225 [57600/225000 (26%)] Loss: 9672.515625\n",
      "Train Epoch: 225 [61696/225000 (27%)] Loss: 7862.904297\n",
      "Train Epoch: 225 [65792/225000 (29%)] Loss: 9437.183594\n",
      "Train Epoch: 225 [69888/225000 (31%)] Loss: 9702.537109\n",
      "Train Epoch: 225 [73984/225000 (33%)] Loss: 13626.166016\n",
      "Train Epoch: 225 [78080/225000 (35%)] Loss: 8190.560547\n",
      "Train Epoch: 225 [82176/225000 (37%)] Loss: 7962.310547\n",
      "Train Epoch: 225 [86272/225000 (38%)] Loss: 7967.988281\n",
      "Train Epoch: 225 [90368/225000 (40%)] Loss: 8165.919922\n",
      "Train Epoch: 225 [94464/225000 (42%)] Loss: 8071.427734\n",
      "Train Epoch: 225 [98560/225000 (44%)] Loss: 8003.492188\n",
      "Train Epoch: 225 [102656/225000 (46%)] Loss: 8087.783203\n",
      "Train Epoch: 225 [106752/225000 (47%)] Loss: 8082.474609\n",
      "Train Epoch: 225 [110848/225000 (49%)] Loss: 7840.150391\n",
      "Train Epoch: 225 [114944/225000 (51%)] Loss: 7745.517578\n",
      "Train Epoch: 225 [119040/225000 (53%)] Loss: 8125.748047\n",
      "Train Epoch: 225 [123136/225000 (55%)] Loss: 19437.925781\n",
      "Train Epoch: 225 [127232/225000 (57%)] Loss: 10621.300781\n",
      "Train Epoch: 225 [131328/225000 (58%)] Loss: 8085.740234\n",
      "Train Epoch: 225 [135424/225000 (60%)] Loss: 8017.789062\n",
      "Train Epoch: 225 [139520/225000 (62%)] Loss: 8000.359375\n",
      "Train Epoch: 225 [143616/225000 (64%)] Loss: 10640.818359\n",
      "Train Epoch: 225 [147712/225000 (66%)] Loss: 8095.736328\n",
      "Train Epoch: 225 [151808/225000 (67%)] Loss: 7963.097656\n",
      "Train Epoch: 225 [155904/225000 (69%)] Loss: 7916.867188\n",
      "Train Epoch: 225 [160000/225000 (71%)] Loss: 8058.605469\n",
      "Train Epoch: 225 [164096/225000 (73%)] Loss: 7827.093750\n",
      "Train Epoch: 225 [168192/225000 (75%)] Loss: 13841.199219\n",
      "Train Epoch: 225 [172288/225000 (77%)] Loss: 8137.316406\n",
      "Train Epoch: 225 [176384/225000 (78%)] Loss: 13964.828125\n",
      "Train Epoch: 225 [180480/225000 (80%)] Loss: 12159.314453\n",
      "Train Epoch: 225 [184576/225000 (82%)] Loss: 7893.101562\n",
      "Train Epoch: 225 [188672/225000 (84%)] Loss: 10425.291016\n",
      "Train Epoch: 225 [192768/225000 (86%)] Loss: 8243.634766\n",
      "Train Epoch: 225 [196864/225000 (87%)] Loss: 10769.634766\n",
      "Train Epoch: 225 [200960/225000 (89%)] Loss: 8258.910156\n",
      "Train Epoch: 225 [205056/225000 (91%)] Loss: 10413.515625\n",
      "Train Epoch: 225 [209152/225000 (93%)] Loss: 8152.400391\n",
      "Train Epoch: 225 [213248/225000 (95%)] Loss: 9638.011719\n",
      "Train Epoch: 225 [217344/225000 (97%)] Loss: 8178.294922\n",
      "Train Epoch: 225 [221440/225000 (98%)] Loss: 11059.818359\n",
      "    epoch          : 225\n",
      "    loss           : 9504.3017878093\n",
      "    val_loss       : 9938.035977190855\n",
      "Train Epoch: 226 [256/225000 (0%)] Loss: 8073.472656\n",
      "Train Epoch: 226 [4352/225000 (2%)] Loss: 8023.244141\n",
      "Train Epoch: 226 [8448/225000 (4%)] Loss: 9650.003906\n",
      "Train Epoch: 226 [12544/225000 (6%)] Loss: 7988.720703\n",
      "Train Epoch: 226 [16640/225000 (7%)] Loss: 11078.046875\n",
      "Train Epoch: 226 [20736/225000 (9%)] Loss: 8104.910156\n",
      "Train Epoch: 226 [24832/225000 (11%)] Loss: 9714.572266\n",
      "Train Epoch: 226 [28928/225000 (13%)] Loss: 16235.558594\n",
      "Train Epoch: 226 [33024/225000 (15%)] Loss: 8018.732422\n",
      "Train Epoch: 226 [37120/225000 (16%)] Loss: 7939.619141\n",
      "Train Epoch: 226 [41216/225000 (18%)] Loss: 7977.261719\n",
      "Train Epoch: 226 [45312/225000 (20%)] Loss: 12075.390625\n",
      "Train Epoch: 226 [49408/225000 (22%)] Loss: 13015.900391\n",
      "Train Epoch: 226 [53504/225000 (24%)] Loss: 8095.175781\n",
      "Train Epoch: 226 [57600/225000 (26%)] Loss: 12176.359375\n",
      "Train Epoch: 226 [61696/225000 (27%)] Loss: 12135.734375\n",
      "Train Epoch: 226 [65792/225000 (29%)] Loss: 10498.615234\n",
      "Train Epoch: 226 [69888/225000 (31%)] Loss: 8130.945312\n",
      "Train Epoch: 226 [73984/225000 (33%)] Loss: 8217.402344\n",
      "Train Epoch: 226 [78080/225000 (35%)] Loss: 10517.076172\n",
      "Train Epoch: 226 [82176/225000 (37%)] Loss: 12388.437500\n",
      "Train Epoch: 226 [86272/225000 (38%)] Loss: 8153.082031\n",
      "Train Epoch: 226 [90368/225000 (40%)] Loss: 8125.205078\n",
      "Train Epoch: 226 [94464/225000 (42%)] Loss: 8180.419922\n",
      "Train Epoch: 226 [98560/225000 (44%)] Loss: 15001.957031\n",
      "Train Epoch: 226 [102656/225000 (46%)] Loss: 11870.191406\n",
      "Train Epoch: 226 [106752/225000 (47%)] Loss: 10558.642578\n",
      "Train Epoch: 226 [110848/225000 (49%)] Loss: 8094.210938\n",
      "Train Epoch: 226 [114944/225000 (51%)] Loss: 8026.408203\n",
      "Train Epoch: 226 [119040/225000 (53%)] Loss: 10434.275391\n",
      "Train Epoch: 226 [123136/225000 (55%)] Loss: 11076.587891\n",
      "Train Epoch: 226 [127232/225000 (57%)] Loss: 13813.158203\n",
      "Train Epoch: 226 [131328/225000 (58%)] Loss: 12439.494141\n",
      "Train Epoch: 226 [135424/225000 (60%)] Loss: 9745.660156\n",
      "Train Epoch: 226 [139520/225000 (62%)] Loss: 9468.390625\n",
      "Train Epoch: 226 [143616/225000 (64%)] Loss: 8290.544922\n",
      "Train Epoch: 226 [147712/225000 (66%)] Loss: 8005.109375\n",
      "Train Epoch: 226 [151808/225000 (67%)] Loss: 9868.240234\n",
      "Train Epoch: 226 [155904/225000 (69%)] Loss: 12445.068359\n",
      "Train Epoch: 226 [160000/225000 (71%)] Loss: 13989.093750\n",
      "Train Epoch: 226 [164096/225000 (73%)] Loss: 9578.173828\n",
      "Train Epoch: 226 [168192/225000 (75%)] Loss: 12081.712891\n",
      "Train Epoch: 226 [172288/225000 (77%)] Loss: 10423.755859\n",
      "Train Epoch: 226 [176384/225000 (78%)] Loss: 12219.554688\n",
      "Train Epoch: 226 [180480/225000 (80%)] Loss: 10354.681641\n",
      "Train Epoch: 226 [184576/225000 (82%)] Loss: 7958.861328\n",
      "Train Epoch: 226 [188672/225000 (84%)] Loss: 8066.312500\n",
      "Train Epoch: 226 [192768/225000 (86%)] Loss: 8036.593750\n",
      "Train Epoch: 226 [196864/225000 (87%)] Loss: 8032.115234\n",
      "Train Epoch: 226 [200960/225000 (89%)] Loss: 8037.017578\n",
      "Train Epoch: 226 [205056/225000 (91%)] Loss: 8170.109375\n",
      "Train Epoch: 226 [209152/225000 (93%)] Loss: 7832.271484\n",
      "Train Epoch: 226 [213248/225000 (95%)] Loss: 12290.349609\n",
      "Train Epoch: 226 [217344/225000 (97%)] Loss: 8068.343750\n",
      "Train Epoch: 226 [221440/225000 (98%)] Loss: 9555.302734\n",
      "    epoch          : 226\n",
      "    loss           : 9540.325744142847\n",
      "    val_loss       : 9692.578645114996\n",
      "Train Epoch: 227 [256/225000 (0%)] Loss: 9751.994141\n",
      "Train Epoch: 227 [4352/225000 (2%)] Loss: 8031.931641\n",
      "Train Epoch: 227 [8448/225000 (4%)] Loss: 8193.390625\n",
      "Train Epoch: 227 [12544/225000 (6%)] Loss: 8047.681641\n",
      "Train Epoch: 227 [16640/225000 (7%)] Loss: 8071.998047\n",
      "Train Epoch: 227 [20736/225000 (9%)] Loss: 9829.677734\n",
      "Train Epoch: 227 [24832/225000 (11%)] Loss: 10622.779297\n",
      "Train Epoch: 227 [28928/225000 (13%)] Loss: 9597.972656\n",
      "Train Epoch: 227 [33024/225000 (15%)] Loss: 8082.681641\n",
      "Train Epoch: 227 [37120/225000 (16%)] Loss: 8265.246094\n",
      "Train Epoch: 227 [41216/225000 (18%)] Loss: 7933.021484\n",
      "Train Epoch: 227 [45312/225000 (20%)] Loss: 12812.425781\n",
      "Train Epoch: 227 [49408/225000 (22%)] Loss: 9636.902344\n",
      "Train Epoch: 227 [53504/225000 (24%)] Loss: 8066.597656\n",
      "Train Epoch: 227 [57600/225000 (26%)] Loss: 8184.917969\n",
      "Train Epoch: 227 [61696/225000 (27%)] Loss: 10561.513672\n",
      "Train Epoch: 227 [65792/225000 (29%)] Loss: 7848.984375\n",
      "Train Epoch: 227 [69888/225000 (31%)] Loss: 12317.763672\n",
      "Train Epoch: 227 [73984/225000 (33%)] Loss: 12191.916016\n",
      "Train Epoch: 227 [78080/225000 (35%)] Loss: 8239.367188\n",
      "Train Epoch: 227 [82176/225000 (37%)] Loss: 12201.972656\n",
      "Train Epoch: 227 [86272/225000 (38%)] Loss: 8080.138672\n",
      "Train Epoch: 227 [90368/225000 (40%)] Loss: 10550.935547\n",
      "Train Epoch: 227 [94464/225000 (42%)] Loss: 7970.279297\n",
      "Train Epoch: 227 [98560/225000 (44%)] Loss: 10663.763672\n",
      "Train Epoch: 227 [102656/225000 (46%)] Loss: 8090.169922\n",
      "Train Epoch: 227 [106752/225000 (47%)] Loss: 8177.361328\n",
      "Train Epoch: 227 [110848/225000 (49%)] Loss: 7842.031250\n",
      "Train Epoch: 227 [114944/225000 (51%)] Loss: 7854.589844\n",
      "Train Epoch: 227 [119040/225000 (53%)] Loss: 13759.656250\n",
      "Train Epoch: 227 [123136/225000 (55%)] Loss: 10617.576172\n",
      "Train Epoch: 227 [127232/225000 (57%)] Loss: 14141.833984\n",
      "Train Epoch: 227 [131328/225000 (58%)] Loss: 13418.537109\n",
      "Train Epoch: 227 [135424/225000 (60%)] Loss: 8145.886719\n",
      "Train Epoch: 227 [139520/225000 (62%)] Loss: 10496.968750\n",
      "Train Epoch: 227 [143616/225000 (64%)] Loss: 16185.085938\n",
      "Train Epoch: 227 [147712/225000 (66%)] Loss: 9609.437500\n",
      "Train Epoch: 227 [151808/225000 (67%)] Loss: 7950.046875\n",
      "Train Epoch: 227 [155904/225000 (69%)] Loss: 7916.462891\n",
      "Train Epoch: 227 [160000/225000 (71%)] Loss: 11443.656250\n",
      "Train Epoch: 227 [164096/225000 (73%)] Loss: 10483.289062\n",
      "Train Epoch: 227 [168192/225000 (75%)] Loss: 8014.640625\n",
      "Train Epoch: 227 [172288/225000 (77%)] Loss: 10526.693359\n",
      "Train Epoch: 227 [176384/225000 (78%)] Loss: 7999.958984\n",
      "Train Epoch: 227 [180480/225000 (80%)] Loss: 8063.593750\n",
      "Train Epoch: 227 [184576/225000 (82%)] Loss: 8240.154297\n",
      "Train Epoch: 227 [188672/225000 (84%)] Loss: 8136.015625\n",
      "Train Epoch: 227 [192768/225000 (86%)] Loss: 8084.558594\n",
      "Train Epoch: 227 [196864/225000 (87%)] Loss: 8015.775391\n",
      "Train Epoch: 227 [200960/225000 (89%)] Loss: 10759.816406\n",
      "Train Epoch: 227 [205056/225000 (91%)] Loss: 7914.056641\n",
      "Train Epoch: 227 [209152/225000 (93%)] Loss: 13833.376953\n",
      "Train Epoch: 227 [213248/225000 (95%)] Loss: 10394.503906\n",
      "Train Epoch: 227 [217344/225000 (97%)] Loss: 7970.865234\n",
      "Train Epoch: 227 [221440/225000 (98%)] Loss: 9764.765625\n",
      "    epoch          : 227\n",
      "    loss           : 9541.371663689206\n",
      "    val_loss       : 9731.543357610703\n",
      "Train Epoch: 228 [256/225000 (0%)] Loss: 9648.888672\n",
      "Train Epoch: 228 [4352/225000 (2%)] Loss: 8067.605469\n",
      "Train Epoch: 228 [8448/225000 (4%)] Loss: 7807.408203\n",
      "Train Epoch: 228 [12544/225000 (6%)] Loss: 7995.931641\n",
      "Train Epoch: 228 [16640/225000 (7%)] Loss: 8339.326172\n",
      "Train Epoch: 228 [20736/225000 (9%)] Loss: 9558.128906\n",
      "Train Epoch: 228 [24832/225000 (11%)] Loss: 8147.853516\n",
      "Train Epoch: 228 [28928/225000 (13%)] Loss: 8148.947266\n",
      "Train Epoch: 228 [33024/225000 (15%)] Loss: 9599.914062\n",
      "Train Epoch: 228 [37120/225000 (16%)] Loss: 12421.576172\n",
      "Train Epoch: 228 [41216/225000 (18%)] Loss: 9731.292969\n",
      "Train Epoch: 228 [45312/225000 (20%)] Loss: 8059.945312\n",
      "Train Epoch: 228 [49408/225000 (22%)] Loss: 14010.718750\n",
      "Train Epoch: 228 [53504/225000 (24%)] Loss: 7899.443359\n",
      "Train Epoch: 228 [57600/225000 (26%)] Loss: 8223.107422\n",
      "Train Epoch: 228 [61696/225000 (27%)] Loss: 8216.593750\n",
      "Train Epoch: 228 [65792/225000 (29%)] Loss: 8088.392578\n",
      "Train Epoch: 228 [69888/225000 (31%)] Loss: 14008.355469\n",
      "Train Epoch: 228 [73984/225000 (33%)] Loss: 8072.082031\n",
      "Train Epoch: 228 [78080/225000 (35%)] Loss: 8142.146484\n",
      "Train Epoch: 228 [82176/225000 (37%)] Loss: 8062.066406\n",
      "Train Epoch: 228 [86272/225000 (38%)] Loss: 12726.876953\n",
      "Train Epoch: 228 [90368/225000 (40%)] Loss: 18235.410156\n",
      "Train Epoch: 228 [94464/225000 (42%)] Loss: 15324.410156\n",
      "Train Epoch: 228 [98560/225000 (44%)] Loss: 20743.521484\n",
      "Train Epoch: 228 [102656/225000 (46%)] Loss: 9803.058594\n",
      "Train Epoch: 228 [106752/225000 (47%)] Loss: 10740.187500\n",
      "Train Epoch: 228 [110848/225000 (49%)] Loss: 8022.625000\n",
      "Train Epoch: 228 [114944/225000 (51%)] Loss: 8145.992188\n",
      "Train Epoch: 228 [119040/225000 (53%)] Loss: 8059.169922\n",
      "Train Epoch: 228 [123136/225000 (55%)] Loss: 10500.693359\n",
      "Train Epoch: 228 [127232/225000 (57%)] Loss: 14044.240234\n",
      "Train Epoch: 228 [131328/225000 (58%)] Loss: 8259.144531\n",
      "Train Epoch: 228 [135424/225000 (60%)] Loss: 8145.521484\n",
      "Train Epoch: 228 [139520/225000 (62%)] Loss: 12324.158203\n",
      "Train Epoch: 228 [143616/225000 (64%)] Loss: 8161.892578\n",
      "Train Epoch: 228 [147712/225000 (66%)] Loss: 13046.900391\n",
      "Train Epoch: 228 [151808/225000 (67%)] Loss: 7936.773438\n",
      "Train Epoch: 228 [155904/225000 (69%)] Loss: 8146.308594\n",
      "Train Epoch: 228 [160000/225000 (71%)] Loss: 10584.074219\n",
      "Train Epoch: 228 [164096/225000 (73%)] Loss: 8162.029297\n",
      "Train Epoch: 228 [168192/225000 (75%)] Loss: 7880.449219\n",
      "Train Epoch: 228 [172288/225000 (77%)] Loss: 8101.791016\n",
      "Train Epoch: 228 [176384/225000 (78%)] Loss: 7960.898438\n",
      "Train Epoch: 228 [180480/225000 (80%)] Loss: 10397.523438\n",
      "Train Epoch: 228 [184576/225000 (82%)] Loss: 9503.294922\n",
      "Train Epoch: 228 [188672/225000 (84%)] Loss: 7851.972656\n",
      "Train Epoch: 228 [192768/225000 (86%)] Loss: 8044.171875\n",
      "Train Epoch: 228 [196864/225000 (87%)] Loss: 7967.472656\n",
      "Train Epoch: 228 [200960/225000 (89%)] Loss: 8102.013672\n",
      "Train Epoch: 228 [205056/225000 (91%)] Loss: 8091.599609\n",
      "Train Epoch: 228 [209152/225000 (93%)] Loss: 8111.792969\n",
      "Train Epoch: 228 [213248/225000 (95%)] Loss: 8104.207031\n",
      "Train Epoch: 228 [217344/225000 (97%)] Loss: 8011.917969\n",
      "Train Epoch: 228 [221440/225000 (98%)] Loss: 8042.507812\n",
      "    epoch          : 228\n",
      "    loss           : 9492.990319921431\n",
      "    val_loss       : 9293.991690573645\n",
      "Train Epoch: 229 [256/225000 (0%)] Loss: 8124.037109\n",
      "Train Epoch: 229 [4352/225000 (2%)] Loss: 15632.929688\n",
      "Train Epoch: 229 [8448/225000 (4%)] Loss: 8083.679688\n",
      "Train Epoch: 229 [12544/225000 (6%)] Loss: 8054.283203\n",
      "Train Epoch: 229 [16640/225000 (7%)] Loss: 8156.935547\n",
      "Train Epoch: 229 [20736/225000 (9%)] Loss: 7972.271484\n",
      "Train Epoch: 229 [24832/225000 (11%)] Loss: 7974.445312\n",
      "Train Epoch: 229 [28928/225000 (13%)] Loss: 15168.687500\n",
      "Train Epoch: 229 [33024/225000 (15%)] Loss: 8178.861328\n",
      "Train Epoch: 229 [37120/225000 (16%)] Loss: 7960.660156\n",
      "Train Epoch: 229 [41216/225000 (18%)] Loss: 9711.488281\n",
      "Train Epoch: 229 [45312/225000 (20%)] Loss: 15031.226562\n",
      "Train Epoch: 229 [49408/225000 (22%)] Loss: 7957.785156\n",
      "Train Epoch: 229 [53504/225000 (24%)] Loss: 10572.289062\n",
      "Train Epoch: 229 [57600/225000 (26%)] Loss: 8063.894531\n",
      "Train Epoch: 229 [61696/225000 (27%)] Loss: 12484.210938\n",
      "Train Epoch: 229 [65792/225000 (29%)] Loss: 14793.427734\n",
      "Train Epoch: 229 [69888/225000 (31%)] Loss: 10528.314453\n",
      "Train Epoch: 229 [73984/225000 (33%)] Loss: 8084.832031\n",
      "Train Epoch: 229 [78080/225000 (35%)] Loss: 8121.587891\n",
      "Train Epoch: 229 [82176/225000 (37%)] Loss: 10430.970703\n",
      "Train Epoch: 229 [86272/225000 (38%)] Loss: 9852.974609\n",
      "Train Epoch: 229 [90368/225000 (40%)] Loss: 8012.578125\n",
      "Train Epoch: 229 [94464/225000 (42%)] Loss: 7985.628906\n",
      "Train Epoch: 229 [98560/225000 (44%)] Loss: 8068.746094\n",
      "Train Epoch: 229 [102656/225000 (46%)] Loss: 8270.677734\n",
      "Train Epoch: 229 [106752/225000 (47%)] Loss: 8098.853516\n",
      "Train Epoch: 229 [110848/225000 (49%)] Loss: 8216.761719\n",
      "Train Epoch: 229 [114944/225000 (51%)] Loss: 20441.416016\n",
      "Train Epoch: 229 [119040/225000 (53%)] Loss: 9631.812500\n",
      "Train Epoch: 229 [123136/225000 (55%)] Loss: 8081.445312\n",
      "Train Epoch: 229 [127232/225000 (57%)] Loss: 9590.953125\n",
      "Train Epoch: 229 [131328/225000 (58%)] Loss: 9763.255859\n",
      "Train Epoch: 229 [135424/225000 (60%)] Loss: 8144.328125\n",
      "Train Epoch: 229 [139520/225000 (62%)] Loss: 10793.009766\n",
      "Train Epoch: 229 [143616/225000 (64%)] Loss: 8094.675781\n",
      "Train Epoch: 229 [147712/225000 (66%)] Loss: 8006.617188\n",
      "Train Epoch: 229 [151808/225000 (67%)] Loss: 9580.371094\n",
      "Train Epoch: 229 [155904/225000 (69%)] Loss: 7971.126953\n",
      "Train Epoch: 229 [160000/225000 (71%)] Loss: 10460.662109\n",
      "Train Epoch: 229 [164096/225000 (73%)] Loss: 7957.490234\n",
      "Train Epoch: 229 [168192/225000 (75%)] Loss: 7980.380859\n",
      "Train Epoch: 229 [172288/225000 (77%)] Loss: 7788.654297\n",
      "Train Epoch: 229 [176384/225000 (78%)] Loss: 8131.031250\n",
      "Train Epoch: 229 [180480/225000 (80%)] Loss: 9680.828125\n",
      "Train Epoch: 229 [184576/225000 (82%)] Loss: 7946.818359\n",
      "Train Epoch: 229 [188672/225000 (84%)] Loss: 8136.480469\n",
      "Train Epoch: 229 [192768/225000 (86%)] Loss: 10629.652344\n",
      "Train Epoch: 229 [196864/225000 (87%)] Loss: 9680.085938\n",
      "Train Epoch: 229 [200960/225000 (89%)] Loss: 10418.808594\n",
      "Train Epoch: 229 [205056/225000 (91%)] Loss: 8328.111328\n",
      "Train Epoch: 229 [209152/225000 (93%)] Loss: 7919.085938\n",
      "Train Epoch: 229 [213248/225000 (95%)] Loss: 7862.906250\n",
      "Train Epoch: 229 [217344/225000 (97%)] Loss: 11337.681641\n",
      "Train Epoch: 229 [221440/225000 (98%)] Loss: 8031.683594\n",
      "    epoch          : 229\n",
      "    loss           : 9542.563338799417\n",
      "    val_loss       : 9293.201680684577\n",
      "Train Epoch: 230 [256/225000 (0%)] Loss: 8153.683594\n",
      "Train Epoch: 230 [4352/225000 (2%)] Loss: 8096.974609\n",
      "Train Epoch: 230 [8448/225000 (4%)] Loss: 8202.208984\n",
      "Train Epoch: 230 [12544/225000 (6%)] Loss: 10635.960938\n",
      "Train Epoch: 230 [16640/225000 (7%)] Loss: 8197.150391\n",
      "Train Epoch: 230 [20736/225000 (9%)] Loss: 7835.707031\n",
      "Train Epoch: 230 [24832/225000 (11%)] Loss: 7997.955078\n",
      "Train Epoch: 230 [28928/225000 (13%)] Loss: 8161.759766\n",
      "Train Epoch: 230 [33024/225000 (15%)] Loss: 10695.964844\n",
      "Train Epoch: 230 [37120/225000 (16%)] Loss: 7977.570312\n",
      "Train Epoch: 230 [41216/225000 (18%)] Loss: 15376.029297\n",
      "Train Epoch: 230 [45312/225000 (20%)] Loss: 10539.539062\n",
      "Train Epoch: 230 [49408/225000 (22%)] Loss: 9487.669922\n",
      "Train Epoch: 230 [53504/225000 (24%)] Loss: 8156.845703\n",
      "Train Epoch: 230 [57600/225000 (26%)] Loss: 13890.794922\n",
      "Train Epoch: 230 [61696/225000 (27%)] Loss: 7883.333984\n",
      "Train Epoch: 230 [65792/225000 (29%)] Loss: 7956.794922\n",
      "Train Epoch: 230 [69888/225000 (31%)] Loss: 7983.791016\n",
      "Train Epoch: 230 [73984/225000 (33%)] Loss: 8245.457031\n",
      "Train Epoch: 230 [78080/225000 (35%)] Loss: 13327.410156\n",
      "Train Epoch: 230 [82176/225000 (37%)] Loss: 8153.335938\n",
      "Train Epoch: 230 [86272/225000 (38%)] Loss: 7996.662109\n",
      "Train Epoch: 230 [90368/225000 (40%)] Loss: 9812.835938\n",
      "Train Epoch: 230 [94464/225000 (42%)] Loss: 8042.865234\n",
      "Train Epoch: 230 [98560/225000 (44%)] Loss: 7994.472656\n",
      "Train Epoch: 230 [102656/225000 (46%)] Loss: 13680.640625\n",
      "Train Epoch: 230 [106752/225000 (47%)] Loss: 9627.748047\n",
      "Train Epoch: 230 [110848/225000 (49%)] Loss: 10408.080078\n",
      "Train Epoch: 230 [114944/225000 (51%)] Loss: 9538.392578\n",
      "Train Epoch: 230 [119040/225000 (53%)] Loss: 9570.156250\n",
      "Train Epoch: 230 [123136/225000 (55%)] Loss: 9672.683594\n",
      "Train Epoch: 230 [127232/225000 (57%)] Loss: 8157.812500\n",
      "Train Epoch: 230 [131328/225000 (58%)] Loss: 9601.025391\n",
      "Train Epoch: 230 [135424/225000 (60%)] Loss: 9699.142578\n",
      "Train Epoch: 230 [139520/225000 (62%)] Loss: 8001.287109\n",
      "Train Epoch: 230 [143616/225000 (64%)] Loss: 13770.119141\n",
      "Train Epoch: 230 [147712/225000 (66%)] Loss: 8196.943359\n",
      "Train Epoch: 230 [151808/225000 (67%)] Loss: 9835.550781\n",
      "Train Epoch: 230 [155904/225000 (69%)] Loss: 15181.837891\n",
      "Train Epoch: 230 [160000/225000 (71%)] Loss: 10505.613281\n",
      "Train Epoch: 230 [164096/225000 (73%)] Loss: 13529.371094\n",
      "Train Epoch: 230 [168192/225000 (75%)] Loss: 10600.382812\n",
      "Train Epoch: 230 [172288/225000 (77%)] Loss: 10614.808594\n",
      "Train Epoch: 230 [176384/225000 (78%)] Loss: 8203.607422\n",
      "Train Epoch: 230 [180480/225000 (80%)] Loss: 8311.660156\n",
      "Train Epoch: 230 [184576/225000 (82%)] Loss: 9682.765625\n",
      "Train Epoch: 230 [188672/225000 (84%)] Loss: 13810.978516\n",
      "Train Epoch: 230 [192768/225000 (86%)] Loss: 8157.279297\n",
      "Train Epoch: 230 [196864/225000 (87%)] Loss: 7800.699219\n",
      "Train Epoch: 230 [200960/225000 (89%)] Loss: 8142.791016\n",
      "Train Epoch: 230 [205056/225000 (91%)] Loss: 8020.339844\n",
      "Train Epoch: 230 [209152/225000 (93%)] Loss: 7949.902344\n",
      "Train Epoch: 230 [213248/225000 (95%)] Loss: 9552.121094\n",
      "Train Epoch: 230 [217344/225000 (97%)] Loss: 8077.814453\n",
      "Train Epoch: 230 [221440/225000 (98%)] Loss: 8032.978516\n",
      "    epoch          : 230\n",
      "    loss           : 9548.651381630403\n",
      "    val_loss       : 9379.607880563151\n",
      "Train Epoch: 231 [256/225000 (0%)] Loss: 10521.902344\n",
      "Train Epoch: 231 [4352/225000 (2%)] Loss: 13836.226562\n",
      "Train Epoch: 231 [8448/225000 (4%)] Loss: 9562.300781\n",
      "Train Epoch: 231 [12544/225000 (6%)] Loss: 8184.244141\n",
      "Train Epoch: 231 [16640/225000 (7%)] Loss: 8049.035156\n",
      "Train Epoch: 231 [20736/225000 (9%)] Loss: 8082.437500\n",
      "Train Epoch: 231 [24832/225000 (11%)] Loss: 8139.414062\n",
      "Train Epoch: 231 [28928/225000 (13%)] Loss: 7889.566406\n",
      "Train Epoch: 231 [33024/225000 (15%)] Loss: 8031.337891\n",
      "Train Epoch: 231 [37120/225000 (16%)] Loss: 15340.699219\n",
      "Train Epoch: 231 [41216/225000 (18%)] Loss: 8164.179688\n",
      "Train Epoch: 231 [45312/225000 (20%)] Loss: 7873.937500\n",
      "Train Epoch: 231 [49408/225000 (22%)] Loss: 10599.828125\n",
      "Train Epoch: 231 [53504/225000 (24%)] Loss: 10488.695312\n",
      "Train Epoch: 231 [57600/225000 (26%)] Loss: 18845.570312\n",
      "Train Epoch: 231 [61696/225000 (27%)] Loss: 7988.142578\n",
      "Train Epoch: 231 [65792/225000 (29%)] Loss: 13636.027344\n",
      "Train Epoch: 231 [69888/225000 (31%)] Loss: 11289.166016\n",
      "Train Epoch: 231 [73984/225000 (33%)] Loss: 10754.886719\n",
      "Train Epoch: 231 [78080/225000 (35%)] Loss: 8021.494141\n",
      "Train Epoch: 231 [82176/225000 (37%)] Loss: 7978.804688\n",
      "Train Epoch: 231 [86272/225000 (38%)] Loss: 10639.638672\n",
      "Train Epoch: 231 [90368/225000 (40%)] Loss: 12736.695312\n",
      "Train Epoch: 231 [94464/225000 (42%)] Loss: 7882.929688\n",
      "Train Epoch: 231 [98560/225000 (44%)] Loss: 8258.011719\n",
      "Train Epoch: 231 [102656/225000 (46%)] Loss: 9509.638672\n",
      "Train Epoch: 231 [106752/225000 (47%)] Loss: 9767.869141\n",
      "Train Epoch: 231 [110848/225000 (49%)] Loss: 9642.529297\n",
      "Train Epoch: 231 [114944/225000 (51%)] Loss: 13639.904297\n",
      "Train Epoch: 231 [119040/225000 (53%)] Loss: 13834.375000\n",
      "Train Epoch: 231 [123136/225000 (55%)] Loss: 8253.359375\n",
      "Train Epoch: 231 [127232/225000 (57%)] Loss: 16038.224609\n",
      "Train Epoch: 231 [131328/225000 (58%)] Loss: 7806.763672\n",
      "Train Epoch: 231 [135424/225000 (60%)] Loss: 13687.412109\n",
      "Train Epoch: 231 [139520/225000 (62%)] Loss: 8017.595703\n",
      "Train Epoch: 231 [143616/225000 (64%)] Loss: 8002.808594\n",
      "Train Epoch: 231 [147712/225000 (66%)] Loss: 7986.009766\n",
      "Train Epoch: 231 [151808/225000 (67%)] Loss: 9496.214844\n",
      "Train Epoch: 231 [155904/225000 (69%)] Loss: 9494.599609\n",
      "Train Epoch: 231 [160000/225000 (71%)] Loss: 10596.234375\n",
      "Train Epoch: 231 [164096/225000 (73%)] Loss: 8093.634766\n",
      "Train Epoch: 231 [168192/225000 (75%)] Loss: 13849.507812\n",
      "Train Epoch: 231 [172288/225000 (77%)] Loss: 8217.519531\n",
      "Train Epoch: 231 [176384/225000 (78%)] Loss: 7895.927734\n",
      "Train Epoch: 231 [180480/225000 (80%)] Loss: 7832.087891\n",
      "Train Epoch: 231 [184576/225000 (82%)] Loss: 10340.859375\n",
      "Train Epoch: 231 [188672/225000 (84%)] Loss: 7922.705078\n",
      "Train Epoch: 231 [192768/225000 (86%)] Loss: 11281.441406\n",
      "Train Epoch: 231 [196864/225000 (87%)] Loss: 8022.830078\n",
      "Train Epoch: 231 [200960/225000 (89%)] Loss: 8311.375000\n",
      "Train Epoch: 231 [205056/225000 (91%)] Loss: 10474.402344\n",
      "Train Epoch: 231 [209152/225000 (93%)] Loss: 8143.472656\n",
      "Train Epoch: 231 [213248/225000 (95%)] Loss: 8317.431641\n",
      "Train Epoch: 231 [217344/225000 (97%)] Loss: 7934.908203\n",
      "Train Epoch: 231 [221440/225000 (98%)] Loss: 8030.251953\n",
      "    epoch          : 231\n",
      "    loss           : 9541.963531667734\n",
      "    val_loss       : 9423.883917438741\n",
      "Train Epoch: 232 [256/225000 (0%)] Loss: 8112.097656\n",
      "Train Epoch: 232 [4352/225000 (2%)] Loss: 8020.669922\n",
      "Train Epoch: 232 [8448/225000 (4%)] Loss: 12139.853516\n",
      "Train Epoch: 232 [12544/225000 (6%)] Loss: 13760.189453\n",
      "Train Epoch: 232 [16640/225000 (7%)] Loss: 8015.412109\n",
      "Train Epoch: 232 [20736/225000 (9%)] Loss: 13230.716797\n",
      "Train Epoch: 232 [24832/225000 (11%)] Loss: 8188.156250\n",
      "Train Epoch: 232 [28928/225000 (13%)] Loss: 8079.583984\n",
      "Train Epoch: 232 [33024/225000 (15%)] Loss: 7837.044922\n",
      "Train Epoch: 232 [37120/225000 (16%)] Loss: 8072.044922\n",
      "Train Epoch: 232 [41216/225000 (18%)] Loss: 8002.007812\n",
      "Train Epoch: 232 [45312/225000 (20%)] Loss: 9462.808594\n",
      "Train Epoch: 232 [49408/225000 (22%)] Loss: 8217.951172\n",
      "Train Epoch: 232 [53504/225000 (24%)] Loss: 8124.146484\n",
      "Train Epoch: 232 [57600/225000 (26%)] Loss: 7924.263672\n",
      "Train Epoch: 232 [61696/225000 (27%)] Loss: 8152.335938\n",
      "Train Epoch: 232 [65792/225000 (29%)] Loss: 8026.841797\n",
      "Train Epoch: 232 [69888/225000 (31%)] Loss: 10263.158203\n",
      "Train Epoch: 232 [73984/225000 (33%)] Loss: 7977.796875\n",
      "Train Epoch: 232 [78080/225000 (35%)] Loss: 8155.349609\n",
      "Train Epoch: 232 [82176/225000 (37%)] Loss: 8031.865234\n",
      "Train Epoch: 232 [86272/225000 (38%)] Loss: 7998.140625\n",
      "Train Epoch: 232 [90368/225000 (40%)] Loss: 8214.833984\n",
      "Train Epoch: 232 [94464/225000 (42%)] Loss: 7937.742188\n",
      "Train Epoch: 232 [98560/225000 (44%)] Loss: 13818.257812\n",
      "Train Epoch: 232 [102656/225000 (46%)] Loss: 9673.623047\n",
      "Train Epoch: 232 [106752/225000 (47%)] Loss: 8146.433594\n",
      "Train Epoch: 232 [110848/225000 (49%)] Loss: 8092.013672\n",
      "Train Epoch: 232 [114944/225000 (51%)] Loss: 9360.091797\n",
      "Train Epoch: 232 [119040/225000 (53%)] Loss: 8061.498047\n",
      "Train Epoch: 232 [123136/225000 (55%)] Loss: 8198.726562\n",
      "Train Epoch: 232 [127232/225000 (57%)] Loss: 13937.673828\n",
      "Train Epoch: 232 [131328/225000 (58%)] Loss: 7986.451172\n",
      "Train Epoch: 232 [135424/225000 (60%)] Loss: 8070.957031\n",
      "Train Epoch: 232 [139520/225000 (62%)] Loss: 10461.689453\n",
      "Train Epoch: 232 [143616/225000 (64%)] Loss: 8014.781250\n",
      "Train Epoch: 232 [147712/225000 (66%)] Loss: 8190.400391\n",
      "Train Epoch: 232 [151808/225000 (67%)] Loss: 16649.117188\n",
      "Train Epoch: 232 [155904/225000 (69%)] Loss: 14993.328125\n",
      "Train Epoch: 232 [160000/225000 (71%)] Loss: 8161.718750\n",
      "Train Epoch: 232 [164096/225000 (73%)] Loss: 10529.177734\n",
      "Train Epoch: 232 [168192/225000 (75%)] Loss: 9466.263672\n",
      "Train Epoch: 232 [172288/225000 (77%)] Loss: 8152.568359\n",
      "Train Epoch: 232 [176384/225000 (78%)] Loss: 9615.464844\n",
      "Train Epoch: 232 [180480/225000 (80%)] Loss: 8017.660156\n",
      "Train Epoch: 232 [184576/225000 (82%)] Loss: 9454.912109\n",
      "Train Epoch: 232 [188672/225000 (84%)] Loss: 10497.978516\n",
      "Train Epoch: 232 [192768/225000 (86%)] Loss: 7946.984375\n",
      "Train Epoch: 232 [196864/225000 (87%)] Loss: 7986.960938\n",
      "Train Epoch: 232 [200960/225000 (89%)] Loss: 13268.552734\n",
      "Train Epoch: 232 [205056/225000 (91%)] Loss: 9417.568359\n",
      "Train Epoch: 232 [209152/225000 (93%)] Loss: 9612.976562\n",
      "Train Epoch: 232 [213248/225000 (95%)] Loss: 8052.113281\n",
      "Train Epoch: 232 [217344/225000 (97%)] Loss: 8007.941406\n",
      "Train Epoch: 232 [221440/225000 (98%)] Loss: 7987.617188\n",
      "    epoch          : 232\n",
      "    loss           : 9414.165800092434\n",
      "    val_loss       : 9496.69282422747\n",
      "Train Epoch: 233 [256/225000 (0%)] Loss: 9665.388672\n",
      "Train Epoch: 233 [4352/225000 (2%)] Loss: 8083.851562\n",
      "Train Epoch: 233 [8448/225000 (4%)] Loss: 9634.134766\n",
      "Train Epoch: 233 [12544/225000 (6%)] Loss: 13528.453125\n",
      "Train Epoch: 233 [16640/225000 (7%)] Loss: 8042.113281\n",
      "Train Epoch: 233 [20736/225000 (9%)] Loss: 8233.951172\n",
      "Train Epoch: 233 [24832/225000 (11%)] Loss: 10311.667969\n",
      "Train Epoch: 233 [28928/225000 (13%)] Loss: 7925.103516\n",
      "Train Epoch: 233 [33024/225000 (15%)] Loss: 8067.726562\n",
      "Train Epoch: 233 [37120/225000 (16%)] Loss: 7981.876953\n",
      "Train Epoch: 233 [41216/225000 (18%)] Loss: 7916.130859\n",
      "Train Epoch: 233 [45312/225000 (20%)] Loss: 9543.380859\n",
      "Train Epoch: 233 [49408/225000 (22%)] Loss: 9845.082031\n",
      "Train Epoch: 233 [53504/225000 (24%)] Loss: 8271.560547\n",
      "Train Epoch: 233 [57600/225000 (26%)] Loss: 8178.660156\n",
      "Train Epoch: 233 [61696/225000 (27%)] Loss: 7984.712891\n",
      "Train Epoch: 233 [65792/225000 (29%)] Loss: 8009.074219\n",
      "Train Epoch: 233 [69888/225000 (31%)] Loss: 9639.767578\n",
      "Train Epoch: 233 [73984/225000 (33%)] Loss: 8050.585938\n",
      "Train Epoch: 233 [78080/225000 (35%)] Loss: 9644.822266\n",
      "Train Epoch: 233 [82176/225000 (37%)] Loss: 9625.623047\n",
      "Train Epoch: 233 [86272/225000 (38%)] Loss: 10309.541016\n",
      "Train Epoch: 233 [90368/225000 (40%)] Loss: 8053.714844\n",
      "Train Epoch: 233 [94464/225000 (42%)] Loss: 12461.531250\n",
      "Train Epoch: 233 [98560/225000 (44%)] Loss: 10541.638672\n",
      "Train Epoch: 233 [102656/225000 (46%)] Loss: 9722.205078\n",
      "Train Epoch: 233 [106752/225000 (47%)] Loss: 8105.919922\n",
      "Train Epoch: 233 [110848/225000 (49%)] Loss: 8128.119141\n",
      "Train Epoch: 233 [114944/225000 (51%)] Loss: 8297.330078\n",
      "Train Epoch: 233 [119040/225000 (53%)] Loss: 8203.833984\n",
      "Train Epoch: 233 [123136/225000 (55%)] Loss: 8057.871094\n",
      "Train Epoch: 233 [127232/225000 (57%)] Loss: 7847.087891\n",
      "Train Epoch: 233 [131328/225000 (58%)] Loss: 8169.187500\n",
      "Train Epoch: 233 [135424/225000 (60%)] Loss: 7940.816406\n",
      "Train Epoch: 233 [139520/225000 (62%)] Loss: 9685.062500\n",
      "Train Epoch: 233 [143616/225000 (64%)] Loss: 7870.894531\n",
      "Train Epoch: 233 [147712/225000 (66%)] Loss: 8004.185547\n",
      "Train Epoch: 233 [151808/225000 (67%)] Loss: 13475.683594\n",
      "Train Epoch: 233 [155904/225000 (69%)] Loss: 8196.429688\n",
      "Train Epoch: 233 [160000/225000 (71%)] Loss: 10521.666016\n",
      "Train Epoch: 233 [164096/225000 (73%)] Loss: 12219.021484\n",
      "Train Epoch: 233 [168192/225000 (75%)] Loss: 10615.302734\n",
      "Train Epoch: 233 [172288/225000 (77%)] Loss: 7944.796875\n",
      "Train Epoch: 233 [176384/225000 (78%)] Loss: 8154.812500\n",
      "Train Epoch: 233 [180480/225000 (80%)] Loss: 10578.125000\n",
      "Train Epoch: 233 [184576/225000 (82%)] Loss: 13489.791016\n",
      "Train Epoch: 233 [188672/225000 (84%)] Loss: 12535.796875\n",
      "Train Epoch: 233 [192768/225000 (86%)] Loss: 8001.707031\n",
      "Train Epoch: 233 [196864/225000 (87%)] Loss: 10434.884766\n",
      "Train Epoch: 233 [200960/225000 (89%)] Loss: 8006.781250\n",
      "Train Epoch: 233 [205056/225000 (91%)] Loss: 8019.062500\n",
      "Train Epoch: 233 [209152/225000 (93%)] Loss: 7894.271484\n",
      "Train Epoch: 233 [213248/225000 (95%)] Loss: 10590.777344\n",
      "Train Epoch: 233 [217344/225000 (97%)] Loss: 9595.031250\n",
      "Train Epoch: 233 [221440/225000 (98%)] Loss: 9660.994141\n",
      "    epoch          : 233\n",
      "    loss           : 9664.376009892278\n",
      "    val_loss       : 9789.59366183804\n",
      "Train Epoch: 234 [256/225000 (0%)] Loss: 9640.873047\n",
      "Train Epoch: 234 [4352/225000 (2%)] Loss: 10645.648438\n",
      "Train Epoch: 234 [8448/225000 (4%)] Loss: 10550.376953\n",
      "Train Epoch: 234 [12544/225000 (6%)] Loss: 7821.945312\n",
      "Train Epoch: 234 [16640/225000 (7%)] Loss: 8048.515625\n",
      "Train Epoch: 234 [20736/225000 (9%)] Loss: 8023.742188\n",
      "Train Epoch: 234 [24832/225000 (11%)] Loss: 8101.697266\n",
      "Train Epoch: 234 [28928/225000 (13%)] Loss: 9608.117188\n",
      "Train Epoch: 234 [33024/225000 (15%)] Loss: 8067.683594\n",
      "Train Epoch: 234 [37120/225000 (16%)] Loss: 7978.373047\n",
      "Train Epoch: 234 [41216/225000 (18%)] Loss: 8075.421875\n",
      "Train Epoch: 234 [45312/225000 (20%)] Loss: 8078.792969\n",
      "Train Epoch: 234 [49408/225000 (22%)] Loss: 13650.658203\n",
      "Train Epoch: 234 [53504/225000 (24%)] Loss: 7910.400391\n",
      "Train Epoch: 234 [57600/225000 (26%)] Loss: 9583.171875\n",
      "Train Epoch: 234 [61696/225000 (27%)] Loss: 7904.121094\n",
      "Train Epoch: 234 [65792/225000 (29%)] Loss: 8133.572266\n",
      "Train Epoch: 234 [69888/225000 (31%)] Loss: 9506.388672\n",
      "Train Epoch: 234 [73984/225000 (33%)] Loss: 7990.519531\n",
      "Train Epoch: 234 [78080/225000 (35%)] Loss: 8086.830078\n",
      "Train Epoch: 234 [82176/225000 (37%)] Loss: 8111.931641\n",
      "Train Epoch: 234 [86272/225000 (38%)] Loss: 12609.158203\n",
      "Train Epoch: 234 [90368/225000 (40%)] Loss: 8270.724609\n",
      "Train Epoch: 234 [94464/225000 (42%)] Loss: 9650.943359\n",
      "Train Epoch: 234 [98560/225000 (44%)] Loss: 7978.396484\n",
      "Train Epoch: 234 [102656/225000 (46%)] Loss: 7907.599609\n",
      "Train Epoch: 234 [106752/225000 (47%)] Loss: 13782.191406\n",
      "Train Epoch: 234 [110848/225000 (49%)] Loss: 8113.630859\n",
      "Train Epoch: 234 [114944/225000 (51%)] Loss: 8039.437500\n",
      "Train Epoch: 234 [119040/225000 (53%)] Loss: 13411.625000\n",
      "Train Epoch: 234 [123136/225000 (55%)] Loss: 7951.849609\n",
      "Train Epoch: 234 [127232/225000 (57%)] Loss: 12384.166016\n",
      "Train Epoch: 234 [131328/225000 (58%)] Loss: 7993.009766\n",
      "Train Epoch: 234 [135424/225000 (60%)] Loss: 10806.224609\n",
      "Train Epoch: 234 [139520/225000 (62%)] Loss: 7976.880859\n",
      "Train Epoch: 234 [143616/225000 (64%)] Loss: 8212.429688\n",
      "Train Epoch: 234 [147712/225000 (66%)] Loss: 7998.179688\n",
      "Train Epoch: 234 [151808/225000 (67%)] Loss: 8117.912109\n",
      "Train Epoch: 234 [155904/225000 (69%)] Loss: 13473.048828\n",
      "Train Epoch: 234 [160000/225000 (71%)] Loss: 8062.630859\n",
      "Train Epoch: 234 [164096/225000 (73%)] Loss: 8238.966797\n",
      "Train Epoch: 234 [168192/225000 (75%)] Loss: 8012.400391\n",
      "Train Epoch: 234 [172288/225000 (77%)] Loss: 13648.968750\n",
      "Train Epoch: 234 [176384/225000 (78%)] Loss: 13899.279297\n",
      "Train Epoch: 234 [180480/225000 (80%)] Loss: 7819.673828\n",
      "Train Epoch: 234 [184576/225000 (82%)] Loss: 7984.841797\n",
      "Train Epoch: 234 [188672/225000 (84%)] Loss: 8156.054688\n",
      "Train Epoch: 234 [192768/225000 (86%)] Loss: 7898.433594\n",
      "Train Epoch: 234 [196864/225000 (87%)] Loss: 8175.554688\n",
      "Train Epoch: 234 [200960/225000 (89%)] Loss: 8029.531250\n",
      "Train Epoch: 234 [205056/225000 (91%)] Loss: 8053.460938\n",
      "Train Epoch: 234 [209152/225000 (93%)] Loss: 8143.806641\n",
      "Train Epoch: 234 [213248/225000 (95%)] Loss: 12528.648438\n",
      "Train Epoch: 234 [217344/225000 (97%)] Loss: 7783.537109\n",
      "Train Epoch: 234 [221440/225000 (98%)] Loss: 8160.392578\n",
      "    epoch          : 234\n",
      "    loss           : 9525.146368831769\n",
      "    val_loss       : 9379.947131142324\n",
      "Validation performance didn't improve for 75 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MolecularVaeCategoryModel(\n",
       "  (_category): FreeCategory(\n",
       "    (generator_0): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 64)\n",
       "      (recurrence2): GRUCell(64, 64)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 64)\n",
       "      (recurrence2): GRUCell(64, 64)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 128)\n",
       "      (recurrence2): GRUCell(128, 128)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 128)\n",
       "      (recurrence2): GRUCell(128, 128)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 256)\n",
       "      (recurrence2): GRUCell(256, 256)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 256)\n",
       "      (recurrence2): GRUCell(256, 256)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 64)\n",
       "      (recurrence2): GRUCell(64, 64)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 64)\n",
       "      (recurrence2): GRUCell(64, 64)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 128)\n",
       "      (recurrence2): GRUCell(128, 128)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 128)\n",
       "      (recurrence2): GRUCell(128, 128)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_10): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 256)\n",
       "      (recurrence2): GRUCell(256, 256)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_11): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 256)\n",
       "      (recurrence2): GRUCell(256, 256)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_12): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 64)\n",
       "      (recurrence2): GRUCell(64, 64)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_13): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 64)\n",
       "      (recurrence2): GRUCell(64, 64)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_14): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 128)\n",
       "      (recurrence2): GRUCell(128, 128)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_15): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 128)\n",
       "      (recurrence2): GRUCell(128, 128)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_16): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 256)\n",
       "      (recurrence2): GRUCell(256, 256)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (generator_17): MolecularDecoder(\n",
       "      (pre_recurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=34, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence1): GRUCell(34, 256)\n",
       "      (recurrence2): GRUCell(256, 256)\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=34, bias=True)\n",
       "        (1): Softmax(dim=-1)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "  )\n",
       "  (_dagger_category): FreeCategory(\n",
       "    (generator_0): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_1): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=64, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(64, 196, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_2): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_3): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=128, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(128, 196, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_4): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_5): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=256, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(256, 196, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=196, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_6): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_7): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=64, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(64, 292, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_8): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_9): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=128, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(128, 292, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_10): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_11): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=256, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(256, 292, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=292, out_features=292, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=292, out_features=292, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_12): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_13): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=64, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(64, 435, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_14): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_15): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=128, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(128, 435, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_16): ConvMolecularEncoder(\n",
       "      (smiles_conv): Sequential(\n",
       "        (0): Conv1d(120, 9, kernel_size=(9,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Conv1d(9, 9, kernel_size=(9,), stride=(1,))\n",
       "        (3): ReLU()\n",
       "        (4): Conv1d(9, 10, kernel_size=(11,), stride=(1,))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (smiles_linear): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_17): RecurrentMolecularEncoder(\n",
       "      (encoder_linear): Sequential(\n",
       "        (0): Linear(in_features=34, out_features=256, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (recurrence): GRU(256, 435, num_layers=3, batch_first=True)\n",
       "      (postrecurrence_linear): Sequential(\n",
       "        (0): Linear(in_features=435, out_features=435, bias=True)\n",
       "        (1): SELU()\n",
       "      )\n",
       "      (embedding_loc): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_log_scale): Linear(in_features=435, out_features=435, bias=True)\n",
       "      (embedding_dist): DiagonalGaussian()\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=4080, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=4080, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=42, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_xs, valid_ys = list(valid_data_loader)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, recons = model(observations=valid_xs, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8036, 0.1845, 0.1845, 0.2321, 0.0417, 0.1429, 0.0952, 0.0833, 0.1071,\n",
       "        0.0952, 0.0774, 0.0357, 0.1488, 0.0952, 0.1667, 0.1488, 0.0833, 0.1607,\n",
       "        0.1310, 0.1131, 0.0833, 0.0655, 0.1012, 0.0833, 0.1131, 0.1429, 0.1250,\n",
       "        0.1429, 0.0655, 0.0893, 0.1250, 0.1131, 0.0833, 0.0893, 0.1071, 0.1131,\n",
       "        0.1786, 0.1667, 0.1250, 0.1964, 0.2321, 0.3690, 0.3750, 0.4940, 0.5655,\n",
       "        0.6488, 0.6905, 0.7262, 0.7619, 0.8155, 0.8512, 0.8750, 0.8988, 0.9107,\n",
       "        0.9405, 0.9583, 0.9702, 0.9762, 0.9940, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(recons == valid_xs).all(dim=-1).to(dtype=torch.float).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:categorical_bpl] *",
   "language": "python",
   "name": "conda-env-categorical_bpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
