{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 33174.363281\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -543368.875000\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -622720.750000\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -572227.312500\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -700327.500000\n",
      "    epoch          : 1\n",
      "    loss           : -566848.2508121906\n",
      "    val_loss       : -682796.03671875\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -788824.937500\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -712259.687500\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -634933.625000\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -705734.875000\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -687289.625000\n",
      "    epoch          : 2\n",
      "    loss           : -695799.5897277228\n",
      "    val_loss       : -702768.77890625\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -726115.500000\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -664801.937500\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -621412.500000\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -667102.812500\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -715624.500000\n",
      "    epoch          : 3\n",
      "    loss           : -699596.6033415842\n",
      "    val_loss       : -702897.92578125\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -833028.500000\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -725016.250000\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -648279.312500\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -715482.562500\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -686656.000000\n",
      "    epoch          : 4\n",
      "    loss           : -709362.530940594\n",
      "    val_loss       : -708437.62578125\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -839882.812500\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -683430.750000\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -712300.125000\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -755220.250000\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -709717.437500\n",
      "    epoch          : 5\n",
      "    loss           : -714933.3465346535\n",
      "    val_loss       : -718632.88984375\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -740354.375000\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -770214.750000\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -673837.125000\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -724335.750000\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -724052.875000\n",
      "    epoch          : 6\n",
      "    loss           : -725231.9337871287\n",
      "    val_loss       : -729435.27734375\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -853387.625000\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -551940.687500\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -670964.062500\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -878652.625000\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -717221.875000\n",
      "    epoch          : 7\n",
      "    loss           : -739094.1522277228\n",
      "    val_loss       : -743412.02578125\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -867738.500000\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -791642.125000\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -721725.437500\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -894939.937500\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -761271.937500\n",
      "    epoch          : 8\n",
      "    loss           : -753490.4845297029\n",
      "    val_loss       : -757160.04921875\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -893938.375000\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -792097.250000\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -688687.000000\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -729718.750000\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -768010.625000\n",
      "    epoch          : 9\n",
      "    loss           : -759785.8558168317\n",
      "    val_loss       : -766408.14375\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -726462.187500\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -802886.750000\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -818152.625000\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -808008.312500\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -778676.250000\n",
      "    epoch          : 10\n",
      "    loss           : -767216.7679455446\n",
      "    val_loss       : -766838.27734375\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -923502.875000\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -810049.125000\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -722914.500000\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -729578.250000\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -783307.562500\n",
      "    epoch          : 11\n",
      "    loss           : -774749.0080445545\n",
      "    val_loss       : -779620.46796875\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -928591.687500\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -824807.125000\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -741102.687500\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -710377.000000\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -751392.187500\n",
      "    epoch          : 12\n",
      "    loss           : -783573.1058168317\n",
      "    val_loss       : -789707.8359375\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -934242.375000\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -835299.937500\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -741253.625000\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -767691.687500\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -750408.000000\n",
      "    epoch          : 13\n",
      "    loss           : -788704.5185643565\n",
      "    val_loss       : -795167.45546875\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -834113.125000\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -747358.750000\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -835285.937500\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -766883.125000\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -800873.375000\n",
      "    epoch          : 14\n",
      "    loss           : -794840.7790841584\n",
      "    val_loss       : -791185.7435668946\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -948344.750000\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -831241.250000\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -836536.000000\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -784927.875000\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -801804.750000\n",
      "    epoch          : 15\n",
      "    loss           : -806109.8663366337\n",
      "    val_loss       : -811349.88125\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -777629.875000\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -784963.187500\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -765922.125000\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -838472.250000\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -793793.687500\n",
      "    epoch          : 16\n",
      "    loss           : -810761.7004950495\n",
      "    val_loss       : -812049.6078125\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -957935.625000\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -743719.187500\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -782508.937500\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -782110.312500\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -851428.375000\n",
      "    epoch          : 17\n",
      "    loss           : -816213.4189356435\n",
      "    val_loss       : -821745.76171875\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -966429.500000\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -782307.750000\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -807669.312500\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -792551.250000\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -808329.625000\n",
      "    epoch          : 18\n",
      "    loss           : -821160.5228960396\n",
      "    val_loss       : -824715.72109375\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -803554.625000\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -873271.687500\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -859538.000000\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -791031.687500\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -782592.875000\n",
      "    epoch          : 19\n",
      "    loss           : -827399.7060643565\n",
      "    val_loss       : -831422.88671875\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -971505.625000\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -830028.500000\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -826638.875000\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -802009.500000\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -817533.875000\n",
      "    epoch          : 20\n",
      "    loss           : -832030.8527227723\n",
      "    val_loss       : -831933.7859375\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -972387.687500\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -874443.125000\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -829444.750000\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -860656.250000\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -792931.000000\n",
      "    epoch          : 21\n",
      "    loss           : -837467.5037128713\n",
      "    val_loss       : -840163.4828125\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -976808.750000\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -819218.750000\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -828295.750000\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -867843.312500\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -796384.187500\n",
      "    epoch          : 22\n",
      "    loss           : -841638.9851485149\n",
      "    val_loss       : -841138.09609375\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -972113.000000\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -890794.125000\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -872414.187500\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -834163.062500\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -800030.125000\n",
      "    epoch          : 23\n",
      "    loss           : -847773.1398514851\n",
      "    val_loss       : -848494.69765625\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -821033.812500\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -821335.625000\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -797920.625000\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -834785.875000\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -850570.500000\n",
      "    epoch          : 24\n",
      "    loss           : -852856.3316831683\n",
      "    val_loss       : -851926.5171875\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -984117.437500\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -833394.250000\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -804775.375000\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -876231.625000\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -851155.000000\n",
      "    epoch          : 25\n",
      "    loss           : -858613.5037128713\n",
      "    val_loss       : -860182.01015625\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -990733.750000\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -835336.750000\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -814022.250000\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -801284.187500\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -859177.812500\n",
      "    epoch          : 26\n",
      "    loss           : -860791.1547029703\n",
      "    val_loss       : -857589.72734375\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -901653.687500\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -832631.125000\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -847398.250000\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -874886.500000\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -860996.125000\n",
      "    epoch          : 27\n",
      "    loss           : -863555.1571782178\n",
      "    val_loss       : -867093.4140625\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -991761.625000\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -817468.500000\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -883685.750000\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -993145.750000\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -859951.750000\n",
      "    epoch          : 28\n",
      "    loss           : -869177.6732673268\n",
      "    val_loss       : -870307.4953125\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -893181.062500\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -850358.375000\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -864224.562500\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -822460.875000\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -824637.750000\n",
      "    epoch          : 29\n",
      "    loss           : -876108.1528465346\n",
      "    val_loss       : -874137.15546875\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -996479.500000\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -838653.187500\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -824458.125000\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -988910.500000\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -818073.562500\n",
      "    epoch          : 30\n",
      "    loss           : -870022.9040841584\n",
      "    val_loss       : -870193.3125\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -993845.250000\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -857786.250000\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -896722.875000\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -870223.000000\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -833696.125000\n",
      "    epoch          : 31\n",
      "    loss           : -874839.332920792\n",
      "    val_loss       : -881794.921875\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -997465.250000\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -916172.187500\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -846384.187500\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -830570.062500\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -874843.625000\n",
      "    epoch          : 32\n",
      "    loss           : -880140.7840346535\n",
      "    val_loss       : -885370.871875\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -858825.250000\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -835022.875000\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -904806.250000\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -999925.562500\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -837707.812500\n",
      "    epoch          : 33\n",
      "    loss           : -885945.0321782178\n",
      "    val_loss       : -880880.43046875\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -998971.375000\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -860345.625000\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -887538.500000\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -885629.125000\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -893121.000000\n",
      "    epoch          : 34\n",
      "    loss           : -886981.3650990099\n",
      "    val_loss       : -890464.30703125\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -1004390.125000\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -835435.750000\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -836854.750000\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -876022.125000\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -878438.250000\n",
      "    epoch          : 35\n",
      "    loss           : -886821.3997524752\n",
      "    val_loss       : -889742.678125\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -1001682.062500\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -916589.875000\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -826751.437500\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -878062.375000\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -883893.000000\n",
      "    epoch          : 36\n",
      "    loss           : -888269.4047029703\n",
      "    val_loss       : -888063.6390625\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -1000082.750000\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -928704.375000\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -852264.812500\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -843211.562500\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -847266.625000\n",
      "    epoch          : 37\n",
      "    loss           : -893171.8298267326\n",
      "    val_loss       : -896477.69140625\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -1006832.250000\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -916021.875000\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -913557.625000\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -916396.250000\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -891743.500000\n",
      "    epoch          : 38\n",
      "    loss           : -897038.3155940594\n",
      "    val_loss       : -890176.02734375\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -1005266.000000\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -869087.500000\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -913540.062500\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -879736.250000\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -878208.750000\n",
      "    epoch          : 39\n",
      "    loss           : -895073.2642326732\n",
      "    val_loss       : -894158.93046875\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -1003801.187500\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -931742.812500\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -874208.875000\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -880938.625000\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -875359.187500\n",
      "    epoch          : 40\n",
      "    loss           : -895297.6912128713\n",
      "    val_loss       : -890552.065625\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -1000221.875000\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -874547.437500\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -857737.625000\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -1009764.250000\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -895243.937500\n",
      "    epoch          : 41\n",
      "    loss           : -902544.0389851485\n",
      "    val_loss       : -901830.66328125\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -1006483.250000\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -934153.875000\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -861069.625000\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -880930.625000\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -857926.625000\n",
      "    epoch          : 42\n",
      "    loss           : -900363.0012376237\n",
      "    val_loss       : -899668.8375\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -936916.000000\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -868104.000000\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -900552.000000\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -854273.375000\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -864613.062500\n",
      "    epoch          : 43\n",
      "    loss           : -902983.7196782178\n",
      "    val_loss       : -905470.03203125\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -939197.875000\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -865505.875000\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -881731.750000\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -863609.875000\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -862342.375000\n",
      "    epoch          : 44\n",
      "    loss           : -907073.0445544554\n",
      "    val_loss       : -906431.915625\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -1005847.312500\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -880946.687500\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -894763.250000\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -889077.875000\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -859011.250000\n",
      "    epoch          : 45\n",
      "    loss           : -908998.8372524752\n",
      "    val_loss       : -908284.97578125\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -1011921.000000\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -926123.562500\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -902179.500000\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -898949.625000\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -913106.375000\n",
      "    epoch          : 46\n",
      "    loss           : -910288.5136138614\n",
      "    val_loss       : -906996.16484375\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -1010301.125000\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -882488.875000\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -925350.000000\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -902882.750000\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -901787.687500\n",
      "    epoch          : 47\n",
      "    loss           : -910298.5538366337\n",
      "    val_loss       : -911013.48984375\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -1010333.625000\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -944768.125000\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -888409.312500\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -874390.500000\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -911850.062500\n",
      "    epoch          : 48\n",
      "    loss           : -911099.073019802\n",
      "    val_loss       : -911094.36953125\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -1012730.875000\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -894036.562500\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -929294.125000\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -904880.187500\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -892510.187500\n",
      "    epoch          : 49\n",
      "    loss           : -915156.042079208\n",
      "    val_loss       : -911759.83203125\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -1010083.687500\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -948049.500000\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -899584.250000\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -898723.375000\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -903905.187500\n",
      "    epoch          : 50\n",
      "    loss           : -913784.7252475248\n",
      "    val_loss       : -906984.71953125\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0824_234706/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -1010099.000000\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -928133.312500\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -863282.125000\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -901314.125000\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -896730.125000\n",
      "    epoch          : 51\n",
      "    loss           : -908644.2054455446\n",
      "    val_loss       : -910022.9953125\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -1013700.625000\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -896206.312500\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -933958.125000\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -922595.375000\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -877305.375000\n",
      "    epoch          : 52\n",
      "    loss           : -917069.3081683168\n",
      "    val_loss       : -918830.4015625\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -1016380.875000\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -900268.125000\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -933127.375000\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -1013714.937500\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -909613.562500\n",
      "    epoch          : 53\n",
      "    loss           : -917729.1367574257\n",
      "    val_loss       : -914836.03046875\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -947012.562500\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -871988.750000\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -929953.312500\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -895273.375000\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -918116.875000\n",
      "    epoch          : 54\n",
      "    loss           : -913404.3471534654\n",
      "    val_loss       : -917745.375\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -1015348.375000\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -896428.625000\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -895924.125000\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -901517.250000\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -917301.250000\n",
      "    epoch          : 55\n",
      "    loss           : -919188.0266089109\n",
      "    val_loss       : -918926.98359375\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -1015547.625000\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -949901.062500\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -900889.312500\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -919766.625000\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -877450.750000\n",
      "    epoch          : 56\n",
      "    loss           : -919496.8323019802\n",
      "    val_loss       : -918662.2875\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -1012973.625000\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -916599.250000\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -918998.500000\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -931938.625000\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -918977.000000\n",
      "    epoch          : 57\n",
      "    loss           : -918356.5655940594\n",
      "    val_loss       : -915483.7421875\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -1013684.750000\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -907976.875000\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -880037.875000\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -1016345.187500\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -921322.687500\n",
      "    epoch          : 58\n",
      "    loss           : -918836.1206683168\n",
      "    val_loss       : -918467.0578125\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -1013715.625000\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -901636.750000\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -924342.000000\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -877093.000000\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -917028.750000\n",
      "    epoch          : 59\n",
      "    loss           : -922779.9535891089\n",
      "    val_loss       : -919781.93203125\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -1013993.125000\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -896825.125000\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -883645.000000\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -878590.250000\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -875393.000000\n",
      "    epoch          : 60\n",
      "    loss           : -921696.8558168317\n",
      "    val_loss       : -919243.0109375\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -1016290.000000\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -953137.000000\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -883997.000000\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -886475.125000\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -921369.000000\n",
      "    epoch          : 61\n",
      "    loss           : -921144.7988861386\n",
      "    val_loss       : -920229.8078125\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -1015805.187500\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -945783.375000\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -928891.937500\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -888510.937500\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -914301.000000\n",
      "    epoch          : 62\n",
      "    loss           : -920999.9764851485\n",
      "    val_loss       : -920003.6453125\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -1015770.062500\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -904627.625000\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -938150.812500\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -923879.625000\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -913473.500000\n",
      "    epoch          : 63\n",
      "    loss           : -922190.9071782178\n",
      "    val_loss       : -921944.3109375\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -1018787.500000\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -903661.000000\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -901973.750000\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -908638.812500\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -927331.062500\n",
      "    epoch          : 64\n",
      "    loss           : -925798.6089108911\n",
      "    val_loss       : -924153.171875\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -908876.000000\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -907562.625000\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -886805.125000\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -935317.875000\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -923108.000000\n",
      "    epoch          : 65\n",
      "    loss           : -921530.2883663366\n",
      "    val_loss       : -919917.7625\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -1015972.000000\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -900822.000000\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -916637.625000\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -1016787.375000\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -919656.250000\n",
      "    epoch          : 66\n",
      "    loss           : -923441.3527227723\n",
      "    val_loss       : -919892.43828125\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -889498.500000\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -946310.875000\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -883562.812500\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -887866.250000\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -929173.250000\n",
      "    epoch          : 67\n",
      "    loss           : -925387.8966584158\n",
      "    val_loss       : -924306.99765625\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -1017017.250000\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -956532.312500\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -917403.250000\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -1018017.500000\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -918113.375000\n",
      "    epoch          : 68\n",
      "    loss           : -926670.6763613861\n",
      "    val_loss       : -924855.3265625\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -905634.000000\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -905434.375000\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -925827.875000\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -925966.812500\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -921783.062500\n",
      "    epoch          : 69\n",
      "    loss           : -923678.8991336634\n",
      "    val_loss       : -922700.01796875\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -1015336.500000\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -903191.125000\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -883454.875000\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -881186.562500\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -928814.625000\n",
      "    epoch          : 70\n",
      "    loss           : -924075.0891089109\n",
      "    val_loss       : -923630.71171875\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -1015705.937500\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -911837.812500\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -892599.500000\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -886699.625000\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -909522.312500\n",
      "    epoch          : 71\n",
      "    loss           : -928210.8007425743\n",
      "    val_loss       : -928064.30390625\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -1020299.937500\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -913152.750000\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -912196.625000\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -938703.750000\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -917125.312500\n",
      "    epoch          : 72\n",
      "    loss           : -929371.2667079208\n",
      "    val_loss       : -922305.26875\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -1018567.812500\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -904159.500000\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -895979.937500\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -895331.375000\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -891154.625000\n",
      "    epoch          : 73\n",
      "    loss           : -928422.0043316832\n",
      "    val_loss       : -926067.946875\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -1017752.125000\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -950959.375000\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -918988.812500\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -1014616.000000\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -928390.250000\n",
      "    epoch          : 74\n",
      "    loss           : -923709.0377475248\n",
      "    val_loss       : -925667.61484375\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -955178.687500\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -939725.625000\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -932728.187500\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -912118.250000\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -894562.437500\n",
      "    epoch          : 75\n",
      "    loss           : -929903.4449257426\n",
      "    val_loss       : -928983.6734375\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -1019114.750000\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -894461.750000\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -887668.375000\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -888536.375000\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -921226.312500\n",
      "    epoch          : 76\n",
      "    loss           : -928314.4393564357\n",
      "    val_loss       : -926643.740625\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -1019583.500000\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -911697.500000\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -896323.375000\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -909303.125000\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -891943.750000\n",
      "    epoch          : 77\n",
      "    loss           : -931038.9275990099\n",
      "    val_loss       : -930078.96640625\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -914438.062500\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -911820.625000\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -922100.375000\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -912157.687500\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -889989.500000\n",
      "    epoch          : 78\n",
      "    loss           : -931374.7475247525\n",
      "    val_loss       : -928024.43046875\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -1019087.187500\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -912415.437500\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -943200.250000\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -899499.250000\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -889162.000000\n",
      "    epoch          : 79\n",
      "    loss           : -930319.8496287129\n",
      "    val_loss       : -929647.39296875\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -1018784.312500\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -917453.125000\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -922678.875000\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -932107.875000\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -932676.250000\n",
      "    epoch          : 80\n",
      "    loss           : -930809.9474009901\n",
      "    val_loss       : -922714.71875\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -1015395.062500\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -930084.375000\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -928957.312500\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -1018957.375000\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -891099.000000\n",
      "    epoch          : 81\n",
      "    loss           : -929254.3966584158\n",
      "    val_loss       : -928637.7578125\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -1020047.375000\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -909966.062500\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -941309.125000\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -929633.875000\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -919783.375000\n",
      "    epoch          : 82\n",
      "    loss           : -931413.7629950495\n",
      "    val_loss       : -930552.78359375\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -1019224.500000\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -915832.250000\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -920184.437500\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -938632.250000\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -916298.500000\n",
      "    epoch          : 83\n",
      "    loss           : -930014.3886138614\n",
      "    val_loss       : -926817.72109375\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -1018471.250000\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -916073.125000\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -896747.375000\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -944488.312500\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -933445.750000\n",
      "    epoch          : 84\n",
      "    loss           : -932536.9022277228\n",
      "    val_loss       : -930832.74375\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -1018429.000000\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -942482.750000\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -920606.562500\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -924482.625000\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -906719.750000\n",
      "    epoch          : 85\n",
      "    loss           : -930502.5878712871\n",
      "    val_loss       : -924901.1578125\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -1018118.750000\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -955790.062500\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -909506.187500\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -938605.000000\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -933689.250000\n",
      "    epoch          : 86\n",
      "    loss           : -930663.1850247525\n",
      "    val_loss       : -927179.48984375\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -1017347.750000\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -896264.375000\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -945257.000000\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -907524.625000\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -938671.625000\n",
      "    epoch          : 87\n",
      "    loss           : -929964.8632425743\n",
      "    val_loss       : -929025.84609375\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -1018724.375000\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -913738.437500\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -948482.062500\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -912993.500000\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -897003.687500\n",
      "    epoch          : 88\n",
      "    loss           : -933848.9814356435\n",
      "    val_loss       : -932242.01640625\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -1020857.437500\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -960848.375000\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -934564.625000\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -937273.000000\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -889424.500000\n",
      "    epoch          : 89\n",
      "    loss           : -933350.8595297029\n",
      "    val_loss       : -932686.42265625\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -1019833.625000\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -960074.125000\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -946069.687500\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -904220.562500\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -928474.875000\n",
      "    epoch          : 90\n",
      "    loss           : -936384.2042079208\n",
      "    val_loss       : -934038.321875\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -1021495.500000\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -917828.375000\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -904699.875000\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -896792.250000\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -927312.375000\n",
      "    epoch          : 91\n",
      "    loss           : -936536.75\n",
      "    val_loss       : -934572.82109375\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -1019684.187500\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -899385.062500\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -926680.750000\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -1018344.937500\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -930693.562500\n",
      "    epoch          : 92\n",
      "    loss           : -933067.8972772277\n",
      "    val_loss       : -926994.64765625\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -1018984.125000\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -910592.937500\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -903280.375000\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -1019574.750000\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -930443.937500\n",
      "    epoch          : 93\n",
      "    loss           : -934826.9573019802\n",
      "    val_loss       : -928310.88671875\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -1016978.000000\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -917413.062500\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -896811.875000\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -928389.312500\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -926331.250000\n",
      "    epoch          : 94\n",
      "    loss           : -930118.9758663366\n",
      "    val_loss       : -933139.05859375\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -1021134.375000\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -960887.562500\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -947078.062500\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -939305.437500\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -923579.625000\n",
      "    epoch          : 95\n",
      "    loss           : -935361.4009900991\n",
      "    val_loss       : -930632.2078125\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -1018252.125000\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -945406.500000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -918487.937500\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -890902.625000\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -934242.625000\n",
      "    epoch          : 96\n",
      "    loss           : -930872.4987623763\n",
      "    val_loss       : -926588.55859375\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -1016092.812500\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -909024.625000\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -947423.875000\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -931978.500000\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -941289.187500\n",
      "    epoch          : 97\n",
      "    loss           : -934445.3632425743\n",
      "    val_loss       : -934456.73359375\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -918225.000000\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -960522.375000\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -907644.187500\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -1020237.000000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -897224.750000\n",
      "    epoch          : 98\n",
      "    loss           : -937518.4467821782\n",
      "    val_loss       : -935052.05625\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -959886.250000\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -912525.125000\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -915471.000000\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -947965.375000\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -930711.625000\n",
      "    epoch          : 99\n",
      "    loss           : -937245.7438118812\n",
      "    val_loss       : -934881.07109375\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -1021618.250000\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -963595.937500\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -930188.625000\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -928264.000000\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -941261.625000\n",
      "    epoch          : 100\n",
      "    loss           : -937938.8477722772\n",
      "    val_loss       : -935175.61796875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0824_234706/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -918724.812500\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -921845.375000\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -926448.437500\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -948610.250000\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -936421.000000\n",
      "    epoch          : 101\n",
      "    loss           : -935893.4214108911\n",
      "    val_loss       : -930408.68046875\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -962609.125000\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -918404.000000\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -927187.125000\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -949953.312500\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -937339.250000\n",
      "    epoch          : 102\n",
      "    loss           : -935815.8273514851\n",
      "    val_loss       : -931260.4375\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -1018307.187500\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -916436.250000\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -949328.750000\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -947650.375000\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -928762.062500\n",
      "    epoch          : 103\n",
      "    loss           : -936770.9486386139\n",
      "    val_loss       : -936196.92734375\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -1019526.500000\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -919736.812500\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -917499.687500\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -940142.125000\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -925818.812500\n",
      "    epoch          : 104\n",
      "    loss           : -937632.8688118812\n",
      "    val_loss       : -932997.23828125\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -1019717.812500\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -920733.812500\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -931639.750000\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -1019986.812500\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -901849.250000\n",
      "    epoch          : 105\n",
      "    loss           : -935625.6806930694\n",
      "    val_loss       : -936894.36640625\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -1022323.750000\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -924934.250000\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -949411.375000\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -1023013.687500\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -905866.000000\n",
      "    epoch          : 106\n",
      "    loss           : -940264.0637376237\n",
      "    val_loss       : -936851.1515625\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -1022714.000000\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -921199.312500\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -914991.687500\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -906116.250000\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -941236.187500\n",
      "    epoch          : 107\n",
      "    loss           : -937208.531559406\n",
      "    val_loss       : -934332.96171875\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -1019988.625000\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -906813.500000\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -925630.750000\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -916074.937500\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -919693.437500\n",
      "    epoch          : 108\n",
      "    loss           : -938735.6844059406\n",
      "    val_loss       : -937791.93828125\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -965938.625000\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -924676.562500\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -952304.312500\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -925016.250000\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -940784.062500\n",
      "    epoch          : 109\n",
      "    loss           : -939284.5142326732\n",
      "    val_loss       : -937007.521875\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -1020505.250000\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -922129.937500\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -890908.625000\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -912409.375000\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -941515.250000\n",
      "    epoch          : 110\n",
      "    loss           : -936370.2530940594\n",
      "    val_loss       : -937045.4109375\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -1020157.750000\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -906365.875000\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -910567.187500\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -905992.312500\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -907742.125000\n",
      "    epoch          : 111\n",
      "    loss           : -940149.4913366337\n",
      "    val_loss       : -937573.25546875\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -1022069.875000\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -907812.875000\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -933686.875000\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -1021076.125000\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -918769.375000\n",
      "    epoch          : 112\n",
      "    loss           : -940408.8155940594\n",
      "    val_loss       : -937164.16484375\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -1021653.187500\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -966231.062500\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -920016.937500\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -920920.875000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -907732.625000\n",
      "    epoch          : 113\n",
      "    loss           : -941309.3756188119\n",
      "    val_loss       : -938391.3515625\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -1021610.812500\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -968057.375000\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -929708.500000\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -1019059.625000\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -916808.500000\n",
      "    epoch          : 114\n",
      "    loss           : -938746.353960396\n",
      "    val_loss       : -936092.390625\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -905224.750000\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -959580.875000\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -946271.750000\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -1018086.125000\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -902298.812500\n",
      "    epoch          : 115\n",
      "    loss           : -937059.9003712871\n",
      "    val_loss       : -937227.38828125\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -919492.000000\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -923901.437500\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -928476.750000\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -912860.062500\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -895649.375000\n",
      "    epoch          : 116\n",
      "    loss           : -939822.6547029703\n",
      "    val_loss       : -936661.30546875\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -1019806.250000\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -950242.625000\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -901579.125000\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -912231.250000\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -943314.312500\n",
      "    epoch          : 117\n",
      "    loss           : -936205.2376237623\n",
      "    val_loss       : -937419.19609375\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -1020592.500000\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -911009.875000\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -905893.062500\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -1021678.437500\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -933412.000000\n",
      "    epoch          : 118\n",
      "    loss           : -940210.1318069306\n",
      "    val_loss       : -939078.96171875\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -1022285.687500\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -952318.750000\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -910006.062500\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -916388.062500\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -934231.625000\n",
      "    epoch          : 119\n",
      "    loss           : -941915.1974009901\n",
      "    val_loss       : -938868.00390625\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -1022850.812500\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -950433.875000\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -910381.312500\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -1019186.125000\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -943773.625000\n",
      "    epoch          : 120\n",
      "    loss           : -940662.4969059406\n",
      "    val_loss       : -936683.18515625\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -1018776.125000\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -922803.250000\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -906890.187500\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -909478.062500\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -906412.000000\n",
      "    epoch          : 121\n",
      "    loss           : -941612.9641089109\n",
      "    val_loss       : -940127.9109375\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -1022377.125000\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -930504.875000\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -952547.750000\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -921850.500000\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -945954.750000\n",
      "    epoch          : 122\n",
      "    loss           : -942996.1844059406\n",
      "    val_loss       : -939867.82265625\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -1022860.250000\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -967111.500000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -907858.937500\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -929288.812500\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -947821.750000\n",
      "    epoch          : 123\n",
      "    loss           : -941808.3650990099\n",
      "    val_loss       : -937439.26875\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -1022014.000000\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -925080.250000\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -953866.812500\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -951072.375000\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -904428.375000\n",
      "    epoch          : 124\n",
      "    loss           : -941395.4876237623\n",
      "    val_loss       : -936436.1234375\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -967206.875000\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -918088.937500\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -902979.500000\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -941664.750000\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -928725.437500\n",
      "    epoch          : 125\n",
      "    loss           : -937502.875\n",
      "    val_loss       : -933684.09765625\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -1019718.125000\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -908476.187500\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -919039.000000\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -908961.000000\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -933187.750000\n",
      "    epoch          : 126\n",
      "    loss           : -940713.8353960396\n",
      "    val_loss       : -939020.4828125\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -1022864.250000\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -908056.375000\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -908198.375000\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -908006.812500\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -934124.875000\n",
      "    epoch          : 127\n",
      "    loss           : -942916.5915841584\n",
      "    val_loss       : -940804.53828125\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -1023035.187500\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -965154.375000\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -909284.875000\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -944483.625000\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -943198.500000\n",
      "    epoch          : 128\n",
      "    loss           : -939701.625\n",
      "    val_loss       : -938156.771875\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -923853.125000\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -966988.125000\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -935843.687500\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -931625.875000\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -945852.875000\n",
      "    epoch          : 129\n",
      "    loss           : -941869.7252475248\n",
      "    val_loss       : -939333.9546875\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -945379.875000\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -967170.250000\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -949766.312500\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -914269.375000\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -905898.875000\n",
      "    epoch          : 130\n",
      "    loss           : -941134.1287128713\n",
      "    val_loss       : -939321.94921875\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -1021111.250000\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -927855.375000\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -933698.375000\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -942740.062500\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -932830.750000\n",
      "    epoch          : 131\n",
      "    loss           : -942521.3242574257\n",
      "    val_loss       : -939752.6046875\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -1023200.625000\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -967100.937500\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -954718.125000\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -918119.000000\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -935569.250000\n",
      "    epoch          : 132\n",
      "    loss           : -942356.760519802\n",
      "    val_loss       : -939916.64296875\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -1021971.375000\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -968812.500000\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -956035.500000\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -922209.500000\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -935816.125000\n",
      "    epoch          : 133\n",
      "    loss           : -942526.5699257426\n",
      "    val_loss       : -940896.51171875\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -1021013.500000\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -954371.687500\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -922150.937500\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -1021742.437500\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -935663.062500\n",
      "    epoch          : 134\n",
      "    loss           : -942762.8985148515\n",
      "    val_loss       : -940558.3640625\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -1022524.125000\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -927990.062500\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -931531.562500\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -918512.625000\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -946340.562500\n",
      "    epoch          : 135\n",
      "    loss           : -943750.7029702971\n",
      "    val_loss       : -940859.28046875\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -1022804.375000\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -928219.375000\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -953471.000000\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -922579.062500\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -943883.812500\n",
      "    epoch          : 136\n",
      "    loss           : -942190.9108910891\n",
      "    val_loss       : -935427.81875\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -968551.687500\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -907410.000000\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -945493.812500\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -952363.625000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -906796.062500\n",
      "    epoch          : 137\n",
      "    loss           : -941468.3125\n",
      "    val_loss       : -940343.24609375\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -1022407.812500\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -907488.500000\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -908441.937500\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -918168.125000\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -909333.187500\n",
      "    epoch          : 138\n",
      "    loss           : -942939.1126237623\n",
      "    val_loss       : -940326.8640625\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -1022309.687500\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -931588.875000\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -933601.562500\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -945427.875000\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -905638.125000\n",
      "    epoch          : 139\n",
      "    loss           : -941381.1435643565\n",
      "    val_loss       : -937008.5390625\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -1021781.000000\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -923615.562500\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -915573.625000\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -948705.000000\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -906130.750000\n",
      "    epoch          : 140\n",
      "    loss           : -941538.1676980198\n",
      "    val_loss       : -940314.21328125\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -1022501.937500\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -934935.375000\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -955412.000000\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -1022764.125000\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -949102.062500\n",
      "    epoch          : 141\n",
      "    loss           : -944583.3248762377\n",
      "    val_loss       : -941814.5984375\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -1024377.125000\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -952760.812500\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -916931.187500\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -922557.500000\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -948628.187500\n",
      "    epoch          : 142\n",
      "    loss           : -944663.3298267326\n",
      "    val_loss       : -941532.95390625\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -1024681.375000\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -929414.375000\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -954237.937500\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -934180.937500\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -914084.375000\n",
      "    epoch          : 143\n",
      "    loss           : -943963.4616336634\n",
      "    val_loss       : -940760.4234375\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -1021516.437500\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -947307.750000\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -909369.000000\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -936126.000000\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -921867.125000\n",
      "    epoch          : 144\n",
      "    loss           : -944582.6157178218\n",
      "    val_loss       : -941898.41015625\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -1022725.625000\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -970684.750000\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -947022.500000\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -918875.875000\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -946306.187500\n",
      "    epoch          : 145\n",
      "    loss           : -944784.405940594\n",
      "    val_loss       : -942256.834375\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -916189.187500\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -917022.875000\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -922133.125000\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -949485.375000\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -909509.750000\n",
      "    epoch          : 146\n",
      "    loss           : -945327.6528465346\n",
      "    val_loss       : -940768.19609375\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -924125.625000\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -967038.062500\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -910257.875000\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -951035.125000\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -932388.250000\n",
      "    epoch          : 147\n",
      "    loss           : -940910.9969059406\n",
      "    val_loss       : -939375.33203125\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -1021348.562500\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -929040.750000\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -945813.750000\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -955033.875000\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -935085.000000\n",
      "    epoch          : 148\n",
      "    loss           : -944287.6757425743\n",
      "    val_loss       : -941055.6828125\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -1022908.187500\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -921411.000000\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -920070.875000\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -1022897.812500\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -913663.250000\n",
      "    epoch          : 149\n",
      "    loss           : -945582.8533415842\n",
      "    val_loss       : -942968.41484375\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -1023189.437500\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -970936.812500\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -948628.875000\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -952512.500000\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -935349.750000\n",
      "    epoch          : 150\n",
      "    loss           : -945563.1101485149\n",
      "    val_loss       : -941458.8921875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0824_234706/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -1022437.437500\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -909020.062500\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -908620.375000\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -912668.125000\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -916139.187500\n",
      "    epoch          : 151\n",
      "    loss           : -940312.3941831683\n",
      "    val_loss       : -937401.0625\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -1022196.687500\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -912096.125000\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -919347.687500\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -1022380.125000\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -932562.812500\n",
      "    epoch          : 152\n",
      "    loss           : -943110.6868811881\n",
      "    val_loss       : -941906.5078125\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -1023350.750000\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -922972.000000\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -914221.312500\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -947076.250000\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -933809.750000\n",
      "    epoch          : 153\n",
      "    loss           : -944490.5915841584\n",
      "    val_loss       : -941462.89765625\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -1022560.937500\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -908429.000000\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -921706.187500\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -947805.875000\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -948198.437500\n",
      "    epoch          : 154\n",
      "    loss           : -944709.3570544554\n",
      "    val_loss       : -942401.71640625\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -970392.875000\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -918034.562500\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -955070.812500\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -913157.000000\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -945807.562500\n",
      "    epoch          : 155\n",
      "    loss           : -944898.4820544554\n",
      "    val_loss       : -940081.5359375\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -924586.250000\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -968864.125000\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -920814.312500\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -912030.562500\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -945464.000000\n",
      "    epoch          : 156\n",
      "    loss           : -943133.9715346535\n",
      "    val_loss       : -940884.70234375\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -1021764.062500\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -926432.875000\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -954325.937500\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -912611.250000\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -950519.625000\n",
      "    epoch          : 157\n",
      "    loss           : -944730.3279702971\n",
      "    val_loss       : -943140.8265625\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -1023468.750000\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -931816.000000\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -956237.875000\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -952625.562500\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -948669.375000\n",
      "    epoch          : 158\n",
      "    loss           : -946128.4381188119\n",
      "    val_loss       : -942916.5875\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -1022769.750000\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -971660.562500\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -956337.187500\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -924247.250000\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -949315.875000\n",
      "    epoch          : 159\n",
      "    loss           : -945747.843440594\n",
      "    val_loss       : -942508.25390625\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -972312.687500\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -931137.875000\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -948069.937500\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -1023915.312500\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -949720.250000\n",
      "    epoch          : 160\n",
      "    loss           : -945449.0705445545\n",
      "    val_loss       : -941710.109375\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -1022008.250000\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -920978.312500\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -916043.375000\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -1021111.000000\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -948517.750000\n",
      "    epoch          : 161\n",
      "    loss           : -943157.6101485149\n",
      "    val_loss       : -939571.3015625\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -955429.000000\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -966672.562500\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -911548.687500\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -954402.875000\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -908803.500000\n",
      "    epoch          : 162\n",
      "    loss           : -944929.2951732674\n",
      "    val_loss       : -941916.82734375\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -969070.125000\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -912297.625000\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -909601.000000\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -1022352.000000\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -906491.375000\n",
      "    epoch          : 163\n",
      "    loss           : -942948.8415841584\n",
      "    val_loss       : -941083.0125\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -925898.875000\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -967742.875000\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -917716.812500\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -937158.062500\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -910754.250000\n",
      "    epoch          : 164\n",
      "    loss           : -944317.5996287129\n",
      "    val_loss       : -940722.0828125\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -1021880.875000\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -966779.625000\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -918292.625000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -957799.500000\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -938619.437500\n",
      "    epoch          : 165\n",
      "    loss           : -944628.1219059406\n",
      "    val_loss       : -943567.1109375\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -1022757.312500\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -928720.750000\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -913380.437500\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -920361.875000\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -913419.375000\n",
      "    epoch          : 166\n",
      "    loss           : -946570.8316831683\n",
      "    val_loss       : -942127.87890625\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -1023627.875000\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -923199.125000\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -937221.312500\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -951637.125000\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -945819.000000\n",
      "    epoch          : 167\n",
      "    loss           : -943934.916460396\n",
      "    val_loss       : -941325.71015625\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -1022478.125000\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -929537.250000\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -920620.625000\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -921442.125000\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -954771.812500\n",
      "    epoch          : 168\n",
      "    loss           : -946205.5556930694\n",
      "    val_loss       : -942717.7359375\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -969869.562500\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -969559.250000\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -911440.500000\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -1023680.750000\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -936890.750000\n",
      "    epoch          : 169\n",
      "    loss           : -946664.957920792\n",
      "    val_loss       : -943051.83125\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -970143.687500\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -932029.375000\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -955539.125000\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -1021426.562500\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -911661.750000\n",
      "    epoch          : 170\n",
      "    loss           : -944321.2543316832\n",
      "    val_loss       : -943433.6234375\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -1022921.625000\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -969110.875000\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -920325.500000\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -1022397.625000\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -950543.562500\n",
      "    epoch          : 171\n",
      "    loss           : -946382.656559406\n",
      "    val_loss       : -943725.421875\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -1024504.875000\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -914058.750000\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -922208.500000\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -907236.875000\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -908815.625000\n",
      "    epoch          : 172\n",
      "    loss           : -943682.6658415842\n",
      "    val_loss       : -941442.428125\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -1023142.687500\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -965138.062500\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -911307.875000\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -1022137.000000\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -949739.812500\n",
      "    epoch          : 173\n",
      "    loss           : -944507.0371287129\n",
      "    val_loss       : -943409.29765625\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -1022374.812500\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -923534.000000\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -937427.625000\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -952190.812500\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -941467.125000\n",
      "    epoch          : 174\n",
      "    loss           : -946315.1138613861\n",
      "    val_loss       : -940705.91953125\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -1023032.437500\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -916126.000000\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -947039.625000\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -1022068.000000\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -910252.750000\n",
      "    epoch          : 175\n",
      "    loss           : -944858.2351485149\n",
      "    val_loss       : -943707.303125\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -1023367.687500\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -929236.500000\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -923657.625000\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -936770.625000\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -913945.875000\n",
      "    epoch          : 176\n",
      "    loss           : -947050.1676980198\n",
      "    val_loss       : -941260.65078125\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -1023535.875000\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -917177.937500\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -952368.125000\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -959245.437500\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -948516.000000\n",
      "    epoch          : 177\n",
      "    loss           : -945643.1126237623\n",
      "    val_loss       : -943407.38515625\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -1023485.937500\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -932673.000000\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -921002.187500\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -923574.937500\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -939150.000000\n",
      "    epoch          : 178\n",
      "    loss           : -947553.2772277228\n",
      "    val_loss       : -944564.33828125\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -1024255.625000\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -968996.625000\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -938228.375000\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -915511.625000\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -917128.562500\n",
      "    epoch          : 179\n",
      "    loss           : -947565.2964108911\n",
      "    val_loss       : -943811.46171875\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -1022948.750000\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -920833.250000\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -949382.250000\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -956774.062500\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -936978.250000\n",
      "    epoch          : 180\n",
      "    loss           : -947358.8521039604\n",
      "    val_loss       : -943756.5765625\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -1022874.750000\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -931361.187500\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -922034.875000\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -1023939.500000\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -936958.125000\n",
      "    epoch          : 181\n",
      "    loss           : -947489.5705445545\n",
      "    val_loss       : -943403.96953125\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -1023198.875000\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -920326.750000\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -923016.187500\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -946987.625000\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -949193.187500\n",
      "    epoch          : 182\n",
      "    loss           : -946958.7896039604\n",
      "    val_loss       : -942450.53671875\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -1024020.000000\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -929576.562500\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -917787.375000\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -936412.375000\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -948107.250000\n",
      "    epoch          : 183\n",
      "    loss           : -943201.6571782178\n",
      "    val_loss       : -939828.115625\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -1022641.562500\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -966642.562500\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -913541.750000\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -924474.187500\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -948163.812500\n",
      "    epoch          : 184\n",
      "    loss           : -946792.5160891089\n",
      "    val_loss       : -944710.42265625\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -1023356.687500\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -930277.125000\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -924184.750000\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -1022567.500000\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -938777.312500\n",
      "    epoch          : 185\n",
      "    loss           : -947479.6930693069\n",
      "    val_loss       : -944035.5140625\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -1022869.625000\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -970701.812500\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -938844.937500\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -924126.125000\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -915899.250000\n",
      "    epoch          : 186\n",
      "    loss           : -947666.4263613861\n",
      "    val_loss       : -943523.14609375\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -1022560.875000\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -969247.250000\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -920489.562500\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -923958.500000\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -953265.750000\n",
      "    epoch          : 187\n",
      "    loss           : -947221.3601485149\n",
      "    val_loss       : -944430.271875\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -1024197.875000\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -922473.125000\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -923657.750000\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -936762.750000\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -950590.375000\n",
      "    epoch          : 188\n",
      "    loss           : -947004.9139851485\n",
      "    val_loss       : -942321.36328125\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -1023560.125000\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -928857.500000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -923957.812500\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -939653.687500\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -950205.812500\n",
      "    epoch          : 189\n",
      "    loss           : -947544.1850247525\n",
      "    val_loss       : -944991.93515625\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -1024234.625000\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -932929.375000\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -924290.312500\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -925419.875000\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -918276.312500\n",
      "    epoch          : 190\n",
      "    loss           : -948767.7543316832\n",
      "    val_loss       : -945258.51796875\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -971870.625000\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -969652.000000\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -923177.625000\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -925528.500000\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -952924.625000\n",
      "    epoch          : 191\n",
      "    loss           : -949035.6169554455\n",
      "    val_loss       : -945118.378125\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -931444.250000\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -972893.000000\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -918616.875000\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -925360.750000\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -951762.125000\n",
      "    epoch          : 192\n",
      "    loss           : -948520.9090346535\n",
      "    val_loss       : -943751.165625\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -1022732.625000\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -933461.687500\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -939156.062500\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -1022069.250000\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -922099.687500\n",
      "    epoch          : 193\n",
      "    loss           : -947474.0266089109\n",
      "    val_loss       : -943409.45234375\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -971846.750000\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -951423.062500\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -957233.750000\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -917691.875000\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -950869.812500\n",
      "    epoch          : 194\n",
      "    loss           : -946374.4653465346\n",
      "    val_loss       : -944268.49765625\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -1023740.812500\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -973074.500000\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -922766.500000\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -936236.375000\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -941912.625000\n",
      "    epoch          : 195\n",
      "    loss           : -947660.343440594\n",
      "    val_loss       : -942821.4859375\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -1022706.625000\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -933472.250000\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -922640.625000\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -1023502.312500\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -920835.937500\n",
      "    epoch          : 196\n",
      "    loss           : -945497.1299504951\n",
      "    val_loss       : -941996.43984375\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -1020337.687500\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -928204.562500\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -923072.937500\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -916970.812500\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -937358.437500\n",
      "    epoch          : 197\n",
      "    loss           : -946667.0792079208\n",
      "    val_loss       : -944981.00859375\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -1024149.687500\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -933533.625000\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -915970.437500\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -916666.250000\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -950659.625000\n",
      "    epoch          : 198\n",
      "    loss           : -948373.8650990099\n",
      "    val_loss       : -943955.00390625\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -1024864.000000\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -931100.625000\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -919612.375000\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -920195.000000\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -938992.312500\n",
      "    epoch          : 199\n",
      "    loss           : -947661.7648514851\n",
      "    val_loss       : -944335.9546875\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -1023970.625000\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -933067.187500\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -952339.625000\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -949949.875000\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -917078.812500\n",
      "    epoch          : 200\n",
      "    loss           : -948601.7035891089\n",
      "    val_loss       : -945126.66796875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0824_234706/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -1024362.750000\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -972024.750000\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -937422.812500\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -927918.250000\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -939320.187500\n",
      "    epoch          : 201\n",
      "    loss           : -948983.8663366337\n",
      "    val_loss       : -944194.725\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -1021996.937500\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -922786.750000\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -957076.875000\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -925640.937500\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -953065.500000\n",
      "    epoch          : 202\n",
      "    loss           : -948328.5544554455\n",
      "    val_loss       : -944843.6671875\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -1023739.687500\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -970188.000000\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -956214.750000\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -912269.687500\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -953406.687500\n",
      "    epoch          : 203\n",
      "    loss           : -948039.0191831683\n",
      "    val_loss       : -944910.3796875\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -1024962.250000\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -933575.812500\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -957597.250000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -949854.500000\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -938532.875000\n",
      "    epoch          : 204\n",
      "    loss           : -948437.781559406\n",
      "    val_loss       : -943290.49375\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -1024020.875000\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -934451.875000\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -938254.500000\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -926430.375000\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -919101.000000\n",
      "    epoch          : 205\n",
      "    loss           : -948850.780940594\n",
      "    val_loss       : -945795.70859375\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -1024670.562500\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -927877.562500\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -925305.500000\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -961098.125000\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -917999.125000\n",
      "    epoch          : 206\n",
      "    loss           : -949580.4337871287\n",
      "    val_loss       : -944876.72265625\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -1024816.437500\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -932913.687500\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -953656.875000\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -922285.437500\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -950193.000000\n",
      "    epoch          : 207\n",
      "    loss           : -946343.6602722772\n",
      "    val_loss       : -943942.02421875\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -1023536.375000\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -931756.125000\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -955412.375000\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -920725.000000\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -916883.437500\n",
      "    epoch          : 208\n",
      "    loss           : -947116.1318069306\n",
      "    val_loss       : -944884.3953125\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -972618.875000\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -930125.187500\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -956285.625000\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -1024822.750000\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -938993.000000\n",
      "    epoch          : 209\n",
      "    loss           : -948395.1689356435\n",
      "    val_loss       : -944618.25625\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -1024286.250000\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -970408.562500\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -922491.875000\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -924526.250000\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -951416.312500\n",
      "    epoch          : 210\n",
      "    loss           : -948229.3849009901\n",
      "    val_loss       : -943029.1734375\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -1022968.625000\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -935704.625000\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -939747.437500\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -959135.500000\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -917559.562500\n",
      "    epoch          : 211\n",
      "    loss           : -948534.6311881188\n",
      "    val_loss       : -946115.47265625\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -1023725.250000\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -974075.750000\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -959832.375000\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -927365.875000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -916195.062500\n",
      "    epoch          : 212\n",
      "    loss           : -949587.9696782178\n",
      "    val_loss       : -945492.36796875\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -1023794.500000\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -933077.250000\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -954739.250000\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -915141.812500\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -916087.062500\n",
      "    epoch          : 213\n",
      "    loss           : -948960.7097772277\n",
      "    val_loss       : -943819.93984375\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -1023203.312500\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -933569.937500\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -958362.875000\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -919082.312500\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -917678.562500\n",
      "    epoch          : 214\n",
      "    loss           : -948804.1732673268\n",
      "    val_loss       : -946370.39140625\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -1024172.750000\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -935093.500000\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -959590.312500\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -925981.937500\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -953946.000000\n",
      "    epoch          : 215\n",
      "    loss           : -949303.0241336634\n",
      "    val_loss       : -945372.58984375\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -969996.812500\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -931497.250000\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -957936.187500\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -1022904.125000\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -939703.750000\n",
      "    epoch          : 216\n",
      "    loss           : -948096.4449257426\n",
      "    val_loss       : -943065.4125\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -923162.625000\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -953178.250000\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -956554.000000\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -1022287.312500\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -916059.500000\n",
      "    epoch          : 217\n",
      "    loss           : -948036.6181930694\n",
      "    val_loss       : -945093.3046875\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -931344.250000\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -931979.250000\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -959110.250000\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -953021.187500\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -938529.000000\n",
      "    epoch          : 218\n",
      "    loss           : -948477.1577970297\n",
      "    val_loss       : -944442.22421875\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -970721.062500\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -919971.437500\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -922429.062500\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -955838.000000\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -939224.062500\n",
      "    epoch          : 219\n",
      "    loss           : -947340.3663366337\n",
      "    val_loss       : -942648.23203125\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -1022885.000000\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -924318.375000\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -923629.250000\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -918791.750000\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -941313.875000\n",
      "    epoch          : 220\n",
      "    loss           : -948595.0860148515\n",
      "    val_loss       : -945762.25703125\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -1023001.125000\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -951287.875000\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -957791.062500\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -935384.562500\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -917925.375000\n",
      "    epoch          : 221\n",
      "    loss           : -948894.0569306931\n",
      "    val_loss       : -945537.9125\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -1025600.937500\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -970110.687500\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -955920.562500\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -919891.437500\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -916619.000000\n",
      "    epoch          : 222\n",
      "    loss           : -948138.4783415842\n",
      "    val_loss       : -944704.8203125\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -931453.937500\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -964552.125000\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -921844.812500\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -918509.875000\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -941346.875000\n",
      "    epoch          : 223\n",
      "    loss           : -948721.8521039604\n",
      "    val_loss       : -946477.56015625\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -933443.625000\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -934339.250000\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -937392.250000\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -958023.125000\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -954787.750000\n",
      "    epoch          : 224\n",
      "    loss           : -949445.5798267326\n",
      "    val_loss       : -945177.6609375\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -1024020.375000\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -951000.000000\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -960444.875000\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -1024390.375000\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -941660.625000\n",
      "    epoch          : 225\n",
      "    loss           : -949751.0061881188\n",
      "    val_loss       : -945783.44296875\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -1025715.500000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -971929.000000\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -917791.375000\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -923954.625000\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -942661.750000\n",
      "    epoch          : 226\n",
      "    loss           : -949578.3298267326\n",
      "    val_loss       : -944449.590625\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -1022911.375000\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -939250.062500\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -925699.687500\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -944267.375000\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -938284.500000\n",
      "    epoch          : 227\n",
      "    loss           : -948195.5693069306\n",
      "    val_loss       : -943578.93828125\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -1023073.875000\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -931801.500000\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -926053.875000\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -939675.375000\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -919331.500000\n",
      "    epoch          : 228\n",
      "    loss           : -949917.9115099009\n",
      "    val_loss       : -946491.85859375\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -1023528.250000\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -934466.687500\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -925527.437500\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -939282.750000\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -917030.625000\n",
      "    epoch          : 229\n",
      "    loss           : -949562.8997524752\n",
      "    val_loss       : -944390.7703125\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -1025259.812500\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -932558.000000\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -952533.375000\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -1024108.500000\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -916933.687500\n",
      "    epoch          : 230\n",
      "    loss           : -949322.3997524752\n",
      "    val_loss       : -945960.1703125\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -1024949.125000\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -925931.625000\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -922777.187500\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -938473.250000\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -951592.000000\n",
      "    epoch          : 231\n",
      "    loss           : -948712.4511138614\n",
      "    val_loss       : -943607.33828125\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -1023334.250000\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -929796.125000\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -939661.125000\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -917726.187500\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -941636.250000\n",
      "    epoch          : 232\n",
      "    loss           : -947411.6256188119\n",
      "    val_loss       : -945525.24453125\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -1023727.500000\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -934192.812500\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -957030.187500\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -957574.000000\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -938119.125000\n",
      "    epoch          : 233\n",
      "    loss           : -949775.280940594\n",
      "    val_loss       : -944564.4609375\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -1022263.500000\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -930339.625000\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -959686.500000\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -929006.687500\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -917632.875000\n",
      "    epoch          : 234\n",
      "    loss           : -949576.7339108911\n",
      "    val_loss       : -946598.23359375\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -1024351.125000\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -935277.000000\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -958985.187500\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -953126.250000\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -953292.500000\n",
      "    epoch          : 235\n",
      "    loss           : -950276.2345297029\n",
      "    val_loss       : -946028.01953125\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -1023462.625000\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -934989.687500\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -935977.000000\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -914771.687500\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -952186.500000\n",
      "    epoch          : 236\n",
      "    loss           : -949295.6237623763\n",
      "    val_loss       : -945393.94296875\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -1022861.687500\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -958607.375000\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -925415.562500\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -953444.812500\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -949280.750000\n",
      "    epoch          : 237\n",
      "    loss           : -949052.1429455446\n",
      "    val_loss       : -942155.35078125\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -1021222.625000\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -971707.375000\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -940461.500000\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -924010.937500\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -954146.000000\n",
      "    epoch          : 238\n",
      "    loss           : -948739.0117574257\n",
      "    val_loss       : -945775.9453125\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -1023601.312500\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -935635.125000\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -959369.750000\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -954385.000000\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -954604.000000\n",
      "    epoch          : 239\n",
      "    loss           : -950377.8811881188\n",
      "    val_loss       : -946303.728125\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -1024200.062500\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -936121.562500\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -942026.750000\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -1024565.125000\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -941011.812500\n",
      "    epoch          : 240\n",
      "    loss           : -950839.9356435643\n",
      "    val_loss       : -946305.2328125\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -1024856.875000\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -934605.625000\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -926781.125000\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -941259.312500\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -940359.562500\n",
      "    epoch          : 241\n",
      "    loss           : -949723.8669554455\n",
      "    val_loss       : -945364.29140625\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -941228.125000\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -970738.000000\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -961171.250000\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -920790.125000\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -955273.625000\n",
      "    epoch          : 242\n",
      "    loss           : -950878.1138613861\n",
      "    val_loss       : -947008.43515625\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -1024529.125000\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -935731.312500\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -925788.250000\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -927855.625000\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -938283.000000\n",
      "    epoch          : 243\n",
      "    loss           : -950991.2035891089\n",
      "    val_loss       : -946570.33515625\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -1023601.000000\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -925092.500000\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -936154.937500\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -952920.687500\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -918450.500000\n",
      "    epoch          : 244\n",
      "    loss           : -950882.8873762377\n",
      "    val_loss       : -946784.94453125\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -1026085.750000\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -959656.750000\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -927261.062500\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -922183.875000\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -952650.500000\n",
      "    epoch          : 245\n",
      "    loss           : -949724.2066831683\n",
      "    val_loss       : -943139.92265625\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -1021827.750000\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -969306.000000\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -921517.375000\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -941593.500000\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -954177.500000\n",
      "    epoch          : 246\n",
      "    loss           : -949619.6280940594\n",
      "    val_loss       : -944942.8125\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -1022857.750000\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -934188.000000\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -952082.375000\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -951269.812500\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -940786.875000\n",
      "    epoch          : 247\n",
      "    loss           : -949479.3224009901\n",
      "    val_loss       : -945504.14921875\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -1024355.125000\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -926652.750000\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -926237.437500\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -953794.187500\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -936082.062500\n",
      "    epoch          : 248\n",
      "    loss           : -948702.2487623763\n",
      "    val_loss       : -942425.84296875\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -1024262.125000\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -969175.750000\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -922176.437500\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -957853.375000\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -916284.187500\n",
      "    epoch          : 249\n",
      "    loss           : -948334.1571782178\n",
      "    val_loss       : -946078.24375\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -1022539.625000\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -933617.625000\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -928156.000000\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -956748.750000\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -936042.187500\n",
      "    epoch          : 250\n",
      "    loss           : -949261.3719059406\n",
      "    val_loss       : -943892.66015625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0824_234706/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -1023687.875000\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -918576.750000\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -954972.375000\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -954148.625000\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -916713.625000\n",
      "    epoch          : 251\n",
      "    loss           : -949401.5550742574\n",
      "    val_loss       : -946668.471875\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -1024703.500000\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -973879.250000\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -957149.875000\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -960515.437500\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -955031.000000\n",
      "    epoch          : 252\n",
      "    loss           : -950941.4096534654\n",
      "    val_loss       : -946555.8859375\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -1025076.750000\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -935423.562500\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -957359.625000\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -934851.000000\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -919077.250000\n",
      "    epoch          : 253\n",
      "    loss           : -949069.7301980198\n",
      "    val_loss       : -945603.5953125\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -1024289.125000\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -969888.500000\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -925818.500000\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -1025203.687500\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -942857.000000\n",
      "    epoch          : 254\n",
      "    loss           : -949851.0303217822\n",
      "    val_loss       : -946409.61640625\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -971851.000000\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -974454.750000\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -961674.625000\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -926460.750000\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -918707.812500\n",
      "    epoch          : 255\n",
      "    loss           : -951273.0736386139\n",
      "    val_loss       : -947234.43828125\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -1023785.000000\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -926845.875000\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -958183.125000\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -943025.500000\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -918451.562500\n",
      "    epoch          : 256\n",
      "    loss           : -951165.9913366337\n",
      "    val_loss       : -945647.93828125\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -1025184.125000\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -934623.250000\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -929329.562500\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -929149.375000\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -938712.125000\n",
      "    epoch          : 257\n",
      "    loss           : -950618.3075495049\n",
      "    val_loss       : -947242.20625\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -1026376.687500\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -935647.312500\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -927489.125000\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -941487.312500\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -954966.000000\n",
      "    epoch          : 258\n",
      "    loss           : -950915.3465346535\n",
      "    val_loss       : -946401.57578125\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -1022958.562500\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -969877.812500\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -939354.125000\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -955965.500000\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -952822.500000\n",
      "    epoch          : 259\n",
      "    loss           : -950190.5457920792\n",
      "    val_loss       : -946176.2734375\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -1024579.562500\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -935760.187500\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -950504.187500\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -934578.562500\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -939921.250000\n",
      "    epoch          : 260\n",
      "    loss           : -948329.4907178218\n",
      "    val_loss       : -944703.7921875\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -1024248.250000\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -972344.937500\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -926137.000000\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -957454.875000\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -917907.000000\n",
      "    epoch          : 261\n",
      "    loss           : -950362.5457920792\n",
      "    val_loss       : -946848.2859375\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -1025027.875000\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -934426.937500\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -953228.250000\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -957838.250000\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -941507.125000\n",
      "    epoch          : 262\n",
      "    loss           : -949699.8013613861\n",
      "    val_loss       : -945339.9390625\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -1023653.125000\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -934274.875000\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -955780.125000\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -953405.125000\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -941667.750000\n",
      "    epoch          : 263\n",
      "    loss           : -951028.8106435643\n",
      "    val_loss       : -947037.7671875\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -974393.437500\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -935552.437500\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -956815.750000\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -954733.125000\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -942362.625000\n",
      "    epoch          : 264\n",
      "    loss           : -951240.2147277228\n",
      "    val_loss       : -946849.23515625\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -1023765.375000\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -971771.625000\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -953820.875000\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -943302.750000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -925905.437500\n",
      "    epoch          : 265\n",
      "    loss           : -951517.4962871287\n",
      "    val_loss       : -947053.421875\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -1024688.875000\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -935567.062500\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -943256.687500\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -957441.812500\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -953723.687500\n",
      "    epoch          : 266\n",
      "    loss           : -951231.6856435643\n",
      "    val_loss       : -945389.6703125\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -1024276.625000\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -926163.812500\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -925037.875000\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -952929.125000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -940726.250000\n",
      "    epoch          : 267\n",
      "    loss           : -950252.9702970297\n",
      "    val_loss       : -945745.50234375\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -1024846.250000\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -972678.500000\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -927793.812500\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -927031.125000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -953241.812500\n",
      "    epoch          : 268\n",
      "    loss           : -950234.4752475248\n",
      "    val_loss       : -944217.18984375\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -1023834.875000\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -974976.312500\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -954995.187500\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -941680.062500\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -940671.000000\n",
      "    epoch          : 269\n",
      "    loss           : -950161.7444306931\n",
      "    val_loss       : -946878.875\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -1025451.062500\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -943077.812500\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -960301.187500\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -1025621.875000\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -943000.000000\n",
      "    epoch          : 270\n",
      "    loss           : -951931.385519802\n",
      "    val_loss       : -947584.08203125\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -1025077.125000\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -928853.937500\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -924946.250000\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -954903.187500\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -955133.125000\n",
      "    epoch          : 271\n",
      "    loss           : -951539.0693069306\n",
      "    val_loss       : -946978.62890625\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -1023335.125000\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -936553.812500\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -921538.937500\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -939790.500000\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -955163.250000\n",
      "    epoch          : 272\n",
      "    loss           : -949310.3867574257\n",
      "    val_loss       : -944897.209375\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -1024225.625000\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -919787.187500\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -936040.750000\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -918668.937500\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -920280.750000\n",
      "    epoch          : 273\n",
      "    loss           : -949600.4183168317\n",
      "    val_loss       : -946143.9140625\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -971809.125000\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -938947.875000\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -927852.437500\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -941766.000000\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -943254.000000\n",
      "    epoch          : 274\n",
      "    loss           : -951867.3366336634\n",
      "    val_loss       : -948232.95234375\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -1025036.375000\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -937172.687500\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -920659.125000\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -929360.125000\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -922663.250000\n",
      "    epoch          : 275\n",
      "    loss           : -952334.0129950495\n",
      "    val_loss       : -947505.20390625\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -1024920.875000\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -937860.187500\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -942241.625000\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -957406.812500\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -922892.687500\n",
      "    epoch          : 276\n",
      "    loss           : -951984.0686881188\n",
      "    val_loss       : -947415.74375\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -1024460.000000\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -934519.312500\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -959283.375000\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -956183.625000\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -953433.562500\n",
      "    epoch          : 277\n",
      "    loss           : -951755.2747524752\n",
      "    val_loss       : -946214.34296875\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -1025059.250000\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -936398.125000\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -943416.437500\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -1025444.625000\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -920331.500000\n",
      "    epoch          : 278\n",
      "    loss           : -952031.8774752475\n",
      "    val_loss       : -947921.309375\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -1024882.750000\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -928433.750000\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -941940.687500\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -928681.562500\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -954910.687500\n",
      "    epoch          : 279\n",
      "    loss           : -951485.4523514851\n",
      "    val_loss       : -946789.4796875\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -1023743.375000\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -972175.625000\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -961843.000000\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -920880.125000\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -941968.750000\n",
      "    epoch          : 280\n",
      "    loss           : -951263.3168316832\n",
      "    val_loss       : -947750.83671875\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -1025107.250000\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -973202.750000\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -927137.750000\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -927627.937500\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -940537.937500\n",
      "    epoch          : 281\n",
      "    loss           : -951126.1392326732\n",
      "    val_loss       : -945648.31875\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -1025551.375000\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -931682.375000\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -958808.875000\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -927445.937500\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -955081.250000\n",
      "    epoch          : 282\n",
      "    loss           : -949375.1423267326\n",
      "    val_loss       : -946345.58515625\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -935426.125000\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -937767.375000\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -959132.500000\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -929747.375000\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -927885.187500\n",
      "    epoch          : 283\n",
      "    loss           : -951464.5408415842\n",
      "    val_loss       : -947147.521875\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -1023703.000000\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -937966.500000\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -959316.500000\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -940454.250000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -926467.062500\n",
      "    epoch          : 284\n",
      "    loss           : -951473.4053217822\n",
      "    val_loss       : -945729.87578125\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -1024331.375000\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -929186.750000\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -960518.687500\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -928082.937500\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -927989.812500\n",
      "    epoch          : 285\n",
      "    loss           : -951645.4418316832\n",
      "    val_loss       : -947431.0296875\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -1024841.000000\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -939019.000000\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -960557.875000\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -946026.250000\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -957294.625000\n",
      "    epoch          : 286\n",
      "    loss           : -952250.75\n",
      "    val_loss       : -946869.971875\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -1023873.625000\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -938398.375000\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -957096.000000\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -927598.125000\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -956438.062500\n",
      "    epoch          : 287\n",
      "    loss           : -950430.5191831683\n",
      "    val_loss       : -946156.63125\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -1024554.625000\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -971936.937500\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -952997.000000\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -920551.937500\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -914450.625000\n",
      "    epoch          : 288\n",
      "    loss           : -949012.2388613861\n",
      "    val_loss       : -946685.6921875\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -1023879.000000\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -934659.812500\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -927043.687500\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -931031.000000\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -943291.000000\n",
      "    epoch          : 289\n",
      "    loss           : -951953.5680693069\n",
      "    val_loss       : -948202.05078125\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -1025574.062500\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -972281.375000\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -928519.437500\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -955101.375000\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -953578.937500\n",
      "    epoch          : 290\n",
      "    loss           : -952563.5618811881\n",
      "    val_loss       : -947350.03828125\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -1025660.500000\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -937676.687500\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -955839.875000\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -942283.750000\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -921792.812500\n",
      "    epoch          : 291\n",
      "    loss           : -952564.9548267326\n",
      "    val_loss       : -948389.796875\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -1025337.125000\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -936057.437500\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -943391.875000\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -956880.250000\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -953792.250000\n",
      "    epoch          : 292\n",
      "    loss           : -952428.8873762377\n",
      "    val_loss       : -947481.3578125\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -1025424.875000\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -936899.437500\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -943154.437500\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -927676.312500\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -921384.875000\n",
      "    epoch          : 293\n",
      "    loss           : -952344.4900990099\n",
      "    val_loss       : -947577.7796875\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -1025114.500000\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -934697.312500\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -923072.437500\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -923540.500000\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -954670.312500\n",
      "    epoch          : 294\n",
      "    loss           : -951839.489480198\n",
      "    val_loss       : -947291.00859375\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -1025244.000000\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -935685.500000\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -957307.375000\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -928825.750000\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -944636.625000\n",
      "    epoch          : 295\n",
      "    loss           : -952393.8521039604\n",
      "    val_loss       : -947858.753125\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -1023972.312500\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -959881.250000\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -928434.500000\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -921521.062500\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -920688.625000\n",
      "    epoch          : 296\n",
      "    loss           : -952197.8471534654\n",
      "    val_loss       : -947625.22109375\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -1025116.187500\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -972430.562500\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -929490.625000\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -945126.375000\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -956748.250000\n",
      "    epoch          : 297\n",
      "    loss           : -952655.9040841584\n",
      "    val_loss       : -948483.103125\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -1025860.375000\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -935934.312500\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -929356.687500\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -929429.812500\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -925247.875000\n",
      "    epoch          : 298\n",
      "    loss           : -953020.2561881188\n",
      "    val_loss       : -948681.07265625\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -929975.812500\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -932163.000000\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -960164.875000\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -924076.250000\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -954757.937500\n",
      "    epoch          : 299\n",
      "    loss           : -952763.7951732674\n",
      "    val_loss       : -947387.3453125\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -1024997.625000\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -938156.375000\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -963140.187500\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -1024969.937500\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -953439.125000\n",
      "    epoch          : 300\n",
      "    loss           : -951343.2122524752\n",
      "    val_loss       : -944349.53359375\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0824_234706/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -1023583.687500\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -971691.562500\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -960598.250000\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -922770.062500\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -928233.375000\n",
      "    epoch          : 301\n",
      "    loss           : -951671.1881188119\n",
      "    val_loss       : -948252.53828125\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -1024455.812500\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -938574.625000\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -941218.062500\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -957051.875000\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -941912.625000\n",
      "    epoch          : 302\n",
      "    loss           : -951815.9356435643\n",
      "    val_loss       : -946676.221875\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -1024676.312500\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -935374.687500\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -960271.875000\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -930201.375000\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -954469.625000\n",
      "    epoch          : 303\n",
      "    loss           : -951736.7821782178\n",
      "    val_loss       : -946889.69375\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -1025424.750000\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -925535.125000\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -927010.000000\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -1024396.125000\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -922453.875000\n",
      "    epoch          : 304\n",
      "    loss           : -951334.5550742574\n",
      "    val_loss       : -947769.06015625\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -1025341.437500\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -935610.625000\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -929290.562500\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -930906.125000\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -953990.187500\n",
      "    epoch          : 305\n",
      "    loss           : -952247.3762376237\n",
      "    val_loss       : -947074.7515625\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -1024030.750000\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -974225.187500\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -932072.250000\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -922658.812500\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -956052.250000\n",
      "    epoch          : 306\n",
      "    loss           : -952222.9356435643\n",
      "    val_loss       : -947924.71328125\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -1025553.437500\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -974339.500000\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -929851.062500\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -930232.500000\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -950200.312500\n",
      "    epoch          : 307\n",
      "    loss           : -952048.0235148515\n",
      "    val_loss       : -944130.26875\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -1024634.687500\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -974547.062500\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -929760.875000\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -1024701.687500\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -956739.437500\n",
      "    epoch          : 308\n",
      "    loss           : -950756.3657178218\n",
      "    val_loss       : -947348.64375\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -1025358.437500\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -938994.875000\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -942385.687500\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -941661.812500\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -956679.562500\n",
      "    epoch          : 309\n",
      "    loss           : -952203.3780940594\n",
      "    val_loss       : -947179.84921875\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -1024967.375000\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -925153.125000\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -954233.312500\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -923594.750000\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -944546.375000\n",
      "    epoch          : 310\n",
      "    loss           : -952029.0061881188\n",
      "    val_loss       : -948112.1671875\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -1024556.812500\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -928808.187500\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -959314.812500\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -954985.312500\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -943195.875000\n",
      "    epoch          : 311\n",
      "    loss           : -953175.7308168317\n",
      "    val_loss       : -948350.4796875\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -1025766.875000\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -939175.562500\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -930975.500000\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -923767.500000\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -944715.437500\n",
      "    epoch          : 312\n",
      "    loss           : -953018.625\n",
      "    val_loss       : -948654.71953125\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -1024890.687500\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -936005.875000\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -962380.750000\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -960064.375000\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -923236.125000\n",
      "    epoch          : 313\n",
      "    loss           : -953309.4220297029\n",
      "    val_loss       : -948672.8984375\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -1025255.875000\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -940221.625000\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -929643.375000\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -942802.812500\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -920103.750000\n",
      "    epoch          : 314\n",
      "    loss           : -951446.104579208\n",
      "    val_loss       : -941855.63359375\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -1023127.062500\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -933164.375000\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -928406.062500\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -928894.625000\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -944967.937500\n",
      "    epoch          : 315\n",
      "    loss           : -950608.4826732674\n",
      "    val_loss       : -947756.2765625\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -1025864.500000\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -975743.000000\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -941237.375000\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -930432.375000\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -943463.937500\n",
      "    epoch          : 316\n",
      "    loss           : -952101.073019802\n",
      "    val_loss       : -946064.74296875\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -1023811.000000\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -936896.750000\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -928402.062500\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -935410.812500\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -942244.062500\n",
      "    epoch          : 317\n",
      "    loss           : -950118.8527227723\n",
      "    val_loss       : -946770.56640625\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -1024631.875000\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -973255.937500\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -961537.687500\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -960759.125000\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -922524.750000\n",
      "    epoch          : 318\n",
      "    loss           : -952925.1107673268\n",
      "    val_loss       : -948690.8625\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -1025923.375000\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -940885.000000\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -943876.750000\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -944015.375000\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -944868.500000\n",
      "    epoch          : 319\n",
      "    loss           : -953207.2172029703\n",
      "    val_loss       : -947814.68515625\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -1024546.625000\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -957259.625000\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -943941.125000\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -1025264.312500\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -921802.312500\n",
      "    epoch          : 320\n",
      "    loss           : -952878.3719059406\n",
      "    val_loss       : -948261.171875\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -1025989.375000\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -973512.500000\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -958479.500000\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -1024918.375000\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -922324.375000\n",
      "    epoch          : 321\n",
      "    loss           : -951863.1646039604\n",
      "    val_loss       : -948496.359375\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -943165.687500\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -975167.625000\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -957418.500000\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -925642.312500\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -927872.375000\n",
      "    epoch          : 322\n",
      "    loss           : -953277.6992574257\n",
      "    val_loss       : -947880.10390625\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -1024633.687500\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -974044.937500\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -924505.750000\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -943433.437500\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -921671.625000\n",
      "    epoch          : 323\n",
      "    loss           : -951726.6850247525\n",
      "    val_loss       : -943762.153125\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -1023416.250000\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -936656.250000\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -960381.000000\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -956531.250000\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -955948.812500\n",
      "    epoch          : 324\n",
      "    loss           : -951258.4622524752\n",
      "    val_loss       : -947468.56640625\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -1025493.125000\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -939605.500000\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -956209.312500\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -959341.875000\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -923080.875000\n",
      "    epoch          : 325\n",
      "    loss           : -952813.2023514851\n",
      "    val_loss       : -948832.88046875\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -1026300.375000\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -959469.000000\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -931725.562500\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -943140.500000\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -921259.625000\n",
      "    epoch          : 326\n",
      "    loss           : -953550.1602722772\n",
      "    val_loss       : -948955.2484375\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -1024930.562500\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -937231.937500\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -928329.000000\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -930811.000000\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -957079.250000\n",
      "    epoch          : 327\n",
      "    loss           : -952766.9368811881\n",
      "    val_loss       : -948054.2234375\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -974847.750000\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -976178.625000\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -960832.312500\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -956843.812500\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -923115.000000\n",
      "    epoch          : 328\n",
      "    loss           : -953309.1584158416\n",
      "    val_loss       : -948823.059375\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -1024655.312500\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -975512.125000\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -930219.062500\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -928909.500000\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -925844.500000\n",
      "    epoch          : 329\n",
      "    loss           : -952803.5235148515\n",
      "    val_loss       : -948112.1515625\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -1025624.437500\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -931548.312500\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -929855.750000\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -1025508.625000\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -924555.062500\n",
      "    epoch          : 330\n",
      "    loss           : -952800.0055693069\n",
      "    val_loss       : -947244.3265625\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -1025486.062500\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -938742.750000\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -960563.750000\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -961617.687500\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -923411.125000\n",
      "    epoch          : 331\n",
      "    loss           : -952911.2896039604\n",
      "    val_loss       : -948610.6390625\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -1025811.687500\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -937887.375000\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -944291.687500\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -956839.875000\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -960766.687500\n",
      "    epoch          : 332\n",
      "    loss           : -953733.5092821782\n",
      "    val_loss       : -948698.94296875\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -1024263.625000\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -928409.562500\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -930697.062500\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -955763.187500\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -944998.375000\n",
      "    epoch          : 333\n",
      "    loss           : -953422.469059406\n",
      "    val_loss       : -948686.00234375\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -1025371.000000\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -938784.187500\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -931620.062500\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -1025481.812500\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -957733.687500\n",
      "    epoch          : 334\n",
      "    loss           : -953410.2400990099\n",
      "    val_loss       : -948435.16328125\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -937480.500000\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -975034.500000\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -927283.000000\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -959138.125000\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -942921.625000\n",
      "    epoch          : 335\n",
      "    loss           : -951941.8310643565\n",
      "    val_loss       : -946118.45859375\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -1024969.000000\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -956615.000000\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -959298.875000\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -1024760.625000\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -956965.562500\n",
      "    epoch          : 336\n",
      "    loss           : -952545.3056930694\n",
      "    val_loss       : -948870.50546875\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -1026275.750000\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -939181.125000\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -961207.875000\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -1025660.312500\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -925374.000000\n",
      "    epoch          : 337\n",
      "    loss           : -953869.145420792\n",
      "    val_loss       : -948583.68828125\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -1025295.875000\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -973944.500000\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -927947.625000\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -956161.500000\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -954998.125000\n",
      "    epoch          : 338\n",
      "    loss           : -953413.1974009901\n",
      "    val_loss       : -947480.89453125\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -1024962.375000\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -927304.625000\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -958205.937500\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -927816.000000\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -944829.062500\n",
      "    epoch          : 339\n",
      "    loss           : -952411.7530940594\n",
      "    val_loss       : -948360.0171875\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -932725.250000\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -938898.375000\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -961427.750000\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -943952.437500\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -923686.500000\n",
      "    epoch          : 340\n",
      "    loss           : -953956.0897277228\n",
      "    val_loss       : -949464.12421875\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -1024473.937500\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -975582.437500\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -930709.437500\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -957894.625000\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -945373.125000\n",
      "    epoch          : 341\n",
      "    loss           : -954016.9350247525\n",
      "    val_loss       : -949060.08203125\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -1026070.250000\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -938544.687500\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -960784.625000\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -929416.125000\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -947503.500000\n",
      "    epoch          : 342\n",
      "    loss           : -954148.698019802\n",
      "    val_loss       : -948927.47734375\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -1026549.937500\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -945824.812500\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -956510.312500\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -958827.625000\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -957348.937500\n",
      "    epoch          : 343\n",
      "    loss           : -953860.510519802\n",
      "    val_loss       : -947776.48828125\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -1024708.375000\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -930061.375000\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -961942.375000\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -945757.375000\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -924412.562500\n",
      "    epoch          : 344\n",
      "    loss           : -953382.0662128713\n",
      "    val_loss       : -948107.50546875\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -1026578.062500\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -961912.500000\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -930102.437500\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -944510.375000\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -955984.125000\n",
      "    epoch          : 345\n",
      "    loss           : -952717.0866336634\n",
      "    val_loss       : -947488.18984375\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -1025788.687500\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -938026.375000\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -927597.125000\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -957140.375000\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -958438.375000\n",
      "    epoch          : 346\n",
      "    loss           : -951696.3756188119\n",
      "    val_loss       : -947795.390625\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -1024634.250000\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -973402.375000\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -928262.812500\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -958461.312500\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -943808.125000\n",
      "    epoch          : 347\n",
      "    loss           : -952268.7735148515\n",
      "    val_loss       : -945695.46640625\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -932783.500000\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -936745.437500\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -931560.250000\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -944741.500000\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -925429.500000\n",
      "    epoch          : 348\n",
      "    loss           : -953802.9071782178\n",
      "    val_loss       : -949286.6578125\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -1027080.250000\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -959014.875000\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -961867.875000\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -958417.062500\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -955946.000000\n",
      "    epoch          : 349\n",
      "    loss           : -954082.2976485149\n",
      "    val_loss       : -948525.36796875\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -1024475.500000\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -957378.687500\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -932338.875000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -960937.500000\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -946343.812500\n",
      "    epoch          : 350\n",
      "    loss           : -954052.3446782178\n",
      "    val_loss       : -949291.37734375\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0824_234706/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -1026285.375000\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -972396.312500\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -929526.250000\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -959817.375000\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -956744.187500\n",
      "    epoch          : 351\n",
      "    loss           : -953487.0952970297\n",
      "    val_loss       : -947738.3734375\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -975368.750000\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -932478.187500\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -929043.625000\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -959871.125000\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -922785.125000\n",
      "    epoch          : 352\n",
      "    loss           : -953070.5129950495\n",
      "    val_loss       : -949136.10703125\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -973899.437500\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -977180.125000\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -960994.750000\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -932720.875000\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -959700.500000\n",
      "    epoch          : 353\n",
      "    loss           : -954592.4300742574\n",
      "    val_loss       : -949739.653125\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -1025725.125000\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -941659.750000\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -930928.562500\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -924689.562500\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -929347.125000\n",
      "    epoch          : 354\n",
      "    loss           : -953661.0216584158\n",
      "    val_loss       : -947090.04296875\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -1023389.000000\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -937544.500000\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -931753.625000\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -958197.375000\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -954322.000000\n",
      "    epoch          : 355\n",
      "    loss           : -952701.5662128713\n",
      "    val_loss       : -948503.4890625\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -1025748.000000\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -939471.625000\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -932751.000000\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -960344.375000\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -926559.500000\n",
      "    epoch          : 356\n",
      "    loss           : -954380.5761138614\n",
      "    val_loss       : -949251.353125\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -1026521.750000\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -973687.375000\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -960966.812500\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -929892.125000\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -942139.625000\n",
      "    epoch          : 357\n",
      "    loss           : -953189.8465346535\n",
      "    val_loss       : -947564.8453125\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -1024246.750000\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -927987.250000\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -944189.000000\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -930426.875000\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -957356.562500\n",
      "    epoch          : 358\n",
      "    loss           : -953417.5\n",
      "    val_loss       : -947592.55078125\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -1024856.562500\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -929036.187500\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -960941.812500\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -1025221.375000\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -957331.875000\n",
      "    epoch          : 359\n",
      "    loss           : -953952.0037128713\n",
      "    val_loss       : -949610.11484375\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -1024549.625000\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -944474.000000\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -957950.875000\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -946178.000000\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -947539.000000\n",
      "    epoch          : 360\n",
      "    loss           : -954664.9195544554\n",
      "    val_loss       : -949578.11328125\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -975388.937500\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -941265.375000\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -932858.875000\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -923581.125000\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -922701.625000\n",
      "    epoch          : 361\n",
      "    loss           : -954138.0952970297\n",
      "    val_loss       : -948962.00546875\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -1025862.625000\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -933958.687500\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -930011.062500\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -953437.437500\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -956172.250000\n",
      "    epoch          : 362\n",
      "    loss           : -952811.6701732674\n",
      "    val_loss       : -947702.07578125\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -974592.375000\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -935798.000000\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -943142.500000\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -929437.062500\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -928165.437500\n",
      "    epoch          : 363\n",
      "    loss           : -951992.5884900991\n",
      "    val_loss       : -948159.73671875\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -1025443.312500\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -941062.375000\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -956566.500000\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -959025.500000\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -959302.375000\n",
      "    epoch          : 364\n",
      "    loss           : -953785.8842821782\n",
      "    val_loss       : -948493.778125\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -1025914.437500\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -962697.812500\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -929984.812500\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -955763.562500\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -924502.000000\n",
      "    epoch          : 365\n",
      "    loss           : -953853.8502475248\n",
      "    val_loss       : -949544.05\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -1025165.625000\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -962204.937500\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -931396.125000\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -930418.937500\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -957925.937500\n",
      "    epoch          : 366\n",
      "    loss           : -954614.1448019802\n",
      "    val_loss       : -949440.065625\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -1026053.500000\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -962827.875000\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -934080.250000\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -1026241.250000\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -925133.375000\n",
      "    epoch          : 367\n",
      "    loss           : -954863.5977722772\n",
      "    val_loss       : -949758.425\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -940330.687500\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -975202.312500\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -930743.625000\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -944350.875000\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -924782.187500\n",
      "    epoch          : 368\n",
      "    loss           : -954675.0179455446\n",
      "    val_loss       : -949466.9234375\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -933317.000000\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -939207.437500\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -928341.875000\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -1024227.937500\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -922658.250000\n",
      "    epoch          : 369\n",
      "    loss           : -953561.9870049505\n",
      "    val_loss       : -948191.4921875\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -1026209.375000\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -924946.250000\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -928401.000000\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -1025254.625000\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -944076.500000\n",
      "    epoch          : 370\n",
      "    loss           : -953297.1404702971\n",
      "    val_loss       : -948810.52109375\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -1025179.125000\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -974082.312500\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -928980.875000\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -945929.437500\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -956667.750000\n",
      "    epoch          : 371\n",
      "    loss           : -953348.6008663366\n",
      "    val_loss       : -947811.240625\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -1025269.125000\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -929079.187500\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -961339.375000\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -961466.500000\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -947553.625000\n",
      "    epoch          : 372\n",
      "    loss           : -954325.1633663366\n",
      "    val_loss       : -949747.540625\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -1024857.625000\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -934376.625000\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -934279.125000\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -961614.500000\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -944164.375000\n",
      "    epoch          : 373\n",
      "    loss           : -955247.6423267326\n",
      "    val_loss       : -949847.3328125\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -1025466.625000\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -939372.375000\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -942875.000000\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -1027205.000000\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -925718.125000\n",
      "    epoch          : 374\n",
      "    loss           : -955014.1206683168\n",
      "    val_loss       : -948871.74765625\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -939184.500000\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -957698.812500\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -946842.000000\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -946881.375000\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -958917.562500\n",
      "    epoch          : 375\n",
      "    loss           : -954687.167079208\n",
      "    val_loss       : -948836.8984375\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -1025353.562500\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -934998.250000\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -931683.687500\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -945561.562500\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -945136.000000\n",
      "    epoch          : 376\n",
      "    loss           : -953011.6676980198\n",
      "    val_loss       : -947515.46015625\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -1026330.187500\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -939286.000000\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -946235.875000\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -1025502.500000\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -925551.250000\n",
      "    epoch          : 377\n",
      "    loss           : -953764.4758663366\n",
      "    val_loss       : -949267.1046875\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -1025256.562500\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -975108.875000\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -926070.750000\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -959708.500000\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -930014.500000\n",
      "    epoch          : 378\n",
      "    loss           : -953803.5544554455\n",
      "    val_loss       : -946167.35078125\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -931583.000000\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -970381.375000\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -962001.000000\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -944273.250000\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -959272.625000\n",
      "    epoch          : 379\n",
      "    loss           : -951549.323019802\n",
      "    val_loss       : -947013.2546875\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -1025400.375000\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -935757.500000\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -942079.875000\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -945461.437500\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -918583.500000\n",
      "    epoch          : 380\n",
      "    loss           : -951578.0544554455\n",
      "    val_loss       : -947709.4703125\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -974635.375000\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -939559.312500\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -947396.125000\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -957773.500000\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -944561.750000\n",
      "    epoch          : 381\n",
      "    loss           : -953817.6342821782\n",
      "    val_loss       : -949091.859375\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -938859.437500\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -939878.250000\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -947422.375000\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -963283.187500\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -958036.250000\n",
      "    epoch          : 382\n",
      "    loss           : -955283.0798267326\n",
      "    val_loss       : -949948.58515625\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -926607.437500\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -975789.750000\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -931850.937500\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -927139.875000\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -924251.187500\n",
      "    epoch          : 383\n",
      "    loss           : -954652.7858910891\n",
      "    val_loss       : -949074.27421875\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -938159.375000\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -941646.875000\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -926383.937500\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -932943.687500\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -932580.000000\n",
      "    epoch          : 384\n",
      "    loss           : -954775.7438118812\n",
      "    val_loss       : -949858.88203125\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -1025610.062500\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -940704.000000\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -959593.875000\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -963035.250000\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -945509.875000\n",
      "    epoch          : 385\n",
      "    loss           : -955428.1212871287\n",
      "    val_loss       : -949885.15078125\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -1026594.875000\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -941559.750000\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -957915.500000\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -926532.875000\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -923384.375000\n",
      "    epoch          : 386\n",
      "    loss           : -955076.7326732674\n",
      "    val_loss       : -949152.93046875\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -975257.500000\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -975325.562500\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -927787.937500\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -930244.500000\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -959229.500000\n",
      "    epoch          : 387\n",
      "    loss           : -954749.3366336634\n",
      "    val_loss       : -949343.3203125\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -1026630.875000\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -933224.750000\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -961799.500000\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -957337.750000\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -922267.625000\n",
      "    epoch          : 388\n",
      "    loss           : -954569.2017326732\n",
      "    val_loss       : -947575.49453125\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -1025278.250000\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -938662.625000\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -927900.312500\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -922211.750000\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -926068.812500\n",
      "    epoch          : 389\n",
      "    loss           : -952063.5129950495\n",
      "    val_loss       : -948666.0296875\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -1025285.312500\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -941171.000000\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -931187.000000\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -1026394.125000\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -947183.000000\n",
      "    epoch          : 390\n",
      "    loss           : -954602.7926980198\n",
      "    val_loss       : -949647.0515625\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -1027176.875000\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -939102.625000\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -927788.687500\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -935390.750000\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -925353.625000\n",
      "    epoch          : 391\n",
      "    loss           : -955277.9777227723\n",
      "    val_loss       : -949430.18125\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -1026894.750000\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -975039.125000\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -945848.000000\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -1024192.375000\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -925004.625000\n",
      "    epoch          : 392\n",
      "    loss           : -953559.344059406\n",
      "    val_loss       : -947640.76796875\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -1025734.625000\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -929403.125000\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -931968.375000\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -959074.937500\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -946081.375000\n",
      "    epoch          : 393\n",
      "    loss           : -954456.7568069306\n",
      "    val_loss       : -949936.89921875\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -1025436.750000\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -934992.687500\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -963364.937500\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -957541.625000\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -948210.812500\n",
      "    epoch          : 394\n",
      "    loss           : -955075.707920792\n",
      "    val_loss       : -949811.415625\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -932074.375000\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -933868.500000\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -934130.750000\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -959482.312500\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -926234.375000\n",
      "    epoch          : 395\n",
      "    loss           : -955600.9746287129\n",
      "    val_loss       : -949992.89765625\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -1025605.500000\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -940598.437500\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -932686.750000\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -1027018.125000\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -927945.250000\n",
      "    epoch          : 396\n",
      "    loss           : -955685.0884900991\n",
      "    val_loss       : -949937.28984375\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -1025799.875000\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -940601.062500\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -929419.750000\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -934925.125000\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -955779.375000\n",
      "    epoch          : 397\n",
      "    loss           : -954817.9356435643\n",
      "    val_loss       : -946076.26015625\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -1025140.500000\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -936872.125000\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -943493.500000\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -1026088.250000\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -924546.250000\n",
      "    epoch          : 398\n",
      "    loss           : -952213.8743811881\n",
      "    val_loss       : -949633.5515625\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -1026372.375000\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -932117.562500\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -946792.000000\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -1026726.187500\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -955682.375000\n",
      "    epoch          : 399\n",
      "    loss           : -955051.0754950495\n",
      "    val_loss       : -948672.68125\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -1026675.125000\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -974766.625000\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -925144.375000\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -959304.750000\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -928635.125000\n",
      "    epoch          : 400\n",
      "    loss           : -953537.5761138614\n",
      "    val_loss       : -948624.34765625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0824_234706/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -1024249.625000\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -975113.125000\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -931557.562500\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -1025760.000000\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -945314.125000\n",
      "    epoch          : 401\n",
      "    loss           : -954978.4783415842\n",
      "    val_loss       : -949857.72890625\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -1025345.500000\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -961345.000000\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -942964.875000\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -1025215.875000\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -924021.375000\n",
      "    epoch          : 402\n",
      "    loss           : -954427.0154702971\n",
      "    val_loss       : -948709.70703125\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -974920.000000\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -938299.562500\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -928491.062500\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -944031.687500\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -926213.312500\n",
      "    epoch          : 403\n",
      "    loss           : -955047.3106435643\n",
      "    val_loss       : -949899.66015625\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -1026541.625000\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -939295.750000\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -961968.250000\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -931555.625000\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -959100.750000\n",
      "    epoch          : 404\n",
      "    loss           : -955463.0222772277\n",
      "    val_loss       : -949587.684375\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -1025820.812500\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -931393.375000\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -926181.312500\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -925664.500000\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -946288.125000\n",
      "    epoch          : 405\n",
      "    loss           : -955598.0470297029\n",
      "    val_loss       : -949493.21328125\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -1026635.125000\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -939493.250000\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -935044.500000\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -1025784.250000\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -958011.875000\n",
      "    epoch          : 406\n",
      "    loss           : -955253.3941831683\n",
      "    val_loss       : -949876.53125\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -974288.687500\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -942501.750000\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -958840.250000\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -961922.062500\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -924939.750000\n",
      "    epoch          : 407\n",
      "    loss           : -955350.1126237623\n",
      "    val_loss       : -949667.928125\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -1025754.812500\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -931492.937500\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -925293.125000\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -944743.750000\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -946462.312500\n",
      "    epoch          : 408\n",
      "    loss           : -954547.3706683168\n",
      "    val_loss       : -949365.41953125\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -1025300.500000\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -941480.250000\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -933922.125000\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -960149.500000\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -931713.312500\n",
      "    epoch          : 409\n",
      "    loss           : -955023.332920792\n",
      "    val_loss       : -948560.60546875\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -1024409.875000\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -962161.875000\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -962384.187500\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -946394.375000\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -926648.875000\n",
      "    epoch          : 410\n",
      "    loss           : -954427.4511138614\n",
      "    val_loss       : -947423.28984375\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -1025130.250000\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -933594.062500\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -931162.125000\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -944012.875000\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -958814.312500\n",
      "    epoch          : 411\n",
      "    loss           : -953252.7116336634\n",
      "    val_loss       : -949553.45546875\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -1026019.750000\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -940634.625000\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -946386.437500\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -964462.125000\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -960468.000000\n",
      "    epoch          : 412\n",
      "    loss           : -955701.0037128713\n",
      "    val_loss       : -950453.78359375\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -1025786.625000\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -942586.000000\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -963093.687500\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -958989.937500\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -927258.687500\n",
      "    epoch          : 413\n",
      "    loss           : -956006.6138613861\n",
      "    val_loss       : -949756.784375\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -976862.000000\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -940538.250000\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -931382.375000\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -946610.937500\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -946322.500000\n",
      "    epoch          : 414\n",
      "    loss           : -955395.8483910891\n",
      "    val_loss       : -949571.37890625\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -1025962.062500\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -939627.562500\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -930023.687500\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -934931.187500\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -931235.875000\n",
      "    epoch          : 415\n",
      "    loss           : -954897.0736386139\n",
      "    val_loss       : -949608.99296875\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -1025637.125000\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -975657.125000\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -930811.750000\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -957814.875000\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -959247.312500\n",
      "    epoch          : 416\n",
      "    loss           : -955343.8143564357\n",
      "    val_loss       : -949532.34296875\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -1025605.437500\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -977476.187500\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -962398.375000\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -933181.750000\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -962043.250000\n",
      "    epoch          : 417\n",
      "    loss           : -955725.7834158416\n",
      "    val_loss       : -950375.30234375\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -1026319.375000\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -976183.687500\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -933219.187500\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -925780.125000\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -928347.875000\n",
      "    epoch          : 418\n",
      "    loss           : -955535.1782178218\n",
      "    val_loss       : -949716.32734375\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -1025347.187500\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -931166.562500\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -930758.312500\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -927529.750000\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -957942.250000\n",
      "    epoch          : 419\n",
      "    loss           : -954563.7258663366\n",
      "    val_loss       : -949133.98203125\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -1025715.125000\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -976940.500000\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -928642.062500\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -933240.000000\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -947408.750000\n",
      "    epoch          : 420\n",
      "    loss           : -954881.0792079208\n",
      "    val_loss       : -948236.8140625\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -1025818.000000\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -973172.687500\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -962083.000000\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -945075.187500\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -945447.562500\n",
      "    epoch          : 421\n",
      "    loss           : -954767.4071782178\n",
      "    val_loss       : -949220.08515625\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -1027206.375000\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -937209.437500\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -930081.500000\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -1026178.562500\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -946214.625000\n",
      "    epoch          : 422\n",
      "    loss           : -955072.0470297029\n",
      "    val_loss       : -949779.225\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -1026740.812500\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -932452.625000\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -963080.937500\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -947893.250000\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -928784.750000\n",
      "    epoch          : 423\n",
      "    loss           : -955896.4832920792\n",
      "    val_loss       : -949920.13203125\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -939369.000000\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -974810.375000\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -963851.250000\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -930152.437500\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -936546.500000\n",
      "    epoch          : 424\n",
      "    loss           : -955554.6800742574\n",
      "    val_loss       : -949323.9890625\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -1025945.687500\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -931261.500000\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -931975.125000\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -927124.687500\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -958753.125000\n",
      "    epoch          : 425\n",
      "    loss           : -955461.4981435643\n",
      "    val_loss       : -950294.80234375\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -1026448.000000\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -947335.125000\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -932064.687500\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -1026881.562500\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -947867.500000\n",
      "    epoch          : 426\n",
      "    loss           : -956022.5971534654\n",
      "    val_loss       : -949542.65625\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -1027723.625000\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -941187.187500\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -961937.125000\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -1025452.000000\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -947819.375000\n",
      "    epoch          : 427\n",
      "    loss           : -955055.218440594\n",
      "    val_loss       : -950072.0859375\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -978796.687500\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -963433.625000\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -962717.500000\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -927536.937500\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -959042.062500\n",
      "    epoch          : 428\n",
      "    loss           : -955336.1899752475\n",
      "    val_loss       : -950124.00078125\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -1025748.312500\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -939602.125000\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -946437.125000\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -962254.375000\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -924132.437500\n",
      "    epoch          : 429\n",
      "    loss           : -955822.8471534654\n",
      "    val_loss       : -949491.45546875\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -976117.312500\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -938780.375000\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -945057.875000\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -948216.500000\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -959961.000000\n",
      "    epoch          : 430\n",
      "    loss           : -955316.2370049505\n",
      "    val_loss       : -949369.15703125\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -1025015.625000\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -939817.375000\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -930525.187500\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -945831.875000\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -958055.625000\n",
      "    epoch          : 431\n",
      "    loss           : -954902.1782178218\n",
      "    val_loss       : -949031.74453125\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -1026505.187500\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -941073.500000\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -945266.875000\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -934501.875000\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -958378.375000\n",
      "    epoch          : 432\n",
      "    loss           : -955419.0637376237\n",
      "    val_loss       : -949381.2203125\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -1026175.000000\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -976509.375000\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -947124.000000\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -1025507.375000\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -933829.250000\n",
      "    epoch          : 433\n",
      "    loss           : -955247.354579208\n",
      "    val_loss       : -949232.6375\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -1026424.500000\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -941658.500000\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -946867.750000\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -960185.875000\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -957941.062500\n",
      "    epoch          : 434\n",
      "    loss           : -955585.2407178218\n",
      "    val_loss       : -949330.79140625\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -1025798.562500\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -975834.437500\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -931744.000000\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -931208.562500\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -946576.500000\n",
      "    epoch          : 435\n",
      "    loss           : -955547.9176980198\n",
      "    val_loss       : -950272.440625\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -1026371.625000\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -978834.937500\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -933389.375000\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -946596.625000\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -942114.750000\n",
      "    epoch          : 436\n",
      "    loss           : -954717.551980198\n",
      "    val_loss       : -947180.9421875\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -975166.500000\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -938631.750000\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -930143.375000\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -961878.187500\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -929448.125000\n",
      "    epoch          : 437\n",
      "    loss           : -954354.0043316832\n",
      "    val_loss       : -949803.9546875\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -1025230.625000\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -974777.000000\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -946253.625000\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -959332.250000\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -924709.937500\n",
      "    epoch          : 438\n",
      "    loss           : -954522.1961633663\n",
      "    val_loss       : -947804.0578125\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -1024461.375000\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -939831.187500\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -944476.375000\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -933120.000000\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -928670.125000\n",
      "    epoch          : 439\n",
      "    loss           : -954971.3929455446\n",
      "    val_loss       : -950035.1578125\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -1026265.875000\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -977877.125000\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -935055.500000\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -963598.375000\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -931803.875000\n",
      "    epoch          : 440\n",
      "    loss           : -955933.8162128713\n",
      "    val_loss       : -950512.621875\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -1026317.687500\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -976098.875000\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -930698.625000\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -929874.625000\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -926039.250000\n",
      "    epoch          : 441\n",
      "    loss           : -955728.3941831683\n",
      "    val_loss       : -949500.871875\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -977161.000000\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -940868.500000\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -964729.500000\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -1026201.750000\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -960056.000000\n",
      "    epoch          : 442\n",
      "    loss           : -955702.2159653465\n",
      "    val_loss       : -949990.36953125\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -1026587.750000\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -940416.500000\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -962854.562500\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -1024457.062500\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -959209.125000\n",
      "    epoch          : 443\n",
      "    loss           : -955468.0247524752\n",
      "    val_loss       : -949258.8734375\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -1026991.750000\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -976473.937500\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -958332.750000\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -928501.750000\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -926342.562500\n",
      "    epoch          : 444\n",
      "    loss           : -954881.3601485149\n",
      "    val_loss       : -949931.1125\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -1026080.312500\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -942127.875000\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -930001.312500\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -949470.500000\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -946062.375000\n",
      "    epoch          : 445\n",
      "    loss           : -955626.0990099009\n",
      "    val_loss       : -949709.884375\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -979038.125000\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -975616.500000\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -962826.625000\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -943705.437500\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -928398.250000\n",
      "    epoch          : 446\n",
      "    loss           : -955338.0241336634\n",
      "    val_loss       : -950396.24765625\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -978957.062500\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -940407.125000\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -931373.187500\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -946686.750000\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -946406.125000\n",
      "    epoch          : 447\n",
      "    loss           : -956103.926980198\n",
      "    val_loss       : -950069.05859375\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -1027284.437500\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -977588.875000\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -931257.875000\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -959823.062500\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -961868.250000\n",
      "    epoch          : 448\n",
      "    loss           : -955503.6404702971\n",
      "    val_loss       : -948831.4765625\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -1025351.750000\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -976735.125000\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -964652.250000\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -960322.750000\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -947683.125000\n",
      "    epoch          : 449\n",
      "    loss           : -956205.6212871287\n",
      "    val_loss       : -950491.30234375\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -1026418.875000\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -940815.000000\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -932725.125000\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -961978.500000\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -943939.500000\n",
      "    epoch          : 450\n",
      "    loss           : -955893.6732673268\n",
      "    val_loss       : -949328.6015625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0824_234706/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -1025280.625000\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -940528.062500\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -932951.562500\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -931479.000000\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -960038.750000\n",
      "    epoch          : 451\n",
      "    loss           : -955754.1639851485\n",
      "    val_loss       : -950417.42578125\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -1025924.500000\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -976943.500000\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -946338.375000\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -945996.062500\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -955722.000000\n",
      "    epoch          : 452\n",
      "    loss           : -954934.6280940594\n",
      "    val_loss       : -947633.26171875\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -944030.375000\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -940394.187500\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -957371.375000\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -1024560.687500\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -956567.750000\n",
      "    epoch          : 453\n",
      "    loss           : -954344.2988861386\n",
      "    val_loss       : -948671.1453125\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -1026117.250000\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -931287.625000\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -958485.500000\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -961749.500000\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -926017.500000\n",
      "    epoch          : 454\n",
      "    loss           : -954182.6101485149\n",
      "    val_loss       : -950011.196875\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -975818.875000\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -942027.000000\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -946515.375000\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -963937.000000\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -959684.187500\n",
      "    epoch          : 455\n",
      "    loss           : -955822.9715346535\n",
      "    val_loss       : -949495.5578125\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -1025913.375000\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -942580.875000\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -959138.062500\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -962423.062500\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -947340.187500\n",
      "    epoch          : 456\n",
      "    loss           : -955985.7400990099\n",
      "    val_loss       : -950483.471875\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -960155.500000\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -977906.000000\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -935455.437500\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -926834.062500\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -960295.625000\n",
      "    epoch          : 457\n",
      "    loss           : -956591.1528465346\n",
      "    val_loss       : -950639.271875\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -1026615.875000\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -942292.625000\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -933606.125000\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -946112.437500\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -928536.125000\n",
      "    epoch          : 458\n",
      "    loss           : -956274.8948019802\n",
      "    val_loss       : -950212.27421875\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -1026263.250000\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -935463.250000\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -963998.375000\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -958930.687500\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -946617.312500\n",
      "    epoch          : 459\n",
      "    loss           : -956433.2580445545\n",
      "    val_loss       : -950243.69765625\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -1026202.125000\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -976815.125000\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -935780.062500\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -928662.187500\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -928457.750000\n",
      "    epoch          : 460\n",
      "    loss           : -956504.6726485149\n",
      "    val_loss       : -950742.21875\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -1027345.750000\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -938468.875000\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -963519.375000\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -1027167.312500\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -959587.625000\n",
      "    epoch          : 461\n",
      "    loss           : -956811.1553217822\n",
      "    val_loss       : -950506.35078125\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -1026719.437500\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -943541.125000\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -933100.312500\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -931778.250000\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -928506.312500\n",
      "    epoch          : 462\n",
      "    loss           : -956397.2524752475\n",
      "    val_loss       : -950549.27890625\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -1027365.500000\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -974465.125000\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -964940.687500\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -964392.875000\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -945843.062500\n",
      "    epoch          : 463\n",
      "    loss           : -956181.1633663366\n",
      "    val_loss       : -948990.265625\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -1025540.500000\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -930043.937500\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -933680.875000\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -929303.062500\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -961821.437500\n",
      "    epoch          : 464\n",
      "    loss           : -955797.9474009901\n",
      "    val_loss       : -950279.49765625\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -1025925.625000\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -975237.312500\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -934983.562500\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -933151.875000\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -948821.125000\n",
      "    epoch          : 465\n",
      "    loss           : -956730.8174504951\n",
      "    val_loss       : -950756.2265625\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -1026183.687500\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -959533.750000\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -964199.500000\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -1025642.312500\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -947619.312500\n",
      "    epoch          : 466\n",
      "    loss           : -956566.5018564357\n",
      "    val_loss       : -950175.31953125\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -1027055.500000\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -941236.625000\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -932796.312500\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -959317.562500\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -931366.375000\n",
      "    epoch          : 467\n",
      "    loss           : -955123.5847772277\n",
      "    val_loss       : -948805.196875\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -1025390.250000\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -975179.750000\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -933580.437500\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -945153.562500\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -931712.000000\n",
      "    epoch          : 468\n",
      "    loss           : -954521.5754950495\n",
      "    val_loss       : -949589.29375\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -1026116.125000\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -972521.875000\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -956744.500000\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -957486.312500\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -959998.125000\n",
      "    epoch          : 469\n",
      "    loss           : -954320.7469059406\n",
      "    val_loss       : -949964.9484375\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -1027671.375000\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -963485.687500\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -934347.125000\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -1027177.250000\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -948507.937500\n",
      "    epoch          : 470\n",
      "    loss           : -956629.6763613861\n",
      "    val_loss       : -950236.13125\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -1026371.062500\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -942918.250000\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -931895.812500\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -1025432.750000\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -927874.250000\n",
      "    epoch          : 471\n",
      "    loss           : -956169.3038366337\n",
      "    val_loss       : -949462.128125\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -942591.000000\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -975402.375000\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -945054.750000\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -1026177.687500\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -958953.500000\n",
      "    epoch          : 472\n",
      "    loss           : -955608.5259900991\n",
      "    val_loss       : -948309.99375\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -1024134.312500\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -938246.875000\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -963351.812500\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -946885.500000\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -926214.000000\n",
      "    epoch          : 473\n",
      "    loss           : -955502.6658415842\n",
      "    val_loss       : -950468.83671875\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -975789.375000\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -943552.250000\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -934548.687500\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -1027081.000000\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -946290.375000\n",
      "    epoch          : 474\n",
      "    loss           : -956117.2840346535\n",
      "    val_loss       : -948455.771875\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -937973.625000\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -945025.500000\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -947151.500000\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -1025331.625000\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -924999.187500\n",
      "    epoch          : 475\n",
      "    loss           : -955072.5061881188\n",
      "    val_loss       : -949076.31796875\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -1025405.187500\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -932679.562500\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -932023.500000\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -1027812.000000\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -929575.125000\n",
      "    epoch          : 476\n",
      "    loss           : -955960.9616336634\n",
      "    val_loss       : -950733.39375\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -927372.687500\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -976837.875000\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -933449.250000\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -947595.875000\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -959414.500000\n",
      "    epoch          : 477\n",
      "    loss           : -956629.833539604\n",
      "    val_loss       : -950260.2546875\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -1025672.750000\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -941990.125000\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -934378.625000\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -932747.375000\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -932490.750000\n",
      "    epoch          : 478\n",
      "    loss           : -956342.5587871287\n",
      "    val_loss       : -949385.23828125\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -1026589.375000\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -972591.625000\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -961789.937500\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -931067.812500\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -960300.750000\n",
      "    epoch          : 479\n",
      "    loss           : -955516.1912128713\n",
      "    val_loss       : -949880.89453125\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -1026767.937500\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -937700.312500\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -932407.437500\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -931974.250000\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -926894.500000\n",
      "    epoch          : 480\n",
      "    loss           : -956576.4566831683\n",
      "    val_loss       : -950846.8984375\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -1027519.937500\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -944539.250000\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -934328.500000\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -929111.437500\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -928548.250000\n",
      "    epoch          : 481\n",
      "    loss           : -956655.6856435643\n",
      "    val_loss       : -950032.81328125\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -1027512.875000\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -946901.875000\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -974223.062500\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -1024036.625000\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -920601.562500\n",
      "    epoch          : 482\n",
      "    loss           : -954196.9226485149\n",
      "    val_loss       : -947449.1140625\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -1023961.625000\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -937171.625000\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -945516.250000\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -934385.125000\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -947279.000000\n",
      "    epoch          : 483\n",
      "    loss           : -955095.5037128713\n",
      "    val_loss       : -950850.61171875\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -1026359.125000\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -944102.250000\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -946264.875000\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -1026058.750000\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -931001.500000\n",
      "    epoch          : 484\n",
      "    loss           : -956738.1379950495\n",
      "    val_loss       : -950493.67109375\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -1027116.375000\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -929282.625000\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -965255.250000\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -932506.375000\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -959060.000000\n",
      "    epoch          : 485\n",
      "    loss           : -957118.4405940594\n",
      "    val_loss       : -950918.365625\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -1027327.937500\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -935073.500000\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -947964.687500\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -961185.875000\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -960526.875000\n",
      "    epoch          : 486\n",
      "    loss           : -956991.0303217822\n",
      "    val_loss       : -950436.065625\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -1027040.250000\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -964447.937500\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -931070.875000\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -961800.312500\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -958510.375000\n",
      "    epoch          : 487\n",
      "    loss           : -956140.0717821782\n",
      "    val_loss       : -950356.69765625\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -1026469.812500\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -959015.000000\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -941659.375000\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -927588.562500\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -945916.375000\n",
      "    epoch          : 488\n",
      "    loss           : -955349.8978960396\n",
      "    val_loss       : -950464.6765625\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -932597.062500\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -941575.875000\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -936232.125000\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -932984.437500\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -947072.125000\n",
      "    epoch          : 489\n",
      "    loss           : -956855.9003712871\n",
      "    val_loss       : -950812.084375\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -1026571.250000\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -941221.125000\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -962947.062500\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -925310.625000\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -929220.125000\n",
      "    epoch          : 490\n",
      "    loss           : -956612.4139851485\n",
      "    val_loss       : -950375.63984375\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -1025945.500000\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -976130.687500\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -963048.062500\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -927199.812500\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -931712.750000\n",
      "    epoch          : 491\n",
      "    loss           : -956839.0235148515\n",
      "    val_loss       : -950627.071875\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -1026893.062500\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -940520.500000\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -950884.937500\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -1026824.375000\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -961188.562500\n",
      "    epoch          : 492\n",
      "    loss           : -956836.395420792\n",
      "    val_loss       : -950503.85\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -976615.000000\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -942869.625000\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -933395.000000\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -961436.812500\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -948000.937500\n",
      "    epoch          : 493\n",
      "    loss           : -956521.4350247525\n",
      "    val_loss       : -949527.37265625\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -1026393.500000\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -932411.875000\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -933500.500000\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -1025779.187500\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -945342.437500\n",
      "    epoch          : 494\n",
      "    loss           : -955692.958539604\n",
      "    val_loss       : -949021.3234375\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -1026491.812500\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -942298.687500\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -930674.500000\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -934426.562500\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -962229.250000\n",
      "    epoch          : 495\n",
      "    loss           : -955960.0748762377\n",
      "    val_loss       : -950037.409375\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -1028320.250000\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -941534.937500\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -962523.500000\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -948831.750000\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -961593.750000\n",
      "    epoch          : 496\n",
      "    loss           : -956851.6299504951\n",
      "    val_loss       : -950996.3109375\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -1026914.625000\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -942023.500000\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -939107.000000\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -934220.500000\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -947022.687500\n",
      "    epoch          : 497\n",
      "    loss           : -957201.9022277228\n",
      "    val_loss       : -950874.56640625\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -1026166.875000\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -939602.500000\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -935262.062500\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -961210.875000\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -960762.375000\n",
      "    epoch          : 498\n",
      "    loss           : -956525.7883663366\n",
      "    val_loss       : -949871.04765625\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -1026772.250000\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -961017.125000\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -963870.125000\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -1025379.875000\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -959449.937500\n",
      "    epoch          : 499\n",
      "    loss           : -955744.6509900991\n",
      "    val_loss       : -949583.78125\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -1025928.812500\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -941318.125000\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -935121.000000\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -960392.750000\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -945426.062500\n",
      "    epoch          : 500\n",
      "    loss           : -956527.0736386139\n",
      "    val_loss       : -950381.24609375\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0824_234706/checkpoint-epoch500.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VaeCategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=12, bias=True)\n",
       "        (1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (4): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=12, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_0_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=12, bias=True)\n",
       "        (1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (4): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=12, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=20, bias=True)\n",
       "        (1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=20, out_features=20, bias=True)\n",
       "        (4): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=20, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_1_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=20, bias=True)\n",
       "        (1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=20, out_features=20, bias=True)\n",
       "        (4): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=20, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=36, bias=True)\n",
       "        (1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (4): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=36, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_2_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=36, bias=True)\n",
       "        (1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (4): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=36, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=68, bias=True)\n",
       "        (1): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=68, out_features=68, bias=True)\n",
       "        (4): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=68, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_3_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=68, bias=True)\n",
       "        (1): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=68, out_features=68, bias=True)\n",
       "        (4): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=68, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=132, bias=True)\n",
       "        (1): LayerNorm((132,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=132, out_features=132, bias=True)\n",
       "        (4): LayerNorm((132,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=132, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_4_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=132, bias=True)\n",
       "        (1): LayerNorm((132,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=132, out_features=132, bias=True)\n",
       "        (4): LayerNorm((132,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=132, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=260, bias=True)\n",
       "        (1): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=260, out_features=260, bias=True)\n",
       "        (4): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=260, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_5_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=260, bias=True)\n",
       "        (1): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=260, out_features=260, bias=True)\n",
       "        (4): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=260, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=396, bias=True)\n",
       "        (1): LayerNorm((396,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=396, out_features=396, bias=True)\n",
       "        (4): LayerNorm((396,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=396, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_6_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=396, bias=True)\n",
       "        (1): LayerNorm((396,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=396, out_features=396, bias=True)\n",
       "        (4): LayerNorm((396,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=396, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_7_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_8_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_9_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_10): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=136, bias=True)\n",
       "        (1): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=136, out_features=136, bias=True)\n",
       "        (4): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=136, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_10_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=136, bias=True)\n",
       "        (1): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=136, out_features=136, bias=True)\n",
       "        (4): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=136, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_11): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=264, bias=True)\n",
       "        (1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=264, out_features=264, bias=True)\n",
       "        (4): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=264, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_11_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=264, bias=True)\n",
       "        (1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=264, out_features=264, bias=True)\n",
       "        (4): LayerNorm((264,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=264, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_12): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=400, bias=True)\n",
       "        (1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (4): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=400, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_12_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "        (1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (4): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=400, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_13): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_13_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_14): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_14_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_15): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=144, bias=True)\n",
       "        (1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=144, out_features=144, bias=True)\n",
       "        (4): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=144, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_15_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=144, bias=True)\n",
       "        (1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=144, out_features=144, bias=True)\n",
       "        (4): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=144, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_16): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=272, bias=True)\n",
       "        (1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=272, out_features=272, bias=True)\n",
       "        (4): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=272, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_16_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=272, bias=True)\n",
       "        (1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=272, out_features=272, bias=True)\n",
       "        (4): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=272, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_17): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=408, bias=True)\n",
       "        (1): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=408, out_features=408, bias=True)\n",
       "        (4): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=408, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_17_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=408, bias=True)\n",
       "        (1): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=408, out_features=408, bias=True)\n",
       "        (4): LayerNorm((408,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=408, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_18): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_18_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_19): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_19_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_20): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_20_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_21): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_21_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_22): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_22_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_23): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_23_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_24): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_24_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_25): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_25_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_26): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_26_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_27): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_27_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "    (global_element_5): StandardNormal()\n",
       "    (global_element_6): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=70, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASWUlEQVR4nO3df5DcdX3H8eeL4xI0gZrwI4YkErTxB2INeIA11kKpglFEa7VGi6GjEzuKP1qKpXY64kw7RauIUnEaCiYCDTJVCmOjgkGkaIm5YAxBkEQM5EgmIUTIj0Jyubz7x37DLJfd797tfne/e/d5PWZ2bnff+93ve/f2dd/vfX/sRxGBmY1/h5XdgJl1hsNulgiH3SwRDrtZIhx2s0Q47GaJcNjHGUmXSbqhyWlfIennknZJ+kTRvRVN0gck3V52H2OFw14QSW+U9FNJT0vaIeknkk4ru69R+jRwV0QcGRFfLbuZRiLixoh4S9l9jBUOewEkHQV8F7gKmArMAD4H7C2zryacADxQryipp4O95JJ0eAvTSlJyn/3kXnCbvBwgIpZFxFBEPBMRt0fEWgBJL5N0p6QnJW2XdKOkFx2cWNJGSZdIWitpj6RrJU2T9L1slfqHkqZkj50tKSQtkrRZ0hZJF9drTNLrszWOpyT9QtKZdR53J3AW8K+Sdkt6uaQlkr4uabmkPcBZkl4l6a7s+R6Q9I6q51gi6eqs793Z2s2LJV0p6beSHpJ0Sk6vIekTkh7J3qd/ORhKSRdmz/dlSTuAy7L77qma/g2SVmVrV6skvaGqdpekf5L0E+D/gJfm/kbHo4jwpcULcBTwJLAUeCswZVj9d4E3AxOBY4G7gSur6huBe4FpVNYKtgH3Aadk09wJfDZ77GwggGXAJOA1wBPAH2f1y4Absuszsr7mU/nD/ubs9rF1XsddwIerbi8BngbmZdMfCWwAPgNMAP4I2AW8ourx24HXAUdkff8G+CDQA/wj8KOc9zGAH1FZO3oJ8PDBfoALgf3Ax4HDgRdk992T1acCvwUuyOoLsttHV722x4BXZ/Xesj83nb54yV6AiNgJvJHKh/Ua4AlJt0maltU3RMQdEbE3Ip4ArgD+cNjTXBURWyPiceB/gJUR8fOI2AvcQiX41T4XEXsi4n7gG1Q+3MP9ObA8IpZHxIGIuAPopxL+kbo1In4SEQeAucBk4PKI2BcRd1L596V63rdExOqIeDbr+9mI+GZEDAHfqvE6hvt8ROyIiMeAK4c99+aIuCoi9kfEM8OmexuwPiKuz+rLgIeA86oesyQiHsjqg6N4D8YFh70gEfFgRFwYETOBk4HjqXxYkXScpJskPS5pJ3ADcMywp9hadf2ZGrcnD3v8pqrrj2bzG+4E4D3ZKvdTkp6i8kdp+iheWvV8jgc2ZcGvnveMqtujfR158xv+ujZR3/HZ46sN7y1v+nHPYW+DiHiIyirtydld/0xlqf97EXEUlSWuWpzNrKrrLwE213jMJuD6iHhR1WVSRFw+ivlUnxa5GZg1bOPWS4DHR/F8jeS9rrxTNDdT+eNWbXhvSZ/i6bAXQNIrJV0saWZ2exaV1c97s4ccCewGnpI0A7ikgNn+g6QXSno18BdUVpGHuwE4T9I5knokHSHpzIN9NmElsAf4tKTebGPfecBNTT5fLZdImpK9h5+k9uuqZTnwcknvl3S4pD8DTqLyb4bhsBdlF3AGsDLban0vsA44uJX8c8CpVDZ2/TfwnQLm+WMqG8tWAF+MiEMOLomITcD5VDaoPUFlSX8JTf7eI2If8A4qGyG3A1cDH8zWZIpyK7AaWEPlvbp2hL09Cbydynv+JJVjBt4eEdsL7G1MU7al0sYISbOpbOHujYj95XZTLEkBzImIDWX3Mh55yW6WCIfdLBFejTdLhJfsZolo+mSCZkzQxDiCSZ2cpVlSnmUP+2JvzWM4Wgq7pHOBr1A57vnfGx2scQSTOENntzJLM8uxMlbUrTW9Gp+d7vg1KvtcTwIWSDqp2eczs/Zq5X/204ENEfFIdrDFTVQO4DCzLtRK2Gfw/BMLBnj+SQcAZOdd90vqHxxz3+VgNn60EvZaGwEO2Y8XEYsjoi8i+nqZ2MLszKwVrYR9gOefoTST2mdemVkXaCXsq4A5kk6UNAF4H3BbMW2ZWdGa3vUWEfslXQT8gMqut+siou6XFZpZuVrazx4Ry6mcR2xmXc6Hy5olwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSI6+lXS1h6HnfzKurVfv39K7rT7j9+XW7/0jO/l1v9k8vrc+i8H6391+N9d+pe5006++d7cuo2Ol+xmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIUccggLm1zlKaGR3GtQTVH2H3O8oHVufUejc+/2b8Z3J1b/9hr35ZbH3rq6SLbGRNWxgp2xo6aH6jx+Skxs0M47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRPp+9C3x3oD+33qOeDnXSXU7snZxbv/WBO3Pr583+/bq1GMw/j388ainskjYCu4AhYH9E9BXRlJkVr4gl+1kRsb2A5zGzNvL/7GaJaDXsAdwuabWkRbUeIGmRpH5J/YPsbXF2ZtasVlfj50XEZknHAXdIeigi7q5+QEQsBhZD5USYFudnZk1qackeEZuzn9uAW4DTi2jKzIrXdNglTZJ05MHrwFuAdUU1ZmbFamU1fhpwiyrnYh8O/EdEfL+QrsaZbw/kf/95r47oUCejt3bfs7n1P132V7n1yY/Wr+160zO50/7sD67OrU9U/sd366L6e4KP+9pPc6cdj5oOe0Q8Ary2wF7MrI28680sEQ67WSIcdrNEOOxmiXDYzRLhU1wL0HPM0bn1yYe1d9faUByoW5s/83X5E7f4VeIn8r9NT3vs4vxTd0/7wl/n1v/tXdfk1ifs8gGb1bxkN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4f3sBVi+dkWp858/49Scahfva845PgDgqA35Q1lfceZbc+sv2tT8MQDjkZfsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kivJ99DHj1VR/Nrc+ke78WWYfX/4itX/Ka3Gmn/1f+fvj9mwaa6ilVXrKbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZonwfvYxYNYVq3PrZZ6xvuHK1+fW+999Rd3a+x5+cf6T/6f3oxep4ZJd0nWStklaV3XfVEl3SFqf/ZzS3jbNrFUjWY1fApw77L5LgRURMQdYkd02sy7WMOwRcTewY9jd5wNLs+tLgXcW3JeZFazZDXTTImILQPbzuHoPlLRIUr+k/kH2Njk7M2tV27fGR8TiiOiLiL5eJrZ7dmZWR7Nh3yppOkD2c1txLZlZOzQb9tuAhdn1hcCtxbRjZu3ScD+7pGXAmcAxkgaAzwKXAzdL+hDwGPCedjbZ7c5pMAb69Y/enVv//p4TcuuHTZ6UWx/a28K2EOV/N/vNm/LPlf+dw9bk1p8+kPP8Z3s/eic1DHtELKhTOrvgXsysjXy4rFkiHHazRDjsZolw2M0S4bCbJcKnuBbhwFBu+YJZ81p7/sOezi33HD21bm1g4Stzp137N1c3mPkLGtTzLTjpnJzqzpae20bHS3azRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBHezz4WNNiPPzRnZt1a4/3o7TW00/vSu4WX7GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIryffRx49zd+WNq8T/7KR3PrM8j/KmrrHC/ZzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEKCI6NrOjNDXOkAd/Ldp3H19dt9arnrbOeygO5Nbnzzi1rfO351sZK9gZO2qOk91wyS7pOknbJK2ruu8ySY9LWpNd5hfZsJkVbySr8UuAc2vc/+WImJtdlhfblpkVrWHYI+JuYEcHejGzNmplA91FktZmq/lT6j1I0iJJ/ZL6B9nbwuzMrBXNhv3rwMuAucAW4Ev1HhgRiyOiLyL6epnY5OzMrFVNhT0itkbEUEQcAK4BTi+2LTMrWlNhlzS96ua7gHX1Hmtm3aHh+eySlgFnAsdIGgA+C5wpaS4QwEbgI23sMXkPX3Nabr1XazrUyaF6lL+8uHJj/fPZPzX7DUW3Yzkahj0iFtS4+9o29GJmbeTDZc0S4bCbJcJhN0uEw26WCIfdLBH+KulOUM0zDp+zfKD+KaoAPSXuWmvVqya8sG7tB5vzX9f2oT259Q/MmtdUT6nykt0sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4T3s3dA4/3o/ptbyzE9k3LrjfbTnzPzdfWLB4aaaWlM86fMLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uE97N3gPejl+MHOcc3DEb+fva3z8jZRz9G+VNolgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyViJEM2zwK+CbwYOAAsjoivSJoKfAuYTWXY5vdGxG/b1+rYNRQHcuveD995verJrc9ZNTG3vv60vUW20xEj+ZTtBy6OiFcBrwc+Jukk4FJgRUTMAVZkt82sSzUMe0RsiYj7suu7gAeBGcD5wNLsYUuBd7arSTNr3ajWHyXNBk4BVgLTImILVP4gAMcV3ZyZFWfEYZc0Gfg28KmI2DmK6RZJ6pfUP8jY+z/HbLwYUdgl9VIJ+o0R8Z3s7q2Spmf16cC2WtNGxOKI6IuIvl7yN3qYWfs0DLskAdcCD0bEFVWl24CF2fWFwK3Ft2dmRRnJKa7zgAuA+6Xnxg7+DHA5cLOkDwGPAe9pT4tj3/wZp+bWG30lshXv14O7c+trPj83tz5JP8ufQcRoW2q7hmGPiHuAegOMn11sO2bWLj6awywRDrtZIhx2s0Q47GaJcNjNEuGwmyXCXyXdBc45Pn+f7iv6e3PrXz1+VdPzbvSVyg8P7sutf/zDF+XWJ/z4/lH3dFAMNRhWuY3DLk9iZdueuyxespslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmifB+9jHgV32DufVzyN9P30691B8WGaD7zupOl5fsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiGoZd0ixJP5L0oKQHJH0yu/8ySY9LWpNd5re/XTNr1ki+vGI/cHFE3CfpSGC1pDuy2pcj4ovta8/MitIw7BGxBdiSXd8l6UFgRrsbM7Nijep/dkmzgVPgubFxLpK0VtJ1kqbUmWaRpH5J/YPsbalZM2veiMMuaTLwbeBTEbET+DrwMmAulSX/l2pNFxGLI6IvIvp6mVhAy2bWjBGFXVIvlaDfGBHfAYiIrRExFBEHgGuA09vXppm1aiRb4wVcCzwYEVdU3T+96mHvAtYV356ZFWUkW+PnARcA90tak933GWCBpLlUvi14I/CRtnRoZoUYydb4ewDVKC0vvh0zaxcfQWeWCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0SoYjo3MykJ4BHq+46BtjesQZGp1t769a+wL01q8jeToiIY2sVOhr2Q2Yu9UdEX2kN5OjW3rq1L3BvzepUb16NN0uEw26WiLLDvrjk+efp1t66tS9wb83qSG+l/s9uZp1T9pLdzDrEYTdLRClhl3SupF9J2iDp0jJ6qEfSRkn3Z8NQ95fcy3WStklaV3XfVEl3SFqf/aw5xl5JvXXFMN45w4yX+t6VPfx5x/9nl9QDPAy8GRgAVgELIuKXHW2kDkkbgb6IKP0ADElvAnYD34yIk7P7vgDsiIjLsz+UUyLib7ukt8uA3WUP452NVjS9ephx4J3AhZT43uX09V468L6VsWQ/HdgQEY9ExD7gJuD8EvroehFxN7Bj2N3nA0uz60upfFg6rk5vXSEitkTEfdn1XcDBYcZLfe9y+uqIMsI+A9hUdXuA7hrvPYDbJa2WtKjsZmqYFhFboPLhAY4ruZ/hGg7j3UnDhhnvmveumeHPW1VG2GsNJdVN+//mRcSpwFuBj2WrqzYyIxrGu1NqDDPeFZod/rxVZYR9AJhVdXsmsLmEPmqKiM3Zz23ALXTfUNRbD46gm/3cVnI/z+mmYbxrDTNOF7x3ZQ5/XkbYVwFzJJ0oaQLwPuC2Evo4hKRJ2YYTJE0C3kL3DUV9G7Awu74QuLXEXp6nW4bxrjfMOCW/d6UPfx4RHb8A86lskf818Pdl9FCnr5cCv8guD5TdG7CMymrdIJU1og8BRwMrgPXZz6ld1Nv1wP3AWirBml5Sb2+k8q/hWmBNdplf9nuX01dH3jcfLmuWCB9BZ5YIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJsl4v8Bqwq0ctUzZ8cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    path.draw()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
