{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f1bd7c2af10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)\n",
    "model.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/54000 (0%)] Loss: 2063.624023\n",
      "Train Epoch: 1 [1408/54000 (3%)] Loss: 8921.085938\n",
      "Train Epoch: 1 [2816/54000 (5%)] Loss: 15598.289062\n",
      "Train Epoch: 1 [4224/54000 (8%)] Loss: 10733.007812\n",
      "Train Epoch: 1 [5632/54000 (10%)] Loss: -11413.138672\n",
      "Train Epoch: 1 [7040/54000 (13%)] Loss: -12542.569336\n",
      "Train Epoch: 1 [8448/54000 (16%)] Loss: -22791.085938\n",
      "Train Epoch: 1 [9856/54000 (18%)] Loss: -26923.623047\n",
      "Train Epoch: 1 [11264/54000 (21%)] Loss: -31148.582031\n",
      "Train Epoch: 1 [12672/54000 (23%)] Loss: -32740.105469\n",
      "Train Epoch: 1 [14080/54000 (26%)] Loss: -36089.082031\n",
      "Train Epoch: 1 [15488/54000 (29%)] Loss: -45486.535156\n",
      "Train Epoch: 1 [16896/54000 (31%)] Loss: -43786.187500\n",
      "Train Epoch: 1 [18304/54000 (34%)] Loss: -47339.839844\n",
      "Train Epoch: 1 [19712/54000 (37%)] Loss: -50386.960938\n",
      "Train Epoch: 1 [21120/54000 (39%)] Loss: -62214.902344\n",
      "Train Epoch: 1 [22528/54000 (42%)] Loss: -59423.292969\n",
      "Train Epoch: 1 [23936/54000 (44%)] Loss: -62661.457031\n",
      "Train Epoch: 1 [25344/54000 (47%)] Loss: -70153.039062\n",
      "Train Epoch: 1 [26752/54000 (50%)] Loss: -66147.468750\n",
      "Train Epoch: 1 [28160/54000 (52%)] Loss: -69557.343750\n",
      "Train Epoch: 1 [29568/54000 (55%)] Loss: -73240.328125\n",
      "Train Epoch: 1 [30976/54000 (57%)] Loss: -72381.476562\n",
      "Train Epoch: 1 [32384/54000 (60%)] Loss: -75667.414062\n",
      "Train Epoch: 1 [33792/54000 (63%)] Loss: -83975.929688\n",
      "Train Epoch: 1 [35200/54000 (65%)] Loss: -92535.171875\n",
      "Train Epoch: 1 [36608/54000 (68%)] Loss: -95907.171875\n",
      "Train Epoch: 1 [38016/54000 (70%)] Loss: -59954.718750\n",
      "Train Epoch: 1 [39424/54000 (73%)] Loss: -76661.968750\n",
      "Train Epoch: 1 [40832/54000 (76%)] Loss: -52022.500000\n",
      "Train Epoch: 1 [42240/54000 (78%)] Loss: -87023.179688\n",
      "Train Epoch: 1 [43648/54000 (81%)] Loss: -95905.289062\n",
      "Train Epoch: 1 [45056/54000 (83%)] Loss: -74640.609375\n",
      "Train Epoch: 1 [46464/54000 (86%)] Loss: -83309.929688\n",
      "Train Epoch: 1 [47872/54000 (89%)] Loss: -97222.445312\n",
      "Train Epoch: 1 [49280/54000 (91%)] Loss: -98935.804688\n",
      "Train Epoch: 1 [50688/54000 (94%)] Loss: -100607.906250\n",
      "Train Epoch: 1 [52096/54000 (96%)] Loss: -86848.773438\n",
      "    epoch          : 1\n",
      "    loss           : -55132.64450555099\n",
      "    val_loss       : -99091.25693121189\n",
      "Train Epoch: 2 [0/54000 (0%)] Loss: -99643.695312\n",
      "Train Epoch: 2 [1408/54000 (3%)] Loss: -102882.640625\n",
      "Train Epoch: 2 [2816/54000 (5%)] Loss: -90349.203125\n",
      "Train Epoch: 2 [4224/54000 (8%)] Loss: -109409.445312\n",
      "Train Epoch: 2 [5632/54000 (10%)] Loss: -99667.695312\n",
      "Train Epoch: 2 [7040/54000 (13%)] Loss: -118147.906250\n",
      "Train Epoch: 2 [8448/54000 (16%)] Loss: -101712.328125\n",
      "Train Epoch: 2 [9856/54000 (18%)] Loss: -89442.304688\n",
      "Train Epoch: 2 [11264/54000 (21%)] Loss: -121840.523438\n",
      "Train Epoch: 2 [12672/54000 (23%)] Loss: -119731.953125\n",
      "Train Epoch: 2 [14080/54000 (26%)] Loss: -122226.796875\n",
      "Train Epoch: 2 [15488/54000 (29%)] Loss: -115344.468750\n",
      "Train Epoch: 2 [16896/54000 (31%)] Loss: -118427.171875\n",
      "Train Epoch: 2 [18304/54000 (34%)] Loss: -98196.609375\n",
      "Train Epoch: 2 [19712/54000 (37%)] Loss: -137948.000000\n",
      "Train Epoch: 2 [21120/54000 (39%)] Loss: -96426.546875\n",
      "Train Epoch: 2 [22528/54000 (42%)] Loss: -121436.257812\n",
      "Train Epoch: 2 [23936/54000 (44%)] Loss: -84716.828125\n",
      "Train Epoch: 2 [25344/54000 (47%)] Loss: -111815.406250\n",
      "Train Epoch: 2 [26752/54000 (50%)] Loss: -133167.718750\n",
      "Train Epoch: 2 [28160/54000 (52%)] Loss: -104127.312500\n",
      "Train Epoch: 2 [29568/54000 (55%)] Loss: -118472.046875\n",
      "Train Epoch: 2 [30976/54000 (57%)] Loss: -131940.875000\n",
      "Train Epoch: 2 [32384/54000 (60%)] Loss: -134586.515625\n",
      "Train Epoch: 2 [33792/54000 (63%)] Loss: -128207.976562\n",
      "Train Epoch: 2 [35200/54000 (65%)] Loss: -150324.546875\n",
      "Train Epoch: 2 [36608/54000 (68%)] Loss: -105301.679688\n",
      "Train Epoch: 2 [38016/54000 (70%)] Loss: -124165.187500\n",
      "Train Epoch: 2 [39424/54000 (73%)] Loss: -142662.500000\n",
      "Train Epoch: 2 [40832/54000 (76%)] Loss: -139253.234375\n",
      "Train Epoch: 2 [42240/54000 (78%)] Loss: -142368.375000\n",
      "Train Epoch: 2 [43648/54000 (81%)] Loss: -136152.406250\n",
      "Train Epoch: 2 [45056/54000 (83%)] Loss: -133794.250000\n",
      "Train Epoch: 2 [46464/54000 (86%)] Loss: -156850.859375\n",
      "Train Epoch: 2 [47872/54000 (89%)] Loss: -129912.148438\n",
      "Train Epoch: 2 [49280/54000 (91%)] Loss: -121935.812500\n",
      "Train Epoch: 2 [50688/54000 (94%)] Loss: -115901.578125\n",
      "Train Epoch: 2 [52096/54000 (96%)] Loss: -129872.570312\n",
      "    epoch          : 2\n",
      "    loss           : -116828.3778222189\n",
      "    val_loss       : -135317.0528772866\n",
      "Train Epoch: 3 [0/54000 (0%)] Loss: -134053.640625\n",
      "Train Epoch: 3 [1408/54000 (3%)] Loss: -109071.929688\n",
      "Train Epoch: 3 [2816/54000 (5%)] Loss: -138364.156250\n",
      "Train Epoch: 3 [4224/54000 (8%)] Loss: -96991.125000\n",
      "Train Epoch: 3 [5632/54000 (10%)] Loss: -141163.265625\n",
      "Train Epoch: 3 [7040/54000 (13%)] Loss: -132889.906250\n",
      "Train Epoch: 3 [8448/54000 (16%)] Loss: -143238.875000\n",
      "Train Epoch: 3 [9856/54000 (18%)] Loss: -128454.773438\n",
      "Train Epoch: 3 [11264/54000 (21%)] Loss: -151990.453125\n",
      "Train Epoch: 3 [12672/54000 (23%)] Loss: -127107.726562\n",
      "Train Epoch: 3 [14080/54000 (26%)] Loss: -147221.500000\n",
      "Train Epoch: 3 [15488/54000 (29%)] Loss: -150360.968750\n",
      "Train Epoch: 3 [16896/54000 (31%)] Loss: -127184.890625\n",
      "Train Epoch: 3 [18304/54000 (34%)] Loss: -116162.937500\n",
      "Train Epoch: 3 [19712/54000 (37%)] Loss: -168740.859375\n",
      "Train Epoch: 3 [21120/54000 (39%)] Loss: -156329.843750\n",
      "Train Epoch: 3 [22528/54000 (42%)] Loss: -146049.968750\n",
      "Train Epoch: 3 [23936/54000 (44%)] Loss: -166254.734375\n",
      "Train Epoch: 3 [25344/54000 (47%)] Loss: -136642.281250\n",
      "Train Epoch: 3 [26752/54000 (50%)] Loss: -126851.304688\n",
      "Train Epoch: 3 [28160/54000 (52%)] Loss: -128024.468750\n",
      "Train Epoch: 3 [29568/54000 (55%)] Loss: -130795.523438\n",
      "Train Epoch: 3 [30976/54000 (57%)] Loss: -128309.640625\n",
      "Train Epoch: 3 [32384/54000 (60%)] Loss: -130559.250000\n",
      "Train Epoch: 3 [33792/54000 (63%)] Loss: -150196.515625\n",
      "Train Epoch: 3 [35200/54000 (65%)] Loss: -159247.234375\n",
      "Train Epoch: 3 [36608/54000 (68%)] Loss: -174609.656250\n",
      "Train Epoch: 3 [38016/54000 (70%)] Loss: -163363.734375\n",
      "Train Epoch: 3 [39424/54000 (73%)] Loss: -150494.890625\n",
      "Train Epoch: 3 [40832/54000 (76%)] Loss: -162859.468750\n",
      "Train Epoch: 3 [42240/54000 (78%)] Loss: -164393.234375\n",
      "Train Epoch: 3 [43648/54000 (81%)] Loss: -133223.718750\n",
      "Train Epoch: 3 [45056/54000 (83%)] Loss: -151899.031250\n",
      "Train Epoch: 3 [46464/54000 (86%)] Loss: -150744.312500\n",
      "Train Epoch: 3 [47872/54000 (89%)] Loss: -136281.265625\n",
      "Train Epoch: 3 [49280/54000 (91%)] Loss: -144661.468750\n",
      "Train Epoch: 3 [50688/54000 (94%)] Loss: -155996.281250\n",
      "Train Epoch: 3 [52096/54000 (96%)] Loss: -145921.406250\n",
      "    epoch          : 3\n",
      "    loss           : -142061.82915296053\n",
      "    val_loss       : -156151.76807831554\n",
      "Train Epoch: 4 [0/54000 (0%)] Loss: -152519.000000\n",
      "Train Epoch: 4 [1408/54000 (3%)] Loss: -181315.562500\n",
      "Train Epoch: 4 [2816/54000 (5%)] Loss: -144019.734375\n",
      "Train Epoch: 4 [4224/54000 (8%)] Loss: -163547.281250\n",
      "Train Epoch: 4 [5632/54000 (10%)] Loss: -136593.812500\n",
      "Train Epoch: 4 [7040/54000 (13%)] Loss: -163768.687500\n",
      "Train Epoch: 4 [8448/54000 (16%)] Loss: -175782.046875\n",
      "Train Epoch: 4 [9856/54000 (18%)] Loss: -184296.375000\n",
      "Train Epoch: 4 [11264/54000 (21%)] Loss: -137353.875000\n",
      "Train Epoch: 4 [12672/54000 (23%)] Loss: -148135.843750\n",
      "Train Epoch: 4 [14080/54000 (26%)] Loss: -141659.375000\n",
      "Train Epoch: 4 [15488/54000 (29%)] Loss: -148655.187500\n",
      "Train Epoch: 4 [16896/54000 (31%)] Loss: -146298.062500\n",
      "Train Epoch: 4 [18304/54000 (34%)] Loss: -151054.765625\n",
      "Train Epoch: 4 [19712/54000 (37%)] Loss: -187242.703125\n",
      "Train Epoch: 4 [21120/54000 (39%)] Loss: -164370.000000\n",
      "Train Epoch: 4 [22528/54000 (42%)] Loss: -172932.468750\n",
      "Train Epoch: 4 [23936/54000 (44%)] Loss: -156239.734375\n",
      "Train Epoch: 4 [25344/54000 (47%)] Loss: -141029.234375\n",
      "Train Epoch: 4 [26752/54000 (50%)] Loss: -189521.906250\n",
      "Train Epoch: 4 [28160/54000 (52%)] Loss: -140754.343750\n",
      "Train Epoch: 4 [29568/54000 (55%)] Loss: -141487.859375\n",
      "Train Epoch: 4 [30976/54000 (57%)] Loss: -160328.046875\n",
      "Train Epoch: 4 [32384/54000 (60%)] Loss: -154068.218750\n",
      "Train Epoch: 4 [33792/54000 (63%)] Loss: -163997.375000\n",
      "Train Epoch: 4 [35200/54000 (65%)] Loss: -163708.750000\n",
      "Train Epoch: 4 [36608/54000 (68%)] Loss: -148039.859375\n",
      "Train Epoch: 4 [38016/54000 (70%)] Loss: -174376.187500\n",
      "Train Epoch: 4 [39424/54000 (73%)] Loss: -192162.437500\n",
      "Train Epoch: 4 [40832/54000 (76%)] Loss: -154210.359375\n",
      "Train Epoch: 4 [42240/54000 (78%)] Loss: -152677.015625\n",
      "Train Epoch: 4 [43648/54000 (81%)] Loss: -168273.453125\n",
      "Train Epoch: 4 [45056/54000 (83%)] Loss: -151256.062500\n",
      "Train Epoch: 4 [46464/54000 (86%)] Loss: -151791.343750\n",
      "Train Epoch: 4 [47872/54000 (89%)] Loss: -144128.953125\n",
      "Train Epoch: 4 [49280/54000 (91%)] Loss: -164852.843750\n",
      "Train Epoch: 4 [50688/54000 (94%)] Loss: -194174.468750\n",
      "Train Epoch: 4 [52096/54000 (96%)] Loss: -174049.156250\n",
      "    epoch          : 4\n",
      "    loss           : -157434.9934771232\n",
      "    val_loss       : -167842.61892625762\n",
      "Train Epoch: 5 [0/54000 (0%)] Loss: -159158.031250\n",
      "Train Epoch: 5 [1408/54000 (3%)] Loss: -125518.953125\n",
      "Train Epoch: 5 [2816/54000 (5%)] Loss: -195327.906250\n",
      "Train Epoch: 5 [4224/54000 (8%)] Loss: -177151.406250\n",
      "Train Epoch: 5 [5632/54000 (10%)] Loss: -144221.671875\n",
      "Train Epoch: 5 [7040/54000 (13%)] Loss: -171829.906250\n",
      "Train Epoch: 5 [8448/54000 (16%)] Loss: -175601.000000\n",
      "Train Epoch: 5 [9856/54000 (18%)] Loss: -173496.343750\n",
      "Train Epoch: 5 [11264/54000 (21%)] Loss: -176059.578125\n",
      "Train Epoch: 5 [12672/54000 (23%)] Loss: -172842.750000\n",
      "Train Epoch: 5 [14080/54000 (26%)] Loss: -174379.937500\n",
      "Train Epoch: 5 [15488/54000 (29%)] Loss: -176533.171875\n",
      "Train Epoch: 5 [16896/54000 (31%)] Loss: -145142.250000\n",
      "Train Epoch: 5 [18304/54000 (34%)] Loss: -176964.250000\n",
      "Train Epoch: 5 [19712/54000 (37%)] Loss: -176169.171875\n",
      "Train Epoch: 5 [21120/54000 (39%)] Loss: -162339.218750\n",
      "Train Epoch: 5 [22528/54000 (42%)] Loss: -164474.968750\n",
      "Train Epoch: 5 [23936/54000 (44%)] Loss: -197065.812500\n",
      "Train Epoch: 5 [25344/54000 (47%)] Loss: -197657.968750\n",
      "Train Epoch: 5 [26752/54000 (50%)] Loss: -177537.343750\n",
      "Train Epoch: 5 [28160/54000 (52%)] Loss: -159954.000000\n",
      "Train Epoch: 5 [29568/54000 (55%)] Loss: -152393.078125\n",
      "Train Epoch: 5 [30976/54000 (57%)] Loss: -164267.093750\n",
      "Train Epoch: 5 [32384/54000 (60%)] Loss: -168921.640625\n",
      "Train Epoch: 5 [33792/54000 (63%)] Loss: -177745.156250\n",
      "Train Epoch: 5 [35200/54000 (65%)] Loss: -157585.375000\n",
      "Train Epoch: 5 [36608/54000 (68%)] Loss: -174813.640625\n",
      "Train Epoch: 5 [38016/54000 (70%)] Loss: -178241.828125\n",
      "Train Epoch: 5 [39424/54000 (73%)] Loss: -178313.734375\n",
      "Train Epoch: 5 [40832/54000 (76%)] Loss: -169993.625000\n",
      "Train Epoch: 5 [42240/54000 (78%)] Loss: -181003.546875\n",
      "Train Epoch: 5 [43648/54000 (81%)] Loss: -158640.156250\n",
      "Train Epoch: 5 [45056/54000 (83%)] Loss: -165994.968750\n",
      "Train Epoch: 5 [46464/54000 (86%)] Loss: -156641.125000\n",
      "Train Epoch: 5 [47872/54000 (89%)] Loss: -159245.031250\n",
      "Train Epoch: 5 [49280/54000 (91%)] Loss: -170002.656250\n",
      "Train Epoch: 5 [50688/54000 (94%)] Loss: -182936.875000\n",
      "Train Epoch: 5 [52096/54000 (96%)] Loss: -145651.375000\n",
      "    epoch          : 5\n",
      "    loss           : -166531.46925463516\n",
      "    val_loss       : -171437.1492711509\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 6 [0/54000 (0%)] Loss: -199374.453125\n",
      "Train Epoch: 6 [1408/54000 (3%)] Loss: -180707.656250\n",
      "Train Epoch: 6 [2816/54000 (5%)] Loss: -169887.468750\n",
      "Train Epoch: 6 [4224/54000 (8%)] Loss: -169534.578125\n",
      "Train Epoch: 6 [5632/54000 (10%)] Loss: -161374.765625\n",
      "Train Epoch: 6 [7040/54000 (13%)] Loss: -163709.625000\n",
      "Train Epoch: 6 [8448/54000 (16%)] Loss: -179610.218750\n",
      "Train Epoch: 6 [9856/54000 (18%)] Loss: -186708.359375\n",
      "Train Epoch: 6 [11264/54000 (21%)] Loss: -155708.296875\n",
      "Train Epoch: 6 [12672/54000 (23%)] Loss: -134948.890625\n",
      "Train Epoch: 6 [14080/54000 (26%)] Loss: -155953.125000\n",
      "Train Epoch: 6 [15488/54000 (29%)] Loss: -178762.796875\n",
      "Train Epoch: 6 [16896/54000 (31%)] Loss: -146196.812500\n",
      "Train Epoch: 6 [18304/54000 (34%)] Loss: -158703.437500\n",
      "Train Epoch: 6 [19712/54000 (37%)] Loss: -198039.656250\n",
      "Train Epoch: 6 [21120/54000 (39%)] Loss: -149305.375000\n",
      "Train Epoch: 6 [22528/54000 (42%)] Loss: -181686.218750\n",
      "Train Epoch: 6 [23936/54000 (44%)] Loss: -182623.437500\n",
      "Train Epoch: 6 [25344/54000 (47%)] Loss: -178383.625000\n",
      "Train Epoch: 6 [26752/54000 (50%)] Loss: -148144.515625\n",
      "Train Epoch: 6 [28160/54000 (52%)] Loss: -165331.218750\n",
      "Train Epoch: 6 [29568/54000 (55%)] Loss: -193672.750000\n",
      "Train Epoch: 6 [30976/54000 (57%)] Loss: -179883.234375\n",
      "Train Epoch: 6 [32384/54000 (60%)] Loss: -166179.296875\n",
      "Train Epoch: 6 [33792/54000 (63%)] Loss: -153383.468750\n",
      "Train Epoch: 6 [35200/54000 (65%)] Loss: -182816.109375\n",
      "Train Epoch: 6 [36608/54000 (68%)] Loss: -199962.593750\n",
      "Train Epoch: 6 [38016/54000 (70%)] Loss: -170730.109375\n",
      "Train Epoch: 6 [39424/54000 (73%)] Loss: -175242.875000\n",
      "Train Epoch: 6 [40832/54000 (76%)] Loss: -144627.484375\n",
      "Train Epoch: 6 [42240/54000 (78%)] Loss: -138972.218750\n",
      "Train Epoch: 6 [43648/54000 (81%)] Loss: -142957.515625\n",
      "Train Epoch: 6 [45056/54000 (83%)] Loss: -181242.796875\n",
      "Train Epoch: 6 [46464/54000 (86%)] Loss: -172189.687500\n",
      "Train Epoch: 6 [47872/54000 (89%)] Loss: -164208.062500\n",
      "Train Epoch: 6 [49280/54000 (91%)] Loss: -176047.609375\n",
      "Train Epoch: 6 [50688/54000 (94%)] Loss: -166051.921875\n",
      "Train Epoch: 6 [52096/54000 (96%)] Loss: -182472.312500\n",
      "    epoch          : 6\n",
      "    loss           : -170710.25684061006\n",
      "    val_loss       : -171966.8934117759\n",
      "Train Epoch: 7 [0/54000 (0%)] Loss: -151631.421875\n",
      "Train Epoch: 7 [1408/54000 (3%)] Loss: -166733.906250\n",
      "Train Epoch: 7 [2816/54000 (5%)] Loss: -164179.484375\n",
      "Train Epoch: 7 [4224/54000 (8%)] Loss: -179472.937500\n",
      "Train Epoch: 7 [5632/54000 (10%)] Loss: -182345.125000\n",
      "Train Epoch: 7 [7040/54000 (13%)] Loss: -171826.703125\n",
      "Train Epoch: 7 [8448/54000 (16%)] Loss: -204264.671875\n",
      "Train Epoch: 7 [9856/54000 (18%)] Loss: -183937.359375\n",
      "Train Epoch: 7 [11264/54000 (21%)] Loss: -175408.578125\n",
      "Train Epoch: 7 [12672/54000 (23%)] Loss: -194129.375000\n",
      "Train Epoch: 7 [14080/54000 (26%)] Loss: -178495.421875\n",
      "Train Epoch: 7 [15488/54000 (29%)] Loss: -178326.281250\n",
      "Train Epoch: 7 [16896/54000 (31%)] Loss: -178469.812500\n",
      "Train Epoch: 7 [18304/54000 (34%)] Loss: -142844.312500\n",
      "Train Epoch: 7 [19712/54000 (37%)] Loss: -170874.375000\n",
      "Train Epoch: 7 [21120/54000 (39%)] Loss: -170206.203125\n",
      "Train Epoch: 7 [22528/54000 (42%)] Loss: -152326.453125\n",
      "Train Epoch: 7 [23936/54000 (44%)] Loss: -178658.828125\n",
      "Train Epoch: 7 [25344/54000 (47%)] Loss: -183324.109375\n",
      "Train Epoch: 7 [26752/54000 (50%)] Loss: -177038.718750\n",
      "Train Epoch: 7 [28160/54000 (52%)] Loss: -164409.875000\n",
      "Train Epoch: 7 [29568/54000 (55%)] Loss: -180976.312500\n",
      "Train Epoch: 7 [30976/54000 (57%)] Loss: -202648.890625\n",
      "Train Epoch: 7 [32384/54000 (60%)] Loss: -181134.375000\n",
      "Train Epoch: 7 [33792/54000 (63%)] Loss: -155387.734375\n",
      "Train Epoch: 7 [35200/54000 (65%)] Loss: -182732.796875\n",
      "Train Epoch: 7 [36608/54000 (68%)] Loss: -203552.875000\n",
      "Train Epoch: 7 [38016/54000 (70%)] Loss: -173648.468750\n",
      "Train Epoch: 7 [39424/54000 (73%)] Loss: -183452.187500\n",
      "Train Epoch: 7 [40832/54000 (76%)] Loss: -169580.015625\n",
      "Train Epoch: 7 [42240/54000 (78%)] Loss: -148249.156250\n",
      "Train Epoch: 7 [43648/54000 (81%)] Loss: -172375.156250\n",
      "Train Epoch: 7 [45056/54000 (83%)] Loss: -171611.953125\n",
      "Train Epoch: 7 [46464/54000 (86%)] Loss: -174357.750000\n",
      "Train Epoch: 7 [47872/54000 (89%)] Loss: -173874.609375\n",
      "Train Epoch: 7 [49280/54000 (91%)] Loss: -174737.609375\n",
      "Train Epoch: 7 [50688/54000 (94%)] Loss: -168999.781250\n",
      "Train Epoch: 7 [52096/54000 (96%)] Loss: -166230.515625\n",
      "    epoch          : 7\n",
      "    loss           : -173694.94093899522\n",
      "    val_loss       : -177535.30006669209\n",
      "Train Epoch: 8 [0/54000 (0%)] Loss: -205083.546875\n",
      "Train Epoch: 8 [1408/54000 (3%)] Loss: -194431.468750\n",
      "Train Epoch: 8 [2816/54000 (5%)] Loss: -173412.671875\n",
      "Train Epoch: 8 [4224/54000 (8%)] Loss: -181852.890625\n",
      "Train Epoch: 8 [5632/54000 (10%)] Loss: -154943.562500\n",
      "Train Epoch: 8 [7040/54000 (13%)] Loss: -181705.187500\n",
      "Train Epoch: 8 [8448/54000 (16%)] Loss: -175786.656250\n",
      "Train Epoch: 8 [9856/54000 (18%)] Loss: -196977.046875\n",
      "Train Epoch: 8 [11264/54000 (21%)] Loss: -188048.359375\n",
      "Train Epoch: 8 [12672/54000 (23%)] Loss: -179457.031250\n",
      "Train Epoch: 8 [14080/54000 (26%)] Loss: -173490.031250\n",
      "Train Epoch: 8 [15488/54000 (29%)] Loss: -178414.562500\n",
      "Train Epoch: 8 [16896/54000 (31%)] Loss: -168013.046875\n",
      "Train Epoch: 8 [18304/54000 (34%)] Loss: -179764.953125\n",
      "Train Epoch: 8 [19712/54000 (37%)] Loss: -139748.250000\n",
      "Train Epoch: 8 [21120/54000 (39%)] Loss: -175480.671875\n",
      "Train Epoch: 8 [22528/54000 (42%)] Loss: -205552.984375\n",
      "Train Epoch: 8 [23936/54000 (44%)] Loss: -175377.734375\n",
      "Train Epoch: 8 [25344/54000 (47%)] Loss: -178141.281250\n",
      "Train Epoch: 8 [26752/54000 (50%)] Loss: -179779.281250\n",
      "Train Epoch: 8 [28160/54000 (52%)] Loss: -185561.812500\n",
      "Train Epoch: 8 [29568/54000 (55%)] Loss: -166754.140625\n",
      "Train Epoch: 8 [30976/54000 (57%)] Loss: -181573.343750\n",
      "Train Epoch: 8 [32384/54000 (60%)] Loss: -182676.562500\n",
      "Train Epoch: 8 [33792/54000 (63%)] Loss: -157326.890625\n",
      "Train Epoch: 8 [35200/54000 (65%)] Loss: -181028.203125\n",
      "Train Epoch: 8 [36608/54000 (68%)] Loss: -170599.343750\n",
      "Train Epoch: 8 [38016/54000 (70%)] Loss: -173414.421875\n",
      "Train Epoch: 8 [39424/54000 (73%)] Loss: -173308.656250\n",
      "Train Epoch: 8 [40832/54000 (76%)] Loss: -182938.375000\n",
      "Train Epoch: 8 [42240/54000 (78%)] Loss: -175427.343750\n",
      "Train Epoch: 8 [43648/54000 (81%)] Loss: -171649.218750\n",
      "Train Epoch: 8 [45056/54000 (83%)] Loss: -168849.343750\n",
      "Train Epoch: 8 [46464/54000 (86%)] Loss: -177610.656250\n",
      "Train Epoch: 8 [47872/54000 (89%)] Loss: -177996.250000\n",
      "Train Epoch: 8 [49280/54000 (91%)] Loss: -176177.250000\n",
      "Train Epoch: 8 [50688/54000 (94%)] Loss: -205860.281250\n",
      "Train Epoch: 8 [52096/54000 (96%)] Loss: -177942.156250\n",
      "    epoch          : 8\n",
      "    loss           : -175657.78248355264\n",
      "    val_loss       : -179967.2762719131\n",
      "Train Epoch: 9 [0/54000 (0%)] Loss: -206301.734375\n",
      "Train Epoch: 9 [1408/54000 (3%)] Loss: -177621.687500\n",
      "Train Epoch: 9 [2816/54000 (5%)] Loss: -174192.046875\n",
      "Train Epoch: 9 [4224/54000 (8%)] Loss: -176962.781250\n",
      "Train Epoch: 9 [5632/54000 (10%)] Loss: -158759.000000\n",
      "Train Epoch: 9 [7040/54000 (13%)] Loss: -173671.625000\n",
      "Train Epoch: 9 [8448/54000 (16%)] Loss: -176229.765625\n",
      "Train Epoch: 9 [9856/54000 (18%)] Loss: -178004.281250\n",
      "Train Epoch: 9 [11264/54000 (21%)] Loss: -178353.812500\n",
      "Train Epoch: 9 [12672/54000 (23%)] Loss: -206521.718750\n",
      "Train Epoch: 9 [14080/54000 (26%)] Loss: -172914.281250\n",
      "Train Epoch: 9 [15488/54000 (29%)] Loss: -177403.125000\n",
      "Train Epoch: 9 [16896/54000 (31%)] Loss: -180111.000000\n",
      "Train Epoch: 9 [18304/54000 (34%)] Loss: -179740.265625\n",
      "Train Epoch: 9 [19712/54000 (37%)] Loss: -207392.984375\n",
      "Train Epoch: 9 [21120/54000 (39%)] Loss: -173396.125000\n",
      "Train Epoch: 9 [22528/54000 (42%)] Loss: -159900.812500\n",
      "Train Epoch: 9 [23936/54000 (44%)] Loss: -175356.468750\n",
      "Train Epoch: 9 [25344/54000 (47%)] Loss: -169780.593750\n",
      "Train Epoch: 9 [26752/54000 (50%)] Loss: -190159.437500\n",
      "Train Epoch: 9 [28160/54000 (52%)] Loss: -183378.781250\n",
      "Train Epoch: 9 [29568/54000 (55%)] Loss: -175959.343750\n",
      "Train Epoch: 9 [30976/54000 (57%)] Loss: -158382.468750\n",
      "Train Epoch: 9 [32384/54000 (60%)] Loss: -203909.078125\n",
      "Train Epoch: 9 [33792/54000 (63%)] Loss: -176196.500000\n",
      "Train Epoch: 9 [35200/54000 (65%)] Loss: -151334.406250\n",
      "Train Epoch: 9 [36608/54000 (68%)] Loss: -158006.921875\n",
      "Train Epoch: 9 [38016/54000 (70%)] Loss: -179965.812500\n",
      "Train Epoch: 9 [39424/54000 (73%)] Loss: -182506.125000\n",
      "Train Epoch: 9 [40832/54000 (76%)] Loss: -181853.125000\n",
      "Train Epoch: 9 [42240/54000 (78%)] Loss: -179240.859375\n",
      "Train Epoch: 9 [43648/54000 (81%)] Loss: -198459.812500\n",
      "Train Epoch: 9 [45056/54000 (83%)] Loss: -180564.203125\n",
      "Train Epoch: 9 [46464/54000 (86%)] Loss: -183886.265625\n",
      "Train Epoch: 9 [47872/54000 (89%)] Loss: -177778.593750\n",
      "Train Epoch: 9 [49280/54000 (91%)] Loss: -170889.656250\n",
      "Train Epoch: 9 [50688/54000 (94%)] Loss: -193790.296875\n",
      "Train Epoch: 9 [52096/54000 (96%)] Loss: -166587.250000\n",
      "    epoch          : 9\n",
      "    loss           : -177316.4277437201\n",
      "    val_loss       : -182491.13419397865\n",
      "Train Epoch: 10 [0/54000 (0%)] Loss: -172842.375000\n",
      "Train Epoch: 10 [1408/54000 (3%)] Loss: -165669.671875\n",
      "Train Epoch: 10 [2816/54000 (5%)] Loss: -170368.250000\n",
      "Train Epoch: 10 [4224/54000 (8%)] Loss: -184299.562500\n",
      "Train Epoch: 10 [5632/54000 (10%)] Loss: -177278.187500\n",
      "Train Epoch: 10 [7040/54000 (13%)] Loss: -173295.000000\n",
      "Train Epoch: 10 [8448/54000 (16%)] Loss: -195230.218750\n",
      "Train Epoch: 10 [9856/54000 (18%)] Loss: -179320.796875\n",
      "Train Epoch: 10 [11264/54000 (21%)] Loss: -184971.171875\n",
      "Train Epoch: 10 [12672/54000 (23%)] Loss: -180484.312500\n",
      "Train Epoch: 10 [14080/54000 (26%)] Loss: -184574.734375\n",
      "Train Epoch: 10 [15488/54000 (29%)] Loss: -158041.234375\n",
      "Train Epoch: 10 [16896/54000 (31%)] Loss: -167634.500000\n",
      "Train Epoch: 10 [18304/54000 (34%)] Loss: -175003.062500\n",
      "Train Epoch: 10 [19712/54000 (37%)] Loss: -176163.906250\n",
      "Train Epoch: 10 [21120/54000 (39%)] Loss: -160183.859375\n",
      "Train Epoch: 10 [22528/54000 (42%)] Loss: -156894.546875\n",
      "Train Epoch: 10 [23936/54000 (44%)] Loss: -175047.453125\n",
      "Train Epoch: 10 [25344/54000 (47%)] Loss: -184168.156250\n",
      "Train Epoch: 10 [26752/54000 (50%)] Loss: -183481.250000\n",
      "Train Epoch: 10 [28160/54000 (52%)] Loss: -178278.734375\n",
      "Train Epoch: 10 [29568/54000 (55%)] Loss: -182853.906250\n",
      "Train Epoch: 10 [30976/54000 (57%)] Loss: -160494.843750\n",
      "Train Epoch: 10 [32384/54000 (60%)] Loss: -195800.875000\n",
      "Train Epoch: 10 [33792/54000 (63%)] Loss: -194182.953125\n",
      "Train Epoch: 10 [35200/54000 (65%)] Loss: -177071.781250\n",
      "Train Epoch: 10 [36608/54000 (68%)] Loss: -159860.500000\n",
      "Train Epoch: 10 [38016/54000 (70%)] Loss: -191932.562500\n",
      "Train Epoch: 10 [39424/54000 (73%)] Loss: -182228.109375\n",
      "Train Epoch: 10 [40832/54000 (76%)] Loss: -192110.390625\n",
      "Train Epoch: 10 [42240/54000 (78%)] Loss: -181876.203125\n",
      "Train Epoch: 10 [43648/54000 (81%)] Loss: -182321.828125\n",
      "Train Epoch: 10 [45056/54000 (83%)] Loss: -169809.312500\n",
      "Train Epoch: 10 [46464/54000 (86%)] Loss: -180540.171875\n",
      "Train Epoch: 10 [47872/54000 (89%)] Loss: -207608.875000\n",
      "Train Epoch: 10 [49280/54000 (91%)] Loss: -173966.062500\n",
      "Train Epoch: 10 [50688/54000 (94%)] Loss: -183170.937500\n",
      "Train Epoch: 10 [52096/54000 (96%)] Loss: -158259.203125\n",
      "    epoch          : 10\n",
      "    loss           : -178224.95200358852\n",
      "    val_loss       : -181558.23513719512\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch10.pth ...\n",
      "Train Epoch: 11 [0/54000 (0%)] Loss: -188858.484375\n",
      "Train Epoch: 11 [1408/54000 (3%)] Loss: -186586.062500\n",
      "Train Epoch: 11 [2816/54000 (5%)] Loss: -181604.468750\n",
      "Train Epoch: 11 [4224/54000 (8%)] Loss: -169086.328125\n",
      "Train Epoch: 11 [5632/54000 (10%)] Loss: -181884.546875\n",
      "Train Epoch: 11 [7040/54000 (13%)] Loss: -149799.671875\n",
      "Train Epoch: 11 [8448/54000 (16%)] Loss: -189486.828125\n",
      "Train Epoch: 11 [9856/54000 (18%)] Loss: -186577.781250\n",
      "Train Epoch: 11 [11264/54000 (21%)] Loss: -144068.703125\n",
      "Train Epoch: 11 [12672/54000 (23%)] Loss: -187975.656250\n",
      "Train Epoch: 11 [14080/54000 (26%)] Loss: -186868.968750\n",
      "Train Epoch: 11 [15488/54000 (29%)] Loss: -184589.359375\n",
      "Train Epoch: 11 [16896/54000 (31%)] Loss: -185296.859375\n",
      "Train Epoch: 11 [18304/54000 (34%)] Loss: -182388.218750\n",
      "Train Epoch: 11 [19712/54000 (37%)] Loss: -161574.531250\n",
      "Train Epoch: 11 [21120/54000 (39%)] Loss: -207818.859375\n",
      "Train Epoch: 11 [22528/54000 (42%)] Loss: -179158.437500\n",
      "Train Epoch: 11 [23936/54000 (44%)] Loss: -171741.656250\n",
      "Train Epoch: 11 [25344/54000 (47%)] Loss: -148025.250000\n",
      "Train Epoch: 11 [26752/54000 (50%)] Loss: -191481.687500\n",
      "Train Epoch: 11 [28160/54000 (52%)] Loss: -180379.375000\n",
      "Train Epoch: 11 [29568/54000 (55%)] Loss: -187655.187500\n",
      "Train Epoch: 11 [30976/54000 (57%)] Loss: -179262.750000\n",
      "Train Epoch: 11 [32384/54000 (60%)] Loss: -196026.906250\n",
      "Train Epoch: 11 [33792/54000 (63%)] Loss: -162199.453125\n",
      "Train Epoch: 11 [35200/54000 (65%)] Loss: -178084.250000\n",
      "Train Epoch: 11 [36608/54000 (68%)] Loss: -149930.734375\n",
      "Train Epoch: 11 [38016/54000 (70%)] Loss: -186449.656250\n",
      "Train Epoch: 11 [39424/54000 (73%)] Loss: -193757.125000\n",
      "Train Epoch: 11 [40832/54000 (76%)] Loss: -200754.421875\n",
      "Train Epoch: 11 [42240/54000 (78%)] Loss: -185092.562500\n",
      "Train Epoch: 11 [43648/54000 (81%)] Loss: -182644.234375\n",
      "Train Epoch: 11 [45056/54000 (83%)] Loss: -173135.625000\n",
      "Train Epoch: 11 [46464/54000 (86%)] Loss: -181684.734375\n",
      "Train Epoch: 11 [47872/54000 (89%)] Loss: -185441.531250\n",
      "Train Epoch: 11 [49280/54000 (91%)] Loss: -178868.328125\n",
      "Train Epoch: 11 [50688/54000 (94%)] Loss: -210603.546875\n",
      "Train Epoch: 11 [52096/54000 (96%)] Loss: -184872.531250\n",
      "    epoch          : 11\n",
      "    loss           : -179547.24943929425\n",
      "    val_loss       : -183215.15067644816\n",
      "Train Epoch: 12 [0/54000 (0%)] Loss: -192840.125000\n",
      "Train Epoch: 12 [1408/54000 (3%)] Loss: -174097.343750\n",
      "Train Epoch: 12 [2816/54000 (5%)] Loss: -160294.218750\n",
      "Train Epoch: 12 [4224/54000 (8%)] Loss: -178034.109375\n",
      "Train Epoch: 12 [5632/54000 (10%)] Loss: -177707.906250\n",
      "Train Epoch: 12 [7040/54000 (13%)] Loss: -176348.562500\n",
      "Train Epoch: 12 [8448/54000 (16%)] Loss: -156623.656250\n",
      "Train Epoch: 12 [9856/54000 (18%)] Loss: -151442.406250\n",
      "Train Epoch: 12 [11264/54000 (21%)] Loss: -176804.093750\n",
      "Train Epoch: 12 [12672/54000 (23%)] Loss: -177748.890625\n",
      "Train Epoch: 12 [14080/54000 (26%)] Loss: -180926.203125\n",
      "Train Epoch: 12 [15488/54000 (29%)] Loss: -181142.937500\n",
      "Train Epoch: 12 [16896/54000 (31%)] Loss: -161889.640625\n",
      "Train Epoch: 12 [18304/54000 (34%)] Loss: -179640.968750\n",
      "Train Epoch: 12 [19712/54000 (37%)] Loss: -179976.187500\n",
      "Train Epoch: 12 [21120/54000 (39%)] Loss: -185978.218750\n",
      "Train Epoch: 12 [22528/54000 (42%)] Loss: -160664.187500\n",
      "Train Epoch: 12 [23936/54000 (44%)] Loss: -151747.328125\n",
      "Train Epoch: 12 [25344/54000 (47%)] Loss: -151535.031250\n",
      "Train Epoch: 12 [26752/54000 (50%)] Loss: -149559.500000\n",
      "Train Epoch: 12 [28160/54000 (52%)] Loss: -178017.593750\n",
      "Train Epoch: 12 [29568/54000 (55%)] Loss: -172571.453125\n",
      "Train Epoch: 12 [30976/54000 (57%)] Loss: -168790.859375\n",
      "Train Epoch: 12 [32384/54000 (60%)] Loss: -160125.281250\n",
      "Train Epoch: 12 [33792/54000 (63%)] Loss: -187685.046875\n",
      "Train Epoch: 12 [35200/54000 (65%)] Loss: -172045.875000\n",
      "Train Epoch: 12 [36608/54000 (68%)] Loss: -187436.984375\n",
      "Train Epoch: 12 [38016/54000 (70%)] Loss: -193453.000000\n",
      "Train Epoch: 12 [39424/54000 (73%)] Loss: -198469.781250\n",
      "Train Epoch: 12 [40832/54000 (76%)] Loss: -175346.796875\n",
      "Train Epoch: 12 [42240/54000 (78%)] Loss: -178172.484375\n",
      "Train Epoch: 12 [43648/54000 (81%)] Loss: -182429.109375\n",
      "Train Epoch: 12 [45056/54000 (83%)] Loss: -173680.671875\n",
      "Train Epoch: 12 [46464/54000 (86%)] Loss: -189947.187500\n",
      "Train Epoch: 12 [47872/54000 (89%)] Loss: -148783.671875\n",
      "Train Epoch: 12 [49280/54000 (91%)] Loss: -178832.171875\n",
      "Train Epoch: 12 [50688/54000 (94%)] Loss: -179334.484375\n",
      "Train Epoch: 12 [52096/54000 (96%)] Loss: -186697.328125\n",
      "    epoch          : 12\n",
      "    loss           : -180248.35601076554\n",
      "    val_loss       : -185196.6085413491\n",
      "Train Epoch: 13 [0/54000 (0%)] Loss: -149271.562500\n",
      "Train Epoch: 13 [1408/54000 (3%)] Loss: -188474.468750\n",
      "Train Epoch: 13 [2816/54000 (5%)] Loss: -189808.546875\n",
      "Train Epoch: 13 [4224/54000 (8%)] Loss: -180963.109375\n",
      "Train Epoch: 13 [5632/54000 (10%)] Loss: -160423.593750\n",
      "Train Epoch: 13 [7040/54000 (13%)] Loss: -152191.250000\n",
      "Train Epoch: 13 [8448/54000 (16%)] Loss: -164950.296875\n",
      "Train Epoch: 13 [9856/54000 (18%)] Loss: -204121.906250\n",
      "Train Epoch: 13 [11264/54000 (21%)] Loss: -194929.015625\n",
      "Train Epoch: 13 [12672/54000 (23%)] Loss: -163413.250000\n",
      "Train Epoch: 13 [14080/54000 (26%)] Loss: -173171.187500\n",
      "Train Epoch: 13 [15488/54000 (29%)] Loss: -210559.468750\n",
      "Train Epoch: 13 [16896/54000 (31%)] Loss: -169926.796875\n",
      "Train Epoch: 13 [18304/54000 (34%)] Loss: -188518.828125\n",
      "Train Epoch: 13 [19712/54000 (37%)] Loss: -209340.406250\n",
      "Train Epoch: 13 [21120/54000 (39%)] Loss: -182850.156250\n",
      "Train Epoch: 13 [22528/54000 (42%)] Loss: -182889.250000\n",
      "Train Epoch: 13 [23936/54000 (44%)] Loss: -180840.187500\n",
      "Train Epoch: 13 [25344/54000 (47%)] Loss: -184328.218750\n",
      "Train Epoch: 13 [26752/54000 (50%)] Loss: -204080.062500\n",
      "Train Epoch: 13 [28160/54000 (52%)] Loss: -187615.812500\n",
      "Train Epoch: 13 [29568/54000 (55%)] Loss: -184083.937500\n",
      "Train Epoch: 13 [30976/54000 (57%)] Loss: -177526.171875\n",
      "Train Epoch: 13 [32384/54000 (60%)] Loss: -198646.906250\n",
      "Train Epoch: 13 [33792/54000 (63%)] Loss: -164331.453125\n",
      "Train Epoch: 13 [35200/54000 (65%)] Loss: -178942.578125\n",
      "Train Epoch: 13 [36608/54000 (68%)] Loss: -186988.609375\n",
      "Train Epoch: 13 [38016/54000 (70%)] Loss: -172983.609375\n",
      "Train Epoch: 13 [39424/54000 (73%)] Loss: -189352.859375\n",
      "Train Epoch: 13 [40832/54000 (76%)] Loss: -161040.406250\n",
      "Train Epoch: 13 [42240/54000 (78%)] Loss: -173031.921875\n",
      "Train Epoch: 13 [43648/54000 (81%)] Loss: -185878.531250\n",
      "Train Epoch: 13 [45056/54000 (83%)] Loss: -189156.546875\n",
      "Train Epoch: 13 [46464/54000 (86%)] Loss: -191314.750000\n",
      "Train Epoch: 13 [47872/54000 (89%)] Loss: -171437.750000\n",
      "Train Epoch: 13 [49280/54000 (91%)] Loss: -184103.968750\n",
      "Train Epoch: 13 [50688/54000 (94%)] Loss: -160431.062500\n",
      "Train Epoch: 13 [52096/54000 (96%)] Loss: -184935.359375\n",
      "    epoch          : 13\n",
      "    loss           : -180976.51327003588\n",
      "    val_loss       : -185731.28996760672\n",
      "Train Epoch: 14 [0/54000 (0%)] Loss: -207127.484375\n",
      "Train Epoch: 14 [1408/54000 (3%)] Loss: -202380.203125\n",
      "Train Epoch: 14 [2816/54000 (5%)] Loss: -185598.046875\n",
      "Train Epoch: 14 [4224/54000 (8%)] Loss: -174307.062500\n",
      "Train Epoch: 14 [5632/54000 (10%)] Loss: -182453.875000\n",
      "Train Epoch: 14 [7040/54000 (13%)] Loss: -188055.906250\n",
      "Train Epoch: 14 [8448/54000 (16%)] Loss: -186131.296875\n",
      "Train Epoch: 14 [9856/54000 (18%)] Loss: -188440.000000\n",
      "Train Epoch: 14 [11264/54000 (21%)] Loss: -188299.703125\n",
      "Train Epoch: 14 [12672/54000 (23%)] Loss: -184694.640625\n",
      "Train Epoch: 14 [14080/54000 (26%)] Loss: -148600.250000\n",
      "Train Epoch: 14 [15488/54000 (29%)] Loss: -156139.125000\n",
      "Train Epoch: 14 [16896/54000 (31%)] Loss: -185289.687500\n",
      "Train Epoch: 14 [18304/54000 (34%)] Loss: -188405.734375\n",
      "Train Epoch: 14 [19712/54000 (37%)] Loss: -172306.671875\n",
      "Train Epoch: 14 [21120/54000 (39%)] Loss: -211798.125000\n",
      "Train Epoch: 14 [22528/54000 (42%)] Loss: -182433.781250\n",
      "Train Epoch: 14 [23936/54000 (44%)] Loss: -193495.343750\n",
      "Train Epoch: 14 [25344/54000 (47%)] Loss: -186064.296875\n",
      "Train Epoch: 14 [26752/54000 (50%)] Loss: -172953.968750\n",
      "Train Epoch: 14 [28160/54000 (52%)] Loss: -210901.312500\n",
      "Train Epoch: 14 [29568/54000 (55%)] Loss: -172238.796875\n",
      "Train Epoch: 14 [30976/54000 (57%)] Loss: -158074.390625\n",
      "Train Epoch: 14 [32384/54000 (60%)] Loss: -158561.093750\n",
      "Train Epoch: 14 [33792/54000 (63%)] Loss: -159287.750000\n",
      "Train Epoch: 14 [35200/54000 (65%)] Loss: -176029.250000\n",
      "Train Epoch: 14 [36608/54000 (68%)] Loss: -180322.937500\n",
      "Train Epoch: 14 [38016/54000 (70%)] Loss: -200580.921875\n",
      "Train Epoch: 14 [39424/54000 (73%)] Loss: -183450.781250\n",
      "Train Epoch: 14 [40832/54000 (76%)] Loss: -177545.750000\n",
      "Train Epoch: 14 [42240/54000 (78%)] Loss: -183433.703125\n",
      "Train Epoch: 14 [43648/54000 (81%)] Loss: -181702.562500\n",
      "Train Epoch: 14 [45056/54000 (83%)] Loss: -194856.406250\n",
      "Train Epoch: 14 [46464/54000 (86%)] Loss: -193284.859375\n",
      "Train Epoch: 14 [47872/54000 (89%)] Loss: -173526.390625\n",
      "Train Epoch: 14 [49280/54000 (91%)] Loss: -210117.281250\n",
      "Train Epoch: 14 [50688/54000 (94%)] Loss: -168635.203125\n",
      "Train Epoch: 14 [52096/54000 (96%)] Loss: -181812.421875\n",
      "    epoch          : 14\n",
      "    loss           : -181960.11991626795\n",
      "    val_loss       : -186886.50409679877\n",
      "Train Epoch: 15 [0/54000 (0%)] Loss: -210203.515625\n",
      "Train Epoch: 15 [1408/54000 (3%)] Loss: -206973.984375\n",
      "Train Epoch: 15 [2816/54000 (5%)] Loss: -186108.750000\n",
      "Train Epoch: 15 [4224/54000 (8%)] Loss: -189869.031250\n",
      "Train Epoch: 15 [5632/54000 (10%)] Loss: -176597.890625\n",
      "Train Epoch: 15 [7040/54000 (13%)] Loss: -179276.218750\n",
      "Train Epoch: 15 [8448/54000 (16%)] Loss: -175908.875000\n",
      "Train Epoch: 15 [9856/54000 (18%)] Loss: -207272.828125\n",
      "Train Epoch: 15 [11264/54000 (21%)] Loss: -148398.109375\n",
      "Train Epoch: 15 [12672/54000 (23%)] Loss: -169508.421875\n",
      "Train Epoch: 15 [14080/54000 (26%)] Loss: -188634.859375\n",
      "Train Epoch: 15 [15488/54000 (29%)] Loss: -203833.875000\n",
      "Train Epoch: 15 [16896/54000 (31%)] Loss: -182037.406250\n",
      "Train Epoch: 15 [18304/54000 (34%)] Loss: -181273.656250\n",
      "Train Epoch: 15 [19712/54000 (37%)] Loss: -176493.843750\n",
      "Train Epoch: 15 [21120/54000 (39%)] Loss: -183935.500000\n",
      "Train Epoch: 15 [22528/54000 (42%)] Loss: -162049.609375\n",
      "Train Epoch: 15 [23936/54000 (44%)] Loss: -180480.796875\n",
      "Train Epoch: 15 [25344/54000 (47%)] Loss: -187014.015625\n",
      "Train Epoch: 15 [26752/54000 (50%)] Loss: -187900.640625\n",
      "Train Epoch: 15 [28160/54000 (52%)] Loss: -190408.906250\n",
      "Train Epoch: 15 [29568/54000 (55%)] Loss: -153720.859375\n",
      "Train Epoch: 15 [30976/54000 (57%)] Loss: -160583.703125\n",
      "Train Epoch: 15 [32384/54000 (60%)] Loss: -186346.734375\n",
      "Train Epoch: 15 [33792/54000 (63%)] Loss: -199971.031250\n",
      "Train Epoch: 15 [35200/54000 (65%)] Loss: -156583.328125\n",
      "Train Epoch: 15 [36608/54000 (68%)] Loss: -197796.578125\n",
      "Train Epoch: 15 [38016/54000 (70%)] Loss: -184753.656250\n",
      "Train Epoch: 15 [39424/54000 (73%)] Loss: -197515.796875\n",
      "Train Epoch: 15 [40832/54000 (76%)] Loss: -173228.375000\n",
      "Train Epoch: 15 [42240/54000 (78%)] Loss: -168048.890625\n",
      "Train Epoch: 15 [43648/54000 (81%)] Loss: -183493.609375\n",
      "Train Epoch: 15 [45056/54000 (83%)] Loss: -213282.140625\n",
      "Train Epoch: 15 [46464/54000 (86%)] Loss: -157571.187500\n",
      "Train Epoch: 15 [47872/54000 (89%)] Loss: -187839.984375\n",
      "Train Epoch: 15 [49280/54000 (91%)] Loss: -183449.281250\n",
      "Train Epoch: 15 [50688/54000 (94%)] Loss: -213657.781250\n",
      "Train Epoch: 15 [52096/54000 (96%)] Loss: -178921.656250\n",
      "    epoch          : 15\n",
      "    loss           : -182880.20787230862\n",
      "    val_loss       : -186707.01398151676\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch15.pth ...\n",
      "Train Epoch: 16 [0/54000 (0%)] Loss: -166302.359375\n",
      "Train Epoch: 16 [1408/54000 (3%)] Loss: -212784.781250\n",
      "Train Epoch: 16 [2816/54000 (5%)] Loss: -210670.687500\n",
      "Train Epoch: 16 [4224/54000 (8%)] Loss: -188541.250000\n",
      "Train Epoch: 16 [5632/54000 (10%)] Loss: -213170.187500\n",
      "Train Epoch: 16 [7040/54000 (13%)] Loss: -188808.343750\n",
      "Train Epoch: 16 [8448/54000 (16%)] Loss: -164082.093750\n",
      "Train Epoch: 16 [9856/54000 (18%)] Loss: -165191.625000\n",
      "Train Epoch: 16 [11264/54000 (21%)] Loss: -169175.765625\n",
      "Train Epoch: 16 [12672/54000 (23%)] Loss: -186985.296875\n",
      "Train Epoch: 16 [14080/54000 (26%)] Loss: -191516.796875\n",
      "Train Epoch: 16 [15488/54000 (29%)] Loss: -204946.656250\n",
      "Train Epoch: 16 [16896/54000 (31%)] Loss: -191628.796875\n",
      "Train Epoch: 16 [18304/54000 (34%)] Loss: -197214.406250\n",
      "Train Epoch: 16 [19712/54000 (37%)] Loss: -212431.765625\n",
      "Train Epoch: 16 [21120/54000 (39%)] Loss: -167426.312500\n",
      "Train Epoch: 16 [22528/54000 (42%)] Loss: -188864.062500\n",
      "Train Epoch: 16 [23936/54000 (44%)] Loss: -170464.765625\n",
      "Train Epoch: 16 [25344/54000 (47%)] Loss: -180417.953125\n",
      "Train Epoch: 16 [26752/54000 (50%)] Loss: -172755.687500\n",
      "Train Epoch: 16 [28160/54000 (52%)] Loss: -175904.312500\n",
      "Train Epoch: 16 [29568/54000 (55%)] Loss: -185646.500000\n",
      "Train Epoch: 16 [30976/54000 (57%)] Loss: -197887.234375\n",
      "Train Epoch: 16 [32384/54000 (60%)] Loss: -189650.781250\n",
      "Train Epoch: 16 [33792/54000 (63%)] Loss: -183310.406250\n",
      "Train Epoch: 16 [35200/54000 (65%)] Loss: -196769.531250\n",
      "Train Epoch: 16 [36608/54000 (68%)] Loss: -168529.078125\n",
      "Train Epoch: 16 [38016/54000 (70%)] Loss: -171190.906250\n",
      "Train Epoch: 16 [39424/54000 (73%)] Loss: -198635.312500\n",
      "Train Epoch: 16 [40832/54000 (76%)] Loss: -197667.093750\n",
      "Train Epoch: 16 [42240/54000 (78%)] Loss: -191257.437500\n",
      "Train Epoch: 16 [43648/54000 (81%)] Loss: -212962.484375\n",
      "Train Epoch: 16 [45056/54000 (83%)] Loss: -192867.625000\n",
      "Train Epoch: 16 [46464/54000 (86%)] Loss: -185653.718750\n",
      "Train Epoch: 16 [47872/54000 (89%)] Loss: -171952.015625\n",
      "Train Epoch: 16 [49280/54000 (91%)] Loss: -164574.125000\n",
      "Train Epoch: 16 [50688/54000 (94%)] Loss: -208436.812500\n",
      "Train Epoch: 16 [52096/54000 (96%)] Loss: -174522.437500\n",
      "    epoch          : 16\n",
      "    loss           : -183676.8200882177\n",
      "    val_loss       : -190822.60685022865\n",
      "Train Epoch: 17 [0/54000 (0%)] Loss: -178667.921875\n",
      "Train Epoch: 17 [1408/54000 (3%)] Loss: -192085.312500\n",
      "Train Epoch: 17 [2816/54000 (5%)] Loss: -205665.281250\n",
      "Train Epoch: 17 [4224/54000 (8%)] Loss: -195190.921875\n",
      "Train Epoch: 17 [5632/54000 (10%)] Loss: -168457.406250\n",
      "Train Epoch: 17 [7040/54000 (13%)] Loss: -170273.859375\n",
      "Train Epoch: 17 [8448/54000 (16%)] Loss: -182972.750000\n",
      "Train Epoch: 17 [9856/54000 (18%)] Loss: -175319.812500\n",
      "Train Epoch: 17 [11264/54000 (21%)] Loss: -177940.343750\n",
      "Train Epoch: 17 [12672/54000 (23%)] Loss: -186042.000000\n",
      "Train Epoch: 17 [14080/54000 (26%)] Loss: -182667.812500\n",
      "Train Epoch: 17 [15488/54000 (29%)] Loss: -205849.281250\n",
      "Train Epoch: 17 [16896/54000 (31%)] Loss: -167707.078125\n",
      "Train Epoch: 17 [18304/54000 (34%)] Loss: -196521.531250\n",
      "Train Epoch: 17 [19712/54000 (37%)] Loss: -197677.578125\n",
      "Train Epoch: 17 [21120/54000 (39%)] Loss: -175174.968750\n",
      "Train Epoch: 17 [22528/54000 (42%)] Loss: -171694.046875\n",
      "Train Epoch: 17 [23936/54000 (44%)] Loss: -167140.125000\n",
      "Train Epoch: 17 [25344/54000 (47%)] Loss: -184958.750000\n",
      "Train Epoch: 17 [26752/54000 (50%)] Loss: -188066.937500\n",
      "Train Epoch: 17 [28160/54000 (52%)] Loss: -181453.296875\n",
      "Train Epoch: 17 [29568/54000 (55%)] Loss: -174448.062500\n",
      "Train Epoch: 17 [30976/54000 (57%)] Loss: -180524.687500\n",
      "Train Epoch: 17 [32384/54000 (60%)] Loss: -208522.250000\n",
      "Train Epoch: 17 [33792/54000 (63%)] Loss: -190373.937500\n",
      "Train Epoch: 17 [35200/54000 (65%)] Loss: -189754.203125\n",
      "Train Epoch: 17 [36608/54000 (68%)] Loss: -167442.953125\n",
      "Train Epoch: 17 [38016/54000 (70%)] Loss: -213046.250000\n",
      "Train Epoch: 17 [39424/54000 (73%)] Loss: -170504.390625\n",
      "Train Epoch: 17 [40832/54000 (76%)] Loss: -188967.515625\n",
      "Train Epoch: 17 [42240/54000 (78%)] Loss: -154138.468750\n",
      "Train Epoch: 17 [43648/54000 (81%)] Loss: -167726.312500\n",
      "Train Epoch: 17 [45056/54000 (83%)] Loss: -152986.781250\n",
      "Train Epoch: 17 [46464/54000 (86%)] Loss: -163485.296875\n",
      "Train Epoch: 17 [47872/54000 (89%)] Loss: -174197.312500\n",
      "Train Epoch: 17 [49280/54000 (91%)] Loss: -180574.343750\n",
      "Train Epoch: 17 [50688/54000 (94%)] Loss: -191272.281250\n",
      "Train Epoch: 17 [52096/54000 (96%)] Loss: -192026.031250\n",
      "    epoch          : 17\n",
      "    loss           : -184345.71078797846\n",
      "    val_loss       : -188363.69159679877\n",
      "Train Epoch: 18 [0/54000 (0%)] Loss: -183546.171875\n",
      "Train Epoch: 18 [1408/54000 (3%)] Loss: -175727.312500\n",
      "Train Epoch: 18 [2816/54000 (5%)] Loss: -175847.312500\n",
      "Train Epoch: 18 [4224/54000 (8%)] Loss: -164159.203125\n",
      "Train Epoch: 18 [5632/54000 (10%)] Loss: -185691.703125\n",
      "Train Epoch: 18 [7040/54000 (13%)] Loss: -210456.562500\n",
      "Train Epoch: 18 [8448/54000 (16%)] Loss: -208757.468750\n",
      "Train Epoch: 18 [9856/54000 (18%)] Loss: -198822.281250\n",
      "Train Epoch: 18 [11264/54000 (21%)] Loss: -183048.000000\n",
      "Train Epoch: 18 [12672/54000 (23%)] Loss: -182601.015625\n",
      "Train Epoch: 18 [14080/54000 (26%)] Loss: -193547.562500\n",
      "Train Epoch: 18 [15488/54000 (29%)] Loss: -186958.250000\n",
      "Train Epoch: 18 [16896/54000 (31%)] Loss: -204407.687500\n",
      "Train Epoch: 18 [18304/54000 (34%)] Loss: -159282.218750\n",
      "Train Epoch: 18 [19712/54000 (37%)] Loss: -205847.062500\n",
      "Train Epoch: 18 [21120/54000 (39%)] Loss: -186145.296875\n",
      "Train Epoch: 18 [22528/54000 (42%)] Loss: -167466.484375\n",
      "Train Epoch: 18 [23936/54000 (44%)] Loss: -174616.375000\n",
      "Train Epoch: 18 [25344/54000 (47%)] Loss: -180340.687500\n",
      "Train Epoch: 18 [26752/54000 (50%)] Loss: -186090.968750\n",
      "Train Epoch: 18 [28160/54000 (52%)] Loss: -191797.203125\n",
      "Train Epoch: 18 [29568/54000 (55%)] Loss: -180959.406250\n",
      "Train Epoch: 18 [30976/54000 (57%)] Loss: -184185.359375\n",
      "Train Epoch: 18 [32384/54000 (60%)] Loss: -183880.953125\n",
      "Train Epoch: 18 [33792/54000 (63%)] Loss: -178119.156250\n",
      "Train Epoch: 18 [35200/54000 (65%)] Loss: -182673.828125\n",
      "Train Epoch: 18 [36608/54000 (68%)] Loss: -201858.593750\n",
      "Train Epoch: 18 [38016/54000 (70%)] Loss: -172387.828125\n",
      "Train Epoch: 18 [39424/54000 (73%)] Loss: -209187.656250\n",
      "Train Epoch: 18 [40832/54000 (76%)] Loss: -193875.468750\n",
      "Train Epoch: 18 [42240/54000 (78%)] Loss: -161165.218750\n",
      "Train Epoch: 18 [43648/54000 (81%)] Loss: -161167.500000\n",
      "Train Epoch: 18 [45056/54000 (83%)] Loss: -152950.250000\n",
      "Train Epoch: 18 [46464/54000 (86%)] Loss: -182008.875000\n",
      "Train Epoch: 18 [47872/54000 (89%)] Loss: -185191.328125\n",
      "Train Epoch: 18 [49280/54000 (91%)] Loss: -187503.968750\n",
      "Train Epoch: 18 [50688/54000 (94%)] Loss: -191920.546875\n",
      "Train Epoch: 18 [52096/54000 (96%)] Loss: -192547.484375\n",
      "    epoch          : 18\n",
      "    loss           : -184869.95409688994\n",
      "    val_loss       : -190909.11947408537\n",
      "Train Epoch: 19 [0/54000 (0%)] Loss: -211596.171875\n",
      "Train Epoch: 19 [1408/54000 (3%)] Loss: -190174.718750\n",
      "Train Epoch: 19 [2816/54000 (5%)] Loss: -181290.734375\n",
      "Train Epoch: 19 [4224/54000 (8%)] Loss: -193606.031250\n",
      "Train Epoch: 19 [5632/54000 (10%)] Loss: -163159.953125\n",
      "Train Epoch: 19 [7040/54000 (13%)] Loss: -165279.281250\n",
      "Train Epoch: 19 [8448/54000 (16%)] Loss: -173585.093750\n",
      "Train Epoch: 19 [9856/54000 (18%)] Loss: -199361.359375\n",
      "Train Epoch: 19 [11264/54000 (21%)] Loss: -183896.687500\n",
      "Train Epoch: 19 [12672/54000 (23%)] Loss: -185789.953125\n",
      "Train Epoch: 19 [14080/54000 (26%)] Loss: -181025.843750\n",
      "Train Epoch: 19 [15488/54000 (29%)] Loss: -185574.625000\n",
      "Train Epoch: 19 [16896/54000 (31%)] Loss: -180857.406250\n",
      "Train Epoch: 19 [18304/54000 (34%)] Loss: -165372.296875\n",
      "Train Epoch: 19 [19712/54000 (37%)] Loss: -164467.109375\n",
      "Train Epoch: 19 [21120/54000 (39%)] Loss: -168008.625000\n",
      "Train Epoch: 19 [22528/54000 (42%)] Loss: -166950.625000\n",
      "Train Epoch: 19 [23936/54000 (44%)] Loss: -170630.406250\n",
      "Train Epoch: 19 [25344/54000 (47%)] Loss: -190486.828125\n",
      "Train Epoch: 19 [26752/54000 (50%)] Loss: -200616.203125\n",
      "Train Epoch: 19 [28160/54000 (52%)] Loss: -187147.031250\n",
      "Train Epoch: 19 [29568/54000 (55%)] Loss: -186422.500000\n",
      "Train Epoch: 19 [30976/54000 (57%)] Loss: -186447.265625\n",
      "Train Epoch: 19 [32384/54000 (60%)] Loss: -183890.984375\n",
      "Train Epoch: 19 [33792/54000 (63%)] Loss: -181220.812500\n",
      "Train Epoch: 19 [35200/54000 (65%)] Loss: -175406.812500\n",
      "Train Epoch: 19 [36608/54000 (68%)] Loss: -213714.109375\n",
      "Train Epoch: 19 [38016/54000 (70%)] Loss: -183263.593750\n",
      "Train Epoch: 19 [39424/54000 (73%)] Loss: -200675.484375\n",
      "Train Epoch: 19 [40832/54000 (76%)] Loss: -193365.765625\n",
      "Train Epoch: 19 [42240/54000 (78%)] Loss: -198926.796875\n",
      "Train Epoch: 19 [43648/54000 (81%)] Loss: -184207.062500\n",
      "Train Epoch: 19 [45056/54000 (83%)] Loss: -205096.453125\n",
      "Train Epoch: 19 [46464/54000 (86%)] Loss: -185504.796875\n",
      "Train Epoch: 19 [47872/54000 (89%)] Loss: -187426.968750\n",
      "Train Epoch: 19 [49280/54000 (91%)] Loss: -191606.468750\n",
      "Train Epoch: 19 [50688/54000 (94%)] Loss: -184159.140625\n",
      "Train Epoch: 19 [52096/54000 (96%)] Loss: -194348.718750\n",
      "    epoch          : 19\n",
      "    loss           : -185703.79179126795\n",
      "    val_loss       : -192475.29818502287\n",
      "Train Epoch: 20 [0/54000 (0%)] Loss: -210314.156250\n",
      "Train Epoch: 20 [1408/54000 (3%)] Loss: -211034.140625\n",
      "Train Epoch: 20 [2816/54000 (5%)] Loss: -192957.953125\n",
      "Train Epoch: 20 [4224/54000 (8%)] Loss: -158237.343750\n",
      "Train Epoch: 20 [5632/54000 (10%)] Loss: -182514.937500\n",
      "Train Epoch: 20 [7040/54000 (13%)] Loss: -213968.437500\n",
      "Train Epoch: 20 [8448/54000 (16%)] Loss: -189624.859375\n",
      "Train Epoch: 20 [9856/54000 (18%)] Loss: -161608.093750\n",
      "Train Epoch: 20 [11264/54000 (21%)] Loss: -199788.015625\n",
      "Train Epoch: 20 [12672/54000 (23%)] Loss: -177649.656250\n",
      "Train Epoch: 20 [14080/54000 (26%)] Loss: -182421.343750\n",
      "Train Epoch: 20 [15488/54000 (29%)] Loss: -178942.156250\n",
      "Train Epoch: 20 [16896/54000 (31%)] Loss: -164987.562500\n",
      "Train Epoch: 20 [18304/54000 (34%)] Loss: -162084.968750\n",
      "Train Epoch: 20 [19712/54000 (37%)] Loss: -188062.343750\n",
      "Train Epoch: 20 [21120/54000 (39%)] Loss: -163845.171875\n",
      "Train Epoch: 20 [22528/54000 (42%)] Loss: -195499.562500\n",
      "Train Epoch: 20 [23936/54000 (44%)] Loss: -194065.031250\n",
      "Train Epoch: 20 [25344/54000 (47%)] Loss: -187478.921875\n",
      "Train Epoch: 20 [26752/54000 (50%)] Loss: -187501.593750\n",
      "Train Epoch: 20 [28160/54000 (52%)] Loss: -161073.125000\n",
      "Train Epoch: 20 [29568/54000 (55%)] Loss: -174724.687500\n",
      "Train Epoch: 20 [30976/54000 (57%)] Loss: -190207.296875\n",
      "Train Epoch: 20 [32384/54000 (60%)] Loss: -173762.203125\n",
      "Train Epoch: 20 [33792/54000 (63%)] Loss: -190243.593750\n",
      "Train Epoch: 20 [35200/54000 (65%)] Loss: -183626.718750\n",
      "Train Epoch: 20 [36608/54000 (68%)] Loss: -192181.468750\n",
      "Train Epoch: 20 [38016/54000 (70%)] Loss: -215752.421875\n",
      "Train Epoch: 20 [39424/54000 (73%)] Loss: -185585.921875\n",
      "Train Epoch: 20 [40832/54000 (76%)] Loss: -197002.406250\n",
      "Train Epoch: 20 [42240/54000 (78%)] Loss: -192192.625000\n",
      "Train Epoch: 20 [43648/54000 (81%)] Loss: -186888.593750\n",
      "Train Epoch: 20 [45056/54000 (83%)] Loss: -192815.109375\n",
      "Train Epoch: 20 [46464/54000 (86%)] Loss: -194834.140625\n",
      "Train Epoch: 20 [47872/54000 (89%)] Loss: -192787.921875\n",
      "Train Epoch: 20 [49280/54000 (91%)] Loss: -190845.109375\n",
      "Train Epoch: 20 [50688/54000 (94%)] Loss: -192860.343750\n",
      "Train Epoch: 20 [52096/54000 (96%)] Loss: -193953.843750\n",
      "    epoch          : 20\n",
      "    loss           : -186040.04777212918\n",
      "    val_loss       : -190400.3896246189\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch20.pth ...\n",
      "Train Epoch: 21 [0/54000 (0%)] Loss: -185799.484375\n",
      "Train Epoch: 21 [1408/54000 (3%)] Loss: -194510.343750\n",
      "Train Epoch: 21 [2816/54000 (5%)] Loss: -213418.296875\n",
      "Train Epoch: 21 [4224/54000 (8%)] Loss: -213904.531250\n",
      "Train Epoch: 21 [5632/54000 (10%)] Loss: -193429.171875\n",
      "Train Epoch: 21 [7040/54000 (13%)] Loss: -183651.187500\n",
      "Train Epoch: 21 [8448/54000 (16%)] Loss: -189781.281250\n",
      "Train Epoch: 21 [9856/54000 (18%)] Loss: -182228.843750\n",
      "Train Epoch: 21 [11264/54000 (21%)] Loss: -186072.843750\n",
      "Train Epoch: 21 [12672/54000 (23%)] Loss: -169654.156250\n",
      "Train Epoch: 21 [14080/54000 (26%)] Loss: -191741.828125\n",
      "Train Epoch: 21 [15488/54000 (29%)] Loss: -186184.250000\n",
      "Train Epoch: 21 [16896/54000 (31%)] Loss: -181647.781250\n",
      "Train Epoch: 21 [18304/54000 (34%)] Loss: -161167.375000\n",
      "Train Epoch: 21 [19712/54000 (37%)] Loss: -190281.109375\n",
      "Train Epoch: 21 [21120/54000 (39%)] Loss: -211669.375000\n",
      "Train Epoch: 21 [22528/54000 (42%)] Loss: -199539.453125\n",
      "Train Epoch: 21 [23936/54000 (44%)] Loss: -171164.718750\n",
      "Train Epoch: 21 [25344/54000 (47%)] Loss: -192170.062500\n",
      "Train Epoch: 21 [26752/54000 (50%)] Loss: -201988.640625\n",
      "Train Epoch: 21 [28160/54000 (52%)] Loss: -187901.187500\n",
      "Train Epoch: 21 [29568/54000 (55%)] Loss: -185180.562500\n",
      "Train Epoch: 21 [30976/54000 (57%)] Loss: -192660.468750\n",
      "Train Epoch: 21 [32384/54000 (60%)] Loss: -185320.937500\n",
      "Train Epoch: 21 [33792/54000 (63%)] Loss: -169900.000000\n",
      "Train Epoch: 21 [35200/54000 (65%)] Loss: -192320.468750\n",
      "Train Epoch: 21 [36608/54000 (68%)] Loss: -181426.750000\n",
      "Train Epoch: 21 [38016/54000 (70%)] Loss: -160208.203125\n",
      "Train Epoch: 21 [39424/54000 (73%)] Loss: -174467.875000\n",
      "Train Epoch: 21 [40832/54000 (76%)] Loss: -194492.500000\n",
      "Train Epoch: 21 [42240/54000 (78%)] Loss: -196294.593750\n",
      "Train Epoch: 21 [43648/54000 (81%)] Loss: -188615.484375\n",
      "Train Epoch: 21 [45056/54000 (83%)] Loss: -185134.500000\n",
      "Train Epoch: 21 [46464/54000 (86%)] Loss: -192685.046875\n",
      "Train Epoch: 21 [47872/54000 (89%)] Loss: -163821.921875\n",
      "Train Epoch: 21 [49280/54000 (91%)] Loss: -193740.796875\n",
      "Train Epoch: 21 [50688/54000 (94%)] Loss: -161027.625000\n",
      "Train Epoch: 21 [52096/54000 (96%)] Loss: -214694.265625\n",
      "    epoch          : 21\n",
      "    loss           : -186759.07289174641\n",
      "    val_loss       : -191527.55292492377\n",
      "Train Epoch: 22 [0/54000 (0%)] Loss: -167547.531250\n",
      "Train Epoch: 22 [1408/54000 (3%)] Loss: -165878.031250\n",
      "Train Epoch: 22 [2816/54000 (5%)] Loss: -183111.000000\n",
      "Train Epoch: 22 [4224/54000 (8%)] Loss: -194787.656250\n",
      "Train Epoch: 22 [5632/54000 (10%)] Loss: -188761.156250\n",
      "Train Epoch: 22 [7040/54000 (13%)] Loss: -180145.781250\n",
      "Train Epoch: 22 [8448/54000 (16%)] Loss: -185182.281250\n",
      "Train Epoch: 22 [9856/54000 (18%)] Loss: -190998.531250\n",
      "Train Epoch: 22 [11264/54000 (21%)] Loss: -191291.281250\n",
      "Train Epoch: 22 [12672/54000 (23%)] Loss: -183806.000000\n",
      "Train Epoch: 22 [14080/54000 (26%)] Loss: -172545.281250\n",
      "Train Epoch: 22 [15488/54000 (29%)] Loss: -188106.250000\n",
      "Train Epoch: 22 [16896/54000 (31%)] Loss: -184146.953125\n",
      "Train Epoch: 22 [18304/54000 (34%)] Loss: -175396.031250\n",
      "Train Epoch: 22 [19712/54000 (37%)] Loss: -170565.625000\n",
      "Train Epoch: 22 [21120/54000 (39%)] Loss: -214687.812500\n",
      "Train Epoch: 22 [22528/54000 (42%)] Loss: -199981.984375\n",
      "Train Epoch: 22 [23936/54000 (44%)] Loss: -179109.484375\n",
      "Train Epoch: 22 [25344/54000 (47%)] Loss: -180360.593750\n",
      "Train Epoch: 22 [26752/54000 (50%)] Loss: -192755.406250\n",
      "Train Epoch: 22 [28160/54000 (52%)] Loss: -157905.812500\n",
      "Train Epoch: 22 [29568/54000 (55%)] Loss: -182765.406250\n",
      "Train Epoch: 22 [30976/54000 (57%)] Loss: -182653.375000\n",
      "Train Epoch: 22 [32384/54000 (60%)] Loss: -164311.718750\n",
      "Train Epoch: 22 [33792/54000 (63%)] Loss: -179520.312500\n",
      "Train Epoch: 22 [35200/54000 (65%)] Loss: -180980.062500\n",
      "Train Epoch: 22 [36608/54000 (68%)] Loss: -191537.375000\n",
      "Train Epoch: 22 [38016/54000 (70%)] Loss: -190718.968750\n",
      "Train Epoch: 22 [39424/54000 (73%)] Loss: -191194.500000\n",
      "Train Epoch: 22 [40832/54000 (76%)] Loss: -171794.812500\n",
      "Train Epoch: 22 [42240/54000 (78%)] Loss: -188364.640625\n",
      "Train Epoch: 22 [43648/54000 (81%)] Loss: -185677.109375\n",
      "Train Epoch: 22 [45056/54000 (83%)] Loss: -193310.796875\n",
      "Train Epoch: 22 [46464/54000 (86%)] Loss: -195644.953125\n",
      "Train Epoch: 22 [47872/54000 (89%)] Loss: -189202.468750\n",
      "Train Epoch: 22 [49280/54000 (91%)] Loss: -186403.593750\n",
      "Train Epoch: 22 [50688/54000 (94%)] Loss: -209207.187500\n",
      "Train Epoch: 22 [52096/54000 (96%)] Loss: -169903.093750\n",
      "    epoch          : 22\n",
      "    loss           : -187102.56036931818\n",
      "    val_loss       : -193525.52188929115\n",
      "Train Epoch: 23 [0/54000 (0%)] Loss: -198527.843750\n",
      "Train Epoch: 23 [1408/54000 (3%)] Loss: -201144.578125\n",
      "Train Epoch: 23 [2816/54000 (5%)] Loss: -184834.828125\n",
      "Train Epoch: 23 [4224/54000 (8%)] Loss: -193410.796875\n",
      "Train Epoch: 23 [5632/54000 (10%)] Loss: -187711.500000\n",
      "Train Epoch: 23 [7040/54000 (13%)] Loss: -186616.515625\n",
      "Train Epoch: 23 [8448/54000 (16%)] Loss: -198524.015625\n",
      "Train Epoch: 23 [9856/54000 (18%)] Loss: -169098.500000\n",
      "Train Epoch: 23 [11264/54000 (21%)] Loss: -195050.078125\n",
      "Train Epoch: 23 [12672/54000 (23%)] Loss: -183103.343750\n",
      "Train Epoch: 23 [14080/54000 (26%)] Loss: -217136.093750\n",
      "Train Epoch: 23 [15488/54000 (29%)] Loss: -194425.421875\n",
      "Train Epoch: 23 [16896/54000 (31%)] Loss: -182208.343750\n",
      "Train Epoch: 23 [18304/54000 (34%)] Loss: -194950.875000\n",
      "Train Epoch: 23 [19712/54000 (37%)] Loss: -183615.609375\n",
      "Train Epoch: 23 [21120/54000 (39%)] Loss: -185920.859375\n",
      "Train Epoch: 23 [22528/54000 (42%)] Loss: -183955.968750\n",
      "Train Epoch: 23 [23936/54000 (44%)] Loss: -194096.671875\n",
      "Train Epoch: 23 [25344/54000 (47%)] Loss: -183766.125000\n",
      "Train Epoch: 23 [26752/54000 (50%)] Loss: -216144.531250\n",
      "Train Epoch: 23 [28160/54000 (52%)] Loss: -164894.406250\n",
      "Train Epoch: 23 [29568/54000 (55%)] Loss: -179304.000000\n",
      "Train Epoch: 23 [30976/54000 (57%)] Loss: -190032.843750\n",
      "Train Epoch: 23 [32384/54000 (60%)] Loss: -189699.984375\n",
      "Train Epoch: 23 [33792/54000 (63%)] Loss: -216748.734375\n",
      "Train Epoch: 23 [35200/54000 (65%)] Loss: -185531.062500\n",
      "Train Epoch: 23 [36608/54000 (68%)] Loss: -195265.187500\n",
      "Train Epoch: 23 [38016/54000 (70%)] Loss: -199074.531250\n",
      "Train Epoch: 23 [39424/54000 (73%)] Loss: -216748.093750\n",
      "Train Epoch: 23 [40832/54000 (76%)] Loss: -194923.562500\n",
      "Train Epoch: 23 [42240/54000 (78%)] Loss: -191621.343750\n",
      "Train Epoch: 23 [43648/54000 (81%)] Loss: -157819.562500\n",
      "Train Epoch: 23 [45056/54000 (83%)] Loss: -187238.234375\n",
      "Train Epoch: 23 [46464/54000 (86%)] Loss: -188775.093750\n",
      "Train Epoch: 23 [47872/54000 (89%)] Loss: -189747.890625\n",
      "Train Epoch: 23 [49280/54000 (91%)] Loss: -183707.218750\n",
      "Train Epoch: 23 [50688/54000 (94%)] Loss: -195944.109375\n",
      "Train Epoch: 23 [52096/54000 (96%)] Loss: -175044.750000\n",
      "    epoch          : 23\n",
      "    loss           : -188251.9828797847\n",
      "    val_loss       : -195498.14183974848\n",
      "Train Epoch: 24 [0/54000 (0%)] Loss: -176784.828125\n",
      "Train Epoch: 24 [1408/54000 (3%)] Loss: -217193.671875\n",
      "Train Epoch: 24 [2816/54000 (5%)] Loss: -204117.234375\n",
      "Train Epoch: 24 [4224/54000 (8%)] Loss: -201586.421875\n",
      "Train Epoch: 24 [5632/54000 (10%)] Loss: -189642.468750\n",
      "Train Epoch: 24 [7040/54000 (13%)] Loss: -167252.046875\n",
      "Train Epoch: 24 [8448/54000 (16%)] Loss: -176673.000000\n",
      "Train Epoch: 24 [9856/54000 (18%)] Loss: -177206.187500\n",
      "Train Epoch: 24 [11264/54000 (21%)] Loss: -187345.390625\n",
      "Train Epoch: 24 [12672/54000 (23%)] Loss: -187213.656250\n",
      "Train Epoch: 24 [14080/54000 (26%)] Loss: -191816.062500\n",
      "Train Epoch: 24 [15488/54000 (29%)] Loss: -179763.312500\n",
      "Train Epoch: 24 [16896/54000 (31%)] Loss: -176656.390625\n",
      "Train Epoch: 24 [18304/54000 (34%)] Loss: -202678.234375\n",
      "Train Epoch: 24 [19712/54000 (37%)] Loss: -194921.062500\n",
      "Train Epoch: 24 [21120/54000 (39%)] Loss: -203246.734375\n",
      "Train Epoch: 24 [22528/54000 (42%)] Loss: -179626.718750\n",
      "Train Epoch: 24 [23936/54000 (44%)] Loss: -213264.843750\n",
      "Train Epoch: 24 [25344/54000 (47%)] Loss: -218306.234375\n",
      "Train Epoch: 24 [26752/54000 (50%)] Loss: -179877.562500\n",
      "Train Epoch: 24 [28160/54000 (52%)] Loss: -169444.312500\n",
      "Train Epoch: 24 [29568/54000 (55%)] Loss: -186397.968750\n",
      "Train Epoch: 24 [30976/54000 (57%)] Loss: -180049.406250\n",
      "Train Epoch: 24 [32384/54000 (60%)] Loss: -174142.203125\n",
      "Train Epoch: 24 [33792/54000 (63%)] Loss: -195760.234375\n",
      "Train Epoch: 24 [35200/54000 (65%)] Loss: -199328.062500\n",
      "Train Epoch: 24 [36608/54000 (68%)] Loss: -187199.296875\n",
      "Train Epoch: 24 [38016/54000 (70%)] Loss: -183811.953125\n",
      "Train Epoch: 24 [39424/54000 (73%)] Loss: -167575.156250\n",
      "Train Epoch: 24 [40832/54000 (76%)] Loss: -185338.812500\n",
      "Train Epoch: 24 [42240/54000 (78%)] Loss: -209846.984375\n",
      "Train Epoch: 24 [43648/54000 (81%)] Loss: -212847.468750\n",
      "Train Epoch: 24 [45056/54000 (83%)] Loss: -159477.093750\n",
      "Train Epoch: 24 [46464/54000 (86%)] Loss: -190291.031250\n",
      "Train Epoch: 24 [47872/54000 (89%)] Loss: -190985.593750\n",
      "Train Epoch: 24 [49280/54000 (91%)] Loss: -177145.187500\n",
      "Train Epoch: 24 [50688/54000 (94%)] Loss: -177310.562500\n",
      "Train Epoch: 24 [52096/54000 (96%)] Loss: -185341.781250\n",
      "    epoch          : 24\n",
      "    loss           : -188474.83922697368\n",
      "    val_loss       : -195918.02467606709\n",
      "Train Epoch: 25 [0/54000 (0%)] Loss: -192005.812500\n",
      "Train Epoch: 25 [1408/54000 (3%)] Loss: -195499.812500\n",
      "Train Epoch: 25 [2816/54000 (5%)] Loss: -175796.937500\n",
      "Train Epoch: 25 [4224/54000 (8%)] Loss: -191673.484375\n",
      "Train Epoch: 25 [5632/54000 (10%)] Loss: -189350.359375\n",
      "Train Epoch: 25 [7040/54000 (13%)] Loss: -191584.781250\n",
      "Train Epoch: 25 [8448/54000 (16%)] Loss: -188098.125000\n",
      "Train Epoch: 25 [9856/54000 (18%)] Loss: -191859.968750\n",
      "Train Epoch: 25 [11264/54000 (21%)] Loss: -193870.937500\n",
      "Train Epoch: 25 [12672/54000 (23%)] Loss: -195434.500000\n",
      "Train Epoch: 25 [14080/54000 (26%)] Loss: -189432.687500\n",
      "Train Epoch: 25 [15488/54000 (29%)] Loss: -217763.968750\n",
      "Train Epoch: 25 [16896/54000 (31%)] Loss: -193266.468750\n",
      "Train Epoch: 25 [18304/54000 (34%)] Loss: -161261.125000\n",
      "Train Epoch: 25 [19712/54000 (37%)] Loss: -169472.437500\n",
      "Train Epoch: 25 [21120/54000 (39%)] Loss: -218245.953125\n",
      "Train Epoch: 25 [22528/54000 (42%)] Loss: -196239.500000\n",
      "Train Epoch: 25 [23936/54000 (44%)] Loss: -196022.437500\n",
      "Train Epoch: 25 [25344/54000 (47%)] Loss: -201182.406250\n",
      "Train Epoch: 25 [26752/54000 (50%)] Loss: -195678.625000\n",
      "Train Epoch: 25 [28160/54000 (52%)] Loss: -209368.171875\n",
      "Train Epoch: 25 [29568/54000 (55%)] Loss: -183652.453125\n",
      "Train Epoch: 25 [30976/54000 (57%)] Loss: -193006.609375\n",
      "Train Epoch: 25 [32384/54000 (60%)] Loss: -170438.734375\n",
      "Train Epoch: 25 [33792/54000 (63%)] Loss: -172054.140625\n",
      "Train Epoch: 25 [35200/54000 (65%)] Loss: -180261.375000\n",
      "Train Epoch: 25 [36608/54000 (68%)] Loss: -192370.343750\n",
      "Train Epoch: 25 [38016/54000 (70%)] Loss: -176826.031250\n",
      "Train Epoch: 25 [39424/54000 (73%)] Loss: -210494.046875\n",
      "Train Epoch: 25 [40832/54000 (76%)] Loss: -177579.984375\n",
      "Train Epoch: 25 [42240/54000 (78%)] Loss: -196111.500000\n",
      "Train Epoch: 25 [43648/54000 (81%)] Loss: -189987.875000\n",
      "Train Epoch: 25 [45056/54000 (83%)] Loss: -193526.390625\n",
      "Train Epoch: 25 [46464/54000 (86%)] Loss: -185943.000000\n",
      "Train Epoch: 25 [47872/54000 (89%)] Loss: -189300.734375\n",
      "Train Epoch: 25 [49280/54000 (91%)] Loss: -187205.515625\n",
      "Train Epoch: 25 [50688/54000 (94%)] Loss: -211695.812500\n",
      "Train Epoch: 25 [52096/54000 (96%)] Loss: -188818.468750\n",
      "    epoch          : 25\n",
      "    loss           : -189133.95368570575\n",
      "    val_loss       : -195488.5156726372\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch25.pth ...\n",
      "Train Epoch: 26 [0/54000 (0%)] Loss: -218757.593750\n",
      "Train Epoch: 26 [1408/54000 (3%)] Loss: -176403.593750\n",
      "Train Epoch: 26 [2816/54000 (5%)] Loss: -185676.156250\n",
      "Train Epoch: 26 [4224/54000 (8%)] Loss: -188467.781250\n",
      "Train Epoch: 26 [5632/54000 (10%)] Loss: -195130.781250\n",
      "Train Epoch: 26 [7040/54000 (13%)] Loss: -189348.218750\n",
      "Train Epoch: 26 [8448/54000 (16%)] Loss: -196214.250000\n",
      "Train Epoch: 26 [9856/54000 (18%)] Loss: -177499.578125\n",
      "Train Epoch: 26 [11264/54000 (21%)] Loss: -188086.703125\n",
      "Train Epoch: 26 [12672/54000 (23%)] Loss: -190097.250000\n",
      "Train Epoch: 26 [14080/54000 (26%)] Loss: -188187.187500\n",
      "Train Epoch: 26 [15488/54000 (29%)] Loss: -174659.281250\n",
      "Train Epoch: 26 [16896/54000 (31%)] Loss: -187869.796875\n",
      "Train Epoch: 26 [18304/54000 (34%)] Loss: -212830.125000\n",
      "Train Epoch: 26 [19712/54000 (37%)] Loss: -178783.015625\n",
      "Train Epoch: 26 [21120/54000 (39%)] Loss: -203316.093750\n",
      "Train Epoch: 26 [22528/54000 (42%)] Loss: -189366.062500\n",
      "Train Epoch: 26 [23936/54000 (44%)] Loss: -185184.406250\n",
      "Train Epoch: 26 [25344/54000 (47%)] Loss: -205738.218750\n",
      "Train Epoch: 26 [26752/54000 (50%)] Loss: -179563.437500\n",
      "Train Epoch: 26 [28160/54000 (52%)] Loss: -169951.781250\n",
      "Train Epoch: 26 [29568/54000 (55%)] Loss: -166370.281250\n",
      "Train Epoch: 26 [30976/54000 (57%)] Loss: -216573.281250\n",
      "Train Epoch: 26 [32384/54000 (60%)] Loss: -196607.000000\n",
      "Train Epoch: 26 [33792/54000 (63%)] Loss: -189435.687500\n",
      "Train Epoch: 26 [35200/54000 (65%)] Loss: -176603.203125\n",
      "Train Epoch: 26 [36608/54000 (68%)] Loss: -211742.984375\n",
      "Train Epoch: 26 [38016/54000 (70%)] Loss: -201960.031250\n",
      "Train Epoch: 26 [39424/54000 (73%)] Loss: -175386.218750\n",
      "Train Epoch: 26 [40832/54000 (76%)] Loss: -187014.421875\n",
      "Train Epoch: 26 [42240/54000 (78%)] Loss: -177498.718750\n",
      "Train Epoch: 26 [43648/54000 (81%)] Loss: -213281.687500\n",
      "Train Epoch: 26 [45056/54000 (83%)] Loss: -180647.890625\n",
      "Train Epoch: 26 [46464/54000 (86%)] Loss: -192700.062500\n",
      "Train Epoch: 26 [47872/54000 (89%)] Loss: -198439.531250\n",
      "Train Epoch: 26 [49280/54000 (91%)] Loss: -194976.000000\n",
      "Train Epoch: 26 [50688/54000 (94%)] Loss: -214910.765625\n",
      "Train Epoch: 26 [52096/54000 (96%)] Loss: -184020.734375\n",
      "    epoch          : 26\n",
      "    loss           : -189373.39182117226\n",
      "    val_loss       : -196995.53358422255\n",
      "Train Epoch: 27 [0/54000 (0%)] Loss: -196206.125000\n",
      "Train Epoch: 27 [1408/54000 (3%)] Loss: -218936.906250\n",
      "Train Epoch: 27 [2816/54000 (5%)] Loss: -187608.718750\n",
      "Train Epoch: 27 [4224/54000 (8%)] Loss: -179863.937500\n",
      "Train Epoch: 27 [5632/54000 (10%)] Loss: -196961.453125\n",
      "Train Epoch: 27 [7040/54000 (13%)] Loss: -193972.703125\n",
      "Train Epoch: 27 [8448/54000 (16%)] Loss: -166770.812500\n",
      "Train Epoch: 27 [9856/54000 (18%)] Loss: -174598.390625\n",
      "Train Epoch: 27 [11264/54000 (21%)] Loss: -193853.359375\n",
      "Train Epoch: 27 [12672/54000 (23%)] Loss: -183839.140625\n",
      "Train Epoch: 27 [14080/54000 (26%)] Loss: -197085.203125\n",
      "Train Epoch: 27 [15488/54000 (29%)] Loss: -190983.484375\n",
      "Train Epoch: 27 [16896/54000 (31%)] Loss: -191650.781250\n",
      "Train Epoch: 27 [18304/54000 (34%)] Loss: -197256.796875\n",
      "Train Epoch: 27 [19712/54000 (37%)] Loss: -191542.937500\n",
      "Train Epoch: 27 [21120/54000 (39%)] Loss: -178750.515625\n",
      "Train Epoch: 27 [22528/54000 (42%)] Loss: -201686.593750\n",
      "Train Epoch: 27 [23936/54000 (44%)] Loss: -180937.750000\n",
      "Train Epoch: 27 [25344/54000 (47%)] Loss: -189561.468750\n",
      "Train Epoch: 27 [26752/54000 (50%)] Loss: -179305.656250\n",
      "Train Epoch: 27 [28160/54000 (52%)] Loss: -209082.125000\n",
      "Train Epoch: 27 [29568/54000 (55%)] Loss: -190473.437500\n",
      "Train Epoch: 27 [30976/54000 (57%)] Loss: -181576.312500\n",
      "Train Epoch: 27 [32384/54000 (60%)] Loss: -193868.015625\n",
      "Train Epoch: 27 [33792/54000 (63%)] Loss: -191540.796875\n",
      "Train Epoch: 27 [35200/54000 (65%)] Loss: -218378.062500\n",
      "Train Epoch: 27 [36608/54000 (68%)] Loss: -182163.859375\n",
      "Train Epoch: 27 [38016/54000 (70%)] Loss: -187284.656250\n",
      "Train Epoch: 27 [39424/54000 (73%)] Loss: -197936.093750\n",
      "Train Epoch: 27 [40832/54000 (76%)] Loss: -174147.125000\n",
      "Train Epoch: 27 [42240/54000 (78%)] Loss: -190203.921875\n",
      "Train Epoch: 27 [43648/54000 (81%)] Loss: -190285.312500\n",
      "Train Epoch: 27 [45056/54000 (83%)] Loss: -181327.921875\n",
      "Train Epoch: 27 [46464/54000 (86%)] Loss: -191037.890625\n",
      "Train Epoch: 27 [47872/54000 (89%)] Loss: -195096.125000\n",
      "Train Epoch: 27 [49280/54000 (91%)] Loss: -195638.843750\n",
      "Train Epoch: 27 [50688/54000 (94%)] Loss: -189292.843750\n",
      "Train Epoch: 27 [52096/54000 (96%)] Loss: -188326.281250\n",
      "    epoch          : 27\n",
      "    loss           : -190311.50751345695\n",
      "    val_loss       : -197995.37185594512\n",
      "Train Epoch: 28 [0/54000 (0%)] Loss: -218470.875000\n",
      "Train Epoch: 28 [1408/54000 (3%)] Loss: -182722.281250\n",
      "Train Epoch: 28 [2816/54000 (5%)] Loss: -170108.578125\n",
      "Train Epoch: 28 [4224/54000 (8%)] Loss: -180395.109375\n",
      "Train Epoch: 28 [5632/54000 (10%)] Loss: -176135.468750\n",
      "Train Epoch: 28 [7040/54000 (13%)] Loss: -216165.875000\n",
      "Train Epoch: 28 [8448/54000 (16%)] Loss: -184069.484375\n",
      "Train Epoch: 28 [9856/54000 (18%)] Loss: -196713.890625\n",
      "Train Epoch: 28 [11264/54000 (21%)] Loss: -204308.734375\n",
      "Train Epoch: 28 [12672/54000 (23%)] Loss: -189401.437500\n",
      "Train Epoch: 28 [14080/54000 (26%)] Loss: -219205.437500\n",
      "Train Epoch: 28 [15488/54000 (29%)] Loss: -182147.828125\n",
      "Train Epoch: 28 [16896/54000 (31%)] Loss: -193972.921875\n",
      "Train Epoch: 28 [18304/54000 (34%)] Loss: -184013.671875\n",
      "Train Epoch: 28 [19712/54000 (37%)] Loss: -190328.328125\n",
      "Train Epoch: 28 [21120/54000 (39%)] Loss: -165064.375000\n",
      "Train Epoch: 28 [22528/54000 (42%)] Loss: -162989.296875\n",
      "Train Epoch: 28 [23936/54000 (44%)] Loss: -221043.890625\n",
      "Train Epoch: 28 [25344/54000 (47%)] Loss: -182820.250000\n",
      "Train Epoch: 28 [26752/54000 (50%)] Loss: -165878.109375\n",
      "Train Epoch: 28 [28160/54000 (52%)] Loss: -191353.031250\n",
      "Train Epoch: 28 [29568/54000 (55%)] Loss: -172121.062500\n",
      "Train Epoch: 28 [30976/54000 (57%)] Loss: -189070.812500\n",
      "Train Epoch: 28 [32384/54000 (60%)] Loss: -197574.656250\n",
      "Train Epoch: 28 [33792/54000 (63%)] Loss: -184213.984375\n",
      "Train Epoch: 28 [35200/54000 (65%)] Loss: -176706.500000\n",
      "Train Epoch: 28 [36608/54000 (68%)] Loss: -216401.437500\n",
      "Train Epoch: 28 [38016/54000 (70%)] Loss: -184832.812500\n",
      "Train Epoch: 28 [39424/54000 (73%)] Loss: -169597.812500\n",
      "Train Epoch: 28 [40832/54000 (76%)] Loss: -194134.234375\n",
      "Train Epoch: 28 [42240/54000 (78%)] Loss: -185087.546875\n",
      "Train Epoch: 28 [43648/54000 (81%)] Loss: -198888.937500\n",
      "Train Epoch: 28 [45056/54000 (83%)] Loss: -195234.031250\n",
      "Train Epoch: 28 [46464/54000 (86%)] Loss: -197605.593750\n",
      "Train Epoch: 28 [47872/54000 (89%)] Loss: -194590.359375\n",
      "Train Epoch: 28 [49280/54000 (91%)] Loss: -188984.031250\n",
      "Train Epoch: 28 [50688/54000 (94%)] Loss: -198812.515625\n",
      "Train Epoch: 28 [52096/54000 (96%)] Loss: -175984.125000\n",
      "    epoch          : 28\n",
      "    loss           : -190271.34109599283\n",
      "    val_loss       : -198739.25152439025\n",
      "Train Epoch: 29 [0/54000 (0%)] Loss: -212943.140625\n",
      "Train Epoch: 29 [1408/54000 (3%)] Loss: -176202.812500\n",
      "Train Epoch: 29 [2816/54000 (5%)] Loss: -191813.812500\n",
      "Train Epoch: 29 [4224/54000 (8%)] Loss: -198320.437500\n",
      "Train Epoch: 29 [5632/54000 (10%)] Loss: -185672.656250\n",
      "Train Epoch: 29 [7040/54000 (13%)] Loss: -185342.203125\n",
      "Train Epoch: 29 [8448/54000 (16%)] Loss: -169843.312500\n",
      "Train Epoch: 29 [9856/54000 (18%)] Loss: -195633.578125\n",
      "Train Epoch: 29 [11264/54000 (21%)] Loss: -185830.531250\n",
      "Train Epoch: 29 [12672/54000 (23%)] Loss: -191724.281250\n",
      "Train Epoch: 29 [14080/54000 (26%)] Loss: -170220.406250\n",
      "Train Epoch: 29 [15488/54000 (29%)] Loss: -180204.109375\n",
      "Train Epoch: 29 [16896/54000 (31%)] Loss: -196089.843750\n",
      "Train Epoch: 29 [18304/54000 (34%)] Loss: -210693.875000\n",
      "Train Epoch: 29 [19712/54000 (37%)] Loss: -177854.093750\n",
      "Train Epoch: 29 [21120/54000 (39%)] Loss: -191786.796875\n",
      "Train Epoch: 29 [22528/54000 (42%)] Loss: -184209.937500\n",
      "Train Epoch: 29 [23936/54000 (44%)] Loss: -187527.906250\n",
      "Train Epoch: 29 [25344/54000 (47%)] Loss: -207149.281250\n",
      "Train Epoch: 29 [26752/54000 (50%)] Loss: -169129.843750\n",
      "Train Epoch: 29 [28160/54000 (52%)] Loss: -192554.843750\n",
      "Train Epoch: 29 [29568/54000 (55%)] Loss: -205885.843750\n",
      "Train Epoch: 29 [30976/54000 (57%)] Loss: -198240.078125\n",
      "Train Epoch: 29 [32384/54000 (60%)] Loss: -200828.250000\n",
      "Train Epoch: 29 [33792/54000 (63%)] Loss: -171622.031250\n",
      "Train Epoch: 29 [35200/54000 (65%)] Loss: -195304.906250\n",
      "Train Epoch: 29 [36608/54000 (68%)] Loss: -179041.562500\n",
      "Train Epoch: 29 [38016/54000 (70%)] Loss: -191291.984375\n",
      "Train Epoch: 29 [39424/54000 (73%)] Loss: -178771.250000\n",
      "Train Epoch: 29 [40832/54000 (76%)] Loss: -201543.843750\n",
      "Train Epoch: 29 [42240/54000 (78%)] Loss: -179937.500000\n",
      "Train Epoch: 29 [43648/54000 (81%)] Loss: -199613.718750\n",
      "Train Epoch: 29 [45056/54000 (83%)] Loss: -216516.468750\n",
      "Train Epoch: 29 [46464/54000 (86%)] Loss: -197862.906250\n",
      "Train Epoch: 29 [47872/54000 (89%)] Loss: -181979.968750\n",
      "Train Epoch: 29 [49280/54000 (91%)] Loss: -196195.156250\n",
      "Train Epoch: 29 [50688/54000 (94%)] Loss: -177446.359375\n",
      "Train Epoch: 29 [52096/54000 (96%)] Loss: -219980.156250\n",
      "    epoch          : 29\n",
      "    loss           : -191290.38142942585\n",
      "    val_loss       : -199376.99821360517\n",
      "Train Epoch: 30 [0/54000 (0%)] Loss: -172891.203125\n",
      "Train Epoch: 30 [1408/54000 (3%)] Loss: -180659.718750\n",
      "Train Epoch: 30 [2816/54000 (5%)] Loss: -164491.765625\n",
      "Train Epoch: 30 [4224/54000 (8%)] Loss: -181840.421875\n",
      "Train Epoch: 30 [5632/54000 (10%)] Loss: -196625.453125\n",
      "Train Epoch: 30 [7040/54000 (13%)] Loss: -183056.671875\n",
      "Train Epoch: 30 [8448/54000 (16%)] Loss: -203839.062500\n",
      "Train Epoch: 30 [9856/54000 (18%)] Loss: -206116.531250\n",
      "Train Epoch: 30 [11264/54000 (21%)] Loss: -179303.437500\n",
      "Train Epoch: 30 [12672/54000 (23%)] Loss: -192948.921875\n",
      "Train Epoch: 30 [14080/54000 (26%)] Loss: -173111.156250\n",
      "Train Epoch: 30 [15488/54000 (29%)] Loss: -193375.296875\n",
      "Train Epoch: 30 [16896/54000 (31%)] Loss: -184987.281250\n",
      "Train Epoch: 30 [18304/54000 (34%)] Loss: -221761.312500\n",
      "Train Epoch: 30 [19712/54000 (37%)] Loss: -191991.812500\n",
      "Train Epoch: 30 [21120/54000 (39%)] Loss: -205813.421875\n",
      "Train Epoch: 30 [22528/54000 (42%)] Loss: -166716.875000\n",
      "Train Epoch: 30 [23936/54000 (44%)] Loss: -190383.062500\n",
      "Train Epoch: 30 [25344/54000 (47%)] Loss: -220545.984375\n",
      "Train Epoch: 30 [26752/54000 (50%)] Loss: -182778.265625\n",
      "Train Epoch: 30 [28160/54000 (52%)] Loss: -180265.937500\n",
      "Train Epoch: 30 [29568/54000 (55%)] Loss: -173860.031250\n",
      "Train Epoch: 30 [30976/54000 (57%)] Loss: -177406.000000\n",
      "Train Epoch: 30 [32384/54000 (60%)] Loss: -170449.250000\n",
      "Train Epoch: 30 [33792/54000 (63%)] Loss: -181516.406250\n",
      "Train Epoch: 30 [35200/54000 (65%)] Loss: -190318.484375\n",
      "Train Epoch: 30 [36608/54000 (68%)] Loss: -191912.125000\n",
      "Train Epoch: 30 [38016/54000 (70%)] Loss: -194113.531250\n",
      "Train Epoch: 30 [39424/54000 (73%)] Loss: -193779.296875\n",
      "Train Epoch: 30 [40832/54000 (76%)] Loss: -201081.156250\n",
      "Train Epoch: 30 [42240/54000 (78%)] Loss: -188986.937500\n",
      "Train Epoch: 30 [43648/54000 (81%)] Loss: -198763.687500\n",
      "Train Epoch: 30 [45056/54000 (83%)] Loss: -190652.031250\n",
      "Train Epoch: 30 [46464/54000 (86%)] Loss: -185960.109375\n",
      "Train Epoch: 30 [47872/54000 (89%)] Loss: -186309.140625\n",
      "Train Epoch: 30 [49280/54000 (91%)] Loss: -180681.671875\n",
      "Train Epoch: 30 [50688/54000 (94%)] Loss: -180307.093750\n",
      "Train Epoch: 30 [52096/54000 (96%)] Loss: -197328.078125\n",
      "    epoch          : 30\n",
      "    loss           : -191607.48736543063\n",
      "    val_loss       : -198224.69021532012\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch30.pth ...\n",
      "Train Epoch: 31 [0/54000 (0%)] Loss: -169235.843750\n",
      "Train Epoch: 31 [1408/54000 (3%)] Loss: -192864.640625\n",
      "Train Epoch: 31 [2816/54000 (5%)] Loss: -199648.218750\n",
      "Train Epoch: 31 [4224/54000 (8%)] Loss: -185392.781250\n",
      "Train Epoch: 31 [5632/54000 (10%)] Loss: -189056.843750\n",
      "Train Epoch: 31 [7040/54000 (13%)] Loss: -177844.312500\n",
      "Train Epoch: 31 [8448/54000 (16%)] Loss: -193805.312500\n",
      "Train Epoch: 31 [9856/54000 (18%)] Loss: -183814.125000\n",
      "Train Epoch: 31 [11264/54000 (21%)] Loss: -218923.328125\n",
      "Train Epoch: 31 [12672/54000 (23%)] Loss: -198439.250000\n",
      "Train Epoch: 31 [14080/54000 (26%)] Loss: -180962.562500\n",
      "Train Epoch: 31 [15488/54000 (29%)] Loss: -180715.203125\n",
      "Train Epoch: 31 [16896/54000 (31%)] Loss: -213994.468750\n",
      "Train Epoch: 31 [18304/54000 (34%)] Loss: -195178.875000\n",
      "Train Epoch: 31 [19712/54000 (37%)] Loss: -197504.859375\n",
      "Train Epoch: 31 [21120/54000 (39%)] Loss: -196258.500000\n",
      "Train Epoch: 31 [22528/54000 (42%)] Loss: -199912.984375\n",
      "Train Epoch: 31 [23936/54000 (44%)] Loss: -193504.250000\n",
      "Train Epoch: 31 [25344/54000 (47%)] Loss: -169955.703125\n",
      "Train Epoch: 31 [26752/54000 (50%)] Loss: -178578.718750\n",
      "Train Epoch: 31 [28160/54000 (52%)] Loss: -166772.765625\n",
      "Train Epoch: 31 [29568/54000 (55%)] Loss: -190778.062500\n",
      "Train Epoch: 31 [30976/54000 (57%)] Loss: -197418.859375\n",
      "Train Epoch: 31 [32384/54000 (60%)] Loss: -194020.109375\n",
      "Train Epoch: 31 [33792/54000 (63%)] Loss: -200100.859375\n",
      "Train Epoch: 31 [35200/54000 (65%)] Loss: -198848.234375\n",
      "Train Epoch: 31 [36608/54000 (68%)] Loss: -199131.203125\n",
      "Train Epoch: 31 [38016/54000 (70%)] Loss: -185399.234375\n",
      "Train Epoch: 31 [39424/54000 (73%)] Loss: -163553.250000\n",
      "Train Epoch: 31 [40832/54000 (76%)] Loss: -194465.156250\n",
      "Train Epoch: 31 [42240/54000 (78%)] Loss: -180463.578125\n",
      "Train Epoch: 31 [43648/54000 (81%)] Loss: -194544.312500\n",
      "Train Epoch: 31 [45056/54000 (83%)] Loss: -220478.515625\n",
      "Train Epoch: 31 [46464/54000 (86%)] Loss: -200702.343750\n",
      "Train Epoch: 31 [47872/54000 (89%)] Loss: -190614.218750\n",
      "Train Epoch: 31 [49280/54000 (91%)] Loss: -181669.531250\n",
      "Train Epoch: 31 [50688/54000 (94%)] Loss: -165364.687500\n",
      "Train Epoch: 31 [52096/54000 (96%)] Loss: -185778.093750\n",
      "    epoch          : 31\n",
      "    loss           : -192034.41196919855\n",
      "    val_loss       : -200528.96944073934\n",
      "Train Epoch: 32 [0/54000 (0%)] Loss: -180586.921875\n",
      "Train Epoch: 32 [1408/54000 (3%)] Loss: -205824.562500\n",
      "Train Epoch: 32 [2816/54000 (5%)] Loss: -205200.906250\n",
      "Train Epoch: 32 [4224/54000 (8%)] Loss: -200900.718750\n",
      "Train Epoch: 32 [5632/54000 (10%)] Loss: -175789.078125\n",
      "Train Epoch: 32 [7040/54000 (13%)] Loss: -184421.125000\n",
      "Train Epoch: 32 [8448/54000 (16%)] Loss: -186749.609375\n",
      "Train Epoch: 32 [9856/54000 (18%)] Loss: -189411.265625\n",
      "Train Epoch: 32 [11264/54000 (21%)] Loss: -190336.937500\n",
      "Train Epoch: 32 [12672/54000 (23%)] Loss: -186202.203125\n",
      "Train Epoch: 32 [14080/54000 (26%)] Loss: -188511.078125\n",
      "Train Epoch: 32 [15488/54000 (29%)] Loss: -189152.718750\n",
      "Train Epoch: 32 [16896/54000 (31%)] Loss: -187481.656250\n",
      "Train Epoch: 32 [18304/54000 (34%)] Loss: -198141.687500\n",
      "Train Epoch: 32 [19712/54000 (37%)] Loss: -187495.500000\n",
      "Train Epoch: 32 [21120/54000 (39%)] Loss: -185312.109375\n",
      "Train Epoch: 32 [22528/54000 (42%)] Loss: -195363.078125\n",
      "Train Epoch: 32 [23936/54000 (44%)] Loss: -198798.171875\n",
      "Train Epoch: 32 [25344/54000 (47%)] Loss: -186567.890625\n",
      "Train Epoch: 32 [26752/54000 (50%)] Loss: -193086.937500\n",
      "Train Epoch: 32 [28160/54000 (52%)] Loss: -181523.468750\n",
      "Train Epoch: 32 [29568/54000 (55%)] Loss: -187728.968750\n",
      "Train Epoch: 32 [30976/54000 (57%)] Loss: -187154.656250\n",
      "Train Epoch: 32 [32384/54000 (60%)] Loss: -186557.609375\n",
      "Train Epoch: 32 [33792/54000 (63%)] Loss: -178835.671875\n",
      "Train Epoch: 32 [35200/54000 (65%)] Loss: -190617.187500\n",
      "Train Epoch: 32 [36608/54000 (68%)] Loss: -165817.734375\n",
      "Train Epoch: 32 [38016/54000 (70%)] Loss: -199761.250000\n",
      "Train Epoch: 32 [39424/54000 (73%)] Loss: -200543.531250\n",
      "Train Epoch: 32 [40832/54000 (76%)] Loss: -195953.093750\n",
      "Train Epoch: 32 [42240/54000 (78%)] Loss: -181940.031250\n",
      "Train Epoch: 32 [43648/54000 (81%)] Loss: -183854.015625\n",
      "Train Epoch: 32 [45056/54000 (83%)] Loss: -186718.578125\n",
      "Train Epoch: 32 [46464/54000 (86%)] Loss: -180128.328125\n",
      "Train Epoch: 32 [47872/54000 (89%)] Loss: -195446.203125\n",
      "Train Epoch: 32 [49280/54000 (91%)] Loss: -195378.390625\n",
      "Train Epoch: 32 [50688/54000 (94%)] Loss: -191026.984375\n",
      "Train Epoch: 32 [52096/54000 (96%)] Loss: -190016.609375\n",
      "    epoch          : 32\n",
      "    loss           : -192584.35948714116\n",
      "    val_loss       : -199951.94269245426\n",
      "Train Epoch: 33 [0/54000 (0%)] Loss: -221062.015625\n",
      "Train Epoch: 33 [1408/54000 (3%)] Loss: -202498.984375\n",
      "Train Epoch: 33 [2816/54000 (5%)] Loss: -207374.375000\n",
      "Train Epoch: 33 [4224/54000 (8%)] Loss: -199632.187500\n",
      "Train Epoch: 33 [5632/54000 (10%)] Loss: -194101.437500\n",
      "Train Epoch: 33 [7040/54000 (13%)] Loss: -180385.843750\n",
      "Train Epoch: 33 [8448/54000 (16%)] Loss: -175822.406250\n",
      "Train Epoch: 33 [9856/54000 (18%)] Loss: -194124.984375\n",
      "Train Epoch: 33 [11264/54000 (21%)] Loss: -175203.890625\n",
      "Train Epoch: 33 [12672/54000 (23%)] Loss: -163513.046875\n",
      "Train Epoch: 33 [14080/54000 (26%)] Loss: -195981.812500\n",
      "Train Epoch: 33 [15488/54000 (29%)] Loss: -219472.640625\n",
      "Train Epoch: 33 [16896/54000 (31%)] Loss: -187919.218750\n",
      "Train Epoch: 33 [18304/54000 (34%)] Loss: -184456.062500\n",
      "Train Epoch: 33 [19712/54000 (37%)] Loss: -185625.562500\n",
      "Train Epoch: 33 [21120/54000 (39%)] Loss: -221898.609375\n",
      "Train Epoch: 33 [22528/54000 (42%)] Loss: -197996.187500\n",
      "Train Epoch: 33 [23936/54000 (44%)] Loss: -172879.265625\n",
      "Train Epoch: 33 [25344/54000 (47%)] Loss: -173140.765625\n",
      "Train Epoch: 33 [26752/54000 (50%)] Loss: -175217.390625\n",
      "Train Epoch: 33 [28160/54000 (52%)] Loss: -222492.687500\n",
      "Train Epoch: 33 [29568/54000 (55%)] Loss: -200061.906250\n",
      "Train Epoch: 33 [30976/54000 (57%)] Loss: -186949.359375\n",
      "Train Epoch: 33 [32384/54000 (60%)] Loss: -184183.500000\n",
      "Train Epoch: 33 [33792/54000 (63%)] Loss: -189300.593750\n",
      "Train Epoch: 33 [35200/54000 (65%)] Loss: -207339.546875\n",
      "Train Epoch: 33 [36608/54000 (68%)] Loss: -192989.531250\n",
      "Train Epoch: 33 [38016/54000 (70%)] Loss: -184475.625000\n",
      "Train Epoch: 33 [39424/54000 (73%)] Loss: -207915.531250\n",
      "Train Epoch: 33 [40832/54000 (76%)] Loss: -207786.296875\n",
      "Train Epoch: 33 [42240/54000 (78%)] Loss: -200807.093750\n",
      "Train Epoch: 33 [43648/54000 (81%)] Loss: -191257.281250\n",
      "Train Epoch: 33 [45056/54000 (83%)] Loss: -170811.234375\n",
      "Train Epoch: 33 [46464/54000 (86%)] Loss: -201507.312500\n",
      "Train Epoch: 33 [47872/54000 (89%)] Loss: -200474.515625\n",
      "Train Epoch: 33 [49280/54000 (91%)] Loss: -196079.390625\n",
      "Train Epoch: 33 [50688/54000 (94%)] Loss: -214945.906250\n",
      "Train Epoch: 33 [52096/54000 (96%)] Loss: -201024.250000\n",
      "    epoch          : 33\n",
      "    loss           : -192615.85077751195\n",
      "    val_loss       : -201055.84615567836\n",
      "Train Epoch: 34 [0/54000 (0%)] Loss: -187118.703125\n",
      "Train Epoch: 34 [1408/54000 (3%)] Loss: -180646.406250\n",
      "Train Epoch: 34 [2816/54000 (5%)] Loss: -192185.468750\n",
      "Train Epoch: 34 [4224/54000 (8%)] Loss: -204894.562500\n",
      "Train Epoch: 34 [5632/54000 (10%)] Loss: -191277.500000\n",
      "Train Epoch: 34 [7040/54000 (13%)] Loss: -191821.906250\n",
      "Train Epoch: 34 [8448/54000 (16%)] Loss: -163145.328125\n",
      "Train Epoch: 34 [9856/54000 (18%)] Loss: -166753.031250\n",
      "Train Epoch: 34 [11264/54000 (21%)] Loss: -193433.218750\n",
      "Train Epoch: 34 [12672/54000 (23%)] Loss: -223340.375000\n",
      "Train Epoch: 34 [14080/54000 (26%)] Loss: -196882.531250\n",
      "Train Epoch: 34 [15488/54000 (29%)] Loss: -178614.843750\n",
      "Train Epoch: 34 [16896/54000 (31%)] Loss: -181165.390625\n",
      "Train Epoch: 34 [18304/54000 (34%)] Loss: -206640.406250\n",
      "Train Epoch: 34 [19712/54000 (37%)] Loss: -192585.875000\n",
      "Train Epoch: 34 [21120/54000 (39%)] Loss: -189909.312500\n",
      "Train Epoch: 34 [22528/54000 (42%)] Loss: -169062.218750\n",
      "Train Epoch: 34 [23936/54000 (44%)] Loss: -195176.062500\n",
      "Train Epoch: 34 [25344/54000 (47%)] Loss: -192765.453125\n",
      "Train Epoch: 34 [26752/54000 (50%)] Loss: -180614.953125\n",
      "Train Epoch: 34 [28160/54000 (52%)] Loss: -192056.890625\n",
      "Train Epoch: 34 [29568/54000 (55%)] Loss: -200736.843750\n",
      "Train Epoch: 34 [30976/54000 (57%)] Loss: -189645.078125\n",
      "Train Epoch: 34 [32384/54000 (60%)] Loss: -189436.015625\n",
      "Train Epoch: 34 [33792/54000 (63%)] Loss: -204979.500000\n",
      "Train Epoch: 34 [35200/54000 (65%)] Loss: -184086.484375\n",
      "Train Epoch: 34 [36608/54000 (68%)] Loss: -188086.000000\n",
      "Train Epoch: 34 [38016/54000 (70%)] Loss: -196850.062500\n",
      "Train Epoch: 34 [39424/54000 (73%)] Loss: -211447.578125\n",
      "Train Epoch: 34 [40832/54000 (76%)] Loss: -181760.031250\n",
      "Train Epoch: 34 [42240/54000 (78%)] Loss: -201239.203125\n",
      "Train Epoch: 34 [43648/54000 (81%)] Loss: -170698.125000\n",
      "Train Epoch: 34 [45056/54000 (83%)] Loss: -191618.406250\n",
      "Train Epoch: 34 [46464/54000 (86%)] Loss: -184303.375000\n",
      "Train Epoch: 34 [47872/54000 (89%)] Loss: -171889.359375\n",
      "Train Epoch: 34 [49280/54000 (91%)] Loss: -184335.359375\n",
      "Train Epoch: 34 [50688/54000 (94%)] Loss: -212846.031250\n",
      "Train Epoch: 34 [52096/54000 (96%)] Loss: -194187.593750\n",
      "    epoch          : 34\n",
      "    loss           : -193037.41309061006\n",
      "    val_loss       : -201801.02702219892\n",
      "Train Epoch: 35 [0/54000 (0%)] Loss: -215926.500000\n",
      "Train Epoch: 35 [1408/54000 (3%)] Loss: -171023.828125\n",
      "Train Epoch: 35 [2816/54000 (5%)] Loss: -186866.609375\n",
      "Train Epoch: 35 [4224/54000 (8%)] Loss: -207905.890625\n",
      "Train Epoch: 35 [5632/54000 (10%)] Loss: -200022.062500\n",
      "Train Epoch: 35 [7040/54000 (13%)] Loss: -198539.109375\n",
      "Train Epoch: 35 [8448/54000 (16%)] Loss: -195211.656250\n",
      "Train Epoch: 35 [9856/54000 (18%)] Loss: -221230.843750\n",
      "Train Epoch: 35 [11264/54000 (21%)] Loss: -189562.671875\n",
      "Train Epoch: 35 [12672/54000 (23%)] Loss: -195094.796875\n",
      "Train Epoch: 35 [14080/54000 (26%)] Loss: -187651.953125\n",
      "Train Epoch: 35 [15488/54000 (29%)] Loss: -217963.203125\n",
      "Train Epoch: 35 [16896/54000 (31%)] Loss: -192160.015625\n",
      "Train Epoch: 35 [18304/54000 (34%)] Loss: -210213.687500\n",
      "Train Epoch: 35 [19712/54000 (37%)] Loss: -188922.171875\n",
      "Train Epoch: 35 [21120/54000 (39%)] Loss: -192808.031250\n",
      "Train Epoch: 35 [22528/54000 (42%)] Loss: -181029.421875\n",
      "Train Epoch: 35 [23936/54000 (44%)] Loss: -186159.046875\n",
      "Train Epoch: 35 [25344/54000 (47%)] Loss: -191030.562500\n",
      "Train Epoch: 35 [26752/54000 (50%)] Loss: -202759.562500\n",
      "Train Epoch: 35 [28160/54000 (52%)] Loss: -223595.937500\n",
      "Train Epoch: 35 [29568/54000 (55%)] Loss: -185696.562500\n",
      "Train Epoch: 35 [30976/54000 (57%)] Loss: -223227.515625\n",
      "Train Epoch: 35 [32384/54000 (60%)] Loss: -186850.546875\n",
      "Train Epoch: 35 [33792/54000 (63%)] Loss: -201236.968750\n",
      "Train Epoch: 35 [35200/54000 (65%)] Loss: -170355.437500\n",
      "Train Epoch: 35 [36608/54000 (68%)] Loss: -202033.687500\n",
      "Train Epoch: 35 [38016/54000 (70%)] Loss: -224185.312500\n",
      "Train Epoch: 35 [39424/54000 (73%)] Loss: -197061.687500\n",
      "Train Epoch: 35 [40832/54000 (76%)] Loss: -193687.718750\n",
      "Train Epoch: 35 [42240/54000 (78%)] Loss: -201475.812500\n",
      "Train Epoch: 35 [43648/54000 (81%)] Loss: -217222.437500\n",
      "Train Epoch: 35 [45056/54000 (83%)] Loss: -183930.765625\n",
      "Train Epoch: 35 [46464/54000 (86%)] Loss: -182010.296875\n",
      "Train Epoch: 35 [47872/54000 (89%)] Loss: -183606.656250\n",
      "Train Epoch: 35 [49280/54000 (91%)] Loss: -219632.109375\n",
      "Train Epoch: 35 [50688/54000 (94%)] Loss: -194264.750000\n",
      "Train Epoch: 35 [52096/54000 (96%)] Loss: -198909.328125\n",
      "    epoch          : 35\n",
      "    loss           : -193699.1082909689\n",
      "    val_loss       : -200306.9442168445\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch35.pth ...\n",
      "Train Epoch: 36 [0/54000 (0%)] Loss: -194198.656250\n",
      "Train Epoch: 36 [1408/54000 (3%)] Loss: -188408.578125\n",
      "Train Epoch: 36 [2816/54000 (5%)] Loss: -200601.406250\n",
      "Train Epoch: 36 [4224/54000 (8%)] Loss: -197348.781250\n",
      "Train Epoch: 36 [5632/54000 (10%)] Loss: -186211.843750\n",
      "Train Epoch: 36 [7040/54000 (13%)] Loss: -197441.000000\n",
      "Train Epoch: 36 [8448/54000 (16%)] Loss: -181522.000000\n",
      "Train Epoch: 36 [9856/54000 (18%)] Loss: -199552.171875\n",
      "Train Epoch: 36 [11264/54000 (21%)] Loss: -181416.109375\n",
      "Train Epoch: 36 [12672/54000 (23%)] Loss: -198333.453125\n",
      "Train Epoch: 36 [14080/54000 (26%)] Loss: -184291.531250\n",
      "Train Epoch: 36 [15488/54000 (29%)] Loss: -187889.484375\n",
      "Train Epoch: 36 [16896/54000 (31%)] Loss: -189770.656250\n",
      "Train Epoch: 36 [18304/54000 (34%)] Loss: -183329.640625\n",
      "Train Epoch: 36 [19712/54000 (37%)] Loss: -194549.437500\n",
      "Train Epoch: 36 [21120/54000 (39%)] Loss: -196307.281250\n",
      "Train Epoch: 36 [22528/54000 (42%)] Loss: -188903.296875\n",
      "Train Epoch: 36 [23936/54000 (44%)] Loss: -193183.062500\n",
      "Train Epoch: 36 [25344/54000 (47%)] Loss: -199633.437500\n",
      "Train Epoch: 36 [26752/54000 (50%)] Loss: -185631.546875\n",
      "Train Epoch: 36 [28160/54000 (52%)] Loss: -219843.093750\n",
      "Train Epoch: 36 [29568/54000 (55%)] Loss: -181805.703125\n",
      "Train Epoch: 36 [30976/54000 (57%)] Loss: -173309.984375\n",
      "Train Epoch: 36 [32384/54000 (60%)] Loss: -208268.171875\n",
      "Train Epoch: 36 [33792/54000 (63%)] Loss: -214720.156250\n",
      "Train Epoch: 36 [35200/54000 (65%)] Loss: -195916.500000\n",
      "Train Epoch: 36 [36608/54000 (68%)] Loss: -185444.093750\n",
      "Train Epoch: 36 [38016/54000 (70%)] Loss: -201658.468750\n",
      "Train Epoch: 36 [39424/54000 (73%)] Loss: -223269.265625\n",
      "Train Epoch: 36 [40832/54000 (76%)] Loss: -196203.000000\n",
      "Train Epoch: 36 [42240/54000 (78%)] Loss: -190151.531250\n",
      "Train Epoch: 36 [43648/54000 (81%)] Loss: -212740.562500\n",
      "Train Epoch: 36 [45056/54000 (83%)] Loss: -223764.390625\n",
      "Train Epoch: 36 [46464/54000 (86%)] Loss: -194753.484375\n",
      "Train Epoch: 36 [47872/54000 (89%)] Loss: -201300.578125\n",
      "Train Epoch: 36 [49280/54000 (91%)] Loss: -196797.890625\n",
      "Train Epoch: 36 [50688/54000 (94%)] Loss: -180495.312500\n",
      "Train Epoch: 36 [52096/54000 (96%)] Loss: -212233.468750\n",
      "    epoch          : 36\n",
      "    loss           : -194478.90239982057\n",
      "    val_loss       : -202020.00210794588\n",
      "Train Epoch: 37 [0/54000 (0%)] Loss: -224450.265625\n",
      "Train Epoch: 37 [1408/54000 (3%)] Loss: -180666.296875\n",
      "Train Epoch: 37 [2816/54000 (5%)] Loss: -204363.812500\n",
      "Train Epoch: 37 [4224/54000 (8%)] Loss: -197325.937500\n",
      "Train Epoch: 37 [5632/54000 (10%)] Loss: -192418.437500\n",
      "Train Epoch: 37 [7040/54000 (13%)] Loss: -186417.500000\n",
      "Train Epoch: 37 [8448/54000 (16%)] Loss: -186257.343750\n",
      "Train Epoch: 37 [9856/54000 (18%)] Loss: -187369.906250\n",
      "Train Epoch: 37 [11264/54000 (21%)] Loss: -175193.968750\n",
      "Train Epoch: 37 [12672/54000 (23%)] Loss: -185360.734375\n",
      "Train Epoch: 37 [14080/54000 (26%)] Loss: -183525.515625\n",
      "Train Epoch: 37 [15488/54000 (29%)] Loss: -200136.421875\n",
      "Train Epoch: 37 [16896/54000 (31%)] Loss: -197857.125000\n",
      "Train Epoch: 37 [18304/54000 (34%)] Loss: -200922.406250\n",
      "Train Epoch: 37 [19712/54000 (37%)] Loss: -192087.125000\n",
      "Train Epoch: 37 [21120/54000 (39%)] Loss: -203332.687500\n",
      "Train Epoch: 37 [22528/54000 (42%)] Loss: -194986.671875\n",
      "Train Epoch: 37 [23936/54000 (44%)] Loss: -183542.312500\n",
      "Train Epoch: 37 [25344/54000 (47%)] Loss: -185993.015625\n",
      "Train Epoch: 37 [26752/54000 (50%)] Loss: -193567.140625\n",
      "Train Epoch: 37 [28160/54000 (52%)] Loss: -194799.156250\n",
      "Train Epoch: 37 [29568/54000 (55%)] Loss: -194370.578125\n",
      "Train Epoch: 37 [30976/54000 (57%)] Loss: -213635.812500\n",
      "Train Epoch: 37 [32384/54000 (60%)] Loss: -200062.562500\n",
      "Train Epoch: 37 [33792/54000 (63%)] Loss: -195494.093750\n",
      "Train Epoch: 37 [35200/54000 (65%)] Loss: -197456.828125\n",
      "Train Epoch: 37 [36608/54000 (68%)] Loss: -184732.609375\n",
      "Train Epoch: 37 [38016/54000 (70%)] Loss: -189383.546875\n",
      "Train Epoch: 37 [39424/54000 (73%)] Loss: -200258.281250\n",
      "Train Epoch: 37 [40832/54000 (76%)] Loss: -200672.875000\n",
      "Train Epoch: 37 [42240/54000 (78%)] Loss: -169762.562500\n",
      "Train Epoch: 37 [43648/54000 (81%)] Loss: -196975.281250\n",
      "Train Epoch: 37 [45056/54000 (83%)] Loss: -195377.109375\n",
      "Train Epoch: 37 [46464/54000 (86%)] Loss: -194210.156250\n",
      "Train Epoch: 37 [47872/54000 (89%)] Loss: -191756.671875\n",
      "Train Epoch: 37 [49280/54000 (91%)] Loss: -192834.812500\n",
      "Train Epoch: 37 [50688/54000 (94%)] Loss: -188391.468750\n",
      "Train Epoch: 37 [52096/54000 (96%)] Loss: -180273.375000\n",
      "    epoch          : 37\n",
      "    loss           : -194842.04066985645\n",
      "    val_loss       : -200876.85065739328\n",
      "Train Epoch: 38 [0/54000 (0%)] Loss: -215822.359375\n",
      "Train Epoch: 38 [1408/54000 (3%)] Loss: -218243.250000\n",
      "Train Epoch: 38 [2816/54000 (5%)] Loss: -183810.937500\n",
      "Train Epoch: 38 [4224/54000 (8%)] Loss: -200704.625000\n",
      "Train Epoch: 38 [5632/54000 (10%)] Loss: -204388.500000\n",
      "Train Epoch: 38 [7040/54000 (13%)] Loss: -208868.953125\n",
      "Train Epoch: 38 [8448/54000 (16%)] Loss: -197201.625000\n",
      "Train Epoch: 38 [9856/54000 (18%)] Loss: -181766.937500\n",
      "Train Epoch: 38 [11264/54000 (21%)] Loss: -188424.937500\n",
      "Train Epoch: 38 [12672/54000 (23%)] Loss: -196728.859375\n",
      "Train Epoch: 38 [14080/54000 (26%)] Loss: -195638.390625\n",
      "Train Epoch: 38 [15488/54000 (29%)] Loss: -197496.750000\n",
      "Train Epoch: 38 [16896/54000 (31%)] Loss: -221635.437500\n",
      "Train Epoch: 38 [18304/54000 (34%)] Loss: -190315.468750\n",
      "Train Epoch: 38 [19712/54000 (37%)] Loss: -182894.093750\n",
      "Train Epoch: 38 [21120/54000 (39%)] Loss: -188472.765625\n",
      "Train Epoch: 38 [22528/54000 (42%)] Loss: -174338.562500\n",
      "Train Epoch: 38 [23936/54000 (44%)] Loss: -192390.046875\n",
      "Train Epoch: 38 [25344/54000 (47%)] Loss: -195692.921875\n",
      "Train Epoch: 38 [26752/54000 (50%)] Loss: -191008.546875\n",
      "Train Epoch: 38 [28160/54000 (52%)] Loss: -219229.046875\n",
      "Train Epoch: 38 [29568/54000 (55%)] Loss: -190985.343750\n",
      "Train Epoch: 38 [30976/54000 (57%)] Loss: -199692.968750\n",
      "Train Epoch: 38 [32384/54000 (60%)] Loss: -198782.953125\n",
      "Train Epoch: 38 [33792/54000 (63%)] Loss: -197907.296875\n",
      "Train Epoch: 38 [35200/54000 (65%)] Loss: -185228.234375\n",
      "Train Epoch: 38 [36608/54000 (68%)] Loss: -202742.671875\n",
      "Train Epoch: 38 [38016/54000 (70%)] Loss: -205429.125000\n",
      "Train Epoch: 38 [39424/54000 (73%)] Loss: -186021.031250\n",
      "Train Epoch: 38 [40832/54000 (76%)] Loss: -226349.187500\n",
      "Train Epoch: 38 [42240/54000 (78%)] Loss: -170107.875000\n",
      "Train Epoch: 38 [43648/54000 (81%)] Loss: -201276.796875\n",
      "Train Epoch: 38 [45056/54000 (83%)] Loss: -200584.781250\n",
      "Train Epoch: 38 [46464/54000 (86%)] Loss: -196130.437500\n",
      "Train Epoch: 38 [47872/54000 (89%)] Loss: -213367.890625\n",
      "Train Epoch: 38 [49280/54000 (91%)] Loss: -195002.656250\n",
      "Train Epoch: 38 [50688/54000 (94%)] Loss: -194610.781250\n",
      "Train Epoch: 38 [52096/54000 (96%)] Loss: -173022.437500\n",
      "    epoch          : 38\n",
      "    loss           : -194822.9932715311\n",
      "    val_loss       : -202281.9638195503\n",
      "Train Epoch: 39 [0/54000 (0%)] Loss: -194607.406250\n",
      "Train Epoch: 39 [1408/54000 (3%)] Loss: -214016.843750\n",
      "Train Epoch: 39 [2816/54000 (5%)] Loss: -204725.609375\n",
      "Train Epoch: 39 [4224/54000 (8%)] Loss: -198068.140625\n",
      "Train Epoch: 39 [5632/54000 (10%)] Loss: -195551.531250\n",
      "Train Epoch: 39 [7040/54000 (13%)] Loss: -196767.078125\n",
      "Train Epoch: 39 [8448/54000 (16%)] Loss: -196610.671875\n",
      "Train Epoch: 39 [9856/54000 (18%)] Loss: -185647.718750\n",
      "Train Epoch: 39 [11264/54000 (21%)] Loss: -201065.359375\n",
      "Train Epoch: 39 [12672/54000 (23%)] Loss: -199059.671875\n",
      "Train Epoch: 39 [14080/54000 (26%)] Loss: -190323.531250\n",
      "Train Epoch: 39 [15488/54000 (29%)] Loss: -196748.421875\n",
      "Train Epoch: 39 [16896/54000 (31%)] Loss: -196399.937500\n",
      "Train Epoch: 39 [18304/54000 (34%)] Loss: -199740.718750\n",
      "Train Epoch: 39 [19712/54000 (37%)] Loss: -190452.500000\n",
      "Train Epoch: 39 [21120/54000 (39%)] Loss: -200043.296875\n",
      "Train Epoch: 39 [22528/54000 (42%)] Loss: -186620.156250\n",
      "Train Epoch: 39 [23936/54000 (44%)] Loss: -198304.171875\n",
      "Train Epoch: 39 [25344/54000 (47%)] Loss: -203627.406250\n",
      "Train Epoch: 39 [26752/54000 (50%)] Loss: -201206.609375\n",
      "Train Epoch: 39 [28160/54000 (52%)] Loss: -195921.031250\n",
      "Train Epoch: 39 [29568/54000 (55%)] Loss: -188983.203125\n",
      "Train Epoch: 39 [30976/54000 (57%)] Loss: -225925.203125\n",
      "Train Epoch: 39 [32384/54000 (60%)] Loss: -185512.921875\n",
      "Train Epoch: 39 [33792/54000 (63%)] Loss: -197146.625000\n",
      "Train Epoch: 39 [35200/54000 (65%)] Loss: -195829.140625\n",
      "Train Epoch: 39 [36608/54000 (68%)] Loss: -225780.296875\n",
      "Train Epoch: 39 [38016/54000 (70%)] Loss: -191605.343750\n",
      "Train Epoch: 39 [39424/54000 (73%)] Loss: -182971.312500\n",
      "Train Epoch: 39 [40832/54000 (76%)] Loss: -188838.906250\n",
      "Train Epoch: 39 [42240/54000 (78%)] Loss: -197570.875000\n",
      "Train Epoch: 39 [43648/54000 (81%)] Loss: -216330.625000\n",
      "Train Epoch: 39 [45056/54000 (83%)] Loss: -201263.593750\n",
      "Train Epoch: 39 [46464/54000 (86%)] Loss: -188340.296875\n",
      "Train Epoch: 39 [47872/54000 (89%)] Loss: -189907.906250\n",
      "Train Epoch: 39 [49280/54000 (91%)] Loss: -187600.437500\n",
      "Train Epoch: 39 [50688/54000 (94%)] Loss: -191526.062500\n",
      "Train Epoch: 39 [52096/54000 (96%)] Loss: -197976.703125\n",
      "    epoch          : 39\n",
      "    loss           : -195389.4426958732\n",
      "    val_loss       : -201994.4922827744\n",
      "Train Epoch: 40 [0/54000 (0%)] Loss: -177040.265625\n",
      "Train Epoch: 40 [1408/54000 (3%)] Loss: -175005.843750\n",
      "Train Epoch: 40 [2816/54000 (5%)] Loss: -197806.484375\n",
      "Train Epoch: 40 [4224/54000 (8%)] Loss: -201281.656250\n",
      "Train Epoch: 40 [5632/54000 (10%)] Loss: -191880.750000\n",
      "Train Epoch: 40 [7040/54000 (13%)] Loss: -186351.484375\n",
      "Train Epoch: 40 [8448/54000 (16%)] Loss: -193935.156250\n",
      "Train Epoch: 40 [9856/54000 (18%)] Loss: -193235.968750\n",
      "Train Epoch: 40 [11264/54000 (21%)] Loss: -199043.765625\n",
      "Train Epoch: 40 [12672/54000 (23%)] Loss: -182626.765625\n",
      "Train Epoch: 40 [14080/54000 (26%)] Loss: -224754.703125\n",
      "Train Epoch: 40 [15488/54000 (29%)] Loss: -197602.281250\n",
      "Train Epoch: 40 [16896/54000 (31%)] Loss: -199078.562500\n",
      "Train Epoch: 40 [18304/54000 (34%)] Loss: -204407.093750\n",
      "Train Epoch: 40 [19712/54000 (37%)] Loss: -172244.281250\n",
      "Train Epoch: 40 [21120/54000 (39%)] Loss: -183980.156250\n",
      "Train Epoch: 40 [22528/54000 (42%)] Loss: -173000.546875\n",
      "Train Epoch: 40 [23936/54000 (44%)] Loss: -188213.234375\n",
      "Train Epoch: 40 [25344/54000 (47%)] Loss: -170921.453125\n",
      "Train Epoch: 40 [26752/54000 (50%)] Loss: -197924.437500\n",
      "Train Epoch: 40 [28160/54000 (52%)] Loss: -226400.734375\n",
      "Train Epoch: 40 [29568/54000 (55%)] Loss: -182382.437500\n",
      "Train Epoch: 40 [30976/54000 (57%)] Loss: -193396.546875\n",
      "Train Epoch: 40 [32384/54000 (60%)] Loss: -193403.343750\n",
      "Train Epoch: 40 [33792/54000 (63%)] Loss: -202755.468750\n",
      "Train Epoch: 40 [35200/54000 (65%)] Loss: -197282.125000\n",
      "Train Epoch: 40 [36608/54000 (68%)] Loss: -194330.734375\n",
      "Train Epoch: 40 [38016/54000 (70%)] Loss: -190336.093750\n",
      "Train Epoch: 40 [39424/54000 (73%)] Loss: -193208.968750\n",
      "Train Epoch: 40 [40832/54000 (76%)] Loss: -189794.484375\n",
      "Train Epoch: 40 [42240/54000 (78%)] Loss: -184441.375000\n",
      "Train Epoch: 40 [43648/54000 (81%)] Loss: -190880.640625\n",
      "Train Epoch: 40 [45056/54000 (83%)] Loss: -187990.593750\n",
      "Train Epoch: 40 [46464/54000 (86%)] Loss: -198315.031250\n",
      "Train Epoch: 40 [47872/54000 (89%)] Loss: -186109.031250\n",
      "Train Epoch: 40 [49280/54000 (91%)] Loss: -199351.250000\n",
      "Train Epoch: 40 [50688/54000 (94%)] Loss: -196772.296875\n",
      "Train Epoch: 40 [52096/54000 (96%)] Loss: -194947.343750\n",
      "    epoch          : 40\n",
      "    loss           : -195218.81549043063\n",
      "    val_loss       : -202557.75264386434\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch40.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 41 [0/54000 (0%)] Loss: -187460.437500\n",
      "Train Epoch: 41 [1408/54000 (3%)] Loss: -202434.453125\n",
      "Train Epoch: 41 [2816/54000 (5%)] Loss: -192224.578125\n",
      "Train Epoch: 41 [4224/54000 (8%)] Loss: -182961.500000\n",
      "Train Epoch: 41 [5632/54000 (10%)] Loss: -173114.062500\n",
      "Train Epoch: 41 [7040/54000 (13%)] Loss: -200418.171875\n",
      "Train Epoch: 41 [8448/54000 (16%)] Loss: -192916.843750\n",
      "Train Epoch: 41 [9856/54000 (18%)] Loss: -203423.328125\n",
      "Train Epoch: 41 [11264/54000 (21%)] Loss: -216871.828125\n",
      "Train Epoch: 41 [12672/54000 (23%)] Loss: -186440.312500\n",
      "Train Epoch: 41 [14080/54000 (26%)] Loss: -186875.078125\n",
      "Train Epoch: 41 [15488/54000 (29%)] Loss: -185381.296875\n",
      "Train Epoch: 41 [16896/54000 (31%)] Loss: -225365.921875\n",
      "Train Epoch: 41 [18304/54000 (34%)] Loss: -208982.031250\n",
      "Train Epoch: 41 [19712/54000 (37%)] Loss: -204218.468750\n",
      "Train Epoch: 41 [21120/54000 (39%)] Loss: -209556.593750\n",
      "Train Epoch: 41 [22528/54000 (42%)] Loss: -171034.328125\n",
      "Train Epoch: 41 [23936/54000 (44%)] Loss: -198060.078125\n",
      "Train Epoch: 41 [25344/54000 (47%)] Loss: -212602.968750\n",
      "Train Epoch: 41 [26752/54000 (50%)] Loss: -193614.031250\n",
      "Train Epoch: 41 [28160/54000 (52%)] Loss: -211374.093750\n",
      "Train Epoch: 41 [29568/54000 (55%)] Loss: -203456.625000\n",
      "Train Epoch: 41 [30976/54000 (57%)] Loss: -189151.203125\n",
      "Train Epoch: 41 [32384/54000 (60%)] Loss: -225660.984375\n",
      "Train Epoch: 41 [33792/54000 (63%)] Loss: -176465.265625\n",
      "Train Epoch: 41 [35200/54000 (65%)] Loss: -189914.968750\n",
      "Train Epoch: 41 [36608/54000 (68%)] Loss: -192193.703125\n",
      "Train Epoch: 41 [38016/54000 (70%)] Loss: -191968.000000\n",
      "Train Epoch: 41 [39424/54000 (73%)] Loss: -206647.906250\n",
      "Train Epoch: 41 [40832/54000 (76%)] Loss: -201872.843750\n",
      "Train Epoch: 41 [42240/54000 (78%)] Loss: -212285.171875\n",
      "Train Epoch: 41 [43648/54000 (81%)] Loss: -202694.687500\n",
      "Train Epoch: 41 [45056/54000 (83%)] Loss: -191684.937500\n",
      "Train Epoch: 41 [46464/54000 (86%)] Loss: -198457.812500\n",
      "Train Epoch: 41 [47872/54000 (89%)] Loss: -176127.593750\n",
      "Train Epoch: 41 [49280/54000 (91%)] Loss: -172931.921875\n",
      "Train Epoch: 41 [50688/54000 (94%)] Loss: -173775.656250\n",
      "Train Epoch: 41 [52096/54000 (96%)] Loss: -194894.250000\n",
      "    epoch          : 41\n",
      "    loss           : -196335.53857655503\n",
      "    val_loss       : -202566.37080792684\n",
      "Train Epoch: 42 [0/54000 (0%)] Loss: -224239.343750\n",
      "Train Epoch: 42 [1408/54000 (3%)] Loss: -206800.406250\n",
      "Train Epoch: 42 [2816/54000 (5%)] Loss: -195385.281250\n",
      "Train Epoch: 42 [4224/54000 (8%)] Loss: -191885.906250\n",
      "Train Epoch: 42 [5632/54000 (10%)] Loss: -187627.812500\n",
      "Train Epoch: 42 [7040/54000 (13%)] Loss: -172527.343750\n",
      "Train Epoch: 42 [8448/54000 (16%)] Loss: -174027.031250\n",
      "Train Epoch: 42 [9856/54000 (18%)] Loss: -218929.171875\n",
      "Train Epoch: 42 [11264/54000 (21%)] Loss: -194387.156250\n",
      "Train Epoch: 42 [12672/54000 (23%)] Loss: -205627.296875\n",
      "Train Epoch: 42 [14080/54000 (26%)] Loss: -204432.875000\n",
      "Train Epoch: 42 [15488/54000 (29%)] Loss: -194253.203125\n",
      "Train Epoch: 42 [16896/54000 (31%)] Loss: -224484.328125\n",
      "Train Epoch: 42 [18304/54000 (34%)] Loss: -188694.984375\n",
      "Train Epoch: 42 [19712/54000 (37%)] Loss: -182745.421875\n",
      "Train Epoch: 42 [21120/54000 (39%)] Loss: -188677.078125\n",
      "Train Epoch: 42 [22528/54000 (42%)] Loss: -199325.687500\n",
      "Train Epoch: 42 [23936/54000 (44%)] Loss: -185525.531250\n",
      "Train Epoch: 42 [25344/54000 (47%)] Loss: -193713.218750\n",
      "Train Epoch: 42 [26752/54000 (50%)] Loss: -202290.203125\n",
      "Train Epoch: 42 [28160/54000 (52%)] Loss: -212109.437500\n",
      "Train Epoch: 42 [29568/54000 (55%)] Loss: -224615.203125\n",
      "Train Epoch: 42 [30976/54000 (57%)] Loss: -197518.140625\n",
      "Train Epoch: 42 [32384/54000 (60%)] Loss: -188777.359375\n",
      "Train Epoch: 42 [33792/54000 (63%)] Loss: -173005.375000\n",
      "Train Epoch: 42 [35200/54000 (65%)] Loss: -186455.812500\n",
      "Train Epoch: 42 [36608/54000 (68%)] Loss: -177553.750000\n",
      "Train Epoch: 42 [38016/54000 (70%)] Loss: -198859.375000\n",
      "Train Epoch: 42 [39424/54000 (73%)] Loss: -176740.593750\n",
      "Train Epoch: 42 [40832/54000 (76%)] Loss: -225322.906250\n",
      "Train Epoch: 42 [42240/54000 (78%)] Loss: -194571.843750\n",
      "Train Epoch: 42 [43648/54000 (81%)] Loss: -197972.734375\n",
      "Train Epoch: 42 [45056/54000 (83%)] Loss: -186620.984375\n",
      "Train Epoch: 42 [46464/54000 (86%)] Loss: -205362.781250\n",
      "Train Epoch: 42 [47872/54000 (89%)] Loss: -190629.625000\n",
      "Train Epoch: 42 [49280/54000 (91%)] Loss: -210686.484375\n",
      "Train Epoch: 42 [50688/54000 (94%)] Loss: -196251.015625\n",
      "Train Epoch: 42 [52096/54000 (96%)] Loss: -188968.218750\n",
      "    epoch          : 42\n",
      "    loss           : -196494.89709180623\n",
      "    val_loss       : -205136.15797684834\n",
      "Train Epoch: 43 [0/54000 (0%)] Loss: -195959.296875\n",
      "Train Epoch: 43 [1408/54000 (3%)] Loss: -210206.843750\n",
      "Train Epoch: 43 [2816/54000 (5%)] Loss: -201887.859375\n",
      "Train Epoch: 43 [4224/54000 (8%)] Loss: -188851.343750\n",
      "Train Epoch: 43 [5632/54000 (10%)] Loss: -197345.578125\n",
      "Train Epoch: 43 [7040/54000 (13%)] Loss: -199278.531250\n",
      "Train Epoch: 43 [8448/54000 (16%)] Loss: -188660.562500\n",
      "Train Epoch: 43 [9856/54000 (18%)] Loss: -186893.437500\n",
      "Train Epoch: 43 [11264/54000 (21%)] Loss: -193477.812500\n",
      "Train Epoch: 43 [12672/54000 (23%)] Loss: -197107.125000\n",
      "Train Epoch: 43 [14080/54000 (26%)] Loss: -208444.078125\n",
      "Train Epoch: 43 [15488/54000 (29%)] Loss: -221046.968750\n",
      "Train Epoch: 43 [16896/54000 (31%)] Loss: -191840.328125\n",
      "Train Epoch: 43 [18304/54000 (34%)] Loss: -183724.468750\n",
      "Train Epoch: 43 [19712/54000 (37%)] Loss: -180206.468750\n",
      "Train Epoch: 43 [21120/54000 (39%)] Loss: -198679.921875\n",
      "Train Epoch: 43 [22528/54000 (42%)] Loss: -185693.375000\n",
      "Train Epoch: 43 [23936/54000 (44%)] Loss: -198631.984375\n",
      "Train Epoch: 43 [25344/54000 (47%)] Loss: -226297.421875\n",
      "Train Epoch: 43 [26752/54000 (50%)] Loss: -172258.109375\n",
      "Train Epoch: 43 [28160/54000 (52%)] Loss: -170066.312500\n",
      "Train Epoch: 43 [29568/54000 (55%)] Loss: -211869.125000\n",
      "Train Epoch: 43 [30976/54000 (57%)] Loss: -204554.828125\n",
      "Train Epoch: 43 [32384/54000 (60%)] Loss: -225740.015625\n",
      "Train Epoch: 43 [33792/54000 (63%)] Loss: -203281.031250\n",
      "Train Epoch: 43 [35200/54000 (65%)] Loss: -191419.000000\n",
      "Train Epoch: 43 [36608/54000 (68%)] Loss: -213812.656250\n",
      "Train Epoch: 43 [38016/54000 (70%)] Loss: -184443.093750\n",
      "Train Epoch: 43 [39424/54000 (73%)] Loss: -197079.718750\n",
      "Train Epoch: 43 [40832/54000 (76%)] Loss: -177284.875000\n",
      "Train Epoch: 43 [42240/54000 (78%)] Loss: -193291.609375\n",
      "Train Epoch: 43 [43648/54000 (81%)] Loss: -202494.031250\n",
      "Train Epoch: 43 [45056/54000 (83%)] Loss: -213923.250000\n",
      "Train Epoch: 43 [46464/54000 (86%)] Loss: -185697.718750\n",
      "Train Epoch: 43 [47872/54000 (89%)] Loss: -185655.000000\n",
      "Train Epoch: 43 [49280/54000 (91%)] Loss: -199291.609375\n",
      "Train Epoch: 43 [50688/54000 (94%)] Loss: -199278.265625\n",
      "Train Epoch: 43 [52096/54000 (96%)] Loss: -194259.687500\n",
      "    epoch          : 43\n",
      "    loss           : -196319.5831339713\n",
      "    val_loss       : -203808.25750285824\n",
      "Train Epoch: 44 [0/54000 (0%)] Loss: -186161.953125\n",
      "Train Epoch: 44 [1408/54000 (3%)] Loss: -180007.828125\n",
      "Train Epoch: 44 [2816/54000 (5%)] Loss: -194850.125000\n",
      "Train Epoch: 44 [4224/54000 (8%)] Loss: -186879.703125\n",
      "Train Epoch: 44 [5632/54000 (10%)] Loss: -193849.671875\n",
      "Train Epoch: 44 [7040/54000 (13%)] Loss: -186505.812500\n",
      "Train Epoch: 44 [8448/54000 (16%)] Loss: -187840.250000\n",
      "Train Epoch: 44 [9856/54000 (18%)] Loss: -193519.625000\n",
      "Train Epoch: 44 [11264/54000 (21%)] Loss: -201712.515625\n",
      "Train Epoch: 44 [12672/54000 (23%)] Loss: -194684.031250\n",
      "Train Epoch: 44 [14080/54000 (26%)] Loss: -191412.046875\n",
      "Train Epoch: 44 [15488/54000 (29%)] Loss: -187179.546875\n",
      "Train Epoch: 44 [16896/54000 (31%)] Loss: -190567.906250\n",
      "Train Epoch: 44 [18304/54000 (34%)] Loss: -200271.000000\n",
      "Train Epoch: 44 [19712/54000 (37%)] Loss: -219828.156250\n",
      "Train Epoch: 44 [21120/54000 (39%)] Loss: -187579.437500\n",
      "Train Epoch: 44 [22528/54000 (42%)] Loss: -196708.406250\n",
      "Train Epoch: 44 [23936/54000 (44%)] Loss: -193727.875000\n",
      "Train Epoch: 44 [25344/54000 (47%)] Loss: -194134.062500\n",
      "Train Epoch: 44 [26752/54000 (50%)] Loss: -201946.843750\n",
      "Train Epoch: 44 [28160/54000 (52%)] Loss: -185669.546875\n",
      "Train Epoch: 44 [29568/54000 (55%)] Loss: -199341.687500\n",
      "Train Epoch: 44 [30976/54000 (57%)] Loss: -203716.281250\n",
      "Train Epoch: 44 [32384/54000 (60%)] Loss: -212195.687500\n",
      "Train Epoch: 44 [33792/54000 (63%)] Loss: -195253.046875\n",
      "Train Epoch: 44 [35200/54000 (65%)] Loss: -202040.000000\n",
      "Train Epoch: 44 [36608/54000 (68%)] Loss: -187011.203125\n",
      "Train Epoch: 44 [38016/54000 (70%)] Loss: -198129.718750\n",
      "Train Epoch: 44 [39424/54000 (73%)] Loss: -178989.734375\n",
      "Train Epoch: 44 [40832/54000 (76%)] Loss: -202542.828125\n",
      "Train Epoch: 44 [42240/54000 (78%)] Loss: -227353.078125\n",
      "Train Epoch: 44 [43648/54000 (81%)] Loss: -197938.656250\n",
      "Train Epoch: 44 [45056/54000 (83%)] Loss: -198972.484375\n",
      "Train Epoch: 44 [46464/54000 (86%)] Loss: -198161.796875\n",
      "Train Epoch: 44 [47872/54000 (89%)] Loss: -197733.296875\n",
      "Train Epoch: 44 [49280/54000 (91%)] Loss: -200035.750000\n",
      "Train Epoch: 44 [50688/54000 (94%)] Loss: -198280.312500\n",
      "Train Epoch: 44 [52096/54000 (96%)] Loss: -181323.468750\n",
      "    epoch          : 44\n",
      "    loss           : -196341.13318630384\n",
      "    val_loss       : -205283.71933355564\n",
      "Train Epoch: 45 [0/54000 (0%)] Loss: -216890.187500\n",
      "Train Epoch: 45 [1408/54000 (3%)] Loss: -195551.656250\n",
      "Train Epoch: 45 [2816/54000 (5%)] Loss: -197622.359375\n",
      "Train Epoch: 45 [4224/54000 (8%)] Loss: -191448.781250\n",
      "Train Epoch: 45 [5632/54000 (10%)] Loss: -189118.421875\n",
      "Train Epoch: 45 [7040/54000 (13%)] Loss: -175219.593750\n",
      "Train Epoch: 45 [8448/54000 (16%)] Loss: -191278.125000\n",
      "Train Epoch: 45 [9856/54000 (18%)] Loss: -208634.531250\n",
      "Train Epoch: 45 [11264/54000 (21%)] Loss: -192716.843750\n",
      "Train Epoch: 45 [12672/54000 (23%)] Loss: -211828.109375\n",
      "Train Epoch: 45 [14080/54000 (26%)] Loss: -198217.656250\n",
      "Train Epoch: 45 [15488/54000 (29%)] Loss: -202936.843750\n",
      "Train Epoch: 45 [16896/54000 (31%)] Loss: -179603.093750\n",
      "Train Epoch: 45 [18304/54000 (34%)] Loss: -189580.421875\n",
      "Train Epoch: 45 [19712/54000 (37%)] Loss: -188118.062500\n",
      "Train Epoch: 45 [21120/54000 (39%)] Loss: -195474.765625\n",
      "Train Epoch: 45 [22528/54000 (42%)] Loss: -226519.359375\n",
      "Train Epoch: 45 [23936/54000 (44%)] Loss: -198843.609375\n",
      "Train Epoch: 45 [25344/54000 (47%)] Loss: -214817.093750\n",
      "Train Epoch: 45 [26752/54000 (50%)] Loss: -175970.750000\n",
      "Train Epoch: 45 [28160/54000 (52%)] Loss: -196596.953125\n",
      "Train Epoch: 45 [29568/54000 (55%)] Loss: -190722.062500\n",
      "Train Epoch: 45 [30976/54000 (57%)] Loss: -195242.734375\n",
      "Train Epoch: 45 [32384/54000 (60%)] Loss: -200596.890625\n",
      "Train Epoch: 45 [33792/54000 (63%)] Loss: -215350.015625\n",
      "Train Epoch: 45 [35200/54000 (65%)] Loss: -212445.437500\n",
      "Train Epoch: 45 [36608/54000 (68%)] Loss: -198694.953125\n",
      "Train Epoch: 45 [38016/54000 (70%)] Loss: -186218.734375\n",
      "Train Epoch: 45 [39424/54000 (73%)] Loss: -221373.515625\n",
      "Train Epoch: 45 [40832/54000 (76%)] Loss: -189883.500000\n",
      "Train Epoch: 45 [42240/54000 (78%)] Loss: -187573.078125\n",
      "Train Epoch: 45 [43648/54000 (81%)] Loss: -185490.625000\n",
      "Train Epoch: 45 [45056/54000 (83%)] Loss: -184120.203125\n",
      "Train Epoch: 45 [46464/54000 (86%)] Loss: -199551.859375\n",
      "Train Epoch: 45 [47872/54000 (89%)] Loss: -196361.203125\n",
      "Train Epoch: 45 [49280/54000 (91%)] Loss: -199712.843750\n",
      "Train Epoch: 45 [50688/54000 (94%)] Loss: -227180.609375\n",
      "Train Epoch: 45 [52096/54000 (96%)] Loss: -205886.671875\n",
      "    epoch          : 45\n",
      "    loss           : -197726.64013905503\n",
      "    val_loss       : -205682.44871855946\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch45.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 46 [0/54000 (0%)] Loss: -192131.250000\n",
      "Train Epoch: 46 [1408/54000 (3%)] Loss: -196088.015625\n",
      "Train Epoch: 46 [2816/54000 (5%)] Loss: -206493.375000\n",
      "Train Epoch: 46 [4224/54000 (8%)] Loss: -206506.968750\n",
      "Train Epoch: 46 [5632/54000 (10%)] Loss: -189158.218750\n",
      "Train Epoch: 46 [7040/54000 (13%)] Loss: -188732.281250\n",
      "Train Epoch: 46 [8448/54000 (16%)] Loss: -175408.656250\n",
      "Train Epoch: 46 [9856/54000 (18%)] Loss: -181901.046875\n",
      "Train Epoch: 46 [11264/54000 (21%)] Loss: -191754.031250\n",
      "Train Epoch: 46 [12672/54000 (23%)] Loss: -184365.093750\n",
      "Train Epoch: 46 [14080/54000 (26%)] Loss: -185873.859375\n",
      "Train Epoch: 46 [15488/54000 (29%)] Loss: -198848.875000\n",
      "Train Epoch: 46 [16896/54000 (31%)] Loss: -203076.656250\n",
      "Train Epoch: 46 [18304/54000 (34%)] Loss: -204864.156250\n",
      "Train Epoch: 46 [19712/54000 (37%)] Loss: -191084.718750\n",
      "Train Epoch: 46 [21120/54000 (39%)] Loss: -197396.500000\n",
      "Train Epoch: 46 [22528/54000 (42%)] Loss: -206914.531250\n",
      "Train Epoch: 46 [23936/54000 (44%)] Loss: -190606.468750\n",
      "Train Epoch: 46 [25344/54000 (47%)] Loss: -189525.156250\n",
      "Train Epoch: 46 [26752/54000 (50%)] Loss: -191137.750000\n",
      "Train Epoch: 46 [28160/54000 (52%)] Loss: -222081.906250\n",
      "Train Epoch: 46 [29568/54000 (55%)] Loss: -178968.500000\n",
      "Train Epoch: 46 [30976/54000 (57%)] Loss: -189577.031250\n",
      "Train Epoch: 46 [32384/54000 (60%)] Loss: -191731.468750\n",
      "Train Epoch: 46 [33792/54000 (63%)] Loss: -203345.328125\n",
      "Train Epoch: 46 [35200/54000 (65%)] Loss: -212550.796875\n",
      "Train Epoch: 46 [36608/54000 (68%)] Loss: -212874.375000\n",
      "Train Epoch: 46 [38016/54000 (70%)] Loss: -191026.500000\n",
      "Train Epoch: 46 [39424/54000 (73%)] Loss: -204980.140625\n",
      "Train Epoch: 46 [40832/54000 (76%)] Loss: -173717.281250\n",
      "Train Epoch: 46 [42240/54000 (78%)] Loss: -174774.546875\n",
      "Train Epoch: 46 [43648/54000 (81%)] Loss: -198663.203125\n",
      "Train Epoch: 46 [45056/54000 (83%)] Loss: -201956.140625\n",
      "Train Epoch: 46 [46464/54000 (86%)] Loss: -205140.828125\n",
      "Train Epoch: 46 [47872/54000 (89%)] Loss: -195205.671875\n",
      "Train Epoch: 46 [49280/54000 (91%)] Loss: -195550.031250\n",
      "Train Epoch: 46 [50688/54000 (94%)] Loss: -228472.218750\n",
      "Train Epoch: 46 [52096/54000 (96%)] Loss: -176546.968750\n",
      "    epoch          : 46\n",
      "    loss           : -197498.37899970094\n",
      "    val_loss       : -203179.5399437881\n",
      "Train Epoch: 47 [0/54000 (0%)] Loss: -204954.625000\n",
      "Train Epoch: 47 [1408/54000 (3%)] Loss: -228406.109375\n",
      "Train Epoch: 47 [2816/54000 (5%)] Loss: -196101.640625\n",
      "Train Epoch: 47 [4224/54000 (8%)] Loss: -197064.843750\n",
      "Train Epoch: 47 [5632/54000 (10%)] Loss: -200333.765625\n",
      "Train Epoch: 47 [7040/54000 (13%)] Loss: -189960.687500\n",
      "Train Epoch: 47 [8448/54000 (16%)] Loss: -225758.015625\n",
      "Train Epoch: 47 [9856/54000 (18%)] Loss: -206903.468750\n",
      "Train Epoch: 47 [11264/54000 (21%)] Loss: -180699.156250\n",
      "Train Epoch: 47 [12672/54000 (23%)] Loss: -193170.781250\n",
      "Train Epoch: 47 [14080/54000 (26%)] Loss: -228955.281250\n",
      "Train Epoch: 47 [15488/54000 (29%)] Loss: -195228.515625\n",
      "Train Epoch: 47 [16896/54000 (31%)] Loss: -199775.828125\n",
      "Train Epoch: 47 [18304/54000 (34%)] Loss: -192748.890625\n",
      "Train Epoch: 47 [19712/54000 (37%)] Loss: -180689.218750\n",
      "Train Epoch: 47 [21120/54000 (39%)] Loss: -194882.203125\n",
      "Train Epoch: 47 [22528/54000 (42%)] Loss: -188977.578125\n",
      "Train Epoch: 47 [23936/54000 (44%)] Loss: -174796.453125\n",
      "Train Epoch: 47 [25344/54000 (47%)] Loss: -199923.734375\n",
      "Train Epoch: 47 [26752/54000 (50%)] Loss: -195679.421875\n",
      "Train Epoch: 47 [28160/54000 (52%)] Loss: -193585.875000\n",
      "Train Epoch: 47 [29568/54000 (55%)] Loss: -218399.578125\n",
      "Train Epoch: 47 [30976/54000 (57%)] Loss: -200518.687500\n",
      "Train Epoch: 47 [32384/54000 (60%)] Loss: -189336.781250\n",
      "Train Epoch: 47 [33792/54000 (63%)] Loss: -174867.687500\n",
      "Train Epoch: 47 [35200/54000 (65%)] Loss: -203018.156250\n",
      "Train Epoch: 47 [36608/54000 (68%)] Loss: -189169.609375\n",
      "Train Epoch: 47 [38016/54000 (70%)] Loss: -190772.031250\n",
      "Train Epoch: 47 [39424/54000 (73%)] Loss: -206004.765625\n",
      "Train Epoch: 47 [40832/54000 (76%)] Loss: -192161.562500\n",
      "Train Epoch: 47 [42240/54000 (78%)] Loss: -198718.125000\n",
      "Train Epoch: 47 [43648/54000 (81%)] Loss: -186985.125000\n",
      "Train Epoch: 47 [45056/54000 (83%)] Loss: -190209.625000\n",
      "Train Epoch: 47 [46464/54000 (86%)] Loss: -188295.656250\n",
      "Train Epoch: 47 [47872/54000 (89%)] Loss: -182650.656250\n",
      "Train Epoch: 47 [49280/54000 (91%)] Loss: -194212.531250\n",
      "Train Epoch: 47 [50688/54000 (94%)] Loss: -194085.984375\n",
      "Train Epoch: 47 [52096/54000 (96%)] Loss: -228262.359375\n",
      "    epoch          : 47\n",
      "    loss           : -197941.51880233255\n",
      "    val_loss       : -207613.85332507623\n",
      "Train Epoch: 48 [0/54000 (0%)] Loss: -205275.937500\n",
      "Train Epoch: 48 [1408/54000 (3%)] Loss: -190562.843750\n",
      "Train Epoch: 48 [2816/54000 (5%)] Loss: -199711.171875\n",
      "Train Epoch: 48 [4224/54000 (8%)] Loss: -183531.156250\n",
      "Train Epoch: 48 [5632/54000 (10%)] Loss: -187054.468750\n",
      "Train Epoch: 48 [7040/54000 (13%)] Loss: -187065.328125\n",
      "Train Epoch: 48 [8448/54000 (16%)] Loss: -195180.921875\n",
      "Train Epoch: 48 [9856/54000 (18%)] Loss: -212575.812500\n",
      "Train Epoch: 48 [11264/54000 (21%)] Loss: -211560.812500\n",
      "Train Epoch: 48 [12672/54000 (23%)] Loss: -219024.468750\n",
      "Train Epoch: 48 [14080/54000 (26%)] Loss: -192639.484375\n",
      "Train Epoch: 48 [15488/54000 (29%)] Loss: -190169.000000\n",
      "Train Epoch: 48 [16896/54000 (31%)] Loss: -192407.578125\n",
      "Train Epoch: 48 [18304/54000 (34%)] Loss: -189538.906250\n",
      "Train Epoch: 48 [19712/54000 (37%)] Loss: -185316.406250\n",
      "Train Epoch: 48 [21120/54000 (39%)] Loss: -203445.828125\n",
      "Train Epoch: 48 [22528/54000 (42%)] Loss: -207917.078125\n",
      "Train Epoch: 48 [23936/54000 (44%)] Loss: -226257.968750\n",
      "Train Epoch: 48 [25344/54000 (47%)] Loss: -192542.406250\n",
      "Train Epoch: 48 [26752/54000 (50%)] Loss: -193159.234375\n",
      "Train Epoch: 48 [28160/54000 (52%)] Loss: -183596.093750\n",
      "Train Epoch: 48 [29568/54000 (55%)] Loss: -213035.828125\n",
      "Train Epoch: 48 [30976/54000 (57%)] Loss: -203084.578125\n",
      "Train Epoch: 48 [32384/54000 (60%)] Loss: -213532.500000\n",
      "Train Epoch: 48 [33792/54000 (63%)] Loss: -192458.234375\n",
      "Train Epoch: 48 [35200/54000 (65%)] Loss: -195238.296875\n",
      "Train Epoch: 48 [36608/54000 (68%)] Loss: -195628.171875\n",
      "Train Epoch: 48 [38016/54000 (70%)] Loss: -192309.828125\n",
      "Train Epoch: 48 [39424/54000 (73%)] Loss: -192487.281250\n",
      "Train Epoch: 48 [40832/54000 (76%)] Loss: -199061.984375\n",
      "Train Epoch: 48 [42240/54000 (78%)] Loss: -204546.796875\n",
      "Train Epoch: 48 [43648/54000 (81%)] Loss: -184531.906250\n",
      "Train Epoch: 48 [45056/54000 (83%)] Loss: -182761.140625\n",
      "Train Epoch: 48 [46464/54000 (86%)] Loss: -200018.500000\n",
      "Train Epoch: 48 [47872/54000 (89%)] Loss: -189438.437500\n",
      "Train Epoch: 48 [49280/54000 (91%)] Loss: -191009.406250\n",
      "Train Epoch: 48 [50688/54000 (94%)] Loss: -199432.984375\n",
      "Train Epoch: 48 [52096/54000 (96%)] Loss: -205858.968750\n",
      "    epoch          : 48\n",
      "    loss           : -198084.61857057415\n",
      "    val_loss       : -204463.1287633384\n",
      "Train Epoch: 49 [0/54000 (0%)] Loss: -185911.703125\n",
      "Train Epoch: 49 [1408/54000 (3%)] Loss: -214055.921875\n",
      "Train Epoch: 49 [2816/54000 (5%)] Loss: -199395.234375\n",
      "Train Epoch: 49 [4224/54000 (8%)] Loss: -174325.578125\n",
      "Train Epoch: 49 [5632/54000 (10%)] Loss: -174665.609375\n",
      "Train Epoch: 49 [7040/54000 (13%)] Loss: -188901.203125\n",
      "Train Epoch: 49 [8448/54000 (16%)] Loss: -198777.656250\n",
      "Train Epoch: 49 [9856/54000 (18%)] Loss: -205419.875000\n",
      "Train Epoch: 49 [11264/54000 (21%)] Loss: -185422.187500\n",
      "Train Epoch: 49 [12672/54000 (23%)] Loss: -204789.421875\n",
      "Train Epoch: 49 [14080/54000 (26%)] Loss: -199620.062500\n",
      "Train Epoch: 49 [15488/54000 (29%)] Loss: -203057.312500\n",
      "Train Epoch: 49 [16896/54000 (31%)] Loss: -225269.937500\n",
      "Train Epoch: 49 [18304/54000 (34%)] Loss: -182400.000000\n",
      "Train Epoch: 49 [19712/54000 (37%)] Loss: -214104.921875\n",
      "Train Epoch: 49 [21120/54000 (39%)] Loss: -199694.937500\n",
      "Train Epoch: 49 [22528/54000 (42%)] Loss: -171808.250000\n",
      "Train Epoch: 49 [23936/54000 (44%)] Loss: -199866.953125\n",
      "Train Epoch: 49 [25344/54000 (47%)] Loss: -182977.125000\n",
      "Train Epoch: 49 [26752/54000 (50%)] Loss: -193021.500000\n",
      "Train Epoch: 49 [28160/54000 (52%)] Loss: -191393.812500\n",
      "Train Epoch: 49 [29568/54000 (55%)] Loss: -178604.406250\n",
      "Train Epoch: 49 [30976/54000 (57%)] Loss: -190307.437500\n",
      "Train Epoch: 49 [32384/54000 (60%)] Loss: -222964.375000\n",
      "Train Epoch: 49 [33792/54000 (63%)] Loss: -190453.218750\n",
      "Train Epoch: 49 [35200/54000 (65%)] Loss: -196935.000000\n",
      "Train Epoch: 49 [36608/54000 (68%)] Loss: -178651.734375\n",
      "Train Epoch: 49 [38016/54000 (70%)] Loss: -204320.812500\n",
      "Train Epoch: 49 [39424/54000 (73%)] Loss: -171888.031250\n",
      "Train Epoch: 49 [40832/54000 (76%)] Loss: -191629.218750\n",
      "Train Epoch: 49 [42240/54000 (78%)] Loss: -180190.250000\n",
      "Train Epoch: 49 [43648/54000 (81%)] Loss: -222543.843750\n",
      "Train Epoch: 49 [45056/54000 (83%)] Loss: -195867.828125\n",
      "Train Epoch: 49 [46464/54000 (86%)] Loss: -201371.968750\n",
      "Train Epoch: 49 [47872/54000 (89%)] Loss: -192915.203125\n",
      "Train Epoch: 49 [49280/54000 (91%)] Loss: -189485.625000\n",
      "Train Epoch: 49 [50688/54000 (94%)] Loss: -228392.781250\n",
      "Train Epoch: 49 [52096/54000 (96%)] Loss: -198021.187500\n",
      "    epoch          : 49\n",
      "    loss           : -198271.65329694975\n",
      "    val_loss       : -207551.41575362041\n",
      "Train Epoch: 50 [0/54000 (0%)] Loss: -193330.796875\n",
      "Train Epoch: 50 [1408/54000 (3%)] Loss: -198139.859375\n",
      "Train Epoch: 50 [2816/54000 (5%)] Loss: -177072.906250\n",
      "Train Epoch: 50 [4224/54000 (8%)] Loss: -190228.625000\n",
      "Train Epoch: 50 [5632/54000 (10%)] Loss: -201003.828125\n",
      "Train Epoch: 50 [7040/54000 (13%)] Loss: -206561.453125\n",
      "Train Epoch: 50 [8448/54000 (16%)] Loss: -214337.156250\n",
      "Train Epoch: 50 [9856/54000 (18%)] Loss: -207296.421875\n",
      "Train Epoch: 50 [11264/54000 (21%)] Loss: -199443.656250\n",
      "Train Epoch: 50 [12672/54000 (23%)] Loss: -193798.968750\n",
      "Train Epoch: 50 [14080/54000 (26%)] Loss: -191648.781250\n",
      "Train Epoch: 50 [15488/54000 (29%)] Loss: -208249.843750\n",
      "Train Epoch: 50 [16896/54000 (31%)] Loss: -203627.625000\n",
      "Train Epoch: 50 [18304/54000 (34%)] Loss: -217882.796875\n",
      "Train Epoch: 50 [19712/54000 (37%)] Loss: -188710.234375\n",
      "Train Epoch: 50 [21120/54000 (39%)] Loss: -198039.687500\n",
      "Train Epoch: 50 [22528/54000 (42%)] Loss: -215026.484375\n",
      "Train Epoch: 50 [23936/54000 (44%)] Loss: -220785.265625\n",
      "Train Epoch: 50 [25344/54000 (47%)] Loss: -211966.796875\n",
      "Train Epoch: 50 [26752/54000 (50%)] Loss: -200197.046875\n",
      "Train Epoch: 50 [28160/54000 (52%)] Loss: -207553.828125\n",
      "Train Epoch: 50 [29568/54000 (55%)] Loss: -222157.812500\n",
      "Train Epoch: 50 [30976/54000 (57%)] Loss: -191402.625000\n",
      "Train Epoch: 50 [32384/54000 (60%)] Loss: -194623.937500\n",
      "Train Epoch: 50 [33792/54000 (63%)] Loss: -176374.218750\n",
      "Train Epoch: 50 [35200/54000 (65%)] Loss: -181307.328125\n",
      "Train Epoch: 50 [36608/54000 (68%)] Loss: -203109.625000\n",
      "Train Epoch: 50 [38016/54000 (70%)] Loss: -205895.968750\n",
      "Train Epoch: 50 [39424/54000 (73%)] Loss: -201303.765625\n",
      "Train Epoch: 50 [40832/54000 (76%)] Loss: -198054.734375\n",
      "Train Epoch: 50 [42240/54000 (78%)] Loss: -190142.406250\n",
      "Train Epoch: 50 [43648/54000 (81%)] Loss: -192972.781250\n",
      "Train Epoch: 50 [45056/54000 (83%)] Loss: -221065.781250\n",
      "Train Epoch: 50 [46464/54000 (86%)] Loss: -194910.640625\n",
      "Train Epoch: 50 [47872/54000 (89%)] Loss: -192234.468750\n",
      "Train Epoch: 50 [49280/54000 (91%)] Loss: -190888.343750\n",
      "Train Epoch: 50 [50688/54000 (94%)] Loss: -226801.031250\n",
      "Train Epoch: 50 [52096/54000 (96%)] Loss: -190797.968750\n",
      "    epoch          : 50\n",
      "    loss           : -198249.26506429425\n",
      "    val_loss       : -207428.43136671113\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [0/54000 (0%)] Loss: -198227.000000\n",
      "Train Epoch: 51 [1408/54000 (3%)] Loss: -213736.078125\n",
      "Train Epoch: 51 [2816/54000 (5%)] Loss: -187929.359375\n",
      "Train Epoch: 51 [4224/54000 (8%)] Loss: -189785.234375\n",
      "Train Epoch: 51 [5632/54000 (10%)] Loss: -198268.609375\n",
      "Train Epoch: 51 [7040/54000 (13%)] Loss: -205107.265625\n",
      "Train Epoch: 51 [8448/54000 (16%)] Loss: -194066.265625\n",
      "Train Epoch: 51 [9856/54000 (18%)] Loss: -195372.750000\n",
      "Train Epoch: 51 [11264/54000 (21%)] Loss: -201644.828125\n",
      "Train Epoch: 51 [12672/54000 (23%)] Loss: -189023.734375\n",
      "Train Epoch: 51 [14080/54000 (26%)] Loss: -181087.750000\n",
      "Train Epoch: 51 [15488/54000 (29%)] Loss: -208290.109375\n",
      "Train Epoch: 51 [16896/54000 (31%)] Loss: -213923.687500\n",
      "Train Epoch: 51 [18304/54000 (34%)] Loss: -205667.515625\n",
      "Train Epoch: 51 [19712/54000 (37%)] Loss: -196330.843750\n",
      "Train Epoch: 51 [21120/54000 (39%)] Loss: -191218.671875\n",
      "Train Epoch: 51 [22528/54000 (42%)] Loss: -191766.000000\n",
      "Train Epoch: 51 [23936/54000 (44%)] Loss: -194876.593750\n",
      "Train Epoch: 51 [25344/54000 (47%)] Loss: -188925.812500\n",
      "Train Epoch: 51 [26752/54000 (50%)] Loss: -198860.281250\n",
      "Train Epoch: 51 [28160/54000 (52%)] Loss: -208153.781250\n",
      "Train Epoch: 51 [29568/54000 (55%)] Loss: -206072.703125\n",
      "Train Epoch: 51 [30976/54000 (57%)] Loss: -213518.437500\n",
      "Train Epoch: 51 [32384/54000 (60%)] Loss: -200809.390625\n",
      "Train Epoch: 51 [33792/54000 (63%)] Loss: -228744.109375\n",
      "Train Epoch: 51 [35200/54000 (65%)] Loss: -190358.750000\n",
      "Train Epoch: 51 [36608/54000 (68%)] Loss: -214686.000000\n",
      "Train Epoch: 51 [38016/54000 (70%)] Loss: -188128.140625\n",
      "Train Epoch: 51 [39424/54000 (73%)] Loss: -176306.531250\n",
      "Train Epoch: 51 [40832/54000 (76%)] Loss: -208260.062500\n",
      "Train Epoch: 51 [42240/54000 (78%)] Loss: -194547.500000\n",
      "Train Epoch: 51 [43648/54000 (81%)] Loss: -178315.343750\n",
      "Train Epoch: 51 [45056/54000 (83%)] Loss: -196970.000000\n",
      "Train Epoch: 51 [46464/54000 (86%)] Loss: -207910.140625\n",
      "Train Epoch: 51 [47872/54000 (89%)] Loss: -197871.921875\n",
      "Train Epoch: 51 [49280/54000 (91%)] Loss: -175129.937500\n",
      "Train Epoch: 51 [50688/54000 (94%)] Loss: -229383.406250\n",
      "Train Epoch: 51 [52096/54000 (96%)] Loss: -197856.000000\n",
      "    epoch          : 51\n",
      "    loss           : -199484.58978767943\n",
      "    val_loss       : -207319.7237399962\n",
      "Train Epoch: 52 [0/54000 (0%)] Loss: -188631.437500\n",
      "Train Epoch: 52 [1408/54000 (3%)] Loss: -197591.687500\n",
      "Train Epoch: 52 [2816/54000 (5%)] Loss: -204657.062500\n",
      "Train Epoch: 52 [4224/54000 (8%)] Loss: -208611.562500\n",
      "Train Epoch: 52 [5632/54000 (10%)] Loss: -193738.328125\n",
      "Train Epoch: 52 [7040/54000 (13%)] Loss: -197330.406250\n",
      "Train Epoch: 52 [8448/54000 (16%)] Loss: -196959.062500\n",
      "Train Epoch: 52 [9856/54000 (18%)] Loss: -192200.843750\n",
      "Train Epoch: 52 [11264/54000 (21%)] Loss: -210511.687500\n",
      "Train Epoch: 52 [12672/54000 (23%)] Loss: -192991.375000\n",
      "Train Epoch: 52 [14080/54000 (26%)] Loss: -201969.343750\n",
      "Train Epoch: 52 [15488/54000 (29%)] Loss: -196306.000000\n",
      "Train Epoch: 52 [16896/54000 (31%)] Loss: -205481.968750\n",
      "Train Epoch: 52 [18304/54000 (34%)] Loss: -204647.906250\n",
      "Train Epoch: 52 [19712/54000 (37%)] Loss: -196346.718750\n",
      "Train Epoch: 52 [21120/54000 (39%)] Loss: -206511.140625\n",
      "Train Epoch: 52 [22528/54000 (42%)] Loss: -201310.343750\n",
      "Train Epoch: 52 [23936/54000 (44%)] Loss: -182080.703125\n",
      "Train Epoch: 52 [25344/54000 (47%)] Loss: -189140.125000\n",
      "Train Epoch: 52 [26752/54000 (50%)] Loss: -227467.328125\n",
      "Train Epoch: 52 [28160/54000 (52%)] Loss: -192048.578125\n",
      "Train Epoch: 52 [29568/54000 (55%)] Loss: -207544.828125\n",
      "Train Epoch: 52 [30976/54000 (57%)] Loss: -195509.140625\n",
      "Train Epoch: 52 [32384/54000 (60%)] Loss: -208596.437500\n",
      "Train Epoch: 52 [33792/54000 (63%)] Loss: -194330.703125\n",
      "Train Epoch: 52 [35200/54000 (65%)] Loss: -215402.234375\n",
      "Train Epoch: 52 [36608/54000 (68%)] Loss: -182982.171875\n",
      "Train Epoch: 52 [38016/54000 (70%)] Loss: -191588.875000\n",
      "Train Epoch: 52 [39424/54000 (73%)] Loss: -200741.812500\n",
      "Train Epoch: 52 [40832/54000 (76%)] Loss: -165191.718750\n",
      "Train Epoch: 52 [42240/54000 (78%)] Loss: -213440.750000\n",
      "Train Epoch: 52 [43648/54000 (81%)] Loss: -205514.046875\n",
      "Train Epoch: 52 [45056/54000 (83%)] Loss: -201119.250000\n",
      "Train Epoch: 52 [46464/54000 (86%)] Loss: -191376.062500\n",
      "Train Epoch: 52 [47872/54000 (89%)] Loss: -189454.640625\n",
      "Train Epoch: 52 [49280/54000 (91%)] Loss: -179247.906250\n",
      "Train Epoch: 52 [50688/54000 (94%)] Loss: -199675.625000\n",
      "Train Epoch: 52 [52096/54000 (96%)] Loss: -199149.734375\n",
      "    epoch          : 52\n",
      "    loss           : -199432.57038726076\n",
      "    val_loss       : -207851.23684022485\n",
      "Train Epoch: 53 [0/54000 (0%)] Loss: -198615.562500\n",
      "Train Epoch: 53 [1408/54000 (3%)] Loss: -185942.062500\n",
      "Train Epoch: 53 [2816/54000 (5%)] Loss: -190779.421875\n",
      "Train Epoch: 53 [4224/54000 (8%)] Loss: -185896.000000\n",
      "Train Epoch: 53 [5632/54000 (10%)] Loss: -190380.109375\n",
      "Train Epoch: 53 [7040/54000 (13%)] Loss: -214108.343750\n",
      "Train Epoch: 53 [8448/54000 (16%)] Loss: -187866.656250\n",
      "Train Epoch: 53 [9856/54000 (18%)] Loss: -189349.609375\n",
      "Train Epoch: 53 [11264/54000 (21%)] Loss: -201650.625000\n",
      "Train Epoch: 53 [12672/54000 (23%)] Loss: -194202.781250\n",
      "Train Epoch: 53 [14080/54000 (26%)] Loss: -187889.796875\n",
      "Train Epoch: 53 [15488/54000 (29%)] Loss: -228070.750000\n",
      "Train Epoch: 53 [16896/54000 (31%)] Loss: -200795.750000\n",
      "Train Epoch: 53 [18304/54000 (34%)] Loss: -191874.406250\n",
      "Train Epoch: 53 [19712/54000 (37%)] Loss: -222112.843750\n",
      "Train Epoch: 53 [21120/54000 (39%)] Loss: -173580.312500\n",
      "Train Epoch: 53 [22528/54000 (42%)] Loss: -215928.640625\n",
      "Train Epoch: 53 [23936/54000 (44%)] Loss: -192842.906250\n",
      "Train Epoch: 53 [25344/54000 (47%)] Loss: -194353.890625\n",
      "Train Epoch: 53 [26752/54000 (50%)] Loss: -215403.000000\n",
      "Train Epoch: 53 [28160/54000 (52%)] Loss: -200583.343750\n",
      "Train Epoch: 53 [29568/54000 (55%)] Loss: -196654.937500\n",
      "Train Epoch: 53 [30976/54000 (57%)] Loss: -229174.734375\n",
      "Train Epoch: 53 [32384/54000 (60%)] Loss: -210715.953125\n",
      "Train Epoch: 53 [33792/54000 (63%)] Loss: -210131.984375\n",
      "Train Epoch: 53 [35200/54000 (65%)] Loss: -187292.187500\n",
      "Train Epoch: 53 [36608/54000 (68%)] Loss: -186674.328125\n",
      "Train Epoch: 53 [38016/54000 (70%)] Loss: -201481.937500\n",
      "Train Epoch: 53 [39424/54000 (73%)] Loss: -190185.156250\n",
      "Train Epoch: 53 [40832/54000 (76%)] Loss: -190588.687500\n",
      "Train Epoch: 53 [42240/54000 (78%)] Loss: -181818.937500\n",
      "Train Epoch: 53 [43648/54000 (81%)] Loss: -222362.640625\n",
      "Train Epoch: 53 [45056/54000 (83%)] Loss: -200031.828125\n",
      "Train Epoch: 53 [46464/54000 (86%)] Loss: -194356.531250\n",
      "Train Epoch: 53 [47872/54000 (89%)] Loss: -199518.671875\n",
      "Train Epoch: 53 [49280/54000 (91%)] Loss: -222560.187500\n",
      "Train Epoch: 53 [50688/54000 (94%)] Loss: -198453.765625\n",
      "Train Epoch: 53 [52096/54000 (96%)] Loss: -199564.937500\n",
      "    epoch          : 53\n",
      "    loss           : -199563.8686827153\n",
      "    val_loss       : -206415.24453363186\n",
      "Train Epoch: 54 [0/54000 (0%)] Loss: -184846.875000\n",
      "Train Epoch: 54 [1408/54000 (3%)] Loss: -189509.421875\n",
      "Train Epoch: 54 [2816/54000 (5%)] Loss: -190790.437500\n",
      "Train Epoch: 54 [4224/54000 (8%)] Loss: -205591.218750\n",
      "Train Epoch: 54 [5632/54000 (10%)] Loss: -193806.515625\n",
      "Train Epoch: 54 [7040/54000 (13%)] Loss: -193125.562500\n",
      "Train Epoch: 54 [8448/54000 (16%)] Loss: -191135.343750\n",
      "Train Epoch: 54 [9856/54000 (18%)] Loss: -192381.187500\n",
      "Train Epoch: 54 [11264/54000 (21%)] Loss: -200123.125000\n",
      "Train Epoch: 54 [12672/54000 (23%)] Loss: -187728.312500\n",
      "Train Epoch: 54 [14080/54000 (26%)] Loss: -229763.078125\n",
      "Train Epoch: 54 [15488/54000 (29%)] Loss: -207708.703125\n",
      "Train Epoch: 54 [16896/54000 (31%)] Loss: -189495.359375\n",
      "Train Epoch: 54 [18304/54000 (34%)] Loss: -203095.203125\n",
      "Train Epoch: 54 [19712/54000 (37%)] Loss: -202016.843750\n",
      "Train Epoch: 54 [21120/54000 (39%)] Loss: -201125.328125\n",
      "Train Epoch: 54 [22528/54000 (42%)] Loss: -203719.093750\n",
      "Train Epoch: 54 [23936/54000 (44%)] Loss: -202123.062500\n",
      "Train Epoch: 54 [25344/54000 (47%)] Loss: -189014.296875\n",
      "Train Epoch: 54 [26752/54000 (50%)] Loss: -192731.015625\n",
      "Train Epoch: 54 [28160/54000 (52%)] Loss: -208821.156250\n",
      "Train Epoch: 54 [29568/54000 (55%)] Loss: -205037.156250\n",
      "Train Epoch: 54 [30976/54000 (57%)] Loss: -194458.875000\n",
      "Train Epoch: 54 [32384/54000 (60%)] Loss: -191889.953125\n",
      "Train Epoch: 54 [33792/54000 (63%)] Loss: -178563.515625\n",
      "Train Epoch: 54 [35200/54000 (65%)] Loss: -186744.828125\n",
      "Train Epoch: 54 [36608/54000 (68%)] Loss: -197625.062500\n",
      "Train Epoch: 54 [38016/54000 (70%)] Loss: -210194.796875\n",
      "Train Epoch: 54 [39424/54000 (73%)] Loss: -191354.312500\n",
      "Train Epoch: 54 [40832/54000 (76%)] Loss: -206256.500000\n",
      "Train Epoch: 54 [42240/54000 (78%)] Loss: -196310.968750\n",
      "Train Epoch: 54 [43648/54000 (81%)] Loss: -230526.312500\n",
      "Train Epoch: 54 [45056/54000 (83%)] Loss: -201712.906250\n",
      "Train Epoch: 54 [46464/54000 (86%)] Loss: -197054.046875\n",
      "Train Epoch: 54 [47872/54000 (89%)] Loss: -195679.562500\n",
      "Train Epoch: 54 [49280/54000 (91%)] Loss: -200629.312500\n",
      "Train Epoch: 54 [50688/54000 (94%)] Loss: -201713.765625\n",
      "Train Epoch: 54 [52096/54000 (96%)] Loss: -191973.656250\n",
      "    epoch          : 54\n",
      "    loss           : -199318.0234375\n",
      "    val_loss       : -206535.27525962272\n",
      "Train Epoch: 55 [0/54000 (0%)] Loss: -229757.984375\n",
      "Train Epoch: 55 [1408/54000 (3%)] Loss: -218782.484375\n",
      "Train Epoch: 55 [2816/54000 (5%)] Loss: -190089.593750\n",
      "Train Epoch: 55 [4224/54000 (8%)] Loss: -199312.671875\n",
      "Train Epoch: 55 [5632/54000 (10%)] Loss: -195210.000000\n",
      "Train Epoch: 55 [7040/54000 (13%)] Loss: -194702.562500\n",
      "Train Epoch: 55 [8448/54000 (16%)] Loss: -207947.578125\n",
      "Train Epoch: 55 [9856/54000 (18%)] Loss: -191534.015625\n",
      "Train Epoch: 55 [11264/54000 (21%)] Loss: -214180.203125\n",
      "Train Epoch: 55 [12672/54000 (23%)] Loss: -202623.421875\n",
      "Train Epoch: 55 [14080/54000 (26%)] Loss: -191116.609375\n",
      "Train Epoch: 55 [15488/54000 (29%)] Loss: -192927.281250\n",
      "Train Epoch: 55 [16896/54000 (31%)] Loss: -195152.796875\n",
      "Train Epoch: 55 [18304/54000 (34%)] Loss: -208479.843750\n",
      "Train Epoch: 55 [19712/54000 (37%)] Loss: -178167.968750\n",
      "Train Epoch: 55 [21120/54000 (39%)] Loss: -195596.937500\n",
      "Train Epoch: 55 [22528/54000 (42%)] Loss: -194072.406250\n",
      "Train Epoch: 55 [23936/54000 (44%)] Loss: -207432.046875\n",
      "Train Epoch: 55 [25344/54000 (47%)] Loss: -209822.343750\n",
      "Train Epoch: 55 [26752/54000 (50%)] Loss: -200520.640625\n",
      "Train Epoch: 55 [28160/54000 (52%)] Loss: -207153.390625\n",
      "Train Epoch: 55 [29568/54000 (55%)] Loss: -214379.703125\n",
      "Train Epoch: 55 [30976/54000 (57%)] Loss: -201968.046875\n",
      "Train Epoch: 55 [32384/54000 (60%)] Loss: -201917.406250\n",
      "Train Epoch: 55 [33792/54000 (63%)] Loss: -201201.718750\n",
      "Train Epoch: 55 [35200/54000 (65%)] Loss: -199998.250000\n",
      "Train Epoch: 55 [36608/54000 (68%)] Loss: -208981.875000\n",
      "Train Epoch: 55 [38016/54000 (70%)] Loss: -189172.828125\n",
      "Train Epoch: 55 [39424/54000 (73%)] Loss: -205957.796875\n",
      "Train Epoch: 55 [40832/54000 (76%)] Loss: -182358.234375\n",
      "Train Epoch: 55 [42240/54000 (78%)] Loss: -200377.203125\n",
      "Train Epoch: 55 [43648/54000 (81%)] Loss: -193197.718750\n",
      "Train Epoch: 55 [45056/54000 (83%)] Loss: -181689.562500\n",
      "Train Epoch: 55 [46464/54000 (86%)] Loss: -202897.328125\n",
      "Train Epoch: 55 [47872/54000 (89%)] Loss: -196118.343750\n",
      "Train Epoch: 55 [49280/54000 (91%)] Loss: -202455.843750\n",
      "Train Epoch: 55 [50688/54000 (94%)] Loss: -189238.312500\n",
      "Train Epoch: 55 [52096/54000 (96%)] Loss: -198686.812500\n",
      "    epoch          : 55\n",
      "    loss           : -199913.72140400717\n",
      "    val_loss       : -209007.53544207316\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch55.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 56 [0/54000 (0%)] Loss: -220599.312500\n",
      "Train Epoch: 56 [1408/54000 (3%)] Loss: -200236.281250\n",
      "Train Epoch: 56 [2816/54000 (5%)] Loss: -195110.703125\n",
      "Train Epoch: 56 [4224/54000 (8%)] Loss: -190895.171875\n",
      "Train Epoch: 56 [5632/54000 (10%)] Loss: -203714.281250\n",
      "Train Epoch: 56 [7040/54000 (13%)] Loss: -207129.375000\n",
      "Train Epoch: 56 [8448/54000 (16%)] Loss: -196254.609375\n",
      "Train Epoch: 56 [9856/54000 (18%)] Loss: -181145.343750\n",
      "Train Epoch: 56 [11264/54000 (21%)] Loss: -186764.671875\n",
      "Train Epoch: 56 [12672/54000 (23%)] Loss: -207101.109375\n",
      "Train Epoch: 56 [14080/54000 (26%)] Loss: -188090.843750\n",
      "Train Epoch: 56 [15488/54000 (29%)] Loss: -192455.390625\n",
      "Train Epoch: 56 [16896/54000 (31%)] Loss: -223635.031250\n",
      "Train Epoch: 56 [18304/54000 (34%)] Loss: -192369.750000\n",
      "Train Epoch: 56 [19712/54000 (37%)] Loss: -203940.343750\n",
      "Train Epoch: 56 [21120/54000 (39%)] Loss: -193859.296875\n",
      "Train Epoch: 56 [22528/54000 (42%)] Loss: -193840.093750\n",
      "Train Epoch: 56 [23936/54000 (44%)] Loss: -189676.687500\n",
      "Train Epoch: 56 [25344/54000 (47%)] Loss: -185238.328125\n",
      "Train Epoch: 56 [26752/54000 (50%)] Loss: -187560.750000\n",
      "Train Epoch: 56 [28160/54000 (52%)] Loss: -220910.156250\n",
      "Train Epoch: 56 [29568/54000 (55%)] Loss: -190410.781250\n",
      "Train Epoch: 56 [30976/54000 (57%)] Loss: -207063.937500\n",
      "Train Epoch: 56 [32384/54000 (60%)] Loss: -190714.593750\n",
      "Train Epoch: 56 [33792/54000 (63%)] Loss: -199034.687500\n",
      "Train Epoch: 56 [35200/54000 (65%)] Loss: -223238.890625\n",
      "Train Epoch: 56 [36608/54000 (68%)] Loss: -191994.968750\n",
      "Train Epoch: 56 [38016/54000 (70%)] Loss: -191493.500000\n",
      "Train Epoch: 56 [39424/54000 (73%)] Loss: -182853.578125\n",
      "Train Epoch: 56 [40832/54000 (76%)] Loss: -200707.296875\n",
      "Train Epoch: 56 [42240/54000 (78%)] Loss: -221235.421875\n",
      "Train Epoch: 56 [43648/54000 (81%)] Loss: -203106.218750\n",
      "Train Epoch: 56 [45056/54000 (83%)] Loss: -203369.656250\n",
      "Train Epoch: 56 [46464/54000 (86%)] Loss: -223684.109375\n",
      "Train Epoch: 56 [47872/54000 (89%)] Loss: -200590.500000\n",
      "Train Epoch: 56 [49280/54000 (91%)] Loss: -205564.859375\n",
      "Train Epoch: 56 [50688/54000 (94%)] Loss: -229529.015625\n",
      "Train Epoch: 56 [52096/54000 (96%)] Loss: -206412.765625\n",
      "    epoch          : 56\n",
      "    loss           : -200270.36098235645\n",
      "    val_loss       : -208581.16539634147\n",
      "Train Epoch: 57 [0/54000 (0%)] Loss: -214205.312500\n",
      "Train Epoch: 57 [1408/54000 (3%)] Loss: -229604.031250\n",
      "Train Epoch: 57 [2816/54000 (5%)] Loss: -205323.515625\n",
      "Train Epoch: 57 [4224/54000 (8%)] Loss: -204903.468750\n",
      "Train Epoch: 57 [5632/54000 (10%)] Loss: -192720.593750\n",
      "Train Epoch: 57 [7040/54000 (13%)] Loss: -201654.625000\n",
      "Train Epoch: 57 [8448/54000 (16%)] Loss: -210019.031250\n",
      "Train Epoch: 57 [9856/54000 (18%)] Loss: -208773.218750\n",
      "Train Epoch: 57 [11264/54000 (21%)] Loss: -201054.453125\n",
      "Train Epoch: 57 [12672/54000 (23%)] Loss: -188048.156250\n",
      "Train Epoch: 57 [14080/54000 (26%)] Loss: -191336.421875\n",
      "Train Epoch: 57 [15488/54000 (29%)] Loss: -192183.109375\n",
      "Train Epoch: 57 [16896/54000 (31%)] Loss: -193730.671875\n",
      "Train Epoch: 57 [18304/54000 (34%)] Loss: -202982.343750\n",
      "Train Epoch: 57 [19712/54000 (37%)] Loss: -195362.890625\n",
      "Train Epoch: 57 [21120/54000 (39%)] Loss: -180541.375000\n",
      "Train Epoch: 57 [22528/54000 (42%)] Loss: -182166.140625\n",
      "Train Epoch: 57 [23936/54000 (44%)] Loss: -207269.687500\n",
      "Train Epoch: 57 [25344/54000 (47%)] Loss: -228249.093750\n",
      "Train Epoch: 57 [26752/54000 (50%)] Loss: -202325.968750\n",
      "Train Epoch: 57 [28160/54000 (52%)] Loss: -205152.312500\n",
      "Train Epoch: 57 [29568/54000 (55%)] Loss: -229884.359375\n",
      "Train Epoch: 57 [30976/54000 (57%)] Loss: -191882.921875\n",
      "Train Epoch: 57 [32384/54000 (60%)] Loss: -206580.031250\n",
      "Train Epoch: 57 [33792/54000 (63%)] Loss: -193861.031250\n",
      "Train Epoch: 57 [35200/54000 (65%)] Loss: -193080.218750\n",
      "Train Epoch: 57 [36608/54000 (68%)] Loss: -193912.625000\n",
      "Train Epoch: 57 [38016/54000 (70%)] Loss: -187606.500000\n",
      "Train Epoch: 57 [39424/54000 (73%)] Loss: -189751.046875\n",
      "Train Epoch: 57 [40832/54000 (76%)] Loss: -230658.843750\n",
      "Train Epoch: 57 [42240/54000 (78%)] Loss: -193726.875000\n",
      "Train Epoch: 57 [43648/54000 (81%)] Loss: -198887.750000\n",
      "Train Epoch: 57 [45056/54000 (83%)] Loss: -215277.000000\n",
      "Train Epoch: 57 [46464/54000 (86%)] Loss: -185411.625000\n",
      "Train Epoch: 57 [47872/54000 (89%)] Loss: -203812.625000\n",
      "Train Epoch: 57 [49280/54000 (91%)] Loss: -193523.125000\n",
      "Train Epoch: 57 [50688/54000 (94%)] Loss: -223488.968750\n",
      "Train Epoch: 57 [52096/54000 (96%)] Loss: -201946.375000\n",
      "    epoch          : 57\n",
      "    loss           : -199814.44632177035\n",
      "    val_loss       : -207926.12738185975\n",
      "Train Epoch: 58 [0/54000 (0%)] Loss: -231049.859375\n",
      "Train Epoch: 58 [1408/54000 (3%)] Loss: -203338.921875\n",
      "Train Epoch: 58 [2816/54000 (5%)] Loss: -210005.343750\n",
      "Train Epoch: 58 [4224/54000 (8%)] Loss: -201541.937500\n",
      "Train Epoch: 58 [5632/54000 (10%)] Loss: -184682.500000\n",
      "Train Epoch: 58 [7040/54000 (13%)] Loss: -205090.250000\n",
      "Train Epoch: 58 [8448/54000 (16%)] Loss: -203669.906250\n",
      "Train Epoch: 58 [9856/54000 (18%)] Loss: -194734.984375\n",
      "Train Epoch: 58 [11264/54000 (21%)] Loss: -189858.281250\n",
      "Train Epoch: 58 [12672/54000 (23%)] Loss: -220746.562500\n",
      "Train Epoch: 58 [14080/54000 (26%)] Loss: -208096.625000\n",
      "Train Epoch: 58 [15488/54000 (29%)] Loss: -199503.406250\n",
      "Train Epoch: 58 [16896/54000 (31%)] Loss: -194432.781250\n",
      "Train Epoch: 58 [18304/54000 (34%)] Loss: -185273.437500\n",
      "Train Epoch: 58 [19712/54000 (37%)] Loss: -223289.718750\n",
      "Train Epoch: 58 [21120/54000 (39%)] Loss: -198174.515625\n",
      "Train Epoch: 58 [22528/54000 (42%)] Loss: -207543.125000\n",
      "Train Epoch: 58 [23936/54000 (44%)] Loss: -209239.656250\n",
      "Train Epoch: 58 [25344/54000 (47%)] Loss: -198671.781250\n",
      "Train Epoch: 58 [26752/54000 (50%)] Loss: -203477.218750\n",
      "Train Epoch: 58 [28160/54000 (52%)] Loss: -208283.390625\n",
      "Train Epoch: 58 [29568/54000 (55%)] Loss: -201325.468750\n",
      "Train Epoch: 58 [30976/54000 (57%)] Loss: -191521.062500\n",
      "Train Epoch: 58 [32384/54000 (60%)] Loss: -194418.781250\n",
      "Train Epoch: 58 [33792/54000 (63%)] Loss: -192138.593750\n",
      "Train Epoch: 58 [35200/54000 (65%)] Loss: -193942.484375\n",
      "Train Epoch: 58 [36608/54000 (68%)] Loss: -190999.781250\n",
      "Train Epoch: 58 [38016/54000 (70%)] Loss: -203410.859375\n",
      "Train Epoch: 58 [39424/54000 (73%)] Loss: -220119.593750\n",
      "Train Epoch: 58 [40832/54000 (76%)] Loss: -201980.531250\n",
      "Train Epoch: 58 [42240/54000 (78%)] Loss: -191600.343750\n",
      "Train Epoch: 58 [43648/54000 (81%)] Loss: -215869.937500\n",
      "Train Epoch: 58 [45056/54000 (83%)] Loss: -199772.000000\n",
      "Train Epoch: 58 [46464/54000 (86%)] Loss: -228894.625000\n",
      "Train Epoch: 58 [47872/54000 (89%)] Loss: -204137.687500\n",
      "Train Epoch: 58 [49280/54000 (91%)] Loss: -178344.593750\n",
      "Train Epoch: 58 [50688/54000 (94%)] Loss: -202200.781250\n",
      "Train Epoch: 58 [52096/54000 (96%)] Loss: -229785.718750\n",
      "    epoch          : 58\n",
      "    loss           : -200423.83683462918\n",
      "    val_loss       : -208076.97797970654\n",
      "Train Epoch: 59 [0/54000 (0%)] Loss: -220805.578125\n",
      "Train Epoch: 59 [1408/54000 (3%)] Loss: -226151.531250\n",
      "Train Epoch: 59 [2816/54000 (5%)] Loss: -194917.406250\n",
      "Train Epoch: 59 [4224/54000 (8%)] Loss: -192344.218750\n",
      "Train Epoch: 59 [5632/54000 (10%)] Loss: -183220.718750\n",
      "Train Epoch: 59 [7040/54000 (13%)] Loss: -215196.906250\n",
      "Train Epoch: 59 [8448/54000 (16%)] Loss: -204897.718750\n",
      "Train Epoch: 59 [9856/54000 (18%)] Loss: -195102.218750\n",
      "Train Epoch: 59 [11264/54000 (21%)] Loss: -189757.640625\n",
      "Train Epoch: 59 [12672/54000 (23%)] Loss: -192730.562500\n",
      "Train Epoch: 59 [14080/54000 (26%)] Loss: -191502.484375\n",
      "Train Epoch: 59 [15488/54000 (29%)] Loss: -190855.796875\n",
      "Train Epoch: 59 [16896/54000 (31%)] Loss: -189095.171875\n",
      "Train Epoch: 59 [18304/54000 (34%)] Loss: -182943.187500\n",
      "Train Epoch: 59 [19712/54000 (37%)] Loss: -214804.015625\n",
      "Train Epoch: 59 [21120/54000 (39%)] Loss: -205258.546875\n",
      "Train Epoch: 59 [22528/54000 (42%)] Loss: -201620.171875\n",
      "Train Epoch: 59 [23936/54000 (44%)] Loss: -188987.515625\n",
      "Train Epoch: 59 [25344/54000 (47%)] Loss: -198923.687500\n",
      "Train Epoch: 59 [26752/54000 (50%)] Loss: -202333.015625\n",
      "Train Epoch: 59 [28160/54000 (52%)] Loss: -229897.609375\n",
      "Train Epoch: 59 [29568/54000 (55%)] Loss: -204230.484375\n",
      "Train Epoch: 59 [30976/54000 (57%)] Loss: -199833.234375\n",
      "Train Epoch: 59 [32384/54000 (60%)] Loss: -197736.609375\n",
      "Train Epoch: 59 [33792/54000 (63%)] Loss: -208936.468750\n",
      "Train Epoch: 59 [35200/54000 (65%)] Loss: -186081.953125\n",
      "Train Epoch: 59 [36608/54000 (68%)] Loss: -215033.515625\n",
      "Train Epoch: 59 [38016/54000 (70%)] Loss: -195665.281250\n",
      "Train Epoch: 59 [39424/54000 (73%)] Loss: -206804.406250\n",
      "Train Epoch: 59 [40832/54000 (76%)] Loss: -200001.468750\n",
      "Train Epoch: 59 [42240/54000 (78%)] Loss: -199556.875000\n",
      "Train Epoch: 59 [43648/54000 (81%)] Loss: -200760.843750\n",
      "Train Epoch: 59 [45056/54000 (83%)] Loss: -203891.671875\n",
      "Train Epoch: 59 [46464/54000 (86%)] Loss: -190788.546875\n",
      "Train Epoch: 59 [47872/54000 (89%)] Loss: -184825.812500\n",
      "Train Epoch: 59 [49280/54000 (91%)] Loss: -210392.234375\n",
      "Train Epoch: 59 [50688/54000 (94%)] Loss: -204022.312500\n",
      "Train Epoch: 59 [52096/54000 (96%)] Loss: -223861.531250\n",
      "    epoch          : 59\n",
      "    loss           : -200810.28487589714\n",
      "    val_loss       : -208419.2236209032\n",
      "Train Epoch: 60 [0/54000 (0%)] Loss: -208888.375000\n",
      "Train Epoch: 60 [1408/54000 (3%)] Loss: -211934.343750\n",
      "Train Epoch: 60 [2816/54000 (5%)] Loss: -196773.875000\n",
      "Train Epoch: 60 [4224/54000 (8%)] Loss: -196514.437500\n",
      "Train Epoch: 60 [5632/54000 (10%)] Loss: -201867.484375\n",
      "Train Epoch: 60 [7040/54000 (13%)] Loss: -207003.937500\n",
      "Train Epoch: 60 [8448/54000 (16%)] Loss: -203265.968750\n",
      "Train Epoch: 60 [9856/54000 (18%)] Loss: -199604.109375\n",
      "Train Epoch: 60 [11264/54000 (21%)] Loss: -200936.312500\n",
      "Train Epoch: 60 [12672/54000 (23%)] Loss: -199032.515625\n",
      "Train Epoch: 60 [14080/54000 (26%)] Loss: -221175.765625\n",
      "Train Epoch: 60 [15488/54000 (29%)] Loss: -200491.562500\n",
      "Train Epoch: 60 [16896/54000 (31%)] Loss: -189655.750000\n",
      "Train Epoch: 60 [18304/54000 (34%)] Loss: -185921.750000\n",
      "Train Epoch: 60 [19712/54000 (37%)] Loss: -192784.078125\n",
      "Train Epoch: 60 [21120/54000 (39%)] Loss: -191682.515625\n",
      "Train Epoch: 60 [22528/54000 (42%)] Loss: -193522.437500\n",
      "Train Epoch: 60 [23936/54000 (44%)] Loss: -203713.562500\n",
      "Train Epoch: 60 [25344/54000 (47%)] Loss: -191035.093750\n",
      "Train Epoch: 60 [26752/54000 (50%)] Loss: -201770.796875\n",
      "Train Epoch: 60 [28160/54000 (52%)] Loss: -207156.562500\n",
      "Train Epoch: 60 [29568/54000 (55%)] Loss: -203144.546875\n",
      "Train Epoch: 60 [30976/54000 (57%)] Loss: -206683.890625\n",
      "Train Epoch: 60 [32384/54000 (60%)] Loss: -209423.093750\n",
      "Train Epoch: 60 [33792/54000 (63%)] Loss: -210316.187500\n",
      "Train Epoch: 60 [35200/54000 (65%)] Loss: -223763.125000\n",
      "Train Epoch: 60 [36608/54000 (68%)] Loss: -173717.937500\n",
      "Train Epoch: 60 [38016/54000 (70%)] Loss: -201502.593750\n",
      "Train Epoch: 60 [39424/54000 (73%)] Loss: -211290.046875\n",
      "Train Epoch: 60 [40832/54000 (76%)] Loss: -205180.203125\n",
      "Train Epoch: 60 [42240/54000 (78%)] Loss: -221628.921875\n",
      "Train Epoch: 60 [43648/54000 (81%)] Loss: -207196.171875\n",
      "Train Epoch: 60 [45056/54000 (83%)] Loss: -202342.578125\n",
      "Train Epoch: 60 [46464/54000 (86%)] Loss: -176203.906250\n",
      "Train Epoch: 60 [47872/54000 (89%)] Loss: -192094.343750\n",
      "Train Epoch: 60 [49280/54000 (91%)] Loss: -223595.156250\n",
      "Train Epoch: 60 [50688/54000 (94%)] Loss: -209750.546875\n",
      "Train Epoch: 60 [52096/54000 (96%)] Loss: -208597.859375\n",
      "    epoch          : 60\n",
      "    loss           : -201134.5162978469\n",
      "    val_loss       : -208991.0879501715\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch60.pth ...\n",
      "Train Epoch: 61 [0/54000 (0%)] Loss: -222989.062500\n",
      "Train Epoch: 61 [1408/54000 (3%)] Loss: -182435.937500\n",
      "Train Epoch: 61 [2816/54000 (5%)] Loss: -192511.750000\n",
      "Train Epoch: 61 [4224/54000 (8%)] Loss: -197798.187500\n",
      "Train Epoch: 61 [5632/54000 (10%)] Loss: -186717.687500\n",
      "Train Epoch: 61 [7040/54000 (13%)] Loss: -191978.187500\n",
      "Train Epoch: 61 [8448/54000 (16%)] Loss: -207134.515625\n",
      "Train Epoch: 61 [9856/54000 (18%)] Loss: -215810.656250\n",
      "Train Epoch: 61 [11264/54000 (21%)] Loss: -206304.578125\n",
      "Train Epoch: 61 [12672/54000 (23%)] Loss: -192802.312500\n",
      "Train Epoch: 61 [14080/54000 (26%)] Loss: -192989.125000\n",
      "Train Epoch: 61 [15488/54000 (29%)] Loss: -201573.625000\n",
      "Train Epoch: 61 [16896/54000 (31%)] Loss: -209360.578125\n",
      "Train Epoch: 61 [18304/54000 (34%)] Loss: -197510.656250\n",
      "Train Epoch: 61 [19712/54000 (37%)] Loss: -191513.953125\n",
      "Train Epoch: 61 [21120/54000 (39%)] Loss: -215559.656250\n",
      "Train Epoch: 61 [22528/54000 (42%)] Loss: -180839.406250\n",
      "Train Epoch: 61 [23936/54000 (44%)] Loss: -181169.937500\n",
      "Train Epoch: 61 [25344/54000 (47%)] Loss: -204162.546875\n",
      "Train Epoch: 61 [26752/54000 (50%)] Loss: -203148.828125\n",
      "Train Epoch: 61 [28160/54000 (52%)] Loss: -195690.437500\n",
      "Train Epoch: 61 [29568/54000 (55%)] Loss: -205156.500000\n",
      "Train Epoch: 61 [30976/54000 (57%)] Loss: -203641.500000\n",
      "Train Epoch: 61 [32384/54000 (60%)] Loss: -202240.171875\n",
      "Train Epoch: 61 [33792/54000 (63%)] Loss: -209208.468750\n",
      "Train Epoch: 61 [35200/54000 (65%)] Loss: -213584.203125\n",
      "Train Epoch: 61 [36608/54000 (68%)] Loss: -196848.953125\n",
      "Train Epoch: 61 [38016/54000 (70%)] Loss: -205700.640625\n",
      "Train Epoch: 61 [39424/54000 (73%)] Loss: -230616.312500\n",
      "Train Epoch: 61 [40832/54000 (76%)] Loss: -206115.468750\n",
      "Train Epoch: 61 [42240/54000 (78%)] Loss: -192334.187500\n",
      "Train Epoch: 61 [43648/54000 (81%)] Loss: -230474.843750\n",
      "Train Epoch: 61 [45056/54000 (83%)] Loss: -209707.031250\n",
      "Train Epoch: 61 [46464/54000 (86%)] Loss: -203031.906250\n",
      "Train Epoch: 61 [47872/54000 (89%)] Loss: -203281.109375\n",
      "Train Epoch: 61 [49280/54000 (91%)] Loss: -181743.328125\n",
      "Train Epoch: 61 [50688/54000 (94%)] Loss: -230668.312500\n",
      "Train Epoch: 61 [52096/54000 (96%)] Loss: -198206.906250\n",
      "    epoch          : 61\n",
      "    loss           : -200644.97342254786\n",
      "    val_loss       : -206813.235172923\n",
      "Train Epoch: 62 [0/54000 (0%)] Loss: -184611.484375\n",
      "Train Epoch: 62 [1408/54000 (3%)] Loss: -176827.093750\n",
      "Train Epoch: 62 [2816/54000 (5%)] Loss: -206246.187500\n",
      "Train Epoch: 62 [4224/54000 (8%)] Loss: -205874.156250\n",
      "Train Epoch: 62 [5632/54000 (10%)] Loss: -193851.921875\n",
      "Train Epoch: 62 [7040/54000 (13%)] Loss: -188217.578125\n",
      "Train Epoch: 62 [8448/54000 (16%)] Loss: -205161.953125\n",
      "Train Epoch: 62 [9856/54000 (18%)] Loss: -230492.734375\n",
      "Train Epoch: 62 [11264/54000 (21%)] Loss: -202392.484375\n",
      "Train Epoch: 62 [12672/54000 (23%)] Loss: -197864.656250\n",
      "Train Epoch: 62 [14080/54000 (26%)] Loss: -208258.000000\n",
      "Train Epoch: 62 [15488/54000 (29%)] Loss: -223562.921875\n",
      "Train Epoch: 62 [16896/54000 (31%)] Loss: -203216.156250\n",
      "Train Epoch: 62 [18304/54000 (34%)] Loss: -189878.890625\n",
      "Train Epoch: 62 [19712/54000 (37%)] Loss: -174458.781250\n",
      "Train Epoch: 62 [21120/54000 (39%)] Loss: -215305.890625\n",
      "Train Epoch: 62 [22528/54000 (42%)] Loss: -216381.015625\n",
      "Train Epoch: 62 [23936/54000 (44%)] Loss: -214404.343750\n",
      "Train Epoch: 62 [25344/54000 (47%)] Loss: -203269.765625\n",
      "Train Epoch: 62 [26752/54000 (50%)] Loss: -230222.328125\n",
      "Train Epoch: 62 [28160/54000 (52%)] Loss: -194011.000000\n",
      "Train Epoch: 62 [29568/54000 (55%)] Loss: -210594.593750\n",
      "Train Epoch: 62 [30976/54000 (57%)] Loss: -197571.437500\n",
      "Train Epoch: 62 [32384/54000 (60%)] Loss: -220066.656250\n",
      "Train Epoch: 62 [33792/54000 (63%)] Loss: -201208.656250\n",
      "Train Epoch: 62 [35200/54000 (65%)] Loss: -203443.828125\n",
      "Train Epoch: 62 [36608/54000 (68%)] Loss: -183297.531250\n",
      "Train Epoch: 62 [38016/54000 (70%)] Loss: -185504.906250\n",
      "Train Epoch: 62 [39424/54000 (73%)] Loss: -187849.343750\n",
      "Train Epoch: 62 [40832/54000 (76%)] Loss: -210233.328125\n",
      "Train Epoch: 62 [42240/54000 (78%)] Loss: -188594.968750\n",
      "Train Epoch: 62 [43648/54000 (81%)] Loss: -204157.875000\n",
      "Train Epoch: 62 [45056/54000 (83%)] Loss: -222970.171875\n",
      "Train Epoch: 62 [46464/54000 (86%)] Loss: -195858.609375\n",
      "Train Epoch: 62 [47872/54000 (89%)] Loss: -209962.171875\n",
      "Train Epoch: 62 [49280/54000 (91%)] Loss: -197590.015625\n",
      "Train Epoch: 62 [50688/54000 (94%)] Loss: -199442.406250\n",
      "Train Epoch: 62 [52096/54000 (96%)] Loss: -198199.218750\n",
      "    epoch          : 62\n",
      "    loss           : -201129.8823639354\n",
      "    val_loss       : -209125.56897865853\n",
      "Train Epoch: 63 [0/54000 (0%)] Loss: -230627.171875\n",
      "Train Epoch: 63 [1408/54000 (3%)] Loss: -175295.984375\n",
      "Train Epoch: 63 [2816/54000 (5%)] Loss: -197949.250000\n",
      "Train Epoch: 63 [4224/54000 (8%)] Loss: -191191.765625\n",
      "Train Epoch: 63 [5632/54000 (10%)] Loss: -195151.187500\n",
      "Train Epoch: 63 [7040/54000 (13%)] Loss: -197240.937500\n",
      "Train Epoch: 63 [8448/54000 (16%)] Loss: -193648.828125\n",
      "Train Epoch: 63 [9856/54000 (18%)] Loss: -187741.562500\n",
      "Train Epoch: 63 [11264/54000 (21%)] Loss: -216754.781250\n",
      "Train Epoch: 63 [12672/54000 (23%)] Loss: -210273.031250\n",
      "Train Epoch: 63 [14080/54000 (26%)] Loss: -191437.500000\n",
      "Train Epoch: 63 [15488/54000 (29%)] Loss: -201068.156250\n",
      "Train Epoch: 63 [16896/54000 (31%)] Loss: -196636.171875\n",
      "Train Epoch: 63 [18304/54000 (34%)] Loss: -217917.187500\n",
      "Train Epoch: 63 [19712/54000 (37%)] Loss: -197160.718750\n",
      "Train Epoch: 63 [21120/54000 (39%)] Loss: -195809.984375\n",
      "Train Epoch: 63 [22528/54000 (42%)] Loss: -192224.015625\n",
      "Train Epoch: 63 [23936/54000 (44%)] Loss: -215387.421875\n",
      "Train Epoch: 63 [25344/54000 (47%)] Loss: -222128.750000\n",
      "Train Epoch: 63 [26752/54000 (50%)] Loss: -185334.406250\n",
      "Train Epoch: 63 [28160/54000 (52%)] Loss: -209116.546875\n",
      "Train Epoch: 63 [29568/54000 (55%)] Loss: -204382.125000\n",
      "Train Epoch: 63 [30976/54000 (57%)] Loss: -201820.968750\n",
      "Train Epoch: 63 [32384/54000 (60%)] Loss: -194451.375000\n",
      "Train Epoch: 63 [33792/54000 (63%)] Loss: -193031.250000\n",
      "Train Epoch: 63 [35200/54000 (65%)] Loss: -194215.671875\n",
      "Train Epoch: 63 [36608/54000 (68%)] Loss: -200341.531250\n",
      "Train Epoch: 63 [38016/54000 (70%)] Loss: -222048.687500\n",
      "Train Epoch: 63 [39424/54000 (73%)] Loss: -215008.328125\n",
      "Train Epoch: 63 [40832/54000 (76%)] Loss: -197250.843750\n",
      "Train Epoch: 63 [42240/54000 (78%)] Loss: -192129.750000\n",
      "Train Epoch: 63 [43648/54000 (81%)] Loss: -193777.171875\n",
      "Train Epoch: 63 [45056/54000 (83%)] Loss: -205785.453125\n",
      "Train Epoch: 63 [46464/54000 (86%)] Loss: -211283.468750\n",
      "Train Epoch: 63 [47872/54000 (89%)] Loss: -204345.281250\n",
      "Train Epoch: 63 [49280/54000 (91%)] Loss: -190612.125000\n",
      "Train Epoch: 63 [50688/54000 (94%)] Loss: -230300.156250\n",
      "Train Epoch: 63 [52096/54000 (96%)] Loss: -202203.562500\n",
      "    epoch          : 63\n",
      "    loss           : -201531.40352123205\n",
      "    val_loss       : -209953.89964033916\n",
      "Train Epoch: 64 [0/54000 (0%)] Loss: -211061.546875\n",
      "Train Epoch: 64 [1408/54000 (3%)] Loss: -230905.078125\n",
      "Train Epoch: 64 [2816/54000 (5%)] Loss: -182397.718750\n",
      "Train Epoch: 64 [4224/54000 (8%)] Loss: -200764.593750\n",
      "Train Epoch: 64 [5632/54000 (10%)] Loss: -215682.906250\n",
      "Train Epoch: 64 [7040/54000 (13%)] Loss: -179096.656250\n",
      "Train Epoch: 64 [8448/54000 (16%)] Loss: -181660.203125\n",
      "Train Epoch: 64 [9856/54000 (18%)] Loss: -179934.546875\n",
      "Train Epoch: 64 [11264/54000 (21%)] Loss: -209703.562500\n",
      "Train Epoch: 64 [12672/54000 (23%)] Loss: -210027.937500\n",
      "Train Epoch: 64 [14080/54000 (26%)] Loss: -199894.734375\n",
      "Train Epoch: 64 [15488/54000 (29%)] Loss: -176884.328125\n",
      "Train Epoch: 64 [16896/54000 (31%)] Loss: -202848.703125\n",
      "Train Epoch: 64 [18304/54000 (34%)] Loss: -201266.500000\n",
      "Train Epoch: 64 [19712/54000 (37%)] Loss: -205686.718750\n",
      "Train Epoch: 64 [21120/54000 (39%)] Loss: -216174.968750\n",
      "Train Epoch: 64 [22528/54000 (42%)] Loss: -199885.187500\n",
      "Train Epoch: 64 [23936/54000 (44%)] Loss: -190015.609375\n",
      "Train Epoch: 64 [25344/54000 (47%)] Loss: -230969.718750\n",
      "Train Epoch: 64 [26752/54000 (50%)] Loss: -202929.656250\n",
      "Train Epoch: 64 [28160/54000 (52%)] Loss: -192045.562500\n",
      "Train Epoch: 64 [29568/54000 (55%)] Loss: -190686.671875\n",
      "Train Epoch: 64 [30976/54000 (57%)] Loss: -224841.062500\n",
      "Train Epoch: 64 [32384/54000 (60%)] Loss: -191408.562500\n",
      "Train Epoch: 64 [33792/54000 (63%)] Loss: -205211.328125\n",
      "Train Epoch: 64 [35200/54000 (65%)] Loss: -231719.734375\n",
      "Train Epoch: 64 [36608/54000 (68%)] Loss: -203128.781250\n",
      "Train Epoch: 64 [38016/54000 (70%)] Loss: -188864.515625\n",
      "Train Epoch: 64 [39424/54000 (73%)] Loss: -206425.703125\n",
      "Train Epoch: 64 [40832/54000 (76%)] Loss: -189966.312500\n",
      "Train Epoch: 64 [42240/54000 (78%)] Loss: -203885.078125\n",
      "Train Epoch: 64 [43648/54000 (81%)] Loss: -208152.078125\n",
      "Train Epoch: 64 [45056/54000 (83%)] Loss: -210164.156250\n",
      "Train Epoch: 64 [46464/54000 (86%)] Loss: -210107.500000\n",
      "Train Epoch: 64 [47872/54000 (89%)] Loss: -202050.234375\n",
      "Train Epoch: 64 [49280/54000 (91%)] Loss: -204031.031250\n",
      "Train Epoch: 64 [50688/54000 (94%)] Loss: -194689.250000\n",
      "Train Epoch: 64 [52096/54000 (96%)] Loss: -212283.625000\n",
      "    epoch          : 64\n",
      "    loss           : -201948.72775867226\n",
      "    val_loss       : -208496.81323837652\n",
      "Train Epoch: 65 [0/54000 (0%)] Loss: -203632.343750\n",
      "Train Epoch: 65 [1408/54000 (3%)] Loss: -194146.500000\n",
      "Train Epoch: 65 [2816/54000 (5%)] Loss: -207200.671875\n",
      "Train Epoch: 65 [4224/54000 (8%)] Loss: -179966.875000\n",
      "Train Epoch: 65 [5632/54000 (10%)] Loss: -202935.765625\n",
      "Train Epoch: 65 [7040/54000 (13%)] Loss: -183327.437500\n",
      "Train Epoch: 65 [8448/54000 (16%)] Loss: -196771.312500\n",
      "Train Epoch: 65 [9856/54000 (18%)] Loss: -202009.812500\n",
      "Train Epoch: 65 [11264/54000 (21%)] Loss: -231680.125000\n",
      "Train Epoch: 65 [12672/54000 (23%)] Loss: -192538.593750\n",
      "Train Epoch: 65 [14080/54000 (26%)] Loss: -194629.921875\n",
      "Train Epoch: 65 [15488/54000 (29%)] Loss: -186608.765625\n",
      "Train Epoch: 65 [16896/54000 (31%)] Loss: -191646.375000\n",
      "Train Epoch: 65 [18304/54000 (34%)] Loss: -211209.218750\n",
      "Train Epoch: 65 [19712/54000 (37%)] Loss: -202606.984375\n",
      "Train Epoch: 65 [21120/54000 (39%)] Loss: -178041.750000\n",
      "Train Epoch: 65 [22528/54000 (42%)] Loss: -195126.250000\n",
      "Train Epoch: 65 [23936/54000 (44%)] Loss: -203145.953125\n",
      "Train Epoch: 65 [25344/54000 (47%)] Loss: -203695.203125\n",
      "Train Epoch: 65 [26752/54000 (50%)] Loss: -187720.031250\n",
      "Train Epoch: 65 [28160/54000 (52%)] Loss: -194359.234375\n",
      "Train Epoch: 65 [29568/54000 (55%)] Loss: -183756.562500\n",
      "Train Epoch: 65 [30976/54000 (57%)] Loss: -204923.750000\n",
      "Train Epoch: 65 [32384/54000 (60%)] Loss: -194163.078125\n",
      "Train Epoch: 65 [33792/54000 (63%)] Loss: -195430.296875\n",
      "Train Epoch: 65 [35200/54000 (65%)] Loss: -196516.250000\n",
      "Train Epoch: 65 [36608/54000 (68%)] Loss: -204693.078125\n",
      "Train Epoch: 65 [38016/54000 (70%)] Loss: -206001.609375\n",
      "Train Epoch: 65 [39424/54000 (73%)] Loss: -212700.109375\n",
      "Train Epoch: 65 [40832/54000 (76%)] Loss: -207585.953125\n",
      "Train Epoch: 65 [42240/54000 (78%)] Loss: -200131.859375\n",
      "Train Epoch: 65 [43648/54000 (81%)] Loss: -200153.343750\n",
      "Train Epoch: 65 [45056/54000 (83%)] Loss: -194415.187500\n",
      "Train Epoch: 65 [46464/54000 (86%)] Loss: -194473.562500\n",
      "Train Epoch: 65 [47872/54000 (89%)] Loss: -192303.250000\n",
      "Train Epoch: 65 [49280/54000 (91%)] Loss: -198220.921875\n",
      "Train Epoch: 65 [50688/54000 (94%)] Loss: -229353.562500\n",
      "Train Epoch: 65 [52096/54000 (96%)] Loss: -210826.968750\n",
      "    epoch          : 65\n",
      "    loss           : -201901.03438995214\n",
      "    val_loss       : -209718.4345584032\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch65.pth ...\n",
      "Train Epoch: 66 [0/54000 (0%)] Loss: -226582.437500\n",
      "Train Epoch: 66 [1408/54000 (3%)] Loss: -206628.921875\n",
      "Train Epoch: 66 [2816/54000 (5%)] Loss: -182608.156250\n",
      "Train Epoch: 66 [4224/54000 (8%)] Loss: -216373.125000\n",
      "Train Epoch: 66 [5632/54000 (10%)] Loss: -194996.359375\n",
      "Train Epoch: 66 [7040/54000 (13%)] Loss: -189259.375000\n",
      "Train Epoch: 66 [8448/54000 (16%)] Loss: -210774.031250\n",
      "Train Epoch: 66 [9856/54000 (18%)] Loss: -206587.203125\n",
      "Train Epoch: 66 [11264/54000 (21%)] Loss: -204698.437500\n",
      "Train Epoch: 66 [12672/54000 (23%)] Loss: -197586.562500\n",
      "Train Epoch: 66 [14080/54000 (26%)] Loss: -204455.406250\n",
      "Train Epoch: 66 [15488/54000 (29%)] Loss: -229929.906250\n",
      "Train Epoch: 66 [16896/54000 (31%)] Loss: -202211.171875\n",
      "Train Epoch: 66 [18304/54000 (34%)] Loss: -199723.171875\n",
      "Train Epoch: 66 [19712/54000 (37%)] Loss: -192633.500000\n",
      "Train Epoch: 66 [21120/54000 (39%)] Loss: -208006.390625\n",
      "Train Epoch: 66 [22528/54000 (42%)] Loss: -181159.625000\n",
      "Train Epoch: 66 [23936/54000 (44%)] Loss: -198812.531250\n",
      "Train Epoch: 66 [25344/54000 (47%)] Loss: -207342.828125\n",
      "Train Epoch: 66 [26752/54000 (50%)] Loss: -192802.296875\n",
      "Train Epoch: 66 [28160/54000 (52%)] Loss: -198427.718750\n",
      "Train Epoch: 66 [29568/54000 (55%)] Loss: -205287.640625\n",
      "Train Epoch: 66 [30976/54000 (57%)] Loss: -205265.062500\n",
      "Train Epoch: 66 [32384/54000 (60%)] Loss: -208198.843750\n",
      "Train Epoch: 66 [33792/54000 (63%)] Loss: -231447.468750\n",
      "Train Epoch: 66 [35200/54000 (65%)] Loss: -195952.218750\n",
      "Train Epoch: 66 [36608/54000 (68%)] Loss: -206068.296875\n",
      "Train Epoch: 66 [38016/54000 (70%)] Loss: -210394.984375\n",
      "Train Epoch: 66 [39424/54000 (73%)] Loss: -182137.265625\n",
      "Train Epoch: 66 [40832/54000 (76%)] Loss: -183494.296875\n",
      "Train Epoch: 66 [42240/54000 (78%)] Loss: -216849.312500\n",
      "Train Epoch: 66 [43648/54000 (81%)] Loss: -202764.656250\n",
      "Train Epoch: 66 [45056/54000 (83%)] Loss: -211428.359375\n",
      "Train Epoch: 66 [46464/54000 (86%)] Loss: -202891.593750\n",
      "Train Epoch: 66 [47872/54000 (89%)] Loss: -208591.265625\n",
      "Train Epoch: 66 [49280/54000 (91%)] Loss: -208192.562500\n",
      "Train Epoch: 66 [50688/54000 (94%)] Loss: -200189.375000\n",
      "Train Epoch: 66 [52096/54000 (96%)] Loss: -203983.500000\n",
      "    epoch          : 66\n",
      "    loss           : -201905.31732206937\n",
      "    val_loss       : -209358.57276581554\n",
      "Train Epoch: 67 [0/54000 (0%)] Loss: -232468.015625\n",
      "Train Epoch: 67 [1408/54000 (3%)] Loss: -190147.578125\n",
      "Train Epoch: 67 [2816/54000 (5%)] Loss: -216159.500000\n",
      "Train Epoch: 67 [4224/54000 (8%)] Loss: -209983.609375\n",
      "Train Epoch: 67 [5632/54000 (10%)] Loss: -217197.031250\n",
      "Train Epoch: 67 [7040/54000 (13%)] Loss: -229793.906250\n",
      "Train Epoch: 67 [8448/54000 (16%)] Loss: -177232.046875\n",
      "Train Epoch: 67 [9856/54000 (18%)] Loss: -202932.687500\n",
      "Train Epoch: 67 [11264/54000 (21%)] Loss: -181564.531250\n",
      "Train Epoch: 67 [12672/54000 (23%)] Loss: -205263.343750\n",
      "Train Epoch: 67 [14080/54000 (26%)] Loss: -202334.500000\n",
      "Train Epoch: 67 [15488/54000 (29%)] Loss: -225691.343750\n",
      "Train Epoch: 67 [16896/54000 (31%)] Loss: -212323.484375\n",
      "Train Epoch: 67 [18304/54000 (34%)] Loss: -197374.765625\n",
      "Train Epoch: 67 [19712/54000 (37%)] Loss: -207387.500000\n",
      "Train Epoch: 67 [21120/54000 (39%)] Loss: -216110.328125\n",
      "Train Epoch: 67 [22528/54000 (42%)] Loss: -201192.390625\n",
      "Train Epoch: 67 [23936/54000 (44%)] Loss: -193802.593750\n",
      "Train Epoch: 67 [25344/54000 (47%)] Loss: -190828.890625\n",
      "Train Epoch: 67 [26752/54000 (50%)] Loss: -226273.390625\n",
      "Train Epoch: 67 [28160/54000 (52%)] Loss: -183672.609375\n",
      "Train Epoch: 67 [29568/54000 (55%)] Loss: -205198.093750\n",
      "Train Epoch: 67 [30976/54000 (57%)] Loss: -181847.828125\n",
      "Train Epoch: 67 [32384/54000 (60%)] Loss: -203162.250000\n",
      "Train Epoch: 67 [33792/54000 (63%)] Loss: -188934.468750\n",
      "Train Epoch: 67 [35200/54000 (65%)] Loss: -203083.125000\n",
      "Train Epoch: 67 [36608/54000 (68%)] Loss: -209778.281250\n",
      "Train Epoch: 67 [38016/54000 (70%)] Loss: -209640.156250\n",
      "Train Epoch: 67 [39424/54000 (73%)] Loss: -197998.343750\n",
      "Train Epoch: 67 [40832/54000 (76%)] Loss: -188973.890625\n",
      "Train Epoch: 67 [42240/54000 (78%)] Loss: -202916.656250\n",
      "Train Epoch: 67 [43648/54000 (81%)] Loss: -194458.812500\n",
      "Train Epoch: 67 [45056/54000 (83%)] Loss: -192514.671875\n",
      "Train Epoch: 67 [46464/54000 (86%)] Loss: -210690.375000\n",
      "Train Epoch: 67 [47872/54000 (89%)] Loss: -202236.250000\n",
      "Train Epoch: 67 [49280/54000 (91%)] Loss: -190902.546875\n",
      "Train Epoch: 67 [50688/54000 (94%)] Loss: -180185.265625\n",
      "Train Epoch: 67 [52096/54000 (96%)] Loss: -194346.484375\n",
      "    epoch          : 67\n",
      "    loss           : -202265.5662006579\n",
      "    val_loss       : -211668.41275247713\n",
      "Train Epoch: 68 [0/54000 (0%)] Loss: -231906.718750\n",
      "Train Epoch: 68 [1408/54000 (3%)] Loss: -188455.187500\n",
      "Train Epoch: 68 [2816/54000 (5%)] Loss: -195366.796875\n",
      "Train Epoch: 68 [4224/54000 (8%)] Loss: -206055.125000\n",
      "Train Epoch: 68 [5632/54000 (10%)] Loss: -216788.843750\n",
      "Train Epoch: 68 [7040/54000 (13%)] Loss: -212183.468750\n",
      "Train Epoch: 68 [8448/54000 (16%)] Loss: -202587.968750\n",
      "Train Epoch: 68 [9856/54000 (18%)] Loss: -198926.046875\n",
      "Train Epoch: 68 [11264/54000 (21%)] Loss: -190308.296875\n",
      "Train Epoch: 68 [12672/54000 (23%)] Loss: -191696.437500\n",
      "Train Epoch: 68 [14080/54000 (26%)] Loss: -203776.656250\n",
      "Train Epoch: 68 [15488/54000 (29%)] Loss: -182145.500000\n",
      "Train Epoch: 68 [16896/54000 (31%)] Loss: -229572.703125\n",
      "Train Epoch: 68 [18304/54000 (34%)] Loss: -176769.781250\n",
      "Train Epoch: 68 [19712/54000 (37%)] Loss: -205385.437500\n",
      "Train Epoch: 68 [21120/54000 (39%)] Loss: -192076.031250\n",
      "Train Epoch: 68 [22528/54000 (42%)] Loss: -216732.328125\n",
      "Train Epoch: 68 [23936/54000 (44%)] Loss: -205111.890625\n",
      "Train Epoch: 68 [25344/54000 (47%)] Loss: -198647.250000\n",
      "Train Epoch: 68 [26752/54000 (50%)] Loss: -218770.468750\n",
      "Train Epoch: 68 [28160/54000 (52%)] Loss: -213490.718750\n",
      "Train Epoch: 68 [29568/54000 (55%)] Loss: -223443.156250\n",
      "Train Epoch: 68 [30976/54000 (57%)] Loss: -217242.093750\n",
      "Train Epoch: 68 [32384/54000 (60%)] Loss: -205549.203125\n",
      "Train Epoch: 68 [33792/54000 (63%)] Loss: -223737.250000\n",
      "Train Epoch: 68 [35200/54000 (65%)] Loss: -184426.625000\n",
      "Train Epoch: 68 [36608/54000 (68%)] Loss: -204079.687500\n",
      "Train Epoch: 68 [38016/54000 (70%)] Loss: -231361.078125\n",
      "Train Epoch: 68 [39424/54000 (73%)] Loss: -202249.312500\n",
      "Train Epoch: 68 [40832/54000 (76%)] Loss: -184169.031250\n",
      "Train Epoch: 68 [42240/54000 (78%)] Loss: -203925.250000\n",
      "Train Epoch: 68 [43648/54000 (81%)] Loss: -205010.687500\n",
      "Train Epoch: 68 [45056/54000 (83%)] Loss: -216201.906250\n",
      "Train Epoch: 68 [46464/54000 (86%)] Loss: -206005.609375\n",
      "Train Epoch: 68 [47872/54000 (89%)] Loss: -202644.812500\n",
      "Train Epoch: 68 [49280/54000 (91%)] Loss: -196163.421875\n",
      "Train Epoch: 68 [50688/54000 (94%)] Loss: -181205.484375\n",
      "Train Epoch: 68 [52096/54000 (96%)] Loss: -231943.343750\n",
      "    epoch          : 68\n",
      "    loss           : -202790.33328349283\n",
      "    val_loss       : -210190.40085508765\n",
      "Train Epoch: 69 [0/54000 (0%)] Loss: -183585.187500\n",
      "Train Epoch: 69 [1408/54000 (3%)] Loss: -179066.593750\n",
      "Train Epoch: 69 [2816/54000 (5%)] Loss: -191206.484375\n",
      "Train Epoch: 69 [4224/54000 (8%)] Loss: -199433.312500\n",
      "Train Epoch: 69 [5632/54000 (10%)] Loss: -196317.687500\n",
      "Train Epoch: 69 [7040/54000 (13%)] Loss: -205085.687500\n",
      "Train Epoch: 69 [8448/54000 (16%)] Loss: -199067.859375\n",
      "Train Epoch: 69 [9856/54000 (18%)] Loss: -210399.671875\n",
      "Train Epoch: 69 [11264/54000 (21%)] Loss: -208229.515625\n",
      "Train Epoch: 69 [12672/54000 (23%)] Loss: -195094.968750\n",
      "Train Epoch: 69 [14080/54000 (26%)] Loss: -204541.203125\n",
      "Train Epoch: 69 [15488/54000 (29%)] Loss: -208572.828125\n",
      "Train Epoch: 69 [16896/54000 (31%)] Loss: -210269.140625\n",
      "Train Epoch: 69 [18304/54000 (34%)] Loss: -183558.468750\n",
      "Train Epoch: 69 [19712/54000 (37%)] Loss: -210903.250000\n",
      "Train Epoch: 69 [21120/54000 (39%)] Loss: -230746.390625\n",
      "Train Epoch: 69 [22528/54000 (42%)] Loss: -194791.484375\n",
      "Train Epoch: 69 [23936/54000 (44%)] Loss: -205269.031250\n",
      "Train Epoch: 69 [25344/54000 (47%)] Loss: -196918.046875\n",
      "Train Epoch: 69 [26752/54000 (50%)] Loss: -230016.671875\n",
      "Train Epoch: 69 [28160/54000 (52%)] Loss: -196856.453125\n",
      "Train Epoch: 69 [29568/54000 (55%)] Loss: -201516.265625\n",
      "Train Epoch: 69 [30976/54000 (57%)] Loss: -208680.875000\n",
      "Train Epoch: 69 [32384/54000 (60%)] Loss: -207045.984375\n",
      "Train Epoch: 69 [33792/54000 (63%)] Loss: -194835.796875\n",
      "Train Epoch: 69 [35200/54000 (65%)] Loss: -217819.718750\n",
      "Train Epoch: 69 [36608/54000 (68%)] Loss: -197807.625000\n",
      "Train Epoch: 69 [38016/54000 (70%)] Loss: -183049.468750\n",
      "Train Epoch: 69 [39424/54000 (73%)] Loss: -193873.000000\n",
      "Train Epoch: 69 [40832/54000 (76%)] Loss: -194001.171875\n",
      "Train Epoch: 69 [42240/54000 (78%)] Loss: -180386.531250\n",
      "Train Epoch: 69 [43648/54000 (81%)] Loss: -213294.015625\n",
      "Train Epoch: 69 [45056/54000 (83%)] Loss: -203509.937500\n",
      "Train Epoch: 69 [46464/54000 (86%)] Loss: -221752.937500\n",
      "Train Epoch: 69 [47872/54000 (89%)] Loss: -201979.015625\n",
      "Train Epoch: 69 [49280/54000 (91%)] Loss: -205080.187500\n",
      "Train Epoch: 69 [50688/54000 (94%)] Loss: -228335.968750\n",
      "Train Epoch: 69 [52096/54000 (96%)] Loss: -192314.187500\n",
      "    epoch          : 69\n",
      "    loss           : -203102.94235944975\n",
      "    val_loss       : -209788.7333150724\n",
      "Train Epoch: 70 [0/54000 (0%)] Loss: -230816.437500\n",
      "Train Epoch: 70 [1408/54000 (3%)] Loss: -231248.593750\n",
      "Train Epoch: 70 [2816/54000 (5%)] Loss: -202882.437500\n",
      "Train Epoch: 70 [4224/54000 (8%)] Loss: -200024.437500\n",
      "Train Epoch: 70 [5632/54000 (10%)] Loss: -193097.109375\n",
      "Train Epoch: 70 [7040/54000 (13%)] Loss: -210302.437500\n",
      "Train Epoch: 70 [8448/54000 (16%)] Loss: -225997.671875\n",
      "Train Epoch: 70 [9856/54000 (18%)] Loss: -196060.500000\n",
      "Train Epoch: 70 [11264/54000 (21%)] Loss: -179612.781250\n",
      "Train Epoch: 70 [12672/54000 (23%)] Loss: -211201.656250\n",
      "Train Epoch: 70 [14080/54000 (26%)] Loss: -207440.781250\n",
      "Train Epoch: 70 [15488/54000 (29%)] Loss: -191365.953125\n",
      "Train Epoch: 70 [16896/54000 (31%)] Loss: -208263.468750\n",
      "Train Epoch: 70 [18304/54000 (34%)] Loss: -229738.593750\n",
      "Train Epoch: 70 [19712/54000 (37%)] Loss: -218513.968750\n",
      "Train Epoch: 70 [21120/54000 (39%)] Loss: -194601.203125\n",
      "Train Epoch: 70 [22528/54000 (42%)] Loss: -208735.296875\n",
      "Train Epoch: 70 [23936/54000 (44%)] Loss: -199834.515625\n",
      "Train Epoch: 70 [25344/54000 (47%)] Loss: -194861.562500\n",
      "Train Epoch: 70 [26752/54000 (50%)] Loss: -203551.875000\n",
      "Train Epoch: 70 [28160/54000 (52%)] Loss: -194982.390625\n",
      "Train Epoch: 70 [29568/54000 (55%)] Loss: -205884.937500\n",
      "Train Epoch: 70 [30976/54000 (57%)] Loss: -232556.750000\n",
      "Train Epoch: 70 [32384/54000 (60%)] Loss: -195298.203125\n",
      "Train Epoch: 70 [33792/54000 (63%)] Loss: -199564.593750\n",
      "Train Epoch: 70 [35200/54000 (65%)] Loss: -210565.937500\n",
      "Train Epoch: 70 [36608/54000 (68%)] Loss: -228429.171875\n",
      "Train Epoch: 70 [38016/54000 (70%)] Loss: -215945.734375\n",
      "Train Epoch: 70 [39424/54000 (73%)] Loss: -194422.750000\n",
      "Train Epoch: 70 [40832/54000 (76%)] Loss: -192303.171875\n",
      "Train Epoch: 70 [42240/54000 (78%)] Loss: -180836.828125\n",
      "Train Epoch: 70 [43648/54000 (81%)] Loss: -206637.578125\n",
      "Train Epoch: 70 [45056/54000 (83%)] Loss: -199670.281250\n",
      "Train Epoch: 70 [46464/54000 (86%)] Loss: -210709.562500\n",
      "Train Epoch: 70 [47872/54000 (89%)] Loss: -190909.390625\n",
      "Train Epoch: 70 [49280/54000 (91%)] Loss: -231820.531250\n",
      "Train Epoch: 70 [50688/54000 (94%)] Loss: -185527.656250\n",
      "Train Epoch: 70 [52096/54000 (96%)] Loss: -193381.890625\n",
      "    epoch          : 70\n",
      "    loss           : -203202.26110197368\n",
      "    val_loss       : -210041.48570884147\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch70.pth ...\n",
      "Train Epoch: 71 [0/54000 (0%)] Loss: -232317.125000\n",
      "Train Epoch: 71 [1408/54000 (3%)] Loss: -200509.156250\n",
      "Train Epoch: 71 [2816/54000 (5%)] Loss: -202619.250000\n",
      "Train Epoch: 71 [4224/54000 (8%)] Loss: -193627.000000\n",
      "Train Epoch: 71 [5632/54000 (10%)] Loss: -205001.437500\n",
      "Train Epoch: 71 [7040/54000 (13%)] Loss: -202851.562500\n",
      "Train Epoch: 71 [8448/54000 (16%)] Loss: -196144.171875\n",
      "Train Epoch: 71 [9856/54000 (18%)] Loss: -232580.531250\n",
      "Train Epoch: 71 [11264/54000 (21%)] Loss: -194144.453125\n",
      "Train Epoch: 71 [12672/54000 (23%)] Loss: -196611.156250\n",
      "Train Epoch: 71 [14080/54000 (26%)] Loss: -196946.484375\n",
      "Train Epoch: 71 [15488/54000 (29%)] Loss: -193244.609375\n",
      "Train Epoch: 71 [16896/54000 (31%)] Loss: -193408.656250\n",
      "Train Epoch: 71 [18304/54000 (34%)] Loss: -195911.828125\n",
      "Train Epoch: 71 [19712/54000 (37%)] Loss: -178076.062500\n",
      "Train Epoch: 71 [21120/54000 (39%)] Loss: -188170.859375\n",
      "Train Epoch: 71 [22528/54000 (42%)] Loss: -223304.937500\n",
      "Train Epoch: 71 [23936/54000 (44%)] Loss: -205540.984375\n",
      "Train Epoch: 71 [25344/54000 (47%)] Loss: -211308.578125\n",
      "Train Epoch: 71 [26752/54000 (50%)] Loss: -193802.375000\n",
      "Train Epoch: 71 [28160/54000 (52%)] Loss: -185989.062500\n",
      "Train Epoch: 71 [29568/54000 (55%)] Loss: -221093.906250\n",
      "Train Epoch: 71 [30976/54000 (57%)] Loss: -226628.093750\n",
      "Train Epoch: 71 [32384/54000 (60%)] Loss: -204268.500000\n",
      "Train Epoch: 71 [33792/54000 (63%)] Loss: -187288.156250\n",
      "Train Epoch: 71 [35200/54000 (65%)] Loss: -200632.343750\n",
      "Train Epoch: 71 [36608/54000 (68%)] Loss: -196330.437500\n",
      "Train Epoch: 71 [38016/54000 (70%)] Loss: -194597.281250\n",
      "Train Epoch: 71 [39424/54000 (73%)] Loss: -216690.234375\n",
      "Train Epoch: 71 [40832/54000 (76%)] Loss: -201091.375000\n",
      "Train Epoch: 71 [42240/54000 (78%)] Loss: -195353.125000\n",
      "Train Epoch: 71 [43648/54000 (81%)] Loss: -202353.968750\n",
      "Train Epoch: 71 [45056/54000 (83%)] Loss: -200930.156250\n",
      "Train Epoch: 71 [46464/54000 (86%)] Loss: -210391.578125\n",
      "Train Epoch: 71 [47872/54000 (89%)] Loss: -199875.046875\n",
      "Train Epoch: 71 [49280/54000 (91%)] Loss: -192989.015625\n",
      "Train Epoch: 71 [50688/54000 (94%)] Loss: -211841.328125\n",
      "Train Epoch: 71 [52096/54000 (96%)] Loss: -212704.328125\n",
      "    epoch          : 71\n",
      "    loss           : -203041.4934958134\n",
      "    val_loss       : -211285.04365948934\n",
      "Train Epoch: 72 [0/54000 (0%)] Loss: -223281.968750\n",
      "Train Epoch: 72 [1408/54000 (3%)] Loss: -197653.687500\n",
      "Train Epoch: 72 [2816/54000 (5%)] Loss: -206284.640625\n",
      "Train Epoch: 72 [4224/54000 (8%)] Loss: -179452.296875\n",
      "Train Epoch: 72 [5632/54000 (10%)] Loss: -203240.203125\n",
      "Train Epoch: 72 [7040/54000 (13%)] Loss: -196802.062500\n",
      "Train Epoch: 72 [8448/54000 (16%)] Loss: -195299.140625\n",
      "Train Epoch: 72 [9856/54000 (18%)] Loss: -209086.640625\n",
      "Train Epoch: 72 [11264/54000 (21%)] Loss: -203372.343750\n",
      "Train Epoch: 72 [12672/54000 (23%)] Loss: -200822.234375\n",
      "Train Epoch: 72 [14080/54000 (26%)] Loss: -212334.984375\n",
      "Train Epoch: 72 [15488/54000 (29%)] Loss: -209548.468750\n",
      "Train Epoch: 72 [16896/54000 (31%)] Loss: -205096.890625\n",
      "Train Epoch: 72 [18304/54000 (34%)] Loss: -195490.421875\n",
      "Train Epoch: 72 [19712/54000 (37%)] Loss: -185014.515625\n",
      "Train Epoch: 72 [21120/54000 (39%)] Loss: -218537.406250\n",
      "Train Epoch: 72 [22528/54000 (42%)] Loss: -204835.156250\n",
      "Train Epoch: 72 [23936/54000 (44%)] Loss: -179712.875000\n",
      "Train Epoch: 72 [25344/54000 (47%)] Loss: -202711.187500\n",
      "Train Epoch: 72 [26752/54000 (50%)] Loss: -195621.625000\n",
      "Train Epoch: 72 [28160/54000 (52%)] Loss: -196969.062500\n",
      "Train Epoch: 72 [29568/54000 (55%)] Loss: -205407.843750\n",
      "Train Epoch: 72 [30976/54000 (57%)] Loss: -212129.578125\n",
      "Train Epoch: 72 [32384/54000 (60%)] Loss: -213718.312500\n",
      "Train Epoch: 72 [33792/54000 (63%)] Loss: -226189.468750\n",
      "Train Epoch: 72 [35200/54000 (65%)] Loss: -183808.218750\n",
      "Train Epoch: 72 [36608/54000 (68%)] Loss: -206617.421875\n",
      "Train Epoch: 72 [38016/54000 (70%)] Loss: -218380.937500\n",
      "Train Epoch: 72 [39424/54000 (73%)] Loss: -206288.078125\n",
      "Train Epoch: 72 [40832/54000 (76%)] Loss: -225564.750000\n",
      "Train Epoch: 72 [42240/54000 (78%)] Loss: -200302.031250\n",
      "Train Epoch: 72 [43648/54000 (81%)] Loss: -205027.000000\n",
      "Train Epoch: 72 [45056/54000 (83%)] Loss: -204574.234375\n",
      "Train Epoch: 72 [46464/54000 (86%)] Loss: -200717.625000\n",
      "Train Epoch: 72 [47872/54000 (89%)] Loss: -182300.531250\n",
      "Train Epoch: 72 [49280/54000 (91%)] Loss: -205088.812500\n",
      "Train Epoch: 72 [50688/54000 (94%)] Loss: -232741.531250\n",
      "Train Epoch: 72 [52096/54000 (96%)] Loss: -198488.406250\n",
      "    epoch          : 72\n",
      "    loss           : -203107.72185257176\n",
      "    val_loss       : -210687.90878668064\n",
      "Train Epoch: 73 [0/54000 (0%)] Loss: -214982.265625\n",
      "Train Epoch: 73 [1408/54000 (3%)] Loss: -205769.765625\n",
      "Train Epoch: 73 [2816/54000 (5%)] Loss: -218422.250000\n",
      "Train Epoch: 73 [4224/54000 (8%)] Loss: -207863.156250\n",
      "Train Epoch: 73 [5632/54000 (10%)] Loss: -206453.375000\n",
      "Train Epoch: 73 [7040/54000 (13%)] Loss: -207051.000000\n",
      "Train Epoch: 73 [8448/54000 (16%)] Loss: -223966.500000\n",
      "Train Epoch: 73 [9856/54000 (18%)] Loss: -192272.203125\n",
      "Train Epoch: 73 [11264/54000 (21%)] Loss: -202415.421875\n",
      "Train Epoch: 73 [12672/54000 (23%)] Loss: -194228.609375\n",
      "Train Epoch: 73 [14080/54000 (26%)] Loss: -198109.500000\n",
      "Train Epoch: 73 [15488/54000 (29%)] Loss: -229100.953125\n",
      "Train Epoch: 73 [16896/54000 (31%)] Loss: -198711.000000\n",
      "Train Epoch: 73 [18304/54000 (34%)] Loss: -179365.546875\n",
      "Train Epoch: 73 [19712/54000 (37%)] Loss: -198173.218750\n",
      "Train Epoch: 73 [21120/54000 (39%)] Loss: -207909.906250\n",
      "Train Epoch: 73 [22528/54000 (42%)] Loss: -198766.796875\n",
      "Train Epoch: 73 [23936/54000 (44%)] Loss: -190409.406250\n",
      "Train Epoch: 73 [25344/54000 (47%)] Loss: -204364.546875\n",
      "Train Epoch: 73 [26752/54000 (50%)] Loss: -216917.437500\n",
      "Train Epoch: 73 [28160/54000 (52%)] Loss: -214981.828125\n",
      "Train Epoch: 73 [29568/54000 (55%)] Loss: -195381.109375\n",
      "Train Epoch: 73 [30976/54000 (57%)] Loss: -217448.937500\n",
      "Train Epoch: 73 [32384/54000 (60%)] Loss: -203882.968750\n",
      "Train Epoch: 73 [33792/54000 (63%)] Loss: -199051.671875\n",
      "Train Epoch: 73 [35200/54000 (65%)] Loss: -192067.156250\n",
      "Train Epoch: 73 [36608/54000 (68%)] Loss: -229962.375000\n",
      "Train Epoch: 73 [38016/54000 (70%)] Loss: -197246.343750\n",
      "Train Epoch: 73 [39424/54000 (73%)] Loss: -185041.343750\n",
      "Train Epoch: 73 [40832/54000 (76%)] Loss: -194089.437500\n",
      "Train Epoch: 73 [42240/54000 (78%)] Loss: -213750.687500\n",
      "Train Epoch: 73 [43648/54000 (81%)] Loss: -232189.250000\n",
      "Train Epoch: 73 [45056/54000 (83%)] Loss: -194116.484375\n",
      "Train Epoch: 73 [46464/54000 (86%)] Loss: -193328.093750\n",
      "Train Epoch: 73 [47872/54000 (89%)] Loss: -205698.328125\n",
      "Train Epoch: 73 [49280/54000 (91%)] Loss: -233545.437500\n",
      "Train Epoch: 73 [50688/54000 (94%)] Loss: -229809.312500\n",
      "Train Epoch: 73 [52096/54000 (96%)] Loss: -213916.875000\n",
      "    epoch          : 73\n",
      "    loss           : -203253.84554425837\n",
      "    val_loss       : -213264.46707078887\n",
      "Train Epoch: 74 [0/54000 (0%)] Loss: -230126.062500\n",
      "Train Epoch: 74 [1408/54000 (3%)] Loss: -213013.281250\n",
      "Train Epoch: 74 [2816/54000 (5%)] Loss: -210905.687500\n",
      "Train Epoch: 74 [4224/54000 (8%)] Loss: -179793.078125\n",
      "Train Epoch: 74 [5632/54000 (10%)] Loss: -206373.656250\n",
      "Train Epoch: 74 [7040/54000 (13%)] Loss: -193283.781250\n",
      "Train Epoch: 74 [8448/54000 (16%)] Loss: -198876.875000\n",
      "Train Epoch: 74 [9856/54000 (18%)] Loss: -199210.859375\n",
      "Train Epoch: 74 [11264/54000 (21%)] Loss: -211134.078125\n",
      "Train Epoch: 74 [12672/54000 (23%)] Loss: -206652.531250\n",
      "Train Epoch: 74 [14080/54000 (26%)] Loss: -196739.546875\n",
      "Train Epoch: 74 [15488/54000 (29%)] Loss: -193331.250000\n",
      "Train Epoch: 74 [16896/54000 (31%)] Loss: -191623.046875\n",
      "Train Epoch: 74 [18304/54000 (34%)] Loss: -175043.812500\n",
      "Train Epoch: 74 [19712/54000 (37%)] Loss: -190394.156250\n",
      "Train Epoch: 74 [21120/54000 (39%)] Loss: -206986.687500\n",
      "Train Epoch: 74 [22528/54000 (42%)] Loss: -196759.703125\n",
      "Train Epoch: 74 [23936/54000 (44%)] Loss: -210650.125000\n",
      "Train Epoch: 74 [25344/54000 (47%)] Loss: -218109.953125\n",
      "Train Epoch: 74 [26752/54000 (50%)] Loss: -201909.312500\n",
      "Train Epoch: 74 [28160/54000 (52%)] Loss: -212496.500000\n",
      "Train Epoch: 74 [29568/54000 (55%)] Loss: -204047.765625\n",
      "Train Epoch: 74 [30976/54000 (57%)] Loss: -224603.984375\n",
      "Train Epoch: 74 [32384/54000 (60%)] Loss: -194596.296875\n",
      "Train Epoch: 74 [33792/54000 (63%)] Loss: -207808.953125\n",
      "Train Epoch: 74 [35200/54000 (65%)] Loss: -197052.171875\n",
      "Train Epoch: 74 [36608/54000 (68%)] Loss: -227015.625000\n",
      "Train Epoch: 74 [38016/54000 (70%)] Loss: -210629.031250\n",
      "Train Epoch: 74 [39424/54000 (73%)] Loss: -192045.187500\n",
      "Train Epoch: 74 [40832/54000 (76%)] Loss: -185210.843750\n",
      "Train Epoch: 74 [42240/54000 (78%)] Loss: -201049.750000\n",
      "Train Epoch: 74 [43648/54000 (81%)] Loss: -197238.625000\n",
      "Train Epoch: 74 [45056/54000 (83%)] Loss: -194198.093750\n",
      "Train Epoch: 74 [46464/54000 (86%)] Loss: -206320.609375\n",
      "Train Epoch: 74 [47872/54000 (89%)] Loss: -202669.421875\n",
      "Train Epoch: 74 [49280/54000 (91%)] Loss: -207438.921875\n",
      "Train Epoch: 74 [50688/54000 (94%)] Loss: -198788.734375\n",
      "Train Epoch: 74 [52096/54000 (96%)] Loss: -196030.687500\n",
      "    epoch          : 74\n",
      "    loss           : -202946.76472787082\n",
      "    val_loss       : -213119.641780202\n",
      "Train Epoch: 75 [0/54000 (0%)] Loss: -231900.640625\n",
      "Train Epoch: 75 [1408/54000 (3%)] Loss: -210824.468750\n",
      "Train Epoch: 75 [2816/54000 (5%)] Loss: -219232.703125\n",
      "Train Epoch: 75 [4224/54000 (8%)] Loss: -206106.984375\n",
      "Train Epoch: 75 [5632/54000 (10%)] Loss: -200966.937500\n",
      "Train Epoch: 75 [7040/54000 (13%)] Loss: -196477.750000\n",
      "Train Epoch: 75 [8448/54000 (16%)] Loss: -214676.906250\n",
      "Train Epoch: 75 [9856/54000 (18%)] Loss: -206342.640625\n",
      "Train Epoch: 75 [11264/54000 (21%)] Loss: -195125.812500\n",
      "Train Epoch: 75 [12672/54000 (23%)] Loss: -204843.515625\n",
      "Train Epoch: 75 [14080/54000 (26%)] Loss: -218443.078125\n",
      "Train Epoch: 75 [15488/54000 (29%)] Loss: -224313.906250\n",
      "Train Epoch: 75 [16896/54000 (31%)] Loss: -212832.625000\n",
      "Train Epoch: 75 [18304/54000 (34%)] Loss: -212088.218750\n",
      "Train Epoch: 75 [19712/54000 (37%)] Loss: -195225.437500\n",
      "Train Epoch: 75 [21120/54000 (39%)] Loss: -208299.515625\n",
      "Train Epoch: 75 [22528/54000 (42%)] Loss: -193096.593750\n",
      "Train Epoch: 75 [23936/54000 (44%)] Loss: -198147.046875\n",
      "Train Epoch: 75 [25344/54000 (47%)] Loss: -203783.640625\n",
      "Train Epoch: 75 [26752/54000 (50%)] Loss: -208707.671875\n",
      "Train Epoch: 75 [28160/54000 (52%)] Loss: -187302.062500\n",
      "Train Epoch: 75 [29568/54000 (55%)] Loss: -194535.281250\n",
      "Train Epoch: 75 [30976/54000 (57%)] Loss: -192862.562500\n",
      "Train Epoch: 75 [32384/54000 (60%)] Loss: -202880.125000\n",
      "Train Epoch: 75 [33792/54000 (63%)] Loss: -223873.468750\n",
      "Train Epoch: 75 [35200/54000 (65%)] Loss: -181451.843750\n",
      "Train Epoch: 75 [36608/54000 (68%)] Loss: -233447.078125\n",
      "Train Epoch: 75 [38016/54000 (70%)] Loss: -219309.062500\n",
      "Train Epoch: 75 [39424/54000 (73%)] Loss: -189483.625000\n",
      "Train Epoch: 75 [40832/54000 (76%)] Loss: -202469.531250\n",
      "Train Epoch: 75 [42240/54000 (78%)] Loss: -184488.343750\n",
      "Train Epoch: 75 [43648/54000 (81%)] Loss: -224386.593750\n",
      "Train Epoch: 75 [45056/54000 (83%)] Loss: -196578.750000\n",
      "Train Epoch: 75 [46464/54000 (86%)] Loss: -204230.546875\n",
      "Train Epoch: 75 [47872/54000 (89%)] Loss: -203623.703125\n",
      "Train Epoch: 75 [49280/54000 (91%)] Loss: -208602.734375\n",
      "Train Epoch: 75 [50688/54000 (94%)] Loss: -212279.671875\n",
      "Train Epoch: 75 [52096/54000 (96%)] Loss: -213626.093750\n",
      "    epoch          : 75\n",
      "    loss           : -203539.0069527512\n",
      "    val_loss       : -214594.53169064404\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch75.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 76 [0/54000 (0%)] Loss: -182403.687500\n",
      "Train Epoch: 76 [1408/54000 (3%)] Loss: -230077.156250\n",
      "Train Epoch: 76 [2816/54000 (5%)] Loss: -197069.375000\n",
      "Train Epoch: 76 [4224/54000 (8%)] Loss: -213251.531250\n",
      "Train Epoch: 76 [5632/54000 (10%)] Loss: -194664.625000\n",
      "Train Epoch: 76 [7040/54000 (13%)] Loss: -198258.562500\n",
      "Train Epoch: 76 [8448/54000 (16%)] Loss: -176885.750000\n",
      "Train Epoch: 76 [9856/54000 (18%)] Loss: -207553.296875\n",
      "Train Epoch: 76 [11264/54000 (21%)] Loss: -187160.390625\n",
      "Train Epoch: 76 [12672/54000 (23%)] Loss: -207327.375000\n",
      "Train Epoch: 76 [14080/54000 (26%)] Loss: -197072.093750\n",
      "Train Epoch: 76 [15488/54000 (29%)] Loss: -211852.765625\n",
      "Train Epoch: 76 [16896/54000 (31%)] Loss: -195368.812500\n",
      "Train Epoch: 76 [18304/54000 (34%)] Loss: -192657.750000\n",
      "Train Epoch: 76 [19712/54000 (37%)] Loss: -215273.109375\n",
      "Train Epoch: 76 [21120/54000 (39%)] Loss: -212227.906250\n",
      "Train Epoch: 76 [22528/54000 (42%)] Loss: -198294.687500\n",
      "Train Epoch: 76 [23936/54000 (44%)] Loss: -195165.968750\n",
      "Train Epoch: 76 [25344/54000 (47%)] Loss: -198353.906250\n",
      "Train Epoch: 76 [26752/54000 (50%)] Loss: -200242.468750\n",
      "Train Epoch: 76 [28160/54000 (52%)] Loss: -232549.062500\n",
      "Train Epoch: 76 [29568/54000 (55%)] Loss: -194946.796875\n",
      "Train Epoch: 76 [30976/54000 (57%)] Loss: -193991.703125\n",
      "Train Epoch: 76 [32384/54000 (60%)] Loss: -196418.531250\n",
      "Train Epoch: 76 [33792/54000 (63%)] Loss: -196927.531250\n",
      "Train Epoch: 76 [35200/54000 (65%)] Loss: -193574.593750\n",
      "Train Epoch: 76 [36608/54000 (68%)] Loss: -204223.250000\n",
      "Train Epoch: 76 [38016/54000 (70%)] Loss: -197448.250000\n",
      "Train Epoch: 76 [39424/54000 (73%)] Loss: -199594.875000\n",
      "Train Epoch: 76 [40832/54000 (76%)] Loss: -198442.265625\n",
      "Train Epoch: 76 [42240/54000 (78%)] Loss: -199342.750000\n",
      "Train Epoch: 76 [43648/54000 (81%)] Loss: -208252.046875\n",
      "Train Epoch: 76 [45056/54000 (83%)] Loss: -187115.812500\n",
      "Train Epoch: 76 [46464/54000 (86%)] Loss: -229052.218750\n",
      "Train Epoch: 76 [47872/54000 (89%)] Loss: -189524.531250\n",
      "Train Epoch: 76 [49280/54000 (91%)] Loss: -190441.937500\n",
      "Train Epoch: 76 [50688/54000 (94%)] Loss: -233497.156250\n",
      "Train Epoch: 76 [52096/54000 (96%)] Loss: -206881.250000\n",
      "    epoch          : 76\n",
      "    loss           : -203887.50048594497\n",
      "    val_loss       : -210322.75734803735\n",
      "Train Epoch: 77 [0/54000 (0%)] Loss: -193156.625000\n",
      "Train Epoch: 77 [1408/54000 (3%)] Loss: -206144.484375\n",
      "Train Epoch: 77 [2816/54000 (5%)] Loss: -194967.031250\n",
      "Train Epoch: 77 [4224/54000 (8%)] Loss: -210179.406250\n",
      "Train Epoch: 77 [5632/54000 (10%)] Loss: -212640.156250\n",
      "Train Epoch: 77 [7040/54000 (13%)] Loss: -220144.812500\n",
      "Train Epoch: 77 [8448/54000 (16%)] Loss: -209949.125000\n",
      "Train Epoch: 77 [9856/54000 (18%)] Loss: -205824.703125\n",
      "Train Epoch: 77 [11264/54000 (21%)] Loss: -206274.343750\n",
      "Train Epoch: 77 [12672/54000 (23%)] Loss: -192490.468750\n",
      "Train Epoch: 77 [14080/54000 (26%)] Loss: -201040.156250\n",
      "Train Epoch: 77 [15488/54000 (29%)] Loss: -227455.015625\n",
      "Train Epoch: 77 [16896/54000 (31%)] Loss: -196838.359375\n",
      "Train Epoch: 77 [18304/54000 (34%)] Loss: -206432.156250\n",
      "Train Epoch: 77 [19712/54000 (37%)] Loss: -218612.437500\n",
      "Train Epoch: 77 [21120/54000 (39%)] Loss: -233508.265625\n",
      "Train Epoch: 77 [22528/54000 (42%)] Loss: -185142.437500\n",
      "Train Epoch: 77 [23936/54000 (44%)] Loss: -209292.812500\n",
      "Train Epoch: 77 [25344/54000 (47%)] Loss: -197691.796875\n",
      "Train Epoch: 77 [26752/54000 (50%)] Loss: -187760.843750\n",
      "Train Epoch: 77 [28160/54000 (52%)] Loss: -183204.562500\n",
      "Train Epoch: 77 [29568/54000 (55%)] Loss: -233923.640625\n",
      "Train Epoch: 77 [30976/54000 (57%)] Loss: -204697.406250\n",
      "Train Epoch: 77 [32384/54000 (60%)] Loss: -194019.625000\n",
      "Train Epoch: 77 [33792/54000 (63%)] Loss: -214542.500000\n",
      "Train Epoch: 77 [35200/54000 (65%)] Loss: -233499.218750\n",
      "Train Epoch: 77 [36608/54000 (68%)] Loss: -183542.906250\n",
      "Train Epoch: 77 [38016/54000 (70%)] Loss: -175222.906250\n",
      "Train Epoch: 77 [39424/54000 (73%)] Loss: -183305.437500\n",
      "Train Epoch: 77 [40832/54000 (76%)] Loss: -194763.843750\n",
      "Train Epoch: 77 [42240/54000 (78%)] Loss: -220168.625000\n",
      "Train Epoch: 77 [43648/54000 (81%)] Loss: -230150.890625\n",
      "Train Epoch: 77 [45056/54000 (83%)] Loss: -195693.687500\n",
      "Train Epoch: 77 [46464/54000 (86%)] Loss: -202881.546875\n",
      "Train Epoch: 77 [47872/54000 (89%)] Loss: -203797.796875\n",
      "Train Epoch: 77 [49280/54000 (91%)] Loss: -195414.375000\n",
      "Train Epoch: 77 [50688/54000 (94%)] Loss: -233240.437500\n",
      "Train Epoch: 77 [52096/54000 (96%)] Loss: -214338.781250\n",
      "    epoch          : 77\n",
      "    loss           : -204026.0050089713\n",
      "    val_loss       : -213042.1034798971\n",
      "Train Epoch: 78 [0/54000 (0%)] Loss: -202737.359375\n",
      "Train Epoch: 78 [1408/54000 (3%)] Loss: -204911.109375\n",
      "Train Epoch: 78 [2816/54000 (5%)] Loss: -197960.093750\n",
      "Train Epoch: 78 [4224/54000 (8%)] Loss: -200020.250000\n",
      "Train Epoch: 78 [5632/54000 (10%)] Loss: -189954.375000\n",
      "Train Epoch: 78 [7040/54000 (13%)] Loss: -201732.734375\n",
      "Train Epoch: 78 [8448/54000 (16%)] Loss: -214315.343750\n",
      "Train Epoch: 78 [9856/54000 (18%)] Loss: -232004.875000\n",
      "Train Epoch: 78 [11264/54000 (21%)] Loss: -209370.093750\n",
      "Train Epoch: 78 [12672/54000 (23%)] Loss: -205839.312500\n",
      "Train Epoch: 78 [14080/54000 (26%)] Loss: -214168.640625\n",
      "Train Epoch: 78 [15488/54000 (29%)] Loss: -208800.046875\n",
      "Train Epoch: 78 [16896/54000 (31%)] Loss: -232577.609375\n",
      "Train Epoch: 78 [18304/54000 (34%)] Loss: -198539.578125\n",
      "Train Epoch: 78 [19712/54000 (37%)] Loss: -184233.375000\n",
      "Train Epoch: 78 [21120/54000 (39%)] Loss: -207381.593750\n",
      "Train Epoch: 78 [22528/54000 (42%)] Loss: -196316.968750\n",
      "Train Epoch: 78 [23936/54000 (44%)] Loss: -185233.906250\n",
      "Train Epoch: 78 [25344/54000 (47%)] Loss: -196323.359375\n",
      "Train Epoch: 78 [26752/54000 (50%)] Loss: -209926.734375\n",
      "Train Epoch: 78 [28160/54000 (52%)] Loss: -201859.515625\n",
      "Train Epoch: 78 [29568/54000 (55%)] Loss: -233280.218750\n",
      "Train Epoch: 78 [30976/54000 (57%)] Loss: -199972.781250\n",
      "Train Epoch: 78 [32384/54000 (60%)] Loss: -197611.375000\n",
      "Train Epoch: 78 [33792/54000 (63%)] Loss: -206789.937500\n",
      "Train Epoch: 78 [35200/54000 (65%)] Loss: -207054.843750\n",
      "Train Epoch: 78 [36608/54000 (68%)] Loss: -211014.796875\n",
      "Train Epoch: 78 [38016/54000 (70%)] Loss: -193562.218750\n",
      "Train Epoch: 78 [39424/54000 (73%)] Loss: -214321.765625\n",
      "Train Epoch: 78 [40832/54000 (76%)] Loss: -230128.031250\n",
      "Train Epoch: 78 [42240/54000 (78%)] Loss: -206617.359375\n",
      "Train Epoch: 78 [43648/54000 (81%)] Loss: -230940.421875\n",
      "Train Epoch: 78 [45056/54000 (83%)] Loss: -211192.234375\n",
      "Train Epoch: 78 [46464/54000 (86%)] Loss: -211681.921875\n",
      "Train Epoch: 78 [47872/54000 (89%)] Loss: -197696.484375\n",
      "Train Epoch: 78 [49280/54000 (91%)] Loss: -197821.765625\n",
      "Train Epoch: 78 [50688/54000 (94%)] Loss: -226009.000000\n",
      "Train Epoch: 78 [52096/54000 (96%)] Loss: -201277.453125\n",
      "    epoch          : 78\n",
      "    loss           : -204068.8007625598\n",
      "    val_loss       : -209885.3427615282\n",
      "Train Epoch: 79 [0/54000 (0%)] Loss: -202180.062500\n",
      "Train Epoch: 79 [1408/54000 (3%)] Loss: -200433.343750\n",
      "Train Epoch: 79 [2816/54000 (5%)] Loss: -210331.125000\n",
      "Train Epoch: 79 [4224/54000 (8%)] Loss: -192094.250000\n",
      "Train Epoch: 79 [5632/54000 (10%)] Loss: -197124.906250\n",
      "Train Epoch: 79 [7040/54000 (13%)] Loss: -225036.046875\n",
      "Train Epoch: 79 [8448/54000 (16%)] Loss: -208441.656250\n",
      "Train Epoch: 79 [9856/54000 (18%)] Loss: -192412.156250\n",
      "Train Epoch: 79 [11264/54000 (21%)] Loss: -206933.171875\n",
      "Train Epoch: 79 [12672/54000 (23%)] Loss: -203056.765625\n",
      "Train Epoch: 79 [14080/54000 (26%)] Loss: -233912.968750\n",
      "Train Epoch: 79 [15488/54000 (29%)] Loss: -222804.718750\n",
      "Train Epoch: 79 [16896/54000 (31%)] Loss: -192236.375000\n",
      "Train Epoch: 79 [18304/54000 (34%)] Loss: -196344.078125\n",
      "Train Epoch: 79 [19712/54000 (37%)] Loss: -181978.109375\n",
      "Train Epoch: 79 [21120/54000 (39%)] Loss: -212313.937500\n",
      "Train Epoch: 79 [22528/54000 (42%)] Loss: -218197.015625\n",
      "Train Epoch: 79 [23936/54000 (44%)] Loss: -191118.875000\n",
      "Train Epoch: 79 [25344/54000 (47%)] Loss: -209959.312500\n",
      "Train Epoch: 79 [26752/54000 (50%)] Loss: -209701.281250\n",
      "Train Epoch: 79 [28160/54000 (52%)] Loss: -233608.875000\n",
      "Train Epoch: 79 [29568/54000 (55%)] Loss: -209372.750000\n",
      "Train Epoch: 79 [30976/54000 (57%)] Loss: -208811.468750\n",
      "Train Epoch: 79 [32384/54000 (60%)] Loss: -198962.156250\n",
      "Train Epoch: 79 [33792/54000 (63%)] Loss: -193648.000000\n",
      "Train Epoch: 79 [35200/54000 (65%)] Loss: -196726.375000\n",
      "Train Epoch: 79 [36608/54000 (68%)] Loss: -195358.000000\n",
      "Train Epoch: 79 [38016/54000 (70%)] Loss: -186234.937500\n",
      "Train Epoch: 79 [39424/54000 (73%)] Loss: -213286.531250\n",
      "Train Epoch: 79 [40832/54000 (76%)] Loss: -227528.265625\n",
      "Train Epoch: 79 [42240/54000 (78%)] Loss: -184754.625000\n",
      "Train Epoch: 79 [43648/54000 (81%)] Loss: -218478.546875\n",
      "Train Epoch: 79 [45056/54000 (83%)] Loss: -200565.656250\n",
      "Train Epoch: 79 [46464/54000 (86%)] Loss: -200183.453125\n",
      "Train Epoch: 79 [47872/54000 (89%)] Loss: -200190.125000\n",
      "Train Epoch: 79 [49280/54000 (91%)] Loss: -213412.890625\n",
      "Train Epoch: 79 [50688/54000 (94%)] Loss: -199400.562500\n",
      "Train Epoch: 79 [52096/54000 (96%)] Loss: -212677.187500\n",
      "    epoch          : 79\n",
      "    loss           : -204317.35922547846\n",
      "    val_loss       : -212620.96405773627\n",
      "Train Epoch: 80 [0/54000 (0%)] Loss: -234592.843750\n",
      "Train Epoch: 80 [1408/54000 (3%)] Loss: -206394.984375\n",
      "Train Epoch: 80 [2816/54000 (5%)] Loss: -212991.968750\n",
      "Train Epoch: 80 [4224/54000 (8%)] Loss: -219957.875000\n",
      "Train Epoch: 80 [5632/54000 (10%)] Loss: -220613.625000\n",
      "Train Epoch: 80 [7040/54000 (13%)] Loss: -220483.062500\n",
      "Train Epoch: 80 [8448/54000 (16%)] Loss: -234612.906250\n",
      "Train Epoch: 80 [9856/54000 (18%)] Loss: -209688.265625\n",
      "Train Epoch: 80 [11264/54000 (21%)] Loss: -197941.515625\n",
      "Train Epoch: 80 [12672/54000 (23%)] Loss: -199870.281250\n",
      "Train Epoch: 80 [14080/54000 (26%)] Loss: -185205.906250\n",
      "Train Epoch: 80 [15488/54000 (29%)] Loss: -181716.625000\n",
      "Train Epoch: 80 [16896/54000 (31%)] Loss: -199339.515625\n",
      "Train Epoch: 80 [18304/54000 (34%)] Loss: -178571.156250\n",
      "Train Epoch: 80 [19712/54000 (37%)] Loss: -177707.140625\n",
      "Train Epoch: 80 [21120/54000 (39%)] Loss: -221033.390625\n",
      "Train Epoch: 80 [22528/54000 (42%)] Loss: -219100.890625\n",
      "Train Epoch: 80 [23936/54000 (44%)] Loss: -210681.750000\n",
      "Train Epoch: 80 [25344/54000 (47%)] Loss: -187517.000000\n",
      "Train Epoch: 80 [26752/54000 (50%)] Loss: -195077.531250\n",
      "Train Epoch: 80 [28160/54000 (52%)] Loss: -226279.421875\n",
      "Train Epoch: 80 [29568/54000 (55%)] Loss: -197627.937500\n",
      "Train Epoch: 80 [30976/54000 (57%)] Loss: -185816.453125\n",
      "Train Epoch: 80 [32384/54000 (60%)] Loss: -201722.218750\n",
      "Train Epoch: 80 [33792/54000 (63%)] Loss: -219536.125000\n",
      "Train Epoch: 80 [35200/54000 (65%)] Loss: -196071.046875\n",
      "Train Epoch: 80 [36608/54000 (68%)] Loss: -183306.234375\n",
      "Train Epoch: 80 [38016/54000 (70%)] Loss: -201098.312500\n",
      "Train Epoch: 80 [39424/54000 (73%)] Loss: -207666.421875\n",
      "Train Epoch: 80 [40832/54000 (76%)] Loss: -212429.531250\n",
      "Train Epoch: 80 [42240/54000 (78%)] Loss: -202749.218750\n",
      "Train Epoch: 80 [43648/54000 (81%)] Loss: -218292.968750\n",
      "Train Epoch: 80 [45056/54000 (83%)] Loss: -199702.796875\n",
      "Train Epoch: 80 [46464/54000 (86%)] Loss: -207737.828125\n",
      "Train Epoch: 80 [47872/54000 (89%)] Loss: -195589.046875\n",
      "Train Epoch: 80 [49280/54000 (91%)] Loss: -208047.484375\n",
      "Train Epoch: 80 [50688/54000 (94%)] Loss: -199981.875000\n",
      "Train Epoch: 80 [52096/54000 (96%)] Loss: -199473.734375\n",
      "    epoch          : 80\n",
      "    loss           : -203851.6256728469\n",
      "    val_loss       : -212311.96411728277\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch80.pth ...\n",
      "Train Epoch: 81 [0/54000 (0%)] Loss: -233888.390625\n",
      "Train Epoch: 81 [1408/54000 (3%)] Loss: -165777.156250\n",
      "Train Epoch: 81 [2816/54000 (5%)] Loss: -194080.671875\n",
      "Train Epoch: 81 [4224/54000 (8%)] Loss: -218807.718750\n",
      "Train Epoch: 81 [5632/54000 (10%)] Loss: -220913.984375\n",
      "Train Epoch: 81 [7040/54000 (13%)] Loss: -197315.421875\n",
      "Train Epoch: 81 [8448/54000 (16%)] Loss: -205056.250000\n",
      "Train Epoch: 81 [9856/54000 (18%)] Loss: -196167.562500\n",
      "Train Epoch: 81 [11264/54000 (21%)] Loss: -207189.234375\n",
      "Train Epoch: 81 [12672/54000 (23%)] Loss: -197239.265625\n",
      "Train Epoch: 81 [14080/54000 (26%)] Loss: -213491.187500\n",
      "Train Epoch: 81 [15488/54000 (29%)] Loss: -198713.218750\n",
      "Train Epoch: 81 [16896/54000 (31%)] Loss: -190969.609375\n",
      "Train Epoch: 81 [18304/54000 (34%)] Loss: -200077.234375\n",
      "Train Epoch: 81 [19712/54000 (37%)] Loss: -210124.781250\n",
      "Train Epoch: 81 [21120/54000 (39%)] Loss: -206468.421875\n",
      "Train Epoch: 81 [22528/54000 (42%)] Loss: -207185.031250\n",
      "Train Epoch: 81 [23936/54000 (44%)] Loss: -214478.562500\n",
      "Train Epoch: 81 [25344/54000 (47%)] Loss: -214643.515625\n",
      "Train Epoch: 81 [26752/54000 (50%)] Loss: -208214.078125\n",
      "Train Epoch: 81 [28160/54000 (52%)] Loss: -225192.187500\n",
      "Train Epoch: 81 [29568/54000 (55%)] Loss: -206915.859375\n",
      "Train Epoch: 81 [30976/54000 (57%)] Loss: -199471.781250\n",
      "Train Epoch: 81 [32384/54000 (60%)] Loss: -184820.312500\n",
      "Train Epoch: 81 [33792/54000 (63%)] Loss: -200457.093750\n",
      "Train Epoch: 81 [35200/54000 (65%)] Loss: -205380.828125\n",
      "Train Epoch: 81 [36608/54000 (68%)] Loss: -205152.531250\n",
      "Train Epoch: 81 [38016/54000 (70%)] Loss: -196288.859375\n",
      "Train Epoch: 81 [39424/54000 (73%)] Loss: -204654.984375\n",
      "Train Epoch: 81 [40832/54000 (76%)] Loss: -231853.015625\n",
      "Train Epoch: 81 [42240/54000 (78%)] Loss: -212591.578125\n",
      "Train Epoch: 81 [43648/54000 (81%)] Loss: -196357.812500\n",
      "Train Epoch: 81 [45056/54000 (83%)] Loss: -200627.062500\n",
      "Train Epoch: 81 [46464/54000 (86%)] Loss: -202258.406250\n",
      "Train Epoch: 81 [47872/54000 (89%)] Loss: -211886.375000\n",
      "Train Epoch: 81 [49280/54000 (91%)] Loss: -208602.781250\n",
      "Train Epoch: 81 [50688/54000 (94%)] Loss: -198898.015625\n",
      "Train Epoch: 81 [52096/54000 (96%)] Loss: -177445.562500\n",
      "    epoch          : 81\n",
      "    loss           : -204933.02855861245\n",
      "    val_loss       : -212223.48362471416\n",
      "Train Epoch: 82 [0/54000 (0%)] Loss: -197566.968750\n",
      "Train Epoch: 82 [1408/54000 (3%)] Loss: -207024.015625\n",
      "Train Epoch: 82 [2816/54000 (5%)] Loss: -210267.046875\n",
      "Train Epoch: 82 [4224/54000 (8%)] Loss: -217798.343750\n",
      "Train Epoch: 82 [5632/54000 (10%)] Loss: -208205.390625\n",
      "Train Epoch: 82 [7040/54000 (13%)] Loss: -192083.734375\n",
      "Train Epoch: 82 [8448/54000 (16%)] Loss: -196007.406250\n",
      "Train Epoch: 82 [9856/54000 (18%)] Loss: -206376.125000\n",
      "Train Epoch: 82 [11264/54000 (21%)] Loss: -202534.390625\n",
      "Train Epoch: 82 [12672/54000 (23%)] Loss: -192286.875000\n",
      "Train Epoch: 82 [14080/54000 (26%)] Loss: -193727.687500\n",
      "Train Epoch: 82 [15488/54000 (29%)] Loss: -210922.578125\n",
      "Train Epoch: 82 [16896/54000 (31%)] Loss: -198215.734375\n",
      "Train Epoch: 82 [18304/54000 (34%)] Loss: -200170.687500\n",
      "Train Epoch: 82 [19712/54000 (37%)] Loss: -185219.156250\n",
      "Train Epoch: 82 [21120/54000 (39%)] Loss: -233373.375000\n",
      "Train Epoch: 82 [22528/54000 (42%)] Loss: -186504.343750\n",
      "Train Epoch: 82 [23936/54000 (44%)] Loss: -210652.562500\n",
      "Train Epoch: 82 [25344/54000 (47%)] Loss: -199794.406250\n",
      "Train Epoch: 82 [26752/54000 (50%)] Loss: -202098.453125\n",
      "Train Epoch: 82 [28160/54000 (52%)] Loss: -214303.140625\n",
      "Train Epoch: 82 [29568/54000 (55%)] Loss: -207822.890625\n",
      "Train Epoch: 82 [30976/54000 (57%)] Loss: -195603.906250\n",
      "Train Epoch: 82 [32384/54000 (60%)] Loss: -223563.484375\n",
      "Train Epoch: 82 [33792/54000 (63%)] Loss: -202920.406250\n",
      "Train Epoch: 82 [35200/54000 (65%)] Loss: -191371.687500\n",
      "Train Epoch: 82 [36608/54000 (68%)] Loss: -186840.250000\n",
      "Train Epoch: 82 [38016/54000 (70%)] Loss: -201478.015625\n",
      "Train Epoch: 82 [39424/54000 (73%)] Loss: -191382.687500\n",
      "Train Epoch: 82 [40832/54000 (76%)] Loss: -193746.093750\n",
      "Train Epoch: 82 [42240/54000 (78%)] Loss: -213409.578125\n",
      "Train Epoch: 82 [43648/54000 (81%)] Loss: -234732.953125\n",
      "Train Epoch: 82 [45056/54000 (83%)] Loss: -202728.218750\n",
      "Train Epoch: 82 [46464/54000 (86%)] Loss: -203353.000000\n",
      "Train Epoch: 82 [47872/54000 (89%)] Loss: -197194.359375\n",
      "Train Epoch: 82 [49280/54000 (91%)] Loss: -184864.187500\n",
      "Train Epoch: 82 [50688/54000 (94%)] Loss: -212808.640625\n",
      "Train Epoch: 82 [52096/54000 (96%)] Loss: -193908.093750\n",
      "    epoch          : 82\n",
      "    loss           : -204619.9383597488\n",
      "    val_loss       : -213057.27272294209\n",
      "Train Epoch: 83 [0/54000 (0%)] Loss: -231211.015625\n",
      "Train Epoch: 83 [1408/54000 (3%)] Loss: -234555.250000\n",
      "Train Epoch: 83 [2816/54000 (5%)] Loss: -228584.906250\n",
      "Train Epoch: 83 [4224/54000 (8%)] Loss: -216269.593750\n",
      "Train Epoch: 83 [5632/54000 (10%)] Loss: -213945.656250\n",
      "Train Epoch: 83 [7040/54000 (13%)] Loss: -214777.656250\n",
      "Train Epoch: 83 [8448/54000 (16%)] Loss: -227070.875000\n",
      "Train Epoch: 83 [9856/54000 (18%)] Loss: -228687.375000\n",
      "Train Epoch: 83 [11264/54000 (21%)] Loss: -205228.703125\n",
      "Train Epoch: 83 [12672/54000 (23%)] Loss: -200692.484375\n",
      "Train Epoch: 83 [14080/54000 (26%)] Loss: -197139.250000\n",
      "Train Epoch: 83 [15488/54000 (29%)] Loss: -225175.796875\n",
      "Train Epoch: 83 [16896/54000 (31%)] Loss: -200609.625000\n",
      "Train Epoch: 83 [18304/54000 (34%)] Loss: -190493.062500\n",
      "Train Epoch: 83 [19712/54000 (37%)] Loss: -203317.625000\n",
      "Train Epoch: 83 [21120/54000 (39%)] Loss: -223090.406250\n",
      "Train Epoch: 83 [22528/54000 (42%)] Loss: -200353.296875\n",
      "Train Epoch: 83 [23936/54000 (44%)] Loss: -186820.453125\n",
      "Train Epoch: 83 [25344/54000 (47%)] Loss: -205750.671875\n",
      "Train Epoch: 83 [26752/54000 (50%)] Loss: -201071.031250\n",
      "Train Epoch: 83 [28160/54000 (52%)] Loss: -227384.812500\n",
      "Train Epoch: 83 [29568/54000 (55%)] Loss: -211371.687500\n",
      "Train Epoch: 83 [30976/54000 (57%)] Loss: -203467.156250\n",
      "Train Epoch: 83 [32384/54000 (60%)] Loss: -214626.000000\n",
      "Train Epoch: 83 [33792/54000 (63%)] Loss: -223275.734375\n",
      "Train Epoch: 83 [35200/54000 (65%)] Loss: -202844.859375\n",
      "Train Epoch: 83 [36608/54000 (68%)] Loss: -185259.421875\n",
      "Train Epoch: 83 [38016/54000 (70%)] Loss: -205298.640625\n",
      "Train Epoch: 83 [39424/54000 (73%)] Loss: -234823.875000\n",
      "Train Epoch: 83 [40832/54000 (76%)] Loss: -199445.718750\n",
      "Train Epoch: 83 [42240/54000 (78%)] Loss: -206526.562500\n",
      "Train Epoch: 83 [43648/54000 (81%)] Loss: -189763.765625\n",
      "Train Epoch: 83 [45056/54000 (83%)] Loss: -226944.968750\n",
      "Train Epoch: 83 [46464/54000 (86%)] Loss: -196061.609375\n",
      "Train Epoch: 83 [47872/54000 (89%)] Loss: -192089.578125\n",
      "Train Epoch: 83 [49280/54000 (91%)] Loss: -213234.625000\n",
      "Train Epoch: 83 [50688/54000 (94%)] Loss: -205876.656250\n",
      "Train Epoch: 83 [52096/54000 (96%)] Loss: -211404.171875\n",
      "    epoch          : 83\n",
      "    loss           : -205179.55894886365\n",
      "    val_loss       : -214507.06089224466\n",
      "Train Epoch: 84 [0/54000 (0%)] Loss: -234799.750000\n",
      "Train Epoch: 84 [1408/54000 (3%)] Loss: -229188.687500\n",
      "Train Epoch: 84 [2816/54000 (5%)] Loss: -201393.390625\n",
      "Train Epoch: 84 [4224/54000 (8%)] Loss: -219147.718750\n",
      "Train Epoch: 84 [5632/54000 (10%)] Loss: -197501.531250\n",
      "Train Epoch: 84 [7040/54000 (13%)] Loss: -198353.171875\n",
      "Train Epoch: 84 [8448/54000 (16%)] Loss: -186760.250000\n",
      "Train Epoch: 84 [9856/54000 (18%)] Loss: -185298.593750\n",
      "Train Epoch: 84 [11264/54000 (21%)] Loss: -214286.968750\n",
      "Train Epoch: 84 [12672/54000 (23%)] Loss: -202617.921875\n",
      "Train Epoch: 84 [14080/54000 (26%)] Loss: -204943.265625\n",
      "Train Epoch: 84 [15488/54000 (29%)] Loss: -195951.406250\n",
      "Train Epoch: 84 [16896/54000 (31%)] Loss: -223571.968750\n",
      "Train Epoch: 84 [18304/54000 (34%)] Loss: -230668.125000\n",
      "Train Epoch: 84 [19712/54000 (37%)] Loss: -190457.343750\n",
      "Train Epoch: 84 [21120/54000 (39%)] Loss: -234231.625000\n",
      "Train Epoch: 84 [22528/54000 (42%)] Loss: -203650.140625\n",
      "Train Epoch: 84 [23936/54000 (44%)] Loss: -197704.109375\n",
      "Train Epoch: 84 [25344/54000 (47%)] Loss: -190661.187500\n",
      "Train Epoch: 84 [26752/54000 (50%)] Loss: -208845.562500\n",
      "Train Epoch: 84 [28160/54000 (52%)] Loss: -205394.625000\n",
      "Train Epoch: 84 [29568/54000 (55%)] Loss: -212481.000000\n",
      "Train Epoch: 84 [30976/54000 (57%)] Loss: -203613.437500\n",
      "Train Epoch: 84 [32384/54000 (60%)] Loss: -204479.171875\n",
      "Train Epoch: 84 [33792/54000 (63%)] Loss: -203602.312500\n",
      "Train Epoch: 84 [35200/54000 (65%)] Loss: -198820.562500\n",
      "Train Epoch: 84 [36608/54000 (68%)] Loss: -205188.125000\n",
      "Train Epoch: 84 [38016/54000 (70%)] Loss: -207119.078125\n",
      "Train Epoch: 84 [39424/54000 (73%)] Loss: -201504.187500\n",
      "Train Epoch: 84 [40832/54000 (76%)] Loss: -204935.468750\n",
      "Train Epoch: 84 [42240/54000 (78%)] Loss: -192866.421875\n",
      "Train Epoch: 84 [43648/54000 (81%)] Loss: -210306.062500\n",
      "Train Epoch: 84 [45056/54000 (83%)] Loss: -199601.812500\n",
      "Train Epoch: 84 [46464/54000 (86%)] Loss: -201120.468750\n",
      "Train Epoch: 84 [47872/54000 (89%)] Loss: -201331.500000\n",
      "Train Epoch: 84 [49280/54000 (91%)] Loss: -196030.296875\n",
      "Train Epoch: 84 [50688/54000 (94%)] Loss: -182298.781250\n",
      "Train Epoch: 84 [52096/54000 (96%)] Loss: -182953.062500\n",
      "    epoch          : 84\n",
      "    loss           : -205031.2116103469\n",
      "    val_loss       : -213079.41356230946\n",
      "Train Epoch: 85 [0/54000 (0%)] Loss: -197576.671875\n",
      "Train Epoch: 85 [1408/54000 (3%)] Loss: -207628.765625\n",
      "Train Epoch: 85 [2816/54000 (5%)] Loss: -213993.453125\n",
      "Train Epoch: 85 [4224/54000 (8%)] Loss: -206735.812500\n",
      "Train Epoch: 85 [5632/54000 (10%)] Loss: -183348.500000\n",
      "Train Epoch: 85 [7040/54000 (13%)] Loss: -195277.234375\n",
      "Train Epoch: 85 [8448/54000 (16%)] Loss: -184591.625000\n",
      "Train Epoch: 85 [9856/54000 (18%)] Loss: -195680.875000\n",
      "Train Epoch: 85 [11264/54000 (21%)] Loss: -207830.875000\n",
      "Train Epoch: 85 [12672/54000 (23%)] Loss: -207389.953125\n",
      "Train Epoch: 85 [14080/54000 (26%)] Loss: -207564.828125\n",
      "Train Epoch: 85 [15488/54000 (29%)] Loss: -199871.093750\n",
      "Train Epoch: 85 [16896/54000 (31%)] Loss: -207208.437500\n",
      "Train Epoch: 85 [18304/54000 (34%)] Loss: -199943.781250\n",
      "Train Epoch: 85 [19712/54000 (37%)] Loss: -199408.125000\n",
      "Train Epoch: 85 [21120/54000 (39%)] Loss: -198939.640625\n",
      "Train Epoch: 85 [22528/54000 (42%)] Loss: -198594.187500\n",
      "Train Epoch: 85 [23936/54000 (44%)] Loss: -210557.625000\n",
      "Train Epoch: 85 [25344/54000 (47%)] Loss: -202407.750000\n",
      "Train Epoch: 85 [26752/54000 (50%)] Loss: -197751.828125\n",
      "Train Epoch: 85 [28160/54000 (52%)] Loss: -204283.921875\n",
      "Train Epoch: 85 [29568/54000 (55%)] Loss: -187189.437500\n",
      "Train Epoch: 85 [30976/54000 (57%)] Loss: -212547.828125\n",
      "Train Epoch: 85 [32384/54000 (60%)] Loss: -190382.234375\n",
      "Train Epoch: 85 [33792/54000 (63%)] Loss: -207190.062500\n",
      "Train Epoch: 85 [35200/54000 (65%)] Loss: -195585.343750\n",
      "Train Epoch: 85 [36608/54000 (68%)] Loss: -187415.750000\n",
      "Train Epoch: 85 [38016/54000 (70%)] Loss: -184483.437500\n",
      "Train Epoch: 85 [39424/54000 (73%)] Loss: -176380.718750\n",
      "Train Epoch: 85 [40832/54000 (76%)] Loss: -213409.234375\n",
      "Train Epoch: 85 [42240/54000 (78%)] Loss: -201738.375000\n",
      "Train Epoch: 85 [43648/54000 (81%)] Loss: -235271.812500\n",
      "Train Epoch: 85 [45056/54000 (83%)] Loss: -207493.750000\n",
      "Train Epoch: 85 [46464/54000 (86%)] Loss: -207088.546875\n",
      "Train Epoch: 85 [47872/54000 (89%)] Loss: -207572.812500\n",
      "Train Epoch: 85 [49280/54000 (91%)] Loss: -203393.781250\n",
      "Train Epoch: 85 [50688/54000 (94%)] Loss: -229287.218750\n",
      "Train Epoch: 85 [52096/54000 (96%)] Loss: -213400.875000\n",
      "    epoch          : 85\n",
      "    loss           : -205528.25814892346\n",
      "    val_loss       : -212023.18230754574\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch85.pth ...\n",
      "Train Epoch: 86 [0/54000 (0%)] Loss: -206956.171875\n",
      "Train Epoch: 86 [1408/54000 (3%)] Loss: -214756.671875\n",
      "Train Epoch: 86 [2816/54000 (5%)] Loss: -197699.890625\n",
      "Train Epoch: 86 [4224/54000 (8%)] Loss: -196589.765625\n",
      "Train Epoch: 86 [5632/54000 (10%)] Loss: -204352.875000\n",
      "Train Epoch: 86 [7040/54000 (13%)] Loss: -193184.375000\n",
      "Train Epoch: 86 [8448/54000 (16%)] Loss: -203590.078125\n",
      "Train Epoch: 86 [9856/54000 (18%)] Loss: -202986.125000\n",
      "Train Epoch: 86 [11264/54000 (21%)] Loss: -178722.812500\n",
      "Train Epoch: 86 [12672/54000 (23%)] Loss: -205342.453125\n",
      "Train Epoch: 86 [14080/54000 (26%)] Loss: -198041.343750\n",
      "Train Epoch: 86 [15488/54000 (29%)] Loss: -210495.265625\n",
      "Train Epoch: 86 [16896/54000 (31%)] Loss: -196654.406250\n",
      "Train Epoch: 86 [18304/54000 (34%)] Loss: -186849.250000\n",
      "Train Epoch: 86 [19712/54000 (37%)] Loss: -200747.390625\n",
      "Train Epoch: 86 [21120/54000 (39%)] Loss: -207911.781250\n",
      "Train Epoch: 86 [22528/54000 (42%)] Loss: -203828.000000\n",
      "Train Epoch: 86 [23936/54000 (44%)] Loss: -211441.593750\n",
      "Train Epoch: 86 [25344/54000 (47%)] Loss: -203923.875000\n",
      "Train Epoch: 86 [26752/54000 (50%)] Loss: -200046.875000\n",
      "Train Epoch: 86 [28160/54000 (52%)] Loss: -202232.406250\n",
      "Train Epoch: 86 [29568/54000 (55%)] Loss: -193785.015625\n",
      "Train Epoch: 86 [30976/54000 (57%)] Loss: -234619.593750\n",
      "Train Epoch: 86 [32384/54000 (60%)] Loss: -208210.234375\n",
      "Train Epoch: 86 [33792/54000 (63%)] Loss: -204825.500000\n",
      "Train Epoch: 86 [35200/54000 (65%)] Loss: -194506.531250\n",
      "Train Epoch: 86 [36608/54000 (68%)] Loss: -194732.078125\n",
      "Train Epoch: 86 [38016/54000 (70%)] Loss: -235453.140625\n",
      "Train Epoch: 86 [39424/54000 (73%)] Loss: -201534.546875\n",
      "Train Epoch: 86 [40832/54000 (76%)] Loss: -211105.859375\n",
      "Train Epoch: 86 [42240/54000 (78%)] Loss: -208467.375000\n",
      "Train Epoch: 86 [43648/54000 (81%)] Loss: -223124.062500\n",
      "Train Epoch: 86 [45056/54000 (83%)] Loss: -206267.828125\n",
      "Train Epoch: 86 [46464/54000 (86%)] Loss: -187413.250000\n",
      "Train Epoch: 86 [47872/54000 (89%)] Loss: -182834.109375\n",
      "Train Epoch: 86 [49280/54000 (91%)] Loss: -194639.234375\n",
      "Train Epoch: 86 [50688/54000 (94%)] Loss: -206097.921875\n",
      "Train Epoch: 86 [52096/54000 (96%)] Loss: -226274.234375\n",
      "    epoch          : 86\n",
      "    loss           : -205489.94542464116\n",
      "    val_loss       : -215053.15640482088\n",
      "Train Epoch: 87 [0/54000 (0%)] Loss: -235301.750000\n",
      "Train Epoch: 87 [1408/54000 (3%)] Loss: -219966.625000\n",
      "Train Epoch: 87 [2816/54000 (5%)] Loss: -195725.515625\n",
      "Train Epoch: 87 [4224/54000 (8%)] Loss: -194486.281250\n",
      "Train Epoch: 87 [5632/54000 (10%)] Loss: -184113.750000\n",
      "Train Epoch: 87 [7040/54000 (13%)] Loss: -235645.203125\n",
      "Train Epoch: 87 [8448/54000 (16%)] Loss: -212525.343750\n",
      "Train Epoch: 87 [9856/54000 (18%)] Loss: -205415.671875\n",
      "Train Epoch: 87 [11264/54000 (21%)] Loss: -206913.328125\n",
      "Train Epoch: 87 [12672/54000 (23%)] Loss: -203657.859375\n",
      "Train Epoch: 87 [14080/54000 (26%)] Loss: -208028.578125\n",
      "Train Epoch: 87 [15488/54000 (29%)] Loss: -183395.812500\n",
      "Train Epoch: 87 [16896/54000 (31%)] Loss: -211980.093750\n",
      "Train Epoch: 87 [18304/54000 (34%)] Loss: -211996.062500\n",
      "Train Epoch: 87 [19712/54000 (37%)] Loss: -204452.046875\n",
      "Train Epoch: 87 [21120/54000 (39%)] Loss: -196561.390625\n",
      "Train Epoch: 87 [22528/54000 (42%)] Loss: -203816.156250\n",
      "Train Epoch: 87 [23936/54000 (44%)] Loss: -197742.546875\n",
      "Train Epoch: 87 [25344/54000 (47%)] Loss: -199731.687500\n",
      "Train Epoch: 87 [26752/54000 (50%)] Loss: -205213.531250\n",
      "Train Epoch: 87 [28160/54000 (52%)] Loss: -226870.281250\n",
      "Train Epoch: 87 [29568/54000 (55%)] Loss: -205895.000000\n",
      "Train Epoch: 87 [30976/54000 (57%)] Loss: -199374.109375\n",
      "Train Epoch: 87 [32384/54000 (60%)] Loss: -222780.500000\n",
      "Train Epoch: 87 [33792/54000 (63%)] Loss: -203818.750000\n",
      "Train Epoch: 87 [35200/54000 (65%)] Loss: -203906.781250\n",
      "Train Epoch: 87 [36608/54000 (68%)] Loss: -220053.781250\n",
      "Train Epoch: 87 [38016/54000 (70%)] Loss: -224344.828125\n",
      "Train Epoch: 87 [39424/54000 (73%)] Loss: -200226.000000\n",
      "Train Epoch: 87 [40832/54000 (76%)] Loss: -198287.406250\n",
      "Train Epoch: 87 [42240/54000 (78%)] Loss: -204767.359375\n",
      "Train Epoch: 87 [43648/54000 (81%)] Loss: -212021.218750\n",
      "Train Epoch: 87 [45056/54000 (83%)] Loss: -206063.500000\n",
      "Train Epoch: 87 [46464/54000 (86%)] Loss: -215341.984375\n",
      "Train Epoch: 87 [47872/54000 (89%)] Loss: -187961.750000\n",
      "Train Epoch: 87 [49280/54000 (91%)] Loss: -195776.718750\n",
      "Train Epoch: 87 [50688/54000 (94%)] Loss: -186932.046875\n",
      "Train Epoch: 87 [52096/54000 (96%)] Loss: -201090.921875\n",
      "    epoch          : 87\n",
      "    loss           : -205539.44086423446\n",
      "    val_loss       : -212254.12288014483\n",
      "Train Epoch: 88 [0/54000 (0%)] Loss: -234684.390625\n",
      "Train Epoch: 88 [1408/54000 (3%)] Loss: -213661.671875\n",
      "Train Epoch: 88 [2816/54000 (5%)] Loss: -205487.906250\n",
      "Train Epoch: 88 [4224/54000 (8%)] Loss: -184779.812500\n",
      "Train Epoch: 88 [5632/54000 (10%)] Loss: -201287.593750\n",
      "Train Epoch: 88 [7040/54000 (13%)] Loss: -189897.875000\n",
      "Train Epoch: 88 [8448/54000 (16%)] Loss: -192939.843750\n",
      "Train Epoch: 88 [9856/54000 (18%)] Loss: -197229.234375\n",
      "Train Epoch: 88 [11264/54000 (21%)] Loss: -194785.359375\n",
      "Train Epoch: 88 [12672/54000 (23%)] Loss: -187249.906250\n",
      "Train Epoch: 88 [14080/54000 (26%)] Loss: -202154.031250\n",
      "Train Epoch: 88 [15488/54000 (29%)] Loss: -206381.875000\n",
      "Train Epoch: 88 [16896/54000 (31%)] Loss: -180584.843750\n",
      "Train Epoch: 88 [18304/54000 (34%)] Loss: -227206.593750\n",
      "Train Epoch: 88 [19712/54000 (37%)] Loss: -191132.421875\n",
      "Train Epoch: 88 [21120/54000 (39%)] Loss: -202946.953125\n",
      "Train Epoch: 88 [22528/54000 (42%)] Loss: -205210.531250\n",
      "Train Epoch: 88 [23936/54000 (44%)] Loss: -200216.562500\n",
      "Train Epoch: 88 [25344/54000 (47%)] Loss: -211979.906250\n",
      "Train Epoch: 88 [26752/54000 (50%)] Loss: -211290.937500\n",
      "Train Epoch: 88 [28160/54000 (52%)] Loss: -219671.281250\n",
      "Train Epoch: 88 [29568/54000 (55%)] Loss: -188842.828125\n",
      "Train Epoch: 88 [30976/54000 (57%)] Loss: -201912.500000\n",
      "Train Epoch: 88 [32384/54000 (60%)] Loss: -209224.484375\n",
      "Train Epoch: 88 [33792/54000 (63%)] Loss: -191351.250000\n",
      "Train Epoch: 88 [35200/54000 (65%)] Loss: -214446.671875\n",
      "Train Epoch: 88 [36608/54000 (68%)] Loss: -199631.031250\n",
      "Train Epoch: 88 [38016/54000 (70%)] Loss: -206786.906250\n",
      "Train Epoch: 88 [39424/54000 (73%)] Loss: -230084.718750\n",
      "Train Epoch: 88 [40832/54000 (76%)] Loss: -205920.234375\n",
      "Train Epoch: 88 [42240/54000 (78%)] Loss: -213846.921875\n",
      "Train Epoch: 88 [43648/54000 (81%)] Loss: -211069.500000\n",
      "Train Epoch: 88 [45056/54000 (83%)] Loss: -186110.140625\n",
      "Train Epoch: 88 [46464/54000 (86%)] Loss: -196569.781250\n",
      "Train Epoch: 88 [47872/54000 (89%)] Loss: -209099.187500\n",
      "Train Epoch: 88 [49280/54000 (91%)] Loss: -174119.562500\n",
      "Train Epoch: 88 [50688/54000 (94%)] Loss: -199256.750000\n",
      "Train Epoch: 88 [52096/54000 (96%)] Loss: -197473.765625\n",
      "    epoch          : 88\n",
      "    loss           : -205263.48452452154\n",
      "    val_loss       : -211329.52174637959\n",
      "Train Epoch: 89 [0/54000 (0%)] Loss: -234049.765625\n",
      "Train Epoch: 89 [1408/54000 (3%)] Loss: -232328.250000\n",
      "Train Epoch: 89 [2816/54000 (5%)] Loss: -228757.281250\n",
      "Train Epoch: 89 [4224/54000 (8%)] Loss: -218261.843750\n",
      "Train Epoch: 89 [5632/54000 (10%)] Loss: -192888.921875\n",
      "Train Epoch: 89 [7040/54000 (13%)] Loss: -214416.015625\n",
      "Train Epoch: 89 [8448/54000 (16%)] Loss: -210080.093750\n",
      "Train Epoch: 89 [9856/54000 (18%)] Loss: -199857.078125\n",
      "Train Epoch: 89 [11264/54000 (21%)] Loss: -189151.218750\n",
      "Train Epoch: 89 [12672/54000 (23%)] Loss: -203876.515625\n",
      "Train Epoch: 89 [14080/54000 (26%)] Loss: -214312.921875\n",
      "Train Epoch: 89 [15488/54000 (29%)] Loss: -206992.031250\n",
      "Train Epoch: 89 [16896/54000 (31%)] Loss: -219946.781250\n",
      "Train Epoch: 89 [18304/54000 (34%)] Loss: -218421.640625\n",
      "Train Epoch: 89 [19712/54000 (37%)] Loss: -208871.359375\n",
      "Train Epoch: 89 [21120/54000 (39%)] Loss: -186063.187500\n",
      "Train Epoch: 89 [22528/54000 (42%)] Loss: -201005.375000\n",
      "Train Epoch: 89 [23936/54000 (44%)] Loss: -213712.593750\n",
      "Train Epoch: 89 [25344/54000 (47%)] Loss: -206237.625000\n",
      "Train Epoch: 89 [26752/54000 (50%)] Loss: -205570.531250\n",
      "Train Epoch: 89 [28160/54000 (52%)] Loss: -201941.968750\n",
      "Train Epoch: 89 [29568/54000 (55%)] Loss: -203886.906250\n",
      "Train Epoch: 89 [30976/54000 (57%)] Loss: -212618.765625\n",
      "Train Epoch: 89 [32384/54000 (60%)] Loss: -203027.171875\n",
      "Train Epoch: 89 [33792/54000 (63%)] Loss: -209420.859375\n",
      "Train Epoch: 89 [35200/54000 (65%)] Loss: -195949.156250\n",
      "Train Epoch: 89 [36608/54000 (68%)] Loss: -214976.484375\n",
      "Train Epoch: 89 [38016/54000 (70%)] Loss: -189813.531250\n",
      "Train Epoch: 89 [39424/54000 (73%)] Loss: -228544.656250\n",
      "Train Epoch: 89 [40832/54000 (76%)] Loss: -198096.671875\n",
      "Train Epoch: 89 [42240/54000 (78%)] Loss: -210560.656250\n",
      "Train Epoch: 89 [43648/54000 (81%)] Loss: -203450.281250\n",
      "Train Epoch: 89 [45056/54000 (83%)] Loss: -199241.703125\n",
      "Train Epoch: 89 [46464/54000 (86%)] Loss: -201885.875000\n",
      "Train Epoch: 89 [47872/54000 (89%)] Loss: -207886.953125\n",
      "Train Epoch: 89 [49280/54000 (91%)] Loss: -195872.281250\n",
      "Train Epoch: 89 [50688/54000 (94%)] Loss: -198620.187500\n",
      "Train Epoch: 89 [52096/54000 (96%)] Loss: -203698.859375\n",
      "    epoch          : 89\n",
      "    loss           : -205297.73564593302\n",
      "    val_loss       : -211239.19376429115\n",
      "Train Epoch: 90 [0/54000 (0%)] Loss: -210022.156250\n",
      "Train Epoch: 90 [1408/54000 (3%)] Loss: -209200.765625\n",
      "Train Epoch: 90 [2816/54000 (5%)] Loss: -200326.453125\n",
      "Train Epoch: 90 [4224/54000 (8%)] Loss: -186057.656250\n",
      "Train Epoch: 90 [5632/54000 (10%)] Loss: -212930.093750\n",
      "Train Epoch: 90 [7040/54000 (13%)] Loss: -234843.781250\n",
      "Train Epoch: 90 [8448/54000 (16%)] Loss: -197898.093750\n",
      "Train Epoch: 90 [9856/54000 (18%)] Loss: -189862.250000\n",
      "Train Epoch: 90 [11264/54000 (21%)] Loss: -192450.937500\n",
      "Train Epoch: 90 [12672/54000 (23%)] Loss: -234073.890625\n",
      "Train Epoch: 90 [14080/54000 (26%)] Loss: -185155.390625\n",
      "Train Epoch: 90 [15488/54000 (29%)] Loss: -205795.640625\n",
      "Train Epoch: 90 [16896/54000 (31%)] Loss: -195810.281250\n",
      "Train Epoch: 90 [18304/54000 (34%)] Loss: -210975.687500\n",
      "Train Epoch: 90 [19712/54000 (37%)] Loss: -197451.781250\n",
      "Train Epoch: 90 [21120/54000 (39%)] Loss: -220491.828125\n",
      "Train Epoch: 90 [22528/54000 (42%)] Loss: -225892.281250\n",
      "Train Epoch: 90 [23936/54000 (44%)] Loss: -199363.343750\n",
      "Train Epoch: 90 [25344/54000 (47%)] Loss: -207091.640625\n",
      "Train Epoch: 90 [26752/54000 (50%)] Loss: -184105.312500\n",
      "Train Epoch: 90 [28160/54000 (52%)] Loss: -201489.750000\n",
      "Train Epoch: 90 [29568/54000 (55%)] Loss: -235783.156250\n",
      "Train Epoch: 90 [30976/54000 (57%)] Loss: -205051.343750\n",
      "Train Epoch: 90 [32384/54000 (60%)] Loss: -214224.609375\n",
      "Train Epoch: 90 [33792/54000 (63%)] Loss: -207406.328125\n",
      "Train Epoch: 90 [35200/54000 (65%)] Loss: -220874.687500\n",
      "Train Epoch: 90 [36608/54000 (68%)] Loss: -183488.109375\n",
      "Train Epoch: 90 [38016/54000 (70%)] Loss: -230308.875000\n",
      "Train Epoch: 90 [39424/54000 (73%)] Loss: -208087.062500\n",
      "Train Epoch: 90 [40832/54000 (76%)] Loss: -199381.828125\n",
      "Train Epoch: 90 [42240/54000 (78%)] Loss: -196198.718750\n",
      "Train Epoch: 90 [43648/54000 (81%)] Loss: -200439.593750\n",
      "Train Epoch: 90 [45056/54000 (83%)] Loss: -195631.000000\n",
      "Train Epoch: 90 [46464/54000 (86%)] Loss: -206195.968750\n",
      "Train Epoch: 90 [47872/54000 (89%)] Loss: -195567.781250\n",
      "Train Epoch: 90 [49280/54000 (91%)] Loss: -201049.734375\n",
      "Train Epoch: 90 [50688/54000 (94%)] Loss: -235780.015625\n",
      "Train Epoch: 90 [52096/54000 (96%)] Loss: -198740.687500\n",
      "    epoch          : 90\n",
      "    loss           : -205731.75725179425\n",
      "    val_loss       : -213452.74250905108\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch90.pth ...\n",
      "Train Epoch: 91 [0/54000 (0%)] Loss: -202308.421875\n",
      "Train Epoch: 91 [1408/54000 (3%)] Loss: -201136.437500\n",
      "Train Epoch: 91 [2816/54000 (5%)] Loss: -231315.218750\n",
      "Train Epoch: 91 [4224/54000 (8%)] Loss: -204084.765625\n",
      "Train Epoch: 91 [5632/54000 (10%)] Loss: -205599.515625\n",
      "Train Epoch: 91 [7040/54000 (13%)] Loss: -208616.734375\n",
      "Train Epoch: 91 [8448/54000 (16%)] Loss: -183479.359375\n",
      "Train Epoch: 91 [9856/54000 (18%)] Loss: -206415.203125\n",
      "Train Epoch: 91 [11264/54000 (21%)] Loss: -196698.484375\n",
      "Train Epoch: 91 [12672/54000 (23%)] Loss: -195232.484375\n",
      "Train Epoch: 91 [14080/54000 (26%)] Loss: -220830.890625\n",
      "Train Epoch: 91 [15488/54000 (29%)] Loss: -226825.218750\n",
      "Train Epoch: 91 [16896/54000 (31%)] Loss: -208612.156250\n",
      "Train Epoch: 91 [18304/54000 (34%)] Loss: -213513.593750\n",
      "Train Epoch: 91 [19712/54000 (37%)] Loss: -220973.484375\n",
      "Train Epoch: 91 [21120/54000 (39%)] Loss: -213570.640625\n",
      "Train Epoch: 91 [22528/54000 (42%)] Loss: -219544.515625\n",
      "Train Epoch: 91 [23936/54000 (44%)] Loss: -198283.687500\n",
      "Train Epoch: 91 [25344/54000 (47%)] Loss: -212873.265625\n",
      "Train Epoch: 91 [26752/54000 (50%)] Loss: -205879.750000\n",
      "Train Epoch: 91 [28160/54000 (52%)] Loss: -201946.937500\n",
      "Train Epoch: 91 [29568/54000 (55%)] Loss: -202096.671875\n",
      "Train Epoch: 91 [30976/54000 (57%)] Loss: -208086.625000\n",
      "Train Epoch: 91 [32384/54000 (60%)] Loss: -214699.281250\n",
      "Train Epoch: 91 [33792/54000 (63%)] Loss: -229705.343750\n",
      "Train Epoch: 91 [35200/54000 (65%)] Loss: -199574.875000\n",
      "Train Epoch: 91 [36608/54000 (68%)] Loss: -200043.062500\n",
      "Train Epoch: 91 [38016/54000 (70%)] Loss: -231425.515625\n",
      "Train Epoch: 91 [39424/54000 (73%)] Loss: -197093.843750\n",
      "Train Epoch: 91 [40832/54000 (76%)] Loss: -198778.359375\n",
      "Train Epoch: 91 [42240/54000 (78%)] Loss: -182223.062500\n",
      "Train Epoch: 91 [43648/54000 (81%)] Loss: -208500.390625\n",
      "Train Epoch: 91 [45056/54000 (83%)] Loss: -204406.250000\n",
      "Train Epoch: 91 [46464/54000 (86%)] Loss: -199478.078125\n",
      "Train Epoch: 91 [47872/54000 (89%)] Loss: -203700.265625\n",
      "Train Epoch: 91 [49280/54000 (91%)] Loss: -186056.125000\n",
      "Train Epoch: 91 [50688/54000 (94%)] Loss: -185558.937500\n",
      "Train Epoch: 91 [52096/54000 (96%)] Loss: -188737.312500\n",
      "    epoch          : 91\n",
      "    loss           : -205887.94243421053\n",
      "    val_loss       : -214843.66226419588\n",
      "Train Epoch: 92 [0/54000 (0%)] Loss: -206345.843750\n",
      "Train Epoch: 92 [1408/54000 (3%)] Loss: -185955.968750\n",
      "Train Epoch: 92 [2816/54000 (5%)] Loss: -199217.703125\n",
      "Train Epoch: 92 [4224/54000 (8%)] Loss: -206421.281250\n",
      "Train Epoch: 92 [5632/54000 (10%)] Loss: -204811.578125\n",
      "Train Epoch: 92 [7040/54000 (13%)] Loss: -195639.406250\n",
      "Train Epoch: 92 [8448/54000 (16%)] Loss: -187286.875000\n",
      "Train Epoch: 92 [9856/54000 (18%)] Loss: -235205.312500\n",
      "Train Epoch: 92 [11264/54000 (21%)] Loss: -199533.343750\n",
      "Train Epoch: 92 [12672/54000 (23%)] Loss: -213910.093750\n",
      "Train Epoch: 92 [14080/54000 (26%)] Loss: -204593.593750\n",
      "Train Epoch: 92 [15488/54000 (29%)] Loss: -214405.453125\n",
      "Train Epoch: 92 [16896/54000 (31%)] Loss: -198153.140625\n",
      "Train Epoch: 92 [18304/54000 (34%)] Loss: -182490.687500\n",
      "Train Epoch: 92 [19712/54000 (37%)] Loss: -197110.984375\n",
      "Train Epoch: 92 [21120/54000 (39%)] Loss: -198818.546875\n",
      "Train Epoch: 92 [22528/54000 (42%)] Loss: -197293.468750\n",
      "Train Epoch: 92 [23936/54000 (44%)] Loss: -214676.562500\n",
      "Train Epoch: 92 [25344/54000 (47%)] Loss: -204064.156250\n",
      "Train Epoch: 92 [26752/54000 (50%)] Loss: -207582.156250\n",
      "Train Epoch: 92 [28160/54000 (52%)] Loss: -204356.500000\n",
      "Train Epoch: 92 [29568/54000 (55%)] Loss: -207792.281250\n",
      "Train Epoch: 92 [30976/54000 (57%)] Loss: -211576.953125\n",
      "Train Epoch: 92 [32384/54000 (60%)] Loss: -199037.375000\n",
      "Train Epoch: 92 [33792/54000 (63%)] Loss: -209986.265625\n",
      "Train Epoch: 92 [35200/54000 (65%)] Loss: -188127.000000\n",
      "Train Epoch: 92 [36608/54000 (68%)] Loss: -188595.890625\n",
      "Train Epoch: 92 [38016/54000 (70%)] Loss: -209594.390625\n",
      "Train Epoch: 92 [39424/54000 (73%)] Loss: -221098.765625\n",
      "Train Epoch: 92 [40832/54000 (76%)] Loss: -210972.937500\n",
      "Train Epoch: 92 [42240/54000 (78%)] Loss: -203602.093750\n",
      "Train Epoch: 92 [43648/54000 (81%)] Loss: -199715.625000\n",
      "Train Epoch: 92 [45056/54000 (83%)] Loss: -236369.843750\n",
      "Train Epoch: 92 [46464/54000 (86%)] Loss: -188680.750000\n",
      "Train Epoch: 92 [47872/54000 (89%)] Loss: -197757.500000\n",
      "Train Epoch: 92 [49280/54000 (91%)] Loss: -191812.281250\n",
      "Train Epoch: 92 [50688/54000 (94%)] Loss: -192121.937500\n",
      "Train Epoch: 92 [52096/54000 (96%)] Loss: -200755.078125\n",
      "    epoch          : 92\n",
      "    loss           : -206006.66350179425\n",
      "    val_loss       : -213730.84043921495\n",
      "Train Epoch: 93 [0/54000 (0%)] Loss: -202005.406250\n",
      "Train Epoch: 93 [1408/54000 (3%)] Loss: -208696.484375\n",
      "Train Epoch: 93 [2816/54000 (5%)] Loss: -198724.171875\n",
      "Train Epoch: 93 [4224/54000 (8%)] Loss: -204082.078125\n",
      "Train Epoch: 93 [5632/54000 (10%)] Loss: -207929.875000\n",
      "Train Epoch: 93 [7040/54000 (13%)] Loss: -208518.468750\n",
      "Train Epoch: 93 [8448/54000 (16%)] Loss: -191202.765625\n",
      "Train Epoch: 93 [9856/54000 (18%)] Loss: -195751.734375\n",
      "Train Epoch: 93 [11264/54000 (21%)] Loss: -207047.765625\n",
      "Train Epoch: 93 [12672/54000 (23%)] Loss: -208655.515625\n",
      "Train Epoch: 93 [14080/54000 (26%)] Loss: -199907.765625\n",
      "Train Epoch: 93 [15488/54000 (29%)] Loss: -207709.125000\n",
      "Train Epoch: 93 [16896/54000 (31%)] Loss: -236222.218750\n",
      "Train Epoch: 93 [18304/54000 (34%)] Loss: -188446.031250\n",
      "Train Epoch: 93 [19712/54000 (37%)] Loss: -210183.937500\n",
      "Train Epoch: 93 [21120/54000 (39%)] Loss: -211360.296875\n",
      "Train Epoch: 93 [22528/54000 (42%)] Loss: -209609.750000\n",
      "Train Epoch: 93 [23936/54000 (44%)] Loss: -199226.140625\n",
      "Train Epoch: 93 [25344/54000 (47%)] Loss: -215122.187500\n",
      "Train Epoch: 93 [26752/54000 (50%)] Loss: -190417.218750\n",
      "Train Epoch: 93 [28160/54000 (52%)] Loss: -206847.875000\n",
      "Train Epoch: 93 [29568/54000 (55%)] Loss: -208611.500000\n",
      "Train Epoch: 93 [30976/54000 (57%)] Loss: -202403.593750\n",
      "Train Epoch: 93 [32384/54000 (60%)] Loss: -202785.484375\n",
      "Train Epoch: 93 [33792/54000 (63%)] Loss: -188950.421875\n",
      "Train Epoch: 93 [35200/54000 (65%)] Loss: -235477.984375\n",
      "Train Epoch: 93 [36608/54000 (68%)] Loss: -194609.531250\n",
      "Train Epoch: 93 [38016/54000 (70%)] Loss: -208185.671875\n",
      "Train Epoch: 93 [39424/54000 (73%)] Loss: -213826.718750\n",
      "Train Epoch: 93 [40832/54000 (76%)] Loss: -218610.937500\n",
      "Train Epoch: 93 [42240/54000 (78%)] Loss: -203004.187500\n",
      "Train Epoch: 93 [43648/54000 (81%)] Loss: -207088.359375\n",
      "Train Epoch: 93 [45056/54000 (83%)] Loss: -234854.484375\n",
      "Train Epoch: 93 [46464/54000 (86%)] Loss: -198416.421875\n",
      "Train Epoch: 93 [47872/54000 (89%)] Loss: -182924.109375\n",
      "Train Epoch: 93 [49280/54000 (91%)] Loss: -196427.859375\n",
      "Train Epoch: 93 [50688/54000 (94%)] Loss: -199301.218750\n",
      "Train Epoch: 93 [52096/54000 (96%)] Loss: -179801.531250\n",
      "    epoch          : 93\n",
      "    loss           : -206492.05173444975\n",
      "    val_loss       : -215310.60423018291\n",
      "Train Epoch: 94 [0/54000 (0%)] Loss: -234773.703125\n",
      "Train Epoch: 94 [1408/54000 (3%)] Loss: -210001.875000\n",
      "Train Epoch: 94 [2816/54000 (5%)] Loss: -209531.640625\n",
      "Train Epoch: 94 [4224/54000 (8%)] Loss: -194723.609375\n",
      "Train Epoch: 94 [5632/54000 (10%)] Loss: -210213.703125\n",
      "Train Epoch: 94 [7040/54000 (13%)] Loss: -198717.625000\n",
      "Train Epoch: 94 [8448/54000 (16%)] Loss: -197112.250000\n",
      "Train Epoch: 94 [9856/54000 (18%)] Loss: -208468.250000\n",
      "Train Epoch: 94 [11264/54000 (21%)] Loss: -198373.171875\n",
      "Train Epoch: 94 [12672/54000 (23%)] Loss: -204550.703125\n",
      "Train Epoch: 94 [14080/54000 (26%)] Loss: -203996.890625\n",
      "Train Epoch: 94 [15488/54000 (29%)] Loss: -214178.718750\n",
      "Train Epoch: 94 [16896/54000 (31%)] Loss: -209804.312500\n",
      "Train Epoch: 94 [18304/54000 (34%)] Loss: -210821.031250\n",
      "Train Epoch: 94 [19712/54000 (37%)] Loss: -206015.906250\n",
      "Train Epoch: 94 [21120/54000 (39%)] Loss: -208262.156250\n",
      "Train Epoch: 94 [22528/54000 (42%)] Loss: -183833.734375\n",
      "Train Epoch: 94 [23936/54000 (44%)] Loss: -200862.843750\n",
      "Train Epoch: 94 [25344/54000 (47%)] Loss: -205427.031250\n",
      "Train Epoch: 94 [26752/54000 (50%)] Loss: -208431.484375\n",
      "Train Epoch: 94 [28160/54000 (52%)] Loss: -207978.531250\n",
      "Train Epoch: 94 [29568/54000 (55%)] Loss: -213779.250000\n",
      "Train Epoch: 94 [30976/54000 (57%)] Loss: -199746.687500\n",
      "Train Epoch: 94 [32384/54000 (60%)] Loss: -184833.343750\n",
      "Train Epoch: 94 [33792/54000 (63%)] Loss: -188860.718750\n",
      "Train Epoch: 94 [35200/54000 (65%)] Loss: -193516.750000\n",
      "Train Epoch: 94 [36608/54000 (68%)] Loss: -204871.656250\n",
      "Train Epoch: 94 [38016/54000 (70%)] Loss: -192012.656250\n",
      "Train Epoch: 94 [39424/54000 (73%)] Loss: -195614.343750\n",
      "Train Epoch: 94 [40832/54000 (76%)] Loss: -229780.781250\n",
      "Train Epoch: 94 [42240/54000 (78%)] Loss: -201810.515625\n",
      "Train Epoch: 94 [43648/54000 (81%)] Loss: -212968.218750\n",
      "Train Epoch: 94 [45056/54000 (83%)] Loss: -201968.125000\n",
      "Train Epoch: 94 [46464/54000 (86%)] Loss: -221881.984375\n",
      "Train Epoch: 94 [47872/54000 (89%)] Loss: -236577.468750\n",
      "Train Epoch: 94 [49280/54000 (91%)] Loss: -214619.812500\n",
      "Train Epoch: 94 [50688/54000 (94%)] Loss: -222884.562500\n",
      "Train Epoch: 94 [52096/54000 (96%)] Loss: -216035.812500\n",
      "    epoch          : 94\n",
      "    loss           : -206073.68693929425\n",
      "    val_loss       : -216111.8376881669\n",
      "Train Epoch: 95 [0/54000 (0%)] Loss: -195892.296875\n",
      "Train Epoch: 95 [1408/54000 (3%)] Loss: -235107.781250\n",
      "Train Epoch: 95 [2816/54000 (5%)] Loss: -187618.781250\n",
      "Train Epoch: 95 [4224/54000 (8%)] Loss: -185525.718750\n",
      "Train Epoch: 95 [5632/54000 (10%)] Loss: -197547.515625\n",
      "Train Epoch: 95 [7040/54000 (13%)] Loss: -213755.578125\n",
      "Train Epoch: 95 [8448/54000 (16%)] Loss: -202651.093750\n",
      "Train Epoch: 95 [9856/54000 (18%)] Loss: -231334.218750\n",
      "Train Epoch: 95 [11264/54000 (21%)] Loss: -205300.937500\n",
      "Train Epoch: 95 [12672/54000 (23%)] Loss: -230867.812500\n",
      "Train Epoch: 95 [14080/54000 (26%)] Loss: -212411.875000\n",
      "Train Epoch: 95 [15488/54000 (29%)] Loss: -195247.953125\n",
      "Train Epoch: 95 [16896/54000 (31%)] Loss: -199442.250000\n",
      "Train Epoch: 95 [18304/54000 (34%)] Loss: -200867.296875\n",
      "Train Epoch: 95 [19712/54000 (37%)] Loss: -203526.921875\n",
      "Train Epoch: 95 [21120/54000 (39%)] Loss: -204816.906250\n",
      "Train Epoch: 95 [22528/54000 (42%)] Loss: -197249.671875\n",
      "Train Epoch: 95 [23936/54000 (44%)] Loss: -207696.812500\n",
      "Train Epoch: 95 [25344/54000 (47%)] Loss: -188571.968750\n",
      "Train Epoch: 95 [26752/54000 (50%)] Loss: -212291.765625\n",
      "Train Epoch: 95 [28160/54000 (52%)] Loss: -206453.296875\n",
      "Train Epoch: 95 [29568/54000 (55%)] Loss: -192571.781250\n",
      "Train Epoch: 95 [30976/54000 (57%)] Loss: -198842.093750\n",
      "Train Epoch: 95 [32384/54000 (60%)] Loss: -222781.343750\n",
      "Train Epoch: 95 [33792/54000 (63%)] Loss: -185706.218750\n",
      "Train Epoch: 95 [35200/54000 (65%)] Loss: -181853.453125\n",
      "Train Epoch: 95 [36608/54000 (68%)] Loss: -189824.421875\n",
      "Train Epoch: 95 [38016/54000 (70%)] Loss: -215327.765625\n",
      "Train Epoch: 95 [39424/54000 (73%)] Loss: -187509.468750\n",
      "Train Epoch: 95 [40832/54000 (76%)] Loss: -228142.609375\n",
      "Train Epoch: 95 [42240/54000 (78%)] Loss: -219058.593750\n",
      "Train Epoch: 95 [43648/54000 (81%)] Loss: -195491.000000\n",
      "Train Epoch: 95 [45056/54000 (83%)] Loss: -207686.796875\n",
      "Train Epoch: 95 [46464/54000 (86%)] Loss: -217566.734375\n",
      "Train Epoch: 95 [47872/54000 (89%)] Loss: -208067.796875\n",
      "Train Epoch: 95 [49280/54000 (91%)] Loss: -199191.562500\n",
      "Train Epoch: 95 [50688/54000 (94%)] Loss: -214122.546875\n",
      "Train Epoch: 95 [52096/54000 (96%)] Loss: -217113.250000\n",
      "    epoch          : 95\n",
      "    loss           : -206092.36670155503\n",
      "    val_loss       : -212416.79626762576\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch95.pth ...\n",
      "Train Epoch: 96 [0/54000 (0%)] Loss: -231933.703125\n",
      "Train Epoch: 96 [1408/54000 (3%)] Loss: -202600.781250\n",
      "Train Epoch: 96 [2816/54000 (5%)] Loss: -188315.500000\n",
      "Train Epoch: 96 [4224/54000 (8%)] Loss: -206171.015625\n",
      "Train Epoch: 96 [5632/54000 (10%)] Loss: -207542.359375\n",
      "Train Epoch: 96 [7040/54000 (13%)] Loss: -214402.250000\n",
      "Train Epoch: 96 [8448/54000 (16%)] Loss: -190172.468750\n",
      "Train Epoch: 96 [9856/54000 (18%)] Loss: -209197.406250\n",
      "Train Epoch: 96 [11264/54000 (21%)] Loss: -204075.906250\n",
      "Train Epoch: 96 [12672/54000 (23%)] Loss: -207925.031250\n",
      "Train Epoch: 96 [14080/54000 (26%)] Loss: -213564.625000\n",
      "Train Epoch: 96 [15488/54000 (29%)] Loss: -205644.031250\n",
      "Train Epoch: 96 [16896/54000 (31%)] Loss: -224764.484375\n",
      "Train Epoch: 96 [18304/54000 (34%)] Loss: -189249.343750\n",
      "Train Epoch: 96 [19712/54000 (37%)] Loss: -208878.218750\n",
      "Train Epoch: 96 [21120/54000 (39%)] Loss: -209328.937500\n",
      "Train Epoch: 96 [22528/54000 (42%)] Loss: -190565.546875\n",
      "Train Epoch: 96 [23936/54000 (44%)] Loss: -192069.781250\n",
      "Train Epoch: 96 [25344/54000 (47%)] Loss: -203808.156250\n",
      "Train Epoch: 96 [26752/54000 (50%)] Loss: -214122.578125\n",
      "Train Epoch: 96 [28160/54000 (52%)] Loss: -219635.343750\n",
      "Train Epoch: 96 [29568/54000 (55%)] Loss: -206704.484375\n",
      "Train Epoch: 96 [30976/54000 (57%)] Loss: -214777.390625\n",
      "Train Epoch: 96 [32384/54000 (60%)] Loss: -208204.562500\n",
      "Train Epoch: 96 [33792/54000 (63%)] Loss: -205947.015625\n",
      "Train Epoch: 96 [35200/54000 (65%)] Loss: -183421.890625\n",
      "Train Epoch: 96 [36608/54000 (68%)] Loss: -213953.718750\n",
      "Train Epoch: 96 [38016/54000 (70%)] Loss: -208606.312500\n",
      "Train Epoch: 96 [39424/54000 (73%)] Loss: -199445.031250\n",
      "Train Epoch: 96 [40832/54000 (76%)] Loss: -196601.625000\n",
      "Train Epoch: 96 [42240/54000 (78%)] Loss: -216632.265625\n",
      "Train Epoch: 96 [43648/54000 (81%)] Loss: -222061.687500\n",
      "Train Epoch: 96 [45056/54000 (83%)] Loss: -192759.343750\n",
      "Train Epoch: 96 [46464/54000 (86%)] Loss: -209611.000000\n",
      "Train Epoch: 96 [47872/54000 (89%)] Loss: -198556.968750\n",
      "Train Epoch: 96 [49280/54000 (91%)] Loss: -204313.453125\n",
      "Train Epoch: 96 [50688/54000 (94%)] Loss: -203907.406250\n",
      "Train Epoch: 96 [52096/54000 (96%)] Loss: -233753.140625\n",
      "    epoch          : 96\n",
      "    loss           : -206865.81268690192\n",
      "    val_loss       : -213562.53724037728\n",
      "Train Epoch: 97 [0/54000 (0%)] Loss: -189028.281250\n",
      "Train Epoch: 97 [1408/54000 (3%)] Loss: -191089.250000\n",
      "Train Epoch: 97 [2816/54000 (5%)] Loss: -214067.796875\n",
      "Train Epoch: 97 [4224/54000 (8%)] Loss: -188124.625000\n",
      "Train Epoch: 97 [5632/54000 (10%)] Loss: -203365.968750\n",
      "Train Epoch: 97 [7040/54000 (13%)] Loss: -212220.312500\n",
      "Train Epoch: 97 [8448/54000 (16%)] Loss: -204311.968750\n",
      "Train Epoch: 97 [9856/54000 (18%)] Loss: -212687.250000\n",
      "Train Epoch: 97 [11264/54000 (21%)] Loss: -200758.890625\n",
      "Train Epoch: 97 [12672/54000 (23%)] Loss: -208023.078125\n",
      "Train Epoch: 97 [14080/54000 (26%)] Loss: -202759.984375\n",
      "Train Epoch: 97 [15488/54000 (29%)] Loss: -225257.890625\n",
      "Train Epoch: 97 [16896/54000 (31%)] Loss: -194874.437500\n",
      "Train Epoch: 97 [18304/54000 (34%)] Loss: -204828.968750\n",
      "Train Epoch: 97 [19712/54000 (37%)] Loss: -211732.562500\n",
      "Train Epoch: 97 [21120/54000 (39%)] Loss: -236852.515625\n",
      "Train Epoch: 97 [22528/54000 (42%)] Loss: -208794.203125\n",
      "Train Epoch: 97 [23936/54000 (44%)] Loss: -236878.812500\n",
      "Train Epoch: 97 [25344/54000 (47%)] Loss: -203403.843750\n",
      "Train Epoch: 97 [26752/54000 (50%)] Loss: -209348.296875\n",
      "Train Epoch: 97 [28160/54000 (52%)] Loss: -210741.500000\n",
      "Train Epoch: 97 [29568/54000 (55%)] Loss: -198155.468750\n",
      "Train Epoch: 97 [30976/54000 (57%)] Loss: -206629.671875\n",
      "Train Epoch: 97 [32384/54000 (60%)] Loss: -199949.593750\n",
      "Train Epoch: 97 [33792/54000 (63%)] Loss: -227961.109375\n",
      "Train Epoch: 97 [35200/54000 (65%)] Loss: -198643.171875\n",
      "Train Epoch: 97 [36608/54000 (68%)] Loss: -209441.218750\n",
      "Train Epoch: 97 [38016/54000 (70%)] Loss: -189105.250000\n",
      "Train Epoch: 97 [39424/54000 (73%)] Loss: -236558.859375\n",
      "Train Epoch: 97 [40832/54000 (76%)] Loss: -200072.796875\n",
      "Train Epoch: 97 [42240/54000 (78%)] Loss: -214975.875000\n",
      "Train Epoch: 97 [43648/54000 (81%)] Loss: -207902.140625\n",
      "Train Epoch: 97 [45056/54000 (83%)] Loss: -226485.093750\n",
      "Train Epoch: 97 [46464/54000 (86%)] Loss: -210295.953125\n",
      "Train Epoch: 97 [47872/54000 (89%)] Loss: -215989.234375\n",
      "Train Epoch: 97 [49280/54000 (91%)] Loss: -209521.593750\n",
      "Train Epoch: 97 [50688/54000 (94%)] Loss: -205809.828125\n",
      "Train Epoch: 97 [52096/54000 (96%)] Loss: -236378.125000\n",
      "    epoch          : 97\n",
      "    loss           : -206646.40759569377\n",
      "    val_loss       : -214027.6855111471\n",
      "Train Epoch: 98 [0/54000 (0%)] Loss: -235779.578125\n",
      "Train Epoch: 98 [1408/54000 (3%)] Loss: -198204.328125\n",
      "Train Epoch: 98 [2816/54000 (5%)] Loss: -213284.421875\n",
      "Train Epoch: 98 [4224/54000 (8%)] Loss: -208151.281250\n",
      "Train Epoch: 98 [5632/54000 (10%)] Loss: -189654.640625\n",
      "Train Epoch: 98 [7040/54000 (13%)] Loss: -235562.375000\n",
      "Train Epoch: 98 [8448/54000 (16%)] Loss: -222885.796875\n",
      "Train Epoch: 98 [9856/54000 (18%)] Loss: -189712.359375\n",
      "Train Epoch: 98 [11264/54000 (21%)] Loss: -198088.906250\n",
      "Train Epoch: 98 [12672/54000 (23%)] Loss: -220588.234375\n",
      "Train Epoch: 98 [14080/54000 (26%)] Loss: -180763.125000\n",
      "Train Epoch: 98 [15488/54000 (29%)] Loss: -207310.906250\n",
      "Train Epoch: 98 [16896/54000 (31%)] Loss: -213914.109375\n",
      "Train Epoch: 98 [18304/54000 (34%)] Loss: -235894.515625\n",
      "Train Epoch: 98 [19712/54000 (37%)] Loss: -197895.437500\n",
      "Train Epoch: 98 [21120/54000 (39%)] Loss: -197147.093750\n",
      "Train Epoch: 98 [22528/54000 (42%)] Loss: -208946.406250\n",
      "Train Epoch: 98 [23936/54000 (44%)] Loss: -207455.656250\n",
      "Train Epoch: 98 [25344/54000 (47%)] Loss: -210515.781250\n",
      "Train Epoch: 98 [26752/54000 (50%)] Loss: -214653.875000\n",
      "Train Epoch: 98 [28160/54000 (52%)] Loss: -199910.218750\n",
      "Train Epoch: 98 [29568/54000 (55%)] Loss: -180176.875000\n",
      "Train Epoch: 98 [30976/54000 (57%)] Loss: -234463.000000\n",
      "Train Epoch: 98 [32384/54000 (60%)] Loss: -205290.750000\n",
      "Train Epoch: 98 [33792/54000 (63%)] Loss: -218786.375000\n",
      "Train Epoch: 98 [35200/54000 (65%)] Loss: -209553.187500\n",
      "Train Epoch: 98 [36608/54000 (68%)] Loss: -235328.390625\n",
      "Train Epoch: 98 [38016/54000 (70%)] Loss: -220649.984375\n",
      "Train Epoch: 98 [39424/54000 (73%)] Loss: -200744.343750\n",
      "Train Epoch: 98 [40832/54000 (76%)] Loss: -201689.500000\n",
      "Train Epoch: 98 [42240/54000 (78%)] Loss: -179018.750000\n",
      "Train Epoch: 98 [43648/54000 (81%)] Loss: -209791.312500\n",
      "Train Epoch: 98 [45056/54000 (83%)] Loss: -212516.015625\n",
      "Train Epoch: 98 [46464/54000 (86%)] Loss: -188897.046875\n",
      "Train Epoch: 98 [47872/54000 (89%)] Loss: -207047.203125\n",
      "Train Epoch: 98 [49280/54000 (91%)] Loss: -195056.093750\n",
      "Train Epoch: 98 [50688/54000 (94%)] Loss: -208226.750000\n",
      "Train Epoch: 98 [52096/54000 (96%)] Loss: -236550.265625\n",
      "    epoch          : 98\n",
      "    loss           : -206597.63266297846\n",
      "    val_loss       : -214494.8680568788\n",
      "Train Epoch: 99 [0/54000 (0%)] Loss: -236418.453125\n",
      "Train Epoch: 99 [1408/54000 (3%)] Loss: -207503.265625\n",
      "Train Epoch: 99 [2816/54000 (5%)] Loss: -202747.359375\n",
      "Train Epoch: 99 [4224/54000 (8%)] Loss: -216186.218750\n",
      "Train Epoch: 99 [5632/54000 (10%)] Loss: -193350.296875\n",
      "Train Epoch: 99 [7040/54000 (13%)] Loss: -204822.250000\n",
      "Train Epoch: 99 [8448/54000 (16%)] Loss: -236500.250000\n",
      "Train Epoch: 99 [9856/54000 (18%)] Loss: -190422.500000\n",
      "Train Epoch: 99 [11264/54000 (21%)] Loss: -210765.765625\n",
      "Train Epoch: 99 [12672/54000 (23%)] Loss: -187022.000000\n",
      "Train Epoch: 99 [14080/54000 (26%)] Loss: -203307.812500\n",
      "Train Epoch: 99 [15488/54000 (29%)] Loss: -194518.546875\n",
      "Train Epoch: 99 [16896/54000 (31%)] Loss: -203972.687500\n",
      "Train Epoch: 99 [18304/54000 (34%)] Loss: -207323.546875\n",
      "Train Epoch: 99 [19712/54000 (37%)] Loss: -224185.156250\n",
      "Train Epoch: 99 [21120/54000 (39%)] Loss: -201845.796875\n",
      "Train Epoch: 99 [22528/54000 (42%)] Loss: -216444.031250\n",
      "Train Epoch: 99 [23936/54000 (44%)] Loss: -207531.921875\n",
      "Train Epoch: 99 [25344/54000 (47%)] Loss: -187525.312500\n",
      "Train Epoch: 99 [26752/54000 (50%)] Loss: -208455.781250\n",
      "Train Epoch: 99 [28160/54000 (52%)] Loss: -234892.062500\n",
      "Train Epoch: 99 [29568/54000 (55%)] Loss: -183084.687500\n",
      "Train Epoch: 99 [30976/54000 (57%)] Loss: -234275.906250\n",
      "Train Epoch: 99 [32384/54000 (60%)] Loss: -197077.625000\n",
      "Train Epoch: 99 [33792/54000 (63%)] Loss: -201983.625000\n",
      "Train Epoch: 99 [35200/54000 (65%)] Loss: -211296.125000\n",
      "Train Epoch: 99 [36608/54000 (68%)] Loss: -193216.328125\n",
      "Train Epoch: 99 [38016/54000 (70%)] Loss: -198128.828125\n",
      "Train Epoch: 99 [39424/54000 (73%)] Loss: -194847.593750\n",
      "Train Epoch: 99 [40832/54000 (76%)] Loss: -208770.875000\n",
      "Train Epoch: 99 [42240/54000 (78%)] Loss: -221269.906250\n",
      "Train Epoch: 99 [43648/54000 (81%)] Loss: -184826.125000\n",
      "Train Epoch: 99 [45056/54000 (83%)] Loss: -230929.125000\n",
      "Train Epoch: 99 [46464/54000 (86%)] Loss: -237147.296875\n",
      "Train Epoch: 99 [47872/54000 (89%)] Loss: -207953.406250\n",
      "Train Epoch: 99 [49280/54000 (91%)] Loss: -205620.328125\n",
      "Train Epoch: 99 [50688/54000 (94%)] Loss: -193617.468750\n",
      "Train Epoch: 99 [52096/54000 (96%)] Loss: -184508.093750\n",
      "    epoch          : 99\n",
      "    loss           : -206805.12634569377\n",
      "    val_loss       : -214239.54345703125\n",
      "Train Epoch: 100 [0/54000 (0%)] Loss: -209902.328125\n",
      "Train Epoch: 100 [1408/54000 (3%)] Loss: -209082.390625\n",
      "Train Epoch: 100 [2816/54000 (5%)] Loss: -190209.812500\n",
      "Train Epoch: 100 [4224/54000 (8%)] Loss: -212642.671875\n",
      "Train Epoch: 100 [5632/54000 (10%)] Loss: -207897.343750\n",
      "Train Epoch: 100 [7040/54000 (13%)] Loss: -227922.078125\n",
      "Train Epoch: 100 [8448/54000 (16%)] Loss: -188713.500000\n",
      "Train Epoch: 100 [9856/54000 (18%)] Loss: -206857.406250\n",
      "Train Epoch: 100 [11264/54000 (21%)] Loss: -208782.328125\n",
      "Train Epoch: 100 [12672/54000 (23%)] Loss: -199333.578125\n",
      "Train Epoch: 100 [14080/54000 (26%)] Loss: -183274.937500\n",
      "Train Epoch: 100 [15488/54000 (29%)] Loss: -214891.906250\n",
      "Train Epoch: 100 [16896/54000 (31%)] Loss: -219558.281250\n",
      "Train Epoch: 100 [18304/54000 (34%)] Loss: -236923.687500\n",
      "Train Epoch: 100 [19712/54000 (37%)] Loss: -200728.031250\n",
      "Train Epoch: 100 [21120/54000 (39%)] Loss: -203192.812500\n",
      "Train Epoch: 100 [22528/54000 (42%)] Loss: -214648.468750\n",
      "Train Epoch: 100 [23936/54000 (44%)] Loss: -227292.984375\n",
      "Train Epoch: 100 [25344/54000 (47%)] Loss: -199162.906250\n",
      "Train Epoch: 100 [26752/54000 (50%)] Loss: -204253.625000\n",
      "Train Epoch: 100 [28160/54000 (52%)] Loss: -214825.593750\n",
      "Train Epoch: 100 [29568/54000 (55%)] Loss: -207919.031250\n",
      "Train Epoch: 100 [30976/54000 (57%)] Loss: -207907.843750\n",
      "Train Epoch: 100 [32384/54000 (60%)] Loss: -207876.171875\n",
      "Train Epoch: 100 [33792/54000 (63%)] Loss: -209194.187500\n",
      "Train Epoch: 100 [35200/54000 (65%)] Loss: -215193.218750\n",
      "Train Epoch: 100 [36608/54000 (68%)] Loss: -221155.875000\n",
      "Train Epoch: 100 [38016/54000 (70%)] Loss: -237439.312500\n",
      "Train Epoch: 100 [39424/54000 (73%)] Loss: -195938.296875\n",
      "Train Epoch: 100 [40832/54000 (76%)] Loss: -207743.375000\n",
      "Train Epoch: 100 [42240/54000 (78%)] Loss: -203652.765625\n",
      "Train Epoch: 100 [43648/54000 (81%)] Loss: -228123.562500\n",
      "Train Epoch: 100 [45056/54000 (83%)] Loss: -213317.156250\n",
      "Train Epoch: 100 [46464/54000 (86%)] Loss: -213403.859375\n",
      "Train Epoch: 100 [47872/54000 (89%)] Loss: -206717.265625\n",
      "Train Epoch: 100 [49280/54000 (91%)] Loss: -208650.828125\n",
      "Train Epoch: 100 [50688/54000 (94%)] Loss: -205818.390625\n",
      "Train Epoch: 100 [52096/54000 (96%)] Loss: -206394.890625\n",
      "    epoch          : 100\n",
      "    loss           : -207415.40202601676\n",
      "    val_loss       : -212265.41457459985\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0428_202427/checkpoint-epoch100.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
