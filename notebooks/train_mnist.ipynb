{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"factor\": 0.5,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 55811.554688\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -569759.062500\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -662986.312500\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -717169.562500\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -697024.375000\n",
      "    epoch          : 1\n",
      "    loss           : -584063.0531694771\n",
      "    val_loss       : -679937.4578125\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -780140.937500\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -673355.562500\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -691738.625000\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -630032.250000\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -685911.312500\n",
      "    epoch          : 2\n",
      "    loss           : -691226.8316831683\n",
      "    val_loss       : -701330.78515625\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -828208.500000\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -743295.875000\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -733940.562500\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -680612.500000\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -627360.312500\n",
      "    epoch          : 3\n",
      "    loss           : -699308.948019802\n",
      "    val_loss       : -705387.19609375\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -834089.000000\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -721793.000000\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -644340.875000\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -705752.437500\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -684794.000000\n",
      "    epoch          : 4\n",
      "    loss           : -706818.5767326732\n",
      "    val_loss       : -709127.52734375\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -844379.562500\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -732220.250000\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -638916.000000\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -694290.750000\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -714957.375000\n",
      "    epoch          : 5\n",
      "    loss           : -713155.0792079208\n",
      "    val_loss       : -717969.65078125\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -852921.375000\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -684191.187500\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -659485.375000\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -765681.250000\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -719683.750000\n",
      "    epoch          : 6\n",
      "    loss           : -719465.5655940594\n",
      "    val_loss       : -725158.965625\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -852303.875000\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -700713.375000\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -725948.375000\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -655416.500000\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -730119.937500\n",
      "    epoch          : 7\n",
      "    loss           : -728307.9461633663\n",
      "    val_loss       : -737753.52109375\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -867099.375000\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -704581.375000\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -750400.625000\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -751365.125000\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -683394.375000\n",
      "    epoch          : 8\n",
      "    loss           : -743058.7939356435\n",
      "    val_loss       : -748594.97265625\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -883371.437500\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -716270.000000\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -722384.687500\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -757827.437500\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -728474.250000\n",
      "    epoch          : 9\n",
      "    loss           : -754778.8199257426\n",
      "    val_loss       : -760150.09453125\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -912465.750000\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -728200.500000\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -714336.125000\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -692509.750000\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -774054.062500\n",
      "    epoch          : 10\n",
      "    loss           : -764457.1534653465\n",
      "    val_loss       : -763055.66796875\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -912101.250000\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -733701.500000\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -821804.312500\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -927651.500000\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -746286.437500\n",
      "    epoch          : 11\n",
      "    loss           : -770515.0488861386\n",
      "    val_loss       : -769967.771875\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -913600.875000\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -742107.812500\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -719225.625000\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -744400.062500\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -741909.500000\n",
      "    epoch          : 12\n",
      "    loss           : -775635.2407178218\n",
      "    val_loss       : -780086.19765625\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -932630.750000\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -740945.875000\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -739935.937500\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -820326.625000\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -786529.437500\n",
      "    epoch          : 13\n",
      "    loss           : -784994.4028465346\n",
      "    val_loss       : -790180.603125\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -833740.937500\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -831590.062500\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -766560.687500\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -765790.687500\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -794324.187500\n",
      "    epoch          : 14\n",
      "    loss           : -794190.5513613861\n",
      "    val_loss       : -793655.796875\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -951352.875000\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -773838.625000\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -737389.875000\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -766425.125000\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -761956.250000\n",
      "    epoch          : 15\n",
      "    loss           : -800171.3199257426\n",
      "    val_loss       : -809302.7734375\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -952703.000000\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -783127.250000\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -836729.437500\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -790793.875000\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -770923.562500\n",
      "    epoch          : 16\n",
      "    loss           : -809235.9158415842\n",
      "    val_loss       : -811453.67421875\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -779817.250000\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -848194.000000\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -842217.687500\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -791924.875000\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -774408.375000\n",
      "    epoch          : 17\n",
      "    loss           : -808111.1336633663\n",
      "    val_loss       : -810086.74609375\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -957452.625000\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -790655.625000\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -784173.812500\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -959262.812500\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -817632.750000\n",
      "    epoch          : 18\n",
      "    loss           : -819658.0674504951\n",
      "    val_loss       : -825376.89375\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -966082.312500\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -803246.875000\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -799877.750000\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -821650.375000\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -785985.750000\n",
      "    epoch          : 19\n",
      "    loss           : -824139.7202970297\n",
      "    val_loss       : -824243.04375\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -964768.437500\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -790552.250000\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -796035.250000\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -769739.687500\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -815073.187500\n",
      "    epoch          : 20\n",
      "    loss           : -830552.9436881188\n",
      "    val_loss       : -831549.09609375\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -968976.000000\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -809701.937500\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -776266.062500\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -814977.000000\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -822611.375000\n",
      "    epoch          : 21\n",
      "    loss           : -832004.0228960396\n",
      "    val_loss       : -833726.25859375\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -967589.000000\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -815707.875000\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -804225.375000\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -830045.250000\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -832655.062500\n",
      "    epoch          : 22\n",
      "    loss           : -836578.6868811881\n",
      "    val_loss       : -838781.57109375\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -882004.312500\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -811545.375000\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -818967.437500\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -866457.125000\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -835179.062500\n",
      "    epoch          : 23\n",
      "    loss           : -844426.5977722772\n",
      "    val_loss       : -842295.48359375\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -979791.562500\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -812849.875000\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -873093.125000\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -864838.625000\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -822372.625000\n",
      "    epoch          : 24\n",
      "    loss           : -845406.5476485149\n",
      "    val_loss       : -848505.59765625\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -980288.125000\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -891622.312500\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -801913.375000\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -874885.875000\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -803904.687500\n",
      "    epoch          : 25\n",
      "    loss           : -854797.7147277228\n",
      "    val_loss       : -850193.465625\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -982273.687500\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -833762.750000\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -812728.437500\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -988076.062500\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -844911.687500\n",
      "    epoch          : 26\n",
      "    loss           : -855209.7852722772\n",
      "    val_loss       : -855055.71640625\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -982861.500000\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -842385.750000\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -885841.562500\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -852548.000000\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -853596.812500\n",
      "    epoch          : 27\n",
      "    loss           : -862330.5767326732\n",
      "    val_loss       : -859008.7796875\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -985894.312500\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -840193.125000\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -893170.187500\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -812369.375000\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -854053.062500\n",
      "    epoch          : 28\n",
      "    loss           : -866413.2772277228\n",
      "    val_loss       : -868753.35078125\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -991612.750000\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -843703.375000\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -855178.875000\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -837228.125000\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -819759.125000\n",
      "    epoch          : 29\n",
      "    loss           : -867958.8168316832\n",
      "    val_loss       : -866137.09765625\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -903377.812500\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -852280.625000\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -827041.062500\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -841996.062500\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -822648.125000\n",
      "    epoch          : 30\n",
      "    loss           : -870923.4523514851\n",
      "    val_loss       : -873608.99921875\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -993202.312500\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -908428.875000\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -899482.312500\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -900048.625000\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -878054.562500\n",
      "    epoch          : 31\n",
      "    loss           : -878723.208539604\n",
      "    val_loss       : -877566.7546875\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -999400.312500\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -851317.625000\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -860300.000000\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -857226.875000\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -833205.000000\n",
      "    epoch          : 32\n",
      "    loss           : -878050.4634900991\n",
      "    val_loss       : -877892.1953125\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -828730.625000\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -914170.375000\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -829566.500000\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -992730.875000\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -825985.062500\n",
      "    epoch          : 33\n",
      "    loss           : -879404.2852722772\n",
      "    val_loss       : -882301.584375\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -1000054.750000\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -887034.500000\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -842686.125000\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -858266.000000\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -828927.500000\n",
      "    epoch          : 34\n",
      "    loss           : -887710.8038366337\n",
      "    val_loss       : -882866.490625\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -1001718.437500\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -860047.000000\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -828249.125000\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -854864.625000\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -844002.375000\n",
      "    epoch          : 35\n",
      "    loss           : -882447.7611386139\n",
      "    val_loss       : -889535.53359375\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -1001127.875000\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -865935.312500\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -910756.500000\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -890602.750000\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -879291.062500\n",
      "    epoch          : 36\n",
      "    loss           : -890668.3310643565\n",
      "    val_loss       : -886145.59453125\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -922930.000000\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -866894.062500\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -847390.437500\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -1005794.250000\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -890989.062500\n",
      "    epoch          : 37\n",
      "    loss           : -893454.6206683168\n",
      "    val_loss       : -885277.98359375\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -1000238.000000\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -874368.562500\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -851278.562500\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -902104.437500\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -889543.625000\n",
      "    epoch          : 38\n",
      "    loss           : -893212.7537128713\n",
      "    val_loss       : -894914.01328125\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -1004715.812500\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -850995.125000\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -851758.625000\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -877212.687500\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -892012.187500\n",
      "    epoch          : 39\n",
      "    loss           : -896950.4474009901\n",
      "    val_loss       : -899125.4\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -1009220.812500\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -871895.125000\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -876795.500000\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -914825.687500\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -886264.000000\n",
      "    epoch          : 40\n",
      "    loss           : -896396.7827970297\n",
      "    val_loss       : -897878.4796875\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -1006109.250000\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -876896.125000\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -857088.750000\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -1010282.000000\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -896431.250000\n",
      "    epoch          : 41\n",
      "    loss           : -901568.9981435643\n",
      "    val_loss       : -900638.66484375\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -1005757.375000\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -882052.687500\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -857451.000000\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -880776.875000\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -895958.437500\n",
      "    epoch          : 42\n",
      "    loss           : -901034.5457920792\n",
      "    val_loss       : -901871.13125\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -877043.125000\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -932340.812500\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -857669.812500\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -918630.062500\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -893731.000000\n",
      "    epoch          : 43\n",
      "    loss           : -902894.6992574257\n",
      "    val_loss       : -900029.9375\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -1010294.000000\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -931328.625000\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -885927.687500\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -897557.437500\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -922174.000000\n",
      "    epoch          : 44\n",
      "    loss           : -903307.8131188119\n",
      "    val_loss       : -901331.54609375\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -1006719.625000\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -880401.000000\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -902779.312500\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -906793.000000\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -899013.687500\n",
      "    epoch          : 45\n",
      "    loss           : -905499.1955445545\n",
      "    val_loss       : -904861.10625\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -902239.625000\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -884425.687500\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -899405.437500\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -917378.437500\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -903540.125000\n",
      "    epoch          : 46\n",
      "    loss           : -906977.4275990099\n",
      "    val_loss       : -906364.24375\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -1011536.625000\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -884549.562500\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -888032.250000\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -891165.000000\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -883267.562500\n",
      "    epoch          : 47\n",
      "    loss           : -907318.176980198\n",
      "    val_loss       : -904216.52734375\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -1011815.875000\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -893543.875000\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -892577.312500\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -908582.062500\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -905350.875000\n",
      "    epoch          : 48\n",
      "    loss           : -908923.8069306931\n",
      "    val_loss       : -908228.5359375\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -1013370.750000\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -943026.500000\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -906837.500000\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -1014305.500000\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -911300.437500\n",
      "    epoch          : 49\n",
      "    loss           : -911940.1225247525\n",
      "    val_loss       : -905393.25703125\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -1011659.125000\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -884531.562500\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -922932.500000\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -1007764.312500\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -898048.375000\n",
      "    epoch          : 50\n",
      "    loss           : -906641.396039604\n",
      "    val_loss       : -908553.67109375\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0806_094159/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -1010228.687500\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -895379.625000\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -914572.125000\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -874903.125000\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -868614.000000\n",
      "    epoch          : 51\n",
      "    loss           : -912174.823019802\n",
      "    val_loss       : -905733.85078125\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -1006908.750000\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -889349.375000\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -930692.937500\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -870801.000000\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -865582.812500\n",
      "    epoch          : 52\n",
      "    loss           : -913165.0853960396\n",
      "    val_loss       : -913207.83359375\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -889387.187500\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -946779.812500\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -898588.250000\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -931658.187500\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -867672.750000\n",
      "    epoch          : 53\n",
      "    loss           : -913179.7271039604\n",
      "    val_loss       : -907481.08359375\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -1013214.750000\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -937646.312500\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -876186.937500\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -900174.500000\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -874363.125000\n",
      "    epoch          : 54\n",
      "    loss           : -914645.5030940594\n",
      "    val_loss       : -917156.23203125\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -1015927.562500\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -898183.562500\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -933995.937500\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -935337.937500\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -917460.625000\n",
      "    epoch          : 55\n",
      "    loss           : -917967.396039604\n",
      "    val_loss       : -910447.8078125\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -944383.125000\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -885754.187500\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -910605.125000\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -1014053.375000\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -913249.375000\n",
      "    epoch          : 56\n",
      "    loss           : -915391.1856435643\n",
      "    val_loss       : -918909.63828125\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -1014653.625000\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -898779.312500\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -935510.625000\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -919500.625000\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -931398.500000\n",
      "    epoch          : 57\n",
      "    loss           : -918551.468440594\n",
      "    val_loss       : -911645.5296875\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -1012689.625000\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -898616.250000\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -918463.750000\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -931746.562500\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -903080.187500\n",
      "    epoch          : 58\n",
      "    loss           : -917794.4047029703\n",
      "    val_loss       : -918473.25859375\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -1014556.437500\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -903985.000000\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -932878.500000\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -919090.000000\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -921804.625000\n",
      "    epoch          : 59\n",
      "    loss           : -919851.4907178218\n",
      "    val_loss       : -919378.00078125\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -1016072.625000\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -900069.625000\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -899204.062500\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -1014228.625000\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -921064.062500\n",
      "    epoch          : 60\n",
      "    loss           : -920807.0860148515\n",
      "    val_loss       : -919615.33203125\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -1017778.500000\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -945019.062500\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -929107.375000\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -1011498.625000\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -879488.937500\n",
      "    epoch          : 61\n",
      "    loss           : -919885.3050742574\n",
      "    val_loss       : -920907.959375\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -926702.312500\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -902228.625000\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -890139.000000\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -877260.750000\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -914940.937500\n",
      "    epoch          : 62\n",
      "    loss           : -920838.1163366337\n",
      "    val_loss       : -914798.6390625\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -1013931.750000\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -898508.562500\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -884776.750000\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -902282.937500\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -924588.687500\n",
      "    epoch          : 63\n",
      "    loss           : -922909.2425742574\n",
      "    val_loss       : -922755.94453125\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -954748.125000\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -953086.375000\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -917884.625000\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -884173.687500\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -922571.437500\n",
      "    epoch          : 64\n",
      "    loss           : -923116.0086633663\n",
      "    val_loss       : -917425.6578125\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -1016074.250000\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -900540.187500\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -909040.500000\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -919548.187500\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -878602.812500\n",
      "    epoch          : 65\n",
      "    loss           : -922380.4764851485\n",
      "    val_loss       : -921889.2265625\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -1015051.625000\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -903234.625000\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -938623.625000\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -914401.000000\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -919821.000000\n",
      "    epoch          : 66\n",
      "    loss           : -922761.9232673268\n",
      "    val_loss       : -921441.659375\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -1018167.250000\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -898165.500000\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -920656.375000\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -926664.687500\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -928859.937500\n",
      "    epoch          : 67\n",
      "    loss           : -926036.8316831683\n",
      "    val_loss       : -923341.7375\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -1018863.500000\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -951763.000000\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -891050.125000\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -926900.062500\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -917614.000000\n",
      "    epoch          : 68\n",
      "    loss           : -926807.457920792\n",
      "    val_loss       : -925128.28125\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -1017678.875000\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -890096.750000\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -942457.125000\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -923854.000000\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -922326.500000\n",
      "    epoch          : 69\n",
      "    loss           : -928466.7803217822\n",
      "    val_loss       : -925688.50078125\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -1018988.000000\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -890939.437500\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -918345.000000\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -933961.000000\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -877505.625000\n",
      "    epoch          : 70\n",
      "    loss           : -925437.5544554455\n",
      "    val_loss       : -917662.91328125\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -1015089.312500\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -901892.500000\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -883878.500000\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -937945.000000\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -916888.125000\n",
      "    epoch          : 71\n",
      "    loss           : -922898.6373762377\n",
      "    val_loss       : -922349.56953125\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -897837.375000\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -955330.125000\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -940214.500000\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -917762.000000\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -924542.500000\n",
      "    epoch          : 72\n",
      "    loss           : -923472.8050742574\n",
      "    val_loss       : -922444.39140625\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -1016987.312500\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -905863.375000\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -891857.937500\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -932019.812500\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -928217.750000\n",
      "    epoch          : 73\n",
      "    loss           : -926921.2469059406\n",
      "    val_loss       : -926387.18671875\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -1018152.812500\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -925787.562500\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -896592.375000\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -941064.250000\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -922324.625000\n",
      "    epoch          : 74\n",
      "    loss           : -930890.7877475248\n",
      "    val_loss       : -928891.74296875\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -1019395.812500\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -913080.875000\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -943978.625000\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -1014194.250000\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -923452.000000\n",
      "    epoch          : 75\n",
      "    loss           : -930532.4034653465\n",
      "    val_loss       : -928457.60546875\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -1016902.250000\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -910709.750000\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -896554.062500\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -929906.375000\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -933040.062500\n",
      "    epoch          : 76\n",
      "    loss           : -929840.510519802\n",
      "    val_loss       : -924061.1859375\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -1017607.625000\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -888875.875000\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -944103.312500\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -944671.312500\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -889897.000000\n",
      "    epoch          : 77\n",
      "    loss           : -928470.5303217822\n",
      "    val_loss       : -929097.565625\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -1019442.750000\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -911535.625000\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -944401.875000\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -912536.562500\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -891080.437500\n",
      "    epoch          : 78\n",
      "    loss           : -932111.9306930694\n",
      "    val_loss       : -930106.165625\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -915789.625000\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -914754.750000\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -942249.375000\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -929438.062500\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -927118.937500\n",
      "    epoch          : 79\n",
      "    loss           : -927187.3743811881\n",
      "    val_loss       : -921407.01875\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -1017323.250000\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -895092.437500\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -894569.187500\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -942247.187500\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -927478.500000\n",
      "    epoch          : 80\n",
      "    loss           : -929436.1676980198\n",
      "    val_loss       : -931161.77109375\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -1021461.125000\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -917994.812500\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -927368.500000\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -896076.750000\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -926975.187500\n",
      "    epoch          : 81\n",
      "    loss           : -933297.9492574257\n",
      "    val_loss       : -928347.0578125\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -1017946.875000\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -895839.437500\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -905411.000000\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -892384.937500\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -934489.250000\n",
      "    epoch          : 82\n",
      "    loss           : -928967.4127475248\n",
      "    val_loss       : -930275.54140625\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -1020559.812500\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -944462.625000\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -892823.000000\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -926884.187500\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -892751.250000\n",
      "    epoch          : 83\n",
      "    loss           : -933630.9870049505\n",
      "    val_loss       : -930845.7890625\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -957955.750000\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -899178.500000\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -945111.062500\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -925968.000000\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -894303.750000\n",
      "    epoch          : 84\n",
      "    loss           : -933225.4870049505\n",
      "    val_loss       : -931161.28984375\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -1018787.437500\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -916147.000000\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -904730.937500\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -914459.750000\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -898075.437500\n",
      "    epoch          : 85\n",
      "    loss           : -933863.5259900991\n",
      "    val_loss       : -930122.90625\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -1019310.375000\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -917192.062500\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -922199.187500\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -926900.125000\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -937687.000000\n",
      "    epoch          : 86\n",
      "    loss           : -933002.4096534654\n",
      "    val_loss       : -931773.778125\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -1021476.687500\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -935953.187500\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -945686.562500\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -894383.625000\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -893429.875000\n",
      "    epoch          : 87\n",
      "    loss           : -932149.7747524752\n",
      "    val_loss       : -931143.04296875\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -1018603.687500\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -913986.000000\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -901455.562500\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -896286.500000\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -930652.625000\n",
      "    epoch          : 88\n",
      "    loss           : -933757.7902227723\n",
      "    val_loss       : -932715.0859375\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -1020210.437500\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -948087.187500\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -897321.687500\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -893973.125000\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -894945.250000\n",
      "    epoch          : 89\n",
      "    loss           : -934088.9900990099\n",
      "    val_loss       : -933338.4734375\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -1021231.562500\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -898555.187500\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -943274.437500\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -933893.250000\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -929115.000000\n",
      "    epoch          : 90\n",
      "    loss           : -933358.5068069306\n",
      "    val_loss       : -931022.55078125\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -1019917.125000\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -916591.062500\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -897093.000000\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -893709.125000\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -896957.250000\n",
      "    epoch          : 91\n",
      "    loss           : -930521.417079208\n",
      "    val_loss       : -928953.7171875\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -1017451.250000\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -917868.875000\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -931097.250000\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -946876.625000\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -915857.625000\n",
      "    epoch          : 92\n",
      "    loss           : -935333.4251237623\n",
      "    val_loss       : -931836.21328125\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -1018721.687500\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -913860.125000\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -931633.125000\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -936750.250000\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -931363.812500\n",
      "    epoch          : 93\n",
      "    loss           : -934456.3409653465\n",
      "    val_loss       : -932526.25546875\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -1019187.687500\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -957638.125000\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -914266.375000\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -946229.437500\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -898467.125000\n",
      "    epoch          : 94\n",
      "    loss           : -936101.198019802\n",
      "    val_loss       : -934763.74453125\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -937588.000000\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -904757.562500\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -937228.750000\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -939747.500000\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -913325.812500\n",
      "    epoch          : 95\n",
      "    loss           : -936251.7134900991\n",
      "    val_loss       : -932891.78984375\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -1018855.187500\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -928559.250000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -927935.375000\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -917590.000000\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -913641.500000\n",
      "    epoch          : 96\n",
      "    loss           : -936151.8737623763\n",
      "    val_loss       : -934720.871875\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -1021308.250000\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -959123.625000\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -902730.687500\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -936636.250000\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -897470.750000\n",
      "    epoch          : 97\n",
      "    loss           : -935572.1961633663\n",
      "    val_loss       : -930976.3625\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -907923.125000\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -962039.375000\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -906803.000000\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -919384.250000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -896558.375000\n",
      "    epoch          : 98\n",
      "    loss           : -938114.0699257426\n",
      "    val_loss       : -936354.39296875\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -1021422.125000\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -916644.062500\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -908216.125000\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -940041.812500\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -938745.812500\n",
      "    epoch          : 99\n",
      "    loss           : -938839.2995049505\n",
      "    val_loss       : -934569.8\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -933667.250000\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -920359.625000\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -950136.937500\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -932715.000000\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -903890.500000\n",
      "    epoch          : 100\n",
      "    loss           : -937517.3632425743\n",
      "    val_loss       : -935418.67109375\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0806_094159/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -1020351.750000\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -920572.375000\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -901681.312500\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -947305.937500\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -930295.125000\n",
      "    epoch          : 101\n",
      "    loss           : -935718.6639851485\n",
      "    val_loss       : -934265.759375\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -1019876.187500\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -956491.375000\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -948417.875000\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -1018028.500000\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -937220.625000\n",
      "    epoch          : 102\n",
      "    loss           : -932424.1745049505\n",
      "    val_loss       : -933841.55390625\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -1021102.250000\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -930293.562500\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -947542.937500\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -939375.375000\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -916036.125000\n",
      "    epoch          : 103\n",
      "    loss           : -937319.8601485149\n",
      "    val_loss       : -935055.128125\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -1019729.500000\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -922619.312500\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -950035.875000\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -950611.750000\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -940563.375000\n",
      "    epoch          : 104\n",
      "    loss           : -938571.6992574257\n",
      "    val_loss       : -937232.909375\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -1020678.562500\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -965147.125000\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -907357.375000\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -933638.125000\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -900906.000000\n",
      "    epoch          : 105\n",
      "    loss           : -939929.3044554455\n",
      "    val_loss       : -937739.5421875\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -1021675.500000\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -910780.312500\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -918610.875000\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -901787.625000\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -901920.625000\n",
      "    epoch          : 106\n",
      "    loss           : -939604.4653465346\n",
      "    val_loss       : -935748.03828125\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -1021063.187500\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -906551.000000\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -908091.812500\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -932099.187500\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -937485.750000\n",
      "    epoch          : 107\n",
      "    loss           : -936654.6652227723\n",
      "    val_loss       : -933142.95703125\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -1019956.812500\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -914644.000000\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -899800.062500\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -911944.500000\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -940972.500000\n",
      "    epoch          : 108\n",
      "    loss           : -935595.1707920792\n",
      "    val_loss       : -937305.40546875\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -1021570.562500\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -950372.312500\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -946766.437500\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -1020175.750000\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -907172.500000\n",
      "    epoch          : 109\n",
      "    loss           : -939309.7982673268\n",
      "    val_loss       : -937469.27890625\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -1021215.625000\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -961093.750000\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -934191.625000\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -930490.375000\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -940842.250000\n",
      "    epoch          : 110\n",
      "    loss           : -939524.875\n",
      "    val_loss       : -934686.68125\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -1021637.000000\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -922520.625000\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -909447.562500\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -903688.312500\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -942499.812500\n",
      "    epoch          : 111\n",
      "    loss           : -940372.1429455446\n",
      "    val_loss       : -937657.02109375\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -1022290.937500\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -912864.750000\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -916759.062500\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -936449.375000\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -924216.750000\n",
      "    epoch          : 112\n",
      "    loss           : -937353.2685643565\n",
      "    val_loss       : -930988.33828125\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -959848.000000\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -914721.125000\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -903926.187500\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -917525.625000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -906989.437500\n",
      "    epoch          : 113\n",
      "    loss           : -937495.854579208\n",
      "    val_loss       : -935176.128125\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -1019713.000000\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -921220.250000\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -930485.062500\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -947623.250000\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -932529.250000\n",
      "    epoch          : 114\n",
      "    loss           : -938697.3069306931\n",
      "    val_loss       : -937881.78671875\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -1021385.625000\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -964933.437500\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -952046.312500\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -944642.250000\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -904590.000000\n",
      "    epoch          : 115\n",
      "    loss           : -940857.9733910891\n",
      "    val_loss       : -938760.94140625\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -1023375.875000\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -924314.937500\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -948339.500000\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -905367.375000\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -910853.750000\n",
      "    epoch          : 116\n",
      "    loss           : -938817.5426980198\n",
      "    val_loss       : -934067.2609375\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -962305.625000\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -919460.375000\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -948555.187500\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -904089.937500\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -906865.062500\n",
      "    epoch          : 117\n",
      "    loss           : -938017.270420792\n",
      "    val_loss       : -937880.0515625\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -1022562.500000\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -924819.250000\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -908793.125000\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -904911.562500\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -905945.000000\n",
      "    epoch          : 118\n",
      "    loss           : -940906.5080445545\n",
      "    val_loss       : -937090.5421875\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -1021078.500000\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -964947.562500\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -903202.875000\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -902895.562500\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -906616.875000\n",
      "    epoch          : 119\n",
      "    loss           : -939215.4288366337\n",
      "    val_loss       : -936082.65390625\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -1020050.500000\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -920133.812500\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -911823.500000\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -933607.687500\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -936739.812500\n",
      "    epoch          : 120\n",
      "    loss           : -940652.8199257426\n",
      "    val_loss       : -937660.7171875\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -1022532.750000\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -925076.625000\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -952697.562500\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -907234.562500\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -935859.125000\n",
      "    epoch          : 121\n",
      "    loss           : -941663.1132425743\n",
      "    val_loss       : -939534.2515625\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -952925.250000\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -924874.000000\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -953247.250000\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -903493.125000\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -952474.375000\n",
      "    epoch          : 122\n",
      "    loss           : -941066.9566831683\n",
      "    val_loss       : -939000.5953125\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -1020306.250000\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -913233.250000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -929890.250000\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -936761.875000\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -896874.625000\n",
      "    epoch          : 123\n",
      "    loss           : -939065.4591584158\n",
      "    val_loss       : -937006.1609375\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -1022500.875000\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -963238.125000\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -915002.812500\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -909232.687500\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -942560.687500\n",
      "    epoch          : 124\n",
      "    loss           : -940356.1293316832\n",
      "    val_loss       : -937365.61875\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -1020995.500000\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -925923.875000\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -952826.750000\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -937130.125000\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -947446.875000\n",
      "    epoch          : 125\n",
      "    loss           : -941607.4300742574\n",
      "    val_loss       : -939115.03828125\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -1021706.000000\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -925123.312500\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -920049.312500\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -951675.000000\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -917141.625000\n",
      "    epoch          : 126\n",
      "    loss           : -941593.1107673268\n",
      "    val_loss       : -937251.8703125\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -1020262.375000\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -924402.625000\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -950667.875000\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -913523.125000\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -944607.375000\n",
      "    epoch          : 127\n",
      "    loss           : -938615.4115099009\n",
      "    val_loss       : -934652.55390625\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -1021509.937500\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -918486.125000\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -909365.437500\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -1022493.875000\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -905951.625000\n",
      "    epoch          : 128\n",
      "    loss           : -939567.9350247525\n",
      "    val_loss       : -938799.10859375\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -1022179.562500\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -914936.875000\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -944385.312500\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -945427.750000\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -943727.375000\n",
      "    epoch          : 129\n",
      "    loss           : -941569.1212871287\n",
      "    val_loss       : -938960.06015625\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -1021528.875000\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -967958.000000\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -952531.500000\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -906156.125000\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -944528.687500\n",
      "    epoch          : 130\n",
      "    loss           : -943214.1522277228\n",
      "    val_loss       : -941455.65625\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -1022965.625000\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -968915.750000\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -953913.250000\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -947530.187500\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -905983.750000\n",
      "    epoch          : 131\n",
      "    loss           : -943909.8985148515\n",
      "    val_loss       : -939964.825\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -1020427.250000\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -923768.000000\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -913591.562500\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -949287.687500\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -943882.625000\n",
      "    epoch          : 132\n",
      "    loss           : -942237.8811881188\n",
      "    val_loss       : -939950.453125\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -945289.875000\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -914810.250000\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -953628.562500\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -936696.500000\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -945028.375000\n",
      "    epoch          : 133\n",
      "    loss           : -941611.8273514851\n",
      "    val_loss       : -935358.46328125\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -916870.250000\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -925555.562500\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -950372.250000\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -905282.125000\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -908384.937500\n",
      "    epoch          : 134\n",
      "    loss           : -938578.2648514851\n",
      "    val_loss       : -936751.1203125\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -1022894.625000\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -909439.562500\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -914847.437500\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -944515.937500\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -934700.000000\n",
      "    epoch          : 135\n",
      "    loss           : -941533.6615099009\n",
      "    val_loss       : -936110.228125\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -1021704.062500\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -921406.750000\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -948906.625000\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -934454.625000\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -905647.875000\n",
      "    epoch          : 136\n",
      "    loss           : -940669.8261138614\n",
      "    val_loss       : -940021.72265625\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -938196.312500\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -968171.750000\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -922466.000000\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -939666.750000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -946530.437500\n",
      "    epoch          : 137\n",
      "    loss           : -943734.3923267326\n",
      "    val_loss       : -940375.39609375\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -1023005.062500\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -923705.250000\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -952273.375000\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -1022623.250000\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -936454.437500\n",
      "    epoch          : 138\n",
      "    loss           : -943206.7673267326\n",
      "    val_loss       : -939421.79609375\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -1023247.062500\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -938289.000000\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -918733.437500\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -948333.875000\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -945375.062500\n",
      "    epoch          : 139\n",
      "    loss           : -943832.3570544554\n",
      "    val_loss       : -939907.4265625\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -1021993.750000\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -966958.125000\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -953131.937500\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -943721.437500\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -941760.000000\n",
      "    epoch          : 140\n",
      "    loss           : -941982.2524752475\n",
      "    val_loss       : -938430.79296875\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -1020876.125000\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -927119.937500\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -910431.562500\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -910948.500000\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -915255.750000\n",
      "    epoch          : 141\n",
      "    loss           : -942631.0823019802\n",
      "    val_loss       : -938864.00859375\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -955073.125000\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -925299.312500\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -912940.000000\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -936871.000000\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -935585.875000\n",
      "    epoch          : 142\n",
      "    loss           : -941554.8935643565\n",
      "    val_loss       : -938832.13203125\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -919365.625000\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -965548.125000\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -934786.625000\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -946395.000000\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -919999.875000\n",
      "    epoch          : 143\n",
      "    loss           : -943448.5129950495\n",
      "    val_loss       : -939678.72578125\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -1020100.687500\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -967854.062500\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -947318.125000\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -1022367.250000\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -906854.000000\n",
      "    epoch          : 144\n",
      "    loss           : -943612.0699257426\n",
      "    val_loss       : -938205.91484375\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -1020900.125000\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -922777.562500\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -919010.250000\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -1021873.375000\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -937403.500000\n",
      "    epoch          : 145\n",
      "    loss           : -942838.0464108911\n",
      "    val_loss       : -941395.55\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -1021840.375000\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -966377.625000\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -946975.250000\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -940778.375000\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -907387.125000\n",
      "    epoch          : 146\n",
      "    loss           : -944732.6738861386\n",
      "    val_loss       : -940784.7125\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -1024452.687500\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -925047.125000\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -927670.687500\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -921137.562500\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -938441.125000\n",
      "    epoch          : 147\n",
      "    loss           : -943633.645420792\n",
      "    val_loss       : -941622.51875\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -1021658.875000\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -966733.875000\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -912852.500000\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -947546.687500\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -920915.125000\n",
      "    epoch          : 148\n",
      "    loss           : -944358.7246287129\n",
      "    val_loss       : -941588.0078125\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -969178.375000\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -968243.062500\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -914365.125000\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -922101.875000\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -947971.875000\n",
      "    epoch          : 149\n",
      "    loss           : -943865.8118811881\n",
      "    val_loss       : -941956.30859375\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -967584.250000\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -913134.125000\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -921888.125000\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -1019706.562500\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -936712.500000\n",
      "    epoch          : 150\n",
      "    loss           : -939952.8236386139\n",
      "    val_loss       : -932209.9015625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0806_094159/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -1020341.625000\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -922870.000000\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -916068.562500\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -938964.187500\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -921403.687500\n",
      "    epoch          : 151\n",
      "    loss           : -941675.2902227723\n",
      "    val_loss       : -941893.69453125\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -1022676.437500\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -918482.375000\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -954649.750000\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -912785.000000\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -948347.875000\n",
      "    epoch          : 152\n",
      "    loss           : -946004.2128712871\n",
      "    val_loss       : -942573.03671875\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -1023585.875000\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -922656.937500\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -948651.937500\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -918205.562500\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -913686.500000\n",
      "    epoch          : 153\n",
      "    loss           : -945928.9931930694\n",
      "    val_loss       : -942340.19296875\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -1022794.437500\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -929544.000000\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -911809.500000\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -1021457.625000\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -906718.375000\n",
      "    epoch          : 154\n",
      "    loss           : -943097.5253712871\n",
      "    val_loss       : -935913.4\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -1019798.437500\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -922826.312500\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -917174.687500\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -1023712.812500\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -923963.812500\n",
      "    epoch          : 155\n",
      "    loss           : -942714.8316831683\n",
      "    val_loss       : -939164.56796875\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -1020269.500000\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -965906.562500\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -954720.937500\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -939266.562500\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -949688.500000\n",
      "    epoch          : 156\n",
      "    loss           : -944853.6905940594\n",
      "    val_loss       : -941954.690625\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -1022335.375000\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -928471.312500\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -910880.500000\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -955645.125000\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -911777.250000\n",
      "    epoch          : 157\n",
      "    loss           : -945487.6113861386\n",
      "    val_loss       : -943345.44375\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -1023718.375000\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -929300.312500\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -953196.875000\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -911460.312500\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -913596.375000\n",
      "    epoch          : 158\n",
      "    loss           : -945411.5959158416\n",
      "    val_loss       : -942970.20546875\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -969894.375000\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -952070.312500\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -955567.250000\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -949564.500000\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -939342.062500\n",
      "    epoch          : 159\n",
      "    loss           : -946140.7537128713\n",
      "    val_loss       : -942003.66953125\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -1024026.375000\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -969852.875000\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -921727.312500\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -911314.562500\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -938910.687500\n",
      "    epoch          : 160\n",
      "    loss           : -944125.2512376237\n",
      "    val_loss       : -939810.5734375\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -1021612.125000\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -916549.312500\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -915294.000000\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -923381.062500\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -949719.062500\n",
      "    epoch          : 161\n",
      "    loss           : -944515.3929455446\n",
      "    val_loss       : -942092.4359375\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -1023816.125000\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -916231.000000\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -946824.625000\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -922044.312500\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -947878.562500\n",
      "    epoch          : 162\n",
      "    loss           : -945456.1095297029\n",
      "    val_loss       : -943304.646875\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -1023292.375000\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -930892.375000\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -952234.312500\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -955786.937500\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -949779.937500\n",
      "    epoch          : 163\n",
      "    loss           : -946776.7221534654\n",
      "    val_loss       : -943594.63046875\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -969814.062500\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -930927.812500\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -947969.250000\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -1023139.812500\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -954097.125000\n",
      "    epoch          : 164\n",
      "    loss           : -946930.7172029703\n",
      "    val_loss       : -943358.34375\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -1024105.500000\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -929535.437500\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -940739.375000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -924438.187500\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -914964.062500\n",
      "    epoch          : 165\n",
      "    loss           : -946867.9678217822\n",
      "    val_loss       : -943205.13125\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -1022919.125000\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -933938.687500\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -919132.125000\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -1024076.812500\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -937686.062500\n",
      "    epoch          : 166\n",
      "    loss           : -946102.7530940594\n",
      "    val_loss       : -941850.44609375\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -1023282.625000\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -952048.562500\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -949374.125000\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -938960.250000\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -948506.125000\n",
      "    epoch          : 167\n",
      "    loss           : -945285.7530940594\n",
      "    val_loss       : -940983.60078125\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -1022033.187500\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -928363.750000\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -955589.812500\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -955408.625000\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -942382.750000\n",
      "    epoch          : 168\n",
      "    loss           : -946290.8360148515\n",
      "    val_loss       : -942367.56875\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -1024399.000000\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -969498.000000\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -955566.687500\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -910955.250000\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -949020.500000\n",
      "    epoch          : 169\n",
      "    loss           : -946175.6435643565\n",
      "    val_loss       : -944039.4140625\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -1023405.312500\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -930123.437500\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -941097.687500\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -1025416.562500\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -913797.375000\n",
      "    epoch          : 170\n",
      "    loss           : -946262.8032178218\n",
      "    val_loss       : -943032.34765625\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -1022380.125000\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -971637.562500\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -918889.187500\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -1024378.687500\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -920735.500000\n",
      "    epoch          : 171\n",
      "    loss           : -946002.0167079208\n",
      "    val_loss       : -942885.8640625\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -926049.812500\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -967764.500000\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -922206.875000\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -954068.250000\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -941256.187500\n",
      "    epoch          : 172\n",
      "    loss           : -946564.7877475248\n",
      "    val_loss       : -944105.4796875\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -1023836.375000\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -970875.875000\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -939967.000000\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -949287.750000\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -922574.562500\n",
      "    epoch          : 173\n",
      "    loss           : -946678.156559406\n",
      "    val_loss       : -941643.26484375\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -1021248.812500\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -931601.875000\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -912776.250000\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -923893.375000\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -909554.625000\n",
      "    epoch          : 174\n",
      "    loss           : -944133.7252475248\n",
      "    val_loss       : -939727.8921875\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -1023156.625000\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -954511.875000\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -911716.000000\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -920188.000000\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -911169.000000\n",
      "    epoch          : 175\n",
      "    loss           : -943580.7178217822\n",
      "    val_loss       : -941749.45\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -1022206.500000\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -928746.625000\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -918016.375000\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -914841.250000\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -937527.000000\n",
      "    epoch          : 176\n",
      "    loss           : -944234.551980198\n",
      "    val_loss       : -942297.4078125\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -968540.187500\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -931081.250000\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -919505.625000\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -950069.250000\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -941610.562500\n",
      "    epoch          : 177\n",
      "    loss           : -946668.4597772277\n",
      "    val_loss       : -943172.459375\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -1024355.500000\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -915064.437500\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -922529.562500\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -940371.500000\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -923060.375000\n",
      "    epoch          : 178\n",
      "    loss           : -947310.0383663366\n",
      "    val_loss       : -943186.6015625\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -1022996.500000\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -952330.250000\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -956278.187500\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -1022485.625000\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -939024.750000\n",
      "    epoch          : 179\n",
      "    loss           : -945454.2549504951\n",
      "    val_loss       : -943527.86640625\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -930036.625000\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -932044.062500\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -949797.500000\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -939943.750000\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -947608.687500\n",
      "    epoch          : 180\n",
      "    loss           : -945768.6757425743\n",
      "    val_loss       : -942433.28671875\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -1023032.625000\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -932244.000000\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -921105.437500\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -1024693.625000\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -951002.375000\n",
      "    epoch          : 181\n",
      "    loss           : -947440.031559406\n",
      "    val_loss       : -943838.77109375\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -969898.937500\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -954233.500000\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -957478.125000\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -911631.500000\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -950063.375000\n",
      "    epoch          : 182\n",
      "    loss           : -946360.948019802\n",
      "    val_loss       : -943836.68984375\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -970673.875000\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -931049.125000\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -923895.875000\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -923529.750000\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -949719.625000\n",
      "    epoch          : 183\n",
      "    loss           : -947076.6652227723\n",
      "    val_loss       : -943220.271875\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -1023714.187500\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -931667.812500\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -951656.687500\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -922097.125000\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -940471.500000\n",
      "    epoch          : 184\n",
      "    loss           : -947372.0358910891\n",
      "    val_loss       : -943200.11484375\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -1023739.687500\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -957254.375000\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -920881.125000\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -956470.375000\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -916539.875000\n",
      "    epoch          : 185\n",
      "    loss           : -947130.4944306931\n",
      "    val_loss       : -944100.9984375\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -1023446.000000\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -933014.625000\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -948528.937500\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -936812.187500\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -912969.500000\n",
      "    epoch          : 186\n",
      "    loss           : -946337.2357673268\n",
      "    val_loss       : -942570.6515625\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -1022726.000000\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -932471.625000\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -913230.687500\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -915534.875000\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -916356.937500\n",
      "    epoch          : 187\n",
      "    loss           : -947599.5173267326\n",
      "    val_loss       : -944375.778125\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -969298.812500\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -939769.375000\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -915696.000000\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -957347.000000\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -925278.250000\n",
      "    epoch          : 188\n",
      "    loss           : -948387.6225247525\n",
      "    val_loss       : -944843.8796875\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -1023592.625000\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -932293.875000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -920683.000000\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -925052.937500\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -951108.062500\n",
      "    epoch          : 189\n",
      "    loss           : -947728.0216584158\n",
      "    val_loss       : -943289.4703125\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -1021876.875000\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -969194.937500\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -924248.125000\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -1023898.750000\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -910347.000000\n",
      "    epoch          : 190\n",
      "    loss           : -946166.4325495049\n",
      "    val_loss       : -940312.8515625\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -1022965.312500\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -969592.250000\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -937553.000000\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -939945.125000\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -922290.187500\n",
      "    epoch          : 191\n",
      "    loss           : -946476.1918316832\n",
      "    val_loss       : -943473.628125\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -1022203.062500\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -956968.937500\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -919498.562500\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -948923.500000\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -953032.687500\n",
      "    epoch          : 192\n",
      "    loss           : -946497.3428217822\n",
      "    val_loss       : -944602.1\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -1023143.500000\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -932074.375000\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -949760.000000\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -922911.625000\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -911545.125000\n",
      "    epoch          : 193\n",
      "    loss           : -947085.4461633663\n",
      "    val_loss       : -941543.0453125\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -1023107.937500\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -916523.875000\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -915231.062500\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -916699.250000\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -941857.625000\n",
      "    epoch          : 194\n",
      "    loss           : -946853.5247524752\n",
      "    val_loss       : -944622.56015625\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -1024158.500000\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -931519.125000\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -957798.312500\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -956979.625000\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -916645.750000\n",
      "    epoch          : 195\n",
      "    loss           : -948680.3892326732\n",
      "    val_loss       : -945080.071875\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -1025538.187500\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -971345.937500\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -924764.000000\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -957157.187500\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -915882.937500\n",
      "    epoch          : 196\n",
      "    loss           : -949032.1194306931\n",
      "    val_loss       : -945542.203125\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -1024709.500000\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -924989.500000\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -923921.687500\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -950361.000000\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -915722.375000\n",
      "    epoch          : 197\n",
      "    loss           : -948571.625\n",
      "    val_loss       : -944348.9375\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -1023767.875000\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -974093.375000\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -940709.625000\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -915108.375000\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -924060.250000\n",
      "    epoch          : 198\n",
      "    loss           : -947878.3706683168\n",
      "    val_loss       : -944397.20703125\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -1023321.375000\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -952011.187500\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -937402.250000\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -924718.312500\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -943831.625000\n",
      "    epoch          : 199\n",
      "    loss           : -947755.4368811881\n",
      "    val_loss       : -945473.5828125\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -1023317.000000\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -934999.562500\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -925221.687500\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -914132.625000\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -942032.187500\n",
      "    epoch          : 200\n",
      "    loss           : -948157.5501237623\n",
      "    val_loss       : -943888.40625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0806_094159/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -1022553.500000\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -931931.062500\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -955413.937500\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -1024761.375000\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -915086.000000\n",
      "    epoch          : 201\n",
      "    loss           : -946910.9300742574\n",
      "    val_loss       : -942623.80390625\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -1023681.750000\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -928653.437500\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -949089.375000\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -915396.875000\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -924546.437500\n",
      "    epoch          : 202\n",
      "    loss           : -947311.3929455446\n",
      "    val_loss       : -945657.6203125\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -1023580.812500\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -932024.875000\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -924443.875000\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -1023174.625000\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -917060.125000\n",
      "    epoch          : 203\n",
      "    loss           : -948571.4362623763\n",
      "    val_loss       : -944256.2125\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -1023022.500000\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -969014.125000\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -926056.375000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -940338.625000\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -951078.812500\n",
      "    epoch          : 204\n",
      "    loss           : -948822.2097772277\n",
      "    val_loss       : -944847.721875\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -1025331.875000\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -952451.312500\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -920733.062500\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -1024095.125000\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -942718.625000\n",
      "    epoch          : 205\n",
      "    loss           : -947805.4721534654\n",
      "    val_loss       : -944551.6984375\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -1022882.500000\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -922243.250000\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -917178.000000\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -952463.750000\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -934152.375000\n",
      "    epoch          : 206\n",
      "    loss           : -947065.8285891089\n",
      "    val_loss       : -942181.5421875\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -1022842.875000\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -929230.250000\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -923888.312500\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -957661.750000\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -916916.812500\n",
      "    epoch          : 207\n",
      "    loss           : -947519.0866336634\n",
      "    val_loss       : -945410.18125\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -1024899.000000\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -954928.250000\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -921758.187500\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -940760.500000\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -917408.562500\n",
      "    epoch          : 208\n",
      "    loss           : -948253.3589108911\n",
      "    val_loss       : -945552.5875\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -1023174.875000\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -956912.187500\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -957187.875000\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -927386.562500\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -950267.375000\n",
      "    epoch          : 209\n",
      "    loss           : -949498.7580445545\n",
      "    val_loss       : -945003.10390625\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -971415.812500\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -932930.625000\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -958008.250000\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -953345.250000\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -940994.437500\n",
      "    epoch          : 210\n",
      "    loss           : -948463.9511138614\n",
      "    val_loss       : -944942.8984375\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -1022937.562500\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -935175.687500\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -920046.000000\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -919738.000000\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -943732.125000\n",
      "    epoch          : 211\n",
      "    loss           : -949515.7524752475\n",
      "    val_loss       : -945448.178125\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -1023402.625000\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -934109.187500\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -950721.812500\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -951961.000000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -943463.250000\n",
      "    epoch          : 212\n",
      "    loss           : -949879.6126237623\n",
      "    val_loss       : -946099.89609375\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -973408.312500\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -931765.750000\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -925560.000000\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -1025612.250000\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -953274.937500\n",
      "    epoch          : 213\n",
      "    loss           : -949331.4504950495\n",
      "    val_loss       : -945489.1203125\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -1022794.500000\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -934747.687500\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -958620.562500\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -941856.875000\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -951173.500000\n",
      "    epoch          : 214\n",
      "    loss           : -949244.3242574257\n",
      "    val_loss       : -945173.39140625\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -1024389.437500\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -926335.062500\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -959109.750000\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -917519.625000\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -943024.500000\n",
      "    epoch          : 215\n",
      "    loss           : -949249.3279702971\n",
      "    val_loss       : -945324.2\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -1023575.375000\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -925662.625000\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -952193.875000\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -948827.437500\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -940616.875000\n",
      "    epoch          : 216\n",
      "    loss           : -947918.1763613861\n",
      "    val_loss       : -942888.9\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -1024091.625000\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -955142.125000\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -948703.125000\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -919112.500000\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -954006.000000\n",
      "    epoch          : 217\n",
      "    loss           : -948358.0303217822\n",
      "    val_loss       : -946193.19375\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -1023984.125000\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -933918.187500\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -916799.625000\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -924850.500000\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -943252.812500\n",
      "    epoch          : 218\n",
      "    loss           : -949831.3285891089\n",
      "    val_loss       : -945768.34765625\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -1024575.812500\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -953952.375000\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -951826.625000\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -953911.625000\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -950169.937500\n",
      "    epoch          : 219\n",
      "    loss           : -949815.7431930694\n",
      "    val_loss       : -945753.94921875\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -1025188.187500\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -970750.250000\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -915537.875000\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -1024658.625000\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -953651.000000\n",
      "    epoch          : 220\n",
      "    loss           : -948278.5922029703\n",
      "    val_loss       : -946111.878125\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -1024434.000000\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -971963.625000\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -925702.687500\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -1022691.437500\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -950958.500000\n",
      "    epoch          : 221\n",
      "    loss           : -948305.1200495049\n",
      "    val_loss       : -945313.41015625\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -1024584.750000\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -935158.812500\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -959596.125000\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -917975.375000\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -920251.625000\n",
      "    epoch          : 222\n",
      "    loss           : -949678.7772277228\n",
      "    val_loss       : -946389.50703125\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -1022814.062500\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -972713.625000\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -958119.875000\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -951633.000000\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -916650.625000\n",
      "    epoch          : 223\n",
      "    loss           : -950136.9839108911\n",
      "    val_loss       : -945164.1703125\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -1024307.000000\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -924231.187500\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -957960.000000\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -944557.000000\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -951119.750000\n",
      "    epoch          : 224\n",
      "    loss           : -949197.6361386139\n",
      "    val_loss       : -941835.89140625\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -971510.875000\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -969288.312500\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -951105.812500\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -926628.750000\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -944982.125000\n",
      "    epoch          : 225\n",
      "    loss           : -948156.9771039604\n",
      "    val_loss       : -945712.73828125\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -1023534.875000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -932882.375000\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -923512.625000\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -922547.625000\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -951783.750000\n",
      "    epoch          : 226\n",
      "    loss           : -948282.6652227723\n",
      "    val_loss       : -945919.990625\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -1024291.375000\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -973462.875000\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -925273.437500\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -953237.625000\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -919090.312500\n",
      "    epoch          : 227\n",
      "    loss           : -950446.7370049505\n",
      "    val_loss       : -946763.09140625\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -1024920.500000\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -970200.000000\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -959678.000000\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -951852.625000\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -943998.187500\n",
      "    epoch          : 228\n",
      "    loss           : -950580.6107673268\n",
      "    val_loss       : -946538.3703125\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -1023120.750000\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -934932.250000\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -960700.375000\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -959893.062500\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -954524.562500\n",
      "    epoch          : 229\n",
      "    loss           : -950935.905940594\n",
      "    val_loss       : -946649.278125\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -1025764.187500\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -926787.000000\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -927185.875000\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -1025015.062500\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -951954.187500\n",
      "    epoch          : 230\n",
      "    loss           : -949361.7271039604\n",
      "    val_loss       : -942740.0921875\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -1023167.625000\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -931281.875000\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -958585.500000\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -1023446.000000\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -953907.750000\n",
      "    epoch          : 231\n",
      "    loss           : -947424.5823019802\n",
      "    val_loss       : -946037.73984375\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -1024035.500000\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -972354.000000\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -926572.125000\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -945319.625000\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -959666.625000\n",
      "    epoch          : 232\n",
      "    loss           : -950555.5662128713\n",
      "    val_loss       : -945007.23984375\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -923287.375000\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -972201.062500\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -958062.000000\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -925484.000000\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -942131.250000\n",
      "    epoch          : 233\n",
      "    loss           : -949942.1318069306\n",
      "    val_loss       : -946078.140625\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -1024285.625000\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -927812.500000\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -945755.000000\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -1024840.562500\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -920266.625000\n",
      "    epoch          : 234\n",
      "    loss           : -950636.2073019802\n",
      "    val_loss       : -946830.67734375\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -1023936.812500\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -945499.750000\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -957584.125000\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -917526.625000\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -921037.500000\n",
      "    epoch          : 235\n",
      "    loss           : -951202.1058168317\n",
      "    val_loss       : -947434.55234375\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -1025733.437500\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -972757.875000\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -944401.375000\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -945593.000000\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -920589.750000\n",
      "    epoch          : 236\n",
      "    loss           : -951356.3044554455\n",
      "    val_loss       : -946906.0203125\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -1024766.125000\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -938010.687500\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -959527.500000\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -926777.625000\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -954755.562500\n",
      "    epoch          : 237\n",
      "    loss           : -950862.6608910891\n",
      "    val_loss       : -946661.0734375\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -1025394.937500\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -933058.687500\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -946705.375000\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -927885.062500\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -952985.125000\n",
      "    epoch          : 238\n",
      "    loss           : -950705.2778465346\n",
      "    val_loss       : -946357.11171875\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -1026306.750000\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -934144.125000\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -924357.937500\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -942585.750000\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -942273.125000\n",
      "    epoch          : 239\n",
      "    loss           : -949369.270420792\n",
      "    val_loss       : -945233.49453125\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -1024193.625000\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -970907.875000\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -951896.750000\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -919023.500000\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -919428.437500\n",
      "    epoch          : 240\n",
      "    loss           : -950132.9981435643\n",
      "    val_loss       : -947136.5796875\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -1024733.500000\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -937072.812500\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -944547.250000\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -943547.875000\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -919822.062500\n",
      "    epoch          : 241\n",
      "    loss           : -951347.4548267326\n",
      "    val_loss       : -946949.9453125\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -1024913.437500\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -927519.250000\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -958458.875000\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -944272.625000\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -944433.125000\n",
      "    epoch          : 242\n",
      "    loss           : -951164.4746287129\n",
      "    val_loss       : -946756.93828125\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -974101.875000\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -936432.000000\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -920003.500000\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -917179.687500\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -953079.000000\n",
      "    epoch          : 243\n",
      "    loss           : -949249.8316831683\n",
      "    val_loss       : -945362.515625\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -1024378.437500\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -934518.312500\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -957534.250000\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -943706.500000\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -946188.250000\n",
      "    epoch          : 244\n",
      "    loss           : -950450.5340346535\n",
      "    val_loss       : -946503.3203125\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -1023575.250000\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -970141.875000\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -924687.437500\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -953108.625000\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -953356.625000\n",
      "    epoch          : 245\n",
      "    loss           : -949997.593440594\n",
      "    val_loss       : -945183.3984375\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -1024113.375000\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -929292.125000\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -924824.875000\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -951531.687500\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -952065.437500\n",
      "    epoch          : 246\n",
      "    loss           : -949205.3836633663\n",
      "    val_loss       : -946013.690625\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -1025010.500000\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -969930.812500\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -923209.375000\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -917528.000000\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -916073.000000\n",
      "    epoch          : 247\n",
      "    loss           : -949115.1200495049\n",
      "    val_loss       : -945129.50859375\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -1023553.500000\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -933875.750000\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -949734.625000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -938632.062500\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -951078.500000\n",
      "    epoch          : 248\n",
      "    loss           : -947756.6429455446\n",
      "    val_loss       : -945332.5515625\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -1024666.250000\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -956604.062500\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -943126.375000\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -926919.937500\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -920731.250000\n",
      "    epoch          : 249\n",
      "    loss           : -950672.8867574257\n",
      "    val_loss       : -947224.96953125\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -1023896.375000\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -927428.125000\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -942306.062500\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -942213.312500\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -953169.187500\n",
      "    epoch          : 250\n",
      "    loss           : -950052.4294554455\n",
      "    val_loss       : -946018.61015625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0806_094159/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -1024460.250000\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -934248.875000\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -925725.000000\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -1024946.750000\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -954718.375000\n",
      "    epoch          : 251\n",
      "    loss           : -950990.6689356435\n",
      "    val_loss       : -947695.775\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -973257.250000\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -929360.750000\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -921252.500000\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -961110.750000\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -953921.937500\n",
      "    epoch          : 252\n",
      "    loss           : -951659.6027227723\n",
      "    val_loss       : -947132.040625\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -934529.937500\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -935162.312500\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -944846.000000\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -1025289.437500\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -954117.500000\n",
      "    epoch          : 253\n",
      "    loss           : -951566.0847772277\n",
      "    val_loss       : -946298.040625\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -1024510.187500\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -932922.375000\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -956598.000000\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -925606.000000\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -957587.125000\n",
      "    epoch          : 254\n",
      "    loss           : -949580.6287128713\n",
      "    val_loss       : -944834.778125\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -1025263.250000\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -926069.625000\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -946407.125000\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -959592.312500\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -956323.500000\n",
      "    epoch          : 255\n",
      "    loss           : -951140.1262376237\n",
      "    val_loss       : -947197.47265625\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -1025051.625000\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -928809.250000\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -928242.812500\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -943871.312500\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -954528.875000\n",
      "    epoch          : 256\n",
      "    loss           : -951558.0990099009\n",
      "    val_loss       : -945575.05234375\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -1025096.000000\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -964393.562500\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -920673.437500\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -951727.000000\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -919036.375000\n",
      "    epoch          : 257\n",
      "    loss           : -947126.3063118812\n",
      "    val_loss       : -944299.74609375\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -1023595.812500\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -935001.375000\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -960443.250000\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -1024831.625000\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -918942.500000\n",
      "    epoch          : 258\n",
      "    loss           : -950971.4993811881\n",
      "    val_loss       : -947345.63359375\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -1023693.875000\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -973459.750000\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -928007.250000\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -956588.375000\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -953763.250000\n",
      "    epoch          : 259\n",
      "    loss           : -951935.0495049505\n",
      "    val_loss       : -946294.534375\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -1023761.250000\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -944960.250000\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -926350.687500\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -955081.937500\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -954479.625000\n",
      "    epoch          : 260\n",
      "    loss           : -950770.0024752475\n",
      "    val_loss       : -945668.2421875\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -931984.812500\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -976123.875000\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -952523.750000\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -928615.875000\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -921399.750000\n",
      "    epoch          : 261\n",
      "    loss           : -950745.9232673268\n",
      "    val_loss       : -947454.75703125\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -1025560.687500\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -937536.000000\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -959839.437500\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -918388.125000\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -927153.125000\n",
      "    epoch          : 262\n",
      "    loss           : -952157.7159653465\n",
      "    val_loss       : -948116.26875\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -1026028.500000\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -959334.625000\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -942436.750000\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -1025740.125000\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -957549.625000\n",
      "    epoch          : 263\n",
      "    loss           : -951741.8267326732\n",
      "    val_loss       : -947462.0984375\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -1025034.875000\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -939142.750000\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -954829.375000\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -929460.875000\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -944092.875000\n",
      "    epoch          : 264\n",
      "    loss           : -952297.801980198\n",
      "    val_loss       : -947938.80859375\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -960729.625000\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -928418.000000\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -952876.125000\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -955258.375000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -955126.125000\n",
      "    epoch          : 265\n",
      "    loss           : -952141.9084158416\n",
      "    val_loss       : -947609.18984375\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -1024199.125000\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -926086.812500\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -959085.875000\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -957413.937500\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -919894.250000\n",
      "    epoch          : 266\n",
      "    loss           : -951026.521039604\n",
      "    val_loss       : -946446.48046875\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -1024402.625000\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -936648.687500\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -956461.375000\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -945336.875000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -953437.625000\n",
      "    epoch          : 267\n",
      "    loss           : -950520.9319306931\n",
      "    val_loss       : -946126.6296875\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -1024175.250000\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -934290.250000\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -919887.937500\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -921995.625000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -922814.125000\n",
      "    epoch          : 268\n",
      "    loss           : -951470.7116336634\n",
      "    val_loss       : -947750.2171875\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -1024884.062500\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -973824.625000\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -928480.750000\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -921004.500000\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -919894.375000\n",
      "    epoch          : 269\n",
      "    loss           : -952399.6918316832\n",
      "    val_loss       : -948156.48671875\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -1024969.250000\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -972702.625000\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -927807.187500\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -1024880.562500\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -929870.875000\n",
      "    epoch          : 270\n",
      "    loss           : -952214.1280940594\n",
      "    val_loss       : -947520.10546875\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -972296.937500\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -974890.125000\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -928147.187500\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -927393.125000\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -954207.250000\n",
      "    epoch          : 271\n",
      "    loss           : -951667.6466584158\n",
      "    val_loss       : -946738.23828125\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -1023553.562500\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -939082.187500\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -960959.187500\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -1025371.625000\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -944464.125000\n",
      "    epoch          : 272\n",
      "    loss           : -951358.9783415842\n",
      "    val_loss       : -946445.60390625\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -925932.625000\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -943842.125000\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -957305.937500\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -956514.437500\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -913641.687500\n",
      "    epoch          : 273\n",
      "    loss           : -950207.1819306931\n",
      "    val_loss       : -943122.88671875\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -1024545.875000\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -929004.375000\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -921473.125000\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -951969.062500\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -943323.375000\n",
      "    epoch          : 274\n",
      "    loss           : -947835.1831683168\n",
      "    val_loss       : -945848.16875\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -1024847.875000\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -968445.812500\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -928216.562500\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -926642.687500\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -954485.062500\n",
      "    epoch          : 275\n",
      "    loss           : -950601.146039604\n",
      "    val_loss       : -947965.94609375\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -1025508.500000\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -928343.750000\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -955910.500000\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -1025752.875000\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -944127.000000\n",
      "    epoch          : 276\n",
      "    loss           : -952286.9715346535\n",
      "    val_loss       : -947572.0875\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -1024045.812500\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -972445.250000\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -930702.562500\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -1022866.000000\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -949190.937500\n",
      "    epoch          : 277\n",
      "    loss           : -950177.5482673268\n",
      "    val_loss       : -944057.6375\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -1022615.062500\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -971680.625000\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -951783.500000\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -919246.250000\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -943637.125000\n",
      "    epoch          : 278\n",
      "    loss           : -949184.7766089109\n",
      "    val_loss       : -947437.79296875\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -972286.312500\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -972394.625000\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -929264.375000\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -928418.312500\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -921947.812500\n",
      "    epoch          : 279\n",
      "    loss           : -952078.6410891089\n",
      "    val_loss       : -948154.35546875\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -1024871.875000\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -957744.437500\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -940304.500000\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -959732.750000\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -954491.875000\n",
      "    epoch          : 280\n",
      "    loss           : -952059.0983910891\n",
      "    val_loss       : -948214.94296875\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -1025375.312500\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -926345.312500\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -959084.625000\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -921742.562500\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -945436.312500\n",
      "    epoch          : 281\n",
      "    loss           : -952330.8948019802\n",
      "    val_loss       : -948241.12421875\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -1025164.312500\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -961695.625000\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -929221.062500\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -929056.312500\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -955232.687500\n",
      "    epoch          : 282\n",
      "    loss           : -952743.9003712871\n",
      "    val_loss       : -948145.93359375\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -1024522.437500\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -971887.875000\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -928280.812500\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -920930.375000\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -953868.062500\n",
      "    epoch          : 283\n",
      "    loss           : -952290.457920792\n",
      "    val_loss       : -947702.82890625\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -934214.500000\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -974063.125000\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -930224.250000\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -955480.750000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -946589.750000\n",
      "    epoch          : 284\n",
      "    loss           : -952443.2926980198\n",
      "    val_loss       : -947577.4265625\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -1024456.312500\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -939431.500000\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -945597.500000\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -959986.000000\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -954054.375000\n",
      "    epoch          : 285\n",
      "    loss           : -952353.6720297029\n",
      "    val_loss       : -947613.0234375\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -1025279.625000\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -934465.687500\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -930298.625000\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -1024840.125000\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -948633.625000\n",
      "    epoch          : 286\n",
      "    loss           : -951385.5996287129\n",
      "    val_loss       : -946462.55546875\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -1024178.125000\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -937536.000000\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -928307.687500\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -946563.625000\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -955655.875000\n",
      "    epoch          : 287\n",
      "    loss           : -951340.7141089109\n",
      "    val_loss       : -947516.69921875\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -974769.625000\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -930719.250000\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -921981.000000\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -958849.000000\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -949927.000000\n",
      "    epoch          : 288\n",
      "    loss           : -949905.8533415842\n",
      "    val_loss       : -945302.70234375\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -1024009.625000\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -959543.000000\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -926366.750000\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -946199.000000\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -921104.500000\n",
      "    epoch          : 289\n",
      "    loss           : -952049.7264851485\n",
      "    val_loss       : -948333.0890625\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -1025566.750000\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -973337.937500\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -928325.500000\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -1026397.625000\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -954314.687500\n",
      "    epoch          : 290\n",
      "    loss           : -952948.6837871287\n",
      "    val_loss       : -948342.23828125\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -1025786.687500\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -973654.562500\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -928600.250000\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -954868.500000\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -955294.000000\n",
      "    epoch          : 291\n",
      "    loss           : -952720.4362623763\n",
      "    val_loss       : -947026.8859375\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -1025485.875000\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -957413.437500\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -920905.812500\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -921590.250000\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -921579.000000\n",
      "    epoch          : 292\n",
      "    loss           : -951840.5959158416\n",
      "    val_loss       : -947575.725\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -1024989.250000\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -936307.125000\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -930098.125000\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -943684.562500\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -921630.437500\n",
      "    epoch          : 293\n",
      "    loss           : -951703.8217821782\n",
      "    val_loss       : -946599.47265625\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -1025703.250000\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -965498.000000\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -925225.500000\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -943920.000000\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -955422.812500\n",
      "    epoch          : 294\n",
      "    loss           : -950551.7456683168\n",
      "    val_loss       : -947071.2515625\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -1026251.625000\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -926287.562500\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -921785.000000\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -958365.625000\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -921572.000000\n",
      "    epoch          : 295\n",
      "    loss           : -951357.2011138614\n",
      "    val_loss       : -948121.07109375\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -1025849.812500\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -921378.250000\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -949201.687500\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -921275.312500\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -956558.125000\n",
      "    epoch          : 296\n",
      "    loss           : -952720.0612623763\n",
      "    val_loss       : -949028.93046875\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -1024742.000000\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -960672.875000\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -928266.687500\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -945141.875000\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -955782.312500\n",
      "    epoch          : 297\n",
      "    loss           : -952640.9300742574\n",
      "    val_loss       : -947987.86640625\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -1024937.750000\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -935691.687500\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -929362.312500\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -930173.687500\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -958543.375000\n",
      "    epoch          : 298\n",
      "    loss           : -952849.4641089109\n",
      "    val_loss       : -948833.55703125\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -1025909.625000\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -938031.500000\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -922253.062500\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -932040.687500\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -956075.062500\n",
      "    epoch          : 299\n",
      "    loss           : -953606.3774752475\n",
      "    val_loss       : -949086.04609375\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -1024912.687500\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -938574.312500\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -928613.625000\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -920715.312500\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -918306.937500\n",
      "    epoch          : 300\n",
      "    loss           : -952363.8106435643\n",
      "    val_loss       : -946116.0046875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0806_094159/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -1024852.000000\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -938509.625000\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -928969.500000\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -927840.625000\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -956687.250000\n",
      "    epoch          : 301\n",
      "    loss           : -952533.9913366337\n",
      "    val_loss       : -948263.2078125\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -929169.625000\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -938726.250000\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -923269.625000\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -943100.125000\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -944032.562500\n",
      "    epoch          : 302\n",
      "    loss           : -952018.7821782178\n",
      "    val_loss       : -947208.9953125\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -1025805.625000\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -924240.312500\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -927486.187500\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -920962.000000\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -943496.562500\n",
      "    epoch          : 303\n",
      "    loss           : -951558.9084158416\n",
      "    val_loss       : -948364.178125\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -1026361.562500\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -972036.125000\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -956867.000000\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -930295.500000\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -923123.000000\n",
      "    epoch          : 304\n",
      "    loss           : -953233.5810643565\n",
      "    val_loss       : -949153.44375\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -1025006.375000\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -931925.437500\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -931617.250000\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -962396.125000\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -948480.625000\n",
      "    epoch          : 305\n",
      "    loss           : -953138.5897277228\n",
      "    val_loss       : -948435.38203125\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -1025674.625000\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -937778.125000\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -930297.125000\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -946337.375000\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -957691.250000\n",
      "    epoch          : 306\n",
      "    loss           : -953469.6163366337\n",
      "    val_loss       : -948386.821875\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -1025949.250000\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -937522.250000\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -928361.375000\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -928330.937500\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -945255.812500\n",
      "    epoch          : 307\n",
      "    loss           : -952436.3700495049\n",
      "    val_loss       : -948134.034375\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -1023881.437500\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -925899.437500\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -947452.937500\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -946127.000000\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -929488.750000\n",
      "    epoch          : 308\n",
      "    loss           : -952685.8044554455\n",
      "    val_loss       : -947743.51953125\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -1024878.187500\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -936368.750000\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -945308.187500\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -955974.875000\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -927248.437500\n",
      "    epoch          : 309\n",
      "    loss           : -952722.8155940594\n",
      "    val_loss       : -948532.3609375\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -1026005.375000\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -976494.250000\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -961588.375000\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -944623.125000\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -921203.562500\n",
      "    epoch          : 310\n",
      "    loss           : -952864.7400990099\n",
      "    val_loss       : -946736.2671875\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -1026107.250000\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -935752.500000\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -945895.625000\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -1024312.750000\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -959672.937500\n",
      "    epoch          : 311\n",
      "    loss           : -951609.2382425743\n",
      "    val_loss       : -947019.6046875\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -1024903.500000\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -927569.375000\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -958690.875000\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -927659.250000\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -944956.500000\n",
      "    epoch          : 312\n",
      "    loss           : -952501.5284653465\n",
      "    val_loss       : -948829.2890625\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -973430.500000\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -937848.375000\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -960706.750000\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -924626.750000\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -921998.062500\n",
      "    epoch          : 313\n",
      "    loss           : -952619.3805693069\n",
      "    val_loss       : -948559.47421875\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -935817.187500\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -971803.625000\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -923453.250000\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -955411.625000\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -954878.125000\n",
      "    epoch          : 314\n",
      "    loss           : -952513.3657178218\n",
      "    val_loss       : -947398.0984375\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -1026012.625000\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -926335.000000\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -930329.750000\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -930231.187500\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -927955.312500\n",
      "    epoch          : 315\n",
      "    loss           : -952851.7271039604\n",
      "    val_loss       : -948424.89609375\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -1025256.625000\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -937458.250000\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -928828.625000\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -947186.000000\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -957487.625000\n",
      "    epoch          : 316\n",
      "    loss           : -953227.1800742574\n",
      "    val_loss       : -948395.0109375\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -1026046.312500\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -935738.125000\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -961421.687500\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -947614.250000\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -954505.625000\n",
      "    epoch          : 317\n",
      "    loss           : -953098.4610148515\n",
      "    val_loss       : -948549.82421875\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -1025966.250000\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -973957.500000\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -925210.562500\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -927330.062500\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -920848.625000\n",
      "    epoch          : 318\n",
      "    loss           : -952721.5086633663\n",
      "    val_loss       : -949073.41328125\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -960961.625000\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -938571.625000\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -930477.500000\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -929922.250000\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -948348.562500\n",
      "    epoch          : 319\n",
      "    loss           : -953775.6441831683\n",
      "    val_loss       : -948421.97578125\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -1026178.125000\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -936536.750000\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -928964.375000\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -1024544.062500\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -922405.625000\n",
      "    epoch          : 320\n",
      "    loss           : -952304.1076732674\n",
      "    val_loss       : -948622.2375\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -924803.875000\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -960662.437500\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -929926.062500\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -1025769.000000\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -923174.375000\n",
      "    epoch          : 321\n",
      "    loss           : -953149.6590346535\n",
      "    val_loss       : -949416.5234375\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -1025811.812500\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -940839.375000\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -923638.375000\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -961242.250000\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -953720.875000\n",
      "    epoch          : 322\n",
      "    loss           : -954066.9053217822\n",
      "    val_loss       : -949224.25390625\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -1025832.500000\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -973367.500000\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -957561.875000\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -1025312.937500\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -957978.750000\n",
      "    epoch          : 323\n",
      "    loss           : -953758.9344059406\n",
      "    val_loss       : -948304.8359375\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -1025101.625000\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -960049.500000\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -927186.250000\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -1025514.500000\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -957949.437500\n",
      "    epoch          : 324\n",
      "    loss           : -953050.8787128713\n",
      "    val_loss       : -948890.371875\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -974129.000000\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -927570.625000\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -959883.125000\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -956650.125000\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -930240.000000\n",
      "    epoch          : 325\n",
      "    loss           : -953160.7444306931\n",
      "    val_loss       : -948012.7953125\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -1024360.437500\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -936816.125000\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -946742.625000\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -1025414.875000\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -955310.625000\n",
      "    epoch          : 326\n",
      "    loss           : -953401.1373762377\n",
      "    val_loss       : -947720.6015625\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -1025941.875000\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -928304.062500\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -930254.750000\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -923051.000000\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -956187.500000\n",
      "    epoch          : 327\n",
      "    loss           : -952861.1974009901\n",
      "    val_loss       : -948823.27421875\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -927374.062500\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -940031.000000\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -932238.750000\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -959561.000000\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -931154.125000\n",
      "    epoch          : 328\n",
      "    loss           : -953850.3613861386\n",
      "    val_loss       : -948000.50546875\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -1024414.375000\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -932005.750000\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -928240.500000\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -958564.437500\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -949316.875000\n",
      "    epoch          : 329\n",
      "    loss           : -952907.1998762377\n",
      "    val_loss       : -948180.18359375\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -1025207.250000\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -935310.000000\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -958303.500000\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -923781.375000\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -922910.937500\n",
      "    epoch          : 330\n",
      "    loss           : -953552.9659653465\n",
      "    val_loss       : -949632.16015625\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -1026196.562500\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -932613.375000\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -926340.500000\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -948230.625000\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -949435.812500\n",
      "    epoch          : 331\n",
      "    loss           : -950238.3706683168\n",
      "    val_loss       : -946261.09140625\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -1024190.312500\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -932784.125000\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -959265.125000\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -919938.625000\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -947884.250000\n",
      "    epoch          : 332\n",
      "    loss           : -951294.4870049505\n",
      "    val_loss       : -948529.796875\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -1025679.750000\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -938946.625000\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -962138.625000\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -930114.625000\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -944574.625000\n",
      "    epoch          : 333\n",
      "    loss           : -954153.0556930694\n",
      "    val_loss       : -949624.0546875\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -1026707.750000\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -973555.875000\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -931309.312500\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -949197.750000\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -921880.250000\n",
      "    epoch          : 334\n",
      "    loss           : -954226.7134900991\n",
      "    val_loss       : -948972.1453125\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -974851.000000\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -972856.875000\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -960399.000000\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -923403.312500\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -955932.000000\n",
      "    epoch          : 335\n",
      "    loss           : -953447.2023514851\n",
      "    val_loss       : -948684.2859375\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -1025852.750000\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -924546.250000\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -957528.000000\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -1025410.875000\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -961739.062500\n",
      "    epoch          : 336\n",
      "    loss           : -954202.8211633663\n",
      "    val_loss       : -948505.8453125\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -1025478.500000\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -973091.125000\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -946801.750000\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -948479.437500\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -947017.812500\n",
      "    epoch          : 337\n",
      "    loss           : -953718.7388613861\n",
      "    val_loss       : -948629.63125\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -1026646.625000\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -974382.375000\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -947822.062500\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -923679.375000\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -947696.625000\n",
      "    epoch          : 338\n",
      "    loss           : -953502.6868811881\n",
      "    val_loss       : -948788.30703125\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -1025097.375000\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -962602.625000\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -976714.000000\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -928603.750000\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -955441.062500\n",
      "    epoch          : 339\n",
      "    loss           : -954527.4003712871\n",
      "    val_loss       : -948839.76953125\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -1025793.625000\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -929740.375000\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -960206.375000\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -929117.812500\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -956414.312500\n",
      "    epoch          : 340\n",
      "    loss           : -953311.6101485149\n",
      "    val_loss       : -947698.7359375\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -1025610.687500\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -937117.937500\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -928318.937500\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -960491.125000\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -953441.750000\n",
      "    epoch          : 341\n",
      "    loss           : -952864.8007425743\n",
      "    val_loss       : -948972.16875\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -974661.312500\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -940065.000000\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -956033.375000\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -945328.375000\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -920256.562500\n",
      "    epoch          : 342\n",
      "    loss           : -952316.9882425743\n",
      "    val_loss       : -945775.69453125\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -1024407.250000\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -935940.875000\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -957673.625000\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -960904.625000\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -955332.375000\n",
      "    epoch          : 343\n",
      "    loss           : -952687.6540841584\n",
      "    val_loss       : -949016.11328125\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -1025176.875000\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -961744.375000\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -930714.312500\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -925375.000000\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -949131.875000\n",
      "    epoch          : 344\n",
      "    loss           : -954239.917079208\n",
      "    val_loss       : -948525.85234375\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -1025048.750000\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -937818.875000\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -959070.937500\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -929925.250000\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -924074.062500\n",
      "    epoch          : 345\n",
      "    loss           : -954404.9820544554\n",
      "    val_loss       : -949812.009375\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -1026336.250000\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -941579.375000\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -929585.750000\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -947997.875000\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -957295.250000\n",
      "    epoch          : 346\n",
      "    loss           : -953666.0148514851\n",
      "    val_loss       : -947632.54375\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -1024106.500000\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -960586.187500\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -928489.500000\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -1025398.250000\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -958586.500000\n",
      "    epoch          : 347\n",
      "    loss           : -953625.0439356435\n",
      "    val_loss       : -948762.56484375\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -975815.875000\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -937804.625000\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -962214.562500\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -945548.375000\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -923251.312500\n",
      "    epoch          : 348\n",
      "    loss           : -953315.2642326732\n",
      "    val_loss       : -948408.3484375\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -1027128.937500\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -932246.875000\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -930358.875000\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -1024583.250000\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -952291.125000\n",
      "    epoch          : 349\n",
      "    loss           : -952710.8706683168\n",
      "    val_loss       : -946755.565625\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -1025109.375000\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -931522.062500\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -927873.375000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -923262.125000\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -949391.875000\n",
      "    epoch          : 350\n",
      "    loss           : -952482.7103960396\n",
      "    val_loss       : -948335.45390625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0806_094159/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -1024557.562500\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -930945.062500\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -946442.375000\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -929930.312500\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -947110.375000\n",
      "    epoch          : 351\n",
      "    loss           : -953725.0606435643\n",
      "    val_loss       : -949184.67578125\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -1026185.062500\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -922498.937500\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -922786.750000\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -1025055.250000\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -926513.625000\n",
      "    epoch          : 352\n",
      "    loss           : -953973.3471534654\n",
      "    val_loss       : -948878.3890625\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -1025508.000000\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -924500.250000\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -960886.500000\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -946738.250000\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -948878.875000\n",
      "    epoch          : 353\n",
      "    loss           : -953903.8774752475\n",
      "    val_loss       : -949463.58515625\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -1026774.000000\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -956346.937500\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -961190.625000\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -946974.625000\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -929713.125000\n",
      "    epoch          : 354\n",
      "    loss           : -953962.3904702971\n",
      "    val_loss       : -949374.35078125\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -1025241.125000\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -974821.375000\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -948836.750000\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -948142.625000\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -947407.125000\n",
      "    epoch          : 355\n",
      "    loss           : -954563.4882425743\n",
      "    val_loss       : -949687.8625\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -1025259.875000\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -973741.937500\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -963225.125000\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -958162.562500\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -947017.875000\n",
      "    epoch          : 356\n",
      "    loss           : -953955.1330445545\n",
      "    val_loss       : -948809.55\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -1025512.687500\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -938045.500000\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -962700.562500\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -928173.000000\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -946440.500000\n",
      "    epoch          : 357\n",
      "    loss           : -953603.5402227723\n",
      "    val_loss       : -948751.3828125\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -1026456.625000\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -962064.562500\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -947846.250000\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -960453.000000\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -957226.375000\n",
      "    epoch          : 358\n",
      "    loss           : -953598.0643564357\n",
      "    val_loss       : -948274.33984375\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -1024740.000000\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -929838.250000\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -929519.375000\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -944946.562500\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -946104.562500\n",
      "    epoch          : 359\n",
      "    loss           : -953303.8787128713\n",
      "    val_loss       : -948523.36875\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -1025675.687500\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -956492.625000\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -960298.812500\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -918781.250000\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -957553.187500\n",
      "    epoch          : 360\n",
      "    loss           : -951954.9306930694\n",
      "    val_loss       : -948870.29453125\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -1025829.250000\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -936759.500000\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -958473.000000\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -929127.500000\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -963003.375000\n",
      "    epoch          : 361\n",
      "    loss           : -954508.7431930694\n",
      "    val_loss       : -949671.24765625\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -1024754.625000\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -942150.250000\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -958974.000000\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -924466.250000\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -926036.187500\n",
      "    epoch          : 362\n",
      "    loss           : -955154.7450495049\n",
      "    val_loss       : -950082.97421875\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -1025236.000000\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -932001.312500\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -940531.250000\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -950417.750000\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -922694.687500\n",
      "    epoch          : 363\n",
      "    loss           : -955240.1683168317\n",
      "    val_loss       : -950004.39921875\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -975271.812500\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -940981.062500\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -933999.375000\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -959384.000000\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -948012.312500\n",
      "    epoch          : 364\n",
      "    loss           : -955176.4412128713\n",
      "    val_loss       : -949532.63203125\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -930124.937500\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -932171.125000\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -933672.375000\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -957616.562500\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -920605.375000\n",
      "    epoch          : 365\n",
      "    loss           : -953747.3261138614\n",
      "    val_loss       : -946060.98515625\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -1024724.312500\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -922431.000000\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -920837.562500\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -923207.875000\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -956790.187500\n",
      "    epoch          : 366\n",
      "    loss           : -951964.6745049505\n",
      "    val_loss       : -948482.51640625\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -1024238.375000\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -973990.312500\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -933216.375000\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -1025884.187500\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -957932.375000\n",
      "    epoch          : 367\n",
      "    loss           : -954609.1472772277\n",
      "    val_loss       : -949423.1484375\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -1026488.000000\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -940062.250000\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -964106.625000\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -956891.000000\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -929948.562500\n",
      "    epoch          : 368\n",
      "    loss           : -954937.5297029703\n",
      "    val_loss       : -949622.0765625\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -1025720.125000\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -940895.750000\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -959636.000000\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -962136.500000\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -932087.062500\n",
      "    epoch          : 369\n",
      "    loss           : -954649.4938118812\n",
      "    val_loss       : -949989.1859375\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -975634.500000\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -940136.125000\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -930492.750000\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -932804.000000\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -924456.500000\n",
      "    epoch          : 370\n",
      "    loss           : -955119.0754950495\n",
      "    val_loss       : -950096.38125\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -1025749.750000\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -938334.125000\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -926513.250000\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -924515.000000\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -945144.312500\n",
      "    epoch          : 371\n",
      "    loss           : -952507.583539604\n",
      "    val_loss       : -948065.9859375\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -1025583.375000\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -940468.500000\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -931421.500000\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -956152.875000\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -954616.875000\n",
      "    epoch          : 372\n",
      "    loss           : -953523.978960396\n",
      "    val_loss       : -946051.309375\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -1026211.875000\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -943719.375000\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -924231.625000\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -1024983.937500\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -921821.875000\n",
      "    epoch          : 373\n",
      "    loss           : -951722.5241336634\n",
      "    val_loss       : -948594.1765625\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -974909.812500\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -955302.625000\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -924153.062500\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -932762.625000\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -948033.250000\n",
      "    epoch          : 374\n",
      "    loss           : -954707.0445544554\n",
      "    val_loss       : -950200.77734375\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -940802.625000\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -940855.750000\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -930713.000000\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -960085.125000\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -948665.750000\n",
      "    epoch          : 375\n",
      "    loss           : -955281.7821782178\n",
      "    val_loss       : -950123.13203125\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -1026144.000000\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -974373.437500\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -958070.750000\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -945141.625000\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -945543.625000\n",
      "    epoch          : 376\n",
      "    loss           : -953958.8038366337\n",
      "    val_loss       : -948114.4578125\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -976119.562500\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -939779.312500\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -961560.625000\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -947441.312500\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -959229.125000\n",
      "    epoch          : 377\n",
      "    loss           : -954140.0055693069\n",
      "    val_loss       : -949835.5359375\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -1025884.812500\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -974651.187500\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -933305.500000\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -923258.125000\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -959270.687500\n",
      "    epoch          : 378\n",
      "    loss           : -954615.2586633663\n",
      "    val_loss       : -949658.69765625\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -1026054.875000\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -973896.875000\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -924554.125000\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -924200.875000\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -947048.375000\n",
      "    epoch          : 379\n",
      "    loss           : -954075.7035891089\n",
      "    val_loss       : -949399.528125\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -1023916.000000\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -936828.125000\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -961512.187500\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -1025058.875000\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -923310.062500\n",
      "    epoch          : 380\n",
      "    loss           : -952811.5507425743\n",
      "    val_loss       : -949304.85625\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -1026599.312500\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -939495.562500\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -957768.937500\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -963475.125000\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -959279.125000\n",
      "    epoch          : 381\n",
      "    loss           : -955018.0179455446\n",
      "    val_loss       : -950029.609375\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -1026906.875000\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -939208.375000\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -930960.375000\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -961176.750000\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -948639.000000\n",
      "    epoch          : 382\n",
      "    loss           : -955261.3613861386\n",
      "    val_loss       : -949616.99921875\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -1025659.062500\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -976534.937500\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -922861.437500\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -925940.875000\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -948894.625000\n",
      "    epoch          : 383\n",
      "    loss           : -955375.4368811881\n",
      "    val_loss       : -950159.72578125\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -1025424.000000\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -976425.375000\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -950627.000000\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -935051.625000\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -948324.875000\n",
      "    epoch          : 384\n",
      "    loss           : -955411.4084158416\n",
      "    val_loss       : -949377.1046875\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -1026274.125000\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -974560.562500\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -931926.250000\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -931459.562500\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -924436.375000\n",
      "    epoch          : 385\n",
      "    loss           : -955058.9826732674\n",
      "    val_loss       : -949418.4625\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -1026516.687500\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -938868.187500\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -963128.875000\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -957228.062500\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -949479.062500\n",
      "    epoch          : 386\n",
      "    loss           : -954394.3044554455\n",
      "    val_loss       : -948948.0078125\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -1025572.937500\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -962253.937500\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -925235.500000\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -934231.937500\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -925433.562500\n",
      "    epoch          : 387\n",
      "    loss           : -955065.2568069306\n",
      "    val_loss       : -949778.5890625\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -1027618.875000\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -942007.500000\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -958455.125000\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -926053.312500\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -961566.625000\n",
      "    epoch          : 388\n",
      "    loss           : -955226.2599009901\n",
      "    val_loss       : -949136.23828125\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -977081.562500\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -935597.375000\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -936579.750000\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -934934.375000\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -923906.875000\n",
      "    epoch          : 389\n",
      "    loss           : -955132.6070544554\n",
      "    val_loss       : -949730.53125\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -1026684.375000\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -932939.250000\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -963595.000000\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -931526.250000\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -923284.250000\n",
      "    epoch          : 390\n",
      "    loss           : -955278.8013613861\n",
      "    val_loss       : -949618.17265625\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -1025480.750000\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -941511.250000\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -927016.625000\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -1024520.375000\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -957838.000000\n",
      "    epoch          : 391\n",
      "    loss           : -953442.7889851485\n",
      "    val_loss       : -947875.115625\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -1025754.250000\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -939086.812500\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -962510.187500\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -931953.187500\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -947550.187500\n",
      "    epoch          : 392\n",
      "    loss           : -953894.9356435643\n",
      "    val_loss       : -949673.184375\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -1025519.125000\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -940489.125000\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -931747.625000\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -1026231.312500\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -947010.500000\n",
      "    epoch          : 393\n",
      "    loss           : -955397.4573019802\n",
      "    val_loss       : -949574.74375\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -1026000.062500\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -937884.812500\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -932198.250000\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -959478.125000\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -926034.250000\n",
      "    epoch          : 394\n",
      "    loss           : -954536.8094059406\n",
      "    val_loss       : -947667.4640625\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -1023343.500000\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -935661.250000\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -923665.500000\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -1026030.562500\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -921074.500000\n",
      "    epoch          : 395\n",
      "    loss           : -953255.6577970297\n",
      "    val_loss       : -949422.95\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -1024864.500000\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -933484.750000\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -962491.812500\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -963206.937500\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -957865.062500\n",
      "    epoch          : 396\n",
      "    loss           : -955092.271039604\n",
      "    val_loss       : -949727.2171875\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -976449.000000\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -976492.625000\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -935191.000000\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -924809.000000\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -925530.437500\n",
      "    epoch          : 397\n",
      "    loss           : -955457.1485148515\n",
      "    val_loss       : -949295.678125\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -1027359.062500\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -958947.812500\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -926703.187500\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -933333.250000\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -931697.875000\n",
      "    epoch          : 398\n",
      "    loss           : -954363.8316831683\n",
      "    val_loss       : -949796.21796875\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -1026531.562500\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -976689.000000\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -957520.875000\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -959201.125000\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -946382.812500\n",
      "    epoch          : 399\n",
      "    loss           : -954967.5977722772\n",
      "    val_loss       : -949506.58125\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -1025629.000000\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -948413.875000\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -930617.000000\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -1026023.500000\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -925114.000000\n",
      "    epoch          : 400\n",
      "    loss           : -955204.6219059406\n",
      "    val_loss       : -950194.8875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0806_094159/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -1026711.000000\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -976743.125000\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -948889.625000\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -934375.312500\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -958469.562500\n",
      "    epoch          : 401\n",
      "    loss           : -955620.1905940594\n",
      "    val_loss       : -949966.83203125\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -1026019.937500\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -950578.000000\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -934335.750000\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -951683.562500\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -959849.937500\n",
      "    epoch          : 402\n",
      "    loss           : -955624.3032178218\n",
      "    val_loss       : -950127.64453125\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -1026037.062500\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -973430.187500\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -930048.375000\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -927594.125000\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -955712.750000\n",
      "    epoch          : 403\n",
      "    loss           : -953974.7110148515\n",
      "    val_loss       : -949543.9140625\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -1026383.750000\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -937340.750000\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -931887.062500\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -931491.875000\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -959313.875000\n",
      "    epoch          : 404\n",
      "    loss           : -954975.0049504951\n",
      "    val_loss       : -950238.6296875\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -1027267.750000\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -962591.750000\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -941399.875000\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -958005.250000\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -926127.125000\n",
      "    epoch          : 405\n",
      "    loss           : -955471.5575495049\n",
      "    val_loss       : -949931.0\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -1026005.687500\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -976184.250000\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -934277.000000\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -925387.375000\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -960506.375000\n",
      "    epoch          : 406\n",
      "    loss           : -955344.0866336634\n",
      "    val_loss       : -950244.68671875\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -977087.187500\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -940201.500000\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -932545.312500\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -959262.125000\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -957755.875000\n",
      "    epoch          : 407\n",
      "    loss           : -955111.1107673268\n",
      "    val_loss       : -949515.22578125\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -948775.437500\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -940697.687500\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -947747.812500\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -931862.125000\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -924107.187500\n",
      "    epoch          : 408\n",
      "    loss           : -955383.7134900991\n",
      "    val_loss       : -949560.11640625\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -1026047.937500\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -931270.687500\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -936116.062500\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -936050.687500\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -957788.812500\n",
      "    epoch          : 409\n",
      "    loss           : -955142.1138613861\n",
      "    val_loss       : -949357.85625\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -938494.125000\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -976030.625000\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -964315.750000\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -1024375.000000\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -926466.562500\n",
      "    epoch          : 410\n",
      "    loss           : -955325.4653465346\n",
      "    val_loss       : -950193.1953125\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -1026251.687500\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -959642.250000\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -932691.125000\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -961034.250000\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -923532.500000\n",
      "    epoch          : 411\n",
      "    loss           : -955081.6021039604\n",
      "    val_loss       : -949873.2671875\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -1026090.250000\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -963007.375000\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -948010.375000\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -949679.875000\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -955853.312500\n",
      "    epoch          : 412\n",
      "    loss           : -954972.4393564357\n",
      "    val_loss       : -947674.04453125\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -1026293.437500\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -935175.937500\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -958119.187500\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -924018.812500\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -930263.687500\n",
      "    epoch          : 413\n",
      "    loss           : -953935.8106435643\n",
      "    val_loss       : -950189.2125\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -1027207.750000\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -962526.312500\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -931612.500000\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -923434.750000\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -957165.187500\n",
      "    epoch          : 414\n",
      "    loss           : -955696.2431930694\n",
      "    val_loss       : -949589.2078125\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -1025273.500000\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -941158.937500\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -959275.187500\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -957857.187500\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -925201.875000\n",
      "    epoch          : 415\n",
      "    loss           : -955643.6095297029\n",
      "    val_loss       : -949905.2828125\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -1026284.625000\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -937418.000000\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -932985.375000\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -931685.187500\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -923726.125000\n",
      "    epoch          : 416\n",
      "    loss           : -955095.0290841584\n",
      "    val_loss       : -950373.49453125\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -1027734.062500\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -975465.000000\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -958862.625000\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -957200.750000\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -947734.625000\n",
      "    epoch          : 417\n",
      "    loss           : -955475.3694306931\n",
      "    val_loss       : -948977.13984375\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -1024949.562500\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -975574.812500\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -923364.250000\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -948842.000000\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -947418.250000\n",
      "    epoch          : 418\n",
      "    loss           : -953915.4399752475\n",
      "    val_loss       : -950050.47578125\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -1026669.187500\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -942618.687500\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -962335.750000\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -927809.812500\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -928600.000000\n",
      "    epoch          : 419\n",
      "    loss           : -955906.6404702971\n",
      "    val_loss       : -950698.54140625\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -940696.625000\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -940649.625000\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -958574.500000\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -934035.500000\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -932985.375000\n",
      "    epoch          : 420\n",
      "    loss           : -956509.3038366337\n",
      "    val_loss       : -950527.571875\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -1026275.062500\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -976841.375000\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -929151.437500\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -957509.812500\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -959368.000000\n",
      "    epoch          : 421\n",
      "    loss           : -955462.114480198\n",
      "    val_loss       : -948833.96875\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -974812.375000\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -929062.875000\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -963533.250000\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -1026564.500000\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -932305.750000\n",
      "    epoch          : 422\n",
      "    loss           : -954555.2042079208\n",
      "    val_loss       : -949052.13046875\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -1024324.125000\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -961327.500000\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -961818.687500\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -948226.312500\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -958994.625000\n",
      "    epoch          : 423\n",
      "    loss           : -955493.0321782178\n",
      "    val_loss       : -950292.50390625\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -938987.062500\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -975949.250000\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -948614.562500\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -960503.875000\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -948673.250000\n",
      "    epoch          : 424\n",
      "    loss           : -955302.6181930694\n",
      "    val_loss       : -948511.1265625\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -1024736.875000\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -934885.187500\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -953222.437500\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -945166.187500\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -925120.562500\n",
      "    epoch          : 425\n",
      "    loss           : -953044.1478960396\n",
      "    val_loss       : -947736.3390625\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -974088.562500\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -935748.312500\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -932354.437500\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -951097.375000\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -960079.312500\n",
      "    epoch          : 426\n",
      "    loss           : -954363.5612623763\n",
      "    val_loss       : -950006.040625\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -976514.500000\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -941857.625000\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -963435.437500\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -931893.187500\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -958577.250000\n",
      "    epoch          : 427\n",
      "    loss           : -955239.8558168317\n",
      "    val_loss       : -949381.771875\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -1025784.625000\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -941440.062500\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -963466.875000\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -924505.750000\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -928239.812500\n",
      "    epoch          : 428\n",
      "    loss           : -955591.1472772277\n",
      "    val_loss       : -949926.93359375\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -1027149.812500\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -942063.500000\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -930411.937500\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -959187.750000\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -960727.312500\n",
      "    epoch          : 429\n",
      "    loss           : -956062.8143564357\n",
      "    val_loss       : -950973.05078125\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -962857.562500\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -961195.125000\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -952061.375000\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -949749.500000\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -947505.625000\n",
      "    epoch          : 430\n",
      "    loss           : -956276.0278465346\n",
      "    val_loss       : -950924.38984375\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -960135.687500\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -976540.750000\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -931181.500000\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -949534.687500\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -925578.125000\n",
      "    epoch          : 431\n",
      "    loss           : -956238.1243811881\n",
      "    val_loss       : -950098.37734375\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -1026217.437500\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -941616.625000\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -929070.000000\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -958018.812500\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -926947.625000\n",
      "    epoch          : 432\n",
      "    loss           : -954656.4900990099\n",
      "    val_loss       : -949003.5890625\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -977738.437500\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -940645.125000\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -958457.437500\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -958600.000000\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -958835.000000\n",
      "    epoch          : 433\n",
      "    loss           : -955558.1126237623\n",
      "    val_loss       : -950315.2765625\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -1026504.312500\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -951569.312500\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -928197.062500\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -927914.375000\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -948775.875000\n",
      "    epoch          : 434\n",
      "    loss           : -956333.8558168317\n",
      "    val_loss       : -949899.71796875\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -1026737.687500\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -961500.875000\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -931222.875000\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -1024889.250000\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -927601.500000\n",
      "    epoch          : 435\n",
      "    loss           : -955610.8180693069\n",
      "    val_loss       : -950341.603125\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -1026937.875000\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -941211.125000\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -926511.000000\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -959879.875000\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -927415.375000\n",
      "    epoch          : 436\n",
      "    loss           : -954861.0965346535\n",
      "    val_loss       : -949941.025\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -1026380.812500\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -930508.375000\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -935393.500000\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -927355.250000\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -947877.375000\n",
      "    epoch          : 437\n",
      "    loss           : -956198.2388613861\n",
      "    val_loss       : -950182.471875\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -1026520.250000\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -974559.375000\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -959395.625000\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -946464.500000\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -926668.750000\n",
      "    epoch          : 438\n",
      "    loss           : -953784.958539604\n",
      "    val_loss       : -949768.86796875\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -1026745.062500\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -974899.750000\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -934169.312500\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -928408.625000\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -958479.375000\n",
      "    epoch          : 439\n",
      "    loss           : -956194.5655940594\n",
      "    val_loss       : -950821.665625\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -941605.125000\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -932913.250000\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -933559.437500\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -959952.812500\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -948269.250000\n",
      "    epoch          : 440\n",
      "    loss           : -956496.3112623763\n",
      "    val_loss       : -950092.19296875\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -974144.562500\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -942229.500000\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -936797.812500\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -960029.875000\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -958781.250000\n",
      "    epoch          : 441\n",
      "    loss           : -956036.3743811881\n",
      "    val_loss       : -950120.6046875\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -1025917.375000\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -935615.375000\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -932587.625000\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -924084.687500\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -926417.875000\n",
      "    epoch          : 442\n",
      "    loss           : -956189.1274752475\n",
      "    val_loss       : -950717.80859375\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -1026938.875000\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -942220.750000\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -961403.500000\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -958378.250000\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -959370.250000\n",
      "    epoch          : 443\n",
      "    loss           : -955588.510519802\n",
      "    val_loss       : -948386.6234375\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -1026241.375000\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -945485.562500\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -932126.875000\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -1025903.500000\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -923093.250000\n",
      "    epoch          : 444\n",
      "    loss           : -954051.698019802\n",
      "    val_loss       : -949007.69609375\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -949948.687500\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -939361.125000\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -949284.062500\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -945965.312500\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -927890.812500\n",
      "    epoch          : 445\n",
      "    loss           : -955808.5179455446\n",
      "    val_loss       : -950679.30390625\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -1026280.000000\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -934507.375000\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -957722.937500\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -948049.875000\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -947201.812500\n",
      "    epoch          : 446\n",
      "    loss           : -955759.2889851485\n",
      "    val_loss       : -949622.1078125\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -1026202.062500\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -942106.312500\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -960592.750000\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -926566.375000\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -934450.375000\n",
      "    epoch          : 447\n",
      "    loss           : -956407.7877475248\n",
      "    val_loss       : -951016.00625\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -1026660.312500\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -975717.312500\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -931592.562500\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -962487.187500\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -949537.250000\n",
      "    epoch          : 448\n",
      "    loss           : -956298.3471534654\n",
      "    val_loss       : -950229.94296875\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -1025990.562500\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -940251.125000\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -929090.562500\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -962861.250000\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -932683.937500\n",
      "    epoch          : 449\n",
      "    loss           : -956123.6844059406\n",
      "    val_loss       : -950569.5046875\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -1026639.875000\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -976702.125000\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -927534.125000\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -961500.312500\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -924552.812500\n",
      "    epoch          : 450\n",
      "    loss           : -955692.9826732674\n",
      "    val_loss       : -948733.78203125\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0806_094159/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -1024264.750000\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -976064.437500\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -930383.375000\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -930409.875000\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -949250.750000\n",
      "    epoch          : 451\n",
      "    loss           : -954562.2419554455\n",
      "    val_loss       : -950608.0125\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -1026677.125000\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -941496.750000\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -962911.750000\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -958467.250000\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -932225.875000\n",
      "    epoch          : 452\n",
      "    loss           : -956399.6243811881\n",
      "    val_loss       : -950067.16328125\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -977649.000000\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -973611.625000\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -960832.750000\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -928814.000000\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -946313.000000\n",
      "    epoch          : 453\n",
      "    loss           : -954265.6386138614\n",
      "    val_loss       : -949483.31953125\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -1027443.625000\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -929194.000000\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -926486.625000\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -1026759.437500\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -948223.375000\n",
      "    epoch          : 454\n",
      "    loss           : -955488.1466584158\n",
      "    val_loss       : -950166.5171875\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -1026864.000000\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -940582.687500\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -949034.750000\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -959430.875000\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -959344.875000\n",
      "    epoch          : 455\n",
      "    loss           : -955098.0451732674\n",
      "    val_loss       : -948593.4\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -1024899.437500\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -937957.750000\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -928371.937500\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -949546.250000\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -926770.562500\n",
      "    epoch          : 456\n",
      "    loss           : -955287.8279702971\n",
      "    val_loss       : -950968.575\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -934219.625000\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -940414.625000\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -949498.875000\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -948451.375000\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -927505.062500\n",
      "    epoch          : 457\n",
      "    loss           : -955774.0123762377\n",
      "    val_loss       : -950219.69609375\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -1026457.875000\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -940779.625000\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -965549.562500\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -962078.625000\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -933012.187500\n",
      "    epoch          : 458\n",
      "    loss           : -956418.8137376237\n",
      "    val_loss       : -950762.58046875\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -1027061.062500\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -934878.125000\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -957958.875000\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -958767.000000\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -957569.812500\n",
      "    epoch          : 459\n",
      "    loss           : -955692.4121287129\n",
      "    val_loss       : -950433.5234375\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -1027050.625000\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -940462.562500\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -963220.375000\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -932711.000000\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -926621.500000\n",
      "    epoch          : 460\n",
      "    loss           : -956447.1875\n",
      "    val_loss       : -950838.6078125\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -1027501.750000\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -940931.375000\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -935599.187500\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -928159.375000\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -927933.687500\n",
      "    epoch          : 461\n",
      "    loss           : -956536.2871287129\n",
      "    val_loss       : -949846.4703125\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -1025859.562500\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -943620.500000\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -931495.250000\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -926466.937500\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -926746.750000\n",
      "    epoch          : 462\n",
      "    loss           : -956682.6107673268\n",
      "    val_loss       : -950802.16484375\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -1026728.250000\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -930907.000000\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -934044.750000\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -1027651.750000\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -925325.875000\n",
      "    epoch          : 463\n",
      "    loss           : -956553.3100247525\n",
      "    val_loss       : -950464.8828125\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -1025309.250000\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -940542.250000\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -928065.437500\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -1026138.500000\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -932798.625000\n",
      "    epoch          : 464\n",
      "    loss           : -955977.5587871287\n",
      "    val_loss       : -950406.253125\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -1026656.687500\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -940965.750000\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -932082.875000\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -948858.875000\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -951581.375000\n",
      "    epoch          : 465\n",
      "    loss           : -956442.7172029703\n",
      "    val_loss       : -950682.25859375\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -1027584.500000\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -977177.937500\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -962260.000000\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -961507.500000\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -958407.250000\n",
      "    epoch          : 466\n",
      "    loss           : -955590.9387376237\n",
      "    val_loss       : -949473.2078125\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -1025719.875000\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -932044.000000\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -931196.625000\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -958670.375000\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -961038.937500\n",
      "    epoch          : 467\n",
      "    loss           : -955702.7617574257\n",
      "    val_loss       : -950517.24296875\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -1025781.437500\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -935204.250000\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -932697.750000\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -961520.000000\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -927293.125000\n",
      "    epoch          : 468\n",
      "    loss           : -955660.75\n",
      "    val_loss       : -950267.3828125\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -1026138.625000\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -942213.625000\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -947966.562500\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -932872.187500\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -950945.875000\n",
      "    epoch          : 469\n",
      "    loss           : -956505.7543316832\n",
      "    val_loss       : -950891.48359375\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -1026011.250000\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -944104.875000\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -933192.812500\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -960889.500000\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -949443.250000\n",
      "    epoch          : 470\n",
      "    loss           : -956577.8106435643\n",
      "    val_loss       : -950830.1984375\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -1026930.062500\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -934547.000000\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -961896.875000\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -932868.437500\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -949812.500000\n",
      "    epoch          : 471\n",
      "    loss           : -956829.8056930694\n",
      "    val_loss       : -950920.77265625\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -1024901.750000\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -935742.250000\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -932756.625000\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -950665.125000\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -960102.625000\n",
      "    epoch          : 472\n",
      "    loss           : -956880.1509900991\n",
      "    val_loss       : -950718.1578125\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -976294.937500\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -935160.687500\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -963162.687500\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -1027447.125000\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -947370.562500\n",
      "    epoch          : 473\n",
      "    loss           : -956694.6813118812\n",
      "    val_loss       : -950971.759375\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -1027060.250000\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -977322.250000\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -927061.562500\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -959488.187500\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -931793.500000\n",
      "    epoch          : 474\n",
      "    loss           : -956362.6689356435\n",
      "    val_loss       : -950892.31015625\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -1027640.000000\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -976658.750000\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -933161.625000\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -960103.312500\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -949235.375000\n",
      "    epoch          : 475\n",
      "    loss           : -956723.6732673268\n",
      "    val_loss       : -950641.68125\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -1027505.437500\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -943295.125000\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -927450.312500\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -1027170.500000\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -949407.375000\n",
      "    epoch          : 476\n",
      "    loss           : -956477.6379950495\n",
      "    val_loss       : -950996.9109375\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -1026766.625000\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -941840.125000\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -936413.062500\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -933185.500000\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -950977.750000\n",
      "    epoch          : 477\n",
      "    loss           : -956984.8205445545\n",
      "    val_loss       : -950645.73125\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -1027192.000000\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -964252.187500\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -929062.750000\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -932836.437500\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -959214.625000\n",
      "    epoch          : 478\n",
      "    loss           : -956714.3186881188\n",
      "    val_loss       : -950898.703125\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -1027273.250000\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -935250.187500\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -949334.875000\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -963784.000000\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -925205.250000\n",
      "    epoch          : 479\n",
      "    loss           : -956370.0136138614\n",
      "    val_loss       : -949891.83671875\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -1027390.625000\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -947967.187500\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -934387.500000\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -932639.812500\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -949972.437500\n",
      "    epoch          : 480\n",
      "    loss           : -956410.0334158416\n",
      "    val_loss       : -950198.24453125\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -1028164.000000\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -971175.000000\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -931562.125000\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -932433.562500\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -960494.875000\n",
      "    epoch          : 481\n",
      "    loss           : -956211.8292079208\n",
      "    val_loss       : -950454.49609375\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -1027401.250000\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -939970.562500\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -949635.937500\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -933710.875000\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -955761.500000\n",
      "    epoch          : 482\n",
      "    loss           : -955525.7537128713\n",
      "    val_loss       : -949425.06796875\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -1025213.125000\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -935076.625000\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -963040.062500\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -926553.625000\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -959458.625000\n",
      "    epoch          : 483\n",
      "    loss           : -955957.0129950495\n",
      "    val_loss       : -950644.91328125\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -1027047.875000\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -975081.000000\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -950306.875000\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -962474.687500\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -933050.000000\n",
      "    epoch          : 484\n",
      "    loss           : -956905.1540841584\n",
      "    val_loss       : -950876.7578125\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -1026011.875000\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -977490.250000\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -932888.125000\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -949367.062500\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -950016.562500\n",
      "    epoch          : 485\n",
      "    loss           : -956925.1732673268\n",
      "    val_loss       : -950010.575\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -976149.687500\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -975703.687500\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -964273.125000\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -932679.187500\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -960258.375000\n",
      "    epoch          : 486\n",
      "    loss           : -956953.3582920792\n",
      "    val_loss       : -950927.36328125\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -1025927.937500\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -941511.187500\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -938005.312500\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -946681.625000\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -957501.437500\n",
      "    epoch          : 487\n",
      "    loss           : -956232.176980198\n",
      "    val_loss       : -949626.25625\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -1025600.875000\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -941487.187500\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -933876.437500\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -1027171.500000\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -951067.312500\n",
      "    epoch          : 488\n",
      "    loss           : -956212.9040841584\n",
      "    val_loss       : -950588.88515625\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -978353.062500\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -941106.125000\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -961565.375000\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -1027025.000000\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -950654.250000\n",
      "    epoch          : 489\n",
      "    loss           : -956264.9474009901\n",
      "    val_loss       : -950572.39375\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -1027455.187500\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -942368.125000\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -929188.062500\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -931480.375000\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -932338.625000\n",
      "    epoch          : 490\n",
      "    loss           : -956915.7493811881\n",
      "    val_loss       : -950653.54609375\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -1027132.875000\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -943696.187500\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -932906.125000\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -948242.125000\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -927697.250000\n",
      "    epoch          : 491\n",
      "    loss           : -956160.4504950495\n",
      "    val_loss       : -950311.53828125\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -1026997.875000\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -946767.000000\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -931850.062500\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -1027441.812500\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -959124.750000\n",
      "    epoch          : 492\n",
      "    loss           : -956168.0006188119\n",
      "    val_loss       : -949412.8515625\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -1027129.000000\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -946698.187500\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -962057.437500\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -959368.312500\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -927641.437500\n",
      "    epoch          : 493\n",
      "    loss           : -956004.2698019802\n",
      "    val_loss       : -950620.71328125\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -977718.687500\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -943388.562500\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -930501.875000\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -932055.437500\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -931066.437500\n",
      "    epoch          : 494\n",
      "    loss           : -956056.5983910891\n",
      "    val_loss       : -950898.30703125\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -1026157.750000\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -941632.812500\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -934911.625000\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -933450.562500\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -928195.687500\n",
      "    epoch          : 495\n",
      "    loss           : -956982.9176980198\n",
      "    val_loss       : -951122.2234375\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -1028072.562500\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -943575.062500\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -937022.250000\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -1026007.562500\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -950342.500000\n",
      "    epoch          : 496\n",
      "    loss           : -957202.551980198\n",
      "    val_loss       : -950799.74453125\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -1027144.250000\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -937761.250000\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -962517.750000\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -959039.312500\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -949735.937500\n",
      "    epoch          : 497\n",
      "    loss           : -956978.7370049505\n",
      "    val_loss       : -950692.6734375\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -1027409.312500\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -962245.187500\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -963659.750000\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -931128.437500\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -928271.250000\n",
      "    epoch          : 498\n",
      "    loss           : -956856.5878712871\n",
      "    val_loss       : -950445.5234375\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -1026265.437500\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -960902.687500\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -931698.437500\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -949368.937500\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -927985.500000\n",
      "    epoch          : 499\n",
      "    loss           : -956934.0736386139\n",
      "    val_loss       : -950491.7015625\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -1027130.875000\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -972542.375000\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -962206.562500\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -1025182.500000\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -946282.625000\n",
      "    epoch          : 500\n",
      "    loss           : -954806.042079208\n",
      "    val_loss       : -948981.04921875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0806_094159/checkpoint-epoch500.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAECategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_0_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_1): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_2_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_3): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_4_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_5): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_6_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_7): LadderDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (noise_linear): Linear(in_features=3584, out_features=128, bias=True)\n",
       "      (ladder_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (ladder_linear): Linear(in_features=3584, out_features=128, bias=True)\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_8_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_9): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_10): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_10_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_11): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_11_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_12): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_12_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_13): LadderDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_13_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (noise_linear): Linear(in_features=3584, out_features=256, bias=True)\n",
       "      (ladder_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (ladder_linear): Linear(in_features=3584, out_features=256, bias=True)\n",
       "    )\n",
       "    (generator_14): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_14_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_15): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_15_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_16): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_16_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_17): LadderDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_17_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (noise_linear): Linear(in_features=3584, out_features=512, bias=True)\n",
       "      (ladder_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (ladder_linear): Linear(in_features=3584, out_features=512, bias=True)\n",
       "    )\n",
       "    (generator_18): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_18_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_19): LadderDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_19_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (noise_linear): Linear(in_features=3584, out_features=1024, bias=True)\n",
       "      (ladder_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (ladder_linear): Linear(in_features=3584, out_features=1024, bias=True)\n",
       "    )\n",
       "    (generator_20): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_20_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_21): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_21_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_22): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_22_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_23): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_23_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=512, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_24): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=784, bias=True)\n",
       "        (1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (4): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_24_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (4): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=784, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "    (global_element_5): StandardNormal()\n",
       "  )\n",
       "  (guide_embedding): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (guide_confidences): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_distances): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=35, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGAAAAEuCAYAAABxvReQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAANwklEQVR4nO3dbUxTdxvH8d9f6DisTZTReguKLzY3eaVDtgoOVpJlGRAyAjIZyRTNZqaLLCEuYW2c4rIsZsSHFzOZrzRsbDyqMbomaLIFR3Bse7EV1uEeICE4KXcyoN60UOC6X6jnXgdKiy0XeF+fpAk9/Z/+T/vtOaeIBUVEEHyWcW/A/zsJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwiw1ncHx8/E2/3/+vaG3Mw0TTtEGfz7dqrnEqnB/KK6VIfogfGqUUiEjNNU4OQcwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkALOHIkBfXx8sFgtycnKQk5ODoaEhjIyMwGq1wmQyoaurSx/b0dGBzMxM2Gw2lJWVIRAIMG45ACIK+XJ7+OLT29tLW7duDVo2MTFBHo+HysvLyeVy6csHBgZobGyMiIgcDgc1NTVFZZvuPFdzPqeLcg+w2+04ceJEWOu0t7cjOzsbDocDRASDwQCLxTJjXHJyMuLj4wEAsbGxWLbs9lNgtVrR3d394BsfrlAq0QLuAR6Ph5KTk/VXKRHR4cOHyWg0Bl3i4uIIANXX15Pf76dbt27R9PQ0vf7669Tc3Kyv+8894K4//viDnn32WRofHyciooaGBiouLo7Y40CIe8CiC/DRRx/RG2+8cd8xXq+XNm/eTPn5+TQxMRF026VLl+i9997Tr88WYGRkhLKzs+mXX37Rl/l8PkpISKAbN25E4FEs8kNQXV0dtmzZgtLSUqxatQopKSlwOp0AAKfTCZvNds91fT4fCgoKYDQa0dLSAoPBgNHRUf32q1evYt26dfdcf3JyEmVlZaiursb69ev15ZqmIT09Ha2trRF4hGEIpRJFeA+oqqoiTdOooaGBJiYmqKamhtauXUtERGazmTo7O2ddb3x8nHJzcykzM5O8Xq++/MKFC7Rp0ybKysqi7du3UyAQICKivLw8SkpKooyMDDp9+jQREdXW1lJiYiLZbDay2WxUX1+v309FRQVVVlZG5DFiMR+C8vPzyW6369cHBwcJAPl8PoqNjSW32z1jnUAgQEVFRZSWlkbDw8MR2Y5/cjgctGvXrojcV6gBWA5BLpcLJSUl+nWPxwOTyQRN05CQkACv1xs0fnp6Gjt37kRPTw9aW1uxfPnyqGyX1+vFihUronLf97LgAYaHh9Hf3x/0FrG5uRl5eXkAgA0bNuD69etB6+zduxfXrl3DlStXYDabo7ZtbrcbGzdujNr9zyqU3YQieAhqa2ujmJgYOnLkCAUCAbp48SJZLBbq7u4mIqKjR4/S7t279fGVlZWUkpJCfX19Dzz3/fj9fkpISKCBgYGI3B8W6zng5MmTtGPHDiosLCSTyUTp6enU3t6u3z40NESrV6+msbExcrlcBIAMBsOM7wNWrlxJU1NTD7w9dzU2NlJRUVHE7m/RBtizZw8dO3bsvmPsdjsdP378gecKh9VqnfUbtvlatAGee+45cjqdD3w/i12oARb8JNzV1YXU1NSFnnbRkk/KR4l8Un6JkADMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkALOw/pShpmmDSin5U4Yh0DRtMJRxYX1AY7FRSqUCOE9ES/YjN3IIYiYBmEkAZhKAmQRgJgGYSQBmEoCZBGAmAZhJAGYSgJkEYCYBmEkAAEqpfUqp75VS40qpMws5d1g/kHmI3QDwAYCXAMQv5MQSAAARnQUApdQzANYs5NxyCGImAZhJAGYSgJmchAEopWJx+7mIARCjlNIATBLRZLTnlj3gtgMAfADeBfDana8PLMTE8v+CmMkewEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkALOlHuDfAD7m3ogHEdZvzIqPj7/p9/vlTxmGQNO0QZ/Pt2qucWEFUErRUv4VZwtJKQUiUnONW+qHoCVPAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjBbNAG++eYb5OTkICcnB0899RQqKysxPT2NnTt3Ijs7G1lZWXC73UHrfPHFF7BYLPOes6+vDxaLRZ93aGgIADAyMgKr1QqTyYSuri4AQEdHBzIzM2Gz2VBWVoZAIDD/B/t3RBTy5fbw6CsvL6evv/6afvjhB3r11VeJiKitrY12796tj5mamqLi4mJKS0ub9zy9vb20devWGcsnJibI4/FQeXk5uVwuIiIaGBigsbExIiJyOBzU1NR03/u+81zN+ZxGbQ+w2+04ceJE2OsFAgF0dnYiOzsba9asQUxMDIgIf/31F8xmsz7u888/R0lJCZYt+99DqK6uRnV1dVjztbe3Izs7Gw6H4+6LDAaDYcaelZycjPj4eABAbGxs0LxWqxXd3d3hPlQAUToEDQ0Noba2Fm+++aa+7Pfff4fRaMSff/6pL6urq0NycjL6+/v1ZZcvX8YLL7yAZcuWwWw2Iy4uDqmpqaioqMBbb70FAJiamkJjYyNKS0vvux3vv/8+TCZT0EXTNCil0NDQgKSkJPz2229oa2uDx+PB2bNn53xsvb29cDqdKCgo0Je98847OHjwYMjPz99FJcCZM2eQn5+vv2IA4IknnkBBQYG+V3R0dGDfvn04f/48UlJS9HFNTU145ZVXAACtra2Ynp5GT08PWlpasH//fgDAZ599hm3btgW9Cmdz8OBB3Lp1S7/cvHkTTz/9NPLz81FcXIy4uDgYjUYopVBcXIwff/zxvvc3OjqK8vJyfPrpp3jkkUf05S+//DK++uqroBdXqOYdoK6uDlu2bEFpaSlWrVqFlJQUOJ1OAIDT6YTNZpuxTlVVFU6dOoWuri4UFxfjk08+gdVq1W8PBAL47rvvkJWVBeD2+SkxMREAYDabMTIyAgD4+eefUVtbi9zcXPz66694++2359xen8+HgoICGI1GtLS0wGAwYHR0VL/96tWrWLdu3T3Xn5ycRFlZGaqrq7F+/fqg2zRNQ3p6OlpbW+fcjhlCOVHQLCfhqqoq0jSNGhoaaGJigmpqamjt2rVERGQ2m6mzs3PWk9OLL75Ijz76KB0+fHjGbV9++SVVVFTo1wOBAG3bto2ef/552rx5M7W3t89YJz09Xf/60KFDdOjQoRljxsfHKTc3lzIzM8nr9erLL1y4QJs2baKsrCzavn07BQIB/ba8vDxKSkqijIwMOn36NNXW1lJiYiLZbDay2WxUX18fNEdFRQVVVlbq1xHiSXjeAfLz88lut+vXBwcHCQD5fD6KjY0lt9s944mYmpqi3NxcMplM5Pf7Z9z+oGYLEAgEqKioiNLS0mh4eDjic97lcDho165d+vVQA8z7EORyuVBSUqJf93g8+kkuISEBXq93xjr79+/H8PAwnnzySdTV1c136pDd/T6ip6cHra2tWL58edTm8nq9WLFiRdjrzSvA8PAw+vv7g96qNTc3Iy8vDwCwYcMGXL9+PWidU6dO4dy5czh//jyqqqpQU1Ojv+2Llr179+LatWu4cuVK0FvYaHC73di4cWP4K4aym9A/DkFtbW0UExNDR44coUAgQBcvXiSLxULd3d1ERHT06NGgb5ouX75Mjz32GP30009ERDQ5OUmPP/44nTt3LqKHgb8fgiorKyklJYX6+voiOsds/H4/JSQk0MDAgL4M0TwHnDx5knbs2EGFhYVkMpkoPT096AQ5NDREq1evprGxMXK73ZSYmEiXLl0K2uiPP/6YMjIyIvpE3A3gcrkIABkMBjIajUGXlStX0tTUVETnbWxspKKioqBlUQ2wZ88eOnbs2H03ym630/HjxyPx+EJ2r3dB0Wa1WvV/srgr1ACx8zneuVwuFBYW3nfMhx9+OJ+7XpK+/fbbea87rwBdXV1ITU2d96TRkpOTw70JYZNPykeJfFJ+iZAAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMwvp8gKZpg0op+Ut6IdA0bTCUcWF9PmCxUUqlAjhPRIvv0yIhkkMQMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwAAlFL7lFLfK6XGlVJnFnLuef3GrIfQDQAfAHgJQPwcYyNKAgAgorMAoJR6BsCahZxbDkHMJAAzCcBMAjCTkzAApVQsbj8XMQBilFIagEkimoz23LIH3HYAgA/AuwBeu/P1gYWYWP5jFjPZA5hJAGYSgJkEYCYBmEkAZhKAmQRgJgGYSQBmSz3AfwC0cW/Eg1jS/xb0MFjqe8CSJwGYSQBmEoCZBGAmAZhJAGYSgJkEYCYBmEkAZhKAmQRgJgGYSQBmEoCZBGAmAZhJAGb/BRu1hD9XOoC4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGAAAAEuCAYAAABxvReQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAANwklEQVR4nO3dbUxTdxvH8d9f6DisTZTReguKLzY3eaVDtgoOVpJlGRAyAjIZyRTNZqaLLCEuYW2c4rIsZsSHFzOZrzRsbDyqMbomaLIFR3Bse7EV1uEeICE4KXcyoN60UOC6X6jnXgdKiy0XeF+fpAk9/Z/+T/vtOaeIBUVEEHyWcW/A/zsJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwiw1ncHx8/E2/3/+vaG3Mw0TTtEGfz7dqrnEqnB/KK6VIfogfGqUUiEjNNU4OQcwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkALOHIkBfXx8sFgtycnKQk5ODoaEhjIyMwGq1wmQyoaurSx/b0dGBzMxM2Gw2lJWVIRAIMG45ACIK+XJ7+OLT29tLW7duDVo2MTFBHo+HysvLyeVy6csHBgZobGyMiIgcDgc1NTVFZZvuPFdzPqeLcg+w2+04ceJEWOu0t7cjOzsbDocDRASDwQCLxTJjXHJyMuLj4wEAsbGxWLbs9lNgtVrR3d394BsfrlAq0QLuAR6Ph5KTk/VXKRHR4cOHyWg0Bl3i4uIIANXX15Pf76dbt27R9PQ0vf7669Tc3Kyv+8894K4//viDnn32WRofHyciooaGBiouLo7Y40CIe8CiC/DRRx/RG2+8cd8xXq+XNm/eTPn5+TQxMRF026VLl+i9997Tr88WYGRkhLKzs+mXX37Rl/l8PkpISKAbN25E4FEs8kNQXV0dtmzZgtLSUqxatQopKSlwOp0AAKfTCZvNds91fT4fCgoKYDQa0dLSAoPBgNHRUf32q1evYt26dfdcf3JyEmVlZaiursb69ev15ZqmIT09Ha2trRF4hGEIpRJFeA+oqqoiTdOooaGBJiYmqKamhtauXUtERGazmTo7O2ddb3x8nHJzcykzM5O8Xq++/MKFC7Rp0ybKysqi7du3UyAQICKivLw8SkpKooyMDDp9+jQREdXW1lJiYiLZbDay2WxUX1+v309FRQVVVlZG5DFiMR+C8vPzyW6369cHBwcJAPl8PoqNjSW32z1jnUAgQEVFRZSWlkbDw8MR2Y5/cjgctGvXrojcV6gBWA5BLpcLJSUl+nWPxwOTyQRN05CQkACv1xs0fnp6Gjt37kRPTw9aW1uxfPnyqGyX1+vFihUronLf97LgAYaHh9Hf3x/0FrG5uRl5eXkAgA0bNuD69etB6+zduxfXrl3DlStXYDabo7ZtbrcbGzdujNr9zyqU3YQieAhqa2ujmJgYOnLkCAUCAbp48SJZLBbq7u4mIqKjR4/S7t279fGVlZWUkpJCfX19Dzz3/fj9fkpISKCBgYGI3B8W6zng5MmTtGPHDiosLCSTyUTp6enU3t6u3z40NESrV6+msbExcrlcBIAMBsOM7wNWrlxJU1NTD7w9dzU2NlJRUVHE7m/RBtizZw8dO3bsvmPsdjsdP378gecKh9VqnfUbtvlatAGee+45cjqdD3w/i12oARb8JNzV1YXU1NSFnnbRkk/KR4l8Un6JkADMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkALOw/pShpmmDSin5U4Yh0DRtMJRxYX1AY7FRSqUCOE9ES/YjN3IIYiYBmEkAZhKAmQRgJgGYSQBmEoCZBGAmAZhJAGYSgJkEYCYBmEkAAEqpfUqp75VS40qpMws5d1g/kHmI3QDwAYCXAMQv5MQSAAARnQUApdQzANYs5NxyCGImAZhJAGYSgJmchAEopWJx+7mIARCjlNIATBLRZLTnlj3gtgMAfADeBfDana8PLMTE8v+CmMkewEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkALOlHuDfAD7m3ogHEdZvzIqPj7/p9/vlTxmGQNO0QZ/Pt2qucWEFUErRUv4VZwtJKQUiUnONW+qHoCVPAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjBbNAG++eYb5OTkICcnB0899RQqKysxPT2NnTt3Ijs7G1lZWXC73UHrfPHFF7BYLPOes6+vDxaLRZ93aGgIADAyMgKr1QqTyYSuri4AQEdHBzIzM2Gz2VBWVoZAIDD/B/t3RBTy5fbw6CsvL6evv/6afvjhB3r11VeJiKitrY12796tj5mamqLi4mJKS0ub9zy9vb20devWGcsnJibI4/FQeXk5uVwuIiIaGBigsbExIiJyOBzU1NR03/u+81zN+ZxGbQ+w2+04ceJE2OsFAgF0dnYiOzsba9asQUxMDIgIf/31F8xmsz7u888/R0lJCZYt+99DqK6uRnV1dVjztbe3Izs7Gw6H4+6LDAaDYcaelZycjPj4eABAbGxs0LxWqxXd3d3hPlQAUToEDQ0Noba2Fm+++aa+7Pfff4fRaMSff/6pL6urq0NycjL6+/v1ZZcvX8YLL7yAZcuWwWw2Iy4uDqmpqaioqMBbb70FAJiamkJjYyNKS0vvux3vv/8+TCZT0EXTNCil0NDQgKSkJPz2229oa2uDx+PB2bNn53xsvb29cDqdKCgo0Je98847OHjwYMjPz99FJcCZM2eQn5+vv2IA4IknnkBBQYG+V3R0dGDfvn04f/48UlJS9HFNTU145ZVXAACtra2Ynp5GT08PWlpasH//fgDAZ599hm3btgW9Cmdz8OBB3Lp1S7/cvHkTTz/9NPLz81FcXIy4uDgYjUYopVBcXIwff/zxvvc3OjqK8vJyfPrpp3jkkUf05S+//DK++uqroBdXqOYdoK6uDlu2bEFpaSlWrVqFlJQUOJ1OAIDT6YTNZpuxTlVVFU6dOoWuri4UFxfjk08+gdVq1W8PBAL47rvvkJWVBeD2+SkxMREAYDabMTIyAgD4+eefUVtbi9zcXPz66694++2359xen8+HgoICGI1GtLS0wGAwYHR0VL/96tWrWLdu3T3Xn5ycRFlZGaqrq7F+/fqg2zRNQ3p6OlpbW+fcjhlCOVHQLCfhqqoq0jSNGhoaaGJigmpqamjt2rVERGQ2m6mzs3PWk9OLL75Ijz76KB0+fHjGbV9++SVVVFTo1wOBAG3bto2ef/552rx5M7W3t89YJz09Xf/60KFDdOjQoRljxsfHKTc3lzIzM8nr9erLL1y4QJs2baKsrCzavn07BQIB/ba8vDxKSkqijIwMOn36NNXW1lJiYiLZbDay2WxUX18fNEdFRQVVVlbq1xHiSXjeAfLz88lut+vXBwcHCQD5fD6KjY0lt9s944mYmpqi3NxcMplM5Pf7Z9z+oGYLEAgEqKioiNLS0mh4eDjic97lcDho165d+vVQA8z7EORyuVBSUqJf93g8+kkuISEBXq93xjr79+/H8PAwnnzySdTV1c136pDd/T6ip6cHra2tWL58edTm8nq9WLFiRdjrzSvA8PAw+vv7g96qNTc3Iy8vDwCwYcMGXL9+PWidU6dO4dy5czh//jyqqqpQU1Ojv+2Llr179+LatWu4cuVK0FvYaHC73di4cWP4K4aym9A/DkFtbW0UExNDR44coUAgQBcvXiSLxULd3d1ERHT06NGgb5ouX75Mjz32GP30009ERDQ5OUmPP/44nTt3LqKHgb8fgiorKyklJYX6+voiOsds/H4/JSQk0MDAgL4M0TwHnDx5knbs2EGFhYVkMpkoPT096AQ5NDREq1evprGxMXK73ZSYmEiXLl0K2uiPP/6YMjIyIvpE3A3gcrkIABkMBjIajUGXlStX0tTUVETnbWxspKKioqBlUQ2wZ88eOnbs2H03ym630/HjxyPx+EJ2r3dB0Wa1WvV/srgr1ACx8zneuVwuFBYW3nfMhx9+OJ+7XpK+/fbbea87rwBdXV1ITU2d96TRkpOTw70JYZNPykeJfFJ+iZAAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMwvp8gKZpg0op+Ut6IdA0bTCUcWF9PmCxUUqlAjhPRIvv0yIhkkMQMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwAAlFL7lFLfK6XGlVJnFnLuef3GrIfQDQAfAHgJQPwcYyNKAgAgorMAoJR6BsCahZxbDkHMJAAzCcBMAjCTkzAApVQsbj8XMQBilFIagEkimoz23LIH3HYAgA/AuwBeu/P1gYWYWP5jFjPZA5hJAGYSgJkEYCYBmEkAZhKAmQRgJgGYSQBmSz3AfwC0cW/Eg1jS/xb0MFjqe8CSJwGYSQBmEoCZBGAmAZhJAGYSgJkEYCYBmEkAZhKAmQRgJgGYSQBmEoCZBGAmAZhJAGb/BRu1hD9XOoC4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGAAAAEuCAYAAABxvReQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAANwklEQVR4nO3dbUxTdxvH8d9f6DisTZTReguKLzY3eaVDtgoOVpJlGRAyAjIZyRTNZqaLLCEuYW2c4rIsZsSHFzOZrzRsbDyqMbomaLIFR3Bse7EV1uEeICE4KXcyoN60UOC6X6jnXgdKiy0XeF+fpAk9/Z/+T/vtOaeIBUVEEHyWcW/A/zsJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwiw1ncHx8/E2/3/+vaG3Mw0TTtEGfz7dqrnEqnB/KK6VIfogfGqUUiEjNNU4OQcwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkALOHIkBfXx8sFgtycnKQk5ODoaEhjIyMwGq1wmQyoaurSx/b0dGBzMxM2Gw2lJWVIRAIMG45ACIK+XJ7+OLT29tLW7duDVo2MTFBHo+HysvLyeVy6csHBgZobGyMiIgcDgc1NTVFZZvuPFdzPqeLcg+w2+04ceJEWOu0t7cjOzsbDocDRASDwQCLxTJjXHJyMuLj4wEAsbGxWLbs9lNgtVrR3d394BsfrlAq0QLuAR6Ph5KTk/VXKRHR4cOHyWg0Bl3i4uIIANXX15Pf76dbt27R9PQ0vf7669Tc3Kyv+8894K4//viDnn32WRofHyciooaGBiouLo7Y40CIe8CiC/DRRx/RG2+8cd8xXq+XNm/eTPn5+TQxMRF026VLl+i9997Tr88WYGRkhLKzs+mXX37Rl/l8PkpISKAbN25E4FEs8kNQXV0dtmzZgtLSUqxatQopKSlwOp0AAKfTCZvNds91fT4fCgoKYDQa0dLSAoPBgNHRUf32q1evYt26dfdcf3JyEmVlZaiursb69ev15ZqmIT09Ha2trRF4hGEIpRJFeA+oqqoiTdOooaGBJiYmqKamhtauXUtERGazmTo7O2ddb3x8nHJzcykzM5O8Xq++/MKFC7Rp0ybKysqi7du3UyAQICKivLw8SkpKooyMDDp9+jQREdXW1lJiYiLZbDay2WxUX1+v309FRQVVVlZG5DFiMR+C8vPzyW6369cHBwcJAPl8PoqNjSW32z1jnUAgQEVFRZSWlkbDw8MR2Y5/cjgctGvXrojcV6gBWA5BLpcLJSUl+nWPxwOTyQRN05CQkACv1xs0fnp6Gjt37kRPTw9aW1uxfPnyqGyX1+vFihUronLf97LgAYaHh9Hf3x/0FrG5uRl5eXkAgA0bNuD69etB6+zduxfXrl3DlStXYDabo7ZtbrcbGzdujNr9zyqU3YQieAhqa2ujmJgYOnLkCAUCAbp48SJZLBbq7u4mIqKjR4/S7t279fGVlZWUkpJCfX19Dzz3/fj9fkpISKCBgYGI3B8W6zng5MmTtGPHDiosLCSTyUTp6enU3t6u3z40NESrV6+msbExcrlcBIAMBsOM7wNWrlxJU1NTD7w9dzU2NlJRUVHE7m/RBtizZw8dO3bsvmPsdjsdP378gecKh9VqnfUbtvlatAGee+45cjqdD3w/i12oARb8JNzV1YXU1NSFnnbRkk/KR4l8Un6JkADMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkALOw/pShpmmDSin5U4Yh0DRtMJRxYX1AY7FRSqUCOE9ES/YjN3IIYiYBmEkAZhKAmQRgJgGYSQBmEoCZBGAmAZhJAGYSgJkEYCYBmEkAAEqpfUqp75VS40qpMws5d1g/kHmI3QDwAYCXAMQv5MQSAAARnQUApdQzANYs5NxyCGImAZhJAGYSgJmchAEopWJx+7mIARCjlNIATBLRZLTnlj3gtgMAfADeBfDana8PLMTE8v+CmMkewEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkALOlHuDfAD7m3ogHEdZvzIqPj7/p9/vlTxmGQNO0QZ/Pt2qucWEFUErRUv4VZwtJKQUiUnONW+qHoCVPAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjBbNAG++eYb5OTkICcnB0899RQqKysxPT2NnTt3Ijs7G1lZWXC73UHrfPHFF7BYLPOes6+vDxaLRZ93aGgIADAyMgKr1QqTyYSuri4AQEdHBzIzM2Gz2VBWVoZAIDD/B/t3RBTy5fbw6CsvL6evv/6afvjhB3r11VeJiKitrY12796tj5mamqLi4mJKS0ub9zy9vb20devWGcsnJibI4/FQeXk5uVwuIiIaGBigsbExIiJyOBzU1NR03/u+81zN+ZxGbQ+w2+04ceJE2OsFAgF0dnYiOzsba9asQUxMDIgIf/31F8xmsz7u888/R0lJCZYt+99DqK6uRnV1dVjztbe3Izs7Gw6H4+6LDAaDYcaelZycjPj4eABAbGxs0LxWqxXd3d3hPlQAUToEDQ0Noba2Fm+++aa+7Pfff4fRaMSff/6pL6urq0NycjL6+/v1ZZcvX8YLL7yAZcuWwWw2Iy4uDqmpqaioqMBbb70FAJiamkJjYyNKS0vvux3vv/8+TCZT0EXTNCil0NDQgKSkJPz2229oa2uDx+PB2bNn53xsvb29cDqdKCgo0Je98847OHjwYMjPz99FJcCZM2eQn5+vv2IA4IknnkBBQYG+V3R0dGDfvn04f/48UlJS9HFNTU145ZVXAACtra2Ynp5GT08PWlpasH//fgDAZ599hm3btgW9Cmdz8OBB3Lp1S7/cvHkTTz/9NPLz81FcXIy4uDgYjUYopVBcXIwff/zxvvc3OjqK8vJyfPrpp3jkkUf05S+//DK++uqroBdXqOYdoK6uDlu2bEFpaSlWrVqFlJQUOJ1OAIDT6YTNZpuxTlVVFU6dOoWuri4UFxfjk08+gdVq1W8PBAL47rvvkJWVBeD2+SkxMREAYDabMTIyAgD4+eefUVtbi9zcXPz66694++2359xen8+HgoICGI1GtLS0wGAwYHR0VL/96tWrWLdu3T3Xn5ycRFlZGaqrq7F+/fqg2zRNQ3p6OlpbW+fcjhlCOVHQLCfhqqoq0jSNGhoaaGJigmpqamjt2rVERGQ2m6mzs3PWk9OLL75Ijz76KB0+fHjGbV9++SVVVFTo1wOBAG3bto2ef/552rx5M7W3t89YJz09Xf/60KFDdOjQoRljxsfHKTc3lzIzM8nr9erLL1y4QJs2baKsrCzavn07BQIB/ba8vDxKSkqijIwMOn36NNXW1lJiYiLZbDay2WxUX18fNEdFRQVVVlbq1xHiSXjeAfLz88lut+vXBwcHCQD5fD6KjY0lt9s944mYmpqi3NxcMplM5Pf7Z9z+oGYLEAgEqKioiNLS0mh4eDjic97lcDho165d+vVQA8z7EORyuVBSUqJf93g8+kkuISEBXq93xjr79+/H8PAwnnzySdTV1c136pDd/T6ip6cHra2tWL58edTm8nq9WLFiRdjrzSvA8PAw+vv7g96qNTc3Iy8vDwCwYcMGXL9+PWidU6dO4dy5czh//jyqqqpQU1Ojv+2Llr179+LatWu4cuVK0FvYaHC73di4cWP4K4aym9A/DkFtbW0UExNDR44coUAgQBcvXiSLxULd3d1ERHT06NGgb5ouX75Mjz32GP30009ERDQ5OUmPP/44nTt3LqKHgb8fgiorKyklJYX6+voiOsds/H4/JSQk0MDAgL4M0TwHnDx5knbs2EGFhYVkMpkoPT096AQ5NDREq1evprGxMXK73ZSYmEiXLl0K2uiPP/6YMjIyIvpE3A3gcrkIABkMBjIajUGXlStX0tTUVETnbWxspKKioqBlUQ2wZ88eOnbs2H03ym630/HjxyPx+EJ2r3dB0Wa1WvV/srgr1ACx8zneuVwuFBYW3nfMhx9+OJ+7XpK+/fbbea87rwBdXV1ITU2d96TRkpOTw70JYZNPykeJfFJ+iZAAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwDMwvp8gKZpg0op+Ut6IdA0bTCUcWF9PmCxUUqlAjhPRIvv0yIhkkMQMwnATAIwkwDMJAAzCcBMAjCTAMwkADMJwEwCMJMAzCQAMwnATAIwkwAAlFL7lFLfK6XGlVJnFnLuef3GrIfQDQAfAHgJQPwcYyNKAgAgorMAoJR6BsCahZxbDkHMJAAzCcBMAjCTkzAApVQsbj8XMQBilFIagEkimoz23LIH3HYAgA/AuwBeu/P1gYWYWP5jFjPZA5hJAGYSgJkEYCYBmEkAZhKAmQRgJgGYSQBmSz3AfwC0cW/Eg1jS/xb0MFjqe8CSJwGYSQBmEoCZBGAmAZhJAGYSgJkEYCYBmEkAZhKAmQRgJgGYSQBmEoCZBGAmAZhJAGb/BRu1hD9XOoC4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAR7ElEQVR4nO3dfbBcdX3H8feHm0uQJNREQohJBORBjTwEvIKKQ6M8GNJpwU51SH0IHTR0Rqy21OrQdoydWhjHJypKJ0CGBBClAgVbUDAUM5QRuUCEYKhJY0KeTAIRCZGH5ObbP/ZcZ7nunr139+yevff3ec3s7O75nrPnuyf3k3N2zzl7FBGY2dh3QNkNmFlnOOxmiXDYzRLhsJslwmE3S4TDbpYIh32MkbRY0o1NTvsmSY9J2i3pr4rurWiS3iDpBUk9ZfcyGjjsBZH0bkkPSvqNpF2S/kfS28vua4T+Drg/IiZFxL+W3UwjEfF0REyMiIGyexkNHPYCSDoE+E/gG8AUYAbwBeDlMvtqwhHAk/WK3bQGlTSuzOlHI4e9GMcBRMTNETEQES9GxD0R8TiApKMl3SfpWUnPSLpJ0msHJ5a0QdJnJD0uaY+k6yRNk3R3tkn9I0mTs3GPlBSSFknaKmmbpEvrNSbpHdkWx3OSfiZpbp3x7gPeA1yVbRofJ+l6SVdLukvSHuA9kv5A0nJJOyVtlPQPkg7IXuPCbIvma9n81kt6VzZ8k6Qdkhbm9Hq/pMsl/TTbQrpD0pQh7/siSU8D91UNG5eN83pJd2ZbVuskfbzqtRdL+p6kGyU9D1w4rH/ZsSQifGvxBhwCPAssA84FJg+pHwOcDYwHpgIrga9X1TcAPwGmUdkq2AE8CpycTXMf8Pls3COBAG4GJgAnADuBs7L6YuDG7PGMrK/5VP5jPzt7PrXO+7gf+FjV8+uB3wCnZ9MfBCwH7gAmZb38ArgoG/9CYB/wF0AP8M/A08A3s/dxDrAbmJgz/y3A8dl7u7XqvQy+7+VZ7TVVw8Zl4/wY+FbW55xsuZxZtVz2Audn7+U1Zf/ddPzvtOwGxsoNeEsWjs3ZH/ydwLQ6454PPFb1fAPwoarntwJXVz3/JPAf2ePBP/A3V9W/BFyXPa4O+2eBG4bM+4fAwjp91Qr78qrnPVQ+msyuGnYxlc/5g2FfW1U7Iet1WtWwZ4E5OfO/our5bOCVbL6D7/uNVfXfhR2YBQwAk6rqlwPXVy2XlWX/nZR582Z8QSJiTURcGBEzqayZXg98HUDSYZK+I2lLtgl5I3DokJfYXvX4xRrPJw4Zf1PV443Z/IY6AvhAtkn9nKTngHcD00fw1qrncyhwYDa/6nnPqHo+tG8iotF7qTe/jUAvr15Wm6jt9cCuiNid01u9aZPgsLdBRDxFZa14fDbociproBMj4hDgw4BanM2sqsdvALbWGGcTlTX7a6tuEyLiihHMp/q0yGeobAofMWTeW0bweo0MfV97s/nW6qfaVmCKpEk5vSV9iqfDXgBJb5Z0qaSZ2fNZwAIqn8Oh8vn2BeA5STOAzxQw23+UdLCkt1L5jPzdGuPcCPyxpPdJ6pF0kKS5g32OVFR2cd0CfFHSJElHAH+TzacoH5Y0W9LBwD8B34th7FqLiE3Ag8Dl2fs8EbgIuKnA3kY1h70Yu4HTgIeyb61/AqwGBr8l/wJwCpUvu/4LuK2Aef4YWAesAL4cEfcMHSELwHnAZVS+rNpE5T+aVv7dPwnsAdYDDwDfBpa28HpD3UBlq+hXVL5oG8nBPQuofI7fCtxO5UvNewvsbVRT9uWFjRKSjgR+CfRGxL5yuymWpPupfLl4bdm9jEVes5slwmE3S4Q3480S4TW7WSI6ejLAgRofBzGhk7M0S8pL7OGVeLnmMRytnjk0D7iSyuGM1zY6WOMgJnCazmxllmaW46FYUbfW9GZ8drrjN6mc+DEbWCBpdrOvZ2bt1cpn9lOBdRGxPiJeAb5D5QAOM+tCrYR9Bq8+sWAzrz7pAIDsvOt+Sf17R91vOZiNHa2EvdaXAL+3Hy8ilkREX0T09TK+hdmZWStaCftmXn2G0kxqn3llZl2glbA/DBwr6ShJBwIXUPnBBjPrQk3veouIfZIuofLLJz3A0oio+2OFZlaulvazR8RdwF0F9WJmbeTDZc0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiWrpks6QNwG5gANgXEX1FNGVmxWsp7Jn3RMQzBbyOmbWRN+PNEtFq2AO4R9IjkhbVGkHSIkn9kvr38nKLszOzZrW6GX96RGyVdBhwr6SnImJl9QgRsQRYAnCIpkSL8zOzJrW0Zo+Irdn9DuB24NQimjKz4jUddkkTJE0afAycA6wuqjEzK1Yrm/HTgNslDb7OtyPiB4V0ZYXZ82en5dbf9LdP5tb/bdaPc+vPDLyYW5+77DN1a0d/5ee50w785vncOuFPhSPRdNgjYj1wUoG9mFkbedebWSIcdrNEOOxmiXDYzRLhsJslQtHB3ReHaEqcpjM7Nr9U/PW6NXVr8w72Icq1nDv/z3Pr+1fl7xbsVg/FCp6PXapV85rdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0tEET84aW228y/fmVufd/CqDnUydtx917dz6/P+6EO59Xgs/9RgVHNX9+/86lP1/00P//qD+a/dJK/ZzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNE+Hz2UeCHW9u3H/1P152dW99zxs62zbvdBuaeUre2aMmtudPu2jcxt/799x6fW9+3I/9apz1TX1e3NrB9R+60eXw+u5k57GapcNjNEuGwmyXCYTdLhMNulgiH3SwRPp+9C/RMndq2135mYE9ufTTvR2+k5/5H69au2XRG7rT7v3hYbn3crx5pqqdBrexLb1bDNbukpZJ2SFpdNWyKpHslrc3uJ7e3TTNr1XA2468H5g0Z9jlgRUQcC6zInptZF2sY9ohYCewaMvg8YFn2eBlwfsF9mVnBmv2CblpEbAPI7ut+wJG0SFK/pP69+LpjZmVp+7fxEbEkIvoioq+X8e2enZnV0WzYt0uaDpDdd/6rRTMbkWbDfiewMHu8ELijmHbMrF0a7meXdDMwFzhU0mbg88AVwC2SLgKeBj7QzibHujtW/aDBGD1Nv/aHj/rDBmPsa/q1u94B9Zdb7wfzjz8YeLa1/ejdqGHYI2JBnZJ/hcJsFPHhsmaJcNjNEuGwmyXCYTdLhMNulgif4toFetX8rrVGXnrfybn18XfXPw0UYNwRM3PrR//71tz6Y/9S/+ecJ933VO60+198KbceJx2XW+fh1XVLA88OPd1j7POa3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhPezj3H3X3NNi6/Q4qmeVz3c4vzrO+r7J+bWj/tp5y5HPhp4zW6WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcL72S3XQOzPrfeovPXFzB96XTUSXlpmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSK8n70L/Hrgt7n1yT0H59bz9oXPn1H/d9vL9q2ND+TWj+6dmFv/0Teuyq3/yW1vH3FPY1nDNbukpZJ2SFpdNWyxpC2SVmW3+e1t08xaNZzN+OuBeTWGfy0i5mS3u4pty8yK1jDsEbESSO9aOWZjTCtf0F0i6fFsM39yvZEkLZLUL6l/Ly+3MDsza0WzYb8aOBqYA2wDvlJvxIhYEhF9EdHXy/gmZ2dmrWoq7BGxPSIGImI/cA1warFtmVnRmgq7pOlVT98P1L82rpl1hYb72SXdDMwFDpW0Gfg8MFfSHCCADcDFbexxzLtg1rvKbqEUl7zprNz63et/klsfr94i2xnzGoY9IhbUGHxdG3oxszby4bJmiXDYzRLhsJslwmE3S4TDbpYIn+Jqpdn/0ktlt5AUr9nNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0T4fHYrzYSVU8tuISles5slwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiRjOJZtnAcuBw4H9wJKIuFLSFOC7wJFULtv8wYj4dftaHb00Ln8x68ADc+v7f/vbItvpqI23nFC39tQxN7T02h/deEaDMZ5v6fXHmuGs2fcBl0bEW4B3AJ+QNBv4HLAiIo4FVmTPzaxLNQx7RGyLiEezx7uBNcAM4DxgWTbaMuD8djVpZq0b0Wd2SUcCJwMPAdMiYhtU/kMADiu6OTMrzrDDLmkicCvw6YgY9ochSYsk9Uvq38vLzfRoZgUYVtgl9VIJ+k0RcVs2eLuk6Vl9OrCj1rQRsSQi+iKir5fxRfRsZk1oGHZJAq4D1kTEV6tKdwILs8cLgTuKb8/MijKcU1xPBz4CPCFpVTbsMuAK4BZJFwFPAx9oT4vdb+3yU/LrZ17b0uuvemVfbv2Jl2fWrc0evyV32pPy9/oxjp7ceo8arS9WNajXNxD7c+vb3+ldayPRMOwR8QCgOuUzi23HzNrFR9CZJcJhN0uEw26WCIfdLBEOu1kiHHazRPinpIfpgIMOqltbf9bSRlO3NO+3jc/fGf628TUPXsz0tjTvdrpky2m59bVv9+HVRfKa3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhPezD1Psyz+nfLTaGwO59Tff97Hc+jEfbXC+ekRO0fvRO8lrdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEd7PPkx5+9nnn3R27rT7v5t/Pvo7X/fL3PpDF7w1tz6wZm1uvRXH8FjbXts6y2t2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDfezS5oFLAcOB/YDSyLiSkmLgY8DO7NRL4uIu9rVaDcb2Lkzf4T35pcfpMFF0mnffnRLx3AOqtkHXBoRj0qaBDwi6d6s9rWI+HL72jOzojQMe0RsA7Zlj3dLWgPMaHdjZlasEX1ml3QkcDLwUDboEkmPS1oqaXKdaRZJ6pfUv9c/Q2RWmmGHXdJE4Fbg0xHxPHA1cDQwh8qa/yu1pouIJRHRFxF9vYwvoGUza8awwi6pl0rQb4qI2wAiYntEDETEfuAa4NT2tWlmrWoYdkkCrgPWRMRXq4ZPrxrt/cDq4tszs6IM59v404GPAE9IGvzd4MuABZLmAAFsAC5uS4dmVojhfBv/AKAapST3qZuNVj6CziwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyVCEdG5mUk7gY1Vgw4FnulYAyPTrb11a1/g3ppVZG9HRMTUWoWOhv33Zi71R0RfaQ3k6NbeurUvcG/N6lRv3ow3S4TDbpaIssO+pOT55+nW3rq1L3BvzepIb6V+Zjezzil7zW5mHeKwmyWilLBLmifpfyWtk/S5MnqoR9IGSU9IWiWpv+RelkraIWl11bApku6VtDa7r3mNvZJ6WyxpS7bsVkmaX1JvsyT9t6Q1kp6U9KlseKnLLqevjiy3jn9ml9QD/AI4G9gMPAwsiIifd7SROiRtAPoiovQDMCSdAbwALI+I47NhXwJ2RcQV2X+UkyPis13S22LghbIv451drWh69WXGgfOBCylx2eX09UE6sNzKWLOfCqyLiPUR8QrwHeC8EvroehGxEtg1ZPB5wLLs8TIqfywdV6e3rhAR2yLi0ezxbmDwMuOlLrucvjqijLDPADZVPd9Md13vPYB7JD0iaVHZzdQwLSK2QeWPBzis5H6GangZ704acpnxrll2zVz+vFVlhL3WpaS6af/f6RFxCnAu8Ilsc9WGZ1iX8e6UGpcZ7wrNXv68VWWEfTMwq+r5TGBrCX3UFBFbs/sdwO1036Wotw9eQTe731FyP7/TTZfxrnWZcbpg2ZV5+fMywv4wcKykoyQdCFwA3FlCH79H0oTsixMkTQDOofsuRX0nsDB7vBC4o8ReXqVbLuNd7zLjlLzsSr/8eUR0/AbMp/KN/P8Bf19GD3X6eiPws+z2ZNm9ATdT2azbS2WL6CLgdcAKYG12P6WLersBeAJ4nEqwppfU27upfDR8HFiV3eaXvexy+urIcvPhsmaJ8BF0Zolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1ki/h/LxH9GDBEpagAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    path.draw()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
