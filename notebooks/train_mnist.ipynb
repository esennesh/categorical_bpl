{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 414034.843750\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -129541.296875\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -316116.812500\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -31859.978516\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -203433.156250\n",
      "    epoch          : 1\n",
      "    loss           : -143785.82619121287\n",
      "    val_loss       : -215253.8886352539\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -372416.687500\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -23642.730469\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -158834.953125\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -352049.875000\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -226787.203125\n",
      "    epoch          : 2\n",
      "    loss           : -235669.80776222152\n",
      "    val_loss       : -240575.16691894532\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -467575.718750\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -253934.093750\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -61995.656250\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -257680.984375\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -333694.437500\n",
      "    epoch          : 3\n",
      "    loss           : -267440.09813969675\n",
      "    val_loss       : -288112.5368041992\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -495986.875000\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -380005.968750\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -288893.718750\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -267854.593750\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -350917.437500\n",
      "    epoch          : 4\n",
      "    loss           : -295209.95745668316\n",
      "    val_loss       : -275201.34780273435\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -460259.031250\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -340574.875000\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -196734.750000\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -55447.003906\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -365016.687500\n",
      "    epoch          : 5\n",
      "    loss           : -283536.6019492574\n",
      "    val_loss       : -268531.857409668\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -460102.218750\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -45288.527344\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -222247.078125\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -264576.250000\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -288044.312500\n",
      "    epoch          : 6\n",
      "    loss           : -299433.38269647275\n",
      "    val_loss       : -315495.2271240234\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -488236.968750\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -244782.843750\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -120764.976562\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -120136.734375\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -329426.812500\n",
      "    epoch          : 7\n",
      "    loss           : -323165.20304764854\n",
      "    val_loss       : -326874.3545898438\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -562224.250000\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -369337.875000\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -392777.218750\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -546250.000000\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -350333.031250\n",
      "    epoch          : 8\n",
      "    loss           : -319574.07008044556\n",
      "    val_loss       : -322149.4791503906\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -517607.375000\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -204037.312500\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -403679.750000\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -139938.953125\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -264210.968750\n",
      "    epoch          : 9\n",
      "    loss           : -313613.1790686881\n",
      "    val_loss       : -320838.0528564453\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -530018.125000\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -381578.562500\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -147681.687500\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -528227.187500\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -293303.937500\n",
      "    epoch          : 10\n",
      "    loss           : -330996.9653465347\n",
      "    val_loss       : -334354.15415039065\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -410003.093750\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -435581.968750\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -448271.156250\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -384379.500000\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -376724.062500\n",
      "    epoch          : 11\n",
      "    loss           : -345773.66738861386\n",
      "    val_loss       : -324483.4123413086\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -516891.656250\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -405448.937500\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -330597.062500\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -181696.718750\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -325882.937500\n",
      "    epoch          : 12\n",
      "    loss           : -346575.57889851485\n",
      "    val_loss       : -363781.1889404297\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -557786.000000\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -310094.187500\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -314936.843750\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -393074.406250\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -328427.500000\n",
      "    epoch          : 13\n",
      "    loss           : -359395.8287438119\n",
      "    val_loss       : -344204.60987548827\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -526298.875000\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -278459.562500\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -186695.484375\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -365408.750000\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -367014.812500\n",
      "    epoch          : 14\n",
      "    loss           : -361126.8078589109\n",
      "    val_loss       : -362746.7264648437\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -555066.250000\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -307582.093750\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -326581.812500\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -336667.781250\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -400147.687500\n",
      "    epoch          : 15\n",
      "    loss           : -356631.33415841585\n",
      "    val_loss       : -363134.00533447263\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -554235.437500\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -385184.125000\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -394854.437500\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -403250.312500\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -216357.421875\n",
      "    epoch          : 16\n",
      "    loss           : -367376.1254641089\n",
      "    val_loss       : -371139.7258178711\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -547786.500000\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -402638.812500\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -377087.812500\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -341229.750000\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -475862.625000\n",
      "    epoch          : 17\n",
      "    loss           : -409771.7781559406\n",
      "    val_loss       : -502075.30661621096\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -425609.562500\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -429574.093750\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -379632.812500\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -645401.500000\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -601539.437500\n",
      "    epoch          : 18\n",
      "    loss           : -484218.398592203\n",
      "    val_loss       : -482873.01623535156\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -713832.187500\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -543895.125000\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -504552.687500\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -585979.750000\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -523615.312500\n",
      "    epoch          : 19\n",
      "    loss           : -506905.9668935643\n",
      "    val_loss       : -480669.97783203126\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -647212.875000\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -419929.906250\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -459726.968750\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -635408.625000\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -468695.218750\n",
      "    epoch          : 20\n",
      "    loss           : -512520.6907487624\n",
      "    val_loss       : -501822.98813476565\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -487869.125000\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -525907.625000\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -631696.437500\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -410356.968750\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -392001.062500\n",
      "    epoch          : 21\n",
      "    loss           : -527526.0681466584\n",
      "    val_loss       : -506314.24106445315\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -664069.250000\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -429989.812500\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -403194.468750\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -230721.687500\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -457095.500000\n",
      "    epoch          : 22\n",
      "    loss           : -517009.23360148515\n",
      "    val_loss       : -524104.1682128906\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -758728.312500\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -438552.906250\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -567087.437500\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -477649.000000\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -573649.750000\n",
      "    epoch          : 23\n",
      "    loss           : -532893.7032797029\n",
      "    val_loss       : -500456.9537109375\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -673625.562500\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -640473.062500\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -535840.625000\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -570031.750000\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -530875.500000\n",
      "    epoch          : 24\n",
      "    loss           : -532865.5413056931\n",
      "    val_loss       : -534019.7114257812\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -699239.687500\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -580478.812500\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -435875.062500\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -330026.593750\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -648049.562500\n",
      "    epoch          : 25\n",
      "    loss           : -530338.2937809406\n",
      "    val_loss       : -516571.77456054685\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -643793.062500\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -641313.937500\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -482621.375000\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -487493.906250\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -594975.000000\n",
      "    epoch          : 26\n",
      "    loss           : -534217.3214727723\n",
      "    val_loss       : -566889.9317382813\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -751782.375000\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -440771.906250\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -481066.343750\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -453221.531250\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -383654.875000\n",
      "    epoch          : 27\n",
      "    loss           : -524752.3638613861\n",
      "    val_loss       : -496515.6010986328\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -702318.125000\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -545690.937500\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -334568.343750\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -611657.250000\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -492995.687500\n",
      "    epoch          : 28\n",
      "    loss           : -543647.3516398515\n",
      "    val_loss       : -547720.9166992188\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -713682.375000\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -555379.812500\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -482852.062500\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -545278.375000\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -586830.125000\n",
      "    epoch          : 29\n",
      "    loss           : -542894.5116027228\n",
      "    val_loss       : -524162.8830444336\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -791216.062500\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -727603.875000\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -596450.750000\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -674045.625000\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -408510.843750\n",
      "    epoch          : 30\n",
      "    loss           : -548058.5423886139\n",
      "    val_loss       : -472599.6632080078\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -590662.625000\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -502444.125000\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -476137.250000\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -640844.500000\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -471234.562500\n",
      "    epoch          : 31\n",
      "    loss           : -531312.1011757426\n",
      "    val_loss       : -522312.46790771483\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -790301.375000\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -234858.484375\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -541757.437500\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -565359.687500\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -319793.937500\n",
      "    epoch          : 32\n",
      "    loss           : -561455.8825804455\n",
      "    val_loss       : -563860.2223388671\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -571038.000000\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -656535.250000\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -678794.687500\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -526048.750000\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -472047.375000\n",
      "    epoch          : 33\n",
      "    loss           : -549923.1084467822\n",
      "    val_loss       : -564627.7150634766\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -726660.250000\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -653930.250000\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -517762.656250\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -432660.937500\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -370728.468750\n",
      "    epoch          : 34\n",
      "    loss           : -570348.7111695545\n",
      "    val_loss       : -599576.6544921875\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -677162.125000\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -503564.937500\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -329329.031250\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -638295.875000\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -598380.250000\n",
      "    epoch          : 35\n",
      "    loss           : -553945.3449876237\n",
      "    val_loss       : -527969.39296875\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -792712.250000\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -609716.250000\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -648571.000000\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -535888.500000\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -592068.750000\n",
      "    epoch          : 36\n",
      "    loss           : -577256.5776608911\n",
      "    val_loss       : -563136.8834472656\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -655853.750000\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -588803.062500\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -752590.750000\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -393846.062500\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -486441.375000\n",
      "    epoch          : 37\n",
      "    loss           : -552820.6779084158\n",
      "    val_loss       : -563050.271069336\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -746884.625000\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -534807.687500\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -246583.437500\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -701671.687500\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -439343.593750\n",
      "    epoch          : 38\n",
      "    loss           : -558491.3367883663\n",
      "    val_loss       : -566738.3487915039\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -783893.625000\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -686400.250000\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -343771.750000\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -594575.812500\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -503803.187500\n",
      "    epoch          : 39\n",
      "    loss           : -543889.0600247525\n",
      "    val_loss       : -596777.77421875\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -631604.062500\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -688656.375000\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -433767.562500\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -408646.187500\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -448058.125000\n",
      "    epoch          : 40\n",
      "    loss           : -573009.5037128713\n",
      "    val_loss       : -615678.3635986329\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -288527.156250\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -592646.500000\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -547603.250000\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -527722.250000\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -661134.625000\n",
      "    epoch          : 41\n",
      "    loss           : -577009.9094987623\n",
      "    val_loss       : -587557.9067626953\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -642459.250000\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -416898.843750\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -474915.375000\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -901087.250000\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -633137.062500\n",
      "    epoch          : 42\n",
      "    loss           : -576869.4149133663\n",
      "    val_loss       : -555726.8006103516\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -487372.312500\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -445966.750000\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -560205.375000\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -726529.500000\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -549396.250000\n",
      "    epoch          : 43\n",
      "    loss           : -564046.9362623763\n",
      "    val_loss       : -558926.9557739258\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -755625.500000\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -587806.062500\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -673222.750000\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -609448.812500\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -556124.062500\n",
      "    epoch          : 44\n",
      "    loss           : -614540.9811262377\n",
      "    val_loss       : -604201.8311767578\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -774307.437500\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -577925.125000\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -577614.000000\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -308177.406250\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -484683.125000\n",
      "    epoch          : 45\n",
      "    loss           : -586419.896039604\n",
      "    val_loss       : -627465.7579345703\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -841754.062500\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -567486.125000\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -466577.281250\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -563654.562500\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -709802.125000\n",
      "    epoch          : 46\n",
      "    loss           : -598530.0804455446\n",
      "    val_loss       : -621551.0568603516\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -660367.437500\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -585823.000000\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -735750.562500\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -858046.500000\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -632627.812500\n",
      "    epoch          : 47\n",
      "    loss           : -612193.458230198\n",
      "    val_loss       : -619239.2918701172\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -772232.625000\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -549118.687500\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -776906.125000\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -450217.562500\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -681796.750000\n",
      "    epoch          : 48\n",
      "    loss           : -617603.3787128713\n",
      "    val_loss       : -622597.2301757813\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -705091.500000\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -584504.375000\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -771797.000000\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -821641.625000\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -587325.125000\n",
      "    epoch          : 49\n",
      "    loss           : -630072.8845915842\n",
      "    val_loss       : -596861.9380126953\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -855305.000000\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -686821.125000\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -709860.500000\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -548675.000000\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -728043.437500\n",
      "    epoch          : 50\n",
      "    loss           : -630910.3988242574\n",
      "    val_loss       : -634635.8768798828\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0923_134115/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -709035.562500\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -716421.375000\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -607453.687500\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -566874.062500\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -728071.000000\n",
      "    epoch          : 51\n",
      "    loss           : -626337.2305074257\n",
      "    val_loss       : -622829.2982177734\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -919279.625000\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -697109.812500\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -446221.781250\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -669159.125000\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -607595.312500\n",
      "    epoch          : 52\n",
      "    loss           : -651426.7849628713\n",
      "    val_loss       : -588309.8348388672\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -842899.250000\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -765098.375000\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -442610.500000\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -684115.500000\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -571990.937500\n",
      "    epoch          : 53\n",
      "    loss           : -635229.6215965346\n",
      "    val_loss       : -599425.5295898437\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -598593.812500\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -680904.625000\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -662490.125000\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -698712.187500\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -599525.500000\n",
      "    epoch          : 54\n",
      "    loss           : -651360.0720915842\n",
      "    val_loss       : -662470.9125976562\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -772616.562500\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -573398.812500\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -577322.437500\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -904305.937500\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -673343.000000\n",
      "    epoch          : 55\n",
      "    loss           : -649763.9597772277\n",
      "    val_loss       : -602841.4169921875\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -794733.312500\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -720720.250000\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -799812.437500\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -716270.250000\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -636507.375000\n",
      "    epoch          : 56\n",
      "    loss           : -665093.6612004951\n",
      "    val_loss       : -667217.4559326172\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -831219.437500\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -701463.125000\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -783930.250000\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -621249.375000\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -609459.625000\n",
      "    epoch          : 57\n",
      "    loss           : -645656.2694925743\n",
      "    val_loss       : -663873.1681762695\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -841913.750000\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -603725.937500\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -627578.750000\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -892902.437500\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -532735.875000\n",
      "    epoch          : 58\n",
      "    loss           : -646954.4532797029\n",
      "    val_loss       : -587986.8370727539\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -849022.000000\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -671072.375000\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -498213.125000\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -370661.312500\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -596375.937500\n",
      "    epoch          : 59\n",
      "    loss           : -636062.1333539604\n",
      "    val_loss       : -656965.5772460938\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -791121.562500\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -670727.875000\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -666218.812500\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -699120.687500\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -740399.375000\n",
      "    epoch          : 60\n",
      "    loss           : -641135.1413985149\n",
      "    val_loss       : -654251.1577392578\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -819820.250000\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -491674.000000\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -504032.968750\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -495594.937500\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -507223.781250\n",
      "    epoch          : 61\n",
      "    loss           : -644973.8199257426\n",
      "    val_loss       : -662987.7264404297\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -908598.875000\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -696436.375000\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -714346.500000\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -620140.000000\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -733369.312500\n",
      "    epoch          : 62\n",
      "    loss           : -653185.3623143565\n",
      "    val_loss       : -683639.9588378906\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -845144.625000\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -581690.187500\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -639218.562500\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -707645.750000\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -623665.000000\n",
      "    epoch          : 63\n",
      "    loss           : -649837.3867574257\n",
      "    val_loss       : -587841.3260009766\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -702989.375000\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -603428.250000\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -666569.625000\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -764038.187500\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -616799.562500\n",
      "    epoch          : 64\n",
      "    loss           : -647777.7484529703\n",
      "    val_loss       : -655791.528552246\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -690813.750000\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -386125.406250\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -692692.125000\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -528519.250000\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -615911.125000\n",
      "    epoch          : 65\n",
      "    loss           : -657809.3146658416\n",
      "    val_loss       : -697963.427758789\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -760147.000000\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -670295.375000\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -377073.625000\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -352710.187500\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -598493.562500\n",
      "    epoch          : 66\n",
      "    loss           : -660072.5388304455\n",
      "    val_loss       : -635417.309375\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -654445.375000\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -564502.937500\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -649814.312500\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -782408.875000\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -731903.625000\n",
      "    epoch          : 67\n",
      "    loss           : -661714.7766089109\n",
      "    val_loss       : -631257.8015625\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -828151.687500\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -514729.781250\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -386102.750000\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -619682.375000\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -675056.812500\n",
      "    epoch          : 68\n",
      "    loss           : -664038.0993193069\n",
      "    val_loss       : -671709.2896484375\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -851160.750000\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -790589.000000\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -640527.250000\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -574475.937500\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -623064.937500\n",
      "    epoch          : 69\n",
      "    loss           : -661386.9043935643\n",
      "    val_loss       : -685762.5280761719\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -834226.750000\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -674809.812500\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -804887.937500\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -652521.937500\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -752147.062500\n",
      "    epoch          : 70\n",
      "    loss           : -650315.2527846535\n",
      "    val_loss       : -660186.2965209961\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -937708.875000\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -772278.562500\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -758572.000000\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -842549.312500\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -394789.750000\n",
      "    epoch          : 71\n",
      "    loss           : -661068.5129950495\n",
      "    val_loss       : -639408.9225463867\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -737600.937500\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -586088.812500\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -610327.625000\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -828845.187500\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -703087.250000\n",
      "    epoch          : 72\n",
      "    loss           : -667786.5068069306\n",
      "    val_loss       : -626086.429272461\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -834823.312500\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -687556.500000\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -557873.187500\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -635270.812500\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -494028.312500\n",
      "    epoch          : 73\n",
      "    loss           : -663583.3220915842\n",
      "    val_loss       : -700968.5973144531\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -950187.812500\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -791584.312500\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -681099.187500\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -644843.875000\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -647885.437500\n",
      "    epoch          : 74\n",
      "    loss           : -659073.9888613861\n",
      "    val_loss       : -647721.6109863281\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -717775.437500\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -482978.500000\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -698030.250000\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -636238.562500\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -733592.187500\n",
      "    epoch          : 75\n",
      "    loss           : -657734.4625618812\n",
      "    val_loss       : -641416.640246582\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -779072.312500\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -727124.500000\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -657073.500000\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -511256.656250\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -632569.875000\n",
      "    epoch          : 76\n",
      "    loss           : -648217.0915841584\n",
      "    val_loss       : -612741.8859619141\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -831959.750000\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -722548.000000\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -788553.250000\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -740311.000000\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -730299.875000\n",
      "    epoch          : 77\n",
      "    loss           : -671127.2797029703\n",
      "    val_loss       : -671773.5975463868\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -923151.875000\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -609184.000000\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -614106.500000\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -514709.593750\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -620947.000000\n",
      "    epoch          : 78\n",
      "    loss           : -668456.4975247525\n",
      "    val_loss       : -647485.3214355469\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -751364.500000\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -606051.375000\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -650505.875000\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -634157.000000\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -639537.125000\n",
      "    epoch          : 79\n",
      "    loss           : -659696.2063737623\n",
      "    val_loss       : -654802.4837524414\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -851265.812500\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -575814.875000\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -701805.750000\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -642133.500000\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -617265.250000\n",
      "    epoch          : 80\n",
      "    loss           : -666687.6621287129\n",
      "    val_loss       : -641089.1742797851\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -903594.500000\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -620838.625000\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -585028.000000\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -803575.875000\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -682998.187500\n",
      "    epoch          : 81\n",
      "    loss           : -675203.1048886139\n",
      "    val_loss       : -669217.9646484375\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -904674.750000\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -776654.500000\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -653985.312500\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -718675.750000\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -764444.375000\n",
      "    epoch          : 82\n",
      "    loss           : -674094.2363861386\n",
      "    val_loss       : -679859.4455444335\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -874998.000000\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -597297.125000\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -529207.500000\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -673885.625000\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -571329.187500\n",
      "    epoch          : 83\n",
      "    loss           : -690536.5795173268\n",
      "    val_loss       : -695681.988305664\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -845671.687500\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -630257.187500\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -574011.562500\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -561258.375000\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -594435.812500\n",
      "    epoch          : 84\n",
      "    loss           : -674745.1469678218\n",
      "    val_loss       : -689515.7545043945\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -939176.000000\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -583703.937500\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -623196.937500\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -712119.000000\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -806366.125000\n",
      "    epoch          : 85\n",
      "    loss           : -673684.8060024752\n",
      "    val_loss       : -689161.617565918\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -764311.750000\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -628664.812500\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -673025.875000\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -557181.250000\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -710242.312500\n",
      "    epoch          : 86\n",
      "    loss           : -669810.1738861386\n",
      "    val_loss       : -656617.7219726562\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -745222.625000\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -640910.250000\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -602753.937500\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -860498.625000\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -719524.250000\n",
      "    epoch          : 87\n",
      "    loss           : -690795.7017326732\n",
      "    val_loss       : -673866.1740234375\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -481439.812500\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -649819.562500\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -718323.062500\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -665863.500000\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -732591.625000\n",
      "    epoch          : 88\n",
      "    loss           : -697199.1844059406\n",
      "    val_loss       : -689269.3458496094\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -925283.937500\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -522614.593750\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -555385.750000\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -750695.937500\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -616792.687500\n",
      "    epoch          : 89\n",
      "    loss           : -671700.1500618812\n",
      "    val_loss       : -633911.5598266602\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -753184.500000\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -697921.500000\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -695437.937500\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -700965.437500\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -619944.750000\n",
      "    epoch          : 90\n",
      "    loss           : -678931.4495668317\n",
      "    val_loss       : -701675.2125488281\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -898305.250000\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -715732.250000\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -769676.875000\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -678006.875000\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -709679.375000\n",
      "    epoch          : 91\n",
      "    loss           : -681073.8917079208\n",
      "    val_loss       : -681629.4644165039\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -935436.062500\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -718674.000000\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -591652.250000\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -757993.500000\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -563809.187500\n",
      "    epoch          : 92\n",
      "    loss           : -673570.5674504951\n",
      "    val_loss       : -685075.5010620117\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -903711.562500\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -612356.812500\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -794868.187500\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -790913.562500\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -740037.875000\n",
      "    epoch          : 93\n",
      "    loss           : -675718.3734529703\n",
      "    val_loss       : -711758.2787109375\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -935580.375000\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -645012.250000\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -673468.625000\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -630545.062500\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -741253.000000\n",
      "    epoch          : 94\n",
      "    loss           : -682861.5801361386\n",
      "    val_loss       : -695783.1425048828\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -843356.250000\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -727219.750000\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -688715.000000\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -388636.531250\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -585180.750000\n",
      "    epoch          : 95\n",
      "    loss           : -682575.7954826732\n",
      "    val_loss       : -693210.972265625\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -505952.562500\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -625790.125000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -544022.500000\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -687360.562500\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -781913.187500\n",
      "    epoch          : 96\n",
      "    loss           : -680301.1655321782\n",
      "    val_loss       : -628785.762878418\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -898995.062500\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -735619.625000\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -625500.312500\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -698663.500000\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -651494.812500\n",
      "    epoch          : 97\n",
      "    loss           : -689395.6741955446\n",
      "    val_loss       : -685085.9989501953\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -712531.625000\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -608762.187500\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -670541.000000\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -608937.375000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -740449.750000\n",
      "    epoch          : 98\n",
      "    loss           : -658467.7305074257\n",
      "    val_loss       : -630063.2355224609\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -673146.312500\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -702639.062500\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -656363.250000\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -794708.125000\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -704754.625000\n",
      "    epoch          : 99\n",
      "    loss           : -686166.1305693069\n",
      "    val_loss       : -602881.2418334961\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -650484.187500\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -741081.625000\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -746768.812500\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -611198.875000\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -668775.937500\n",
      "    epoch          : 100\n",
      "    loss           : -675873.2858910891\n",
      "    val_loss       : -725943.4401123046\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0923_134115/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -815026.437500\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -787150.687500\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -644087.500000\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -902378.187500\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -602392.875000\n",
      "    epoch          : 101\n",
      "    loss           : -698968.5547648515\n",
      "    val_loss       : -702656.4696777344\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -913351.187500\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -828432.125000\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -533560.937500\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -733375.375000\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -738759.812500\n",
      "    epoch          : 102\n",
      "    loss           : -690033.1519183168\n",
      "    val_loss       : -715589.3593017578\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -753113.562500\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -702262.125000\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -776359.750000\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -446849.656250\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -609513.000000\n",
      "    epoch          : 103\n",
      "    loss           : -694170.7026608911\n",
      "    val_loss       : -691683.7236816406\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -800088.187500\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -670485.000000\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -752352.562500\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -870418.437500\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -740877.062500\n",
      "    epoch          : 104\n",
      "    loss           : -701804.0983910891\n",
      "    val_loss       : -704656.3719848633\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -724681.187500\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -814747.812500\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -672897.875000\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -485450.312500\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -713981.312500\n",
      "    epoch          : 105\n",
      "    loss           : -693137.5649752475\n",
      "    val_loss       : -667552.6421142578\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -859517.250000\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -784613.875000\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -688238.250000\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -545524.375000\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -567489.250000\n",
      "    epoch          : 106\n",
      "    loss           : -681952.0637376237\n",
      "    val_loss       : -725208.0569335937\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -917716.125000\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -644704.875000\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -483482.218750\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -954445.375000\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -721415.687500\n",
      "    epoch          : 107\n",
      "    loss           : -700627.3688118812\n",
      "    val_loss       : -681118.2758789062\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -892560.750000\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -743445.937500\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -814091.375000\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -927067.062500\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -633080.937500\n",
      "    epoch          : 108\n",
      "    loss           : -707254.9981435643\n",
      "    val_loss       : -711297.590234375\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -913345.312500\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -635857.500000\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -690014.625000\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -839265.812500\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -534032.375000\n",
      "    epoch          : 109\n",
      "    loss           : -692654.0510519802\n",
      "    val_loss       : -705614.3977050781\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -742563.062500\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -721048.187500\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -685338.187500\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -672836.250000\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -733006.187500\n",
      "    epoch          : 110\n",
      "    loss           : -678526.3053836634\n",
      "    val_loss       : -689168.8695068359\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -648378.750000\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -729304.437500\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -552005.750000\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -588376.687500\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -736633.375000\n",
      "    epoch          : 111\n",
      "    loss           : -687799.2902227723\n",
      "    val_loss       : -671394.9936889649\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -775881.250000\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -681850.312500\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -709911.562500\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -746970.250000\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -728794.625000\n",
      "    epoch          : 112\n",
      "    loss           : -693920.614789604\n",
      "    val_loss       : -687511.5417480469\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -918592.500000\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -794040.687500\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -699586.187500\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -870499.937500\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -569983.687500\n",
      "    epoch          : 113\n",
      "    loss           : -685750.7902227723\n",
      "    val_loss       : -683038.3183471679\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -931083.062500\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -480457.187500\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -536831.125000\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -779506.000000\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -730128.625000\n",
      "    epoch          : 114\n",
      "    loss           : -677362.8780940594\n",
      "    val_loss       : -660513.0596801757\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -920312.125000\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -644629.062500\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -693333.375000\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -807792.062500\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -712793.937500\n",
      "    epoch          : 115\n",
      "    loss           : -715673.0281559406\n",
      "    val_loss       : -687454.9432250976\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -627245.750000\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -742684.750000\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -656759.875000\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -638647.875000\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -678628.625000\n",
      "    epoch          : 116\n",
      "    loss           : -711662.0095915842\n",
      "    val_loss       : -707098.0294189453\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -852004.125000\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -650910.750000\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -721028.875000\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -733483.937500\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -583297.500000\n",
      "    epoch          : 117\n",
      "    loss           : -706912.1308787129\n",
      "    val_loss       : -675481.2171386719\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -951967.875000\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -721801.000000\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -657390.125000\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -527507.125000\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -656909.187500\n",
      "    epoch          : 118\n",
      "    loss           : -700107.8032178218\n",
      "    val_loss       : -712512.0425048828\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -762974.500000\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -762138.437500\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -698871.125000\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -770195.000000\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -557897.562500\n",
      "    epoch          : 119\n",
      "    loss           : -719938.0306311881\n",
      "    val_loss       : -700643.8760375977\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -877006.687500\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -723531.875000\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -622086.125000\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -773195.125000\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -684740.125000\n",
      "    epoch          : 120\n",
      "    loss           : -709683.1055074257\n",
      "    val_loss       : -712216.5168945312\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -846971.937500\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -689260.187500\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -711132.500000\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -719202.000000\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -760727.875000\n",
      "    epoch          : 121\n",
      "    loss           : -708965.0751856435\n",
      "    val_loss       : -664788.3336669921\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -941847.250000\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -840789.562500\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -744719.375000\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -816291.187500\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -728608.750000\n",
      "    epoch          : 122\n",
      "    loss           : -713282.6321163366\n",
      "    val_loss       : -712364.638671875\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -934908.437500\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -807833.750000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -719399.625000\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -752327.562500\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -778390.875000\n",
      "    epoch          : 123\n",
      "    loss           : -700287.7323638614\n",
      "    val_loss       : -698160.2786010742\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -844207.500000\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -730732.062500\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -729959.937500\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -739425.875000\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -796426.437500\n",
      "    epoch          : 124\n",
      "    loss           : -709623.6717202971\n",
      "    val_loss       : -742890.0302734375\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -863221.750000\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -752622.000000\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -745752.750000\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -940630.687500\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -790350.375000\n",
      "    epoch          : 125\n",
      "    loss           : -710751.0037128713\n",
      "    val_loss       : -697606.0520507812\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -837010.375000\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -531558.500000\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -581323.000000\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -702012.312500\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -750026.937500\n",
      "    epoch          : 126\n",
      "    loss           : -713513.8548886139\n",
      "    val_loss       : -699490.9087646485\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -664978.625000\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -659482.000000\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -769155.000000\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -438752.750000\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -793858.187500\n",
      "    epoch          : 127\n",
      "    loss           : -707670.78125\n",
      "    val_loss       : -701903.8412597657\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -828721.625000\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -589868.187500\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -781860.250000\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -759329.500000\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -653337.875000\n",
      "    epoch          : 128\n",
      "    loss           : -707256.2874381188\n",
      "    val_loss       : -678017.7956176758\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -879568.062500\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -673933.687500\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -773225.812500\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -512213.656250\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -781121.812500\n",
      "    epoch          : 129\n",
      "    loss           : -711341.8722153465\n",
      "    val_loss       : -715743.1709594727\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -952062.812500\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -644791.625000\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -714220.875000\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -774760.187500\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -678953.250000\n",
      "    epoch          : 130\n",
      "    loss           : -705234.0801361386\n",
      "    val_loss       : -740959.8326171875\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -945864.375000\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -709491.062500\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -515177.000000\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -670487.125000\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -656392.375000\n",
      "    epoch          : 131\n",
      "    loss           : -694375.145420792\n",
      "    val_loss       : -687385.4200683594\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -934946.750000\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -727117.187500\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -679043.625000\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -868532.750000\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -763597.125000\n",
      "    epoch          : 132\n",
      "    loss           : -710941.5652846535\n",
      "    val_loss       : -701650.1686401367\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -934439.125000\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -658208.625000\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -623142.750000\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -769211.500000\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -767624.312500\n",
      "    epoch          : 133\n",
      "    loss           : -715372.4681311881\n",
      "    val_loss       : -749548.9063598632\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -859628.625000\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -623919.437500\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -701124.312500\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -837957.750000\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -492403.437500\n",
      "    epoch          : 134\n",
      "    loss           : -719966.729269802\n",
      "    val_loss       : -728980.0539916992\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -720546.187500\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -646356.000000\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -598821.062500\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -837168.625000\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -718480.625000\n",
      "    epoch          : 135\n",
      "    loss           : -710030.9777227723\n",
      "    val_loss       : -705366.9330566407\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -952679.812500\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -716062.000000\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -756214.500000\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -866549.500000\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -780841.125000\n",
      "    epoch          : 136\n",
      "    loss           : -714849.7023514851\n",
      "    val_loss       : -673347.0749511719\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -866532.687500\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -763996.375000\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -791118.062500\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -792218.875000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -688188.125000\n",
      "    epoch          : 137\n",
      "    loss           : -713451.0467202971\n",
      "    val_loss       : -702529.3599609375\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -940753.250000\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -740739.812500\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -677182.500000\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -791140.875000\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -652029.937500\n",
      "    epoch          : 138\n",
      "    loss           : -714798.2363861386\n",
      "    val_loss       : -680395.494921875\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -714494.562500\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -670035.812500\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -746545.625000\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -774388.062500\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -680822.187500\n",
      "    epoch          : 139\n",
      "    loss           : -709939.8626237623\n",
      "    val_loss       : -734678.6062866211\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -834839.250000\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -564397.000000\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -812294.625000\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -929038.875000\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -698247.687500\n",
      "    epoch          : 140\n",
      "    loss           : -724230.6577970297\n",
      "    val_loss       : -681859.4260009766\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -750851.000000\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -586355.375000\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -531905.437500\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -511107.062500\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -583875.250000\n",
      "    epoch          : 141\n",
      "    loss           : -711491.1438737623\n",
      "    val_loss       : -722116.9017333984\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -850512.750000\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -746746.750000\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -776732.500000\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -863943.250000\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -535909.750000\n",
      "    epoch          : 142\n",
      "    loss           : -725849.2874381188\n",
      "    val_loss       : -716014.7036132812\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -680296.437500\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -821759.625000\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -679101.250000\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -835154.875000\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -744066.750000\n",
      "    epoch          : 143\n",
      "    loss           : -708560.0575495049\n",
      "    val_loss       : -699101.9200683594\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -915277.750000\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -699699.375000\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -770554.125000\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -733410.062500\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -745728.000000\n",
      "    epoch          : 144\n",
      "    loss           : -711788.1194306931\n",
      "    val_loss       : -699860.3062011718\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -814429.500000\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -675092.562500\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -806493.500000\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -807215.312500\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -654939.812500\n",
      "    epoch          : 145\n",
      "    loss           : -717786.2862004951\n",
      "    val_loss       : -719450.9817749023\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -866947.312500\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -698782.812500\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -681578.750000\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -876648.875000\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -654635.750000\n",
      "    epoch          : 146\n",
      "    loss           : -727603.0587871287\n",
      "    val_loss       : -702147.5360107422\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -932833.687500\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -671253.187500\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -709127.312500\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -690086.937500\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -769514.500000\n",
      "    epoch          : 147\n",
      "    loss           : -731761.375\n",
      "    val_loss       : -726863.038684082\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -895899.687500\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -834721.625000\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -537677.375000\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -864206.250000\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -634219.125000\n",
      "    epoch          : 148\n",
      "    loss           : -723149.9511138614\n",
      "    val_loss       : -723186.1017578125\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -869051.187500\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -849466.250000\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -719092.937500\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -912308.250000\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -747308.250000\n",
      "    epoch          : 149\n",
      "    loss           : -731607.9461633663\n",
      "    val_loss       : -758536.6438476562\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -714529.312500\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -680458.750000\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -734203.000000\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -668203.375000\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -778498.875000\n",
      "    epoch          : 150\n",
      "    loss           : -743537.0055693069\n",
      "    val_loss       : -752266.9354736328\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0923_134115/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -793042.750000\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -692921.750000\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -719418.687500\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -702211.562500\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -784901.500000\n",
      "    epoch          : 151\n",
      "    loss           : -734372.8422029703\n",
      "    val_loss       : -711442.304296875\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -932624.187500\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -679451.625000\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -716986.000000\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -722712.437500\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -780253.125000\n",
      "    epoch          : 152\n",
      "    loss           : -735855.6711014851\n",
      "    val_loss       : -729281.4138305665\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -892065.875000\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -582343.375000\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -733039.062500\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -765886.000000\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -566678.250000\n",
      "    epoch          : 153\n",
      "    loss           : -742888.5872524752\n",
      "    val_loss       : -753616.2306274414\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -696977.500000\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -793785.687500\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -661280.375000\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -606607.750000\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -716555.875000\n",
      "    epoch          : 154\n",
      "    loss           : -744561.7564975248\n",
      "    val_loss       : -725933.5603881836\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -844957.000000\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -610715.750000\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -653382.500000\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -932938.750000\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -751793.937500\n",
      "    epoch          : 155\n",
      "    loss           : -734175.9740099009\n",
      "    val_loss       : -736506.9495239258\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -760910.937500\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -761047.500000\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -666034.312500\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -747470.625000\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -791136.437500\n",
      "    epoch          : 156\n",
      "    loss           : -739974.7103960396\n",
      "    val_loss       : -738745.5037231445\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -882143.062500\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -763646.375000\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -583833.437500\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -597812.875000\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -795216.125000\n",
      "    epoch          : 157\n",
      "    loss           : -750497.9458539604\n",
      "    val_loss       : -730942.8181274415\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -768360.500000\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -708568.750000\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -745766.500000\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -754725.750000\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -768663.000000\n",
      "    epoch          : 158\n",
      "    loss           : -737336.7852722772\n",
      "    val_loss       : -748002.4972900391\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -916959.187500\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -756321.125000\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -723350.187500\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -918429.312500\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -709462.625000\n",
      "    epoch          : 159\n",
      "    loss           : -752720.2413366337\n",
      "    val_loss       : -749907.1741210937\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -946392.250000\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -759368.875000\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -728302.750000\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -739622.875000\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -755142.687500\n",
      "    epoch          : 160\n",
      "    loss           : -766633.4969059406\n",
      "    val_loss       : -745018.8350952149\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -935978.625000\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -645040.750000\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -718427.562500\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -706502.750000\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -589253.375000\n",
      "    epoch          : 161\n",
      "    loss           : -749519.1528465346\n",
      "    val_loss       : -738570.0447509766\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -948798.375000\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -743882.750000\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -651307.625000\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -824250.562500\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -740868.625000\n",
      "    epoch          : 162\n",
      "    loss           : -749116.0922029703\n",
      "    val_loss       : -738337.8916503906\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -931772.750000\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -834045.812500\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -759037.937500\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -753126.375000\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -777150.500000\n",
      "    epoch          : 163\n",
      "    loss           : -752433.7506188119\n",
      "    val_loss       : -727274.1892456055\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -807458.625000\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -698053.687500\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -790154.625000\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -747286.750000\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -787735.625000\n",
      "    epoch          : 164\n",
      "    loss           : -747894.6082920792\n",
      "    val_loss       : -751337.0299560546\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -937842.375000\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -830738.875000\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -752925.312500\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -753052.000000\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -721490.062500\n",
      "    epoch          : 165\n",
      "    loss           : -749803.9529702971\n",
      "    val_loss       : -753248.1379882812\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -731196.125000\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -664631.000000\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -797261.875000\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -834324.062500\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -540761.250000\n",
      "    epoch          : 166\n",
      "    loss           : -755953.6899752475\n",
      "    val_loss       : -744876.1811645508\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -955759.250000\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -831448.062500\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -779997.250000\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -764964.875000\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -796677.187500\n",
      "    epoch          : 167\n",
      "    loss           : -753149.8907797029\n",
      "    val_loss       : -757245.2107177734\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -812688.312500\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -728887.187500\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -777740.000000\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -790369.062500\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -615186.500000\n",
      "    epoch          : 168\n",
      "    loss           : -760325.6701732674\n",
      "    val_loss       : -744717.9440795898\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -772621.500000\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -654376.000000\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -706371.187500\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -612190.937500\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -789403.687500\n",
      "    epoch          : 169\n",
      "    loss           : -756198.9443069306\n",
      "    val_loss       : -759463.2120117188\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -779559.875000\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -652590.062500\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -679777.750000\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -798637.500000\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -752726.687500\n",
      "    epoch          : 170\n",
      "    loss           : -758768.1912128713\n",
      "    val_loss       : -744556.2346191406\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -952672.125000\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -729795.187500\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -678426.187500\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -735226.687500\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -704248.437500\n",
      "    epoch          : 171\n",
      "    loss           : -753903.780940594\n",
      "    val_loss       : -751904.464099121\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -955034.000000\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -824435.125000\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -814122.625000\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -787292.875000\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -757130.000000\n",
      "    epoch          : 172\n",
      "    loss           : -747605.6974009901\n",
      "    val_loss       : -718632.2444335937\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -797749.312500\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -660712.250000\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -744890.125000\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -722294.375000\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -793905.000000\n",
      "    epoch          : 173\n",
      "    loss           : -761494.635519802\n",
      "    val_loss       : -763258.0546875\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -929664.000000\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -757927.187500\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -515677.000000\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -707623.250000\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -745716.312500\n",
      "    epoch          : 174\n",
      "    loss           : -763581.1813118812\n",
      "    val_loss       : -789904.112890625\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -945306.500000\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -725586.750000\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -750790.312500\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -769551.375000\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -702872.625000\n",
      "    epoch          : 175\n",
      "    loss           : -747223.1992574257\n",
      "    val_loss       : -736300.1516235352\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -874497.000000\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -795007.062500\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -775324.375000\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -924147.062500\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -635312.125000\n",
      "    epoch          : 176\n",
      "    loss           : -753057.0631188119\n",
      "    val_loss       : -755710.0726928711\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -927585.625000\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -753350.250000\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -719206.000000\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -759814.125000\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -702891.500000\n",
      "    epoch          : 177\n",
      "    loss           : -752602.6936881188\n",
      "    val_loss       : -768057.43125\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -709972.062500\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -757102.937500\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -809177.875000\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -656614.062500\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -743612.187500\n",
      "    epoch          : 178\n",
      "    loss           : -755702.6639851485\n",
      "    val_loss       : -766758.5840820313\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -941170.000000\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -771596.125000\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -812750.500000\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -935857.375000\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -742777.437500\n",
      "    epoch          : 179\n",
      "    loss           : -754096.437809406\n",
      "    val_loss       : -770251.7357421875\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -945117.125000\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -835433.375000\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -763704.750000\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -768756.250000\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -584689.250000\n",
      "    epoch          : 180\n",
      "    loss           : -767262.3001237623\n",
      "    val_loss       : -739549.9660644531\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -752392.687500\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -658614.125000\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -770723.187500\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -586131.875000\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -790706.750000\n",
      "    epoch          : 181\n",
      "    loss           : -761230.0525990099\n",
      "    val_loss       : -747329.5716796875\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -806463.000000\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -773603.750000\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -780183.062500\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -944736.312500\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -716532.875000\n",
      "    epoch          : 182\n",
      "    loss           : -761815.9495668317\n",
      "    val_loss       : -771444.2381835937\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -905128.875000\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -789140.687500\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -842952.625000\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -740546.375000\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -668115.750000\n",
      "    epoch          : 183\n",
      "    loss           : -763671.8997524752\n",
      "    val_loss       : -775468.7490722656\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -968384.625000\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -784885.375000\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -799764.625000\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -756958.250000\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -776891.750000\n",
      "    epoch          : 184\n",
      "    loss           : -767939.6299504951\n",
      "    val_loss       : -735716.4633544922\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -956239.375000\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -640423.000000\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -846335.687500\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -695486.000000\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -781971.062500\n",
      "    epoch          : 185\n",
      "    loss           : -770108.8811881188\n",
      "    val_loss       : -720672.858959961\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -912260.812500\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -723409.125000\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -670615.000000\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -794247.250000\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -738469.000000\n",
      "    epoch          : 186\n",
      "    loss           : -753766.7524752475\n",
      "    val_loss       : -766118.8696777343\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -946492.187500\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -845434.687500\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -737808.375000\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -755252.250000\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -604714.625000\n",
      "    epoch          : 187\n",
      "    loss           : -759999.8873762377\n",
      "    val_loss       : -771269.8919189454\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -921690.875000\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -692233.750000\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -761238.875000\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -720627.062500\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -636709.125000\n",
      "    epoch          : 188\n",
      "    loss           : -756642.4467821782\n",
      "    val_loss       : -755172.1295654296\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -795561.375000\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -755457.625000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -719572.750000\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -932866.000000\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -733030.437500\n",
      "    epoch          : 189\n",
      "    loss           : -759099.7889851485\n",
      "    val_loss       : -767153.5786621093\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -947321.187500\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -775012.625000\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -673893.500000\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -718656.562500\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -754581.250000\n",
      "    epoch          : 190\n",
      "    loss           : -770667.8063118812\n",
      "    val_loss       : -752643.766015625\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -940077.000000\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -657422.750000\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -683497.000000\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -719436.500000\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -767744.125000\n",
      "    epoch          : 191\n",
      "    loss           : -744766.5389851485\n",
      "    val_loss       : -743966.0698242188\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -968954.625000\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -840745.562500\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -592505.812500\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -922517.812500\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -740623.250000\n",
      "    epoch          : 192\n",
      "    loss           : -757727.8620049505\n",
      "    val_loss       : -757398.9018798828\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -970272.000000\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -679614.750000\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -631123.812500\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -779386.500000\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -729151.187500\n",
      "    epoch          : 193\n",
      "    loss           : -761050.249690594\n",
      "    val_loss       : -757520.4598266601\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -953015.312500\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -824234.250000\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -735248.437500\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -686065.375000\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -762714.750000\n",
      "    epoch          : 194\n",
      "    loss           : -746641.7976485149\n",
      "    val_loss       : -750276.9448608399\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -938686.500000\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -762169.437500\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -748204.750000\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -736626.937500\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -814664.375000\n",
      "    epoch          : 195\n",
      "    loss           : -769947.6021039604\n",
      "    val_loss       : -781724.6502197266\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -821571.312500\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -666541.375000\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -755782.750000\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -812555.125000\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -773086.250000\n",
      "    epoch          : 196\n",
      "    loss           : -762116.2103960396\n",
      "    val_loss       : -777870.5679077149\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -854565.500000\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -710683.625000\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -687179.937500\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -636634.437500\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -768468.937500\n",
      "    epoch          : 197\n",
      "    loss           : -761190.9820544554\n",
      "    val_loss       : -752770.6411865235\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -904698.312500\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -758119.000000\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -766163.312500\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -727610.062500\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -766808.062500\n",
      "    epoch          : 198\n",
      "    loss           : -763354.3638613861\n",
      "    val_loss       : -766173.9865844727\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -952418.625000\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -705907.875000\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -799406.312500\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -768295.500000\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -760126.000000\n",
      "    epoch          : 199\n",
      "    loss           : -762564.7574257426\n",
      "    val_loss       : -777182.8085449219\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -714949.437500\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -765694.187500\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -783193.000000\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -943320.250000\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -734728.625000\n",
      "    epoch          : 200\n",
      "    loss           : -760230.1237623763\n",
      "    val_loss       : -777636.4729248047\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0923_134115/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -763398.062500\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -801447.312500\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -767923.250000\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -710365.875000\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -779501.187500\n",
      "    epoch          : 201\n",
      "    loss           : -766141.854579208\n",
      "    val_loss       : -770758.4056640625\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -902710.750000\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -677820.000000\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -774995.562500\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -732408.875000\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -743235.750000\n",
      "    epoch          : 202\n",
      "    loss           : -764705.1639851485\n",
      "    val_loss       : -765121.7001342773\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -834247.687500\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -762133.000000\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -753100.250000\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -745348.937500\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -801308.875000\n",
      "    epoch          : 203\n",
      "    loss           : -763984.8715965346\n",
      "    val_loss       : -744119.4203491211\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -879513.375000\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -737095.000000\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -765919.500000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -741628.125000\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -642506.625000\n",
      "    epoch          : 204\n",
      "    loss           : -765807.0198019802\n",
      "    val_loss       : -783716.3734375\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -689622.562500\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -864640.312500\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -784587.187500\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -860549.687500\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -803874.250000\n",
      "    epoch          : 205\n",
      "    loss           : -752970.5225866337\n",
      "    val_loss       : -777411.3310791015\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -893292.312500\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -765536.687500\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -821704.750000\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -939736.250000\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -768095.812500\n",
      "    epoch          : 206\n",
      "    loss           : -769420.4443069306\n",
      "    val_loss       : -775928.6749511719\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -942857.937500\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -712203.750000\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -684364.000000\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -676615.250000\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -789144.875000\n",
      "    epoch          : 207\n",
      "    loss           : -764238.3941831683\n",
      "    val_loss       : -783320.7396240234\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -942053.250000\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -773354.062500\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -821289.250000\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -975510.875000\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -766816.625000\n",
      "    epoch          : 208\n",
      "    loss           : -771365.2722772277\n",
      "    val_loss       : -784125.611328125\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -909706.125000\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -826134.250000\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -716192.375000\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -734662.312500\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -818289.375000\n",
      "    epoch          : 209\n",
      "    loss           : -769042.2957920792\n",
      "    val_loss       : -735735.2461547852\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -841233.000000\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -772643.437500\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -684505.625000\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -771210.125000\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -740512.187500\n",
      "    epoch          : 210\n",
      "    loss           : -771231.4275990099\n",
      "    val_loss       : -750659.3448242188\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -949144.750000\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -653371.062500\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -750393.625000\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -794133.625000\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -708214.125000\n",
      "    epoch          : 211\n",
      "    loss           : -768362.2032797029\n",
      "    val_loss       : -774350.7611938476\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -830670.375000\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -762133.500000\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -796909.687500\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -591684.125000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -742115.375000\n",
      "    epoch          : 212\n",
      "    loss           : -771439.3094059406\n",
      "    val_loss       : -793099.2752685547\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -959889.750000\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -838047.562500\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -681673.625000\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -918928.625000\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -764104.500000\n",
      "    epoch          : 213\n",
      "    loss           : -772265.176980198\n",
      "    val_loss       : -766123.1048339844\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -955269.187500\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -741825.250000\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -743393.187500\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -747276.687500\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -748260.187500\n",
      "    epoch          : 214\n",
      "    loss           : -768171.3360148515\n",
      "    val_loss       : -776299.5056762695\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -928968.437500\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -722292.250000\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -704719.375000\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -760834.375000\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -767364.375000\n",
      "    epoch          : 215\n",
      "    loss           : -775013.5160891089\n",
      "    val_loss       : -728608.6201293946\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -881607.500000\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -746546.437500\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -782997.312500\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -912740.250000\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -786084.375000\n",
      "    epoch          : 216\n",
      "    loss           : -771499.2806311881\n",
      "    val_loss       : -765908.672265625\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -935159.875000\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -766454.937500\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -724514.125000\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -725506.062500\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -788052.375000\n",
      "    epoch          : 217\n",
      "    loss           : -764513.7332920792\n",
      "    val_loss       : -777117.6088256836\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -964012.500000\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -756305.000000\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -592612.062500\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -721336.437500\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -761420.500000\n",
      "    epoch          : 218\n",
      "    loss           : -773988.6089108911\n",
      "    val_loss       : -778721.6052856445\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -950801.125000\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -778180.562500\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -732547.000000\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -796357.625000\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -589890.937500\n",
      "    epoch          : 219\n",
      "    loss           : -779686.2444306931\n",
      "    val_loss       : -775620.608984375\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -961780.937500\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -773132.000000\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -725561.250000\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -770918.687500\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -761299.562500\n",
      "    epoch          : 220\n",
      "    loss           : -773521.6912128713\n",
      "    val_loss       : -781769.9894165039\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -968509.437500\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -494388.468750\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -779769.625000\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -720335.500000\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -727780.750000\n",
      "    epoch          : 221\n",
      "    loss           : -766122.1469678218\n",
      "    val_loss       : -751114.8817382812\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -804441.000000\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -717970.250000\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -768181.125000\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -966995.687500\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -754949.125000\n",
      "    epoch          : 222\n",
      "    loss           : -762360.0705445545\n",
      "    val_loss       : -780498.2434570312\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -737323.562500\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -756527.062500\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -689409.437500\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -764062.937500\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -752788.687500\n",
      "    epoch          : 223\n",
      "    loss           : -764021.573019802\n",
      "    val_loss       : -778481.1652587891\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -955187.562500\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -746494.312500\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -802370.187500\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -703127.687500\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -638093.375000\n",
      "    epoch          : 224\n",
      "    loss           : -765229.2258663366\n",
      "    val_loss       : -786435.3204589844\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -958078.250000\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -672379.437500\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -757701.500000\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -808513.937500\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -751053.375000\n",
      "    epoch          : 225\n",
      "    loss           : -763405.6720297029\n",
      "    val_loss       : -749493.8185180664\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -789878.500000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -775877.875000\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -772846.812500\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -654414.062500\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -760547.250000\n",
      "    epoch          : 226\n",
      "    loss           : -772771.9461633663\n",
      "    val_loss       : -779466.1625488282\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -929330.312500\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -625891.250000\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -713141.750000\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -673756.875000\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -815297.937500\n",
      "    epoch          : 227\n",
      "    loss           : -761875.3125\n",
      "    val_loss       : -774365.898828125\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -961605.062500\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -768318.250000\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -810055.687500\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -755978.437500\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -753822.312500\n",
      "    epoch          : 228\n",
      "    loss           : -772229.7518564357\n",
      "    val_loss       : -767219.7389526367\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -936268.500000\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -659231.750000\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -802783.562500\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -804358.750000\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -788004.375000\n",
      "    epoch          : 229\n",
      "    loss           : -770636.0129950495\n",
      "    val_loss       : -773904.5155395508\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -934511.375000\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -755715.437500\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -707002.375000\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -753643.875000\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -763843.687500\n",
      "    epoch          : 230\n",
      "    loss           : -776924.7586633663\n",
      "    val_loss       : -785611.5444824218\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -944215.250000\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -837918.125000\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -790095.875000\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -779095.000000\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -609588.312500\n",
      "    epoch          : 231\n",
      "    loss           : -774062.9900990099\n",
      "    val_loss       : -781840.0432006835\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -750477.375000\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -699766.500000\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -670108.125000\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -746938.875000\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -745999.750000\n",
      "    epoch          : 232\n",
      "    loss           : -781332.4331683168\n",
      "    val_loss       : -782213.8317871094\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -964467.750000\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -862638.500000\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -756149.625000\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -946191.250000\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -737626.187500\n",
      "    epoch          : 233\n",
      "    loss           : -782084.2469059406\n",
      "    val_loss       : -767346.8999023438\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -961552.500000\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -758572.937500\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -822134.875000\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -872672.875000\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -769589.125000\n",
      "    epoch          : 234\n",
      "    loss           : -769945.2103960396\n",
      "    val_loss       : -758284.2088623047\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -961742.187500\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -737074.375000\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -821008.250000\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -955952.125000\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -754781.062500\n",
      "    epoch          : 235\n",
      "    loss           : -768121.7574257426\n",
      "    val_loss       : -710205.8258056641\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -748705.250000\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -718739.375000\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -788574.125000\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -955127.250000\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -754852.000000\n",
      "    epoch          : 236\n",
      "    loss           : -756958.9238861386\n",
      "    val_loss       : -725040.3608520508\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -942749.125000\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -749379.250000\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -788840.812500\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -728369.875000\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -761070.062500\n",
      "    epoch          : 237\n",
      "    loss           : -762883.8793316832\n",
      "    val_loss       : -771485.7797241211\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -751428.437500\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -747699.187500\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -838447.312500\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -734871.062500\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -747698.375000\n",
      "    epoch          : 238\n",
      "    loss           : -771868.6311881188\n",
      "    val_loss       : -791336.3965332031\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -926129.062500\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -826981.625000\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -743170.000000\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -764487.437500\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -782367.000000\n",
      "    epoch          : 239\n",
      "    loss           : -774233.7128712871\n",
      "    val_loss       : -792680.5321289062\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -907254.625000\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -766571.125000\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -799878.375000\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -769736.375000\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -772207.375000\n",
      "    epoch          : 240\n",
      "    loss           : -785689.0724009901\n",
      "    val_loss       : -785389.7374267578\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -754277.375000\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -754673.875000\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -811159.125000\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -732262.875000\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -757588.125000\n",
      "    epoch          : 241\n",
      "    loss           : -781087.5396039604\n",
      "    val_loss       : -758619.6548583985\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -972958.187500\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -835916.000000\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -735266.375000\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -940731.875000\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -750993.250000\n",
      "    epoch          : 242\n",
      "    loss           : -775674.9749381188\n",
      "    val_loss       : -788213.9186523438\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -934168.687500\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -854795.375000\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -728689.000000\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -829052.812500\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -777928.625000\n",
      "    epoch          : 243\n",
      "    loss           : -787557.9721534654\n",
      "    val_loss       : -754291.7577026368\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -745975.750000\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -751068.687500\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -623273.625000\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -729060.000000\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -755964.812500\n",
      "    epoch          : 244\n",
      "    loss           : -764832.6800742574\n",
      "    val_loss       : -743560.1671508789\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -931174.000000\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -739291.500000\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -800590.875000\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -740206.000000\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -791327.125000\n",
      "    epoch          : 245\n",
      "    loss           : -758255.2858910891\n",
      "    val_loss       : -745420.2067993165\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -915940.875000\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -725402.625000\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -745424.062500\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -660265.375000\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -751492.937500\n",
      "    epoch          : 246\n",
      "    loss           : -763462.8189975248\n",
      "    val_loss       : -736520.8174072265\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -856082.125000\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -741915.250000\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -793570.625000\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -721473.062500\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -814295.312500\n",
      "    epoch          : 247\n",
      "    loss           : -774074.7617574257\n",
      "    val_loss       : -793981.974633789\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -816228.187500\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -737227.937500\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -755668.625000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -937412.375000\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -779349.750000\n",
      "    epoch          : 248\n",
      "    loss           : -776259.364480198\n",
      "    val_loss       : -785434.86171875\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -944812.625000\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -718381.625000\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -793642.937500\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -968993.250000\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -778437.500000\n",
      "    epoch          : 249\n",
      "    loss           : -778913.635519802\n",
      "    val_loss       : -765242.5372924805\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -964830.125000\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -653246.875000\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -773764.500000\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -705582.125000\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -731851.000000\n",
      "    epoch          : 250\n",
      "    loss           : -780282.6547029703\n",
      "    val_loss       : -756634.641027832\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0923_134115/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -815439.312500\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -611948.062500\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -787793.375000\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -817262.125000\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -807288.437500\n",
      "    epoch          : 251\n",
      "    loss           : -789719.9362623763\n",
      "    val_loss       : -793176.7568847656\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -882792.375000\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -752774.375000\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -802319.750000\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -747372.625000\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -775531.375000\n",
      "    epoch          : 252\n",
      "    loss           : -775708.1540841584\n",
      "    val_loss       : -740635.5086791993\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -905974.875000\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -669836.250000\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -748293.250000\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -711772.875000\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -794853.625000\n",
      "    epoch          : 253\n",
      "    loss           : -762728.7883663366\n",
      "    val_loss       : -726569.5062133789\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -932811.500000\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -739360.000000\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -819200.250000\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -765382.187500\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -758303.437500\n",
      "    epoch          : 254\n",
      "    loss           : -758144.9566831683\n",
      "    val_loss       : -758721.0695800781\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -665567.125000\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -826166.375000\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -764891.437500\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -733007.312500\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -719389.500000\n",
      "    epoch          : 255\n",
      "    loss           : -775117.4641089109\n",
      "    val_loss       : -782474.8421630859\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -929632.812500\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -781587.750000\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -678596.437500\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -700020.187500\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -787846.875000\n",
      "    epoch          : 256\n",
      "    loss           : -775039.8248762377\n",
      "    val_loss       : -762770.6618041992\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -924668.812500\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -761894.625000\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -812514.937500\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -744114.250000\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -741126.187500\n",
      "    epoch          : 257\n",
      "    loss           : -783564.4003712871\n",
      "    val_loss       : -752970.1374267578\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -919962.937500\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -845276.500000\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -800716.500000\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -819862.000000\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -798288.687500\n",
      "    epoch          : 258\n",
      "    loss           : -779378.0445544554\n",
      "    val_loss       : -776074.9349365234\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -976592.000000\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -748250.750000\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -850088.500000\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -976851.375000\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -780816.562500\n",
      "    epoch          : 259\n",
      "    loss           : -790678.7469059406\n",
      "    val_loss       : -807556.96171875\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -789347.125000\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -763242.500000\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -823798.625000\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -798465.375000\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -752982.000000\n",
      "    epoch          : 260\n",
      "    loss           : -776767.5377475248\n",
      "    val_loss       : -731113.7881591797\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -929928.312500\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -737086.125000\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -767890.062500\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -976303.000000\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -768292.500000\n",
      "    epoch          : 261\n",
      "    loss           : -760494.1379950495\n",
      "    val_loss       : -798077.2460449219\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -912464.000000\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -733309.000000\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -761778.125000\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -749441.625000\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -756965.625000\n",
      "    epoch          : 262\n",
      "    loss           : -778057.9102722772\n",
      "    val_loss       : -789955.5462158204\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -940209.437500\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -741479.375000\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -804196.750000\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -787789.500000\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -795759.750000\n",
      "    epoch          : 263\n",
      "    loss           : -786151.9201732674\n",
      "    val_loss       : -785503.9683227539\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -884534.750000\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -752752.812500\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -755687.875000\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -983235.000000\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -779669.250000\n",
      "    epoch          : 264\n",
      "    loss           : -783209.5092821782\n",
      "    val_loss       : -775861.9912963867\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -963275.687500\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -741457.062500\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -802348.250000\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -809790.750000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -817677.750000\n",
      "    epoch          : 265\n",
      "    loss           : -774722.7097772277\n",
      "    val_loss       : -769791.6844482422\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -941065.062500\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -840204.437500\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -819100.562500\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -805344.187500\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -800255.312500\n",
      "    epoch          : 266\n",
      "    loss           : -772158.9613242574\n",
      "    val_loss       : -786877.6047119141\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -926716.375000\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -818914.250000\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -819379.937500\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -778163.625000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -623672.062500\n",
      "    epoch          : 267\n",
      "    loss           : -766375.1181930694\n",
      "    val_loss       : -754821.6309326172\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -950374.500000\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -748104.750000\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -803878.312500\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -959877.875000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -802228.937500\n",
      "    epoch          : 268\n",
      "    loss           : -778158.854579208\n",
      "    val_loss       : -770861.8531860352\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -945250.875000\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -763013.937500\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -836989.625000\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -762702.375000\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -804548.062500\n",
      "    epoch          : 269\n",
      "    loss           : -782528.457920792\n",
      "    val_loss       : -760088.6847290039\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -970213.750000\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -789268.500000\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -799848.125000\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -606621.187500\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -769147.000000\n",
      "    epoch          : 270\n",
      "    loss           : -784274.010519802\n",
      "    val_loss       : -759966.1253051758\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -832729.250000\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -741326.437500\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -595534.062500\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -911197.875000\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -765811.562500\n",
      "    epoch          : 271\n",
      "    loss           : -777775.7252475248\n",
      "    val_loss       : -774792.7286621094\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -953690.437500\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -731539.750000\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -711050.000000\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -955418.000000\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -772276.500000\n",
      "    epoch          : 272\n",
      "    loss           : -785803.6237623763\n",
      "    val_loss       : -798064.874609375\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -867420.250000\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -795338.625000\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -788693.875000\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -726825.437500\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -796013.187500\n",
      "    epoch          : 273\n",
      "    loss           : -787243.8137376237\n",
      "    val_loss       : -756168.518762207\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -832789.187500\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -769580.000000\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -567816.125000\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -902034.625000\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -795867.562500\n",
      "    epoch          : 274\n",
      "    loss           : -789600.1547029703\n",
      "    val_loss       : -763778.3409301757\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -954919.437500\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -842612.625000\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -798065.500000\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -790448.875000\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -776345.375000\n",
      "    epoch          : 275\n",
      "    loss           : -785280.2348391089\n",
      "    val_loss       : -783781.4987792969\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -840875.812500\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -739622.375000\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -750174.625000\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -743948.625000\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -781541.875000\n",
      "    epoch          : 276\n",
      "    loss           : -787161.832920792\n",
      "    val_loss       : -794396.7758178711\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -961568.625000\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -740461.437500\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -740306.250000\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -819810.187500\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -782095.562500\n",
      "    epoch          : 277\n",
      "    loss           : -794353.6497524752\n",
      "    val_loss       : -796759.4417602539\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -986667.750000\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -791783.562500\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -753375.062500\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -770983.687500\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -782918.375000\n",
      "    epoch          : 278\n",
      "    loss           : -788659.2988861386\n",
      "    val_loss       : -799665.7427246093\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -975728.562500\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -767349.875000\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -862974.625000\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -742922.687500\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -820697.500000\n",
      "    epoch          : 279\n",
      "    loss           : -799726.2821782178\n",
      "    val_loss       : -795777.8006103516\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -940373.312500\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -747755.125000\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -774336.875000\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -812016.812500\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -769558.312500\n",
      "    epoch          : 280\n",
      "    loss           : -794786.4201732674\n",
      "    val_loss       : -786443.9380737304\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -964304.250000\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -782093.687500\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -738505.250000\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -703729.312500\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -795837.562500\n",
      "    epoch          : 281\n",
      "    loss           : -785945.9424504951\n",
      "    val_loss       : -796341.6875\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -850034.750000\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -751630.062500\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -775077.875000\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -831186.562500\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -788581.250000\n",
      "    epoch          : 282\n",
      "    loss           : -792924.0587871287\n",
      "    val_loss       : -778040.0043945312\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -946597.312500\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -781904.125000\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -793419.937500\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -767480.750000\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -795213.812500\n",
      "    epoch          : 283\n",
      "    loss           : -784127.7314356435\n",
      "    val_loss       : -777608.7352294922\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -948445.125000\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -882577.875000\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -654084.562500\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -710142.500000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -664793.187500\n",
      "    epoch          : 284\n",
      "    loss           : -780037.8892326732\n",
      "    val_loss       : -789629.6143066406\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -667683.562500\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -873116.625000\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -851006.812500\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -619260.625000\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -729649.562500\n",
      "    epoch          : 285\n",
      "    loss           : -775813.3610767326\n",
      "    val_loss       : -794152.9143554687\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -834532.437500\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -814108.937500\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -754594.187500\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -777821.062500\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -779952.187500\n",
      "    epoch          : 286\n",
      "    loss           : -776611.5922029703\n",
      "    val_loss       : -777979.3347412109\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -753075.750000\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -800468.750000\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -821865.625000\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -682718.500000\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -766232.062500\n",
      "    epoch          : 287\n",
      "    loss           : -786380.8341584158\n",
      "    val_loss       : -788099.4789550782\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -945928.812500\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -872669.937500\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -805564.687500\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -794214.812500\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -795233.875000\n",
      "    epoch          : 288\n",
      "    loss           : -790185.4529702971\n",
      "    val_loss       : -790082.872277832\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -958941.000000\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -812975.062500\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -725691.750000\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -833995.875000\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -795448.000000\n",
      "    epoch          : 289\n",
      "    loss           : -795871.1881188119\n",
      "    val_loss       : -789680.9392456055\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -955391.750000\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -791575.500000\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -714831.375000\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -824056.375000\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -769583.812500\n",
      "    epoch          : 290\n",
      "    loss           : -771816.6166460396\n",
      "    val_loss       : -796538.4156738281\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -931849.375000\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -817318.687500\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -743071.187500\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -954625.312500\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -764057.250000\n",
      "    epoch          : 291\n",
      "    loss           : -786272.3675742574\n",
      "    val_loss       : -789327.283996582\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -971838.125000\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -759004.875000\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -765728.875000\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -770465.500000\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -645229.000000\n",
      "    epoch          : 292\n",
      "    loss           : -797670.9504950495\n",
      "    val_loss       : -808601.3914306641\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -954238.375000\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -709115.437500\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -735081.250000\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -781546.500000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -743587.500000\n",
      "    epoch          : 293\n",
      "    loss           : -786383.875\n",
      "    val_loss       : -779720.6389648437\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -839371.687500\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -749387.125000\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -811164.750000\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -809814.375000\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -668709.875000\n",
      "    epoch          : 294\n",
      "    loss           : -777767.2227722772\n",
      "    val_loss       : -760371.7070434571\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -964714.812500\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -749268.000000\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -772010.062500\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -954768.875000\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -752323.750000\n",
      "    epoch          : 295\n",
      "    loss           : -788501.2475247525\n",
      "    val_loss       : -788622.9428710938\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -833637.187500\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -685194.500000\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -796194.625000\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -969404.437500\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -870668.750000\n",
      "    epoch          : 296\n",
      "    loss           : -787872.6373762377\n",
      "    val_loss       : -798199.6503173828\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -962822.437500\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -812729.500000\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -836424.750000\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -755367.687500\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -780112.250000\n",
      "    epoch          : 297\n",
      "    loss           : -793144.2363861386\n",
      "    val_loss       : -759210.6162841797\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -920177.750000\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -697771.437500\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -687025.437500\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -784582.187500\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -828148.812500\n",
      "    epoch          : 298\n",
      "    loss           : -785413.2852722772\n",
      "    val_loss       : -804417.7078125\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -966565.187500\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -727381.562500\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -762531.125000\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -812720.125000\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -790312.750000\n",
      "    epoch          : 299\n",
      "    loss           : -789900.6714108911\n",
      "    val_loss       : -776835.1424438476\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -963080.125000\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -820059.625000\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -830815.687500\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -642995.625000\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -785271.375000\n",
      "    epoch          : 300\n",
      "    loss           : -787766.3743811881\n",
      "    val_loss       : -801773.2340332031\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0923_134115/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -955315.250000\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -844795.000000\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -758893.250000\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -960518.562500\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -807076.375000\n",
      "    epoch          : 301\n",
      "    loss           : -787483.708539604\n",
      "    val_loss       : -785657.8303466797\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -954686.812500\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -757636.250000\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -824849.875000\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -762353.062500\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -809278.812500\n",
      "    epoch          : 302\n",
      "    loss           : -789994.5179455446\n",
      "    val_loss       : -790506.11953125\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -957804.500000\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -735092.250000\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -804247.750000\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -744584.875000\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -767232.125000\n",
      "    epoch          : 303\n",
      "    loss           : -788345.1113861386\n",
      "    val_loss       : -778338.5731079101\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -930591.000000\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -774729.250000\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -797129.375000\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -960538.562500\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -821064.500000\n",
      "    epoch          : 304\n",
      "    loss           : -795984.2345297029\n",
      "    val_loss       : -785339.4903564453\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -964211.125000\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -781138.875000\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -751923.375000\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -772369.125000\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -779750.750000\n",
      "    epoch          : 305\n",
      "    loss           : -795722.9511138614\n",
      "    val_loss       : -793687.20546875\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -965955.000000\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -645363.312500\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -788812.000000\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -805239.375000\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -775015.750000\n",
      "    epoch          : 306\n",
      "    loss           : -789953.3205445545\n",
      "    val_loss       : -787898.237487793\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -976274.937500\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -826901.375000\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -799399.250000\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -852996.687500\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -848890.687500\n",
      "    epoch          : 307\n",
      "    loss           : -789518.0501237623\n",
      "    val_loss       : -808507.3715454101\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -795657.125000\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -850700.375000\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -747622.312500\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -843903.500000\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -791217.250000\n",
      "    epoch          : 308\n",
      "    loss           : -799050.406559406\n",
      "    val_loss       : -800071.9325805664\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -971212.187500\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -803566.812500\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -818026.687500\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -828173.687500\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -776472.250000\n",
      "    epoch          : 309\n",
      "    loss           : -799774.0074257426\n",
      "    val_loss       : -785801.4887451172\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -747550.750000\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -799588.312500\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -776072.437500\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -745986.750000\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -817071.062500\n",
      "    epoch          : 310\n",
      "    loss           : -791421.4486386139\n",
      "    val_loss       : -789084.1715209961\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -973910.812500\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -784196.000000\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -863183.250000\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -826120.750000\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -802951.625000\n",
      "    epoch          : 311\n",
      "    loss           : -790513.010519802\n",
      "    val_loss       : -806688.6297607422\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -941302.000000\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -773419.687500\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -781749.250000\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -971413.375000\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -804320.875000\n",
      "    epoch          : 312\n",
      "    loss           : -797654.6299504951\n",
      "    val_loss       : -814944.6301025391\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -933198.750000\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -754248.687500\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -835288.812500\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -976907.312500\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -755829.000000\n",
      "    epoch          : 313\n",
      "    loss           : -803465.7103960396\n",
      "    val_loss       : -791268.3082763671\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -966689.625000\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -794120.562500\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -621330.500000\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -560917.250000\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -820287.500000\n",
      "    epoch          : 314\n",
      "    loss           : -794059.8650990099\n",
      "    val_loss       : -802202.0998046875\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -671963.687500\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -826591.250000\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -802143.437500\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -769091.125000\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -791242.937500\n",
      "    epoch          : 315\n",
      "    loss           : -797171.5853960396\n",
      "    val_loss       : -785042.1337036133\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -956845.187500\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -754230.250000\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -781625.937500\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -764126.937500\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -806160.875000\n",
      "    epoch          : 316\n",
      "    loss           : -795458.5488861386\n",
      "    val_loss       : -798002.0858154297\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -849280.812500\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -669245.437500\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -817967.375000\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -702461.875000\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -826827.187500\n",
      "    epoch          : 317\n",
      "    loss           : -795879.5278465346\n",
      "    val_loss       : -795401.0174072266\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -971430.750000\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -658714.000000\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -837444.750000\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -803120.375000\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -795344.437500\n",
      "    epoch          : 318\n",
      "    loss           : -799575.6101485149\n",
      "    val_loss       : -816628.4244628906\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -941670.875000\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -801843.750000\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -853961.562500\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -807706.500000\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -793690.750000\n",
      "    epoch          : 319\n",
      "    loss           : -798378.5990099009\n",
      "    val_loss       : -794089.4096069336\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -963992.250000\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -854363.500000\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -823015.812500\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -780068.937500\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -805539.750000\n",
      "    epoch          : 320\n",
      "    loss           : -804216.9919554455\n",
      "    val_loss       : -803262.0242675781\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -786647.875000\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -859294.500000\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -667343.375000\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -739479.250000\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -768501.812500\n",
      "    epoch          : 321\n",
      "    loss           : -791293.0099009901\n",
      "    val_loss       : -807430.4544433594\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -766053.062500\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -765905.875000\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -751764.250000\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -734499.375000\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -808426.125000\n",
      "    epoch          : 322\n",
      "    loss           : -788392.2896039604\n",
      "    val_loss       : -785986.2797729492\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -960638.312500\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -805654.000000\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -735613.375000\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -638316.625000\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -816533.000000\n",
      "    epoch          : 323\n",
      "    loss           : -789374.864480198\n",
      "    val_loss       : -793237.7555664063\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -963124.875000\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -660788.937500\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -811251.562500\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -761353.812500\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -764208.875000\n",
      "    epoch          : 324\n",
      "    loss           : -800658.1305693069\n",
      "    val_loss       : -761579.5762817382\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -957171.125000\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -826923.000000\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -799631.125000\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -855633.062500\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -770688.875000\n",
      "    epoch          : 325\n",
      "    loss           : -801949.4548267326\n",
      "    val_loss       : -808787.4272216797\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -951244.562500\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -779925.750000\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -755130.687500\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -822595.437500\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -785641.875000\n",
      "    epoch          : 326\n",
      "    loss           : -794865.5959158416\n",
      "    val_loss       : -763615.0626098632\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -984353.937500\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -800715.250000\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -745850.937500\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -835393.562500\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -778526.000000\n",
      "    epoch          : 327\n",
      "    loss           : -800258.7772277228\n",
      "    val_loss       : -793410.5505859375\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -772195.562500\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -869366.750000\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -814402.437500\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -870592.187500\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -804182.437500\n",
      "    epoch          : 328\n",
      "    loss           : -794420.1448019802\n",
      "    val_loss       : -790442.2372192383\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -869597.000000\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -762114.500000\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -776225.812500\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -852945.375000\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -821079.937500\n",
      "    epoch          : 329\n",
      "    loss           : -797331.8118811881\n",
      "    val_loss       : -784861.9103637695\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -951386.750000\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -788034.312500\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -836800.875000\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -721105.000000\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -812205.750000\n",
      "    epoch          : 330\n",
      "    loss           : -805559.1955445545\n",
      "    val_loss       : -777739.1495483399\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -975175.250000\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -792400.000000\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -744180.750000\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -694020.187500\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -644923.000000\n",
      "    epoch          : 331\n",
      "    loss           : -789352.6955445545\n",
      "    val_loss       : -810507.9665527344\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -570161.875000\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -796775.250000\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -693260.875000\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -772988.687500\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -770727.125000\n",
      "    epoch          : 332\n",
      "    loss           : -803801.8570544554\n",
      "    val_loss       : -805795.0611083985\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -948691.250000\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -774199.125000\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -776501.500000\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -954580.125000\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -801291.375000\n",
      "    epoch          : 333\n",
      "    loss           : -797954.9783415842\n",
      "    val_loss       : -795417.5911499023\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -871555.250000\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -775936.500000\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -879325.125000\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -761966.625000\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -795047.875000\n",
      "    epoch          : 334\n",
      "    loss           : -808909.2469059406\n",
      "    val_loss       : -798757.8755615235\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -966727.312500\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -618680.125000\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -795575.750000\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -720798.000000\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -742922.875000\n",
      "    epoch          : 335\n",
      "    loss           : -791018.2685643565\n",
      "    val_loss       : -791790.6994018555\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -976473.437500\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -748751.125000\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -800988.500000\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -806205.250000\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -804949.500000\n",
      "    epoch          : 336\n",
      "    loss           : -790859.7521658416\n",
      "    val_loss       : -795267.1404907226\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -991736.625000\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -807078.500000\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -650113.750000\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -669372.375000\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -754469.000000\n",
      "    epoch          : 337\n",
      "    loss           : -799733.1738861386\n",
      "    val_loss       : -790143.3195800781\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -881187.500000\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -763037.000000\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -852027.375000\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -744114.625000\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -763535.875000\n",
      "    epoch          : 338\n",
      "    loss           : -804868.7933168317\n",
      "    val_loss       : -808532.535131836\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -976769.250000\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -838869.250000\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -602254.687500\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -767090.125000\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -671239.750000\n",
      "    epoch          : 339\n",
      "    loss           : -801451.1819306931\n",
      "    val_loss       : -819662.2972412109\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -913269.375000\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -861350.187500\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -708802.875000\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -760928.875000\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -825424.812500\n",
      "    epoch          : 340\n",
      "    loss           : -799358.5501237623\n",
      "    val_loss       : -800456.8999023438\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -829281.562500\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -853578.125000\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -818206.937500\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -990981.125000\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -761091.250000\n",
      "    epoch          : 341\n",
      "    loss           : -801872.7858910891\n",
      "    val_loss       : -801888.6811523438\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -829197.000000\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -841875.062500\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -827665.437500\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -750157.500000\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -664687.687500\n",
      "    epoch          : 342\n",
      "    loss           : -798728.5736386139\n",
      "    val_loss       : -794517.3908691406\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -938457.250000\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -634056.125000\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -740217.000000\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -715204.625000\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -807029.000000\n",
      "    epoch          : 343\n",
      "    loss           : -786979.7543316832\n",
      "    val_loss       : -806862.0194091797\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -818998.062500\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -708050.125000\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -776926.812500\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -819166.000000\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -740353.875000\n",
      "    epoch          : 344\n",
      "    loss           : -794326.7227722772\n",
      "    val_loss       : -797531.2546386719\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -956284.937500\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -798172.250000\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -803188.562500\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -828082.500000\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -809629.750000\n",
      "    epoch          : 345\n",
      "    loss           : -803731.1120049505\n",
      "    val_loss       : -811253.2879516601\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -969149.562500\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -768152.812500\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -818761.000000\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -962589.375000\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -816054.125000\n",
      "    epoch          : 346\n",
      "    loss           : -796948.364480198\n",
      "    val_loss       : -811401.8187011719\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -687784.437500\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -786570.625000\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -834780.687500\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -795854.437500\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -776362.750000\n",
      "    epoch          : 347\n",
      "    loss           : -793911.0990099009\n",
      "    val_loss       : -811772.6795898437\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -853696.562500\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -885514.125000\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -855132.312500\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -954461.937500\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -784432.125000\n",
      "    epoch          : 348\n",
      "    loss           : -803203.9826732674\n",
      "    val_loss       : -816330.3719726562\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -834407.500000\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -809650.500000\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -866092.250000\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -757521.437500\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -829383.312500\n",
      "    epoch          : 349\n",
      "    loss           : -799892.771039604\n",
      "    val_loss       : -809951.3367431641\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -828731.375000\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -852447.250000\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -775034.125000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -955870.500000\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -838066.937500\n",
      "    epoch          : 350\n",
      "    loss           : -809669.8762376237\n",
      "    val_loss       : -805451.5234863281\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0923_134115/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -976602.187500\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -881680.562500\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -781706.375000\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -959181.562500\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -816359.125000\n",
      "    epoch          : 351\n",
      "    loss           : -803835.7054455446\n",
      "    val_loss       : -815203.9763916016\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -976103.625000\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -825559.062500\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -773326.375000\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -763145.312500\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -783108.125000\n",
      "    epoch          : 352\n",
      "    loss           : -815264.708539604\n",
      "    val_loss       : -788173.9848022461\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -975160.312500\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -897467.500000\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -786616.937500\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -855857.375000\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -849335.125000\n",
      "    epoch          : 353\n",
      "    loss           : -813203.8452970297\n",
      "    val_loss       : -812219.4436523437\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -981418.437500\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -637854.000000\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -873493.625000\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -738840.125000\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -751569.937500\n",
      "    epoch          : 354\n",
      "    loss           : -789109.2660891089\n",
      "    val_loss       : -800127.9330078125\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -948212.875000\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -730812.812500\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -825386.250000\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -740869.000000\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -789498.875000\n",
      "    epoch          : 355\n",
      "    loss           : -795682.3613861386\n",
      "    val_loss       : -812361.3030761719\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -960855.625000\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -765265.187500\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -723519.062500\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -832391.750000\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -801331.187500\n",
      "    epoch          : 356\n",
      "    loss           : -798440.7561881188\n",
      "    val_loss       : -794847.7618530274\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -987341.437500\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -782822.375000\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -801353.437500\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -798865.750000\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -793223.875000\n",
      "    epoch          : 357\n",
      "    loss           : -798564.4183168317\n",
      "    val_loss       : -797976.2486694336\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -933442.937500\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -847963.375000\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -786828.000000\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -618391.250000\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -784608.562500\n",
      "    epoch          : 358\n",
      "    loss           : -789002.9474009901\n",
      "    val_loss       : -766190.8822998047\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -935617.125000\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -819470.875000\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -859530.625000\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -817931.000000\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -816919.062500\n",
      "    epoch          : 359\n",
      "    loss           : -805513.9461633663\n",
      "    val_loss       : -778520.6075317382\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -974359.250000\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -805827.750000\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -500279.000000\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -677795.187500\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -806751.875000\n",
      "    epoch          : 360\n",
      "    loss           : -806769.655940594\n",
      "    val_loss       : -826721.0768066406\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -976226.812500\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -749817.687500\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -830020.062500\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -816002.625000\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -731724.750000\n",
      "    epoch          : 361\n",
      "    loss           : -805817.1633663366\n",
      "    val_loss       : -804428.5905395508\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -972413.875000\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -864249.437500\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -858368.375000\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -793830.000000\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -788651.062500\n",
      "    epoch          : 362\n",
      "    loss           : -805034.0049504951\n",
      "    val_loss       : -807992.7466796875\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -917858.750000\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -848864.500000\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -747650.125000\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -812522.250000\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -809981.312500\n",
      "    epoch          : 363\n",
      "    loss           : -798528.093440594\n",
      "    val_loss       : -822216.4704833984\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -1001144.250000\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -849406.625000\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -777089.750000\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -834369.500000\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -848117.562500\n",
      "    epoch          : 364\n",
      "    loss           : -811968.1608910891\n",
      "    val_loss       : -826888.7694580078\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -981695.562500\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -797045.375000\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -832979.937500\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -958906.312500\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -798775.875000\n",
      "    epoch          : 365\n",
      "    loss           : -813614.2995049505\n",
      "    val_loss       : -813811.7502441406\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -972690.750000\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -822293.500000\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -811006.437500\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -783979.562500\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -846397.875000\n",
      "    epoch          : 366\n",
      "    loss           : -801047.7431930694\n",
      "    val_loss       : -797241.4985229492\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -966433.250000\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -864531.812500\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -812243.250000\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -834300.875000\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -773735.937500\n",
      "    epoch          : 367\n",
      "    loss           : -795776.0086633663\n",
      "    val_loss       : -786699.6391967774\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -622333.750000\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -825874.125000\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -820530.812500\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -586497.062500\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -677748.750000\n",
      "    epoch          : 368\n",
      "    loss           : -796829.5779702971\n",
      "    val_loss       : -796481.9066040039\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -890423.125000\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -805297.125000\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -794545.187500\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -800261.125000\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -730390.625000\n",
      "    epoch          : 369\n",
      "    loss           : -811754.7060643565\n",
      "    val_loss       : -814591.1068603515\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -951346.000000\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -874998.937500\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -780696.125000\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -944873.875000\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -658891.312500\n",
      "    epoch          : 370\n",
      "    loss           : -794351.4009900991\n",
      "    val_loss       : -794012.22734375\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -810132.937500\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -656728.750000\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -776712.500000\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -754515.875000\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -753995.437500\n",
      "    epoch          : 371\n",
      "    loss           : -800704.0823019802\n",
      "    val_loss       : -783085.6547851562\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -943724.500000\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -863639.312500\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -759095.187500\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -832477.187500\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -790714.375000\n",
      "    epoch          : 372\n",
      "    loss           : -806491.5142326732\n",
      "    val_loss       : -796080.523413086\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -967526.875000\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -778560.875000\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -788441.812500\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -827924.937500\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -835101.500000\n",
      "    epoch          : 373\n",
      "    loss           : -806127.1274752475\n",
      "    val_loss       : -795267.7180297852\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -774269.500000\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -638703.000000\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -747176.125000\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -803472.750000\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -775734.750000\n",
      "    epoch          : 374\n",
      "    loss           : -801606.905940594\n",
      "    val_loss       : -807862.7346435547\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -989560.750000\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -809871.062500\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -831614.250000\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -862732.125000\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -822000.687500\n",
      "    epoch          : 375\n",
      "    loss           : -803244.7784653465\n",
      "    val_loss       : -807643.5739501953\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -954131.625000\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -755945.125000\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -761286.750000\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -820044.875000\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -693029.687500\n",
      "    epoch          : 376\n",
      "    loss           : -800700.2673267326\n",
      "    val_loss       : -815364.377734375\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -974611.312500\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -884253.062500\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -808285.125000\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -819189.375000\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -863486.500000\n",
      "    epoch          : 377\n",
      "    loss           : -800277.3242574257\n",
      "    val_loss       : -779058.4529052734\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -817534.937500\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -796738.125000\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -829313.375000\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -972378.625000\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -757593.250000\n",
      "    epoch          : 378\n",
      "    loss           : -806150.2747524752\n",
      "    val_loss       : -772986.4323974609\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -967040.625000\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -831027.875000\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -779732.250000\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -828398.125000\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -835453.875000\n",
      "    epoch          : 379\n",
      "    loss           : -806337.5052599009\n",
      "    val_loss       : -763278.2161621094\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -985124.062500\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -777816.375000\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -784212.125000\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -793597.812500\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -779540.937500\n",
      "    epoch          : 380\n",
      "    loss           : -805634.5587871287\n",
      "    val_loss       : -812919.3489257812\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -777428.000000\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -718773.250000\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -581567.437500\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -779158.437500\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -791452.187500\n",
      "    epoch          : 381\n",
      "    loss           : -807450.1017945545\n",
      "    val_loss       : -824062.2708251954\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -961204.125000\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -777025.125000\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -858185.375000\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -792805.250000\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -762091.250000\n",
      "    epoch          : 382\n",
      "    loss           : -810963.3168316832\n",
      "    val_loss       : -798848.9075805664\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -921932.125000\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -889957.937500\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -835008.375000\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -754459.250000\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -698998.000000\n",
      "    epoch          : 383\n",
      "    loss           : -811443.7153465346\n",
      "    val_loss       : -811312.8912353516\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -929600.375000\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -782217.000000\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -843860.875000\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -694191.875000\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -755728.687500\n",
      "    epoch          : 384\n",
      "    loss           : -794179.9121287129\n",
      "    val_loss       : -801080.239453125\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -967866.312500\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -715926.125000\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -844589.500000\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -852129.562500\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -792847.875000\n",
      "    epoch          : 385\n",
      "    loss           : -810318.8917079208\n",
      "    val_loss       : -814458.7170288085\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -817359.812500\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -885687.562500\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -609936.250000\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -869949.500000\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -811715.500000\n",
      "    epoch          : 386\n",
      "    loss           : -811641.9566831683\n",
      "    val_loss       : -811309.4372924805\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -982366.750000\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -819036.500000\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -651412.437500\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -830034.875000\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -810258.500000\n",
      "    epoch          : 387\n",
      "    loss           : -801286.3298267326\n",
      "    val_loss       : -811452.1251220703\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -939081.250000\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -830129.562500\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -761217.312500\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -738413.250000\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -808834.750000\n",
      "    epoch          : 388\n",
      "    loss           : -788636.9195544554\n",
      "    val_loss       : -771826.8612670898\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -966464.437500\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -761385.312500\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -739511.375000\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -808781.875000\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -831515.750000\n",
      "    epoch          : 389\n",
      "    loss           : -797378.9294554455\n",
      "    val_loss       : -814743.8445556641\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -968059.312500\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -784180.625000\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -744121.500000\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -770588.687500\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -776831.625000\n",
      "    epoch          : 390\n",
      "    loss           : -799073.489480198\n",
      "    val_loss       : -811898.1850097657\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -923219.750000\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -821235.875000\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -687301.000000\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -962657.062500\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -771615.437500\n",
      "    epoch          : 391\n",
      "    loss           : -797236.1503712871\n",
      "    val_loss       : -796296.7635009766\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -980320.812500\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -759354.625000\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -758470.500000\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -738341.687500\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -795265.625000\n",
      "    epoch          : 392\n",
      "    loss           : -808401.3935643565\n",
      "    val_loss       : -775559.1402587891\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -950940.000000\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -676983.250000\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -841751.937500\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -775894.687500\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -791319.250000\n",
      "    epoch          : 393\n",
      "    loss           : -811604.0365099009\n",
      "    val_loss       : -809631.7068237305\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -982895.750000\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -821108.625000\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -793234.062500\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -826510.375000\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -809719.687500\n",
      "    epoch          : 394\n",
      "    loss           : -821365.2648514851\n",
      "    val_loss       : -817133.1693725586\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -986151.375000\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -826577.312500\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -640062.562500\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -872723.750000\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -835146.750000\n",
      "    epoch          : 395\n",
      "    loss           : -814589.176980198\n",
      "    val_loss       : -786127.6126220704\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -815148.375000\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -801979.250000\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -623577.812500\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -794871.562500\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -824930.687500\n",
      "    epoch          : 396\n",
      "    loss           : -804284.2004950495\n",
      "    val_loss       : -786234.6350585937\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -965417.500000\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -812433.125000\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -780429.812500\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -861148.625000\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -808379.750000\n",
      "    epoch          : 397\n",
      "    loss           : -800193.5606435643\n",
      "    val_loss       : -820594.6733093262\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -954131.062500\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -782865.312500\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -772444.812500\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -787943.687500\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -803941.125000\n",
      "    epoch          : 398\n",
      "    loss           : -818151.5365099009\n",
      "    val_loss       : -833882.1942382812\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -995544.500000\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -779289.500000\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -864101.625000\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -952448.125000\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -820850.250000\n",
      "    epoch          : 399\n",
      "    loss           : -801899.4387376237\n",
      "    val_loss       : -772459.6141418457\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -942661.625000\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -785398.187500\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -791466.000000\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -650921.125000\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -780228.000000\n",
      "    epoch          : 400\n",
      "    loss           : -811605.9702970297\n",
      "    val_loss       : -783472.0904296875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0923_134115/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -956819.562500\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -723398.000000\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -821127.500000\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -866338.125000\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -549547.625000\n",
      "    epoch          : 401\n",
      "    loss           : -815376.6998762377\n",
      "    val_loss       : -818379.9993408204\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -749534.375000\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -792388.500000\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -769504.937500\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -843185.062500\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -875909.250000\n",
      "    epoch          : 402\n",
      "    loss           : -816102.5198019802\n",
      "    val_loss       : -809434.8528869629\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -952879.187500\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -701363.562500\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -779562.500000\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -799964.687500\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -752979.500000\n",
      "    epoch          : 403\n",
      "    loss           : -810039.1813118812\n",
      "    val_loss       : -808176.811730957\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -959527.875000\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -821355.875000\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -735296.000000\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -876279.125000\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -826387.437500\n",
      "    epoch          : 404\n",
      "    loss           : -803208.4337871287\n",
      "    val_loss       : -822613.7332519531\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -935668.750000\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -707727.000000\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -808593.000000\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -812485.187500\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -820238.375000\n",
      "    epoch          : 405\n",
      "    loss           : -807730.0922029703\n",
      "    val_loss       : -819687.5979003906\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -811010.375000\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -870353.500000\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -780547.875000\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -821733.625000\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -774263.250000\n",
      "    epoch          : 406\n",
      "    loss           : -809483.1008663366\n",
      "    val_loss       : -834652.4133544922\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -831752.312500\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -750776.562500\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -696973.937500\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -852130.375000\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -801284.375000\n",
      "    epoch          : 407\n",
      "    loss           : -812945.7153465346\n",
      "    val_loss       : -824857.1997924804\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -959045.625000\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -788981.750000\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -714194.562500\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -851406.687500\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -768774.562500\n",
      "    epoch          : 408\n",
      "    loss           : -817727.5990099009\n",
      "    val_loss       : -826772.4343139648\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -953231.750000\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -802274.125000\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -799759.312500\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -963784.437500\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -742816.375000\n",
      "    epoch          : 409\n",
      "    loss           : -805592.957920792\n",
      "    val_loss       : -822915.0823486329\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -834685.500000\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -874131.187500\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -870886.437500\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -734091.750000\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -801332.062500\n",
      "    epoch          : 410\n",
      "    loss           : -804901.4965965346\n",
      "    val_loss       : -798758.9806518555\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -965423.062500\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -770278.125000\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -813972.875000\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -833977.375000\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -770753.437500\n",
      "    epoch          : 411\n",
      "    loss           : -816485.2747524752\n",
      "    val_loss       : -818216.7384521484\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -874603.875000\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -768535.125000\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -800914.437500\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -960196.875000\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -802296.750000\n",
      "    epoch          : 412\n",
      "    loss           : -808923.0507425743\n",
      "    val_loss       : -796228.572869873\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -768565.375000\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -807090.125000\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -773169.125000\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -777497.625000\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -753601.375000\n",
      "    epoch          : 413\n",
      "    loss           : -810001.417079208\n",
      "    val_loss       : -829791.2769042968\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -983829.062500\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -646220.250000\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -850328.625000\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -993136.437500\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -792692.625000\n",
      "    epoch          : 414\n",
      "    loss           : -823944.3118811881\n",
      "    val_loss       : -802414.2032470703\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -864622.187500\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -823813.312500\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -798174.000000\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -804171.687500\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -844852.687500\n",
      "    epoch          : 415\n",
      "    loss           : -808990.8428217822\n",
      "    val_loss       : -835280.984350586\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -853304.187500\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -813144.750000\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -787436.000000\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -780596.375000\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -805205.312500\n",
      "    epoch          : 416\n",
      "    loss           : -810455.6905940594\n",
      "    val_loss       : -830319.7681396485\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -959068.062500\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -748581.625000\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -859717.125000\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -791387.125000\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -702568.375000\n",
      "    epoch          : 417\n",
      "    loss           : -813932.6002475248\n",
      "    val_loss       : -796938.6009277344\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -968805.562500\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -783965.250000\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -807286.000000\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -856050.937500\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -855618.000000\n",
      "    epoch          : 418\n",
      "    loss           : -820166.9235767326\n",
      "    val_loss       : -803181.0810180664\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -975734.125000\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -793183.937500\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -848451.000000\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -851471.750000\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -842649.000000\n",
      "    epoch          : 419\n",
      "    loss           : -817085.0297029703\n",
      "    val_loss       : -788660.2166625976\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -978285.687500\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -811752.750000\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -845117.875000\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -834577.750000\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -787414.250000\n",
      "    epoch          : 420\n",
      "    loss           : -813145.2382425743\n",
      "    val_loss       : -790969.8843994141\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -764785.687500\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -811190.875000\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -830881.250000\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -756054.625000\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -799855.125000\n",
      "    epoch          : 421\n",
      "    loss           : -812802.6782178218\n",
      "    val_loss       : -821962.1699096679\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -970601.687500\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -815986.875000\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -793425.187500\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -814861.125000\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -823534.687500\n",
      "    epoch          : 422\n",
      "    loss           : -816877.6311881188\n",
      "    val_loss       : -812333.1847167969\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -841174.062500\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -879064.812500\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -830523.437500\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -736061.562500\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -800933.250000\n",
      "    epoch          : 423\n",
      "    loss           : -802234.364480198\n",
      "    val_loss       : -819700.5962158203\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -946416.250000\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -745999.500000\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -830257.500000\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -811247.500000\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -829074.812500\n",
      "    epoch          : 424\n",
      "    loss           : -811514.5569306931\n",
      "    val_loss       : -803002.1176513672\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -839042.937500\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -877782.687500\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -802734.062500\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -690988.000000\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -799301.312500\n",
      "    epoch          : 425\n",
      "    loss           : -813598.9622524752\n",
      "    val_loss       : -802482.8641357422\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -851719.812500\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -826956.875000\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -862099.437500\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -815210.125000\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -813211.875000\n",
      "    epoch          : 426\n",
      "    loss           : -811814.9820544554\n",
      "    val_loss       : -804876.62578125\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -953644.687500\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -731126.125000\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -842508.500000\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -592959.437500\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -823900.500000\n",
      "    epoch          : 427\n",
      "    loss           : -809416.3038366337\n",
      "    val_loss       : -808597.8498779297\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -977718.125000\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -875370.375000\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -850980.562500\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -979807.750000\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -803964.000000\n",
      "    epoch          : 428\n",
      "    loss           : -810644.0408415842\n",
      "    val_loss       : -812520.0501708984\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -823343.312500\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -838871.625000\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -794680.500000\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -812429.375000\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -811651.125000\n",
      "    epoch          : 429\n",
      "    loss           : -813640.3672648515\n",
      "    val_loss       : -806873.6262817383\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -950642.437500\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -820851.812500\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -791507.187500\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -797222.062500\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -832698.750000\n",
      "    epoch          : 430\n",
      "    loss           : -807236.6318069306\n",
      "    val_loss       : -811598.7970581055\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -917834.375000\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -831217.000000\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -782949.375000\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -862395.125000\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -811970.437500\n",
      "    epoch          : 431\n",
      "    loss           : -804905.2462871287\n",
      "    val_loss       : -796418.5071533204\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -976905.750000\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -710920.000000\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -749051.187500\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -923787.562500\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -781668.812500\n",
      "    epoch          : 432\n",
      "    loss           : -809076.2648514851\n",
      "    val_loss       : -790540.9647827148\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -923193.750000\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -775312.937500\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -770105.312500\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -809959.687500\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -851924.375000\n",
      "    epoch          : 433\n",
      "    loss           : -813422.6701732674\n",
      "    val_loss       : -819222.646875\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -972112.625000\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -900437.375000\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -797250.437500\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -979991.562500\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -733817.125000\n",
      "    epoch          : 434\n",
      "    loss           : -814183.6726485149\n",
      "    val_loss       : -829649.8274169922\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -984007.687500\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -813104.312500\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -795205.437500\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -842356.687500\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -851131.250000\n",
      "    epoch          : 435\n",
      "    loss           : -812780.5891089109\n",
      "    val_loss       : -808577.8667358399\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -953560.500000\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -834110.000000\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -783989.562500\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -776579.375000\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -694558.000000\n",
      "    epoch          : 436\n",
      "    loss           : -810840.0383663366\n",
      "    val_loss       : -801670.054711914\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -971336.500000\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -766339.375000\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -871045.500000\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -755380.812500\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -789841.812500\n",
      "    epoch          : 437\n",
      "    loss           : -797255.5785891089\n",
      "    val_loss       : -769708.4763427734\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -961060.250000\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -711175.812500\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -820938.062500\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -662655.187500\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -815247.312500\n",
      "    epoch          : 438\n",
      "    loss           : -802648.8186881188\n",
      "    val_loss       : -803796.8046142578\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -947605.187500\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -797899.250000\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -777692.750000\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -794866.750000\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -809472.937500\n",
      "    epoch          : 439\n",
      "    loss           : -804117.1528465346\n",
      "    val_loss       : -794602.3208007812\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -990929.437500\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -611142.375000\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -837099.375000\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -811257.937500\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -796066.000000\n",
      "    epoch          : 440\n",
      "    loss           : -813204.9721534654\n",
      "    val_loss       : -812179.5911621094\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -950038.312500\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -765535.687500\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -786685.250000\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -966413.250000\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -794432.187500\n",
      "    epoch          : 441\n",
      "    loss           : -806252.1089108911\n",
      "    val_loss       : -833381.1952636719\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -956456.250000\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -870747.375000\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -820137.125000\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -871940.375000\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -741224.312500\n",
      "    epoch          : 442\n",
      "    loss           : -819653.3508663366\n",
      "    val_loss       : -845017.1999023438\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -946350.375000\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -749648.250000\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -808382.562500\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -775475.375000\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -804973.687500\n",
      "    epoch          : 443\n",
      "    loss           : -819699.4653465346\n",
      "    val_loss       : -786348.7260131836\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -969666.937500\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -796332.437500\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -801294.062500\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -827442.625000\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -851293.125000\n",
      "    epoch          : 444\n",
      "    loss           : -812264.9096534654\n",
      "    val_loss       : -821938.6577636718\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -762343.500000\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -695417.062500\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -736522.687500\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -656526.000000\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -849741.500000\n",
      "    epoch          : 445\n",
      "    loss           : -814296.4647277228\n",
      "    val_loss       : -791585.5876708984\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -977026.687500\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -769412.375000\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -795680.437500\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -782359.812500\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -780152.000000\n",
      "    epoch          : 446\n",
      "    loss           : -817801.8366336634\n",
      "    val_loss       : -827770.7614746094\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -725492.250000\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -829263.750000\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -843644.187500\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -847435.062500\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -865780.000000\n",
      "    epoch          : 447\n",
      "    loss           : -823130.3787128713\n",
      "    val_loss       : -801922.5825317383\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -966008.250000\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -795272.625000\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -758612.437500\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -762494.125000\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -760277.500000\n",
      "    epoch          : 448\n",
      "    loss           : -814199.7004950495\n",
      "    val_loss       : -802896.2951782227\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -938582.062500\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -851241.812500\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -862803.625000\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -974266.562500\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -854388.625000\n",
      "    epoch          : 449\n",
      "    loss           : -817604.521039604\n",
      "    val_loss       : -813527.5991455078\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -942671.875000\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -815968.937500\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -807216.187500\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -781202.687500\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -747615.000000\n",
      "    epoch          : 450\n",
      "    loss           : -799999.6441831683\n",
      "    val_loss       : -813174.2946289063\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0923_134115/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -876763.375000\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -796913.625000\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -857532.375000\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -809430.187500\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -838802.375000\n",
      "    epoch          : 451\n",
      "    loss           : -815933.7487623763\n",
      "    val_loss       : -801331.42399292\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -954849.250000\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -818553.875000\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -796452.625000\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -837255.187500\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -798221.625000\n",
      "    epoch          : 452\n",
      "    loss           : -816677.083539604\n",
      "    val_loss       : -808658.3500488282\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -801402.250000\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -794413.812500\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -849491.000000\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -836692.812500\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -850176.437500\n",
      "    epoch          : 453\n",
      "    loss           : -820642.6336633663\n",
      "    val_loss       : -787895.72868042\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -973188.187500\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -778578.250000\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -849394.625000\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -832565.000000\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -864483.562500\n",
      "    epoch          : 454\n",
      "    loss           : -814105.1850247525\n",
      "    val_loss       : -829075.7846801758\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -961316.437500\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -871424.625000\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -841229.625000\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -926648.875000\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -777721.687500\n",
      "    epoch          : 455\n",
      "    loss           : -804523.8273514851\n",
      "    val_loss       : -774785.8963378906\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -850799.000000\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -796797.000000\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -705796.625000\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -805706.625000\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -808539.125000\n",
      "    epoch          : 456\n",
      "    loss           : -805504.031559406\n",
      "    val_loss       : -811895.7025268555\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -679543.125000\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -808696.812500\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -783880.750000\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -859939.750000\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -814043.562500\n",
      "    epoch          : 457\n",
      "    loss           : -818507.5408415842\n",
      "    val_loss       : -799785.9485656738\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -962848.312500\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -799242.875000\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -854327.000000\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -770094.687500\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -843450.750000\n",
      "    epoch          : 458\n",
      "    loss           : -813168.1943069306\n",
      "    val_loss       : -821517.5205566406\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -947585.312500\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -812924.312500\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -737100.875000\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -812234.125000\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -774022.000000\n",
      "    epoch          : 459\n",
      "    loss           : -813311.2407178218\n",
      "    val_loss       : -834421.6618652344\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -852636.437500\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -871825.125000\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -849443.875000\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -682777.312500\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -798538.687500\n",
      "    epoch          : 460\n",
      "    loss           : -821849.7103960396\n",
      "    val_loss       : -790587.4620239257\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -892078.687500\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -828265.750000\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -841794.000000\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -839651.437500\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -835234.937500\n",
      "    epoch          : 461\n",
      "    loss           : -824466.9845297029\n",
      "    val_loss       : -828705.9405029297\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -982912.000000\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -786796.562500\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -798783.312500\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -816048.562500\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -844641.375000\n",
      "    epoch          : 462\n",
      "    loss           : -817640.0198019802\n",
      "    val_loss       : -787393.1702819824\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -974661.687500\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -771483.187500\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -848529.000000\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -802648.375000\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -832137.250000\n",
      "    epoch          : 463\n",
      "    loss           : -819489.5371287129\n",
      "    val_loss       : -815212.4475830079\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -977746.500000\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -781246.000000\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -756384.687500\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -849849.125000\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -826927.937500\n",
      "    epoch          : 464\n",
      "    loss           : -812172.8378712871\n",
      "    val_loss       : -834053.7217041015\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -845940.187500\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -826784.875000\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -844297.750000\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -791050.312500\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -796263.375000\n",
      "    epoch          : 465\n",
      "    loss           : -811642.5024752475\n",
      "    val_loss       : -828509.6844604493\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -943238.500000\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -768912.562500\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -620245.625000\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -788578.187500\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -785667.000000\n",
      "    epoch          : 466\n",
      "    loss           : -815232.5612623763\n",
      "    val_loss       : -811893.7752563476\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -816825.125000\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -818408.062500\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -750761.500000\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -976270.812500\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -826235.875000\n",
      "    epoch          : 467\n",
      "    loss           : -813090.8793316832\n",
      "    val_loss       : -823245.5746704101\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -976826.562500\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -771546.375000\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -781773.375000\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -800478.312500\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -782348.812500\n",
      "    epoch          : 468\n",
      "    loss           : -826738.2240099009\n",
      "    val_loss       : -830016.494165039\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -976918.875000\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -776776.250000\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -806680.687500\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -833287.312500\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -708854.250000\n",
      "    epoch          : 469\n",
      "    loss           : -818769.6163366337\n",
      "    val_loss       : -821040.2293823243\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -843873.625000\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -791567.937500\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -773485.625000\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -984195.437500\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -814775.000000\n",
      "    epoch          : 470\n",
      "    loss           : -834867.1101485149\n",
      "    val_loss       : -817465.3703979492\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -816260.937500\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -777852.000000\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -794959.000000\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -876298.625000\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -825021.875000\n",
      "    epoch          : 471\n",
      "    loss           : -834102.3032178218\n",
      "    val_loss       : -829109.494519043\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -842823.625000\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -856172.750000\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -807937.312500\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -831033.687500\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -809188.500000\n",
      "    epoch          : 472\n",
      "    loss           : -829383.354579208\n",
      "    val_loss       : -831561.7233642578\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -969947.250000\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -788544.687500\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -840568.875000\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -857811.125000\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -823191.125000\n",
      "    epoch          : 473\n",
      "    loss           : -819457.6219059406\n",
      "    val_loss       : -830734.2922790528\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -953901.687500\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -816072.687500\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -655503.062500\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -849608.375000\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -832779.312500\n",
      "    epoch          : 474\n",
      "    loss           : -812961.3564356435\n",
      "    val_loss       : -823195.4665344239\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -990661.625000\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -768804.125000\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -763034.875000\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -849020.875000\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -865738.625000\n",
      "    epoch          : 475\n",
      "    loss           : -822657.9047029703\n",
      "    val_loss       : -816381.1953735352\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -967179.062500\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -818934.000000\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -789306.125000\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -572969.562500\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -819579.062500\n",
      "    epoch          : 476\n",
      "    loss           : -808039.6571782178\n",
      "    val_loss       : -811532.3564941406\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -820711.000000\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -808140.375000\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -759196.500000\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -937480.250000\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -801688.625000\n",
      "    epoch          : 477\n",
      "    loss           : -806618.6175742574\n",
      "    val_loss       : -783358.9950683594\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -946012.062500\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -825760.625000\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -847993.875000\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -800870.312500\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -861723.812500\n",
      "    epoch          : 478\n",
      "    loss           : -825717.5847772277\n",
      "    val_loss       : -836059.5934814453\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -933268.250000\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -825690.062500\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -871802.125000\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -691334.375000\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -857478.500000\n",
      "    epoch          : 479\n",
      "    loss           : -827233.135519802\n",
      "    val_loss       : -806385.7597534179\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -966531.500000\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -797841.500000\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -621855.812500\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -839982.312500\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -829571.375000\n",
      "    epoch          : 480\n",
      "    loss           : -806850.2376237623\n",
      "    val_loss       : -818098.9803283692\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -656417.500000\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -844435.375000\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -811559.750000\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -955270.937500\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -795009.812500\n",
      "    epoch          : 481\n",
      "    loss           : -823539.0266089109\n",
      "    val_loss       : -838814.8317626953\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -928239.000000\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -859792.250000\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -817009.250000\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -983057.437500\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -816437.062500\n",
      "    epoch          : 482\n",
      "    loss           : -809467.2973391089\n",
      "    val_loss       : -783776.2614135742\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -875718.687500\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -780437.625000\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -839697.875000\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -777882.375000\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -690409.312500\n",
      "    epoch          : 483\n",
      "    loss           : -809590.271039604\n",
      "    val_loss       : -800685.0385498047\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -968024.750000\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -768937.812500\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -871805.750000\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -861454.187500\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -776485.375000\n",
      "    epoch          : 484\n",
      "    loss           : -806874.9814356435\n",
      "    val_loss       : -826921.6439819336\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -972960.562500\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -787785.375000\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -805565.562500\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -982806.687500\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -800048.500000\n",
      "    epoch          : 485\n",
      "    loss           : -830501.8564356435\n",
      "    val_loss       : -803681.9421142578\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -980852.875000\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -797168.000000\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -839089.250000\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -855640.750000\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -848253.750000\n",
      "    epoch          : 486\n",
      "    loss           : -824967.1813118812\n",
      "    val_loss       : -829361.0849121094\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -948407.437500\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -875486.312500\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -866555.625000\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -983150.062500\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -756199.000000\n",
      "    epoch          : 487\n",
      "    loss           : -825315.9152227723\n",
      "    val_loss       : -817930.7653503418\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -984397.062500\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -702316.000000\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -778922.687500\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -775422.500000\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -780262.875000\n",
      "    epoch          : 488\n",
      "    loss           : -817933.4752475248\n",
      "    val_loss       : -808162.937890625\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -982330.250000\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -733323.250000\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -803158.687500\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -744887.500000\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -842588.375000\n",
      "    epoch          : 489\n",
      "    loss           : -814052.3174504951\n",
      "    val_loss       : -807522.7823181152\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -967333.437500\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -877387.250000\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -836425.000000\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -985709.500000\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -694367.625000\n",
      "    epoch          : 490\n",
      "    loss           : -823066.8304455446\n",
      "    val_loss       : -823539.0430786132\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -985226.562500\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -758073.500000\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -848323.750000\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -721530.125000\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -777913.687500\n",
      "    epoch          : 491\n",
      "    loss           : -812604.1410891089\n",
      "    val_loss       : -799411.4092346191\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -961084.062500\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -826634.375000\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -773983.187500\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -789468.875000\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -709646.437500\n",
      "    epoch          : 492\n",
      "    loss           : -823637.6596534654\n",
      "    val_loss       : -809927.9832092285\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -981917.750000\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -789935.437500\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -862987.562500\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -793588.437500\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -851266.687500\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   493: reducing learning rate of group 0 to 1.0000e-04.\n",
      "    epoch          : 493\n",
      "    loss           : -812225.1540841584\n",
      "    val_loss       : -819562.7457885742\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -978942.062500\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -787015.375000\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -807731.562500\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -772101.875000\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -730077.750000\n",
      "    epoch          : 494\n",
      "    loss           : -817424.051980198\n",
      "    val_loss       : -825244.8571533203\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -955021.250000\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -759984.500000\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -746167.625000\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -884107.500000\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -867015.312500\n",
      "    epoch          : 495\n",
      "    loss           : -816405.4774133663\n",
      "    val_loss       : -812644.8998596191\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -978320.875000\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -839029.562500\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -851428.187500\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -965959.375000\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -708974.187500\n",
      "    epoch          : 496\n",
      "    loss           : -815812.2490717822\n",
      "    val_loss       : -840310.1918701172\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -973820.625000\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -878686.250000\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -779777.625000\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -861452.250000\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -806060.250000\n",
      "    epoch          : 497\n",
      "    loss           : -821567.7147277228\n",
      "    val_loss       : -821247.5134155273\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -810248.187500\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -766344.875000\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -841191.875000\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -778575.312500\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -844498.500000\n",
      "    epoch          : 498\n",
      "    loss           : -818575.9511138614\n",
      "    val_loss       : -818921.5242736817\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -1001534.625000\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -787210.937500\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -829956.062500\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -720768.500000\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -734435.562500\n",
      "    epoch          : 499\n",
      "    loss           : -822195.8174504951\n",
      "    val_loss       : -829458.0277648926\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -978324.562500\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -764436.000000\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -869485.125000\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -833399.500000\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -836822.125000\n",
      "    epoch          : 500\n",
      "    loss           : -826741.1132425743\n",
      "    val_loss       : -835952.7805725097\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0923_134115/checkpoint-epoch500.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VaeCategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=12, bias=True)\n",
       "        (1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (4): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=12, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_0_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=12, bias=True)\n",
       "        (1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (4): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=12, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=20, bias=True)\n",
       "        (1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=20, out_features=20, bias=True)\n",
       "        (4): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=20, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_1_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=20, bias=True)\n",
       "        (1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=20, out_features=20, bias=True)\n",
       "        (4): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=20, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=36, bias=True)\n",
       "        (1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (4): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=36, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_2_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=36, bias=True)\n",
       "        (1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (4): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=36, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=68, bias=True)\n",
       "        (1): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=68, out_features=68, bias=True)\n",
       "        (4): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=68, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_3_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=68, bias=True)\n",
       "        (1): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=68, out_features=68, bias=True)\n",
       "        (4): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=68, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=102, bias=True)\n",
       "        (1): LayerNorm((102,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=102, out_features=102, bias=True)\n",
       "        (4): LayerNorm((102,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=102, out_features=392, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_4_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=102, bias=True)\n",
       "        (1): LayerNorm((102,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=102, out_features=102, bias=True)\n",
       "        (4): LayerNorm((102,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=102, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_5_dagger): DensityEncoder(\n",
       "      (conv_layers): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=2744, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_6_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_7_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_8_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=106, bias=True)\n",
       "        (1): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=106, out_features=106, bias=True)\n",
       "        (4): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=106, out_features=392, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_9_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=106, bias=True)\n",
       "        (1): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=106, out_features=106, bias=True)\n",
       "        (4): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=106, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_10): DensityDecoder(\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_10_dagger): DensityEncoder(\n",
       "      (conv_layers): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=2744, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_11): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_11_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_12): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_12_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_13): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=114, bias=True)\n",
       "        (1): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=114, out_features=114, bias=True)\n",
       "        (4): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=114, out_features=392, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_13_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=114, bias=True)\n",
       "        (1): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=114, out_features=114, bias=True)\n",
       "        (4): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=114, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_14): DensityDecoder(\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_14_dagger): DensityEncoder(\n",
       "      (conv_layers): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=2744, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_15): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_15_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_16): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=130, bias=True)\n",
       "        (1): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=130, out_features=130, bias=True)\n",
       "        (4): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=130, out_features=392, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_16_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=130, bias=True)\n",
       "        (1): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=130, out_features=130, bias=True)\n",
       "        (4): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=130, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_17): DensityDecoder(\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_17_dagger): DensityEncoder(\n",
       "      (conv_layers): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=2744, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_18): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=162, bias=True)\n",
       "        (1): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=162, out_features=162, bias=True)\n",
       "        (4): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=162, out_features=392, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_18_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=162, bias=True)\n",
       "        (1): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=162, out_features=162, bias=True)\n",
       "        (4): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=162, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_19): DensityDecoder(\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_19_dagger): DensityEncoder(\n",
       "      (conv_layers): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=2744, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_20): DensityDecoder(\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_20_dagger): DensityEncoder(\n",
       "      (conv_layers): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=2744, out_features=392, bias=True)\n",
       "        (1): LayerNorm((392,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=392, out_features=392, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "    (global_element_5): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=54, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAATXElEQVR4nO3de5ScdX3H8fcnm83FJEBCSMgNokBAQAwSkZsSSqGIF/APL9gqtLbxD6/ncLCWnh7wnPZIW0UtrZ7GQhOEgrZKw7GxEoOIiKRZIOZiECIGEhJzJbCJ5LLZb/+YJ+2w7PObzczszmZ/n9c5e3ZmvvPM853JfvI889x+igjMbOgb1uoGzGxgOOxmmXDYzTLhsJtlwmE3y4TDbpYJh32IkXSzpLvqnPZUSU9K6pT06Wb31myS/lDSA63u40jhsDeJpIskPSrpJUk7Jf1M0ltb3ddh+hzwUESMi4h/aHUztUTE3RFxeav7OFI47E0g6Sjg+8BtwARgGvAFYF8r+6rDicCasqKktgHsJUnS8AamlaTs/vaze8P9ZBZARNwTEQcj4pWIeCAiVgJIOknSg5J2SNou6W5JxxyaWNJ6STdIWilpj6TbJU2W9INilfpHksYXz50pKSTNk7RJ0mZJ15c1Jum8Yo1jl6RfSJpb8rwHgUuAf5S0W9IsSQskfUPSYkl7gEskvVHSQ8XrrZH03qrXWCDp60Xfu4u1m+MlfVXSi5KeknR2oteQ9GlJzxaf098fCqWk64rX+4qkncDNxWOPVE1/gaTlxdrVckkXVNUekvQ3kn4G/A54Q+Lfc2iKCP80+AMcBewAFgLvBMb3qJ8MXAaMBI4DHga+WlVfDzwGTKayVrAVeAI4u5jmQeCm4rkzgQDuAcYAbwK2Ab9f1G8G7ipuTyv6upLKf+yXFfePK3kfDwF/WnV/AfAScGEx/ThgHXAjMAL4PaATOLXq+duBc4BRRd+/AT4KtAF/Dfw48TkG8GMqa0cnAE8f6ge4DugCPgUMB0YXjz1S1CcALwIfKerXFPePrXpvzwNnFPX2Vv/dDPSPl+xNEBEvAxdR+WP9JrBN0v2SJhf1dRGxJCL2RcQ24Fbg4h4vc1tEbImIF4CfAssi4smI2AfcRyX41b4QEXsiYhXwr1T+uHv6I2BxRCyOiO6IWAJ0UAl/Xy2KiJ9FRDcwGxgL3BIR+yPiQSpfX6rnfV9EPB4Re4u+90bEnRFxEPh2L++jp7+NiJ0R8Tzw1R6vvSkibouIroh4pcd07wKeiYhvFfV7gKeA91Q9Z0FErCnqBw7jMxgSHPYmiYi1EXFdREwHzgSmUvljRdIkSfdKekHSy8BdwMQeL7Gl6vYrvdwf2+P5G6puP1fMr6cTgfcXq9y7JO2i8p/SlMN4a9XzmQpsKIJfPe9pVfcP932k5tfzfW2g3NTi+dV69paafshz2PtBRDxFZZX2zOKhL1JZ6p8VEUdRWeKqwdnMqLp9ArCpl+dsAL4VEcdU/YyJiFsOYz7Vp0VuAmb02Lh1AvDCYbxeLan3lTpFcxOV/9yq9ewt61M8HfYmkHSapOslTS/uz6Cy+vlY8ZRxwG5gl6RpwA1NmO1fSXqdpDOAP6ayitzTXcB7JP2BpDZJoyTNPdRnHZYBe4DPSWovNva9B7i3ztfrzQ2Sxhef4Wfo/X31ZjEwS9KHJQ2X9EHgdCpfMwyHvVk6gbcBy4qt1o8Bq4FDW8m/ALyFysau/wK+14R5/oTKxrKlwJci4jUHl0TEBuAqKhvUtlFZ0t9Anf/uEbEfeC+VjZDbga8DHy3WZJplEfA4sILKZ3V7H3vbAbybyme+g8oxA++OiO1N7O2IpmJLpR0hJM2ksoW7PSK6WtxOU0kK4JSIWNfqXoYiL9nNMuGwm2XCq/FmmfCS3SwTdZ9MUI8RGhmjGDOQszTLyl72sD/29XoMR0Nhl3QF8DUqxz3/S62DNUYxhrfp0kZmaWYJy2Jpaa3u1fjidMd/orLP9XTgGkmn1/t6Zta/GvnOfi6wLiKeLQ62uJfKARxmNgg1EvZpvPrEgo28+qQDAIrzrjskdRw44q7lYDZ0NBL23jYCvGY/XkTMj4g5ETGnnZENzM7MGtFI2Dfy6jOUptP7mVdmNgg0EvblwCmSXi9pBPAh4P7mtGVmzVb3rreI6JL0SeCHVHa93RERpRcrNLPWamg/e0QspnIesZkNcj5c1iwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMjGgQzbbwBs2blyy3t3ZOUCdWKt5yW6WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcL72Y8Aw2afnqzHrS+V1p5eMaOheY/anl4eHLOuO1k/eunTpbWDO3bW1ZPVp6GwS1oPdAIHga6ImNOMpsys+ZqxZL8kIrY34XXMrB/5O7tZJhoNewAPSHpc0rzeniBpnqQOSR0H2Nfg7MysXo2uxl8YEZskTQKWSHoqIh6ufkJEzAfmAxylCdHg/MysTg0t2SNiU/F7K3AfcG4zmjKz5qs77JLGSBp36DZwObC6WY2ZWXM1sho/GbhP0qHX+beI+O+mdDXEDBs1Kln/7Z+8JVnfe0n6nPPvn/Tt0tqqGccnp/2Pbem9pRNG7EnWJ41I93bHhXNLa8euUHLacRv2J+vtP3o8WbdXqzvsEfEs8OYm9mJm/ci73swy4bCbZcJhN8uEw26WCYfdLBM+xbUJui+anayvv2J0sr7ww7cl6xPa9ibrJwwvf/01+9OnoL534pPJ+nP7JybrJ45InwN10bm/LK1tP2tsctq1v5qerM/6UbJsPXjJbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwvvZm2DrOa9L1mee/3yyft6othpzGHOYHf2/5/Yfl6xv70rv6z5j9MZkfdrwF5P1ySPLT4Ed3XYgOe2Es36XrD/5Fxck69O/+Giynhsv2c0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTHg/e18NK98XfjB9pWikxgbCuWnbGcn6A5tOK61tW5Pezz7m5PLhngG++9jFyfr+N6cvNd32VPkxAm+9Ij3MwPOd45P1fcemz9XnvLPKa4+tTE87BHnJbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwvvZ+6r7YGmpxmXd6dw/Mlmfu/rqZH3HkqnJ+oz//G1p7ahnHktOW2s46e69a5P1mhL7ulfuPDM56dh3lb8vgJE70suqUPmQ0OnBooemmkt2SXdI2ippddVjEyQtkfRM8Tt99IOZtVxfVuMXAFf0eOzzwNKIOAVYWtw3s0GsZtgj4mFgZ4+HrwIWFrcXAlc3ty0za7Z6N9BNjojNAMXvSWVPlDRPUoekjgPsq3N2Ztaoft8aHxHzI2JORMxpJ72hysz6T71h3yJpCkDxe2vzWjKz/lBv2O8Hri1uXwssak47ZtZfau5nl3QPMBeYKGkjcBNwC/AdSR8Dngfe359NDnZTHi2/NjrA7hfS55SPfjp9TvnUlenrn5cfAVBb994aBwk0qO1XG0pr7W8sPw8fYMuKycn6iBo7y4ft7SqtNXaFgSNTzbBHxDUlpUub3IuZ9SMfLmuWCYfdLBMOu1kmHHazTDjsZpnwKa5NEMtXJetjlqenr3FB5CPaS5edWl6blZ62e3p6t+C+zTVOzx1V/uftU1zNbMhy2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmvJ/d+lXXyPI92l3j0kcYzJqavibKM9tnJOttneWXQRvKxzaU8ZLdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uE97NbQ4afmN7Xvf3y8nPST5u+JTntjDEvJuu/2XNCsq7NHrukmpfsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmvJ/dGrLpXen97KPHlA9Hff0JP0xOu2zPyenX3pK++vvBHTuT9dzUXLJLukPSVkmrqx67WdILklYUP1f2b5tm1qi+rMYvAK7o5fGvRMTs4mdxc9sys2arGfaIeBjw+pDZEa6RDXSflLSyWM0fX/YkSfMkdUjqOED5NcHMrH/VG/ZvACcBs4HNwJfLnhgR8yNiTkTMaWdknbMzs0bVFfaI2BIRByOiG/gmcG5z2zKzZqsr7JKmVN19H7C67LlmNjjU3M8u6R5gLjBR0kbgJmCupNlAAOuBj/dfi9ZK2+edn6zvOmd/sv7PZ32ntHbMsFeS027ad0yyHjkOst6AmmGPiGt6efj2fujFzPqRD5c1y4TDbpYJh90sEw67WSYcdrNM+BTXzGl4+k+ga3R6/9bkKbuS9TaVD47cRiSnXbPr+GT96PVdybq9mpfsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmvJ99iNPI9NWBOq86O1nvekf5paABrpv582T90tEHS2t3d05NTrt+/aRkfdai/0nW7dW8ZDfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuH97ENc23ETk/Wt56TPV3/TcVuT9be/bl369Q+Wn8/+gx1zk9NO/klbsm6Hx0t2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTfRmyeQZwJ3A80A3Mj4ivSZoAfBuYSWXY5g9ExIv916rVY+fbZyTrba/fnax/etrSZH3CsPLz1QEmtY0trbXXmPaoZ9NDOtvh6cuSvQu4PiLeCJwHfELS6cDngaURcQqwtLhvZoNUzbBHxOaIeKK43QmsBaYBVwELi6ctBK7upx7NrAkO6zu7pJnA2cAyYHJEbIbKfwhA+hpCZtZSfQ67pLHAd4HPRsTLhzHdPEkdkjoOsK+eHs2sCfoUdkntVIJ+d0R8r3h4i6QpRX0K0OsZExExPyLmRMScdtIXPzSz/lMz7JIE3A6sjYhbq0r3A9cWt68FFjW/PTNrlr6c4noh8BFglaQVxWM3ArcA35H0MeB54P390qHVNGzcuNLaSyel/z//4KlPJOvnj0p/9Rqp8l1rAFsP7imt/fTXJyenPenRJ5N1Ozw1wx4RjwBlJz1f2tx2zKy/+Ag6s0w47GaZcNjNMuGwm2XCYTfLhMNulglfSnoI2P/WWaW1oy5IXwr6o8csS9Zr7UevZW9Eaa37gJc1A8mftlkmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCe9nHwI6TxhRWtu25ejktCPPbGzeS19JD6u86MVLSmujnvGViwaSl+xmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSa8n30IGL/g56W19j1vS0578fBPJetvmLo9WX/lQHuyvnlb+X7+0xasT07blaza4fKS3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLRM397JJmAHcCxwPdwPyI+Jqkm4E/A7YVT70xIhb3V6NWn7H/nr4u/KlL0ue7/+788mvSA3QfnT6f/ZRfl4/P3vXCpuS01lx9OaimC7g+Ip6QNA54XNKSovaViPhS/7VnZs1SM+wRsRnYXNzulLQWmNbfjZlZcx3Wd3ZJM4GzgUPrhp+UtFLSHZLGl0wzT1KHpI4D7GusWzOrW5/DLmks8F3gsxHxMvAN4CRgNpUl/5d7my4i5kfEnIiY046vOWbWKn0Ku6R2KkG/OyK+BxARWyLiYER0A98Ezu2/Ns2sUTXDLknA7cDaiLi16vEpVU97H7C6+e2ZWbP0ZWv8hcBHgFWSVhSP3QhcI2k2EMB64OP90J/1s4O7XkrWR/5gebpe4/XLB2y2gdaXrfGPAOql5H3qZkcQH0FnlgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMqGIgTvjWNI24LmqhyYC6TGBW2ew9jZY+wL3Vq9m9nZiRBzXW2FAw/6amUsdETGnZQ0kDNbeBmtf4N7qNVC9eTXeLBMOu1kmWh32+S2ef8pg7W2w9gXurV4D0ltLv7Ob2cBp9ZLdzAaIw26WiZaEXdIVkn4laZ2kz7eihzKS1ktaJWmFpI4W93KHpK2SVlc9NkHSEknPFL97HWOvRb3dLOmF4rNbIenKFvU2Q9KPJa2VtEbSZ4rHW/rZJfoakM9twL+zS2oDngYuAzYCy4FrIuKXA9pICUnrgTkR0fIDMCS9A9gN3BkRZxaP/R2wMyJuKf6jHB8Rfz5IersZ2N3qYbyL0YqmVA8zDlwNXEcLP7tEXx9gAD63VizZzwXWRcSzEbEfuBe4qgV9DHoR8TCws8fDVwELi9sLqfyxDLiS3gaFiNgcEU8UtzuBQ8OMt/SzS/Q1IFoR9mnAhqr7Gxlc470H8ICkxyXNa3UzvZgcEZuh8scDTGpxPz3VHMZ7IPUYZnzQfHb1DH/eqFaEvbehpAbT/r8LI+ItwDuBTxSrq9Y3fRrGe6D0Msz4oFDv8OeNakXYNwIzqu5PBza1oI9eRcSm4vdW4D4G31DUWw6NoFv83trifv7PYBrGu7dhxhkEn10rhz9vRdiXA6dIer2kEcCHgPtb0MdrSBpTbDhB0hjgcgbfUNT3A9cWt68FFrWwl1cZLMN4lw0zTos/u5YPfx4RA/4DXElli/yvgb9sRQ8lfb0B+EXxs6bVvQH3UFmtO0BljehjwLHAUuCZ4veEQdTbt4BVwEoqwZrSot4uovLVcCWwovi5stWfXaKvAfncfLisWSZ8BJ1ZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulon/Bejt687yXu++AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    path.draw()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
