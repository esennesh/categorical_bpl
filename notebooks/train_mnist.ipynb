{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eli/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"cooldown\": 25,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 1272.399048\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -336.015289\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -57.784088\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -701.855896\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -495.986786\n",
      "    epoch          : 1\n",
      "    loss           : -166.50546600795028\n",
      "    val_loss       : -444.5042000441812\n",
      "    val_log_likelihood: 483.4782241821289\n",
      "    val_log_marginal: 468.0377277702093\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -794.184937\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -439.754730\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -451.524841\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -534.251709\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -382.482178\n",
      "    epoch          : 2\n",
      "    loss           : -460.0559721465158\n",
      "    val_loss       : -496.2767107015476\n",
      "    val_log_likelihood: 590.1225719451904\n",
      "    val_log_marginal: 570.9039652064442\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -1011.146973\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -541.447510\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -468.505341\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -918.838867\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -499.648865\n",
      "    epoch          : 3\n",
      "    loss           : -493.90989821028\n",
      "    val_loss       : -507.4747114391066\n",
      "    val_log_likelihood: 565.8342872619629\n",
      "    val_log_marginal: 540.592910097912\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -851.064819\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -487.733032\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -600.106323\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -120.328575\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -644.293579\n",
      "    epoch          : 4\n",
      "    loss           : -546.7329639397045\n",
      "    val_loss       : -583.4365570273251\n",
      "    val_log_likelihood: 596.9134902954102\n",
      "    val_log_marginal: 583.8969862479717\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -1009.358459\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -691.263184\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -746.551636\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -191.087952\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -737.772583\n",
      "    epoch          : 5\n",
      "    loss           : -629.5414767123685\n",
      "    val_loss       : -673.4242414327339\n",
      "    val_log_likelihood: 682.1729843139649\n",
      "    val_log_marginal: 676.8442002164281\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -1110.384644\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -746.152466\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -494.648438\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -609.763550\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -561.567322\n",
      "    epoch          : 6\n",
      "    loss           : -676.2226499047609\n",
      "    val_loss       : -664.8482318472118\n",
      "    val_log_likelihood: 671.1168441772461\n",
      "    val_log_marginal: 666.3105369974985\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -1081.029297\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -475.275635\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -238.684052\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -609.179749\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -750.485107\n",
      "    epoch          : 7\n",
      "    loss           : -660.513612501692\n",
      "    val_loss       : -667.3159363230691\n",
      "    val_log_likelihood: 673.319465637207\n",
      "    val_log_marginal: 667.922502079606\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -1103.442383\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -234.684082\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -486.233093\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -1110.173584\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -646.235291\n",
      "    epoch          : 8\n",
      "    loss           : -663.8680125321492\n",
      "    val_loss       : -658.0032375249081\n",
      "    val_log_likelihood: 662.605256652832\n",
      "    val_log_marginal: 658.3466121642256\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -1114.458618\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -630.189941\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -464.499084\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -627.491821\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -663.971436\n",
      "    epoch          : 9\n",
      "    loss           : -661.5122109592551\n",
      "    val_loss       : -715.6204966476187\n",
      "    val_log_likelihood: 721.4983123779297\n",
      "    val_log_marginal: 716.1334959045726\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -1120.447632\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -761.746582\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -804.674683\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -567.133850\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -672.654236\n",
      "    epoch          : 10\n",
      "    loss           : -708.0839354260133\n",
      "    val_loss       : -691.6758793596179\n",
      "    val_log_likelihood: 706.3597183227539\n",
      "    val_log_marginal: 700.8993763839543\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -1218.667236\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -723.404419\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -663.392639\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -707.677246\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -692.634399\n",
      "    epoch          : 11\n",
      "    loss           : -694.6345387071666\n",
      "    val_loss       : -704.9650490572676\n",
      "    val_log_likelihood: 711.1135940551758\n",
      "    val_log_marginal: 705.3185646605936\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -1255.185547\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -706.655334\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -711.652771\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -190.247253\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -711.929199\n",
      "    epoch          : 12\n",
      "    loss           : -722.2759591282004\n",
      "    val_loss       : -744.0011043630541\n",
      "    val_log_likelihood: 751.0701248168946\n",
      "    val_log_marginal: 744.560199155286\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -1245.086914\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -720.026978\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -638.254395\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -1225.952637\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -747.811401\n",
      "    epoch          : 13\n",
      "    loss           : -742.7061906569074\n",
      "    val_loss       : -738.8501740835607\n",
      "    val_log_likelihood: 746.2626281738281\n",
      "    val_log_marginal: 739.2314369555563\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -1217.117676\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -717.563660\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -274.935944\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -768.962402\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -940.266479\n",
      "    epoch          : 14\n",
      "    loss           : -758.4061803534479\n",
      "    val_loss       : -715.7549504523165\n",
      "    val_log_likelihood: 737.9650909423829\n",
      "    val_log_marginal: 718.4523130413145\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -1189.306396\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -769.581665\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -1058.456543\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -924.285400\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -992.468994\n",
      "    epoch          : 15\n",
      "    loss           : -895.1234985955871\n",
      "    val_loss       : -963.6896009660326\n",
      "    val_log_likelihood: 978.7311584472657\n",
      "    val_log_marginal: 964.5014991257709\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -1362.360840\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -1169.841064\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -951.013184\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -858.305664\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -1238.718750\n",
      "    epoch          : 16\n",
      "    loss           : -1136.4658420676053\n",
      "    val_loss       : -1243.3963287382387\n",
      "    val_log_likelihood: 1257.3762451171874\n",
      "    val_log_marginal: 1243.9372368048876\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -1645.609863\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -1324.707520\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -1387.914917\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -1119.996704\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -1277.472656\n",
      "    epoch          : 17\n",
      "    loss           : -1279.0306892017327\n",
      "    val_loss       : -1298.0714167535305\n",
      "    val_log_likelihood: 1309.9615295410156\n",
      "    val_log_marginal: 1298.7373831342907\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -1722.739624\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -1373.842896\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -1005.378601\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -1724.886230\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -1294.778076\n",
      "    epoch          : 18\n",
      "    loss           : -1289.062513899095\n",
      "    val_loss       : -1311.5168963519857\n",
      "    val_log_likelihood: 1322.9478271484375\n",
      "    val_log_marginal: 1311.9882330406458\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -1735.582031\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -1216.891113\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -1359.321655\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -1353.552490\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -1309.870117\n",
      "    epoch          : 19\n",
      "    loss           : -1317.9988173683091\n",
      "    val_loss       : -1345.9579048627988\n",
      "    val_log_likelihood: 1361.0106689453125\n",
      "    val_log_marginal: 1349.4564115434466\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -1419.806274\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -1270.971680\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -1145.596802\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -1302.298950\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -1376.247192\n",
      "    epoch          : 20\n",
      "    loss           : -1365.3121253287438\n",
      "    val_loss       : -1361.3479973543435\n",
      "    val_log_likelihood: 1374.28984375\n",
      "    val_log_marginal: 1362.1613037455827\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -1734.626709\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -1438.554688\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -1198.598877\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -1707.416382\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -1410.134644\n",
      "    epoch          : 21\n",
      "    loss           : -1376.6226842899134\n",
      "    val_loss       : -1363.9439091414213\n",
      "    val_log_likelihood: 1378.9880859375\n",
      "    val_log_marginal: 1365.2150468524546\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -1757.039307\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -1317.798828\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -1323.901001\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -1745.399048\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -1414.128418\n",
      "    epoch          : 22\n",
      "    loss           : -1390.1671988609994\n",
      "    val_loss       : -1415.622475546971\n",
      "    val_log_likelihood: 1430.2847900390625\n",
      "    val_log_marginal: 1416.5618172992022\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -1762.410034\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -1492.309814\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -1401.269043\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -1727.259766\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -1497.518799\n",
      "    epoch          : 23\n",
      "    loss           : -1406.009354695235\n",
      "    val_loss       : -1420.901932124328\n",
      "    val_log_likelihood: 1436.7480102539062\n",
      "    val_log_marginal: 1421.6416107792415\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -1764.297119\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -1432.047607\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -1277.240967\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -1261.678223\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -1434.311157\n",
      "    epoch          : 24\n",
      "    loss           : -1437.8308190071937\n",
      "    val_loss       : -1448.9716835130007\n",
      "    val_log_likelihood: 1465.434521484375\n",
      "    val_log_marginal: 1450.216453557834\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -1810.186890\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -1294.605713\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -1544.151489\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -1460.210327\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -1457.704102\n",
      "    epoch          : 25\n",
      "    loss           : -1458.3487198329208\n",
      "    val_loss       : -1468.5720435626804\n",
      "    val_log_likelihood: 1485.2119750976562\n",
      "    val_log_marginal: 1469.1997558068483\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -1821.078857\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -1501.636597\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -1519.827026\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -1433.072876\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -1419.384033\n",
      "    epoch          : 26\n",
      "    loss           : -1460.0017947961787\n",
      "    val_loss       : -1468.707577223517\n",
      "    val_log_likelihood: 1485.7052490234375\n",
      "    val_log_marginal: 1469.1998956006028\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -1832.185913\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -1322.898926\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -1323.205444\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -1813.415649\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -1289.709229\n",
      "    epoch          : 27\n",
      "    loss           : -1474.7149356048885\n",
      "    val_loss       : -1464.9645118592307\n",
      "    val_log_likelihood: 1482.4796997070312\n",
      "    val_log_marginal: 1465.8539530348085\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -1545.649048\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -1408.102051\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -1437.318481\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -1536.605713\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -1438.191040\n",
      "    epoch          : 28\n",
      "    loss           : -1465.897080223159\n",
      "    val_loss       : -1473.1459564095362\n",
      "    val_log_likelihood: 1490.204150390625\n",
      "    val_log_marginal: 1473.7893030375199\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -1400.379639\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -1456.106567\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -1351.567261\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -1797.345215\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -1434.359619\n",
      "    epoch          : 29\n",
      "    loss           : -1463.991647248221\n",
      "    val_loss       : -1468.678556457162\n",
      "    val_log_likelihood: 1486.0872314453125\n",
      "    val_log_marginal: 1469.3644330929965\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -1841.010498\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -1529.025879\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -1292.422974\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -1560.876709\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -1441.006348\n",
      "    epoch          : 30\n",
      "    loss           : -1475.5335427463644\n",
      "    val_loss       : -1484.4183612802067\n",
      "    val_log_likelihood: 1501.9620849609375\n",
      "    val_log_marginal: 1485.2672111088773\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -1841.067261\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -1551.952393\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -1329.874023\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -1373.358643\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -1461.084717\n",
      "    epoch          : 31\n",
      "    loss           : -1460.2662861134747\n",
      "    val_loss       : -1459.3886035727337\n",
      "    val_log_likelihood: 1477.8633056640624\n",
      "    val_log_marginal: 1459.9330437805504\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -1833.123291\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -1352.360352\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -1247.459595\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -1565.193359\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -1473.676270\n",
      "    epoch          : 32\n",
      "    loss           : -1478.8552391127785\n",
      "    val_loss       : -1484.6274080025032\n",
      "    val_log_likelihood: 1502.8489501953125\n",
      "    val_log_marginal: 1485.4721708582997\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -1835.639404\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -1398.230713\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -1432.507324\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -1435.890625\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -1483.697021\n",
      "    epoch          : 33\n",
      "    loss           : -1487.8362263594524\n",
      "    val_loss       : -1486.686925739795\n",
      "    val_log_likelihood: 1505.5069213867187\n",
      "    val_log_marginal: 1487.8684268519282\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -1829.738159\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -1429.026489\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -1359.038818\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -1844.821289\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -1454.554688\n",
      "    epoch          : 34\n",
      "    loss           : -1499.38465443224\n",
      "    val_loss       : -1501.259370899573\n",
      "    val_log_likelihood: 1519.51181640625\n",
      "    val_log_marginal: 1501.729655915438\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -1847.223389\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -1368.122681\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -1386.587280\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -1580.305176\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -1480.141602\n",
      "    epoch          : 35\n",
      "    loss           : -1493.8182675201115\n",
      "    val_loss       : -1500.1773008849473\n",
      "    val_log_likelihood: 1518.5802124023437\n",
      "    val_log_marginal: 1500.7366623651237\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -1847.000977\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -1412.251953\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -1495.759277\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -1493.626465\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -1446.866089\n",
      "    epoch          : 36\n",
      "    loss           : -1505.9500563215502\n",
      "    val_loss       : -1509.8011204158888\n",
      "    val_log_likelihood: 1527.8618530273438\n",
      "    val_log_marginal: 1510.2440802499718\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -1839.038330\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -1384.551636\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -1334.872314\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -1577.310059\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -1498.446045\n",
      "    epoch          : 37\n",
      "    loss           : -1497.708794622138\n",
      "    val_loss       : -1506.892042880319\n",
      "    val_log_likelihood: 1525.7957397460937\n",
      "    val_log_marginal: 1507.4023197490724\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -1838.321899\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -1596.694458\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -1518.159058\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -1731.834229\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -1513.454102\n",
      "    epoch          : 38\n",
      "    loss           : -1497.206702506188\n",
      "    val_loss       : -1482.5476340690627\n",
      "    val_log_likelihood: 1502.2453002929688\n",
      "    val_log_marginal: 1483.6905967947096\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -1800.995239\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -1440.640503\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -1341.244507\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -1820.781006\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -1492.293701\n",
      "    epoch          : 39\n",
      "    loss           : -1502.896322420328\n",
      "    val_loss       : -1512.001878587529\n",
      "    val_log_likelihood: 1531.0576049804688\n",
      "    val_log_marginal: 1512.4224290389568\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -1842.058594\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -1516.873047\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -1340.704590\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -1514.311035\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -1519.096680\n",
      "    epoch          : 40\n",
      "    loss           : -1513.8827629467048\n",
      "    val_loss       : -1515.732595089078\n",
      "    val_log_likelihood: 1535.2354125976562\n",
      "    val_log_marginal: 1516.795848575607\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -1848.790649\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -1591.158691\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -1360.750122\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -1523.857788\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -1497.054688\n",
      "    epoch          : 41\n",
      "    loss           : -1522.3661758876083\n",
      "    val_loss       : -1519.853974954784\n",
      "    val_log_likelihood: 1538.3727416992188\n",
      "    val_log_marginal: 1520.7403837520746\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -1612.847900\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -1376.363037\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -1370.787109\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -1401.643433\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -1498.498047\n",
      "    epoch          : 42\n",
      "    loss           : -1502.2246214611696\n",
      "    val_loss       : -1497.364208745584\n",
      "    val_log_likelihood: 1516.034228515625\n",
      "    val_log_marginal: 1498.0659129526466\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -1857.047119\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -1439.194824\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -1373.541870\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -1510.985718\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -1355.731567\n",
      "    epoch          : 43\n",
      "    loss           : -1520.3208938447556\n",
      "    val_loss       : -1528.766999546066\n",
      "    val_log_likelihood: 1547.1059692382812\n",
      "    val_log_marginal: 1529.6027970854987\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -1863.062256\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -1467.196533\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -1543.800293\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -1530.884155\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -1352.612549\n",
      "    epoch          : 44\n",
      "    loss           : -1533.0867847404857\n",
      "    val_loss       : -1531.4493504858576\n",
      "    val_log_likelihood: 1549.6641357421875\n",
      "    val_log_marginal: 1532.2549669142888\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -1873.367065\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -1611.793701\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -1474.547119\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -1355.407959\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -1536.709595\n",
      "    epoch          : 45\n",
      "    loss           : -1538.0978000754178\n",
      "    val_loss       : -1537.929414599482\n",
      "    val_log_likelihood: 1556.241357421875\n",
      "    val_log_marginal: 1538.9599115978926\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -1866.306030\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -1625.952637\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -1523.978027\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -1609.613159\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -1543.521729\n",
      "    epoch          : 46\n",
      "    loss           : -1542.4521617322864\n",
      "    val_loss       : -1544.332916367054\n",
      "    val_log_likelihood: 1562.3245361328125\n",
      "    val_log_marginal: 1544.6868984764624\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -1878.110718\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -1628.245972\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -1626.644287\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -1404.363770\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -1524.354980\n",
      "    epoch          : 47\n",
      "    loss           : -1546.7492603264232\n",
      "    val_loss       : -1546.4020154202358\n",
      "    val_log_likelihood: 1565.359765625\n",
      "    val_log_marginal: 1547.5436953260069\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -1871.837280\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -1425.747559\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -1391.627686\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -1471.297485\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -1543.847534\n",
      "    epoch          : 48\n",
      "    loss           : -1545.476969803914\n",
      "    val_loss       : -1551.7991232134402\n",
      "    val_log_likelihood: 1570.785107421875\n",
      "    val_log_marginal: 1552.6673916261643\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -1868.301514\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -1493.482178\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -1468.692627\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -1487.698730\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -1533.166260\n",
      "    epoch          : 49\n",
      "    loss           : -1551.555855023979\n",
      "    val_loss       : -1546.791981240362\n",
      "    val_log_likelihood: 1566.4831909179688\n",
      "    val_log_marginal: 1547.5523132395188\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -1875.522095\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -1631.920776\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -1527.334229\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -1623.163208\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -1532.595947\n",
      "    epoch          : 50\n",
      "    loss           : -1563.1806072575032\n",
      "    val_loss       : -1536.0978288907559\n",
      "    val_log_likelihood: 1583.1522216796875\n",
      "    val_log_marginal: 1564.4443318810313\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -1882.906860\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -1513.792480\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -1426.431641\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -1615.672974\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -1530.717285\n",
      "    epoch          : 51\n",
      "    loss           : -1564.6405645691523\n",
      "    val_loss       : -1563.0811779865994\n",
      "    val_log_likelihood: 1582.5815307617188\n",
      "    val_log_marginal: 1563.5343044925494\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -1881.051880\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -1487.688477\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -1496.713379\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -1472.747192\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -1457.615967\n",
      "    epoch          : 52\n",
      "    loss           : -1524.6837508702042\n",
      "    val_loss       : -1520.747477888409\n",
      "    val_log_likelihood: 1541.7743041992187\n",
      "    val_log_marginal: 1521.461162412039\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -1869.205811\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -1612.951904\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -1495.767334\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -1398.112305\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -1504.176025\n",
      "    epoch          : 53\n",
      "    loss           : -1539.331350798654\n",
      "    val_loss       : -1511.8566854361445\n",
      "    val_log_likelihood: 1558.5685791015626\n",
      "    val_log_marginal: 1538.9296018529683\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -1860.528442\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -1499.547852\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -1615.983032\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -1520.414062\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -1506.956543\n",
      "    epoch          : 54\n",
      "    loss           : -1554.814316551284\n",
      "    val_loss       : -1561.1665024919434\n",
      "    val_log_likelihood: 1581.33125\n",
      "    val_log_marginal: 1561.791464260639\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -1877.816772\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -1501.155151\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -1535.363770\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -1560.242676\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -1425.361328\n",
      "    epoch          : 55\n",
      "    loss           : -1562.9715999187808\n",
      "    val_loss       : -1563.1403511691838\n",
      "    val_log_likelihood: 1583.5841674804688\n",
      "    val_log_marginal: 1564.1958651638695\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -1877.145142\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -1505.804443\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -1433.871216\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -1612.286133\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -1493.524902\n",
      "    epoch          : 56\n",
      "    loss           : -1561.330970084313\n",
      "    val_loss       : -1564.1983327290043\n",
      "    val_log_likelihood: 1584.4638427734376\n",
      "    val_log_marginal: 1564.7184680763633\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -1883.725586\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -1512.064941\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -1443.017090\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -1461.204590\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -1503.537598\n",
      "    epoch          : 57\n",
      "    loss           : -1563.6925411413213\n",
      "    val_loss       : -1573.027962489985\n",
      "    val_log_likelihood: 1593.6556274414063\n",
      "    val_log_marginal: 1574.0442287497626\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -1665.739868\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -1456.053955\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -1441.615601\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -1863.481689\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -1578.744385\n",
      "    epoch          : 58\n",
      "    loss           : -1576.5944691270886\n",
      "    val_loss       : -1573.5385070528835\n",
      "    val_log_likelihood: 1594.1476196289063\n",
      "    val_log_marginal: 1574.1938106021719\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -1883.564331\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -1494.520386\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -1647.760498\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -1551.674438\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -1537.771851\n",
      "    epoch          : 59\n",
      "    loss           : -1578.2952627049815\n",
      "    val_loss       : -1574.0162579825148\n",
      "    val_log_likelihood: 1594.8611450195312\n",
      "    val_log_marginal: 1574.7877718497068\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -1883.318481\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -1622.848022\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -1449.197998\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -1465.085449\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -1497.357910\n",
      "    epoch          : 60\n",
      "    loss           : -1560.67790116414\n",
      "    val_loss       : -1578.4159215822815\n",
      "    val_log_likelihood: 1600.055419921875\n",
      "    val_log_marginal: 1578.6384264361793\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -1892.348877\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -1451.334961\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -1472.520630\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -1545.067383\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -1506.257568\n",
      "    epoch          : 61\n",
      "    loss           : -1586.030839070235\n",
      "    val_loss       : -1583.3069991536438\n",
      "    val_log_likelihood: 1605.930859375\n",
      "    val_log_marginal: 1584.1776821930514\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -1881.910645\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -1479.853516\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -1356.889160\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -1516.460083\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -1499.018677\n",
      "    epoch          : 62\n",
      "    loss           : -1533.9632036567914\n",
      "    val_loss       : -1564.5135832928122\n",
      "    val_log_likelihood: 1587.016259765625\n",
      "    val_log_marginal: 1565.0312934340689\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -1617.776611\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -1503.211792\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -1621.733398\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -1560.707275\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -1559.647827\n",
      "    epoch          : 63\n",
      "    loss           : -1571.7017290474164\n",
      "    val_loss       : -1564.1679017210379\n",
      "    val_log_likelihood: 1587.7957397460937\n",
      "    val_log_marginal: 1565.1900438476355\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -1462.148560\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -1464.499512\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -1498.581299\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -1616.542480\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -1550.337646\n",
      "    epoch          : 64\n",
      "    loss           : -1567.6374197478342\n",
      "    val_loss       : -1580.683938394673\n",
      "    val_log_likelihood: 1604.0823486328125\n",
      "    val_log_marginal: 1581.2621333058923\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -1891.672485\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -1485.861938\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -1617.527710\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -1571.713257\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -1567.843750\n",
      "    epoch          : 65\n",
      "    loss           : -1572.4113080619586\n",
      "    val_loss       : -1585.6784832051023\n",
      "    val_log_likelihood: 1609.3000854492188\n",
      "    val_log_marginal: 1585.9759166018266\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -1901.875488\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -1671.770874\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -1477.144043\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -1580.528198\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -1574.942383\n",
      "    epoch          : 66\n",
      "    loss           : -1588.3206497041306\n",
      "    val_loss       : -1584.7263694426044\n",
      "    val_log_likelihood: 1608.6407104492187\n",
      "    val_log_marginal: 1585.4360439170132\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -1878.296875\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -1534.569824\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -1588.716797\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -1898.017334\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -1595.291626\n",
      "    epoch          : 67\n",
      "    loss           : -1594.0063391959313\n",
      "    val_loss       : -1589.9686764826997\n",
      "    val_log_likelihood: 1614.238720703125\n",
      "    val_log_marginal: 1590.6815210488435\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -1898.047607\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -1526.652344\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -1655.689941\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -1498.383301\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -1520.630127\n",
      "    epoch          : 68\n",
      "    loss           : -1600.0915297706529\n",
      "    val_loss       : -1605.149518190138\n",
      "    val_log_likelihood: 1629.137158203125\n",
      "    val_log_marginal: 1605.8785160437553\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -1900.640503\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -1649.660522\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -1583.787354\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -1520.417236\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -1513.340332\n",
      "    epoch          : 69\n",
      "    loss           : -1593.651776908648\n",
      "    val_loss       : -1595.579341013357\n",
      "    val_log_likelihood: 1619.4696899414062\n",
      "    val_log_marginal: 1596.1206984967043\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -1527.856445\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -1691.872559\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -1660.461426\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -1580.467896\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -1515.963257\n",
      "    epoch          : 70\n",
      "    loss           : -1596.07633866414\n",
      "    val_loss       : -1594.6425841921941\n",
      "    val_log_likelihood: 1618.9372680664062\n",
      "    val_log_marginal: 1595.2510198492557\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -1893.982788\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -1550.711426\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -1648.509521\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -1515.997803\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -1532.664917\n",
      "    epoch          : 71\n",
      "    loss           : -1606.8432689704518\n",
      "    val_loss       : -1600.1739960819482\n",
      "    val_log_likelihood: 1624.5703491210938\n",
      "    val_log_marginal: 1600.7709152039147\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -1886.814087\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -1554.348633\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -1658.353882\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -1495.854858\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -1505.700195\n",
      "    epoch          : 72\n",
      "    loss           : -1599.9838637550279\n",
      "    val_loss       : -1595.339056187123\n",
      "    val_log_likelihood: 1620.3375732421875\n",
      "    val_log_marginal: 1596.058715793951\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -1871.311279\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -1668.638062\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -1496.221069\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -1583.542480\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -1520.615723\n",
      "    epoch          : 73\n",
      "    loss           : -1607.2591854888615\n",
      "    val_loss       : -1607.2702988067642\n",
      "    val_log_likelihood: 1632.3111328125\n",
      "    val_log_marginal: 1608.2245273258536\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -1906.046875\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -1547.297974\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -1512.235107\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -1528.614258\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -1616.094238\n",
      "    epoch          : 74\n",
      "    loss           : -1615.8976035543008\n",
      "    val_loss       : -1623.4571497321128\n",
      "    val_log_likelihood: 1648.36884765625\n",
      "    val_log_marginal: 1624.0727509713688\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -1910.807495\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -1560.247559\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -1605.956543\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -1815.436890\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -1567.610107\n",
      "    epoch          : 75\n",
      "    loss           : -1594.2418490872524\n",
      "    val_loss       : -1592.0101559780537\n",
      "    val_log_likelihood: 1617.3369140625\n",
      "    val_log_marginal: 1592.8694715809077\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -1892.172485\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -1558.760742\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -1566.569702\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -1518.377441\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -1526.144287\n",
      "    epoch          : 76\n",
      "    loss           : -1596.8638722636913\n",
      "    val_loss       : -1611.583309979737\n",
      "    val_log_likelihood: 1636.818603515625\n",
      "    val_log_marginal: 1612.275680969283\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -1898.378662\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -1542.760986\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -1510.603760\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -1580.849854\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -1503.332397\n",
      "    epoch          : 77\n",
      "    loss           : -1609.028211536974\n",
      "    val_loss       : -1608.7488622995093\n",
      "    val_log_likelihood: 1634.0759155273438\n",
      "    val_log_marginal: 1609.7014892507345\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -1563.077271\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -1643.975342\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -1510.812744\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -1522.455444\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -1605.372070\n",
      "    epoch          : 78\n",
      "    loss           : -1612.009659266708\n",
      "    val_loss       : -1604.1864399341866\n",
      "    val_log_likelihood: 1629.9099731445312\n",
      "    val_log_marginal: 1605.145676054433\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -1890.094604\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -1565.375488\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -1543.953003\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -1676.021851\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -1530.171997\n",
      "    epoch          : 79\n",
      "    loss           : -1617.787840588258\n",
      "    val_loss       : -1619.2718534281478\n",
      "    val_log_likelihood: 1645.164501953125\n",
      "    val_log_marginal: 1620.3449024710803\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -1685.666870\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -1558.504150\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -1615.451416\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -1670.816650\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -1520.326050\n",
      "    epoch          : 80\n",
      "    loss           : -1613.2934969156095\n",
      "    val_loss       : -1614.3278441203759\n",
      "    val_log_likelihood: 1640.5734497070312\n",
      "    val_log_marginal: 1615.2580216344445\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -1911.152100\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -1603.990845\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -1543.243286\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -1672.444946\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -1540.305664\n",
      "    epoch          : 81\n",
      "    loss           : -1615.6534653465346\n",
      "    val_loss       : -1615.7705884451047\n",
      "    val_log_likelihood: 1641.9260986328125\n",
      "    val_log_marginal: 1616.4742540139705\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -1886.735474\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -1572.944336\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -1633.877197\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -1512.952637\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -1610.367065\n",
      "    epoch          : 82\n",
      "    loss           : -1629.7855454246596\n",
      "    val_loss       : -1632.5493289796636\n",
      "    val_log_likelihood: 1658.3528076171874\n",
      "    val_log_marginal: 1632.924696803468\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -1906.555664\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -1524.484375\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -1615.849976\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -1871.316650\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -1542.337646\n",
      "    epoch          : 83\n",
      "    loss           : -1626.0705687267946\n",
      "    val_loss       : -1630.0143049256876\n",
      "    val_log_likelihood: 1656.5965698242187\n",
      "    val_log_marginal: 1630.9349135894329\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -1574.722412\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -1662.853638\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -1500.607666\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -1471.180542\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -1622.046631\n",
      "    epoch          : 84\n",
      "    loss           : -1625.183268632039\n",
      "    val_loss       : -1632.9550260128453\n",
      "    val_log_likelihood: 1659.576708984375\n",
      "    val_log_marginal: 1633.7798651736232\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -1904.888184\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -1917.282715\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -1527.338501\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -1551.463623\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -1617.422852\n",
      "    epoch          : 85\n",
      "    loss           : -1632.2425343730663\n",
      "    val_loss       : -1636.641186561808\n",
      "    val_log_likelihood: 1663.2205322265625\n",
      "    val_log_marginal: 1637.2284330498428\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -1902.315430\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -1570.983398\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -1687.810791\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -1908.970947\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -1548.863403\n",
      "    epoch          : 86\n",
      "    loss           : -1632.654594194771\n",
      "    val_loss       : -1643.1465737877413\n",
      "    val_log_likelihood: 1670.0366577148438\n",
      "    val_log_marginal: 1643.8613437527997\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -1920.782349\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -1574.642334\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -1515.004883\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -1627.293823\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -1630.408936\n",
      "    epoch          : 87\n",
      "    loss           : -1634.002629950495\n",
      "    val_loss       : -1633.6160228930414\n",
      "    val_log_likelihood: 1660.8415283203126\n",
      "    val_log_marginal: 1634.1362768084568\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -1910.451904\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -1687.292114\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -1516.989258\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -1915.223633\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -1644.316528\n",
      "    epoch          : 88\n",
      "    loss           : -1641.7027503287438\n",
      "    val_loss       : -1625.128560709022\n",
      "    val_log_likelihood: 1652.5605834960938\n",
      "    val_log_marginal: 1625.735631144456\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -1901.163330\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -1675.055420\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -1644.914062\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -1912.223267\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -1654.189453\n",
      "    epoch          : 89\n",
      "    loss           : -1635.7556369894803\n",
      "    val_loss       : -1642.174719930999\n",
      "    val_log_likelihood: 1669.6790771484375\n",
      "    val_log_marginal: 1642.9244594115764\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -1927.031494\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -1577.504517\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -1601.420044\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -1617.787109\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -1639.189697\n",
      "    epoch          : 90\n",
      "    loss           : -1630.2141294573794\n",
      "    val_loss       : -1631.9801706023513\n",
      "    val_log_likelihood: 1659.8382690429687\n",
      "    val_log_marginal: 1632.6528566200286\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -1519.929932\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -1725.632812\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -1627.414185\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -1589.060181\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -1650.446533\n",
      "    epoch          : 91\n",
      "    loss           : -1648.5600900177908\n",
      "    val_loss       : -1656.4387806206942\n",
      "    val_log_likelihood: 1684.3177734375\n",
      "    val_log_marginal: 1657.3284448307254\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -1927.345459\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -1540.113281\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -1628.177490\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -1614.668213\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -1601.335449\n",
      "    epoch          : 92\n",
      "    loss           : -1608.4789603960396\n",
      "    val_loss       : -1625.965882677585\n",
      "    val_log_likelihood: 1653.8463012695313\n",
      "    val_log_marginal: 1626.3886469837278\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -1604.967163\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -1621.382446\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -1624.840820\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -1631.643555\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -1632.761963\n",
      "    epoch          : 93\n",
      "    loss           : -1643.4731602432705\n",
      "    val_loss       : -1650.4133239829912\n",
      "    val_log_likelihood: 1677.9239868164063\n",
      "    val_log_marginal: 1650.7360046131275\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -1926.159546\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -1602.881226\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -1637.478760\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -1599.900879\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -1629.328125\n",
      "    epoch          : 94\n",
      "    loss           : -1642.7999231319616\n",
      "    val_loss       : -1636.3299325447529\n",
      "    val_log_likelihood: 1664.4206665039062\n",
      "    val_log_marginal: 1636.9425443898722\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -1903.265991\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -1584.815918\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -1585.333740\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -1890.160767\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -1633.398438\n",
      "    epoch          : 95\n",
      "    loss           : -1635.360517143023\n",
      "    val_loss       : -1648.105161079578\n",
      "    val_log_likelihood: 1675.8359619140624\n",
      "    val_log_marginal: 1648.5330029722304\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -1597.220947\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -1719.161621\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -1564.310791\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -1569.335205\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -1632.426514\n",
      "    epoch          : 96\n",
      "    loss           : -1655.8312275197247\n",
      "    val_loss       : -1655.9482612024992\n",
      "    val_log_likelihood: 1684.1003662109374\n",
      "    val_log_marginal: 1656.5247426733272\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -1590.059570\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -1611.157959\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -1709.855225\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -1707.639893\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -1660.343140\n",
      "    epoch          : 97\n",
      "    loss           : -1663.4844934444616\n",
      "    val_loss       : -1655.5479077566415\n",
      "    val_log_likelihood: 1683.8319580078125\n",
      "    val_log_marginal: 1656.274217266217\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -1546.620361\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -1598.029297\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -1713.183594\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -1503.557373\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -1641.578613\n",
      "    epoch          : 98\n",
      "    loss           : -1651.1821313234839\n",
      "    val_loss       : -1652.5476416241377\n",
      "    val_log_likelihood: 1680.6512817382813\n",
      "    val_log_marginal: 1653.091726609373\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -1929.517090\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -1601.081299\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -1214.803955\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -1921.570068\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -1645.069336\n",
      "    epoch          : 99\n",
      "    loss           : -1627.1629602413366\n",
      "    val_loss       : -1608.2225924903528\n",
      "    val_log_likelihood: 1637.7279663085938\n",
      "    val_log_marginal: 1609.1661821036344\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -1919.149536\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -1385.712646\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -1531.175293\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -1538.643433\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -1549.133545\n",
      "    epoch          : 100\n",
      "    loss           : -1611.494058439047\n",
      "    val_loss       : -1643.8517755223438\n",
      "    val_log_likelihood: 1672.843701171875\n",
      "    val_log_marginal: 1644.5295323010535\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -1933.824463\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -1726.389893\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -1540.906006\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -1699.577759\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -1536.813477\n",
      "    epoch          : 101\n",
      "    loss           : -1642.699655060721\n",
      "    val_loss       : -1636.989493449591\n",
      "    val_log_likelihood: 1666.0359375\n",
      "    val_log_marginal: 1637.6735102321952\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -1932.228149\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -1564.340576\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -1536.578247\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -1558.859009\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -1648.144287\n",
      "    epoch          : 102\n",
      "    loss           : -1647.4238619662747\n",
      "    val_loss       : -1661.684265685454\n",
      "    val_log_likelihood: 1690.5292846679688\n",
      "    val_log_marginal: 1662.2632429569137\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -1937.472900\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -1716.112549\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -1516.714966\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -1518.720825\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -1530.692993\n",
      "    epoch          : 103\n",
      "    loss           : -1630.2060329323947\n",
      "    val_loss       : -1634.2479127220809\n",
      "    val_log_likelihood: 1663.2521484375\n",
      "    val_log_marginal: 1634.5331692841767\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -1684.828491\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -1540.747681\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -1535.618042\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -1538.832520\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -1553.712646\n",
      "    epoch          : 104\n",
      "    loss           : -1646.8806792910736\n",
      "    val_loss       : -1653.784119812958\n",
      "    val_log_likelihood: 1683.0879272460938\n",
      "    val_log_marginal: 1654.5449235045949\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -1593.427979\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -1595.506104\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -1708.677734\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -1887.257690\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -1653.447266\n",
      "    epoch          : 105\n",
      "    loss           : -1658.0268965617265\n",
      "    val_loss       : -1656.1264003042131\n",
      "    val_log_likelihood: 1685.603466796875\n",
      "    val_log_marginal: 1657.1269802618772\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -1894.674072\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -1641.729004\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -1680.961670\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -1699.050659\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -1503.862671\n",
      "    epoch          : 106\n",
      "    loss           : -1641.3789533860613\n",
      "    val_loss       : -1623.4602213345468\n",
      "    val_log_likelihood: 1653.1504272460938\n",
      "    val_log_marginal: 1624.357312997803\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -1910.256470\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -1581.178101\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -1596.171387\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -1548.580322\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -1640.645630\n",
      "    epoch          : 107\n",
      "    loss           : -1639.6977176477412\n",
      "    val_loss       : -1660.198586916551\n",
      "    val_log_likelihood: 1689.6198364257812\n",
      "    val_log_marginal: 1660.9172936137766\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -1728.345459\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -1734.037354\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -1704.682129\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -1555.627808\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -1666.766846\n",
      "    epoch          : 108\n",
      "    loss           : -1667.4988808206992\n",
      "    val_loss       : -1670.1409256558866\n",
      "    val_log_likelihood: 1699.4984375\n",
      "    val_log_marginal: 1670.7814931605012\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -1938.512695\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -1613.327271\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -1634.526855\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -1633.843506\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -1571.990234\n",
      "    epoch          : 109\n",
      "    loss           : -1659.5325698097154\n",
      "    val_loss       : -1668.1858623825015\n",
      "    val_log_likelihood: 1697.1115356445312\n",
      "    val_log_marginal: 1668.548603410169\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -1941.027100\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -1616.615723\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -1626.348389\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -1677.669922\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -1652.601807\n",
      "    epoch          : 110\n",
      "    loss           : -1657.178359229966\n",
      "    val_loss       : -1643.452173706703\n",
      "    val_log_likelihood: 1672.9046142578125\n",
      "    val_log_marginal: 1644.0991974097149\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -1725.042969\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -1700.112183\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -1620.481201\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -1917.214111\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -1575.743408\n",
      "    epoch          : 111\n",
      "    loss           : -1643.9493952080754\n",
      "    val_loss       : -1657.5087639583276\n",
      "    val_log_likelihood: 1686.9351196289062\n",
      "    val_log_marginal: 1658.0599261622963\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -1930.846680\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -1567.706421\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -1713.041504\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -1546.304932\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -1471.678101\n",
      "    epoch          : 112\n",
      "    loss           : -1653.430852606745\n",
      "    val_loss       : -1609.8577624626457\n",
      "    val_log_likelihood: 1640.1176513671876\n",
      "    val_log_marginal: 1610.747091941354\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -1704.669922\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -1596.078369\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -1485.905518\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -1574.587036\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -1633.796875\n",
      "    epoch          : 113\n",
      "    loss           : -1625.6846271174968\n",
      "    val_loss       : -1629.4263059809805\n",
      "    val_log_likelihood: 1659.2453247070312\n",
      "    val_log_marginal: 1629.9989625811745\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -1880.542480\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -1535.709961\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -1712.914307\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -1572.607178\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -1658.778564\n",
      "    epoch          : 114\n",
      "    loss           : -1654.2653240543782\n",
      "    val_loss       : -1659.6034042870626\n",
      "    val_log_likelihood: 1689.6614379882812\n",
      "    val_log_marginal: 1660.4262301567942\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -1916.573608\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -1610.233276\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -1561.528442\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -1564.362671\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -1576.087646\n",
      "    epoch          : 115\n",
      "    loss           : -1657.7242383295948\n",
      "    val_loss       : -1618.9040011702104\n",
      "    val_log_likelihood: 1703.0288330078124\n",
      "    val_log_marginal: 1673.8580954555423\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -1925.598389\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -1630.541016\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -1671.659790\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -1945.037842\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -1676.247803\n",
      "    epoch          : 116\n",
      "    loss           : -1674.8181539101176\n",
      "    val_loss       : -1679.1278435293584\n",
      "    val_log_likelihood: 1708.6451538085937\n",
      "    val_log_marginal: 1679.5298053432423\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -1613.592773\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -1759.676270\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -1626.546753\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -1657.127930\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -1518.713989\n",
      "    epoch          : 117\n",
      "    loss           : -1669.4856138323794\n",
      "    val_loss       : -1649.6637284768744\n",
      "    val_log_likelihood: 1679.3881225585938\n",
      "    val_log_marginal: 1650.257499201737\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -1919.290283\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -1592.941650\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -1567.051758\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -1579.672852\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -1663.319458\n",
      "    epoch          : 118\n",
      "    loss           : -1671.3280391881963\n",
      "    val_loss       : -1678.3368908166885\n",
      "    val_log_likelihood: 1708.2776611328125\n",
      "    val_log_marginal: 1679.1213841397316\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -1745.824951\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -1635.791870\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -1616.050537\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -1679.684326\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -1687.925781\n",
      "    epoch          : 119\n",
      "    loss           : -1673.217152208385\n",
      "    val_loss       : -1685.948220149055\n",
      "    val_log_likelihood: 1715.5552124023438\n",
      "    val_log_marginal: 1686.3885477740318\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -1947.629395\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -1635.073975\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -1084.692017\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -1674.399170\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -1585.823730\n",
      "    epoch          : 120\n",
      "    loss           : -1682.4801375889542\n",
      "    val_loss       : -1679.1987348448486\n",
      "    val_log_likelihood: 1708.9593872070313\n",
      "    val_log_marginal: 1679.8189240511506\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -1933.759033\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -1636.362915\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -1590.833496\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -1644.964600\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -1587.862061\n",
      "    epoch          : 121\n",
      "    loss           : -1678.3492177831065\n",
      "    val_loss       : -1666.9170949149877\n",
      "    val_log_likelihood: 1697.2878295898438\n",
      "    val_log_marginal: 1667.805197541614\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -1568.918091\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -1630.355469\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -1588.915527\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -1950.951416\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -1676.339844\n",
      "    epoch          : 122\n",
      "    loss           : -1675.9562939936573\n",
      "    val_loss       : -1658.093148906529\n",
      "    val_log_likelihood: 1688.4459350585937\n",
      "    val_log_marginal: 1658.91682399196\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -1585.901123\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -1611.615356\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -1594.214844\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -1671.484863\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -1687.200928\n",
      "    epoch          : 123\n",
      "    loss           : -1674.9408236966274\n",
      "    val_loss       : -1672.0965024231002\n",
      "    val_log_likelihood: 1702.1024291992187\n",
      "    val_log_marginal: 1672.5349257212133\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -1945.467529\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -1561.924805\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -1576.938843\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -1933.703369\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -1687.958008\n",
      "    epoch          : 124\n",
      "    loss           : -1670.1237563331529\n",
      "    val_loss       : -1687.993887799792\n",
      "    val_log_likelihood: 1718.225537109375\n",
      "    val_log_marginal: 1688.7535544674843\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -1953.099609\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -1588.661987\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -1730.470581\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -1684.251587\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -1585.455078\n",
      "    epoch          : 125\n",
      "    loss           : -1680.8737043626238\n",
      "    val_loss       : -1684.0658355947585\n",
      "    val_log_likelihood: 1714.1426147460938\n",
      "    val_log_marginal: 1684.5748427344085\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -1956.023926\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -1594.237061\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -1642.555420\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -1668.381958\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -1588.700073\n",
      "    epoch          : 126\n",
      "    loss           : -1675.1467635655167\n",
      "    val_loss       : -1688.7446456499397\n",
      "    val_log_likelihood: 1719.1232543945312\n",
      "    val_log_marginal: 1689.6089502882212\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -1952.336670\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -1676.147705\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -1664.429077\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -1931.062012\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -1665.494629\n",
      "    epoch          : 127\n",
      "    loss           : -1677.2263981280942\n",
      "    val_loss       : -1689.6022828483954\n",
      "    val_log_likelihood: 1719.7624267578126\n",
      "    val_log_marginal: 1690.2759228188545\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -1946.881104\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -1724.857422\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -1727.371216\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -1721.520874\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -1578.601562\n",
      "    epoch          : 128\n",
      "    loss           : -1669.3356764387377\n",
      "    val_loss       : -1679.8738668523729\n",
      "    val_log_likelihood: 1710.26513671875\n",
      "    val_log_marginal: 1680.7491274181752\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -1610.398193\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -1604.519775\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -1585.161499\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -1723.298096\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -1557.560303\n",
      "    epoch          : 129\n",
      "    loss           : -1663.258301989867\n",
      "    val_loss       : -1657.1696581559256\n",
      "    val_log_likelihood: 1687.3109985351562\n",
      "    val_log_marginal: 1657.586880767\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -1894.413940\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -1593.314331\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -1649.308350\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -1548.961182\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -1671.430298\n",
      "    epoch          : 130\n",
      "    loss           : -1652.2639232673268\n",
      "    val_loss       : -1616.6051001096143\n",
      "    val_log_likelihood: 1686.224951171875\n",
      "    val_log_marginal: 1656.2922870289535\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -1933.304932\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -1578.405884\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -1581.060791\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -1674.843140\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -1570.229736\n",
      "    epoch          : 131\n",
      "    loss           : -1646.4851642268718\n",
      "    val_loss       : -1661.6446670729667\n",
      "    val_log_likelihood: 1692.47138671875\n",
      "    val_log_marginal: 1662.2197713527828\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -1944.344604\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -1746.896484\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -1566.868652\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -1723.070068\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -1592.063232\n",
      "    epoch          : 132\n",
      "    loss           : -1670.1117535581684\n",
      "    val_loss       : -1686.0397947097197\n",
      "    val_log_likelihood: 1716.6156982421876\n",
      "    val_log_marginal: 1686.3629078544043\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -1759.645508\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -1617.331421\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -1583.612671\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -1635.490112\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -1678.831421\n",
      "    epoch          : 133\n",
      "    loss           : -1685.8934326171875\n",
      "    val_loss       : -1590.2506820512936\n",
      "    val_log_likelihood: 1710.4596801757812\n",
      "    val_log_marginal: 1680.1872400235384\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -1619.245483\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -1754.429199\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -1741.473267\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -1687.437866\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -1650.941650\n",
      "    epoch          : 134\n",
      "    loss           : -1676.7851719620205\n",
      "    val_loss       : -1696.4632784048094\n",
      "    val_log_likelihood: 1727.2306762695312\n",
      "    val_log_marginal: 1697.1410080548376\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -1951.048340\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -1640.853760\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -1590.582153\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -1644.368530\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -1688.386353\n",
      "    epoch          : 135\n",
      "    loss           : -1689.9604552618348\n",
      "    val_loss       : -1694.5561543579213\n",
      "    val_log_likelihood: 1725.5607543945312\n",
      "    val_log_marginal: 1695.4184572529048\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -1939.784546\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -1771.720947\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -1588.419434\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -1659.117920\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -1594.503296\n",
      "    epoch          : 136\n",
      "    loss           : -1685.463514271349\n",
      "    val_loss       : -1697.0826662084087\n",
      "    val_log_likelihood: 1727.7553833007812\n",
      "    val_log_marginal: 1697.5155681356587\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -1951.772705\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -1593.682983\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -1647.013672\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -1567.097900\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -1597.810669\n",
      "    epoch          : 137\n",
      "    loss           : -1693.0305538366338\n",
      "    val_loss       : -1694.2907991455868\n",
      "    val_log_likelihood: 1725.3623291015624\n",
      "    val_log_marginal: 1695.0786551681806\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -1950.479126\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -1168.171143\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -1693.613770\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -1666.408813\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -1595.760376\n",
      "    epoch          : 138\n",
      "    loss           : -1684.0384823638615\n",
      "    val_loss       : -1701.987083516456\n",
      "    val_log_likelihood: 1733.04013671875\n",
      "    val_log_marginal: 1702.6900792796193\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -1762.733643\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -1647.792603\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -1630.787354\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -1678.511963\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -1647.059448\n",
      "    epoch          : 139\n",
      "    loss           : -1668.4233809367265\n",
      "    val_loss       : -1697.177338593267\n",
      "    val_log_likelihood: 1728.304736328125\n",
      "    val_log_marginal: 1697.9245211373502\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -1768.416016\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -1630.746094\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -1587.817139\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -1965.176514\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -1647.876099\n",
      "    epoch          : 140\n",
      "    loss           : -1689.1462233137377\n",
      "    val_loss       : -1665.742304300517\n",
      "    val_log_likelihood: 1730.4083251953125\n",
      "    val_log_marginal: 1700.1832161743273\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -1966.433594\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -1766.805420\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -1589.604614\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -1668.561157\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -1689.792480\n",
      "    epoch          : 141\n",
      "    loss           : -1682.9419706741182\n",
      "    val_loss       : -1660.123673387058\n",
      "    val_log_likelihood: 1727.0972778320313\n",
      "    val_log_marginal: 1696.8471399422735\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -1950.591309\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -1591.596924\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -1658.193481\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -1942.990234\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -1596.054932\n",
      "    epoch          : 142\n",
      "    loss           : -1686.9888045811417\n",
      "    val_loss       : -1653.2407426774503\n",
      "    val_log_likelihood: 1722.621533203125\n",
      "    val_log_marginal: 1692.35499750264\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -1951.115845\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -1583.859863\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -1571.008423\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -1700.056274\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -1675.582886\n",
      "    epoch          : 143\n",
      "    loss           : -1666.7762475344214\n",
      "    val_loss       : -1631.0924974303693\n",
      "    val_log_likelihood: 1709.7016235351562\n",
      "    val_log_marginal: 1679.2917851988227\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -1868.879150\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -1626.136475\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -1597.068726\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -1912.853027\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -1688.629761\n",
      "    epoch          : 144\n",
      "    loss           : -1678.2444536567914\n",
      "    val_loss       : -1696.5123062942178\n",
      "    val_log_likelihood: 1727.2628051757813\n",
      "    val_log_marginal: 1696.9692714374512\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -1922.556396\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -1667.760254\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -1629.343018\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -1684.860229\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -1691.877686\n",
      "    epoch          : 145\n",
      "    loss           : -1687.4926250193378\n",
      "    val_loss       : -1695.2142161048948\n",
      "    val_log_likelihood: 1726.1749633789063\n",
      "    val_log_marginal: 1695.7912903036922\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -1927.646729\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -1645.190674\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -1591.764282\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -1546.880005\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -1671.224731\n",
      "    epoch          : 146\n",
      "    loss           : -1669.9818888749226\n",
      "    val_loss       : -1680.4344953607767\n",
      "    val_log_likelihood: 1711.4859497070313\n",
      "    val_log_marginal: 1681.1738381240516\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -1627.554321\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -1608.990356\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -1739.626221\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -1696.637695\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -1685.241333\n",
      "    epoch          : 147\n",
      "    loss           : -1673.6435123859067\n",
      "    val_loss       : -1637.6335341254248\n",
      "    val_log_likelihood: 1726.9450439453126\n",
      "    val_log_marginal: 1696.7083127301187\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -1758.827515\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -1588.296265\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -1704.637573\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -1706.307373\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -1698.444824\n",
      "    epoch          : 148\n",
      "    loss           : -1695.4665587774598\n",
      "    val_loss       : -1649.1902276907117\n",
      "    val_log_likelihood: 1680.5242065429688\n",
      "    val_log_marginal: 1649.926751464233\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -1932.075439\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -1751.947510\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -1594.154419\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -1636.207153\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -1673.981201\n",
      "    epoch          : 149\n",
      "    loss           : -1660.436280505492\n",
      "    val_loss       : -1630.7568621030077\n",
      "    val_log_likelihood: 1662.7109985351562\n",
      "    val_log_marginal: 1631.9382929197618\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -1929.674927\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -1606.283325\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -1554.203613\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -1668.924316\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -1698.395630\n",
      "    epoch          : 150\n",
      "    loss           : -1658.9822284962872\n",
      "    val_loss       : -1691.5401390032844\n",
      "    val_log_likelihood: 1722.7559448242187\n",
      "    val_log_marginal: 1692.1101654663573\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -1949.737183\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -1676.336670\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -1584.253906\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -1731.863525\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -1661.328491\n",
      "    epoch          : 151\n",
      "    loss           : -1675.9310810353497\n",
      "    val_loss       : -1659.8903365761041\n",
      "    val_log_likelihood: 1724.7252685546875\n",
      "    val_log_marginal: 1694.0329993125051\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -1942.421509\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -1756.696533\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -1542.421997\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -1667.401123\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -1602.374512\n",
      "    epoch          : 152\n",
      "    loss           : -1661.4877518757735\n",
      "    val_loss       : -1652.3079159714282\n",
      "    val_log_likelihood: 1684.005615234375\n",
      "    val_log_marginal: 1653.1308105085059\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -1945.344971\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -1701.202637\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -1652.976318\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -1718.478271\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -1597.669922\n",
      "    epoch          : 153\n",
      "    loss           : -1666.7091487469058\n",
      "    val_loss       : -1701.3819410944357\n",
      "    val_log_likelihood: 1732.7003662109375\n",
      "    val_log_marginal: 1701.8634169447855\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -1964.088257\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -1735.898193\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -1713.796021\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -1666.898682\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -1544.309082\n",
      "    epoch          : 154\n",
      "    loss           : -1680.219886099938\n",
      "    val_loss       : -1642.1670898670332\n",
      "    val_log_likelihood: 1706.4931030273438\n",
      "    val_log_marginal: 1675.2391420666129\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -1754.882568\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -1613.785156\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -1718.670898\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -1725.295410\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -1636.231445\n",
      "    epoch          : 155\n",
      "    loss           : -1674.1269579594677\n",
      "    val_loss       : -1632.8653479311615\n",
      "    val_log_likelihood: 1690.8335083007812\n",
      "    val_log_marginal: 1659.8032835651188\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -1925.566650\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -1623.300415\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -1725.756104\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -1690.125732\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -1702.583984\n",
      "    epoch          : 156\n",
      "    loss           : -1668.4489576887377\n",
      "    val_loss       : -1664.0551885491236\n",
      "    val_log_likelihood: 1729.8384643554687\n",
      "    val_log_marginal: 1698.7868802712633\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -1635.530640\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -1450.050171\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -1670.413086\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -1708.782349\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -1706.453979\n",
      "    epoch          : 157\n",
      "    loss           : -1698.765269666615\n",
      "    val_loss       : -1710.3044597385451\n",
      "    val_log_likelihood: 1741.9131469726562\n",
      "    val_log_marginal: 1710.8996536244006\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -1960.029663\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -1657.445312\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -1264.934570\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -1680.564575\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -1602.858276\n",
      "    epoch          : 158\n",
      "    loss           : -1697.0737087136447\n",
      "    val_loss       : -1709.2592095056548\n",
      "    val_log_likelihood: 1740.960498046875\n",
      "    val_log_marginal: 1709.9510835383087\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -1693.532104\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -1784.884277\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -1572.333496\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -1603.262695\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -1506.743408\n",
      "    epoch          : 159\n",
      "    loss           : -1663.491913143951\n",
      "    val_loss       : -1661.1717771392316\n",
      "    val_log_likelihood: 1693.3289916992187\n",
      "    val_log_marginal: 1661.8961698315447\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -1798.777222\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -1735.506348\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -1627.963013\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -1712.619507\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -1663.363525\n",
      "    epoch          : 160\n",
      "    loss           : -1648.9596769125155\n",
      "    val_loss       : -1626.243077502027\n",
      "    val_log_likelihood: 1717.708251953125\n",
      "    val_log_marginal: 1686.3992719691246\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -1582.305298\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -1593.148926\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -1737.388916\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -1919.209839\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -1605.669678\n",
      "    epoch          : 161\n",
      "    loss           : -1688.314126798422\n",
      "    val_loss       : -1670.4803953429684\n",
      "    val_log_likelihood: 1732.2659301757812\n",
      "    val_log_marginal: 1701.1808397214124\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -1359.744873\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -1786.061157\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -1673.862915\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -1690.865356\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -1691.598877\n",
      "    epoch          : 162\n",
      "    loss           : -1689.348497447401\n",
      "    val_loss       : -1678.8060602854937\n",
      "    val_log_likelihood: 1711.0071411132812\n",
      "    val_log_marginal: 1679.703376194436\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -1777.531616\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -1602.572388\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -1641.582520\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -1357.059692\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -1669.133301\n",
      "    epoch          : 163\n",
      "    loss           : -1662.0162704014542\n",
      "    val_loss       : -1680.2645958865062\n",
      "    val_log_likelihood: 1712.343798828125\n",
      "    val_log_marginal: 1680.8599734921008\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -1927.600464\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -1628.692993\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -1667.122192\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -1720.157959\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -1649.946411\n",
      "    epoch          : 164\n",
      "    loss           : -1670.0911623510983\n",
      "    val_loss       : -1674.9907600050792\n",
      "    val_log_likelihood: 1707.4851196289062\n",
      "    val_log_marginal: 1675.8571698885412\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -1903.862793\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -1764.015747\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -1575.688721\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -1547.543457\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -1585.887695\n",
      "    epoch          : 165\n",
      "    loss           : -1655.3059662167389\n",
      "    val_loss       : -1686.204953746125\n",
      "    val_log_likelihood: 1718.6256103515625\n",
      "    val_log_marginal: 1687.1034655999392\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -1943.732056\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -1634.865234\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -1727.686768\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -1664.970581\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -1588.650146\n",
      "    epoch          : 166\n",
      "    loss           : -1675.6936180190285\n",
      "    val_loss       : -1704.5379222154618\n",
      "    val_log_likelihood: 1736.7930786132813\n",
      "    val_log_marginal: 1705.2616372343152\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -1946.931152\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -1649.587891\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -1678.679443\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -1597.935181\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -1601.329102\n",
      "    epoch          : 167\n",
      "    loss           : -1689.8048023186107\n",
      "    val_loss       : -1699.7754537541418\n",
      "    val_log_likelihood: 1732.1804809570312\n",
      "    val_log_marginal: 1700.628526643291\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -1928.880493\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -1660.058838\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -1218.009888\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -1952.446289\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -1610.675049\n",
      "    epoch          : 168\n",
      "    loss           : -1685.3022618057705\n",
      "    val_loss       : -1678.6030235772953\n",
      "    val_log_likelihood: 1744.9766723632813\n",
      "    val_log_marginal: 1713.3691034985968\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -1954.794067\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -1603.195312\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -1758.089600\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -1953.030762\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -1614.694580\n",
      "    epoch          : 169\n",
      "    loss           : -1688.3675537109375\n",
      "    val_loss       : -1711.0226101016626\n",
      "    val_log_likelihood: 1743.2819702148438\n",
      "    val_log_marginal: 1711.8203995250453\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -1957.701904\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -1649.978027\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -1703.607666\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -1568.065796\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -1605.872803\n",
      "    epoch          : 170\n",
      "    loss           : -1680.5984515199566\n",
      "    val_loss       : -1695.4315787391738\n",
      "    val_log_likelihood: 1727.7809692382812\n",
      "    val_log_marginal: 1696.2161865284877\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -1948.431885\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -1656.894043\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -1751.036499\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -1598.680908\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -1592.585693\n",
      "    epoch          : 171\n",
      "    loss           : -1699.594780950263\n",
      "    val_loss       : -1669.068039933592\n",
      "    val_log_likelihood: 1741.9650756835938\n",
      "    val_log_marginal: 1710.5165001310531\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -1959.488281\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -1786.277588\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -1709.180176\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -1722.821899\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -1601.881836\n",
      "    epoch          : 172\n",
      "    loss           : -1698.384600044477\n",
      "    val_loss       : -1689.4307344954461\n",
      "    val_log_likelihood: 1751.7020141601563\n",
      "    val_log_marginal: 1720.1627980675548\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -1960.709595\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -1748.799316\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -1614.780273\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -1965.467773\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -1718.664307\n",
      "    epoch          : 173\n",
      "    loss           : -1703.8356002958694\n",
      "    val_loss       : -1596.9082326708362\n",
      "    val_log_likelihood: 1748.8786987304688\n",
      "    val_log_marginal: 1717.2642427698665\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -1962.166748\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -1733.995605\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -1654.745239\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -1955.092041\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -1698.431396\n",
      "    epoch          : 174\n",
      "    loss           : -1689.7172440632735\n",
      "    val_loss       : -1602.7008900122717\n",
      "    val_log_likelihood: 1741.3058471679688\n",
      "    val_log_marginal: 1709.493282641843\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -1648.197021\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -1780.176025\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -1709.576172\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -1709.914795\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -1710.764038\n",
      "    epoch          : 175\n",
      "    loss           : -1695.812684918394\n",
      "    val_loss       : -1682.220724139735\n",
      "    val_log_likelihood: 1746.0435302734375\n",
      "    val_log_marginal: 1714.3715002660588\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -1964.278931\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -1658.503052\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -1689.536377\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -1960.642578\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -1710.405273\n",
      "    epoch          : 176\n",
      "    loss           : -1702.6993686185024\n",
      "    val_loss       : -1685.1575914734974\n",
      "    val_log_likelihood: 1717.5765991210938\n",
      "    val_log_marginal: 1685.8626597288996\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -1833.413086\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -1595.601562\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -1705.712891\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -1938.826172\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -1721.807983\n",
      "    epoch          : 177\n",
      "    loss           : -1685.202067460164\n",
      "    val_loss       : -1721.1725321475417\n",
      "    val_log_likelihood: 1752.9860473632812\n",
      "    val_log_marginal: 1721.6017303545027\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -1950.197754\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -1782.221924\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -1697.802734\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -1619.849609\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -1625.360474\n",
      "    epoch          : 178\n",
      "    loss           : -1701.152558883818\n",
      "    val_loss       : -1665.4873876281083\n",
      "    val_log_likelihood: 1753.9810424804687\n",
      "    val_log_marginal: 1722.6061803416378\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -1963.104126\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -1665.819824\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -1754.502930\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -1695.724731\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -1723.281738\n",
      "    epoch          : 179\n",
      "    loss           : -1687.3560996480508\n",
      "    val_loss       : -1655.7506246287376\n",
      "    val_log_likelihood: 1741.7102783203125\n",
      "    val_log_marginal: 1710.4237937804392\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -1959.463013\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -1618.356079\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -1625.815796\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -1726.585449\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -1615.015625\n",
      "    epoch          : 180\n",
      "    loss           : -1695.1754114132116\n",
      "    val_loss       : -1712.6463494136929\n",
      "    val_log_likelihood: 1744.6151489257813\n",
      "    val_log_marginal: 1713.3833747213364\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -1961.126709\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -1759.272949\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -1720.762451\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -1754.615967\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -1619.237183\n",
      "    epoch          : 181\n",
      "    loss           : -1687.825438244508\n",
      "    val_loss       : -1659.1861971750855\n",
      "    val_log_likelihood: 1754.6335083007812\n",
      "    val_log_marginal: 1723.3634289544075\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -1959.417725\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -1791.284180\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -1759.753784\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -1625.032593\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -1613.413452\n",
      "    epoch          : 182\n",
      "    loss           : -1682.2586331509128\n",
      "    val_loss       : -1698.378421324119\n",
      "    val_log_likelihood: 1757.7134521484375\n",
      "    val_log_marginal: 1726.3272988628603\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -1964.555420\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -1794.479248\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -1618.727661\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -1725.680664\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -1427.206421\n",
      "    epoch          : 183\n",
      "    loss           : -1698.5879848971226\n",
      "    val_loss       : -1727.781385897845\n",
      "    val_log_likelihood: 1759.5978637695312\n",
      "    val_log_marginal: 1728.1894236875146\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -1968.468994\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -1802.701904\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -1629.332031\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -1625.948608\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -1730.043701\n",
      "    epoch          : 184\n",
      "    loss           : -1695.3695068359375\n",
      "    val_loss       : -1730.0726536672562\n",
      "    val_log_likelihood: 1761.8046875\n",
      "    val_log_marginal: 1730.437721036275\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -1794.745728\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -1627.903809\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -1640.015381\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -1613.567871\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -1726.939209\n",
      "    epoch          : 185\n",
      "    loss           : -1701.9778146271658\n",
      "    val_loss       : -1692.4955853555352\n",
      "    val_log_likelihood: 1751.0707275390625\n",
      "    val_log_marginal: 1719.6104080643504\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -1963.227661\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -1512.352051\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -1692.644531\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -1757.274658\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -1446.696045\n",
      "    epoch          : 186\n",
      "    loss           : -1699.5427983350094\n",
      "    val_loss       : -1687.2614374525845\n",
      "    val_log_likelihood: 1719.6019897460938\n",
      "    val_log_marginal: 1687.927404130011\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -1931.347656\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -1542.043091\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -1567.289429\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -1660.337769\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -1703.639160\n",
      "    epoch          : 187\n",
      "    loss           : -1656.2704111231435\n",
      "    val_loss       : -1656.0080754907801\n",
      "    val_log_likelihood: 1722.49052734375\n",
      "    val_log_marginal: 1690.5496055547148\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -1952.603149\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -1585.632080\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -1619.686890\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -1641.967651\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -1576.990601\n",
      "    epoch          : 188\n",
      "    loss           : -1664.6019299195545\n",
      "    val_loss       : -1651.8842790726571\n",
      "    val_log_likelihood: 1735.4284057617188\n",
      "    val_log_marginal: 1703.5360889667304\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -1956.418213\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -1633.333740\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -1588.359131\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -1945.635620\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -1574.013062\n",
      "    epoch          : 189\n",
      "    loss           : -1651.5026444538985\n",
      "    val_loss       : -1661.0659252632408\n",
      "    val_log_likelihood: 1720.032373046875\n",
      "    val_log_marginal: 1687.5626534122973\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -1942.453979\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -1775.739990\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -1566.030273\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -1701.336792\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -1602.526489\n",
      "    epoch          : 190\n",
      "    loss           : -1669.018664671643\n",
      "    val_loss       : -1701.8228551624343\n",
      "    val_log_likelihood: 1734.7798706054687\n",
      "    val_log_marginal: 1702.4165362856038\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -1947.484863\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -1656.209717\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -1694.079590\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -1310.555054\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -1602.303467\n",
      "    epoch          : 191\n",
      "    loss           : -1671.3840573754642\n",
      "    val_loss       : -1636.1117645328864\n",
      "    val_log_likelihood: 1703.599365234375\n",
      "    val_log_marginal: 1671.181772611491\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -1952.514893\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -1613.562012\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -1674.836426\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -1673.618774\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -1710.410034\n",
      "    epoch          : 192\n",
      "    loss           : -1657.5183008779393\n",
      "    val_loss       : -1705.0279425011947\n",
      "    val_log_likelihood: 1738.3194458007813\n",
      "    val_log_marginal: 1705.9802121649495\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -1958.344238\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -1776.514038\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -1502.534424\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -1586.000732\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -1592.628540\n",
      "    epoch          : 193\n",
      "    loss           : -1634.8170383566678\n",
      "    val_loss       : -1670.645534346439\n",
      "    val_log_likelihood: 1703.683984375\n",
      "    val_log_marginal: 1671.3530374247791\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -1927.663574\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -1765.373535\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -1558.626709\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -1673.280396\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -1672.788696\n",
      "    epoch          : 194\n",
      "    loss           : -1681.6945522799351\n",
      "    val_loss       : -1705.8960258657112\n",
      "    val_log_likelihood: 1738.7539916992187\n",
      "    val_log_marginal: 1706.6060033811707\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -1954.474609\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -1578.769165\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -1584.834473\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -1757.447754\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -1709.231567\n",
      "    epoch          : 195\n",
      "    loss           : -1671.6563817392482\n",
      "    val_loss       : -1713.889166302979\n",
      "    val_log_likelihood: 1746.6431396484375\n",
      "    val_log_marginal: 1714.6499719504268\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -1501.411987\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -1628.510620\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -1713.342773\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -1599.704590\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -1619.904297\n",
      "    epoch          : 196\n",
      "    loss           : -1674.6943456064357\n",
      "    val_loss       : -1704.3981104327365\n",
      "    val_log_likelihood: 1736.8459106445312\n",
      "    val_log_marginal: 1704.7939141910524\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -1946.160889\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -1602.038574\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -1291.639893\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -1697.884155\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -1758.855591\n",
      "    epoch          : 197\n",
      "    loss           : -1676.5996516765933\n",
      "    val_loss       : -1660.3103657791391\n",
      "    val_log_likelihood: 1750.4025634765626\n",
      "    val_log_marginal: 1718.4163392942398\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -1969.666870\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -1714.806519\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -1769.231445\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -1305.565430\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -1686.821533\n",
      "    epoch          : 198\n",
      "    loss           : -1672.9419428759281\n",
      "    val_loss       : -1722.325212559849\n",
      "    val_log_likelihood: 1754.914208984375\n",
      "    val_log_marginal: 1722.913675410208\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -1965.077393\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -1656.711792\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -1765.000610\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -1624.083618\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -1719.187134\n",
      "    epoch          : 199\n",
      "    loss           : -1682.4259371615872\n",
      "    val_loss       : -1662.6940622828902\n",
      "    val_log_likelihood: 1753.7967041015625\n",
      "    val_log_marginal: 1721.8681141581387\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -1964.585449\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -1792.921387\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -1709.464600\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -1674.192505\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -1709.904541\n",
      "    epoch          : 200\n",
      "    loss           : -1666.7327784170018\n",
      "    val_loss       : -1715.50658008717\n",
      "    val_log_likelihood: 1748.0139282226562\n",
      "    val_log_marginal: 1715.9278782248189\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -1968.548218\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -1793.605957\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -1760.086060\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -1650.090576\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -1696.818481\n",
      "    epoch          : 201\n",
      "    loss           : -1699.7305769212176\n",
      "    val_loss       : -1652.136516147107\n",
      "    val_log_likelihood: 1744.6768676757813\n",
      "    val_log_marginal: 1712.6679652873427\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -1967.252441\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -1597.515015\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -1712.513184\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -1612.468994\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -1711.969238\n",
      "    epoch          : 202\n",
      "    loss           : -1683.4705991839419\n",
      "    val_loss       : -1715.694525208883\n",
      "    val_log_likelihood: 1748.4856079101562\n",
      "    val_log_marginal: 1716.3373506148407\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -1959.652100\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -1659.644165\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -1761.414551\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -1408.793579\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -1623.426270\n",
      "    epoch          : 203\n",
      "    loss           : -1657.0461752107828\n",
      "    val_loss       : -1694.6062217559665\n",
      "    val_log_likelihood: 1758.348828125\n",
      "    val_log_marginal: 1726.2527857463806\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -1675.643555\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -1790.648804\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -1042.757935\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -1621.435425\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -1693.078613\n",
      "    epoch          : 204\n",
      "    loss           : -1679.7779516843286\n",
      "    val_loss       : -1684.6519637163728\n",
      "    val_log_likelihood: 1749.2244018554688\n",
      "    val_log_marginal: 1717.2116727737987\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -1786.137451\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -1762.476807\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -1416.270752\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -1969.339478\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -1717.772217\n",
      "    epoch          : 205\n",
      "    loss           : -1681.321014706451\n",
      "    val_loss       : -1659.68465489652\n",
      "    val_log_likelihood: 1754.9900390625\n",
      "    val_log_marginal: 1723.0427690196782\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -1970.172241\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -1407.291870\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -1293.498657\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -1634.272217\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -1713.408325\n",
      "    epoch          : 206\n",
      "    loss           : -1698.7714481164912\n",
      "    val_loss       : -1720.7261128121986\n",
      "    val_log_likelihood: 1753.1830932617188\n",
      "    val_log_marginal: 1721.1720861230126\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -1966.864746\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -1674.422241\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -1697.252686\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -1774.682861\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -1702.715332\n",
      "    epoch          : 207\n",
      "    loss           : -1693.0429663327661\n",
      "    val_loss       : -1700.8018557965756\n",
      "    val_log_likelihood: 1764.977685546875\n",
      "    val_log_marginal: 1733.0946058230397\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -1966.534180\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -1669.965942\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -1701.754028\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -1735.288818\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -1733.779541\n",
      "    epoch          : 208\n",
      "    loss           : -1695.1891739344833\n",
      "    val_loss       : -1701.63323076237\n",
      "    val_log_likelihood: 1763.4453125\n",
      "    val_log_marginal: 1731.522624610737\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -1776.633057\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -1768.879883\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -1626.228027\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -1704.287109\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -1691.416504\n",
      "    epoch          : 209\n",
      "    loss           : -1682.9514655689202\n",
      "    val_loss       : -1692.1563179483637\n",
      "    val_log_likelihood: 1724.6182373046875\n",
      "    val_log_marginal: 1692.66056540463\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -1909.183105\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -1661.902100\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -1353.692017\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -1959.083984\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -1685.545166\n",
      "    epoch          : 210\n",
      "    loss           : -1681.235680306312\n",
      "    val_loss       : -1695.2043744184077\n",
      "    val_log_likelihood: 1757.900732421875\n",
      "    val_log_marginal: 1725.9806363794953\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -1795.843018\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -1288.262451\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -1473.850830\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -1711.673584\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -1716.129395\n",
      "    epoch          : 211\n",
      "    loss           : -1668.0955532564976\n",
      "    val_loss       : -1730.54537307024\n",
      "    val_log_likelihood: 1762.989697265625\n",
      "    val_log_marginal: 1730.9238926921062\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -1972.200439\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -1666.396484\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -1336.981201\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -1705.258911\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -1720.344116\n",
      "    epoch          : 212\n",
      "    loss           : -1682.3758291112315\n",
      "    val_loss       : -1715.4908447081223\n",
      "    val_log_likelihood: 1748.3688598632812\n",
      "    val_log_marginal: 1716.1680427655563\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -1948.919556\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -1488.635864\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -1615.100342\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -1612.567871\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -1614.582520\n",
      "    epoch          : 213\n",
      "    loss           : -1699.1103648572864\n",
      "    val_loss       : -1686.3654019808396\n",
      "    val_log_likelihood: 1758.1087280273437\n",
      "    val_log_marginal: 1725.8981443900618\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -1968.804077\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -1674.561035\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -1632.458252\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -1627.589600\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -1604.340576\n",
      "    epoch          : 214\n",
      "    loss           : -1681.6433903155942\n",
      "    val_loss       : -1714.7092432999984\n",
      "    val_log_likelihood: 1747.5232421875\n",
      "    val_log_marginal: 1715.3185747176035\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -1944.519531\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -1759.766846\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -1610.890869\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -1679.609375\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -1675.153320\n",
      "    epoch          : 215\n",
      "    loss           : -1689.4760089534343\n",
      "    val_loss       : -1722.1869299413636\n",
      "    val_log_likelihood: 1754.8103515625\n",
      "    val_log_marginal: 1722.7048552603082\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -1971.542969\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -1636.239990\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -1448.492188\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -1303.718872\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -1691.046631\n",
      "    epoch          : 216\n",
      "    loss           : -1657.4674126653388\n",
      "    val_loss       : -1670.1990337196737\n",
      "    val_log_likelihood: 1739.5547973632813\n",
      "    val_log_marginal: 1707.4993451897055\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -1964.169800\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -1669.606689\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -1610.041016\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -1596.993408\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -1702.874756\n",
      "    epoch          : 217\n",
      "    loss           : -1664.9513725054146\n",
      "    val_loss       : -1565.8516656288878\n",
      "    val_log_likelihood: 1716.9690185546874\n",
      "    val_log_marginal: 1684.5421248149128\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -1939.419067\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -1638.478271\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -1729.347900\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -1589.521729\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -1618.854248\n",
      "    epoch          : 218\n",
      "    loss           : -1671.839058148979\n",
      "    val_loss       : -1592.3808070655912\n",
      "    val_log_likelihood: 1744.4081909179688\n",
      "    val_log_marginal: 1712.042067554073\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -1945.201050\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -1657.299438\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -1720.014648\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -1599.606323\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -1713.915161\n",
      "    epoch          : 219\n",
      "    loss           : -1669.9487921082148\n",
      "    val_loss       : -1684.7851620523259\n",
      "    val_log_likelihood: 1754.4531127929688\n",
      "    val_log_marginal: 1722.1555215332658\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -1968.162598\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -1366.703735\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -980.927979\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -1718.490479\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -1672.631958\n",
      "    epoch          : 220\n",
      "    loss           : -1674.6718030872912\n",
      "    val_loss       : -1720.558707670681\n",
      "    val_log_likelihood: 1753.3845336914062\n",
      "    val_log_marginal: 1721.081281494356\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -1667.273193\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -1799.611938\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -1679.981567\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -1699.466553\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -1692.064575\n",
      "    epoch          : 221\n",
      "    loss           : -1677.5461788366338\n",
      "    val_loss       : -1720.1030618648977\n",
      "    val_log_likelihood: 1753.2658081054688\n",
      "    val_log_marginal: 1720.8756891120224\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -1968.356079\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -1666.725830\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -1625.702515\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -1720.928589\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -1418.480957\n",
      "    epoch          : 222\n",
      "    loss           : -1674.004097211479\n",
      "    val_loss       : -1671.724391054921\n",
      "    val_log_likelihood: 1764.33818359375\n",
      "    val_log_marginal: 1732.0831082601119\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -1968.296387\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -1807.264160\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -1715.934204\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -1712.103760\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -1411.609497\n",
      "    epoch          : 223\n",
      "    loss           : -1680.3890707185953\n",
      "    val_loss       : -1698.5433974392713\n",
      "    val_log_likelihood: 1762.1034301757813\n",
      "    val_log_marginal: 1729.6814714923162\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -1965.006592\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -1808.845581\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -1770.499634\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -1977.965698\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -1728.986206\n",
      "    epoch          : 224\n",
      "    loss           : -1697.2896933980508\n",
      "    val_loss       : -1736.505745269917\n",
      "    val_log_likelihood: 1769.3941284179687\n",
      "    val_log_marginal: 1737.0392455179244\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -1681.703491\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -1633.969971\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -1341.415649\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -1638.737793\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -1721.611206\n",
      "    epoch          : 225\n",
      "    loss           : -1678.2402821153698\n",
      "    val_loss       : -1701.5270032482222\n",
      "    val_log_likelihood: 1766.87119140625\n",
      "    val_log_marginal: 1734.537528407201\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -1974.865234\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -1680.034424\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -1696.296021\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -1692.244263\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -1638.879028\n",
      "    epoch          : 226\n",
      "    loss           : -1694.125832132774\n",
      "    val_loss       : -1709.349794610031\n",
      "    val_log_likelihood: 1774.1092895507813\n",
      "    val_log_marginal: 1741.720217098687\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -1977.974243\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -1802.808838\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -1767.909424\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -1779.169067\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -1639.596680\n",
      "    epoch          : 227\n",
      "    loss           : -1685.8386182124073\n",
      "    val_loss       : -1735.7098715387285\n",
      "    val_log_likelihood: 1768.8217651367188\n",
      "    val_log_marginal: 1736.5109145540744\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -1665.729980\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -1628.229492\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -1763.942627\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -1767.149780\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -1707.971924\n",
      "    epoch          : 228\n",
      "    loss           : -1651.7931524597773\n",
      "    val_loss       : -1690.6143435148522\n",
      "    val_log_likelihood: 1724.2447387695313\n",
      "    val_log_marginal: 1691.6705050323167\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -1951.039551\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -1647.353149\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -1614.569336\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -1735.559204\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -1724.425781\n",
      "    epoch          : 229\n",
      "    loss           : -1664.586548455871\n",
      "    val_loss       : -1696.8807256083935\n",
      "    val_log_likelihood: 1762.3638061523438\n",
      "    val_log_marginal: 1729.9976002137723\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -1668.683350\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -1609.631592\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -1638.479736\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -1619.265625\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -1726.942139\n",
      "    epoch          : 230\n",
      "    loss           : -1659.061009775294\n",
      "    val_loss       : -1593.5395657891409\n",
      "    val_log_likelihood: 1744.1848999023437\n",
      "    val_log_marginal: 1711.7942823905498\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -1671.215088\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -1765.955566\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -1729.660156\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -1734.723022\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -1089.626709\n",
      "    epoch          : 231\n",
      "    loss           : -1655.6271948483911\n",
      "    val_loss       : -1695.1885344456882\n",
      "    val_log_likelihood: 1760.3578002929687\n",
      "    val_log_marginal: 1727.81558807105\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -1672.547607\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -1667.743164\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -1725.519043\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -1630.579346\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -1629.053345\n",
      "    epoch          : 232\n",
      "    loss           : -1669.7465826355585\n",
      "    val_loss       : -1664.0863356014713\n",
      "    val_log_likelihood: 1763.6721557617188\n",
      "    val_log_marginal: 1731.1937192741782\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -1790.271240\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -1684.652100\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -1706.875610\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -1702.125488\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -1737.905273\n",
      "    epoch          : 233\n",
      "    loss           : -1694.8825224319307\n",
      "    val_loss       : -1637.7683394929395\n",
      "    val_log_likelihood: 1762.2360473632812\n",
      "    val_log_marginal: 1729.6013158595276\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -1953.364258\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -1637.326172\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -1418.180176\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -1775.215576\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -1649.191528\n",
      "    epoch          : 234\n",
      "    loss           : -1681.5634656849475\n",
      "    val_loss       : -1677.0793246721848\n",
      "    val_log_likelihood: 1768.7461059570312\n",
      "    val_log_marginal: 1736.1828694660217\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -1978.513428\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -1720.491333\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -1729.463989\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -1707.352295\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -1636.907715\n",
      "    epoch          : 235\n",
      "    loss           : -1676.9418087194463\n",
      "    val_loss       : -1634.338337819092\n",
      "    val_log_likelihood: 1760.0768188476563\n",
      "    val_log_marginal: 1727.537565960938\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -1969.282959\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -1788.265137\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -1774.050415\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -1705.335205\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -1710.749512\n",
      "    epoch          : 236\n",
      "    loss           : -1681.634875609143\n",
      "    val_loss       : -1633.6490456694737\n",
      "    val_log_likelihood: 1760.6225708007812\n",
      "    val_log_marginal: 1727.9350656164206\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -1969.765015\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -1674.207764\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -1630.641113\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -1974.912964\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -1636.265137\n",
      "    epoch          : 237\n",
      "    loss           : -1674.8960722366182\n",
      "    val_loss       : -1667.0773760503157\n",
      "    val_log_likelihood: 1762.5022094726562\n",
      "    val_log_marginal: 1729.872773519531\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -1974.183472\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -1797.694214\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -1729.622070\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -1957.791504\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -1733.946289\n",
      "    epoch          : 238\n",
      "    loss           : -1689.864232431544\n",
      "    val_loss       : -1701.5658751023934\n",
      "    val_log_likelihood: 1767.4860961914062\n",
      "    val_log_marginal: 1734.9071880351753\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -1964.332031\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -1702.163330\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -1685.822144\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -1698.678101\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -1726.964722\n",
      "    epoch          : 239\n",
      "    loss           : -1676.0457866404315\n",
      "    val_loss       : -1733.0480942759664\n",
      "    val_log_likelihood: 1766.17529296875\n",
      "    val_log_marginal: 1733.6154408440245\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -1350.547974\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -1794.788330\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -1691.715454\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -1335.932007\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -1336.329956\n",
      "    epoch          : 240\n",
      "    loss           : -1670.5624589070235\n",
      "    val_loss       : -1643.8746682005003\n",
      "    val_log_likelihood: 1769.56875\n",
      "    val_log_marginal: 1737.0597476728065\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -1632.505493\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -1639.775024\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -1690.734497\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -1744.473633\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -1338.935547\n",
      "    epoch          : 241\n",
      "    loss           : -1687.5214747060643\n",
      "    val_loss       : -1677.7429616931827\n",
      "    val_log_likelihood: 1774.3483764648438\n",
      "    val_log_marginal: 1741.9649469777876\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -1702.650635\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -1786.512207\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -1763.848877\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -1737.244873\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -1613.351440\n",
      "    epoch          : 242\n",
      "    loss           : -1671.480879679765\n",
      "    val_loss       : -1727.4661751149222\n",
      "    val_log_likelihood: 1760.7998168945312\n",
      "    val_log_marginal: 1728.0315371278673\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -1362.580322\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -1737.093750\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -1741.181885\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -1972.132812\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -1737.424927\n",
      "    epoch          : 243\n",
      "    loss           : -1682.7490887028157\n",
      "    val_loss       : -1681.309084880352\n",
      "    val_log_likelihood: 1771.6852905273438\n",
      "    val_log_marginal: 1739.0154562676375\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -1967.157471\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -1681.393311\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -1743.193115\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -1624.683960\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -1639.559204\n",
      "    epoch          : 244\n",
      "    loss           : -1673.4374830793627\n",
      "    val_loss       : -1614.1295101130381\n",
      "    val_log_likelihood: 1770.0843139648437\n",
      "    val_log_marginal: 1737.4477174196393\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -1969.862183\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -1684.108154\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -1635.301270\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -1432.792603\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -1637.451050\n",
      "    epoch          : 245\n",
      "    loss           : -1680.5238538685412\n",
      "    val_loss       : -1675.1386033706367\n",
      "    val_log_likelihood: 1775.697802734375\n",
      "    val_log_marginal: 1743.0285573061556\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -1688.544800\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -1675.420654\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -1647.305908\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -1962.727295\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -1637.430176\n",
      "    epoch          : 246\n",
      "    loss           : -1697.286594504177\n",
      "    val_loss       : -1675.7468096021562\n",
      "    val_log_likelihood: 1778.4484008789063\n",
      "    val_log_marginal: 1745.7649415496737\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -1781.693726\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -1817.912964\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -1754.713013\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -1608.570190\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -1603.172852\n",
      "    epoch          : 247\n",
      "    loss           : -1680.2693620436262\n",
      "    val_loss       : -1663.491638757661\n",
      "    val_log_likelihood: 1758.71435546875\n",
      "    val_log_marginal: 1725.8422650646419\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -1785.517334\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -1675.942261\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -1623.189575\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -1625.932373\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -1727.492188\n",
      "    epoch          : 248\n",
      "    loss           : -1665.83094349474\n",
      "    val_loss       : -1673.2944821203128\n",
      "    val_log_likelihood: 1761.2559448242187\n",
      "    val_log_marginal: 1728.432693495229\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -1960.281128\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -1790.982178\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -1478.368408\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -1706.996582\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -1715.201172\n",
      "    epoch          : 249\n",
      "    loss           : -1675.441306539101\n",
      "    val_loss       : -1702.1095669588074\n",
      "    val_log_likelihood: 1774.3081787109375\n",
      "    val_log_marginal: 1741.4939194541425\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -1977.534546\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -1501.621948\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -1638.884644\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -1436.289307\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -1627.411499\n",
      "    epoch          : 250\n",
      "    loss           : -1671.5113005685334\n",
      "    val_loss       : -1674.1374142110349\n",
      "    val_log_likelihood: 1770.492431640625\n",
      "    val_log_marginal: 1737.6407575808212\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -1727.237061\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -1737.798584\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -1635.668701\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -1315.568237\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -1714.598999\n",
      "    epoch          : 251\n",
      "    loss           : -1640.191705987005\n",
      "    val_loss       : -1637.1256685281173\n",
      "    val_log_likelihood: 1765.8649291992188\n",
      "    val_log_marginal: 1733.0214283499868\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -1969.155029\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -1689.581421\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -1636.646240\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -1688.076172\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -1353.320557\n",
      "    epoch          : 252\n",
      "    loss           : -1686.9162911896658\n",
      "    val_loss       : -1702.5595594486222\n",
      "    val_log_likelihood: 1772.520751953125\n",
      "    val_log_marginal: 1739.5399113092571\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -1511.156860\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -1809.067871\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -1644.431274\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -1436.553345\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -1730.236328\n",
      "    epoch          : 253\n",
      "    loss           : -1687.7848480526764\n",
      "    val_loss       : -1712.4356828587129\n",
      "    val_log_likelihood: 1769.5919067382813\n",
      "    val_log_marginal: 1736.7573995303362\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -1801.552979\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -1683.594360\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -1443.666504\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -1174.075439\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -1464.030762\n",
      "    epoch          : 254\n",
      "    loss           : -1688.8828608446781\n",
      "    val_loss       : -1658.6481271006166\n",
      "    val_log_likelihood: 1772.741845703125\n",
      "    val_log_marginal: 1739.9314346123488\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -1738.936157\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -1642.826660\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -1779.952637\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -1464.420654\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -1749.151611\n",
      "    epoch          : 255\n",
      "    loss           : -1699.5997544090346\n",
      "    val_loss       : -1621.343821949698\n",
      "    val_log_likelihood: 1776.6880004882812\n",
      "    val_log_marginal: 1743.782058904247\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -1713.507812\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -1406.166016\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -1634.020386\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -1963.731934\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -1638.285889\n",
      "    epoch          : 256\n",
      "    loss           : -1693.965493985922\n",
      "    val_loss       : -1679.17412269637\n",
      "    val_log_likelihood: 1765.2987548828125\n",
      "    val_log_marginal: 1732.1971945080907\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -1800.122559\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -1663.188843\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -1689.011719\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -1725.324951\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -1609.195312\n",
      "    epoch          : 257\n",
      "    loss           : -1688.2059579981435\n",
      "    val_loss       : -1619.2835464805364\n",
      "    val_log_likelihood: 1741.070849609375\n",
      "    val_log_marginal: 1708.100198950991\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -1732.465942\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -1774.521484\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -1618.517822\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -1708.566895\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -1721.314331\n",
      "    epoch          : 258\n",
      "    loss           : -1653.1102125715502\n",
      "    val_loss       : -1673.7368476485833\n",
      "    val_log_likelihood: 1757.6009155273437\n",
      "    val_log_marginal: 1724.591175749468\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -1757.085083\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -1812.874634\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -1719.060059\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -1721.126587\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -1722.125244\n",
      "    epoch          : 259\n",
      "    loss           : -1686.7480420405323\n",
      "    val_loss       : -1730.7483513152226\n",
      "    val_log_likelihood: 1764.4104614257812\n",
      "    val_log_marginal: 1731.2768117459368\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -1744.507080\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -1640.907471\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -1313.189087\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -1735.333984\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -1692.744629\n",
      "    epoch          : 260\n",
      "    loss           : -1685.572643922107\n",
      "    val_loss       : -1614.4677746435627\n",
      "    val_log_likelihood: 1752.8105224609376\n",
      "    val_log_marginal: 1719.714750596133\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -1738.043335\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -1799.113892\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -1645.706421\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -1703.451660\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -1737.029785\n",
      "    epoch          : 261\n",
      "    loss           : -1685.8630032681003\n",
      "    val_loss       : -1742.836116214469\n",
      "    val_log_likelihood: 1776.2913696289063\n",
      "    val_log_marginal: 1743.2898985267232\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -1692.705933\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -1732.265503\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -1637.010620\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -1567.312134\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -1737.176270\n",
      "    epoch          : 262\n",
      "    loss           : -1666.2862198329208\n",
      "    val_loss       : -1731.8543546210974\n",
      "    val_log_likelihood: 1765.38603515625\n",
      "    val_log_marginal: 1732.2798403971974\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -1763.573364\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -1404.104614\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -1726.348755\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -1653.630371\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -1728.222046\n",
      "    epoch          : 263\n",
      "    loss           : -1693.0823201094524\n",
      "    val_loss       : -1675.332192093134\n",
      "    val_log_likelihood: 1771.4813598632813\n",
      "    val_log_marginal: 1738.4587264469635\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -1975.547607\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -1680.475586\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -1779.749756\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -1692.574341\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -1438.033569\n",
      "    epoch          : 264\n",
      "    loss           : -1674.7594888517172\n",
      "    val_loss       : -1606.6248812383042\n",
      "    val_log_likelihood: 1760.83212890625\n",
      "    val_log_marginal: 1727.599735460059\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -1764.443359\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -1688.896973\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -1643.968018\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -1643.855225\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -1631.450073\n",
      "    epoch          : 265\n",
      "    loss           : -1693.2460792465965\n",
      "    val_loss       : -1700.7516262203455\n",
      "    val_log_likelihood: 1760.8851318359375\n",
      "    val_log_marginal: 1727.7125764098123\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -1797.539062\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -1683.721924\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -1637.224365\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -1728.519653\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -1636.471191\n",
      "    epoch          : 266\n",
      "    loss           : -1699.2257684386602\n",
      "    val_loss       : -1708.6881638476625\n",
      "    val_log_likelihood: 1768.3440307617188\n",
      "    val_log_marginal: 1735.2045928072184\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -1972.922974\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -1684.290405\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -1709.680420\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -1709.761230\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -1734.964478\n",
      "    epoch          : 267\n",
      "    loss           : -1693.9977821878867\n",
      "    val_loss       : -1707.7406852968038\n",
      "    val_log_likelihood: 1777.4542602539063\n",
      "    val_log_marginal: 1744.3674062561245\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -1977.869141\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -1387.241821\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -1718.224487\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -1977.573730\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -1508.807983\n",
      "    epoch          : 268\n",
      "    loss           : -1709.5417915570854\n",
      "    val_loss       : -1705.0441571459173\n",
      "    val_log_likelihood: 1774.3651489257813\n",
      "    val_log_marginal: 1741.1917308073491\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -1974.796875\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -1359.520752\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -1714.797485\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -1708.132568\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -1697.070923\n",
      "    epoch          : 269\n",
      "    loss           : -1699.3569142558788\n",
      "    val_loss       : -1654.5533556165174\n",
      "    val_log_likelihood: 1779.7085083007812\n",
      "    val_log_marginal: 1746.5603661713635\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -1772.560791\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -1820.174805\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -1789.601196\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -1746.648315\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -1620.393066\n",
      "    epoch          : 270\n",
      "    loss           : -1704.980657294245\n",
      "    val_loss       : -1729.0063145088031\n",
      "    val_log_likelihood: 1763.1220581054688\n",
      "    val_log_marginal: 1729.7981162611395\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -1968.217407\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -1810.141602\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -1779.687500\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -1666.084106\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -1716.247070\n",
      "    epoch          : 271\n",
      "    loss           : -1701.1191454594677\n",
      "    val_loss       : -1713.3068774461747\n",
      "    val_log_likelihood: 1773.105810546875\n",
      "    val_log_marginal: 1739.8115725052098\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -1975.877197\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -1680.364136\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -1705.062134\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -1784.820190\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -1737.892944\n",
      "    epoch          : 272\n",
      "    loss           : -1700.4679414352568\n",
      "    val_loss       : -1691.1884569600224\n",
      "    val_log_likelihood: 1781.2287841796874\n",
      "    val_log_marginal: 1748.0528481390327\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -1822.158691\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -1813.941406\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -1355.303711\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -1628.294678\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -1664.850708\n",
      "    epoch          : 273\n",
      "    loss           : -1663.1220014213336\n",
      "    val_loss       : -1699.5566059950738\n",
      "    val_log_likelihood: 1762.1271118164063\n",
      "    val_log_marginal: 1728.9230094518512\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -1973.498535\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -1680.027832\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -1689.861084\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -1357.665405\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -1732.440186\n",
      "    epoch          : 274\n",
      "    loss           : -1694.7970333288213\n",
      "    val_loss       : -1703.9133225476369\n",
      "    val_log_likelihood: 1765.2422973632813\n",
      "    val_log_marginal: 1732.103139966343\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -1964.234375\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -1544.271118\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -1423.953125\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -1351.073975\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -1740.375366\n",
      "    epoch          : 275\n",
      "    loss           : -1677.5555371577198\n",
      "    val_loss       : -1743.6850170850753\n",
      "    val_log_likelihood: 1777.1070068359375\n",
      "    val_log_marginal: 1744.087130235508\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -1974.552856\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -1815.627686\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -1738.989014\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -1720.154541\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -1735.363647\n",
      "    epoch          : 276\n",
      "    loss           : -1695.0601184202892\n",
      "    val_loss       : -1714.9717542113735\n",
      "    val_log_likelihood: 1777.1687622070312\n",
      "    val_log_marginal: 1744.2343305032568\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -1679.783447\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -1823.628540\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -981.586304\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -1772.569824\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -1748.742920\n",
      "    epoch          : 277\n",
      "    loss           : -1685.0129068204672\n",
      "    val_loss       : -1742.487960341759\n",
      "    val_log_likelihood: 1776.03916015625\n",
      "    val_log_marginal: 1743.1373664394384\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -1974.835449\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -1812.858032\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -1502.633545\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -1782.584351\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -1640.462402\n",
      "    epoch          : 278\n",
      "    loss           : -1678.9145229830601\n",
      "    val_loss       : -1664.6707382602617\n",
      "    val_log_likelihood: 1776.3239013671875\n",
      "    val_log_marginal: 1743.314417962411\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -1975.381470\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -1691.872681\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -1785.445679\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -1972.980347\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -1738.998535\n",
      "    epoch          : 279\n",
      "    loss           : -1704.4045434328589\n",
      "    val_loss       : -1705.0054554617031\n",
      "    val_log_likelihood: 1765.418603515625\n",
      "    val_log_marginal: 1732.5825916472822\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -1974.389160\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -1650.617920\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -1443.614014\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -1775.020142\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -1704.202759\n",
      "    epoch          : 280\n",
      "    loss           : -1678.5854129602412\n",
      "    val_loss       : -1678.5379091564566\n",
      "    val_log_likelihood: 1741.0171875\n",
      "    val_log_marginal: 1707.7979684289544\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -1946.126831\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -1670.022461\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -1349.040894\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -1727.065186\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -1454.830322\n",
      "    epoch          : 281\n",
      "    loss           : -1692.556258702042\n",
      "    val_loss       : -1739.5283356577158\n",
      "    val_log_likelihood: 1773.45009765625\n",
      "    val_log_marginal: 1740.3559106703847\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -1772.963379\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -1673.833374\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -1612.276489\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -1719.651855\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -1458.044312\n",
      "    epoch          : 282\n",
      "    loss           : -1682.0845620939047\n",
      "    val_loss       : -1691.1425156071782\n",
      "    val_log_likelihood: 1780.3817260742187\n",
      "    val_log_marginal: 1747.3152062345296\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -1976.106567\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -1665.239868\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -1715.748047\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -1719.423706\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -1737.941162\n",
      "    epoch          : 283\n",
      "    loss           : -1684.9575860051825\n",
      "    val_loss       : -1718.7471918681636\n",
      "    val_log_likelihood: 1779.4544067382812\n",
      "    val_log_marginal: 1746.2794080820106\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -1676.385620\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -1681.257568\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -1746.098877\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -1744.222046\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -1746.290527\n",
      "    epoch          : 284\n",
      "    loss           : -1713.6178727858137\n",
      "    val_loss       : -1721.2567040145398\n",
      "    val_log_likelihood: 1783.74609375\n",
      "    val_log_marginal: 1750.6971255693436\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -1980.954834\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -1660.651733\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -1725.971924\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -1793.127075\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -1748.911743\n",
      "    epoch          : 285\n",
      "    loss           : -1681.9847339592357\n",
      "    val_loss       : -1727.3015030678362\n",
      "    val_log_likelihood: 1787.490625\n",
      "    val_log_marginal: 1754.50102086775\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -1775.964844\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -1407.522827\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -1649.757446\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -1650.482910\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -1347.222656\n",
      "    epoch          : 286\n",
      "    loss           : -1688.1956255317914\n",
      "    val_loss       : -1710.4957706892862\n",
      "    val_log_likelihood: 1772.1512084960937\n",
      "    val_log_marginal: 1739.2333505246788\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -1969.139526\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -1806.372192\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -1741.364868\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -1754.886230\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -1738.937500\n",
      "    epoch          : 287\n",
      "    loss           : -1688.9428650506652\n",
      "    val_loss       : -1679.4060598932206\n",
      "    val_log_likelihood: 1781.5494018554687\n",
      "    val_log_marginal: 1748.5093901255932\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -1979.134277\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -1805.736816\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -1626.646240\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -1700.872192\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -1636.785522\n",
      "    epoch          : 288\n",
      "    loss           : -1674.1680195119122\n",
      "    val_loss       : -1689.0830352067946\n",
      "    val_log_likelihood: 1773.4025512695312\n",
      "    val_log_marginal: 1740.4543710695993\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -1762.431274\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -1661.472412\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -1636.245483\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -1698.077637\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -1729.200806\n",
      "    epoch          : 289\n",
      "    loss           : -1663.375676825495\n",
      "    val_loss       : -1702.1988661980256\n",
      "    val_log_likelihood: 1765.6428466796874\n",
      "    val_log_marginal: 1732.6405656445772\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -1555.646362\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -1712.462036\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -1687.290039\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -1422.352417\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -1356.906372\n",
      "    epoch          : 290\n",
      "    loss           : -1652.124375145034\n",
      "    val_loss       : -1665.7767995402216\n",
      "    val_log_likelihood: 1751.9426635742188\n",
      "    val_log_marginal: 1718.7351796446248\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -1769.508789\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -1790.071167\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -1616.191528\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -1971.734741\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -1717.201172\n",
      "    epoch          : 291\n",
      "    loss           : -1667.2144485322556\n",
      "    val_loss       : -1701.8378310305998\n",
      "    val_log_likelihood: 1764.378125\n",
      "    val_log_marginal: 1731.2404996898026\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -1972.845947\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -1673.327026\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -1663.651123\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -1778.443726\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -1738.408569\n",
      "    epoch          : 292\n",
      "    loss           : -1681.632462001083\n",
      "    val_loss       : -1686.1948744274675\n",
      "    val_log_likelihood: 1773.9677368164062\n",
      "    val_log_marginal: 1740.9603142719716\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -1976.694946\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -1073.719727\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -1369.442871\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -1714.315552\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -1650.775391\n",
      "    epoch          : 293\n",
      "    loss           : -1674.5134458636294\n",
      "    val_loss       : -1712.4824676334858\n",
      "    val_log_likelihood: 1777.4163696289063\n",
      "    val_log_marginal: 1744.3613050040033\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -1982.145264\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -1687.225098\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -1648.095459\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -1740.444336\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -1745.564209\n",
      "    epoch          : 294\n",
      "    loss           : -1707.1465701868037\n",
      "    val_loss       : -1626.5595089245587\n",
      "    val_log_likelihood: 1771.5305053710938\n",
      "    val_log_marginal: 1738.407430017367\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -1984.467529\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -1815.158325\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -1743.235596\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -1340.269775\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -1755.250000\n",
      "    epoch          : 295\n",
      "    loss           : -1681.611301535427\n",
      "    val_loss       : -1699.3385932769627\n",
      "    val_log_likelihood: 1783.0359985351563\n",
      "    val_log_marginal: 1750.0368835903275\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -1977.203491\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -1811.064453\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -1659.968018\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -1983.351807\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -1734.799927\n",
      "    epoch          : 296\n",
      "    loss           : -1677.8785611898593\n",
      "    val_loss       : -1693.6267783207818\n",
      "    val_log_likelihood: 1777.8892700195313\n",
      "    val_log_marginal: 1745.013070415333\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -1976.271851\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -1736.029419\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -1342.954346\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -1736.627808\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -1683.106934\n",
      "    epoch          : 297\n",
      "    loss           : -1692.7401364770267\n",
      "    val_loss       : -1700.398458910547\n",
      "    val_log_likelihood: 1764.6612182617187\n",
      "    val_log_marginal: 1731.8221442744277\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -1759.772095\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -1690.273926\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -1347.611084\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -1972.885132\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -1436.721802\n",
      "    epoch          : 298\n",
      "    loss           : -1689.7215249845297\n",
      "    val_loss       : -1643.9663689916954\n",
      "    val_log_likelihood: 1773.9207763671875\n",
      "    val_log_marginal: 1741.1047384638339\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -1761.211670\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -1689.605713\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -1779.107422\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -1735.873047\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -1747.426514\n",
      "    epoch          : 299\n",
      "    loss           : -1690.680748665687\n",
      "    val_loss       : -1688.1468457212673\n",
      "    val_log_likelihood: 1781.3064697265625\n",
      "    val_log_marginal: 1748.5321362277289\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -1378.441040\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -1399.048218\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -1791.002930\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -1748.082031\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -1353.316406\n",
      "    epoch          : 300\n",
      "    loss           : -1682.370528117265\n",
      "    val_loss       : -1722.6926930421964\n",
      "    val_log_likelihood: 1785.3892578125\n",
      "    val_log_marginal: 1752.5972341850706\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -1983.576904\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -1370.093994\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -1711.072510\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -1747.642944\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -1749.396240\n",
      "    epoch          : 301\n",
      "    loss           : -1680.8680939627166\n",
      "    val_loss       : -1691.6653201829643\n",
      "    val_log_likelihood: 1783.397900390625\n",
      "    val_log_marginal: 1750.6946622993796\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -1964.146851\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -1693.177368\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -1788.417114\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -1961.145630\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -1741.851318\n",
      "    epoch          : 302\n",
      "    loss           : -1723.5927226755878\n",
      "    val_loss       : -1617.9828744627534\n",
      "    val_log_likelihood: 1766.8791870117188\n",
      "    val_log_marginal: 1734.1594938691705\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -1557.732178\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -1636.256348\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -1780.509277\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -1964.702148\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -1746.546997\n",
      "    epoch          : 303\n",
      "    loss           : -1696.4155684367265\n",
      "    val_loss       : -1634.1465706579388\n",
      "    val_log_likelihood: 1776.2226440429688\n",
      "    val_log_marginal: 1743.4484729599208\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -1976.559814\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -1707.309448\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -1644.896484\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -1792.336304\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -1643.556396\n",
      "    epoch          : 304\n",
      "    loss           : -1710.492705996674\n",
      "    val_loss       : -1719.5509138602763\n",
      "    val_log_likelihood: 1780.740185546875\n",
      "    val_log_marginal: 1748.0364576103482\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -1974.062622\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -1822.001953\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -1794.317871\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -1754.421631\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -1751.755737\n",
      "    epoch          : 305\n",
      "    loss           : -1707.2231517829518\n",
      "    val_loss       : -1642.2762730825693\n",
      "    val_log_likelihood: 1786.9772827148438\n",
      "    val_log_marginal: 1754.3004007467364\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -1982.208740\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -1702.521240\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -1659.359863\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -1747.807861\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -1655.747559\n",
      "    epoch          : 306\n",
      "    loss           : -1713.9636701829363\n",
      "    val_loss       : -1726.207713453658\n",
      "    val_log_likelihood: 1788.086083984375\n",
      "    val_log_marginal: 1755.2106845710427\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -1983.169800\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -1818.738647\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -1398.751099\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -1794.878052\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -1734.995361\n",
      "    epoch          : 307\n",
      "    loss           : -1699.0013693630106\n",
      "    val_loss       : -1727.901476140134\n",
      "    val_log_likelihood: 1787.264501953125\n",
      "    val_log_marginal: 1754.4207546625425\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -1605.868530\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -1822.924072\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -1662.362305\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -1714.718018\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -1452.959717\n",
      "    epoch          : 308\n",
      "    loss           : -1692.1109715829982\n",
      "    val_loss       : -1576.6440428517758\n",
      "    val_log_likelihood: 1777.91650390625\n",
      "    val_log_marginal: 1745.0869241904468\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -1971.444458\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -1421.976929\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -1716.619995\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -1468.293335\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -1721.796021\n",
      "    epoch          : 309\n",
      "    loss           : -1703.1830160310953\n",
      "    val_loss       : -1690.3886214204133\n",
      "    val_log_likelihood: 1785.366943359375\n",
      "    val_log_marginal: 1752.5850343402476\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -1965.183838\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -1819.308105\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -1653.778931\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -1726.301636\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -1664.156738\n",
      "    epoch          : 310\n",
      "    loss           : -1709.333116588026\n",
      "    val_loss       : -1732.434304956533\n",
      "    val_log_likelihood: 1790.7714965820312\n",
      "    val_log_marginal: 1758.1191602434963\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -1800.914307\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -1824.142090\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -1668.134277\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -1380.002686\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -1447.227417\n",
      "    epoch          : 311\n",
      "    loss           : -1682.6194645343442\n",
      "    val_loss       : -1716.8808826813474\n",
      "    val_log_likelihood: 1768.843994140625\n",
      "    val_log_marginal: 1735.9049805659802\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -1811.504395\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -1703.376465\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -1788.368164\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -1746.783325\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -1354.677246\n",
      "    epoch          : 312\n",
      "    loss           : -1703.6358255820699\n",
      "    val_loss       : -1693.4336723057554\n",
      "    val_log_likelihood: 1777.8903442382812\n",
      "    val_log_marginal: 1745.0811763421775\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -1974.218018\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -1746.251465\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -1639.289429\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -1689.033447\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -1654.442383\n",
      "    epoch          : 313\n",
      "    loss           : -1695.4344675800587\n",
      "    val_loss       : -1707.4122820658608\n",
      "    val_log_likelihood: 1769.3124877929688\n",
      "    val_log_marginal: 1736.5056456871803\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -1966.337524\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -1684.829468\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -1781.686768\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -1738.660400\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -1645.308472\n",
      "    epoch          : 314\n",
      "    loss           : -1702.1466414952042\n",
      "    val_loss       : -1697.7169102910907\n",
      "    val_log_likelihood: 1781.1531372070312\n",
      "    val_log_marginal: 1748.299919671938\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -1972.253174\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -1783.854736\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -1371.598633\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -1657.493164\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -1650.991699\n",
      "    epoch          : 315\n",
      "    loss           : -1677.4547409208694\n",
      "    val_loss       : -1750.9748544609174\n",
      "    val_log_likelihood: 1784.33291015625\n",
      "    val_log_marginal: 1751.4785210844136\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -1826.263184\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -1680.403320\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -1719.155762\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -1358.236084\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -1751.914429\n",
      "    epoch          : 316\n",
      "    loss           : -1706.6220509746288\n",
      "    val_loss       : -1662.832850509137\n",
      "    val_log_likelihood: 1786.51845703125\n",
      "    val_log_marginal: 1753.7296109747142\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -1978.067627\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -1825.416626\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -1787.593262\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -1754.254639\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -1646.880127\n",
      "    epoch          : 317\n",
      "    loss           : -1697.8530116317295\n",
      "    val_loss       : -1633.2411298882216\n",
      "    val_log_likelihood: 1786.0971313476562\n",
      "    val_log_marginal: 1753.2674041446298\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -1776.899902\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -1696.479492\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -1717.907593\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -1742.128174\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -1745.392090\n",
      "    epoch          : 318\n",
      "    loss           : -1707.2627436571781\n",
      "    val_loss       : -1730.3491571543739\n",
      "    val_log_likelihood: 1787.9048095703124\n",
      "    val_log_marginal: 1755.1000172834842\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -1980.511353\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -1827.220581\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -1083.464722\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -1963.818359\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -1374.457397\n",
      "    epoch          : 319\n",
      "    loss           : -1664.187348318572\n",
      "    val_loss       : -1699.928444766067\n",
      "    val_log_likelihood: 1767.5436157226563\n",
      "    val_log_marginal: 1734.7691504307538\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -1974.666138\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -1677.202881\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -1355.905029\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -1734.136597\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -1729.209351\n",
      "    epoch          : 320\n",
      "    loss           : -1701.485654925356\n",
      "    val_loss       : -1669.4309707934037\n",
      "    val_log_likelihood: 1780.2881103515624\n",
      "    val_log_marginal: 1747.5688485424967\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -1981.337280\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -1700.782349\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -1741.830688\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -1723.378418\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -1657.410034\n",
      "    epoch          : 321\n",
      "    loss           : -1714.1993776831296\n",
      "    val_loss       : -1709.7235797448084\n",
      "    val_log_likelihood: 1789.5107421875\n",
      "    val_log_marginal: 1756.849261876568\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -1984.675781\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -1663.788086\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -1791.131592\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -1973.795532\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -1745.278320\n",
      "    epoch          : 322\n",
      "    loss           : -1692.6676533009747\n",
      "    val_loss       : -1737.0103053878993\n",
      "    val_log_likelihood: 1789.268896484375\n",
      "    val_log_marginal: 1756.6375675836784\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -1979.212646\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -1692.026367\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -1783.168701\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -1725.125488\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -1645.505127\n",
      "    epoch          : 323\n",
      "    loss           : -1703.8864250560798\n",
      "    val_loss       : -1724.8717504961417\n",
      "    val_log_likelihood: 1785.6566772460938\n",
      "    val_log_marginal: 1752.9268010478468\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -1971.961670\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -1431.037354\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -1661.920410\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -1658.875122\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -1748.018311\n",
      "    epoch          : 324\n",
      "    loss           : -1686.82221002862\n",
      "    val_loss       : -1668.8708534086122\n",
      "    val_log_likelihood: 1780.944677734375\n",
      "    val_log_marginal: 1748.3304473079472\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -1974.430664\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -1611.129517\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -1313.815796\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -1711.746582\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -1625.448486\n",
      "    epoch          : 325\n",
      "    loss           : -1648.4531407120205\n",
      "    val_loss       : -1716.6376487134025\n",
      "    val_log_likelihood: 1750.2575317382812\n",
      "    val_log_marginal: 1717.2540584150702\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -1960.916260\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -1638.774292\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -1299.689819\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -1643.391602\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -1726.145508\n",
      "    epoch          : 326\n",
      "    loss           : -1696.2464091990253\n",
      "    val_loss       : -1718.7749723628162\n",
      "    val_log_likelihood: 1773.8023681640625\n",
      "    val_log_marginal: 1740.869571330771\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -1333.067505\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -1690.560669\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -1770.779175\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -1654.176025\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -1724.756714\n",
      "    epoch          : 327\n",
      "    loss           : -1704.444658638227\n",
      "    val_loss       : -1727.0747651323677\n",
      "    val_log_likelihood: 1788.0625244140624\n",
      "    val_log_marginal: 1755.2538576949387\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -1976.023926\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -1492.951172\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -1759.202515\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -1357.460693\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -1643.938721\n",
      "    epoch          : 328\n",
      "    loss           : -1700.4516734510364\n",
      "    val_loss       : -1719.7136622540652\n",
      "    val_log_likelihood: 1779.846826171875\n",
      "    val_log_marginal: 1747.0587890785187\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -1975.802979\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -1437.939819\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -1790.941162\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -1654.039062\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -1755.698486\n",
      "    epoch          : 329\n",
      "    loss           : -1716.9601410214264\n",
      "    val_loss       : -1693.9560474418104\n",
      "    val_log_likelihood: 1789.78603515625\n",
      "    val_log_marginal: 1757.0212857944712\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -1775.116211\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -1446.049561\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -1375.386841\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -1969.815674\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -1640.680298\n",
      "    epoch          : 330\n",
      "    loss           : -1683.4186359065593\n",
      "    val_loss       : -1689.9319803001358\n",
      "    val_log_likelihood: 1780.9100463867187\n",
      "    val_log_marginal: 1747.9787808809428\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -1787.179932\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -1707.371582\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -1710.979858\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -1717.200439\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -1730.590698\n",
      "    epoch          : 331\n",
      "    loss           : -1677.3470471070545\n",
      "    val_loss       : -1719.3513968769462\n",
      "    val_log_likelihood: 1776.7980102539063\n",
      "    val_log_marginal: 1744.0474363740534\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -1968.748657\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -1764.337280\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -1310.421021\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -1740.481934\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -1770.932251\n",
      "    epoch          : 332\n",
      "    loss           : -1698.6636950804455\n",
      "    val_loss       : -1652.529002332501\n",
      "    val_log_likelihood: 1770.306640625\n",
      "    val_log_marginal: 1737.3314958178962\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -1829.008545\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -1694.511597\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -1629.818115\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -1648.903320\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -1781.529907\n",
      "    epoch          : 333\n",
      "    loss           : -1691.7429561803838\n",
      "    val_loss       : -1636.50249880217\n",
      "    val_log_likelihood: 1767.2124755859375\n",
      "    val_log_marginal: 1734.1534465853292\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -1791.158447\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -1438.535034\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -1789.542603\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -1654.404541\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -1657.223145\n",
      "    epoch          : 334\n",
      "    loss           : -1691.2251411664604\n",
      "    val_loss       : -1703.2368894344195\n",
      "    val_log_likelihood: 1781.8484008789062\n",
      "    val_log_marginal: 1748.8588736284523\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -1790.483887\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -1742.896851\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -1788.302002\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -1745.355835\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -1706.739990\n",
      "    epoch          : 335\n",
      "    loss           : -1688.471573329208\n",
      "    val_loss       : -1689.0172568272799\n",
      "    val_log_likelihood: 1781.4259033203125\n",
      "    val_log_marginal: 1748.5013713759668\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -1980.692017\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -1817.544678\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -1703.545776\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -1365.045166\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -1653.347412\n",
      "    epoch          : 336\n",
      "    loss           : -1682.2843899868503\n",
      "    val_loss       : -1692.1760069677607\n",
      "    val_log_likelihood: 1780.218212890625\n",
      "    val_log_marginal: 1747.365431617573\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -1973.861328\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -1703.806641\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -1652.502686\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -1775.457031\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -1399.842407\n",
      "    epoch          : 337\n",
      "    loss           : -1690.5115906366027\n",
      "    val_loss       : -1667.6981731612236\n",
      "    val_log_likelihood: 1773.9693969726563\n",
      "    val_log_marginal: 1740.808268845894\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -1971.278809\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -1440.814209\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -1791.293335\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -1650.888916\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -1469.431396\n",
      "    epoch          : 338\n",
      "    loss           : -1689.2534892771503\n",
      "    val_loss       : -1632.3029412716628\n",
      "    val_log_likelihood: 1786.5705444335938\n",
      "    val_log_marginal: 1753.577972904465\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -1701.424561\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -1825.435303\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -1493.387695\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -1652.864258\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -1401.478516\n",
      "    epoch          : 339\n",
      "    loss           : -1693.8633453066986\n",
      "    val_loss       : -1657.8380121702328\n",
      "    val_log_likelihood: 1778.6532348632813\n",
      "    val_log_marginal: 1745.6303683083504\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -1821.453369\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -1689.976196\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -1796.006958\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -1659.483521\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -1471.935059\n",
      "    epoch          : 340\n",
      "    loss           : -1703.9792528813427\n",
      "    val_loss       : -1729.3560606639833\n",
      "    val_log_likelihood: 1763.1902099609374\n",
      "    val_log_marginal: 1730.0066265877335\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -1787.823364\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -1438.049561\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -1447.958984\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -1462.727051\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -1735.844849\n",
      "    epoch          : 341\n",
      "    loss           : -1689.7485448251857\n",
      "    val_loss       : -1669.2648873953149\n",
      "    val_log_likelihood: 1781.6423095703126\n",
      "    val_log_marginal: 1748.5395584433838\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -1778.842529\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -1828.226562\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -1795.794922\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -1380.985596\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -1745.040771\n",
      "    epoch          : 342\n",
      "    loss           : -1673.1020145227412\n",
      "    val_loss       : -1701.0899446438998\n",
      "    val_log_likelihood: 1789.6757446289062\n",
      "    val_log_marginal: 1756.7246193658561\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -1990.003296\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -1836.509644\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -1676.552246\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -1721.328735\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -1758.958618\n",
      "    epoch          : 343\n",
      "    loss           : -1724.3663160871752\n",
      "    val_loss       : -1709.0070894246921\n",
      "    val_log_likelihood: 1793.9122314453125\n",
      "    val_log_marginal: 1760.8630121523013\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -1791.224731\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -1472.881836\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -1717.388306\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -1755.154663\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -1756.793579\n",
      "    epoch          : 344\n",
      "    loss           : -1688.1636310237468\n",
      "    val_loss       : -1728.7180445065721\n",
      "    val_log_likelihood: 1788.556494140625\n",
      "    val_log_marginal: 1755.6659139421295\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -1785.317871\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -1701.207031\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -1667.952393\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -1981.899658\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -1741.593994\n",
      "    epoch          : 345\n",
      "    loss           : -1727.4754433206992\n",
      "    val_loss       : -1756.795082431659\n",
      "    val_log_likelihood: 1790.045654296875\n",
      "    val_log_marginal: 1757.1284204301276\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -1979.912720\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -1705.048218\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -1730.358032\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -1707.473389\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -1718.253784\n",
      "    epoch          : 346\n",
      "    loss           : -1702.8324820641244\n",
      "    val_loss       : -1678.4308741714806\n",
      "    val_log_likelihood: 1785.1114135742187\n",
      "    val_log_marginal: 1752.3703453208236\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -1357.873657\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -1703.898682\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -1656.601440\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -1664.535645\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -1730.216553\n",
      "    epoch          : 347\n",
      "    loss           : -1708.6332645227412\n",
      "    val_loss       : -1625.0990884672851\n",
      "    val_log_likelihood: 1794.2290405273438\n",
      "    val_log_marginal: 1761.4692533022146\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -1980.680542\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -1667.743408\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -1705.871216\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -1750.579346\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -1764.928955\n",
      "    epoch          : 348\n",
      "    loss           : -1702.7817165261447\n",
      "    val_loss       : -1680.818448002264\n",
      "    val_log_likelihood: 1788.9510986328125\n",
      "    val_log_marginal: 1756.2444086689502\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -1964.511475\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -1666.615967\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -1740.569946\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -1703.001953\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -1658.643311\n",
      "    epoch          : 349\n",
      "    loss           : -1711.2211116375308\n",
      "    val_loss       : -1704.9132760381326\n",
      "    val_log_likelihood: 1791.6525146484375\n",
      "    val_log_marginal: 1758.8103191958185\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -1787.081787\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -1833.456421\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -1409.833008\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -1658.145264\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -1728.166260\n",
      "    epoch          : 350\n",
      "    loss           : -1700.1557169999226\n",
      "    val_loss       : -1694.3119083577767\n",
      "    val_log_likelihood: 1776.0408447265625\n",
      "    val_log_marginal: 1743.222049246356\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -1978.572998\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -1830.814941\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -1660.469482\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -1569.709229\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -1274.661377\n",
      "    epoch          : 351\n",
      "    loss           : -1694.604356822401\n",
      "    val_loss       : -1716.1828945407644\n",
      "    val_log_likelihood: 1791.9719848632812\n",
      "    val_log_marginal: 1759.1399536191705\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -1976.957031\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -1697.495483\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -1661.928467\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -1977.182373\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -1656.550293\n",
      "    epoch          : 352\n",
      "    loss           : -1721.7181372312036\n",
      "    val_loss       : -1698.4682912779972\n",
      "    val_log_likelihood: 1790.7773315429688\n",
      "    val_log_marginal: 1758.0581371698777\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -1984.543823\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -1831.994385\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -1728.225220\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -1559.016113\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -1754.161499\n",
      "    epoch          : 353\n",
      "    loss           : -1710.513131623221\n",
      "    val_loss       : -1687.2992973461746\n",
      "    val_log_likelihood: 1793.9328125\n",
      "    val_log_marginal: 1761.1554574470968\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -1975.084473\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -1675.555420\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -1465.584961\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -1701.089233\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -1735.207153\n",
      "    epoch          : 354\n",
      "    loss           : -1675.0704357789295\n",
      "    val_loss       : -1626.8075577737764\n",
      "    val_log_likelihood: 1759.9487548828124\n",
      "    val_log_marginal: 1726.6157723706215\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -1970.673462\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -1689.031006\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -1651.526611\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -1396.875977\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -1634.978027\n",
      "    epoch          : 355\n",
      "    loss           : -1672.774931350557\n",
      "    val_loss       : -1724.008822579123\n",
      "    val_log_likelihood: 1780.01123046875\n",
      "    val_log_marginal: 1746.8894803577314\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -1976.942017\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -1692.451416\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -1756.209473\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -1742.813477\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -1649.327271\n",
      "    epoch          : 356\n",
      "    loss           : -1722.9143743231746\n",
      "    val_loss       : -1674.3533898685127\n",
      "    val_log_likelihood: 1788.37529296875\n",
      "    val_log_marginal: 1755.3157270569354\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -1982.996338\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -1823.549561\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -1656.522461\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -1518.082520\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -1746.708008\n",
      "    epoch          : 357\n",
      "    loss           : -1702.3276439704518\n",
      "    val_loss       : -1683.9809798391536\n",
      "    val_log_likelihood: 1789.3947875976562\n",
      "    val_log_marginal: 1756.341356497732\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -1800.737793\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -1701.137573\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -1481.673828\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -1795.607666\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -1651.635986\n",
      "    epoch          : 358\n",
      "    loss           : -1712.7885693842823\n",
      "    val_loss       : -1684.2258271524684\n",
      "    val_log_likelihood: 1790.5167846679688\n",
      "    val_log_marginal: 1757.4719751317061\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -1809.509277\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -1672.070801\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -1547.946045\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -1791.177612\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -1656.361206\n",
      "    epoch          : 359\n",
      "    loss           : -1711.028429088026\n",
      "    val_loss       : -1756.571554609202\n",
      "    val_log_likelihood: 1790.1921997070312\n",
      "    val_log_marginal: 1757.2008945935854\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -1788.462036\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -1832.710205\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -1483.460205\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -1482.269409\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -1756.717163\n",
      "    epoch          : 360\n",
      "    loss           : -1717.1246035736385\n",
      "    val_loss       : -1716.0703226255253\n",
      "    val_log_likelihood: 1793.6836059570312\n",
      "    val_log_marginal: 1760.7541347440333\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -1835.936768\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -1400.449951\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -1402.919556\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -1797.340210\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -1663.687378\n",
      "    epoch          : 361\n",
      "    loss           : -1730.0000362585088\n",
      "    val_loss       : -1680.7943856457248\n",
      "    val_log_likelihood: 1788.7781372070312\n",
      "    val_log_marginal: 1755.7432979900627\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -1835.348877\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -1678.415894\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -1725.041992\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -1649.816528\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -1730.856323\n",
      "    epoch          : 362\n",
      "    loss           : -1695.9914151937655\n",
      "    val_loss       : -1684.4247374990955\n",
      "    val_log_likelihood: 1769.0664306640624\n",
      "    val_log_marginal: 1736.0400897923857\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -1953.041992\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -1427.502197\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -1787.583740\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -1714.084839\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -1744.083984\n",
      "    epoch          : 363\n",
      "    loss           : -1697.3711626411664\n",
      "    val_loss       : -1679.7442557960749\n",
      "    val_log_likelihood: 1784.6951904296875\n",
      "    val_log_marginal: 1751.6943535355508\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -1975.132935\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -1691.772949\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -1461.028320\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -1780.988892\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -1722.184448\n",
      "    epoch          : 364\n",
      "    loss           : -1698.911783048422\n",
      "    val_loss       : -1705.8825387476013\n",
      "    val_log_likelihood: 1757.55732421875\n",
      "    val_log_marginal: 1724.0828098025172\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -1964.851074\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -1775.993530\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -1699.216797\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -1473.083862\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -1753.358398\n",
      "    epoch          : 365\n",
      "    loss           : -1689.822782913057\n",
      "    val_loss       : -1728.023559536971\n",
      "    val_log_likelihood: 1786.1815551757813\n",
      "    val_log_marginal: 1752.8988904205028\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -1981.021118\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -1819.484985\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -1476.633789\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -1670.304199\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -1742.609497\n",
      "    epoch          : 366\n",
      "    loss           : -1686.1655587677908\n",
      "    val_loss       : -1689.1345341343433\n",
      "    val_log_likelihood: 1787.3609497070313\n",
      "    val_log_marginal: 1754.0054358173165\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -1977.665527\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -1681.991089\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -1411.706299\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -1735.179688\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -1707.483765\n",
      "    epoch          : 367\n",
      "    loss           : -1697.8054525545328\n",
      "    val_loss       : -1713.8420241860672\n",
      "    val_log_likelihood: 1776.5449340820312\n",
      "    val_log_marginal: 1743.1843678613973\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -1966.921631\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -1817.920654\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -1787.580688\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -1749.114746\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -1746.784912\n",
      "    epoch          : 368\n",
      "    loss           : -1708.3995651396194\n",
      "    val_loss       : -1727.5149806490167\n",
      "    val_log_likelihood: 1787.0826171875\n",
      "    val_log_marginal: 1753.762938405946\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -1979.058350\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -1653.256836\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -1795.192139\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -1980.512573\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -1490.141846\n",
      "    epoch          : 369\n",
      "    loss           : -1694.7793911471226\n",
      "    val_loss       : -1674.0516306051984\n",
      "    val_log_likelihood: 1786.7743774414062\n",
      "    val_log_marginal: 1753.4725549179336\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -1801.019653\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -1511.806396\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -1752.259277\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -1800.054443\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -1755.191895\n",
      "    epoch          : 370\n",
      "    loss           : -1680.5786042166228\n",
      "    val_loss       : -1664.3345241528004\n",
      "    val_log_likelihood: 1792.3736694335937\n",
      "    val_log_marginal: 1759.180717884378\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -1806.009033\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -1746.267090\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -1753.936768\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -1803.480713\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -1428.032227\n",
      "    epoch          : 371\n",
      "    loss           : -1715.7183499477878\n",
      "    val_loss       : -1708.5770994925872\n",
      "    val_log_likelihood: 1791.089599609375\n",
      "    val_log_marginal: 1757.9313265473156\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -1973.075073\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -1447.497925\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -1666.743652\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -1978.733887\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -1724.011230\n",
      "    epoch          : 372\n",
      "    loss           : -1701.4718730662128\n",
      "    val_loss       : -1706.560500322655\n",
      "    val_log_likelihood: 1787.1967407226562\n",
      "    val_log_marginal: 1753.805567053052\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -1951.447510\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -1827.740234\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -1797.196777\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -1799.711670\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -1756.836304\n",
      "    epoch          : 373\n",
      "    loss           : -1720.7406888149753\n",
      "    val_loss       : -1706.2040678117423\n",
      "    val_log_likelihood: 1796.5335205078125\n",
      "    val_log_marginal: 1763.2349020339993\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -1979.638062\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -1704.228760\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -1669.971680\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -1586.229248\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -1497.811523\n",
      "    epoch          : 374\n",
      "    loss           : -1712.9132793162128\n",
      "    val_loss       : -1761.9163792802021\n",
      "    val_log_likelihood: 1795.5294311523437\n",
      "    val_log_marginal: 1762.282780493045\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -1977.755249\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -1706.337036\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -1669.264648\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -1814.187134\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -1753.249512\n",
      "    epoch          : 375\n",
      "    loss           : -1714.802139735458\n",
      "    val_loss       : -1742.9593369821087\n",
      "    val_log_likelihood: 1793.9972412109375\n",
      "    val_log_marginal: 1760.8521785784512\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -1973.818970\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -1835.123657\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -1723.195801\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -1760.454834\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -1762.092529\n",
      "    epoch          : 376\n",
      "    loss           : -1716.5968779006807\n",
      "    val_loss       : -1750.0486249351875\n",
      "    val_log_likelihood: 1784.0378295898438\n",
      "    val_log_marginal: 1750.7820421677147\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -1657.990234\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -1435.031006\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -1723.590332\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -1431.564331\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -1745.081543\n",
      "    epoch          : 377\n",
      "    loss           : -1687.8177592966815\n",
      "    val_loss       : -1757.3082692302764\n",
      "    val_log_likelihood: 1790.916357421875\n",
      "    val_log_marginal: 1757.8929546037136\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -1802.642700\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -1779.082031\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -1721.127686\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -1731.543701\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -1620.859741\n",
      "    epoch          : 378\n",
      "    loss           : -1675.0363854134437\n",
      "    val_loss       : -1682.4313053451478\n",
      "    val_log_likelihood: 1763.5694702148437\n",
      "    val_log_marginal: 1729.5151579058802\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -1877.662109\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -1454.933105\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -1788.820068\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -1649.865723\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -1719.186646\n",
      "    epoch          : 379\n",
      "    loss           : -1693.4710765876393\n",
      "    val_loss       : -1738.0729661399498\n",
      "    val_log_likelihood: 1772.5729736328126\n",
      "    val_log_marginal: 1738.6276522587984\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -1920.846924\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -1663.243652\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -1789.065186\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -1420.430786\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -1723.170166\n",
      "    epoch          : 380\n",
      "    loss           : -1690.112386873453\n",
      "    val_loss       : -1698.586393260956\n",
      "    val_log_likelihood: 1755.9720947265625\n",
      "    val_log_marginal: 1722.1897907253356\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -1916.866821\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -1622.438721\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -1435.332031\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -1724.144531\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -1477.339111\n",
      "    epoch          : 381\n",
      "    loss           : -1677.3378749129795\n",
      "    val_loss       : -1714.9738855756818\n",
      "    val_log_likelihood: 1775.4744506835937\n",
      "    val_log_marginal: 1741.7830306220912\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -1936.994751\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -1783.280151\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -1671.935059\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -1499.555054\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -1654.039917\n",
      "    epoch          : 382\n",
      "    loss           : -1699.6651949740872\n",
      "    val_loss       : -1634.111607753858\n",
      "    val_log_likelihood: 1745.274560546875\n",
      "    val_log_marginal: 1711.6275771426517\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -1911.372192\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -1673.698120\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -1621.401855\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -1731.732300\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -1619.931885\n",
      "    epoch          : 383\n",
      "    loss           : -1663.0210045540687\n",
      "    val_loss       : -1693.0863486494868\n",
      "    val_log_likelihood: 1776.6667358398438\n",
      "    val_log_marginal: 1743.009970906863\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -1949.684326\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -1357.027710\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -1735.981934\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -1610.155884\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -1703.529785\n",
      "    epoch          : 384\n",
      "    loss           : -1678.3418053353187\n",
      "    val_loss       : -1568.7797084746883\n",
      "    val_log_likelihood: 1736.1112548828125\n",
      "    val_log_marginal: 1702.1734020177275\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -1924.427856\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -1753.185547\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -1507.741455\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -1701.784058\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -1710.708252\n",
      "    epoch          : 385\n",
      "    loss           : -1653.1073904509592\n",
      "    val_loss       : -1720.8667088678108\n",
      "    val_log_likelihood: 1755.4651733398437\n",
      "    val_log_marginal: 1721.3961767245085\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -1940.431641\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -1646.541504\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -1698.347290\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -1767.415405\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -1600.248901\n",
      "    epoch          : 386\n",
      "    loss           : -1675.505941560953\n",
      "    val_loss       : -1725.6393360547722\n",
      "    val_log_likelihood: 1760.0075073242188\n",
      "    val_log_marginal: 1725.9823827665236\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -1926.715332\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -1795.300415\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -1720.948608\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -1768.158569\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -1742.395996\n",
      "    epoch          : 387\n",
      "    loss           : -1691.7353563969677\n",
      "    val_loss       : -1689.0063806768508\n",
      "    val_log_likelihood: 1775.4091918945312\n",
      "    val_log_marginal: 1741.4780059268346\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -1639.233765\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -1664.459717\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -1788.218994\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -1752.970825\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -1746.427002\n",
      "    epoch          : 388\n",
      "    loss           : -1697.9279168761602\n",
      "    val_loss       : -1639.9138749465346\n",
      "    val_log_likelihood: 1784.3841918945313\n",
      "    val_log_marginal: 1750.4300814252347\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -1789.891602\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -1748.252441\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -1504.044922\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -1668.634766\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -1744.185791\n",
      "    epoch          : 389\n",
      "    loss           : -1680.731338954208\n",
      "    val_loss       : -1679.5467300806195\n",
      "    val_log_likelihood: 1779.1851318359375\n",
      "    val_log_marginal: 1745.3933957245208\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -1685.148682\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -1433.932983\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -1461.729004\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -1717.027832\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -1727.045776\n",
      "    epoch          : 390\n",
      "    loss           : -1673.4137035407643\n",
      "    val_loss       : -1666.5190906191244\n",
      "    val_log_likelihood: 1769.119677734375\n",
      "    val_log_marginal: 1735.2116469151854\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -1951.910400\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -1813.385376\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -1648.915283\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -1518.114502\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -1741.691406\n",
      "    epoch          : 391\n",
      "    loss           : -1696.7103537380106\n",
      "    val_loss       : -1694.9635605614633\n",
      "    val_log_likelihood: 1772.8656005859375\n",
      "    val_log_marginal: 1738.9042622851746\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -1751.534790\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -1726.094238\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -1648.069092\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -1364.595215\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -1651.196533\n",
      "    epoch          : 392\n",
      "    loss           : -1690.1567612449721\n",
      "    val_loss       : -1633.3449677722529\n",
      "    val_log_likelihood: 1785.0646240234375\n",
      "    val_log_marginal: 1751.307813337123\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -1974.260254\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -1808.743896\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -1729.003418\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -1657.838257\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -1758.002686\n",
      "    epoch          : 393\n",
      "    loss           : -1703.7297931331218\n",
      "    val_loss       : -1677.8267248764635\n",
      "    val_log_likelihood: 1779.23935546875\n",
      "    val_log_marginal: 1745.4770764987916\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -1813.997192\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -1702.666016\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -1789.722168\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -1731.885010\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -1744.723511\n",
      "    epoch          : 394\n",
      "    loss           : -1718.90093450263\n",
      "    val_loss       : -1713.0459492480381\n",
      "    val_log_likelihood: 1792.1482177734374\n",
      "    val_log_marginal: 1758.4326776903122\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -1827.300415\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -1712.102295\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -1644.255737\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -1654.424561\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -1656.979004\n",
      "    epoch          : 395\n",
      "    loss           : -1699.6710531404703\n",
      "    val_loss       : -1687.6920090869069\n",
      "    val_log_likelihood: 1781.817626953125\n",
      "    val_log_marginal: 1748.1525749423502\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -1968.394897\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -1780.590942\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -1655.711548\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -1488.530151\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -1665.785156\n",
      "    epoch          : 396\n",
      "    loss           : -1695.0701626314976\n",
      "    val_loss       : -1702.6947831403463\n",
      "    val_log_likelihood: 1787.168798828125\n",
      "    val_log_marginal: 1753.4135855984005\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -1966.722412\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -1803.826050\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -1506.268921\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -1729.887451\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -1744.957275\n",
      "    epoch          : 397\n",
      "    loss           : -1691.253909875851\n",
      "    val_loss       : -1748.641554219462\n",
      "    val_log_likelihood: 1783.030224609375\n",
      "    val_log_marginal: 1749.2482251460412\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -1973.931396\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -1668.488525\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -1660.985718\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -1737.978760\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -1734.602295\n",
      "    epoch          : 398\n",
      "    loss           : -1715.2175679726176\n",
      "    val_loss       : -1674.065335980989\n",
      "    val_log_likelihood: 1779.955029296875\n",
      "    val_log_marginal: 1746.2785346906633\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -1968.661865\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -1747.951172\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -1631.326660\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -1790.341797\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -1451.993408\n",
      "    epoch          : 399\n",
      "    loss           : -1692.2533478689666\n",
      "    val_loss       : -1713.1711590707303\n",
      "    val_log_likelihood: 1788.0538940429688\n",
      "    val_log_marginal: 1754.4155172090977\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -1956.460938\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -1654.602783\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -1733.156250\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -1665.573608\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -1661.958374\n",
      "    epoch          : 400\n",
      "    loss           : -1705.3905802811726\n",
      "    val_loss       : -1738.4777317017317\n",
      "    val_log_likelihood: 1792.2540893554688\n",
      "    val_log_marginal: 1758.6464675539332\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -1965.330078\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -1713.702637\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -1739.847900\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -1756.982178\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -1664.067871\n",
      "    epoch          : 401\n",
      "    loss           : -1723.26524791151\n",
      "    val_loss       : -1739.925155167468\n",
      "    val_log_likelihood: 1793.63564453125\n",
      "    val_log_marginal: 1760.1318672549721\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -1836.669556\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -1453.606323\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -1764.008301\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -1801.138794\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -1755.302612\n",
      "    epoch          : 402\n",
      "    loss           : -1714.0546778310643\n",
      "    val_loss       : -1736.5740078656004\n",
      "    val_log_likelihood: 1770.8344848632812\n",
      "    val_log_marginal: 1737.0173346731813\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -1809.040039\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -1266.667603\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -1717.148804\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -1644.553955\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -1730.673340\n",
      "    epoch          : 403\n",
      "    loss           : -1714.6728310160117\n",
      "    val_loss       : -1754.891374721378\n",
      "    val_log_likelihood: 1789.1286743164062\n",
      "    val_log_marginal: 1755.4725095009176\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -1825.378906\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -1823.376953\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -1467.390381\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -1978.605103\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -1661.980103\n",
      "    epoch          : 404\n",
      "    loss           : -1714.2363619662747\n",
      "    val_loss       : -1739.3084347164258\n",
      "    val_log_likelihood: 1794.6448974609375\n",
      "    val_log_marginal: 1761.137465403624\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -1980.859375\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -1703.702637\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -1745.873901\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -1746.718018\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -1487.398438\n",
      "    epoch          : 405\n",
      "    loss           : -1705.3150368869894\n",
      "    val_loss       : -1625.2762560846284\n",
      "    val_log_likelihood: 1762.4609741210938\n",
      "    val_log_marginal: 1728.7030352141708\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -1956.258545\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -1776.876343\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -1577.114380\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -1733.433472\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -1731.459106\n",
      "    epoch          : 406\n",
      "    loss           : -1657.2237446095685\n",
      "    val_loss       : -1704.406971040927\n",
      "    val_log_likelihood: 1761.8803833007812\n",
      "    val_log_marginal: 1728.1355601307005\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -1782.572388\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -1251.488159\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -1646.768066\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -1969.542603\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -1742.302002\n",
      "    epoch          : 407\n",
      "    loss           : -1682.1505598313738\n",
      "    val_loss       : -1704.1895572809503\n",
      "    val_log_likelihood: 1780.2211303710938\n",
      "    val_log_marginal: 1746.4977452266965\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -1977.482910\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -1815.761108\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -1615.922974\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -1745.223633\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -1651.115112\n",
      "    epoch          : 408\n",
      "    loss           : -1699.8838337813274\n",
      "    val_loss       : -1707.7215044420213\n",
      "    val_log_likelihood: 1781.4580688476562\n",
      "    val_log_marginal: 1747.8265061911195\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -1787.182373\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -1707.953857\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -1792.063477\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -1648.907104\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -1802.398438\n",
      "    epoch          : 409\n",
      "    loss           : -1720.9446562209932\n",
      "    val_loss       : -1721.0935828382148\n",
      "    val_log_likelihood: 1792.6847290039063\n",
      "    val_log_marginal: 1759.1724028054625\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -1814.439453\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -1825.891602\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -1646.320312\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -1757.921753\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -1461.650757\n",
      "    epoch          : 410\n",
      "    loss           : -1708.5533725247524\n",
      "    val_loss       : -1619.5262602766975\n",
      "    val_log_likelihood: 1790.0685424804688\n",
      "    val_log_marginal: 1756.5611985694632\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -1977.518188\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -1504.058960\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -1451.654297\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -1754.652832\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -1532.112915\n",
      "    epoch          : 411\n",
      "    loss           : -1705.7846812635364\n",
      "    val_loss       : -1739.8690452203155\n",
      "    val_log_likelihood: 1794.1855590820312\n",
      "    val_log_marginal: 1760.6409181382508\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -1979.646484\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -1648.700073\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -1759.000000\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -1708.691650\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -1664.806396\n",
      "    epoch          : 412\n",
      "    loss           : -1706.6038250309407\n",
      "    val_loss       : -1686.2346736952663\n",
      "    val_log_likelihood: 1779.3226318359375\n",
      "    val_log_marginal: 1745.7536484975367\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -1812.935425\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -1704.272461\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -1785.673706\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -1752.763916\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -1535.456665\n",
      "    epoch          : 413\n",
      "    loss           : -1716.4226569751702\n",
      "    val_loss       : -1667.861637223512\n",
      "    val_log_likelihood: 1789.7564086914062\n",
      "    val_log_marginal: 1756.2178528863944\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -1978.750732\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -1702.165649\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -1675.664917\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -1658.871094\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -1474.295166\n",
      "    epoch          : 414\n",
      "    loss           : -1726.2607820718595\n",
      "    val_loss       : -1738.045329133421\n",
      "    val_log_likelihood: 1793.4450927734374\n",
      "    val_log_marginal: 1759.8601863119752\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -1983.427856\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -1519.611694\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -1651.255127\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -1982.933838\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -1714.451660\n",
      "    epoch          : 415\n",
      "    loss           : -1730.2350107325185\n",
      "    val_loss       : -1729.7833389356733\n",
      "    val_log_likelihood: 1790.079638671875\n",
      "    val_log_marginal: 1756.4881270583842\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -1979.183350\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -1698.013428\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -1661.431519\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -1984.131348\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -1758.629883\n",
      "    epoch          : 416\n",
      "    loss           : -1724.9339696298732\n",
      "    val_loss       : -1715.1773777410388\n",
      "    val_log_likelihood: 1786.872021484375\n",
      "    val_log_marginal: 1753.2688376219662\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -1981.576294\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -1721.482178\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -1753.037720\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -1605.188599\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -1672.909668\n",
      "    epoch          : 417\n",
      "    loss           : -1719.8395863145886\n",
      "    val_loss       : -1642.4286073364317\n",
      "    val_log_likelihood: 1796.6546142578125\n",
      "    val_log_marginal: 1763.2125409726054\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -1982.998657\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -1642.005859\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -1801.292358\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -1755.801270\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -1545.523193\n",
      "    epoch          : 418\n",
      "    loss           : -1707.4075214650372\n",
      "    val_loss       : -1754.7143313214183\n",
      "    val_log_likelihood: 1788.6719604492187\n",
      "    val_log_marginal: 1755.1235066184258\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -1869.451660\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -1832.593750\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -1763.581909\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -1974.476318\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -1734.523560\n",
      "    epoch          : 419\n",
      "    loss           : -1714.664063708617\n",
      "    val_loss       : -1714.089191686362\n",
      "    val_log_likelihood: 1785.1980224609374\n",
      "    val_log_marginal: 1751.5506310220808\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -1707.814941\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -1704.249634\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -1661.159912\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -1641.340210\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -1359.726807\n",
      "    epoch          : 420\n",
      "    loss           : -1712.8601352200649\n",
      "    val_loss       : -1717.8546056220307\n",
      "    val_log_likelihood: 1786.9460693359374\n",
      "    val_log_marginal: 1753.3058761116117\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -1871.894043\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -1694.359253\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -1617.871948\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -1667.708008\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -1761.615723\n",
      "    epoch          : 421\n",
      "    loss           : -1723.253825272664\n",
      "    val_loss       : -1691.0464498087763\n",
      "    val_log_likelihood: 1797.2444702148437\n",
      "    val_log_marginal: 1763.6291418762182\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -1982.467407\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -1540.676025\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -1232.353394\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -1794.865601\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -1759.895752\n",
      "    epoch          : 422\n",
      "    loss           : -1712.1840397296567\n",
      "    val_loss       : -1704.3397278418765\n",
      "    val_log_likelihood: 1793.7424926757812\n",
      "    val_log_marginal: 1760.0372861210956\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -1976.144165\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -1711.486206\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -1667.761719\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -1785.587158\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -1760.322021\n",
      "    epoch          : 423\n",
      "    loss           : -1715.2948336459622\n",
      "    val_loss       : -1728.528287968412\n",
      "    val_log_likelihood: 1796.82705078125\n",
      "    val_log_marginal: 1763.0620624212231\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -1984.818359\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -1802.287842\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -1558.442749\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -1718.530518\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -1654.030151\n",
      "    epoch          : 424\n",
      "    loss           : -1714.3622696376083\n",
      "    val_loss       : -1731.9727100294083\n",
      "    val_log_likelihood: 1783.5735961914063\n",
      "    val_log_marginal: 1750.068845540181\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -1970.242310\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -1490.769287\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -1730.077637\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -1748.373535\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -1760.013062\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0000e-04.\n",
      "    epoch          : 425\n",
      "    loss           : -1730.1536563080135\n",
      "    val_loss       : -1731.3754499187694\n",
      "    val_log_likelihood: 1798.3774291992188\n",
      "    val_log_marginal: 1764.8729188840837\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -1979.385986\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -1674.276733\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -1623.332764\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -1764.530640\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -1755.918945\n",
      "    epoch          : 426\n",
      "    loss           : -1722.4049797435798\n",
      "    val_loss       : -1728.4455515971408\n",
      "    val_log_likelihood: 1799.2987182617187\n",
      "    val_log_marginal: 1765.7631340455264\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -1984.834717\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -1719.184570\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -1677.895020\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -1986.192871\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -1675.057251\n",
      "    epoch          : 427\n",
      "    loss           : -1741.8578400564666\n",
      "    val_loss       : -1748.5201263204217\n",
      "    val_log_likelihood: 1800.0255126953125\n",
      "    val_log_marginal: 1766.5453154898837\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -1722.950073\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -1557.963257\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -1739.696167\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -1985.826904\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -1764.695557\n",
      "    epoch          : 428\n",
      "    loss           : -1741.3467074856899\n",
      "    val_loss       : -1698.3526447825134\n",
      "    val_log_likelihood: 1801.25166015625\n",
      "    val_log_marginal: 1767.7302059683948\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -1516.561890\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -1562.064819\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -1672.761353\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -1680.813599\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -1667.723633\n",
      "    epoch          : 429\n",
      "    loss           : -1725.5772402923885\n",
      "    val_loss       : -1767.5978915400804\n",
      "    val_log_likelihood: 1801.497509765625\n",
      "    val_log_marginal: 1767.9780715204863\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -1864.953979\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -1722.951294\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -1672.501465\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -1798.764648\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -1757.699463\n",
      "    epoch          : 430\n",
      "    loss           : -1729.6371901106127\n",
      "    val_loss       : -1662.7963636050001\n",
      "    val_log_likelihood: 1802.2899047851563\n",
      "    val_log_marginal: 1768.827158901468\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -1982.152954\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -1651.132080\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -1799.632080\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -1585.855591\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -1507.583496\n",
      "    epoch          : 431\n",
      "    loss           : -1749.2827897780012\n",
      "    val_loss       : -1751.7602326480671\n",
      "    val_log_likelihood: 1802.2384399414063\n",
      "    val_log_marginal: 1768.8546239826828\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -1979.816528\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -1680.839844\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -1633.348145\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -1988.334351\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -1674.330566\n",
      "    epoch          : 432\n",
      "    loss           : -1745.370714244276\n",
      "    val_loss       : -1734.5027553983032\n",
      "    val_log_likelihood: 1803.1471435546875\n",
      "    val_log_marginal: 1769.651104548201\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -1982.165527\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -1468.039795\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -1586.664673\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -1769.807373\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -1675.566772\n",
      "    epoch          : 433\n",
      "    loss           : -1724.6568120068844\n",
      "    val_loss       : -1735.5942355416714\n",
      "    val_log_likelihood: 1802.57744140625\n",
      "    val_log_marginal: 1769.0732692744773\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -1988.492432\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -1721.635742\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -1744.661865\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -1524.785645\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -1499.803345\n",
      "    epoch          : 434\n",
      "    loss           : -1732.9556969368812\n",
      "    val_loss       : -1735.0454832604155\n",
      "    val_log_likelihood: 1803.420263671875\n",
      "    val_log_marginal: 1769.9856123815644\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -1985.151245\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -1766.924072\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -1582.909668\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -1737.164795\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -1666.472778\n",
      "    epoch          : 435\n",
      "    loss           : -1740.0382877765317\n",
      "    val_loss       : -1753.7283731367438\n",
      "    val_log_likelihood: 1803.0459716796875\n",
      "    val_log_marginal: 1769.6097390118987\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -1986.055908\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -1720.037842\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -1799.706665\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -1760.549805\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -1758.564209\n",
      "    epoch          : 436\n",
      "    loss           : -1733.4086055944463\n",
      "    val_loss       : -1737.6308721153066\n",
      "    val_log_likelihood: 1804.0814575195313\n",
      "    val_log_marginal: 1770.7040522467348\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -1984.771240\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -1723.644897\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -1687.466064\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -1497.801636\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -1759.903564\n",
      "    epoch          : 437\n",
      "    loss           : -1734.715391253481\n",
      "    val_loss       : -1744.499245973304\n",
      "    val_log_likelihood: 1803.4359130859375\n",
      "    val_log_marginal: 1770.0530629457212\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -1722.578613\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -1838.210449\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -1683.219727\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -1739.108276\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -1768.939087\n",
      "    epoch          : 438\n",
      "    loss           : -1743.6152960144648\n",
      "    val_loss       : -1754.4254982553423\n",
      "    val_log_likelihood: 1805.3308471679688\n",
      "    val_log_marginal: 1771.9408054154378\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -1988.720459\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -1829.825439\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -1742.705078\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -1762.106689\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -1763.603027\n",
      "    epoch          : 439\n",
      "    loss           : -1741.7221280843905\n",
      "    val_loss       : -1737.8664622278884\n",
      "    val_log_likelihood: 1804.3285766601562\n",
      "    val_log_marginal: 1770.8927345402049\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -1984.993408\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -1678.767944\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -1766.927490\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -1768.124878\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -1681.256226\n",
      "    epoch          : 440\n",
      "    loss           : -1727.6293498124226\n",
      "    val_loss       : -1720.9494132736697\n",
      "    val_log_likelihood: 1804.9063110351562\n",
      "    val_log_marginal: 1771.539454218292\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -1984.446289\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -1840.094971\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -1800.055176\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -1807.584473\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -1767.899414\n",
      "    epoch          : 441\n",
      "    loss           : -1735.1405367709622\n",
      "    val_loss       : -1755.22516433727\n",
      "    val_log_likelihood: 1804.3804321289062\n",
      "    val_log_marginal: 1770.9922553284875\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -1986.306396\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -1723.094238\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -1806.319336\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -1983.234863\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -1769.780273\n",
      "    epoch          : 442\n",
      "    loss           : -1745.1890615331065\n",
      "    val_loss       : -1666.7880436981097\n",
      "    val_log_likelihood: 1804.162548828125\n",
      "    val_log_marginal: 1770.7009299550232\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -1983.640381\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -1721.133789\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -1581.576538\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -1744.729004\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -1774.633057\n",
      "    epoch          : 443\n",
      "    loss           : -1743.5593237546411\n",
      "    val_loss       : -1718.096717241034\n",
      "    val_log_likelihood: 1804.7166137695312\n",
      "    val_log_marginal: 1771.4139949153912\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -1839.931641\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -1578.639404\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -1814.860840\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -1803.415771\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -1738.278809\n",
      "    epoch          : 444\n",
      "    loss           : -1743.7314090539912\n",
      "    val_loss       : -1719.895778861083\n",
      "    val_log_likelihood: 1805.067529296875\n",
      "    val_log_marginal: 1771.7184652397505\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -1874.777588\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -1568.346680\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -1739.802856\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -1807.099487\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -1488.418701\n",
      "    epoch          : 445\n",
      "    loss           : -1741.0918609316986\n",
      "    val_loss       : -1754.7574424684049\n",
      "    val_log_likelihood: 1805.7645629882813\n",
      "    val_log_marginal: 1772.3310239221908\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -1838.129517\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -1680.878296\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -1503.153442\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -1769.279053\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -1769.457153\n",
      "    epoch          : 446\n",
      "    loss           : -1728.8935595219677\n",
      "    val_loss       : -1741.0148957973347\n",
      "    val_log_likelihood: 1806.739453125\n",
      "    val_log_marginal: 1773.4175978157668\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -1868.755127\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -1685.952759\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -1806.074951\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -1771.535400\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -1765.383667\n",
      "    epoch          : 447\n",
      "    loss           : -1745.5434896639078\n",
      "    val_loss       : -1731.6199400747196\n",
      "    val_log_likelihood: 1805.9744995117187\n",
      "    val_log_marginal: 1772.6147851689645\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -1866.839966\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -1843.114258\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -1670.202393\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -1986.413940\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -1769.997559\n",
      "    epoch          : 448\n",
      "    loss           : -1749.1239352084622\n",
      "    val_loss       : -1773.7742730788887\n",
      "    val_log_likelihood: 1807.7750854492188\n",
      "    val_log_marginal: 1774.4458032608204\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -1990.168213\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -1833.811035\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -1772.998047\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -1770.149780\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -1528.531372\n",
      "    epoch          : 449\n",
      "    loss           : -1731.4044008160581\n",
      "    val_loss       : -1760.7749030001462\n",
      "    val_log_likelihood: 1806.3544311523438\n",
      "    val_log_marginal: 1773.0095284868032\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -1987.907471\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -1741.775757\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -1801.812500\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -1779.188232\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -1768.921143\n",
      "    epoch          : 450\n",
      "    loss           : -1743.1696160949102\n",
      "    val_loss       : -1754.7789408564568\n",
      "    val_log_likelihood: 1805.8989135742188\n",
      "    val_log_marginal: 1772.5641634412154\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -1874.110596\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -1414.173096\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -1805.461914\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -1987.700439\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -1679.708496\n",
      "    epoch          : 451\n",
      "    loss           : -1746.0898896774443\n",
      "    val_loss       : -1772.6208622980862\n",
      "    val_log_likelihood: 1806.3658081054687\n",
      "    val_log_marginal: 1772.927591645382\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -1871.094238\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -1722.411865\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -1746.442749\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -1679.855957\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -1595.416992\n",
      "    epoch          : 452\n",
      "    loss           : -1743.5992455812964\n",
      "    val_loss       : -1739.904848844558\n",
      "    val_log_likelihood: 1806.9427001953125\n",
      "    val_log_marginal: 1773.5740094605833\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -1840.609863\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -1731.473755\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -1748.648804\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -1515.955322\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -1771.842041\n",
      "    epoch          : 453\n",
      "    loss           : -1745.0213949373453\n",
      "    val_loss       : -1726.5161644039677\n",
      "    val_log_likelihood: 1806.7203369140625\n",
      "    val_log_marginal: 1773.406589151916\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -1983.513306\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -1725.907837\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -1766.390381\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -1499.322632\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -1666.144287\n",
      "    epoch          : 454\n",
      "    loss           : -1738.0951109026919\n",
      "    val_loss       : -1731.3973174590617\n",
      "    val_log_likelihood: 1806.6504272460938\n",
      "    val_log_marginal: 1773.2153560605107\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -1984.119141\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -1687.810547\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -1676.407959\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -1802.796997\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -1519.812012\n",
      "    epoch          : 455\n",
      "    loss           : -1736.7565313660273\n",
      "    val_loss       : -1740.7179136116058\n",
      "    val_log_likelihood: 1807.369091796875\n",
      "    val_log_marginal: 1773.9903811734607\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -1993.246582\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -1726.117188\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -1809.786987\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -1771.893677\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -1593.147339\n",
      "    epoch          : 456\n",
      "    loss           : -1737.2594078743812\n",
      "    val_loss       : -1773.7967699265107\n",
      "    val_log_likelihood: 1807.95283203125\n",
      "    val_log_marginal: 1774.6534112196416\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -1872.836670\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -1718.942871\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -1766.859009\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -1986.992920\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -1596.842041\n",
      "    epoch          : 457\n",
      "    loss           : -1759.5370658647896\n",
      "    val_loss       : -1740.8678102625533\n",
      "    val_log_likelihood: 1807.5126708984376\n",
      "    val_log_marginal: 1774.1262820517054\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -1872.934204\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -1728.226562\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -1684.328613\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -1810.427979\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -1769.085205\n",
      "    epoch          : 458\n",
      "    loss           : -1746.6517309812036\n",
      "    val_loss       : -1756.5789116568862\n",
      "    val_log_likelihood: 1807.385888671875\n",
      "    val_log_marginal: 1774.011412762478\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -1988.524414\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -1745.950684\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -1510.894287\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -1597.993408\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -1778.051758\n",
      "    epoch          : 459\n",
      "    loss           : -1737.5037890141552\n",
      "    val_loss       : -1739.5099735805766\n",
      "    val_log_likelihood: 1807.354541015625\n",
      "    val_log_marginal: 1773.971037883684\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -1986.719238\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -1730.809814\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -1773.630981\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -1814.235596\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -1674.857788\n",
      "    epoch          : 460\n",
      "    loss           : -1737.2687927850402\n",
      "    val_loss       : -1720.9090721627697\n",
      "    val_log_likelihood: 1807.8042114257812\n",
      "    val_log_marginal: 1774.4049652024332\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -1989.954956\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -1724.459839\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -1745.041260\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -1808.651978\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -1674.675903\n",
      "    epoch          : 461\n",
      "    loss           : -1751.8541211420948\n",
      "    val_loss       : -1745.2655522000045\n",
      "    val_log_likelihood: 1807.5444946289062\n",
      "    val_log_marginal: 1774.198820892349\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -1982.807007\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -1745.595581\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -1685.247559\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -1778.154541\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -1771.018311\n",
      "    epoch          : 462\n",
      "    loss           : -1743.2203635036356\n",
      "    val_loss       : -1741.3061107618735\n",
      "    val_log_likelihood: 1807.439111328125\n",
      "    val_log_marginal: 1774.0846618984226\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -1839.494873\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -1723.369629\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -1806.955933\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -1750.973633\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -1743.782837\n",
      "    epoch          : 463\n",
      "    loss           : -1744.0094719310798\n",
      "    val_loss       : -1718.335843460262\n",
      "    val_log_likelihood: 1807.8463256835937\n",
      "    val_log_marginal: 1774.4638266630318\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -1986.026245\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -1840.364258\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -1745.615723\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -1744.172363\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -1772.825684\n",
      "    epoch          : 464\n",
      "    loss           : -1743.9259528736077\n",
      "    val_loss       : -1742.453721316345\n",
      "    val_log_likelihood: 1807.460107421875\n",
      "    val_log_marginal: 1774.0270702114788\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -1877.100586\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -1734.255127\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -1809.193604\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -1988.081665\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -1767.960693\n",
      "    epoch          : 465\n",
      "    loss           : -1738.829180122602\n",
      "    val_loss       : -1757.386649928987\n",
      "    val_log_likelihood: 1808.3453369140625\n",
      "    val_log_marginal: 1775.0774841990321\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -1727.856934\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -1841.195557\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -1769.686768\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -1690.171997\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -1767.630005\n",
      "    epoch          : 466\n",
      "    loss           : -1748.8425063331529\n",
      "    val_loss       : -1757.2005552880466\n",
      "    val_log_likelihood: 1808.1792358398438\n",
      "    val_log_marginal: 1774.8238418172018\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -1845.489014\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -1728.349121\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -1768.861694\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -1687.007568\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -1678.159546\n",
      "    epoch          : 467\n",
      "    loss           : -1738.8674944887066\n",
      "    val_loss       : -1745.8654593419283\n",
      "    val_log_likelihood: 1808.4503540039063\n",
      "    val_log_marginal: 1775.1740899626166\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -1986.459473\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -1728.443481\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -1742.300781\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -1778.114746\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -1775.154053\n",
      "    epoch          : 468\n",
      "    loss           : -1745.17463789836\n",
      "    val_loss       : -1747.3420755604282\n",
      "    val_log_likelihood: 1808.8779052734376\n",
      "    val_log_marginal: 1775.5770312872585\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -1988.708130\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -1572.405762\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -1814.009155\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -1749.336670\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -1771.190674\n",
      "    epoch          : 469\n",
      "    loss           : -1749.6852314259747\n",
      "    val_loss       : -1695.8719584908335\n",
      "    val_log_likelihood: 1808.4535400390625\n",
      "    val_log_marginal: 1775.0645163144916\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -1873.014771\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -1681.106934\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -1725.786865\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -1989.809204\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -1681.592285\n",
      "    epoch          : 470\n",
      "    loss           : -1752.308811301052\n",
      "    val_loss       : -1757.254060235992\n",
      "    val_log_likelihood: 1807.8507446289063\n",
      "    val_log_marginal: 1774.46365561524\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -1987.361450\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -1841.281860\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -1751.446777\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -1769.428955\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -1597.975342\n",
      "    epoch          : 471\n",
      "    loss           : -1736.2129099628712\n",
      "    val_loss       : -1757.4489614058286\n",
      "    val_log_likelihood: 1807.7604248046875\n",
      "    val_log_marginal: 1774.3936115849763\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -1729.369385\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -1728.264160\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -1811.489502\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -1878.283447\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -1775.966797\n",
      "    epoch          : 472\n",
      "    loss           : -1740.139862362701\n",
      "    val_loss       : -1727.048063324392\n",
      "    val_log_likelihood: 1808.0181884765625\n",
      "    val_log_marginal: 1774.6156484738935\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -1671.518066\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -1722.726685\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -1748.786011\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -1771.658447\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -1774.884033\n",
      "    epoch          : 473\n",
      "    loss           : -1731.7304554552136\n",
      "    val_loss       : -1714.9052094658837\n",
      "    val_log_likelihood: 1808.5180419921876\n",
      "    val_log_marginal: 1775.172272879258\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -1878.695801\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -1684.974365\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -1811.437988\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -1674.785645\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -1601.441528\n",
      "    epoch          : 474\n",
      "    loss           : -1726.1719982789295\n",
      "    val_loss       : -1775.4684752704575\n",
      "    val_log_likelihood: 1809.0320922851563\n",
      "    val_log_marginal: 1775.7427704390138\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -1986.564819\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -1836.094482\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -1604.825439\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -1773.314697\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -1535.262451\n",
      "    epoch          : 475\n",
      "    loss           : -1752.9246016398515\n",
      "    val_loss       : -1757.3155827810988\n",
      "    val_log_likelihood: 1808.330517578125\n",
      "    val_log_marginal: 1774.9503864016383\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -1689.291504\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -1837.498779\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -1741.268555\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -1394.231567\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -1780.110840\n",
      "    epoch          : 476\n",
      "    loss           : -1744.6469484839108\n",
      "    val_loss       : -1775.3151224778965\n",
      "    val_log_likelihood: 1808.9614501953124\n",
      "    val_log_marginal: 1775.6002376038236\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -1984.204712\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -1688.347656\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -1601.829346\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -1604.098022\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -1774.819214\n",
      "    epoch          : 477\n",
      "    loss           : -1744.9819819384281\n",
      "    val_loss       : -1707.1557109564542\n",
      "    val_log_likelihood: 1808.902392578125\n",
      "    val_log_marginal: 1775.5443536039443\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -1987.785400\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -1730.990723\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -1681.605957\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -1679.506958\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -1534.322144\n",
      "    epoch          : 478\n",
      "    loss           : -1741.1206804030012\n",
      "    val_loss       : -1736.609122989513\n",
      "    val_log_likelihood: 1809.6023193359374\n",
      "    val_log_marginal: 1776.267639082528\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -1879.630859\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -1727.921875\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -1806.999146\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -1603.241089\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -1766.860474\n",
      "    epoch          : 479\n",
      "    loss           : -1725.4767123685024\n",
      "    val_loss       : -1741.176787099056\n",
      "    val_log_likelihood: 1808.1675537109375\n",
      "    val_log_marginal: 1774.7941561166197\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -1879.570068\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -1842.215454\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -1680.790039\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -1603.588867\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -1768.020508\n",
      "    epoch          : 480\n",
      "    loss           : -1747.8156955832303\n",
      "    val_loss       : -1702.7955345191062\n",
      "    val_log_likelihood: 1809.13818359375\n",
      "    val_log_marginal: 1775.7352412140801\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -1988.263916\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -1845.212036\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -1769.435181\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -1986.669189\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -1679.529785\n",
      "    epoch          : 481\n",
      "    loss           : -1753.1413199547494\n",
      "    val_loss       : -1725.3363264394925\n",
      "    val_log_likelihood: 1808.0189208984375\n",
      "    val_log_marginal: 1774.703419115694\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -1990.993286\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -1724.327026\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -1688.309814\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -1988.771606\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -1769.540527\n",
      "    epoch          : 482\n",
      "    loss           : -1745.626820177135\n",
      "    val_loss       : -1726.6012621363625\n",
      "    val_log_likelihood: 1809.0816650390625\n",
      "    val_log_marginal: 1775.7119740521798\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -1983.590210\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -1671.444824\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -1648.261353\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -1990.206665\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -1771.442017\n",
      "    epoch          : 483\n",
      "    loss           : -1744.248885655167\n",
      "    val_loss       : -1719.441642223485\n",
      "    val_log_likelihood: 1809.6683227539063\n",
      "    val_log_marginal: 1776.3413840886205\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -1989.933105\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -1574.871338\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -1809.693359\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -1772.294189\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -1769.593506\n",
      "    epoch          : 484\n",
      "    loss           : -1749.4782545637377\n",
      "    val_loss       : -1753.0591161003335\n",
      "    val_log_likelihood: 1808.7668334960938\n",
      "    val_log_marginal: 1775.4645935777576\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -1987.856445\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -1723.042236\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -1773.479492\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -1766.993042\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -1606.695801\n",
      "    epoch          : 485\n",
      "    loss           : -1744.9307075727104\n",
      "    val_loss       : -1743.9110685160383\n",
      "    val_log_likelihood: 1809.50947265625\n",
      "    val_log_marginal: 1776.2086655247826\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -1879.629639\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -1727.385254\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -1744.932373\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -1746.380615\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -1807.819946\n",
      "    epoch          : 486\n",
      "    loss           : -1748.6240137685643\n",
      "    val_loss       : -1774.707059012167\n",
      "    val_log_likelihood: 1808.6644775390625\n",
      "    val_log_marginal: 1775.3627044517548\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -1879.561523\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -1840.235840\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -1680.244629\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -1991.906738\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -1675.646118\n",
      "    epoch          : 487\n",
      "    loss           : -1744.1975484413676\n",
      "    val_loss       : -1691.565558466688\n",
      "    val_log_likelihood: 1809.2987548828125\n",
      "    val_log_marginal: 1776.0328249439603\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -1882.690186\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -1838.729126\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -1778.020752\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -1812.220825\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -1767.960205\n",
      "    epoch          : 488\n",
      "    loss           : -1736.8019705290842\n",
      "    val_loss       : -1760.6161344505847\n",
      "    val_log_likelihood: 1808.8026000976563\n",
      "    val_log_marginal: 1775.404284474142\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -1769.932495\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -1696.644409\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -1679.231079\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -1813.698242\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -1770.101318\n",
      "    epoch          : 489\n",
      "    loss           : -1741.735599328976\n",
      "    val_loss       : -1729.3858248591423\n",
      "    val_log_likelihood: 1809.717333984375\n",
      "    val_log_marginal: 1776.4110957365483\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -1990.333130\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -1516.985596\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -1753.837402\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -1813.454102\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -1674.447510\n",
      "    epoch          : 490\n",
      "    loss           : -1742.1728648572864\n",
      "    val_loss       : -1748.9875064915045\n",
      "    val_log_likelihood: 1809.9758544921874\n",
      "    val_log_marginal: 1776.670554501011\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -1988.903931\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -1810.398926\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -1765.955200\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -1774.908936\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -1770.065796\n",
      "    epoch          : 491\n",
      "    loss           : -1744.5258221012532\n",
      "    val_loss       : -1751.8247419120744\n",
      "    val_log_likelihood: 1809.5552368164062\n",
      "    val_log_marginal: 1776.2340559163663\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -1987.344116\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -1732.841187\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -1811.393066\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -1770.403931\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -1775.759766\n",
      "    epoch          : 492\n",
      "    loss           : -1742.1228861289449\n",
      "    val_loss       : -1689.1791765693574\n",
      "    val_log_likelihood: 1809.446240234375\n",
      "    val_log_marginal: 1776.1655892495066\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -1582.110718\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -1665.923096\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -1681.822876\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -1598.697632\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -1743.705322\n",
      "    epoch          : 493\n",
      "    loss           : -1745.3838325727104\n",
      "    val_loss       : -1775.6681885421276\n",
      "    val_log_likelihood: 1809.720068359375\n",
      "    val_log_marginal: 1776.40252729665\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -1983.136230\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -1847.607910\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -1642.134521\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -1781.635620\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -1775.339844\n",
      "    epoch          : 494\n",
      "    loss           : -1743.9061291383045\n",
      "    val_loss       : -1702.2204634228722\n",
      "    val_log_likelihood: 1810.3956420898437\n",
      "    val_log_marginal: 1777.0995828364044\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -1608.340820\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -1769.444092\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -1778.491577\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -1502.288086\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -1677.677368\n",
      "    epoch          : 495\n",
      "    loss           : -1734.9150293935643\n",
      "    val_loss       : -1742.7792204437778\n",
      "    val_log_likelihood: 1810.3489501953125\n",
      "    val_log_marginal: 1777.0422448646277\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -1761.333862\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -1748.601074\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -1648.594727\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -1687.389404\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -1771.302002\n",
      "    epoch          : 496\n",
      "    loss           : -1738.8170649462406\n",
      "    val_loss       : -1759.7575593080371\n",
      "    val_log_likelihood: 1809.5894165039062\n",
      "    val_log_marginal: 1776.3247771274298\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -1986.427734\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -1735.212646\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -1755.734863\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -1606.396973\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -1771.975830\n",
      "    epoch          : 497\n",
      "    loss           : -1748.3804955812964\n",
      "    val_loss       : -1743.465158564411\n",
      "    val_log_likelihood: 1810.0366821289062\n",
      "    val_log_marginal: 1776.7875506985933\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -1989.258667\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -1736.130859\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -1812.431885\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -1815.992188\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -1779.693604\n",
      "    epoch          : 498\n",
      "    loss           : -1741.486274945854\n",
      "    val_loss       : -1776.102903196402\n",
      "    val_log_likelihood: 1809.9315795898438\n",
      "    val_log_marginal: 1776.6355391796678\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -1986.815918\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -1663.383545\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -1749.891846\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -1517.888062\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -1777.816772\n",
      "    epoch          : 499\n",
      "    loss           : -1740.322483176052\n",
      "    val_loss       : -1753.2186854092404\n",
      "    val_log_likelihood: 1811.0529663085938\n",
      "    val_log_marginal: 1777.7563925255097\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -1729.207275\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -1687.848389\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -1503.639282\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -1813.112671\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -1777.011230\n",
      "    epoch          : 500\n",
      "    loss           : -1746.8114182878248\n",
      "    val_loss       : -1776.9285572489723\n",
      "    val_log_likelihood: 1810.750537109375\n",
      "    val_log_marginal: 1777.4754667482634\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch500.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 501 [512/54000 (1%)] Loss: -1983.442505\n",
      "Train Epoch: 501 [11776/54000 (22%)] Loss: -1732.427002\n",
      "Train Epoch: 501 [23040/54000 (43%)] Loss: -1649.687866\n",
      "Train Epoch: 501 [34304/54000 (64%)] Loss: -1775.069336\n",
      "Train Epoch: 501 [45568/54000 (84%)] Loss: -1776.220459\n",
      "    epoch          : 501\n",
      "    loss           : -1750.4488839631033\n",
      "    val_loss       : -1744.108153161034\n",
      "    val_log_likelihood: 1810.2592651367188\n",
      "    val_log_marginal: 1776.9618809279054\n",
      "Train Epoch: 502 [512/54000 (1%)] Loss: -1664.978271\n",
      "Train Epoch: 502 [11776/54000 (22%)] Loss: -1691.528687\n",
      "Train Epoch: 502 [23040/54000 (43%)] Loss: -1813.190186\n",
      "Train Epoch: 502 [34304/54000 (64%)] Loss: -1536.460693\n",
      "Train Epoch: 502 [45568/54000 (84%)] Loss: -1776.583496\n",
      "    epoch          : 502\n",
      "    loss           : -1742.0745789178527\n",
      "    val_loss       : -1759.8802140345797\n",
      "    val_log_likelihood: 1810.2372314453125\n",
      "    val_log_marginal: 1776.9272378031164\n",
      "Train Epoch: 503 [512/54000 (1%)] Loss: -1987.861572\n",
      "Train Epoch: 503 [11776/54000 (22%)] Loss: -1722.472412\n",
      "Train Epoch: 503 [23040/54000 (43%)] Loss: -1688.671509\n",
      "Train Epoch: 503 [34304/54000 (64%)] Loss: -1881.756836\n",
      "Train Epoch: 503 [45568/54000 (84%)] Loss: -1687.689697\n",
      "    epoch          : 503\n",
      "    loss           : -1753.187526589573\n",
      "    val_loss       : -1759.2314207792283\n",
      "    val_log_likelihood: 1810.1610961914062\n",
      "    val_log_marginal: 1776.8765847560026\n",
      "Train Epoch: 504 [512/54000 (1%)] Loss: -1719.619995\n",
      "Train Epoch: 504 [11776/54000 (22%)] Loss: -1720.115112\n",
      "Train Epoch: 504 [23040/54000 (43%)] Loss: -1812.546997\n",
      "Train Epoch: 504 [34304/54000 (64%)] Loss: -1986.840088\n",
      "Train Epoch: 504 [45568/54000 (84%)] Loss: -1772.766357\n",
      "    epoch          : 504\n",
      "    loss           : -1734.6519569925742\n",
      "    val_loss       : -1759.5330587100239\n",
      "    val_log_likelihood: 1809.634814453125\n",
      "    val_log_marginal: 1776.3530142657492\n",
      "Train Epoch: 505 [512/54000 (1%)] Loss: -1988.480347\n",
      "Train Epoch: 505 [11776/54000 (22%)] Loss: -1730.709473\n",
      "Train Epoch: 505 [23040/54000 (43%)] Loss: -1778.617554\n",
      "Train Epoch: 505 [34304/54000 (64%)] Loss: -1810.215210\n",
      "Train Epoch: 505 [45568/54000 (84%)] Loss: -1771.873291\n",
      "    epoch          : 505\n",
      "    loss           : -1737.9871632793163\n",
      "    val_loss       : -1759.972498991899\n",
      "    val_log_likelihood: 1809.4030029296875\n",
      "    val_log_marginal: 1776.1556118223816\n",
      "Train Epoch: 506 [512/54000 (1%)] Loss: -1876.821045\n",
      "Train Epoch: 506 [11776/54000 (22%)] Loss: -1747.990967\n",
      "Train Epoch: 506 [23040/54000 (43%)] Loss: -1808.973389\n",
      "Train Epoch: 506 [34304/54000 (64%)] Loss: -1809.035889\n",
      "Train Epoch: 506 [45568/54000 (84%)] Loss: -1780.426758\n",
      "    epoch          : 506\n",
      "    loss           : -1747.0998414294554\n",
      "    val_loss       : -1776.8710547329858\n",
      "    val_log_likelihood: 1810.6321044921874\n",
      "    val_log_marginal: 1777.3700471725315\n",
      "Train Epoch: 507 [512/54000 (1%)] Loss: -1987.774048\n",
      "Train Epoch: 507 [11776/54000 (22%)] Loss: -1576.369873\n",
      "Train Epoch: 507 [23040/54000 (43%)] Loss: -1812.383057\n",
      "Train Epoch: 507 [34304/54000 (64%)] Loss: -1598.002930\n",
      "Train Epoch: 507 [45568/54000 (84%)] Loss: -1537.752808\n",
      "    epoch          : 507\n",
      "    loss           : -1727.9759255588644\n",
      "    val_loss       : -1694.3888766294344\n",
      "    val_log_likelihood: 1807.8732543945312\n",
      "    val_log_marginal: 1774.5959003564367\n",
      "Train Epoch: 508 [512/54000 (1%)] Loss: -1984.277832\n",
      "Train Epoch: 508 [11776/54000 (22%)] Loss: -1728.314819\n",
      "Train Epoch: 508 [23040/54000 (43%)] Loss: -1749.055664\n",
      "Train Epoch: 508 [34304/54000 (64%)] Loss: -1817.860596\n",
      "Train Epoch: 508 [45568/54000 (84%)] Loss: -1775.291016\n",
      "    epoch          : 508\n",
      "    loss           : -1745.6899909595452\n",
      "    val_loss       : -1744.3445793883875\n",
      "    val_log_likelihood: 1809.6614990234375\n",
      "    val_log_marginal: 1776.5533056411891\n",
      "Train Epoch: 509 [512/54000 (1%)] Loss: -1988.423584\n",
      "Train Epoch: 509 [11776/54000 (22%)] Loss: -1694.304443\n",
      "Train Epoch: 509 [23040/54000 (43%)] Loss: -1691.481689\n",
      "Train Epoch: 509 [34304/54000 (64%)] Loss: -1769.318848\n",
      "Train Epoch: 509 [45568/54000 (84%)] Loss: -1686.567993\n",
      "    epoch          : 509\n",
      "    loss           : -1749.7675442837253\n",
      "    val_loss       : -1741.93415107131\n",
      "    val_log_likelihood: 1809.3532958984374\n",
      "    val_log_marginal: 1776.157248187527\n",
      "Train Epoch: 510 [512/54000 (1%)] Loss: -1988.292358\n",
      "Train Epoch: 510 [11776/54000 (22%)] Loss: -1747.432861\n",
      "Train Epoch: 510 [23040/54000 (43%)] Loss: -1746.505249\n",
      "Train Epoch: 510 [34304/54000 (64%)] Loss: -1690.919922\n",
      "Train Epoch: 510 [45568/54000 (84%)] Loss: -1766.710083\n",
      "    epoch          : 510\n",
      "    loss           : -1745.401859094601\n",
      "    val_loss       : -1744.9310058264061\n",
      "    val_log_likelihood: 1809.8711669921875\n",
      "    val_log_marginal: 1776.6127756950937\n",
      "Train Epoch: 511 [512/54000 (1%)] Loss: -1986.029175\n",
      "Train Epoch: 511 [11776/54000 (22%)] Loss: -1692.584106\n",
      "Train Epoch: 511 [23040/54000 (43%)] Loss: -1684.659424\n",
      "Train Epoch: 511 [34304/54000 (64%)] Loss: -1684.414917\n",
      "Train Epoch: 511 [45568/54000 (84%)] Loss: -1653.610352\n",
      "    epoch          : 511\n",
      "    loss           : -1758.3992061803838\n",
      "    val_loss       : -1736.2188187269494\n",
      "    val_log_likelihood: 1809.2907470703126\n",
      "    val_log_marginal: 1776.038948120605\n",
      "Train Epoch: 512 [512/54000 (1%)] Loss: -1847.569580\n",
      "Train Epoch: 512 [11776/54000 (22%)] Loss: -1718.699341\n",
      "Train Epoch: 512 [23040/54000 (43%)] Loss: -1818.132324\n",
      "Train Epoch: 512 [34304/54000 (64%)] Loss: -1775.165039\n",
      "Train Epoch: 512 [45568/54000 (84%)] Loss: -1536.743896\n",
      "    epoch          : 512\n",
      "    loss           : -1751.6604402749845\n",
      "    val_loss       : -1760.1912164747714\n",
      "    val_log_likelihood: 1810.0322265625\n",
      "    val_log_marginal: 1776.809437553212\n",
      "Train Epoch: 513 [512/54000 (1%)] Loss: -1841.839355\n",
      "Train Epoch: 513 [11776/54000 (22%)] Loss: -1723.775513\n",
      "Train Epoch: 513 [23040/54000 (43%)] Loss: -1747.123413\n",
      "Train Epoch: 513 [34304/54000 (64%)] Loss: -1807.547729\n",
      "Train Epoch: 513 [45568/54000 (84%)] Loss: -1392.899902\n",
      "    epoch          : 513\n",
      "    loss           : -1742.66937799737\n",
      "    val_loss       : -1777.0579550074413\n",
      "    val_log_likelihood: 1810.8349609375\n",
      "    val_log_marginal: 1777.6648898359388\n",
      "Train Epoch: 514 [512/54000 (1%)] Loss: -1985.740967\n",
      "Train Epoch: 514 [11776/54000 (22%)] Loss: -1731.334717\n",
      "Train Epoch: 514 [23040/54000 (43%)] Loss: -1605.828979\n",
      "Train Epoch: 514 [34304/54000 (64%)] Loss: -1687.830444\n",
      "Train Epoch: 514 [45568/54000 (84%)] Loss: -1776.905640\n",
      "    epoch          : 514\n",
      "    loss           : -1748.9743628171411\n",
      "    val_loss       : -1750.2794471869245\n",
      "    val_log_likelihood: 1810.5654052734376\n",
      "    val_log_marginal: 1777.3600665245206\n",
      "Train Epoch: 515 [512/54000 (1%)] Loss: -1988.623291\n",
      "Train Epoch: 515 [11776/54000 (22%)] Loss: -1581.947998\n",
      "Train Epoch: 515 [23040/54000 (43%)] Loss: -1810.446533\n",
      "Train Epoch: 515 [34304/54000 (64%)] Loss: -1780.336182\n",
      "Train Epoch: 515 [45568/54000 (84%)] Loss: -1770.150146\n",
      "    epoch          : 515\n",
      "    loss           : -1733.4579933090965\n",
      "    val_loss       : -1759.618382192403\n",
      "    val_log_likelihood: 1809.6488891601562\n",
      "    val_log_marginal: 1776.4741278309375\n",
      "Train Epoch: 516 [512/54000 (1%)] Loss: -1985.243652\n",
      "Train Epoch: 516 [11776/54000 (22%)] Loss: -1815.533203\n",
      "Train Epoch: 516 [23040/54000 (43%)] Loss: -1681.570435\n",
      "Train Epoch: 516 [34304/54000 (64%)] Loss: -1991.727295\n",
      "Train Epoch: 516 [45568/54000 (84%)] Loss: -1772.443359\n",
      "    epoch          : 516\n",
      "    loss           : -1739.5444118386447\n",
      "    val_loss       : -1762.4130608832463\n",
      "    val_log_likelihood: 1809.910595703125\n",
      "    val_log_marginal: 1776.7599338760035\n",
      "Train Epoch: 517 [512/54000 (1%)] Loss: -1990.176758\n",
      "Train Epoch: 517 [11776/54000 (22%)] Loss: -1731.612549\n",
      "Train Epoch: 517 [23040/54000 (43%)] Loss: -1743.465454\n",
      "Train Epoch: 517 [34304/54000 (64%)] Loss: -1531.807251\n",
      "Train Epoch: 517 [45568/54000 (84%)] Loss: -1771.627686\n",
      "    epoch          : 517\n",
      "    loss           : -1740.2267232460551\n",
      "    val_loss       : -1761.3534949027003\n",
      "    val_log_likelihood: 1810.3996215820312\n",
      "    val_log_marginal: 1777.1880608592182\n",
      "Train Epoch: 518 [512/54000 (1%)] Loss: -1989.395264\n",
      "Train Epoch: 518 [11776/54000 (22%)] Loss: -1540.690186\n",
      "Train Epoch: 518 [23040/54000 (43%)] Loss: -1773.765503\n",
      "Train Epoch: 518 [34304/54000 (64%)] Loss: -1768.765625\n",
      "Train Epoch: 518 [45568/54000 (84%)] Loss: -1750.537109\n",
      "    epoch          : 518\n",
      "    loss           : -1740.1661316522277\n",
      "    val_loss       : -1721.940058017522\n",
      "    val_log_likelihood: 1810.6146850585938\n",
      "    val_log_marginal: 1777.3329067461425\n",
      "Train Epoch: 519 [512/54000 (1%)] Loss: -1672.963623\n",
      "Train Epoch: 519 [11776/54000 (22%)] Loss: -1731.623413\n",
      "Train Epoch: 519 [23040/54000 (43%)] Loss: -1814.776855\n",
      "Train Epoch: 519 [34304/54000 (64%)] Loss: -1775.226318\n",
      "Train Epoch: 519 [45568/54000 (84%)] Loss: -1771.300049\n",
      "    epoch          : 519\n",
      "    loss           : -1728.309410775062\n",
      "    val_loss       : -1765.8049258416518\n",
      "    val_log_likelihood: 1810.39716796875\n",
      "    val_log_marginal: 1777.1719042491168\n",
      "Train Epoch: 520 [512/54000 (1%)] Loss: -1984.929443\n",
      "Train Epoch: 520 [11776/54000 (22%)] Loss: -1573.250488\n",
      "Train Epoch: 520 [23040/54000 (43%)] Loss: -1654.713623\n",
      "Train Epoch: 520 [34304/54000 (64%)] Loss: -1808.851074\n",
      "Train Epoch: 520 [45568/54000 (84%)] Loss: -1680.614868\n",
      "    epoch          : 520\n",
      "    loss           : -1741.5972634494894\n",
      "    val_loss       : -1714.1286926884204\n",
      "    val_log_likelihood: 1809.7524780273438\n",
      "    val_log_marginal: 1776.5943510185928\n",
      "Train Epoch: 521 [512/54000 (1%)] Loss: -1574.280029\n",
      "Train Epoch: 521 [11776/54000 (22%)] Loss: -1839.742798\n",
      "Train Epoch: 521 [23040/54000 (43%)] Loss: -1689.253174\n",
      "Train Epoch: 521 [34304/54000 (64%)] Loss: -1777.917725\n",
      "Train Epoch: 521 [45568/54000 (84%)] Loss: -1682.362671\n",
      "    epoch          : 521\n",
      "    loss           : -1754.2166687616027\n",
      "    val_loss       : -1712.5589219512417\n",
      "    val_log_likelihood: 1811.0883178710938\n",
      "    val_log_marginal: 1777.9483424726873\n",
      "Train Epoch: 522 [512/54000 (1%)] Loss: -1881.742432\n",
      "Train Epoch: 522 [11776/54000 (22%)] Loss: -1742.737549\n",
      "Train Epoch: 522 [23040/54000 (43%)] Loss: -1689.003662\n",
      "Train Epoch: 522 [34304/54000 (64%)] Loss: -1988.136353\n",
      "Train Epoch: 522 [45568/54000 (84%)] Loss: -1678.209961\n",
      "    epoch          : 522\n",
      "    loss           : -1746.4743193069307\n",
      "    val_loss       : -1730.3335972627624\n",
      "    val_log_likelihood: 1809.3894409179688\n",
      "    val_log_marginal: 1776.2370624605567\n",
      "Train Epoch: 523 [512/54000 (1%)] Loss: -1987.356079\n",
      "Train Epoch: 523 [11776/54000 (22%)] Loss: -1849.893066\n",
      "Train Epoch: 523 [23040/54000 (43%)] Loss: -1744.986572\n",
      "Train Epoch: 523 [34304/54000 (64%)] Loss: -1538.547729\n",
      "Train Epoch: 523 [45568/54000 (84%)] Loss: -1815.707153\n",
      "    epoch          : 523\n",
      "    loss           : -1728.7575308922494\n",
      "    val_loss       : -1761.3136915428563\n",
      "    val_log_likelihood: 1810.625439453125\n",
      "    val_log_marginal: 1777.4476720366627\n",
      "Train Epoch: 524 [512/54000 (1%)] Loss: -1987.475098\n",
      "Train Epoch: 524 [11776/54000 (22%)] Loss: -1575.775879\n",
      "Train Epoch: 524 [23040/54000 (43%)] Loss: -1517.724121\n",
      "Train Epoch: 524 [34304/54000 (64%)] Loss: -1681.234619\n",
      "Train Epoch: 524 [45568/54000 (84%)] Loss: -1780.177002\n",
      "    epoch          : 524\n",
      "    loss           : -1734.809437364635\n",
      "    val_loss       : -1745.4028886932879\n",
      "    val_log_likelihood: 1811.7313110351563\n",
      "    val_log_marginal: 1778.5887845542281\n",
      "Train Epoch: 525 [512/54000 (1%)] Loss: -1989.066406\n",
      "Train Epoch: 525 [11776/54000 (22%)] Loss: -1571.883057\n",
      "Train Epoch: 525 [23040/54000 (43%)] Loss: -1745.762573\n",
      "Train Epoch: 525 [34304/54000 (64%)] Loss: -1679.306641\n",
      "Train Epoch: 525 [45568/54000 (84%)] Loss: -1610.961914\n",
      "    epoch          : 525\n",
      "    loss           : -1740.402910591352\n",
      "    val_loss       : -1761.713825270906\n",
      "    val_log_likelihood: 1810.7467407226563\n",
      "    val_log_marginal: 1777.5714632602426\n",
      "Train Epoch: 526 [512/54000 (1%)] Loss: -1733.934937\n",
      "Train Epoch: 526 [11776/54000 (22%)] Loss: -1743.352539\n",
      "Train Epoch: 526 [23040/54000 (43%)] Loss: -1657.166748\n",
      "Train Epoch: 526 [34304/54000 (64%)] Loss: -1674.977417\n",
      "Train Epoch: 526 [45568/54000 (84%)] Loss: -1684.019287\n",
      "    epoch          : 526\n",
      "    loss           : -1753.183539362237\n",
      "    val_loss       : -1777.81211806722\n",
      "    val_log_likelihood: 1811.1167846679687\n",
      "    val_log_marginal: 1777.9712952654904\n",
      "Train Epoch: 527 [512/54000 (1%)] Loss: -1988.856201\n",
      "Train Epoch: 527 [11776/54000 (22%)] Loss: -1735.493652\n",
      "Train Epoch: 527 [23040/54000 (43%)] Loss: -1771.384033\n",
      "Train Epoch: 527 [34304/54000 (64%)] Loss: -1680.643188\n",
      "Train Epoch: 527 [45568/54000 (84%)] Loss: -1672.651245\n",
      "    epoch          : 527\n",
      "    loss           : -1744.8900992516244\n",
      "    val_loss       : -1752.361218519695\n",
      "    val_log_likelihood: 1811.6236694335937\n",
      "    val_log_marginal: 1778.5259329166263\n",
      "Train Epoch: 528 [512/54000 (1%)] Loss: -1990.669678\n",
      "Train Epoch: 528 [11776/54000 (22%)] Loss: -1576.357910\n",
      "Train Epoch: 528 [23040/54000 (43%)] Loss: -1693.377930\n",
      "Train Epoch: 528 [34304/54000 (64%)] Loss: -1820.317383\n",
      "Train Epoch: 528 [45568/54000 (84%)] Loss: -1776.610596\n",
      "    epoch          : 528\n",
      "    loss           : -1748.632054697169\n",
      "    val_loss       : -1732.0022062718867\n",
      "    val_log_likelihood: 1811.6370849609375\n",
      "    val_log_marginal: 1778.5165152710713\n",
      "Train Epoch: 529 [512/54000 (1%)] Loss: -1687.819580\n",
      "Train Epoch: 529 [11776/54000 (22%)] Loss: -1730.809326\n",
      "Train Epoch: 529 [23040/54000 (43%)] Loss: -1519.217163\n",
      "Train Epoch: 529 [34304/54000 (64%)] Loss: -1680.852051\n",
      "Train Epoch: 529 [45568/54000 (84%)] Loss: -1683.997803\n",
      "    epoch          : 529\n",
      "    loss           : -1751.7072802250927\n",
      "    val_loss       : -1695.2088131161406\n",
      "    val_log_likelihood: 1811.5670043945313\n",
      "    val_log_marginal: 1778.4356650803948\n",
      "Train Epoch: 530 [512/54000 (1%)] Loss: -1990.314209\n",
      "Train Epoch: 530 [11776/54000 (22%)] Loss: -1769.470947\n",
      "Train Epoch: 530 [23040/54000 (43%)] Loss: -1746.122925\n",
      "Train Epoch: 530 [34304/54000 (64%)] Loss: -1814.653564\n",
      "Train Epoch: 530 [45568/54000 (84%)] Loss: -1775.606445\n",
      "    epoch          : 530\n",
      "    loss           : -1753.5545920192606\n",
      "    val_loss       : -1721.299876386486\n",
      "    val_log_likelihood: 1809.8473266601563\n",
      "    val_log_marginal: 1776.727461331656\n",
      "Train Epoch: 531 [512/54000 (1%)] Loss: -1986.485107\n",
      "Train Epoch: 531 [11776/54000 (22%)] Loss: -1724.597656\n",
      "Train Epoch: 531 [23040/54000 (43%)] Loss: -1748.730103\n",
      "Train Epoch: 531 [34304/54000 (64%)] Loss: -1685.774170\n",
      "Train Epoch: 531 [45568/54000 (84%)] Loss: -1777.772705\n",
      "    epoch          : 531\n",
      "    loss           : -1752.139374081451\n",
      "    val_loss       : -1744.4148253077642\n",
      "    val_log_likelihood: 1811.0571899414062\n",
      "    val_log_marginal: 1777.9305341634445\n",
      "Train Epoch: 532 [512/54000 (1%)] Loss: -1847.777100\n",
      "Train Epoch: 532 [11776/54000 (22%)] Loss: -1730.480713\n",
      "Train Epoch: 532 [23040/54000 (43%)] Loss: -1772.886963\n",
      "Train Epoch: 532 [34304/54000 (64%)] Loss: -1813.806396\n",
      "Train Epoch: 532 [45568/54000 (84%)] Loss: -1612.485229\n",
      "    epoch          : 532\n",
      "    loss           : -1747.8661384204827\n",
      "    val_loss       : -1730.439248325862\n",
      "    val_log_likelihood: 1811.4490478515625\n",
      "    val_log_marginal: 1778.2580120810576\n",
      "Train Epoch: 533 [512/54000 (1%)] Loss: -1986.964844\n",
      "Train Epoch: 533 [11776/54000 (22%)] Loss: -1848.380249\n",
      "Train Epoch: 533 [23040/54000 (43%)] Loss: -1606.668091\n",
      "Train Epoch: 533 [34304/54000 (64%)] Loss: -1687.732300\n",
      "Train Epoch: 533 [45568/54000 (84%)] Loss: -1683.026367\n",
      "    epoch          : 533\n",
      "    loss           : -1743.4430475518254\n",
      "    val_loss       : -1745.860943605751\n",
      "    val_log_likelihood: 1811.2662231445313\n",
      "    val_log_marginal: 1778.1534696898495\n",
      "Train Epoch: 534 [512/54000 (1%)] Loss: -1991.173096\n",
      "Train Epoch: 534 [11776/54000 (22%)] Loss: -1581.895142\n",
      "Train Epoch: 534 [23040/54000 (43%)] Loss: -1742.640259\n",
      "Train Epoch: 534 [34304/54000 (64%)] Loss: -1810.075806\n",
      "Train Epoch: 534 [45568/54000 (84%)] Loss: -1610.194092\n",
      "    epoch          : 534\n",
      "    loss           : -1734.1071318069307\n",
      "    val_loss       : -1738.571288271062\n",
      "    val_log_likelihood: 1810.84267578125\n",
      "    val_log_marginal: 1777.7893008388073\n",
      "Train Epoch: 535 [512/54000 (1%)] Loss: -1888.572510\n",
      "Train Epoch: 535 [11776/54000 (22%)] Loss: -1680.567993\n",
      "Train Epoch: 535 [23040/54000 (43%)] Loss: -1748.350098\n",
      "Train Epoch: 535 [34304/54000 (64%)] Loss: -1773.408813\n",
      "Train Epoch: 535 [45568/54000 (84%)] Loss: -1778.116333\n",
      "    epoch          : 535\n",
      "    loss           : -1750.7649614692914\n",
      "    val_loss       : -1751.2464698955416\n",
      "    val_log_likelihood: 1811.76376953125\n",
      "    val_log_marginal: 1778.5966235671192\n",
      "Train Epoch: 536 [512/54000 (1%)] Loss: -1893.456665\n",
      "Train Epoch: 536 [11776/54000 (22%)] Loss: -1848.482300\n",
      "Train Epoch: 536 [23040/54000 (43%)] Loss: -1814.489014\n",
      "Train Epoch: 536 [34304/54000 (64%)] Loss: -1815.135132\n",
      "Train Epoch: 536 [45568/54000 (84%)] Loss: -1613.298096\n",
      "    epoch          : 536\n",
      "    loss           : -1757.5979874110458\n",
      "    val_loss       : -1762.015358903259\n",
      "    val_log_likelihood: 1811.36376953125\n",
      "    val_log_marginal: 1778.2464113008232\n",
      "Train Epoch: 537 [512/54000 (1%)] Loss: -1990.864258\n",
      "Train Epoch: 537 [11776/54000 (22%)] Loss: -1682.833496\n",
      "Train Epoch: 537 [23040/54000 (43%)] Loss: -1778.988281\n",
      "Train Epoch: 537 [34304/54000 (64%)] Loss: -1778.796997\n",
      "Train Epoch: 537 [45568/54000 (84%)] Loss: -1775.717285\n",
      "    epoch          : 537\n",
      "    loss           : -1754.4049301902846\n",
      "    val_loss       : -1736.4062453508377\n",
      "    val_log_likelihood: 1812.1794189453126\n",
      "    val_log_marginal: 1779.0291429464614\n",
      "Train Epoch: 538 [512/54000 (1%)] Loss: -1992.302734\n",
      "Train Epoch: 538 [11776/54000 (22%)] Loss: -1611.964844\n",
      "Train Epoch: 538 [23040/54000 (43%)] Loss: -1750.042725\n",
      "Train Epoch: 538 [34304/54000 (64%)] Loss: -1814.174438\n",
      "Train Epoch: 538 [45568/54000 (84%)] Loss: -1545.326660\n",
      "    epoch          : 538\n",
      "    loss           : -1738.063963635133\n",
      "    val_loss       : -1744.8571085449307\n",
      "    val_log_likelihood: 1811.7082641601562\n",
      "    val_log_marginal: 1778.6895110297949\n",
      "Train Epoch: 539 [512/54000 (1%)] Loss: -1989.011597\n",
      "Train Epoch: 539 [11776/54000 (22%)] Loss: -1773.599487\n",
      "Train Epoch: 539 [23040/54000 (43%)] Loss: -1809.342896\n",
      "Train Epoch: 539 [34304/54000 (64%)] Loss: -1743.959961\n",
      "Train Epoch: 539 [45568/54000 (84%)] Loss: -1528.407104\n",
      "    epoch          : 539\n",
      "    loss           : -1752.8136058845143\n",
      "    val_loss       : -1750.0610508754849\n",
      "    val_log_likelihood: 1812.4810424804687\n",
      "    val_log_marginal: 1779.3627264367176\n",
      "Train Epoch: 540 [512/54000 (1%)] Loss: -1988.671997\n",
      "Train Epoch: 540 [11776/54000 (22%)] Loss: -1735.449463\n",
      "Train Epoch: 540 [23040/54000 (43%)] Loss: -1775.583130\n",
      "Train Epoch: 540 [34304/54000 (64%)] Loss: -1769.295898\n",
      "Train Epoch: 540 [45568/54000 (84%)] Loss: -1776.366211\n",
      "    epoch          : 540\n",
      "    loss           : -1744.3009758373298\n",
      "    val_loss       : -1762.688594982587\n",
      "    val_log_likelihood: 1812.27138671875\n",
      "    val_log_marginal: 1779.170052969153\n",
      "Train Epoch: 541 [512/54000 (1%)] Loss: -1992.117310\n",
      "Train Epoch: 541 [11776/54000 (22%)] Loss: -1688.659912\n",
      "Train Epoch: 541 [23040/54000 (43%)] Loss: -1615.545044\n",
      "Train Epoch: 541 [34304/54000 (64%)] Loss: -1679.233398\n",
      "Train Epoch: 541 [45568/54000 (84%)] Loss: -1777.168945\n",
      "    epoch          : 541\n",
      "    loss           : -1754.8559546140161\n",
      "    val_loss       : -1760.9224139729515\n",
      "    val_log_likelihood: 1811.3374145507812\n",
      "    val_log_marginal: 1778.3102682668716\n",
      "Train Epoch: 542 [512/54000 (1%)] Loss: -1987.270264\n",
      "Train Epoch: 542 [11776/54000 (22%)] Loss: -1843.163086\n",
      "Train Epoch: 542 [23040/54000 (43%)] Loss: -1779.009399\n",
      "Train Epoch: 542 [34304/54000 (64%)] Loss: -1746.696411\n",
      "Train Epoch: 542 [45568/54000 (84%)] Loss: -1778.614868\n",
      "    epoch          : 542\n",
      "    loss           : -1739.4789060082767\n",
      "    val_loss       : -1763.176167649217\n",
      "    val_log_likelihood: 1812.1519775390625\n",
      "    val_log_marginal: 1779.1073347497731\n",
      "Train Epoch: 543 [512/54000 (1%)] Loss: -1986.017700\n",
      "Train Epoch: 543 [11776/54000 (22%)] Loss: -1843.529541\n",
      "Train Epoch: 543 [23040/54000 (43%)] Loss: -1697.335327\n",
      "Train Epoch: 543 [34304/54000 (64%)] Loss: -1771.641846\n",
      "Train Epoch: 543 [45568/54000 (84%)] Loss: -1770.464478\n",
      "    epoch          : 543\n",
      "    loss           : -1752.366536055461\n",
      "    val_loss       : -1762.7946049747989\n",
      "    val_log_likelihood: 1812.5005737304687\n",
      "    val_log_marginal: 1779.440309613571\n",
      "Train Epoch: 544 [512/54000 (1%)] Loss: -1278.546387\n",
      "Train Epoch: 544 [11776/54000 (22%)] Loss: -1845.603760\n",
      "Train Epoch: 544 [23040/54000 (43%)] Loss: -1816.041626\n",
      "Train Epoch: 544 [34304/54000 (64%)] Loss: -1781.946777\n",
      "Train Epoch: 544 [45568/54000 (84%)] Loss: -1515.736572\n",
      "    epoch          : 544\n",
      "    loss           : -1748.2691082340657\n",
      "    val_loss       : -1778.266880087182\n",
      "    val_log_likelihood: 1811.93310546875\n",
      "    val_log_marginal: 1778.8624252650886\n",
      "Train Epoch: 545 [512/54000 (1%)] Loss: -1985.418457\n",
      "Train Epoch: 545 [11776/54000 (22%)] Loss: -1742.618164\n",
      "Train Epoch: 545 [23040/54000 (43%)] Loss: -1744.660889\n",
      "Train Epoch: 545 [34304/54000 (64%)] Loss: -1773.321533\n",
      "Train Epoch: 545 [45568/54000 (84%)] Loss: -1769.101562\n",
      "    epoch          : 545\n",
      "    loss           : -1752.3685665319463\n",
      "    val_loss       : -1698.8448302829638\n",
      "    val_log_likelihood: 1811.9082763671875\n",
      "    val_log_marginal: 1778.9055789684085\n",
      "Train Epoch: 546 [512/54000 (1%)] Loss: -1983.268799\n",
      "Train Epoch: 546 [11776/54000 (22%)] Loss: -1585.710083\n",
      "Train Epoch: 546 [23040/54000 (43%)] Loss: -1773.068848\n",
      "Train Epoch: 546 [34304/54000 (64%)] Loss: -1989.381348\n",
      "Train Epoch: 546 [45568/54000 (84%)] Loss: -1774.202637\n",
      "    epoch          : 546\n",
      "    loss           : -1742.5801784402072\n",
      "    val_loss       : -1717.7868824981153\n",
      "    val_log_likelihood: 1811.4671630859375\n",
      "    val_log_marginal: 1778.4251006234692\n",
      "Train Epoch: 547 [512/54000 (1%)] Loss: -1987.466431\n",
      "Train Epoch: 547 [11776/54000 (22%)] Loss: -1741.608643\n",
      "Train Epoch: 547 [23040/54000 (43%)] Loss: -1689.305786\n",
      "Train Epoch: 547 [34304/54000 (64%)] Loss: -1988.567993\n",
      "Train Epoch: 547 [45568/54000 (84%)] Loss: -1773.362549\n",
      "    epoch          : 547\n",
      "    loss           : -1750.9134267674815\n",
      "    val_loss       : -1761.5830492740497\n",
      "    val_log_likelihood: 1811.7651611328124\n",
      "    val_log_marginal: 1778.7832057800144\n",
      "Train Epoch: 548 [512/54000 (1%)] Loss: -1985.374390\n",
      "Train Epoch: 548 [11776/54000 (22%)] Loss: -1729.083130\n",
      "Train Epoch: 548 [23040/54000 (43%)] Loss: -1520.462036\n",
      "Train Epoch: 548 [34304/54000 (64%)] Loss: -1815.195679\n",
      "Train Epoch: 548 [45568/54000 (84%)] Loss: -1779.864868\n",
      "    epoch          : 548\n",
      "    loss           : -1752.6813275932086\n",
      "    val_loss       : -1746.8231238413603\n",
      "    val_log_likelihood: 1811.0149658203125\n",
      "    val_log_marginal: 1778.1325791407376\n",
      "Train Epoch: 549 [512/54000 (1%)] Loss: -1984.584106\n",
      "Train Epoch: 549 [11776/54000 (22%)] Loss: -1731.687134\n",
      "Train Epoch: 549 [23040/54000 (43%)] Loss: -1516.204102\n",
      "Train Epoch: 549 [34304/54000 (64%)] Loss: -1776.000488\n",
      "Train Epoch: 549 [45568/54000 (84%)] Loss: -1774.810425\n",
      "    epoch          : 549\n",
      "    loss           : -1748.4358031017946\n",
      "    val_loss       : -1723.203775969334\n",
      "    val_log_likelihood: 1811.8794921875\n",
      "    val_log_marginal: 1778.881710304738\n",
      "Train Epoch: 550 [512/54000 (1%)] Loss: -1988.925293\n",
      "Train Epoch: 550 [11776/54000 (22%)] Loss: -1514.296021\n",
      "Train Epoch: 550 [23040/54000 (43%)] Loss: -1769.850708\n",
      "Train Epoch: 550 [34304/54000 (64%)] Loss: -1819.597656\n",
      "Train Epoch: 550 [45568/54000 (84%)] Loss: -1779.885742\n",
      "    epoch          : 550\n",
      "    loss           : -1739.8667922822556\n",
      "    val_loss       : -1725.339877235517\n",
      "    val_log_likelihood: 1812.098876953125\n",
      "    val_log_marginal: 1779.0538150059188\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch550.pth ...\n",
      "Train Epoch: 551 [512/54000 (1%)] Loss: -1890.486328\n",
      "Train Epoch: 551 [11776/54000 (22%)] Loss: -1690.234619\n",
      "Train Epoch: 551 [23040/54000 (43%)] Loss: -1682.341309\n",
      "Train Epoch: 551 [34304/54000 (64%)] Loss: -1816.412598\n",
      "Train Epoch: 551 [45568/54000 (84%)] Loss: -1775.896484\n",
      "    epoch          : 551\n",
      "    loss           : -1755.7192153175279\n",
      "    val_loss       : -1739.3353051805868\n",
      "    val_log_likelihood: 1812.6983032226562\n",
      "    val_log_marginal: 1779.7248175013287\n",
      "Train Epoch: 552 [512/54000 (1%)] Loss: -1889.239014\n",
      "Train Epoch: 552 [11776/54000 (22%)] Loss: -1851.505859\n",
      "Train Epoch: 552 [23040/54000 (43%)] Loss: -1780.570312\n",
      "Train Epoch: 552 [34304/54000 (64%)] Loss: -1520.851074\n",
      "Train Epoch: 552 [45568/54000 (84%)] Loss: -1522.193604\n",
      "    epoch          : 552\n",
      "    loss           : -1756.4909244952817\n",
      "    val_loss       : -1763.1533350871875\n",
      "    val_log_likelihood: 1812.346337890625\n",
      "    val_log_marginal: 1779.4320818226784\n",
      "Train Epoch: 553 [512/54000 (1%)] Loss: -1886.540161\n",
      "Train Epoch: 553 [11776/54000 (22%)] Loss: -1589.287842\n",
      "Train Epoch: 553 [23040/54000 (43%)] Loss: -1778.043213\n",
      "Train Epoch: 553 [34304/54000 (64%)] Loss: -1990.125488\n",
      "Train Epoch: 553 [45568/54000 (84%)] Loss: -1816.945435\n",
      "    epoch          : 553\n",
      "    loss           : -1748.8988979830601\n",
      "    val_loss       : -1779.082154591754\n",
      "    val_log_likelihood: 1812.7063110351562\n",
      "    val_log_marginal: 1779.7432024528657\n",
      "Train Epoch: 554 [512/54000 (1%)] Loss: -1735.175781\n",
      "Train Epoch: 554 [11776/54000 (22%)] Loss: -1680.666016\n",
      "Train Epoch: 554 [23040/54000 (43%)] Loss: -1780.423828\n",
      "Train Epoch: 554 [34304/54000 (64%)] Loss: -1690.571045\n",
      "Train Epoch: 554 [45568/54000 (84%)] Loss: -1783.123779\n",
      "    epoch          : 554\n",
      "    loss           : -1731.8863464959777\n",
      "    val_loss       : -1779.1345469625667\n",
      "    val_log_likelihood: 1812.6649169921875\n",
      "    val_log_marginal: 1779.673843509334\n",
      "Train Epoch: 555 [512/54000 (1%)] Loss: -1894.752563\n",
      "Train Epoch: 555 [11776/54000 (22%)] Loss: -1581.391113\n",
      "Train Epoch: 555 [23040/54000 (43%)] Loss: -1523.646484\n",
      "Train Epoch: 555 [34304/54000 (64%)] Loss: -1049.958740\n",
      "Train Epoch: 555 [45568/54000 (84%)] Loss: -1777.438965\n",
      "    epoch          : 555\n",
      "    loss           : -1733.2167618251083\n",
      "    val_loss       : -1748.1549763157964\n",
      "    val_log_likelihood: 1812.2773681640624\n",
      "    val_log_marginal: 1779.2676984276623\n",
      "Train Epoch: 556 [512/54000 (1%)] Loss: -1795.548340\n",
      "Train Epoch: 556 [11776/54000 (22%)] Loss: -1733.968994\n",
      "Train Epoch: 556 [23040/54000 (43%)] Loss: -1774.343994\n",
      "Train Epoch: 556 [34304/54000 (64%)] Loss: -1669.119629\n",
      "Train Epoch: 556 [45568/54000 (84%)] Loss: -1517.088623\n",
      "    epoch          : 556\n",
      "    loss           : -1748.6847528136602\n",
      "    val_loss       : -1767.5067155521363\n",
      "    val_log_likelihood: 1811.3236450195313\n",
      "    val_log_marginal: 1778.3265679541976\n",
      "Train Epoch: 557 [512/54000 (1%)] Loss: -1725.822266\n",
      "Train Epoch: 557 [11776/54000 (22%)] Loss: -1728.780029\n",
      "Train Epoch: 557 [23040/54000 (43%)] Loss: -1689.178223\n",
      "Train Epoch: 557 [34304/54000 (64%)] Loss: -1751.732422\n",
      "Train Epoch: 557 [45568/54000 (84%)] Loss: -1777.207642\n",
      "    epoch          : 557\n",
      "    loss           : -1755.202527943224\n",
      "    val_loss       : -1709.0818279828877\n",
      "    val_log_likelihood: 1812.981640625\n",
      "    val_log_marginal: 1779.9385975343139\n",
      "Train Epoch: 558 [512/54000 (1%)] Loss: -1886.368896\n",
      "Train Epoch: 558 [11776/54000 (22%)] Loss: -1843.489990\n",
      "Train Epoch: 558 [23040/54000 (43%)] Loss: -1780.007935\n",
      "Train Epoch: 558 [34304/54000 (64%)] Loss: -1679.398804\n",
      "Train Epoch: 558 [45568/54000 (84%)] Loss: -1684.716064\n",
      "    epoch          : 558\n",
      "    loss           : -1741.0868101214419\n",
      "    val_loss       : -1748.2931369394064\n",
      "    val_log_likelihood: 1812.631591796875\n",
      "    val_log_marginal: 1779.7210920210928\n",
      "Train Epoch: 559 [512/54000 (1%)] Loss: -1844.367065\n",
      "Train Epoch: 559 [11776/54000 (22%)] Loss: -1819.332275\n",
      "Train Epoch: 559 [23040/54000 (43%)] Loss: -1695.292358\n",
      "Train Epoch: 559 [34304/54000 (64%)] Loss: -1986.503784\n",
      "Train Epoch: 559 [45568/54000 (84%)] Loss: -1680.941650\n",
      "    epoch          : 559\n",
      "    loss           : -1758.4715745378248\n",
      "    val_loss       : -1764.2636192446575\n",
      "    val_log_likelihood: 1812.6802856445313\n",
      "    val_log_marginal: 1779.6698228526288\n",
      "Train Epoch: 560 [512/54000 (1%)] Loss: -1304.163818\n",
      "Train Epoch: 560 [11776/54000 (22%)] Loss: -1691.084717\n",
      "Train Epoch: 560 [23040/54000 (43%)] Loss: -1810.770386\n",
      "Train Epoch: 560 [34304/54000 (64%)] Loss: -1778.561279\n",
      "Train Epoch: 560 [45568/54000 (84%)] Loss: -1773.896729\n",
      "    epoch          : 560\n",
      "    loss           : -1740.969290251779\n",
      "    val_loss       : -1763.2876568386332\n",
      "    val_log_likelihood: 1812.8713500976562\n",
      "    val_log_marginal: 1779.8237951058902\n",
      "Train Epoch: 561 [512/54000 (1%)] Loss: -1891.111572\n",
      "Train Epoch: 561 [11776/54000 (22%)] Loss: -1849.351074\n",
      "Train Epoch: 561 [23040/54000 (43%)] Loss: -1775.899902\n",
      "Train Epoch: 561 [34304/54000 (64%)] Loss: -1992.019287\n",
      "Train Epoch: 561 [45568/54000 (84%)] Loss: -1778.693848\n",
      "    epoch          : 561\n",
      "    loss           : -1757.2469579111232\n",
      "    val_loss       : -1737.7948339160532\n",
      "    val_log_likelihood: 1812.5677001953125\n",
      "    val_log_marginal: 1779.5190451096744\n",
      "Train Epoch: 562 [512/54000 (1%)] Loss: -1685.121338\n",
      "Train Epoch: 562 [11776/54000 (22%)] Loss: -1730.473633\n",
      "Train Epoch: 562 [23040/54000 (43%)] Loss: -1818.376343\n",
      "Train Epoch: 562 [34304/54000 (64%)] Loss: -1747.471313\n",
      "Train Epoch: 562 [45568/54000 (84%)] Loss: -1778.696289\n",
      "    epoch          : 562\n",
      "    loss           : -1755.2614818610768\n",
      "    val_loss       : -1740.352185708843\n",
      "    val_log_likelihood: 1812.62265625\n",
      "    val_log_marginal: 1779.6307021259859\n",
      "Train Epoch: 563 [512/54000 (1%)] Loss: -1993.307373\n",
      "Train Epoch: 563 [11776/54000 (22%)] Loss: -1842.636963\n",
      "Train Epoch: 563 [23040/54000 (43%)] Loss: -1687.989258\n",
      "Train Epoch: 563 [34304/54000 (64%)] Loss: -1990.061279\n",
      "Train Epoch: 563 [45568/54000 (84%)] Loss: -1772.761475\n",
      "    epoch          : 563\n",
      "    loss           : -1750.146787737856\n",
      "    val_loss       : -1768.9815815048291\n",
      "    val_log_likelihood: 1812.3471557617188\n",
      "    val_log_marginal: 1779.3092687383773\n",
      "Train Epoch: 564 [512/54000 (1%)] Loss: -1845.506714\n",
      "Train Epoch: 564 [11776/54000 (22%)] Loss: -1749.805420\n",
      "Train Epoch: 564 [23040/54000 (43%)] Loss: -1810.521362\n",
      "Train Epoch: 564 [34304/54000 (64%)] Loss: -1772.752441\n",
      "Train Epoch: 564 [45568/54000 (84%)] Loss: -1775.592651\n",
      "    epoch          : 564\n",
      "    loss           : -1756.8504396948483\n",
      "    val_loss       : -1745.6737576292828\n",
      "    val_log_likelihood: 1812.3382934570313\n",
      "    val_log_marginal: 1779.3577897895004\n",
      "Train Epoch: 565 [512/54000 (1%)] Loss: -1886.457520\n",
      "Train Epoch: 565 [11776/54000 (22%)] Loss: -1726.621338\n",
      "Train Epoch: 565 [23040/54000 (43%)] Loss: -1815.798584\n",
      "Train Epoch: 565 [34304/54000 (64%)] Loss: -1987.289551\n",
      "Train Epoch: 565 [45568/54000 (84%)] Loss: -1777.542480\n",
      "    epoch          : 565\n",
      "    loss           : -1744.9273186107673\n",
      "    val_loss       : -1713.6380995959044\n",
      "    val_log_likelihood: 1812.1991455078125\n",
      "    val_log_marginal: 1779.2405369412722\n",
      "Train Epoch: 566 [512/54000 (1%)] Loss: -1987.057129\n",
      "Train Epoch: 566 [11776/54000 (22%)] Loss: -1776.294922\n",
      "Train Epoch: 566 [23040/54000 (43%)] Loss: -1694.278076\n",
      "Train Epoch: 566 [34304/54000 (64%)] Loss: -1810.723145\n",
      "Train Epoch: 566 [45568/54000 (84%)] Loss: -1816.618652\n",
      "    epoch          : 566\n",
      "    loss           : -1762.7709791731127\n",
      "    val_loss       : -1751.121595900692\n",
      "    val_log_likelihood: 1812.98505859375\n",
      "    val_log_marginal: 1779.9413062512504\n",
      "Train Epoch: 567 [512/54000 (1%)] Loss: -1989.144531\n",
      "Train Epoch: 567 [11776/54000 (22%)] Loss: -1848.010986\n",
      "Train Epoch: 567 [23040/54000 (43%)] Loss: -1749.598999\n",
      "Train Epoch: 567 [34304/54000 (64%)] Loss: -1772.591553\n",
      "Train Epoch: 567 [45568/54000 (84%)] Loss: -1775.888794\n",
      "    epoch          : 567\n",
      "    loss           : -1749.9840559251238\n",
      "    val_loss       : -1739.6292984753848\n",
      "    val_log_likelihood: 1813.0072021484375\n",
      "    val_log_marginal: 1780.0301046680659\n",
      "Train Epoch: 568 [512/54000 (1%)] Loss: -1989.974121\n",
      "Train Epoch: 568 [11776/54000 (22%)] Loss: -1580.274902\n",
      "Train Epoch: 568 [23040/54000 (43%)] Loss: -1750.047607\n",
      "Train Epoch: 568 [34304/54000 (64%)] Loss: -1774.583618\n",
      "Train Epoch: 568 [45568/54000 (84%)] Loss: -1748.658691\n",
      "    epoch          : 568\n",
      "    loss           : -1739.1303348352412\n",
      "    val_loss       : -1753.710355355218\n",
      "    val_log_likelihood: 1812.5062255859375\n",
      "    val_log_marginal: 1779.5054203247994\n",
      "Train Epoch: 569 [512/54000 (1%)] Loss: -1892.413940\n",
      "Train Epoch: 569 [11776/54000 (22%)] Loss: -1848.129883\n",
      "Train Epoch: 569 [23040/54000 (43%)] Loss: -1746.624023\n",
      "Train Epoch: 569 [34304/54000 (64%)] Loss: -1989.950317\n",
      "Train Epoch: 569 [45568/54000 (84%)] Loss: -1772.034302\n",
      "    epoch          : 569\n",
      "    loss           : -1754.1584158415842\n",
      "    val_loss       : -1764.113266056031\n",
      "    val_log_likelihood: 1813.3306030273438\n",
      "    val_log_marginal: 1780.2766488681696\n",
      "Train Epoch: 570 [512/54000 (1%)] Loss: -1696.266113\n",
      "Train Epoch: 570 [11776/54000 (22%)] Loss: -1822.520752\n",
      "Train Epoch: 570 [23040/54000 (43%)] Loss: -1679.414795\n",
      "Train Epoch: 570 [34304/54000 (64%)] Loss: -1756.207886\n",
      "Train Epoch: 570 [45568/54000 (84%)] Loss: -1517.634766\n",
      "    epoch          : 570\n",
      "    loss           : -1744.7367559754023\n",
      "    val_loss       : -1762.3185261981562\n",
      "    val_log_likelihood: 1812.2935791015625\n",
      "    val_log_marginal: 1779.229833200865\n",
      "Train Epoch: 571 [512/54000 (1%)] Loss: -1897.484619\n",
      "Train Epoch: 571 [11776/54000 (22%)] Loss: -1695.101929\n",
      "Train Epoch: 571 [23040/54000 (43%)] Loss: -1816.056030\n",
      "Train Epoch: 571 [34304/54000 (64%)] Loss: -1753.705688\n",
      "Train Epoch: 571 [45568/54000 (84%)] Loss: -1773.702393\n",
      "    epoch          : 571\n",
      "    loss           : -1757.4674785349628\n",
      "    val_loss       : -1763.5647436669096\n",
      "    val_log_likelihood: 1812.487109375\n",
      "    val_log_marginal: 1779.484004489705\n",
      "Train Epoch: 572 [512/54000 (1%)] Loss: -1728.175293\n",
      "Train Epoch: 572 [11776/54000 (22%)] Loss: -1812.776733\n",
      "Train Epoch: 572 [23040/54000 (43%)] Loss: -1694.350464\n",
      "Train Epoch: 572 [34304/54000 (64%)] Loss: -1545.973145\n",
      "Train Epoch: 572 [45568/54000 (84%)] Loss: -1747.765381\n",
      "    epoch          : 572\n",
      "    loss           : -1748.4381864944307\n",
      "    val_loss       : -1730.7019238337875\n",
      "    val_log_likelihood: 1811.8445068359374\n",
      "    val_log_marginal: 1778.75070980452\n",
      "Train Epoch: 573 [512/54000 (1%)] Loss: -1992.137817\n",
      "Train Epoch: 573 [11776/54000 (22%)] Loss: -1732.147583\n",
      "Train Epoch: 573 [23040/54000 (43%)] Loss: -1687.422363\n",
      "Train Epoch: 573 [34304/54000 (64%)] Loss: -1987.522461\n",
      "Train Epoch: 573 [45568/54000 (84%)] Loss: -1779.073730\n",
      "    epoch          : 573\n",
      "    loss           : -1732.075058738784\n",
      "    val_loss       : -1716.2028637273238\n",
      "    val_log_likelihood: 1813.3750854492187\n",
      "    val_log_marginal: 1780.3880929548293\n",
      "Train Epoch: 574 [512/54000 (1%)] Loss: -1990.032593\n",
      "Train Epoch: 574 [11776/54000 (22%)] Loss: -1733.018677\n",
      "Train Epoch: 574 [23040/54000 (43%)] Loss: -1779.402832\n",
      "Train Epoch: 574 [34304/54000 (64%)] Loss: -1894.499268\n",
      "Train Epoch: 574 [45568/54000 (84%)] Loss: -1777.534424\n",
      "    epoch          : 574\n",
      "    loss           : -1744.2277880375927\n",
      "    val_loss       : -1668.9288735786454\n",
      "    val_log_likelihood: 1813.4906860351562\n",
      "    val_log_marginal: 1780.49684479945\n",
      "Train Epoch: 575 [512/54000 (1%)] Loss: -1987.085083\n",
      "Train Epoch: 575 [11776/54000 (22%)] Loss: -1581.994507\n",
      "Train Epoch: 575 [23040/54000 (43%)] Loss: -1749.360840\n",
      "Train Epoch: 575 [34304/54000 (64%)] Loss: -1405.307861\n",
      "Train Epoch: 575 [45568/54000 (84%)] Loss: -1547.148682\n",
      "    epoch          : 575\n",
      "    loss           : -1751.7955092628404\n",
      "    val_loss       : -1770.252229797095\n",
      "    val_log_likelihood: 1813.1068481445313\n",
      "    val_log_marginal: 1780.1082485258253\n",
      "Train Epoch: 576 [512/54000 (1%)] Loss: -1992.360352\n",
      "Train Epoch: 576 [11776/54000 (22%)] Loss: -1843.491211\n",
      "Train Epoch: 576 [23040/54000 (43%)] Loss: -1816.874512\n",
      "Train Epoch: 576 [34304/54000 (64%)] Loss: -1749.132080\n",
      "Train Epoch: 576 [45568/54000 (84%)] Loss: -1676.025391\n",
      "    epoch          : 576\n",
      "    loss           : -1742.9093960299351\n",
      "    val_loss       : -1763.7038848806174\n",
      "    val_log_likelihood: 1812.8562133789062\n",
      "    val_log_marginal: 1779.817277092859\n",
      "Train Epoch: 577 [512/54000 (1%)] Loss: -1989.786499\n",
      "Train Epoch: 577 [11776/54000 (22%)] Loss: -1730.364258\n",
      "Train Epoch: 577 [23040/54000 (43%)] Loss: -1819.729980\n",
      "Train Epoch: 577 [34304/54000 (64%)] Loss: -1518.855103\n",
      "Train Epoch: 577 [45568/54000 (84%)] Loss: -1783.201538\n",
      "    epoch          : 577\n",
      "    loss           : -1748.4091446376083\n",
      "    val_loss       : -1715.1074921121822\n",
      "    val_log_likelihood: 1812.7455810546876\n",
      "    val_log_marginal: 1779.6805511119521\n",
      "Train Epoch: 578 [512/54000 (1%)] Loss: -1896.675171\n",
      "Train Epoch: 578 [11776/54000 (22%)] Loss: -1754.317017\n",
      "Train Epoch: 578 [23040/54000 (43%)] Loss: -1815.389160\n",
      "Train Epoch: 578 [34304/54000 (64%)] Loss: -1819.867432\n",
      "Train Epoch: 578 [45568/54000 (84%)] Loss: -1685.678955\n",
      "    epoch          : 578\n",
      "    loss           : -1756.7940939723856\n",
      "    val_loss       : -1732.1210370123385\n",
      "    val_log_likelihood: 1813.1833618164062\n",
      "    val_log_marginal: 1780.1414912519679\n",
      "Train Epoch: 579 [512/54000 (1%)] Loss: -1988.333252\n",
      "Train Epoch: 579 [11776/54000 (22%)] Loss: -1775.501831\n",
      "Train Epoch: 579 [23040/54000 (43%)] Loss: -1695.625854\n",
      "Train Epoch: 579 [34304/54000 (64%)] Loss: -1777.628052\n",
      "Train Epoch: 579 [45568/54000 (84%)] Loss: -1778.629517\n",
      "    epoch          : 579\n",
      "    loss           : -1750.6677753712872\n",
      "    val_loss       : -1764.8231875194238\n",
      "    val_log_likelihood: 1813.4176879882812\n",
      "    val_log_marginal: 1780.3570760813523\n",
      "Train Epoch: 580 [512/54000 (1%)] Loss: -1892.649292\n",
      "Train Epoch: 580 [11776/54000 (22%)] Loss: -1749.126831\n",
      "Train Epoch: 580 [23040/54000 (43%)] Loss: -1697.926147\n",
      "Train Epoch: 580 [34304/54000 (64%)] Loss: -1774.342285\n",
      "Train Epoch: 580 [45568/54000 (84%)] Loss: -1680.921509\n",
      "    epoch          : 580\n",
      "    loss           : -1745.1440864789604\n",
      "    val_loss       : -1716.1042783068492\n",
      "    val_log_likelihood: 1813.3171020507812\n",
      "    val_log_marginal: 1780.2885946934719\n",
      "Train Epoch: 581 [512/54000 (1%)] Loss: -1898.010986\n",
      "Train Epoch: 581 [11776/54000 (22%)] Loss: -1814.901123\n",
      "Train Epoch: 581 [23040/54000 (43%)] Loss: -1690.529663\n",
      "Train Epoch: 581 [34304/54000 (64%)] Loss: -1777.042969\n",
      "Train Epoch: 581 [45568/54000 (84%)] Loss: -1780.158203\n",
      "    epoch          : 581\n",
      "    loss           : -1756.6892041499073\n",
      "    val_loss       : -1765.9694216856733\n",
      "    val_log_likelihood: 1813.0215698242187\n",
      "    val_log_marginal: 1779.9514289253966\n",
      "Train Epoch: 582 [512/54000 (1%)] Loss: -1990.569336\n",
      "Train Epoch: 582 [11776/54000 (22%)] Loss: -1725.146729\n",
      "Train Epoch: 582 [23040/54000 (43%)] Loss: -1774.638428\n",
      "Train Epoch: 582 [34304/54000 (64%)] Loss: -1776.798584\n",
      "Train Epoch: 582 [45568/54000 (84%)] Loss: -1515.036133\n",
      "    epoch          : 582\n",
      "    loss           : -1732.5216354521194\n",
      "    val_loss       : -1749.5208069622518\n",
      "    val_log_likelihood: 1813.4549072265625\n",
      "    val_log_marginal: 1780.4184197094291\n",
      "Train Epoch: 583 [512/54000 (1%)] Loss: -1990.624756\n",
      "Train Epoch: 583 [11776/54000 (22%)] Loss: -1688.499878\n",
      "Train Epoch: 583 [23040/54000 (43%)] Loss: -1750.554932\n",
      "Train Epoch: 583 [34304/54000 (64%)] Loss: -1894.251099\n",
      "Train Epoch: 583 [45568/54000 (84%)] Loss: -1781.060059\n",
      "    epoch          : 583\n",
      "    loss           : -1747.1518578859839\n",
      "    val_loss       : -1766.490202748403\n",
      "    val_log_likelihood: 1813.2263549804688\n",
      "    val_log_marginal: 1780.2652586709908\n",
      "Train Epoch: 584 [512/54000 (1%)] Loss: -1990.617554\n",
      "Train Epoch: 584 [11776/54000 (22%)] Loss: -1685.548584\n",
      "Train Epoch: 584 [23040/54000 (43%)] Loss: -1524.564453\n",
      "Train Epoch: 584 [34304/54000 (64%)] Loss: -1761.502930\n",
      "Train Epoch: 584 [45568/54000 (84%)] Loss: -1778.885986\n",
      "    epoch          : 584\n",
      "    loss           : -1752.1942211188893\n",
      "    val_loss       : -1763.8694823011756\n",
      "    val_log_likelihood: 1812.8940795898438\n",
      "    val_log_marginal: 1779.824681627378\n",
      "Train Epoch: 585 [512/54000 (1%)] Loss: -1988.286987\n",
      "Train Epoch: 585 [11776/54000 (22%)] Loss: -1732.491455\n",
      "Train Epoch: 585 [23040/54000 (43%)] Loss: -1554.440430\n",
      "Train Epoch: 585 [34304/54000 (64%)] Loss: -1665.761108\n",
      "Train Epoch: 585 [45568/54000 (84%)] Loss: -1684.449829\n",
      "    epoch          : 585\n",
      "    loss           : -1744.395152479115\n",
      "    val_loss       : -1770.5697693638504\n",
      "    val_log_likelihood: 1813.3801635742188\n",
      "    val_log_marginal: 1780.3111187987638\n",
      "Train Epoch: 586 [512/54000 (1%)] Loss: -1896.359619\n",
      "Train Epoch: 586 [11776/54000 (22%)] Loss: -1701.725586\n",
      "Train Epoch: 586 [23040/54000 (43%)] Loss: -1752.583252\n",
      "Train Epoch: 586 [34304/54000 (64%)] Loss: -1690.620239\n",
      "Train Epoch: 586 [45568/54000 (84%)] Loss: -1687.080322\n",
      "    epoch          : 586\n",
      "    loss           : -1746.6130516127785\n",
      "    val_loss       : -1752.5445045268164\n",
      "    val_log_likelihood: 1814.0361572265624\n",
      "    val_log_marginal: 1781.0096899170428\n",
      "Train Epoch: 587 [512/54000 (1%)] Loss: -1988.539551\n",
      "Train Epoch: 587 [11776/54000 (22%)] Loss: -1741.033936\n",
      "Train Epoch: 587 [23040/54000 (43%)] Loss: -1699.433960\n",
      "Train Epoch: 587 [34304/54000 (64%)] Loss: -1541.680298\n",
      "Train Epoch: 587 [45568/54000 (84%)] Loss: -1559.921753\n",
      "    epoch          : 587\n",
      "    loss           : -1753.8584854579208\n",
      "    val_loss       : -1764.5343363117427\n",
      "    val_log_likelihood: 1812.9972778320312\n",
      "    val_log_marginal: 1779.9181367004742\n",
      "Train Epoch: 588 [512/54000 (1%)] Loss: -1993.142334\n",
      "Train Epoch: 588 [11776/54000 (22%)] Loss: -1699.082886\n",
      "Train Epoch: 588 [23040/54000 (43%)] Loss: -1778.956787\n",
      "Train Epoch: 588 [34304/54000 (64%)] Loss: -1783.466064\n",
      "Train Epoch: 588 [45568/54000 (84%)] Loss: -1683.310547\n",
      "    epoch          : 588\n",
      "    loss           : -1759.5704309444616\n",
      "    val_loss       : -1767.4468031005933\n",
      "    val_log_likelihood: 1814.014990234375\n",
      "    val_log_marginal: 1780.9268428963765\n",
      "Train Epoch: 589 [512/54000 (1%)] Loss: -1989.552490\n",
      "Train Epoch: 589 [11776/54000 (22%)] Loss: -1733.495728\n",
      "Train Epoch: 589 [23040/54000 (43%)] Loss: -1684.009766\n",
      "Train Epoch: 589 [34304/54000 (64%)] Loss: -1817.261963\n",
      "Train Epoch: 589 [45568/54000 (84%)] Loss: -1670.821045\n",
      "    epoch          : 589\n",
      "    loss           : -1758.705837136448\n",
      "    val_loss       : -1747.940919888392\n",
      "    val_log_likelihood: 1813.750537109375\n",
      "    val_log_marginal: 1780.7618483077736\n",
      "Train Epoch: 590 [512/54000 (1%)] Loss: -1990.916260\n",
      "Train Epoch: 590 [11776/54000 (22%)] Loss: -1590.541870\n",
      "Train Epoch: 590 [23040/54000 (43%)] Loss: -1776.304810\n",
      "Train Epoch: 590 [34304/54000 (64%)] Loss: -1809.126953\n",
      "Train Epoch: 590 [45568/54000 (84%)] Loss: -1776.441406\n",
      "    epoch          : 590\n",
      "    loss           : -1750.2294184618656\n",
      "    val_loss       : -1752.3850622206926\n",
      "    val_log_likelihood: 1813.9760986328124\n",
      "    val_log_marginal: 1781.0080182831734\n",
      "Train Epoch: 591 [512/54000 (1%)] Loss: -1899.696045\n",
      "Train Epoch: 591 [11776/54000 (22%)] Loss: -1850.935791\n",
      "Train Epoch: 591 [23040/54000 (43%)] Loss: -1529.735352\n",
      "Train Epoch: 591 [34304/54000 (64%)] Loss: -1785.015259\n",
      "Train Epoch: 591 [45568/54000 (84%)] Loss: -1781.433960\n",
      "    epoch          : 591\n",
      "    loss           : -1754.5095069809715\n",
      "    val_loss       : -1761.5573624959216\n",
      "    val_log_likelihood: 1813.157958984375\n",
      "    val_log_marginal: 1780.1261715654284\n",
      "Train Epoch: 592 [512/54000 (1%)] Loss: -1988.734375\n",
      "Train Epoch: 592 [11776/54000 (22%)] Loss: -1773.974854\n",
      "Train Epoch: 592 [23040/54000 (43%)] Loss: -1816.944336\n",
      "Train Epoch: 592 [34304/54000 (64%)] Loss: -1422.612549\n",
      "Train Epoch: 592 [45568/54000 (84%)] Loss: -1777.750488\n",
      "    epoch          : 592\n",
      "    loss           : -1742.008870039836\n",
      "    val_loss       : -1748.9954145437107\n",
      "    val_log_likelihood: 1813.3053833007812\n",
      "    val_log_marginal: 1780.2727850113065\n",
      "Train Epoch: 593 [512/54000 (1%)] Loss: -1847.569458\n",
      "Train Epoch: 593 [11776/54000 (22%)] Loss: -1728.125488\n",
      "Train Epoch: 593 [23040/54000 (43%)] Loss: -1821.213135\n",
      "Train Epoch: 593 [34304/54000 (64%)] Loss: -1817.707520\n",
      "Train Epoch: 593 [45568/54000 (84%)] Loss: -1811.684082\n",
      "    epoch          : 593\n",
      "    loss           : -1743.4033710744122\n",
      "    val_loss       : -1722.5061609454453\n",
      "    val_log_likelihood: 1812.607958984375\n",
      "    val_log_marginal: 1779.5640001667111\n",
      "Train Epoch: 594 [512/54000 (1%)] Loss: -1989.453979\n",
      "Train Epoch: 594 [11776/54000 (22%)] Loss: -1749.410645\n",
      "Train Epoch: 594 [23040/54000 (43%)] Loss: -1813.594116\n",
      "Train Epoch: 594 [34304/54000 (64%)] Loss: -1896.150879\n",
      "Train Epoch: 594 [45568/54000 (84%)] Loss: -1552.163696\n",
      "    epoch          : 594\n",
      "    loss           : -1739.120360119508\n",
      "    val_loss       : -1752.912002277933\n",
      "    val_log_likelihood: 1813.989892578125\n",
      "    val_log_marginal: 1780.9950208965688\n",
      "Train Epoch: 595 [512/54000 (1%)] Loss: -1992.395630\n",
      "Train Epoch: 595 [11776/54000 (22%)] Loss: -1697.904663\n",
      "Train Epoch: 595 [23040/54000 (43%)] Loss: -1686.080933\n",
      "Train Epoch: 595 [34304/54000 (64%)] Loss: -1775.060791\n",
      "Train Epoch: 595 [45568/54000 (84%)] Loss: -1782.991699\n",
      "    epoch          : 595\n",
      "    loss           : -1746.2066142771503\n",
      "    val_loss       : -1738.996451306902\n",
      "    val_log_likelihood: 1812.9630249023437\n",
      "    val_log_marginal: 1779.9046969356611\n",
      "Train Epoch: 596 [512/54000 (1%)] Loss: -1988.743164\n",
      "Train Epoch: 596 [11776/54000 (22%)] Loss: -1779.767090\n",
      "Train Epoch: 596 [23040/54000 (43%)] Loss: -1778.068970\n",
      "Train Epoch: 596 [34304/54000 (64%)] Loss: -1777.715576\n",
      "Train Epoch: 596 [45568/54000 (84%)] Loss: -1778.344971\n",
      "    epoch          : 596\n",
      "    loss           : -1751.818085018951\n",
      "    val_loss       : -1719.126708499901\n",
      "    val_log_likelihood: 1814.2419311523438\n",
      "    val_log_marginal: 1781.294466264789\n",
      "Train Epoch: 597 [512/54000 (1%)] Loss: -1987.871338\n",
      "Train Epoch: 597 [11776/54000 (22%)] Loss: -1729.000000\n",
      "Train Epoch: 597 [23040/54000 (43%)] Loss: -1750.919922\n",
      "Train Epoch: 597 [34304/54000 (64%)] Loss: -1810.288208\n",
      "Train Epoch: 597 [45568/54000 (84%)] Loss: -1776.057373\n",
      "    epoch          : 597\n",
      "    loss           : -1748.2751839515006\n",
      "    val_loss       : -1726.145388093777\n",
      "    val_log_likelihood: 1813.0384155273437\n",
      "    val_log_marginal: 1779.9959372953854\n",
      "Train Epoch: 598 [512/54000 (1%)] Loss: -1784.531738\n",
      "Train Epoch: 598 [11776/54000 (22%)] Loss: -1529.478271\n",
      "Train Epoch: 598 [23040/54000 (43%)] Loss: -1685.918335\n",
      "Train Epoch: 598 [34304/54000 (64%)] Loss: -1779.885010\n",
      "Train Epoch: 598 [45568/54000 (84%)] Loss: -1781.399780\n",
      "    epoch          : 598\n",
      "    loss           : -1747.4932861328125\n",
      "    val_loss       : -1720.4856708951295\n",
      "    val_log_likelihood: 1814.4734008789062\n",
      "    val_log_marginal: 1781.4108235636384\n",
      "Train Epoch: 599 [512/54000 (1%)] Loss: -1585.707031\n",
      "Train Epoch: 599 [11776/54000 (22%)] Loss: -1696.934082\n",
      "Train Epoch: 599 [23040/54000 (43%)] Loss: -1687.111084\n",
      "Train Epoch: 599 [34304/54000 (64%)] Loss: -1773.612793\n",
      "Train Epoch: 599 [45568/54000 (84%)] Loss: -1752.791504\n",
      "    epoch          : 599\n",
      "    loss           : -1753.8948430731746\n",
      "    val_loss       : -1765.7594620233401\n",
      "    val_log_likelihood: 1813.9643310546876\n",
      "    val_log_marginal: 1780.9387563500575\n",
      "Train Epoch: 600 [512/54000 (1%)] Loss: -1902.403564\n",
      "Train Epoch: 600 [11776/54000 (22%)] Loss: -1699.096191\n",
      "Train Epoch: 600 [23040/54000 (43%)] Loss: -1738.820679\n",
      "Train Epoch: 600 [34304/54000 (64%)] Loss: -1819.242065\n",
      "Train Epoch: 600 [45568/54000 (84%)] Loss: -1820.718506\n",
      "    epoch          : 600\n",
      "    loss           : -1757.693278397664\n",
      "    val_loss       : -1737.980737721175\n",
      "    val_log_likelihood: 1814.8224365234375\n",
      "    val_log_marginal: 1781.8008416969872\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch600.pth ...\n",
      "Train Epoch: 601 [512/54000 (1%)] Loss: -1993.713135\n",
      "Train Epoch: 601 [11776/54000 (22%)] Loss: -1596.573608\n",
      "Train Epoch: 601 [23040/54000 (43%)] Loss: -1619.290649\n",
      "Train Epoch: 601 [34304/54000 (64%)] Loss: -1812.518921\n",
      "Train Epoch: 601 [45568/54000 (84%)] Loss: -1683.468872\n",
      "    epoch          : 601\n",
      "    loss           : -1744.6144439395111\n",
      "    val_loss       : -1771.4397912656889\n",
      "    val_log_likelihood: 1813.6586547851562\n",
      "    val_log_marginal: 1780.7133270572886\n",
      "Train Epoch: 602 [512/54000 (1%)] Loss: -1902.225342\n",
      "Train Epoch: 602 [11776/54000 (22%)] Loss: -1849.322266\n",
      "Train Epoch: 602 [23040/54000 (43%)] Loss: -1777.222778\n",
      "Train Epoch: 602 [34304/54000 (64%)] Loss: -1991.635986\n",
      "Train Epoch: 602 [45568/54000 (84%)] Loss: -1683.129517\n",
      "    epoch          : 602\n",
      "    loss           : -1749.6861523920948\n",
      "    val_loss       : -1780.41484450344\n",
      "    val_log_likelihood: 1813.7553344726562\n",
      "    val_log_marginal: 1780.738353562329\n",
      "Train Epoch: 603 [512/54000 (1%)] Loss: -1987.865479\n",
      "Train Epoch: 603 [11776/54000 (22%)] Loss: -1844.765869\n",
      "Train Epoch: 603 [23040/54000 (43%)] Loss: -1814.375000\n",
      "Train Epoch: 603 [34304/54000 (64%)] Loss: -1900.707642\n",
      "Train Epoch: 603 [45568/54000 (84%)] Loss: -1779.976562\n",
      "    epoch          : 603\n",
      "    loss           : -1743.5366960280012\n",
      "    val_loss       : -1726.447816810943\n",
      "    val_log_likelihood: 1813.400439453125\n",
      "    val_log_marginal: 1780.3920178579406\n",
      "Train Epoch: 604 [512/54000 (1%)] Loss: -1988.832031\n",
      "Train Epoch: 604 [11776/54000 (22%)] Loss: -1592.287231\n",
      "Train Epoch: 604 [23040/54000 (43%)] Loss: -1748.059204\n",
      "Train Epoch: 604 [34304/54000 (64%)] Loss: -1537.880981\n",
      "Train Epoch: 604 [45568/54000 (84%)] Loss: -1683.564575\n",
      "    epoch          : 604\n",
      "    loss           : -1748.7113689762532\n",
      "    val_loss       : -1703.1655487198382\n",
      "    val_log_likelihood: 1813.1656372070313\n",
      "    val_log_marginal: 1780.1460322095675\n",
      "Train Epoch: 605 [512/54000 (1%)] Loss: -1844.345093\n",
      "Train Epoch: 605 [11776/54000 (22%)] Loss: -1735.893311\n",
      "Train Epoch: 605 [23040/54000 (43%)] Loss: -1622.864746\n",
      "Train Epoch: 605 [34304/54000 (64%)] Loss: -1776.098022\n",
      "Train Epoch: 605 [45568/54000 (84%)] Loss: -1685.544189\n",
      "    epoch          : 605\n",
      "    loss           : -1753.7114717086943\n",
      "    val_loss       : -1763.8672343619169\n",
      "    val_log_likelihood: 1813.13427734375\n",
      "    val_log_marginal: 1780.1272537503405\n",
      "Train Epoch: 606 [512/54000 (1%)] Loss: -1989.924072\n",
      "Train Epoch: 606 [11776/54000 (22%)] Loss: -1696.843384\n",
      "Train Epoch: 606 [23040/54000 (43%)] Loss: -1682.037720\n",
      "Train Epoch: 606 [34304/54000 (64%)] Loss: -1989.207764\n",
      "Train Epoch: 606 [45568/54000 (84%)] Loss: -1780.261963\n",
      "    epoch          : 606\n",
      "    loss           : -1744.315733050356\n",
      "    val_loss       : -1711.751164847985\n",
      "    val_log_likelihood: 1812.82236328125\n",
      "    val_log_marginal: 1779.795804997906\n",
      "Train Epoch: 607 [512/54000 (1%)] Loss: -1901.103882\n",
      "Train Epoch: 607 [11776/54000 (22%)] Loss: -1732.582031\n",
      "Train Epoch: 607 [23040/54000 (43%)] Loss: -1697.396362\n",
      "Train Epoch: 607 [34304/54000 (64%)] Loss: -1624.876465\n",
      "Train Epoch: 607 [45568/54000 (84%)] Loss: -1778.642334\n",
      "    epoch          : 607\n",
      "    loss           : -1742.6047846728031\n",
      "    val_loss       : -1691.3480247482657\n",
      "    val_log_likelihood: 1812.8188110351562\n",
      "    val_log_marginal: 1779.8406198021025\n",
      "Train Epoch: 608 [512/54000 (1%)] Loss: -1845.778076\n",
      "Train Epoch: 608 [11776/54000 (22%)] Loss: -1817.378174\n",
      "Train Epoch: 608 [23040/54000 (43%)] Loss: -1674.152710\n",
      "Train Epoch: 608 [34304/54000 (64%)] Loss: -1903.955078\n",
      "Train Epoch: 608 [45568/54000 (84%)] Loss: -1777.626465\n",
      "    epoch          : 608\n",
      "    loss           : -1747.845566551284\n",
      "    val_loss       : -1756.52475534603\n",
      "    val_log_likelihood: 1813.1274047851562\n",
      "    val_log_marginal: 1780.1949736643583\n",
      "Train Epoch: 609 [512/54000 (1%)] Loss: -1898.306274\n",
      "Train Epoch: 609 [11776/54000 (22%)] Loss: -1594.981445\n",
      "Train Epoch: 609 [23040/54000 (43%)] Loss: -1469.901123\n",
      "Train Epoch: 609 [34304/54000 (64%)] Loss: -1900.971802\n",
      "Train Epoch: 609 [45568/54000 (84%)] Loss: -1776.855957\n",
      "    epoch          : 609\n",
      "    loss           : -1748.0312161587253\n",
      "    val_loss       : -1722.7118093280121\n",
      "    val_log_likelihood: 1813.4913208007813\n",
      "    val_log_marginal: 1780.4560015702095\n",
      "Train Epoch: 610 [512/54000 (1%)] Loss: -1991.065063\n",
      "Train Epoch: 610 [11776/54000 (22%)] Loss: -1734.085327\n",
      "Train Epoch: 610 [23040/54000 (43%)] Loss: -1622.965210\n",
      "Train Epoch: 610 [34304/54000 (64%)] Loss: -1682.876221\n",
      "Train Epoch: 610 [45568/54000 (84%)] Loss: -1776.113770\n",
      "    epoch          : 610\n",
      "    loss           : -1737.2071895788213\n",
      "    val_loss       : -1733.0976579776034\n",
      "    val_log_likelihood: 1814.5281127929688\n",
      "    val_log_marginal: 1781.5382111183112\n",
      "Train Epoch: 611 [512/54000 (1%)] Loss: -1810.870117\n",
      "Train Epoch: 611 [11776/54000 (22%)] Loss: -1530.392334\n",
      "Train Epoch: 611 [23040/54000 (43%)] Loss: -1685.224854\n",
      "Train Epoch: 611 [34304/54000 (64%)] Loss: -1984.840088\n",
      "Train Epoch: 611 [45568/54000 (84%)] Loss: -1779.224609\n",
      "    epoch          : 611\n",
      "    loss           : -1738.18830977336\n",
      "    val_loss       : -1736.8173541516067\n",
      "    val_log_likelihood: 1813.8870727539063\n",
      "    val_log_marginal: 1780.8070405226451\n",
      "Train Epoch: 612 [512/54000 (1%)] Loss: -1852.318848\n",
      "Train Epoch: 612 [11776/54000 (22%)] Loss: -1694.292969\n",
      "Train Epoch: 612 [23040/54000 (43%)] Loss: -1700.033203\n",
      "Train Epoch: 612 [34304/54000 (64%)] Loss: -1775.597168\n",
      "Train Epoch: 612 [45568/54000 (84%)] Loss: -1770.728882\n",
      "    epoch          : 612\n",
      "    loss           : -1755.8418693920173\n",
      "    val_loss       : -1757.2026395652442\n",
      "    val_log_likelihood: 1814.6547607421876\n",
      "    val_log_marginal: 1781.6308018898285\n",
      "Train Epoch: 613 [512/54000 (1%)] Loss: -1992.850342\n",
      "Train Epoch: 613 [11776/54000 (22%)] Loss: -1736.162842\n",
      "Train Epoch: 613 [23040/54000 (43%)] Loss: -1700.092407\n",
      "Train Epoch: 613 [34304/54000 (64%)] Loss: -1818.277588\n",
      "Train Epoch: 613 [45568/54000 (84%)] Loss: -1753.393921\n",
      "    epoch          : 613\n",
      "    loss           : -1752.752851127398\n",
      "    val_loss       : -1780.824552159384\n",
      "    val_log_likelihood: 1814.1730834960938\n",
      "    val_log_marginal: 1781.2258093098033\n",
      "Train Epoch: 614 [512/54000 (1%)] Loss: -1812.188843\n",
      "Train Epoch: 614 [11776/54000 (22%)] Loss: -1695.455444\n",
      "Train Epoch: 614 [23040/54000 (43%)] Loss: -1694.861328\n",
      "Train Epoch: 614 [34304/54000 (64%)] Loss: -1682.735474\n",
      "Train Epoch: 614 [45568/54000 (84%)] Loss: -1756.676270\n",
      "    epoch          : 614\n",
      "    loss           : -1754.5148563196162\n",
      "    val_loss       : -1742.9244564315304\n",
      "    val_log_likelihood: 1814.308154296875\n",
      "    val_log_marginal: 1781.3553161344862\n",
      "Train Epoch: 615 [512/54000 (1%)] Loss: -1848.654785\n",
      "Train Epoch: 615 [11776/54000 (22%)] Loss: -1846.564331\n",
      "Train Epoch: 615 [23040/54000 (43%)] Loss: -1697.873657\n",
      "Train Epoch: 615 [34304/54000 (64%)] Loss: -1775.908203\n",
      "Train Epoch: 615 [45568/54000 (84%)] Loss: -1782.350586\n",
      "    epoch          : 615\n",
      "    loss           : -1754.908446057008\n",
      "    val_loss       : -1768.338741046004\n",
      "    val_log_likelihood: 1814.358251953125\n",
      "    val_log_marginal: 1781.3586920585483\n",
      "Train Epoch: 616 [512/54000 (1%)] Loss: -1995.575317\n",
      "Train Epoch: 616 [11776/54000 (22%)] Loss: -1850.048706\n",
      "Train Epoch: 616 [23040/54000 (43%)] Loss: -1815.927734\n",
      "Train Epoch: 616 [34304/54000 (64%)] Loss: -1991.792480\n",
      "Train Epoch: 616 [45568/54000 (84%)] Loss: -1556.256104\n",
      "    epoch          : 616\n",
      "    loss           : -1745.3708351059715\n",
      "    val_loss       : -1757.4257504329084\n",
      "    val_log_likelihood: 1813.8851806640625\n",
      "    val_log_marginal: 1780.8839689987951\n",
      "Train Epoch: 617 [512/54000 (1%)] Loss: -1735.903931\n",
      "Train Epoch: 617 [11776/54000 (22%)] Loss: -1850.679688\n",
      "Train Epoch: 617 [23040/54000 (43%)] Loss: -1687.748291\n",
      "Train Epoch: 617 [34304/54000 (64%)] Loss: -1901.813354\n",
      "Train Epoch: 617 [45568/54000 (84%)] Loss: -1777.088623\n",
      "    epoch          : 617\n",
      "    loss           : -1750.8937782816367\n",
      "    val_loss       : -1766.8668766716496\n",
      "    val_log_likelihood: 1815.0026611328126\n",
      "    val_log_marginal: 1781.995557325706\n",
      "Train Epoch: 618 [512/54000 (1%)] Loss: -1903.836548\n",
      "Train Epoch: 618 [11776/54000 (22%)] Loss: -1854.347534\n",
      "Train Epoch: 618 [23040/54000 (43%)] Loss: -1769.977539\n",
      "Train Epoch: 618 [34304/54000 (64%)] Loss: -1775.830811\n",
      "Train Epoch: 618 [45568/54000 (84%)] Loss: -1771.509766\n",
      "    epoch          : 618\n",
      "    loss           : -1750.9313348449102\n",
      "    val_loss       : -1747.4507387692108\n",
      "    val_log_likelihood: 1813.083154296875\n",
      "    val_log_marginal: 1780.0723240155726\n",
      "Train Epoch: 619 [512/54000 (1%)] Loss: -1901.984497\n",
      "Train Epoch: 619 [11776/54000 (22%)] Loss: -1737.436401\n",
      "Train Epoch: 619 [23040/54000 (43%)] Loss: -1776.279785\n",
      "Train Epoch: 619 [34304/54000 (64%)] Loss: -1746.843262\n",
      "Train Epoch: 619 [45568/54000 (84%)] Loss: -1754.072876\n",
      "    epoch          : 619\n",
      "    loss           : -1747.767686900526\n",
      "    val_loss       : -1765.4156763093547\n",
      "    val_log_likelihood: 1814.2795288085938\n",
      "    val_log_marginal: 1781.3163760315626\n",
      "Train Epoch: 620 [512/54000 (1%)] Loss: -1990.085205\n",
      "Train Epoch: 620 [11776/54000 (22%)] Loss: -1850.470215\n",
      "Train Epoch: 620 [23040/54000 (43%)] Loss: -1819.972412\n",
      "Train Epoch: 620 [34304/54000 (64%)] Loss: -1626.320190\n",
      "Train Epoch: 620 [45568/54000 (84%)] Loss: -1679.576416\n",
      "    epoch          : 620\n",
      "    loss           : -1749.6463453840502\n",
      "    val_loss       : -1751.6063655659557\n",
      "    val_log_likelihood: 1815.4538208007812\n",
      "    val_log_marginal: 1782.5414180267603\n",
      "Train Epoch: 621 [512/54000 (1%)] Loss: -1909.961792\n",
      "Train Epoch: 621 [11776/54000 (22%)] Loss: -1846.038086\n",
      "Train Epoch: 621 [23040/54000 (43%)] Loss: -1776.777954\n",
      "Train Epoch: 621 [34304/54000 (64%)] Loss: -1776.971680\n",
      "Train Epoch: 621 [45568/54000 (84%)] Loss: -1775.613770\n",
      "    epoch          : 621\n",
      "    loss           : -1756.6005194635675\n",
      "    val_loss       : -1742.2762632369995\n",
      "    val_log_likelihood: 1813.9979858398438\n",
      "    val_log_marginal: 1781.028011310739\n",
      "Train Epoch: 622 [512/54000 (1%)] Loss: -1988.979004\n",
      "Train Epoch: 622 [11776/54000 (22%)] Loss: -1845.115601\n",
      "Train Epoch: 622 [23040/54000 (43%)] Loss: -1538.443115\n",
      "Train Epoch: 622 [34304/54000 (64%)] Loss: -1690.633667\n",
      "Train Epoch: 622 [45568/54000 (84%)] Loss: -1473.277588\n",
      "    epoch          : 622\n",
      "    loss           : -1756.6031276589572\n",
      "    val_loss       : -1754.4218214077875\n",
      "    val_log_likelihood: 1814.0590576171876\n",
      "    val_log_marginal: 1781.0697668222099\n",
      "Train Epoch: 623 [512/54000 (1%)] Loss: -1986.802979\n",
      "Train Epoch: 623 [11776/54000 (22%)] Loss: -1736.210693\n",
      "Train Epoch: 623 [23040/54000 (43%)] Loss: -1778.134277\n",
      "Train Epoch: 623 [34304/54000 (64%)] Loss: -1818.294922\n",
      "Train Epoch: 623 [45568/54000 (84%)] Loss: -1530.380005\n",
      "    epoch          : 623\n",
      "    loss           : -1757.7586742438893\n",
      "    val_loss       : -1729.0894676448777\n",
      "    val_log_likelihood: 1815.2998046875\n",
      "    val_log_marginal: 1782.3326514939174\n",
      "Train Epoch: 624 [512/54000 (1%)] Loss: -1987.673828\n",
      "Train Epoch: 624 [11776/54000 (22%)] Loss: -1820.925903\n",
      "Train Epoch: 624 [23040/54000 (43%)] Loss: -1771.952881\n",
      "Train Epoch: 624 [34304/54000 (64%)] Loss: -1779.022827\n",
      "Train Epoch: 624 [45568/54000 (84%)] Loss: -1784.370239\n",
      "    epoch          : 624\n",
      "    loss           : -1743.1937545927444\n",
      "    val_loss       : -1735.1901082009076\n",
      "    val_log_likelihood: 1813.6182006835938\n",
      "    val_log_marginal: 1780.6792691048236\n",
      "Train Epoch: 625 [512/54000 (1%)] Loss: -1687.858521\n",
      "Train Epoch: 625 [11776/54000 (22%)] Loss: -1736.249756\n",
      "Train Epoch: 625 [23040/54000 (43%)] Loss: -1811.472168\n",
      "Train Epoch: 625 [34304/54000 (64%)] Loss: -1822.915527\n",
      "Train Epoch: 625 [45568/54000 (84%)] Loss: -1777.186768\n",
      "    epoch          : 625\n",
      "    loss           : -1757.9836401608911\n",
      "    val_loss       : -1728.5823614224792\n",
      "    val_log_likelihood: 1814.126953125\n",
      "    val_log_marginal: 1781.1266515877608\n",
      "Train Epoch: 626 [512/54000 (1%)] Loss: -1845.051392\n",
      "Train Epoch: 626 [11776/54000 (22%)] Loss: -1739.809937\n",
      "Train Epoch: 626 [23040/54000 (43%)] Loss: -1623.101807\n",
      "Train Epoch: 626 [34304/54000 (64%)] Loss: -1779.276611\n",
      "Train Epoch: 626 [45568/54000 (84%)] Loss: -1783.612793\n",
      "    epoch          : 626\n",
      "    loss           : -1749.436927115563\n",
      "    val_loss       : -1780.6101261010394\n",
      "    val_log_likelihood: 1814.3040283203125\n",
      "    val_log_marginal: 1781.2998291160911\n",
      "Train Epoch: 627 [512/54000 (1%)] Loss: -1900.729004\n",
      "Train Epoch: 627 [11776/54000 (22%)] Loss: -1694.318604\n",
      "Train Epoch: 627 [23040/54000 (43%)] Loss: -1822.605835\n",
      "Train Epoch: 627 [34304/54000 (64%)] Loss: -1814.800659\n",
      "Train Epoch: 627 [45568/54000 (84%)] Loss: -1752.314697\n",
      "    epoch          : 627\n",
      "    loss           : -1759.3575258160581\n",
      "    val_loss       : -1767.7713330175727\n",
      "    val_log_likelihood: 1814.1368408203125\n",
      "    val_log_marginal: 1781.2275589171797\n",
      "Train Epoch: 628 [512/54000 (1%)] Loss: -1850.065674\n",
      "Train Epoch: 628 [11776/54000 (22%)] Loss: -1593.952881\n",
      "Train Epoch: 628 [23040/54000 (43%)] Loss: -1753.889526\n",
      "Train Epoch: 628 [34304/54000 (64%)] Loss: -1778.366943\n",
      "Train Epoch: 628 [45568/54000 (84%)] Loss: -1680.548340\n",
      "    epoch          : 628\n",
      "    loss           : -1747.5092350421567\n",
      "    val_loss       : -1764.9697924293578\n",
      "    val_log_likelihood: 1814.0118041992187\n",
      "    val_log_marginal: 1781.117659507243\n",
      "Train Epoch: 629 [512/54000 (1%)] Loss: -1846.687500\n",
      "Train Epoch: 629 [11776/54000 (22%)] Loss: -1847.108154\n",
      "Train Epoch: 629 [23040/54000 (43%)] Loss: -1748.139282\n",
      "Train Epoch: 629 [34304/54000 (64%)] Loss: -1527.872437\n",
      "Train Epoch: 629 [45568/54000 (84%)] Loss: -1782.022095\n",
      "    epoch          : 629\n",
      "    loss           : -1743.8600796236851\n",
      "    val_loss       : -1724.7978352760897\n",
      "    val_log_likelihood: 1814.22705078125\n",
      "    val_log_marginal: 1781.26854607624\n",
      "Train Epoch: 630 [512/54000 (1%)] Loss: -1987.059082\n",
      "Train Epoch: 630 [11776/54000 (22%)] Loss: -1625.794678\n",
      "Train Epoch: 630 [23040/54000 (43%)] Loss: -1745.675781\n",
      "Train Epoch: 630 [34304/54000 (64%)] Loss: -1988.834473\n",
      "Train Epoch: 630 [45568/54000 (84%)] Loss: -1550.082153\n",
      "    epoch          : 630\n",
      "    loss           : -1756.0917098545792\n",
      "    val_loss       : -1768.4408698698505\n",
      "    val_log_likelihood: 1814.6703857421876\n",
      "    val_log_marginal: 1781.7708946544676\n",
      "Train Epoch: 631 [512/54000 (1%)] Loss: -1846.548340\n",
      "Train Epoch: 631 [11776/54000 (22%)] Loss: -1844.088135\n",
      "Train Epoch: 631 [23040/54000 (43%)] Loss: -1753.282959\n",
      "Train Epoch: 631 [34304/54000 (64%)] Loss: -1990.878296\n",
      "Train Epoch: 631 [45568/54000 (84%)] Loss: -1676.831543\n",
      "    epoch          : 631\n",
      "    loss           : -1742.4241097327506\n",
      "    val_loss       : -1683.0022984756156\n",
      "    val_log_likelihood: 1814.6389526367188\n",
      "    val_log_marginal: 1781.6746369596572\n",
      "Train Epoch: 632 [512/54000 (1%)] Loss: -1848.357300\n",
      "Train Epoch: 632 [11776/54000 (22%)] Loss: -1848.535400\n",
      "Train Epoch: 632 [23040/54000 (43%)] Loss: -1698.034058\n",
      "Train Epoch: 632 [34304/54000 (64%)] Loss: -1899.141235\n",
      "Train Epoch: 632 [45568/54000 (84%)] Loss: -1783.972778\n",
      "    epoch          : 632\n",
      "    loss           : -1745.0915164758662\n",
      "    val_loss       : -1758.6745776578784\n",
      "    val_log_likelihood: 1814.828515625\n",
      "    val_log_marginal: 1781.9049032639875\n",
      "Train Epoch: 633 [512/54000 (1%)] Loss: -1902.983643\n",
      "Train Epoch: 633 [11776/54000 (22%)] Loss: -1845.509521\n",
      "Train Epoch: 633 [23040/54000 (43%)] Loss: -1776.994141\n",
      "Train Epoch: 633 [34304/54000 (64%)] Loss: -1986.851562\n",
      "Train Epoch: 633 [45568/54000 (84%)] Loss: -1623.806885\n",
      "    epoch          : 633\n",
      "    loss           : -1745.0057445563893\n",
      "    val_loss       : -1741.5939389912412\n",
      "    val_log_likelihood: 1812.65126953125\n",
      "    val_log_marginal: 1779.7534006934613\n",
      "Train Epoch: 634 [512/54000 (1%)] Loss: -1989.576050\n",
      "Train Epoch: 634 [11776/54000 (22%)] Loss: -1586.380371\n",
      "Train Epoch: 634 [23040/54000 (43%)] Loss: -1747.869873\n",
      "Train Epoch: 634 [34304/54000 (64%)] Loss: -1768.518066\n",
      "Train Epoch: 634 [45568/54000 (84%)] Loss: -1776.364990\n",
      "    epoch          : 634\n",
      "    loss           : -1754.0666987353031\n",
      "    val_loss       : -1764.5707034921274\n",
      "    val_log_likelihood: 1813.497998046875\n",
      "    val_log_marginal: 1780.5427889775485\n",
      "Train Epoch: 635 [512/54000 (1%)] Loss: -1903.322266\n",
      "Train Epoch: 635 [11776/54000 (22%)] Loss: -1847.736938\n",
      "Train Epoch: 635 [23040/54000 (43%)] Loss: -1522.222290\n",
      "Train Epoch: 635 [34304/54000 (64%)] Loss: -1674.497437\n",
      "Train Epoch: 635 [45568/54000 (84%)] Loss: -1773.766479\n",
      "    epoch          : 635\n",
      "    loss           : -1741.056073783648\n",
      "    val_loss       : -1767.5616608301177\n",
      "    val_log_likelihood: 1813.6887451171874\n",
      "    val_log_marginal: 1780.7708225142537\n",
      "Train Epoch: 636 [512/54000 (1%)] Loss: -1990.703125\n",
      "Train Epoch: 636 [11776/54000 (22%)] Loss: -1739.199219\n",
      "Train Epoch: 636 [23040/54000 (43%)] Loss: -1700.181274\n",
      "Train Epoch: 636 [34304/54000 (64%)] Loss: -1524.353027\n",
      "Train Epoch: 636 [45568/54000 (84%)] Loss: -1774.189819\n",
      "    epoch          : 636\n",
      "    loss           : -1754.2376684812036\n",
      "    val_loss       : -1748.1375080209225\n",
      "    val_log_likelihood: 1813.2539672851562\n",
      "    val_log_marginal: 1780.2949727419764\n",
      "Train Epoch: 637 [512/54000 (1%)] Loss: -1992.612061\n",
      "Train Epoch: 637 [11776/54000 (22%)] Loss: -1845.709595\n",
      "Train Epoch: 637 [23040/54000 (43%)] Loss: -1780.887207\n",
      "Train Epoch: 637 [34304/54000 (64%)] Loss: -1989.311035\n",
      "Train Epoch: 637 [45568/54000 (84%)] Loss: -1776.968384\n",
      "    epoch          : 637\n",
      "    loss           : -1757.60405950263\n",
      "    val_loss       : -1771.5015540722757\n",
      "    val_log_likelihood: 1813.777490234375\n",
      "    val_log_marginal: 1780.8072192942905\n",
      "Train Epoch: 638 [512/54000 (1%)] Loss: -1732.594482\n",
      "Train Epoch: 638 [11776/54000 (22%)] Loss: -1739.707764\n",
      "Train Epoch: 638 [23040/54000 (43%)] Loss: -1779.396240\n",
      "Train Epoch: 638 [34304/54000 (64%)] Loss: -1779.214600\n",
      "Train Epoch: 638 [45568/54000 (84%)] Loss: -1686.592896\n",
      "    epoch          : 638\n",
      "    loss           : -1743.5371444248917\n",
      "    val_loss       : -1705.7720224328339\n",
      "    val_log_likelihood: 1814.47333984375\n",
      "    val_log_marginal: 1781.5132787576863\n",
      "Train Epoch: 639 [512/54000 (1%)] Loss: -1990.787842\n",
      "Train Epoch: 639 [11776/54000 (22%)] Loss: -1736.443115\n",
      "Train Epoch: 639 [23040/54000 (43%)] Loss: -1682.868774\n",
      "Train Epoch: 639 [34304/54000 (64%)] Loss: -1754.208862\n",
      "Train Epoch: 639 [45568/54000 (84%)] Loss: -1559.147339\n",
      "    epoch          : 639\n",
      "    loss           : -1744.1218600131497\n",
      "    val_loss       : -1722.395482673496\n",
      "    val_log_likelihood: 1814.5885009765625\n",
      "    val_log_marginal: 1781.7069228056826\n",
      "Train Epoch: 640 [512/54000 (1%)] Loss: -1905.822021\n",
      "Train Epoch: 640 [11776/54000 (22%)] Loss: -1844.576416\n",
      "Train Epoch: 640 [23040/54000 (43%)] Loss: -1776.178711\n",
      "Train Epoch: 640 [34304/54000 (64%)] Loss: -1685.623901\n",
      "Train Epoch: 640 [45568/54000 (84%)] Loss: -1784.681885\n",
      "    epoch          : 640\n",
      "    loss           : -1742.576768931776\n",
      "    val_loss       : -1752.9568227367477\n",
      "    val_log_likelihood: 1815.3107788085938\n",
      "    val_log_marginal: 1782.3458291253833\n",
      "Train Epoch: 641 [512/54000 (1%)] Loss: -1992.666626\n",
      "Train Epoch: 641 [11776/54000 (22%)] Loss: -1563.503906\n",
      "Train Epoch: 641 [23040/54000 (43%)] Loss: -1553.858154\n",
      "Train Epoch: 641 [34304/54000 (64%)] Loss: -1754.939819\n",
      "Train Epoch: 641 [45568/54000 (84%)] Loss: -1747.802002\n",
      "    epoch          : 641\n",
      "    loss           : -1744.8846435546875\n",
      "    val_loss       : -1713.3603679429739\n",
      "    val_log_likelihood: 1813.9222534179687\n",
      "    val_log_marginal: 1781.0014051120731\n",
      "Train Epoch: 642 [512/54000 (1%)] Loss: -1841.799561\n",
      "Train Epoch: 642 [11776/54000 (22%)] Loss: -1732.547363\n",
      "Train Epoch: 642 [23040/54000 (43%)] Loss: -1815.199463\n",
      "Train Epoch: 642 [34304/54000 (64%)] Loss: -1751.164795\n",
      "Train Epoch: 642 [45568/54000 (84%)] Loss: -1626.437134\n",
      "    epoch          : 642\n",
      "    loss           : -1756.8362686610458\n",
      "    val_loss       : -1780.6305150570347\n",
      "    val_log_likelihood: 1814.08251953125\n",
      "    val_log_marginal: 1781.1688175185664\n",
      "Train Epoch: 643 [512/54000 (1%)] Loss: -1991.443115\n",
      "Train Epoch: 643 [11776/54000 (22%)] Loss: -1593.760986\n",
      "Train Epoch: 643 [23040/54000 (43%)] Loss: -1689.737305\n",
      "Train Epoch: 643 [34304/54000 (64%)] Loss: -1533.280273\n",
      "Train Epoch: 643 [45568/54000 (84%)] Loss: -1632.070557\n",
      "    epoch          : 643\n",
      "    loss           : -1754.3427806892018\n",
      "    val_loss       : -1766.9847247745843\n",
      "    val_log_likelihood: 1814.5287475585938\n",
      "    val_log_marginal: 1781.600134758513\n",
      "Train Epoch: 644 [512/54000 (1%)] Loss: -1991.354736\n",
      "Train Epoch: 644 [11776/54000 (22%)] Loss: -1700.344849\n",
      "Train Epoch: 644 [23040/54000 (43%)] Loss: -1752.388672\n",
      "Train Epoch: 644 [34304/54000 (64%)] Loss: -1623.170898\n",
      "Train Epoch: 644 [45568/54000 (84%)] Loss: -1555.740967\n",
      "    epoch          : 644\n",
      "    loss           : -1750.6201075185643\n",
      "    val_loss       : -1715.4615722108633\n",
      "    val_log_likelihood: 1814.929931640625\n",
      "    val_log_marginal: 1781.924725240847\n",
      "Train Epoch: 645 [512/54000 (1%)] Loss: -1989.713623\n",
      "Train Epoch: 645 [11776/54000 (22%)] Loss: -1732.462524\n",
      "Train Epoch: 645 [23040/54000 (43%)] Loss: -1776.441895\n",
      "Train Epoch: 645 [34304/54000 (64%)] Loss: -1676.121338\n",
      "Train Epoch: 645 [45568/54000 (84%)] Loss: -1568.968750\n",
      "    epoch          : 645\n",
      "    loss           : -1758.4366261699413\n",
      "    val_loss       : -1706.0910265408456\n",
      "    val_log_likelihood: 1814.7107055664062\n",
      "    val_log_marginal: 1781.742078578945\n",
      "Train Epoch: 646 [512/54000 (1%)] Loss: -1990.700317\n",
      "Train Epoch: 646 [11776/54000 (22%)] Loss: -1844.796509\n",
      "Train Epoch: 646 [23040/54000 (43%)] Loss: -1778.958618\n",
      "Train Epoch: 646 [34304/54000 (64%)] Loss: -1698.747314\n",
      "Train Epoch: 646 [45568/54000 (84%)] Loss: -1633.486816\n",
      "    epoch          : 646\n",
      "    loss           : -1750.9300512937036\n",
      "    val_loss       : -1767.2029939798638\n",
      "    val_log_likelihood: 1814.9235473632812\n",
      "    val_log_marginal: 1782.0222784118705\n",
      "Train Epoch: 647 [512/54000 (1%)] Loss: -1991.480713\n",
      "Train Epoch: 647 [11776/54000 (22%)] Loss: -1694.608887\n",
      "Train Epoch: 647 [23040/54000 (43%)] Loss: -1750.737549\n",
      "Train Epoch: 647 [34304/54000 (64%)] Loss: -1989.639648\n",
      "Train Epoch: 647 [45568/54000 (84%)] Loss: -1776.874756\n",
      "    epoch          : 647\n",
      "    loss           : -1754.9629824798885\n",
      "    val_loss       : -1728.7127549400552\n",
      "    val_log_likelihood: 1814.6846069335938\n",
      "    val_log_marginal: 1781.7213465327666\n",
      "Train Epoch: 648 [512/54000 (1%)] Loss: -1596.806152\n",
      "Train Epoch: 648 [11776/54000 (22%)] Loss: -1849.880005\n",
      "Train Epoch: 648 [23040/54000 (43%)] Loss: -1684.400146\n",
      "Train Epoch: 648 [34304/54000 (64%)] Loss: -1754.635132\n",
      "Train Epoch: 648 [45568/54000 (84%)] Loss: -1783.725586\n",
      "    epoch          : 648\n",
      "    loss           : -1761.6305971051206\n",
      "    val_loss       : -1758.5161013741047\n",
      "    val_log_likelihood: 1814.534228515625\n",
      "    val_log_marginal: 1781.5885499598257\n",
      "Train Epoch: 649 [512/54000 (1%)] Loss: -1989.940918\n",
      "Train Epoch: 649 [11776/54000 (22%)] Loss: -1848.068726\n",
      "Train Epoch: 649 [23040/54000 (43%)] Loss: -1697.521118\n",
      "Train Epoch: 649 [34304/54000 (64%)] Loss: -1688.105225\n",
      "Train Epoch: 649 [45568/54000 (84%)] Loss: -1783.832031\n",
      "    epoch          : 649\n",
      "    loss           : -1752.5242158493193\n",
      "    val_loss       : -1765.5728814940899\n",
      "    val_log_likelihood: 1813.5933349609375\n",
      "    val_log_marginal: 1780.598425525098\n",
      "Train Epoch: 650 [512/54000 (1%)] Loss: -1898.791992\n",
      "Train Epoch: 650 [11776/54000 (22%)] Loss: -1736.753906\n",
      "Train Epoch: 650 [23040/54000 (43%)] Loss: -1818.099121\n",
      "Train Epoch: 650 [34304/54000 (64%)] Loss: -1758.430054\n",
      "Train Epoch: 650 [45568/54000 (84%)] Loss: -1783.290039\n",
      "    epoch          : 650\n",
      "    loss           : -1757.8279848004331\n",
      "    val_loss       : -1768.1113003822043\n",
      "    val_log_likelihood: 1814.2919799804688\n",
      "    val_log_marginal: 1781.3357927773147\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch650.pth ...\n",
      "Train Epoch: 651 [512/54000 (1%)] Loss: -1988.393677\n",
      "Train Epoch: 651 [11776/54000 (22%)] Loss: -1841.430908\n",
      "Train Epoch: 651 [23040/54000 (43%)] Loss: -1688.090820\n",
      "Train Epoch: 651 [34304/54000 (64%)] Loss: -1992.608765\n",
      "Train Epoch: 651 [45568/54000 (84%)] Loss: -1550.521851\n",
      "    epoch          : 651\n",
      "    loss           : -1757.4625461691678\n",
      "    val_loss       : -1714.8242780847474\n",
      "    val_log_likelihood: 1814.2208251953125\n",
      "    val_log_marginal: 1781.299748910591\n",
      "Train Epoch: 652 [512/54000 (1%)] Loss: -1991.709839\n",
      "Train Epoch: 652 [11776/54000 (22%)] Loss: -1841.265869\n",
      "Train Epoch: 652 [23040/54000 (43%)] Loss: -1526.645752\n",
      "Train Epoch: 652 [34304/54000 (64%)] Loss: -1774.582520\n",
      "Train Epoch: 652 [45568/54000 (84%)] Loss: -1781.524536\n",
      "    epoch          : 652\n",
      "    loss           : -1751.2633008295948\n",
      "    val_loss       : -1780.9174614043907\n",
      "    val_log_likelihood: 1814.5965698242187\n",
      "    val_log_marginal: 1781.5427331167225\n",
      "Train Epoch: 653 [512/54000 (1%)] Loss: -1904.172607\n",
      "Train Epoch: 653 [11776/54000 (22%)] Loss: -1817.261719\n",
      "Train Epoch: 653 [23040/54000 (43%)] Loss: -1693.647705\n",
      "Train Epoch: 653 [34304/54000 (64%)] Loss: -1988.705200\n",
      "Train Epoch: 653 [45568/54000 (84%)] Loss: -1777.822998\n",
      "    epoch          : 653\n",
      "    loss           : -1751.4934468788676\n",
      "    val_loss       : -1782.401090631634\n",
      "    val_log_likelihood: 1815.8681518554688\n",
      "    val_log_marginal: 1782.9220927562565\n",
      "Train Epoch: 654 [512/54000 (1%)] Loss: -1822.691406\n",
      "Train Epoch: 654 [11776/54000 (22%)] Loss: -1676.096069\n",
      "Train Epoch: 654 [23040/54000 (43%)] Loss: -1686.565552\n",
      "Train Epoch: 654 [34304/54000 (64%)] Loss: -1689.765137\n",
      "Train Epoch: 654 [45568/54000 (84%)] Loss: -1781.753174\n",
      "    epoch          : 654\n",
      "    loss           : -1743.2608618405786\n",
      "    val_loss       : -1736.3205918019637\n",
      "    val_log_likelihood: 1814.9050537109374\n",
      "    val_log_marginal: 1781.9400268759578\n",
      "Train Epoch: 655 [512/54000 (1%)] Loss: -1990.074951\n",
      "Train Epoch: 655 [11776/54000 (22%)] Loss: -1728.498779\n",
      "Train Epoch: 655 [23040/54000 (43%)] Loss: -1536.266724\n",
      "Train Epoch: 655 [34304/54000 (64%)] Loss: -1903.296387\n",
      "Train Epoch: 655 [45568/54000 (84%)] Loss: -1691.856201\n",
      "    epoch          : 655\n",
      "    loss           : -1751.4160265025525\n",
      "    val_loss       : -1729.5559231201187\n",
      "    val_log_likelihood: 1814.523388671875\n",
      "    val_log_marginal: 1781.5557729113848\n",
      "Train Epoch: 656 [512/54000 (1%)] Loss: -1905.402588\n",
      "Train Epoch: 656 [11776/54000 (22%)] Loss: -1734.634766\n",
      "Train Epoch: 656 [23040/54000 (43%)] Loss: -1541.639526\n",
      "Train Epoch: 656 [34304/54000 (64%)] Loss: -1782.076904\n",
      "Train Epoch: 656 [45568/54000 (84%)] Loss: -1774.576294\n",
      "    epoch          : 656\n",
      "    loss           : -1738.0458041653774\n",
      "    val_loss       : -1735.4733041144907\n",
      "    val_log_likelihood: 1814.1765869140625\n",
      "    val_log_marginal: 1781.24946529679\n",
      "Train Epoch: 657 [512/54000 (1%)] Loss: -1848.403564\n",
      "Train Epoch: 657 [11776/54000 (22%)] Loss: -1734.216431\n",
      "Train Epoch: 657 [23040/54000 (43%)] Loss: -1692.057129\n",
      "Train Epoch: 657 [34304/54000 (64%)] Loss: -1526.097900\n",
      "Train Epoch: 657 [45568/54000 (84%)] Loss: -1779.486938\n",
      "    epoch          : 657\n",
      "    loss           : -1757.6688437886758\n",
      "    val_loss       : -1744.4001813566313\n",
      "    val_log_likelihood: 1814.8553955078125\n",
      "    val_log_marginal: 1781.8331681264012\n",
      "Train Epoch: 658 [512/54000 (1%)] Loss: -1992.132812\n",
      "Train Epoch: 658 [11776/54000 (22%)] Loss: -1694.081543\n",
      "Train Epoch: 658 [23040/54000 (43%)] Loss: -1633.230835\n",
      "Train Epoch: 658 [34304/54000 (64%)] Loss: -1822.128540\n",
      "Train Epoch: 658 [45568/54000 (84%)] Loss: -1777.361206\n",
      "    epoch          : 658\n",
      "    loss           : -1747.5444529316212\n",
      "    val_loss       : -1753.9452151877806\n",
      "    val_log_likelihood: 1814.2627807617187\n",
      "    val_log_marginal: 1781.2029737991136\n",
      "Train Epoch: 659 [512/54000 (1%)] Loss: -1992.940308\n",
      "Train Epoch: 659 [11776/54000 (22%)] Loss: -1730.138428\n",
      "Train Epoch: 659 [23040/54000 (43%)] Loss: -1779.018555\n",
      "Train Epoch: 659 [34304/54000 (64%)] Loss: -1775.523315\n",
      "Train Epoch: 659 [45568/54000 (84%)] Loss: -1780.403687\n",
      "    epoch          : 659\n",
      "    loss           : -1741.9771740601795\n",
      "    val_loss       : -1738.7387364067138\n",
      "    val_log_likelihood: 1815.2325927734375\n",
      "    val_log_marginal: 1782.256767133996\n",
      "Train Epoch: 660 [512/54000 (1%)] Loss: -1988.304810\n",
      "Train Epoch: 660 [11776/54000 (22%)] Loss: -1726.365234\n",
      "Train Epoch: 660 [23040/54000 (43%)] Loss: -1775.922607\n",
      "Train Epoch: 660 [34304/54000 (64%)] Loss: -1990.876587\n",
      "Train Epoch: 660 [45568/54000 (84%)] Loss: -1782.975342\n",
      "    epoch          : 660\n",
      "    loss           : -1764.7810227800123\n",
      "    val_loss       : -1746.2356433089822\n",
      "    val_log_likelihood: 1814.7463623046874\n",
      "    val_log_marginal: 1781.7145290199667\n",
      "Train Epoch: 661 [512/54000 (1%)] Loss: -1992.377075\n",
      "Train Epoch: 661 [11776/54000 (22%)] Loss: -1742.871704\n",
      "Train Epoch: 661 [23040/54000 (43%)] Loss: -1699.139526\n",
      "Train Epoch: 661 [34304/54000 (64%)] Loss: -1564.673950\n",
      "Train Epoch: 661 [45568/54000 (84%)] Loss: -1774.385132\n",
      "    epoch          : 661\n",
      "    loss           : -1747.3567087909962\n",
      "    val_loss       : -1750.3262088665738\n",
      "    val_log_likelihood: 1814.9873779296875\n",
      "    val_log_marginal: 1781.9566691725909\n",
      "Train Epoch: 662 [512/54000 (1%)] Loss: -1986.707520\n",
      "Train Epoch: 662 [11776/54000 (22%)] Loss: -1731.527100\n",
      "Train Epoch: 662 [23040/54000 (43%)] Loss: -1695.060913\n",
      "Train Epoch: 662 [34304/54000 (64%)] Loss: -1908.205078\n",
      "Train Epoch: 662 [45568/54000 (84%)] Loss: -1678.142090\n",
      "    epoch          : 662\n",
      "    loss           : -1759.8124722018101\n",
      "    val_loss       : -1744.7715290756896\n",
      "    val_log_likelihood: 1815.4197021484374\n",
      "    val_log_marginal: 1782.4894513454287\n",
      "Train Epoch: 663 [512/54000 (1%)] Loss: -1907.474854\n",
      "Train Epoch: 663 [11776/54000 (22%)] Loss: -1785.026611\n",
      "Train Epoch: 663 [23040/54000 (43%)] Loss: -1700.538574\n",
      "Train Epoch: 663 [34304/54000 (64%)] Loss: -1757.105835\n",
      "Train Epoch: 663 [45568/54000 (84%)] Loss: -1780.548828\n",
      "    epoch          : 663\n",
      "    loss           : -1761.5621144511913\n",
      "    val_loss       : -1780.9530805537477\n",
      "    val_log_likelihood: 1814.378515625\n",
      "    val_log_marginal: 1781.3514946121722\n",
      "Train Epoch: 664 [512/54000 (1%)] Loss: -1990.956665\n",
      "Train Epoch: 664 [11776/54000 (22%)] Loss: -1843.036743\n",
      "Train Epoch: 664 [23040/54000 (43%)] Loss: -1700.129272\n",
      "Train Epoch: 664 [34304/54000 (64%)] Loss: -1751.855957\n",
      "Train Epoch: 664 [45568/54000 (84%)] Loss: -1779.953979\n",
      "    epoch          : 664\n",
      "    loss           : -1750.220080687268\n",
      "    val_loss       : -1765.7741970872507\n",
      "    val_log_likelihood: 1813.7380737304688\n",
      "    val_log_marginal: 1780.7082701910288\n",
      "Train Epoch: 665 [512/54000 (1%)] Loss: -1991.119873\n",
      "Train Epoch: 665 [11776/54000 (22%)] Loss: -1725.538208\n",
      "Train Epoch: 665 [23040/54000 (43%)] Loss: -1533.457031\n",
      "Train Epoch: 665 [34304/54000 (64%)] Loss: -1479.346191\n",
      "Train Epoch: 665 [45568/54000 (84%)] Loss: -1690.140747\n",
      "    epoch          : 665\n",
      "    loss           : -1756.0732095548422\n",
      "    val_loss       : -1758.9024080237373\n",
      "    val_log_likelihood: 1815.274365234375\n",
      "    val_log_marginal: 1782.3103624146431\n",
      "Train Epoch: 666 [512/54000 (1%)] Loss: -1907.027832\n",
      "Train Epoch: 666 [11776/54000 (22%)] Loss: -1692.439697\n",
      "Train Epoch: 666 [23040/54000 (43%)] Loss: -1691.041992\n",
      "Train Epoch: 666 [34304/54000 (64%)] Loss: -1534.121826\n",
      "Train Epoch: 666 [45568/54000 (84%)] Loss: -1675.025391\n",
      "    epoch          : 666\n",
      "    loss           : -1748.3760563312192\n",
      "    val_loss       : -1754.2429800087587\n",
      "    val_log_likelihood: 1814.5252807617187\n",
      "    val_log_marginal: 1781.5658926417686\n",
      "Train Epoch: 667 [512/54000 (1%)] Loss: -1989.567505\n",
      "Train Epoch: 667 [11776/54000 (22%)] Loss: -1849.589355\n",
      "Train Epoch: 667 [23040/54000 (43%)] Loss: -1684.423950\n",
      "Train Epoch: 667 [34304/54000 (64%)] Loss: -1780.129639\n",
      "Train Epoch: 667 [45568/54000 (84%)] Loss: -1777.786255\n",
      "    epoch          : 667\n",
      "    loss           : -1748.1608052773051\n",
      "    val_loss       : -1728.12335599456\n",
      "    val_log_likelihood: 1814.424365234375\n",
      "    val_log_marginal: 1781.3608626832574\n",
      "Train Epoch: 668 [512/54000 (1%)] Loss: -1846.879028\n",
      "Train Epoch: 668 [11776/54000 (22%)] Loss: -1602.213989\n",
      "Train Epoch: 668 [23040/54000 (43%)] Loss: -1750.834595\n",
      "Train Epoch: 668 [34304/54000 (64%)] Loss: -1820.890259\n",
      "Train Epoch: 668 [45568/54000 (84%)] Loss: -1550.295776\n",
      "    epoch          : 668\n",
      "    loss           : -1739.293670956451\n",
      "    val_loss       : -1742.5048438334838\n",
      "    val_log_likelihood: 1814.0717041015625\n",
      "    val_log_marginal: 1781.039983507329\n",
      "Train Epoch: 669 [512/54000 (1%)] Loss: -1912.607178\n",
      "Train Epoch: 669 [11776/54000 (22%)] Loss: -1844.276001\n",
      "Train Epoch: 669 [23040/54000 (43%)] Loss: -1784.002563\n",
      "Train Epoch: 669 [34304/54000 (64%)] Loss: -1626.479248\n",
      "Train Epoch: 669 [45568/54000 (84%)] Loss: -1748.384277\n",
      "    epoch          : 669\n",
      "    loss           : -1757.215631768255\n",
      "    val_loss       : -1752.7355903660878\n",
      "    val_log_likelihood: 1814.1613647460938\n",
      "    val_log_marginal: 1781.128296698934\n",
      "Train Epoch: 670 [512/54000 (1%)] Loss: -1844.227051\n",
      "Train Epoch: 670 [11776/54000 (22%)] Loss: -1783.562866\n",
      "Train Epoch: 670 [23040/54000 (43%)] Loss: -1536.729492\n",
      "Train Epoch: 670 [34304/54000 (64%)] Loss: -1816.290039\n",
      "Train Epoch: 670 [45568/54000 (84%)] Loss: -1775.965332\n",
      "    epoch          : 670\n",
      "    loss           : -1750.835452844601\n",
      "    val_loss       : -1765.1048902342095\n",
      "    val_log_likelihood: 1814.2425415039063\n",
      "    val_log_marginal: 1781.2321946050972\n",
      "Train Epoch: 671 [512/54000 (1%)] Loss: -1909.612305\n",
      "Train Epoch: 671 [11776/54000 (22%)] Loss: -1732.787354\n",
      "Train Epoch: 671 [23040/54000 (43%)] Loss: -1775.943359\n",
      "Train Epoch: 671 [34304/54000 (64%)] Loss: -1683.707642\n",
      "Train Epoch: 671 [45568/54000 (84%)] Loss: -1779.531738\n",
      "    epoch          : 671\n",
      "    loss           : -1747.3594511428682\n",
      "    val_loss       : -1768.0969881653787\n",
      "    val_log_likelihood: 1815.8997680664063\n",
      "    val_log_marginal: 1782.9353869926185\n",
      "Train Epoch: 672 [512/54000 (1%)] Loss: -1990.209961\n",
      "Train Epoch: 672 [11776/54000 (22%)] Loss: -1734.262207\n",
      "Train Epoch: 672 [23040/54000 (43%)] Loss: -1626.060303\n",
      "Train Epoch: 672 [34304/54000 (64%)] Loss: -1542.759277\n",
      "Train Epoch: 672 [45568/54000 (84%)] Loss: -1751.776611\n",
      "    epoch          : 672\n",
      "    loss           : -1752.2499250657488\n",
      "    val_loss       : -1750.6759534709156\n",
      "    val_log_likelihood: 1814.5968872070312\n",
      "    val_log_marginal: 1781.5654590096328\n",
      "Train Epoch: 673 [512/54000 (1%)] Loss: -1991.896240\n",
      "Train Epoch: 673 [11776/54000 (22%)] Loss: -1461.127808\n",
      "Train Epoch: 673 [23040/54000 (43%)] Loss: -1687.777588\n",
      "Train Epoch: 673 [34304/54000 (64%)] Loss: -1989.923340\n",
      "Train Epoch: 673 [45568/54000 (84%)] Loss: -1783.046631\n",
      "    epoch          : 673\n",
      "    loss           : -1748.0640506555537\n",
      "    val_loss       : -1754.2198548853398\n",
      "    val_log_likelihood: 1815.2803955078125\n",
      "    val_log_marginal: 1782.266338592409\n",
      "Train Epoch: 674 [512/54000 (1%)] Loss: -1908.393799\n",
      "Train Epoch: 674 [11776/54000 (22%)] Loss: -1690.332764\n",
      "Train Epoch: 674 [23040/54000 (43%)] Loss: -1699.081543\n",
      "Train Epoch: 674 [34304/54000 (64%)] Loss: -1779.828613\n",
      "Train Epoch: 674 [45568/54000 (84%)] Loss: -1775.832275\n",
      "    epoch          : 674\n",
      "    loss           : -1754.462755259901\n",
      "    val_loss       : -1782.3837452221662\n",
      "    val_log_likelihood: 1815.9101928710938\n",
      "    val_log_marginal: 1782.9413016509275\n",
      "Train Epoch: 675 [512/54000 (1%)] Loss: -1847.125732\n",
      "Train Epoch: 675 [11776/54000 (22%)] Loss: -1598.022705\n",
      "Train Epoch: 675 [23040/54000 (43%)] Loss: -1697.866943\n",
      "Train Epoch: 675 [34304/54000 (64%)] Loss: -1753.096558\n",
      "Train Epoch: 675 [45568/54000 (84%)] Loss: -1682.629150\n",
      "    epoch          : 675\n",
      "    loss           : -1752.0977227239325\n",
      "    val_loss       : -1767.3653650576248\n",
      "    val_log_likelihood: 1815.5380126953125\n",
      "    val_log_marginal: 1782.6008464109436\n",
      "Train Epoch: 676 [512/54000 (1%)] Loss: -1994.243164\n",
      "Train Epoch: 676 [11776/54000 (22%)] Loss: -1679.261475\n",
      "Train Epoch: 676 [23040/54000 (43%)] Loss: -1699.985229\n",
      "Train Epoch: 676 [34304/54000 (64%)] Loss: -1823.785156\n",
      "Train Epoch: 676 [45568/54000 (84%)] Loss: -1783.573853\n",
      "    epoch          : 676\n",
      "    loss           : -1762.0048973159035\n",
      "    val_loss       : -1767.3887008465827\n",
      "    val_log_likelihood: 1815.1287719726563\n",
      "    val_log_marginal: 1782.1525921519853\n",
      "Train Epoch: 677 [512/54000 (1%)] Loss: -1755.698242\n",
      "Train Epoch: 677 [11776/54000 (22%)] Loss: -1594.431641\n",
      "Train Epoch: 677 [23040/54000 (43%)] Loss: -1774.772461\n",
      "Train Epoch: 677 [34304/54000 (64%)] Loss: -1992.479980\n",
      "Train Epoch: 677 [45568/54000 (84%)] Loss: -1777.648926\n",
      "    epoch          : 677\n",
      "    loss           : -1745.00206431776\n",
      "    val_loss       : -1742.9381724370644\n",
      "    val_log_likelihood: 1815.0016845703126\n",
      "    val_log_marginal: 1781.984824225679\n",
      "Train Epoch: 678 [512/54000 (1%)] Loss: -1990.821411\n",
      "Train Epoch: 678 [11776/54000 (22%)] Loss: -1699.917969\n",
      "Train Epoch: 678 [23040/54000 (43%)] Loss: -1816.699097\n",
      "Train Epoch: 678 [34304/54000 (64%)] Loss: -1913.143555\n",
      "Train Epoch: 678 [45568/54000 (84%)] Loss: -1759.158569\n",
      "    epoch          : 678\n",
      "    loss           : -1745.918293867961\n",
      "    val_loss       : -1732.2711901266127\n",
      "    val_log_likelihood: 1815.1323364257812\n",
      "    val_log_marginal: 1782.1389880802483\n",
      "Train Epoch: 679 [512/54000 (1%)] Loss: -1990.028564\n",
      "Train Epoch: 679 [11776/54000 (22%)] Loss: -1850.632324\n",
      "Train Epoch: 679 [23040/54000 (43%)] Loss: -1752.809082\n",
      "Train Epoch: 679 [34304/54000 (64%)] Loss: -1825.320679\n",
      "Train Epoch: 679 [45568/54000 (84%)] Loss: -1782.464355\n",
      "    epoch          : 679\n",
      "    loss           : -1752.6885792949413\n",
      "    val_loss       : -1781.8964739266783\n",
      "    val_log_likelihood: 1815.34970703125\n",
      "    val_log_marginal: 1782.3269373685785\n",
      "Train Epoch: 680 [512/54000 (1%)] Loss: -1632.078979\n",
      "Train Epoch: 680 [11776/54000 (22%)] Loss: -1732.188965\n",
      "Train Epoch: 680 [23040/54000 (43%)] Loss: -1702.348022\n",
      "Train Epoch: 680 [34304/54000 (64%)] Loss: -1633.854980\n",
      "Train Epoch: 680 [45568/54000 (84%)] Loss: -1787.111816\n",
      "    epoch          : 680\n",
      "    loss           : -1754.8127949025372\n",
      "    val_loss       : -1753.6084443729371\n",
      "    val_log_likelihood: 1814.9715698242187\n",
      "    val_log_marginal: 1781.9841442939621\n",
      "Train Epoch: 681 [512/54000 (1%)] Loss: -1742.844971\n",
      "Train Epoch: 681 [11776/54000 (22%)] Loss: -1704.303955\n",
      "Train Epoch: 681 [23040/54000 (43%)] Loss: -1775.124512\n",
      "Train Epoch: 681 [34304/54000 (64%)] Loss: -1812.507324\n",
      "Train Epoch: 681 [45568/54000 (84%)] Loss: -1306.004272\n",
      "    epoch          : 681\n",
      "    loss           : -1744.9748982344524\n",
      "    val_loss       : -1781.6207015063615\n",
      "    val_log_likelihood: 1815.168896484375\n",
      "    val_log_marginal: 1782.149660198763\n",
      "Train Epoch: 682 [512/54000 (1%)] Loss: -1847.108276\n",
      "Train Epoch: 682 [11776/54000 (22%)] Loss: -1818.741699\n",
      "Train Epoch: 682 [23040/54000 (43%)] Loss: -1681.199585\n",
      "Train Epoch: 682 [34304/54000 (64%)] Loss: -1684.697998\n",
      "Train Epoch: 682 [45568/54000 (84%)] Loss: -1784.066528\n",
      "    epoch          : 682\n",
      "    loss           : -1757.1111492496907\n",
      "    val_loss       : -1774.5032088289038\n",
      "    val_log_likelihood: 1816.0847412109374\n",
      "    val_log_marginal: 1783.1099224645645\n",
      "Train Epoch: 683 [512/54000 (1%)] Loss: -1735.137695\n",
      "Train Epoch: 683 [11776/54000 (22%)] Loss: -1843.042725\n",
      "Train Epoch: 683 [23040/54000 (43%)] Loss: -1757.623657\n",
      "Train Epoch: 683 [34304/54000 (64%)] Loss: -1681.858643\n",
      "Train Epoch: 683 [45568/54000 (84%)] Loss: -1755.903442\n",
      "    epoch          : 683\n",
      "    loss           : -1766.0691280553838\n",
      "    val_loss       : -1768.0552318500354\n",
      "    val_log_likelihood: 1815.6792358398438\n",
      "    val_log_marginal: 1782.6587298143654\n",
      "Train Epoch: 684 [512/54000 (1%)] Loss: -1845.018677\n",
      "Train Epoch: 684 [11776/54000 (22%)] Loss: -1734.009766\n",
      "Train Epoch: 684 [23040/54000 (43%)] Loss: -1752.710205\n",
      "Train Epoch: 684 [34304/54000 (64%)] Loss: -1777.084229\n",
      "Train Epoch: 684 [45568/54000 (84%)] Loss: -1778.249146\n",
      "    epoch          : 684\n",
      "    loss           : -1748.7833203608448\n",
      "    val_loss       : -1748.0225657498463\n",
      "    val_log_likelihood: 1816.1919555664062\n",
      "    val_log_marginal: 1783.1827871311955\n",
      "Train Epoch: 685 [512/54000 (1%)] Loss: -1993.346191\n",
      "Train Epoch: 685 [11776/54000 (22%)] Loss: -1847.540039\n",
      "Train Epoch: 685 [23040/54000 (43%)] Loss: -1776.470581\n",
      "Train Epoch: 685 [34304/54000 (64%)] Loss: -1755.403809\n",
      "Train Epoch: 685 [45568/54000 (84%)] Loss: -1782.573242\n",
      "    epoch          : 685\n",
      "    loss           : -1752.796742052135\n",
      "    val_loss       : -1781.616157701984\n",
      "    val_log_likelihood: 1815.0788208007812\n",
      "    val_log_marginal: 1782.129430651298\n",
      "Train Epoch: 686 [512/54000 (1%)] Loss: -1989.397461\n",
      "Train Epoch: 686 [11776/54000 (22%)] Loss: -1742.191895\n",
      "Train Epoch: 686 [23040/54000 (43%)] Loss: -1698.699829\n",
      "Train Epoch: 686 [34304/54000 (64%)] Loss: -1776.245117\n",
      "Train Epoch: 686 [45568/54000 (84%)] Loss: -1687.328613\n",
      "    epoch          : 686\n",
      "    loss           : -1760.1654379060953\n",
      "    val_loss       : -1767.7492348944768\n",
      "    val_log_likelihood: 1815.2343627929688\n",
      "    val_log_marginal: 1782.2070709628238\n",
      "Train Epoch: 687 [512/54000 (1%)] Loss: -1990.712769\n",
      "Train Epoch: 687 [11776/54000 (22%)] Loss: -1845.053467\n",
      "Train Epoch: 687 [23040/54000 (43%)] Loss: -1697.718018\n",
      "Train Epoch: 687 [34304/54000 (64%)] Loss: -1820.976807\n",
      "Train Epoch: 687 [45568/54000 (84%)] Loss: -1786.767090\n",
      "    epoch          : 687\n",
      "    loss           : -1755.5992467899134\n",
      "    val_loss       : -1745.3790359510108\n",
      "    val_log_likelihood: 1815.989892578125\n",
      "    val_log_marginal: 1782.9967192765405\n",
      "Train Epoch: 688 [512/54000 (1%)] Loss: -1991.481689\n",
      "Train Epoch: 688 [11776/54000 (22%)] Loss: -1853.470459\n",
      "Train Epoch: 688 [23040/54000 (43%)] Loss: -1624.581177\n",
      "Train Epoch: 688 [34304/54000 (64%)] Loss: -1537.484741\n",
      "Train Epoch: 688 [45568/54000 (84%)] Loss: -1780.556885\n",
      "    epoch          : 688\n",
      "    loss           : -1759.2431447246288\n",
      "    val_loss       : -1768.4614611068741\n",
      "    val_log_likelihood: 1815.9825805664063\n",
      "    val_log_marginal: 1783.0548923868687\n",
      "Train Epoch: 689 [512/54000 (1%)] Loss: -1731.675171\n",
      "Train Epoch: 689 [11776/54000 (22%)] Loss: -1855.095215\n",
      "Train Epoch: 689 [23040/54000 (43%)] Loss: -1776.080933\n",
      "Train Epoch: 689 [34304/54000 (64%)] Loss: -1629.703491\n",
      "Train Epoch: 689 [45568/54000 (84%)] Loss: -1778.828125\n",
      "    epoch          : 689\n",
      "    loss           : -1763.5248527904548\n",
      "    val_loss       : -1768.5020176114515\n",
      "    val_log_likelihood: 1815.5953857421875\n",
      "    val_log_marginal: 1782.6628874216228\n",
      "Train Epoch: 690 [512/54000 (1%)] Loss: -1991.237427\n",
      "Train Epoch: 690 [11776/54000 (22%)] Loss: -1755.443726\n",
      "Train Epoch: 690 [23040/54000 (43%)] Loss: -1819.677002\n",
      "Train Epoch: 690 [34304/54000 (64%)] Loss: -1752.020630\n",
      "Train Epoch: 690 [45568/54000 (84%)] Loss: -1778.670410\n",
      "    epoch          : 690\n",
      "    loss           : -1772.019745175201\n",
      "    val_loss       : -1724.2283534839748\n",
      "    val_log_likelihood: 1816.27236328125\n",
      "    val_log_marginal: 1783.2484394962783\n",
      "Train Epoch: 691 [512/54000 (1%)] Loss: -1911.271118\n",
      "Train Epoch: 691 [11776/54000 (22%)] Loss: -1775.627686\n",
      "Train Epoch: 691 [23040/54000 (43%)] Loss: -1539.965820\n",
      "Train Epoch: 691 [34304/54000 (64%)] Loss: -1780.848999\n",
      "Train Epoch: 691 [45568/54000 (84%)] Loss: -1685.596558\n",
      "    epoch          : 691\n",
      "    loss           : -1755.6420547938583\n",
      "    val_loss       : -1759.329312763363\n",
      "    val_log_likelihood: 1815.9914184570312\n",
      "    val_log_marginal: 1783.0501929489642\n",
      "Train Epoch: 692 [512/54000 (1%)] Loss: -1992.994995\n",
      "Train Epoch: 692 [11776/54000 (22%)] Loss: -1850.828613\n",
      "Train Epoch: 692 [23040/54000 (43%)] Loss: -1692.604980\n",
      "Train Epoch: 692 [34304/54000 (64%)] Loss: -1637.887695\n",
      "Train Epoch: 692 [45568/54000 (84%)] Loss: -1780.268311\n",
      "    epoch          : 692\n",
      "    loss           : -1756.350259610922\n",
      "    val_loss       : -1739.7181441433727\n",
      "    val_log_likelihood: 1815.6051879882812\n",
      "    val_log_marginal: 1782.559580905057\n",
      "Train Epoch: 693 [512/54000 (1%)] Loss: -1991.249756\n",
      "Train Epoch: 693 [11776/54000 (22%)] Loss: -1538.444702\n",
      "Train Epoch: 693 [23040/54000 (43%)] Loss: -1825.325195\n",
      "Train Epoch: 693 [34304/54000 (64%)] Loss: -1548.419678\n",
      "Train Epoch: 693 [45568/54000 (84%)] Loss: -1787.668701\n",
      "    epoch          : 693\n",
      "    loss           : -1756.8387487430384\n",
      "    val_loss       : -1759.3499388106168\n",
      "    val_log_likelihood: 1815.3999145507812\n",
      "    val_log_marginal: 1782.4505138482898\n",
      "Train Epoch: 694 [512/54000 (1%)] Loss: -1914.082031\n",
      "Train Epoch: 694 [11776/54000 (22%)] Loss: -1819.356689\n",
      "Train Epoch: 694 [23040/54000 (43%)] Loss: -1822.946289\n",
      "Train Epoch: 694 [34304/54000 (64%)] Loss: -1772.831299\n",
      "Train Epoch: 694 [45568/54000 (84%)] Loss: -1777.270996\n",
      "    epoch          : 694\n",
      "    loss           : -1752.0085630511294\n",
      "    val_loss       : -1768.2523169891908\n",
      "    val_log_likelihood: 1815.5320678710937\n",
      "    val_log_marginal: 1782.5856769394\n",
      "Train Epoch: 695 [512/54000 (1%)] Loss: -1989.468140\n",
      "Train Epoch: 695 [11776/54000 (22%)] Loss: -1734.751465\n",
      "Train Epoch: 695 [23040/54000 (43%)] Loss: -1777.060791\n",
      "Train Epoch: 695 [34304/54000 (64%)] Loss: -1783.461060\n",
      "Train Epoch: 695 [45568/54000 (84%)] Loss: -1753.071045\n",
      "    epoch          : 695\n",
      "    loss           : -1753.3267483852878\n",
      "    val_loss       : -1747.346112238802\n",
      "    val_log_likelihood: 1816.1015747070312\n",
      "    val_log_marginal: 1783.1575752969832\n",
      "Train Epoch: 696 [512/54000 (1%)] Loss: -1993.631226\n",
      "Train Epoch: 696 [11776/54000 (22%)] Loss: -1817.582153\n",
      "Train Epoch: 696 [23040/54000 (43%)] Loss: -1822.042603\n",
      "Train Epoch: 696 [34304/54000 (64%)] Loss: -1648.886963\n",
      "Train Epoch: 696 [45568/54000 (84%)] Loss: -1774.630981\n",
      "    epoch          : 696\n",
      "    loss           : -1746.4883054223392\n",
      "    val_loss       : -1730.9433358747513\n",
      "    val_log_likelihood: 1816.1044677734376\n",
      "    val_log_marginal: 1783.0796230155981\n",
      "Train Epoch: 697 [512/54000 (1%)] Loss: -1991.778198\n",
      "Train Epoch: 697 [11776/54000 (22%)] Loss: -1692.959717\n",
      "Train Epoch: 697 [23040/54000 (43%)] Loss: -1635.954956\n",
      "Train Epoch: 697 [34304/54000 (64%)] Loss: -1782.277710\n",
      "Train Epoch: 697 [45568/54000 (84%)] Loss: -1778.264282\n",
      "    epoch          : 697\n",
      "    loss           : -1749.162383731049\n",
      "    val_loss       : -1774.0362586930394\n",
      "    val_log_likelihood: 1815.227978515625\n",
      "    val_log_marginal: 1782.225308550466\n",
      "Train Epoch: 698 [512/54000 (1%)] Loss: -1994.287109\n",
      "Train Epoch: 698 [11776/54000 (22%)] Loss: -1737.937744\n",
      "Train Epoch: 698 [23040/54000 (43%)] Loss: -1815.850098\n",
      "Train Epoch: 698 [34304/54000 (64%)] Loss: -1543.020752\n",
      "Train Epoch: 698 [45568/54000 (84%)] Loss: -1777.540771\n",
      "    epoch          : 698\n",
      "    loss           : -1754.9754409034654\n",
      "    val_loss       : -1739.711566291377\n",
      "    val_log_likelihood: 1816.600244140625\n",
      "    val_log_marginal: 1783.6029405432153\n",
      "Train Epoch: 699 [512/54000 (1%)] Loss: -1993.329834\n",
      "Train Epoch: 699 [11776/54000 (22%)] Loss: -1733.560669\n",
      "Train Epoch: 699 [23040/54000 (43%)] Loss: -1820.325928\n",
      "Train Epoch: 699 [34304/54000 (64%)] Loss: -1685.666260\n",
      "Train Epoch: 699 [45568/54000 (84%)] Loss: -1624.107056\n",
      "    epoch          : 699\n",
      "    loss           : -1740.9429847037438\n",
      "    val_loss       : -1733.1861940879376\n",
      "    val_log_likelihood: 1816.8252685546875\n",
      "    val_log_marginal: 1783.8925045312922\n",
      "Train Epoch: 700 [512/54000 (1%)] Loss: -1990.479492\n",
      "Train Epoch: 700 [11776/54000 (22%)] Loss: -1815.591431\n",
      "Train Epoch: 700 [23040/54000 (43%)] Loss: -1821.149780\n",
      "Train Epoch: 700 [34304/54000 (64%)] Loss: -1991.560913\n",
      "Train Epoch: 700 [45568/54000 (84%)] Loss: -1778.004395\n",
      "    epoch          : 700\n",
      "    loss           : -1753.8170431911356\n",
      "    val_loss       : -1783.3816616944969\n",
      "    val_log_likelihood: 1816.8570190429687\n",
      "    val_log_marginal: 1783.8679009299726\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch700.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 701 [512/54000 (1%)] Loss: -1915.533569\n",
      "Train Epoch: 701 [11776/54000 (22%)] Loss: -1735.533203\n",
      "Train Epoch: 701 [23040/54000 (43%)] Loss: -1699.073120\n",
      "Train Epoch: 701 [34304/54000 (64%)] Loss: -1707.077271\n",
      "Train Epoch: 701 [45568/54000 (84%)] Loss: -1778.046387\n",
      "    epoch          : 701\n",
      "    loss           : -1756.235922029703\n",
      "    val_loss       : -1783.7827573543414\n",
      "    val_log_likelihood: 1817.2854736328125\n",
      "    val_log_marginal: 1784.311306532048\n",
      "Train Epoch: 702 [512/54000 (1%)] Loss: -1989.279907\n",
      "Train Epoch: 702 [11776/54000 (22%)] Loss: -1734.094604\n",
      "Train Epoch: 702 [23040/54000 (43%)] Loss: -1784.931152\n",
      "Train Epoch: 702 [34304/54000 (64%)] Loss: -1992.681641\n",
      "Train Epoch: 702 [45568/54000 (84%)] Loss: -1782.031006\n",
      "    epoch          : 702\n",
      "    loss           : -1750.3548535639698\n",
      "    val_loss       : -1768.4982182344422\n",
      "    val_log_likelihood: 1816.0885498046875\n",
      "    val_log_marginal: 1783.083863763488\n",
      "Train Epoch: 703 [512/54000 (1%)] Loss: -1989.772583\n",
      "Train Epoch: 703 [11776/54000 (22%)] Loss: -1846.685547\n",
      "Train Epoch: 703 [23040/54000 (43%)] Loss: -1815.930176\n",
      "Train Epoch: 703 [34304/54000 (64%)] Loss: -1688.261230\n",
      "Train Epoch: 703 [45568/54000 (84%)] Loss: -1630.422974\n",
      "    epoch          : 703\n",
      "    loss           : -1757.465086682008\n",
      "    val_loss       : -1746.6403254264965\n",
      "    val_log_likelihood: 1817.0472534179687\n",
      "    val_log_marginal: 1784.0831908373777\n",
      "Train Epoch: 704 [512/54000 (1%)] Loss: -1694.984863\n",
      "Train Epoch: 704 [11776/54000 (22%)] Loss: -1703.732422\n",
      "Train Epoch: 704 [23040/54000 (43%)] Loss: -1817.070801\n",
      "Train Epoch: 704 [34304/54000 (64%)] Loss: -1780.704834\n",
      "Train Epoch: 704 [45568/54000 (84%)] Loss: -1780.050781\n",
      "    epoch          : 704\n",
      "    loss           : -1755.1630255066523\n",
      "    val_loss       : -1732.3100194068625\n",
      "    val_log_likelihood: 1815.8634887695312\n",
      "    val_log_marginal: 1783.0104027351676\n",
      "Train Epoch: 705 [512/54000 (1%)] Loss: -1993.450928\n",
      "Train Epoch: 705 [11776/54000 (22%)] Loss: -1600.151001\n",
      "Train Epoch: 705 [23040/54000 (43%)] Loss: -1812.467285\n",
      "Train Epoch: 705 [34304/54000 (64%)] Loss: -1785.780273\n",
      "Train Epoch: 705 [45568/54000 (84%)] Loss: -1687.273315\n",
      "    epoch          : 705\n",
      "    loss           : -1746.373124226485\n",
      "    val_loss       : -1732.3510916655882\n",
      "    val_log_likelihood: 1816.8401611328125\n",
      "    val_log_marginal: 1783.9332182068379\n",
      "Train Epoch: 706 [512/54000 (1%)] Loss: -1991.716553\n",
      "Train Epoch: 706 [11776/54000 (22%)] Loss: -1734.590698\n",
      "Train Epoch: 706 [23040/54000 (43%)] Loss: -1760.355469\n",
      "Train Epoch: 706 [34304/54000 (64%)] Loss: -1786.857422\n",
      "Train Epoch: 706 [45568/54000 (84%)] Loss: -1778.434692\n",
      "    epoch          : 706\n",
      "    loss           : -1759.9466564820545\n",
      "    val_loss       : -1755.2047388192266\n",
      "    val_log_likelihood: 1816.0395263671876\n",
      "    val_log_marginal: 1783.0991811167448\n",
      "Train Epoch: 707 [512/54000 (1%)] Loss: -1992.810669\n",
      "Train Epoch: 707 [11776/54000 (22%)] Loss: -1728.075562\n",
      "Train Epoch: 707 [23040/54000 (43%)] Loss: -1689.752319\n",
      "Train Epoch: 707 [34304/54000 (64%)] Loss: -1988.509766\n",
      "Train Epoch: 707 [45568/54000 (84%)] Loss: -1783.622803\n",
      "    epoch          : 707\n",
      "    loss           : -1740.525582795096\n",
      "    val_loss       : -1781.9485688850284\n",
      "    val_log_likelihood: 1815.4833129882813\n",
      "    val_log_marginal: 1782.5074134703725\n",
      "Train Epoch: 708 [512/54000 (1%)] Loss: -1990.988281\n",
      "Train Epoch: 708 [11776/54000 (22%)] Loss: -1742.448242\n",
      "Train Epoch: 708 [23040/54000 (43%)] Loss: -1751.210449\n",
      "Train Epoch: 708 [34304/54000 (64%)] Loss: -1780.940674\n",
      "Train Epoch: 708 [45568/54000 (84%)] Loss: -1775.766724\n",
      "    epoch          : 708\n",
      "    loss           : -1760.753114605894\n",
      "    val_loss       : -1716.5070723960175\n",
      "    val_log_likelihood: 1815.8430419921874\n",
      "    val_log_marginal: 1782.824203032555\n",
      "Train Epoch: 709 [512/54000 (1%)] Loss: -1638.093384\n",
      "Train Epoch: 709 [11776/54000 (22%)] Loss: -1848.497314\n",
      "Train Epoch: 709 [23040/54000 (43%)] Loss: -1819.610107\n",
      "Train Epoch: 709 [34304/54000 (64%)] Loss: -1782.268799\n",
      "Train Epoch: 709 [45568/54000 (84%)] Loss: -1785.359009\n",
      "    epoch          : 709\n",
      "    loss           : -1755.5188556331218\n",
      "    val_loss       : -1738.6825957216322\n",
      "    val_log_likelihood: 1817.037353515625\n",
      "    val_log_marginal: 1784.110340334863\n",
      "Train Epoch: 710 [512/54000 (1%)] Loss: -1985.282471\n",
      "Train Epoch: 710 [11776/54000 (22%)] Loss: -1703.220703\n",
      "Train Epoch: 710 [23040/54000 (43%)] Loss: -1780.320923\n",
      "Train Epoch: 710 [34304/54000 (64%)] Loss: -1778.317627\n",
      "Train Epoch: 710 [45568/54000 (84%)] Loss: -1557.506836\n",
      "    epoch          : 710\n",
      "    loss           : -1753.7956615485768\n",
      "    val_loss       : -1742.473004485853\n",
      "    val_log_likelihood: 1816.3518676757812\n",
      "    val_log_marginal: 1783.4708293695003\n",
      "Train Epoch: 711 [512/54000 (1%)] Loss: -1989.644409\n",
      "Train Epoch: 711 [11776/54000 (22%)] Loss: -1593.117676\n",
      "Train Epoch: 711 [23040/54000 (43%)] Loss: -1530.551147\n",
      "Train Epoch: 711 [34304/54000 (64%)] Loss: -1685.951660\n",
      "Train Epoch: 711 [45568/54000 (84%)] Loss: -1772.126953\n",
      "    epoch          : 711\n",
      "    loss           : -1761.1536405959932\n",
      "    val_loss       : -1782.1549789464102\n",
      "    val_log_likelihood: 1815.3889892578125\n",
      "    val_log_marginal: 1782.4309332392259\n",
      "Train Epoch: 712 [512/54000 (1%)] Loss: -1990.257202\n",
      "Train Epoch: 712 [11776/54000 (22%)] Loss: -1846.345459\n",
      "Train Epoch: 712 [23040/54000 (43%)] Loss: -1539.218994\n",
      "Train Epoch: 712 [34304/54000 (64%)] Loss: -1683.626953\n",
      "Train Epoch: 712 [45568/54000 (84%)] Loss: -1785.536499\n",
      "    epoch          : 712\n",
      "    loss           : -1755.4502412399443\n",
      "    val_loss       : -1770.5607703454793\n",
      "    val_log_likelihood: 1815.91904296875\n",
      "    val_log_marginal: 1782.9979968506843\n",
      "Train Epoch: 713 [512/54000 (1%)] Loss: -1923.856934\n",
      "Train Epoch: 713 [11776/54000 (22%)] Loss: -1754.993896\n",
      "Train Epoch: 713 [23040/54000 (43%)] Loss: -1752.174316\n",
      "Train Epoch: 713 [34304/54000 (64%)] Loss: -1693.175171\n",
      "Train Epoch: 713 [45568/54000 (84%)] Loss: -1784.028564\n",
      "    epoch          : 713\n",
      "    loss           : -1767.3022775177908\n",
      "    val_loss       : -1761.0153536474331\n",
      "    val_log_likelihood: 1816.5736572265625\n",
      "    val_log_marginal: 1783.723391238258\n",
      "Train Epoch: 714 [512/54000 (1%)] Loss: -1991.232178\n",
      "Train Epoch: 714 [11776/54000 (22%)] Loss: -1847.490845\n",
      "Train Epoch: 714 [23040/54000 (43%)] Loss: -1576.330322\n",
      "Train Epoch: 714 [34304/54000 (64%)] Loss: -1987.735596\n",
      "Train Epoch: 714 [45568/54000 (84%)] Loss: -1565.124634\n",
      "    epoch          : 714\n",
      "    loss           : -1753.4720724880106\n",
      "    val_loss       : -1733.7373941414057\n",
      "    val_log_likelihood: 1816.2275390625\n",
      "    val_log_marginal: 1783.2818819161505\n",
      "Train Epoch: 715 [512/54000 (1%)] Loss: -1989.850098\n",
      "Train Epoch: 715 [11776/54000 (22%)] Loss: -1738.347534\n",
      "Train Epoch: 715 [23040/54000 (43%)] Loss: -1825.525757\n",
      "Train Epoch: 715 [34304/54000 (64%)] Loss: -1817.641968\n",
      "Train Epoch: 715 [45568/54000 (84%)] Loss: -1786.886963\n",
      "    epoch          : 715\n",
      "    loss           : -1753.3908002494586\n",
      "    val_loss       : -1712.1097846772523\n",
      "    val_log_likelihood: 1816.8080688476562\n",
      "    val_log_marginal: 1783.982028030321\n",
      "Train Epoch: 716 [512/54000 (1%)] Loss: -1990.670166\n",
      "Train Epoch: 716 [11776/54000 (22%)] Loss: -1633.864258\n",
      "Train Epoch: 716 [23040/54000 (43%)] Loss: -1604.572266\n",
      "Train Epoch: 716 [34304/54000 (64%)] Loss: -1781.300293\n",
      "Train Epoch: 716 [45568/54000 (84%)] Loss: -1634.265381\n",
      "    epoch          : 716\n",
      "    loss           : -1763.3848647315904\n",
      "    val_loss       : -1783.167322546616\n",
      "    val_log_likelihood: 1816.6388305664063\n",
      "    val_log_marginal: 1783.7043428804725\n",
      "Train Epoch: 717 [512/54000 (1%)] Loss: -1849.406250\n",
      "Train Epoch: 717 [11776/54000 (22%)] Loss: -1604.740967\n",
      "Train Epoch: 717 [23040/54000 (43%)] Loss: -1531.038330\n",
      "Train Epoch: 717 [34304/54000 (64%)] Loss: -1781.515503\n",
      "Train Epoch: 717 [45568/54000 (84%)] Loss: -1787.943481\n",
      "    epoch          : 717\n",
      "    loss           : -1761.974476427135\n",
      "    val_loss       : -1783.0286119140685\n",
      "    val_log_likelihood: 1816.4190795898437\n",
      "    val_log_marginal: 1783.5207404874352\n",
      "Train Epoch: 718 [512/54000 (1%)] Loss: -1991.805908\n",
      "Train Epoch: 718 [11776/54000 (22%)] Loss: -1629.349243\n",
      "Train Epoch: 718 [23040/54000 (43%)] Loss: -1821.144653\n",
      "Train Epoch: 718 [34304/54000 (64%)] Loss: -1527.800781\n",
      "Train Epoch: 718 [45568/54000 (84%)] Loss: -1632.906738\n",
      "    epoch          : 718\n",
      "    loss           : -1743.8068424640317\n",
      "    val_loss       : -1749.7847096448763\n",
      "    val_log_likelihood: 1816.9469848632812\n",
      "    val_log_marginal: 1784.0957029979677\n",
      "Train Epoch: 719 [512/54000 (1%)] Loss: -1852.510742\n",
      "Train Epoch: 719 [11776/54000 (22%)] Loss: -1742.082520\n",
      "Train Epoch: 719 [23040/54000 (43%)] Loss: -1688.181396\n",
      "Train Epoch: 719 [34304/54000 (64%)] Loss: -1750.580566\n",
      "Train Epoch: 719 [45568/54000 (84%)] Loss: -1784.937256\n",
      "    epoch          : 719\n",
      "    loss           : -1747.2483043104114\n",
      "    val_loss       : -1730.33326143194\n",
      "    val_log_likelihood: 1817.3928588867188\n",
      "    val_log_marginal: 1784.5311299797147\n",
      "Train Epoch: 720 [512/54000 (1%)] Loss: -1919.085693\n",
      "Train Epoch: 720 [11776/54000 (22%)] Loss: -1852.729736\n",
      "Train Epoch: 720 [23040/54000 (43%)] Loss: -1698.308105\n",
      "Train Epoch: 720 [34304/54000 (64%)] Loss: -1677.634277\n",
      "Train Epoch: 720 [45568/54000 (84%)] Loss: -1689.481323\n",
      "    epoch          : 720\n",
      "    loss           : -1759.8020140392946\n",
      "    val_loss       : -1748.0828836549074\n",
      "    val_log_likelihood: 1816.5661865234374\n",
      "    val_log_marginal: 1783.706683894441\n",
      "Train Epoch: 721 [512/54000 (1%)] Loss: -1819.859375\n",
      "Train Epoch: 721 [11776/54000 (22%)] Loss: -1848.495605\n",
      "Train Epoch: 721 [23040/54000 (43%)] Loss: -1687.263916\n",
      "Train Epoch: 721 [34304/54000 (64%)] Loss: -1641.465210\n",
      "Train Epoch: 721 [45568/54000 (84%)] Loss: -1774.619873\n",
      "    epoch          : 721\n",
      "    loss           : -1751.1035180422339\n",
      "    val_loss       : -1732.4163783539086\n",
      "    val_log_likelihood: 1816.5879150390624\n",
      "    val_log_marginal: 1783.699811302498\n",
      "Train Epoch: 722 [512/54000 (1%)] Loss: -1921.822266\n",
      "Train Epoch: 722 [11776/54000 (22%)] Loss: -1683.152832\n",
      "Train Epoch: 722 [23040/54000 (43%)] Loss: -1471.758057\n",
      "Train Epoch: 722 [34304/54000 (64%)] Loss: -1683.445801\n",
      "Train Epoch: 722 [45568/54000 (84%)] Loss: -1684.036621\n",
      "    epoch          : 722\n",
      "    loss           : -1756.127493376779\n",
      "    val_loss       : -1770.8202645314857\n",
      "    val_log_likelihood: 1815.9294677734374\n",
      "    val_log_marginal: 1783.1171791364548\n",
      "Train Epoch: 723 [512/54000 (1%)] Loss: -1987.603271\n",
      "Train Epoch: 723 [11776/54000 (22%)] Loss: -1741.361084\n",
      "Train Epoch: 723 [23040/54000 (43%)] Loss: -1735.959229\n",
      "Train Epoch: 723 [34304/54000 (64%)] Loss: -1773.909790\n",
      "Train Epoch: 723 [45568/54000 (84%)] Loss: -1782.476807\n",
      "    epoch          : 723\n",
      "    loss           : -1754.9936716816212\n",
      "    val_loss       : -1768.4788593424485\n",
      "    val_log_likelihood: 1816.1006591796875\n",
      "    val_log_marginal: 1783.225642214343\n",
      "Train Epoch: 724 [512/54000 (1%)] Loss: -1989.562012\n",
      "Train Epoch: 724 [11776/54000 (22%)] Loss: -1778.827759\n",
      "Train Epoch: 724 [23040/54000 (43%)] Loss: -1638.777832\n",
      "Train Epoch: 724 [34304/54000 (64%)] Loss: -1992.552002\n",
      "Train Epoch: 724 [45568/54000 (84%)] Loss: -1634.006836\n",
      "    epoch          : 724\n",
      "    loss           : -1755.3038402595143\n",
      "    val_loss       : -1782.4300931911916\n",
      "    val_log_likelihood: 1815.7343017578125\n",
      "    val_log_marginal: 1782.8775012101978\n",
      "Train Epoch: 725 [512/54000 (1%)] Loss: -1856.471313\n",
      "Train Epoch: 725 [11776/54000 (22%)] Loss: -1826.467285\n",
      "Train Epoch: 725 [23040/54000 (43%)] Loss: -1631.547119\n",
      "Train Epoch: 725 [34304/54000 (64%)] Loss: -1779.100708\n",
      "Train Epoch: 725 [45568/54000 (84%)] Loss: -1687.098022\n",
      "    epoch          : 725\n",
      "    loss           : -1762.532063399211\n",
      "    val_loss       : -1760.6685386467725\n",
      "    val_log_likelihood: 1815.7177124023438\n",
      "    val_log_marginal: 1782.8026965665836\n",
      "Train Epoch: 726 [512/54000 (1%)] Loss: -1923.622314\n",
      "Train Epoch: 726 [11776/54000 (22%)] Loss: -1744.423340\n",
      "Train Epoch: 726 [23040/54000 (43%)] Loss: -1783.118896\n",
      "Train Epoch: 726 [34304/54000 (64%)] Loss: -1685.725830\n",
      "Train Epoch: 726 [45568/54000 (84%)] Loss: -1784.633057\n",
      "    epoch          : 726\n",
      "    loss           : -1753.658612846148\n",
      "    val_loss       : -1760.1433185510336\n",
      "    val_log_likelihood: 1817.5398803710937\n",
      "    val_log_marginal: 1784.700862485543\n",
      "Train Epoch: 727 [512/54000 (1%)] Loss: -1989.889282\n",
      "Train Epoch: 727 [11776/54000 (22%)] Loss: -1699.265381\n",
      "Train Epoch: 727 [23040/54000 (43%)] Loss: -1784.327393\n",
      "Train Epoch: 727 [34304/54000 (64%)] Loss: -1819.186279\n",
      "Train Epoch: 727 [45568/54000 (84%)] Loss: -1778.922363\n",
      "    epoch          : 727\n",
      "    loss           : -1761.1785393138923\n",
      "    val_loss       : -1761.999286826141\n",
      "    val_log_likelihood: 1816.263427734375\n",
      "    val_log_marginal: 1783.3550848614425\n",
      "Train Epoch: 728 [512/54000 (1%)] Loss: -1990.444946\n",
      "Train Epoch: 728 [11776/54000 (22%)] Loss: -1790.658813\n",
      "Train Epoch: 728 [23040/54000 (43%)] Loss: -1688.619019\n",
      "Train Epoch: 728 [34304/54000 (64%)] Loss: -1703.017090\n",
      "Train Epoch: 728 [45568/54000 (84%)] Loss: -1777.341064\n",
      "    epoch          : 728\n",
      "    loss           : -1750.7356065806775\n",
      "    val_loss       : -1740.6209075110032\n",
      "    val_log_likelihood: 1816.8870239257812\n",
      "    val_log_marginal: 1784.0473553251486\n",
      "Train Epoch: 729 [512/54000 (1%)] Loss: -1702.375610\n",
      "Train Epoch: 729 [11776/54000 (22%)] Loss: -1735.379517\n",
      "Train Epoch: 729 [23040/54000 (43%)] Loss: -1753.090210\n",
      "Train Epoch: 729 [34304/54000 (64%)] Loss: -1550.968262\n",
      "Train Epoch: 729 [45568/54000 (84%)] Loss: -1693.063232\n",
      "    epoch          : 729\n",
      "    loss           : -1760.5003190748762\n",
      "    val_loss       : -1741.504446822591\n",
      "    val_log_likelihood: 1817.5103637695313\n",
      "    val_log_marginal: 1784.5935335297158\n",
      "Train Epoch: 730 [512/54000 (1%)] Loss: -1993.810425\n",
      "Train Epoch: 730 [11776/54000 (22%)] Loss: -1851.075073\n",
      "Train Epoch: 730 [23040/54000 (43%)] Loss: -1817.697632\n",
      "Train Epoch: 730 [34304/54000 (64%)] Loss: -1699.754028\n",
      "Train Epoch: 730 [45568/54000 (84%)] Loss: -1782.282715\n",
      "    epoch          : 730\n",
      "    loss           : -1745.527017423422\n",
      "    val_loss       : -1761.5139442222194\n",
      "    val_log_likelihood: 1816.8707885742188\n",
      "    val_log_marginal: 1783.91794427298\n",
      "Train Epoch: 731 [512/54000 (1%)] Loss: -1991.031982\n",
      "Train Epoch: 731 [11776/54000 (22%)] Loss: -1698.286499\n",
      "Train Epoch: 731 [23040/54000 (43%)] Loss: -1684.818970\n",
      "Train Epoch: 731 [34304/54000 (64%)] Loss: -1642.967651\n",
      "Train Epoch: 731 [45568/54000 (84%)] Loss: -1777.934326\n",
      "    epoch          : 731\n",
      "    loss           : -1757.6289509688274\n",
      "    val_loss       : -1784.298413586989\n",
      "    val_log_likelihood: 1817.6365966796875\n",
      "    val_log_marginal: 1784.771501434967\n",
      "Train Epoch: 732 [512/54000 (1%)] Loss: -1990.101074\n",
      "Train Epoch: 732 [11776/54000 (22%)] Loss: -1597.688721\n",
      "Train Epoch: 732 [23040/54000 (43%)] Loss: -1747.718262\n",
      "Train Epoch: 732 [34304/54000 (64%)] Loss: -1751.393188\n",
      "Train Epoch: 732 [45568/54000 (84%)] Loss: -1759.370850\n",
      "    epoch          : 732\n",
      "    loss           : -1764.9385007348392\n",
      "    val_loss       : -1754.5237839004026\n",
      "    val_log_likelihood: 1815.9427001953125\n",
      "    val_log_marginal: 1783.0621470686049\n",
      "Train Epoch: 733 [512/54000 (1%)] Loss: -1921.000000\n",
      "Train Epoch: 733 [11776/54000 (22%)] Loss: -1734.014893\n",
      "Train Epoch: 733 [23040/54000 (43%)] Loss: -1541.543823\n",
      "Train Epoch: 733 [34304/54000 (64%)] Loss: -1816.487183\n",
      "Train Epoch: 733 [45568/54000 (84%)] Loss: -1566.259644\n",
      "    epoch          : 733\n",
      "    loss           : -1749.8441983968905\n",
      "    val_loss       : -1757.1766484340653\n",
      "    val_log_likelihood: 1816.3081909179687\n",
      "    val_log_marginal: 1783.3975153502997\n",
      "Train Epoch: 734 [512/54000 (1%)] Loss: -1993.457275\n",
      "Train Epoch: 734 [11776/54000 (22%)] Loss: -1851.590698\n",
      "Train Epoch: 734 [23040/54000 (43%)] Loss: -1706.075684\n",
      "Train Epoch: 734 [34304/54000 (64%)] Loss: -1540.515869\n",
      "Train Epoch: 734 [45568/54000 (84%)] Loss: -1777.957520\n",
      "    epoch          : 734\n",
      "    loss           : -1759.9563665106746\n",
      "    val_loss       : -1768.7239504177123\n",
      "    val_log_likelihood: 1815.8134521484376\n",
      "    val_log_marginal: 1782.894998657896\n",
      "Train Epoch: 735 [512/54000 (1%)] Loss: -1924.331299\n",
      "Train Epoch: 735 [11776/54000 (22%)] Loss: -1691.547852\n",
      "Train Epoch: 735 [23040/54000 (43%)] Loss: -1822.078613\n",
      "Train Epoch: 735 [34304/54000 (64%)] Loss: -1990.698486\n",
      "Train Epoch: 735 [45568/54000 (84%)] Loss: -1779.109619\n",
      "    epoch          : 735\n",
      "    loss           : -1764.9725317624536\n",
      "    val_loss       : -1768.5449600864201\n",
      "    val_log_likelihood: 1816.5435424804687\n",
      "    val_log_marginal: 1783.6484841879458\n",
      "Train Epoch: 736 [512/54000 (1%)] Loss: -1921.702881\n",
      "Train Epoch: 736 [11776/54000 (22%)] Loss: -1848.324219\n",
      "Train Epoch: 736 [23040/54000 (43%)] Loss: -1820.563354\n",
      "Train Epoch: 736 [34304/54000 (64%)] Loss: -1535.617432\n",
      "Train Epoch: 736 [45568/54000 (84%)] Loss: -1785.809937\n",
      "    epoch          : 736\n",
      "    loss           : -1759.7570716178063\n",
      "    val_loss       : -1784.2049179613591\n",
      "    val_log_likelihood: 1817.5152099609375\n",
      "    val_log_marginal: 1784.6348713833838\n",
      "Train Epoch: 737 [512/54000 (1%)] Loss: -1992.636108\n",
      "Train Epoch: 737 [11776/54000 (22%)] Loss: -1700.679199\n",
      "Train Epoch: 737 [23040/54000 (43%)] Loss: -1566.487549\n",
      "Train Epoch: 737 [34304/54000 (64%)] Loss: -1682.484131\n",
      "Train Epoch: 737 [45568/54000 (84%)] Loss: -1782.082642\n",
      "    epoch          : 737\n",
      "    loss           : -1757.033394086479\n",
      "    val_loss       : -1720.7432660400868\n",
      "    val_log_likelihood: 1816.98017578125\n",
      "    val_log_marginal: 1784.0096465303075\n",
      "Train Epoch: 738 [512/54000 (1%)] Loss: -1922.382324\n",
      "Train Epoch: 738 [11776/54000 (22%)] Loss: -1849.956787\n",
      "Train Epoch: 738 [23040/54000 (43%)] Loss: -1633.830811\n",
      "Train Epoch: 738 [34304/54000 (64%)] Loss: -1819.139160\n",
      "Train Epoch: 738 [45568/54000 (84%)] Loss: -1781.784790\n",
      "    epoch          : 738\n",
      "    loss           : -1750.8556802579672\n",
      "    val_loss       : -1772.0249567711726\n",
      "    val_log_likelihood: 1817.1638305664062\n",
      "    val_log_marginal: 1784.364301292245\n",
      "Train Epoch: 739 [512/54000 (1%)] Loss: -1991.397339\n",
      "Train Epoch: 739 [11776/54000 (22%)] Loss: -1699.844360\n",
      "Train Epoch: 739 [23040/54000 (43%)] Loss: -1818.020142\n",
      "Train Epoch: 739 [34304/54000 (64%)] Loss: -1818.809814\n",
      "Train Epoch: 739 [45568/54000 (84%)] Loss: -1775.954346\n",
      "    epoch          : 739\n",
      "    loss           : -1752.0982871480508\n",
      "    val_loss       : -1783.0459776649252\n",
      "    val_log_likelihood: 1816.4091064453125\n",
      "    val_log_marginal: 1783.5916810836811\n",
      "Train Epoch: 740 [512/54000 (1%)] Loss: -1743.316650\n",
      "Train Epoch: 740 [11776/54000 (22%)] Loss: -1851.214111\n",
      "Train Epoch: 740 [23040/54000 (43%)] Loss: -1779.184082\n",
      "Train Epoch: 740 [34304/54000 (64%)] Loss: -1758.792480\n",
      "Train Epoch: 740 [45568/54000 (84%)] Loss: -1679.736450\n",
      "    epoch          : 740\n",
      "    loss           : -1767.3958232615253\n",
      "    val_loss       : -1783.1489180222154\n",
      "    val_log_likelihood: 1816.3303466796874\n",
      "    val_log_marginal: 1783.4681059726233\n",
      "Train Epoch: 741 [512/54000 (1%)] Loss: -1924.224854\n",
      "Train Epoch: 741 [11776/54000 (22%)] Loss: -1699.781982\n",
      "Train Epoch: 741 [23040/54000 (43%)] Loss: -1686.220215\n",
      "Train Epoch: 741 [34304/54000 (64%)] Loss: -1691.067383\n",
      "Train Epoch: 741 [45568/54000 (84%)] Loss: -1492.343140\n",
      "    epoch          : 741\n",
      "    loss           : -1754.9042896232982\n",
      "    val_loss       : -1753.887138443254\n",
      "    val_log_likelihood: 1816.36064453125\n",
      "    val_log_marginal: 1783.5007206254\n",
      "Train Epoch: 742 [512/54000 (1%)] Loss: -1989.383545\n",
      "Train Epoch: 742 [11776/54000 (22%)] Loss: -1563.968018\n",
      "Train Epoch: 742 [23040/54000 (43%)] Loss: -1817.737305\n",
      "Train Epoch: 742 [34304/54000 (64%)] Loss: -1761.104248\n",
      "Train Epoch: 742 [45568/54000 (84%)] Loss: -1774.828491\n",
      "    epoch          : 742\n",
      "    loss           : -1758.0031315265317\n",
      "    val_loss       : -1761.0643032167106\n",
      "    val_log_likelihood: 1816.5495849609374\n",
      "    val_log_marginal: 1783.7579730373182\n",
      "Train Epoch: 743 [512/54000 (1%)] Loss: -1991.014771\n",
      "Train Epoch: 743 [11776/54000 (22%)] Loss: -1735.556641\n",
      "Train Epoch: 743 [23040/54000 (43%)] Loss: -1775.687012\n",
      "Train Epoch: 743 [34304/54000 (64%)] Loss: -1989.853027\n",
      "Train Epoch: 743 [45568/54000 (84%)] Loss: -1785.358887\n",
      "    epoch          : 743\n",
      "    loss           : -1755.0622413559715\n",
      "    val_loss       : -1776.8042855584995\n",
      "    val_log_likelihood: 1817.1351318359375\n",
      "    val_log_marginal: 1784.3045527685456\n",
      "Train Epoch: 744 [512/54000 (1%)] Loss: -1993.124268\n",
      "Train Epoch: 744 [11776/54000 (22%)] Loss: -1775.195923\n",
      "Train Epoch: 744 [23040/54000 (43%)] Loss: -1779.985474\n",
      "Train Epoch: 744 [34304/54000 (64%)] Loss: -1990.842285\n",
      "Train Epoch: 744 [45568/54000 (84%)] Loss: -1631.662598\n",
      "    epoch          : 744\n",
      "    loss           : -1747.199327525526\n",
      "    val_loss       : -1775.5739116810262\n",
      "    val_log_likelihood: 1815.8057495117187\n",
      "    val_log_marginal: 1782.940309009468\n",
      "Train Epoch: 745 [512/54000 (1%)] Loss: -1988.915649\n",
      "Train Epoch: 745 [11776/54000 (22%)] Loss: -1820.488892\n",
      "Train Epoch: 745 [23040/54000 (43%)] Loss: -1751.442627\n",
      "Train Epoch: 745 [34304/54000 (64%)] Loss: -1571.854736\n",
      "Train Epoch: 745 [45568/54000 (84%)] Loss: -1690.133545\n",
      "    epoch          : 745\n",
      "    loss           : -1751.8703262782333\n",
      "    val_loss       : -1718.3302147962154\n",
      "    val_log_likelihood: 1816.2281616210937\n",
      "    val_log_marginal: 1783.4264720324427\n",
      "Train Epoch: 746 [512/54000 (1%)] Loss: -1988.211060\n",
      "Train Epoch: 746 [11776/54000 (22%)] Loss: -1601.535522\n",
      "Train Epoch: 746 [23040/54000 (43%)] Loss: -1752.644531\n",
      "Train Epoch: 746 [34304/54000 (64%)] Loss: -1689.185547\n",
      "Train Epoch: 746 [45568/54000 (84%)] Loss: -1786.509277\n",
      "    epoch          : 746\n",
      "    loss           : -1764.2548948986696\n",
      "    val_loss       : -1755.0213773330674\n",
      "    val_log_likelihood: 1815.9002319335937\n",
      "    val_log_marginal: 1783.0264581810684\n",
      "Train Epoch: 747 [512/54000 (1%)] Loss: -1920.538574\n",
      "Train Epoch: 747 [11776/54000 (22%)] Loss: -1599.226562\n",
      "Train Epoch: 747 [23040/54000 (43%)] Loss: -1823.560669\n",
      "Train Epoch: 747 [34304/54000 (64%)] Loss: -1630.509033\n",
      "Train Epoch: 747 [45568/54000 (84%)] Loss: -1783.581421\n",
      "    epoch          : 747\n",
      "    loss           : -1751.561170521349\n",
      "    val_loss       : -1733.1342017378659\n",
      "    val_log_likelihood: 1816.3923706054688\n",
      "    val_log_marginal: 1783.459117087569\n",
      "Train Epoch: 748 [512/54000 (1%)] Loss: -1549.882446\n",
      "Train Epoch: 748 [11776/54000 (22%)] Loss: -1699.532959\n",
      "Train Epoch: 748 [23040/54000 (43%)] Loss: -1822.343140\n",
      "Train Epoch: 748 [34304/54000 (64%)] Loss: -1684.792725\n",
      "Train Epoch: 748 [45568/54000 (84%)] Loss: -1779.774780\n",
      "    epoch          : 748\n",
      "    loss           : -1760.1764955426206\n",
      "    val_loss       : -1783.0868607815355\n",
      "    val_log_likelihood: 1816.6266845703126\n",
      "    val_log_marginal: 1783.8187342274518\n",
      "Train Epoch: 749 [512/54000 (1%)] Loss: -1851.679932\n",
      "Train Epoch: 749 [11776/54000 (22%)] Loss: -1814.337158\n",
      "Train Epoch: 749 [23040/54000 (43%)] Loss: -1780.269043\n",
      "Train Epoch: 749 [34304/54000 (64%)] Loss: -1753.334961\n",
      "Train Epoch: 749 [45568/54000 (84%)] Loss: -1782.347656\n",
      "    epoch          : 749\n",
      "    loss           : -1746.5696973139698\n",
      "    val_loss       : -1761.4575266024099\n",
      "    val_log_likelihood: 1816.6822631835937\n",
      "    val_log_marginal: 1783.8607301034026\n",
      "Train Epoch: 750 [512/54000 (1%)] Loss: -1989.888428\n",
      "Train Epoch: 750 [11776/54000 (22%)] Loss: -1739.116577\n",
      "Train Epoch: 750 [23040/54000 (43%)] Loss: -1820.173218\n",
      "Train Epoch: 750 [34304/54000 (64%)] Loss: -1993.167603\n",
      "Train Epoch: 750 [45568/54000 (84%)] Loss: -1684.487793\n",
      "    epoch          : 750\n",
      "    loss           : -1755.9415271116955\n",
      "    val_loss       : -1784.376375074126\n",
      "    val_log_likelihood: 1817.5500122070312\n",
      "    val_log_marginal: 1784.7298780869758\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch750.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 751 [512/54000 (1%)] Loss: -1923.431396\n",
      "Train Epoch: 751 [11776/54000 (22%)] Loss: -1694.997803\n",
      "Train Epoch: 751 [23040/54000 (43%)] Loss: -1777.587769\n",
      "Train Epoch: 751 [34304/54000 (64%)] Loss: -1786.533081\n",
      "Train Epoch: 751 [45568/54000 (84%)] Loss: -1443.712891\n",
      "    epoch          : 751\n",
      "    loss           : -1744.8956093363242\n",
      "    val_loss       : -1768.2970247840508\n",
      "    val_log_likelihood: 1816.8921875\n",
      "    val_log_marginal: 1784.0344355868517\n",
      "Train Epoch: 752 [512/54000 (1%)] Loss: -1992.348145\n",
      "Train Epoch: 752 [11776/54000 (22%)] Loss: -1700.340088\n",
      "Train Epoch: 752 [23040/54000 (43%)] Loss: -1750.085938\n",
      "Train Epoch: 752 [34304/54000 (64%)] Loss: -1755.440186\n",
      "Train Epoch: 752 [45568/54000 (84%)] Loss: -1689.150146\n",
      "    epoch          : 752\n",
      "    loss           : -1759.0703862256344\n",
      "    val_loss       : -1770.6272820368408\n",
      "    val_log_likelihood: 1817.7015625\n",
      "    val_log_marginal: 1784.8981406167288\n",
      "Train Epoch: 753 [512/54000 (1%)] Loss: -1987.062134\n",
      "Train Epoch: 753 [11776/54000 (22%)] Loss: -1847.156494\n",
      "Train Epoch: 753 [23040/54000 (43%)] Loss: -1785.659058\n",
      "Train Epoch: 753 [34304/54000 (64%)] Loss: -1751.979126\n",
      "Train Epoch: 753 [45568/54000 (84%)] Loss: -1687.984497\n",
      "    epoch          : 753\n",
      "    loss           : -1765.049779306544\n",
      "    val_loss       : -1761.863911165297\n",
      "    val_log_likelihood: 1817.054638671875\n",
      "    val_log_marginal: 1784.1735972445458\n",
      "Train Epoch: 754 [512/54000 (1%)] Loss: -1858.434570\n",
      "Train Epoch: 754 [11776/54000 (22%)] Loss: -1741.829956\n",
      "Train Epoch: 754 [23040/54000 (43%)] Loss: -1821.677979\n",
      "Train Epoch: 754 [34304/54000 (64%)] Loss: -1992.309814\n",
      "Train Epoch: 754 [45568/54000 (84%)] Loss: -1695.638184\n",
      "    epoch          : 754\n",
      "    loss           : -1756.6717045850094\n",
      "    val_loss       : -1745.5034598879515\n",
      "    val_log_likelihood: 1817.545263671875\n",
      "    val_log_marginal: 1784.6975619055484\n",
      "Train Epoch: 755 [512/54000 (1%)] Loss: -1927.618408\n",
      "Train Epoch: 755 [11776/54000 (22%)] Loss: -1848.457886\n",
      "Train Epoch: 755 [23040/54000 (43%)] Loss: -1537.261719\n",
      "Train Epoch: 755 [34304/54000 (64%)] Loss: -1694.817139\n",
      "Train Epoch: 755 [45568/54000 (84%)] Loss: -1562.670410\n",
      "    epoch          : 755\n",
      "    loss           : -1760.8472646581065\n",
      "    val_loss       : -1755.8164916286246\n",
      "    val_log_likelihood: 1816.9651977539063\n",
      "    val_log_marginal: 1784.1408725846559\n",
      "Train Epoch: 756 [512/54000 (1%)] Loss: -1851.990234\n",
      "Train Epoch: 756 [11776/54000 (22%)] Loss: -1759.322388\n",
      "Train Epoch: 756 [23040/54000 (43%)] Loss: -1754.576782\n",
      "Train Epoch: 756 [34304/54000 (64%)] Loss: -1815.410400\n",
      "Train Epoch: 756 [45568/54000 (84%)] Loss: -1780.819336\n",
      "    epoch          : 756\n",
      "    loss           : -1754.2850982363861\n",
      "    val_loss       : -1770.250658487901\n",
      "    val_log_likelihood: 1817.252880859375\n",
      "    val_log_marginal: 1784.4862409042285\n",
      "Train Epoch: 757 [512/54000 (1%)] Loss: -1699.492065\n",
      "Train Epoch: 757 [11776/54000 (22%)] Loss: -1708.469360\n",
      "Train Epoch: 757 [23040/54000 (43%)] Loss: -1688.683105\n",
      "Train Epoch: 757 [34304/54000 (64%)] Loss: -1532.506592\n",
      "Train Epoch: 757 [45568/54000 (84%)] Loss: -1643.030518\n",
      "    epoch          : 757\n",
      "    loss           : -1761.5496366897432\n",
      "    val_loss       : -1783.9714843899012\n",
      "    val_log_likelihood: 1817.155419921875\n",
      "    val_log_marginal: 1784.2977728191763\n",
      "Train Epoch: 758 [512/54000 (1%)] Loss: -1991.260864\n",
      "Train Epoch: 758 [11776/54000 (22%)] Loss: -1846.492554\n",
      "Train Epoch: 758 [23040/54000 (43%)] Loss: -1784.202637\n",
      "Train Epoch: 758 [34304/54000 (64%)] Loss: -1782.999268\n",
      "Train Epoch: 758 [45568/54000 (84%)] Loss: -1779.923340\n",
      "    epoch          : 758\n",
      "    loss           : -1750.6548480043316\n",
      "    val_loss       : -1748.228186960332\n",
      "    val_log_likelihood: 1817.1757568359376\n",
      "    val_log_marginal: 1784.3334029842167\n",
      "Train Epoch: 759 [512/54000 (1%)] Loss: -1992.923340\n",
      "Train Epoch: 759 [11776/54000 (22%)] Loss: -1738.941895\n",
      "Train Epoch: 759 [23040/54000 (43%)] Loss: -1697.535278\n",
      "Train Epoch: 759 [34304/54000 (64%)] Loss: -1627.880371\n",
      "Train Epoch: 759 [45568/54000 (84%)] Loss: -1782.308472\n",
      "    epoch          : 759\n",
      "    loss           : -1758.4887284382735\n",
      "    val_loss       : -1754.378851564601\n",
      "    val_log_likelihood: 1817.4013305664062\n",
      "    val_log_marginal: 1784.5663324017078\n",
      "Train Epoch: 760 [512/54000 (1%)] Loss: -1854.783813\n",
      "Train Epoch: 760 [11776/54000 (22%)] Loss: -1597.796753\n",
      "Train Epoch: 760 [23040/54000 (43%)] Loss: -1779.733154\n",
      "Train Epoch: 760 [34304/54000 (64%)] Loss: -1681.171509\n",
      "Train Epoch: 760 [45568/54000 (84%)] Loss: -1778.301514\n",
      "    epoch          : 760\n",
      "    loss           : -1768.2371922861232\n",
      "    val_loss       : -1736.6663038052618\n",
      "    val_log_likelihood: 1815.8722290039063\n",
      "    val_log_marginal: 1783.0701120186598\n",
      "Train Epoch: 761 [512/54000 (1%)] Loss: -1991.403809\n",
      "Train Epoch: 761 [11776/54000 (22%)] Loss: -1845.998047\n",
      "Train Epoch: 761 [23040/54000 (43%)] Loss: -1782.378662\n",
      "Train Epoch: 761 [34304/54000 (64%)] Loss: -1780.177612\n",
      "Train Epoch: 761 [45568/54000 (84%)] Loss: -1810.909302\n",
      "    epoch          : 761\n",
      "    loss           : -1762.4284184521969\n",
      "    val_loss       : -1757.392664901726\n",
      "    val_log_likelihood: 1817.0202270507812\n",
      "    val_log_marginal: 1784.142497490719\n",
      "Train Epoch: 762 [512/54000 (1%)] Loss: -1992.637817\n",
      "Train Epoch: 762 [11776/54000 (22%)] Loss: -1739.454102\n",
      "Train Epoch: 762 [23040/54000 (43%)] Loss: -1786.768799\n",
      "Train Epoch: 762 [34304/54000 (64%)] Loss: -1639.423584\n",
      "Train Epoch: 762 [45568/54000 (84%)] Loss: -1684.503418\n",
      "    epoch          : 762\n",
      "    loss           : -1763.1637555112934\n",
      "    val_loss       : -1708.4510525397957\n",
      "    val_log_likelihood: 1817.1014282226563\n",
      "    val_log_marginal: 1784.247696077901\n",
      "Train Epoch: 763 [512/54000 (1%)] Loss: -1922.431030\n",
      "Train Epoch: 763 [11776/54000 (22%)] Loss: -1822.113281\n",
      "Train Epoch: 763 [23040/54000 (43%)] Loss: -1742.065674\n",
      "Train Epoch: 763 [34304/54000 (64%)] Loss: -1701.724609\n",
      "Train Epoch: 763 [45568/54000 (84%)] Loss: -1560.778564\n",
      "    epoch          : 763\n",
      "    loss           : -1752.9578301458075\n",
      "    val_loss       : -1769.653537862189\n",
      "    val_log_likelihood: 1816.7053466796874\n",
      "    val_log_marginal: 1783.8993466120214\n",
      "Train Epoch: 764 [512/54000 (1%)] Loss: -1991.714722\n",
      "Train Epoch: 764 [11776/54000 (22%)] Loss: -1856.715088\n",
      "Train Epoch: 764 [23040/54000 (43%)] Loss: -1688.121826\n",
      "Train Epoch: 764 [34304/54000 (64%)] Loss: -1787.820679\n",
      "Train Epoch: 764 [45568/54000 (84%)] Loss: -1753.957764\n",
      "    epoch          : 764\n",
      "    loss           : -1766.5792151724938\n",
      "    val_loss       : -1756.1617819506675\n",
      "    val_log_likelihood: 1816.793310546875\n",
      "    val_log_marginal: 1783.975732735917\n",
      "Train Epoch: 765 [512/54000 (1%)] Loss: -1990.394775\n",
      "Train Epoch: 765 [11776/54000 (22%)] Loss: -1602.423462\n",
      "Train Epoch: 765 [23040/54000 (43%)] Loss: -1702.051514\n",
      "Train Epoch: 765 [34304/54000 (64%)] Loss: -1822.342651\n",
      "Train Epoch: 765 [45568/54000 (84%)] Loss: -1679.919800\n",
      "    epoch          : 765\n",
      "    loss           : -1757.104055876779\n",
      "    val_loss       : -1727.030052267015\n",
      "    val_log_likelihood: 1816.93779296875\n",
      "    val_log_marginal: 1784.0832946695427\n",
      "Train Epoch: 766 [512/54000 (1%)] Loss: -1990.814941\n",
      "Train Epoch: 766 [11776/54000 (22%)] Loss: -1853.369995\n",
      "Train Epoch: 766 [23040/54000 (43%)] Loss: -1699.050781\n",
      "Train Epoch: 766 [34304/54000 (64%)] Loss: -1752.176025\n",
      "Train Epoch: 766 [45568/54000 (84%)] Loss: -1683.084961\n",
      "    epoch          : 766\n",
      "    loss           : -1756.8609304900217\n",
      "    val_loss       : -1734.5248135514557\n",
      "    val_log_likelihood: 1817.7065673828124\n",
      "    val_log_marginal: 1784.8043016474433\n",
      "Train Epoch: 767 [512/54000 (1%)] Loss: -1697.866455\n",
      "Train Epoch: 767 [11776/54000 (22%)] Loss: -1695.987183\n",
      "Train Epoch: 767 [23040/54000 (43%)] Loss: -1819.653809\n",
      "Train Epoch: 767 [34304/54000 (64%)] Loss: -1685.858154\n",
      "Train Epoch: 767 [45568/54000 (84%)] Loss: -1575.199463\n",
      "    epoch          : 767\n",
      "    loss           : -1758.6122357963336\n",
      "    val_loss       : -1729.5508321337402\n",
      "    val_log_likelihood: 1816.5358642578126\n",
      "    val_log_marginal: 1783.727625599191\n",
      "Train Epoch: 768 [512/54000 (1%)] Loss: -1990.904785\n",
      "Train Epoch: 768 [11776/54000 (22%)] Loss: -1783.769287\n",
      "Train Epoch: 768 [23040/54000 (43%)] Loss: -1701.502197\n",
      "Train Epoch: 768 [34304/54000 (64%)] Loss: -1817.806763\n",
      "Train Epoch: 768 [45568/54000 (84%)] Loss: -1784.719604\n",
      "    epoch          : 768\n",
      "    loss           : -1755.5436056911356\n",
      "    val_loss       : -1723.1069418616594\n",
      "    val_log_likelihood: 1817.1771850585938\n",
      "    val_log_marginal: 1784.3132402044273\n",
      "Train Epoch: 769 [512/54000 (1%)] Loss: -1987.430542\n",
      "Train Epoch: 769 [11776/54000 (22%)] Loss: -1741.839844\n",
      "Train Epoch: 769 [23040/54000 (43%)] Loss: -1785.288818\n",
      "Train Epoch: 769 [34304/54000 (64%)] Loss: -1991.074341\n",
      "Train Epoch: 769 [45568/54000 (84%)] Loss: -1775.383301\n",
      "    epoch          : 769\n",
      "    loss           : -1751.2982383199258\n",
      "    val_loss       : -1749.5506623549386\n",
      "    val_log_likelihood: 1817.9437133789063\n",
      "    val_log_marginal: 1785.0758224848676\n",
      "Train Epoch: 770 [512/54000 (1%)] Loss: -1920.750000\n",
      "Train Epoch: 770 [11776/54000 (22%)] Loss: -1739.046265\n",
      "Train Epoch: 770 [23040/54000 (43%)] Loss: -1752.937012\n",
      "Train Epoch: 770 [34304/54000 (64%)] Loss: -1786.554565\n",
      "Train Epoch: 770 [45568/54000 (84%)] Loss: -1773.273560\n",
      "    epoch          : 770\n",
      "    loss           : -1753.2886419012996\n",
      "    val_loss       : -1776.3232719009743\n",
      "    val_log_likelihood: 1816.1785766601563\n",
      "    val_log_marginal: 1783.3890194293112\n",
      "Train Epoch: 771 [512/54000 (1%)] Loss: -1991.524292\n",
      "Train Epoch: 771 [11776/54000 (22%)] Loss: -1596.578735\n",
      "Train Epoch: 771 [23040/54000 (43%)] Loss: -1779.349487\n",
      "Train Epoch: 771 [34304/54000 (64%)] Loss: -1783.052490\n",
      "Train Epoch: 771 [45568/54000 (84%)] Loss: -1777.724121\n",
      "    epoch          : 771\n",
      "    loss           : -1758.8830240079672\n",
      "    val_loss       : -1725.9368390629068\n",
      "    val_log_likelihood: 1816.8509887695313\n",
      "    val_log_marginal: 1784.0189746875317\n",
      "Train Epoch: 772 [512/54000 (1%)] Loss: -1992.480225\n",
      "Train Epoch: 772 [11776/54000 (22%)] Loss: -1741.892334\n",
      "Train Epoch: 772 [23040/54000 (43%)] Loss: -1545.586914\n",
      "Train Epoch: 772 [34304/54000 (64%)] Loss: -1680.498657\n",
      "Train Epoch: 772 [45568/54000 (84%)] Loss: -1757.634766\n",
      "    epoch          : 772\n",
      "    loss           : -1755.988196646813\n",
      "    val_loss       : -1731.5840859325604\n",
      "    val_log_likelihood: 1816.8570190429687\n",
      "    val_log_marginal: 1784.0786823924645\n",
      "Train Epoch: 773 [512/54000 (1%)] Loss: -1991.277100\n",
      "Train Epoch: 773 [11776/54000 (22%)] Loss: -1700.548706\n",
      "Train Epoch: 773 [23040/54000 (43%)] Loss: -1696.812866\n",
      "Train Epoch: 773 [34304/54000 (64%)] Loss: -1819.546265\n",
      "Train Epoch: 773 [45568/54000 (84%)] Loss: -1754.549561\n",
      "    epoch          : 773\n",
      "    loss           : -1749.1688159904857\n",
      "    val_loss       : -1758.6725776553153\n",
      "    val_log_likelihood: 1816.8567993164063\n",
      "    val_log_marginal: 1784.1337086904794\n",
      "Train Epoch: 774 [512/54000 (1%)] Loss: -1993.977661\n",
      "Train Epoch: 774 [11776/54000 (22%)] Loss: -1733.346558\n",
      "Train Epoch: 774 [23040/54000 (43%)] Loss: -1820.995483\n",
      "Train Epoch: 774 [34304/54000 (64%)] Loss: -1990.551270\n",
      "Train Epoch: 774 [45568/54000 (84%)] Loss: -1754.889160\n",
      "    epoch          : 774\n",
      "    loss           : -1752.1706434193225\n",
      "    val_loss       : -1748.5097977124155\n",
      "    val_log_likelihood: 1815.85869140625\n",
      "    val_log_marginal: 1783.0220293905622\n",
      "Train Epoch: 775 [512/54000 (1%)] Loss: -1991.970581\n",
      "Train Epoch: 775 [11776/54000 (22%)] Loss: -1692.436035\n",
      "Train Epoch: 775 [23040/54000 (43%)] Loss: -1701.001465\n",
      "Train Epoch: 775 [34304/54000 (64%)] Loss: -1772.274414\n",
      "Train Epoch: 775 [45568/54000 (84%)] Loss: -1781.726440\n",
      "    epoch          : 775\n",
      "    loss           : -1761.784802125232\n",
      "    val_loss       : -1757.0443152328953\n",
      "    val_log_likelihood: 1816.031103515625\n",
      "    val_log_marginal: 1783.315217037833\n",
      "Train Epoch: 776 [512/54000 (1%)] Loss: -1853.937744\n",
      "Train Epoch: 776 [11776/54000 (22%)] Loss: -1548.128174\n",
      "Train Epoch: 776 [23040/54000 (43%)] Loss: -1534.582031\n",
      "Train Epoch: 776 [34304/54000 (64%)] Loss: -1757.340454\n",
      "Train Epoch: 776 [45568/54000 (84%)] Loss: -1575.142578\n",
      "    epoch          : 776\n",
      "    loss           : -1758.2642036664604\n",
      "    val_loss       : -1726.129469094798\n",
      "    val_log_likelihood: 1816.8329223632813\n",
      "    val_log_marginal: 1783.9814071078902\n",
      "Train Epoch: 777 [512/54000 (1%)] Loss: -1989.682129\n",
      "Train Epoch: 777 [11776/54000 (22%)] Loss: -1601.383057\n",
      "Train Epoch: 777 [23040/54000 (43%)] Loss: -1695.302856\n",
      "Train Epoch: 777 [34304/54000 (64%)] Loss: -1990.988037\n",
      "Train Epoch: 777 [45568/54000 (84%)] Loss: -1650.160156\n",
      "    epoch          : 777\n",
      "    loss           : -1752.5407799446937\n",
      "    val_loss       : -1783.8592270795257\n",
      "    val_log_likelihood: 1816.848291015625\n",
      "    val_log_marginal: 1784.078573575684\n",
      "Train Epoch: 778 [512/54000 (1%)] Loss: -1991.474487\n",
      "Train Epoch: 778 [11776/54000 (22%)] Loss: -1740.064941\n",
      "Train Epoch: 778 [23040/54000 (43%)] Loss: -1821.894287\n",
      "Train Epoch: 778 [34304/54000 (64%)] Loss: -1993.551392\n",
      "Train Epoch: 778 [45568/54000 (84%)] Loss: -1784.046753\n",
      "    epoch          : 778\n",
      "    loss           : -1756.0755107615253\n",
      "    val_loss       : -1731.731936156936\n",
      "    val_log_likelihood: 1816.8351684570312\n",
      "    val_log_marginal: 1784.0676331546158\n",
      "Train Epoch: 779 [512/54000 (1%)] Loss: -1988.851929\n",
      "Train Epoch: 779 [11776/54000 (22%)] Loss: -1725.147949\n",
      "Train Epoch: 779 [23040/54000 (43%)] Loss: -1779.794067\n",
      "Train Epoch: 779 [34304/54000 (64%)] Loss: -1786.409912\n",
      "Train Epoch: 779 [45568/54000 (84%)] Loss: -1781.858154\n",
      "    epoch          : 779\n",
      "    loss           : -1759.1555743831218\n",
      "    val_loss       : -1784.4652928162366\n",
      "    val_log_likelihood: 1817.719873046875\n",
      "    val_log_marginal: 1784.9412232842697\n",
      "Train Epoch: 780 [512/54000 (1%)] Loss: -1821.775757\n",
      "Train Epoch: 780 [11776/54000 (22%)] Loss: -1849.142212\n",
      "Train Epoch: 780 [23040/54000 (43%)] Loss: -1757.539551\n",
      "Train Epoch: 780 [34304/54000 (64%)] Loss: -1753.714478\n",
      "Train Epoch: 780 [45568/54000 (84%)] Loss: -1781.463379\n",
      "    epoch          : 780\n",
      "    loss           : -1753.7012395575496\n",
      "    val_loss       : -1770.3967387443408\n",
      "    val_log_likelihood: 1817.3098022460938\n",
      "    val_log_marginal: 1784.5732474975412\n",
      "Train Epoch: 781 [512/54000 (1%)] Loss: -1606.829590\n",
      "Train Epoch: 781 [11776/54000 (22%)] Loss: -1848.393555\n",
      "Train Epoch: 781 [23040/54000 (43%)] Loss: -1699.418823\n",
      "Train Epoch: 781 [34304/54000 (64%)] Loss: -1991.115112\n",
      "Train Epoch: 781 [45568/54000 (84%)] Loss: -1755.727539\n",
      "    epoch          : 781\n",
      "    loss           : -1757.1810302734375\n",
      "    val_loss       : -1755.2424358814956\n",
      "    val_log_likelihood: 1816.5786865234375\n",
      "    val_log_marginal: 1783.8067357841906\n",
      "Train Epoch: 782 [512/54000 (1%)] Loss: -1991.092529\n",
      "Train Epoch: 782 [11776/54000 (22%)] Loss: -1818.889282\n",
      "Train Epoch: 782 [23040/54000 (43%)] Loss: -1699.392334\n",
      "Train Epoch: 782 [34304/54000 (64%)] Loss: -1987.256470\n",
      "Train Epoch: 782 [45568/54000 (84%)] Loss: -1752.472534\n",
      "    epoch          : 782\n",
      "    loss           : -1748.1941232209158\n",
      "    val_loss       : -1782.494198884815\n",
      "    val_log_likelihood: 1815.5689453125\n",
      "    val_log_marginal: 1782.8106566496863\n",
      "Train Epoch: 783 [512/54000 (1%)] Loss: -1988.144409\n",
      "Train Epoch: 783 [11776/54000 (22%)] Loss: -1740.906250\n",
      "Train Epoch: 783 [23040/54000 (43%)] Loss: -1703.017334\n",
      "Train Epoch: 783 [34304/54000 (64%)] Loss: -1747.969727\n",
      "Train Epoch: 783 [45568/54000 (84%)] Loss: -1683.734985\n",
      "    epoch          : 783\n",
      "    loss           : -1767.023407284576\n",
      "    val_loss       : -1731.672871438414\n",
      "    val_log_likelihood: 1816.6451416015625\n",
      "    val_log_marginal: 1783.9176697488626\n",
      "Train Epoch: 784 [512/54000 (1%)] Loss: -1929.715454\n",
      "Train Epoch: 784 [11776/54000 (22%)] Loss: -1743.349976\n",
      "Train Epoch: 784 [23040/54000 (43%)] Loss: -1705.797363\n",
      "Train Epoch: 784 [34304/54000 (64%)] Loss: -1679.191284\n",
      "Train Epoch: 784 [45568/54000 (84%)] Loss: -1783.030396\n",
      "    epoch          : 784\n",
      "    loss           : -1754.7041922087717\n",
      "    val_loss       : -1776.032038644515\n",
      "    val_log_likelihood: 1815.6553955078125\n",
      "    val_log_marginal: 1782.9680807014054\n",
      "Train Epoch: 785 [512/54000 (1%)] Loss: -1989.979980\n",
      "Train Epoch: 785 [11776/54000 (22%)] Loss: -1611.706787\n",
      "Train Epoch: 785 [23040/54000 (43%)] Loss: -1777.295898\n",
      "Train Epoch: 785 [34304/54000 (64%)] Loss: -1927.806030\n",
      "Train Epoch: 785 [45568/54000 (84%)] Loss: -1752.706543\n",
      "    epoch          : 785\n",
      "    loss           : -1757.2625394009128\n",
      "    val_loss       : -1770.4761763716117\n",
      "    val_log_likelihood: 1817.3928588867188\n",
      "    val_log_marginal: 1784.6268109741118\n",
      "Train Epoch: 786 [512/54000 (1%)] Loss: -1922.994385\n",
      "Train Epoch: 786 [11776/54000 (22%)] Loss: -1742.184570\n",
      "Train Epoch: 786 [23040/54000 (43%)] Loss: -1704.093994\n",
      "Train Epoch: 786 [34304/54000 (64%)] Loss: -1782.376099\n",
      "Train Epoch: 786 [45568/54000 (84%)] Loss: -1782.035278\n",
      "    epoch          : 786\n",
      "    loss           : -1763.1242434057858\n",
      "    val_loss       : -1721.4945164354517\n",
      "    val_log_likelihood: 1817.8321166992187\n",
      "    val_log_marginal: 1785.0517227408577\n",
      "Train Epoch: 787 [512/54000 (1%)] Loss: -1991.362305\n",
      "Train Epoch: 787 [11776/54000 (22%)] Loss: -1700.793823\n",
      "Train Epoch: 787 [23040/54000 (43%)] Loss: -1819.569824\n",
      "Train Epoch: 787 [34304/54000 (64%)] Loss: -1777.228760\n",
      "Train Epoch: 787 [45568/54000 (84%)] Loss: -1632.266846\n",
      "    epoch          : 787\n",
      "    loss           : -1747.726888826578\n",
      "    val_loss       : -1757.379644709453\n",
      "    val_log_likelihood: 1816.719873046875\n",
      "    val_log_marginal: 1784.0075935640875\n",
      "Train Epoch: 788 [512/54000 (1%)] Loss: -1988.018066\n",
      "Train Epoch: 788 [11776/54000 (22%)] Loss: -1735.038086\n",
      "Train Epoch: 788 [23040/54000 (43%)] Loss: -1544.165039\n",
      "Train Epoch: 788 [34304/54000 (64%)] Loss: -1543.229004\n",
      "Train Epoch: 788 [45568/54000 (84%)] Loss: -1775.346069\n",
      "    epoch          : 788\n",
      "    loss           : -1753.56000299737\n",
      "    val_loss       : -1764.6398478869348\n",
      "    val_log_likelihood: 1817.2668823242188\n",
      "    val_log_marginal: 1784.5533336121746\n",
      "Train Epoch: 789 [512/54000 (1%)] Loss: -1613.093506\n",
      "Train Epoch: 789 [11776/54000 (22%)] Loss: -1849.775879\n",
      "Train Epoch: 789 [23040/54000 (43%)] Loss: -1816.942383\n",
      "Train Epoch: 789 [34304/54000 (64%)] Loss: -1572.278564\n",
      "Train Epoch: 789 [45568/54000 (84%)] Loss: -1781.708984\n",
      "    epoch          : 789\n",
      "    loss           : -1753.0823201094524\n",
      "    val_loss       : -1769.021967254579\n",
      "    val_log_likelihood: 1816.3975952148437\n",
      "    val_log_marginal: 1783.6322972133196\n",
      "Train Epoch: 790 [512/54000 (1%)] Loss: -1992.606689\n",
      "Train Epoch: 790 [11776/54000 (22%)] Loss: -1732.047729\n",
      "Train Epoch: 790 [23040/54000 (43%)] Loss: -1756.737427\n",
      "Train Epoch: 790 [34304/54000 (64%)] Loss: -1822.387207\n",
      "Train Epoch: 790 [45568/54000 (84%)] Loss: -1635.140991\n",
      "    epoch          : 790\n",
      "    loss           : -1760.878744295328\n",
      "    val_loss       : -1739.721421012096\n",
      "    val_log_likelihood: 1817.6144409179688\n",
      "    val_log_marginal: 1784.8029759547558\n",
      "Train Epoch: 791 [512/54000 (1%)] Loss: -1989.240601\n",
      "Train Epoch: 791 [11776/54000 (22%)] Loss: -1700.827271\n",
      "Train Epoch: 791 [23040/54000 (43%)] Loss: -1696.187256\n",
      "Train Epoch: 791 [34304/54000 (64%)] Loss: -1761.342041\n",
      "Train Epoch: 791 [45568/54000 (84%)] Loss: -1790.345947\n",
      "    epoch          : 791\n",
      "    loss           : -1755.261226842899\n",
      "    val_loss       : -1736.26592114456\n",
      "    val_log_likelihood: 1817.82197265625\n",
      "    val_log_marginal: 1785.1416229646652\n",
      "Train Epoch: 792 [512/54000 (1%)] Loss: -1990.556274\n",
      "Train Epoch: 792 [11776/54000 (22%)] Loss: -1788.412354\n",
      "Train Epoch: 792 [23040/54000 (43%)] Loss: -1755.661865\n",
      "Train Epoch: 792 [34304/54000 (64%)] Loss: -1781.846924\n",
      "Train Epoch: 792 [45568/54000 (84%)] Loss: -1629.593384\n",
      "    epoch          : 792\n",
      "    loss           : -1755.1417949412128\n",
      "    val_loss       : -1762.3659849340097\n",
      "    val_log_likelihood: 1817.3285888671876\n",
      "    val_log_marginal: 1784.5931427945084\n",
      "Train Epoch: 793 [512/54000 (1%)] Loss: -1848.393921\n",
      "Train Epoch: 793 [11776/54000 (22%)] Loss: -1853.333740\n",
      "Train Epoch: 793 [23040/54000 (43%)] Loss: -1554.233643\n",
      "Train Epoch: 793 [34304/54000 (64%)] Loss: -1703.134766\n",
      "Train Epoch: 793 [45568/54000 (84%)] Loss: -1464.909912\n",
      "    epoch          : 793\n",
      "    loss           : -1768.2844407487623\n",
      "    val_loss       : -1747.3112974479795\n",
      "    val_log_likelihood: 1817.3950805664062\n",
      "    val_log_marginal: 1784.6600937138348\n",
      "Train Epoch: 794 [512/54000 (1%)] Loss: -1846.060303\n",
      "Train Epoch: 794 [11776/54000 (22%)] Loss: -1704.081665\n",
      "Train Epoch: 794 [23040/54000 (43%)] Loss: -1557.230103\n",
      "Train Epoch: 794 [34304/54000 (64%)] Loss: -1989.033691\n",
      "Train Epoch: 794 [45568/54000 (84%)] Loss: -1821.196533\n",
      "    epoch          : 794\n",
      "    loss           : -1754.9175602374692\n",
      "    val_loss       : -1773.216591306962\n",
      "    val_log_likelihood: 1817.5496337890625\n",
      "    val_log_marginal: 1784.7846151884646\n",
      "Train Epoch: 795 [512/54000 (1%)] Loss: -1695.770264\n",
      "Train Epoch: 795 [11776/54000 (22%)] Loss: -1739.938232\n",
      "Train Epoch: 795 [23040/54000 (43%)] Loss: -1693.063965\n",
      "Train Epoch: 795 [34304/54000 (64%)] Loss: -1686.790527\n",
      "Train Epoch: 795 [45568/54000 (84%)] Loss: -1785.743774\n",
      "    epoch          : 795\n",
      "    loss           : -1764.3722274327042\n",
      "    val_loss       : -1771.152716286108\n",
      "    val_log_likelihood: 1817.1611450195312\n",
      "    val_log_marginal: 1784.5180962424427\n",
      "Train Epoch: 796 [512/54000 (1%)] Loss: -1925.869873\n",
      "Train Epoch: 796 [11776/54000 (22%)] Loss: -1850.065430\n",
      "Train Epoch: 796 [23040/54000 (43%)] Loss: -1782.768311\n",
      "Train Epoch: 796 [34304/54000 (64%)] Loss: -1696.523193\n",
      "Train Epoch: 796 [45568/54000 (84%)] Loss: -1576.813354\n",
      "    epoch          : 796\n",
      "    loss           : -1755.8328760732518\n",
      "    val_loss       : -1771.2911886844784\n",
      "    val_log_likelihood: 1817.025341796875\n",
      "    val_log_marginal: 1784.3487118106336\n",
      "Train Epoch: 797 [512/54000 (1%)] Loss: -1991.558105\n",
      "Train Epoch: 797 [11776/54000 (22%)] Loss: -1775.005371\n",
      "Train Epoch: 797 [23040/54000 (43%)] Loss: -1641.247681\n",
      "Train Epoch: 797 [34304/54000 (64%)] Loss: -1689.071777\n",
      "Train Epoch: 797 [45568/54000 (84%)] Loss: -1783.755249\n",
      "    epoch          : 797\n",
      "    loss           : -1760.9909039487934\n",
      "    val_loss       : -1763.196289478056\n",
      "    val_log_likelihood: 1817.255517578125\n",
      "    val_log_marginal: 1784.5261191387315\n",
      "Train Epoch: 798 [512/54000 (1%)] Loss: -1987.084961\n",
      "Train Epoch: 798 [11776/54000 (22%)] Loss: -1742.777466\n",
      "Train Epoch: 798 [23040/54000 (43%)] Loss: -1756.351929\n",
      "Train Epoch: 798 [34304/54000 (64%)] Loss: -1818.865234\n",
      "Train Epoch: 798 [45568/54000 (84%)] Loss: -1776.053711\n",
      "    epoch          : 798\n",
      "    loss           : -1751.6744203473081\n",
      "    val_loss       : -1717.4443805435671\n",
      "    val_log_likelihood: 1817.5756103515625\n",
      "    val_log_marginal: 1784.9027048565454\n",
      "Train Epoch: 799 [512/54000 (1%)] Loss: -1990.636963\n",
      "Train Epoch: 799 [11776/54000 (22%)] Loss: -1849.458618\n",
      "Train Epoch: 799 [23040/54000 (43%)] Loss: -1775.836304\n",
      "Train Epoch: 799 [34304/54000 (64%)] Loss: -1580.125488\n",
      "Train Epoch: 799 [45568/54000 (84%)] Loss: -1541.161743\n",
      "    epoch          : 799\n",
      "    loss           : -1755.945178343518\n",
      "    val_loss       : -1746.446988640353\n",
      "    val_log_likelihood: 1816.761328125\n",
      "    val_log_marginal: 1784.0687686033489\n",
      "Train Epoch: 800 [512/54000 (1%)] Loss: -1993.499023\n",
      "Train Epoch: 800 [11776/54000 (22%)] Loss: -1755.364502\n",
      "Train Epoch: 800 [23040/54000 (43%)] Loss: -1782.718994\n",
      "Train Epoch: 800 [34304/54000 (64%)] Loss: -1780.617798\n",
      "Train Epoch: 800 [45568/54000 (84%)] Loss: -1772.626221\n",
      "    epoch          : 800\n",
      "    loss           : -1757.969481213258\n",
      "    val_loss       : -1784.1494018286467\n",
      "    val_log_likelihood: 1817.2787475585938\n",
      "    val_log_marginal: 1784.5760089654475\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch800.pth ...\n",
      "Train Epoch: 801 [512/54000 (1%)] Loss: -1990.431885\n",
      "Train Epoch: 801 [11776/54000 (22%)] Loss: -1850.257568\n",
      "Train Epoch: 801 [23040/54000 (43%)] Loss: -1749.901611\n",
      "Train Epoch: 801 [34304/54000 (64%)] Loss: -1648.955078\n",
      "Train Epoch: 801 [45568/54000 (84%)] Loss: -1644.883667\n",
      "    epoch          : 801\n",
      "    loss           : -1759.9532748685024\n",
      "    val_loss       : -1743.7652329048142\n",
      "    val_log_likelihood: 1817.0373046875\n",
      "    val_log_marginal: 1784.2236244920641\n",
      "Train Epoch: 802 [512/54000 (1%)] Loss: -1990.544922\n",
      "Train Epoch: 802 [11776/54000 (22%)] Loss: -1738.795776\n",
      "Train Epoch: 802 [23040/54000 (43%)] Loss: -1818.269775\n",
      "Train Epoch: 802 [34304/54000 (64%)] Loss: -1929.116943\n",
      "Train Epoch: 802 [45568/54000 (84%)] Loss: -1779.027832\n",
      "    epoch          : 802\n",
      "    loss           : -1749.8245390334932\n",
      "    val_loss       : -1746.6240331225097\n",
      "    val_log_likelihood: 1817.7911499023437\n",
      "    val_log_marginal: 1785.0325602126454\n",
      "Train Epoch: 803 [512/54000 (1%)] Loss: -1990.565674\n",
      "Train Epoch: 803 [11776/54000 (22%)] Loss: -1845.329224\n",
      "Train Epoch: 803 [23040/54000 (43%)] Loss: -1779.477783\n",
      "Train Epoch: 803 [34304/54000 (64%)] Loss: -1689.764404\n",
      "Train Epoch: 803 [45568/54000 (84%)] Loss: -1649.084473\n",
      "    epoch          : 803\n",
      "    loss           : -1763.9339043645575\n",
      "    val_loss       : -1772.4694902658462\n",
      "    val_log_likelihood: 1818.318310546875\n",
      "    val_log_marginal: 1785.6480929667268\n",
      "Train Epoch: 804 [512/54000 (1%)] Loss: -1991.864990\n",
      "Train Epoch: 804 [11776/54000 (22%)] Loss: -1618.859863\n",
      "Train Epoch: 804 [23040/54000 (43%)] Loss: -1699.523926\n",
      "Train Epoch: 804 [34304/54000 (64%)] Loss: -1993.274414\n",
      "Train Epoch: 804 [45568/54000 (84%)] Loss: -1688.289429\n",
      "    epoch          : 804\n",
      "    loss           : -1748.280192460164\n",
      "    val_loss       : -1783.6252612860874\n",
      "    val_log_likelihood: 1816.84853515625\n",
      "    val_log_marginal: 1784.0837077805784\n",
      "Train Epoch: 805 [512/54000 (1%)] Loss: -1992.389282\n",
      "Train Epoch: 805 [11776/54000 (22%)] Loss: -1732.682495\n",
      "Train Epoch: 805 [23040/54000 (43%)] Loss: -1757.351807\n",
      "Train Epoch: 805 [34304/54000 (64%)] Loss: -1930.999023\n",
      "Train Epoch: 805 [45568/54000 (84%)] Loss: -1784.648193\n",
      "    epoch          : 805\n",
      "    loss           : -1763.1849147683322\n",
      "    val_loss       : -1763.3666453570127\n",
      "    val_log_likelihood: 1817.6178955078126\n",
      "    val_log_marginal: 1784.9063380908221\n",
      "Train Epoch: 806 [512/54000 (1%)] Loss: -1995.747070\n",
      "Train Epoch: 806 [11776/54000 (22%)] Loss: -1711.560913\n",
      "Train Epoch: 806 [23040/54000 (43%)] Loss: -1688.366943\n",
      "Train Epoch: 806 [34304/54000 (64%)] Loss: -1784.924194\n",
      "Train Epoch: 806 [45568/54000 (84%)] Loss: -1647.965454\n",
      "    epoch          : 806\n",
      "    loss           : -1744.2179015508973\n",
      "    val_loss       : -1770.2538644429296\n",
      "    val_log_likelihood: 1817.7564086914062\n",
      "    val_log_marginal: 1785.0072152459197\n",
      "Train Epoch: 807 [512/54000 (1%)] Loss: -1989.052979\n",
      "Train Epoch: 807 [11776/54000 (22%)] Loss: -1734.069580\n",
      "Train Epoch: 807 [23040/54000 (43%)] Loss: -1816.078125\n",
      "Train Epoch: 807 [34304/54000 (64%)] Loss: -1752.841064\n",
      "Train Epoch: 807 [45568/54000 (84%)] Loss: -1586.706543\n",
      "    epoch          : 807\n",
      "    loss           : -1767.1945039352568\n",
      "    val_loss       : -1746.0084452381357\n",
      "    val_log_likelihood: 1818.3153930664062\n",
      "    val_log_marginal: 1785.5210094303416\n",
      "Train Epoch: 808 [512/54000 (1%)] Loss: -1990.910767\n",
      "Train Epoch: 808 [11776/54000 (22%)] Loss: -1640.488159\n",
      "Train Epoch: 808 [23040/54000 (43%)] Loss: -1815.130859\n",
      "Train Epoch: 808 [34304/54000 (64%)] Loss: -1783.226562\n",
      "Train Epoch: 808 [45568/54000 (84%)] Loss: -1552.146973\n",
      "    epoch          : 808\n",
      "    loss           : -1766.7824815806775\n",
      "    val_loss       : -1758.1326658107341\n",
      "    val_log_likelihood: 1817.8790893554688\n",
      "    val_log_marginal: 1785.1259604275976\n",
      "Train Epoch: 809 [512/54000 (1%)] Loss: -1929.803467\n",
      "Train Epoch: 809 [11776/54000 (22%)] Loss: -1701.403076\n",
      "Train Epoch: 809 [23040/54000 (43%)] Loss: -1759.246338\n",
      "Train Epoch: 809 [34304/54000 (64%)] Loss: -1777.774536\n",
      "Train Epoch: 809 [45568/54000 (84%)] Loss: -1790.804688\n",
      "    epoch          : 809\n",
      "    loss           : -1764.3245559541306\n",
      "    val_loss       : -1784.9645169025287\n",
      "    val_log_likelihood: 1818.3902954101563\n",
      "    val_log_marginal: 1785.641420488432\n",
      "Train Epoch: 810 [512/54000 (1%)] Loss: -1928.419067\n",
      "Train Epoch: 810 [11776/54000 (22%)] Loss: -1739.008179\n",
      "Train Epoch: 810 [23040/54000 (43%)] Loss: -1818.635620\n",
      "Train Epoch: 810 [34304/54000 (64%)] Loss: -1992.730713\n",
      "Train Epoch: 810 [45568/54000 (84%)] Loss: -1686.864380\n",
      "    epoch          : 810\n",
      "    loss           : -1762.4195762105508\n",
      "    val_loss       : -1733.847709498927\n",
      "    val_log_likelihood: 1817.3766845703126\n",
      "    val_log_marginal: 1784.6373856886119\n",
      "Train Epoch: 811 [512/54000 (1%)] Loss: -1992.069824\n",
      "Train Epoch: 811 [11776/54000 (22%)] Loss: -1741.341309\n",
      "Train Epoch: 811 [23040/54000 (43%)] Loss: -1588.868774\n",
      "Train Epoch: 811 [34304/54000 (64%)] Loss: -1991.742188\n",
      "Train Epoch: 811 [45568/54000 (84%)] Loss: -1777.444580\n",
      "    epoch          : 811\n",
      "    loss           : -1755.1999402943225\n",
      "    val_loss       : -1765.6470721421763\n",
      "    val_log_likelihood: 1817.7801391601563\n",
      "    val_log_marginal: 1785.0296846490353\n",
      "Train Epoch: 812 [512/54000 (1%)] Loss: -1993.298584\n",
      "Train Epoch: 812 [11776/54000 (22%)] Loss: -1682.324585\n",
      "Train Epoch: 812 [23040/54000 (43%)] Loss: -1736.810425\n",
      "Train Epoch: 812 [34304/54000 (64%)] Loss: -1707.635010\n",
      "Train Epoch: 812 [45568/54000 (84%)] Loss: -1692.230225\n",
      "    epoch          : 812\n",
      "    loss           : -1755.3982090713955\n",
      "    val_loss       : -1769.983533608541\n",
      "    val_log_likelihood: 1817.3328369140625\n",
      "    val_log_marginal: 1784.6176800962537\n",
      "Train Epoch: 813 [512/54000 (1%)] Loss: -1992.430786\n",
      "Train Epoch: 813 [11776/54000 (22%)] Loss: -1852.094116\n",
      "Train Epoch: 813 [23040/54000 (43%)] Loss: -1790.486816\n",
      "Train Epoch: 813 [34304/54000 (64%)] Loss: -1778.891479\n",
      "Train Epoch: 813 [45568/54000 (84%)] Loss: -1788.370117\n",
      "    epoch          : 813\n",
      "    loss           : -1763.842879795792\n",
      "    val_loss       : -1770.2222006618977\n",
      "    val_log_likelihood: 1816.92236328125\n",
      "    val_log_marginal: 1784.1292685616513\n",
      "Train Epoch: 814 [512/54000 (1%)] Loss: -1739.954224\n",
      "Train Epoch: 814 [11776/54000 (22%)] Loss: -1566.245483\n",
      "Train Epoch: 814 [23040/54000 (43%)] Loss: -1701.371948\n",
      "Train Epoch: 814 [34304/54000 (64%)] Loss: -1552.964722\n",
      "Train Epoch: 814 [45568/54000 (84%)] Loss: -1650.160400\n",
      "    epoch          : 814\n",
      "    loss           : -1763.6459042388615\n",
      "    val_loss       : -1744.0130523191765\n",
      "    val_log_likelihood: 1816.9781616210937\n",
      "    val_log_marginal: 1784.2881739910692\n",
      "Train Epoch: 815 [512/54000 (1%)] Loss: -1851.572998\n",
      "Train Epoch: 815 [11776/54000 (22%)] Loss: -1731.686768\n",
      "Train Epoch: 815 [23040/54000 (43%)] Loss: -1703.144043\n",
      "Train Epoch: 815 [34304/54000 (64%)] Loss: -1688.526733\n",
      "Train Epoch: 815 [45568/54000 (84%)] Loss: -1587.964844\n",
      "    epoch          : 815\n",
      "    loss           : -1756.3614441522277\n",
      "    val_loss       : -1745.558677895367\n",
      "    val_log_likelihood: 1817.3146118164063\n",
      "    val_log_marginal: 1784.6546847816558\n",
      "Train Epoch: 816 [512/54000 (1%)] Loss: -1849.636475\n",
      "Train Epoch: 816 [11776/54000 (22%)] Loss: -1735.439209\n",
      "Train Epoch: 816 [23040/54000 (43%)] Loss: -1751.342651\n",
      "Train Epoch: 816 [34304/54000 (64%)] Loss: -1755.891113\n",
      "Train Epoch: 816 [45568/54000 (84%)] Loss: -1783.965820\n",
      "    epoch          : 816\n",
      "    loss           : -1755.863362227336\n",
      "    val_loss       : -1784.250670411624\n",
      "    val_log_likelihood: 1817.471826171875\n",
      "    val_log_marginal: 1784.7588039931907\n",
      "Train Epoch: 817 [512/54000 (1%)] Loss: -1780.878418\n",
      "Train Epoch: 817 [11776/54000 (22%)] Loss: -1851.868286\n",
      "Train Epoch: 817 [23040/54000 (43%)] Loss: -1785.923584\n",
      "Train Epoch: 817 [34304/54000 (64%)] Loss: -1993.673096\n",
      "Train Epoch: 817 [45568/54000 (84%)] Loss: -1687.174072\n",
      "    epoch          : 817\n",
      "    loss           : -1759.4512178024443\n",
      "    val_loss       : -1751.6172070568427\n",
      "    val_log_likelihood: 1817.8521728515625\n",
      "    val_log_marginal: 1785.0899878624828\n",
      "Train Epoch: 818 [512/54000 (1%)] Loss: -1993.132324\n",
      "Train Epoch: 818 [11776/54000 (22%)] Loss: -1752.781006\n",
      "Train Epoch: 818 [23040/54000 (43%)] Loss: -1823.295410\n",
      "Train Epoch: 818 [34304/54000 (64%)] Loss: -1705.046997\n",
      "Train Epoch: 818 [45568/54000 (84%)] Loss: -1755.115601\n",
      "    epoch          : 818\n",
      "    loss           : -1763.5970700707767\n",
      "    val_loss       : -1757.9432973429562\n",
      "    val_log_likelihood: 1817.664697265625\n",
      "    val_log_marginal: 1784.904431057275\n",
      "Train Epoch: 819 [512/54000 (1%)] Loss: -1700.662354\n",
      "Train Epoch: 819 [11776/54000 (22%)] Loss: -1691.994995\n",
      "Train Epoch: 819 [23040/54000 (43%)] Loss: -1824.366821\n",
      "Train Epoch: 819 [34304/54000 (64%)] Loss: -1751.128418\n",
      "Train Epoch: 819 [45568/54000 (84%)] Loss: -1755.089355\n",
      "    epoch          : 819\n",
      "    loss           : -1761.4673274578433\n",
      "    val_loss       : -1784.7407765949145\n",
      "    val_log_likelihood: 1817.9075073242188\n",
      "    val_log_marginal: 1785.2066795814783\n",
      "Train Epoch: 820 [512/54000 (1%)] Loss: -1991.593018\n",
      "Train Epoch: 820 [11776/54000 (22%)] Loss: -1740.107910\n",
      "Train Epoch: 820 [23040/54000 (43%)] Loss: -1819.328735\n",
      "Train Epoch: 820 [34304/54000 (64%)] Loss: -1992.321655\n",
      "Train Epoch: 820 [45568/54000 (84%)] Loss: -1788.369873\n",
      "    epoch          : 820\n",
      "    loss           : -1760.0882483756188\n",
      "    val_loss       : -1733.1483725916594\n",
      "    val_log_likelihood: 1818.29267578125\n",
      "    val_log_marginal: 1785.627549913153\n",
      "Train Epoch: 821 [512/54000 (1%)] Loss: -1995.973022\n",
      "Train Epoch: 821 [11776/54000 (22%)] Loss: -1736.853760\n",
      "Train Epoch: 821 [23040/54000 (43%)] Loss: -1825.540527\n",
      "Train Epoch: 821 [34304/54000 (64%)] Loss: -1812.911743\n",
      "Train Epoch: 821 [45568/54000 (84%)] Loss: -1653.186768\n",
      "    epoch          : 821\n",
      "    loss           : -1760.7602466545482\n",
      "    val_loss       : -1763.9237151866778\n",
      "    val_log_likelihood: 1818.0291137695312\n",
      "    val_log_marginal: 1785.3105928961188\n",
      "Train Epoch: 822 [512/54000 (1%)] Loss: -1854.114990\n",
      "Train Epoch: 822 [11776/54000 (22%)] Loss: -1849.801392\n",
      "Train Epoch: 822 [23040/54000 (43%)] Loss: -1784.659546\n",
      "Train Epoch: 822 [34304/54000 (64%)] Loss: -1761.161499\n",
      "Train Epoch: 822 [45568/54000 (84%)] Loss: -1784.282349\n",
      "    epoch          : 822\n",
      "    loss           : -1763.6374535891089\n",
      "    val_loss       : -1772.259509102069\n",
      "    val_log_likelihood: 1818.7319946289062\n",
      "    val_log_marginal: 1786.0523237436323\n",
      "Train Epoch: 823 [512/54000 (1%)] Loss: -1989.735474\n",
      "Train Epoch: 823 [11776/54000 (22%)] Loss: -1739.477295\n",
      "Train Epoch: 823 [23040/54000 (43%)] Loss: -1761.765259\n",
      "Train Epoch: 823 [34304/54000 (64%)] Loss: -1989.510132\n",
      "Train Epoch: 823 [45568/54000 (84%)] Loss: -1653.926880\n",
      "    epoch          : 823\n",
      "    loss           : -1764.0865974048577\n",
      "    val_loss       : -1761.5715355666355\n",
      "    val_log_likelihood: 1819.0211303710937\n",
      "    val_log_marginal: 1786.3829624438913\n",
      "Train Epoch: 824 [512/54000 (1%)] Loss: -1868.980713\n",
      "Train Epoch: 824 [11776/54000 (22%)] Loss: -1703.957153\n",
      "Train Epoch: 824 [23040/54000 (43%)] Loss: -1823.736816\n",
      "Train Epoch: 824 [34304/54000 (64%)] Loss: -1988.350220\n",
      "Train Epoch: 824 [45568/54000 (84%)] Loss: -1782.731445\n",
      "    epoch          : 824\n",
      "    loss           : -1760.2095947265625\n",
      "    val_loss       : -1765.978094372526\n",
      "    val_log_likelihood: 1818.2875732421876\n",
      "    val_log_marginal: 1785.5998779434879\n",
      "Train Epoch: 825 [512/54000 (1%)] Loss: -1989.320801\n",
      "Train Epoch: 825 [11776/54000 (22%)] Loss: -1849.463013\n",
      "Train Epoch: 825 [23040/54000 (43%)] Loss: -1553.327881\n",
      "Train Epoch: 825 [34304/54000 (64%)] Loss: -1691.094482\n",
      "Train Epoch: 825 [45568/54000 (84%)] Loss: -1686.980957\n",
      "    epoch          : 825\n",
      "    loss           : -1758.4042207321318\n",
      "    val_loss       : -1739.469492468983\n",
      "    val_log_likelihood: 1817.9659545898437\n",
      "    val_log_marginal: 1785.2076233748346\n",
      "Train Epoch: 826 [512/54000 (1%)] Loss: -1931.400391\n",
      "Train Epoch: 826 [11776/54000 (22%)] Loss: -1619.252197\n",
      "Train Epoch: 826 [23040/54000 (43%)] Loss: -1778.297363\n",
      "Train Epoch: 826 [34304/54000 (64%)] Loss: -1791.582520\n",
      "Train Epoch: 826 [45568/54000 (84%)] Loss: -1685.993530\n",
      "    epoch          : 826\n",
      "    loss           : -1759.1386960473392\n",
      "    val_loss       : -1773.5521239791065\n",
      "    val_log_likelihood: 1818.4618286132813\n",
      "    val_log_marginal: 1785.713063504921\n",
      "Train Epoch: 827 [512/54000 (1%)] Loss: -1989.966553\n",
      "Train Epoch: 827 [11776/54000 (22%)] Loss: -1502.302490\n",
      "Train Epoch: 827 [23040/54000 (43%)] Loss: -1697.169189\n",
      "Train Epoch: 827 [34304/54000 (64%)] Loss: -1928.573486\n",
      "Train Epoch: 827 [45568/54000 (84%)] Loss: -1652.805664\n",
      "    epoch          : 827\n",
      "    loss           : -1755.8993790126083\n",
      "    val_loss       : -1771.8338948157616\n",
      "    val_log_likelihood: 1817.5985595703125\n",
      "    val_log_marginal: 1784.8878822568804\n",
      "Train Epoch: 828 [512/54000 (1%)] Loss: -1998.411865\n",
      "Train Epoch: 828 [11776/54000 (22%)] Loss: -1742.132202\n",
      "Train Epoch: 828 [23040/54000 (43%)] Loss: -1710.979492\n",
      "Train Epoch: 828 [34304/54000 (64%)] Loss: -1704.468872\n",
      "Train Epoch: 828 [45568/54000 (84%)] Loss: -1790.390259\n",
      "    epoch          : 828\n",
      "    loss           : -1760.1946791847154\n",
      "    val_loss       : -1763.3752375781537\n",
      "    val_log_likelihood: 1817.0825805664062\n",
      "    val_log_marginal: 1784.3612382938531\n",
      "Train Epoch: 829 [512/54000 (1%)] Loss: -1991.956299\n",
      "Train Epoch: 829 [11776/54000 (22%)] Loss: -1729.398315\n",
      "Train Epoch: 829 [23040/54000 (43%)] Loss: -1646.796997\n",
      "Train Epoch: 829 [34304/54000 (64%)] Loss: -1991.851196\n",
      "Train Epoch: 829 [45568/54000 (84%)] Loss: -1686.907471\n",
      "    epoch          : 829\n",
      "    loss           : -1768.6802072052908\n",
      "    val_loss       : -1773.1529436551034\n",
      "    val_log_likelihood: 1818.8629150390625\n",
      "    val_log_marginal: 1786.1202229379724\n",
      "Train Epoch: 830 [512/54000 (1%)] Loss: -1993.075684\n",
      "Train Epoch: 830 [11776/54000 (22%)] Loss: -1742.219482\n",
      "Train Epoch: 830 [23040/54000 (43%)] Loss: -1584.364624\n",
      "Train Epoch: 830 [34304/54000 (64%)] Loss: -1789.675049\n",
      "Train Epoch: 830 [45568/54000 (84%)] Loss: -1588.388916\n",
      "    epoch          : 830\n",
      "    loss           : -1760.3267653059252\n",
      "    val_loss       : -1772.876859916933\n",
      "    val_log_likelihood: 1818.4562377929688\n",
      "    val_log_marginal: 1785.7550769630147\n",
      "Train Epoch: 831 [512/54000 (1%)] Loss: -1989.121094\n",
      "Train Epoch: 831 [11776/54000 (22%)] Loss: -1618.426880\n",
      "Train Epoch: 831 [23040/54000 (43%)] Loss: -1698.358154\n",
      "Train Epoch: 831 [34304/54000 (64%)] Loss: -1703.663208\n",
      "Train Epoch: 831 [45568/54000 (84%)] Loss: -1683.956909\n",
      "    epoch          : 831\n",
      "    loss           : -1758.1001460009281\n",
      "    val_loss       : -1738.2401291290298\n",
      "    val_log_likelihood: 1818.155029296875\n",
      "    val_log_marginal: 1785.4054560796744\n",
      "Train Epoch: 832 [512/54000 (1%)] Loss: -1927.930054\n",
      "Train Epoch: 832 [11776/54000 (22%)] Loss: -1612.058350\n",
      "Train Epoch: 832 [23040/54000 (43%)] Loss: -1781.921509\n",
      "Train Epoch: 832 [34304/54000 (64%)] Loss: -1778.390503\n",
      "Train Epoch: 832 [45568/54000 (84%)] Loss: -1691.105713\n",
      "    epoch          : 832\n",
      "    loss           : -1761.1705261834777\n",
      "    val_loss       : -1754.8540763497354\n",
      "    val_log_likelihood: 1818.8347290039062\n",
      "    val_log_marginal: 1786.1066761229188\n",
      "Train Epoch: 833 [512/54000 (1%)] Loss: -1987.821045\n",
      "Train Epoch: 833 [11776/54000 (22%)] Loss: -1738.908813\n",
      "Train Epoch: 833 [23040/54000 (43%)] Loss: -1788.952271\n",
      "Train Epoch: 833 [34304/54000 (64%)] Loss: -1781.502930\n",
      "Train Epoch: 833 [45568/54000 (84%)] Loss: -1561.299316\n",
      "    epoch          : 833\n",
      "    loss           : -1774.207222211479\n",
      "    val_loss       : -1762.9450133061036\n",
      "    val_log_likelihood: 1818.5590698242188\n",
      "    val_log_marginal: 1785.800923223038\n",
      "Train Epoch: 834 [512/54000 (1%)] Loss: -1992.822144\n",
      "Train Epoch: 834 [11776/54000 (22%)] Loss: -1755.761963\n",
      "Train Epoch: 834 [23040/54000 (43%)] Loss: -1553.895996\n",
      "Train Epoch: 834 [34304/54000 (64%)] Loss: -1687.334351\n",
      "Train Epoch: 834 [45568/54000 (84%)] Loss: -1781.978027\n",
      "    epoch          : 834\n",
      "    loss           : -1767.705211072865\n",
      "    val_loss       : -1775.117040444538\n",
      "    val_log_likelihood: 1818.3329956054688\n",
      "    val_log_marginal: 1785.5640687633306\n",
      "Train Epoch: 835 [512/54000 (1%)] Loss: -1856.391602\n",
      "Train Epoch: 835 [11776/54000 (22%)] Loss: -1761.462280\n",
      "Train Epoch: 835 [23040/54000 (43%)] Loss: -1756.963989\n",
      "Train Epoch: 835 [34304/54000 (64%)] Loss: -1991.512695\n",
      "Train Epoch: 835 [45568/54000 (84%)] Loss: -1783.590454\n",
      "    epoch          : 835\n",
      "    loss           : -1761.559679088026\n",
      "    val_loss       : -1747.9288200501353\n",
      "    val_log_likelihood: 1818.315380859375\n",
      "    val_log_marginal: 1785.5659091290386\n",
      "Train Epoch: 836 [512/54000 (1%)] Loss: -1929.363525\n",
      "Train Epoch: 836 [11776/54000 (22%)] Loss: -1736.669312\n",
      "Train Epoch: 836 [23040/54000 (43%)] Loss: -1564.059814\n",
      "Train Epoch: 836 [34304/54000 (64%)] Loss: -1646.185181\n",
      "Train Epoch: 836 [45568/54000 (84%)] Loss: -1784.321045\n",
      "    epoch          : 836\n",
      "    loss           : -1754.1495675568533\n",
      "    val_loss       : -1711.7092621993274\n",
      "    val_log_likelihood: 1818.5630615234375\n",
      "    val_log_marginal: 1785.871506774798\n",
      "Train Epoch: 837 [512/54000 (1%)] Loss: -1618.786255\n",
      "Train Epoch: 837 [11776/54000 (22%)] Loss: -1750.796143\n",
      "Train Epoch: 837 [23040/54000 (43%)] Loss: -1782.894897\n",
      "Train Epoch: 837 [34304/54000 (64%)] Loss: -1786.933716\n",
      "Train Epoch: 837 [45568/54000 (84%)] Loss: -1685.135254\n",
      "    epoch          : 837\n",
      "    loss           : -1760.2700171140161\n",
      "    val_loss       : -1745.3186141589656\n",
      "    val_log_likelihood: 1818.6551513671875\n",
      "    val_log_marginal: 1785.9276672256942\n",
      "Train Epoch: 838 [512/54000 (1%)] Loss: -1990.795776\n",
      "Train Epoch: 838 [11776/54000 (22%)] Loss: -1745.515625\n",
      "Train Epoch: 838 [23040/54000 (43%)] Loss: -1778.424927\n",
      "Train Epoch: 838 [34304/54000 (64%)] Loss: -1753.618286\n",
      "Train Epoch: 838 [45568/54000 (84%)] Loss: -1782.127930\n",
      "    epoch          : 838\n",
      "    loss           : -1751.4471230081992\n",
      "    val_loss       : -1765.154324803129\n",
      "    val_log_likelihood: 1818.9625244140625\n",
      "    val_log_marginal: 1786.1871905934065\n",
      "Train Epoch: 839 [512/54000 (1%)] Loss: -1991.854370\n",
      "Train Epoch: 839 [11776/54000 (22%)] Loss: -1743.140869\n",
      "Train Epoch: 839 [23040/54000 (43%)] Loss: -1756.095703\n",
      "Train Epoch: 839 [34304/54000 (64%)] Loss: -1785.185181\n",
      "Train Epoch: 839 [45568/54000 (84%)] Loss: -1784.105225\n",
      "    epoch          : 839\n",
      "    loss           : -1762.4755291325032\n",
      "    val_loss       : -1773.2305164314807\n",
      "    val_log_likelihood: 1818.57333984375\n",
      "    val_log_marginal: 1785.844942643866\n",
      "Train Epoch: 840 [512/54000 (1%)] Loss: -1992.599365\n",
      "Train Epoch: 840 [11776/54000 (22%)] Loss: -1627.455078\n",
      "Train Epoch: 840 [23040/54000 (43%)] Loss: -1696.632080\n",
      "Train Epoch: 840 [34304/54000 (64%)] Loss: -1784.113525\n",
      "Train Epoch: 840 [45568/54000 (84%)] Loss: -1657.219727\n",
      "    epoch          : 840\n",
      "    loss           : -1755.362550036742\n",
      "    val_loss       : -1754.3669484945015\n",
      "    val_log_likelihood: 1818.1070068359375\n",
      "    val_log_marginal: 1785.3555229891092\n",
      "Train Epoch: 841 [512/54000 (1%)] Loss: -1989.005371\n",
      "Train Epoch: 841 [11776/54000 (22%)] Loss: -1685.513184\n",
      "Train Epoch: 841 [23040/54000 (43%)] Loss: -1565.323730\n",
      "Train Epoch: 841 [34304/54000 (64%)] Loss: -1793.783936\n",
      "Train Epoch: 841 [45568/54000 (84%)] Loss: -1755.758423\n",
      "    epoch          : 841\n",
      "    loss           : -1760.8783551206684\n",
      "    val_loss       : -1752.761172906868\n",
      "    val_log_likelihood: 1818.341796875\n",
      "    val_log_marginal: 1785.60566445131\n",
      "Train Epoch: 842 [512/54000 (1%)] Loss: -1990.358398\n",
      "Train Epoch: 842 [11776/54000 (22%)] Loss: -1709.342896\n",
      "Train Epoch: 842 [23040/54000 (43%)] Loss: -1818.718750\n",
      "Train Epoch: 842 [34304/54000 (64%)] Loss: -1699.269897\n",
      "Train Epoch: 842 [45568/54000 (84%)] Loss: -1685.529785\n",
      "    epoch          : 842\n",
      "    loss           : -1769.8940284653465\n",
      "    val_loss       : -1785.3401461016388\n",
      "    val_log_likelihood: 1818.5988647460938\n",
      "    val_log_marginal: 1785.8388037953548\n",
      "Train Epoch: 843 [512/54000 (1%)] Loss: -1928.715820\n",
      "Train Epoch: 843 [11776/54000 (22%)] Loss: -1705.494629\n",
      "Train Epoch: 843 [23040/54000 (43%)] Loss: -1702.175659\n",
      "Train Epoch: 843 [34304/54000 (64%)] Loss: -1758.531006\n",
      "Train Epoch: 843 [45568/54000 (84%)] Loss: -1689.082764\n",
      "    epoch          : 843\n",
      "    loss           : -1764.004364315826\n",
      "    val_loss       : -1738.857292475924\n",
      "    val_log_likelihood: 1818.7221069335938\n",
      "    val_log_marginal: 1785.9741081614047\n",
      "Train Epoch: 844 [512/54000 (1%)] Loss: -1991.504517\n",
      "Train Epoch: 844 [11776/54000 (22%)] Loss: -1731.257812\n",
      "Train Epoch: 844 [23040/54000 (43%)] Loss: -1764.555420\n",
      "Train Epoch: 844 [34304/54000 (64%)] Loss: -1788.609741\n",
      "Train Epoch: 844 [45568/54000 (84%)] Loss: -1783.976807\n",
      "    epoch          : 844\n",
      "    loss           : -1765.1008796314202\n",
      "    val_loss       : -1747.2167908899487\n",
      "    val_log_likelihood: 1818.818505859375\n",
      "    val_log_marginal: 1786.1256883341819\n",
      "Train Epoch: 845 [512/54000 (1%)] Loss: -1852.317017\n",
      "Train Epoch: 845 [11776/54000 (22%)] Loss: -1613.093140\n",
      "Train Epoch: 845 [23040/54000 (43%)] Loss: -1753.701782\n",
      "Train Epoch: 845 [34304/54000 (64%)] Loss: -1758.695557\n",
      "Train Epoch: 845 [45568/54000 (84%)] Loss: -1787.178467\n",
      "    epoch          : 845\n",
      "    loss           : -1754.7060716081373\n",
      "    val_loss       : -1725.4221923490986\n",
      "    val_log_likelihood: 1818.34404296875\n",
      "    val_log_marginal: 1785.6548654232174\n",
      "Train Epoch: 846 [512/54000 (1%)] Loss: -1994.454590\n",
      "Train Epoch: 846 [11776/54000 (22%)] Loss: -1705.604004\n",
      "Train Epoch: 846 [23040/54000 (43%)] Loss: -1752.416504\n",
      "Train Epoch: 846 [34304/54000 (64%)] Loss: -1819.131592\n",
      "Train Epoch: 846 [45568/54000 (84%)] Loss: -1690.307129\n",
      "    epoch          : 846\n",
      "    loss           : -1756.0595280109067\n",
      "    val_loss       : -1736.1780626233667\n",
      "    val_log_likelihood: 1818.0740600585937\n",
      "    val_log_marginal: 1785.2304661456496\n",
      "Train Epoch: 847 [512/54000 (1%)] Loss: -1856.774170\n",
      "Train Epoch: 847 [11776/54000 (22%)] Loss: -1740.507935\n",
      "Train Epoch: 847 [23040/54000 (43%)] Loss: -1649.337402\n",
      "Train Epoch: 847 [34304/54000 (64%)] Loss: -1984.569580\n",
      "Train Epoch: 847 [45568/54000 (84%)] Loss: -1782.148315\n",
      "    epoch          : 847\n",
      "    loss           : -1753.5715102394029\n",
      "    val_loss       : -1773.5171898031608\n",
      "    val_log_likelihood: 1819.0126098632813\n",
      "    val_log_marginal: 1786.3123092565686\n",
      "Train Epoch: 848 [512/54000 (1%)] Loss: -1990.698730\n",
      "Train Epoch: 848 [11776/54000 (22%)] Loss: -1851.026245\n",
      "Train Epoch: 848 [23040/54000 (43%)] Loss: -1565.108887\n",
      "Train Epoch: 848 [34304/54000 (64%)] Loss: -1782.655151\n",
      "Train Epoch: 848 [45568/54000 (84%)] Loss: -1785.662720\n",
      "    epoch          : 848\n",
      "    loss           : -1760.0101004118967\n",
      "    val_loss       : -1718.3888863714412\n",
      "    val_log_likelihood: 1818.7029418945312\n",
      "    val_log_marginal: 1785.9408306840814\n",
      "Train Epoch: 849 [512/54000 (1%)] Loss: -1989.518799\n",
      "Train Epoch: 849 [11776/54000 (22%)] Loss: -1723.890991\n",
      "Train Epoch: 849 [23040/54000 (43%)] Loss: -1709.279053\n",
      "Train Epoch: 849 [34304/54000 (64%)] Loss: -1388.823975\n",
      "Train Epoch: 849 [45568/54000 (84%)] Loss: -1787.336548\n",
      "    epoch          : 849\n",
      "    loss           : -1754.3183714611696\n",
      "    val_loss       : -1778.5328802153467\n",
      "    val_log_likelihood: 1817.8572631835937\n",
      "    val_log_marginal: 1785.088705470966\n",
      "Train Epoch: 850 [512/54000 (1%)] Loss: -1989.302124\n",
      "Train Epoch: 850 [11776/54000 (22%)] Loss: -1743.344971\n",
      "Train Epoch: 850 [23040/54000 (43%)] Loss: -1694.593262\n",
      "Train Epoch: 850 [34304/54000 (64%)] Loss: -1787.184814\n",
      "Train Epoch: 850 [45568/54000 (84%)] Loss: -1787.598633\n",
      "    epoch          : 850\n",
      "    loss           : -1772.4225361134747\n",
      "    val_loss       : -1748.607372346148\n",
      "    val_log_likelihood: 1817.87724609375\n",
      "    val_log_marginal: 1785.0956324857223\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch850.pth ...\n",
      "Train Epoch: 851 [512/54000 (1%)] Loss: -1990.701050\n",
      "Train Epoch: 851 [11776/54000 (22%)] Loss: -1565.452148\n",
      "Train Epoch: 851 [23040/54000 (43%)] Loss: -1708.330322\n",
      "Train Epoch: 851 [34304/54000 (64%)] Loss: -1821.839233\n",
      "Train Epoch: 851 [45568/54000 (84%)] Loss: -1694.233521\n",
      "    epoch          : 851\n",
      "    loss           : -1759.8647642230044\n",
      "    val_loss       : -1764.4172199893742\n",
      "    val_log_likelihood: 1818.1223510742188\n",
      "    val_log_marginal: 1785.3214099440725\n",
      "Train Epoch: 852 [512/54000 (1%)] Loss: -1986.881348\n",
      "Train Epoch: 852 [11776/54000 (22%)] Loss: -1701.564209\n",
      "Train Epoch: 852 [23040/54000 (43%)] Loss: -1741.674194\n",
      "Train Epoch: 852 [34304/54000 (64%)] Loss: -1992.421631\n",
      "Train Epoch: 852 [45568/54000 (84%)] Loss: -1684.120483\n",
      "    epoch          : 852\n",
      "    loss           : -1757.5292811629795\n",
      "    val_loss       : -1763.9453481161968\n",
      "    val_log_likelihood: 1818.3078369140626\n",
      "    val_log_marginal: 1785.6048323299735\n",
      "Train Epoch: 853 [512/54000 (1%)] Loss: -1994.162354\n",
      "Train Epoch: 853 [11776/54000 (22%)] Loss: -1850.273560\n",
      "Train Epoch: 853 [23040/54000 (43%)] Loss: -1704.409180\n",
      "Train Epoch: 853 [34304/54000 (64%)] Loss: -1992.683960\n",
      "Train Epoch: 853 [45568/54000 (84%)] Loss: -1663.491211\n",
      "    epoch          : 853\n",
      "    loss           : -1761.7742907835705\n",
      "    val_loss       : -1784.6216278921813\n",
      "    val_log_likelihood: 1817.8411254882812\n",
      "    val_log_marginal: 1785.1198988993533\n",
      "Train Epoch: 854 [512/54000 (1%)] Loss: -1992.221436\n",
      "Train Epoch: 854 [11776/54000 (22%)] Loss: -1854.500244\n",
      "Train Epoch: 854 [23040/54000 (43%)] Loss: -1822.525269\n",
      "Train Epoch: 854 [34304/54000 (64%)] Loss: -1988.896484\n",
      "Train Epoch: 854 [45568/54000 (84%)] Loss: -1694.796143\n",
      "    epoch          : 854\n",
      "    loss           : -1765.0413165706218\n",
      "    val_loss       : -1762.7050404610113\n",
      "    val_log_likelihood: 1817.773046875\n",
      "    val_log_marginal: 1785.0656167946304\n",
      "Train Epoch: 855 [512/54000 (1%)] Loss: -1857.205811\n",
      "Train Epoch: 855 [11776/54000 (22%)] Loss: -1628.290527\n",
      "Train Epoch: 855 [23040/54000 (43%)] Loss: -1688.595947\n",
      "Train Epoch: 855 [34304/54000 (64%)] Loss: -1690.836548\n",
      "Train Epoch: 855 [45568/54000 (84%)] Loss: -1689.604248\n",
      "    epoch          : 855\n",
      "    loss           : -1763.7511602722773\n",
      "    val_loss       : -1751.0380898592994\n",
      "    val_log_likelihood: 1817.5818481445312\n",
      "    val_log_marginal: 1784.8376381713897\n",
      "Train Epoch: 856 [512/54000 (1%)] Loss: -1988.083252\n",
      "Train Epoch: 856 [11776/54000 (22%)] Loss: -1848.512695\n",
      "Train Epoch: 856 [23040/54000 (43%)] Loss: -1686.837036\n",
      "Train Epoch: 856 [34304/54000 (64%)] Loss: -1589.009766\n",
      "Train Epoch: 856 [45568/54000 (84%)] Loss: -1781.583740\n",
      "    epoch          : 856\n",
      "    loss           : -1765.4589698715965\n",
      "    val_loss       : -1760.3224218400196\n",
      "    val_log_likelihood: 1817.59443359375\n",
      "    val_log_marginal: 1784.8348958960967\n",
      "Train Epoch: 857 [512/54000 (1%)] Loss: -1932.907471\n",
      "Train Epoch: 857 [11776/54000 (22%)] Loss: -1559.619385\n",
      "Train Epoch: 857 [23040/54000 (43%)] Loss: -1753.595215\n",
      "Train Epoch: 857 [34304/54000 (64%)] Loss: -1781.705688\n",
      "Train Epoch: 857 [45568/54000 (84%)] Loss: -1684.243286\n",
      "    epoch          : 857\n",
      "    loss           : -1755.7834110071162\n",
      "    val_loss       : -1750.5962507238612\n",
      "    val_log_likelihood: 1818.1813354492188\n",
      "    val_log_marginal: 1785.393897440657\n",
      "Train Epoch: 858 [512/54000 (1%)] Loss: -1935.254395\n",
      "Train Epoch: 858 [11776/54000 (22%)] Loss: -1561.896973\n",
      "Train Epoch: 858 [23040/54000 (43%)] Loss: -1691.799316\n",
      "Train Epoch: 858 [34304/54000 (64%)] Loss: -1593.117554\n",
      "Train Epoch: 858 [45568/54000 (84%)] Loss: -1788.152954\n",
      "    epoch          : 858\n",
      "    loss           : -1761.8102084622524\n",
      "    val_loss       : -1729.27113001924\n",
      "    val_log_likelihood: 1818.3169189453124\n",
      "    val_log_marginal: 1785.6031193295428\n",
      "Train Epoch: 859 [512/54000 (1%)] Loss: -1613.373291\n",
      "Train Epoch: 859 [11776/54000 (22%)] Loss: -1735.278687\n",
      "Train Epoch: 859 [23040/54000 (43%)] Loss: -1819.272949\n",
      "Train Epoch: 859 [34304/54000 (64%)] Loss: -1785.967529\n",
      "Train Epoch: 859 [45568/54000 (84%)] Loss: -1781.225098\n",
      "    epoch          : 859\n",
      "    loss           : -1762.9776272915378\n",
      "    val_loss       : -1784.2006958121433\n",
      "    val_log_likelihood: 1817.3958251953125\n",
      "    val_log_marginal: 1784.6153873372828\n",
      "Train Epoch: 860 [512/54000 (1%)] Loss: -1853.007080\n",
      "Train Epoch: 860 [11776/54000 (22%)] Loss: -1733.855225\n",
      "Train Epoch: 860 [23040/54000 (43%)] Loss: -1700.576782\n",
      "Train Epoch: 860 [34304/54000 (64%)] Loss: -1992.931641\n",
      "Train Epoch: 860 [45568/54000 (84%)] Loss: -1690.686035\n",
      "    epoch          : 860\n",
      "    loss           : -1772.8130462948639\n",
      "    val_loss       : -1739.4036980150267\n",
      "    val_log_likelihood: 1818.6858154296874\n",
      "    val_log_marginal: 1786.025429348275\n",
      "Train Epoch: 861 [512/54000 (1%)] Loss: -1997.209595\n",
      "Train Epoch: 861 [11776/54000 (22%)] Loss: -1617.447876\n",
      "Train Epoch: 861 [23040/54000 (43%)] Loss: -1779.854126\n",
      "Train Epoch: 861 [34304/54000 (64%)] Loss: -1778.915405\n",
      "Train Epoch: 861 [45568/54000 (84%)] Loss: -1688.946289\n",
      "    epoch          : 861\n",
      "    loss           : -1766.6593223043008\n",
      "    val_loss       : -1751.6345765758306\n",
      "    val_log_likelihood: 1818.9239135742187\n",
      "    val_log_marginal: 1786.2228897783905\n",
      "Train Epoch: 862 [512/54000 (1%)] Loss: -1994.229248\n",
      "Train Epoch: 862 [11776/54000 (22%)] Loss: -1733.778809\n",
      "Train Epoch: 862 [23040/54000 (43%)] Loss: -1816.770142\n",
      "Train Epoch: 862 [34304/54000 (64%)] Loss: -1934.467407\n",
      "Train Epoch: 862 [45568/54000 (84%)] Loss: -1788.961182\n",
      "    epoch          : 862\n",
      "    loss           : -1753.9622246770575\n",
      "    val_loss       : -1767.8865545883775\n",
      "    val_log_likelihood: 1818.3230102539062\n",
      "    val_log_marginal: 1785.5859192913947\n",
      "Train Epoch: 863 [512/54000 (1%)] Loss: -1868.186035\n",
      "Train Epoch: 863 [11776/54000 (22%)] Loss: -1739.675537\n",
      "Train Epoch: 863 [23040/54000 (43%)] Loss: -1593.833008\n",
      "Train Epoch: 863 [34304/54000 (64%)] Loss: -1564.604736\n",
      "Train Epoch: 863 [45568/54000 (84%)] Loss: -1780.105469\n",
      "    epoch          : 863\n",
      "    loss           : -1759.1092831451115\n",
      "    val_loss       : -1729.7185009939597\n",
      "    val_log_likelihood: 1818.6387329101562\n",
      "    val_log_marginal: 1785.8853632759303\n",
      "Train Epoch: 864 [512/54000 (1%)] Loss: -1937.687622\n",
      "Train Epoch: 864 [11776/54000 (22%)] Loss: -1848.632568\n",
      "Train Epoch: 864 [23040/54000 (43%)] Loss: -1691.630371\n",
      "Train Epoch: 864 [34304/54000 (64%)] Loss: -1782.096191\n",
      "Train Epoch: 864 [45568/54000 (84%)] Loss: -1775.908447\n",
      "    epoch          : 864\n",
      "    loss           : -1768.4822816754331\n",
      "    val_loss       : -1773.5118348052724\n",
      "    val_log_likelihood: 1818.6905151367187\n",
      "    val_log_marginal: 1785.938518094644\n",
      "Train Epoch: 865 [512/54000 (1%)] Loss: -1934.498535\n",
      "Train Epoch: 865 [11776/54000 (22%)] Loss: -1735.723267\n",
      "Train Epoch: 865 [23040/54000 (43%)] Loss: -1827.151123\n",
      "Train Epoch: 865 [34304/54000 (64%)] Loss: -1750.879028\n",
      "Train Epoch: 865 [45568/54000 (84%)] Loss: -1772.921753\n",
      "    epoch          : 865\n",
      "    loss           : -1766.8589870339572\n",
      "    val_loss       : -1738.2740643113852\n",
      "    val_log_likelihood: 1818.0076904296875\n",
      "    val_log_marginal: 1785.2437288749468\n",
      "Train Epoch: 866 [512/54000 (1%)] Loss: -1993.035767\n",
      "Train Epoch: 866 [11776/54000 (22%)] Loss: -1852.081055\n",
      "Train Epoch: 866 [23040/54000 (43%)] Loss: -1711.390869\n",
      "Train Epoch: 866 [34304/54000 (64%)] Loss: -1714.112061\n",
      "Train Epoch: 866 [45568/54000 (84%)] Loss: -1782.781128\n",
      "    epoch          : 866\n",
      "    loss           : -1762.2895036451887\n",
      "    val_loss       : -1730.7984185777605\n",
      "    val_log_likelihood: 1818.546728515625\n",
      "    val_log_marginal: 1785.7984639797378\n",
      "Train Epoch: 867 [512/54000 (1%)] Loss: -1991.010620\n",
      "Train Epoch: 867 [11776/54000 (22%)] Loss: -1853.841187\n",
      "Train Epoch: 867 [23040/54000 (43%)] Loss: -1783.132080\n",
      "Train Epoch: 867 [34304/54000 (64%)] Loss: -1776.295410\n",
      "Train Epoch: 867 [45568/54000 (84%)] Loss: -1778.793091\n",
      "    epoch          : 867\n",
      "    loss           : -1766.875622437732\n",
      "    val_loss       : -1719.8504691926762\n",
      "    val_log_likelihood: 1819.5365356445313\n",
      "    val_log_marginal: 1786.7722933795303\n",
      "Train Epoch: 868 [512/54000 (1%)] Loss: -1991.420532\n",
      "Train Epoch: 868 [11776/54000 (22%)] Loss: -1739.767334\n",
      "Train Epoch: 868 [23040/54000 (43%)] Loss: -1781.236572\n",
      "Train Epoch: 868 [34304/54000 (64%)] Loss: -1783.750000\n",
      "Train Epoch: 868 [45568/54000 (84%)] Loss: -1660.772949\n",
      "    epoch          : 868\n",
      "    loss           : -1761.4941007406405\n",
      "    val_loss       : -1764.897870489955\n",
      "    val_log_likelihood: 1818.187744140625\n",
      "    val_log_marginal: 1785.4209065731616\n",
      "Train Epoch: 869 [512/54000 (1%)] Loss: -1992.485840\n",
      "Train Epoch: 869 [11776/54000 (22%)] Loss: -1624.109009\n",
      "Train Epoch: 869 [23040/54000 (43%)] Loss: -1785.259399\n",
      "Train Epoch: 869 [34304/54000 (64%)] Loss: -1825.588989\n",
      "Train Epoch: 869 [45568/54000 (84%)] Loss: -1677.524414\n",
      "    epoch          : 869\n",
      "    loss           : -1766.8237655186417\n",
      "    val_loss       : -1745.7258991722017\n",
      "    val_log_likelihood: 1819.2733154296875\n",
      "    val_log_marginal: 1786.6257314231584\n",
      "Train Epoch: 870 [512/54000 (1%)] Loss: -1990.893188\n",
      "Train Epoch: 870 [11776/54000 (22%)] Loss: -1784.934937\n",
      "Train Epoch: 870 [23040/54000 (43%)] Loss: -1755.065186\n",
      "Train Epoch: 870 [34304/54000 (64%)] Loss: -1719.614502\n",
      "Train Epoch: 870 [45568/54000 (84%)] Loss: -1689.029907\n",
      "    epoch          : 870\n",
      "    loss           : -1768.707276599242\n",
      "    val_loss       : -1785.1166727604345\n",
      "    val_log_likelihood: 1818.38505859375\n",
      "    val_log_marginal: 1785.6807391475886\n",
      "Train Epoch: 871 [512/54000 (1%)] Loss: -1990.221558\n",
      "Train Epoch: 871 [11776/54000 (22%)] Loss: -1703.861938\n",
      "Train Epoch: 871 [23040/54000 (43%)] Loss: -1739.318726\n",
      "Train Epoch: 871 [34304/54000 (64%)] Loss: -1820.350586\n",
      "Train Epoch: 871 [45568/54000 (84%)] Loss: -1654.934082\n",
      "    epoch          : 871\n",
      "    loss           : -1761.9324201829363\n",
      "    val_loss       : -1753.1622495353222\n",
      "    val_log_likelihood: 1817.3339721679688\n",
      "    val_log_marginal: 1784.5616896454292\n",
      "Train Epoch: 872 [512/54000 (1%)] Loss: -1851.032837\n",
      "Train Epoch: 872 [11776/54000 (22%)] Loss: -1731.312256\n",
      "Train Epoch: 872 [23040/54000 (43%)] Loss: -1785.033447\n",
      "Train Epoch: 872 [34304/54000 (64%)] Loss: -1776.395264\n",
      "Train Epoch: 872 [45568/54000 (84%)] Loss: -1686.259033\n",
      "    epoch          : 872\n",
      "    loss           : -1761.5856764387377\n",
      "    val_loss       : -1745.6260186618194\n",
      "    val_log_likelihood: 1818.3491088867188\n",
      "    val_log_marginal: 1785.58713969776\n",
      "Train Epoch: 873 [512/54000 (1%)] Loss: -1990.422363\n",
      "Train Epoch: 873 [11776/54000 (22%)] Loss: -1852.109131\n",
      "Train Epoch: 873 [23040/54000 (43%)] Loss: -1660.377441\n",
      "Train Epoch: 873 [34304/54000 (64%)] Loss: -1816.479736\n",
      "Train Epoch: 873 [45568/54000 (84%)] Loss: -1822.927490\n",
      "    epoch          : 873\n",
      "    loss           : -1754.6401173808788\n",
      "    val_loss       : -1785.2797010704876\n",
      "    val_log_likelihood: 1818.621533203125\n",
      "    val_log_marginal: 1785.8977050553572\n",
      "Train Epoch: 874 [512/54000 (1%)] Loss: -1851.903687\n",
      "Train Epoch: 874 [11776/54000 (22%)] Loss: -1738.759033\n",
      "Train Epoch: 874 [23040/54000 (43%)] Loss: -1826.992920\n",
      "Train Epoch: 874 [34304/54000 (64%)] Loss: -1654.885864\n",
      "Train Epoch: 874 [45568/54000 (84%)] Loss: -1782.498047\n",
      "    epoch          : 874\n",
      "    loss           : -1764.372154915687\n",
      "    val_loss       : -1760.3303336424754\n",
      "    val_log_likelihood: 1818.305712890625\n",
      "    val_log_marginal: 1785.5460283849388\n",
      "Train Epoch: 875 [512/54000 (1%)] Loss: -1936.144653\n",
      "Train Epoch: 875 [11776/54000 (22%)] Loss: -1700.453125\n",
      "Train Epoch: 875 [23040/54000 (43%)] Loss: -1565.561035\n",
      "Train Epoch: 875 [34304/54000 (64%)] Loss: -1554.062988\n",
      "Train Epoch: 875 [45568/54000 (84%)] Loss: -1657.328247\n",
      "    epoch          : 875\n",
      "    loss           : -1754.0130131787594\n",
      "    val_loss       : -1727.0124183500186\n",
      "    val_log_likelihood: 1818.8328002929688\n",
      "    val_log_marginal: 1786.123441211514\n",
      "Train Epoch: 876 [512/54000 (1%)] Loss: -1708.667603\n",
      "Train Epoch: 876 [11776/54000 (22%)] Loss: -1737.509033\n",
      "Train Epoch: 876 [23040/54000 (43%)] Loss: -1710.935181\n",
      "Train Epoch: 876 [34304/54000 (64%)] Loss: -1583.900757\n",
      "Train Epoch: 876 [45568/54000 (84%)] Loss: -1784.053833\n",
      "    epoch          : 876\n",
      "    loss           : -1754.9625038675742\n",
      "    val_loss       : -1768.9424176730215\n",
      "    val_log_likelihood: 1817.9204711914062\n",
      "    val_log_marginal: 1785.1923383721557\n",
      "Train Epoch: 877 [512/54000 (1%)] Loss: -1996.861816\n",
      "Train Epoch: 877 [11776/54000 (22%)] Loss: -1703.004395\n",
      "Train Epoch: 877 [23040/54000 (43%)] Loss: -1823.407227\n",
      "Train Epoch: 877 [34304/54000 (64%)] Loss: -1666.303833\n",
      "Train Epoch: 877 [45568/54000 (84%)] Loss: -1789.583374\n",
      "    epoch          : 877\n",
      "    loss           : -1765.970376798422\n",
      "    val_loss       : -1774.3456817068159\n",
      "    val_log_likelihood: 1819.35224609375\n",
      "    val_log_marginal: 1786.6397167731266\n",
      "Train Epoch: 878 [512/54000 (1%)] Loss: -1734.126953\n",
      "Train Epoch: 878 [11776/54000 (22%)] Loss: -1736.856445\n",
      "Train Epoch: 878 [23040/54000 (43%)] Loss: -1705.519531\n",
      "Train Epoch: 878 [34304/54000 (64%)] Loss: -1993.206299\n",
      "Train Epoch: 878 [45568/54000 (84%)] Loss: -1659.508667\n",
      "    epoch          : 878\n",
      "    loss           : -1761.1875882290378\n",
      "    val_loss       : -1750.1557429073378\n",
      "    val_log_likelihood: 1818.9810791015625\n",
      "    val_log_marginal: 1786.2380997490138\n",
      "Train Epoch: 879 [512/54000 (1%)] Loss: -1991.043579\n",
      "Train Epoch: 879 [11776/54000 (22%)] Loss: -1735.411377\n",
      "Train Epoch: 879 [23040/54000 (43%)] Loss: -1425.937866\n",
      "Train Epoch: 879 [34304/54000 (64%)] Loss: -1758.604736\n",
      "Train Epoch: 879 [45568/54000 (84%)] Loss: -1786.853271\n",
      "    epoch          : 879\n",
      "    loss           : -1765.500705832302\n",
      "    val_loss       : -1773.4442419508473\n",
      "    val_log_likelihood: 1818.5008178710937\n",
      "    val_log_marginal: 1785.7999485921114\n",
      "Train Epoch: 880 [512/54000 (1%)] Loss: -1994.014526\n",
      "Train Epoch: 880 [11776/54000 (22%)] Loss: -1625.471558\n",
      "Train Epoch: 880 [23040/54000 (43%)] Loss: -1780.968506\n",
      "Train Epoch: 880 [34304/54000 (64%)] Loss: -1701.722412\n",
      "Train Epoch: 880 [45568/54000 (84%)] Loss: -1781.685059\n",
      "    epoch          : 880\n",
      "    loss           : -1754.8360088084003\n",
      "    val_loss       : -1751.8261985020713\n",
      "    val_log_likelihood: 1818.35283203125\n",
      "    val_log_marginal: 1785.6705138275968\n",
      "Train Epoch: 881 [512/54000 (1%)] Loss: -1994.211426\n",
      "Train Epoch: 881 [11776/54000 (22%)] Loss: -1707.112305\n",
      "Train Epoch: 881 [23040/54000 (43%)] Loss: -1819.733765\n",
      "Train Epoch: 881 [34304/54000 (64%)] Loss: -1757.919556\n",
      "Train Epoch: 881 [45568/54000 (84%)] Loss: -1686.170044\n",
      "    epoch          : 881\n",
      "    loss           : -1763.6323314704518\n",
      "    val_loss       : -1763.3617576878519\n",
      "    val_log_likelihood: 1818.5313110351562\n",
      "    val_log_marginal: 1785.7179132070391\n",
      "Train Epoch: 882 [512/54000 (1%)] Loss: -1991.276978\n",
      "Train Epoch: 882 [11776/54000 (22%)] Loss: -1703.217285\n",
      "Train Epoch: 882 [23040/54000 (43%)] Loss: -1702.975952\n",
      "Train Epoch: 882 [34304/54000 (64%)] Loss: -1822.861938\n",
      "Train Epoch: 882 [45568/54000 (84%)] Loss: -1697.981445\n",
      "    epoch          : 882\n",
      "    loss           : -1761.6639766881963\n",
      "    val_loss       : -1772.2955389130861\n",
      "    val_log_likelihood: 1818.8707885742188\n",
      "    val_log_marginal: 1786.135237095497\n",
      "Train Epoch: 883 [512/54000 (1%)] Loss: -1993.647339\n",
      "Train Epoch: 883 [11776/54000 (22%)] Loss: -1710.906494\n",
      "Train Epoch: 883 [23040/54000 (43%)] Loss: -1689.143188\n",
      "Train Epoch: 883 [34304/54000 (64%)] Loss: -1823.636475\n",
      "Train Epoch: 883 [45568/54000 (84%)] Loss: -1659.684814\n",
      "    epoch          : 883\n",
      "    loss           : -1769.326112652769\n",
      "    val_loss       : -1720.5594962732866\n",
      "    val_log_likelihood: 1818.89248046875\n",
      "    val_log_marginal: 1786.2140730729916\n",
      "Train Epoch: 884 [512/54000 (1%)] Loss: -1854.219116\n",
      "Train Epoch: 884 [11776/54000 (22%)] Loss: -1630.017334\n",
      "Train Epoch: 884 [23040/54000 (43%)] Loss: -1556.036011\n",
      "Train Epoch: 884 [34304/54000 (64%)] Loss: -1754.221680\n",
      "Train Epoch: 884 [45568/54000 (84%)] Loss: -1789.093018\n",
      "    epoch          : 884\n",
      "    loss           : -1763.450931360226\n",
      "    val_loss       : -1761.9000161826611\n",
      "    val_log_likelihood: 1818.6383422851563\n",
      "    val_log_marginal: 1785.8973866585643\n",
      "Train Epoch: 885 [512/54000 (1%)] Loss: -1931.764160\n",
      "Train Epoch: 885 [11776/54000 (22%)] Loss: -1738.936035\n",
      "Train Epoch: 885 [23040/54000 (43%)] Loss: -1697.638550\n",
      "Train Epoch: 885 [34304/54000 (64%)] Loss: -1782.037354\n",
      "Train Epoch: 885 [45568/54000 (84%)] Loss: -1691.832397\n",
      "    epoch          : 885\n",
      "    loss           : -1763.5655529664295\n",
      "    val_loss       : -1752.1633934823797\n",
      "    val_log_likelihood: 1819.3572265625\n",
      "    val_log_marginal: 1786.6839738104493\n",
      "Train Epoch: 886 [512/54000 (1%)] Loss: -1990.362183\n",
      "Train Epoch: 886 [11776/54000 (22%)] Loss: -1572.413086\n",
      "Train Epoch: 886 [23040/54000 (43%)] Loss: -1757.441162\n",
      "Train Epoch: 886 [34304/54000 (64%)] Loss: -1823.441895\n",
      "Train Epoch: 886 [45568/54000 (84%)] Loss: -1786.302124\n",
      "    epoch          : 886\n",
      "    loss           : -1760.146918268487\n",
      "    val_loss       : -1738.4475934056566\n",
      "    val_log_likelihood: 1818.4200805664063\n",
      "    val_log_marginal: 1785.7840436065903\n",
      "Train Epoch: 887 [512/54000 (1%)] Loss: -1991.969482\n",
      "Train Epoch: 887 [11776/54000 (22%)] Loss: -1748.153442\n",
      "Train Epoch: 887 [23040/54000 (43%)] Loss: -1702.335571\n",
      "Train Epoch: 887 [34304/54000 (64%)] Loss: -1993.710449\n",
      "Train Epoch: 887 [45568/54000 (84%)] Loss: -1688.500122\n",
      "    epoch          : 887\n",
      "    loss           : -1757.3352908899287\n",
      "    val_loss       : -1762.0086767736823\n",
      "    val_log_likelihood: 1818.9774169921875\n",
      "    val_log_marginal: 1786.2745874855918\n",
      "Train Epoch: 888 [512/54000 (1%)] Loss: -1995.454468\n",
      "Train Epoch: 888 [11776/54000 (22%)] Loss: -1742.178467\n",
      "Train Epoch: 888 [23040/54000 (43%)] Loss: -1785.823242\n",
      "Train Epoch: 888 [34304/54000 (64%)] Loss: -1699.969360\n",
      "Train Epoch: 888 [45568/54000 (84%)] Loss: -1716.229980\n",
      "    epoch          : 888\n",
      "    loss           : -1759.6427847984994\n",
      "    val_loss       : -1742.5459271566942\n",
      "    val_log_likelihood: 1819.1698486328125\n",
      "    val_log_marginal: 1786.4463550943321\n",
      "Train Epoch: 889 [512/54000 (1%)] Loss: -1860.677612\n",
      "Train Epoch: 889 [11776/54000 (22%)] Loss: -1701.687012\n",
      "Train Epoch: 889 [23040/54000 (43%)] Loss: -1825.007812\n",
      "Train Epoch: 889 [34304/54000 (64%)] Loss: -1665.737305\n",
      "Train Epoch: 889 [45568/54000 (84%)] Loss: -1784.198730\n",
      "    epoch          : 889\n",
      "    loss           : -1765.966227616414\n",
      "    val_loss       : -1720.3842388536782\n",
      "    val_log_likelihood: 1819.178125\n",
      "    val_log_marginal: 1786.4671376904394\n",
      "Train Epoch: 890 [512/54000 (1%)] Loss: -1993.684082\n",
      "Train Epoch: 890 [11776/54000 (22%)] Loss: -1743.425659\n",
      "Train Epoch: 890 [23040/54000 (43%)] Loss: -1821.000122\n",
      "Train Epoch: 890 [34304/54000 (64%)] Loss: -1822.267334\n",
      "Train Epoch: 890 [45568/54000 (84%)] Loss: -1784.876221\n",
      "    epoch          : 890\n",
      "    loss           : -1764.1188058381033\n",
      "    val_loss       : -1733.3269287699834\n",
      "    val_log_likelihood: 1818.6021362304687\n",
      "    val_log_marginal: 1785.9137916024774\n",
      "Train Epoch: 891 [512/54000 (1%)] Loss: -1934.257812\n",
      "Train Epoch: 891 [11776/54000 (22%)] Loss: -1742.536621\n",
      "Train Epoch: 891 [23040/54000 (43%)] Loss: -1657.940674\n",
      "Train Epoch: 891 [34304/54000 (64%)] Loss: -1688.190796\n",
      "Train Epoch: 891 [45568/54000 (84%)] Loss: -1784.684326\n",
      "    epoch          : 891\n",
      "    loss           : -1763.899980903852\n",
      "    val_loss       : -1758.8667283197865\n",
      "    val_log_likelihood: 1818.3316772460937\n",
      "    val_log_marginal: 1785.6274328175932\n",
      "Train Epoch: 892 [512/54000 (1%)] Loss: -1992.201538\n",
      "Train Epoch: 892 [11776/54000 (22%)] Loss: -1740.484497\n",
      "Train Epoch: 892 [23040/54000 (43%)] Loss: -1707.637451\n",
      "Train Epoch: 892 [34304/54000 (64%)] Loss: -1824.523438\n",
      "Train Epoch: 892 [45568/54000 (84%)] Loss: -1690.783936\n",
      "    epoch          : 892\n",
      "    loss           : -1768.4202385326423\n",
      "    val_loss       : -1759.993758755736\n",
      "    val_log_likelihood: 1818.5912109375\n",
      "    val_log_marginal: 1785.9555176649264\n",
      "Train Epoch: 893 [512/54000 (1%)] Loss: -1988.906982\n",
      "Train Epoch: 893 [11776/54000 (22%)] Loss: -1850.075806\n",
      "Train Epoch: 893 [23040/54000 (43%)] Loss: -1785.665039\n",
      "Train Epoch: 893 [34304/54000 (64%)] Loss: -1689.082031\n",
      "Train Epoch: 893 [45568/54000 (84%)] Loss: -1782.257568\n",
      "    epoch          : 893\n",
      "    loss           : -1768.7274121577198\n",
      "    val_loss       : -1736.4120737437158\n",
      "    val_log_likelihood: 1818.812744140625\n",
      "    val_log_marginal: 1786.1540866878556\n",
      "Train Epoch: 894 [512/54000 (1%)] Loss: -1936.821045\n",
      "Train Epoch: 894 [11776/54000 (22%)] Loss: -1489.899048\n",
      "Train Epoch: 894 [23040/54000 (43%)] Loss: -1685.568604\n",
      "Train Epoch: 894 [34304/54000 (64%)] Loss: -1783.067749\n",
      "Train Epoch: 894 [45568/54000 (84%)] Loss: -1759.201416\n",
      "    epoch          : 894\n",
      "    loss           : -1762.58984375\n",
      "    val_loss       : -1769.0970310598611\n",
      "    val_log_likelihood: 1819.7035034179687\n",
      "    val_log_marginal: 1787.0104079250245\n",
      "Train Epoch: 895 [512/54000 (1%)] Loss: -1990.791992\n",
      "Train Epoch: 895 [11776/54000 (22%)] Loss: -1743.896729\n",
      "Train Epoch: 895 [23040/54000 (43%)] Loss: -1758.739868\n",
      "Train Epoch: 895 [34304/54000 (64%)] Loss: -1785.731201\n",
      "Train Epoch: 895 [45568/54000 (84%)] Loss: -1785.705933\n",
      "    epoch          : 895\n",
      "    loss           : -1770.2231759552908\n",
      "    val_loss       : -1746.6713115133346\n",
      "    val_log_likelihood: 1818.3806396484374\n",
      "    val_log_marginal: 1785.681753352451\n",
      "Train Epoch: 896 [512/54000 (1%)] Loss: -1937.174561\n",
      "Train Epoch: 896 [11776/54000 (22%)] Loss: -1844.243164\n",
      "Train Epoch: 896 [23040/54000 (43%)] Loss: -1762.177734\n",
      "Train Epoch: 896 [34304/54000 (64%)] Loss: -1601.919067\n",
      "Train Epoch: 896 [45568/54000 (84%)] Loss: -1687.073975\n",
      "    epoch          : 896\n",
      "    loss           : -1760.8161814472462\n",
      "    val_loss       : -1762.0274009801446\n",
      "    val_log_likelihood: 1820.1764404296875\n",
      "    val_log_marginal: 1787.43580984883\n",
      "Train Epoch: 897 [512/54000 (1%)] Loss: -1989.218628\n",
      "Train Epoch: 897 [11776/54000 (22%)] Loss: -1745.917725\n",
      "Train Epoch: 897 [23040/54000 (43%)] Loss: -1702.901733\n",
      "Train Epoch: 897 [34304/54000 (64%)] Loss: -1756.531128\n",
      "Train Epoch: 897 [45568/54000 (84%)] Loss: -1760.517944\n",
      "    epoch          : 897\n",
      "    loss           : -1765.1584327622215\n",
      "    val_loss       : -1765.3520904922857\n",
      "    val_log_likelihood: 1819.9114135742188\n",
      "    val_log_marginal: 1787.2238350949094\n",
      "Train Epoch: 898 [512/54000 (1%)] Loss: -1935.512695\n",
      "Train Epoch: 898 [11776/54000 (22%)] Loss: -1737.757080\n",
      "Train Epoch: 898 [23040/54000 (43%)] Loss: -1707.325562\n",
      "Train Epoch: 898 [34304/54000 (64%)] Loss: -1931.776855\n",
      "Train Epoch: 898 [45568/54000 (84%)] Loss: -1760.005737\n",
      "    epoch          : 898\n",
      "    loss           : -1769.868327225789\n",
      "    val_loss       : -1763.2948217725382\n",
      "    val_log_likelihood: 1819.2163940429687\n",
      "    val_log_marginal: 1786.487071013078\n",
      "Train Epoch: 899 [512/54000 (1%)] Loss: -1990.841919\n",
      "Train Epoch: 899 [11776/54000 (22%)] Loss: -1733.567383\n",
      "Train Epoch: 899 [23040/54000 (43%)] Loss: -1790.696777\n",
      "Train Epoch: 899 [34304/54000 (64%)] Loss: -1683.694946\n",
      "Train Epoch: 899 [45568/54000 (84%)] Loss: -1691.139648\n",
      "    epoch          : 899\n",
      "    loss           : -1763.8280621519184\n",
      "    val_loss       : -1749.0345936460421\n",
      "    val_log_likelihood: 1819.2328369140625\n",
      "    val_log_marginal: 1786.564183592424\n",
      "Train Epoch: 900 [512/54000 (1%)] Loss: -1995.528442\n",
      "Train Epoch: 900 [11776/54000 (22%)] Loss: -1620.349609\n",
      "Train Epoch: 900 [23040/54000 (43%)] Loss: -1782.877563\n",
      "Train Epoch: 900 [34304/54000 (64%)] Loss: -1785.972412\n",
      "Train Epoch: 900 [45568/54000 (84%)] Loss: -1783.651123\n",
      "    epoch          : 900\n",
      "    loss           : -1758.7636960473392\n",
      "    val_loss       : -1773.4602671472355\n",
      "    val_log_likelihood: 1820.159912109375\n",
      "    val_log_marginal: 1787.4640416328082\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch900.pth ...\n",
      "Train Epoch: 901 [512/54000 (1%)] Loss: -1853.984985\n",
      "Train Epoch: 901 [11776/54000 (22%)] Loss: -1786.711548\n",
      "Train Epoch: 901 [23040/54000 (43%)] Loss: -1754.982666\n",
      "Train Epoch: 901 [34304/54000 (64%)] Loss: -1825.319336\n",
      "Train Epoch: 901 [45568/54000 (84%)] Loss: -1668.326904\n",
      "    epoch          : 901\n",
      "    loss           : -1758.9394954265933\n",
      "    val_loss       : -1766.2592175552622\n",
      "    val_log_likelihood: 1819.7844116210938\n",
      "    val_log_marginal: 1787.14025205411\n",
      "Train Epoch: 902 [512/54000 (1%)] Loss: -1848.695435\n",
      "Train Epoch: 902 [11776/54000 (22%)] Loss: -1501.629028\n",
      "Train Epoch: 902 [23040/54000 (43%)] Loss: -1827.957275\n",
      "Train Epoch: 902 [34304/54000 (64%)] Loss: -1936.856689\n",
      "Train Epoch: 902 [45568/54000 (84%)] Loss: -1781.907349\n",
      "    epoch          : 902\n",
      "    loss           : -1757.431581402769\n",
      "    val_loss       : -1733.0107342854142\n",
      "    val_log_likelihood: 1818.3695190429687\n",
      "    val_log_marginal: 1785.605560318008\n",
      "Train Epoch: 903 [512/54000 (1%)] Loss: -1935.993652\n",
      "Train Epoch: 903 [11776/54000 (22%)] Loss: -1850.791504\n",
      "Train Epoch: 903 [23040/54000 (43%)] Loss: -1359.067627\n",
      "Train Epoch: 903 [34304/54000 (64%)] Loss: -1786.582397\n",
      "Train Epoch: 903 [45568/54000 (84%)] Loss: -1686.066040\n",
      "    epoch          : 903\n",
      "    loss           : -1759.9584284112004\n",
      "    val_loss       : -1773.4732070770115\n",
      "    val_log_likelihood: 1818.1827392578125\n",
      "    val_log_marginal: 1785.4449181441305\n",
      "Train Epoch: 904 [512/54000 (1%)] Loss: -1741.588623\n",
      "Train Epoch: 904 [11776/54000 (22%)] Loss: -1742.505249\n",
      "Train Epoch: 904 [23040/54000 (43%)] Loss: -1752.856323\n",
      "Train Epoch: 904 [34304/54000 (64%)] Loss: -1694.151123\n",
      "Train Epoch: 904 [45568/54000 (84%)] Loss: -1693.523560\n",
      "    epoch          : 904\n",
      "    loss           : -1764.1419387666306\n",
      "    val_loss       : -1774.2774835918099\n",
      "    val_log_likelihood: 1819.0214721679688\n",
      "    val_log_marginal: 1786.3101237949063\n",
      "Train Epoch: 905 [512/54000 (1%)] Loss: -1704.972900\n",
      "Train Epoch: 905 [11776/54000 (22%)] Loss: -1730.681641\n",
      "Train Epoch: 905 [23040/54000 (43%)] Loss: -1823.256958\n",
      "Train Epoch: 905 [34304/54000 (64%)] Loss: -1781.162720\n",
      "Train Epoch: 905 [45568/54000 (84%)] Loss: -1664.333008\n",
      "    epoch          : 905\n",
      "    loss           : -1757.6307989441523\n",
      "    val_loss       : -1762.9364761851728\n",
      "    val_log_likelihood: 1819.624169921875\n",
      "    val_log_marginal: 1786.9336505468934\n",
      "Train Epoch: 906 [512/54000 (1%)] Loss: -1988.603027\n",
      "Train Epoch: 906 [11776/54000 (22%)] Loss: -1854.857422\n",
      "Train Epoch: 906 [23040/54000 (43%)] Loss: -1825.372314\n",
      "Train Epoch: 906 [34304/54000 (64%)] Loss: -1706.442139\n",
      "Train Epoch: 906 [45568/54000 (84%)] Loss: -1786.679199\n",
      "    epoch          : 906\n",
      "    loss           : -1759.9048069113553\n",
      "    val_loss       : -1764.7773253951223\n",
      "    val_log_likelihood: 1817.6133544921875\n",
      "    val_log_marginal: 1784.8939732518047\n",
      "Train Epoch: 907 [512/54000 (1%)] Loss: -1989.938232\n",
      "Train Epoch: 907 [11776/54000 (22%)] Loss: -1736.884766\n",
      "Train Epoch: 907 [23040/54000 (43%)] Loss: -1784.615112\n",
      "Train Epoch: 907 [34304/54000 (64%)] Loss: -1754.602905\n",
      "Train Epoch: 907 [45568/54000 (84%)] Loss: -1682.455811\n",
      "    epoch          : 907\n",
      "    loss           : -1755.7124712349164\n",
      "    val_loss       : -1763.1038716288283\n",
      "    val_log_likelihood: 1819.7408081054687\n",
      "    val_log_marginal: 1786.995360821511\n",
      "Train Epoch: 908 [512/54000 (1%)] Loss: -1990.671509\n",
      "Train Epoch: 908 [11776/54000 (22%)] Loss: -1850.558960\n",
      "Train Epoch: 908 [23040/54000 (43%)] Loss: -1706.372314\n",
      "Train Epoch: 908 [34304/54000 (64%)] Loss: -1690.008545\n",
      "Train Epoch: 908 [45568/54000 (84%)] Loss: -1791.517334\n",
      "    epoch          : 908\n",
      "    loss           : -1768.7587515953744\n",
      "    val_loss       : -1746.3014061871916\n",
      "    val_log_likelihood: 1818.8406005859374\n",
      "    val_log_marginal: 1786.1373334694654\n",
      "Train Epoch: 909 [512/54000 (1%)] Loss: -1990.753418\n",
      "Train Epoch: 909 [11776/54000 (22%)] Loss: -1561.515625\n",
      "Train Epoch: 909 [23040/54000 (43%)] Loss: -1712.071899\n",
      "Train Epoch: 909 [34304/54000 (64%)] Loss: -1756.778320\n",
      "Train Epoch: 909 [45568/54000 (84%)] Loss: -1692.993652\n",
      "    epoch          : 909\n",
      "    loss           : -1766.2381060005414\n",
      "    val_loss       : -1760.2973908076062\n",
      "    val_log_likelihood: 1818.4186645507812\n",
      "    val_log_marginal: 1785.6250897277146\n",
      "Train Epoch: 910 [512/54000 (1%)] Loss: -1992.693604\n",
      "Train Epoch: 910 [11776/54000 (22%)] Loss: -1703.267212\n",
      "Train Epoch: 910 [23040/54000 (43%)] Loss: -1756.383911\n",
      "Train Epoch: 910 [34304/54000 (64%)] Loss: -1694.503174\n",
      "Train Epoch: 910 [45568/54000 (84%)] Loss: -1683.929321\n",
      "    epoch          : 910\n",
      "    loss           : -1763.0237227336015\n",
      "    val_loss       : -1780.0040078546851\n",
      "    val_log_likelihood: 1818.9883056640624\n",
      "    val_log_marginal: 1786.317744193791\n",
      "Train Epoch: 911 [512/54000 (1%)] Loss: -1993.147949\n",
      "Train Epoch: 911 [11776/54000 (22%)] Loss: -1746.365967\n",
      "Train Epoch: 911 [23040/54000 (43%)] Loss: -1783.279297\n",
      "Train Epoch: 911 [34304/54000 (64%)] Loss: -1782.048584\n",
      "Train Epoch: 911 [45568/54000 (84%)] Loss: -1604.930176\n",
      "    epoch          : 911\n",
      "    loss           : -1765.8422162650836\n",
      "    val_loss       : -1748.6552505739032\n",
      "    val_log_likelihood: 1819.0401489257813\n",
      "    val_log_marginal: 1786.3403819726627\n",
      "Train Epoch: 912 [512/54000 (1%)] Loss: -1930.546509\n",
      "Train Epoch: 912 [11776/54000 (22%)] Loss: -1627.805664\n",
      "Train Epoch: 912 [23040/54000 (43%)] Loss: -1705.010620\n",
      "Train Epoch: 912 [34304/54000 (64%)] Loss: -1824.796997\n",
      "Train Epoch: 912 [45568/54000 (84%)] Loss: -1689.922241\n",
      "    epoch          : 912\n",
      "    loss           : -1751.052136109607\n",
      "    val_loss       : -1761.6405677150935\n",
      "    val_log_likelihood: 1819.4210327148437\n",
      "    val_log_marginal: 1786.6849686253627\n",
      "Train Epoch: 913 [512/54000 (1%)] Loss: -1990.510742\n",
      "Train Epoch: 913 [11776/54000 (22%)] Loss: -1740.336670\n",
      "Train Epoch: 913 [23040/54000 (43%)] Loss: -1685.510498\n",
      "Train Epoch: 913 [34304/54000 (64%)] Loss: -1763.190308\n",
      "Train Epoch: 913 [45568/54000 (84%)] Loss: -1785.350586\n",
      "    epoch          : 913\n",
      "    loss           : -1762.7979023244122\n",
      "    val_loss       : -1786.2277471024543\n",
      "    val_log_likelihood: 1819.4187133789062\n",
      "    val_log_marginal: 1786.6291855458155\n",
      "Train Epoch: 914 [512/54000 (1%)] Loss: -1990.476807\n",
      "Train Epoch: 914 [11776/54000 (22%)] Loss: -1619.067871\n",
      "Train Epoch: 914 [23040/54000 (43%)] Loss: -1758.649170\n",
      "Train Epoch: 914 [34304/54000 (64%)] Loss: -1705.618042\n",
      "Train Epoch: 914 [45568/54000 (84%)] Loss: -1788.036011\n",
      "    epoch          : 914\n",
      "    loss           : -1771.9267904451578\n",
      "    val_loss       : -1708.0313262814657\n",
      "    val_log_likelihood: 1819.4382080078126\n",
      "    val_log_marginal: 1786.6670116033406\n",
      "Train Epoch: 915 [512/54000 (1%)] Loss: -1743.346558\n",
      "Train Epoch: 915 [11776/54000 (22%)] Loss: -1736.989868\n",
      "Train Epoch: 915 [23040/54000 (43%)] Loss: -1786.610596\n",
      "Train Epoch: 915 [34304/54000 (64%)] Loss: -1699.058472\n",
      "Train Epoch: 915 [45568/54000 (84%)] Loss: -1668.657837\n",
      "    epoch          : 915\n",
      "    loss           : -1767.2721962503867\n",
      "    val_loss       : -1755.8043324215337\n",
      "    val_log_likelihood: 1818.7642578125\n",
      "    val_log_marginal: 1786.006669824943\n",
      "Train Epoch: 916 [512/54000 (1%)] Loss: -1927.755615\n",
      "Train Epoch: 916 [11776/54000 (22%)] Loss: -1851.131958\n",
      "Train Epoch: 916 [23040/54000 (43%)] Loss: -1759.071289\n",
      "Train Epoch: 916 [34304/54000 (64%)] Loss: -1778.057373\n",
      "Train Epoch: 916 [45568/54000 (84%)] Loss: -1587.123047\n",
      "    epoch          : 916\n",
      "    loss           : -1755.4800179358756\n",
      "    val_loss       : -1763.684463214688\n",
      "    val_log_likelihood: 1819.0626220703125\n",
      "    val_log_marginal: 1786.3239599000663\n",
      "Train Epoch: 917 [512/54000 (1%)] Loss: -1992.632202\n",
      "Train Epoch: 917 [11776/54000 (22%)] Loss: -1851.034180\n",
      "Train Epoch: 917 [23040/54000 (43%)] Loss: -1783.211914\n",
      "Train Epoch: 917 [34304/54000 (64%)] Loss: -1788.370972\n",
      "Train Epoch: 917 [45568/54000 (84%)] Loss: -1754.620605\n",
      "    epoch          : 917\n",
      "    loss           : -1758.516466197401\n",
      "    val_loss       : -1736.1551340591163\n",
      "    val_log_likelihood: 1819.077490234375\n",
      "    val_log_marginal: 1786.3450077436278\n",
      "Train Epoch: 918 [512/54000 (1%)] Loss: -1930.260254\n",
      "Train Epoch: 918 [11776/54000 (22%)] Loss: -1704.141846\n",
      "Train Epoch: 918 [23040/54000 (43%)] Loss: -1783.459106\n",
      "Train Epoch: 918 [34304/54000 (64%)] Loss: -1779.519287\n",
      "Train Epoch: 918 [45568/54000 (84%)] Loss: -1786.508057\n",
      "    epoch          : 918\n",
      "    loss           : -1763.2431398901608\n",
      "    val_loss       : -1777.1819267813116\n",
      "    val_log_likelihood: 1819.8269165039062\n",
      "    val_log_marginal: 1787.1055362243205\n",
      "Train Epoch: 919 [512/54000 (1%)] Loss: -1931.865112\n",
      "Train Epoch: 919 [11776/54000 (22%)] Loss: -1736.153564\n",
      "Train Epoch: 919 [23040/54000 (43%)] Loss: -1689.092285\n",
      "Train Epoch: 919 [34304/54000 (64%)] Loss: -1992.367676\n",
      "Train Epoch: 919 [45568/54000 (84%)] Loss: -1783.085693\n",
      "    epoch          : 919\n",
      "    loss           : -1767.0952378074721\n",
      "    val_loss       : -1780.0640787694604\n",
      "    val_log_likelihood: 1819.0927124023438\n",
      "    val_log_marginal: 1786.3128797147424\n",
      "Train Epoch: 920 [512/54000 (1%)] Loss: -1992.656372\n",
      "Train Epoch: 920 [11776/54000 (22%)] Loss: -1851.355347\n",
      "Train Epoch: 920 [23040/54000 (43%)] Loss: -1827.869873\n",
      "Train Epoch: 920 [34304/54000 (64%)] Loss: -1693.377197\n",
      "Train Epoch: 920 [45568/54000 (84%)] Loss: -1787.731567\n",
      "    epoch          : 920\n",
      "    loss           : -1755.3230149675123\n",
      "    val_loss       : -1720.9158993478864\n",
      "    val_log_likelihood: 1819.1377197265624\n",
      "    val_log_marginal: 1786.4159615535289\n",
      "Train Epoch: 921 [512/54000 (1%)] Loss: -1985.841553\n",
      "Train Epoch: 921 [11776/54000 (22%)] Loss: -1742.171509\n",
      "Train Epoch: 921 [23040/54000 (43%)] Loss: -1683.061768\n",
      "Train Epoch: 921 [34304/54000 (64%)] Loss: -1695.595825\n",
      "Train Epoch: 921 [45568/54000 (84%)] Loss: -1685.300781\n",
      "    epoch          : 921\n",
      "    loss           : -1762.1447911026455\n",
      "    val_loss       : -1774.3456985212863\n",
      "    val_log_likelihood: 1818.7971435546874\n",
      "    val_log_marginal: 1786.1426089625807\n",
      "Train Epoch: 922 [512/54000 (1%)] Loss: -1847.085205\n",
      "Train Epoch: 922 [11776/54000 (22%)] Loss: -1701.858154\n",
      "Train Epoch: 922 [23040/54000 (43%)] Loss: -1744.662598\n",
      "Train Epoch: 922 [34304/54000 (64%)] Loss: -1561.073486\n",
      "Train Epoch: 922 [45568/54000 (84%)] Loss: -1759.057739\n",
      "    epoch          : 922\n",
      "    loss           : -1771.4549886873453\n",
      "    val_loss       : -1786.3079387495295\n",
      "    val_log_likelihood: 1819.517822265625\n",
      "    val_log_marginal: 1786.7883965265005\n",
      "Train Epoch: 923 [512/54000 (1%)] Loss: -1991.862427\n",
      "Train Epoch: 923 [11776/54000 (22%)] Loss: -1737.147705\n",
      "Train Epoch: 923 [23040/54000 (43%)] Loss: -1754.623779\n",
      "Train Epoch: 923 [34304/54000 (64%)] Loss: -1786.602417\n",
      "Train Epoch: 923 [45568/54000 (84%)] Loss: -1789.394287\n",
      "    epoch          : 923\n",
      "    loss           : -1760.7414828763149\n",
      "    val_loss       : -1756.8872086640447\n",
      "    val_log_likelihood: 1819.0076904296875\n",
      "    val_log_marginal: 1786.2588513229043\n",
      "Train Epoch: 924 [512/54000 (1%)] Loss: -1990.221191\n",
      "Train Epoch: 924 [11776/54000 (22%)] Loss: -1740.922363\n",
      "Train Epoch: 924 [23040/54000 (43%)] Loss: -1557.776733\n",
      "Train Epoch: 924 [34304/54000 (64%)] Loss: -1722.824707\n",
      "Train Epoch: 924 [45568/54000 (84%)] Loss: -1786.306152\n",
      "    epoch          : 924\n",
      "    loss           : -1758.7094061823175\n",
      "    val_loss       : -1786.247293013707\n",
      "    val_log_likelihood: 1819.6489135742188\n",
      "    val_log_marginal: 1786.8955775578002\n",
      "Train Epoch: 925 [512/54000 (1%)] Loss: -1935.425171\n",
      "Train Epoch: 925 [11776/54000 (22%)] Loss: -1786.832275\n",
      "Train Epoch: 925 [23040/54000 (43%)] Loss: -1787.149658\n",
      "Train Epoch: 925 [34304/54000 (64%)] Loss: -1781.056885\n",
      "Train Epoch: 925 [45568/54000 (84%)] Loss: -1691.554321\n",
      "    epoch          : 925\n",
      "    loss           : -1756.8089007387066\n",
      "    val_loss       : -1786.135479879938\n",
      "    val_log_likelihood: 1819.4771484375\n",
      "    val_log_marginal: 1786.770128549224\n",
      "Train Epoch: 926 [512/54000 (1%)] Loss: -1933.413330\n",
      "Train Epoch: 926 [11776/54000 (22%)] Loss: -1627.739990\n",
      "Train Epoch: 926 [23040/54000 (43%)] Loss: -1745.142700\n",
      "Train Epoch: 926 [34304/54000 (64%)] Loss: -1751.257446\n",
      "Train Epoch: 926 [45568/54000 (84%)] Loss: -1664.986206\n",
      "    epoch          : 926\n",
      "    loss           : -1765.001035784731\n",
      "    val_loss       : -1774.5259443914517\n",
      "    val_log_likelihood: 1820.5580688476562\n",
      "    val_log_marginal: 1787.7881700035184\n",
      "Train Epoch: 927 [512/54000 (1%)] Loss: -1875.425537\n",
      "Train Epoch: 927 [11776/54000 (22%)] Loss: -1855.431519\n",
      "Train Epoch: 927 [23040/54000 (43%)] Loss: -1694.686768\n",
      "Train Epoch: 927 [34304/54000 (64%)] Loss: -1990.935303\n",
      "Train Epoch: 927 [45568/54000 (84%)] Loss: -1670.191895\n",
      "    epoch          : 927\n",
      "    loss           : -1758.7207478438274\n",
      "    val_loss       : -1749.8264110136777\n",
      "    val_log_likelihood: 1819.7305419921875\n",
      "    val_log_marginal: 1787.0280072141472\n",
      "Train Epoch: 928 [512/54000 (1%)] Loss: -1993.618652\n",
      "Train Epoch: 928 [11776/54000 (22%)] Loss: -1743.237061\n",
      "Train Epoch: 928 [23040/54000 (43%)] Loss: -1664.350098\n",
      "Train Epoch: 928 [34304/54000 (64%)] Loss: -1571.582764\n",
      "Train Epoch: 928 [45568/54000 (84%)] Loss: -1685.787842\n",
      "    epoch          : 928\n",
      "    loss           : -1758.4408007329052\n",
      "    val_loss       : -1763.333373546228\n",
      "    val_log_likelihood: 1819.5461059570312\n",
      "    val_log_marginal: 1786.7172729568763\n",
      "Train Epoch: 929 [512/54000 (1%)] Loss: -1845.053223\n",
      "Train Epoch: 929 [11776/54000 (22%)] Loss: -1738.117310\n",
      "Train Epoch: 929 [23040/54000 (43%)] Loss: -1781.462524\n",
      "Train Epoch: 929 [34304/54000 (64%)] Loss: -1674.302612\n",
      "Train Epoch: 929 [45568/54000 (84%)] Loss: -1787.736816\n",
      "    epoch          : 929\n",
      "    loss           : -1769.7464635867884\n",
      "    val_loss       : -1774.8883121106774\n",
      "    val_log_likelihood: 1819.5292236328125\n",
      "    val_log_marginal: 1786.7709771160037\n",
      "Train Epoch: 930 [512/54000 (1%)] Loss: -1992.276733\n",
      "Train Epoch: 930 [11776/54000 (22%)] Loss: -1853.811035\n",
      "Train Epoch: 930 [23040/54000 (43%)] Loss: -1823.141113\n",
      "Train Epoch: 930 [34304/54000 (64%)] Loss: -1994.780151\n",
      "Train Epoch: 930 [45568/54000 (84%)] Loss: -1786.916016\n",
      "    epoch          : 930\n",
      "    loss           : -1754.769017587794\n",
      "    val_loss       : -1785.8873127212748\n",
      "    val_log_likelihood: 1819.2088623046875\n",
      "    val_log_marginal: 1786.4792637524333\n",
      "Train Epoch: 931 [512/54000 (1%)] Loss: -1994.529663\n",
      "Train Epoch: 931 [11776/54000 (22%)] Loss: -1854.442871\n",
      "Train Epoch: 931 [23040/54000 (43%)] Loss: -1660.109619\n",
      "Train Epoch: 931 [34304/54000 (64%)] Loss: -1570.032837\n",
      "Train Epoch: 931 [45568/54000 (84%)] Loss: -1782.117310\n",
      "    epoch          : 931\n",
      "    loss           : -1755.0941343401919\n",
      "    val_loss       : -1747.3689214181154\n",
      "    val_log_likelihood: 1819.7233032226563\n",
      "    val_log_marginal: 1787.0100325312465\n",
      "Train Epoch: 932 [512/54000 (1%)] Loss: -1992.683105\n",
      "Train Epoch: 932 [11776/54000 (22%)] Loss: -1852.452271\n",
      "Train Epoch: 932 [23040/54000 (43%)] Loss: -1565.056152\n",
      "Train Epoch: 932 [34304/54000 (64%)] Loss: -1699.951294\n",
      "Train Epoch: 932 [45568/54000 (84%)] Loss: -1698.382690\n",
      "    epoch          : 932\n",
      "    loss           : -1759.0080808129642\n",
      "    val_loss       : -1773.6096173461526\n",
      "    val_log_likelihood: 1820.1017700195312\n",
      "    val_log_marginal: 1787.3445257443934\n",
      "Train Epoch: 933 [512/54000 (1%)] Loss: -1993.348755\n",
      "Train Epoch: 933 [11776/54000 (22%)] Loss: -1845.535767\n",
      "Train Epoch: 933 [23040/54000 (43%)] Loss: -1784.656860\n",
      "Train Epoch: 933 [34304/54000 (64%)] Loss: -1561.457520\n",
      "Train Epoch: 933 [45568/54000 (84%)] Loss: -1691.401001\n",
      "    epoch          : 933\n",
      "    loss           : -1754.3182445563893\n",
      "    val_loss       : -1780.4571418985724\n",
      "    val_log_likelihood: 1819.4626708984374\n",
      "    val_log_marginal: 1786.7295734284962\n",
      "Train Epoch: 934 [512/54000 (1%)] Loss: -1990.675537\n",
      "Train Epoch: 934 [11776/54000 (22%)] Loss: -1622.865234\n",
      "Train Epoch: 934 [23040/54000 (43%)] Loss: -1824.819336\n",
      "Train Epoch: 934 [34304/54000 (64%)] Loss: -1760.449097\n",
      "Train Epoch: 934 [45568/54000 (84%)] Loss: -1787.884521\n",
      "    epoch          : 934\n",
      "    loss           : -1770.0842671913676\n",
      "    val_loss       : -1786.0174767039716\n",
      "    val_log_likelihood: 1819.1034301757813\n",
      "    val_log_marginal: 1786.3298479614368\n",
      "Train Epoch: 935 [512/54000 (1%)] Loss: -1624.523438\n",
      "Train Epoch: 935 [11776/54000 (22%)] Loss: -1851.479248\n",
      "Train Epoch: 935 [23040/54000 (43%)] Loss: -1823.983887\n",
      "Train Epoch: 935 [34304/54000 (64%)] Loss: -1789.810059\n",
      "Train Epoch: 935 [45568/54000 (84%)] Loss: -1690.296997\n",
      "    epoch          : 935\n",
      "    loss           : -1770.6415000386758\n",
      "    val_loss       : -1764.0720944315194\n",
      "    val_log_likelihood: 1819.950244140625\n",
      "    val_log_marginal: 1787.2635643605142\n",
      "Train Epoch: 936 [512/54000 (1%)] Loss: -1990.792847\n",
      "Train Epoch: 936 [11776/54000 (22%)] Loss: -1757.764893\n",
      "Train Epoch: 936 [23040/54000 (43%)] Loss: -1702.615234\n",
      "Train Epoch: 936 [34304/54000 (64%)] Loss: -1657.625732\n",
      "Train Epoch: 936 [45568/54000 (84%)] Loss: -1668.880371\n",
      "    epoch          : 936\n",
      "    loss           : -1764.7067810662902\n",
      "    val_loss       : -1774.4279435783624\n",
      "    val_log_likelihood: 1819.7640502929687\n",
      "    val_log_marginal: 1787.008329079673\n",
      "Train Epoch: 937 [512/54000 (1%)] Loss: -1996.374634\n",
      "Train Epoch: 937 [11776/54000 (22%)] Loss: -1626.359985\n",
      "Train Epoch: 937 [23040/54000 (43%)] Loss: -1755.978760\n",
      "Train Epoch: 937 [34304/54000 (64%)] Loss: -1778.657227\n",
      "Train Epoch: 937 [45568/54000 (84%)] Loss: -1782.589111\n",
      "    epoch          : 937\n",
      "    loss           : -1764.0021779277538\n",
      "    val_loss       : -1739.8837441569194\n",
      "    val_log_likelihood: 1819.282470703125\n",
      "    val_log_marginal: 1786.5239539738745\n",
      "Train Epoch: 938 [512/54000 (1%)] Loss: -1994.874634\n",
      "Train Epoch: 938 [11776/54000 (22%)] Loss: -1829.698608\n",
      "Train Epoch: 938 [23040/54000 (43%)] Loss: -1705.862549\n",
      "Train Epoch: 938 [34304/54000 (64%)] Loss: -1756.742920\n",
      "Train Epoch: 938 [45568/54000 (84%)] Loss: -1781.186523\n",
      "    epoch          : 938\n",
      "    loss           : -1765.5076312074566\n",
      "    val_loss       : -1738.2627695186063\n",
      "    val_log_likelihood: 1819.8348022460937\n",
      "    val_log_marginal: 1787.1260019830174\n",
      "Train Epoch: 939 [512/54000 (1%)] Loss: -1991.577026\n",
      "Train Epoch: 939 [11776/54000 (22%)] Loss: -1850.078979\n",
      "Train Epoch: 939 [23040/54000 (43%)] Loss: -1706.472412\n",
      "Train Epoch: 939 [34304/54000 (64%)] Loss: -1787.966919\n",
      "Train Epoch: 939 [45568/54000 (84%)] Loss: -1684.544922\n",
      "    epoch          : 939\n",
      "    loss           : -1764.7592205387532\n",
      "    val_loss       : -1770.297598307021\n",
      "    val_log_likelihood: 1820.4437866210938\n",
      "    val_log_marginal: 1787.699530343339\n",
      "Train Epoch: 940 [512/54000 (1%)] Loss: -1991.466553\n",
      "Train Epoch: 940 [11776/54000 (22%)] Loss: -1745.020996\n",
      "Train Epoch: 940 [23040/54000 (43%)] Loss: -1755.318115\n",
      "Train Epoch: 940 [34304/54000 (64%)] Loss: -1820.634155\n",
      "Train Epoch: 940 [45568/54000 (84%)] Loss: -1785.965088\n",
      "    epoch          : 940\n",
      "    loss           : -1765.2939960744122\n",
      "    val_loss       : -1747.5170378364624\n",
      "    val_log_likelihood: 1820.0812622070312\n",
      "    val_log_marginal: 1787.3238722000292\n",
      "Train Epoch: 941 [512/54000 (1%)] Loss: -1996.007080\n",
      "Train Epoch: 941 [11776/54000 (22%)] Loss: -1740.071533\n",
      "Train Epoch: 941 [23040/54000 (43%)] Loss: -1761.155273\n",
      "Train Epoch: 941 [34304/54000 (64%)] Loss: -1785.109131\n",
      "Train Epoch: 941 [45568/54000 (84%)] Loss: -1686.880127\n",
      "    epoch          : 941\n",
      "    loss           : -1758.3589471495977\n",
      "    val_loss       : -1722.7254108786583\n",
      "    val_log_likelihood: 1819.9214233398438\n",
      "    val_log_marginal: 1787.2380780111998\n",
      "Train Epoch: 942 [512/54000 (1%)] Loss: -1851.696777\n",
      "Train Epoch: 942 [11776/54000 (22%)] Loss: -1710.755127\n",
      "Train Epoch: 942 [23040/54000 (43%)] Loss: -1822.258179\n",
      "Train Epoch: 942 [34304/54000 (64%)] Loss: -1761.122070\n",
      "Train Epoch: 942 [45568/54000 (84%)] Loss: -1690.612549\n",
      "    epoch          : 942\n",
      "    loss           : -1774.6220062558014\n",
      "    val_loss       : -1756.6650507997722\n",
      "    val_log_likelihood: 1820.3415283203126\n",
      "    val_log_marginal: 1787.7280095944218\n",
      "Train Epoch: 943 [512/54000 (1%)] Loss: -1996.880371\n",
      "Train Epoch: 943 [11776/54000 (22%)] Loss: -1707.805176\n",
      "Train Epoch: 943 [23040/54000 (43%)] Loss: -1818.926392\n",
      "Train Epoch: 943 [34304/54000 (64%)] Loss: -1782.845337\n",
      "Train Epoch: 943 [45568/54000 (84%)] Loss: -1691.663208\n",
      "    epoch          : 943\n",
      "    loss           : -1756.1413852200649\n",
      "    val_loss       : -1750.3318290380762\n",
      "    val_log_likelihood: 1819.8662963867187\n",
      "    val_log_marginal: 1787.1280467762663\n",
      "Train Epoch: 944 [512/54000 (1%)] Loss: -1990.058472\n",
      "Train Epoch: 944 [11776/54000 (22%)] Loss: -1849.225464\n",
      "Train Epoch: 944 [23040/54000 (43%)] Loss: -1788.127441\n",
      "Train Epoch: 944 [34304/54000 (64%)] Loss: -1992.674316\n",
      "Train Epoch: 944 [45568/54000 (84%)] Loss: -1695.456909\n",
      "    epoch          : 944\n",
      "    loss           : -1770.8956105449413\n",
      "    val_loss       : -1774.458333980292\n",
      "    val_log_likelihood: 1819.1842041015625\n",
      "    val_log_marginal: 1786.4839125227184\n",
      "Train Epoch: 945 [512/54000 (1%)] Loss: -1990.796875\n",
      "Train Epoch: 945 [11776/54000 (22%)] Loss: -1746.088379\n",
      "Train Epoch: 945 [23040/54000 (43%)] Loss: -1788.924561\n",
      "Train Epoch: 945 [34304/54000 (64%)] Loss: -1760.629272\n",
      "Train Epoch: 945 [45568/54000 (84%)] Loss: -1780.817627\n",
      "    epoch          : 945\n",
      "    loss           : -1750.3751136099938\n",
      "    val_loss       : -1756.3040032796562\n",
      "    val_log_likelihood: 1819.4784912109376\n",
      "    val_log_marginal: 1786.7428702746952\n",
      "Train Epoch: 946 [512/54000 (1%)] Loss: -1990.763428\n",
      "Train Epoch: 946 [11776/54000 (22%)] Loss: -1700.324951\n",
      "Train Epoch: 946 [23040/54000 (43%)] Loss: -1822.368896\n",
      "Train Epoch: 946 [34304/54000 (64%)] Loss: -1678.512939\n",
      "Train Epoch: 946 [45568/54000 (84%)] Loss: -1597.756104\n",
      "    epoch          : 946\n",
      "    loss           : -1754.549317614867\n",
      "    val_loss       : -1773.5593583708628\n",
      "    val_log_likelihood: 1819.9564453125\n",
      "    val_log_marginal: 1787.2142554689199\n",
      "Train Epoch: 947 [512/54000 (1%)] Loss: -1992.478638\n",
      "Train Epoch: 947 [11776/54000 (22%)] Loss: -1845.668335\n",
      "Train Epoch: 947 [23040/54000 (43%)] Loss: -1680.969971\n",
      "Train Epoch: 947 [34304/54000 (64%)] Loss: -1695.122925\n",
      "Train Epoch: 947 [45568/54000 (84%)] Loss: -1690.402832\n",
      "    epoch          : 947\n",
      "    loss           : -1761.1126213451423\n",
      "    val_loss       : -1767.065159679018\n",
      "    val_log_likelihood: 1819.0248779296876\n",
      "    val_log_marginal: 1786.3611824769528\n",
      "Train Epoch: 948 [512/54000 (1%)] Loss: -1993.947754\n",
      "Train Epoch: 948 [11776/54000 (22%)] Loss: -1784.864014\n",
      "Train Epoch: 948 [23040/54000 (43%)] Loss: -1711.674683\n",
      "Train Epoch: 948 [34304/54000 (64%)] Loss: -1933.237061\n",
      "Train Epoch: 948 [45568/54000 (84%)] Loss: -1783.296631\n",
      "    epoch          : 948\n",
      "    loss           : -1763.1680412670173\n",
      "    val_loss       : -1763.3838476803153\n",
      "    val_log_likelihood: 1819.0349609375\n",
      "    val_log_marginal: 1786.261119408493\n",
      "Train Epoch: 949 [512/54000 (1%)] Loss: -1991.167725\n",
      "Train Epoch: 949 [11776/54000 (22%)] Loss: -1703.756104\n",
      "Train Epoch: 949 [23040/54000 (43%)] Loss: -1759.788696\n",
      "Train Epoch: 949 [34304/54000 (64%)] Loss: -1785.123047\n",
      "Train Epoch: 949 [45568/54000 (84%)] Loss: -1787.318237\n",
      "    epoch          : 949\n",
      "    loss           : -1766.8027247060643\n",
      "    val_loss       : -1746.2612462779507\n",
      "    val_log_likelihood: 1819.697216796875\n",
      "    val_log_marginal: 1787.0418029668515\n",
      "Train Epoch: 950 [512/54000 (1%)] Loss: -1993.441406\n",
      "Train Epoch: 950 [11776/54000 (22%)] Loss: -1747.738037\n",
      "Train Epoch: 950 [23040/54000 (43%)] Loss: -1709.680664\n",
      "Train Epoch: 950 [34304/54000 (64%)] Loss: -1818.985229\n",
      "Train Epoch: 950 [45568/54000 (84%)] Loss: -1783.504272\n",
      "    epoch          : 950\n",
      "    loss           : -1760.2640380859375\n",
      "    val_loss       : -1763.6635892752558\n",
      "    val_log_likelihood: 1819.0251586914062\n",
      "    val_log_marginal: 1786.3117029153589\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch950.pth ...\n",
      "Train Epoch: 951 [512/54000 (1%)] Loss: -1993.732422\n",
      "Train Epoch: 951 [11776/54000 (22%)] Loss: -1845.043457\n",
      "Train Epoch: 951 [23040/54000 (43%)] Loss: -1785.634766\n",
      "Train Epoch: 951 [34304/54000 (64%)] Loss: -1822.506836\n",
      "Train Epoch: 951 [45568/54000 (84%)] Loss: -1784.921387\n",
      "    epoch          : 951\n",
      "    loss           : -1765.104222665919\n",
      "    val_loss       : -1762.4593373410403\n",
      "    val_log_likelihood: 1820.5488037109376\n",
      "    val_log_marginal: 1787.89164516383\n",
      "Train Epoch: 952 [512/54000 (1%)] Loss: -1993.548096\n",
      "Train Epoch: 952 [11776/54000 (22%)] Loss: -1567.169067\n",
      "Train Epoch: 952 [23040/54000 (43%)] Loss: -1821.494141\n",
      "Train Epoch: 952 [34304/54000 (64%)] Loss: -1705.459717\n",
      "Train Epoch: 952 [45568/54000 (84%)] Loss: -1782.055542\n",
      "    epoch          : 952\n",
      "    loss           : -1774.5065120281558\n",
      "    val_loss       : -1774.7324963739143\n",
      "    val_log_likelihood: 1819.5785278320313\n",
      "    val_log_marginal: 1786.8786509671475\n",
      "Train Epoch: 953 [512/54000 (1%)] Loss: -1739.842041\n",
      "Train Epoch: 953 [11776/54000 (22%)] Loss: -1629.355713\n",
      "Train Epoch: 953 [23040/54000 (43%)] Loss: -1822.746582\n",
      "Train Epoch: 953 [34304/54000 (64%)] Loss: -1817.451904\n",
      "Train Epoch: 953 [45568/54000 (84%)] Loss: -1783.913330\n",
      "    epoch          : 953\n",
      "    loss           : -1770.1552734375\n",
      "    val_loss       : -1775.1522813865915\n",
      "    val_log_likelihood: 1819.5563354492188\n",
      "    val_log_marginal: 1786.8755929600898\n",
      "Train Epoch: 954 [512/54000 (1%)] Loss: -1990.609619\n",
      "Train Epoch: 954 [11776/54000 (22%)] Loss: -1629.091797\n",
      "Train Epoch: 954 [23040/54000 (43%)] Loss: -1756.573975\n",
      "Train Epoch: 954 [34304/54000 (64%)] Loss: -1784.524292\n",
      "Train Epoch: 954 [45568/54000 (84%)] Loss: -1826.071045\n",
      "    epoch          : 954\n",
      "    loss           : -1756.7519857576578\n",
      "    val_loss       : -1714.1979201965034\n",
      "    val_log_likelihood: 1820.3671142578125\n",
      "    val_log_marginal: 1787.5893289412154\n",
      "Train Epoch: 955 [512/54000 (1%)] Loss: -1992.635010\n",
      "Train Epoch: 955 [11776/54000 (22%)] Loss: -1852.233398\n",
      "Train Epoch: 955 [23040/54000 (43%)] Loss: -1752.793213\n",
      "Train Epoch: 955 [34304/54000 (64%)] Loss: -1785.399170\n",
      "Train Epoch: 955 [45568/54000 (84%)] Loss: -1783.381592\n",
      "    epoch          : 955\n",
      "    loss           : -1769.9440748762377\n",
      "    val_loss       : -1766.4554455310106\n",
      "    val_log_likelihood: 1820.1164916992188\n",
      "    val_log_marginal: 1787.392529802397\n",
      "Train Epoch: 956 [512/54000 (1%)] Loss: -1993.575317\n",
      "Train Epoch: 956 [11776/54000 (22%)] Loss: -1734.260254\n",
      "Train Epoch: 956 [23040/54000 (43%)] Loss: -1826.782349\n",
      "Train Epoch: 956 [34304/54000 (64%)] Loss: -1784.491699\n",
      "Train Epoch: 956 [45568/54000 (84%)] Loss: -1686.204346\n",
      "    epoch          : 956\n",
      "    loss           : -1759.047692025062\n",
      "    val_loss       : -1774.891642889008\n",
      "    val_log_likelihood: 1819.2583129882812\n",
      "    val_log_marginal: 1786.5888532602598\n",
      "Train Epoch: 957 [512/54000 (1%)] Loss: -1744.948608\n",
      "Train Epoch: 957 [11776/54000 (22%)] Loss: -1625.572876\n",
      "Train Epoch: 957 [23040/54000 (43%)] Loss: -1703.341064\n",
      "Train Epoch: 957 [34304/54000 (64%)] Loss: -1656.781738\n",
      "Train Epoch: 957 [45568/54000 (84%)] Loss: -1780.804443\n",
      "    epoch          : 957\n",
      "    loss           : -1768.8471534653465\n",
      "    val_loss       : -1753.3072681300341\n",
      "    val_log_likelihood: 1818.9079223632812\n",
      "    val_log_marginal: 1786.2333497796208\n",
      "Train Epoch: 958 [512/54000 (1%)] Loss: -1992.052856\n",
      "Train Epoch: 958 [11776/54000 (22%)] Loss: -1756.997803\n",
      "Train Epoch: 958 [23040/54000 (43%)] Loss: -1822.781128\n",
      "Train Epoch: 958 [34304/54000 (64%)] Loss: -1784.305908\n",
      "Train Epoch: 958 [45568/54000 (84%)] Loss: -1791.906860\n",
      "    epoch          : 958\n",
      "    loss           : -1760.5348963973545\n",
      "    val_loss       : -1786.8912354985252\n",
      "    val_log_likelihood: 1819.9332275390625\n",
      "    val_log_marginal: 1787.183540001884\n",
      "Train Epoch: 959 [512/54000 (1%)] Loss: -1990.780518\n",
      "Train Epoch: 959 [11776/54000 (22%)] Loss: -1734.709473\n",
      "Train Epoch: 959 [23040/54000 (43%)] Loss: -1682.528931\n",
      "Train Epoch: 959 [34304/54000 (64%)] Loss: -1787.121216\n",
      "Train Epoch: 959 [45568/54000 (84%)] Loss: -1781.039673\n",
      "    epoch          : 959\n",
      "    loss           : -1770.7458157681003\n",
      "    val_loss       : -1786.8753219287842\n",
      "    val_log_likelihood: 1820.3156982421874\n",
      "    val_log_marginal: 1787.5781367313116\n",
      "Train Epoch: 960 [512/54000 (1%)] Loss: -1995.055908\n",
      "Train Epoch: 960 [11776/54000 (22%)] Loss: -1854.244141\n",
      "Train Epoch: 960 [23040/54000 (43%)] Loss: -1782.405640\n",
      "Train Epoch: 960 [34304/54000 (64%)] Loss: -1784.780029\n",
      "Train Epoch: 960 [45568/54000 (84%)] Loss: -1790.283203\n",
      "    epoch          : 960\n",
      "    loss           : -1765.8929165377476\n",
      "    val_loss       : -1759.363794315234\n",
      "    val_log_likelihood: 1820.131298828125\n",
      "    val_log_marginal: 1787.3649232339114\n",
      "Train Epoch: 961 [512/54000 (1%)] Loss: -1993.141479\n",
      "Train Epoch: 961 [11776/54000 (22%)] Loss: -1631.076172\n",
      "Train Epoch: 961 [23040/54000 (43%)] Loss: -1692.193115\n",
      "Train Epoch: 961 [34304/54000 (64%)] Loss: -1569.154419\n",
      "Train Epoch: 961 [45568/54000 (84%)] Loss: -1780.377930\n",
      "    epoch          : 961\n",
      "    loss           : -1764.7434166634437\n",
      "    val_loss       : -1764.7782720087096\n",
      "    val_log_likelihood: 1819.885302734375\n",
      "    val_log_marginal: 1787.1782096009701\n",
      "Train Epoch: 962 [512/54000 (1%)] Loss: -1701.429321\n",
      "Train Epoch: 962 [11776/54000 (22%)] Loss: -1702.680908\n",
      "Train Epoch: 962 [23040/54000 (43%)] Loss: -1829.055664\n",
      "Train Epoch: 962 [34304/54000 (64%)] Loss: -1688.097412\n",
      "Train Epoch: 962 [45568/54000 (84%)] Loss: -1679.732910\n",
      "    epoch          : 962\n",
      "    loss           : -1773.8630806195854\n",
      "    val_loss       : -1777.850815006718\n",
      "    val_log_likelihood: 1820.456982421875\n",
      "    val_log_marginal: 1787.7472075237456\n",
      "Train Epoch: 963 [512/54000 (1%)] Loss: -1989.288940\n",
      "Train Epoch: 963 [11776/54000 (22%)] Loss: -1630.898926\n",
      "Train Epoch: 963 [23040/54000 (43%)] Loss: -1783.149048\n",
      "Train Epoch: 963 [34304/54000 (64%)] Loss: -1575.228516\n",
      "Train Epoch: 963 [45568/54000 (84%)] Loss: -1790.828125\n",
      "    epoch          : 963\n",
      "    loss           : -1761.6812393641708\n",
      "    val_loss       : -1763.4367485875264\n",
      "    val_log_likelihood: 1820.0414428710938\n",
      "    val_log_marginal: 1787.2394956920361\n",
      "Train Epoch: 964 [512/54000 (1%)] Loss: -1850.501953\n",
      "Train Epoch: 964 [11776/54000 (22%)] Loss: -1822.179932\n",
      "Train Epoch: 964 [23040/54000 (43%)] Loss: -1786.508057\n",
      "Train Epoch: 964 [34304/54000 (64%)] Loss: -1789.229736\n",
      "Train Epoch: 964 [45568/54000 (84%)] Loss: -1790.558472\n",
      "    epoch          : 964\n",
      "    loss           : -1766.266820322169\n",
      "    val_loss       : -1736.954315439239\n",
      "    val_log_likelihood: 1820.1303833007812\n",
      "    val_log_marginal: 1787.3863707330079\n",
      "Train Epoch: 965 [512/54000 (1%)] Loss: -1745.742432\n",
      "Train Epoch: 965 [11776/54000 (22%)] Loss: -1761.123779\n",
      "Train Epoch: 965 [23040/54000 (43%)] Loss: -1706.290405\n",
      "Train Epoch: 965 [34304/54000 (64%)] Loss: -1935.139160\n",
      "Train Epoch: 965 [45568/54000 (84%)] Loss: -1787.043579\n",
      "    epoch          : 965\n",
      "    loss           : -1771.4655109065593\n",
      "    val_loss       : -1748.1232115600258\n",
      "    val_log_likelihood: 1820.0373779296874\n",
      "    val_log_marginal: 1787.3193971809953\n",
      "Train Epoch: 966 [512/54000 (1%)] Loss: -1991.662842\n",
      "Train Epoch: 966 [11776/54000 (22%)] Loss: -1737.562256\n",
      "Train Epoch: 966 [23040/54000 (43%)] Loss: -1712.210693\n",
      "Train Epoch: 966 [34304/54000 (64%)] Loss: -1699.837524\n",
      "Train Epoch: 966 [45568/54000 (84%)] Loss: -1660.690430\n",
      "    epoch          : 966\n",
      "    loss           : -1757.314967995823\n",
      "    val_loss       : -1743.4159876121207\n",
      "    val_log_likelihood: 1819.7817626953124\n",
      "    val_log_marginal: 1787.0920644063503\n",
      "Train Epoch: 967 [512/54000 (1%)] Loss: -1995.467041\n",
      "Train Epoch: 967 [11776/54000 (22%)] Loss: -1742.567261\n",
      "Train Epoch: 967 [23040/54000 (43%)] Loss: -1785.475708\n",
      "Train Epoch: 967 [34304/54000 (64%)] Loss: -1763.458374\n",
      "Train Epoch: 967 [45568/54000 (84%)] Loss: -1691.150757\n",
      "    epoch          : 967\n",
      "    loss           : -1759.0227703434407\n",
      "    val_loss       : -1772.4862421737985\n",
      "    val_log_likelihood: 1820.5165161132813\n",
      "    val_log_marginal: 1787.8105835106223\n",
      "Train Epoch: 968 [512/54000 (1%)] Loss: -1992.859009\n",
      "Train Epoch: 968 [11776/54000 (22%)] Loss: -1705.785645\n",
      "Train Epoch: 968 [23040/54000 (43%)] Loss: -1706.615112\n",
      "Train Epoch: 968 [34304/54000 (64%)] Loss: -1992.569702\n",
      "Train Epoch: 968 [45568/54000 (84%)] Loss: -1687.742676\n",
      "    epoch          : 968\n",
      "    loss           : -1774.5883861579518\n",
      "    val_loss       : -1771.1230401961134\n",
      "    val_log_likelihood: 1820.8705322265625\n",
      "    val_log_marginal: 1788.1531860690563\n",
      "Train Epoch: 969 [512/54000 (1%)] Loss: -1987.045654\n",
      "Train Epoch: 969 [11776/54000 (22%)] Loss: -1702.995483\n",
      "Train Epoch: 969 [23040/54000 (43%)] Loss: -1691.194092\n",
      "Train Epoch: 969 [34304/54000 (64%)] Loss: -1996.640625\n",
      "Train Epoch: 969 [45568/54000 (84%)] Loss: -1781.806641\n",
      "    epoch          : 969\n",
      "    loss           : -1760.723525245591\n",
      "    val_loss       : -1732.1687099762262\n",
      "    val_log_likelihood: 1820.1511962890625\n",
      "    val_log_marginal: 1787.4364871573096\n",
      "Train Epoch: 970 [512/54000 (1%)] Loss: -1992.131592\n",
      "Train Epoch: 970 [11776/54000 (22%)] Loss: -1746.072510\n",
      "Train Epoch: 970 [23040/54000 (43%)] Loss: -1688.976685\n",
      "Train Epoch: 970 [34304/54000 (64%)] Loss: -1820.473267\n",
      "Train Epoch: 970 [45568/54000 (84%)] Loss: -1791.395630\n",
      "    epoch          : 970\n",
      "    loss           : -1767.518420531018\n",
      "    val_loss       : -1776.1808319007978\n",
      "    val_log_likelihood: 1820.3520385742188\n",
      "    val_log_marginal: 1787.6747934847\n",
      "Train Epoch: 971 [512/54000 (1%)] Loss: -1991.848267\n",
      "Train Epoch: 971 [11776/54000 (22%)] Loss: -1741.670898\n",
      "Train Epoch: 971 [23040/54000 (43%)] Loss: -1759.590088\n",
      "Train Epoch: 971 [34304/54000 (64%)] Loss: -1821.236572\n",
      "Train Epoch: 971 [45568/54000 (84%)] Loss: -1782.002930\n",
      "    epoch          : 971\n",
      "    loss           : -1772.5464217686417\n",
      "    val_loss       : -1777.380406596139\n",
      "    val_log_likelihood: 1820.243017578125\n",
      "    val_log_marginal: 1787.516354029253\n",
      "Train Epoch: 972 [512/54000 (1%)] Loss: -1938.441406\n",
      "Train Epoch: 972 [11776/54000 (22%)] Loss: -1713.742432\n",
      "Train Epoch: 972 [23040/54000 (43%)] Loss: -1825.282349\n",
      "Train Epoch: 972 [34304/54000 (64%)] Loss: -1757.786865\n",
      "Train Epoch: 972 [45568/54000 (84%)] Loss: -1787.671753\n",
      "    epoch          : 972\n",
      "    loss           : -1782.3591997505414\n",
      "    val_loss       : -1775.0597504498437\n",
      "    val_log_likelihood: 1820.4218383789062\n",
      "    val_log_marginal: 1787.7400100436062\n",
      "Train Epoch: 973 [512/54000 (1%)] Loss: -1994.732056\n",
      "Train Epoch: 973 [11776/54000 (22%)] Loss: -1625.454712\n",
      "Train Epoch: 973 [23040/54000 (43%)] Loss: -1821.077271\n",
      "Train Epoch: 973 [34304/54000 (64%)] Loss: -1704.926880\n",
      "Train Epoch: 973 [45568/54000 (84%)] Loss: -1779.614990\n",
      "    epoch          : 973\n",
      "    loss           : -1764.836887472927\n",
      "    val_loss       : -1786.9032188195736\n",
      "    val_log_likelihood: 1820.0659057617188\n",
      "    val_log_marginal: 1787.420161991194\n",
      "Train Epoch: 974 [512/54000 (1%)] Loss: -1991.365967\n",
      "Train Epoch: 974 [11776/54000 (22%)] Loss: -1851.119873\n",
      "Train Epoch: 974 [23040/54000 (43%)] Loss: -1822.628662\n",
      "Train Epoch: 974 [34304/54000 (64%)] Loss: -1534.424194\n",
      "Train Epoch: 974 [45568/54000 (84%)] Loss: -1599.961670\n",
      "    epoch          : 974\n",
      "    loss           : -1755.9443311030323\n",
      "    val_loss       : -1766.0032568285242\n",
      "    val_log_likelihood: 1820.1052856445312\n",
      "    val_log_marginal: 1787.484801027977\n",
      "Train Epoch: 975 [512/54000 (1%)] Loss: -1995.471924\n",
      "Train Epoch: 975 [11776/54000 (22%)] Loss: -1627.559082\n",
      "Train Epoch: 975 [23040/54000 (43%)] Loss: -1703.953369\n",
      "Train Epoch: 975 [34304/54000 (64%)] Loss: -1596.791992\n",
      "Train Epoch: 975 [45568/54000 (84%)] Loss: -1783.491821\n",
      "    epoch          : 975\n",
      "    loss           : -1759.8633537670173\n",
      "    val_loss       : -1760.6368692878634\n",
      "    val_log_likelihood: 1819.7645141601563\n",
      "    val_log_marginal: 1787.07412010098\n",
      "Train Epoch: 976 [512/54000 (1%)] Loss: -1857.140625\n",
      "Train Epoch: 976 [11776/54000 (22%)] Loss: -1744.462158\n",
      "Train Epoch: 976 [23040/54000 (43%)] Loss: -1705.126831\n",
      "Train Epoch: 976 [34304/54000 (64%)] Loss: -1706.839355\n",
      "Train Epoch: 976 [45568/54000 (84%)] Loss: -1676.510620\n",
      "    epoch          : 976\n",
      "    loss           : -1768.3970330870977\n",
      "    val_loss       : -1786.5088493814692\n",
      "    val_log_likelihood: 1819.8885498046875\n",
      "    val_log_marginal: 1787.1584280665963\n",
      "Train Epoch: 977 [512/54000 (1%)] Loss: -1936.519043\n",
      "Train Epoch: 977 [11776/54000 (22%)] Loss: -1783.764404\n",
      "Train Epoch: 977 [23040/54000 (43%)] Loss: -1711.417969\n",
      "Train Epoch: 977 [34304/54000 (64%)] Loss: -1784.493286\n",
      "Train Epoch: 977 [45568/54000 (84%)] Loss: -1818.754272\n",
      "    epoch          : 977\n",
      "    loss           : -1761.9640944074877\n",
      "    val_loss       : -1768.024866004847\n",
      "    val_log_likelihood: 1819.7758056640625\n",
      "    val_log_marginal: 1787.1647797662765\n",
      "Train Epoch: 978 [512/54000 (1%)] Loss: -1994.751953\n",
      "Train Epoch: 978 [11776/54000 (22%)] Loss: -1635.897461\n",
      "Train Epoch: 978 [23040/54000 (43%)] Loss: -1708.211182\n",
      "Train Epoch: 978 [34304/54000 (64%)] Loss: -1820.395874\n",
      "Train Epoch: 978 [45568/54000 (84%)] Loss: -1687.220459\n",
      "    epoch          : 978\n",
      "    loss           : -1772.7892026995669\n",
      "    val_loss       : -1781.2804905125872\n",
      "    val_log_likelihood: 1819.8716186523438\n",
      "    val_log_marginal: 1787.151733360864\n",
      "Train Epoch: 979 [512/54000 (1%)] Loss: -1992.495605\n",
      "Train Epoch: 979 [11776/54000 (22%)] Loss: -1747.484863\n",
      "Train Epoch: 979 [23040/54000 (43%)] Loss: -1824.174683\n",
      "Train Epoch: 979 [34304/54000 (64%)] Loss: -1780.174683\n",
      "Train Epoch: 979 [45568/54000 (84%)] Loss: -1672.029297\n",
      "    epoch          : 979\n",
      "    loss           : -1769.9283507696473\n",
      "    val_loss       : -1763.978102885559\n",
      "    val_log_likelihood: 1819.791357421875\n",
      "    val_log_marginal: 1787.1220754068345\n",
      "Train Epoch: 980 [512/54000 (1%)] Loss: -1994.002441\n",
      "Train Epoch: 980 [11776/54000 (22%)] Loss: -1850.858398\n",
      "Train Epoch: 980 [23040/54000 (43%)] Loss: -1786.559692\n",
      "Train Epoch: 980 [34304/54000 (64%)] Loss: -1825.728149\n",
      "Train Epoch: 980 [45568/54000 (84%)] Loss: -1757.207031\n",
      "    epoch          : 980\n",
      "    loss           : -1766.0467601813893\n",
      "    val_loss       : -1751.1026965215801\n",
      "    val_log_likelihood: 1819.9277221679688\n",
      "    val_log_marginal: 1787.2083126779646\n",
      "Train Epoch: 981 [512/54000 (1%)] Loss: -1989.786621\n",
      "Train Epoch: 981 [11776/54000 (22%)] Loss: -1822.460449\n",
      "Train Epoch: 981 [23040/54000 (43%)] Loss: -1819.492920\n",
      "Train Epoch: 981 [34304/54000 (64%)] Loss: -1668.785522\n",
      "Train Epoch: 981 [45568/54000 (84%)] Loss: -1780.892334\n",
      "    epoch          : 981\n",
      "    loss           : -1771.5694580078125\n",
      "    val_loss       : -1775.7643068753182\n",
      "    val_log_likelihood: 1820.3775146484375\n",
      "    val_log_marginal: 1787.7489932838828\n",
      "Train Epoch: 982 [512/54000 (1%)] Loss: -1786.611816\n",
      "Train Epoch: 982 [11776/54000 (22%)] Loss: -1630.841675\n",
      "Train Epoch: 982 [23040/54000 (43%)] Loss: -1664.083008\n",
      "Train Epoch: 982 [34304/54000 (64%)] Loss: -1817.757568\n",
      "Train Epoch: 982 [45568/54000 (84%)] Loss: -1658.939819\n",
      "    epoch          : 982\n",
      "    loss           : -1761.2438831895886\n",
      "    val_loss       : -1711.968826714158\n",
      "    val_log_likelihood: 1820.7742065429688\n",
      "    val_log_marginal: 1788.1239905581265\n",
      "Train Epoch: 983 [512/54000 (1%)] Loss: -1990.655029\n",
      "Train Epoch: 983 [11776/54000 (22%)] Loss: -1630.510620\n",
      "Train Epoch: 983 [23040/54000 (43%)] Loss: -1386.491821\n",
      "Train Epoch: 983 [34304/54000 (64%)] Loss: -1788.783569\n",
      "Train Epoch: 983 [45568/54000 (84%)] Loss: -1673.559326\n",
      "    epoch          : 983\n",
      "    loss           : -1763.316353070854\n",
      "    val_loss       : -1765.152342877537\n",
      "    val_log_likelihood: 1820.4756103515624\n",
      "    val_log_marginal: 1787.8164811786264\n",
      "Train Epoch: 984 [512/54000 (1%)] Loss: -1992.228394\n",
      "Train Epoch: 984 [11776/54000 (22%)] Loss: -1383.816284\n",
      "Train Epoch: 984 [23040/54000 (43%)] Loss: -1758.332031\n",
      "Train Epoch: 984 [34304/54000 (64%)] Loss: -1603.768311\n",
      "Train Epoch: 984 [45568/54000 (84%)] Loss: -1781.793457\n",
      "    epoch          : 984\n",
      "    loss           : -1762.1558656598081\n",
      "    val_loss       : -1760.4100763380527\n",
      "    val_log_likelihood: 1820.7314453125\n",
      "    val_log_marginal: 1788.044971980527\n",
      "Train Epoch: 985 [512/54000 (1%)] Loss: -1748.242920\n",
      "Train Epoch: 985 [11776/54000 (22%)] Loss: -1860.222290\n",
      "Train Epoch: 985 [23040/54000 (43%)] Loss: -1783.727661\n",
      "Train Epoch: 985 [34304/54000 (64%)] Loss: -1755.677979\n",
      "Train Epoch: 985 [45568/54000 (84%)] Loss: -1779.598145\n",
      "    epoch          : 985\n",
      "    loss           : -1769.570910765393\n",
      "    val_loss       : -1777.582050475292\n",
      "    val_log_likelihood: 1820.8215576171874\n",
      "    val_log_marginal: 1788.1332556132227\n",
      "Train Epoch: 986 [512/54000 (1%)] Loss: -1992.683105\n",
      "Train Epoch: 986 [11776/54000 (22%)] Loss: -1702.206787\n",
      "Train Epoch: 986 [23040/54000 (43%)] Loss: -1784.826660\n",
      "Train Epoch: 986 [34304/54000 (64%)] Loss: -1757.068970\n",
      "Train Epoch: 986 [45568/54000 (84%)] Loss: -1780.764893\n",
      "    epoch          : 986\n",
      "    loss           : -1766.0218252049815\n",
      "    val_loss       : -1778.4506487902254\n",
      "    val_log_likelihood: 1820.6802490234375\n",
      "    val_log_marginal: 1787.9357066592536\n",
      "Train Epoch: 987 [512/54000 (1%)] Loss: -1988.982910\n",
      "Train Epoch: 987 [11776/54000 (22%)] Loss: -1707.841553\n",
      "Train Epoch: 987 [23040/54000 (43%)] Loss: -1732.783569\n",
      "Train Epoch: 987 [34304/54000 (64%)] Loss: -1572.960083\n",
      "Train Epoch: 987 [45568/54000 (84%)] Loss: -1696.014038\n",
      "    epoch          : 987\n",
      "    loss           : -1764.9335732035117\n",
      "    val_loss       : -1759.6117921162397\n",
      "    val_log_likelihood: 1820.7445556640625\n",
      "    val_log_marginal: 1788.0757331797352\n",
      "Train Epoch: 988 [512/54000 (1%)] Loss: -1687.083618\n",
      "Train Epoch: 988 [11776/54000 (22%)] Loss: -1744.435059\n",
      "Train Epoch: 988 [23040/54000 (43%)] Loss: -1821.741089\n",
      "Train Epoch: 988 [34304/54000 (64%)] Loss: -1766.926514\n",
      "Train Epoch: 988 [45568/54000 (84%)] Loss: -1688.692017\n",
      "    epoch          : 988\n",
      "    loss           : -1762.2030863242574\n",
      "    val_loss       : -1776.985993182659\n",
      "    val_log_likelihood: 1820.968603515625\n",
      "    val_log_marginal: 1788.257429822162\n",
      "Train Epoch: 989 [512/54000 (1%)] Loss: -1994.793945\n",
      "Train Epoch: 989 [11776/54000 (22%)] Loss: -1860.181519\n",
      "Train Epoch: 989 [23040/54000 (43%)] Loss: -1823.632080\n",
      "Train Epoch: 989 [34304/54000 (64%)] Loss: -1699.727173\n",
      "Train Epoch: 989 [45568/54000 (84%)] Loss: -1782.306396\n",
      "    epoch          : 989\n",
      "    loss           : -1767.2392493521813\n",
      "    val_loss       : -1741.7667679572478\n",
      "    val_log_likelihood: 1820.4619262695312\n",
      "    val_log_marginal: 1787.755856890179\n",
      "Train Epoch: 990 [512/54000 (1%)] Loss: -1933.131592\n",
      "Train Epoch: 990 [11776/54000 (22%)] Loss: -1739.044434\n",
      "Train Epoch: 990 [23040/54000 (43%)] Loss: -1687.303955\n",
      "Train Epoch: 990 [34304/54000 (64%)] Loss: -1667.719849\n",
      "Train Epoch: 990 [45568/54000 (84%)] Loss: -1787.192017\n",
      "    epoch          : 990\n",
      "    loss           : -1769.1747418394184\n",
      "    val_loss       : -1767.0543595079332\n",
      "    val_log_likelihood: 1820.4160766601562\n",
      "    val_log_marginal: 1787.6719799240461\n",
      "Train Epoch: 991 [512/54000 (1%)] Loss: -1938.155518\n",
      "Train Epoch: 991 [11776/54000 (22%)] Loss: -1848.927002\n",
      "Train Epoch: 991 [23040/54000 (43%)] Loss: -1760.516357\n",
      "Train Epoch: 991 [34304/54000 (64%)] Loss: -1939.167725\n",
      "Train Epoch: 991 [45568/54000 (84%)] Loss: -1786.890625\n",
      "    epoch          : 991\n",
      "    loss           : -1768.806477461711\n",
      "    val_loss       : -1748.7470598980785\n",
      "    val_log_likelihood: 1820.1210571289062\n",
      "    val_log_marginal: 1787.475791570708\n",
      "Train Epoch: 992 [512/54000 (1%)] Loss: -1992.525879\n",
      "Train Epoch: 992 [11776/54000 (22%)] Loss: -1745.352539\n",
      "Train Epoch: 992 [23040/54000 (43%)] Loss: -1756.670776\n",
      "Train Epoch: 992 [34304/54000 (64%)] Loss: -1787.006470\n",
      "Train Epoch: 992 [45568/54000 (84%)] Loss: -1782.188110\n",
      "    epoch          : 992\n",
      "    loss           : -1765.8756647393254\n",
      "    val_loss       : -1778.5874809829518\n",
      "    val_log_likelihood: 1821.4986938476563\n",
      "    val_log_marginal: 1788.8124087434262\n",
      "Train Epoch: 993 [512/54000 (1%)] Loss: -1989.181763\n",
      "Train Epoch: 993 [11776/54000 (22%)] Loss: -1859.388306\n",
      "Train Epoch: 993 [23040/54000 (43%)] Loss: -1821.559204\n",
      "Train Epoch: 993 [34304/54000 (64%)] Loss: -1764.645874\n",
      "Train Epoch: 993 [45568/54000 (84%)] Loss: -1781.986450\n",
      "    epoch          : 993\n",
      "    loss           : -1763.7808753287438\n",
      "    val_loss       : -1775.9697024658321\n",
      "    val_log_likelihood: 1821.5845825195313\n",
      "    val_log_marginal: 1788.924413079396\n",
      "Train Epoch: 994 [512/54000 (1%)] Loss: -1992.324707\n",
      "Train Epoch: 994 [11776/54000 (22%)] Loss: -1851.305908\n",
      "Train Epoch: 994 [23040/54000 (43%)] Loss: -1681.156128\n",
      "Train Epoch: 994 [34304/54000 (64%)] Loss: -1990.805664\n",
      "Train Epoch: 994 [45568/54000 (84%)] Loss: -1680.115967\n",
      "    epoch          : 994\n",
      "    loss           : -1771.2036773379486\n",
      "    val_loss       : -1763.5586172604933\n",
      "    val_log_likelihood: 1820.0871826171874\n",
      "    val_log_marginal: 1787.4078751277236\n",
      "Train Epoch: 995 [512/54000 (1%)] Loss: -1937.917725\n",
      "Train Epoch: 995 [11776/54000 (22%)] Loss: -1855.833862\n",
      "Train Epoch: 995 [23040/54000 (43%)] Loss: -1692.961670\n",
      "Train Epoch: 995 [34304/54000 (64%)] Loss: -1785.754272\n",
      "Train Epoch: 995 [45568/54000 (84%)] Loss: -1679.398193\n",
      "    epoch          : 995\n",
      "    loss           : -1768.7212965559252\n",
      "    val_loss       : -1755.6346054255962\n",
      "    val_log_likelihood: 1820.8063232421875\n",
      "    val_log_marginal: 1788.0399868737907\n",
      "Train Epoch: 996 [512/54000 (1%)] Loss: -1994.557129\n",
      "Train Epoch: 996 [11776/54000 (22%)] Loss: -1748.188721\n",
      "Train Epoch: 996 [23040/54000 (43%)] Loss: -1726.790405\n",
      "Train Epoch: 996 [34304/54000 (64%)] Loss: -1760.420654\n",
      "Train Epoch: 996 [45568/54000 (84%)] Loss: -1787.781738\n",
      "    epoch          : 996\n",
      "    loss           : -1775.3178505472617\n",
      "    val_loss       : -1752.9278926102445\n",
      "    val_log_likelihood: 1820.482421875\n",
      "    val_log_marginal: 1787.7530026357622\n",
      "Train Epoch: 997 [512/54000 (1%)] Loss: -1993.361328\n",
      "Train Epoch: 997 [11776/54000 (22%)] Loss: -1740.890137\n",
      "Train Epoch: 997 [23040/54000 (43%)] Loss: -1606.460449\n",
      "Train Epoch: 997 [34304/54000 (64%)] Loss: -1686.881348\n",
      "Train Epoch: 997 [45568/54000 (84%)] Loss: -1786.890503\n",
      "    epoch          : 997\n",
      "    loss           : -1771.3281104965965\n",
      "    val_loss       : -1786.566297726892\n",
      "    val_log_likelihood: 1819.9006591796874\n",
      "    val_log_marginal: 1787.160321142152\n",
      "Train Epoch: 998 [512/54000 (1%)] Loss: -1993.691162\n",
      "Train Epoch: 998 [11776/54000 (22%)] Loss: -1741.519165\n",
      "Train Epoch: 998 [23040/54000 (43%)] Loss: -1788.705566\n",
      "Train Epoch: 998 [34304/54000 (64%)] Loss: -1783.104004\n",
      "Train Epoch: 998 [45568/54000 (84%)] Loss: -1669.968384\n",
      "    epoch          : 998\n",
      "    loss           : -1765.238739315826\n",
      "    val_loss       : -1764.9354699358344\n",
      "    val_log_likelihood: 1820.3379638671875\n",
      "    val_log_marginal: 1787.6621943298735\n",
      "Train Epoch: 999 [512/54000 (1%)] Loss: -1993.830200\n",
      "Train Epoch: 999 [11776/54000 (22%)] Loss: -1743.154175\n",
      "Train Epoch: 999 [23040/54000 (43%)] Loss: -1705.300537\n",
      "Train Epoch: 999 [34304/54000 (64%)] Loss: -1785.150269\n",
      "Train Epoch: 999 [45568/54000 (84%)] Loss: -1826.398926\n",
      "    epoch          : 999\n",
      "    loss           : -1758.7305255549968\n",
      "    val_loss       : -1776.644130582176\n",
      "    val_log_likelihood: 1820.8409423828125\n",
      "    val_log_marginal: 1788.0633809093385\n",
      "Train Epoch: 1000 [512/54000 (1%)] Loss: -1938.612549\n",
      "Train Epoch: 1000 [11776/54000 (22%)] Loss: -1855.608643\n",
      "Train Epoch: 1000 [23040/54000 (43%)] Loss: -1817.410522\n",
      "Train Epoch: 1000 [34304/54000 (64%)] Loss: -1757.309692\n",
      "Train Epoch: 1000 [45568/54000 (84%)] Loss: -1786.560913\n",
      "    epoch          : 1000\n",
      "    loss           : -1769.348824982596\n",
      "    val_loss       : -1777.0262670112775\n",
      "    val_log_likelihood: 1821.9947509765625\n",
      "    val_log_marginal: 1789.2292705703499\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1007_131734/checkpoint-epoch1000.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VaeOperadicModel(\n",
       "  (_category): FreeOperad(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=12, bias=True)\n",
       "        (1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (4): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=12, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=20, bias=True)\n",
       "        (1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=20, out_features=20, bias=True)\n",
       "        (4): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=20, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=36, bias=True)\n",
       "        (1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (4): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=36, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=68, bias=True)\n",
       "        (1): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=68, out_features=68, bias=True)\n",
       "        (4): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=68, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=102, bias=True)\n",
       "        (1): LayerNorm((102,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=102, out_features=102, bias=True)\n",
       "        (4): LayerNorm((102,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=102, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=106, bias=True)\n",
       "        (1): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=106, out_features=106, bias=True)\n",
       "        (4): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=106, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_10): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_11): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_12): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_13): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=114, bias=True)\n",
       "        (1): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=114, out_features=114, bias=True)\n",
       "        (4): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=114, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_14): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_15): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_16): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=130, bias=True)\n",
       "        (1): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=130, out_features=130, bias=True)\n",
       "        (4): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=130, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_17): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_18): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=162, bias=True)\n",
       "        (1): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=162, out_features=162, bias=True)\n",
       "        (4): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=162, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_19): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_20): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "    (global_element_5): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=54, bias=True)\n",
       "  )\n",
       "  (encoders): ModuleDict(\n",
       "    ($p(Z^{16} | \\mathbb{R}^{8})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (3): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=48, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{32} | \\mathbb{R}^{8})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (3): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=96, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{64} | \\mathbb{R}^{8})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (3): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=192, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{128} | \\mathbb{R}^{8})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=384, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | \\mathbb{R}^{8})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | \\mathbb{R}^{8})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{32} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (3): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=96, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{64} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (3): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=192, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{128} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=384, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{64} | \\mathbb{R}^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (3): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=192, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{128} | \\mathbb{R}^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=384, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | \\mathbb{R}^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | \\mathbb{R}^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{128} | \\mathbb{R}^{64})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=384, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | \\mathbb{R}^{64})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | \\mathbb{R}^{64})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{196} | \\mathbb{R}^{128})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | \\mathbb{R}^{128})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(X^{784} | \\mathbb{R}^{196})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{128})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{32})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{64})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{16})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{8})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    path.draw()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:categorical_bpl] *",
   "language": "python",
   "name": "conda-env-categorical_bpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
