{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"cooldown\": 25,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 703871.312500\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -212725.093750\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -54967.796875\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -273825.625000\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -192979.859375\n",
      "    epoch          : 1\n",
      "    loss           : -59106.028571704825\n",
      "    val_loss       : -140388.07201846837\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -341509.437500\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -171554.218750\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -167488.437500\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -213616.000000\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -185360.203125\n",
      "    epoch          : 2\n",
      "    loss           : -177092.83857711943\n",
      "    val_loss       : -185944.67962594033\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -428281.781250\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -232003.015625\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -234285.312500\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -406642.187500\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -269033.093750\n",
      "    epoch          : 3\n",
      "    loss           : -209206.23781230662\n",
      "    val_loss       : -235787.86292819976\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -425555.406250\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -244503.218750\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -283833.500000\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -103496.000000\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -269348.125000\n",
      "    epoch          : 4\n",
      "    loss           : -244650.19527382427\n",
      "    val_loss       : -255907.4877900362\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -436462.656250\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -339563.500000\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -321299.218750\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -56740.839844\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -312098.687500\n",
      "    epoch          : 5\n",
      "    loss           : -274858.3908957302\n",
      "    val_loss       : -291964.8794703007\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -507792.968750\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -307038.593750\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -214636.218750\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -224374.984375\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -253460.000000\n",
      "    epoch          : 6\n",
      "    loss           : -276201.50224319304\n",
      "    val_loss       : -248971.4043697834\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -429149.937500\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -214225.093750\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -107791.226562\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -311437.656250\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -285051.531250\n",
      "    epoch          : 7\n",
      "    loss           : -272121.4296875\n",
      "    val_loss       : -283372.3251935482\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -470001.968750\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -132596.218750\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -235472.328125\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -513732.812500\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -287952.312500\n",
      "    epoch          : 8\n",
      "    loss           : -291015.8899288366\n",
      "    val_loss       : -305478.18712444307\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -488862.187500\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -250653.046875\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -254719.125000\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -332516.468750\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -329933.000000\n",
      "    epoch          : 9\n",
      "    loss           : -314659.25278465345\n",
      "    val_loss       : -328941.5342795372\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -527293.125000\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -317559.281250\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -329660.687500\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -298663.250000\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -350250.625000\n",
      "    epoch          : 10\n",
      "    loss           : -331897.97137995047\n",
      "    val_loss       : -343693.17810645106\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -564448.000000\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -369161.593750\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -331866.968750\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -389012.000000\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -347854.875000\n",
      "    epoch          : 11\n",
      "    loss           : -342019.97849628713\n",
      "    val_loss       : -353797.226491785\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -547365.750000\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -301902.218750\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -323252.437500\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -124730.187500\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -413988.250000\n",
      "    epoch          : 12\n",
      "    loss           : -373121.6557858911\n",
      "    val_loss       : -404293.04378018377\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -522284.375000\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -499058.937500\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -460420.406250\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -618401.312500\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -588828.750000\n",
      "    epoch          : 13\n",
      "    loss           : -427960.8244121287\n",
      "    val_loss       : -481511.4790437698\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -725676.937500\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -505798.312500\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -369192.250000\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -435269.250000\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -506261.187500\n",
      "    epoch          : 14\n",
      "    loss           : -496180.77065284655\n",
      "    val_loss       : -539039.9286015034\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -621287.500000\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -536251.625000\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -655878.500000\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -546508.125000\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -578484.750000\n",
      "    epoch          : 15\n",
      "    loss           : -540931.6908261139\n",
      "    val_loss       : -555655.5316116571\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -722905.187500\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -634705.500000\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -589096.375000\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -97521.484375\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -669892.812500\n",
      "    epoch          : 16\n",
      "    loss           : -545493.1110767326\n",
      "    val_loss       : -523471.51489214896\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -568533.000000\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -591706.687500\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -378895.437500\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -623050.687500\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -567796.250000\n",
      "    epoch          : 17\n",
      "    loss           : -546857.6805383663\n",
      "    val_loss       : -587266.5976728201\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -695171.875000\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -748118.562500\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -190443.953125\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -685898.937500\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -489956.375000\n",
      "    epoch          : 18\n",
      "    loss           : -549774.4625618812\n",
      "    val_loss       : -541264.1707055091\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -802490.062500\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -423925.718750\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -656282.312500\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -603535.375000\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -688216.437500\n",
      "    epoch          : 19\n",
      "    loss           : -558153.7168935643\n",
      "    val_loss       : -567567.829505682\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -756223.562500\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -507028.312500\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -464739.562500\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -458395.250000\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -567707.375000\n",
      "    epoch          : 20\n",
      "    loss           : -566313.0095915842\n",
      "    val_loss       : -555569.0122313022\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -866443.750000\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -601061.187500\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -434307.687500\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -629956.500000\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -573525.062500\n",
      "    epoch          : 21\n",
      "    loss           : -571253.1704826732\n",
      "    val_loss       : -564174.1838326931\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -811523.937500\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -359260.875000\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -689869.312500\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -807357.687500\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -544688.437500\n",
      "    epoch          : 22\n",
      "    loss           : -572619.1630569306\n",
      "    val_loss       : -595201.6609445096\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -755540.000000\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -531986.812500\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -719109.937500\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -701819.812500\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -696062.625000\n",
      "    epoch          : 23\n",
      "    loss           : -590543.8582920792\n",
      "    val_loss       : -580323.2315436124\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -762690.625000\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -576834.687500\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -296233.968750\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -438839.093750\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -505874.937500\n",
      "    epoch          : 24\n",
      "    loss           : -577596.0049504951\n",
      "    val_loss       : -662991.7638050556\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -721726.062500\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -537759.062500\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -588230.375000\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -746753.375000\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -673979.562500\n",
      "    epoch          : 25\n",
      "    loss           : -592039.937190594\n",
      "    val_loss       : -618946.1494765043\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -642458.500000\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -610508.750000\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -649931.750000\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -638494.437500\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -335279.593750\n",
      "    epoch          : 26\n",
      "    loss           : -603143.3592202971\n",
      "    val_loss       : -592339.243572855\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -904648.250000\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -516526.562500\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -544251.187500\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -782511.625000\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -566302.250000\n",
      "    epoch          : 27\n",
      "    loss           : -600550.1853341584\n",
      "    val_loss       : -633043.4514030457\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -790952.062500\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -595928.875000\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -454739.000000\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -569550.437500\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -678188.187500\n",
      "    epoch          : 28\n",
      "    loss           : -581599.4719987623\n",
      "    val_loss       : -575451.1895509005\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -548173.000000\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -510046.906250\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -646930.875000\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -637057.250000\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -697827.750000\n",
      "    epoch          : 29\n",
      "    loss           : -574666.6998762377\n",
      "    val_loss       : -575309.1921642304\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -684238.625000\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -637402.250000\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -407202.937500\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -678394.437500\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -615301.250000\n",
      "    epoch          : 30\n",
      "    loss           : -607363.5182549505\n",
      "    val_loss       : -617159.5292431355\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -809763.000000\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -739279.000000\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -617267.937500\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -661622.625000\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -550839.125000\n",
      "    epoch          : 31\n",
      "    loss           : -607419.6243811881\n",
      "    val_loss       : -584440.6488516808\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -895990.125000\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -581229.500000\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -329083.281250\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -737955.375000\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -685506.500000\n",
      "    epoch          : 32\n",
      "    loss           : -611723.2212252475\n",
      "    val_loss       : -607586.0046712875\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -785064.875000\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -712524.625000\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -586139.375000\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -485545.093750\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -647321.937500\n",
      "    epoch          : 33\n",
      "    loss           : -625941.6150990099\n",
      "    val_loss       : -612873.9853905797\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -845972.875000\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -505025.468750\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -618254.250000\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -811734.812500\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -678751.750000\n",
      "    epoch          : 34\n",
      "    loss           : -623466.8363242574\n",
      "    val_loss       : -612982.8910223484\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -790878.250000\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -453427.875000\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -617020.125000\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -659368.500000\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -628838.500000\n",
      "    epoch          : 35\n",
      "    loss           : -625675.7348391089\n",
      "    val_loss       : -652475.494073391\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -865383.625000\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -580216.187500\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -702867.500000\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -752456.562500\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -578942.000000\n",
      "    epoch          : 36\n",
      "    loss           : -641629.573019802\n",
      "    val_loss       : -667218.558338213\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -832983.125000\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -544564.312500\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -552302.812500\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -710297.250000\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -711406.625000\n",
      "    epoch          : 37\n",
      "    loss           : -641787.3715965346\n",
      "    val_loss       : -613751.5781243325\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -930033.875000\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -803147.625000\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -680768.812500\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -800701.000000\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -738555.625000\n",
      "    epoch          : 38\n",
      "    loss           : -649931.1178836634\n",
      "    val_loss       : -656868.1362439633\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -800570.125000\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -693582.500000\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -422175.250000\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -871119.250000\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -506284.750000\n",
      "    epoch          : 39\n",
      "    loss           : -660306.9947400991\n",
      "    val_loss       : -689185.2833043098\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -799955.750000\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -590917.250000\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -626575.875000\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -620762.000000\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -525019.937500\n",
      "    epoch          : 40\n",
      "    loss           : -674470.5974628713\n",
      "    val_loss       : -603500.5017865658\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -895285.500000\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -423269.562500\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -507850.625000\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -717112.125000\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -687706.000000\n",
      "    epoch          : 41\n",
      "    loss           : -671438.4993811881\n",
      "    val_loss       : -678164.0339787245\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -712616.875000\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -619774.000000\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -652156.812500\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -678896.562500\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -665196.062500\n",
      "    epoch          : 42\n",
      "    loss           : -671475.0705445545\n",
      "    val_loss       : -675488.5528084517\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -861083.875000\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -633219.437500\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -579559.000000\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -682537.375000\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -685058.750000\n",
      "    epoch          : 43\n",
      "    loss           : -671829.1531559406\n",
      "    val_loss       : -641997.7879089832\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -783975.437500\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -696766.750000\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -661073.000000\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -729809.125000\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -706447.062500\n",
      "    epoch          : 44\n",
      "    loss           : -666977.9817450495\n",
      "    val_loss       : -685743.0799053669\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -917569.250000\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -780582.875000\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -616126.750000\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -719918.187500\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -755246.312500\n",
      "    epoch          : 45\n",
      "    loss           : -682688.7691831683\n",
      "    val_loss       : -684564.0857104778\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -835587.000000\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -588771.125000\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -753143.375000\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -723857.187500\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -699676.750000\n",
      "    epoch          : 46\n",
      "    loss           : -687129.2880569306\n",
      "    val_loss       : -678954.0815978765\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -745838.875000\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -725494.562500\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -676286.500000\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -596033.125000\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -757680.750000\n",
      "    epoch          : 47\n",
      "    loss           : -677703.6228341584\n",
      "    val_loss       : -690603.206096375\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -878949.750000\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -679749.750000\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -462049.187500\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -718591.375000\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -613995.125000\n",
      "    epoch          : 48\n",
      "    loss           : -682316.0525990099\n",
      "    val_loss       : -712201.6513752223\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -871629.812500\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -718043.375000\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -574300.187500\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -676338.125000\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -745454.187500\n",
      "    epoch          : 49\n",
      "    loss           : -691702.8196163366\n",
      "    val_loss       : -704867.7116946459\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -923295.312500\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -765961.312500\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -673440.375000\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -721792.937500\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -621507.062500\n",
      "    epoch          : 50\n",
      "    loss           : -693925.3712871287\n",
      "    val_loss       : -639320.1314687729\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0327_104948/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -852703.375000\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -645002.250000\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -677196.625000\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -751220.000000\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -713459.312500\n",
      "    epoch          : 51\n",
      "    loss           : -684785.8400371287\n",
      "    val_loss       : -669853.7467621565\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -791952.750000\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -705454.187500\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -727221.187500\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -700612.562500\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -709328.125000\n",
      "    epoch          : 52\n",
      "    loss           : -679534.3729888614\n",
      "    val_loss       : -690660.6951121092\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -917674.062500\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -804798.625000\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -775023.375000\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -496362.968750\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -788135.375000\n",
      "    epoch          : 53\n",
      "    loss           : -708468.7781559406\n",
      "    val_loss       : -690148.8736141205\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -711885.250000\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -736255.062500\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -760938.125000\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -747014.875000\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -708660.500000\n",
      "    epoch          : 54\n",
      "    loss           : -709646.0470297029\n",
      "    val_loss       : -716860.5984999419\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -937747.687500\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -704941.000000\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -777590.312500\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -677590.750000\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -635939.562500\n",
      "    epoch          : 55\n",
      "    loss           : -709040.4195544554\n",
      "    val_loss       : -742037.0439400912\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -795181.125000\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -746753.562500\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -596518.750000\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -811217.437500\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -667865.875000\n",
      "    epoch          : 56\n",
      "    loss           : -707107.8712871287\n",
      "    val_loss       : -711508.0908038855\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -936505.562500\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -687416.125000\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -643535.875000\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -513365.343750\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -674475.875000\n",
      "    epoch          : 57\n",
      "    loss           : -690622.9245049505\n",
      "    val_loss       : -700577.7367937088\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -761869.250000\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -702721.625000\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -394187.500000\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -895491.875000\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -704835.000000\n",
      "    epoch          : 58\n",
      "    loss           : -708547.1806930694\n",
      "    val_loss       : -707383.4900288343\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -924866.250000\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -617646.187500\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -782596.437500\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -632274.750000\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -765296.562500\n",
      "    epoch          : 59\n",
      "    loss           : -714118.1865717822\n",
      "    val_loss       : -727664.4895351648\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -883499.125000\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -685884.000000\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -618926.937500\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -686756.000000\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -617826.187500\n",
      "    epoch          : 60\n",
      "    loss           : -707484.8991336634\n",
      "    val_loss       : -701336.5757712603\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -922489.750000\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -700179.500000\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -566709.687500\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -775346.125000\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -540385.187500\n",
      "    epoch          : 61\n",
      "    loss           : -698255.176670792\n",
      "    val_loss       : -741123.714051795\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -873382.750000\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -817853.437500\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -709717.500000\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -657143.375000\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -682519.187500\n",
      "    epoch          : 62\n",
      "    loss           : -707323.4805074257\n",
      "    val_loss       : -725530.4514618159\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -718719.875000\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -609719.687500\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -796710.250000\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -777076.187500\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -719434.375000\n",
      "    epoch          : 63\n",
      "    loss           : -714224.2286509901\n",
      "    val_loss       : -712605.0371660471\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -446609.687500\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -711925.187500\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -734627.875000\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -733976.750000\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -757245.687500\n",
      "    epoch          : 64\n",
      "    loss           : -712609.7354579208\n",
      "    val_loss       : -726462.9951853752\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -877394.125000\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -704942.437500\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -714436.625000\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -728736.812500\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -759983.750000\n",
      "    epoch          : 65\n",
      "    loss           : -713840.6720297029\n",
      "    val_loss       : -701175.1206192493\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -914187.562500\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -783069.125000\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -688651.750000\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -568464.250000\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -761964.375000\n",
      "    epoch          : 66\n",
      "    loss           : -712927.1138613861\n",
      "    val_loss       : -705511.5098423004\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -896115.062500\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -546684.937500\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -661353.187500\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -918444.250000\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -762848.250000\n",
      "    epoch          : 67\n",
      "    loss           : -703179.4885519802\n",
      "    val_loss       : -699676.0658677578\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -930407.625000\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -694523.937500\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -615919.500000\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -684769.375000\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -714229.875000\n",
      "    epoch          : 68\n",
      "    loss           : -707455.5488861386\n",
      "    val_loss       : -703192.0249426604\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -781213.812500\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -725078.000000\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -767843.875000\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -697118.750000\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -558976.937500\n",
      "    epoch          : 69\n",
      "    loss           : -710649.9863861386\n",
      "    val_loss       : -706371.9338161468\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -592521.750000\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -819792.437500\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -790150.125000\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -709197.875000\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -741571.000000\n",
      "    epoch          : 70\n",
      "    loss           : -705117.2988861386\n",
      "    val_loss       : -720877.118588233\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -898316.625000\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -724120.125000\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -595885.875000\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -709191.125000\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -708761.000000\n",
      "    epoch          : 71\n",
      "    loss           : -717146.7413366337\n",
      "    val_loss       : -737856.6980412721\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -917038.000000\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -626442.125000\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -775715.125000\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -581734.312500\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -384218.062500\n",
      "    epoch          : 72\n",
      "    loss           : -721744.8688118812\n",
      "    val_loss       : -735517.8363537074\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -955071.000000\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -712475.750000\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -692685.562500\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -670067.125000\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -760942.625000\n",
      "    epoch          : 73\n",
      "    loss           : -722505.3913985149\n",
      "    val_loss       : -715443.2478628159\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -771754.875000\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -733494.000000\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -689990.375000\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -578265.062500\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -734786.375000\n",
      "    epoch          : 74\n",
      "    loss           : -716316.2283415842\n",
      "    val_loss       : -742895.361830616\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -937721.750000\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -636409.437500\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -764265.000000\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -913248.062500\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -781752.500000\n",
      "    epoch          : 75\n",
      "    loss           : -721497.0581683168\n",
      "    val_loss       : -744847.0722510815\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -925066.375000\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -733918.875000\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -748654.500000\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -710848.125000\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -677302.250000\n",
      "    epoch          : 76\n",
      "    loss           : -729412.5290841584\n",
      "    val_loss       : -727928.4191419601\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -817087.062500\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -763733.375000\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -598700.687500\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -638385.375000\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -582299.062500\n",
      "    epoch          : 77\n",
      "    loss           : -737866.3168316832\n",
      "    val_loss       : -679983.8093990565\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -579136.875000\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -841629.875000\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -660615.562500\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -647990.687500\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -574758.312500\n",
      "    epoch          : 78\n",
      "    loss           : -722850.0569306931\n",
      "    val_loss       : -734551.3855908632\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -901625.625000\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -752362.687500\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -733256.500000\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -692137.812500\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -756941.750000\n",
      "    epoch          : 79\n",
      "    loss           : -733104.5139232674\n",
      "    val_loss       : -683861.7688049317\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -809269.000000\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -393026.656250\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -783712.250000\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -776734.312500\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -616094.750000\n",
      "    epoch          : 80\n",
      "    loss           : -713527.8814975248\n",
      "    val_loss       : -747061.7743431568\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -796917.750000\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -752677.937500\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -618572.562500\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -796817.750000\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -738507.125000\n",
      "    epoch          : 81\n",
      "    loss           : -731984.4699876237\n",
      "    val_loss       : -722981.618575406\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -921391.875000\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -696459.000000\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -738123.437500\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -695206.625000\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -771930.000000\n",
      "    epoch          : 82\n",
      "    loss           : -734291.2772277228\n",
      "    val_loss       : -734496.436559105\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -899580.875000\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -692676.625000\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -790632.375000\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -883827.375000\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -596718.125000\n",
      "    epoch          : 83\n",
      "    loss           : -724410.5238242574\n",
      "    val_loss       : -734405.6674966097\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -745259.250000\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -782412.250000\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -463211.750000\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -644986.000000\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -756228.375000\n",
      "    epoch          : 84\n",
      "    loss           : -707082.5024752475\n",
      "    val_loss       : -747411.2391289711\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -911976.062500\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -896725.500000\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -665660.125000\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -600914.875000\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -748237.125000\n",
      "    epoch          : 85\n",
      "    loss           : -726161.6921410891\n",
      "    val_loss       : -697996.5913999795\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -900091.937500\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -730227.875000\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -791584.062500\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -919161.187500\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -725426.375000\n",
      "    epoch          : 86\n",
      "    loss           : -732083.8258044554\n",
      "    val_loss       : -741551.3019782543\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -826755.187500\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -723678.187500\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -688879.562500\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -776176.000000\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -769092.875000\n",
      "    epoch          : 87\n",
      "    loss           : -739244.6197400991\n",
      "    val_loss       : -737713.7589121342\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -918689.687500\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -815185.250000\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -676709.625000\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -809053.250000\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -779282.062500\n",
      "    epoch          : 88\n",
      "    loss           : -741350.6577970297\n",
      "    val_loss       : -765351.3655748845\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -914846.375000\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -823722.625000\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -617414.687500\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -935764.125000\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -789683.375000\n",
      "    epoch          : 89\n",
      "    loss           : -724284.6865717822\n",
      "    val_loss       : -754846.8452586889\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -943255.000000\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -772253.125000\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -766589.187500\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -674508.125000\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -693071.750000\n",
      "    epoch          : 90\n",
      "    loss           : -732107.3886138614\n",
      "    val_loss       : -722741.3565215111\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -585656.750000\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -804397.312500\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -761315.562500\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -478578.937500\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -766467.437500\n",
      "    epoch          : 91\n",
      "    loss           : -741891.2060643565\n",
      "    val_loss       : -752897.7419863224\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -903263.812500\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -637709.562500\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -777238.875000\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -786898.875000\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -788702.437500\n",
      "    epoch          : 92\n",
      "    loss           : -746705.1738861386\n",
      "    val_loss       : -734676.4948135614\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -794180.937500\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -775388.125000\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -754052.187500\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -761259.187500\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -800035.812500\n",
      "    epoch          : 93\n",
      "    loss           : -749675.1101485149\n",
      "    val_loss       : -774470.2732605934\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -836227.187500\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -745308.250000\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -704884.875000\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -709894.812500\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -761415.375000\n",
      "    epoch          : 94\n",
      "    loss           : -746397.0965346535\n",
      "    val_loss       : -753844.7640722513\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -937660.562500\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -784281.125000\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -736428.750000\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -921132.750000\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -567259.437500\n",
      "    epoch          : 95\n",
      "    loss           : -742998.9529702971\n",
      "    val_loss       : -751692.9065707207\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -651768.687500\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -801410.875000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -762977.250000\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -715006.562500\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -789122.125000\n",
      "    epoch          : 96\n",
      "    loss           : -747463.4498762377\n",
      "    val_loss       : -757144.7652175188\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -724840.500000\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -671017.250000\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -648369.375000\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -567672.750000\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -770456.062500\n",
      "    epoch          : 97\n",
      "    loss           : -727166.4115099009\n",
      "    val_loss       : -754380.4562463164\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -663993.125000\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -618582.250000\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -789858.625000\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -736372.750000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -773818.500000\n",
      "    epoch          : 98\n",
      "    loss           : -725761.1268564357\n",
      "    val_loss       : -709712.1257240295\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -896141.187500\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -707778.125000\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -581010.500000\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -941180.687500\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -613449.125000\n",
      "    epoch          : 99\n",
      "    loss           : -745935.8514851485\n",
      "    val_loss       : -758847.2577463627\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -723094.687500\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -741183.500000\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -759908.000000\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -734960.687500\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -634952.750000\n",
      "    epoch          : 100\n",
      "    loss           : -744811.9727722772\n",
      "    val_loss       : -724930.3582539797\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0327_104948/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -833787.000000\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -721226.437500\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -697767.187500\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -743965.375000\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -643639.500000\n",
      "    epoch          : 101\n",
      "    loss           : -740821.0959158416\n",
      "    val_loss       : -771886.428230834\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -942342.687500\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -769521.062500\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -738838.625000\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -681851.750000\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -754185.750000\n",
      "    epoch          : 102\n",
      "    loss           : -762390.3155940594\n",
      "    val_loss       : -770865.549527216\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -929025.375000\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -821261.500000\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -727977.625000\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -607210.812500\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -743027.437500\n",
      "    epoch          : 103\n",
      "    loss           : -751841.1658415842\n",
      "    val_loss       : -733701.3556131363\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -840460.375000\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -602989.187500\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -628130.937500\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -746399.625000\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -749909.437500\n",
      "    epoch          : 104\n",
      "    loss           : -759151.9857673268\n",
      "    val_loss       : -763884.934412074\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -747085.562500\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -750057.750000\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -820824.000000\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -935290.687500\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -772493.625000\n",
      "    epoch          : 105\n",
      "    loss           : -757964.6639851485\n",
      "    val_loss       : -749691.3997840643\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -929092.000000\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -811300.625000\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -824860.625000\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -814891.250000\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -633063.500000\n",
      "    epoch          : 106\n",
      "    loss           : -750071.1407797029\n",
      "    val_loss       : -734399.2719795943\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -840651.812500\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -723188.812500\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -737838.875000\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -753094.687500\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -768130.312500\n",
      "    epoch          : 107\n",
      "    loss           : -741259.8452970297\n",
      "    val_loss       : -748579.2665447711\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -811882.375000\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -828915.500000\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -694298.625000\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -708710.875000\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -760468.437500\n",
      "    epoch          : 108\n",
      "    loss           : -746940.2976485149\n",
      "    val_loss       : -775608.0930993318\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -950953.500000\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -755937.375000\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -744932.312500\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -672458.062500\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -747807.500000\n",
      "    epoch          : 109\n",
      "    loss           : -754647.530940594\n",
      "    val_loss       : -771576.2151883363\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -919078.750000\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -741227.125000\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -689930.500000\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -770437.000000\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -787449.750000\n",
      "    epoch          : 110\n",
      "    loss           : -747142.4962871287\n",
      "    val_loss       : -724115.2771463394\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -836883.187500\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -831994.875000\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -754670.437500\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -926426.812500\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -764241.437500\n",
      "    epoch          : 111\n",
      "    loss           : -755538.7277227723\n",
      "    val_loss       : -769290.7054511786\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -945370.812500\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -736366.125000\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -813985.687500\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -718765.500000\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -732006.500000\n",
      "    epoch          : 112\n",
      "    loss           : -754418.2450495049\n",
      "    val_loss       : -747879.9191365242\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -822756.062500\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -745678.562500\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -675223.312500\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -763288.500000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -768346.437500\n",
      "    epoch          : 113\n",
      "    loss           : -755354.1819306931\n",
      "    val_loss       : -784448.703759706\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -943336.312500\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -715934.000000\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -780074.125000\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -741014.125000\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -781605.375000\n",
      "    epoch          : 114\n",
      "    loss           : -757923.8787128713\n",
      "    val_loss       : -727821.4851888537\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -950162.375000\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -647015.250000\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -705570.562500\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -630638.375000\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -713265.937500\n",
      "    epoch          : 115\n",
      "    loss           : -750688.5816831683\n",
      "    val_loss       : -760477.7135107279\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -846486.187500\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -739009.750000\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -582581.562500\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -954777.500000\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -773752.062500\n",
      "    epoch          : 116\n",
      "    loss           : -759935.573019802\n",
      "    val_loss       : -757093.1706858635\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -780460.062500\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -853588.375000\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -734829.937500\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -724759.312500\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -744321.125000\n",
      "    epoch          : 117\n",
      "    loss           : -767035.4306930694\n",
      "    val_loss       : -738497.0585624218\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -889418.562500\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -728160.812500\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -690464.000000\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -761618.250000\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -765022.812500\n",
      "    epoch          : 118\n",
      "    loss           : -768217.4594678218\n",
      "    val_loss       : -782615.1479092598\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -836006.625000\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -649813.562500\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -545817.562500\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -757809.312500\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -777709.500000\n",
      "    epoch          : 119\n",
      "    loss           : -761364.9183168317\n",
      "    val_loss       : -743313.840883851\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -947332.250000\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -734987.750000\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -696482.250000\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -758322.062500\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -618332.500000\n",
      "    epoch          : 120\n",
      "    loss           : -754068.0649752475\n",
      "    val_loss       : -752478.57439816\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -935237.750000\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -746708.250000\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -759458.250000\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -697675.312500\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -630871.312500\n",
      "    epoch          : 121\n",
      "    loss           : -751214.1875\n",
      "    val_loss       : -723055.9330676675\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -685108.312500\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -754492.312500\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -570331.250000\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -931447.750000\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -769571.750000\n",
      "    epoch          : 122\n",
      "    loss           : -761410.9331683168\n",
      "    val_loss       : -749375.316214323\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -751855.500000\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -765527.375000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -754930.437500\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -773658.562500\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -673109.625000\n",
      "    epoch          : 123\n",
      "    loss           : -766588.5655940594\n",
      "    val_loss       : -771387.5406062364\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -827016.000000\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -645283.000000\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -586515.375000\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -943413.625000\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -788104.500000\n",
      "    epoch          : 124\n",
      "    loss           : -757506.0445544554\n",
      "    val_loss       : -738636.5058209181\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -842642.000000\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -743612.812500\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -814156.812500\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -782626.937500\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -644312.437500\n",
      "    epoch          : 125\n",
      "    loss           : -762065.0191831683\n",
      "    val_loss       : -755807.0599448442\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -918304.625000\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -674819.562500\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -661608.687500\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -797096.125000\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -761677.125000\n",
      "    epoch          : 126\n",
      "    loss           : -765987.7444306931\n",
      "    val_loss       : -782390.2189783811\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -943266.062500\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -734560.000000\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -796335.375000\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -933132.500000\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -790593.000000\n",
      "    epoch          : 127\n",
      "    loss           : -768414.9381188119\n",
      "    val_loss       : -760373.6825978041\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -945612.750000\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -797226.562500\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -732459.437500\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -722677.500000\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -776505.500000\n",
      "    epoch          : 128\n",
      "    loss           : -769354.3013613861\n",
      "    val_loss       : -772967.196144104\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -759994.500000\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -754896.250000\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -751247.187500\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -822980.812500\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -633163.562500\n",
      "    epoch          : 129\n",
      "    loss           : -759242.7233910891\n",
      "    val_loss       : -776372.570541358\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -932838.250000\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -675179.625000\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -801996.750000\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -762784.750000\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -778667.625000\n",
      "    epoch          : 130\n",
      "    loss           : -755655.0408415842\n",
      "    val_loss       : -727566.0135501385\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -935972.500000\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -767419.500000\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -767195.500000\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -655917.937500\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -726033.187500\n",
      "    epoch          : 131\n",
      "    loss           : -765525.1107673268\n",
      "    val_loss       : -791830.6061826944\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -939979.000000\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -803991.750000\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -672970.375000\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -673707.625000\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -760087.375000\n",
      "    epoch          : 132\n",
      "    loss           : -767877.5544554455\n",
      "    val_loss       : -753703.2611648322\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -718921.937500\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -765034.750000\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -662528.875000\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -576176.000000\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -803738.750000\n",
      "    epoch          : 133\n",
      "    loss           : -768276.3811881188\n",
      "    val_loss       : -783931.0679378748\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -788122.062500\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -852044.937500\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -796591.562500\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -623120.937500\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -674121.437500\n",
      "    epoch          : 134\n",
      "    loss           : -770077.7982673268\n",
      "    val_loss       : -738823.575328207\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -871353.875000\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -729904.500000\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -751061.375000\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -770918.875000\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -780984.437500\n",
      "    epoch          : 135\n",
      "    loss           : -755772.1707920792\n",
      "    val_loss       : -760657.2999389172\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -864827.687500\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -817867.500000\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -758908.500000\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -813474.750000\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -767106.625000\n",
      "    epoch          : 136\n",
      "    loss           : -767224.2964108911\n",
      "    val_loss       : -787984.9195298434\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -955988.000000\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -676194.812500\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -744572.312500\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -641329.437500\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -634295.250000\n",
      "    epoch          : 137\n",
      "    loss           : -764753.9251237623\n",
      "    val_loss       : -765292.9454599858\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -940303.625000\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -730565.437500\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -706312.375000\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -713995.875000\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -765548.875000\n",
      "    epoch          : 138\n",
      "    loss           : -774971.8211633663\n",
      "    val_loss       : -765038.0898108244\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -841534.687500\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -765426.375000\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -621558.875000\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -802130.500000\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -620329.875000\n",
      "    epoch          : 139\n",
      "    loss           : -765835.083539604\n",
      "    val_loss       : -785114.4406579018\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -843493.375000\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -735244.125000\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -748668.937500\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -944319.500000\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -789403.062500\n",
      "    epoch          : 140\n",
      "    loss           : -765443.082920792\n",
      "    val_loss       : -781260.5250025749\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -937249.375000\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -838010.312500\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -728953.437500\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -739770.250000\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -538630.625000\n",
      "    epoch          : 141\n",
      "    loss           : -771184.6404702971\n",
      "    val_loss       : -749041.4406903744\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -945108.000000\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -662106.687500\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -677601.687500\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -929269.875000\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -760077.562500\n",
      "    epoch          : 142\n",
      "    loss           : -777043.9418316832\n",
      "    val_loss       : -724853.510533309\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -958106.500000\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -798102.250000\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -759542.750000\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -842082.812500\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -716379.750000\n",
      "    epoch          : 143\n",
      "    loss           : -780011.1992574257\n",
      "    val_loss       : -795394.7777609586\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -958092.375000\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -650884.000000\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -745845.625000\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -953472.000000\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -761607.500000\n",
      "    epoch          : 144\n",
      "    loss           : -773934.2784653465\n",
      "    val_loss       : -747866.8096657753\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -955024.937500\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -583776.125000\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -793813.125000\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -737686.375000\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -732816.750000\n",
      "    epoch          : 145\n",
      "    loss           : -760517.1930693069\n",
      "    val_loss       : -757801.065462327\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -926572.750000\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -665401.125000\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -728106.312500\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -592036.875000\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -792805.437500\n",
      "    epoch          : 146\n",
      "    loss           : -761784.2048267326\n",
      "    val_loss       : -774815.9481184721\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -772017.250000\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -608411.437500\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -820630.062500\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -740088.250000\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -800699.687500\n",
      "    epoch          : 147\n",
      "    loss           : -767968.6763613861\n",
      "    val_loss       : -763310.1393274546\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -833648.875000\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -746421.187500\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -789672.125000\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -718216.375000\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -550292.125000\n",
      "    epoch          : 148\n",
      "    loss           : -771347.5092821782\n",
      "    val_loss       : -778298.3847156524\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -880508.187500\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -789204.625000\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -706521.625000\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -776780.250000\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -792476.875000\n",
      "    epoch          : 149\n",
      "    loss           : -765576.7889851485\n",
      "    val_loss       : -772323.1040080786\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -813759.500000\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -740775.875000\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -690628.500000\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -778185.375000\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -807924.625000\n",
      "    epoch          : 150\n",
      "    loss           : -771353.1157178218\n",
      "    val_loss       : -774003.0822552681\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0327_104948/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -927601.812500\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -771741.937500\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -727812.250000\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -829448.000000\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -781547.125000\n",
      "    epoch          : 151\n",
      "    loss           : -778982.1528465346\n",
      "    val_loss       : -783223.5409495116\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -962220.687500\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -775998.500000\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -711980.125000\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -941336.312500\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -692682.062500\n",
      "    epoch          : 152\n",
      "    loss           : -774838.4709158416\n",
      "    val_loss       : -769903.6341356516\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -945008.562500\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -813877.625000\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -748122.937500\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -802576.625000\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -612019.625000\n",
      "    epoch          : 153\n",
      "    loss           : -780507.5501237623\n",
      "    val_loss       : -795871.7943455934\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -955506.750000\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -867421.250000\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -827119.125000\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -729168.875000\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -784193.375000\n",
      "    epoch          : 154\n",
      "    loss           : -778852.9201732674\n",
      "    val_loss       : -779856.2774600983\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -854419.625000\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -753015.125000\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -853183.312500\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -831961.125000\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -807909.750000\n",
      "    epoch          : 155\n",
      "    loss           : -778982.8886138614\n",
      "    val_loss       : -792093.8274296522\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -949580.000000\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -645101.000000\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -802548.687500\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -788524.687500\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -759990.687500\n",
      "    epoch          : 156\n",
      "    loss           : -777629.7908415842\n",
      "    val_loss       : -791317.8396157026\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -664298.875000\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -823293.875000\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -755277.375000\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -819434.687500\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -782781.375000\n",
      "    epoch          : 157\n",
      "    loss           : -773513.405940594\n",
      "    val_loss       : -770525.7783937216\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -950802.437500\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -751043.750000\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -681007.625000\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -726881.375000\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -784159.187500\n",
      "    epoch          : 158\n",
      "    loss           : -773574.9730816832\n",
      "    val_loss       : -764207.8762370109\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -928104.937500\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -842421.125000\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -772277.375000\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -737611.062500\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -745761.750000\n",
      "    epoch          : 159\n",
      "    loss           : -791752.8310643565\n",
      "    val_loss       : -782044.9874773503\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -930408.625000\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -832019.125000\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -737154.062500\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -845240.000000\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -797618.000000\n",
      "    epoch          : 160\n",
      "    loss           : -781397.0451732674\n",
      "    val_loss       : -768657.1506406546\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -702146.000000\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -666538.937500\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -841360.312500\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -900675.000000\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -748384.250000\n",
      "    epoch          : 161\n",
      "    loss           : -776356.5136138614\n",
      "    val_loss       : -750817.7810446739\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -743107.000000\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -817239.125000\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -760995.500000\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -681866.375000\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -751106.500000\n",
      "    epoch          : 162\n",
      "    loss           : -773260.8490099009\n",
      "    val_loss       : -789019.3906099796\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -869103.500000\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -805384.625000\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -725488.187500\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -758757.312500\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -750351.250000\n",
      "    epoch          : 163\n",
      "    loss           : -787272.8063118812\n",
      "    val_loss       : -801300.3624636888\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -949740.625000\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -786497.875000\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -798987.062500\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -839942.062500\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -725828.250000\n",
      "    epoch          : 164\n",
      "    loss           : -784908.3737623763\n",
      "    val_loss       : -798905.9762749433\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -931076.750000\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -825146.500000\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -749290.750000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -609988.375000\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -759387.375000\n",
      "    epoch          : 165\n",
      "    loss           : -773942.4412128713\n",
      "    val_loss       : -793126.4438099861\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -957543.500000\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -735856.750000\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -824864.437500\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -705464.500000\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -732476.375000\n",
      "    epoch          : 166\n",
      "    loss           : -774191.2995049505\n",
      "    val_loss       : -741213.579893446\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -930979.250000\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -782733.687500\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -734921.625000\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -759347.687500\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -774716.875000\n",
      "    epoch          : 167\n",
      "    loss           : -787671.4907178218\n",
      "    val_loss       : -790041.9252912641\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -967604.937500\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -766278.250000\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -622679.875000\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -781428.937500\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -764148.750000\n",
      "    epoch          : 168\n",
      "    loss           : -784083.9832920792\n",
      "    val_loss       : -784690.1429448128\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -950835.125000\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -579580.000000\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -851617.000000\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -829104.312500\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -756429.625000\n",
      "    epoch          : 169\n",
      "    loss           : -783558.0513613861\n",
      "    val_loss       : -759846.2496413707\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -956126.750000\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -762643.125000\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -790500.312500\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -719939.187500\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -689795.312500\n",
      "    epoch          : 170\n",
      "    loss           : -770093.1497524752\n",
      "    val_loss       : -733337.0900594711\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -954513.062500\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -748015.375000\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -810052.062500\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -736843.687500\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -734104.937500\n",
      "    epoch          : 171\n",
      "    loss           : -766629.7419554455\n",
      "    val_loss       : -786949.4405905008\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -822942.687500\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -826301.687500\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -781492.875000\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -587934.437500\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -764259.375000\n",
      "    epoch          : 172\n",
      "    loss           : -773059.0228960396\n",
      "    val_loss       : -791085.4276528836\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -947896.750000\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -812273.687500\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -751482.750000\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -934433.125000\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -683629.875000\n",
      "    epoch          : 173\n",
      "    loss           : -784585.7246287129\n",
      "    val_loss       : -774774.7531761766\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -922906.625000\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -882235.937500\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -761549.000000\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -882534.750000\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -680970.062500\n",
      "    epoch          : 174\n",
      "    loss           : -774060.2865099009\n",
      "    val_loss       : -791995.6028610468\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -617116.312500\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -683395.375000\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -826803.375000\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -793183.062500\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -799739.812500\n",
      "    epoch          : 175\n",
      "    loss           : -765319.8496287129\n",
      "    val_loss       : -794620.0265875577\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -936760.500000\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -745357.250000\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -695638.687500\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -907460.000000\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -786124.437500\n",
      "    epoch          : 176\n",
      "    loss           : -781794.0049504951\n",
      "    val_loss       : -759968.79894526\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -928433.687500\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -735907.125000\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -791033.375000\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -973031.750000\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -759574.500000\n",
      "    epoch          : 177\n",
      "    loss           : -782840.0191831683\n",
      "    val_loss       : -781717.8953737855\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -963856.625000\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -846223.250000\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -742348.562500\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -708536.375000\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -754943.125000\n",
      "    epoch          : 178\n",
      "    loss           : -793646.8923267326\n",
      "    val_loss       : -797263.9560858726\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -894707.062500\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -731254.500000\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -808557.562500\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -768060.875000\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -781468.625000\n",
      "    epoch          : 179\n",
      "    loss           : -783265.1602722772\n",
      "    val_loss       : -788670.9512243628\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -839286.562500\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -664741.875000\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -769611.062500\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -788327.875000\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -754558.500000\n",
      "    epoch          : 180\n",
      "    loss           : -768054.5030940594\n",
      "    val_loss       : -780114.7013593197\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -959310.875000\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -812938.187500\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -783831.000000\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -836055.812500\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -743950.125000\n",
      "    epoch          : 181\n",
      "    loss           : -781827.3824257426\n",
      "    val_loss       : -788174.4677798748\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -977904.812500\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -869555.875000\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -829074.125000\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -770079.000000\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -593879.000000\n",
      "    epoch          : 182\n",
      "    loss           : -787131.5495049505\n",
      "    val_loss       : -776254.9666870832\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -956278.062500\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -739636.625000\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -752832.062500\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -772342.250000\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -775152.437500\n",
      "    epoch          : 183\n",
      "    loss           : -769254.895420792\n",
      "    val_loss       : -763084.4614490032\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -885480.687500\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -864247.000000\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -688132.125000\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -787727.187500\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -828315.812500\n",
      "    epoch          : 184\n",
      "    loss           : -781845.6695544554\n",
      "    val_loss       : -794071.4539806604\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -769314.187500\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -744262.312500\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -739284.875000\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -757323.062500\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -775928.187500\n",
      "    epoch          : 185\n",
      "    loss           : -780946.2722772277\n",
      "    val_loss       : -745947.4376307011\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -878401.187500\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -866659.375000\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -458004.812500\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -850286.500000\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -803206.562500\n",
      "    epoch          : 186\n",
      "    loss           : -786324.582920792\n",
      "    val_loss       : -776522.1832585573\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -940551.375000\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -755778.500000\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -737310.812500\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -751363.625000\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -787470.250000\n",
      "    epoch          : 187\n",
      "    loss           : -781427.1256188119\n",
      "    val_loss       : -798207.3883514404\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -913066.437500\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -758358.375000\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -672619.750000\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -782369.000000\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -775641.500000\n",
      "    epoch          : 188\n",
      "    loss           : -793866.2759900991\n",
      "    val_loss       : -812554.9162775755\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -967177.875000\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -785678.375000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -761269.062500\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -970340.687500\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -787877.187500\n",
      "    epoch          : 189\n",
      "    loss           : -794026.3149752475\n",
      "    val_loss       : -762651.0534750223\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -946817.062500\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -839710.625000\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -522911.937500\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -819096.312500\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -653331.062500\n",
      "    epoch          : 190\n",
      "    loss           : -791108.0928217822\n",
      "    val_loss       : -774272.9344120503\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -964628.375000\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -630852.125000\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -687489.875000\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -769289.875000\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -738111.812500\n",
      "    epoch          : 191\n",
      "    loss           : -786209.8948019802\n",
      "    val_loss       : -772650.4520407438\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -886491.937500\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -763983.937500\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -676267.750000\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -775636.125000\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -837355.250000\n",
      "    epoch          : 192\n",
      "    loss           : -787417.4418316832\n",
      "    val_loss       : -785596.7305277586\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -955658.875000\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -851552.562500\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -753453.812500\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -755593.562500\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -787286.125000\n",
      "    epoch          : 193\n",
      "    loss           : -789912.0092821782\n",
      "    val_loss       : -741333.9551146984\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -929713.875000\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -782239.125000\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -715409.750000\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -735137.125000\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -804282.750000\n",
      "    epoch          : 194\n",
      "    loss           : -780177.2004950495\n",
      "    val_loss       : -745854.5043132544\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -914226.687500\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -727844.187500\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -761586.062500\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -827861.375000\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -790445.312500\n",
      "    epoch          : 195\n",
      "    loss           : -765268.2119430694\n",
      "    val_loss       : -777311.703759551\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -795907.687500\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -764356.000000\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -698626.875000\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -738013.500000\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -742301.812500\n",
      "    epoch          : 196\n",
      "    loss           : -771921.9387376237\n",
      "    val_loss       : -770284.441692257\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -920556.125000\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -620333.125000\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -739015.937500\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -778653.437500\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -816454.687500\n",
      "    epoch          : 197\n",
      "    loss           : -783745.9771039604\n",
      "    val_loss       : -780253.1215282201\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -966719.312500\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -801591.562500\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -844535.000000\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -738194.125000\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -719360.750000\n",
      "    epoch          : 198\n",
      "    loss           : -790713.4356435643\n",
      "    val_loss       : -782916.4121164798\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -836345.000000\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -703946.625000\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -800438.125000\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -734322.625000\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -776079.875000\n",
      "    epoch          : 199\n",
      "    loss           : -782305.1132425743\n",
      "    val_loss       : -788627.0141763687\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -955836.625000\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -866536.000000\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -817754.187500\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -769239.062500\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -701867.312500\n",
      "    epoch          : 200\n",
      "    loss           : -788321.2660891089\n",
      "    val_loss       : -788007.5694227457\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0327_104948/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -961820.875000\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -862884.250000\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -848770.687500\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -767251.625000\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -794617.687500\n",
      "    epoch          : 201\n",
      "    loss           : -787855.468440594\n",
      "    val_loss       : -756124.4430505752\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -844436.562500\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -704948.562500\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -870568.562500\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -760921.437500\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -811986.375000\n",
      "    epoch          : 202\n",
      "    loss           : -791000.489480198\n",
      "    val_loss       : -797699.7512539625\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -983066.125000\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -796767.375000\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -819516.812500\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -682045.187500\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -774916.125000\n",
      "    epoch          : 203\n",
      "    loss           : -790108.2753712871\n",
      "    val_loss       : -787660.5376017451\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -944040.500000\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -875555.250000\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -778448.250000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -757357.875000\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -614712.250000\n",
      "    epoch          : 204\n",
      "    loss           : -799016.6862623763\n",
      "    val_loss       : -782480.4338592768\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -888669.375000\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -764389.625000\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -802913.562500\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -978856.875000\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -765663.000000\n",
      "    epoch          : 205\n",
      "    loss           : -798414.0872524752\n",
      "    val_loss       : -800200.1387542725\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -989256.125000\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -846187.187500\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -754612.625000\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -804960.125000\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -714432.125000\n",
      "    epoch          : 206\n",
      "    loss           : -779097.4350247525\n",
      "    val_loss       : -778339.7757671833\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -950275.062500\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -818845.500000\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -741153.875000\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -849406.437500\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -619057.000000\n",
      "    epoch          : 207\n",
      "    loss           : -784018.9294554455\n",
      "    val_loss       : -787794.4335135699\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -884690.687500\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -744987.187500\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -754299.375000\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -789883.500000\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -803683.187500\n",
      "    epoch          : 208\n",
      "    loss           : -791867.4009900991\n",
      "    val_loss       : -771333.7251446486\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -863103.875000\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -854054.625000\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -616124.875000\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -818906.562500\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -742731.687500\n",
      "    epoch          : 209\n",
      "    loss           : -801260.6763613861\n",
      "    val_loss       : -794813.2607851267\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -984585.312500\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -765975.687500\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -841077.375000\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -985739.500000\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -844889.812500\n",
      "    epoch          : 210\n",
      "    loss           : -801203.7017326732\n",
      "    val_loss       : -765050.1489513874\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -831951.937500\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -766202.750000\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -740973.125000\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -808911.687500\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -775761.062500\n",
      "    epoch          : 211\n",
      "    loss           : -793985.4820544554\n",
      "    val_loss       : -783065.9187292814\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -986926.875000\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -773430.875000\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -747180.000000\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -823591.875000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -723459.062500\n",
      "    epoch          : 212\n",
      "    loss           : -782535.9647277228\n",
      "    val_loss       : -759530.2450891256\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -946014.000000\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -822812.000000\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -737465.437500\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -744284.125000\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -755751.750000\n",
      "    epoch          : 213\n",
      "    loss           : -794791.3168316832\n",
      "    val_loss       : -795283.0757706404\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -962534.000000\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -781760.250000\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -785325.812500\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -715013.875000\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -779974.625000\n",
      "    epoch          : 214\n",
      "    loss           : -803409.3582920792\n",
      "    val_loss       : -793815.025531435\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -924069.687500\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -837923.750000\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -761023.000000\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -767386.125000\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -851633.312500\n",
      "    epoch          : 215\n",
      "    loss           : -797418.4888613861\n",
      "    val_loss       : -788061.6878684282\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -966032.000000\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -798194.125000\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -851531.562500\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -638961.312500\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -710524.687500\n",
      "    epoch          : 216\n",
      "    loss           : -792177.3279702971\n",
      "    val_loss       : -799308.84007442\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -967824.125000\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -822231.187500\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -746187.625000\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -755455.312500\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -808775.250000\n",
      "    epoch          : 217\n",
      "    loss           : -796901.0099009901\n",
      "    val_loss       : -818693.0131631136\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -972401.625000\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -749035.625000\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -837662.062500\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -640211.187500\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -670732.375000\n",
      "    epoch          : 218\n",
      "    loss           : -795866.5903465346\n",
      "    val_loss       : -805644.3105773211\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -978729.250000\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -771803.000000\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -810942.125000\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -701175.625000\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -809194.500000\n",
      "    epoch          : 219\n",
      "    loss           : -797860.3131188119\n",
      "    val_loss       : -791639.8374515057\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -840244.312500\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -672810.125000\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -772641.750000\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -809825.625000\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -741280.500000\n",
      "    epoch          : 220\n",
      "    loss           : -803879.2209158416\n",
      "    val_loss       : -799962.6106727838\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -956140.062500\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -847196.562500\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -734796.062500\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -811983.250000\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -830680.625000\n",
      "    epoch          : 221\n",
      "    loss           : -790581.3756188119\n",
      "    val_loss       : -801137.2897332668\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -966229.437500\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -791533.625000\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -722950.125000\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -700471.750000\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -694332.500000\n",
      "    epoch          : 222\n",
      "    loss           : -786356.0699257426\n",
      "    val_loss       : -759100.9080112219\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -866736.875000\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -718904.750000\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -798150.937500\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -794183.062500\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -802738.562500\n",
      "    epoch          : 223\n",
      "    loss           : -770478.1219059406\n",
      "    val_loss       : -774533.1218158721\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -942163.000000\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -879860.562500\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -816139.062500\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -909892.687500\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -820184.000000\n",
      "    epoch          : 224\n",
      "    loss           : -786252.1602722772\n",
      "    val_loss       : -772421.2191570759\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -727792.125000\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -596223.000000\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -762770.687500\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -753545.500000\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -799856.000000\n",
      "    epoch          : 225\n",
      "    loss           : -783866.1813118812\n",
      "    val_loss       : -786271.6022782803\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -944874.187500\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -762949.125000\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -760495.937500\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -783381.750000\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -712759.437500\n",
      "    epoch          : 226\n",
      "    loss           : -790889.1689356435\n",
      "    val_loss       : -781127.574761343\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -819020.812500\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -839922.500000\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -820259.562500\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -839208.750000\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -760069.562500\n",
      "    epoch          : 227\n",
      "    loss           : -783773.8137376237\n",
      "    val_loss       : -785069.1668212891\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -952937.500000\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -763770.437500\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -842094.250000\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -757674.625000\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -668959.687500\n",
      "    epoch          : 228\n",
      "    loss           : -794317.1714108911\n",
      "    val_loss       : -811741.3425150871\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -970933.875000\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -808719.125000\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -783234.562500\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -786862.812500\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -761131.187500\n",
      "    epoch          : 229\n",
      "    loss           : -797330.9467821782\n",
      "    val_loss       : -819635.3277428031\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -777568.937500\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -799360.062500\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -720057.125000\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -765478.187500\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -822982.625000\n",
      "    epoch          : 230\n",
      "    loss           : -802636.1782178218\n",
      "    val_loss       : -800621.0174783826\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -956724.375000\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -850921.000000\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -774234.937500\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -741391.687500\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -830926.437500\n",
      "    epoch          : 231\n",
      "    loss           : -800660.4709158416\n",
      "    val_loss       : -798327.9035166621\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -947934.250000\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -753006.500000\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -773620.937500\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -640309.437500\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -772527.375000\n",
      "    epoch          : 232\n",
      "    loss           : -802797.417079208\n",
      "    val_loss       : -788201.2613004803\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -814030.625000\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -805662.750000\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -757007.437500\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -620467.625000\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -840483.687500\n",
      "    epoch          : 233\n",
      "    loss           : -807982.3360148515\n",
      "    val_loss       : -784333.3650089741\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -940169.812500\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -738844.250000\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -815952.500000\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -804843.937500\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -772598.437500\n",
      "    epoch          : 234\n",
      "    loss           : -786929.3978960396\n",
      "    val_loss       : -807223.4291815758\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -927106.625000\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -837232.812500\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -787904.812500\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -791892.500000\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -668220.437500\n",
      "    epoch          : 235\n",
      "    loss           : -796019.2314356435\n",
      "    val_loss       : -786024.901965642\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -926523.750000\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -738950.500000\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -690331.125000\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -803985.000000\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -828673.750000\n",
      "    epoch          : 236\n",
      "    loss           : -808903.4573019802\n",
      "    val_loss       : -796396.7113144398\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -944317.625000\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -773785.937500\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -770291.812500\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -947617.062500\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -759714.375000\n",
      "    epoch          : 237\n",
      "    loss           : -811078.3601485149\n",
      "    val_loss       : -809412.4378585577\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -953847.625000\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -877763.750000\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -760939.312500\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -944017.562500\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -757602.000000\n",
      "    epoch          : 238\n",
      "    loss           : -805454.0841584158\n",
      "    val_loss       : -789628.7691657662\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -959365.500000\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -735217.125000\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -772466.500000\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -855666.500000\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -719094.750000\n",
      "    epoch          : 239\n",
      "    loss           : -799671.3923267326\n",
      "    val_loss       : -824829.8578646898\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -807955.625000\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -819565.500000\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -759574.250000\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -787139.437500\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -784318.687500\n",
      "    epoch          : 240\n",
      "    loss           : -806680.2586633663\n",
      "    val_loss       : -814047.5982719421\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -754347.312500\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -787893.000000\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -720145.312500\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -800107.437500\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -781381.312500\n",
      "    epoch          : 241\n",
      "    loss           : -795674.2568069306\n",
      "    val_loss       : -810262.6725124121\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -978618.562500\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -819720.125000\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -831798.812500\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -839670.625000\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -681568.312500\n",
      "    epoch          : 242\n",
      "    loss           : -796659.5086633663\n",
      "    val_loss       : -803032.2895553112\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -774413.625000\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -778088.750000\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -821813.062500\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -978510.437500\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -799021.000000\n",
      "    epoch          : 243\n",
      "    loss           : -807654.0451732674\n",
      "    val_loss       : -792385.5598091364\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -875328.437500\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -730619.500000\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -621408.812500\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -763515.812500\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -650781.312500\n",
      "    epoch          : 244\n",
      "    loss           : -786984.0983910891\n",
      "    val_loss       : -800379.4100775957\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -987114.125000\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -715144.625000\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -784738.562500\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -774017.875000\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -657336.062500\n",
      "    epoch          : 245\n",
      "    loss           : -792353.2976485149\n",
      "    val_loss       : -808283.4250852108\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -957895.437500\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -626602.562500\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -799829.312500\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -971964.562500\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -777685.000000\n",
      "    epoch          : 246\n",
      "    loss           : -801081.0668316832\n",
      "    val_loss       : -799738.5480822563\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -819484.500000\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -823487.500000\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -852676.812500\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -780521.625000\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -757267.687500\n",
      "    epoch          : 247\n",
      "    loss           : -803262.0810643565\n",
      "    val_loss       : -805807.9165061951\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -853183.562500\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -775086.625000\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -764454.125000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -782436.312500\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -835225.125000\n",
      "    epoch          : 248\n",
      "    loss           : -800812.4900990099\n",
      "    val_loss       : -810116.7730685472\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -952109.000000\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -883384.687500\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -826189.875000\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -790755.125000\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -797373.062500\n",
      "    epoch          : 249\n",
      "    loss           : -798435.896039604\n",
      "    val_loss       : -800789.0173196793\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -861065.000000\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -810545.000000\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -747852.562500\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -795131.937500\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -732298.125000\n",
      "    epoch          : 250\n",
      "    loss           : -804420.1844059406\n",
      "    val_loss       : -780301.6967299699\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0327_104948/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -860760.750000\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -779271.500000\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -764205.750000\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -789807.750000\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -685206.187500\n",
      "    epoch          : 251\n",
      "    loss           : -800481.0173267326\n",
      "    val_loss       : -803283.0781317115\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -974286.625000\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -782045.250000\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -743748.000000\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -683086.250000\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -792758.000000\n",
      "    epoch          : 252\n",
      "    loss           : -796374.5507425743\n",
      "    val_loss       : -797611.8013125897\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -876280.500000\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -888474.625000\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -751575.500000\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -793949.562500\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -712969.500000\n",
      "    epoch          : 253\n",
      "    loss           : -799653.1497524752\n",
      "    val_loss       : -808749.6509379387\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -889471.437500\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -792601.437500\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -798753.437500\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -785051.625000\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -806827.437500\n",
      "    epoch          : 254\n",
      "    loss           : -802364.9616336634\n",
      "    val_loss       : -800542.2096932649\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -900433.375000\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -756137.125000\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -794642.875000\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -845747.875000\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -784403.750000\n",
      "    epoch          : 255\n",
      "    loss           : -801693.7462871287\n",
      "    val_loss       : -782767.706109643\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -703103.000000\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -784957.812500\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -721254.437500\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -1000772.187500\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -764191.750000\n",
      "    epoch          : 256\n",
      "    loss           : -806410.0142326732\n",
      "    val_loss       : -784646.8876769065\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -676916.875000\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -689475.500000\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -757606.500000\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -819091.250000\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -751151.437500\n",
      "    epoch          : 257\n",
      "    loss           : -799534.3972772277\n",
      "    val_loss       : -796349.2031835079\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -972855.375000\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -794659.750000\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -743325.875000\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -848342.812500\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -823313.062500\n",
      "    epoch          : 258\n",
      "    loss           : -799881.0525990099\n",
      "    val_loss       : -786907.7003949642\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -884676.500000\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -868319.625000\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -774427.750000\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -855426.562500\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -789489.187500\n",
      "    epoch          : 259\n",
      "    loss           : -804051.5068069306\n",
      "    val_loss       : -817248.3004104376\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -857626.875000\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -779367.937500\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -707224.500000\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -826553.375000\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -751526.812500\n",
      "    epoch          : 260\n",
      "    loss           : -809823.7382425743\n",
      "    val_loss       : -837691.2547403097\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -941138.625000\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -760275.500000\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -778596.687500\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -735194.687500\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -799554.625000\n",
      "    epoch          : 261\n",
      "    loss           : -792228.4263613861\n",
      "    val_loss       : -801791.0952130556\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -753948.125000\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -847077.062500\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -793780.875000\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -978779.687500\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -758355.250000\n",
      "    epoch          : 262\n",
      "    loss           : -794659.1361386139\n",
      "    val_loss       : -817301.3283914089\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -981255.250000\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -781492.437500\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -830834.750000\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -717869.562500\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -800331.062500\n",
      "    epoch          : 263\n",
      "    loss           : -806254.0080445545\n",
      "    val_loss       : -817318.2074625492\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -999633.625000\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -739761.750000\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -841150.750000\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -728017.125000\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -787978.750000\n",
      "    epoch          : 264\n",
      "    loss           : -799972.6751237623\n",
      "    val_loss       : -784523.2767002105\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -954839.250000\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -739631.875000\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -749878.812500\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -644068.250000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -789252.125000\n",
      "    epoch          : 265\n",
      "    loss           : -791530.8719059406\n",
      "    val_loss       : -787352.0846232533\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -786122.687500\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -779326.250000\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -780438.875000\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -681311.250000\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -760857.625000\n",
      "    epoch          : 266\n",
      "    loss           : -787200.2852722772\n",
      "    val_loss       : -803579.5309417725\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -815851.625000\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -801312.937500\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -821517.250000\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -801828.062500\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -831366.750000\n",
      "    epoch          : 267\n",
      "    loss           : -806387.9659653465\n",
      "    val_loss       : -798190.2788343191\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -850679.250000\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -796867.500000\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -681624.000000\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -990023.187500\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -853218.125000\n",
      "    epoch          : 268\n",
      "    loss           : -802635.0068069306\n",
      "    val_loss       : -819169.7254258872\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -972169.250000\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -747952.875000\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -776419.312500\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -805069.125000\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -711914.062500\n",
      "    epoch          : 269\n",
      "    loss           : -812877.9096534654\n",
      "    val_loss       : -763677.0550797224\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -976333.687500\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -754504.062500\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -868032.375000\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -778539.687500\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -794462.937500\n",
      "    epoch          : 270\n",
      "    loss           : -802939.8867574257\n",
      "    val_loss       : -802476.0957857848\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -935579.250000\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -829060.250000\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -830036.500000\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -802553.250000\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -815579.500000\n",
      "    epoch          : 271\n",
      "    loss           : -799218.8496287129\n",
      "    val_loss       : -779450.9082707405\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -870406.875000\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -775916.375000\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -835032.437500\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -862163.312500\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -784308.875000\n",
      "    epoch          : 272\n",
      "    loss           : -800266.9845297029\n",
      "    val_loss       : -817749.3794272185\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -731471.375000\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -790058.312500\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -772806.062500\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -751926.500000\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -762722.875000\n",
      "    epoch          : 273\n",
      "    loss           : -804813.7425742574\n",
      "    val_loss       : -802201.5125909805\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -981907.500000\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -771944.250000\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -785583.625000\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -766228.875000\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -790734.937500\n",
      "    epoch          : 274\n",
      "    loss           : -804503.7004950495\n",
      "    val_loss       : -820655.2937266827\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -848991.250000\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -866921.750000\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -781029.750000\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -736817.375000\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -823787.625000\n",
      "    epoch          : 275\n",
      "    loss           : -811516.228960396\n",
      "    val_loss       : -811569.0723271131\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -965593.125000\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -821944.125000\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -796041.125000\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -791708.000000\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -822742.812500\n",
      "    epoch          : 276\n",
      "    loss           : -802453.1138613861\n",
      "    val_loss       : -794109.9241162181\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -698621.187500\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -869753.375000\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -775609.375000\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -951255.375000\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -815096.625000\n",
      "    epoch          : 277\n",
      "    loss           : -806191.0742574257\n",
      "    val_loss       : -796278.0336348533\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -874677.125000\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -894124.000000\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -835291.687500\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -867760.687500\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -805634.000000\n",
      "    epoch          : 278\n",
      "    loss           : -808991.6318069306\n",
      "    val_loss       : -795967.5793292761\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -978676.750000\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -776332.687500\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -844711.750000\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -962837.062500\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -764921.250000\n",
      "    epoch          : 279\n",
      "    loss           : -811797.4121287129\n",
      "    val_loss       : -812484.6495012045\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -971729.562500\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -755174.000000\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -849530.500000\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -933659.687500\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -812232.625000\n",
      "    epoch          : 280\n",
      "    loss           : -807354.7728960396\n",
      "    val_loss       : -813781.5636255264\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -972845.312500\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -781813.500000\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -789897.000000\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -826511.625000\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -832265.250000\n",
      "    epoch          : 281\n",
      "    loss           : -821785.7110148515\n",
      "    val_loss       : -829325.7738620758\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -834416.187500\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -800710.000000\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -816297.687500\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -783544.875000\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -751631.125000\n",
      "    epoch          : 282\n",
      "    loss           : -816607.521039604\n",
      "    val_loss       : -809936.8771072865\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -943852.125000\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -755710.875000\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -791533.750000\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -709989.000000\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -822700.750000\n",
      "    epoch          : 283\n",
      "    loss           : -807699.1423267326\n",
      "    val_loss       : -815856.6061521291\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -801126.375000\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -812680.750000\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -802624.125000\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -819439.875000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -717218.062500\n",
      "    epoch          : 284\n",
      "    loss           : -803439.1955445545\n",
      "    val_loss       : -799270.684432578\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -967495.812500\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -789153.437500\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -841932.625000\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -842419.375000\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -794282.375000\n",
      "    epoch          : 285\n",
      "    loss           : -803752.9925742574\n",
      "    val_loss       : -814776.2352491617\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -962565.687500\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -685293.812500\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -757567.375000\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -792763.187500\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -800565.375000\n",
      "    epoch          : 286\n",
      "    loss           : -813134.1646039604\n",
      "    val_loss       : -780842.994505167\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -951923.625000\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -883669.750000\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -800371.750000\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -967773.125000\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -854092.500000\n",
      "    epoch          : 287\n",
      "    loss           : -812415.2493811881\n",
      "    val_loss       : -817449.1821110726\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -992522.937500\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -758776.062500\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -792237.750000\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -778509.625000\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -772145.750000\n",
      "    epoch          : 288\n",
      "    loss           : -816777.7691831683\n",
      "    val_loss       : -828952.7971710085\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -966717.375000\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -798099.812500\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -788467.625000\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -798669.312500\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -800668.812500\n",
      "    epoch          : 289\n",
      "    loss           : -816437.1076732674\n",
      "    val_loss       : -798372.1918059349\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -971935.937500\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -841381.000000\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -814214.375000\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -840216.250000\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -772102.562500\n",
      "    epoch          : 290\n",
      "    loss           : -818556.8174504951\n",
      "    val_loss       : -821100.9820022822\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -935508.937500\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -890228.562500\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -749983.750000\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -935617.250000\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -834550.437500\n",
      "    epoch          : 291\n",
      "    loss           : -806862.0928217822\n",
      "    val_loss       : -802795.1766992093\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -954237.500000\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -769156.812500\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -784428.062500\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -818201.500000\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -832891.625000\n",
      "    epoch          : 292\n",
      "    loss           : -807857.8861386139\n",
      "    val_loss       : -807300.2682077885\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -985384.500000\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -761566.500000\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -766658.437500\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -849414.125000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -768027.812500\n",
      "    epoch          : 293\n",
      "    loss           : -804312.2011138614\n",
      "    val_loss       : -810117.1489686251\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -956870.875000\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -814271.625000\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -776267.250000\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -809709.875000\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -807026.937500\n",
      "    epoch          : 294\n",
      "    loss           : -810697.1658415842\n",
      "    val_loss       : -814125.4888030529\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -878836.375000\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -846595.500000\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -785418.062500\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -775413.875000\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -823920.625000\n",
      "    epoch          : 295\n",
      "    loss           : -812999.655940594\n",
      "    val_loss       : -826962.9860667109\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -968418.750000\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -880094.062500\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -799843.687500\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -953237.687500\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -844232.500000\n",
      "    epoch          : 296\n",
      "    loss           : -809803.1646039604\n",
      "    val_loss       : -781373.0332564354\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -867012.875000\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -828280.937500\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -691849.687500\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -811448.125000\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -767553.375000\n",
      "    epoch          : 297\n",
      "    loss           : -815588.0581683168\n",
      "    val_loss       : -827969.7271585464\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -951262.375000\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -826557.812500\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -798615.500000\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -979707.750000\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -840786.875000\n",
      "    epoch          : 298\n",
      "    loss           : -812493.5878712871\n",
      "    val_loss       : -811177.546790576\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -936052.875000\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -799497.062500\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -721066.750000\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -711092.562500\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -781820.187500\n",
      "    epoch          : 299\n",
      "    loss           : -804254.6522277228\n",
      "    val_loss       : -815657.5929269552\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -802831.687500\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -777788.562500\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -773391.687500\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -742451.812500\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -725807.750000\n",
      "    epoch          : 300\n",
      "    loss           : -804724.8459158416\n",
      "    val_loss       : -788685.2733835697\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0327_104948/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -962114.375000\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -668061.187500\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -704082.875000\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -809088.812500\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -841742.000000\n",
      "    epoch          : 301\n",
      "    loss           : -815479.5488861386\n",
      "    val_loss       : -802121.6110657692\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -868954.687500\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -773720.812500\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -771923.250000\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -972236.312500\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -786719.750000\n",
      "    epoch          : 302\n",
      "    loss           : -812745.9443069306\n",
      "    val_loss       : -828497.1725498915\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -834928.875000\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -807711.750000\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -830863.750000\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -955187.250000\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -852921.750000\n",
      "    epoch          : 303\n",
      "    loss           : -811452.9529702971\n",
      "    val_loss       : -810885.1086937904\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -950351.687500\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -836924.312500\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -759408.375000\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -862795.875000\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -724385.687500\n",
      "    epoch          : 304\n",
      "    loss           : -814968.1448019802\n",
      "    val_loss       : -802716.7288295984\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -750954.562500\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -849270.125000\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -831300.312500\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -828176.500000\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -812130.125000\n",
      "    epoch          : 305\n",
      "    loss           : -815104.9214108911\n",
      "    val_loss       : -822603.8071316242\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -963655.750000\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -659823.625000\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -760159.625000\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -853717.250000\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -807782.250000\n",
      "    epoch          : 306\n",
      "    loss           : -826406.6893564357\n",
      "    val_loss       : -822030.0619374275\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -953255.875000\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -810858.125000\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -809882.000000\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -858936.875000\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -798048.000000\n",
      "    epoch          : 307\n",
      "    loss           : -811330.0358910891\n",
      "    val_loss       : -820060.2329778433\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -970667.312500\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -876617.000000\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -689451.875000\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -824160.125000\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -849576.750000\n",
      "    epoch          : 308\n",
      "    loss           : -814543.9195544554\n",
      "    val_loss       : -816133.2390492916\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -992627.937500\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -595808.375000\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -776063.000000\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -824098.437500\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -827458.687500\n",
      "    epoch          : 309\n",
      "    loss           : -821822.6949257426\n",
      "    val_loss       : -804160.0812996387\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -860044.687500\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -844193.750000\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -796715.875000\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -800807.750000\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -762515.937500\n",
      "    epoch          : 310\n",
      "    loss           : -818209.8774752475\n",
      "    val_loss       : -818358.4620063544\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -951037.187500\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -761984.437500\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -776696.312500\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -830744.187500\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -813515.125000\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.0000e-04.\n",
      "    epoch          : 311\n",
      "    loss           : -811949.4245049505\n",
      "    val_loss       : -791771.0754588604\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -879842.812500\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -762621.125000\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -848066.187500\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -765293.375000\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -725942.937500\n",
      "    epoch          : 312\n",
      "    loss           : -784118.375\n",
      "    val_loss       : -782485.5111781597\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -948832.562500\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -861571.500000\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -719737.500000\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -784649.937500\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -789887.750000\n",
      "    epoch          : 313\n",
      "    loss           : -797793.9096534654\n",
      "    val_loss       : -801981.7979763746\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -968160.250000\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -797987.062500\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -859287.312500\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -824880.500000\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -768058.625000\n",
      "    epoch          : 314\n",
      "    loss           : -809528.6466584158\n",
      "    val_loss       : -802391.7345832109\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -968112.250000\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -863684.500000\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -731737.187500\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -798356.125000\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -791625.625000\n",
      "    epoch          : 315\n",
      "    loss           : -803994.604579208\n",
      "    val_loss       : -822516.8830504895\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -786049.312500\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -664702.187500\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -782658.750000\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -719305.812500\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -828791.500000\n",
      "    epoch          : 316\n",
      "    loss           : -812737.3978960396\n",
      "    val_loss       : -802749.1619956612\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -960114.375000\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -772938.937500\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -861138.125000\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -803201.500000\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -758765.687500\n",
      "    epoch          : 317\n",
      "    loss           : -810401.6231435643\n",
      "    val_loss       : -820842.4344122648\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -967706.250000\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -738351.812500\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -754145.875000\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -819121.437500\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -804139.625000\n",
      "    epoch          : 318\n",
      "    loss           : -814445.0625\n",
      "    val_loss       : -804340.9586596012\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -978787.250000\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -869604.687500\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -792806.562500\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -955737.250000\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -779012.187500\n",
      "    epoch          : 319\n",
      "    loss           : -814422.0340346535\n",
      "    val_loss       : -811491.6673243523\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -980729.750000\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -742514.312500\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -771718.625000\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -834151.125000\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -846908.000000\n",
      "    epoch          : 320\n",
      "    loss           : -818376.5996287129\n",
      "    val_loss       : -807214.2530704141\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -962140.250000\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -794003.125000\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -843578.750000\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -743807.312500\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -797839.125000\n",
      "    epoch          : 321\n",
      "    loss           : -817687.5501237623\n",
      "    val_loss       : -837266.7310072898\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -991996.437500\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -737081.875000\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -855145.750000\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -968363.687500\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -807656.812500\n",
      "    epoch          : 322\n",
      "    loss           : -817190.9424504951\n",
      "    val_loss       : -826903.2106547356\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -981960.000000\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -771561.250000\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -844722.937500\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -772539.187500\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -768600.625000\n",
      "    epoch          : 323\n",
      "    loss           : -819957.9121287129\n",
      "    val_loss       : -819457.0065460205\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -982009.125000\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -656967.625000\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -774112.812500\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -799820.687500\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -841820.937500\n",
      "    epoch          : 324\n",
      "    loss           : -808624.1893564357\n",
      "    val_loss       : -818671.9507339239\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -960150.000000\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -773880.250000\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -782401.312500\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -832470.625000\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -793141.500000\n",
      "    epoch          : 325\n",
      "    loss           : -818495.1633663366\n",
      "    val_loss       : -831345.4297001839\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -978543.812500\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -725219.500000\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -750834.750000\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -749033.937500\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -811041.875000\n",
      "    epoch          : 326\n",
      "    loss           : -823356.9461633663\n",
      "    val_loss       : -829383.7006279707\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -753394.625000\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -780087.250000\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -866461.250000\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -790838.437500\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -820702.000000\n",
      "    epoch          : 327\n",
      "    loss           : -820338.4040841584\n",
      "    val_loss       : -833678.4064767361\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -948792.562500\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -840161.125000\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -842128.812500\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -797647.125000\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -787943.562500\n",
      "    epoch          : 328\n",
      "    loss           : -816305.1905940594\n",
      "    val_loss       : -807364.6841648817\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -1003437.375000\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -813581.750000\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -831916.000000\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -783923.812500\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -832541.375000\n",
      "    epoch          : 329\n",
      "    loss           : -819203.9504950495\n",
      "    val_loss       : -815812.3617733002\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -961749.500000\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -792965.625000\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -777428.312500\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -969422.875000\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -780315.625000\n",
      "    epoch          : 330\n",
      "    loss           : -826751.667079208\n",
      "    val_loss       : -819836.9986147166\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -974701.250000\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -830592.812500\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -850475.250000\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -843710.625000\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -808372.687500\n",
      "    epoch          : 331\n",
      "    loss           : -828687.6745049505\n",
      "    val_loss       : -813583.3757202148\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -983107.375000\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -853307.562500\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -750066.500000\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -734304.562500\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -823649.875000\n",
      "    epoch          : 332\n",
      "    loss           : -828222.8551980198\n",
      "    val_loss       : -813058.3080999613\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -863370.750000\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -791949.625000\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -752060.812500\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -766422.250000\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -776398.687500\n",
      "    epoch          : 333\n",
      "    loss           : -824842.7196782178\n",
      "    val_loss       : -807665.7517909289\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -976836.187500\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -784034.875000\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -852608.875000\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -792395.062500\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -795755.812500\n",
      "    epoch          : 334\n",
      "    loss           : -823058.3737623763\n",
      "    val_loss       : -809537.4990231276\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -993176.250000\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -835955.312500\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -892562.250000\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -798257.062500\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -667604.437500\n",
      "    epoch          : 335\n",
      "    loss           : -821733.3780940594\n",
      "    val_loss       : -838501.1012539864\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -984044.312500\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -868586.500000\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -790538.875000\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -738534.375000\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -819272.812500\n",
      "    epoch          : 336\n",
      "    loss           : -822066.6126237623\n",
      "    val_loss       : -793638.4366877794\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -960412.500000\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -763362.812500\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -798709.937500\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -996095.375000\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -797746.875000\n",
      "    epoch          : 337\n",
      "    loss           : -825736.6961633663\n",
      "    val_loss       : -822762.5028542519\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -989398.750000\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -806438.312500\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -896064.812500\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -797087.812500\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -821713.500000\n",
      "    epoch          : 338\n",
      "    loss           : -820020.0990099009\n",
      "    val_loss       : -811588.7060325623\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -834846.062500\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -780440.375000\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -851004.812500\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -777080.625000\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -792810.250000\n",
      "    epoch          : 339\n",
      "    loss           : -824173.176980198\n",
      "    val_loss       : -811565.5690581321\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -829855.437500\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -792212.250000\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -877998.500000\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -788385.937500\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -858870.937500\n",
      "    epoch          : 340\n",
      "    loss           : -828278.1256188119\n",
      "    val_loss       : -821208.4907178998\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -752935.437500\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -686964.562500\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -833707.750000\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -854379.750000\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -830535.000000\n",
      "    epoch          : 341\n",
      "    loss           : -832022.1150990099\n",
      "    val_loss       : -828628.1368643045\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -943772.875000\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -865180.437500\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -884037.812500\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -830272.062500\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -835180.375000\n",
      "    epoch          : 342\n",
      "    loss           : -827814.6676980198\n",
      "    val_loss       : -825176.8300479889\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -992406.562500\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -877258.187500\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -793086.250000\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -759616.250000\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -836924.312500\n",
      "    epoch          : 343\n",
      "    loss           : -827734.2803217822\n",
      "    val_loss       : -838579.9267675638\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -930179.625000\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -838825.187500\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -827181.625000\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -796050.625000\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -807539.875000\n",
      "    epoch          : 344\n",
      "    loss           : -823809.9993811881\n",
      "    val_loss       : -826341.574725604\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -957882.937500\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -748261.187500\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -728637.750000\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -942836.187500\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -806499.500000\n",
      "    epoch          : 345\n",
      "    loss           : -821166.5464108911\n",
      "    val_loss       : -812629.0464406013\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -990120.812500\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -812373.812500\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -747924.375000\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -740600.375000\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -857683.125000\n",
      "    epoch          : 346\n",
      "    loss           : -824643.0129950495\n",
      "    val_loss       : -817838.5680966139\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -796248.000000\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -704194.562500\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -787028.187500\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -824621.500000\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -712863.312500\n",
      "    epoch          : 347\n",
      "    loss           : -824446.9350247525\n",
      "    val_loss       : -819953.1027381181\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -970863.750000\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -801975.375000\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -777184.812500\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -715009.937500\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -832186.750000\n",
      "    epoch          : 348\n",
      "    loss           : -829908.8502475248\n",
      "    val_loss       : -813157.4253061533\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -991173.750000\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -808882.937500\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -797341.000000\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -825636.750000\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -796946.062500\n",
      "    epoch          : 349\n",
      "    loss           : -827162.6293316832\n",
      "    val_loss       : -810326.0593728542\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -962181.562500\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -895343.062500\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -809987.750000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -762012.875000\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -867045.062500\n",
      "    epoch          : 350\n",
      "    loss           : -830251.3112623763\n",
      "    val_loss       : -830949.6252933502\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0327_104948/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -957019.625000\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -771264.375000\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -735231.875000\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -883199.187500\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -852537.312500\n",
      "    epoch          : 351\n",
      "    loss           : -830036.0637376237\n",
      "    val_loss       : -838031.7040990352\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -958325.500000\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -805330.937500\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -770323.187500\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -953635.250000\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -813873.500000\n",
      "    epoch          : 352\n",
      "    loss           : -828624.5655940594\n",
      "    val_loss       : -802466.2829620838\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -990339.437500\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -857299.187500\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -760923.687500\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -854152.937500\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -825600.000000\n",
      "    epoch          : 353\n",
      "    loss           : -831145.2388613861\n",
      "    val_loss       : -821206.1919074773\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -986847.250000\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -788045.625000\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -842165.187500\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -850482.437500\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -717963.625000\n",
      "    epoch          : 354\n",
      "    loss           : -828139.9436881188\n",
      "    val_loss       : -823519.1193167449\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -887385.687500\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -804641.062500\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -745975.125000\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -821138.187500\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -808952.187500\n",
      "    epoch          : 355\n",
      "    loss           : -826478.9659653465\n",
      "    val_loss       : -818009.7595330358\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -970771.562500\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -843215.750000\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -787365.500000\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -848657.375000\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -804398.000000\n",
      "    epoch          : 356\n",
      "    loss           : -830984.7685643565\n",
      "    val_loss       : -851996.6148453236\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -992450.250000\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -911165.375000\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -787763.500000\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -747138.687500\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -841482.625000\n",
      "    epoch          : 357\n",
      "    loss           : -828698.6163366337\n",
      "    val_loss       : -833257.7118575334\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -982930.000000\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -825137.500000\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -855573.437500\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -856421.500000\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -787012.437500\n",
      "    epoch          : 358\n",
      "    loss           : -830720.5228960396\n",
      "    val_loss       : -841251.0273026467\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -951430.937500\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -814260.500000\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -883463.000000\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -857222.750000\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -780299.500000\n",
      "    epoch          : 359\n",
      "    loss           : -823138.0327970297\n",
      "    val_loss       : -808561.0100453853\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -945601.125000\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -891550.312500\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -828707.750000\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -803911.937500\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -850090.437500\n",
      "    epoch          : 360\n",
      "    loss           : -826101.7574257426\n",
      "    val_loss       : -811324.4669985771\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -868403.875000\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -796801.750000\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -792578.812500\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -871438.375000\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -826696.875000\n",
      "    epoch          : 361\n",
      "    loss           : -835007.1138613861\n",
      "    val_loss       : -822583.4683638572\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -880319.562500\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -785930.187500\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -792685.250000\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -802052.562500\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -823839.562500\n",
      "    epoch          : 362\n",
      "    loss           : -822140.4956683168\n",
      "    val_loss       : -818813.9397614598\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -990635.250000\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -804498.500000\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -849097.437500\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -832050.750000\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -840882.625000\n",
      "    epoch          : 363\n",
      "    loss           : -826336.729579208\n",
      "    val_loss       : -805119.9144636154\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -978453.125000\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -807097.500000\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -813369.187500\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -888165.250000\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -872210.375000\n",
      "    epoch          : 364\n",
      "    loss           : -831675.6658415842\n",
      "    val_loss       : -815765.2950987816\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -947871.500000\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -668951.812500\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -841455.375000\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -875487.000000\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -847760.000000\n",
      "    epoch          : 365\n",
      "    loss           : -830025.0587871287\n",
      "    val_loss       : -817694.2522403479\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -884564.937500\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -855700.125000\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -806777.187500\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -771186.875000\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -847284.625000\n",
      "    epoch          : 366\n",
      "    loss           : -830494.0928217822\n",
      "    val_loss       : -839108.2890848636\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -990839.250000\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -817290.312500\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -823561.375000\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -871435.562500\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -807579.187500\n",
      "    epoch          : 367\n",
      "    loss           : -830492.6070544554\n",
      "    val_loss       : -827938.5928560257\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -949094.187500\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -924051.000000\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -870538.875000\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -813412.750000\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -827592.500000\n",
      "    epoch          : 368\n",
      "    loss           : -831131.0866336634\n",
      "    val_loss       : -845179.0370303631\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -983891.562500\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -682077.500000\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -861140.187500\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -876617.000000\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -868556.125000\n",
      "    epoch          : 369\n",
      "    loss           : -825226.2636138614\n",
      "    val_loss       : -820237.399234581\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -982877.187500\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -849447.500000\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -857022.937500\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -853632.812500\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -846271.937500\n",
      "    epoch          : 370\n",
      "    loss           : -830012.2858910891\n",
      "    val_loss       : -808441.2023264884\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -994276.625000\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -825945.625000\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -855339.187500\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -895008.062500\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -665466.562500\n",
      "    epoch          : 371\n",
      "    loss           : -825470.156559406\n",
      "    val_loss       : -818811.0264469146\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -867838.750000\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -732377.750000\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -770848.312500\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -990823.562500\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -862321.812500\n",
      "    epoch          : 372\n",
      "    loss           : -830121.9306930694\n",
      "    val_loss       : -829158.7463730931\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -974012.375000\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -914845.875000\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -872319.375000\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -887845.125000\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -798590.375000\n",
      "    epoch          : 373\n",
      "    loss           : -830854.5816831683\n",
      "    val_loss       : -818527.3271208763\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -866006.125000\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -802746.000000\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -778084.687500\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -855749.125000\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -855673.437500\n",
      "    epoch          : 374\n",
      "    loss           : -822408.5724009901\n",
      "    val_loss       : -837280.2782462358\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -991207.875000\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -826099.687500\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -807181.687500\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -880788.562500\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -724639.375000\n",
      "    epoch          : 375\n",
      "    loss           : -824467.5841584158\n",
      "    val_loss       : -812987.1455784797\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -972319.375000\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -862460.250000\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -862112.500000\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -737665.500000\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -694184.187500\n",
      "    epoch          : 376\n",
      "    loss           : -828013.9022277228\n",
      "    val_loss       : -838576.7837903261\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -809436.250000\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -825643.437500\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -785091.125000\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -802029.500000\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -831881.875000\n",
      "    epoch          : 377\n",
      "    loss           : -836056.9752475248\n",
      "    val_loss       : -813853.2881753445\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -986750.437500\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -924219.312500\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -827386.750000\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -733426.125000\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -681589.375000\n",
      "    epoch          : 378\n",
      "    loss           : -834746.280940594\n",
      "    val_loss       : -824987.9189473152\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -981644.312500\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -709535.937500\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -854856.187500\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -790216.000000\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -843768.937500\n",
      "    epoch          : 379\n",
      "    loss           : -832715.9133663366\n",
      "    val_loss       : -839174.6140025854\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -1008918.125000\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -768219.437500\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -866269.000000\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -759143.500000\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -845720.875000\n",
      "    epoch          : 380\n",
      "    loss           : -833223.3422029703\n",
      "    val_loss       : -834674.0066950083\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -975696.500000\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -801837.687500\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -818869.812500\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -807334.000000\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -857874.812500\n",
      "    epoch          : 381\n",
      "    loss           : -837191.5272277228\n",
      "    val_loss       : -822157.0266533613\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -987192.312500\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -867699.937500\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -784898.875000\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -853055.562500\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -684924.750000\n",
      "    epoch          : 382\n",
      "    loss           : -831343.6027227723\n",
      "    val_loss       : -824774.0118340969\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -984523.250000\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -775749.000000\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -807050.625000\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -856886.625000\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -795464.437500\n",
      "    epoch          : 383\n",
      "    loss           : -832318.9900990099\n",
      "    val_loss       : -832322.4349892854\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -984080.250000\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -785671.125000\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -886184.437500\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -776938.062500\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -854778.625000\n",
      "    epoch          : 384\n",
      "    loss           : -829462.5847772277\n",
      "    val_loss       : -827381.2174879551\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -946119.437500\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -897761.750000\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -830619.625000\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -850861.000000\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -854763.875000\n",
      "    epoch          : 385\n",
      "    loss           : -833080.1021039604\n",
      "    val_loss       : -829398.3029448747\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -983977.812500\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -811892.250000\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -835678.812500\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -863547.312500\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -815962.000000\n",
      "    epoch          : 386\n",
      "    loss           : -830345.7493811881\n",
      "    val_loss       : -825742.1926145554\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -880785.250000\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -879360.625000\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -789778.375000\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -696459.187500\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -730838.000000\n",
      "    epoch          : 387\n",
      "    loss           : -830514.9176980198\n",
      "    val_loss       : -821295.2294451117\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -799227.375000\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -768218.250000\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -875501.750000\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -865906.812500\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -817972.500000\n",
      "    epoch          : 388\n",
      "    loss           : -832992.2530940594\n",
      "    val_loss       : -823010.4280929565\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -978205.687500\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -808061.625000\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -861963.750000\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -828615.000000\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -840548.687500\n",
      "    epoch          : 389\n",
      "    loss           : -832084.2172029703\n",
      "    val_loss       : -811347.7179582119\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -798606.312500\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -803394.062500\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -828654.375000\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -848435.312500\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -847012.625000\n",
      "    epoch          : 390\n",
      "    loss           : -832514.1033415842\n",
      "    val_loss       : -835044.1872762442\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -982625.937500\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -890708.312500\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -767022.687500\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -846057.437500\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -828133.937500\n",
      "    epoch          : 391\n",
      "    loss           : -831136.5074257426\n",
      "    val_loss       : -834911.0240574598\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -884193.562500\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -711928.187500\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -784373.562500\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -870046.625000\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -814042.875000\n",
      "    epoch          : 392\n",
      "    loss           : -834286.0569306931\n",
      "    val_loss       : -831360.0102411747\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -986139.562500\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -902006.312500\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -685297.437500\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -795226.250000\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -862035.187500\n",
      "    epoch          : 393\n",
      "    loss           : -831629.9461633663\n",
      "    val_loss       : -852724.4796617031\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -900666.062500\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -814106.375000\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -877725.500000\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -846074.375000\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -844301.687500\n",
      "    epoch          : 394\n",
      "    loss           : -826729.9857673268\n",
      "    val_loss       : -807998.280581522\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -787151.437500\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -784128.750000\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -796468.187500\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -982796.062500\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -820146.250000\n",
      "    epoch          : 395\n",
      "    loss           : -829510.9783415842\n",
      "    val_loss       : -846746.2466680526\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -779213.000000\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -907325.875000\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -793232.062500\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -844382.687500\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -817291.250000\n",
      "    epoch          : 396\n",
      "    loss           : -836286.5247524752\n",
      "    val_loss       : -837465.4092119217\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -957038.312500\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -876288.562500\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -804196.250000\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -816700.562500\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -852114.812500\n",
      "    epoch          : 397\n",
      "    loss           : -835306.3323019802\n",
      "    val_loss       : -836107.867856574\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -972187.000000\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -789033.187500\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -787440.875000\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -722057.437500\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -856381.750000\n",
      "    epoch          : 398\n",
      "    loss           : -834462.8849009901\n",
      "    val_loss       : -824801.8658941507\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -980475.625000\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -860755.937500\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -768114.375000\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -869328.937500\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -820376.250000\n",
      "    epoch          : 399\n",
      "    loss           : -835443.6689356435\n",
      "    val_loss       : -835570.8423242569\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -989965.000000\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -775083.687500\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -810551.000000\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -768162.062500\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -773894.500000\n",
      "    epoch          : 400\n",
      "    loss           : -826770.6268564357\n",
      "    val_loss       : -828339.6524523497\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0327_104948/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -984196.812500\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -813577.125000\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -839434.875000\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -798652.500000\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -802881.500000\n",
      "    epoch          : 401\n",
      "    loss           : -820757.6138613861\n",
      "    val_loss       : -831370.318525672\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -993324.812500\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -794537.750000\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -877500.000000\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -834243.375000\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -794768.812500\n",
      "    epoch          : 402\n",
      "    loss           : -833096.5030940594\n",
      "    val_loss       : -840769.515231061\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -999466.562500\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -771062.312500\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -781480.687500\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -803869.437500\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -845049.125000\n",
      "    epoch          : 403\n",
      "    loss           : -834007.4882425743\n",
      "    val_loss       : -842788.039051485\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -896787.750000\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -891312.375000\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -808917.750000\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -977028.875000\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -781653.250000\n",
      "    epoch          : 404\n",
      "    loss           : -838507.718440594\n",
      "    val_loss       : -825951.7336180687\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -874326.937500\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -835225.250000\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -817836.812500\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -727214.812500\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -855122.125000\n",
      "    epoch          : 405\n",
      "    loss           : -829177.2970297029\n",
      "    val_loss       : -832521.746369338\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -960460.250000\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -893137.875000\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -810223.625000\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -864098.312500\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -868331.250000\n",
      "    epoch          : 406\n",
      "    loss           : -833469.4189356435\n",
      "    val_loss       : -830215.4791288615\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -1002696.000000\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -777071.937500\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -797951.625000\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -998933.125000\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -706003.375000\n",
      "    epoch          : 407\n",
      "    loss           : -826402.4882425743\n",
      "    val_loss       : -844254.4244523526\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -988445.125000\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -769811.125000\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -785451.500000\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -805615.937500\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -791768.250000\n",
      "    epoch          : 408\n",
      "    loss           : -830290.1986386139\n",
      "    val_loss       : -799290.439145422\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -977858.750000\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -856151.375000\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -879304.000000\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -651602.187500\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -845335.875000\n",
      "    epoch          : 409\n",
      "    loss           : -836652.9900990099\n",
      "    val_loss       : -830821.6589750051\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -995679.187500\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -784484.125000\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -736435.312500\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -828957.500000\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -777770.875000\n",
      "    epoch          : 410\n",
      "    loss           : -823983.7011138614\n",
      "    val_loss       : -820316.2841555834\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -986008.875000\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -809460.500000\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -784187.250000\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -815382.750000\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -867317.312500\n",
      "    epoch          : 411\n",
      "    loss           : -829568.2865099009\n",
      "    val_loss       : -840098.8561435938\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -919724.125000\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -764473.812500\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -861891.375000\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -845990.937500\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -767203.750000\n",
      "    epoch          : 412\n",
      "    loss           : -832472.0470297029\n",
      "    val_loss       : -823710.2909161806\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -890285.687500\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -798049.875000\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -812479.750000\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -828611.562500\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -813146.312500\n",
      "    epoch          : 413\n",
      "    loss           : -821103.9313118812\n",
      "    val_loss       : -822302.3797117949\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -988473.062500\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -735747.312500\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -828717.125000\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -774354.625000\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -808932.500000\n",
      "    epoch          : 414\n",
      "    loss           : -836350.2797029703\n",
      "    val_loss       : -820278.8967746735\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -900833.750000\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -783405.812500\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -755568.750000\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -984410.500000\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -789414.250000\n",
      "    epoch          : 415\n",
      "    loss           : -835293.4251237623\n",
      "    val_loss       : -824717.1721677065\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -970886.750000\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -824204.250000\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -789523.812500\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -968327.750000\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -830632.312500\n",
      "    epoch          : 416\n",
      "    loss           : -835491.7667079208\n",
      "    val_loss       : -831466.3436102628\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -958234.500000\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -803270.750000\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -830982.375000\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -855559.625000\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -704976.187500\n",
      "    epoch          : 417\n",
      "    loss           : -835521.4900990099\n",
      "    val_loss       : -836522.7217908144\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -991456.000000\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -873737.125000\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -779516.500000\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -836470.625000\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -800832.000000\n",
      "    epoch          : 418\n",
      "    loss           : -832657.1522277228\n",
      "    val_loss       : -816017.9924470067\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -983082.437500\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -902299.750000\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -870900.750000\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -870936.000000\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -822497.500000\n",
      "    epoch          : 419\n",
      "    loss           : -834187.166460396\n",
      "    val_loss       : -836664.4842845679\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -820067.500000\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -785325.187500\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -795946.875000\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -815998.750000\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -854523.500000\n",
      "    epoch          : 420\n",
      "    loss           : -836832.9368811881\n",
      "    val_loss       : -845255.8272401333\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -988707.375000\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -787212.437500\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -848461.250000\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -642926.562500\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -856787.500000\n",
      "    epoch          : 421\n",
      "    loss           : -824969.3248762377\n",
      "    val_loss       : -819931.5391296148\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -990729.125000\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -809661.125000\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -729402.937500\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -874253.812500\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -848444.000000\n",
      "    epoch          : 422\n",
      "    loss           : -834072.8873762377\n",
      "    val_loss       : -830099.8055971503\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -969590.687500\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -779080.687500\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -803650.750000\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -871441.125000\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -839278.812500\n",
      "    epoch          : 423\n",
      "    loss           : -831682.3626237623\n",
      "    val_loss       : -809322.5432576656\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -976318.125000\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -881384.937500\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -824669.500000\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -740084.437500\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -784792.062500\n",
      "    epoch          : 424\n",
      "    loss           : -830042.6689356435\n",
      "    val_loss       : -823685.6541962385\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -984138.562500\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -798922.750000\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -867385.125000\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -856898.937500\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -845027.750000\n",
      "    epoch          : 425\n",
      "    loss           : -832337.2004950495\n",
      "    val_loss       : -813458.5681888104\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -974739.125000\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -787551.625000\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -862601.812500\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -880028.437500\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -644860.625000\n",
      "    epoch          : 426\n",
      "    loss           : -834957.4653465346\n",
      "    val_loss       : -817194.164107132\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -991834.125000\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -821472.812500\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -767663.312500\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -984157.625000\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -807727.375000\n",
      "    epoch          : 427\n",
      "    loss           : -828868.1738861386\n",
      "    val_loss       : -843032.8322212696\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -848140.312500\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -721890.625000\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -677740.750000\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -972327.812500\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -828988.000000\n",
      "    epoch          : 428\n",
      "    loss           : -829768.1509900991\n",
      "    val_loss       : -844051.0772777081\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -612316.375000\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -832180.500000\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -811088.125000\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -800152.562500\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -815803.625000\n",
      "    epoch          : 429\n",
      "    loss           : -832860.0711633663\n",
      "    val_loss       : -840559.9445398807\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -964541.500000\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -836475.062500\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -753643.000000\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -873944.750000\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -872645.500000\n",
      "    epoch          : 430\n",
      "    loss           : -829851.6775990099\n",
      "    val_loss       : -843084.3250675083\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -992931.500000\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -871060.875000\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -867836.000000\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -810118.375000\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -704335.125000\n",
      "    epoch          : 431\n",
      "    loss           : -832730.9201732674\n",
      "    val_loss       : -832499.8986074686\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -961602.625000\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -781086.812500\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -845815.562500\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -851788.187500\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -790943.500000\n",
      "    epoch          : 432\n",
      "    loss           : -834382.020420792\n",
      "    val_loss       : -817002.8035289526\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -871559.187500\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -844159.937500\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -832840.250000\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -824682.875000\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -761285.000000\n",
      "    epoch          : 433\n",
      "    loss           : -833758.9931930694\n",
      "    val_loss       : -821627.2077016353\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -885279.000000\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -819384.625000\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -770661.250000\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -804125.500000\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -821235.125000\n",
      "    epoch          : 434\n",
      "    loss           : -830634.4096534654\n",
      "    val_loss       : -839825.2825377465\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -992441.500000\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -660452.000000\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -780302.375000\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -794133.375000\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -797856.000000\n",
      "    epoch          : 435\n",
      "    loss           : -832553.5136138614\n",
      "    val_loss       : -838233.1806138993\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -943211.625000\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -791081.875000\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -888162.187500\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -877892.812500\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -823077.000000\n",
      "    epoch          : 436\n",
      "    loss           : -830227.323019802\n",
      "    val_loss       : -830249.7532354116\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -972406.625000\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -779303.875000\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -791637.875000\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -792510.625000\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -850325.875000\n",
      "    epoch          : 437\n",
      "    loss           : -835610.9003712871\n",
      "    val_loss       : -815157.8535660028\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -729152.375000\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -893314.937500\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -806139.187500\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -742936.062500\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -831668.375000\n",
      "    epoch          : 438\n",
      "    loss           : -830080.2896039604\n",
      "    val_loss       : -820738.9947203398\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -953102.437500\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -890053.312500\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -831821.937500\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -804978.750000\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -816848.875000\n",
      "    epoch          : 439\n",
      "    loss           : -837704.5346534654\n",
      "    val_loss       : -851762.9392490864\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -980774.500000\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -666228.500000\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -743803.187500\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -820186.750000\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -821858.937500\n",
      "    epoch          : 440\n",
      "    loss           : -828812.0136138614\n",
      "    val_loss       : -842611.7070592403\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -955401.125000\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -798578.875000\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -878985.937500\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -824286.250000\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -866739.812500\n",
      "    epoch          : 441\n",
      "    loss           : -836880.1268564357\n",
      "    val_loss       : -834801.0674502135\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -989480.250000\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -718228.937500\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -869020.875000\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -984577.875000\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -853918.625000\n",
      "    epoch          : 442\n",
      "    loss           : -831119.854579208\n",
      "    val_loss       : -844141.3758882284\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -1001384.875000\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -804586.875000\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -827502.875000\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -711141.125000\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -851769.562500\n",
      "    epoch          : 443\n",
      "    loss           : -833708.1311881188\n",
      "    val_loss       : -839827.1696356058\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -866638.687500\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -796951.500000\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -852094.250000\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -784671.562500\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -785313.437500\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00444: reducing learning rate of group 0 to 1.0000e-05.\n",
      "    epoch          : 444\n",
      "    loss           : -836309.7586633663\n",
      "    val_loss       : -846384.3260301829\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -992520.187500\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -819710.000000\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -794902.250000\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -862245.937500\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -768943.312500\n",
      "    epoch          : 445\n",
      "    loss           : -832674.6150990099\n",
      "    val_loss       : -841674.8408976793\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -843033.937500\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -819756.562500\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -858016.250000\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -879781.187500\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -847929.500000\n",
      "    epoch          : 446\n",
      "    loss           : -833571.9300742574\n",
      "    val_loss       : -845246.8338147283\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -879094.875000\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -747611.500000\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -865678.875000\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -849781.375000\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -760529.125000\n",
      "    epoch          : 447\n",
      "    loss           : -829516.635519802\n",
      "    val_loss       : -844892.9472444772\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -992621.750000\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -905882.187500\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -771459.562500\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -978340.875000\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -883860.000000\n",
      "    epoch          : 448\n",
      "    loss           : -838254.9331683168\n",
      "    val_loss       : -831687.0585582256\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -896519.437500\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -887495.312500\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -857539.250000\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -882305.062500\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -800704.250000\n",
      "    epoch          : 449\n",
      "    loss           : -833611.6887376237\n",
      "    val_loss       : -835393.6752855063\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -985305.000000\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -832966.125000\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -851372.687500\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -766534.687500\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -857106.000000\n",
      "    epoch          : 450\n",
      "    loss           : -831976.6503712871\n",
      "    val_loss       : -842995.5455135346\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0327_104948/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -967842.375000\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -847316.250000\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -894500.562500\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -1002231.500000\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -805995.625000\n",
      "    epoch          : 451\n",
      "    loss           : -833482.3007425743\n",
      "    val_loss       : -821285.3878313542\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -977539.125000\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -745648.750000\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -798192.875000\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -794396.000000\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -835904.500000\n",
      "    epoch          : 452\n",
      "    loss           : -834772.4981435643\n",
      "    val_loss       : -839288.9401068449\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -875976.750000\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -832246.812500\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -825113.062500\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -810422.500000\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -861837.937500\n",
      "    epoch          : 453\n",
      "    loss           : -837663.4517326732\n",
      "    val_loss       : -817546.8509451628\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -961399.312500\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -817205.250000\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -816173.437500\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -848283.750000\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -814131.062500\n",
      "    epoch          : 454\n",
      "    loss           : -833515.6856435643\n",
      "    val_loss       : -838354.5249459982\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -953903.625000\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -795966.687500\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -811841.562500\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -841238.375000\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -795935.125000\n",
      "    epoch          : 455\n",
      "    loss           : -839825.2896039604\n",
      "    val_loss       : -860053.457698226\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -979372.937500\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -694939.312500\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -875860.750000\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -829528.375000\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -832443.500000\n",
      "    epoch          : 456\n",
      "    loss           : -835369.3533415842\n",
      "    val_loss       : -833230.257303071\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -986757.562500\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -814198.562500\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -776247.875000\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -955673.875000\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -852937.562500\n",
      "    epoch          : 457\n",
      "    loss           : -833271.7586633663\n",
      "    val_loss       : -837225.1157446622\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -980423.000000\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -686676.937500\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -808527.875000\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -803148.500000\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -825344.375000\n",
      "    epoch          : 458\n",
      "    loss           : -836187.0365099009\n",
      "    val_loss       : -836150.164031291\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -979356.000000\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -858201.875000\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -812110.062500\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -875875.875000\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -830727.937500\n",
      "    epoch          : 459\n",
      "    loss           : -837103.291460396\n",
      "    val_loss       : -846949.3115533352\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -962309.812500\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -805458.812500\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -864861.625000\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -846258.375000\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -791159.125000\n",
      "    epoch          : 460\n",
      "    loss           : -838106.9071782178\n",
      "    val_loss       : -836628.2244276047\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -978830.875000\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -761387.312500\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -753342.562500\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -862701.000000\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -810539.625000\n",
      "    epoch          : 461\n",
      "    loss           : -838544.9839108911\n",
      "    val_loss       : -817591.7967882395\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -971462.750000\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -643411.687500\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -805727.312500\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -818978.687500\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -834914.875000\n",
      "    epoch          : 462\n",
      "    loss           : -834207.1967821782\n",
      "    val_loss       : -838691.4683868885\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -908790.062500\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -808122.625000\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -887819.250000\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -840522.500000\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -832866.250000\n",
      "    epoch          : 463\n",
      "    loss           : -836173.4424504951\n",
      "    val_loss       : -843498.8115757226\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -960374.062500\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -899853.125000\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -847896.625000\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -717015.312500\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -854125.500000\n",
      "    epoch          : 464\n",
      "    loss           : -837027.1782178218\n",
      "    val_loss       : -852071.1267295361\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -968448.437500\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -857072.125000\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -868411.312500\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -987457.437500\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -849933.812500\n",
      "    epoch          : 465\n",
      "    loss           : -841879.8675742574\n",
      "    val_loss       : -841333.4569557428\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -824776.500000\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -918959.812500\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -879293.187500\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -757611.375000\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -866555.500000\n",
      "    epoch          : 466\n",
      "    loss           : -837689.4375\n",
      "    val_loss       : -848182.1519420386\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -908139.062500\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -806433.875000\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -836093.000000\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -813885.312500\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -805986.125000\n",
      "    epoch          : 467\n",
      "    loss           : -837644.2722772277\n",
      "    val_loss       : -822363.3836888552\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -962684.500000\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -697329.687500\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -820627.750000\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -794872.500000\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -836061.375000\n",
      "    epoch          : 468\n",
      "    loss           : -833721.5872524752\n",
      "    val_loss       : -813925.9092526675\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -975360.062500\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -806676.625000\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -852505.687500\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -807791.000000\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -831610.375000\n",
      "    epoch          : 469\n",
      "    loss           : -833014.416460396\n",
      "    val_loss       : -836687.0651708841\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -928849.750000\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -776560.062500\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -706927.437500\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -995157.375000\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -797036.000000\n",
      "    epoch          : 470\n",
      "    loss           : -836128.9566831683\n",
      "    val_loss       : -851951.4197741747\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -980415.250000\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -910215.875000\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -827282.187500\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -811069.250000\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -744120.000000\n",
      "    epoch          : 471\n",
      "    loss           : -837122.792079208\n",
      "    val_loss       : -832910.8679029464\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -830619.187500\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -831253.750000\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -866455.375000\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -987105.312500\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -821038.812500\n",
      "    epoch          : 472\n",
      "    loss           : -835842.7667079208\n",
      "    val_loss       : -832118.5889644623\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -914908.312500\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -792641.625000\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -795272.375000\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -824879.562500\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -856537.250000\n",
      "    epoch          : 473\n",
      "    loss           : -838633.8762376237\n",
      "    val_loss       : -845814.9763868332\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -988395.812500\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -769384.062500\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -883735.125000\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -823991.000000\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -850916.562500\n",
      "    epoch          : 474\n",
      "    loss           : -838241.6782178218\n",
      "    val_loss       : -834257.7595681071\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -976379.000000\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -875466.187500\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -843642.125000\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -875666.750000\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -824934.625000\n",
      "    epoch          : 475\n",
      "    loss           : -837788.167079208\n",
      "    val_loss       : -825177.2123401165\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -810300.625000\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -901294.375000\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -789060.437500\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -786721.375000\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -792422.812500\n",
      "    epoch          : 476\n",
      "    loss           : -833805.0711633663\n",
      "    val_loss       : -854501.8448304653\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -983608.750000\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -782791.875000\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -840671.187500\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -859252.812500\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -845830.125000\n",
      "    epoch          : 477\n",
      "    loss           : -839929.6435643565\n",
      "    val_loss       : -825883.1908330203\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -973288.750000\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -849214.437500\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -821750.437500\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -800425.562500\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -765507.187500\n",
      "    epoch          : 478\n",
      "    loss           : -836236.7821782178\n",
      "    val_loss       : -838090.8560288191\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -895574.000000\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -806586.937500\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -868377.187500\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -732050.875000\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -832797.312500\n",
      "    epoch          : 479\n",
      "    loss           : -835716.6225247525\n",
      "    val_loss       : -830682.483913064\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -979328.750000\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -884552.687500\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -812488.625000\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -779362.187500\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -830583.625000\n",
      "    epoch          : 480\n",
      "    loss           : -840245.2271039604\n",
      "    val_loss       : -823901.7975741863\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -878410.875000\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -905523.125000\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -869290.875000\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -895082.687500\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -807488.125000\n",
      "    epoch          : 481\n",
      "    loss           : -835709.5612623763\n",
      "    val_loss       : -846604.4360740185\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -1003913.250000\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -821302.625000\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -798304.437500\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -1001899.125000\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -847099.625000\n",
      "    epoch          : 482\n",
      "    loss           : -845427.9399752475\n",
      "    val_loss       : -819488.6875410795\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -991978.625000\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -867213.625000\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -859362.250000\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -978521.000000\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -848983.312500\n",
      "    epoch          : 483\n",
      "    loss           : -837438.9356435643\n",
      "    val_loss       : -814596.1958844423\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -1002861.687500\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -807174.000000\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -855502.000000\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -826345.187500\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -734669.750000\n",
      "    epoch          : 484\n",
      "    loss           : -833024.1212871287\n",
      "    val_loss       : -826595.2372713566\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -873435.750000\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -777809.125000\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -860238.500000\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -867267.062500\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -811180.437500\n",
      "    epoch          : 485\n",
      "    loss           : -835692.6868811881\n",
      "    val_loss       : -831534.4397325277\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -998061.125000\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -845971.187500\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -833643.500000\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -813513.375000\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -876977.375000\n",
      "    epoch          : 486\n",
      "    loss           : -840196.6311881188\n",
      "    val_loss       : -834991.9907803774\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -976067.812500\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -879538.375000\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -806955.812500\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -981865.312500\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -773115.125000\n",
      "    epoch          : 487\n",
      "    loss           : -837786.0198019802\n",
      "    val_loss       : -841736.4842608928\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -973216.250000\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -883081.875000\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -785334.562500\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -838243.250000\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -870836.500000\n",
      "    epoch          : 488\n",
      "    loss           : -836361.7339108911\n",
      "    val_loss       : -838943.7332974195\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -894954.000000\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -698259.875000\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -800014.375000\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -857993.812500\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -801195.312500\n",
      "    epoch          : 489\n",
      "    loss           : -835129.3298267326\n",
      "    val_loss       : -841032.9942878007\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -986692.375000\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -792020.375000\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -782150.937500\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -877736.312500\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -806055.500000\n",
      "    epoch          : 490\n",
      "    loss           : -840770.4801980198\n",
      "    val_loss       : -828908.9089229346\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -978867.500000\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -893284.875000\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -830359.000000\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -842722.687500\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -866880.750000\n",
      "    epoch          : 491\n",
      "    loss           : -837331.4356435643\n",
      "    val_loss       : -843309.1329123735\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -959360.750000\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -829770.250000\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -829608.625000\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -827502.500000\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -779326.812500\n",
      "    epoch          : 492\n",
      "    loss           : -840851.2227722772\n",
      "    val_loss       : -826111.9502745628\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -724310.500000\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -919217.000000\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -777606.937500\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -847105.937500\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -843039.125000\n",
      "    epoch          : 493\n",
      "    loss           : -833142.8595297029\n",
      "    val_loss       : -819176.9913167476\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -948902.375000\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -791252.500000\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -846953.625000\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -773763.000000\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -779405.750000\n",
      "    epoch          : 494\n",
      "    loss           : -828962.8712871287\n",
      "    val_loss       : -854595.2093096494\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -846328.812500\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -856282.125000\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -827412.750000\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -811363.625000\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -808478.562500\n",
      "    epoch          : 495\n",
      "    loss           : -843307.9876237623\n",
      "    val_loss       : -804306.5346977233\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -979044.875000\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -800342.812500\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -868855.625000\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -770177.375000\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -826157.000000\n",
      "    epoch          : 496\n",
      "    loss           : -836381.9956683168\n",
      "    val_loss       : -819695.8626304626\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -958797.250000\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -804124.687500\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -807552.500000\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -841674.062500\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -792949.500000\n",
      "    epoch          : 497\n",
      "    loss           : -831873.9820544554\n",
      "    val_loss       : -824114.4133318901\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -969197.625000\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -766936.000000\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -874187.562500\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -857786.562500\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -845657.812500\n",
      "    epoch          : 498\n",
      "    loss           : -831145.8849009901\n",
      "    val_loss       : -821617.4124872207\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -974780.937500\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -882582.562500\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -822861.125000\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -791615.750000\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -836418.125000\n",
      "    epoch          : 499\n",
      "    loss           : -840727.5482673268\n",
      "    val_loss       : -823039.3290376902\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -810558.562500\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -789708.437500\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -704546.875000\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -871270.812500\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -770693.625000\n",
      "    epoch          : 500\n",
      "    loss           : -839805.2450495049\n",
      "    val_loss       : -839730.0371718645\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0327_104948/checkpoint-epoch500.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VaeCategoryModel(\n",
       "  (_category): FreeCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=12, bias=True)\n",
       "        (1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (4): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=12, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=20, bias=True)\n",
       "        (1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=20, out_features=20, bias=True)\n",
       "        (4): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=20, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=36, bias=True)\n",
       "        (1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (4): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=36, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=68, bias=True)\n",
       "        (1): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=68, out_features=68, bias=True)\n",
       "        (4): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=68, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=102, bias=True)\n",
       "        (1): LayerNorm((102,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=102, out_features=102, bias=True)\n",
       "        (4): LayerNorm((102,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=102, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=106, bias=True)\n",
       "        (1): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=106, out_features=106, bias=True)\n",
       "        (4): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=106, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_10): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_11): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_12): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_13): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=114, bias=True)\n",
       "        (1): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=114, out_features=114, bias=True)\n",
       "        (4): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=114, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_14): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_15): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_16): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=130, bias=True)\n",
       "        (1): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=130, out_features=130, bias=True)\n",
       "        (4): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=130, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_17): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_18): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=162, bias=True)\n",
       "        (1): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=162, out_features=162, bias=True)\n",
       "        (4): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=162, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_19): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_20): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "    (global_element_5): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=54, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (encoders): ModuleDict(\n",
       "    ($p(Z^{16} | Z^{8})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (3): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=48, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{32} | Z^{8})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (3): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=96, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{64} | Z^{8})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (3): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=192, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{128} | Z^{8})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=384, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | Z^{8})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=16, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | Z^{8})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{32} | Z^{16})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (3): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=96, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{64} | Z^{16})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (3): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=192, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{128} | Z^{16})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=384, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | Z^{16})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | Z^{16})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{64} | Z^{32})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (3): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=192, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{128} | Z^{32})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=384, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | Z^{32})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | Z^{32})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{128} | Z^{64})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=384, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | Z^{64})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | Z^{64})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{196} | Z^{128})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | Z^{128})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(X^{784} | Z^{196})$†): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{128})$†): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{32})$†): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{64})$†): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{8})$†): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196})$†): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{16})$†): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEFCAYAAADOqip4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPkElEQVR4nO3dX4xc51nH8d/j+M8mtpPJ2kldt4mb2TSkTZWK7ZgQ2quyRqitSoU2RRSJSqCuL7jhBke5QtwgbKkEJLjwXrXcgJxFCBQhyq4olAY38cZJWxXaVHGa/0lt704cp4mzth8u9p14Mp7zzuzMzszZx9+PNPLMec6Z8/jYvz1nz5kzr7m7AMS0adQNABgcAg4ERsCBwAg4EBgBBwLbPOoGIjOzaUnjkpYk1SVV3X12wOucknTU3Se6nH9SUk3Sp9z94CB7w/CxBx8QM6tK2u/us+4+p9WQVwa9XndfkHRqDYs8JOlYWcNtZs+OuoeNjIAPTlXS2cYLdz+ptQVvWCruXh91ExmfGnUDGxkBH5xFSQ+Z2aG0N1fak0taPZROj8NmVmmatmxmk+n5UTOrptdHG+/TNN9V79HKzGbSPIda50mH5+NpnqqZTZvZs2n+R5r6mk7TptOvAF332rK+wr7brTv192TT8u36aNszEnfnMaCHpElJ85Jcq/9RK021o+nPKUmHm6bPS5pMzw9LOlQw33vvl9bzSPN7NE0/nJ5XGuts6XG+9XVartr0Hoea+25ab1e9trx/tu/mdbf5u2T7aF6Ox+qDPfgAuftJdz/g7iZpQashaNSaf+ettCzaOJQ/2/R8qc371xvr0WqoWv2OpLNpT1hNj07GU9+N9R6UdLKp/mzLurrqtcu+W9fdLNdHbrlrGgEfkMYhZIO7P6imgKXD0yllgpvUW+trUJF0Mv3nP+nuB7pYJhvOZLzxZB177Xbd7fpY63LXDAI+OJV0mUySlH43PJWez0g666tnvBv1ybWuoOn316pWjxBaPSLpQNP8a15Heo/m5fYXrKtrXfQ9lD6uBQR8wNJJoGlJM5IeTJMXJE207OXHG4fSTSfmDkh6IAXioKSplpNXU+k9Dkr6Wlpf4z1m0g+Qxgmoqw7hW9ZXSfPU0g8gSe9ddqs3Tm5p9ff4Uz302qxd31etu83fpV0fVy2HKyydpMAGY2ZPuvuGu4S0UfveqNiDA4ER8A0oHZZWN9ph6UbteyPjEB0IjD04EBgBBwIb+O2iW22bj2n7oFcDXNPe1PIZd7+ldXpPAU/XIevq4v7mMW3XffbrvawGQJcWfO75dtPXfIje+HRW41NY7T5AAaAcevkdfL+u3FRwSu//+KCk925RXDSzxRVd6Kc/AH3oJeCVlte7Wmfw1W8xqbl7bYu29dQYgP71EvC6mu4mAlBevQT8hK7sxatavdkeQAmtOeC++rVD1XRyrdJ8yyOAcunpMpm7H0lPCTdQYnySDQiMgAOBEXAgMAIOBEbAgcAIOBAYAQcCI+BAYAQcCIyAA4ERcCAwAg4ERsCBwAg4EBgBBwIj4EBgBBwIjIADgRFwIDACDgRGwIHACDgQGAEHAiPgQGAEHAiMgAOBEXAgMAIOBEbAgcB6Gl0U17ZN27dn63b73uLiysXssm/dfUu2fsN//V+2funeieK+Hns6u2xE7MGBwHoKuJktm9m8mR1a74YArJ9eD9EfcPeFde0EwLrr9RC9YmbVoqKZzZjZopktruhCj6sA0K9eAz4uacnMjrYruvusu9fcvbZF23rvDkBfegp4CnBdUt3Mpte3JQDrZc0BT4ffk4NoBsD66uUk2zFJ1cae293n1rcldGPzB/cU1p77g8LTI5Kkd/ZeytZ9LF/XRcvXt14uLP3yxAvZRcd0Plv/zF+8nK3/zX98vLC2/f5fyy572zd/mq1fOn06Wy+jNQc8HZqfTA/CDZQYH3QBAiPgQGAEHAiMgAOBEXAgMG4XHZFNn7g7W//Z9Hi2fv3k2cLazMS/Zpf94ZsfztYXX7stW79uU/FlMEm6c/xMYe0rH3g8u+yXtucvk3Wybar47/71b38uu+zLX/lotr7nrzfeZTL24EBgBBwIjIADgRFwIDACDgRGwIHACDgQGNfBR+SVA/nr3Jc/1uG2yb2nCms3bHo3u+zPzufX/Wb9hmx97Kf5b+k5MbGzsPaF3d/PLnvJz2Xr11nv+6S9E/nr2Nv+8eae37us2IMDgRFwIDACDgRGwIHACDgQGAEHAiPgQGBcBx+RLec8Wz+3PJatf/eV4q9GfvSF/NfWjz+d/7n+Syfq2bouvZUt//zTxdeTH941lV329/f/Q37dHfxR5cXC2uwzn8kue+PLb2TrHb5MupTYgwOBEXAgMAIOBEbAgcAIOBAYAQcCI+BAYFwHH5Fb//mZ/Ayb7sqW7fHie7rv+kH+nmp/8kfZev5bzzvb80bx966/8rkdfb573t/Wi7/T/dwrxfepS9Kenzyx3u2MHHtwILCOATezaTObbzNtysxmBtcagH51DLi7zzW/NrPpNH0hvc5/9hDAyPRyiL5fUuMLwU5JuuqDz2Y2Y2aLZra4ogv99AegD70EvNLyelfrDO4+6+41d69tUf4L+gAMTi8Br0vKfy0ngFLoJeAndGUvXpU0XzwrgFHqeB08nUSrmdm0u8+5+5yZHUrTK42TbVibS2eKx/eWpBufy3+3uV9nxbWnftxTT+vljfs+VFj77IdPDHTdX3/iNwpr+x7N34MfUceApwDf3DLtSHpKuIES44MuQGAEHAiMgAOBEXAgMAIOBMbtoiU19mp++GB7u/gjwBfXu5kWm6sfydZf/s3iLxj+8z2Pd3j3LdnqN87dmq3v+/viy4dbvzXYS3RlxB4cCIyAA4ERcCAwAg4ERsCBwAg4EBgBBwLjOnhJ2dl6tv7uR/cW1rZeyg90e3nn9vy6L+RvVf3Jwfy16EenHi6sbbPrs8t28vDR6Wx9z7f+p6/3j4Y9OBAYAQcCI+BAYAQcCIyAA4ERcCAwAg4ExnXwklq5Y09+Bi/+CuB65muLJen83vzP9Qs3Z8v6y9/6RrZ+z9ber3UffOn+bH3Pw1znXgv24EBgBBwIjIADgRFwIDACDgRGwIHACDgQGNfBS2rzj1/I1n/+23cX1pY+eTm77KZdb2frf1b7l2z9i9t/ka334wd/9cls/UZ9b2DrjqjjHtzMps1svmXaspnNm9mhwbUGoF/djA8+Z2YHWyY/kMYNB1Bivf4OXjGz6rp2AmDd9RrwcUlLZna0XdHMZsxs0cwWV1Q8hhaAweop4O4+6+51SXUzu+pb8FK95u61LdrWb48AerTmgKe98+QgmgGwvro5iz4lqda0pz6Wpk9LqyfhBtcegH50cxZ9QdLNTa/rkk6mB+EekEvLy9n6zpdWCmtv37o1/+Yfyn/v+dLFHfnldbZDvdg9x38vW//I8Vez9UGPfR4Nn2QDAiPgQGAEHAiMgAOBEXAgMAIOBMbtohvU9aeWCmtj+z6QXXbp9fzwwfvuPd1TTw0vXTxfWNv0+E3ZZS8+96O+1o33Yw8OBEbAgcAIOBAYAQcCI+BAYAQcCIyAA4FxHXyjWn6jsLTj1d3ZRd/87DvZ+ju+JVv/u3P59//T//5aYe2uIwz/O0zswYHACDgQGAEHAiPgQGAEHAiMgAOBEXAgMK6Db1TjlcLSmU/k/1kvXbwuW//OG8VDE0vSY6/cka3vPs5/q7JgDw4ERsCBwAg4EBgBBwIj4EBgBBwIjIADgXHBckQ277stW3/39vw91/U7xgprv7ijeGhhSdJK/uf6vz3z8Wx9zz/lhyfePnc8v34MTTbgZlaRVE2P/e7+YJo+LakuqeruswPuEUCPOh2if1lSzd3nJMnMZlK45e4LadrUYFsE0KtswN19tmkPXZV0StL+9KfSn5ODaw9AP7o6yWZmVUlLaa9daSnvajP/jJktmtniii703yWAnnR7Fn3a3Q+m53VJ47mZ056/5u61LdrWT38A+tAx4GY27e5H0vNJSSd0ZS9elTQ/sO4A9KXTWfQpSYfN7KE06UF3nzOzQ6lWaZxsw/u9/aVfydZfuy9/y+aOe4qHB5akG8eWC2uf3lHPLrtl06Vs/T+f+li2ftNjz2frF7NVDFM24Cm8E22mH0lPCTdQYnySDQiMgAOBEXAgMAIOBEbAgcAIOBAYt4v26MLn92fry189n63/4Z0nsvXfvempbP32zTsKa+cv54cHfvStD2br37lwT7Z+8dXXsnWUB3twIDACDgRGwIHACDgQGAEHAiPgQGAEHAiM6+AZZ2buL6wt33s5u+xXq09n61/c+f1sPXedu5PvvnNTtn7s9Vq2fucff6/ndaNc2IMDgRFwIDACDgRGwIHACDgQGAEHAiPgQGBcB8/Y/XTxPd0rO3dml/3mjb+arf/7rXdn63dVTmfrZy9sL6z97xN3ZJed+BOG971WsAcHAiPgQGAEHAiMgAOBEXAgMAIOBEbAgcC4Dp7zxA8LS7e9flt20Xcf35WvV3Zn68/ccGu2Xjn+UmFt4kWuc2NVdg9uZhUzmzSzaTM73DR92czmzezQ4FsE0KtOh+hfllRz9zlJMrOZNP0Bdz/g7kcG2h2AvmQP0d19tullVdJ8el4xs6q7nxpYZwD61tVJNjOrSlpy94U0aVzSkpkdLZh/xswWzWxxRRfWqVUAa9XtWfRpdz/YeOHus+5el1Q3s+nWmVO95u61Ldq2Tq0CWKuOZ9HNbLrxu7aZTUqqSVp095ODbg5Af7IBN7MpSYfN7KE06UFJxyRVG3vuxgm4a83F51/M1jd1qI/1u/4+l8e1odNJtgVJE21KJ9Pjmgw3sFHwSTYgMAIOBEbAgcAIOBAYAQcCI+BAYAQcCIyAA4ERcCAwAg4ERsCBwAg4EBgBBwIj4EBg5u6DXYHZaUnPN03aLenMQFfaO3rrTVl7K2tf0vr3ts/db2mdOPCAX7VCs0V3rw11pV2it96Utbey9iUNrzcO0YHACDgQ2CgCPtt5lpGht96Utbey9iUNqbeh/w4OYHg4RAcCI+BAYEMNeBqldKppEMNSKONoqWlbzbeZNvLtV9DbSLdhZiTckW+zUY7SO7SANw2UsJBeTw1r3V0o3WiprQNKlGn7FQx2MepteNVIuCXaZiMbpXeYe/D9khqjkZ6SNDnEdXdSSQMsllmZt5804m2YxsNrnJmuanUblWKbFfQmDWGbDTPglZbXu4a47k6yo6WWRKXldZm2n1SSbdgyEm6lpTzSbbbWUXrXwzADXtfqX6h0Oo2WWhJ1lXT7SaXahs0j4dZVrm22plF618MwA35CV36iViXNF886POl3tbId7rZTyu0nlWcbthkJtzTbrLW3YW2zoQU8nWCophMdlabDlFE7Jr3vJFYpBlRM26nW0lcptl9rbyrBNmwaCfdJM3tS0nhZtlm73jSkbcYn2YDA+KALEBgBBwIj4EBgBBwIjIADgRFwIDACDgT2/yexxr/d7k9lAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPCElEQVR4nO3dMXIT2aLH4f95NYHJVL7B3FjegTAruHI4mTxeAdYOcHkFFOzA2gFGO0A7wGgH7vgSWNUvgqxfYFkPA2YYBkvI5/uqXFhSq88xya9O95Fduq4LANTifzY9AQBYJ+EDoCrCB0BVhA+AqggfAFURPgCqInwAVEX4AKiK8AFQFeEDoCrCB0BVhA+AqggfAFURPgCqInwAVEX4AKiK8AFQFeEDoCrCB0BVhA+AqggfAFURPgCqInwAVEX4AKiK8AFQFeEDoCrCB0BVhA+AqggfAFURPgCq8ts6B3v06NF/P378+Ps6xwRgO+zs7Lz/8OHDv+97nNJ13X2P8f+DldKtczwAtkcpJV3Xlfsex6VOAKoifABURfgAqIrwAVAV4QOgKsIHQFWED4CqCB8AVRE+AKoifABURfgAqIrwAVAV4QOgKsIHv5C2bTOfzzOdTtO27aanAw+S8MEv5OLiIhcXF+n3+2maZtPTgQdJ+GBNptPprcfj8TgnJyeZTqeZTCbZ29vL/v5+3r17l6dPn6bf7ye5XgV+/l7gx631L7BDrWazWQaDwa3nHj9+nOPj4yTJwcFB3rx5k/Pz85ydnaVt20wmkzx79iy9Xi9J0jTNKobAj/MX2OEnms/nmc1m6ff7q8uVo9EoJycnefHixVffc3h4mNPT0wwGg8zn8ywWi+zu7qbX690K3bfOAQ+Bv8AOW+rq6ir9fj+DwSCvXr1Kkjs3qozH4xwdHa1Wg4PBIMPhMIPB4IvVnXt+8HMIH/xEg8EgTdOsQvatnZkvX77M3t5eRqNR2rb9y12cu7u7P3GmUC/hg3synU4zHo/vfO3y8jLPnj1Lkpyfn6/u5QH3S/jgJ2qaJm3bZjabZbFYZDQaJcmtqM3n85ycnOTg4GAVx8vLy788tzDCz2FzC/xEk8kk/X4/w+Hw1vOfbnj5Ef/0/bANbG6BLdO2bV6/fv3V14bDYebz+Q+fN4nowU9ixQfAL8GKDwDugfABUBXhA6AqwgdAVYQPgKoIHwBVET4AqiJ8AFRF+ACoivABUBXhA6AqwgdAVYQPgKoIHwBVET4AqiJ8AFTlt3UOtrOz876U8vs6xwRgO+zs7Lxfxzhr/QvswLVSyh9Jjruu+2PTc4HauNQJQFWED4CqCB8AVRE+AKoifABUZa0fZwC4b6WUXpL+8mvWdV270Qnxy7HiAx6a/eVXk+v4wS1WfMDWKaUMkxwuH14meZLkrOu6WZKL5WvjJP/ZzAz5lQkfsHW6rpstL2mm67ppKWWUZLF87s+u68bL74+TvNzYRPklCR/wkOwmuViuCBdJphueD78g4QO2Wb+UcpzkIMmrruuEjr9kcwuwzZrlv29Ej+8lfMC2O8/1/b3jTU+E7SB8wNZZ3sM7WH7tJmmT9EopLzY5L7aDe3zA1ll+bGH2yVPNZ4/hTlZ8AFRF+ACoivABUBXhA6AqwgdAVYQPgKoIHwBVET4AqiJ8AFRF+ACoivABUBXhA6AqwgdAVYQPgKoIHwBVET4AqiJ8AFRF+ACoivABUBXhA6AqwgdAVYQPgKoIHwBVET4AqiJ8AFRF+ACoivABUJXSdd3aBnv06NF/P378+PvaBgRga+zs7Lz/8OHDv+97nLWGr5TSrXM8ALZHKSVd15X7HselTgCqInwAVEX4AKiK8AFQFeEDoCrCB0BVhA+AqggfAFURPgCqInwAVEX4AKiK8AFQFeEDoCrCB0BVftv0BOChm0wm6ff7GQ6Hmc/nubi4SJL8+eef6fV6q9d3d3czGAy++7xt26ZpmjRNk+FwmF6vd08/ATwswgf3bH9/P03TJElevXqV09PTLBaLTCaT9Hq9VRT/rouLizRNszr/34km1MylTvhB0+n01uPxeJyTk5NMp9NMJpPs7e198Z6jo6M0TZP5fJ6rq6u8e/cubdtmOp1mPp+vjptMJn957v39/bx79y5Pnz5Nv99PktW5gLtZ8cEPmM1mX6ywHj9+nOPj4yTJwcFB3rx588X7BoNB2rbNYrHIkydP8vbt2wyHwywWi0yn0ztXbV879/n5ec7OztK2bSaTSZ49e7a63Nk0zSqGwG2l67r1DVZKt87x4J+az+eZzWbp9/vp9/tpmiaj0SgnJyd58eLFV99zeHiY09PTVcRevnyZq6urnJ6epmmaLBaLNE2T4+PjtG2b8/Pz7O7u3rpPN5lMVqG769zz+TyLxSK7u7urS6Y3vjU/+FWVUtJ1Xbnvcaz44C9cXV1lOBxmMBjk+fPnGY1Gadv2q8eOx+McHR3dWrk9e/Zs9f3nK7per/fVwH3Pub91T+/mniLwJff44BsGg8GtjSN3BS+5Xtnt7e2twvitY/+uv3vu3d3dnzY2PDTCB99pOp1mPB7f+drl5eVqdXd+fv7TPl5wn+eGGgkffEPTNGnbNrPZLIvFIqPRKEluhWc+n+fk5CQHBwerOF5eXv6U8X/03MIId7O5Bb7h0w+ff+rTDS/3Ne733vv73H3PDe7Luja3WPHBHdq2zevXr7/62s1vYfnV3Nz7Ez24mxUfAL8EKz4AuAfCB0BVhA+AqggfAFURPgCqInwAVEX4AKiK8AFQFeEDoCrCB0BVhA+AqggfAFURPgCqInwAVEX4AKiK8AFQld/WOdjOzs77Usrv6xwTgO2ws7Pzfh3jrPUvsAPXSil/JDnuuu6PTc8FauNSJwBVET4AqiJ8AFRF+ACoivABUJW1fpwBYB1KKcdJmq7rZqWUQZL95UvnXde1N68nWXRdN9/YRNkI4QMeoosk/eX3R0meJ9lNclxKabOM4obmxoYJH7B1SinDJIfLh5dJniQ5uyNmr3IdwX6SfyXZS7IopYxyHUArvsoIH7B1lpcwe8vvp8uILUopva7r2s+OnS+P3U3yNteRnC0fj5IIX2WED3hIdpO0SYZJ/lVKmeV6pbebpN913WT53J9JFkkmm5oom+NXlsEG+JVl/9xyldfPdegOkrzqum660UmxFXycAdhmzfLfN6LH9xI+YNud5/r+3vGmJ8J2ED5g6yx3dR4sv27u6/VKKS82OS+2g80twNZZfmzh048uNJ89hjtZ8QFQFeEDoCrCB0BVhA+AqggfAFURPgCqInwAVEX4AKiK8AFQFeEDoCrCB0BVhA+AqggfAFURPgCqInwAVEX4AKiK8AFQFeEDoCrCB0BVhA+AqggfAFURPgCqInwAVEX4AKiK8AFQFeEDoCql67q1Dfbo0aP/fvz48fe1DQjA1tjZ2Xn/4cOHf9/3OGsNXymlW+d4AGyPUkq6riv3PY5LnQBURfgAqIrwAVAV4QOgKsIHQFWED4CqCB8AVRE+AKoifABURfgAqIrwAVAV4QOgKsIHQFWED4CqCB/cg+l0msPDw4zH40wmk7Rtm/l8nul0mrZtV8ednJz80Pknk0lms1mSZD6fZzKZrMb59PX5fP5PfxR4cH7b9ATgIer3+3n9+nWapsnu7m4uLi7SNE329/fTNE0Gg0Hm8/mtCP4dN+dJklevXuX09DSLxSKTySS9Xi/9fj/D4fAn/kTwcFjxwXeaTqdfPC6lrFZV4/E4Jycnads2g8EgSdI0TXq9Xvb39/Pu3bs8ffo0/X5/dY5er7f6fjKZ3Dr/zfmm02kmk0n29va+Oq+jo6M0TZP5fJ6rq6u8e/cubdtmOp2u4vr53KFmVnzwHWaz2SpmN0ajUUajURaLRdq2zeHh4a1V1nQ6Xb3n/Pw8Z2dnads2k8kk/X4//X4/TdOkaZpbMbzx+PHjHB8fJ0kODg7y5s2br85tMBikbdssFos8efIkb9++zXA4zGKxyHQ6zbNnz5LkznGgNlZ88In5fJ6XL1+uVks3K6U3b958NRqnp6d58eJFLi4uvri0eHOZM7m+NDmbzdI0zSqY/X7/m5c6b6J3eHiYFy9e3Bp/Npvl7du3q3uHN5dSR6NRTk9Pc35+nvl8vjrHaDTK2dnZP/q/gYfCig8+c3V1leFwmMFgkOfPn2c0Gt0ZqMFgkIuLi+zv73/x2s1K6+a4z/V6vTtXcTfG43GOjo6+eP+3zt3r9VbB+9TNPUGonRUffGIwGKw2nyT5y80n0+k0p6enef78+U+fy8uXL7O3t7cK749uhLlxs/qE2gkf3GE6nWY8Hn/z9dFolOPj45++eWQ6neby8nK1sjs/P7+1EQb4ccIHn2iaJm3bZjabZbFYZDQaJbm9+3I+n+fg4OCLEP3oZ/I+N5/Pc3JykoODg1V8Ly8v//F5hROula7r1jdYKd06x4O/62bH5ecbVWaz2Won5n2O/bV7cz/DOuYP/1QpJV3Xlfsex4oPltq2zevXr7/62nA43NrfgnJzb1D04JoVHwC/BCs+ALgHwgdAVYQPgKoIHwBVET4AqiJ8AFRF+ACoivABUBXhA6AqwgdAVYQPgKoIHwBVET4AqiJ8AFRF+ACoivABUJXf1jnYzs7O+1LK7+scE4DtsLOz834d46z1L7AD10opfyQ57rruj03PBWrjUicAVRE+AKoifABURfgAqIrwAVAV4QMelFLKqJTyupRyVko5LqX0SimD5fO9T457scFpskFr/RwfwBo0XdcdllL6SRZJ9pP0k1ws/52XUgZJepubIpskfMDWKaUMkxwuH14meZLkrOu6Wdd18+Xz/a7rmlLKxfLYcZL/fHKadl3z5dcifMDW6bpudnPZsuu6aSlllGRRSul1XdcuH98E8M+u68bL449LKU2SJkm/lNLvuq7ZxM/A5ggf8JDs5nol108yWz53sVwhLpJMl6vAXlzqrJbwAdusX0o5TnKQ5FXXddMk6bru5c0Bn1z6zCfPtcv3UCG7OoFtdnOZ8s1N9OCvCB+w7c5zfX/veNMTYTsIH7B1lvfsDpZfN/f1ej6bx/dwjw/YOl3XzfL/m1eS60ueszsOh1us+ACoivABUBXhA6AqwgdAVYQPNuN/c/07JoE1K13XbXoOALA2VnwAVEX4AKiK8AFQFeEDoCrCB0BVhA+AqggfAFURPgCqInwAVEX4AKiK8AFQFeEDoCrCB0BVhA+AqggfAFURPgCqInwAVEX4AKiK8AFQFeEDoCrCB0BVhA+AqggfAFURPgCqInwAVEX4AKiK8AFQFeEDoCrCB0BVhA+AqvwfPsHRH8nedaUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    path.draw()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:categorical_bpl] *",
   "language": "python",
   "name": "conda-env-categorical_bpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
