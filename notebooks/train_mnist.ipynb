{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fa918575610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.enable_validation(True)\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)\n",
    "model.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.5,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/54000 (0%)] Loss: 41894.992188\n",
      "Train Epoch: 1 [4096/54000 (8%)] Loss: -9522.256836\n",
      "Train Epoch: 1 [8192/54000 (15%)] Loss: 29017.179688\n",
      "Train Epoch: 1 [12288/54000 (23%)] Loss: -29229.769531\n",
      "Train Epoch: 1 [16384/54000 (30%)] Loss: -35152.195312\n",
      "Train Epoch: 1 [20480/54000 (38%)] Loss: -52823.496094\n",
      "Train Epoch: 1 [24576/54000 (46%)] Loss: -57761.617188\n",
      "Train Epoch: 1 [28672/54000 (53%)] Loss: -78456.648438\n",
      "Train Epoch: 1 [32768/54000 (61%)] Loss: -91714.476562\n",
      "Train Epoch: 1 [36864/54000 (68%)] Loss: -93105.195312\n",
      "Train Epoch: 1 [40960/54000 (76%)] Loss: -88335.968750\n",
      "Train Epoch: 1 [45056/54000 (83%)] Loss: -130833.429688\n",
      "Train Epoch: 1 [49152/54000 (91%)] Loss: -158482.375000\n",
      "    epoch          : 1\n",
      "    loss           : -58937.252324695124\n",
      "    val_loss       : -136694.025390625\n",
      "Train Epoch: 2 [0/54000 (0%)] Loss: -109107.351562\n",
      "Train Epoch: 2 [4096/54000 (8%)] Loss: -163611.375000\n",
      "Train Epoch: 2 [8192/54000 (15%)] Loss: -149721.203125\n",
      "Train Epoch: 2 [12288/54000 (23%)] Loss: -180391.718750\n",
      "Train Epoch: 2 [16384/54000 (30%)] Loss: -165972.031250\n",
      "Train Epoch: 2 [20480/54000 (38%)] Loss: -210689.453125\n",
      "Train Epoch: 2 [24576/54000 (46%)] Loss: -179249.296875\n",
      "Train Epoch: 2 [28672/54000 (53%)] Loss: -177267.125000\n",
      "Train Epoch: 2 [32768/54000 (61%)] Loss: -120462.375000\n",
      "Train Epoch: 2 [36864/54000 (68%)] Loss: -137059.218750\n",
      "Train Epoch: 2 [40960/54000 (76%)] Loss: -153875.031250\n",
      "Train Epoch: 2 [45056/54000 (83%)] Loss: -249612.328125\n",
      "Train Epoch: 2 [49152/54000 (91%)] Loss: -197106.156250\n",
      "    epoch          : 2\n",
      "    loss           : -175046.57027439025\n",
      "    val_loss       : -218449.37421875\n",
      "Train Epoch: 3 [0/54000 (0%)] Loss: -288140.062500\n",
      "Train Epoch: 3 [4096/54000 (8%)] Loss: -266531.875000\n",
      "Train Epoch: 3 [8192/54000 (15%)] Loss: -159220.062500\n",
      "Train Epoch: 3 [12288/54000 (23%)] Loss: -274908.343750\n",
      "Train Epoch: 3 [16384/54000 (30%)] Loss: -247833.718750\n",
      "Train Epoch: 3 [20480/54000 (38%)] Loss: -166179.312500\n",
      "Train Epoch: 3 [24576/54000 (46%)] Loss: -194200.109375\n",
      "Train Epoch: 3 [28672/54000 (53%)] Loss: -274489.187500\n",
      "Train Epoch: 3 [32768/54000 (61%)] Loss: -283237.687500\n",
      "Train Epoch: 3 [36864/54000 (68%)] Loss: -268195.437500\n",
      "Train Epoch: 3 [40960/54000 (76%)] Loss: -295576.500000\n",
      "Train Epoch: 3 [45056/54000 (83%)] Loss: -261268.171875\n",
      "Train Epoch: 3 [49152/54000 (91%)] Loss: -209830.328125\n",
      "    epoch          : 3\n",
      "    loss           : -255421.8893292683\n",
      "    val_loss       : -297877.9588867187\n",
      "Train Epoch: 4 [0/54000 (0%)] Loss: -349190.156250\n",
      "Train Epoch: 4 [4096/54000 (8%)] Loss: -269880.250000\n",
      "Train Epoch: 4 [8192/54000 (15%)] Loss: -336021.500000\n",
      "Train Epoch: 4 [12288/54000 (23%)] Loss: -318461.812500\n",
      "Train Epoch: 4 [16384/54000 (30%)] Loss: -296839.406250\n",
      "Train Epoch: 4 [20480/54000 (38%)] Loss: -339493.125000\n",
      "Train Epoch: 4 [24576/54000 (46%)] Loss: -332337.187500\n",
      "Train Epoch: 4 [28672/54000 (53%)] Loss: -325727.843750\n",
      "Train Epoch: 4 [32768/54000 (61%)] Loss: -283383.781250\n",
      "Train Epoch: 4 [36864/54000 (68%)] Loss: -282308.500000\n",
      "Train Epoch: 4 [40960/54000 (76%)] Loss: -331890.062500\n",
      "Train Epoch: 4 [45056/54000 (83%)] Loss: -362882.343750\n",
      "Train Epoch: 4 [49152/54000 (91%)] Loss: -389624.000000\n",
      "    epoch          : 4\n",
      "    loss           : -326188.98338414636\n",
      "    val_loss       : -348783.22421875\n",
      "Train Epoch: 5 [0/54000 (0%)] Loss: -332100.468750\n",
      "Train Epoch: 5 [4096/54000 (8%)] Loss: -348857.000000\n",
      "Train Epoch: 5 [8192/54000 (15%)] Loss: -320100.062500\n",
      "Train Epoch: 5 [12288/54000 (23%)] Loss: -349085.500000\n",
      "Train Epoch: 5 [16384/54000 (30%)] Loss: -342905.312500\n",
      "Train Epoch: 5 [20480/54000 (38%)] Loss: -356444.656250\n",
      "Train Epoch: 5 [24576/54000 (46%)] Loss: -355847.500000\n",
      "Train Epoch: 5 [28672/54000 (53%)] Loss: -398551.593750\n",
      "Train Epoch: 5 [32768/54000 (61%)] Loss: -346579.500000\n",
      "Train Epoch: 5 [36864/54000 (68%)] Loss: -362048.125000\n",
      "Train Epoch: 5 [40960/54000 (76%)] Loss: -348796.218750\n",
      "Train Epoch: 5 [45056/54000 (83%)] Loss: -302355.687500\n",
      "Train Epoch: 5 [49152/54000 (91%)] Loss: -399803.562500\n",
      "    epoch          : 5\n",
      "    loss           : -347265.6772865854\n",
      "    val_loss       : -358073.99296875\n",
      "Train Epoch: 6 [0/54000 (0%)] Loss: -360247.531250\n",
      "Train Epoch: 6 [4096/54000 (8%)] Loss: -371373.906250\n",
      "Train Epoch: 6 [8192/54000 (15%)] Loss: -374394.125000\n",
      "Train Epoch: 6 [12288/54000 (23%)] Loss: -369278.062500\n",
      "Train Epoch: 6 [16384/54000 (30%)] Loss: -325593.625000\n",
      "Train Epoch: 6 [20480/54000 (38%)] Loss: -361470.250000\n",
      "Train Epoch: 6 [24576/54000 (46%)] Loss: -364623.500000\n",
      "Train Epoch: 6 [28672/54000 (53%)] Loss: -404777.656250\n",
      "Train Epoch: 6 [32768/54000 (61%)] Loss: -364088.937500\n",
      "Train Epoch: 6 [36864/54000 (68%)] Loss: -377714.125000\n",
      "Train Epoch: 6 [40960/54000 (76%)] Loss: -362635.562500\n",
      "Train Epoch: 6 [45056/54000 (83%)] Loss: -364534.375000\n",
      "Train Epoch: 6 [49152/54000 (91%)] Loss: -407341.625000\n",
      "    epoch          : 6\n",
      "    loss           : -355302.10015243903\n",
      "    val_loss       : -364476.04765625\n",
      "Train Epoch: 7 [0/54000 (0%)] Loss: -328338.000000\n",
      "Train Epoch: 7 [4096/54000 (8%)] Loss: -363143.281250\n",
      "Train Epoch: 7 [8192/54000 (15%)] Loss: -325346.000000\n",
      "Train Epoch: 7 [12288/54000 (23%)] Loss: -347457.375000\n",
      "Train Epoch: 7 [16384/54000 (30%)] Loss: -380791.250000\n",
      "Train Epoch: 7 [20480/54000 (38%)] Loss: -366938.656250\n",
      "Train Epoch: 7 [24576/54000 (46%)] Loss: -369685.593750\n",
      "Train Epoch: 7 [28672/54000 (53%)] Loss: -407882.750000\n",
      "Train Epoch: 7 [32768/54000 (61%)] Loss: -367870.062500\n",
      "Train Epoch: 7 [36864/54000 (68%)] Loss: -409088.062500\n",
      "Train Epoch: 7 [40960/54000 (76%)] Loss: -381517.125000\n",
      "Train Epoch: 7 [45056/54000 (83%)] Loss: -366221.031250\n",
      "Train Epoch: 7 [49152/54000 (91%)] Loss: -409460.187500\n",
      "    epoch          : 7\n",
      "    loss           : -357553.7439786585\n",
      "    val_loss       : -369848.453125\n",
      "Train Epoch: 8 [0/54000 (0%)] Loss: -409945.625000\n",
      "Train Epoch: 8 [4096/54000 (8%)] Loss: -334153.843750\n",
      "Train Epoch: 8 [8192/54000 (15%)] Loss: -370189.250000\n",
      "Train Epoch: 8 [12288/54000 (23%)] Loss: -355126.312500\n",
      "Train Epoch: 8 [16384/54000 (30%)] Loss: -371172.968750\n",
      "Train Epoch: 8 [20480/54000 (38%)] Loss: -368005.343750\n",
      "Train Epoch: 8 [24576/54000 (46%)] Loss: -357088.031250\n",
      "Train Epoch: 8 [28672/54000 (53%)] Loss: -412595.843750\n",
      "Train Epoch: 8 [32768/54000 (61%)] Loss: -374760.125000\n",
      "Train Epoch: 8 [36864/54000 (68%)] Loss: -368749.312500\n",
      "Train Epoch: 8 [40960/54000 (76%)] Loss: -371801.718750\n",
      "Train Epoch: 8 [45056/54000 (83%)] Loss: -332585.937500\n",
      "Train Epoch: 8 [49152/54000 (91%)] Loss: -415382.218750\n",
      "    epoch          : 8\n",
      "    loss           : -364203.037804878\n",
      "    val_loss       : -374948.32265625\n",
      "Train Epoch: 9 [0/54000 (0%)] Loss: -414538.937500\n",
      "Train Epoch: 9 [4096/54000 (8%)] Loss: -388862.625000\n",
      "Train Epoch: 9 [8192/54000 (15%)] Loss: -359472.718750\n",
      "Train Epoch: 9 [12288/54000 (23%)] Loss: -388440.937500\n",
      "Train Epoch: 9 [16384/54000 (30%)] Loss: -369111.500000\n",
      "Train Epoch: 9 [20480/54000 (38%)] Loss: -370987.687500\n",
      "Train Epoch: 9 [24576/54000 (46%)] Loss: -368623.937500\n",
      "Train Epoch: 9 [28672/54000 (53%)] Loss: -358296.000000\n",
      "Train Epoch: 9 [32768/54000 (61%)] Loss: -321435.125000\n",
      "Train Epoch: 9 [36864/54000 (68%)] Loss: -372835.812500\n",
      "Train Epoch: 9 [40960/54000 (76%)] Loss: -322980.812500\n",
      "Train Epoch: 9 [45056/54000 (83%)] Loss: -387696.750000\n",
      "Train Epoch: 9 [49152/54000 (91%)] Loss: -418255.062500\n",
      "    epoch          : 9\n",
      "    loss           : -367450.39207317075\n",
      "    val_loss       : -378578.96953125\n",
      "Train Epoch: 10 [0/54000 (0%)] Loss: -418184.750000\n",
      "Train Epoch: 10 [4096/54000 (8%)] Loss: -358926.250000\n",
      "Train Epoch: 10 [8192/54000 (15%)] Loss: -371889.500000\n",
      "Train Epoch: 10 [12288/54000 (23%)] Loss: -418185.500000\n",
      "Train Epoch: 10 [16384/54000 (30%)] Loss: -374237.687500\n",
      "Train Epoch: 10 [20480/54000 (38%)] Loss: -375663.812500\n",
      "Train Epoch: 10 [24576/54000 (46%)] Loss: -372215.000000\n",
      "Train Epoch: 10 [28672/54000 (53%)] Loss: -418129.625000\n",
      "Train Epoch: 10 [32768/54000 (61%)] Loss: -326495.343750\n",
      "Train Epoch: 10 [36864/54000 (68%)] Loss: -375455.625000\n",
      "Train Epoch: 10 [40960/54000 (76%)] Loss: -362493.312500\n",
      "Train Epoch: 10 [45056/54000 (83%)] Loss: -395198.500000\n",
      "Train Epoch: 10 [49152/54000 (91%)] Loss: -373570.531250\n",
      "    epoch          : 10\n",
      "    loss           : -370294.1945121951\n",
      "    val_loss       : -382154.3125\n",
      "Train Epoch: 11 [0/54000 (0%)] Loss: -389249.062500\n",
      "Train Epoch: 11 [4096/54000 (8%)] Loss: -381100.781250\n",
      "Train Epoch: 11 [8192/54000 (15%)] Loss: -366740.437500\n",
      "Train Epoch: 11 [12288/54000 (23%)] Loss: -385509.281250\n",
      "Train Epoch: 11 [16384/54000 (30%)] Loss: -375811.187500\n",
      "Train Epoch: 11 [20480/54000 (38%)] Loss: -378834.968750\n",
      "Train Epoch: 11 [24576/54000 (46%)] Loss: -340772.187500\n",
      "Train Epoch: 11 [28672/54000 (53%)] Loss: -375636.875000\n",
      "Train Epoch: 11 [32768/54000 (61%)] Loss: -382321.093750\n",
      "Train Epoch: 11 [36864/54000 (68%)] Loss: -347288.062500\n",
      "Train Epoch: 11 [40960/54000 (76%)] Loss: -367990.562500\n",
      "Train Epoch: 11 [45056/54000 (83%)] Loss: -327973.968750\n",
      "Train Epoch: 11 [49152/54000 (91%)] Loss: -422777.343750\n",
      "    epoch          : 11\n",
      "    loss           : -372208.92881097563\n",
      "    val_loss       : -385366.4203125\n",
      "Train Epoch: 12 [0/54000 (0%)] Loss: -366619.312500\n",
      "Train Epoch: 12 [4096/54000 (8%)] Loss: -384201.125000\n",
      "Train Epoch: 12 [8192/54000 (15%)] Loss: -343831.375000\n",
      "Train Epoch: 12 [12288/54000 (23%)] Loss: -376280.687500\n",
      "Train Epoch: 12 [16384/54000 (30%)] Loss: -365141.375000\n",
      "Train Epoch: 12 [20480/54000 (38%)] Loss: -344977.437500\n",
      "Train Epoch: 12 [24576/54000 (46%)] Loss: -376893.687500\n",
      "Train Epoch: 12 [28672/54000 (53%)] Loss: -366127.625000\n",
      "Train Epoch: 12 [32768/54000 (61%)] Loss: -369310.062500\n",
      "Train Epoch: 12 [36864/54000 (68%)] Loss: -384129.937500\n",
      "Train Epoch: 12 [40960/54000 (76%)] Loss: -375946.718750\n",
      "Train Epoch: 12 [45056/54000 (83%)] Loss: -384443.250000\n",
      "Train Epoch: 12 [49152/54000 (91%)] Loss: -424466.656250\n",
      "    epoch          : 12\n",
      "    loss           : -375305.41021341464\n",
      "    val_loss       : -388707.20703125\n",
      "Train Epoch: 13 [0/54000 (0%)] Loss: -424626.375000\n",
      "Train Epoch: 13 [4096/54000 (8%)] Loss: -376456.312500\n",
      "Train Epoch: 13 [8192/54000 (15%)] Loss: -380966.843750\n",
      "Train Epoch: 13 [12288/54000 (23%)] Loss: -375268.187500\n",
      "Train Epoch: 13 [16384/54000 (30%)] Loss: -337630.937500\n",
      "Train Epoch: 13 [20480/54000 (38%)] Loss: -377078.375000\n",
      "Train Epoch: 13 [24576/54000 (46%)] Loss: -396363.718750\n",
      "Train Epoch: 13 [28672/54000 (53%)] Loss: -371001.687500\n",
      "Train Epoch: 13 [32768/54000 (61%)] Loss: -399158.250000\n",
      "Train Epoch: 13 [36864/54000 (68%)] Loss: -382551.937500\n",
      "Train Epoch: 13 [40960/54000 (76%)] Loss: -378751.406250\n",
      "Train Epoch: 13 [45056/54000 (83%)] Loss: -342791.968750\n",
      "Train Epoch: 13 [49152/54000 (91%)] Loss: -426601.375000\n",
      "    epoch          : 13\n",
      "    loss           : -377477.4975609756\n",
      "    val_loss       : -391252.0171875\n",
      "Train Epoch: 14 [0/54000 (0%)] Loss: -384788.468750\n",
      "Train Epoch: 14 [4096/54000 (8%)] Loss: -384862.937500\n",
      "Train Epoch: 14 [8192/54000 (15%)] Loss: -373370.187500\n",
      "Train Epoch: 14 [12288/54000 (23%)] Loss: -379410.812500\n",
      "Train Epoch: 14 [16384/54000 (30%)] Loss: -339254.875000\n",
      "Train Epoch: 14 [20480/54000 (38%)] Loss: -383235.937500\n",
      "Train Epoch: 14 [24576/54000 (46%)] Loss: -388776.875000\n",
      "Train Epoch: 14 [28672/54000 (53%)] Loss: -371746.937500\n",
      "Train Epoch: 14 [32768/54000 (61%)] Loss: -339757.625000\n",
      "Train Epoch: 14 [36864/54000 (68%)] Loss: -399069.812500\n",
      "Train Epoch: 14 [40960/54000 (76%)] Loss: -379858.031250\n",
      "Train Epoch: 14 [45056/54000 (83%)] Loss: -375532.562500\n",
      "Train Epoch: 14 [49152/54000 (91%)] Loss: -427374.000000\n",
      "    epoch          : 14\n",
      "    loss           : -379305.26051829266\n",
      "    val_loss       : -393564.40859375\n",
      "Train Epoch: 15 [0/54000 (0%)] Loss: -429374.562500\n",
      "Train Epoch: 15 [4096/54000 (8%)] Loss: -345540.875000\n",
      "Train Epoch: 15 [8192/54000 (15%)] Loss: -376171.625000\n",
      "Train Epoch: 15 [12288/54000 (23%)] Loss: -374208.687500\n",
      "Train Epoch: 15 [16384/54000 (30%)] Loss: -372887.750000\n",
      "Train Epoch: 15 [20480/54000 (38%)] Loss: -386768.625000\n",
      "Train Epoch: 15 [24576/54000 (46%)] Loss: -395958.625000\n",
      "Train Epoch: 15 [28672/54000 (53%)] Loss: -370522.000000\n",
      "Train Epoch: 15 [32768/54000 (61%)] Loss: -385150.437500\n",
      "Train Epoch: 15 [36864/54000 (68%)] Loss: -341496.062500\n",
      "Train Epoch: 15 [40960/54000 (76%)] Loss: -378988.125000\n",
      "Train Epoch: 15 [45056/54000 (83%)] Loss: -400865.031250\n",
      "Train Epoch: 15 [49152/54000 (91%)] Loss: -372435.625000\n",
      "    epoch          : 15\n",
      "    loss           : -381108.718902439\n",
      "    val_loss       : -395494.296875\n",
      "Train Epoch: 16 [0/54000 (0%)] Loss: -430258.656250\n",
      "Train Epoch: 16 [4096/54000 (8%)] Loss: -342725.875000\n",
      "Train Epoch: 16 [8192/54000 (15%)] Loss: -389178.625000\n",
      "Train Epoch: 16 [12288/54000 (23%)] Loss: -380186.718750\n",
      "Train Epoch: 16 [16384/54000 (30%)] Loss: -353272.062500\n",
      "Train Epoch: 16 [20480/54000 (38%)] Loss: -372831.406250\n",
      "Train Epoch: 16 [24576/54000 (46%)] Loss: -402499.812500\n",
      "Train Epoch: 16 [28672/54000 (53%)] Loss: -386239.937500\n",
      "Train Epoch: 16 [32768/54000 (61%)] Loss: -382840.500000\n",
      "Train Epoch: 16 [36864/54000 (68%)] Loss: -429550.562500\n",
      "Train Epoch: 16 [40960/54000 (76%)] Loss: -374483.187500\n",
      "Train Epoch: 16 [45056/54000 (83%)] Loss: -374780.875000\n",
      "Train Epoch: 16 [49152/54000 (91%)] Loss: -430355.156250\n",
      "    epoch          : 16\n",
      "    loss           : -381947.6263719512\n",
      "    val_loss       : -397348.9234375\n",
      "Train Epoch: 17 [0/54000 (0%)] Loss: -431174.375000\n",
      "Train Epoch: 17 [4096/54000 (8%)] Loss: -389640.437500\n",
      "Train Epoch: 17 [8192/54000 (15%)] Loss: -388983.437500\n",
      "Train Epoch: 17 [12288/54000 (23%)] Loss: -346013.812500\n",
      "Train Epoch: 17 [16384/54000 (30%)] Loss: -352119.625000\n",
      "Train Epoch: 17 [20480/54000 (38%)] Loss: -388999.718750\n",
      "Train Epoch: 17 [24576/54000 (46%)] Loss: -381787.281250\n",
      "Train Epoch: 17 [28672/54000 (53%)] Loss: -390077.625000\n",
      "Train Epoch: 17 [32768/54000 (61%)] Loss: -383171.343750\n",
      "Train Epoch: 17 [36864/54000 (68%)] Loss: -377083.937500\n",
      "Train Epoch: 17 [40960/54000 (76%)] Loss: -376246.937500\n",
      "Train Epoch: 17 [45056/54000 (83%)] Loss: -389963.781250\n",
      "Train Epoch: 17 [49152/54000 (91%)] Loss: -390483.125000\n",
      "    epoch          : 17\n",
      "    loss           : -384241.0867378049\n",
      "    val_loss       : -399311.128125\n",
      "Train Epoch: 18 [0/54000 (0%)] Loss: -432244.187500\n",
      "Train Epoch: 18 [4096/54000 (8%)] Loss: -376511.187500\n",
      "Train Epoch: 18 [8192/54000 (15%)] Loss: -394863.781250\n",
      "Train Epoch: 18 [12288/54000 (23%)] Loss: -432222.531250\n",
      "Train Epoch: 18 [16384/54000 (30%)] Loss: -392243.500000\n",
      "Train Epoch: 18 [20480/54000 (38%)] Loss: -388145.250000\n",
      "Train Epoch: 18 [24576/54000 (46%)] Loss: -346863.687500\n",
      "Train Epoch: 18 [28672/54000 (53%)] Loss: -379211.437500\n",
      "Train Epoch: 18 [32768/54000 (61%)] Loss: -405552.187500\n",
      "Train Epoch: 18 [36864/54000 (68%)] Loss: -394313.875000\n",
      "Train Epoch: 18 [40960/54000 (76%)] Loss: -387720.875000\n",
      "Train Epoch: 18 [45056/54000 (83%)] Loss: -391126.218750\n",
      "Train Epoch: 18 [49152/54000 (91%)] Loss: -433499.687500\n",
      "    epoch          : 18\n",
      "    loss           : -385718.91600609757\n",
      "    val_loss       : -400900.05078125\n",
      "Train Epoch: 19 [0/54000 (0%)] Loss: -434267.187500\n",
      "Train Epoch: 19 [4096/54000 (8%)] Loss: -405141.093750\n",
      "Train Epoch: 19 [8192/54000 (15%)] Loss: -391541.125000\n",
      "Train Epoch: 19 [12288/54000 (23%)] Loss: -434057.656250\n",
      "Train Epoch: 19 [16384/54000 (30%)] Loss: -393378.281250\n",
      "Train Epoch: 19 [20480/54000 (38%)] Loss: -392916.906250\n",
      "Train Epoch: 19 [24576/54000 (46%)] Loss: -405792.062500\n",
      "Train Epoch: 19 [28672/54000 (53%)] Loss: -434600.250000\n",
      "Train Epoch: 19 [32768/54000 (61%)] Loss: -393499.125000\n",
      "Train Epoch: 19 [36864/54000 (68%)] Loss: -406962.250000\n",
      "Train Epoch: 19 [40960/54000 (76%)] Loss: -373212.562500\n",
      "Train Epoch: 19 [45056/54000 (83%)] Loss: -393254.500000\n",
      "Train Epoch: 19 [49152/54000 (91%)] Loss: -434277.250000\n",
      "    epoch          : 19\n",
      "    loss           : -387100.7056402439\n",
      "    val_loss       : -402522.13359375\n",
      "Train Epoch: 20 [0/54000 (0%)] Loss: -378868.125000\n",
      "Train Epoch: 20 [4096/54000 (8%)] Loss: -349182.687500\n",
      "Train Epoch: 20 [8192/54000 (15%)] Loss: -379869.875000\n",
      "Train Epoch: 20 [12288/54000 (23%)] Loss: -383221.937500\n",
      "Train Epoch: 20 [16384/54000 (30%)] Loss: -381236.875000\n",
      "Train Epoch: 20 [20480/54000 (38%)] Loss: -393940.281250\n",
      "Train Epoch: 20 [24576/54000 (46%)] Loss: -347375.812500\n",
      "Train Epoch: 20 [28672/54000 (53%)] Loss: -437043.843750\n",
      "Train Epoch: 20 [32768/54000 (61%)] Loss: -389983.875000\n",
      "Train Epoch: 20 [36864/54000 (68%)] Loss: -435151.375000\n",
      "Train Epoch: 20 [40960/54000 (76%)] Loss: -384886.000000\n",
      "Train Epoch: 20 [45056/54000 (83%)] Loss: -356552.156250\n",
      "Train Epoch: 20 [49152/54000 (91%)] Loss: -435924.937500\n",
      "    epoch          : 20\n",
      "    loss           : -388429.86280487804\n",
      "    val_loss       : -388086.95703125\n",
      "Train Epoch: 21 [0/54000 (0%)] Loss: -436848.531250\n",
      "Train Epoch: 21 [4096/54000 (8%)] Loss: -411316.531250\n",
      "Train Epoch: 21 [8192/54000 (15%)] Loss: -396179.125000\n",
      "Train Epoch: 21 [12288/54000 (23%)] Loss: -380388.750000\n",
      "Train Epoch: 21 [16384/54000 (30%)] Loss: -394073.937500\n",
      "Train Epoch: 21 [20480/54000 (38%)] Loss: -394945.906250\n",
      "Train Epoch: 21 [24576/54000 (46%)] Loss: -358345.562500\n",
      "Train Epoch: 21 [28672/54000 (53%)] Loss: -379821.250000\n",
      "Train Epoch: 21 [32768/54000 (61%)] Loss: -436634.687500\n",
      "Train Epoch: 21 [36864/54000 (68%)] Loss: -397749.281250\n",
      "Train Epoch: 21 [40960/54000 (76%)] Loss: -380783.250000\n",
      "Train Epoch: 21 [45056/54000 (83%)] Loss: -410457.625000\n",
      "Train Epoch: 21 [49152/54000 (91%)] Loss: -436607.625000\n",
      "    epoch          : 21\n",
      "    loss           : -389836.79542682925\n",
      "    val_loss       : -405323.03125\n",
      "Train Epoch: 22 [0/54000 (0%)] Loss: -436995.531250\n",
      "Train Epoch: 22 [4096/54000 (8%)] Loss: -399203.000000\n",
      "Train Epoch: 22 [8192/54000 (15%)] Loss: -383091.343750\n",
      "Train Epoch: 22 [12288/54000 (23%)] Loss: -359554.406250\n",
      "Train Epoch: 22 [16384/54000 (30%)] Loss: -398465.562500\n",
      "Train Epoch: 22 [20480/54000 (38%)] Loss: -397171.062500\n",
      "Train Epoch: 22 [24576/54000 (46%)] Loss: -382151.937500\n",
      "Train Epoch: 22 [28672/54000 (53%)] Loss: -437255.375000\n",
      "Train Epoch: 22 [32768/54000 (61%)] Loss: -412412.906250\n",
      "Train Epoch: 22 [36864/54000 (68%)] Loss: -363255.500000\n",
      "Train Epoch: 22 [40960/54000 (76%)] Loss: -354285.375000\n",
      "Train Epoch: 22 [45056/54000 (83%)] Loss: -397756.000000\n",
      "Train Epoch: 22 [49152/54000 (91%)] Loss: -438385.625000\n",
      "    epoch          : 22\n",
      "    loss           : -391153.3461890244\n",
      "    val_loss       : -406775.92734375\n",
      "Train Epoch: 23 [0/54000 (0%)] Loss: -396003.781250\n",
      "Train Epoch: 23 [4096/54000 (8%)] Loss: -385606.062500\n",
      "Train Epoch: 23 [8192/54000 (15%)] Loss: -398402.062500\n",
      "Train Epoch: 23 [12288/54000 (23%)] Loss: -353850.562500\n",
      "Train Epoch: 23 [16384/54000 (30%)] Loss: -397049.812500\n",
      "Train Epoch: 23 [20480/54000 (38%)] Loss: -398884.343750\n",
      "Train Epoch: 23 [24576/54000 (46%)] Loss: -397571.906250\n",
      "Train Epoch: 23 [28672/54000 (53%)] Loss: -437984.000000\n",
      "Train Epoch: 23 [32768/54000 (61%)] Loss: -413495.093750\n",
      "Train Epoch: 23 [36864/54000 (68%)] Loss: -439048.687500\n",
      "Train Epoch: 23 [40960/54000 (76%)] Loss: -392336.375000\n",
      "Train Epoch: 23 [45056/54000 (83%)] Loss: -395619.125000\n",
      "Train Epoch: 23 [49152/54000 (91%)] Loss: -383873.875000\n",
      "    epoch          : 23\n",
      "    loss           : -392494.3368902439\n",
      "    val_loss       : -408294.79765625\n",
      "Train Epoch: 24 [0/54000 (0%)] Loss: -440216.843750\n",
      "Train Epoch: 24 [4096/54000 (8%)] Loss: -398350.000000\n",
      "Train Epoch: 24 [8192/54000 (15%)] Loss: -402134.437500\n",
      "Train Epoch: 24 [12288/54000 (23%)] Loss: -386125.062500\n",
      "Train Epoch: 24 [16384/54000 (30%)] Loss: -365386.125000\n",
      "Train Epoch: 24 [20480/54000 (38%)] Loss: -384032.187500\n",
      "Train Epoch: 24 [24576/54000 (46%)] Loss: -361796.875000\n",
      "Train Epoch: 24 [28672/54000 (53%)] Loss: -440786.812500\n",
      "Train Epoch: 24 [32768/54000 (61%)] Loss: -388047.562500\n",
      "Train Epoch: 24 [36864/54000 (68%)] Loss: -414689.000000\n",
      "Train Epoch: 24 [40960/54000 (76%)] Loss: -389246.500000\n",
      "Train Epoch: 24 [45056/54000 (83%)] Loss: -412909.125000\n",
      "Train Epoch: 24 [49152/54000 (91%)] Loss: -392782.437500\n",
      "    epoch          : 24\n",
      "    loss           : -393743.50868902437\n",
      "    val_loss       : -409418.48046875\n",
      "Train Epoch: 25 [0/54000 (0%)] Loss: -399488.937500\n",
      "Train Epoch: 25 [4096/54000 (8%)] Loss: -403009.687500\n",
      "Train Epoch: 25 [8192/54000 (15%)] Loss: -402885.968750\n",
      "Train Epoch: 25 [12288/54000 (23%)] Loss: -389939.750000\n",
      "Train Epoch: 25 [16384/54000 (30%)] Loss: -441265.187500\n",
      "Train Epoch: 25 [20480/54000 (38%)] Loss: -397302.656250\n",
      "Train Epoch: 25 [24576/54000 (46%)] Loss: -385056.937500\n",
      "Train Epoch: 25 [28672/54000 (53%)] Loss: -367766.062500\n",
      "Train Epoch: 25 [32768/54000 (61%)] Loss: -388161.812500\n",
      "Train Epoch: 25 [36864/54000 (68%)] Loss: -441490.312500\n",
      "Train Epoch: 25 [40960/54000 (76%)] Loss: -397501.500000\n",
      "Train Epoch: 25 [45056/54000 (83%)] Loss: -414543.875000\n",
      "Train Epoch: 25 [49152/54000 (91%)] Loss: -387723.750000\n",
      "    epoch          : 25\n",
      "    loss           : -395128.41021341464\n",
      "    val_loss       : -411341.22265625\n",
      "Train Epoch: 26 [0/54000 (0%)] Loss: -388430.406250\n",
      "Train Epoch: 26 [4096/54000 (8%)] Loss: -389229.750000\n",
      "Train Epoch: 26 [8192/54000 (15%)] Loss: -387011.625000\n",
      "Train Epoch: 26 [12288/54000 (23%)] Loss: -389225.093750\n",
      "Train Epoch: 26 [16384/54000 (30%)] Loss: -390012.000000\n",
      "Train Epoch: 26 [20480/54000 (38%)] Loss: -399785.156250\n",
      "Train Epoch: 26 [24576/54000 (46%)] Loss: -398670.625000\n",
      "Train Epoch: 26 [28672/54000 (53%)] Loss: -442115.531250\n",
      "Train Epoch: 26 [32768/54000 (61%)] Loss: -402400.562500\n",
      "Train Epoch: 26 [36864/54000 (68%)] Loss: -418576.500000\n",
      "Train Epoch: 26 [40960/54000 (76%)] Loss: -389533.125000\n",
      "Train Epoch: 26 [45056/54000 (83%)] Loss: -414152.437500\n",
      "Train Epoch: 26 [49152/54000 (91%)] Loss: -443999.687500\n",
      "    epoch          : 26\n",
      "    loss           : -396381.7256097561\n",
      "    val_loss       : -412497.5234375\n",
      "Train Epoch: 27 [0/54000 (0%)] Loss: -441915.875000\n",
      "Train Epoch: 27 [4096/54000 (8%)] Loss: -370393.593750\n",
      "Train Epoch: 27 [8192/54000 (15%)] Loss: -386245.500000\n",
      "Train Epoch: 27 [12288/54000 (23%)] Loss: -406552.875000\n",
      "Train Epoch: 27 [16384/54000 (30%)] Loss: -366777.875000\n",
      "Train Epoch: 27 [20480/54000 (38%)] Loss: -403592.062500\n",
      "Train Epoch: 27 [24576/54000 (46%)] Loss: -371484.437500\n",
      "Train Epoch: 27 [28672/54000 (53%)] Loss: -388172.125000\n",
      "Train Epoch: 27 [32768/54000 (61%)] Loss: -393259.750000\n",
      "Train Epoch: 27 [36864/54000 (68%)] Loss: -369230.750000\n",
      "Train Epoch: 27 [40960/54000 (76%)] Loss: -394660.562500\n",
      "Train Epoch: 27 [45056/54000 (83%)] Loss: -420136.500000\n",
      "Train Epoch: 27 [49152/54000 (91%)] Loss: -443524.062500\n",
      "    epoch          : 27\n",
      "    loss           : -397701.0070121951\n",
      "    val_loss       : -413811.03671875\n",
      "Train Epoch: 28 [0/54000 (0%)] Loss: -444327.812500\n",
      "Train Epoch: 28 [4096/54000 (8%)] Loss: -372592.687500\n",
      "Train Epoch: 28 [8192/54000 (15%)] Loss: -417659.750000\n",
      "Train Epoch: 28 [12288/54000 (23%)] Loss: -366676.812500\n",
      "Train Epoch: 28 [16384/54000 (30%)] Loss: -393894.062500\n",
      "Train Epoch: 28 [20480/54000 (38%)] Loss: -389134.375000\n",
      "Train Epoch: 28 [24576/54000 (46%)] Loss: -394360.500000\n",
      "Train Epoch: 28 [28672/54000 (53%)] Loss: -444507.406250\n",
      "Train Epoch: 28 [32768/54000 (61%)] Loss: -404142.031250\n",
      "Train Epoch: 28 [36864/54000 (68%)] Loss: -443812.687500\n",
      "Train Epoch: 28 [40960/54000 (76%)] Loss: -393797.562500\n",
      "Train Epoch: 28 [45056/54000 (83%)] Loss: -419023.125000\n",
      "Train Epoch: 28 [49152/54000 (91%)] Loss: -400616.968750\n",
      "    epoch          : 28\n",
      "    loss           : -398981.16082317074\n",
      "    val_loss       : -415537.48671875\n",
      "Train Epoch: 29 [0/54000 (0%)] Loss: -391443.625000\n",
      "Train Epoch: 29 [4096/54000 (8%)] Loss: -396466.437500\n",
      "Train Epoch: 29 [8192/54000 (15%)] Loss: -396500.437500\n",
      "Train Epoch: 29 [12288/54000 (23%)] Loss: -403260.843750\n",
      "Train Epoch: 29 [16384/54000 (30%)] Loss: -389458.125000\n",
      "Train Epoch: 29 [20480/54000 (38%)] Loss: -405161.437500\n",
      "Train Epoch: 29 [24576/54000 (46%)] Loss: -402563.781250\n",
      "Train Epoch: 29 [28672/54000 (53%)] Loss: -390613.218750\n",
      "Train Epoch: 29 [32768/54000 (61%)] Loss: -400010.562500\n",
      "Train Epoch: 29 [36864/54000 (68%)] Loss: -446005.750000\n",
      "Train Epoch: 29 [40960/54000 (76%)] Loss: -392856.906250\n",
      "Train Epoch: 29 [45056/54000 (83%)] Loss: -372065.093750\n",
      "Train Epoch: 29 [49152/54000 (91%)] Loss: -444865.843750\n",
      "    epoch          : 29\n",
      "    loss           : -400313.97865853657\n",
      "    val_loss       : -416759.2953125\n",
      "Train Epoch: 30 [0/54000 (0%)] Loss: -406467.937500\n",
      "Train Epoch: 30 [4096/54000 (8%)] Loss: -390854.500000\n",
      "Train Epoch: 30 [8192/54000 (15%)] Loss: -390261.281250\n",
      "Train Epoch: 30 [12288/54000 (23%)] Loss: -370817.187500\n",
      "Train Epoch: 30 [16384/54000 (30%)] Loss: -375225.875000\n",
      "Train Epoch: 30 [20480/54000 (38%)] Loss: -390188.437500\n",
      "Train Epoch: 30 [24576/54000 (46%)] Loss: -370436.125000\n",
      "Train Epoch: 30 [28672/54000 (53%)] Loss: -405984.375000\n",
      "Train Epoch: 30 [32768/54000 (61%)] Loss: -394356.281250\n",
      "Train Epoch: 30 [36864/54000 (68%)] Loss: -419712.687500\n",
      "Train Epoch: 30 [40960/54000 (76%)] Loss: -394263.500000\n",
      "Train Epoch: 30 [45056/54000 (83%)] Loss: -373508.375000\n",
      "Train Epoch: 30 [49152/54000 (91%)] Loss: -446460.531250\n",
      "    epoch          : 30\n",
      "    loss           : -401666.82713414636\n",
      "    val_loss       : -418521.65390625\n",
      "Train Epoch: 31 [0/54000 (0%)] Loss: -370604.093750\n",
      "Train Epoch: 31 [4096/54000 (8%)] Loss: -423062.562500\n",
      "Train Epoch: 31 [8192/54000 (15%)] Loss: -393196.437500\n",
      "Train Epoch: 31 [12288/54000 (23%)] Loss: -394596.125000\n",
      "Train Epoch: 31 [16384/54000 (30%)] Loss: -376551.062500\n",
      "Train Epoch: 31 [20480/54000 (38%)] Loss: -411664.125000\n",
      "Train Epoch: 31 [24576/54000 (46%)] Loss: -422903.437500\n",
      "Train Epoch: 31 [28672/54000 (53%)] Loss: -393069.562500\n",
      "Train Epoch: 31 [32768/54000 (61%)] Loss: -408159.125000\n",
      "Train Epoch: 31 [36864/54000 (68%)] Loss: -447722.625000\n",
      "Train Epoch: 31 [40960/54000 (76%)] Loss: -397221.250000\n",
      "Train Epoch: 31 [45056/54000 (83%)] Loss: -409615.812500\n",
      "Train Epoch: 31 [49152/54000 (91%)] Loss: -446062.000000\n",
      "    epoch          : 31\n",
      "    loss           : -402991.91554878047\n",
      "    val_loss       : -419694.50390625\n",
      "Train Epoch: 32 [0/54000 (0%)] Loss: -395027.187500\n",
      "Train Epoch: 32 [4096/54000 (8%)] Loss: -395095.406250\n",
      "Train Epoch: 32 [8192/54000 (15%)] Loss: -410622.500000\n",
      "Train Epoch: 32 [12288/54000 (23%)] Loss: -405997.250000\n",
      "Train Epoch: 32 [16384/54000 (30%)] Loss: -370095.750000\n",
      "Train Epoch: 32 [20480/54000 (38%)] Loss: -404474.937500\n",
      "Train Epoch: 32 [24576/54000 (46%)] Loss: -373886.125000\n",
      "Train Epoch: 32 [28672/54000 (53%)] Loss: -395332.437500\n",
      "Train Epoch: 32 [32768/54000 (61%)] Loss: -418932.812500\n",
      "Train Epoch: 32 [36864/54000 (68%)] Loss: -380050.875000\n",
      "Train Epoch: 32 [40960/54000 (76%)] Loss: -408356.812500\n",
      "Train Epoch: 32 [45056/54000 (83%)] Loss: -376892.625000\n",
      "Train Epoch: 32 [49152/54000 (91%)] Loss: -448805.093750\n",
      "    epoch          : 32\n",
      "    loss           : -404385.10838414636\n",
      "    val_loss       : -421299.68359375\n",
      "Train Epoch: 33 [0/54000 (0%)] Loss: -397094.781250\n",
      "Train Epoch: 33 [4096/54000 (8%)] Loss: -409074.062500\n",
      "Train Epoch: 33 [8192/54000 (15%)] Loss: -412606.437500\n",
      "Train Epoch: 33 [12288/54000 (23%)] Loss: -408138.562500\n",
      "Train Epoch: 33 [16384/54000 (30%)] Loss: -374639.875000\n",
      "Train Epoch: 33 [20480/54000 (38%)] Loss: -413159.375000\n",
      "Train Epoch: 33 [24576/54000 (46%)] Loss: -397600.250000\n",
      "Train Epoch: 33 [28672/54000 (53%)] Loss: -400191.843750\n",
      "Train Epoch: 33 [32768/54000 (61%)] Loss: -400666.531250\n",
      "Train Epoch: 33 [36864/54000 (68%)] Loss: -377828.187500\n",
      "Train Epoch: 33 [40960/54000 (76%)] Loss: -412503.843750\n",
      "Train Epoch: 33 [45056/54000 (83%)] Loss: -397725.625000\n",
      "Train Epoch: 33 [49152/54000 (91%)] Loss: -449247.312500\n",
      "    epoch          : 33\n",
      "    loss           : -405661.6637195122\n",
      "    val_loss       : -422390.2359375\n",
      "Train Epoch: 34 [0/54000 (0%)] Loss: -415194.625000\n",
      "Train Epoch: 34 [4096/54000 (8%)] Loss: -426293.562500\n",
      "Train Epoch: 34 [8192/54000 (15%)] Loss: -401099.312500\n",
      "Train Epoch: 34 [12288/54000 (23%)] Loss: -371197.437500\n",
      "Train Epoch: 34 [16384/54000 (30%)] Loss: -380431.906250\n",
      "Train Epoch: 34 [20480/54000 (38%)] Loss: -411089.000000\n",
      "Train Epoch: 34 [24576/54000 (46%)] Loss: -380818.781250\n",
      "Train Epoch: 34 [28672/54000 (53%)] Loss: -451577.812500\n",
      "Train Epoch: 34 [32768/54000 (61%)] Loss: -410234.062500\n",
      "Train Epoch: 34 [36864/54000 (68%)] Loss: -425973.562500\n",
      "Train Epoch: 34 [40960/54000 (76%)] Loss: -412543.312500\n",
      "Train Epoch: 34 [45056/54000 (83%)] Loss: -381357.187500\n",
      "Train Epoch: 34 [49152/54000 (91%)] Loss: -450231.812500\n",
      "    epoch          : 34\n",
      "    loss           : -406966.3829268293\n",
      "    val_loss       : -423605.57109375\n",
      "Train Epoch: 35 [0/54000 (0%)] Loss: -425704.531250\n",
      "Train Epoch: 35 [4096/54000 (8%)] Loss: -414297.937500\n",
      "Train Epoch: 35 [8192/54000 (15%)] Loss: -379800.937500\n",
      "Train Epoch: 35 [12288/54000 (23%)] Loss: -402767.375000\n",
      "Train Epoch: 35 [16384/54000 (30%)] Loss: -379526.593750\n",
      "Train Epoch: 35 [20480/54000 (38%)] Loss: -452866.062500\n",
      "Train Epoch: 35 [24576/54000 (46%)] Loss: -428553.875000\n",
      "Train Epoch: 35 [28672/54000 (53%)] Loss: -401259.250000\n",
      "Train Epoch: 35 [32768/54000 (61%)] Loss: -379783.500000\n",
      "Train Epoch: 35 [36864/54000 (68%)] Loss: -451754.937500\n",
      "Train Epoch: 35 [40960/54000 (76%)] Loss: -402463.062500\n",
      "Train Epoch: 35 [45056/54000 (83%)] Loss: -426645.156250\n",
      "Train Epoch: 35 [49152/54000 (91%)] Loss: -452163.656250\n",
      "    epoch          : 35\n",
      "    loss           : -408355.7426829268\n",
      "    val_loss       : -424846.34140625\n",
      "Train Epoch: 36 [0/54000 (0%)] Loss: -453098.968750\n",
      "Train Epoch: 36 [4096/54000 (8%)] Loss: -381303.812500\n",
      "Train Epoch: 36 [8192/54000 (15%)] Loss: -399269.625000\n",
      "Train Epoch: 36 [12288/54000 (23%)] Loss: -412970.031250\n",
      "Train Epoch: 36 [16384/54000 (30%)] Loss: -453874.375000\n",
      "Train Epoch: 36 [20480/54000 (38%)] Loss: -415525.156250\n",
      "Train Epoch: 36 [24576/54000 (46%)] Loss: -385820.343750\n",
      "Train Epoch: 36 [28672/54000 (53%)] Loss: -402414.562500\n",
      "Train Epoch: 36 [32768/54000 (61%)] Loss: -413447.812500\n",
      "Train Epoch: 36 [36864/54000 (68%)] Loss: -382045.062500\n",
      "Train Epoch: 36 [40960/54000 (76%)] Loss: -402085.062500\n",
      "Train Epoch: 36 [45056/54000 (83%)] Loss: -376130.375000\n",
      "Train Epoch: 36 [49152/54000 (91%)] Loss: -454257.656250\n",
      "    epoch          : 36\n",
      "    loss           : -409401.9881097561\n",
      "    val_loss       : -426093.478125\n",
      "Train Epoch: 37 [0/54000 (0%)] Loss: -402584.375000\n",
      "Train Epoch: 37 [4096/54000 (8%)] Loss: -411995.937500\n",
      "Train Epoch: 37 [8192/54000 (15%)] Loss: -399517.906250\n",
      "Train Epoch: 37 [12288/54000 (23%)] Loss: -402978.500000\n",
      "Train Epoch: 37 [16384/54000 (30%)] Loss: -385201.187500\n",
      "Train Epoch: 37 [20480/54000 (38%)] Loss: -415429.062500\n",
      "Train Epoch: 37 [24576/54000 (46%)] Loss: -387739.937500\n",
      "Train Epoch: 37 [28672/54000 (53%)] Loss: -403013.562500\n",
      "Train Epoch: 37 [32768/54000 (61%)] Loss: -410283.000000\n",
      "Train Epoch: 37 [36864/54000 (68%)] Loss: -454877.187500\n",
      "Train Epoch: 37 [40960/54000 (76%)] Loss: -400182.937500\n",
      "Train Epoch: 37 [45056/54000 (83%)] Loss: -427291.406250\n",
      "Train Epoch: 37 [49152/54000 (91%)] Loss: -453705.625000\n",
      "    epoch          : 37\n",
      "    loss           : -410891.0663109756\n",
      "    val_loss       : -427433.18203125\n",
      "Train Epoch: 38 [0/54000 (0%)] Loss: -419864.250000\n",
      "Train Epoch: 38 [4096/54000 (8%)] Loss: -385774.437500\n",
      "Train Epoch: 38 [8192/54000 (15%)] Loss: -405075.250000\n",
      "Train Epoch: 38 [12288/54000 (23%)] Loss: -406275.437500\n",
      "Train Epoch: 38 [16384/54000 (30%)] Loss: -387167.562500\n",
      "Train Epoch: 38 [20480/54000 (38%)] Loss: -415882.625000\n",
      "Train Epoch: 38 [24576/54000 (46%)] Loss: -415922.500000\n",
      "Train Epoch: 38 [28672/54000 (53%)] Loss: -402651.000000\n",
      "Train Epoch: 38 [32768/54000 (61%)] Loss: -416142.312500\n",
      "Train Epoch: 38 [36864/54000 (68%)] Loss: -378728.000000\n",
      "Train Epoch: 38 [40960/54000 (76%)] Loss: -406017.312500\n",
      "Train Epoch: 38 [45056/54000 (83%)] Loss: -417184.437500\n",
      "Train Epoch: 38 [49152/54000 (91%)] Loss: -455182.343750\n",
      "    epoch          : 38\n",
      "    loss           : -412110.1335365854\n",
      "    val_loss       : -428770.8578125\n",
      "Train Epoch: 39 [0/54000 (0%)] Loss: -458027.593750\n",
      "Train Epoch: 39 [4096/54000 (8%)] Loss: -417228.937500\n",
      "Train Epoch: 39 [8192/54000 (15%)] Loss: -406542.718750\n",
      "Train Epoch: 39 [12288/54000 (23%)] Loss: -404497.031250\n",
      "Train Epoch: 39 [16384/54000 (30%)] Loss: -406432.468750\n",
      "Train Epoch: 39 [20480/54000 (38%)] Loss: -417083.000000\n",
      "Train Epoch: 39 [24576/54000 (46%)] Loss: -408121.687500\n",
      "Train Epoch: 39 [28672/54000 (53%)] Loss: -456659.718750\n",
      "Train Epoch: 39 [32768/54000 (61%)] Loss: -411428.406250\n",
      "Train Epoch: 39 [36864/54000 (68%)] Loss: -428695.031250\n",
      "Train Epoch: 39 [40960/54000 (76%)] Loss: -412332.062500\n",
      "Train Epoch: 39 [45056/54000 (83%)] Loss: -432357.937500\n",
      "Train Epoch: 39 [49152/54000 (91%)] Loss: -457622.718750\n",
      "    epoch          : 39\n",
      "    loss           : -413404.61021341465\n",
      "    val_loss       : -429999.3015625\n",
      "Train Epoch: 40 [0/54000 (0%)] Loss: -457049.906250\n",
      "Train Epoch: 40 [4096/54000 (8%)] Loss: -407099.125000\n",
      "Train Epoch: 40 [8192/54000 (15%)] Loss: -422618.687500\n",
      "Train Epoch: 40 [12288/54000 (23%)] Loss: -417012.812500\n",
      "Train Epoch: 40 [16384/54000 (30%)] Loss: -434147.000000\n",
      "Train Epoch: 40 [20480/54000 (38%)] Loss: -421334.062500\n",
      "Train Epoch: 40 [24576/54000 (46%)] Loss: -420440.562500\n",
      "Train Epoch: 40 [28672/54000 (53%)] Loss: -406175.500000\n",
      "Train Epoch: 40 [32768/54000 (61%)] Loss: -431514.906250\n",
      "Train Epoch: 40 [36864/54000 (68%)] Loss: -458903.906250\n",
      "Train Epoch: 40 [40960/54000 (76%)] Loss: -407201.593750\n",
      "Train Epoch: 40 [45056/54000 (83%)] Loss: -431848.250000\n",
      "Train Epoch: 40 [49152/54000 (91%)] Loss: -458805.125000\n",
      "    epoch          : 40\n",
      "    loss           : -414835.2288109756\n",
      "    val_loss       : -431637.5828125\n",
      "Train Epoch: 41 [0/54000 (0%)] Loss: -458611.718750\n",
      "Train Epoch: 41 [4096/54000 (8%)] Loss: -404250.375000\n",
      "Train Epoch: 41 [8192/54000 (15%)] Loss: -410222.375000\n",
      "Train Epoch: 41 [12288/54000 (23%)] Loss: -418657.250000\n",
      "Train Epoch: 41 [16384/54000 (30%)] Loss: -408826.937500\n",
      "Train Epoch: 41 [20480/54000 (38%)] Loss: -417940.625000\n",
      "Train Epoch: 41 [24576/54000 (46%)] Loss: -409014.093750\n",
      "Train Epoch: 41 [28672/54000 (53%)] Loss: -459184.218750\n",
      "Train Epoch: 41 [32768/54000 (61%)] Loss: -411473.000000\n",
      "Train Epoch: 41 [36864/54000 (68%)] Loss: -433041.812500\n",
      "Train Epoch: 41 [40960/54000 (76%)] Loss: -410383.000000\n",
      "Train Epoch: 41 [45056/54000 (83%)] Loss: -387816.625000\n",
      "Train Epoch: 41 [49152/54000 (91%)] Loss: -425479.562500\n",
      "    epoch          : 41\n",
      "    loss           : -416003.2457317073\n",
      "    val_loss       : -432598.53125\n",
      "Train Epoch: 42 [0/54000 (0%)] Loss: -459420.625000\n",
      "Train Epoch: 42 [4096/54000 (8%)] Loss: -421815.812500\n",
      "Train Epoch: 42 [8192/54000 (15%)] Loss: -410596.750000\n",
      "Train Epoch: 42 [12288/54000 (23%)] Loss: -393584.718750\n",
      "Train Epoch: 42 [16384/54000 (30%)] Loss: -420403.750000\n",
      "Train Epoch: 42 [20480/54000 (38%)] Loss: -417677.343750\n",
      "Train Epoch: 42 [24576/54000 (46%)] Loss: -389228.656250\n",
      "Train Epoch: 42 [28672/54000 (53%)] Loss: -409108.156250\n",
      "Train Epoch: 42 [32768/54000 (61%)] Loss: -392567.750000\n",
      "Train Epoch: 42 [36864/54000 (68%)] Loss: -434067.875000\n",
      "Train Epoch: 42 [40960/54000 (76%)] Loss: -408734.937500\n",
      "Train Epoch: 42 [45056/54000 (83%)] Loss: -390229.750000\n",
      "Train Epoch: 42 [49152/54000 (91%)] Loss: -459804.500000\n",
      "    epoch          : 42\n",
      "    loss           : -417020.72042682924\n",
      "    val_loss       : -433165.6515625\n",
      "Train Epoch: 43 [0/54000 (0%)] Loss: -411379.250000\n",
      "Train Epoch: 43 [4096/54000 (8%)] Loss: -434569.437500\n",
      "Train Epoch: 43 [8192/54000 (15%)] Loss: -394541.031250\n",
      "Train Epoch: 43 [12288/54000 (23%)] Loss: -393108.406250\n",
      "Train Epoch: 43 [16384/54000 (30%)] Loss: -420854.531250\n",
      "Train Epoch: 43 [20480/54000 (38%)] Loss: -409815.218750\n",
      "Train Epoch: 43 [24576/54000 (46%)] Loss: -420157.281250\n",
      "Train Epoch: 43 [28672/54000 (53%)] Loss: -461594.468750\n",
      "Train Epoch: 43 [32768/54000 (61%)] Loss: -424075.437500\n",
      "Train Epoch: 43 [36864/54000 (68%)] Loss: -412501.062500\n",
      "Train Epoch: 43 [40960/54000 (76%)] Loss: -395521.906250\n",
      "Train Epoch: 43 [45056/54000 (83%)] Loss: -408508.250000\n",
      "Train Epoch: 43 [49152/54000 (91%)] Loss: -461341.500000\n",
      "    epoch          : 43\n",
      "    loss           : -418344.26112804876\n",
      "    val_loss       : -434684.72578125\n",
      "Train Epoch: 44 [0/54000 (0%)] Loss: -462105.500000\n",
      "Train Epoch: 44 [4096/54000 (8%)] Loss: -409883.312500\n",
      "Train Epoch: 44 [8192/54000 (15%)] Loss: -413198.562500\n",
      "Train Epoch: 44 [12288/54000 (23%)] Loss: -399865.187500\n",
      "Train Epoch: 44 [16384/54000 (30%)] Loss: -435531.031250\n",
      "Train Epoch: 44 [20480/54000 (38%)] Loss: -425357.687500\n",
      "Train Epoch: 44 [24576/54000 (46%)] Loss: -422126.437500\n",
      "Train Epoch: 44 [28672/54000 (53%)] Loss: -462184.812500\n",
      "Train Epoch: 44 [32768/54000 (61%)] Loss: -395139.437500\n",
      "Train Epoch: 44 [36864/54000 (68%)] Loss: -461665.125000\n",
      "Train Epoch: 44 [40960/54000 (76%)] Loss: -409702.437500\n",
      "Train Epoch: 44 [45056/54000 (83%)] Loss: -425625.250000\n",
      "Train Epoch: 44 [49152/54000 (91%)] Loss: -462854.781250\n",
      "    epoch          : 44\n",
      "    loss           : -419383.2681402439\n",
      "    val_loss       : -434569.63515625\n",
      "Train Epoch: 45 [0/54000 (0%)] Loss: -395128.125000\n",
      "Train Epoch: 45 [4096/54000 (8%)] Loss: -423801.437500\n",
      "Train Epoch: 45 [8192/54000 (15%)] Loss: -426349.375000\n",
      "Train Epoch: 45 [12288/54000 (23%)] Loss: -425404.437500\n",
      "Train Epoch: 45 [16384/54000 (30%)] Loss: -399082.062500\n",
      "Train Epoch: 45 [20480/54000 (38%)] Loss: -423929.375000\n",
      "Train Epoch: 45 [24576/54000 (46%)] Loss: -437219.937500\n",
      "Train Epoch: 45 [28672/54000 (53%)] Loss: -410999.406250\n",
      "Train Epoch: 45 [32768/54000 (61%)] Loss: -426215.656250\n",
      "Train Epoch: 45 [36864/54000 (68%)] Loss: -413518.562500\n",
      "Train Epoch: 45 [40960/54000 (76%)] Loss: -422417.625000\n",
      "Train Epoch: 45 [45056/54000 (83%)] Loss: -397044.375000\n",
      "Train Epoch: 45 [49152/54000 (91%)] Loss: -464608.625000\n",
      "    epoch          : 45\n",
      "    loss           : -420448.58140243904\n",
      "    val_loss       : -437173.6890625\n",
      "Train Epoch: 46 [0/54000 (0%)] Loss: -396906.875000\n",
      "Train Epoch: 46 [4096/54000 (8%)] Loss: -423545.500000\n",
      "Train Epoch: 46 [8192/54000 (15%)] Loss: -429249.437500\n",
      "Train Epoch: 46 [12288/54000 (23%)] Loss: -462293.937500\n",
      "Train Epoch: 46 [16384/54000 (30%)] Loss: -392430.000000\n",
      "Train Epoch: 46 [20480/54000 (38%)] Loss: -429120.187500\n",
      "Train Epoch: 46 [24576/54000 (46%)] Loss: -400827.156250\n",
      "Train Epoch: 46 [28672/54000 (53%)] Loss: -411510.562500\n",
      "Train Epoch: 46 [32768/54000 (61%)] Loss: -412863.468750\n",
      "Train Epoch: 46 [36864/54000 (68%)] Loss: -463850.531250\n",
      "Train Epoch: 46 [40960/54000 (76%)] Loss: -411472.468750\n",
      "Train Epoch: 46 [45056/54000 (83%)] Loss: -396861.093750\n",
      "Train Epoch: 46 [49152/54000 (91%)] Loss: -464879.406250\n",
      "    epoch          : 46\n",
      "    loss           : -421341.4910060976\n",
      "    val_loss       : -437236.00078125\n",
      "Train Epoch: 47 [0/54000 (0%)] Loss: -415763.125000\n",
      "Train Epoch: 47 [4096/54000 (8%)] Loss: -425038.937500\n",
      "Train Epoch: 47 [8192/54000 (15%)] Loss: -414861.062500\n",
      "Train Epoch: 47 [12288/54000 (23%)] Loss: -464268.937500\n",
      "Train Epoch: 47 [16384/54000 (30%)] Loss: -412058.187500\n",
      "Train Epoch: 47 [20480/54000 (38%)] Loss: -418623.250000\n",
      "Train Epoch: 47 [24576/54000 (46%)] Loss: -414468.187500\n",
      "Train Epoch: 47 [28672/54000 (53%)] Loss: -415891.125000\n",
      "Train Epoch: 47 [32768/54000 (61%)] Loss: -413661.750000\n",
      "Train Epoch: 47 [36864/54000 (68%)] Loss: -428938.156250\n",
      "Train Epoch: 47 [40960/54000 (76%)] Loss: -413900.062500\n",
      "Train Epoch: 47 [45056/54000 (83%)] Loss: -403028.125000\n",
      "Train Epoch: 47 [49152/54000 (91%)] Loss: -465152.375000\n",
      "    epoch          : 47\n",
      "    loss           : -422210.78033536585\n",
      "    val_loss       : -438581.17421875\n",
      "Train Epoch: 48 [0/54000 (0%)] Loss: -464952.718750\n",
      "Train Epoch: 48 [4096/54000 (8%)] Loss: -415260.000000\n",
      "Train Epoch: 48 [8192/54000 (15%)] Loss: -416218.968750\n",
      "Train Epoch: 48 [12288/54000 (23%)] Loss: -465221.968750\n",
      "Train Epoch: 48 [16384/54000 (30%)] Loss: -438370.625000\n",
      "Train Epoch: 48 [20480/54000 (38%)] Loss: -427496.062500\n",
      "Train Epoch: 48 [24576/54000 (46%)] Loss: -424468.187500\n",
      "Train Epoch: 48 [28672/54000 (53%)] Loss: -416339.468750\n",
      "Train Epoch: 48 [32768/54000 (61%)] Loss: -415016.062500\n",
      "Train Epoch: 48 [36864/54000 (68%)] Loss: -396363.437500\n",
      "Train Epoch: 48 [40960/54000 (76%)] Loss: -414254.375000\n",
      "Train Epoch: 48 [45056/54000 (83%)] Loss: -439995.062500\n",
      "Train Epoch: 48 [49152/54000 (91%)] Loss: -465476.343750\n",
      "    epoch          : 48\n",
      "    loss           : -423125.78094512195\n",
      "    val_loss       : -439399.10625\n",
      "Train Epoch: 49 [0/54000 (0%)] Loss: -466652.250000\n",
      "Train Epoch: 49 [4096/54000 (8%)] Loss: -414891.625000\n",
      "Train Epoch: 49 [8192/54000 (15%)] Loss: -429122.875000\n",
      "Train Epoch: 49 [12288/54000 (23%)] Loss: -416487.250000\n",
      "Train Epoch: 49 [16384/54000 (30%)] Loss: -467058.000000\n",
      "Train Epoch: 49 [20480/54000 (38%)] Loss: -416721.125000\n",
      "Train Epoch: 49 [24576/54000 (46%)] Loss: -439869.500000\n",
      "Train Epoch: 49 [28672/54000 (53%)] Loss: -414922.375000\n",
      "Train Epoch: 49 [32768/54000 (61%)] Loss: -414131.250000\n",
      "Train Epoch: 49 [36864/54000 (68%)] Loss: -405484.187500\n",
      "Train Epoch: 49 [40960/54000 (76%)] Loss: -423659.875000\n",
      "Train Epoch: 49 [45056/54000 (83%)] Loss: -430896.750000\n",
      "Train Epoch: 49 [49152/54000 (91%)] Loss: -467160.156250\n",
      "    epoch          : 49\n",
      "    loss           : -423854.84954268293\n",
      "    val_loss       : -439683.33515625\n",
      "Train Epoch: 50 [0/54000 (0%)] Loss: -466360.625000\n",
      "Train Epoch: 50 [4096/54000 (8%)] Loss: -426437.125000\n",
      "Train Epoch: 50 [8192/54000 (15%)] Loss: -398076.593750\n",
      "Train Epoch: 50 [12288/54000 (23%)] Loss: -417769.062500\n",
      "Train Epoch: 50 [16384/54000 (30%)] Loss: -441196.750000\n",
      "Train Epoch: 50 [20480/54000 (38%)] Loss: -424841.062500\n",
      "Train Epoch: 50 [24576/54000 (46%)] Loss: -411574.750000\n",
      "Train Epoch: 50 [28672/54000 (53%)] Loss: -401597.156250\n",
      "Train Epoch: 50 [32768/54000 (61%)] Loss: -425175.343750\n",
      "Train Epoch: 50 [36864/54000 (68%)] Loss: -441234.062500\n",
      "Train Epoch: 50 [40960/54000 (76%)] Loss: -426910.250000\n",
      "Train Epoch: 50 [45056/54000 (83%)] Loss: -403678.531250\n",
      "Train Epoch: 50 [49152/54000 (91%)] Loss: -467937.531250\n",
      "    epoch          : 50\n",
      "    loss           : -424353.30076219514\n",
      "    val_loss       : -440876.71171875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0510_004522/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [0/54000 (0%)] Loss: -433017.343750\n",
      "Train Epoch: 51 [4096/54000 (8%)] Loss: -398277.750000\n",
      "Train Epoch: 51 [8192/54000 (15%)] Loss: -432068.000000\n",
      "Train Epoch: 51 [12288/54000 (23%)] Loss: -417070.156250\n",
      "Train Epoch: 51 [16384/54000 (30%)] Loss: -417223.437500\n",
      "Train Epoch: 51 [20480/54000 (38%)] Loss: -431005.437500\n",
      "Train Epoch: 51 [24576/54000 (46%)] Loss: -443470.468750\n",
      "Train Epoch: 51 [28672/54000 (53%)] Loss: -419182.093750\n",
      "Train Epoch: 51 [32768/54000 (61%)] Loss: -427381.625000\n",
      "Train Epoch: 51 [36864/54000 (68%)] Loss: -418228.312500\n",
      "Train Epoch: 51 [40960/54000 (76%)] Loss: -430169.250000\n",
      "Train Epoch: 51 [45056/54000 (83%)] Loss: -404990.281250\n",
      "Train Epoch: 51 [49152/54000 (91%)] Loss: -467407.687500\n",
      "    epoch          : 51\n",
      "    loss           : -425326.69283536583\n",
      "    val_loss       : -441512.459375\n",
      "Train Epoch: 52 [0/54000 (0%)] Loss: -466429.812500\n",
      "Train Epoch: 52 [4096/54000 (8%)] Loss: -419098.187500\n",
      "Train Epoch: 52 [8192/54000 (15%)] Loss: -430301.812500\n",
      "Train Epoch: 52 [12288/54000 (23%)] Loss: -429384.281250\n",
      "Train Epoch: 52 [16384/54000 (30%)] Loss: -420022.062500\n",
      "Train Epoch: 52 [20480/54000 (38%)] Loss: -429390.250000\n",
      "Train Epoch: 52 [24576/54000 (46%)] Loss: -406577.375000\n",
      "Train Epoch: 52 [28672/54000 (53%)] Loss: -417431.250000\n",
      "Train Epoch: 52 [32768/54000 (61%)] Loss: -467489.906250\n",
      "Train Epoch: 52 [36864/54000 (68%)] Loss: -406432.375000\n",
      "Train Epoch: 52 [40960/54000 (76%)] Loss: -415847.687500\n",
      "Train Epoch: 52 [45056/54000 (83%)] Loss: -443964.437500\n",
      "Train Epoch: 52 [49152/54000 (91%)] Loss: -416715.062500\n",
      "    epoch          : 52\n",
      "    loss           : -425931.37652439025\n",
      "    val_loss       : -442058.97734375\n",
      "Train Epoch: 53 [0/54000 (0%)] Loss: -469254.687500\n",
      "Train Epoch: 53 [4096/54000 (8%)] Loss: -417522.937500\n",
      "Train Epoch: 53 [8192/54000 (15%)] Loss: -406213.875000\n",
      "Train Epoch: 53 [12288/54000 (23%)] Loss: -467330.062500\n",
      "Train Epoch: 53 [16384/54000 (30%)] Loss: -408096.187500\n",
      "Train Epoch: 53 [20480/54000 (38%)] Loss: -426421.625000\n",
      "Train Epoch: 53 [24576/54000 (46%)] Loss: -418202.156250\n",
      "Train Epoch: 53 [28672/54000 (53%)] Loss: -423210.906250\n",
      "Train Epoch: 53 [32768/54000 (61%)] Loss: -467749.625000\n",
      "Train Epoch: 53 [36864/54000 (68%)] Loss: -439786.687500\n",
      "Train Epoch: 53 [40960/54000 (76%)] Loss: -427436.812500\n",
      "Train Epoch: 53 [45056/54000 (83%)] Loss: -440809.375000\n",
      "Train Epoch: 53 [49152/54000 (91%)] Loss: -426048.250000\n",
      "    epoch          : 53\n",
      "    loss           : -426627.2663109756\n",
      "    val_loss       : -442054.340625\n",
      "Train Epoch: 54 [0/54000 (0%)] Loss: -469296.687500\n",
      "Train Epoch: 54 [4096/54000 (8%)] Loss: -408061.875000\n",
      "Train Epoch: 54 [8192/54000 (15%)] Loss: -405039.937500\n",
      "Train Epoch: 54 [12288/54000 (23%)] Loss: -398289.312500\n",
      "Train Epoch: 54 [16384/54000 (30%)] Loss: -429730.093750\n",
      "Train Epoch: 54 [20480/54000 (38%)] Loss: -433348.781250\n",
      "Train Epoch: 54 [24576/54000 (46%)] Loss: -428933.000000\n",
      "Train Epoch: 54 [28672/54000 (53%)] Loss: -421910.125000\n",
      "Train Epoch: 54 [32768/54000 (61%)] Loss: -428203.750000\n",
      "Train Epoch: 54 [36864/54000 (68%)] Loss: -444092.906250\n",
      "Train Epoch: 54 [40960/54000 (76%)] Loss: -420948.281250\n",
      "Train Epoch: 54 [45056/54000 (83%)] Loss: -433767.125000\n",
      "Train Epoch: 54 [49152/54000 (91%)] Loss: -470091.718750\n",
      "    epoch          : 54\n",
      "    loss           : -427102.34375\n",
      "    val_loss       : -442904.64375\n",
      "Train Epoch: 55 [0/54000 (0%)] Loss: -469423.000000\n",
      "Train Epoch: 55 [4096/54000 (8%)] Loss: -403793.843750\n",
      "Train Epoch: 55 [8192/54000 (15%)] Loss: -433679.312500\n",
      "Train Epoch: 55 [12288/54000 (23%)] Loss: -418807.750000\n",
      "Train Epoch: 55 [16384/54000 (30%)] Loss: -422784.093750\n",
      "Train Epoch: 55 [20480/54000 (38%)] Loss: -433152.875000\n",
      "Train Epoch: 55 [24576/54000 (46%)] Loss: -407071.000000\n",
      "Train Epoch: 55 [28672/54000 (53%)] Loss: -421472.375000\n",
      "Train Epoch: 55 [32768/54000 (61%)] Loss: -430979.812500\n",
      "Train Epoch: 55 [36864/54000 (68%)] Loss: -442970.937500\n",
      "Train Epoch: 55 [40960/54000 (76%)] Loss: -418486.875000\n",
      "Train Epoch: 55 [45056/54000 (83%)] Loss: -406134.843750\n",
      "Train Epoch: 55 [49152/54000 (91%)] Loss: -470440.406250\n",
      "    epoch          : 55\n",
      "    loss           : -427923.76539634145\n",
      "    val_loss       : -443898.6953125\n",
      "Train Epoch: 56 [0/54000 (0%)] Loss: -419612.656250\n",
      "Train Epoch: 56 [4096/54000 (8%)] Loss: -424013.187500\n",
      "Train Epoch: 56 [8192/54000 (15%)] Loss: -420101.187500\n",
      "Train Epoch: 56 [12288/54000 (23%)] Loss: -431835.125000\n",
      "Train Epoch: 56 [16384/54000 (30%)] Loss: -420595.312500\n",
      "Train Epoch: 56 [20480/54000 (38%)] Loss: -432874.937500\n",
      "Train Epoch: 56 [24576/54000 (46%)] Loss: -427700.062500\n",
      "Train Epoch: 56 [28672/54000 (53%)] Loss: -470783.312500\n",
      "Train Epoch: 56 [32768/54000 (61%)] Loss: -433822.687500\n",
      "Train Epoch: 56 [36864/54000 (68%)] Loss: -469422.750000\n",
      "Train Epoch: 56 [40960/54000 (76%)] Loss: -406137.437500\n",
      "Train Epoch: 56 [45056/54000 (83%)] Loss: -408215.937500\n",
      "Train Epoch: 56 [49152/54000 (91%)] Loss: -470038.156250\n",
      "    epoch          : 56\n",
      "    loss           : -428548.0050304878\n",
      "    val_loss       : -444233.40625\n",
      "Train Epoch: 57 [0/54000 (0%)] Loss: -407451.687500\n",
      "Train Epoch: 57 [4096/54000 (8%)] Loss: -421035.875000\n",
      "Train Epoch: 57 [8192/54000 (15%)] Loss: -423661.000000\n",
      "Train Epoch: 57 [12288/54000 (23%)] Loss: -433154.500000\n",
      "Train Epoch: 57 [16384/54000 (30%)] Loss: -404721.812500\n",
      "Train Epoch: 57 [20480/54000 (38%)] Loss: -436381.437500\n",
      "Train Epoch: 57 [24576/54000 (46%)] Loss: -416853.437500\n",
      "Train Epoch: 57 [28672/54000 (53%)] Loss: -470713.437500\n",
      "Train Epoch: 57 [32768/54000 (61%)] Loss: -432055.562500\n",
      "Train Epoch: 57 [36864/54000 (68%)] Loss: -436057.906250\n",
      "Train Epoch: 57 [40960/54000 (76%)] Loss: -422400.500000\n",
      "Train Epoch: 57 [45056/54000 (83%)] Loss: -443744.781250\n",
      "Train Epoch: 57 [49152/54000 (91%)] Loss: -471823.750000\n",
      "    epoch          : 57\n",
      "    loss           : -429079.54527439026\n",
      "    val_loss       : -444894.70078125\n",
      "Train Epoch: 58 [0/54000 (0%)] Loss: -470199.437500\n",
      "Train Epoch: 58 [4096/54000 (8%)] Loss: -406054.093750\n",
      "Train Epoch: 58 [8192/54000 (15%)] Loss: -445874.718750\n",
      "Train Epoch: 58 [12288/54000 (23%)] Loss: -407866.875000\n",
      "Train Epoch: 58 [16384/54000 (30%)] Loss: -419650.062500\n",
      "Train Epoch: 58 [20480/54000 (38%)] Loss: -436757.500000\n",
      "Train Epoch: 58 [24576/54000 (46%)] Loss: -421999.156250\n",
      "Train Epoch: 58 [28672/54000 (53%)] Loss: -417951.875000\n",
      "Train Epoch: 58 [32768/54000 (61%)] Loss: -424827.343750\n",
      "Train Epoch: 58 [36864/54000 (68%)] Loss: -404534.687500\n",
      "Train Epoch: 58 [40960/54000 (76%)] Loss: -417161.750000\n",
      "Train Epoch: 58 [45056/54000 (83%)] Loss: -436377.000000\n",
      "Train Epoch: 58 [49152/54000 (91%)] Loss: -471991.812500\n",
      "    epoch          : 58\n",
      "    loss           : -429622.87042682926\n",
      "    val_loss       : -445078.628125\n",
      "Train Epoch: 59 [0/54000 (0%)] Loss: -471257.187500\n",
      "Train Epoch: 59 [4096/54000 (8%)] Loss: -418664.656250\n",
      "Train Epoch: 59 [8192/54000 (15%)] Loss: -405820.125000\n",
      "Train Epoch: 59 [12288/54000 (23%)] Loss: -432361.312500\n",
      "Train Epoch: 59 [16384/54000 (30%)] Loss: -408923.187500\n",
      "Train Epoch: 59 [20480/54000 (38%)] Loss: -429843.906250\n",
      "Train Epoch: 59 [24576/54000 (46%)] Loss: -409465.062500\n",
      "Train Epoch: 59 [28672/54000 (53%)] Loss: -421310.093750\n",
      "Train Epoch: 59 [32768/54000 (61%)] Loss: -471903.812500\n",
      "Train Epoch: 59 [36864/54000 (68%)] Loss: -444708.968750\n",
      "Train Epoch: 59 [40960/54000 (76%)] Loss: -436768.562500\n",
      "Train Epoch: 59 [45056/54000 (83%)] Loss: -420417.562500\n",
      "Train Epoch: 59 [49152/54000 (91%)] Loss: -472389.937500\n",
      "    epoch          : 59\n",
      "    loss           : -430195.79588414636\n",
      "    val_loss       : -445629.52109375\n",
      "Train Epoch: 60 [0/54000 (0%)] Loss: -438391.062500\n",
      "Train Epoch: 60 [4096/54000 (8%)] Loss: -405490.968750\n",
      "Train Epoch: 60 [8192/54000 (15%)] Loss: -408014.187500\n",
      "Train Epoch: 60 [12288/54000 (23%)] Loss: -422142.968750\n",
      "Train Epoch: 60 [16384/54000 (30%)] Loss: -431318.000000\n",
      "Train Epoch: 60 [20480/54000 (38%)] Loss: -436190.437500\n",
      "Train Epoch: 60 [24576/54000 (46%)] Loss: -443593.406250\n",
      "Train Epoch: 60 [28672/54000 (53%)] Loss: -424222.343750\n",
      "Train Epoch: 60 [32768/54000 (61%)] Loss: -422490.906250\n",
      "Train Epoch: 60 [36864/54000 (68%)] Loss: -431817.062500\n",
      "Train Epoch: 60 [40960/54000 (76%)] Loss: -428579.500000\n",
      "Train Epoch: 60 [45056/54000 (83%)] Loss: -434227.187500\n",
      "Train Epoch: 60 [49152/54000 (91%)] Loss: -472287.062500\n",
      "    epoch          : 60\n",
      "    loss           : -430527.3506097561\n",
      "    val_loss       : -446557.0671875\n",
      "Train Epoch: 61 [0/54000 (0%)] Loss: -472887.125000\n",
      "Train Epoch: 61 [4096/54000 (8%)] Loss: -422716.000000\n",
      "Train Epoch: 61 [8192/54000 (15%)] Loss: -437787.468750\n",
      "Train Epoch: 61 [12288/54000 (23%)] Loss: -423174.093750\n",
      "Train Epoch: 61 [16384/54000 (30%)] Loss: -445405.093750\n",
      "Train Epoch: 61 [20480/54000 (38%)] Loss: -437342.125000\n",
      "Train Epoch: 61 [24576/54000 (46%)] Loss: -432849.187500\n",
      "Train Epoch: 61 [28672/54000 (53%)] Loss: -473351.937500\n",
      "Train Epoch: 61 [32768/54000 (61%)] Loss: -433496.531250\n",
      "Train Epoch: 61 [36864/54000 (68%)] Loss: -410957.531250\n",
      "Train Epoch: 61 [40960/54000 (76%)] Loss: -422646.093750\n",
      "Train Epoch: 61 [45056/54000 (83%)] Loss: -409413.437500\n",
      "Train Epoch: 61 [49152/54000 (91%)] Loss: -473184.625000\n",
      "    epoch          : 61\n",
      "    loss           : -431140.8762195122\n",
      "    val_loss       : -446474.371875\n",
      "Train Epoch: 62 [0/54000 (0%)] Loss: -473936.625000\n",
      "Train Epoch: 62 [4096/54000 (8%)] Loss: -406988.406250\n",
      "Train Epoch: 62 [8192/54000 (15%)] Loss: -437172.312500\n",
      "Train Epoch: 62 [12288/54000 (23%)] Loss: -420719.625000\n",
      "Train Epoch: 62 [16384/54000 (30%)] Loss: -421292.000000\n",
      "Train Epoch: 62 [20480/54000 (38%)] Loss: -435788.937500\n",
      "Train Epoch: 62 [24576/54000 (46%)] Loss: -424688.750000\n",
      "Train Epoch: 62 [28672/54000 (53%)] Loss: -422031.187500\n",
      "Train Epoch: 62 [32768/54000 (61%)] Loss: -420269.625000\n",
      "Train Epoch: 62 [36864/54000 (68%)] Loss: -408676.531250\n",
      "Train Epoch: 62 [40960/54000 (76%)] Loss: -425197.937500\n",
      "Train Epoch: 62 [45056/54000 (83%)] Loss: -437487.750000\n",
      "Train Epoch: 62 [49152/54000 (91%)] Loss: -473525.187500\n",
      "    epoch          : 62\n",
      "    loss           : -431700.3138719512\n",
      "    val_loss       : -447207.83359375\n",
      "Train Epoch: 63 [0/54000 (0%)] Loss: -473508.125000\n",
      "Train Epoch: 63 [4096/54000 (8%)] Loss: -424566.437500\n",
      "Train Epoch: 63 [8192/54000 (15%)] Loss: -424138.875000\n",
      "Train Epoch: 63 [12288/54000 (23%)] Loss: -412935.375000\n",
      "Train Epoch: 63 [16384/54000 (30%)] Loss: -436245.750000\n",
      "Train Epoch: 63 [20480/54000 (38%)] Loss: -439769.437500\n",
      "Train Epoch: 63 [24576/54000 (46%)] Loss: -421460.562500\n",
      "Train Epoch: 63 [28672/54000 (53%)] Loss: -424806.437500\n",
      "Train Epoch: 63 [32768/54000 (61%)] Loss: -448101.437500\n",
      "Train Epoch: 63 [36864/54000 (68%)] Loss: -410024.031250\n",
      "Train Epoch: 63 [40960/54000 (76%)] Loss: -420018.000000\n",
      "Train Epoch: 63 [45056/54000 (83%)] Loss: -439301.750000\n",
      "Train Epoch: 63 [49152/54000 (91%)] Loss: -474403.937500\n",
      "    epoch          : 63\n",
      "    loss           : -432131.4644817073\n",
      "    val_loss       : -447297.32109375\n",
      "Train Epoch: 64 [0/54000 (0%)] Loss: -473864.843750\n",
      "Train Epoch: 64 [4096/54000 (8%)] Loss: -426179.250000\n",
      "Train Epoch: 64 [8192/54000 (15%)] Loss: -436376.218750\n",
      "Train Epoch: 64 [12288/54000 (23%)] Loss: -412062.843750\n",
      "Train Epoch: 64 [16384/54000 (30%)] Loss: -407885.906250\n",
      "Train Epoch: 64 [20480/54000 (38%)] Loss: -432960.062500\n",
      "Train Epoch: 64 [24576/54000 (46%)] Loss: -422016.187500\n",
      "Train Epoch: 64 [28672/54000 (53%)] Loss: -439354.312500\n",
      "Train Epoch: 64 [32768/54000 (61%)] Loss: -411838.906250\n",
      "Train Epoch: 64 [36864/54000 (68%)] Loss: -447746.500000\n",
      "Train Epoch: 64 [40960/54000 (76%)] Loss: -432918.062500\n",
      "Train Epoch: 64 [45056/54000 (83%)] Loss: -409278.875000\n",
      "Train Epoch: 64 [49152/54000 (91%)] Loss: -475097.937500\n",
      "    epoch          : 64\n",
      "    loss           : -432529.2306402439\n",
      "    val_loss       : -447576.7484375\n",
      "Train Epoch: 65 [0/54000 (0%)] Loss: -424133.500000\n",
      "Train Epoch: 65 [4096/54000 (8%)] Loss: -447982.031250\n",
      "Train Epoch: 65 [8192/54000 (15%)] Loss: -425634.250000\n",
      "Train Epoch: 65 [12288/54000 (23%)] Loss: -426847.750000\n",
      "Train Epoch: 65 [16384/54000 (30%)] Loss: -409440.000000\n",
      "Train Epoch: 65 [20480/54000 (38%)] Loss: -426363.250000\n",
      "Train Epoch: 65 [24576/54000 (46%)] Loss: -448391.593750\n",
      "Train Epoch: 65 [28672/54000 (53%)] Loss: -475994.875000\n",
      "Train Epoch: 65 [32768/54000 (61%)] Loss: -433932.437500\n",
      "Train Epoch: 65 [36864/54000 (68%)] Loss: -447403.156250\n",
      "Train Epoch: 65 [40960/54000 (76%)] Loss: -427529.437500\n",
      "Train Epoch: 65 [45056/54000 (83%)] Loss: -412856.156250\n",
      "Train Epoch: 65 [49152/54000 (91%)] Loss: -475095.437500\n",
      "    epoch          : 65\n",
      "    loss           : -432985.49908536585\n",
      "    val_loss       : -447516.81484375\n",
      "Train Epoch: 66 [0/54000 (0%)] Loss: -474849.000000\n",
      "Train Epoch: 66 [4096/54000 (8%)] Loss: -439330.343750\n",
      "Train Epoch: 66 [8192/54000 (15%)] Loss: -412741.156250\n",
      "Train Epoch: 66 [12288/54000 (23%)] Loss: -413602.906250\n",
      "Train Epoch: 66 [16384/54000 (30%)] Loss: -475466.781250\n",
      "Train Epoch: 66 [20480/54000 (38%)] Loss: -431996.687500\n",
      "Train Epoch: 66 [24576/54000 (46%)] Loss: -431877.750000\n",
      "Train Epoch: 66 [28672/54000 (53%)] Loss: -426731.062500\n",
      "Train Epoch: 66 [32768/54000 (61%)] Loss: -423133.781250\n",
      "Train Epoch: 66 [36864/54000 (68%)] Loss: -474796.968750\n",
      "Train Epoch: 66 [40960/54000 (76%)] Loss: -406228.218750\n",
      "Train Epoch: 66 [45056/54000 (83%)] Loss: -440459.843750\n",
      "Train Epoch: 66 [49152/54000 (91%)] Loss: -474720.625000\n",
      "    epoch          : 66\n",
      "    loss           : -433375.01524390245\n",
      "    val_loss       : -448306.22578125\n",
      "Train Epoch: 67 [0/54000 (0%)] Loss: -475327.406250\n",
      "Train Epoch: 67 [4096/54000 (8%)] Loss: -436515.968750\n",
      "Train Epoch: 67 [8192/54000 (15%)] Loss: -428851.000000\n",
      "Train Epoch: 67 [12288/54000 (23%)] Loss: -435136.281250\n",
      "Train Epoch: 67 [16384/54000 (30%)] Loss: -416106.656250\n",
      "Train Epoch: 67 [20480/54000 (38%)] Loss: -437811.218750\n",
      "Train Epoch: 67 [24576/54000 (46%)] Loss: -410074.937500\n",
      "Train Epoch: 67 [28672/54000 (53%)] Loss: -424530.500000\n",
      "Train Epoch: 67 [32768/54000 (61%)] Loss: -431853.875000\n",
      "Train Epoch: 67 [36864/54000 (68%)] Loss: -447453.000000\n",
      "Train Epoch: 67 [40960/54000 (76%)] Loss: -429186.250000\n",
      "Train Epoch: 67 [45056/54000 (83%)] Loss: -416357.375000\n",
      "Train Epoch: 67 [49152/54000 (91%)] Loss: -475804.375000\n",
      "    epoch          : 67\n",
      "    loss           : -433985.95884146343\n",
      "    val_loss       : -448063.11015625\n",
      "Train Epoch: 68 [0/54000 (0%)] Loss: -442015.781250\n",
      "Train Epoch: 68 [4096/54000 (8%)] Loss: -421925.125000\n",
      "Train Epoch: 68 [8192/54000 (15%)] Loss: -438907.437500\n",
      "Train Epoch: 68 [12288/54000 (23%)] Loss: -422490.218750\n",
      "Train Epoch: 68 [16384/54000 (30%)] Loss: -409233.437500\n",
      "Train Epoch: 68 [20480/54000 (38%)] Loss: -437499.125000\n",
      "Train Epoch: 68 [24576/54000 (46%)] Loss: -413677.187500\n",
      "Train Epoch: 68 [28672/54000 (53%)] Loss: -440298.187500\n",
      "Train Epoch: 68 [32768/54000 (61%)] Loss: -422876.375000\n",
      "Train Epoch: 68 [36864/54000 (68%)] Loss: -412146.625000\n",
      "Train Epoch: 68 [40960/54000 (76%)] Loss: -425467.718750\n",
      "Train Epoch: 68 [45056/54000 (83%)] Loss: -414459.625000\n",
      "Train Epoch: 68 [49152/54000 (91%)] Loss: -477377.812500\n",
      "    epoch          : 68\n",
      "    loss           : -434308.94984756096\n",
      "    val_loss       : -448856.43203125\n",
      "Train Epoch: 69 [0/54000 (0%)] Loss: -475621.750000\n",
      "Train Epoch: 69 [4096/54000 (8%)] Loss: -424425.812500\n",
      "Train Epoch: 69 [8192/54000 (15%)] Loss: -432510.750000\n",
      "Train Epoch: 69 [12288/54000 (23%)] Loss: -449798.968750\n",
      "Train Epoch: 69 [16384/54000 (30%)] Loss: -412445.250000\n",
      "Train Epoch: 69 [20480/54000 (38%)] Loss: -441293.906250\n",
      "Train Epoch: 69 [24576/54000 (46%)] Loss: -410872.000000\n",
      "Train Epoch: 69 [28672/54000 (53%)] Loss: -478018.812500\n",
      "Train Epoch: 69 [32768/54000 (61%)] Loss: -438880.812500\n",
      "Train Epoch: 69 [36864/54000 (68%)] Loss: -415005.125000\n",
      "Train Epoch: 69 [40960/54000 (76%)] Loss: -427057.875000\n",
      "Train Epoch: 69 [45056/54000 (83%)] Loss: -448052.343750\n",
      "Train Epoch: 69 [49152/54000 (91%)] Loss: -475946.093750\n",
      "    epoch          : 69\n",
      "    loss           : -434815.50274390244\n",
      "    val_loss       : -449318.40546875\n",
      "Train Epoch: 70 [0/54000 (0%)] Loss: -476404.062500\n",
      "Train Epoch: 70 [4096/54000 (8%)] Loss: -412618.468750\n",
      "Train Epoch: 70 [8192/54000 (15%)] Loss: -428597.593750\n",
      "Train Epoch: 70 [12288/54000 (23%)] Loss: -427006.625000\n",
      "Train Epoch: 70 [16384/54000 (30%)] Loss: -450246.625000\n",
      "Train Epoch: 70 [20480/54000 (38%)] Loss: -429522.125000\n",
      "Train Epoch: 70 [24576/54000 (46%)] Loss: -477379.187500\n",
      "Train Epoch: 70 [28672/54000 (53%)] Loss: -478257.812500\n",
      "Train Epoch: 70 [32768/54000 (61%)] Loss: -440380.593750\n",
      "Train Epoch: 70 [36864/54000 (68%)] Loss: -413823.031250\n",
      "Train Epoch: 70 [40960/54000 (76%)] Loss: -436710.937500\n",
      "Train Epoch: 70 [45056/54000 (83%)] Loss: -439663.937500\n",
      "Train Epoch: 70 [49152/54000 (91%)] Loss: -439389.562500\n",
      "    epoch          : 70\n",
      "    loss           : -435170.14222560974\n",
      "    val_loss       : -449547.571875\n",
      "Train Epoch: 71 [0/54000 (0%)] Loss: -448131.343750\n",
      "Train Epoch: 71 [4096/54000 (8%)] Loss: -416590.656250\n",
      "Train Epoch: 71 [8192/54000 (15%)] Loss: -428041.937500\n",
      "Train Epoch: 71 [12288/54000 (23%)] Loss: -416186.000000\n",
      "Train Epoch: 71 [16384/54000 (30%)] Loss: -449072.750000\n",
      "Train Epoch: 71 [20480/54000 (38%)] Loss: -444960.000000\n",
      "Train Epoch: 71 [24576/54000 (46%)] Loss: -415038.281250\n",
      "Train Epoch: 71 [28672/54000 (53%)] Loss: -478268.562500\n",
      "Train Epoch: 71 [32768/54000 (61%)] Loss: -440894.156250\n",
      "Train Epoch: 71 [36864/54000 (68%)] Loss: -478472.000000\n",
      "Train Epoch: 71 [40960/54000 (76%)] Loss: -426360.562500\n",
      "Train Epoch: 71 [45056/54000 (83%)] Loss: -451096.781250\n",
      "Train Epoch: 71 [49152/54000 (91%)] Loss: -477865.375000\n",
      "    epoch          : 71\n",
      "    loss           : -435356.5288109756\n",
      "    val_loss       : -449517.78046875\n",
      "Train Epoch: 72 [0/54000 (0%)] Loss: -429859.000000\n",
      "Train Epoch: 72 [4096/54000 (8%)] Loss: -409016.187500\n",
      "Train Epoch: 72 [8192/54000 (15%)] Loss: -418644.500000\n",
      "Train Epoch: 72 [12288/54000 (23%)] Loss: -423951.687500\n",
      "Train Epoch: 72 [16384/54000 (30%)] Loss: -478474.750000\n",
      "Train Epoch: 72 [20480/54000 (38%)] Loss: -442269.156250\n",
      "Train Epoch: 72 [24576/54000 (46%)] Loss: -426730.125000\n",
      "Train Epoch: 72 [28672/54000 (53%)] Loss: -427441.406250\n",
      "Train Epoch: 72 [32768/54000 (61%)] Loss: -434992.656250\n",
      "Train Epoch: 72 [36864/54000 (68%)] Loss: -416641.812500\n",
      "Train Epoch: 72 [40960/54000 (76%)] Loss: -439768.531250\n",
      "Train Epoch: 72 [45056/54000 (83%)] Loss: -449210.250000\n",
      "Train Epoch: 72 [49152/54000 (91%)] Loss: -478709.000000\n",
      "    epoch          : 72\n",
      "    loss           : -435845.187652439\n",
      "    val_loss       : -450057.59453125\n",
      "Train Epoch: 73 [0/54000 (0%)] Loss: -476481.875000\n",
      "Train Epoch: 73 [4096/54000 (8%)] Loss: -431101.312500\n",
      "Train Epoch: 73 [8192/54000 (15%)] Loss: -450451.218750\n",
      "Train Epoch: 73 [12288/54000 (23%)] Loss: -436971.812500\n",
      "Train Epoch: 73 [16384/54000 (30%)] Loss: -415632.312500\n",
      "Train Epoch: 73 [20480/54000 (38%)] Loss: -479513.500000\n",
      "Train Epoch: 73 [24576/54000 (46%)] Loss: -438242.968750\n",
      "Train Epoch: 73 [28672/54000 (53%)] Loss: -426091.343750\n",
      "Train Epoch: 73 [32768/54000 (61%)] Loss: -424489.875000\n",
      "Train Epoch: 73 [36864/54000 (68%)] Loss: -448876.250000\n",
      "Train Epoch: 73 [40960/54000 (76%)] Loss: -433710.687500\n",
      "Train Epoch: 73 [45056/54000 (83%)] Loss: -440186.875000\n",
      "Train Epoch: 73 [49152/54000 (91%)] Loss: -478988.187500\n",
      "    epoch          : 73\n",
      "    loss           : -436149.81829268293\n",
      "    val_loss       : -449968.91171875\n",
      "Train Epoch: 74 [0/54000 (0%)] Loss: -440531.375000\n",
      "Train Epoch: 74 [4096/54000 (8%)] Loss: -425093.031250\n",
      "Train Epoch: 74 [8192/54000 (15%)] Loss: -439180.437500\n",
      "Train Epoch: 74 [12288/54000 (23%)] Loss: -478843.593750\n",
      "Train Epoch: 74 [16384/54000 (30%)] Loss: -413685.406250\n",
      "Train Epoch: 74 [20480/54000 (38%)] Loss: -431907.437500\n",
      "Train Epoch: 74 [24576/54000 (46%)] Loss: -412923.937500\n",
      "Train Epoch: 74 [28672/54000 (53%)] Loss: -426346.500000\n",
      "Train Epoch: 74 [32768/54000 (61%)] Loss: -442792.937500\n",
      "Train Epoch: 74 [36864/54000 (68%)] Loss: -478586.500000\n",
      "Train Epoch: 74 [40960/54000 (76%)] Loss: -426581.750000\n",
      "Train Epoch: 74 [45056/54000 (83%)] Loss: -449066.562500\n",
      "Train Epoch: 74 [49152/54000 (91%)] Loss: -479480.781250\n",
      "    epoch          : 74\n",
      "    loss           : -436541.42515243904\n",
      "    val_loss       : -450239.7390625\n",
      "Train Epoch: 75 [0/54000 (0%)] Loss: -417981.625000\n",
      "Train Epoch: 75 [4096/54000 (8%)] Loss: -428115.625000\n",
      "Train Epoch: 75 [8192/54000 (15%)] Loss: -431830.656250\n",
      "Train Epoch: 75 [12288/54000 (23%)] Loss: -413952.906250\n",
      "Train Epoch: 75 [16384/54000 (30%)] Loss: -416312.062500\n",
      "Train Epoch: 75 [20480/54000 (38%)] Loss: -443001.531250\n",
      "Train Epoch: 75 [24576/54000 (46%)] Loss: -436628.718750\n",
      "Train Epoch: 75 [28672/54000 (53%)] Loss: -431357.500000\n",
      "Train Epoch: 75 [32768/54000 (61%)] Loss: -426765.625000\n",
      "Train Epoch: 75 [36864/54000 (68%)] Loss: -441742.062500\n",
      "Train Epoch: 75 [40960/54000 (76%)] Loss: -438531.875000\n",
      "Train Epoch: 75 [45056/54000 (83%)] Loss: -450348.312500\n",
      "Train Epoch: 75 [49152/54000 (91%)] Loss: -479681.250000\n",
      "    epoch          : 75\n",
      "    loss           : -436954.8705792683\n",
      "    val_loss       : -450966.634375\n",
      "Train Epoch: 76 [0/54000 (0%)] Loss: -417554.062500\n",
      "Train Epoch: 76 [4096/54000 (8%)] Loss: -428206.656250\n",
      "Train Epoch: 76 [8192/54000 (15%)] Loss: -417513.625000\n",
      "Train Epoch: 76 [12288/54000 (23%)] Loss: -478025.656250\n",
      "Train Epoch: 76 [16384/54000 (30%)] Loss: -417208.593750\n",
      "Train Epoch: 76 [20480/54000 (38%)] Loss: -439838.718750\n",
      "Train Epoch: 76 [24576/54000 (46%)] Loss: -449608.093750\n",
      "Train Epoch: 76 [28672/54000 (53%)] Loss: -424415.062500\n",
      "Train Epoch: 76 [32768/54000 (61%)] Loss: -428559.000000\n",
      "Train Epoch: 76 [36864/54000 (68%)] Loss: -419224.687500\n",
      "Train Epoch: 76 [40960/54000 (76%)] Loss: -436380.187500\n",
      "Train Epoch: 76 [45056/54000 (83%)] Loss: -443747.125000\n",
      "Train Epoch: 76 [49152/54000 (91%)] Loss: -479867.000000\n",
      "    epoch          : 76\n",
      "    loss           : -437168.64695121953\n",
      "    val_loss       : -450587.16484375\n",
      "Train Epoch: 77 [0/54000 (0%)] Loss: -480383.781250\n",
      "Train Epoch: 77 [4096/54000 (8%)] Loss: -416640.156250\n",
      "Train Epoch: 77 [8192/54000 (15%)] Loss: -442490.281250\n",
      "Train Epoch: 77 [12288/54000 (23%)] Loss: -436446.312500\n",
      "Train Epoch: 77 [16384/54000 (30%)] Loss: -417871.875000\n",
      "Train Epoch: 77 [20480/54000 (38%)] Loss: -438725.906250\n",
      "Train Epoch: 77 [24576/54000 (46%)] Loss: -419362.187500\n",
      "Train Epoch: 77 [28672/54000 (53%)] Loss: -426410.250000\n",
      "Train Epoch: 77 [32768/54000 (61%)] Loss: -436074.375000\n",
      "Train Epoch: 77 [36864/54000 (68%)] Loss: -480721.000000\n",
      "Train Epoch: 77 [40960/54000 (76%)] Loss: -452728.718750\n",
      "Train Epoch: 77 [45056/54000 (83%)] Loss: -440718.125000\n",
      "Train Epoch: 77 [49152/54000 (91%)] Loss: -479446.625000\n",
      "    epoch          : 77\n",
      "    loss           : -437614.3132621951\n",
      "    val_loss       : -451095.740625\n",
      "Train Epoch: 78 [0/54000 (0%)] Loss: -480776.062500\n",
      "Train Epoch: 78 [4096/54000 (8%)] Loss: -426887.000000\n",
      "Train Epoch: 78 [8192/54000 (15%)] Loss: -451499.750000\n",
      "Train Epoch: 78 [12288/54000 (23%)] Loss: -427447.937500\n",
      "Train Epoch: 78 [16384/54000 (30%)] Loss: -430470.875000\n",
      "Train Epoch: 78 [20480/54000 (38%)] Loss: -440940.437500\n",
      "Train Epoch: 78 [24576/54000 (46%)] Loss: -451965.062500\n",
      "Train Epoch: 78 [28672/54000 (53%)] Loss: -445164.062500\n",
      "Train Epoch: 78 [32768/54000 (61%)] Loss: -435193.562500\n",
      "Train Epoch: 78 [36864/54000 (68%)] Loss: -451879.062500\n",
      "Train Epoch: 78 [40960/54000 (76%)] Loss: -425380.437500\n",
      "Train Epoch: 78 [45056/54000 (83%)] Loss: -438110.531250\n",
      "Train Epoch: 78 [49152/54000 (91%)] Loss: -480475.000000\n",
      "    epoch          : 78\n",
      "    loss           : -437842.2620426829\n",
      "    val_loss       : -450904.53984375\n",
      "Train Epoch: 79 [0/54000 (0%)] Loss: -481117.718750\n",
      "Train Epoch: 79 [4096/54000 (8%)] Loss: -416429.375000\n",
      "Train Epoch: 79 [8192/54000 (15%)] Loss: -431427.000000\n",
      "Train Epoch: 79 [12288/54000 (23%)] Loss: -437378.375000\n",
      "Train Epoch: 79 [16384/54000 (30%)] Loss: -418875.937500\n",
      "Train Epoch: 79 [20480/54000 (38%)] Loss: -445130.312500\n",
      "Train Epoch: 79 [24576/54000 (46%)] Loss: -429564.250000\n",
      "Train Epoch: 79 [28672/54000 (53%)] Loss: -431893.562500\n",
      "Train Epoch: 79 [32768/54000 (61%)] Loss: -444660.812500\n",
      "Train Epoch: 79 [36864/54000 (68%)] Loss: -417821.218750\n",
      "Train Epoch: 79 [40960/54000 (76%)] Loss: -424919.750000\n",
      "Train Epoch: 79 [45056/54000 (83%)] Loss: -416765.000000\n",
      "Train Epoch: 79 [49152/54000 (91%)] Loss: -481772.906250\n",
      "    epoch          : 79\n",
      "    loss           : -438191.0163109756\n",
      "    val_loss       : -450893.634375\n",
      "Train Epoch: 80 [0/54000 (0%)] Loss: -448633.000000\n",
      "Train Epoch: 80 [4096/54000 (8%)] Loss: -419264.250000\n",
      "Train Epoch: 80 [8192/54000 (15%)] Loss: -452323.375000\n",
      "Train Epoch: 80 [12288/54000 (23%)] Loss: -440572.906250\n",
      "Train Epoch: 80 [16384/54000 (30%)] Loss: -448490.281250\n",
      "Train Epoch: 80 [20480/54000 (38%)] Loss: -444755.531250\n",
      "Train Epoch: 80 [24576/54000 (46%)] Loss: -452148.875000\n",
      "Train Epoch: 80 [28672/54000 (53%)] Loss: -481806.218750\n",
      "Train Epoch: 80 [32768/54000 (61%)] Loss: -433117.000000\n",
      "Train Epoch: 80 [36864/54000 (68%)] Loss: -481660.062500\n",
      "Train Epoch: 80 [40960/54000 (76%)] Loss: -440120.125000\n",
      "Train Epoch: 80 [45056/54000 (83%)] Loss: -451381.625000\n",
      "Train Epoch: 80 [49152/54000 (91%)] Loss: -426193.562500\n",
      "    epoch          : 80\n",
      "    loss           : -438664.2908536585\n",
      "    val_loss       : -451554.81953125\n",
      "Train Epoch: 81 [0/54000 (0%)] Loss: -414753.156250\n",
      "Train Epoch: 81 [4096/54000 (8%)] Loss: -437953.125000\n",
      "Train Epoch: 81 [8192/54000 (15%)] Loss: -420067.625000\n",
      "Train Epoch: 81 [12288/54000 (23%)] Loss: -442999.312500\n",
      "Train Epoch: 81 [16384/54000 (30%)] Loss: -481081.375000\n",
      "Train Epoch: 81 [20480/54000 (38%)] Loss: -445816.500000\n",
      "Train Epoch: 81 [24576/54000 (46%)] Loss: -431732.718750\n",
      "Train Epoch: 81 [28672/54000 (53%)] Loss: -427828.625000\n",
      "Train Epoch: 81 [32768/54000 (61%)] Loss: -449794.000000\n",
      "Train Epoch: 81 [36864/54000 (68%)] Loss: -482109.531250\n",
      "Train Epoch: 81 [40960/54000 (76%)] Loss: -439963.437500\n",
      "Train Epoch: 81 [45056/54000 (83%)] Loss: -453271.875000\n",
      "Train Epoch: 81 [49152/54000 (91%)] Loss: -481668.718750\n",
      "    epoch          : 81\n",
      "    loss           : -438829.0131097561\n",
      "    val_loss       : -451968.3953125\n",
      "Train Epoch: 82 [0/54000 (0%)] Loss: -482429.437500\n",
      "Train Epoch: 82 [4096/54000 (8%)] Loss: -451974.000000\n",
      "Train Epoch: 82 [8192/54000 (15%)] Loss: -432957.437500\n",
      "Train Epoch: 82 [12288/54000 (23%)] Loss: -420135.125000\n",
      "Train Epoch: 82 [16384/54000 (30%)] Loss: -419830.312500\n",
      "Train Epoch: 82 [20480/54000 (38%)] Loss: -447348.187500\n",
      "Train Epoch: 82 [24576/54000 (46%)] Loss: -416587.562500\n",
      "Train Epoch: 82 [28672/54000 (53%)] Loss: -431142.312500\n",
      "Train Epoch: 82 [32768/54000 (61%)] Loss: -482240.093750\n",
      "Train Epoch: 82 [36864/54000 (68%)] Loss: -429340.750000\n",
      "Train Epoch: 82 [40960/54000 (76%)] Loss: -439992.437500\n",
      "Train Epoch: 82 [45056/54000 (83%)] Loss: -452327.343750\n",
      "Train Epoch: 82 [49152/54000 (91%)] Loss: -482773.437500\n",
      "    epoch          : 82\n",
      "    loss           : -439077.4303353659\n",
      "    val_loss       : -451955.21015625\n",
      "Train Epoch: 83 [0/54000 (0%)] Loss: -423336.125000\n",
      "Train Epoch: 83 [4096/54000 (8%)] Loss: -437733.312500\n",
      "Train Epoch: 83 [8192/54000 (15%)] Loss: -420352.500000\n",
      "Train Epoch: 83 [12288/54000 (23%)] Loss: -432729.562500\n",
      "Train Epoch: 83 [16384/54000 (30%)] Loss: -419956.000000\n",
      "Train Epoch: 83 [20480/54000 (38%)] Loss: -446828.000000\n",
      "Train Epoch: 83 [24576/54000 (46%)] Loss: -453166.093750\n",
      "Train Epoch: 83 [28672/54000 (53%)] Loss: -428146.875000\n",
      "Train Epoch: 83 [32768/54000 (61%)] Loss: -444950.781250\n",
      "Train Epoch: 83 [36864/54000 (68%)] Loss: -419858.500000\n",
      "Train Epoch: 83 [40960/54000 (76%)] Loss: -439815.406250\n",
      "Train Epoch: 83 [45056/54000 (83%)] Loss: -420214.843750\n",
      "Train Epoch: 83 [49152/54000 (91%)] Loss: -483329.437500\n",
      "    epoch          : 83\n",
      "    loss           : -439327.0780487805\n",
      "    val_loss       : -452427.96953125\n",
      "Train Epoch: 84 [0/54000 (0%)] Loss: -433750.625000\n",
      "Train Epoch: 84 [4096/54000 (8%)] Loss: -441243.625000\n",
      "Train Epoch: 84 [8192/54000 (15%)] Loss: -416102.281250\n",
      "Train Epoch: 84 [12288/54000 (23%)] Loss: -420730.812500\n",
      "Train Epoch: 84 [16384/54000 (30%)] Loss: -431985.875000\n",
      "Train Epoch: 84 [20480/54000 (38%)] Loss: -437514.875000\n",
      "Train Epoch: 84 [24576/54000 (46%)] Loss: -440560.812500\n",
      "Train Epoch: 84 [28672/54000 (53%)] Loss: -445556.656250\n",
      "Train Epoch: 84 [32768/54000 (61%)] Loss: -442155.781250\n",
      "Train Epoch: 84 [36864/54000 (68%)] Loss: -451774.750000\n",
      "Train Epoch: 84 [40960/54000 (76%)] Loss: -447482.187500\n",
      "Train Epoch: 84 [45056/54000 (83%)] Loss: -422358.156250\n",
      "Train Epoch: 84 [49152/54000 (91%)] Loss: -428379.875000\n",
      "    epoch          : 84\n",
      "    loss           : -439826.35\n",
      "    val_loss       : -452245.03359375\n",
      "Train Epoch: 85 [0/54000 (0%)] Loss: -482912.187500\n",
      "Train Epoch: 85 [4096/54000 (8%)] Loss: -440016.218750\n",
      "Train Epoch: 85 [8192/54000 (15%)] Loss: -433082.406250\n",
      "Train Epoch: 85 [12288/54000 (23%)] Loss: -421775.875000\n",
      "Train Epoch: 85 [16384/54000 (30%)] Loss: -440467.062500\n",
      "Train Epoch: 85 [20480/54000 (38%)] Loss: -430483.750000\n",
      "Train Epoch: 85 [24576/54000 (46%)] Loss: -442240.062500\n",
      "Train Epoch: 85 [28672/54000 (53%)] Loss: -443887.937500\n",
      "Train Epoch: 85 [32768/54000 (61%)] Loss: -428143.062500\n",
      "Train Epoch: 85 [36864/54000 (68%)] Loss: -419232.250000\n",
      "Train Epoch: 85 [40960/54000 (76%)] Loss: -437958.093750\n",
      "Train Epoch: 85 [45056/54000 (83%)] Loss: -440227.250000\n",
      "Train Epoch: 85 [49152/54000 (91%)] Loss: -484031.750000\n",
      "    epoch          : 85\n",
      "    loss           : -439994.6629573171\n",
      "    val_loss       : -452535.30234375\n",
      "Train Epoch: 86 [0/54000 (0%)] Loss: -432720.468750\n",
      "Train Epoch: 86 [4096/54000 (8%)] Loss: -420034.312500\n",
      "Train Epoch: 86 [8192/54000 (15%)] Loss: -434552.875000\n",
      "Train Epoch: 86 [12288/54000 (23%)] Loss: -444389.968750\n",
      "Train Epoch: 86 [16384/54000 (30%)] Loss: -420494.625000\n",
      "Train Epoch: 86 [20480/54000 (38%)] Loss: -445819.937500\n",
      "Train Epoch: 86 [24576/54000 (46%)] Loss: -439032.718750\n",
      "Train Epoch: 86 [28672/54000 (53%)] Loss: -483441.375000\n",
      "Train Epoch: 86 [32768/54000 (61%)] Loss: -438338.312500\n",
      "Train Epoch: 86 [36864/54000 (68%)] Loss: -416772.625000\n",
      "Train Epoch: 86 [40960/54000 (76%)] Loss: -428252.156250\n",
      "Train Epoch: 86 [45056/54000 (83%)] Loss: -433478.562500\n",
      "Train Epoch: 86 [49152/54000 (91%)] Loss: -481533.687500\n",
      "    epoch          : 86\n",
      "    loss           : -440127.006097561\n",
      "    val_loss       : -452584.13203125\n",
      "Train Epoch: 87 [0/54000 (0%)] Loss: -484085.500000\n",
      "Train Epoch: 87 [4096/54000 (8%)] Loss: -432742.750000\n",
      "Train Epoch: 87 [8192/54000 (15%)] Loss: -420779.125000\n",
      "Train Epoch: 87 [12288/54000 (23%)] Loss: -428246.968750\n",
      "Train Epoch: 87 [16384/54000 (30%)] Loss: -433389.250000\n",
      "Train Epoch: 87 [20480/54000 (38%)] Loss: -444675.437500\n",
      "Train Epoch: 87 [24576/54000 (46%)] Loss: -454829.375000\n",
      "Train Epoch: 87 [28672/54000 (53%)] Loss: -483138.562500\n",
      "Train Epoch: 87 [32768/54000 (61%)] Loss: -428706.187500\n",
      "Train Epoch: 87 [36864/54000 (68%)] Loss: -420495.250000\n",
      "Train Epoch: 87 [40960/54000 (76%)] Loss: -431554.312500\n",
      "Train Epoch: 87 [45056/54000 (83%)] Loss: -444769.250000\n",
      "Train Epoch: 87 [49152/54000 (91%)] Loss: -484757.250000\n",
      "    epoch          : 87\n",
      "    loss           : -440450.9800304878\n",
      "    val_loss       : -452823.43515625\n",
      "Train Epoch: 88 [0/54000 (0%)] Loss: -484306.531250\n",
      "Train Epoch: 88 [4096/54000 (8%)] Loss: -418955.250000\n",
      "Train Epoch: 88 [8192/54000 (15%)] Loss: -445852.125000\n",
      "Train Epoch: 88 [12288/54000 (23%)] Loss: -432660.875000\n",
      "Train Epoch: 88 [16384/54000 (30%)] Loss: -433683.437500\n",
      "Train Epoch: 88 [20480/54000 (38%)] Loss: -448719.562500\n",
      "Train Epoch: 88 [24576/54000 (46%)] Loss: -409475.000000\n",
      "Train Epoch: 88 [28672/54000 (53%)] Loss: -453715.218750\n",
      "Train Epoch: 88 [32768/54000 (61%)] Loss: -484421.500000\n",
      "Train Epoch: 88 [36864/54000 (68%)] Loss: -483927.468750\n",
      "Train Epoch: 88 [40960/54000 (76%)] Loss: -430721.687500\n",
      "Train Epoch: 88 [45056/54000 (83%)] Loss: -422085.500000\n",
      "Train Epoch: 88 [49152/54000 (91%)] Loss: -435947.312500\n",
      "    epoch          : 88\n",
      "    loss           : -440615.6327743902\n",
      "    val_loss       : -453199.8859375\n",
      "Train Epoch: 89 [0/54000 (0%)] Loss: -484453.093750\n",
      "Train Epoch: 89 [4096/54000 (8%)] Loss: -423058.437500\n",
      "Train Epoch: 89 [8192/54000 (15%)] Loss: -433919.468750\n",
      "Train Epoch: 89 [12288/54000 (23%)] Loss: -440297.593750\n",
      "Train Epoch: 89 [16384/54000 (30%)] Loss: -454171.781250\n",
      "Train Epoch: 89 [20480/54000 (38%)] Loss: -445636.937500\n",
      "Train Epoch: 89 [24576/54000 (46%)] Loss: -423829.875000\n",
      "Train Epoch: 89 [28672/54000 (53%)] Loss: -433897.781250\n",
      "Train Epoch: 89 [32768/54000 (61%)] Loss: -446515.781250\n",
      "Train Epoch: 89 [36864/54000 (68%)] Loss: -453184.375000\n",
      "Train Epoch: 89 [40960/54000 (76%)] Loss: -427475.968750\n",
      "Train Epoch: 89 [45056/54000 (83%)] Loss: -445859.250000\n",
      "Train Epoch: 89 [49152/54000 (91%)] Loss: -434657.562500\n",
      "    epoch          : 89\n",
      "    loss           : -441053.31143292686\n",
      "    val_loss       : -453322.4\n",
      "Train Epoch: 90 [0/54000 (0%)] Loss: -424627.531250\n",
      "Train Epoch: 90 [4096/54000 (8%)] Loss: -432363.375000\n",
      "Train Epoch: 90 [8192/54000 (15%)] Loss: -447968.343750\n",
      "Train Epoch: 90 [12288/54000 (23%)] Loss: -433910.875000\n",
      "Train Epoch: 90 [16384/54000 (30%)] Loss: -432671.031250\n",
      "Train Epoch: 90 [20480/54000 (38%)] Loss: -447568.968750\n",
      "Train Epoch: 90 [24576/54000 (46%)] Loss: -452649.031250\n",
      "Train Epoch: 90 [28672/54000 (53%)] Loss: -433613.062500\n",
      "Train Epoch: 90 [32768/54000 (61%)] Loss: -448191.000000\n",
      "Train Epoch: 90 [36864/54000 (68%)] Loss: -485035.312500\n",
      "Train Epoch: 90 [40960/54000 (76%)] Loss: -440542.093750\n",
      "Train Epoch: 90 [45056/54000 (83%)] Loss: -453122.187500\n",
      "Train Epoch: 90 [49152/54000 (91%)] Loss: -444016.687500\n",
      "    epoch          : 90\n",
      "    loss           : -441208.8888719512\n",
      "    val_loss       : -453271.040625\n",
      "Train Epoch: 91 [0/54000 (0%)] Loss: -485635.281250\n",
      "Train Epoch: 91 [4096/54000 (8%)] Loss: -451289.437500\n",
      "Train Epoch: 91 [8192/54000 (15%)] Loss: -435893.875000\n",
      "Train Epoch: 91 [12288/54000 (23%)] Loss: -485198.062500\n",
      "Train Epoch: 91 [16384/54000 (30%)] Loss: -427746.625000\n",
      "Train Epoch: 91 [20480/54000 (38%)] Loss: -446791.406250\n",
      "Train Epoch: 91 [24576/54000 (46%)] Loss: -452435.625000\n",
      "Train Epoch: 91 [28672/54000 (53%)] Loss: -433921.531250\n",
      "Train Epoch: 91 [32768/54000 (61%)] Loss: -428509.218750\n",
      "Train Epoch: 91 [36864/54000 (68%)] Loss: -486159.656250\n",
      "Train Epoch: 91 [40960/54000 (76%)] Loss: -434291.593750\n",
      "Train Epoch: 91 [45056/54000 (83%)] Loss: -454140.781250\n",
      "Train Epoch: 91 [49152/54000 (91%)] Loss: -486177.250000\n",
      "    epoch          : 91\n",
      "    loss           : -441466.0132621951\n",
      "    val_loss       : -453388.68046875\n",
      "Train Epoch: 92 [0/54000 (0%)] Loss: -485954.093750\n",
      "Train Epoch: 92 [4096/54000 (8%)] Loss: -414195.812500\n",
      "Train Epoch: 92 [8192/54000 (15%)] Loss: -433145.156250\n",
      "Train Epoch: 92 [12288/54000 (23%)] Loss: -430718.625000\n",
      "Train Epoch: 92 [16384/54000 (30%)] Loss: -421598.187500\n",
      "Train Epoch: 92 [20480/54000 (38%)] Loss: -447234.187500\n",
      "Train Epoch: 92 [24576/54000 (46%)] Loss: -422733.750000\n",
      "Train Epoch: 92 [28672/54000 (53%)] Loss: -485865.281250\n",
      "Train Epoch: 92 [32768/54000 (61%)] Loss: -483751.843750\n",
      "Train Epoch: 92 [36864/54000 (68%)] Loss: -454157.500000\n",
      "Train Epoch: 92 [40960/54000 (76%)] Loss: -431328.156250\n",
      "Train Epoch: 92 [45056/54000 (83%)] Loss: -425997.000000\n",
      "Train Epoch: 92 [49152/54000 (91%)] Loss: -442927.812500\n",
      "    epoch          : 92\n",
      "    loss           : -441629.9448170732\n",
      "    val_loss       : -453722.34140625\n",
      "Train Epoch: 93 [0/54000 (0%)] Loss: -453124.906250\n",
      "Train Epoch: 93 [4096/54000 (8%)] Loss: -432950.375000\n",
      "Train Epoch: 93 [8192/54000 (15%)] Loss: -436325.437500\n",
      "Train Epoch: 93 [12288/54000 (23%)] Loss: -423713.062500\n",
      "Train Epoch: 93 [16384/54000 (30%)] Loss: -433526.562500\n",
      "Train Epoch: 93 [20480/54000 (38%)] Loss: -447708.625000\n",
      "Train Epoch: 93 [24576/54000 (46%)] Loss: -440403.343750\n",
      "Train Epoch: 93 [28672/54000 (53%)] Loss: -482937.156250\n",
      "Train Epoch: 93 [32768/54000 (61%)] Loss: -447553.843750\n",
      "Train Epoch: 93 [36864/54000 (68%)] Loss: -426409.875000\n",
      "Train Epoch: 93 [40960/54000 (76%)] Loss: -434117.937500\n",
      "Train Epoch: 93 [45056/54000 (83%)] Loss: -454486.718750\n",
      "Train Epoch: 93 [49152/54000 (91%)] Loss: -485575.812500\n",
      "    epoch          : 93\n",
      "    loss           : -442056.9746951219\n",
      "    val_loss       : -453639.30859375\n",
      "Train Epoch: 94 [0/54000 (0%)] Loss: -486245.218750\n",
      "Train Epoch: 94 [4096/54000 (8%)] Loss: -434030.187500\n",
      "Train Epoch: 94 [8192/54000 (15%)] Loss: -435516.125000\n",
      "Train Epoch: 94 [12288/54000 (23%)] Loss: -424837.250000\n",
      "Train Epoch: 94 [16384/54000 (30%)] Loss: -451772.343750\n",
      "Train Epoch: 94 [20480/54000 (38%)] Loss: -447236.468750\n",
      "Train Epoch: 94 [24576/54000 (46%)] Loss: -432492.812500\n",
      "Train Epoch: 94 [28672/54000 (53%)] Loss: -438089.312500\n",
      "Train Epoch: 94 [32768/54000 (61%)] Loss: -422716.500000\n",
      "Train Epoch: 94 [36864/54000 (68%)] Loss: -432427.218750\n",
      "Train Epoch: 94 [40960/54000 (76%)] Loss: -442590.000000\n",
      "Train Epoch: 94 [45056/54000 (83%)] Loss: -446996.937500\n",
      "Train Epoch: 94 [49152/54000 (91%)] Loss: -440395.437500\n",
      "    epoch          : 94\n",
      "    loss           : -442047.7109756098\n",
      "    val_loss       : -454211.61640625\n",
      "Train Epoch: 95 [0/54000 (0%)] Loss: -431032.250000\n",
      "Train Epoch: 95 [4096/54000 (8%)] Loss: -430378.781250\n",
      "Train Epoch: 95 [8192/54000 (15%)] Loss: -448986.718750\n",
      "Train Epoch: 95 [12288/54000 (23%)] Loss: -443787.312500\n",
      "Train Epoch: 95 [16384/54000 (30%)] Loss: -423813.750000\n",
      "Train Epoch: 95 [20480/54000 (38%)] Loss: -439884.031250\n",
      "Train Epoch: 95 [24576/54000 (46%)] Loss: -431206.750000\n",
      "Train Epoch: 95 [28672/54000 (53%)] Loss: -440912.875000\n",
      "Train Epoch: 95 [32768/54000 (61%)] Loss: -445622.562500\n",
      "Train Epoch: 95 [36864/54000 (68%)] Loss: -454493.375000\n",
      "Train Epoch: 95 [40960/54000 (76%)] Loss: -445017.187500\n",
      "Train Epoch: 95 [45056/54000 (83%)] Loss: -425952.687500\n",
      "Train Epoch: 95 [49152/54000 (91%)] Loss: -486192.468750\n",
      "    epoch          : 95\n",
      "    loss           : -442364.5233231707\n",
      "    val_loss       : -453542.61484375\n",
      "Train Epoch: 96 [0/54000 (0%)] Loss: -441035.500000\n",
      "Train Epoch: 96 [4096/54000 (8%)] Loss: -431028.375000\n",
      "Train Epoch: 96 [8192/54000 (15%)] Loss: -447686.625000\n",
      "Train Epoch: 96 [12288/54000 (23%)] Loss: -429023.375000\n",
      "Train Epoch: 96 [16384/54000 (30%)] Loss: -426964.968750\n",
      "Train Epoch: 96 [20480/54000 (38%)] Loss: -447330.375000\n",
      "Train Epoch: 96 [24576/54000 (46%)] Loss: -433715.093750\n",
      "Train Epoch: 96 [28672/54000 (53%)] Loss: -433923.687500\n",
      "Train Epoch: 96 [32768/54000 (61%)] Loss: -443051.031250\n",
      "Train Epoch: 96 [36864/54000 (68%)] Loss: -422488.156250\n",
      "Train Epoch: 96 [40960/54000 (76%)] Loss: -440704.125000\n",
      "Train Epoch: 96 [45056/54000 (83%)] Loss: -423595.187500\n",
      "Train Epoch: 96 [49152/54000 (91%)] Loss: -486424.781250\n",
      "    epoch          : 96\n",
      "    loss           : -442589.3214939024\n",
      "    val_loss       : -453821.64765625\n",
      "Train Epoch: 97 [0/54000 (0%)] Loss: -488253.687500\n",
      "Train Epoch: 97 [4096/54000 (8%)] Loss: -441912.500000\n",
      "Train Epoch: 97 [8192/54000 (15%)] Loss: -447630.343750\n",
      "Train Epoch: 97 [12288/54000 (23%)] Loss: -455288.000000\n",
      "Train Epoch: 97 [16384/54000 (30%)] Loss: -429534.562500\n",
      "Train Epoch: 97 [20480/54000 (38%)] Loss: -447195.656250\n",
      "Train Epoch: 97 [24576/54000 (46%)] Loss: -432561.875000\n",
      "Train Epoch: 97 [28672/54000 (53%)] Loss: -486226.781250\n",
      "Train Epoch: 97 [32768/54000 (61%)] Loss: -435813.875000\n",
      "Train Epoch: 97 [36864/54000 (68%)] Loss: -454477.937500\n",
      "Train Epoch: 97 [40960/54000 (76%)] Loss: -425570.562500\n",
      "Train Epoch: 97 [45056/54000 (83%)] Loss: -436143.125000\n",
      "Train Epoch: 97 [49152/54000 (91%)] Loss: -487730.156250\n",
      "    epoch          : 97\n",
      "    loss           : -442869.2050304878\n",
      "    val_loss       : -453768.34140625\n",
      "Train Epoch: 98 [0/54000 (0%)] Loss: -434472.437500\n",
      "Train Epoch: 98 [4096/54000 (8%)] Loss: -456568.281250\n",
      "Train Epoch: 98 [8192/54000 (15%)] Loss: -456021.468750\n",
      "Train Epoch: 98 [12288/54000 (23%)] Loss: -486036.937500\n",
      "Train Epoch: 98 [16384/54000 (30%)] Loss: -443438.593750\n",
      "Train Epoch: 98 [20480/54000 (38%)] Loss: -447065.312500\n",
      "Train Epoch: 98 [24576/54000 (46%)] Loss: -425697.250000\n",
      "Train Epoch: 98 [28672/54000 (53%)] Loss: -433127.375000\n",
      "Train Epoch: 98 [32768/54000 (61%)] Loss: -433341.906250\n",
      "Train Epoch: 98 [36864/54000 (68%)] Loss: -424734.125000\n",
      "Train Epoch: 98 [40960/54000 (76%)] Loss: -430713.562500\n",
      "Train Epoch: 98 [45056/54000 (83%)] Loss: -450213.875000\n",
      "Train Epoch: 98 [49152/54000 (91%)] Loss: -487330.593750\n",
      "    epoch          : 98\n",
      "    loss           : -443072.5082317073\n",
      "    val_loss       : -454222.8703125\n",
      "Train Epoch: 99 [0/54000 (0%)] Loss: -487582.312500\n",
      "Train Epoch: 99 [4096/54000 (8%)] Loss: -441624.656250\n",
      "Train Epoch: 99 [8192/54000 (15%)] Loss: -448205.218750\n",
      "Train Epoch: 99 [12288/54000 (23%)] Loss: -454122.468750\n",
      "Train Epoch: 99 [16384/54000 (30%)] Loss: -455765.000000\n",
      "Train Epoch: 99 [20480/54000 (38%)] Loss: -450409.093750\n",
      "Train Epoch: 99 [24576/54000 (46%)] Loss: -435237.937500\n",
      "Train Epoch: 99 [28672/54000 (53%)] Loss: -433723.843750\n",
      "Train Epoch: 99 [32768/54000 (61%)] Loss: -435054.125000\n",
      "Train Epoch: 99 [36864/54000 (68%)] Loss: -424284.250000\n",
      "Train Epoch: 99 [40960/54000 (76%)] Loss: -428860.750000\n",
      "Train Epoch: 99 [45056/54000 (83%)] Loss: -447720.312500\n",
      "Train Epoch: 99 [49152/54000 (91%)] Loss: -430212.437500\n",
      "    epoch          : 99\n",
      "    loss           : -443323.74649390246\n",
      "    val_loss       : -453852.34453125\n",
      "Train Epoch: 100 [0/54000 (0%)] Loss: -488304.562500\n",
      "Train Epoch: 100 [4096/54000 (8%)] Loss: -429630.000000\n",
      "Train Epoch: 100 [8192/54000 (15%)] Loss: -425592.718750\n",
      "Train Epoch: 100 [12288/54000 (23%)] Loss: -433713.937500\n",
      "Train Epoch: 100 [16384/54000 (30%)] Loss: -453780.031250\n",
      "Train Epoch: 100 [20480/54000 (38%)] Loss: -436230.687500\n",
      "Train Epoch: 100 [24576/54000 (46%)] Loss: -428997.093750\n",
      "Train Epoch: 100 [28672/54000 (53%)] Loss: -437927.812500\n",
      "Train Epoch: 100 [32768/54000 (61%)] Loss: -442674.937500\n",
      "Train Epoch: 100 [36864/54000 (68%)] Loss: -488565.000000\n",
      "Train Epoch: 100 [40960/54000 (76%)] Loss: -434151.406250\n",
      "Train Epoch: 100 [45056/54000 (83%)] Loss: -443731.968750\n",
      "Train Epoch: 100 [49152/54000 (91%)] Loss: -488664.062500\n",
      "    epoch          : 100\n",
      "    loss           : -443410.0975609756\n",
      "    val_loss       : -454346.3546875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0510_004522/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [0/54000 (0%)] Loss: -488553.468750\n",
      "Train Epoch: 101 [4096/54000 (8%)] Loss: -431478.250000\n",
      "Train Epoch: 101 [8192/54000 (15%)] Loss: -446453.375000\n",
      "Train Epoch: 101 [12288/54000 (23%)] Loss: -438671.843750\n",
      "Train Epoch: 101 [16384/54000 (30%)] Loss: -455259.562500\n",
      "Train Epoch: 101 [20480/54000 (38%)] Loss: -448835.375000\n",
      "Train Epoch: 101 [24576/54000 (46%)] Loss: -445784.687500\n",
      "Train Epoch: 101 [28672/54000 (53%)] Loss: -437741.437500\n",
      "Train Epoch: 101 [32768/54000 (61%)] Loss: -440580.156250\n",
      "Train Epoch: 101 [36864/54000 (68%)] Loss: -489409.000000\n",
      "Train Epoch: 101 [40960/54000 (76%)] Loss: -438588.000000\n",
      "Train Epoch: 101 [45056/54000 (83%)] Loss: -456950.812500\n",
      "Train Epoch: 101 [49152/54000 (91%)] Loss: -433994.750000\n",
      "    epoch          : 101\n",
      "    loss           : -443787.11387195124\n",
      "    val_loss       : -453964.365625\n",
      "Train Epoch: 102 [0/54000 (0%)] Loss: -489108.781250\n",
      "Train Epoch: 102 [4096/54000 (8%)] Loss: -424208.062500\n",
      "Train Epoch: 102 [8192/54000 (15%)] Loss: -449676.000000\n",
      "Train Epoch: 102 [12288/54000 (23%)] Loss: -456595.843750\n",
      "Train Epoch: 102 [16384/54000 (30%)] Loss: -446671.812500\n",
      "Train Epoch: 102 [20480/54000 (38%)] Loss: -436994.437500\n",
      "Train Epoch: 102 [24576/54000 (46%)] Loss: -433808.687500\n",
      "Train Epoch: 102 [28672/54000 (53%)] Loss: -438863.875000\n",
      "Train Epoch: 102 [32768/54000 (61%)] Loss: -455367.500000\n",
      "Train Epoch: 102 [36864/54000 (68%)] Loss: -489444.062500\n",
      "Train Epoch: 102 [40960/54000 (76%)] Loss: -443827.906250\n",
      "Train Epoch: 102 [45056/54000 (83%)] Loss: -454406.875000\n",
      "Train Epoch: 102 [49152/54000 (91%)] Loss: -489327.781250\n",
      "    epoch          : 102\n",
      "    loss           : -443910.2461890244\n",
      "    val_loss       : -454468.17109375\n",
      "Train Epoch: 103 [0/54000 (0%)] Loss: -446084.156250\n",
      "Train Epoch: 103 [4096/54000 (8%)] Loss: -425573.500000\n",
      "Train Epoch: 103 [8192/54000 (15%)] Loss: -425438.718750\n",
      "Train Epoch: 103 [12288/54000 (23%)] Loss: -436576.281250\n",
      "Train Epoch: 103 [16384/54000 (30%)] Loss: -449865.000000\n",
      "Train Epoch: 103 [20480/54000 (38%)] Loss: -433979.125000\n",
      "Train Epoch: 103 [24576/54000 (46%)] Loss: -435574.750000\n",
      "Train Epoch: 103 [28672/54000 (53%)] Loss: -488903.875000\n",
      "Train Epoch: 103 [32768/54000 (61%)] Loss: -446947.843750\n",
      "Train Epoch: 103 [36864/54000 (68%)] Loss: -428612.562500\n",
      "Train Epoch: 103 [40960/54000 (76%)] Loss: -448685.625000\n",
      "Train Epoch: 103 [45056/54000 (83%)] Loss: -449277.343750\n",
      "Train Epoch: 103 [49152/54000 (91%)] Loss: -489743.906250\n",
      "    epoch          : 103\n",
      "    loss           : -444116.5413109756\n",
      "    val_loss       : -454599.08515625\n",
      "Train Epoch: 104 [0/54000 (0%)] Loss: -488563.625000\n",
      "Train Epoch: 104 [4096/54000 (8%)] Loss: -432178.875000\n",
      "Train Epoch: 104 [8192/54000 (15%)] Loss: -437057.812500\n",
      "Train Epoch: 104 [12288/54000 (23%)] Loss: -436142.125000\n",
      "Train Epoch: 104 [16384/54000 (30%)] Loss: -446031.968750\n",
      "Train Epoch: 104 [20480/54000 (38%)] Loss: -438893.312500\n",
      "Train Epoch: 104 [24576/54000 (46%)] Loss: -424811.687500\n",
      "Train Epoch: 104 [28672/54000 (53%)] Loss: -489646.156250\n",
      "Train Epoch: 104 [32768/54000 (61%)] Loss: -438081.937500\n",
      "Train Epoch: 104 [36864/54000 (68%)] Loss: -489469.718750\n",
      "Train Epoch: 104 [40960/54000 (76%)] Loss: -425332.687500\n",
      "Train Epoch: 104 [45056/54000 (83%)] Loss: -437542.250000\n",
      "Train Epoch: 104 [49152/54000 (91%)] Loss: -488708.812500\n",
      "    epoch          : 104\n",
      "    loss           : -444471.18582317076\n",
      "    val_loss       : -454747.6671875\n",
      "Train Epoch: 105 [0/54000 (0%)] Loss: -490180.812500\n",
      "Train Epoch: 105 [4096/54000 (8%)] Loss: -437915.875000\n",
      "Train Epoch: 105 [8192/54000 (15%)] Loss: -431019.406250\n",
      "Train Epoch: 105 [12288/54000 (23%)] Loss: -427553.562500\n",
      "Train Epoch: 105 [16384/54000 (30%)] Loss: -447178.250000\n",
      "Train Epoch: 105 [20480/54000 (38%)] Loss: -450778.187500\n",
      "Train Epoch: 105 [24576/54000 (46%)] Loss: -444611.250000\n",
      "Train Epoch: 105 [28672/54000 (53%)] Loss: -489651.781250\n",
      "Train Epoch: 105 [32768/54000 (61%)] Loss: -434912.500000\n",
      "Train Epoch: 105 [36864/54000 (68%)] Loss: -424266.437500\n",
      "Train Epoch: 105 [40960/54000 (76%)] Loss: -432182.218750\n",
      "Train Epoch: 105 [45056/54000 (83%)] Loss: -449255.906250\n",
      "Train Epoch: 105 [49152/54000 (91%)] Loss: -442158.093750\n",
      "    epoch          : 105\n",
      "    loss           : -444629.1695121951\n",
      "    val_loss       : -454899.1234375\n",
      "Train Epoch: 106 [0/54000 (0%)] Loss: -489785.000000\n",
      "Train Epoch: 106 [4096/54000 (8%)] Loss: -426162.312500\n",
      "Train Epoch: 106 [8192/54000 (15%)] Loss: -451980.375000\n",
      "Train Epoch: 106 [12288/54000 (23%)] Loss: -428521.687500\n",
      "Train Epoch: 106 [16384/54000 (30%)] Loss: -427012.312500\n",
      "Train Epoch: 106 [20480/54000 (38%)] Loss: -449674.937500\n",
      "Train Epoch: 106 [24576/54000 (46%)] Loss: -425277.625000\n",
      "Train Epoch: 106 [28672/54000 (53%)] Loss: -449123.875000\n",
      "Train Epoch: 106 [32768/54000 (61%)] Loss: -457465.812500\n",
      "Train Epoch: 106 [36864/54000 (68%)] Loss: -428637.906250\n",
      "Train Epoch: 106 [40960/54000 (76%)] Loss: -453972.437500\n",
      "Train Epoch: 106 [45056/54000 (83%)] Loss: -436837.968750\n",
      "Train Epoch: 106 [49152/54000 (91%)] Loss: -489736.093750\n",
      "    epoch          : 106\n",
      "    loss           : -444704.40716463415\n",
      "    val_loss       : -454836.66953125\n",
      "Train Epoch: 107 [0/54000 (0%)] Loss: -490497.906250\n",
      "Train Epoch: 107 [4096/54000 (8%)] Loss: -436002.593750\n",
      "Train Epoch: 107 [8192/54000 (15%)] Loss: -429679.906250\n",
      "Train Epoch: 107 [12288/54000 (23%)] Loss: -442918.031250\n",
      "Train Epoch: 107 [16384/54000 (30%)] Loss: -428247.843750\n",
      "Train Epoch: 107 [20480/54000 (38%)] Loss: -450862.093750\n",
      "Train Epoch: 107 [24576/54000 (46%)] Loss: -433275.750000\n",
      "Train Epoch: 107 [28672/54000 (53%)] Loss: -489634.781250\n",
      "Train Epoch: 107 [32768/54000 (61%)] Loss: -432209.500000\n",
      "Train Epoch: 107 [36864/54000 (68%)] Loss: -429132.062500\n",
      "Train Epoch: 107 [40960/54000 (76%)] Loss: -429487.312500\n",
      "Train Epoch: 107 [45056/54000 (83%)] Loss: -451222.437500\n",
      "Train Epoch: 107 [49152/54000 (91%)] Loss: -488975.250000\n",
      "    epoch          : 107\n",
      "    loss           : -445036.8280487805\n",
      "    val_loss       : -455493.5734375\n",
      "Train Epoch: 108 [0/54000 (0%)] Loss: -490704.375000\n",
      "Train Epoch: 108 [4096/54000 (8%)] Loss: -433418.968750\n",
      "Train Epoch: 108 [8192/54000 (15%)] Loss: -433228.937500\n",
      "Train Epoch: 108 [12288/54000 (23%)] Loss: -434422.250000\n",
      "Train Epoch: 108 [16384/54000 (30%)] Loss: -456075.750000\n",
      "Train Epoch: 108 [20480/54000 (38%)] Loss: -436181.562500\n",
      "Train Epoch: 108 [24576/54000 (46%)] Loss: -427832.437500\n",
      "Train Epoch: 108 [28672/54000 (53%)] Loss: -488819.531250\n",
      "Train Epoch: 108 [32768/54000 (61%)] Loss: -438003.625000\n",
      "Train Epoch: 108 [36864/54000 (68%)] Loss: -427818.437500\n",
      "Train Epoch: 108 [40960/54000 (76%)] Loss: -430833.000000\n",
      "Train Epoch: 108 [45056/54000 (83%)] Loss: -425749.000000\n",
      "Train Epoch: 108 [49152/54000 (91%)] Loss: -488995.687500\n",
      "    epoch          : 108\n",
      "    loss           : -445089.8335365854\n",
      "    val_loss       : -455439.17109375\n",
      "Train Epoch: 109 [0/54000 (0%)] Loss: -432685.687500\n",
      "Train Epoch: 109 [4096/54000 (8%)] Loss: -437321.718750\n",
      "Train Epoch: 109 [8192/54000 (15%)] Loss: -449881.375000\n",
      "Train Epoch: 109 [12288/54000 (23%)] Loss: -442758.812500\n",
      "Train Epoch: 109 [16384/54000 (30%)] Loss: -427069.750000\n",
      "Train Epoch: 109 [20480/54000 (38%)] Loss: -452582.750000\n",
      "Train Epoch: 109 [24576/54000 (46%)] Loss: -432699.750000\n",
      "Train Epoch: 109 [28672/54000 (53%)] Loss: -425946.375000\n",
      "Train Epoch: 109 [32768/54000 (61%)] Loss: -455606.531250\n",
      "Train Epoch: 109 [36864/54000 (68%)] Loss: -455406.687500\n",
      "Train Epoch: 109 [40960/54000 (76%)] Loss: -433019.312500\n",
      "Train Epoch: 109 [45056/54000 (83%)] Loss: -455940.687500\n",
      "Train Epoch: 109 [49152/54000 (91%)] Loss: -490071.781250\n",
      "    epoch          : 109\n",
      "    loss           : -445250.60548780486\n",
      "    val_loss       : -455758.75234375\n",
      "Train Epoch: 110 [0/54000 (0%)] Loss: -451657.500000\n",
      "Train Epoch: 110 [4096/54000 (8%)] Loss: -439590.687500\n",
      "Train Epoch: 110 [8192/54000 (15%)] Loss: -456631.187500\n",
      "Train Epoch: 110 [12288/54000 (23%)] Loss: -436882.750000\n",
      "Train Epoch: 110 [16384/54000 (30%)] Loss: -427022.562500\n",
      "Train Epoch: 110 [20480/54000 (38%)] Loss: -450762.343750\n",
      "Train Epoch: 110 [24576/54000 (46%)] Loss: -448430.218750\n",
      "Train Epoch: 110 [28672/54000 (53%)] Loss: -489882.812500\n",
      "Train Epoch: 110 [32768/54000 (61%)] Loss: -428113.625000\n",
      "Train Epoch: 110 [36864/54000 (68%)] Loss: -429259.750000\n",
      "Train Epoch: 110 [40960/54000 (76%)] Loss: -434550.687500\n",
      "Train Epoch: 110 [45056/54000 (83%)] Loss: -424689.375000\n",
      "Train Epoch: 110 [49152/54000 (91%)] Loss: -489631.562500\n",
      "    epoch          : 110\n",
      "    loss           : -445673.6820121951\n",
      "    val_loss       : -455236.00625\n",
      "Train Epoch: 111 [0/54000 (0%)] Loss: -491213.281250\n",
      "Train Epoch: 111 [4096/54000 (8%)] Loss: -430856.031250\n",
      "Train Epoch: 111 [8192/54000 (15%)] Loss: -436920.937500\n",
      "Train Epoch: 111 [12288/54000 (23%)] Loss: -439641.812500\n",
      "Train Epoch: 111 [16384/54000 (30%)] Loss: -457912.000000\n",
      "Train Epoch: 111 [20480/54000 (38%)] Loss: -451687.156250\n",
      "Train Epoch: 111 [24576/54000 (46%)] Loss: -428221.375000\n",
      "Train Epoch: 111 [28672/54000 (53%)] Loss: -439474.687500\n",
      "Train Epoch: 111 [32768/54000 (61%)] Loss: -490333.812500\n",
      "Train Epoch: 111 [36864/54000 (68%)] Loss: -425758.937500\n",
      "Train Epoch: 111 [40960/54000 (76%)] Loss: -444847.218750\n",
      "Train Epoch: 111 [45056/54000 (83%)] Loss: -452154.781250\n",
      "Train Epoch: 111 [49152/54000 (91%)] Loss: -490950.593750\n",
      "    epoch          : 111\n",
      "    loss           : -445894.55030487804\n",
      "    val_loss       : -455998.06953125\n",
      "Train Epoch: 112 [0/54000 (0%)] Loss: -490430.687500\n",
      "Train Epoch: 112 [4096/54000 (8%)] Loss: -432582.843750\n",
      "Train Epoch: 112 [8192/54000 (15%)] Loss: -441710.250000\n",
      "Train Epoch: 112 [12288/54000 (23%)] Loss: -491183.625000\n",
      "Train Epoch: 112 [16384/54000 (30%)] Loss: -420816.250000\n",
      "Train Epoch: 112 [20480/54000 (38%)] Loss: -452149.593750\n",
      "Train Epoch: 112 [24576/54000 (46%)] Loss: -427143.937500\n",
      "Train Epoch: 112 [28672/54000 (53%)] Loss: -439176.375000\n",
      "Train Epoch: 112 [32768/54000 (61%)] Loss: -436311.531250\n",
      "Train Epoch: 112 [36864/54000 (68%)] Loss: -452207.312500\n",
      "Train Epoch: 112 [40960/54000 (76%)] Loss: -431927.875000\n",
      "Train Epoch: 112 [45056/54000 (83%)] Loss: -428428.187500\n",
      "Train Epoch: 112 [49152/54000 (91%)] Loss: -490892.968750\n",
      "    epoch          : 112\n",
      "    loss           : -446006.9274390244\n",
      "    val_loss       : -455798.91328125\n",
      "Train Epoch: 113 [0/54000 (0%)] Loss: -490135.218750\n",
      "Train Epoch: 113 [4096/54000 (8%)] Loss: -439188.312500\n",
      "Train Epoch: 113 [8192/54000 (15%)] Loss: -492090.593750\n",
      "Train Epoch: 113 [12288/54000 (23%)] Loss: -425295.656250\n",
      "Train Epoch: 113 [16384/54000 (30%)] Loss: -447475.718750\n",
      "Train Epoch: 113 [20480/54000 (38%)] Loss: -451823.312500\n",
      "Train Epoch: 113 [24576/54000 (46%)] Loss: -430986.000000\n",
      "Train Epoch: 113 [28672/54000 (53%)] Loss: -492375.562500\n",
      "Train Epoch: 113 [32768/54000 (61%)] Loss: -458795.656250\n",
      "Train Epoch: 113 [36864/54000 (68%)] Loss: -434816.718750\n",
      "Train Epoch: 113 [40960/54000 (76%)] Loss: -449425.718750\n",
      "Train Epoch: 113 [45056/54000 (83%)] Loss: -455080.468750\n",
      "Train Epoch: 113 [49152/54000 (91%)] Loss: -439672.375000\n",
      "    epoch          : 113\n",
      "    loss           : -446181.90045731707\n",
      "    val_loss       : -455557.046875\n",
      "Train Epoch: 114 [0/54000 (0%)] Loss: -489967.062500\n",
      "Train Epoch: 114 [4096/54000 (8%)] Loss: -437958.156250\n",
      "Train Epoch: 114 [8192/54000 (15%)] Loss: -427816.250000\n",
      "Train Epoch: 114 [12288/54000 (23%)] Loss: -446282.968750\n",
      "Train Epoch: 114 [16384/54000 (30%)] Loss: -428008.750000\n",
      "Train Epoch: 114 [20480/54000 (38%)] Loss: -449643.937500\n",
      "Train Epoch: 114 [24576/54000 (46%)] Loss: -426868.375000\n",
      "Train Epoch: 114 [28672/54000 (53%)] Loss: -439588.125000\n",
      "Train Epoch: 114 [32768/54000 (61%)] Loss: -432282.750000\n",
      "Train Epoch: 114 [36864/54000 (68%)] Loss: -455179.718750\n",
      "Train Epoch: 114 [40960/54000 (76%)] Loss: -433661.656250\n",
      "Train Epoch: 114 [45056/54000 (83%)] Loss: -456024.750000\n",
      "Train Epoch: 114 [49152/54000 (91%)] Loss: -490808.500000\n",
      "    epoch          : 114\n",
      "    loss           : -446338.09420731704\n",
      "    val_loss       : -455669.5828125\n",
      "Train Epoch: 115 [0/54000 (0%)] Loss: -490924.750000\n",
      "Train Epoch: 115 [4096/54000 (8%)] Loss: -447373.125000\n",
      "Train Epoch: 115 [8192/54000 (15%)] Loss: -455798.281250\n",
      "Train Epoch: 115 [12288/54000 (23%)] Loss: -437936.750000\n",
      "Train Epoch: 115 [16384/54000 (30%)] Loss: -430787.000000\n",
      "Train Epoch: 115 [20480/54000 (38%)] Loss: -452478.343750\n",
      "Train Epoch: 115 [24576/54000 (46%)] Loss: -429611.375000\n",
      "Train Epoch: 115 [28672/54000 (53%)] Loss: -438745.968750\n",
      "Train Epoch: 115 [32768/54000 (61%)] Loss: -443241.062500\n",
      "Train Epoch: 115 [36864/54000 (68%)] Loss: -426871.843750\n",
      "Train Epoch: 115 [40960/54000 (76%)] Loss: -447492.687500\n",
      "Train Epoch: 115 [45056/54000 (83%)] Loss: -449720.531250\n",
      "Train Epoch: 115 [49152/54000 (91%)] Loss: -491440.718750\n",
      "    epoch          : 115\n",
      "    loss           : -446503.9868902439\n",
      "    val_loss       : -455249.5640625\n",
      "Train Epoch: 116 [0/54000 (0%)] Loss: -492021.937500\n",
      "Train Epoch: 116 [4096/54000 (8%)] Loss: -434733.437500\n",
      "Train Epoch: 116 [8192/54000 (15%)] Loss: -427874.562500\n",
      "Train Epoch: 116 [12288/54000 (23%)] Loss: -433028.687500\n",
      "Train Epoch: 116 [16384/54000 (30%)] Loss: -489746.562500\n",
      "Train Epoch: 116 [20480/54000 (38%)] Loss: -453519.281250\n",
      "Train Epoch: 116 [24576/54000 (46%)] Loss: -433618.406250\n",
      "Train Epoch: 116 [28672/54000 (53%)] Loss: -436725.593750\n",
      "Train Epoch: 116 [32768/54000 (61%)] Loss: -437982.906250\n",
      "Train Epoch: 116 [36864/54000 (68%)] Loss: -489693.218750\n",
      "Train Epoch: 116 [40960/54000 (76%)] Loss: -425317.906250\n",
      "Train Epoch: 116 [45056/54000 (83%)] Loss: -433276.406250\n",
      "Train Epoch: 116 [49152/54000 (91%)] Loss: -434862.625000\n",
      "    epoch          : 116\n",
      "    loss           : -446426.06356707314\n",
      "    val_loss       : -455939.6265625\n",
      "Train Epoch: 117 [0/54000 (0%)] Loss: -458904.500000\n",
      "Train Epoch: 117 [4096/54000 (8%)] Loss: -450359.281250\n",
      "Train Epoch: 117 [8192/54000 (15%)] Loss: -431287.500000\n",
      "Train Epoch: 117 [12288/54000 (23%)] Loss: -446536.843750\n",
      "Train Epoch: 117 [16384/54000 (30%)] Loss: -456820.375000\n",
      "Train Epoch: 117 [20480/54000 (38%)] Loss: -453183.937500\n",
      "Train Epoch: 117 [24576/54000 (46%)] Loss: -428181.468750\n",
      "Train Epoch: 117 [28672/54000 (53%)] Loss: -430124.812500\n",
      "Train Epoch: 117 [32768/54000 (61%)] Loss: -430546.125000\n",
      "Train Epoch: 117 [36864/54000 (68%)] Loss: -492179.250000\n",
      "Train Epoch: 117 [40960/54000 (76%)] Loss: -430455.062500\n",
      "Train Epoch: 117 [45056/54000 (83%)] Loss: -458464.468750\n",
      "Train Epoch: 117 [49152/54000 (91%)] Loss: -490916.687500\n",
      "    epoch          : 117\n",
      "    loss           : -446923.18780487805\n",
      "    val_loss       : -455495.89765625\n",
      "Train Epoch: 118 [0/54000 (0%)] Loss: -492370.531250\n",
      "Train Epoch: 118 [4096/54000 (8%)] Loss: -446789.750000\n",
      "Train Epoch: 118 [8192/54000 (15%)] Loss: -425253.687500\n",
      "Train Epoch: 118 [12288/54000 (23%)] Loss: -431882.625000\n",
      "Train Epoch: 118 [16384/54000 (30%)] Loss: -455998.000000\n",
      "Train Epoch: 118 [20480/54000 (38%)] Loss: -453377.343750\n",
      "Train Epoch: 118 [24576/54000 (46%)] Loss: -427602.468750\n",
      "Train Epoch: 118 [28672/54000 (53%)] Loss: -439572.343750\n",
      "Train Epoch: 118 [32768/54000 (61%)] Loss: -443843.125000\n",
      "Train Epoch: 118 [36864/54000 (68%)] Loss: -492816.562500\n",
      "Train Epoch: 118 [40960/54000 (76%)] Loss: -435527.281250\n",
      "Train Epoch: 118 [45056/54000 (83%)] Loss: -453998.281250\n",
      "Train Epoch: 118 [49152/54000 (91%)] Loss: -491188.125000\n",
      "    epoch          : 118\n",
      "    loss           : -446933.04237804876\n",
      "    val_loss       : -456315.20703125\n",
      "Train Epoch: 119 [0/54000 (0%)] Loss: -490882.625000\n",
      "Train Epoch: 119 [4096/54000 (8%)] Loss: -458268.718750\n",
      "Train Epoch: 119 [8192/54000 (15%)] Loss: -438905.093750\n",
      "Train Epoch: 119 [12288/54000 (23%)] Loss: -447957.781250\n",
      "Train Epoch: 119 [16384/54000 (30%)] Loss: -431416.750000\n",
      "Train Epoch: 119 [20480/54000 (38%)] Loss: -454785.093750\n",
      "Train Epoch: 119 [24576/54000 (46%)] Loss: -429528.312500\n",
      "Train Epoch: 119 [28672/54000 (53%)] Loss: -439804.187500\n",
      "Train Epoch: 119 [32768/54000 (61%)] Loss: -431437.375000\n",
      "Train Epoch: 119 [36864/54000 (68%)] Loss: -432651.156250\n",
      "Train Epoch: 119 [40960/54000 (76%)] Loss: -430904.875000\n",
      "Train Epoch: 119 [45056/54000 (83%)] Loss: -453488.343750\n",
      "Train Epoch: 119 [49152/54000 (91%)] Loss: -491707.062500\n",
      "    epoch          : 119\n",
      "    loss           : -447313.1442073171\n",
      "    val_loss       : -456516.60546875\n",
      "Train Epoch: 120 [0/54000 (0%)] Loss: -491709.906250\n",
      "Train Epoch: 120 [4096/54000 (8%)] Loss: -458207.906250\n",
      "Train Epoch: 120 [8192/54000 (15%)] Loss: -455923.812500\n",
      "Train Epoch: 120 [12288/54000 (23%)] Loss: -433491.437500\n",
      "Train Epoch: 120 [16384/54000 (30%)] Loss: -459244.031250\n",
      "Train Epoch: 120 [20480/54000 (38%)] Loss: -453657.093750\n",
      "Train Epoch: 120 [24576/54000 (46%)] Loss: -448714.843750\n",
      "Train Epoch: 120 [28672/54000 (53%)] Loss: -441098.906250\n",
      "Train Epoch: 120 [32768/54000 (61%)] Loss: -438909.625000\n",
      "Train Epoch: 120 [36864/54000 (68%)] Loss: -456884.937500\n",
      "Train Epoch: 120 [40960/54000 (76%)] Loss: -447210.343750\n",
      "Train Epoch: 120 [45056/54000 (83%)] Loss: -430835.218750\n",
      "Train Epoch: 120 [49152/54000 (91%)] Loss: -492538.437500\n",
      "    epoch          : 120\n",
      "    loss           : -447326.62042682926\n",
      "    val_loss       : -456717.03828125\n",
      "Train Epoch: 121 [0/54000 (0%)] Loss: -455337.312500\n",
      "Train Epoch: 121 [4096/54000 (8%)] Loss: -430651.812500\n",
      "Train Epoch: 121 [8192/54000 (15%)] Loss: -447683.562500\n",
      "Train Epoch: 121 [12288/54000 (23%)] Loss: -434216.625000\n",
      "Train Epoch: 121 [16384/54000 (30%)] Loss: -450429.875000\n",
      "Train Epoch: 121 [20480/54000 (38%)] Loss: -444549.406250\n",
      "Train Epoch: 121 [24576/54000 (46%)] Loss: -446621.000000\n",
      "Train Epoch: 121 [28672/54000 (53%)] Loss: -438202.562500\n",
      "Train Epoch: 121 [32768/54000 (61%)] Loss: -447716.625000\n",
      "Train Epoch: 121 [36864/54000 (68%)] Loss: -430935.875000\n",
      "Train Epoch: 121 [40960/54000 (76%)] Loss: -446413.593750\n",
      "Train Epoch: 121 [45056/54000 (83%)] Loss: -427732.750000\n",
      "Train Epoch: 121 [49152/54000 (91%)] Loss: -492931.656250\n",
      "    epoch          : 121\n",
      "    loss           : -447571.006097561\n",
      "    val_loss       : -456700.2125\n",
      "Train Epoch: 122 [0/54000 (0%)] Loss: -492874.187500\n",
      "Train Epoch: 122 [4096/54000 (8%)] Loss: -450547.093750\n",
      "Train Epoch: 122 [8192/54000 (15%)] Loss: -454178.343750\n",
      "Train Epoch: 122 [12288/54000 (23%)] Loss: -436153.531250\n",
      "Train Epoch: 122 [16384/54000 (30%)] Loss: -447039.437500\n",
      "Train Epoch: 122 [20480/54000 (38%)] Loss: -442390.750000\n",
      "Train Epoch: 122 [24576/54000 (46%)] Loss: -448762.781250\n",
      "Train Epoch: 122 [28672/54000 (53%)] Loss: -493298.687500\n",
      "Train Epoch: 122 [32768/54000 (61%)] Loss: -448385.437500\n",
      "Train Epoch: 122 [36864/54000 (68%)] Loss: -438929.375000\n",
      "Train Epoch: 122 [40960/54000 (76%)] Loss: -445003.312500\n",
      "Train Epoch: 122 [45056/54000 (83%)] Loss: -438534.875000\n",
      "Train Epoch: 122 [49152/54000 (91%)] Loss: -493253.250000\n",
      "    epoch          : 122\n",
      "    loss           : -447744.25335365854\n",
      "    val_loss       : -456735.7890625\n",
      "Train Epoch: 123 [0/54000 (0%)] Loss: -455278.375000\n",
      "Train Epoch: 123 [4096/54000 (8%)] Loss: -430928.062500\n",
      "Train Epoch: 123 [8192/54000 (15%)] Loss: -438462.875000\n",
      "Train Epoch: 123 [12288/54000 (23%)] Loss: -431877.812500\n",
      "Train Epoch: 123 [16384/54000 (30%)] Loss: -459249.125000\n",
      "Train Epoch: 123 [20480/54000 (38%)] Loss: -444004.687500\n",
      "Train Epoch: 123 [24576/54000 (46%)] Loss: -426085.187500\n",
      "Train Epoch: 123 [28672/54000 (53%)] Loss: -440995.031250\n",
      "Train Epoch: 123 [32768/54000 (61%)] Loss: -434273.375000\n",
      "Train Epoch: 123 [36864/54000 (68%)] Loss: -458769.437500\n",
      "Train Epoch: 123 [40960/54000 (76%)] Loss: -434500.375000\n",
      "Train Epoch: 123 [45056/54000 (83%)] Loss: -439608.562500\n",
      "Train Epoch: 123 [49152/54000 (91%)] Loss: -492502.343750\n",
      "    epoch          : 123\n",
      "    loss           : -448015.56356707314\n",
      "    val_loss       : -456810.2265625\n",
      "Train Epoch: 124 [0/54000 (0%)] Loss: -434619.656250\n",
      "Train Epoch: 124 [4096/54000 (8%)] Loss: -432347.437500\n",
      "Train Epoch: 124 [8192/54000 (15%)] Loss: -440185.875000\n",
      "Train Epoch: 124 [12288/54000 (23%)] Loss: -431958.750000\n",
      "Train Epoch: 124 [16384/54000 (30%)] Loss: -459221.125000\n",
      "Train Epoch: 124 [20480/54000 (38%)] Loss: -455098.875000\n",
      "Train Epoch: 124 [24576/54000 (46%)] Loss: -441293.062500\n",
      "Train Epoch: 124 [28672/54000 (53%)] Loss: -439228.062500\n",
      "Train Epoch: 124 [32768/54000 (61%)] Loss: -452133.843750\n",
      "Train Epoch: 124 [36864/54000 (68%)] Loss: -439411.437500\n",
      "Train Epoch: 124 [40960/54000 (76%)] Loss: -441905.500000\n",
      "Train Epoch: 124 [45056/54000 (83%)] Loss: -439864.187500\n",
      "Train Epoch: 124 [49152/54000 (91%)] Loss: -435567.375000\n",
      "    epoch          : 124\n",
      "    loss           : -447960.4399390244\n",
      "    val_loss       : -457092.3078125\n",
      "Train Epoch: 125 [0/54000 (0%)] Loss: -457475.562500\n",
      "Train Epoch: 125 [4096/54000 (8%)] Loss: -448811.718750\n",
      "Train Epoch: 125 [8192/54000 (15%)] Loss: -431930.500000\n",
      "Train Epoch: 125 [12288/54000 (23%)] Loss: -434132.312500\n",
      "Train Epoch: 125 [16384/54000 (30%)] Loss: -458827.312500\n",
      "Train Epoch: 125 [20480/54000 (38%)] Loss: -446523.312500\n",
      "Train Epoch: 125 [24576/54000 (46%)] Loss: -430700.875000\n",
      "Train Epoch: 125 [28672/54000 (53%)] Loss: -493343.125000\n",
      "Train Epoch: 125 [32768/54000 (61%)] Loss: -436243.468750\n",
      "Train Epoch: 125 [36864/54000 (68%)] Loss: -438452.187500\n",
      "Train Epoch: 125 [40960/54000 (76%)] Loss: -435332.250000\n",
      "Train Epoch: 125 [45056/54000 (83%)] Loss: -458862.250000\n",
      "Train Epoch: 125 [49152/54000 (91%)] Loss: -492859.406250\n",
      "    epoch          : 125\n",
      "    loss           : -448253.8782012195\n",
      "    val_loss       : -456978.34921875\n",
      "Train Epoch: 126 [0/54000 (0%)] Loss: -495292.406250\n",
      "Train Epoch: 126 [4096/54000 (8%)] Loss: -449832.937500\n",
      "Train Epoch: 126 [8192/54000 (15%)] Loss: -451978.500000\n",
      "Train Epoch: 126 [12288/54000 (23%)] Loss: -441923.406250\n",
      "Train Epoch: 126 [16384/54000 (30%)] Loss: -459566.312500\n",
      "Train Epoch: 126 [20480/54000 (38%)] Loss: -439472.562500\n",
      "Train Epoch: 126 [24576/54000 (46%)] Loss: -494127.968750\n",
      "Train Epoch: 126 [28672/54000 (53%)] Loss: -494604.312500\n",
      "Train Epoch: 126 [32768/54000 (61%)] Loss: -447076.500000\n",
      "Train Epoch: 126 [36864/54000 (68%)] Loss: -434108.437500\n",
      "Train Epoch: 126 [40960/54000 (76%)] Loss: -441039.812500\n",
      "Train Epoch: 126 [45056/54000 (83%)] Loss: -433479.468750\n",
      "Train Epoch: 126 [49152/54000 (91%)] Loss: -494055.125000\n",
      "    epoch          : 126\n",
      "    loss           : -448293.91158536583\n",
      "    val_loss       : -456880.3703125\n",
      "Train Epoch: 127 [0/54000 (0%)] Loss: -492894.718750\n",
      "Train Epoch: 127 [4096/54000 (8%)] Loss: -428237.500000\n",
      "Train Epoch: 127 [8192/54000 (15%)] Loss: -456898.093750\n",
      "Train Epoch: 127 [12288/54000 (23%)] Loss: -493539.375000\n",
      "Train Epoch: 127 [16384/54000 (30%)] Loss: -440750.687500\n",
      "Train Epoch: 127 [20480/54000 (38%)] Loss: -453321.750000\n",
      "Train Epoch: 127 [24576/54000 (46%)] Loss: -419914.218750\n",
      "Train Epoch: 127 [28672/54000 (53%)] Loss: -493249.000000\n",
      "Train Epoch: 127 [32768/54000 (61%)] Loss: -493103.625000\n",
      "Train Epoch: 127 [36864/54000 (68%)] Loss: -492461.125000\n",
      "Train Epoch: 127 [40960/54000 (76%)] Loss: -434562.625000\n",
      "Train Epoch: 127 [45056/54000 (83%)] Loss: -429287.437500\n",
      "Train Epoch: 127 [49152/54000 (91%)] Loss: -435736.000000\n",
      "    epoch          : 127\n",
      "    loss           : -448316.8913109756\n",
      "    val_loss       : -457249.00078125\n",
      "Train Epoch: 128 [0/54000 (0%)] Loss: -442031.750000\n",
      "Train Epoch: 128 [4096/54000 (8%)] Loss: -449854.437500\n",
      "Train Epoch: 128 [8192/54000 (15%)] Loss: -458915.031250\n",
      "Train Epoch: 128 [12288/54000 (23%)] Loss: -440175.093750\n",
      "Train Epoch: 128 [16384/54000 (30%)] Loss: -460749.593750\n",
      "Train Epoch: 128 [20480/54000 (38%)] Loss: -453897.062500\n",
      "Train Epoch: 128 [24576/54000 (46%)] Loss: -432637.687500\n",
      "Train Epoch: 128 [28672/54000 (53%)] Loss: -437546.375000\n",
      "Train Epoch: 128 [32768/54000 (61%)] Loss: -493109.312500\n",
      "Train Epoch: 128 [36864/54000 (68%)] Loss: -450341.875000\n",
      "Train Epoch: 128 [40960/54000 (76%)] Loss: -435686.750000\n",
      "Train Epoch: 128 [45056/54000 (83%)] Loss: -457936.000000\n",
      "Train Epoch: 128 [49152/54000 (91%)] Loss: -492253.812500\n",
      "    epoch          : 128\n",
      "    loss           : -448797.6653963415\n",
      "    val_loss       : -457171.9140625\n",
      "Train Epoch: 129 [0/54000 (0%)] Loss: -493777.750000\n",
      "Train Epoch: 129 [4096/54000 (8%)] Loss: -445573.093750\n",
      "Train Epoch: 129 [8192/54000 (15%)] Loss: -456781.906250\n",
      "Train Epoch: 129 [12288/54000 (23%)] Loss: -446791.281250\n",
      "Train Epoch: 129 [16384/54000 (30%)] Loss: -494383.843750\n",
      "Train Epoch: 129 [20480/54000 (38%)] Loss: -444544.375000\n",
      "Train Epoch: 129 [24576/54000 (46%)] Loss: -433374.937500\n",
      "Train Epoch: 129 [28672/54000 (53%)] Loss: -441931.625000\n",
      "Train Epoch: 129 [32768/54000 (61%)] Loss: -491495.968750\n",
      "Train Epoch: 129 [36864/54000 (68%)] Loss: -431948.812500\n",
      "Train Epoch: 129 [40960/54000 (76%)] Loss: -453749.062500\n",
      "Train Epoch: 129 [45056/54000 (83%)] Loss: -442175.500000\n",
      "Train Epoch: 129 [49152/54000 (91%)] Loss: -493291.062500\n",
      "    epoch          : 129\n",
      "    loss           : -448866.65838414634\n",
      "    val_loss       : -457238.65390625\n",
      "Train Epoch: 130 [0/54000 (0%)] Loss: -493079.156250\n",
      "Train Epoch: 130 [4096/54000 (8%)] Loss: -436968.000000\n",
      "Train Epoch: 130 [8192/54000 (15%)] Loss: -440735.812500\n",
      "Train Epoch: 130 [12288/54000 (23%)] Loss: -460740.500000\n",
      "Train Epoch: 130 [16384/54000 (30%)] Loss: -451833.593750\n",
      "Train Epoch: 130 [20480/54000 (38%)] Loss: -447187.375000\n",
      "Train Epoch: 130 [24576/54000 (46%)] Loss: -451167.781250\n",
      "Train Epoch: 130 [28672/54000 (53%)] Loss: -453046.281250\n",
      "Train Epoch: 130 [32768/54000 (61%)] Loss: -440489.812500\n",
      "Train Epoch: 130 [36864/54000 (68%)] Loss: -494304.125000\n",
      "Train Epoch: 130 [40960/54000 (76%)] Loss: -455943.843750\n",
      "Train Epoch: 130 [45056/54000 (83%)] Loss: -456357.312500\n",
      "Train Epoch: 130 [49152/54000 (91%)] Loss: -493688.562500\n",
      "    epoch          : 130\n",
      "    loss           : -449188.7887195122\n",
      "    val_loss       : -457346.47578125\n",
      "Train Epoch: 131 [0/54000 (0%)] Loss: -453910.125000\n",
      "Train Epoch: 131 [4096/54000 (8%)] Loss: -433084.093750\n",
      "Train Epoch: 131 [8192/54000 (15%)] Loss: -443788.343750\n",
      "Train Epoch: 131 [12288/54000 (23%)] Loss: -432958.000000\n",
      "Train Epoch: 131 [16384/54000 (30%)] Loss: -441694.906250\n",
      "Train Epoch: 131 [20480/54000 (38%)] Loss: -454321.031250\n",
      "Train Epoch: 131 [24576/54000 (46%)] Loss: -434166.062500\n",
      "Train Epoch: 131 [28672/54000 (53%)] Loss: -493686.250000\n",
      "Train Epoch: 131 [32768/54000 (61%)] Loss: -434136.687500\n",
      "Train Epoch: 131 [36864/54000 (68%)] Loss: -453316.625000\n",
      "Train Epoch: 131 [40960/54000 (76%)] Loss: -436133.000000\n",
      "Train Epoch: 131 [45056/54000 (83%)] Loss: -432454.718750\n",
      "Train Epoch: 131 [49152/54000 (91%)] Loss: -493403.250000\n",
      "    epoch          : 131\n",
      "    loss           : -449265.52713414637\n",
      "    val_loss       : -457418.45859375\n",
      "Train Epoch: 132 [0/54000 (0%)] Loss: -492966.468750\n",
      "Train Epoch: 132 [4096/54000 (8%)] Loss: -431579.750000\n",
      "Train Epoch: 132 [8192/54000 (15%)] Loss: -442168.250000\n",
      "Train Epoch: 132 [12288/54000 (23%)] Loss: -495470.000000\n",
      "Train Epoch: 132 [16384/54000 (30%)] Loss: -449221.375000\n",
      "Train Epoch: 132 [20480/54000 (38%)] Loss: -454469.625000\n",
      "Train Epoch: 132 [24576/54000 (46%)] Loss: -434269.187500\n",
      "Train Epoch: 132 [28672/54000 (53%)] Loss: -441374.562500\n",
      "Train Epoch: 132 [32768/54000 (61%)] Loss: -459548.250000\n",
      "Train Epoch: 132 [36864/54000 (68%)] Loss: -452913.093750\n",
      "Train Epoch: 132 [40960/54000 (76%)] Loss: -431311.750000\n",
      "Train Epoch: 132 [45056/54000 (83%)] Loss: -451598.281250\n",
      "Train Epoch: 132 [49152/54000 (91%)] Loss: -493553.625000\n",
      "    epoch          : 132\n",
      "    loss           : -449331.90274390246\n",
      "    val_loss       : -456967.6609375\n",
      "Train Epoch: 133 [0/54000 (0%)] Loss: -439374.968750\n",
      "Train Epoch: 133 [4096/54000 (8%)] Loss: -433413.187500\n",
      "Train Epoch: 133 [8192/54000 (15%)] Loss: -440126.125000\n",
      "Train Epoch: 133 [12288/54000 (23%)] Loss: -493779.562500\n",
      "Train Epoch: 133 [16384/54000 (30%)] Loss: -458319.687500\n",
      "Train Epoch: 133 [20480/54000 (38%)] Loss: -455608.875000\n",
      "Train Epoch: 133 [24576/54000 (46%)] Loss: -442483.875000\n",
      "Train Epoch: 133 [28672/54000 (53%)] Loss: -439488.656250\n",
      "Train Epoch: 133 [32768/54000 (61%)] Loss: -491006.343750\n",
      "Train Epoch: 133 [36864/54000 (68%)] Loss: -494454.562500\n",
      "Train Epoch: 133 [40960/54000 (76%)] Loss: -442669.906250\n",
      "Train Epoch: 133 [45056/54000 (83%)] Loss: -433199.937500\n",
      "Train Epoch: 133 [49152/54000 (91%)] Loss: -492690.375000\n",
      "    epoch          : 133\n",
      "    loss           : -449589.6907012195\n",
      "    val_loss       : -456787.48671875\n",
      "Train Epoch: 134 [0/54000 (0%)] Loss: -495457.812500\n",
      "Train Epoch: 134 [4096/54000 (8%)] Loss: -433860.562500\n",
      "Train Epoch: 134 [8192/54000 (15%)] Loss: -440391.687500\n",
      "Train Epoch: 134 [12288/54000 (23%)] Loss: -450724.250000\n",
      "Train Epoch: 134 [16384/54000 (30%)] Loss: -440342.031250\n",
      "Train Epoch: 134 [20480/54000 (38%)] Loss: -455658.250000\n",
      "Train Epoch: 134 [24576/54000 (46%)] Loss: -432688.437500\n",
      "Train Epoch: 134 [28672/54000 (53%)] Loss: -439722.125000\n",
      "Train Epoch: 134 [32768/54000 (61%)] Loss: -436423.125000\n",
      "Train Epoch: 134 [36864/54000 (68%)] Loss: -446240.968750\n",
      "Train Epoch: 134 [40960/54000 (76%)] Loss: -439396.625000\n",
      "Train Epoch: 134 [45056/54000 (83%)] Loss: -459027.500000\n",
      "Train Epoch: 134 [49152/54000 (91%)] Loss: -494749.281250\n",
      "    epoch          : 134\n",
      "    loss           : -449587.5236280488\n",
      "    val_loss       : -457523.6953125\n",
      "Train Epoch: 135 [0/54000 (0%)] Loss: -454587.625000\n",
      "Train Epoch: 135 [4096/54000 (8%)] Loss: -437743.562500\n",
      "Train Epoch: 135 [8192/54000 (15%)] Loss: -458203.562500\n",
      "Train Epoch: 135 [12288/54000 (23%)] Loss: -495833.343750\n",
      "Train Epoch: 135 [16384/54000 (30%)] Loss: -432023.718750\n",
      "Train Epoch: 135 [20480/54000 (38%)] Loss: -453396.687500\n",
      "Train Epoch: 135 [24576/54000 (46%)] Loss: -430184.937500\n",
      "Train Epoch: 135 [28672/54000 (53%)] Loss: -444097.468750\n",
      "Train Epoch: 135 [32768/54000 (61%)] Loss: -443460.250000\n",
      "Train Epoch: 135 [36864/54000 (68%)] Loss: -435316.156250\n",
      "Train Epoch: 135 [40960/54000 (76%)] Loss: -442572.687500\n",
      "Train Epoch: 135 [45056/54000 (83%)] Loss: -454451.562500\n",
      "Train Epoch: 135 [49152/54000 (91%)] Loss: -494823.875000\n",
      "    epoch          : 135\n",
      "    loss           : -449721.9961890244\n",
      "    val_loss       : -457088.82734375\n",
      "Train Epoch: 136 [0/54000 (0%)] Loss: -485839.250000\n",
      "Train Epoch: 136 [4096/54000 (8%)] Loss: -443113.812500\n",
      "Train Epoch: 136 [8192/54000 (15%)] Loss: -435779.312500\n",
      "Train Epoch: 136 [12288/54000 (23%)] Loss: -433203.343750\n",
      "Train Epoch: 136 [16384/54000 (30%)] Loss: -447858.125000\n",
      "Train Epoch: 136 [20480/54000 (38%)] Loss: -453506.000000\n",
      "Train Epoch: 136 [24576/54000 (46%)] Loss: -434562.062500\n",
      "Train Epoch: 136 [28672/54000 (53%)] Loss: -442934.781250\n",
      "Train Epoch: 136 [32768/54000 (61%)] Loss: -431002.531250\n",
      "Train Epoch: 136 [36864/54000 (68%)] Loss: -458911.906250\n",
      "Train Epoch: 136 [40960/54000 (76%)] Loss: -439893.156250\n",
      "Train Epoch: 136 [45056/54000 (83%)] Loss: -433045.156250\n",
      "Train Epoch: 136 [49152/54000 (91%)] Loss: -493787.062500\n",
      "    epoch          : 136\n",
      "    loss           : -449998.5240853659\n",
      "    val_loss       : -457943.59921875\n",
      "Train Epoch: 137 [0/54000 (0%)] Loss: -456052.312500\n",
      "Train Epoch: 137 [4096/54000 (8%)] Loss: -459630.250000\n",
      "Train Epoch: 137 [8192/54000 (15%)] Loss: -494562.750000\n",
      "Train Epoch: 137 [12288/54000 (23%)] Loss: -440551.468750\n",
      "Train Epoch: 137 [16384/54000 (30%)] Loss: -438160.375000\n",
      "Train Epoch: 137 [20480/54000 (38%)] Loss: -444755.625000\n",
      "Train Epoch: 137 [24576/54000 (46%)] Loss: -435883.000000\n",
      "Train Epoch: 137 [28672/54000 (53%)] Loss: -493633.812500\n",
      "Train Epoch: 137 [32768/54000 (61%)] Loss: -436665.093750\n",
      "Train Epoch: 137 [36864/54000 (68%)] Loss: -431904.781250\n",
      "Train Epoch: 137 [40960/54000 (76%)] Loss: -435375.062500\n",
      "Train Epoch: 137 [45056/54000 (83%)] Loss: -438657.781250\n",
      "Train Epoch: 137 [49152/54000 (91%)] Loss: -495132.000000\n",
      "    epoch          : 137\n",
      "    loss           : -450072.82484756096\n",
      "    val_loss       : -457330.0921875\n",
      "Train Epoch: 138 [0/54000 (0%)] Loss: -432940.093750\n",
      "Train Epoch: 138 [4096/54000 (8%)] Loss: -443569.687500\n",
      "Train Epoch: 138 [8192/54000 (15%)] Loss: -459873.093750\n",
      "Train Epoch: 138 [12288/54000 (23%)] Loss: -432703.437500\n",
      "Train Epoch: 138 [16384/54000 (30%)] Loss: -430925.750000\n",
      "Train Epoch: 138 [20480/54000 (38%)] Loss: -456059.250000\n",
      "Train Epoch: 138 [24576/54000 (46%)] Loss: -442352.843750\n",
      "Train Epoch: 138 [28672/54000 (53%)] Loss: -493901.500000\n",
      "Train Epoch: 138 [32768/54000 (61%)] Loss: -450020.187500\n",
      "Train Epoch: 138 [36864/54000 (68%)] Loss: -428026.218750\n",
      "Train Epoch: 138 [40960/54000 (76%)] Loss: -447414.593750\n",
      "Train Epoch: 138 [45056/54000 (83%)] Loss: -434123.843750\n",
      "Train Epoch: 138 [49152/54000 (91%)] Loss: -494925.093750\n",
      "    epoch          : 138\n",
      "    loss           : -450020.8850609756\n",
      "    val_loss       : -457053.14140625\n",
      "Train Epoch: 139 [0/54000 (0%)] Loss: -458828.750000\n",
      "Train Epoch: 139 [4096/54000 (8%)] Loss: -449394.250000\n",
      "Train Epoch: 139 [8192/54000 (15%)] Loss: -441027.812500\n",
      "Train Epoch: 139 [12288/54000 (23%)] Loss: -493940.562500\n",
      "Train Epoch: 139 [16384/54000 (30%)] Loss: -456560.687500\n",
      "Train Epoch: 139 [20480/54000 (38%)] Loss: -446258.312500\n",
      "Train Epoch: 139 [24576/54000 (46%)] Loss: -435153.937500\n",
      "Train Epoch: 139 [28672/54000 (53%)] Loss: -440486.687500\n",
      "Train Epoch: 139 [32768/54000 (61%)] Loss: -495209.218750\n",
      "Train Epoch: 139 [36864/54000 (68%)] Loss: -431555.562500\n",
      "Train Epoch: 139 [40960/54000 (76%)] Loss: -444581.843750\n",
      "Train Epoch: 139 [45056/54000 (83%)] Loss: -459902.906250\n",
      "Train Epoch: 139 [49152/54000 (91%)] Loss: -496209.593750\n",
      "    epoch          : 139\n",
      "    loss           : -450321.093597561\n",
      "    val_loss       : -457736.425\n",
      "Train Epoch: 140 [0/54000 (0%)] Loss: -441812.156250\n",
      "Train Epoch: 140 [4096/54000 (8%)] Loss: -458199.187500\n",
      "Train Epoch: 140 [8192/54000 (15%)] Loss: -459014.500000\n",
      "Train Epoch: 140 [12288/54000 (23%)] Loss: -437137.500000\n",
      "Train Epoch: 140 [16384/54000 (30%)] Loss: -460156.500000\n",
      "Train Epoch: 140 [20480/54000 (38%)] Loss: -458919.312500\n",
      "Train Epoch: 140 [24576/54000 (46%)] Loss: -449256.906250\n",
      "Train Epoch: 140 [28672/54000 (53%)] Loss: -442114.437500\n",
      "Train Epoch: 140 [32768/54000 (61%)] Loss: -434724.187500\n",
      "Train Epoch: 140 [36864/54000 (68%)] Loss: -458959.687500\n",
      "Train Epoch: 140 [40960/54000 (76%)] Loss: -453580.937500\n",
      "Train Epoch: 140 [45056/54000 (83%)] Loss: -460031.343750\n",
      "Train Epoch: 140 [49152/54000 (91%)] Loss: -495650.500000\n",
      "    epoch          : 140\n",
      "    loss           : -450511.5253048781\n",
      "    val_loss       : -457900.1875\n",
      "Train Epoch: 141 [0/54000 (0%)] Loss: -495443.687500\n",
      "Train Epoch: 141 [4096/54000 (8%)] Loss: -437618.562500\n",
      "Train Epoch: 141 [8192/54000 (15%)] Loss: -457279.031250\n",
      "Train Epoch: 141 [12288/54000 (23%)] Loss: -436522.156250\n",
      "Train Epoch: 141 [16384/54000 (30%)] Loss: -435781.187500\n",
      "Train Epoch: 141 [20480/54000 (38%)] Loss: -447170.500000\n",
      "Train Epoch: 141 [24576/54000 (46%)] Loss: -460363.156250\n",
      "Train Epoch: 141 [28672/54000 (53%)] Loss: -443084.281250\n",
      "Train Epoch: 141 [32768/54000 (61%)] Loss: -433993.281250\n",
      "Train Epoch: 141 [36864/54000 (68%)] Loss: -459032.375000\n",
      "Train Epoch: 141 [40960/54000 (76%)] Loss: -456360.687500\n",
      "Train Epoch: 141 [45056/54000 (83%)] Loss: -458810.281250\n",
      "Train Epoch: 141 [49152/54000 (91%)] Loss: -495598.562500\n",
      "    epoch          : 141\n",
      "    loss           : -450642.7577743902\n",
      "    val_loss       : -457583.21484375\n",
      "Train Epoch: 142 [0/54000 (0%)] Loss: -431428.937500\n",
      "Train Epoch: 142 [4096/54000 (8%)] Loss: -447084.312500\n",
      "Train Epoch: 142 [8192/54000 (15%)] Loss: -435492.093750\n",
      "Train Epoch: 142 [12288/54000 (23%)] Loss: -436967.562500\n",
      "Train Epoch: 142 [16384/54000 (30%)] Loss: -496064.593750\n",
      "Train Epoch: 142 [20480/54000 (38%)] Loss: -454327.250000\n",
      "Train Epoch: 142 [24576/54000 (46%)] Loss: -442309.250000\n",
      "Train Epoch: 142 [28672/54000 (53%)] Loss: -455208.781250\n",
      "Train Epoch: 142 [32768/54000 (61%)] Loss: -459856.625000\n",
      "Train Epoch: 142 [36864/54000 (68%)] Loss: -432909.250000\n",
      "Train Epoch: 142 [40960/54000 (76%)] Loss: -447655.562500\n",
      "Train Epoch: 142 [45056/54000 (83%)] Loss: -435450.250000\n",
      "Train Epoch: 142 [49152/54000 (91%)] Loss: -452864.062500\n",
      "    epoch          : 142\n",
      "    loss           : -450704.1820121951\n",
      "    val_loss       : -458185.83828125\n",
      "Train Epoch: 143 [0/54000 (0%)] Loss: -496855.562500\n",
      "Train Epoch: 143 [4096/54000 (8%)] Loss: -449994.437500\n",
      "Train Epoch: 143 [8192/54000 (15%)] Loss: -435221.718750\n",
      "Train Epoch: 143 [12288/54000 (23%)] Loss: -496643.000000\n",
      "Train Epoch: 143 [16384/54000 (30%)] Loss: -441585.875000\n",
      "Train Epoch: 143 [20480/54000 (38%)] Loss: -453990.500000\n",
      "Train Epoch: 143 [24576/54000 (46%)] Loss: -434002.250000\n",
      "Train Epoch: 143 [28672/54000 (53%)] Loss: -444668.531250\n",
      "Train Epoch: 143 [32768/54000 (61%)] Loss: -443335.937500\n",
      "Train Epoch: 143 [36864/54000 (68%)] Loss: -425693.437500\n",
      "Train Epoch: 143 [40960/54000 (76%)] Loss: -435924.500000\n",
      "Train Epoch: 143 [45056/54000 (83%)] Loss: -458599.687500\n",
      "Train Epoch: 143 [49152/54000 (91%)] Loss: -496288.312500\n",
      "    epoch          : 143\n",
      "    loss           : -450914.74664634146\n",
      "    val_loss       : -458027.54921875\n",
      "Train Epoch: 144 [0/54000 (0%)] Loss: -443094.906250\n",
      "Train Epoch: 144 [4096/54000 (8%)] Loss: -461629.250000\n",
      "Train Epoch: 144 [8192/54000 (15%)] Loss: -456237.125000\n",
      "Train Epoch: 144 [12288/54000 (23%)] Loss: -434893.812500\n",
      "Train Epoch: 144 [16384/54000 (30%)] Loss: -437077.875000\n",
      "Train Epoch: 144 [20480/54000 (38%)] Loss: -456092.500000\n",
      "Train Epoch: 144 [24576/54000 (46%)] Loss: -437634.625000\n",
      "Train Epoch: 144 [28672/54000 (53%)] Loss: -442499.031250\n",
      "Train Epoch: 144 [32768/54000 (61%)] Loss: -461014.062500\n",
      "Train Epoch: 144 [36864/54000 (68%)] Loss: -496273.437500\n",
      "Train Epoch: 144 [40960/54000 (76%)] Loss: -435501.062500\n",
      "Train Epoch: 144 [45056/54000 (83%)] Loss: -437280.531250\n",
      "Train Epoch: 144 [49152/54000 (91%)] Loss: -440185.000000\n",
      "    epoch          : 144\n",
      "    loss           : -451042.58734756097\n",
      "    val_loss       : -458334.7515625\n",
      "Train Epoch: 145 [0/54000 (0%)] Loss: -444797.906250\n",
      "Train Epoch: 145 [4096/54000 (8%)] Loss: -446066.937500\n",
      "Train Epoch: 145 [8192/54000 (15%)] Loss: -434251.125000\n",
      "Train Epoch: 145 [12288/54000 (23%)] Loss: -451659.687500\n",
      "Train Epoch: 145 [16384/54000 (30%)] Loss: -441994.062500\n",
      "Train Epoch: 145 [20480/54000 (38%)] Loss: -458791.468750\n",
      "Train Epoch: 145 [24576/54000 (46%)] Loss: -438116.406250\n",
      "Train Epoch: 145 [28672/54000 (53%)] Loss: -440901.875000\n",
      "Train Epoch: 145 [32768/54000 (61%)] Loss: -433835.062500\n",
      "Train Epoch: 145 [36864/54000 (68%)] Loss: -445013.125000\n",
      "Train Epoch: 145 [40960/54000 (76%)] Loss: -442677.437500\n",
      "Train Epoch: 145 [45056/54000 (83%)] Loss: -461112.187500\n",
      "Train Epoch: 145 [49152/54000 (91%)] Loss: -497169.656250\n",
      "    epoch          : 145\n",
      "    loss           : -451113.3798780488\n",
      "    val_loss       : -458241.71953125\n",
      "Train Epoch: 146 [0/54000 (0%)] Loss: -496553.812500\n",
      "Train Epoch: 146 [4096/54000 (8%)] Loss: -430223.156250\n",
      "Train Epoch: 146 [8192/54000 (15%)] Loss: -436785.000000\n",
      "Train Epoch: 146 [12288/54000 (23%)] Loss: -452936.375000\n",
      "Train Epoch: 146 [16384/54000 (30%)] Loss: -494904.625000\n",
      "Train Epoch: 146 [20480/54000 (38%)] Loss: -458312.250000\n",
      "Train Epoch: 146 [24576/54000 (46%)] Loss: -441837.750000\n",
      "Train Epoch: 146 [28672/54000 (53%)] Loss: -440145.187500\n",
      "Train Epoch: 146 [32768/54000 (61%)] Loss: -494366.062500\n",
      "Train Epoch: 146 [36864/54000 (68%)] Loss: -461003.500000\n",
      "Train Epoch: 146 [40960/54000 (76%)] Loss: -452291.000000\n",
      "Train Epoch: 146 [45056/54000 (83%)] Loss: -435058.687500\n",
      "Train Epoch: 146 [49152/54000 (91%)] Loss: -437232.875000\n",
      "    epoch          : 146\n",
      "    loss           : -451277.39161585364\n",
      "    val_loss       : -458315.534375\n",
      "Train Epoch: 147 [0/54000 (0%)] Loss: -458532.812500\n",
      "Train Epoch: 147 [4096/54000 (8%)] Loss: -437614.437500\n",
      "Train Epoch: 147 [8192/54000 (15%)] Loss: -443439.000000\n",
      "Train Epoch: 147 [12288/54000 (23%)] Loss: -453712.718750\n",
      "Train Epoch: 147 [16384/54000 (30%)] Loss: -435778.625000\n",
      "Train Epoch: 147 [20480/54000 (38%)] Loss: -456469.000000\n",
      "Train Epoch: 147 [24576/54000 (46%)] Loss: -443113.187500\n",
      "Train Epoch: 147 [28672/54000 (53%)] Loss: -495593.437500\n",
      "Train Epoch: 147 [32768/54000 (61%)] Loss: -453027.406250\n",
      "Train Epoch: 147 [36864/54000 (68%)] Loss: -432782.031250\n",
      "Train Epoch: 147 [40960/54000 (76%)] Loss: -449149.812500\n",
      "Train Epoch: 147 [45056/54000 (83%)] Loss: -435403.812500\n",
      "Train Epoch: 147 [49152/54000 (91%)] Loss: -497349.375000\n",
      "    epoch          : 147\n",
      "    loss           : -451520.0443597561\n",
      "    val_loss       : -458006.21484375\n",
      "Train Epoch: 148 [0/54000 (0%)] Loss: -496663.312500\n",
      "Train Epoch: 148 [4096/54000 (8%)] Loss: -461065.312500\n",
      "Train Epoch: 148 [8192/54000 (15%)] Loss: -455595.812500\n",
      "Train Epoch: 148 [12288/54000 (23%)] Loss: -447348.312500\n",
      "Train Epoch: 148 [16384/54000 (30%)] Loss: -433923.937500\n",
      "Train Epoch: 148 [20480/54000 (38%)] Loss: -447040.937500\n",
      "Train Epoch: 148 [24576/54000 (46%)] Loss: -460088.406250\n",
      "Train Epoch: 148 [28672/54000 (53%)] Loss: -444159.750000\n",
      "Train Epoch: 148 [32768/54000 (61%)] Loss: -458231.562500\n",
      "Train Epoch: 148 [36864/54000 (68%)] Loss: -448181.000000\n",
      "Train Epoch: 148 [40960/54000 (76%)] Loss: -436028.437500\n",
      "Train Epoch: 148 [45056/54000 (83%)] Loss: -458205.656250\n",
      "Train Epoch: 148 [49152/54000 (91%)] Loss: -495556.250000\n",
      "    epoch          : 148\n",
      "    loss           : -451538.8539634146\n",
      "    val_loss       : -458226.97265625\n",
      "Train Epoch: 149 [0/54000 (0%)] Loss: -497036.062500\n",
      "Train Epoch: 149 [4096/54000 (8%)] Loss: -453383.687500\n",
      "Train Epoch: 149 [8192/54000 (15%)] Loss: -445165.562500\n",
      "Train Epoch: 149 [12288/54000 (23%)] Loss: -435752.843750\n",
      "Train Epoch: 149 [16384/54000 (30%)] Loss: -494974.156250\n",
      "Train Epoch: 149 [20480/54000 (38%)] Loss: -457217.062500\n",
      "Train Epoch: 149 [24576/54000 (46%)] Loss: -434394.937500\n",
      "Train Epoch: 149 [28672/54000 (53%)] Loss: -441772.531250\n",
      "Train Epoch: 149 [32768/54000 (61%)] Loss: -456856.750000\n",
      "Train Epoch: 149 [36864/54000 (68%)] Loss: -461135.812500\n",
      "Train Epoch: 149 [40960/54000 (76%)] Loss: -447346.125000\n",
      "Train Epoch: 149 [45056/54000 (83%)] Loss: -459140.000000\n",
      "Train Epoch: 149 [49152/54000 (91%)] Loss: -438066.875000\n",
      "    epoch          : 149\n",
      "    loss           : -451674.125\n",
      "    val_loss       : -458480.565625\n",
      "Train Epoch: 150 [0/54000 (0%)] Loss: -497723.625000\n",
      "Train Epoch: 150 [4096/54000 (8%)] Loss: -460070.656250\n",
      "Train Epoch: 150 [8192/54000 (15%)] Loss: -442954.468750\n",
      "Train Epoch: 150 [12288/54000 (23%)] Loss: -453802.781250\n",
      "Train Epoch: 150 [16384/54000 (30%)] Loss: -497283.625000\n",
      "Train Epoch: 150 [20480/54000 (38%)] Loss: -457439.656250\n",
      "Train Epoch: 150 [24576/54000 (46%)] Loss: -436995.562500\n",
      "Train Epoch: 150 [28672/54000 (53%)] Loss: -442747.062500\n",
      "Train Epoch: 150 [32768/54000 (61%)] Loss: -496275.062500\n",
      "Train Epoch: 150 [36864/54000 (68%)] Loss: -442900.125000\n",
      "Train Epoch: 150 [40960/54000 (76%)] Loss: -447968.937500\n",
      "Train Epoch: 150 [45056/54000 (83%)] Loss: -459138.468750\n",
      "Train Epoch: 150 [49152/54000 (91%)] Loss: -437799.750000\n",
      "    epoch          : 150\n",
      "    loss           : -451730.3711890244\n",
      "    val_loss       : -458312.45\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0510_004522/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [0/54000 (0%)] Loss: -495056.437500\n",
      "Train Epoch: 151 [4096/54000 (8%)] Loss: -435041.437500\n",
      "Train Epoch: 151 [8192/54000 (15%)] Loss: -435120.656250\n",
      "Train Epoch: 151 [12288/54000 (23%)] Loss: -451058.187500\n",
      "Train Epoch: 151 [16384/54000 (30%)] Loss: -496611.500000\n",
      "Train Epoch: 151 [20480/54000 (38%)] Loss: -457299.281250\n",
      "Train Epoch: 151 [24576/54000 (46%)] Loss: -444716.875000\n",
      "Train Epoch: 151 [28672/54000 (53%)] Loss: -443314.562500\n",
      "Train Epoch: 151 [32768/54000 (61%)] Loss: -452120.906250\n",
      "Train Epoch: 151 [36864/54000 (68%)] Loss: -495980.781250\n",
      "Train Epoch: 151 [40960/54000 (76%)] Loss: -438495.812500\n",
      "Train Epoch: 151 [45056/54000 (83%)] Loss: -442051.625000\n",
      "Train Epoch: 151 [49152/54000 (91%)] Loss: -497147.718750\n",
      "    epoch          : 151\n",
      "    loss           : -451979.42469512194\n",
      "    val_loss       : -458635.7359375\n",
      "Train Epoch: 152 [0/54000 (0%)] Loss: -445759.218750\n",
      "Train Epoch: 152 [4096/54000 (8%)] Loss: -454946.437500\n",
      "Train Epoch: 152 [8192/54000 (15%)] Loss: -445381.343750\n",
      "Train Epoch: 152 [12288/54000 (23%)] Loss: -432546.156250\n",
      "Train Epoch: 152 [16384/54000 (30%)] Loss: -458271.593750\n",
      "Train Epoch: 152 [20480/54000 (38%)] Loss: -458829.250000\n",
      "Train Epoch: 152 [24576/54000 (46%)] Loss: -446635.937500\n",
      "Train Epoch: 152 [28672/54000 (53%)] Loss: -442484.031250\n",
      "Train Epoch: 152 [32768/54000 (61%)] Loss: -452956.875000\n",
      "Train Epoch: 152 [36864/54000 (68%)] Loss: -444786.625000\n",
      "Train Epoch: 152 [40960/54000 (76%)] Loss: -448548.125000\n",
      "Train Epoch: 152 [45056/54000 (83%)] Loss: -435733.843750\n",
      "Train Epoch: 152 [49152/54000 (91%)] Loss: -497629.875000\n",
      "    epoch          : 152\n",
      "    loss           : -452089.92713414633\n",
      "    val_loss       : -458434.3140625\n",
      "Train Epoch: 153 [0/54000 (0%)] Loss: -496591.750000\n",
      "Train Epoch: 153 [4096/54000 (8%)] Loss: -442658.343750\n",
      "Train Epoch: 153 [8192/54000 (15%)] Loss: -459028.718750\n",
      "Train Epoch: 153 [12288/54000 (23%)] Loss: -448439.656250\n",
      "Train Epoch: 153 [16384/54000 (30%)] Loss: -435586.718750\n",
      "Train Epoch: 153 [20480/54000 (38%)] Loss: -446170.718750\n",
      "Train Epoch: 153 [24576/54000 (46%)] Loss: -454086.937500\n",
      "Train Epoch: 153 [28672/54000 (53%)] Loss: -496407.781250\n",
      "Train Epoch: 153 [32768/54000 (61%)] Loss: -444739.250000\n",
      "Train Epoch: 153 [36864/54000 (68%)] Loss: -435859.281250\n",
      "Train Epoch: 153 [40960/54000 (76%)] Loss: -443966.937500\n",
      "Train Epoch: 153 [45056/54000 (83%)] Loss: -457257.750000\n",
      "Train Epoch: 153 [49152/54000 (91%)] Loss: -495874.937500\n",
      "    epoch          : 153\n",
      "    loss           : -452225.0911585366\n",
      "    val_loss       : -458923.8234375\n",
      "Train Epoch: 154 [0/54000 (0%)] Loss: -497936.937500\n",
      "Train Epoch: 154 [4096/54000 (8%)] Loss: -439253.562500\n",
      "Train Epoch: 154 [8192/54000 (15%)] Loss: -440183.031250\n",
      "Train Epoch: 154 [12288/54000 (23%)] Loss: -460944.562500\n",
      "Train Epoch: 154 [16384/54000 (30%)] Loss: -439137.625000\n",
      "Train Epoch: 154 [20480/54000 (38%)] Loss: -457033.125000\n",
      "Train Epoch: 154 [24576/54000 (46%)] Loss: -461076.593750\n",
      "Train Epoch: 154 [28672/54000 (53%)] Loss: -439831.343750\n",
      "Train Epoch: 154 [32768/54000 (61%)] Loss: -443325.468750\n",
      "Train Epoch: 154 [36864/54000 (68%)] Loss: -457112.593750\n",
      "Train Epoch: 154 [40960/54000 (76%)] Loss: -446560.687500\n",
      "Train Epoch: 154 [45056/54000 (83%)] Loss: -438910.937500\n",
      "Train Epoch: 154 [49152/54000 (91%)] Loss: -496904.687500\n",
      "    epoch          : 154\n",
      "    loss           : -452339.7536585366\n",
      "    val_loss       : -458031.07421875\n",
      "Train Epoch: 155 [0/54000 (0%)] Loss: -458353.718750\n",
      "Train Epoch: 155 [4096/54000 (8%)] Loss: -439197.906250\n",
      "Train Epoch: 155 [8192/54000 (15%)] Loss: -443880.812500\n",
      "Train Epoch: 155 [12288/54000 (23%)] Loss: -460661.437500\n",
      "Train Epoch: 155 [16384/54000 (30%)] Loss: -446013.062500\n",
      "Train Epoch: 155 [20480/54000 (38%)] Loss: -456044.968750\n",
      "Train Epoch: 155 [24576/54000 (46%)] Loss: -444225.031250\n",
      "Train Epoch: 155 [28672/54000 (53%)] Loss: -442288.406250\n",
      "Train Epoch: 155 [32768/54000 (61%)] Loss: -452713.750000\n",
      "Train Epoch: 155 [36864/54000 (68%)] Loss: -435896.312500\n",
      "Train Epoch: 155 [40960/54000 (76%)] Loss: -436991.281250\n",
      "Train Epoch: 155 [45056/54000 (83%)] Loss: -434880.562500\n",
      "Train Epoch: 155 [49152/54000 (91%)] Loss: -496934.718750\n",
      "    epoch          : 155\n",
      "    loss           : -452251.1632621951\n",
      "    val_loss       : -458196.63046875\n",
      "Train Epoch: 156 [0/54000 (0%)] Loss: -496273.125000\n",
      "Train Epoch: 156 [4096/54000 (8%)] Loss: -439159.843750\n",
      "Train Epoch: 156 [8192/54000 (15%)] Loss: -459609.000000\n",
      "Train Epoch: 156 [12288/54000 (23%)] Loss: -438454.250000\n",
      "Train Epoch: 156 [16384/54000 (30%)] Loss: -435788.750000\n",
      "Train Epoch: 156 [20480/54000 (38%)] Loss: -456784.875000\n",
      "Train Epoch: 156 [24576/54000 (46%)] Loss: -461252.500000\n",
      "Train Epoch: 156 [28672/54000 (53%)] Loss: -447653.281250\n",
      "Train Epoch: 156 [32768/54000 (61%)] Loss: -449165.562500\n",
      "Train Epoch: 156 [36864/54000 (68%)] Loss: -455744.687500\n",
      "Train Epoch: 156 [40960/54000 (76%)] Loss: -453681.937500\n",
      "Train Epoch: 156 [45056/54000 (83%)] Loss: -462246.437500\n",
      "Train Epoch: 156 [49152/54000 (91%)] Loss: -498384.812500\n",
      "    epoch          : 156\n",
      "    loss           : -452640.77652439027\n",
      "    val_loss       : -458429.33046875\n",
      "Train Epoch: 157 [0/54000 (0%)] Loss: -497341.156250\n",
      "Train Epoch: 157 [4096/54000 (8%)] Loss: -459781.500000\n",
      "Train Epoch: 157 [8192/54000 (15%)] Loss: -458267.687500\n",
      "Train Epoch: 157 [12288/54000 (23%)] Loss: -461659.000000\n",
      "Train Epoch: 157 [16384/54000 (30%)] Loss: -461170.562500\n",
      "Train Epoch: 157 [20480/54000 (38%)] Loss: -448761.562500\n",
      "Train Epoch: 157 [24576/54000 (46%)] Loss: -461155.250000\n",
      "Train Epoch: 157 [28672/54000 (53%)] Loss: -497216.062500\n",
      "Train Epoch: 157 [32768/54000 (61%)] Loss: -446026.625000\n",
      "Train Epoch: 157 [36864/54000 (68%)] Loss: -453173.187500\n",
      "Train Epoch: 157 [40960/54000 (76%)] Loss: -437614.656250\n",
      "Train Epoch: 157 [45056/54000 (83%)] Loss: -435126.437500\n",
      "Train Epoch: 157 [49152/54000 (91%)] Loss: -497371.562500\n",
      "    epoch          : 157\n",
      "    loss           : -452503.6594512195\n",
      "    val_loss       : -458644.99765625\n",
      "Train Epoch: 158 [0/54000 (0%)] Loss: -497683.000000\n",
      "Train Epoch: 158 [4096/54000 (8%)] Loss: -461520.656250\n",
      "Train Epoch: 158 [8192/54000 (15%)] Loss: -444393.750000\n",
      "Train Epoch: 158 [12288/54000 (23%)] Loss: -455267.781250\n",
      "Train Epoch: 158 [16384/54000 (30%)] Loss: -497533.187500\n",
      "Train Epoch: 158 [20480/54000 (38%)] Loss: -457376.250000\n",
      "Train Epoch: 158 [24576/54000 (46%)] Loss: -460687.093750\n",
      "Train Epoch: 158 [28672/54000 (53%)] Loss: -435779.437500\n",
      "Train Epoch: 158 [32768/54000 (61%)] Loss: -497033.187500\n",
      "Train Epoch: 158 [36864/54000 (68%)] Loss: -497004.375000\n",
      "Train Epoch: 158 [40960/54000 (76%)] Loss: -437005.531250\n",
      "Train Epoch: 158 [45056/54000 (83%)] Loss: -460559.812500\n",
      "Train Epoch: 158 [49152/54000 (91%)] Loss: -497799.250000\n",
      "    epoch          : 158\n",
      "    loss           : -452868.23597560974\n",
      "    val_loss       : -458711.79609375\n",
      "Train Epoch: 159 [0/54000 (0%)] Loss: -497618.875000\n",
      "Train Epoch: 159 [4096/54000 (8%)] Loss: -450777.218750\n",
      "Train Epoch: 159 [8192/54000 (15%)] Loss: -440796.375000\n",
      "Train Epoch: 159 [12288/54000 (23%)] Loss: -454379.125000\n",
      "Train Epoch: 159 [16384/54000 (30%)] Loss: -497213.656250\n",
      "Train Epoch: 159 [20480/54000 (38%)] Loss: -443049.906250\n",
      "Train Epoch: 159 [24576/54000 (46%)] Loss: -460198.562500\n",
      "Train Epoch: 159 [28672/54000 (53%)] Loss: -444768.437500\n",
      "Train Epoch: 159 [32768/54000 (61%)] Loss: -462521.718750\n",
      "Train Epoch: 159 [36864/54000 (68%)] Loss: -496931.562500\n",
      "Train Epoch: 159 [40960/54000 (76%)] Loss: -446111.625000\n",
      "Train Epoch: 159 [45056/54000 (83%)] Loss: -463242.187500\n",
      "Train Epoch: 159 [49152/54000 (91%)] Loss: -497718.375000\n",
      "    epoch          : 159\n",
      "    loss           : -452948.9861280488\n",
      "    val_loss       : -458610.34140625\n",
      "Train Epoch: 160 [0/54000 (0%)] Loss: -445362.875000\n",
      "Train Epoch: 160 [4096/54000 (8%)] Loss: -437360.718750\n",
      "Train Epoch: 160 [8192/54000 (15%)] Loss: -459671.312500\n",
      "Train Epoch: 160 [12288/54000 (23%)] Loss: -438337.250000\n",
      "Train Epoch: 160 [16384/54000 (30%)] Loss: -452845.187500\n",
      "Train Epoch: 160 [20480/54000 (38%)] Loss: -446484.250000\n",
      "Train Epoch: 160 [24576/54000 (46%)] Loss: -445396.687500\n",
      "Train Epoch: 160 [28672/54000 (53%)] Loss: -438406.875000\n",
      "Train Epoch: 160 [32768/54000 (61%)] Loss: -446605.625000\n",
      "Train Epoch: 160 [36864/54000 (68%)] Loss: -437564.093750\n",
      "Train Epoch: 160 [40960/54000 (76%)] Loss: -449194.187500\n",
      "Train Epoch: 160 [45056/54000 (83%)] Loss: -431733.406250\n",
      "Train Epoch: 160 [49152/54000 (91%)] Loss: -439620.187500\n",
      "    epoch          : 160\n",
      "    loss           : -452852.1725609756\n",
      "    val_loss       : -458474.89296875\n",
      "Train Epoch: 161 [0/54000 (0%)] Loss: -452335.250000\n",
      "Train Epoch: 161 [4096/54000 (8%)] Loss: -444847.125000\n",
      "Train Epoch: 161 [8192/54000 (15%)] Loss: -437293.250000\n",
      "Train Epoch: 161 [12288/54000 (23%)] Loss: -437084.500000\n",
      "Train Epoch: 161 [16384/54000 (30%)] Loss: -438492.000000\n",
      "Train Epoch: 161 [20480/54000 (38%)] Loss: -459281.781250\n",
      "Train Epoch: 161 [24576/54000 (46%)] Loss: -436522.968750\n",
      "Train Epoch: 161 [28672/54000 (53%)] Loss: -497328.312500\n",
      "Train Epoch: 161 [32768/54000 (61%)] Loss: -446138.187500\n",
      "Train Epoch: 161 [36864/54000 (68%)] Loss: -462457.781250\n",
      "Train Epoch: 161 [40960/54000 (76%)] Loss: -447873.812500\n",
      "Train Epoch: 161 [45056/54000 (83%)] Loss: -457956.312500\n",
      "Train Epoch: 161 [49152/54000 (91%)] Loss: -496843.250000\n",
      "    epoch          : 161\n",
      "    loss           : -452976.9917682927\n",
      "    val_loss       : -459021.17890625\n",
      "Train Epoch: 162 [0/54000 (0%)] Loss: -444993.625000\n",
      "Train Epoch: 162 [4096/54000 (8%)] Loss: -435621.625000\n",
      "Train Epoch: 162 [8192/54000 (15%)] Loss: -458610.031250\n",
      "Train Epoch: 162 [12288/54000 (23%)] Loss: -454672.656250\n",
      "Train Epoch: 162 [16384/54000 (30%)] Loss: -444510.812500\n",
      "Train Epoch: 162 [20480/54000 (38%)] Loss: -459001.125000\n",
      "Train Epoch: 162 [24576/54000 (46%)] Loss: -449292.500000\n",
      "Train Epoch: 162 [28672/54000 (53%)] Loss: -443319.156250\n",
      "Train Epoch: 162 [32768/54000 (61%)] Loss: -435117.750000\n",
      "Train Epoch: 162 [36864/54000 (68%)] Loss: -444950.875000\n",
      "Train Epoch: 162 [40960/54000 (76%)] Loss: -448256.906250\n",
      "Train Epoch: 162 [45056/54000 (83%)] Loss: -461475.281250\n",
      "Train Epoch: 162 [49152/54000 (91%)] Loss: -443870.156250\n",
      "    epoch          : 162\n",
      "    loss           : -452889.75640243903\n",
      "    val_loss       : -459186.5078125\n",
      "Train Epoch: 163 [0/54000 (0%)] Loss: -460618.031250\n",
      "Train Epoch: 163 [4096/54000 (8%)] Loss: -437472.906250\n",
      "Train Epoch: 163 [8192/54000 (15%)] Loss: -444599.281250\n",
      "Train Epoch: 163 [12288/54000 (23%)] Loss: -440070.312500\n",
      "Train Epoch: 163 [16384/54000 (30%)] Loss: -444504.593750\n",
      "Train Epoch: 163 [20480/54000 (38%)] Loss: -461155.468750\n",
      "Train Epoch: 163 [24576/54000 (46%)] Loss: -437210.906250\n",
      "Train Epoch: 163 [28672/54000 (53%)] Loss: -498389.750000\n",
      "Train Epoch: 163 [32768/54000 (61%)] Loss: -444274.375000\n",
      "Train Epoch: 163 [36864/54000 (68%)] Loss: -434088.343750\n",
      "Train Epoch: 163 [40960/54000 (76%)] Loss: -443451.781250\n",
      "Train Epoch: 163 [45056/54000 (83%)] Loss: -439567.312500\n",
      "Train Epoch: 163 [49152/54000 (91%)] Loss: -498001.812500\n",
      "    epoch          : 163\n",
      "    loss           : -453262.2667682927\n",
      "    val_loss       : -458726.9765625\n",
      "Train Epoch: 164 [0/54000 (0%)] Loss: -497034.375000\n",
      "Train Epoch: 164 [4096/54000 (8%)] Loss: -436760.187500\n",
      "Train Epoch: 164 [8192/54000 (15%)] Loss: -445138.406250\n",
      "Train Epoch: 164 [12288/54000 (23%)] Loss: -436618.312500\n",
      "Train Epoch: 164 [16384/54000 (30%)] Loss: -462973.843750\n",
      "Train Epoch: 164 [20480/54000 (38%)] Loss: -448182.562500\n",
      "Train Epoch: 164 [24576/54000 (46%)] Loss: -434775.187500\n",
      "Train Epoch: 164 [28672/54000 (53%)] Loss: -446135.156250\n",
      "Train Epoch: 164 [32768/54000 (61%)] Loss: -457676.187500\n",
      "Train Epoch: 164 [36864/54000 (68%)] Loss: -435750.406250\n",
      "Train Epoch: 164 [40960/54000 (76%)] Loss: -445112.062500\n",
      "Train Epoch: 164 [45056/54000 (83%)] Loss: -436352.500000\n",
      "Train Epoch: 164 [49152/54000 (91%)] Loss: -497768.187500\n",
      "    epoch          : 164\n",
      "    loss           : -453324.9207317073\n",
      "    val_loss       : -459090.81640625\n",
      "Train Epoch: 165 [0/54000 (0%)] Loss: -498326.937500\n",
      "Train Epoch: 165 [4096/54000 (8%)] Loss: -440293.250000\n",
      "Train Epoch: 165 [8192/54000 (15%)] Loss: -460846.187500\n",
      "Train Epoch: 165 [12288/54000 (23%)] Loss: -448615.437500\n",
      "Train Epoch: 165 [16384/54000 (30%)] Loss: -497680.250000\n",
      "Train Epoch: 165 [20480/54000 (38%)] Loss: -443122.687500\n",
      "Train Epoch: 165 [24576/54000 (46%)] Loss: -436470.500000\n",
      "Train Epoch: 165 [28672/54000 (53%)] Loss: -458117.687500\n",
      "Train Epoch: 165 [32768/54000 (61%)] Loss: -497552.062500\n",
      "Train Epoch: 165 [36864/54000 (68%)] Loss: -461738.500000\n",
      "Train Epoch: 165 [40960/54000 (76%)] Loss: -443370.437500\n",
      "Train Epoch: 165 [45056/54000 (83%)] Loss: -441230.062500\n",
      "Train Epoch: 165 [49152/54000 (91%)] Loss: -496786.687500\n",
      "    epoch          : 165\n",
      "    loss           : -453580.7661585366\n",
      "    val_loss       : -459107.63125\n",
      "Train Epoch: 166 [0/54000 (0%)] Loss: -461600.562500\n",
      "Train Epoch: 166 [4096/54000 (8%)] Loss: -438723.031250\n",
      "Train Epoch: 166 [8192/54000 (15%)] Loss: -459840.812500\n",
      "Train Epoch: 166 [12288/54000 (23%)] Loss: -497111.750000\n",
      "Train Epoch: 166 [16384/54000 (30%)] Loss: -498052.906250\n",
      "Train Epoch: 166 [20480/54000 (38%)] Loss: -460020.062500\n",
      "Train Epoch: 166 [24576/54000 (46%)] Loss: -436713.343750\n",
      "Train Epoch: 166 [28672/54000 (53%)] Loss: -445917.500000\n",
      "Train Epoch: 166 [32768/54000 (61%)] Loss: -453856.250000\n",
      "Train Epoch: 166 [36864/54000 (68%)] Loss: -436063.437500\n",
      "Train Epoch: 166 [40960/54000 (76%)] Loss: -443976.062500\n",
      "Train Epoch: 166 [45056/54000 (83%)] Loss: -441495.750000\n",
      "Train Epoch: 166 [49152/54000 (91%)] Loss: -499170.156250\n",
      "    epoch          : 166\n",
      "    loss           : -453474.2361280488\n",
      "    val_loss       : -459237.52578125\n",
      "Train Epoch: 167 [0/54000 (0%)] Loss: -498634.500000\n",
      "Train Epoch: 167 [4096/54000 (8%)] Loss: -446412.250000\n",
      "Train Epoch: 167 [8192/54000 (15%)] Loss: -446577.250000\n",
      "Train Epoch: 167 [12288/54000 (23%)] Loss: -445446.625000\n",
      "Train Epoch: 167 [16384/54000 (30%)] Loss: -440164.375000\n",
      "Train Epoch: 167 [20480/54000 (38%)] Loss: -446964.125000\n",
      "Train Epoch: 167 [24576/54000 (46%)] Loss: -453906.531250\n",
      "Train Epoch: 167 [28672/54000 (53%)] Loss: -443344.593750\n",
      "Train Epoch: 167 [32768/54000 (61%)] Loss: -436062.406250\n",
      "Train Epoch: 167 [36864/54000 (68%)] Loss: -462638.875000\n",
      "Train Epoch: 167 [40960/54000 (76%)] Loss: -441212.500000\n",
      "Train Epoch: 167 [45056/54000 (83%)] Loss: -463664.062500\n",
      "Train Epoch: 167 [49152/54000 (91%)] Loss: -497138.875000\n",
      "    epoch          : 167\n",
      "    loss           : -453729.6318597561\n",
      "    val_loss       : -459287.12890625\n",
      "Train Epoch: 168 [0/54000 (0%)] Loss: -460530.875000\n",
      "Train Epoch: 168 [4096/54000 (8%)] Loss: -436194.562500\n",
      "Train Epoch: 168 [8192/54000 (15%)] Loss: -459024.781250\n",
      "Train Epoch: 168 [12288/54000 (23%)] Loss: -440943.375000\n",
      "Train Epoch: 168 [16384/54000 (30%)] Loss: -498458.062500\n",
      "Train Epoch: 168 [20480/54000 (38%)] Loss: -457773.125000\n",
      "Train Epoch: 168 [24576/54000 (46%)] Loss: -439209.750000\n",
      "Train Epoch: 168 [28672/54000 (53%)] Loss: -447269.843750\n",
      "Train Epoch: 168 [32768/54000 (61%)] Loss: -498260.218750\n",
      "Train Epoch: 168 [36864/54000 (68%)] Loss: -498530.843750\n",
      "Train Epoch: 168 [40960/54000 (76%)] Loss: -437159.187500\n",
      "Train Epoch: 168 [45056/54000 (83%)] Loss: -441594.875000\n",
      "Train Epoch: 168 [49152/54000 (91%)] Loss: -499267.312500\n",
      "    epoch          : 168\n",
      "    loss           : -453612.0019817073\n",
      "    val_loss       : -459021.5453125\n",
      "Train Epoch: 169 [0/54000 (0%)] Loss: -462761.750000\n",
      "Train Epoch: 169 [4096/54000 (8%)] Loss: -448426.156250\n",
      "Train Epoch: 169 [8192/54000 (15%)] Loss: -436300.125000\n",
      "Train Epoch: 169 [12288/54000 (23%)] Loss: -445099.937500\n",
      "Train Epoch: 169 [16384/54000 (30%)] Loss: -460893.250000\n",
      "Train Epoch: 169 [20480/54000 (38%)] Loss: -446404.031250\n",
      "Train Epoch: 169 [24576/54000 (46%)] Loss: -449710.468750\n",
      "Train Epoch: 169 [28672/54000 (53%)] Loss: -497889.875000\n",
      "Train Epoch: 169 [32768/54000 (61%)] Loss: -493753.968750\n",
      "Train Epoch: 169 [36864/54000 (68%)] Loss: -460828.781250\n",
      "Train Epoch: 169 [40960/54000 (76%)] Loss: -445523.343750\n",
      "Train Epoch: 169 [45056/54000 (83%)] Loss: -455984.968750\n",
      "Train Epoch: 169 [49152/54000 (91%)] Loss: -497664.125000\n",
      "    epoch          : 169\n",
      "    loss           : -453835.88734756096\n",
      "    val_loss       : -459075.74765625\n",
      "Train Epoch: 170 [0/54000 (0%)] Loss: -497285.750000\n",
      "Train Epoch: 170 [4096/54000 (8%)] Loss: -454238.750000\n",
      "Train Epoch: 170 [8192/54000 (15%)] Loss: -446382.937500\n",
      "Train Epoch: 170 [12288/54000 (23%)] Loss: -446331.625000\n",
      "Train Epoch: 170 [16384/54000 (30%)] Loss: -496911.812500\n",
      "Train Epoch: 170 [20480/54000 (38%)] Loss: -459620.000000\n",
      "Train Epoch: 170 [24576/54000 (46%)] Loss: -453975.531250\n",
      "Train Epoch: 170 [28672/54000 (53%)] Loss: -498555.750000\n",
      "Train Epoch: 170 [32768/54000 (61%)] Loss: -438428.687500\n",
      "Train Epoch: 170 [36864/54000 (68%)] Loss: -439754.812500\n",
      "Train Epoch: 170 [40960/54000 (76%)] Loss: -457320.718750\n",
      "Train Epoch: 170 [45056/54000 (83%)] Loss: -462310.750000\n",
      "Train Epoch: 170 [49152/54000 (91%)] Loss: -440513.406250\n",
      "    epoch          : 170\n",
      "    loss           : -453965.900152439\n",
      "    val_loss       : -458967.5859375\n",
      "Train Epoch: 171 [0/54000 (0%)] Loss: -437581.593750\n",
      "Train Epoch: 171 [4096/54000 (8%)] Loss: -438594.937500\n",
      "Train Epoch: 171 [8192/54000 (15%)] Loss: -440780.968750\n",
      "Train Epoch: 171 [12288/54000 (23%)] Loss: -461349.281250\n",
      "Train Epoch: 171 [16384/54000 (30%)] Loss: -437940.125000\n",
      "Train Epoch: 171 [20480/54000 (38%)] Loss: -459723.625000\n",
      "Train Epoch: 171 [24576/54000 (46%)] Loss: -454141.812500\n",
      "Train Epoch: 171 [28672/54000 (53%)] Loss: -445223.187500\n",
      "Train Epoch: 171 [32768/54000 (61%)] Loss: -497510.718750\n",
      "Train Epoch: 171 [36864/54000 (68%)] Loss: -441164.000000\n",
      "Train Epoch: 171 [40960/54000 (76%)] Loss: -435985.843750\n",
      "Train Epoch: 171 [45056/54000 (83%)] Loss: -440069.250000\n",
      "Train Epoch: 171 [49152/54000 (91%)] Loss: -498666.250000\n",
      "    epoch          : 171\n",
      "    loss           : -454047.8830792683\n",
      "    val_loss       : -458927.42578125\n",
      "Train Epoch: 172 [0/54000 (0%)] Loss: -496704.875000\n",
      "Train Epoch: 172 [4096/54000 (8%)] Loss: -436267.687500\n",
      "Train Epoch: 172 [8192/54000 (15%)] Loss: -446012.531250\n",
      "Train Epoch: 172 [12288/54000 (23%)] Loss: -447978.468750\n",
      "Train Epoch: 172 [16384/54000 (30%)] Loss: -432996.593750\n",
      "Train Epoch: 172 [20480/54000 (38%)] Loss: -461277.593750\n",
      "Train Epoch: 172 [24576/54000 (46%)] Loss: -439585.031250\n",
      "Train Epoch: 172 [28672/54000 (53%)] Loss: -496144.937500\n",
      "Train Epoch: 172 [32768/54000 (61%)] Loss: -498813.468750\n",
      "Train Epoch: 172 [36864/54000 (68%)] Loss: -499166.250000\n",
      "Train Epoch: 172 [40960/54000 (76%)] Loss: -438721.625000\n",
      "Train Epoch: 172 [45056/54000 (83%)] Loss: -462324.875000\n",
      "Train Epoch: 172 [49152/54000 (91%)] Loss: -499416.250000\n",
      "    epoch          : 172\n",
      "    loss           : -453963.2431402439\n",
      "    val_loss       : -459271.12421875\n",
      "Train Epoch: 173 [0/54000 (0%)] Loss: -449199.312500\n",
      "Train Epoch: 173 [4096/54000 (8%)] Loss: -461337.968750\n",
      "Train Epoch: 173 [8192/54000 (15%)] Loss: -461898.156250\n",
      "Train Epoch: 173 [12288/54000 (23%)] Loss: -455506.593750\n",
      "Train Epoch: 173 [16384/54000 (30%)] Loss: -438990.781250\n",
      "Train Epoch: 173 [20480/54000 (38%)] Loss: -445760.406250\n",
      "Train Epoch: 173 [24576/54000 (46%)] Loss: -441738.625000\n",
      "Train Epoch: 173 [28672/54000 (53%)] Loss: -499036.625000\n",
      "Train Epoch: 173 [32768/54000 (61%)] Loss: -455771.281250\n",
      "Train Epoch: 173 [36864/54000 (68%)] Loss: -441718.687500\n",
      "Train Epoch: 173 [40960/54000 (76%)] Loss: -448513.593750\n",
      "Train Epoch: 173 [45056/54000 (83%)] Loss: -456802.031250\n",
      "Train Epoch: 173 [49152/54000 (91%)] Loss: -498514.875000\n",
      "    epoch          : 173\n",
      "    loss           : -454082.6794207317\n",
      "    val_loss       : -459116.6640625\n",
      "Train Epoch: 174 [0/54000 (0%)] Loss: -499163.218750\n",
      "Train Epoch: 174 [4096/54000 (8%)] Loss: -448590.750000\n",
      "Train Epoch: 174 [8192/54000 (15%)] Loss: -440946.750000\n",
      "Train Epoch: 174 [12288/54000 (23%)] Loss: -451083.718750\n",
      "Train Epoch: 174 [16384/54000 (30%)] Loss: -497869.781250\n",
      "Train Epoch: 174 [20480/54000 (38%)] Loss: -460994.500000\n",
      "Train Epoch: 174 [24576/54000 (46%)] Loss: -438629.750000\n",
      "Train Epoch: 174 [28672/54000 (53%)] Loss: -448195.718750\n",
      "Train Epoch: 174 [32768/54000 (61%)] Loss: -461597.625000\n",
      "Train Epoch: 174 [36864/54000 (68%)] Loss: -462351.593750\n",
      "Train Epoch: 174 [40960/54000 (76%)] Loss: -455278.312500\n",
      "Train Epoch: 174 [45056/54000 (83%)] Loss: -461541.812500\n",
      "Train Epoch: 174 [49152/54000 (91%)] Loss: -499118.437500\n",
      "    epoch          : 174\n",
      "    loss           : -454155.92088414636\n",
      "    val_loss       : -458754.465625\n",
      "Train Epoch: 175 [0/54000 (0%)] Loss: -499425.437500\n",
      "Train Epoch: 175 [4096/54000 (8%)] Loss: -439325.500000\n",
      "Train Epoch: 175 [8192/54000 (15%)] Loss: -443736.187500\n",
      "Train Epoch: 175 [12288/54000 (23%)] Loss: -441699.812500\n",
      "Train Epoch: 175 [16384/54000 (30%)] Loss: -462510.562500\n",
      "Train Epoch: 175 [20480/54000 (38%)] Loss: -459850.218750\n",
      "Train Epoch: 175 [24576/54000 (46%)] Loss: -441655.156250\n",
      "Train Epoch: 175 [28672/54000 (53%)] Loss: -446710.125000\n",
      "Train Epoch: 175 [32768/54000 (61%)] Loss: -498494.875000\n",
      "Train Epoch: 175 [36864/54000 (68%)] Loss: -460941.187500\n",
      "Train Epoch: 175 [40960/54000 (76%)] Loss: -438825.406250\n",
      "Train Epoch: 175 [45056/54000 (83%)] Loss: -441029.437500\n",
      "Train Epoch: 175 [49152/54000 (91%)] Loss: -441025.218750\n",
      "    epoch          : 175\n",
      "    loss           : -454251.0114329268\n",
      "    val_loss       : -459224.48515625\n",
      "Train Epoch: 176 [0/54000 (0%)] Loss: -459336.718750\n",
      "Train Epoch: 176 [4096/54000 (8%)] Loss: -463487.906250\n",
      "Train Epoch: 176 [8192/54000 (15%)] Loss: -448639.125000\n",
      "Train Epoch: 176 [12288/54000 (23%)] Loss: -462738.343750\n",
      "Train Epoch: 176 [16384/54000 (30%)] Loss: -447201.156250\n",
      "Train Epoch: 176 [20480/54000 (38%)] Loss: -446620.906250\n",
      "Train Epoch: 176 [24576/54000 (46%)] Loss: -446165.937500\n",
      "Train Epoch: 176 [28672/54000 (53%)] Loss: -498223.000000\n",
      "Train Epoch: 176 [32768/54000 (61%)] Loss: -496730.937500\n",
      "Train Epoch: 176 [36864/54000 (68%)] Loss: -460537.500000\n",
      "Train Epoch: 176 [40960/54000 (76%)] Loss: -445157.062500\n",
      "Train Epoch: 176 [45056/54000 (83%)] Loss: -437286.593750\n",
      "Train Epoch: 176 [49152/54000 (91%)] Loss: -499351.687500\n",
      "    epoch          : 176\n",
      "    loss           : -454402.6262195122\n",
      "    val_loss       : -459259.775\n",
      "Train Epoch: 177 [0/54000 (0%)] Loss: -499017.468750\n",
      "Train Epoch: 177 [4096/54000 (8%)] Loss: -463264.875000\n",
      "Train Epoch: 177 [8192/54000 (15%)] Loss: -464408.125000\n",
      "Train Epoch: 177 [12288/54000 (23%)] Loss: -448016.125000\n",
      "Train Epoch: 177 [16384/54000 (30%)] Loss: -456250.968750\n",
      "Train Epoch: 177 [20480/54000 (38%)] Loss: -444241.625000\n",
      "Train Epoch: 177 [24576/54000 (46%)] Loss: -439488.687500\n",
      "Train Epoch: 177 [28672/54000 (53%)] Loss: -459253.093750\n",
      "Train Epoch: 177 [32768/54000 (61%)] Loss: -439348.593750\n",
      "Train Epoch: 177 [36864/54000 (68%)] Loss: -462792.875000\n",
      "Train Epoch: 177 [40960/54000 (76%)] Loss: -463181.687500\n",
      "Train Epoch: 177 [45056/54000 (83%)] Loss: -457177.343750\n",
      "Train Epoch: 177 [49152/54000 (91%)] Loss: -496754.500000\n",
      "    epoch          : 177\n",
      "    loss           : -454496.63734756096\n",
      "    val_loss       : -459477.89921875\n",
      "Train Epoch: 178 [0/54000 (0%)] Loss: -500059.375000\n",
      "Train Epoch: 178 [4096/54000 (8%)] Loss: -440835.750000\n",
      "Train Epoch: 178 [8192/54000 (15%)] Loss: -445999.531250\n",
      "Train Epoch: 178 [12288/54000 (23%)] Loss: -439337.468750\n",
      "Train Epoch: 178 [16384/54000 (30%)] Loss: -455563.687500\n",
      "Train Epoch: 178 [20480/54000 (38%)] Loss: -460720.187500\n",
      "Train Epoch: 178 [24576/54000 (46%)] Loss: -454642.968750\n",
      "Train Epoch: 178 [28672/54000 (53%)] Loss: -497755.187500\n",
      "Train Epoch: 178 [32768/54000 (61%)] Loss: -449059.000000\n",
      "Train Epoch: 178 [36864/54000 (68%)] Loss: -443145.656250\n",
      "Train Epoch: 178 [40960/54000 (76%)] Loss: -440416.031250\n",
      "Train Epoch: 178 [45056/54000 (83%)] Loss: -459143.312500\n",
      "Train Epoch: 178 [49152/54000 (91%)] Loss: -497668.781250\n",
      "    epoch          : 178\n",
      "    loss           : -454627.6173780488\n",
      "    val_loss       : -459205.64609375\n",
      "Train Epoch: 179 [0/54000 (0%)] Loss: -445074.000000\n",
      "Train Epoch: 179 [4096/54000 (8%)] Loss: -437632.406250\n",
      "Train Epoch: 179 [8192/54000 (15%)] Loss: -460841.062500\n",
      "Train Epoch: 179 [12288/54000 (23%)] Loss: -461758.656250\n",
      "Train Epoch: 179 [16384/54000 (30%)] Loss: -442719.937500\n",
      "Train Epoch: 179 [20480/54000 (38%)] Loss: -499096.906250\n",
      "Train Epoch: 179 [24576/54000 (46%)] Loss: -440340.812500\n",
      "Train Epoch: 179 [28672/54000 (53%)] Loss: -448564.312500\n",
      "Train Epoch: 179 [32768/54000 (61%)] Loss: -456744.250000\n",
      "Train Epoch: 179 [36864/54000 (68%)] Loss: -498155.906250\n",
      "Train Epoch: 179 [40960/54000 (76%)] Loss: -451275.125000\n",
      "Train Epoch: 179 [45056/54000 (83%)] Loss: -459604.250000\n",
      "Train Epoch: 179 [49152/54000 (91%)] Loss: -498461.375000\n",
      "    epoch          : 179\n",
      "    loss           : -454718.64085365855\n",
      "    val_loss       : -459009.03984375\n",
      "Train Epoch: 180 [0/54000 (0%)] Loss: -497778.937500\n",
      "Train Epoch: 180 [4096/54000 (8%)] Loss: -436590.218750\n",
      "Train Epoch: 180 [8192/54000 (15%)] Loss: -447174.812500\n",
      "Train Epoch: 180 [12288/54000 (23%)] Loss: -462910.781250\n",
      "Train Epoch: 180 [16384/54000 (30%)] Loss: -437741.812500\n",
      "Train Epoch: 180 [20480/54000 (38%)] Loss: -443523.562500\n",
      "Train Epoch: 180 [24576/54000 (46%)] Loss: -454165.031250\n",
      "Train Epoch: 180 [28672/54000 (53%)] Loss: -499508.000000\n",
      "Train Epoch: 180 [32768/54000 (61%)] Loss: -433406.218750\n",
      "Train Epoch: 180 [36864/54000 (68%)] Loss: -460243.843750\n",
      "Train Epoch: 180 [40960/54000 (76%)] Loss: -445834.000000\n",
      "Train Epoch: 180 [45056/54000 (83%)] Loss: -438970.531250\n",
      "Train Epoch: 180 [49152/54000 (91%)] Loss: -499350.562500\n",
      "    epoch          : 180\n",
      "    loss           : -454700.23521341465\n",
      "    val_loss       : -459274.00390625\n",
      "Train Epoch: 181 [0/54000 (0%)] Loss: -498905.656250\n",
      "Train Epoch: 181 [4096/54000 (8%)] Loss: -441664.843750\n",
      "Train Epoch: 181 [8192/54000 (15%)] Loss: -445006.437500\n",
      "Train Epoch: 181 [12288/54000 (23%)] Loss: -463127.125000\n",
      "Train Epoch: 181 [16384/54000 (30%)] Loss: -455347.187500\n",
      "Train Epoch: 181 [20480/54000 (38%)] Loss: -462142.343750\n",
      "Train Epoch: 181 [24576/54000 (46%)] Loss: -440160.625000\n",
      "Train Epoch: 181 [28672/54000 (53%)] Loss: -499462.812500\n",
      "Train Epoch: 181 [32768/54000 (61%)] Loss: -448426.125000\n",
      "Train Epoch: 181 [36864/54000 (68%)] Loss: -441549.843750\n",
      "Train Epoch: 181 [40960/54000 (76%)] Loss: -443148.562500\n",
      "Train Epoch: 181 [45056/54000 (83%)] Loss: -463168.312500\n",
      "Train Epoch: 181 [49152/54000 (91%)] Loss: -500159.375000\n",
      "    epoch          : 181\n",
      "    loss           : -454930.0275914634\n",
      "    val_loss       : -459431.7234375\n",
      "Train Epoch: 182 [0/54000 (0%)] Loss: -499781.937500\n",
      "Train Epoch: 182 [4096/54000 (8%)] Loss: -440353.656250\n",
      "Train Epoch: 182 [8192/54000 (15%)] Loss: -438727.250000\n",
      "Train Epoch: 182 [12288/54000 (23%)] Loss: -462930.031250\n",
      "Train Epoch: 182 [16384/54000 (30%)] Loss: -441262.312500\n",
      "Train Epoch: 182 [20480/54000 (38%)] Loss: -449140.375000\n",
      "Train Epoch: 182 [24576/54000 (46%)] Loss: -455171.468750\n",
      "Train Epoch: 182 [28672/54000 (53%)] Loss: -446663.906250\n",
      "Train Epoch: 182 [32768/54000 (61%)] Loss: -446116.875000\n",
      "Train Epoch: 182 [36864/54000 (68%)] Loss: -442562.937500\n",
      "Train Epoch: 182 [40960/54000 (76%)] Loss: -459842.250000\n",
      "Train Epoch: 182 [45056/54000 (83%)] Loss: -461898.312500\n",
      "Train Epoch: 182 [49152/54000 (91%)] Loss: -498751.875000\n",
      "    epoch          : 182\n",
      "    loss           : -454881.21402439027\n",
      "    val_loss       : -458913.371875\n",
      "Train Epoch: 183 [0/54000 (0%)] Loss: -498627.468750\n",
      "Train Epoch: 183 [4096/54000 (8%)] Loss: -454299.312500\n",
      "Train Epoch: 183 [8192/54000 (15%)] Loss: -447315.312500\n",
      "Train Epoch: 183 [12288/54000 (23%)] Loss: -500187.750000\n",
      "Train Epoch: 183 [16384/54000 (30%)] Loss: -497643.437500\n",
      "Train Epoch: 183 [20480/54000 (38%)] Loss: -461621.812500\n",
      "Train Epoch: 183 [24576/54000 (46%)] Loss: -462745.937500\n",
      "Train Epoch: 183 [28672/54000 (53%)] Loss: -446675.187500\n",
      "Train Epoch: 183 [32768/54000 (61%)] Loss: -499828.281250\n",
      "Train Epoch: 183 [36864/54000 (68%)] Loss: -500505.687500\n",
      "Train Epoch: 183 [40960/54000 (76%)] Loss: -447762.687500\n",
      "Train Epoch: 183 [45056/54000 (83%)] Loss: -460640.187500\n",
      "Train Epoch: 183 [49152/54000 (91%)] Loss: -499445.375000\n",
      "    epoch          : 183\n",
      "    loss           : -455210.61981707317\n",
      "    val_loss       : -459264.08515625\n",
      "Train Epoch: 184 [0/54000 (0%)] Loss: -498287.750000\n",
      "Train Epoch: 184 [4096/54000 (8%)] Loss: -442685.625000\n",
      "Train Epoch: 184 [8192/54000 (15%)] Loss: -458698.875000\n",
      "Train Epoch: 184 [12288/54000 (23%)] Loss: -440900.375000\n",
      "Train Epoch: 184 [16384/54000 (30%)] Loss: -448622.375000\n",
      "Train Epoch: 184 [20480/54000 (38%)] Loss: -452486.156250\n",
      "Train Epoch: 184 [24576/54000 (46%)] Loss: -439837.250000\n",
      "Train Epoch: 184 [28672/54000 (53%)] Loss: -498376.062500\n",
      "Train Epoch: 184 [32768/54000 (61%)] Loss: -499046.375000\n",
      "Train Epoch: 184 [36864/54000 (68%)] Loss: -499445.937500\n",
      "Train Epoch: 184 [40960/54000 (76%)] Loss: -440254.156250\n",
      "Train Epoch: 184 [45056/54000 (83%)] Loss: -441883.125000\n",
      "Train Epoch: 184 [49152/54000 (91%)] Loss: -450519.812500\n",
      "    epoch          : 184\n",
      "    loss           : -455178.3074695122\n",
      "    val_loss       : -459288.71171875\n",
      "Train Epoch: 185 [0/54000 (0%)] Loss: -457361.000000\n",
      "Train Epoch: 185 [4096/54000 (8%)] Loss: -447740.312500\n",
      "Train Epoch: 185 [8192/54000 (15%)] Loss: -454059.093750\n",
      "Train Epoch: 185 [12288/54000 (23%)] Loss: -440168.875000\n",
      "Train Epoch: 185 [16384/54000 (30%)] Loss: -456242.343750\n",
      "Train Epoch: 185 [20480/54000 (38%)] Loss: -462216.625000\n",
      "Train Epoch: 185 [24576/54000 (46%)] Loss: -458708.062500\n",
      "Train Epoch: 185 [28672/54000 (53%)] Loss: -499916.781250\n",
      "Train Epoch: 185 [32768/54000 (61%)] Loss: -499565.625000\n",
      "Train Epoch: 185 [36864/54000 (68%)] Loss: -499834.375000\n",
      "Train Epoch: 185 [40960/54000 (76%)] Loss: -461138.312500\n",
      "Train Epoch: 185 [45056/54000 (83%)] Loss: -463415.312500\n",
      "Train Epoch: 185 [49152/54000 (91%)] Loss: -441182.937500\n",
      "    epoch          : 185\n",
      "    loss           : -455302.00152439025\n",
      "    val_loss       : -459679.58515625\n",
      "Train Epoch: 186 [0/54000 (0%)] Loss: -498710.375000\n",
      "Train Epoch: 186 [4096/54000 (8%)] Loss: -454986.468750\n",
      "Train Epoch: 186 [8192/54000 (15%)] Loss: -441648.406250\n",
      "Train Epoch: 186 [12288/54000 (23%)] Loss: -499305.531250\n",
      "Train Epoch: 186 [16384/54000 (30%)] Loss: -460022.531250\n",
      "Train Epoch: 186 [20480/54000 (38%)] Loss: -450915.937500\n",
      "Train Epoch: 186 [24576/54000 (46%)] Loss: -460235.031250\n",
      "Train Epoch: 186 [28672/54000 (53%)] Loss: -461112.562500\n",
      "Train Epoch: 186 [32768/54000 (61%)] Loss: -448932.781250\n",
      "Train Epoch: 186 [36864/54000 (68%)] Loss: -463774.312500\n",
      "Train Epoch: 186 [40960/54000 (76%)] Loss: -447869.656250\n",
      "Train Epoch: 186 [45056/54000 (83%)] Loss: -462802.281250\n",
      "Train Epoch: 186 [49152/54000 (91%)] Loss: -498334.875000\n",
      "    epoch          : 186\n",
      "    loss           : -455255.7342987805\n",
      "    val_loss       : -459671.66953125\n",
      "Train Epoch: 187 [0/54000 (0%)] Loss: -462703.625000\n",
      "Train Epoch: 187 [4096/54000 (8%)] Loss: -446568.031250\n",
      "Train Epoch: 187 [8192/54000 (15%)] Loss: -462869.687500\n",
      "Train Epoch: 187 [12288/54000 (23%)] Loss: -456049.875000\n",
      "Train Epoch: 187 [16384/54000 (30%)] Loss: -463487.062500\n",
      "Train Epoch: 187 [20480/54000 (38%)] Loss: -461046.562500\n",
      "Train Epoch: 187 [24576/54000 (46%)] Loss: -440981.625000\n",
      "Train Epoch: 187 [28672/54000 (53%)] Loss: -441022.562500\n",
      "Train Epoch: 187 [32768/54000 (61%)] Loss: -462932.625000\n",
      "Train Epoch: 187 [36864/54000 (68%)] Loss: -498599.875000\n",
      "Train Epoch: 187 [40960/54000 (76%)] Loss: -455590.000000\n",
      "Train Epoch: 187 [45056/54000 (83%)] Loss: -441695.593750\n",
      "Train Epoch: 187 [49152/54000 (91%)] Loss: -499312.000000\n",
      "    epoch          : 187\n",
      "    loss           : -455475.9923780488\n",
      "    val_loss       : -459405.71484375\n",
      "Train Epoch: 188 [0/54000 (0%)] Loss: -500217.406250\n",
      "Train Epoch: 188 [4096/54000 (8%)] Loss: -464496.625000\n",
      "Train Epoch: 188 [8192/54000 (15%)] Loss: -446829.093750\n",
      "Train Epoch: 188 [12288/54000 (23%)] Loss: -499024.156250\n",
      "Train Epoch: 188 [16384/54000 (30%)] Loss: -441489.125000\n",
      "Train Epoch: 188 [20480/54000 (38%)] Loss: -461436.781250\n",
      "Train Epoch: 188 [24576/54000 (46%)] Loss: -437956.187500\n",
      "Train Epoch: 188 [28672/54000 (53%)] Loss: -497678.187500\n",
      "Train Epoch: 188 [32768/54000 (61%)] Loss: -459213.437500\n",
      "Train Epoch: 188 [36864/54000 (68%)] Loss: -432358.250000\n",
      "Train Epoch: 188 [40960/54000 (76%)] Loss: -447489.250000\n",
      "Train Epoch: 188 [45056/54000 (83%)] Loss: -443109.250000\n",
      "Train Epoch: 188 [49152/54000 (91%)] Loss: -499635.375000\n",
      "    epoch          : 188\n",
      "    loss           : -455196.47850609757\n",
      "    val_loss       : -459268.17265625\n",
      "Train Epoch: 189 [0/54000 (0%)] Loss: -461935.062500\n",
      "Train Epoch: 189 [4096/54000 (8%)] Loss: -439967.218750\n",
      "Train Epoch: 189 [8192/54000 (15%)] Loss: -444590.906250\n",
      "Train Epoch: 189 [12288/54000 (23%)] Loss: -453682.843750\n",
      "Train Epoch: 189 [16384/54000 (30%)] Loss: -438158.500000\n",
      "Train Epoch: 189 [20480/54000 (38%)] Loss: -462345.312500\n",
      "Train Epoch: 189 [24576/54000 (46%)] Loss: -443041.000000\n",
      "Train Epoch: 189 [28672/54000 (53%)] Loss: -499513.968750\n",
      "Train Epoch: 189 [32768/54000 (61%)] Loss: -499058.750000\n",
      "Train Epoch: 189 [36864/54000 (68%)] Loss: -449389.812500\n",
      "Train Epoch: 189 [40960/54000 (76%)] Loss: -463536.437500\n",
      "Train Epoch: 189 [45056/54000 (83%)] Loss: -460965.750000\n",
      "Train Epoch: 189 [49152/54000 (91%)] Loss: -500243.968750\n",
      "    epoch          : 189\n",
      "    loss           : -455596.58079268294\n",
      "    val_loss       : -459957.5921875\n",
      "Train Epoch: 190 [0/54000 (0%)] Loss: -461942.437500\n",
      "Train Epoch: 190 [4096/54000 (8%)] Loss: -445563.187500\n",
      "Train Epoch: 190 [8192/54000 (15%)] Loss: -448715.000000\n",
      "Train Epoch: 190 [12288/54000 (23%)] Loss: -456964.156250\n",
      "Train Epoch: 190 [16384/54000 (30%)] Loss: -463695.906250\n",
      "Train Epoch: 190 [20480/54000 (38%)] Loss: -457676.375000\n",
      "Train Epoch: 190 [24576/54000 (46%)] Loss: -439991.156250\n",
      "Train Epoch: 190 [28672/54000 (53%)] Loss: -500304.062500\n",
      "Train Epoch: 190 [32768/54000 (61%)] Loss: -448677.531250\n",
      "Train Epoch: 190 [36864/54000 (68%)] Loss: -463415.187500\n",
      "Train Epoch: 190 [40960/54000 (76%)] Loss: -440447.593750\n",
      "Train Epoch: 190 [45056/54000 (83%)] Loss: -462350.187500\n",
      "Train Epoch: 190 [49152/54000 (91%)] Loss: -499309.343750\n",
      "    epoch          : 190\n",
      "    loss           : -455712.206097561\n",
      "    val_loss       : -459148.07890625\n",
      "Train Epoch: 191 [0/54000 (0%)] Loss: -457417.437500\n",
      "Train Epoch: 191 [4096/54000 (8%)] Loss: -448882.156250\n",
      "Train Epoch: 191 [8192/54000 (15%)] Loss: -443679.750000\n",
      "Train Epoch: 191 [12288/54000 (23%)] Loss: -441019.656250\n",
      "Train Epoch: 191 [16384/54000 (30%)] Loss: -449187.312500\n",
      "Train Epoch: 191 [20480/54000 (38%)] Loss: -461356.625000\n",
      "Train Epoch: 191 [24576/54000 (46%)] Loss: -461699.312500\n",
      "Train Epoch: 191 [28672/54000 (53%)] Loss: -445997.937500\n",
      "Train Epoch: 191 [32768/54000 (61%)] Loss: -441758.375000\n",
      "Train Epoch: 191 [36864/54000 (68%)] Loss: -449714.312500\n",
      "Train Epoch: 191 [40960/54000 (76%)] Loss: -450177.875000\n",
      "Train Epoch: 191 [45056/54000 (83%)] Loss: -462717.093750\n",
      "Train Epoch: 191 [49152/54000 (91%)] Loss: -440721.187500\n",
      "    epoch          : 191\n",
      "    loss           : -455767.08201219514\n",
      "    val_loss       : -459466.7734375\n",
      "Train Epoch: 192 [0/54000 (0%)] Loss: -498657.000000\n",
      "Train Epoch: 192 [4096/54000 (8%)] Loss: -463559.437500\n",
      "Train Epoch: 192 [8192/54000 (15%)] Loss: -463219.968750\n",
      "Train Epoch: 192 [12288/54000 (23%)] Loss: -439188.187500\n",
      "Train Epoch: 192 [16384/54000 (30%)] Loss: -457180.781250\n",
      "Train Epoch: 192 [20480/54000 (38%)] Loss: -448944.312500\n",
      "Train Epoch: 192 [24576/54000 (46%)] Loss: -453859.250000\n",
      "Train Epoch: 192 [28672/54000 (53%)] Loss: -441864.875000\n",
      "Train Epoch: 192 [32768/54000 (61%)] Loss: -449101.687500\n",
      "Train Epoch: 192 [36864/54000 (68%)] Loss: -440253.968750\n",
      "Train Epoch: 192 [40960/54000 (76%)] Loss: -437590.593750\n",
      "Train Epoch: 192 [45056/54000 (83%)] Loss: -464210.343750\n",
      "Train Epoch: 192 [49152/54000 (91%)] Loss: -499195.125000\n",
      "    epoch          : 192\n",
      "    loss           : -455814.5544207317\n",
      "    val_loss       : -459282.721875\n",
      "Train Epoch: 193 [0/54000 (0%)] Loss: -499328.250000\n",
      "Train Epoch: 193 [4096/54000 (8%)] Loss: -448765.750000\n",
      "Train Epoch: 193 [8192/54000 (15%)] Loss: -441618.093750\n",
      "Train Epoch: 193 [12288/54000 (23%)] Loss: -462627.718750\n",
      "Train Epoch: 193 [16384/54000 (30%)] Loss: -442547.281250\n",
      "Train Epoch: 193 [20480/54000 (38%)] Loss: -464525.125000\n",
      "Train Epoch: 193 [24576/54000 (46%)] Loss: -449056.187500\n",
      "Train Epoch: 193 [28672/54000 (53%)] Loss: -446224.125000\n",
      "Train Epoch: 193 [32768/54000 (61%)] Loss: -457578.718750\n",
      "Train Epoch: 193 [36864/54000 (68%)] Loss: -500440.812500\n",
      "Train Epoch: 193 [40960/54000 (76%)] Loss: -449879.500000\n",
      "Train Epoch: 193 [45056/54000 (83%)] Loss: -462621.375000\n",
      "Train Epoch: 193 [49152/54000 (91%)] Loss: -497532.812500\n",
      "    epoch          : 193\n",
      "    loss           : -455745.7769817073\n",
      "    val_loss       : -459443.5625\n",
      "Train Epoch: 194 [0/54000 (0%)] Loss: -498080.687500\n",
      "Train Epoch: 194 [4096/54000 (8%)] Loss: -440031.656250\n",
      "Train Epoch: 194 [8192/54000 (15%)] Loss: -447619.500000\n",
      "Train Epoch: 194 [12288/54000 (23%)] Loss: -497411.968750\n",
      "Train Epoch: 194 [16384/54000 (30%)] Loss: -442291.750000\n",
      "Train Epoch: 194 [20480/54000 (38%)] Loss: -446527.968750\n",
      "Train Epoch: 194 [24576/54000 (46%)] Loss: -464367.625000\n",
      "Train Epoch: 194 [28672/54000 (53%)] Loss: -499618.343750\n",
      "Train Epoch: 194 [32768/54000 (61%)] Loss: -440155.281250\n",
      "Train Epoch: 194 [36864/54000 (68%)] Loss: -498500.500000\n",
      "Train Epoch: 194 [40960/54000 (76%)] Loss: -450593.906250\n",
      "Train Epoch: 194 [45056/54000 (83%)] Loss: -462560.656250\n",
      "Train Epoch: 194 [49152/54000 (91%)] Loss: -500442.562500\n",
      "    epoch          : 194\n",
      "    loss           : -455836.5205792683\n",
      "    val_loss       : -459822.2328125\n",
      "Train Epoch: 195 [0/54000 (0%)] Loss: -500370.812500\n",
      "Train Epoch: 195 [4096/54000 (8%)] Loss: -447502.281250\n",
      "Train Epoch: 195 [8192/54000 (15%)] Loss: -448231.562500\n",
      "Train Epoch: 195 [12288/54000 (23%)] Loss: -441303.687500\n",
      "Train Epoch: 195 [16384/54000 (30%)] Loss: -500273.625000\n",
      "Train Epoch: 195 [20480/54000 (38%)] Loss: -459227.562500\n",
      "Train Epoch: 195 [24576/54000 (46%)] Loss: -464269.125000\n",
      "Train Epoch: 195 [28672/54000 (53%)] Loss: -448175.250000\n",
      "Train Epoch: 195 [32768/54000 (61%)] Loss: -441402.750000\n",
      "Train Epoch: 195 [36864/54000 (68%)] Loss: -464507.875000\n",
      "Train Epoch: 195 [40960/54000 (76%)] Loss: -446010.875000\n",
      "Train Epoch: 195 [45056/54000 (83%)] Loss: -441943.781250\n",
      "Train Epoch: 195 [49152/54000 (91%)] Loss: -498098.750000\n",
      "    epoch          : 195\n",
      "    loss           : -455831.2925304878\n",
      "    val_loss       : -459688.53125\n",
      "Train Epoch: 196 [0/54000 (0%)] Loss: -500099.031250\n",
      "Train Epoch: 196 [4096/54000 (8%)] Loss: -439879.625000\n",
      "Train Epoch: 196 [8192/54000 (15%)] Loss: -447855.593750\n",
      "Train Epoch: 196 [12288/54000 (23%)] Loss: -499571.625000\n",
      "Train Epoch: 196 [16384/54000 (30%)] Loss: -441750.468750\n",
      "Train Epoch: 196 [20480/54000 (38%)] Loss: -447868.812500\n",
      "Train Epoch: 196 [24576/54000 (46%)] Loss: -443803.343750\n",
      "Train Epoch: 196 [28672/54000 (53%)] Loss: -461924.281250\n",
      "Train Epoch: 196 [32768/54000 (61%)] Loss: -442023.562500\n",
      "Train Epoch: 196 [36864/54000 (68%)] Loss: -438391.312500\n",
      "Train Epoch: 196 [40960/54000 (76%)] Loss: -460737.500000\n",
      "Train Epoch: 196 [45056/54000 (83%)] Loss: -457373.375000\n",
      "Train Epoch: 196 [49152/54000 (91%)] Loss: -500518.968750\n",
      "    epoch          : 196\n",
      "    loss           : -455948.1798780488\n",
      "    val_loss       : -459515.77890625\n",
      "Train Epoch: 197 [0/54000 (0%)] Loss: -499107.375000\n",
      "Train Epoch: 197 [4096/54000 (8%)] Loss: -462758.031250\n",
      "Train Epoch: 197 [8192/54000 (15%)] Loss: -439265.812500\n",
      "Train Epoch: 197 [12288/54000 (23%)] Loss: -450296.968750\n",
      "Train Epoch: 197 [16384/54000 (30%)] Loss: -464641.906250\n",
      "Train Epoch: 197 [20480/54000 (38%)] Loss: -460704.875000\n",
      "Train Epoch: 197 [24576/54000 (46%)] Loss: -455940.500000\n",
      "Train Epoch: 197 [28672/54000 (53%)] Loss: -449035.812500\n",
      "Train Epoch: 197 [32768/54000 (61%)] Loss: -443198.562500\n",
      "Train Epoch: 197 [36864/54000 (68%)] Loss: -498741.125000\n",
      "Train Epoch: 197 [40960/54000 (76%)] Loss: -444344.687500\n",
      "Train Epoch: 197 [45056/54000 (83%)] Loss: -443146.031250\n",
      "Train Epoch: 197 [49152/54000 (91%)] Loss: -499676.312500\n",
      "    epoch          : 197\n",
      "    loss           : -456105.9044207317\n",
      "    val_loss       : -459434.69453125\n",
      "Train Epoch: 198 [0/54000 (0%)] Loss: -450223.406250\n",
      "Train Epoch: 198 [4096/54000 (8%)] Loss: -447666.593750\n",
      "Train Epoch: 198 [8192/54000 (15%)] Loss: -440516.062500\n",
      "Train Epoch: 198 [12288/54000 (23%)] Loss: -449315.781250\n",
      "Train Epoch: 198 [16384/54000 (30%)] Loss: -443136.687500\n",
      "Train Epoch: 198 [20480/54000 (38%)] Loss: -461002.343750\n",
      "Train Epoch: 198 [24576/54000 (46%)] Loss: -462607.812500\n",
      "Train Epoch: 198 [28672/54000 (53%)] Loss: -499070.250000\n",
      "Train Epoch: 198 [32768/54000 (61%)] Loss: -440574.875000\n",
      "Train Epoch: 198 [36864/54000 (68%)] Loss: -461311.781250\n",
      "Train Epoch: 198 [40960/54000 (76%)] Loss: -443321.250000\n",
      "Train Epoch: 198 [45056/54000 (83%)] Loss: -460798.281250\n",
      "Train Epoch: 198 [49152/54000 (91%)] Loss: -500236.500000\n",
      "    epoch          : 198\n",
      "    loss           : -456221.0504573171\n",
      "    val_loss       : -459344.69453125\n",
      "Train Epoch: 199 [0/54000 (0%)] Loss: -498549.843750\n",
      "Train Epoch: 199 [4096/54000 (8%)] Loss: -462232.125000\n",
      "Train Epoch: 199 [8192/54000 (15%)] Loss: -451259.718750\n",
      "Train Epoch: 199 [12288/54000 (23%)] Loss: -499944.562500\n",
      "Train Epoch: 199 [16384/54000 (30%)] Loss: -457070.125000\n",
      "Train Epoch: 199 [20480/54000 (38%)] Loss: -461747.250000\n",
      "Train Epoch: 199 [24576/54000 (46%)] Loss: -459266.781250\n",
      "Train Epoch: 199 [28672/54000 (53%)] Loss: -447500.375000\n",
      "Train Epoch: 199 [32768/54000 (61%)] Loss: -457683.437500\n",
      "Train Epoch: 199 [36864/54000 (68%)] Loss: -441642.312500\n",
      "Train Epoch: 199 [40960/54000 (76%)] Loss: -450381.562500\n",
      "Train Epoch: 199 [45056/54000 (83%)] Loss: -441647.500000\n",
      "Train Epoch: 199 [49152/54000 (91%)] Loss: -456345.343750\n",
      "    epoch          : 199\n",
      "    loss           : -456282.36448170734\n",
      "    val_loss       : -459395.48671875\n",
      "Train Epoch: 200 [0/54000 (0%)] Loss: -443118.062500\n",
      "Train Epoch: 200 [4096/54000 (8%)] Loss: -450367.843750\n",
      "Train Epoch: 200 [8192/54000 (15%)] Loss: -440345.000000\n",
      "Train Epoch: 200 [12288/54000 (23%)] Loss: -436283.500000\n",
      "Train Epoch: 200 [16384/54000 (30%)] Loss: -441296.406250\n",
      "Train Epoch: 200 [20480/54000 (38%)] Loss: -460439.750000\n",
      "Train Epoch: 200 [24576/54000 (46%)] Loss: -442956.500000\n",
      "Train Epoch: 200 [28672/54000 (53%)] Loss: -449584.656250\n",
      "Train Epoch: 200 [32768/54000 (61%)] Loss: -455975.781250\n",
      "Train Epoch: 200 [36864/54000 (68%)] Loss: -440401.125000\n",
      "Train Epoch: 200 [40960/54000 (76%)] Loss: -445915.625000\n",
      "Train Epoch: 200 [45056/54000 (83%)] Loss: -459650.531250\n",
      "Train Epoch: 200 [49152/54000 (91%)] Loss: -496822.250000\n",
      "    epoch          : 200\n",
      "    loss           : -456218.593902439\n",
      "    val_loss       : -459723.73203125\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0510_004522/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [0/54000 (0%)] Loss: -497667.593750\n",
      "Train Epoch: 201 [4096/54000 (8%)] Loss: -441110.125000\n",
      "Train Epoch: 201 [8192/54000 (15%)] Loss: -443500.000000\n",
      "Train Epoch: 201 [12288/54000 (23%)] Loss: -457469.468750\n",
      "Train Epoch: 201 [16384/54000 (30%)] Loss: -442810.968750\n",
      "Train Epoch: 201 [20480/54000 (38%)] Loss: -461953.343750\n",
      "Train Epoch: 201 [24576/54000 (46%)] Loss: -443129.656250\n",
      "Train Epoch: 201 [28672/54000 (53%)] Loss: -447274.468750\n",
      "Train Epoch: 201 [32768/54000 (61%)] Loss: -500266.250000\n",
      "Train Epoch: 201 [36864/54000 (68%)] Loss: -497384.937500\n",
      "Train Epoch: 201 [40960/54000 (76%)] Loss: -453646.281250\n",
      "Train Epoch: 201 [45056/54000 (83%)] Loss: -463627.000000\n",
      "Train Epoch: 201 [49152/54000 (91%)] Loss: -447521.562500\n",
      "    epoch          : 201\n",
      "    loss           : -456201.2614329268\n",
      "    val_loss       : -459780.99921875\n",
      "Train Epoch: 202 [0/54000 (0%)] Loss: -443076.250000\n",
      "Train Epoch: 202 [4096/54000 (8%)] Loss: -463675.281250\n",
      "Train Epoch: 202 [8192/54000 (15%)] Loss: -443167.000000\n",
      "Train Epoch: 202 [12288/54000 (23%)] Loss: -451745.437500\n",
      "Train Epoch: 202 [16384/54000 (30%)] Loss: -440286.062500\n",
      "Train Epoch: 202 [20480/54000 (38%)] Loss: -461849.687500\n",
      "Train Epoch: 202 [24576/54000 (46%)] Loss: -448226.937500\n",
      "Train Epoch: 202 [28672/54000 (53%)] Loss: -500241.343750\n",
      "Train Epoch: 202 [32768/54000 (61%)] Loss: -442895.500000\n",
      "Train Epoch: 202 [36864/54000 (68%)] Loss: -444637.093750\n",
      "Train Epoch: 202 [40960/54000 (76%)] Loss: -453334.718750\n",
      "Train Epoch: 202 [45056/54000 (83%)] Loss: -439817.312500\n",
      "Train Epoch: 202 [49152/54000 (91%)] Loss: -496553.156250\n",
      "    epoch          : 202\n",
      "    loss           : -456416.3971036585\n",
      "    val_loss       : -459710.22578125\n",
      "Train Epoch: 203 [0/54000 (0%)] Loss: -463331.718750\n",
      "Train Epoch: 203 [4096/54000 (8%)] Loss: -455865.218750\n",
      "Train Epoch: 203 [8192/54000 (15%)] Loss: -459324.062500\n",
      "Train Epoch: 203 [12288/54000 (23%)] Loss: -450378.875000\n",
      "Train Epoch: 203 [16384/54000 (30%)] Loss: -444237.625000\n",
      "Train Epoch: 203 [20480/54000 (38%)] Loss: -450009.125000\n",
      "Train Epoch: 203 [24576/54000 (46%)] Loss: -457784.750000\n",
      "Train Epoch: 203 [28672/54000 (53%)] Loss: -461975.093750\n",
      "Train Epoch: 203 [32768/54000 (61%)] Loss: -499598.281250\n",
      "Train Epoch: 203 [36864/54000 (68%)] Loss: -435131.500000\n",
      "Train Epoch: 203 [40960/54000 (76%)] Loss: -451310.312500\n",
      "Train Epoch: 203 [45056/54000 (83%)] Loss: -463744.781250\n",
      "Train Epoch: 203 [49152/54000 (91%)] Loss: -499511.000000\n",
      "    epoch          : 203\n",
      "    loss           : -456455.70335365855\n",
      "    val_loss       : -459724.41953125\n",
      "Train Epoch: 204 [0/54000 (0%)] Loss: -499044.500000\n",
      "Train Epoch: 204 [4096/54000 (8%)] Loss: -448441.687500\n",
      "Train Epoch: 204 [8192/54000 (15%)] Loss: -448895.062500\n",
      "Train Epoch: 204 [12288/54000 (23%)] Loss: -442907.312500\n",
      "Train Epoch: 204 [16384/54000 (30%)] Loss: -441065.281250\n",
      "Train Epoch: 204 [20480/54000 (38%)] Loss: -499729.593750\n",
      "Train Epoch: 204 [24576/54000 (46%)] Loss: -457257.593750\n",
      "Train Epoch: 204 [28672/54000 (53%)] Loss: -448115.875000\n",
      "Train Epoch: 204 [32768/54000 (61%)] Loss: -458253.281250\n",
      "Train Epoch: 204 [36864/54000 (68%)] Loss: -439790.625000\n",
      "Train Epoch: 204 [40960/54000 (76%)] Loss: -442336.968750\n",
      "Train Epoch: 204 [45056/54000 (83%)] Loss: -461618.187500\n",
      "Train Epoch: 204 [49152/54000 (91%)] Loss: -499846.656250\n",
      "    epoch          : 204\n",
      "    loss           : -456435.1262195122\n",
      "    val_loss       : -459163.5515625\n",
      "Train Epoch: 205 [0/54000 (0%)] Loss: -460138.750000\n",
      "Train Epoch: 205 [4096/54000 (8%)] Loss: -454071.968750\n",
      "Train Epoch: 205 [8192/54000 (15%)] Loss: -463653.218750\n",
      "Train Epoch: 205 [12288/54000 (23%)] Loss: -449914.937500\n",
      "Train Epoch: 205 [16384/54000 (30%)] Loss: -448850.812500\n",
      "Train Epoch: 205 [20480/54000 (38%)] Loss: -452906.500000\n",
      "Train Epoch: 205 [24576/54000 (46%)] Loss: -464881.312500\n",
      "Train Epoch: 205 [28672/54000 (53%)] Loss: -453742.375000\n",
      "Train Epoch: 205 [32768/54000 (61%)] Loss: -459938.812500\n",
      "Train Epoch: 205 [36864/54000 (68%)] Loss: -443272.750000\n",
      "Train Epoch: 205 [40960/54000 (76%)] Loss: -450790.000000\n",
      "Train Epoch: 205 [45056/54000 (83%)] Loss: -464858.750000\n",
      "Train Epoch: 205 [49152/54000 (91%)] Loss: -500931.562500\n",
      "    epoch          : 205\n",
      "    loss           : -456701.06219512195\n",
      "    val_loss       : -459938.878125\n",
      "Train Epoch: 206 [0/54000 (0%)] Loss: -463323.000000\n",
      "Train Epoch: 206 [4096/54000 (8%)] Loss: -456564.406250\n",
      "Train Epoch: 206 [8192/54000 (15%)] Loss: -450626.968750\n",
      "Train Epoch: 206 [12288/54000 (23%)] Loss: -457479.187500\n",
      "Train Epoch: 206 [16384/54000 (30%)] Loss: -441037.468750\n",
      "Train Epoch: 206 [20480/54000 (38%)] Loss: -464428.843750\n",
      "Train Epoch: 206 [24576/54000 (46%)] Loss: -464381.312500\n",
      "Train Epoch: 206 [28672/54000 (53%)] Loss: -449398.875000\n",
      "Train Epoch: 206 [32768/54000 (61%)] Loss: -441521.500000\n",
      "Train Epoch: 206 [36864/54000 (68%)] Loss: -461590.968750\n",
      "Train Epoch: 206 [40960/54000 (76%)] Loss: -441247.937500\n",
      "Train Epoch: 206 [45056/54000 (83%)] Loss: -463985.062500\n",
      "Train Epoch: 206 [49152/54000 (91%)] Loss: -441484.968750\n",
      "    epoch          : 206\n",
      "    loss           : -456500.99024390243\n",
      "    val_loss       : -459969.20078125\n",
      "Train Epoch: 207 [0/54000 (0%)] Loss: -500852.875000\n",
      "Train Epoch: 207 [4096/54000 (8%)] Loss: -442400.125000\n",
      "Train Epoch: 207 [8192/54000 (15%)] Loss: -450348.718750\n",
      "Train Epoch: 207 [12288/54000 (23%)] Loss: -442043.375000\n",
      "Train Epoch: 207 [16384/54000 (30%)] Loss: -440262.906250\n",
      "Train Epoch: 207 [20480/54000 (38%)] Loss: -461874.437500\n",
      "Train Epoch: 207 [24576/54000 (46%)] Loss: -464411.437500\n",
      "Train Epoch: 207 [28672/54000 (53%)] Loss: -447874.937500\n",
      "Train Epoch: 207 [32768/54000 (61%)] Loss: -456409.125000\n",
      "Train Epoch: 207 [36864/54000 (68%)] Loss: -438538.968750\n",
      "Train Epoch: 207 [40960/54000 (76%)] Loss: -453871.312500\n",
      "Train Epoch: 207 [45056/54000 (83%)] Loss: -441208.000000\n",
      "Train Epoch: 207 [49152/54000 (91%)] Loss: -500457.531250\n",
      "    epoch          : 207\n",
      "    loss           : -456840.24009146343\n",
      "    val_loss       : -459854.4734375\n",
      "Train Epoch: 208 [0/54000 (0%)] Loss: -495840.468750\n",
      "Train Epoch: 208 [4096/54000 (8%)] Loss: -449642.000000\n",
      "Train Epoch: 208 [8192/54000 (15%)] Loss: -461516.937500\n",
      "Train Epoch: 208 [12288/54000 (23%)] Loss: -445409.125000\n",
      "Train Epoch: 208 [16384/54000 (30%)] Loss: -443834.593750\n",
      "Train Epoch: 208 [20480/54000 (38%)] Loss: -461806.875000\n",
      "Train Epoch: 208 [24576/54000 (46%)] Loss: -441617.218750\n",
      "Train Epoch: 208 [28672/54000 (53%)] Loss: -450525.812500\n",
      "Train Epoch: 208 [32768/54000 (61%)] Loss: -462251.156250\n",
      "Train Epoch: 208 [36864/54000 (68%)] Loss: -499505.281250\n",
      "Train Epoch: 208 [40960/54000 (76%)] Loss: -441836.312500\n",
      "Train Epoch: 208 [45056/54000 (83%)] Loss: -462899.031250\n",
      "Train Epoch: 208 [49152/54000 (91%)] Loss: -500097.843750\n",
      "    epoch          : 208\n",
      "    loss           : -456609.1338414634\n",
      "    val_loss       : -459989.7953125\n",
      "Train Epoch: 209 [0/54000 (0%)] Loss: -499953.875000\n",
      "Train Epoch: 209 [4096/54000 (8%)] Loss: -440535.187500\n",
      "Train Epoch: 209 [8192/54000 (15%)] Loss: -450455.875000\n",
      "Train Epoch: 209 [12288/54000 (23%)] Loss: -500532.812500\n",
      "Train Epoch: 209 [16384/54000 (30%)] Loss: -499662.437500\n",
      "Train Epoch: 209 [20480/54000 (38%)] Loss: -460296.375000\n",
      "Train Epoch: 209 [24576/54000 (46%)] Loss: -458830.156250\n",
      "Train Epoch: 209 [28672/54000 (53%)] Loss: -450456.312500\n",
      "Train Epoch: 209 [32768/54000 (61%)] Loss: -436794.625000\n",
      "Train Epoch: 209 [36864/54000 (68%)] Loss: -464696.843750\n",
      "Train Epoch: 209 [40960/54000 (76%)] Loss: -443038.156250\n",
      "Train Epoch: 209 [45056/54000 (83%)] Loss: -464970.718750\n",
      "Train Epoch: 209 [49152/54000 (91%)] Loss: -499729.125000\n",
      "    epoch          : 209\n",
      "    loss           : -456965.9981707317\n",
      "    val_loss       : -459870.4421875\n",
      "Train Epoch: 210 [0/54000 (0%)] Loss: -501613.000000\n",
      "Train Epoch: 210 [4096/54000 (8%)] Loss: -444327.593750\n",
      "Train Epoch: 210 [8192/54000 (15%)] Loss: -464940.875000\n",
      "Train Epoch: 210 [12288/54000 (23%)] Loss: -459201.937500\n",
      "Train Epoch: 210 [16384/54000 (30%)] Loss: -464108.250000\n",
      "Train Epoch: 210 [20480/54000 (38%)] Loss: -462879.937500\n",
      "Train Epoch: 210 [24576/54000 (46%)] Loss: -463805.781250\n",
      "Train Epoch: 210 [28672/54000 (53%)] Loss: -499912.781250\n",
      "Train Epoch: 210 [32768/54000 (61%)] Loss: -449830.187500\n",
      "Train Epoch: 210 [36864/54000 (68%)] Loss: -464588.031250\n",
      "Train Epoch: 210 [40960/54000 (76%)] Loss: -452747.656250\n",
      "Train Epoch: 210 [45056/54000 (83%)] Loss: -447302.343750\n",
      "Train Epoch: 210 [49152/54000 (91%)] Loss: -500907.625000\n",
      "    epoch          : 210\n",
      "    loss           : -456849.51158536586\n",
      "    val_loss       : -459848.00703125\n",
      "Train Epoch: 211 [0/54000 (0%)] Loss: -500370.187500\n",
      "Train Epoch: 211 [4096/54000 (8%)] Loss: -442936.250000\n",
      "Train Epoch: 211 [8192/54000 (15%)] Loss: -462735.625000\n",
      "Train Epoch: 211 [12288/54000 (23%)] Loss: -451367.875000\n",
      "Train Epoch: 211 [16384/54000 (30%)] Loss: -441838.875000\n",
      "Train Epoch: 211 [20480/54000 (38%)] Loss: -451510.687500\n",
      "Train Epoch: 211 [24576/54000 (46%)] Loss: -465177.781250\n",
      "Train Epoch: 211 [28672/54000 (53%)] Loss: -448506.062500\n",
      "Train Epoch: 211 [32768/54000 (61%)] Loss: -443376.968750\n",
      "Train Epoch: 211 [36864/54000 (68%)] Loss: -462161.531250\n",
      "Train Epoch: 211 [40960/54000 (76%)] Loss: -442119.000000\n",
      "Train Epoch: 211 [45056/54000 (83%)] Loss: -442725.687500\n",
      "Train Epoch: 211 [49152/54000 (91%)] Loss: -499110.687500\n",
      "    epoch          : 211\n",
      "    loss           : -456793.15533536585\n",
      "    val_loss       : -459133.9484375\n",
      "Train Epoch: 212 [0/54000 (0%)] Loss: -448404.218750\n",
      "Train Epoch: 212 [4096/54000 (8%)] Loss: -456264.656250\n",
      "Train Epoch: 212 [8192/54000 (15%)] Loss: -464193.875000\n",
      "Train Epoch: 212 [12288/54000 (23%)] Loss: -450169.750000\n",
      "Train Epoch: 212 [16384/54000 (30%)] Loss: -464820.531250\n",
      "Train Epoch: 212 [20480/54000 (38%)] Loss: -441496.593750\n",
      "Train Epoch: 212 [24576/54000 (46%)] Loss: -457841.562500\n",
      "Train Epoch: 212 [28672/54000 (53%)] Loss: -441646.062500\n",
      "Train Epoch: 212 [32768/54000 (61%)] Loss: -500483.531250\n",
      "Train Epoch: 212 [36864/54000 (68%)] Loss: -499862.437500\n",
      "Train Epoch: 212 [40960/54000 (76%)] Loss: -458269.625000\n",
      "Train Epoch: 212 [45056/54000 (83%)] Loss: -464861.625000\n",
      "Train Epoch: 212 [49152/54000 (91%)] Loss: -442077.437500\n",
      "    epoch          : 212\n",
      "    loss           : -457079.28018292686\n",
      "    val_loss       : -459773.58828125\n",
      "Train Epoch: 213 [0/54000 (0%)] Loss: -440877.687500\n",
      "Train Epoch: 213 [4096/54000 (8%)] Loss: -449281.656250\n",
      "Train Epoch: 213 [8192/54000 (15%)] Loss: -463832.937500\n",
      "Train Epoch: 213 [12288/54000 (23%)] Loss: -458639.343750\n",
      "Train Epoch: 213 [16384/54000 (30%)] Loss: -453466.250000\n",
      "Train Epoch: 213 [20480/54000 (38%)] Loss: -461599.000000\n",
      "Train Epoch: 213 [24576/54000 (46%)] Loss: -441562.375000\n",
      "Train Epoch: 213 [28672/54000 (53%)] Loss: -449779.375000\n",
      "Train Epoch: 213 [32768/54000 (61%)] Loss: -443503.062500\n",
      "Train Epoch: 213 [36864/54000 (68%)] Loss: -441142.343750\n",
      "Train Epoch: 213 [40960/54000 (76%)] Loss: -450047.218750\n",
      "Train Epoch: 213 [45056/54000 (83%)] Loss: -465433.500000\n",
      "Train Epoch: 213 [49152/54000 (91%)] Loss: -498563.062500\n",
      "    epoch          : 213\n",
      "    loss           : -457056.1082317073\n",
      "    val_loss       : -459778.90078125\n",
      "Train Epoch: 214 [0/54000 (0%)] Loss: -500555.937500\n",
      "Train Epoch: 214 [4096/54000 (8%)] Loss: -442389.593750\n",
      "Train Epoch: 214 [8192/54000 (15%)] Loss: -462980.843750\n",
      "Train Epoch: 214 [12288/54000 (23%)] Loss: -500258.031250\n",
      "Train Epoch: 214 [16384/54000 (30%)] Loss: -441001.187500\n",
      "Train Epoch: 214 [20480/54000 (38%)] Loss: -462982.187500\n",
      "Train Epoch: 214 [24576/54000 (46%)] Loss: -441973.312500\n",
      "Train Epoch: 214 [28672/54000 (53%)] Loss: -449324.562500\n",
      "Train Epoch: 214 [32768/54000 (61%)] Loss: -453446.843750\n",
      "Train Epoch: 214 [36864/54000 (68%)] Loss: -445067.312500\n",
      "Train Epoch: 214 [40960/54000 (76%)] Loss: -449953.218750\n",
      "Train Epoch: 214 [45056/54000 (83%)] Loss: -443602.218750\n",
      "Train Epoch: 214 [49152/54000 (91%)] Loss: -501489.687500\n",
      "    epoch          : 214\n",
      "    loss           : -457171.89207317075\n",
      "    val_loss       : -460128.04296875\n",
      "Train Epoch: 215 [0/54000 (0%)] Loss: -500398.531250\n",
      "Train Epoch: 215 [4096/54000 (8%)] Loss: -464271.750000\n",
      "Train Epoch: 215 [8192/54000 (15%)] Loss: -463036.062500\n",
      "Train Epoch: 215 [12288/54000 (23%)] Loss: -446577.468750\n",
      "Train Epoch: 215 [16384/54000 (30%)] Loss: -441735.031250\n",
      "Train Epoch: 215 [20480/54000 (38%)] Loss: -451478.312500\n",
      "Train Epoch: 215 [24576/54000 (46%)] Loss: -458092.062500\n",
      "Train Epoch: 215 [28672/54000 (53%)] Loss: -447591.781250\n",
      "Train Epoch: 215 [32768/54000 (61%)] Loss: -455552.187500\n",
      "Train Epoch: 215 [36864/54000 (68%)] Loss: -440707.687500\n",
      "Train Epoch: 215 [40960/54000 (76%)] Loss: -441660.531250\n",
      "Train Epoch: 215 [45056/54000 (83%)] Loss: -463290.906250\n",
      "Train Epoch: 215 [49152/54000 (91%)] Loss: -443040.312500\n",
      "    epoch          : 215\n",
      "    loss           : -457229.2324695122\n",
      "    val_loss       : -460150.61484375\n",
      "Train Epoch: 216 [0/54000 (0%)] Loss: -460525.906250\n",
      "Train Epoch: 216 [4096/54000 (8%)] Loss: -451102.125000\n",
      "Train Epoch: 216 [8192/54000 (15%)] Loss: -441483.937500\n",
      "Train Epoch: 216 [12288/54000 (23%)] Loss: -457474.375000\n",
      "Train Epoch: 216 [16384/54000 (30%)] Loss: -462961.375000\n",
      "Train Epoch: 216 [20480/54000 (38%)] Loss: -462786.500000\n",
      "Train Epoch: 216 [24576/54000 (46%)] Loss: -455511.562500\n",
      "Train Epoch: 216 [28672/54000 (53%)] Loss: -448218.187500\n",
      "Train Epoch: 216 [32768/54000 (61%)] Loss: -449602.906250\n",
      "Train Epoch: 216 [36864/54000 (68%)] Loss: -444234.406250\n",
      "Train Epoch: 216 [40960/54000 (76%)] Loss: -458770.656250\n",
      "Train Epoch: 216 [45056/54000 (83%)] Loss: -444165.187500\n",
      "Train Epoch: 216 [49152/54000 (91%)] Loss: -494544.500000\n",
      "    epoch          : 216\n",
      "    loss           : -457155.02606707317\n",
      "    val_loss       : -459157.28046875\n",
      "Train Epoch: 217 [0/54000 (0%)] Loss: -450925.531250\n",
      "Train Epoch: 217 [4096/54000 (8%)] Loss: -450629.531250\n",
      "Train Epoch: 217 [8192/54000 (15%)] Loss: -442263.000000\n",
      "Train Epoch: 217 [12288/54000 (23%)] Loss: -445156.281250\n",
      "Train Epoch: 217 [16384/54000 (30%)] Loss: -499988.625000\n",
      "Train Epoch: 217 [20480/54000 (38%)] Loss: -463673.625000\n",
      "Train Epoch: 217 [24576/54000 (46%)] Loss: -463984.750000\n",
      "Train Epoch: 217 [28672/54000 (53%)] Loss: -445433.625000\n",
      "Train Epoch: 217 [32768/54000 (61%)] Loss: -443409.812500\n",
      "Train Epoch: 217 [36864/54000 (68%)] Loss: -500310.937500\n",
      "Train Epoch: 217 [40960/54000 (76%)] Loss: -457735.187500\n",
      "Train Epoch: 217 [45056/54000 (83%)] Loss: -463883.687500\n",
      "Train Epoch: 217 [49152/54000 (91%)] Loss: -500309.750000\n",
      "    epoch          : 217\n",
      "    loss           : -457394.7118902439\n",
      "    val_loss       : -459940.1703125\n",
      "Train Epoch: 218 [0/54000 (0%)] Loss: -445115.937500\n",
      "Train Epoch: 218 [4096/54000 (8%)] Loss: -453023.343750\n",
      "Train Epoch: 218 [8192/54000 (15%)] Loss: -462603.000000\n",
      "Train Epoch: 218 [12288/54000 (23%)] Loss: -455145.187500\n",
      "Train Epoch: 218 [16384/54000 (30%)] Loss: -440611.093750\n",
      "Train Epoch: 218 [20480/54000 (38%)] Loss: -453724.312500\n",
      "Train Epoch: 218 [24576/54000 (46%)] Loss: -441690.687500\n",
      "Train Epoch: 218 [28672/54000 (53%)] Loss: -501457.468750\n",
      "Train Epoch: 218 [32768/54000 (61%)] Loss: -442928.750000\n",
      "Train Epoch: 218 [36864/54000 (68%)] Loss: -452251.437500\n",
      "Train Epoch: 218 [40960/54000 (76%)] Loss: -449696.531250\n",
      "Train Epoch: 218 [45056/54000 (83%)] Loss: -465805.593750\n",
      "Train Epoch: 218 [49152/54000 (91%)] Loss: -500815.375000\n",
      "    epoch          : 218\n",
      "    loss           : -457436.46783536585\n",
      "    val_loss       : -460040.79296875\n",
      "Train Epoch: 219 [0/54000 (0%)] Loss: -465422.250000\n",
      "Train Epoch: 219 [4096/54000 (8%)] Loss: -443392.000000\n",
      "Train Epoch: 219 [8192/54000 (15%)] Loss: -442676.375000\n",
      "Train Epoch: 219 [12288/54000 (23%)] Loss: -460195.312500\n",
      "Train Epoch: 219 [16384/54000 (30%)] Loss: -463211.781250\n",
      "Train Epoch: 219 [20480/54000 (38%)] Loss: -448710.906250\n",
      "Train Epoch: 219 [24576/54000 (46%)] Loss: -458976.750000\n",
      "Train Epoch: 219 [28672/54000 (53%)] Loss: -497028.156250\n",
      "Train Epoch: 219 [32768/54000 (61%)] Loss: -449806.625000\n",
      "Train Epoch: 219 [36864/54000 (68%)] Loss: -463158.375000\n",
      "Train Epoch: 219 [40960/54000 (76%)] Loss: -451796.250000\n",
      "Train Epoch: 219 [45056/54000 (83%)] Loss: -462705.562500\n",
      "Train Epoch: 219 [49152/54000 (91%)] Loss: -500773.343750\n",
      "    epoch          : 219\n",
      "    loss           : -457487.1012195122\n",
      "    val_loss       : -460018.8390625\n",
      "Train Epoch: 220 [0/54000 (0%)] Loss: -443132.656250\n",
      "Train Epoch: 220 [4096/54000 (8%)] Loss: -458400.125000\n",
      "Train Epoch: 220 [8192/54000 (15%)] Loss: -441329.812500\n",
      "Train Epoch: 220 [12288/54000 (23%)] Loss: -501456.906250\n",
      "Train Epoch: 220 [16384/54000 (30%)] Loss: -445223.750000\n",
      "Train Epoch: 220 [20480/54000 (38%)] Loss: -462887.656250\n",
      "Train Epoch: 220 [24576/54000 (46%)] Loss: -455973.875000\n",
      "Train Epoch: 220 [28672/54000 (53%)] Loss: -451819.812500\n",
      "Train Epoch: 220 [32768/54000 (61%)] Loss: -442656.812500\n",
      "Train Epoch: 220 [36864/54000 (68%)] Loss: -500517.500000\n",
      "Train Epoch: 220 [40960/54000 (76%)] Loss: -453481.656250\n",
      "Train Epoch: 220 [45056/54000 (83%)] Loss: -463397.343750\n",
      "Train Epoch: 220 [49152/54000 (91%)] Loss: -499379.187500\n",
      "    epoch          : 220\n",
      "    loss           : -457602.80030487804\n",
      "    val_loss       : -460165.78515625\n",
      "Train Epoch: 221 [0/54000 (0%)] Loss: -442295.187500\n",
      "Train Epoch: 221 [4096/54000 (8%)] Loss: -451719.250000\n",
      "Train Epoch: 221 [8192/54000 (15%)] Loss: -449120.500000\n",
      "Train Epoch: 221 [12288/54000 (23%)] Loss: -442915.750000\n",
      "Train Epoch: 221 [16384/54000 (30%)] Loss: -442928.531250\n",
      "Train Epoch: 221 [20480/54000 (38%)] Loss: -462317.375000\n",
      "Train Epoch: 221 [24576/54000 (46%)] Loss: -462485.125000\n",
      "Train Epoch: 221 [28672/54000 (53%)] Loss: -450702.343750\n",
      "Train Epoch: 221 [32768/54000 (61%)] Loss: -442110.625000\n",
      "Train Epoch: 221 [36864/54000 (68%)] Loss: -441869.031250\n",
      "Train Epoch: 221 [40960/54000 (76%)] Loss: -443009.562500\n",
      "Train Epoch: 221 [45056/54000 (83%)] Loss: -441422.500000\n",
      "Train Epoch: 221 [49152/54000 (91%)] Loss: -501069.500000\n",
      "    epoch          : 221\n",
      "    loss           : -457702.8850609756\n",
      "    val_loss       : -460129.65859375\n",
      "Train Epoch: 222 [0/54000 (0%)] Loss: -451172.875000\n",
      "Train Epoch: 222 [4096/54000 (8%)] Loss: -450336.750000\n",
      "Train Epoch: 222 [8192/54000 (15%)] Loss: -448566.093750\n",
      "Train Epoch: 222 [12288/54000 (23%)] Loss: -452464.968750\n",
      "Train Epoch: 222 [16384/54000 (30%)] Loss: -441597.062500\n",
      "Train Epoch: 222 [20480/54000 (38%)] Loss: -451585.718750\n",
      "Train Epoch: 222 [24576/54000 (46%)] Loss: -462441.156250\n",
      "Train Epoch: 222 [28672/54000 (53%)] Loss: -449383.781250\n",
      "Train Epoch: 222 [32768/54000 (61%)] Loss: -444006.281250\n",
      "Train Epoch: 222 [36864/54000 (68%)] Loss: -448242.406250\n",
      "Train Epoch: 222 [40960/54000 (76%)] Loss: -457469.250000\n",
      "Train Epoch: 222 [45056/54000 (83%)] Loss: -462377.187500\n",
      "Train Epoch: 222 [49152/54000 (91%)] Loss: -500414.750000\n",
      "    epoch          : 222\n",
      "    loss           : -457831.0733231707\n",
      "    val_loss       : -459911.78671875\n",
      "Train Epoch: 223 [0/54000 (0%)] Loss: -460782.781250\n",
      "Train Epoch: 223 [4096/54000 (8%)] Loss: -450141.000000\n",
      "Train Epoch: 223 [8192/54000 (15%)] Loss: -466205.218750\n",
      "Train Epoch: 223 [12288/54000 (23%)] Loss: -458432.937500\n",
      "Train Epoch: 223 [16384/54000 (30%)] Loss: -450253.562500\n",
      "Train Epoch: 223 [20480/54000 (38%)] Loss: -463818.968750\n",
      "Train Epoch: 223 [24576/54000 (46%)] Loss: -452162.125000\n",
      "Train Epoch: 223 [28672/54000 (53%)] Loss: -445109.375000\n",
      "Train Epoch: 223 [32768/54000 (61%)] Loss: -452886.750000\n",
      "Train Epoch: 223 [36864/54000 (68%)] Loss: -466109.656250\n",
      "Train Epoch: 223 [40960/54000 (76%)] Loss: -459633.156250\n",
      "Train Epoch: 223 [45056/54000 (83%)] Loss: -463873.250000\n",
      "Train Epoch: 223 [49152/54000 (91%)] Loss: -501569.625000\n",
      "    epoch          : 223\n",
      "    loss           : -457711.4905487805\n",
      "    val_loss       : -460082.27421875\n",
      "Train Epoch: 224 [0/54000 (0%)] Loss: -500420.875000\n",
      "Train Epoch: 224 [4096/54000 (8%)] Loss: -457550.625000\n",
      "Train Epoch: 224 [8192/54000 (15%)] Loss: -450305.406250\n",
      "Train Epoch: 224 [12288/54000 (23%)] Loss: -440836.156250\n",
      "Train Epoch: 224 [16384/54000 (30%)] Loss: -457212.812500\n",
      "Train Epoch: 224 [20480/54000 (38%)] Loss: -501128.500000\n",
      "Train Epoch: 224 [24576/54000 (46%)] Loss: -441392.500000\n",
      "Train Epoch: 224 [28672/54000 (53%)] Loss: -452104.937500\n",
      "Train Epoch: 224 [32768/54000 (61%)] Loss: -451216.687500\n",
      "Train Epoch: 224 [36864/54000 (68%)] Loss: -443901.562500\n",
      "Train Epoch: 224 [40960/54000 (76%)] Loss: -452331.093750\n",
      "Train Epoch: 224 [45056/54000 (83%)] Loss: -463537.312500\n",
      "Train Epoch: 224 [49152/54000 (91%)] Loss: -444124.625000\n",
      "    epoch          : 224\n",
      "    loss           : -457892.5762195122\n",
      "    val_loss       : -460034.68359375\n",
      "Train Epoch: 225 [0/54000 (0%)] Loss: -500452.750000\n",
      "Train Epoch: 225 [4096/54000 (8%)] Loss: -451221.875000\n",
      "Train Epoch: 225 [8192/54000 (15%)] Loss: -450529.906250\n",
      "Train Epoch: 225 [12288/54000 (23%)] Loss: -453170.562500\n",
      "Train Epoch: 225 [16384/54000 (30%)] Loss: -465185.250000\n",
      "Train Epoch: 225 [20480/54000 (38%)] Loss: -443291.937500\n",
      "Train Epoch: 225 [24576/54000 (46%)] Loss: -451260.062500\n",
      "Train Epoch: 225 [28672/54000 (53%)] Loss: -455180.562500\n",
      "Train Epoch: 225 [32768/54000 (61%)] Loss: -501192.093750\n",
      "Train Epoch: 225 [36864/54000 (68%)] Loss: -444917.750000\n",
      "Train Epoch: 225 [40960/54000 (76%)] Loss: -443843.000000\n",
      "Train Epoch: 225 [45056/54000 (83%)] Loss: -462996.343750\n",
      "Train Epoch: 225 [49152/54000 (91%)] Loss: -502203.937500\n",
      "    epoch          : 225\n",
      "    loss           : -457954.6182926829\n",
      "    val_loss       : -460408.271875\n",
      "Train Epoch: 226 [0/54000 (0%)] Loss: -500819.062500\n",
      "Train Epoch: 226 [4096/54000 (8%)] Loss: -465264.906250\n",
      "Train Epoch: 226 [8192/54000 (15%)] Loss: -451224.343750\n",
      "Train Epoch: 226 [12288/54000 (23%)] Loss: -444160.906250\n",
      "Train Epoch: 226 [16384/54000 (30%)] Loss: -444779.187500\n",
      "Train Epoch: 226 [20480/54000 (38%)] Loss: -462929.562500\n",
      "Train Epoch: 226 [24576/54000 (46%)] Loss: -444633.062500\n",
      "Train Epoch: 226 [28672/54000 (53%)] Loss: -497739.312500\n",
      "Train Epoch: 226 [32768/54000 (61%)] Loss: -445364.218750\n",
      "Train Epoch: 226 [36864/54000 (68%)] Loss: -457914.250000\n",
      "Train Epoch: 226 [40960/54000 (76%)] Loss: -442049.500000\n",
      "Train Epoch: 226 [45056/54000 (83%)] Loss: -442335.468750\n",
      "Train Epoch: 226 [49152/54000 (91%)] Loss: -500752.437500\n",
      "    epoch          : 226\n",
      "    loss           : -457999.23216463416\n",
      "    val_loss       : -460286.884375\n",
      "Train Epoch: 227 [0/54000 (0%)] Loss: -501129.125000\n",
      "Train Epoch: 227 [4096/54000 (8%)] Loss: -446165.156250\n",
      "Train Epoch: 227 [8192/54000 (15%)] Loss: -449438.156250\n",
      "Train Epoch: 227 [12288/54000 (23%)] Loss: -500692.843750\n",
      "Train Epoch: 227 [16384/54000 (30%)] Loss: -445335.312500\n",
      "Train Epoch: 227 [20480/54000 (38%)] Loss: -477667.218750\n",
      "Train Epoch: 227 [24576/54000 (46%)] Loss: -452359.187500\n",
      "Train Epoch: 227 [28672/54000 (53%)] Loss: -448524.625000\n",
      "Train Epoch: 227 [32768/54000 (61%)] Loss: -453182.062500\n",
      "Train Epoch: 227 [36864/54000 (68%)] Loss: -465216.250000\n",
      "Train Epoch: 227 [40960/54000 (76%)] Loss: -446520.187500\n",
      "Train Epoch: 227 [45056/54000 (83%)] Loss: -462248.750000\n",
      "Train Epoch: 227 [49152/54000 (91%)] Loss: -501532.687500\n",
      "    epoch          : 227\n",
      "    loss           : -457898.50625\n",
      "    val_loss       : -460295.4\n",
      "Train Epoch: 228 [0/54000 (0%)] Loss: -465737.000000\n",
      "Train Epoch: 228 [4096/54000 (8%)] Loss: -451056.500000\n",
      "Train Epoch: 228 [8192/54000 (15%)] Loss: -449995.062500\n",
      "Train Epoch: 228 [12288/54000 (23%)] Loss: -444910.937500\n",
      "Train Epoch: 228 [16384/54000 (30%)] Loss: -465276.687500\n",
      "Train Epoch: 228 [20480/54000 (38%)] Loss: -461842.250000\n",
      "Train Epoch: 228 [24576/54000 (46%)] Loss: -442706.593750\n",
      "Train Epoch: 228 [28672/54000 (53%)] Loss: -450169.750000\n",
      "Train Epoch: 228 [32768/54000 (61%)] Loss: -463926.000000\n",
      "Train Epoch: 228 [36864/54000 (68%)] Loss: -500082.062500\n",
      "Train Epoch: 228 [40960/54000 (76%)] Loss: -457083.406250\n",
      "Train Epoch: 228 [45056/54000 (83%)] Loss: -445334.656250\n",
      "Train Epoch: 228 [49152/54000 (91%)] Loss: -501293.812500\n",
      "    epoch          : 228\n",
      "    loss           : -458072.3306402439\n",
      "    val_loss       : -460120.67265625\n",
      "Train Epoch: 229 [0/54000 (0%)] Loss: -501152.625000\n",
      "Train Epoch: 229 [4096/54000 (8%)] Loss: -463650.281250\n",
      "Train Epoch: 229 [8192/54000 (15%)] Loss: -446774.375000\n",
      "Train Epoch: 229 [12288/54000 (23%)] Loss: -451040.093750\n",
      "Train Epoch: 229 [16384/54000 (30%)] Loss: -458206.562500\n",
      "Train Epoch: 229 [20480/54000 (38%)] Loss: -464196.781250\n",
      "Train Epoch: 229 [24576/54000 (46%)] Loss: -464786.937500\n",
      "Train Epoch: 229 [28672/54000 (53%)] Loss: -450917.562500\n",
      "Train Epoch: 229 [32768/54000 (61%)] Loss: -453147.906250\n",
      "Train Epoch: 229 [36864/54000 (68%)] Loss: -440357.031250\n",
      "Train Epoch: 229 [40960/54000 (76%)] Loss: -457118.031250\n",
      "Train Epoch: 229 [45056/54000 (83%)] Loss: -465933.625000\n",
      "Train Epoch: 229 [49152/54000 (91%)] Loss: -502587.875000\n",
      "    epoch          : 229\n",
      "    loss           : -458187.837195122\n",
      "    val_loss       : -460197.68203125\n",
      "Train Epoch: 230 [0/54000 (0%)] Loss: -445195.906250\n",
      "Train Epoch: 230 [4096/54000 (8%)] Loss: -465099.625000\n",
      "Train Epoch: 230 [8192/54000 (15%)] Loss: -460315.750000\n",
      "Train Epoch: 230 [12288/54000 (23%)] Loss: -451724.562500\n",
      "Train Epoch: 230 [16384/54000 (30%)] Loss: -445581.375000\n",
      "Train Epoch: 230 [20480/54000 (38%)] Loss: -461040.312500\n",
      "Train Epoch: 230 [24576/54000 (46%)] Loss: -459096.875000\n",
      "Train Epoch: 230 [28672/54000 (53%)] Loss: -451689.250000\n",
      "Train Epoch: 230 [32768/54000 (61%)] Loss: -451383.187500\n",
      "Train Epoch: 230 [36864/54000 (68%)] Loss: -443804.281250\n",
      "Train Epoch: 230 [40960/54000 (76%)] Loss: -453692.375000\n",
      "Train Epoch: 230 [45056/54000 (83%)] Loss: -443236.531250\n",
      "Train Epoch: 230 [49152/54000 (91%)] Loss: -501606.625000\n",
      "    epoch          : 230\n",
      "    loss           : -458224.1942073171\n",
      "    val_loss       : -460351.01796875\n",
      "Train Epoch: 231 [0/54000 (0%)] Loss: -450848.968750\n",
      "Train Epoch: 231 [4096/54000 (8%)] Loss: -465903.125000\n",
      "Train Epoch: 231 [8192/54000 (15%)] Loss: -464197.093750\n",
      "Train Epoch: 231 [12288/54000 (23%)] Loss: -458439.562500\n",
      "Train Epoch: 231 [16384/54000 (30%)] Loss: -463678.906250\n",
      "Train Epoch: 231 [20480/54000 (38%)] Loss: -450368.406250\n",
      "Train Epoch: 231 [24576/54000 (46%)] Loss: -466168.500000\n",
      "Train Epoch: 231 [28672/54000 (53%)] Loss: -462960.187500\n",
      "Train Epoch: 231 [32768/54000 (61%)] Loss: -444799.500000\n",
      "Train Epoch: 231 [36864/54000 (68%)] Loss: -444403.281250\n",
      "Train Epoch: 231 [40960/54000 (76%)] Loss: -453138.187500\n",
      "Train Epoch: 231 [45056/54000 (83%)] Loss: -444581.625000\n",
      "Train Epoch: 231 [49152/54000 (91%)] Loss: -458563.312500\n",
      "    epoch          : 231\n",
      "    loss           : -458277.87728658534\n",
      "    val_loss       : -460224.05078125\n",
      "Train Epoch: 232 [0/54000 (0%)] Loss: -501274.625000\n",
      "Train Epoch: 232 [4096/54000 (8%)] Loss: -443511.406250\n",
      "Train Epoch: 232 [8192/54000 (15%)] Loss: -443011.843750\n",
      "Train Epoch: 232 [12288/54000 (23%)] Loss: -453073.062500\n",
      "Train Epoch: 232 [16384/54000 (30%)] Loss: -443425.125000\n",
      "Train Epoch: 232 [20480/54000 (38%)] Loss: -462356.812500\n",
      "Train Epoch: 232 [24576/54000 (46%)] Loss: -444349.718750\n",
      "Train Epoch: 232 [28672/54000 (53%)] Loss: -452798.906250\n",
      "Train Epoch: 232 [32768/54000 (61%)] Loss: -453588.031250\n",
      "Train Epoch: 232 [36864/54000 (68%)] Loss: -443373.937500\n",
      "Train Epoch: 232 [40960/54000 (76%)] Loss: -464479.812500\n",
      "Train Epoch: 232 [45056/54000 (83%)] Loss: -463601.718750\n",
      "Train Epoch: 232 [49152/54000 (91%)] Loss: -451447.562500\n",
      "    epoch          : 232\n",
      "    loss           : -458286.53658536583\n",
      "    val_loss       : -460571.7046875\n",
      "Train Epoch: 233 [0/54000 (0%)] Loss: -464563.906250\n",
      "Train Epoch: 233 [4096/54000 (8%)] Loss: -445160.906250\n",
      "Train Epoch: 233 [8192/54000 (15%)] Loss: -464159.906250\n",
      "Train Epoch: 233 [12288/54000 (23%)] Loss: -458623.687500\n",
      "Train Epoch: 233 [16384/54000 (30%)] Loss: -440624.437500\n",
      "Train Epoch: 233 [20480/54000 (38%)] Loss: -462925.750000\n",
      "Train Epoch: 233 [24576/54000 (46%)] Loss: -452020.406250\n",
      "Train Epoch: 233 [28672/54000 (53%)] Loss: -500719.250000\n",
      "Train Epoch: 233 [32768/54000 (61%)] Loss: -458736.218750\n",
      "Train Epoch: 233 [36864/54000 (68%)] Loss: -500237.031250\n",
      "Train Epoch: 233 [40960/54000 (76%)] Loss: -452028.468750\n",
      "Train Epoch: 233 [45056/54000 (83%)] Loss: -465094.750000\n",
      "Train Epoch: 233 [49152/54000 (91%)] Loss: -501309.187500\n",
      "    epoch          : 233\n",
      "    loss           : -458374.14466463414\n",
      "    val_loss       : -460237.01015625\n",
      "Train Epoch: 234 [0/54000 (0%)] Loss: -453495.312500\n",
      "Train Epoch: 234 [4096/54000 (8%)] Loss: -450715.093750\n",
      "Train Epoch: 234 [8192/54000 (15%)] Loss: -452266.250000\n",
      "Train Epoch: 234 [12288/54000 (23%)] Loss: -444830.500000\n",
      "Train Epoch: 234 [16384/54000 (30%)] Loss: -444274.750000\n",
      "Train Epoch: 234 [20480/54000 (38%)] Loss: -449557.187500\n",
      "Train Epoch: 234 [24576/54000 (46%)] Loss: -453549.625000\n",
      "Train Epoch: 234 [28672/54000 (53%)] Loss: -449860.843750\n",
      "Train Epoch: 234 [32768/54000 (61%)] Loss: -500763.125000\n",
      "Train Epoch: 234 [36864/54000 (68%)] Loss: -465661.062500\n",
      "Train Epoch: 234 [40960/54000 (76%)] Loss: -452532.406250\n",
      "Train Epoch: 234 [45056/54000 (83%)] Loss: -464070.250000\n",
      "Train Epoch: 234 [49152/54000 (91%)] Loss: -497575.875000\n",
      "    epoch          : 234\n",
      "    loss           : -458388.1931402439\n",
      "    val_loss       : -460231.0765625\n",
      "Train Epoch: 235 [0/54000 (0%)] Loss: -499110.937500\n",
      "Train Epoch: 235 [4096/54000 (8%)] Loss: -464244.343750\n",
      "Train Epoch: 235 [8192/54000 (15%)] Loss: -465249.125000\n",
      "Train Epoch: 235 [12288/54000 (23%)] Loss: -458089.750000\n",
      "Train Epoch: 235 [16384/54000 (30%)] Loss: -445655.718750\n",
      "Train Epoch: 235 [20480/54000 (38%)] Loss: -465215.500000\n",
      "Train Epoch: 235 [24576/54000 (46%)] Loss: -446078.156250\n",
      "Train Epoch: 235 [28672/54000 (53%)] Loss: -499339.437500\n",
      "Train Epoch: 235 [32768/54000 (61%)] Loss: -464595.593750\n",
      "Train Epoch: 235 [36864/54000 (68%)] Loss: -501592.937500\n",
      "Train Epoch: 235 [40960/54000 (76%)] Loss: -451427.250000\n",
      "Train Epoch: 235 [45056/54000 (83%)] Loss: -445912.250000\n",
      "Train Epoch: 235 [49152/54000 (91%)] Loss: -502212.625000\n",
      "    epoch          : 235\n",
      "    loss           : -458475.95716463414\n",
      "    val_loss       : -460481.58359375\n",
      "Train Epoch: 236 [0/54000 (0%)] Loss: -501693.781250\n",
      "Train Epoch: 236 [4096/54000 (8%)] Loss: -464019.562500\n",
      "Train Epoch: 236 [8192/54000 (15%)] Loss: -465238.406250\n",
      "Train Epoch: 236 [12288/54000 (23%)] Loss: -501071.500000\n",
      "Train Epoch: 236 [16384/54000 (30%)] Loss: -446068.125000\n",
      "Train Epoch: 236 [20480/54000 (38%)] Loss: -501451.781250\n",
      "Train Epoch: 236 [24576/54000 (46%)] Loss: -444114.437500\n",
      "Train Epoch: 236 [28672/54000 (53%)] Loss: -466403.812500\n",
      "Train Epoch: 236 [32768/54000 (61%)] Loss: -453698.906250\n",
      "Train Epoch: 236 [36864/54000 (68%)] Loss: -443672.625000\n",
      "Train Epoch: 236 [40960/54000 (76%)] Loss: -444904.906250\n",
      "Train Epoch: 236 [45056/54000 (83%)] Loss: -465960.531250\n",
      "Train Epoch: 236 [49152/54000 (91%)] Loss: -457840.937500\n",
      "    epoch          : 236\n",
      "    loss           : -458520.62835365854\n",
      "    val_loss       : -460559.5515625\n",
      "Train Epoch: 237 [0/54000 (0%)] Loss: -501927.312500\n",
      "Train Epoch: 237 [4096/54000 (8%)] Loss: -446069.750000\n",
      "Train Epoch: 237 [8192/54000 (15%)] Loss: -450629.562500\n",
      "Train Epoch: 237 [12288/54000 (23%)] Loss: -500999.312500\n",
      "Train Epoch: 237 [16384/54000 (30%)] Loss: -441703.437500\n",
      "Train Epoch: 237 [20480/54000 (38%)] Loss: -451131.250000\n",
      "Train Epoch: 237 [24576/54000 (46%)] Loss: -447471.906250\n",
      "Train Epoch: 237 [28672/54000 (53%)] Loss: -448844.687500\n",
      "Train Epoch: 237 [32768/54000 (61%)] Loss: -445297.625000\n",
      "Train Epoch: 237 [36864/54000 (68%)] Loss: -465532.875000\n",
      "Train Epoch: 237 [40960/54000 (76%)] Loss: -453628.687500\n",
      "Train Epoch: 237 [45056/54000 (83%)] Loss: -462593.437500\n",
      "Train Epoch: 237 [49152/54000 (91%)] Loss: -501966.437500\n",
      "    epoch          : 237\n",
      "    loss           : -458627.38597560977\n",
      "    val_loss       : -460436.55234375\n",
      "Train Epoch: 238 [0/54000 (0%)] Loss: -500709.875000\n",
      "Train Epoch: 238 [4096/54000 (8%)] Loss: -451964.000000\n",
      "Train Epoch: 238 [8192/54000 (15%)] Loss: -451600.312500\n",
      "Train Epoch: 238 [12288/54000 (23%)] Loss: -446048.531250\n",
      "Train Epoch: 238 [16384/54000 (30%)] Loss: -459249.937500\n",
      "Train Epoch: 238 [20480/54000 (38%)] Loss: -464748.750000\n",
      "Train Epoch: 238 [24576/54000 (46%)] Loss: -458828.000000\n",
      "Train Epoch: 238 [28672/54000 (53%)] Loss: -451723.000000\n",
      "Train Epoch: 238 [32768/54000 (61%)] Loss: -445104.625000\n",
      "Train Epoch: 238 [36864/54000 (68%)] Loss: -500531.906250\n",
      "Train Epoch: 238 [40960/54000 (76%)] Loss: -462115.437500\n",
      "Train Epoch: 238 [45056/54000 (83%)] Loss: -460997.187500\n",
      "Train Epoch: 238 [49152/54000 (91%)] Loss: -453528.937500\n",
      "    epoch          : 238\n",
      "    loss           : -458864.03353658534\n",
      "    val_loss       : -460618.78046875\n",
      "Train Epoch: 239 [0/54000 (0%)] Loss: -501133.625000\n",
      "Train Epoch: 239 [4096/54000 (8%)] Loss: -464245.156250\n",
      "Train Epoch: 239 [8192/54000 (15%)] Loss: -450691.781250\n",
      "Train Epoch: 239 [12288/54000 (23%)] Loss: -458626.843750\n",
      "Train Epoch: 239 [16384/54000 (30%)] Loss: -465240.593750\n",
      "Train Epoch: 239 [20480/54000 (38%)] Loss: -450478.687500\n",
      "Train Epoch: 239 [24576/54000 (46%)] Loss: -453823.281250\n",
      "Train Epoch: 239 [28672/54000 (53%)] Loss: -450222.468750\n",
      "Train Epoch: 239 [32768/54000 (61%)] Loss: -501960.781250\n",
      "Train Epoch: 239 [36864/54000 (68%)] Loss: -443160.593750\n",
      "Train Epoch: 239 [40960/54000 (76%)] Loss: -453063.562500\n",
      "Train Epoch: 239 [45056/54000 (83%)] Loss: -464903.781250\n",
      "Train Epoch: 239 [49152/54000 (91%)] Loss: -500721.875000\n",
      "    epoch          : 239\n",
      "    loss           : -458796.98231707315\n",
      "    val_loss       : -460616.8140625\n",
      "Train Epoch: 240 [0/54000 (0%)] Loss: -499828.406250\n",
      "Train Epoch: 240 [4096/54000 (8%)] Loss: -445361.093750\n",
      "Train Epoch: 240 [8192/54000 (15%)] Loss: -463707.968750\n",
      "Train Epoch: 240 [12288/54000 (23%)] Loss: -452358.750000\n",
      "Train Epoch: 240 [16384/54000 (30%)] Loss: -443073.562500\n",
      "Train Epoch: 240 [20480/54000 (38%)] Loss: -465357.968750\n",
      "Train Epoch: 240 [24576/54000 (46%)] Loss: -445951.718750\n",
      "Train Epoch: 240 [28672/54000 (53%)] Loss: -448715.062500\n",
      "Train Epoch: 240 [32768/54000 (61%)] Loss: -444875.000000\n",
      "Train Epoch: 240 [36864/54000 (68%)] Loss: -500659.687500\n",
      "Train Epoch: 240 [40960/54000 (76%)] Loss: -453046.937500\n",
      "Train Epoch: 240 [45056/54000 (83%)] Loss: -450128.625000\n",
      "Train Epoch: 240 [49152/54000 (91%)] Loss: -501013.468750\n",
      "    epoch          : 240\n",
      "    loss           : -458740.6945121951\n",
      "    val_loss       : -459928.86875\n",
      "Train Epoch: 241 [0/54000 (0%)] Loss: -450914.250000\n",
      "Train Epoch: 241 [4096/54000 (8%)] Loss: -462606.750000\n",
      "Train Epoch: 241 [8192/54000 (15%)] Loss: -453778.093750\n",
      "Train Epoch: 241 [12288/54000 (23%)] Loss: -445545.468750\n",
      "Train Epoch: 241 [16384/54000 (30%)] Loss: -459516.500000\n",
      "Train Epoch: 241 [20480/54000 (38%)] Loss: -465435.125000\n",
      "Train Epoch: 241 [24576/54000 (46%)] Loss: -453268.250000\n",
      "Train Epoch: 241 [28672/54000 (53%)] Loss: -501388.875000\n",
      "Train Epoch: 241 [32768/54000 (61%)] Loss: -465818.281250\n",
      "Train Epoch: 241 [36864/54000 (68%)] Loss: -447475.375000\n",
      "Train Epoch: 241 [40960/54000 (76%)] Loss: -460675.687500\n",
      "Train Epoch: 241 [45056/54000 (83%)] Loss: -464148.218750\n",
      "Train Epoch: 241 [49152/54000 (91%)] Loss: -453955.437500\n",
      "    epoch          : 241\n",
      "    loss           : -458782.49664634146\n",
      "    val_loss       : -460653.3921875\n",
      "Train Epoch: 242 [0/54000 (0%)] Loss: -500088.218750\n",
      "Train Epoch: 242 [4096/54000 (8%)] Loss: -465391.687500\n",
      "Train Epoch: 242 [8192/54000 (15%)] Loss: -446107.500000\n",
      "Train Epoch: 242 [12288/54000 (23%)] Loss: -445039.250000\n",
      "Train Epoch: 242 [16384/54000 (30%)] Loss: -460420.406250\n",
      "Train Epoch: 242 [20480/54000 (38%)] Loss: -449856.593750\n",
      "Train Epoch: 242 [24576/54000 (46%)] Loss: -443998.562500\n",
      "Train Epoch: 242 [28672/54000 (53%)] Loss: -502754.031250\n",
      "Train Epoch: 242 [32768/54000 (61%)] Loss: -450732.437500\n",
      "Train Epoch: 242 [36864/54000 (68%)] Loss: -446055.781250\n",
      "Train Epoch: 242 [40960/54000 (76%)] Loss: -454693.812500\n",
      "Train Epoch: 242 [45056/54000 (83%)] Loss: -466336.406250\n",
      "Train Epoch: 242 [49152/54000 (91%)] Loss: -501987.125000\n",
      "    epoch          : 242\n",
      "    loss           : -458998.0701219512\n",
      "    val_loss       : -460228.109375\n",
      "Train Epoch: 243 [0/54000 (0%)] Loss: -501510.250000\n",
      "Train Epoch: 243 [4096/54000 (8%)] Loss: -444257.406250\n",
      "Train Epoch: 243 [8192/54000 (15%)] Loss: -457972.312500\n",
      "Train Epoch: 243 [12288/54000 (23%)] Loss: -443156.156250\n",
      "Train Epoch: 243 [16384/54000 (30%)] Loss: -460463.687500\n",
      "Train Epoch: 243 [20480/54000 (38%)] Loss: -463010.687500\n",
      "Train Epoch: 243 [24576/54000 (46%)] Loss: -439177.281250\n",
      "Train Epoch: 243 [28672/54000 (53%)] Loss: -452339.593750\n",
      "Train Epoch: 243 [32768/54000 (61%)] Loss: -502422.062500\n",
      "Train Epoch: 243 [36864/54000 (68%)] Loss: -500665.468750\n",
      "Train Epoch: 243 [40960/54000 (76%)] Loss: -453376.562500\n",
      "Train Epoch: 243 [45056/54000 (83%)] Loss: -443522.531250\n",
      "Train Epoch: 243 [49152/54000 (91%)] Loss: -501157.250000\n",
      "    epoch          : 243\n",
      "    loss           : -458873.9567073171\n",
      "    val_loss       : -460287.2390625\n",
      "Train Epoch: 244 [0/54000 (0%)] Loss: -451508.750000\n",
      "Train Epoch: 244 [4096/54000 (8%)] Loss: -451390.625000\n",
      "Train Epoch: 244 [8192/54000 (15%)] Loss: -466791.593750\n",
      "Train Epoch: 244 [12288/54000 (23%)] Loss: -446767.937500\n",
      "Train Epoch: 244 [16384/54000 (30%)] Loss: -464810.531250\n",
      "Train Epoch: 244 [20480/54000 (38%)] Loss: -451910.031250\n",
      "Train Epoch: 244 [24576/54000 (46%)] Loss: -445123.000000\n",
      "Train Epoch: 244 [28672/54000 (53%)] Loss: -464579.968750\n",
      "Train Epoch: 244 [32768/54000 (61%)] Loss: -502355.375000\n",
      "Train Epoch: 244 [36864/54000 (68%)] Loss: -462755.437500\n",
      "Train Epoch: 244 [40960/54000 (76%)] Loss: -440137.750000\n",
      "Train Epoch: 244 [45056/54000 (83%)] Loss: -465645.937500\n",
      "Train Epoch: 244 [49152/54000 (91%)] Loss: -460265.656250\n",
      "    epoch          : 244\n",
      "    loss           : -458945.0925304878\n",
      "    val_loss       : -460542.88671875\n",
      "Train Epoch: 245 [0/54000 (0%)] Loss: -464229.125000\n",
      "Train Epoch: 245 [4096/54000 (8%)] Loss: -445941.562500\n",
      "Train Epoch: 245 [8192/54000 (15%)] Loss: -443152.937500\n",
      "Train Epoch: 245 [12288/54000 (23%)] Loss: -463828.062500\n",
      "Train Epoch: 245 [16384/54000 (30%)] Loss: -465977.843750\n",
      "Train Epoch: 245 [20480/54000 (38%)] Loss: -450978.750000\n",
      "Train Epoch: 245 [24576/54000 (46%)] Loss: -453249.250000\n",
      "Train Epoch: 245 [28672/54000 (53%)] Loss: -462620.406250\n",
      "Train Epoch: 245 [32768/54000 (61%)] Loss: -453250.000000\n",
      "Train Epoch: 245 [36864/54000 (68%)] Loss: -462857.562500\n",
      "Train Epoch: 245 [40960/54000 (76%)] Loss: -461909.375000\n",
      "Train Epoch: 245 [45056/54000 (83%)] Loss: -443808.625000\n",
      "Train Epoch: 245 [49152/54000 (91%)] Loss: -501409.125000\n",
      "    epoch          : 245\n",
      "    loss           : -458734.2263719512\n",
      "    val_loss       : -459106.240625\n",
      "Train Epoch: 246 [0/54000 (0%)] Loss: -501145.406250\n",
      "Train Epoch: 246 [4096/54000 (8%)] Loss: -459785.187500\n",
      "Train Epoch: 246 [8192/54000 (15%)] Loss: -445632.000000\n",
      "Train Epoch: 246 [12288/54000 (23%)] Loss: -446853.000000\n",
      "Train Epoch: 246 [16384/54000 (30%)] Loss: -464938.187500\n",
      "Train Epoch: 246 [20480/54000 (38%)] Loss: -465039.500000\n",
      "Train Epoch: 246 [24576/54000 (46%)] Loss: -445227.750000\n",
      "Train Epoch: 246 [28672/54000 (53%)] Loss: -453025.218750\n",
      "Train Epoch: 246 [32768/54000 (61%)] Loss: -453675.187500\n",
      "Train Epoch: 246 [36864/54000 (68%)] Loss: -501926.562500\n",
      "Train Epoch: 246 [40960/54000 (76%)] Loss: -459367.593750\n",
      "Train Epoch: 246 [45056/54000 (83%)] Loss: -443865.187500\n",
      "Train Epoch: 246 [49152/54000 (91%)] Loss: -501081.687500\n",
      "    epoch          : 246\n",
      "    loss           : -458926.2445121951\n",
      "    val_loss       : -460662.45859375\n",
      "Train Epoch: 247 [0/54000 (0%)] Loss: -501987.687500\n",
      "Train Epoch: 247 [4096/54000 (8%)] Loss: -453447.562500\n",
      "Train Epoch: 247 [8192/54000 (15%)] Loss: -446510.156250\n",
      "Train Epoch: 247 [12288/54000 (23%)] Loss: -444098.968750\n",
      "Train Epoch: 247 [16384/54000 (30%)] Loss: -448120.656250\n",
      "Train Epoch: 247 [20480/54000 (38%)] Loss: -465773.312500\n",
      "Train Epoch: 247 [24576/54000 (46%)] Loss: -447545.812500\n",
      "Train Epoch: 247 [28672/54000 (53%)] Loss: -451532.875000\n",
      "Train Epoch: 247 [32768/54000 (61%)] Loss: -444667.281250\n",
      "Train Epoch: 247 [36864/54000 (68%)] Loss: -444085.718750\n",
      "Train Epoch: 247 [40960/54000 (76%)] Loss: -444899.500000\n",
      "Train Epoch: 247 [45056/54000 (83%)] Loss: -465357.406250\n",
      "Train Epoch: 247 [49152/54000 (91%)] Loss: -501939.312500\n",
      "    epoch          : 247\n",
      "    loss           : -459180.3210365854\n",
      "    val_loss       : -460778.35078125\n",
      "Train Epoch: 248 [0/54000 (0%)] Loss: -465256.625000\n",
      "Train Epoch: 248 [4096/54000 (8%)] Loss: -460287.250000\n",
      "Train Epoch: 248 [8192/54000 (15%)] Loss: -453920.437500\n",
      "Train Epoch: 248 [12288/54000 (23%)] Loss: -447387.843750\n",
      "Train Epoch: 248 [16384/54000 (30%)] Loss: -459581.437500\n",
      "Train Epoch: 248 [20480/54000 (38%)] Loss: -462845.875000\n",
      "Train Epoch: 248 [24576/54000 (46%)] Loss: -466151.968750\n",
      "Train Epoch: 248 [28672/54000 (53%)] Loss: -450456.000000\n",
      "Train Epoch: 248 [32768/54000 (61%)] Loss: -446101.531250\n",
      "Train Epoch: 248 [36864/54000 (68%)] Loss: -502000.250000\n",
      "Train Epoch: 248 [40960/54000 (76%)] Loss: -451786.625000\n",
      "Train Epoch: 248 [45056/54000 (83%)] Loss: -462304.000000\n",
      "Train Epoch: 248 [49152/54000 (91%)] Loss: -500995.687500\n",
      "    epoch          : 248\n",
      "    loss           : -459195.2850609756\n",
      "    val_loss       : -460508.078125\n",
      "Train Epoch: 249 [0/54000 (0%)] Loss: -445758.375000\n",
      "Train Epoch: 249 [4096/54000 (8%)] Loss: -446585.875000\n",
      "Train Epoch: 249 [8192/54000 (15%)] Loss: -452852.468750\n",
      "Train Epoch: 249 [12288/54000 (23%)] Loss: -443819.875000\n",
      "Train Epoch: 249 [16384/54000 (30%)] Loss: -461381.875000\n",
      "Train Epoch: 249 [20480/54000 (38%)] Loss: -464908.875000\n",
      "Train Epoch: 249 [24576/54000 (46%)] Loss: -458878.156250\n",
      "Train Epoch: 249 [28672/54000 (53%)] Loss: -501473.687500\n",
      "Train Epoch: 249 [32768/54000 (61%)] Loss: -452971.750000\n",
      "Train Epoch: 249 [36864/54000 (68%)] Loss: -445959.718750\n",
      "Train Epoch: 249 [40960/54000 (76%)] Loss: -444988.625000\n",
      "Train Epoch: 249 [45056/54000 (83%)] Loss: -446099.625000\n",
      "Train Epoch: 249 [49152/54000 (91%)] Loss: -443585.125000\n",
      "    epoch          : 249\n",
      "    loss           : -458929.944054878\n",
      "    val_loss       : -460176.84921875\n",
      "Train Epoch: 250 [0/54000 (0%)] Loss: -501058.937500\n",
      "Train Epoch: 250 [4096/54000 (8%)] Loss: -460001.406250\n",
      "Train Epoch: 250 [8192/54000 (15%)] Loss: -465792.500000\n",
      "Train Epoch: 250 [12288/54000 (23%)] Loss: -445263.593750\n",
      "Train Epoch: 250 [16384/54000 (30%)] Loss: -465514.593750\n",
      "Train Epoch: 250 [20480/54000 (38%)] Loss: -453002.250000\n",
      "Train Epoch: 250 [24576/54000 (46%)] Loss: -445442.218750\n",
      "Train Epoch: 250 [28672/54000 (53%)] Loss: -449623.625000\n",
      "Train Epoch: 250 [32768/54000 (61%)] Loss: -445367.875000\n",
      "Train Epoch: 250 [36864/54000 (68%)] Loss: -447006.906250\n",
      "Train Epoch: 250 [40960/54000 (76%)] Loss: -454466.125000\n",
      "Train Epoch: 250 [45056/54000 (83%)] Loss: -463941.218750\n",
      "Train Epoch: 250 [49152/54000 (91%)] Loss: -501339.937500\n",
      "    epoch          : 250\n",
      "    loss           : -459036.774695122\n",
      "    val_loss       : -460411.5921875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0510_004522/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [0/54000 (0%)] Loss: -501426.281250\n",
      "Train Epoch: 251 [4096/54000 (8%)] Loss: -467155.062500\n",
      "Train Epoch: 251 [8192/54000 (15%)] Loss: -449910.500000\n",
      "Train Epoch: 251 [12288/54000 (23%)] Loss: -456817.375000\n",
      "Train Epoch: 251 [16384/54000 (30%)] Loss: -464473.500000\n",
      "Train Epoch: 251 [20480/54000 (38%)] Loss: -445054.093750\n",
      "Train Epoch: 251 [24576/54000 (46%)] Loss: -464970.812500\n",
      "Train Epoch: 251 [28672/54000 (53%)] Loss: -449315.625000\n",
      "Train Epoch: 251 [32768/54000 (61%)] Loss: -453716.375000\n",
      "Train Epoch: 251 [36864/54000 (68%)] Loss: -447410.750000\n",
      "Train Epoch: 251 [40960/54000 (76%)] Loss: -455069.031250\n",
      "Train Epoch: 251 [45056/54000 (83%)] Loss: -464117.843750\n",
      "Train Epoch: 251 [49152/54000 (91%)] Loss: -500422.062500\n",
      "    epoch          : 251\n",
      "    loss           : -459118.0474085366\n",
      "    val_loss       : -459718.92109375\n",
      "Train Epoch: 252 [0/54000 (0%)] Loss: -501575.718750\n",
      "Train Epoch: 252 [4096/54000 (8%)] Loss: -442948.250000\n",
      "Train Epoch: 252 [8192/54000 (15%)] Loss: -462446.375000\n",
      "Train Epoch: 252 [12288/54000 (23%)] Loss: -458971.875000\n",
      "Train Epoch: 252 [16384/54000 (30%)] Loss: -453107.937500\n",
      "Train Epoch: 252 [20480/54000 (38%)] Loss: -464037.312500\n",
      "Train Epoch: 252 [24576/54000 (46%)] Loss: -448311.906250\n",
      "Train Epoch: 252 [28672/54000 (53%)] Loss: -499455.000000\n",
      "Train Epoch: 252 [32768/54000 (61%)] Loss: -461051.062500\n",
      "Train Epoch: 252 [36864/54000 (68%)] Loss: -454984.218750\n",
      "Train Epoch: 252 [40960/54000 (76%)] Loss: -446375.437500\n",
      "Train Epoch: 252 [45056/54000 (83%)] Loss: -461944.375000\n",
      "Train Epoch: 252 [49152/54000 (91%)] Loss: -501260.031250\n",
      "    epoch          : 252\n",
      "    loss           : -459335.3461890244\n",
      "    val_loss       : -460701.96484375\n",
      "Train Epoch: 253 [0/54000 (0%)] Loss: -452941.437500\n",
      "Train Epoch: 253 [4096/54000 (8%)] Loss: -446711.312500\n",
      "Train Epoch: 253 [8192/54000 (15%)] Loss: -449721.437500\n",
      "Train Epoch: 253 [12288/54000 (23%)] Loss: -460455.593750\n",
      "Train Epoch: 253 [16384/54000 (30%)] Loss: -466150.000000\n",
      "Train Epoch: 253 [20480/54000 (38%)] Loss: -464377.531250\n",
      "Train Epoch: 253 [24576/54000 (46%)] Loss: -466156.812500\n",
      "Train Epoch: 253 [28672/54000 (53%)] Loss: -501382.437500\n",
      "Train Epoch: 253 [32768/54000 (61%)] Loss: -458609.781250\n",
      "Train Epoch: 253 [36864/54000 (68%)] Loss: -466248.531250\n",
      "Train Epoch: 253 [40960/54000 (76%)] Loss: -459595.250000\n",
      "Train Epoch: 253 [45056/54000 (83%)] Loss: -444027.437500\n",
      "Train Epoch: 253 [49152/54000 (91%)] Loss: -501864.562500\n",
      "    epoch          : 253\n",
      "    loss           : -459312.4268292683\n",
      "    val_loss       : -460075.4640625\n",
      "Train Epoch: 254 [0/54000 (0%)] Loss: -501486.531250\n",
      "Train Epoch: 254 [4096/54000 (8%)] Loss: -444949.250000\n",
      "Train Epoch: 254 [8192/54000 (15%)] Loss: -452005.687500\n",
      "Train Epoch: 254 [12288/54000 (23%)] Loss: -453333.437500\n",
      "Train Epoch: 254 [16384/54000 (30%)] Loss: -466787.812500\n",
      "Train Epoch: 254 [20480/54000 (38%)] Loss: -453480.125000\n",
      "Train Epoch: 254 [24576/54000 (46%)] Loss: -459868.156250\n",
      "Train Epoch: 254 [28672/54000 (53%)] Loss: -451592.375000\n",
      "Train Epoch: 254 [32768/54000 (61%)] Loss: -499853.750000\n",
      "Train Epoch: 254 [36864/54000 (68%)] Loss: -464888.000000\n",
      "Train Epoch: 254 [40960/54000 (76%)] Loss: -452399.187500\n",
      "Train Epoch: 254 [45056/54000 (83%)] Loss: -465367.593750\n",
      "Train Epoch: 254 [49152/54000 (91%)] Loss: -501126.187500\n",
      "    epoch          : 254\n",
      "    loss           : -459256.3245426829\n",
      "    val_loss       : -460852.940625\n",
      "Train Epoch: 255 [0/54000 (0%)] Loss: -500973.750000\n",
      "Train Epoch: 255 [4096/54000 (8%)] Loss: -461916.500000\n",
      "Train Epoch: 255 [8192/54000 (15%)] Loss: -452375.937500\n",
      "Train Epoch: 255 [12288/54000 (23%)] Loss: -453571.187500\n",
      "Train Epoch: 255 [16384/54000 (30%)] Loss: -501619.062500\n",
      "Train Epoch: 255 [20480/54000 (38%)] Loss: -454175.656250\n",
      "Train Epoch: 255 [24576/54000 (46%)] Loss: -460357.250000\n",
      "Train Epoch: 255 [28672/54000 (53%)] Loss: -454738.468750\n",
      "Train Epoch: 255 [32768/54000 (61%)] Loss: -465386.093750\n",
      "Train Epoch: 255 [36864/54000 (68%)] Loss: -501933.312500\n",
      "Train Epoch: 255 [40960/54000 (76%)] Loss: -452889.375000\n",
      "Train Epoch: 255 [45056/54000 (83%)] Loss: -443877.187500\n",
      "Train Epoch: 255 [49152/54000 (91%)] Loss: -501577.593750\n",
      "    epoch          : 255\n",
      "    loss           : -459441.26234756096\n",
      "    val_loss       : -460494.57734375\n",
      "Train Epoch: 256 [0/54000 (0%)] Loss: -501732.937500\n",
      "Train Epoch: 256 [4096/54000 (8%)] Loss: -454794.718750\n",
      "Train Epoch: 256 [8192/54000 (15%)] Loss: -452514.375000\n",
      "Train Epoch: 256 [12288/54000 (23%)] Loss: -464713.093750\n",
      "Train Epoch: 256 [16384/54000 (30%)] Loss: -447661.125000\n",
      "Train Epoch: 256 [20480/54000 (38%)] Loss: -463951.656250\n",
      "Train Epoch: 256 [24576/54000 (46%)] Loss: -463549.468750\n",
      "Train Epoch: 256 [28672/54000 (53%)] Loss: -452752.000000\n",
      "Train Epoch: 256 [32768/54000 (61%)] Loss: -456058.093750\n",
      "Train Epoch: 256 [36864/54000 (68%)] Loss: -443717.625000\n",
      "Train Epoch: 256 [40960/54000 (76%)] Loss: -453915.875000\n",
      "Train Epoch: 256 [45056/54000 (83%)] Loss: -446762.312500\n",
      "Train Epoch: 256 [49152/54000 (91%)] Loss: -501682.937500\n",
      "    epoch          : 256\n",
      "    loss           : -459462.76158536586\n",
      "    val_loss       : -460649.84765625\n",
      "Train Epoch: 257 [0/54000 (0%)] Loss: -453942.562500\n",
      "Train Epoch: 257 [4096/54000 (8%)] Loss: -448664.468750\n",
      "Train Epoch: 257 [8192/54000 (15%)] Loss: -452487.812500\n",
      "Train Epoch: 257 [12288/54000 (23%)] Loss: -444404.406250\n",
      "Train Epoch: 257 [16384/54000 (30%)] Loss: -445449.875000\n",
      "Train Epoch: 257 [20480/54000 (38%)] Loss: -453140.093750\n",
      "Train Epoch: 257 [24576/54000 (46%)] Loss: -445910.562500\n",
      "Train Epoch: 257 [28672/54000 (53%)] Loss: -452669.500000\n",
      "Train Epoch: 257 [32768/54000 (61%)] Loss: -460023.875000\n",
      "Train Epoch: 257 [36864/54000 (68%)] Loss: -465278.781250\n",
      "Train Epoch: 257 [40960/54000 (76%)] Loss: -445880.468750\n",
      "Train Epoch: 257 [45056/54000 (83%)] Loss: -453065.468750\n",
      "Train Epoch: 257 [49152/54000 (91%)] Loss: -502237.343750\n",
      "    epoch          : 257\n",
      "    loss           : -459583.55731707317\n",
      "    val_loss       : -460598.1890625\n",
      "Train Epoch: 258 [0/54000 (0%)] Loss: -501464.406250\n",
      "Train Epoch: 258 [4096/54000 (8%)] Loss: -445764.500000\n",
      "Train Epoch: 258 [8192/54000 (15%)] Loss: -466208.437500\n",
      "Train Epoch: 258 [12288/54000 (23%)] Loss: -502159.781250\n",
      "Train Epoch: 258 [16384/54000 (30%)] Loss: -464239.656250\n",
      "Train Epoch: 258 [20480/54000 (38%)] Loss: -459195.375000\n",
      "Train Epoch: 258 [24576/54000 (46%)] Loss: -447669.062500\n",
      "Train Epoch: 258 [28672/54000 (53%)] Loss: -465533.218750\n",
      "Train Epoch: 258 [32768/54000 (61%)] Loss: -466868.000000\n",
      "Train Epoch: 258 [36864/54000 (68%)] Loss: -456808.593750\n",
      "Train Epoch: 258 [40960/54000 (76%)] Loss: -450610.750000\n",
      "Train Epoch: 258 [45056/54000 (83%)] Loss: -467182.562500\n",
      "Train Epoch: 258 [49152/54000 (91%)] Loss: -455582.312500\n",
      "    epoch          : 258\n",
      "    loss           : -459698.92286585364\n",
      "    val_loss       : -460658.6359375\n",
      "Train Epoch: 259 [0/54000 (0%)] Loss: -468065.437500\n",
      "Train Epoch: 259 [4096/54000 (8%)] Loss: -466895.062500\n",
      "Train Epoch: 259 [8192/54000 (15%)] Loss: -464382.218750\n",
      "Train Epoch: 259 [12288/54000 (23%)] Loss: -453055.562500\n",
      "Train Epoch: 259 [16384/54000 (30%)] Loss: -501290.843750\n",
      "Train Epoch: 259 [20480/54000 (38%)] Loss: -463353.531250\n",
      "Train Epoch: 259 [24576/54000 (46%)] Loss: -446288.875000\n",
      "Train Epoch: 259 [28672/54000 (53%)] Loss: -452720.500000\n",
      "Train Epoch: 259 [32768/54000 (61%)] Loss: -464131.937500\n",
      "Train Epoch: 259 [36864/54000 (68%)] Loss: -447096.062500\n",
      "Train Epoch: 259 [40960/54000 (76%)] Loss: -453756.156250\n",
      "Train Epoch: 259 [45056/54000 (83%)] Loss: -458167.250000\n",
      "Train Epoch: 259 [49152/54000 (91%)] Loss: -500164.687500\n",
      "    epoch          : 259\n",
      "    loss           : -459413.37713414634\n",
      "    val_loss       : -460721.34921875\n",
      "Train Epoch: 260 [0/54000 (0%)] Loss: -501597.000000\n",
      "Train Epoch: 260 [4096/54000 (8%)] Loss: -445992.562500\n",
      "Train Epoch: 260 [8192/54000 (15%)] Loss: -446427.156250\n",
      "Train Epoch: 260 [12288/54000 (23%)] Loss: -500977.187500\n",
      "Train Epoch: 260 [16384/54000 (30%)] Loss: -461995.937500\n",
      "Train Epoch: 260 [20480/54000 (38%)] Loss: -465133.343750\n",
      "Train Epoch: 260 [24576/54000 (46%)] Loss: -466363.843750\n",
      "Train Epoch: 260 [28672/54000 (53%)] Loss: -451840.187500\n",
      "Train Epoch: 260 [32768/54000 (61%)] Loss: -456131.375000\n",
      "Train Epoch: 260 [36864/54000 (68%)] Loss: -495642.718750\n",
      "Train Epoch: 260 [40960/54000 (76%)] Loss: -460714.500000\n",
      "Train Epoch: 260 [45056/54000 (83%)] Loss: -464962.468750\n",
      "Train Epoch: 260 [49152/54000 (91%)] Loss: -445533.812500\n",
      "    epoch          : 260\n",
      "    loss           : -459659.2740853659\n",
      "    val_loss       : -460690.63515625\n",
      "Train Epoch: 261 [0/54000 (0%)] Loss: -501379.375000\n",
      "Train Epoch: 261 [4096/54000 (8%)] Loss: -461707.000000\n",
      "Train Epoch: 261 [8192/54000 (15%)] Loss: -451555.375000\n",
      "Train Epoch: 261 [12288/54000 (23%)] Loss: -446428.562500\n",
      "Train Epoch: 261 [16384/54000 (30%)] Loss: -446834.062500\n",
      "Train Epoch: 261 [20480/54000 (38%)] Loss: -452585.000000\n",
      "Train Epoch: 261 [24576/54000 (46%)] Loss: -448336.937500\n",
      "Train Epoch: 261 [28672/54000 (53%)] Loss: -501279.500000\n",
      "Train Epoch: 261 [32768/54000 (61%)] Loss: -444131.500000\n",
      "Train Epoch: 261 [36864/54000 (68%)] Loss: -448072.375000\n",
      "Train Epoch: 261 [40960/54000 (76%)] Loss: -446817.125000\n",
      "Train Epoch: 261 [45056/54000 (83%)] Loss: -446572.312500\n",
      "Train Epoch: 261 [49152/54000 (91%)] Loss: -498694.750000\n",
      "    epoch          : 261\n",
      "    loss           : -459854.51463414636\n",
      "    val_loss       : -460679.390625\n",
      "Train Epoch: 262 [0/54000 (0%)] Loss: -463263.375000\n",
      "Train Epoch: 262 [4096/54000 (8%)] Loss: -459858.750000\n",
      "Train Epoch: 262 [8192/54000 (15%)] Loss: -464918.000000\n",
      "Train Epoch: 262 [12288/54000 (23%)] Loss: -458592.625000\n",
      "Train Epoch: 262 [16384/54000 (30%)] Loss: -447421.687500\n",
      "Train Epoch: 262 [20480/54000 (38%)] Loss: -465658.437500\n",
      "Train Epoch: 262 [24576/54000 (46%)] Loss: -464625.250000\n",
      "Train Epoch: 262 [28672/54000 (53%)] Loss: -501238.906250\n",
      "Train Epoch: 262 [32768/54000 (61%)] Loss: -462595.375000\n",
      "Train Epoch: 262 [36864/54000 (68%)] Loss: -444708.718750\n",
      "Train Epoch: 262 [40960/54000 (76%)] Loss: -464354.437500\n",
      "Train Epoch: 262 [45056/54000 (83%)] Loss: -444723.718750\n",
      "Train Epoch: 262 [49152/54000 (91%)] Loss: -501800.312500\n",
      "    epoch          : 262\n",
      "    loss           : -459800.9638719512\n",
      "    val_loss       : -461104.3828125\n",
      "Train Epoch: 263 [0/54000 (0%)] Loss: -464862.281250\n",
      "Train Epoch: 263 [4096/54000 (8%)] Loss: -460811.375000\n",
      "Train Epoch: 263 [8192/54000 (15%)] Loss: -445480.062500\n",
      "Train Epoch: 263 [12288/54000 (23%)] Loss: -445399.281250\n",
      "Train Epoch: 263 [16384/54000 (30%)] Loss: -502507.093750\n",
      "Train Epoch: 263 [20480/54000 (38%)] Loss: -464775.875000\n",
      "Train Epoch: 263 [24576/54000 (46%)] Loss: -460173.562500\n",
      "Train Epoch: 263 [28672/54000 (53%)] Loss: -467555.281250\n",
      "Train Epoch: 263 [32768/54000 (61%)] Loss: -502343.781250\n",
      "Train Epoch: 263 [36864/54000 (68%)] Loss: -501187.718750\n",
      "Train Epoch: 263 [40960/54000 (76%)] Loss: -443496.750000\n",
      "Train Epoch: 263 [45056/54000 (83%)] Loss: -465678.843750\n",
      "Train Epoch: 263 [49152/54000 (91%)] Loss: -501210.875000\n",
      "    epoch          : 263\n",
      "    loss           : -459792.29420731706\n",
      "    val_loss       : -461002.45546875\n",
      "Train Epoch: 264 [0/54000 (0%)] Loss: -501664.093750\n",
      "Train Epoch: 264 [4096/54000 (8%)] Loss: -445388.562500\n",
      "Train Epoch: 264 [8192/54000 (15%)] Loss: -453004.375000\n",
      "Train Epoch: 264 [12288/54000 (23%)] Loss: -460948.062500\n",
      "Train Epoch: 264 [16384/54000 (30%)] Loss: -502935.781250\n",
      "Train Epoch: 264 [20480/54000 (38%)] Loss: -452874.250000\n",
      "Train Epoch: 264 [24576/54000 (46%)] Loss: -466111.937500\n",
      "Train Epoch: 264 [28672/54000 (53%)] Loss: -499829.843750\n",
      "Train Epoch: 264 [32768/54000 (61%)] Loss: -454561.156250\n",
      "Train Epoch: 264 [36864/54000 (68%)] Loss: -466106.687500\n",
      "Train Epoch: 264 [40960/54000 (76%)] Loss: -446072.500000\n",
      "Train Epoch: 264 [45056/54000 (83%)] Loss: -465400.687500\n",
      "Train Epoch: 264 [49152/54000 (91%)] Loss: -502192.781250\n",
      "    epoch          : 264\n",
      "    loss           : -459954.2257621951\n",
      "    val_loss       : -460796.44609375\n",
      "Train Epoch: 265 [0/54000 (0%)] Loss: -466300.687500\n",
      "Train Epoch: 265 [4096/54000 (8%)] Loss: -466213.312500\n",
      "Train Epoch: 265 [8192/54000 (15%)] Loss: -451087.562500\n",
      "Train Epoch: 265 [12288/54000 (23%)] Loss: -461015.000000\n",
      "Train Epoch: 265 [16384/54000 (30%)] Loss: -463704.406250\n",
      "Train Epoch: 265 [20480/54000 (38%)] Loss: -446334.968750\n",
      "Train Epoch: 265 [24576/54000 (46%)] Loss: -446966.406250\n",
      "Train Epoch: 265 [28672/54000 (53%)] Loss: -501169.875000\n",
      "Train Epoch: 265 [32768/54000 (61%)] Loss: -445881.062500\n",
      "Train Epoch: 265 [36864/54000 (68%)] Loss: -501763.625000\n",
      "Train Epoch: 265 [40960/54000 (76%)] Loss: -445187.750000\n",
      "Train Epoch: 265 [45056/54000 (83%)] Loss: -463681.875000\n",
      "Train Epoch: 265 [49152/54000 (91%)] Loss: -502575.406250\n",
      "    epoch          : 265\n",
      "    loss           : -459588.5957317073\n",
      "    val_loss       : -460216.13203125\n",
      "Train Epoch: 266 [0/54000 (0%)] Loss: -466269.250000\n",
      "Train Epoch: 266 [4096/54000 (8%)] Loss: -447472.656250\n",
      "Train Epoch: 266 [8192/54000 (15%)] Loss: -466934.062500\n",
      "Train Epoch: 266 [12288/54000 (23%)] Loss: -445817.000000\n",
      "Train Epoch: 266 [16384/54000 (30%)] Loss: -446048.187500\n",
      "Train Epoch: 266 [20480/54000 (38%)] Loss: -464506.000000\n",
      "Train Epoch: 266 [24576/54000 (46%)] Loss: -454274.875000\n",
      "Train Epoch: 266 [28672/54000 (53%)] Loss: -499865.562500\n",
      "Train Epoch: 266 [32768/54000 (61%)] Loss: -447721.906250\n",
      "Train Epoch: 266 [36864/54000 (68%)] Loss: -460445.375000\n",
      "Train Epoch: 266 [40960/54000 (76%)] Loss: -443785.875000\n",
      "Train Epoch: 266 [45056/54000 (83%)] Loss: -466599.625000\n",
      "Train Epoch: 266 [49152/54000 (91%)] Loss: -502328.843750\n",
      "    epoch          : 266\n",
      "    loss           : -459676.9217987805\n",
      "    val_loss       : -460504.05390625\n",
      "Train Epoch: 267 [0/54000 (0%)] Loss: -500840.750000\n",
      "Train Epoch: 267 [4096/54000 (8%)] Loss: -454782.906250\n",
      "Train Epoch: 267 [8192/54000 (15%)] Loss: -451850.812500\n",
      "Train Epoch: 267 [12288/54000 (23%)] Loss: -453045.187500\n",
      "Train Epoch: 267 [16384/54000 (30%)] Loss: -455746.906250\n",
      "Train Epoch: 267 [20480/54000 (38%)] Loss: -444131.937500\n",
      "Train Epoch: 267 [24576/54000 (46%)] Loss: -450873.781250\n",
      "Train Epoch: 267 [28672/54000 (53%)] Loss: -502774.000000\n",
      "Train Epoch: 267 [32768/54000 (61%)] Loss: -466146.281250\n",
      "Train Epoch: 267 [36864/54000 (68%)] Loss: -448058.781250\n",
      "Train Epoch: 267 [40960/54000 (76%)] Loss: -452845.750000\n",
      "Train Epoch: 267 [45056/54000 (83%)] Loss: -443609.656250\n",
      "Train Epoch: 267 [49152/54000 (91%)] Loss: -501548.750000\n",
      "    epoch          : 267\n",
      "    loss           : -459959.07423780486\n",
      "    val_loss       : -460693.19609375\n",
      "Train Epoch: 268 [0/54000 (0%)] Loss: -501834.156250\n",
      "Train Epoch: 268 [4096/54000 (8%)] Loss: -460892.781250\n",
      "Train Epoch: 268 [8192/54000 (15%)] Loss: -466068.781250\n",
      "Train Epoch: 268 [12288/54000 (23%)] Loss: -446155.062500\n",
      "Train Epoch: 268 [16384/54000 (30%)] Loss: -455836.968750\n",
      "Train Epoch: 268 [20480/54000 (38%)] Loss: -464768.031250\n",
      "Train Epoch: 268 [24576/54000 (46%)] Loss: -461475.437500\n",
      "Train Epoch: 268 [28672/54000 (53%)] Loss: -455493.875000\n",
      "Train Epoch: 268 [32768/54000 (61%)] Loss: -460463.062500\n",
      "Train Epoch: 268 [36864/54000 (68%)] Loss: -464649.187500\n",
      "Train Epoch: 268 [40960/54000 (76%)] Loss: -460726.812500\n",
      "Train Epoch: 268 [45056/54000 (83%)] Loss: -466345.968750\n",
      "Train Epoch: 268 [49152/54000 (91%)] Loss: -444229.812500\n",
      "    epoch          : 268\n",
      "    loss           : -460086.956097561\n",
      "    val_loss       : -461057.2953125\n",
      "Train Epoch: 269 [0/54000 (0%)] Loss: -464525.062500\n",
      "Train Epoch: 269 [4096/54000 (8%)] Loss: -450117.500000\n",
      "Train Epoch: 269 [8192/54000 (15%)] Loss: -452889.843750\n",
      "Train Epoch: 269 [12288/54000 (23%)] Loss: -446768.125000\n",
      "Train Epoch: 269 [16384/54000 (30%)] Loss: -446665.781250\n",
      "Train Epoch: 269 [20480/54000 (38%)] Loss: -465305.000000\n",
      "Train Epoch: 269 [24576/54000 (46%)] Loss: -447235.625000\n",
      "Train Epoch: 269 [28672/54000 (53%)] Loss: -453372.500000\n",
      "Train Epoch: 269 [32768/54000 (61%)] Loss: -464893.375000\n",
      "Train Epoch: 269 [36864/54000 (68%)] Loss: -447896.968750\n",
      "Train Epoch: 269 [40960/54000 (76%)] Loss: -457178.843750\n",
      "Train Epoch: 269 [45056/54000 (83%)] Loss: -463895.000000\n",
      "Train Epoch: 269 [49152/54000 (91%)] Loss: -500678.031250\n",
      "    epoch          : 269\n",
      "    loss           : -459967.0817073171\n",
      "    val_loss       : -460659.34921875\n",
      "Train Epoch: 270 [0/54000 (0%)] Loss: -500740.250000\n",
      "Train Epoch: 270 [4096/54000 (8%)] Loss: -449335.125000\n",
      "Train Epoch: 270 [8192/54000 (15%)] Loss: -446583.187500\n",
      "Train Epoch: 270 [12288/54000 (23%)] Loss: -503159.656250\n",
      "Train Epoch: 270 [16384/54000 (30%)] Loss: -501459.125000\n",
      "Train Epoch: 270 [20480/54000 (38%)] Loss: -501129.343750\n",
      "Train Epoch: 270 [24576/54000 (46%)] Loss: -449390.062500\n",
      "Train Epoch: 270 [28672/54000 (53%)] Loss: -454363.312500\n",
      "Train Epoch: 270 [32768/54000 (61%)] Loss: -446815.531250\n",
      "Train Epoch: 270 [36864/54000 (68%)] Loss: -445878.875000\n",
      "Train Epoch: 270 [40960/54000 (76%)] Loss: -444104.593750\n",
      "Train Epoch: 270 [45056/54000 (83%)] Loss: -449800.156250\n",
      "Train Epoch: 270 [49152/54000 (91%)] Loss: -502579.750000\n",
      "    epoch          : 270\n",
      "    loss           : -460068.6737804878\n",
      "    val_loss       : -460735.85078125\n",
      "Train Epoch: 271 [0/54000 (0%)] Loss: -501631.187500\n",
      "Train Epoch: 271 [4096/54000 (8%)] Loss: -463065.656250\n",
      "Train Epoch: 271 [8192/54000 (15%)] Loss: -454341.750000\n",
      "Train Epoch: 271 [12288/54000 (23%)] Loss: -461177.093750\n",
      "Train Epoch: 271 [16384/54000 (30%)] Loss: -461172.375000\n",
      "Train Epoch: 271 [20480/54000 (38%)] Loss: -465590.406250\n",
      "Train Epoch: 271 [24576/54000 (46%)] Loss: -465295.218750\n",
      "Train Epoch: 271 [28672/54000 (53%)] Loss: -454707.156250\n",
      "Train Epoch: 271 [32768/54000 (61%)] Loss: -447690.437500\n",
      "Train Epoch: 271 [36864/54000 (68%)] Loss: -449752.187500\n",
      "Train Epoch: 271 [40960/54000 (76%)] Loss: -455546.937500\n",
      "Train Epoch: 271 [45056/54000 (83%)] Loss: -447747.312500\n",
      "Train Epoch: 271 [49152/54000 (91%)] Loss: -502432.281250\n",
      "    epoch          : 271\n",
      "    loss           : -460225.125152439\n",
      "    val_loss       : -460750.7859375\n",
      "Train Epoch: 272 [0/54000 (0%)] Loss: -501213.843750\n",
      "Train Epoch: 272 [4096/54000 (8%)] Loss: -453213.812500\n",
      "Train Epoch: 272 [8192/54000 (15%)] Loss: -446977.187500\n",
      "Train Epoch: 272 [12288/54000 (23%)] Loss: -501269.781250\n",
      "Train Epoch: 272 [16384/54000 (30%)] Loss: -445244.093750\n",
      "Train Epoch: 272 [20480/54000 (38%)] Loss: -456116.687500\n",
      "Train Epoch: 272 [24576/54000 (46%)] Loss: -447583.187500\n",
      "Train Epoch: 272 [28672/54000 (53%)] Loss: -452333.437500\n",
      "Train Epoch: 272 [32768/54000 (61%)] Loss: -465890.437500\n",
      "Train Epoch: 272 [36864/54000 (68%)] Loss: -501664.437500\n",
      "Train Epoch: 272 [40960/54000 (76%)] Loss: -455987.093750\n",
      "Train Epoch: 272 [45056/54000 (83%)] Loss: -466178.062500\n",
      "Train Epoch: 272 [49152/54000 (91%)] Loss: -501163.718750\n",
      "    epoch          : 272\n",
      "    loss           : -460025.2602134146\n",
      "    val_loss       : -460997.925\n",
      "Train Epoch: 273 [0/54000 (0%)] Loss: -501187.843750\n",
      "Train Epoch: 273 [4096/54000 (8%)] Loss: -454233.500000\n",
      "Train Epoch: 273 [8192/54000 (15%)] Loss: -453996.406250\n",
      "Train Epoch: 273 [12288/54000 (23%)] Loss: -444261.125000\n",
      "Train Epoch: 273 [16384/54000 (30%)] Loss: -453297.562500\n",
      "Train Epoch: 273 [20480/54000 (38%)] Loss: -465788.187500\n",
      "Train Epoch: 273 [24576/54000 (46%)] Loss: -447892.906250\n",
      "Train Epoch: 273 [28672/54000 (53%)] Loss: -451504.937500\n",
      "Train Epoch: 273 [32768/54000 (61%)] Loss: -499486.656250\n",
      "Train Epoch: 273 [36864/54000 (68%)] Loss: -502359.625000\n",
      "Train Epoch: 273 [40960/54000 (76%)] Loss: -458846.437500\n",
      "Train Epoch: 273 [45056/54000 (83%)] Loss: -466166.250000\n",
      "Train Epoch: 273 [49152/54000 (91%)] Loss: -451400.250000\n",
      "    epoch          : 273\n",
      "    loss           : -460240.03155487805\n",
      "    val_loss       : -461026.8515625\n",
      "Train Epoch: 274 [0/54000 (0%)] Loss: -465700.218750\n",
      "Train Epoch: 274 [4096/54000 (8%)] Loss: -447830.218750\n",
      "Train Epoch: 274 [8192/54000 (15%)] Loss: -450828.250000\n",
      "Train Epoch: 274 [12288/54000 (23%)] Loss: -450963.906250\n",
      "Train Epoch: 274 [16384/54000 (30%)] Loss: -452047.500000\n",
      "Train Epoch: 274 [20480/54000 (38%)] Loss: -467145.125000\n",
      "Train Epoch: 274 [24576/54000 (46%)] Loss: -444333.250000\n",
      "Train Epoch: 274 [28672/54000 (53%)] Loss: -464537.875000\n",
      "Train Epoch: 274 [32768/54000 (61%)] Loss: -444812.937500\n",
      "Train Epoch: 274 [36864/54000 (68%)] Loss: -464163.187500\n",
      "Train Epoch: 274 [40960/54000 (76%)] Loss: -461005.500000\n",
      "Train Epoch: 274 [45056/54000 (83%)] Loss: -464399.125000\n",
      "Train Epoch: 274 [49152/54000 (91%)] Loss: -461939.687500\n",
      "    epoch          : 274\n",
      "    loss           : -460366.96768292686\n",
      "    val_loss       : -460569.39453125\n",
      "Train Epoch: 275 [0/54000 (0%)] Loss: -465829.156250\n",
      "Train Epoch: 275 [4096/54000 (8%)] Loss: -451979.937500\n",
      "Train Epoch: 275 [8192/54000 (15%)] Loss: -446895.843750\n",
      "Train Epoch: 275 [12288/54000 (23%)] Loss: -443517.125000\n",
      "Train Epoch: 275 [16384/54000 (30%)] Loss: -445394.406250\n",
      "Train Epoch: 275 [20480/54000 (38%)] Loss: -464481.000000\n",
      "Train Epoch: 275 [24576/54000 (46%)] Loss: -444263.343750\n",
      "Train Epoch: 275 [28672/54000 (53%)] Loss: -502820.312500\n",
      "Train Epoch: 275 [32768/54000 (61%)] Loss: -445190.156250\n",
      "Train Epoch: 275 [36864/54000 (68%)] Loss: -447517.125000\n",
      "Train Epoch: 275 [40960/54000 (76%)] Loss: -466093.906250\n",
      "Train Epoch: 275 [45056/54000 (83%)] Loss: -467629.187500\n",
      "Train Epoch: 275 [49152/54000 (91%)] Loss: -501776.125000\n",
      "    epoch          : 275\n",
      "    loss           : -460257.6201219512\n",
      "    val_loss       : -460897.04296875\n",
      "Train Epoch: 276 [0/54000 (0%)] Loss: -464723.468750\n",
      "Train Epoch: 276 [4096/54000 (8%)] Loss: -447802.625000\n",
      "Train Epoch: 276 [8192/54000 (15%)] Loss: -465279.218750\n",
      "Train Epoch: 276 [12288/54000 (23%)] Loss: -460102.562500\n",
      "Train Epoch: 276 [16384/54000 (30%)] Loss: -447611.875000\n",
      "Train Epoch: 276 [20480/54000 (38%)] Loss: -465921.437500\n",
      "Train Epoch: 276 [24576/54000 (46%)] Loss: -446101.375000\n",
      "Train Epoch: 276 [28672/54000 (53%)] Loss: -450458.218750\n",
      "Train Epoch: 276 [32768/54000 (61%)] Loss: -458874.812500\n",
      "Train Epoch: 276 [36864/54000 (68%)] Loss: -501953.812500\n",
      "Train Epoch: 276 [40960/54000 (76%)] Loss: -452187.062500\n",
      "Train Epoch: 276 [45056/54000 (83%)] Loss: -466821.500000\n",
      "Train Epoch: 276 [49152/54000 (91%)] Loss: -500749.750000\n",
      "    epoch          : 276\n",
      "    loss           : -460314.337652439\n",
      "    val_loss       : -460736.7859375\n",
      "Train Epoch: 277 [0/54000 (0%)] Loss: -500631.750000\n",
      "Train Epoch: 277 [4096/54000 (8%)] Loss: -461373.000000\n",
      "Train Epoch: 277 [8192/54000 (15%)] Loss: -466109.375000\n",
      "Train Epoch: 277 [12288/54000 (23%)] Loss: -454413.375000\n",
      "Train Epoch: 277 [16384/54000 (30%)] Loss: -500593.625000\n",
      "Train Epoch: 277 [20480/54000 (38%)] Loss: -454785.000000\n",
      "Train Epoch: 277 [24576/54000 (46%)] Loss: -464245.750000\n",
      "Train Epoch: 277 [28672/54000 (53%)] Loss: -454635.531250\n",
      "Train Epoch: 277 [32768/54000 (61%)] Loss: -465585.625000\n",
      "Train Epoch: 277 [36864/54000 (68%)] Loss: -446760.125000\n",
      "Train Epoch: 277 [40960/54000 (76%)] Loss: -462813.437500\n",
      "Train Epoch: 277 [45056/54000 (83%)] Loss: -466204.937500\n",
      "Train Epoch: 277 [49152/54000 (91%)] Loss: -502626.531250\n",
      "    epoch          : 277\n",
      "    loss           : -460225.8844512195\n",
      "    val_loss       : -460810.58515625\n",
      "Train Epoch: 278 [0/54000 (0%)] Loss: -452288.687500\n",
      "Train Epoch: 278 [4096/54000 (8%)] Loss: -466222.468750\n",
      "Train Epoch: 278 [8192/54000 (15%)] Loss: -451320.687500\n",
      "Train Epoch: 278 [12288/54000 (23%)] Loss: -462941.437500\n",
      "Train Epoch: 278 [16384/54000 (30%)] Loss: -454627.812500\n",
      "Train Epoch: 278 [20480/54000 (38%)] Loss: -466075.687500\n",
      "Train Epoch: 278 [24576/54000 (46%)] Loss: -449202.375000\n",
      "Train Epoch: 278 [28672/54000 (53%)] Loss: -452406.781250\n",
      "Train Epoch: 278 [32768/54000 (61%)] Loss: -502630.718750\n",
      "Train Epoch: 278 [36864/54000 (68%)] Loss: -448047.437500\n",
      "Train Epoch: 278 [40960/54000 (76%)] Loss: -446393.906250\n",
      "Train Epoch: 278 [45056/54000 (83%)] Loss: -448132.125000\n",
      "Train Epoch: 278 [49152/54000 (91%)] Loss: -501265.875000\n",
      "    epoch          : 278\n",
      "    loss           : -460399.14329268294\n",
      "    val_loss       : -461146.91328125\n",
      "Train Epoch: 279 [0/54000 (0%)] Loss: -501407.250000\n",
      "Train Epoch: 279 [4096/54000 (8%)] Loss: -465426.718750\n",
      "Train Epoch: 279 [8192/54000 (15%)] Loss: -453217.031250\n",
      "Train Epoch: 279 [12288/54000 (23%)] Loss: -465761.750000\n",
      "Train Epoch: 279 [16384/54000 (30%)] Loss: -452169.687500\n",
      "Train Epoch: 279 [20480/54000 (38%)] Loss: -466417.531250\n",
      "Train Epoch: 279 [24576/54000 (46%)] Loss: -453632.906250\n",
      "Train Epoch: 279 [28672/54000 (53%)] Loss: -501050.531250\n",
      "Train Epoch: 279 [32768/54000 (61%)] Loss: -452778.875000\n",
      "Train Epoch: 279 [36864/54000 (68%)] Loss: -502530.312500\n",
      "Train Epoch: 279 [40960/54000 (76%)] Loss: -444453.718750\n",
      "Train Epoch: 279 [45056/54000 (83%)] Loss: -448671.375000\n",
      "Train Epoch: 279 [49152/54000 (91%)] Loss: -502070.093750\n",
      "    epoch          : 279\n",
      "    loss           : -460474.96661585366\n",
      "    val_loss       : -461000.72890625\n",
      "Train Epoch: 280 [0/54000 (0%)] Loss: -452641.562500\n",
      "Train Epoch: 280 [4096/54000 (8%)] Loss: -465792.843750\n",
      "Train Epoch: 280 [8192/54000 (15%)] Loss: -466192.375000\n",
      "Train Epoch: 280 [12288/54000 (23%)] Loss: -446203.062500\n",
      "Train Epoch: 280 [16384/54000 (30%)] Loss: -438129.062500\n",
      "Train Epoch: 280 [20480/54000 (38%)] Loss: -464397.437500\n",
      "Train Epoch: 280 [24576/54000 (46%)] Loss: -449300.125000\n",
      "Train Epoch: 280 [28672/54000 (53%)] Loss: -455400.687500\n",
      "Train Epoch: 280 [32768/54000 (61%)] Loss: -502632.843750\n",
      "Train Epoch: 280 [36864/54000 (68%)] Loss: -448916.875000\n",
      "Train Epoch: 280 [40960/54000 (76%)] Loss: -455599.156250\n",
      "Train Epoch: 280 [45056/54000 (83%)] Loss: -447266.000000\n",
      "Train Epoch: 280 [49152/54000 (91%)] Loss: -456734.406250\n",
      "    epoch          : 280\n",
      "    loss           : -460527.0304878049\n",
      "    val_loss       : -460866.99921875\n",
      "Train Epoch: 281 [0/54000 (0%)] Loss: -502423.375000\n",
      "Train Epoch: 281 [4096/54000 (8%)] Loss: -450813.718750\n",
      "Train Epoch: 281 [8192/54000 (15%)] Loss: -465797.625000\n",
      "Train Epoch: 281 [12288/54000 (23%)] Loss: -449597.218750\n",
      "Train Epoch: 281 [16384/54000 (30%)] Loss: -447361.062500\n",
      "Train Epoch: 281 [20480/54000 (38%)] Loss: -466136.750000\n",
      "Train Epoch: 281 [24576/54000 (46%)] Loss: -441821.406250\n",
      "Train Epoch: 281 [28672/54000 (53%)] Loss: -501623.500000\n",
      "Train Epoch: 281 [32768/54000 (61%)] Loss: -446258.000000\n",
      "Train Epoch: 281 [36864/54000 (68%)] Loss: -459611.937500\n",
      "Train Epoch: 281 [40960/54000 (76%)] Loss: -452036.937500\n",
      "Train Epoch: 281 [45056/54000 (83%)] Loss: -447862.812500\n",
      "Train Epoch: 281 [49152/54000 (91%)] Loss: -502876.625000\n",
      "    epoch          : 281\n",
      "    loss           : -460460.71768292686\n",
      "    val_loss       : -461019.428125\n",
      "Train Epoch: 282 [0/54000 (0%)] Loss: -501991.187500\n",
      "Train Epoch: 282 [4096/54000 (8%)] Loss: -461016.062500\n",
      "Train Epoch: 282 [8192/54000 (15%)] Loss: -452733.218750\n",
      "Train Epoch: 282 [12288/54000 (23%)] Loss: -502009.750000\n",
      "Train Epoch: 282 [16384/54000 (30%)] Loss: -444698.750000\n",
      "Train Epoch: 282 [20480/54000 (38%)] Loss: -465704.281250\n",
      "Train Epoch: 282 [24576/54000 (46%)] Loss: -445732.187500\n",
      "Train Epoch: 282 [28672/54000 (53%)] Loss: -501399.125000\n",
      "Train Epoch: 282 [32768/54000 (61%)] Loss: -456133.812500\n",
      "Train Epoch: 282 [36864/54000 (68%)] Loss: -461188.875000\n",
      "Train Epoch: 282 [40960/54000 (76%)] Loss: -444714.312500\n",
      "Train Epoch: 282 [45056/54000 (83%)] Loss: -466719.000000\n",
      "Train Epoch: 282 [49152/54000 (91%)] Loss: -502307.437500\n",
      "    epoch          : 282\n",
      "    loss           : -460424.0864329268\n",
      "    val_loss       : -460716.16015625\n",
      "Train Epoch: 283 [0/54000 (0%)] Loss: -502915.812500\n",
      "Train Epoch: 283 [4096/54000 (8%)] Loss: -465388.406250\n",
      "Train Epoch: 283 [8192/54000 (15%)] Loss: -452888.875000\n",
      "Train Epoch: 283 [12288/54000 (23%)] Loss: -501958.968750\n",
      "Train Epoch: 283 [16384/54000 (30%)] Loss: -447766.750000\n",
      "Train Epoch: 283 [20480/54000 (38%)] Loss: -464264.875000\n",
      "Train Epoch: 283 [24576/54000 (46%)] Loss: -465570.750000\n",
      "Train Epoch: 283 [28672/54000 (53%)] Loss: -453008.593750\n",
      "Train Epoch: 283 [32768/54000 (61%)] Loss: -461960.093750\n",
      "Train Epoch: 283 [36864/54000 (68%)] Loss: -501651.937500\n",
      "Train Epoch: 283 [40960/54000 (76%)] Loss: -453417.500000\n",
      "Train Epoch: 283 [45056/54000 (83%)] Loss: -445250.625000\n",
      "Train Epoch: 283 [49152/54000 (91%)] Loss: -502850.968750\n",
      "    epoch          : 283\n",
      "    loss           : -460528.1304878049\n",
      "    val_loss       : -460994.0578125\n",
      "Train Epoch: 284 [0/54000 (0%)] Loss: -466437.687500\n",
      "Train Epoch: 284 [4096/54000 (8%)] Loss: -458264.312500\n",
      "Train Epoch: 284 [8192/54000 (15%)] Loss: -465479.250000\n",
      "Train Epoch: 284 [12288/54000 (23%)] Loss: -449194.406250\n",
      "Train Epoch: 284 [16384/54000 (30%)] Loss: -453985.125000\n",
      "Train Epoch: 284 [20480/54000 (38%)] Loss: -464591.968750\n",
      "Train Epoch: 284 [24576/54000 (46%)] Loss: -446474.875000\n",
      "Train Epoch: 284 [28672/54000 (53%)] Loss: -453867.437500\n",
      "Train Epoch: 284 [32768/54000 (61%)] Loss: -445423.125000\n",
      "Train Epoch: 284 [36864/54000 (68%)] Loss: -445843.437500\n",
      "Train Epoch: 284 [40960/54000 (76%)] Loss: -460087.812500\n",
      "Train Epoch: 284 [45056/54000 (83%)] Loss: -463569.625000\n",
      "Train Epoch: 284 [49152/54000 (91%)] Loss: -501277.593750\n",
      "    epoch          : 284\n",
      "    loss           : -460336.9280487805\n",
      "    val_loss       : -461061.803125\n",
      "Train Epoch: 285 [0/54000 (0%)] Loss: -465734.375000\n",
      "Train Epoch: 285 [4096/54000 (8%)] Loss: -449482.843750\n",
      "Train Epoch: 285 [8192/54000 (15%)] Loss: -452812.875000\n",
      "Train Epoch: 285 [12288/54000 (23%)] Loss: -462385.281250\n",
      "Train Epoch: 285 [16384/54000 (30%)] Loss: -443228.500000\n",
      "Train Epoch: 285 [20480/54000 (38%)] Loss: -466253.593750\n",
      "Train Epoch: 285 [24576/54000 (46%)] Loss: -467411.906250\n",
      "Train Epoch: 285 [28672/54000 (53%)] Loss: -502248.656250\n",
      "Train Epoch: 285 [32768/54000 (61%)] Loss: -454943.656250\n",
      "Train Epoch: 285 [36864/54000 (68%)] Loss: -448389.250000\n",
      "Train Epoch: 285 [40960/54000 (76%)] Loss: -455582.250000\n",
      "Train Epoch: 285 [45056/54000 (83%)] Loss: -447567.625000\n",
      "Train Epoch: 285 [49152/54000 (91%)] Loss: -503305.312500\n",
      "    epoch          : 285\n",
      "    loss           : -460687.3856707317\n",
      "    val_loss       : -461028.7125\n",
      "Train Epoch: 286 [0/54000 (0%)] Loss: -451657.281250\n",
      "Train Epoch: 286 [4096/54000 (8%)] Loss: -448825.312500\n",
      "Train Epoch: 286 [8192/54000 (15%)] Loss: -453504.625000\n",
      "Train Epoch: 286 [12288/54000 (23%)] Loss: -460858.750000\n",
      "Train Epoch: 286 [16384/54000 (30%)] Loss: -455276.156250\n",
      "Train Epoch: 286 [20480/54000 (38%)] Loss: -451480.281250\n",
      "Train Epoch: 286 [24576/54000 (46%)] Loss: -448942.000000\n",
      "Train Epoch: 286 [28672/54000 (53%)] Loss: -466903.656250\n",
      "Train Epoch: 286 [32768/54000 (61%)] Loss: -502367.156250\n",
      "Train Epoch: 286 [36864/54000 (68%)] Loss: -466512.281250\n",
      "Train Epoch: 286 [40960/54000 (76%)] Loss: -453658.531250\n",
      "Train Epoch: 286 [45056/54000 (83%)] Loss: -464591.812500\n",
      "Train Epoch: 286 [49152/54000 (91%)] Loss: -500961.843750\n",
      "    epoch          : 286\n",
      "    loss           : -460685.6964939024\n",
      "    val_loss       : -461153.86640625\n",
      "Train Epoch: 287 [0/54000 (0%)] Loss: -463015.000000\n",
      "Train Epoch: 287 [4096/54000 (8%)] Loss: -453117.312500\n",
      "Train Epoch: 287 [8192/54000 (15%)] Loss: -447169.250000\n",
      "Train Epoch: 287 [12288/54000 (23%)] Loss: -453416.000000\n",
      "Train Epoch: 287 [16384/54000 (30%)] Loss: -451520.500000\n",
      "Train Epoch: 287 [20480/54000 (38%)] Loss: -455047.625000\n",
      "Train Epoch: 287 [24576/54000 (46%)] Loss: -446627.125000\n",
      "Train Epoch: 287 [28672/54000 (53%)] Loss: -453066.000000\n",
      "Train Epoch: 287 [32768/54000 (61%)] Loss: -449523.437500\n",
      "Train Epoch: 287 [36864/54000 (68%)] Loss: -444804.875000\n",
      "Train Epoch: 287 [40960/54000 (76%)] Loss: -448636.031250\n",
      "Train Epoch: 287 [45056/54000 (83%)] Loss: -466439.843750\n",
      "Train Epoch: 287 [49152/54000 (91%)] Loss: -502192.812500\n",
      "    epoch          : 287\n",
      "    loss           : -460705.96783536585\n",
      "    val_loss       : -460993.9\n",
      "Train Epoch: 288 [0/54000 (0%)] Loss: -453469.000000\n",
      "Train Epoch: 288 [4096/54000 (8%)] Loss: -445393.250000\n",
      "Train Epoch: 288 [8192/54000 (15%)] Loss: -465008.718750\n",
      "Train Epoch: 288 [12288/54000 (23%)] Loss: -501021.968750\n",
      "Train Epoch: 288 [16384/54000 (30%)] Loss: -447417.468750\n",
      "Train Epoch: 288 [20480/54000 (38%)] Loss: -465377.562500\n",
      "Train Epoch: 288 [24576/54000 (46%)] Loss: -447357.937500\n",
      "Train Epoch: 288 [28672/54000 (53%)] Loss: -454389.812500\n",
      "Train Epoch: 288 [32768/54000 (61%)] Loss: -462667.531250\n",
      "Train Epoch: 288 [36864/54000 (68%)] Loss: -447046.031250\n",
      "Train Epoch: 288 [40960/54000 (76%)] Loss: -459403.125000\n",
      "Train Epoch: 288 [45056/54000 (83%)] Loss: -449423.218750\n",
      "Train Epoch: 288 [49152/54000 (91%)] Loss: -502834.718750\n",
      "    epoch          : 288\n",
      "    loss           : -460747.69161585363\n",
      "    val_loss       : -461219.64921875\n",
      "Train Epoch: 289 [0/54000 (0%)] Loss: -466535.812500\n",
      "Train Epoch: 289 [4096/54000 (8%)] Loss: -445053.375000\n",
      "Train Epoch: 289 [8192/54000 (15%)] Loss: -455754.062500\n",
      "Train Epoch: 289 [12288/54000 (23%)] Loss: -442440.187500\n",
      "Train Epoch: 289 [16384/54000 (30%)] Loss: -446081.343750\n",
      "Train Epoch: 289 [20480/54000 (38%)] Loss: -467753.187500\n",
      "Train Epoch: 289 [24576/54000 (46%)] Loss: -444940.625000\n",
      "Train Epoch: 289 [28672/54000 (53%)] Loss: -503130.250000\n",
      "Train Epoch: 289 [32768/54000 (61%)] Loss: -455220.750000\n",
      "Train Epoch: 289 [36864/54000 (68%)] Loss: -503926.968750\n",
      "Train Epoch: 289 [40960/54000 (76%)] Loss: -461435.687500\n",
      "Train Epoch: 289 [45056/54000 (83%)] Loss: -450271.718750\n",
      "Train Epoch: 289 [49152/54000 (91%)] Loss: -502272.750000\n",
      "    epoch          : 289\n",
      "    loss           : -460882.29710365855\n",
      "    val_loss       : -461120.51796875\n",
      "Train Epoch: 290 [0/54000 (0%)] Loss: -446426.812500\n",
      "Train Epoch: 290 [4096/54000 (8%)] Loss: -447559.750000\n",
      "Train Epoch: 290 [8192/54000 (15%)] Loss: -468352.812500\n",
      "Train Epoch: 290 [12288/54000 (23%)] Loss: -455221.437500\n",
      "Train Epoch: 290 [16384/54000 (30%)] Loss: -449240.687500\n",
      "Train Epoch: 290 [20480/54000 (38%)] Loss: -466763.281250\n",
      "Train Epoch: 290 [24576/54000 (46%)] Loss: -448044.312500\n",
      "Train Epoch: 290 [28672/54000 (53%)] Loss: -455294.406250\n",
      "Train Epoch: 290 [32768/54000 (61%)] Loss: -501880.468750\n",
      "Train Epoch: 290 [36864/54000 (68%)] Loss: -446660.625000\n",
      "Train Epoch: 290 [40960/54000 (76%)] Loss: -446677.468750\n",
      "Train Epoch: 290 [45056/54000 (83%)] Loss: -465869.625000\n",
      "Train Epoch: 290 [49152/54000 (91%)] Loss: -502545.625000\n",
      "    epoch          : 290\n",
      "    loss           : -460933.78704268293\n",
      "    val_loss       : -461025.775\n",
      "Train Epoch: 291 [0/54000 (0%)] Loss: -461286.843750\n",
      "Train Epoch: 291 [4096/54000 (8%)] Loss: -448913.000000\n",
      "Train Epoch: 291 [8192/54000 (15%)] Loss: -461830.250000\n",
      "Train Epoch: 291 [12288/54000 (23%)] Loss: -454525.312500\n",
      "Train Epoch: 291 [16384/54000 (30%)] Loss: -460376.625000\n",
      "Train Epoch: 291 [20480/54000 (38%)] Loss: -454892.250000\n",
      "Train Epoch: 291 [24576/54000 (46%)] Loss: -466203.343750\n",
      "Train Epoch: 291 [28672/54000 (53%)] Loss: -454162.875000\n",
      "Train Epoch: 291 [32768/54000 (61%)] Loss: -456921.437500\n",
      "Train Epoch: 291 [36864/54000 (68%)] Loss: -446433.625000\n",
      "Train Epoch: 291 [40960/54000 (76%)] Loss: -457306.781250\n",
      "Train Epoch: 291 [45056/54000 (83%)] Loss: -447781.968750\n",
      "Train Epoch: 291 [49152/54000 (91%)] Loss: -501870.406250\n",
      "    epoch          : 291\n",
      "    loss           : -460841.6448170732\n",
      "    val_loss       : -461169.3109375\n",
      "Train Epoch: 292 [0/54000 (0%)] Loss: -503130.906250\n",
      "Train Epoch: 292 [4096/54000 (8%)] Loss: -444247.187500\n",
      "Train Epoch: 292 [8192/54000 (15%)] Loss: -449800.937500\n",
      "Train Epoch: 292 [12288/54000 (23%)] Loss: -502737.937500\n",
      "Train Epoch: 292 [16384/54000 (30%)] Loss: -452330.375000\n",
      "Train Epoch: 292 [20480/54000 (38%)] Loss: -465460.406250\n",
      "Train Epoch: 292 [24576/54000 (46%)] Loss: -464505.875000\n",
      "Train Epoch: 292 [28672/54000 (53%)] Loss: -454409.531250\n",
      "Train Epoch: 292 [32768/54000 (61%)] Loss: -453799.562500\n",
      "Train Epoch: 292 [36864/54000 (68%)] Loss: -466682.718750\n",
      "Train Epoch: 292 [40960/54000 (76%)] Loss: -457080.062500\n",
      "Train Epoch: 292 [45056/54000 (83%)] Loss: -462494.125000\n",
      "Train Epoch: 292 [49152/54000 (91%)] Loss: -501882.781250\n",
      "    epoch          : 292\n",
      "    loss           : -460900.3068597561\n",
      "    val_loss       : -461326.96484375\n",
      "Train Epoch: 293 [0/54000 (0%)] Loss: -452552.281250\n",
      "Train Epoch: 293 [4096/54000 (8%)] Loss: -460386.312500\n",
      "Train Epoch: 293 [8192/54000 (15%)] Loss: -451967.937500\n",
      "Train Epoch: 293 [12288/54000 (23%)] Loss: -447274.812500\n",
      "Train Epoch: 293 [16384/54000 (30%)] Loss: -447930.250000\n",
      "Train Epoch: 293 [20480/54000 (38%)] Loss: -454359.562500\n",
      "Train Epoch: 293 [24576/54000 (46%)] Loss: -467144.250000\n",
      "Train Epoch: 293 [28672/54000 (53%)] Loss: -501983.343750\n",
      "Train Epoch: 293 [32768/54000 (61%)] Loss: -462540.625000\n",
      "Train Epoch: 293 [36864/54000 (68%)] Loss: -465199.968750\n",
      "Train Epoch: 293 [40960/54000 (76%)] Loss: -461782.187500\n",
      "Train Epoch: 293 [45056/54000 (83%)] Loss: -466540.437500\n",
      "Train Epoch: 293 [49152/54000 (91%)] Loss: -455583.281250\n",
      "    epoch          : 293\n",
      "    loss           : -461039.0806402439\n",
      "    val_loss       : -461242.6859375\n",
      "Train Epoch: 294 [0/54000 (0%)] Loss: -502970.218750\n",
      "Train Epoch: 294 [4096/54000 (8%)] Loss: -453842.531250\n",
      "Train Epoch: 294 [8192/54000 (15%)] Loss: -465216.281250\n",
      "Train Epoch: 294 [12288/54000 (23%)] Loss: -453113.062500\n",
      "Train Epoch: 294 [16384/54000 (30%)] Loss: -466342.281250\n",
      "Train Epoch: 294 [20480/54000 (38%)] Loss: -465379.250000\n",
      "Train Epoch: 294 [24576/54000 (46%)] Loss: -445268.187500\n",
      "Train Epoch: 294 [28672/54000 (53%)] Loss: -445895.000000\n",
      "Train Epoch: 294 [32768/54000 (61%)] Loss: -461860.906250\n",
      "Train Epoch: 294 [36864/54000 (68%)] Loss: -467275.531250\n",
      "Train Epoch: 294 [40960/54000 (76%)] Loss: -462660.250000\n",
      "Train Epoch: 294 [45056/54000 (83%)] Loss: -465438.687500\n",
      "Train Epoch: 294 [49152/54000 (91%)] Loss: -461125.562500\n",
      "    epoch          : 294\n",
      "    loss           : -461002.2079268293\n",
      "    val_loss       : -461138.48984375\n",
      "Train Epoch: 295 [0/54000 (0%)] Loss: -448627.468750\n",
      "Train Epoch: 295 [4096/54000 (8%)] Loss: -456789.750000\n",
      "Train Epoch: 295 [8192/54000 (15%)] Loss: -454829.812500\n",
      "Train Epoch: 295 [12288/54000 (23%)] Loss: -447905.156250\n",
      "Train Epoch: 295 [16384/54000 (30%)] Loss: -449683.593750\n",
      "Train Epoch: 295 [20480/54000 (38%)] Loss: -467454.156250\n",
      "Train Epoch: 295 [24576/54000 (46%)] Loss: -464366.625000\n",
      "Train Epoch: 295 [28672/54000 (53%)] Loss: -501952.000000\n",
      "Train Epoch: 295 [32768/54000 (61%)] Loss: -461231.062500\n",
      "Train Epoch: 295 [36864/54000 (68%)] Loss: -501898.843750\n",
      "Train Epoch: 295 [40960/54000 (76%)] Loss: -459787.843750\n",
      "Train Epoch: 295 [45056/54000 (83%)] Loss: -448902.875000\n",
      "Train Epoch: 295 [49152/54000 (91%)] Loss: -502692.312500\n",
      "    epoch          : 295\n",
      "    loss           : -460957.0379573171\n",
      "    val_loss       : -461106.55078125\n",
      "Train Epoch: 296 [0/54000 (0%)] Loss: -502764.187500\n",
      "Train Epoch: 296 [4096/54000 (8%)] Loss: -445132.156250\n",
      "Train Epoch: 296 [8192/54000 (15%)] Loss: -465323.000000\n",
      "Train Epoch: 296 [12288/54000 (23%)] Loss: -467060.468750\n",
      "Train Epoch: 296 [16384/54000 (30%)] Loss: -455739.812500\n",
      "Train Epoch: 296 [20480/54000 (38%)] Loss: -465276.750000\n",
      "Train Epoch: 296 [24576/54000 (46%)] Loss: -466930.250000\n",
      "Train Epoch: 296 [28672/54000 (53%)] Loss: -454855.437500\n",
      "Train Epoch: 296 [32768/54000 (61%)] Loss: -466431.125000\n",
      "Train Epoch: 296 [36864/54000 (68%)] Loss: -502797.875000\n",
      "Train Epoch: 296 [40960/54000 (76%)] Loss: -455391.062500\n",
      "Train Epoch: 296 [45056/54000 (83%)] Loss: -465816.375000\n",
      "Train Epoch: 296 [49152/54000 (91%)] Loss: -500501.750000\n",
      "    epoch          : 296\n",
      "    loss           : -460884.58094512194\n",
      "    val_loss       : -460817.71796875\n",
      "Train Epoch: 297 [0/54000 (0%)] Loss: -503906.812500\n",
      "Train Epoch: 297 [4096/54000 (8%)] Loss: -447682.125000\n",
      "Train Epoch: 297 [8192/54000 (15%)] Loss: -467526.875000\n",
      "Train Epoch: 297 [12288/54000 (23%)] Loss: -444755.937500\n",
      "Train Epoch: 297 [16384/54000 (30%)] Loss: -466745.875000\n",
      "Train Epoch: 297 [20480/54000 (38%)] Loss: -453957.250000\n",
      "Train Epoch: 297 [24576/54000 (46%)] Loss: -462592.781250\n",
      "Train Epoch: 297 [28672/54000 (53%)] Loss: -502713.437500\n",
      "Train Epoch: 297 [32768/54000 (61%)] Loss: -462705.375000\n",
      "Train Epoch: 297 [36864/54000 (68%)] Loss: -449251.593750\n",
      "Train Epoch: 297 [40960/54000 (76%)] Loss: -455569.937500\n",
      "Train Epoch: 297 [45056/54000 (83%)] Loss: -466120.593750\n",
      "Train Epoch: 297 [49152/54000 (91%)] Loss: -503032.250000\n",
      "    epoch          : 297\n",
      "    loss           : -461102.8983231707\n",
      "    val_loss       : -461357.76171875\n",
      "Train Epoch: 298 [0/54000 (0%)] Loss: -500192.687500\n",
      "Train Epoch: 298 [4096/54000 (8%)] Loss: -465881.593750\n",
      "Train Epoch: 298 [8192/54000 (15%)] Loss: -448994.375000\n",
      "Train Epoch: 298 [12288/54000 (23%)] Loss: -454016.218750\n",
      "Train Epoch: 298 [16384/54000 (30%)] Loss: -462712.625000\n",
      "Train Epoch: 298 [20480/54000 (38%)] Loss: -465969.281250\n",
      "Train Epoch: 298 [24576/54000 (46%)] Loss: -464068.312500\n",
      "Train Epoch: 298 [28672/54000 (53%)] Loss: -453342.250000\n",
      "Train Epoch: 298 [32768/54000 (61%)] Loss: -441645.468750\n",
      "Train Epoch: 298 [36864/54000 (68%)] Loss: -448126.718750\n",
      "Train Epoch: 298 [40960/54000 (76%)] Loss: -454997.562500\n",
      "Train Epoch: 298 [45056/54000 (83%)] Loss: -466500.468750\n",
      "Train Epoch: 298 [49152/54000 (91%)] Loss: -452767.218750\n",
      "    epoch          : 298\n",
      "    loss           : -460945.41082317074\n",
      "    val_loss       : -461085.7015625\n",
      "Train Epoch: 299 [0/54000 (0%)] Loss: -503277.125000\n",
      "Train Epoch: 299 [4096/54000 (8%)] Loss: -447513.812500\n",
      "Train Epoch: 299 [8192/54000 (15%)] Loss: -455424.031250\n",
      "Train Epoch: 299 [12288/54000 (23%)] Loss: -455621.812500\n",
      "Train Epoch: 299 [16384/54000 (30%)] Loss: -448747.687500\n",
      "Train Epoch: 299 [20480/54000 (38%)] Loss: -462918.062500\n",
      "Train Epoch: 299 [24576/54000 (46%)] Loss: -446302.593750\n",
      "Train Epoch: 299 [28672/54000 (53%)] Loss: -502511.375000\n",
      "Train Epoch: 299 [32768/54000 (61%)] Loss: -454737.250000\n",
      "Train Epoch: 299 [36864/54000 (68%)] Loss: -447911.156250\n",
      "Train Epoch: 299 [40960/54000 (76%)] Loss: -446356.312500\n",
      "Train Epoch: 299 [45056/54000 (83%)] Loss: -448828.468750\n",
      "Train Epoch: 299 [49152/54000 (91%)] Loss: -499831.281250\n",
      "    epoch          : 299\n",
      "    loss           : -461065.7025914634\n",
      "    val_loss       : -461298.6234375\n",
      "Train Epoch: 300 [0/54000 (0%)] Loss: -464636.250000\n",
      "Train Epoch: 300 [4096/54000 (8%)] Loss: -463394.156250\n",
      "Train Epoch: 300 [8192/54000 (15%)] Loss: -448808.125000\n",
      "Train Epoch: 300 [12288/54000 (23%)] Loss: -465836.875000\n",
      "Train Epoch: 300 [16384/54000 (30%)] Loss: -453133.125000\n",
      "Train Epoch: 300 [20480/54000 (38%)] Loss: -465156.687500\n",
      "Train Epoch: 300 [24576/54000 (46%)] Loss: -448250.625000\n",
      "Train Epoch: 300 [28672/54000 (53%)] Loss: -454736.500000\n",
      "Train Epoch: 300 [32768/54000 (61%)] Loss: -425803.781250\n",
      "Train Epoch: 300 [36864/54000 (68%)] Loss: -466103.500000\n",
      "Train Epoch: 300 [40960/54000 (76%)] Loss: -460576.468750\n",
      "Train Epoch: 300 [45056/54000 (83%)] Loss: -467158.000000\n",
      "Train Epoch: 300 [49152/54000 (91%)] Loss: -502893.343750\n",
      "    epoch          : 300\n",
      "    loss           : -460948.662804878\n",
      "    val_loss       : -461304.46875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0510_004522/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [0/54000 (0%)] Loss: -503076.781250\n",
      "Train Epoch: 301 [4096/54000 (8%)] Loss: -454597.437500\n",
      "Train Epoch: 301 [8192/54000 (15%)] Loss: -446686.187500\n",
      "Train Epoch: 301 [12288/54000 (23%)] Loss: -448112.125000\n",
      "Train Epoch: 301 [16384/54000 (30%)] Loss: -447335.500000\n",
      "Train Epoch: 301 [20480/54000 (38%)] Loss: -452201.437500\n",
      "Train Epoch: 301 [24576/54000 (46%)] Loss: -461501.750000\n",
      "Train Epoch: 301 [28672/54000 (53%)] Loss: -466309.343750\n",
      "Train Epoch: 301 [32768/54000 (61%)] Loss: -455759.562500\n",
      "Train Epoch: 301 [36864/54000 (68%)] Loss: -448616.937500\n",
      "Train Epoch: 301 [40960/54000 (76%)] Loss: -464418.812500\n",
      "Train Epoch: 301 [45056/54000 (83%)] Loss: -448270.406250\n",
      "Train Epoch: 301 [49152/54000 (91%)] Loss: -501360.281250\n",
      "    epoch          : 301\n",
      "    loss           : -460794.82301829266\n",
      "    val_loss       : -461656.653125\n",
      "Train Epoch: 302 [0/54000 (0%)] Loss: -502614.250000\n",
      "Train Epoch: 302 [4096/54000 (8%)] Loss: -453563.343750\n",
      "Train Epoch: 302 [8192/54000 (15%)] Loss: -451570.750000\n",
      "Train Epoch: 302 [12288/54000 (23%)] Loss: -450236.000000\n",
      "Train Epoch: 302 [16384/54000 (30%)] Loss: -465360.187500\n",
      "Train Epoch: 302 [20480/54000 (38%)] Loss: -467903.875000\n",
      "Train Epoch: 302 [24576/54000 (46%)] Loss: -451182.968750\n",
      "Train Epoch: 302 [28672/54000 (53%)] Loss: -455701.437500\n",
      "Train Epoch: 302 [32768/54000 (61%)] Loss: -467139.000000\n",
      "Train Epoch: 302 [36864/54000 (68%)] Loss: -448683.000000\n",
      "Train Epoch: 302 [40960/54000 (76%)] Loss: -455663.843750\n",
      "Train Epoch: 302 [45056/54000 (83%)] Loss: -466563.375000\n",
      "Train Epoch: 302 [49152/54000 (91%)] Loss: -501814.562500\n",
      "    epoch          : 302\n",
      "    loss           : -461344.42469512194\n",
      "    val_loss       : -461226.12265625\n",
      "Train Epoch: 303 [0/54000 (0%)] Loss: -501923.250000\n",
      "Train Epoch: 303 [4096/54000 (8%)] Loss: -457669.875000\n",
      "Train Epoch: 303 [8192/54000 (15%)] Loss: -467826.000000\n",
      "Train Epoch: 303 [12288/54000 (23%)] Loss: -448012.562500\n",
      "Train Epoch: 303 [16384/54000 (30%)] Loss: -461701.281250\n",
      "Train Epoch: 303 [20480/54000 (38%)] Loss: -456574.500000\n",
      "Train Epoch: 303 [24576/54000 (46%)] Loss: -454416.500000\n",
      "Train Epoch: 303 [28672/54000 (53%)] Loss: -502814.437500\n",
      "Train Epoch: 303 [32768/54000 (61%)] Loss: -457055.031250\n",
      "Train Epoch: 303 [36864/54000 (68%)] Loss: -448249.250000\n",
      "Train Epoch: 303 [40960/54000 (76%)] Loss: -462742.468750\n",
      "Train Epoch: 303 [45056/54000 (83%)] Loss: -467259.968750\n",
      "Train Epoch: 303 [49152/54000 (91%)] Loss: -503613.968750\n",
      "    epoch          : 303\n",
      "    loss           : -461276.59268292686\n",
      "    val_loss       : -461326.775\n",
      "Train Epoch: 304 [0/54000 (0%)] Loss: -503639.500000\n",
      "Train Epoch: 304 [4096/54000 (8%)] Loss: -462551.468750\n",
      "Train Epoch: 304 [8192/54000 (15%)] Loss: -453112.812500\n",
      "Train Epoch: 304 [12288/54000 (23%)] Loss: -445286.437500\n",
      "Train Epoch: 304 [16384/54000 (30%)] Loss: -466309.125000\n",
      "Train Epoch: 304 [20480/54000 (38%)] Loss: -467336.656250\n",
      "Train Epoch: 304 [24576/54000 (46%)] Loss: -462441.218750\n",
      "Train Epoch: 304 [28672/54000 (53%)] Loss: -502935.750000\n",
      "Train Epoch: 304 [32768/54000 (61%)] Loss: -467023.750000\n",
      "Train Epoch: 304 [36864/54000 (68%)] Loss: -466381.687500\n",
      "Train Epoch: 304 [40960/54000 (76%)] Loss: -448696.750000\n",
      "Train Epoch: 304 [45056/54000 (83%)] Loss: -461813.500000\n",
      "Train Epoch: 304 [49152/54000 (91%)] Loss: -501762.375000\n",
      "    epoch          : 304\n",
      "    loss           : -461361.75167682924\n",
      "    val_loss       : -461540.19375\n",
      "Train Epoch: 305 [0/54000 (0%)] Loss: -455437.593750\n",
      "Train Epoch: 305 [4096/54000 (8%)] Loss: -464357.281250\n",
      "Train Epoch: 305 [8192/54000 (15%)] Loss: -456525.750000\n",
      "Train Epoch: 305 [12288/54000 (23%)] Loss: -447163.187500\n",
      "Train Epoch: 305 [16384/54000 (30%)] Loss: -466197.562500\n",
      "Train Epoch: 305 [20480/54000 (38%)] Loss: -453742.906250\n",
      "Train Epoch: 305 [24576/54000 (46%)] Loss: -445956.875000\n",
      "Train Epoch: 305 [28672/54000 (53%)] Loss: -454678.031250\n",
      "Train Epoch: 305 [32768/54000 (61%)] Loss: -449661.000000\n",
      "Train Epoch: 305 [36864/54000 (68%)] Loss: -456676.781250\n",
      "Train Epoch: 305 [40960/54000 (76%)] Loss: -466126.875000\n",
      "Train Epoch: 305 [45056/54000 (83%)] Loss: -456578.437500\n",
      "Train Epoch: 305 [49152/54000 (91%)] Loss: -455811.687500\n",
      "    epoch          : 305\n",
      "    loss           : -461412.10426829266\n",
      "    val_loss       : -461451.5890625\n",
      "Train Epoch: 306 [0/54000 (0%)] Loss: -502579.406250\n",
      "Train Epoch: 306 [4096/54000 (8%)] Loss: -456118.875000\n",
      "Train Epoch: 306 [8192/54000 (15%)] Loss: -448858.656250\n",
      "Train Epoch: 306 [12288/54000 (23%)] Loss: -450586.843750\n",
      "Train Epoch: 306 [16384/54000 (30%)] Loss: -502748.312500\n",
      "Train Epoch: 306 [20480/54000 (38%)] Loss: -465049.562500\n",
      "Train Epoch: 306 [24576/54000 (46%)] Loss: -445624.906250\n",
      "Train Epoch: 306 [28672/54000 (53%)] Loss: -455518.062500\n",
      "Train Epoch: 306 [32768/54000 (61%)] Loss: -466697.750000\n",
      "Train Epoch: 306 [36864/54000 (68%)] Loss: -502423.187500\n",
      "Train Epoch: 306 [40960/54000 (76%)] Loss: -457690.406250\n",
      "Train Epoch: 306 [45056/54000 (83%)] Loss: -466588.625000\n",
      "Train Epoch: 306 [49152/54000 (91%)] Loss: -478043.906250\n",
      "    epoch          : 306\n",
      "    loss           : -461177.76173780486\n",
      "    val_loss       : -461274.00703125\n",
      "Train Epoch: 307 [0/54000 (0%)] Loss: -446921.625000\n",
      "Train Epoch: 307 [4096/54000 (8%)] Loss: -459854.500000\n",
      "Train Epoch: 307 [8192/54000 (15%)] Loss: -454189.687500\n",
      "Train Epoch: 307 [12288/54000 (23%)] Loss: -449873.468750\n",
      "Train Epoch: 307 [16384/54000 (30%)] Loss: -467796.156250\n",
      "Train Epoch: 307 [20480/54000 (38%)] Loss: -451097.625000\n",
      "Train Epoch: 307 [24576/54000 (46%)] Loss: -457092.843750\n",
      "Train Epoch: 307 [28672/54000 (53%)] Loss: -502459.500000\n",
      "Train Epoch: 307 [32768/54000 (61%)] Loss: -463539.093750\n",
      "Train Epoch: 307 [36864/54000 (68%)] Loss: -464906.781250\n",
      "Train Epoch: 307 [40960/54000 (76%)] Loss: -451701.937500\n",
      "Train Epoch: 307 [45056/54000 (83%)] Loss: -466806.625000\n",
      "Train Epoch: 307 [49152/54000 (91%)] Loss: -503093.125000\n",
      "    epoch          : 307\n",
      "    loss           : -461329.8977134146\n",
      "    val_loss       : -461137.12578125\n",
      "Train Epoch: 308 [0/54000 (0%)] Loss: -448234.250000\n",
      "Train Epoch: 308 [4096/54000 (8%)] Loss: -450817.937500\n",
      "Train Epoch: 308 [8192/54000 (15%)] Loss: -455828.125000\n",
      "Train Epoch: 308 [12288/54000 (23%)] Loss: -462902.062500\n",
      "Train Epoch: 308 [16384/54000 (30%)] Loss: -466835.250000\n",
      "Train Epoch: 308 [20480/54000 (38%)] Loss: -453740.406250\n",
      "Train Epoch: 308 [24576/54000 (46%)] Loss: -503783.968750\n",
      "Train Epoch: 308 [28672/54000 (53%)] Loss: -502796.250000\n",
      "Train Epoch: 308 [32768/54000 (61%)] Loss: -450819.906250\n",
      "Train Epoch: 308 [36864/54000 (68%)] Loss: -467479.625000\n",
      "Train Epoch: 308 [40960/54000 (76%)] Loss: -449645.187500\n",
      "Train Epoch: 308 [45056/54000 (83%)] Loss: -464383.125000\n",
      "Train Epoch: 308 [49152/54000 (91%)] Loss: -500564.843750\n",
      "    epoch          : 308\n",
      "    loss           : -461335.8669207317\n",
      "    val_loss       : -461223.9890625\n",
      "Train Epoch: 309 [0/54000 (0%)] Loss: -502423.531250\n",
      "Train Epoch: 309 [4096/54000 (8%)] Loss: -457264.625000\n",
      "Train Epoch: 309 [8192/54000 (15%)] Loss: -466465.875000\n",
      "Train Epoch: 309 [12288/54000 (23%)] Loss: -461872.750000\n",
      "Train Epoch: 309 [16384/54000 (30%)] Loss: -454498.000000\n",
      "Train Epoch: 309 [20480/54000 (38%)] Loss: -461851.750000\n",
      "Train Epoch: 309 [24576/54000 (46%)] Loss: -465588.593750\n",
      "Train Epoch: 309 [28672/54000 (53%)] Loss: -454954.250000\n",
      "Train Epoch: 309 [32768/54000 (61%)] Loss: -501777.062500\n",
      "Train Epoch: 309 [36864/54000 (68%)] Loss: -504045.687500\n",
      "Train Epoch: 309 [40960/54000 (76%)] Loss: -462472.593750\n",
      "Train Epoch: 309 [45056/54000 (83%)] Loss: -447202.562500\n",
      "Train Epoch: 309 [49152/54000 (91%)] Loss: -503522.062500\n",
      "    epoch          : 309\n",
      "    loss           : -461481.6326219512\n",
      "    val_loss       : -461433.35\n",
      "Train Epoch: 310 [0/54000 (0%)] Loss: -503033.125000\n",
      "Train Epoch: 310 [4096/54000 (8%)] Loss: -454584.500000\n",
      "Train Epoch: 310 [8192/54000 (15%)] Loss: -445233.000000\n",
      "Train Epoch: 310 [12288/54000 (23%)] Loss: -460924.375000\n",
      "Train Epoch: 310 [16384/54000 (30%)] Loss: -446076.843750\n",
      "Train Epoch: 310 [20480/54000 (38%)] Loss: -455598.718750\n",
      "Train Epoch: 310 [24576/54000 (46%)] Loss: -465520.093750\n",
      "Train Epoch: 310 [28672/54000 (53%)] Loss: -466448.250000\n",
      "Train Epoch: 310 [32768/54000 (61%)] Loss: -467122.187500\n",
      "Train Epoch: 310 [36864/54000 (68%)] Loss: -445641.625000\n",
      "Train Epoch: 310 [40960/54000 (76%)] Loss: -457683.937500\n",
      "Train Epoch: 310 [45056/54000 (83%)] Loss: -467411.093750\n",
      "Train Epoch: 310 [49152/54000 (91%)] Loss: -503286.687500\n",
      "    epoch          : 310\n",
      "    loss           : -461504.2338414634\n",
      "    val_loss       : -461332.02265625\n",
      "Train Epoch: 311 [0/54000 (0%)] Loss: -434493.906250\n",
      "Train Epoch: 311 [4096/54000 (8%)] Loss: -458268.187500\n",
      "Train Epoch: 311 [8192/54000 (15%)] Loss: -448704.093750\n",
      "Train Epoch: 311 [12288/54000 (23%)] Loss: -462625.781250\n",
      "Train Epoch: 311 [16384/54000 (30%)] Loss: -465672.437500\n",
      "Train Epoch: 311 [20480/54000 (38%)] Loss: -456121.937500\n",
      "Train Epoch: 311 [24576/54000 (46%)] Loss: -448847.187500\n",
      "Train Epoch: 311 [28672/54000 (53%)] Loss: -454646.062500\n",
      "Train Epoch: 311 [32768/54000 (61%)] Loss: -446380.906250\n",
      "Train Epoch: 311 [36864/54000 (68%)] Loss: -449697.062500\n",
      "Train Epoch: 311 [40960/54000 (76%)] Loss: -454821.625000\n",
      "Train Epoch: 311 [45056/54000 (83%)] Loss: -451504.000000\n",
      "Train Epoch: 311 [49152/54000 (91%)] Loss: -502783.000000\n",
      "    epoch          : 311\n",
      "    loss           : -461444.337652439\n",
      "    val_loss       : -461593.23671875\n",
      "Train Epoch: 312 [0/54000 (0%)] Loss: -502791.062500\n",
      "Train Epoch: 312 [4096/54000 (8%)] Loss: -447279.187500\n",
      "Train Epoch: 312 [8192/54000 (15%)] Loss: -466716.000000\n",
      "Train Epoch: 312 [12288/54000 (23%)] Loss: -500739.687500\n",
      "Train Epoch: 312 [16384/54000 (30%)] Loss: -449047.500000\n",
      "Train Epoch: 312 [20480/54000 (38%)] Loss: -466185.125000\n",
      "Train Epoch: 312 [24576/54000 (46%)] Loss: -450183.500000\n",
      "Train Epoch: 312 [28672/54000 (53%)] Loss: -503058.187500\n",
      "Train Epoch: 312 [32768/54000 (61%)] Loss: -463347.562500\n",
      "Train Epoch: 312 [36864/54000 (68%)] Loss: -467669.687500\n",
      "Train Epoch: 312 [40960/54000 (76%)] Loss: -461652.406250\n",
      "Train Epoch: 312 [45056/54000 (83%)] Loss: -466699.843750\n",
      "Train Epoch: 312 [49152/54000 (91%)] Loss: -503068.656250\n",
      "    epoch          : 312\n",
      "    loss           : -461580.1489329268\n",
      "    val_loss       : -461448.08046875\n",
      "Train Epoch: 313 [0/54000 (0%)] Loss: -467936.187500\n",
      "Train Epoch: 313 [4096/54000 (8%)] Loss: -446908.125000\n",
      "Train Epoch: 313 [8192/54000 (15%)] Loss: -454168.531250\n",
      "Train Epoch: 313 [12288/54000 (23%)] Loss: -454749.875000\n",
      "Train Epoch: 313 [16384/54000 (30%)] Loss: -446544.812500\n",
      "Train Epoch: 313 [20480/54000 (38%)] Loss: -467841.875000\n",
      "Train Epoch: 313 [24576/54000 (46%)] Loss: -444248.187500\n",
      "Train Epoch: 313 [28672/54000 (53%)] Loss: -456157.187500\n",
      "Train Epoch: 313 [32768/54000 (61%)] Loss: -455951.562500\n",
      "Train Epoch: 313 [36864/54000 (68%)] Loss: -448899.687500\n",
      "Train Epoch: 313 [40960/54000 (76%)] Loss: -457256.062500\n",
      "Train Epoch: 313 [45056/54000 (83%)] Loss: -447749.343750\n",
      "Train Epoch: 313 [49152/54000 (91%)] Loss: -463496.375000\n",
      "    epoch          : 313\n",
      "    loss           : -461483.46707317076\n",
      "    val_loss       : -461402.4125\n",
      "Train Epoch: 314 [0/54000 (0%)] Loss: -458575.031250\n",
      "Train Epoch: 314 [4096/54000 (8%)] Loss: -448379.437500\n",
      "Train Epoch: 314 [8192/54000 (15%)] Loss: -450112.625000\n",
      "Train Epoch: 314 [12288/54000 (23%)] Loss: -455707.562500\n",
      "Train Epoch: 314 [16384/54000 (30%)] Loss: -446920.031250\n",
      "Train Epoch: 314 [20480/54000 (38%)] Loss: -467937.687500\n",
      "Train Epoch: 314 [24576/54000 (46%)] Loss: -454397.343750\n",
      "Train Epoch: 314 [28672/54000 (53%)] Loss: -454474.281250\n",
      "Train Epoch: 314 [32768/54000 (61%)] Loss: -454343.750000\n",
      "Train Epoch: 314 [36864/54000 (68%)] Loss: -500692.812500\n",
      "Train Epoch: 314 [40960/54000 (76%)] Loss: -449430.062500\n",
      "Train Epoch: 314 [45056/54000 (83%)] Loss: -465915.875000\n",
      "Train Epoch: 314 [49152/54000 (91%)] Loss: -502859.718750\n",
      "    epoch          : 314\n",
      "    loss           : -461601.17103658535\n",
      "    val_loss       : -461595.796875\n",
      "Train Epoch: 315 [0/54000 (0%)] Loss: -501972.625000\n",
      "Train Epoch: 315 [4096/54000 (8%)] Loss: -455466.875000\n",
      "Train Epoch: 315 [8192/54000 (15%)] Loss: -448541.531250\n",
      "Train Epoch: 315 [12288/54000 (23%)] Loss: -448449.375000\n",
      "Train Epoch: 315 [16384/54000 (30%)] Loss: -461652.562500\n",
      "Train Epoch: 315 [20480/54000 (38%)] Loss: -466001.906250\n",
      "Train Epoch: 315 [24576/54000 (46%)] Loss: -461210.250000\n",
      "Train Epoch: 315 [28672/54000 (53%)] Loss: -453431.562500\n",
      "Train Epoch: 315 [32768/54000 (61%)] Loss: -466705.875000\n",
      "Train Epoch: 315 [36864/54000 (68%)] Loss: -503298.656250\n",
      "Train Epoch: 315 [40960/54000 (76%)] Loss: -446893.375000\n",
      "Train Epoch: 315 [45056/54000 (83%)] Loss: -455805.125000\n",
      "Train Epoch: 315 [49152/54000 (91%)] Loss: -447644.687500\n",
      "    epoch          : 315\n",
      "    loss           : -461737.70945121953\n",
      "    val_loss       : -461161.7953125\n",
      "Train Epoch: 316 [0/54000 (0%)] Loss: -454110.312500\n",
      "Train Epoch: 316 [4096/54000 (8%)] Loss: -455860.500000\n",
      "Train Epoch: 316 [8192/54000 (15%)] Loss: -457110.437500\n",
      "Train Epoch: 316 [12288/54000 (23%)] Loss: -464210.875000\n",
      "Train Epoch: 316 [16384/54000 (30%)] Loss: -447306.000000\n",
      "Train Epoch: 316 [20480/54000 (38%)] Loss: -467391.156250\n",
      "Train Epoch: 316 [24576/54000 (46%)] Loss: -466931.125000\n",
      "Train Epoch: 316 [28672/54000 (53%)] Loss: -456530.468750\n",
      "Train Epoch: 316 [32768/54000 (61%)] Loss: -502431.968750\n",
      "Train Epoch: 316 [36864/54000 (68%)] Loss: -465199.281250\n",
      "Train Epoch: 316 [40960/54000 (76%)] Loss: -455413.093750\n",
      "Train Epoch: 316 [45056/54000 (83%)] Loss: -444781.625000\n",
      "Train Epoch: 316 [49152/54000 (91%)] Loss: -504043.812500\n",
      "    epoch          : 316\n",
      "    loss           : -461484.37164634146\n",
      "    val_loss       : -461549.7046875\n",
      "Train Epoch: 317 [0/54000 (0%)] Loss: -503117.750000\n",
      "Train Epoch: 317 [4096/54000 (8%)] Loss: -445212.562500\n",
      "Train Epoch: 317 [8192/54000 (15%)] Loss: -466444.062500\n",
      "Train Epoch: 317 [12288/54000 (23%)] Loss: -456254.718750\n",
      "Train Epoch: 317 [16384/54000 (30%)] Loss: -503990.250000\n",
      "Train Epoch: 317 [20480/54000 (38%)] Loss: -467481.625000\n",
      "Train Epoch: 317 [24576/54000 (46%)] Loss: -447760.625000\n",
      "Train Epoch: 317 [28672/54000 (53%)] Loss: -455907.625000\n",
      "Train Epoch: 317 [32768/54000 (61%)] Loss: -455515.375000\n",
      "Train Epoch: 317 [36864/54000 (68%)] Loss: -449284.875000\n",
      "Train Epoch: 317 [40960/54000 (76%)] Loss: -454853.187500\n",
      "Train Epoch: 317 [45056/54000 (83%)] Loss: -467371.500000\n",
      "Train Epoch: 317 [49152/54000 (91%)] Loss: -502424.875000\n",
      "    epoch          : 317\n",
      "    loss           : -461740.89451219514\n",
      "    val_loss       : -461755.6828125\n",
      "Train Epoch: 318 [0/54000 (0%)] Loss: -503415.500000\n",
      "Train Epoch: 318 [4096/54000 (8%)] Loss: -449636.875000\n",
      "Train Epoch: 318 [8192/54000 (15%)] Loss: -466913.093750\n",
      "Train Epoch: 318 [12288/54000 (23%)] Loss: -464707.937500\n",
      "Train Epoch: 318 [16384/54000 (30%)] Loss: -450843.875000\n",
      "Train Epoch: 318 [20480/54000 (38%)] Loss: -467533.031250\n",
      "Train Epoch: 318 [24576/54000 (46%)] Loss: -446281.750000\n",
      "Train Epoch: 318 [28672/54000 (53%)] Loss: -447402.750000\n",
      "Train Epoch: 318 [32768/54000 (61%)] Loss: -465873.750000\n",
      "Train Epoch: 318 [36864/54000 (68%)] Loss: -439521.187500\n",
      "Train Epoch: 318 [40960/54000 (76%)] Loss: -461033.000000\n",
      "Train Epoch: 318 [45056/54000 (83%)] Loss: -445952.593750\n",
      "Train Epoch: 318 [49152/54000 (91%)] Loss: -501863.937500\n",
      "    epoch          : 318\n",
      "    loss           : -461611.84603658534\n",
      "    val_loss       : -461417.56484375\n",
      "Train Epoch: 319 [0/54000 (0%)] Loss: -500965.750000\n",
      "Train Epoch: 319 [4096/54000 (8%)] Loss: -468180.156250\n",
      "Train Epoch: 319 [8192/54000 (15%)] Loss: -462940.562500\n",
      "Train Epoch: 319 [12288/54000 (23%)] Loss: -447194.781250\n",
      "Train Epoch: 319 [16384/54000 (30%)] Loss: -503414.812500\n",
      "Train Epoch: 319 [20480/54000 (38%)] Loss: -454348.125000\n",
      "Train Epoch: 319 [24576/54000 (46%)] Loss: -450949.000000\n",
      "Train Epoch: 319 [28672/54000 (53%)] Loss: -466192.437500\n",
      "Train Epoch: 319 [32768/54000 (61%)] Loss: -461252.187500\n",
      "Train Epoch: 319 [36864/54000 (68%)] Loss: -466641.375000\n",
      "Train Epoch: 319 [40960/54000 (76%)] Loss: -447402.625000\n",
      "Train Epoch: 319 [45056/54000 (83%)] Loss: -445559.812500\n",
      "Train Epoch: 319 [49152/54000 (91%)] Loss: -454682.562500\n",
      "    epoch          : 319\n",
      "    loss           : -461762.1713414634\n",
      "    val_loss       : -461384.26484375\n",
      "Train Epoch: 320 [0/54000 (0%)] Loss: -454669.687500\n",
      "Train Epoch: 320 [4096/54000 (8%)] Loss: -468346.531250\n",
      "Train Epoch: 320 [8192/54000 (15%)] Loss: -448920.593750\n",
      "Train Epoch: 320 [12288/54000 (23%)] Loss: -467653.937500\n",
      "Train Epoch: 320 [16384/54000 (30%)] Loss: -449068.687500\n",
      "Train Epoch: 320 [20480/54000 (38%)] Loss: -449381.812500\n",
      "Train Epoch: 320 [24576/54000 (46%)] Loss: -455143.062500\n",
      "Train Epoch: 320 [28672/54000 (53%)] Loss: -467858.562500\n",
      "Train Epoch: 320 [32768/54000 (61%)] Loss: -503087.687500\n",
      "Train Epoch: 320 [36864/54000 (68%)] Loss: -466596.593750\n",
      "Train Epoch: 320 [40960/54000 (76%)] Loss: -453489.250000\n",
      "Train Epoch: 320 [45056/54000 (83%)] Loss: -450062.687500\n",
      "Train Epoch: 320 [49152/54000 (91%)] Loss: -501542.625000\n",
      "    epoch          : 320\n",
      "    loss           : -461584.90731707314\n",
      "    val_loss       : -461575.75\n",
      "Train Epoch: 321 [0/54000 (0%)] Loss: -502366.281250\n",
      "Train Epoch: 321 [4096/54000 (8%)] Loss: -466909.531250\n",
      "Train Epoch: 321 [8192/54000 (15%)] Loss: -455303.125000\n",
      "Train Epoch: 321 [12288/54000 (23%)] Loss: -455557.093750\n",
      "Train Epoch: 321 [16384/54000 (30%)] Loss: -447149.062500\n",
      "Train Epoch: 321 [20480/54000 (38%)] Loss: -466845.187500\n",
      "Train Epoch: 321 [24576/54000 (46%)] Loss: -441108.687500\n",
      "Train Epoch: 321 [28672/54000 (53%)] Loss: -453108.375000\n",
      "Train Epoch: 321 [32768/54000 (61%)] Loss: -449628.718750\n",
      "Train Epoch: 321 [36864/54000 (68%)] Loss: -467917.656250\n",
      "Train Epoch: 321 [40960/54000 (76%)] Loss: -448138.750000\n",
      "Train Epoch: 321 [45056/54000 (83%)] Loss: -466277.937500\n",
      "Train Epoch: 321 [49152/54000 (91%)] Loss: -496517.937500\n",
      "    epoch          : 321\n",
      "    loss           : -461772.3513719512\n",
      "    val_loss       : -461417.01640625\n",
      "Train Epoch: 322 [0/54000 (0%)] Loss: -502954.750000\n",
      "Train Epoch: 322 [4096/54000 (8%)] Loss: -468448.875000\n",
      "Train Epoch: 322 [8192/54000 (15%)] Loss: -450834.468750\n",
      "Train Epoch: 322 [12288/54000 (23%)] Loss: -446364.593750\n",
      "Train Epoch: 322 [16384/54000 (30%)] Loss: -449972.062500\n",
      "Train Epoch: 322 [20480/54000 (38%)] Loss: -449726.812500\n",
      "Train Epoch: 322 [24576/54000 (46%)] Loss: -449348.625000\n",
      "Train Epoch: 322 [28672/54000 (53%)] Loss: -468425.937500\n",
      "Train Epoch: 322 [32768/54000 (61%)] Loss: -455998.812500\n",
      "Train Epoch: 322 [36864/54000 (68%)] Loss: -447032.562500\n",
      "Train Epoch: 322 [40960/54000 (76%)] Loss: -456544.125000\n",
      "Train Epoch: 322 [45056/54000 (83%)] Loss: -466824.156250\n",
      "Train Epoch: 322 [49152/54000 (91%)] Loss: -446431.750000\n",
      "    epoch          : 322\n",
      "    loss           : -461857.7211890244\n",
      "    val_loss       : -461742.9015625\n",
      "Train Epoch: 323 [0/54000 (0%)] Loss: -503119.062500\n",
      "Train Epoch: 323 [4096/54000 (8%)] Loss: -448843.625000\n",
      "Train Epoch: 323 [8192/54000 (15%)] Loss: -466071.343750\n",
      "Train Epoch: 323 [12288/54000 (23%)] Loss: -448755.312500\n",
      "Train Epoch: 323 [16384/54000 (30%)] Loss: -451111.500000\n",
      "Train Epoch: 323 [20480/54000 (38%)] Loss: -467011.656250\n",
      "Train Epoch: 323 [24576/54000 (46%)] Loss: -456870.750000\n",
      "Train Epoch: 323 [28672/54000 (53%)] Loss: -503611.625000\n",
      "Train Epoch: 323 [32768/54000 (61%)] Loss: -448454.875000\n",
      "Train Epoch: 323 [36864/54000 (68%)] Loss: -466631.937500\n",
      "Train Epoch: 323 [40960/54000 (76%)] Loss: -447127.343750\n",
      "Train Epoch: 323 [45056/54000 (83%)] Loss: -466506.375000\n",
      "Train Epoch: 323 [49152/54000 (91%)] Loss: -503078.562500\n",
      "    epoch          : 323\n",
      "    loss           : -461800.7969512195\n",
      "    val_loss       : -461232.3828125\n",
      "Train Epoch: 324 [0/54000 (0%)] Loss: -467520.812500\n",
      "Train Epoch: 324 [4096/54000 (8%)] Loss: -447219.437500\n",
      "Train Epoch: 324 [8192/54000 (15%)] Loss: -455996.750000\n",
      "Train Epoch: 324 [12288/54000 (23%)] Loss: -449853.968750\n",
      "Train Epoch: 324 [16384/54000 (30%)] Loss: -447667.906250\n",
      "Train Epoch: 324 [20480/54000 (38%)] Loss: -453583.687500\n",
      "Train Epoch: 324 [24576/54000 (46%)] Loss: -467081.718750\n",
      "Train Epoch: 324 [28672/54000 (53%)] Loss: -453311.593750\n",
      "Train Epoch: 324 [32768/54000 (61%)] Loss: -448911.187500\n",
      "Train Epoch: 324 [36864/54000 (68%)] Loss: -504329.031250\n",
      "Train Epoch: 324 [40960/54000 (76%)] Loss: -467081.875000\n",
      "Train Epoch: 324 [45056/54000 (83%)] Loss: -455871.187500\n",
      "Train Epoch: 324 [49152/54000 (91%)] Loss: -503435.843750\n",
      "    epoch          : 324\n",
      "    loss           : -461898.9609756098\n",
      "    val_loss       : -461585.665625\n",
      "Train Epoch: 325 [0/54000 (0%)] Loss: -462911.875000\n",
      "Train Epoch: 325 [4096/54000 (8%)] Loss: -447844.593750\n",
      "Train Epoch: 325 [8192/54000 (15%)] Loss: -468160.937500\n",
      "Train Epoch: 325 [12288/54000 (23%)] Loss: -456718.250000\n",
      "Train Epoch: 325 [16384/54000 (30%)] Loss: -463604.437500\n",
      "Train Epoch: 325 [20480/54000 (38%)] Loss: -454905.562500\n",
      "Train Epoch: 325 [24576/54000 (46%)] Loss: -447765.750000\n",
      "Train Epoch: 325 [28672/54000 (53%)] Loss: -503119.125000\n",
      "Train Epoch: 325 [32768/54000 (61%)] Loss: -448947.093750\n",
      "Train Epoch: 325 [36864/54000 (68%)] Loss: -448186.687500\n",
      "Train Epoch: 325 [40960/54000 (76%)] Loss: -448733.375000\n",
      "Train Epoch: 325 [45056/54000 (83%)] Loss: -466499.781250\n",
      "Train Epoch: 325 [49152/54000 (91%)] Loss: -502016.437500\n",
      "    epoch          : 325\n",
      "    loss           : -461884.2836890244\n",
      "    val_loss       : -461515.36953125\n",
      "Train Epoch: 326 [0/54000 (0%)] Loss: -503419.250000\n",
      "Train Epoch: 326 [4096/54000 (8%)] Loss: -447613.937500\n",
      "Train Epoch: 326 [8192/54000 (15%)] Loss: -454145.625000\n",
      "Train Epoch: 326 [12288/54000 (23%)] Loss: -450281.625000\n",
      "Train Epoch: 326 [16384/54000 (30%)] Loss: -460223.875000\n",
      "Train Epoch: 326 [20480/54000 (38%)] Loss: -466336.187500\n",
      "Train Epoch: 326 [24576/54000 (46%)] Loss: -448430.937500\n",
      "Train Epoch: 326 [28672/54000 (53%)] Loss: -502617.187500\n",
      "Train Epoch: 326 [32768/54000 (61%)] Loss: -457025.312500\n",
      "Train Epoch: 326 [36864/54000 (68%)] Loss: -448664.156250\n",
      "Train Epoch: 326 [40960/54000 (76%)] Loss: -455534.437500\n",
      "Train Epoch: 326 [45056/54000 (83%)] Loss: -449568.000000\n",
      "Train Epoch: 326 [49152/54000 (91%)] Loss: -500342.125000\n",
      "    epoch          : 326\n",
      "    loss           : -461797.90853658534\n",
      "    val_loss       : -461709.33046875\n",
      "Train Epoch: 327 [0/54000 (0%)] Loss: -503085.218750\n",
      "Train Epoch: 327 [4096/54000 (8%)] Loss: -445085.062500\n",
      "Train Epoch: 327 [8192/54000 (15%)] Loss: -466180.843750\n",
      "Train Epoch: 327 [12288/54000 (23%)] Loss: -461828.125000\n",
      "Train Epoch: 327 [16384/54000 (30%)] Loss: -458217.781250\n",
      "Train Epoch: 327 [20480/54000 (38%)] Loss: -468147.625000\n",
      "Train Epoch: 327 [24576/54000 (46%)] Loss: -451090.375000\n",
      "Train Epoch: 327 [28672/54000 (53%)] Loss: -502515.625000\n",
      "Train Epoch: 327 [32768/54000 (61%)] Loss: -456967.187500\n",
      "Train Epoch: 327 [36864/54000 (68%)] Loss: -448132.687500\n",
      "Train Epoch: 327 [40960/54000 (76%)] Loss: -454536.937500\n",
      "Train Epoch: 327 [45056/54000 (83%)] Loss: -456192.062500\n",
      "Train Epoch: 327 [49152/54000 (91%)] Loss: -504110.250000\n",
      "    epoch          : 327\n",
      "    loss           : -461842.07545731706\n",
      "    val_loss       : -461261.28359375\n",
      "Train Epoch: 328 [0/54000 (0%)] Loss: -453560.312500\n",
      "Train Epoch: 328 [4096/54000 (8%)] Loss: -450359.750000\n",
      "Train Epoch: 328 [8192/54000 (15%)] Loss: -449116.062500\n",
      "Train Epoch: 328 [12288/54000 (23%)] Loss: -461308.750000\n",
      "Train Epoch: 328 [16384/54000 (30%)] Loss: -455932.375000\n",
      "Train Epoch: 328 [20480/54000 (38%)] Loss: -466766.187500\n",
      "Train Epoch: 328 [24576/54000 (46%)] Loss: -454537.375000\n",
      "Train Epoch: 328 [28672/54000 (53%)] Loss: -447644.406250\n",
      "Train Epoch: 328 [32768/54000 (61%)] Loss: -449837.875000\n",
      "Train Epoch: 328 [36864/54000 (68%)] Loss: -467055.250000\n",
      "Train Epoch: 328 [40960/54000 (76%)] Loss: -469234.156250\n",
      "Train Epoch: 328 [45056/54000 (83%)] Loss: -466376.875000\n",
      "Train Epoch: 328 [49152/54000 (91%)] Loss: -456578.343750\n",
      "    epoch          : 328\n",
      "    loss           : -462008.6951219512\n",
      "    val_loss       : -461844.50625\n",
      "Train Epoch: 329 [0/54000 (0%)] Loss: -503207.187500\n",
      "Train Epoch: 329 [4096/54000 (8%)] Loss: -456578.093750\n",
      "Train Epoch: 329 [8192/54000 (15%)] Loss: -467665.750000\n",
      "Train Epoch: 329 [12288/54000 (23%)] Loss: -449352.875000\n",
      "Train Epoch: 329 [16384/54000 (30%)] Loss: -448672.218750\n",
      "Train Epoch: 329 [20480/54000 (38%)] Loss: -467782.593750\n",
      "Train Epoch: 329 [24576/54000 (46%)] Loss: -449946.250000\n",
      "Train Epoch: 329 [28672/54000 (53%)] Loss: -455084.781250\n",
      "Train Epoch: 329 [32768/54000 (61%)] Loss: -445407.000000\n",
      "Train Epoch: 329 [36864/54000 (68%)] Loss: -467241.875000\n",
      "Train Epoch: 329 [40960/54000 (76%)] Loss: -454129.562500\n",
      "Train Epoch: 329 [45056/54000 (83%)] Loss: -468193.250000\n",
      "Train Epoch: 329 [49152/54000 (91%)] Loss: -502641.750000\n",
      "    epoch          : 329\n",
      "    loss           : -462065.7422256098\n",
      "    val_loss       : -461629.92890625\n",
      "Train Epoch: 330 [0/54000 (0%)] Loss: -447262.625000\n",
      "Train Epoch: 330 [4096/54000 (8%)] Loss: -468350.875000\n",
      "Train Epoch: 330 [8192/54000 (15%)] Loss: -455430.156250\n",
      "Train Epoch: 330 [12288/54000 (23%)] Loss: -447695.250000\n",
      "Train Epoch: 330 [16384/54000 (30%)] Loss: -448974.750000\n",
      "Train Epoch: 330 [20480/54000 (38%)] Loss: -468310.187500\n",
      "Train Epoch: 330 [24576/54000 (46%)] Loss: -467177.343750\n",
      "Train Epoch: 330 [28672/54000 (53%)] Loss: -455101.343750\n",
      "Train Epoch: 330 [32768/54000 (61%)] Loss: -448003.281250\n",
      "Train Epoch: 330 [36864/54000 (68%)] Loss: -467505.937500\n",
      "Train Epoch: 330 [40960/54000 (76%)] Loss: -448036.343750\n",
      "Train Epoch: 330 [45056/54000 (83%)] Loss: -452439.437500\n",
      "Train Epoch: 330 [49152/54000 (91%)] Loss: -449748.000000\n",
      "    epoch          : 330\n",
      "    loss           : -462173.28033536585\n",
      "    val_loss       : -461515.74921875\n",
      "Train Epoch: 331 [0/54000 (0%)] Loss: -467570.437500\n",
      "Train Epoch: 331 [4096/54000 (8%)] Loss: -464752.812500\n",
      "Train Epoch: 331 [8192/54000 (15%)] Loss: -455236.812500\n",
      "Train Epoch: 331 [12288/54000 (23%)] Loss: -454503.718750\n",
      "Train Epoch: 331 [16384/54000 (30%)] Loss: -448029.437500\n",
      "Train Epoch: 331 [20480/54000 (38%)] Loss: -467139.562500\n",
      "Train Epoch: 331 [24576/54000 (46%)] Loss: -448939.343750\n",
      "Train Epoch: 331 [28672/54000 (53%)] Loss: -454640.125000\n",
      "Train Epoch: 331 [32768/54000 (61%)] Loss: -503264.437500\n",
      "Train Epoch: 331 [36864/54000 (68%)] Loss: -466650.093750\n",
      "Train Epoch: 331 [40960/54000 (76%)] Loss: -456436.312500\n",
      "Train Epoch: 331 [45056/54000 (83%)] Loss: -465821.437500\n",
      "Train Epoch: 331 [49152/54000 (91%)] Loss: -503280.437500\n",
      "    epoch          : 331\n",
      "    loss           : -462035.2993902439\n",
      "    val_loss       : -461834.62265625\n",
      "Train Epoch: 332 [0/54000 (0%)] Loss: -502685.343750\n",
      "Train Epoch: 332 [4096/54000 (8%)] Loss: -449096.500000\n",
      "Train Epoch: 332 [8192/54000 (15%)] Loss: -456397.312500\n",
      "Train Epoch: 332 [12288/54000 (23%)] Loss: -466425.656250\n",
      "Train Epoch: 332 [16384/54000 (30%)] Loss: -462841.406250\n",
      "Train Epoch: 332 [20480/54000 (38%)] Loss: -467600.937500\n",
      "Train Epoch: 332 [24576/54000 (46%)] Loss: -450029.812500\n",
      "Train Epoch: 332 [28672/54000 (53%)] Loss: -453665.468750\n",
      "Train Epoch: 332 [32768/54000 (61%)] Loss: -503244.656250\n",
      "Train Epoch: 332 [36864/54000 (68%)] Loss: -448005.968750\n",
      "Train Epoch: 332 [40960/54000 (76%)] Loss: -463337.437500\n",
      "Train Epoch: 332 [45056/54000 (83%)] Loss: -467697.750000\n",
      "Train Epoch: 332 [49152/54000 (91%)] Loss: -501647.437500\n",
      "    epoch          : 332\n",
      "    loss           : -462195.76097560977\n",
      "    val_loss       : -461308.759375\n",
      "Train Epoch: 333 [0/54000 (0%)] Loss: -461721.906250\n",
      "Train Epoch: 333 [4096/54000 (8%)] Loss: -463166.937500\n",
      "Train Epoch: 333 [8192/54000 (15%)] Loss: -456961.875000\n",
      "Train Epoch: 333 [12288/54000 (23%)] Loss: -464097.406250\n",
      "Train Epoch: 333 [16384/54000 (30%)] Loss: -467077.437500\n",
      "Train Epoch: 333 [20480/54000 (38%)] Loss: -468258.281250\n",
      "Train Epoch: 333 [24576/54000 (46%)] Loss: -457032.125000\n",
      "Train Epoch: 333 [28672/54000 (53%)] Loss: -503024.250000\n",
      "Train Epoch: 333 [32768/54000 (61%)] Loss: -458588.718750\n",
      "Train Epoch: 333 [36864/54000 (68%)] Loss: -448529.843750\n",
      "Train Epoch: 333 [40960/54000 (76%)] Loss: -464931.468750\n",
      "Train Epoch: 333 [45056/54000 (83%)] Loss: -468212.187500\n",
      "Train Epoch: 333 [49152/54000 (91%)] Loss: -503209.875000\n",
      "    epoch          : 333\n",
      "    loss           : -462225.2745426829\n",
      "    val_loss       : -461872.52734375\n",
      "Train Epoch: 334 [0/54000 (0%)] Loss: -503217.812500\n",
      "Train Epoch: 334 [4096/54000 (8%)] Loss: -467470.250000\n",
      "Train Epoch: 334 [8192/54000 (15%)] Loss: -466827.500000\n",
      "Train Epoch: 334 [12288/54000 (23%)] Loss: -446805.281250\n",
      "Train Epoch: 334 [16384/54000 (30%)] Loss: -462764.531250\n",
      "Train Epoch: 334 [20480/54000 (38%)] Loss: -453933.125000\n",
      "Train Epoch: 334 [24576/54000 (46%)] Loss: -449823.437500\n",
      "Train Epoch: 334 [28672/54000 (53%)] Loss: -465582.750000\n",
      "Train Epoch: 334 [32768/54000 (61%)] Loss: -455593.250000\n",
      "Train Epoch: 334 [36864/54000 (68%)] Loss: -469319.031250\n",
      "Train Epoch: 334 [40960/54000 (76%)] Loss: -454558.625000\n",
      "Train Epoch: 334 [45056/54000 (83%)] Loss: -449791.468750\n",
      "Train Epoch: 334 [49152/54000 (91%)] Loss: -502612.718750\n",
      "    epoch          : 334\n",
      "    loss           : -462209.7475609756\n",
      "    val_loss       : -461569.9171875\n",
      "Train Epoch: 335 [0/54000 (0%)] Loss: -503266.031250\n",
      "Train Epoch: 335 [4096/54000 (8%)] Loss: -457201.312500\n",
      "Train Epoch: 335 [8192/54000 (15%)] Loss: -470396.000000\n",
      "Train Epoch: 335 [12288/54000 (23%)] Loss: -457448.968750\n",
      "Train Epoch: 335 [16384/54000 (30%)] Loss: -503323.593750\n",
      "Train Epoch: 335 [20480/54000 (38%)] Loss: -465511.875000\n",
      "Train Epoch: 335 [24576/54000 (46%)] Loss: -456347.062500\n",
      "Train Epoch: 335 [28672/54000 (53%)] Loss: -456717.718750\n",
      "Train Epoch: 335 [32768/54000 (61%)] Loss: -457360.437500\n",
      "Train Epoch: 335 [36864/54000 (68%)] Loss: -467679.437500\n",
      "Train Epoch: 335 [40960/54000 (76%)] Loss: -454152.093750\n",
      "Train Epoch: 335 [45056/54000 (83%)] Loss: -454658.687500\n",
      "Train Epoch: 335 [49152/54000 (91%)] Loss: -463333.218750\n",
      "    epoch          : 335\n",
      "    loss           : -462132.3394817073\n",
      "    val_loss       : -461314.80234375\n",
      "Train Epoch: 336 [0/54000 (0%)] Loss: -465068.562500\n",
      "Train Epoch: 336 [4096/54000 (8%)] Loss: -448317.531250\n",
      "Train Epoch: 336 [8192/54000 (15%)] Loss: -456212.312500\n",
      "Train Epoch: 336 [12288/54000 (23%)] Loss: -467619.187500\n",
      "Train Epoch: 336 [16384/54000 (30%)] Loss: -447376.875000\n",
      "Train Epoch: 336 [20480/54000 (38%)] Loss: -466448.187500\n",
      "Train Epoch: 336 [24576/54000 (46%)] Loss: -503400.937500\n",
      "Train Epoch: 336 [28672/54000 (53%)] Loss: -503301.906250\n",
      "Train Epoch: 336 [32768/54000 (61%)] Loss: -449032.968750\n",
      "Train Epoch: 336 [36864/54000 (68%)] Loss: -460315.875000\n",
      "Train Epoch: 336 [40960/54000 (76%)] Loss: -457384.750000\n",
      "Train Epoch: 336 [45056/54000 (83%)] Loss: -467850.937500\n",
      "Train Epoch: 336 [49152/54000 (91%)] Loss: -504282.343750\n",
      "    epoch          : 336\n",
      "    loss           : -462241.1736280488\n",
      "    val_loss       : -461482.0578125\n",
      "Train Epoch: 337 [0/54000 (0%)] Loss: -503374.000000\n",
      "Train Epoch: 337 [4096/54000 (8%)] Loss: -457397.750000\n",
      "Train Epoch: 337 [8192/54000 (15%)] Loss: -466742.343750\n",
      "Train Epoch: 337 [12288/54000 (23%)] Loss: -448617.218750\n",
      "Train Epoch: 337 [16384/54000 (30%)] Loss: -455925.968750\n",
      "Train Epoch: 337 [20480/54000 (38%)] Loss: -503422.062500\n",
      "Train Epoch: 337 [24576/54000 (46%)] Loss: -457549.937500\n",
      "Train Epoch: 337 [28672/54000 (53%)] Loss: -469301.000000\n",
      "Train Epoch: 337 [32768/54000 (61%)] Loss: -450794.718750\n",
      "Train Epoch: 337 [36864/54000 (68%)] Loss: -450940.562500\n",
      "Train Epoch: 337 [40960/54000 (76%)] Loss: -466293.843750\n",
      "Train Epoch: 337 [45056/54000 (83%)] Loss: -449697.812500\n",
      "Train Epoch: 337 [49152/54000 (91%)] Loss: -502714.093750\n",
      "    epoch          : 337\n",
      "    loss           : -462320.7945121951\n",
      "    val_loss       : -461890.04921875\n",
      "Train Epoch: 338 [0/54000 (0%)] Loss: -502640.093750\n",
      "Train Epoch: 338 [4096/54000 (8%)] Loss: -448155.031250\n",
      "Train Epoch: 338 [8192/54000 (15%)] Loss: -456671.062500\n",
      "Train Epoch: 338 [12288/54000 (23%)] Loss: -451550.656250\n",
      "Train Epoch: 338 [16384/54000 (30%)] Loss: -466758.531250\n",
      "Train Epoch: 338 [20480/54000 (38%)] Loss: -456321.625000\n",
      "Train Epoch: 338 [24576/54000 (46%)] Loss: -464871.968750\n",
      "Train Epoch: 338 [28672/54000 (53%)] Loss: -503613.562500\n",
      "Train Epoch: 338 [32768/54000 (61%)] Loss: -468696.593750\n",
      "Train Epoch: 338 [36864/54000 (68%)] Loss: -466777.625000\n",
      "Train Epoch: 338 [40960/54000 (76%)] Loss: -457812.750000\n",
      "Train Epoch: 338 [45056/54000 (83%)] Loss: -455272.312500\n",
      "Train Epoch: 338 [49152/54000 (91%)] Loss: -503230.687500\n",
      "    epoch          : 338\n",
      "    loss           : -462349.5945121951\n",
      "    val_loss       : -461700.41328125\n",
      "Train Epoch: 339 [0/54000 (0%)] Loss: -502486.937500\n",
      "Train Epoch: 339 [4096/54000 (8%)] Loss: -455301.500000\n",
      "Train Epoch: 339 [8192/54000 (15%)] Loss: -456433.375000\n",
      "Train Epoch: 339 [12288/54000 (23%)] Loss: -502649.312500\n",
      "Train Epoch: 339 [16384/54000 (30%)] Loss: -466754.250000\n",
      "Train Epoch: 339 [20480/54000 (38%)] Loss: -467028.468750\n",
      "Train Epoch: 339 [24576/54000 (46%)] Loss: -448372.875000\n",
      "Train Epoch: 339 [28672/54000 (53%)] Loss: -456190.593750\n",
      "Train Epoch: 339 [32768/54000 (61%)] Loss: -463285.062500\n",
      "Train Epoch: 339 [36864/54000 (68%)] Loss: -450423.468750\n",
      "Train Epoch: 339 [40960/54000 (76%)] Loss: -452717.656250\n",
      "Train Epoch: 339 [45056/54000 (83%)] Loss: -466156.656250\n",
      "Train Epoch: 339 [49152/54000 (91%)] Loss: -502590.937500\n",
      "    epoch          : 339\n",
      "    loss           : -462116.5699695122\n",
      "    val_loss       : -462198.7625\n",
      "Train Epoch: 340 [0/54000 (0%)] Loss: -502049.593750\n",
      "Train Epoch: 340 [4096/54000 (8%)] Loss: -450544.031250\n",
      "Train Epoch: 340 [8192/54000 (15%)] Loss: -467063.000000\n",
      "Train Epoch: 340 [12288/54000 (23%)] Loss: -455630.062500\n",
      "Train Epoch: 340 [16384/54000 (30%)] Loss: -465271.218750\n",
      "Train Epoch: 340 [20480/54000 (38%)] Loss: -467758.187500\n",
      "Train Epoch: 340 [24576/54000 (46%)] Loss: -449971.687500\n",
      "Train Epoch: 340 [28672/54000 (53%)] Loss: -455486.281250\n",
      "Train Epoch: 340 [32768/54000 (61%)] Loss: -464181.437500\n",
      "Train Epoch: 340 [36864/54000 (68%)] Loss: -449904.625000\n",
      "Train Epoch: 340 [40960/54000 (76%)] Loss: -461969.312500\n",
      "Train Epoch: 340 [45056/54000 (83%)] Loss: -466698.312500\n",
      "Train Epoch: 340 [49152/54000 (91%)] Loss: -503254.968750\n",
      "    epoch          : 340\n",
      "    loss           : -462073.18414634146\n",
      "    val_loss       : -461920.2359375\n",
      "Train Epoch: 341 [0/54000 (0%)] Loss: -456546.187500\n",
      "Train Epoch: 341 [4096/54000 (8%)] Loss: -459078.281250\n",
      "Train Epoch: 341 [8192/54000 (15%)] Loss: -455889.500000\n",
      "Train Epoch: 341 [12288/54000 (23%)] Loss: -457309.875000\n",
      "Train Epoch: 341 [16384/54000 (30%)] Loss: -446906.218750\n",
      "Train Epoch: 341 [20480/54000 (38%)] Loss: -468364.593750\n",
      "Train Epoch: 341 [24576/54000 (46%)] Loss: -456985.125000\n",
      "Train Epoch: 341 [28672/54000 (53%)] Loss: -502548.187500\n",
      "Train Epoch: 341 [32768/54000 (61%)] Loss: -459264.000000\n",
      "Train Epoch: 341 [36864/54000 (68%)] Loss: -467184.562500\n",
      "Train Epoch: 341 [40960/54000 (76%)] Loss: -456477.250000\n",
      "Train Epoch: 341 [45056/54000 (83%)] Loss: -466937.125000\n",
      "Train Epoch: 341 [49152/54000 (91%)] Loss: -503308.312500\n",
      "    epoch          : 341\n",
      "    loss           : -462288.96524390246\n",
      "    val_loss       : -461501.928125\n",
      "Train Epoch: 342 [0/54000 (0%)] Loss: -449137.375000\n",
      "Train Epoch: 342 [4096/54000 (8%)] Loss: -464585.000000\n",
      "Train Epoch: 342 [8192/54000 (15%)] Loss: -455909.812500\n",
      "Train Epoch: 342 [12288/54000 (23%)] Loss: -448713.250000\n",
      "Train Epoch: 342 [16384/54000 (30%)] Loss: -453449.187500\n",
      "Train Epoch: 342 [20480/54000 (38%)] Loss: -449235.000000\n",
      "Train Epoch: 342 [24576/54000 (46%)] Loss: -463476.125000\n",
      "Train Epoch: 342 [28672/54000 (53%)] Loss: -456433.937500\n",
      "Train Epoch: 342 [32768/54000 (61%)] Loss: -463973.093750\n",
      "Train Epoch: 342 [36864/54000 (68%)] Loss: -503640.687500\n",
      "Train Epoch: 342 [40960/54000 (76%)] Loss: -447950.718750\n",
      "Train Epoch: 342 [45056/54000 (83%)] Loss: -469652.187500\n",
      "Train Epoch: 342 [49152/54000 (91%)] Loss: -501948.656250\n",
      "    epoch          : 342\n",
      "    loss           : -462369.2417682927\n",
      "    val_loss       : -461794.46953125\n",
      "Train Epoch: 343 [0/54000 (0%)] Loss: -456340.718750\n",
      "Train Epoch: 343 [4096/54000 (8%)] Loss: -466898.125000\n",
      "Train Epoch: 343 [8192/54000 (15%)] Loss: -467532.281250\n",
      "Train Epoch: 343 [12288/54000 (23%)] Loss: -455612.468750\n",
      "Train Epoch: 343 [16384/54000 (30%)] Loss: -449825.656250\n",
      "Train Epoch: 343 [20480/54000 (38%)] Loss: -459703.250000\n",
      "Train Epoch: 343 [24576/54000 (46%)] Loss: -448459.218750\n",
      "Train Epoch: 343 [28672/54000 (53%)] Loss: -502473.500000\n",
      "Train Epoch: 343 [32768/54000 (61%)] Loss: -468563.437500\n",
      "Train Epoch: 343 [36864/54000 (68%)] Loss: -447814.375000\n",
      "Train Epoch: 343 [40960/54000 (76%)] Loss: -463383.375000\n",
      "Train Epoch: 343 [45056/54000 (83%)] Loss: -469274.000000\n",
      "Train Epoch: 343 [49152/54000 (91%)] Loss: -503037.000000\n",
      "    epoch          : 343\n",
      "    loss           : -462507.6282012195\n",
      "    val_loss       : -461857.01796875\n",
      "Train Epoch: 344 [0/54000 (0%)] Loss: -503531.187500\n",
      "Train Epoch: 344 [4096/54000 (8%)] Loss: -468160.593750\n",
      "Train Epoch: 344 [8192/54000 (15%)] Loss: -455666.437500\n",
      "Train Epoch: 344 [12288/54000 (23%)] Loss: -456312.156250\n",
      "Train Epoch: 344 [16384/54000 (30%)] Loss: -503220.062500\n",
      "Train Epoch: 344 [20480/54000 (38%)] Loss: -468070.125000\n",
      "Train Epoch: 344 [24576/54000 (46%)] Loss: -448711.781250\n",
      "Train Epoch: 344 [28672/54000 (53%)] Loss: -456267.937500\n",
      "Train Epoch: 344 [32768/54000 (61%)] Loss: -462206.093750\n",
      "Train Epoch: 344 [36864/54000 (68%)] Loss: -501324.562500\n",
      "Train Epoch: 344 [40960/54000 (76%)] Loss: -456091.906250\n",
      "Train Epoch: 344 [45056/54000 (83%)] Loss: -468375.437500\n",
      "Train Epoch: 344 [49152/54000 (91%)] Loss: -503114.500000\n",
      "    epoch          : 344\n",
      "    loss           : -462376.5425304878\n",
      "    val_loss       : -461764.5859375\n",
      "Train Epoch: 345 [0/54000 (0%)] Loss: -503525.937500\n",
      "Train Epoch: 345 [4096/54000 (8%)] Loss: -467909.750000\n",
      "Train Epoch: 345 [8192/54000 (15%)] Loss: -468742.875000\n",
      "Train Epoch: 345 [12288/54000 (23%)] Loss: -450876.031250\n",
      "Train Epoch: 345 [16384/54000 (30%)] Loss: -448547.531250\n",
      "Train Epoch: 345 [20480/54000 (38%)] Loss: -468296.500000\n",
      "Train Epoch: 345 [24576/54000 (46%)] Loss: -451075.968750\n",
      "Train Epoch: 345 [28672/54000 (53%)] Loss: -451499.375000\n",
      "Train Epoch: 345 [32768/54000 (61%)] Loss: -465848.250000\n",
      "Train Epoch: 345 [36864/54000 (68%)] Loss: -463458.187500\n",
      "Train Epoch: 345 [40960/54000 (76%)] Loss: -455415.843750\n",
      "Train Epoch: 345 [45056/54000 (83%)] Loss: -449714.250000\n",
      "Train Epoch: 345 [49152/54000 (91%)] Loss: -503654.062500\n",
      "    epoch          : 345\n",
      "    loss           : -462450.6292682927\n",
      "    val_loss       : -461917.325\n",
      "Train Epoch: 346 [0/54000 (0%)] Loss: -449271.187500\n",
      "Train Epoch: 346 [4096/54000 (8%)] Loss: -448567.000000\n",
      "Train Epoch: 346 [8192/54000 (15%)] Loss: -455974.250000\n",
      "Train Epoch: 346 [12288/54000 (23%)] Loss: -458721.062500\n",
      "Train Epoch: 346 [16384/54000 (30%)] Loss: -504912.031250\n",
      "Train Epoch: 346 [20480/54000 (38%)] Loss: -469577.812500\n",
      "Train Epoch: 346 [24576/54000 (46%)] Loss: -468523.875000\n",
      "Train Epoch: 346 [28672/54000 (53%)] Loss: -453637.812500\n",
      "Train Epoch: 346 [32768/54000 (61%)] Loss: -467905.812500\n",
      "Train Epoch: 346 [36864/54000 (68%)] Loss: -449356.062500\n",
      "Train Epoch: 346 [40960/54000 (76%)] Loss: -446725.875000\n",
      "Train Epoch: 346 [45056/54000 (83%)] Loss: -452307.218750\n",
      "Train Epoch: 346 [49152/54000 (91%)] Loss: -503227.562500\n",
      "    epoch          : 346\n",
      "    loss           : -462446.7105182927\n",
      "    val_loss       : -461690.91640625\n",
      "Train Epoch: 347 [0/54000 (0%)] Loss: -502321.375000\n",
      "Train Epoch: 347 [4096/54000 (8%)] Loss: -456089.281250\n",
      "Train Epoch: 347 [8192/54000 (15%)] Loss: -468727.250000\n",
      "Train Epoch: 347 [12288/54000 (23%)] Loss: -451444.781250\n",
      "Train Epoch: 347 [16384/54000 (30%)] Loss: -446887.000000\n",
      "Train Epoch: 347 [20480/54000 (38%)] Loss: -467978.156250\n",
      "Train Epoch: 347 [24576/54000 (46%)] Loss: -449400.250000\n",
      "Train Epoch: 347 [28672/54000 (53%)] Loss: -453821.156250\n",
      "Train Epoch: 347 [32768/54000 (61%)] Loss: -451265.375000\n",
      "Train Epoch: 347 [36864/54000 (68%)] Loss: -503094.968750\n",
      "Train Epoch: 347 [40960/54000 (76%)] Loss: -447344.812500\n",
      "Train Epoch: 347 [45056/54000 (83%)] Loss: -467082.875000\n",
      "Train Epoch: 347 [49152/54000 (91%)] Loss: -454912.406250\n",
      "    epoch          : 347\n",
      "    loss           : -462447.0274390244\n",
      "    val_loss       : -461858.5953125\n",
      "Train Epoch: 348 [0/54000 (0%)] Loss: -504377.968750\n",
      "Train Epoch: 348 [4096/54000 (8%)] Loss: -467057.875000\n",
      "Train Epoch: 348 [8192/54000 (15%)] Loss: -450128.750000\n",
      "Train Epoch: 348 [12288/54000 (23%)] Loss: -452949.437500\n",
      "Train Epoch: 348 [16384/54000 (30%)] Loss: -448885.156250\n",
      "Train Epoch: 348 [20480/54000 (38%)] Loss: -468403.562500\n",
      "Train Epoch: 348 [24576/54000 (46%)] Loss: -449214.125000\n",
      "Train Epoch: 348 [28672/54000 (53%)] Loss: -502677.750000\n",
      "Train Epoch: 348 [32768/54000 (61%)] Loss: -451031.843750\n",
      "Train Epoch: 348 [36864/54000 (68%)] Loss: -462171.093750\n",
      "Train Epoch: 348 [40960/54000 (76%)] Loss: -459387.531250\n",
      "Train Epoch: 348 [45056/54000 (83%)] Loss: -467815.468750\n",
      "Train Epoch: 348 [49152/54000 (91%)] Loss: -503180.937500\n",
      "    epoch          : 348\n",
      "    loss           : -462641.47286585363\n",
      "    val_loss       : -461926.9671875\n",
      "Train Epoch: 349 [0/54000 (0%)] Loss: -503768.906250\n",
      "Train Epoch: 349 [4096/54000 (8%)] Loss: -467661.843750\n",
      "Train Epoch: 349 [8192/54000 (15%)] Loss: -469626.125000\n",
      "Train Epoch: 349 [12288/54000 (23%)] Loss: -467548.187500\n",
      "Train Epoch: 349 [16384/54000 (30%)] Loss: -448242.250000\n",
      "Train Epoch: 349 [20480/54000 (38%)] Loss: -468926.781250\n",
      "Train Epoch: 349 [24576/54000 (46%)] Loss: -448314.218750\n",
      "Train Epoch: 349 [28672/54000 (53%)] Loss: -457884.437500\n",
      "Train Epoch: 349 [32768/54000 (61%)] Loss: -467644.062500\n",
      "Train Epoch: 349 [36864/54000 (68%)] Loss: -448738.750000\n",
      "Train Epoch: 349 [40960/54000 (76%)] Loss: -457841.687500\n",
      "Train Epoch: 349 [45056/54000 (83%)] Loss: -467288.000000\n",
      "Train Epoch: 349 [49152/54000 (91%)] Loss: -503117.000000\n",
      "    epoch          : 349\n",
      "    loss           : -462621.7826219512\n",
      "    val_loss       : -461537.91953125\n",
      "Train Epoch: 350 [0/54000 (0%)] Loss: -503629.531250\n",
      "Train Epoch: 350 [4096/54000 (8%)] Loss: -456887.562500\n",
      "Train Epoch: 350 [8192/54000 (15%)] Loss: -458077.031250\n",
      "Train Epoch: 350 [12288/54000 (23%)] Loss: -448225.062500\n",
      "Train Epoch: 350 [16384/54000 (30%)] Loss: -457510.250000\n",
      "Train Epoch: 350 [20480/54000 (38%)] Loss: -467969.062500\n",
      "Train Epoch: 350 [24576/54000 (46%)] Loss: -457554.562500\n",
      "Train Epoch: 350 [28672/54000 (53%)] Loss: -452609.187500\n",
      "Train Epoch: 350 [32768/54000 (61%)] Loss: -448110.125000\n",
      "Train Epoch: 350 [36864/54000 (68%)] Loss: -467892.312500\n",
      "Train Epoch: 350 [40960/54000 (76%)] Loss: -448285.875000\n",
      "Train Epoch: 350 [45056/54000 (83%)] Loss: -467958.562500\n",
      "Train Epoch: 350 [49152/54000 (91%)] Loss: -458410.875000\n",
      "    epoch          : 350\n",
      "    loss           : -462582.5858231707\n",
      "    val_loss       : -462063.23515625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0510_004522/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [0/54000 (0%)] Loss: -503372.312500\n",
      "Train Epoch: 351 [4096/54000 (8%)] Loss: -450324.875000\n",
      "Train Epoch: 351 [8192/54000 (15%)] Loss: -449471.562500\n",
      "Train Epoch: 351 [12288/54000 (23%)] Loss: -458500.625000\n",
      "Train Epoch: 351 [16384/54000 (30%)] Loss: -448229.250000\n",
      "Train Epoch: 351 [20480/54000 (38%)] Loss: -467849.562500\n",
      "Train Epoch: 351 [24576/54000 (46%)] Loss: -466735.750000\n",
      "Train Epoch: 351 [28672/54000 (53%)] Loss: -456561.187500\n",
      "Train Epoch: 351 [32768/54000 (61%)] Loss: -468175.625000\n",
      "Train Epoch: 351 [36864/54000 (68%)] Loss: -502878.593750\n",
      "Train Epoch: 351 [40960/54000 (76%)] Loss: -463780.437500\n",
      "Train Epoch: 351 [45056/54000 (83%)] Loss: -468774.281250\n",
      "Train Epoch: 351 [49152/54000 (91%)] Loss: -502362.468750\n",
      "    epoch          : 351\n",
      "    loss           : -462484.93125\n",
      "    val_loss       : -461873.83515625\n",
      "Train Epoch: 352 [0/54000 (0%)] Loss: -504116.687500\n",
      "Train Epoch: 352 [4096/54000 (8%)] Loss: -467259.750000\n",
      "Train Epoch: 352 [8192/54000 (15%)] Loss: -468520.812500\n",
      "Train Epoch: 352 [12288/54000 (23%)] Loss: -457712.062500\n",
      "Train Epoch: 352 [16384/54000 (30%)] Loss: -467240.093750\n",
      "Train Epoch: 352 [20480/54000 (38%)] Loss: -455992.218750\n",
      "Train Epoch: 352 [24576/54000 (46%)] Loss: -451731.218750\n",
      "Train Epoch: 352 [28672/54000 (53%)] Loss: -455320.375000\n",
      "Train Epoch: 352 [32768/54000 (61%)] Loss: -466553.875000\n",
      "Train Epoch: 352 [36864/54000 (68%)] Loss: -468105.250000\n",
      "Train Epoch: 352 [40960/54000 (76%)] Loss: -458819.218750\n",
      "Train Epoch: 352 [45056/54000 (83%)] Loss: -470294.187500\n",
      "Train Epoch: 352 [49152/54000 (91%)] Loss: -502696.500000\n",
      "    epoch          : 352\n",
      "    loss           : -462469.8699695122\n",
      "    val_loss       : -461934.69921875\n",
      "Train Epoch: 353 [0/54000 (0%)] Loss: -464987.593750\n",
      "Train Epoch: 353 [4096/54000 (8%)] Loss: -451938.281250\n",
      "Train Epoch: 353 [8192/54000 (15%)] Loss: -467197.593750\n",
      "Train Epoch: 353 [12288/54000 (23%)] Loss: -502302.031250\n",
      "Train Epoch: 353 [16384/54000 (30%)] Loss: -452860.031250\n",
      "Train Epoch: 353 [20480/54000 (38%)] Loss: -456796.812500\n",
      "Train Epoch: 353 [24576/54000 (46%)] Loss: -451203.250000\n",
      "Train Epoch: 353 [28672/54000 (53%)] Loss: -457452.593750\n",
      "Train Epoch: 353 [32768/54000 (61%)] Loss: -448647.343750\n",
      "Train Epoch: 353 [36864/54000 (68%)] Loss: -451064.187500\n",
      "Train Epoch: 353 [40960/54000 (76%)] Loss: -449117.437500\n",
      "Train Epoch: 353 [45056/54000 (83%)] Loss: -465378.968750\n",
      "Train Epoch: 353 [49152/54000 (91%)] Loss: -503488.718750\n",
      "    epoch          : 353\n",
      "    loss           : -462592.45213414636\n",
      "    val_loss       : -461949.684375\n",
      "Train Epoch: 354 [0/54000 (0%)] Loss: -502852.281250\n",
      "Train Epoch: 354 [4096/54000 (8%)] Loss: -463224.031250\n",
      "Train Epoch: 354 [8192/54000 (15%)] Loss: -455926.000000\n",
      "Train Epoch: 354 [12288/54000 (23%)] Loss: -449028.593750\n",
      "Train Epoch: 354 [16384/54000 (30%)] Loss: -467877.656250\n",
      "Train Epoch: 354 [20480/54000 (38%)] Loss: -468224.625000\n",
      "Train Epoch: 354 [24576/54000 (46%)] Loss: -458050.937500\n",
      "Train Epoch: 354 [28672/54000 (53%)] Loss: -457558.062500\n",
      "Train Epoch: 354 [32768/54000 (61%)] Loss: -448716.593750\n",
      "Train Epoch: 354 [36864/54000 (68%)] Loss: -451437.718750\n",
      "Train Epoch: 354 [40960/54000 (76%)] Loss: -456789.437500\n",
      "Train Epoch: 354 [45056/54000 (83%)] Loss: -466757.375000\n",
      "Train Epoch: 354 [49152/54000 (91%)] Loss: -501195.187500\n",
      "    epoch          : 354\n",
      "    loss           : -462671.8112804878\n",
      "    val_loss       : -461975.20234375\n",
      "Train Epoch: 355 [0/54000 (0%)] Loss: -503514.406250\n",
      "Train Epoch: 355 [4096/54000 (8%)] Loss: -464577.312500\n",
      "Train Epoch: 355 [8192/54000 (15%)] Loss: -468010.250000\n",
      "Train Epoch: 355 [12288/54000 (23%)] Loss: -453074.906250\n",
      "Train Epoch: 355 [16384/54000 (30%)] Loss: -448766.500000\n",
      "Train Epoch: 355 [20480/54000 (38%)] Loss: -456145.031250\n",
      "Train Epoch: 355 [24576/54000 (46%)] Loss: -467327.843750\n",
      "Train Epoch: 355 [28672/54000 (53%)] Loss: -452445.875000\n",
      "Train Epoch: 355 [32768/54000 (61%)] Loss: -456551.250000\n",
      "Train Epoch: 355 [36864/54000 (68%)] Loss: -448348.437500\n",
      "Train Epoch: 355 [40960/54000 (76%)] Loss: -456089.000000\n",
      "Train Epoch: 355 [45056/54000 (83%)] Loss: -449145.750000\n",
      "Train Epoch: 355 [49152/54000 (91%)] Loss: -456596.593750\n",
      "    epoch          : 355\n",
      "    loss           : -462735.01951219514\n",
      "    val_loss       : -462115.1125\n",
      "Train Epoch: 356 [0/54000 (0%)] Loss: -502642.750000\n",
      "Train Epoch: 356 [4096/54000 (8%)] Loss: -458693.531250\n",
      "Train Epoch: 356 [8192/54000 (15%)] Loss: -466494.968750\n",
      "Train Epoch: 356 [12288/54000 (23%)] Loss: -448239.937500\n",
      "Train Epoch: 356 [16384/54000 (30%)] Loss: -466676.562500\n",
      "Train Epoch: 356 [20480/54000 (38%)] Loss: -468726.625000\n",
      "Train Epoch: 356 [24576/54000 (46%)] Loss: -448450.718750\n",
      "Train Epoch: 356 [28672/54000 (53%)] Loss: -504468.937500\n",
      "Train Epoch: 356 [32768/54000 (61%)] Loss: -455423.187500\n",
      "Train Epoch: 356 [36864/54000 (68%)] Loss: -450195.937500\n",
      "Train Epoch: 356 [40960/54000 (76%)] Loss: -464327.343750\n",
      "Train Epoch: 356 [45056/54000 (83%)] Loss: -467694.125000\n",
      "Train Epoch: 356 [49152/54000 (91%)] Loss: -501977.500000\n",
      "    epoch          : 356\n",
      "    loss           : -462647.87835365854\n",
      "    val_loss       : -461635.29375\n",
      "Train Epoch: 357 [0/54000 (0%)] Loss: -503643.562500\n",
      "Train Epoch: 357 [4096/54000 (8%)] Loss: -451664.750000\n",
      "Train Epoch: 357 [8192/54000 (15%)] Loss: -449063.031250\n",
      "Train Epoch: 357 [12288/54000 (23%)] Loss: -450063.312500\n",
      "Train Epoch: 357 [16384/54000 (30%)] Loss: -466045.875000\n",
      "Train Epoch: 357 [20480/54000 (38%)] Loss: -457747.062500\n",
      "Train Epoch: 357 [24576/54000 (46%)] Loss: -457095.156250\n",
      "Train Epoch: 357 [28672/54000 (53%)] Loss: -503726.375000\n",
      "Train Epoch: 357 [32768/54000 (61%)] Loss: -467583.500000\n",
      "Train Epoch: 357 [36864/54000 (68%)] Loss: -455480.281250\n",
      "Train Epoch: 357 [40960/54000 (76%)] Loss: -449051.500000\n",
      "Train Epoch: 357 [45056/54000 (83%)] Loss: -468701.625000\n",
      "Train Epoch: 357 [49152/54000 (91%)] Loss: -502616.125000\n",
      "    epoch          : 357\n",
      "    loss           : -462569.9725609756\n",
      "    val_loss       : -462001.72421875\n",
      "Train Epoch: 358 [0/54000 (0%)] Loss: -504119.656250\n",
      "Train Epoch: 358 [4096/54000 (8%)] Loss: -451042.718750\n",
      "Train Epoch: 358 [8192/54000 (15%)] Loss: -464290.750000\n",
      "Train Epoch: 358 [12288/54000 (23%)] Loss: -502733.500000\n",
      "Train Epoch: 358 [16384/54000 (30%)] Loss: -461150.187500\n",
      "Train Epoch: 358 [20480/54000 (38%)] Loss: -468131.562500\n",
      "Train Epoch: 358 [24576/54000 (46%)] Loss: -465242.281250\n",
      "Train Epoch: 358 [28672/54000 (53%)] Loss: -455341.937500\n",
      "Train Epoch: 358 [32768/54000 (61%)] Loss: -448543.937500\n",
      "Train Epoch: 358 [36864/54000 (68%)] Loss: -462648.593750\n",
      "Train Epoch: 358 [40960/54000 (76%)] Loss: -449929.781250\n",
      "Train Epoch: 358 [45056/54000 (83%)] Loss: -449403.750000\n",
      "Train Epoch: 358 [49152/54000 (91%)] Loss: -503400.437500\n",
      "    epoch          : 358\n",
      "    loss           : -462853.94207317074\n",
      "    val_loss       : -462072.12734375\n",
      "Train Epoch: 359 [0/54000 (0%)] Loss: -467710.625000\n",
      "Train Epoch: 359 [4096/54000 (8%)] Loss: -458132.187500\n",
      "Train Epoch: 359 [8192/54000 (15%)] Loss: -454809.906250\n",
      "Train Epoch: 359 [12288/54000 (23%)] Loss: -447871.000000\n",
      "Train Epoch: 359 [16384/54000 (30%)] Loss: -448108.406250\n",
      "Train Epoch: 359 [20480/54000 (38%)] Loss: -457185.375000\n",
      "Train Epoch: 359 [24576/54000 (46%)] Loss: -447963.062500\n",
      "Train Epoch: 359 [28672/54000 (53%)] Loss: -454693.093750\n",
      "Train Epoch: 359 [32768/54000 (61%)] Loss: -463368.906250\n",
      "Train Epoch: 359 [36864/54000 (68%)] Loss: -451029.375000\n",
      "Train Epoch: 359 [40960/54000 (76%)] Loss: -457593.000000\n",
      "Train Epoch: 359 [45056/54000 (83%)] Loss: -451226.500000\n",
      "Train Epoch: 359 [49152/54000 (91%)] Loss: -501436.312500\n",
      "    epoch          : 359\n",
      "    loss           : -462798.84207317076\n",
      "    val_loss       : -461786.82421875\n",
      "Train Epoch: 360 [0/54000 (0%)] Loss: -503235.531250\n",
      "Train Epoch: 360 [4096/54000 (8%)] Loss: -457767.500000\n",
      "Train Epoch: 360 [8192/54000 (15%)] Loss: -450810.375000\n",
      "Train Epoch: 360 [12288/54000 (23%)] Loss: -457008.937500\n",
      "Train Epoch: 360 [16384/54000 (30%)] Loss: -449617.312500\n",
      "Train Epoch: 360 [20480/54000 (38%)] Loss: -456472.000000\n",
      "Train Epoch: 360 [24576/54000 (46%)] Loss: -448208.812500\n",
      "Train Epoch: 360 [28672/54000 (53%)] Loss: -504177.593750\n",
      "Train Epoch: 360 [32768/54000 (61%)] Loss: -450739.843750\n",
      "Train Epoch: 360 [36864/54000 (68%)] Loss: -468684.375000\n",
      "Train Epoch: 360 [40960/54000 (76%)] Loss: -451515.343750\n",
      "Train Epoch: 360 [45056/54000 (83%)] Loss: -467972.500000\n",
      "Train Epoch: 360 [49152/54000 (91%)] Loss: -459153.593750\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   360: reducing learning rate of group 0 to 5.0000e-05.\n",
      "    epoch          : 360\n",
      "    loss           : -462777.47926829266\n",
      "    val_loss       : -461915.69296875\n",
      "Train Epoch: 361 [0/54000 (0%)] Loss: -466467.375000\n",
      "Train Epoch: 361 [4096/54000 (8%)] Loss: -449050.000000\n",
      "Train Epoch: 361 [8192/54000 (15%)] Loss: -466761.937500\n",
      "Train Epoch: 361 [12288/54000 (23%)] Loss: -452865.812500\n",
      "Train Epoch: 361 [16384/54000 (30%)] Loss: -454613.125000\n",
      "Train Epoch: 361 [20480/54000 (38%)] Loss: -468834.250000\n",
      "Train Epoch: 361 [24576/54000 (46%)] Loss: -465560.406250\n",
      "Train Epoch: 361 [28672/54000 (53%)] Loss: -455932.625000\n",
      "Train Epoch: 361 [32768/54000 (61%)] Loss: -468348.812500\n",
      "Train Epoch: 361 [36864/54000 (68%)] Loss: -467738.250000\n",
      "Train Epoch: 361 [40960/54000 (76%)] Loss: -447921.812500\n",
      "Train Epoch: 361 [45056/54000 (83%)] Loss: -463236.343750\n",
      "Train Epoch: 361 [49152/54000 (91%)] Loss: -503799.875000\n",
      "    epoch          : 361\n",
      "    loss           : -462848.45762195124\n",
      "    val_loss       : -462319.80390625\n",
      "Train Epoch: 362 [0/54000 (0%)] Loss: -453461.250000\n",
      "Train Epoch: 362 [4096/54000 (8%)] Loss: -464569.687500\n",
      "Train Epoch: 362 [8192/54000 (15%)] Loss: -449613.500000\n",
      "Train Epoch: 362 [12288/54000 (23%)] Loss: -458807.375000\n",
      "Train Epoch: 362 [16384/54000 (30%)] Loss: -503529.187500\n",
      "Train Epoch: 362 [20480/54000 (38%)] Loss: -470377.437500\n",
      "Train Epoch: 362 [24576/54000 (46%)] Loss: -451343.968750\n",
      "Train Epoch: 362 [28672/54000 (53%)] Loss: -457955.656250\n",
      "Train Epoch: 362 [32768/54000 (61%)] Loss: -457575.156250\n",
      "Train Epoch: 362 [36864/54000 (68%)] Loss: -462251.906250\n",
      "Train Epoch: 362 [40960/54000 (76%)] Loss: -459047.625000\n",
      "Train Epoch: 362 [45056/54000 (83%)] Loss: -467699.875000\n",
      "Train Epoch: 362 [49152/54000 (91%)] Loss: -502320.062500\n",
      "    epoch          : 362\n",
      "    loss           : -463055.8274390244\n",
      "    val_loss       : -462347.21015625\n",
      "Train Epoch: 363 [0/54000 (0%)] Loss: -502629.656250\n",
      "Train Epoch: 363 [4096/54000 (8%)] Loss: -467469.687500\n",
      "Train Epoch: 363 [8192/54000 (15%)] Loss: -467980.218750\n",
      "Train Epoch: 363 [12288/54000 (23%)] Loss: -458717.781250\n",
      "Train Epoch: 363 [16384/54000 (30%)] Loss: -454165.968750\n",
      "Train Epoch: 363 [20480/54000 (38%)] Loss: -466530.656250\n",
      "Train Epoch: 363 [24576/54000 (46%)] Loss: -465645.656250\n",
      "Train Epoch: 363 [28672/54000 (53%)] Loss: -503324.500000\n",
      "Train Epoch: 363 [32768/54000 (61%)] Loss: -445980.375000\n",
      "Train Epoch: 363 [36864/54000 (68%)] Loss: -453172.593750\n",
      "Train Epoch: 363 [40960/54000 (76%)] Loss: -448014.875000\n",
      "Train Epoch: 363 [45056/54000 (83%)] Loss: -468644.500000\n",
      "Train Epoch: 363 [49152/54000 (91%)] Loss: -503669.187500\n",
      "    epoch          : 363\n",
      "    loss           : -463056.3655487805\n",
      "    val_loss       : -462389.0953125\n",
      "Train Epoch: 364 [0/54000 (0%)] Loss: -503796.812500\n",
      "Train Epoch: 364 [4096/54000 (8%)] Loss: -459065.625000\n",
      "Train Epoch: 364 [8192/54000 (15%)] Loss: -450054.937500\n",
      "Train Epoch: 364 [12288/54000 (23%)] Loss: -465140.687500\n",
      "Train Epoch: 364 [16384/54000 (30%)] Loss: -467949.875000\n",
      "Train Epoch: 364 [20480/54000 (38%)] Loss: -469869.593750\n",
      "Train Epoch: 364 [24576/54000 (46%)] Loss: -457425.500000\n",
      "Train Epoch: 364 [28672/54000 (53%)] Loss: -451659.750000\n",
      "Train Epoch: 364 [32768/54000 (61%)] Loss: -447648.500000\n",
      "Train Epoch: 364 [36864/54000 (68%)] Loss: -503070.000000\n",
      "Train Epoch: 364 [40960/54000 (76%)] Loss: -447863.500000\n",
      "Train Epoch: 364 [45056/54000 (83%)] Loss: -467733.343750\n",
      "Train Epoch: 364 [49152/54000 (91%)] Loss: -502968.625000\n",
      "    epoch          : 364\n",
      "    loss           : -463200.931402439\n",
      "    val_loss       : -461979.1546875\n",
      "Train Epoch: 365 [0/54000 (0%)] Loss: -502677.312500\n",
      "Train Epoch: 365 [4096/54000 (8%)] Loss: -450450.781250\n",
      "Train Epoch: 365 [8192/54000 (15%)] Loss: -467575.593750\n",
      "Train Epoch: 365 [12288/54000 (23%)] Loss: -504037.312500\n",
      "Train Epoch: 365 [16384/54000 (30%)] Loss: -468739.687500\n",
      "Train Epoch: 365 [20480/54000 (38%)] Loss: -456381.125000\n",
      "Train Epoch: 365 [24576/54000 (46%)] Loss: -467298.343750\n",
      "Train Epoch: 365 [28672/54000 (53%)] Loss: -503789.562500\n",
      "Train Epoch: 365 [32768/54000 (61%)] Loss: -450972.718750\n",
      "Train Epoch: 365 [36864/54000 (68%)] Loss: -449966.906250\n",
      "Train Epoch: 365 [40960/54000 (76%)] Loss: -458180.062500\n",
      "Train Epoch: 365 [45056/54000 (83%)] Loss: -451238.687500\n",
      "Train Epoch: 365 [49152/54000 (91%)] Loss: -503012.781250\n",
      "    epoch          : 365\n",
      "    loss           : -463248.41158536583\n",
      "    val_loss       : -462436.6859375\n",
      "Train Epoch: 366 [0/54000 (0%)] Loss: -503232.343750\n",
      "Train Epoch: 366 [4096/54000 (8%)] Loss: -457297.906250\n",
      "Train Epoch: 366 [8192/54000 (15%)] Loss: -467282.750000\n",
      "Train Epoch: 366 [12288/54000 (23%)] Loss: -464368.531250\n",
      "Train Epoch: 366 [16384/54000 (30%)] Loss: -469085.625000\n",
      "Train Epoch: 366 [20480/54000 (38%)] Loss: -458214.531250\n",
      "Train Epoch: 366 [24576/54000 (46%)] Loss: -467819.250000\n",
      "Train Epoch: 366 [28672/54000 (53%)] Loss: -453296.218750\n",
      "Train Epoch: 366 [32768/54000 (61%)] Loss: -502302.750000\n",
      "Train Epoch: 366 [36864/54000 (68%)] Loss: -450406.937500\n",
      "Train Epoch: 366 [40960/54000 (76%)] Loss: -448230.843750\n",
      "Train Epoch: 366 [45056/54000 (83%)] Loss: -469855.625000\n",
      "Train Epoch: 366 [49152/54000 (91%)] Loss: -503330.312500\n",
      "    epoch          : 366\n",
      "    loss           : -463230.7731707317\n",
      "    val_loss       : -462160.803125\n",
      "Train Epoch: 367 [0/54000 (0%)] Loss: -503586.312500\n",
      "Train Epoch: 367 [4096/54000 (8%)] Loss: -457137.343750\n",
      "Train Epoch: 367 [8192/54000 (15%)] Loss: -456312.000000\n",
      "Train Epoch: 367 [12288/54000 (23%)] Loss: -469699.781250\n",
      "Train Epoch: 367 [16384/54000 (30%)] Loss: -449329.031250\n",
      "Train Epoch: 367 [20480/54000 (38%)] Loss: -468461.250000\n",
      "Train Epoch: 367 [24576/54000 (46%)] Loss: -450799.937500\n",
      "Train Epoch: 367 [28672/54000 (53%)] Loss: -503536.875000\n",
      "Train Epoch: 367 [32768/54000 (61%)] Loss: -467155.000000\n",
      "Train Epoch: 367 [36864/54000 (68%)] Loss: -502516.406250\n",
      "Train Epoch: 367 [40960/54000 (76%)] Loss: -470214.593750\n",
      "Train Epoch: 367 [45056/54000 (83%)] Loss: -468527.625000\n",
      "Train Epoch: 367 [49152/54000 (91%)] Loss: -504249.062500\n",
      "    epoch          : 367\n",
      "    loss           : -463343.3672256098\n",
      "    val_loss       : -462012.11953125\n",
      "Train Epoch: 368 [0/54000 (0%)] Loss: -502558.125000\n",
      "Train Epoch: 368 [4096/54000 (8%)] Loss: -456398.531250\n",
      "Train Epoch: 368 [8192/54000 (15%)] Loss: -448152.625000\n",
      "Train Epoch: 368 [12288/54000 (23%)] Loss: -450243.125000\n",
      "Train Epoch: 368 [16384/54000 (30%)] Loss: -453093.750000\n",
      "Train Epoch: 368 [20480/54000 (38%)] Loss: -470473.437500\n",
      "Train Epoch: 368 [24576/54000 (46%)] Loss: -464081.437500\n",
      "Train Epoch: 368 [28672/54000 (53%)] Loss: -456525.812500\n",
      "Train Epoch: 368 [32768/54000 (61%)] Loss: -459923.718750\n",
      "Train Epoch: 368 [36864/54000 (68%)] Loss: -468443.468750\n",
      "Train Epoch: 368 [40960/54000 (76%)] Loss: -469398.031250\n",
      "Train Epoch: 368 [45056/54000 (83%)] Loss: -468275.843750\n",
      "Train Epoch: 368 [49152/54000 (91%)] Loss: -503327.468750\n",
      "    epoch          : 368\n",
      "    loss           : -463277.8368902439\n",
      "    val_loss       : -462182.49453125\n",
      "Train Epoch: 369 [0/54000 (0%)] Loss: -454703.875000\n",
      "Train Epoch: 369 [4096/54000 (8%)] Loss: -468821.812500\n",
      "Train Epoch: 369 [8192/54000 (15%)] Loss: -454156.250000\n",
      "Train Epoch: 369 [12288/54000 (23%)] Loss: -449700.875000\n",
      "Train Epoch: 369 [16384/54000 (30%)] Loss: -456964.812500\n",
      "Train Epoch: 369 [20480/54000 (38%)] Loss: -469159.562500\n",
      "Train Epoch: 369 [24576/54000 (46%)] Loss: -464401.375000\n",
      "Train Epoch: 369 [28672/54000 (53%)] Loss: -449278.312500\n",
      "Train Epoch: 369 [32768/54000 (61%)] Loss: -463558.875000\n",
      "Train Epoch: 369 [36864/54000 (68%)] Loss: -502463.812500\n",
      "Train Epoch: 369 [40960/54000 (76%)] Loss: -456329.531250\n",
      "Train Epoch: 369 [45056/54000 (83%)] Loss: -447402.000000\n",
      "Train Epoch: 369 [49152/54000 (91%)] Loss: -501852.531250\n",
      "    epoch          : 369\n",
      "    loss           : -463172.7554878049\n",
      "    val_loss       : -462405.8765625\n",
      "Train Epoch: 370 [0/54000 (0%)] Loss: -503323.812500\n",
      "Train Epoch: 370 [4096/54000 (8%)] Loss: -464516.156250\n",
      "Train Epoch: 370 [8192/54000 (15%)] Loss: -468879.468750\n",
      "Train Epoch: 370 [12288/54000 (23%)] Loss: -447776.437500\n",
      "Train Epoch: 370 [16384/54000 (30%)] Loss: -466865.062500\n",
      "Train Epoch: 370 [20480/54000 (38%)] Loss: -503725.656250\n",
      "Train Epoch: 370 [24576/54000 (46%)] Loss: -454693.968750\n",
      "Train Epoch: 370 [28672/54000 (53%)] Loss: -467514.437500\n",
      "Train Epoch: 370 [32768/54000 (61%)] Loss: -502115.875000\n",
      "Train Epoch: 370 [36864/54000 (68%)] Loss: -503492.468750\n",
      "Train Epoch: 370 [40960/54000 (76%)] Loss: -465751.000000\n",
      "Train Epoch: 370 [45056/54000 (83%)] Loss: -450721.250000\n",
      "Train Epoch: 370 [49152/54000 (91%)] Loss: -503508.312500\n",
      "    epoch          : 370\n",
      "    loss           : -463338.6082317073\n",
      "    val_loss       : -462548.99296875\n",
      "Train Epoch: 371 [0/54000 (0%)] Loss: -504018.468750\n",
      "Train Epoch: 371 [4096/54000 (8%)] Loss: -468346.750000\n",
      "Train Epoch: 371 [8192/54000 (15%)] Loss: -451596.812500\n",
      "Train Epoch: 371 [12288/54000 (23%)] Loss: -463584.312500\n",
      "Train Epoch: 371 [16384/54000 (30%)] Loss: -449198.906250\n",
      "Train Epoch: 371 [20480/54000 (38%)] Loss: -456989.718750\n",
      "Train Epoch: 371 [24576/54000 (46%)] Loss: -448469.375000\n",
      "Train Epoch: 371 [28672/54000 (53%)] Loss: -457288.312500\n",
      "Train Epoch: 371 [32768/54000 (61%)] Loss: -465163.312500\n",
      "Train Epoch: 371 [36864/54000 (68%)] Loss: -467915.937500\n",
      "Train Epoch: 371 [40960/54000 (76%)] Loss: -459201.500000\n",
      "Train Epoch: 371 [45056/54000 (83%)] Loss: -468431.062500\n",
      "Train Epoch: 371 [49152/54000 (91%)] Loss: -464669.093750\n",
      "    epoch          : 371\n",
      "    loss           : -463291.53658536583\n",
      "    val_loss       : -462544.21640625\n",
      "Train Epoch: 372 [0/54000 (0%)] Loss: -504368.593750\n",
      "Train Epoch: 372 [4096/54000 (8%)] Loss: -455216.937500\n",
      "Train Epoch: 372 [8192/54000 (15%)] Loss: -467793.656250\n",
      "Train Epoch: 372 [12288/54000 (23%)] Loss: -452661.500000\n",
      "Train Epoch: 372 [16384/54000 (30%)] Loss: -503962.656250\n",
      "Train Epoch: 372 [20480/54000 (38%)] Loss: -469604.843750\n",
      "Train Epoch: 372 [24576/54000 (46%)] Loss: -463460.218750\n",
      "Train Epoch: 372 [28672/54000 (53%)] Loss: -456136.687500\n",
      "Train Epoch: 372 [32768/54000 (61%)] Loss: -452328.187500\n",
      "Train Epoch: 372 [36864/54000 (68%)] Loss: -463533.375000\n",
      "Train Epoch: 372 [40960/54000 (76%)] Loss: -449364.593750\n",
      "Train Epoch: 372 [45056/54000 (83%)] Loss: -470221.625000\n",
      "Train Epoch: 372 [49152/54000 (91%)] Loss: -503399.156250\n",
      "    epoch          : 372\n",
      "    loss           : -463038.78125\n",
      "    val_loss       : -462302.825\n",
      "Train Epoch: 373 [0/54000 (0%)] Loss: -504456.843750\n",
      "Train Epoch: 373 [4096/54000 (8%)] Loss: -451771.406250\n",
      "Train Epoch: 373 [8192/54000 (15%)] Loss: -457639.406250\n",
      "Train Epoch: 373 [12288/54000 (23%)] Loss: -468104.750000\n",
      "Train Epoch: 373 [16384/54000 (30%)] Loss: -463319.437500\n",
      "Train Epoch: 373 [20480/54000 (38%)] Loss: -468881.031250\n",
      "Train Epoch: 373 [24576/54000 (46%)] Loss: -457269.218750\n",
      "Train Epoch: 373 [28672/54000 (53%)] Loss: -454184.281250\n",
      "Train Epoch: 373 [32768/54000 (61%)] Loss: -456586.437500\n",
      "Train Epoch: 373 [36864/54000 (68%)] Loss: -450273.093750\n",
      "Train Epoch: 373 [40960/54000 (76%)] Loss: -456768.812500\n",
      "Train Epoch: 373 [45056/54000 (83%)] Loss: -468508.937500\n",
      "Train Epoch: 373 [49152/54000 (91%)] Loss: -502776.000000\n",
      "    epoch          : 373\n",
      "    loss           : -463152.69161585363\n",
      "    val_loss       : -462384.77265625\n",
      "Train Epoch: 374 [0/54000 (0%)] Loss: -503692.968750\n",
      "Train Epoch: 374 [4096/54000 (8%)] Loss: -450395.937500\n",
      "Train Epoch: 374 [8192/54000 (15%)] Loss: -468465.250000\n",
      "Train Epoch: 374 [12288/54000 (23%)] Loss: -452099.343750\n",
      "Train Epoch: 374 [16384/54000 (30%)] Loss: -452043.343750\n",
      "Train Epoch: 374 [20480/54000 (38%)] Loss: -467984.156250\n",
      "Train Epoch: 374 [24576/54000 (46%)] Loss: -450729.312500\n",
      "Train Epoch: 374 [28672/54000 (53%)] Loss: -457325.468750\n",
      "Train Epoch: 374 [32768/54000 (61%)] Loss: -457202.125000\n",
      "Train Epoch: 374 [36864/54000 (68%)] Loss: -503647.312500\n",
      "Train Epoch: 374 [40960/54000 (76%)] Loss: -458290.781250\n",
      "Train Epoch: 374 [45056/54000 (83%)] Loss: -457170.062500\n",
      "Train Epoch: 374 [49152/54000 (91%)] Loss: -503578.312500\n",
      "    epoch          : 374\n",
      "    loss           : -463395.85609756096\n",
      "    val_loss       : -462625.19609375\n",
      "Train Epoch: 375 [0/54000 (0%)] Loss: -456294.187500\n",
      "Train Epoch: 375 [4096/54000 (8%)] Loss: -463937.687500\n",
      "Train Epoch: 375 [8192/54000 (15%)] Loss: -468674.062500\n",
      "Train Epoch: 375 [12288/54000 (23%)] Loss: -458378.187500\n",
      "Train Epoch: 375 [16384/54000 (30%)] Loss: -468234.875000\n",
      "Train Epoch: 375 [20480/54000 (38%)] Loss: -455826.093750\n",
      "Train Epoch: 375 [24576/54000 (46%)] Loss: -468069.000000\n",
      "Train Epoch: 375 [28672/54000 (53%)] Loss: -503058.343750\n",
      "Train Epoch: 375 [32768/54000 (61%)] Loss: -459221.968750\n",
      "Train Epoch: 375 [36864/54000 (68%)] Loss: -504338.125000\n",
      "Train Epoch: 375 [40960/54000 (76%)] Loss: -455839.125000\n",
      "Train Epoch: 375 [45056/54000 (83%)] Loss: -468257.531250\n",
      "Train Epoch: 375 [49152/54000 (91%)] Loss: -503228.562500\n",
      "    epoch          : 375\n",
      "    loss           : -463362.36143292685\n",
      "    val_loss       : -462509.28828125\n",
      "Train Epoch: 376 [0/54000 (0%)] Loss: -503473.750000\n",
      "Train Epoch: 376 [4096/54000 (8%)] Loss: -449943.375000\n",
      "Train Epoch: 376 [8192/54000 (15%)] Loss: -455039.875000\n",
      "Train Epoch: 376 [12288/54000 (23%)] Loss: -466270.687500\n",
      "Train Epoch: 376 [16384/54000 (30%)] Loss: -452461.125000\n",
      "Train Epoch: 376 [20480/54000 (38%)] Loss: -467729.687500\n",
      "Train Epoch: 376 [24576/54000 (46%)] Loss: -464816.000000\n",
      "Train Epoch: 376 [28672/54000 (53%)] Loss: -503491.562500\n",
      "Train Epoch: 376 [32768/54000 (61%)] Loss: -448516.968750\n",
      "Train Epoch: 376 [36864/54000 (68%)] Loss: -448582.937500\n",
      "Train Epoch: 376 [40960/54000 (76%)] Loss: -450498.562500\n",
      "Train Epoch: 376 [45056/54000 (83%)] Loss: -468036.125000\n",
      "Train Epoch: 376 [49152/54000 (91%)] Loss: -502621.875000\n",
      "    epoch          : 376\n",
      "    loss           : -463219.9466463415\n",
      "    val_loss       : -462136.70703125\n",
      "Train Epoch: 377 [0/54000 (0%)] Loss: -502763.562500\n",
      "Train Epoch: 377 [4096/54000 (8%)] Loss: -449298.500000\n",
      "Train Epoch: 377 [8192/54000 (15%)] Loss: -456730.218750\n",
      "Train Epoch: 377 [12288/54000 (23%)] Loss: -458454.843750\n",
      "Train Epoch: 377 [16384/54000 (30%)] Loss: -450601.093750\n",
      "Train Epoch: 377 [20480/54000 (38%)] Loss: -469432.250000\n",
      "Train Epoch: 377 [24576/54000 (46%)] Loss: -457285.156250\n",
      "Train Epoch: 377 [28672/54000 (53%)] Loss: -502934.968750\n",
      "Train Epoch: 377 [32768/54000 (61%)] Loss: -457555.281250\n",
      "Train Epoch: 377 [36864/54000 (68%)] Loss: -467238.437500\n",
      "Train Epoch: 377 [40960/54000 (76%)] Loss: -458079.562500\n",
      "Train Epoch: 377 [45056/54000 (83%)] Loss: -452324.937500\n",
      "Train Epoch: 377 [49152/54000 (91%)] Loss: -501624.500000\n",
      "    epoch          : 377\n",
      "    loss           : -463455.2554878049\n",
      "    val_loss       : -462258.13984375\n",
      "Train Epoch: 378 [0/54000 (0%)] Loss: -502114.656250\n",
      "Train Epoch: 378 [4096/54000 (8%)] Loss: -461866.625000\n",
      "Train Epoch: 378 [8192/54000 (15%)] Loss: -469210.812500\n",
      "Train Epoch: 378 [12288/54000 (23%)] Loss: -463217.687500\n",
      "Train Epoch: 378 [16384/54000 (30%)] Loss: -458421.125000\n",
      "Train Epoch: 378 [20480/54000 (38%)] Loss: -458801.718750\n",
      "Train Epoch: 378 [24576/54000 (46%)] Loss: -456780.625000\n",
      "Train Epoch: 378 [28672/54000 (53%)] Loss: -468362.500000\n",
      "Train Epoch: 378 [32768/54000 (61%)] Loss: -467550.562500\n",
      "Train Epoch: 378 [36864/54000 (68%)] Loss: -451521.406250\n",
      "Train Epoch: 378 [40960/54000 (76%)] Loss: -468156.468750\n",
      "Train Epoch: 378 [45056/54000 (83%)] Loss: -449672.312500\n",
      "Train Epoch: 378 [49152/54000 (91%)] Loss: -503397.375000\n",
      "    epoch          : 378\n",
      "    loss           : -463401.2955792683\n",
      "    val_loss       : -462481.29375\n",
      "Train Epoch: 379 [0/54000 (0%)] Loss: -449733.687500\n",
      "Train Epoch: 379 [4096/54000 (8%)] Loss: -460187.375000\n",
      "Train Epoch: 379 [8192/54000 (15%)] Loss: -466964.312500\n",
      "Train Epoch: 379 [12288/54000 (23%)] Loss: -463939.687500\n",
      "Train Epoch: 379 [16384/54000 (30%)] Loss: -451370.093750\n",
      "Train Epoch: 379 [20480/54000 (38%)] Loss: -468405.187500\n",
      "Train Epoch: 379 [24576/54000 (46%)] Loss: -450888.906250\n",
      "Train Epoch: 379 [28672/54000 (53%)] Loss: -455461.093750\n",
      "Train Epoch: 379 [32768/54000 (61%)] Loss: -459233.031250\n",
      "Train Epoch: 379 [36864/54000 (68%)] Loss: -450588.312500\n",
      "Train Epoch: 379 [40960/54000 (76%)] Loss: -462749.343750\n",
      "Train Epoch: 379 [45056/54000 (83%)] Loss: -468953.000000\n",
      "Train Epoch: 379 [49152/54000 (91%)] Loss: -503097.468750\n",
      "    epoch          : 379\n",
      "    loss           : -463478.97286585363\n",
      "    val_loss       : -462309.58671875\n",
      "Train Epoch: 380 [0/54000 (0%)] Loss: -504050.562500\n",
      "Train Epoch: 380 [4096/54000 (8%)] Loss: -464168.187500\n",
      "Train Epoch: 380 [8192/54000 (15%)] Loss: -452010.937500\n",
      "Train Epoch: 380 [12288/54000 (23%)] Loss: -465614.187500\n",
      "Train Epoch: 380 [16384/54000 (30%)] Loss: -428547.500000\n",
      "Train Epoch: 380 [20480/54000 (38%)] Loss: -457380.562500\n",
      "Train Epoch: 380 [24576/54000 (46%)] Loss: -451799.687500\n",
      "Train Epoch: 380 [28672/54000 (53%)] Loss: -454953.468750\n",
      "Train Epoch: 380 [32768/54000 (61%)] Loss: -448334.156250\n",
      "Train Epoch: 380 [36864/54000 (68%)] Loss: -452114.000000\n",
      "Train Epoch: 380 [40960/54000 (76%)] Loss: -468941.937500\n",
      "Train Epoch: 380 [45056/54000 (83%)] Loss: -467936.531250\n",
      "Train Epoch: 380 [49152/54000 (91%)] Loss: -504506.281250\n",
      "    epoch          : 380\n",
      "    loss           : -463232.29359756096\n",
      "    val_loss       : -462147.53359375\n",
      "Train Epoch: 381 [0/54000 (0%)] Loss: -467115.218750\n",
      "Train Epoch: 381 [4096/54000 (8%)] Loss: -449639.656250\n",
      "Train Epoch: 381 [8192/54000 (15%)] Loss: -456732.375000\n",
      "Train Epoch: 381 [12288/54000 (23%)] Loss: -504099.156250\n",
      "Train Epoch: 381 [16384/54000 (30%)] Loss: -469201.187500\n",
      "Train Epoch: 381 [20480/54000 (38%)] Loss: -455923.812500\n",
      "Train Epoch: 381 [24576/54000 (46%)] Loss: -467543.687500\n",
      "Train Epoch: 381 [28672/54000 (53%)] Loss: -452132.531250\n",
      "Train Epoch: 381 [32768/54000 (61%)] Loss: -465889.031250\n",
      "Train Epoch: 381 [36864/54000 (68%)] Loss: -468805.875000\n",
      "Train Epoch: 381 [40960/54000 (76%)] Loss: -457527.093750\n",
      "Train Epoch: 381 [45056/54000 (83%)] Loss: -502678.843750\n",
      "Train Epoch: 381 [49152/54000 (91%)] Loss: -503963.625000\n",
      "    epoch          : 381\n",
      "    loss           : -463300.3068597561\n",
      "    val_loss       : -462065.94765625\n",
      "Train Epoch: 382 [0/54000 (0%)] Loss: -468740.218750\n",
      "Train Epoch: 382 [4096/54000 (8%)] Loss: -461718.562500\n",
      "Train Epoch: 382 [8192/54000 (15%)] Loss: -457188.218750\n",
      "Train Epoch: 382 [12288/54000 (23%)] Loss: -464177.875000\n",
      "Train Epoch: 382 [16384/54000 (30%)] Loss: -451021.906250\n",
      "Train Epoch: 382 [20480/54000 (38%)] Loss: -467739.593750\n",
      "Train Epoch: 382 [24576/54000 (46%)] Loss: -462905.156250\n",
      "Train Epoch: 382 [28672/54000 (53%)] Loss: -457429.593750\n",
      "Train Epoch: 382 [32768/54000 (61%)] Loss: -455997.125000\n",
      "Train Epoch: 382 [36864/54000 (68%)] Loss: -466443.406250\n",
      "Train Epoch: 382 [40960/54000 (76%)] Loss: -458561.187500\n",
      "Train Epoch: 382 [45056/54000 (83%)] Loss: -449326.750000\n",
      "Train Epoch: 382 [49152/54000 (91%)] Loss: -503937.656250\n",
      "    epoch          : 382\n",
      "    loss           : -462977.8649390244\n",
      "    val_loss       : -462012.99453125\n",
      "Train Epoch: 383 [0/54000 (0%)] Loss: -502748.125000\n",
      "Train Epoch: 383 [4096/54000 (8%)] Loss: -449571.781250\n",
      "Train Epoch: 383 [8192/54000 (15%)] Loss: -459855.312500\n",
      "Train Epoch: 383 [12288/54000 (23%)] Loss: -465254.250000\n",
      "Train Epoch: 383 [16384/54000 (30%)] Loss: -453366.437500\n",
      "Train Epoch: 383 [20480/54000 (38%)] Loss: -470289.562500\n",
      "Train Epoch: 383 [24576/54000 (46%)] Loss: -449811.187500\n",
      "Train Epoch: 383 [28672/54000 (53%)] Loss: -456236.656250\n",
      "Train Epoch: 383 [32768/54000 (61%)] Loss: -470793.937500\n",
      "Train Epoch: 383 [36864/54000 (68%)] Loss: -450284.218750\n",
      "Train Epoch: 383 [40960/54000 (76%)] Loss: -448779.406250\n",
      "Train Epoch: 383 [45056/54000 (83%)] Loss: -458071.843750\n",
      "Train Epoch: 383 [49152/54000 (91%)] Loss: -455010.437500\n",
      "    epoch          : 383\n",
      "    loss           : -463358.7173780488\n",
      "    val_loss       : -462126.80390625\n",
      "Train Epoch: 384 [0/54000 (0%)] Loss: -503455.062500\n",
      "Train Epoch: 384 [4096/54000 (8%)] Loss: -451424.281250\n",
      "Train Epoch: 384 [8192/54000 (15%)] Loss: -456367.031250\n",
      "Train Epoch: 384 [12288/54000 (23%)] Loss: -451707.500000\n",
      "Train Epoch: 384 [16384/54000 (30%)] Loss: -465885.062500\n",
      "Train Epoch: 384 [20480/54000 (38%)] Loss: -467528.250000\n",
      "Train Epoch: 384 [24576/54000 (46%)] Loss: -443564.937500\n",
      "Train Epoch: 384 [28672/54000 (53%)] Loss: -503925.500000\n",
      "Train Epoch: 384 [32768/54000 (61%)] Loss: -469995.687500\n",
      "Train Epoch: 384 [36864/54000 (68%)] Loss: -501880.656250\n",
      "Train Epoch: 384 [40960/54000 (76%)] Loss: -447720.937500\n",
      "Train Epoch: 384 [45056/54000 (83%)] Loss: -453212.562500\n",
      "Train Epoch: 384 [49152/54000 (91%)] Loss: -502878.500000\n",
      "    epoch          : 384\n",
      "    loss           : -463398.7224085366\n",
      "    val_loss       : -462444.24296875\n",
      "Train Epoch: 385 [0/54000 (0%)] Loss: -502276.937500\n",
      "Train Epoch: 385 [4096/54000 (8%)] Loss: -454606.812500\n",
      "Train Epoch: 385 [8192/54000 (15%)] Loss: -454157.875000\n",
      "Train Epoch: 385 [12288/54000 (23%)] Loss: -465136.312500\n",
      "Train Epoch: 385 [16384/54000 (30%)] Loss: -451533.593750\n",
      "Train Epoch: 385 [20480/54000 (38%)] Loss: -469683.000000\n",
      "Train Epoch: 385 [24576/54000 (46%)] Loss: -450940.125000\n",
      "Train Epoch: 385 [28672/54000 (53%)] Loss: -503677.531250\n",
      "Train Epoch: 385 [32768/54000 (61%)] Loss: -459810.375000\n",
      "Train Epoch: 385 [36864/54000 (68%)] Loss: -451419.093750\n",
      "Train Epoch: 385 [40960/54000 (76%)] Loss: -457099.500000\n",
      "Train Epoch: 385 [45056/54000 (83%)] Loss: -468962.562500\n",
      "Train Epoch: 385 [49152/54000 (91%)] Loss: -504358.343750\n",
      "    epoch          : 385\n",
      "    loss           : -463398.1033536585\n",
      "    val_loss       : -462470.221875\n",
      "Train Epoch: 386 [0/54000 (0%)] Loss: -503308.937500\n",
      "Train Epoch: 386 [4096/54000 (8%)] Loss: -456278.968750\n",
      "Train Epoch: 386 [8192/54000 (15%)] Loss: -466920.500000\n",
      "Train Epoch: 386 [12288/54000 (23%)] Loss: -503199.968750\n",
      "Train Epoch: 386 [16384/54000 (30%)] Loss: -467879.968750\n",
      "Train Epoch: 386 [20480/54000 (38%)] Loss: -501088.156250\n",
      "Train Epoch: 386 [24576/54000 (46%)] Loss: -452386.093750\n",
      "Train Epoch: 386 [28672/54000 (53%)] Loss: -455812.187500\n",
      "Train Epoch: 386 [32768/54000 (61%)] Loss: -503664.125000\n",
      "Train Epoch: 386 [36864/54000 (68%)] Loss: -467589.437500\n",
      "Train Epoch: 386 [40960/54000 (76%)] Loss: -451223.562500\n",
      "Train Epoch: 386 [45056/54000 (83%)] Loss: -466762.437500\n",
      "Train Epoch: 386 [49152/54000 (91%)] Loss: -503397.062500\n",
      "    epoch          : 386\n",
      "    loss           : -463462.28643292683\n",
      "    val_loss       : -462228.5453125\n",
      "Train Epoch: 387 [0/54000 (0%)] Loss: -466569.750000\n",
      "Train Epoch: 387 [4096/54000 (8%)] Loss: -450006.531250\n",
      "Train Epoch: 387 [8192/54000 (15%)] Loss: -459000.625000\n",
      "Train Epoch: 387 [12288/54000 (23%)] Loss: -449785.437500\n",
      "Train Epoch: 387 [16384/54000 (30%)] Loss: -458104.343750\n",
      "Train Epoch: 387 [20480/54000 (38%)] Loss: -467714.593750\n",
      "Train Epoch: 387 [24576/54000 (46%)] Loss: -469576.250000\n",
      "Train Epoch: 387 [28672/54000 (53%)] Loss: -455855.093750\n",
      "Train Epoch: 387 [32768/54000 (61%)] Loss: -449123.375000\n",
      "Train Epoch: 387 [36864/54000 (68%)] Loss: -453234.625000\n",
      "Train Epoch: 387 [40960/54000 (76%)] Loss: -450316.187500\n",
      "Train Epoch: 387 [45056/54000 (83%)] Loss: -468902.312500\n",
      "Train Epoch: 387 [49152/54000 (91%)] Loss: -504750.406250\n",
      "    epoch          : 387\n",
      "    loss           : -463127.09420731704\n",
      "    val_loss       : -461392.846875\n",
      "Train Epoch: 388 [0/54000 (0%)] Loss: -451165.312500\n",
      "Train Epoch: 388 [4096/54000 (8%)] Loss: -451290.343750\n",
      "Train Epoch: 388 [8192/54000 (15%)] Loss: -467629.750000\n",
      "Train Epoch: 388 [12288/54000 (23%)] Loss: -459762.437500\n",
      "Train Epoch: 388 [16384/54000 (30%)] Loss: -450482.875000\n",
      "Train Epoch: 388 [20480/54000 (38%)] Loss: -459439.343750\n",
      "Train Epoch: 388 [24576/54000 (46%)] Loss: -453450.187500\n",
      "Train Epoch: 388 [28672/54000 (53%)] Loss: -455971.812500\n",
      "Train Epoch: 388 [32768/54000 (61%)] Loss: -468180.437500\n",
      "Train Epoch: 388 [36864/54000 (68%)] Loss: -504124.812500\n",
      "Train Epoch: 388 [40960/54000 (76%)] Loss: -462793.500000\n",
      "Train Epoch: 388 [45056/54000 (83%)] Loss: -467583.406250\n",
      "Train Epoch: 388 [49152/54000 (91%)] Loss: -464947.718750\n",
      "    epoch          : 388\n",
      "    loss           : -463575.3467987805\n",
      "    val_loss       : -462501.80390625\n",
      "Train Epoch: 389 [0/54000 (0%)] Loss: -503783.750000\n",
      "Train Epoch: 389 [4096/54000 (8%)] Loss: -467739.062500\n",
      "Train Epoch: 389 [8192/54000 (15%)] Loss: -452434.156250\n",
      "Train Epoch: 389 [12288/54000 (23%)] Loss: -458460.937500\n",
      "Train Epoch: 389 [16384/54000 (30%)] Loss: -468643.718750\n",
      "Train Epoch: 389 [20480/54000 (38%)] Loss: -469193.875000\n",
      "Train Epoch: 389 [24576/54000 (46%)] Loss: -452110.062500\n",
      "Train Epoch: 389 [28672/54000 (53%)] Loss: -503060.031250\n",
      "Train Epoch: 389 [32768/54000 (61%)] Loss: -459352.718750\n",
      "Train Epoch: 389 [36864/54000 (68%)] Loss: -449268.125000\n",
      "Train Epoch: 389 [40960/54000 (76%)] Loss: -456305.031250\n",
      "Train Epoch: 389 [45056/54000 (83%)] Loss: -448244.343750\n",
      "Train Epoch: 389 [49152/54000 (91%)] Loss: -503546.312500\n",
      "    epoch          : 389\n",
      "    loss           : -463458.97286585363\n",
      "    val_loss       : -462521.90390625\n",
      "Train Epoch: 390 [0/54000 (0%)] Loss: -457593.593750\n",
      "Train Epoch: 390 [4096/54000 (8%)] Loss: -449817.156250\n",
      "Train Epoch: 390 [8192/54000 (15%)] Loss: -456332.375000\n",
      "Train Epoch: 390 [12288/54000 (23%)] Loss: -449976.000000\n",
      "Train Epoch: 390 [16384/54000 (30%)] Loss: -450196.343750\n",
      "Train Epoch: 390 [20480/54000 (38%)] Loss: -468288.093750\n",
      "Train Epoch: 390 [24576/54000 (46%)] Loss: -451434.968750\n",
      "Train Epoch: 390 [28672/54000 (53%)] Loss: -454809.343750\n",
      "Train Epoch: 390 [32768/54000 (61%)] Loss: -454992.125000\n",
      "Train Epoch: 390 [36864/54000 (68%)] Loss: -452309.468750\n",
      "Train Epoch: 390 [40960/54000 (76%)] Loss: -451512.843750\n",
      "Train Epoch: 390 [45056/54000 (83%)] Loss: -468217.687500\n",
      "Train Epoch: 390 [49152/54000 (91%)] Loss: -449216.718750\n",
      "    epoch          : 390\n",
      "    loss           : -463525.77073170734\n",
      "    val_loss       : -462273.79453125\n",
      "Train Epoch: 391 [0/54000 (0%)] Loss: -503453.437500\n",
      "Train Epoch: 391 [4096/54000 (8%)] Loss: -457216.125000\n",
      "Train Epoch: 391 [8192/54000 (15%)] Loss: -469838.406250\n",
      "Train Epoch: 391 [12288/54000 (23%)] Loss: -464477.687500\n",
      "Train Epoch: 391 [16384/54000 (30%)] Loss: -456311.375000\n",
      "Train Epoch: 391 [20480/54000 (38%)] Loss: -468665.593750\n",
      "Train Epoch: 391 [24576/54000 (46%)] Loss: -458302.093750\n",
      "Train Epoch: 391 [28672/54000 (53%)] Loss: -458023.968750\n",
      "Train Epoch: 391 [32768/54000 (61%)] Loss: -449417.937500\n",
      "Train Epoch: 391 [36864/54000 (68%)] Loss: -466933.093750\n",
      "Train Epoch: 391 [40960/54000 (76%)] Loss: -454929.093750\n",
      "Train Epoch: 391 [45056/54000 (83%)] Loss: -449477.687500\n",
      "Train Epoch: 391 [49152/54000 (91%)] Loss: -503992.468750\n",
      "    epoch          : 391\n",
      "    loss           : -463396.3980182927\n",
      "    val_loss       : -462429.66875\n",
      "Train Epoch: 392 [0/54000 (0%)] Loss: -503310.125000\n",
      "Train Epoch: 392 [4096/54000 (8%)] Loss: -447102.218750\n",
      "Train Epoch: 392 [8192/54000 (15%)] Loss: -470095.750000\n",
      "Train Epoch: 392 [12288/54000 (23%)] Loss: -456224.406250\n",
      "Train Epoch: 392 [16384/54000 (30%)] Loss: -458869.562500\n",
      "Train Epoch: 392 [20480/54000 (38%)] Loss: -447164.468750\n",
      "Train Epoch: 392 [24576/54000 (46%)] Loss: -457338.375000\n",
      "Train Epoch: 392 [28672/54000 (53%)] Loss: -456096.562500\n",
      "Train Epoch: 392 [32768/54000 (61%)] Loss: -457330.750000\n",
      "Train Epoch: 392 [36864/54000 (68%)] Loss: -468006.562500\n",
      "Train Epoch: 392 [40960/54000 (76%)] Loss: -448488.437500\n",
      "Train Epoch: 392 [45056/54000 (83%)] Loss: -465177.875000\n",
      "Train Epoch: 392 [49152/54000 (91%)] Loss: -503685.437500\n",
      "    epoch          : 392\n",
      "    loss           : -463439.68414634146\n",
      "    val_loss       : -462465.8859375\n",
      "Train Epoch: 393 [0/54000 (0%)] Loss: -453491.593750\n",
      "Train Epoch: 393 [4096/54000 (8%)] Loss: -458669.500000\n",
      "Train Epoch: 393 [8192/54000 (15%)] Loss: -467032.312500\n",
      "Train Epoch: 393 [12288/54000 (23%)] Loss: -448888.875000\n",
      "Train Epoch: 393 [16384/54000 (30%)] Loss: -449860.187500\n",
      "Train Epoch: 393 [20480/54000 (38%)] Loss: -448018.812500\n",
      "Train Epoch: 393 [24576/54000 (46%)] Loss: -468681.500000\n",
      "Train Epoch: 393 [28672/54000 (53%)] Loss: -456572.687500\n",
      "Train Epoch: 393 [32768/54000 (61%)] Loss: -469182.625000\n",
      "Train Epoch: 393 [36864/54000 (68%)] Loss: -449933.656250\n",
      "Train Epoch: 393 [40960/54000 (76%)] Loss: -463902.031250\n",
      "Train Epoch: 393 [45056/54000 (83%)] Loss: -458063.156250\n",
      "Train Epoch: 393 [49152/54000 (91%)] Loss: -503236.375000\n",
      "    epoch          : 393\n",
      "    loss           : -463633.38018292683\n",
      "    val_loss       : -462275.553125\n",
      "Train Epoch: 394 [0/54000 (0%)] Loss: -502458.000000\n",
      "Train Epoch: 394 [4096/54000 (8%)] Loss: -458126.093750\n",
      "Train Epoch: 394 [8192/54000 (15%)] Loss: -462892.125000\n",
      "Train Epoch: 394 [12288/54000 (23%)] Loss: -503925.000000\n",
      "Train Epoch: 394 [16384/54000 (30%)] Loss: -468472.250000\n",
      "Train Epoch: 394 [20480/54000 (38%)] Loss: -469498.375000\n",
      "Train Epoch: 394 [24576/54000 (46%)] Loss: -459679.156250\n",
      "Train Epoch: 394 [28672/54000 (53%)] Loss: -455939.000000\n",
      "Train Epoch: 394 [32768/54000 (61%)] Loss: -467823.812500\n",
      "Train Epoch: 394 [36864/54000 (68%)] Loss: -468333.000000\n",
      "Train Epoch: 394 [40960/54000 (76%)] Loss: -458795.031250\n",
      "Train Epoch: 394 [45056/54000 (83%)] Loss: -452709.031250\n",
      "Train Epoch: 394 [49152/54000 (91%)] Loss: -503990.250000\n",
      "    epoch          : 394\n",
      "    loss           : -463622.1762195122\n",
      "    val_loss       : -462262.015625\n",
      "Train Epoch: 395 [0/54000 (0%)] Loss: -504927.562500\n",
      "Train Epoch: 395 [4096/54000 (8%)] Loss: -463772.812500\n",
      "Train Epoch: 395 [8192/54000 (15%)] Loss: -458218.562500\n",
      "Train Epoch: 395 [12288/54000 (23%)] Loss: -458228.562500\n",
      "Train Epoch: 395 [16384/54000 (30%)] Loss: -453379.625000\n",
      "Train Epoch: 395 [20480/54000 (38%)] Loss: -469825.656250\n",
      "Train Epoch: 395 [24576/54000 (46%)] Loss: -449466.750000\n",
      "Train Epoch: 395 [28672/54000 (53%)] Loss: -455583.625000\n",
      "Train Epoch: 395 [32768/54000 (61%)] Loss: -502667.843750\n",
      "Train Epoch: 395 [36864/54000 (68%)] Loss: -502603.343750\n",
      "Train Epoch: 395 [40960/54000 (76%)] Loss: -450903.343750\n",
      "Train Epoch: 395 [45056/54000 (83%)] Loss: -468759.250000\n",
      "Train Epoch: 395 [49152/54000 (91%)] Loss: -503642.875000\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   395: reducing learning rate of group 0 to 2.5000e-05.\n",
      "    epoch          : 395\n",
      "    loss           : -463527.0710365854\n",
      "    val_loss       : -462478.73359375\n",
      "Train Epoch: 396 [0/54000 (0%)] Loss: -503857.687500\n",
      "Train Epoch: 396 [4096/54000 (8%)] Loss: -449579.437500\n",
      "Train Epoch: 396 [8192/54000 (15%)] Loss: -457265.718750\n",
      "Train Epoch: 396 [12288/54000 (23%)] Loss: -465331.750000\n",
      "Train Epoch: 396 [16384/54000 (30%)] Loss: -450626.187500\n",
      "Train Epoch: 396 [20480/54000 (38%)] Loss: -469262.125000\n",
      "Train Epoch: 396 [24576/54000 (46%)] Loss: -458165.406250\n",
      "Train Epoch: 396 [28672/54000 (53%)] Loss: -456112.500000\n",
      "Train Epoch: 396 [32768/54000 (61%)] Loss: -458306.000000\n",
      "Train Epoch: 396 [36864/54000 (68%)] Loss: -467587.437500\n",
      "Train Epoch: 396 [40960/54000 (76%)] Loss: -468272.343750\n",
      "Train Epoch: 396 [45056/54000 (83%)] Loss: -464435.250000\n",
      "Train Epoch: 396 [49152/54000 (91%)] Loss: -504093.500000\n",
      "    epoch          : 396\n",
      "    loss           : -463695.2493902439\n",
      "    val_loss       : -462502.56796875\n",
      "Train Epoch: 397 [0/54000 (0%)] Loss: -503754.250000\n",
      "Train Epoch: 397 [4096/54000 (8%)] Loss: -450675.000000\n",
      "Train Epoch: 397 [8192/54000 (15%)] Loss: -457170.468750\n",
      "Train Epoch: 397 [12288/54000 (23%)] Loss: -457952.656250\n",
      "Train Epoch: 397 [16384/54000 (30%)] Loss: -451439.187500\n",
      "Train Epoch: 397 [20480/54000 (38%)] Loss: -469495.843750\n",
      "Train Epoch: 397 [24576/54000 (46%)] Loss: -450239.906250\n",
      "Train Epoch: 397 [28672/54000 (53%)] Loss: -469994.968750\n",
      "Train Epoch: 397 [32768/54000 (61%)] Loss: -466338.062500\n",
      "Train Epoch: 397 [36864/54000 (68%)] Loss: -452210.875000\n",
      "Train Epoch: 397 [40960/54000 (76%)] Loss: -453756.937500\n",
      "Train Epoch: 397 [45056/54000 (83%)] Loss: -468034.468750\n",
      "Train Epoch: 397 [49152/54000 (91%)] Loss: -504218.062500\n",
      "    epoch          : 397\n",
      "    loss           : -463707.5070121951\n",
      "    val_loss       : -462684.009375\n",
      "Train Epoch: 398 [0/54000 (0%)] Loss: -503466.000000\n",
      "Train Epoch: 398 [4096/54000 (8%)] Loss: -464694.437500\n",
      "Train Epoch: 398 [8192/54000 (15%)] Loss: -457907.500000\n",
      "Train Epoch: 398 [12288/54000 (23%)] Loss: -449781.500000\n",
      "Train Epoch: 398 [16384/54000 (30%)] Loss: -458359.250000\n",
      "Train Epoch: 398 [20480/54000 (38%)] Loss: -457119.625000\n",
      "Train Epoch: 398 [24576/54000 (46%)] Loss: -467892.187500\n",
      "Train Epoch: 398 [28672/54000 (53%)] Loss: -503811.500000\n",
      "Train Epoch: 398 [32768/54000 (61%)] Loss: -463190.343750\n",
      "Train Epoch: 398 [36864/54000 (68%)] Loss: -467535.250000\n",
      "Train Epoch: 398 [40960/54000 (76%)] Loss: -468929.500000\n",
      "Train Epoch: 398 [45056/54000 (83%)] Loss: -452381.062500\n",
      "Train Epoch: 398 [49152/54000 (91%)] Loss: -501689.125000\n",
      "    epoch          : 398\n",
      "    loss           : -463681.1021341463\n",
      "    val_loss       : -462419.12421875\n",
      "Train Epoch: 399 [0/54000 (0%)] Loss: -504319.937500\n",
      "Train Epoch: 399 [4096/54000 (8%)] Loss: -468713.531250\n",
      "Train Epoch: 399 [8192/54000 (15%)] Loss: -465270.281250\n",
      "Train Epoch: 399 [12288/54000 (23%)] Loss: -449500.500000\n",
      "Train Epoch: 399 [16384/54000 (30%)] Loss: -504136.718750\n",
      "Train Epoch: 399 [20480/54000 (38%)] Loss: -467138.656250\n",
      "Train Epoch: 399 [24576/54000 (46%)] Loss: -465013.406250\n",
      "Train Epoch: 399 [28672/54000 (53%)] Loss: -451623.625000\n",
      "Train Epoch: 399 [32768/54000 (61%)] Loss: -449357.312500\n",
      "Train Epoch: 399 [36864/54000 (68%)] Loss: -449238.000000\n",
      "Train Epoch: 399 [40960/54000 (76%)] Loss: -450475.687500\n",
      "Train Epoch: 399 [45056/54000 (83%)] Loss: -460667.375000\n",
      "Train Epoch: 399 [49152/54000 (91%)] Loss: -503687.000000\n",
      "    epoch          : 399\n",
      "    loss           : -463649.45472560974\n",
      "    val_loss       : -462604.20703125\n",
      "Train Epoch: 400 [0/54000 (0%)] Loss: -453090.406250\n",
      "Train Epoch: 400 [4096/54000 (8%)] Loss: -457715.500000\n",
      "Train Epoch: 400 [8192/54000 (15%)] Loss: -467319.718750\n",
      "Train Epoch: 400 [12288/54000 (23%)] Loss: -449541.125000\n",
      "Train Epoch: 400 [16384/54000 (30%)] Loss: -450766.843750\n",
      "Train Epoch: 400 [20480/54000 (38%)] Loss: -468558.000000\n",
      "Train Epoch: 400 [24576/54000 (46%)] Loss: -458418.000000\n",
      "Train Epoch: 400 [28672/54000 (53%)] Loss: -504653.718750\n",
      "Train Epoch: 400 [32768/54000 (61%)] Loss: -448792.937500\n",
      "Train Epoch: 400 [36864/54000 (68%)] Loss: -450805.625000\n",
      "Train Epoch: 400 [40960/54000 (76%)] Loss: -457138.812500\n",
      "Train Epoch: 400 [45056/54000 (83%)] Loss: -469796.343750\n",
      "Train Epoch: 400 [49152/54000 (91%)] Loss: -502768.187500\n",
      "    epoch          : 400\n",
      "    loss           : -463695.78993902437\n",
      "    val_loss       : -462315.3453125\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0510_004522/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [0/54000 (0%)] Loss: -502880.312500\n",
      "Train Epoch: 401 [4096/54000 (8%)] Loss: -451395.937500\n",
      "Train Epoch: 401 [8192/54000 (15%)] Loss: -467112.468750\n",
      "Train Epoch: 401 [12288/54000 (23%)] Loss: -448606.437500\n",
      "Train Epoch: 401 [16384/54000 (30%)] Loss: -502728.531250\n",
      "Train Epoch: 401 [20480/54000 (38%)] Loss: -470282.843750\n",
      "Train Epoch: 401 [24576/54000 (46%)] Loss: -456642.500000\n",
      "Train Epoch: 401 [28672/54000 (53%)] Loss: -451972.937500\n",
      "Train Epoch: 401 [32768/54000 (61%)] Loss: -451365.375000\n",
      "Train Epoch: 401 [36864/54000 (68%)] Loss: -451104.500000\n",
      "Train Epoch: 401 [40960/54000 (76%)] Loss: -448859.187500\n",
      "Train Epoch: 401 [45056/54000 (83%)] Loss: -449255.843750\n",
      "Train Epoch: 401 [49152/54000 (91%)] Loss: -503978.375000\n",
      "    epoch          : 401\n",
      "    loss           : -463624.90853658534\n",
      "    val_loss       : -462557.7140625\n",
      "Train Epoch: 402 [0/54000 (0%)] Loss: -503611.437500\n",
      "Train Epoch: 402 [4096/54000 (8%)] Loss: -469170.437500\n",
      "Train Epoch: 402 [8192/54000 (15%)] Loss: -469002.562500\n",
      "Train Epoch: 402 [12288/54000 (23%)] Loss: -450817.000000\n",
      "Train Epoch: 402 [16384/54000 (30%)] Loss: -468765.968750\n",
      "Train Epoch: 402 [20480/54000 (38%)] Loss: -468495.125000\n",
      "Train Epoch: 402 [24576/54000 (46%)] Loss: -466395.468750\n",
      "Train Epoch: 402 [28672/54000 (53%)] Loss: -454075.656250\n",
      "Train Epoch: 402 [32768/54000 (61%)] Loss: -503852.656250\n",
      "Train Epoch: 402 [36864/54000 (68%)] Loss: -504844.750000\n",
      "Train Epoch: 402 [40960/54000 (76%)] Loss: -464559.781250\n",
      "Train Epoch: 402 [45056/54000 (83%)] Loss: -454688.562500\n",
      "Train Epoch: 402 [49152/54000 (91%)] Loss: -503062.281250\n",
      "    epoch          : 402\n",
      "    loss           : -463734.1141768293\n",
      "    val_loss       : -462591.01015625\n",
      "Train Epoch: 403 [0/54000 (0%)] Loss: -503926.281250\n",
      "Train Epoch: 403 [4096/54000 (8%)] Loss: -463806.125000\n",
      "Train Epoch: 403 [8192/54000 (15%)] Loss: -455384.750000\n",
      "Train Epoch: 403 [12288/54000 (23%)] Loss: -451851.843750\n",
      "Train Epoch: 403 [16384/54000 (30%)] Loss: -450755.125000\n",
      "Train Epoch: 403 [20480/54000 (38%)] Loss: -468850.062500\n",
      "Train Epoch: 403 [24576/54000 (46%)] Loss: -452479.937500\n",
      "Train Epoch: 403 [28672/54000 (53%)] Loss: -502187.156250\n",
      "Train Epoch: 403 [32768/54000 (61%)] Loss: -469437.781250\n",
      "Train Epoch: 403 [36864/54000 (68%)] Loss: -469737.531250\n",
      "Train Epoch: 403 [40960/54000 (76%)] Loss: -451218.406250\n",
      "Train Epoch: 403 [45056/54000 (83%)] Loss: -446499.562500\n",
      "Train Epoch: 403 [49152/54000 (91%)] Loss: -503986.843750\n",
      "    epoch          : 403\n",
      "    loss           : -463523.6032012195\n",
      "    val_loss       : -462477.84609375\n",
      "Train Epoch: 404 [0/54000 (0%)] Loss: -503829.250000\n",
      "Train Epoch: 404 [4096/54000 (8%)] Loss: -449638.656250\n",
      "Train Epoch: 404 [8192/54000 (15%)] Loss: -468009.250000\n",
      "Train Epoch: 404 [12288/54000 (23%)] Loss: -458175.625000\n",
      "Train Epoch: 404 [16384/54000 (30%)] Loss: -452040.031250\n",
      "Train Epoch: 404 [20480/54000 (38%)] Loss: -458842.343750\n",
      "Train Epoch: 404 [24576/54000 (46%)] Loss: -449565.250000\n",
      "Train Epoch: 404 [28672/54000 (53%)] Loss: -503361.750000\n",
      "Train Epoch: 404 [32768/54000 (61%)] Loss: -465127.718750\n",
      "Train Epoch: 404 [36864/54000 (68%)] Loss: -449575.687500\n",
      "Train Epoch: 404 [40960/54000 (76%)] Loss: -457810.468750\n",
      "Train Epoch: 404 [45056/54000 (83%)] Loss: -451748.375000\n",
      "Train Epoch: 404 [49152/54000 (91%)] Loss: -503689.718750\n",
      "    epoch          : 404\n",
      "    loss           : -463628.66493902437\n",
      "    val_loss       : -462704.334375\n",
      "Train Epoch: 405 [0/54000 (0%)] Loss: -468661.093750\n",
      "Train Epoch: 405 [4096/54000 (8%)] Loss: -465684.531250\n",
      "Train Epoch: 405 [8192/54000 (15%)] Loss: -454600.625000\n",
      "Train Epoch: 405 [12288/54000 (23%)] Loss: -464294.875000\n",
      "Train Epoch: 405 [16384/54000 (30%)] Loss: -451189.062500\n",
      "Train Epoch: 405 [20480/54000 (38%)] Loss: -469068.500000\n",
      "Train Epoch: 405 [24576/54000 (46%)] Loss: -464726.187500\n",
      "Train Epoch: 405 [28672/54000 (53%)] Loss: -504692.781250\n",
      "Train Epoch: 405 [32768/54000 (61%)] Loss: -458762.875000\n",
      "Train Epoch: 405 [36864/54000 (68%)] Loss: -458727.750000\n",
      "Train Epoch: 405 [40960/54000 (76%)] Loss: -458498.437500\n",
      "Train Epoch: 405 [45056/54000 (83%)] Loss: -465245.312500\n",
      "Train Epoch: 405 [49152/54000 (91%)] Loss: -504295.875000\n",
      "    epoch          : 405\n",
      "    loss           : -463711.7163109756\n",
      "    val_loss       : -462519.309375\n",
      "Train Epoch: 406 [0/54000 (0%)] Loss: -468780.750000\n",
      "Train Epoch: 406 [4096/54000 (8%)] Loss: -458416.937500\n",
      "Train Epoch: 406 [8192/54000 (15%)] Loss: -448912.343750\n",
      "Train Epoch: 406 [12288/54000 (23%)] Loss: -464139.625000\n",
      "Train Epoch: 406 [16384/54000 (30%)] Loss: -451246.875000\n",
      "Train Epoch: 406 [20480/54000 (38%)] Loss: -468028.125000\n",
      "Train Epoch: 406 [24576/54000 (46%)] Loss: -459302.500000\n",
      "Train Epoch: 406 [28672/54000 (53%)] Loss: -467455.718750\n",
      "Train Epoch: 406 [32768/54000 (61%)] Loss: -450457.250000\n",
      "Train Epoch: 406 [36864/54000 (68%)] Loss: -503282.062500\n",
      "Train Epoch: 406 [40960/54000 (76%)] Loss: -458333.187500\n",
      "Train Epoch: 406 [45056/54000 (83%)] Loss: -470259.625000\n",
      "Train Epoch: 406 [49152/54000 (91%)] Loss: -504479.812500\n",
      "    epoch          : 406\n",
      "    loss           : -463842.96875\n",
      "    val_loss       : -462622.94609375\n",
      "Train Epoch: 407 [0/54000 (0%)] Loss: -502458.343750\n",
      "Train Epoch: 407 [4096/54000 (8%)] Loss: -451340.812500\n",
      "Train Epoch: 407 [8192/54000 (15%)] Loss: -468095.218750\n",
      "Train Epoch: 407 [12288/54000 (23%)] Loss: -504014.156250\n",
      "Train Epoch: 407 [16384/54000 (30%)] Loss: -504556.031250\n",
      "Train Epoch: 407 [20480/54000 (38%)] Loss: -467621.187500\n",
      "Train Epoch: 407 [24576/54000 (46%)] Loss: -450636.562500\n",
      "Train Epoch: 407 [28672/54000 (53%)] Loss: -457230.312500\n",
      "Train Epoch: 407 [32768/54000 (61%)] Loss: -450960.843750\n",
      "Train Epoch: 407 [36864/54000 (68%)] Loss: -503918.031250\n",
      "Train Epoch: 407 [40960/54000 (76%)] Loss: -451350.562500\n",
      "Train Epoch: 407 [45056/54000 (83%)] Loss: -454925.750000\n",
      "Train Epoch: 407 [49152/54000 (91%)] Loss: -504190.687500\n",
      "    epoch          : 407\n",
      "    loss           : -463827.5867378049\n",
      "    val_loss       : -462369.7515625\n",
      "Train Epoch: 408 [0/54000 (0%)] Loss: -504594.968750\n",
      "Train Epoch: 408 [4096/54000 (8%)] Loss: -452196.000000\n",
      "Train Epoch: 408 [8192/54000 (15%)] Loss: -469398.406250\n",
      "Train Epoch: 408 [12288/54000 (23%)] Loss: -447956.687500\n",
      "Train Epoch: 408 [16384/54000 (30%)] Loss: -502594.500000\n",
      "Train Epoch: 408 [20480/54000 (38%)] Loss: -469021.343750\n",
      "Train Epoch: 408 [24576/54000 (46%)] Loss: -458952.062500\n",
      "Train Epoch: 408 [28672/54000 (53%)] Loss: -456137.187500\n",
      "Train Epoch: 408 [32768/54000 (61%)] Loss: -504407.593750\n",
      "Train Epoch: 408 [36864/54000 (68%)] Loss: -452103.375000\n",
      "Train Epoch: 408 [40960/54000 (76%)] Loss: -466042.250000\n",
      "Train Epoch: 408 [45056/54000 (83%)] Loss: -459266.968750\n",
      "Train Epoch: 408 [49152/54000 (91%)] Loss: -463844.875000\n",
      "    epoch          : 408\n",
      "    loss           : -463865.69801829266\n",
      "    val_loss       : -462857.68203125\n",
      "Train Epoch: 409 [0/54000 (0%)] Loss: -503872.187500\n",
      "Train Epoch: 409 [4096/54000 (8%)] Loss: -465105.062500\n",
      "Train Epoch: 409 [8192/54000 (15%)] Loss: -457908.562500\n",
      "Train Epoch: 409 [12288/54000 (23%)] Loss: -465156.750000\n",
      "Train Epoch: 409 [16384/54000 (30%)] Loss: -469764.625000\n",
      "Train Epoch: 409 [20480/54000 (38%)] Loss: -468605.406250\n",
      "Train Epoch: 409 [24576/54000 (46%)] Loss: -454008.125000\n",
      "Train Epoch: 409 [28672/54000 (53%)] Loss: -503688.843750\n",
      "Train Epoch: 409 [32768/54000 (61%)] Loss: -457036.375000\n",
      "Train Epoch: 409 [36864/54000 (68%)] Loss: -450934.000000\n",
      "Train Epoch: 409 [40960/54000 (76%)] Loss: -464449.000000\n",
      "Train Epoch: 409 [45056/54000 (83%)] Loss: -452018.312500\n",
      "Train Epoch: 409 [49152/54000 (91%)] Loss: -504212.968750\n",
      "    epoch          : 409\n",
      "    loss           : -463836.13673780486\n",
      "    val_loss       : -462324.7375\n",
      "Train Epoch: 410 [0/54000 (0%)] Loss: -504817.250000\n",
      "Train Epoch: 410 [4096/54000 (8%)] Loss: -469648.656250\n",
      "Train Epoch: 410 [8192/54000 (15%)] Loss: -452044.781250\n",
      "Train Epoch: 410 [12288/54000 (23%)] Loss: -453243.718750\n",
      "Train Epoch: 410 [16384/54000 (30%)] Loss: -465268.531250\n",
      "Train Epoch: 410 [20480/54000 (38%)] Loss: -467463.718750\n",
      "Train Epoch: 410 [24576/54000 (46%)] Loss: -457679.687500\n",
      "Train Epoch: 410 [28672/54000 (53%)] Loss: -457011.187500\n",
      "Train Epoch: 410 [32768/54000 (61%)] Loss: -451140.250000\n",
      "Train Epoch: 410 [36864/54000 (68%)] Loss: -457636.125000\n",
      "Train Epoch: 410 [40960/54000 (76%)] Loss: -468658.875000\n",
      "Train Epoch: 410 [45056/54000 (83%)] Loss: -470035.125000\n",
      "Train Epoch: 410 [49152/54000 (91%)] Loss: -503378.437500\n",
      "    epoch          : 410\n",
      "    loss           : -463834.6525914634\n",
      "    val_loss       : -462725.6078125\n",
      "Train Epoch: 411 [0/54000 (0%)] Loss: -503279.875000\n",
      "Train Epoch: 411 [4096/54000 (8%)] Loss: -438097.343750\n",
      "Train Epoch: 411 [8192/54000 (15%)] Loss: -469446.625000\n",
      "Train Epoch: 411 [12288/54000 (23%)] Loss: -450365.687500\n",
      "Train Epoch: 411 [16384/54000 (30%)] Loss: -503506.718750\n",
      "Train Epoch: 411 [20480/54000 (38%)] Loss: -455700.375000\n",
      "Train Epoch: 411 [24576/54000 (46%)] Loss: -448184.156250\n",
      "Train Epoch: 411 [28672/54000 (53%)] Loss: -504994.937500\n",
      "Train Epoch: 411 [32768/54000 (61%)] Loss: -449044.812500\n",
      "Train Epoch: 411 [36864/54000 (68%)] Loss: -457520.500000\n",
      "Train Epoch: 411 [40960/54000 (76%)] Loss: -464496.843750\n",
      "Train Epoch: 411 [45056/54000 (83%)] Loss: -445175.937500\n",
      "Train Epoch: 411 [49152/54000 (91%)] Loss: -504031.250000\n",
      "    epoch          : 411\n",
      "    loss           : -463681.0407012195\n",
      "    val_loss       : -462427.53984375\n",
      "Train Epoch: 412 [0/54000 (0%)] Loss: -503324.281250\n",
      "Train Epoch: 412 [4096/54000 (8%)] Loss: -468244.437500\n",
      "Train Epoch: 412 [8192/54000 (15%)] Loss: -466776.781250\n",
      "Train Epoch: 412 [12288/54000 (23%)] Loss: -453895.437500\n",
      "Train Epoch: 412 [16384/54000 (30%)] Loss: -447673.218750\n",
      "Train Epoch: 412 [20480/54000 (38%)] Loss: -466764.500000\n",
      "Train Epoch: 412 [24576/54000 (46%)] Loss: -450369.500000\n",
      "Train Epoch: 412 [28672/54000 (53%)] Loss: -504720.281250\n",
      "Train Epoch: 412 [32768/54000 (61%)] Loss: -448540.625000\n",
      "Train Epoch: 412 [36864/54000 (68%)] Loss: -468098.375000\n",
      "Train Epoch: 412 [40960/54000 (76%)] Loss: -464513.250000\n",
      "Train Epoch: 412 [45056/54000 (83%)] Loss: -468182.312500\n",
      "Train Epoch: 412 [49152/54000 (91%)] Loss: -503478.562500\n",
      "    epoch          : 412\n",
      "    loss           : -463798.2842987805\n",
      "    val_loss       : -462538.45703125\n",
      "Train Epoch: 413 [0/54000 (0%)] Loss: -455810.656250\n",
      "Train Epoch: 413 [4096/54000 (8%)] Loss: -469645.125000\n",
      "Train Epoch: 413 [8192/54000 (15%)] Loss: -468103.937500\n",
      "Train Epoch: 413 [12288/54000 (23%)] Loss: -469110.812500\n",
      "Train Epoch: 413 [16384/54000 (30%)] Loss: -452904.781250\n",
      "Train Epoch: 413 [20480/54000 (38%)] Loss: -470842.437500\n",
      "Train Epoch: 413 [24576/54000 (46%)] Loss: -465406.375000\n",
      "Train Epoch: 413 [28672/54000 (53%)] Loss: -502713.500000\n",
      "Train Epoch: 413 [32768/54000 (61%)] Loss: -459467.187500\n",
      "Train Epoch: 413 [36864/54000 (68%)] Loss: -452280.187500\n",
      "Train Epoch: 413 [40960/54000 (76%)] Loss: -456342.531250\n",
      "Train Epoch: 413 [45056/54000 (83%)] Loss: -467660.718750\n",
      "Train Epoch: 413 [49152/54000 (91%)] Loss: -502267.906250\n",
      "    epoch          : 413\n",
      "    loss           : -463700.76356707315\n",
      "    val_loss       : -462663.7578125\n",
      "Train Epoch: 414 [0/54000 (0%)] Loss: -469644.500000\n",
      "Train Epoch: 414 [4096/54000 (8%)] Loss: -451060.062500\n",
      "Train Epoch: 414 [8192/54000 (15%)] Loss: -455447.812500\n",
      "Train Epoch: 414 [12288/54000 (23%)] Loss: -466066.468750\n",
      "Train Epoch: 414 [16384/54000 (30%)] Loss: -457608.750000\n",
      "Train Epoch: 414 [20480/54000 (38%)] Loss: -469136.656250\n",
      "Train Epoch: 414 [24576/54000 (46%)] Loss: -448138.812500\n",
      "Train Epoch: 414 [28672/54000 (53%)] Loss: -457429.531250\n",
      "Train Epoch: 414 [32768/54000 (61%)] Loss: -449991.375000\n",
      "Train Epoch: 414 [36864/54000 (68%)] Loss: -467552.593750\n",
      "Train Epoch: 414 [40960/54000 (76%)] Loss: -464062.562500\n",
      "Train Epoch: 414 [45056/54000 (83%)] Loss: -452759.250000\n",
      "Train Epoch: 414 [49152/54000 (91%)] Loss: -504049.562500\n",
      "    epoch          : 414\n",
      "    loss           : -463780.21722560975\n",
      "    val_loss       : -462600.99765625\n",
      "Train Epoch: 415 [0/54000 (0%)] Loss: -457770.593750\n",
      "Train Epoch: 415 [4096/54000 (8%)] Loss: -468009.718750\n",
      "Train Epoch: 415 [8192/54000 (15%)] Loss: -469191.375000\n",
      "Train Epoch: 415 [12288/54000 (23%)] Loss: -451850.093750\n",
      "Train Epoch: 415 [16384/54000 (30%)] Loss: -452778.406250\n",
      "Train Epoch: 415 [20480/54000 (38%)] Loss: -455686.687500\n",
      "Train Epoch: 415 [24576/54000 (46%)] Loss: -459453.000000\n",
      "Train Epoch: 415 [28672/54000 (53%)] Loss: -470949.687500\n",
      "Train Epoch: 415 [32768/54000 (61%)] Loss: -465832.937500\n",
      "Train Epoch: 415 [36864/54000 (68%)] Loss: -504267.531250\n",
      "Train Epoch: 415 [40960/54000 (76%)] Loss: -457969.625000\n",
      "Train Epoch: 415 [45056/54000 (83%)] Loss: -469196.156250\n",
      "Train Epoch: 415 [49152/54000 (91%)] Loss: -503903.500000\n",
      "    epoch          : 415\n",
      "    loss           : -463803.09420731704\n",
      "    val_loss       : -462396.64765625\n",
      "Train Epoch: 416 [0/54000 (0%)] Loss: -503735.937500\n",
      "Train Epoch: 416 [4096/54000 (8%)] Loss: -448358.750000\n",
      "Train Epoch: 416 [8192/54000 (15%)] Loss: -470167.062500\n",
      "Train Epoch: 416 [12288/54000 (23%)] Loss: -464090.093750\n",
      "Train Epoch: 416 [16384/54000 (30%)] Loss: -458696.687500\n",
      "Train Epoch: 416 [20480/54000 (38%)] Loss: -457958.156250\n",
      "Train Epoch: 416 [24576/54000 (46%)] Loss: -452027.468750\n",
      "Train Epoch: 416 [28672/54000 (53%)] Loss: -504059.062500\n",
      "Train Epoch: 416 [32768/54000 (61%)] Loss: -458016.343750\n",
      "Train Epoch: 416 [36864/54000 (68%)] Loss: -467954.312500\n",
      "Train Epoch: 416 [40960/54000 (76%)] Loss: -452533.031250\n",
      "Train Epoch: 416 [45056/54000 (83%)] Loss: -470710.093750\n",
      "Train Epoch: 416 [49152/54000 (91%)] Loss: -452829.312500\n",
      "    epoch          : 416\n",
      "    loss           : -463788.97911585367\n",
      "    val_loss       : -462676.47734375\n",
      "Train Epoch: 417 [0/54000 (0%)] Loss: -504771.125000\n",
      "Train Epoch: 417 [4096/54000 (8%)] Loss: -448862.125000\n",
      "Train Epoch: 417 [8192/54000 (15%)] Loss: -453845.750000\n",
      "Train Epoch: 417 [12288/54000 (23%)] Loss: -466007.125000\n",
      "Train Epoch: 417 [16384/54000 (30%)] Loss: -460327.500000\n",
      "Train Epoch: 417 [20480/54000 (38%)] Loss: -457953.093750\n",
      "Train Epoch: 417 [24576/54000 (46%)] Loss: -458463.687500\n",
      "Train Epoch: 417 [28672/54000 (53%)] Loss: -503074.187500\n",
      "Train Epoch: 417 [32768/54000 (61%)] Loss: -449708.500000\n",
      "Train Epoch: 417 [36864/54000 (68%)] Loss: -452245.250000\n",
      "Train Epoch: 417 [40960/54000 (76%)] Loss: -470406.375000\n",
      "Train Epoch: 417 [45056/54000 (83%)] Loss: -468393.406250\n",
      "Train Epoch: 417 [49152/54000 (91%)] Loss: -504131.781250\n",
      "    epoch          : 417\n",
      "    loss           : -463891.2768292683\n",
      "    val_loss       : -462366.45234375\n",
      "Train Epoch: 418 [0/54000 (0%)] Loss: -503798.406250\n",
      "Train Epoch: 418 [4096/54000 (8%)] Loss: -464140.968750\n",
      "Train Epoch: 418 [8192/54000 (15%)] Loss: -457737.000000\n",
      "Train Epoch: 418 [12288/54000 (23%)] Loss: -450490.906250\n",
      "Train Epoch: 418 [16384/54000 (30%)] Loss: -468155.437500\n",
      "Train Epoch: 418 [20480/54000 (38%)] Loss: -468691.375000\n",
      "Train Epoch: 418 [24576/54000 (46%)] Loss: -449246.062500\n",
      "Train Epoch: 418 [28672/54000 (53%)] Loss: -459250.968750\n",
      "Train Epoch: 418 [32768/54000 (61%)] Loss: -458198.937500\n",
      "Train Epoch: 418 [36864/54000 (68%)] Loss: -451712.125000\n",
      "Train Epoch: 418 [40960/54000 (76%)] Loss: -457653.625000\n",
      "Train Epoch: 418 [45056/54000 (83%)] Loss: -451608.375000\n",
      "Train Epoch: 418 [49152/54000 (91%)] Loss: -505197.750000\n",
      "    epoch          : 418\n",
      "    loss           : -463848.8057926829\n",
      "    val_loss       : -462611.35546875\n",
      "Train Epoch: 419 [0/54000 (0%)] Loss: -501838.375000\n",
      "Train Epoch: 419 [4096/54000 (8%)] Loss: -450527.750000\n",
      "Train Epoch: 419 [8192/54000 (15%)] Loss: -450189.000000\n",
      "Train Epoch: 419 [12288/54000 (23%)] Loss: -450289.562500\n",
      "Train Epoch: 419 [16384/54000 (30%)] Loss: -450794.156250\n",
      "Train Epoch: 419 [20480/54000 (38%)] Loss: -465321.812500\n",
      "Train Epoch: 419 [24576/54000 (46%)] Loss: -447675.468750\n",
      "Train Epoch: 419 [28672/54000 (53%)] Loss: -455661.781250\n",
      "Train Epoch: 419 [32768/54000 (61%)] Loss: -454376.906250\n",
      "Train Epoch: 419 [36864/54000 (68%)] Loss: -503014.375000\n",
      "Train Epoch: 419 [40960/54000 (76%)] Loss: -458386.562500\n",
      "Train Epoch: 419 [45056/54000 (83%)] Loss: -462946.375000\n",
      "Train Epoch: 419 [49152/54000 (91%)] Loss: -501708.031250\n",
      "    epoch          : 419\n",
      "    loss           : -463735.51951219514\n",
      "    val_loss       : -462644.375\n",
      "Train Epoch: 420 [0/54000 (0%)] Loss: -458836.375000\n",
      "Train Epoch: 420 [4096/54000 (8%)] Loss: -449801.406250\n",
      "Train Epoch: 420 [8192/54000 (15%)] Loss: -447436.906250\n",
      "Train Epoch: 420 [12288/54000 (23%)] Loss: -449675.500000\n",
      "Train Epoch: 420 [16384/54000 (30%)] Loss: -504354.781250\n",
      "Train Epoch: 420 [20480/54000 (38%)] Loss: -457561.000000\n",
      "Train Epoch: 420 [24576/54000 (46%)] Loss: -458875.000000\n",
      "Train Epoch: 420 [28672/54000 (53%)] Loss: -469466.187500\n",
      "Train Epoch: 420 [32768/54000 (61%)] Loss: -464544.062500\n",
      "Train Epoch: 420 [36864/54000 (68%)] Loss: -504257.937500\n",
      "Train Epoch: 420 [40960/54000 (76%)] Loss: -465255.875000\n",
      "Train Epoch: 420 [45056/54000 (83%)] Loss: -467717.687500\n",
      "Train Epoch: 420 [49152/54000 (91%)] Loss: -504729.906250\n",
      "    epoch          : 420\n",
      "    loss           : -463863.95640243904\n",
      "    val_loss       : -462640.453125\n",
      "Train Epoch: 421 [0/54000 (0%)] Loss: -467695.281250\n",
      "Train Epoch: 421 [4096/54000 (8%)] Loss: -451185.906250\n",
      "Train Epoch: 421 [8192/54000 (15%)] Loss: -455398.781250\n",
      "Train Epoch: 421 [12288/54000 (23%)] Loss: -456683.750000\n",
      "Train Epoch: 421 [16384/54000 (30%)] Loss: -463100.656250\n",
      "Train Epoch: 421 [20480/54000 (38%)] Loss: -468656.281250\n",
      "Train Epoch: 421 [24576/54000 (46%)] Loss: -451561.937500\n",
      "Train Epoch: 421 [28672/54000 (53%)] Loss: -457100.781250\n",
      "Train Epoch: 421 [32768/54000 (61%)] Loss: -468388.593750\n",
      "Train Epoch: 421 [36864/54000 (68%)] Loss: -449479.437500\n",
      "Train Epoch: 421 [40960/54000 (76%)] Loss: -467859.406250\n",
      "Train Epoch: 421 [45056/54000 (83%)] Loss: -470473.781250\n",
      "Train Epoch: 421 [49152/54000 (91%)] Loss: -504107.781250\n",
      "    epoch          : 421\n",
      "    loss           : -463839.36082317075\n",
      "    val_loss       : -462534.27265625\n",
      "Train Epoch: 422 [0/54000 (0%)] Loss: -502867.687500\n",
      "Train Epoch: 422 [4096/54000 (8%)] Loss: -451563.125000\n",
      "Train Epoch: 422 [8192/54000 (15%)] Loss: -469956.375000\n",
      "Train Epoch: 422 [12288/54000 (23%)] Loss: -464381.500000\n",
      "Train Epoch: 422 [16384/54000 (30%)] Loss: -468758.437500\n",
      "Train Epoch: 422 [20480/54000 (38%)] Loss: -464037.343750\n",
      "Train Epoch: 422 [24576/54000 (46%)] Loss: -466865.312500\n",
      "Train Epoch: 422 [28672/54000 (53%)] Loss: -456278.750000\n",
      "Train Epoch: 422 [32768/54000 (61%)] Loss: -458373.750000\n",
      "Train Epoch: 422 [36864/54000 (68%)] Loss: -467808.062500\n",
      "Train Epoch: 422 [40960/54000 (76%)] Loss: -464886.187500\n",
      "Train Epoch: 422 [45056/54000 (83%)] Loss: -454062.218750\n",
      "Train Epoch: 422 [49152/54000 (91%)] Loss: -503063.375000\n",
      "    epoch          : 422\n",
      "    loss           : -463869.5263719512\n",
      "    val_loss       : -462454.31328125\n",
      "Train Epoch: 423 [0/54000 (0%)] Loss: -450923.937500\n",
      "Train Epoch: 423 [4096/54000 (8%)] Loss: -470080.406250\n",
      "Train Epoch: 423 [8192/54000 (15%)] Loss: -465165.218750\n",
      "Train Epoch: 423 [12288/54000 (23%)] Loss: -449545.281250\n",
      "Train Epoch: 423 [16384/54000 (30%)] Loss: -469569.437500\n",
      "Train Epoch: 423 [20480/54000 (38%)] Loss: -458583.625000\n",
      "Train Epoch: 423 [24576/54000 (46%)] Loss: -458705.750000\n",
      "Train Epoch: 423 [28672/54000 (53%)] Loss: -457876.625000\n",
      "Train Epoch: 423 [32768/54000 (61%)] Loss: -503509.062500\n",
      "Train Epoch: 423 [36864/54000 (68%)] Loss: -468606.343750\n",
      "Train Epoch: 423 [40960/54000 (76%)] Loss: -459344.656250\n",
      "Train Epoch: 423 [45056/54000 (83%)] Loss: -465116.562500\n",
      "Train Epoch: 423 [49152/54000 (91%)] Loss: -503044.781250\n",
      "    epoch          : 423\n",
      "    loss           : -463870.00640243903\n",
      "    val_loss       : -462764.68828125\n",
      "Train Epoch: 424 [0/54000 (0%)] Loss: -502013.875000\n",
      "Train Epoch: 424 [4096/54000 (8%)] Loss: -458741.406250\n",
      "Train Epoch: 424 [8192/54000 (15%)] Loss: -465049.906250\n",
      "Train Epoch: 424 [12288/54000 (23%)] Loss: -453028.218750\n",
      "Train Epoch: 424 [16384/54000 (30%)] Loss: -448623.781250\n",
      "Train Epoch: 424 [20480/54000 (38%)] Loss: -466695.750000\n",
      "Train Epoch: 424 [24576/54000 (46%)] Loss: -452712.750000\n",
      "Train Epoch: 424 [28672/54000 (53%)] Loss: -456655.531250\n",
      "Train Epoch: 424 [32768/54000 (61%)] Loss: -470115.875000\n",
      "Train Epoch: 424 [36864/54000 (68%)] Loss: -451647.750000\n",
      "Train Epoch: 424 [40960/54000 (76%)] Loss: -451984.312500\n",
      "Train Epoch: 424 [45056/54000 (83%)] Loss: -449627.750000\n",
      "Train Epoch: 424 [49152/54000 (91%)] Loss: -503379.031250\n",
      "    epoch          : 424\n",
      "    loss           : -463962.14984756097\n",
      "    val_loss       : -462633.91796875\n",
      "Train Epoch: 425 [0/54000 (0%)] Loss: -504220.906250\n",
      "Train Epoch: 425 [4096/54000 (8%)] Loss: -468606.125000\n",
      "Train Epoch: 425 [8192/54000 (15%)] Loss: -456650.250000\n",
      "Train Epoch: 425 [12288/54000 (23%)] Loss: -464686.937500\n",
      "Train Epoch: 425 [16384/54000 (30%)] Loss: -452693.906250\n",
      "Train Epoch: 425 [20480/54000 (38%)] Loss: -470003.031250\n",
      "Train Epoch: 425 [24576/54000 (46%)] Loss: -467753.312500\n",
      "Train Epoch: 425 [28672/54000 (53%)] Loss: -457185.593750\n",
      "Train Epoch: 425 [32768/54000 (61%)] Loss: -504596.375000\n",
      "Train Epoch: 425 [36864/54000 (68%)] Loss: -504064.468750\n",
      "Train Epoch: 425 [40960/54000 (76%)] Loss: -455392.781250\n",
      "Train Epoch: 425 [45056/54000 (83%)] Loss: -452581.437500\n",
      "Train Epoch: 425 [49152/54000 (91%)] Loss: -503585.875000\n",
      "    epoch          : 425\n",
      "    loss           : -463940.8678353659\n",
      "    val_loss       : -462259.8046875\n",
      "Train Epoch: 426 [0/54000 (0%)] Loss: -504184.750000\n",
      "Train Epoch: 426 [4096/54000 (8%)] Loss: -452352.125000\n",
      "Train Epoch: 426 [8192/54000 (15%)] Loss: -451264.562500\n",
      "Train Epoch: 426 [12288/54000 (23%)] Loss: -504548.500000\n",
      "Train Epoch: 426 [16384/54000 (30%)] Loss: -449673.656250\n",
      "Train Epoch: 426 [20480/54000 (38%)] Loss: -465183.156250\n",
      "Train Epoch: 426 [24576/54000 (46%)] Loss: -459405.000000\n",
      "Train Epoch: 426 [28672/54000 (53%)] Loss: -456676.875000\n",
      "Train Epoch: 426 [32768/54000 (61%)] Loss: -451308.593750\n",
      "Train Epoch: 426 [36864/54000 (68%)] Loss: -453112.468750\n",
      "Train Epoch: 426 [40960/54000 (76%)] Loss: -459482.812500\n",
      "Train Epoch: 426 [45056/54000 (83%)] Loss: -467849.000000\n",
      "Train Epoch: 426 [49152/54000 (91%)] Loss: -502712.656250\n",
      "    epoch          : 426\n",
      "    loss           : -463879.96646341466\n",
      "    val_loss       : -462623.08515625\n",
      "Train Epoch: 427 [0/54000 (0%)] Loss: -501571.906250\n",
      "Train Epoch: 427 [4096/54000 (8%)] Loss: -468680.843750\n",
      "Train Epoch: 427 [8192/54000 (15%)] Loss: -456777.656250\n",
      "Train Epoch: 427 [12288/54000 (23%)] Loss: -449130.250000\n",
      "Train Epoch: 427 [16384/54000 (30%)] Loss: -450305.593750\n",
      "Train Epoch: 427 [20480/54000 (38%)] Loss: -469934.250000\n",
      "Train Epoch: 427 [24576/54000 (46%)] Loss: -467599.718750\n",
      "Train Epoch: 427 [28672/54000 (53%)] Loss: -456252.125000\n",
      "Train Epoch: 427 [32768/54000 (61%)] Loss: -458488.062500\n",
      "Train Epoch: 427 [36864/54000 (68%)] Loss: -449246.718750\n",
      "Train Epoch: 427 [40960/54000 (76%)] Loss: -470235.437500\n",
      "Train Epoch: 427 [45056/54000 (83%)] Loss: -449150.250000\n",
      "Train Epoch: 427 [49152/54000 (91%)] Loss: -503585.375000\n",
      "    epoch          : 427\n",
      "    loss           : -463847.4442073171\n",
      "    val_loss       : -462757.6\n",
      "Train Epoch: 428 [0/54000 (0%)] Loss: -503020.125000\n",
      "Train Epoch: 428 [4096/54000 (8%)] Loss: -458414.750000\n",
      "Train Epoch: 428 [8192/54000 (15%)] Loss: -456123.906250\n",
      "Train Epoch: 428 [12288/54000 (23%)] Loss: -449786.812500\n",
      "Train Epoch: 428 [16384/54000 (30%)] Loss: -459489.468750\n",
      "Train Epoch: 428 [20480/54000 (38%)] Loss: -470010.156250\n",
      "Train Epoch: 428 [24576/54000 (46%)] Loss: -464228.875000\n",
      "Train Epoch: 428 [28672/54000 (53%)] Loss: -457435.250000\n",
      "Train Epoch: 428 [32768/54000 (61%)] Loss: -503481.437500\n",
      "Train Epoch: 428 [36864/54000 (68%)] Loss: -504725.531250\n",
      "Train Epoch: 428 [40960/54000 (76%)] Loss: -459059.218750\n",
      "Train Epoch: 428 [45056/54000 (83%)] Loss: -451710.406250\n",
      "Train Epoch: 428 [49152/54000 (91%)] Loss: -503468.468750\n",
      "    epoch          : 428\n",
      "    loss           : -464037.12469512195\n",
      "    val_loss       : -462784.89375\n",
      "Train Epoch: 429 [0/54000 (0%)] Loss: -504275.250000\n",
      "Train Epoch: 429 [4096/54000 (8%)] Loss: -448775.062500\n",
      "Train Epoch: 429 [8192/54000 (15%)] Loss: -469229.593750\n",
      "Train Epoch: 429 [12288/54000 (23%)] Loss: -503272.250000\n",
      "Train Epoch: 429 [16384/54000 (30%)] Loss: -468514.187500\n",
      "Train Epoch: 429 [20480/54000 (38%)] Loss: -503674.593750\n",
      "Train Epoch: 429 [24576/54000 (46%)] Loss: -457953.812500\n",
      "Train Epoch: 429 [28672/54000 (53%)] Loss: -455907.562500\n",
      "Train Epoch: 429 [32768/54000 (61%)] Loss: -448541.250000\n",
      "Train Epoch: 429 [36864/54000 (68%)] Loss: -503922.000000\n",
      "Train Epoch: 429 [40960/54000 (76%)] Loss: -459458.187500\n",
      "Train Epoch: 429 [45056/54000 (83%)] Loss: -468373.375000\n",
      "Train Epoch: 429 [49152/54000 (91%)] Loss: -503313.500000\n",
      "    epoch          : 429\n",
      "    loss           : -463922.40411585366\n",
      "    val_loss       : -462882.3328125\n",
      "Train Epoch: 430 [0/54000 (0%)] Loss: -503848.218750\n",
      "Train Epoch: 430 [4096/54000 (8%)] Loss: -460333.125000\n",
      "Train Epoch: 430 [8192/54000 (15%)] Loss: -452426.437500\n",
      "Train Epoch: 430 [12288/54000 (23%)] Loss: -462653.375000\n",
      "Train Epoch: 430 [16384/54000 (30%)] Loss: -502046.937500\n",
      "Train Epoch: 430 [20480/54000 (38%)] Loss: -458933.125000\n",
      "Train Epoch: 430 [24576/54000 (46%)] Loss: -465493.937500\n",
      "Train Epoch: 430 [28672/54000 (53%)] Loss: -504268.937500\n",
      "Train Epoch: 430 [32768/54000 (61%)] Loss: -465517.812500\n",
      "Train Epoch: 430 [36864/54000 (68%)] Loss: -458503.312500\n",
      "Train Epoch: 430 [40960/54000 (76%)] Loss: -464185.062500\n",
      "Train Epoch: 430 [45056/54000 (83%)] Loss: -458231.718750\n",
      "Train Epoch: 430 [49152/54000 (91%)] Loss: -504608.500000\n",
      "    epoch          : 430\n",
      "    loss           : -463971.62164634146\n",
      "    val_loss       : -462612.96484375\n",
      "Train Epoch: 431 [0/54000 (0%)] Loss: -481102.781250\n",
      "Train Epoch: 431 [4096/54000 (8%)] Loss: -451693.062500\n",
      "Train Epoch: 431 [8192/54000 (15%)] Loss: -468396.375000\n",
      "Train Epoch: 431 [12288/54000 (23%)] Loss: -450258.250000\n",
      "Train Epoch: 431 [16384/54000 (30%)] Loss: -449737.687500\n",
      "Train Epoch: 431 [20480/54000 (38%)] Loss: -454395.281250\n",
      "Train Epoch: 431 [24576/54000 (46%)] Loss: -453096.843750\n",
      "Train Epoch: 431 [28672/54000 (53%)] Loss: -503040.468750\n",
      "Train Epoch: 431 [32768/54000 (61%)] Loss: -466537.468750\n",
      "Train Epoch: 431 [36864/54000 (68%)] Loss: -468143.093750\n",
      "Train Epoch: 431 [40960/54000 (76%)] Loss: -455930.875000\n",
      "Train Epoch: 431 [45056/54000 (83%)] Loss: -470643.687500\n",
      "Train Epoch: 431 [49152/54000 (91%)] Loss: -502901.218750\n",
      "    epoch          : 431\n",
      "    loss           : -463829.77972560975\n",
      "    val_loss       : -462745.10234375\n",
      "Train Epoch: 432 [0/54000 (0%)] Loss: -504070.062500\n",
      "Train Epoch: 432 [4096/54000 (8%)] Loss: -468688.406250\n",
      "Train Epoch: 432 [8192/54000 (15%)] Loss: -470237.000000\n",
      "Train Epoch: 432 [12288/54000 (23%)] Loss: -447993.343750\n",
      "Train Epoch: 432 [16384/54000 (30%)] Loss: -454796.375000\n",
      "Train Epoch: 432 [20480/54000 (38%)] Loss: -470202.562500\n",
      "Train Epoch: 432 [24576/54000 (46%)] Loss: -459319.312500\n",
      "Train Epoch: 432 [28672/54000 (53%)] Loss: -457848.718750\n",
      "Train Epoch: 432 [32768/54000 (61%)] Loss: -449405.437500\n",
      "Train Epoch: 432 [36864/54000 (68%)] Loss: -451847.093750\n",
      "Train Epoch: 432 [40960/54000 (76%)] Loss: -467851.031250\n",
      "Train Epoch: 432 [45056/54000 (83%)] Loss: -460561.062500\n",
      "Train Epoch: 432 [49152/54000 (91%)] Loss: -455537.375000\n",
      "    epoch          : 432\n",
      "    loss           : -463971.3742378049\n",
      "    val_loss       : -462555.159375\n",
      "Train Epoch: 433 [0/54000 (0%)] Loss: -465361.000000\n",
      "Train Epoch: 433 [4096/54000 (8%)] Loss: -450007.562500\n",
      "Train Epoch: 433 [8192/54000 (15%)] Loss: -468675.375000\n",
      "Train Epoch: 433 [12288/54000 (23%)] Loss: -452079.437500\n",
      "Train Epoch: 433 [16384/54000 (30%)] Loss: -452309.250000\n",
      "Train Epoch: 433 [20480/54000 (38%)] Loss: -458067.125000\n",
      "Train Epoch: 433 [24576/54000 (46%)] Loss: -459183.500000\n",
      "Train Epoch: 433 [28672/54000 (53%)] Loss: -456462.312500\n",
      "Train Epoch: 433 [32768/54000 (61%)] Loss: -467915.250000\n",
      "Train Epoch: 433 [36864/54000 (68%)] Loss: -503548.156250\n",
      "Train Epoch: 433 [40960/54000 (76%)] Loss: -468090.125000\n",
      "Train Epoch: 433 [45056/54000 (83%)] Loss: -451615.625000\n",
      "Train Epoch: 433 [49152/54000 (91%)] Loss: -504283.687500\n",
      "    epoch          : 433\n",
      "    loss           : -463907.9608231707\n",
      "    val_loss       : -462857.959375\n",
      "Train Epoch: 434 [0/54000 (0%)] Loss: -450437.968750\n",
      "Train Epoch: 434 [4096/54000 (8%)] Loss: -449827.406250\n",
      "Train Epoch: 434 [8192/54000 (15%)] Loss: -456176.000000\n",
      "Train Epoch: 434 [12288/54000 (23%)] Loss: -466701.687500\n",
      "Train Epoch: 434 [16384/54000 (30%)] Loss: -451850.218750\n",
      "Train Epoch: 434 [20480/54000 (38%)] Loss: -469899.062500\n",
      "Train Epoch: 434 [24576/54000 (46%)] Loss: -450620.343750\n",
      "Train Epoch: 434 [28672/54000 (53%)] Loss: -456822.437500\n",
      "Train Epoch: 434 [32768/54000 (61%)] Loss: -448501.562500\n",
      "Train Epoch: 434 [36864/54000 (68%)] Loss: -450432.781250\n",
      "Train Epoch: 434 [40960/54000 (76%)] Loss: -453629.937500\n",
      "Train Epoch: 434 [45056/54000 (83%)] Loss: -469992.562500\n",
      "Train Epoch: 434 [49152/54000 (91%)] Loss: -502679.750000\n",
      "    epoch          : 434\n",
      "    loss           : -463939.40213414637\n",
      "    val_loss       : -462563.509375\n",
      "Train Epoch: 435 [0/54000 (0%)] Loss: -503120.250000\n",
      "Train Epoch: 435 [4096/54000 (8%)] Loss: -454193.750000\n",
      "Train Epoch: 435 [8192/54000 (15%)] Loss: -456871.531250\n",
      "Train Epoch: 435 [12288/54000 (23%)] Loss: -453157.125000\n",
      "Train Epoch: 435 [16384/54000 (30%)] Loss: -467820.093750\n",
      "Train Epoch: 435 [20480/54000 (38%)] Loss: -469110.312500\n",
      "Train Epoch: 435 [24576/54000 (46%)] Loss: -463179.750000\n",
      "Train Epoch: 435 [28672/54000 (53%)] Loss: -458621.875000\n",
      "Train Epoch: 435 [32768/54000 (61%)] Loss: -504291.750000\n",
      "Train Epoch: 435 [36864/54000 (68%)] Loss: -503070.125000\n",
      "Train Epoch: 435 [40960/54000 (76%)] Loss: -445603.968750\n",
      "Train Epoch: 435 [45056/54000 (83%)] Loss: -465786.093750\n",
      "Train Epoch: 435 [49152/54000 (91%)] Loss: -503176.343750\n",
      "    epoch          : 435\n",
      "    loss           : -463976.193597561\n",
      "    val_loss       : -462364.659375\n",
      "Train Epoch: 436 [0/54000 (0%)] Loss: -504199.187500\n",
      "Train Epoch: 436 [4096/54000 (8%)] Loss: -452543.062500\n",
      "Train Epoch: 436 [8192/54000 (15%)] Loss: -451215.187500\n",
      "Train Epoch: 436 [12288/54000 (23%)] Loss: -503797.281250\n",
      "Train Epoch: 436 [16384/54000 (30%)] Loss: -456619.875000\n",
      "Train Epoch: 436 [20480/54000 (38%)] Loss: -469528.031250\n",
      "Train Epoch: 436 [24576/54000 (46%)] Loss: -454459.031250\n",
      "Train Epoch: 436 [28672/54000 (53%)] Loss: -456124.781250\n",
      "Train Epoch: 436 [32768/54000 (61%)] Loss: -468414.937500\n",
      "Train Epoch: 436 [36864/54000 (68%)] Loss: -505047.375000\n",
      "Train Epoch: 436 [40960/54000 (76%)] Loss: -468873.187500\n",
      "Train Epoch: 436 [45056/54000 (83%)] Loss: -469142.312500\n",
      "Train Epoch: 436 [49152/54000 (91%)] Loss: -503773.593750\n",
      "    epoch          : 436\n",
      "    loss           : -463999.29893292685\n",
      "    val_loss       : -462437.14609375\n",
      "Train Epoch: 437 [0/54000 (0%)] Loss: -503194.843750\n",
      "Train Epoch: 437 [4096/54000 (8%)] Loss: -452867.375000\n",
      "Train Epoch: 437 [8192/54000 (15%)] Loss: -456789.562500\n",
      "Train Epoch: 437 [12288/54000 (23%)] Loss: -456124.343750\n",
      "Train Epoch: 437 [16384/54000 (30%)] Loss: -451043.375000\n",
      "Train Epoch: 437 [20480/54000 (38%)] Loss: -469200.125000\n",
      "Train Epoch: 437 [24576/54000 (46%)] Loss: -451158.843750\n",
      "Train Epoch: 437 [28672/54000 (53%)] Loss: -456375.406250\n",
      "Train Epoch: 437 [32768/54000 (61%)] Loss: -469869.625000\n",
      "Train Epoch: 437 [36864/54000 (68%)] Loss: -449311.750000\n",
      "Train Epoch: 437 [40960/54000 (76%)] Loss: -455968.718750\n",
      "Train Epoch: 437 [45056/54000 (83%)] Loss: -468974.593750\n",
      "Train Epoch: 437 [49152/54000 (91%)] Loss: -503229.062500\n",
      "    epoch          : 437\n",
      "    loss           : -463928.07667682925\n",
      "    val_loss       : -462595.50703125\n",
      "Train Epoch: 438 [0/54000 (0%)] Loss: -503269.343750\n",
      "Train Epoch: 438 [4096/54000 (8%)] Loss: -465336.000000\n",
      "Train Epoch: 438 [8192/54000 (15%)] Loss: -468595.375000\n",
      "Train Epoch: 438 [12288/54000 (23%)] Loss: -451256.187500\n",
      "Train Epoch: 438 [16384/54000 (30%)] Loss: -463424.406250\n",
      "Train Epoch: 438 [20480/54000 (38%)] Loss: -459270.312500\n",
      "Train Epoch: 438 [24576/54000 (46%)] Loss: -450609.906250\n",
      "Train Epoch: 438 [28672/54000 (53%)] Loss: -503884.750000\n",
      "Train Epoch: 438 [32768/54000 (61%)] Loss: -449961.500000\n",
      "Train Epoch: 438 [36864/54000 (68%)] Loss: -452571.093750\n",
      "Train Epoch: 438 [40960/54000 (76%)] Loss: -457587.562500\n",
      "Train Epoch: 438 [45056/54000 (83%)] Loss: -453972.093750\n",
      "Train Epoch: 438 [49152/54000 (91%)] Loss: -452206.687500\n",
      "    epoch          : 438\n",
      "    loss           : -464022.7544207317\n",
      "    val_loss       : -462725.1109375\n",
      "Train Epoch: 439 [0/54000 (0%)] Loss: -472249.281250\n",
      "Train Epoch: 439 [4096/54000 (8%)] Loss: -453104.000000\n",
      "Train Epoch: 439 [8192/54000 (15%)] Loss: -450676.562500\n",
      "Train Epoch: 439 [12288/54000 (23%)] Loss: -450067.625000\n",
      "Train Epoch: 439 [16384/54000 (30%)] Loss: -467651.093750\n",
      "Train Epoch: 439 [20480/54000 (38%)] Loss: -467797.750000\n",
      "Train Epoch: 439 [24576/54000 (46%)] Loss: -466790.906250\n",
      "Train Epoch: 439 [28672/54000 (53%)] Loss: -504444.000000\n",
      "Train Epoch: 439 [32768/54000 (61%)] Loss: -456194.125000\n",
      "Train Epoch: 439 [36864/54000 (68%)] Loss: -468795.750000\n",
      "Train Epoch: 439 [40960/54000 (76%)] Loss: -464667.593750\n",
      "Train Epoch: 439 [45056/54000 (83%)] Loss: -467602.687500\n",
      "Train Epoch: 439 [49152/54000 (91%)] Loss: -504079.375000\n",
      "    epoch          : 439\n",
      "    loss           : -463948.3522865854\n",
      "    val_loss       : -462382.75546875\n",
      "Train Epoch: 440 [0/54000 (0%)] Loss: -503546.250000\n",
      "Train Epoch: 440 [4096/54000 (8%)] Loss: -466578.562500\n",
      "Train Epoch: 440 [8192/54000 (15%)] Loss: -469443.812500\n",
      "Train Epoch: 440 [12288/54000 (23%)] Loss: -456283.812500\n",
      "Train Epoch: 440 [16384/54000 (30%)] Loss: -503741.562500\n",
      "Train Epoch: 440 [20480/54000 (38%)] Loss: -457063.156250\n",
      "Train Epoch: 440 [24576/54000 (46%)] Loss: -467808.625000\n",
      "Train Epoch: 440 [28672/54000 (53%)] Loss: -503523.937500\n",
      "Train Epoch: 440 [32768/54000 (61%)] Loss: -449694.687500\n",
      "Train Epoch: 440 [36864/54000 (68%)] Loss: -451996.687500\n",
      "Train Epoch: 440 [40960/54000 (76%)] Loss: -457148.312500\n",
      "Train Epoch: 440 [45056/54000 (83%)] Loss: -467859.843750\n",
      "Train Epoch: 440 [49152/54000 (91%)] Loss: -504359.906250\n",
      "    epoch          : 440\n",
      "    loss           : -464023.9315548781\n",
      "    val_loss       : -462541.94296875\n",
      "Train Epoch: 441 [0/54000 (0%)] Loss: -502852.062500\n",
      "Train Epoch: 441 [4096/54000 (8%)] Loss: -467009.500000\n",
      "Train Epoch: 441 [8192/54000 (15%)] Loss: -451897.906250\n",
      "Train Epoch: 441 [12288/54000 (23%)] Loss: -449124.437500\n",
      "Train Epoch: 441 [16384/54000 (30%)] Loss: -467637.593750\n",
      "Train Epoch: 441 [20480/54000 (38%)] Loss: -452186.625000\n",
      "Train Epoch: 441 [24576/54000 (46%)] Loss: -467748.250000\n",
      "Train Epoch: 441 [28672/54000 (53%)] Loss: -455784.468750\n",
      "Train Epoch: 441 [32768/54000 (61%)] Loss: -458196.375000\n",
      "Train Epoch: 441 [36864/54000 (68%)] Loss: -469364.531250\n",
      "Train Epoch: 441 [40960/54000 (76%)] Loss: -459737.687500\n",
      "Train Epoch: 441 [45056/54000 (83%)] Loss: -467994.687500\n",
      "Train Epoch: 441 [49152/54000 (91%)] Loss: -459047.500000\n",
      "    epoch          : 441\n",
      "    loss           : -463928.4140243902\n",
      "    val_loss       : -462667.08828125\n",
      "Train Epoch: 442 [0/54000 (0%)] Loss: -504064.562500\n",
      "Train Epoch: 442 [4096/54000 (8%)] Loss: -451763.250000\n",
      "Train Epoch: 442 [8192/54000 (15%)] Loss: -458239.062500\n",
      "Train Epoch: 442 [12288/54000 (23%)] Loss: -503037.906250\n",
      "Train Epoch: 442 [16384/54000 (30%)] Loss: -454514.375000\n",
      "Train Epoch: 442 [20480/54000 (38%)] Loss: -457671.687500\n",
      "Train Epoch: 442 [24576/54000 (46%)] Loss: -468903.218750\n",
      "Train Epoch: 442 [28672/54000 (53%)] Loss: -504289.625000\n",
      "Train Epoch: 442 [32768/54000 (61%)] Loss: -458301.156250\n",
      "Train Epoch: 442 [36864/54000 (68%)] Loss: -469233.875000\n",
      "Train Epoch: 442 [40960/54000 (76%)] Loss: -456287.468750\n",
      "Train Epoch: 442 [45056/54000 (83%)] Loss: -469710.468750\n",
      "Train Epoch: 442 [49152/54000 (91%)] Loss: -503034.687500\n",
      "    epoch          : 442\n",
      "    loss           : -464013.22164634144\n",
      "    val_loss       : -462697.69921875\n",
      "Train Epoch: 443 [0/54000 (0%)] Loss: -467877.343750\n",
      "Train Epoch: 443 [4096/54000 (8%)] Loss: -464485.562500\n",
      "Train Epoch: 443 [8192/54000 (15%)] Loss: -456860.218750\n",
      "Train Epoch: 443 [12288/54000 (23%)] Loss: -468267.843750\n",
      "Train Epoch: 443 [16384/54000 (30%)] Loss: -450877.562500\n",
      "Train Epoch: 443 [20480/54000 (38%)] Loss: -457496.437500\n",
      "Train Epoch: 443 [24576/54000 (46%)] Loss: -450414.562500\n",
      "Train Epoch: 443 [28672/54000 (53%)] Loss: -457471.906250\n",
      "Train Epoch: 443 [32768/54000 (61%)] Loss: -450013.375000\n",
      "Train Epoch: 443 [36864/54000 (68%)] Loss: -450469.875000\n",
      "Train Epoch: 443 [40960/54000 (76%)] Loss: -465843.875000\n",
      "Train Epoch: 443 [45056/54000 (83%)] Loss: -466018.343750\n",
      "Train Epoch: 443 [49152/54000 (91%)] Loss: -504117.093750\n",
      "    epoch          : 443\n",
      "    loss           : -464039.08399390243\n",
      "    val_loss       : -462902.475\n",
      "Train Epoch: 444 [0/54000 (0%)] Loss: -505004.812500\n",
      "Train Epoch: 444 [4096/54000 (8%)] Loss: -466218.562500\n",
      "Train Epoch: 444 [8192/54000 (15%)] Loss: -455970.656250\n",
      "Train Epoch: 444 [12288/54000 (23%)] Loss: -460085.687500\n",
      "Train Epoch: 444 [16384/54000 (30%)] Loss: -448502.781250\n",
      "Train Epoch: 444 [20480/54000 (38%)] Loss: -457097.812500\n",
      "Train Epoch: 444 [24576/54000 (46%)] Loss: -466195.312500\n",
      "Train Epoch: 444 [28672/54000 (53%)] Loss: -504400.531250\n",
      "Train Epoch: 444 [32768/54000 (61%)] Loss: -468758.000000\n",
      "Train Epoch: 444 [36864/54000 (68%)] Loss: -465179.375000\n",
      "Train Epoch: 444 [40960/54000 (76%)] Loss: -458303.281250\n",
      "Train Epoch: 444 [45056/54000 (83%)] Loss: -469242.812500\n",
      "Train Epoch: 444 [49152/54000 (91%)] Loss: -502691.062500\n",
      "    epoch          : 444\n",
      "    loss           : -464040.656097561\n",
      "    val_loss       : -462567.09765625\n",
      "Train Epoch: 445 [0/54000 (0%)] Loss: -504122.875000\n",
      "Train Epoch: 445 [4096/54000 (8%)] Loss: -461749.625000\n",
      "Train Epoch: 445 [8192/54000 (15%)] Loss: -470049.562500\n",
      "Train Epoch: 445 [12288/54000 (23%)] Loss: -467919.937500\n",
      "Train Epoch: 445 [16384/54000 (30%)] Loss: -449050.843750\n",
      "Train Epoch: 445 [20480/54000 (38%)] Loss: -452278.875000\n",
      "Train Epoch: 445 [24576/54000 (46%)] Loss: -464506.781250\n",
      "Train Epoch: 445 [28672/54000 (53%)] Loss: -503888.500000\n",
      "Train Epoch: 445 [32768/54000 (61%)] Loss: -456414.625000\n",
      "Train Epoch: 445 [36864/54000 (68%)] Loss: -469282.000000\n",
      "Train Epoch: 445 [40960/54000 (76%)] Loss: -457219.468750\n",
      "Train Epoch: 445 [45056/54000 (83%)] Loss: -457843.562500\n",
      "Train Epoch: 445 [49152/54000 (91%)] Loss: -465967.750000\n",
      "    epoch          : 445\n",
      "    loss           : -464039.8539634146\n",
      "    val_loss       : -462440.14921875\n",
      "Train Epoch: 446 [0/54000 (0%)] Loss: -465802.812500\n",
      "Train Epoch: 446 [4096/54000 (8%)] Loss: -468084.875000\n",
      "Train Epoch: 446 [8192/54000 (15%)] Loss: -466467.125000\n",
      "Train Epoch: 446 [12288/54000 (23%)] Loss: -457713.625000\n",
      "Train Epoch: 446 [16384/54000 (30%)] Loss: -467378.375000\n",
      "Train Epoch: 446 [20480/54000 (38%)] Loss: -471015.562500\n",
      "Train Epoch: 446 [24576/54000 (46%)] Loss: -459376.187500\n",
      "Train Epoch: 446 [28672/54000 (53%)] Loss: -504780.375000\n",
      "Train Epoch: 446 [32768/54000 (61%)] Loss: -460716.343750\n",
      "Train Epoch: 446 [36864/54000 (68%)] Loss: -467887.437500\n",
      "Train Epoch: 446 [40960/54000 (76%)] Loss: -450750.687500\n",
      "Train Epoch: 446 [45056/54000 (83%)] Loss: -467957.718750\n",
      "Train Epoch: 446 [49152/54000 (91%)] Loss: -503765.656250\n",
      "    epoch          : 446\n",
      "    loss           : -463988.70167682925\n",
      "    val_loss       : -462418.7609375\n",
      "Train Epoch: 447 [0/54000 (0%)] Loss: -503527.062500\n",
      "Train Epoch: 447 [4096/54000 (8%)] Loss: -460025.000000\n",
      "Train Epoch: 447 [8192/54000 (15%)] Loss: -452402.812500\n",
      "Train Epoch: 447 [12288/54000 (23%)] Loss: -453338.843750\n",
      "Train Epoch: 447 [16384/54000 (30%)] Loss: -462781.343750\n",
      "Train Epoch: 447 [20480/54000 (38%)] Loss: -468407.937500\n",
      "Train Epoch: 447 [24576/54000 (46%)] Loss: -468339.812500\n",
      "Train Epoch: 447 [28672/54000 (53%)] Loss: -456693.968750\n",
      "Train Epoch: 447 [32768/54000 (61%)] Loss: -459020.187500\n",
      "Train Epoch: 447 [36864/54000 (68%)] Loss: -448654.406250\n",
      "Train Epoch: 447 [40960/54000 (76%)] Loss: -453955.625000\n",
      "Train Epoch: 447 [45056/54000 (83%)] Loss: -467940.875000\n",
      "Train Epoch: 447 [49152/54000 (91%)] Loss: -457117.937500\n",
      "    epoch          : 447\n",
      "    loss           : -463957.11859756097\n",
      "    val_loss       : -462399.08359375\n",
      "Train Epoch: 448 [0/54000 (0%)] Loss: -503948.593750\n",
      "Train Epoch: 448 [4096/54000 (8%)] Loss: -458222.593750\n",
      "Train Epoch: 448 [8192/54000 (15%)] Loss: -458803.156250\n",
      "Train Epoch: 448 [12288/54000 (23%)] Loss: -451526.375000\n",
      "Train Epoch: 448 [16384/54000 (30%)] Loss: -469249.437500\n",
      "Train Epoch: 448 [20480/54000 (38%)] Loss: -468721.125000\n",
      "Train Epoch: 448 [24576/54000 (46%)] Loss: -468606.906250\n",
      "Train Epoch: 448 [28672/54000 (53%)] Loss: -457257.718750\n",
      "Train Epoch: 448 [32768/54000 (61%)] Loss: -449674.437500\n",
      "Train Epoch: 448 [36864/54000 (68%)] Loss: -504442.125000\n",
      "Train Epoch: 448 [40960/54000 (76%)] Loss: -454402.187500\n",
      "Train Epoch: 448 [45056/54000 (83%)] Loss: -469564.406250\n",
      "Train Epoch: 448 [49152/54000 (91%)] Loss: -463887.718750\n",
      "    epoch          : 448\n",
      "    loss           : -464043.2399390244\n",
      "    val_loss       : -462695.97265625\n",
      "Train Epoch: 449 [0/54000 (0%)] Loss: -503959.125000\n",
      "Train Epoch: 449 [4096/54000 (8%)] Loss: -468892.000000\n",
      "Train Epoch: 449 [8192/54000 (15%)] Loss: -457753.656250\n",
      "Train Epoch: 449 [12288/54000 (23%)] Loss: -449117.031250\n",
      "Train Epoch: 449 [16384/54000 (30%)] Loss: -468523.062500\n",
      "Train Epoch: 449 [20480/54000 (38%)] Loss: -468924.437500\n",
      "Train Epoch: 449 [24576/54000 (46%)] Loss: -451037.812500\n",
      "Train Epoch: 449 [28672/54000 (53%)] Loss: -457629.750000\n",
      "Train Epoch: 449 [32768/54000 (61%)] Loss: -459680.625000\n",
      "Train Epoch: 449 [36864/54000 (68%)] Loss: -502537.843750\n",
      "Train Epoch: 449 [40960/54000 (76%)] Loss: -456712.500000\n",
      "Train Epoch: 449 [45056/54000 (83%)] Loss: -452283.000000\n",
      "Train Epoch: 449 [49152/54000 (91%)] Loss: -502741.343750\n",
      "    epoch          : 449\n",
      "    loss           : -463915.42286585364\n",
      "    val_loss       : -462825.19921875\n",
      "Train Epoch: 450 [0/54000 (0%)] Loss: -503521.406250\n",
      "Train Epoch: 450 [4096/54000 (8%)] Loss: -449754.750000\n",
      "Train Epoch: 450 [8192/54000 (15%)] Loss: -457516.625000\n",
      "Train Epoch: 450 [12288/54000 (23%)] Loss: -466035.468750\n",
      "Train Epoch: 450 [16384/54000 (30%)] Loss: -458619.000000\n",
      "Train Epoch: 450 [20480/54000 (38%)] Loss: -469208.750000\n",
      "Train Epoch: 450 [24576/54000 (46%)] Loss: -459306.312500\n",
      "Train Epoch: 450 [28672/54000 (53%)] Loss: -456228.375000\n",
      "Train Epoch: 450 [32768/54000 (61%)] Loss: -468666.375000\n",
      "Train Epoch: 450 [36864/54000 (68%)] Loss: -458606.625000\n",
      "Train Epoch: 450 [40960/54000 (76%)] Loss: -457858.812500\n",
      "Train Epoch: 450 [45056/54000 (83%)] Loss: -460367.375000\n",
      "Train Epoch: 450 [49152/54000 (91%)] Loss: -503532.468750\n",
      "    epoch          : 450\n",
      "    loss           : -464119.82957317075\n",
      "    val_loss       : -462814.04765625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0510_004522/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [0/54000 (0%)] Loss: -504649.031250\n",
      "Train Epoch: 451 [4096/54000 (8%)] Loss: -460411.812500\n",
      "Train Epoch: 451 [8192/54000 (15%)] Loss: -453651.812500\n",
      "Train Epoch: 451 [12288/54000 (23%)] Loss: -460733.687500\n",
      "Train Epoch: 451 [16384/54000 (30%)] Loss: -453100.843750\n",
      "Train Epoch: 451 [20480/54000 (38%)] Loss: -470230.781250\n",
      "Train Epoch: 451 [24576/54000 (46%)] Loss: -442806.687500\n",
      "Train Epoch: 451 [28672/54000 (53%)] Loss: -457527.312500\n",
      "Train Epoch: 451 [32768/54000 (61%)] Loss: -459006.562500\n",
      "Train Epoch: 451 [36864/54000 (68%)] Loss: -502866.250000\n",
      "Train Epoch: 451 [40960/54000 (76%)] Loss: -449131.937500\n",
      "Train Epoch: 451 [45056/54000 (83%)] Loss: -468866.187500\n",
      "Train Epoch: 451 [49152/54000 (91%)] Loss: -456748.437500\n",
      "    epoch          : 451\n",
      "    loss           : -463941.43963414634\n",
      "    val_loss       : -462701.9234375\n",
      "Train Epoch: 452 [0/54000 (0%)] Loss: -503738.937500\n",
      "Train Epoch: 452 [4096/54000 (8%)] Loss: -450602.031250\n",
      "Train Epoch: 452 [8192/54000 (15%)] Loss: -456459.562500\n",
      "Train Epoch: 452 [12288/54000 (23%)] Loss: -449664.000000\n",
      "Train Epoch: 452 [16384/54000 (30%)] Loss: -456883.531250\n",
      "Train Epoch: 452 [20480/54000 (38%)] Loss: -469042.406250\n",
      "Train Epoch: 452 [24576/54000 (46%)] Loss: -450287.000000\n",
      "Train Epoch: 452 [28672/54000 (53%)] Loss: -455427.781250\n",
      "Train Epoch: 452 [32768/54000 (61%)] Loss: -469855.750000\n",
      "Train Epoch: 452 [36864/54000 (68%)] Loss: -459150.531250\n",
      "Train Epoch: 452 [40960/54000 (76%)] Loss: -468731.812500\n",
      "Train Epoch: 452 [45056/54000 (83%)] Loss: -449977.375000\n",
      "Train Epoch: 452 [49152/54000 (91%)] Loss: -504417.625000\n",
      "    epoch          : 452\n",
      "    loss           : -464017.35960365855\n",
      "    val_loss       : -462492.5453125\n",
      "Train Epoch: 453 [0/54000 (0%)] Loss: -504263.531250\n",
      "Train Epoch: 453 [4096/54000 (8%)] Loss: -457061.218750\n",
      "Train Epoch: 453 [8192/54000 (15%)] Loss: -453803.468750\n",
      "Train Epoch: 453 [12288/54000 (23%)] Loss: -450446.156250\n",
      "Train Epoch: 453 [16384/54000 (30%)] Loss: -468185.937500\n",
      "Train Epoch: 453 [20480/54000 (38%)] Loss: -469523.125000\n",
      "Train Epoch: 453 [24576/54000 (46%)] Loss: -459549.875000\n",
      "Train Epoch: 453 [28672/54000 (53%)] Loss: -458813.937500\n",
      "Train Epoch: 453 [32768/54000 (61%)] Loss: -459123.406250\n",
      "Train Epoch: 453 [36864/54000 (68%)] Loss: -468564.500000\n",
      "Train Epoch: 453 [40960/54000 (76%)] Loss: -449558.281250\n",
      "Train Epoch: 453 [45056/54000 (83%)] Loss: -452614.781250\n",
      "Train Epoch: 453 [49152/54000 (91%)] Loss: -503063.218750\n",
      "    epoch          : 453\n",
      "    loss           : -464001.72515243903\n",
      "    val_loss       : -462834.828125\n",
      "Train Epoch: 454 [0/54000 (0%)] Loss: -504402.875000\n",
      "Train Epoch: 454 [4096/54000 (8%)] Loss: -468764.187500\n",
      "Train Epoch: 454 [8192/54000 (15%)] Loss: -457379.375000\n",
      "Train Epoch: 454 [12288/54000 (23%)] Loss: -503234.250000\n",
      "Train Epoch: 454 [16384/54000 (30%)] Loss: -452363.656250\n",
      "Train Epoch: 454 [20480/54000 (38%)] Loss: -468397.656250\n",
      "Train Epoch: 454 [24576/54000 (46%)] Loss: -452623.468750\n",
      "Train Epoch: 454 [28672/54000 (53%)] Loss: -453783.468750\n",
      "Train Epoch: 454 [32768/54000 (61%)] Loss: -469086.812500\n",
      "Train Epoch: 454 [36864/54000 (68%)] Loss: -467280.000000\n",
      "Train Epoch: 454 [40960/54000 (76%)] Loss: -459242.156250\n",
      "Train Epoch: 454 [45056/54000 (83%)] Loss: -469688.125000\n",
      "Train Epoch: 454 [49152/54000 (91%)] Loss: -459068.843750\n",
      "    epoch          : 454\n",
      "    loss           : -463840.89222560974\n",
      "    val_loss       : -462479.721875\n",
      "Train Epoch: 455 [0/54000 (0%)] Loss: -454454.125000\n",
      "Train Epoch: 455 [4096/54000 (8%)] Loss: -459600.281250\n",
      "Train Epoch: 455 [8192/54000 (15%)] Loss: -469261.937500\n",
      "Train Epoch: 455 [12288/54000 (23%)] Loss: -458215.250000\n",
      "Train Epoch: 455 [16384/54000 (30%)] Loss: -503678.062500\n",
      "Train Epoch: 455 [20480/54000 (38%)] Loss: -469973.187500\n",
      "Train Epoch: 455 [24576/54000 (46%)] Loss: -451301.000000\n",
      "Train Epoch: 455 [28672/54000 (53%)] Loss: -460272.718750\n",
      "Train Epoch: 455 [32768/54000 (61%)] Loss: -503178.187500\n",
      "Train Epoch: 455 [36864/54000 (68%)] Loss: -451081.437500\n",
      "Train Epoch: 455 [40960/54000 (76%)] Loss: -465208.562500\n",
      "Train Epoch: 455 [45056/54000 (83%)] Loss: -450931.625000\n",
      "Train Epoch: 455 [49152/54000 (91%)] Loss: -503424.625000\n",
      "    epoch          : 455\n",
      "    loss           : -464108.68780487805\n",
      "    val_loss       : -462739.30625\n",
      "Train Epoch: 456 [0/54000 (0%)] Loss: -503016.093750\n",
      "Train Epoch: 456 [4096/54000 (8%)] Loss: -464982.281250\n",
      "Train Epoch: 456 [8192/54000 (15%)] Loss: -456599.031250\n",
      "Train Epoch: 456 [12288/54000 (23%)] Loss: -503788.000000\n",
      "Train Epoch: 456 [16384/54000 (30%)] Loss: -504346.718750\n",
      "Train Epoch: 456 [20480/54000 (38%)] Loss: -471553.562500\n",
      "Train Epoch: 456 [24576/54000 (46%)] Loss: -451487.312500\n",
      "Train Epoch: 456 [28672/54000 (53%)] Loss: -457417.312500\n",
      "Train Epoch: 456 [32768/54000 (61%)] Loss: -465712.875000\n",
      "Train Epoch: 456 [36864/54000 (68%)] Loss: -451208.843750\n",
      "Train Epoch: 456 [40960/54000 (76%)] Loss: -458633.312500\n",
      "Train Epoch: 456 [45056/54000 (83%)] Loss: -468224.062500\n",
      "Train Epoch: 456 [49152/54000 (91%)] Loss: -503158.031250\n",
      "    epoch          : 456\n",
      "    loss           : -464090.54649390245\n",
      "    val_loss       : -462778.56015625\n",
      "Train Epoch: 457 [0/54000 (0%)] Loss: -469152.656250\n",
      "Train Epoch: 457 [4096/54000 (8%)] Loss: -447868.750000\n",
      "Train Epoch: 457 [8192/54000 (15%)] Loss: -458712.906250\n",
      "Train Epoch: 457 [12288/54000 (23%)] Loss: -468835.968750\n",
      "Train Epoch: 457 [16384/54000 (30%)] Loss: -459548.750000\n",
      "Train Epoch: 457 [20480/54000 (38%)] Loss: -467703.656250\n",
      "Train Epoch: 457 [24576/54000 (46%)] Loss: -459683.531250\n",
      "Train Epoch: 457 [28672/54000 (53%)] Loss: -458064.625000\n",
      "Train Epoch: 457 [32768/54000 (61%)] Loss: -469503.562500\n",
      "Train Epoch: 457 [36864/54000 (68%)] Loss: -460524.968750\n",
      "Train Epoch: 457 [40960/54000 (76%)] Loss: -455954.125000\n",
      "Train Epoch: 457 [45056/54000 (83%)] Loss: -470533.312500\n",
      "Train Epoch: 457 [49152/54000 (91%)] Loss: -465590.062500\n",
      "    epoch          : 457\n",
      "    loss           : -464095.47225609754\n",
      "    val_loss       : -462714.91171875\n",
      "Train Epoch: 458 [0/54000 (0%)] Loss: -501225.250000\n",
      "Train Epoch: 458 [4096/54000 (8%)] Loss: -454106.656250\n",
      "Train Epoch: 458 [8192/54000 (15%)] Loss: -450217.625000\n",
      "Train Epoch: 458 [12288/54000 (23%)] Loss: -458952.437500\n",
      "Train Epoch: 458 [16384/54000 (30%)] Loss: -452012.406250\n",
      "Train Epoch: 458 [20480/54000 (38%)] Loss: -468047.031250\n",
      "Train Epoch: 458 [24576/54000 (46%)] Loss: -469689.656250\n",
      "Train Epoch: 458 [28672/54000 (53%)] Loss: -457598.531250\n",
      "Train Epoch: 458 [32768/54000 (61%)] Loss: -468188.187500\n",
      "Train Epoch: 458 [36864/54000 (68%)] Loss: -452265.937500\n",
      "Train Epoch: 458 [40960/54000 (76%)] Loss: -468482.125000\n",
      "Train Epoch: 458 [45056/54000 (83%)] Loss: -467477.937500\n",
      "Train Epoch: 458 [49152/54000 (91%)] Loss: -502934.312500\n",
      "    epoch          : 458\n",
      "    loss           : -464135.06341463415\n",
      "    val_loss       : -462775.0015625\n",
      "Train Epoch: 459 [0/54000 (0%)] Loss: -503897.968750\n",
      "Train Epoch: 459 [4096/54000 (8%)] Loss: -468417.000000\n",
      "Train Epoch: 459 [8192/54000 (15%)] Loss: -457786.750000\n",
      "Train Epoch: 459 [12288/54000 (23%)] Loss: -453503.375000\n",
      "Train Epoch: 459 [16384/54000 (30%)] Loss: -503714.562500\n",
      "Train Epoch: 459 [20480/54000 (38%)] Loss: -469672.937500\n",
      "Train Epoch: 459 [24576/54000 (46%)] Loss: -451823.906250\n",
      "Train Epoch: 459 [28672/54000 (53%)] Loss: -469510.750000\n",
      "Train Epoch: 459 [32768/54000 (61%)] Loss: -503206.843750\n",
      "Train Epoch: 459 [36864/54000 (68%)] Loss: -452593.062500\n",
      "Train Epoch: 459 [40960/54000 (76%)] Loss: -469787.656250\n",
      "Train Epoch: 459 [45056/54000 (83%)] Loss: -466428.500000\n",
      "Train Epoch: 459 [49152/54000 (91%)] Loss: -502121.125000\n",
      "    epoch          : 459\n",
      "    loss           : -464106.6338414634\n",
      "    val_loss       : -462635.50859375\n",
      "Train Epoch: 460 [0/54000 (0%)] Loss: -503777.187500\n",
      "Train Epoch: 460 [4096/54000 (8%)] Loss: -469084.937500\n",
      "Train Epoch: 460 [8192/54000 (15%)] Loss: -470029.562500\n",
      "Train Epoch: 460 [12288/54000 (23%)] Loss: -450966.125000\n",
      "Train Epoch: 460 [16384/54000 (30%)] Loss: -450592.000000\n",
      "Train Epoch: 460 [20480/54000 (38%)] Loss: -471504.937500\n",
      "Train Epoch: 460 [24576/54000 (46%)] Loss: -463930.812500\n",
      "Train Epoch: 460 [28672/54000 (53%)] Loss: -503814.500000\n",
      "Train Epoch: 460 [32768/54000 (61%)] Loss: -470041.718750\n",
      "Train Epoch: 460 [36864/54000 (68%)] Loss: -469736.875000\n",
      "Train Epoch: 460 [40960/54000 (76%)] Loss: -456763.187500\n",
      "Train Epoch: 460 [45056/54000 (83%)] Loss: -467370.750000\n",
      "Train Epoch: 460 [49152/54000 (91%)] Loss: -504735.187500\n",
      "    epoch          : 460\n",
      "    loss           : -464079.0582317073\n",
      "    val_loss       : -462657.584375\n",
      "Train Epoch: 461 [0/54000 (0%)] Loss: -448228.187500\n",
      "Train Epoch: 461 [4096/54000 (8%)] Loss: -469027.687500\n",
      "Train Epoch: 461 [8192/54000 (15%)] Loss: -452013.500000\n",
      "Train Epoch: 461 [12288/54000 (23%)] Loss: -448906.125000\n",
      "Train Epoch: 461 [16384/54000 (30%)] Loss: -469930.093750\n",
      "Train Epoch: 461 [20480/54000 (38%)] Loss: -468609.781250\n",
      "Train Epoch: 461 [24576/54000 (46%)] Loss: -451581.312500\n",
      "Train Epoch: 461 [28672/54000 (53%)] Loss: -503956.562500\n",
      "Train Epoch: 461 [32768/54000 (61%)] Loss: -450478.187500\n",
      "Train Epoch: 461 [36864/54000 (68%)] Loss: -458943.343750\n",
      "Train Epoch: 461 [40960/54000 (76%)] Loss: -450850.500000\n",
      "Train Epoch: 461 [45056/54000 (83%)] Loss: -458619.968750\n",
      "Train Epoch: 461 [49152/54000 (91%)] Loss: -457649.375000\n",
      "    epoch          : 461\n",
      "    loss           : -464078.1832317073\n",
      "    val_loss       : -462636.96015625\n",
      "Train Epoch: 462 [0/54000 (0%)] Loss: -504150.250000\n",
      "Train Epoch: 462 [4096/54000 (8%)] Loss: -469439.375000\n",
      "Train Epoch: 462 [8192/54000 (15%)] Loss: -451132.437500\n",
      "Train Epoch: 462 [12288/54000 (23%)] Loss: -463628.593750\n",
      "Train Epoch: 462 [16384/54000 (30%)] Loss: -456643.593750\n",
      "Train Epoch: 462 [20480/54000 (38%)] Loss: -467287.125000\n",
      "Train Epoch: 462 [24576/54000 (46%)] Loss: -449974.500000\n",
      "Train Epoch: 462 [28672/54000 (53%)] Loss: -456516.687500\n",
      "Train Epoch: 462 [32768/54000 (61%)] Loss: -466298.875000\n",
      "Train Epoch: 462 [36864/54000 (68%)] Loss: -464159.906250\n",
      "Train Epoch: 462 [40960/54000 (76%)] Loss: -457561.375000\n",
      "Train Epoch: 462 [45056/54000 (83%)] Loss: -468504.687500\n",
      "Train Epoch: 462 [49152/54000 (91%)] Loss: -503246.031250\n",
      "    epoch          : 462\n",
      "    loss           : -464103.38719512196\n",
      "    val_loss       : -462728.26953125\n",
      "Train Epoch: 463 [0/54000 (0%)] Loss: -458230.312500\n",
      "Train Epoch: 463 [4096/54000 (8%)] Loss: -460333.031250\n",
      "Train Epoch: 463 [8192/54000 (15%)] Loss: -455007.812500\n",
      "Train Epoch: 463 [12288/54000 (23%)] Loss: -450401.156250\n",
      "Train Epoch: 463 [16384/54000 (30%)] Loss: -451524.375000\n",
      "Train Epoch: 463 [20480/54000 (38%)] Loss: -468815.250000\n",
      "Train Epoch: 463 [24576/54000 (46%)] Loss: -452728.187500\n",
      "Train Epoch: 463 [28672/54000 (53%)] Loss: -504616.375000\n",
      "Train Epoch: 463 [32768/54000 (61%)] Loss: -464912.343750\n",
      "Train Epoch: 463 [36864/54000 (68%)] Loss: -468340.562500\n",
      "Train Epoch: 463 [40960/54000 (76%)] Loss: -459154.156250\n",
      "Train Epoch: 463 [45056/54000 (83%)] Loss: -466875.093750\n",
      "Train Epoch: 463 [49152/54000 (91%)] Loss: -503682.843750\n",
      "    epoch          : 463\n",
      "    loss           : -464150.8210365854\n",
      "    val_loss       : -462698.4\n",
      "Train Epoch: 464 [0/54000 (0%)] Loss: -505078.218750\n",
      "Train Epoch: 464 [4096/54000 (8%)] Loss: -468819.812500\n",
      "Train Epoch: 464 [8192/54000 (15%)] Loss: -456845.500000\n",
      "Train Epoch: 464 [12288/54000 (23%)] Loss: -463526.468750\n",
      "Train Epoch: 464 [16384/54000 (30%)] Loss: -468375.875000\n",
      "Train Epoch: 464 [20480/54000 (38%)] Loss: -470524.875000\n",
      "Train Epoch: 464 [24576/54000 (46%)] Loss: -452195.250000\n",
      "Train Epoch: 464 [28672/54000 (53%)] Loss: -503725.718750\n",
      "Train Epoch: 464 [32768/54000 (61%)] Loss: -452718.593750\n",
      "Train Epoch: 464 [36864/54000 (68%)] Loss: -469337.062500\n",
      "Train Epoch: 464 [40960/54000 (76%)] Loss: -457397.687500\n",
      "Train Epoch: 464 [45056/54000 (83%)] Loss: -469579.375000\n",
      "Train Epoch: 464 [49152/54000 (91%)] Loss: -504728.187500\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   464: reducing learning rate of group 0 to 1.2500e-05.\n",
      "    epoch          : 464\n",
      "    loss           : -464178.90152439027\n",
      "    val_loss       : -462632.1734375\n",
      "Train Epoch: 465 [0/54000 (0%)] Loss: -504158.687500\n",
      "Train Epoch: 465 [4096/54000 (8%)] Loss: -466278.812500\n",
      "Train Epoch: 465 [8192/54000 (15%)] Loss: -468174.468750\n",
      "Train Epoch: 465 [12288/54000 (23%)] Loss: -454047.125000\n",
      "Train Epoch: 465 [16384/54000 (30%)] Loss: -451386.187500\n",
      "Train Epoch: 465 [20480/54000 (38%)] Loss: -456523.031250\n",
      "Train Epoch: 465 [24576/54000 (46%)] Loss: -464220.625000\n",
      "Train Epoch: 465 [28672/54000 (53%)] Loss: -469184.031250\n",
      "Train Epoch: 465 [32768/54000 (61%)] Loss: -459776.562500\n",
      "Train Epoch: 465 [36864/54000 (68%)] Loss: -503988.281250\n",
      "Train Epoch: 465 [40960/54000 (76%)] Loss: -452191.062500\n",
      "Train Epoch: 465 [45056/54000 (83%)] Loss: -467353.375000\n",
      "Train Epoch: 465 [49152/54000 (91%)] Loss: -504208.687500\n",
      "    epoch          : 465\n",
      "    loss           : -464094.55213414633\n",
      "    val_loss       : -462124.0671875\n",
      "Train Epoch: 466 [0/54000 (0%)] Loss: -503588.812500\n",
      "Train Epoch: 466 [4096/54000 (8%)] Loss: -457407.031250\n",
      "Train Epoch: 466 [8192/54000 (15%)] Loss: -467684.500000\n",
      "Train Epoch: 466 [12288/54000 (23%)] Loss: -464695.375000\n",
      "Train Epoch: 466 [16384/54000 (30%)] Loss: -467864.562500\n",
      "Train Epoch: 466 [20480/54000 (38%)] Loss: -470237.875000\n",
      "Train Epoch: 466 [24576/54000 (46%)] Loss: -458252.187500\n",
      "Train Epoch: 466 [28672/54000 (53%)] Loss: -457230.875000\n",
      "Train Epoch: 466 [32768/54000 (61%)] Loss: -468973.218750\n",
      "Train Epoch: 466 [36864/54000 (68%)] Loss: -502831.437500\n",
      "Train Epoch: 466 [40960/54000 (76%)] Loss: -448562.812500\n",
      "Train Epoch: 466 [45056/54000 (83%)] Loss: -450949.187500\n",
      "Train Epoch: 466 [49152/54000 (91%)] Loss: -503650.125000\n",
      "    epoch          : 466\n",
      "    loss           : -464153.70640243904\n",
      "    val_loss       : -462788.84609375\n",
      "Train Epoch: 467 [0/54000 (0%)] Loss: -504561.375000\n",
      "Train Epoch: 467 [4096/54000 (8%)] Loss: -459421.968750\n",
      "Train Epoch: 467 [8192/54000 (15%)] Loss: -463798.812500\n",
      "Train Epoch: 467 [12288/54000 (23%)] Loss: -459044.812500\n",
      "Train Epoch: 467 [16384/54000 (30%)] Loss: -465737.343750\n",
      "Train Epoch: 467 [20480/54000 (38%)] Loss: -468272.156250\n",
      "Train Epoch: 467 [24576/54000 (46%)] Loss: -466636.343750\n",
      "Train Epoch: 467 [28672/54000 (53%)] Loss: -457877.250000\n",
      "Train Epoch: 467 [32768/54000 (61%)] Loss: -503633.187500\n",
      "Train Epoch: 467 [36864/54000 (68%)] Loss: -468630.218750\n",
      "Train Epoch: 467 [40960/54000 (76%)] Loss: -458852.406250\n",
      "Train Epoch: 467 [45056/54000 (83%)] Loss: -449927.312500\n",
      "Train Epoch: 467 [49152/54000 (91%)] Loss: -502874.843750\n",
      "    epoch          : 467\n",
      "    loss           : -464180.77957317076\n",
      "    val_loss       : -462895.37265625\n",
      "Train Epoch: 468 [0/54000 (0%)] Loss: -457719.187500\n",
      "Train Epoch: 468 [4096/54000 (8%)] Loss: -464720.593750\n",
      "Train Epoch: 468 [8192/54000 (15%)] Loss: -457312.687500\n",
      "Train Epoch: 468 [12288/54000 (23%)] Loss: -453799.375000\n",
      "Train Epoch: 468 [16384/54000 (30%)] Loss: -466112.437500\n",
      "Train Epoch: 468 [20480/54000 (38%)] Loss: -457669.718750\n",
      "Train Epoch: 468 [24576/54000 (46%)] Loss: -457269.218750\n",
      "Train Epoch: 468 [28672/54000 (53%)] Loss: -503397.062500\n",
      "Train Epoch: 468 [32768/54000 (61%)] Loss: -457615.843750\n",
      "Train Epoch: 468 [36864/54000 (68%)] Loss: -450218.406250\n",
      "Train Epoch: 468 [40960/54000 (76%)] Loss: -468053.437500\n",
      "Train Epoch: 468 [45056/54000 (83%)] Loss: -451292.843750\n",
      "Train Epoch: 468 [49152/54000 (91%)] Loss: -504023.687500\n",
      "    epoch          : 468\n",
      "    loss           : -464163.3882621951\n",
      "    val_loss       : -462734.8109375\n",
      "Train Epoch: 469 [0/54000 (0%)] Loss: -470717.875000\n",
      "Train Epoch: 469 [4096/54000 (8%)] Loss: -452607.406250\n",
      "Train Epoch: 469 [8192/54000 (15%)] Loss: -457907.250000\n",
      "Train Epoch: 469 [12288/54000 (23%)] Loss: -449891.281250\n",
      "Train Epoch: 469 [16384/54000 (30%)] Loss: -467131.687500\n",
      "Train Epoch: 469 [20480/54000 (38%)] Loss: -470559.156250\n",
      "Train Epoch: 469 [24576/54000 (46%)] Loss: -461218.750000\n",
      "Train Epoch: 469 [28672/54000 (53%)] Loss: -458109.437500\n",
      "Train Epoch: 469 [32768/54000 (61%)] Loss: -447576.250000\n",
      "Train Epoch: 469 [36864/54000 (68%)] Loss: -504221.687500\n",
      "Train Epoch: 469 [40960/54000 (76%)] Loss: -457459.500000\n",
      "Train Epoch: 469 [45056/54000 (83%)] Loss: -460665.000000\n",
      "Train Epoch: 469 [49152/54000 (91%)] Loss: -464909.937500\n",
      "    epoch          : 469\n",
      "    loss           : -464182.4074695122\n",
      "    val_loss       : -462693.69140625\n",
      "Train Epoch: 470 [0/54000 (0%)] Loss: -504132.125000\n",
      "Train Epoch: 470 [4096/54000 (8%)] Loss: -464912.406250\n",
      "Train Epoch: 470 [8192/54000 (15%)] Loss: -470018.250000\n",
      "Train Epoch: 470 [12288/54000 (23%)] Loss: -452376.437500\n",
      "Train Epoch: 470 [16384/54000 (30%)] Loss: -465751.750000\n",
      "Train Epoch: 470 [20480/54000 (38%)] Loss: -456441.875000\n",
      "Train Epoch: 470 [24576/54000 (46%)] Loss: -452047.968750\n",
      "Train Epoch: 470 [28672/54000 (53%)] Loss: -468256.000000\n",
      "Train Epoch: 470 [32768/54000 (61%)] Loss: -459699.625000\n",
      "Train Epoch: 470 [36864/54000 (68%)] Loss: -452452.875000\n",
      "Train Epoch: 470 [40960/54000 (76%)] Loss: -465098.781250\n",
      "Train Epoch: 470 [45056/54000 (83%)] Loss: -452462.250000\n",
      "Train Epoch: 470 [49152/54000 (91%)] Loss: -504235.593750\n",
      "    epoch          : 470\n",
      "    loss           : -464163.8155487805\n",
      "    val_loss       : -462959.146875\n",
      "Train Epoch: 471 [0/54000 (0%)] Loss: -504402.250000\n",
      "Train Epoch: 471 [4096/54000 (8%)] Loss: -453715.718750\n",
      "Train Epoch: 471 [8192/54000 (15%)] Loss: -450528.562500\n",
      "Train Epoch: 471 [12288/54000 (23%)] Loss: -467439.562500\n",
      "Train Epoch: 471 [16384/54000 (30%)] Loss: -449910.875000\n",
      "Train Epoch: 471 [20480/54000 (38%)] Loss: -502749.625000\n",
      "Train Epoch: 471 [24576/54000 (46%)] Loss: -468513.343750\n",
      "Train Epoch: 471 [28672/54000 (53%)] Loss: -457100.812500\n",
      "Train Epoch: 471 [32768/54000 (61%)] Loss: -468512.687500\n",
      "Train Epoch: 471 [36864/54000 (68%)] Loss: -467683.187500\n",
      "Train Epoch: 471 [40960/54000 (76%)] Loss: -465617.906250\n",
      "Train Epoch: 471 [45056/54000 (83%)] Loss: -453558.562500\n",
      "Train Epoch: 471 [49152/54000 (91%)] Loss: -456534.625000\n",
      "    epoch          : 471\n",
      "    loss           : -464239.51829268294\n",
      "    val_loss       : -462838.11484375\n",
      "Train Epoch: 472 [0/54000 (0%)] Loss: -503956.125000\n",
      "Train Epoch: 472 [4096/54000 (8%)] Loss: -463937.000000\n",
      "Train Epoch: 472 [8192/54000 (15%)] Loss: -456976.250000\n",
      "Train Epoch: 472 [12288/54000 (23%)] Loss: -448907.562500\n",
      "Train Epoch: 472 [16384/54000 (30%)] Loss: -504311.125000\n",
      "Train Epoch: 472 [20480/54000 (38%)] Loss: -469469.468750\n",
      "Train Epoch: 472 [24576/54000 (46%)] Loss: -464788.812500\n",
      "Train Epoch: 472 [28672/54000 (53%)] Loss: -456911.437500\n",
      "Train Epoch: 472 [32768/54000 (61%)] Loss: -504994.750000\n",
      "Train Epoch: 472 [36864/54000 (68%)] Loss: -450040.468750\n",
      "Train Epoch: 472 [40960/54000 (76%)] Loss: -459684.718750\n",
      "Train Epoch: 472 [45056/54000 (83%)] Loss: -469498.875000\n",
      "Train Epoch: 472 [49152/54000 (91%)] Loss: -502417.125000\n",
      "    epoch          : 472\n",
      "    loss           : -464073.9655487805\n",
      "    val_loss       : -462758.68046875\n",
      "Train Epoch: 473 [0/54000 (0%)] Loss: -504187.812500\n",
      "Train Epoch: 473 [4096/54000 (8%)] Loss: -452428.312500\n",
      "Train Epoch: 473 [8192/54000 (15%)] Loss: -467352.312500\n",
      "Train Epoch: 473 [12288/54000 (23%)] Loss: -451161.125000\n",
      "Train Epoch: 473 [16384/54000 (30%)] Loss: -468405.031250\n",
      "Train Epoch: 473 [20480/54000 (38%)] Loss: -457444.250000\n",
      "Train Epoch: 473 [24576/54000 (46%)] Loss: -467767.062500\n",
      "Train Epoch: 473 [28672/54000 (53%)] Loss: -503537.937500\n",
      "Train Epoch: 473 [32768/54000 (61%)] Loss: -449292.625000\n",
      "Train Epoch: 473 [36864/54000 (68%)] Loss: -464936.625000\n",
      "Train Epoch: 473 [40960/54000 (76%)] Loss: -448834.312500\n",
      "Train Epoch: 473 [45056/54000 (83%)] Loss: -465901.843750\n",
      "Train Epoch: 473 [49152/54000 (91%)] Loss: -502350.500000\n",
      "    epoch          : 473\n",
      "    loss           : -464213.7440548781\n",
      "    val_loss       : -462677.17578125\n",
      "Train Epoch: 474 [0/54000 (0%)] Loss: -504113.093750\n",
      "Train Epoch: 474 [4096/54000 (8%)] Loss: -464946.218750\n",
      "Train Epoch: 474 [8192/54000 (15%)] Loss: -451942.937500\n",
      "Train Epoch: 474 [12288/54000 (23%)] Loss: -450255.250000\n",
      "Train Epoch: 474 [16384/54000 (30%)] Loss: -465523.218750\n",
      "Train Epoch: 474 [20480/54000 (38%)] Loss: -456662.093750\n",
      "Train Epoch: 474 [24576/54000 (46%)] Loss: -464811.812500\n",
      "Train Epoch: 474 [28672/54000 (53%)] Loss: -471025.531250\n",
      "Train Epoch: 474 [32768/54000 (61%)] Loss: -464321.375000\n",
      "Train Epoch: 474 [36864/54000 (68%)] Loss: -451045.937500\n",
      "Train Epoch: 474 [40960/54000 (76%)] Loss: -450721.406250\n",
      "Train Epoch: 474 [45056/54000 (83%)] Loss: -468782.093750\n",
      "Train Epoch: 474 [49152/54000 (91%)] Loss: -503729.031250\n",
      "    epoch          : 474\n",
      "    loss           : -464369.4399390244\n",
      "    val_loss       : -462741.2203125\n",
      "Train Epoch: 475 [0/54000 (0%)] Loss: -503174.843750\n",
      "Train Epoch: 475 [4096/54000 (8%)] Loss: -456944.437500\n",
      "Train Epoch: 475 [8192/54000 (15%)] Loss: -454092.250000\n",
      "Train Epoch: 475 [12288/54000 (23%)] Loss: -503358.406250\n",
      "Train Epoch: 475 [16384/54000 (30%)] Loss: -449016.937500\n",
      "Train Epoch: 475 [20480/54000 (38%)] Loss: -466051.593750\n",
      "Train Epoch: 475 [24576/54000 (46%)] Loss: -449173.187500\n",
      "Train Epoch: 475 [28672/54000 (53%)] Loss: -456195.312500\n",
      "Train Epoch: 475 [32768/54000 (61%)] Loss: -449489.312500\n",
      "Train Epoch: 475 [36864/54000 (68%)] Loss: -467915.968750\n",
      "Train Epoch: 475 [40960/54000 (76%)] Loss: -466251.906250\n",
      "Train Epoch: 475 [45056/54000 (83%)] Loss: -468989.562500\n",
      "Train Epoch: 475 [49152/54000 (91%)] Loss: -503609.093750\n",
      "    epoch          : 475\n",
      "    loss           : -464110.54786585364\n",
      "    val_loss       : -462446.41484375\n",
      "Train Epoch: 476 [0/54000 (0%)] Loss: -504192.343750\n",
      "Train Epoch: 476 [4096/54000 (8%)] Loss: -467956.343750\n",
      "Train Epoch: 476 [8192/54000 (15%)] Loss: -451317.062500\n",
      "Train Epoch: 476 [12288/54000 (23%)] Loss: -467081.625000\n",
      "Train Epoch: 476 [16384/54000 (30%)] Loss: -459394.218750\n",
      "Train Epoch: 476 [20480/54000 (38%)] Loss: -466818.437500\n",
      "Train Epoch: 476 [24576/54000 (46%)] Loss: -468734.218750\n",
      "Train Epoch: 476 [28672/54000 (53%)] Loss: -503650.937500\n",
      "Train Epoch: 476 [32768/54000 (61%)] Loss: -469028.000000\n",
      "Train Epoch: 476 [36864/54000 (68%)] Loss: -459735.000000\n",
      "Train Epoch: 476 [40960/54000 (76%)] Loss: -458602.000000\n",
      "Train Epoch: 476 [45056/54000 (83%)] Loss: -469762.843750\n",
      "Train Epoch: 476 [49152/54000 (91%)] Loss: -503819.093750\n",
      "    epoch          : 476\n",
      "    loss           : -464189.4463414634\n",
      "    val_loss       : -462501.01171875\n",
      "Train Epoch: 477 [0/54000 (0%)] Loss: -503428.687500\n",
      "Train Epoch: 477 [4096/54000 (8%)] Loss: -455092.687500\n",
      "Train Epoch: 477 [8192/54000 (15%)] Loss: -458860.500000\n",
      "Train Epoch: 477 [12288/54000 (23%)] Loss: -465971.937500\n",
      "Train Epoch: 477 [16384/54000 (30%)] Loss: -467968.000000\n",
      "Train Epoch: 477 [20480/54000 (38%)] Loss: -469388.812500\n",
      "Train Epoch: 477 [24576/54000 (46%)] Loss: -459186.656250\n",
      "Train Epoch: 477 [28672/54000 (53%)] Loss: -456657.281250\n",
      "Train Epoch: 477 [32768/54000 (61%)] Loss: -466169.656250\n",
      "Train Epoch: 477 [36864/54000 (68%)] Loss: -453179.062500\n",
      "Train Epoch: 477 [40960/54000 (76%)] Loss: -465236.718750\n",
      "Train Epoch: 477 [45056/54000 (83%)] Loss: -453399.843750\n",
      "Train Epoch: 477 [49152/54000 (91%)] Loss: -459792.312500\n",
      "    epoch          : 477\n",
      "    loss           : -464228.2637195122\n",
      "    val_loss       : -463061.39140625\n",
      "Train Epoch: 478 [0/54000 (0%)] Loss: -503870.687500\n",
      "Train Epoch: 478 [4096/54000 (8%)] Loss: -467158.406250\n",
      "Train Epoch: 478 [8192/54000 (15%)] Loss: -470582.562500\n",
      "Train Epoch: 478 [12288/54000 (23%)] Loss: -466096.375000\n",
      "Train Epoch: 478 [16384/54000 (30%)] Loss: -450738.937500\n",
      "Train Epoch: 478 [20480/54000 (38%)] Loss: -471507.875000\n",
      "Train Epoch: 478 [24576/54000 (46%)] Loss: -460050.437500\n",
      "Train Epoch: 478 [28672/54000 (53%)] Loss: -503981.312500\n",
      "Train Epoch: 478 [32768/54000 (61%)] Loss: -465208.000000\n",
      "Train Epoch: 478 [36864/54000 (68%)] Loss: -459895.687500\n",
      "Train Epoch: 478 [40960/54000 (76%)] Loss: -454146.000000\n",
      "Train Epoch: 478 [45056/54000 (83%)] Loss: -470275.593750\n",
      "Train Epoch: 478 [49152/54000 (91%)] Loss: -503830.750000\n",
      "    epoch          : 478\n",
      "    loss           : -464348.51478658535\n",
      "    val_loss       : -462602.88203125\n",
      "Train Epoch: 479 [0/54000 (0%)] Loss: -504343.093750\n",
      "Train Epoch: 479 [4096/54000 (8%)] Loss: -465296.750000\n",
      "Train Epoch: 479 [8192/54000 (15%)] Loss: -468661.187500\n",
      "Train Epoch: 479 [12288/54000 (23%)] Loss: -452112.437500\n",
      "Train Epoch: 479 [16384/54000 (30%)] Loss: -450290.156250\n",
      "Train Epoch: 479 [20480/54000 (38%)] Loss: -469803.218750\n",
      "Train Epoch: 479 [24576/54000 (46%)] Loss: -455169.000000\n",
      "Train Epoch: 479 [28672/54000 (53%)] Loss: -455550.375000\n",
      "Train Epoch: 479 [32768/54000 (61%)] Loss: -459080.968750\n",
      "Train Epoch: 479 [36864/54000 (68%)] Loss: -469781.500000\n",
      "Train Epoch: 479 [40960/54000 (76%)] Loss: -459797.875000\n",
      "Train Epoch: 479 [45056/54000 (83%)] Loss: -469133.875000\n",
      "Train Epoch: 479 [49152/54000 (91%)] Loss: -451117.500000\n",
      "    epoch          : 479\n",
      "    loss           : -464330.5106707317\n",
      "    val_loss       : -462666.63828125\n",
      "Train Epoch: 480 [0/54000 (0%)] Loss: -502990.000000\n",
      "Train Epoch: 480 [4096/54000 (8%)] Loss: -466517.687500\n",
      "Train Epoch: 480 [8192/54000 (15%)] Loss: -456710.718750\n",
      "Train Epoch: 480 [12288/54000 (23%)] Loss: -466619.000000\n",
      "Train Epoch: 480 [16384/54000 (30%)] Loss: -503233.843750\n",
      "Train Epoch: 480 [20480/54000 (38%)] Loss: -460279.000000\n",
      "Train Epoch: 480 [24576/54000 (46%)] Loss: -469988.937500\n",
      "Train Epoch: 480 [28672/54000 (53%)] Loss: -458813.812500\n",
      "Train Epoch: 480 [32768/54000 (61%)] Loss: -504250.468750\n",
      "Train Epoch: 480 [36864/54000 (68%)] Loss: -503728.937500\n",
      "Train Epoch: 480 [40960/54000 (76%)] Loss: -464080.281250\n",
      "Train Epoch: 480 [45056/54000 (83%)] Loss: -469233.562500\n",
      "Train Epoch: 480 [49152/54000 (91%)] Loss: -504068.375000\n",
      "    epoch          : 480\n",
      "    loss           : -464276.5923780488\n",
      "    val_loss       : -462469.65078125\n",
      "Train Epoch: 481 [0/54000 (0%)] Loss: -502776.062500\n",
      "Train Epoch: 481 [4096/54000 (8%)] Loss: -458536.593750\n",
      "Train Epoch: 481 [8192/54000 (15%)] Loss: -459814.625000\n",
      "Train Epoch: 481 [12288/54000 (23%)] Loss: -460654.031250\n",
      "Train Epoch: 481 [16384/54000 (30%)] Loss: -452903.937500\n",
      "Train Epoch: 481 [20480/54000 (38%)] Loss: -459506.968750\n",
      "Train Epoch: 481 [24576/54000 (46%)] Loss: -469093.500000\n",
      "Train Epoch: 481 [28672/54000 (53%)] Loss: -459270.718750\n",
      "Train Epoch: 481 [32768/54000 (61%)] Loss: -449577.406250\n",
      "Train Epoch: 481 [36864/54000 (68%)] Loss: -452320.812500\n",
      "Train Epoch: 481 [40960/54000 (76%)] Loss: -450329.187500\n",
      "Train Epoch: 481 [45056/54000 (83%)] Loss: -469416.968750\n",
      "Train Epoch: 481 [49152/54000 (91%)] Loss: -503519.437500\n",
      "    epoch          : 481\n",
      "    loss           : -464262.62454268296\n",
      "    val_loss       : -462750.778125\n",
      "Train Epoch: 482 [0/54000 (0%)] Loss: -504984.875000\n",
      "Train Epoch: 482 [4096/54000 (8%)] Loss: -449685.437500\n",
      "Train Epoch: 482 [8192/54000 (15%)] Loss: -468945.968750\n",
      "Train Epoch: 482 [12288/54000 (23%)] Loss: -457885.875000\n",
      "Train Epoch: 482 [16384/54000 (30%)] Loss: -445496.343750\n",
      "Train Epoch: 482 [20480/54000 (38%)] Loss: -458343.000000\n",
      "Train Epoch: 482 [24576/54000 (46%)] Loss: -450216.906250\n",
      "Train Epoch: 482 [28672/54000 (53%)] Loss: -468804.812500\n",
      "Train Epoch: 482 [32768/54000 (61%)] Loss: -452510.156250\n",
      "Train Epoch: 482 [36864/54000 (68%)] Loss: -453836.468750\n",
      "Train Epoch: 482 [40960/54000 (76%)] Loss: -450707.312500\n",
      "Train Epoch: 482 [45056/54000 (83%)] Loss: -465601.437500\n",
      "Train Epoch: 482 [49152/54000 (91%)] Loss: -502409.468750\n",
      "    epoch          : 482\n",
      "    loss           : -464162.13551829266\n",
      "    val_loss       : -462737.5578125\n",
      "Train Epoch: 483 [0/54000 (0%)] Loss: -469722.343750\n",
      "Train Epoch: 483 [4096/54000 (8%)] Loss: -468282.218750\n",
      "Train Epoch: 483 [8192/54000 (15%)] Loss: -466689.375000\n",
      "Train Epoch: 483 [12288/54000 (23%)] Loss: -451069.500000\n",
      "Train Epoch: 483 [16384/54000 (30%)] Loss: -457320.500000\n",
      "Train Epoch: 483 [20480/54000 (38%)] Loss: -470479.968750\n",
      "Train Epoch: 483 [24576/54000 (46%)] Loss: -451625.812500\n",
      "Train Epoch: 483 [28672/54000 (53%)] Loss: -504361.625000\n",
      "Train Epoch: 483 [32768/54000 (61%)] Loss: -452615.375000\n",
      "Train Epoch: 483 [36864/54000 (68%)] Loss: -460071.125000\n",
      "Train Epoch: 483 [40960/54000 (76%)] Loss: -458554.125000\n",
      "Train Epoch: 483 [45056/54000 (83%)] Loss: -469624.375000\n",
      "Train Epoch: 483 [49152/54000 (91%)] Loss: -504481.875000\n",
      "    epoch          : 483\n",
      "    loss           : -464317.83856707317\n",
      "    val_loss       : -462994.3734375\n",
      "Train Epoch: 484 [0/54000 (0%)] Loss: -468860.031250\n",
      "Train Epoch: 484 [4096/54000 (8%)] Loss: -453439.687500\n",
      "Train Epoch: 484 [8192/54000 (15%)] Loss: -471354.218750\n",
      "Train Epoch: 484 [12288/54000 (23%)] Loss: -459520.906250\n",
      "Train Epoch: 484 [16384/54000 (30%)] Loss: -451817.687500\n",
      "Train Epoch: 484 [20480/54000 (38%)] Loss: -468641.593750\n",
      "Train Epoch: 484 [24576/54000 (46%)] Loss: -448432.343750\n",
      "Train Epoch: 484 [28672/54000 (53%)] Loss: -502779.687500\n",
      "Train Epoch: 484 [32768/54000 (61%)] Loss: -453163.343750\n",
      "Train Epoch: 484 [36864/54000 (68%)] Loss: -460443.375000\n",
      "Train Epoch: 484 [40960/54000 (76%)] Loss: -467946.875000\n",
      "Train Epoch: 484 [45056/54000 (83%)] Loss: -453659.718750\n",
      "Train Epoch: 484 [49152/54000 (91%)] Loss: -503148.125000\n",
      "    epoch          : 484\n",
      "    loss           : -464230.4140243902\n",
      "    val_loss       : -462954.73359375\n",
      "Train Epoch: 485 [0/54000 (0%)] Loss: -504265.906250\n",
      "Train Epoch: 485 [4096/54000 (8%)] Loss: -457934.000000\n",
      "Train Epoch: 485 [8192/54000 (15%)] Loss: -451904.156250\n",
      "Train Epoch: 485 [12288/54000 (23%)] Loss: -460456.281250\n",
      "Train Epoch: 485 [16384/54000 (30%)] Loss: -451387.500000\n",
      "Train Epoch: 485 [20480/54000 (38%)] Loss: -469808.562500\n",
      "Train Epoch: 485 [24576/54000 (46%)] Loss: -451775.562500\n",
      "Train Epoch: 485 [28672/54000 (53%)] Loss: -454858.000000\n",
      "Train Epoch: 485 [32768/54000 (61%)] Loss: -458802.531250\n",
      "Train Epoch: 485 [36864/54000 (68%)] Loss: -465595.156250\n",
      "Train Epoch: 485 [40960/54000 (76%)] Loss: -451182.125000\n",
      "Train Epoch: 485 [45056/54000 (83%)] Loss: -469454.562500\n",
      "Train Epoch: 485 [49152/54000 (91%)] Loss: -504388.125000\n",
      "    epoch          : 485\n",
      "    loss           : -464245.87774390244\n",
      "    val_loss       : -463025.978125\n",
      "Train Epoch: 486 [0/54000 (0%)] Loss: -504782.687500\n",
      "Train Epoch: 486 [4096/54000 (8%)] Loss: -468421.750000\n",
      "Train Epoch: 486 [8192/54000 (15%)] Loss: -458471.437500\n",
      "Train Epoch: 486 [12288/54000 (23%)] Loss: -453365.875000\n",
      "Train Epoch: 486 [16384/54000 (30%)] Loss: -465553.312500\n",
      "Train Epoch: 486 [20480/54000 (38%)] Loss: -470974.750000\n",
      "Train Epoch: 486 [24576/54000 (46%)] Loss: -453927.906250\n",
      "Train Epoch: 486 [28672/54000 (53%)] Loss: -457320.562500\n",
      "Train Epoch: 486 [32768/54000 (61%)] Loss: -504302.687500\n",
      "Train Epoch: 486 [36864/54000 (68%)] Loss: -450124.625000\n",
      "Train Epoch: 486 [40960/54000 (76%)] Loss: -456825.281250\n",
      "Train Epoch: 486 [45056/54000 (83%)] Loss: -469625.625000\n",
      "Train Epoch: 486 [49152/54000 (91%)] Loss: -505271.750000\n",
      "    epoch          : 486\n",
      "    loss           : -464266.9469512195\n",
      "    val_loss       : -462684.4953125\n",
      "Train Epoch: 487 [0/54000 (0%)] Loss: -456451.812500\n",
      "Train Epoch: 487 [4096/54000 (8%)] Loss: -458139.687500\n",
      "Train Epoch: 487 [8192/54000 (15%)] Loss: -454469.562500\n",
      "Train Epoch: 487 [12288/54000 (23%)] Loss: -468320.687500\n",
      "Train Epoch: 487 [16384/54000 (30%)] Loss: -466151.218750\n",
      "Train Epoch: 487 [20480/54000 (38%)] Loss: -458939.843750\n",
      "Train Epoch: 487 [24576/54000 (46%)] Loss: -449766.000000\n",
      "Train Epoch: 487 [28672/54000 (53%)] Loss: -503591.687500\n",
      "Train Epoch: 487 [32768/54000 (61%)] Loss: -469442.562500\n",
      "Train Epoch: 487 [36864/54000 (68%)] Loss: -449449.625000\n",
      "Train Epoch: 487 [40960/54000 (76%)] Loss: -458641.500000\n",
      "Train Epoch: 487 [45056/54000 (83%)] Loss: -454143.906250\n",
      "Train Epoch: 487 [49152/54000 (91%)] Loss: -503059.687500\n",
      "    epoch          : 487\n",
      "    loss           : -464245.6173780488\n",
      "    val_loss       : -462641.85234375\n",
      "Train Epoch: 488 [0/54000 (0%)] Loss: -504022.625000\n",
      "Train Epoch: 488 [4096/54000 (8%)] Loss: -463410.125000\n",
      "Train Epoch: 488 [8192/54000 (15%)] Loss: -458049.156250\n",
      "Train Epoch: 488 [12288/54000 (23%)] Loss: -466261.250000\n",
      "Train Epoch: 488 [16384/54000 (30%)] Loss: -444629.968750\n",
      "Train Epoch: 488 [20480/54000 (38%)] Loss: -469244.625000\n",
      "Train Epoch: 488 [24576/54000 (46%)] Loss: -468548.000000\n",
      "Train Epoch: 488 [28672/54000 (53%)] Loss: -456750.750000\n",
      "Train Epoch: 488 [32768/54000 (61%)] Loss: -458640.562500\n",
      "Train Epoch: 488 [36864/54000 (68%)] Loss: -469572.750000\n",
      "Train Epoch: 488 [40960/54000 (76%)] Loss: -460038.875000\n",
      "Train Epoch: 488 [45056/54000 (83%)] Loss: -466988.406250\n",
      "Train Epoch: 488 [49152/54000 (91%)] Loss: -504962.156250\n",
      "    epoch          : 488\n",
      "    loss           : -464153.30716463417\n",
      "    val_loss       : -462765.0421875\n",
      "Train Epoch: 489 [0/54000 (0%)] Loss: -503303.906250\n",
      "Train Epoch: 489 [4096/54000 (8%)] Loss: -456931.906250\n",
      "Train Epoch: 489 [8192/54000 (15%)] Loss: -470786.593750\n",
      "Train Epoch: 489 [12288/54000 (23%)] Loss: -459626.625000\n",
      "Train Epoch: 489 [16384/54000 (30%)] Loss: -465999.125000\n",
      "Train Epoch: 489 [20480/54000 (38%)] Loss: -458720.406250\n",
      "Train Epoch: 489 [24576/54000 (46%)] Loss: -454241.625000\n",
      "Train Epoch: 489 [28672/54000 (53%)] Loss: -503453.500000\n",
      "Train Epoch: 489 [32768/54000 (61%)] Loss: -451036.687500\n",
      "Train Epoch: 489 [36864/54000 (68%)] Loss: -453562.000000\n",
      "Train Epoch: 489 [40960/54000 (76%)] Loss: -461630.812500\n",
      "Train Epoch: 489 [45056/54000 (83%)] Loss: -450683.968750\n",
      "Train Epoch: 489 [49152/54000 (91%)] Loss: -504454.375000\n",
      "    epoch          : 489\n",
      "    loss           : -464276.1887195122\n",
      "    val_loss       : -462710.5515625\n",
      "Train Epoch: 490 [0/54000 (0%)] Loss: -504052.750000\n",
      "Train Epoch: 490 [4096/54000 (8%)] Loss: -452399.718750\n",
      "Train Epoch: 490 [8192/54000 (15%)] Loss: -470716.625000\n",
      "Train Epoch: 490 [12288/54000 (23%)] Loss: -466264.250000\n",
      "Train Epoch: 490 [16384/54000 (30%)] Loss: -449843.281250\n",
      "Train Epoch: 490 [20480/54000 (38%)] Loss: -455048.500000\n",
      "Train Epoch: 490 [24576/54000 (46%)] Loss: -450748.500000\n",
      "Train Epoch: 490 [28672/54000 (53%)] Loss: -457234.156250\n",
      "Train Epoch: 490 [32768/54000 (61%)] Loss: -466882.281250\n",
      "Train Epoch: 490 [36864/54000 (68%)] Loss: -505118.875000\n",
      "Train Epoch: 490 [40960/54000 (76%)] Loss: -451205.718750\n",
      "Train Epoch: 490 [45056/54000 (83%)] Loss: -453857.312500\n",
      "Train Epoch: 490 [49152/54000 (91%)] Loss: -504875.468750\n",
      "    epoch          : 490\n",
      "    loss           : -464139.66158536583\n",
      "    val_loss       : -462790.271875\n",
      "Train Epoch: 491 [0/54000 (0%)] Loss: -458447.531250\n",
      "Train Epoch: 491 [4096/54000 (8%)] Loss: -453723.250000\n",
      "Train Epoch: 491 [8192/54000 (15%)] Loss: -450189.968750\n",
      "Train Epoch: 491 [12288/54000 (23%)] Loss: -461723.031250\n",
      "Train Epoch: 491 [16384/54000 (30%)] Loss: -455843.750000\n",
      "Train Epoch: 491 [20480/54000 (38%)] Loss: -458992.906250\n",
      "Train Epoch: 491 [24576/54000 (46%)] Loss: -458051.562500\n",
      "Train Epoch: 491 [28672/54000 (53%)] Loss: -457332.062500\n",
      "Train Epoch: 491 [32768/54000 (61%)] Loss: -468126.125000\n",
      "Train Epoch: 491 [36864/54000 (68%)] Loss: -464835.375000\n",
      "Train Epoch: 491 [40960/54000 (76%)] Loss: -459337.375000\n",
      "Train Epoch: 491 [45056/54000 (83%)] Loss: -466410.500000\n",
      "Train Epoch: 491 [49152/54000 (91%)] Loss: -504952.875000\n",
      "    epoch          : 491\n",
      "    loss           : -464272.34268292686\n",
      "    val_loss       : -462904.9984375\n",
      "Train Epoch: 492 [0/54000 (0%)] Loss: -502595.625000\n",
      "Train Epoch: 492 [4096/54000 (8%)] Loss: -468077.562500\n",
      "Train Epoch: 492 [8192/54000 (15%)] Loss: -469120.343750\n",
      "Train Epoch: 492 [12288/54000 (23%)] Loss: -460828.125000\n",
      "Train Epoch: 492 [16384/54000 (30%)] Loss: -469708.906250\n",
      "Train Epoch: 492 [20480/54000 (38%)] Loss: -470854.937500\n",
      "Train Epoch: 492 [24576/54000 (46%)] Loss: -452866.625000\n",
      "Train Epoch: 492 [28672/54000 (53%)] Loss: -456900.000000\n",
      "Train Epoch: 492 [32768/54000 (61%)] Loss: -450301.593750\n",
      "Train Epoch: 492 [36864/54000 (68%)] Loss: -467398.937500\n",
      "Train Epoch: 492 [40960/54000 (76%)] Loss: -450939.968750\n",
      "Train Epoch: 492 [45056/54000 (83%)] Loss: -469245.500000\n",
      "Train Epoch: 492 [49152/54000 (91%)] Loss: -502465.468750\n",
      "    epoch          : 492\n",
      "    loss           : -464305.38917682925\n",
      "    val_loss       : -462635.5453125\n",
      "Train Epoch: 493 [0/54000 (0%)] Loss: -503802.125000\n",
      "Train Epoch: 493 [4096/54000 (8%)] Loss: -459132.781250\n",
      "Train Epoch: 493 [8192/54000 (15%)] Loss: -469417.250000\n",
      "Train Epoch: 493 [12288/54000 (23%)] Loss: -504859.468750\n",
      "Train Epoch: 493 [16384/54000 (30%)] Loss: -468684.750000\n",
      "Train Epoch: 493 [20480/54000 (38%)] Loss: -468544.406250\n",
      "Train Epoch: 493 [24576/54000 (46%)] Loss: -450216.718750\n",
      "Train Epoch: 493 [28672/54000 (53%)] Loss: -504264.562500\n",
      "Train Epoch: 493 [32768/54000 (61%)] Loss: -450546.625000\n",
      "Train Epoch: 493 [36864/54000 (68%)] Loss: -504968.562500\n",
      "Train Epoch: 493 [40960/54000 (76%)] Loss: -459933.812500\n",
      "Train Epoch: 493 [45056/54000 (83%)] Loss: -468324.875000\n",
      "Train Epoch: 493 [49152/54000 (91%)] Loss: -503592.750000\n",
      "    epoch          : 493\n",
      "    loss           : -464319.368445122\n",
      "    val_loss       : -462653.71640625\n",
      "Train Epoch: 494 [0/54000 (0%)] Loss: -503830.031250\n",
      "Train Epoch: 494 [4096/54000 (8%)] Loss: -450953.875000\n",
      "Train Epoch: 494 [8192/54000 (15%)] Loss: -449025.125000\n",
      "Train Epoch: 494 [12288/54000 (23%)] Loss: -467071.875000\n",
      "Train Epoch: 494 [16384/54000 (30%)] Loss: -457868.062500\n",
      "Train Epoch: 494 [20480/54000 (38%)] Loss: -468686.562500\n",
      "Train Epoch: 494 [24576/54000 (46%)] Loss: -448445.312500\n",
      "Train Epoch: 494 [28672/54000 (53%)] Loss: -450403.843750\n",
      "Train Epoch: 494 [32768/54000 (61%)] Loss: -452939.781250\n",
      "Train Epoch: 494 [36864/54000 (68%)] Loss: -470003.187500\n",
      "Train Epoch: 494 [40960/54000 (76%)] Loss: -449398.875000\n",
      "Train Epoch: 494 [45056/54000 (83%)] Loss: -460075.250000\n",
      "Train Epoch: 494 [49152/54000 (91%)] Loss: -505414.906250\n",
      "    epoch          : 494\n",
      "    loss           : -464214.55152439023\n",
      "    val_loss       : -462716.6609375\n",
      "Train Epoch: 495 [0/54000 (0%)] Loss: -469747.031250\n",
      "Train Epoch: 495 [4096/54000 (8%)] Loss: -450819.625000\n",
      "Train Epoch: 495 [8192/54000 (15%)] Loss: -456193.312500\n",
      "Train Epoch: 495 [12288/54000 (23%)] Loss: -465388.125000\n",
      "Train Epoch: 495 [16384/54000 (30%)] Loss: -469509.437500\n",
      "Train Epoch: 495 [20480/54000 (38%)] Loss: -468792.812500\n",
      "Train Epoch: 495 [24576/54000 (46%)] Loss: -452716.437500\n",
      "Train Epoch: 495 [28672/54000 (53%)] Loss: -503274.437500\n",
      "Train Epoch: 495 [32768/54000 (61%)] Loss: -457046.187500\n",
      "Train Epoch: 495 [36864/54000 (68%)] Loss: -449350.968750\n",
      "Train Epoch: 495 [40960/54000 (76%)] Loss: -458318.875000\n",
      "Train Epoch: 495 [45056/54000 (83%)] Loss: -466092.375000\n",
      "Train Epoch: 495 [49152/54000 (91%)] Loss: -504136.062500\n",
      "    epoch          : 495\n",
      "    loss           : -464238.3318597561\n",
      "    val_loss       : -462764.70546875\n",
      "Train Epoch: 496 [0/54000 (0%)] Loss: -458301.000000\n",
      "Train Epoch: 496 [4096/54000 (8%)] Loss: -451685.312500\n",
      "Train Epoch: 496 [8192/54000 (15%)] Loss: -468948.343750\n",
      "Train Epoch: 496 [12288/54000 (23%)] Loss: -459773.875000\n",
      "Train Epoch: 496 [16384/54000 (30%)] Loss: -450955.218750\n",
      "Train Epoch: 496 [20480/54000 (38%)] Loss: -459092.750000\n",
      "Train Epoch: 496 [24576/54000 (46%)] Loss: -504489.750000\n",
      "Train Epoch: 496 [28672/54000 (53%)] Loss: -460268.312500\n",
      "Train Epoch: 496 [32768/54000 (61%)] Loss: -468309.562500\n",
      "Train Epoch: 496 [36864/54000 (68%)] Loss: -468514.687500\n",
      "Train Epoch: 496 [40960/54000 (76%)] Loss: -451217.625000\n",
      "Train Epoch: 496 [45056/54000 (83%)] Loss: -468172.781250\n",
      "Train Epoch: 496 [49152/54000 (91%)] Loss: -504193.312500\n",
      "    epoch          : 496\n",
      "    loss           : -464274.17591463414\n",
      "    val_loss       : -462817.6984375\n",
      "Train Epoch: 497 [0/54000 (0%)] Loss: -503670.843750\n",
      "Train Epoch: 497 [4096/54000 (8%)] Loss: -465351.687500\n",
      "Train Epoch: 497 [8192/54000 (15%)] Loss: -469827.875000\n",
      "Train Epoch: 497 [12288/54000 (23%)] Loss: -464178.875000\n",
      "Train Epoch: 497 [16384/54000 (30%)] Loss: -469112.687500\n",
      "Train Epoch: 497 [20480/54000 (38%)] Loss: -468944.062500\n",
      "Train Epoch: 497 [24576/54000 (46%)] Loss: -458150.000000\n",
      "Train Epoch: 497 [28672/54000 (53%)] Loss: -504903.781250\n",
      "Train Epoch: 497 [32768/54000 (61%)] Loss: -465607.437500\n",
      "Train Epoch: 497 [36864/54000 (68%)] Loss: -469358.218750\n",
      "Train Epoch: 497 [40960/54000 (76%)] Loss: -451838.281250\n",
      "Train Epoch: 497 [45056/54000 (83%)] Loss: -453005.000000\n",
      "Train Epoch: 497 [49152/54000 (91%)] Loss: -504935.562500\n",
      "    epoch          : 497\n",
      "    loss           : -464285.2230182927\n",
      "    val_loss       : -462996.7859375\n",
      "Train Epoch: 498 [0/54000 (0%)] Loss: -503903.375000\n",
      "Train Epoch: 498 [4096/54000 (8%)] Loss: -460200.218750\n",
      "Train Epoch: 498 [8192/54000 (15%)] Loss: -451308.562500\n",
      "Train Epoch: 498 [12288/54000 (23%)] Loss: -449966.093750\n",
      "Train Epoch: 498 [16384/54000 (30%)] Loss: -449716.812500\n",
      "Train Epoch: 498 [20480/54000 (38%)] Loss: -503896.187500\n",
      "Train Epoch: 498 [24576/54000 (46%)] Loss: -453320.500000\n",
      "Train Epoch: 498 [28672/54000 (53%)] Loss: -457243.281250\n",
      "Train Epoch: 498 [32768/54000 (61%)] Loss: -452786.093750\n",
      "Train Epoch: 498 [36864/54000 (68%)] Loss: -451531.562500\n",
      "Train Epoch: 498 [40960/54000 (76%)] Loss: -458149.687500\n",
      "Train Epoch: 498 [45056/54000 (83%)] Loss: -467059.937500\n",
      "Train Epoch: 498 [49152/54000 (91%)] Loss: -504162.281250\n",
      "    epoch          : 498\n",
      "    loss           : -464249.16143292683\n",
      "    val_loss       : -462499.5328125\n",
      "Train Epoch: 499 [0/54000 (0%)] Loss: -504424.093750\n",
      "Train Epoch: 499 [4096/54000 (8%)] Loss: -455999.531250\n",
      "Train Epoch: 499 [8192/54000 (15%)] Loss: -470251.500000\n",
      "Train Epoch: 499 [12288/54000 (23%)] Loss: -452993.281250\n",
      "Train Epoch: 499 [16384/54000 (30%)] Loss: -451735.375000\n",
      "Train Epoch: 499 [20480/54000 (38%)] Loss: -468825.500000\n",
      "Train Epoch: 499 [24576/54000 (46%)] Loss: -466274.125000\n",
      "Train Epoch: 499 [28672/54000 (53%)] Loss: -504456.468750\n",
      "Train Epoch: 499 [32768/54000 (61%)] Loss: -451393.250000\n",
      "Train Epoch: 499 [36864/54000 (68%)] Loss: -468874.031250\n",
      "Train Epoch: 499 [40960/54000 (76%)] Loss: -469002.437500\n",
      "Train Epoch: 499 [45056/54000 (83%)] Loss: -453754.187500\n",
      "Train Epoch: 499 [49152/54000 (91%)] Loss: -504412.875000\n",
      "    epoch          : 499\n",
      "    loss           : -464295.79222560977\n",
      "    val_loss       : -462541.3359375\n",
      "Train Epoch: 500 [0/54000 (0%)] Loss: -504312.281250\n",
      "Train Epoch: 500 [4096/54000 (8%)] Loss: -468374.500000\n",
      "Train Epoch: 500 [8192/54000 (15%)] Loss: -449669.875000\n",
      "Train Epoch: 500 [12288/54000 (23%)] Loss: -497165.312500\n",
      "Train Epoch: 500 [16384/54000 (30%)] Loss: -449716.812500\n",
      "Train Epoch: 500 [20480/54000 (38%)] Loss: -469021.250000\n",
      "Train Epoch: 500 [24576/54000 (46%)] Loss: -455215.156250\n",
      "Train Epoch: 500 [28672/54000 (53%)] Loss: -504155.812500\n",
      "Train Epoch: 500 [32768/54000 (61%)] Loss: -459712.000000\n",
      "Train Epoch: 500 [36864/54000 (68%)] Loss: -452521.312500\n",
      "Train Epoch: 500 [40960/54000 (76%)] Loss: -463296.812500\n",
      "Train Epoch: 500 [45056/54000 (83%)] Loss: -470094.062500\n",
      "Train Epoch: 500 [49152/54000 (91%)] Loss: -502964.781250\n",
      "    epoch          : 500\n",
      "    loss           : -464100.96722560975\n",
      "    val_loss       : -462732.70703125\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0510_004522/checkpoint-epoch500.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
