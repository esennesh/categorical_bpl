{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 48573.597656\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -149413.343750\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -287092.656250\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -273241.375000\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -419853.875000\n",
      "    epoch          : 1\n",
      "    loss           : -254271.6375599474\n",
      "    val_loss       : -453310.55703125\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -479659.500000\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -504243.937500\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -547765.000000\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -575863.875000\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -603978.875000\n",
      "    epoch          : 2\n",
      "    loss           : -559963.4746287129\n",
      "    val_loss       : -628840.953125\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -660111.500000\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -602726.437500\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -639750.500000\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -617268.500000\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -703248.875000\n",
      "    epoch          : 3\n",
      "    loss           : -674318.6509900991\n",
      "    val_loss       : -719994.8140625\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -811870.500000\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -689424.312500\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -755195.312500\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -818387.000000\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -691440.000000\n",
      "    epoch          : 4\n",
      "    loss           : -734412.9709158416\n",
      "    val_loss       : -741478.6859375\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -815225.687500\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -795584.812500\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -800269.875000\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -750758.625000\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -797752.562500\n",
      "    epoch          : 5\n",
      "    loss           : -777035.3025990099\n",
      "    val_loss       : -798066.05390625\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -907138.625000\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -777313.375000\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -729728.937500\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -742862.437500\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -801643.500000\n",
      "    epoch          : 6\n",
      "    loss           : -802429.2456683168\n",
      "    val_loss       : -805650.44765625\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -848505.187500\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -790830.375000\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -771036.375000\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -815591.625000\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -822044.625000\n",
      "    epoch          : 7\n",
      "    loss           : -813666.0272277228\n",
      "    val_loss       : -819125.0859375\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -904436.375000\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -855552.562500\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -855253.187500\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -831710.250000\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -856013.125000\n",
      "    epoch          : 8\n",
      "    loss           : -833380.6633663366\n",
      "    val_loss       : -845511.6421875\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -819896.562500\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -870181.187500\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -785848.187500\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -849848.812500\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -855648.000000\n",
      "    epoch          : 9\n",
      "    loss           : -842964.707920792\n",
      "    val_loss       : -855362.01328125\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -949139.750000\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -869380.000000\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -832337.500000\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -878719.875000\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -855478.500000\n",
      "    epoch          : 10\n",
      "    loss           : -863251.5402227723\n",
      "    val_loss       : -851697.23828125\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -961217.625000\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -902316.750000\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -850124.500000\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -830564.437500\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -871854.375000\n",
      "    epoch          : 11\n",
      "    loss           : -874165.1497524752\n",
      "    val_loss       : -872271.50859375\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -975264.562500\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -860672.562500\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -851744.375000\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -878921.000000\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -864447.937500\n",
      "    epoch          : 12\n",
      "    loss           : -875778.5847772277\n",
      "    val_loss       : -875460.17265625\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -975819.437500\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -869847.062500\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -854844.875000\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -875449.687500\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -887579.125000\n",
      "    epoch          : 13\n",
      "    loss           : -882239.323019802\n",
      "    val_loss       : -876834.7140625\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -968371.375000\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -853774.312500\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -886362.375000\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -969302.250000\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -878463.250000\n",
      "    epoch          : 14\n",
      "    loss           : -877743.958539604\n",
      "    val_loss       : -885558.265625\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -978071.250000\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -862145.437500\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -852427.312500\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -885890.812500\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -892678.875000\n",
      "    epoch          : 15\n",
      "    loss           : -883345.6868811881\n",
      "    val_loss       : -873590.76328125\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -967601.500000\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -857986.625000\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -907858.375000\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -899166.937500\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -867867.625000\n",
      "    epoch          : 16\n",
      "    loss           : -892636.5538366337\n",
      "    val_loss       : -897217.21796875\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -982994.562500\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -890152.312500\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -909553.375000\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -911377.375000\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -904789.937500\n",
      "    epoch          : 17\n",
      "    loss           : -896635.5408415842\n",
      "    val_loss       : -896419.3984375\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -989609.500000\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -910195.562500\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -871599.000000\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -893020.500000\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -868608.812500\n",
      "    epoch          : 18\n",
      "    loss           : -901197.7004950495\n",
      "    val_loss       : -898610.10234375\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -989084.687500\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -855825.875000\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -898276.750000\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -899111.250000\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -909012.750000\n",
      "    epoch          : 19\n",
      "    loss           : -895341.9783415842\n",
      "    val_loss       : -908626.91796875\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -997379.000000\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -887110.312500\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -916653.062500\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -862585.375000\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -872705.062500\n",
      "    epoch          : 20\n",
      "    loss           : -906078.0030940594\n",
      "    val_loss       : -903491.203125\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -989555.875000\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -886706.000000\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -871871.312500\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -903640.812500\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -904403.000000\n",
      "    epoch          : 21\n",
      "    loss           : -905603.6219059406\n",
      "    val_loss       : -906527.03515625\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -996558.625000\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -895802.687500\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -898829.375000\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -877284.875000\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -875343.375000\n",
      "    epoch          : 22\n",
      "    loss           : -908506.5699257426\n",
      "    val_loss       : -908730.88984375\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -991325.937500\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -890259.937500\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -888042.062500\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -874352.125000\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -873045.937500\n",
      "    epoch          : 23\n",
      "    loss           : -911060.667079208\n",
      "    val_loss       : -907283.959375\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -987134.375000\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -935094.937500\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -881422.500000\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -919539.625000\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -917129.187500\n",
      "    epoch          : 24\n",
      "    loss           : -910344.3452970297\n",
      "    val_loss       : -911583.44453125\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -997860.687500\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -921339.312500\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -882873.125000\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -886106.500000\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -884382.375000\n",
      "    epoch          : 25\n",
      "    loss           : -911612.1992574257\n",
      "    val_loss       : -913165.6\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -1001055.375000\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -896462.750000\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -923542.250000\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -886619.937500\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -881230.062500\n",
      "    epoch          : 26\n",
      "    loss           : -914974.3180693069\n",
      "    val_loss       : -913965.1515625\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -998726.812500\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -899272.500000\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -925590.625000\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -999703.250000\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -912793.500000\n",
      "    epoch          : 27\n",
      "    loss           : -918730.2357673268\n",
      "    val_loss       : -916243.89921875\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -996561.500000\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -906283.437500\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -899606.375000\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -915593.375000\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -892324.312500\n",
      "    epoch          : 28\n",
      "    loss           : -920782.9152227723\n",
      "    val_loss       : -915211.76484375\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -943442.500000\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -898665.437500\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -888391.937500\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -1000878.562500\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -915227.750000\n",
      "    epoch          : 29\n",
      "    loss           : -916494.041460396\n",
      "    val_loss       : -919445.4921875\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -1004504.750000\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -898635.500000\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -908497.000000\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -912224.625000\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -928944.937500\n",
      "    epoch          : 30\n",
      "    loss           : -923006.0179455446\n",
      "    val_loss       : -918294.24609375\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -944489.125000\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -904946.187500\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -930271.687500\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -888375.250000\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -892412.187500\n",
      "    epoch          : 31\n",
      "    loss           : -920774.7227722772\n",
      "    val_loss       : -923776.61328125\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -899809.875000\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -895156.375000\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -899357.125000\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -928973.062500\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -925562.750000\n",
      "    epoch          : 32\n",
      "    loss           : -924468.8069306931\n",
      "    val_loss       : -917315.265625\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -1002177.250000\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -908743.562500\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -899945.562500\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -1010000.375000\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -895368.687500\n",
      "    epoch          : 33\n",
      "    loss           : -926726.9882425743\n",
      "    val_loss       : -922628.50546875\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -1007117.125000\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -930486.812500\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -932399.500000\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -929913.062500\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -897843.000000\n",
      "    epoch          : 34\n",
      "    loss           : -925141.7153465346\n",
      "    val_loss       : -925969.69609375\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -1003779.187500\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -905290.625000\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -900666.562500\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -915686.187500\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -897231.562500\n",
      "    epoch          : 35\n",
      "    loss           : -926215.8075495049\n",
      "    val_loss       : -927109.36875\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -949859.625000\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -911716.125000\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -925847.750000\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -897456.312500\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -926165.125000\n",
      "    epoch          : 36\n",
      "    loss           : -927093.7227722772\n",
      "    val_loss       : -916921.40234375\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -992057.000000\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -904329.500000\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -933608.250000\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -933919.125000\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -921517.500000\n",
      "    epoch          : 37\n",
      "    loss           : -927129.9987623763\n",
      "    val_loss       : -925259.43359375\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -995412.250000\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -912630.625000\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -934094.500000\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -925284.750000\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -936128.500000\n",
      "    epoch          : 38\n",
      "    loss           : -928937.3112623763\n",
      "    val_loss       : -924320.0734375\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -1006700.875000\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -909985.625000\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -899714.187500\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -931021.437500\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -897586.875000\n",
      "    epoch          : 39\n",
      "    loss           : -927512.1126237623\n",
      "    val_loss       : -930937.7296875\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -1009304.437500\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -956484.875000\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -899973.562500\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -1007045.125000\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -935722.437500\n",
      "    epoch          : 40\n",
      "    loss           : -932261.9028465346\n",
      "    val_loss       : -931124.50703125\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -955794.375000\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -936822.375000\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -932594.000000\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -903219.687500\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -936571.875000\n",
      "    epoch          : 41\n",
      "    loss           : -930985.3632425743\n",
      "    val_loss       : -929707.92890625\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -1008761.750000\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -900432.750000\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -938497.375000\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -934150.125000\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -930105.750000\n",
      "    epoch          : 42\n",
      "    loss           : -931870.4121287129\n",
      "    val_loss       : -932829.5625\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -1011620.125000\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -940837.437500\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -906952.687500\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -930256.250000\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -902059.937500\n",
      "    epoch          : 43\n",
      "    loss           : -931482.603960396\n",
      "    val_loss       : -930452.61328125\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -1005453.812500\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -900877.500000\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -938337.625000\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -902866.625000\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -927624.750000\n",
      "    epoch          : 44\n",
      "    loss           : -930957.7995049505\n",
      "    val_loss       : -933289.40234375\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -1007912.625000\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -907512.125000\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -908224.437500\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -926974.562500\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -906618.625000\n",
      "    epoch          : 45\n",
      "    loss           : -931838.9758663366\n",
      "    val_loss       : -931539.26875\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -1007871.250000\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -908427.875000\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -909166.437500\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -943093.750000\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -930795.437500\n",
      "    epoch          : 46\n",
      "    loss           : -935330.7215346535\n",
      "    val_loss       : -934666.26015625\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -939699.500000\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -936751.875000\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -909404.375000\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -938214.500000\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -899799.875000\n",
      "    epoch          : 47\n",
      "    loss           : -932459.9405940594\n",
      "    val_loss       : -932712.065625\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -1009140.750000\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -910709.250000\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -931975.750000\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -906913.125000\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -909854.375000\n",
      "    epoch          : 48\n",
      "    loss           : -933656.4430693069\n",
      "    val_loss       : -932137.128125\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -1005979.250000\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -955935.875000\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -933769.875000\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -904199.500000\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -937604.687500\n",
      "    epoch          : 49\n",
      "    loss           : -931675.2964108911\n",
      "    val_loss       : -933642.49296875\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -1012534.062500\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -926434.187500\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -930330.812500\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -1012080.687500\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -938577.875000\n",
      "    epoch          : 50\n",
      "    loss           : -935968.7165841584\n",
      "    val_loss       : -933186.03671875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0818_234119/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -1012404.625000\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -917922.437500\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -914049.062500\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -1012114.125000\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -938782.375000\n",
      "    epoch          : 51\n",
      "    loss           : -935207.7951732674\n",
      "    val_loss       : -931370.16875\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -1008552.625000\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -900856.812500\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -919131.687500\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -909833.250000\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -904916.750000\n",
      "    epoch          : 52\n",
      "    loss           : -931127.6509900991\n",
      "    val_loss       : -934175.1484375\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -1008337.687500\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -908860.500000\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -941491.125000\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -910805.687500\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -911960.750000\n",
      "    epoch          : 53\n",
      "    loss           : -934705.9356435643\n",
      "    val_loss       : -938139.84375\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -1012091.687500\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -925644.250000\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -906922.625000\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -901346.375000\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -931734.000000\n",
      "    epoch          : 54\n",
      "    loss           : -936905.9622524752\n",
      "    val_loss       : -933686.8734375\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -1013203.562500\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -909468.625000\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -910684.187500\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -931848.562500\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -937767.937500\n",
      "    epoch          : 55\n",
      "    loss           : -937454.0142326732\n",
      "    val_loss       : -928892.3671875\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -904978.875000\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -919473.000000\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -904595.750000\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -931175.750000\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -911424.250000\n",
      "    epoch          : 56\n",
      "    loss           : -935043.1441831683\n",
      "    val_loss       : -938272.9\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -1013916.250000\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -924176.500000\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -906358.375000\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -931878.625000\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -931213.375000\n",
      "    epoch          : 57\n",
      "    loss           : -934188.4603960396\n",
      "    val_loss       : -929179.203125\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -1009125.000000\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -956428.437500\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -916120.687500\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -912102.375000\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -910138.625000\n",
      "    epoch          : 58\n",
      "    loss           : -936591.5396039604\n",
      "    val_loss       : -937119.825\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -1011269.500000\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -955865.125000\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -937921.500000\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -904781.812500\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -935124.500000\n",
      "    epoch          : 59\n",
      "    loss           : -935021.0495049505\n",
      "    val_loss       : -931589.1453125\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -1006669.625000\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -912991.562500\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -931880.437500\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -947001.187500\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -938790.562500\n",
      "    epoch          : 60\n",
      "    loss           : -937524.7240099009\n",
      "    val_loss       : -933508.11171875\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -1011460.250000\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -953424.125000\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -940164.250000\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -940566.812500\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -930187.375000\n",
      "    epoch          : 61\n",
      "    loss           : -935127.4152227723\n",
      "    val_loss       : -935459.13515625\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -1011822.187500\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -922894.625000\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -911690.000000\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -915324.312500\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -937772.375000\n",
      "    epoch          : 62\n",
      "    loss           : -939734.4189356435\n",
      "    val_loss       : -939136.06640625\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -1014714.375000\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -922948.250000\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -918932.750000\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -912214.125000\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -933921.375000\n",
      "    epoch          : 63\n",
      "    loss           : -938762.1058168317\n",
      "    val_loss       : -936397.3609375\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -1012103.125000\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -911710.250000\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -909986.937500\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -940316.625000\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -942063.625000\n",
      "    epoch          : 64\n",
      "    loss           : -936507.8582920792\n",
      "    val_loss       : -936479.309375\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -915425.125000\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -923305.000000\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -942196.875000\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -929321.750000\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -932644.250000\n",
      "    epoch          : 65\n",
      "    loss           : -937631.1992574257\n",
      "    val_loss       : -934249.703125\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -960516.250000\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -921789.125000\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -942486.375000\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -944230.812500\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -914640.000000\n",
      "    epoch          : 66\n",
      "    loss           : -939196.479579208\n",
      "    val_loss       : -939265.25390625\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -1010872.000000\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -922668.375000\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -914515.500000\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -894139.312500\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -939087.625000\n",
      "    epoch          : 67\n",
      "    loss           : -935283.4461633663\n",
      "    val_loss       : -933065.24609375\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -1008013.250000\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -922942.812500\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -919856.625000\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -947131.500000\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -935118.875000\n",
      "    epoch          : 68\n",
      "    loss           : -940591.3712871287\n",
      "    val_loss       : -939442.74921875\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -1014982.500000\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -925148.875000\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -933660.500000\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -910062.375000\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -901748.875000\n",
      "    epoch          : 69\n",
      "    loss           : -939530.375\n",
      "    val_loss       : -935614.2109375\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -1014562.500000\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -918659.937500\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -912493.187500\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -912663.375000\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -943217.000000\n",
      "    epoch          : 70\n",
      "    loss           : -936877.5198019802\n",
      "    val_loss       : -939089.54296875\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -1013590.687500\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -955872.750000\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -917853.875000\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -912171.000000\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -944739.875000\n",
      "    epoch          : 71\n",
      "    loss           : -940135.1070544554\n",
      "    val_loss       : -938997.59765625\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -1013609.000000\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -961069.750000\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -916545.875000\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -909133.562500\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -911287.062500\n",
      "    epoch          : 72\n",
      "    loss           : -938955.3125\n",
      "    val_loss       : -937011.6890625\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -1009325.125000\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -960964.000000\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -904049.375000\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -940862.750000\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -932484.125000\n",
      "    epoch          : 73\n",
      "    loss           : -936659.1720297029\n",
      "    val_loss       : -938088.0609375\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -918806.812500\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -957239.625000\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -910183.875000\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -938967.500000\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -904136.375000\n",
      "    epoch          : 74\n",
      "    loss           : -938143.8310643565\n",
      "    val_loss       : -936119.26640625\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -918636.125000\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -917594.500000\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -942525.750000\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -942198.875000\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -915448.312500\n",
      "    epoch          : 75\n",
      "    loss           : -939396.0847772277\n",
      "    val_loss       : -938683.04296875\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -1015606.625000\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -924471.437500\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -914151.750000\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -1009630.000000\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -932262.625000\n",
      "    epoch          : 76\n",
      "    loss           : -940163.5810643565\n",
      "    val_loss       : -936610.88203125\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -1013948.875000\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -911209.562500\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -934394.312500\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -916487.500000\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -920983.750000\n",
      "    epoch          : 77\n",
      "    loss           : -941357.2902227723\n",
      "    val_loss       : -940686.01796875\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -1015665.750000\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -960500.500000\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -935574.500000\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -932317.125000\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -909066.187500\n",
      "    epoch          : 78\n",
      "    loss           : -940926.9306930694\n",
      "    val_loss       : -935890.6203125\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -1009577.000000\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -924567.750000\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -933627.187500\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -911640.875000\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -937121.562500\n",
      "    epoch          : 79\n",
      "    loss           : -939647.1943069306\n",
      "    val_loss       : -937126.07421875\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -1011312.125000\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -923408.000000\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -918652.750000\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -1015426.312500\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -913709.500000\n",
      "    epoch          : 80\n",
      "    loss           : -939799.7506188119\n",
      "    val_loss       : -938691.821875\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -1015346.250000\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -919894.562500\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -912942.125000\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -1011349.375000\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -942488.625000\n",
      "    epoch          : 81\n",
      "    loss           : -940123.4313118812\n",
      "    val_loss       : -937989.68515625\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -1012294.375000\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -926048.000000\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -911081.812500\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -943243.375000\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -909140.375000\n",
      "    epoch          : 82\n",
      "    loss           : -938495.0693069306\n",
      "    val_loss       : -936804.85625\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -962272.125000\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -926535.125000\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -945336.687500\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -945604.250000\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -909418.625000\n",
      "    epoch          : 83\n",
      "    loss           : -939782.1540841584\n",
      "    val_loss       : -936499.1578125\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -962403.000000\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -923769.437500\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -919350.937500\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -1013325.812500\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -940871.375000\n",
      "    epoch          : 84\n",
      "    loss           : -939276.1831683168\n",
      "    val_loss       : -932998.16328125\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -1009987.875000\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -958239.250000\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -917806.625000\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -915955.187500\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -913180.937500\n",
      "    epoch          : 85\n",
      "    loss           : -939730.7827970297\n",
      "    val_loss       : -938405.17421875\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -1013658.000000\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -925016.250000\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -945717.750000\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -933203.062500\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -912756.125000\n",
      "    epoch          : 86\n",
      "    loss           : -941113.1707920792\n",
      "    val_loss       : -939623.571875\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -1014371.500000\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -959277.125000\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -914318.875000\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -919958.937500\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -944899.187500\n",
      "    epoch          : 87\n",
      "    loss           : -941782.7382425743\n",
      "    val_loss       : -939990.15703125\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -1013398.250000\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -918245.062500\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -921776.562500\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -947322.875000\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -936099.750000\n",
      "    epoch          : 88\n",
      "    loss           : -941537.551980198\n",
      "    val_loss       : -938061.06484375\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -1012841.187500\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -926640.312500\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -911341.312500\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -937946.875000\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -941253.625000\n",
      "    epoch          : 89\n",
      "    loss           : -939381.3824257426\n",
      "    val_loss       : -937951.8125\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -1012375.375000\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -917267.562500\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -914232.375000\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -1014927.625000\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -944191.500000\n",
      "    epoch          : 90\n",
      "    loss           : -940092.5167079208\n",
      "    val_loss       : -938687.715625\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -918439.750000\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -955753.375000\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -944137.875000\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -999226.062500\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -912961.625000\n",
      "    epoch          : 91\n",
      "    loss           : -939455.4938118812\n",
      "    val_loss       : -937224.246875\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -1011500.937500\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -924078.625000\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -942528.812500\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -944285.625000\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -918093.375000\n",
      "    epoch          : 92\n",
      "    loss           : -941601.5798267326\n",
      "    val_loss       : -939582.8921875\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -1015296.625000\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -928775.875000\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -949636.375000\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -916670.687500\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -937748.875000\n",
      "    epoch          : 93\n",
      "    loss           : -942477.957920792\n",
      "    val_loss       : -939727.3375\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -1014786.125000\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -918649.500000\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -915299.250000\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -916147.625000\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -943501.437500\n",
      "    epoch          : 94\n",
      "    loss           : -942071.9263613861\n",
      "    val_loss       : -941285.2984375\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -1015899.312500\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -925995.625000\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -911620.062500\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -918163.125000\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -943583.187500\n",
      "    epoch          : 95\n",
      "    loss           : -941224.3409653465\n",
      "    val_loss       : -934967.6796875\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -1000836.125000\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -925117.375000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -953530.937500\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -1015858.125000\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -915236.375000\n",
      "    epoch          : 96\n",
      "    loss           : -941873.0321782178\n",
      "    val_loss       : -938782.803125\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -1013005.125000\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -926527.125000\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -942163.250000\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -914146.312500\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -912620.625000\n",
      "    epoch          : 97\n",
      "    loss           : -938850.0625\n",
      "    val_loss       : -939337.71171875\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -1011666.250000\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -960566.750000\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -948178.500000\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -933747.937500\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -943939.250000\n",
      "    epoch          : 98\n",
      "    loss           : -940553.9201732674\n",
      "    val_loss       : -939278.68515625\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -1014884.875000\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -927330.875000\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -944282.125000\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -944160.062500\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -932961.250000\n",
      "    epoch          : 99\n",
      "    loss           : -940366.4616336634\n",
      "    val_loss       : -938471.4546875\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -1014866.937500\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -927679.375000\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -920002.562500\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -912542.250000\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -945774.375000\n",
      "    epoch          : 100\n",
      "    loss           : -942308.6782178218\n",
      "    val_loss       : -941124.5890625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0818_234119/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -1015197.375000\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -946495.937500\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -917536.875000\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -947863.375000\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -918387.000000\n",
      "    epoch          : 101\n",
      "    loss           : -944423.4461633663\n",
      "    val_loss       : -942917.2828125\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -1015712.687500\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -960787.562500\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -916065.812500\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -944859.500000\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -914591.750000\n",
      "    epoch          : 102\n",
      "    loss           : -942744.0662128713\n",
      "    val_loss       : -939817.25703125\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -944904.687500\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -916919.625000\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -932045.812500\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -911372.875000\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -944641.437500\n",
      "    epoch          : 103\n",
      "    loss           : -940277.9981435643\n",
      "    val_loss       : -935183.5484375\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -963571.375000\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -940741.625000\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -919680.062500\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -921117.125000\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -947642.437500\n",
      "    epoch          : 104\n",
      "    loss           : -941181.5996287129\n",
      "    val_loss       : -941558.5421875\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -1016456.500000\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -925380.187500\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -950589.375000\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -1017133.875000\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -943344.500000\n",
      "    epoch          : 105\n",
      "    loss           : -943132.8706683168\n",
      "    val_loss       : -940198.8421875\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -1015070.062500\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -925830.062500\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -914659.937500\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -914968.187500\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -912990.750000\n",
      "    epoch          : 106\n",
      "    loss           : -938688.3576732674\n",
      "    val_loss       : -940578.77734375\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -1018054.750000\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -931519.250000\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -941080.000000\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -920761.812500\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -917586.750000\n",
      "    epoch          : 107\n",
      "    loss           : -944789.6794554455\n",
      "    val_loss       : -940427.584375\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -1014070.875000\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -910381.812500\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -917470.250000\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -942734.125000\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -915621.625000\n",
      "    epoch          : 108\n",
      "    loss           : -941208.9245049505\n",
      "    val_loss       : -937772.4703125\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -1009707.000000\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -956894.000000\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -914890.750000\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -948359.750000\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -948850.500000\n",
      "    epoch          : 109\n",
      "    loss           : -941924.0123762377\n",
      "    val_loss       : -941923.5515625\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -1015035.062500\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -918374.687500\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -946076.437500\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -948814.750000\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -939752.250000\n",
      "    epoch          : 110\n",
      "    loss           : -944044.2883663366\n",
      "    val_loss       : -941186.4578125\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -1016564.875000\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -926971.750000\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -949524.375000\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -945315.500000\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -916147.562500\n",
      "    epoch          : 111\n",
      "    loss           : -942705.1658415842\n",
      "    val_loss       : -940821.625\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -1017388.750000\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -964055.750000\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -918442.687500\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -911363.187500\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -908238.937500\n",
      "    epoch          : 112\n",
      "    loss           : -940571.1949257426\n",
      "    val_loss       : -936047.4703125\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -1011578.500000\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -926159.812500\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -950815.187500\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -1018176.500000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -939971.062500\n",
      "    epoch          : 113\n",
      "    loss           : -943114.718440594\n",
      "    val_loss       : -940281.890625\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -1013743.125000\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -923681.875000\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -948269.875000\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -908623.750000\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -937823.062500\n",
      "    epoch          : 114\n",
      "    loss           : -941006.8712871287\n",
      "    val_loss       : -940769.83671875\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -1015398.062500\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -930239.500000\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -921960.375000\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -921548.125000\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -918390.000000\n",
      "    epoch          : 115\n",
      "    loss           : -944110.7957920792\n",
      "    val_loss       : -942158.47109375\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -1015776.250000\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -927759.187500\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -947414.937500\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -934516.937500\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -938824.125000\n",
      "    epoch          : 116\n",
      "    loss           : -942259.9752475248\n",
      "    val_loss       : -941436.79140625\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -1015265.750000\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -930608.562500\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -939531.125000\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -922903.750000\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -937857.125000\n",
      "    epoch          : 117\n",
      "    loss           : -944437.667079208\n",
      "    val_loss       : -942230.78203125\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -1016053.312500\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -929548.750000\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -935913.875000\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -917586.500000\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -947164.000000\n",
      "    epoch          : 118\n",
      "    loss           : -942580.1089108911\n",
      "    val_loss       : -936912.18671875\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -1011001.625000\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -923994.687500\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -921099.187500\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -1015817.937500\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -940243.375000\n",
      "    epoch          : 119\n",
      "    loss           : -941372.104579208\n",
      "    val_loss       : -942051.61953125\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -965135.062500\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -964365.250000\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -923182.250000\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -918286.000000\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -941507.750000\n",
      "    epoch          : 120\n",
      "    loss           : -944496.1163366337\n",
      "    val_loss       : -942799.28828125\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -928504.375000\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -956987.125000\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -945991.125000\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -1017091.375000\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -917684.500000\n",
      "    epoch          : 121\n",
      "    loss           : -941773.5080445545\n",
      "    val_loss       : -941806.28046875\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -1014630.187500\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -928244.500000\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -918461.500000\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -936054.187500\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -912648.000000\n",
      "    epoch          : 122\n",
      "    loss           : -941122.8360148515\n",
      "    val_loss       : -940001.6765625\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -964077.312500\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -926593.625000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -916422.812500\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -919718.250000\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -916514.125000\n",
      "    epoch          : 123\n",
      "    loss           : -943483.3725247525\n",
      "    val_loss       : -943423.0609375\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -1017796.250000\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -929515.250000\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -954038.437500\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -939477.750000\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -947382.250000\n",
      "    epoch          : 124\n",
      "    loss           : -944792.4758663366\n",
      "    val_loss       : -941678.51171875\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -1013604.562500\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -949851.437500\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -936621.812500\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -949311.437500\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -949817.937500\n",
      "    epoch          : 125\n",
      "    loss           : -944540.031559406\n",
      "    val_loss       : -942288.059375\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -1017737.375000\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -929095.625000\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -924698.875000\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -949809.000000\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -920105.375000\n",
      "    epoch          : 126\n",
      "    loss           : -944870.1831683168\n",
      "    val_loss       : -942824.778125\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -1018935.000000\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -920531.812500\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -946634.250000\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -943682.562500\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -915055.250000\n",
      "    epoch          : 127\n",
      "    loss           : -941140.1169554455\n",
      "    val_loss       : -940183.7921875\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -1016018.250000\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -929828.687500\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -920695.125000\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -946547.000000\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -939600.250000\n",
      "    epoch          : 128\n",
      "    loss           : -942961.4938118812\n",
      "    val_loss       : -942295.05859375\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -1017494.500000\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -929141.000000\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -938658.187500\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -922467.875000\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -936321.250000\n",
      "    epoch          : 129\n",
      "    loss           : -944644.6998762377\n",
      "    val_loss       : -941789.93046875\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -1015269.375000\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -959453.625000\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -915584.625000\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -942700.937500\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -943562.875000\n",
      "    epoch          : 130\n",
      "    loss           : -942026.5798267326\n",
      "    val_loss       : -939902.3625\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -1014797.625000\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -964159.750000\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -936477.875000\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -915818.000000\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -914363.562500\n",
      "    epoch          : 131\n",
      "    loss           : -942875.9523514851\n",
      "    val_loss       : -939250.17578125\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -1018664.937500\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -904924.625000\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -916298.000000\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -924072.875000\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -946765.375000\n",
      "    epoch          : 132\n",
      "    loss           : -943519.1590346535\n",
      "    val_loss       : -941405.38125\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -1015460.750000\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -927086.000000\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -946360.375000\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -948365.750000\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -918621.812500\n",
      "    epoch          : 133\n",
      "    loss           : -943973.5928217822\n",
      "    val_loss       : -943427.7546875\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -1015986.875000\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -929194.437500\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -916893.375000\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -944839.750000\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -946373.250000\n",
      "    epoch          : 134\n",
      "    loss           : -944123.6534653465\n",
      "    val_loss       : -940476.60078125\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -1012095.437500\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -945515.500000\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -920877.750000\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -916210.000000\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -935910.875000\n",
      "    epoch          : 135\n",
      "    loss           : -942503.3632425743\n",
      "    val_loss       : -940637.53671875\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -1012264.437500\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -943673.187500\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -953093.687500\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -951897.000000\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -940119.500000\n",
      "    epoch          : 136\n",
      "    loss           : -943354.5043316832\n",
      "    val_loss       : -941857.61875\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -1016995.750000\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -949492.812500\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -950911.000000\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -917766.625000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -936495.875000\n",
      "    epoch          : 137\n",
      "    loss           : -944915.8527227723\n",
      "    val_loss       : -942609.984375\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -923903.000000\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -930701.750000\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -946637.125000\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -937945.125000\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -935156.562500\n",
      "    epoch          : 138\n",
      "    loss           : -943801.6014851485\n",
      "    val_loss       : -939653.909375\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -1016176.187500\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -926968.687500\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -934473.625000\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -937488.000000\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -940182.312500\n",
      "    epoch          : 139\n",
      "    loss           : -943198.3391089109\n",
      "    val_loss       : -942992.8859375\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -1015700.500000\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -963497.375000\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -920330.937500\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -1018482.125000\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -919071.062500\n",
      "    epoch          : 140\n",
      "    loss           : -943490.6225247525\n",
      "    val_loss       : -942194.64375\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -1015649.437500\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -949361.125000\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -948606.625000\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -946936.187500\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -947383.250000\n",
      "    epoch          : 141\n",
      "    loss           : -943673.2926980198\n",
      "    val_loss       : -938173.0765625\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -1014972.250000\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -927560.562500\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -919132.750000\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -918111.937500\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -937255.750000\n",
      "    epoch          : 142\n",
      "    loss           : -943262.5513613861\n",
      "    val_loss       : -940931.94296875\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -1015169.750000\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -927779.125000\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -918998.500000\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -947065.500000\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -949370.812500\n",
      "    epoch          : 143\n",
      "    loss           : -943430.4603960396\n",
      "    val_loss       : -942583.60703125\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -1018998.187500\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -965518.125000\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -924038.625000\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -948244.250000\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -938965.875000\n",
      "    epoch          : 144\n",
      "    loss           : -944427.7277227723\n",
      "    val_loss       : -940302.99375\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -1015020.125000\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -916303.187500\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -920641.187500\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -920073.000000\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -945159.375000\n",
      "    epoch          : 145\n",
      "    loss           : -942272.4950495049\n",
      "    val_loss       : -939398.721875\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -1015938.937500\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -960840.375000\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -950348.375000\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -938983.500000\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -936696.000000\n",
      "    epoch          : 146\n",
      "    loss           : -943768.8917079208\n",
      "    val_loss       : -941165.1421875\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -1017655.500000\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -929329.937500\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -913741.000000\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -916585.750000\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -938664.500000\n",
      "    epoch          : 147\n",
      "    loss           : -943230.2908415842\n",
      "    val_loss       : -942706.8703125\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -1017176.187500\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -922702.125000\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -917546.125000\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -936106.437500\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -921707.500000\n",
      "    epoch          : 148\n",
      "    loss           : -945011.8341584158\n",
      "    val_loss       : -943747.61796875\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -928393.875000\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -965169.437500\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -952164.500000\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -1018083.187500\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -940187.500000\n",
      "    epoch          : 149\n",
      "    loss           : -945257.5965346535\n",
      "    val_loss       : -940989.5078125\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -965935.562500\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -915608.187500\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -912254.312500\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -1015086.250000\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -938514.812500\n",
      "    epoch          : 150\n",
      "    loss           : -941256.5606435643\n",
      "    val_loss       : -941101.90078125\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0818_234119/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -1014752.937500\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -948940.937500\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -918796.625000\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -922769.250000\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -921820.250000\n",
      "    epoch          : 151\n",
      "    loss           : -945481.3446782178\n",
      "    val_loss       : -945134.90625\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -1018383.250000\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -931632.375000\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -921761.125000\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -919650.750000\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -945162.125000\n",
      "    epoch          : 152\n",
      "    loss           : -944171.8762376237\n",
      "    val_loss       : -940056.5234375\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -1014064.875000\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -917664.812500\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -923995.125000\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -918684.875000\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -936904.750000\n",
      "    epoch          : 153\n",
      "    loss           : -944691.573019802\n",
      "    val_loss       : -941833.596875\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -966378.500000\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -938080.312500\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -948558.500000\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -915180.375000\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -946363.312500\n",
      "    epoch          : 154\n",
      "    loss           : -943885.3298267326\n",
      "    val_loss       : -941557.2765625\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -1015573.125000\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -928795.312500\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -927127.125000\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -938599.812500\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -949729.750000\n",
      "    epoch          : 155\n",
      "    loss           : -945638.5346534654\n",
      "    val_loss       : -943412.42578125\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -1017222.062500\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -965448.562500\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -951875.750000\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -943026.875000\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -916405.375000\n",
      "    epoch          : 156\n",
      "    loss           : -943376.5092821782\n",
      "    val_loss       : -942709.9609375\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -921572.062500\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -914148.062500\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -936800.187500\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -916987.125000\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -939131.312500\n",
      "    epoch          : 157\n",
      "    loss           : -944504.801980198\n",
      "    val_loss       : -943249.184375\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -1017944.250000\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -964201.000000\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -922407.875000\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -947184.875000\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -940313.750000\n",
      "    epoch          : 158\n",
      "    loss           : -944915.1763613861\n",
      "    val_loss       : -943172.3828125\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -1017800.437500\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -926097.562500\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -916941.750000\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -919440.500000\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -943915.312500\n",
      "    epoch          : 159\n",
      "    loss           : -943705.3347772277\n",
      "    val_loss       : -938584.26875\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -1015690.250000\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -964197.375000\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -914120.062500\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -936417.750000\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -915799.625000\n",
      "    epoch          : 160\n",
      "    loss           : -943450.8001237623\n",
      "    val_loss       : -942801.5109375\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -1016108.875000\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -930494.000000\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -923512.500000\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -917285.937500\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -948719.875000\n",
      "    epoch          : 161\n",
      "    loss           : -944319.5724009901\n",
      "    val_loss       : -941343.903125\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -1013323.500000\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -920162.437500\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -952376.125000\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -953547.562500\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -943040.625000\n",
      "    epoch          : 162\n",
      "    loss           : -944996.1720297029\n",
      "    val_loss       : -944336.2953125\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -1018550.250000\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -931900.437500\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -922802.500000\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -945308.562500\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -945372.875000\n",
      "    epoch          : 163\n",
      "    loss           : -945224.843440594\n",
      "    val_loss       : -941491.35625\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -1017129.687500\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -966113.625000\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -916582.750000\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -1019075.750000\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -940346.125000\n",
      "    epoch          : 164\n",
      "    loss           : -945042.3904702971\n",
      "    val_loss       : -944505.1640625\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -1018310.937500\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -929263.312500\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -920239.875000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -951831.000000\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -947577.875000\n",
      "    epoch          : 165\n",
      "    loss           : -945643.6305693069\n",
      "    val_loss       : -942069.71953125\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -1017186.187500\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -929624.250000\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -945220.875000\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -917926.625000\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -938080.312500\n",
      "    epoch          : 166\n",
      "    loss           : -943462.3428217822\n",
      "    val_loss       : -942369.50703125\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -1016207.500000\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -931567.125000\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -922964.750000\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -923232.875000\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -948077.562500\n",
      "    epoch          : 167\n",
      "    loss           : -945404.146039604\n",
      "    val_loss       : -942930.4515625\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -1016011.500000\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -949734.062500\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -922069.625000\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -1015696.500000\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -947941.187500\n",
      "    epoch          : 168\n",
      "    loss           : -944485.2153465346\n",
      "    val_loss       : -944286.22734375\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -1017522.000000\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -932247.125000\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -919865.500000\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -919743.000000\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -940142.375000\n",
      "    epoch          : 169\n",
      "    loss           : -945884.364480198\n",
      "    val_loss       : -943468.575\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -966618.437500\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -952839.375000\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -928796.750000\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -915726.125000\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -939331.000000\n",
      "    epoch          : 170\n",
      "    loss           : -944801.1899752475\n",
      "    val_loss       : -941749.66796875\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -1018712.000000\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -961316.812500\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -939027.562500\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -1018150.125000\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -949253.812500\n",
      "    epoch          : 171\n",
      "    loss           : -944041.219059406\n",
      "    val_loss       : -939680.27421875\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -1016236.687500\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -964042.625000\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -917635.500000\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -949696.812500\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -948744.937500\n",
      "    epoch          : 172\n",
      "    loss           : -942658.979579208\n",
      "    val_loss       : -940395.69609375\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -1016788.000000\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -930230.250000\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -923566.375000\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -937085.500000\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -939288.937500\n",
      "    epoch          : 173\n",
      "    loss           : -944881.5129950495\n",
      "    val_loss       : -943198.68203125\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -1016738.000000\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -963763.187500\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -921122.937500\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -946976.250000\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -921491.062500\n",
      "    epoch          : 174\n",
      "    loss           : -945780.0897277228\n",
      "    val_loss       : -944024.1984375\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -1018733.500000\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -947680.875000\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -923344.562500\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -916061.625000\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -916693.375000\n",
      "    epoch          : 175\n",
      "    loss           : -944899.7568069306\n",
      "    val_loss       : -939205.04375\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -919463.625000\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -926870.812500\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -918689.750000\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -917229.375000\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -949180.875000\n",
      "    epoch          : 176\n",
      "    loss           : -945120.5971534654\n",
      "    val_loss       : -943977.02734375\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -1018447.625000\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -930410.937500\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -954510.250000\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -921603.375000\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -940905.375000\n",
      "    epoch          : 177\n",
      "    loss           : -946545.9851485149\n",
      "    val_loss       : -943617.08359375\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -1017618.562500\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -930084.562500\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -939185.500000\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -1018917.562500\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -920178.687500\n",
      "    epoch          : 178\n",
      "    loss           : -944763.0297029703\n",
      "    val_loss       : -943078.84453125\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -1017963.875000\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -918492.375000\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -922761.375000\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -1013921.125000\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -918133.062500\n",
      "    epoch          : 179\n",
      "    loss           : -944403.4121287129\n",
      "    val_loss       : -940032.84296875\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -1014359.875000\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -911911.812500\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -916151.062500\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -1019236.562500\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -943335.187500\n",
      "    epoch          : 180\n",
      "    loss           : -945300.2060643565\n",
      "    val_loss       : -944783.35546875\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -1019384.000000\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -933222.250000\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -920935.625000\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -916633.250000\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -938544.062500\n",
      "    epoch          : 181\n",
      "    loss           : -943932.9622524752\n",
      "    val_loss       : -939541.18359375\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -963470.125000\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -916998.625000\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -919693.250000\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -917517.625000\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -912654.625000\n",
      "    epoch          : 182\n",
      "    loss           : -943509.4077970297\n",
      "    val_loss       : -940895.4171875\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -1016466.000000\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -959097.125000\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -918559.625000\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -1016355.812500\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -941315.125000\n",
      "    epoch          : 183\n",
      "    loss           : -942797.0160891089\n",
      "    val_loss       : -944730.22421875\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -1018233.312500\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -963770.250000\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -919134.375000\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -951199.562500\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -948142.125000\n",
      "    epoch          : 184\n",
      "    loss           : -945896.7951732674\n",
      "    val_loss       : -943482.90859375\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -1018815.500000\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -950968.187500\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -923928.625000\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -918467.375000\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -940045.500000\n",
      "    epoch          : 185\n",
      "    loss           : -945480.8743811881\n",
      "    val_loss       : -943008.8640625\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -967311.500000\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -964201.812500\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -952948.062500\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -941991.437500\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -920296.687500\n",
      "    epoch          : 186\n",
      "    loss           : -946675.291460396\n",
      "    val_loss       : -944484.03359375\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -928600.437500\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -966596.437500\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -939305.125000\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -1015114.000000\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -912657.250000\n",
      "    epoch          : 187\n",
      "    loss           : -944140.0191831683\n",
      "    val_loss       : -940797.178125\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -1015042.312500\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -925591.000000\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -952543.000000\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -920180.000000\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -916752.125000\n",
      "    epoch          : 188\n",
      "    loss           : -945412.4628712871\n",
      "    val_loss       : -943975.871875\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -1018789.625000\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -960498.500000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -921840.062500\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -948879.625000\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -915318.250000\n",
      "    epoch          : 189\n",
      "    loss           : -944571.9405940594\n",
      "    val_loss       : -943141.43515625\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -1017194.250000\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -926166.437500\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -954054.375000\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -919715.375000\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -919309.562500\n",
      "    epoch          : 190\n",
      "    loss           : -945580.3038366337\n",
      "    val_loss       : -941884.19921875\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -964063.312500\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -946024.375000\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -938382.937500\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -946479.500000\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -948725.625000\n",
      "    epoch          : 191\n",
      "    loss           : -944792.510519802\n",
      "    val_loss       : -944061.9140625\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -1018683.250000\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -930943.687500\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -927879.750000\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -949375.250000\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -942379.000000\n",
      "    epoch          : 192\n",
      "    loss           : -947499.4331683168\n",
      "    val_loss       : -944397.7625\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -1018227.750000\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -967140.625000\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -952396.750000\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -946249.562500\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -939471.000000\n",
      "    epoch          : 193\n",
      "    loss           : -945367.1076732674\n",
      "    val_loss       : -942286.55078125\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -1015077.375000\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -926137.812500\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -915889.375000\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -952039.062500\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -917906.000000\n",
      "    epoch          : 194\n",
      "    loss           : -944737.9851485149\n",
      "    val_loss       : -943077.0578125\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -952648.125000\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -932100.687500\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -968787.500000\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -948542.187500\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -919078.625000\n",
      "    epoch          : 195\n",
      "    loss           : -946727.3353960396\n",
      "    val_loss       : -942205.4625\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -1016211.750000\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -928233.750000\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -949193.000000\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -917504.250000\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -921288.312500\n",
      "    epoch          : 196\n",
      "    loss           : -945729.4375\n",
      "    val_loss       : -942262.9203125\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -1017130.375000\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -930000.750000\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -921215.500000\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -927826.187500\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -946664.562500\n",
      "    epoch          : 197\n",
      "    loss           : -946010.0587871287\n",
      "    val_loss       : -941754.4671875\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -1015894.875000\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -961593.125000\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -945788.875000\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -931852.750000\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -947724.000000\n",
      "    epoch          : 198\n",
      "    loss           : -942747.8811881188\n",
      "    val_loss       : -943043.16015625\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -964809.500000\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -965623.375000\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -924888.500000\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -1017679.312500\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -946825.312500\n",
      "    epoch          : 199\n",
      "    loss           : -945510.104579208\n",
      "    val_loss       : -942567.19375\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -1017914.312500\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -928942.437500\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -918433.687500\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -933510.375000\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -939419.625000\n",
      "    epoch          : 200\n",
      "    loss           : -944301.9275990099\n",
      "    val_loss       : -941160.22890625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0818_234119/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -1015531.000000\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -966320.875000\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -948372.625000\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -949816.125000\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -922551.125000\n",
      "    epoch          : 201\n",
      "    loss           : -946649.5600247525\n",
      "    val_loss       : -945255.24140625\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -1018014.750000\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -965752.250000\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -924054.812500\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -922715.312500\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -942290.000000\n",
      "    epoch          : 202\n",
      "    loss           : -945864.6652227723\n",
      "    val_loss       : -943533.953125\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -1019284.125000\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -965391.812500\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -923628.000000\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -942717.937500\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -942263.812500\n",
      "    epoch          : 203\n",
      "    loss           : -947840.5878712871\n",
      "    val_loss       : -944629.4140625\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -965929.500000\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -933070.312500\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -945867.250000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -919816.812500\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -948376.812500\n",
      "    epoch          : 204\n",
      "    loss           : -945511.6305693069\n",
      "    val_loss       : -943575.35234375\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -1017177.562500\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -930266.000000\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -917555.500000\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -925323.312500\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -950796.750000\n",
      "    epoch          : 205\n",
      "    loss           : -945869.1033415842\n",
      "    val_loss       : -943918.6015625\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -1019368.187500\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -918355.625000\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -924292.625000\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -950730.875000\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -919312.625000\n",
      "    epoch          : 206\n",
      "    loss           : -945171.7351485149\n",
      "    val_loss       : -943393.6171875\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -1016676.437500\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -964923.437500\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -940928.562500\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -940903.000000\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -951140.250000\n",
      "    epoch          : 207\n",
      "    loss           : -946696.4931930694\n",
      "    val_loss       : -943594.184375\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -1017508.437500\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -930924.750000\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -925033.875000\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -1017910.000000\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -949098.500000\n",
      "    epoch          : 208\n",
      "    loss           : -945614.5668316832\n",
      "    val_loss       : -944237.240625\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -1020521.937500\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -931482.500000\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -948866.187500\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -940756.500000\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -940469.375000\n",
      "    epoch          : 209\n",
      "    loss           : -944644.2846534654\n",
      "    val_loss       : -935719.85234375\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -1014784.750000\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -927538.125000\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -939329.625000\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -927360.250000\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -921395.687500\n",
      "    epoch          : 210\n",
      "    loss           : -945165.5136138614\n",
      "    val_loss       : -944681.90390625\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -1020307.000000\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -927708.312500\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -944969.812500\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -1016752.562500\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -948606.562500\n",
      "    epoch          : 211\n",
      "    loss           : -945110.1571782178\n",
      "    val_loss       : -940680.196875\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -1014226.500000\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -928267.000000\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -938749.000000\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -950111.375000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -949772.500000\n",
      "    epoch          : 212\n",
      "    loss           : -945336.4672029703\n",
      "    val_loss       : -942791.228125\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -1018534.437500\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -928426.125000\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -919097.875000\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -1018672.187500\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -918341.187500\n",
      "    epoch          : 213\n",
      "    loss           : -945597.3521039604\n",
      "    val_loss       : -943611.19296875\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -1017855.000000\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -930900.437500\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -922103.312500\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -922771.312500\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -948414.750000\n",
      "    epoch          : 214\n",
      "    loss           : -945971.1720297029\n",
      "    val_loss       : -942726.77734375\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -1015943.000000\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -930527.562500\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -952828.312500\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -1017666.750000\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -949729.750000\n",
      "    epoch          : 215\n",
      "    loss           : -945502.1633663366\n",
      "    val_loss       : -943060.31015625\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -965445.375000\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -930952.125000\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -927515.250000\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -941986.250000\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -950599.375000\n",
      "    epoch          : 216\n",
      "    loss           : -947083.083539604\n",
      "    val_loss       : -943913.46875\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -1017821.875000\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -952166.312500\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -920192.625000\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -947089.125000\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -942106.937500\n",
      "    epoch          : 217\n",
      "    loss           : -945379.4449257426\n",
      "    val_loss       : -944653.66328125\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -966939.875000\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -931300.687500\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -951082.187500\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -949970.687500\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -917379.625000\n",
      "    epoch          : 218\n",
      "    loss           : -945157.6887376237\n",
      "    val_loss       : -941315.72421875\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -1016115.937500\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -965478.500000\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -953023.625000\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -941126.500000\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -920345.812500\n",
      "    epoch          : 219\n",
      "    loss           : -946002.7586633663\n",
      "    val_loss       : -945191.3828125\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -1020014.250000\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -965498.625000\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -947913.000000\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -919433.562500\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -919903.437500\n",
      "    epoch          : 220\n",
      "    loss           : -946541.0464108911\n",
      "    val_loss       : -941732.490625\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -1015844.000000\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -965319.375000\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -950900.312500\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -921294.562500\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -940033.812500\n",
      "    epoch          : 221\n",
      "    loss           : -945809.6899752475\n",
      "    val_loss       : -943940.03515625\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -1020284.625000\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -934543.375000\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -949747.437500\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -920162.625000\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -919739.875000\n",
      "    epoch          : 222\n",
      "    loss           : -947684.6311881188\n",
      "    val_loss       : -944799.715625\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -966081.750000\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -930392.625000\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -920478.625000\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -939982.937500\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -918751.250000\n",
      "    epoch          : 223\n",
      "    loss           : -946572.0142326732\n",
      "    val_loss       : -943039.22265625\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -964962.562500\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -953499.687500\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -952772.125000\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -927969.062500\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -941244.125000\n",
      "    epoch          : 224\n",
      "    loss           : -946133.760519802\n",
      "    val_loss       : -942636.75546875\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -1015366.250000\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -926959.062500\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -952674.250000\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -1016798.875000\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -919431.562500\n",
      "    epoch          : 225\n",
      "    loss           : -944623.5779702971\n",
      "    val_loss       : -945409.30703125\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -1019012.562500\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -933601.250000\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -951094.125000\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -954314.500000\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -943437.250000\n",
      "    epoch          : 226\n",
      "    loss           : -948317.9282178218\n",
      "    val_loss       : -945830.04765625\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -1019822.000000\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -932497.687500\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -918503.625000\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -920152.937500\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -919612.000000\n",
      "    epoch          : 227\n",
      "    loss           : -946914.468440594\n",
      "    val_loss       : -942318.79296875\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -1019246.500000\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -925576.250000\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -921048.500000\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -1018056.187500\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -921903.625000\n",
      "    epoch          : 228\n",
      "    loss           : -946760.0853960396\n",
      "    val_loss       : -944536.10234375\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -1018669.812500\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -929866.125000\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -914762.250000\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -920079.812500\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -950325.875000\n",
      "    epoch          : 229\n",
      "    loss           : -945208.3502475248\n",
      "    val_loss       : -944744.3078125\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -1020340.875000\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -920595.687500\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -923645.062500\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -923287.687500\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -922866.375000\n",
      "    epoch          : 230\n",
      "    loss           : -948494.5618811881\n",
      "    val_loss       : -945734.98359375\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -1018188.125000\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -933757.000000\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -922237.937500\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -947766.750000\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -916013.750000\n",
      "    epoch          : 231\n",
      "    loss           : -946720.146039604\n",
      "    val_loss       : -942479.6984375\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -952446.062500\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -964654.125000\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -926222.562500\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -939403.250000\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -949853.062500\n",
      "    epoch          : 232\n",
      "    loss           : -945701.3514851485\n",
      "    val_loss       : -943083.9125\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -1018537.375000\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -965625.625000\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -920959.750000\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -943635.750000\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -950528.937500\n",
      "    epoch          : 233\n",
      "    loss           : -946603.1608910891\n",
      "    val_loss       : -944526.00234375\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -1019977.375000\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -931355.375000\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -918505.937500\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -919985.562500\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -942334.250000\n",
      "    epoch          : 234\n",
      "    loss           : -946879.9034653465\n",
      "    val_loss       : -944306.97578125\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -1017795.375000\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -963895.000000\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -947804.375000\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -936760.500000\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -947947.937500\n",
      "    epoch          : 235\n",
      "    loss           : -943851.1194306931\n",
      "    val_loss       : -941457.909375\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -966431.312500\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -925858.625000\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -949492.500000\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -920448.437500\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -950823.562500\n",
      "    epoch          : 236\n",
      "    loss           : -945411.6720297029\n",
      "    val_loss       : -944377.6125\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -1016762.000000\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -932545.437500\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -955304.625000\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -922246.875000\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -920682.187500\n",
      "    epoch          : 237\n",
      "    loss           : -947544.5018564357\n",
      "    val_loss       : -945304.56640625\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -1020866.562500\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -965254.187500\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -910877.062500\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -925925.750000\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -946224.812500\n",
      "    epoch          : 238\n",
      "    loss           : -944067.094059406\n",
      "    val_loss       : -942728.20625\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -1018494.125000\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -921957.625000\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -919692.687500\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -953642.625000\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -949889.062500\n",
      "    epoch          : 239\n",
      "    loss           : -946412.9913366337\n",
      "    val_loss       : -943173.47890625\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -1018642.250000\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -916481.250000\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -916355.875000\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -1019368.437500\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -951660.125000\n",
      "    epoch          : 240\n",
      "    loss           : -946049.7896039604\n",
      "    val_loss       : -945561.8875\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -1019565.812500\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -933500.125000\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -927832.875000\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -1019340.562500\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -921041.687500\n",
      "    epoch          : 241\n",
      "    loss           : -947499.6639851485\n",
      "    val_loss       : -944426.528125\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -1017753.750000\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -966270.562500\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -923707.375000\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -1014769.125000\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -947534.687500\n",
      "    epoch          : 242\n",
      "    loss           : -944011.020420792\n",
      "    val_loss       : -938017.7375\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -1015489.812500\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -961169.187500\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -947773.062500\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -942033.500000\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -940840.312500\n",
      "    epoch          : 243\n",
      "    loss           : -944454.4647277228\n",
      "    val_loss       : -943751.08671875\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -1019384.125000\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -931960.687500\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -948153.625000\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -954209.750000\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -915762.375000\n",
      "    epoch          : 244\n",
      "    loss           : -946153.6887376237\n",
      "    val_loss       : -943385.71640625\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -966190.062500\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -946971.812500\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -947768.000000\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -948255.937500\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -918945.812500\n",
      "    epoch          : 245\n",
      "    loss           : -945531.1868811881\n",
      "    val_loss       : -944363.946875\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -953310.062500\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -950469.000000\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -955913.562500\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -950141.500000\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -939942.062500\n",
      "    epoch          : 246\n",
      "    loss           : -947129.8570544554\n",
      "    val_loss       : -944538.60625\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -1018118.562500\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -931702.125000\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -940258.500000\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -949222.375000\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -952418.062500\n",
      "    epoch          : 247\n",
      "    loss           : -947181.8100247525\n",
      "    val_loss       : -943272.05859375\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -1017356.250000\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -926666.437500\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -921454.875000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -951231.000000\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -918813.000000\n",
      "    epoch          : 248\n",
      "    loss           : -946506.4826732674\n",
      "    val_loss       : -939850.48671875\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -1017666.187500\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -932164.000000\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -919082.500000\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -920512.875000\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -949655.875000\n",
      "    epoch          : 249\n",
      "    loss           : -945720.1342821782\n",
      "    val_loss       : -944943.6609375\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -1019431.562500\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -931814.875000\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -954068.500000\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -940963.500000\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -950781.125000\n",
      "    epoch          : 250\n",
      "    loss           : -947914.8490099009\n",
      "    val_loss       : -944922.18046875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0818_234119/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -1018379.250000\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -954132.375000\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -949159.562500\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -916134.375000\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -949172.125000\n",
      "    epoch          : 251\n",
      "    loss           : -945380.7271039604\n",
      "    val_loss       : -944328.146875\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -1019713.687500\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -924041.062500\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -923617.687500\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -954804.500000\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -951860.437500\n",
      "    epoch          : 252\n",
      "    loss           : -948692.9665841584\n",
      "    val_loss       : -946629.28125\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -1020025.375000\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -932452.875000\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -929297.312500\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -923486.000000\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -938922.750000\n",
      "    epoch          : 253\n",
      "    loss           : -947759.0383663366\n",
      "    val_loss       : -942195.7609375\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -1020054.187500\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -948252.750000\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -948617.562500\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -913793.250000\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -945519.125000\n",
      "    epoch          : 254\n",
      "    loss           : -944379.2178217822\n",
      "    val_loss       : -943391.1703125\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -1018994.250000\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -964404.437500\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -921622.875000\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -1020124.937500\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -949039.250000\n",
      "    epoch          : 255\n",
      "    loss           : -945645.6441831683\n",
      "    val_loss       : -942051.734375\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -1019344.062500\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -964178.250000\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -919703.187500\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -952803.937500\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -951660.312500\n",
      "    epoch          : 256\n",
      "    loss           : -946959.978960396\n",
      "    val_loss       : -946441.10859375\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -1019660.375000\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -932837.312500\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -920740.437500\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -948401.125000\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -946619.000000\n",
      "    epoch          : 257\n",
      "    loss           : -946385.5971534654\n",
      "    val_loss       : -943078.46171875\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -1015577.125000\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -968672.375000\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -928090.250000\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -1018636.000000\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -951622.625000\n",
      "    epoch          : 258\n",
      "    loss           : -947856.2827970297\n",
      "    val_loss       : -944517.321875\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -1020015.937500\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -967149.125000\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -917481.000000\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -918022.750000\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -918861.500000\n",
      "    epoch          : 259\n",
      "    loss           : -946519.6738861386\n",
      "    val_loss       : -943616.1671875\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -1018622.312500\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -916579.750000\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -941457.875000\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -1017746.000000\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -951985.125000\n",
      "    epoch          : 260\n",
      "    loss           : -946824.2425742574\n",
      "    val_loss       : -943347.90703125\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -1016638.375000\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -918539.500000\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -940126.625000\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -940791.625000\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -947947.750000\n",
      "    epoch          : 261\n",
      "    loss           : -945205.301980198\n",
      "    val_loss       : -942165.06484375\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -1017657.437500\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -933123.750000\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -925062.187500\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -920679.937500\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -941808.000000\n",
      "    epoch          : 262\n",
      "    loss           : -946380.7042079208\n",
      "    val_loss       : -943658.3515625\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -1019841.250000\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -921577.937500\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -918785.250000\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -947779.000000\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -918964.312500\n",
      "    epoch          : 263\n",
      "    loss           : -946263.7271039604\n",
      "    val_loss       : -945743.5953125\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -1019102.500000\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -920917.000000\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -926146.375000\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -952594.625000\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -952794.000000\n",
      "    epoch          : 264\n",
      "    loss           : -948422.1720297029\n",
      "    val_loss       : -946089.434375\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -1018651.000000\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -931616.437500\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -927951.812500\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -919397.625000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -921510.937500\n",
      "    epoch          : 265\n",
      "    loss           : -947685.0377475248\n",
      "    val_loss       : -945235.03359375\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -1020380.937500\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -933820.125000\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -925956.250000\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -949262.062500\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -954467.875000\n",
      "    epoch          : 266\n",
      "    loss           : -948149.5996287129\n",
      "    val_loss       : -945723.5671875\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -1019838.125000\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -929344.812500\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -952223.375000\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -917361.125000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -918557.500000\n",
      "    epoch          : 267\n",
      "    loss           : -945499.4851485149\n",
      "    val_loss       : -942317.92578125\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -1017947.875000\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -930372.375000\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -955581.812500\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -922751.500000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -951521.625000\n",
      "    epoch          : 268\n",
      "    loss           : -947939.3304455446\n",
      "    val_loss       : -945853.61640625\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -969601.000000\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -967623.312500\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -951174.625000\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -918378.375000\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -920273.062500\n",
      "    epoch          : 269\n",
      "    loss           : -947639.0754950495\n",
      "    val_loss       : -945231.56484375\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -1020319.750000\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -966244.125000\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -942748.375000\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -942990.000000\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -942613.875000\n",
      "    epoch          : 270\n",
      "    loss           : -948513.1819306931\n",
      "    val_loss       : -946978.1046875\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -1021191.750000\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -926325.875000\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -925653.000000\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -940153.875000\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -950284.375000\n",
      "    epoch          : 271\n",
      "    loss           : -947189.125\n",
      "    val_loss       : -944503.1421875\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -1018842.750000\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -950080.000000\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -951384.937500\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -936248.750000\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -945687.250000\n",
      "    epoch          : 272\n",
      "    loss           : -944690.6751237623\n",
      "    val_loss       : -942875.19609375\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -1018779.875000\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -944980.062500\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -919606.500000\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -938213.500000\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -948868.625000\n",
      "    epoch          : 273\n",
      "    loss           : -944138.1441831683\n",
      "    val_loss       : -941292.521875\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -1016900.562500\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -966350.437500\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -920948.812500\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -921872.375000\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -943167.875000\n",
      "    epoch          : 274\n",
      "    loss           : -947738.0686881188\n",
      "    val_loss       : -946969.671875\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -968139.625000\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -925435.062500\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -920616.125000\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -922804.125000\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -921184.750000\n",
      "    epoch          : 275\n",
      "    loss           : -948231.478960396\n",
      "    val_loss       : -945473.68203125\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -1019019.125000\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -948245.125000\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -917638.875000\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -919836.125000\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -920643.375000\n",
      "    epoch          : 276\n",
      "    loss           : -946099.0340346535\n",
      "    val_loss       : -942839.2140625\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -1017206.750000\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -920046.125000\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -919591.312500\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -1018996.375000\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -939651.625000\n",
      "    epoch          : 277\n",
      "    loss           : -946679.4121287129\n",
      "    val_loss       : -943323.83203125\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -1019772.687500\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -966522.750000\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -949363.750000\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -939714.437500\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -952612.562500\n",
      "    epoch          : 278\n",
      "    loss           : -947009.8118811881\n",
      "    val_loss       : -943460.26953125\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -1018001.000000\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -953780.312500\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -922960.312500\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -918911.000000\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -922130.625000\n",
      "    epoch          : 279\n",
      "    loss           : -946980.4548267326\n",
      "    val_loss       : -945404.159375\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -967858.312500\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -932863.250000\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -924107.625000\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -949443.062500\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -920453.187500\n",
      "    epoch          : 280\n",
      "    loss           : -946564.4399752475\n",
      "    val_loss       : -943420.1375\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -1019819.062500\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -953103.625000\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -925407.625000\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -922705.312500\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -943558.000000\n",
      "    epoch          : 281\n",
      "    loss           : -947418.5284653465\n",
      "    val_loss       : -945104.79296875\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -1018236.750000\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -930625.375000\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -922613.812500\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -1019515.500000\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -948745.625000\n",
      "    epoch          : 282\n",
      "    loss           : -946912.6410891089\n",
      "    val_loss       : -940012.14921875\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -1018772.562500\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -931112.500000\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -940955.312500\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -923422.750000\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -920898.500000\n",
      "    epoch          : 283\n",
      "    loss           : -947562.1844059406\n",
      "    val_loss       : -946882.67890625\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -1019600.875000\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -951925.312500\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -922525.875000\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -940554.500000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -940098.687500\n",
      "    epoch          : 284\n",
      "    loss           : -947784.0142326732\n",
      "    val_loss       : -940233.6828125\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -921312.250000\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -922307.437500\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -915400.125000\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -949843.562500\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -950275.187500\n",
      "    epoch          : 285\n",
      "    loss           : -943757.6905940594\n",
      "    val_loss       : -943788.24609375\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -966400.625000\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -930525.625000\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -920370.625000\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -921814.875000\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -938309.312500\n",
      "    epoch          : 286\n",
      "    loss           : -944243.8094059406\n",
      "    val_loss       : -942187.15390625\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -1016045.250000\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -924379.500000\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -918200.000000\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -922795.812500\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -923425.375000\n",
      "    epoch          : 287\n",
      "    loss           : -946819.3094059406\n",
      "    val_loss       : -945711.5296875\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -1019927.187500\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -926760.187500\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -942107.937500\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -1019342.750000\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -942218.625000\n",
      "    epoch          : 288\n",
      "    loss           : -948120.6732673268\n",
      "    val_loss       : -945673.8421875\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -1019856.687500\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -927057.250000\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -941194.250000\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -950653.125000\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -921563.687500\n",
      "    epoch          : 289\n",
      "    loss           : -947715.4801980198\n",
      "    val_loss       : -943648.43203125\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -1016434.125000\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -966599.187500\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -954903.312500\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -1018869.875000\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -942559.250000\n",
      "    epoch          : 290\n",
      "    loss           : -947711.8830445545\n",
      "    val_loss       : -946187.7515625\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -970119.437500\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -940637.625000\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -927132.000000\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -1018867.875000\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -922080.375000\n",
      "    epoch          : 291\n",
      "    loss           : -947285.7240099009\n",
      "    val_loss       : -943664.34296875\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -1016950.062500\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -963838.500000\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -951842.375000\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -921986.312500\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -920728.437500\n",
      "    epoch          : 292\n",
      "    loss           : -946561.6120049505\n",
      "    val_loss       : -944268.03125\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -1017634.375000\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -939807.750000\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -924191.000000\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -1018501.250000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -952185.500000\n",
      "    epoch          : 293\n",
      "    loss           : -947893.1200495049\n",
      "    val_loss       : -946406.03515625\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -1020251.125000\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -934574.750000\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -953160.875000\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -943642.875000\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -937854.500000\n",
      "    epoch          : 294\n",
      "    loss           : -947184.8001237623\n",
      "    val_loss       : -937747.72890625\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -1018477.187500\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -963822.500000\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -940887.875000\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -949175.250000\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -919093.750000\n",
      "    epoch          : 295\n",
      "    loss           : -945298.9913366337\n",
      "    val_loss       : -944452.48515625\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -1018745.000000\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -968754.562500\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -922061.375000\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -1019871.875000\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -954174.625000\n",
      "    epoch          : 296\n",
      "    loss           : -949093.5748762377\n",
      "    val_loss       : -944670.79296875\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -1016989.750000\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -931944.875000\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -921746.875000\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -951598.937500\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -950435.687500\n",
      "    epoch          : 297\n",
      "    loss           : -948186.0278465346\n",
      "    val_loss       : -945574.440625\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -1019618.437500\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -965427.375000\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -920549.750000\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -921422.875000\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -944199.250000\n",
      "    epoch          : 298\n",
      "    loss           : -946857.551980198\n",
      "    val_loss       : -945934.7203125\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -968768.625000\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -929779.625000\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -947454.937500\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -1016307.437500\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -939277.125000\n",
      "    epoch          : 299\n",
      "    loss           : -945251.2865099009\n",
      "    val_loss       : -943972.04609375\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -1017807.125000\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -930944.750000\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -937957.062500\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -918779.250000\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -917939.687500\n",
      "    epoch          : 300\n",
      "    loss           : -945659.0550742574\n",
      "    val_loss       : -944009.60546875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0818_234119/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -1018594.875000\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -930207.375000\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -941299.375000\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -918468.750000\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -922349.062500\n",
      "    epoch          : 301\n",
      "    loss           : -947429.9950495049\n",
      "    val_loss       : -945255.79765625\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -930427.937500\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -933600.125000\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -920788.750000\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -954854.625000\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -954163.750000\n",
      "    epoch          : 302\n",
      "    loss           : -948441.4956683168\n",
      "    val_loss       : -946565.2703125\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -932508.375000\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -965946.125000\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -950266.937500\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -919279.750000\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -938002.000000\n",
      "    epoch          : 303\n",
      "    loss           : -945262.9418316832\n",
      "    val_loss       : -942796.865625\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -1019703.125000\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -934315.312500\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -928811.687500\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -952553.687500\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -951441.437500\n",
      "    epoch          : 304\n",
      "    loss           : -946944.1633663366\n",
      "    val_loss       : -944639.35625\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -1019894.375000\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -965222.750000\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -919129.625000\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -922635.875000\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -943248.375000\n",
      "    epoch          : 305\n",
      "    loss           : -947498.1027227723\n",
      "    val_loss       : -946486.29609375\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -1020867.437500\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -932704.875000\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -952786.562500\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -949883.562500\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -940615.937500\n",
      "    epoch          : 306\n",
      "    loss           : -948350.875\n",
      "    val_loss       : -944084.2828125\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -1016315.062500\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -932215.437500\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -935021.812500\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -943194.937500\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -923987.625000\n",
      "    epoch          : 307\n",
      "    loss           : -948723.9882425743\n",
      "    val_loss       : -946249.84296875\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -1020244.812500\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -951567.125000\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -925178.625000\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -918945.625000\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -948934.812500\n",
      "    epoch          : 308\n",
      "    loss           : -946424.667079208\n",
      "    val_loss       : -945866.18125\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -1018639.750000\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -931464.500000\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -921152.687500\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -920763.500000\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -941213.375000\n",
      "    epoch          : 309\n",
      "    loss           : -947934.9777227723\n",
      "    val_loss       : -945189.59453125\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -1020709.750000\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -932803.062500\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -942745.125000\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -922643.937500\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -923274.000000\n",
      "    epoch          : 310\n",
      "    loss           : -948388.1714108911\n",
      "    val_loss       : -946495.2734375\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -1020256.062500\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -967781.250000\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -953596.125000\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -952978.437500\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -950541.687500\n",
      "    epoch          : 311\n",
      "    loss           : -948040.0068069306\n",
      "    val_loss       : -943351.26171875\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -1017334.625000\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -962386.812500\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -954112.250000\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -916315.000000\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -917532.625000\n",
      "    epoch          : 312\n",
      "    loss           : -945929.6076732674\n",
      "    val_loss       : -944926.40859375\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -1020916.187500\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -932297.562500\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -944297.062500\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -942091.000000\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -921048.875000\n",
      "    epoch          : 313\n",
      "    loss           : -947586.4535891089\n",
      "    val_loss       : -943768.4703125\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -1019679.937500\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -929828.500000\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -951614.125000\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -918192.500000\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -949556.937500\n",
      "    epoch          : 314\n",
      "    loss           : -946814.093440594\n",
      "    val_loss       : -945248.1375\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -1019330.562500\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -931599.375000\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -920237.875000\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -1018818.062500\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -941833.812500\n",
      "    epoch          : 315\n",
      "    loss           : -947485.0167079208\n",
      "    val_loss       : -945823.546875\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -1020376.687500\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -968909.500000\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -952636.000000\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -921153.375000\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -949068.875000\n",
      "    epoch          : 316\n",
      "    loss           : -948528.3830445545\n",
      "    val_loss       : -944959.303125\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -1019674.562500\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -931210.500000\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -922437.125000\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -1018502.125000\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -921852.250000\n",
      "    epoch          : 317\n",
      "    loss           : -947133.2283415842\n",
      "    val_loss       : -944032.43359375\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -1019757.625000\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -931069.125000\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -927045.375000\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -921021.000000\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -949839.250000\n",
      "    epoch          : 318\n",
      "    loss           : -946686.3533415842\n",
      "    val_loss       : -944243.6703125\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -1020157.937500\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -966130.562500\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -941893.812500\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -1019465.000000\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -922670.562500\n",
      "    epoch          : 319\n",
      "    loss           : -947940.8731435643\n",
      "    val_loss       : -946426.86015625\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -1020341.625000\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -948428.250000\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -948613.437500\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -914983.687500\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -942837.312500\n",
      "    epoch          : 320\n",
      "    loss           : -947377.6992574257\n",
      "    val_loss       : -945360.27265625\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -1022088.000000\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -968620.750000\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -924510.312500\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -952320.375000\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -950671.000000\n",
      "    epoch          : 321\n",
      "    loss           : -948105.9418316832\n",
      "    val_loss       : -944530.3953125\n",
      "Validation performance didn't improve for 50 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAECategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_0_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_1): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_2_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_3): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_4_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_5): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_6_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_7): LadderDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (noise_linear): Linear(in_features=2744, out_features=4, bias=True)\n",
       "      (ladder_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (ladder_linear): Linear(in_features=2744, out_features=128, bias=True)\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_8_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_9): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_10): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_10_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_11): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_11_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_12): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_12_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_13): LadderDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_13_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (noise_linear): Linear(in_features=2744, out_features=4, bias=True)\n",
       "      (ladder_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (ladder_linear): Linear(in_features=2744, out_features=256, bias=True)\n",
       "    )\n",
       "    (generator_14): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_14_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_15): LadderDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_15_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=4, bias=True)\n",
       "      )\n",
       "      (ladder_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_16): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_16_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_17): LadderDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_17_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (noise_linear): Linear(in_features=2744, out_features=4, bias=True)\n",
       "      (ladder_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (ladder_linear): Linear(in_features=2744, out_features=512, bias=True)\n",
       "    )\n",
       "    (generator_18): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_18_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_19): LadderDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (noise_layer): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (generator_19_dagger): LadderEncoder(\n",
       "      (ladder_distribution): DiagonalGaussian()\n",
       "      (noise_distribution): DiagonalGaussian()\n",
       "      (noise_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (noise_linear): Linear(in_features=2744, out_features=4, bias=True)\n",
       "      (ladder_convs): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (ladder_linear): Linear(in_features=2744, out_features=1024, bias=True)\n",
       "    )\n",
       "    (generator_20): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_20_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=64, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_21): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_21_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=128, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_22): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_22_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_23): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    (generator_23_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=512, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_24): LadderPrior(\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=784, bias=True)\n",
       "        (1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (4): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_24_dagger): LadderPosterior(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (noise_dense): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (4): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): PReLU(num_parameters=1)\n",
       "        (9): Linear(in_features=784, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "    (global_element_5): StandardNormal()\n",
       "  )\n",
       "  (guide_embedding): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_distances): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=35, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKsAAAEuCAYAAADx4uLGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAT8ElEQVR4nO3df0xV9/3H8ddHLvNQWCotVEpLf7hWdG20jnmdTgKpscMbN8PFzlINaFsTTNys6x8dzHXULV3r3Gqatql/NDEbaLHYiSvB3DAr7QzS1hB7cVQ3nSmRFkw28Bru5fLj/f2D9n698kO43B+88fVIblLO+Zx73jc8PT2gXIyIgEiDGbEegGi8GCupwVhJDcZKajBWUoOxkhqMldRgrKQGYyU1GCupwVhJDcZKajBWUoOxkhqMldRgrKQGYyU1GCupwVhJDVuoByYkJHzl8/lmh3OYaLIsq8Pr9abFeg4aPxPqDwwaY0TzDxsaYyAiJtZz0PjxNoDUYKykBmMlNRgrqcFYSQ3GSmowVlKDsZIajJXUYKykBmMlNRgrqcFYSY0pG+vFixeRmpqK3Nxc5Obm4vLly+ju7obdbkdSUhJaWloCaxsbG7F06VLk5OSgsLAQfX19MZycImXKxgoAOTk5OH78OI4fP47U1FTccsstqK2txdq1a4PW3XvvvTh27BgaGhowZ84c1NTUxGhiiqSoxVpaWoo9e/ZM6JgTJ04gOzsbZWVlEBHEx8cjNTV12Lr09HQkJCQAAGw2G2bMGHpZdrsdZ86cmfzwNDWISEiPoUPHp7OzU9LT06Wnpyew7cUXX5TExMSgx8yZMwWAvPPOO+Lz+eTq1asyODgoTz/9tFRXVweOLS4uFrfbPew8Fy5ckMWLF0tvb6+IiFRVVYnT6Rxxpq/nD/n18xH9R1Ri3bVrlzzzzDNjrvF4PLJkyRJxOBzi9/uD9tXW1sqvf/3rwMcjxdrd3S3Z2dny+eefB7Z5vV5JTk6W9vb2YedjrPoeYbsNqKysxLJly7Bu3TqkpaUhIyMDdXV1AIC6ujrk5OSMeqzX68Xq1auRmJiIQ4cOIT4+HleuXAns/+ijj/DAAw+Menx/fz8KCwtRXl6OzMzMwHbLspCVlQWXyxWGV0ixFrZY3W43mpubUVBQgLa2Nmzbtg0lJSWBfddGdC2/3w+n0wm/34+amhpYlgUAaGhoQFZWFrKzs3Hp0iU8+eSTAACHwwGXy4XNmzdj3759AIADBw6gqakJO3fuRG5uLqqqqgLPP3/+fJw+fTpcL5NiKdRLMq67DXA4HFJaWhr4uKOjQwCI1+sVm80mra2tcr2+vj7Jz8+XRYsWSVdX17D94VBWViabNm0ath28DVD3COuV9dpvKXV2diIpKQmWZSE5ORkejydo/eDgIDZu3IizZ8/C5XLh1ltvDdcoQTweD2bNmhWR56boCkusXV1daGtrC/q2UnV1NVatWgUAWLBgAc6dOxd0zJYtW3Dy5EnU19cjJSUlHGOMqLW1FQsXLozY81MUhXpJxjW3AR9++KHExcXJyy+/LH19ffL+++9LamqqnDlzRkRE/vjHP8rmzZsD67dv3y4ZGRly8eJFiSSfzyfJycly6dKlYfvA2wB1j7DE+sYbb0hRUZGsWbNGkpKSJCsrS06cOBHYf/nyZbnrrrukp6dH3G63AJD4+Phh32e94447ZGBgQMLl4MGDkp+fP+I+xqrvEZZ3ZNmyZQvmzp2L7du3j7q+rKwMd9xxB5599tmQzheKJUuW4O2338bDDz88bB/fkUWfsMS6fPly7NixA3l5eeGcLaIYqz5h+QKrpaUF8+bNC8dTEY2Kb8xGakzpfyJIdC3GSmowVlKDsZIajJXUYKykBmMlNRgrqcFYSQ3GSmowVlKDsZIajJXUYKykBmMlNRgrqRHyr3C3LKvDGKP6V7jHegaamJB/UiCSjDEbAOSJyIZYz0JTB28DSA3GSmowVlKDsZIajJXUYKykBmMlNRgrqcFYSY1pEasxZqsx5lNjTK8xZl+s56HICPnfBkwx7QB+B+BHABJiPAtFyLSIVUTeAwBjzPcB3B3jcShCpsVtAN0cGCupwVhJDcZKakyLL7CMMTYMvZY4AHHGGAtAv4j0x3YyCqfpcmXdAcAL4JcANnz93ztiOhGF3bS4sopIOYDyGI9BETZdrqx0E2CspAZjJTUYK6nBWEkNxkpqMFZSg7GSGoyV1GCspAZjJTUYK6nBWEkNxkpqMFZSg7GSGoyV1GCspAZjJTUYK6nBWEkNxkpqMFZSg7GSGoyV1GCspAZjJTWmxXtd3YwSEhK+8vl8s2M9Rygsy+rwer1pEz2OsSrl8/lmi0isxwiJMSakP2S8DSA1GCupwVhJDcZKajBWUoOxkhqMldRgrKQGYyU1GCupwVhJDcZKajBWUoOx0rg1NjZi6dKlyMnJQWFhIfr6+qJ6fsZK43bvvffi2LFjaGhowJw5c1BTUxPV8zPWm1hpaSn27Nkz7vXp6elISEgAANhsNsyY8f/52O12nDlzJuwzBhGRKffA0K9hr4j1HFP5MfSpC11nZ6ekp6dLT09PYNuLL74oiYmJQY+ZM2cKAHnnnXcC6y5cuCCLFy+W3t7ewLaqqipxOp3jOvfXs0/8NYdyUKQfjDXyse7atUueeeaZMdd4PB5ZsmSJOBwO8fv9IiLS3d0t2dnZ8vnnnwet9Xq9kpycLO3t7Tc8d6ix8jZgGqusrMSyZcuwbt06pKWlISMjA3V1dQCAuro65OTkjHqs1+vF6tWrkZiYiEOHDiE+Ph79/f0oLCxEeXk5MjMzg9ZbloWsrCy4XK6IvR7GOo253W40NzejoKAAbW1t2LZtG0pKSgL7rg/uG36/H06nE36/HzU1NbAsCwBw4MABNDU1YefOncjNzUVVVVXQcfPnz8fp06cj94JCuRxH+gHeBoTlNsDhcEhpaWng446ODgEgXq9XbDabtLa2Djumr69P8vPzZdGiRdLV1XXDc1yrrKxMNm3adMN14G0AXc/tdmPt2rWBjzs7O5GUlATLspCcnAyPxxO0fnBwEBs3bsTZs2fhcrlw6623Tuh8Ho8Hs2bNCsvsI2Gs01RXVxfa2tqQmpoa2FZdXY1Vq1YBABYsWIBz584FHbNlyxacPHkS9fX1SElJmfA5W1tbsXDhwskNPgbGOk253W7ExcVh//796O/vR21tLd58802Ul5cDABwOBxoaGgLrf/GLX6Curg5///vfceedd074fL29vTh16hRWrlwZrpcwDN/kYppyu91Yv349GhsbkZycjMzMTBw+fBjf/e53AQBFRUV45JFH4PV6cf78ebz66quIj4/HQw89FPQ8iYmJ+PLLL4P+AmAkR44cQW5uLtLT0yP2mhjrNOV2u/HII49g+/btI+5PSUlBUVER9u7di2efffabL9pCtnv3brz99tuTeo4bYazTlNvtxpo1a8Zc89JLL4XtfE1NTWF7rtHwnnWaamlpwbx582I9RljxyjpNdXV1xXqEsOOVldRgrKQGYyU1GCupwVhJDcZKajBWUoOxkhqMldRgrKQGYyU1GCupwVhJDcZKajBWUoOxkhqMldTgTwooZVlWR6i/Cj3WLMvqCOU4xqqU1+tNi8TzGmMyARwRkZHfCCuGeBtAajBWUoOxkhqMldRgrKQGYyU1GCupwVhJDcZKajBWmhRjzFZjzKfGmF5jzL5Inot/3UqT1Q7gdwB+BCAhkidirDQpIvIeABhjvg/g7kiei7cBpAZjJTUYK6nBWEkNfoFFk2KMsWGoozgAccYYC0C/iPSH+1y8stJk7QDgBfBLDP2CaO/X28KOV1aaFBEpB1AejXPxykpqMFZSg7GSGoyV1GCspAZjJTUYK6nBWEkNxkpqMFZSg7GSGoyV1GCspAZjJTUYK6nBWEkNxkpqMFZSg7GSGoyV1GCspAZjJTUYK6nBWEkNxkpqMFZSw4hISAcmJCR85fP5VP4KcYoty7I6Qvmt3iHHaoyRUI+lm5sxBiJiJnocbwNIDcZKajBWUoOxkhqMldRgrKQGYyU1GCupwVhJDcZKajBWUoOxkhqMldSYFrH+4x//QG5uLnJzczF37lxs374dg4OD2LhxI7Kzs7F8+XK0trYGHXPgwAGkpqZGbcaLFy8iNTU1MOfly5fR3d0Nu92OpKQktLS0AAAaGxuxdOlS5OTkoLCwEH19fVGZL1bnnRARCekxdOjUU1xcLMePH5dTp07JE088ISIiH374oWzevDmwZmBgQJxOpyxatChqc/3nP/+RgoKCoG1+v186OzuluLhY3G63iIhcunRJenp6RESkrKxM3n333ajMF83zft3OhJubklfW0tJS7NmzZ8LH9fX14eOPP0Z2djbuvvtuxMXFQUTwv//9DykpKYF1+/fvx9q1azFjRugvP5QZT5w4gezsbJSVlUFEEB8fP+zqnp6ejoSEBACAzWab1IwTMdp57XY7zpw5E7R2pG1REUrhEsEra2dnp6Snpwf+lIuI/Pvf/5ZbbrlF2tvbA9sqKirkzjvvlC+++CKwrba2VrZu3SoiQ1fPp556SubOnSv33HOPtLW1iYhIf3+//PjHP5aBgQHJysoadY7f/OY38uqrr457xhvx+Xxy9epVGRwclKefflqqq6sD+669sn7jwoULsnjxYunt7R33OcLh+vNWVVWJ0+kMWjPStonAdLmy7tu3Dw6HI/CnHAC+853vYPXq1YErWWNjI7Zu3YrDhw8jIyMjsO7dd9/F448/DgBwuVwYHBzE2bNncejQITz33HMAgIqKCvz0pz+d1BVrpBkBoLi4GD/84Q+xfv16bNiwAfn5+di9ezcAYObMmUhMTIQxBk6nE6dPnx71+a9cuYLi4mL85S9/wbe+9a1h+0UEzc3Nw7afPn0aAwMDoz7vWPONdt6f/OQn+OCDD/Dll18G1o20LRpiEmtlZSWWLVuGdevWIS0tDRkZGairqwMA1NXVIScnZ9gxzz//PPbu3YuWlhY4nU689dZbsNvtgf19fX345JNPsHz5cgBDn9Dbb78dAJCSkoLu7m4AwD//+U/8+c9/Rl5eHv71r3/h5z//+YTnH2nGrq4u2O125Ofno7KyEhUVFdiwYQNuu+02nD9/HleuXAms/eijj/DAAw+M+Nz9/f0oLCxEeXk5MjMzR1xz8eJFPPbYYzh69GjQc65YsWLYF5LjnW+081qWhaysLLhcrjG3RUNMYnW73WhubkZBQQHa2tqwbds2lJSUBPaN9En63ve+B7vdjiVLlmDLli1Yt25d0P76+no8+uijgSvmypUr0dbWhpycHDzxxBN44YUXAACvvPIKXC4Xjh49igcffBCvvfZaSPNfP+OxY8ewYsWKoCv2V199hfvuuw9Xr15FQ0MDsrKykJ2djUuXLuHJJ58EADgcDrhcLmzevBn79u3DgQMH0NTUhJ07dyI3NxdVVVXDzn///ffj0KFDWL9+PT744AN8/PHHcDqdqKiowMMPPzzizDeab6zzzp8/f9j/CUbaFnGh3DvIJO9ZHQ6HlJaWBj7u6OgQAOL1esVms0lra+uwYwYGBiQvL0+SkpLE5/OFfO7xGuuedaQZd+3aJc3NzVJfXx/Y9vLLL8vPfvYz8fv9EZnx6NGjctttt0lqaqq89957Y66dzHxlZWWyadOmG24bL4R4z2qL7h+NIW63G7/97W8DH3d2diIpKQmWZSE5ORkej2fYMc899xy6urrw4IMPorKyEk899VQ0Rw4y0oxxcXFwuVz49NNP8frrr6OnpwfJycn4/e9/j/j4+IjMcc8998Bms2FwcBD33XffmGsnM5/H48GsWbNuuC3Son4b0NXVhba2tqBv2VRXV2PVqlUAgAULFuDcuXNBx+zduxd//etfcfjwYTz//PP4wx/+8M3VPSaun/GLL77AnDlzcPXqVRw8eBAPPfQQysvLUVJSgvvvvz8iM5w/fx4rV67EK6+8grfeegsOh2PUbydNdr7W1lYsXLjwhtsiLeqxut1uxMXFYf/+/ejv70dtbS3efPNNlJeXAxi6h2toaAisr6+vR1lZGf72t79h9uzZWLt2Lfx+P2pqaqI9esD1MzY0NMButyMxMREAsGrVKrS3t+Ozzz6LyPnb29uxYsUK/OpXv8LGjRtRUFCA3bt347HHHsOFCxeGrZ/MfL29vTh16hRWrlw55raoCOXeQSZxz/rGG29IUVGRrFmzRpKSkiQrK0tOnDgR2H/58mW56667pKenR1pbW+X222+X2traoOd4/fXX5Qc/+EFI5x+vse5Zr51RRKSkpETy8vLk5MmTgTVFRUXy6KOPypEjR8I+m8/nC/o+7TcOHz4sHo9n2PbJzHfw4EHJz8+/4baJQIj3rFGPtaSkRP70pz+Nuaa0tHTUUKJlrFhFpsaM0WC324f9hcVI2yYi1Fij/gWW2+3GmjVrxlzz0ksvRWma0GmYMRyamprGtS0aoh5rS0sL5s2bF+3TTlhaWhq+/e1vx3oMugbfmI2ijm/MRtMeYyU1GCupwVhJDcZKajBWUoOxkhqMldRgrKQGYyU1GCupwVhJDcZKajBWUoOxkhqMldRgrKRGyD/WYllWhzFmdjiHoZuDZVkdoRwX8o+10OiMMQ0AXhCRhhsupnHjbQCpwVhJDcZKajBWUoOxkhqMldRgrKQGYyU1GCupwVinAGPMVmPMp8aYXmPMvljPM1XF5Bdg0DDtAH4H4EcAEm6w9qbFWKcAEXkPAIwx3wdwd4zHmbJ4G0BqMFZSg7GSGoyV1OAXWFOAMcaGoc9FHIA4Y4wFoF9E+mM72dTCK+vUsAOAF8AvAWz4+r93xHSiKYhX1ilARMoBlMd4jCmPV1ZSg7GSGow1MpoA/DfWQ0w3/FFsUoNXVlKDsZIajJXUYKykBmMlNRgrqcFYSQ3GSmowVlKDsZIajJXUYKykBmMlNRgrqcFYSQ3GSmowVlKDsZIajJXU+D8I2/S2w7y2QQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    path.draw()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
