{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f61e2cf9a30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)\n",
    "model.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.5,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/54000 (0%)] Loss: 9951.667969\n",
      "Train Epoch: 1 [1408/54000 (3%)] Loss: 20145.953125\n",
      "Train Epoch: 1 [2816/54000 (5%)] Loss: -7962.097656\n",
      "Train Epoch: 1 [4224/54000 (8%)] Loss: -2746.914062\n",
      "Train Epoch: 1 [5632/54000 (10%)] Loss: -7774.252930\n",
      "Train Epoch: 1 [7040/54000 (13%)] Loss: -20732.222656\n",
      "Train Epoch: 1 [8448/54000 (16%)] Loss: -5636.065430\n",
      "Train Epoch: 1 [9856/54000 (18%)] Loss: -28350.390625\n",
      "Train Epoch: 1 [11264/54000 (21%)] Loss: -26584.441406\n",
      "Train Epoch: 1 [12672/54000 (23%)] Loss: -23316.277344\n",
      "Train Epoch: 1 [14080/54000 (26%)] Loss: -37661.425781\n",
      "Train Epoch: 1 [15488/54000 (29%)] Loss: -36423.679688\n",
      "Train Epoch: 1 [16896/54000 (31%)] Loss: -41786.691406\n",
      "Train Epoch: 1 [18304/54000 (34%)] Loss: -53540.820312\n",
      "Train Epoch: 1 [19712/54000 (37%)] Loss: -40059.417969\n",
      "Train Epoch: 1 [21120/54000 (39%)] Loss: -46349.636719\n",
      "Train Epoch: 1 [22528/54000 (42%)] Loss: -59281.292969\n",
      "Train Epoch: 1 [23936/54000 (44%)] Loss: -46640.015625\n",
      "Train Epoch: 1 [25344/54000 (47%)] Loss: -69708.898438\n",
      "Train Epoch: 1 [26752/54000 (50%)] Loss: -58707.027344\n",
      "Train Epoch: 1 [28160/54000 (52%)] Loss: -49233.968750\n",
      "Train Epoch: 1 [29568/54000 (55%)] Loss: -43770.929688\n",
      "Train Epoch: 1 [30976/54000 (57%)] Loss: -50766.738281\n",
      "Train Epoch: 1 [32384/54000 (60%)] Loss: -69553.093750\n",
      "Train Epoch: 1 [33792/54000 (63%)] Loss: -96703.421875\n",
      "Train Epoch: 1 [35200/54000 (65%)] Loss: -69137.632812\n",
      "Train Epoch: 1 [36608/54000 (68%)] Loss: -65737.617188\n",
      "Train Epoch: 1 [38016/54000 (70%)] Loss: -94242.453125\n",
      "Train Epoch: 1 [39424/54000 (73%)] Loss: -74484.078125\n",
      "Train Epoch: 1 [40832/54000 (76%)] Loss: -85792.937500\n",
      "Train Epoch: 1 [42240/54000 (78%)] Loss: -102134.695312\n",
      "Train Epoch: 1 [43648/54000 (81%)] Loss: -79568.046875\n",
      "Train Epoch: 1 [45056/54000 (83%)] Loss: -73402.601562\n",
      "Train Epoch: 1 [46464/54000 (86%)] Loss: -103010.632812\n",
      "Train Epoch: 1 [47872/54000 (89%)] Loss: -91573.382812\n",
      "Train Epoch: 1 [49280/54000 (91%)] Loss: -80578.109375\n",
      "Train Epoch: 1 [50688/54000 (94%)] Loss: -100700.445312\n",
      "Train Epoch: 1 [52096/54000 (96%)] Loss: -88283.789062\n",
      "    epoch          : 1\n",
      "    loss           : -55937.354267905204\n",
      "    val_loss       : -102499.19807545732\n",
      "Train Epoch: 2 [0/54000 (0%)] Loss: -116398.554688\n",
      "Train Epoch: 2 [1408/54000 (3%)] Loss: -83640.343750\n",
      "Train Epoch: 2 [2816/54000 (5%)] Loss: -124351.031250\n",
      "Train Epoch: 2 [4224/54000 (8%)] Loss: -95800.578125\n",
      "Train Epoch: 2 [5632/54000 (10%)] Loss: -98508.890625\n",
      "Train Epoch: 2 [7040/54000 (13%)] Loss: -82888.523438\n",
      "Train Epoch: 2 [8448/54000 (16%)] Loss: -102755.429688\n",
      "Train Epoch: 2 [9856/54000 (18%)] Loss: -103388.671875\n",
      "Train Epoch: 2 [11264/54000 (21%)] Loss: -101685.156250\n",
      "Train Epoch: 2 [12672/54000 (23%)] Loss: -98186.187500\n",
      "Train Epoch: 2 [14080/54000 (26%)] Loss: -108015.875000\n",
      "Train Epoch: 2 [15488/54000 (29%)] Loss: -136744.625000\n",
      "Train Epoch: 2 [16896/54000 (31%)] Loss: -106550.023438\n",
      "Train Epoch: 2 [18304/54000 (34%)] Loss: -122211.031250\n",
      "Train Epoch: 2 [19712/54000 (37%)] Loss: -115578.093750\n",
      "Train Epoch: 2 [21120/54000 (39%)] Loss: -112782.398438\n",
      "Train Epoch: 2 [22528/54000 (42%)] Loss: -107701.953125\n",
      "Train Epoch: 2 [23936/54000 (44%)] Loss: -118657.765625\n",
      "Train Epoch: 2 [25344/54000 (47%)] Loss: -119766.132812\n",
      "Train Epoch: 2 [26752/54000 (50%)] Loss: -104101.765625\n",
      "Train Epoch: 2 [28160/54000 (52%)] Loss: -99008.875000\n",
      "Train Epoch: 2 [29568/54000 (55%)] Loss: -98251.468750\n",
      "Train Epoch: 2 [30976/54000 (57%)] Loss: -117684.929688\n",
      "Train Epoch: 2 [32384/54000 (60%)] Loss: -133971.578125\n",
      "Train Epoch: 2 [33792/54000 (63%)] Loss: -118558.375000\n",
      "Train Epoch: 2 [35200/54000 (65%)] Loss: -118393.109375\n",
      "Train Epoch: 2 [36608/54000 (68%)] Loss: -121170.640625\n",
      "Train Epoch: 2 [38016/54000 (70%)] Loss: -117052.828125\n",
      "Train Epoch: 2 [39424/54000 (73%)] Loss: -127904.718750\n",
      "Train Epoch: 2 [40832/54000 (76%)] Loss: -142505.453125\n",
      "Train Epoch: 2 [42240/54000 (78%)] Loss: -129351.539062\n",
      "Train Epoch: 2 [43648/54000 (81%)] Loss: -128793.703125\n",
      "Train Epoch: 2 [45056/54000 (83%)] Loss: -132410.203125\n",
      "Train Epoch: 2 [46464/54000 (86%)] Loss: -98821.507812\n",
      "Train Epoch: 2 [47872/54000 (89%)] Loss: -129331.171875\n",
      "Train Epoch: 2 [49280/54000 (91%)] Loss: -140877.375000\n",
      "Train Epoch: 2 [50688/54000 (94%)] Loss: -122448.882812\n",
      "Train Epoch: 2 [52096/54000 (96%)] Loss: -168905.750000\n",
      "    epoch          : 2\n",
      "    loss           : -117422.52250299042\n",
      "    val_loss       : -133782.20357755336\n",
      "Train Epoch: 3 [0/54000 (0%)] Loss: -148549.921875\n",
      "Train Epoch: 3 [1408/54000 (3%)] Loss: -149600.437500\n",
      "Train Epoch: 3 [2816/54000 (5%)] Loss: -149313.890625\n",
      "Train Epoch: 3 [4224/54000 (8%)] Loss: -150381.328125\n",
      "Train Epoch: 3 [5632/54000 (10%)] Loss: -146415.437500\n",
      "Train Epoch: 3 [7040/54000 (13%)] Loss: -130140.820312\n",
      "Train Epoch: 3 [8448/54000 (16%)] Loss: -148212.343750\n",
      "Train Epoch: 3 [9856/54000 (18%)] Loss: -143882.859375\n",
      "Train Epoch: 3 [11264/54000 (21%)] Loss: -144756.375000\n",
      "Train Epoch: 3 [12672/54000 (23%)] Loss: -139810.515625\n",
      "Train Epoch: 3 [14080/54000 (26%)] Loss: -127511.718750\n",
      "Train Epoch: 3 [15488/54000 (29%)] Loss: -157756.375000\n",
      "Train Epoch: 3 [16896/54000 (31%)] Loss: -117278.179688\n",
      "Train Epoch: 3 [18304/54000 (34%)] Loss: -137202.140625\n",
      "Train Epoch: 3 [19712/54000 (37%)] Loss: -144464.781250\n",
      "Train Epoch: 3 [21120/54000 (39%)] Loss: -129900.828125\n",
      "Train Epoch: 3 [22528/54000 (42%)] Loss: -155696.843750\n",
      "Train Epoch: 3 [23936/54000 (44%)] Loss: -147043.906250\n",
      "Train Epoch: 3 [25344/54000 (47%)] Loss: -146700.546875\n",
      "Train Epoch: 3 [26752/54000 (50%)] Loss: -142254.468750\n",
      "Train Epoch: 3 [28160/54000 (52%)] Loss: -159383.156250\n",
      "Train Epoch: 3 [29568/54000 (55%)] Loss: -145604.968750\n",
      "Train Epoch: 3 [30976/54000 (57%)] Loss: -144250.750000\n",
      "Train Epoch: 3 [32384/54000 (60%)] Loss: -162397.093750\n",
      "Train Epoch: 3 [33792/54000 (63%)] Loss: -129438.570312\n",
      "Train Epoch: 3 [35200/54000 (65%)] Loss: -139616.000000\n",
      "Train Epoch: 3 [36608/54000 (68%)] Loss: -111498.375000\n",
      "Train Epoch: 3 [38016/54000 (70%)] Loss: -151495.421875\n",
      "Train Epoch: 3 [39424/54000 (73%)] Loss: -149175.906250\n",
      "Train Epoch: 3 [40832/54000 (76%)] Loss: -156691.078125\n",
      "Train Epoch: 3 [42240/54000 (78%)] Loss: -149573.046875\n",
      "Train Epoch: 3 [43648/54000 (81%)] Loss: -153148.484375\n",
      "Train Epoch: 3 [45056/54000 (83%)] Loss: -148290.468750\n",
      "Train Epoch: 3 [46464/54000 (86%)] Loss: -131512.906250\n",
      "Train Epoch: 3 [47872/54000 (89%)] Loss: -151603.984375\n",
      "Train Epoch: 3 [49280/54000 (91%)] Loss: -141024.546875\n",
      "Train Epoch: 3 [50688/54000 (94%)] Loss: -159250.750000\n",
      "Train Epoch: 3 [52096/54000 (96%)] Loss: -187171.890625\n",
      "    epoch          : 3\n",
      "    loss           : -143665.81192060406\n",
      "    val_loss       : -156305.49585556402\n",
      "Train Epoch: 4 [0/54000 (0%)] Loss: -162880.250000\n",
      "Train Epoch: 4 [1408/54000 (3%)] Loss: -135915.718750\n",
      "Train Epoch: 4 [2816/54000 (5%)] Loss: -162455.015625\n",
      "Train Epoch: 4 [4224/54000 (8%)] Loss: -153354.765625\n",
      "Train Epoch: 4 [5632/54000 (10%)] Loss: -144549.890625\n",
      "Train Epoch: 4 [7040/54000 (13%)] Loss: -158108.625000\n",
      "Train Epoch: 4 [8448/54000 (16%)] Loss: -173737.593750\n",
      "Train Epoch: 4 [9856/54000 (18%)] Loss: -134239.437500\n",
      "Train Epoch: 4 [11264/54000 (21%)] Loss: -151223.015625\n",
      "Train Epoch: 4 [12672/54000 (23%)] Loss: -164260.593750\n",
      "Train Epoch: 4 [14080/54000 (26%)] Loss: -143340.484375\n",
      "Train Epoch: 4 [15488/54000 (29%)] Loss: -143651.000000\n",
      "Train Epoch: 4 [16896/54000 (31%)] Loss: -155665.968750\n",
      "Train Epoch: 4 [18304/54000 (34%)] Loss: -154287.906250\n",
      "Train Epoch: 4 [19712/54000 (37%)] Loss: -143568.921875\n",
      "Train Epoch: 4 [21120/54000 (39%)] Loss: -168773.281250\n",
      "Train Epoch: 4 [22528/54000 (42%)] Loss: -146304.843750\n",
      "Train Epoch: 4 [23936/54000 (44%)] Loss: -157718.125000\n",
      "Train Epoch: 4 [25344/54000 (47%)] Loss: -143506.250000\n",
      "Train Epoch: 4 [26752/54000 (50%)] Loss: -143740.187500\n",
      "Train Epoch: 4 [28160/54000 (52%)] Loss: -172990.000000\n",
      "Train Epoch: 4 [29568/54000 (55%)] Loss: -152538.531250\n",
      "Train Epoch: 4 [30976/54000 (57%)] Loss: -165202.375000\n",
      "Train Epoch: 4 [32384/54000 (60%)] Loss: -170843.843750\n",
      "Train Epoch: 4 [33792/54000 (63%)] Loss: -160862.750000\n",
      "Train Epoch: 4 [35200/54000 (65%)] Loss: -158610.437500\n",
      "Train Epoch: 4 [36608/54000 (68%)] Loss: -127805.148438\n",
      "Train Epoch: 4 [38016/54000 (70%)] Loss: -163721.156250\n",
      "Train Epoch: 4 [39424/54000 (73%)] Loss: -173268.859375\n",
      "Train Epoch: 4 [40832/54000 (76%)] Loss: -168269.046875\n",
      "Train Epoch: 4 [42240/54000 (78%)] Loss: -151306.968750\n",
      "Train Epoch: 4 [43648/54000 (81%)] Loss: -155826.484375\n",
      "Train Epoch: 4 [45056/54000 (83%)] Loss: -159434.765625\n",
      "Train Epoch: 4 [46464/54000 (86%)] Loss: -176012.937500\n",
      "Train Epoch: 4 [47872/54000 (89%)] Loss: -152752.000000\n",
      "Train Epoch: 4 [49280/54000 (91%)] Loss: -171947.515625\n",
      "Train Epoch: 4 [50688/54000 (94%)] Loss: -161636.031250\n",
      "Train Epoch: 4 [52096/54000 (96%)] Loss: -153161.609375\n",
      "    epoch          : 4\n",
      "    loss           : -159019.13617673446\n",
      "    val_loss       : -169246.57355182926\n",
      "Train Epoch: 5 [0/54000 (0%)] Loss: -145338.531250\n",
      "Train Epoch: 5 [1408/54000 (3%)] Loss: -189115.812500\n",
      "Train Epoch: 5 [2816/54000 (5%)] Loss: -154334.671875\n",
      "Train Epoch: 5 [4224/54000 (8%)] Loss: -169431.718750\n",
      "Train Epoch: 5 [5632/54000 (10%)] Loss: -146012.234375\n",
      "Train Epoch: 5 [7040/54000 (13%)] Loss: -174337.421875\n",
      "Train Epoch: 5 [8448/54000 (16%)] Loss: -160897.078125\n",
      "Train Epoch: 5 [9856/54000 (18%)] Loss: -164643.406250\n",
      "Train Epoch: 5 [11264/54000 (21%)] Loss: -165089.000000\n",
      "Train Epoch: 5 [12672/54000 (23%)] Loss: -175619.218750\n",
      "Train Epoch: 5 [14080/54000 (26%)] Loss: -165763.812500\n",
      "Train Epoch: 5 [15488/54000 (29%)] Loss: -162903.687500\n",
      "Train Epoch: 5 [16896/54000 (31%)] Loss: -166540.531250\n",
      "Train Epoch: 5 [18304/54000 (34%)] Loss: -147360.421875\n",
      "Train Epoch: 5 [19712/54000 (37%)] Loss: -192190.593750\n",
      "Train Epoch: 5 [21120/54000 (39%)] Loss: -158551.281250\n",
      "Train Epoch: 5 [22528/54000 (42%)] Loss: -179447.031250\n",
      "Train Epoch: 5 [23936/54000 (44%)] Loss: -168604.765625\n",
      "Train Epoch: 5 [25344/54000 (47%)] Loss: -168652.109375\n",
      "Train Epoch: 5 [26752/54000 (50%)] Loss: -187615.421875\n",
      "Train Epoch: 5 [28160/54000 (52%)] Loss: -164471.765625\n",
      "Train Epoch: 5 [29568/54000 (55%)] Loss: -172554.750000\n",
      "Train Epoch: 5 [30976/54000 (57%)] Loss: -177867.921875\n",
      "Train Epoch: 5 [32384/54000 (60%)] Loss: -200956.500000\n",
      "Train Epoch: 5 [33792/54000 (63%)] Loss: -177638.125000\n",
      "Train Epoch: 5 [35200/54000 (65%)] Loss: -161738.578125\n",
      "Train Epoch: 5 [36608/54000 (68%)] Loss: -158337.687500\n",
      "Train Epoch: 5 [38016/54000 (70%)] Loss: -150840.828125\n",
      "Train Epoch: 5 [39424/54000 (73%)] Loss: -152356.640625\n",
      "Train Epoch: 5 [40832/54000 (76%)] Loss: -178757.859375\n",
      "Train Epoch: 5 [42240/54000 (78%)] Loss: -165019.625000\n",
      "Train Epoch: 5 [43648/54000 (81%)] Loss: -170701.328125\n",
      "Train Epoch: 5 [45056/54000 (83%)] Loss: -195794.703125\n",
      "Train Epoch: 5 [46464/54000 (86%)] Loss: -173040.718750\n",
      "Train Epoch: 5 [47872/54000 (89%)] Loss: -173700.968750\n",
      "Train Epoch: 5 [49280/54000 (91%)] Loss: -165838.812500\n",
      "Train Epoch: 5 [50688/54000 (94%)] Loss: -161088.156250\n",
      "Train Epoch: 5 [52096/54000 (96%)] Loss: -179076.093750\n",
      "    epoch          : 5\n",
      "    loss           : -169115.84988038277\n",
      "    val_loss       : -174760.77765339176\n",
      "Train Epoch: 6 [0/54000 (0%)] Loss: -172398.000000\n",
      "Train Epoch: 6 [1408/54000 (3%)] Loss: -174565.562500\n",
      "Train Epoch: 6 [2816/54000 (5%)] Loss: -152458.687500\n",
      "Train Epoch: 6 [4224/54000 (8%)] Loss: -172503.781250\n",
      "Train Epoch: 6 [5632/54000 (10%)] Loss: -181613.734375\n",
      "Train Epoch: 6 [7040/54000 (13%)] Loss: -186981.812500\n",
      "Train Epoch: 6 [8448/54000 (16%)] Loss: -187956.000000\n",
      "Train Epoch: 6 [9856/54000 (18%)] Loss: -185368.187500\n",
      "Train Epoch: 6 [11264/54000 (21%)] Loss: -156714.640625\n",
      "Train Epoch: 6 [12672/54000 (23%)] Loss: -162149.640625\n",
      "Train Epoch: 6 [14080/54000 (26%)] Loss: -138937.671875\n",
      "Train Epoch: 6 [15488/54000 (29%)] Loss: -149866.156250\n",
      "Train Epoch: 6 [16896/54000 (31%)] Loss: -179118.250000\n",
      "Train Epoch: 6 [18304/54000 (34%)] Loss: -151460.312500\n",
      "Train Epoch: 6 [19712/54000 (37%)] Loss: -183624.531250\n",
      "Train Epoch: 6 [21120/54000 (39%)] Loss: -183719.781250\n",
      "Train Epoch: 6 [22528/54000 (42%)] Loss: -177147.968750\n",
      "Train Epoch: 6 [23936/54000 (44%)] Loss: -178407.843750\n",
      "Train Epoch: 6 [25344/54000 (47%)] Loss: -171014.750000\n",
      "Train Epoch: 6 [26752/54000 (50%)] Loss: -174292.656250\n",
      "Train Epoch: 6 [28160/54000 (52%)] Loss: -197162.203125\n",
      "Train Epoch: 6 [29568/54000 (55%)] Loss: -183471.312500\n",
      "Train Epoch: 6 [30976/54000 (57%)] Loss: -183871.234375\n",
      "Train Epoch: 6 [32384/54000 (60%)] Loss: -175395.890625\n",
      "Train Epoch: 6 [33792/54000 (63%)] Loss: -201299.015625\n",
      "Train Epoch: 6 [35200/54000 (65%)] Loss: -182496.656250\n",
      "Train Epoch: 6 [36608/54000 (68%)] Loss: -152528.375000\n",
      "Train Epoch: 6 [38016/54000 (70%)] Loss: -199634.156250\n",
      "Train Epoch: 6 [39424/54000 (73%)] Loss: -168035.937500\n",
      "Train Epoch: 6 [40832/54000 (76%)] Loss: -160386.937500\n",
      "Train Epoch: 6 [42240/54000 (78%)] Loss: -158896.000000\n",
      "Train Epoch: 6 [43648/54000 (81%)] Loss: -180949.140625\n",
      "Train Epoch: 6 [45056/54000 (83%)] Loss: -170714.609375\n",
      "Train Epoch: 6 [46464/54000 (86%)] Loss: -168079.765625\n",
      "Train Epoch: 6 [47872/54000 (89%)] Loss: -184368.593750\n",
      "Train Epoch: 6 [49280/54000 (91%)] Loss: -180164.031250\n",
      "Train Epoch: 6 [50688/54000 (94%)] Loss: -185298.093750\n",
      "Train Epoch: 6 [52096/54000 (96%)] Loss: -179351.312500\n",
      "    epoch          : 6\n",
      "    loss           : -174930.90482954544\n",
      "    val_loss       : -179895.01728039252\n",
      "Train Epoch: 7 [0/54000 (0%)] Loss: -200325.250000\n",
      "Train Epoch: 7 [1408/54000 (3%)] Loss: -180113.937500\n",
      "Train Epoch: 7 [2816/54000 (5%)] Loss: -152737.234375\n",
      "Train Epoch: 7 [4224/54000 (8%)] Loss: -150734.750000\n",
      "Train Epoch: 7 [5632/54000 (10%)] Loss: -200682.500000\n",
      "Train Epoch: 7 [7040/54000 (13%)] Loss: -152604.625000\n",
      "Train Epoch: 7 [8448/54000 (16%)] Loss: -181538.484375\n",
      "Train Epoch: 7 [9856/54000 (18%)] Loss: -181028.125000\n",
      "Train Epoch: 7 [11264/54000 (21%)] Loss: -173682.656250\n",
      "Train Epoch: 7 [12672/54000 (23%)] Loss: -178962.250000\n",
      "Train Epoch: 7 [14080/54000 (26%)] Loss: -177678.343750\n",
      "Train Epoch: 7 [15488/54000 (29%)] Loss: -193058.968750\n",
      "Train Epoch: 7 [16896/54000 (31%)] Loss: -169807.531250\n",
      "Train Epoch: 7 [18304/54000 (34%)] Loss: -165638.828125\n",
      "Train Epoch: 7 [19712/54000 (37%)] Loss: -175670.375000\n",
      "Train Epoch: 7 [21120/54000 (39%)] Loss: -148030.296875\n",
      "Train Epoch: 7 [22528/54000 (42%)] Loss: -173752.093750\n",
      "Train Epoch: 7 [23936/54000 (44%)] Loss: -185495.109375\n",
      "Train Epoch: 7 [25344/54000 (47%)] Loss: -184856.125000\n",
      "Train Epoch: 7 [26752/54000 (50%)] Loss: -176949.687500\n",
      "Train Epoch: 7 [28160/54000 (52%)] Loss: -174877.375000\n",
      "Train Epoch: 7 [29568/54000 (55%)] Loss: -183211.250000\n",
      "Train Epoch: 7 [30976/54000 (57%)] Loss: -155657.468750\n",
      "Train Epoch: 7 [32384/54000 (60%)] Loss: -176008.000000\n",
      "Train Epoch: 7 [33792/54000 (63%)] Loss: -171808.968750\n",
      "Train Epoch: 7 [35200/54000 (65%)] Loss: -152917.468750\n",
      "Train Epoch: 7 [36608/54000 (68%)] Loss: -147842.312500\n",
      "Train Epoch: 7 [38016/54000 (70%)] Loss: -149594.281250\n",
      "Train Epoch: 7 [39424/54000 (73%)] Loss: -174106.375000\n",
      "Train Epoch: 7 [40832/54000 (76%)] Loss: -193697.593750\n",
      "Train Epoch: 7 [42240/54000 (78%)] Loss: -172407.593750\n",
      "Train Epoch: 7 [43648/54000 (81%)] Loss: -172416.765625\n",
      "Train Epoch: 7 [45056/54000 (83%)] Loss: -175345.078125\n",
      "Train Epoch: 7 [46464/54000 (86%)] Loss: -183746.796875\n",
      "Train Epoch: 7 [47872/54000 (89%)] Loss: -179522.750000\n",
      "Train Epoch: 7 [49280/54000 (91%)] Loss: -203189.234375\n",
      "Train Epoch: 7 [50688/54000 (94%)] Loss: -182935.062500\n",
      "Train Epoch: 7 [52096/54000 (96%)] Loss: -178049.328125\n",
      "    epoch          : 7\n",
      "    loss           : -177387.34995514355\n",
      "    val_loss       : -182754.91539634147\n",
      "Train Epoch: 8 [0/54000 (0%)] Loss: -205110.359375\n",
      "Train Epoch: 8 [1408/54000 (3%)] Loss: -177926.187500\n",
      "Train Epoch: 8 [2816/54000 (5%)] Loss: -174381.015625\n",
      "Train Epoch: 8 [4224/54000 (8%)] Loss: -165665.078125\n",
      "Train Epoch: 8 [5632/54000 (10%)] Loss: -182852.546875\n",
      "Train Epoch: 8 [7040/54000 (13%)] Loss: -183893.062500\n",
      "Train Epoch: 8 [8448/54000 (16%)] Loss: -183121.312500\n",
      "Train Epoch: 8 [9856/54000 (18%)] Loss: -183916.359375\n",
      "Train Epoch: 8 [11264/54000 (21%)] Loss: -151506.218750\n",
      "Train Epoch: 8 [12672/54000 (23%)] Loss: -185417.234375\n",
      "Train Epoch: 8 [14080/54000 (26%)] Loss: -203530.281250\n",
      "Train Epoch: 8 [15488/54000 (29%)] Loss: -176623.812500\n",
      "Train Epoch: 8 [16896/54000 (31%)] Loss: -175549.609375\n",
      "Train Epoch: 8 [18304/54000 (34%)] Loss: -206165.343750\n",
      "Train Epoch: 8 [19712/54000 (37%)] Loss: -180852.031250\n",
      "Train Epoch: 8 [21120/54000 (39%)] Loss: -164990.640625\n",
      "Train Epoch: 8 [22528/54000 (42%)] Loss: -156535.125000\n",
      "Train Epoch: 8 [23936/54000 (44%)] Loss: -205252.718750\n",
      "Train Epoch: 8 [25344/54000 (47%)] Loss: -181705.031250\n",
      "Train Epoch: 8 [26752/54000 (50%)] Loss: -169823.781250\n",
      "Train Epoch: 8 [28160/54000 (52%)] Loss: -176227.406250\n",
      "Train Epoch: 8 [29568/54000 (55%)] Loss: -179665.765625\n",
      "Train Epoch: 8 [30976/54000 (57%)] Loss: -175341.875000\n",
      "Train Epoch: 8 [32384/54000 (60%)] Loss: -184551.984375\n",
      "Train Epoch: 8 [33792/54000 (63%)] Loss: -165817.187500\n",
      "Train Epoch: 8 [35200/54000 (65%)] Loss: -178975.640625\n",
      "Train Epoch: 8 [36608/54000 (68%)] Loss: -180830.343750\n",
      "Train Epoch: 8 [38016/54000 (70%)] Loss: -201743.500000\n",
      "Train Epoch: 8 [39424/54000 (73%)] Loss: -180370.906250\n",
      "Train Epoch: 8 [40832/54000 (76%)] Loss: -193645.781250\n",
      "Train Epoch: 8 [42240/54000 (78%)] Loss: -175342.125000\n",
      "Train Epoch: 8 [43648/54000 (81%)] Loss: -202148.390625\n",
      "Train Epoch: 8 [45056/54000 (83%)] Loss: -184951.140625\n",
      "Train Epoch: 8 [46464/54000 (86%)] Loss: -184716.484375\n",
      "Train Epoch: 8 [47872/54000 (89%)] Loss: -180338.296875\n",
      "Train Epoch: 8 [49280/54000 (91%)] Loss: -177872.937500\n",
      "Train Epoch: 8 [50688/54000 (94%)] Loss: -168009.968750\n",
      "Train Epoch: 8 [52096/54000 (96%)] Loss: -166837.828125\n",
      "    epoch          : 8\n",
      "    loss           : -180086.43824760764\n",
      "    val_loss       : -186873.3009241616\n",
      "Train Epoch: 9 [0/54000 (0%)] Loss: -207408.265625\n",
      "Train Epoch: 9 [1408/54000 (3%)] Loss: -194914.328125\n",
      "Train Epoch: 9 [2816/54000 (5%)] Loss: -186139.453125\n",
      "Train Epoch: 9 [4224/54000 (8%)] Loss: -191141.687500\n",
      "Train Epoch: 9 [5632/54000 (10%)] Loss: -193921.687500\n",
      "Train Epoch: 9 [7040/54000 (13%)] Loss: -186913.921875\n",
      "Train Epoch: 9 [8448/54000 (16%)] Loss: -179688.890625\n",
      "Train Epoch: 9 [9856/54000 (18%)] Loss: -189406.625000\n",
      "Train Epoch: 9 [11264/54000 (21%)] Loss: -191554.125000\n",
      "Train Epoch: 9 [12672/54000 (23%)] Loss: -183931.562500\n",
      "Train Epoch: 9 [14080/54000 (26%)] Loss: -178828.125000\n",
      "Train Epoch: 9 [15488/54000 (29%)] Loss: -206735.218750\n",
      "Train Epoch: 9 [16896/54000 (31%)] Loss: -179494.906250\n",
      "Train Epoch: 9 [18304/54000 (34%)] Loss: -153482.968750\n",
      "Train Epoch: 9 [19712/54000 (37%)] Loss: -161468.562500\n",
      "Train Epoch: 9 [21120/54000 (39%)] Loss: -168183.703125\n",
      "Train Epoch: 9 [22528/54000 (42%)] Loss: -205915.781250\n",
      "Train Epoch: 9 [23936/54000 (44%)] Loss: -167199.515625\n",
      "Train Epoch: 9 [25344/54000 (47%)] Loss: -177713.703125\n",
      "Train Epoch: 9 [26752/54000 (50%)] Loss: -179291.359375\n",
      "Train Epoch: 9 [28160/54000 (52%)] Loss: -206059.218750\n",
      "Train Epoch: 9 [29568/54000 (55%)] Loss: -174211.156250\n",
      "Train Epoch: 9 [30976/54000 (57%)] Loss: -197404.500000\n",
      "Train Epoch: 9 [32384/54000 (60%)] Loss: -188863.078125\n",
      "Train Epoch: 9 [33792/54000 (63%)] Loss: -191572.484375\n",
      "Train Epoch: 9 [35200/54000 (65%)] Loss: -154569.187500\n",
      "Train Epoch: 9 [36608/54000 (68%)] Loss: -178943.703125\n",
      "Train Epoch: 9 [38016/54000 (70%)] Loss: -160952.406250\n",
      "Train Epoch: 9 [39424/54000 (73%)] Loss: -187960.906250\n",
      "Train Epoch: 9 [40832/54000 (76%)] Loss: -190910.593750\n",
      "Train Epoch: 9 [42240/54000 (78%)] Loss: -207487.750000\n",
      "Train Epoch: 9 [43648/54000 (81%)] Loss: -177242.203125\n",
      "Train Epoch: 9 [45056/54000 (83%)] Loss: -179904.015625\n",
      "Train Epoch: 9 [46464/54000 (86%)] Loss: -202960.375000\n",
      "Train Epoch: 9 [47872/54000 (89%)] Loss: -182639.578125\n",
      "Train Epoch: 9 [49280/54000 (91%)] Loss: -189575.531250\n",
      "Train Epoch: 9 [50688/54000 (94%)] Loss: -208156.593750\n",
      "Train Epoch: 9 [52096/54000 (96%)] Loss: -176324.703125\n",
      "    epoch          : 9\n",
      "    loss           : -181913.91963217704\n",
      "    val_loss       : -188362.87245141005\n",
      "Train Epoch: 10 [0/54000 (0%)] Loss: -208311.093750\n",
      "Train Epoch: 10 [1408/54000 (3%)] Loss: -202614.546875\n",
      "Train Epoch: 10 [2816/54000 (5%)] Loss: -179165.468750\n",
      "Train Epoch: 10 [4224/54000 (8%)] Loss: -155254.031250\n",
      "Train Epoch: 10 [5632/54000 (10%)] Loss: -181105.796875\n",
      "Train Epoch: 10 [7040/54000 (13%)] Loss: -187175.859375\n",
      "Train Epoch: 10 [8448/54000 (16%)] Loss: -207906.968750\n",
      "Train Epoch: 10 [9856/54000 (18%)] Loss: -184723.031250\n",
      "Train Epoch: 10 [11264/54000 (21%)] Loss: -193862.250000\n",
      "Train Epoch: 10 [12672/54000 (23%)] Loss: -184645.437500\n",
      "Train Epoch: 10 [14080/54000 (26%)] Loss: -207642.031250\n",
      "Train Epoch: 10 [15488/54000 (29%)] Loss: -181175.109375\n",
      "Train Epoch: 10 [16896/54000 (31%)] Loss: -182315.906250\n",
      "Train Epoch: 10 [18304/54000 (34%)] Loss: -196611.312500\n",
      "Train Epoch: 10 [19712/54000 (37%)] Loss: -189905.312500\n",
      "Train Epoch: 10 [21120/54000 (39%)] Loss: -184855.812500\n",
      "Train Epoch: 10 [22528/54000 (42%)] Loss: -156184.781250\n",
      "Train Epoch: 10 [23936/54000 (44%)] Loss: -178616.375000\n",
      "Train Epoch: 10 [25344/54000 (47%)] Loss: -179565.500000\n",
      "Train Epoch: 10 [26752/54000 (50%)] Loss: -186723.250000\n",
      "Train Epoch: 10 [28160/54000 (52%)] Loss: -185030.500000\n",
      "Train Epoch: 10 [29568/54000 (55%)] Loss: -184062.187500\n",
      "Train Epoch: 10 [30976/54000 (57%)] Loss: -194956.296875\n",
      "Train Epoch: 10 [32384/54000 (60%)] Loss: -188085.062500\n",
      "Train Epoch: 10 [33792/54000 (63%)] Loss: -179000.843750\n",
      "Train Epoch: 10 [35200/54000 (65%)] Loss: -186186.796875\n",
      "Train Epoch: 10 [36608/54000 (68%)] Loss: -171457.500000\n",
      "Train Epoch: 10 [38016/54000 (70%)] Loss: -168741.187500\n",
      "Train Epoch: 10 [39424/54000 (73%)] Loss: -156932.750000\n",
      "Train Epoch: 10 [40832/54000 (76%)] Loss: -207576.218750\n",
      "Train Epoch: 10 [42240/54000 (78%)] Loss: -193521.968750\n",
      "Train Epoch: 10 [43648/54000 (81%)] Loss: -180305.843750\n",
      "Train Epoch: 10 [45056/54000 (83%)] Loss: -178673.562500\n",
      "Train Epoch: 10 [46464/54000 (86%)] Loss: -184855.875000\n",
      "Train Epoch: 10 [47872/54000 (89%)] Loss: -208026.375000\n",
      "Train Epoch: 10 [49280/54000 (91%)] Loss: -182957.437500\n",
      "Train Epoch: 10 [50688/54000 (94%)] Loss: -184061.000000\n",
      "Train Epoch: 10 [52096/54000 (96%)] Loss: -171688.390625\n",
      "    epoch          : 10\n",
      "    loss           : -183052.72816985645\n",
      "    val_loss       : -189509.50433498475\n",
      "Train Epoch: 11 [0/54000 (0%)] Loss: -210592.093750\n",
      "Train Epoch: 11 [1408/54000 (3%)] Loss: -182708.546875\n",
      "Train Epoch: 11 [2816/54000 (5%)] Loss: -176638.687500\n",
      "Train Epoch: 11 [4224/54000 (8%)] Loss: -190567.140625\n",
      "Train Epoch: 11 [5632/54000 (10%)] Loss: -194234.703125\n",
      "Train Epoch: 11 [7040/54000 (13%)] Loss: -183736.656250\n",
      "Train Epoch: 11 [8448/54000 (16%)] Loss: -169610.031250\n",
      "Train Epoch: 11 [9856/54000 (18%)] Loss: -186120.656250\n",
      "Train Epoch: 11 [11264/54000 (21%)] Loss: -181672.250000\n",
      "Train Epoch: 11 [12672/54000 (23%)] Loss: -180253.937500\n",
      "Train Epoch: 11 [14080/54000 (26%)] Loss: -186613.750000\n",
      "Train Epoch: 11 [15488/54000 (29%)] Loss: -184290.921875\n",
      "Train Epoch: 11 [16896/54000 (31%)] Loss: -193291.000000\n",
      "Train Epoch: 11 [18304/54000 (34%)] Loss: -186207.687500\n",
      "Train Epoch: 11 [19712/54000 (37%)] Loss: -192606.234375\n",
      "Train Epoch: 11 [21120/54000 (39%)] Loss: -208429.390625\n",
      "Train Epoch: 11 [22528/54000 (42%)] Loss: -191856.250000\n",
      "Train Epoch: 11 [23936/54000 (44%)] Loss: -193744.968750\n",
      "Train Epoch: 11 [25344/54000 (47%)] Loss: -170736.734375\n",
      "Train Epoch: 11 [26752/54000 (50%)] Loss: -197680.031250\n",
      "Train Epoch: 11 [28160/54000 (52%)] Loss: -199186.031250\n",
      "Train Epoch: 11 [29568/54000 (55%)] Loss: -193282.953125\n",
      "Train Epoch: 11 [30976/54000 (57%)] Loss: -206402.265625\n",
      "Train Epoch: 11 [32384/54000 (60%)] Loss: -182405.265625\n",
      "Train Epoch: 11 [33792/54000 (63%)] Loss: -182132.234375\n",
      "Train Epoch: 11 [35200/54000 (65%)] Loss: -180176.343750\n",
      "Train Epoch: 11 [36608/54000 (68%)] Loss: -164184.656250\n",
      "Train Epoch: 11 [38016/54000 (70%)] Loss: -167300.562500\n",
      "Train Epoch: 11 [39424/54000 (73%)] Loss: -161670.562500\n",
      "Train Epoch: 11 [40832/54000 (76%)] Loss: -199025.406250\n",
      "Train Epoch: 11 [42240/54000 (78%)] Loss: -190529.156250\n",
      "Train Epoch: 11 [43648/54000 (81%)] Loss: -189061.484375\n",
      "Train Epoch: 11 [45056/54000 (83%)] Loss: -180661.796875\n",
      "Train Epoch: 11 [46464/54000 (86%)] Loss: -171962.359375\n",
      "Train Epoch: 11 [47872/54000 (89%)] Loss: -189829.031250\n",
      "Train Epoch: 11 [49280/54000 (91%)] Loss: -195916.968750\n",
      "Train Epoch: 11 [50688/54000 (94%)] Loss: -207838.343750\n",
      "Train Epoch: 11 [52096/54000 (96%)] Loss: -169833.609375\n",
      "    epoch          : 11\n",
      "    loss           : -184910.9210152512\n",
      "    val_loss       : -190506.1465081936\n",
      "Train Epoch: 12 [0/54000 (0%)] Loss: -211916.062500\n",
      "Train Epoch: 12 [1408/54000 (3%)] Loss: -179423.531250\n",
      "Train Epoch: 12 [2816/54000 (5%)] Loss: -172332.906250\n",
      "Train Epoch: 12 [4224/54000 (8%)] Loss: -182620.468750\n",
      "Train Epoch: 12 [5632/54000 (10%)] Loss: -182256.796875\n",
      "Train Epoch: 12 [7040/54000 (13%)] Loss: -211311.812500\n",
      "Train Epoch: 12 [8448/54000 (16%)] Loss: -162702.781250\n",
      "Train Epoch: 12 [9856/54000 (18%)] Loss: -180670.218750\n",
      "Train Epoch: 12 [11264/54000 (21%)] Loss: -163116.328125\n",
      "Train Epoch: 12 [12672/54000 (23%)] Loss: -201047.750000\n",
      "Train Epoch: 12 [14080/54000 (26%)] Loss: -183086.656250\n",
      "Train Epoch: 12 [15488/54000 (29%)] Loss: -174534.750000\n",
      "Train Epoch: 12 [16896/54000 (31%)] Loss: -185731.000000\n",
      "Train Epoch: 12 [18304/54000 (34%)] Loss: -186777.265625\n",
      "Train Epoch: 12 [19712/54000 (37%)] Loss: -162155.328125\n",
      "Train Epoch: 12 [21120/54000 (39%)] Loss: -181817.453125\n",
      "Train Epoch: 12 [22528/54000 (42%)] Loss: -193769.640625\n",
      "Train Epoch: 12 [23936/54000 (44%)] Loss: -188747.046875\n",
      "Train Epoch: 12 [25344/54000 (47%)] Loss: -170618.203125\n",
      "Train Epoch: 12 [26752/54000 (50%)] Loss: -211915.781250\n",
      "Train Epoch: 12 [28160/54000 (52%)] Loss: -199618.000000\n",
      "Train Epoch: 12 [29568/54000 (55%)] Loss: -201820.843750\n",
      "Train Epoch: 12 [30976/54000 (57%)] Loss: -188547.500000\n",
      "Train Epoch: 12 [32384/54000 (60%)] Loss: -187036.968750\n",
      "Train Epoch: 12 [33792/54000 (63%)] Loss: -188684.625000\n",
      "Train Epoch: 12 [35200/54000 (65%)] Loss: -184439.875000\n",
      "Train Epoch: 12 [36608/54000 (68%)] Loss: -163050.328125\n",
      "Train Epoch: 12 [38016/54000 (70%)] Loss: -169269.953125\n",
      "Train Epoch: 12 [39424/54000 (73%)] Loss: -213445.703125\n",
      "Train Epoch: 12 [40832/54000 (76%)] Loss: -188143.968750\n",
      "Train Epoch: 12 [42240/54000 (78%)] Loss: -195398.562500\n",
      "Train Epoch: 12 [43648/54000 (81%)] Loss: -163209.156250\n",
      "Train Epoch: 12 [45056/54000 (83%)] Loss: -185115.656250\n",
      "Train Epoch: 12 [46464/54000 (86%)] Loss: -187044.218750\n",
      "Train Epoch: 12 [47872/54000 (89%)] Loss: -179500.375000\n",
      "Train Epoch: 12 [49280/54000 (91%)] Loss: -183993.593750\n",
      "Train Epoch: 12 [50688/54000 (94%)] Loss: -170129.750000\n",
      "Train Epoch: 12 [52096/54000 (96%)] Loss: -167907.859375\n",
      "    epoch          : 12\n",
      "    loss           : -185941.74529007176\n",
      "    val_loss       : -193150.80328220275\n",
      "Train Epoch: 13 [0/54000 (0%)] Loss: -194745.578125\n",
      "Train Epoch: 13 [1408/54000 (3%)] Loss: -194405.937500\n",
      "Train Epoch: 13 [2816/54000 (5%)] Loss: -186784.171875\n",
      "Train Epoch: 13 [4224/54000 (8%)] Loss: -181554.468750\n",
      "Train Epoch: 13 [5632/54000 (10%)] Loss: -184880.781250\n",
      "Train Epoch: 13 [7040/54000 (13%)] Loss: -173692.953125\n",
      "Train Epoch: 13 [8448/54000 (16%)] Loss: -190014.281250\n",
      "Train Epoch: 13 [9856/54000 (18%)] Loss: -185799.125000\n",
      "Train Epoch: 13 [11264/54000 (21%)] Loss: -194700.312500\n",
      "Train Epoch: 13 [12672/54000 (23%)] Loss: -181905.750000\n",
      "Train Epoch: 13 [14080/54000 (26%)] Loss: -170683.718750\n",
      "Train Epoch: 13 [15488/54000 (29%)] Loss: -188342.406250\n",
      "Train Epoch: 13 [16896/54000 (31%)] Loss: -190807.093750\n",
      "Train Epoch: 13 [18304/54000 (34%)] Loss: -185191.859375\n",
      "Train Epoch: 13 [19712/54000 (37%)] Loss: -187385.609375\n",
      "Train Epoch: 13 [21120/54000 (39%)] Loss: -199958.625000\n",
      "Train Epoch: 13 [22528/54000 (42%)] Loss: -183320.984375\n",
      "Train Epoch: 13 [23936/54000 (44%)] Loss: -184498.312500\n",
      "Train Epoch: 13 [25344/54000 (47%)] Loss: -188841.328125\n",
      "Train Epoch: 13 [26752/54000 (50%)] Loss: -168002.500000\n",
      "Train Epoch: 13 [28160/54000 (52%)] Loss: -174439.187500\n",
      "Train Epoch: 13 [29568/54000 (55%)] Loss: -184630.687500\n",
      "Train Epoch: 13 [30976/54000 (57%)] Loss: -191333.750000\n",
      "Train Epoch: 13 [32384/54000 (60%)] Loss: -182927.812500\n",
      "Train Epoch: 13 [33792/54000 (63%)] Loss: -190456.218750\n",
      "Train Epoch: 13 [35200/54000 (65%)] Loss: -196332.296875\n",
      "Train Epoch: 13 [36608/54000 (68%)] Loss: -185084.593750\n",
      "Train Epoch: 13 [38016/54000 (70%)] Loss: -212086.125000\n",
      "Train Epoch: 13 [39424/54000 (73%)] Loss: -183534.390625\n",
      "Train Epoch: 13 [40832/54000 (76%)] Loss: -183168.203125\n",
      "Train Epoch: 13 [42240/54000 (78%)] Loss: -191276.406250\n",
      "Train Epoch: 13 [43648/54000 (81%)] Loss: -169745.296875\n",
      "Train Epoch: 13 [45056/54000 (83%)] Loss: -181461.000000\n",
      "Train Epoch: 13 [46464/54000 (86%)] Loss: -192039.218750\n",
      "Train Epoch: 13 [47872/54000 (89%)] Loss: -187878.453125\n",
      "Train Epoch: 13 [49280/54000 (91%)] Loss: -190187.250000\n",
      "Train Epoch: 13 [50688/54000 (94%)] Loss: -192429.218750\n",
      "Train Epoch: 13 [52096/54000 (96%)] Loss: -192948.125000\n",
      "    epoch          : 13\n",
      "    loss           : -186511.6158791866\n",
      "    val_loss       : -194057.96809498858\n",
      "Train Epoch: 14 [0/54000 (0%)] Loss: -192202.609375\n",
      "Train Epoch: 14 [1408/54000 (3%)] Loss: -189040.031250\n",
      "Train Epoch: 14 [2816/54000 (5%)] Loss: -187549.546875\n",
      "Train Epoch: 14 [4224/54000 (8%)] Loss: -183537.687500\n",
      "Train Epoch: 14 [5632/54000 (10%)] Loss: -201175.765625\n",
      "Train Epoch: 14 [7040/54000 (13%)] Loss: -195819.031250\n",
      "Train Epoch: 14 [8448/54000 (16%)] Loss: -202009.953125\n",
      "Train Epoch: 14 [9856/54000 (18%)] Loss: -211747.562500\n",
      "Train Epoch: 14 [11264/54000 (21%)] Loss: -179610.312500\n",
      "Train Epoch: 14 [12672/54000 (23%)] Loss: -194716.890625\n",
      "Train Epoch: 14 [14080/54000 (26%)] Loss: -178413.812500\n",
      "Train Epoch: 14 [15488/54000 (29%)] Loss: -212700.671875\n",
      "Train Epoch: 14 [16896/54000 (31%)] Loss: -184721.593750\n",
      "Train Epoch: 14 [18304/54000 (34%)] Loss: -190852.343750\n",
      "Train Epoch: 14 [19712/54000 (37%)] Loss: -178229.375000\n",
      "Train Epoch: 14 [21120/54000 (39%)] Loss: -194959.875000\n",
      "Train Epoch: 14 [22528/54000 (42%)] Loss: -195700.406250\n",
      "Train Epoch: 14 [23936/54000 (44%)] Loss: -166793.015625\n",
      "Train Epoch: 14 [25344/54000 (47%)] Loss: -191255.484375\n",
      "Train Epoch: 14 [26752/54000 (50%)] Loss: -196716.875000\n",
      "Train Epoch: 14 [28160/54000 (52%)] Loss: -183049.468750\n",
      "Train Epoch: 14 [29568/54000 (55%)] Loss: -188383.468750\n",
      "Train Epoch: 14 [30976/54000 (57%)] Loss: -154750.250000\n",
      "Train Epoch: 14 [32384/54000 (60%)] Loss: -196073.171875\n",
      "Train Epoch: 14 [33792/54000 (63%)] Loss: -207382.796875\n",
      "Train Epoch: 14 [35200/54000 (65%)] Loss: -182083.250000\n",
      "Train Epoch: 14 [36608/54000 (68%)] Loss: -173445.578125\n",
      "Train Epoch: 14 [38016/54000 (70%)] Loss: -170560.093750\n",
      "Train Epoch: 14 [39424/54000 (73%)] Loss: -165795.234375\n",
      "Train Epoch: 14 [40832/54000 (76%)] Loss: -185993.218750\n",
      "Train Epoch: 14 [42240/54000 (78%)] Loss: -188226.578125\n",
      "Train Epoch: 14 [43648/54000 (81%)] Loss: -198979.312500\n",
      "Train Epoch: 14 [45056/54000 (83%)] Loss: -201955.171875\n",
      "Train Epoch: 14 [46464/54000 (86%)] Loss: -190230.000000\n",
      "Train Epoch: 14 [47872/54000 (89%)] Loss: -155999.906250\n",
      "Train Epoch: 14 [49280/54000 (91%)] Loss: -189296.765625\n",
      "Train Epoch: 14 [50688/54000 (94%)] Loss: -206657.187500\n",
      "Train Epoch: 14 [52096/54000 (96%)] Loss: -195596.843750\n",
      "    epoch          : 14\n",
      "    loss           : -186872.176659689\n",
      "    val_loss       : -195235.18599942836\n",
      "Train Epoch: 15 [0/54000 (0%)] Loss: -173404.875000\n",
      "Train Epoch: 15 [1408/54000 (3%)] Loss: -212372.609375\n",
      "Train Epoch: 15 [2816/54000 (5%)] Loss: -184041.687500\n",
      "Train Epoch: 15 [4224/54000 (8%)] Loss: -200181.062500\n",
      "Train Epoch: 15 [5632/54000 (10%)] Loss: -202417.140625\n",
      "Train Epoch: 15 [7040/54000 (13%)] Loss: -166719.828125\n",
      "Train Epoch: 15 [8448/54000 (16%)] Loss: -154653.437500\n",
      "Train Epoch: 15 [9856/54000 (18%)] Loss: -169937.875000\n",
      "Train Epoch: 15 [11264/54000 (21%)] Loss: -196077.593750\n",
      "Train Epoch: 15 [12672/54000 (23%)] Loss: -186758.437500\n",
      "Train Epoch: 15 [14080/54000 (26%)] Loss: -191176.062500\n",
      "Train Epoch: 15 [15488/54000 (29%)] Loss: -213306.734375\n",
      "Train Epoch: 15 [16896/54000 (31%)] Loss: -196568.250000\n",
      "Train Epoch: 15 [18304/54000 (34%)] Loss: -188575.500000\n",
      "Train Epoch: 15 [19712/54000 (37%)] Loss: -171272.781250\n",
      "Train Epoch: 15 [21120/54000 (39%)] Loss: -169501.406250\n",
      "Train Epoch: 15 [22528/54000 (42%)] Loss: -170942.843750\n",
      "Train Epoch: 15 [23936/54000 (44%)] Loss: -191325.218750\n",
      "Train Epoch: 15 [25344/54000 (47%)] Loss: -177127.718750\n",
      "Train Epoch: 15 [26752/54000 (50%)] Loss: -212232.703125\n",
      "Train Epoch: 15 [28160/54000 (52%)] Loss: -191255.875000\n",
      "Train Epoch: 15 [29568/54000 (55%)] Loss: -186073.000000\n",
      "Train Epoch: 15 [30976/54000 (57%)] Loss: -184964.140625\n",
      "Train Epoch: 15 [32384/54000 (60%)] Loss: -196985.906250\n",
      "Train Epoch: 15 [33792/54000 (63%)] Loss: -187471.046875\n",
      "Train Epoch: 15 [35200/54000 (65%)] Loss: -200138.906250\n",
      "Train Epoch: 15 [36608/54000 (68%)] Loss: -213013.218750\n",
      "Train Epoch: 15 [38016/54000 (70%)] Loss: -185427.187500\n",
      "Train Epoch: 15 [39424/54000 (73%)] Loss: -191692.843750\n",
      "Train Epoch: 15 [40832/54000 (76%)] Loss: -191811.375000\n",
      "Train Epoch: 15 [42240/54000 (78%)] Loss: -208646.968750\n",
      "Train Epoch: 15 [43648/54000 (81%)] Loss: -184143.359375\n",
      "Train Epoch: 15 [45056/54000 (83%)] Loss: -187615.828125\n",
      "Train Epoch: 15 [46464/54000 (86%)] Loss: -190998.062500\n",
      "Train Epoch: 15 [47872/54000 (89%)] Loss: -184696.781250\n",
      "Train Epoch: 15 [49280/54000 (91%)] Loss: -185954.812500\n",
      "Train Epoch: 15 [50688/54000 (94%)] Loss: -189289.937500\n",
      "Train Epoch: 15 [52096/54000 (96%)] Loss: -166649.656250\n",
      "    epoch          : 15\n",
      "    loss           : -187798.2350104665\n",
      "    val_loss       : -193918.8367116044\n",
      "Train Epoch: 16 [0/54000 (0%)] Loss: -171885.765625\n",
      "Train Epoch: 16 [1408/54000 (3%)] Loss: -165378.781250\n",
      "Train Epoch: 16 [2816/54000 (5%)] Loss: -181107.625000\n",
      "Train Epoch: 16 [4224/54000 (8%)] Loss: -194896.343750\n",
      "Train Epoch: 16 [5632/54000 (10%)] Loss: -183228.484375\n",
      "Train Epoch: 16 [7040/54000 (13%)] Loss: -188630.312500\n",
      "Train Epoch: 16 [8448/54000 (16%)] Loss: -191885.125000\n",
      "Train Epoch: 16 [9856/54000 (18%)] Loss: -187505.453125\n",
      "Train Epoch: 16 [11264/54000 (21%)] Loss: -155039.765625\n",
      "Train Epoch: 16 [12672/54000 (23%)] Loss: -191937.812500\n",
      "Train Epoch: 16 [14080/54000 (26%)] Loss: -212587.453125\n",
      "Train Epoch: 16 [15488/54000 (29%)] Loss: -192663.437500\n",
      "Train Epoch: 16 [16896/54000 (31%)] Loss: -186660.093750\n",
      "Train Epoch: 16 [18304/54000 (34%)] Loss: -182391.296875\n",
      "Train Epoch: 16 [19712/54000 (37%)] Loss: -184342.937500\n",
      "Train Epoch: 16 [21120/54000 (39%)] Loss: -185387.250000\n",
      "Train Epoch: 16 [22528/54000 (42%)] Loss: -184069.031250\n",
      "Train Epoch: 16 [23936/54000 (44%)] Loss: -187171.656250\n",
      "Train Epoch: 16 [25344/54000 (47%)] Loss: -194798.781250\n",
      "Train Epoch: 16 [26752/54000 (50%)] Loss: -195563.187500\n",
      "Train Epoch: 16 [28160/54000 (52%)] Loss: -194088.625000\n",
      "Train Epoch: 16 [29568/54000 (55%)] Loss: -186784.781250\n",
      "Train Epoch: 16 [30976/54000 (57%)] Loss: -175234.015625\n",
      "Train Epoch: 16 [32384/54000 (60%)] Loss: -190422.906250\n",
      "Train Epoch: 16 [33792/54000 (63%)] Loss: -193640.031250\n",
      "Train Epoch: 16 [35200/54000 (65%)] Loss: -192727.750000\n",
      "Train Epoch: 16 [36608/54000 (68%)] Loss: -184991.406250\n",
      "Train Epoch: 16 [38016/54000 (70%)] Loss: -184243.078125\n",
      "Train Epoch: 16 [39424/54000 (73%)] Loss: -188065.656250\n",
      "Train Epoch: 16 [40832/54000 (76%)] Loss: -164264.468750\n",
      "Train Epoch: 16 [42240/54000 (78%)] Loss: -195323.531250\n",
      "Train Epoch: 16 [43648/54000 (81%)] Loss: -183799.421875\n",
      "Train Epoch: 16 [45056/54000 (83%)] Loss: -185624.843750\n",
      "Train Epoch: 16 [46464/54000 (86%)] Loss: -187035.109375\n",
      "Train Epoch: 16 [47872/54000 (89%)] Loss: -187524.937500\n",
      "Train Epoch: 16 [49280/54000 (91%)] Loss: -198022.968750\n",
      "Train Epoch: 16 [50688/54000 (94%)] Loss: -215752.109375\n",
      "Train Epoch: 16 [52096/54000 (96%)] Loss: -183045.687500\n",
      "    epoch          : 16\n",
      "    loss           : -188108.49794407896\n",
      "    val_loss       : -195319.56810927973\n",
      "Train Epoch: 17 [0/54000 (0%)] Loss: -216041.281250\n",
      "Train Epoch: 17 [1408/54000 (3%)] Loss: -185832.593750\n",
      "Train Epoch: 17 [2816/54000 (5%)] Loss: -193782.953125\n",
      "Train Epoch: 17 [4224/54000 (8%)] Loss: -187227.640625\n",
      "Train Epoch: 17 [5632/54000 (10%)] Loss: -192937.093750\n",
      "Train Epoch: 17 [7040/54000 (13%)] Loss: -173450.359375\n",
      "Train Epoch: 17 [8448/54000 (16%)] Loss: -195798.125000\n",
      "Train Epoch: 17 [9856/54000 (18%)] Loss: -195818.593750\n",
      "Train Epoch: 17 [11264/54000 (21%)] Loss: -196505.031250\n",
      "Train Epoch: 17 [12672/54000 (23%)] Loss: -172188.171875\n",
      "Train Epoch: 17 [14080/54000 (26%)] Loss: -197232.093750\n",
      "Train Epoch: 17 [15488/54000 (29%)] Loss: -188278.031250\n",
      "Train Epoch: 17 [16896/54000 (31%)] Loss: -187622.500000\n",
      "Train Epoch: 17 [18304/54000 (34%)] Loss: -180613.234375\n",
      "Train Epoch: 17 [19712/54000 (37%)] Loss: -175577.093750\n",
      "Train Epoch: 17 [21120/54000 (39%)] Loss: -204293.046875\n",
      "Train Epoch: 17 [22528/54000 (42%)] Loss: -196018.109375\n",
      "Train Epoch: 17 [23936/54000 (44%)] Loss: -214672.656250\n",
      "Train Epoch: 17 [25344/54000 (47%)] Loss: -197890.890625\n",
      "Train Epoch: 17 [26752/54000 (50%)] Loss: -186424.562500\n",
      "Train Epoch: 17 [28160/54000 (52%)] Loss: -174188.500000\n",
      "Train Epoch: 17 [29568/54000 (55%)] Loss: -191944.187500\n",
      "Train Epoch: 17 [30976/54000 (57%)] Loss: -194319.234375\n",
      "Train Epoch: 17 [32384/54000 (60%)] Loss: -191078.265625\n",
      "Train Epoch: 17 [33792/54000 (63%)] Loss: -181972.843750\n",
      "Train Epoch: 17 [35200/54000 (65%)] Loss: -199506.656250\n",
      "Train Epoch: 17 [36608/54000 (68%)] Loss: -216452.937500\n",
      "Train Epoch: 17 [38016/54000 (70%)] Loss: -190519.187500\n",
      "Train Epoch: 17 [39424/54000 (73%)] Loss: -195960.593750\n",
      "Train Epoch: 17 [40832/54000 (76%)] Loss: -214508.250000\n",
      "Train Epoch: 17 [42240/54000 (78%)] Loss: -199191.484375\n",
      "Train Epoch: 17 [43648/54000 (81%)] Loss: -182685.468750\n",
      "Train Epoch: 17 [45056/54000 (83%)] Loss: -185369.406250\n",
      "Train Epoch: 17 [46464/54000 (86%)] Loss: -216598.875000\n",
      "Train Epoch: 17 [47872/54000 (89%)] Loss: -188177.375000\n",
      "Train Epoch: 17 [49280/54000 (91%)] Loss: -178598.343750\n",
      "Train Epoch: 17 [50688/54000 (94%)] Loss: -175089.312500\n",
      "Train Epoch: 17 [52096/54000 (96%)] Loss: -215369.921875\n",
      "    epoch          : 17\n",
      "    loss           : -189592.63449461723\n",
      "    val_loss       : -196378.336735423\n",
      "Train Epoch: 18 [0/54000 (0%)] Loss: -215483.796875\n",
      "Train Epoch: 18 [1408/54000 (3%)] Loss: -194926.312500\n",
      "Train Epoch: 18 [2816/54000 (5%)] Loss: -178703.906250\n",
      "Train Epoch: 18 [4224/54000 (8%)] Loss: -197162.750000\n",
      "Train Epoch: 18 [5632/54000 (10%)] Loss: -203739.062500\n",
      "Train Epoch: 18 [7040/54000 (13%)] Loss: -199149.062500\n",
      "Train Epoch: 18 [8448/54000 (16%)] Loss: -174005.687500\n",
      "Train Epoch: 18 [9856/54000 (18%)] Loss: -184155.093750\n",
      "Train Epoch: 18 [11264/54000 (21%)] Loss: -198093.937500\n",
      "Train Epoch: 18 [12672/54000 (23%)] Loss: -196362.781250\n",
      "Train Epoch: 18 [14080/54000 (26%)] Loss: -189191.796875\n",
      "Train Epoch: 18 [15488/54000 (29%)] Loss: -187468.656250\n",
      "Train Epoch: 18 [16896/54000 (31%)] Loss: -174166.531250\n",
      "Train Epoch: 18 [18304/54000 (34%)] Loss: -174255.281250\n",
      "Train Epoch: 18 [19712/54000 (37%)] Loss: -170808.343750\n",
      "Train Epoch: 18 [21120/54000 (39%)] Loss: -196949.093750\n",
      "Train Epoch: 18 [22528/54000 (42%)] Loss: -187544.562500\n",
      "Train Epoch: 18 [23936/54000 (44%)] Loss: -197758.562500\n",
      "Train Epoch: 18 [25344/54000 (47%)] Loss: -215432.765625\n",
      "Train Epoch: 18 [26752/54000 (50%)] Loss: -186928.281250\n",
      "Train Epoch: 18 [28160/54000 (52%)] Loss: -197874.078125\n",
      "Train Epoch: 18 [29568/54000 (55%)] Loss: -198837.062500\n",
      "Train Epoch: 18 [30976/54000 (57%)] Loss: -189123.593750\n",
      "Train Epoch: 18 [32384/54000 (60%)] Loss: -192398.468750\n",
      "Train Epoch: 18 [33792/54000 (63%)] Loss: -190383.718750\n",
      "Train Epoch: 18 [35200/54000 (65%)] Loss: -166281.218750\n",
      "Train Epoch: 18 [36608/54000 (68%)] Loss: -174894.875000\n",
      "Train Epoch: 18 [38016/54000 (70%)] Loss: -172288.843750\n",
      "Train Epoch: 18 [39424/54000 (73%)] Loss: -185584.281250\n",
      "Train Epoch: 18 [40832/54000 (76%)] Loss: -172694.468750\n",
      "Train Epoch: 18 [42240/54000 (78%)] Loss: -171368.218750\n",
      "Train Epoch: 18 [43648/54000 (81%)] Loss: -204424.781250\n",
      "Train Epoch: 18 [45056/54000 (83%)] Loss: -165700.828125\n",
      "Train Epoch: 18 [46464/54000 (86%)] Loss: -188619.375000\n",
      "Train Epoch: 18 [47872/54000 (89%)] Loss: -214702.281250\n",
      "Train Epoch: 18 [49280/54000 (91%)] Loss: -198030.625000\n",
      "Train Epoch: 18 [50688/54000 (94%)] Loss: -216454.000000\n",
      "Train Epoch: 18 [52096/54000 (96%)] Loss: -200667.359375\n",
      "    epoch          : 18\n",
      "    loss           : -190484.26928827752\n",
      "    val_loss       : -198787.84149914252\n",
      "Train Epoch: 19 [0/54000 (0%)] Loss: -216296.843750\n",
      "Train Epoch: 19 [1408/54000 (3%)] Loss: -201104.390625\n",
      "Train Epoch: 19 [2816/54000 (5%)] Loss: -195926.203125\n",
      "Train Epoch: 19 [4224/54000 (8%)] Loss: -170948.718750\n",
      "Train Epoch: 19 [5632/54000 (10%)] Loss: -181269.921875\n",
      "Train Epoch: 19 [7040/54000 (13%)] Loss: -184106.812500\n",
      "Train Epoch: 19 [8448/54000 (16%)] Loss: -180501.453125\n",
      "Train Epoch: 19 [9856/54000 (18%)] Loss: -207692.234375\n",
      "Train Epoch: 19 [11264/54000 (21%)] Loss: -167380.906250\n",
      "Train Epoch: 19 [12672/54000 (23%)] Loss: -189141.953125\n",
      "Train Epoch: 19 [14080/54000 (26%)] Loss: -199348.421875\n",
      "Train Epoch: 19 [15488/54000 (29%)] Loss: -206875.484375\n",
      "Train Epoch: 19 [16896/54000 (31%)] Loss: -190854.328125\n",
      "Train Epoch: 19 [18304/54000 (34%)] Loss: -173810.343750\n",
      "Train Epoch: 19 [19712/54000 (37%)] Loss: -170283.781250\n",
      "Train Epoch: 19 [21120/54000 (39%)] Loss: -199368.546875\n",
      "Train Epoch: 19 [22528/54000 (42%)] Loss: -185629.984375\n",
      "Train Epoch: 19 [23936/54000 (44%)] Loss: -192971.750000\n",
      "Train Epoch: 19 [25344/54000 (47%)] Loss: -187888.640625\n",
      "Train Epoch: 19 [26752/54000 (50%)] Loss: -210765.937500\n",
      "Train Epoch: 19 [28160/54000 (52%)] Loss: -178423.187500\n",
      "Train Epoch: 19 [29568/54000 (55%)] Loss: -191230.656250\n",
      "Train Epoch: 19 [30976/54000 (57%)] Loss: -200696.687500\n",
      "Train Epoch: 19 [32384/54000 (60%)] Loss: -188077.375000\n",
      "Train Epoch: 19 [33792/54000 (63%)] Loss: -184384.656250\n",
      "Train Epoch: 19 [35200/54000 (65%)] Loss: -174911.031250\n",
      "Train Epoch: 19 [36608/54000 (68%)] Loss: -199219.140625\n",
      "Train Epoch: 19 [38016/54000 (70%)] Loss: -186375.562500\n",
      "Train Epoch: 19 [39424/54000 (73%)] Loss: -199113.687500\n",
      "Train Epoch: 19 [40832/54000 (76%)] Loss: -184313.671875\n",
      "Train Epoch: 19 [42240/54000 (78%)] Loss: -196989.718750\n",
      "Train Epoch: 19 [43648/54000 (81%)] Loss: -195459.718750\n",
      "Train Epoch: 19 [45056/54000 (83%)] Loss: -184936.968750\n",
      "Train Epoch: 19 [46464/54000 (86%)] Loss: -203551.750000\n",
      "Train Epoch: 19 [47872/54000 (89%)] Loss: -209425.531250\n",
      "Train Epoch: 19 [49280/54000 (91%)] Loss: -198197.578125\n",
      "Train Epoch: 19 [50688/54000 (94%)] Loss: -186056.171875\n",
      "Train Epoch: 19 [52096/54000 (96%)] Loss: -182443.484375\n",
      "    epoch          : 19\n",
      "    loss           : -189919.7639055024\n",
      "    val_loss       : -197009.10998237424\n",
      "Train Epoch: 20 [0/54000 (0%)] Loss: -217118.015625\n",
      "Train Epoch: 20 [1408/54000 (3%)] Loss: -195861.218750\n",
      "Train Epoch: 20 [2816/54000 (5%)] Loss: -187958.656250\n",
      "Train Epoch: 20 [4224/54000 (8%)] Loss: -186204.984375\n",
      "Train Epoch: 20 [5632/54000 (10%)] Loss: -175833.937500\n",
      "Train Epoch: 20 [7040/54000 (13%)] Loss: -174734.000000\n",
      "Train Epoch: 20 [8448/54000 (16%)] Loss: -173240.875000\n",
      "Train Epoch: 20 [9856/54000 (18%)] Loss: -183165.609375\n",
      "Train Epoch: 20 [11264/54000 (21%)] Loss: -183575.734375\n",
      "Train Epoch: 20 [12672/54000 (23%)] Loss: -176416.828125\n",
      "Train Epoch: 20 [14080/54000 (26%)] Loss: -187667.187500\n",
      "Train Epoch: 20 [15488/54000 (29%)] Loss: -211006.671875\n",
      "Train Epoch: 20 [16896/54000 (31%)] Loss: -171055.781250\n",
      "Train Epoch: 20 [18304/54000 (34%)] Loss: -217767.890625\n",
      "Train Epoch: 20 [19712/54000 (37%)] Loss: -199799.531250\n",
      "Train Epoch: 20 [21120/54000 (39%)] Loss: -182082.218750\n",
      "Train Epoch: 20 [22528/54000 (42%)] Loss: -192127.125000\n",
      "Train Epoch: 20 [23936/54000 (44%)] Loss: -161880.953125\n",
      "Train Epoch: 20 [25344/54000 (47%)] Loss: -162081.125000\n",
      "Train Epoch: 20 [26752/54000 (50%)] Loss: -184676.796875\n",
      "Train Epoch: 20 [28160/54000 (52%)] Loss: -180802.625000\n",
      "Train Epoch: 20 [29568/54000 (55%)] Loss: -188649.859375\n",
      "Train Epoch: 20 [30976/54000 (57%)] Loss: -182156.187500\n",
      "Train Epoch: 20 [32384/54000 (60%)] Loss: -188081.078125\n",
      "Train Epoch: 20 [33792/54000 (63%)] Loss: -204235.531250\n",
      "Train Epoch: 20 [35200/54000 (65%)] Loss: -192967.156250\n",
      "Train Epoch: 20 [36608/54000 (68%)] Loss: -177774.187500\n",
      "Train Epoch: 20 [38016/54000 (70%)] Loss: -214947.406250\n",
      "Train Epoch: 20 [39424/54000 (73%)] Loss: -213805.187500\n",
      "Train Epoch: 20 [40832/54000 (76%)] Loss: -175550.968750\n",
      "Train Epoch: 20 [42240/54000 (78%)] Loss: -193121.281250\n",
      "Train Epoch: 20 [43648/54000 (81%)] Loss: -199101.968750\n",
      "Train Epoch: 20 [45056/54000 (83%)] Loss: -189935.609375\n",
      "Train Epoch: 20 [46464/54000 (86%)] Loss: -210879.156250\n",
      "Train Epoch: 20 [47872/54000 (89%)] Loss: -195351.734375\n",
      "Train Epoch: 20 [49280/54000 (91%)] Loss: -187580.843750\n",
      "Train Epoch: 20 [50688/54000 (94%)] Loss: -174700.312500\n",
      "Train Epoch: 20 [52096/54000 (96%)] Loss: -216713.421875\n",
      "    epoch          : 20\n",
      "    loss           : -190180.64081190192\n",
      "    val_loss       : -198041.73090939404\n",
      "Train Epoch: 21 [0/54000 (0%)] Loss: -217377.000000\n",
      "Train Epoch: 21 [1408/54000 (3%)] Loss: -218609.625000\n",
      "Train Epoch: 21 [2816/54000 (5%)] Loss: -182501.093750\n",
      "Train Epoch: 21 [4224/54000 (8%)] Loss: -188143.515625\n",
      "Train Epoch: 21 [5632/54000 (10%)] Loss: -200694.531250\n",
      "Train Epoch: 21 [7040/54000 (13%)] Loss: -188244.890625\n",
      "Train Epoch: 21 [8448/54000 (16%)] Loss: -216511.203125\n",
      "Train Epoch: 21 [9856/54000 (18%)] Loss: -175441.640625\n",
      "Train Epoch: 21 [11264/54000 (21%)] Loss: -195062.000000\n",
      "Train Epoch: 21 [12672/54000 (23%)] Loss: -206097.015625\n",
      "Train Epoch: 21 [14080/54000 (26%)] Loss: -195155.890625\n",
      "Train Epoch: 21 [15488/54000 (29%)] Loss: -170969.453125\n",
      "Train Epoch: 21 [16896/54000 (31%)] Loss: -179255.484375\n",
      "Train Epoch: 21 [18304/54000 (34%)] Loss: -187610.953125\n",
      "Train Epoch: 21 [19712/54000 (37%)] Loss: -186560.437500\n",
      "Train Epoch: 21 [21120/54000 (39%)] Loss: -206115.843750\n",
      "Train Epoch: 21 [22528/54000 (42%)] Loss: -199631.656250\n",
      "Train Epoch: 21 [23936/54000 (44%)] Loss: -217470.359375\n",
      "Train Epoch: 21 [25344/54000 (47%)] Loss: -164586.937500\n",
      "Train Epoch: 21 [26752/54000 (50%)] Loss: -172511.609375\n",
      "Train Epoch: 21 [28160/54000 (52%)] Loss: -171568.171875\n",
      "Train Epoch: 21 [29568/54000 (55%)] Loss: -188464.265625\n",
      "Train Epoch: 21 [30976/54000 (57%)] Loss: -190636.156250\n",
      "Train Epoch: 21 [32384/54000 (60%)] Loss: -186896.156250\n",
      "Train Epoch: 21 [33792/54000 (63%)] Loss: -198072.250000\n",
      "Train Epoch: 21 [35200/54000 (65%)] Loss: -183077.593750\n",
      "Train Epoch: 21 [36608/54000 (68%)] Loss: -176900.500000\n",
      "Train Epoch: 21 [38016/54000 (70%)] Loss: -203225.390625\n",
      "Train Epoch: 21 [39424/54000 (73%)] Loss: -176235.609375\n",
      "Train Epoch: 21 [40832/54000 (76%)] Loss: -176603.203125\n",
      "Train Epoch: 21 [42240/54000 (78%)] Loss: -212447.203125\n",
      "Train Epoch: 21 [43648/54000 (81%)] Loss: -187452.468750\n",
      "Train Epoch: 21 [45056/54000 (83%)] Loss: -187390.406250\n",
      "Train Epoch: 21 [46464/54000 (86%)] Loss: -188026.812500\n",
      "Train Epoch: 21 [47872/54000 (89%)] Loss: -187242.484375\n",
      "Train Epoch: 21 [49280/54000 (91%)] Loss: -189020.218750\n",
      "Train Epoch: 21 [50688/54000 (94%)] Loss: -217137.281250\n",
      "Train Epoch: 21 [52096/54000 (96%)] Loss: -212935.015625\n",
      "    epoch          : 21\n",
      "    loss           : -190495.89305472487\n",
      "    val_loss       : -197270.65354658916\n",
      "Train Epoch: 22 [0/54000 (0%)] Loss: -215970.812500\n",
      "Train Epoch: 22 [1408/54000 (3%)] Loss: -209092.546875\n",
      "Train Epoch: 22 [2816/54000 (5%)] Loss: -177418.328125\n",
      "Train Epoch: 22 [4224/54000 (8%)] Loss: -190372.140625\n",
      "Train Epoch: 22 [5632/54000 (10%)] Loss: -194316.828125\n",
      "Train Epoch: 22 [7040/54000 (13%)] Loss: -167433.109375\n",
      "Train Epoch: 22 [8448/54000 (16%)] Loss: -188574.062500\n",
      "Train Epoch: 22 [9856/54000 (18%)] Loss: -187119.171875\n",
      "Train Epoch: 22 [11264/54000 (21%)] Loss: -205981.421875\n",
      "Train Epoch: 22 [12672/54000 (23%)] Loss: -187391.109375\n",
      "Train Epoch: 22 [14080/54000 (26%)] Loss: -190789.093750\n",
      "Train Epoch: 22 [15488/54000 (29%)] Loss: -188137.875000\n",
      "Train Epoch: 22 [16896/54000 (31%)] Loss: -190462.906250\n",
      "Train Epoch: 22 [18304/54000 (34%)] Loss: -184877.000000\n",
      "Train Epoch: 22 [19712/54000 (37%)] Loss: -185545.781250\n",
      "Train Epoch: 22 [21120/54000 (39%)] Loss: -190149.906250\n",
      "Train Epoch: 22 [22528/54000 (42%)] Loss: -187937.218750\n",
      "Train Epoch: 22 [23936/54000 (44%)] Loss: -186405.281250\n",
      "Train Epoch: 22 [25344/54000 (47%)] Loss: -168838.718750\n",
      "Train Epoch: 22 [26752/54000 (50%)] Loss: -190089.812500\n",
      "Train Epoch: 22 [28160/54000 (52%)] Loss: -176447.218750\n",
      "Train Epoch: 22 [29568/54000 (55%)] Loss: -177380.656250\n",
      "Train Epoch: 22 [30976/54000 (57%)] Loss: -182630.531250\n",
      "Train Epoch: 22 [32384/54000 (60%)] Loss: -212095.093750\n",
      "Train Epoch: 22 [33792/54000 (63%)] Loss: -205668.296875\n",
      "Train Epoch: 22 [35200/54000 (65%)] Loss: -206831.015625\n",
      "Train Epoch: 22 [36608/54000 (68%)] Loss: -187153.484375\n",
      "Train Epoch: 22 [38016/54000 (70%)] Loss: -189377.515625\n",
      "Train Epoch: 22 [39424/54000 (73%)] Loss: -218248.843750\n",
      "Train Epoch: 22 [40832/54000 (76%)] Loss: -193553.859375\n",
      "Train Epoch: 22 [42240/54000 (78%)] Loss: -185439.312500\n",
      "Train Epoch: 22 [43648/54000 (81%)] Loss: -174236.421875\n",
      "Train Epoch: 22 [45056/54000 (83%)] Loss: -175892.156250\n",
      "Train Epoch: 22 [46464/54000 (86%)] Loss: -173058.437500\n",
      "Train Epoch: 22 [47872/54000 (89%)] Loss: -200026.437500\n",
      "Train Epoch: 22 [49280/54000 (91%)] Loss: -173364.218750\n",
      "Train Epoch: 22 [50688/54000 (94%)] Loss: -212789.140625\n",
      "Train Epoch: 22 [52096/54000 (96%)] Loss: -202222.906250\n",
      "    epoch          : 22\n",
      "    loss           : -191437.42475328947\n",
      "    val_loss       : -199727.37673875762\n",
      "Train Epoch: 23 [0/54000 (0%)] Loss: -182587.812500\n",
      "Train Epoch: 23 [1408/54000 (3%)] Loss: -214382.781250\n",
      "Train Epoch: 23 [2816/54000 (5%)] Loss: -194464.031250\n",
      "Train Epoch: 23 [4224/54000 (8%)] Loss: -185634.437500\n",
      "Train Epoch: 23 [5632/54000 (10%)] Loss: -188890.484375\n",
      "Train Epoch: 23 [7040/54000 (13%)] Loss: -188978.984375\n",
      "Train Epoch: 23 [8448/54000 (16%)] Loss: -170141.250000\n",
      "Train Epoch: 23 [9856/54000 (18%)] Loss: -212613.250000\n",
      "Train Epoch: 23 [11264/54000 (21%)] Loss: -173941.968750\n",
      "Train Epoch: 23 [12672/54000 (23%)] Loss: -208386.609375\n",
      "Train Epoch: 23 [14080/54000 (26%)] Loss: -204022.156250\n",
      "Train Epoch: 23 [15488/54000 (29%)] Loss: -209015.562500\n",
      "Train Epoch: 23 [16896/54000 (31%)] Loss: -208763.515625\n",
      "Train Epoch: 23 [18304/54000 (34%)] Loss: -193228.031250\n",
      "Train Epoch: 23 [19712/54000 (37%)] Loss: -212970.187500\n",
      "Train Epoch: 23 [21120/54000 (39%)] Loss: -200334.671875\n",
      "Train Epoch: 23 [22528/54000 (42%)] Loss: -175653.031250\n",
      "Train Epoch: 23 [23936/54000 (44%)] Loss: -173862.468750\n",
      "Train Epoch: 23 [25344/54000 (47%)] Loss: -171286.156250\n",
      "Train Epoch: 23 [26752/54000 (50%)] Loss: -188529.750000\n",
      "Train Epoch: 23 [28160/54000 (52%)] Loss: -189763.093750\n",
      "Train Epoch: 23 [29568/54000 (55%)] Loss: -206525.843750\n",
      "Train Epoch: 23 [30976/54000 (57%)] Loss: -178480.062500\n",
      "Train Epoch: 23 [32384/54000 (60%)] Loss: -218072.984375\n",
      "Train Epoch: 23 [33792/54000 (63%)] Loss: -183708.625000\n",
      "Train Epoch: 23 [35200/54000 (65%)] Loss: -181744.187500\n",
      "Train Epoch: 23 [36608/54000 (68%)] Loss: -189503.812500\n",
      "Train Epoch: 23 [38016/54000 (70%)] Loss: -183187.437500\n",
      "Train Epoch: 23 [39424/54000 (73%)] Loss: -177533.906250\n",
      "Train Epoch: 23 [40832/54000 (76%)] Loss: -187420.296875\n",
      "Train Epoch: 23 [42240/54000 (78%)] Loss: -189967.484375\n",
      "Train Epoch: 23 [43648/54000 (81%)] Loss: -211590.812500\n",
      "Train Epoch: 23 [45056/54000 (83%)] Loss: -173139.015625\n",
      "Train Epoch: 23 [46464/54000 (86%)] Loss: -177874.750000\n",
      "Train Epoch: 23 [47872/54000 (89%)] Loss: -190654.000000\n",
      "Train Epoch: 23 [49280/54000 (91%)] Loss: -176262.562500\n",
      "Train Epoch: 23 [50688/54000 (94%)] Loss: -190028.125000\n",
      "Train Epoch: 23 [52096/54000 (96%)] Loss: -188579.906250\n",
      "    epoch          : 23\n",
      "    loss           : -192044.46803977274\n",
      "    val_loss       : -200430.72333508002\n",
      "Train Epoch: 24 [0/54000 (0%)] Loss: -172720.656250\n",
      "Train Epoch: 24 [1408/54000 (3%)] Loss: -175676.171875\n",
      "Train Epoch: 24 [2816/54000 (5%)] Loss: -177303.531250\n",
      "Train Epoch: 24 [4224/54000 (8%)] Loss: -199955.984375\n",
      "Train Epoch: 24 [5632/54000 (10%)] Loss: -190839.937500\n",
      "Train Epoch: 24 [7040/54000 (13%)] Loss: -216353.421875\n",
      "Train Epoch: 24 [8448/54000 (16%)] Loss: -191104.500000\n",
      "Train Epoch: 24 [9856/54000 (18%)] Loss: -187538.812500\n",
      "Train Epoch: 24 [11264/54000 (21%)] Loss: -208004.828125\n",
      "Train Epoch: 24 [12672/54000 (23%)] Loss: -219209.031250\n",
      "Train Epoch: 24 [14080/54000 (26%)] Loss: -196059.234375\n",
      "Train Epoch: 24 [15488/54000 (29%)] Loss: -182530.812500\n",
      "Train Epoch: 24 [16896/54000 (31%)] Loss: -190253.562500\n",
      "Train Epoch: 24 [18304/54000 (34%)] Loss: -219845.578125\n",
      "Train Epoch: 24 [19712/54000 (37%)] Loss: -178368.140625\n",
      "Train Epoch: 24 [21120/54000 (39%)] Loss: -177928.468750\n",
      "Train Epoch: 24 [22528/54000 (42%)] Loss: -207279.000000\n",
      "Train Epoch: 24 [23936/54000 (44%)] Loss: -177211.187500\n",
      "Train Epoch: 24 [25344/54000 (47%)] Loss: -180145.218750\n",
      "Train Epoch: 24 [26752/54000 (50%)] Loss: -172522.906250\n",
      "Train Epoch: 24 [28160/54000 (52%)] Loss: -199914.015625\n",
      "Train Epoch: 24 [29568/54000 (55%)] Loss: -193134.718750\n",
      "Train Epoch: 24 [30976/54000 (57%)] Loss: -195797.531250\n",
      "Train Epoch: 24 [32384/54000 (60%)] Loss: -218803.609375\n",
      "Train Epoch: 24 [33792/54000 (63%)] Loss: -190679.437500\n",
      "Train Epoch: 24 [35200/54000 (65%)] Loss: -203884.984375\n",
      "Train Epoch: 24 [36608/54000 (68%)] Loss: -213001.718750\n",
      "Train Epoch: 24 [38016/54000 (70%)] Loss: -176947.531250\n",
      "Train Epoch: 24 [39424/54000 (73%)] Loss: -204987.703125\n",
      "Train Epoch: 24 [40832/54000 (76%)] Loss: -166297.390625\n",
      "Train Epoch: 24 [42240/54000 (78%)] Loss: -201404.000000\n",
      "Train Epoch: 24 [43648/54000 (81%)] Loss: -188514.656250\n",
      "Train Epoch: 24 [45056/54000 (83%)] Loss: -188372.046875\n",
      "Train Epoch: 24 [46464/54000 (86%)] Loss: -197431.921875\n",
      "Train Epoch: 24 [47872/54000 (89%)] Loss: -201492.750000\n",
      "Train Epoch: 24 [49280/54000 (91%)] Loss: -191790.687500\n",
      "Train Epoch: 24 [50688/54000 (94%)] Loss: -217696.546875\n",
      "Train Epoch: 24 [52096/54000 (96%)] Loss: -192500.531250\n",
      "    epoch          : 24\n",
      "    loss           : -192743.83690938994\n",
      "    val_loss       : -200658.05704554115\n",
      "Train Epoch: 25 [0/54000 (0%)] Loss: -200419.125000\n",
      "Train Epoch: 25 [1408/54000 (3%)] Loss: -212400.687500\n",
      "Train Epoch: 25 [2816/54000 (5%)] Loss: -188661.812500\n",
      "Train Epoch: 25 [4224/54000 (8%)] Loss: -190142.250000\n",
      "Train Epoch: 25 [5632/54000 (10%)] Loss: -179888.609375\n",
      "Train Epoch: 25 [7040/54000 (13%)] Loss: -189523.078125\n",
      "Train Epoch: 25 [8448/54000 (16%)] Loss: -175866.453125\n",
      "Train Epoch: 25 [9856/54000 (18%)] Loss: -170389.515625\n",
      "Train Epoch: 25 [11264/54000 (21%)] Loss: -201040.562500\n",
      "Train Epoch: 25 [12672/54000 (23%)] Loss: -203212.328125\n",
      "Train Epoch: 25 [14080/54000 (26%)] Loss: -191918.812500\n",
      "Train Epoch: 25 [15488/54000 (29%)] Loss: -197592.046875\n",
      "Train Epoch: 25 [16896/54000 (31%)] Loss: -195524.359375\n",
      "Train Epoch: 25 [18304/54000 (34%)] Loss: -207372.156250\n",
      "Train Epoch: 25 [19712/54000 (37%)] Loss: -213588.437500\n",
      "Train Epoch: 25 [21120/54000 (39%)] Loss: -201257.078125\n",
      "Train Epoch: 25 [22528/54000 (42%)] Loss: -201719.109375\n",
      "Train Epoch: 25 [23936/54000 (44%)] Loss: -177658.390625\n",
      "Train Epoch: 25 [25344/54000 (47%)] Loss: -200719.765625\n",
      "Train Epoch: 25 [26752/54000 (50%)] Loss: -187156.312500\n",
      "Train Epoch: 25 [28160/54000 (52%)] Loss: -188601.156250\n",
      "Train Epoch: 25 [29568/54000 (55%)] Loss: -196935.984375\n",
      "Train Epoch: 25 [30976/54000 (57%)] Loss: -195614.390625\n",
      "Train Epoch: 25 [32384/54000 (60%)] Loss: -200983.765625\n",
      "Train Epoch: 25 [33792/54000 (63%)] Loss: -220551.953125\n",
      "Train Epoch: 25 [35200/54000 (65%)] Loss: -195401.890625\n",
      "Train Epoch: 25 [36608/54000 (68%)] Loss: -189018.031250\n",
      "Train Epoch: 25 [38016/54000 (70%)] Loss: -175884.468750\n",
      "Train Epoch: 25 [39424/54000 (73%)] Loss: -171644.609375\n",
      "Train Epoch: 25 [40832/54000 (76%)] Loss: -192584.250000\n",
      "Train Epoch: 25 [42240/54000 (78%)] Loss: -198818.687500\n",
      "Train Epoch: 25 [43648/54000 (81%)] Loss: -190807.359375\n",
      "Train Epoch: 25 [45056/54000 (83%)] Loss: -220695.765625\n",
      "Train Epoch: 25 [46464/54000 (86%)] Loss: -196107.656250\n",
      "Train Epoch: 25 [47872/54000 (89%)] Loss: -190939.000000\n",
      "Train Epoch: 25 [49280/54000 (91%)] Loss: -212006.609375\n",
      "Train Epoch: 25 [50688/54000 (94%)] Loss: -191940.796875\n",
      "Train Epoch: 25 [52096/54000 (96%)] Loss: -198132.218750\n",
      "    epoch          : 25\n",
      "    loss           : -194266.72304874402\n",
      "    val_loss       : -202963.22184641767\n",
      "Train Epoch: 26 [0/54000 (0%)] Loss: -211876.781250\n",
      "Train Epoch: 26 [1408/54000 (3%)] Loss: -217900.562500\n",
      "Train Epoch: 26 [2816/54000 (5%)] Loss: -182790.546875\n",
      "Train Epoch: 26 [4224/54000 (8%)] Loss: -188343.843750\n",
      "Train Epoch: 26 [5632/54000 (10%)] Loss: -191125.718750\n",
      "Train Epoch: 26 [7040/54000 (13%)] Loss: -198296.453125\n",
      "Train Epoch: 26 [8448/54000 (16%)] Loss: -176548.234375\n",
      "Train Epoch: 26 [9856/54000 (18%)] Loss: -204263.390625\n",
      "Train Epoch: 26 [11264/54000 (21%)] Loss: -196932.890625\n",
      "Train Epoch: 26 [12672/54000 (23%)] Loss: -194801.484375\n",
      "Train Epoch: 26 [14080/54000 (26%)] Loss: -210273.109375\n",
      "Train Epoch: 26 [15488/54000 (29%)] Loss: -193836.406250\n",
      "Train Epoch: 26 [16896/54000 (31%)] Loss: -174105.125000\n",
      "Train Epoch: 26 [18304/54000 (34%)] Loss: -188598.265625\n",
      "Train Epoch: 26 [19712/54000 (37%)] Loss: -194546.031250\n",
      "Train Epoch: 26 [21120/54000 (39%)] Loss: -190918.984375\n",
      "Train Epoch: 26 [22528/54000 (42%)] Loss: -195792.625000\n",
      "Train Epoch: 26 [23936/54000 (44%)] Loss: -188143.406250\n",
      "Train Epoch: 26 [25344/54000 (47%)] Loss: -197978.984375\n",
      "Train Epoch: 26 [26752/54000 (50%)] Loss: -196315.375000\n",
      "Train Epoch: 26 [28160/54000 (52%)] Loss: -195264.406250\n",
      "Train Epoch: 26 [29568/54000 (55%)] Loss: -210649.671875\n",
      "Train Epoch: 26 [30976/54000 (57%)] Loss: -202952.937500\n",
      "Train Epoch: 26 [32384/54000 (60%)] Loss: -181438.953125\n",
      "Train Epoch: 26 [33792/54000 (63%)] Loss: -177564.250000\n",
      "Train Epoch: 26 [35200/54000 (65%)] Loss: -193872.250000\n",
      "Train Epoch: 26 [36608/54000 (68%)] Loss: -185755.937500\n",
      "Train Epoch: 26 [38016/54000 (70%)] Loss: -219268.875000\n",
      "Train Epoch: 26 [39424/54000 (73%)] Loss: -218207.375000\n",
      "Train Epoch: 26 [40832/54000 (76%)] Loss: -187678.781250\n",
      "Train Epoch: 26 [42240/54000 (78%)] Loss: -209965.671875\n",
      "Train Epoch: 26 [43648/54000 (81%)] Loss: -188637.812500\n",
      "Train Epoch: 26 [45056/54000 (83%)] Loss: -199513.265625\n",
      "Train Epoch: 26 [46464/54000 (86%)] Loss: -213568.062500\n",
      "Train Epoch: 26 [47872/54000 (89%)] Loss: -202003.953125\n",
      "Train Epoch: 26 [49280/54000 (91%)] Loss: -184774.375000\n",
      "Train Epoch: 26 [50688/54000 (94%)] Loss: -191816.468750\n",
      "Train Epoch: 26 [52096/54000 (96%)] Loss: -179022.656250\n",
      "    epoch          : 26\n",
      "    loss           : -194829.96534838516\n",
      "    val_loss       : -202424.43990567836\n",
      "Train Epoch: 27 [0/54000 (0%)] Loss: -220717.750000\n",
      "Train Epoch: 27 [1408/54000 (3%)] Loss: -190710.921875\n",
      "Train Epoch: 27 [2816/54000 (5%)] Loss: -194660.937500\n",
      "Train Epoch: 27 [4224/54000 (8%)] Loss: -179650.156250\n",
      "Train Epoch: 27 [5632/54000 (10%)] Loss: -178094.421875\n",
      "Train Epoch: 27 [7040/54000 (13%)] Loss: -192869.718750\n",
      "Train Epoch: 27 [8448/54000 (16%)] Loss: -202708.765625\n",
      "Train Epoch: 27 [9856/54000 (18%)] Loss: -196602.531250\n",
      "Train Epoch: 27 [11264/54000 (21%)] Loss: -199227.609375\n",
      "Train Epoch: 27 [12672/54000 (23%)] Loss: -200388.484375\n",
      "Train Epoch: 27 [14080/54000 (26%)] Loss: -220220.078125\n",
      "Train Epoch: 27 [15488/54000 (29%)] Loss: -210852.328125\n",
      "Train Epoch: 27 [16896/54000 (31%)] Loss: -178989.812500\n",
      "Train Epoch: 27 [18304/54000 (34%)] Loss: -176044.375000\n",
      "Train Epoch: 27 [19712/54000 (37%)] Loss: -189752.375000\n",
      "Train Epoch: 27 [21120/54000 (39%)] Loss: -176850.000000\n",
      "Train Epoch: 27 [22528/54000 (42%)] Loss: -174717.062500\n",
      "Train Epoch: 27 [23936/54000 (44%)] Loss: -173697.421875\n",
      "Train Epoch: 27 [25344/54000 (47%)] Loss: -177602.812500\n",
      "Train Epoch: 27 [26752/54000 (50%)] Loss: -209247.281250\n",
      "Train Epoch: 27 [28160/54000 (52%)] Loss: -200675.781250\n",
      "Train Epoch: 27 [29568/54000 (55%)] Loss: -213634.484375\n",
      "Train Epoch: 27 [30976/54000 (57%)] Loss: -204441.828125\n",
      "Train Epoch: 27 [32384/54000 (60%)] Loss: -190240.875000\n",
      "Train Epoch: 27 [33792/54000 (63%)] Loss: -201030.281250\n",
      "Train Epoch: 27 [35200/54000 (65%)] Loss: -178329.359375\n",
      "Train Epoch: 27 [36608/54000 (68%)] Loss: -178143.359375\n",
      "Train Epoch: 27 [38016/54000 (70%)] Loss: -177837.281250\n",
      "Train Epoch: 27 [39424/54000 (73%)] Loss: -209067.312500\n",
      "Train Epoch: 27 [40832/54000 (76%)] Loss: -214992.062500\n",
      "Train Epoch: 27 [42240/54000 (78%)] Loss: -174541.843750\n",
      "Train Epoch: 27 [43648/54000 (81%)] Loss: -185930.578125\n",
      "Train Epoch: 27 [45056/54000 (83%)] Loss: -218684.718750\n",
      "Train Epoch: 27 [46464/54000 (86%)] Loss: -193369.437500\n",
      "Train Epoch: 27 [47872/54000 (89%)] Loss: -199598.718750\n",
      "Train Epoch: 27 [49280/54000 (91%)] Loss: -198469.203125\n",
      "Train Epoch: 27 [50688/54000 (94%)] Loss: -203332.984375\n",
      "Train Epoch: 27 [52096/54000 (96%)] Loss: -203210.375000\n",
      "    epoch          : 27\n",
      "    loss           : -195359.79044557415\n",
      "    val_loss       : -204316.63029963797\n",
      "Train Epoch: 28 [0/54000 (0%)] Loss: -192279.062500\n",
      "Train Epoch: 28 [1408/54000 (3%)] Loss: -189862.375000\n",
      "Train Epoch: 28 [2816/54000 (5%)] Loss: -201912.000000\n",
      "Train Epoch: 28 [4224/54000 (8%)] Loss: -200423.203125\n",
      "Train Epoch: 28 [5632/54000 (10%)] Loss: -193445.031250\n",
      "Train Epoch: 28 [7040/54000 (13%)] Loss: -190520.078125\n",
      "Train Epoch: 28 [8448/54000 (16%)] Loss: -189597.500000\n",
      "Train Epoch: 28 [9856/54000 (18%)] Loss: -179918.750000\n",
      "Train Epoch: 28 [11264/54000 (21%)] Loss: -195114.312500\n",
      "Train Epoch: 28 [12672/54000 (23%)] Loss: -179889.468750\n",
      "Train Epoch: 28 [14080/54000 (26%)] Loss: -187293.140625\n",
      "Train Epoch: 28 [15488/54000 (29%)] Loss: -196055.734375\n",
      "Train Epoch: 28 [16896/54000 (31%)] Loss: -202203.218750\n",
      "Train Epoch: 28 [18304/54000 (34%)] Loss: -203584.593750\n",
      "Train Epoch: 28 [19712/54000 (37%)] Loss: -205400.171875\n",
      "Train Epoch: 28 [21120/54000 (39%)] Loss: -195326.843750\n",
      "Train Epoch: 28 [22528/54000 (42%)] Loss: -177071.031250\n",
      "Train Epoch: 28 [23936/54000 (44%)] Loss: -191564.031250\n",
      "Train Epoch: 28 [25344/54000 (47%)] Loss: -199537.906250\n",
      "Train Epoch: 28 [26752/54000 (50%)] Loss: -183131.734375\n",
      "Train Epoch: 28 [28160/54000 (52%)] Loss: -176817.468750\n",
      "Train Epoch: 28 [29568/54000 (55%)] Loss: -189516.640625\n",
      "Train Epoch: 28 [30976/54000 (57%)] Loss: -195304.781250\n",
      "Train Epoch: 28 [32384/54000 (60%)] Loss: -217566.718750\n",
      "Train Epoch: 28 [33792/54000 (63%)] Loss: -173123.140625\n",
      "Train Epoch: 28 [35200/54000 (65%)] Loss: -182813.468750\n",
      "Train Epoch: 28 [36608/54000 (68%)] Loss: -217298.390625\n",
      "Train Epoch: 28 [38016/54000 (70%)] Loss: -178671.265625\n",
      "Train Epoch: 28 [39424/54000 (73%)] Loss: -180227.562500\n",
      "Train Epoch: 28 [40832/54000 (76%)] Loss: -213200.937500\n",
      "Train Epoch: 28 [42240/54000 (78%)] Loss: -218356.046875\n",
      "Train Epoch: 28 [43648/54000 (81%)] Loss: -189583.718750\n",
      "Train Epoch: 28 [45056/54000 (83%)] Loss: -200821.593750\n",
      "Train Epoch: 28 [46464/54000 (86%)] Loss: -189879.968750\n",
      "Train Epoch: 28 [47872/54000 (89%)] Loss: -190518.031250\n",
      "Train Epoch: 28 [49280/54000 (91%)] Loss: -216789.875000\n",
      "Train Epoch: 28 [50688/54000 (94%)] Loss: -193095.843750\n",
      "Train Epoch: 28 [52096/54000 (96%)] Loss: -181493.828125\n",
      "    epoch          : 28\n",
      "    loss           : -195530.76969946173\n",
      "    val_loss       : -203812.15199838034\n",
      "Train Epoch: 29 [0/54000 (0%)] Loss: -217817.296875\n",
      "Train Epoch: 29 [1408/54000 (3%)] Loss: -206458.531250\n",
      "Train Epoch: 29 [2816/54000 (5%)] Loss: -183404.421875\n",
      "Train Epoch: 29 [4224/54000 (8%)] Loss: -205919.953125\n",
      "Train Epoch: 29 [5632/54000 (10%)] Loss: -208562.687500\n",
      "Train Epoch: 29 [7040/54000 (13%)] Loss: -185516.484375\n",
      "Train Epoch: 29 [8448/54000 (16%)] Loss: -217539.125000\n",
      "Train Epoch: 29 [9856/54000 (18%)] Loss: -185362.093750\n",
      "Train Epoch: 29 [11264/54000 (21%)] Loss: -179195.671875\n",
      "Train Epoch: 29 [12672/54000 (23%)] Loss: -220391.562500\n",
      "Train Epoch: 29 [14080/54000 (26%)] Loss: -178500.843750\n",
      "Train Epoch: 29 [15488/54000 (29%)] Loss: -204629.203125\n",
      "Train Epoch: 29 [16896/54000 (31%)] Loss: -193336.281250\n",
      "Train Epoch: 29 [18304/54000 (34%)] Loss: -221331.484375\n",
      "Train Epoch: 29 [19712/54000 (37%)] Loss: -191899.640625\n",
      "Train Epoch: 29 [21120/54000 (39%)] Loss: -192329.031250\n",
      "Train Epoch: 29 [22528/54000 (42%)] Loss: -190052.250000\n",
      "Train Epoch: 29 [23936/54000 (44%)] Loss: -191546.125000\n",
      "Train Epoch: 29 [25344/54000 (47%)] Loss: -196566.671875\n",
      "Train Epoch: 29 [26752/54000 (50%)] Loss: -193921.359375\n",
      "Train Epoch: 29 [28160/54000 (52%)] Loss: -192538.562500\n",
      "Train Epoch: 29 [29568/54000 (55%)] Loss: -200469.406250\n",
      "Train Epoch: 29 [30976/54000 (57%)] Loss: -205665.859375\n",
      "Train Epoch: 29 [32384/54000 (60%)] Loss: -186017.906250\n",
      "Train Epoch: 29 [33792/54000 (63%)] Loss: -200452.312500\n",
      "Train Epoch: 29 [35200/54000 (65%)] Loss: -221185.718750\n",
      "Train Epoch: 29 [36608/54000 (68%)] Loss: -191666.218750\n",
      "Train Epoch: 29 [38016/54000 (70%)] Loss: -196623.906250\n",
      "Train Epoch: 29 [39424/54000 (73%)] Loss: -184111.671875\n",
      "Train Epoch: 29 [40832/54000 (76%)] Loss: -180420.843750\n",
      "Train Epoch: 29 [42240/54000 (78%)] Loss: -219961.140625\n",
      "Train Epoch: 29 [43648/54000 (81%)] Loss: -190881.968750\n",
      "Train Epoch: 29 [45056/54000 (83%)] Loss: -194051.890625\n",
      "Train Epoch: 29 [46464/54000 (86%)] Loss: -204219.375000\n",
      "Train Epoch: 29 [47872/54000 (89%)] Loss: -200516.078125\n",
      "Train Epoch: 29 [49280/54000 (91%)] Loss: -204460.265625\n",
      "Train Epoch: 29 [50688/54000 (94%)] Loss: -216651.796875\n",
      "Train Epoch: 29 [52096/54000 (96%)] Loss: -203742.125000\n",
      "    epoch          : 29\n",
      "    loss           : -195477.01383074163\n",
      "    val_loss       : -203582.03764529346\n",
      "Train Epoch: 30 [0/54000 (0%)] Loss: -221764.875000\n",
      "Train Epoch: 30 [1408/54000 (3%)] Loss: -202279.671875\n",
      "Train Epoch: 30 [2816/54000 (5%)] Loss: -211278.406250\n",
      "Train Epoch: 30 [4224/54000 (8%)] Loss: -171870.484375\n",
      "Train Epoch: 30 [5632/54000 (10%)] Loss: -188563.812500\n",
      "Train Epoch: 30 [7040/54000 (13%)] Loss: -191002.843750\n",
      "Train Epoch: 30 [8448/54000 (16%)] Loss: -202614.093750\n",
      "Train Epoch: 30 [9856/54000 (18%)] Loss: -178115.109375\n",
      "Train Epoch: 30 [11264/54000 (21%)] Loss: -177832.640625\n",
      "Train Epoch: 30 [12672/54000 (23%)] Loss: -182078.671875\n",
      "Train Epoch: 30 [14080/54000 (26%)] Loss: -206946.250000\n",
      "Train Epoch: 30 [15488/54000 (29%)] Loss: -211893.937500\n",
      "Train Epoch: 30 [16896/54000 (31%)] Loss: -200672.687500\n",
      "Train Epoch: 30 [18304/54000 (34%)] Loss: -195931.375000\n",
      "Train Epoch: 30 [19712/54000 (37%)] Loss: -197088.609375\n",
      "Train Epoch: 30 [21120/54000 (39%)] Loss: -206050.312500\n",
      "Train Epoch: 30 [22528/54000 (42%)] Loss: -182410.718750\n",
      "Train Epoch: 30 [23936/54000 (44%)] Loss: -209733.968750\n",
      "Train Epoch: 30 [25344/54000 (47%)] Loss: -188373.125000\n",
      "Train Epoch: 30 [26752/54000 (50%)] Loss: -201626.125000\n",
      "Train Epoch: 30 [28160/54000 (52%)] Loss: -201661.625000\n",
      "Train Epoch: 30 [29568/54000 (55%)] Loss: -186155.125000\n",
      "Train Epoch: 30 [30976/54000 (57%)] Loss: -221490.531250\n",
      "Train Epoch: 30 [32384/54000 (60%)] Loss: -183786.734375\n",
      "Train Epoch: 30 [33792/54000 (63%)] Loss: -202649.406250\n",
      "Train Epoch: 30 [35200/54000 (65%)] Loss: -194485.406250\n",
      "Train Epoch: 30 [36608/54000 (68%)] Loss: -189789.656250\n",
      "Train Epoch: 30 [38016/54000 (70%)] Loss: -184077.671875\n",
      "Train Epoch: 30 [39424/54000 (73%)] Loss: -206259.156250\n",
      "Train Epoch: 30 [40832/54000 (76%)] Loss: -192532.593750\n",
      "Train Epoch: 30 [42240/54000 (78%)] Loss: -185695.750000\n",
      "Train Epoch: 30 [43648/54000 (81%)] Loss: -189145.828125\n",
      "Train Epoch: 30 [45056/54000 (83%)] Loss: -191652.843750\n",
      "Train Epoch: 30 [46464/54000 (86%)] Loss: -190258.125000\n",
      "Train Epoch: 30 [47872/54000 (89%)] Loss: -197801.312500\n",
      "Train Epoch: 30 [49280/54000 (91%)] Loss: -191692.734375\n",
      "Train Epoch: 30 [50688/54000 (94%)] Loss: -220954.328125\n",
      "Train Epoch: 30 [52096/54000 (96%)] Loss: -188988.937500\n",
      "    epoch          : 30\n",
      "    loss           : -195502.3476749402\n",
      "    val_loss       : -204763.3631383384\n",
      "Train Epoch: 31 [0/54000 (0%)] Loss: -211152.625000\n",
      "Train Epoch: 31 [1408/54000 (3%)] Loss: -190895.500000\n",
      "Train Epoch: 31 [2816/54000 (5%)] Loss: -221101.375000\n",
      "Train Epoch: 31 [4224/54000 (8%)] Loss: -176006.640625\n",
      "Train Epoch: 31 [5632/54000 (10%)] Loss: -195615.593750\n",
      "Train Epoch: 31 [7040/54000 (13%)] Loss: -211816.531250\n",
      "Train Epoch: 31 [8448/54000 (16%)] Loss: -181652.875000\n",
      "Train Epoch: 31 [9856/54000 (18%)] Loss: -212219.968750\n",
      "Train Epoch: 31 [11264/54000 (21%)] Loss: -211036.000000\n",
      "Train Epoch: 31 [12672/54000 (23%)] Loss: -206082.781250\n",
      "Train Epoch: 31 [14080/54000 (26%)] Loss: -192887.234375\n",
      "Train Epoch: 31 [15488/54000 (29%)] Loss: -193620.468750\n",
      "Train Epoch: 31 [16896/54000 (31%)] Loss: -201587.406250\n",
      "Train Epoch: 31 [18304/54000 (34%)] Loss: -189928.843750\n",
      "Train Epoch: 31 [19712/54000 (37%)] Loss: -207529.328125\n",
      "Train Epoch: 31 [21120/54000 (39%)] Loss: -216953.421875\n",
      "Train Epoch: 31 [22528/54000 (42%)] Loss: -193138.250000\n",
      "Train Epoch: 31 [23936/54000 (44%)] Loss: -195677.125000\n",
      "Train Epoch: 31 [25344/54000 (47%)] Loss: -192440.156250\n",
      "Train Epoch: 31 [26752/54000 (50%)] Loss: -222597.625000\n",
      "Train Epoch: 31 [28160/54000 (52%)] Loss: -194613.109375\n",
      "Train Epoch: 31 [29568/54000 (55%)] Loss: -203766.687500\n",
      "Train Epoch: 31 [30976/54000 (57%)] Loss: -183028.703125\n",
      "Train Epoch: 31 [32384/54000 (60%)] Loss: -194240.171875\n",
      "Train Epoch: 31 [33792/54000 (63%)] Loss: -221456.828125\n",
      "Train Epoch: 31 [35200/54000 (65%)] Loss: -184865.406250\n",
      "Train Epoch: 31 [36608/54000 (68%)] Loss: -178331.468750\n",
      "Train Epoch: 31 [38016/54000 (70%)] Loss: -177674.718750\n",
      "Train Epoch: 31 [39424/54000 (73%)] Loss: -212350.000000\n",
      "Train Epoch: 31 [40832/54000 (76%)] Loss: -203227.531250\n",
      "Train Epoch: 31 [42240/54000 (78%)] Loss: -205027.187500\n",
      "Train Epoch: 31 [43648/54000 (81%)] Loss: -192517.593750\n",
      "Train Epoch: 31 [45056/54000 (83%)] Loss: -195135.515625\n",
      "Train Epoch: 31 [46464/54000 (86%)] Loss: -199511.000000\n",
      "Train Epoch: 31 [47872/54000 (89%)] Loss: -202936.515625\n",
      "Train Epoch: 31 [49280/54000 (91%)] Loss: -192624.937500\n",
      "Train Epoch: 31 [50688/54000 (94%)] Loss: -184696.781250\n",
      "Train Epoch: 31 [52096/54000 (96%)] Loss: -218247.859375\n",
      "    epoch          : 31\n",
      "    loss           : -196596.67606160286\n",
      "    val_loss       : -204147.29479087272\n",
      "Train Epoch: 32 [0/54000 (0%)] Loss: -175234.531250\n",
      "Train Epoch: 32 [1408/54000 (3%)] Loss: -180385.687500\n",
      "Train Epoch: 32 [2816/54000 (5%)] Loss: -183183.875000\n",
      "Train Epoch: 32 [4224/54000 (8%)] Loss: -192461.281250\n",
      "Train Epoch: 32 [5632/54000 (10%)] Loss: -177354.312500\n",
      "Train Epoch: 32 [7040/54000 (13%)] Loss: -197308.421875\n",
      "Train Epoch: 32 [8448/54000 (16%)] Loss: -185003.390625\n",
      "Train Epoch: 32 [9856/54000 (18%)] Loss: -219256.015625\n",
      "Train Epoch: 32 [11264/54000 (21%)] Loss: -197611.937500\n",
      "Train Epoch: 32 [12672/54000 (23%)] Loss: -198756.406250\n",
      "Train Epoch: 32 [14080/54000 (26%)] Loss: -187305.031250\n",
      "Train Epoch: 32 [15488/54000 (29%)] Loss: -213978.125000\n",
      "Train Epoch: 32 [16896/54000 (31%)] Loss: -203391.625000\n",
      "Train Epoch: 32 [18304/54000 (34%)] Loss: -200154.781250\n",
      "Train Epoch: 32 [19712/54000 (37%)] Loss: -190699.718750\n",
      "Train Epoch: 32 [21120/54000 (39%)] Loss: -192956.468750\n",
      "Train Epoch: 32 [22528/54000 (42%)] Loss: -192377.546875\n",
      "Train Epoch: 32 [23936/54000 (44%)] Loss: -182239.546875\n",
      "Train Epoch: 32 [25344/54000 (47%)] Loss: -203176.593750\n",
      "Train Epoch: 32 [26752/54000 (50%)] Loss: -199159.281250\n",
      "Train Epoch: 32 [28160/54000 (52%)] Loss: -200103.625000\n",
      "Train Epoch: 32 [29568/54000 (55%)] Loss: -177613.312500\n",
      "Train Epoch: 32 [30976/54000 (57%)] Loss: -179111.093750\n",
      "Train Epoch: 32 [32384/54000 (60%)] Loss: -189504.375000\n",
      "Train Epoch: 32 [33792/54000 (63%)] Loss: -197840.250000\n",
      "Train Epoch: 32 [35200/54000 (65%)] Loss: -196807.625000\n",
      "Train Epoch: 32 [36608/54000 (68%)] Loss: -192158.593750\n",
      "Train Epoch: 32 [38016/54000 (70%)] Loss: -196334.437500\n",
      "Train Epoch: 32 [39424/54000 (73%)] Loss: -212572.562500\n",
      "Train Epoch: 32 [40832/54000 (76%)] Loss: -191930.000000\n",
      "Train Epoch: 32 [42240/54000 (78%)] Loss: -179024.343750\n",
      "Train Epoch: 32 [43648/54000 (81%)] Loss: -199150.500000\n",
      "Train Epoch: 32 [45056/54000 (83%)] Loss: -181679.843750\n",
      "Train Epoch: 32 [46464/54000 (86%)] Loss: -177621.343750\n",
      "Train Epoch: 32 [47872/54000 (89%)] Loss: -185589.484375\n",
      "Train Epoch: 32 [49280/54000 (91%)] Loss: -197289.781250\n",
      "Train Epoch: 32 [50688/54000 (94%)] Loss: -205408.062500\n",
      "Train Epoch: 32 [52096/54000 (96%)] Loss: -217166.000000\n",
      "    epoch          : 32\n",
      "    loss           : -196409.76338217704\n",
      "    val_loss       : -205217.36475800304\n",
      "Train Epoch: 33 [0/54000 (0%)] Loss: -194276.062500\n",
      "Train Epoch: 33 [1408/54000 (3%)] Loss: -194825.609375\n",
      "Train Epoch: 33 [2816/54000 (5%)] Loss: -189669.875000\n",
      "Train Epoch: 33 [4224/54000 (8%)] Loss: -193739.718750\n",
      "Train Epoch: 33 [5632/54000 (10%)] Loss: -205107.890625\n",
      "Train Epoch: 33 [7040/54000 (13%)] Loss: -182961.187500\n",
      "Train Epoch: 33 [8448/54000 (16%)] Loss: -217166.109375\n",
      "Train Epoch: 33 [9856/54000 (18%)] Loss: -192159.093750\n",
      "Train Epoch: 33 [11264/54000 (21%)] Loss: -204326.156250\n",
      "Train Epoch: 33 [12672/54000 (23%)] Loss: -216161.312500\n",
      "Train Epoch: 33 [14080/54000 (26%)] Loss: -188012.093750\n",
      "Train Epoch: 33 [15488/54000 (29%)] Loss: -199399.390625\n",
      "Train Epoch: 33 [16896/54000 (31%)] Loss: -192422.656250\n",
      "Train Epoch: 33 [18304/54000 (34%)] Loss: -222517.328125\n",
      "Train Epoch: 33 [19712/54000 (37%)] Loss: -181134.718750\n",
      "Train Epoch: 33 [21120/54000 (39%)] Loss: -197010.046875\n",
      "Train Epoch: 33 [22528/54000 (42%)] Loss: -200352.437500\n",
      "Train Epoch: 33 [23936/54000 (44%)] Loss: -182716.750000\n",
      "Train Epoch: 33 [25344/54000 (47%)] Loss: -180818.578125\n",
      "Train Epoch: 33 [26752/54000 (50%)] Loss: -206309.125000\n",
      "Train Epoch: 33 [28160/54000 (52%)] Loss: -191199.250000\n",
      "Train Epoch: 33 [29568/54000 (55%)] Loss: -195321.234375\n",
      "Train Epoch: 33 [30976/54000 (57%)] Loss: -189612.062500\n",
      "Train Epoch: 33 [32384/54000 (60%)] Loss: -209820.375000\n",
      "Train Epoch: 33 [33792/54000 (63%)] Loss: -184281.906250\n",
      "Train Epoch: 33 [35200/54000 (65%)] Loss: -201164.984375\n",
      "Train Epoch: 33 [36608/54000 (68%)] Loss: -220119.265625\n",
      "Train Epoch: 33 [38016/54000 (70%)] Loss: -195351.015625\n",
      "Train Epoch: 33 [39424/54000 (73%)] Loss: -190688.671875\n",
      "Train Epoch: 33 [40832/54000 (76%)] Loss: -194900.937500\n",
      "Train Epoch: 33 [42240/54000 (78%)] Loss: -211127.828125\n",
      "Train Epoch: 33 [43648/54000 (81%)] Loss: -210514.281250\n",
      "Train Epoch: 33 [45056/54000 (83%)] Loss: -221490.515625\n",
      "Train Epoch: 33 [46464/54000 (86%)] Loss: -194600.437500\n",
      "Train Epoch: 33 [47872/54000 (89%)] Loss: -188607.906250\n",
      "Train Epoch: 33 [49280/54000 (91%)] Loss: -191713.468750\n",
      "Train Epoch: 33 [50688/54000 (94%)] Loss: -215348.281250\n",
      "Train Epoch: 33 [52096/54000 (96%)] Loss: -204363.828125\n",
      "    epoch          : 33\n",
      "    loss           : -197194.68084629186\n",
      "    val_loss       : -206645.34314262576\n",
      "Train Epoch: 34 [0/54000 (0%)] Loss: -179605.406250\n",
      "Train Epoch: 34 [1408/54000 (3%)] Loss: -188682.265625\n",
      "Train Epoch: 34 [2816/54000 (5%)] Loss: -211068.000000\n",
      "Train Epoch: 34 [4224/54000 (8%)] Loss: -210879.421875\n",
      "Train Epoch: 34 [5632/54000 (10%)] Loss: -193037.562500\n",
      "Train Epoch: 34 [7040/54000 (13%)] Loss: -199514.093750\n",
      "Train Epoch: 34 [8448/54000 (16%)] Loss: -194178.953125\n",
      "Train Epoch: 34 [9856/54000 (18%)] Loss: -190705.937500\n",
      "Train Epoch: 34 [11264/54000 (21%)] Loss: -191461.703125\n",
      "Train Epoch: 34 [12672/54000 (23%)] Loss: -206223.437500\n",
      "Train Epoch: 34 [14080/54000 (26%)] Loss: -195729.921875\n",
      "Train Epoch: 34 [15488/54000 (29%)] Loss: -204574.921875\n",
      "Train Epoch: 34 [16896/54000 (31%)] Loss: -220570.171875\n",
      "Train Epoch: 34 [18304/54000 (34%)] Loss: -209405.031250\n",
      "Train Epoch: 34 [19712/54000 (37%)] Loss: -182627.828125\n",
      "Train Epoch: 34 [21120/54000 (39%)] Loss: -222307.500000\n",
      "Train Epoch: 34 [22528/54000 (42%)] Loss: -181718.203125\n",
      "Train Epoch: 34 [23936/54000 (44%)] Loss: -177946.609375\n",
      "Train Epoch: 34 [25344/54000 (47%)] Loss: -183204.578125\n",
      "Train Epoch: 34 [26752/54000 (50%)] Loss: -222042.375000\n",
      "Train Epoch: 34 [28160/54000 (52%)] Loss: -180206.093750\n",
      "Train Epoch: 34 [29568/54000 (55%)] Loss: -204853.453125\n",
      "Train Epoch: 34 [30976/54000 (57%)] Loss: -192356.156250\n",
      "Train Epoch: 34 [32384/54000 (60%)] Loss: -209044.921875\n",
      "Train Epoch: 34 [33792/54000 (63%)] Loss: -213022.796875\n",
      "Train Epoch: 34 [35200/54000 (65%)] Loss: -196366.156250\n",
      "Train Epoch: 34 [36608/54000 (68%)] Loss: -189204.406250\n",
      "Train Epoch: 34 [38016/54000 (70%)] Loss: -180708.359375\n",
      "Train Epoch: 34 [39424/54000 (73%)] Loss: -195329.843750\n",
      "Train Epoch: 34 [40832/54000 (76%)] Loss: -191427.171875\n",
      "Train Epoch: 34 [42240/54000 (78%)] Loss: -191202.765625\n",
      "Train Epoch: 34 [43648/54000 (81%)] Loss: -179097.531250\n",
      "Train Epoch: 34 [45056/54000 (83%)] Loss: -200402.875000\n",
      "Train Epoch: 34 [46464/54000 (86%)] Loss: -193213.546875\n",
      "Train Epoch: 34 [47872/54000 (89%)] Loss: -195149.031250\n",
      "Train Epoch: 34 [49280/54000 (91%)] Loss: -198039.484375\n",
      "Train Epoch: 34 [50688/54000 (94%)] Loss: -216403.062500\n",
      "Train Epoch: 34 [52096/54000 (96%)] Loss: -179399.156250\n",
      "    epoch          : 34\n",
      "    loss           : -197312.88363486843\n",
      "    val_loss       : -206204.88513481326\n",
      "Train Epoch: 35 [0/54000 (0%)] Loss: -222947.156250\n",
      "Train Epoch: 35 [1408/54000 (3%)] Loss: -183492.031250\n",
      "Train Epoch: 35 [2816/54000 (5%)] Loss: -198230.109375\n",
      "Train Epoch: 35 [4224/54000 (8%)] Loss: -210489.578125\n",
      "Train Epoch: 35 [5632/54000 (10%)] Loss: -178791.062500\n",
      "Train Epoch: 35 [7040/54000 (13%)] Loss: -186039.218750\n",
      "Train Epoch: 35 [8448/54000 (16%)] Loss: -211190.968750\n",
      "Train Epoch: 35 [9856/54000 (18%)] Loss: -218719.656250\n",
      "Train Epoch: 35 [11264/54000 (21%)] Loss: -206194.000000\n",
      "Train Epoch: 35 [12672/54000 (23%)] Loss: -203235.625000\n",
      "Train Epoch: 35 [14080/54000 (26%)] Loss: -193498.109375\n",
      "Train Epoch: 35 [15488/54000 (29%)] Loss: -206221.031250\n",
      "Train Epoch: 35 [16896/54000 (31%)] Loss: -195118.593750\n",
      "Train Epoch: 35 [18304/54000 (34%)] Loss: -186039.562500\n",
      "Train Epoch: 35 [19712/54000 (37%)] Loss: -192591.468750\n",
      "Train Epoch: 35 [21120/54000 (39%)] Loss: -187142.078125\n",
      "Train Epoch: 35 [22528/54000 (42%)] Loss: -206573.437500\n",
      "Train Epoch: 35 [23936/54000 (44%)] Loss: -198475.718750\n",
      "Train Epoch: 35 [25344/54000 (47%)] Loss: -198850.296875\n",
      "Train Epoch: 35 [26752/54000 (50%)] Loss: -180565.015625\n",
      "Train Epoch: 35 [28160/54000 (52%)] Loss: -188737.687500\n",
      "Train Epoch: 35 [29568/54000 (55%)] Loss: -221225.078125\n",
      "Train Epoch: 35 [30976/54000 (57%)] Loss: -179554.921875\n",
      "Train Epoch: 35 [32384/54000 (60%)] Loss: -197876.890625\n",
      "Train Epoch: 35 [33792/54000 (63%)] Loss: -177061.593750\n",
      "Train Epoch: 35 [35200/54000 (65%)] Loss: -205993.750000\n",
      "Train Epoch: 35 [36608/54000 (68%)] Loss: -216497.546875\n",
      "Train Epoch: 35 [38016/54000 (70%)] Loss: -202705.718750\n",
      "Train Epoch: 35 [39424/54000 (73%)] Loss: -213699.062500\n",
      "Train Epoch: 35 [40832/54000 (76%)] Loss: -189729.640625\n",
      "Train Epoch: 35 [42240/54000 (78%)] Loss: -197597.500000\n",
      "Train Epoch: 35 [43648/54000 (81%)] Loss: -206196.578125\n",
      "Train Epoch: 35 [45056/54000 (83%)] Loss: -190972.093750\n",
      "Train Epoch: 35 [46464/54000 (86%)] Loss: -196624.906250\n",
      "Train Epoch: 35 [47872/54000 (89%)] Loss: -181759.390625\n",
      "Train Epoch: 35 [49280/54000 (91%)] Loss: -201008.375000\n",
      "Train Epoch: 35 [50688/54000 (94%)] Loss: -195209.359375\n",
      "Train Epoch: 35 [52096/54000 (96%)] Loss: -192074.281250\n",
      "    epoch          : 35\n",
      "    loss           : -197881.3912978469\n",
      "    val_loss       : -206369.28647818216\n",
      "Train Epoch: 36 [0/54000 (0%)] Loss: -217799.343750\n",
      "Train Epoch: 36 [1408/54000 (3%)] Loss: -190510.125000\n",
      "Train Epoch: 36 [2816/54000 (5%)] Loss: -196436.687500\n",
      "Train Epoch: 36 [4224/54000 (8%)] Loss: -185691.531250\n",
      "Train Epoch: 36 [5632/54000 (10%)] Loss: -181323.500000\n",
      "Train Epoch: 36 [7040/54000 (13%)] Loss: -204235.093750\n",
      "Train Epoch: 36 [8448/54000 (16%)] Loss: -181648.515625\n",
      "Train Epoch: 36 [9856/54000 (18%)] Loss: -198905.171875\n",
      "Train Epoch: 36 [11264/54000 (21%)] Loss: -196392.921875\n",
      "Train Epoch: 36 [12672/54000 (23%)] Loss: -206158.625000\n",
      "Train Epoch: 36 [14080/54000 (26%)] Loss: -206136.171875\n",
      "Train Epoch: 36 [15488/54000 (29%)] Loss: -213510.921875\n",
      "Train Epoch: 36 [16896/54000 (31%)] Loss: -190115.937500\n",
      "Train Epoch: 36 [18304/54000 (34%)] Loss: -194103.718750\n",
      "Train Epoch: 36 [19712/54000 (37%)] Loss: -217432.500000\n",
      "Train Epoch: 36 [21120/54000 (39%)] Loss: -196551.625000\n",
      "Train Epoch: 36 [22528/54000 (42%)] Loss: -193171.484375\n",
      "Train Epoch: 36 [23936/54000 (44%)] Loss: -206357.406250\n",
      "Train Epoch: 36 [25344/54000 (47%)] Loss: -193669.906250\n",
      "Train Epoch: 36 [26752/54000 (50%)] Loss: -197617.843750\n",
      "Train Epoch: 36 [28160/54000 (52%)] Loss: -198534.656250\n",
      "Train Epoch: 36 [29568/54000 (55%)] Loss: -195532.437500\n",
      "Train Epoch: 36 [30976/54000 (57%)] Loss: -198728.046875\n",
      "Train Epoch: 36 [32384/54000 (60%)] Loss: -211267.500000\n",
      "Train Epoch: 36 [33792/54000 (63%)] Loss: -196141.406250\n",
      "Train Epoch: 36 [35200/54000 (65%)] Loss: -194060.296875\n",
      "Train Epoch: 36 [36608/54000 (68%)] Loss: -213564.718750\n",
      "Train Epoch: 36 [38016/54000 (70%)] Loss: -181408.484375\n",
      "Train Epoch: 36 [39424/54000 (73%)] Loss: -222214.750000\n",
      "Train Epoch: 36 [40832/54000 (76%)] Loss: -196597.750000\n",
      "Train Epoch: 36 [42240/54000 (78%)] Loss: -196395.390625\n",
      "Train Epoch: 36 [43648/54000 (81%)] Loss: -193273.093750\n",
      "Train Epoch: 36 [45056/54000 (83%)] Loss: -223258.671875\n",
      "Train Epoch: 36 [46464/54000 (86%)] Loss: -181536.828125\n",
      "Train Epoch: 36 [47872/54000 (89%)] Loss: -187854.031250\n",
      "Train Epoch: 36 [49280/54000 (91%)] Loss: -199597.093750\n",
      "Train Epoch: 36 [50688/54000 (94%)] Loss: -220899.343750\n",
      "Train Epoch: 36 [52096/54000 (96%)] Loss: -204918.031250\n",
      "    epoch          : 36\n",
      "    loss           : -198422.62376644736\n",
      "    val_loss       : -207674.49492663873\n",
      "Train Epoch: 37 [0/54000 (0%)] Loss: -220668.703125\n",
      "Train Epoch: 37 [1408/54000 (3%)] Loss: -190304.484375\n",
      "Train Epoch: 37 [2816/54000 (5%)] Loss: -211522.421875\n",
      "Train Epoch: 37 [4224/54000 (8%)] Loss: -191641.281250\n",
      "Train Epoch: 37 [5632/54000 (10%)] Loss: -211006.125000\n",
      "Train Epoch: 37 [7040/54000 (13%)] Loss: -221526.375000\n",
      "Train Epoch: 37 [8448/54000 (16%)] Loss: -205049.437500\n",
      "Train Epoch: 37 [9856/54000 (18%)] Loss: -195831.031250\n",
      "Train Epoch: 37 [11264/54000 (21%)] Loss: -193993.781250\n",
      "Train Epoch: 37 [12672/54000 (23%)] Loss: -184147.671875\n",
      "Train Epoch: 37 [14080/54000 (26%)] Loss: -206991.593750\n",
      "Train Epoch: 37 [15488/54000 (29%)] Loss: -175473.687500\n",
      "Train Epoch: 37 [16896/54000 (31%)] Loss: -185610.468750\n",
      "Train Epoch: 37 [18304/54000 (34%)] Loss: -201847.156250\n",
      "Train Epoch: 37 [19712/54000 (37%)] Loss: -200315.875000\n",
      "Train Epoch: 37 [21120/54000 (39%)] Loss: -196323.125000\n",
      "Train Epoch: 37 [22528/54000 (42%)] Loss: -194212.968750\n",
      "Train Epoch: 37 [23936/54000 (44%)] Loss: -185536.000000\n",
      "Train Epoch: 37 [25344/54000 (47%)] Loss: -197775.968750\n",
      "Train Epoch: 37 [26752/54000 (50%)] Loss: -195997.796875\n",
      "Train Epoch: 37 [28160/54000 (52%)] Loss: -185719.250000\n",
      "Train Epoch: 37 [29568/54000 (55%)] Loss: -219435.640625\n",
      "Train Epoch: 37 [30976/54000 (57%)] Loss: -195341.953125\n",
      "Train Epoch: 37 [32384/54000 (60%)] Loss: -183081.031250\n",
      "Train Epoch: 37 [33792/54000 (63%)] Loss: -200806.718750\n",
      "Train Epoch: 37 [35200/54000 (65%)] Loss: -192750.421875\n",
      "Train Epoch: 37 [36608/54000 (68%)] Loss: -203057.468750\n",
      "Train Epoch: 37 [38016/54000 (70%)] Loss: -203704.234375\n",
      "Train Epoch: 37 [39424/54000 (73%)] Loss: -197416.484375\n",
      "Train Epoch: 37 [40832/54000 (76%)] Loss: -182944.140625\n",
      "Train Epoch: 37 [42240/54000 (78%)] Loss: -180592.250000\n",
      "Train Epoch: 37 [43648/54000 (81%)] Loss: -187234.015625\n",
      "Train Epoch: 37 [45056/54000 (83%)] Loss: -199026.609375\n",
      "Train Epoch: 37 [46464/54000 (86%)] Loss: -198670.796875\n",
      "Train Epoch: 37 [47872/54000 (89%)] Loss: -198837.625000\n",
      "Train Epoch: 37 [49280/54000 (91%)] Loss: -180160.796875\n",
      "Train Epoch: 37 [50688/54000 (94%)] Loss: -184150.234375\n",
      "Train Epoch: 37 [52096/54000 (96%)] Loss: -185312.578125\n",
      "    epoch          : 37\n",
      "    loss           : -198763.63318630384\n",
      "    val_loss       : -207644.52383050686\n",
      "Train Epoch: 38 [0/54000 (0%)] Loss: -205117.312500\n",
      "Train Epoch: 38 [1408/54000 (3%)] Loss: -201978.968750\n",
      "Train Epoch: 38 [2816/54000 (5%)] Loss: -197937.140625\n",
      "Train Epoch: 38 [4224/54000 (8%)] Loss: -204191.296875\n",
      "Train Epoch: 38 [5632/54000 (10%)] Loss: -204778.171875\n",
      "Train Epoch: 38 [7040/54000 (13%)] Loss: -186076.343750\n",
      "Train Epoch: 38 [8448/54000 (16%)] Loss: -207206.078125\n",
      "Train Epoch: 38 [9856/54000 (18%)] Loss: -195052.906250\n",
      "Train Epoch: 38 [11264/54000 (21%)] Loss: -192175.000000\n",
      "Train Epoch: 38 [12672/54000 (23%)] Loss: -209226.640625\n",
      "Train Epoch: 38 [14080/54000 (26%)] Loss: -195451.406250\n",
      "Train Epoch: 38 [15488/54000 (29%)] Loss: -193331.687500\n",
      "Train Epoch: 38 [16896/54000 (31%)] Loss: -181042.656250\n",
      "Train Epoch: 38 [18304/54000 (34%)] Loss: -195311.312500\n",
      "Train Epoch: 38 [19712/54000 (37%)] Loss: -219895.484375\n",
      "Train Epoch: 38 [21120/54000 (39%)] Loss: -206447.359375\n",
      "Train Epoch: 38 [22528/54000 (42%)] Loss: -212351.218750\n",
      "Train Epoch: 38 [23936/54000 (44%)] Loss: -198475.781250\n",
      "Train Epoch: 38 [25344/54000 (47%)] Loss: -220576.515625\n",
      "Train Epoch: 38 [26752/54000 (50%)] Loss: -195773.000000\n",
      "Train Epoch: 38 [28160/54000 (52%)] Loss: -198065.906250\n",
      "Train Epoch: 38 [29568/54000 (55%)] Loss: -195817.187500\n",
      "Train Epoch: 38 [30976/54000 (57%)] Loss: -193669.656250\n",
      "Train Epoch: 38 [32384/54000 (60%)] Loss: -203607.359375\n",
      "Train Epoch: 38 [33792/54000 (63%)] Loss: -192863.500000\n",
      "Train Epoch: 38 [35200/54000 (65%)] Loss: -185037.750000\n",
      "Train Epoch: 38 [36608/54000 (68%)] Loss: -196804.843750\n",
      "Train Epoch: 38 [38016/54000 (70%)] Loss: -215046.671875\n",
      "Train Epoch: 38 [39424/54000 (73%)] Loss: -182164.828125\n",
      "Train Epoch: 38 [40832/54000 (76%)] Loss: -203007.812500\n",
      "Train Epoch: 38 [42240/54000 (78%)] Loss: -199419.312500\n",
      "Train Epoch: 38 [43648/54000 (81%)] Loss: -194564.937500\n",
      "Train Epoch: 38 [45056/54000 (83%)] Loss: -204395.859375\n",
      "Train Epoch: 38 [46464/54000 (86%)] Loss: -179621.593750\n",
      "Train Epoch: 38 [47872/54000 (89%)] Loss: -204315.187500\n",
      "Train Epoch: 38 [49280/54000 (91%)] Loss: -199082.875000\n",
      "Train Epoch: 38 [50688/54000 (94%)] Loss: -223029.671875\n",
      "Train Epoch: 38 [52096/54000 (96%)] Loss: -189052.312500\n",
      "    epoch          : 38\n",
      "    loss           : -199320.3218450957\n",
      "    val_loss       : -209906.162109375\n",
      "Train Epoch: 39 [0/54000 (0%)] Loss: -219557.812500\n",
      "Train Epoch: 39 [1408/54000 (3%)] Loss: -191727.484375\n",
      "Train Epoch: 39 [2816/54000 (5%)] Loss: -198093.437500\n",
      "Train Epoch: 39 [4224/54000 (8%)] Loss: -216981.656250\n",
      "Train Epoch: 39 [5632/54000 (10%)] Loss: -207290.093750\n",
      "Train Epoch: 39 [7040/54000 (13%)] Loss: -219978.796875\n",
      "Train Epoch: 39 [8448/54000 (16%)] Loss: -198918.656250\n",
      "Train Epoch: 39 [9856/54000 (18%)] Loss: -179552.984375\n",
      "Train Epoch: 39 [11264/54000 (21%)] Loss: -185618.906250\n",
      "Train Epoch: 39 [12672/54000 (23%)] Loss: -199181.234375\n",
      "Train Epoch: 39 [14080/54000 (26%)] Loss: -222600.515625\n",
      "Train Epoch: 39 [15488/54000 (29%)] Loss: -198863.921875\n",
      "Train Epoch: 39 [16896/54000 (31%)] Loss: -198673.312500\n",
      "Train Epoch: 39 [18304/54000 (34%)] Loss: -197239.968750\n",
      "Train Epoch: 39 [19712/54000 (37%)] Loss: -220260.937500\n",
      "Train Epoch: 39 [21120/54000 (39%)] Loss: -208109.640625\n",
      "Train Epoch: 39 [22528/54000 (42%)] Loss: -182557.828125\n",
      "Train Epoch: 39 [23936/54000 (44%)] Loss: -221225.437500\n",
      "Train Epoch: 39 [25344/54000 (47%)] Loss: -189532.281250\n",
      "Train Epoch: 39 [26752/54000 (50%)] Loss: -197159.828125\n",
      "Train Epoch: 39 [28160/54000 (52%)] Loss: -182938.578125\n",
      "Train Epoch: 39 [29568/54000 (55%)] Loss: -220780.218750\n",
      "Train Epoch: 39 [30976/54000 (57%)] Loss: -207676.250000\n",
      "Train Epoch: 39 [32384/54000 (60%)] Loss: -182806.875000\n",
      "Train Epoch: 39 [33792/54000 (63%)] Loss: -199240.562500\n",
      "Train Epoch: 39 [35200/54000 (65%)] Loss: -213381.421875\n",
      "Train Epoch: 39 [36608/54000 (68%)] Loss: -192808.578125\n",
      "Train Epoch: 39 [38016/54000 (70%)] Loss: -213951.109375\n",
      "Train Epoch: 39 [39424/54000 (73%)] Loss: -186557.953125\n",
      "Train Epoch: 39 [40832/54000 (76%)] Loss: -183491.687500\n",
      "Train Epoch: 39 [42240/54000 (78%)] Loss: -187019.359375\n",
      "Train Epoch: 39 [43648/54000 (81%)] Loss: -221130.656250\n",
      "Train Epoch: 39 [45056/54000 (83%)] Loss: -199937.750000\n",
      "Train Epoch: 39 [46464/54000 (86%)] Loss: -196265.453125\n",
      "Train Epoch: 39 [47872/54000 (89%)] Loss: -186907.656250\n",
      "Train Epoch: 39 [49280/54000 (91%)] Loss: -208542.218750\n",
      "Train Epoch: 39 [50688/54000 (94%)] Loss: -222916.906250\n",
      "Train Epoch: 39 [52096/54000 (96%)] Loss: -194181.750000\n",
      "    epoch          : 39\n",
      "    loss           : -199401.97248803827\n",
      "    val_loss       : -208676.89460270578\n",
      "Train Epoch: 40 [0/54000 (0%)] Loss: -179289.515625\n",
      "Train Epoch: 40 [1408/54000 (3%)] Loss: -182766.437500\n",
      "Train Epoch: 40 [2816/54000 (5%)] Loss: -200261.625000\n",
      "Train Epoch: 40 [4224/54000 (8%)] Loss: -194448.250000\n",
      "Train Epoch: 40 [5632/54000 (10%)] Loss: -184221.578125\n",
      "Train Epoch: 40 [7040/54000 (13%)] Loss: -199109.171875\n",
      "Train Epoch: 40 [8448/54000 (16%)] Loss: -181842.156250\n",
      "Train Epoch: 40 [9856/54000 (18%)] Loss: -201476.812500\n",
      "Train Epoch: 40 [11264/54000 (21%)] Loss: -204928.515625\n",
      "Train Epoch: 40 [12672/54000 (23%)] Loss: -205812.093750\n",
      "Train Epoch: 40 [14080/54000 (26%)] Loss: -207449.187500\n",
      "Train Epoch: 40 [15488/54000 (29%)] Loss: -194594.187500\n",
      "Train Epoch: 40 [16896/54000 (31%)] Loss: -197238.703125\n",
      "Train Epoch: 40 [18304/54000 (34%)] Loss: -195864.656250\n",
      "Train Epoch: 40 [19712/54000 (37%)] Loss: -223905.046875\n",
      "Train Epoch: 40 [21120/54000 (39%)] Loss: -193651.765625\n",
      "Train Epoch: 40 [22528/54000 (42%)] Loss: -184002.468750\n",
      "Train Epoch: 40 [23936/54000 (44%)] Loss: -198932.000000\n",
      "Train Epoch: 40 [25344/54000 (47%)] Loss: -196715.656250\n",
      "Train Epoch: 40 [26752/54000 (50%)] Loss: -206599.296875\n",
      "Train Epoch: 40 [28160/54000 (52%)] Loss: -196239.781250\n",
      "Train Epoch: 40 [29568/54000 (55%)] Loss: -206115.765625\n",
      "Train Epoch: 40 [30976/54000 (57%)] Loss: -214455.062500\n",
      "Train Epoch: 40 [32384/54000 (60%)] Loss: -213260.812500\n",
      "Train Epoch: 40 [33792/54000 (63%)] Loss: -218001.937500\n",
      "Train Epoch: 40 [35200/54000 (65%)] Loss: -209938.531250\n",
      "Train Epoch: 40 [36608/54000 (68%)] Loss: -186479.703125\n",
      "Train Epoch: 40 [38016/54000 (70%)] Loss: -220725.953125\n",
      "Train Epoch: 40 [39424/54000 (73%)] Loss: -224763.937500\n",
      "Train Epoch: 40 [40832/54000 (76%)] Loss: -186739.156250\n",
      "Train Epoch: 40 [42240/54000 (78%)] Loss: -220444.687500\n",
      "Train Epoch: 40 [43648/54000 (81%)] Loss: -209310.468750\n",
      "Train Epoch: 40 [45056/54000 (83%)] Loss: -183704.593750\n",
      "Train Epoch: 40 [46464/54000 (86%)] Loss: -206733.156250\n",
      "Train Epoch: 40 [47872/54000 (89%)] Loss: -185798.296875\n",
      "Train Epoch: 40 [49280/54000 (91%)] Loss: -187987.390625\n",
      "Train Epoch: 40 [50688/54000 (94%)] Loss: -185691.187500\n",
      "Train Epoch: 40 [52096/54000 (96%)] Loss: -224330.796875\n",
      "    epoch          : 40\n",
      "    loss           : -200435.12929874402\n",
      "    val_loss       : -210081.96448647103\n",
      "Train Epoch: 41 [0/54000 (0%)] Loss: -225361.546875\n",
      "Train Epoch: 41 [1408/54000 (3%)] Loss: -220789.421875\n",
      "Train Epoch: 41 [2816/54000 (5%)] Loss: -187064.625000\n",
      "Train Epoch: 41 [4224/54000 (8%)] Loss: -184950.765625\n",
      "Train Epoch: 41 [5632/54000 (10%)] Loss: -199509.609375\n",
      "Train Epoch: 41 [7040/54000 (13%)] Loss: -207102.437500\n",
      "Train Epoch: 41 [8448/54000 (16%)] Loss: -203446.718750\n",
      "Train Epoch: 41 [9856/54000 (18%)] Loss: -219855.562500\n",
      "Train Epoch: 41 [11264/54000 (21%)] Loss: -198875.781250\n",
      "Train Epoch: 41 [12672/54000 (23%)] Loss: -207119.765625\n",
      "Train Epoch: 41 [14080/54000 (26%)] Loss: -195667.031250\n",
      "Train Epoch: 41 [15488/54000 (29%)] Loss: -224825.906250\n",
      "Train Epoch: 41 [16896/54000 (31%)] Loss: -199051.890625\n",
      "Train Epoch: 41 [18304/54000 (34%)] Loss: -184212.781250\n",
      "Train Epoch: 41 [19712/54000 (37%)] Loss: -187723.875000\n",
      "Train Epoch: 41 [21120/54000 (39%)] Loss: -203928.000000\n",
      "Train Epoch: 41 [22528/54000 (42%)] Loss: -222630.843750\n",
      "Train Epoch: 41 [23936/54000 (44%)] Loss: -201901.546875\n",
      "Train Epoch: 41 [25344/54000 (47%)] Loss: -203318.437500\n",
      "Train Epoch: 41 [26752/54000 (50%)] Loss: -199761.375000\n",
      "Train Epoch: 41 [28160/54000 (52%)] Loss: -196545.781250\n",
      "Train Epoch: 41 [29568/54000 (55%)] Loss: -193717.468750\n",
      "Train Epoch: 41 [30976/54000 (57%)] Loss: -192947.515625\n",
      "Train Epoch: 41 [32384/54000 (60%)] Loss: -200771.375000\n",
      "Train Epoch: 41 [33792/54000 (63%)] Loss: -209022.203125\n",
      "Train Epoch: 41 [35200/54000 (65%)] Loss: -224706.593750\n",
      "Train Epoch: 41 [36608/54000 (68%)] Loss: -185962.750000\n",
      "Train Epoch: 41 [38016/54000 (70%)] Loss: -183720.218750\n",
      "Train Epoch: 41 [39424/54000 (73%)] Loss: -183769.812500\n",
      "Train Epoch: 41 [40832/54000 (76%)] Loss: -211832.187500\n",
      "Train Epoch: 41 [42240/54000 (78%)] Loss: -207726.421875\n",
      "Train Epoch: 41 [43648/54000 (81%)] Loss: -184531.359375\n",
      "Train Epoch: 41 [45056/54000 (83%)] Loss: -225104.203125\n",
      "Train Epoch: 41 [46464/54000 (86%)] Loss: -213275.906250\n",
      "Train Epoch: 41 [47872/54000 (89%)] Loss: -199654.484375\n",
      "Train Epoch: 41 [49280/54000 (91%)] Loss: -196764.125000\n",
      "Train Epoch: 41 [50688/54000 (94%)] Loss: -189440.906250\n",
      "Train Epoch: 41 [52096/54000 (96%)] Loss: -189490.718750\n",
      "    epoch          : 41\n",
      "    loss           : -201072.16854814594\n",
      "    val_loss       : -209128.01995998475\n",
      "Train Epoch: 42 [0/54000 (0%)] Loss: -225574.906250\n",
      "Train Epoch: 42 [1408/54000 (3%)] Loss: -208606.937500\n",
      "Train Epoch: 42 [2816/54000 (5%)] Loss: -184289.156250\n",
      "Train Epoch: 42 [4224/54000 (8%)] Loss: -184753.375000\n",
      "Train Epoch: 42 [5632/54000 (10%)] Loss: -197150.078125\n",
      "Train Epoch: 42 [7040/54000 (13%)] Loss: -201373.031250\n",
      "Train Epoch: 42 [8448/54000 (16%)] Loss: -184095.906250\n",
      "Train Epoch: 42 [9856/54000 (18%)] Loss: -212683.937500\n",
      "Train Epoch: 42 [11264/54000 (21%)] Loss: -208279.562500\n",
      "Train Epoch: 42 [12672/54000 (23%)] Loss: -224976.359375\n",
      "Train Epoch: 42 [14080/54000 (26%)] Loss: -208712.750000\n",
      "Train Epoch: 42 [15488/54000 (29%)] Loss: -195803.343750\n",
      "Train Epoch: 42 [16896/54000 (31%)] Loss: -193906.281250\n",
      "Train Epoch: 42 [18304/54000 (34%)] Loss: -224794.703125\n",
      "Train Epoch: 42 [19712/54000 (37%)] Loss: -200539.750000\n",
      "Train Epoch: 42 [21120/54000 (39%)] Loss: -188999.781250\n",
      "Train Epoch: 42 [22528/54000 (42%)] Loss: -197681.000000\n",
      "Train Epoch: 42 [23936/54000 (44%)] Loss: -184293.125000\n",
      "Train Epoch: 42 [25344/54000 (47%)] Loss: -185740.671875\n",
      "Train Epoch: 42 [26752/54000 (50%)] Loss: -183187.562500\n",
      "Train Epoch: 42 [28160/54000 (52%)] Loss: -187386.453125\n",
      "Train Epoch: 42 [29568/54000 (55%)] Loss: -200561.812500\n",
      "Train Epoch: 42 [30976/54000 (57%)] Loss: -198461.125000\n",
      "Train Epoch: 42 [32384/54000 (60%)] Loss: -205437.046875\n",
      "Train Epoch: 42 [33792/54000 (63%)] Loss: -199892.375000\n",
      "Train Epoch: 42 [35200/54000 (65%)] Loss: -208831.875000\n",
      "Train Epoch: 42 [36608/54000 (68%)] Loss: -196193.625000\n",
      "Train Epoch: 42 [38016/54000 (70%)] Loss: -226514.406250\n",
      "Train Epoch: 42 [39424/54000 (73%)] Loss: -193101.281250\n",
      "Train Epoch: 42 [40832/54000 (76%)] Loss: -215238.218750\n",
      "Train Epoch: 42 [42240/54000 (78%)] Loss: -210560.984375\n",
      "Train Epoch: 42 [43648/54000 (81%)] Loss: -192255.250000\n",
      "Train Epoch: 42 [45056/54000 (83%)] Loss: -218004.593750\n",
      "Train Epoch: 42 [46464/54000 (86%)] Loss: -200627.031250\n",
      "Train Epoch: 42 [47872/54000 (89%)] Loss: -206926.093750\n",
      "Train Epoch: 42 [49280/54000 (91%)] Loss: -188850.203125\n",
      "Train Epoch: 42 [50688/54000 (94%)] Loss: -225704.015625\n",
      "Train Epoch: 42 [52096/54000 (96%)] Loss: -189193.109375\n",
      "    epoch          : 42\n",
      "    loss           : -201119.67385616028\n",
      "    val_loss       : -210377.58338891005\n",
      "Train Epoch: 43 [0/54000 (0%)] Loss: -223197.640625\n",
      "Train Epoch: 43 [1408/54000 (3%)] Loss: -185823.718750\n",
      "Train Epoch: 43 [2816/54000 (5%)] Loss: -192607.218750\n",
      "Train Epoch: 43 [4224/54000 (8%)] Loss: -199924.015625\n",
      "Train Epoch: 43 [5632/54000 (10%)] Loss: -214656.640625\n",
      "Train Epoch: 43 [7040/54000 (13%)] Loss: -206541.875000\n",
      "Train Epoch: 43 [8448/54000 (16%)] Loss: -183148.718750\n",
      "Train Epoch: 43 [9856/54000 (18%)] Loss: -185509.062500\n",
      "Train Epoch: 43 [11264/54000 (21%)] Loss: -195761.562500\n",
      "Train Epoch: 43 [12672/54000 (23%)] Loss: -198909.734375\n",
      "Train Epoch: 43 [14080/54000 (26%)] Loss: -193073.406250\n",
      "Train Epoch: 43 [15488/54000 (29%)] Loss: -193055.781250\n",
      "Train Epoch: 43 [16896/54000 (31%)] Loss: -183814.500000\n",
      "Train Epoch: 43 [18304/54000 (34%)] Loss: -198013.671875\n",
      "Train Epoch: 43 [19712/54000 (37%)] Loss: -200289.328125\n",
      "Train Epoch: 43 [21120/54000 (39%)] Loss: -226762.796875\n",
      "Train Epoch: 43 [22528/54000 (42%)] Loss: -207636.312500\n",
      "Train Epoch: 43 [23936/54000 (44%)] Loss: -215029.562500\n",
      "Train Epoch: 43 [25344/54000 (47%)] Loss: -189203.062500\n",
      "Train Epoch: 43 [26752/54000 (50%)] Loss: -215234.062500\n",
      "Train Epoch: 43 [28160/54000 (52%)] Loss: -215143.859375\n",
      "Train Epoch: 43 [29568/54000 (55%)] Loss: -206722.750000\n",
      "Train Epoch: 43 [30976/54000 (57%)] Loss: -205821.500000\n",
      "Train Epoch: 43 [32384/54000 (60%)] Loss: -195751.031250\n",
      "Train Epoch: 43 [33792/54000 (63%)] Loss: -192928.328125\n",
      "Train Epoch: 43 [35200/54000 (65%)] Loss: -196772.625000\n",
      "Train Epoch: 43 [36608/54000 (68%)] Loss: -186606.046875\n",
      "Train Epoch: 43 [38016/54000 (70%)] Loss: -187452.468750\n",
      "Train Epoch: 43 [39424/54000 (73%)] Loss: -193163.843750\n",
      "Train Epoch: 43 [40832/54000 (76%)] Loss: -210234.187500\n",
      "Train Epoch: 43 [42240/54000 (78%)] Loss: -226464.500000\n",
      "Train Epoch: 43 [43648/54000 (81%)] Loss: -214692.171875\n",
      "Train Epoch: 43 [45056/54000 (83%)] Loss: -195314.625000\n",
      "Train Epoch: 43 [46464/54000 (86%)] Loss: -199760.531250\n",
      "Train Epoch: 43 [47872/54000 (89%)] Loss: -203191.234375\n",
      "Train Epoch: 43 [49280/54000 (91%)] Loss: -208793.593750\n",
      "Train Epoch: 43 [50688/54000 (94%)] Loss: -207659.171875\n",
      "Train Epoch: 43 [52096/54000 (96%)] Loss: -211059.046875\n",
      "    epoch          : 43\n",
      "    loss           : -201446.29085675837\n",
      "    val_loss       : -209434.626953125\n",
      "Train Epoch: 44 [0/54000 (0%)] Loss: -225189.125000\n",
      "Train Epoch: 44 [1408/54000 (3%)] Loss: -196170.859375\n",
      "Train Epoch: 44 [2816/54000 (5%)] Loss: -211809.359375\n",
      "Train Epoch: 44 [4224/54000 (8%)] Loss: -201090.812500\n",
      "Train Epoch: 44 [5632/54000 (10%)] Loss: -208480.640625\n",
      "Train Epoch: 44 [7040/54000 (13%)] Loss: -201348.000000\n",
      "Train Epoch: 44 [8448/54000 (16%)] Loss: -183196.718750\n",
      "Train Epoch: 44 [9856/54000 (18%)] Loss: -191748.406250\n",
      "Train Epoch: 44 [11264/54000 (21%)] Loss: -186258.734375\n",
      "Train Epoch: 44 [12672/54000 (23%)] Loss: -210741.343750\n",
      "Train Epoch: 44 [14080/54000 (26%)] Loss: -217353.765625\n",
      "Train Epoch: 44 [15488/54000 (29%)] Loss: -204481.187500\n",
      "Train Epoch: 44 [16896/54000 (31%)] Loss: -198155.468750\n",
      "Train Epoch: 44 [18304/54000 (34%)] Loss: -190028.468750\n",
      "Train Epoch: 44 [19712/54000 (37%)] Loss: -223016.171875\n",
      "Train Epoch: 44 [21120/54000 (39%)] Loss: -225099.609375\n",
      "Train Epoch: 44 [22528/54000 (42%)] Loss: -196400.156250\n",
      "Train Epoch: 44 [23936/54000 (44%)] Loss: -184273.359375\n",
      "Train Epoch: 44 [25344/54000 (47%)] Loss: -182255.640625\n",
      "Train Epoch: 44 [26752/54000 (50%)] Loss: -193579.687500\n",
      "Train Epoch: 44 [28160/54000 (52%)] Loss: -199951.375000\n",
      "Train Epoch: 44 [29568/54000 (55%)] Loss: -210268.843750\n",
      "Train Epoch: 44 [30976/54000 (57%)] Loss: -200597.328125\n",
      "Train Epoch: 44 [32384/54000 (60%)] Loss: -198066.140625\n",
      "Train Epoch: 44 [33792/54000 (63%)] Loss: -199429.109375\n",
      "Train Epoch: 44 [35200/54000 (65%)] Loss: -184966.953125\n",
      "Train Epoch: 44 [36608/54000 (68%)] Loss: -215386.187500\n",
      "Train Epoch: 44 [38016/54000 (70%)] Loss: -216783.468750\n",
      "Train Epoch: 44 [39424/54000 (73%)] Loss: -214260.515625\n",
      "Train Epoch: 44 [40832/54000 (76%)] Loss: -201612.656250\n",
      "Train Epoch: 44 [42240/54000 (78%)] Loss: -186833.203125\n",
      "Train Epoch: 44 [43648/54000 (81%)] Loss: -227882.765625\n",
      "Train Epoch: 44 [45056/54000 (83%)] Loss: -202124.578125\n",
      "Train Epoch: 44 [46464/54000 (86%)] Loss: -195652.015625\n",
      "Train Epoch: 44 [47872/54000 (89%)] Loss: -198181.890625\n",
      "Train Epoch: 44 [49280/54000 (91%)] Loss: -222041.531250\n",
      "Train Epoch: 44 [50688/54000 (94%)] Loss: -203503.968750\n",
      "Train Epoch: 44 [52096/54000 (96%)] Loss: -208203.421875\n",
      "    epoch          : 44\n",
      "    loss           : -201195.27601674641\n",
      "    val_loss       : -210900.25015482088\n",
      "Train Epoch: 45 [0/54000 (0%)] Loss: -217588.593750\n",
      "Train Epoch: 45 [1408/54000 (3%)] Loss: -201514.281250\n",
      "Train Epoch: 45 [2816/54000 (5%)] Loss: -201241.312500\n",
      "Train Epoch: 45 [4224/54000 (8%)] Loss: -195461.625000\n",
      "Train Epoch: 45 [5632/54000 (10%)] Loss: -215891.359375\n",
      "Train Epoch: 45 [7040/54000 (13%)] Loss: -201567.921875\n",
      "Train Epoch: 45 [8448/54000 (16%)] Loss: -201521.687500\n",
      "Train Epoch: 45 [9856/54000 (18%)] Loss: -187609.671875\n",
      "Train Epoch: 45 [11264/54000 (21%)] Loss: -202437.890625\n",
      "Train Epoch: 45 [12672/54000 (23%)] Loss: -208384.437500\n",
      "Train Epoch: 45 [14080/54000 (26%)] Loss: -219007.187500\n",
      "Train Epoch: 45 [15488/54000 (29%)] Loss: -209385.093750\n",
      "Train Epoch: 45 [16896/54000 (31%)] Loss: -201312.921875\n",
      "Train Epoch: 45 [18304/54000 (34%)] Loss: -204329.500000\n",
      "Train Epoch: 45 [19712/54000 (37%)] Loss: -226523.765625\n",
      "Train Epoch: 45 [21120/54000 (39%)] Loss: -185448.312500\n",
      "Train Epoch: 45 [22528/54000 (42%)] Loss: -190225.468750\n",
      "Train Epoch: 45 [23936/54000 (44%)] Loss: -192492.156250\n",
      "Train Epoch: 45 [25344/54000 (47%)] Loss: -206545.687500\n",
      "Train Epoch: 45 [26752/54000 (50%)] Loss: -224640.093750\n",
      "Train Epoch: 45 [28160/54000 (52%)] Loss: -203046.531250\n",
      "Train Epoch: 45 [29568/54000 (55%)] Loss: -194880.546875\n",
      "Train Epoch: 45 [30976/54000 (57%)] Loss: -200946.625000\n",
      "Train Epoch: 45 [32384/54000 (60%)] Loss: -198581.843750\n",
      "Train Epoch: 45 [33792/54000 (63%)] Loss: -184329.125000\n",
      "Train Epoch: 45 [35200/54000 (65%)] Loss: -208528.140625\n",
      "Train Epoch: 45 [36608/54000 (68%)] Loss: -195405.593750\n",
      "Train Epoch: 45 [38016/54000 (70%)] Loss: -227461.765625\n",
      "Train Epoch: 45 [39424/54000 (73%)] Loss: -191087.250000\n",
      "Train Epoch: 45 [40832/54000 (76%)] Loss: -210852.593750\n",
      "Train Epoch: 45 [42240/54000 (78%)] Loss: -198216.000000\n",
      "Train Epoch: 45 [43648/54000 (81%)] Loss: -196727.390625\n",
      "Train Epoch: 45 [45056/54000 (83%)] Loss: -199000.609375\n",
      "Train Epoch: 45 [46464/54000 (86%)] Loss: -196526.437500\n",
      "Train Epoch: 45 [47872/54000 (89%)] Loss: -206916.531250\n",
      "Train Epoch: 45 [49280/54000 (91%)] Loss: -195128.875000\n",
      "Train Epoch: 45 [50688/54000 (94%)] Loss: -196853.265625\n",
      "Train Epoch: 45 [52096/54000 (96%)] Loss: -199631.796875\n",
      "    epoch          : 45\n",
      "    loss           : -201884.33436752393\n",
      "    val_loss       : -211037.41359803735\n",
      "Train Epoch: 46 [0/54000 (0%)] Loss: -220759.000000\n",
      "Train Epoch: 46 [1408/54000 (3%)] Loss: -209407.437500\n",
      "Train Epoch: 46 [2816/54000 (5%)] Loss: -196977.781250\n",
      "Train Epoch: 46 [4224/54000 (8%)] Loss: -199694.234375\n",
      "Train Epoch: 46 [5632/54000 (10%)] Loss: -200190.218750\n",
      "Train Epoch: 46 [7040/54000 (13%)] Loss: -194951.421875\n",
      "Train Epoch: 46 [8448/54000 (16%)] Loss: -225637.468750\n",
      "Train Epoch: 46 [9856/54000 (18%)] Loss: -185186.375000\n",
      "Train Epoch: 46 [11264/54000 (21%)] Loss: -198027.015625\n",
      "Train Epoch: 46 [12672/54000 (23%)] Loss: -196933.843750\n",
      "Train Epoch: 46 [14080/54000 (26%)] Loss: -186719.765625\n",
      "Train Epoch: 46 [15488/54000 (29%)] Loss: -191562.875000\n",
      "Train Epoch: 46 [16896/54000 (31%)] Loss: -219057.250000\n",
      "Train Epoch: 46 [18304/54000 (34%)] Loss: -214868.437500\n",
      "Train Epoch: 46 [19712/54000 (37%)] Loss: -208647.218750\n",
      "Train Epoch: 46 [21120/54000 (39%)] Loss: -192700.625000\n",
      "Train Epoch: 46 [22528/54000 (42%)] Loss: -186248.781250\n",
      "Train Epoch: 46 [23936/54000 (44%)] Loss: -202525.765625\n",
      "Train Epoch: 46 [25344/54000 (47%)] Loss: -207384.656250\n",
      "Train Epoch: 46 [26752/54000 (50%)] Loss: -202231.171875\n",
      "Train Epoch: 46 [28160/54000 (52%)] Loss: -188980.937500\n",
      "Train Epoch: 46 [29568/54000 (55%)] Loss: -191133.015625\n",
      "Train Epoch: 46 [30976/54000 (57%)] Loss: -193647.562500\n",
      "Train Epoch: 46 [32384/54000 (60%)] Loss: -220978.656250\n",
      "Train Epoch: 46 [33792/54000 (63%)] Loss: -191055.625000\n",
      "Train Epoch: 46 [35200/54000 (65%)] Loss: -202082.265625\n",
      "Train Epoch: 46 [36608/54000 (68%)] Loss: -204451.687500\n",
      "Train Epoch: 46 [38016/54000 (70%)] Loss: -183230.484375\n",
      "Train Epoch: 46 [39424/54000 (73%)] Loss: -183349.156250\n",
      "Train Epoch: 46 [40832/54000 (76%)] Loss: -205416.953125\n",
      "Train Epoch: 46 [42240/54000 (78%)] Loss: -198327.031250\n",
      "Train Epoch: 46 [43648/54000 (81%)] Loss: -194630.593750\n",
      "Train Epoch: 46 [45056/54000 (83%)] Loss: -195040.531250\n",
      "Train Epoch: 46 [46464/54000 (86%)] Loss: -191111.625000\n",
      "Train Epoch: 46 [47872/54000 (89%)] Loss: -193255.468750\n",
      "Train Epoch: 46 [49280/54000 (91%)] Loss: -202790.031250\n",
      "Train Epoch: 46 [50688/54000 (94%)] Loss: -227590.906250\n",
      "Train Epoch: 46 [52096/54000 (96%)] Loss: -226653.953125\n",
      "    epoch          : 46\n",
      "    loss           : -201657.44049043063\n",
      "    val_loss       : -210902.72116758765\n",
      "Train Epoch: 47 [0/54000 (0%)] Loss: -215847.562500\n",
      "Train Epoch: 47 [1408/54000 (3%)] Loss: -216711.375000\n",
      "Train Epoch: 47 [2816/54000 (5%)] Loss: -217279.562500\n",
      "Train Epoch: 47 [4224/54000 (8%)] Loss: -217087.890625\n",
      "Train Epoch: 47 [5632/54000 (10%)] Loss: -200183.906250\n",
      "Train Epoch: 47 [7040/54000 (13%)] Loss: -198692.187500\n",
      "Train Epoch: 47 [8448/54000 (16%)] Loss: -191667.968750\n",
      "Train Epoch: 47 [9856/54000 (18%)] Loss: -201226.000000\n",
      "Train Epoch: 47 [11264/54000 (21%)] Loss: -194618.906250\n",
      "Train Epoch: 47 [12672/54000 (23%)] Loss: -198833.796875\n",
      "Train Epoch: 47 [14080/54000 (26%)] Loss: -216717.906250\n",
      "Train Epoch: 47 [15488/54000 (29%)] Loss: -213548.500000\n",
      "Train Epoch: 47 [16896/54000 (31%)] Loss: -208366.968750\n",
      "Train Epoch: 47 [18304/54000 (34%)] Loss: -207753.984375\n",
      "Train Epoch: 47 [19712/54000 (37%)] Loss: -198741.437500\n",
      "Train Epoch: 47 [21120/54000 (39%)] Loss: -227970.171875\n",
      "Train Epoch: 47 [22528/54000 (42%)] Loss: -195377.937500\n",
      "Train Epoch: 47 [23936/54000 (44%)] Loss: -201288.250000\n",
      "Train Epoch: 47 [25344/54000 (47%)] Loss: -198827.781250\n",
      "Train Epoch: 47 [26752/54000 (50%)] Loss: -199771.687500\n",
      "Train Epoch: 47 [28160/54000 (52%)] Loss: -200488.765625\n",
      "Train Epoch: 47 [29568/54000 (55%)] Loss: -202379.359375\n",
      "Train Epoch: 47 [30976/54000 (57%)] Loss: -208333.515625\n",
      "Train Epoch: 47 [32384/54000 (60%)] Loss: -188589.062500\n",
      "Train Epoch: 47 [33792/54000 (63%)] Loss: -227179.953125\n",
      "Train Epoch: 47 [35200/54000 (65%)] Loss: -200819.343750\n",
      "Train Epoch: 47 [36608/54000 (68%)] Loss: -198718.796875\n",
      "Train Epoch: 47 [38016/54000 (70%)] Loss: -222305.421875\n",
      "Train Epoch: 47 [39424/54000 (73%)] Loss: -199840.812500\n",
      "Train Epoch: 47 [40832/54000 (76%)] Loss: -216433.593750\n",
      "Train Epoch: 47 [42240/54000 (78%)] Loss: -188764.328125\n",
      "Train Epoch: 47 [43648/54000 (81%)] Loss: -191609.984375\n",
      "Train Epoch: 47 [45056/54000 (83%)] Loss: -200196.921875\n",
      "Train Epoch: 47 [46464/54000 (86%)] Loss: -207816.937500\n",
      "Train Epoch: 47 [47872/54000 (89%)] Loss: -203349.609375\n",
      "Train Epoch: 47 [49280/54000 (91%)] Loss: -196320.343750\n",
      "Train Epoch: 47 [50688/54000 (94%)] Loss: -188029.437500\n",
      "Train Epoch: 47 [52096/54000 (96%)] Loss: -197179.546875\n",
      "    epoch          : 47\n",
      "    loss           : -202055.30289324163\n",
      "    val_loss       : -212023.98380335365\n",
      "Train Epoch: 48 [0/54000 (0%)] Loss: -194422.796875\n",
      "Train Epoch: 48 [1408/54000 (3%)] Loss: -196313.093750\n",
      "Train Epoch: 48 [2816/54000 (5%)] Loss: -185995.531250\n",
      "Train Epoch: 48 [4224/54000 (8%)] Loss: -193504.250000\n",
      "Train Epoch: 48 [5632/54000 (10%)] Loss: -189431.578125\n",
      "Train Epoch: 48 [7040/54000 (13%)] Loss: -216433.968750\n",
      "Train Epoch: 48 [8448/54000 (16%)] Loss: -192715.687500\n",
      "Train Epoch: 48 [9856/54000 (18%)] Loss: -228835.718750\n",
      "Train Epoch: 48 [11264/54000 (21%)] Loss: -204314.953125\n",
      "Train Epoch: 48 [12672/54000 (23%)] Loss: -189732.828125\n",
      "Train Epoch: 48 [14080/54000 (26%)] Loss: -185494.718750\n",
      "Train Epoch: 48 [15488/54000 (29%)] Loss: -192840.843750\n",
      "Train Epoch: 48 [16896/54000 (31%)] Loss: -194355.859375\n",
      "Train Epoch: 48 [18304/54000 (34%)] Loss: -208029.875000\n",
      "Train Epoch: 48 [19712/54000 (37%)] Loss: -209063.078125\n",
      "Train Epoch: 48 [21120/54000 (39%)] Loss: -227290.312500\n",
      "Train Epoch: 48 [22528/54000 (42%)] Loss: -200578.640625\n",
      "Train Epoch: 48 [23936/54000 (44%)] Loss: -203063.000000\n",
      "Train Epoch: 48 [25344/54000 (47%)] Loss: -227654.046875\n",
      "Train Epoch: 48 [26752/54000 (50%)] Loss: -196665.609375\n",
      "Train Epoch: 48 [28160/54000 (52%)] Loss: -195222.968750\n",
      "Train Epoch: 48 [29568/54000 (55%)] Loss: -201859.671875\n",
      "Train Epoch: 48 [30976/54000 (57%)] Loss: -203815.562500\n",
      "Train Epoch: 48 [32384/54000 (60%)] Loss: -216041.218750\n",
      "Train Epoch: 48 [33792/54000 (63%)] Loss: -215893.890625\n",
      "Train Epoch: 48 [35200/54000 (65%)] Loss: -217062.781250\n",
      "Train Epoch: 48 [36608/54000 (68%)] Loss: -197061.812500\n",
      "Train Epoch: 48 [38016/54000 (70%)] Loss: -222798.640625\n",
      "Train Epoch: 48 [39424/54000 (73%)] Loss: -213899.984375\n",
      "Train Epoch: 48 [40832/54000 (76%)] Loss: -210064.171875\n",
      "Train Epoch: 48 [42240/54000 (78%)] Loss: -198003.453125\n",
      "Train Epoch: 48 [43648/54000 (81%)] Loss: -227759.343750\n",
      "Train Epoch: 48 [45056/54000 (83%)] Loss: -202941.406250\n",
      "Train Epoch: 48 [46464/54000 (86%)] Loss: -210288.078125\n",
      "Train Epoch: 48 [47872/54000 (89%)] Loss: -182898.718750\n",
      "Train Epoch: 48 [49280/54000 (91%)] Loss: -182672.906250\n",
      "Train Epoch: 48 [50688/54000 (94%)] Loss: -228823.750000\n",
      "Train Epoch: 48 [52096/54000 (96%)] Loss: -189209.531250\n",
      "    epoch          : 48\n",
      "    loss           : -202921.97663726076\n",
      "    val_loss       : -212122.90998951983\n",
      "Train Epoch: 49 [0/54000 (0%)] Loss: -227196.671875\n",
      "Train Epoch: 49 [1408/54000 (3%)] Loss: -203510.687500\n",
      "Train Epoch: 49 [2816/54000 (5%)] Loss: -201024.437500\n",
      "Train Epoch: 49 [4224/54000 (8%)] Loss: -199036.781250\n",
      "Train Epoch: 49 [5632/54000 (10%)] Loss: -216743.750000\n",
      "Train Epoch: 49 [7040/54000 (13%)] Loss: -200029.953125\n",
      "Train Epoch: 49 [8448/54000 (16%)] Loss: -226487.843750\n",
      "Train Epoch: 49 [9856/54000 (18%)] Loss: -200070.187500\n",
      "Train Epoch: 49 [11264/54000 (21%)] Loss: -190242.250000\n",
      "Train Epoch: 49 [12672/54000 (23%)] Loss: -185045.343750\n",
      "Train Epoch: 49 [14080/54000 (26%)] Loss: -207622.437500\n",
      "Train Epoch: 49 [15488/54000 (29%)] Loss: -230410.031250\n",
      "Train Epoch: 49 [16896/54000 (31%)] Loss: -193443.718750\n",
      "Train Epoch: 49 [18304/54000 (34%)] Loss: -209485.562500\n",
      "Train Epoch: 49 [19712/54000 (37%)] Loss: -185344.375000\n",
      "Train Epoch: 49 [21120/54000 (39%)] Loss: -192454.843750\n",
      "Train Epoch: 49 [22528/54000 (42%)] Loss: -186381.359375\n",
      "Train Epoch: 49 [23936/54000 (44%)] Loss: -189773.312500\n",
      "Train Epoch: 49 [25344/54000 (47%)] Loss: -202496.359375\n",
      "Train Epoch: 49 [26752/54000 (50%)] Loss: -188571.062500\n",
      "Train Epoch: 49 [28160/54000 (52%)] Loss: -189071.843750\n",
      "Train Epoch: 49 [29568/54000 (55%)] Loss: -188530.140625\n",
      "Train Epoch: 49 [30976/54000 (57%)] Loss: -202264.921875\n",
      "Train Epoch: 49 [32384/54000 (60%)] Loss: -202523.593750\n",
      "Train Epoch: 49 [33792/54000 (63%)] Loss: -227712.328125\n",
      "Train Epoch: 49 [35200/54000 (65%)] Loss: -196746.562500\n",
      "Train Epoch: 49 [36608/54000 (68%)] Loss: -200820.281250\n",
      "Train Epoch: 49 [38016/54000 (70%)] Loss: -186641.796875\n",
      "Train Epoch: 49 [39424/54000 (73%)] Loss: -212393.281250\n",
      "Train Epoch: 49 [40832/54000 (76%)] Loss: -188539.453125\n",
      "Train Epoch: 49 [42240/54000 (78%)] Loss: -184221.750000\n",
      "Train Epoch: 49 [43648/54000 (81%)] Loss: -197568.125000\n",
      "Train Epoch: 49 [45056/54000 (83%)] Loss: -209015.515625\n",
      "Train Epoch: 49 [46464/54000 (86%)] Loss: -221339.406250\n",
      "Train Epoch: 49 [47872/54000 (89%)] Loss: -203428.750000\n",
      "Train Epoch: 49 [49280/54000 (91%)] Loss: -201191.156250\n",
      "Train Epoch: 49 [50688/54000 (94%)] Loss: -199734.062500\n",
      "Train Epoch: 49 [52096/54000 (96%)] Loss: -197278.171875\n",
      "    epoch          : 49\n",
      "    loss           : -203039.65505382774\n",
      "    val_loss       : -212663.06577505716\n",
      "Train Epoch: 50 [0/54000 (0%)] Loss: -222355.375000\n",
      "Train Epoch: 50 [1408/54000 (3%)] Loss: -202871.937500\n",
      "Train Epoch: 50 [2816/54000 (5%)] Loss: -191479.093750\n",
      "Train Epoch: 50 [4224/54000 (8%)] Loss: -200022.156250\n",
      "Train Epoch: 50 [5632/54000 (10%)] Loss: -201713.640625\n",
      "Train Epoch: 50 [7040/54000 (13%)] Loss: -210318.265625\n",
      "Train Epoch: 50 [8448/54000 (16%)] Loss: -195683.562500\n",
      "Train Epoch: 50 [9856/54000 (18%)] Loss: -196537.343750\n",
      "Train Epoch: 50 [11264/54000 (21%)] Loss: -199098.843750\n",
      "Train Epoch: 50 [12672/54000 (23%)] Loss: -209449.359375\n",
      "Train Epoch: 50 [14080/54000 (26%)] Loss: -229295.531250\n",
      "Train Epoch: 50 [15488/54000 (29%)] Loss: -207873.750000\n",
      "Train Epoch: 50 [16896/54000 (31%)] Loss: -197263.203125\n",
      "Train Epoch: 50 [18304/54000 (34%)] Loss: -187509.843750\n",
      "Train Epoch: 50 [19712/54000 (37%)] Loss: -227114.343750\n",
      "Train Epoch: 50 [21120/54000 (39%)] Loss: -200504.000000\n",
      "Train Epoch: 50 [22528/54000 (42%)] Loss: -202090.281250\n",
      "Train Epoch: 50 [23936/54000 (44%)] Loss: -219439.312500\n",
      "Train Epoch: 50 [25344/54000 (47%)] Loss: -205009.031250\n",
      "Train Epoch: 50 [26752/54000 (50%)] Loss: -206447.421875\n",
      "Train Epoch: 50 [28160/54000 (52%)] Loss: -195775.859375\n",
      "Train Epoch: 50 [29568/54000 (55%)] Loss: -201767.031250\n",
      "Train Epoch: 50 [30976/54000 (57%)] Loss: -186700.171875\n",
      "Train Epoch: 50 [32384/54000 (60%)] Loss: -211980.375000\n",
      "Train Epoch: 50 [33792/54000 (63%)] Loss: -210041.343750\n",
      "Train Epoch: 50 [35200/54000 (65%)] Loss: -198991.125000\n",
      "Train Epoch: 50 [36608/54000 (68%)] Loss: -184120.250000\n",
      "Train Epoch: 50 [38016/54000 (70%)] Loss: -201189.562500\n",
      "Train Epoch: 50 [39424/54000 (73%)] Loss: -207732.531250\n",
      "Train Epoch: 50 [40832/54000 (76%)] Loss: -215522.062500\n",
      "Train Epoch: 50 [42240/54000 (78%)] Loss: -199997.593750\n",
      "Train Epoch: 50 [43648/54000 (81%)] Loss: -218889.718750\n",
      "Train Epoch: 50 [45056/54000 (83%)] Loss: -185646.906250\n",
      "Train Epoch: 50 [46464/54000 (86%)] Loss: -199363.250000\n",
      "Train Epoch: 50 [47872/54000 (89%)] Loss: -191529.750000\n",
      "Train Epoch: 50 [49280/54000 (91%)] Loss: -197717.656250\n",
      "Train Epoch: 50 [50688/54000 (94%)] Loss: -196604.062500\n",
      "Train Epoch: 50 [52096/54000 (96%)] Loss: -212609.515625\n",
      "    epoch          : 50\n",
      "    loss           : -203098.2376270933\n",
      "    val_loss       : -212512.0301543445\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0508_092801/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [0/54000 (0%)] Loss: -228919.390625\n",
      "Train Epoch: 51 [1408/54000 (3%)] Loss: -201434.375000\n",
      "Train Epoch: 51 [2816/54000 (5%)] Loss: -210418.906250\n",
      "Train Epoch: 51 [4224/54000 (8%)] Loss: -193251.109375\n",
      "Train Epoch: 51 [5632/54000 (10%)] Loss: -217318.578125\n",
      "Train Epoch: 51 [7040/54000 (13%)] Loss: -212640.921875\n",
      "Train Epoch: 51 [8448/54000 (16%)] Loss: -224231.125000\n",
      "Train Epoch: 51 [9856/54000 (18%)] Loss: -186938.312500\n",
      "Train Epoch: 51 [11264/54000 (21%)] Loss: -186326.203125\n",
      "Train Epoch: 51 [12672/54000 (23%)] Loss: -202216.718750\n",
      "Train Epoch: 51 [14080/54000 (26%)] Loss: -200750.750000\n",
      "Train Epoch: 51 [15488/54000 (29%)] Loss: -224431.046875\n",
      "Train Epoch: 51 [16896/54000 (31%)] Loss: -190138.375000\n",
      "Train Epoch: 51 [18304/54000 (34%)] Loss: -207824.593750\n",
      "Train Epoch: 51 [19712/54000 (37%)] Loss: -183208.093750\n",
      "Train Epoch: 51 [21120/54000 (39%)] Loss: -229202.453125\n",
      "Train Epoch: 51 [22528/54000 (42%)] Loss: -208210.125000\n",
      "Train Epoch: 51 [23936/54000 (44%)] Loss: -203752.859375\n",
      "Train Epoch: 51 [25344/54000 (47%)] Loss: -194915.406250\n",
      "Train Epoch: 51 [26752/54000 (50%)] Loss: -211459.312500\n",
      "Train Epoch: 51 [28160/54000 (52%)] Loss: -208466.593750\n",
      "Train Epoch: 51 [29568/54000 (55%)] Loss: -202832.296875\n",
      "Train Epoch: 51 [30976/54000 (57%)] Loss: -200042.843750\n",
      "Train Epoch: 51 [32384/54000 (60%)] Loss: -190971.046875\n",
      "Train Epoch: 51 [33792/54000 (63%)] Loss: -226269.750000\n",
      "Train Epoch: 51 [35200/54000 (65%)] Loss: -192456.015625\n",
      "Train Epoch: 51 [36608/54000 (68%)] Loss: -208397.750000\n",
      "Train Epoch: 51 [38016/54000 (70%)] Loss: -203529.921875\n",
      "Train Epoch: 51 [39424/54000 (73%)] Loss: -227820.437500\n",
      "Train Epoch: 51 [40832/54000 (76%)] Loss: -206037.296875\n",
      "Train Epoch: 51 [42240/54000 (78%)] Loss: -197686.937500\n",
      "Train Epoch: 51 [43648/54000 (81%)] Loss: -199779.500000\n",
      "Train Epoch: 51 [45056/54000 (83%)] Loss: -202543.437500\n",
      "Train Epoch: 51 [46464/54000 (86%)] Loss: -188455.718750\n",
      "Train Epoch: 51 [47872/54000 (89%)] Loss: -190454.312500\n",
      "Train Epoch: 51 [49280/54000 (91%)] Loss: -193623.093750\n",
      "Train Epoch: 51 [50688/54000 (94%)] Loss: -211407.406250\n",
      "Train Epoch: 51 [52096/54000 (96%)] Loss: -190813.468750\n",
      "    epoch          : 51\n",
      "    loss           : -203780.47559061006\n",
      "    val_loss       : -213321.2275033346\n",
      "Train Epoch: 52 [0/54000 (0%)] Loss: -225480.031250\n",
      "Train Epoch: 52 [1408/54000 (3%)] Loss: -209736.390625\n",
      "Train Epoch: 52 [2816/54000 (5%)] Loss: -210005.187500\n",
      "Train Epoch: 52 [4224/54000 (8%)] Loss: -190729.593750\n",
      "Train Epoch: 52 [5632/54000 (10%)] Loss: -198188.906250\n",
      "Train Epoch: 52 [7040/54000 (13%)] Loss: -216477.640625\n",
      "Train Epoch: 52 [8448/54000 (16%)] Loss: -187825.968750\n",
      "Train Epoch: 52 [9856/54000 (18%)] Loss: -193739.015625\n",
      "Train Epoch: 52 [11264/54000 (21%)] Loss: -196830.031250\n",
      "Train Epoch: 52 [12672/54000 (23%)] Loss: -216297.593750\n",
      "Train Epoch: 52 [14080/54000 (26%)] Loss: -204116.468750\n",
      "Train Epoch: 52 [15488/54000 (29%)] Loss: -197526.734375\n",
      "Train Epoch: 52 [16896/54000 (31%)] Loss: -198212.062500\n",
      "Train Epoch: 52 [18304/54000 (34%)] Loss: -208616.687500\n",
      "Train Epoch: 52 [19712/54000 (37%)] Loss: -203869.765625\n",
      "Train Epoch: 52 [21120/54000 (39%)] Loss: -191969.062500\n",
      "Train Epoch: 52 [22528/54000 (42%)] Loss: -208882.781250\n",
      "Train Epoch: 52 [23936/54000 (44%)] Loss: -191263.578125\n",
      "Train Epoch: 52 [25344/54000 (47%)] Loss: -228162.718750\n",
      "Train Epoch: 52 [26752/54000 (50%)] Loss: -207918.250000\n",
      "Train Epoch: 52 [28160/54000 (52%)] Loss: -203436.609375\n",
      "Train Epoch: 52 [29568/54000 (55%)] Loss: -202985.343750\n",
      "Train Epoch: 52 [30976/54000 (57%)] Loss: -199595.593750\n",
      "Train Epoch: 52 [32384/54000 (60%)] Loss: -201378.640625\n",
      "Train Epoch: 52 [33792/54000 (63%)] Loss: -228020.750000\n",
      "Train Epoch: 52 [35200/54000 (65%)] Loss: -209679.015625\n",
      "Train Epoch: 52 [36608/54000 (68%)] Loss: -186557.656250\n",
      "Train Epoch: 52 [38016/54000 (70%)] Loss: -194129.796875\n",
      "Train Epoch: 52 [39424/54000 (73%)] Loss: -199132.859375\n",
      "Train Epoch: 52 [40832/54000 (76%)] Loss: -192207.609375\n",
      "Train Epoch: 52 [42240/54000 (78%)] Loss: -188737.218750\n",
      "Train Epoch: 52 [43648/54000 (81%)] Loss: -204329.750000\n",
      "Train Epoch: 52 [45056/54000 (83%)] Loss: -218042.328125\n",
      "Train Epoch: 52 [46464/54000 (86%)] Loss: -228784.593750\n",
      "Train Epoch: 52 [47872/54000 (89%)] Loss: -205474.593750\n",
      "Train Epoch: 52 [49280/54000 (91%)] Loss: -202401.421875\n",
      "Train Epoch: 52 [50688/54000 (94%)] Loss: -205989.562500\n",
      "Train Epoch: 52 [52096/54000 (96%)] Loss: -192195.875000\n",
      "    epoch          : 52\n",
      "    loss           : -203899.98190789475\n",
      "    val_loss       : -213837.99578410824\n",
      "Train Epoch: 53 [0/54000 (0%)] Loss: -225652.375000\n",
      "Train Epoch: 53 [1408/54000 (3%)] Loss: -184814.546875\n",
      "Train Epoch: 53 [2816/54000 (5%)] Loss: -197839.781250\n",
      "Train Epoch: 53 [4224/54000 (8%)] Loss: -204781.375000\n",
      "Train Epoch: 53 [5632/54000 (10%)] Loss: -199132.531250\n",
      "Train Epoch: 53 [7040/54000 (13%)] Loss: -189351.781250\n",
      "Train Epoch: 53 [8448/54000 (16%)] Loss: -228341.171875\n",
      "Train Epoch: 53 [9856/54000 (18%)] Loss: -229144.234375\n",
      "Train Epoch: 53 [11264/54000 (21%)] Loss: -187693.750000\n",
      "Train Epoch: 53 [12672/54000 (23%)] Loss: -200588.875000\n",
      "Train Epoch: 53 [14080/54000 (26%)] Loss: -215735.625000\n",
      "Train Epoch: 53 [15488/54000 (29%)] Loss: -205004.500000\n",
      "Train Epoch: 53 [16896/54000 (31%)] Loss: -192339.765625\n",
      "Train Epoch: 53 [18304/54000 (34%)] Loss: -205957.828125\n",
      "Train Epoch: 53 [19712/54000 (37%)] Loss: -207184.656250\n",
      "Train Epoch: 53 [21120/54000 (39%)] Loss: -200244.750000\n",
      "Train Epoch: 53 [22528/54000 (42%)] Loss: -207713.656250\n",
      "Train Epoch: 53 [23936/54000 (44%)] Loss: -192703.421875\n",
      "Train Epoch: 53 [25344/54000 (47%)] Loss: -204389.828125\n",
      "Train Epoch: 53 [26752/54000 (50%)] Loss: -205904.812500\n",
      "Train Epoch: 53 [28160/54000 (52%)] Loss: -203277.718750\n",
      "Train Epoch: 53 [29568/54000 (55%)] Loss: -219176.718750\n",
      "Train Epoch: 53 [30976/54000 (57%)] Loss: -206517.750000\n",
      "Train Epoch: 53 [32384/54000 (60%)] Loss: -213456.593750\n",
      "Train Epoch: 53 [33792/54000 (63%)] Loss: -205957.734375\n",
      "Train Epoch: 53 [35200/54000 (65%)] Loss: -195339.968750\n",
      "Train Epoch: 53 [36608/54000 (68%)] Loss: -198736.046875\n",
      "Train Epoch: 53 [38016/54000 (70%)] Loss: -188003.140625\n",
      "Train Epoch: 53 [39424/54000 (73%)] Loss: -204143.093750\n",
      "Train Epoch: 53 [40832/54000 (76%)] Loss: -198994.796875\n",
      "Train Epoch: 53 [42240/54000 (78%)] Loss: -201362.500000\n",
      "Train Epoch: 53 [43648/54000 (81%)] Loss: -210379.359375\n",
      "Train Epoch: 53 [45056/54000 (83%)] Loss: -228849.171875\n",
      "Train Epoch: 53 [46464/54000 (86%)] Loss: -204001.406250\n",
      "Train Epoch: 53 [47872/54000 (89%)] Loss: -189526.578125\n",
      "Train Epoch: 53 [49280/54000 (91%)] Loss: -199543.156250\n",
      "Train Epoch: 53 [50688/54000 (94%)] Loss: -212573.281250\n",
      "Train Epoch: 53 [52096/54000 (96%)] Loss: -207544.671875\n",
      "    epoch          : 53\n",
      "    loss           : -204286.07562051434\n",
      "    val_loss       : -214555.66576552973\n",
      "Train Epoch: 54 [0/54000 (0%)] Loss: -202121.625000\n",
      "Train Epoch: 54 [1408/54000 (3%)] Loss: -201916.312500\n",
      "Train Epoch: 54 [2816/54000 (5%)] Loss: -190324.843750\n",
      "Train Epoch: 54 [4224/54000 (8%)] Loss: -205235.031250\n",
      "Train Epoch: 54 [5632/54000 (10%)] Loss: -211032.843750\n",
      "Train Epoch: 54 [7040/54000 (13%)] Loss: -197859.000000\n",
      "Train Epoch: 54 [8448/54000 (16%)] Loss: -217446.093750\n",
      "Train Epoch: 54 [9856/54000 (18%)] Loss: -202209.828125\n",
      "Train Epoch: 54 [11264/54000 (21%)] Loss: -209545.718750\n",
      "Train Epoch: 54 [12672/54000 (23%)] Loss: -202147.093750\n",
      "Train Epoch: 54 [14080/54000 (26%)] Loss: -229365.234375\n",
      "Train Epoch: 54 [15488/54000 (29%)] Loss: -203174.984375\n",
      "Train Epoch: 54 [16896/54000 (31%)] Loss: -196277.656250\n",
      "Train Epoch: 54 [18304/54000 (34%)] Loss: -204864.468750\n",
      "Train Epoch: 54 [19712/54000 (37%)] Loss: -188151.828125\n",
      "Train Epoch: 54 [21120/54000 (39%)] Loss: -190311.125000\n",
      "Train Epoch: 54 [22528/54000 (42%)] Loss: -205056.531250\n",
      "Train Epoch: 54 [23936/54000 (44%)] Loss: -210968.406250\n",
      "Train Epoch: 54 [25344/54000 (47%)] Loss: -216872.500000\n",
      "Train Epoch: 54 [26752/54000 (50%)] Loss: -202362.781250\n",
      "Train Epoch: 54 [28160/54000 (52%)] Loss: -199022.281250\n",
      "Train Epoch: 54 [29568/54000 (55%)] Loss: -230188.093750\n",
      "Train Epoch: 54 [30976/54000 (57%)] Loss: -198201.906250\n",
      "Train Epoch: 54 [32384/54000 (60%)] Loss: -209249.187500\n",
      "Train Epoch: 54 [33792/54000 (63%)] Loss: -186560.812500\n",
      "Train Epoch: 54 [35200/54000 (65%)] Loss: -190207.578125\n",
      "Train Epoch: 54 [36608/54000 (68%)] Loss: -204653.468750\n",
      "Train Epoch: 54 [38016/54000 (70%)] Loss: -219173.656250\n",
      "Train Epoch: 54 [39424/54000 (73%)] Loss: -215859.453125\n",
      "Train Epoch: 54 [40832/54000 (76%)] Loss: -230607.906250\n",
      "Train Epoch: 54 [42240/54000 (78%)] Loss: -205024.125000\n",
      "Train Epoch: 54 [43648/54000 (81%)] Loss: -209837.343750\n",
      "Train Epoch: 54 [45056/54000 (83%)] Loss: -202009.796875\n",
      "Train Epoch: 54 [46464/54000 (86%)] Loss: -203750.593750\n",
      "Train Epoch: 54 [47872/54000 (89%)] Loss: -204325.000000\n",
      "Train Epoch: 54 [49280/54000 (91%)] Loss: -190078.671875\n",
      "Train Epoch: 54 [50688/54000 (94%)] Loss: -211456.718750\n",
      "Train Epoch: 54 [52096/54000 (96%)] Loss: -212703.843750\n",
      "    epoch          : 54\n",
      "    loss           : -204578.170118122\n",
      "    val_loss       : -213979.41194264483\n",
      "Train Epoch: 55 [0/54000 (0%)] Loss: -203183.968750\n",
      "Train Epoch: 55 [1408/54000 (3%)] Loss: -213337.156250\n",
      "Train Epoch: 55 [2816/54000 (5%)] Loss: -197421.218750\n",
      "Train Epoch: 55 [4224/54000 (8%)] Loss: -218924.515625\n",
      "Train Epoch: 55 [5632/54000 (10%)] Loss: -198390.093750\n",
      "Train Epoch: 55 [7040/54000 (13%)] Loss: -229526.875000\n",
      "Train Epoch: 55 [8448/54000 (16%)] Loss: -186213.968750\n",
      "Train Epoch: 55 [9856/54000 (18%)] Loss: -199721.359375\n",
      "Train Epoch: 55 [11264/54000 (21%)] Loss: -205221.500000\n",
      "Train Epoch: 55 [12672/54000 (23%)] Loss: -211838.656250\n",
      "Train Epoch: 55 [14080/54000 (26%)] Loss: -229323.953125\n",
      "Train Epoch: 55 [15488/54000 (29%)] Loss: -195694.250000\n",
      "Train Epoch: 55 [16896/54000 (31%)] Loss: -188175.296875\n",
      "Train Epoch: 55 [18304/54000 (34%)] Loss: -189775.593750\n",
      "Train Epoch: 55 [19712/54000 (37%)] Loss: -189067.437500\n",
      "Train Epoch: 55 [21120/54000 (39%)] Loss: -192253.687500\n",
      "Train Epoch: 55 [22528/54000 (42%)] Loss: -216770.421875\n",
      "Train Epoch: 55 [23936/54000 (44%)] Loss: -204393.625000\n",
      "Train Epoch: 55 [25344/54000 (47%)] Loss: -196606.125000\n",
      "Train Epoch: 55 [26752/54000 (50%)] Loss: -199873.171875\n",
      "Train Epoch: 55 [28160/54000 (52%)] Loss: -199520.421875\n",
      "Train Epoch: 55 [29568/54000 (55%)] Loss: -203449.046875\n",
      "Train Epoch: 55 [30976/54000 (57%)] Loss: -197137.500000\n",
      "Train Epoch: 55 [32384/54000 (60%)] Loss: -230567.281250\n",
      "Train Epoch: 55 [33792/54000 (63%)] Loss: -206674.359375\n",
      "Train Epoch: 55 [35200/54000 (65%)] Loss: -195207.250000\n",
      "Train Epoch: 55 [36608/54000 (68%)] Loss: -213411.203125\n",
      "Train Epoch: 55 [38016/54000 (70%)] Loss: -217029.218750\n",
      "Train Epoch: 55 [39424/54000 (73%)] Loss: -197553.406250\n",
      "Train Epoch: 55 [40832/54000 (76%)] Loss: -189148.140625\n",
      "Train Epoch: 55 [42240/54000 (78%)] Loss: -199056.906250\n",
      "Train Epoch: 55 [43648/54000 (81%)] Loss: -218809.375000\n",
      "Train Epoch: 55 [45056/54000 (83%)] Loss: -194452.812500\n",
      "Train Epoch: 55 [46464/54000 (86%)] Loss: -179792.937500\n",
      "Train Epoch: 55 [47872/54000 (89%)] Loss: -197484.546875\n",
      "Train Epoch: 55 [49280/54000 (91%)] Loss: -195905.296875\n",
      "Train Epoch: 55 [50688/54000 (94%)] Loss: -195582.609375\n",
      "Train Epoch: 55 [52096/54000 (96%)] Loss: -209554.171875\n",
      "    epoch          : 55\n",
      "    loss           : -205124.45577900717\n",
      "    val_loss       : -214622.82917301828\n",
      "Train Epoch: 56 [0/54000 (0%)] Loss: -194920.218750\n",
      "Train Epoch: 56 [1408/54000 (3%)] Loss: -229511.250000\n",
      "Train Epoch: 56 [2816/54000 (5%)] Loss: -203399.093750\n",
      "Train Epoch: 56 [4224/54000 (8%)] Loss: -198851.406250\n",
      "Train Epoch: 56 [5632/54000 (10%)] Loss: -218715.750000\n",
      "Train Epoch: 56 [7040/54000 (13%)] Loss: -192249.000000\n",
      "Train Epoch: 56 [8448/54000 (16%)] Loss: -186287.484375\n",
      "Train Epoch: 56 [9856/54000 (18%)] Loss: -216199.281250\n",
      "Train Epoch: 56 [11264/54000 (21%)] Loss: -190240.390625\n",
      "Train Epoch: 56 [12672/54000 (23%)] Loss: -198491.875000\n",
      "Train Epoch: 56 [14080/54000 (26%)] Loss: -198523.656250\n",
      "Train Epoch: 56 [15488/54000 (29%)] Loss: -197037.625000\n",
      "Train Epoch: 56 [16896/54000 (31%)] Loss: -230592.562500\n",
      "Train Epoch: 56 [18304/54000 (34%)] Loss: -191954.890625\n",
      "Train Epoch: 56 [19712/54000 (37%)] Loss: -216841.015625\n",
      "Train Epoch: 56 [21120/54000 (39%)] Loss: -200755.390625\n",
      "Train Epoch: 56 [22528/54000 (42%)] Loss: -221375.687500\n",
      "Train Epoch: 56 [23936/54000 (44%)] Loss: -204499.671875\n",
      "Train Epoch: 56 [25344/54000 (47%)] Loss: -206259.593750\n",
      "Train Epoch: 56 [26752/54000 (50%)] Loss: -190733.562500\n",
      "Train Epoch: 56 [28160/54000 (52%)] Loss: -200155.875000\n",
      "Train Epoch: 56 [29568/54000 (55%)] Loss: -204740.468750\n",
      "Train Epoch: 56 [30976/54000 (57%)] Loss: -210635.000000\n",
      "Train Epoch: 56 [32384/54000 (60%)] Loss: -206272.953125\n",
      "Train Epoch: 56 [33792/54000 (63%)] Loss: -203627.687500\n",
      "Train Epoch: 56 [35200/54000 (65%)] Loss: -195700.671875\n",
      "Train Epoch: 56 [36608/54000 (68%)] Loss: -187677.046875\n",
      "Train Epoch: 56 [38016/54000 (70%)] Loss: -222878.468750\n",
      "Train Epoch: 56 [39424/54000 (73%)] Loss: -197206.437500\n",
      "Train Epoch: 56 [40832/54000 (76%)] Loss: -195857.968750\n",
      "Train Epoch: 56 [42240/54000 (78%)] Loss: -208227.578125\n",
      "Train Epoch: 56 [43648/54000 (81%)] Loss: -229060.671875\n",
      "Train Epoch: 56 [45056/54000 (83%)] Loss: -198919.406250\n",
      "Train Epoch: 56 [46464/54000 (86%)] Loss: -202558.421875\n",
      "Train Epoch: 56 [47872/54000 (89%)] Loss: -191913.281250\n",
      "Train Epoch: 56 [49280/54000 (91%)] Loss: -206685.125000\n",
      "Train Epoch: 56 [50688/54000 (94%)] Loss: -205856.406250\n",
      "Train Epoch: 56 [52096/54000 (96%)] Loss: -214251.671875\n",
      "    epoch          : 56\n",
      "    loss           : -205315.46908642346\n",
      "    val_loss       : -214772.9472060785\n",
      "Train Epoch: 57 [0/54000 (0%)] Loss: -195605.062500\n",
      "Train Epoch: 57 [1408/54000 (3%)] Loss: -202404.687500\n",
      "Train Epoch: 57 [2816/54000 (5%)] Loss: -195260.375000\n",
      "Train Epoch: 57 [4224/54000 (8%)] Loss: -201914.750000\n",
      "Train Epoch: 57 [5632/54000 (10%)] Loss: -198684.687500\n",
      "Train Epoch: 57 [7040/54000 (13%)] Loss: -191683.687500\n",
      "Train Epoch: 57 [8448/54000 (16%)] Loss: -196169.828125\n",
      "Train Epoch: 57 [9856/54000 (18%)] Loss: -228191.484375\n",
      "Train Epoch: 57 [11264/54000 (21%)] Loss: -199836.687500\n",
      "Train Epoch: 57 [12672/54000 (23%)] Loss: -207073.343750\n",
      "Train Epoch: 57 [14080/54000 (26%)] Loss: -206125.140625\n",
      "Train Epoch: 57 [15488/54000 (29%)] Loss: -209460.953125\n",
      "Train Epoch: 57 [16896/54000 (31%)] Loss: -190699.750000\n",
      "Train Epoch: 57 [18304/54000 (34%)] Loss: -191618.828125\n",
      "Train Epoch: 57 [19712/54000 (37%)] Loss: -204142.250000\n",
      "Train Epoch: 57 [21120/54000 (39%)] Loss: -208573.265625\n",
      "Train Epoch: 57 [22528/54000 (42%)] Loss: -218177.515625\n",
      "Train Epoch: 57 [23936/54000 (44%)] Loss: -219345.500000\n",
      "Train Epoch: 57 [25344/54000 (47%)] Loss: -204294.156250\n",
      "Train Epoch: 57 [26752/54000 (50%)] Loss: -195289.906250\n",
      "Train Epoch: 57 [28160/54000 (52%)] Loss: -193225.765625\n",
      "Train Epoch: 57 [29568/54000 (55%)] Loss: -191111.593750\n",
      "Train Epoch: 57 [30976/54000 (57%)] Loss: -200832.015625\n",
      "Train Epoch: 57 [32384/54000 (60%)] Loss: -224159.890625\n",
      "Train Epoch: 57 [33792/54000 (63%)] Loss: -211399.468750\n",
      "Train Epoch: 57 [35200/54000 (65%)] Loss: -196823.296875\n",
      "Train Epoch: 57 [36608/54000 (68%)] Loss: -191822.703125\n",
      "Train Epoch: 57 [38016/54000 (70%)] Loss: -187325.218750\n",
      "Train Epoch: 57 [39424/54000 (73%)] Loss: -200749.640625\n",
      "Train Epoch: 57 [40832/54000 (76%)] Loss: -206261.437500\n",
      "Train Epoch: 57 [42240/54000 (78%)] Loss: -205175.000000\n",
      "Train Epoch: 57 [43648/54000 (81%)] Loss: -200262.062500\n",
      "Train Epoch: 57 [45056/54000 (83%)] Loss: -205372.656250\n",
      "Train Epoch: 57 [46464/54000 (86%)] Loss: -213107.875000\n",
      "Train Epoch: 57 [47872/54000 (89%)] Loss: -207257.125000\n",
      "Train Epoch: 57 [49280/54000 (91%)] Loss: -207602.812500\n",
      "Train Epoch: 57 [50688/54000 (94%)] Loss: -225470.046875\n",
      "Train Epoch: 57 [52096/54000 (96%)] Loss: -208724.812500\n",
      "    epoch          : 57\n",
      "    loss           : -205613.6161034689\n",
      "    val_loss       : -214547.6239400724\n",
      "Train Epoch: 58 [0/54000 (0%)] Loss: -230302.937500\n",
      "Train Epoch: 58 [1408/54000 (3%)] Loss: -215279.984375\n",
      "Train Epoch: 58 [2816/54000 (5%)] Loss: -205809.578125\n",
      "Train Epoch: 58 [4224/54000 (8%)] Loss: -193289.468750\n",
      "Train Epoch: 58 [5632/54000 (10%)] Loss: -197329.390625\n",
      "Train Epoch: 58 [7040/54000 (13%)] Loss: -200323.781250\n",
      "Train Epoch: 58 [8448/54000 (16%)] Loss: -188378.296875\n",
      "Train Epoch: 58 [9856/54000 (18%)] Loss: -211765.656250\n",
      "Train Epoch: 58 [11264/54000 (21%)] Loss: -230386.687500\n",
      "Train Epoch: 58 [12672/54000 (23%)] Loss: -197524.906250\n",
      "Train Epoch: 58 [14080/54000 (26%)] Loss: -202674.562500\n",
      "Train Epoch: 58 [15488/54000 (29%)] Loss: -229359.937500\n",
      "Train Epoch: 58 [16896/54000 (31%)] Loss: -196670.562500\n",
      "Train Epoch: 58 [18304/54000 (34%)] Loss: -217663.125000\n",
      "Train Epoch: 58 [19712/54000 (37%)] Loss: -218719.546875\n",
      "Train Epoch: 58 [21120/54000 (39%)] Loss: -206231.000000\n",
      "Train Epoch: 58 [22528/54000 (42%)] Loss: -217404.546875\n",
      "Train Epoch: 58 [23936/54000 (44%)] Loss: -206070.750000\n",
      "Train Epoch: 58 [25344/54000 (47%)] Loss: -196237.734375\n",
      "Train Epoch: 58 [26752/54000 (50%)] Loss: -206135.875000\n",
      "Train Epoch: 58 [28160/54000 (52%)] Loss: -199277.343750\n",
      "Train Epoch: 58 [29568/54000 (55%)] Loss: -203786.625000\n",
      "Train Epoch: 58 [30976/54000 (57%)] Loss: -226477.281250\n",
      "Train Epoch: 58 [32384/54000 (60%)] Loss: -204909.906250\n",
      "Train Epoch: 58 [33792/54000 (63%)] Loss: -192377.781250\n",
      "Train Epoch: 58 [35200/54000 (65%)] Loss: -200782.093750\n",
      "Train Epoch: 58 [36608/54000 (68%)] Loss: -229658.000000\n",
      "Train Epoch: 58 [38016/54000 (70%)] Loss: -203879.578125\n",
      "Train Epoch: 58 [39424/54000 (73%)] Loss: -219152.281250\n",
      "Train Epoch: 58 [40832/54000 (76%)] Loss: -202448.406250\n",
      "Train Epoch: 58 [42240/54000 (78%)] Loss: -204857.437500\n",
      "Train Epoch: 58 [43648/54000 (81%)] Loss: -189317.000000\n",
      "Train Epoch: 58 [45056/54000 (83%)] Loss: -196729.125000\n",
      "Train Epoch: 58 [46464/54000 (86%)] Loss: -201747.578125\n",
      "Train Epoch: 58 [47872/54000 (89%)] Loss: -207031.312500\n",
      "Train Epoch: 58 [49280/54000 (91%)] Loss: -196819.046875\n",
      "Train Epoch: 58 [50688/54000 (94%)] Loss: -230988.125000\n",
      "Train Epoch: 58 [52096/54000 (96%)] Loss: -193954.578125\n",
      "    epoch          : 58\n",
      "    loss           : -205760.57633074163\n",
      "    val_loss       : -215458.74659394054\n",
      "Train Epoch: 59 [0/54000 (0%)] Loss: -186876.890625\n",
      "Train Epoch: 59 [1408/54000 (3%)] Loss: -214759.000000\n",
      "Train Epoch: 59 [2816/54000 (5%)] Loss: -199288.390625\n",
      "Train Epoch: 59 [4224/54000 (8%)] Loss: -188076.046875\n",
      "Train Epoch: 59 [5632/54000 (10%)] Loss: -213626.296875\n",
      "Train Epoch: 59 [7040/54000 (13%)] Loss: -212513.640625\n",
      "Train Epoch: 59 [8448/54000 (16%)] Loss: -231998.531250\n",
      "Train Epoch: 59 [9856/54000 (18%)] Loss: -192073.562500\n",
      "Train Epoch: 59 [11264/54000 (21%)] Loss: -230016.328125\n",
      "Train Epoch: 59 [12672/54000 (23%)] Loss: -204619.187500\n",
      "Train Epoch: 59 [14080/54000 (26%)] Loss: -207901.500000\n",
      "Train Epoch: 59 [15488/54000 (29%)] Loss: -205857.984375\n",
      "Train Epoch: 59 [16896/54000 (31%)] Loss: -200878.593750\n",
      "Train Epoch: 59 [18304/54000 (34%)] Loss: -220289.687500\n",
      "Train Epoch: 59 [19712/54000 (37%)] Loss: -185713.046875\n",
      "Train Epoch: 59 [21120/54000 (39%)] Loss: -219777.546875\n",
      "Train Epoch: 59 [22528/54000 (42%)] Loss: -212025.625000\n",
      "Train Epoch: 59 [23936/54000 (44%)] Loss: -226094.453125\n",
      "Train Epoch: 59 [25344/54000 (47%)] Loss: -197775.390625\n",
      "Train Epoch: 59 [26752/54000 (50%)] Loss: -194218.062500\n",
      "Train Epoch: 59 [28160/54000 (52%)] Loss: -211739.031250\n",
      "Train Epoch: 59 [29568/54000 (55%)] Loss: -230047.656250\n",
      "Train Epoch: 59 [30976/54000 (57%)] Loss: -205634.359375\n",
      "Train Epoch: 59 [32384/54000 (60%)] Loss: -213105.109375\n",
      "Train Epoch: 59 [33792/54000 (63%)] Loss: -206109.984375\n",
      "Train Epoch: 59 [35200/54000 (65%)] Loss: -204586.546875\n",
      "Train Epoch: 59 [36608/54000 (68%)] Loss: -222268.875000\n",
      "Train Epoch: 59 [38016/54000 (70%)] Loss: -203847.140625\n",
      "Train Epoch: 59 [39424/54000 (73%)] Loss: -201746.718750\n",
      "Train Epoch: 59 [40832/54000 (76%)] Loss: -202374.406250\n",
      "Train Epoch: 59 [42240/54000 (78%)] Loss: -205117.687500\n",
      "Train Epoch: 59 [43648/54000 (81%)] Loss: -205906.156250\n",
      "Train Epoch: 59 [45056/54000 (83%)] Loss: -225742.718750\n",
      "Train Epoch: 59 [46464/54000 (86%)] Loss: -199598.312500\n",
      "Train Epoch: 59 [47872/54000 (89%)] Loss: -201932.046875\n",
      "Train Epoch: 59 [49280/54000 (91%)] Loss: -205004.468750\n",
      "Train Epoch: 59 [50688/54000 (94%)] Loss: -190540.062500\n",
      "Train Epoch: 59 [52096/54000 (96%)] Loss: -220088.734375\n",
      "    epoch          : 59\n",
      "    loss           : -206136.42886513157\n",
      "    val_loss       : -215227.10807688642\n",
      "Train Epoch: 60 [0/54000 (0%)] Loss: -232112.296875\n",
      "Train Epoch: 60 [1408/54000 (3%)] Loss: -226981.468750\n",
      "Train Epoch: 60 [2816/54000 (5%)] Loss: -213407.812500\n",
      "Train Epoch: 60 [4224/54000 (8%)] Loss: -194436.781250\n",
      "Train Epoch: 60 [5632/54000 (10%)] Loss: -219618.687500\n",
      "Train Epoch: 60 [7040/54000 (13%)] Loss: -197381.812500\n",
      "Train Epoch: 60 [8448/54000 (16%)] Loss: -193956.187500\n",
      "Train Epoch: 60 [9856/54000 (18%)] Loss: -207053.843750\n",
      "Train Epoch: 60 [11264/54000 (21%)] Loss: -214346.281250\n",
      "Train Epoch: 60 [12672/54000 (23%)] Loss: -198214.156250\n",
      "Train Epoch: 60 [14080/54000 (26%)] Loss: -202240.203125\n",
      "Train Epoch: 60 [15488/54000 (29%)] Loss: -225879.343750\n",
      "Train Epoch: 60 [16896/54000 (31%)] Loss: -196263.312500\n",
      "Train Epoch: 60 [18304/54000 (34%)] Loss: -208634.937500\n",
      "Train Epoch: 60 [19712/54000 (37%)] Loss: -216835.843750\n",
      "Train Epoch: 60 [21120/54000 (39%)] Loss: -190985.796875\n",
      "Train Epoch: 60 [22528/54000 (42%)] Loss: -193010.812500\n",
      "Train Epoch: 60 [23936/54000 (44%)] Loss: -213319.812500\n",
      "Train Epoch: 60 [25344/54000 (47%)] Loss: -199244.234375\n",
      "Train Epoch: 60 [26752/54000 (50%)] Loss: -192503.375000\n",
      "Train Epoch: 60 [28160/54000 (52%)] Loss: -198635.015625\n",
      "Train Epoch: 60 [29568/54000 (55%)] Loss: -202248.500000\n",
      "Train Epoch: 60 [30976/54000 (57%)] Loss: -203668.218750\n",
      "Train Epoch: 60 [32384/54000 (60%)] Loss: -214163.859375\n",
      "Train Epoch: 60 [33792/54000 (63%)] Loss: -198404.187500\n",
      "Train Epoch: 60 [35200/54000 (65%)] Loss: -212530.312500\n",
      "Train Epoch: 60 [36608/54000 (68%)] Loss: -190397.484375\n",
      "Train Epoch: 60 [38016/54000 (70%)] Loss: -230906.734375\n",
      "Train Epoch: 60 [39424/54000 (73%)] Loss: -233714.156250\n",
      "Train Epoch: 60 [40832/54000 (76%)] Loss: -192191.593750\n",
      "Train Epoch: 60 [42240/54000 (78%)] Loss: -197883.656250\n",
      "Train Epoch: 60 [43648/54000 (81%)] Loss: -220212.406250\n",
      "Train Epoch: 60 [45056/54000 (83%)] Loss: -200199.671875\n",
      "Train Epoch: 60 [46464/54000 (86%)] Loss: -200091.750000\n",
      "Train Epoch: 60 [47872/54000 (89%)] Loss: -201821.843750\n",
      "Train Epoch: 60 [49280/54000 (91%)] Loss: -194850.718750\n",
      "Train Epoch: 60 [50688/54000 (94%)] Loss: -213473.156250\n",
      "Train Epoch: 60 [52096/54000 (96%)] Loss: -213163.390625\n",
      "    epoch          : 60\n",
      "    loss           : -206244.93193032296\n",
      "    val_loss       : -216137.92798447027\n",
      "Train Epoch: 61 [0/54000 (0%)] Loss: -200071.875000\n",
      "Train Epoch: 61 [1408/54000 (3%)] Loss: -205388.546875\n",
      "Train Epoch: 61 [2816/54000 (5%)] Loss: -196608.578125\n",
      "Train Epoch: 61 [4224/54000 (8%)] Loss: -191522.140625\n",
      "Train Epoch: 61 [5632/54000 (10%)] Loss: -202205.328125\n",
      "Train Epoch: 61 [7040/54000 (13%)] Loss: -205257.750000\n",
      "Train Epoch: 61 [8448/54000 (16%)] Loss: -193334.375000\n",
      "Train Epoch: 61 [9856/54000 (18%)] Loss: -190231.750000\n",
      "Train Epoch: 61 [11264/54000 (21%)] Loss: -208329.343750\n",
      "Train Epoch: 61 [12672/54000 (23%)] Loss: -226547.000000\n",
      "Train Epoch: 61 [14080/54000 (26%)] Loss: -209301.265625\n",
      "Train Epoch: 61 [15488/54000 (29%)] Loss: -207730.906250\n",
      "Train Epoch: 61 [16896/54000 (31%)] Loss: -198959.156250\n",
      "Train Epoch: 61 [18304/54000 (34%)] Loss: -202259.656250\n",
      "Train Epoch: 61 [19712/54000 (37%)] Loss: -192652.062500\n",
      "Train Epoch: 61 [21120/54000 (39%)] Loss: -192021.312500\n",
      "Train Epoch: 61 [22528/54000 (42%)] Loss: -216735.828125\n",
      "Train Epoch: 61 [23936/54000 (44%)] Loss: -209815.046875\n",
      "Train Epoch: 61 [25344/54000 (47%)] Loss: -205809.593750\n",
      "Train Epoch: 61 [26752/54000 (50%)] Loss: -197211.843750\n",
      "Train Epoch: 61 [28160/54000 (52%)] Loss: -197755.781250\n",
      "Train Epoch: 61 [29568/54000 (55%)] Loss: -198017.875000\n",
      "Train Epoch: 61 [30976/54000 (57%)] Loss: -218175.125000\n",
      "Train Epoch: 61 [32384/54000 (60%)] Loss: -213060.234375\n",
      "Train Epoch: 61 [33792/54000 (63%)] Loss: -213692.015625\n",
      "Train Epoch: 61 [35200/54000 (65%)] Loss: -199864.406250\n",
      "Train Epoch: 61 [36608/54000 (68%)] Loss: -198835.218750\n",
      "Train Epoch: 61 [38016/54000 (70%)] Loss: -205210.140625\n",
      "Train Epoch: 61 [39424/54000 (73%)] Loss: -207585.687500\n",
      "Train Epoch: 61 [40832/54000 (76%)] Loss: -208691.468750\n",
      "Train Epoch: 61 [42240/54000 (78%)] Loss: -198394.656250\n",
      "Train Epoch: 61 [43648/54000 (81%)] Loss: -211719.109375\n",
      "Train Epoch: 61 [45056/54000 (83%)] Loss: -202727.375000\n",
      "Train Epoch: 61 [46464/54000 (86%)] Loss: -224915.750000\n",
      "Train Epoch: 61 [47872/54000 (89%)] Loss: -205040.875000\n",
      "Train Epoch: 61 [49280/54000 (91%)] Loss: -212379.468750\n",
      "Train Epoch: 61 [50688/54000 (94%)] Loss: -207783.937500\n",
      "Train Epoch: 61 [52096/54000 (96%)] Loss: -207158.250000\n",
      "    epoch          : 61\n",
      "    loss           : -206586.58369467704\n",
      "    val_loss       : -215597.753787157\n",
      "Train Epoch: 62 [0/54000 (0%)] Loss: -225543.593750\n",
      "Train Epoch: 62 [1408/54000 (3%)] Loss: -213869.265625\n",
      "Train Epoch: 62 [2816/54000 (5%)] Loss: -194594.593750\n",
      "Train Epoch: 62 [4224/54000 (8%)] Loss: -207618.500000\n",
      "Train Epoch: 62 [5632/54000 (10%)] Loss: -204586.546875\n",
      "Train Epoch: 62 [7040/54000 (13%)] Loss: -204967.656250\n",
      "Train Epoch: 62 [8448/54000 (16%)] Loss: -229851.390625\n",
      "Train Epoch: 62 [9856/54000 (18%)] Loss: -192577.187500\n",
      "Train Epoch: 62 [11264/54000 (21%)] Loss: -205139.500000\n",
      "Train Epoch: 62 [12672/54000 (23%)] Loss: -198964.500000\n",
      "Train Epoch: 62 [14080/54000 (26%)] Loss: -206020.390625\n",
      "Train Epoch: 62 [15488/54000 (29%)] Loss: -193908.062500\n",
      "Train Epoch: 62 [16896/54000 (31%)] Loss: -199613.234375\n",
      "Train Epoch: 62 [18304/54000 (34%)] Loss: -215658.046875\n",
      "Train Epoch: 62 [19712/54000 (37%)] Loss: -214704.765625\n",
      "Train Epoch: 62 [21120/54000 (39%)] Loss: -229860.484375\n",
      "Train Epoch: 62 [22528/54000 (42%)] Loss: -199682.937500\n",
      "Train Epoch: 62 [23936/54000 (44%)] Loss: -201831.953125\n",
      "Train Epoch: 62 [25344/54000 (47%)] Loss: -204448.156250\n",
      "Train Epoch: 62 [26752/54000 (50%)] Loss: -228839.265625\n",
      "Train Epoch: 62 [28160/54000 (52%)] Loss: -188790.281250\n",
      "Train Epoch: 62 [29568/54000 (55%)] Loss: -198039.015625\n",
      "Train Epoch: 62 [30976/54000 (57%)] Loss: -206221.062500\n",
      "Train Epoch: 62 [32384/54000 (60%)] Loss: -201755.062500\n",
      "Train Epoch: 62 [33792/54000 (63%)] Loss: -229760.828125\n",
      "Train Epoch: 62 [35200/54000 (65%)] Loss: -217983.359375\n",
      "Train Epoch: 62 [36608/54000 (68%)] Loss: -210279.156250\n",
      "Train Epoch: 62 [38016/54000 (70%)] Loss: -189434.968750\n",
      "Train Epoch: 62 [39424/54000 (73%)] Loss: -209980.656250\n",
      "Train Epoch: 62 [40832/54000 (76%)] Loss: -214306.078125\n",
      "Train Epoch: 62 [42240/54000 (78%)] Loss: -188437.000000\n",
      "Train Epoch: 62 [43648/54000 (81%)] Loss: -201463.390625\n",
      "Train Epoch: 62 [45056/54000 (83%)] Loss: -201296.203125\n",
      "Train Epoch: 62 [46464/54000 (86%)] Loss: -200879.656250\n",
      "Train Epoch: 62 [47872/54000 (89%)] Loss: -198641.812500\n",
      "Train Epoch: 62 [49280/54000 (91%)] Loss: -196621.062500\n",
      "Train Epoch: 62 [50688/54000 (94%)] Loss: -232755.359375\n",
      "Train Epoch: 62 [52096/54000 (96%)] Loss: -199729.578125\n",
      "    epoch          : 62\n",
      "    loss           : -206689.89331638755\n",
      "    val_loss       : -216482.0780297256\n",
      "Train Epoch: 63 [0/54000 (0%)] Loss: -198194.656250\n",
      "Train Epoch: 63 [1408/54000 (3%)] Loss: -213773.718750\n",
      "Train Epoch: 63 [2816/54000 (5%)] Loss: -196454.640625\n",
      "Train Epoch: 63 [4224/54000 (8%)] Loss: -199606.250000\n",
      "Train Epoch: 63 [5632/54000 (10%)] Loss: -203431.453125\n",
      "Train Epoch: 63 [7040/54000 (13%)] Loss: -211844.109375\n",
      "Train Epoch: 63 [8448/54000 (16%)] Loss: -213354.906250\n",
      "Train Epoch: 63 [9856/54000 (18%)] Loss: -206171.906250\n",
      "Train Epoch: 63 [11264/54000 (21%)] Loss: -200203.281250\n",
      "Train Epoch: 63 [12672/54000 (23%)] Loss: -199538.812500\n",
      "Train Epoch: 63 [14080/54000 (26%)] Loss: -215643.500000\n",
      "Train Epoch: 63 [15488/54000 (29%)] Loss: -194663.718750\n",
      "Train Epoch: 63 [16896/54000 (31%)] Loss: -192208.703125\n",
      "Train Epoch: 63 [18304/54000 (34%)] Loss: -195419.937500\n",
      "Train Epoch: 63 [19712/54000 (37%)] Loss: -190503.390625\n",
      "Train Epoch: 63 [21120/54000 (39%)] Loss: -197250.921875\n",
      "Train Epoch: 63 [22528/54000 (42%)] Loss: -228296.156250\n",
      "Train Epoch: 63 [23936/54000 (44%)] Loss: -212788.781250\n",
      "Train Epoch: 63 [25344/54000 (47%)] Loss: -207199.796875\n",
      "Train Epoch: 63 [26752/54000 (50%)] Loss: -204928.656250\n",
      "Train Epoch: 63 [28160/54000 (52%)] Loss: -207175.281250\n",
      "Train Epoch: 63 [29568/54000 (55%)] Loss: -214974.812500\n",
      "Train Epoch: 63 [30976/54000 (57%)] Loss: -204253.140625\n",
      "Train Epoch: 63 [32384/54000 (60%)] Loss: -203474.015625\n",
      "Train Epoch: 63 [33792/54000 (63%)] Loss: -221230.703125\n",
      "Train Epoch: 63 [35200/54000 (65%)] Loss: -190837.296875\n",
      "Train Epoch: 63 [36608/54000 (68%)] Loss: -215695.531250\n",
      "Train Epoch: 63 [38016/54000 (70%)] Loss: -201377.984375\n",
      "Train Epoch: 63 [39424/54000 (73%)] Loss: -233197.890625\n",
      "Train Epoch: 63 [40832/54000 (76%)] Loss: -198687.953125\n",
      "Train Epoch: 63 [42240/54000 (78%)] Loss: -207614.312500\n",
      "Train Epoch: 63 [43648/54000 (81%)] Loss: -202240.375000\n",
      "Train Epoch: 63 [45056/54000 (83%)] Loss: -220635.796875\n",
      "Train Epoch: 63 [46464/54000 (86%)] Loss: -199094.187500\n",
      "Train Epoch: 63 [47872/54000 (89%)] Loss: -207513.546875\n",
      "Train Epoch: 63 [49280/54000 (91%)] Loss: -205422.640625\n",
      "Train Epoch: 63 [50688/54000 (94%)] Loss: -197978.312500\n",
      "Train Epoch: 63 [52096/54000 (96%)] Loss: -194646.562500\n",
      "    epoch          : 63\n",
      "    loss           : -207134.8014354067\n",
      "    val_loss       : -217331.20765053353\n",
      "Train Epoch: 64 [0/54000 (0%)] Loss: -232203.234375\n",
      "Train Epoch: 64 [1408/54000 (3%)] Loss: -204531.421875\n",
      "Train Epoch: 64 [2816/54000 (5%)] Loss: -198476.890625\n",
      "Train Epoch: 64 [4224/54000 (8%)] Loss: -198497.531250\n",
      "Train Epoch: 64 [5632/54000 (10%)] Loss: -204318.046875\n",
      "Train Epoch: 64 [7040/54000 (13%)] Loss: -205677.484375\n",
      "Train Epoch: 64 [8448/54000 (16%)] Loss: -209545.875000\n",
      "Train Epoch: 64 [9856/54000 (18%)] Loss: -232196.562500\n",
      "Train Epoch: 64 [11264/54000 (21%)] Loss: -211461.187500\n",
      "Train Epoch: 64 [12672/54000 (23%)] Loss: -205293.000000\n",
      "Train Epoch: 64 [14080/54000 (26%)] Loss: -219126.828125\n",
      "Train Epoch: 64 [15488/54000 (29%)] Loss: -200403.125000\n",
      "Train Epoch: 64 [16896/54000 (31%)] Loss: -232934.437500\n",
      "Train Epoch: 64 [18304/54000 (34%)] Loss: -206720.703125\n",
      "Train Epoch: 64 [19712/54000 (37%)] Loss: -203412.500000\n",
      "Train Epoch: 64 [21120/54000 (39%)] Loss: -206444.656250\n",
      "Train Epoch: 64 [22528/54000 (42%)] Loss: -189869.875000\n",
      "Train Epoch: 64 [23936/54000 (44%)] Loss: -200267.687500\n",
      "Train Epoch: 64 [25344/54000 (47%)] Loss: -200487.625000\n",
      "Train Epoch: 64 [26752/54000 (50%)] Loss: -200167.031250\n",
      "Train Epoch: 64 [28160/54000 (52%)] Loss: -218893.375000\n",
      "Train Epoch: 64 [29568/54000 (55%)] Loss: -220331.500000\n",
      "Train Epoch: 64 [30976/54000 (57%)] Loss: -213768.953125\n",
      "Train Epoch: 64 [32384/54000 (60%)] Loss: -200509.500000\n",
      "Train Epoch: 64 [33792/54000 (63%)] Loss: -200653.343750\n",
      "Train Epoch: 64 [35200/54000 (65%)] Loss: -197700.593750\n",
      "Train Epoch: 64 [36608/54000 (68%)] Loss: -201637.781250\n",
      "Train Epoch: 64 [38016/54000 (70%)] Loss: -189296.203125\n",
      "Train Epoch: 64 [39424/54000 (73%)] Loss: -195321.671875\n",
      "Train Epoch: 64 [40832/54000 (76%)] Loss: -228036.921875\n",
      "Train Epoch: 64 [42240/54000 (78%)] Loss: -196369.500000\n",
      "Train Epoch: 64 [43648/54000 (81%)] Loss: -198316.171875\n",
      "Train Epoch: 64 [45056/54000 (83%)] Loss: -198077.500000\n",
      "Train Epoch: 64 [46464/54000 (86%)] Loss: -227339.187500\n",
      "Train Epoch: 64 [47872/54000 (89%)] Loss: -200413.000000\n",
      "Train Epoch: 64 [49280/54000 (91%)] Loss: -214595.968750\n",
      "Train Epoch: 64 [50688/54000 (94%)] Loss: -197283.593750\n",
      "Train Epoch: 64 [52096/54000 (96%)] Loss: -189216.390625\n",
      "    epoch          : 64\n",
      "    loss           : -207553.09487141148\n",
      "    val_loss       : -216519.20575695502\n",
      "Train Epoch: 65 [0/54000 (0%)] Loss: -227790.250000\n",
      "Train Epoch: 65 [1408/54000 (3%)] Loss: -194563.328125\n",
      "Train Epoch: 65 [2816/54000 (5%)] Loss: -197040.140625\n",
      "Train Epoch: 65 [4224/54000 (8%)] Loss: -210535.484375\n",
      "Train Epoch: 65 [5632/54000 (10%)] Loss: -204317.734375\n",
      "Train Epoch: 65 [7040/54000 (13%)] Loss: -208176.593750\n",
      "Train Epoch: 65 [8448/54000 (16%)] Loss: -203614.015625\n",
      "Train Epoch: 65 [9856/54000 (18%)] Loss: -203693.781250\n",
      "Train Epoch: 65 [11264/54000 (21%)] Loss: -211902.656250\n",
      "Train Epoch: 65 [12672/54000 (23%)] Loss: -217211.234375\n",
      "Train Epoch: 65 [14080/54000 (26%)] Loss: -209192.546875\n",
      "Train Epoch: 65 [15488/54000 (29%)] Loss: -213450.734375\n",
      "Train Epoch: 65 [16896/54000 (31%)] Loss: -215324.828125\n",
      "Train Epoch: 65 [18304/54000 (34%)] Loss: -215211.921875\n",
      "Train Epoch: 65 [19712/54000 (37%)] Loss: -196657.656250\n",
      "Train Epoch: 65 [21120/54000 (39%)] Loss: -191283.203125\n",
      "Train Epoch: 65 [22528/54000 (42%)] Loss: -197900.250000\n",
      "Train Epoch: 65 [23936/54000 (44%)] Loss: -221238.406250\n",
      "Train Epoch: 65 [25344/54000 (47%)] Loss: -227888.468750\n",
      "Train Epoch: 65 [26752/54000 (50%)] Loss: -202151.187500\n",
      "Train Epoch: 65 [28160/54000 (52%)] Loss: -209070.015625\n",
      "Train Epoch: 65 [29568/54000 (55%)] Loss: -203232.218750\n",
      "Train Epoch: 65 [30976/54000 (57%)] Loss: -203534.703125\n",
      "Train Epoch: 65 [32384/54000 (60%)] Loss: -201769.968750\n",
      "Train Epoch: 65 [33792/54000 (63%)] Loss: -202112.328125\n",
      "Train Epoch: 65 [35200/54000 (65%)] Loss: -201320.281250\n",
      "Train Epoch: 65 [36608/54000 (68%)] Loss: -231744.593750\n",
      "Train Epoch: 65 [38016/54000 (70%)] Loss: -217918.687500\n",
      "Train Epoch: 65 [39424/54000 (73%)] Loss: -216526.875000\n",
      "Train Epoch: 65 [40832/54000 (76%)] Loss: -202747.859375\n",
      "Train Epoch: 65 [42240/54000 (78%)] Loss: -200239.234375\n",
      "Train Epoch: 65 [43648/54000 (81%)] Loss: -220896.203125\n",
      "Train Epoch: 65 [45056/54000 (83%)] Loss: -210757.343750\n",
      "Train Epoch: 65 [46464/54000 (86%)] Loss: -199608.937500\n",
      "Train Epoch: 65 [47872/54000 (89%)] Loss: -212153.375000\n",
      "Train Epoch: 65 [49280/54000 (91%)] Loss: -197458.203125\n",
      "Train Epoch: 65 [50688/54000 (94%)] Loss: -230232.656250\n",
      "Train Epoch: 65 [52096/54000 (96%)] Loss: -192946.734375\n",
      "    epoch          : 65\n",
      "    loss           : -207722.599805622\n",
      "    val_loss       : -216913.3496689215\n",
      "Train Epoch: 66 [0/54000 (0%)] Loss: -225903.656250\n",
      "Train Epoch: 66 [1408/54000 (3%)] Loss: -214423.500000\n",
      "Train Epoch: 66 [2816/54000 (5%)] Loss: -208512.140625\n",
      "Train Epoch: 66 [4224/54000 (8%)] Loss: -202372.921875\n",
      "Train Epoch: 66 [5632/54000 (10%)] Loss: -208097.718750\n",
      "Train Epoch: 66 [7040/54000 (13%)] Loss: -197033.515625\n",
      "Train Epoch: 66 [8448/54000 (16%)] Loss: -198188.250000\n",
      "Train Epoch: 66 [9856/54000 (18%)] Loss: -193995.296875\n",
      "Train Epoch: 66 [11264/54000 (21%)] Loss: -207007.953125\n",
      "Train Epoch: 66 [12672/54000 (23%)] Loss: -207744.312500\n",
      "Train Epoch: 66 [14080/54000 (26%)] Loss: -202993.625000\n",
      "Train Epoch: 66 [15488/54000 (29%)] Loss: -202808.375000\n",
      "Train Epoch: 66 [16896/54000 (31%)] Loss: -198049.468750\n",
      "Train Epoch: 66 [18304/54000 (34%)] Loss: -210475.781250\n",
      "Train Epoch: 66 [19712/54000 (37%)] Loss: -195285.937500\n",
      "Train Epoch: 66 [21120/54000 (39%)] Loss: -198796.078125\n",
      "Train Epoch: 66 [22528/54000 (42%)] Loss: -201102.828125\n",
      "Train Epoch: 66 [23936/54000 (44%)] Loss: -203479.093750\n",
      "Train Epoch: 66 [25344/54000 (47%)] Loss: -202682.437500\n",
      "Train Epoch: 66 [26752/54000 (50%)] Loss: -203165.578125\n",
      "Train Epoch: 66 [28160/54000 (52%)] Loss: -203860.968750\n",
      "Train Epoch: 66 [29568/54000 (55%)] Loss: -211593.437500\n",
      "Train Epoch: 66 [30976/54000 (57%)] Loss: -210710.515625\n",
      "Train Epoch: 66 [32384/54000 (60%)] Loss: -216291.390625\n",
      "Train Epoch: 66 [33792/54000 (63%)] Loss: -215611.281250\n",
      "Train Epoch: 66 [35200/54000 (65%)] Loss: -218047.890625\n",
      "Train Epoch: 66 [36608/54000 (68%)] Loss: -200311.062500\n",
      "Train Epoch: 66 [38016/54000 (70%)] Loss: -198141.859375\n",
      "Train Epoch: 66 [39424/54000 (73%)] Loss: -202459.312500\n",
      "Train Epoch: 66 [40832/54000 (76%)] Loss: -206247.390625\n",
      "Train Epoch: 66 [42240/54000 (78%)] Loss: -186860.593750\n",
      "Train Epoch: 66 [43648/54000 (81%)] Loss: -205715.593750\n",
      "Train Epoch: 66 [45056/54000 (83%)] Loss: -229137.031250\n",
      "Train Epoch: 66 [46464/54000 (86%)] Loss: -211276.656250\n",
      "Train Epoch: 66 [47872/54000 (89%)] Loss: -203575.843750\n",
      "Train Epoch: 66 [49280/54000 (91%)] Loss: -198520.359375\n",
      "Train Epoch: 66 [50688/54000 (94%)] Loss: -205588.812500\n",
      "Train Epoch: 66 [52096/54000 (96%)] Loss: -202899.406250\n",
      "    epoch          : 66\n",
      "    loss           : -207749.81743421053\n",
      "    val_loss       : -217491.97032202745\n",
      "Train Epoch: 67 [0/54000 (0%)] Loss: -197885.375000\n",
      "Train Epoch: 67 [1408/54000 (3%)] Loss: -203091.875000\n",
      "Train Epoch: 67 [2816/54000 (5%)] Loss: -190577.171875\n",
      "Train Epoch: 67 [4224/54000 (8%)] Loss: -218166.156250\n",
      "Train Epoch: 67 [5632/54000 (10%)] Loss: -211459.218750\n",
      "Train Epoch: 67 [7040/54000 (13%)] Loss: -201855.625000\n",
      "Train Epoch: 67 [8448/54000 (16%)] Loss: -211206.328125\n",
      "Train Epoch: 67 [9856/54000 (18%)] Loss: -230773.156250\n",
      "Train Epoch: 67 [11264/54000 (21%)] Loss: -209816.843750\n",
      "Train Epoch: 67 [12672/54000 (23%)] Loss: -203057.375000\n",
      "Train Epoch: 67 [14080/54000 (26%)] Loss: -215802.031250\n",
      "Train Epoch: 67 [15488/54000 (29%)] Loss: -198870.234375\n",
      "Train Epoch: 67 [16896/54000 (31%)] Loss: -210942.906250\n",
      "Train Epoch: 67 [18304/54000 (34%)] Loss: -213458.312500\n",
      "Train Epoch: 67 [19712/54000 (37%)] Loss: -194483.296875\n",
      "Train Epoch: 67 [21120/54000 (39%)] Loss: -203750.812500\n",
      "Train Epoch: 67 [22528/54000 (42%)] Loss: -197462.781250\n",
      "Train Epoch: 67 [23936/54000 (44%)] Loss: -211055.671875\n",
      "Train Epoch: 67 [25344/54000 (47%)] Loss: -192827.093750\n",
      "Train Epoch: 67 [26752/54000 (50%)] Loss: -210684.093750\n",
      "Train Epoch: 67 [28160/54000 (52%)] Loss: -202638.875000\n",
      "Train Epoch: 67 [29568/54000 (55%)] Loss: -218516.734375\n",
      "Train Epoch: 67 [30976/54000 (57%)] Loss: -199299.984375\n",
      "Train Epoch: 67 [32384/54000 (60%)] Loss: -201338.906250\n",
      "Train Epoch: 67 [33792/54000 (63%)] Loss: -223802.312500\n",
      "Train Epoch: 67 [35200/54000 (65%)] Loss: -190080.656250\n",
      "Train Epoch: 67 [36608/54000 (68%)] Loss: -194217.796875\n",
      "Train Epoch: 67 [38016/54000 (70%)] Loss: -215504.906250\n",
      "Train Epoch: 67 [39424/54000 (73%)] Loss: -229226.406250\n",
      "Train Epoch: 67 [40832/54000 (76%)] Loss: -194076.203125\n",
      "Train Epoch: 67 [42240/54000 (78%)] Loss: -219890.781250\n",
      "Train Epoch: 67 [43648/54000 (81%)] Loss: -206265.875000\n",
      "Train Epoch: 67 [45056/54000 (83%)] Loss: -229035.687500\n",
      "Train Epoch: 67 [46464/54000 (86%)] Loss: -202533.843750\n",
      "Train Epoch: 67 [47872/54000 (89%)] Loss: -209584.875000\n",
      "Train Epoch: 67 [49280/54000 (91%)] Loss: -203868.312500\n",
      "Train Epoch: 67 [50688/54000 (94%)] Loss: -191551.265625\n",
      "Train Epoch: 67 [52096/54000 (96%)] Loss: -208160.187500\n",
      "    epoch          : 67\n",
      "    loss           : -208063.9420604067\n",
      "    val_loss       : -217249.6769007241\n",
      "Train Epoch: 68 [0/54000 (0%)] Loss: -228475.828125\n",
      "Train Epoch: 68 [1408/54000 (3%)] Loss: -202143.718750\n",
      "Train Epoch: 68 [2816/54000 (5%)] Loss: -206375.921875\n",
      "Train Epoch: 68 [4224/54000 (8%)] Loss: -212066.656250\n",
      "Train Epoch: 68 [5632/54000 (10%)] Loss: -208064.390625\n",
      "Train Epoch: 68 [7040/54000 (13%)] Loss: -203164.343750\n",
      "Train Epoch: 68 [8448/54000 (16%)] Loss: -208324.687500\n",
      "Train Epoch: 68 [9856/54000 (18%)] Loss: -222491.156250\n",
      "Train Epoch: 68 [11264/54000 (21%)] Loss: -202719.453125\n",
      "Train Epoch: 68 [12672/54000 (23%)] Loss: -192597.781250\n",
      "Train Epoch: 68 [14080/54000 (26%)] Loss: -209247.656250\n",
      "Train Epoch: 68 [15488/54000 (29%)] Loss: -200491.140625\n",
      "Train Epoch: 68 [16896/54000 (31%)] Loss: -212176.234375\n",
      "Train Epoch: 68 [18304/54000 (34%)] Loss: -215795.265625\n",
      "Train Epoch: 68 [19712/54000 (37%)] Loss: -196439.921875\n",
      "Train Epoch: 68 [21120/54000 (39%)] Loss: -209941.593750\n",
      "Train Epoch: 68 [22528/54000 (42%)] Loss: -216174.843750\n",
      "Train Epoch: 68 [23936/54000 (44%)] Loss: -220841.312500\n",
      "Train Epoch: 68 [25344/54000 (47%)] Loss: -203949.500000\n",
      "Train Epoch: 68 [26752/54000 (50%)] Loss: -228807.109375\n",
      "Train Epoch: 68 [28160/54000 (52%)] Loss: -210401.781250\n",
      "Train Epoch: 68 [29568/54000 (55%)] Loss: -203864.031250\n",
      "Train Epoch: 68 [30976/54000 (57%)] Loss: -199424.281250\n",
      "Train Epoch: 68 [32384/54000 (60%)] Loss: -201099.203125\n",
      "Train Epoch: 68 [33792/54000 (63%)] Loss: -188934.281250\n",
      "Train Epoch: 68 [35200/54000 (65%)] Loss: -210184.140625\n",
      "Train Epoch: 68 [36608/54000 (68%)] Loss: -200607.750000\n",
      "Train Epoch: 68 [38016/54000 (70%)] Loss: -225130.296875\n",
      "Train Epoch: 68 [39424/54000 (73%)] Loss: -205846.406250\n",
      "Train Epoch: 68 [40832/54000 (76%)] Loss: -206991.125000\n",
      "Train Epoch: 68 [42240/54000 (78%)] Loss: -219239.562500\n",
      "Train Epoch: 68 [43648/54000 (81%)] Loss: -193661.859375\n",
      "Train Epoch: 68 [45056/54000 (83%)] Loss: -229261.031250\n",
      "Train Epoch: 68 [46464/54000 (86%)] Loss: -198894.781250\n",
      "Train Epoch: 68 [47872/54000 (89%)] Loss: -188178.531250\n",
      "Train Epoch: 68 [49280/54000 (91%)] Loss: -211347.578125\n",
      "Train Epoch: 68 [50688/54000 (94%)] Loss: -197891.171875\n",
      "Train Epoch: 68 [52096/54000 (96%)] Loss: -198894.859375\n",
      "    epoch          : 68\n",
      "    loss           : -208429.68484599283\n",
      "    val_loss       : -217529.298947218\n",
      "Train Epoch: 69 [0/54000 (0%)] Loss: -223175.265625\n",
      "Train Epoch: 69 [1408/54000 (3%)] Loss: -235301.750000\n",
      "Train Epoch: 69 [2816/54000 (5%)] Loss: -207972.296875\n",
      "Train Epoch: 69 [4224/54000 (8%)] Loss: -216567.171875\n",
      "Train Epoch: 69 [5632/54000 (10%)] Loss: -204725.921875\n",
      "Train Epoch: 69 [7040/54000 (13%)] Loss: -191289.187500\n",
      "Train Epoch: 69 [8448/54000 (16%)] Loss: -205933.390625\n",
      "Train Epoch: 69 [9856/54000 (18%)] Loss: -214377.968750\n",
      "Train Epoch: 69 [11264/54000 (21%)] Loss: -220663.109375\n",
      "Train Epoch: 69 [12672/54000 (23%)] Loss: -189435.468750\n",
      "Train Epoch: 69 [14080/54000 (26%)] Loss: -235209.937500\n",
      "Train Epoch: 69 [15488/54000 (29%)] Loss: -202795.859375\n",
      "Train Epoch: 69 [16896/54000 (31%)] Loss: -209742.515625\n",
      "Train Epoch: 69 [18304/54000 (34%)] Loss: -206476.250000\n",
      "Train Epoch: 69 [19712/54000 (37%)] Loss: -205626.234375\n",
      "Train Epoch: 69 [21120/54000 (39%)] Loss: -203561.515625\n",
      "Train Epoch: 69 [22528/54000 (42%)] Loss: -203354.656250\n",
      "Train Epoch: 69 [23936/54000 (44%)] Loss: -196409.203125\n",
      "Train Epoch: 69 [25344/54000 (47%)] Loss: -234824.312500\n",
      "Train Epoch: 69 [26752/54000 (50%)] Loss: -208898.093750\n",
      "Train Epoch: 69 [28160/54000 (52%)] Loss: -204473.078125\n",
      "Train Epoch: 69 [29568/54000 (55%)] Loss: -204110.781250\n",
      "Train Epoch: 69 [30976/54000 (57%)] Loss: -203496.875000\n",
      "Train Epoch: 69 [32384/54000 (60%)] Loss: -228268.734375\n",
      "Train Epoch: 69 [33792/54000 (63%)] Loss: -220324.078125\n",
      "Train Epoch: 69 [35200/54000 (65%)] Loss: -198363.421875\n",
      "Train Epoch: 69 [36608/54000 (68%)] Loss: -204034.312500\n",
      "Train Epoch: 69 [38016/54000 (70%)] Loss: -211898.312500\n",
      "Train Epoch: 69 [39424/54000 (73%)] Loss: -229272.875000\n",
      "Train Epoch: 69 [40832/54000 (76%)] Loss: -199808.093750\n",
      "Train Epoch: 69 [42240/54000 (78%)] Loss: -211972.781250\n",
      "Train Epoch: 69 [43648/54000 (81%)] Loss: -213814.781250\n",
      "Train Epoch: 69 [45056/54000 (83%)] Loss: -207788.671875\n",
      "Train Epoch: 69 [46464/54000 (86%)] Loss: -199300.875000\n",
      "Train Epoch: 69 [47872/54000 (89%)] Loss: -203729.718750\n",
      "Train Epoch: 69 [49280/54000 (91%)] Loss: -212735.593750\n",
      "Train Epoch: 69 [50688/54000 (94%)] Loss: -203436.343750\n",
      "Train Epoch: 69 [52096/54000 (96%)] Loss: -216834.765625\n",
      "    epoch          : 69\n",
      "    loss           : -208954.0182416268\n",
      "    val_loss       : -217864.5574742759\n",
      "Train Epoch: 70 [0/54000 (0%)] Loss: -202698.171875\n",
      "Train Epoch: 70 [1408/54000 (3%)] Loss: -215070.343750\n",
      "Train Epoch: 70 [2816/54000 (5%)] Loss: -222689.812500\n",
      "Train Epoch: 70 [4224/54000 (8%)] Loss: -201986.468750\n",
      "Train Epoch: 70 [5632/54000 (10%)] Loss: -217827.031250\n",
      "Train Epoch: 70 [7040/54000 (13%)] Loss: -195237.437500\n",
      "Train Epoch: 70 [8448/54000 (16%)] Loss: -232805.500000\n",
      "Train Epoch: 70 [9856/54000 (18%)] Loss: -203271.500000\n",
      "Train Epoch: 70 [11264/54000 (21%)] Loss: -199081.281250\n",
      "Train Epoch: 70 [12672/54000 (23%)] Loss: -210233.718750\n",
      "Train Epoch: 70 [14080/54000 (26%)] Loss: -211840.156250\n",
      "Train Epoch: 70 [15488/54000 (29%)] Loss: -205900.796875\n",
      "Train Epoch: 70 [16896/54000 (31%)] Loss: -216926.390625\n",
      "Train Epoch: 70 [18304/54000 (34%)] Loss: -198004.578125\n",
      "Train Epoch: 70 [19712/54000 (37%)] Loss: -202680.453125\n",
      "Train Epoch: 70 [21120/54000 (39%)] Loss: -207184.484375\n",
      "Train Epoch: 70 [22528/54000 (42%)] Loss: -210245.031250\n",
      "Train Epoch: 70 [23936/54000 (44%)] Loss: -216751.031250\n",
      "Train Epoch: 70 [25344/54000 (47%)] Loss: -202307.671875\n",
      "Train Epoch: 70 [26752/54000 (50%)] Loss: -205358.171875\n",
      "Train Epoch: 70 [28160/54000 (52%)] Loss: -206478.140625\n",
      "Train Epoch: 70 [29568/54000 (55%)] Loss: -203345.234375\n",
      "Train Epoch: 70 [30976/54000 (57%)] Loss: -192630.406250\n",
      "Train Epoch: 70 [32384/54000 (60%)] Loss: -210553.687500\n",
      "Train Epoch: 70 [33792/54000 (63%)] Loss: -212951.750000\n",
      "Train Epoch: 70 [35200/54000 (65%)] Loss: -216761.625000\n",
      "Train Epoch: 70 [36608/54000 (68%)] Loss: -198647.812500\n",
      "Train Epoch: 70 [38016/54000 (70%)] Loss: -201070.812500\n",
      "Train Epoch: 70 [39424/54000 (73%)] Loss: -204487.656250\n",
      "Train Epoch: 70 [40832/54000 (76%)] Loss: -201580.218750\n",
      "Train Epoch: 70 [42240/54000 (78%)] Loss: -213012.250000\n",
      "Train Epoch: 70 [43648/54000 (81%)] Loss: -204076.281250\n",
      "Train Epoch: 70 [45056/54000 (83%)] Loss: -208817.937500\n",
      "Train Epoch: 70 [46464/54000 (86%)] Loss: -219373.187500\n",
      "Train Epoch: 70 [47872/54000 (89%)] Loss: -210719.687500\n",
      "Train Epoch: 70 [49280/54000 (91%)] Loss: -202142.218750\n",
      "Train Epoch: 70 [50688/54000 (94%)] Loss: -202868.406250\n",
      "Train Epoch: 70 [52096/54000 (96%)] Loss: -190531.500000\n",
      "    epoch          : 70\n",
      "    loss           : -208845.62552332535\n",
      "    val_loss       : -219086.64313786203\n",
      "Train Epoch: 71 [0/54000 (0%)] Loss: -230790.156250\n",
      "Train Epoch: 71 [1408/54000 (3%)] Loss: -205311.843750\n",
      "Train Epoch: 71 [2816/54000 (5%)] Loss: -211977.046875\n",
      "Train Epoch: 71 [4224/54000 (8%)] Loss: -220123.812500\n",
      "Train Epoch: 71 [5632/54000 (10%)] Loss: -220795.328125\n",
      "Train Epoch: 71 [7040/54000 (13%)] Loss: -235266.437500\n",
      "Train Epoch: 71 [8448/54000 (16%)] Loss: -234565.562500\n",
      "Train Epoch: 71 [9856/54000 (18%)] Loss: -187549.718750\n",
      "Train Epoch: 71 [11264/54000 (21%)] Loss: -195298.984375\n",
      "Train Epoch: 71 [12672/54000 (23%)] Loss: -212178.937500\n",
      "Train Epoch: 71 [14080/54000 (26%)] Loss: -216110.562500\n",
      "Train Epoch: 71 [15488/54000 (29%)] Loss: -210511.703125\n",
      "Train Epoch: 71 [16896/54000 (31%)] Loss: -206939.906250\n",
      "Train Epoch: 71 [18304/54000 (34%)] Loss: -208875.015625\n",
      "Train Epoch: 71 [19712/54000 (37%)] Loss: -230371.015625\n",
      "Train Epoch: 71 [21120/54000 (39%)] Loss: -234579.453125\n",
      "Train Epoch: 71 [22528/54000 (42%)] Loss: -216079.359375\n",
      "Train Epoch: 71 [23936/54000 (44%)] Loss: -221912.296875\n",
      "Train Epoch: 71 [25344/54000 (47%)] Loss: -207444.437500\n",
      "Train Epoch: 71 [26752/54000 (50%)] Loss: -219655.843750\n",
      "Train Epoch: 71 [28160/54000 (52%)] Loss: -220652.296875\n",
      "Train Epoch: 71 [29568/54000 (55%)] Loss: -204521.843750\n",
      "Train Epoch: 71 [30976/54000 (57%)] Loss: -199883.000000\n",
      "Train Epoch: 71 [32384/54000 (60%)] Loss: -204698.437500\n",
      "Train Epoch: 71 [33792/54000 (63%)] Loss: -209178.937500\n",
      "Train Epoch: 71 [35200/54000 (65%)] Loss: -218064.546875\n",
      "Train Epoch: 71 [36608/54000 (68%)] Loss: -199688.468750\n",
      "Train Epoch: 71 [38016/54000 (70%)] Loss: -209840.531250\n",
      "Train Epoch: 71 [39424/54000 (73%)] Loss: -235534.718750\n",
      "Train Epoch: 71 [40832/54000 (76%)] Loss: -202781.562500\n",
      "Train Epoch: 71 [42240/54000 (78%)] Loss: -211345.390625\n",
      "Train Epoch: 71 [43648/54000 (81%)] Loss: -195316.859375\n",
      "Train Epoch: 71 [45056/54000 (83%)] Loss: -191694.125000\n",
      "Train Epoch: 71 [46464/54000 (86%)] Loss: -204829.968750\n",
      "Train Epoch: 71 [47872/54000 (89%)] Loss: -204718.406250\n",
      "Train Epoch: 71 [49280/54000 (91%)] Loss: -209666.546875\n",
      "Train Epoch: 71 [50688/54000 (94%)] Loss: -203814.234375\n",
      "Train Epoch: 71 [52096/54000 (96%)] Loss: -215591.500000\n",
      "    epoch          : 71\n",
      "    loss           : -209669.2382999402\n",
      "    val_loss       : -218408.35284870426\n",
      "Train Epoch: 72 [0/54000 (0%)] Loss: -207026.640625\n",
      "Train Epoch: 72 [1408/54000 (3%)] Loss: -203149.468750\n",
      "Train Epoch: 72 [2816/54000 (5%)] Loss: -231519.718750\n",
      "Train Epoch: 72 [4224/54000 (8%)] Loss: -211114.625000\n",
      "Train Epoch: 72 [5632/54000 (10%)] Loss: -200508.718750\n",
      "Train Epoch: 72 [7040/54000 (13%)] Loss: -203643.343750\n",
      "Train Epoch: 72 [8448/54000 (16%)] Loss: -193351.796875\n",
      "Train Epoch: 72 [9856/54000 (18%)] Loss: -235724.546875\n",
      "Train Epoch: 72 [11264/54000 (21%)] Loss: -215768.875000\n",
      "Train Epoch: 72 [12672/54000 (23%)] Loss: -208714.859375\n",
      "Train Epoch: 72 [14080/54000 (26%)] Loss: -206853.062500\n",
      "Train Epoch: 72 [15488/54000 (29%)] Loss: -201824.250000\n",
      "Train Epoch: 72 [16896/54000 (31%)] Loss: -215463.328125\n",
      "Train Epoch: 72 [18304/54000 (34%)] Loss: -209847.406250\n",
      "Train Epoch: 72 [19712/54000 (37%)] Loss: -215715.984375\n",
      "Train Epoch: 72 [21120/54000 (39%)] Loss: -196793.562500\n",
      "Train Epoch: 72 [22528/54000 (42%)] Loss: -201136.015625\n",
      "Train Epoch: 72 [23936/54000 (44%)] Loss: -202797.625000\n",
      "Train Epoch: 72 [25344/54000 (47%)] Loss: -195816.093750\n",
      "Train Epoch: 72 [26752/54000 (50%)] Loss: -233505.437500\n",
      "Train Epoch: 72 [28160/54000 (52%)] Loss: -212857.562500\n",
      "Train Epoch: 72 [29568/54000 (55%)] Loss: -210919.468750\n",
      "Train Epoch: 72 [30976/54000 (57%)] Loss: -205726.859375\n",
      "Train Epoch: 72 [32384/54000 (60%)] Loss: -205160.562500\n",
      "Train Epoch: 72 [33792/54000 (63%)] Loss: -231616.390625\n",
      "Train Epoch: 72 [35200/54000 (65%)] Loss: -197007.359375\n",
      "Train Epoch: 72 [36608/54000 (68%)] Loss: -210055.953125\n",
      "Train Epoch: 72 [38016/54000 (70%)] Loss: -202489.093750\n",
      "Train Epoch: 72 [39424/54000 (73%)] Loss: -192280.906250\n",
      "Train Epoch: 72 [40832/54000 (76%)] Loss: -215998.187500\n",
      "Train Epoch: 72 [42240/54000 (78%)] Loss: -213707.375000\n",
      "Train Epoch: 72 [43648/54000 (81%)] Loss: -216535.468750\n",
      "Train Epoch: 72 [45056/54000 (83%)] Loss: -213854.937500\n",
      "Train Epoch: 72 [46464/54000 (86%)] Loss: -200180.156250\n",
      "Train Epoch: 72 [47872/54000 (89%)] Loss: -206791.015625\n",
      "Train Epoch: 72 [49280/54000 (91%)] Loss: -210510.343750\n",
      "Train Epoch: 72 [50688/54000 (94%)] Loss: -208850.031250\n",
      "Train Epoch: 72 [52096/54000 (96%)] Loss: -199057.812500\n",
      "    epoch          : 72\n",
      "    loss           : -209248.73904754786\n",
      "    val_loss       : -218651.43289110137\n",
      "Train Epoch: 73 [0/54000 (0%)] Loss: -216783.562500\n",
      "Train Epoch: 73 [1408/54000 (3%)] Loss: -201507.000000\n",
      "Train Epoch: 73 [2816/54000 (5%)] Loss: -198705.031250\n",
      "Train Epoch: 73 [4224/54000 (8%)] Loss: -220592.578125\n",
      "Train Epoch: 73 [5632/54000 (10%)] Loss: -210903.218750\n",
      "Train Epoch: 73 [7040/54000 (13%)] Loss: -205265.000000\n",
      "Train Epoch: 73 [8448/54000 (16%)] Loss: -209139.187500\n",
      "Train Epoch: 73 [9856/54000 (18%)] Loss: -196267.093750\n",
      "Train Epoch: 73 [11264/54000 (21%)] Loss: -205645.656250\n",
      "Train Epoch: 73 [12672/54000 (23%)] Loss: -203174.531250\n",
      "Train Epoch: 73 [14080/54000 (26%)] Loss: -204875.156250\n",
      "Train Epoch: 73 [15488/54000 (29%)] Loss: -202157.250000\n",
      "Train Epoch: 73 [16896/54000 (31%)] Loss: -215085.531250\n",
      "Train Epoch: 73 [18304/54000 (34%)] Loss: -211863.359375\n",
      "Train Epoch: 73 [19712/54000 (37%)] Loss: -232114.328125\n",
      "Train Epoch: 73 [21120/54000 (39%)] Loss: -220425.843750\n",
      "Train Epoch: 73 [22528/54000 (42%)] Loss: -202185.156250\n",
      "Train Epoch: 73 [23936/54000 (44%)] Loss: -198523.203125\n",
      "Train Epoch: 73 [25344/54000 (47%)] Loss: -203190.406250\n",
      "Train Epoch: 73 [26752/54000 (50%)] Loss: -203284.937500\n",
      "Train Epoch: 73 [28160/54000 (52%)] Loss: -205000.625000\n",
      "Train Epoch: 73 [29568/54000 (55%)] Loss: -206483.812500\n",
      "Train Epoch: 73 [30976/54000 (57%)] Loss: -209794.750000\n",
      "Train Epoch: 73 [32384/54000 (60%)] Loss: -208634.000000\n",
      "Train Epoch: 73 [33792/54000 (63%)] Loss: -232387.968750\n",
      "Train Epoch: 73 [35200/54000 (65%)] Loss: -207474.234375\n",
      "Train Epoch: 73 [36608/54000 (68%)] Loss: -195300.156250\n",
      "Train Epoch: 73 [38016/54000 (70%)] Loss: -207925.156250\n",
      "Train Epoch: 73 [39424/54000 (73%)] Loss: -204747.218750\n",
      "Train Epoch: 73 [40832/54000 (76%)] Loss: -205596.562500\n",
      "Train Epoch: 73 [42240/54000 (78%)] Loss: -196112.984375\n",
      "Train Epoch: 73 [43648/54000 (81%)] Loss: -234658.312500\n",
      "Train Epoch: 73 [45056/54000 (83%)] Loss: -210401.875000\n",
      "Train Epoch: 73 [46464/54000 (86%)] Loss: -203057.343750\n",
      "Train Epoch: 73 [47872/54000 (89%)] Loss: -201828.625000\n",
      "Train Epoch: 73 [49280/54000 (91%)] Loss: -206776.687500\n",
      "Train Epoch: 73 [50688/54000 (94%)] Loss: -207769.203125\n",
      "Train Epoch: 73 [52096/54000 (96%)] Loss: -211535.500000\n",
      "    epoch          : 73\n",
      "    loss           : -209476.81365879186\n",
      "    val_loss       : -218842.62028391767\n",
      "Train Epoch: 74 [0/54000 (0%)] Loss: -203494.937500\n",
      "Train Epoch: 74 [1408/54000 (3%)] Loss: -210422.609375\n",
      "Train Epoch: 74 [2816/54000 (5%)] Loss: -203507.343750\n",
      "Train Epoch: 74 [4224/54000 (8%)] Loss: -208950.390625\n",
      "Train Epoch: 74 [5632/54000 (10%)] Loss: -202431.937500\n",
      "Train Epoch: 74 [7040/54000 (13%)] Loss: -199236.015625\n",
      "Train Epoch: 74 [8448/54000 (16%)] Loss: -220875.281250\n",
      "Train Epoch: 74 [9856/54000 (18%)] Loss: -219802.171875\n",
      "Train Epoch: 74 [11264/54000 (21%)] Loss: -204061.968750\n",
      "Train Epoch: 74 [12672/54000 (23%)] Loss: -214229.593750\n",
      "Train Epoch: 74 [14080/54000 (26%)] Loss: -216400.046875\n",
      "Train Epoch: 74 [15488/54000 (29%)] Loss: -204939.343750\n",
      "Train Epoch: 74 [16896/54000 (31%)] Loss: -197009.500000\n",
      "Train Epoch: 74 [18304/54000 (34%)] Loss: -201344.343750\n",
      "Train Epoch: 74 [19712/54000 (37%)] Loss: -235202.968750\n",
      "Train Epoch: 74 [21120/54000 (39%)] Loss: -203460.093750\n",
      "Train Epoch: 74 [22528/54000 (42%)] Loss: -217099.265625\n",
      "Train Epoch: 74 [23936/54000 (44%)] Loss: -206340.343750\n",
      "Train Epoch: 74 [25344/54000 (47%)] Loss: -209703.343750\n",
      "Train Epoch: 74 [26752/54000 (50%)] Loss: -208219.093750\n",
      "Train Epoch: 74 [28160/54000 (52%)] Loss: -212426.625000\n",
      "Train Epoch: 74 [29568/54000 (55%)] Loss: -203944.421875\n",
      "Train Epoch: 74 [30976/54000 (57%)] Loss: -220139.140625\n",
      "Train Epoch: 74 [32384/54000 (60%)] Loss: -198782.093750\n",
      "Train Epoch: 74 [33792/54000 (63%)] Loss: -213877.093750\n",
      "Train Epoch: 74 [35200/54000 (65%)] Loss: -197344.468750\n",
      "Train Epoch: 74 [36608/54000 (68%)] Loss: -234072.156250\n",
      "Train Epoch: 74 [38016/54000 (70%)] Loss: -201863.437500\n",
      "Train Epoch: 74 [39424/54000 (73%)] Loss: -209381.437500\n",
      "Train Epoch: 74 [40832/54000 (76%)] Loss: -199323.203125\n",
      "Train Epoch: 74 [42240/54000 (78%)] Loss: -234997.093750\n",
      "Train Epoch: 74 [43648/54000 (81%)] Loss: -234957.593750\n",
      "Train Epoch: 74 [45056/54000 (83%)] Loss: -215764.593750\n",
      "Train Epoch: 74 [46464/54000 (86%)] Loss: -206768.468750\n",
      "Train Epoch: 74 [47872/54000 (89%)] Loss: -204239.046875\n",
      "Train Epoch: 74 [49280/54000 (91%)] Loss: -198744.125000\n",
      "Train Epoch: 74 [50688/54000 (94%)] Loss: -205570.406250\n",
      "Train Epoch: 74 [52096/54000 (96%)] Loss: -203371.437500\n",
      "    epoch          : 74\n",
      "    loss           : -210109.81268690192\n",
      "    val_loss       : -218940.13748094512\n",
      "Train Epoch: 75 [0/54000 (0%)] Loss: -190994.796875\n",
      "Train Epoch: 75 [1408/54000 (3%)] Loss: -224152.687500\n",
      "Train Epoch: 75 [2816/54000 (5%)] Loss: -211912.109375\n",
      "Train Epoch: 75 [4224/54000 (8%)] Loss: -209183.906250\n",
      "Train Epoch: 75 [5632/54000 (10%)] Loss: -192259.359375\n",
      "Train Epoch: 75 [7040/54000 (13%)] Loss: -205325.390625\n",
      "Train Epoch: 75 [8448/54000 (16%)] Loss: -201798.281250\n",
      "Train Epoch: 75 [9856/54000 (18%)] Loss: -200742.343750\n",
      "Train Epoch: 75 [11264/54000 (21%)] Loss: -203208.359375\n",
      "Train Epoch: 75 [12672/54000 (23%)] Loss: -210134.218750\n",
      "Train Epoch: 75 [14080/54000 (26%)] Loss: -215989.218750\n",
      "Train Epoch: 75 [15488/54000 (29%)] Loss: -213470.015625\n",
      "Train Epoch: 75 [16896/54000 (31%)] Loss: -228594.734375\n",
      "Train Epoch: 75 [18304/54000 (34%)] Loss: -199852.203125\n",
      "Train Epoch: 75 [19712/54000 (37%)] Loss: -206470.296875\n",
      "Train Epoch: 75 [21120/54000 (39%)] Loss: -193903.578125\n",
      "Train Epoch: 75 [22528/54000 (42%)] Loss: -210269.968750\n",
      "Train Epoch: 75 [23936/54000 (44%)] Loss: -211511.656250\n",
      "Train Epoch: 75 [25344/54000 (47%)] Loss: -209166.750000\n",
      "Train Epoch: 75 [26752/54000 (50%)] Loss: -204380.250000\n",
      "Train Epoch: 75 [28160/54000 (52%)] Loss: -235920.906250\n",
      "Train Epoch: 75 [29568/54000 (55%)] Loss: -205001.312500\n",
      "Train Epoch: 75 [30976/54000 (57%)] Loss: -220221.734375\n",
      "Train Epoch: 75 [32384/54000 (60%)] Loss: -207149.406250\n",
      "Train Epoch: 75 [33792/54000 (63%)] Loss: -231709.734375\n",
      "Train Epoch: 75 [35200/54000 (65%)] Loss: -206588.187500\n",
      "Train Epoch: 75 [36608/54000 (68%)] Loss: -207397.062500\n",
      "Train Epoch: 75 [38016/54000 (70%)] Loss: -206723.812500\n",
      "Train Epoch: 75 [39424/54000 (73%)] Loss: -212641.156250\n",
      "Train Epoch: 75 [40832/54000 (76%)] Loss: -200387.875000\n",
      "Train Epoch: 75 [42240/54000 (78%)] Loss: -214276.750000\n",
      "Train Epoch: 75 [43648/54000 (81%)] Loss: -217962.171875\n",
      "Train Epoch: 75 [45056/54000 (83%)] Loss: -218623.000000\n",
      "Train Epoch: 75 [46464/54000 (86%)] Loss: -204896.203125\n",
      "Train Epoch: 75 [47872/54000 (89%)] Loss: -203286.281250\n",
      "Train Epoch: 75 [49280/54000 (91%)] Loss: -206063.859375\n",
      "Train Epoch: 75 [50688/54000 (94%)] Loss: -231661.390625\n",
      "Train Epoch: 75 [52096/54000 (96%)] Loss: -200569.062500\n",
      "    epoch          : 75\n",
      "    loss           : -210359.58922697368\n",
      "    val_loss       : -219881.82540967988\n",
      "Train Epoch: 76 [0/54000 (0%)] Loss: -215955.546875\n",
      "Train Epoch: 76 [1408/54000 (3%)] Loss: -220874.937500\n",
      "Train Epoch: 76 [2816/54000 (5%)] Loss: -215643.812500\n",
      "Train Epoch: 76 [4224/54000 (8%)] Loss: -205463.453125\n",
      "Train Epoch: 76 [5632/54000 (10%)] Loss: -216969.843750\n",
      "Train Epoch: 76 [7040/54000 (13%)] Loss: -205485.078125\n",
      "Train Epoch: 76 [8448/54000 (16%)] Loss: -192126.875000\n",
      "Train Epoch: 76 [9856/54000 (18%)] Loss: -234402.093750\n",
      "Train Epoch: 76 [11264/54000 (21%)] Loss: -206580.796875\n",
      "Train Epoch: 76 [12672/54000 (23%)] Loss: -194318.062500\n",
      "Train Epoch: 76 [14080/54000 (26%)] Loss: -235014.843750\n",
      "Train Epoch: 76 [15488/54000 (29%)] Loss: -205788.375000\n",
      "Train Epoch: 76 [16896/54000 (31%)] Loss: -210576.234375\n",
      "Train Epoch: 76 [18304/54000 (34%)] Loss: -206546.250000\n",
      "Train Epoch: 76 [19712/54000 (37%)] Loss: -213105.562500\n",
      "Train Epoch: 76 [21120/54000 (39%)] Loss: -209994.343750\n",
      "Train Epoch: 76 [22528/54000 (42%)] Loss: -221187.750000\n",
      "Train Epoch: 76 [23936/54000 (44%)] Loss: -218341.937500\n",
      "Train Epoch: 76 [25344/54000 (47%)] Loss: -210405.843750\n",
      "Train Epoch: 76 [26752/54000 (50%)] Loss: -201854.500000\n",
      "Train Epoch: 76 [28160/54000 (52%)] Loss: -204679.031250\n",
      "Train Epoch: 76 [29568/54000 (55%)] Loss: -201747.718750\n",
      "Train Epoch: 76 [30976/54000 (57%)] Loss: -199467.343750\n",
      "Train Epoch: 76 [32384/54000 (60%)] Loss: -213238.750000\n",
      "Train Epoch: 76 [33792/54000 (63%)] Loss: -192498.406250\n",
      "Train Epoch: 76 [35200/54000 (65%)] Loss: -210440.328125\n",
      "Train Epoch: 76 [36608/54000 (68%)] Loss: -232471.390625\n",
      "Train Epoch: 76 [38016/54000 (70%)] Loss: -198079.437500\n",
      "Train Epoch: 76 [39424/54000 (73%)] Loss: -195335.171875\n",
      "Train Epoch: 76 [40832/54000 (76%)] Loss: -217845.250000\n",
      "Train Epoch: 76 [42240/54000 (78%)] Loss: -214811.453125\n",
      "Train Epoch: 76 [43648/54000 (81%)] Loss: -211774.484375\n",
      "Train Epoch: 76 [45056/54000 (83%)] Loss: -211390.125000\n",
      "Train Epoch: 76 [46464/54000 (86%)] Loss: -217081.250000\n",
      "Train Epoch: 76 [47872/54000 (89%)] Loss: -213903.453125\n",
      "Train Epoch: 76 [49280/54000 (91%)] Loss: -206970.796875\n",
      "Train Epoch: 76 [50688/54000 (94%)] Loss: -232476.828125\n",
      "Train Epoch: 76 [52096/54000 (96%)] Loss: -215706.125000\n",
      "    epoch          : 76\n",
      "    loss           : -210464.53009120814\n",
      "    val_loss       : -220302.0497213224\n",
      "Train Epoch: 77 [0/54000 (0%)] Loss: -233608.984375\n",
      "Train Epoch: 77 [1408/54000 (3%)] Loss: -216881.437500\n",
      "Train Epoch: 77 [2816/54000 (5%)] Loss: -196685.921875\n",
      "Train Epoch: 77 [4224/54000 (8%)] Loss: -204584.812500\n",
      "Train Epoch: 77 [5632/54000 (10%)] Loss: -211352.812500\n",
      "Train Epoch: 77 [7040/54000 (13%)] Loss: -222647.140625\n",
      "Train Epoch: 77 [8448/54000 (16%)] Loss: -235144.546875\n",
      "Train Epoch: 77 [9856/54000 (18%)] Loss: -222380.078125\n",
      "Train Epoch: 77 [11264/54000 (21%)] Loss: -203467.812500\n",
      "Train Epoch: 77 [12672/54000 (23%)] Loss: -216870.515625\n",
      "Train Epoch: 77 [14080/54000 (26%)] Loss: -202987.125000\n",
      "Train Epoch: 77 [15488/54000 (29%)] Loss: -234424.750000\n",
      "Train Epoch: 77 [16896/54000 (31%)] Loss: -208950.078125\n",
      "Train Epoch: 77 [18304/54000 (34%)] Loss: -210842.531250\n",
      "Train Epoch: 77 [19712/54000 (37%)] Loss: -217153.500000\n",
      "Train Epoch: 77 [21120/54000 (39%)] Loss: -199458.843750\n",
      "Train Epoch: 77 [22528/54000 (42%)] Loss: -206923.250000\n",
      "Train Epoch: 77 [23936/54000 (44%)] Loss: -195070.718750\n",
      "Train Epoch: 77 [25344/54000 (47%)] Loss: -214399.343750\n",
      "Train Epoch: 77 [26752/54000 (50%)] Loss: -206070.171875\n",
      "Train Epoch: 77 [28160/54000 (52%)] Loss: -201767.750000\n",
      "Train Epoch: 77 [29568/54000 (55%)] Loss: -208275.515625\n",
      "Train Epoch: 77 [30976/54000 (57%)] Loss: -208205.109375\n",
      "Train Epoch: 77 [32384/54000 (60%)] Loss: -221966.078125\n",
      "Train Epoch: 77 [33792/54000 (63%)] Loss: -208633.921875\n",
      "Train Epoch: 77 [35200/54000 (65%)] Loss: -210320.718750\n",
      "Train Epoch: 77 [36608/54000 (68%)] Loss: -211157.468750\n",
      "Train Epoch: 77 [38016/54000 (70%)] Loss: -207006.640625\n",
      "Train Epoch: 77 [39424/54000 (73%)] Loss: -205011.031250\n",
      "Train Epoch: 77 [40832/54000 (76%)] Loss: -213112.218750\n",
      "Train Epoch: 77 [42240/54000 (78%)] Loss: -217745.781250\n",
      "Train Epoch: 77 [43648/54000 (81%)] Loss: -210105.562500\n",
      "Train Epoch: 77 [45056/54000 (83%)] Loss: -200557.750000\n",
      "Train Epoch: 77 [46464/54000 (86%)] Loss: -204407.218750\n",
      "Train Epoch: 77 [47872/54000 (89%)] Loss: -210165.343750\n",
      "Train Epoch: 77 [49280/54000 (91%)] Loss: -206184.375000\n",
      "Train Epoch: 77 [50688/54000 (94%)] Loss: -213407.593750\n",
      "Train Epoch: 77 [52096/54000 (96%)] Loss: -218681.359375\n",
      "    epoch          : 77\n",
      "    loss           : -210671.10346889953\n",
      "    val_loss       : -219680.73680449696\n",
      "Train Epoch: 78 [0/54000 (0%)] Loss: -231636.640625\n",
      "Train Epoch: 78 [1408/54000 (3%)] Loss: -203572.671875\n",
      "Train Epoch: 78 [2816/54000 (5%)] Loss: -205297.718750\n",
      "Train Epoch: 78 [4224/54000 (8%)] Loss: -205968.250000\n",
      "Train Epoch: 78 [5632/54000 (10%)] Loss: -207158.453125\n",
      "Train Epoch: 78 [7040/54000 (13%)] Loss: -206514.906250\n",
      "Train Epoch: 78 [8448/54000 (16%)] Loss: -205322.859375\n",
      "Train Epoch: 78 [9856/54000 (18%)] Loss: -204609.625000\n",
      "Train Epoch: 78 [11264/54000 (21%)] Loss: -220312.468750\n",
      "Train Epoch: 78 [12672/54000 (23%)] Loss: -209921.859375\n",
      "Train Epoch: 78 [14080/54000 (26%)] Loss: -217326.203125\n",
      "Train Epoch: 78 [15488/54000 (29%)] Loss: -226322.781250\n",
      "Train Epoch: 78 [16896/54000 (31%)] Loss: -201071.781250\n",
      "Train Epoch: 78 [18304/54000 (34%)] Loss: -210560.921875\n",
      "Train Epoch: 78 [19712/54000 (37%)] Loss: -232939.578125\n",
      "Train Epoch: 78 [21120/54000 (39%)] Loss: -218833.500000\n",
      "Train Epoch: 78 [22528/54000 (42%)] Loss: -220216.406250\n",
      "Train Epoch: 78 [23936/54000 (44%)] Loss: -209069.281250\n",
      "Train Epoch: 78 [25344/54000 (47%)] Loss: -205990.843750\n",
      "Train Epoch: 78 [26752/54000 (50%)] Loss: -205753.937500\n",
      "Train Epoch: 78 [28160/54000 (52%)] Loss: -198738.515625\n",
      "Train Epoch: 78 [29568/54000 (55%)] Loss: -219610.750000\n",
      "Train Epoch: 78 [30976/54000 (57%)] Loss: -209204.750000\n",
      "Train Epoch: 78 [32384/54000 (60%)] Loss: -207216.171875\n",
      "Train Epoch: 78 [33792/54000 (63%)] Loss: -205550.078125\n",
      "Train Epoch: 78 [35200/54000 (65%)] Loss: -208746.468750\n",
      "Train Epoch: 78 [36608/54000 (68%)] Loss: -201986.281250\n",
      "Train Epoch: 78 [38016/54000 (70%)] Loss: -213841.656250\n",
      "Train Epoch: 78 [39424/54000 (73%)] Loss: -216163.687500\n",
      "Train Epoch: 78 [40832/54000 (76%)] Loss: -233560.015625\n",
      "Train Epoch: 78 [42240/54000 (78%)] Loss: -212654.031250\n",
      "Train Epoch: 78 [43648/54000 (81%)] Loss: -211846.437500\n",
      "Train Epoch: 78 [45056/54000 (83%)] Loss: -207470.968750\n",
      "Train Epoch: 78 [46464/54000 (86%)] Loss: -207464.875000\n",
      "Train Epoch: 78 [47872/54000 (89%)] Loss: -210716.906250\n",
      "Train Epoch: 78 [49280/54000 (91%)] Loss: -209309.968750\n",
      "Train Epoch: 78 [50688/54000 (94%)] Loss: -232747.687500\n",
      "Train Epoch: 78 [52096/54000 (96%)] Loss: -207617.437500\n",
      "    epoch          : 78\n",
      "    loss           : -210802.72693630384\n",
      "    val_loss       : -220181.38625428735\n",
      "Train Epoch: 79 [0/54000 (0%)] Loss: -193649.718750\n",
      "Train Epoch: 79 [1408/54000 (3%)] Loss: -195431.328125\n",
      "Train Epoch: 79 [2816/54000 (5%)] Loss: -221557.359375\n",
      "Train Epoch: 79 [4224/54000 (8%)] Loss: -199606.828125\n",
      "Train Epoch: 79 [5632/54000 (10%)] Loss: -209427.390625\n",
      "Train Epoch: 79 [7040/54000 (13%)] Loss: -210004.875000\n",
      "Train Epoch: 79 [8448/54000 (16%)] Loss: -211000.437500\n",
      "Train Epoch: 79 [9856/54000 (18%)] Loss: -210859.906250\n",
      "Train Epoch: 79 [11264/54000 (21%)] Loss: -205268.937500\n",
      "Train Epoch: 79 [12672/54000 (23%)] Loss: -203809.500000\n",
      "Train Epoch: 79 [14080/54000 (26%)] Loss: -209052.046875\n",
      "Train Epoch: 79 [15488/54000 (29%)] Loss: -210944.765625\n",
      "Train Epoch: 79 [16896/54000 (31%)] Loss: -208464.625000\n",
      "Train Epoch: 79 [18304/54000 (34%)] Loss: -210110.625000\n",
      "Train Epoch: 79 [19712/54000 (37%)] Loss: -226382.140625\n",
      "Train Epoch: 79 [21120/54000 (39%)] Loss: -217820.375000\n",
      "Train Epoch: 79 [22528/54000 (42%)] Loss: -209840.125000\n",
      "Train Epoch: 79 [23936/54000 (44%)] Loss: -205873.843750\n",
      "Train Epoch: 79 [25344/54000 (47%)] Loss: -196671.140625\n",
      "Train Epoch: 79 [26752/54000 (50%)] Loss: -206256.968750\n",
      "Train Epoch: 79 [28160/54000 (52%)] Loss: -208006.359375\n",
      "Train Epoch: 79 [29568/54000 (55%)] Loss: -213799.562500\n",
      "Train Epoch: 79 [30976/54000 (57%)] Loss: -208489.031250\n",
      "Train Epoch: 79 [32384/54000 (60%)] Loss: -210554.156250\n",
      "Train Epoch: 79 [33792/54000 (63%)] Loss: -213946.765625\n",
      "Train Epoch: 79 [35200/54000 (65%)] Loss: -210850.859375\n",
      "Train Epoch: 79 [36608/54000 (68%)] Loss: -209524.984375\n",
      "Train Epoch: 79 [38016/54000 (70%)] Loss: -213996.625000\n",
      "Train Epoch: 79 [39424/54000 (73%)] Loss: -214730.062500\n",
      "Train Epoch: 79 [40832/54000 (76%)] Loss: -199344.000000\n",
      "Train Epoch: 79 [42240/54000 (78%)] Loss: -217253.187500\n",
      "Train Epoch: 79 [43648/54000 (81%)] Loss: -205662.859375\n",
      "Train Epoch: 79 [45056/54000 (83%)] Loss: -204619.687500\n",
      "Train Epoch: 79 [46464/54000 (86%)] Loss: -204902.171875\n",
      "Train Epoch: 79 [47872/54000 (89%)] Loss: -205447.031250\n",
      "Train Epoch: 79 [49280/54000 (91%)] Loss: -206235.406250\n",
      "Train Epoch: 79 [50688/54000 (94%)] Loss: -218065.468750\n",
      "Train Epoch: 79 [52096/54000 (96%)] Loss: -212047.234375\n",
      "    epoch          : 79\n",
      "    loss           : -210874.35713217704\n",
      "    val_loss       : -219852.31615615473\n",
      "Train Epoch: 80 [0/54000 (0%)] Loss: -235002.406250\n",
      "Train Epoch: 80 [1408/54000 (3%)] Loss: -209913.062500\n",
      "Train Epoch: 80 [2816/54000 (5%)] Loss: -218539.406250\n",
      "Train Epoch: 80 [4224/54000 (8%)] Loss: -215185.281250\n",
      "Train Epoch: 80 [5632/54000 (10%)] Loss: -200006.750000\n",
      "Train Epoch: 80 [7040/54000 (13%)] Loss: -200816.125000\n",
      "Train Epoch: 80 [8448/54000 (16%)] Loss: -216023.875000\n",
      "Train Epoch: 80 [9856/54000 (18%)] Loss: -204977.531250\n",
      "Train Epoch: 80 [11264/54000 (21%)] Loss: -209423.000000\n",
      "Train Epoch: 80 [12672/54000 (23%)] Loss: -198128.359375\n",
      "Train Epoch: 80 [14080/54000 (26%)] Loss: -206627.937500\n",
      "Train Epoch: 80 [15488/54000 (29%)] Loss: -233528.234375\n",
      "Train Epoch: 80 [16896/54000 (31%)] Loss: -206030.843750\n",
      "Train Epoch: 80 [18304/54000 (34%)] Loss: -226593.328125\n",
      "Train Epoch: 80 [19712/54000 (37%)] Loss: -199704.468750\n",
      "Train Epoch: 80 [21120/54000 (39%)] Loss: -205565.843750\n",
      "Train Epoch: 80 [22528/54000 (42%)] Loss: -213862.031250\n",
      "Train Epoch: 80 [23936/54000 (44%)] Loss: -213741.609375\n",
      "Train Epoch: 80 [25344/54000 (47%)] Loss: -208083.671875\n",
      "Train Epoch: 80 [26752/54000 (50%)] Loss: -202009.781250\n",
      "Train Epoch: 80 [28160/54000 (52%)] Loss: -204874.062500\n",
      "Train Epoch: 80 [29568/54000 (55%)] Loss: -209896.562500\n",
      "Train Epoch: 80 [30976/54000 (57%)] Loss: -206842.937500\n",
      "Train Epoch: 80 [32384/54000 (60%)] Loss: -228805.609375\n",
      "Train Epoch: 80 [33792/54000 (63%)] Loss: -204578.625000\n",
      "Train Epoch: 80 [35200/54000 (65%)] Loss: -196877.875000\n",
      "Train Epoch: 80 [36608/54000 (68%)] Loss: -233690.781250\n",
      "Train Epoch: 80 [38016/54000 (70%)] Loss: -200662.500000\n",
      "Train Epoch: 80 [39424/54000 (73%)] Loss: -203785.437500\n",
      "Train Epoch: 80 [40832/54000 (76%)] Loss: -208837.796875\n",
      "Train Epoch: 80 [42240/54000 (78%)] Loss: -206534.296875\n",
      "Train Epoch: 80 [43648/54000 (81%)] Loss: -206146.828125\n",
      "Train Epoch: 80 [45056/54000 (83%)] Loss: -207876.250000\n",
      "Train Epoch: 80 [46464/54000 (86%)] Loss: -211283.593750\n",
      "Train Epoch: 80 [47872/54000 (89%)] Loss: -205498.875000\n",
      "Train Epoch: 80 [49280/54000 (91%)] Loss: -207145.968750\n",
      "Train Epoch: 80 [50688/54000 (94%)] Loss: -232345.453125\n",
      "Train Epoch: 80 [52096/54000 (96%)] Loss: -200773.390625\n",
      "    epoch          : 80\n",
      "    loss           : -211270.06612589714\n",
      "    val_loss       : -220521.69703934834\n",
      "Train Epoch: 81 [0/54000 (0%)] Loss: -209645.609375\n",
      "Train Epoch: 81 [1408/54000 (3%)] Loss: -210132.531250\n",
      "Train Epoch: 81 [2816/54000 (5%)] Loss: -211115.390625\n",
      "Train Epoch: 81 [4224/54000 (8%)] Loss: -205425.437500\n",
      "Train Epoch: 81 [5632/54000 (10%)] Loss: -233774.390625\n",
      "Train Epoch: 81 [7040/54000 (13%)] Loss: -219111.281250\n",
      "Train Epoch: 81 [8448/54000 (16%)] Loss: -222487.390625\n",
      "Train Epoch: 81 [9856/54000 (18%)] Loss: -211830.171875\n",
      "Train Epoch: 81 [11264/54000 (21%)] Loss: -197100.062500\n",
      "Train Epoch: 81 [12672/54000 (23%)] Loss: -199812.718750\n",
      "Train Epoch: 81 [14080/54000 (26%)] Loss: -208660.125000\n",
      "Train Epoch: 81 [15488/54000 (29%)] Loss: -200120.515625\n",
      "Train Epoch: 81 [16896/54000 (31%)] Loss: -205922.812500\n",
      "Train Epoch: 81 [18304/54000 (34%)] Loss: -202998.968750\n",
      "Train Epoch: 81 [19712/54000 (37%)] Loss: -203547.296875\n",
      "Train Epoch: 81 [21120/54000 (39%)] Loss: -218901.078125\n",
      "Train Epoch: 81 [22528/54000 (42%)] Loss: -205325.484375\n",
      "Train Epoch: 81 [23936/54000 (44%)] Loss: -222679.671875\n",
      "Train Epoch: 81 [25344/54000 (47%)] Loss: -199881.328125\n",
      "Train Epoch: 81 [26752/54000 (50%)] Loss: -229431.031250\n",
      "Train Epoch: 81 [28160/54000 (52%)] Loss: -196515.250000\n",
      "Train Epoch: 81 [29568/54000 (55%)] Loss: -214884.015625\n",
      "Train Epoch: 81 [30976/54000 (57%)] Loss: -211207.343750\n",
      "Train Epoch: 81 [32384/54000 (60%)] Loss: -206816.765625\n",
      "Train Epoch: 81 [33792/54000 (63%)] Loss: -233485.015625\n",
      "Train Epoch: 81 [35200/54000 (65%)] Loss: -208541.937500\n",
      "Train Epoch: 81 [36608/54000 (68%)] Loss: -211880.781250\n",
      "Train Epoch: 81 [38016/54000 (70%)] Loss: -233024.375000\n",
      "Train Epoch: 81 [39424/54000 (73%)] Loss: -208668.000000\n",
      "Train Epoch: 81 [40832/54000 (76%)] Loss: -209092.796875\n",
      "Train Epoch: 81 [42240/54000 (78%)] Loss: -198548.718750\n",
      "Train Epoch: 81 [43648/54000 (81%)] Loss: -216027.828125\n",
      "Train Epoch: 81 [45056/54000 (83%)] Loss: -202405.375000\n",
      "Train Epoch: 81 [46464/54000 (86%)] Loss: -194127.187500\n",
      "Train Epoch: 81 [47872/54000 (89%)] Loss: -208198.140625\n",
      "Train Epoch: 81 [49280/54000 (91%)] Loss: -207248.843750\n",
      "Train Epoch: 81 [50688/54000 (94%)] Loss: -232808.968750\n",
      "Train Epoch: 81 [52096/54000 (96%)] Loss: -212488.562500\n",
      "    epoch          : 81\n",
      "    loss           : -211591.3695798445\n",
      "    val_loss       : -220793.96565358233\n",
      "Train Epoch: 82 [0/54000 (0%)] Loss: -237567.343750\n",
      "Train Epoch: 82 [1408/54000 (3%)] Loss: -212419.500000\n",
      "Train Epoch: 82 [2816/54000 (5%)] Loss: -206680.750000\n",
      "Train Epoch: 82 [4224/54000 (8%)] Loss: -218891.937500\n",
      "Train Epoch: 82 [5632/54000 (10%)] Loss: -199184.703125\n",
      "Train Epoch: 82 [7040/54000 (13%)] Loss: -228796.765625\n",
      "Train Epoch: 82 [8448/54000 (16%)] Loss: -204728.406250\n",
      "Train Epoch: 82 [9856/54000 (18%)] Loss: -204547.437500\n",
      "Train Epoch: 82 [11264/54000 (21%)] Loss: -197546.437500\n",
      "Train Epoch: 82 [12672/54000 (23%)] Loss: -206410.250000\n",
      "Train Epoch: 82 [14080/54000 (26%)] Loss: -233376.671875\n",
      "Train Epoch: 82 [15488/54000 (29%)] Loss: -199574.734375\n",
      "Train Epoch: 82 [16896/54000 (31%)] Loss: -207856.296875\n",
      "Train Epoch: 82 [18304/54000 (34%)] Loss: -223289.281250\n",
      "Train Epoch: 82 [19712/54000 (37%)] Loss: -195359.656250\n",
      "Train Epoch: 82 [21120/54000 (39%)] Loss: -213354.015625\n",
      "Train Epoch: 82 [22528/54000 (42%)] Loss: -210447.687500\n",
      "Train Epoch: 82 [23936/54000 (44%)] Loss: -210447.218750\n",
      "Train Epoch: 82 [25344/54000 (47%)] Loss: -209370.343750\n",
      "Train Epoch: 82 [26752/54000 (50%)] Loss: -222144.062500\n",
      "Train Epoch: 82 [28160/54000 (52%)] Loss: -205672.671875\n",
      "Train Epoch: 82 [29568/54000 (55%)] Loss: -215077.953125\n",
      "Train Epoch: 82 [30976/54000 (57%)] Loss: -210199.718750\n",
      "Train Epoch: 82 [32384/54000 (60%)] Loss: -209478.437500\n",
      "Train Epoch: 82 [33792/54000 (63%)] Loss: -207355.656250\n",
      "Train Epoch: 82 [35200/54000 (65%)] Loss: -211509.062500\n",
      "Train Epoch: 82 [36608/54000 (68%)] Loss: -209556.328125\n",
      "Train Epoch: 82 [38016/54000 (70%)] Loss: -208612.953125\n",
      "Train Epoch: 82 [39424/54000 (73%)] Loss: -236143.875000\n",
      "Train Epoch: 82 [40832/54000 (76%)] Loss: -216440.562500\n",
      "Train Epoch: 82 [42240/54000 (78%)] Loss: -209536.437500\n",
      "Train Epoch: 82 [43648/54000 (81%)] Loss: -215424.593750\n",
      "Train Epoch: 82 [45056/54000 (83%)] Loss: -207331.156250\n",
      "Train Epoch: 82 [46464/54000 (86%)] Loss: -196843.359375\n",
      "Train Epoch: 82 [47872/54000 (89%)] Loss: -205914.375000\n",
      "Train Epoch: 82 [49280/54000 (91%)] Loss: -206944.843750\n",
      "Train Epoch: 82 [50688/54000 (94%)] Loss: -207790.375000\n",
      "Train Epoch: 82 [52096/54000 (96%)] Loss: -217746.156250\n",
      "    epoch          : 82\n",
      "    loss           : -211837.71056369616\n",
      "    val_loss       : -220432.40948932926\n",
      "Train Epoch: 83 [0/54000 (0%)] Loss: -219192.828125\n",
      "Train Epoch: 83 [1408/54000 (3%)] Loss: -217897.062500\n",
      "Train Epoch: 83 [2816/54000 (5%)] Loss: -217105.000000\n",
      "Train Epoch: 83 [4224/54000 (8%)] Loss: -218164.671875\n",
      "Train Epoch: 83 [5632/54000 (10%)] Loss: -221740.921875\n",
      "Train Epoch: 83 [7040/54000 (13%)] Loss: -207366.125000\n",
      "Train Epoch: 83 [8448/54000 (16%)] Loss: -205341.625000\n",
      "Train Epoch: 83 [9856/54000 (18%)] Loss: -232588.515625\n",
      "Train Epoch: 83 [11264/54000 (21%)] Loss: -198382.296875\n",
      "Train Epoch: 83 [12672/54000 (23%)] Loss: -217691.218750\n",
      "Train Epoch: 83 [14080/54000 (26%)] Loss: -209455.609375\n",
      "Train Epoch: 83 [15488/54000 (29%)] Loss: -233522.359375\n",
      "Train Epoch: 83 [16896/54000 (31%)] Loss: -221376.265625\n",
      "Train Epoch: 83 [18304/54000 (34%)] Loss: -208471.937500\n",
      "Train Epoch: 83 [19712/54000 (37%)] Loss: -208965.218750\n",
      "Train Epoch: 83 [21120/54000 (39%)] Loss: -222348.968750\n",
      "Train Epoch: 83 [22528/54000 (42%)] Loss: -208186.406250\n",
      "Train Epoch: 83 [23936/54000 (44%)] Loss: -198845.953125\n",
      "Train Epoch: 83 [25344/54000 (47%)] Loss: -200014.375000\n",
      "Train Epoch: 83 [26752/54000 (50%)] Loss: -208074.359375\n",
      "Train Epoch: 83 [28160/54000 (52%)] Loss: -205739.750000\n",
      "Train Epoch: 83 [29568/54000 (55%)] Loss: -211878.343750\n",
      "Train Epoch: 83 [30976/54000 (57%)] Loss: -214400.531250\n",
      "Train Epoch: 83 [32384/54000 (60%)] Loss: -231333.828125\n",
      "Train Epoch: 83 [33792/54000 (63%)] Loss: -198896.656250\n",
      "Train Epoch: 83 [35200/54000 (65%)] Loss: -216880.687500\n",
      "Train Epoch: 83 [36608/54000 (68%)] Loss: -206722.875000\n",
      "Train Epoch: 83 [38016/54000 (70%)] Loss: -207917.453125\n",
      "Train Epoch: 83 [39424/54000 (73%)] Loss: -234642.406250\n",
      "Train Epoch: 83 [40832/54000 (76%)] Loss: -219659.828125\n",
      "Train Epoch: 83 [42240/54000 (78%)] Loss: -207946.515625\n",
      "Train Epoch: 83 [43648/54000 (81%)] Loss: -230666.703125\n",
      "Train Epoch: 83 [45056/54000 (83%)] Loss: -200127.562500\n",
      "Train Epoch: 83 [46464/54000 (86%)] Loss: -210147.218750\n",
      "Train Epoch: 83 [47872/54000 (89%)] Loss: -198311.640625\n",
      "Train Epoch: 83 [49280/54000 (91%)] Loss: -210116.343750\n",
      "Train Epoch: 83 [50688/54000 (94%)] Loss: -198656.156250\n",
      "Train Epoch: 83 [52096/54000 (96%)] Loss: -232636.531250\n",
      "    epoch          : 83\n",
      "    loss           : -212134.6851076555\n",
      "    val_loss       : -221392.9590796494\n",
      "Train Epoch: 84 [0/54000 (0%)] Loss: -233920.093750\n",
      "Train Epoch: 84 [1408/54000 (3%)] Loss: -209526.890625\n",
      "Train Epoch: 84 [2816/54000 (5%)] Loss: -217313.843750\n",
      "Train Epoch: 84 [4224/54000 (8%)] Loss: -207241.671875\n",
      "Train Epoch: 84 [5632/54000 (10%)] Loss: -201429.187500\n",
      "Train Epoch: 84 [7040/54000 (13%)] Loss: -199234.843750\n",
      "Train Epoch: 84 [8448/54000 (16%)] Loss: -198158.093750\n",
      "Train Epoch: 84 [9856/54000 (18%)] Loss: -199871.546875\n",
      "Train Epoch: 84 [11264/54000 (21%)] Loss: -216400.453125\n",
      "Train Epoch: 84 [12672/54000 (23%)] Loss: -200255.203125\n",
      "Train Epoch: 84 [14080/54000 (26%)] Loss: -217613.750000\n",
      "Train Epoch: 84 [15488/54000 (29%)] Loss: -217582.687500\n",
      "Train Epoch: 84 [16896/54000 (31%)] Loss: -212218.296875\n",
      "Train Epoch: 84 [18304/54000 (34%)] Loss: -217909.656250\n",
      "Train Epoch: 84 [19712/54000 (37%)] Loss: -232883.484375\n",
      "Train Epoch: 84 [21120/54000 (39%)] Loss: -213725.437500\n",
      "Train Epoch: 84 [22528/54000 (42%)] Loss: -202318.750000\n",
      "Train Epoch: 84 [23936/54000 (44%)] Loss: -201878.703125\n",
      "Train Epoch: 84 [25344/54000 (47%)] Loss: -212801.093750\n",
      "Train Epoch: 84 [26752/54000 (50%)] Loss: -216628.875000\n",
      "Train Epoch: 84 [28160/54000 (52%)] Loss: -232293.062500\n",
      "Train Epoch: 84 [29568/54000 (55%)] Loss: -209039.625000\n",
      "Train Epoch: 84 [30976/54000 (57%)] Loss: -200978.359375\n",
      "Train Epoch: 84 [32384/54000 (60%)] Loss: -207373.421875\n",
      "Train Epoch: 84 [33792/54000 (63%)] Loss: -235068.781250\n",
      "Train Epoch: 84 [35200/54000 (65%)] Loss: -218452.218750\n",
      "Train Epoch: 84 [36608/54000 (68%)] Loss: -221985.671875\n",
      "Train Epoch: 84 [38016/54000 (70%)] Loss: -234051.343750\n",
      "Train Epoch: 84 [39424/54000 (73%)] Loss: -197490.703125\n",
      "Train Epoch: 84 [40832/54000 (76%)] Loss: -188968.906250\n",
      "Train Epoch: 84 [42240/54000 (78%)] Loss: -196052.437500\n",
      "Train Epoch: 84 [43648/54000 (81%)] Loss: -212039.437500\n",
      "Train Epoch: 84 [45056/54000 (83%)] Loss: -206622.296875\n",
      "Train Epoch: 84 [46464/54000 (86%)] Loss: -235907.781250\n",
      "Train Epoch: 84 [47872/54000 (89%)] Loss: -204578.484375\n",
      "Train Epoch: 84 [49280/54000 (91%)] Loss: -208756.718750\n",
      "Train Epoch: 84 [50688/54000 (94%)] Loss: -214294.234375\n",
      "Train Epoch: 84 [52096/54000 (96%)] Loss: -231502.609375\n",
      "    epoch          : 84\n",
      "    loss           : -212440.3176958732\n",
      "    val_loss       : -221696.7801662538\n",
      "Train Epoch: 85 [0/54000 (0%)] Loss: -214827.187500\n",
      "Train Epoch: 85 [1408/54000 (3%)] Loss: -208959.953125\n",
      "Train Epoch: 85 [2816/54000 (5%)] Loss: -204713.531250\n",
      "Train Epoch: 85 [4224/54000 (8%)] Loss: -209439.750000\n",
      "Train Epoch: 85 [5632/54000 (10%)] Loss: -204771.000000\n",
      "Train Epoch: 85 [7040/54000 (13%)] Loss: -207915.312500\n",
      "Train Epoch: 85 [8448/54000 (16%)] Loss: -210982.812500\n",
      "Train Epoch: 85 [9856/54000 (18%)] Loss: -212185.031250\n",
      "Train Epoch: 85 [11264/54000 (21%)] Loss: -198520.250000\n",
      "Train Epoch: 85 [12672/54000 (23%)] Loss: -218112.125000\n",
      "Train Epoch: 85 [14080/54000 (26%)] Loss: -214181.531250\n",
      "Train Epoch: 85 [15488/54000 (29%)] Loss: -211535.906250\n",
      "Train Epoch: 85 [16896/54000 (31%)] Loss: -232702.875000\n",
      "Train Epoch: 85 [18304/54000 (34%)] Loss: -198526.062500\n",
      "Train Epoch: 85 [19712/54000 (37%)] Loss: -201570.187500\n",
      "Train Epoch: 85 [21120/54000 (39%)] Loss: -213085.937500\n",
      "Train Epoch: 85 [22528/54000 (42%)] Loss: -232255.156250\n",
      "Train Epoch: 85 [23936/54000 (44%)] Loss: -209039.593750\n",
      "Train Epoch: 85 [25344/54000 (47%)] Loss: -222067.656250\n",
      "Train Epoch: 85 [26752/54000 (50%)] Loss: -207835.406250\n",
      "Train Epoch: 85 [28160/54000 (52%)] Loss: -211271.218750\n",
      "Train Epoch: 85 [29568/54000 (55%)] Loss: -212463.828125\n",
      "Train Epoch: 85 [30976/54000 (57%)] Loss: -207209.859375\n",
      "Train Epoch: 85 [32384/54000 (60%)] Loss: -219024.921875\n",
      "Train Epoch: 85 [33792/54000 (63%)] Loss: -233546.062500\n",
      "Train Epoch: 85 [35200/54000 (65%)] Loss: -200825.593750\n",
      "Train Epoch: 85 [36608/54000 (68%)] Loss: -207920.281250\n",
      "Train Epoch: 85 [38016/54000 (70%)] Loss: -198215.703125\n",
      "Train Epoch: 85 [39424/54000 (73%)] Loss: -210501.359375\n",
      "Train Epoch: 85 [40832/54000 (76%)] Loss: -237494.062500\n",
      "Train Epoch: 85 [42240/54000 (78%)] Loss: -209816.640625\n",
      "Train Epoch: 85 [43648/54000 (81%)] Loss: -216692.187500\n",
      "Train Epoch: 85 [45056/54000 (83%)] Loss: -206504.500000\n",
      "Train Epoch: 85 [46464/54000 (86%)] Loss: -210697.515625\n",
      "Train Epoch: 85 [47872/54000 (89%)] Loss: -202043.421875\n",
      "Train Epoch: 85 [49280/54000 (91%)] Loss: -207398.625000\n",
      "Train Epoch: 85 [50688/54000 (94%)] Loss: -209334.562500\n",
      "Train Epoch: 85 [52096/54000 (96%)] Loss: -237975.609375\n",
      "    epoch          : 85\n",
      "    loss           : -212350.57883522726\n",
      "    val_loss       : -221551.75550209603\n",
      "Train Epoch: 86 [0/54000 (0%)] Loss: -203681.468750\n",
      "Train Epoch: 86 [1408/54000 (3%)] Loss: -213940.812500\n",
      "Train Epoch: 86 [2816/54000 (5%)] Loss: -200799.437500\n",
      "Train Epoch: 86 [4224/54000 (8%)] Loss: -200495.640625\n",
      "Train Epoch: 86 [5632/54000 (10%)] Loss: -204125.203125\n",
      "Train Epoch: 86 [7040/54000 (13%)] Loss: -237506.140625\n",
      "Train Epoch: 86 [8448/54000 (16%)] Loss: -204882.140625\n",
      "Train Epoch: 86 [9856/54000 (18%)] Loss: -216430.468750\n",
      "Train Epoch: 86 [11264/54000 (21%)] Loss: -219273.062500\n",
      "Train Epoch: 86 [12672/54000 (23%)] Loss: -219758.312500\n",
      "Train Epoch: 86 [14080/54000 (26%)] Loss: -222875.546875\n",
      "Train Epoch: 86 [15488/54000 (29%)] Loss: -207754.578125\n",
      "Train Epoch: 86 [16896/54000 (31%)] Loss: -212879.406250\n",
      "Train Epoch: 86 [18304/54000 (34%)] Loss: -201004.718750\n",
      "Train Epoch: 86 [19712/54000 (37%)] Loss: -233375.453125\n",
      "Train Epoch: 86 [21120/54000 (39%)] Loss: -204676.109375\n",
      "Train Epoch: 86 [22528/54000 (42%)] Loss: -199102.312500\n",
      "Train Epoch: 86 [23936/54000 (44%)] Loss: -222058.265625\n",
      "Train Epoch: 86 [25344/54000 (47%)] Loss: -230209.046875\n",
      "Train Epoch: 86 [26752/54000 (50%)] Loss: -210311.046875\n",
      "Train Epoch: 86 [28160/54000 (52%)] Loss: -209875.296875\n",
      "Train Epoch: 86 [29568/54000 (55%)] Loss: -219031.562500\n",
      "Train Epoch: 86 [30976/54000 (57%)] Loss: -230648.250000\n",
      "Train Epoch: 86 [32384/54000 (60%)] Loss: -197766.187500\n",
      "Train Epoch: 86 [33792/54000 (63%)] Loss: -208247.046875\n",
      "Train Epoch: 86 [35200/54000 (65%)] Loss: -233441.062500\n",
      "Train Epoch: 86 [36608/54000 (68%)] Loss: -213834.375000\n",
      "Train Epoch: 86 [38016/54000 (70%)] Loss: -216496.062500\n",
      "Train Epoch: 86 [39424/54000 (73%)] Loss: -208745.234375\n",
      "Train Epoch: 86 [40832/54000 (76%)] Loss: -218156.578125\n",
      "Train Epoch: 86 [42240/54000 (78%)] Loss: -219702.812500\n",
      "Train Epoch: 86 [43648/54000 (81%)] Loss: -236717.687500\n",
      "Train Epoch: 86 [45056/54000 (83%)] Loss: -199252.031250\n",
      "Train Epoch: 86 [46464/54000 (86%)] Loss: -208538.500000\n",
      "Train Epoch: 86 [47872/54000 (89%)] Loss: -211792.390625\n",
      "Train Epoch: 86 [49280/54000 (91%)] Loss: -211563.843750\n",
      "Train Epoch: 86 [50688/54000 (94%)] Loss: -237609.562500\n",
      "Train Epoch: 86 [52096/54000 (96%)] Loss: -216856.671875\n",
      "    epoch          : 86\n",
      "    loss           : -212788.3309659091\n",
      "    val_loss       : -221663.36964081554\n",
      "Train Epoch: 87 [0/54000 (0%)] Loss: -238267.812500\n",
      "Train Epoch: 87 [1408/54000 (3%)] Loss: -209843.484375\n",
      "Train Epoch: 87 [2816/54000 (5%)] Loss: -209985.484375\n",
      "Train Epoch: 87 [4224/54000 (8%)] Loss: -203215.625000\n",
      "Train Epoch: 87 [5632/54000 (10%)] Loss: -208104.265625\n",
      "Train Epoch: 87 [7040/54000 (13%)] Loss: -219890.718750\n",
      "Train Epoch: 87 [8448/54000 (16%)] Loss: -201850.812500\n",
      "Train Epoch: 87 [9856/54000 (18%)] Loss: -210266.125000\n",
      "Train Epoch: 87 [11264/54000 (21%)] Loss: -204153.625000\n",
      "Train Epoch: 87 [12672/54000 (23%)] Loss: -207153.500000\n",
      "Train Epoch: 87 [14080/54000 (26%)] Loss: -206474.062500\n",
      "Train Epoch: 87 [15488/54000 (29%)] Loss: -210870.828125\n",
      "Train Epoch: 87 [16896/54000 (31%)] Loss: -204959.218750\n",
      "Train Epoch: 87 [18304/54000 (34%)] Loss: -208019.953125\n",
      "Train Epoch: 87 [19712/54000 (37%)] Loss: -237270.656250\n",
      "Train Epoch: 87 [21120/54000 (39%)] Loss: -222802.031250\n",
      "Train Epoch: 87 [22528/54000 (42%)] Loss: -197893.875000\n",
      "Train Epoch: 87 [23936/54000 (44%)] Loss: -220856.312500\n",
      "Train Epoch: 87 [25344/54000 (47%)] Loss: -199373.406250\n",
      "Train Epoch: 87 [26752/54000 (50%)] Loss: -231516.187500\n",
      "Train Epoch: 87 [28160/54000 (52%)] Loss: -208662.031250\n",
      "Train Epoch: 87 [29568/54000 (55%)] Loss: -206383.515625\n",
      "Train Epoch: 87 [30976/54000 (57%)] Loss: -232869.453125\n",
      "Train Epoch: 87 [32384/54000 (60%)] Loss: -216612.328125\n",
      "Train Epoch: 87 [33792/54000 (63%)] Loss: -217691.718750\n",
      "Train Epoch: 87 [35200/54000 (65%)] Loss: -207932.031250\n",
      "Train Epoch: 87 [36608/54000 (68%)] Loss: -206830.890625\n",
      "Train Epoch: 87 [38016/54000 (70%)] Loss: -211508.531250\n",
      "Train Epoch: 87 [39424/54000 (73%)] Loss: -232908.687500\n",
      "Train Epoch: 87 [40832/54000 (76%)] Loss: -205212.609375\n",
      "Train Epoch: 87 [42240/54000 (78%)] Loss: -209566.640625\n",
      "Train Epoch: 87 [43648/54000 (81%)] Loss: -217735.703125\n",
      "Train Epoch: 87 [45056/54000 (83%)] Loss: -231568.093750\n",
      "Train Epoch: 87 [46464/54000 (86%)] Loss: -197663.031250\n",
      "Train Epoch: 87 [47872/54000 (89%)] Loss: -212944.625000\n",
      "Train Epoch: 87 [49280/54000 (91%)] Loss: -212518.578125\n",
      "Train Epoch: 87 [50688/54000 (94%)] Loss: -200010.843750\n",
      "Train Epoch: 87 [52096/54000 (96%)] Loss: -202524.421875\n",
      "    epoch          : 87\n",
      "    loss           : -212734.90490430623\n",
      "    val_loss       : -221806.43779773248\n",
      "Train Epoch: 88 [0/54000 (0%)] Loss: -233192.640625\n",
      "Train Epoch: 88 [1408/54000 (3%)] Loss: -237441.187500\n",
      "Train Epoch: 88 [2816/54000 (5%)] Loss: -210981.250000\n",
      "Train Epoch: 88 [4224/54000 (8%)] Loss: -210226.593750\n",
      "Train Epoch: 88 [5632/54000 (10%)] Loss: -207743.875000\n",
      "Train Epoch: 88 [7040/54000 (13%)] Loss: -206711.578125\n",
      "Train Epoch: 88 [8448/54000 (16%)] Loss: -197971.765625\n",
      "Train Epoch: 88 [9856/54000 (18%)] Loss: -207760.531250\n",
      "Train Epoch: 88 [11264/54000 (21%)] Loss: -201323.031250\n",
      "Train Epoch: 88 [12672/54000 (23%)] Loss: -206884.906250\n",
      "Train Epoch: 88 [14080/54000 (26%)] Loss: -206192.984375\n",
      "Train Epoch: 88 [15488/54000 (29%)] Loss: -217960.593750\n",
      "Train Epoch: 88 [16896/54000 (31%)] Loss: -207562.671875\n",
      "Train Epoch: 88 [18304/54000 (34%)] Loss: -204609.031250\n",
      "Train Epoch: 88 [19712/54000 (37%)] Loss: -220858.921875\n",
      "Train Epoch: 88 [21120/54000 (39%)] Loss: -207521.515625\n",
      "Train Epoch: 88 [22528/54000 (42%)] Loss: -230764.546875\n",
      "Train Epoch: 88 [23936/54000 (44%)] Loss: -209621.250000\n",
      "Train Epoch: 88 [25344/54000 (47%)] Loss: -207729.906250\n",
      "Train Epoch: 88 [26752/54000 (50%)] Loss: -201376.484375\n",
      "Train Epoch: 88 [28160/54000 (52%)] Loss: -210893.265625\n",
      "Train Epoch: 88 [29568/54000 (55%)] Loss: -212556.515625\n",
      "Train Epoch: 88 [30976/54000 (57%)] Loss: -231408.484375\n",
      "Train Epoch: 88 [32384/54000 (60%)] Loss: -222319.359375\n",
      "Train Epoch: 88 [33792/54000 (63%)] Loss: -211522.843750\n",
      "Train Epoch: 88 [35200/54000 (65%)] Loss: -213662.500000\n",
      "Train Epoch: 88 [36608/54000 (68%)] Loss: -233731.437500\n",
      "Train Epoch: 88 [38016/54000 (70%)] Loss: -234523.046875\n",
      "Train Epoch: 88 [39424/54000 (73%)] Loss: -205204.421875\n",
      "Train Epoch: 88 [40832/54000 (76%)] Loss: -199628.437500\n",
      "Train Epoch: 88 [42240/54000 (78%)] Loss: -219184.640625\n",
      "Train Epoch: 88 [43648/54000 (81%)] Loss: -232414.937500\n",
      "Train Epoch: 88 [45056/54000 (83%)] Loss: -213162.421875\n",
      "Train Epoch: 88 [46464/54000 (86%)] Loss: -210106.125000\n",
      "Train Epoch: 88 [47872/54000 (89%)] Loss: -200875.937500\n",
      "Train Epoch: 88 [49280/54000 (91%)] Loss: -208151.250000\n",
      "Train Epoch: 88 [50688/54000 (94%)] Loss: -205203.718750\n",
      "Train Epoch: 88 [52096/54000 (96%)] Loss: -219122.828125\n",
      "    epoch          : 88\n",
      "    loss           : -212827.04526764355\n",
      "    val_loss       : -222124.14013671875\n",
      "Train Epoch: 89 [0/54000 (0%)] Loss: -234064.468750\n",
      "Train Epoch: 89 [1408/54000 (3%)] Loss: -213932.609375\n",
      "Train Epoch: 89 [2816/54000 (5%)] Loss: -212866.562500\n",
      "Train Epoch: 89 [4224/54000 (8%)] Loss: -209770.078125\n",
      "Train Epoch: 89 [5632/54000 (10%)] Loss: -207169.296875\n",
      "Train Epoch: 89 [7040/54000 (13%)] Loss: -209135.828125\n",
      "Train Epoch: 89 [8448/54000 (16%)] Loss: -236548.750000\n",
      "Train Epoch: 89 [9856/54000 (18%)] Loss: -207129.437500\n",
      "Train Epoch: 89 [11264/54000 (21%)] Loss: -223410.562500\n",
      "Train Epoch: 89 [12672/54000 (23%)] Loss: -219450.187500\n",
      "Train Epoch: 89 [14080/54000 (26%)] Loss: -208572.968750\n",
      "Train Epoch: 89 [15488/54000 (29%)] Loss: -209979.093750\n",
      "Train Epoch: 89 [16896/54000 (31%)] Loss: -211026.859375\n",
      "Train Epoch: 89 [18304/54000 (34%)] Loss: -204272.296875\n",
      "Train Epoch: 89 [19712/54000 (37%)] Loss: -211472.500000\n",
      "Train Epoch: 89 [21120/54000 (39%)] Loss: -235846.000000\n",
      "Train Epoch: 89 [22528/54000 (42%)] Loss: -222411.000000\n",
      "Train Epoch: 89 [23936/54000 (44%)] Loss: -223119.500000\n",
      "Train Epoch: 89 [25344/54000 (47%)] Loss: -209648.093750\n",
      "Train Epoch: 89 [26752/54000 (50%)] Loss: -233793.250000\n",
      "Train Epoch: 89 [28160/54000 (52%)] Loss: -203621.031250\n",
      "Train Epoch: 89 [29568/54000 (55%)] Loss: -200316.562500\n",
      "Train Epoch: 89 [30976/54000 (57%)] Loss: -198544.093750\n",
      "Train Epoch: 89 [32384/54000 (60%)] Loss: -208951.546875\n",
      "Train Epoch: 89 [33792/54000 (63%)] Loss: -207013.859375\n",
      "Train Epoch: 89 [35200/54000 (65%)] Loss: -209066.656250\n",
      "Train Epoch: 89 [36608/54000 (68%)] Loss: -210285.000000\n",
      "Train Epoch: 89 [38016/54000 (70%)] Loss: -198724.781250\n",
      "Train Epoch: 89 [39424/54000 (73%)] Loss: -203364.578125\n",
      "Train Epoch: 89 [40832/54000 (76%)] Loss: -222980.453125\n",
      "Train Epoch: 89 [42240/54000 (78%)] Loss: -209368.500000\n",
      "Train Epoch: 89 [43648/54000 (81%)] Loss: -208292.015625\n",
      "Train Epoch: 89 [45056/54000 (83%)] Loss: -233664.437500\n",
      "Train Epoch: 89 [46464/54000 (86%)] Loss: -210461.890625\n",
      "Train Epoch: 89 [47872/54000 (89%)] Loss: -207885.328125\n",
      "Train Epoch: 89 [49280/54000 (91%)] Loss: -205868.453125\n",
      "Train Epoch: 89 [50688/54000 (94%)] Loss: -234501.406250\n",
      "Train Epoch: 89 [52096/54000 (96%)] Loss: -214262.734375\n",
      "    epoch          : 89\n",
      "    loss           : -213052.67150119616\n",
      "    val_loss       : -221921.43900057164\n",
      "Train Epoch: 90 [0/54000 (0%)] Loss: -230325.937500\n",
      "Train Epoch: 90 [1408/54000 (3%)] Loss: -219068.218750\n",
      "Train Epoch: 90 [2816/54000 (5%)] Loss: -208200.750000\n",
      "Train Epoch: 90 [4224/54000 (8%)] Loss: -213556.578125\n",
      "Train Epoch: 90 [5632/54000 (10%)] Loss: -213812.203125\n",
      "Train Epoch: 90 [7040/54000 (13%)] Loss: -213101.562500\n",
      "Train Epoch: 90 [8448/54000 (16%)] Loss: -207857.453125\n",
      "Train Epoch: 90 [9856/54000 (18%)] Loss: -211038.875000\n",
      "Train Epoch: 90 [11264/54000 (21%)] Loss: -201140.843750\n",
      "Train Epoch: 90 [12672/54000 (23%)] Loss: -218600.937500\n",
      "Train Epoch: 90 [14080/54000 (26%)] Loss: -205043.187500\n",
      "Train Epoch: 90 [15488/54000 (29%)] Loss: -220549.671875\n",
      "Train Epoch: 90 [16896/54000 (31%)] Loss: -221719.156250\n",
      "Train Epoch: 90 [18304/54000 (34%)] Loss: -202284.953125\n",
      "Train Epoch: 90 [19712/54000 (37%)] Loss: -200913.812500\n",
      "Train Epoch: 90 [21120/54000 (39%)] Loss: -209450.437500\n",
      "Train Epoch: 90 [22528/54000 (42%)] Loss: -221746.968750\n",
      "Train Epoch: 90 [23936/54000 (44%)] Loss: -208565.187500\n",
      "Train Epoch: 90 [25344/54000 (47%)] Loss: -221223.593750\n",
      "Train Epoch: 90 [26752/54000 (50%)] Loss: -234265.156250\n",
      "Train Epoch: 90 [28160/54000 (52%)] Loss: -207977.953125\n",
      "Train Epoch: 90 [29568/54000 (55%)] Loss: -210627.093750\n",
      "Train Epoch: 90 [30976/54000 (57%)] Loss: -213003.687500\n",
      "Train Epoch: 90 [32384/54000 (60%)] Loss: -236745.500000\n",
      "Train Epoch: 90 [33792/54000 (63%)] Loss: -218756.875000\n",
      "Train Epoch: 90 [35200/54000 (65%)] Loss: -205795.640625\n",
      "Train Epoch: 90 [36608/54000 (68%)] Loss: -221122.031250\n",
      "Train Epoch: 90 [38016/54000 (70%)] Loss: -232389.062500\n",
      "Train Epoch: 90 [39424/54000 (73%)] Loss: -211020.578125\n",
      "Train Epoch: 90 [40832/54000 (76%)] Loss: -208087.843750\n",
      "Train Epoch: 90 [42240/54000 (78%)] Loss: -210569.421875\n",
      "Train Epoch: 90 [43648/54000 (81%)] Loss: -231646.437500\n",
      "Train Epoch: 90 [45056/54000 (83%)] Loss: -208965.890625\n",
      "Train Epoch: 90 [46464/54000 (86%)] Loss: -206005.718750\n",
      "Train Epoch: 90 [47872/54000 (89%)] Loss: -213311.625000\n",
      "Train Epoch: 90 [49280/54000 (91%)] Loss: -206130.156250\n",
      "Train Epoch: 90 [50688/54000 (94%)] Loss: -208558.625000\n",
      "Train Epoch: 90 [52096/54000 (96%)] Loss: -219656.593750\n",
      "    epoch          : 90\n",
      "    loss           : -213482.62750448566\n",
      "    val_loss       : -221910.10119331174\n",
      "Train Epoch: 91 [0/54000 (0%)] Loss: -233574.515625\n",
      "Train Epoch: 91 [1408/54000 (3%)] Loss: -213432.718750\n",
      "Train Epoch: 91 [2816/54000 (5%)] Loss: -214343.390625\n",
      "Train Epoch: 91 [4224/54000 (8%)] Loss: -218062.156250\n",
      "Train Epoch: 91 [5632/54000 (10%)] Loss: -219542.562500\n",
      "Train Epoch: 91 [7040/54000 (13%)] Loss: -215690.000000\n",
      "Train Epoch: 91 [8448/54000 (16%)] Loss: -236579.875000\n",
      "Train Epoch: 91 [9856/54000 (18%)] Loss: -205112.078125\n",
      "Train Epoch: 91 [11264/54000 (21%)] Loss: -219857.187500\n",
      "Train Epoch: 91 [12672/54000 (23%)] Loss: -207670.187500\n",
      "Train Epoch: 91 [14080/54000 (26%)] Loss: -237916.531250\n",
      "Train Epoch: 91 [15488/54000 (29%)] Loss: -217918.640625\n",
      "Train Epoch: 91 [16896/54000 (31%)] Loss: -211320.921875\n",
      "Train Epoch: 91 [18304/54000 (34%)] Loss: -212373.187500\n",
      "Train Epoch: 91 [19712/54000 (37%)] Loss: -233418.500000\n",
      "Train Epoch: 91 [21120/54000 (39%)] Loss: -210462.390625\n",
      "Train Epoch: 91 [22528/54000 (42%)] Loss: -201699.203125\n",
      "Train Epoch: 91 [23936/54000 (44%)] Loss: -203599.281250\n",
      "Train Epoch: 91 [25344/54000 (47%)] Loss: -214825.609375\n",
      "Train Epoch: 91 [26752/54000 (50%)] Loss: -213751.000000\n",
      "Train Epoch: 91 [28160/54000 (52%)] Loss: -206346.656250\n",
      "Train Epoch: 91 [29568/54000 (55%)] Loss: -218543.875000\n",
      "Train Epoch: 91 [30976/54000 (57%)] Loss: -214117.515625\n",
      "Train Epoch: 91 [32384/54000 (60%)] Loss: -209153.000000\n",
      "Train Epoch: 91 [33792/54000 (63%)] Loss: -235246.406250\n",
      "Train Epoch: 91 [35200/54000 (65%)] Loss: -202938.656250\n",
      "Train Epoch: 91 [36608/54000 (68%)] Loss: -200566.250000\n",
      "Train Epoch: 91 [38016/54000 (70%)] Loss: -237702.125000\n",
      "Train Epoch: 91 [39424/54000 (73%)] Loss: -206580.484375\n",
      "Train Epoch: 91 [40832/54000 (76%)] Loss: -224171.687500\n",
      "Train Epoch: 91 [42240/54000 (78%)] Loss: -219892.421875\n",
      "Train Epoch: 91 [43648/54000 (81%)] Loss: -234463.468750\n",
      "Train Epoch: 91 [45056/54000 (83%)] Loss: -208319.250000\n",
      "Train Epoch: 91 [46464/54000 (86%)] Loss: -205648.500000\n",
      "Train Epoch: 91 [47872/54000 (89%)] Loss: -209339.437500\n",
      "Train Epoch: 91 [49280/54000 (91%)] Loss: -218799.359375\n",
      "Train Epoch: 91 [50688/54000 (94%)] Loss: -210662.390625\n",
      "Train Epoch: 91 [52096/54000 (96%)] Loss: -235368.578125\n",
      "    epoch          : 91\n",
      "    loss           : -213743.48773923446\n",
      "    val_loss       : -222300.48595893674\n",
      "Train Epoch: 92 [0/54000 (0%)] Loss: -202562.484375\n",
      "Train Epoch: 92 [1408/54000 (3%)] Loss: -235402.000000\n",
      "Train Epoch: 92 [2816/54000 (5%)] Loss: -202088.125000\n",
      "Train Epoch: 92 [4224/54000 (8%)] Loss: -211362.468750\n",
      "Train Epoch: 92 [5632/54000 (10%)] Loss: -203919.156250\n",
      "Train Epoch: 92 [7040/54000 (13%)] Loss: -236846.062500\n",
      "Train Epoch: 92 [8448/54000 (16%)] Loss: -216799.750000\n",
      "Train Epoch: 92 [9856/54000 (18%)] Loss: -210983.875000\n",
      "Train Epoch: 92 [11264/54000 (21%)] Loss: -210989.656250\n",
      "Train Epoch: 92 [12672/54000 (23%)] Loss: -231766.703125\n",
      "Train Epoch: 92 [14080/54000 (26%)] Loss: -224633.468750\n",
      "Train Epoch: 92 [15488/54000 (29%)] Loss: -219085.656250\n",
      "Train Epoch: 92 [16896/54000 (31%)] Loss: -205537.562500\n",
      "Train Epoch: 92 [18304/54000 (34%)] Loss: -207455.343750\n",
      "Train Epoch: 92 [19712/54000 (37%)] Loss: -234827.687500\n",
      "Train Epoch: 92 [21120/54000 (39%)] Loss: -209068.203125\n",
      "Train Epoch: 92 [22528/54000 (42%)] Loss: -215784.562500\n",
      "Train Epoch: 92 [23936/54000 (44%)] Loss: -210773.015625\n",
      "Train Epoch: 92 [25344/54000 (47%)] Loss: -217834.890625\n",
      "Train Epoch: 92 [26752/54000 (50%)] Loss: -209634.640625\n",
      "Train Epoch: 92 [28160/54000 (52%)] Loss: -208858.328125\n",
      "Train Epoch: 92 [29568/54000 (55%)] Loss: -200589.468750\n",
      "Train Epoch: 92 [30976/54000 (57%)] Loss: -207775.312500\n",
      "Train Epoch: 92 [32384/54000 (60%)] Loss: -220448.593750\n",
      "Train Epoch: 92 [33792/54000 (63%)] Loss: -201476.625000\n",
      "Train Epoch: 92 [35200/54000 (65%)] Loss: -233605.843750\n",
      "Train Epoch: 92 [36608/54000 (68%)] Loss: -201572.468750\n",
      "Train Epoch: 92 [38016/54000 (70%)] Loss: -218789.312500\n",
      "Train Epoch: 92 [39424/54000 (73%)] Loss: -219433.765625\n",
      "Train Epoch: 92 [40832/54000 (76%)] Loss: -218606.203125\n",
      "Train Epoch: 92 [42240/54000 (78%)] Loss: -211161.531250\n",
      "Train Epoch: 92 [43648/54000 (81%)] Loss: -202781.218750\n",
      "Train Epoch: 92 [45056/54000 (83%)] Loss: -213677.375000\n",
      "Train Epoch: 92 [46464/54000 (86%)] Loss: -212427.890625\n",
      "Train Epoch: 92 [47872/54000 (89%)] Loss: -209246.796875\n",
      "Train Epoch: 92 [49280/54000 (91%)] Loss: -213182.375000\n",
      "Train Epoch: 92 [50688/54000 (94%)] Loss: -208165.468750\n",
      "Train Epoch: 92 [52096/54000 (96%)] Loss: -234551.375000\n",
      "    epoch          : 92\n",
      "    loss           : -213771.63901764355\n",
      "    val_loss       : -222811.4435618331\n",
      "Train Epoch: 93 [0/54000 (0%)] Loss: -234497.031250\n",
      "Train Epoch: 93 [1408/54000 (3%)] Loss: -202761.218750\n",
      "Train Epoch: 93 [2816/54000 (5%)] Loss: -213203.531250\n",
      "Train Epoch: 93 [4224/54000 (8%)] Loss: -210855.531250\n",
      "Train Epoch: 93 [5632/54000 (10%)] Loss: -212264.468750\n",
      "Train Epoch: 93 [7040/54000 (13%)] Loss: -214447.562500\n",
      "Train Epoch: 93 [8448/54000 (16%)] Loss: -201994.250000\n",
      "Train Epoch: 93 [9856/54000 (18%)] Loss: -201717.937500\n",
      "Train Epoch: 93 [11264/54000 (21%)] Loss: -219975.062500\n",
      "Train Epoch: 93 [12672/54000 (23%)] Loss: -212818.218750\n",
      "Train Epoch: 93 [14080/54000 (26%)] Loss: -210964.656250\n",
      "Train Epoch: 93 [15488/54000 (29%)] Loss: -201916.015625\n",
      "Train Epoch: 93 [16896/54000 (31%)] Loss: -212968.406250\n",
      "Train Epoch: 93 [18304/54000 (34%)] Loss: -237335.046875\n",
      "Train Epoch: 93 [19712/54000 (37%)] Loss: -208915.812500\n",
      "Train Epoch: 93 [21120/54000 (39%)] Loss: -223331.828125\n",
      "Train Epoch: 93 [22528/54000 (42%)] Loss: -206988.937500\n",
      "Train Epoch: 93 [23936/54000 (44%)] Loss: -202021.312500\n",
      "Train Epoch: 93 [25344/54000 (47%)] Loss: -213894.421875\n",
      "Train Epoch: 93 [26752/54000 (50%)] Loss: -221063.687500\n",
      "Train Epoch: 93 [28160/54000 (52%)] Loss: -208206.625000\n",
      "Train Epoch: 93 [29568/54000 (55%)] Loss: -208766.703125\n",
      "Train Epoch: 93 [30976/54000 (57%)] Loss: -208979.406250\n",
      "Train Epoch: 93 [32384/54000 (60%)] Loss: -201803.546875\n",
      "Train Epoch: 93 [33792/54000 (63%)] Loss: -205677.656250\n",
      "Train Epoch: 93 [35200/54000 (65%)] Loss: -214678.593750\n",
      "Train Epoch: 93 [36608/54000 (68%)] Loss: -207718.687500\n",
      "Train Epoch: 93 [38016/54000 (70%)] Loss: -212553.375000\n",
      "Train Epoch: 93 [39424/54000 (73%)] Loss: -212649.609375\n",
      "Train Epoch: 93 [40832/54000 (76%)] Loss: -223452.437500\n",
      "Train Epoch: 93 [42240/54000 (78%)] Loss: -206368.171875\n",
      "Train Epoch: 93 [43648/54000 (81%)] Loss: -209025.015625\n",
      "Train Epoch: 93 [45056/54000 (83%)] Loss: -236041.109375\n",
      "Train Epoch: 93 [46464/54000 (86%)] Loss: -235701.484375\n",
      "Train Epoch: 93 [47872/54000 (89%)] Loss: -219347.593750\n",
      "Train Epoch: 93 [49280/54000 (91%)] Loss: -200861.718750\n",
      "Train Epoch: 93 [50688/54000 (94%)] Loss: -219983.343750\n",
      "Train Epoch: 93 [52096/54000 (96%)] Loss: -204439.671875\n",
      "    epoch          : 93\n",
      "    loss           : -213834.03936154305\n",
      "    val_loss       : -222845.31843083078\n",
      "Train Epoch: 94 [0/54000 (0%)] Loss: -232655.390625\n",
      "Train Epoch: 94 [1408/54000 (3%)] Loss: -220551.593750\n",
      "Train Epoch: 94 [2816/54000 (5%)] Loss: -214363.687500\n",
      "Train Epoch: 94 [4224/54000 (8%)] Loss: -214281.031250\n",
      "Train Epoch: 94 [5632/54000 (10%)] Loss: -201643.234375\n",
      "Train Epoch: 94 [7040/54000 (13%)] Loss: -210402.968750\n",
      "Train Epoch: 94 [8448/54000 (16%)] Loss: -207943.375000\n",
      "Train Epoch: 94 [9856/54000 (18%)] Loss: -213269.968750\n",
      "Train Epoch: 94 [11264/54000 (21%)] Loss: -210484.250000\n",
      "Train Epoch: 94 [12672/54000 (23%)] Loss: -210321.531250\n",
      "Train Epoch: 94 [14080/54000 (26%)] Loss: -234098.328125\n",
      "Train Epoch: 94 [15488/54000 (29%)] Loss: -217904.000000\n",
      "Train Epoch: 94 [16896/54000 (31%)] Loss: -208882.125000\n",
      "Train Epoch: 94 [18304/54000 (34%)] Loss: -210479.828125\n",
      "Train Epoch: 94 [19712/54000 (37%)] Loss: -203746.312500\n",
      "Train Epoch: 94 [21120/54000 (39%)] Loss: -213124.500000\n",
      "Train Epoch: 94 [22528/54000 (42%)] Loss: -212626.203125\n",
      "Train Epoch: 94 [23936/54000 (44%)] Loss: -209432.796875\n",
      "Train Epoch: 94 [25344/54000 (47%)] Loss: -208341.421875\n",
      "Train Epoch: 94 [26752/54000 (50%)] Loss: -219787.578125\n",
      "Train Epoch: 94 [28160/54000 (52%)] Loss: -211654.718750\n",
      "Train Epoch: 94 [29568/54000 (55%)] Loss: -218293.640625\n",
      "Train Epoch: 94 [30976/54000 (57%)] Loss: -231422.812500\n",
      "Train Epoch: 94 [32384/54000 (60%)] Loss: -200564.421875\n",
      "Train Epoch: 94 [33792/54000 (63%)] Loss: -222113.453125\n",
      "Train Epoch: 94 [35200/54000 (65%)] Loss: -218099.000000\n",
      "Train Epoch: 94 [36608/54000 (68%)] Loss: -235813.562500\n",
      "Train Epoch: 94 [38016/54000 (70%)] Loss: -218156.000000\n",
      "Train Epoch: 94 [39424/54000 (73%)] Loss: -210905.218750\n",
      "Train Epoch: 94 [40832/54000 (76%)] Loss: -211255.875000\n",
      "Train Epoch: 94 [42240/54000 (78%)] Loss: -235129.843750\n",
      "Train Epoch: 94 [43648/54000 (81%)] Loss: -221398.062500\n",
      "Train Epoch: 94 [45056/54000 (83%)] Loss: -201135.421875\n",
      "Train Epoch: 94 [46464/54000 (86%)] Loss: -208567.000000\n",
      "Train Epoch: 94 [47872/54000 (89%)] Loss: -212807.421875\n",
      "Train Epoch: 94 [49280/54000 (91%)] Loss: -212856.265625\n",
      "Train Epoch: 94 [50688/54000 (94%)] Loss: -213177.812500\n",
      "Train Epoch: 94 [52096/54000 (96%)] Loss: -218344.171875\n",
      "    epoch          : 94\n",
      "    loss           : -214023.85933761962\n",
      "    val_loss       : -223002.67106516767\n",
      "Train Epoch: 95 [0/54000 (0%)] Loss: -222144.328125\n",
      "Train Epoch: 95 [1408/54000 (3%)] Loss: -208765.015625\n",
      "Train Epoch: 95 [2816/54000 (5%)] Loss: -231858.656250\n",
      "Train Epoch: 95 [4224/54000 (8%)] Loss: -203197.171875\n",
      "Train Epoch: 95 [5632/54000 (10%)] Loss: -221498.906250\n",
      "Train Epoch: 95 [7040/54000 (13%)] Loss: -198812.781250\n",
      "Train Epoch: 95 [8448/54000 (16%)] Loss: -208945.656250\n",
      "Train Epoch: 95 [9856/54000 (18%)] Loss: -222728.500000\n",
      "Train Epoch: 95 [11264/54000 (21%)] Loss: -208804.468750\n",
      "Train Epoch: 95 [12672/54000 (23%)] Loss: -222359.468750\n",
      "Train Epoch: 95 [14080/54000 (26%)] Loss: -211040.687500\n",
      "Train Epoch: 95 [15488/54000 (29%)] Loss: -209506.390625\n",
      "Train Epoch: 95 [16896/54000 (31%)] Loss: -236888.546875\n",
      "Train Epoch: 95 [18304/54000 (34%)] Loss: -221161.625000\n",
      "Train Epoch: 95 [19712/54000 (37%)] Loss: -202588.531250\n",
      "Train Epoch: 95 [21120/54000 (39%)] Loss: -235662.031250\n",
      "Train Epoch: 95 [22528/54000 (42%)] Loss: -209237.375000\n",
      "Train Epoch: 95 [23936/54000 (44%)] Loss: -207575.859375\n",
      "Train Epoch: 95 [25344/54000 (47%)] Loss: -208645.000000\n",
      "Train Epoch: 95 [26752/54000 (50%)] Loss: -220624.203125\n",
      "Train Epoch: 95 [28160/54000 (52%)] Loss: -211528.968750\n",
      "Train Epoch: 95 [29568/54000 (55%)] Loss: -208903.750000\n",
      "Train Epoch: 95 [30976/54000 (57%)] Loss: -209021.234375\n",
      "Train Epoch: 95 [32384/54000 (60%)] Loss: -234598.468750\n",
      "Train Epoch: 95 [33792/54000 (63%)] Loss: -213886.984375\n",
      "Train Epoch: 95 [35200/54000 (65%)] Loss: -210366.437500\n",
      "Train Epoch: 95 [36608/54000 (68%)] Loss: -222603.906250\n",
      "Train Epoch: 95 [38016/54000 (70%)] Loss: -233685.031250\n",
      "Train Epoch: 95 [39424/54000 (73%)] Loss: -200845.953125\n",
      "Train Epoch: 95 [40832/54000 (76%)] Loss: -202603.562500\n",
      "Train Epoch: 95 [42240/54000 (78%)] Loss: -202107.390625\n",
      "Train Epoch: 95 [43648/54000 (81%)] Loss: -210933.875000\n",
      "Train Epoch: 95 [45056/54000 (83%)] Loss: -212153.687500\n",
      "Train Epoch: 95 [46464/54000 (86%)] Loss: -210673.328125\n",
      "Train Epoch: 95 [47872/54000 (89%)] Loss: -208858.484375\n",
      "Train Epoch: 95 [49280/54000 (91%)] Loss: -213910.125000\n",
      "Train Epoch: 95 [50688/54000 (94%)] Loss: -234415.562500\n",
      "Train Epoch: 95 [52096/54000 (96%)] Loss: -203258.078125\n",
      "    epoch          : 95\n",
      "    loss           : -214274.59517045456\n",
      "    val_loss       : -223196.74635575459\n",
      "Train Epoch: 96 [0/54000 (0%)] Loss: -205052.687500\n",
      "Train Epoch: 96 [1408/54000 (3%)] Loss: -212538.171875\n",
      "Train Epoch: 96 [2816/54000 (5%)] Loss: -219775.312500\n",
      "Train Epoch: 96 [4224/54000 (8%)] Loss: -203992.546875\n",
      "Train Epoch: 96 [5632/54000 (10%)] Loss: -207537.406250\n",
      "Train Epoch: 96 [7040/54000 (13%)] Loss: -201040.640625\n",
      "Train Epoch: 96 [8448/54000 (16%)] Loss: -211560.375000\n",
      "Train Epoch: 96 [9856/54000 (18%)] Loss: -202057.062500\n",
      "Train Epoch: 96 [11264/54000 (21%)] Loss: -221551.359375\n",
      "Train Epoch: 96 [12672/54000 (23%)] Loss: -216878.750000\n",
      "Train Epoch: 96 [14080/54000 (26%)] Loss: -233125.937500\n",
      "Train Epoch: 96 [15488/54000 (29%)] Loss: -219995.984375\n",
      "Train Epoch: 96 [16896/54000 (31%)] Loss: -208364.406250\n",
      "Train Epoch: 96 [18304/54000 (34%)] Loss: -211397.312500\n",
      "Train Epoch: 96 [19712/54000 (37%)] Loss: -203007.015625\n",
      "Train Epoch: 96 [21120/54000 (39%)] Loss: -236910.296875\n",
      "Train Epoch: 96 [22528/54000 (42%)] Loss: -199268.937500\n",
      "Train Epoch: 96 [23936/54000 (44%)] Loss: -201597.031250\n",
      "Train Epoch: 96 [25344/54000 (47%)] Loss: -217415.265625\n",
      "Train Epoch: 96 [26752/54000 (50%)] Loss: -211142.203125\n",
      "Train Epoch: 96 [28160/54000 (52%)] Loss: -210895.921875\n",
      "Train Epoch: 96 [29568/54000 (55%)] Loss: -209282.187500\n",
      "Train Epoch: 96 [30976/54000 (57%)] Loss: -203062.375000\n",
      "Train Epoch: 96 [32384/54000 (60%)] Loss: -235032.609375\n",
      "Train Epoch: 96 [33792/54000 (63%)] Loss: -218402.703125\n",
      "Train Epoch: 96 [35200/54000 (65%)] Loss: -215897.875000\n",
      "Train Epoch: 96 [36608/54000 (68%)] Loss: -212838.406250\n",
      "Train Epoch: 96 [38016/54000 (70%)] Loss: -213056.171875\n",
      "Train Epoch: 96 [39424/54000 (73%)] Loss: -209419.375000\n",
      "Train Epoch: 96 [40832/54000 (76%)] Loss: -207779.031250\n",
      "Train Epoch: 96 [42240/54000 (78%)] Loss: -221336.812500\n",
      "Train Epoch: 96 [43648/54000 (81%)] Loss: -221788.125000\n",
      "Train Epoch: 96 [45056/54000 (83%)] Loss: -222387.046875\n",
      "Train Epoch: 96 [46464/54000 (86%)] Loss: -211231.968750\n",
      "Train Epoch: 96 [47872/54000 (89%)] Loss: -207027.062500\n",
      "Train Epoch: 96 [49280/54000 (91%)] Loss: -212676.218750\n",
      "Train Epoch: 96 [50688/54000 (94%)] Loss: -218865.046875\n",
      "Train Epoch: 96 [52096/54000 (96%)] Loss: -212242.531250\n",
      "    epoch          : 96\n",
      "    loss           : -214369.59061004786\n",
      "    val_loss       : -223203.78795493522\n",
      "Train Epoch: 97 [0/54000 (0%)] Loss: -235038.406250\n",
      "Train Epoch: 97 [1408/54000 (3%)] Loss: -211061.937500\n",
      "Train Epoch: 97 [2816/54000 (5%)] Loss: -203797.328125\n",
      "Train Epoch: 97 [4224/54000 (8%)] Loss: -212842.218750\n",
      "Train Epoch: 97 [5632/54000 (10%)] Loss: -200239.609375\n",
      "Train Epoch: 97 [7040/54000 (13%)] Loss: -210749.671875\n",
      "Train Epoch: 97 [8448/54000 (16%)] Loss: -210681.828125\n",
      "Train Epoch: 97 [9856/54000 (18%)] Loss: -210449.359375\n",
      "Train Epoch: 97 [11264/54000 (21%)] Loss: -214581.625000\n",
      "Train Epoch: 97 [12672/54000 (23%)] Loss: -212214.250000\n",
      "Train Epoch: 97 [14080/54000 (26%)] Loss: -212789.500000\n",
      "Train Epoch: 97 [15488/54000 (29%)] Loss: -213460.359375\n",
      "Train Epoch: 97 [16896/54000 (31%)] Loss: -206730.906250\n",
      "Train Epoch: 97 [18304/54000 (34%)] Loss: -202636.390625\n",
      "Train Epoch: 97 [19712/54000 (37%)] Loss: -203884.531250\n",
      "Train Epoch: 97 [21120/54000 (39%)] Loss: -201554.515625\n",
      "Train Epoch: 97 [22528/54000 (42%)] Loss: -217117.359375\n",
      "Train Epoch: 97 [23936/54000 (44%)] Loss: -222744.843750\n",
      "Train Epoch: 97 [25344/54000 (47%)] Loss: -218390.500000\n",
      "Train Epoch: 97 [26752/54000 (50%)] Loss: -202973.984375\n",
      "Train Epoch: 97 [28160/54000 (52%)] Loss: -207956.500000\n",
      "Train Epoch: 97 [29568/54000 (55%)] Loss: -209685.328125\n",
      "Train Epoch: 97 [30976/54000 (57%)] Loss: -209497.187500\n",
      "Train Epoch: 97 [32384/54000 (60%)] Loss: -210928.015625\n",
      "Train Epoch: 97 [33792/54000 (63%)] Loss: -221869.500000\n",
      "Train Epoch: 97 [35200/54000 (65%)] Loss: -234817.718750\n",
      "Train Epoch: 97 [36608/54000 (68%)] Loss: -207963.906250\n",
      "Train Epoch: 97 [38016/54000 (70%)] Loss: -203123.234375\n",
      "Train Epoch: 97 [39424/54000 (73%)] Loss: -214772.031250\n",
      "Train Epoch: 97 [40832/54000 (76%)] Loss: -217442.359375\n",
      "Train Epoch: 97 [42240/54000 (78%)] Loss: -200145.156250\n",
      "Train Epoch: 97 [43648/54000 (81%)] Loss: -223301.937500\n",
      "Train Epoch: 97 [45056/54000 (83%)] Loss: -238053.562500\n",
      "Train Epoch: 97 [46464/54000 (86%)] Loss: -208846.531250\n",
      "Train Epoch: 97 [47872/54000 (89%)] Loss: -214744.500000\n",
      "Train Epoch: 97 [49280/54000 (91%)] Loss: -212370.562500\n",
      "Train Epoch: 97 [50688/54000 (94%)] Loss: -232035.656250\n",
      "Train Epoch: 97 [52096/54000 (96%)] Loss: -219726.703125\n",
      "    epoch          : 97\n",
      "    loss           : -214633.75082236843\n",
      "    val_loss       : -223371.3299828506\n",
      "Train Epoch: 98 [0/54000 (0%)] Loss: -211148.250000\n",
      "Train Epoch: 98 [1408/54000 (3%)] Loss: -209472.656250\n",
      "Train Epoch: 98 [2816/54000 (5%)] Loss: -204024.046875\n",
      "Train Epoch: 98 [4224/54000 (8%)] Loss: -202126.140625\n",
      "Train Epoch: 98 [5632/54000 (10%)] Loss: -213658.750000\n",
      "Train Epoch: 98 [7040/54000 (13%)] Loss: -219073.437500\n",
      "Train Epoch: 98 [8448/54000 (16%)] Loss: -201730.062500\n",
      "Train Epoch: 98 [9856/54000 (18%)] Loss: -210115.156250\n",
      "Train Epoch: 98 [11264/54000 (21%)] Loss: -198753.531250\n",
      "Train Epoch: 98 [12672/54000 (23%)] Loss: -210067.031250\n",
      "Train Epoch: 98 [14080/54000 (26%)] Loss: -238218.296875\n",
      "Train Epoch: 98 [15488/54000 (29%)] Loss: -212970.046875\n",
      "Train Epoch: 98 [16896/54000 (31%)] Loss: -209706.484375\n",
      "Train Epoch: 98 [18304/54000 (34%)] Loss: -199363.953125\n",
      "Train Epoch: 98 [19712/54000 (37%)] Loss: -202941.921875\n",
      "Train Epoch: 98 [21120/54000 (39%)] Loss: -205557.750000\n",
      "Train Epoch: 98 [22528/54000 (42%)] Loss: -219135.500000\n",
      "Train Epoch: 98 [23936/54000 (44%)] Loss: -217458.421875\n",
      "Train Epoch: 98 [25344/54000 (47%)] Loss: -214185.453125\n",
      "Train Epoch: 98 [26752/54000 (50%)] Loss: -215699.000000\n",
      "Train Epoch: 98 [28160/54000 (52%)] Loss: -210131.203125\n",
      "Train Epoch: 98 [29568/54000 (55%)] Loss: -209030.171875\n",
      "Train Epoch: 98 [30976/54000 (57%)] Loss: -220654.750000\n",
      "Train Epoch: 98 [32384/54000 (60%)] Loss: -233979.750000\n",
      "Train Epoch: 98 [33792/54000 (63%)] Loss: -201422.156250\n",
      "Train Epoch: 98 [35200/54000 (65%)] Loss: -222113.890625\n",
      "Train Epoch: 98 [36608/54000 (68%)] Loss: -199615.718750\n",
      "Train Epoch: 98 [38016/54000 (70%)] Loss: -235505.156250\n",
      "Train Epoch: 98 [39424/54000 (73%)] Loss: -213971.187500\n",
      "Train Epoch: 98 [40832/54000 (76%)] Loss: -216062.093750\n",
      "Train Epoch: 98 [42240/54000 (78%)] Loss: -213976.062500\n",
      "Train Epoch: 98 [43648/54000 (81%)] Loss: -202293.343750\n",
      "Train Epoch: 98 [45056/54000 (83%)] Loss: -210859.531250\n",
      "Train Epoch: 98 [46464/54000 (86%)] Loss: -209020.171875\n",
      "Train Epoch: 98 [47872/54000 (89%)] Loss: -207846.000000\n",
      "Train Epoch: 98 [49280/54000 (91%)] Loss: -210894.687500\n",
      "Train Epoch: 98 [50688/54000 (94%)] Loss: -209665.281250\n",
      "Train Epoch: 98 [52096/54000 (96%)] Loss: -211186.703125\n",
      "    epoch          : 98\n",
      "    loss           : -214655.76764354066\n",
      "    val_loss       : -223516.8055211509\n",
      "Train Epoch: 99 [0/54000 (0%)] Loss: -235726.765625\n",
      "Train Epoch: 99 [1408/54000 (3%)] Loss: -200447.328125\n",
      "Train Epoch: 99 [2816/54000 (5%)] Loss: -213551.390625\n",
      "Train Epoch: 99 [4224/54000 (8%)] Loss: -203813.406250\n",
      "Train Epoch: 99 [5632/54000 (10%)] Loss: -209400.875000\n",
      "Train Epoch: 99 [7040/54000 (13%)] Loss: -210342.187500\n",
      "Train Epoch: 99 [8448/54000 (16%)] Loss: -212353.062500\n",
      "Train Epoch: 99 [9856/54000 (18%)] Loss: -215585.718750\n",
      "Train Epoch: 99 [11264/54000 (21%)] Loss: -206702.937500\n",
      "Train Epoch: 99 [12672/54000 (23%)] Loss: -203135.687500\n",
      "Train Epoch: 99 [14080/54000 (26%)] Loss: -205932.468750\n",
      "Train Epoch: 99 [15488/54000 (29%)] Loss: -208951.515625\n",
      "Train Epoch: 99 [16896/54000 (31%)] Loss: -220940.312500\n",
      "Train Epoch: 99 [18304/54000 (34%)] Loss: -236479.156250\n",
      "Train Epoch: 99 [19712/54000 (37%)] Loss: -218117.312500\n",
      "Train Epoch: 99 [21120/54000 (39%)] Loss: -218945.343750\n",
      "Train Epoch: 99 [22528/54000 (42%)] Loss: -203940.921875\n",
      "Train Epoch: 99 [23936/54000 (44%)] Loss: -202588.718750\n",
      "Train Epoch: 99 [25344/54000 (47%)] Loss: -216632.906250\n",
      "Train Epoch: 99 [26752/54000 (50%)] Loss: -223538.562500\n",
      "Train Epoch: 99 [28160/54000 (52%)] Loss: -213786.031250\n",
      "Train Epoch: 99 [29568/54000 (55%)] Loss: -210638.734375\n",
      "Train Epoch: 99 [30976/54000 (57%)] Loss: -233029.281250\n",
      "Train Epoch: 99 [32384/54000 (60%)] Loss: -233250.875000\n",
      "Train Epoch: 99 [33792/54000 (63%)] Loss: -213535.343750\n",
      "Train Epoch: 99 [35200/54000 (65%)] Loss: -214507.437500\n",
      "Train Epoch: 99 [36608/54000 (68%)] Loss: -238503.546875\n",
      "Train Epoch: 99 [38016/54000 (70%)] Loss: -202680.515625\n",
      "Train Epoch: 99 [39424/54000 (73%)] Loss: -221778.281250\n",
      "Train Epoch: 99 [40832/54000 (76%)] Loss: -201137.640625\n",
      "Train Epoch: 99 [42240/54000 (78%)] Loss: -210147.937500\n",
      "Train Epoch: 99 [43648/54000 (81%)] Loss: -214273.609375\n",
      "Train Epoch: 99 [45056/54000 (83%)] Loss: -210480.281250\n",
      "Train Epoch: 99 [46464/54000 (86%)] Loss: -210428.640625\n",
      "Train Epoch: 99 [47872/54000 (89%)] Loss: -235769.421875\n",
      "Train Epoch: 99 [49280/54000 (91%)] Loss: -208651.843750\n",
      "Train Epoch: 99 [50688/54000 (94%)] Loss: -211367.531250\n",
      "Train Epoch: 99 [52096/54000 (96%)] Loss: -213262.062500\n",
      "    epoch          : 99\n",
      "    loss           : -214851.6035062799\n",
      "    val_loss       : -223717.284358327\n",
      "Train Epoch: 100 [0/54000 (0%)] Loss: -237283.218750\n",
      "Train Epoch: 100 [1408/54000 (3%)] Loss: -213694.687500\n",
      "Train Epoch: 100 [2816/54000 (5%)] Loss: -224840.203125\n",
      "Train Epoch: 100 [4224/54000 (8%)] Loss: -220212.234375\n",
      "Train Epoch: 100 [5632/54000 (10%)] Loss: -218989.250000\n",
      "Train Epoch: 100 [7040/54000 (13%)] Loss: -207296.140625\n",
      "Train Epoch: 100 [8448/54000 (16%)] Loss: -210015.468750\n",
      "Train Epoch: 100 [9856/54000 (18%)] Loss: -214719.203125\n",
      "Train Epoch: 100 [11264/54000 (21%)] Loss: -203131.296875\n",
      "Train Epoch: 100 [12672/54000 (23%)] Loss: -214024.968750\n",
      "Train Epoch: 100 [14080/54000 (26%)] Loss: -213515.343750\n",
      "Train Epoch: 100 [15488/54000 (29%)] Loss: -202723.000000\n",
      "Train Epoch: 100 [16896/54000 (31%)] Loss: -212163.250000\n",
      "Train Epoch: 100 [18304/54000 (34%)] Loss: -209276.218750\n",
      "Train Epoch: 100 [19712/54000 (37%)] Loss: -221116.375000\n",
      "Train Epoch: 100 [21120/54000 (39%)] Loss: -202389.953125\n",
      "Train Epoch: 100 [22528/54000 (42%)] Loss: -221997.156250\n",
      "Train Epoch: 100 [23936/54000 (44%)] Loss: -225310.375000\n",
      "Train Epoch: 100 [25344/54000 (47%)] Loss: -211312.703125\n",
      "Train Epoch: 100 [26752/54000 (50%)] Loss: -211110.937500\n",
      "Train Epoch: 100 [28160/54000 (52%)] Loss: -210976.750000\n",
      "Train Epoch: 100 [29568/54000 (55%)] Loss: -239046.812500\n",
      "Train Epoch: 100 [30976/54000 (57%)] Loss: -211277.859375\n",
      "Train Epoch: 100 [32384/54000 (60%)] Loss: -221127.484375\n",
      "Train Epoch: 100 [33792/54000 (63%)] Loss: -239084.625000\n",
      "Train Epoch: 100 [35200/54000 (65%)] Loss: -213392.953125\n",
      "Train Epoch: 100 [36608/54000 (68%)] Loss: -212433.640625\n",
      "Train Epoch: 100 [38016/54000 (70%)] Loss: -215028.375000\n",
      "Train Epoch: 100 [39424/54000 (73%)] Loss: -234989.046875\n",
      "Train Epoch: 100 [40832/54000 (76%)] Loss: -206711.875000\n",
      "Train Epoch: 100 [42240/54000 (78%)] Loss: -217246.750000\n",
      "Train Epoch: 100 [43648/54000 (81%)] Loss: -234862.562500\n",
      "Train Epoch: 100 [45056/54000 (83%)] Loss: -207825.187500\n",
      "Train Epoch: 100 [46464/54000 (86%)] Loss: -202477.828125\n",
      "Train Epoch: 100 [47872/54000 (89%)] Loss: -204756.937500\n",
      "Train Epoch: 100 [49280/54000 (91%)] Loss: -215136.250000\n",
      "Train Epoch: 100 [50688/54000 (94%)] Loss: -208923.578125\n",
      "Train Epoch: 100 [52096/54000 (96%)] Loss: -209724.578125\n",
      "    epoch          : 100\n",
      "    loss           : -215086.7493645335\n",
      "    val_loss       : -223535.28477515245\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0508_092801/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [0/54000 (0%)] Loss: -209508.843750\n",
      "Train Epoch: 101 [1408/54000 (3%)] Loss: -211173.281250\n",
      "Train Epoch: 101 [2816/54000 (5%)] Loss: -203208.437500\n",
      "Train Epoch: 101 [4224/54000 (8%)] Loss: -209901.593750\n",
      "Train Epoch: 101 [5632/54000 (10%)] Loss: -210776.156250\n",
      "Train Epoch: 101 [7040/54000 (13%)] Loss: -214012.828125\n",
      "Train Epoch: 101 [8448/54000 (16%)] Loss: -215428.843750\n",
      "Train Epoch: 101 [9856/54000 (18%)] Loss: -234012.937500\n",
      "Train Epoch: 101 [11264/54000 (21%)] Loss: -216143.406250\n",
      "Train Epoch: 101 [12672/54000 (23%)] Loss: -214794.453125\n",
      "Train Epoch: 101 [14080/54000 (26%)] Loss: -220075.062500\n",
      "Train Epoch: 101 [15488/54000 (29%)] Loss: -206854.859375\n",
      "Train Epoch: 101 [16896/54000 (31%)] Loss: -207226.000000\n",
      "Train Epoch: 101 [18304/54000 (34%)] Loss: -212212.000000\n",
      "Train Epoch: 101 [19712/54000 (37%)] Loss: -206136.968750\n",
      "Train Epoch: 101 [21120/54000 (39%)] Loss: -204746.421875\n",
      "Train Epoch: 101 [22528/54000 (42%)] Loss: -208735.796875\n",
      "Train Epoch: 101 [23936/54000 (44%)] Loss: -202114.437500\n",
      "Train Epoch: 101 [25344/54000 (47%)] Loss: -200252.640625\n",
      "Train Epoch: 101 [26752/54000 (50%)] Loss: -205961.625000\n",
      "Train Epoch: 101 [28160/54000 (52%)] Loss: -220330.468750\n",
      "Train Epoch: 101 [29568/54000 (55%)] Loss: -221100.375000\n",
      "Train Epoch: 101 [30976/54000 (57%)] Loss: -220776.500000\n",
      "Train Epoch: 101 [32384/54000 (60%)] Loss: -211666.640625\n",
      "Train Epoch: 101 [33792/54000 (63%)] Loss: -214249.406250\n",
      "Train Epoch: 101 [35200/54000 (65%)] Loss: -236886.453125\n",
      "Train Epoch: 101 [36608/54000 (68%)] Loss: -209695.640625\n",
      "Train Epoch: 101 [38016/54000 (70%)] Loss: -216027.343750\n",
      "Train Epoch: 101 [39424/54000 (73%)] Loss: -211600.937500\n",
      "Train Epoch: 101 [40832/54000 (76%)] Loss: -214798.062500\n",
      "Train Epoch: 101 [42240/54000 (78%)] Loss: -234568.500000\n",
      "Train Epoch: 101 [43648/54000 (81%)] Loss: -221595.406250\n",
      "Train Epoch: 101 [45056/54000 (83%)] Loss: -214808.609375\n",
      "Train Epoch: 101 [46464/54000 (86%)] Loss: -234488.968750\n",
      "Train Epoch: 101 [47872/54000 (89%)] Loss: -210529.578125\n",
      "Train Epoch: 101 [49280/54000 (91%)] Loss: -210634.078125\n",
      "Train Epoch: 101 [50688/54000 (94%)] Loss: -203507.859375\n",
      "Train Epoch: 101 [52096/54000 (96%)] Loss: -210960.265625\n",
      "    epoch          : 101\n",
      "    loss           : -214985.7246187201\n",
      "    val_loss       : -223931.6629787538\n",
      "Train Epoch: 102 [0/54000 (0%)] Loss: -210597.750000\n",
      "Train Epoch: 102 [1408/54000 (3%)] Loss: -211021.281250\n",
      "Train Epoch: 102 [2816/54000 (5%)] Loss: -216851.312500\n",
      "Train Epoch: 102 [4224/54000 (8%)] Loss: -216306.484375\n",
      "Train Epoch: 102 [5632/54000 (10%)] Loss: -210716.078125\n",
      "Train Epoch: 102 [7040/54000 (13%)] Loss: -211193.968750\n",
      "Train Epoch: 102 [8448/54000 (16%)] Loss: -204928.421875\n",
      "Train Epoch: 102 [9856/54000 (18%)] Loss: -211949.921875\n",
      "Train Epoch: 102 [11264/54000 (21%)] Loss: -212112.984375\n",
      "Train Epoch: 102 [12672/54000 (23%)] Loss: -212604.406250\n",
      "Train Epoch: 102 [14080/54000 (26%)] Loss: -214769.156250\n",
      "Train Epoch: 102 [15488/54000 (29%)] Loss: -211537.015625\n",
      "Train Epoch: 102 [16896/54000 (31%)] Loss: -224343.218750\n",
      "Train Epoch: 102 [18304/54000 (34%)] Loss: -237336.531250\n",
      "Train Epoch: 102 [19712/54000 (37%)] Loss: -200990.968750\n",
      "Train Epoch: 102 [21120/54000 (39%)] Loss: -200128.937500\n",
      "Train Epoch: 102 [22528/54000 (42%)] Loss: -204748.078125\n",
      "Train Epoch: 102 [23936/54000 (44%)] Loss: -217439.437500\n",
      "Train Epoch: 102 [25344/54000 (47%)] Loss: -236984.593750\n",
      "Train Epoch: 102 [26752/54000 (50%)] Loss: -220220.125000\n",
      "Train Epoch: 102 [28160/54000 (52%)] Loss: -217257.312500\n",
      "Train Epoch: 102 [29568/54000 (55%)] Loss: -221115.859375\n",
      "Train Epoch: 102 [30976/54000 (57%)] Loss: -211953.906250\n",
      "Train Epoch: 102 [32384/54000 (60%)] Loss: -221449.343750\n",
      "Train Epoch: 102 [33792/54000 (63%)] Loss: -236204.187500\n",
      "Train Epoch: 102 [35200/54000 (65%)] Loss: -202069.703125\n",
      "Train Epoch: 102 [36608/54000 (68%)] Loss: -217500.171875\n",
      "Train Epoch: 102 [38016/54000 (70%)] Loss: -236594.125000\n",
      "Train Epoch: 102 [39424/54000 (73%)] Loss: -202500.140625\n",
      "Train Epoch: 102 [40832/54000 (76%)] Loss: -221143.937500\n",
      "Train Epoch: 102 [42240/54000 (78%)] Loss: -207977.718750\n",
      "Train Epoch: 102 [43648/54000 (81%)] Loss: -213803.078125\n",
      "Train Epoch: 102 [45056/54000 (83%)] Loss: -209264.265625\n",
      "Train Epoch: 102 [46464/54000 (86%)] Loss: -222725.781250\n",
      "Train Epoch: 102 [47872/54000 (89%)] Loss: -211834.984375\n",
      "Train Epoch: 102 [49280/54000 (91%)] Loss: -201780.656250\n",
      "Train Epoch: 102 [50688/54000 (94%)] Loss: -205607.375000\n",
      "Train Epoch: 102 [52096/54000 (96%)] Loss: -202370.093750\n",
      "    epoch          : 102\n",
      "    loss           : -215313.12746710525\n",
      "    val_loss       : -224042.00019054877\n",
      "Train Epoch: 103 [0/54000 (0%)] Loss: -208264.218750\n",
      "Train Epoch: 103 [1408/54000 (3%)] Loss: -238001.515625\n",
      "Train Epoch: 103 [2816/54000 (5%)] Loss: -207879.109375\n",
      "Train Epoch: 103 [4224/54000 (8%)] Loss: -209762.328125\n",
      "Train Epoch: 103 [5632/54000 (10%)] Loss: -211281.625000\n",
      "Train Epoch: 103 [7040/54000 (13%)] Loss: -205481.640625\n",
      "Train Epoch: 103 [8448/54000 (16%)] Loss: -223491.531250\n",
      "Train Epoch: 103 [9856/54000 (18%)] Loss: -207084.953125\n",
      "Train Epoch: 103 [11264/54000 (21%)] Loss: -211458.875000\n",
      "Train Epoch: 103 [12672/54000 (23%)] Loss: -213002.015625\n",
      "Train Epoch: 103 [14080/54000 (26%)] Loss: -235545.328125\n",
      "Train Epoch: 103 [15488/54000 (29%)] Loss: -208669.640625\n",
      "Train Epoch: 103 [16896/54000 (31%)] Loss: -209169.125000\n",
      "Train Epoch: 103 [18304/54000 (34%)] Loss: -206457.781250\n",
      "Train Epoch: 103 [19712/54000 (37%)] Loss: -204323.937500\n",
      "Train Epoch: 103 [21120/54000 (39%)] Loss: -219975.953125\n",
      "Train Epoch: 103 [22528/54000 (42%)] Loss: -236751.984375\n",
      "Train Epoch: 103 [23936/54000 (44%)] Loss: -217859.031250\n",
      "Train Epoch: 103 [25344/54000 (47%)] Loss: -215034.812500\n",
      "Train Epoch: 103 [26752/54000 (50%)] Loss: -205492.687500\n",
      "Train Epoch: 103 [28160/54000 (52%)] Loss: -214414.453125\n",
      "Train Epoch: 103 [29568/54000 (55%)] Loss: -235164.390625\n",
      "Train Epoch: 103 [30976/54000 (57%)] Loss: -235767.078125\n",
      "Train Epoch: 103 [32384/54000 (60%)] Loss: -221661.921875\n",
      "Train Epoch: 103 [33792/54000 (63%)] Loss: -203465.015625\n",
      "Train Epoch: 103 [35200/54000 (65%)] Loss: -205008.703125\n",
      "Train Epoch: 103 [36608/54000 (68%)] Loss: -205246.750000\n",
      "Train Epoch: 103 [38016/54000 (70%)] Loss: -205170.750000\n",
      "Train Epoch: 103 [39424/54000 (73%)] Loss: -236123.578125\n",
      "Train Epoch: 103 [40832/54000 (76%)] Loss: -215167.062500\n",
      "Train Epoch: 103 [42240/54000 (78%)] Loss: -221256.437500\n",
      "Train Epoch: 103 [43648/54000 (81%)] Loss: -212287.890625\n",
      "Train Epoch: 103 [45056/54000 (83%)] Loss: -208414.359375\n",
      "Train Epoch: 103 [46464/54000 (86%)] Loss: -217167.546875\n",
      "Train Epoch: 103 [47872/54000 (89%)] Loss: -211732.250000\n",
      "Train Epoch: 103 [49280/54000 (91%)] Loss: -212657.875000\n",
      "Train Epoch: 103 [50688/54000 (94%)] Loss: -238208.687500\n",
      "Train Epoch: 103 [52096/54000 (96%)] Loss: -204328.687500\n",
      "    epoch          : 103\n",
      "    loss           : -215336.91466058613\n",
      "    val_loss       : -224182.20263671875\n",
      "Train Epoch: 104 [0/54000 (0%)] Loss: -236645.062500\n",
      "Train Epoch: 104 [1408/54000 (3%)] Loss: -235240.156250\n",
      "Train Epoch: 104 [2816/54000 (5%)] Loss: -222758.031250\n",
      "Train Epoch: 104 [4224/54000 (8%)] Loss: -193769.437500\n",
      "Train Epoch: 104 [5632/54000 (10%)] Loss: -213658.609375\n",
      "Train Epoch: 104 [7040/54000 (13%)] Loss: -211134.234375\n",
      "Train Epoch: 104 [8448/54000 (16%)] Loss: -208559.218750\n",
      "Train Epoch: 104 [9856/54000 (18%)] Loss: -214167.171875\n",
      "Train Epoch: 104 [11264/54000 (21%)] Loss: -219711.906250\n",
      "Train Epoch: 104 [12672/54000 (23%)] Loss: -210136.171875\n",
      "Train Epoch: 104 [14080/54000 (26%)] Loss: -216617.265625\n",
      "Train Epoch: 104 [15488/54000 (29%)] Loss: -212570.906250\n",
      "Train Epoch: 104 [16896/54000 (31%)] Loss: -210799.234375\n",
      "Train Epoch: 104 [18304/54000 (34%)] Loss: -213436.703125\n",
      "Train Epoch: 104 [19712/54000 (37%)] Loss: -201162.656250\n",
      "Train Epoch: 104 [21120/54000 (39%)] Loss: -217021.046875\n",
      "Train Epoch: 104 [22528/54000 (42%)] Loss: -223668.843750\n",
      "Train Epoch: 104 [23936/54000 (44%)] Loss: -208305.515625\n",
      "Train Epoch: 104 [25344/54000 (47%)] Loss: -215255.984375\n",
      "Train Epoch: 104 [26752/54000 (50%)] Loss: -239020.828125\n",
      "Train Epoch: 104 [28160/54000 (52%)] Loss: -220433.937500\n",
      "Train Epoch: 104 [29568/54000 (55%)] Loss: -219516.906250\n",
      "Train Epoch: 104 [30976/54000 (57%)] Loss: -213871.656250\n",
      "Train Epoch: 104 [32384/54000 (60%)] Loss: -209475.468750\n",
      "Train Epoch: 104 [33792/54000 (63%)] Loss: -214775.218750\n",
      "Train Epoch: 104 [35200/54000 (65%)] Loss: -205800.875000\n",
      "Train Epoch: 104 [36608/54000 (68%)] Loss: -204516.000000\n",
      "Train Epoch: 104 [38016/54000 (70%)] Loss: -204471.203125\n",
      "Train Epoch: 104 [39424/54000 (73%)] Loss: -220603.515625\n",
      "Train Epoch: 104 [40832/54000 (76%)] Loss: -236161.031250\n",
      "Train Epoch: 104 [42240/54000 (78%)] Loss: -216673.640625\n",
      "Train Epoch: 104 [43648/54000 (81%)] Loss: -210879.187500\n",
      "Train Epoch: 104 [45056/54000 (83%)] Loss: -209244.500000\n",
      "Train Epoch: 104 [46464/54000 (86%)] Loss: -211183.203125\n",
      "Train Epoch: 104 [47872/54000 (89%)] Loss: -212136.671875\n",
      "Train Epoch: 104 [49280/54000 (91%)] Loss: -212203.812500\n",
      "Train Epoch: 104 [50688/54000 (94%)] Loss: -218045.468750\n",
      "Train Epoch: 104 [52096/54000 (96%)] Loss: -219725.859375\n",
      "    epoch          : 104\n",
      "    loss           : -215593.134569378\n",
      "    val_loss       : -224585.2648508956\n",
      "Train Epoch: 105 [0/54000 (0%)] Loss: -213559.031250\n",
      "Train Epoch: 105 [1408/54000 (3%)] Loss: -210253.328125\n",
      "Train Epoch: 105 [2816/54000 (5%)] Loss: -222402.687500\n",
      "Train Epoch: 105 [4224/54000 (8%)] Loss: -217818.781250\n",
      "Train Epoch: 105 [5632/54000 (10%)] Loss: -215777.031250\n",
      "Train Epoch: 105 [7040/54000 (13%)] Loss: -213604.796875\n",
      "Train Epoch: 105 [8448/54000 (16%)] Loss: -217777.109375\n",
      "Train Epoch: 105 [9856/54000 (18%)] Loss: -217754.281250\n",
      "Train Epoch: 105 [11264/54000 (21%)] Loss: -209906.812500\n",
      "Train Epoch: 105 [12672/54000 (23%)] Loss: -210400.640625\n",
      "Train Epoch: 105 [14080/54000 (26%)] Loss: -212394.546875\n",
      "Train Epoch: 105 [15488/54000 (29%)] Loss: -233313.609375\n",
      "Train Epoch: 105 [16896/54000 (31%)] Loss: -204184.437500\n",
      "Train Epoch: 105 [18304/54000 (34%)] Loss: -206533.359375\n",
      "Train Epoch: 105 [19712/54000 (37%)] Loss: -215567.812500\n",
      "Train Epoch: 105 [21120/54000 (39%)] Loss: -213111.515625\n",
      "Train Epoch: 105 [22528/54000 (42%)] Loss: -207859.796875\n",
      "Train Epoch: 105 [23936/54000 (44%)] Loss: -217164.718750\n",
      "Train Epoch: 105 [25344/54000 (47%)] Loss: -213311.468750\n",
      "Train Epoch: 105 [26752/54000 (50%)] Loss: -214915.562500\n",
      "Train Epoch: 105 [28160/54000 (52%)] Loss: -215601.531250\n",
      "Train Epoch: 105 [29568/54000 (55%)] Loss: -224841.281250\n",
      "Train Epoch: 105 [30976/54000 (57%)] Loss: -237558.375000\n",
      "Train Epoch: 105 [32384/54000 (60%)] Loss: -212161.500000\n",
      "Train Epoch: 105 [33792/54000 (63%)] Loss: -209056.578125\n",
      "Train Epoch: 105 [35200/54000 (65%)] Loss: -218722.687500\n",
      "Train Epoch: 105 [36608/54000 (68%)] Loss: -209297.968750\n",
      "Train Epoch: 105 [38016/54000 (70%)] Loss: -236856.265625\n",
      "Train Epoch: 105 [39424/54000 (73%)] Loss: -207975.203125\n",
      "Train Epoch: 105 [40832/54000 (76%)] Loss: -224553.062500\n",
      "Train Epoch: 105 [42240/54000 (78%)] Loss: -224007.187500\n",
      "Train Epoch: 105 [43648/54000 (81%)] Loss: -209590.937500\n",
      "Train Epoch: 105 [45056/54000 (83%)] Loss: -203625.390625\n",
      "Train Epoch: 105 [46464/54000 (86%)] Loss: -235284.781250\n",
      "Train Epoch: 105 [47872/54000 (89%)] Loss: -208227.843750\n",
      "Train Epoch: 105 [49280/54000 (91%)] Loss: -200705.750000\n",
      "Train Epoch: 105 [50688/54000 (94%)] Loss: -212185.328125\n",
      "Train Epoch: 105 [52096/54000 (96%)] Loss: -205121.250000\n",
      "    epoch          : 105\n",
      "    loss           : -215658.4300986842\n",
      "    val_loss       : -224462.66089462652\n",
      "Train Epoch: 106 [0/54000 (0%)] Loss: -215135.281250\n",
      "Train Epoch: 106 [1408/54000 (3%)] Loss: -217750.312500\n",
      "Train Epoch: 106 [2816/54000 (5%)] Loss: -213477.078125\n",
      "Train Epoch: 106 [4224/54000 (8%)] Loss: -205320.312500\n",
      "Train Epoch: 106 [5632/54000 (10%)] Loss: -200896.562500\n",
      "Train Epoch: 106 [7040/54000 (13%)] Loss: -220697.250000\n",
      "Train Epoch: 106 [8448/54000 (16%)] Loss: -212317.718750\n",
      "Train Epoch: 106 [9856/54000 (18%)] Loss: -218425.875000\n",
      "Train Epoch: 106 [11264/54000 (21%)] Loss: -219262.656250\n",
      "Train Epoch: 106 [12672/54000 (23%)] Loss: -204423.781250\n",
      "Train Epoch: 106 [14080/54000 (26%)] Loss: -207150.468750\n",
      "Train Epoch: 106 [15488/54000 (29%)] Loss: -204656.531250\n",
      "Train Epoch: 106 [16896/54000 (31%)] Loss: -211026.062500\n",
      "Train Epoch: 106 [18304/54000 (34%)] Loss: -238356.906250\n",
      "Train Epoch: 106 [19712/54000 (37%)] Loss: -223975.484375\n",
      "Train Epoch: 106 [21120/54000 (39%)] Loss: -225415.187500\n",
      "Train Epoch: 106 [22528/54000 (42%)] Loss: -215326.593750\n",
      "Train Epoch: 106 [23936/54000 (44%)] Loss: -217937.765625\n",
      "Train Epoch: 106 [25344/54000 (47%)] Loss: -212283.468750\n",
      "Train Epoch: 106 [26752/54000 (50%)] Loss: -210093.062500\n",
      "Train Epoch: 106 [28160/54000 (52%)] Loss: -213485.625000\n",
      "Train Epoch: 106 [29568/54000 (55%)] Loss: -235816.046875\n",
      "Train Epoch: 106 [30976/54000 (57%)] Loss: -215675.734375\n",
      "Train Epoch: 106 [32384/54000 (60%)] Loss: -218869.718750\n",
      "Train Epoch: 106 [33792/54000 (63%)] Loss: -210761.562500\n",
      "Train Epoch: 106 [35200/54000 (65%)] Loss: -205448.218750\n",
      "Train Epoch: 106 [36608/54000 (68%)] Loss: -212789.046875\n",
      "Train Epoch: 106 [38016/54000 (70%)] Loss: -214029.515625\n",
      "Train Epoch: 106 [39424/54000 (73%)] Loss: -205538.593750\n",
      "Train Epoch: 106 [40832/54000 (76%)] Loss: -216019.437500\n",
      "Train Epoch: 106 [42240/54000 (78%)] Loss: -238126.406250\n",
      "Train Epoch: 106 [43648/54000 (81%)] Loss: -223020.375000\n",
      "Train Epoch: 106 [45056/54000 (83%)] Loss: -214555.625000\n",
      "Train Epoch: 106 [46464/54000 (86%)] Loss: -217053.593750\n",
      "Train Epoch: 106 [47872/54000 (89%)] Loss: -220967.328125\n",
      "Train Epoch: 106 [49280/54000 (91%)] Loss: -212670.343750\n",
      "Train Epoch: 106 [50688/54000 (94%)] Loss: -238225.171875\n",
      "Train Epoch: 106 [52096/54000 (96%)] Loss: -218605.093750\n",
      "    epoch          : 106\n",
      "    loss           : -215918.67759419855\n",
      "    val_loss       : -224635.53477515245\n",
      "Train Epoch: 107 [0/54000 (0%)] Loss: -202387.968750\n",
      "Train Epoch: 107 [1408/54000 (3%)] Loss: -204193.593750\n",
      "Train Epoch: 107 [2816/54000 (5%)] Loss: -211708.812500\n",
      "Train Epoch: 107 [4224/54000 (8%)] Loss: -223816.843750\n",
      "Train Epoch: 107 [5632/54000 (10%)] Loss: -205269.140625\n",
      "Train Epoch: 107 [7040/54000 (13%)] Loss: -215743.625000\n",
      "Train Epoch: 107 [8448/54000 (16%)] Loss: -212830.421875\n",
      "Train Epoch: 107 [9856/54000 (18%)] Loss: -204529.453125\n",
      "Train Epoch: 107 [11264/54000 (21%)] Loss: -212784.078125\n",
      "Train Epoch: 107 [12672/54000 (23%)] Loss: -214099.546875\n",
      "Train Epoch: 107 [14080/54000 (26%)] Loss: -219656.421875\n",
      "Train Epoch: 107 [15488/54000 (29%)] Loss: -212230.656250\n",
      "Train Epoch: 107 [16896/54000 (31%)] Loss: -239042.500000\n",
      "Train Epoch: 107 [18304/54000 (34%)] Loss: -213465.937500\n",
      "Train Epoch: 107 [19712/54000 (37%)] Loss: -225399.093750\n",
      "Train Epoch: 107 [21120/54000 (39%)] Loss: -236191.187500\n",
      "Train Epoch: 107 [22528/54000 (42%)] Loss: -222848.312500\n",
      "Train Epoch: 107 [23936/54000 (44%)] Loss: -215459.593750\n",
      "Train Epoch: 107 [25344/54000 (47%)] Loss: -217465.812500\n",
      "Train Epoch: 107 [26752/54000 (50%)] Loss: -216853.875000\n",
      "Train Epoch: 107 [28160/54000 (52%)] Loss: -218912.609375\n",
      "Train Epoch: 107 [29568/54000 (55%)] Loss: -215194.812500\n",
      "Train Epoch: 107 [30976/54000 (57%)] Loss: -201062.484375\n",
      "Train Epoch: 107 [32384/54000 (60%)] Loss: -207994.125000\n",
      "Train Epoch: 107 [33792/54000 (63%)] Loss: -204269.781250\n",
      "Train Epoch: 107 [35200/54000 (65%)] Loss: -216464.656250\n",
      "Train Epoch: 107 [36608/54000 (68%)] Loss: -213702.843750\n",
      "Train Epoch: 107 [38016/54000 (70%)] Loss: -223447.093750\n",
      "Train Epoch: 107 [39424/54000 (73%)] Loss: -212359.859375\n",
      "Train Epoch: 107 [40832/54000 (76%)] Loss: -214783.781250\n",
      "Train Epoch: 107 [42240/54000 (78%)] Loss: -216333.015625\n",
      "Train Epoch: 107 [43648/54000 (81%)] Loss: -235600.468750\n",
      "Train Epoch: 107 [45056/54000 (83%)] Loss: -219759.000000\n",
      "Train Epoch: 107 [46464/54000 (86%)] Loss: -220014.703125\n",
      "Train Epoch: 107 [47872/54000 (89%)] Loss: -218181.484375\n",
      "Train Epoch: 107 [49280/54000 (91%)] Loss: -219340.437500\n",
      "Train Epoch: 107 [50688/54000 (94%)] Loss: -212267.390625\n",
      "Train Epoch: 107 [52096/54000 (96%)] Loss: -215235.296875\n",
      "    epoch          : 107\n",
      "    loss           : -216025.69908791865\n",
      "    val_loss       : -224762.01787585748\n",
      "Train Epoch: 108 [0/54000 (0%)] Loss: -235866.078125\n",
      "Train Epoch: 108 [1408/54000 (3%)] Loss: -211769.406250\n",
      "Train Epoch: 108 [2816/54000 (5%)] Loss: -205823.343750\n",
      "Train Epoch: 108 [4224/54000 (8%)] Loss: -203387.593750\n",
      "Train Epoch: 108 [5632/54000 (10%)] Loss: -224122.406250\n",
      "Train Epoch: 108 [7040/54000 (13%)] Loss: -197498.328125\n",
      "Train Epoch: 108 [8448/54000 (16%)] Loss: -207001.812500\n",
      "Train Epoch: 108 [9856/54000 (18%)] Loss: -207487.562500\n",
      "Train Epoch: 108 [11264/54000 (21%)] Loss: -207204.906250\n",
      "Train Epoch: 108 [12672/54000 (23%)] Loss: -236106.906250\n",
      "Train Epoch: 108 [14080/54000 (26%)] Loss: -212346.250000\n",
      "Train Epoch: 108 [15488/54000 (29%)] Loss: -212598.656250\n",
      "Train Epoch: 108 [16896/54000 (31%)] Loss: -210566.031250\n",
      "Train Epoch: 108 [18304/54000 (34%)] Loss: -236050.968750\n",
      "Train Epoch: 108 [19712/54000 (37%)] Loss: -213645.531250\n",
      "Train Epoch: 108 [21120/54000 (39%)] Loss: -212349.937500\n",
      "Train Epoch: 108 [22528/54000 (42%)] Loss: -214521.468750\n",
      "Train Epoch: 108 [23936/54000 (44%)] Loss: -213439.562500\n",
      "Train Epoch: 108 [25344/54000 (47%)] Loss: -208486.718750\n",
      "Train Epoch: 108 [26752/54000 (50%)] Loss: -208804.640625\n",
      "Train Epoch: 108 [28160/54000 (52%)] Loss: -220195.109375\n",
      "Train Epoch: 108 [29568/54000 (55%)] Loss: -225300.187500\n",
      "Train Epoch: 108 [30976/54000 (57%)] Loss: -213320.218750\n",
      "Train Epoch: 108 [32384/54000 (60%)] Loss: -236736.421875\n",
      "Train Epoch: 108 [33792/54000 (63%)] Loss: -210274.515625\n",
      "Train Epoch: 108 [35200/54000 (65%)] Loss: -221740.968750\n",
      "Train Epoch: 108 [36608/54000 (68%)] Loss: -202689.984375\n",
      "Train Epoch: 108 [38016/54000 (70%)] Loss: -207154.046875\n",
      "Train Epoch: 108 [39424/54000 (73%)] Loss: -215930.203125\n",
      "Train Epoch: 108 [40832/54000 (76%)] Loss: -202083.296875\n",
      "Train Epoch: 108 [42240/54000 (78%)] Loss: -221565.328125\n",
      "Train Epoch: 108 [43648/54000 (81%)] Loss: -215654.796875\n",
      "Train Epoch: 108 [45056/54000 (83%)] Loss: -223871.531250\n",
      "Train Epoch: 108 [46464/54000 (86%)] Loss: -207105.171875\n",
      "Train Epoch: 108 [47872/54000 (89%)] Loss: -210384.843750\n",
      "Train Epoch: 108 [49280/54000 (91%)] Loss: -211781.328125\n",
      "Train Epoch: 108 [50688/54000 (94%)] Loss: -236863.281250\n",
      "Train Epoch: 108 [52096/54000 (96%)] Loss: -216467.375000\n",
      "    epoch          : 108\n",
      "    loss           : -215995.27052183016\n",
      "    val_loss       : -224742.14768721416\n",
      "Train Epoch: 109 [0/54000 (0%)] Loss: -211584.687500\n",
      "Train Epoch: 109 [1408/54000 (3%)] Loss: -238470.390625\n",
      "Train Epoch: 109 [2816/54000 (5%)] Loss: -218709.953125\n",
      "Train Epoch: 109 [4224/54000 (8%)] Loss: -216374.890625\n",
      "Train Epoch: 109 [5632/54000 (10%)] Loss: -217414.593750\n",
      "Train Epoch: 109 [7040/54000 (13%)] Loss: -236175.750000\n",
      "Train Epoch: 109 [8448/54000 (16%)] Loss: -225633.906250\n",
      "Train Epoch: 109 [9856/54000 (18%)] Loss: -222545.046875\n",
      "Train Epoch: 109 [11264/54000 (21%)] Loss: -209140.593750\n",
      "Train Epoch: 109 [12672/54000 (23%)] Loss: -219150.734375\n",
      "Train Epoch: 109 [14080/54000 (26%)] Loss: -203162.203125\n",
      "Train Epoch: 109 [15488/54000 (29%)] Loss: -237097.296875\n",
      "Train Epoch: 109 [16896/54000 (31%)] Loss: -208566.687500\n",
      "Train Epoch: 109 [18304/54000 (34%)] Loss: -216800.078125\n",
      "Train Epoch: 109 [19712/54000 (37%)] Loss: -216859.703125\n",
      "Train Epoch: 109 [21120/54000 (39%)] Loss: -217503.468750\n",
      "Train Epoch: 109 [22528/54000 (42%)] Loss: -218947.750000\n",
      "Train Epoch: 109 [23936/54000 (44%)] Loss: -214157.406250\n",
      "Train Epoch: 109 [25344/54000 (47%)] Loss: -210434.093750\n",
      "Train Epoch: 109 [26752/54000 (50%)] Loss: -211793.046875\n",
      "Train Epoch: 109 [28160/54000 (52%)] Loss: -204450.531250\n",
      "Train Epoch: 109 [29568/54000 (55%)] Loss: -210727.140625\n",
      "Train Epoch: 109 [30976/54000 (57%)] Loss: -215516.078125\n",
      "Train Epoch: 109 [32384/54000 (60%)] Loss: -207745.109375\n",
      "Train Epoch: 109 [33792/54000 (63%)] Loss: -236515.281250\n",
      "Train Epoch: 109 [35200/54000 (65%)] Loss: -225501.234375\n",
      "Train Epoch: 109 [36608/54000 (68%)] Loss: -209092.281250\n",
      "Train Epoch: 109 [38016/54000 (70%)] Loss: -202982.593750\n",
      "Train Epoch: 109 [39424/54000 (73%)] Loss: -217593.453125\n",
      "Train Epoch: 109 [40832/54000 (76%)] Loss: -236990.609375\n",
      "Train Epoch: 109 [42240/54000 (78%)] Loss: -215730.125000\n",
      "Train Epoch: 109 [43648/54000 (81%)] Loss: -211982.218750\n",
      "Train Epoch: 109 [45056/54000 (83%)] Loss: -239265.765625\n",
      "Train Epoch: 109 [46464/54000 (86%)] Loss: -203867.812500\n",
      "Train Epoch: 109 [47872/54000 (89%)] Loss: -216729.406250\n",
      "Train Epoch: 109 [49280/54000 (91%)] Loss: -209610.750000\n",
      "Train Epoch: 109 [50688/54000 (94%)] Loss: -237780.718750\n",
      "Train Epoch: 109 [52096/54000 (96%)] Loss: -202461.093750\n",
      "    epoch          : 109\n",
      "    loss           : -216260.0904979067\n",
      "    val_loss       : -224591.97895626904\n",
      "Train Epoch: 110 [0/54000 (0%)] Loss: -237811.687500\n",
      "Train Epoch: 110 [1408/54000 (3%)] Loss: -212315.203125\n",
      "Train Epoch: 110 [2816/54000 (5%)] Loss: -211680.250000\n",
      "Train Epoch: 110 [4224/54000 (8%)] Loss: -215794.765625\n",
      "Train Epoch: 110 [5632/54000 (10%)] Loss: -209664.625000\n",
      "Train Epoch: 110 [7040/54000 (13%)] Loss: -203029.312500\n",
      "Train Epoch: 110 [8448/54000 (16%)] Loss: -222869.312500\n",
      "Train Epoch: 110 [9856/54000 (18%)] Loss: -214176.515625\n",
      "Train Epoch: 110 [11264/54000 (21%)] Loss: -216209.468750\n",
      "Train Epoch: 110 [12672/54000 (23%)] Loss: -201059.328125\n",
      "Train Epoch: 110 [14080/54000 (26%)] Loss: -237261.781250\n",
      "Train Epoch: 110 [15488/54000 (29%)] Loss: -211264.031250\n",
      "Train Epoch: 110 [16896/54000 (31%)] Loss: -207383.093750\n",
      "Train Epoch: 110 [18304/54000 (34%)] Loss: -205974.937500\n",
      "Train Epoch: 110 [19712/54000 (37%)] Loss: -210481.406250\n",
      "Train Epoch: 110 [21120/54000 (39%)] Loss: -210940.718750\n",
      "Train Epoch: 110 [22528/54000 (42%)] Loss: -227725.015625\n",
      "Train Epoch: 110 [23936/54000 (44%)] Loss: -220477.875000\n",
      "Train Epoch: 110 [25344/54000 (47%)] Loss: -216173.546875\n",
      "Train Epoch: 110 [26752/54000 (50%)] Loss: -217154.281250\n",
      "Train Epoch: 110 [28160/54000 (52%)] Loss: -210696.671875\n",
      "Train Epoch: 110 [29568/54000 (55%)] Loss: -216024.593750\n",
      "Train Epoch: 110 [30976/54000 (57%)] Loss: -204724.250000\n",
      "Train Epoch: 110 [32384/54000 (60%)] Loss: -203946.406250\n",
      "Train Epoch: 110 [33792/54000 (63%)] Loss: -216804.359375\n",
      "Train Epoch: 110 [35200/54000 (65%)] Loss: -236326.140625\n",
      "Train Epoch: 110 [36608/54000 (68%)] Loss: -222584.890625\n",
      "Train Epoch: 110 [38016/54000 (70%)] Loss: -210281.093750\n",
      "Train Epoch: 110 [39424/54000 (73%)] Loss: -236190.890625\n",
      "Train Epoch: 110 [40832/54000 (76%)] Loss: -217888.562500\n",
      "Train Epoch: 110 [42240/54000 (78%)] Loss: -211122.828125\n",
      "Train Epoch: 110 [43648/54000 (81%)] Loss: -212894.750000\n",
      "Train Epoch: 110 [45056/54000 (83%)] Loss: -205286.000000\n",
      "Train Epoch: 110 [46464/54000 (86%)] Loss: -219340.640625\n",
      "Train Epoch: 110 [47872/54000 (89%)] Loss: -215314.000000\n",
      "Train Epoch: 110 [49280/54000 (91%)] Loss: -215568.703125\n",
      "Train Epoch: 110 [50688/54000 (94%)] Loss: -205519.375000\n",
      "Train Epoch: 110 [52096/54000 (96%)] Loss: -237981.609375\n",
      "    epoch          : 110\n",
      "    loss           : -216280.88576555025\n",
      "    val_loss       : -224582.30979658916\n",
      "Train Epoch: 111 [0/54000 (0%)] Loss: -235828.984375\n",
      "Train Epoch: 111 [1408/54000 (3%)] Loss: -206107.078125\n",
      "Train Epoch: 111 [2816/54000 (5%)] Loss: -219867.953125\n",
      "Train Epoch: 111 [4224/54000 (8%)] Loss: -218745.015625\n",
      "Train Epoch: 111 [5632/54000 (10%)] Loss: -213907.187500\n",
      "Train Epoch: 111 [7040/54000 (13%)] Loss: -211127.171875\n",
      "Train Epoch: 111 [8448/54000 (16%)] Loss: -219255.703125\n",
      "Train Epoch: 111 [9856/54000 (18%)] Loss: -210727.796875\n",
      "Train Epoch: 111 [11264/54000 (21%)] Loss: -212656.640625\n",
      "Train Epoch: 111 [12672/54000 (23%)] Loss: -207380.015625\n",
      "Train Epoch: 111 [14080/54000 (26%)] Loss: -216745.937500\n",
      "Train Epoch: 111 [15488/54000 (29%)] Loss: -236033.296875\n",
      "Train Epoch: 111 [16896/54000 (31%)] Loss: -215104.812500\n",
      "Train Epoch: 111 [18304/54000 (34%)] Loss: -206198.875000\n",
      "Train Epoch: 111 [19712/54000 (37%)] Loss: -219602.562500\n",
      "Train Epoch: 111 [21120/54000 (39%)] Loss: -237179.906250\n",
      "Train Epoch: 111 [22528/54000 (42%)] Loss: -220687.578125\n",
      "Train Epoch: 111 [23936/54000 (44%)] Loss: -203589.921875\n",
      "Train Epoch: 111 [25344/54000 (47%)] Loss: -215643.312500\n",
      "Train Epoch: 111 [26752/54000 (50%)] Loss: -211819.578125\n",
      "Train Epoch: 111 [28160/54000 (52%)] Loss: -217034.921875\n",
      "Train Epoch: 111 [29568/54000 (55%)] Loss: -236489.734375\n",
      "Train Epoch: 111 [30976/54000 (57%)] Loss: -214900.062500\n",
      "Train Epoch: 111 [32384/54000 (60%)] Loss: -236166.562500\n",
      "Train Epoch: 111 [33792/54000 (63%)] Loss: -218945.718750\n",
      "Train Epoch: 111 [35200/54000 (65%)] Loss: -221804.296875\n",
      "Train Epoch: 111 [36608/54000 (68%)] Loss: -205145.281250\n",
      "Train Epoch: 111 [38016/54000 (70%)] Loss: -214784.000000\n",
      "Train Epoch: 111 [39424/54000 (73%)] Loss: -236591.906250\n",
      "Train Epoch: 111 [40832/54000 (76%)] Loss: -210005.812500\n",
      "Train Epoch: 111 [42240/54000 (78%)] Loss: -209015.625000\n",
      "Train Epoch: 111 [43648/54000 (81%)] Loss: -208736.734375\n",
      "Train Epoch: 111 [45056/54000 (83%)] Loss: -203106.906250\n",
      "Train Epoch: 111 [46464/54000 (86%)] Loss: -210348.031250\n",
      "Train Epoch: 111 [47872/54000 (89%)] Loss: -210405.296875\n",
      "Train Epoch: 111 [49280/54000 (91%)] Loss: -213180.156250\n",
      "Train Epoch: 111 [50688/54000 (94%)] Loss: -207459.250000\n",
      "Train Epoch: 111 [52096/54000 (96%)] Loss: -208777.531250\n",
      "    epoch          : 111\n",
      "    loss           : -216434.37223385167\n",
      "    val_loss       : -224744.6514624619\n",
      "Train Epoch: 112 [0/54000 (0%)] Loss: -239871.000000\n",
      "Train Epoch: 112 [1408/54000 (3%)] Loss: -220878.406250\n",
      "Train Epoch: 112 [2816/54000 (5%)] Loss: -212734.968750\n",
      "Train Epoch: 112 [4224/54000 (8%)] Loss: -213009.125000\n",
      "Train Epoch: 112 [5632/54000 (10%)] Loss: -207182.609375\n",
      "Train Epoch: 112 [7040/54000 (13%)] Loss: -212262.281250\n",
      "Train Epoch: 112 [8448/54000 (16%)] Loss: -203135.937500\n",
      "Train Epoch: 112 [9856/54000 (18%)] Loss: -214450.812500\n",
      "Train Epoch: 112 [11264/54000 (21%)] Loss: -216807.734375\n",
      "Train Epoch: 112 [12672/54000 (23%)] Loss: -206395.500000\n",
      "Train Epoch: 112 [14080/54000 (26%)] Loss: -213795.828125\n",
      "Train Epoch: 112 [15488/54000 (29%)] Loss: -225924.671875\n",
      "Train Epoch: 112 [16896/54000 (31%)] Loss: -219870.812500\n",
      "Train Epoch: 112 [18304/54000 (34%)] Loss: -217034.015625\n",
      "Train Epoch: 112 [19712/54000 (37%)] Loss: -239461.718750\n",
      "Train Epoch: 112 [21120/54000 (39%)] Loss: -213500.109375\n",
      "Train Epoch: 112 [22528/54000 (42%)] Loss: -205659.062500\n",
      "Train Epoch: 112 [23936/54000 (44%)] Loss: -197712.187500\n",
      "Train Epoch: 112 [25344/54000 (47%)] Loss: -239909.000000\n",
      "Train Epoch: 112 [26752/54000 (50%)] Loss: -211041.281250\n",
      "Train Epoch: 112 [28160/54000 (52%)] Loss: -217849.468750\n",
      "Train Epoch: 112 [29568/54000 (55%)] Loss: -212960.406250\n",
      "Train Epoch: 112 [30976/54000 (57%)] Loss: -214371.265625\n",
      "Train Epoch: 112 [32384/54000 (60%)] Loss: -213633.968750\n",
      "Train Epoch: 112 [33792/54000 (63%)] Loss: -215759.500000\n",
      "Train Epoch: 112 [35200/54000 (65%)] Loss: -209600.328125\n",
      "Train Epoch: 112 [36608/54000 (68%)] Loss: -206410.406250\n",
      "Train Epoch: 112 [38016/54000 (70%)] Loss: -240043.906250\n",
      "Train Epoch: 112 [39424/54000 (73%)] Loss: -216133.937500\n",
      "Train Epoch: 112 [40832/54000 (76%)] Loss: -205410.312500\n",
      "Train Epoch: 112 [42240/54000 (78%)] Loss: -223047.187500\n",
      "Train Epoch: 112 [43648/54000 (81%)] Loss: -223187.468750\n",
      "Train Epoch: 112 [45056/54000 (83%)] Loss: -208823.531250\n",
      "Train Epoch: 112 [46464/54000 (86%)] Loss: -211550.718750\n",
      "Train Epoch: 112 [47872/54000 (89%)] Loss: -214608.609375\n",
      "Train Epoch: 112 [49280/54000 (91%)] Loss: -215150.156250\n",
      "Train Epoch: 112 [50688/54000 (94%)] Loss: -220611.937500\n",
      "Train Epoch: 112 [52096/54000 (96%)] Loss: -219074.125000\n",
      "    epoch          : 112\n",
      "    loss           : -216536.25127093302\n",
      "    val_loss       : -224889.75139338797\n",
      "Train Epoch: 113 [0/54000 (0%)] Loss: -236311.437500\n",
      "Train Epoch: 113 [1408/54000 (3%)] Loss: -222709.109375\n",
      "Train Epoch: 113 [2816/54000 (5%)] Loss: -223273.906250\n",
      "Train Epoch: 113 [4224/54000 (8%)] Loss: -214919.906250\n",
      "Train Epoch: 113 [5632/54000 (10%)] Loss: -203401.718750\n",
      "Train Epoch: 113 [7040/54000 (13%)] Loss: -210329.843750\n",
      "Train Epoch: 113 [8448/54000 (16%)] Loss: -235761.031250\n",
      "Train Epoch: 113 [9856/54000 (18%)] Loss: -218478.140625\n",
      "Train Epoch: 113 [11264/54000 (21%)] Loss: -217453.296875\n",
      "Train Epoch: 113 [12672/54000 (23%)] Loss: -240485.937500\n",
      "Train Epoch: 113 [14080/54000 (26%)] Loss: -214251.218750\n",
      "Train Epoch: 113 [15488/54000 (29%)] Loss: -207133.562500\n",
      "Train Epoch: 113 [16896/54000 (31%)] Loss: -217195.500000\n",
      "Train Epoch: 113 [18304/54000 (34%)] Loss: -204378.843750\n",
      "Train Epoch: 113 [19712/54000 (37%)] Loss: -236936.718750\n",
      "Train Epoch: 113 [21120/54000 (39%)] Loss: -208466.781250\n",
      "Train Epoch: 113 [22528/54000 (42%)] Loss: -209083.625000\n",
      "Train Epoch: 113 [23936/54000 (44%)] Loss: -221236.187500\n",
      "Train Epoch: 113 [25344/54000 (47%)] Loss: -225095.406250\n",
      "Train Epoch: 113 [26752/54000 (50%)] Loss: -225177.125000\n",
      "Train Epoch: 113 [28160/54000 (52%)] Loss: -210658.828125\n",
      "Train Epoch: 113 [29568/54000 (55%)] Loss: -211742.828125\n",
      "Train Epoch: 113 [30976/54000 (57%)] Loss: -217098.687500\n",
      "Train Epoch: 113 [32384/54000 (60%)] Loss: -216551.093750\n",
      "Train Epoch: 113 [33792/54000 (63%)] Loss: -237708.468750\n",
      "Train Epoch: 113 [35200/54000 (65%)] Loss: -224784.187500\n",
      "Train Epoch: 113 [36608/54000 (68%)] Loss: -206409.187500\n",
      "Train Epoch: 113 [38016/54000 (70%)] Loss: -220906.953125\n",
      "Train Epoch: 113 [39424/54000 (73%)] Loss: -214559.859375\n",
      "Train Epoch: 113 [40832/54000 (76%)] Loss: -221141.015625\n",
      "Train Epoch: 113 [42240/54000 (78%)] Loss: -204831.625000\n",
      "Train Epoch: 113 [43648/54000 (81%)] Loss: -217103.765625\n",
      "Train Epoch: 113 [45056/54000 (83%)] Loss: -217930.687500\n",
      "Train Epoch: 113 [46464/54000 (86%)] Loss: -217197.125000\n",
      "Train Epoch: 113 [47872/54000 (89%)] Loss: -222695.937500\n",
      "Train Epoch: 113 [49280/54000 (91%)] Loss: -207763.656250\n",
      "Train Epoch: 113 [50688/54000 (94%)] Loss: -217623.078125\n",
      "Train Epoch: 113 [52096/54000 (96%)] Loss: -236473.171875\n",
      "    epoch          : 113\n",
      "    loss           : -216631.49484150717\n",
      "    val_loss       : -224900.40669064404\n",
      "Train Epoch: 114 [0/54000 (0%)] Loss: -224358.062500\n",
      "Train Epoch: 114 [1408/54000 (3%)] Loss: -222472.171875\n",
      "Train Epoch: 114 [2816/54000 (5%)] Loss: -219459.062500\n",
      "Train Epoch: 114 [4224/54000 (8%)] Loss: -212621.828125\n",
      "Train Epoch: 114 [5632/54000 (10%)] Loss: -209026.406250\n",
      "Train Epoch: 114 [7040/54000 (13%)] Loss: -200274.828125\n",
      "Train Epoch: 114 [8448/54000 (16%)] Loss: -211855.671875\n",
      "Train Epoch: 114 [9856/54000 (18%)] Loss: -236994.062500\n",
      "Train Epoch: 114 [11264/54000 (21%)] Loss: -213332.500000\n",
      "Train Epoch: 114 [12672/54000 (23%)] Loss: -211128.843750\n",
      "Train Epoch: 114 [14080/54000 (26%)] Loss: -220023.343750\n",
      "Train Epoch: 114 [15488/54000 (29%)] Loss: -202961.687500\n",
      "Train Epoch: 114 [16896/54000 (31%)] Loss: -220748.281250\n",
      "Train Epoch: 114 [18304/54000 (34%)] Loss: -207609.265625\n",
      "Train Epoch: 114 [19712/54000 (37%)] Loss: -208125.125000\n",
      "Train Epoch: 114 [21120/54000 (39%)] Loss: -221871.015625\n",
      "Train Epoch: 114 [22528/54000 (42%)] Loss: -206033.046875\n",
      "Train Epoch: 114 [23936/54000 (44%)] Loss: -222205.968750\n",
      "Train Epoch: 114 [25344/54000 (47%)] Loss: -215558.156250\n",
      "Train Epoch: 114 [26752/54000 (50%)] Loss: -214280.562500\n",
      "Train Epoch: 114 [28160/54000 (52%)] Loss: -216345.781250\n",
      "Train Epoch: 114 [29568/54000 (55%)] Loss: -218691.000000\n",
      "Train Epoch: 114 [30976/54000 (57%)] Loss: -240451.343750\n",
      "Train Epoch: 114 [32384/54000 (60%)] Loss: -237493.093750\n",
      "Train Epoch: 114 [33792/54000 (63%)] Loss: -206553.234375\n",
      "Train Epoch: 114 [35200/54000 (65%)] Loss: -216923.968750\n",
      "Train Epoch: 114 [36608/54000 (68%)] Loss: -224646.453125\n",
      "Train Epoch: 114 [38016/54000 (70%)] Loss: -215434.656250\n",
      "Train Epoch: 114 [39424/54000 (73%)] Loss: -217387.937500\n",
      "Train Epoch: 114 [40832/54000 (76%)] Loss: -207381.140625\n",
      "Train Epoch: 114 [42240/54000 (78%)] Loss: -216794.296875\n",
      "Train Epoch: 114 [43648/54000 (81%)] Loss: -220994.390625\n",
      "Train Epoch: 114 [45056/54000 (83%)] Loss: -217208.343750\n",
      "Train Epoch: 114 [46464/54000 (86%)] Loss: -217634.921875\n",
      "Train Epoch: 114 [47872/54000 (89%)] Loss: -209410.406250\n",
      "Train Epoch: 114 [49280/54000 (91%)] Loss: -236112.515625\n",
      "Train Epoch: 114 [50688/54000 (94%)] Loss: -214249.906250\n",
      "Train Epoch: 114 [52096/54000 (96%)] Loss: -206418.250000\n",
      "    epoch          : 114\n",
      "    loss           : -216678.28401614833\n",
      "    val_loss       : -225241.59403582316\n",
      "Train Epoch: 115 [0/54000 (0%)] Loss: -240088.750000\n",
      "Train Epoch: 115 [1408/54000 (3%)] Loss: -214048.343750\n",
      "Train Epoch: 115 [2816/54000 (5%)] Loss: -223836.750000\n",
      "Train Epoch: 115 [4224/54000 (8%)] Loss: -210233.062500\n",
      "Train Epoch: 115 [5632/54000 (10%)] Loss: -211634.500000\n",
      "Train Epoch: 115 [7040/54000 (13%)] Loss: -211134.500000\n",
      "Train Epoch: 115 [8448/54000 (16%)] Loss: -204773.625000\n",
      "Train Epoch: 115 [9856/54000 (18%)] Loss: -207018.296875\n",
      "Train Epoch: 115 [11264/54000 (21%)] Loss: -237945.046875\n",
      "Train Epoch: 115 [12672/54000 (23%)] Loss: -211629.203125\n",
      "Train Epoch: 115 [14080/54000 (26%)] Loss: -216262.187500\n",
      "Train Epoch: 115 [15488/54000 (29%)] Loss: -220993.984375\n",
      "Train Epoch: 115 [16896/54000 (31%)] Loss: -221043.437500\n",
      "Train Epoch: 115 [18304/54000 (34%)] Loss: -202839.593750\n",
      "Train Epoch: 115 [19712/54000 (37%)] Loss: -206613.281250\n",
      "Train Epoch: 115 [21120/54000 (39%)] Loss: -239061.187500\n",
      "Train Epoch: 115 [22528/54000 (42%)] Loss: -219945.359375\n",
      "Train Epoch: 115 [23936/54000 (44%)] Loss: -213830.765625\n",
      "Train Epoch: 115 [25344/54000 (47%)] Loss: -205130.703125\n",
      "Train Epoch: 115 [26752/54000 (50%)] Loss: -209253.890625\n",
      "Train Epoch: 115 [28160/54000 (52%)] Loss: -236957.093750\n",
      "Train Epoch: 115 [29568/54000 (55%)] Loss: -221185.968750\n",
      "Train Epoch: 115 [30976/54000 (57%)] Loss: -204341.984375\n",
      "Train Epoch: 115 [32384/54000 (60%)] Loss: -224812.718750\n",
      "Train Epoch: 115 [33792/54000 (63%)] Loss: -214264.531250\n",
      "Train Epoch: 115 [35200/54000 (65%)] Loss: -221682.515625\n",
      "Train Epoch: 115 [36608/54000 (68%)] Loss: -210830.906250\n",
      "Train Epoch: 115 [38016/54000 (70%)] Loss: -201795.218750\n",
      "Train Epoch: 115 [39424/54000 (73%)] Loss: -204686.890625\n",
      "Train Epoch: 115 [40832/54000 (76%)] Loss: -238896.765625\n",
      "Train Epoch: 115 [42240/54000 (78%)] Loss: -203077.531250\n",
      "Train Epoch: 115 [43648/54000 (81%)] Loss: -206985.671875\n",
      "Train Epoch: 115 [45056/54000 (83%)] Loss: -217490.609375\n",
      "Train Epoch: 115 [46464/54000 (86%)] Loss: -236836.390625\n",
      "Train Epoch: 115 [47872/54000 (89%)] Loss: -223812.796875\n",
      "Train Epoch: 115 [49280/54000 (91%)] Loss: -208878.093750\n",
      "Train Epoch: 115 [50688/54000 (94%)] Loss: -208181.359375\n",
      "Train Epoch: 115 [52096/54000 (96%)] Loss: -204051.093750\n",
      "    epoch          : 115\n",
      "    loss           : -216805.92026764355\n",
      "    val_loss       : -225667.82331364328\n",
      "Train Epoch: 116 [0/54000 (0%)] Loss: -212431.093750\n",
      "Train Epoch: 116 [1408/54000 (3%)] Loss: -237897.187500\n",
      "Train Epoch: 116 [2816/54000 (5%)] Loss: -215891.234375\n",
      "Train Epoch: 116 [4224/54000 (8%)] Loss: -209445.859375\n",
      "Train Epoch: 116 [5632/54000 (10%)] Loss: -214754.343750\n",
      "Train Epoch: 116 [7040/54000 (13%)] Loss: -201633.687500\n",
      "Train Epoch: 116 [8448/54000 (16%)] Loss: -209058.406250\n",
      "Train Epoch: 116 [9856/54000 (18%)] Loss: -239049.390625\n",
      "Train Epoch: 116 [11264/54000 (21%)] Loss: -225868.125000\n",
      "Train Epoch: 116 [12672/54000 (23%)] Loss: -217966.437500\n",
      "Train Epoch: 116 [14080/54000 (26%)] Loss: -218790.500000\n",
      "Train Epoch: 116 [15488/54000 (29%)] Loss: -220332.437500\n",
      "Train Epoch: 116 [16896/54000 (31%)] Loss: -217604.796875\n",
      "Train Epoch: 116 [18304/54000 (34%)] Loss: -207786.765625\n",
      "Train Epoch: 116 [19712/54000 (37%)] Loss: -207602.031250\n",
      "Train Epoch: 116 [21120/54000 (39%)] Loss: -209463.250000\n",
      "Train Epoch: 116 [22528/54000 (42%)] Loss: -209148.000000\n",
      "Train Epoch: 116 [23936/54000 (44%)] Loss: -239439.312500\n",
      "Train Epoch: 116 [25344/54000 (47%)] Loss: -210773.890625\n",
      "Train Epoch: 116 [26752/54000 (50%)] Loss: -211756.343750\n",
      "Train Epoch: 116 [28160/54000 (52%)] Loss: -213100.562500\n",
      "Train Epoch: 116 [29568/54000 (55%)] Loss: -211061.921875\n",
      "Train Epoch: 116 [30976/54000 (57%)] Loss: -206157.843750\n",
      "Train Epoch: 116 [32384/54000 (60%)] Loss: -239925.703125\n",
      "Train Epoch: 116 [33792/54000 (63%)] Loss: -221385.921875\n",
      "Train Epoch: 116 [35200/54000 (65%)] Loss: -211142.953125\n",
      "Train Epoch: 116 [36608/54000 (68%)] Loss: -223306.593750\n",
      "Train Epoch: 116 [38016/54000 (70%)] Loss: -215712.375000\n",
      "Train Epoch: 116 [39424/54000 (73%)] Loss: -218707.281250\n",
      "Train Epoch: 116 [40832/54000 (76%)] Loss: -207413.375000\n",
      "Train Epoch: 116 [42240/54000 (78%)] Loss: -199943.203125\n",
      "Train Epoch: 116 [43648/54000 (81%)] Loss: -212571.906250\n",
      "Train Epoch: 116 [45056/54000 (83%)] Loss: -214002.437500\n",
      "Train Epoch: 116 [46464/54000 (86%)] Loss: -213659.593750\n",
      "Train Epoch: 116 [47872/54000 (89%)] Loss: -213386.687500\n",
      "Train Epoch: 116 [49280/54000 (91%)] Loss: -237736.796875\n",
      "Train Epoch: 116 [50688/54000 (94%)] Loss: -219745.437500\n",
      "Train Epoch: 116 [52096/54000 (96%)] Loss: -218213.796875\n",
      "    epoch          : 116\n",
      "    loss           : -216984.91772577752\n",
      "    val_loss       : -225118.6250952744\n",
      "Train Epoch: 117 [0/54000 (0%)] Loss: -210340.937500\n",
      "Train Epoch: 117 [1408/54000 (3%)] Loss: -213153.890625\n",
      "Train Epoch: 117 [2816/54000 (5%)] Loss: -214835.703125\n",
      "Train Epoch: 117 [4224/54000 (8%)] Loss: -204796.390625\n",
      "Train Epoch: 117 [5632/54000 (10%)] Loss: -215654.875000\n",
      "Train Epoch: 117 [7040/54000 (13%)] Loss: -215029.984375\n",
      "Train Epoch: 117 [8448/54000 (16%)] Loss: -236263.468750\n",
      "Train Epoch: 117 [9856/54000 (18%)] Loss: -220640.031250\n",
      "Train Epoch: 117 [11264/54000 (21%)] Loss: -209042.484375\n",
      "Train Epoch: 117 [12672/54000 (23%)] Loss: -203734.312500\n",
      "Train Epoch: 117 [14080/54000 (26%)] Loss: -222130.578125\n",
      "Train Epoch: 117 [15488/54000 (29%)] Loss: -208806.406250\n",
      "Train Epoch: 117 [16896/54000 (31%)] Loss: -202554.500000\n",
      "Train Epoch: 117 [18304/54000 (34%)] Loss: -203920.109375\n",
      "Train Epoch: 117 [19712/54000 (37%)] Loss: -226758.625000\n",
      "Train Epoch: 117 [21120/54000 (39%)] Loss: -220896.328125\n",
      "Train Epoch: 117 [22528/54000 (42%)] Loss: -222439.468750\n",
      "Train Epoch: 117 [23936/54000 (44%)] Loss: -223963.578125\n",
      "Train Epoch: 117 [25344/54000 (47%)] Loss: -237957.703125\n",
      "Train Epoch: 117 [26752/54000 (50%)] Loss: -219363.796875\n",
      "Train Epoch: 117 [28160/54000 (52%)] Loss: -204774.250000\n",
      "Train Epoch: 117 [29568/54000 (55%)] Loss: -206326.843750\n",
      "Train Epoch: 117 [30976/54000 (57%)] Loss: -204423.093750\n",
      "Train Epoch: 117 [32384/54000 (60%)] Loss: -211516.968750\n",
      "Train Epoch: 117 [33792/54000 (63%)] Loss: -210029.781250\n",
      "Train Epoch: 117 [35200/54000 (65%)] Loss: -205326.359375\n",
      "Train Epoch: 117 [36608/54000 (68%)] Loss: -219960.000000\n",
      "Train Epoch: 117 [38016/54000 (70%)] Loss: -238446.343750\n",
      "Train Epoch: 117 [39424/54000 (73%)] Loss: -205776.265625\n",
      "Train Epoch: 117 [40832/54000 (76%)] Loss: -223236.593750\n",
      "Train Epoch: 117 [42240/54000 (78%)] Loss: -221202.687500\n",
      "Train Epoch: 117 [43648/54000 (81%)] Loss: -205703.437500\n",
      "Train Epoch: 117 [45056/54000 (83%)] Loss: -209133.875000\n",
      "Train Epoch: 117 [46464/54000 (86%)] Loss: -205130.343750\n",
      "Train Epoch: 117 [47872/54000 (89%)] Loss: -212780.140625\n",
      "Train Epoch: 117 [49280/54000 (91%)] Loss: -205727.890625\n",
      "Train Epoch: 117 [50688/54000 (94%)] Loss: -205854.062500\n",
      "Train Epoch: 117 [52096/54000 (96%)] Loss: -205493.562500\n",
      "    epoch          : 117\n",
      "    loss           : -217017.37970992824\n",
      "    val_loss       : -225286.1297399009\n",
      "Train Epoch: 118 [0/54000 (0%)] Loss: -213612.703125\n",
      "Train Epoch: 118 [1408/54000 (3%)] Loss: -238554.843750\n",
      "Train Epoch: 118 [2816/54000 (5%)] Loss: -211094.593750\n",
      "Train Epoch: 118 [4224/54000 (8%)] Loss: -217638.968750\n",
      "Train Epoch: 118 [5632/54000 (10%)] Loss: -209462.109375\n",
      "Train Epoch: 118 [7040/54000 (13%)] Loss: -221691.343750\n",
      "Train Epoch: 118 [8448/54000 (16%)] Loss: -213493.921875\n",
      "Train Epoch: 118 [9856/54000 (18%)] Loss: -209264.328125\n",
      "Train Epoch: 118 [11264/54000 (21%)] Loss: -221332.234375\n",
      "Train Epoch: 118 [12672/54000 (23%)] Loss: -206147.015625\n",
      "Train Epoch: 118 [14080/54000 (26%)] Loss: -205273.625000\n",
      "Train Epoch: 118 [15488/54000 (29%)] Loss: -205036.437500\n",
      "Train Epoch: 118 [16896/54000 (31%)] Loss: -205644.078125\n",
      "Train Epoch: 118 [18304/54000 (34%)] Loss: -219798.500000\n",
      "Train Epoch: 118 [19712/54000 (37%)] Loss: -224641.656250\n",
      "Train Epoch: 118 [21120/54000 (39%)] Loss: -224594.593750\n",
      "Train Epoch: 118 [22528/54000 (42%)] Loss: -212329.906250\n",
      "Train Epoch: 118 [23936/54000 (44%)] Loss: -210880.812500\n",
      "Train Epoch: 118 [25344/54000 (47%)] Loss: -210199.046875\n",
      "Train Epoch: 118 [26752/54000 (50%)] Loss: -215339.218750\n",
      "Train Epoch: 118 [28160/54000 (52%)] Loss: -214089.390625\n",
      "Train Epoch: 118 [29568/54000 (55%)] Loss: -215053.234375\n",
      "Train Epoch: 118 [30976/54000 (57%)] Loss: -221232.781250\n",
      "Train Epoch: 118 [32384/54000 (60%)] Loss: -200615.843750\n",
      "Train Epoch: 118 [33792/54000 (63%)] Loss: -210333.500000\n",
      "Train Epoch: 118 [35200/54000 (65%)] Loss: -204105.250000\n",
      "Train Epoch: 118 [36608/54000 (68%)] Loss: -214910.500000\n",
      "Train Epoch: 118 [38016/54000 (70%)] Loss: -239904.234375\n",
      "Train Epoch: 118 [39424/54000 (73%)] Loss: -224257.906250\n",
      "Train Epoch: 118 [40832/54000 (76%)] Loss: -222458.531250\n",
      "Train Epoch: 118 [42240/54000 (78%)] Loss: -219070.312500\n",
      "Train Epoch: 118 [43648/54000 (81%)] Loss: -239702.250000\n",
      "Train Epoch: 118 [45056/54000 (83%)] Loss: -214664.906250\n",
      "Train Epoch: 118 [46464/54000 (86%)] Loss: -213470.937500\n",
      "Train Epoch: 118 [47872/54000 (89%)] Loss: -207359.937500\n",
      "Train Epoch: 118 [49280/54000 (91%)] Loss: -216429.109375\n",
      "Train Epoch: 118 [50688/54000 (94%)] Loss: -221938.734375\n",
      "Train Epoch: 118 [52096/54000 (96%)] Loss: -211005.546875\n",
      "    epoch          : 118\n",
      "    loss           : -217222.97996411484\n",
      "    val_loss       : -225268.1710056212\n",
      "Train Epoch: 119 [0/54000 (0%)] Loss: -239607.234375\n",
      "Train Epoch: 119 [1408/54000 (3%)] Loss: -212007.734375\n",
      "Train Epoch: 119 [2816/54000 (5%)] Loss: -207959.531250\n",
      "Train Epoch: 119 [4224/54000 (8%)] Loss: -212924.468750\n",
      "Train Epoch: 119 [5632/54000 (10%)] Loss: -205735.890625\n",
      "Train Epoch: 119 [7040/54000 (13%)] Loss: -205690.687500\n",
      "Train Epoch: 119 [8448/54000 (16%)] Loss: -208188.093750\n",
      "Train Epoch: 119 [9856/54000 (18%)] Loss: -209909.781250\n",
      "Train Epoch: 119 [11264/54000 (21%)] Loss: -217941.578125\n",
      "Train Epoch: 119 [12672/54000 (23%)] Loss: -215748.531250\n",
      "Train Epoch: 119 [14080/54000 (26%)] Loss: -211411.250000\n",
      "Train Epoch: 119 [15488/54000 (29%)] Loss: -217514.015625\n",
      "Train Epoch: 119 [16896/54000 (31%)] Loss: -222735.937500\n",
      "Train Epoch: 119 [18304/54000 (34%)] Loss: -216965.843750\n",
      "Train Epoch: 119 [19712/54000 (37%)] Loss: -209171.281250\n",
      "Train Epoch: 119 [21120/54000 (39%)] Loss: -207628.578125\n",
      "Train Epoch: 119 [22528/54000 (42%)] Loss: -212341.312500\n",
      "Train Epoch: 119 [23936/54000 (44%)] Loss: -212027.937500\n",
      "Train Epoch: 119 [25344/54000 (47%)] Loss: -217196.937500\n",
      "Train Epoch: 119 [26752/54000 (50%)] Loss: -215531.453125\n",
      "Train Epoch: 119 [28160/54000 (52%)] Loss: -214316.281250\n",
      "Train Epoch: 119 [29568/54000 (55%)] Loss: -213758.750000\n",
      "Train Epoch: 119 [30976/54000 (57%)] Loss: -218989.468750\n",
      "Train Epoch: 119 [32384/54000 (60%)] Loss: -224152.781250\n",
      "Train Epoch: 119 [33792/54000 (63%)] Loss: -212635.609375\n",
      "Train Epoch: 119 [35200/54000 (65%)] Loss: -226567.312500\n",
      "Train Epoch: 119 [36608/54000 (68%)] Loss: -215240.531250\n",
      "Train Epoch: 119 [38016/54000 (70%)] Loss: -206931.359375\n",
      "Train Epoch: 119 [39424/54000 (73%)] Loss: -237033.265625\n",
      "Train Epoch: 119 [40832/54000 (76%)] Loss: -208043.125000\n",
      "Train Epoch: 119 [42240/54000 (78%)] Loss: -214454.875000\n",
      "Train Epoch: 119 [43648/54000 (81%)] Loss: -210832.843750\n",
      "Train Epoch: 119 [45056/54000 (83%)] Loss: -219742.250000\n",
      "Train Epoch: 119 [46464/54000 (86%)] Loss: -211439.609375\n",
      "Train Epoch: 119 [47872/54000 (89%)] Loss: -214337.687500\n",
      "Train Epoch: 119 [49280/54000 (91%)] Loss: -213615.031250\n",
      "Train Epoch: 119 [50688/54000 (94%)] Loss: -238559.187500\n",
      "Train Epoch: 119 [52096/54000 (96%)] Loss: -220488.093750\n",
      "    epoch          : 119\n",
      "    loss           : -217182.7048444976\n",
      "    val_loss       : -225570.61592511434\n",
      "Train Epoch: 120 [0/54000 (0%)] Loss: -216557.187500\n",
      "Train Epoch: 120 [1408/54000 (3%)] Loss: -239681.687500\n",
      "Train Epoch: 120 [2816/54000 (5%)] Loss: -212449.359375\n",
      "Train Epoch: 120 [4224/54000 (8%)] Loss: -213196.093750\n",
      "Train Epoch: 120 [5632/54000 (10%)] Loss: -215749.953125\n",
      "Train Epoch: 120 [7040/54000 (13%)] Loss: -219247.250000\n",
      "Train Epoch: 120 [8448/54000 (16%)] Loss: -224643.359375\n",
      "Train Epoch: 120 [9856/54000 (18%)] Loss: -225994.640625\n",
      "Train Epoch: 120 [11264/54000 (21%)] Loss: -210248.437500\n",
      "Train Epoch: 120 [12672/54000 (23%)] Loss: -208448.312500\n",
      "Train Epoch: 120 [14080/54000 (26%)] Loss: -211666.062500\n",
      "Train Epoch: 120 [15488/54000 (29%)] Loss: -240084.578125\n",
      "Train Epoch: 120 [16896/54000 (31%)] Loss: -219097.250000\n",
      "Train Epoch: 120 [18304/54000 (34%)] Loss: -217030.343750\n",
      "Train Epoch: 120 [19712/54000 (37%)] Loss: -223961.593750\n",
      "Train Epoch: 120 [21120/54000 (39%)] Loss: -225565.531250\n",
      "Train Epoch: 120 [22528/54000 (42%)] Loss: -226489.625000\n",
      "Train Epoch: 120 [23936/54000 (44%)] Loss: -214410.250000\n",
      "Train Epoch: 120 [25344/54000 (47%)] Loss: -225845.062500\n",
      "Train Epoch: 120 [26752/54000 (50%)] Loss: -212523.500000\n",
      "Train Epoch: 120 [28160/54000 (52%)] Loss: -213554.359375\n",
      "Train Epoch: 120 [29568/54000 (55%)] Loss: -221828.375000\n",
      "Train Epoch: 120 [30976/54000 (57%)] Loss: -237613.890625\n",
      "Train Epoch: 120 [32384/54000 (60%)] Loss: -219571.250000\n",
      "Train Epoch: 120 [33792/54000 (63%)] Loss: -218202.093750\n",
      "Train Epoch: 120 [35200/54000 (65%)] Loss: -212113.578125\n",
      "Train Epoch: 120 [36608/54000 (68%)] Loss: -206500.437500\n",
      "Train Epoch: 120 [38016/54000 (70%)] Loss: -210580.515625\n",
      "Train Epoch: 120 [39424/54000 (73%)] Loss: -219102.093750\n",
      "Train Epoch: 120 [40832/54000 (76%)] Loss: -223995.390625\n",
      "Train Epoch: 120 [42240/54000 (78%)] Loss: -206144.703125\n",
      "Train Epoch: 120 [43648/54000 (81%)] Loss: -214419.015625\n",
      "Train Epoch: 120 [45056/54000 (83%)] Loss: -215321.437500\n",
      "Train Epoch: 120 [46464/54000 (86%)] Loss: -221398.953125\n",
      "Train Epoch: 120 [47872/54000 (89%)] Loss: -220447.093750\n",
      "Train Epoch: 120 [49280/54000 (91%)] Loss: -214710.156250\n",
      "Train Epoch: 120 [50688/54000 (94%)] Loss: -203739.953125\n",
      "Train Epoch: 120 [52096/54000 (96%)] Loss: -207261.203125\n",
      "    epoch          : 120\n",
      "    loss           : -217393.4413875598\n",
      "    val_loss       : -225488.93214081554\n",
      "Train Epoch: 121 [0/54000 (0%)] Loss: -239688.937500\n",
      "Train Epoch: 121 [1408/54000 (3%)] Loss: -217687.281250\n",
      "Train Epoch: 121 [2816/54000 (5%)] Loss: -218747.375000\n",
      "Train Epoch: 121 [4224/54000 (8%)] Loss: -214141.062500\n",
      "Train Epoch: 121 [5632/54000 (10%)] Loss: -211445.046875\n",
      "Train Epoch: 121 [7040/54000 (13%)] Loss: -217849.421875\n",
      "Train Epoch: 121 [8448/54000 (16%)] Loss: -218011.296875\n",
      "Train Epoch: 121 [9856/54000 (18%)] Loss: -208237.406250\n",
      "Train Epoch: 121 [11264/54000 (21%)] Loss: -210657.890625\n",
      "Train Epoch: 121 [12672/54000 (23%)] Loss: -207091.765625\n",
      "Train Epoch: 121 [14080/54000 (26%)] Loss: -208578.328125\n",
      "Train Epoch: 121 [15488/54000 (29%)] Loss: -204864.546875\n",
      "Train Epoch: 121 [16896/54000 (31%)] Loss: -213941.265625\n",
      "Train Epoch: 121 [18304/54000 (34%)] Loss: -222963.406250\n",
      "Train Epoch: 121 [19712/54000 (37%)] Loss: -211721.343750\n",
      "Train Epoch: 121 [21120/54000 (39%)] Loss: -206884.640625\n",
      "Train Epoch: 121 [22528/54000 (42%)] Loss: -225455.609375\n",
      "Train Epoch: 121 [23936/54000 (44%)] Loss: -209698.437500\n",
      "Train Epoch: 121 [25344/54000 (47%)] Loss: -214045.859375\n",
      "Train Epoch: 121 [26752/54000 (50%)] Loss: -213278.187500\n",
      "Train Epoch: 121 [28160/54000 (52%)] Loss: -237849.093750\n",
      "Train Epoch: 121 [29568/54000 (55%)] Loss: -205973.984375\n",
      "Train Epoch: 121 [30976/54000 (57%)] Loss: -219287.453125\n",
      "Train Epoch: 121 [32384/54000 (60%)] Loss: -219967.062500\n",
      "Train Epoch: 121 [33792/54000 (63%)] Loss: -219374.015625\n",
      "Train Epoch: 121 [35200/54000 (65%)] Loss: -216033.625000\n",
      "Train Epoch: 121 [36608/54000 (68%)] Loss: -225366.968750\n",
      "Train Epoch: 121 [38016/54000 (70%)] Loss: -223156.156250\n",
      "Train Epoch: 121 [39424/54000 (73%)] Loss: -240315.156250\n",
      "Train Epoch: 121 [40832/54000 (76%)] Loss: -202664.171875\n",
      "Train Epoch: 121 [42240/54000 (78%)] Loss: -211334.078125\n",
      "Train Epoch: 121 [43648/54000 (81%)] Loss: -207591.203125\n",
      "Train Epoch: 121 [45056/54000 (83%)] Loss: -238431.921875\n",
      "Train Epoch: 121 [46464/54000 (86%)] Loss: -214703.375000\n",
      "Train Epoch: 121 [47872/54000 (89%)] Loss: -212171.125000\n",
      "Train Epoch: 121 [49280/54000 (91%)] Loss: -238319.062500\n",
      "Train Epoch: 121 [50688/54000 (94%)] Loss: -221565.765625\n",
      "Train Epoch: 121 [52096/54000 (96%)] Loss: -214617.953125\n",
      "    epoch          : 121\n",
      "    loss           : -217429.31870514355\n",
      "    val_loss       : -225872.94113233613\n",
      "Train Epoch: 122 [0/54000 (0%)] Loss: -238405.750000\n",
      "Train Epoch: 122 [1408/54000 (3%)] Loss: -212169.765625\n",
      "Train Epoch: 122 [2816/54000 (5%)] Loss: -207795.875000\n",
      "Train Epoch: 122 [4224/54000 (8%)] Loss: -211639.609375\n",
      "Train Epoch: 122 [5632/54000 (10%)] Loss: -219420.500000\n",
      "Train Epoch: 122 [7040/54000 (13%)] Loss: -213291.593750\n",
      "Train Epoch: 122 [8448/54000 (16%)] Loss: -224524.093750\n",
      "Train Epoch: 122 [9856/54000 (18%)] Loss: -216496.031250\n",
      "Train Epoch: 122 [11264/54000 (21%)] Loss: -217272.000000\n",
      "Train Epoch: 122 [12672/54000 (23%)] Loss: -216497.015625\n",
      "Train Epoch: 122 [14080/54000 (26%)] Loss: -213513.765625\n",
      "Train Epoch: 122 [15488/54000 (29%)] Loss: -211802.562500\n",
      "Train Epoch: 122 [16896/54000 (31%)] Loss: -224968.750000\n",
      "Train Epoch: 122 [18304/54000 (34%)] Loss: -220853.843750\n",
      "Train Epoch: 122 [19712/54000 (37%)] Loss: -207687.593750\n",
      "Train Epoch: 122 [21120/54000 (39%)] Loss: -207391.437500\n",
      "Train Epoch: 122 [22528/54000 (42%)] Loss: -239972.000000\n",
      "Train Epoch: 122 [23936/54000 (44%)] Loss: -198333.781250\n",
      "Train Epoch: 122 [25344/54000 (47%)] Loss: -224061.437500\n",
      "Train Epoch: 122 [26752/54000 (50%)] Loss: -212443.812500\n",
      "Train Epoch: 122 [28160/54000 (52%)] Loss: -213127.390625\n",
      "Train Epoch: 122 [29568/54000 (55%)] Loss: -213489.968750\n",
      "Train Epoch: 122 [30976/54000 (57%)] Loss: -241086.031250\n",
      "Train Epoch: 122 [32384/54000 (60%)] Loss: -237446.031250\n",
      "Train Epoch: 122 [33792/54000 (63%)] Loss: -220588.593750\n",
      "Train Epoch: 122 [35200/54000 (65%)] Loss: -220206.937500\n",
      "Train Epoch: 122 [36608/54000 (68%)] Loss: -238023.250000\n",
      "Train Epoch: 122 [38016/54000 (70%)] Loss: -218841.687500\n",
      "Train Epoch: 122 [39424/54000 (73%)] Loss: -224043.984375\n",
      "Train Epoch: 122 [40832/54000 (76%)] Loss: -223109.390625\n",
      "Train Epoch: 122 [42240/54000 (78%)] Loss: -220894.203125\n",
      "Train Epoch: 122 [43648/54000 (81%)] Loss: -226342.500000\n",
      "Train Epoch: 122 [45056/54000 (83%)] Loss: -214974.812500\n",
      "Train Epoch: 122 [46464/54000 (86%)] Loss: -214284.937500\n",
      "Train Epoch: 122 [47872/54000 (89%)] Loss: -219554.921875\n",
      "Train Epoch: 122 [49280/54000 (91%)] Loss: -213315.687500\n",
      "Train Epoch: 122 [50688/54000 (94%)] Loss: -208968.015625\n",
      "Train Epoch: 122 [52096/54000 (96%)] Loss: -206454.640625\n",
      "    epoch          : 122\n",
      "    loss           : -217498.565527811\n",
      "    val_loss       : -225993.22919445502\n",
      "Train Epoch: 123 [0/54000 (0%)] Loss: -223138.343750\n",
      "Train Epoch: 123 [1408/54000 (3%)] Loss: -223590.281250\n",
      "Train Epoch: 123 [2816/54000 (5%)] Loss: -227139.000000\n",
      "Train Epoch: 123 [4224/54000 (8%)] Loss: -201154.046875\n",
      "Train Epoch: 123 [5632/54000 (10%)] Loss: -212655.531250\n",
      "Train Epoch: 123 [7040/54000 (13%)] Loss: -202633.765625\n",
      "Train Epoch: 123 [8448/54000 (16%)] Loss: -213539.125000\n",
      "Train Epoch: 123 [9856/54000 (18%)] Loss: -207525.296875\n",
      "Train Epoch: 123 [11264/54000 (21%)] Loss: -215085.812500\n",
      "Train Epoch: 123 [12672/54000 (23%)] Loss: -214589.312500\n",
      "Train Epoch: 123 [14080/54000 (26%)] Loss: -212186.046875\n",
      "Train Epoch: 123 [15488/54000 (29%)] Loss: -215659.812500\n",
      "Train Epoch: 123 [16896/54000 (31%)] Loss: -212084.156250\n",
      "Train Epoch: 123 [18304/54000 (34%)] Loss: -210013.562500\n",
      "Train Epoch: 123 [19712/54000 (37%)] Loss: -219463.812500\n",
      "Train Epoch: 123 [21120/54000 (39%)] Loss: -219089.671875\n",
      "Train Epoch: 123 [22528/54000 (42%)] Loss: -226821.437500\n",
      "Train Epoch: 123 [23936/54000 (44%)] Loss: -226586.875000\n",
      "Train Epoch: 123 [25344/54000 (47%)] Loss: -225200.281250\n",
      "Train Epoch: 123 [26752/54000 (50%)] Loss: -215590.406250\n",
      "Train Epoch: 123 [28160/54000 (52%)] Loss: -215950.921875\n",
      "Train Epoch: 123 [29568/54000 (55%)] Loss: -218114.703125\n",
      "Train Epoch: 123 [30976/54000 (57%)] Loss: -237340.421875\n",
      "Train Epoch: 123 [32384/54000 (60%)] Loss: -239252.890625\n",
      "Train Epoch: 123 [33792/54000 (63%)] Loss: -213742.468750\n",
      "Train Epoch: 123 [35200/54000 (65%)] Loss: -226434.968750\n",
      "Train Epoch: 123 [36608/54000 (68%)] Loss: -205568.484375\n",
      "Train Epoch: 123 [38016/54000 (70%)] Loss: -210448.906250\n",
      "Train Epoch: 123 [39424/54000 (73%)] Loss: -224276.125000\n",
      "Train Epoch: 123 [40832/54000 (76%)] Loss: -216079.250000\n",
      "Train Epoch: 123 [42240/54000 (78%)] Loss: -218635.359375\n",
      "Train Epoch: 123 [43648/54000 (81%)] Loss: -213039.375000\n",
      "Train Epoch: 123 [45056/54000 (83%)] Loss: -214619.484375\n",
      "Train Epoch: 123 [46464/54000 (86%)] Loss: -221895.593750\n",
      "Train Epoch: 123 [47872/54000 (89%)] Loss: -220323.750000\n",
      "Train Epoch: 123 [49280/54000 (91%)] Loss: -222936.218750\n",
      "Train Epoch: 123 [50688/54000 (94%)] Loss: -219653.375000\n",
      "Train Epoch: 123 [52096/54000 (96%)] Loss: -220648.687500\n",
      "    epoch          : 123\n",
      "    loss           : -217664.25702751195\n",
      "    val_loss       : -225897.0371451029\n",
      "Train Epoch: 124 [0/54000 (0%)] Loss: -238187.500000\n",
      "Train Epoch: 124 [1408/54000 (3%)] Loss: -223585.750000\n",
      "Train Epoch: 124 [2816/54000 (5%)] Loss: -215492.531250\n",
      "Train Epoch: 124 [4224/54000 (8%)] Loss: -206033.390625\n",
      "Train Epoch: 124 [5632/54000 (10%)] Loss: -224900.218750\n",
      "Train Epoch: 124 [7040/54000 (13%)] Loss: -239048.656250\n",
      "Train Epoch: 124 [8448/54000 (16%)] Loss: -214789.250000\n",
      "Train Epoch: 124 [9856/54000 (18%)] Loss: -207246.156250\n",
      "Train Epoch: 124 [11264/54000 (21%)] Loss: -213385.593750\n",
      "Train Epoch: 124 [12672/54000 (23%)] Loss: -238083.750000\n",
      "Train Epoch: 124 [14080/54000 (26%)] Loss: -237749.750000\n",
      "Train Epoch: 124 [15488/54000 (29%)] Loss: -218652.187500\n",
      "Train Epoch: 124 [16896/54000 (31%)] Loss: -215947.390625\n",
      "Train Epoch: 124 [18304/54000 (34%)] Loss: -214547.640625\n",
      "Train Epoch: 124 [19712/54000 (37%)] Loss: -220679.312500\n",
      "Train Epoch: 124 [21120/54000 (39%)] Loss: -212609.968750\n",
      "Train Epoch: 124 [22528/54000 (42%)] Loss: -224874.343750\n",
      "Train Epoch: 124 [23936/54000 (44%)] Loss: -205801.109375\n",
      "Train Epoch: 124 [25344/54000 (47%)] Loss: -223490.906250\n",
      "Train Epoch: 124 [26752/54000 (50%)] Loss: -216824.656250\n",
      "Train Epoch: 124 [28160/54000 (52%)] Loss: -216660.593750\n",
      "Train Epoch: 124 [29568/54000 (55%)] Loss: -218106.765625\n",
      "Train Epoch: 124 [30976/54000 (57%)] Loss: -223147.406250\n",
      "Train Epoch: 124 [32384/54000 (60%)] Loss: -216657.609375\n",
      "Train Epoch: 124 [33792/54000 (63%)] Loss: -215969.828125\n",
      "Train Epoch: 124 [35200/54000 (65%)] Loss: -227391.890625\n",
      "Train Epoch: 124 [36608/54000 (68%)] Loss: -226531.125000\n",
      "Train Epoch: 124 [38016/54000 (70%)] Loss: -206743.828125\n",
      "Train Epoch: 124 [39424/54000 (73%)] Loss: -208863.390625\n",
      "Train Epoch: 124 [40832/54000 (76%)] Loss: -207981.765625\n",
      "Train Epoch: 124 [42240/54000 (78%)] Loss: -221890.125000\n",
      "Train Epoch: 124 [43648/54000 (81%)] Loss: -218715.546875\n",
      "Train Epoch: 124 [45056/54000 (83%)] Loss: -217948.640625\n",
      "Train Epoch: 124 [46464/54000 (86%)] Loss: -214938.656250\n",
      "Train Epoch: 124 [47872/54000 (89%)] Loss: -207992.000000\n",
      "Train Epoch: 124 [49280/54000 (91%)] Loss: -212652.812500\n",
      "Train Epoch: 124 [50688/54000 (94%)] Loss: -225599.718750\n",
      "Train Epoch: 124 [52096/54000 (96%)] Loss: -217485.671875\n",
      "    epoch          : 124\n",
      "    loss           : -217862.1165520335\n",
      "    val_loss       : -225565.73106421495\n",
      "Train Epoch: 125 [0/54000 (0%)] Loss: -238374.859375\n",
      "Train Epoch: 125 [1408/54000 (3%)] Loss: -219494.031250\n",
      "Train Epoch: 125 [2816/54000 (5%)] Loss: -215923.781250\n",
      "Train Epoch: 125 [4224/54000 (8%)] Loss: -206053.546875\n",
      "Train Epoch: 125 [5632/54000 (10%)] Loss: -202684.062500\n",
      "Train Epoch: 125 [7040/54000 (13%)] Loss: -225946.843750\n",
      "Train Epoch: 125 [8448/54000 (16%)] Loss: -224248.296875\n",
      "Train Epoch: 125 [9856/54000 (18%)] Loss: -212129.250000\n",
      "Train Epoch: 125 [11264/54000 (21%)] Loss: -213823.953125\n",
      "Train Epoch: 125 [12672/54000 (23%)] Loss: -203968.734375\n",
      "Train Epoch: 125 [14080/54000 (26%)] Loss: -214041.078125\n",
      "Train Epoch: 125 [15488/54000 (29%)] Loss: -219286.609375\n",
      "Train Epoch: 125 [16896/54000 (31%)] Loss: -217842.468750\n",
      "Train Epoch: 125 [18304/54000 (34%)] Loss: -214603.625000\n",
      "Train Epoch: 125 [19712/54000 (37%)] Loss: -241887.312500\n",
      "Train Epoch: 125 [21120/54000 (39%)] Loss: -206018.015625\n",
      "Train Epoch: 125 [22528/54000 (42%)] Loss: -222871.718750\n",
      "Train Epoch: 125 [23936/54000 (44%)] Loss: -206325.671875\n",
      "Train Epoch: 125 [25344/54000 (47%)] Loss: -213094.453125\n",
      "Train Epoch: 125 [26752/54000 (50%)] Loss: -216552.218750\n",
      "Train Epoch: 125 [28160/54000 (52%)] Loss: -210510.562500\n",
      "Train Epoch: 125 [29568/54000 (55%)] Loss: -220137.781250\n",
      "Train Epoch: 125 [30976/54000 (57%)] Loss: -217766.078125\n",
      "Train Epoch: 125 [32384/54000 (60%)] Loss: -239312.500000\n",
      "Train Epoch: 125 [33792/54000 (63%)] Loss: -212813.000000\n",
      "Train Epoch: 125 [35200/54000 (65%)] Loss: -224688.218750\n",
      "Train Epoch: 125 [36608/54000 (68%)] Loss: -206650.031250\n",
      "Train Epoch: 125 [38016/54000 (70%)] Loss: -214360.546875\n",
      "Train Epoch: 125 [39424/54000 (73%)] Loss: -210629.390625\n",
      "Train Epoch: 125 [40832/54000 (76%)] Loss: -210448.031250\n",
      "Train Epoch: 125 [42240/54000 (78%)] Loss: -220334.156250\n",
      "Train Epoch: 125 [43648/54000 (81%)] Loss: -213062.375000\n",
      "Train Epoch: 125 [45056/54000 (83%)] Loss: -238627.578125\n",
      "Train Epoch: 125 [46464/54000 (86%)] Loss: -241420.281250\n",
      "Train Epoch: 125 [47872/54000 (89%)] Loss: -213345.625000\n",
      "Train Epoch: 125 [49280/54000 (91%)] Loss: -211281.156250\n",
      "Train Epoch: 125 [50688/54000 (94%)] Loss: -220241.750000\n",
      "Train Epoch: 125 [52096/54000 (96%)] Loss: -211981.031250\n",
      "    epoch          : 125\n",
      "    loss           : -217875.90610047846\n",
      "    val_loss       : -225551.7521555831\n",
      "Train Epoch: 126 [0/54000 (0%)] Loss: -221001.765625\n",
      "Train Epoch: 126 [1408/54000 (3%)] Loss: -238739.234375\n",
      "Train Epoch: 126 [2816/54000 (5%)] Loss: -208864.421875\n",
      "Train Epoch: 126 [4224/54000 (8%)] Loss: -220830.093750\n",
      "Train Epoch: 126 [5632/54000 (10%)] Loss: -217156.187500\n",
      "Train Epoch: 126 [7040/54000 (13%)] Loss: -213537.734375\n",
      "Train Epoch: 126 [8448/54000 (16%)] Loss: -215021.593750\n",
      "Train Epoch: 126 [9856/54000 (18%)] Loss: -222345.421875\n",
      "Train Epoch: 126 [11264/54000 (21%)] Loss: -214511.156250\n",
      "Train Epoch: 126 [12672/54000 (23%)] Loss: -221251.609375\n",
      "Train Epoch: 126 [14080/54000 (26%)] Loss: -220355.437500\n",
      "Train Epoch: 126 [15488/54000 (29%)] Loss: -240879.687500\n",
      "Train Epoch: 126 [16896/54000 (31%)] Loss: -213184.093750\n",
      "Train Epoch: 126 [18304/54000 (34%)] Loss: -208666.406250\n",
      "Train Epoch: 126 [19712/54000 (37%)] Loss: -221998.625000\n",
      "Train Epoch: 126 [21120/54000 (39%)] Loss: -240887.515625\n",
      "Train Epoch: 126 [22528/54000 (42%)] Loss: -212378.531250\n",
      "Train Epoch: 126 [23936/54000 (44%)] Loss: -223742.296875\n",
      "Train Epoch: 126 [25344/54000 (47%)] Loss: -211941.718750\n",
      "Train Epoch: 126 [26752/54000 (50%)] Loss: -206279.500000\n",
      "Train Epoch: 126 [28160/54000 (52%)] Loss: -209696.937500\n",
      "Train Epoch: 126 [29568/54000 (55%)] Loss: -210570.562500\n",
      "Train Epoch: 126 [30976/54000 (57%)] Loss: -218282.750000\n",
      "Train Epoch: 126 [32384/54000 (60%)] Loss: -212886.671875\n",
      "Train Epoch: 126 [33792/54000 (63%)] Loss: -213860.718750\n",
      "Train Epoch: 126 [35200/54000 (65%)] Loss: -205735.984375\n",
      "Train Epoch: 126 [36608/54000 (68%)] Loss: -213771.468750\n",
      "Train Epoch: 126 [38016/54000 (70%)] Loss: -208655.562500\n",
      "Train Epoch: 126 [39424/54000 (73%)] Loss: -215051.671875\n",
      "Train Epoch: 126 [40832/54000 (76%)] Loss: -219037.656250\n",
      "Train Epoch: 126 [42240/54000 (78%)] Loss: -237236.921875\n",
      "Train Epoch: 126 [43648/54000 (81%)] Loss: -224009.281250\n",
      "Train Epoch: 126 [45056/54000 (83%)] Loss: -212556.593750\n",
      "Train Epoch: 126 [46464/54000 (86%)] Loss: -208848.781250\n",
      "Train Epoch: 126 [47872/54000 (89%)] Loss: -205501.828125\n",
      "Train Epoch: 126 [49280/54000 (91%)] Loss: -207817.984375\n",
      "Train Epoch: 126 [50688/54000 (94%)] Loss: -215735.437500\n",
      "Train Epoch: 126 [52096/54000 (96%)] Loss: -207946.578125\n",
      "    epoch          : 126\n",
      "    loss           : -218027.33459180623\n",
      "    val_loss       : -226307.5330244855\n",
      "Train Epoch: 127 [0/54000 (0%)] Loss: -238138.375000\n",
      "Train Epoch: 127 [1408/54000 (3%)] Loss: -223036.781250\n",
      "Train Epoch: 127 [2816/54000 (5%)] Loss: -214428.531250\n",
      "Train Epoch: 127 [4224/54000 (8%)] Loss: -208237.437500\n",
      "Train Epoch: 127 [5632/54000 (10%)] Loss: -207979.781250\n",
      "Train Epoch: 127 [7040/54000 (13%)] Loss: -212613.046875\n",
      "Train Epoch: 127 [8448/54000 (16%)] Loss: -239059.218750\n",
      "Train Epoch: 127 [9856/54000 (18%)] Loss: -214200.718750\n",
      "Train Epoch: 127 [11264/54000 (21%)] Loss: -213045.421875\n",
      "Train Epoch: 127 [12672/54000 (23%)] Loss: -219423.156250\n",
      "Train Epoch: 127 [14080/54000 (26%)] Loss: -210352.468750\n",
      "Train Epoch: 127 [15488/54000 (29%)] Loss: -241470.500000\n",
      "Train Epoch: 127 [16896/54000 (31%)] Loss: -214230.687500\n",
      "Train Epoch: 127 [18304/54000 (34%)] Loss: -206210.140625\n",
      "Train Epoch: 127 [19712/54000 (37%)] Loss: -226827.062500\n",
      "Train Epoch: 127 [21120/54000 (39%)] Loss: -239242.906250\n",
      "Train Epoch: 127 [22528/54000 (42%)] Loss: -214622.234375\n",
      "Train Epoch: 127 [23936/54000 (44%)] Loss: -225107.968750\n",
      "Train Epoch: 127 [25344/54000 (47%)] Loss: -220957.468750\n",
      "Train Epoch: 127 [26752/54000 (50%)] Loss: -213510.546875\n",
      "Train Epoch: 127 [28160/54000 (52%)] Loss: -218046.937500\n",
      "Train Epoch: 127 [29568/54000 (55%)] Loss: -202813.187500\n",
      "Train Epoch: 127 [30976/54000 (57%)] Loss: -219216.781250\n",
      "Train Epoch: 127 [32384/54000 (60%)] Loss: -213659.125000\n",
      "Train Epoch: 127 [33792/54000 (63%)] Loss: -238320.515625\n",
      "Train Epoch: 127 [35200/54000 (65%)] Loss: -226977.703125\n",
      "Train Epoch: 127 [36608/54000 (68%)] Loss: -223972.921875\n",
      "Train Epoch: 127 [38016/54000 (70%)] Loss: -222900.218750\n",
      "Train Epoch: 127 [39424/54000 (73%)] Loss: -214012.343750\n",
      "Train Epoch: 127 [40832/54000 (76%)] Loss: -220740.187500\n",
      "Train Epoch: 127 [42240/54000 (78%)] Loss: -208667.593750\n",
      "Train Epoch: 127 [43648/54000 (81%)] Loss: -221831.953125\n",
      "Train Epoch: 127 [45056/54000 (83%)] Loss: -239383.015625\n",
      "Train Epoch: 127 [46464/54000 (86%)] Loss: -214161.171875\n",
      "Train Epoch: 127 [47872/54000 (89%)] Loss: -213032.562500\n",
      "Train Epoch: 127 [49280/54000 (91%)] Loss: -204365.125000\n",
      "Train Epoch: 127 [50688/54000 (94%)] Loss: -240992.218750\n",
      "Train Epoch: 127 [52096/54000 (96%)] Loss: -222686.656250\n",
      "    epoch          : 127\n",
      "    loss           : -218070.97839413874\n",
      "    val_loss       : -226184.15491615853\n",
      "Train Epoch: 128 [0/54000 (0%)] Loss: -239320.421875\n",
      "Train Epoch: 128 [1408/54000 (3%)] Loss: -241853.828125\n",
      "Train Epoch: 128 [2816/54000 (5%)] Loss: -221861.812500\n",
      "Train Epoch: 128 [4224/54000 (8%)] Loss: -221106.968750\n",
      "Train Epoch: 128 [5632/54000 (10%)] Loss: -222458.250000\n",
      "Train Epoch: 128 [7040/54000 (13%)] Loss: -209908.390625\n",
      "Train Epoch: 128 [8448/54000 (16%)] Loss: -202292.656250\n",
      "Train Epoch: 128 [9856/54000 (18%)] Loss: -214160.484375\n",
      "Train Epoch: 128 [11264/54000 (21%)] Loss: -211979.296875\n",
      "Train Epoch: 128 [12672/54000 (23%)] Loss: -213444.609375\n",
      "Train Epoch: 128 [14080/54000 (26%)] Loss: -211051.953125\n",
      "Train Epoch: 128 [15488/54000 (29%)] Loss: -214344.234375\n",
      "Train Epoch: 128 [16896/54000 (31%)] Loss: -213508.718750\n",
      "Train Epoch: 128 [18304/54000 (34%)] Loss: -240739.625000\n",
      "Train Epoch: 128 [19712/54000 (37%)] Loss: -223226.640625\n",
      "Train Epoch: 128 [21120/54000 (39%)] Loss: -215499.000000\n",
      "Train Epoch: 128 [22528/54000 (42%)] Loss: -225391.812500\n",
      "Train Epoch: 128 [23936/54000 (44%)] Loss: -211398.093750\n",
      "Train Epoch: 128 [25344/54000 (47%)] Loss: -208683.859375\n",
      "Train Epoch: 128 [26752/54000 (50%)] Loss: -205850.671875\n",
      "Train Epoch: 128 [28160/54000 (52%)] Loss: -209062.796875\n",
      "Train Epoch: 128 [29568/54000 (55%)] Loss: -216679.078125\n",
      "Train Epoch: 128 [30976/54000 (57%)] Loss: -206630.640625\n",
      "Train Epoch: 128 [32384/54000 (60%)] Loss: -221890.875000\n",
      "Train Epoch: 128 [33792/54000 (63%)] Loss: -211654.562500\n",
      "Train Epoch: 128 [35200/54000 (65%)] Loss: -208550.531250\n",
      "Train Epoch: 128 [36608/54000 (68%)] Loss: -215395.125000\n",
      "Train Epoch: 128 [38016/54000 (70%)] Loss: -238140.000000\n",
      "Train Epoch: 128 [39424/54000 (73%)] Loss: -224520.343750\n",
      "Train Epoch: 128 [40832/54000 (76%)] Loss: -207661.484375\n",
      "Train Epoch: 128 [42240/54000 (78%)] Loss: -220116.656250\n",
      "Train Epoch: 128 [43648/54000 (81%)] Loss: -241585.812500\n",
      "Train Epoch: 128 [45056/54000 (83%)] Loss: -213311.812500\n",
      "Train Epoch: 128 [46464/54000 (86%)] Loss: -211345.953125\n",
      "Train Epoch: 128 [47872/54000 (89%)] Loss: -215697.828125\n",
      "Train Epoch: 128 [49280/54000 (91%)] Loss: -210342.718750\n",
      "Train Epoch: 128 [50688/54000 (94%)] Loss: -239011.625000\n",
      "Train Epoch: 128 [52096/54000 (96%)] Loss: -212054.343750\n",
      "    epoch          : 128\n",
      "    loss           : -218149.44310705742\n",
      "    val_loss       : -226135.971620141\n",
      "Train Epoch: 129 [0/54000 (0%)] Loss: -217834.953125\n",
      "Train Epoch: 129 [1408/54000 (3%)] Loss: -213027.890625\n",
      "Train Epoch: 129 [2816/54000 (5%)] Loss: -210003.718750\n",
      "Train Epoch: 129 [4224/54000 (8%)] Loss: -213922.218750\n",
      "Train Epoch: 129 [5632/54000 (10%)] Loss: -220797.484375\n",
      "Train Epoch: 129 [7040/54000 (13%)] Loss: -208742.500000\n",
      "Train Epoch: 129 [8448/54000 (16%)] Loss: -211278.656250\n",
      "Train Epoch: 129 [9856/54000 (18%)] Loss: -225555.296875\n",
      "Train Epoch: 129 [11264/54000 (21%)] Loss: -209005.703125\n",
      "Train Epoch: 129 [12672/54000 (23%)] Loss: -213169.906250\n",
      "Train Epoch: 129 [14080/54000 (26%)] Loss: -220352.750000\n",
      "Train Epoch: 129 [15488/54000 (29%)] Loss: -238343.375000\n",
      "Train Epoch: 129 [16896/54000 (31%)] Loss: -239582.093750\n",
      "Train Epoch: 129 [18304/54000 (34%)] Loss: -218504.890625\n",
      "Train Epoch: 129 [19712/54000 (37%)] Loss: -216585.468750\n",
      "Train Epoch: 129 [21120/54000 (39%)] Loss: -207587.656250\n",
      "Train Epoch: 129 [22528/54000 (42%)] Loss: -220939.453125\n",
      "Train Epoch: 129 [23936/54000 (44%)] Loss: -207301.531250\n",
      "Train Epoch: 129 [25344/54000 (47%)] Loss: -220069.906250\n",
      "Train Epoch: 129 [26752/54000 (50%)] Loss: -223455.453125\n",
      "Train Epoch: 129 [28160/54000 (52%)] Loss: -214834.359375\n",
      "Train Epoch: 129 [29568/54000 (55%)] Loss: -215092.031250\n",
      "Train Epoch: 129 [30976/54000 (57%)] Loss: -212262.953125\n",
      "Train Epoch: 129 [32384/54000 (60%)] Loss: -222194.000000\n",
      "Train Epoch: 129 [33792/54000 (63%)] Loss: -236935.078125\n",
      "Train Epoch: 129 [35200/54000 (65%)] Loss: -203831.515625\n",
      "Train Epoch: 129 [36608/54000 (68%)] Loss: -224922.171875\n",
      "Train Epoch: 129 [38016/54000 (70%)] Loss: -224828.875000\n",
      "Train Epoch: 129 [39424/54000 (73%)] Loss: -239006.062500\n",
      "Train Epoch: 129 [40832/54000 (76%)] Loss: -220413.718750\n",
      "Train Epoch: 129 [42240/54000 (78%)] Loss: -206740.906250\n",
      "Train Epoch: 129 [43648/54000 (81%)] Loss: -210202.765625\n",
      "Train Epoch: 129 [45056/54000 (83%)] Loss: -204274.359375\n",
      "Train Epoch: 129 [46464/54000 (86%)] Loss: -212679.265625\n",
      "Train Epoch: 129 [47872/54000 (89%)] Loss: -240003.437500\n",
      "Train Epoch: 129 [49280/54000 (91%)] Loss: -212465.781250\n",
      "Train Epoch: 129 [50688/54000 (94%)] Loss: -208197.828125\n",
      "Train Epoch: 129 [52096/54000 (96%)] Loss: -236725.625000\n",
      "    epoch          : 129\n",
      "    loss           : -218177.45712470094\n",
      "    val_loss       : -225928.7060546875\n",
      "Train Epoch: 130 [0/54000 (0%)] Loss: -242142.125000\n",
      "Train Epoch: 130 [1408/54000 (3%)] Loss: -211906.406250\n",
      "Train Epoch: 130 [2816/54000 (5%)] Loss: -220678.890625\n",
      "Train Epoch: 130 [4224/54000 (8%)] Loss: -218211.265625\n",
      "Train Epoch: 130 [5632/54000 (10%)] Loss: -219101.031250\n",
      "Train Epoch: 130 [7040/54000 (13%)] Loss: -205769.296875\n",
      "Train Epoch: 130 [8448/54000 (16%)] Loss: -224943.812500\n",
      "Train Epoch: 130 [9856/54000 (18%)] Loss: -215734.468750\n",
      "Train Epoch: 130 [11264/54000 (21%)] Loss: -214962.406250\n",
      "Train Epoch: 130 [12672/54000 (23%)] Loss: -216565.343750\n",
      "Train Epoch: 130 [14080/54000 (26%)] Loss: -238993.265625\n",
      "Train Epoch: 130 [15488/54000 (29%)] Loss: -212534.234375\n",
      "Train Epoch: 130 [16896/54000 (31%)] Loss: -220837.515625\n",
      "Train Epoch: 130 [18304/54000 (34%)] Loss: -216451.468750\n",
      "Train Epoch: 130 [19712/54000 (37%)] Loss: -215441.187500\n",
      "Train Epoch: 130 [21120/54000 (39%)] Loss: -211139.468750\n",
      "Train Epoch: 130 [22528/54000 (42%)] Loss: -209489.359375\n",
      "Train Epoch: 130 [23936/54000 (44%)] Loss: -219163.812500\n",
      "Train Epoch: 130 [25344/54000 (47%)] Loss: -213817.843750\n",
      "Train Epoch: 130 [26752/54000 (50%)] Loss: -215306.796875\n",
      "Train Epoch: 130 [28160/54000 (52%)] Loss: -239129.359375\n",
      "Train Epoch: 130 [29568/54000 (55%)] Loss: -220601.093750\n",
      "Train Epoch: 130 [30976/54000 (57%)] Loss: -204589.312500\n",
      "Train Epoch: 130 [32384/54000 (60%)] Loss: -216211.093750\n",
      "Train Epoch: 130 [33792/54000 (63%)] Loss: -212526.671875\n",
      "Train Epoch: 130 [35200/54000 (65%)] Loss: -210457.343750\n",
      "Train Epoch: 130 [36608/54000 (68%)] Loss: -212298.421875\n",
      "Train Epoch: 130 [38016/54000 (70%)] Loss: -210128.343750\n",
      "Train Epoch: 130 [39424/54000 (73%)] Loss: -209614.250000\n",
      "Train Epoch: 130 [40832/54000 (76%)] Loss: -224016.546875\n",
      "Train Epoch: 130 [42240/54000 (78%)] Loss: -220522.281250\n",
      "Train Epoch: 130 [43648/54000 (81%)] Loss: -216507.875000\n",
      "Train Epoch: 130 [45056/54000 (83%)] Loss: -210681.593750\n",
      "Train Epoch: 130 [46464/54000 (86%)] Loss: -239070.250000\n",
      "Train Epoch: 130 [47872/54000 (89%)] Loss: -215127.125000\n",
      "Train Epoch: 130 [49280/54000 (91%)] Loss: -210751.437500\n",
      "Train Epoch: 130 [50688/54000 (94%)] Loss: -212393.031250\n",
      "Train Epoch: 130 [52096/54000 (96%)] Loss: -239649.062500\n",
      "    epoch          : 130\n",
      "    loss           : -218399.77425986843\n",
      "    val_loss       : -226429.6896555831\n",
      "Train Epoch: 131 [0/54000 (0%)] Loss: -223649.218750\n",
      "Train Epoch: 131 [1408/54000 (3%)] Loss: -221787.031250\n",
      "Train Epoch: 131 [2816/54000 (5%)] Loss: -226212.296875\n",
      "Train Epoch: 131 [4224/54000 (8%)] Loss: -221471.125000\n",
      "Train Epoch: 131 [5632/54000 (10%)] Loss: -219868.859375\n",
      "Train Epoch: 131 [7040/54000 (13%)] Loss: -220303.687500\n",
      "Train Epoch: 131 [8448/54000 (16%)] Loss: -226446.937500\n",
      "Train Epoch: 131 [9856/54000 (18%)] Loss: -239571.390625\n",
      "Train Epoch: 131 [11264/54000 (21%)] Loss: -209935.406250\n",
      "Train Epoch: 131 [12672/54000 (23%)] Loss: -219022.328125\n",
      "Train Epoch: 131 [14080/54000 (26%)] Loss: -238424.515625\n",
      "Train Epoch: 131 [15488/54000 (29%)] Loss: -221401.531250\n",
      "Train Epoch: 131 [16896/54000 (31%)] Loss: -215300.062500\n",
      "Train Epoch: 131 [18304/54000 (34%)] Loss: -212399.531250\n",
      "Train Epoch: 131 [19712/54000 (37%)] Loss: -222079.765625\n",
      "Train Epoch: 131 [21120/54000 (39%)] Loss: -220711.750000\n",
      "Train Epoch: 131 [22528/54000 (42%)] Loss: -225567.187500\n",
      "Train Epoch: 131 [23936/54000 (44%)] Loss: -220275.343750\n",
      "Train Epoch: 131 [25344/54000 (47%)] Loss: -205377.625000\n",
      "Train Epoch: 131 [26752/54000 (50%)] Loss: -217136.500000\n",
      "Train Epoch: 131 [28160/54000 (52%)] Loss: -219311.984375\n",
      "Train Epoch: 131 [29568/54000 (55%)] Loss: -211087.453125\n",
      "Train Epoch: 131 [30976/54000 (57%)] Loss: -208079.453125\n",
      "Train Epoch: 131 [32384/54000 (60%)] Loss: -220657.500000\n",
      "Train Epoch: 131 [33792/54000 (63%)] Loss: -224093.281250\n",
      "Train Epoch: 131 [35200/54000 (65%)] Loss: -240455.734375\n",
      "Train Epoch: 131 [36608/54000 (68%)] Loss: -215446.093750\n",
      "Train Epoch: 131 [38016/54000 (70%)] Loss: -214601.406250\n",
      "Train Epoch: 131 [39424/54000 (73%)] Loss: -218909.281250\n",
      "Train Epoch: 131 [40832/54000 (76%)] Loss: -217448.312500\n",
      "Train Epoch: 131 [42240/54000 (78%)] Loss: -220854.750000\n",
      "Train Epoch: 131 [43648/54000 (81%)] Loss: -220260.546875\n",
      "Train Epoch: 131 [45056/54000 (83%)] Loss: -219275.468750\n",
      "Train Epoch: 131 [46464/54000 (86%)] Loss: -214755.578125\n",
      "Train Epoch: 131 [47872/54000 (89%)] Loss: -213770.390625\n",
      "Train Epoch: 131 [49280/54000 (91%)] Loss: -212787.718750\n",
      "Train Epoch: 131 [50688/54000 (94%)] Loss: -225246.000000\n",
      "Train Epoch: 131 [52096/54000 (96%)] Loss: -204462.984375\n",
      "    epoch          : 131\n",
      "    loss           : -218439.78936154305\n",
      "    val_loss       : -226159.16838557547\n",
      "Train Epoch: 132 [0/54000 (0%)] Loss: -214985.281250\n",
      "Train Epoch: 132 [1408/54000 (3%)] Loss: -217064.906250\n",
      "Train Epoch: 132 [2816/54000 (5%)] Loss: -213715.000000\n",
      "Train Epoch: 132 [4224/54000 (8%)] Loss: -215138.062500\n",
      "Train Epoch: 132 [5632/54000 (10%)] Loss: -204376.203125\n",
      "Train Epoch: 132 [7040/54000 (13%)] Loss: -215747.734375\n",
      "Train Epoch: 132 [8448/54000 (16%)] Loss: -216439.031250\n",
      "Train Epoch: 132 [9856/54000 (18%)] Loss: -206671.125000\n",
      "Train Epoch: 132 [11264/54000 (21%)] Loss: -207619.000000\n",
      "Train Epoch: 132 [12672/54000 (23%)] Loss: -218064.437500\n",
      "Train Epoch: 132 [14080/54000 (26%)] Loss: -212938.375000\n",
      "Train Epoch: 132 [15488/54000 (29%)] Loss: -224411.656250\n",
      "Train Epoch: 132 [16896/54000 (31%)] Loss: -219999.437500\n",
      "Train Epoch: 132 [18304/54000 (34%)] Loss: -218031.359375\n",
      "Train Epoch: 132 [19712/54000 (37%)] Loss: -242357.765625\n",
      "Train Epoch: 132 [21120/54000 (39%)] Loss: -211861.000000\n",
      "Train Epoch: 132 [22528/54000 (42%)] Loss: -211946.718750\n",
      "Train Epoch: 132 [23936/54000 (44%)] Loss: -204436.437500\n",
      "Train Epoch: 132 [25344/54000 (47%)] Loss: -213475.781250\n",
      "Train Epoch: 132 [26752/54000 (50%)] Loss: -214300.468750\n",
      "Train Epoch: 132 [28160/54000 (52%)] Loss: -221494.000000\n",
      "Train Epoch: 132 [29568/54000 (55%)] Loss: -214405.734375\n",
      "Train Epoch: 132 [30976/54000 (57%)] Loss: -238591.656250\n",
      "Train Epoch: 132 [32384/54000 (60%)] Loss: -217936.109375\n",
      "Train Epoch: 132 [33792/54000 (63%)] Loss: -213939.140625\n",
      "Train Epoch: 132 [35200/54000 (65%)] Loss: -216081.734375\n",
      "Train Epoch: 132 [36608/54000 (68%)] Loss: -239068.109375\n",
      "Train Epoch: 132 [38016/54000 (70%)] Loss: -208824.718750\n",
      "Train Epoch: 132 [39424/54000 (73%)] Loss: -225230.406250\n",
      "Train Epoch: 132 [40832/54000 (76%)] Loss: -220431.171875\n",
      "Train Epoch: 132 [42240/54000 (78%)] Loss: -222683.921875\n",
      "Train Epoch: 132 [43648/54000 (81%)] Loss: -209434.453125\n",
      "Train Epoch: 132 [45056/54000 (83%)] Loss: -220351.421875\n",
      "Train Epoch: 132 [46464/54000 (86%)] Loss: -214761.296875\n",
      "Train Epoch: 132 [47872/54000 (89%)] Loss: -216358.312500\n",
      "Train Epoch: 132 [49280/54000 (91%)] Loss: -217026.375000\n",
      "Train Epoch: 132 [50688/54000 (94%)] Loss: -220769.078125\n",
      "Train Epoch: 132 [52096/54000 (96%)] Loss: -211217.265625\n",
      "    epoch          : 132\n",
      "    loss           : -218526.0692284689\n",
      "    val_loss       : -225850.22129858995\n",
      "Train Epoch: 133 [0/54000 (0%)] Loss: -212801.703125\n",
      "Train Epoch: 133 [1408/54000 (3%)] Loss: -239830.734375\n",
      "Train Epoch: 133 [2816/54000 (5%)] Loss: -210000.343750\n",
      "Train Epoch: 133 [4224/54000 (8%)] Loss: -209409.296875\n",
      "Train Epoch: 133 [5632/54000 (10%)] Loss: -213447.406250\n",
      "Train Epoch: 133 [7040/54000 (13%)] Loss: -207613.750000\n",
      "Train Epoch: 133 [8448/54000 (16%)] Loss: -219467.468750\n",
      "Train Epoch: 133 [9856/54000 (18%)] Loss: -218486.781250\n",
      "Train Epoch: 133 [11264/54000 (21%)] Loss: -213877.812500\n",
      "Train Epoch: 133 [12672/54000 (23%)] Loss: -239605.906250\n",
      "Train Epoch: 133 [14080/54000 (26%)] Loss: -215326.750000\n",
      "Train Epoch: 133 [15488/54000 (29%)] Loss: -208102.984375\n",
      "Train Epoch: 133 [16896/54000 (31%)] Loss: -237742.109375\n",
      "Train Epoch: 133 [18304/54000 (34%)] Loss: -212571.515625\n",
      "Train Epoch: 133 [19712/54000 (37%)] Loss: -223034.234375\n",
      "Train Epoch: 133 [21120/54000 (39%)] Loss: -219571.125000\n",
      "Train Epoch: 133 [22528/54000 (42%)] Loss: -240742.609375\n",
      "Train Epoch: 133 [23936/54000 (44%)] Loss: -215232.109375\n",
      "Train Epoch: 133 [25344/54000 (47%)] Loss: -214443.078125\n",
      "Train Epoch: 133 [26752/54000 (50%)] Loss: -213855.062500\n",
      "Train Epoch: 133 [28160/54000 (52%)] Loss: -218715.390625\n",
      "Train Epoch: 133 [29568/54000 (55%)] Loss: -212423.546875\n",
      "Train Epoch: 133 [30976/54000 (57%)] Loss: -213308.468750\n",
      "Train Epoch: 133 [32384/54000 (60%)] Loss: -218727.359375\n",
      "Train Epoch: 133 [33792/54000 (63%)] Loss: -218057.578125\n",
      "Train Epoch: 133 [35200/54000 (65%)] Loss: -226995.984375\n",
      "Train Epoch: 133 [36608/54000 (68%)] Loss: -239744.625000\n",
      "Train Epoch: 133 [38016/54000 (70%)] Loss: -210872.250000\n",
      "Train Epoch: 133 [39424/54000 (73%)] Loss: -220029.453125\n",
      "Train Epoch: 133 [40832/54000 (76%)] Loss: -215578.718750\n",
      "Train Epoch: 133 [42240/54000 (78%)] Loss: -212030.531250\n",
      "Train Epoch: 133 [43648/54000 (81%)] Loss: -207639.546875\n",
      "Train Epoch: 133 [45056/54000 (83%)] Loss: -216702.375000\n",
      "Train Epoch: 133 [46464/54000 (86%)] Loss: -216687.625000\n",
      "Train Epoch: 133 [47872/54000 (89%)] Loss: -213379.437500\n",
      "Train Epoch: 133 [49280/54000 (91%)] Loss: -217941.187500\n",
      "Train Epoch: 133 [50688/54000 (94%)] Loss: -205906.640625\n",
      "Train Epoch: 133 [52096/54000 (96%)] Loss: -209236.843750\n",
      "    epoch          : 133\n",
      "    loss           : -218715.86928080145\n",
      "    val_loss       : -226359.77396150914\n",
      "Train Epoch: 134 [0/54000 (0%)] Loss: -222244.937500\n",
      "Train Epoch: 134 [1408/54000 (3%)] Loss: -215460.218750\n",
      "Train Epoch: 134 [2816/54000 (5%)] Loss: -214522.171875\n",
      "Train Epoch: 134 [4224/54000 (8%)] Loss: -211239.531250\n",
      "Train Epoch: 134 [5632/54000 (10%)] Loss: -218344.218750\n",
      "Train Epoch: 134 [7040/54000 (13%)] Loss: -216588.656250\n",
      "Train Epoch: 134 [8448/54000 (16%)] Loss: -208420.578125\n",
      "Train Epoch: 134 [9856/54000 (18%)] Loss: -219541.328125\n",
      "Train Epoch: 134 [11264/54000 (21%)] Loss: -215200.031250\n",
      "Train Epoch: 134 [12672/54000 (23%)] Loss: -206109.390625\n",
      "Train Epoch: 134 [14080/54000 (26%)] Loss: -212266.828125\n",
      "Train Epoch: 134 [15488/54000 (29%)] Loss: -238088.000000\n",
      "Train Epoch: 134 [16896/54000 (31%)] Loss: -208015.000000\n",
      "Train Epoch: 134 [18304/54000 (34%)] Loss: -212785.656250\n",
      "Train Epoch: 134 [19712/54000 (37%)] Loss: -204519.187500\n",
      "Train Epoch: 134 [21120/54000 (39%)] Loss: -240486.140625\n",
      "Train Epoch: 134 [22528/54000 (42%)] Loss: -221326.687500\n",
      "Train Epoch: 134 [23936/54000 (44%)] Loss: -213289.109375\n",
      "Train Epoch: 134 [25344/54000 (47%)] Loss: -217853.500000\n",
      "Train Epoch: 134 [26752/54000 (50%)] Loss: -207679.593750\n",
      "Train Epoch: 134 [28160/54000 (52%)] Loss: -215557.375000\n",
      "Train Epoch: 134 [29568/54000 (55%)] Loss: -237962.156250\n",
      "Train Epoch: 134 [30976/54000 (57%)] Loss: -213021.218750\n",
      "Train Epoch: 134 [32384/54000 (60%)] Loss: -220101.703125\n",
      "Train Epoch: 134 [33792/54000 (63%)] Loss: -219244.421875\n",
      "Train Epoch: 134 [35200/54000 (65%)] Loss: -215036.093750\n",
      "Train Epoch: 134 [36608/54000 (68%)] Loss: -212881.156250\n",
      "Train Epoch: 134 [38016/54000 (70%)] Loss: -211880.859375\n",
      "Train Epoch: 134 [39424/54000 (73%)] Loss: -205207.953125\n",
      "Train Epoch: 134 [40832/54000 (76%)] Loss: -215215.812500\n",
      "Train Epoch: 134 [42240/54000 (78%)] Loss: -212291.546875\n",
      "Train Epoch: 134 [43648/54000 (81%)] Loss: -210801.656250\n",
      "Train Epoch: 134 [45056/54000 (83%)] Loss: -216435.562500\n",
      "Train Epoch: 134 [46464/54000 (86%)] Loss: -215691.437500\n",
      "Train Epoch: 134 [47872/54000 (89%)] Loss: -216345.484375\n",
      "Train Epoch: 134 [49280/54000 (91%)] Loss: -219384.156250\n",
      "Train Epoch: 134 [50688/54000 (94%)] Loss: -239942.781250\n",
      "Train Epoch: 134 [52096/54000 (96%)] Loss: -222966.234375\n",
      "    epoch          : 134\n",
      "    loss           : -218746.10974880384\n",
      "    val_loss       : -226359.7822027439\n",
      "Train Epoch: 135 [0/54000 (0%)] Loss: -239221.062500\n",
      "Train Epoch: 135 [1408/54000 (3%)] Loss: -216143.843750\n",
      "Train Epoch: 135 [2816/54000 (5%)] Loss: -214058.156250\n",
      "Train Epoch: 135 [4224/54000 (8%)] Loss: -212786.218750\n",
      "Train Epoch: 135 [5632/54000 (10%)] Loss: -212365.593750\n",
      "Train Epoch: 135 [7040/54000 (13%)] Loss: -197312.296875\n",
      "Train Epoch: 135 [8448/54000 (16%)] Loss: -209476.000000\n",
      "Train Epoch: 135 [9856/54000 (18%)] Loss: -218062.000000\n",
      "Train Epoch: 135 [11264/54000 (21%)] Loss: -223667.703125\n",
      "Train Epoch: 135 [12672/54000 (23%)] Loss: -239387.515625\n",
      "Train Epoch: 135 [14080/54000 (26%)] Loss: -224134.171875\n",
      "Train Epoch: 135 [15488/54000 (29%)] Loss: -217225.500000\n",
      "Train Epoch: 135 [16896/54000 (31%)] Loss: -226746.531250\n",
      "Train Epoch: 135 [18304/54000 (34%)] Loss: -203934.375000\n",
      "Train Epoch: 135 [19712/54000 (37%)] Loss: -207033.343750\n",
      "Train Epoch: 135 [21120/54000 (39%)] Loss: -205362.031250\n",
      "Train Epoch: 135 [22528/54000 (42%)] Loss: -210707.218750\n",
      "Train Epoch: 135 [23936/54000 (44%)] Loss: -218412.875000\n",
      "Train Epoch: 135 [25344/54000 (47%)] Loss: -238767.562500\n",
      "Train Epoch: 135 [26752/54000 (50%)] Loss: -220075.250000\n",
      "Train Epoch: 135 [28160/54000 (52%)] Loss: -216273.625000\n",
      "Train Epoch: 135 [29568/54000 (55%)] Loss: -215276.421875\n",
      "Train Epoch: 135 [30976/54000 (57%)] Loss: -226960.875000\n",
      "Train Epoch: 135 [32384/54000 (60%)] Loss: -222452.390625\n",
      "Train Epoch: 135 [33792/54000 (63%)] Loss: -219591.937500\n",
      "Train Epoch: 135 [35200/54000 (65%)] Loss: -209649.484375\n",
      "Train Epoch: 135 [36608/54000 (68%)] Loss: -208753.203125\n",
      "Train Epoch: 135 [38016/54000 (70%)] Loss: -242300.359375\n",
      "Train Epoch: 135 [39424/54000 (73%)] Loss: -208662.156250\n",
      "Train Epoch: 135 [40832/54000 (76%)] Loss: -211888.578125\n",
      "Train Epoch: 135 [42240/54000 (78%)] Loss: -217208.562500\n",
      "Train Epoch: 135 [43648/54000 (81%)] Loss: -219152.546875\n",
      "Train Epoch: 135 [45056/54000 (83%)] Loss: -214361.250000\n",
      "Train Epoch: 135 [46464/54000 (86%)] Loss: -223182.000000\n",
      "Train Epoch: 135 [47872/54000 (89%)] Loss: -215887.531250\n",
      "Train Epoch: 135 [49280/54000 (91%)] Loss: -216265.734375\n",
      "Train Epoch: 135 [50688/54000 (94%)] Loss: -204010.968750\n",
      "Train Epoch: 135 [52096/54000 (96%)] Loss: -210660.531250\n",
      "    epoch          : 135\n",
      "    loss           : -218756.83586273924\n",
      "    val_loss       : -226216.73601848324\n",
      "Train Epoch: 136 [0/54000 (0%)] Loss: -204636.218750\n",
      "Train Epoch: 136 [1408/54000 (3%)] Loss: -220147.453125\n",
      "Train Epoch: 136 [2816/54000 (5%)] Loss: -239928.906250\n",
      "Train Epoch: 136 [4224/54000 (8%)] Loss: -212100.781250\n",
      "Train Epoch: 136 [5632/54000 (10%)] Loss: -225787.468750\n",
      "Train Epoch: 136 [7040/54000 (13%)] Loss: -209566.062500\n",
      "Train Epoch: 136 [8448/54000 (16%)] Loss: -239349.296875\n",
      "Train Epoch: 136 [9856/54000 (18%)] Loss: -214980.187500\n",
      "Train Epoch: 136 [11264/54000 (21%)] Loss: -221836.031250\n",
      "Train Epoch: 136 [12672/54000 (23%)] Loss: -222536.562500\n",
      "Train Epoch: 136 [14080/54000 (26%)] Loss: -213540.093750\n",
      "Train Epoch: 136 [15488/54000 (29%)] Loss: -213303.312500\n",
      "Train Epoch: 136 [16896/54000 (31%)] Loss: -217954.625000\n",
      "Train Epoch: 136 [18304/54000 (34%)] Loss: -239089.687500\n",
      "Train Epoch: 136 [19712/54000 (37%)] Loss: -224212.031250\n",
      "Train Epoch: 136 [21120/54000 (39%)] Loss: -222403.046875\n",
      "Train Epoch: 136 [22528/54000 (42%)] Loss: -221910.796875\n",
      "Train Epoch: 136 [23936/54000 (44%)] Loss: -219754.000000\n",
      "Train Epoch: 136 [25344/54000 (47%)] Loss: -242927.421875\n",
      "Train Epoch: 136 [26752/54000 (50%)] Loss: -210689.015625\n",
      "Train Epoch: 136 [28160/54000 (52%)] Loss: -221978.203125\n",
      "Train Epoch: 136 [29568/54000 (55%)] Loss: -221678.218750\n",
      "Train Epoch: 136 [30976/54000 (57%)] Loss: -219800.578125\n",
      "Train Epoch: 136 [32384/54000 (60%)] Loss: -220458.578125\n",
      "Train Epoch: 136 [33792/54000 (63%)] Loss: -219037.062500\n",
      "Train Epoch: 136 [35200/54000 (65%)] Loss: -242417.687500\n",
      "Train Epoch: 136 [36608/54000 (68%)] Loss: -210055.671875\n",
      "Train Epoch: 136 [38016/54000 (70%)] Loss: -215265.250000\n",
      "Train Epoch: 136 [39424/54000 (73%)] Loss: -221389.171875\n",
      "Train Epoch: 136 [40832/54000 (76%)] Loss: -208913.125000\n",
      "Train Epoch: 136 [42240/54000 (78%)] Loss: -207601.093750\n",
      "Train Epoch: 136 [43648/54000 (81%)] Loss: -241222.000000\n",
      "Train Epoch: 136 [45056/54000 (83%)] Loss: -224663.875000\n",
      "Train Epoch: 136 [46464/54000 (86%)] Loss: -214663.390625\n",
      "Train Epoch: 136 [47872/54000 (89%)] Loss: -217741.484375\n",
      "Train Epoch: 136 [49280/54000 (91%)] Loss: -241218.234375\n",
      "Train Epoch: 136 [50688/54000 (94%)] Loss: -217541.328125\n",
      "Train Epoch: 136 [52096/54000 (96%)] Loss: -212370.125000\n",
      "    epoch          : 136\n",
      "    loss           : -218930.68634120814\n",
      "    val_loss       : -226439.48288633765\n",
      "Train Epoch: 137 [0/54000 (0%)] Loss: -209287.281250\n",
      "Train Epoch: 137 [1408/54000 (3%)] Loss: -212097.500000\n",
      "Train Epoch: 137 [2816/54000 (5%)] Loss: -220323.218750\n",
      "Train Epoch: 137 [4224/54000 (8%)] Loss: -219051.531250\n",
      "Train Epoch: 137 [5632/54000 (10%)] Loss: -216121.031250\n",
      "Train Epoch: 137 [7040/54000 (13%)] Loss: -238425.109375\n",
      "Train Epoch: 137 [8448/54000 (16%)] Loss: -225998.406250\n",
      "Train Epoch: 137 [9856/54000 (18%)] Loss: -218624.031250\n",
      "Train Epoch: 137 [11264/54000 (21%)] Loss: -203943.937500\n",
      "Train Epoch: 137 [12672/54000 (23%)] Loss: -212948.750000\n",
      "Train Epoch: 137 [14080/54000 (26%)] Loss: -209610.281250\n",
      "Train Epoch: 137 [15488/54000 (29%)] Loss: -210112.875000\n",
      "Train Epoch: 137 [16896/54000 (31%)] Loss: -217246.218750\n",
      "Train Epoch: 137 [18304/54000 (34%)] Loss: -215730.781250\n",
      "Train Epoch: 137 [19712/54000 (37%)] Loss: -241440.937500\n",
      "Train Epoch: 137 [21120/54000 (39%)] Loss: -223556.953125\n",
      "Train Epoch: 137 [22528/54000 (42%)] Loss: -213326.937500\n",
      "Train Epoch: 137 [23936/54000 (44%)] Loss: -241957.328125\n",
      "Train Epoch: 137 [25344/54000 (47%)] Loss: -213277.171875\n",
      "Train Epoch: 137 [26752/54000 (50%)] Loss: -214629.812500\n",
      "Train Epoch: 137 [28160/54000 (52%)] Loss: -209594.640625\n",
      "Train Epoch: 137 [29568/54000 (55%)] Loss: -219846.593750\n",
      "Train Epoch: 137 [30976/54000 (57%)] Loss: -207723.421875\n",
      "Train Epoch: 137 [32384/54000 (60%)] Loss: -216520.906250\n",
      "Train Epoch: 137 [33792/54000 (63%)] Loss: -208059.625000\n",
      "Train Epoch: 137 [35200/54000 (65%)] Loss: -219685.687500\n",
      "Train Epoch: 137 [36608/54000 (68%)] Loss: -220016.078125\n",
      "Train Epoch: 137 [38016/54000 (70%)] Loss: -220031.437500\n",
      "Train Epoch: 137 [39424/54000 (73%)] Loss: -223298.625000\n",
      "Train Epoch: 137 [40832/54000 (76%)] Loss: -213982.625000\n",
      "Train Epoch: 137 [42240/54000 (78%)] Loss: -226371.468750\n",
      "Train Epoch: 137 [43648/54000 (81%)] Loss: -213714.000000\n",
      "Train Epoch: 137 [45056/54000 (83%)] Loss: -209605.734375\n",
      "Train Epoch: 137 [46464/54000 (86%)] Loss: -208331.812500\n",
      "Train Epoch: 137 [47872/54000 (89%)] Loss: -218122.171875\n",
      "Train Epoch: 137 [49280/54000 (91%)] Loss: -214880.343750\n",
      "Train Epoch: 137 [50688/54000 (94%)] Loss: -238841.406250\n",
      "Train Epoch: 137 [52096/54000 (96%)] Loss: -222320.890625\n",
      "    epoch          : 137\n",
      "    loss           : -218913.30169706937\n",
      "    val_loss       : -226731.55484232088\n",
      "Train Epoch: 138 [0/54000 (0%)] Loss: -226353.937500\n",
      "Train Epoch: 138 [1408/54000 (3%)] Loss: -212654.781250\n",
      "Train Epoch: 138 [2816/54000 (5%)] Loss: -225225.031250\n",
      "Train Epoch: 138 [4224/54000 (8%)] Loss: -216589.468750\n",
      "Train Epoch: 138 [5632/54000 (10%)] Loss: -215504.765625\n",
      "Train Epoch: 138 [7040/54000 (13%)] Loss: -217627.031250\n",
      "Train Epoch: 138 [8448/54000 (16%)] Loss: -215864.640625\n",
      "Train Epoch: 138 [9856/54000 (18%)] Loss: -212794.250000\n",
      "Train Epoch: 138 [11264/54000 (21%)] Loss: -212879.312500\n",
      "Train Epoch: 138 [12672/54000 (23%)] Loss: -208941.812500\n",
      "Train Epoch: 138 [14080/54000 (26%)] Loss: -211924.187500\n",
      "Train Epoch: 138 [15488/54000 (29%)] Loss: -211712.703125\n",
      "Train Epoch: 138 [16896/54000 (31%)] Loss: -214745.187500\n",
      "Train Epoch: 138 [18304/54000 (34%)] Loss: -225249.015625\n",
      "Train Epoch: 138 [19712/54000 (37%)] Loss: -221577.859375\n",
      "Train Epoch: 138 [21120/54000 (39%)] Loss: -224733.437500\n",
      "Train Epoch: 138 [22528/54000 (42%)] Loss: -215139.031250\n",
      "Train Epoch: 138 [23936/54000 (44%)] Loss: -209717.296875\n",
      "Train Epoch: 138 [25344/54000 (47%)] Loss: -216832.843750\n",
      "Train Epoch: 138 [26752/54000 (50%)] Loss: -240047.625000\n",
      "Train Epoch: 138 [28160/54000 (52%)] Loss: -217505.359375\n",
      "Train Epoch: 138 [29568/54000 (55%)] Loss: -215135.953125\n",
      "Train Epoch: 138 [30976/54000 (57%)] Loss: -223237.375000\n",
      "Train Epoch: 138 [32384/54000 (60%)] Loss: -222898.265625\n",
      "Train Epoch: 138 [33792/54000 (63%)] Loss: -219352.203125\n",
      "Train Epoch: 138 [35200/54000 (65%)] Loss: -216279.531250\n",
      "Train Epoch: 138 [36608/54000 (68%)] Loss: -223258.906250\n",
      "Train Epoch: 138 [38016/54000 (70%)] Loss: -210185.875000\n",
      "Train Epoch: 138 [39424/54000 (73%)] Loss: -213202.718750\n",
      "Train Epoch: 138 [40832/54000 (76%)] Loss: -218198.640625\n",
      "Train Epoch: 138 [42240/54000 (78%)] Loss: -218363.828125\n",
      "Train Epoch: 138 [43648/54000 (81%)] Loss: -209460.468750\n",
      "Train Epoch: 138 [45056/54000 (83%)] Loss: -239738.812500\n",
      "Train Epoch: 138 [46464/54000 (86%)] Loss: -218383.109375\n",
      "Train Epoch: 138 [47872/54000 (89%)] Loss: -211510.031250\n",
      "Train Epoch: 138 [49280/54000 (91%)] Loss: -216863.437500\n",
      "Train Epoch: 138 [50688/54000 (94%)] Loss: -217727.937500\n",
      "Train Epoch: 138 [52096/54000 (96%)] Loss: -240944.140625\n",
      "    epoch          : 138\n",
      "    loss           : -218894.55248205742\n",
      "    val_loss       : -226225.19993330791\n",
      "Train Epoch: 139 [0/54000 (0%)] Loss: -241517.578125\n",
      "Train Epoch: 139 [1408/54000 (3%)] Loss: -212350.234375\n",
      "Train Epoch: 139 [2816/54000 (5%)] Loss: -210488.171875\n",
      "Train Epoch: 139 [4224/54000 (8%)] Loss: -207331.515625\n",
      "Train Epoch: 139 [5632/54000 (10%)] Loss: -217715.000000\n",
      "Train Epoch: 139 [7040/54000 (13%)] Loss: -241217.875000\n",
      "Train Epoch: 139 [8448/54000 (16%)] Loss: -211895.937500\n",
      "Train Epoch: 139 [9856/54000 (18%)] Loss: -216351.687500\n",
      "Train Epoch: 139 [11264/54000 (21%)] Loss: -212857.109375\n",
      "Train Epoch: 139 [12672/54000 (23%)] Loss: -220988.406250\n",
      "Train Epoch: 139 [14080/54000 (26%)] Loss: -204572.765625\n",
      "Train Epoch: 139 [15488/54000 (29%)] Loss: -223488.437500\n",
      "Train Epoch: 139 [16896/54000 (31%)] Loss: -213024.890625\n",
      "Train Epoch: 139 [18304/54000 (34%)] Loss: -208540.296875\n",
      "Train Epoch: 139 [19712/54000 (37%)] Loss: -238116.562500\n",
      "Train Epoch: 139 [21120/54000 (39%)] Loss: -218923.375000\n",
      "Train Epoch: 139 [22528/54000 (42%)] Loss: -223789.281250\n",
      "Train Epoch: 139 [23936/54000 (44%)] Loss: -216971.171875\n",
      "Train Epoch: 139 [25344/54000 (47%)] Loss: -219527.906250\n",
      "Train Epoch: 139 [26752/54000 (50%)] Loss: -220270.687500\n",
      "Train Epoch: 139 [28160/54000 (52%)] Loss: -208014.843750\n",
      "Train Epoch: 139 [29568/54000 (55%)] Loss: -210158.875000\n",
      "Train Epoch: 139 [30976/54000 (57%)] Loss: -223218.843750\n",
      "Train Epoch: 139 [32384/54000 (60%)] Loss: -212272.000000\n",
      "Train Epoch: 139 [33792/54000 (63%)] Loss: -212406.812500\n",
      "Train Epoch: 139 [35200/54000 (65%)] Loss: -214574.531250\n",
      "Train Epoch: 139 [36608/54000 (68%)] Loss: -220209.328125\n",
      "Train Epoch: 139 [38016/54000 (70%)] Loss: -214134.843750\n",
      "Train Epoch: 139 [39424/54000 (73%)] Loss: -217481.906250\n",
      "Train Epoch: 139 [40832/54000 (76%)] Loss: -212928.687500\n",
      "Train Epoch: 139 [42240/54000 (78%)] Loss: -210712.703125\n",
      "Train Epoch: 139 [43648/54000 (81%)] Loss: -210498.578125\n",
      "Train Epoch: 139 [45056/54000 (83%)] Loss: -223536.859375\n",
      "Train Epoch: 139 [46464/54000 (86%)] Loss: -225627.468750\n",
      "Train Epoch: 139 [47872/54000 (89%)] Loss: -208096.140625\n",
      "Train Epoch: 139 [49280/54000 (91%)] Loss: -217554.609375\n",
      "Train Epoch: 139 [50688/54000 (94%)] Loss: -207426.593750\n",
      "Train Epoch: 139 [52096/54000 (96%)] Loss: -211332.281250\n",
      "    epoch          : 139\n",
      "    loss           : -219163.86812200956\n",
      "    val_loss       : -226433.14604373096\n",
      "Train Epoch: 140 [0/54000 (0%)] Loss: -225317.625000\n",
      "Train Epoch: 140 [1408/54000 (3%)] Loss: -218111.171875\n",
      "Train Epoch: 140 [2816/54000 (5%)] Loss: -224808.125000\n",
      "Train Epoch: 140 [4224/54000 (8%)] Loss: -214831.156250\n",
      "Train Epoch: 140 [5632/54000 (10%)] Loss: -227130.734375\n",
      "Train Epoch: 140 [7040/54000 (13%)] Loss: -218581.000000\n",
      "Train Epoch: 140 [8448/54000 (16%)] Loss: -219133.562500\n",
      "Train Epoch: 140 [9856/54000 (18%)] Loss: -214195.093750\n",
      "Train Epoch: 140 [11264/54000 (21%)] Loss: -206734.968750\n",
      "Train Epoch: 140 [12672/54000 (23%)] Loss: -224395.406250\n",
      "Train Epoch: 140 [14080/54000 (26%)] Loss: -241813.156250\n",
      "Train Epoch: 140 [15488/54000 (29%)] Loss: -222562.859375\n",
      "Train Epoch: 140 [16896/54000 (31%)] Loss: -217510.687500\n",
      "Train Epoch: 140 [18304/54000 (34%)] Loss: -224487.703125\n",
      "Train Epoch: 140 [19712/54000 (37%)] Loss: -226180.921875\n",
      "Train Epoch: 140 [21120/54000 (39%)] Loss: -215680.093750\n",
      "Train Epoch: 140 [22528/54000 (42%)] Loss: -216073.078125\n",
      "Train Epoch: 140 [23936/54000 (44%)] Loss: -207621.218750\n",
      "Train Epoch: 140 [25344/54000 (47%)] Loss: -210035.500000\n",
      "Train Epoch: 140 [26752/54000 (50%)] Loss: -205835.406250\n",
      "Train Epoch: 140 [28160/54000 (52%)] Loss: -217366.765625\n",
      "Train Epoch: 140 [29568/54000 (55%)] Loss: -219724.515625\n",
      "Train Epoch: 140 [30976/54000 (57%)] Loss: -219504.500000\n",
      "Train Epoch: 140 [32384/54000 (60%)] Loss: -212296.250000\n",
      "Train Epoch: 140 [33792/54000 (63%)] Loss: -212242.656250\n",
      "Train Epoch: 140 [35200/54000 (65%)] Loss: -212170.187500\n",
      "Train Epoch: 140 [36608/54000 (68%)] Loss: -212638.203125\n",
      "Train Epoch: 140 [38016/54000 (70%)] Loss: -216178.140625\n",
      "Train Epoch: 140 [39424/54000 (73%)] Loss: -208702.656250\n",
      "Train Epoch: 140 [40832/54000 (76%)] Loss: -215981.031250\n",
      "Train Epoch: 140 [42240/54000 (78%)] Loss: -218633.312500\n",
      "Train Epoch: 140 [43648/54000 (81%)] Loss: -211211.125000\n",
      "Train Epoch: 140 [45056/54000 (83%)] Loss: -240786.500000\n",
      "Train Epoch: 140 [46464/54000 (86%)] Loss: -213817.406250\n",
      "Train Epoch: 140 [47872/54000 (89%)] Loss: -212367.859375\n",
      "Train Epoch: 140 [49280/54000 (91%)] Loss: -218185.984375\n",
      "Train Epoch: 140 [50688/54000 (94%)] Loss: -239536.718750\n",
      "Train Epoch: 140 [52096/54000 (96%)] Loss: -208698.515625\n",
      "    epoch          : 140\n",
      "    loss           : -219104.71112440192\n",
      "    val_loss       : -226692.59594131098\n",
      "Train Epoch: 141 [0/54000 (0%)] Loss: -221926.062500\n",
      "Train Epoch: 141 [1408/54000 (3%)] Loss: -243339.578125\n",
      "Train Epoch: 141 [2816/54000 (5%)] Loss: -209233.296875\n",
      "Train Epoch: 141 [4224/54000 (8%)] Loss: -215028.500000\n",
      "Train Epoch: 141 [5632/54000 (10%)] Loss: -225945.281250\n",
      "Train Epoch: 141 [7040/54000 (13%)] Loss: -226802.375000\n",
      "Train Epoch: 141 [8448/54000 (16%)] Loss: -224578.531250\n",
      "Train Epoch: 141 [9856/54000 (18%)] Loss: -219407.859375\n",
      "Train Epoch: 141 [11264/54000 (21%)] Loss: -244117.703125\n",
      "Train Epoch: 141 [12672/54000 (23%)] Loss: -224264.453125\n",
      "Train Epoch: 141 [14080/54000 (26%)] Loss: -207432.625000\n",
      "Train Epoch: 141 [15488/54000 (29%)] Loss: -209058.296875\n",
      "Train Epoch: 141 [16896/54000 (31%)] Loss: -211441.156250\n",
      "Train Epoch: 141 [18304/54000 (34%)] Loss: -206329.265625\n",
      "Train Epoch: 141 [19712/54000 (37%)] Loss: -207905.109375\n",
      "Train Epoch: 141 [21120/54000 (39%)] Loss: -213621.562500\n",
      "Train Epoch: 141 [22528/54000 (42%)] Loss: -212710.906250\n",
      "Train Epoch: 141 [23936/54000 (44%)] Loss: -243971.750000\n",
      "Train Epoch: 141 [25344/54000 (47%)] Loss: -213583.562500\n",
      "Train Epoch: 141 [26752/54000 (50%)] Loss: -214448.000000\n",
      "Train Epoch: 141 [28160/54000 (52%)] Loss: -238988.875000\n",
      "Train Epoch: 141 [29568/54000 (55%)] Loss: -224156.750000\n",
      "Train Epoch: 141 [30976/54000 (57%)] Loss: -221466.250000\n",
      "Train Epoch: 141 [32384/54000 (60%)] Loss: -220148.718750\n",
      "Train Epoch: 141 [33792/54000 (63%)] Loss: -209782.234375\n",
      "Train Epoch: 141 [35200/54000 (65%)] Loss: -238547.140625\n",
      "Train Epoch: 141 [36608/54000 (68%)] Loss: -219721.625000\n",
      "Train Epoch: 141 [38016/54000 (70%)] Loss: -215827.484375\n",
      "Train Epoch: 141 [39424/54000 (73%)] Loss: -242267.640625\n",
      "Train Epoch: 141 [40832/54000 (76%)] Loss: -222851.781250\n",
      "Train Epoch: 141 [42240/54000 (78%)] Loss: -216297.781250\n",
      "Train Epoch: 141 [43648/54000 (81%)] Loss: -219519.390625\n",
      "Train Epoch: 141 [45056/54000 (83%)] Loss: -224871.125000\n",
      "Train Epoch: 141 [46464/54000 (86%)] Loss: -216210.750000\n",
      "Train Epoch: 141 [47872/54000 (89%)] Loss: -207231.218750\n",
      "Train Epoch: 141 [49280/54000 (91%)] Loss: -214944.859375\n",
      "Train Epoch: 141 [50688/54000 (94%)] Loss: -212361.593750\n",
      "Train Epoch: 141 [52096/54000 (96%)] Loss: -209595.562500\n",
      "    epoch          : 141\n",
      "    loss           : -219259.3374327153\n",
      "    val_loss       : -226842.16409227325\n",
      "Train Epoch: 142 [0/54000 (0%)] Loss: -240471.203125\n",
      "Train Epoch: 142 [1408/54000 (3%)] Loss: -221984.984375\n",
      "Train Epoch: 142 [2816/54000 (5%)] Loss: -214811.468750\n",
      "Train Epoch: 142 [4224/54000 (8%)] Loss: -224828.328125\n",
      "Train Epoch: 142 [5632/54000 (10%)] Loss: -217447.312500\n",
      "Train Epoch: 142 [7040/54000 (13%)] Loss: -210409.968750\n",
      "Train Epoch: 142 [8448/54000 (16%)] Loss: -209066.468750\n",
      "Train Epoch: 142 [9856/54000 (18%)] Loss: -220418.031250\n",
      "Train Epoch: 142 [11264/54000 (21%)] Loss: -224746.671875\n",
      "Train Epoch: 142 [12672/54000 (23%)] Loss: -225800.312500\n",
      "Train Epoch: 142 [14080/54000 (26%)] Loss: -224844.250000\n",
      "Train Epoch: 142 [15488/54000 (29%)] Loss: -225862.093750\n",
      "Train Epoch: 142 [16896/54000 (31%)] Loss: -216237.156250\n",
      "Train Epoch: 142 [18304/54000 (34%)] Loss: -216902.687500\n",
      "Train Epoch: 142 [19712/54000 (37%)] Loss: -241159.218750\n",
      "Train Epoch: 142 [21120/54000 (39%)] Loss: -239597.531250\n",
      "Train Epoch: 142 [22528/54000 (42%)] Loss: -213724.812500\n",
      "Train Epoch: 142 [23936/54000 (44%)] Loss: -222793.296875\n",
      "Train Epoch: 142 [25344/54000 (47%)] Loss: -225553.625000\n",
      "Train Epoch: 142 [26752/54000 (50%)] Loss: -226182.218750\n",
      "Train Epoch: 142 [28160/54000 (52%)] Loss: -224677.171875\n",
      "Train Epoch: 142 [29568/54000 (55%)] Loss: -225406.593750\n",
      "Train Epoch: 142 [30976/54000 (57%)] Loss: -214087.171875\n",
      "Train Epoch: 142 [32384/54000 (60%)] Loss: -215211.625000\n",
      "Train Epoch: 142 [33792/54000 (63%)] Loss: -220221.546875\n",
      "Train Epoch: 142 [35200/54000 (65%)] Loss: -215503.625000\n",
      "Train Epoch: 142 [36608/54000 (68%)] Loss: -217327.718750\n",
      "Train Epoch: 142 [38016/54000 (70%)] Loss: -219328.890625\n",
      "Train Epoch: 142 [39424/54000 (73%)] Loss: -209017.171875\n",
      "Train Epoch: 142 [40832/54000 (76%)] Loss: -213774.406250\n",
      "Train Epoch: 142 [42240/54000 (78%)] Loss: -216528.906250\n",
      "Train Epoch: 142 [43648/54000 (81%)] Loss: -213238.437500\n",
      "Train Epoch: 142 [45056/54000 (83%)] Loss: -214833.203125\n",
      "Train Epoch: 142 [46464/54000 (86%)] Loss: -214181.890625\n",
      "Train Epoch: 142 [47872/54000 (89%)] Loss: -212534.656250\n",
      "Train Epoch: 142 [49280/54000 (91%)] Loss: -212954.593750\n",
      "Train Epoch: 142 [50688/54000 (94%)] Loss: -208661.031250\n",
      "Train Epoch: 142 [52096/54000 (96%)] Loss: -221569.531250\n",
      "    epoch          : 142\n",
      "    loss           : -219348.55805173446\n",
      "    val_loss       : -226615.4590796494\n",
      "Train Epoch: 143 [0/54000 (0%)] Loss: -239868.250000\n",
      "Train Epoch: 143 [1408/54000 (3%)] Loss: -217024.562500\n",
      "Train Epoch: 143 [2816/54000 (5%)] Loss: -213562.640625\n",
      "Train Epoch: 143 [4224/54000 (8%)] Loss: -212257.031250\n",
      "Train Epoch: 143 [5632/54000 (10%)] Loss: -227021.468750\n",
      "Train Epoch: 143 [7040/54000 (13%)] Loss: -217078.312500\n",
      "Train Epoch: 143 [8448/54000 (16%)] Loss: -210300.953125\n",
      "Train Epoch: 143 [9856/54000 (18%)] Loss: -213271.546875\n",
      "Train Epoch: 143 [11264/54000 (21%)] Loss: -223749.609375\n",
      "Train Epoch: 143 [12672/54000 (23%)] Loss: -220075.781250\n",
      "Train Epoch: 143 [14080/54000 (26%)] Loss: -211741.093750\n",
      "Train Epoch: 143 [15488/54000 (29%)] Loss: -240178.375000\n",
      "Train Epoch: 143 [16896/54000 (31%)] Loss: -214764.343750\n",
      "Train Epoch: 143 [18304/54000 (34%)] Loss: -222800.312500\n",
      "Train Epoch: 143 [19712/54000 (37%)] Loss: -216240.687500\n",
      "Train Epoch: 143 [21120/54000 (39%)] Loss: -212648.250000\n",
      "Train Epoch: 143 [22528/54000 (42%)] Loss: -211022.750000\n",
      "Train Epoch: 143 [23936/54000 (44%)] Loss: -208032.875000\n",
      "Train Epoch: 143 [25344/54000 (47%)] Loss: -212047.593750\n",
      "Train Epoch: 143 [26752/54000 (50%)] Loss: -214937.109375\n",
      "Train Epoch: 143 [28160/54000 (52%)] Loss: -222896.937500\n",
      "Train Epoch: 143 [29568/54000 (55%)] Loss: -241267.671875\n",
      "Train Epoch: 143 [30976/54000 (57%)] Loss: -221502.156250\n",
      "Train Epoch: 143 [32384/54000 (60%)] Loss: -214353.593750\n",
      "Train Epoch: 143 [33792/54000 (63%)] Loss: -211533.406250\n",
      "Train Epoch: 143 [35200/54000 (65%)] Loss: -225893.281250\n",
      "Train Epoch: 143 [36608/54000 (68%)] Loss: -238248.234375\n",
      "Train Epoch: 143 [38016/54000 (70%)] Loss: -217045.906250\n",
      "Train Epoch: 143 [39424/54000 (73%)] Loss: -220052.203125\n",
      "Train Epoch: 143 [40832/54000 (76%)] Loss: -223461.437500\n",
      "Train Epoch: 143 [42240/54000 (78%)] Loss: -238518.843750\n",
      "Train Epoch: 143 [43648/54000 (81%)] Loss: -217936.781250\n",
      "Train Epoch: 143 [45056/54000 (83%)] Loss: -206898.390625\n",
      "Train Epoch: 143 [46464/54000 (86%)] Loss: -239964.984375\n",
      "Train Epoch: 143 [47872/54000 (89%)] Loss: -219155.359375\n",
      "Train Epoch: 143 [49280/54000 (91%)] Loss: -218778.984375\n",
      "Train Epoch: 143 [50688/54000 (94%)] Loss: -206182.281250\n",
      "Train Epoch: 143 [52096/54000 (96%)] Loss: -206268.703125\n",
      "    epoch          : 143\n",
      "    loss           : -219317.39892344497\n",
      "    val_loss       : -227039.47675900342\n",
      "Train Epoch: 144 [0/54000 (0%)] Loss: -212757.968750\n",
      "Train Epoch: 144 [1408/54000 (3%)] Loss: -214548.468750\n",
      "Train Epoch: 144 [2816/54000 (5%)] Loss: -219307.281250\n",
      "Train Epoch: 144 [4224/54000 (8%)] Loss: -222806.656250\n",
      "Train Epoch: 144 [5632/54000 (10%)] Loss: -214921.390625\n",
      "Train Epoch: 144 [7040/54000 (13%)] Loss: -225768.078125\n",
      "Train Epoch: 144 [8448/54000 (16%)] Loss: -227157.031250\n",
      "Train Epoch: 144 [9856/54000 (18%)] Loss: -226318.906250\n",
      "Train Epoch: 144 [11264/54000 (21%)] Loss: -207008.500000\n",
      "Train Epoch: 144 [12672/54000 (23%)] Loss: -219932.140625\n",
      "Train Epoch: 144 [14080/54000 (26%)] Loss: -219315.406250\n",
      "Train Epoch: 144 [15488/54000 (29%)] Loss: -221633.609375\n",
      "Train Epoch: 144 [16896/54000 (31%)] Loss: -219379.312500\n",
      "Train Epoch: 144 [18304/54000 (34%)] Loss: -219363.000000\n",
      "Train Epoch: 144 [19712/54000 (37%)] Loss: -220539.734375\n",
      "Train Epoch: 144 [21120/54000 (39%)] Loss: -239943.046875\n",
      "Train Epoch: 144 [22528/54000 (42%)] Loss: -211405.218750\n",
      "Train Epoch: 144 [23936/54000 (44%)] Loss: -210722.000000\n",
      "Train Epoch: 144 [25344/54000 (47%)] Loss: -227381.281250\n",
      "Train Epoch: 144 [26752/54000 (50%)] Loss: -225625.578125\n",
      "Train Epoch: 144 [28160/54000 (52%)] Loss: -205636.328125\n",
      "Train Epoch: 144 [29568/54000 (55%)] Loss: -214895.750000\n",
      "Train Epoch: 144 [30976/54000 (57%)] Loss: -211843.281250\n",
      "Train Epoch: 144 [32384/54000 (60%)] Loss: -221503.218750\n",
      "Train Epoch: 144 [33792/54000 (63%)] Loss: -220203.000000\n",
      "Train Epoch: 144 [35200/54000 (65%)] Loss: -216626.390625\n",
      "Train Epoch: 144 [36608/54000 (68%)] Loss: -204723.140625\n",
      "Train Epoch: 144 [38016/54000 (70%)] Loss: -238459.656250\n",
      "Train Epoch: 144 [39424/54000 (73%)] Loss: -225117.375000\n",
      "Train Epoch: 144 [40832/54000 (76%)] Loss: -222345.765625\n",
      "Train Epoch: 144 [42240/54000 (78%)] Loss: -209298.875000\n",
      "Train Epoch: 144 [43648/54000 (81%)] Loss: -218488.375000\n",
      "Train Epoch: 144 [45056/54000 (83%)] Loss: -216204.718750\n",
      "Train Epoch: 144 [46464/54000 (86%)] Loss: -220631.156250\n",
      "Train Epoch: 144 [47872/54000 (89%)] Loss: -221740.796875\n",
      "Train Epoch: 144 [49280/54000 (91%)] Loss: -209224.500000\n",
      "Train Epoch: 144 [50688/54000 (94%)] Loss: -239893.531250\n",
      "Train Epoch: 144 [52096/54000 (96%)] Loss: -222400.546875\n",
      "    epoch          : 144\n",
      "    loss           : -219451.33317135167\n",
      "    val_loss       : -226870.11928949124\n",
      "Train Epoch: 145 [0/54000 (0%)] Loss: -240847.250000\n",
      "Train Epoch: 145 [1408/54000 (3%)] Loss: -214745.296875\n",
      "Train Epoch: 145 [2816/54000 (5%)] Loss: -215002.406250\n",
      "Train Epoch: 145 [4224/54000 (8%)] Loss: -213727.187500\n",
      "Train Epoch: 145 [5632/54000 (10%)] Loss: -224155.437500\n",
      "Train Epoch: 145 [7040/54000 (13%)] Loss: -213217.218750\n",
      "Train Epoch: 145 [8448/54000 (16%)] Loss: -226179.218750\n",
      "Train Epoch: 145 [9856/54000 (18%)] Loss: -222018.750000\n",
      "Train Epoch: 145 [11264/54000 (21%)] Loss: -209169.328125\n",
      "Train Epoch: 145 [12672/54000 (23%)] Loss: -238924.343750\n",
      "Train Epoch: 145 [14080/54000 (26%)] Loss: -217653.609375\n",
      "Train Epoch: 145 [15488/54000 (29%)] Loss: -217438.437500\n",
      "Train Epoch: 145 [16896/54000 (31%)] Loss: -216334.375000\n",
      "Train Epoch: 145 [18304/54000 (34%)] Loss: -217974.109375\n",
      "Train Epoch: 145 [19712/54000 (37%)] Loss: -215879.953125\n",
      "Train Epoch: 145 [21120/54000 (39%)] Loss: -216796.671875\n",
      "Train Epoch: 145 [22528/54000 (42%)] Loss: -223414.468750\n",
      "Train Epoch: 145 [23936/54000 (44%)] Loss: -214076.765625\n",
      "Train Epoch: 145 [25344/54000 (47%)] Loss: -226873.156250\n",
      "Train Epoch: 145 [26752/54000 (50%)] Loss: -212410.437500\n",
      "Train Epoch: 145 [28160/54000 (52%)] Loss: -213710.500000\n",
      "Train Epoch: 145 [29568/54000 (55%)] Loss: -240345.609375\n",
      "Train Epoch: 145 [30976/54000 (57%)] Loss: -221463.078125\n",
      "Train Epoch: 145 [32384/54000 (60%)] Loss: -221968.265625\n",
      "Train Epoch: 145 [33792/54000 (63%)] Loss: -215980.296875\n",
      "Train Epoch: 145 [35200/54000 (65%)] Loss: -239514.671875\n",
      "Train Epoch: 145 [36608/54000 (68%)] Loss: -211686.812500\n",
      "Train Epoch: 145 [38016/54000 (70%)] Loss: -217721.156250\n",
      "Train Epoch: 145 [39424/54000 (73%)] Loss: -215319.562500\n",
      "Train Epoch: 145 [40832/54000 (76%)] Loss: -211603.281250\n",
      "Train Epoch: 145 [42240/54000 (78%)] Loss: -225121.687500\n",
      "Train Epoch: 145 [43648/54000 (81%)] Loss: -211687.640625\n",
      "Train Epoch: 145 [45056/54000 (83%)] Loss: -240326.218750\n",
      "Train Epoch: 145 [46464/54000 (86%)] Loss: -214785.000000\n",
      "Train Epoch: 145 [47872/54000 (89%)] Loss: -213600.281250\n",
      "Train Epoch: 145 [49280/54000 (91%)] Loss: -215548.421875\n",
      "Train Epoch: 145 [50688/54000 (94%)] Loss: -209226.578125\n",
      "Train Epoch: 145 [52096/54000 (96%)] Loss: -223958.453125\n",
      "    epoch          : 145\n",
      "    loss           : -219659.21740430623\n",
      "    val_loss       : -226684.8822587176\n",
      "Train Epoch: 146 [0/54000 (0%)] Loss: -209677.531250\n",
      "Train Epoch: 146 [1408/54000 (3%)] Loss: -211819.421875\n",
      "Train Epoch: 146 [2816/54000 (5%)] Loss: -219614.406250\n",
      "Train Epoch: 146 [4224/54000 (8%)] Loss: -222834.406250\n",
      "Train Epoch: 146 [5632/54000 (10%)] Loss: -206221.484375\n",
      "Train Epoch: 146 [7040/54000 (13%)] Loss: -226428.250000\n",
      "Train Epoch: 146 [8448/54000 (16%)] Loss: -239626.078125\n",
      "Train Epoch: 146 [9856/54000 (18%)] Loss: -222070.140625\n",
      "Train Epoch: 146 [11264/54000 (21%)] Loss: -222578.234375\n",
      "Train Epoch: 146 [12672/54000 (23%)] Loss: -216526.312500\n",
      "Train Epoch: 146 [14080/54000 (26%)] Loss: -240880.515625\n",
      "Train Epoch: 146 [15488/54000 (29%)] Loss: -211667.156250\n",
      "Train Epoch: 146 [16896/54000 (31%)] Loss: -206612.078125\n",
      "Train Epoch: 146 [18304/54000 (34%)] Loss: -214005.000000\n",
      "Train Epoch: 146 [19712/54000 (37%)] Loss: -226429.562500\n",
      "Train Epoch: 146 [21120/54000 (39%)] Loss: -224040.937500\n",
      "Train Epoch: 146 [22528/54000 (42%)] Loss: -219240.078125\n",
      "Train Epoch: 146 [23936/54000 (44%)] Loss: -210650.359375\n",
      "Train Epoch: 146 [25344/54000 (47%)] Loss: -213136.937500\n",
      "Train Epoch: 146 [26752/54000 (50%)] Loss: -241281.843750\n",
      "Train Epoch: 146 [28160/54000 (52%)] Loss: -213674.015625\n",
      "Train Epoch: 146 [29568/54000 (55%)] Loss: -220074.296875\n",
      "Train Epoch: 146 [30976/54000 (57%)] Loss: -217007.343750\n",
      "Train Epoch: 146 [32384/54000 (60%)] Loss: -239331.984375\n",
      "Train Epoch: 146 [33792/54000 (63%)] Loss: -227524.000000\n",
      "Train Epoch: 146 [35200/54000 (65%)] Loss: -215437.375000\n",
      "Train Epoch: 146 [36608/54000 (68%)] Loss: -207613.750000\n",
      "Train Epoch: 146 [38016/54000 (70%)] Loss: -223422.718750\n",
      "Train Epoch: 146 [39424/54000 (73%)] Loss: -223434.328125\n",
      "Train Epoch: 146 [40832/54000 (76%)] Loss: -226591.593750\n",
      "Train Epoch: 146 [42240/54000 (78%)] Loss: -205677.812500\n",
      "Train Epoch: 146 [43648/54000 (81%)] Loss: -222562.859375\n",
      "Train Epoch: 146 [45056/54000 (83%)] Loss: -220547.187500\n",
      "Train Epoch: 146 [46464/54000 (86%)] Loss: -215652.937500\n",
      "Train Epoch: 146 [47872/54000 (89%)] Loss: -220858.796875\n",
      "Train Epoch: 146 [49280/54000 (91%)] Loss: -223561.968750\n",
      "Train Epoch: 146 [50688/54000 (94%)] Loss: -213534.437500\n",
      "Train Epoch: 146 [52096/54000 (96%)] Loss: -238975.500000\n",
      "    epoch          : 146\n",
      "    loss           : -219550.16137111245\n",
      "    val_loss       : -226374.32933974848\n",
      "Train Epoch: 147 [0/54000 (0%)] Loss: -225199.765625\n",
      "Train Epoch: 147 [1408/54000 (3%)] Loss: -241620.906250\n",
      "Train Epoch: 147 [2816/54000 (5%)] Loss: -222882.078125\n",
      "Train Epoch: 147 [4224/54000 (8%)] Loss: -217600.718750\n",
      "Train Epoch: 147 [5632/54000 (10%)] Loss: -212344.812500\n",
      "Train Epoch: 147 [7040/54000 (13%)] Loss: -227720.000000\n",
      "Train Epoch: 147 [8448/54000 (16%)] Loss: -223251.156250\n",
      "Train Epoch: 147 [9856/54000 (18%)] Loss: -240880.250000\n",
      "Train Epoch: 147 [11264/54000 (21%)] Loss: -213839.156250\n",
      "Train Epoch: 147 [12672/54000 (23%)] Loss: -222062.500000\n",
      "Train Epoch: 147 [14080/54000 (26%)] Loss: -242191.687500\n",
      "Train Epoch: 147 [15488/54000 (29%)] Loss: -207056.390625\n",
      "Train Epoch: 147 [16896/54000 (31%)] Loss: -227011.203125\n",
      "Train Epoch: 147 [18304/54000 (34%)] Loss: -216320.125000\n",
      "Train Epoch: 147 [19712/54000 (37%)] Loss: -242247.421875\n",
      "Train Epoch: 147 [21120/54000 (39%)] Loss: -221474.687500\n",
      "Train Epoch: 147 [22528/54000 (42%)] Loss: -215425.687500\n",
      "Train Epoch: 147 [23936/54000 (44%)] Loss: -224261.546875\n",
      "Train Epoch: 147 [25344/54000 (47%)] Loss: -240577.968750\n",
      "Train Epoch: 147 [26752/54000 (50%)] Loss: -221788.140625\n",
      "Train Epoch: 147 [28160/54000 (52%)] Loss: -220100.250000\n",
      "Train Epoch: 147 [29568/54000 (55%)] Loss: -213230.109375\n",
      "Train Epoch: 147 [30976/54000 (57%)] Loss: -218884.671875\n",
      "Train Epoch: 147 [32384/54000 (60%)] Loss: -211408.953125\n",
      "Train Epoch: 147 [33792/54000 (63%)] Loss: -213881.234375\n",
      "Train Epoch: 147 [35200/54000 (65%)] Loss: -220018.656250\n",
      "Train Epoch: 147 [36608/54000 (68%)] Loss: -226652.937500\n",
      "Train Epoch: 147 [38016/54000 (70%)] Loss: -211464.437500\n",
      "Train Epoch: 147 [39424/54000 (73%)] Loss: -219352.062500\n",
      "Train Epoch: 147 [40832/54000 (76%)] Loss: -223981.015625\n",
      "Train Epoch: 147 [42240/54000 (78%)] Loss: -242522.890625\n",
      "Train Epoch: 147 [43648/54000 (81%)] Loss: -218175.843750\n",
      "Train Epoch: 147 [45056/54000 (83%)] Loss: -220905.203125\n",
      "Train Epoch: 147 [46464/54000 (86%)] Loss: -214361.406250\n",
      "Train Epoch: 147 [47872/54000 (89%)] Loss: -214584.218750\n",
      "Train Epoch: 147 [49280/54000 (91%)] Loss: -212479.781250\n",
      "Train Epoch: 147 [50688/54000 (94%)] Loss: -240979.406250\n",
      "Train Epoch: 147 [52096/54000 (96%)] Loss: -209272.390625\n",
      "    epoch          : 147\n",
      "    loss           : -219825.74050538277\n",
      "    val_loss       : -227285.6688619474\n",
      "Train Epoch: 148 [0/54000 (0%)] Loss: -222299.750000\n",
      "Train Epoch: 148 [1408/54000 (3%)] Loss: -243353.265625\n",
      "Train Epoch: 148 [2816/54000 (5%)] Loss: -240676.546875\n",
      "Train Epoch: 148 [4224/54000 (8%)] Loss: -213961.031250\n",
      "Train Epoch: 148 [5632/54000 (10%)] Loss: -225437.312500\n",
      "Train Epoch: 148 [7040/54000 (13%)] Loss: -225496.875000\n",
      "Train Epoch: 148 [8448/54000 (16%)] Loss: -209939.296875\n",
      "Train Epoch: 148 [9856/54000 (18%)] Loss: -209679.406250\n",
      "Train Epoch: 148 [11264/54000 (21%)] Loss: -218842.843750\n",
      "Train Epoch: 148 [12672/54000 (23%)] Loss: -222894.218750\n",
      "Train Epoch: 148 [14080/54000 (26%)] Loss: -219154.671875\n",
      "Train Epoch: 148 [15488/54000 (29%)] Loss: -219746.359375\n",
      "Train Epoch: 148 [16896/54000 (31%)] Loss: -214738.875000\n",
      "Train Epoch: 148 [18304/54000 (34%)] Loss: -225556.046875\n",
      "Train Epoch: 148 [19712/54000 (37%)] Loss: -222473.125000\n",
      "Train Epoch: 148 [21120/54000 (39%)] Loss: -225491.609375\n",
      "Train Epoch: 148 [22528/54000 (42%)] Loss: -211584.640625\n",
      "Train Epoch: 148 [23936/54000 (44%)] Loss: -216180.093750\n",
      "Train Epoch: 148 [25344/54000 (47%)] Loss: -207460.781250\n",
      "Train Epoch: 148 [26752/54000 (50%)] Loss: -210123.921875\n",
      "Train Epoch: 148 [28160/54000 (52%)] Loss: -240924.890625\n",
      "Train Epoch: 148 [29568/54000 (55%)] Loss: -242660.156250\n",
      "Train Epoch: 148 [30976/54000 (57%)] Loss: -212921.406250\n",
      "Train Epoch: 148 [32384/54000 (60%)] Loss: -215470.406250\n",
      "Train Epoch: 148 [33792/54000 (63%)] Loss: -239125.859375\n",
      "Train Epoch: 148 [35200/54000 (65%)] Loss: -212030.875000\n",
      "Train Epoch: 148 [36608/54000 (68%)] Loss: -218767.984375\n",
      "Train Epoch: 148 [38016/54000 (70%)] Loss: -213260.593750\n",
      "Train Epoch: 148 [39424/54000 (73%)] Loss: -223142.078125\n",
      "Train Epoch: 148 [40832/54000 (76%)] Loss: -215885.640625\n",
      "Train Epoch: 148 [42240/54000 (78%)] Loss: -215952.312500\n",
      "Train Epoch: 148 [43648/54000 (81%)] Loss: -214271.093750\n",
      "Train Epoch: 148 [45056/54000 (83%)] Loss: -242628.812500\n",
      "Train Epoch: 148 [46464/54000 (86%)] Loss: -209986.453125\n",
      "Train Epoch: 148 [47872/54000 (89%)] Loss: -212090.343750\n",
      "Train Epoch: 148 [49280/54000 (91%)] Loss: -215274.062500\n",
      "Train Epoch: 148 [50688/54000 (94%)] Loss: -213142.812500\n",
      "Train Epoch: 148 [52096/54000 (96%)] Loss: -211164.437500\n",
      "    epoch          : 148\n",
      "    loss           : -219769.56582685406\n",
      "    val_loss       : -227172.86045279153\n",
      "Train Epoch: 149 [0/54000 (0%)] Loss: -222365.046875\n",
      "Train Epoch: 149 [1408/54000 (3%)] Loss: -207081.468750\n",
      "Train Epoch: 149 [2816/54000 (5%)] Loss: -206342.968750\n",
      "Train Epoch: 149 [4224/54000 (8%)] Loss: -223384.312500\n",
      "Train Epoch: 149 [5632/54000 (10%)] Loss: -214891.750000\n",
      "Train Epoch: 149 [7040/54000 (13%)] Loss: -220205.234375\n",
      "Train Epoch: 149 [8448/54000 (16%)] Loss: -221010.015625\n",
      "Train Epoch: 149 [9856/54000 (18%)] Loss: -211415.687500\n",
      "Train Epoch: 149 [11264/54000 (21%)] Loss: -219128.640625\n",
      "Train Epoch: 149 [12672/54000 (23%)] Loss: -217510.328125\n",
      "Train Epoch: 149 [14080/54000 (26%)] Loss: -226928.156250\n",
      "Train Epoch: 149 [15488/54000 (29%)] Loss: -239814.312500\n",
      "Train Epoch: 149 [16896/54000 (31%)] Loss: -223072.000000\n",
      "Train Epoch: 149 [18304/54000 (34%)] Loss: -241803.312500\n",
      "Train Epoch: 149 [19712/54000 (37%)] Loss: -214768.812500\n",
      "Train Epoch: 149 [21120/54000 (39%)] Loss: -215713.265625\n",
      "Train Epoch: 149 [22528/54000 (42%)] Loss: -226917.468750\n",
      "Train Epoch: 149 [23936/54000 (44%)] Loss: -220489.468750\n",
      "Train Epoch: 149 [25344/54000 (47%)] Loss: -209804.437500\n",
      "Train Epoch: 149 [26752/54000 (50%)] Loss: -222732.093750\n",
      "Train Epoch: 149 [28160/54000 (52%)] Loss: -227390.187500\n",
      "Train Epoch: 149 [29568/54000 (55%)] Loss: -210444.765625\n",
      "Train Epoch: 149 [30976/54000 (57%)] Loss: -215459.625000\n",
      "Train Epoch: 149 [32384/54000 (60%)] Loss: -216040.062500\n",
      "Train Epoch: 149 [33792/54000 (63%)] Loss: -213770.109375\n",
      "Train Epoch: 149 [35200/54000 (65%)] Loss: -224645.312500\n",
      "Train Epoch: 149 [36608/54000 (68%)] Loss: -206573.046875\n",
      "Train Epoch: 149 [38016/54000 (70%)] Loss: -239462.531250\n",
      "Train Epoch: 149 [39424/54000 (73%)] Loss: -219523.921875\n",
      "Train Epoch: 149 [40832/54000 (76%)] Loss: -226123.843750\n",
      "Train Epoch: 149 [42240/54000 (78%)] Loss: -226564.218750\n",
      "Train Epoch: 149 [43648/54000 (81%)] Loss: -222000.515625\n",
      "Train Epoch: 149 [45056/54000 (83%)] Loss: -216761.328125\n",
      "Train Epoch: 149 [46464/54000 (86%)] Loss: -222691.093750\n",
      "Train Epoch: 149 [47872/54000 (89%)] Loss: -226621.781250\n",
      "Train Epoch: 149 [49280/54000 (91%)] Loss: -213034.781250\n",
      "Train Epoch: 149 [50688/54000 (94%)] Loss: -213481.218750\n",
      "Train Epoch: 149 [52096/54000 (96%)] Loss: -214051.203125\n",
      "    epoch          : 149\n",
      "    loss           : -219884.14223235645\n",
      "    val_loss       : -227327.22100681212\n",
      "Train Epoch: 150 [0/54000 (0%)] Loss: -240125.765625\n",
      "Train Epoch: 150 [1408/54000 (3%)] Loss: -213368.781250\n",
      "Train Epoch: 150 [2816/54000 (5%)] Loss: -213294.125000\n",
      "Train Epoch: 150 [4224/54000 (8%)] Loss: -226750.812500\n",
      "Train Epoch: 150 [5632/54000 (10%)] Loss: -215447.734375\n",
      "Train Epoch: 150 [7040/54000 (13%)] Loss: -215779.046875\n",
      "Train Epoch: 150 [8448/54000 (16%)] Loss: -215722.750000\n",
      "Train Epoch: 150 [9856/54000 (18%)] Loss: -218543.734375\n",
      "Train Epoch: 150 [11264/54000 (21%)] Loss: -211424.687500\n",
      "Train Epoch: 150 [12672/54000 (23%)] Loss: -220567.812500\n",
      "Train Epoch: 150 [14080/54000 (26%)] Loss: -218054.000000\n",
      "Train Epoch: 150 [15488/54000 (29%)] Loss: -221952.875000\n",
      "Train Epoch: 150 [16896/54000 (31%)] Loss: -222117.015625\n",
      "Train Epoch: 150 [18304/54000 (34%)] Loss: -221806.093750\n",
      "Train Epoch: 150 [19712/54000 (37%)] Loss: -218243.265625\n",
      "Train Epoch: 150 [21120/54000 (39%)] Loss: -226062.531250\n",
      "Train Epoch: 150 [22528/54000 (42%)] Loss: -206241.171875\n",
      "Train Epoch: 150 [23936/54000 (44%)] Loss: -215576.500000\n",
      "Train Epoch: 150 [25344/54000 (47%)] Loss: -240636.437500\n",
      "Train Epoch: 150 [26752/54000 (50%)] Loss: -227803.000000\n",
      "Train Epoch: 150 [28160/54000 (52%)] Loss: -217081.609375\n",
      "Train Epoch: 150 [29568/54000 (55%)] Loss: -241062.593750\n",
      "Train Epoch: 150 [30976/54000 (57%)] Loss: -216868.062500\n",
      "Train Epoch: 150 [32384/54000 (60%)] Loss: -223478.156250\n",
      "Train Epoch: 150 [33792/54000 (63%)] Loss: -205392.859375\n",
      "Train Epoch: 150 [35200/54000 (65%)] Loss: -206974.531250\n",
      "Train Epoch: 150 [36608/54000 (68%)] Loss: -210944.671875\n",
      "Train Epoch: 150 [38016/54000 (70%)] Loss: -222094.406250\n",
      "Train Epoch: 150 [39424/54000 (73%)] Loss: -215708.453125\n",
      "Train Epoch: 150 [40832/54000 (76%)] Loss: -226950.843750\n",
      "Train Epoch: 150 [42240/54000 (78%)] Loss: -243309.218750\n",
      "Train Epoch: 150 [43648/54000 (81%)] Loss: -215271.062500\n",
      "Train Epoch: 150 [45056/54000 (83%)] Loss: -214993.531250\n",
      "Train Epoch: 150 [46464/54000 (86%)] Loss: -212865.984375\n",
      "Train Epoch: 150 [47872/54000 (89%)] Loss: -220412.031250\n",
      "Train Epoch: 150 [49280/54000 (91%)] Loss: -221442.546875\n",
      "Train Epoch: 150 [50688/54000 (94%)] Loss: -214547.375000\n",
      "Train Epoch: 150 [52096/54000 (96%)] Loss: -209333.937500\n",
      "    epoch          : 150\n",
      "    loss           : -219831.63400867226\n",
      "    val_loss       : -227451.38980325838\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0508_092801/checkpoint-epoch150.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 151 [0/54000 (0%)] Loss: -242284.812500\n",
      "Train Epoch: 151 [1408/54000 (3%)] Loss: -214723.593750\n",
      "Train Epoch: 151 [2816/54000 (5%)] Loss: -226333.906250\n",
      "Train Epoch: 151 [4224/54000 (8%)] Loss: -217971.531250\n",
      "Train Epoch: 151 [5632/54000 (10%)] Loss: -224699.250000\n",
      "Train Epoch: 151 [7040/54000 (13%)] Loss: -224312.359375\n",
      "Train Epoch: 151 [8448/54000 (16%)] Loss: -222614.125000\n",
      "Train Epoch: 151 [9856/54000 (18%)] Loss: -223990.125000\n",
      "Train Epoch: 151 [11264/54000 (21%)] Loss: -208874.515625\n",
      "Train Epoch: 151 [12672/54000 (23%)] Loss: -220826.359375\n",
      "Train Epoch: 151 [14080/54000 (26%)] Loss: -221056.390625\n",
      "Train Epoch: 151 [15488/54000 (29%)] Loss: -216643.031250\n",
      "Train Epoch: 151 [16896/54000 (31%)] Loss: -213223.812500\n",
      "Train Epoch: 151 [18304/54000 (34%)] Loss: -239906.984375\n",
      "Train Epoch: 151 [19712/54000 (37%)] Loss: -225418.062500\n",
      "Train Epoch: 151 [21120/54000 (39%)] Loss: -223698.546875\n",
      "Train Epoch: 151 [22528/54000 (42%)] Loss: -220529.328125\n",
      "Train Epoch: 151 [23936/54000 (44%)] Loss: -227460.062500\n",
      "Train Epoch: 151 [25344/54000 (47%)] Loss: -212135.906250\n",
      "Train Epoch: 151 [26752/54000 (50%)] Loss: -244657.750000\n",
      "Train Epoch: 151 [28160/54000 (52%)] Loss: -218687.625000\n",
      "Train Epoch: 151 [29568/54000 (55%)] Loss: -216175.984375\n",
      "Train Epoch: 151 [30976/54000 (57%)] Loss: -220161.234375\n",
      "Train Epoch: 151 [32384/54000 (60%)] Loss: -219147.781250\n",
      "Train Epoch: 151 [33792/54000 (63%)] Loss: -215849.046875\n",
      "Train Epoch: 151 [35200/54000 (65%)] Loss: -226317.968750\n",
      "Train Epoch: 151 [36608/54000 (68%)] Loss: -218928.984375\n",
      "Train Epoch: 151 [38016/54000 (70%)] Loss: -218280.953125\n",
      "Train Epoch: 151 [39424/54000 (73%)] Loss: -219854.062500\n",
      "Train Epoch: 151 [40832/54000 (76%)] Loss: -219224.781250\n",
      "Train Epoch: 151 [42240/54000 (78%)] Loss: -207738.218750\n",
      "Train Epoch: 151 [43648/54000 (81%)] Loss: -217637.156250\n",
      "Train Epoch: 151 [45056/54000 (83%)] Loss: -240547.062500\n",
      "Train Epoch: 151 [46464/54000 (86%)] Loss: -217388.562500\n",
      "Train Epoch: 151 [47872/54000 (89%)] Loss: -215642.718750\n",
      "Train Epoch: 151 [49280/54000 (91%)] Loss: -240377.640625\n",
      "Train Epoch: 151 [50688/54000 (94%)] Loss: -216056.593750\n",
      "Train Epoch: 151 [52096/54000 (96%)] Loss: -210417.765625\n",
      "    epoch          : 151\n",
      "    loss           : -220009.23620663874\n",
      "    val_loss       : -227163.04375476373\n",
      "Train Epoch: 152 [0/54000 (0%)] Loss: -213485.453125\n",
      "Train Epoch: 152 [1408/54000 (3%)] Loss: -217580.531250\n",
      "Train Epoch: 152 [2816/54000 (5%)] Loss: -218262.906250\n",
      "Train Epoch: 152 [4224/54000 (8%)] Loss: -227576.125000\n",
      "Train Epoch: 152 [5632/54000 (10%)] Loss: -227308.281250\n",
      "Train Epoch: 152 [7040/54000 (13%)] Loss: -216663.000000\n",
      "Train Epoch: 152 [8448/54000 (16%)] Loss: -202959.562500\n",
      "Train Epoch: 152 [9856/54000 (18%)] Loss: -227499.265625\n",
      "Train Epoch: 152 [11264/54000 (21%)] Loss: -214236.484375\n",
      "Train Epoch: 152 [12672/54000 (23%)] Loss: -227987.296875\n",
      "Train Epoch: 152 [14080/54000 (26%)] Loss: -241280.187500\n",
      "Train Epoch: 152 [15488/54000 (29%)] Loss: -211657.875000\n",
      "Train Epoch: 152 [16896/54000 (31%)] Loss: -221070.687500\n",
      "Train Epoch: 152 [18304/54000 (34%)] Loss: -217968.328125\n",
      "Train Epoch: 152 [19712/54000 (37%)] Loss: -223766.359375\n",
      "Train Epoch: 152 [21120/54000 (39%)] Loss: -242787.093750\n",
      "Train Epoch: 152 [22528/54000 (42%)] Loss: -217019.812500\n",
      "Train Epoch: 152 [23936/54000 (44%)] Loss: -218651.609375\n",
      "Train Epoch: 152 [25344/54000 (47%)] Loss: -220206.375000\n",
      "Train Epoch: 152 [26752/54000 (50%)] Loss: -209586.281250\n",
      "Train Epoch: 152 [28160/54000 (52%)] Loss: -216728.656250\n",
      "Train Epoch: 152 [29568/54000 (55%)] Loss: -215848.546875\n",
      "Train Epoch: 152 [30976/54000 (57%)] Loss: -213696.078125\n",
      "Train Epoch: 152 [32384/54000 (60%)] Loss: -225567.906250\n",
      "Train Epoch: 152 [33792/54000 (63%)] Loss: -241150.062500\n",
      "Train Epoch: 152 [35200/54000 (65%)] Loss: -205371.031250\n",
      "Train Epoch: 152 [36608/54000 (68%)] Loss: -240831.625000\n",
      "Train Epoch: 152 [38016/54000 (70%)] Loss: -240081.843750\n",
      "Train Epoch: 152 [39424/54000 (73%)] Loss: -226721.375000\n",
      "Train Epoch: 152 [40832/54000 (76%)] Loss: -221867.953125\n",
      "Train Epoch: 152 [42240/54000 (78%)] Loss: -223979.875000\n",
      "Train Epoch: 152 [43648/54000 (81%)] Loss: -223212.859375\n",
      "Train Epoch: 152 [45056/54000 (83%)] Loss: -221785.234375\n",
      "Train Epoch: 152 [46464/54000 (86%)] Loss: -221768.890625\n",
      "Train Epoch: 152 [47872/54000 (89%)] Loss: -214858.218750\n",
      "Train Epoch: 152 [49280/54000 (91%)] Loss: -216026.343750\n",
      "Train Epoch: 152 [50688/54000 (94%)] Loss: -223624.421875\n",
      "Train Epoch: 152 [52096/54000 (96%)] Loss: -214064.625000\n",
      "    epoch          : 152\n",
      "    loss           : -220041.07655502393\n",
      "    val_loss       : -227265.62682212272\n",
      "Train Epoch: 153 [0/54000 (0%)] Loss: -220586.562500\n",
      "Train Epoch: 153 [1408/54000 (3%)] Loss: -243935.484375\n",
      "Train Epoch: 153 [2816/54000 (5%)] Loss: -209950.125000\n",
      "Train Epoch: 153 [4224/54000 (8%)] Loss: -222744.515625\n",
      "Train Epoch: 153 [5632/54000 (10%)] Loss: -223157.125000\n",
      "Train Epoch: 153 [7040/54000 (13%)] Loss: -223098.750000\n",
      "Train Epoch: 153 [8448/54000 (16%)] Loss: -241066.125000\n",
      "Train Epoch: 153 [9856/54000 (18%)] Loss: -207969.453125\n",
      "Train Epoch: 153 [11264/54000 (21%)] Loss: -218772.578125\n",
      "Train Epoch: 153 [12672/54000 (23%)] Loss: -209923.687500\n",
      "Train Epoch: 153 [14080/54000 (26%)] Loss: -215118.265625\n",
      "Train Epoch: 153 [15488/54000 (29%)] Loss: -242766.062500\n",
      "Train Epoch: 153 [16896/54000 (31%)] Loss: -216224.906250\n",
      "Train Epoch: 153 [18304/54000 (34%)] Loss: -226523.406250\n",
      "Train Epoch: 153 [19712/54000 (37%)] Loss: -222295.765625\n",
      "Train Epoch: 153 [21120/54000 (39%)] Loss: -207578.656250\n",
      "Train Epoch: 153 [22528/54000 (42%)] Loss: -206366.718750\n",
      "Train Epoch: 153 [23936/54000 (44%)] Loss: -211096.218750\n",
      "Train Epoch: 153 [25344/54000 (47%)] Loss: -220288.312500\n",
      "Train Epoch: 153 [26752/54000 (50%)] Loss: -212408.359375\n",
      "Train Epoch: 153 [28160/54000 (52%)] Loss: -240084.515625\n",
      "Train Epoch: 153 [29568/54000 (55%)] Loss: -216966.046875\n",
      "Train Epoch: 153 [30976/54000 (57%)] Loss: -217569.625000\n",
      "Train Epoch: 153 [32384/54000 (60%)] Loss: -221460.437500\n",
      "Train Epoch: 153 [33792/54000 (63%)] Loss: -217334.328125\n",
      "Train Epoch: 153 [35200/54000 (65%)] Loss: -244142.484375\n",
      "Train Epoch: 153 [36608/54000 (68%)] Loss: -227044.937500\n",
      "Train Epoch: 153 [38016/54000 (70%)] Loss: -206186.046875\n",
      "Train Epoch: 153 [39424/54000 (73%)] Loss: -239784.656250\n",
      "Train Epoch: 153 [40832/54000 (76%)] Loss: -222270.890625\n",
      "Train Epoch: 153 [42240/54000 (78%)] Loss: -213894.531250\n",
      "Train Epoch: 153 [43648/54000 (81%)] Loss: -215471.000000\n",
      "Train Epoch: 153 [45056/54000 (83%)] Loss: -222456.375000\n",
      "Train Epoch: 153 [46464/54000 (86%)] Loss: -216044.281250\n",
      "Train Epoch: 153 [47872/54000 (89%)] Loss: -213475.312500\n",
      "Train Epoch: 153 [49280/54000 (91%)] Loss: -240791.218750\n",
      "Train Epoch: 153 [50688/54000 (94%)] Loss: -243311.375000\n",
      "Train Epoch: 153 [52096/54000 (96%)] Loss: -225453.843750\n",
      "    epoch          : 153\n",
      "    loss           : -220150.7137410287\n",
      "    val_loss       : -226819.6093273628\n",
      "Train Epoch: 154 [0/54000 (0%)] Loss: -221897.046875\n",
      "Train Epoch: 154 [1408/54000 (3%)] Loss: -244147.718750\n",
      "Train Epoch: 154 [2816/54000 (5%)] Loss: -214533.531250\n",
      "Train Epoch: 154 [4224/54000 (8%)] Loss: -227028.843750\n",
      "Train Epoch: 154 [5632/54000 (10%)] Loss: -220198.890625\n",
      "Train Epoch: 154 [7040/54000 (13%)] Loss: -210397.765625\n",
      "Train Epoch: 154 [8448/54000 (16%)] Loss: -211181.968750\n",
      "Train Epoch: 154 [9856/54000 (18%)] Loss: -241482.203125\n",
      "Train Epoch: 154 [11264/54000 (21%)] Loss: -224967.140625\n",
      "Train Epoch: 154 [12672/54000 (23%)] Loss: -213899.312500\n",
      "Train Epoch: 154 [14080/54000 (26%)] Loss: -227262.859375\n",
      "Train Epoch: 154 [15488/54000 (29%)] Loss: -223828.156250\n",
      "Train Epoch: 154 [16896/54000 (31%)] Loss: -227053.687500\n",
      "Train Epoch: 154 [18304/54000 (34%)] Loss: -217442.078125\n",
      "Train Epoch: 154 [19712/54000 (37%)] Loss: -214179.078125\n",
      "Train Epoch: 154 [21120/54000 (39%)] Loss: -215372.703125\n",
      "Train Epoch: 154 [22528/54000 (42%)] Loss: -226506.687500\n",
      "Train Epoch: 154 [23936/54000 (44%)] Loss: -208707.125000\n",
      "Train Epoch: 154 [25344/54000 (47%)] Loss: -213903.343750\n",
      "Train Epoch: 154 [26752/54000 (50%)] Loss: -214061.593750\n",
      "Train Epoch: 154 [28160/54000 (52%)] Loss: -224297.312500\n",
      "Train Epoch: 154 [29568/54000 (55%)] Loss: -218718.703125\n",
      "Train Epoch: 154 [30976/54000 (57%)] Loss: -242925.171875\n",
      "Train Epoch: 154 [32384/54000 (60%)] Loss: -215875.703125\n",
      "Train Epoch: 154 [33792/54000 (63%)] Loss: -211570.562500\n",
      "Train Epoch: 154 [35200/54000 (65%)] Loss: -214819.125000\n",
      "Train Epoch: 154 [36608/54000 (68%)] Loss: -224190.937500\n",
      "Train Epoch: 154 [38016/54000 (70%)] Loss: -213302.171875\n",
      "Train Epoch: 154 [39424/54000 (73%)] Loss: -211036.187500\n",
      "Train Epoch: 154 [40832/54000 (76%)] Loss: -222648.921875\n",
      "Train Epoch: 154 [42240/54000 (78%)] Loss: -223724.406250\n",
      "Train Epoch: 154 [43648/54000 (81%)] Loss: -220475.453125\n",
      "Train Epoch: 154 [45056/54000 (83%)] Loss: -242924.937500\n",
      "Train Epoch: 154 [46464/54000 (86%)] Loss: -217251.781250\n",
      "Train Epoch: 154 [47872/54000 (89%)] Loss: -215693.562500\n",
      "Train Epoch: 154 [49280/54000 (91%)] Loss: -214434.062500\n",
      "Train Epoch: 154 [50688/54000 (94%)] Loss: -240821.359375\n",
      "Train Epoch: 154 [52096/54000 (96%)] Loss: -221391.609375\n",
      "    epoch          : 154\n",
      "    loss           : -220222.44819078947\n",
      "    val_loss       : -227353.93203363186\n",
      "Train Epoch: 155 [0/54000 (0%)] Loss: -243240.859375\n",
      "Train Epoch: 155 [1408/54000 (3%)] Loss: -212242.437500\n",
      "Train Epoch: 155 [2816/54000 (5%)] Loss: -214523.140625\n",
      "Train Epoch: 155 [4224/54000 (8%)] Loss: -211691.687500\n",
      "Train Epoch: 155 [5632/54000 (10%)] Loss: -217678.546875\n",
      "Train Epoch: 155 [7040/54000 (13%)] Loss: -216001.453125\n",
      "Train Epoch: 155 [8448/54000 (16%)] Loss: -211646.031250\n",
      "Train Epoch: 155 [9856/54000 (18%)] Loss: -218411.125000\n",
      "Train Epoch: 155 [11264/54000 (21%)] Loss: -222330.109375\n",
      "Train Epoch: 155 [12672/54000 (23%)] Loss: -224084.312500\n",
      "Train Epoch: 155 [14080/54000 (26%)] Loss: -210187.484375\n",
      "Train Epoch: 155 [15488/54000 (29%)] Loss: -217100.500000\n",
      "Train Epoch: 155 [16896/54000 (31%)] Loss: -242486.531250\n",
      "Train Epoch: 155 [18304/54000 (34%)] Loss: -214365.390625\n",
      "Train Epoch: 155 [19712/54000 (37%)] Loss: -226005.000000\n",
      "Train Epoch: 155 [21120/54000 (39%)] Loss: -226316.390625\n",
      "Train Epoch: 155 [22528/54000 (42%)] Loss: -243663.468750\n",
      "Train Epoch: 155 [23936/54000 (44%)] Loss: -214547.171875\n",
      "Train Epoch: 155 [25344/54000 (47%)] Loss: -214630.437500\n",
      "Train Epoch: 155 [26752/54000 (50%)] Loss: -215746.156250\n",
      "Train Epoch: 155 [28160/54000 (52%)] Loss: -216244.468750\n",
      "Train Epoch: 155 [29568/54000 (55%)] Loss: -244165.125000\n",
      "Train Epoch: 155 [30976/54000 (57%)] Loss: -212153.156250\n",
      "Train Epoch: 155 [32384/54000 (60%)] Loss: -210923.562500\n",
      "Train Epoch: 155 [33792/54000 (63%)] Loss: -210320.625000\n",
      "Train Epoch: 155 [35200/54000 (65%)] Loss: -212816.171875\n",
      "Train Epoch: 155 [36608/54000 (68%)] Loss: -217104.750000\n",
      "Train Epoch: 155 [38016/54000 (70%)] Loss: -218385.187500\n",
      "Train Epoch: 155 [39424/54000 (73%)] Loss: -216973.703125\n",
      "Train Epoch: 155 [40832/54000 (76%)] Loss: -218573.187500\n",
      "Train Epoch: 155 [42240/54000 (78%)] Loss: -211192.578125\n",
      "Train Epoch: 155 [43648/54000 (81%)] Loss: -222447.062500\n",
      "Train Epoch: 155 [45056/54000 (83%)] Loss: -216732.609375\n",
      "Train Epoch: 155 [46464/54000 (86%)] Loss: -214512.734375\n",
      "Train Epoch: 155 [47872/54000 (89%)] Loss: -211160.296875\n",
      "Train Epoch: 155 [49280/54000 (91%)] Loss: -216827.937500\n",
      "Train Epoch: 155 [50688/54000 (94%)] Loss: -242068.000000\n",
      "Train Epoch: 155 [52096/54000 (96%)] Loss: -207682.734375\n",
      "    epoch          : 155\n",
      "    loss           : -220257.17049192585\n",
      "    val_loss       : -227499.2372570503\n",
      "Train Epoch: 156 [0/54000 (0%)] Loss: -241914.406250\n",
      "Train Epoch: 156 [1408/54000 (3%)] Loss: -208831.828125\n",
      "Train Epoch: 156 [2816/54000 (5%)] Loss: -210776.703125\n",
      "Train Epoch: 156 [4224/54000 (8%)] Loss: -224579.140625\n",
      "Train Epoch: 156 [5632/54000 (10%)] Loss: -223443.890625\n",
      "Train Epoch: 156 [7040/54000 (13%)] Loss: -216747.812500\n",
      "Train Epoch: 156 [8448/54000 (16%)] Loss: -218932.125000\n",
      "Train Epoch: 156 [9856/54000 (18%)] Loss: -215436.109375\n",
      "Train Epoch: 156 [11264/54000 (21%)] Loss: -214464.406250\n",
      "Train Epoch: 156 [12672/54000 (23%)] Loss: -218401.109375\n",
      "Train Epoch: 156 [14080/54000 (26%)] Loss: -243119.968750\n",
      "Train Epoch: 156 [15488/54000 (29%)] Loss: -223920.500000\n",
      "Train Epoch: 156 [16896/54000 (31%)] Loss: -225595.515625\n",
      "Train Epoch: 156 [18304/54000 (34%)] Loss: -210770.546875\n",
      "Train Epoch: 156 [19712/54000 (37%)] Loss: -243541.531250\n",
      "Train Epoch: 156 [21120/54000 (39%)] Loss: -227073.468750\n",
      "Train Epoch: 156 [22528/54000 (42%)] Loss: -225974.843750\n",
      "Train Epoch: 156 [23936/54000 (44%)] Loss: -227222.468750\n",
      "Train Epoch: 156 [25344/54000 (47%)] Loss: -223014.921875\n",
      "Train Epoch: 156 [26752/54000 (50%)] Loss: -217644.484375\n",
      "Train Epoch: 156 [28160/54000 (52%)] Loss: -205504.906250\n",
      "Train Epoch: 156 [29568/54000 (55%)] Loss: -212016.843750\n",
      "Train Epoch: 156 [30976/54000 (57%)] Loss: -210240.343750\n",
      "Train Epoch: 156 [32384/54000 (60%)] Loss: -212138.765625\n",
      "Train Epoch: 156 [33792/54000 (63%)] Loss: -214139.078125\n",
      "Train Epoch: 156 [35200/54000 (65%)] Loss: -241478.437500\n",
      "Train Epoch: 156 [36608/54000 (68%)] Loss: -221930.421875\n",
      "Train Epoch: 156 [38016/54000 (70%)] Loss: -215732.500000\n",
      "Train Epoch: 156 [39424/54000 (73%)] Loss: -242353.468750\n",
      "Train Epoch: 156 [40832/54000 (76%)] Loss: -227450.531250\n",
      "Train Epoch: 156 [42240/54000 (78%)] Loss: -224854.906250\n",
      "Train Epoch: 156 [43648/54000 (81%)] Loss: -219028.812500\n",
      "Train Epoch: 156 [45056/54000 (83%)] Loss: -207724.781250\n",
      "Train Epoch: 156 [46464/54000 (86%)] Loss: -240360.625000\n",
      "Train Epoch: 156 [47872/54000 (89%)] Loss: -213862.062500\n",
      "Train Epoch: 156 [49280/54000 (91%)] Loss: -215579.843750\n",
      "Train Epoch: 156 [50688/54000 (94%)] Loss: -221830.671875\n",
      "Train Epoch: 156 [52096/54000 (96%)] Loss: -209519.156250\n",
      "    epoch          : 156\n",
      "    loss           : -220362.89447517943\n",
      "    val_loss       : -227325.1377072218\n",
      "Train Epoch: 157 [0/54000 (0%)] Loss: -240216.250000\n",
      "Train Epoch: 157 [1408/54000 (3%)] Loss: -224116.062500\n",
      "Train Epoch: 157 [2816/54000 (5%)] Loss: -224318.171875\n",
      "Train Epoch: 157 [4224/54000 (8%)] Loss: -208808.250000\n",
      "Train Epoch: 157 [5632/54000 (10%)] Loss: -216351.390625\n",
      "Train Epoch: 157 [7040/54000 (13%)] Loss: -217212.843750\n",
      "Train Epoch: 157 [8448/54000 (16%)] Loss: -218887.187500\n",
      "Train Epoch: 157 [9856/54000 (18%)] Loss: -209795.609375\n",
      "Train Epoch: 157 [11264/54000 (21%)] Loss: -218057.562500\n",
      "Train Epoch: 157 [12672/54000 (23%)] Loss: -223572.968750\n",
      "Train Epoch: 157 [14080/54000 (26%)] Loss: -213731.343750\n",
      "Train Epoch: 157 [15488/54000 (29%)] Loss: -242982.750000\n",
      "Train Epoch: 157 [16896/54000 (31%)] Loss: -215984.750000\n",
      "Train Epoch: 157 [18304/54000 (34%)] Loss: -227150.046875\n",
      "Train Epoch: 157 [19712/54000 (37%)] Loss: -220027.125000\n",
      "Train Epoch: 157 [21120/54000 (39%)] Loss: -222031.906250\n",
      "Train Epoch: 157 [22528/54000 (42%)] Loss: -211862.671875\n",
      "Train Epoch: 157 [23936/54000 (44%)] Loss: -213499.875000\n",
      "Train Epoch: 157 [25344/54000 (47%)] Loss: -212970.687500\n",
      "Train Epoch: 157 [26752/54000 (50%)] Loss: -213813.312500\n",
      "Train Epoch: 157 [28160/54000 (52%)] Loss: -206365.578125\n",
      "Train Epoch: 157 [29568/54000 (55%)] Loss: -215873.734375\n",
      "Train Epoch: 157 [30976/54000 (57%)] Loss: -217873.656250\n",
      "Train Epoch: 157 [32384/54000 (60%)] Loss: -243315.734375\n",
      "Train Epoch: 157 [33792/54000 (63%)] Loss: -225769.875000\n",
      "Train Epoch: 157 [35200/54000 (65%)] Loss: -227546.406250\n",
      "Train Epoch: 157 [36608/54000 (68%)] Loss: -212353.937500\n",
      "Train Epoch: 157 [38016/54000 (70%)] Loss: -217827.375000\n",
      "Train Epoch: 157 [39424/54000 (73%)] Loss: -212193.375000\n",
      "Train Epoch: 157 [40832/54000 (76%)] Loss: -215077.562500\n",
      "Train Epoch: 157 [42240/54000 (78%)] Loss: -214466.656250\n",
      "Train Epoch: 157 [43648/54000 (81%)] Loss: -218315.109375\n",
      "Train Epoch: 157 [45056/54000 (83%)] Loss: -222883.343750\n",
      "Train Epoch: 157 [46464/54000 (86%)] Loss: -215275.531250\n",
      "Train Epoch: 157 [47872/54000 (89%)] Loss: -222800.625000\n",
      "Train Epoch: 157 [49280/54000 (91%)] Loss: -217884.718750\n",
      "Train Epoch: 157 [50688/54000 (94%)] Loss: -207423.453125\n",
      "Train Epoch: 157 [52096/54000 (96%)] Loss: -223698.421875\n",
      "    epoch          : 157\n",
      "    loss           : -220360.47716058613\n",
      "    val_loss       : -227545.72477015053\n",
      "Train Epoch: 158 [0/54000 (0%)] Loss: -240497.125000\n",
      "Train Epoch: 158 [1408/54000 (3%)] Loss: -219276.203125\n",
      "Train Epoch: 158 [2816/54000 (5%)] Loss: -219441.156250\n",
      "Train Epoch: 158 [4224/54000 (8%)] Loss: -215450.031250\n",
      "Train Epoch: 158 [5632/54000 (10%)] Loss: -223337.781250\n",
      "Train Epoch: 158 [7040/54000 (13%)] Loss: -213080.343750\n",
      "Train Epoch: 158 [8448/54000 (16%)] Loss: -210890.500000\n",
      "Train Epoch: 158 [9856/54000 (18%)] Loss: -243189.265625\n",
      "Train Epoch: 158 [11264/54000 (21%)] Loss: -216594.562500\n",
      "Train Epoch: 158 [12672/54000 (23%)] Loss: -216787.156250\n",
      "Train Epoch: 158 [14080/54000 (26%)] Loss: -215877.187500\n",
      "Train Epoch: 158 [15488/54000 (29%)] Loss: -206783.078125\n",
      "Train Epoch: 158 [16896/54000 (31%)] Loss: -212594.468750\n",
      "Train Epoch: 158 [18304/54000 (34%)] Loss: -210665.015625\n",
      "Train Epoch: 158 [19712/54000 (37%)] Loss: -226338.640625\n",
      "Train Epoch: 158 [21120/54000 (39%)] Loss: -227597.406250\n",
      "Train Epoch: 158 [22528/54000 (42%)] Loss: -222175.390625\n",
      "Train Epoch: 158 [23936/54000 (44%)] Loss: -216438.765625\n",
      "Train Epoch: 158 [25344/54000 (47%)] Loss: -214550.546875\n",
      "Train Epoch: 158 [26752/54000 (50%)] Loss: -241719.312500\n",
      "Train Epoch: 158 [28160/54000 (52%)] Loss: -214521.218750\n",
      "Train Epoch: 158 [29568/54000 (55%)] Loss: -215147.625000\n",
      "Train Epoch: 158 [30976/54000 (57%)] Loss: -210070.843750\n",
      "Train Epoch: 158 [32384/54000 (60%)] Loss: -223043.328125\n",
      "Train Epoch: 158 [33792/54000 (63%)] Loss: -243074.625000\n",
      "Train Epoch: 158 [35200/54000 (65%)] Loss: -212241.031250\n",
      "Train Epoch: 158 [36608/54000 (68%)] Loss: -218596.656250\n",
      "Train Epoch: 158 [38016/54000 (70%)] Loss: -222374.765625\n",
      "Train Epoch: 158 [39424/54000 (73%)] Loss: -217772.703125\n",
      "Train Epoch: 158 [40832/54000 (76%)] Loss: -213425.031250\n",
      "Train Epoch: 158 [42240/54000 (78%)] Loss: -219816.171875\n",
      "Train Epoch: 158 [43648/54000 (81%)] Loss: -217978.593750\n",
      "Train Epoch: 158 [45056/54000 (83%)] Loss: -243123.093750\n",
      "Train Epoch: 158 [46464/54000 (86%)] Loss: -238653.828125\n",
      "Train Epoch: 158 [47872/54000 (89%)] Loss: -218328.687500\n",
      "Train Epoch: 158 [49280/54000 (91%)] Loss: -224443.093750\n",
      "Train Epoch: 158 [50688/54000 (94%)] Loss: -241735.984375\n",
      "Train Epoch: 158 [52096/54000 (96%)] Loss: -213858.046875\n",
      "    epoch          : 158\n",
      "    loss           : -220509.23605711723\n",
      "    val_loss       : -227112.06523913873\n",
      "Train Epoch: 159 [0/54000 (0%)] Loss: -242808.500000\n",
      "Train Epoch: 159 [1408/54000 (3%)] Loss: -213445.093750\n",
      "Train Epoch: 159 [2816/54000 (5%)] Loss: -222855.062500\n",
      "Train Epoch: 159 [4224/54000 (8%)] Loss: -218249.406250\n",
      "Train Epoch: 159 [5632/54000 (10%)] Loss: -211590.984375\n",
      "Train Epoch: 159 [7040/54000 (13%)] Loss: -215219.171875\n",
      "Train Epoch: 159 [8448/54000 (16%)] Loss: -211605.375000\n",
      "Train Epoch: 159 [9856/54000 (18%)] Loss: -226335.796875\n",
      "Train Epoch: 159 [11264/54000 (21%)] Loss: -215685.546875\n",
      "Train Epoch: 159 [12672/54000 (23%)] Loss: -226349.125000\n",
      "Train Epoch: 159 [14080/54000 (26%)] Loss: -216727.531250\n",
      "Train Epoch: 159 [15488/54000 (29%)] Loss: -218123.265625\n",
      "Train Epoch: 159 [16896/54000 (31%)] Loss: -222358.156250\n",
      "Train Epoch: 159 [18304/54000 (34%)] Loss: -206196.390625\n",
      "Train Epoch: 159 [19712/54000 (37%)] Loss: -212756.609375\n",
      "Train Epoch: 159 [21120/54000 (39%)] Loss: -218082.734375\n",
      "Train Epoch: 159 [22528/54000 (42%)] Loss: -214636.531250\n",
      "Train Epoch: 159 [23936/54000 (44%)] Loss: -215537.875000\n",
      "Train Epoch: 159 [25344/54000 (47%)] Loss: -213719.828125\n",
      "Train Epoch: 159 [26752/54000 (50%)] Loss: -225585.734375\n",
      "Train Epoch: 159 [28160/54000 (52%)] Loss: -217464.562500\n",
      "Train Epoch: 159 [29568/54000 (55%)] Loss: -217535.375000\n",
      "Train Epoch: 159 [30976/54000 (57%)] Loss: -220178.828125\n",
      "Train Epoch: 159 [32384/54000 (60%)] Loss: -220508.296875\n",
      "Train Epoch: 159 [33792/54000 (63%)] Loss: -221045.093750\n",
      "Train Epoch: 159 [35200/54000 (65%)] Loss: -211062.718750\n",
      "Train Epoch: 159 [36608/54000 (68%)] Loss: -214217.796875\n",
      "Train Epoch: 159 [38016/54000 (70%)] Loss: -241783.734375\n",
      "Train Epoch: 159 [39424/54000 (73%)] Loss: -207833.406250\n",
      "Train Epoch: 159 [40832/54000 (76%)] Loss: -208919.375000\n",
      "Train Epoch: 159 [42240/54000 (78%)] Loss: -227427.578125\n",
      "Train Epoch: 159 [43648/54000 (81%)] Loss: -222596.109375\n",
      "Train Epoch: 159 [45056/54000 (83%)] Loss: -223694.640625\n",
      "Train Epoch: 159 [46464/54000 (86%)] Loss: -220646.671875\n",
      "Train Epoch: 159 [47872/54000 (89%)] Loss: -218756.218750\n",
      "Train Epoch: 159 [49280/54000 (91%)] Loss: -220243.671875\n",
      "Train Epoch: 159 [50688/54000 (94%)] Loss: -209546.531250\n",
      "Train Epoch: 159 [52096/54000 (96%)] Loss: -209111.796875\n",
      "    epoch          : 159\n",
      "    loss           : -220421.3739159689\n",
      "    val_loss       : -227307.62593487996\n",
      "Train Epoch: 160 [0/54000 (0%)] Loss: -226047.234375\n",
      "Train Epoch: 160 [1408/54000 (3%)] Loss: -221939.078125\n",
      "Train Epoch: 160 [2816/54000 (5%)] Loss: -224226.250000\n",
      "Train Epoch: 160 [4224/54000 (8%)] Loss: -210009.718750\n",
      "Train Epoch: 160 [5632/54000 (10%)] Loss: -217409.671875\n",
      "Train Epoch: 160 [7040/54000 (13%)] Loss: -217299.671875\n",
      "Train Epoch: 160 [8448/54000 (16%)] Loss: -225605.906250\n",
      "Train Epoch: 160 [9856/54000 (18%)] Loss: -224155.078125\n",
      "Train Epoch: 160 [11264/54000 (21%)] Loss: -215561.625000\n",
      "Train Epoch: 160 [12672/54000 (23%)] Loss: -213427.781250\n",
      "Train Epoch: 160 [14080/54000 (26%)] Loss: -213500.687500\n",
      "Train Epoch: 160 [15488/54000 (29%)] Loss: -225588.031250\n",
      "Train Epoch: 160 [16896/54000 (31%)] Loss: -243819.281250\n",
      "Train Epoch: 160 [18304/54000 (34%)] Loss: -227935.500000\n",
      "Train Epoch: 160 [19712/54000 (37%)] Loss: -211194.625000\n",
      "Train Epoch: 160 [21120/54000 (39%)] Loss: -226714.406250\n",
      "Train Epoch: 160 [22528/54000 (42%)] Loss: -208013.546875\n",
      "Train Epoch: 160 [23936/54000 (44%)] Loss: -214451.421875\n",
      "Train Epoch: 160 [25344/54000 (47%)] Loss: -217034.656250\n",
      "Train Epoch: 160 [26752/54000 (50%)] Loss: -219449.718750\n",
      "Train Epoch: 160 [28160/54000 (52%)] Loss: -218588.218750\n",
      "Train Epoch: 160 [29568/54000 (55%)] Loss: -213942.468750\n",
      "Train Epoch: 160 [30976/54000 (57%)] Loss: -212638.875000\n",
      "Train Epoch: 160 [32384/54000 (60%)] Loss: -218128.656250\n",
      "Train Epoch: 160 [33792/54000 (63%)] Loss: -225531.890625\n",
      "Train Epoch: 160 [35200/54000 (65%)] Loss: -213444.609375\n",
      "Train Epoch: 160 [36608/54000 (68%)] Loss: -225928.531250\n",
      "Train Epoch: 160 [38016/54000 (70%)] Loss: -216390.312500\n",
      "Train Epoch: 160 [39424/54000 (73%)] Loss: -209866.687500\n",
      "Train Epoch: 160 [40832/54000 (76%)] Loss: -212624.406250\n",
      "Train Epoch: 160 [42240/54000 (78%)] Loss: -223437.109375\n",
      "Train Epoch: 160 [43648/54000 (81%)] Loss: -215120.234375\n",
      "Train Epoch: 160 [45056/54000 (83%)] Loss: -218686.593750\n",
      "Train Epoch: 160 [46464/54000 (86%)] Loss: -243276.968750\n",
      "Train Epoch: 160 [47872/54000 (89%)] Loss: -218280.031250\n",
      "Train Epoch: 160 [49280/54000 (91%)] Loss: -218487.312500\n",
      "Train Epoch: 160 [50688/54000 (94%)] Loss: -224266.312500\n",
      "Train Epoch: 160 [52096/54000 (96%)] Loss: -241369.000000\n",
      "    epoch          : 160\n",
      "    loss           : -220731.7063770933\n",
      "    val_loss       : -227145.41083508002\n",
      "Train Epoch: 161 [0/54000 (0%)] Loss: -222105.171875\n",
      "Train Epoch: 161 [1408/54000 (3%)] Loss: -227291.031250\n",
      "Train Epoch: 161 [2816/54000 (5%)] Loss: -221348.531250\n",
      "Train Epoch: 161 [4224/54000 (8%)] Loss: -221046.140625\n",
      "Train Epoch: 161 [5632/54000 (10%)] Loss: -209855.015625\n",
      "Train Epoch: 161 [7040/54000 (13%)] Loss: -212419.218750\n",
      "Train Epoch: 161 [8448/54000 (16%)] Loss: -213651.468750\n",
      "Train Epoch: 161 [9856/54000 (18%)] Loss: -219632.000000\n",
      "Train Epoch: 161 [11264/54000 (21%)] Loss: -243380.453125\n",
      "Train Epoch: 161 [12672/54000 (23%)] Loss: -218213.125000\n",
      "Train Epoch: 161 [14080/54000 (26%)] Loss: -217999.265625\n",
      "Train Epoch: 161 [15488/54000 (29%)] Loss: -213921.312500\n",
      "Train Epoch: 161 [16896/54000 (31%)] Loss: -216692.078125\n",
      "Train Epoch: 161 [18304/54000 (34%)] Loss: -245200.031250\n",
      "Train Epoch: 161 [19712/54000 (37%)] Loss: -228288.828125\n",
      "Train Epoch: 161 [21120/54000 (39%)] Loss: -213856.359375\n",
      "Train Epoch: 161 [22528/54000 (42%)] Loss: -219142.265625\n",
      "Train Epoch: 161 [23936/54000 (44%)] Loss: -208186.546875\n",
      "Train Epoch: 161 [25344/54000 (47%)] Loss: -224937.796875\n",
      "Train Epoch: 161 [26752/54000 (50%)] Loss: -205344.406250\n",
      "Train Epoch: 161 [28160/54000 (52%)] Loss: -218342.765625\n",
      "Train Epoch: 161 [29568/54000 (55%)] Loss: -215572.500000\n",
      "Train Epoch: 161 [30976/54000 (57%)] Loss: -217777.906250\n",
      "Train Epoch: 161 [32384/54000 (60%)] Loss: -222870.250000\n",
      "Train Epoch: 161 [33792/54000 (63%)] Loss: -220268.125000\n",
      "Train Epoch: 161 [35200/54000 (65%)] Loss: -217317.796875\n",
      "Train Epoch: 161 [36608/54000 (68%)] Loss: -217104.765625\n",
      "Train Epoch: 161 [38016/54000 (70%)] Loss: -244123.187500\n",
      "Train Epoch: 161 [39424/54000 (73%)] Loss: -225249.171875\n",
      "Train Epoch: 161 [40832/54000 (76%)] Loss: -210461.140625\n",
      "Train Epoch: 161 [42240/54000 (78%)] Loss: -226817.531250\n",
      "Train Epoch: 161 [43648/54000 (81%)] Loss: -215958.687500\n",
      "Train Epoch: 161 [45056/54000 (83%)] Loss: -242575.125000\n",
      "Train Epoch: 161 [46464/54000 (86%)] Loss: -212591.656250\n",
      "Train Epoch: 161 [47872/54000 (89%)] Loss: -213034.421875\n",
      "Train Epoch: 161 [49280/54000 (91%)] Loss: -212077.578125\n",
      "Train Epoch: 161 [50688/54000 (94%)] Loss: -242055.453125\n",
      "Train Epoch: 161 [52096/54000 (96%)] Loss: -223793.609375\n",
      "    epoch          : 161\n",
      "    loss           : -220693.15681070575\n",
      "    val_loss       : -226865.18408203125\n",
      "Train Epoch: 162 [0/54000 (0%)] Loss: -220010.437500\n",
      "Train Epoch: 162 [1408/54000 (3%)] Loss: -215576.906250\n",
      "Train Epoch: 162 [2816/54000 (5%)] Loss: -225909.640625\n",
      "Train Epoch: 162 [4224/54000 (8%)] Loss: -224803.015625\n",
      "Train Epoch: 162 [5632/54000 (10%)] Loss: -224805.218750\n",
      "Train Epoch: 162 [7040/54000 (13%)] Loss: -225127.171875\n",
      "Train Epoch: 162 [8448/54000 (16%)] Loss: -209943.453125\n",
      "Train Epoch: 162 [9856/54000 (18%)] Loss: -206893.406250\n",
      "Train Epoch: 162 [11264/54000 (21%)] Loss: -220245.375000\n",
      "Train Epoch: 162 [12672/54000 (23%)] Loss: -215741.718750\n",
      "Train Epoch: 162 [14080/54000 (26%)] Loss: -223308.187500\n",
      "Train Epoch: 162 [15488/54000 (29%)] Loss: -241600.421875\n",
      "Train Epoch: 162 [16896/54000 (31%)] Loss: -217399.031250\n",
      "Train Epoch: 162 [18304/54000 (34%)] Loss: -221431.593750\n",
      "Train Epoch: 162 [19712/54000 (37%)] Loss: -242141.250000\n",
      "Train Epoch: 162 [21120/54000 (39%)] Loss: -206489.031250\n",
      "Train Epoch: 162 [22528/54000 (42%)] Loss: -205368.343750\n",
      "Train Epoch: 162 [23936/54000 (44%)] Loss: -213844.687500\n",
      "Train Epoch: 162 [25344/54000 (47%)] Loss: -208716.546875\n",
      "Train Epoch: 162 [26752/54000 (50%)] Loss: -216942.687500\n",
      "Train Epoch: 162 [28160/54000 (52%)] Loss: -216609.156250\n",
      "Train Epoch: 162 [29568/54000 (55%)] Loss: -217805.265625\n",
      "Train Epoch: 162 [30976/54000 (57%)] Loss: -242020.578125\n",
      "Train Epoch: 162 [32384/54000 (60%)] Loss: -214413.531250\n",
      "Train Epoch: 162 [33792/54000 (63%)] Loss: -226762.515625\n",
      "Train Epoch: 162 [35200/54000 (65%)] Loss: -241757.828125\n",
      "Train Epoch: 162 [36608/54000 (68%)] Loss: -204019.531250\n",
      "Train Epoch: 162 [38016/54000 (70%)] Loss: -211367.406250\n",
      "Train Epoch: 162 [39424/54000 (73%)] Loss: -221676.890625\n",
      "Train Epoch: 162 [40832/54000 (76%)] Loss: -226257.828125\n",
      "Train Epoch: 162 [42240/54000 (78%)] Loss: -218720.281250\n",
      "Train Epoch: 162 [43648/54000 (81%)] Loss: -215885.843750\n",
      "Train Epoch: 162 [45056/54000 (83%)] Loss: -216842.875000\n",
      "Train Epoch: 162 [46464/54000 (86%)] Loss: -214093.500000\n",
      "Train Epoch: 162 [47872/54000 (89%)] Loss: -221434.859375\n",
      "Train Epoch: 162 [49280/54000 (91%)] Loss: -218072.093750\n",
      "Train Epoch: 162 [50688/54000 (94%)] Loss: -222195.250000\n",
      "Train Epoch: 162 [52096/54000 (96%)] Loss: -223818.406250\n",
      "    epoch          : 162\n",
      "    loss           : -220637.86584180623\n",
      "    val_loss       : -227264.11625262004\n",
      "Train Epoch: 163 [0/54000 (0%)] Loss: -219474.843750\n",
      "Train Epoch: 163 [1408/54000 (3%)] Loss: -221653.531250\n",
      "Train Epoch: 163 [2816/54000 (5%)] Loss: -216626.937500\n",
      "Train Epoch: 163 [4224/54000 (8%)] Loss: -216737.671875\n",
      "Train Epoch: 163 [5632/54000 (10%)] Loss: -210007.375000\n",
      "Train Epoch: 163 [7040/54000 (13%)] Loss: -217681.578125\n",
      "Train Epoch: 163 [8448/54000 (16%)] Loss: -226339.562500\n",
      "Train Epoch: 163 [9856/54000 (18%)] Loss: -227562.625000\n",
      "Train Epoch: 163 [11264/54000 (21%)] Loss: -214621.625000\n",
      "Train Epoch: 163 [12672/54000 (23%)] Loss: -225340.437500\n",
      "Train Epoch: 163 [14080/54000 (26%)] Loss: -224374.375000\n",
      "Train Epoch: 163 [15488/54000 (29%)] Loss: -215656.125000\n",
      "Train Epoch: 163 [16896/54000 (31%)] Loss: -217322.375000\n",
      "Train Epoch: 163 [18304/54000 (34%)] Loss: -223013.703125\n",
      "Train Epoch: 163 [19712/54000 (37%)] Loss: -226427.125000\n",
      "Train Epoch: 163 [21120/54000 (39%)] Loss: -226746.031250\n",
      "Train Epoch: 163 [22528/54000 (42%)] Loss: -216756.078125\n",
      "Train Epoch: 163 [23936/54000 (44%)] Loss: -227442.968750\n",
      "Train Epoch: 163 [25344/54000 (47%)] Loss: -210524.281250\n",
      "Train Epoch: 163 [26752/54000 (50%)] Loss: -222525.187500\n",
      "Train Epoch: 163 [28160/54000 (52%)] Loss: -219492.718750\n",
      "Train Epoch: 163 [29568/54000 (55%)] Loss: -223592.718750\n",
      "Train Epoch: 163 [30976/54000 (57%)] Loss: -218749.000000\n",
      "Train Epoch: 163 [32384/54000 (60%)] Loss: -224195.406250\n",
      "Train Epoch: 163 [33792/54000 (63%)] Loss: -244380.625000\n",
      "Train Epoch: 163 [35200/54000 (65%)] Loss: -212067.312500\n",
      "Train Epoch: 163 [36608/54000 (68%)] Loss: -228519.843750\n",
      "Train Epoch: 163 [38016/54000 (70%)] Loss: -225606.875000\n",
      "Train Epoch: 163 [39424/54000 (73%)] Loss: -207430.187500\n",
      "Train Epoch: 163 [40832/54000 (76%)] Loss: -215654.328125\n",
      "Train Epoch: 163 [42240/54000 (78%)] Loss: -227592.890625\n",
      "Train Epoch: 163 [43648/54000 (81%)] Loss: -243020.234375\n",
      "Train Epoch: 163 [45056/54000 (83%)] Loss: -227151.015625\n",
      "Train Epoch: 163 [46464/54000 (86%)] Loss: -221410.125000\n",
      "Train Epoch: 163 [47872/54000 (89%)] Loss: -216341.625000\n",
      "Train Epoch: 163 [49280/54000 (91%)] Loss: -216472.968750\n",
      "Train Epoch: 163 [50688/54000 (94%)] Loss: -213442.937500\n",
      "Train Epoch: 163 [52096/54000 (96%)] Loss: -215329.953125\n",
      "    epoch          : 163\n",
      "    loss           : -220764.66660436604\n",
      "    val_loss       : -227467.20753144054\n",
      "Train Epoch: 164 [0/54000 (0%)] Loss: -227776.953125\n",
      "Train Epoch: 164 [1408/54000 (3%)] Loss: -217922.156250\n",
      "Train Epoch: 164 [2816/54000 (5%)] Loss: -220024.390625\n",
      "Train Epoch: 164 [4224/54000 (8%)] Loss: -220204.578125\n",
      "Train Epoch: 164 [5632/54000 (10%)] Loss: -219431.125000\n",
      "Train Epoch: 164 [7040/54000 (13%)] Loss: -212766.953125\n",
      "Train Epoch: 164 [8448/54000 (16%)] Loss: -211596.250000\n",
      "Train Epoch: 164 [9856/54000 (18%)] Loss: -219606.078125\n",
      "Train Epoch: 164 [11264/54000 (21%)] Loss: -219383.546875\n",
      "Train Epoch: 164 [12672/54000 (23%)] Loss: -215317.343750\n",
      "Train Epoch: 164 [14080/54000 (26%)] Loss: -223802.531250\n",
      "Train Epoch: 164 [15488/54000 (29%)] Loss: -221377.250000\n",
      "Train Epoch: 164 [16896/54000 (31%)] Loss: -243766.484375\n",
      "Train Epoch: 164 [18304/54000 (34%)] Loss: -220892.484375\n",
      "Train Epoch: 164 [19712/54000 (37%)] Loss: -212635.343750\n",
      "Train Epoch: 164 [21120/54000 (39%)] Loss: -217139.921875\n",
      "Train Epoch: 164 [22528/54000 (42%)] Loss: -240921.890625\n",
      "Train Epoch: 164 [23936/54000 (44%)] Loss: -212189.187500\n",
      "Train Epoch: 164 [25344/54000 (47%)] Loss: -220261.906250\n",
      "Train Epoch: 164 [26752/54000 (50%)] Loss: -213739.437500\n",
      "Train Epoch: 164 [28160/54000 (52%)] Loss: -222219.000000\n",
      "Train Epoch: 164 [29568/54000 (55%)] Loss: -212743.640625\n",
      "Train Epoch: 164 [30976/54000 (57%)] Loss: -226457.437500\n",
      "Train Epoch: 164 [32384/54000 (60%)] Loss: -226391.828125\n",
      "Train Epoch: 164 [33792/54000 (63%)] Loss: -223070.625000\n",
      "Train Epoch: 164 [35200/54000 (65%)] Loss: -211536.593750\n",
      "Train Epoch: 164 [36608/54000 (68%)] Loss: -210017.312500\n",
      "Train Epoch: 164 [38016/54000 (70%)] Loss: -212571.015625\n",
      "Train Epoch: 164 [39424/54000 (73%)] Loss: -215809.015625\n",
      "Train Epoch: 164 [40832/54000 (76%)] Loss: -214789.640625\n",
      "Train Epoch: 164 [42240/54000 (78%)] Loss: -224367.250000\n",
      "Train Epoch: 164 [43648/54000 (81%)] Loss: -223253.906250\n",
      "Train Epoch: 164 [45056/54000 (83%)] Loss: -219253.359375\n",
      "Train Epoch: 164 [46464/54000 (86%)] Loss: -216266.656250\n",
      "Train Epoch: 164 [47872/54000 (89%)] Loss: -216359.562500\n",
      "Train Epoch: 164 [49280/54000 (91%)] Loss: -217809.375000\n",
      "Train Epoch: 164 [50688/54000 (94%)] Loss: -222880.609375\n",
      "Train Epoch: 164 [52096/54000 (96%)] Loss: -207277.218750\n",
      "    epoch          : 164\n",
      "    loss           : -220772.6252242823\n",
      "    val_loss       : -227508.20152319933\n",
      "Train Epoch: 165 [0/54000 (0%)] Loss: -219885.750000\n",
      "Train Epoch: 165 [1408/54000 (3%)] Loss: -212642.406250\n",
      "Train Epoch: 165 [2816/54000 (5%)] Loss: -225955.671875\n",
      "Train Epoch: 165 [4224/54000 (8%)] Loss: -211059.046875\n",
      "Train Epoch: 165 [5632/54000 (10%)] Loss: -217335.515625\n",
      "Train Epoch: 165 [7040/54000 (13%)] Loss: -221416.171875\n",
      "Train Epoch: 165 [8448/54000 (16%)] Loss: -217214.265625\n",
      "Train Epoch: 165 [9856/54000 (18%)] Loss: -213378.078125\n",
      "Train Epoch: 165 [11264/54000 (21%)] Loss: -226550.671875\n",
      "Train Epoch: 165 [12672/54000 (23%)] Loss: -218004.828125\n",
      "Train Epoch: 165 [14080/54000 (26%)] Loss: -244586.203125\n",
      "Train Epoch: 165 [15488/54000 (29%)] Loss: -206106.437500\n",
      "Train Epoch: 165 [16896/54000 (31%)] Loss: -216558.593750\n",
      "Train Epoch: 165 [18304/54000 (34%)] Loss: -215276.687500\n",
      "Train Epoch: 165 [19712/54000 (37%)] Loss: -242463.046875\n",
      "Train Epoch: 165 [21120/54000 (39%)] Loss: -214647.312500\n",
      "Train Epoch: 165 [22528/54000 (42%)] Loss: -216439.734375\n",
      "Train Epoch: 165 [23936/54000 (44%)] Loss: -222971.281250\n",
      "Train Epoch: 165 [25344/54000 (47%)] Loss: -241555.390625\n",
      "Train Epoch: 165 [26752/54000 (50%)] Loss: -218038.125000\n",
      "Train Epoch: 165 [28160/54000 (52%)] Loss: -226950.593750\n",
      "Train Epoch: 165 [29568/54000 (55%)] Loss: -223811.734375\n",
      "Train Epoch: 165 [30976/54000 (57%)] Loss: -242295.906250\n",
      "Train Epoch: 165 [32384/54000 (60%)] Loss: -207267.906250\n",
      "Train Epoch: 165 [33792/54000 (63%)] Loss: -208213.812500\n",
      "Train Epoch: 165 [35200/54000 (65%)] Loss: -217313.000000\n",
      "Train Epoch: 165 [36608/54000 (68%)] Loss: -214315.312500\n",
      "Train Epoch: 165 [38016/54000 (70%)] Loss: -217745.140625\n",
      "Train Epoch: 165 [39424/54000 (73%)] Loss: -219386.437500\n",
      "Train Epoch: 165 [40832/54000 (76%)] Loss: -217669.812500\n",
      "Train Epoch: 165 [42240/54000 (78%)] Loss: -225646.875000\n",
      "Train Epoch: 165 [43648/54000 (81%)] Loss: -210414.781250\n",
      "Train Epoch: 165 [45056/54000 (83%)] Loss: -222896.328125\n",
      "Train Epoch: 165 [46464/54000 (86%)] Loss: -210597.171875\n",
      "Train Epoch: 165 [47872/54000 (89%)] Loss: -211872.406250\n",
      "Train Epoch: 165 [49280/54000 (91%)] Loss: -217365.593750\n",
      "Train Epoch: 165 [50688/54000 (94%)] Loss: -216210.968750\n",
      "Train Epoch: 165 [52096/54000 (96%)] Loss: -222210.125000\n",
      "    epoch          : 165\n",
      "    loss           : -220967.36591656698\n",
      "    val_loss       : -227523.80801019436\n",
      "Train Epoch: 166 [0/54000 (0%)] Loss: -243967.312500\n",
      "Train Epoch: 166 [1408/54000 (3%)] Loss: -226292.562500\n",
      "Train Epoch: 166 [2816/54000 (5%)] Loss: -218134.875000\n",
      "Train Epoch: 166 [4224/54000 (8%)] Loss: -223519.593750\n",
      "Train Epoch: 166 [5632/54000 (10%)] Loss: -220916.984375\n",
      "Train Epoch: 166 [7040/54000 (13%)] Loss: -217834.296875\n",
      "Train Epoch: 166 [8448/54000 (16%)] Loss: -215612.984375\n",
      "Train Epoch: 166 [9856/54000 (18%)] Loss: -226056.343750\n",
      "Train Epoch: 166 [11264/54000 (21%)] Loss: -226678.312500\n",
      "Train Epoch: 166 [12672/54000 (23%)] Loss: -224119.250000\n",
      "Train Epoch: 166 [14080/54000 (26%)] Loss: -220419.171875\n",
      "Train Epoch: 166 [15488/54000 (29%)] Loss: -241426.406250\n",
      "Train Epoch: 166 [16896/54000 (31%)] Loss: -212249.812500\n",
      "Train Epoch: 166 [18304/54000 (34%)] Loss: -218476.453125\n",
      "Train Epoch: 166 [19712/54000 (37%)] Loss: -214194.406250\n",
      "Train Epoch: 166 [21120/54000 (39%)] Loss: -221759.968750\n",
      "Train Epoch: 166 [22528/54000 (42%)] Loss: -211677.156250\n",
      "Train Epoch: 166 [23936/54000 (44%)] Loss: -215776.109375\n",
      "Train Epoch: 166 [25344/54000 (47%)] Loss: -214574.484375\n",
      "Train Epoch: 166 [26752/54000 (50%)] Loss: -227513.437500\n",
      "Train Epoch: 166 [28160/54000 (52%)] Loss: -227490.750000\n",
      "Train Epoch: 166 [29568/54000 (55%)] Loss: -222131.875000\n",
      "Train Epoch: 166 [30976/54000 (57%)] Loss: -214584.187500\n",
      "Train Epoch: 166 [32384/54000 (60%)] Loss: -214955.968750\n",
      "Train Epoch: 166 [33792/54000 (63%)] Loss: -216777.109375\n",
      "Train Epoch: 166 [35200/54000 (65%)] Loss: -207817.468750\n",
      "Train Epoch: 166 [36608/54000 (68%)] Loss: -223639.046875\n",
      "Train Epoch: 166 [38016/54000 (70%)] Loss: -221780.656250\n",
      "Train Epoch: 166 [39424/54000 (73%)] Loss: -225205.062500\n",
      "Train Epoch: 166 [40832/54000 (76%)] Loss: -223501.187500\n",
      "Train Epoch: 166 [42240/54000 (78%)] Loss: -218465.843750\n",
      "Train Epoch: 166 [43648/54000 (81%)] Loss: -223994.484375\n",
      "Train Epoch: 166 [45056/54000 (83%)] Loss: -220783.562500\n",
      "Train Epoch: 166 [46464/54000 (86%)] Loss: -221808.953125\n",
      "Train Epoch: 166 [47872/54000 (89%)] Loss: -214057.031250\n",
      "Train Epoch: 166 [49280/54000 (91%)] Loss: -224315.390625\n",
      "Train Epoch: 166 [50688/54000 (94%)] Loss: -221952.781250\n",
      "Train Epoch: 166 [52096/54000 (96%)] Loss: -225916.625000\n",
      "    epoch          : 166\n",
      "    loss           : -221069.70570424641\n",
      "    val_loss       : -227637.4361125667\n",
      "Train Epoch: 167 [0/54000 (0%)] Loss: -240404.140625\n",
      "Train Epoch: 167 [1408/54000 (3%)] Loss: -218028.468750\n",
      "Train Epoch: 167 [2816/54000 (5%)] Loss: -223212.125000\n",
      "Train Epoch: 167 [4224/54000 (8%)] Loss: -226049.687500\n",
      "Train Epoch: 167 [5632/54000 (10%)] Loss: -212325.500000\n",
      "Train Epoch: 167 [7040/54000 (13%)] Loss: -207690.250000\n",
      "Train Epoch: 167 [8448/54000 (16%)] Loss: -221005.875000\n",
      "Train Epoch: 167 [9856/54000 (18%)] Loss: -220870.562500\n",
      "Train Epoch: 167 [11264/54000 (21%)] Loss: -214833.859375\n",
      "Train Epoch: 167 [12672/54000 (23%)] Loss: -213434.812500\n",
      "Train Epoch: 167 [14080/54000 (26%)] Loss: -224141.781250\n",
      "Train Epoch: 167 [15488/54000 (29%)] Loss: -220194.703125\n",
      "Train Epoch: 167 [16896/54000 (31%)] Loss: -211548.218750\n",
      "Train Epoch: 167 [18304/54000 (34%)] Loss: -207577.281250\n",
      "Train Epoch: 167 [19712/54000 (37%)] Loss: -217843.968750\n",
      "Train Epoch: 167 [21120/54000 (39%)] Loss: -216278.781250\n",
      "Train Epoch: 167 [22528/54000 (42%)] Loss: -220507.531250\n",
      "Train Epoch: 167 [23936/54000 (44%)] Loss: -226683.468750\n",
      "Train Epoch: 167 [25344/54000 (47%)] Loss: -227112.937500\n",
      "Train Epoch: 167 [26752/54000 (50%)] Loss: -220278.265625\n",
      "Train Epoch: 167 [28160/54000 (52%)] Loss: -228032.593750\n",
      "Train Epoch: 167 [29568/54000 (55%)] Loss: -215258.734375\n",
      "Train Epoch: 167 [30976/54000 (57%)] Loss: -222589.718750\n",
      "Train Epoch: 167 [32384/54000 (60%)] Loss: -213912.421875\n",
      "Train Epoch: 167 [33792/54000 (63%)] Loss: -221729.453125\n",
      "Train Epoch: 167 [35200/54000 (65%)] Loss: -219860.109375\n",
      "Train Epoch: 167 [36608/54000 (68%)] Loss: -211948.203125\n",
      "Train Epoch: 167 [38016/54000 (70%)] Loss: -219195.468750\n",
      "Train Epoch: 167 [39424/54000 (73%)] Loss: -219716.468750\n",
      "Train Epoch: 167 [40832/54000 (76%)] Loss: -222056.109375\n",
      "Train Epoch: 167 [42240/54000 (78%)] Loss: -224537.671875\n",
      "Train Epoch: 167 [43648/54000 (81%)] Loss: -222481.218750\n",
      "Train Epoch: 167 [45056/54000 (83%)] Loss: -221823.500000\n",
      "Train Epoch: 167 [46464/54000 (86%)] Loss: -216689.531250\n",
      "Train Epoch: 167 [47872/54000 (89%)] Loss: -214500.328125\n",
      "Train Epoch: 167 [49280/54000 (91%)] Loss: -214213.765625\n",
      "Train Epoch: 167 [50688/54000 (94%)] Loss: -219077.812500\n",
      "Train Epoch: 167 [52096/54000 (96%)] Loss: -223802.375000\n",
      "    epoch          : 167\n",
      "    loss           : -221137.42546351676\n",
      "    val_loss       : -227641.0464045827\n",
      "Train Epoch: 168 [0/54000 (0%)] Loss: -218732.015625\n",
      "Train Epoch: 168 [1408/54000 (3%)] Loss: -209893.406250\n",
      "Train Epoch: 168 [2816/54000 (5%)] Loss: -226660.484375\n",
      "Train Epoch: 168 [4224/54000 (8%)] Loss: -209704.437500\n",
      "Train Epoch: 168 [5632/54000 (10%)] Loss: -226324.375000\n",
      "Train Epoch: 168 [7040/54000 (13%)] Loss: -222955.625000\n",
      "Train Epoch: 168 [8448/54000 (16%)] Loss: -245021.937500\n",
      "Train Epoch: 168 [9856/54000 (18%)] Loss: -217959.796875\n",
      "Train Epoch: 168 [11264/54000 (21%)] Loss: -220668.140625\n",
      "Train Epoch: 168 [12672/54000 (23%)] Loss: -222564.421875\n",
      "Train Epoch: 168 [14080/54000 (26%)] Loss: -222493.140625\n",
      "Train Epoch: 168 [15488/54000 (29%)] Loss: -219791.671875\n",
      "Train Epoch: 168 [16896/54000 (31%)] Loss: -215436.171875\n",
      "Train Epoch: 168 [18304/54000 (34%)] Loss: -228074.078125\n",
      "Train Epoch: 168 [19712/54000 (37%)] Loss: -223225.781250\n",
      "Train Epoch: 168 [21120/54000 (39%)] Loss: -227557.078125\n",
      "Train Epoch: 168 [22528/54000 (42%)] Loss: -224644.140625\n",
      "Train Epoch: 168 [23936/54000 (44%)] Loss: -219301.593750\n",
      "Train Epoch: 168 [25344/54000 (47%)] Loss: -212608.093750\n",
      "Train Epoch: 168 [26752/54000 (50%)] Loss: -240572.750000\n",
      "Train Epoch: 168 [28160/54000 (52%)] Loss: -211966.390625\n",
      "Train Epoch: 168 [29568/54000 (55%)] Loss: -213987.250000\n",
      "Train Epoch: 168 [30976/54000 (57%)] Loss: -223787.046875\n",
      "Train Epoch: 168 [32384/54000 (60%)] Loss: -242626.625000\n",
      "Train Epoch: 168 [33792/54000 (63%)] Loss: -224583.281250\n",
      "Train Epoch: 168 [35200/54000 (65%)] Loss: -226978.093750\n",
      "Train Epoch: 168 [36608/54000 (68%)] Loss: -227480.468750\n",
      "Train Epoch: 168 [38016/54000 (70%)] Loss: -241140.421875\n",
      "Train Epoch: 168 [39424/54000 (73%)] Loss: -216979.171875\n",
      "Train Epoch: 168 [40832/54000 (76%)] Loss: -225880.109375\n",
      "Train Epoch: 168 [42240/54000 (78%)] Loss: -217493.640625\n",
      "Train Epoch: 168 [43648/54000 (81%)] Loss: -211016.687500\n",
      "Train Epoch: 168 [45056/54000 (83%)] Loss: -220407.375000\n",
      "Train Epoch: 168 [46464/54000 (86%)] Loss: -220153.593750\n",
      "Train Epoch: 168 [47872/54000 (89%)] Loss: -216909.625000\n",
      "Train Epoch: 168 [49280/54000 (91%)] Loss: -214349.828125\n",
      "Train Epoch: 168 [50688/54000 (94%)] Loss: -223778.515625\n",
      "Train Epoch: 168 [52096/54000 (96%)] Loss: -224470.890625\n",
      "    epoch          : 168\n",
      "    loss           : -221072.96452601676\n",
      "    val_loss       : -227782.89298304115\n",
      "Train Epoch: 169 [0/54000 (0%)] Loss: -211421.437500\n",
      "Train Epoch: 169 [1408/54000 (3%)] Loss: -241043.515625\n",
      "Train Epoch: 169 [2816/54000 (5%)] Loss: -220356.859375\n",
      "Train Epoch: 169 [4224/54000 (8%)] Loss: -213370.656250\n",
      "Train Epoch: 169 [5632/54000 (10%)] Loss: -216106.187500\n",
      "Train Epoch: 169 [7040/54000 (13%)] Loss: -223066.687500\n",
      "Train Epoch: 169 [8448/54000 (16%)] Loss: -221291.687500\n",
      "Train Epoch: 169 [9856/54000 (18%)] Loss: -227684.984375\n",
      "Train Epoch: 169 [11264/54000 (21%)] Loss: -220310.406250\n",
      "Train Epoch: 169 [12672/54000 (23%)] Loss: -215678.515625\n",
      "Train Epoch: 169 [14080/54000 (26%)] Loss: -219509.906250\n",
      "Train Epoch: 169 [15488/54000 (29%)] Loss: -216019.265625\n",
      "Train Epoch: 169 [16896/54000 (31%)] Loss: -216535.437500\n",
      "Train Epoch: 169 [18304/54000 (34%)] Loss: -212034.156250\n",
      "Train Epoch: 169 [19712/54000 (37%)] Loss: -244245.562500\n",
      "Train Epoch: 169 [21120/54000 (39%)] Loss: -226350.109375\n",
      "Train Epoch: 169 [22528/54000 (42%)] Loss: -227816.875000\n",
      "Train Epoch: 169 [23936/54000 (44%)] Loss: -227519.421875\n",
      "Train Epoch: 169 [25344/54000 (47%)] Loss: -244444.421875\n",
      "Train Epoch: 169 [26752/54000 (50%)] Loss: -219030.390625\n",
      "Train Epoch: 169 [28160/54000 (52%)] Loss: -214916.500000\n",
      "Train Epoch: 169 [29568/54000 (55%)] Loss: -243000.296875\n",
      "Train Epoch: 169 [30976/54000 (57%)] Loss: -210657.968750\n",
      "Train Epoch: 169 [32384/54000 (60%)] Loss: -215431.859375\n",
      "Train Epoch: 169 [33792/54000 (63%)] Loss: -225309.437500\n",
      "Train Epoch: 169 [35200/54000 (65%)] Loss: -221572.515625\n",
      "Train Epoch: 169 [36608/54000 (68%)] Loss: -216382.968750\n",
      "Train Epoch: 169 [38016/54000 (70%)] Loss: -210870.984375\n",
      "Train Epoch: 169 [39424/54000 (73%)] Loss: -215081.234375\n",
      "Train Epoch: 169 [40832/54000 (76%)] Loss: -227726.125000\n",
      "Train Epoch: 169 [42240/54000 (78%)] Loss: -214340.109375\n",
      "Train Epoch: 169 [43648/54000 (81%)] Loss: -242021.328125\n",
      "Train Epoch: 169 [45056/54000 (83%)] Loss: -221611.171875\n",
      "Train Epoch: 169 [46464/54000 (86%)] Loss: -220117.890625\n",
      "Train Epoch: 169 [47872/54000 (89%)] Loss: -218109.468750\n",
      "Train Epoch: 169 [49280/54000 (91%)] Loss: -217527.015625\n",
      "Train Epoch: 169 [50688/54000 (94%)] Loss: -242845.937500\n",
      "Train Epoch: 169 [52096/54000 (96%)] Loss: -209328.609375\n",
      "    epoch          : 169\n",
      "    loss           : -221177.7209180622\n",
      "    val_loss       : -227832.67503096416\n",
      "Train Epoch: 170 [0/54000 (0%)] Loss: -214525.218750\n",
      "Train Epoch: 170 [1408/54000 (3%)] Loss: -215070.375000\n",
      "Train Epoch: 170 [2816/54000 (5%)] Loss: -224514.140625\n",
      "Train Epoch: 170 [4224/54000 (8%)] Loss: -226016.281250\n",
      "Train Epoch: 170 [5632/54000 (10%)] Loss: -214502.312500\n",
      "Train Epoch: 170 [7040/54000 (13%)] Loss: -221746.000000\n",
      "Train Epoch: 170 [8448/54000 (16%)] Loss: -214271.265625\n",
      "Train Epoch: 170 [9856/54000 (18%)] Loss: -244424.968750\n",
      "Train Epoch: 170 [11264/54000 (21%)] Loss: -223944.906250\n",
      "Train Epoch: 170 [12672/54000 (23%)] Loss: -227494.328125\n",
      "Train Epoch: 170 [14080/54000 (26%)] Loss: -225636.828125\n",
      "Train Epoch: 170 [15488/54000 (29%)] Loss: -213668.250000\n",
      "Train Epoch: 170 [16896/54000 (31%)] Loss: -215284.515625\n",
      "Train Epoch: 170 [18304/54000 (34%)] Loss: -224950.718750\n",
      "Train Epoch: 170 [19712/54000 (37%)] Loss: -224226.921875\n",
      "Train Epoch: 170 [21120/54000 (39%)] Loss: -223920.953125\n",
      "Train Epoch: 170 [22528/54000 (42%)] Loss: -221544.437500\n",
      "Train Epoch: 170 [23936/54000 (44%)] Loss: -218283.843750\n",
      "Train Epoch: 170 [25344/54000 (47%)] Loss: -217945.875000\n",
      "Train Epoch: 170 [26752/54000 (50%)] Loss: -222397.953125\n",
      "Train Epoch: 170 [28160/54000 (52%)] Loss: -244087.328125\n",
      "Train Epoch: 170 [29568/54000 (55%)] Loss: -216882.734375\n",
      "Train Epoch: 170 [30976/54000 (57%)] Loss: -228685.234375\n",
      "Train Epoch: 170 [32384/54000 (60%)] Loss: -244611.156250\n",
      "Train Epoch: 170 [33792/54000 (63%)] Loss: -213522.062500\n",
      "Train Epoch: 170 [35200/54000 (65%)] Loss: -212980.531250\n",
      "Train Epoch: 170 [36608/54000 (68%)] Loss: -217457.968750\n",
      "Train Epoch: 170 [38016/54000 (70%)] Loss: -206517.359375\n",
      "Train Epoch: 170 [39424/54000 (73%)] Loss: -216455.265625\n",
      "Train Epoch: 170 [40832/54000 (76%)] Loss: -227100.765625\n",
      "Train Epoch: 170 [42240/54000 (78%)] Loss: -226594.843750\n",
      "Train Epoch: 170 [43648/54000 (81%)] Loss: -242574.000000\n",
      "Train Epoch: 170 [45056/54000 (83%)] Loss: -224107.562500\n",
      "Train Epoch: 170 [46464/54000 (86%)] Loss: -218712.531250\n",
      "Train Epoch: 170 [47872/54000 (89%)] Loss: -214938.953125\n",
      "Train Epoch: 170 [49280/54000 (91%)] Loss: -215640.734375\n",
      "Train Epoch: 170 [50688/54000 (94%)] Loss: -241746.671875\n",
      "Train Epoch: 170 [52096/54000 (96%)] Loss: -211139.843750\n",
      "    epoch          : 170\n",
      "    loss           : -221261.33537679425\n",
      "    val_loss       : -227459.11109589366\n",
      "Train Epoch: 171 [0/54000 (0%)] Loss: -212854.187500\n",
      "Train Epoch: 171 [1408/54000 (3%)] Loss: -207226.406250\n",
      "Train Epoch: 171 [2816/54000 (5%)] Loss: -217524.546875\n",
      "Train Epoch: 171 [4224/54000 (8%)] Loss: -222115.125000\n",
      "Train Epoch: 171 [5632/54000 (10%)] Loss: -208635.875000\n",
      "Train Epoch: 171 [7040/54000 (13%)] Loss: -243147.218750\n",
      "Train Epoch: 171 [8448/54000 (16%)] Loss: -217297.781250\n",
      "Train Epoch: 171 [9856/54000 (18%)] Loss: -226618.031250\n",
      "Train Epoch: 171 [11264/54000 (21%)] Loss: -216247.500000\n",
      "Train Epoch: 171 [12672/54000 (23%)] Loss: -223635.796875\n",
      "Train Epoch: 171 [14080/54000 (26%)] Loss: -220574.781250\n",
      "Train Epoch: 171 [15488/54000 (29%)] Loss: -218730.171875\n",
      "Train Epoch: 171 [16896/54000 (31%)] Loss: -218768.031250\n",
      "Train Epoch: 171 [18304/54000 (34%)] Loss: -243524.171875\n",
      "Train Epoch: 171 [19712/54000 (37%)] Loss: -216899.765625\n",
      "Train Epoch: 171 [21120/54000 (39%)] Loss: -227734.156250\n",
      "Train Epoch: 171 [22528/54000 (42%)] Loss: -213097.859375\n",
      "Train Epoch: 171 [23936/54000 (44%)] Loss: -226558.843750\n",
      "Train Epoch: 171 [25344/54000 (47%)] Loss: -221916.281250\n",
      "Train Epoch: 171 [26752/54000 (50%)] Loss: -218835.312500\n",
      "Train Epoch: 171 [28160/54000 (52%)] Loss: -219213.562500\n",
      "Train Epoch: 171 [29568/54000 (55%)] Loss: -215528.843750\n",
      "Train Epoch: 171 [30976/54000 (57%)] Loss: -216468.906250\n",
      "Train Epoch: 171 [32384/54000 (60%)] Loss: -218456.906250\n",
      "Train Epoch: 171 [33792/54000 (63%)] Loss: -243751.421875\n",
      "Train Epoch: 171 [35200/54000 (65%)] Loss: -211826.296875\n",
      "Train Epoch: 171 [36608/54000 (68%)] Loss: -224131.906250\n",
      "Train Epoch: 171 [38016/54000 (70%)] Loss: -243423.343750\n",
      "Train Epoch: 171 [39424/54000 (73%)] Loss: -226947.234375\n",
      "Train Epoch: 171 [40832/54000 (76%)] Loss: -219447.109375\n",
      "Train Epoch: 171 [42240/54000 (78%)] Loss: -224634.421875\n",
      "Train Epoch: 171 [43648/54000 (81%)] Loss: -210669.875000\n",
      "Train Epoch: 171 [45056/54000 (83%)] Loss: -242656.015625\n",
      "Train Epoch: 171 [46464/54000 (86%)] Loss: -216802.328125\n",
      "Train Epoch: 171 [47872/54000 (89%)] Loss: -219601.296875\n",
      "Train Epoch: 171 [49280/54000 (91%)] Loss: -217871.968750\n",
      "Train Epoch: 171 [50688/54000 (94%)] Loss: -212582.046875\n",
      "Train Epoch: 171 [52096/54000 (96%)] Loss: -211567.171875\n",
      "    epoch          : 171\n",
      "    loss           : -221356.19609748805\n",
      "    val_loss       : -227697.8988126429\n",
      "Train Epoch: 172 [0/54000 (0%)] Loss: -215679.656250\n",
      "Train Epoch: 172 [1408/54000 (3%)] Loss: -221929.281250\n",
      "Train Epoch: 172 [2816/54000 (5%)] Loss: -219715.843750\n",
      "Train Epoch: 172 [4224/54000 (8%)] Loss: -228257.312500\n",
      "Train Epoch: 172 [5632/54000 (10%)] Loss: -219052.843750\n",
      "Train Epoch: 172 [7040/54000 (13%)] Loss: -214045.515625\n",
      "Train Epoch: 172 [8448/54000 (16%)] Loss: -209269.250000\n",
      "Train Epoch: 172 [9856/54000 (18%)] Loss: -208935.140625\n",
      "Train Epoch: 172 [11264/54000 (21%)] Loss: -223177.875000\n",
      "Train Epoch: 172 [12672/54000 (23%)] Loss: -211498.000000\n",
      "Train Epoch: 172 [14080/54000 (26%)] Loss: -220212.906250\n",
      "Train Epoch: 172 [15488/54000 (29%)] Loss: -209870.437500\n",
      "Train Epoch: 172 [16896/54000 (31%)] Loss: -217243.062500\n",
      "Train Epoch: 172 [18304/54000 (34%)] Loss: -242645.156250\n",
      "Train Epoch: 172 [19712/54000 (37%)] Loss: -227547.750000\n",
      "Train Epoch: 172 [21120/54000 (39%)] Loss: -224102.187500\n",
      "Train Epoch: 172 [22528/54000 (42%)] Loss: -215822.000000\n",
      "Train Epoch: 172 [23936/54000 (44%)] Loss: -212888.859375\n",
      "Train Epoch: 172 [25344/54000 (47%)] Loss: -213484.234375\n",
      "Train Epoch: 172 [26752/54000 (50%)] Loss: -219111.687500\n",
      "Train Epoch: 172 [28160/54000 (52%)] Loss: -216365.031250\n",
      "Train Epoch: 172 [29568/54000 (55%)] Loss: -210431.281250\n",
      "Train Epoch: 172 [30976/54000 (57%)] Loss: -225155.593750\n",
      "Train Epoch: 172 [32384/54000 (60%)] Loss: -225165.890625\n",
      "Train Epoch: 172 [33792/54000 (63%)] Loss: -213651.640625\n",
      "Train Epoch: 172 [35200/54000 (65%)] Loss: -219979.890625\n",
      "Train Epoch: 172 [36608/54000 (68%)] Loss: -219587.171875\n",
      "Train Epoch: 172 [38016/54000 (70%)] Loss: -213570.515625\n",
      "Train Epoch: 172 [39424/54000 (73%)] Loss: -223125.062500\n",
      "Train Epoch: 172 [40832/54000 (76%)] Loss: -226210.078125\n",
      "Train Epoch: 172 [42240/54000 (78%)] Loss: -216391.187500\n",
      "Train Epoch: 172 [43648/54000 (81%)] Loss: -215468.609375\n",
      "Train Epoch: 172 [45056/54000 (83%)] Loss: -211467.062500\n",
      "Train Epoch: 172 [46464/54000 (86%)] Loss: -216045.906250\n",
      "Train Epoch: 172 [47872/54000 (89%)] Loss: -220629.906250\n",
      "Train Epoch: 172 [49280/54000 (91%)] Loss: -211835.906250\n",
      "Train Epoch: 172 [50688/54000 (94%)] Loss: -217205.843750\n",
      "Train Epoch: 172 [52096/54000 (96%)] Loss: -224797.390625\n",
      "    epoch          : 172\n",
      "    loss           : -221458.0787978469\n",
      "    val_loss       : -227591.9004025343\n",
      "Train Epoch: 173 [0/54000 (0%)] Loss: -239071.156250\n",
      "Train Epoch: 173 [1408/54000 (3%)] Loss: -212275.406250\n",
      "Train Epoch: 173 [2816/54000 (5%)] Loss: -223081.406250\n",
      "Train Epoch: 173 [4224/54000 (8%)] Loss: -216115.187500\n",
      "Train Epoch: 173 [5632/54000 (10%)] Loss: -207576.093750\n",
      "Train Epoch: 173 [7040/54000 (13%)] Loss: -221963.203125\n",
      "Train Epoch: 173 [8448/54000 (16%)] Loss: -222393.562500\n",
      "Train Epoch: 173 [9856/54000 (18%)] Loss: -219688.796875\n",
      "Train Epoch: 173 [11264/54000 (21%)] Loss: -213360.359375\n",
      "Train Epoch: 173 [12672/54000 (23%)] Loss: -215136.312500\n",
      "Train Epoch: 173 [14080/54000 (26%)] Loss: -227078.812500\n",
      "Train Epoch: 173 [15488/54000 (29%)] Loss: -225566.718750\n",
      "Train Epoch: 173 [16896/54000 (31%)] Loss: -241349.593750\n",
      "Train Epoch: 173 [18304/54000 (34%)] Loss: -216469.859375\n",
      "Train Epoch: 173 [19712/54000 (37%)] Loss: -216127.656250\n",
      "Train Epoch: 173 [21120/54000 (39%)] Loss: -212462.968750\n",
      "Train Epoch: 173 [22528/54000 (42%)] Loss: -211594.937500\n",
      "Train Epoch: 173 [23936/54000 (44%)] Loss: -211907.375000\n",
      "Train Epoch: 173 [25344/54000 (47%)] Loss: -210676.531250\n",
      "Train Epoch: 173 [26752/54000 (50%)] Loss: -220550.781250\n",
      "Train Epoch: 173 [28160/54000 (52%)] Loss: -210050.203125\n",
      "Train Epoch: 173 [29568/54000 (55%)] Loss: -214258.578125\n",
      "Train Epoch: 173 [30976/54000 (57%)] Loss: -220122.296875\n",
      "Train Epoch: 173 [32384/54000 (60%)] Loss: -243624.843750\n",
      "Train Epoch: 173 [33792/54000 (63%)] Loss: -225246.890625\n",
      "Train Epoch: 173 [35200/54000 (65%)] Loss: -217561.265625\n",
      "Train Epoch: 173 [36608/54000 (68%)] Loss: -244273.765625\n",
      "Train Epoch: 173 [38016/54000 (70%)] Loss: -212453.171875\n",
      "Train Epoch: 173 [39424/54000 (73%)] Loss: -225076.406250\n",
      "Train Epoch: 173 [40832/54000 (76%)] Loss: -227083.718750\n",
      "Train Epoch: 173 [42240/54000 (78%)] Loss: -208490.125000\n",
      "Train Epoch: 173 [43648/54000 (81%)] Loss: -242652.625000\n",
      "Train Epoch: 173 [45056/54000 (83%)] Loss: -213830.609375\n",
      "Train Epoch: 173 [46464/54000 (86%)] Loss: -215242.437500\n",
      "Train Epoch: 173 [47872/54000 (89%)] Loss: -222052.234375\n",
      "Train Epoch: 173 [49280/54000 (91%)] Loss: -217043.078125\n",
      "Train Epoch: 173 [50688/54000 (94%)] Loss: -223224.640625\n",
      "Train Epoch: 173 [52096/54000 (96%)] Loss: -226585.562500\n",
      "    epoch          : 173\n",
      "    loss           : -221349.84393690192\n",
      "    val_loss       : -227946.47522865853\n",
      "Train Epoch: 174 [0/54000 (0%)] Loss: -244415.546875\n",
      "Train Epoch: 174 [1408/54000 (3%)] Loss: -213477.656250\n",
      "Train Epoch: 174 [2816/54000 (5%)] Loss: -213482.593750\n",
      "Train Epoch: 174 [4224/54000 (8%)] Loss: -223278.062500\n",
      "Train Epoch: 174 [5632/54000 (10%)] Loss: -213911.312500\n",
      "Train Epoch: 174 [7040/54000 (13%)] Loss: -215825.218750\n",
      "Train Epoch: 174 [8448/54000 (16%)] Loss: -215164.218750\n",
      "Train Epoch: 174 [9856/54000 (18%)] Loss: -219384.640625\n",
      "Train Epoch: 174 [11264/54000 (21%)] Loss: -226946.812500\n",
      "Train Epoch: 174 [12672/54000 (23%)] Loss: -226554.281250\n",
      "Train Epoch: 174 [14080/54000 (26%)] Loss: -225484.406250\n",
      "Train Epoch: 174 [15488/54000 (29%)] Loss: -223675.046875\n",
      "Train Epoch: 174 [16896/54000 (31%)] Loss: -213443.625000\n",
      "Train Epoch: 174 [18304/54000 (34%)] Loss: -207410.906250\n",
      "Train Epoch: 174 [19712/54000 (37%)] Loss: -209311.296875\n",
      "Train Epoch: 174 [21120/54000 (39%)] Loss: -218473.500000\n",
      "Train Epoch: 174 [22528/54000 (42%)] Loss: -214103.031250\n",
      "Train Epoch: 174 [23936/54000 (44%)] Loss: -207261.312500\n",
      "Train Epoch: 174 [25344/54000 (47%)] Loss: -214726.031250\n",
      "Train Epoch: 174 [26752/54000 (50%)] Loss: -210577.562500\n",
      "Train Epoch: 174 [28160/54000 (52%)] Loss: -227197.000000\n",
      "Train Epoch: 174 [29568/54000 (55%)] Loss: -227741.109375\n",
      "Train Epoch: 174 [30976/54000 (57%)] Loss: -218547.296875\n",
      "Train Epoch: 174 [32384/54000 (60%)] Loss: -222392.171875\n",
      "Train Epoch: 174 [33792/54000 (63%)] Loss: -223495.531250\n",
      "Train Epoch: 174 [35200/54000 (65%)] Loss: -244056.328125\n",
      "Train Epoch: 174 [36608/54000 (68%)] Loss: -210856.093750\n",
      "Train Epoch: 174 [38016/54000 (70%)] Loss: -212634.875000\n",
      "Train Epoch: 174 [39424/54000 (73%)] Loss: -227531.343750\n",
      "Train Epoch: 174 [40832/54000 (76%)] Loss: -210387.734375\n",
      "Train Epoch: 174 [42240/54000 (78%)] Loss: -214278.765625\n",
      "Train Epoch: 174 [43648/54000 (81%)] Loss: -227571.531250\n",
      "Train Epoch: 174 [45056/54000 (83%)] Loss: -218452.468750\n",
      "Train Epoch: 174 [46464/54000 (86%)] Loss: -220037.750000\n",
      "Train Epoch: 174 [47872/54000 (89%)] Loss: -216699.375000\n",
      "Train Epoch: 174 [49280/54000 (91%)] Loss: -215362.156250\n",
      "Train Epoch: 174 [50688/54000 (94%)] Loss: -211981.937500\n",
      "Train Epoch: 174 [52096/54000 (96%)] Loss: -209060.484375\n",
      "    epoch          : 174\n",
      "    loss           : -221449.49551435406\n",
      "    val_loss       : -228101.60383717608\n",
      "Train Epoch: 175 [0/54000 (0%)] Loss: -243409.828125\n",
      "Train Epoch: 175 [1408/54000 (3%)] Loss: -218139.265625\n",
      "Train Epoch: 175 [2816/54000 (5%)] Loss: -245092.968750\n",
      "Train Epoch: 175 [4224/54000 (8%)] Loss: -225323.515625\n",
      "Train Epoch: 175 [5632/54000 (10%)] Loss: -221633.546875\n",
      "Train Epoch: 175 [7040/54000 (13%)] Loss: -225558.031250\n",
      "Train Epoch: 175 [8448/54000 (16%)] Loss: -215488.500000\n",
      "Train Epoch: 175 [9856/54000 (18%)] Loss: -226468.218750\n",
      "Train Epoch: 175 [11264/54000 (21%)] Loss: -226714.062500\n",
      "Train Epoch: 175 [12672/54000 (23%)] Loss: -218725.859375\n",
      "Train Epoch: 175 [14080/54000 (26%)] Loss: -241724.515625\n",
      "Train Epoch: 175 [15488/54000 (29%)] Loss: -220684.578125\n",
      "Train Epoch: 175 [16896/54000 (31%)] Loss: -222008.390625\n",
      "Train Epoch: 175 [18304/54000 (34%)] Loss: -215768.515625\n",
      "Train Epoch: 175 [19712/54000 (37%)] Loss: -214843.859375\n",
      "Train Epoch: 175 [21120/54000 (39%)] Loss: -216327.937500\n",
      "Train Epoch: 175 [22528/54000 (42%)] Loss: -213587.796875\n",
      "Train Epoch: 175 [23936/54000 (44%)] Loss: -223706.093750\n",
      "Train Epoch: 175 [25344/54000 (47%)] Loss: -221892.531250\n",
      "Train Epoch: 175 [26752/54000 (50%)] Loss: -218877.656250\n",
      "Train Epoch: 175 [28160/54000 (52%)] Loss: -216078.734375\n",
      "Train Epoch: 175 [29568/54000 (55%)] Loss: -215008.890625\n",
      "Train Epoch: 175 [30976/54000 (57%)] Loss: -221100.062500\n",
      "Train Epoch: 175 [32384/54000 (60%)] Loss: -219510.375000\n",
      "Train Epoch: 175 [33792/54000 (63%)] Loss: -224705.609375\n",
      "Train Epoch: 175 [35200/54000 (65%)] Loss: -221686.468750\n",
      "Train Epoch: 175 [36608/54000 (68%)] Loss: -212008.421875\n",
      "Train Epoch: 175 [38016/54000 (70%)] Loss: -215211.593750\n",
      "Train Epoch: 175 [39424/54000 (73%)] Loss: -211259.125000\n",
      "Train Epoch: 175 [40832/54000 (76%)] Loss: -242921.203125\n",
      "Train Epoch: 175 [42240/54000 (78%)] Loss: -223118.093750\n",
      "Train Epoch: 175 [43648/54000 (81%)] Loss: -214672.515625\n",
      "Train Epoch: 175 [45056/54000 (83%)] Loss: -218102.687500\n",
      "Train Epoch: 175 [46464/54000 (86%)] Loss: -213961.625000\n",
      "Train Epoch: 175 [47872/54000 (89%)] Loss: -217143.578125\n",
      "Train Epoch: 175 [49280/54000 (91%)] Loss: -214769.968750\n",
      "Train Epoch: 175 [50688/54000 (94%)] Loss: -222216.406250\n",
      "Train Epoch: 175 [52096/54000 (96%)] Loss: -223578.250000\n",
      "    epoch          : 175\n",
      "    loss           : -221421.15636214116\n",
      "    val_loss       : -227393.6698266006\n",
      "Train Epoch: 176 [0/54000 (0%)] Loss: -221895.156250\n",
      "Train Epoch: 176 [1408/54000 (3%)] Loss: -220601.250000\n",
      "Train Epoch: 176 [2816/54000 (5%)] Loss: -213491.656250\n",
      "Train Epoch: 176 [4224/54000 (8%)] Loss: -219206.953125\n",
      "Train Epoch: 176 [5632/54000 (10%)] Loss: -224412.375000\n",
      "Train Epoch: 176 [7040/54000 (13%)] Loss: -212437.156250\n",
      "Train Epoch: 176 [8448/54000 (16%)] Loss: -215495.843750\n",
      "Train Epoch: 176 [9856/54000 (18%)] Loss: -218911.781250\n",
      "Train Epoch: 176 [11264/54000 (21%)] Loss: -215409.640625\n",
      "Train Epoch: 176 [12672/54000 (23%)] Loss: -220190.343750\n",
      "Train Epoch: 176 [14080/54000 (26%)] Loss: -245208.500000\n",
      "Train Epoch: 176 [15488/54000 (29%)] Loss: -225410.187500\n",
      "Train Epoch: 176 [16896/54000 (31%)] Loss: -217257.234375\n",
      "Train Epoch: 176 [18304/54000 (34%)] Loss: -226768.687500\n",
      "Train Epoch: 176 [19712/54000 (37%)] Loss: -214872.750000\n",
      "Train Epoch: 176 [21120/54000 (39%)] Loss: -213272.312500\n",
      "Train Epoch: 176 [22528/54000 (42%)] Loss: -214394.218750\n",
      "Train Epoch: 176 [23936/54000 (44%)] Loss: -210058.437500\n",
      "Train Epoch: 176 [25344/54000 (47%)] Loss: -222029.687500\n",
      "Train Epoch: 176 [26752/54000 (50%)] Loss: -209072.656250\n",
      "Train Epoch: 176 [28160/54000 (52%)] Loss: -221258.562500\n",
      "Train Epoch: 176 [29568/54000 (55%)] Loss: -220553.843750\n",
      "Train Epoch: 176 [30976/54000 (57%)] Loss: -223092.546875\n",
      "Train Epoch: 176 [32384/54000 (60%)] Loss: -217017.437500\n",
      "Train Epoch: 176 [33792/54000 (63%)] Loss: -243782.515625\n",
      "Train Epoch: 176 [35200/54000 (65%)] Loss: -219143.421875\n",
      "Train Epoch: 176 [36608/54000 (68%)] Loss: -221652.578125\n",
      "Train Epoch: 176 [38016/54000 (70%)] Loss: -214034.859375\n",
      "Train Epoch: 176 [39424/54000 (73%)] Loss: -218151.312500\n",
      "Train Epoch: 176 [40832/54000 (76%)] Loss: -244268.968750\n",
      "Train Epoch: 176 [42240/54000 (78%)] Loss: -223032.890625\n",
      "Train Epoch: 176 [43648/54000 (81%)] Loss: -226542.390625\n",
      "Train Epoch: 176 [45056/54000 (83%)] Loss: -245542.671875\n",
      "Train Epoch: 176 [46464/54000 (86%)] Loss: -212740.125000\n",
      "Train Epoch: 176 [47872/54000 (89%)] Loss: -223002.078125\n",
      "Train Epoch: 176 [49280/54000 (91%)] Loss: -221799.281250\n",
      "Train Epoch: 176 [50688/54000 (94%)] Loss: -218406.000000\n",
      "Train Epoch: 176 [52096/54000 (96%)] Loss: -221837.375000\n",
      "    epoch          : 176\n",
      "    loss           : -221716.3558986244\n",
      "    val_loss       : -227609.62098061165\n",
      "Train Epoch: 177 [0/54000 (0%)] Loss: -226582.562500\n",
      "Train Epoch: 177 [1408/54000 (3%)] Loss: -241354.765625\n",
      "Train Epoch: 177 [2816/54000 (5%)] Loss: -216983.000000\n",
      "Train Epoch: 177 [4224/54000 (8%)] Loss: -222857.781250\n",
      "Train Epoch: 177 [5632/54000 (10%)] Loss: -224709.468750\n",
      "Train Epoch: 177 [7040/54000 (13%)] Loss: -210002.984375\n",
      "Train Epoch: 177 [8448/54000 (16%)] Loss: -219654.156250\n",
      "Train Epoch: 177 [9856/54000 (18%)] Loss: -214072.187500\n",
      "Train Epoch: 177 [11264/54000 (21%)] Loss: -211463.218750\n",
      "Train Epoch: 177 [12672/54000 (23%)] Loss: -244812.250000\n",
      "Train Epoch: 177 [14080/54000 (26%)] Loss: -209059.328125\n",
      "Train Epoch: 177 [15488/54000 (29%)] Loss: -211886.656250\n",
      "Train Epoch: 177 [16896/54000 (31%)] Loss: -223272.781250\n",
      "Train Epoch: 177 [18304/54000 (34%)] Loss: -243058.406250\n",
      "Train Epoch: 177 [19712/54000 (37%)] Loss: -213690.375000\n",
      "Train Epoch: 177 [21120/54000 (39%)] Loss: -225859.968750\n",
      "Train Epoch: 177 [22528/54000 (42%)] Loss: -225992.890625\n",
      "Train Epoch: 177 [23936/54000 (44%)] Loss: -245226.140625\n",
      "Train Epoch: 177 [25344/54000 (47%)] Loss: -242653.968750\n",
      "Train Epoch: 177 [26752/54000 (50%)] Loss: -214904.859375\n",
      "Train Epoch: 177 [28160/54000 (52%)] Loss: -209719.140625\n",
      "Train Epoch: 177 [29568/54000 (55%)] Loss: -220537.031250\n",
      "Train Epoch: 177 [30976/54000 (57%)] Loss: -217250.203125\n",
      "Train Epoch: 177 [32384/54000 (60%)] Loss: -244700.812500\n",
      "Train Epoch: 177 [33792/54000 (63%)] Loss: -219161.421875\n",
      "Train Epoch: 177 [35200/54000 (65%)] Loss: -227791.796875\n",
      "Train Epoch: 177 [36608/54000 (68%)] Loss: -242181.328125\n",
      "Train Epoch: 177 [38016/54000 (70%)] Loss: -222639.937500\n",
      "Train Epoch: 177 [39424/54000 (73%)] Loss: -221277.015625\n",
      "Train Epoch: 177 [40832/54000 (76%)] Loss: -220975.015625\n",
      "Train Epoch: 177 [42240/54000 (78%)] Loss: -214569.406250\n",
      "Train Epoch: 177 [43648/54000 (81%)] Loss: -225647.359375\n",
      "Train Epoch: 177 [45056/54000 (83%)] Loss: -243168.546875\n",
      "Train Epoch: 177 [46464/54000 (86%)] Loss: -217105.765625\n",
      "Train Epoch: 177 [47872/54000 (89%)] Loss: -220765.828125\n",
      "Train Epoch: 177 [49280/54000 (91%)] Loss: -214826.484375\n",
      "Train Epoch: 177 [50688/54000 (94%)] Loss: -244538.390625\n",
      "Train Epoch: 177 [52096/54000 (96%)] Loss: -222800.984375\n",
      "    epoch          : 177\n",
      "    loss           : -221778.02680173446\n",
      "    val_loss       : -227576.43161085175\n",
      "Train Epoch: 178 [0/54000 (0%)] Loss: -226942.875000\n",
      "Train Epoch: 178 [1408/54000 (3%)] Loss: -223743.859375\n",
      "Train Epoch: 178 [2816/54000 (5%)] Loss: -222595.937500\n",
      "Train Epoch: 178 [4224/54000 (8%)] Loss: -217095.750000\n",
      "Train Epoch: 178 [5632/54000 (10%)] Loss: -218925.281250\n",
      "Train Epoch: 178 [7040/54000 (13%)] Loss: -208530.140625\n",
      "Train Epoch: 178 [8448/54000 (16%)] Loss: -221384.843750\n",
      "Train Epoch: 178 [9856/54000 (18%)] Loss: -218217.796875\n",
      "Train Epoch: 178 [11264/54000 (21%)] Loss: -221188.843750\n",
      "Train Epoch: 178 [12672/54000 (23%)] Loss: -221388.546875\n",
      "Train Epoch: 178 [14080/54000 (26%)] Loss: -215495.718750\n",
      "Train Epoch: 178 [15488/54000 (29%)] Loss: -245455.375000\n",
      "Train Epoch: 178 [16896/54000 (31%)] Loss: -215293.140625\n",
      "Train Epoch: 178 [18304/54000 (34%)] Loss: -224782.843750\n",
      "Train Epoch: 178 [19712/54000 (37%)] Loss: -244088.828125\n",
      "Train Epoch: 178 [21120/54000 (39%)] Loss: -213911.312500\n",
      "Train Epoch: 178 [22528/54000 (42%)] Loss: -210003.156250\n",
      "Train Epoch: 178 [23936/54000 (44%)] Loss: -216353.375000\n",
      "Train Epoch: 178 [25344/54000 (47%)] Loss: -216064.515625\n",
      "Train Epoch: 178 [26752/54000 (50%)] Loss: -215082.437500\n",
      "Train Epoch: 178 [28160/54000 (52%)] Loss: -216376.500000\n",
      "Train Epoch: 178 [29568/54000 (55%)] Loss: -225420.546875\n",
      "Train Epoch: 178 [30976/54000 (57%)] Loss: -226339.937500\n",
      "Train Epoch: 178 [32384/54000 (60%)] Loss: -243239.062500\n",
      "Train Epoch: 178 [33792/54000 (63%)] Loss: -225118.656250\n",
      "Train Epoch: 178 [35200/54000 (65%)] Loss: -218751.000000\n",
      "Train Epoch: 178 [36608/54000 (68%)] Loss: -215200.687500\n",
      "Train Epoch: 178 [38016/54000 (70%)] Loss: -214829.656250\n",
      "Train Epoch: 178 [39424/54000 (73%)] Loss: -243691.375000\n",
      "Train Epoch: 178 [40832/54000 (76%)] Loss: -212315.359375\n",
      "Train Epoch: 178 [42240/54000 (78%)] Loss: -222211.515625\n",
      "Train Epoch: 178 [43648/54000 (81%)] Loss: -216277.906250\n",
      "Train Epoch: 178 [45056/54000 (83%)] Loss: -218701.531250\n",
      "Train Epoch: 178 [46464/54000 (86%)] Loss: -221678.421875\n",
      "Train Epoch: 178 [47872/54000 (89%)] Loss: -217531.500000\n",
      "Train Epoch: 178 [49280/54000 (91%)] Loss: -212591.671875\n",
      "Train Epoch: 178 [50688/54000 (94%)] Loss: -215768.781250\n",
      "Train Epoch: 178 [52096/54000 (96%)] Loss: -211748.156250\n",
      "    epoch          : 178\n",
      "    loss           : -221896.7239458732\n",
      "    val_loss       : -227824.3135837462\n",
      "Train Epoch: 179 [0/54000 (0%)] Loss: -211574.421875\n",
      "Train Epoch: 179 [1408/54000 (3%)] Loss: -214032.000000\n",
      "Train Epoch: 179 [2816/54000 (5%)] Loss: -225508.171875\n",
      "Train Epoch: 179 [4224/54000 (8%)] Loss: -217783.812500\n",
      "Train Epoch: 179 [5632/54000 (10%)] Loss: -216740.562500\n",
      "Train Epoch: 179 [7040/54000 (13%)] Loss: -213727.218750\n",
      "Train Epoch: 179 [8448/54000 (16%)] Loss: -215309.531250\n",
      "Train Epoch: 179 [9856/54000 (18%)] Loss: -222702.390625\n",
      "Train Epoch: 179 [11264/54000 (21%)] Loss: -227838.843750\n",
      "Train Epoch: 179 [12672/54000 (23%)] Loss: -227181.875000\n",
      "Train Epoch: 179 [14080/54000 (26%)] Loss: -244121.515625\n",
      "Train Epoch: 179 [15488/54000 (29%)] Loss: -222990.359375\n",
      "Train Epoch: 179 [16896/54000 (31%)] Loss: -218643.109375\n",
      "Train Epoch: 179 [18304/54000 (34%)] Loss: -220923.578125\n",
      "Train Epoch: 179 [19712/54000 (37%)] Loss: -221635.015625\n",
      "Train Epoch: 179 [21120/54000 (39%)] Loss: -217777.421875\n",
      "Train Epoch: 179 [22528/54000 (42%)] Loss: -220923.500000\n",
      "Train Epoch: 179 [23936/54000 (44%)] Loss: -223400.734375\n",
      "Train Epoch: 179 [25344/54000 (47%)] Loss: -225815.328125\n",
      "Train Epoch: 179 [26752/54000 (50%)] Loss: -219819.656250\n",
      "Train Epoch: 179 [28160/54000 (52%)] Loss: -210777.093750\n",
      "Train Epoch: 179 [29568/54000 (55%)] Loss: -211551.828125\n",
      "Train Epoch: 179 [30976/54000 (57%)] Loss: -211245.109375\n",
      "Train Epoch: 179 [32384/54000 (60%)] Loss: -241861.640625\n",
      "Train Epoch: 179 [33792/54000 (63%)] Loss: -223747.531250\n",
      "Train Epoch: 179 [35200/54000 (65%)] Loss: -227071.078125\n",
      "Train Epoch: 179 [36608/54000 (68%)] Loss: -226860.609375\n",
      "Train Epoch: 179 [38016/54000 (70%)] Loss: -245131.187500\n",
      "Train Epoch: 179 [39424/54000 (73%)] Loss: -219448.312500\n",
      "Train Epoch: 179 [40832/54000 (76%)] Loss: -225897.312500\n",
      "Train Epoch: 179 [42240/54000 (78%)] Loss: -226124.296875\n",
      "Train Epoch: 179 [43648/54000 (81%)] Loss: -226834.859375\n",
      "Train Epoch: 179 [45056/54000 (83%)] Loss: -207243.718750\n",
      "Train Epoch: 179 [46464/54000 (86%)] Loss: -223964.890625\n",
      "Train Epoch: 179 [47872/54000 (89%)] Loss: -222480.625000\n",
      "Train Epoch: 179 [49280/54000 (91%)] Loss: -216939.156250\n",
      "Train Epoch: 179 [50688/54000 (94%)] Loss: -243129.359375\n",
      "Train Epoch: 179 [52096/54000 (96%)] Loss: -215605.437500\n",
      "    epoch          : 179\n",
      "    loss           : -221603.2788576555\n",
      "    val_loss       : -228112.96747570502\n",
      "Train Epoch: 180 [0/54000 (0%)] Loss: -241594.453125\n",
      "Train Epoch: 180 [1408/54000 (3%)] Loss: -227799.703125\n",
      "Train Epoch: 180 [2816/54000 (5%)] Loss: -227213.468750\n",
      "Train Epoch: 180 [4224/54000 (8%)] Loss: -224824.562500\n",
      "Train Epoch: 180 [5632/54000 (10%)] Loss: -225484.453125\n",
      "Train Epoch: 180 [7040/54000 (13%)] Loss: -209570.281250\n",
      "Train Epoch: 180 [8448/54000 (16%)] Loss: -218592.468750\n",
      "Train Epoch: 180 [9856/54000 (18%)] Loss: -221521.796875\n",
      "Train Epoch: 180 [11264/54000 (21%)] Loss: -216891.484375\n",
      "Train Epoch: 180 [12672/54000 (23%)] Loss: -215329.000000\n",
      "Train Epoch: 180 [14080/54000 (26%)] Loss: -241885.468750\n",
      "Train Epoch: 180 [15488/54000 (29%)] Loss: -226027.078125\n",
      "Train Epoch: 180 [16896/54000 (31%)] Loss: -225269.437500\n",
      "Train Epoch: 180 [18304/54000 (34%)] Loss: -245235.890625\n",
      "Train Epoch: 180 [19712/54000 (37%)] Loss: -211983.046875\n",
      "Train Epoch: 180 [21120/54000 (39%)] Loss: -214275.671875\n",
      "Train Epoch: 180 [22528/54000 (42%)] Loss: -210152.359375\n",
      "Train Epoch: 180 [23936/54000 (44%)] Loss: -224609.328125\n",
      "Train Epoch: 180 [25344/54000 (47%)] Loss: -245259.453125\n",
      "Train Epoch: 180 [26752/54000 (50%)] Loss: -216437.875000\n",
      "Train Epoch: 180 [28160/54000 (52%)] Loss: -229035.750000\n",
      "Train Epoch: 180 [29568/54000 (55%)] Loss: -227380.843750\n",
      "Train Epoch: 180 [30976/54000 (57%)] Loss: -218381.968750\n",
      "Train Epoch: 180 [32384/54000 (60%)] Loss: -226387.421875\n",
      "Train Epoch: 180 [33792/54000 (63%)] Loss: -224748.437500\n",
      "Train Epoch: 180 [35200/54000 (65%)] Loss: -225561.281250\n",
      "Train Epoch: 180 [36608/54000 (68%)] Loss: -246206.656250\n",
      "Train Epoch: 180 [38016/54000 (70%)] Loss: -219738.843750\n",
      "Train Epoch: 180 [39424/54000 (73%)] Loss: -215356.906250\n",
      "Train Epoch: 180 [40832/54000 (76%)] Loss: -211646.640625\n",
      "Train Epoch: 180 [42240/54000 (78%)] Loss: -224175.531250\n",
      "Train Epoch: 180 [43648/54000 (81%)] Loss: -208428.156250\n",
      "Train Epoch: 180 [45056/54000 (83%)] Loss: -215615.718750\n",
      "Train Epoch: 180 [46464/54000 (86%)] Loss: -219123.125000\n",
      "Train Epoch: 180 [47872/54000 (89%)] Loss: -219634.812500\n",
      "Train Epoch: 180 [49280/54000 (91%)] Loss: -220348.343750\n",
      "Train Epoch: 180 [50688/54000 (94%)] Loss: -215805.421875\n",
      "Train Epoch: 180 [52096/54000 (96%)] Loss: -209747.875000\n",
      "    epoch          : 180\n",
      "    loss           : -221939.41611842104\n",
      "    val_loss       : -228026.4719357374\n",
      "Train Epoch: 181 [0/54000 (0%)] Loss: -214358.093750\n",
      "Train Epoch: 181 [1408/54000 (3%)] Loss: -211624.437500\n",
      "Train Epoch: 181 [2816/54000 (5%)] Loss: -228090.437500\n",
      "Train Epoch: 181 [4224/54000 (8%)] Loss: -214065.609375\n",
      "Train Epoch: 181 [5632/54000 (10%)] Loss: -216346.218750\n",
      "Train Epoch: 181 [7040/54000 (13%)] Loss: -209926.546875\n",
      "Train Epoch: 181 [8448/54000 (16%)] Loss: -206835.140625\n",
      "Train Epoch: 181 [9856/54000 (18%)] Loss: -212138.562500\n",
      "Train Epoch: 181 [11264/54000 (21%)] Loss: -223728.125000\n",
      "Train Epoch: 181 [12672/54000 (23%)] Loss: -224223.906250\n",
      "Train Epoch: 181 [14080/54000 (26%)] Loss: -210083.640625\n",
      "Train Epoch: 181 [15488/54000 (29%)] Loss: -215706.078125\n",
      "Train Epoch: 181 [16896/54000 (31%)] Loss: -210240.765625\n",
      "Train Epoch: 181 [18304/54000 (34%)] Loss: -227409.015625\n",
      "Train Epoch: 181 [19712/54000 (37%)] Loss: -222602.375000\n",
      "Train Epoch: 181 [21120/54000 (39%)] Loss: -213444.515625\n",
      "Train Epoch: 181 [22528/54000 (42%)] Loss: -216524.250000\n",
      "Train Epoch: 181 [23936/54000 (44%)] Loss: -214161.515625\n",
      "Train Epoch: 181 [25344/54000 (47%)] Loss: -221898.703125\n",
      "Train Epoch: 181 [26752/54000 (50%)] Loss: -219695.578125\n",
      "Train Epoch: 181 [28160/54000 (52%)] Loss: -210429.937500\n",
      "Train Epoch: 181 [29568/54000 (55%)] Loss: -241171.843750\n",
      "Train Epoch: 181 [30976/54000 (57%)] Loss: -216254.890625\n",
      "Train Epoch: 181 [32384/54000 (60%)] Loss: -216418.390625\n",
      "Train Epoch: 181 [33792/54000 (63%)] Loss: -225086.625000\n",
      "Train Epoch: 181 [35200/54000 (65%)] Loss: -241654.484375\n",
      "Train Epoch: 181 [36608/54000 (68%)] Loss: -219104.125000\n",
      "Train Epoch: 181 [38016/54000 (70%)] Loss: -215362.484375\n",
      "Train Epoch: 181 [39424/54000 (73%)] Loss: -212803.687500\n",
      "Train Epoch: 181 [40832/54000 (76%)] Loss: -220597.078125\n",
      "Train Epoch: 181 [42240/54000 (78%)] Loss: -222356.218750\n",
      "Train Epoch: 181 [43648/54000 (81%)] Loss: -224836.093750\n",
      "Train Epoch: 181 [45056/54000 (83%)] Loss: -227206.578125\n",
      "Train Epoch: 181 [46464/54000 (86%)] Loss: -226422.125000\n",
      "Train Epoch: 181 [47872/54000 (89%)] Loss: -215340.500000\n",
      "Train Epoch: 181 [49280/54000 (91%)] Loss: -228000.312500\n",
      "Train Epoch: 181 [50688/54000 (94%)] Loss: -211092.921875\n",
      "Train Epoch: 181 [52096/54000 (96%)] Loss: -227098.468750\n",
      "    epoch          : 181\n",
      "    loss           : -221855.97476824163\n",
      "    val_loss       : -227830.1557319455\n",
      "Train Epoch: 182 [0/54000 (0%)] Loss: -213299.312500\n",
      "Train Epoch: 182 [1408/54000 (3%)] Loss: -213871.093750\n",
      "Train Epoch: 182 [2816/54000 (5%)] Loss: -225615.640625\n",
      "Train Epoch: 182 [4224/54000 (8%)] Loss: -212089.593750\n",
      "Train Epoch: 182 [5632/54000 (10%)] Loss: -219944.500000\n",
      "Train Epoch: 182 [7040/54000 (13%)] Loss: -215757.218750\n",
      "Train Epoch: 182 [8448/54000 (16%)] Loss: -214921.468750\n",
      "Train Epoch: 182 [9856/54000 (18%)] Loss: -225657.859375\n",
      "Train Epoch: 182 [11264/54000 (21%)] Loss: -219422.343750\n",
      "Train Epoch: 182 [12672/54000 (23%)] Loss: -223761.937500\n",
      "Train Epoch: 182 [14080/54000 (26%)] Loss: -214677.062500\n",
      "Train Epoch: 182 [15488/54000 (29%)] Loss: -213738.734375\n",
      "Train Epoch: 182 [16896/54000 (31%)] Loss: -219809.156250\n",
      "Train Epoch: 182 [18304/54000 (34%)] Loss: -208257.640625\n",
      "Train Epoch: 182 [19712/54000 (37%)] Loss: -225626.484375\n",
      "Train Epoch: 182 [21120/54000 (39%)] Loss: -228105.421875\n",
      "Train Epoch: 182 [22528/54000 (42%)] Loss: -213979.140625\n",
      "Train Epoch: 182 [23936/54000 (44%)] Loss: -219841.343750\n",
      "Train Epoch: 182 [25344/54000 (47%)] Loss: -214110.234375\n",
      "Train Epoch: 182 [26752/54000 (50%)] Loss: -225236.968750\n",
      "Train Epoch: 182 [28160/54000 (52%)] Loss: -218370.687500\n",
      "Train Epoch: 182 [29568/54000 (55%)] Loss: -223029.531250\n",
      "Train Epoch: 182 [30976/54000 (57%)] Loss: -216122.046875\n",
      "Train Epoch: 182 [32384/54000 (60%)] Loss: -216864.406250\n",
      "Train Epoch: 182 [33792/54000 (63%)] Loss: -219040.609375\n",
      "Train Epoch: 182 [35200/54000 (65%)] Loss: -224459.687500\n",
      "Train Epoch: 182 [36608/54000 (68%)] Loss: -244662.625000\n",
      "Train Epoch: 182 [38016/54000 (70%)] Loss: -215521.703125\n",
      "Train Epoch: 182 [39424/54000 (73%)] Loss: -219874.359375\n",
      "Train Epoch: 182 [40832/54000 (76%)] Loss: -215326.375000\n",
      "Train Epoch: 182 [42240/54000 (78%)] Loss: -221903.781250\n",
      "Train Epoch: 182 [43648/54000 (81%)] Loss: -244875.578125\n",
      "Train Epoch: 182 [45056/54000 (83%)] Loss: -226741.406250\n",
      "Train Epoch: 182 [46464/54000 (86%)] Loss: -215255.062500\n",
      "Train Epoch: 182 [47872/54000 (89%)] Loss: -216859.875000\n",
      "Train Epoch: 182 [49280/54000 (91%)] Loss: -214630.531250\n",
      "Train Epoch: 182 [50688/54000 (94%)] Loss: -215781.593750\n",
      "Train Epoch: 182 [52096/54000 (96%)] Loss: -213693.296875\n",
      "    epoch          : 182\n",
      "    loss           : -221987.21011513157\n",
      "    val_loss       : -228321.54263528963\n",
      "Train Epoch: 183 [0/54000 (0%)] Loss: -244968.859375\n",
      "Train Epoch: 183 [1408/54000 (3%)] Loss: -224743.906250\n",
      "Train Epoch: 183 [2816/54000 (5%)] Loss: -212808.875000\n",
      "Train Epoch: 183 [4224/54000 (8%)] Loss: -218186.812500\n",
      "Train Epoch: 183 [5632/54000 (10%)] Loss: -218524.218750\n",
      "Train Epoch: 183 [7040/54000 (13%)] Loss: -226964.750000\n",
      "Train Epoch: 183 [8448/54000 (16%)] Loss: -217618.093750\n",
      "Train Epoch: 183 [9856/54000 (18%)] Loss: -242831.484375\n",
      "Train Epoch: 183 [11264/54000 (21%)] Loss: -219726.750000\n",
      "Train Epoch: 183 [12672/54000 (23%)] Loss: -213064.171875\n",
      "Train Epoch: 183 [14080/54000 (26%)] Loss: -212480.687500\n",
      "Train Epoch: 183 [15488/54000 (29%)] Loss: -219008.437500\n",
      "Train Epoch: 183 [16896/54000 (31%)] Loss: -223735.218750\n",
      "Train Epoch: 183 [18304/54000 (34%)] Loss: -227509.718750\n",
      "Train Epoch: 183 [19712/54000 (37%)] Loss: -228370.625000\n",
      "Train Epoch: 183 [21120/54000 (39%)] Loss: -246084.484375\n",
      "Train Epoch: 183 [22528/54000 (42%)] Loss: -222734.281250\n",
      "Train Epoch: 183 [23936/54000 (44%)] Loss: -214757.062500\n",
      "Train Epoch: 183 [25344/54000 (47%)] Loss: -224998.187500\n",
      "Train Epoch: 183 [26752/54000 (50%)] Loss: -214934.421875\n",
      "Train Epoch: 183 [28160/54000 (52%)] Loss: -216962.000000\n",
      "Train Epoch: 183 [29568/54000 (55%)] Loss: -219344.187500\n",
      "Train Epoch: 183 [30976/54000 (57%)] Loss: -223265.718750\n",
      "Train Epoch: 183 [32384/54000 (60%)] Loss: -222434.140625\n",
      "Train Epoch: 183 [33792/54000 (63%)] Loss: -241052.687500\n",
      "Train Epoch: 183 [35200/54000 (65%)] Loss: -216426.828125\n",
      "Train Epoch: 183 [36608/54000 (68%)] Loss: -216241.984375\n",
      "Train Epoch: 183 [38016/54000 (70%)] Loss: -211622.375000\n",
      "Train Epoch: 183 [39424/54000 (73%)] Loss: -216187.500000\n",
      "Train Epoch: 183 [40832/54000 (76%)] Loss: -224829.093750\n",
      "Train Epoch: 183 [42240/54000 (78%)] Loss: -216950.375000\n",
      "Train Epoch: 183 [43648/54000 (81%)] Loss: -215586.484375\n",
      "Train Epoch: 183 [45056/54000 (83%)] Loss: -245377.515625\n",
      "Train Epoch: 183 [46464/54000 (86%)] Loss: -224567.875000\n",
      "Train Epoch: 183 [47872/54000 (89%)] Loss: -224011.437500\n",
      "Train Epoch: 183 [49280/54000 (91%)] Loss: -223230.468750\n",
      "Train Epoch: 183 [50688/54000 (94%)] Loss: -221659.265625\n",
      "Train Epoch: 183 [52096/54000 (96%)] Loss: -210013.687500\n",
      "    epoch          : 183\n",
      "    loss           : -222060.5132326555\n",
      "    val_loss       : -228142.6837962081\n",
      "Train Epoch: 184 [0/54000 (0%)] Loss: -227362.078125\n",
      "Train Epoch: 184 [1408/54000 (3%)] Loss: -227527.125000\n",
      "Train Epoch: 184 [2816/54000 (5%)] Loss: -245906.156250\n",
      "Train Epoch: 184 [4224/54000 (8%)] Loss: -219812.593750\n",
      "Train Epoch: 184 [5632/54000 (10%)] Loss: -216181.812500\n",
      "Train Epoch: 184 [7040/54000 (13%)] Loss: -211253.421875\n",
      "Train Epoch: 184 [8448/54000 (16%)] Loss: -209821.531250\n",
      "Train Epoch: 184 [9856/54000 (18%)] Loss: -214718.828125\n",
      "Train Epoch: 184 [11264/54000 (21%)] Loss: -209511.062500\n",
      "Train Epoch: 184 [12672/54000 (23%)] Loss: -226233.062500\n",
      "Train Epoch: 184 [14080/54000 (26%)] Loss: -214977.609375\n",
      "Train Epoch: 184 [15488/54000 (29%)] Loss: -244567.671875\n",
      "Train Epoch: 184 [16896/54000 (31%)] Loss: -214442.781250\n",
      "Train Epoch: 184 [18304/54000 (34%)] Loss: -222699.953125\n",
      "Train Epoch: 184 [19712/54000 (37%)] Loss: -220113.234375\n",
      "Train Epoch: 184 [21120/54000 (39%)] Loss: -225251.187500\n",
      "Train Epoch: 184 [22528/54000 (42%)] Loss: -245119.375000\n",
      "Train Epoch: 184 [23936/54000 (44%)] Loss: -222912.031250\n",
      "Train Epoch: 184 [25344/54000 (47%)] Loss: -221653.125000\n",
      "Train Epoch: 184 [26752/54000 (50%)] Loss: -220225.015625\n",
      "Train Epoch: 184 [28160/54000 (52%)] Loss: -225619.453125\n",
      "Train Epoch: 184 [29568/54000 (55%)] Loss: -216990.468750\n",
      "Train Epoch: 184 [30976/54000 (57%)] Loss: -215900.187500\n",
      "Train Epoch: 184 [32384/54000 (60%)] Loss: -223343.437500\n",
      "Train Epoch: 184 [33792/54000 (63%)] Loss: -243971.531250\n",
      "Train Epoch: 184 [35200/54000 (65%)] Loss: -215125.078125\n",
      "Train Epoch: 184 [36608/54000 (68%)] Loss: -219862.328125\n",
      "Train Epoch: 184 [38016/54000 (70%)] Loss: -217215.656250\n",
      "Train Epoch: 184 [39424/54000 (73%)] Loss: -212685.015625\n",
      "Train Epoch: 184 [40832/54000 (76%)] Loss: -246816.265625\n",
      "Train Epoch: 184 [42240/54000 (78%)] Loss: -212732.734375\n",
      "Train Epoch: 184 [43648/54000 (81%)] Loss: -245205.484375\n",
      "Train Epoch: 184 [45056/54000 (83%)] Loss: -217250.343750\n",
      "Train Epoch: 184 [46464/54000 (86%)] Loss: -217946.609375\n",
      "Train Epoch: 184 [47872/54000 (89%)] Loss: -213268.031250\n",
      "Train Epoch: 184 [49280/54000 (91%)] Loss: -225710.625000\n",
      "Train Epoch: 184 [50688/54000 (94%)] Loss: -221836.640625\n",
      "Train Epoch: 184 [52096/54000 (96%)] Loss: -241366.562500\n",
      "    epoch          : 184\n",
      "    loss           : -221967.28566088516\n",
      "    val_loss       : -227904.6698146913\n",
      "Train Epoch: 185 [0/54000 (0%)] Loss: -245600.218750\n",
      "Train Epoch: 185 [1408/54000 (3%)] Loss: -221496.265625\n",
      "Train Epoch: 185 [2816/54000 (5%)] Loss: -215461.468750\n",
      "Train Epoch: 185 [4224/54000 (8%)] Loss: -219871.968750\n",
      "Train Epoch: 185 [5632/54000 (10%)] Loss: -212369.171875\n",
      "Train Epoch: 185 [7040/54000 (13%)] Loss: -209765.375000\n",
      "Train Epoch: 185 [8448/54000 (16%)] Loss: -217185.984375\n",
      "Train Epoch: 185 [9856/54000 (18%)] Loss: -218626.750000\n",
      "Train Epoch: 185 [11264/54000 (21%)] Loss: -215487.718750\n",
      "Train Epoch: 185 [12672/54000 (23%)] Loss: -228417.234375\n",
      "Train Epoch: 185 [14080/54000 (26%)] Loss: -220460.125000\n",
      "Train Epoch: 185 [15488/54000 (29%)] Loss: -220459.265625\n",
      "Train Epoch: 185 [16896/54000 (31%)] Loss: -219103.984375\n",
      "Train Epoch: 185 [18304/54000 (34%)] Loss: -244443.078125\n",
      "Train Epoch: 185 [19712/54000 (37%)] Loss: -211918.968750\n",
      "Train Epoch: 185 [21120/54000 (39%)] Loss: -215050.093750\n",
      "Train Epoch: 185 [22528/54000 (42%)] Loss: -220377.171875\n",
      "Train Epoch: 185 [23936/54000 (44%)] Loss: -214937.343750\n",
      "Train Epoch: 185 [25344/54000 (47%)] Loss: -243232.203125\n",
      "Train Epoch: 185 [26752/54000 (50%)] Loss: -219031.093750\n",
      "Train Epoch: 185 [28160/54000 (52%)] Loss: -211280.015625\n",
      "Train Epoch: 185 [29568/54000 (55%)] Loss: -228862.062500\n",
      "Train Epoch: 185 [30976/54000 (57%)] Loss: -243110.437500\n",
      "Train Epoch: 185 [32384/54000 (60%)] Loss: -224729.328125\n",
      "Train Epoch: 185 [33792/54000 (63%)] Loss: -219997.140625\n",
      "Train Epoch: 185 [35200/54000 (65%)] Loss: -218785.312500\n",
      "Train Epoch: 185 [36608/54000 (68%)] Loss: -242419.625000\n",
      "Train Epoch: 185 [38016/54000 (70%)] Loss: -229043.500000\n",
      "Train Epoch: 185 [39424/54000 (73%)] Loss: -228375.218750\n",
      "Train Epoch: 185 [40832/54000 (76%)] Loss: -215442.593750\n",
      "Train Epoch: 185 [42240/54000 (78%)] Loss: -221761.500000\n",
      "Train Epoch: 185 [43648/54000 (81%)] Loss: -242337.421875\n",
      "Train Epoch: 185 [45056/54000 (83%)] Loss: -216099.718750\n",
      "Train Epoch: 185 [46464/54000 (86%)] Loss: -215243.671875\n",
      "Train Epoch: 185 [47872/54000 (89%)] Loss: -215534.156250\n",
      "Train Epoch: 185 [49280/54000 (91%)] Loss: -222570.781250\n",
      "Train Epoch: 185 [50688/54000 (94%)] Loss: -216926.531250\n",
      "Train Epoch: 185 [52096/54000 (96%)] Loss: -227150.000000\n",
      "    epoch          : 185\n",
      "    loss           : -222131.71646979664\n",
      "    val_loss       : -228086.63426543446\n",
      "Train Epoch: 186 [0/54000 (0%)] Loss: -245355.937500\n",
      "Train Epoch: 186 [1408/54000 (3%)] Loss: -241580.562500\n",
      "Train Epoch: 186 [2816/54000 (5%)] Loss: -225022.281250\n",
      "Train Epoch: 186 [4224/54000 (8%)] Loss: -216226.250000\n",
      "Train Epoch: 186 [5632/54000 (10%)] Loss: -219176.093750\n",
      "Train Epoch: 186 [7040/54000 (13%)] Loss: -219684.718750\n",
      "Train Epoch: 186 [8448/54000 (16%)] Loss: -226843.390625\n",
      "Train Epoch: 186 [9856/54000 (18%)] Loss: -227942.296875\n",
      "Train Epoch: 186 [11264/54000 (21%)] Loss: -215900.546875\n",
      "Train Epoch: 186 [12672/54000 (23%)] Loss: -212066.234375\n",
      "Train Epoch: 186 [14080/54000 (26%)] Loss: -227068.109375\n",
      "Train Epoch: 186 [15488/54000 (29%)] Loss: -219332.609375\n",
      "Train Epoch: 186 [16896/54000 (31%)] Loss: -206293.625000\n",
      "Train Epoch: 186 [18304/54000 (34%)] Loss: -222199.937500\n",
      "Train Epoch: 186 [19712/54000 (37%)] Loss: -214813.015625\n",
      "Train Epoch: 186 [21120/54000 (39%)] Loss: -227088.218750\n",
      "Train Epoch: 186 [22528/54000 (42%)] Loss: -223403.000000\n",
      "Train Epoch: 186 [23936/54000 (44%)] Loss: -218590.390625\n",
      "Train Epoch: 186 [25344/54000 (47%)] Loss: -222572.750000\n",
      "Train Epoch: 186 [26752/54000 (50%)] Loss: -221208.531250\n",
      "Train Epoch: 186 [28160/54000 (52%)] Loss: -219967.984375\n",
      "Train Epoch: 186 [29568/54000 (55%)] Loss: -210018.187500\n",
      "Train Epoch: 186 [30976/54000 (57%)] Loss: -220544.453125\n",
      "Train Epoch: 186 [32384/54000 (60%)] Loss: -218939.281250\n",
      "Train Epoch: 186 [33792/54000 (63%)] Loss: -226051.437500\n",
      "Train Epoch: 186 [35200/54000 (65%)] Loss: -214115.296875\n",
      "Train Epoch: 186 [36608/54000 (68%)] Loss: -227452.656250\n",
      "Train Epoch: 186 [38016/54000 (70%)] Loss: -208199.250000\n",
      "Train Epoch: 186 [39424/54000 (73%)] Loss: -217682.687500\n",
      "Train Epoch: 186 [40832/54000 (76%)] Loss: -222371.750000\n",
      "Train Epoch: 186 [42240/54000 (78%)] Loss: -216855.250000\n",
      "Train Epoch: 186 [43648/54000 (81%)] Loss: -218894.031250\n",
      "Train Epoch: 186 [45056/54000 (83%)] Loss: -215103.218750\n",
      "Train Epoch: 186 [46464/54000 (86%)] Loss: -211674.625000\n",
      "Train Epoch: 186 [47872/54000 (89%)] Loss: -216426.937500\n",
      "Train Epoch: 186 [49280/54000 (91%)] Loss: -217563.046875\n",
      "Train Epoch: 186 [50688/54000 (94%)] Loss: -216896.359375\n",
      "Train Epoch: 186 [52096/54000 (96%)] Loss: -215264.468750\n",
      "    epoch          : 186\n",
      "    loss           : -222227.23119766745\n",
      "    val_loss       : -227967.97409727515\n",
      "Train Epoch: 187 [0/54000 (0%)] Loss: -211770.640625\n",
      "Train Epoch: 187 [1408/54000 (3%)] Loss: -213880.250000\n",
      "Train Epoch: 187 [2816/54000 (5%)] Loss: -216073.312500\n",
      "Train Epoch: 187 [4224/54000 (8%)] Loss: -219995.750000\n",
      "Train Epoch: 187 [5632/54000 (10%)] Loss: -211921.796875\n",
      "Train Epoch: 187 [7040/54000 (13%)] Loss: -228642.890625\n",
      "Train Epoch: 187 [8448/54000 (16%)] Loss: -227099.187500\n",
      "Train Epoch: 187 [9856/54000 (18%)] Loss: -216676.953125\n",
      "Train Epoch: 187 [11264/54000 (21%)] Loss: -214466.625000\n",
      "Train Epoch: 187 [12672/54000 (23%)] Loss: -208373.734375\n",
      "Train Epoch: 187 [14080/54000 (26%)] Loss: -217835.140625\n",
      "Train Epoch: 187 [15488/54000 (29%)] Loss: -220330.343750\n",
      "Train Epoch: 187 [16896/54000 (31%)] Loss: -210922.000000\n",
      "Train Epoch: 187 [18304/54000 (34%)] Loss: -215108.875000\n",
      "Train Epoch: 187 [19712/54000 (37%)] Loss: -224334.796875\n",
      "Train Epoch: 187 [21120/54000 (39%)] Loss: -225129.656250\n",
      "Train Epoch: 187 [22528/54000 (42%)] Loss: -228658.734375\n",
      "Train Epoch: 187 [23936/54000 (44%)] Loss: -223401.296875\n",
      "Train Epoch: 187 [25344/54000 (47%)] Loss: -223506.515625\n",
      "Train Epoch: 187 [26752/54000 (50%)] Loss: -216125.265625\n",
      "Train Epoch: 187 [28160/54000 (52%)] Loss: -217105.281250\n",
      "Train Epoch: 187 [29568/54000 (55%)] Loss: -213460.265625\n",
      "Train Epoch: 187 [30976/54000 (57%)] Loss: -225553.171875\n",
      "Train Epoch: 187 [32384/54000 (60%)] Loss: -244689.421875\n",
      "Train Epoch: 187 [33792/54000 (63%)] Loss: -221746.828125\n",
      "Train Epoch: 187 [35200/54000 (65%)] Loss: -215870.750000\n",
      "Train Epoch: 187 [36608/54000 (68%)] Loss: -216230.078125\n",
      "Train Epoch: 187 [38016/54000 (70%)] Loss: -220489.937500\n",
      "Train Epoch: 187 [39424/54000 (73%)] Loss: -243326.578125\n",
      "Train Epoch: 187 [40832/54000 (76%)] Loss: -212858.281250\n",
      "Train Epoch: 187 [42240/54000 (78%)] Loss: -228597.171875\n",
      "Train Epoch: 187 [43648/54000 (81%)] Loss: -226813.203125\n",
      "Train Epoch: 187 [45056/54000 (83%)] Loss: -210623.390625\n",
      "Train Epoch: 187 [46464/54000 (86%)] Loss: -217026.750000\n",
      "Train Epoch: 187 [47872/54000 (89%)] Loss: -218804.375000\n",
      "Train Epoch: 187 [49280/54000 (91%)] Loss: -222338.062500\n",
      "Train Epoch: 187 [50688/54000 (94%)] Loss: -212943.718750\n",
      "Train Epoch: 187 [52096/54000 (96%)] Loss: -223994.687500\n",
      "    epoch          : 187\n",
      "    loss           : -222280.2180771531\n",
      "    val_loss       : -228308.49562928735\n",
      "Train Epoch: 188 [0/54000 (0%)] Loss: -219133.781250\n",
      "Train Epoch: 188 [1408/54000 (3%)] Loss: -224982.031250\n",
      "Train Epoch: 188 [2816/54000 (5%)] Loss: -215542.437500\n",
      "Train Epoch: 188 [4224/54000 (8%)] Loss: -223965.171875\n",
      "Train Epoch: 188 [5632/54000 (10%)] Loss: -215582.328125\n",
      "Train Epoch: 188 [7040/54000 (13%)] Loss: -218903.062500\n",
      "Train Epoch: 188 [8448/54000 (16%)] Loss: -214771.156250\n",
      "Train Epoch: 188 [9856/54000 (18%)] Loss: -210288.953125\n",
      "Train Epoch: 188 [11264/54000 (21%)] Loss: -212837.000000\n",
      "Train Epoch: 188 [12672/54000 (23%)] Loss: -219694.015625\n",
      "Train Epoch: 188 [14080/54000 (26%)] Loss: -225102.812500\n",
      "Train Epoch: 188 [15488/54000 (29%)] Loss: -245925.937500\n",
      "Train Epoch: 188 [16896/54000 (31%)] Loss: -229659.765625\n",
      "Train Epoch: 188 [18304/54000 (34%)] Loss: -218462.875000\n",
      "Train Epoch: 188 [19712/54000 (37%)] Loss: -216933.687500\n",
      "Train Epoch: 188 [21120/54000 (39%)] Loss: -217630.546875\n",
      "Train Epoch: 188 [22528/54000 (42%)] Loss: -216317.734375\n",
      "Train Epoch: 188 [23936/54000 (44%)] Loss: -220578.156250\n",
      "Train Epoch: 188 [25344/54000 (47%)] Loss: -215875.218750\n",
      "Train Epoch: 188 [26752/54000 (50%)] Loss: -215014.859375\n",
      "Train Epoch: 188 [28160/54000 (52%)] Loss: -214642.562500\n",
      "Train Epoch: 188 [29568/54000 (55%)] Loss: -244357.781250\n",
      "Train Epoch: 188 [30976/54000 (57%)] Loss: -225693.953125\n",
      "Train Epoch: 188 [32384/54000 (60%)] Loss: -217659.640625\n",
      "Train Epoch: 188 [33792/54000 (63%)] Loss: -224876.484375\n",
      "Train Epoch: 188 [35200/54000 (65%)] Loss: -245679.984375\n",
      "Train Epoch: 188 [36608/54000 (68%)] Loss: -212014.843750\n",
      "Train Epoch: 188 [38016/54000 (70%)] Loss: -211637.468750\n",
      "Train Epoch: 188 [39424/54000 (73%)] Loss: -216195.921875\n",
      "Train Epoch: 188 [40832/54000 (76%)] Loss: -226231.843750\n",
      "Train Epoch: 188 [42240/54000 (78%)] Loss: -224948.234375\n",
      "Train Epoch: 188 [43648/54000 (81%)] Loss: -228311.593750\n",
      "Train Epoch: 188 [45056/54000 (83%)] Loss: -216214.937500\n",
      "Train Epoch: 188 [46464/54000 (86%)] Loss: -246807.203125\n",
      "Train Epoch: 188 [47872/54000 (89%)] Loss: -222773.078125\n",
      "Train Epoch: 188 [49280/54000 (91%)] Loss: -217117.812500\n",
      "Train Epoch: 188 [50688/54000 (94%)] Loss: -213921.859375\n",
      "Train Epoch: 188 [52096/54000 (96%)] Loss: -223668.578125\n",
      "    epoch          : 188\n",
      "    loss           : -222222.22151614833\n",
      "    val_loss       : -228484.76246903584\n",
      "Train Epoch: 189 [0/54000 (0%)] Loss: -221381.234375\n",
      "Train Epoch: 189 [1408/54000 (3%)] Loss: -229075.281250\n",
      "Train Epoch: 189 [2816/54000 (5%)] Loss: -224680.375000\n",
      "Train Epoch: 189 [4224/54000 (8%)] Loss: -210444.734375\n",
      "Train Epoch: 189 [5632/54000 (10%)] Loss: -213313.875000\n",
      "Train Epoch: 189 [7040/54000 (13%)] Loss: -212914.750000\n",
      "Train Epoch: 189 [8448/54000 (16%)] Loss: -211934.609375\n",
      "Train Epoch: 189 [9856/54000 (18%)] Loss: -245875.875000\n",
      "Train Epoch: 189 [11264/54000 (21%)] Loss: -217939.687500\n",
      "Train Epoch: 189 [12672/54000 (23%)] Loss: -227416.781250\n",
      "Train Epoch: 189 [14080/54000 (26%)] Loss: -225647.546875\n",
      "Train Epoch: 189 [15488/54000 (29%)] Loss: -218479.968750\n",
      "Train Epoch: 189 [16896/54000 (31%)] Loss: -221511.812500\n",
      "Train Epoch: 189 [18304/54000 (34%)] Loss: -211720.468750\n",
      "Train Epoch: 189 [19712/54000 (37%)] Loss: -217005.890625\n",
      "Train Epoch: 189 [21120/54000 (39%)] Loss: -224393.578125\n",
      "Train Epoch: 189 [22528/54000 (42%)] Loss: -209836.968750\n",
      "Train Epoch: 189 [23936/54000 (44%)] Loss: -212656.437500\n",
      "Train Epoch: 189 [25344/54000 (47%)] Loss: -219859.812500\n",
      "Train Epoch: 189 [26752/54000 (50%)] Loss: -219473.937500\n",
      "Train Epoch: 189 [28160/54000 (52%)] Loss: -214774.234375\n",
      "Train Epoch: 189 [29568/54000 (55%)] Loss: -224336.906250\n",
      "Train Epoch: 189 [30976/54000 (57%)] Loss: -228452.562500\n",
      "Train Epoch: 189 [32384/54000 (60%)] Loss: -224226.187500\n",
      "Train Epoch: 189 [33792/54000 (63%)] Loss: -224054.328125\n",
      "Train Epoch: 189 [35200/54000 (65%)] Loss: -218021.656250\n",
      "Train Epoch: 189 [36608/54000 (68%)] Loss: -216626.656250\n",
      "Train Epoch: 189 [38016/54000 (70%)] Loss: -213613.546875\n",
      "Train Epoch: 189 [39424/54000 (73%)] Loss: -215326.437500\n",
      "Train Epoch: 189 [40832/54000 (76%)] Loss: -217367.531250\n",
      "Train Epoch: 189 [42240/54000 (78%)] Loss: -227550.640625\n",
      "Train Epoch: 189 [43648/54000 (81%)] Loss: -228580.843750\n",
      "Train Epoch: 189 [45056/54000 (83%)] Loss: -215939.390625\n",
      "Train Epoch: 189 [46464/54000 (86%)] Loss: -218817.625000\n",
      "Train Epoch: 189 [47872/54000 (89%)] Loss: -221420.062500\n",
      "Train Epoch: 189 [49280/54000 (91%)] Loss: -218201.203125\n",
      "Train Epoch: 189 [50688/54000 (94%)] Loss: -243906.937500\n",
      "Train Epoch: 189 [52096/54000 (96%)] Loss: -226370.593750\n",
      "    epoch          : 189\n",
      "    loss           : -222504.1165146531\n",
      "    val_loss       : -228741.08276962652\n",
      "Train Epoch: 190 [0/54000 (0%)] Loss: -245937.000000\n",
      "Train Epoch: 190 [1408/54000 (3%)] Loss: -223547.250000\n",
      "Train Epoch: 190 [2816/54000 (5%)] Loss: -224576.843750\n",
      "Train Epoch: 190 [4224/54000 (8%)] Loss: -217835.390625\n",
      "Train Epoch: 190 [5632/54000 (10%)] Loss: -219207.843750\n",
      "Train Epoch: 190 [7040/54000 (13%)] Loss: -216379.875000\n",
      "Train Epoch: 190 [8448/54000 (16%)] Loss: -215197.218750\n",
      "Train Epoch: 190 [9856/54000 (18%)] Loss: -229353.781250\n",
      "Train Epoch: 190 [11264/54000 (21%)] Loss: -208728.937500\n",
      "Train Epoch: 190 [12672/54000 (23%)] Loss: -214010.078125\n",
      "Train Epoch: 190 [14080/54000 (26%)] Loss: -218539.000000\n",
      "Train Epoch: 190 [15488/54000 (29%)] Loss: -222080.640625\n",
      "Train Epoch: 190 [16896/54000 (31%)] Loss: -223251.375000\n",
      "Train Epoch: 190 [18304/54000 (34%)] Loss: -244844.000000\n",
      "Train Epoch: 190 [19712/54000 (37%)] Loss: -219014.218750\n",
      "Train Epoch: 190 [21120/54000 (39%)] Loss: -217515.296875\n",
      "Train Epoch: 190 [22528/54000 (42%)] Loss: -211837.328125\n",
      "Train Epoch: 190 [23936/54000 (44%)] Loss: -226417.734375\n",
      "Train Epoch: 190 [25344/54000 (47%)] Loss: -240341.312500\n",
      "Train Epoch: 190 [26752/54000 (50%)] Loss: -222292.046875\n",
      "Train Epoch: 190 [28160/54000 (52%)] Loss: -222546.093750\n",
      "Train Epoch: 190 [29568/54000 (55%)] Loss: -217231.250000\n",
      "Train Epoch: 190 [30976/54000 (57%)] Loss: -219331.906250\n",
      "Train Epoch: 190 [32384/54000 (60%)] Loss: -218166.171875\n",
      "Train Epoch: 190 [33792/54000 (63%)] Loss: -218920.468750\n",
      "Train Epoch: 190 [35200/54000 (65%)] Loss: -218648.484375\n",
      "Train Epoch: 190 [36608/54000 (68%)] Loss: -246488.046875\n",
      "Train Epoch: 190 [38016/54000 (70%)] Loss: -209197.015625\n",
      "Train Epoch: 190 [39424/54000 (73%)] Loss: -217177.984375\n",
      "Train Epoch: 190 [40832/54000 (76%)] Loss: -225799.296875\n",
      "Train Epoch: 190 [42240/54000 (78%)] Loss: -223313.312500\n",
      "Train Epoch: 190 [43648/54000 (81%)] Loss: -223693.031250\n",
      "Train Epoch: 190 [45056/54000 (83%)] Loss: -222952.828125\n",
      "Train Epoch: 190 [46464/54000 (86%)] Loss: -218899.125000\n",
      "Train Epoch: 190 [47872/54000 (89%)] Loss: -218686.921875\n",
      "Train Epoch: 190 [49280/54000 (91%)] Loss: -222301.062500\n",
      "Train Epoch: 190 [50688/54000 (94%)] Loss: -244891.890625\n",
      "Train Epoch: 190 [52096/54000 (96%)] Loss: -227288.000000\n",
      "    epoch          : 190\n",
      "    loss           : -222477.4691611842\n",
      "    val_loss       : -228577.56288109755\n",
      "Train Epoch: 191 [0/54000 (0%)] Loss: -212410.703125\n",
      "Train Epoch: 191 [1408/54000 (3%)] Loss: -212131.671875\n",
      "Train Epoch: 191 [2816/54000 (5%)] Loss: -219232.062500\n",
      "Train Epoch: 191 [4224/54000 (8%)] Loss: -214122.140625\n",
      "Train Epoch: 191 [5632/54000 (10%)] Loss: -220060.375000\n",
      "Train Epoch: 191 [7040/54000 (13%)] Loss: -220756.750000\n",
      "Train Epoch: 191 [8448/54000 (16%)] Loss: -223801.875000\n",
      "Train Epoch: 191 [9856/54000 (18%)] Loss: -244541.734375\n",
      "Train Epoch: 191 [11264/54000 (21%)] Loss: -221788.843750\n",
      "Train Epoch: 191 [12672/54000 (23%)] Loss: -219264.468750\n",
      "Train Epoch: 191 [14080/54000 (26%)] Loss: -245746.140625\n",
      "Train Epoch: 191 [15488/54000 (29%)] Loss: -225545.578125\n",
      "Train Epoch: 191 [16896/54000 (31%)] Loss: -216611.062500\n",
      "Train Epoch: 191 [18304/54000 (34%)] Loss: -242220.515625\n",
      "Train Epoch: 191 [19712/54000 (37%)] Loss: -228947.328125\n",
      "Train Epoch: 191 [21120/54000 (39%)] Loss: -227435.734375\n",
      "Train Epoch: 191 [22528/54000 (42%)] Loss: -215113.734375\n",
      "Train Epoch: 191 [23936/54000 (44%)] Loss: -213861.203125\n",
      "Train Epoch: 191 [25344/54000 (47%)] Loss: -224919.062500\n",
      "Train Epoch: 191 [26752/54000 (50%)] Loss: -245854.968750\n",
      "Train Epoch: 191 [28160/54000 (52%)] Loss: -220918.203125\n",
      "Train Epoch: 191 [29568/54000 (55%)] Loss: -218808.843750\n",
      "Train Epoch: 191 [30976/54000 (57%)] Loss: -222821.828125\n",
      "Train Epoch: 191 [32384/54000 (60%)] Loss: -245380.875000\n",
      "Train Epoch: 191 [33792/54000 (63%)] Loss: -212987.140625\n",
      "Train Epoch: 191 [35200/54000 (65%)] Loss: -227234.656250\n",
      "Train Epoch: 191 [36608/54000 (68%)] Loss: -229033.453125\n",
      "Train Epoch: 191 [38016/54000 (70%)] Loss: -242055.718750\n",
      "Train Epoch: 191 [39424/54000 (73%)] Loss: -216955.906250\n",
      "Train Epoch: 191 [40832/54000 (76%)] Loss: -215820.921875\n",
      "Train Epoch: 191 [42240/54000 (78%)] Loss: -227155.265625\n",
      "Train Epoch: 191 [43648/54000 (81%)] Loss: -212991.875000\n",
      "Train Epoch: 191 [45056/54000 (83%)] Loss: -221304.781250\n",
      "Train Epoch: 191 [46464/54000 (86%)] Loss: -225759.343750\n",
      "Train Epoch: 191 [47872/54000 (89%)] Loss: -215344.343750\n",
      "Train Epoch: 191 [49280/54000 (91%)] Loss: -222349.359375\n",
      "Train Epoch: 191 [50688/54000 (94%)] Loss: -223663.796875\n",
      "Train Epoch: 191 [52096/54000 (96%)] Loss: -245360.843750\n",
      "    epoch          : 191\n",
      "    loss           : -222496.82064892346\n",
      "    val_loss       : -228076.6619664634\n",
      "Train Epoch: 192 [0/54000 (0%)] Loss: -214296.375000\n",
      "Train Epoch: 192 [1408/54000 (3%)] Loss: -225935.359375\n",
      "Train Epoch: 192 [2816/54000 (5%)] Loss: -226122.125000\n",
      "Train Epoch: 192 [4224/54000 (8%)] Loss: -210174.250000\n",
      "Train Epoch: 192 [5632/54000 (10%)] Loss: -224885.109375\n",
      "Train Epoch: 192 [7040/54000 (13%)] Loss: -218778.515625\n",
      "Train Epoch: 192 [8448/54000 (16%)] Loss: -227452.218750\n",
      "Train Epoch: 192 [9856/54000 (18%)] Loss: -245745.375000\n",
      "Train Epoch: 192 [11264/54000 (21%)] Loss: -225160.968750\n",
      "Train Epoch: 192 [12672/54000 (23%)] Loss: -225819.015625\n",
      "Train Epoch: 192 [14080/54000 (26%)] Loss: -226303.453125\n",
      "Train Epoch: 192 [15488/54000 (29%)] Loss: -241351.859375\n",
      "Train Epoch: 192 [16896/54000 (31%)] Loss: -216543.687500\n",
      "Train Epoch: 192 [18304/54000 (34%)] Loss: -216707.156250\n",
      "Train Epoch: 192 [19712/54000 (37%)] Loss: -215516.421875\n",
      "Train Epoch: 192 [21120/54000 (39%)] Loss: -214045.718750\n",
      "Train Epoch: 192 [22528/54000 (42%)] Loss: -225501.734375\n",
      "Train Epoch: 192 [23936/54000 (44%)] Loss: -219266.500000\n",
      "Train Epoch: 192 [25344/54000 (47%)] Loss: -223559.531250\n",
      "Train Epoch: 192 [26752/54000 (50%)] Loss: -218380.140625\n",
      "Train Epoch: 192 [28160/54000 (52%)] Loss: -218505.140625\n",
      "Train Epoch: 192 [29568/54000 (55%)] Loss: -228998.796875\n",
      "Train Epoch: 192 [30976/54000 (57%)] Loss: -213345.953125\n",
      "Train Epoch: 192 [32384/54000 (60%)] Loss: -217265.640625\n",
      "Train Epoch: 192 [33792/54000 (63%)] Loss: -243087.218750\n",
      "Train Epoch: 192 [35200/54000 (65%)] Loss: -224230.156250\n",
      "Train Epoch: 192 [36608/54000 (68%)] Loss: -219263.718750\n",
      "Train Epoch: 192 [38016/54000 (70%)] Loss: -207060.312500\n",
      "Train Epoch: 192 [39424/54000 (73%)] Loss: -211494.078125\n",
      "Train Epoch: 192 [40832/54000 (76%)] Loss: -218851.031250\n",
      "Train Epoch: 192 [42240/54000 (78%)] Loss: -224822.437500\n",
      "Train Epoch: 192 [43648/54000 (81%)] Loss: -245782.265625\n",
      "Train Epoch: 192 [45056/54000 (83%)] Loss: -222091.750000\n",
      "Train Epoch: 192 [46464/54000 (86%)] Loss: -222573.578125\n",
      "Train Epoch: 192 [47872/54000 (89%)] Loss: -222828.421875\n",
      "Train Epoch: 192 [49280/54000 (91%)] Loss: -214862.906250\n",
      "Train Epoch: 192 [50688/54000 (94%)] Loss: -222064.296875\n",
      "Train Epoch: 192 [52096/54000 (96%)] Loss: -223531.812500\n",
      "    epoch          : 192\n",
      "    loss           : -222362.69149970094\n",
      "    val_loss       : -228326.5405928449\n",
      "Train Epoch: 193 [0/54000 (0%)] Loss: -245034.953125\n",
      "Train Epoch: 193 [1408/54000 (3%)] Loss: -213651.953125\n",
      "Train Epoch: 193 [2816/54000 (5%)] Loss: -215120.968750\n",
      "Train Epoch: 193 [4224/54000 (8%)] Loss: -226286.468750\n",
      "Train Epoch: 193 [5632/54000 (10%)] Loss: -228671.562500\n",
      "Train Epoch: 193 [7040/54000 (13%)] Loss: -214723.625000\n",
      "Train Epoch: 193 [8448/54000 (16%)] Loss: -213445.125000\n",
      "Train Epoch: 193 [9856/54000 (18%)] Loss: -225067.718750\n",
      "Train Epoch: 193 [11264/54000 (21%)] Loss: -245698.593750\n",
      "Train Epoch: 193 [12672/54000 (23%)] Loss: -218997.062500\n",
      "Train Epoch: 193 [14080/54000 (26%)] Loss: -220089.812500\n",
      "Train Epoch: 193 [15488/54000 (29%)] Loss: -220042.625000\n",
      "Train Epoch: 193 [16896/54000 (31%)] Loss: -220113.593750\n",
      "Train Epoch: 193 [18304/54000 (34%)] Loss: -221671.343750\n",
      "Train Epoch: 193 [19712/54000 (37%)] Loss: -215990.093750\n",
      "Train Epoch: 193 [21120/54000 (39%)] Loss: -229257.218750\n",
      "Train Epoch: 193 [22528/54000 (42%)] Loss: -245091.250000\n",
      "Train Epoch: 193 [23936/54000 (44%)] Loss: -224335.625000\n",
      "Train Epoch: 193 [25344/54000 (47%)] Loss: -223123.078125\n",
      "Train Epoch: 193 [26752/54000 (50%)] Loss: -243197.265625\n",
      "Train Epoch: 193 [28160/54000 (52%)] Loss: -222264.531250\n",
      "Train Epoch: 193 [29568/54000 (55%)] Loss: -211854.296875\n",
      "Train Epoch: 193 [30976/54000 (57%)] Loss: -217758.984375\n",
      "Train Epoch: 193 [32384/54000 (60%)] Loss: -224120.843750\n",
      "Train Epoch: 193 [33792/54000 (63%)] Loss: -223399.187500\n",
      "Train Epoch: 193 [35200/54000 (65%)] Loss: -212991.562500\n",
      "Train Epoch: 193 [36608/54000 (68%)] Loss: -216136.843750\n",
      "Train Epoch: 193 [38016/54000 (70%)] Loss: -208350.671875\n",
      "Train Epoch: 193 [39424/54000 (73%)] Loss: -217042.078125\n",
      "Train Epoch: 193 [40832/54000 (76%)] Loss: -213701.968750\n",
      "Train Epoch: 193 [42240/54000 (78%)] Loss: -217991.062500\n",
      "Train Epoch: 193 [43648/54000 (81%)] Loss: -217511.093750\n",
      "Train Epoch: 193 [45056/54000 (83%)] Loss: -215581.687500\n",
      "Train Epoch: 193 [46464/54000 (86%)] Loss: -220924.187500\n",
      "Train Epoch: 193 [47872/54000 (89%)] Loss: -221236.687500\n",
      "Train Epoch: 193 [49280/54000 (91%)] Loss: -218471.390625\n",
      "Train Epoch: 193 [50688/54000 (94%)] Loss: -243490.125000\n",
      "Train Epoch: 193 [52096/54000 (96%)] Loss: -213758.421875\n",
      "    epoch          : 193\n",
      "    loss           : -222641.2717180024\n",
      "    val_loss       : -227998.86253096416\n",
      "Train Epoch: 194 [0/54000 (0%)] Loss: -243110.125000\n",
      "Train Epoch: 194 [1408/54000 (3%)] Loss: -226338.625000\n",
      "Train Epoch: 194 [2816/54000 (5%)] Loss: -214894.109375\n",
      "Train Epoch: 194 [4224/54000 (8%)] Loss: -218238.875000\n",
      "Train Epoch: 194 [5632/54000 (10%)] Loss: -215137.531250\n",
      "Train Epoch: 194 [7040/54000 (13%)] Loss: -214015.093750\n",
      "Train Epoch: 194 [8448/54000 (16%)] Loss: -242582.921875\n",
      "Train Epoch: 194 [9856/54000 (18%)] Loss: -229554.218750\n",
      "Train Epoch: 194 [11264/54000 (21%)] Loss: -220963.343750\n",
      "Train Epoch: 194 [12672/54000 (23%)] Loss: -213993.968750\n",
      "Train Epoch: 194 [14080/54000 (26%)] Loss: -222047.234375\n",
      "Train Epoch: 194 [15488/54000 (29%)] Loss: -223131.406250\n",
      "Train Epoch: 194 [16896/54000 (31%)] Loss: -220835.093750\n",
      "Train Epoch: 194 [18304/54000 (34%)] Loss: -221797.000000\n",
      "Train Epoch: 194 [19712/54000 (37%)] Loss: -245656.734375\n",
      "Train Epoch: 194 [21120/54000 (39%)] Loss: -226105.984375\n",
      "Train Epoch: 194 [22528/54000 (42%)] Loss: -224710.484375\n",
      "Train Epoch: 194 [23936/54000 (44%)] Loss: -229141.390625\n",
      "Train Epoch: 194 [25344/54000 (47%)] Loss: -228781.906250\n",
      "Train Epoch: 194 [26752/54000 (50%)] Loss: -227510.000000\n",
      "Train Epoch: 194 [28160/54000 (52%)] Loss: -211496.703125\n",
      "Train Epoch: 194 [29568/54000 (55%)] Loss: -224687.500000\n",
      "Train Epoch: 194 [30976/54000 (57%)] Loss: -221485.656250\n",
      "Train Epoch: 194 [32384/54000 (60%)] Loss: -219423.609375\n",
      "Train Epoch: 194 [33792/54000 (63%)] Loss: -211380.062500\n",
      "Train Epoch: 194 [35200/54000 (65%)] Loss: -215410.000000\n",
      "Train Epoch: 194 [36608/54000 (68%)] Loss: -220964.281250\n",
      "Train Epoch: 194 [38016/54000 (70%)] Loss: -217919.875000\n",
      "Train Epoch: 194 [39424/54000 (73%)] Loss: -225212.281250\n",
      "Train Epoch: 194 [40832/54000 (76%)] Loss: -212700.171875\n",
      "Train Epoch: 194 [42240/54000 (78%)] Loss: -228930.406250\n",
      "Train Epoch: 194 [43648/54000 (81%)] Loss: -216676.156250\n",
      "Train Epoch: 194 [45056/54000 (83%)] Loss: -244882.578125\n",
      "Train Epoch: 194 [46464/54000 (86%)] Loss: -220991.843750\n",
      "Train Epoch: 194 [47872/54000 (89%)] Loss: -220214.312500\n",
      "Train Epoch: 194 [49280/54000 (91%)] Loss: -214232.781250\n",
      "Train Epoch: 194 [50688/54000 (94%)] Loss: -225884.218750\n",
      "Train Epoch: 194 [52096/54000 (96%)] Loss: -226208.328125\n",
      "    epoch          : 194\n",
      "    loss           : -222655.77504485645\n",
      "    val_loss       : -227699.23095703125\n",
      "Train Epoch: 195 [0/54000 (0%)] Loss: -230112.937500\n",
      "Train Epoch: 195 [1408/54000 (3%)] Loss: -228113.265625\n",
      "Train Epoch: 195 [2816/54000 (5%)] Loss: -246861.250000\n",
      "Train Epoch: 195 [4224/54000 (8%)] Loss: -243893.156250\n",
      "Train Epoch: 195 [5632/54000 (10%)] Loss: -214067.250000\n",
      "Train Epoch: 195 [7040/54000 (13%)] Loss: -220230.421875\n",
      "Train Epoch: 195 [8448/54000 (16%)] Loss: -214245.796875\n",
      "Train Epoch: 195 [9856/54000 (18%)] Loss: -243898.671875\n",
      "Train Epoch: 195 [11264/54000 (21%)] Loss: -226558.000000\n",
      "Train Epoch: 195 [12672/54000 (23%)] Loss: -219799.140625\n",
      "Train Epoch: 195 [14080/54000 (26%)] Loss: -217966.687500\n",
      "Train Epoch: 195 [15488/54000 (29%)] Loss: -246235.156250\n",
      "Train Epoch: 195 [16896/54000 (31%)] Loss: -221533.718750\n",
      "Train Epoch: 195 [18304/54000 (34%)] Loss: -225677.796875\n",
      "Train Epoch: 195 [19712/54000 (37%)] Loss: -225943.984375\n",
      "Train Epoch: 195 [21120/54000 (39%)] Loss: -227805.984375\n",
      "Train Epoch: 195 [22528/54000 (42%)] Loss: -242080.265625\n",
      "Train Epoch: 195 [23936/54000 (44%)] Loss: -222903.812500\n",
      "Train Epoch: 195 [25344/54000 (47%)] Loss: -217728.437500\n",
      "Train Epoch: 195 [26752/54000 (50%)] Loss: -211462.250000\n",
      "Train Epoch: 195 [28160/54000 (52%)] Loss: -227714.906250\n",
      "Train Epoch: 195 [29568/54000 (55%)] Loss: -215623.640625\n",
      "Train Epoch: 195 [30976/54000 (57%)] Loss: -214192.656250\n",
      "Train Epoch: 195 [32384/54000 (60%)] Loss: -223139.171875\n",
      "Train Epoch: 195 [33792/54000 (63%)] Loss: -246040.093750\n",
      "Train Epoch: 195 [35200/54000 (65%)] Loss: -211858.125000\n",
      "Train Epoch: 195 [36608/54000 (68%)] Loss: -227038.828125\n",
      "Train Epoch: 195 [38016/54000 (70%)] Loss: -217534.828125\n",
      "Train Epoch: 195 [39424/54000 (73%)] Loss: -218802.031250\n",
      "Train Epoch: 195 [40832/54000 (76%)] Loss: -197270.562500\n",
      "Train Epoch: 195 [42240/54000 (78%)] Loss: -226327.625000\n",
      "Train Epoch: 195 [43648/54000 (81%)] Loss: -218293.781250\n",
      "Train Epoch: 195 [45056/54000 (83%)] Loss: -211201.250000\n",
      "Train Epoch: 195 [46464/54000 (86%)] Loss: -224444.578125\n",
      "Train Epoch: 195 [47872/54000 (89%)] Loss: -213530.421875\n",
      "Train Epoch: 195 [49280/54000 (91%)] Loss: -219518.781250\n",
      "Train Epoch: 195 [50688/54000 (94%)] Loss: -246476.125000\n",
      "Train Epoch: 195 [52096/54000 (96%)] Loss: -226219.484375\n",
      "    epoch          : 195\n",
      "    loss           : -222565.45835825359\n",
      "    val_loss       : -228472.20345846037\n",
      "Train Epoch: 196 [0/54000 (0%)] Loss: -245199.500000\n",
      "Train Epoch: 196 [1408/54000 (3%)] Loss: -226604.781250\n",
      "Train Epoch: 196 [2816/54000 (5%)] Loss: -224693.453125\n",
      "Train Epoch: 196 [4224/54000 (8%)] Loss: -215967.843750\n",
      "Train Epoch: 196 [5632/54000 (10%)] Loss: -222072.062500\n",
      "Train Epoch: 196 [7040/54000 (13%)] Loss: -218237.375000\n",
      "Train Epoch: 196 [8448/54000 (16%)] Loss: -221966.250000\n",
      "Train Epoch: 196 [9856/54000 (18%)] Loss: -216501.859375\n",
      "Train Epoch: 196 [11264/54000 (21%)] Loss: -228936.062500\n",
      "Train Epoch: 196 [12672/54000 (23%)] Loss: -206372.734375\n",
      "Train Epoch: 196 [14080/54000 (26%)] Loss: -228608.453125\n",
      "Train Epoch: 196 [15488/54000 (29%)] Loss: -217366.906250\n",
      "Train Epoch: 196 [16896/54000 (31%)] Loss: -225235.875000\n",
      "Train Epoch: 196 [18304/54000 (34%)] Loss: -216670.625000\n",
      "Train Epoch: 196 [19712/54000 (37%)] Loss: -216885.625000\n",
      "Train Epoch: 196 [21120/54000 (39%)] Loss: -216115.562500\n",
      "Train Epoch: 196 [22528/54000 (42%)] Loss: -220290.218750\n",
      "Train Epoch: 196 [23936/54000 (44%)] Loss: -228482.921875\n",
      "Train Epoch: 196 [25344/54000 (47%)] Loss: -218977.062500\n",
      "Train Epoch: 196 [26752/54000 (50%)] Loss: -222781.187500\n",
      "Train Epoch: 196 [28160/54000 (52%)] Loss: -226880.500000\n",
      "Train Epoch: 196 [29568/54000 (55%)] Loss: -223169.500000\n",
      "Train Epoch: 196 [30976/54000 (57%)] Loss: -242641.546875\n",
      "Train Epoch: 196 [32384/54000 (60%)] Loss: -215676.484375\n",
      "Train Epoch: 196 [33792/54000 (63%)] Loss: -222946.343750\n",
      "Train Epoch: 196 [35200/54000 (65%)] Loss: -219333.906250\n",
      "Train Epoch: 196 [36608/54000 (68%)] Loss: -246496.171875\n",
      "Train Epoch: 196 [38016/54000 (70%)] Loss: -218795.625000\n",
      "Train Epoch: 196 [39424/54000 (73%)] Loss: -228248.828125\n",
      "Train Epoch: 196 [40832/54000 (76%)] Loss: -222164.703125\n",
      "Train Epoch: 196 [42240/54000 (78%)] Loss: -220463.187500\n",
      "Train Epoch: 196 [43648/54000 (81%)] Loss: -218726.531250\n",
      "Train Epoch: 196 [45056/54000 (83%)] Loss: -226497.546875\n",
      "Train Epoch: 196 [46464/54000 (86%)] Loss: -243670.953125\n",
      "Train Epoch: 196 [47872/54000 (89%)] Loss: -218831.328125\n",
      "Train Epoch: 196 [49280/54000 (91%)] Loss: -216801.968750\n",
      "Train Epoch: 196 [50688/54000 (94%)] Loss: -212561.859375\n",
      "Train Epoch: 196 [52096/54000 (96%)] Loss: -244287.500000\n",
      "    epoch          : 196\n",
      "    loss           : -222777.05629485645\n",
      "    val_loss       : -228352.1229516006\n",
      "Train Epoch: 197 [0/54000 (0%)] Loss: -221832.437500\n",
      "Train Epoch: 197 [1408/54000 (3%)] Loss: -246257.734375\n",
      "Train Epoch: 197 [2816/54000 (5%)] Loss: -245803.218750\n",
      "Train Epoch: 197 [4224/54000 (8%)] Loss: -221900.875000\n",
      "Train Epoch: 197 [5632/54000 (10%)] Loss: -220746.890625\n",
      "Train Epoch: 197 [7040/54000 (13%)] Loss: -213719.843750\n",
      "Train Epoch: 197 [8448/54000 (16%)] Loss: -214662.750000\n",
      "Train Epoch: 197 [9856/54000 (18%)] Loss: -242539.843750\n",
      "Train Epoch: 197 [11264/54000 (21%)] Loss: -228367.203125\n",
      "Train Epoch: 197 [12672/54000 (23%)] Loss: -219447.390625\n",
      "Train Epoch: 197 [14080/54000 (26%)] Loss: -219350.421875\n",
      "Train Epoch: 197 [15488/54000 (29%)] Loss: -218153.031250\n",
      "Train Epoch: 197 [16896/54000 (31%)] Loss: -216442.390625\n",
      "Train Epoch: 197 [18304/54000 (34%)] Loss: -217406.218750\n",
      "Train Epoch: 197 [19712/54000 (37%)] Loss: -217446.203125\n",
      "Train Epoch: 197 [21120/54000 (39%)] Loss: -216746.281250\n",
      "Train Epoch: 197 [22528/54000 (42%)] Loss: -246524.031250\n",
      "Train Epoch: 197 [23936/54000 (44%)] Loss: -211573.125000\n",
      "Train Epoch: 197 [25344/54000 (47%)] Loss: -210602.578125\n",
      "Train Epoch: 197 [26752/54000 (50%)] Loss: -218889.500000\n",
      "Train Epoch: 197 [28160/54000 (52%)] Loss: -215814.250000\n",
      "Train Epoch: 197 [29568/54000 (55%)] Loss: -215189.593750\n",
      "Train Epoch: 197 [30976/54000 (57%)] Loss: -230312.406250\n",
      "Train Epoch: 197 [32384/54000 (60%)] Loss: -215733.937500\n",
      "Train Epoch: 197 [33792/54000 (63%)] Loss: -228091.203125\n",
      "Train Epoch: 197 [35200/54000 (65%)] Loss: -227368.687500\n",
      "Train Epoch: 197 [36608/54000 (68%)] Loss: -219740.000000\n",
      "Train Epoch: 197 [38016/54000 (70%)] Loss: -219152.953125\n",
      "Train Epoch: 197 [39424/54000 (73%)] Loss: -246586.015625\n",
      "Train Epoch: 197 [40832/54000 (76%)] Loss: -214525.078125\n",
      "Train Epoch: 197 [42240/54000 (78%)] Loss: -217528.140625\n",
      "Train Epoch: 197 [43648/54000 (81%)] Loss: -218144.156250\n",
      "Train Epoch: 197 [45056/54000 (83%)] Loss: -246356.265625\n",
      "Train Epoch: 197 [46464/54000 (86%)] Loss: -215488.437500\n",
      "Train Epoch: 197 [47872/54000 (89%)] Loss: -212494.937500\n",
      "Train Epoch: 197 [49280/54000 (91%)] Loss: -218025.593750\n",
      "Train Epoch: 197 [50688/54000 (94%)] Loss: -243717.828125\n",
      "Train Epoch: 197 [52096/54000 (96%)] Loss: -213718.500000\n",
      "    epoch          : 197\n",
      "    loss           : -222787.19860197368\n",
      "    val_loss       : -228442.0161430545\n",
      "Train Epoch: 198 [0/54000 (0%)] Loss: -246357.937500\n",
      "Train Epoch: 198 [1408/54000 (3%)] Loss: -216480.609375\n",
      "Train Epoch: 198 [2816/54000 (5%)] Loss: -219962.156250\n",
      "Train Epoch: 198 [4224/54000 (8%)] Loss: -227407.390625\n",
      "Train Epoch: 198 [5632/54000 (10%)] Loss: -222240.171875\n",
      "Train Epoch: 198 [7040/54000 (13%)] Loss: -216941.984375\n",
      "Train Epoch: 198 [8448/54000 (16%)] Loss: -215978.500000\n",
      "Train Epoch: 198 [9856/54000 (18%)] Loss: -212646.156250\n",
      "Train Epoch: 198 [11264/54000 (21%)] Loss: -246418.875000\n",
      "Train Epoch: 198 [12672/54000 (23%)] Loss: -215554.968750\n",
      "Train Epoch: 198 [14080/54000 (26%)] Loss: -217020.031250\n",
      "Train Epoch: 198 [15488/54000 (29%)] Loss: -224871.609375\n",
      "Train Epoch: 198 [16896/54000 (31%)] Loss: -218763.937500\n",
      "Train Epoch: 198 [18304/54000 (34%)] Loss: -217101.796875\n",
      "Train Epoch: 198 [19712/54000 (37%)] Loss: -242453.156250\n",
      "Train Epoch: 198 [21120/54000 (39%)] Loss: -228384.078125\n",
      "Train Epoch: 198 [22528/54000 (42%)] Loss: -216147.500000\n",
      "Train Epoch: 198 [23936/54000 (44%)] Loss: -229783.718750\n",
      "Train Epoch: 198 [25344/54000 (47%)] Loss: -219161.765625\n",
      "Train Epoch: 198 [26752/54000 (50%)] Loss: -224675.203125\n",
      "Train Epoch: 198 [28160/54000 (52%)] Loss: -226992.078125\n",
      "Train Epoch: 198 [29568/54000 (55%)] Loss: -218636.250000\n",
      "Train Epoch: 198 [30976/54000 (57%)] Loss: -223271.078125\n",
      "Train Epoch: 198 [32384/54000 (60%)] Loss: -244191.359375\n",
      "Train Epoch: 198 [33792/54000 (63%)] Loss: -214072.703125\n",
      "Train Epoch: 198 [35200/54000 (65%)] Loss: -216118.312500\n",
      "Train Epoch: 198 [36608/54000 (68%)] Loss: -217024.250000\n",
      "Train Epoch: 198 [38016/54000 (70%)] Loss: -226974.375000\n",
      "Train Epoch: 198 [39424/54000 (73%)] Loss: -229383.453125\n",
      "Train Epoch: 198 [40832/54000 (76%)] Loss: -229407.312500\n",
      "Train Epoch: 198 [42240/54000 (78%)] Loss: -215898.859375\n",
      "Train Epoch: 198 [43648/54000 (81%)] Loss: -222299.453125\n",
      "Train Epoch: 198 [45056/54000 (83%)] Loss: -216682.203125\n",
      "Train Epoch: 198 [46464/54000 (86%)] Loss: -221583.828125\n",
      "Train Epoch: 198 [47872/54000 (89%)] Loss: -221487.156250\n",
      "Train Epoch: 198 [49280/54000 (91%)] Loss: -217190.703125\n",
      "Train Epoch: 198 [50688/54000 (94%)] Loss: -244972.078125\n",
      "Train Epoch: 198 [52096/54000 (96%)] Loss: -215707.171875\n",
      "    epoch          : 198\n",
      "    loss           : -222860.17277212918\n",
      "    val_loss       : -228381.76795922255\n",
      "Train Epoch: 199 [0/54000 (0%)] Loss: -213862.437500\n",
      "Train Epoch: 199 [1408/54000 (3%)] Loss: -212579.312500\n",
      "Train Epoch: 199 [2816/54000 (5%)] Loss: -229908.828125\n",
      "Train Epoch: 199 [4224/54000 (8%)] Loss: -217853.531250\n",
      "Train Epoch: 199 [5632/54000 (10%)] Loss: -220339.625000\n",
      "Train Epoch: 199 [7040/54000 (13%)] Loss: -214265.906250\n",
      "Train Epoch: 199 [8448/54000 (16%)] Loss: -242444.562500\n",
      "Train Epoch: 199 [9856/54000 (18%)] Loss: -215203.718750\n",
      "Train Epoch: 199 [11264/54000 (21%)] Loss: -217230.171875\n",
      "Train Epoch: 199 [12672/54000 (23%)] Loss: -222800.093750\n",
      "Train Epoch: 199 [14080/54000 (26%)] Loss: -225516.421875\n",
      "Train Epoch: 199 [15488/54000 (29%)] Loss: -243737.859375\n",
      "Train Epoch: 199 [16896/54000 (31%)] Loss: -220472.046875\n",
      "Train Epoch: 199 [18304/54000 (34%)] Loss: -211443.718750\n",
      "Train Epoch: 199 [19712/54000 (37%)] Loss: -226848.109375\n",
      "Train Epoch: 199 [21120/54000 (39%)] Loss: -246489.984375\n",
      "Train Epoch: 199 [22528/54000 (42%)] Loss: -243654.625000\n",
      "Train Epoch: 199 [23936/54000 (44%)] Loss: -220689.656250\n",
      "Train Epoch: 199 [25344/54000 (47%)] Loss: -223883.656250\n",
      "Train Epoch: 199 [26752/54000 (50%)] Loss: -223877.093750\n",
      "Train Epoch: 199 [28160/54000 (52%)] Loss: -225958.890625\n",
      "Train Epoch: 199 [29568/54000 (55%)] Loss: -221177.390625\n",
      "Train Epoch: 199 [30976/54000 (57%)] Loss: -223059.109375\n",
      "Train Epoch: 199 [32384/54000 (60%)] Loss: -218612.406250\n",
      "Train Epoch: 199 [33792/54000 (63%)] Loss: -221920.250000\n",
      "Train Epoch: 199 [35200/54000 (65%)] Loss: -214045.125000\n",
      "Train Epoch: 199 [36608/54000 (68%)] Loss: -215884.953125\n",
      "Train Epoch: 199 [38016/54000 (70%)] Loss: -242236.250000\n",
      "Train Epoch: 199 [39424/54000 (73%)] Loss: -227565.093750\n",
      "Train Epoch: 199 [40832/54000 (76%)] Loss: -227355.593750\n",
      "Train Epoch: 199 [42240/54000 (78%)] Loss: -219716.578125\n",
      "Train Epoch: 199 [43648/54000 (81%)] Loss: -215714.953125\n",
      "Train Epoch: 199 [45056/54000 (83%)] Loss: -214172.953125\n",
      "Train Epoch: 199 [46464/54000 (86%)] Loss: -217099.406250\n",
      "Train Epoch: 199 [47872/54000 (89%)] Loss: -221949.703125\n",
      "Train Epoch: 199 [49280/54000 (91%)] Loss: -216783.953125\n",
      "Train Epoch: 199 [50688/54000 (94%)] Loss: -222396.000000\n",
      "Train Epoch: 199 [52096/54000 (96%)] Loss: -216419.718750\n",
      "    epoch          : 199\n",
      "    loss           : -222974.27687649522\n",
      "    val_loss       : -228326.88130001904\n",
      "Train Epoch: 200 [0/54000 (0%)] Loss: -229485.703125\n",
      "Train Epoch: 200 [1408/54000 (3%)] Loss: -244060.656250\n",
      "Train Epoch: 200 [2816/54000 (5%)] Loss: -213093.828125\n",
      "Train Epoch: 200 [4224/54000 (8%)] Loss: -220354.953125\n",
      "Train Epoch: 200 [5632/54000 (10%)] Loss: -220183.968750\n",
      "Train Epoch: 200 [7040/54000 (13%)] Loss: -227738.453125\n",
      "Train Epoch: 200 [8448/54000 (16%)] Loss: -228679.109375\n",
      "Train Epoch: 200 [9856/54000 (18%)] Loss: -224531.453125\n",
      "Train Epoch: 200 [11264/54000 (21%)] Loss: -213624.390625\n",
      "Train Epoch: 200 [12672/54000 (23%)] Loss: -217487.171875\n",
      "Train Epoch: 200 [14080/54000 (26%)] Loss: -211752.031250\n",
      "Train Epoch: 200 [15488/54000 (29%)] Loss: -217389.312500\n",
      "Train Epoch: 200 [16896/54000 (31%)] Loss: -244462.734375\n",
      "Train Epoch: 200 [18304/54000 (34%)] Loss: -217447.156250\n",
      "Train Epoch: 200 [19712/54000 (37%)] Loss: -226695.500000\n",
      "Train Epoch: 200 [21120/54000 (39%)] Loss: -218425.656250\n",
      "Train Epoch: 200 [22528/54000 (42%)] Loss: -212635.843750\n",
      "Train Epoch: 200 [23936/54000 (44%)] Loss: -214137.000000\n",
      "Train Epoch: 200 [25344/54000 (47%)] Loss: -213000.734375\n",
      "Train Epoch: 200 [26752/54000 (50%)] Loss: -215076.906250\n",
      "Train Epoch: 200 [28160/54000 (52%)] Loss: -221905.015625\n",
      "Train Epoch: 200 [29568/54000 (55%)] Loss: -243898.593750\n",
      "Train Epoch: 200 [30976/54000 (57%)] Loss: -223933.015625\n",
      "Train Epoch: 200 [32384/54000 (60%)] Loss: -218480.250000\n",
      "Train Epoch: 200 [33792/54000 (63%)] Loss: -221656.406250\n",
      "Train Epoch: 200 [35200/54000 (65%)] Loss: -226551.468750\n",
      "Train Epoch: 200 [36608/54000 (68%)] Loss: -227702.515625\n",
      "Train Epoch: 200 [38016/54000 (70%)] Loss: -226769.562500\n",
      "Train Epoch: 200 [39424/54000 (73%)] Loss: -227840.937500\n",
      "Train Epoch: 200 [40832/54000 (76%)] Loss: -222439.718750\n",
      "Train Epoch: 200 [42240/54000 (78%)] Loss: -221736.718750\n",
      "Train Epoch: 200 [43648/54000 (81%)] Loss: -210856.234375\n",
      "Train Epoch: 200 [45056/54000 (83%)] Loss: -215951.531250\n",
      "Train Epoch: 200 [46464/54000 (86%)] Loss: -223174.265625\n",
      "Train Epoch: 200 [47872/54000 (89%)] Loss: -225354.609375\n",
      "Train Epoch: 200 [49280/54000 (91%)] Loss: -227167.187500\n",
      "Train Epoch: 200 [50688/54000 (94%)] Loss: -227126.000000\n",
      "Train Epoch: 200 [52096/54000 (96%)] Loss: -226302.343750\n",
      "    epoch          : 200\n",
      "    loss           : -223090.33825508374\n",
      "    val_loss       : -228745.29025342988\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0508_092801/checkpoint-epoch200.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 201 [0/54000 (0%)] Loss: -243877.171875\n",
      "Train Epoch: 201 [1408/54000 (3%)] Loss: -246263.187500\n",
      "Train Epoch: 201 [2816/54000 (5%)] Loss: -215562.875000\n",
      "Train Epoch: 201 [4224/54000 (8%)] Loss: -246543.687500\n",
      "Train Epoch: 201 [5632/54000 (10%)] Loss: -229428.234375\n",
      "Train Epoch: 201 [7040/54000 (13%)] Loss: -219944.015625\n",
      "Train Epoch: 201 [8448/54000 (16%)] Loss: -227150.203125\n",
      "Train Epoch: 201 [9856/54000 (18%)] Loss: -221231.843750\n",
      "Train Epoch: 201 [11264/54000 (21%)] Loss: -246340.171875\n",
      "Train Epoch: 201 [12672/54000 (23%)] Loss: -213944.921875\n",
      "Train Epoch: 201 [14080/54000 (26%)] Loss: -223402.718750\n",
      "Train Epoch: 201 [15488/54000 (29%)] Loss: -246386.593750\n",
      "Train Epoch: 201 [16896/54000 (31%)] Loss: -213804.875000\n",
      "Train Epoch: 201 [18304/54000 (34%)] Loss: -222238.500000\n",
      "Train Epoch: 201 [19712/54000 (37%)] Loss: -223071.750000\n",
      "Train Epoch: 201 [21120/54000 (39%)] Loss: -215388.921875\n",
      "Train Epoch: 201 [22528/54000 (42%)] Loss: -227841.500000\n",
      "Train Epoch: 201 [23936/54000 (44%)] Loss: -216030.500000\n",
      "Train Epoch: 201 [25344/54000 (47%)] Loss: -215767.625000\n",
      "Train Epoch: 201 [26752/54000 (50%)] Loss: -221053.968750\n",
      "Train Epoch: 201 [28160/54000 (52%)] Loss: -216831.062500\n",
      "Train Epoch: 201 [29568/54000 (55%)] Loss: -220312.781250\n",
      "Train Epoch: 201 [30976/54000 (57%)] Loss: -224917.968750\n",
      "Train Epoch: 201 [32384/54000 (60%)] Loss: -227072.375000\n",
      "Train Epoch: 201 [33792/54000 (63%)] Loss: -215005.906250\n",
      "Train Epoch: 201 [35200/54000 (65%)] Loss: -215430.750000\n",
      "Train Epoch: 201 [36608/54000 (68%)] Loss: -218150.000000\n",
      "Train Epoch: 201 [38016/54000 (70%)] Loss: -211393.375000\n",
      "Train Epoch: 201 [39424/54000 (73%)] Loss: -210663.890625\n",
      "Train Epoch: 201 [40832/54000 (76%)] Loss: -228203.437500\n",
      "Train Epoch: 201 [42240/54000 (78%)] Loss: -223498.828125\n",
      "Train Epoch: 201 [43648/54000 (81%)] Loss: -220053.546875\n",
      "Train Epoch: 201 [45056/54000 (83%)] Loss: -225807.796875\n",
      "Train Epoch: 201 [46464/54000 (86%)] Loss: -225834.250000\n",
      "Train Epoch: 201 [47872/54000 (89%)] Loss: -215511.078125\n",
      "Train Epoch: 201 [49280/54000 (91%)] Loss: -214798.312500\n",
      "Train Epoch: 201 [50688/54000 (94%)] Loss: -246154.875000\n",
      "Train Epoch: 201 [52096/54000 (96%)] Loss: -215262.437500\n",
      "    epoch          : 201\n",
      "    loss           : -223014.07539623205\n",
      "    val_loss       : -228633.6180806974\n",
      "Train Epoch: 202 [0/54000 (0%)] Loss: -245305.375000\n",
      "Train Epoch: 202 [1408/54000 (3%)] Loss: -223946.437500\n",
      "Train Epoch: 202 [2816/54000 (5%)] Loss: -216676.046875\n",
      "Train Epoch: 202 [4224/54000 (8%)] Loss: -218738.609375\n",
      "Train Epoch: 202 [5632/54000 (10%)] Loss: -211938.687500\n",
      "Train Epoch: 202 [7040/54000 (13%)] Loss: -219845.359375\n",
      "Train Epoch: 202 [8448/54000 (16%)] Loss: -214547.562500\n",
      "Train Epoch: 202 [9856/54000 (18%)] Loss: -211430.546875\n",
      "Train Epoch: 202 [11264/54000 (21%)] Loss: -210444.937500\n",
      "Train Epoch: 202 [12672/54000 (23%)] Loss: -213782.328125\n",
      "Train Epoch: 202 [14080/54000 (26%)] Loss: -225491.812500\n",
      "Train Epoch: 202 [15488/54000 (29%)] Loss: -224429.968750\n",
      "Train Epoch: 202 [16896/54000 (31%)] Loss: -222622.812500\n",
      "Train Epoch: 202 [18304/54000 (34%)] Loss: -219710.062500\n",
      "Train Epoch: 202 [19712/54000 (37%)] Loss: -218486.093750\n",
      "Train Epoch: 202 [21120/54000 (39%)] Loss: -218228.375000\n",
      "Train Epoch: 202 [22528/54000 (42%)] Loss: -208673.296875\n",
      "Train Epoch: 202 [23936/54000 (44%)] Loss: -216604.718750\n",
      "Train Epoch: 202 [25344/54000 (47%)] Loss: -210869.000000\n",
      "Train Epoch: 202 [26752/54000 (50%)] Loss: -212852.656250\n",
      "Train Epoch: 202 [28160/54000 (52%)] Loss: -245422.312500\n",
      "Train Epoch: 202 [29568/54000 (55%)] Loss: -216538.375000\n",
      "Train Epoch: 202 [30976/54000 (57%)] Loss: -221544.593750\n",
      "Train Epoch: 202 [32384/54000 (60%)] Loss: -224184.671875\n",
      "Train Epoch: 202 [33792/54000 (63%)] Loss: -246484.578125\n",
      "Train Epoch: 202 [35200/54000 (65%)] Loss: -226224.171875\n",
      "Train Epoch: 202 [36608/54000 (68%)] Loss: -225859.406250\n",
      "Train Epoch: 202 [38016/54000 (70%)] Loss: -213039.843750\n",
      "Train Epoch: 202 [39424/54000 (73%)] Loss: -218302.718750\n",
      "Train Epoch: 202 [40832/54000 (76%)] Loss: -208339.343750\n",
      "Train Epoch: 202 [42240/54000 (78%)] Loss: -214859.562500\n",
      "Train Epoch: 202 [43648/54000 (81%)] Loss: -221725.703125\n",
      "Train Epoch: 202 [45056/54000 (83%)] Loss: -219983.062500\n",
      "Train Epoch: 202 [46464/54000 (86%)] Loss: -244187.328125\n",
      "Train Epoch: 202 [47872/54000 (89%)] Loss: -217111.625000\n",
      "Train Epoch: 202 [49280/54000 (91%)] Loss: -225602.718750\n",
      "Train Epoch: 202 [50688/54000 (94%)] Loss: -213565.250000\n",
      "Train Epoch: 202 [52096/54000 (96%)] Loss: -243278.015625\n",
      "    epoch          : 202\n",
      "    loss           : -222967.7276465311\n",
      "    val_loss       : -228779.34946050876\n",
      "Train Epoch: 203 [0/54000 (0%)] Loss: -225138.921875\n",
      "Train Epoch: 203 [1408/54000 (3%)] Loss: -223787.531250\n",
      "Train Epoch: 203 [2816/54000 (5%)] Loss: -221489.031250\n",
      "Train Epoch: 203 [4224/54000 (8%)] Loss: -213489.968750\n",
      "Train Epoch: 203 [5632/54000 (10%)] Loss: -215420.390625\n",
      "Train Epoch: 203 [7040/54000 (13%)] Loss: -228969.828125\n",
      "Train Epoch: 203 [8448/54000 (16%)] Loss: -227339.171875\n",
      "Train Epoch: 203 [9856/54000 (18%)] Loss: -220134.046875\n",
      "Train Epoch: 203 [11264/54000 (21%)] Loss: -216265.406250\n",
      "Train Epoch: 203 [12672/54000 (23%)] Loss: -224646.312500\n",
      "Train Epoch: 203 [14080/54000 (26%)] Loss: -214633.828125\n",
      "Train Epoch: 203 [15488/54000 (29%)] Loss: -213541.562500\n",
      "Train Epoch: 203 [16896/54000 (31%)] Loss: -218405.140625\n",
      "Train Epoch: 203 [18304/54000 (34%)] Loss: -217645.890625\n",
      "Train Epoch: 203 [19712/54000 (37%)] Loss: -210309.359375\n",
      "Train Epoch: 203 [21120/54000 (39%)] Loss: -246916.859375\n",
      "Train Epoch: 203 [22528/54000 (42%)] Loss: -227237.109375\n",
      "Train Epoch: 203 [23936/54000 (44%)] Loss: -216961.375000\n",
      "Train Epoch: 203 [25344/54000 (47%)] Loss: -216917.078125\n",
      "Train Epoch: 203 [26752/54000 (50%)] Loss: -222276.062500\n",
      "Train Epoch: 203 [28160/54000 (52%)] Loss: -246543.093750\n",
      "Train Epoch: 203 [29568/54000 (55%)] Loss: -228435.828125\n",
      "Train Epoch: 203 [30976/54000 (57%)] Loss: -225836.218750\n",
      "Train Epoch: 203 [32384/54000 (60%)] Loss: -222406.484375\n",
      "Train Epoch: 203 [33792/54000 (63%)] Loss: -213645.843750\n",
      "Train Epoch: 203 [35200/54000 (65%)] Loss: -220702.468750\n",
      "Train Epoch: 203 [36608/54000 (68%)] Loss: -224613.265625\n",
      "Train Epoch: 203 [38016/54000 (70%)] Loss: -247238.468750\n",
      "Train Epoch: 203 [39424/54000 (73%)] Loss: -221911.984375\n",
      "Train Epoch: 203 [40832/54000 (76%)] Loss: -217374.906250\n",
      "Train Epoch: 203 [42240/54000 (78%)] Loss: -220177.093750\n",
      "Train Epoch: 203 [43648/54000 (81%)] Loss: -218910.343750\n",
      "Train Epoch: 203 [45056/54000 (83%)] Loss: -227891.281250\n",
      "Train Epoch: 203 [46464/54000 (86%)] Loss: -216389.296875\n",
      "Train Epoch: 203 [47872/54000 (89%)] Loss: -215498.000000\n",
      "Train Epoch: 203 [49280/54000 (91%)] Loss: -217323.187500\n",
      "Train Epoch: 203 [50688/54000 (94%)] Loss: -218452.953125\n",
      "Train Epoch: 203 [52096/54000 (96%)] Loss: -208967.765625\n",
      "    epoch          : 203\n",
      "    loss           : -223010.45189144736\n",
      "    val_loss       : -228577.73743568978\n",
      "Train Epoch: 204 [0/54000 (0%)] Loss: -225649.140625\n",
      "Train Epoch: 204 [1408/54000 (3%)] Loss: -244599.000000\n",
      "Train Epoch: 204 [2816/54000 (5%)] Loss: -224715.671875\n",
      "Train Epoch: 204 [4224/54000 (8%)] Loss: -229569.468750\n",
      "Train Epoch: 204 [5632/54000 (10%)] Loss: -218639.687500\n",
      "Train Epoch: 204 [7040/54000 (13%)] Loss: -222223.062500\n",
      "Train Epoch: 204 [8448/54000 (16%)] Loss: -246139.484375\n",
      "Train Epoch: 204 [9856/54000 (18%)] Loss: -208326.406250\n",
      "Train Epoch: 204 [11264/54000 (21%)] Loss: -225789.921875\n",
      "Train Epoch: 204 [12672/54000 (23%)] Loss: -222720.031250\n",
      "Train Epoch: 204 [14080/54000 (26%)] Loss: -218969.218750\n",
      "Train Epoch: 204 [15488/54000 (29%)] Loss: -243403.875000\n",
      "Train Epoch: 204 [16896/54000 (31%)] Loss: -218962.578125\n",
      "Train Epoch: 204 [18304/54000 (34%)] Loss: -219109.593750\n",
      "Train Epoch: 204 [19712/54000 (37%)] Loss: -243964.109375\n",
      "Train Epoch: 204 [21120/54000 (39%)] Loss: -227319.359375\n",
      "Train Epoch: 204 [22528/54000 (42%)] Loss: -210882.109375\n",
      "Train Epoch: 204 [23936/54000 (44%)] Loss: -223795.406250\n",
      "Train Epoch: 204 [25344/54000 (47%)] Loss: -222942.531250\n",
      "Train Epoch: 204 [26752/54000 (50%)] Loss: -244348.250000\n",
      "Train Epoch: 204 [28160/54000 (52%)] Loss: -222071.218750\n",
      "Train Epoch: 204 [29568/54000 (55%)] Loss: -216843.062500\n",
      "Train Epoch: 204 [30976/54000 (57%)] Loss: -229879.796875\n",
      "Train Epoch: 204 [32384/54000 (60%)] Loss: -216976.750000\n",
      "Train Epoch: 204 [33792/54000 (63%)] Loss: -228558.812500\n",
      "Train Epoch: 204 [35200/54000 (65%)] Loss: -215693.812500\n",
      "Train Epoch: 204 [36608/54000 (68%)] Loss: -220299.156250\n",
      "Train Epoch: 204 [38016/54000 (70%)] Loss: -219842.859375\n",
      "Train Epoch: 204 [39424/54000 (73%)] Loss: -244492.687500\n",
      "Train Epoch: 204 [40832/54000 (76%)] Loss: -228135.328125\n",
      "Train Epoch: 204 [42240/54000 (78%)] Loss: -226847.156250\n",
      "Train Epoch: 204 [43648/54000 (81%)] Loss: -222525.281250\n",
      "Train Epoch: 204 [45056/54000 (83%)] Loss: -218845.781250\n",
      "Train Epoch: 204 [46464/54000 (86%)] Loss: -244708.234375\n",
      "Train Epoch: 204 [47872/54000 (89%)] Loss: -224851.093750\n",
      "Train Epoch: 204 [49280/54000 (91%)] Loss: -227153.968750\n",
      "Train Epoch: 204 [50688/54000 (94%)] Loss: -245093.156250\n",
      "Train Epoch: 204 [52096/54000 (96%)] Loss: -244507.171875\n",
      "    epoch          : 204\n",
      "    loss           : -223123.02956788277\n",
      "    val_loss       : -228753.4991663491\n",
      "Train Epoch: 205 [0/54000 (0%)] Loss: -215743.109375\n",
      "Train Epoch: 205 [1408/54000 (3%)] Loss: -215084.703125\n",
      "Train Epoch: 205 [2816/54000 (5%)] Loss: -217111.750000\n",
      "Train Epoch: 205 [4224/54000 (8%)] Loss: -216151.843750\n",
      "Train Epoch: 205 [5632/54000 (10%)] Loss: -225384.156250\n",
      "Train Epoch: 205 [7040/54000 (13%)] Loss: -244735.796875\n",
      "Train Epoch: 205 [8448/54000 (16%)] Loss: -220098.250000\n",
      "Train Epoch: 205 [9856/54000 (18%)] Loss: -221499.218750\n",
      "Train Epoch: 205 [11264/54000 (21%)] Loss: -222632.484375\n",
      "Train Epoch: 205 [12672/54000 (23%)] Loss: -211984.515625\n",
      "Train Epoch: 205 [14080/54000 (26%)] Loss: -221816.093750\n",
      "Train Epoch: 205 [15488/54000 (29%)] Loss: -222604.812500\n",
      "Train Epoch: 205 [16896/54000 (31%)] Loss: -216700.031250\n",
      "Train Epoch: 205 [18304/54000 (34%)] Loss: -228247.156250\n",
      "Train Epoch: 205 [19712/54000 (37%)] Loss: -213022.140625\n",
      "Train Epoch: 205 [21120/54000 (39%)] Loss: -246243.796875\n",
      "Train Epoch: 205 [22528/54000 (42%)] Loss: -227360.531250\n",
      "Train Epoch: 205 [23936/54000 (44%)] Loss: -227613.250000\n",
      "Train Epoch: 205 [25344/54000 (47%)] Loss: -228464.250000\n",
      "Train Epoch: 205 [26752/54000 (50%)] Loss: -217041.203125\n",
      "Train Epoch: 205 [28160/54000 (52%)] Loss: -226657.843750\n",
      "Train Epoch: 205 [29568/54000 (55%)] Loss: -204462.859375\n",
      "Train Epoch: 205 [30976/54000 (57%)] Loss: -215148.500000\n",
      "Train Epoch: 205 [32384/54000 (60%)] Loss: -218563.984375\n",
      "Train Epoch: 205 [33792/54000 (63%)] Loss: -226364.343750\n",
      "Train Epoch: 205 [35200/54000 (65%)] Loss: -214226.000000\n",
      "Train Epoch: 205 [36608/54000 (68%)] Loss: -220062.328125\n",
      "Train Epoch: 205 [38016/54000 (70%)] Loss: -246246.500000\n",
      "Train Epoch: 205 [39424/54000 (73%)] Loss: -228210.093750\n",
      "Train Epoch: 205 [40832/54000 (76%)] Loss: -224051.203125\n",
      "Train Epoch: 205 [42240/54000 (78%)] Loss: -212544.187500\n",
      "Train Epoch: 205 [43648/54000 (81%)] Loss: -245400.968750\n",
      "Train Epoch: 205 [45056/54000 (83%)] Loss: -227801.953125\n",
      "Train Epoch: 205 [46464/54000 (86%)] Loss: -220011.890625\n",
      "Train Epoch: 205 [47872/54000 (89%)] Loss: -214818.390625\n",
      "Train Epoch: 205 [49280/54000 (91%)] Loss: -217453.890625\n",
      "Train Epoch: 205 [50688/54000 (94%)] Loss: -245278.093750\n",
      "Train Epoch: 205 [52096/54000 (96%)] Loss: -226392.671875\n",
      "    epoch          : 205\n",
      "    loss           : -223006.30696770336\n",
      "    val_loss       : -228681.4208627096\n",
      "Train Epoch: 206 [0/54000 (0%)] Loss: -228952.109375\n",
      "Train Epoch: 206 [1408/54000 (3%)] Loss: -225565.031250\n",
      "Train Epoch: 206 [2816/54000 (5%)] Loss: -223044.781250\n",
      "Train Epoch: 206 [4224/54000 (8%)] Loss: -223013.546875\n",
      "Train Epoch: 206 [5632/54000 (10%)] Loss: -214304.296875\n",
      "Train Epoch: 206 [7040/54000 (13%)] Loss: -246475.015625\n",
      "Train Epoch: 206 [8448/54000 (16%)] Loss: -208484.781250\n",
      "Train Epoch: 206 [9856/54000 (18%)] Loss: -226964.187500\n",
      "Train Epoch: 206 [11264/54000 (21%)] Loss: -221668.343750\n",
      "Train Epoch: 206 [12672/54000 (23%)] Loss: -216746.562500\n",
      "Train Epoch: 206 [14080/54000 (26%)] Loss: -220094.656250\n",
      "Train Epoch: 206 [15488/54000 (29%)] Loss: -223089.812500\n",
      "Train Epoch: 206 [16896/54000 (31%)] Loss: -242275.046875\n",
      "Train Epoch: 206 [18304/54000 (34%)] Loss: -218639.921875\n",
      "Train Epoch: 206 [19712/54000 (37%)] Loss: -218325.671875\n",
      "Train Epoch: 206 [21120/54000 (39%)] Loss: -227939.937500\n",
      "Train Epoch: 206 [22528/54000 (42%)] Loss: -213336.984375\n",
      "Train Epoch: 206 [23936/54000 (44%)] Loss: -217814.468750\n",
      "Train Epoch: 206 [25344/54000 (47%)] Loss: -217294.984375\n",
      "Train Epoch: 206 [26752/54000 (50%)] Loss: -247311.531250\n",
      "Train Epoch: 206 [28160/54000 (52%)] Loss: -230377.375000\n",
      "Train Epoch: 206 [29568/54000 (55%)] Loss: -217296.718750\n",
      "Train Epoch: 206 [30976/54000 (57%)] Loss: -210404.578125\n",
      "Train Epoch: 206 [32384/54000 (60%)] Loss: -216732.890625\n",
      "Train Epoch: 206 [33792/54000 (63%)] Loss: -245798.890625\n",
      "Train Epoch: 206 [35200/54000 (65%)] Loss: -221190.578125\n",
      "Train Epoch: 206 [36608/54000 (68%)] Loss: -228112.437500\n",
      "Train Epoch: 206 [38016/54000 (70%)] Loss: -225950.890625\n",
      "Train Epoch: 206 [39424/54000 (73%)] Loss: -228381.937500\n",
      "Train Epoch: 206 [40832/54000 (76%)] Loss: -229630.265625\n",
      "Train Epoch: 206 [42240/54000 (78%)] Loss: -227471.578125\n",
      "Train Epoch: 206 [43648/54000 (81%)] Loss: -221804.046875\n",
      "Train Epoch: 206 [45056/54000 (83%)] Loss: -216946.562500\n",
      "Train Epoch: 206 [46464/54000 (86%)] Loss: -245781.750000\n",
      "Train Epoch: 206 [47872/54000 (89%)] Loss: -215732.625000\n",
      "Train Epoch: 206 [49280/54000 (91%)] Loss: -216407.171875\n",
      "Train Epoch: 206 [50688/54000 (94%)] Loss: -226639.828125\n",
      "Train Epoch: 206 [52096/54000 (96%)] Loss: -212281.125000\n",
      "    epoch          : 206\n",
      "    loss           : -223202.16589413874\n",
      "    val_loss       : -228532.8611494855\n",
      "Train Epoch: 207 [0/54000 (0%)] Loss: -226636.406250\n",
      "Train Epoch: 207 [1408/54000 (3%)] Loss: -227867.328125\n",
      "Train Epoch: 207 [2816/54000 (5%)] Loss: -224150.187500\n",
      "Train Epoch: 207 [4224/54000 (8%)] Loss: -226497.312500\n",
      "Train Epoch: 207 [5632/54000 (10%)] Loss: -212323.937500\n",
      "Train Epoch: 207 [7040/54000 (13%)] Loss: -225232.953125\n",
      "Train Epoch: 207 [8448/54000 (16%)] Loss: -215899.578125\n",
      "Train Epoch: 207 [9856/54000 (18%)] Loss: -203244.437500\n",
      "Train Epoch: 207 [11264/54000 (21%)] Loss: -219499.703125\n",
      "Train Epoch: 207 [12672/54000 (23%)] Loss: -244923.156250\n",
      "Train Epoch: 207 [14080/54000 (26%)] Loss: -214114.953125\n",
      "Train Epoch: 207 [15488/54000 (29%)] Loss: -227685.546875\n",
      "Train Epoch: 207 [16896/54000 (31%)] Loss: -225841.031250\n",
      "Train Epoch: 207 [18304/54000 (34%)] Loss: -245297.453125\n",
      "Train Epoch: 207 [19712/54000 (37%)] Loss: -218574.281250\n",
      "Train Epoch: 207 [21120/54000 (39%)] Loss: -218999.218750\n",
      "Train Epoch: 207 [22528/54000 (42%)] Loss: -230562.687500\n",
      "Train Epoch: 207 [23936/54000 (44%)] Loss: -230440.515625\n",
      "Train Epoch: 207 [25344/54000 (47%)] Loss: -227198.937500\n",
      "Train Epoch: 207 [26752/54000 (50%)] Loss: -221784.593750\n",
      "Train Epoch: 207 [28160/54000 (52%)] Loss: -219949.546875\n",
      "Train Epoch: 207 [29568/54000 (55%)] Loss: -219535.000000\n",
      "Train Epoch: 207 [30976/54000 (57%)] Loss: -223910.390625\n",
      "Train Epoch: 207 [32384/54000 (60%)] Loss: -223560.265625\n",
      "Train Epoch: 207 [33792/54000 (63%)] Loss: -213242.234375\n",
      "Train Epoch: 207 [35200/54000 (65%)] Loss: -217419.234375\n",
      "Train Epoch: 207 [36608/54000 (68%)] Loss: -227608.718750\n",
      "Train Epoch: 207 [38016/54000 (70%)] Loss: -245490.671875\n",
      "Train Epoch: 207 [39424/54000 (73%)] Loss: -227680.656250\n",
      "Train Epoch: 207 [40832/54000 (76%)] Loss: -216173.250000\n",
      "Train Epoch: 207 [42240/54000 (78%)] Loss: -246976.453125\n",
      "Train Epoch: 207 [43648/54000 (81%)] Loss: -219039.906250\n",
      "Train Epoch: 207 [45056/54000 (83%)] Loss: -227384.734375\n",
      "Train Epoch: 207 [46464/54000 (86%)] Loss: -215720.562500\n",
      "Train Epoch: 207 [47872/54000 (89%)] Loss: -222223.921875\n",
      "Train Epoch: 207 [49280/54000 (91%)] Loss: -219142.656250\n",
      "Train Epoch: 207 [50688/54000 (94%)] Loss: -244277.687500\n",
      "Train Epoch: 207 [52096/54000 (96%)] Loss: -226325.828125\n",
      "    epoch          : 207\n",
      "    loss           : -223185.90946471292\n",
      "    val_loss       : -228654.37433307926\n",
      "Train Epoch: 208 [0/54000 (0%)] Loss: -246166.000000\n",
      "Train Epoch: 208 [1408/54000 (3%)] Loss: -214662.562500\n",
      "Train Epoch: 208 [2816/54000 (5%)] Loss: -212202.187500\n",
      "Train Epoch: 208 [4224/54000 (8%)] Loss: -228512.187500\n",
      "Train Epoch: 208 [5632/54000 (10%)] Loss: -230381.906250\n",
      "Train Epoch: 208 [7040/54000 (13%)] Loss: -221568.906250\n",
      "Train Epoch: 208 [8448/54000 (16%)] Loss: -244171.953125\n",
      "Train Epoch: 208 [9856/54000 (18%)] Loss: -216748.265625\n",
      "Train Epoch: 208 [11264/54000 (21%)] Loss: -218642.343750\n",
      "Train Epoch: 208 [12672/54000 (23%)] Loss: -221658.687500\n",
      "Train Epoch: 208 [14080/54000 (26%)] Loss: -226520.625000\n",
      "Train Epoch: 208 [15488/54000 (29%)] Loss: -222434.093750\n",
      "Train Epoch: 208 [16896/54000 (31%)] Loss: -219879.437500\n",
      "Train Epoch: 208 [18304/54000 (34%)] Loss: -228278.968750\n",
      "Train Epoch: 208 [19712/54000 (37%)] Loss: -220682.515625\n",
      "Train Epoch: 208 [21120/54000 (39%)] Loss: -221641.734375\n",
      "Train Epoch: 208 [22528/54000 (42%)] Loss: -220971.281250\n",
      "Train Epoch: 208 [23936/54000 (44%)] Loss: -215069.234375\n",
      "Train Epoch: 208 [25344/54000 (47%)] Loss: -223637.359375\n",
      "Train Epoch: 208 [26752/54000 (50%)] Loss: -221877.250000\n",
      "Train Epoch: 208 [28160/54000 (52%)] Loss: -242132.531250\n",
      "Train Epoch: 208 [29568/54000 (55%)] Loss: -212689.078125\n",
      "Train Epoch: 208 [30976/54000 (57%)] Loss: -222087.640625\n",
      "Train Epoch: 208 [32384/54000 (60%)] Loss: -244616.093750\n",
      "Train Epoch: 208 [33792/54000 (63%)] Loss: -223833.859375\n",
      "Train Epoch: 208 [35200/54000 (65%)] Loss: -220203.703125\n",
      "Train Epoch: 208 [36608/54000 (68%)] Loss: -214243.312500\n",
      "Train Epoch: 208 [38016/54000 (70%)] Loss: -246654.406250\n",
      "Train Epoch: 208 [39424/54000 (73%)] Loss: -221758.812500\n",
      "Train Epoch: 208 [40832/54000 (76%)] Loss: -222864.125000\n",
      "Train Epoch: 208 [42240/54000 (78%)] Loss: -222946.765625\n",
      "Train Epoch: 208 [43648/54000 (81%)] Loss: -228666.750000\n",
      "Train Epoch: 208 [45056/54000 (83%)] Loss: -216707.875000\n",
      "Train Epoch: 208 [46464/54000 (86%)] Loss: -219895.953125\n",
      "Train Epoch: 208 [47872/54000 (89%)] Loss: -217799.390625\n",
      "Train Epoch: 208 [49280/54000 (91%)] Loss: -213253.968750\n",
      "Train Epoch: 208 [50688/54000 (94%)] Loss: -215513.875000\n",
      "Train Epoch: 208 [52096/54000 (96%)] Loss: -228696.562500\n",
      "    epoch          : 208\n",
      "    loss           : -223255.2970992823\n",
      "    val_loss       : -228474.1250357279\n",
      "Train Epoch: 209 [0/54000 (0%)] Loss: -222385.125000\n",
      "Train Epoch: 209 [1408/54000 (3%)] Loss: -244427.296875\n",
      "Train Epoch: 209 [2816/54000 (5%)] Loss: -219589.796875\n",
      "Train Epoch: 209 [4224/54000 (8%)] Loss: -217219.187500\n",
      "Train Epoch: 209 [5632/54000 (10%)] Loss: -217207.515625\n",
      "Train Epoch: 209 [7040/54000 (13%)] Loss: -225478.796875\n",
      "Train Epoch: 209 [8448/54000 (16%)] Loss: -229811.828125\n",
      "Train Epoch: 209 [9856/54000 (18%)] Loss: -220179.328125\n",
      "Train Epoch: 209 [11264/54000 (21%)] Loss: -230770.156250\n",
      "Train Epoch: 209 [12672/54000 (23%)] Loss: -227796.656250\n",
      "Train Epoch: 209 [14080/54000 (26%)] Loss: -242860.671875\n",
      "Train Epoch: 209 [15488/54000 (29%)] Loss: -246415.093750\n",
      "Train Epoch: 209 [16896/54000 (31%)] Loss: -221095.421875\n",
      "Train Epoch: 209 [18304/54000 (34%)] Loss: -213209.328125\n",
      "Train Epoch: 209 [19712/54000 (37%)] Loss: -214899.421875\n",
      "Train Epoch: 209 [21120/54000 (39%)] Loss: -227367.796875\n",
      "Train Epoch: 209 [22528/54000 (42%)] Loss: -228477.390625\n",
      "Train Epoch: 209 [23936/54000 (44%)] Loss: -217032.140625\n",
      "Train Epoch: 209 [25344/54000 (47%)] Loss: -216741.968750\n",
      "Train Epoch: 209 [26752/54000 (50%)] Loss: -227770.093750\n",
      "Train Epoch: 209 [28160/54000 (52%)] Loss: -216750.203125\n",
      "Train Epoch: 209 [29568/54000 (55%)] Loss: -226875.375000\n",
      "Train Epoch: 209 [30976/54000 (57%)] Loss: -219259.296875\n",
      "Train Epoch: 209 [32384/54000 (60%)] Loss: -226838.171875\n",
      "Train Epoch: 209 [33792/54000 (63%)] Loss: -245805.718750\n",
      "Train Epoch: 209 [35200/54000 (65%)] Loss: -219398.218750\n",
      "Train Epoch: 209 [36608/54000 (68%)] Loss: -223302.343750\n",
      "Train Epoch: 209 [38016/54000 (70%)] Loss: -221298.328125\n",
      "Train Epoch: 209 [39424/54000 (73%)] Loss: -225102.093750\n",
      "Train Epoch: 209 [40832/54000 (76%)] Loss: -215093.593750\n",
      "Train Epoch: 209 [42240/54000 (78%)] Loss: -229253.593750\n",
      "Train Epoch: 209 [43648/54000 (81%)] Loss: -228565.406250\n",
      "Train Epoch: 209 [45056/54000 (83%)] Loss: -211651.062500\n",
      "Train Epoch: 209 [46464/54000 (86%)] Loss: -221352.468750\n",
      "Train Epoch: 209 [47872/54000 (89%)] Loss: -216284.187500\n",
      "Train Epoch: 209 [49280/54000 (91%)] Loss: -217912.812500\n",
      "Train Epoch: 209 [50688/54000 (94%)] Loss: -212540.984375\n",
      "Train Epoch: 209 [52096/54000 (96%)] Loss: -244932.765625\n",
      "    epoch          : 209\n",
      "    loss           : -223252.8046875\n",
      "    val_loss       : -228688.21241425304\n",
      "Train Epoch: 210 [0/54000 (0%)] Loss: -246112.328125\n",
      "Train Epoch: 210 [1408/54000 (3%)] Loss: -246153.140625\n",
      "Train Epoch: 210 [2816/54000 (5%)] Loss: -244000.531250\n",
      "Train Epoch: 210 [4224/54000 (8%)] Loss: -224524.500000\n",
      "Train Epoch: 210 [5632/54000 (10%)] Loss: -218823.562500\n",
      "Train Epoch: 210 [7040/54000 (13%)] Loss: -219151.828125\n",
      "Train Epoch: 210 [8448/54000 (16%)] Loss: -222268.125000\n",
      "Train Epoch: 210 [9856/54000 (18%)] Loss: -221778.218750\n",
      "Train Epoch: 210 [11264/54000 (21%)] Loss: -221516.703125\n",
      "Train Epoch: 210 [12672/54000 (23%)] Loss: -224181.062500\n",
      "Train Epoch: 210 [14080/54000 (26%)] Loss: -213932.812500\n",
      "Train Epoch: 210 [15488/54000 (29%)] Loss: -212196.593750\n",
      "Train Epoch: 210 [16896/54000 (31%)] Loss: -212865.500000\n",
      "Train Epoch: 210 [18304/54000 (34%)] Loss: -220673.906250\n",
      "Train Epoch: 210 [19712/54000 (37%)] Loss: -216196.171875\n",
      "Train Epoch: 210 [21120/54000 (39%)] Loss: -221529.718750\n",
      "Train Epoch: 210 [22528/54000 (42%)] Loss: -228345.656250\n",
      "Train Epoch: 210 [23936/54000 (44%)] Loss: -224407.234375\n",
      "Train Epoch: 210 [25344/54000 (47%)] Loss: -216322.453125\n",
      "Train Epoch: 210 [26752/54000 (50%)] Loss: -215904.640625\n",
      "Train Epoch: 210 [28160/54000 (52%)] Loss: -221753.203125\n",
      "Train Epoch: 210 [29568/54000 (55%)] Loss: -220819.312500\n",
      "Train Epoch: 210 [30976/54000 (57%)] Loss: -216628.796875\n",
      "Train Epoch: 210 [32384/54000 (60%)] Loss: -228136.718750\n",
      "Train Epoch: 210 [33792/54000 (63%)] Loss: -247667.859375\n",
      "Train Epoch: 210 [35200/54000 (65%)] Loss: -216757.093750\n",
      "Train Epoch: 210 [36608/54000 (68%)] Loss: -215264.546875\n",
      "Train Epoch: 210 [38016/54000 (70%)] Loss: -228967.734375\n",
      "Train Epoch: 210 [39424/54000 (73%)] Loss: -228212.406250\n",
      "Train Epoch: 210 [40832/54000 (76%)] Loss: -230276.562500\n",
      "Train Epoch: 210 [42240/54000 (78%)] Loss: -227673.078125\n",
      "Train Epoch: 210 [43648/54000 (81%)] Loss: -213935.375000\n",
      "Train Epoch: 210 [45056/54000 (83%)] Loss: -229510.656250\n",
      "Train Epoch: 210 [46464/54000 (86%)] Loss: -223645.062500\n",
      "Train Epoch: 210 [47872/54000 (89%)] Loss: -222913.312500\n",
      "Train Epoch: 210 [49280/54000 (91%)] Loss: -220561.609375\n",
      "Train Epoch: 210 [50688/54000 (94%)] Loss: -224839.437500\n",
      "Train Epoch: 210 [52096/54000 (96%)] Loss: -243411.906250\n",
      "    epoch          : 210\n",
      "    loss           : -223458.45331190192\n",
      "    val_loss       : -228403.12226086127\n",
      "Train Epoch: 211 [0/54000 (0%)] Loss: -224924.312500\n",
      "Train Epoch: 211 [1408/54000 (3%)] Loss: -228247.984375\n",
      "Train Epoch: 211 [2816/54000 (5%)] Loss: -222924.250000\n",
      "Train Epoch: 211 [4224/54000 (8%)] Loss: -230134.812500\n",
      "Train Epoch: 211 [5632/54000 (10%)] Loss: -221736.828125\n",
      "Train Epoch: 211 [7040/54000 (13%)] Loss: -219065.218750\n",
      "Train Epoch: 211 [8448/54000 (16%)] Loss: -216379.125000\n",
      "Train Epoch: 211 [9856/54000 (18%)] Loss: -217362.906250\n",
      "Train Epoch: 211 [11264/54000 (21%)] Loss: -214001.609375\n",
      "Train Epoch: 211 [12672/54000 (23%)] Loss: -222125.187500\n",
      "Train Epoch: 211 [14080/54000 (26%)] Loss: -225603.093750\n",
      "Train Epoch: 211 [15488/54000 (29%)] Loss: -244729.750000\n",
      "Train Epoch: 211 [16896/54000 (31%)] Loss: -217346.234375\n",
      "Train Epoch: 211 [18304/54000 (34%)] Loss: -227282.265625\n",
      "Train Epoch: 211 [19712/54000 (37%)] Loss: -244283.078125\n",
      "Train Epoch: 211 [21120/54000 (39%)] Loss: -225172.765625\n",
      "Train Epoch: 211 [22528/54000 (42%)] Loss: -220605.765625\n",
      "Train Epoch: 211 [23936/54000 (44%)] Loss: -218444.343750\n",
      "Train Epoch: 211 [25344/54000 (47%)] Loss: -224136.015625\n",
      "Train Epoch: 211 [26752/54000 (50%)] Loss: -215590.140625\n",
      "Train Epoch: 211 [28160/54000 (52%)] Loss: -221347.218750\n",
      "Train Epoch: 211 [29568/54000 (55%)] Loss: -218803.546875\n",
      "Train Epoch: 211 [30976/54000 (57%)] Loss: -214167.578125\n",
      "Train Epoch: 211 [32384/54000 (60%)] Loss: -247321.187500\n",
      "Train Epoch: 211 [33792/54000 (63%)] Loss: -227837.578125\n",
      "Train Epoch: 211 [35200/54000 (65%)] Loss: -220682.156250\n",
      "Train Epoch: 211 [36608/54000 (68%)] Loss: -213305.953125\n",
      "Train Epoch: 211 [38016/54000 (70%)] Loss: -228623.437500\n",
      "Train Epoch: 211 [39424/54000 (73%)] Loss: -245752.265625\n",
      "Train Epoch: 211 [40832/54000 (76%)] Loss: -223869.687500\n",
      "Train Epoch: 211 [42240/54000 (78%)] Loss: -215849.937500\n",
      "Train Epoch: 211 [43648/54000 (81%)] Loss: -222316.562500\n",
      "Train Epoch: 211 [45056/54000 (83%)] Loss: -245787.750000\n",
      "Train Epoch: 211 [46464/54000 (86%)] Loss: -220622.046875\n",
      "Train Epoch: 211 [47872/54000 (89%)] Loss: -220768.796875\n",
      "Train Epoch: 211 [49280/54000 (91%)] Loss: -215520.000000\n",
      "Train Epoch: 211 [50688/54000 (94%)] Loss: -215825.265625\n",
      "Train Epoch: 211 [52096/54000 (96%)] Loss: -226264.937500\n",
      "    epoch          : 211\n",
      "    loss           : -223433.34229216506\n",
      "    val_loss       : -228613.8105706936\n",
      "Train Epoch: 212 [0/54000 (0%)] Loss: -217369.343750\n",
      "Train Epoch: 212 [1408/54000 (3%)] Loss: -228208.484375\n",
      "Train Epoch: 212 [2816/54000 (5%)] Loss: -226520.093750\n",
      "Train Epoch: 212 [4224/54000 (8%)] Loss: -217554.296875\n",
      "Train Epoch: 212 [5632/54000 (10%)] Loss: -214612.843750\n",
      "Train Epoch: 212 [7040/54000 (13%)] Loss: -218681.390625\n",
      "Train Epoch: 212 [8448/54000 (16%)] Loss: -219962.156250\n",
      "Train Epoch: 212 [9856/54000 (18%)] Loss: -214550.656250\n",
      "Train Epoch: 212 [11264/54000 (21%)] Loss: -228168.765625\n",
      "Train Epoch: 212 [12672/54000 (23%)] Loss: -224212.718750\n",
      "Train Epoch: 212 [14080/54000 (26%)] Loss: -230282.453125\n",
      "Train Epoch: 212 [15488/54000 (29%)] Loss: -248021.187500\n",
      "Train Epoch: 212 [16896/54000 (31%)] Loss: -227872.812500\n",
      "Train Epoch: 212 [18304/54000 (34%)] Loss: -212729.187500\n",
      "Train Epoch: 212 [19712/54000 (37%)] Loss: -220318.843750\n",
      "Train Epoch: 212 [21120/54000 (39%)] Loss: -217327.687500\n",
      "Train Epoch: 212 [22528/54000 (42%)] Loss: -212961.265625\n",
      "Train Epoch: 212 [23936/54000 (44%)] Loss: -218568.937500\n",
      "Train Epoch: 212 [25344/54000 (47%)] Loss: -229190.437500\n",
      "Train Epoch: 212 [26752/54000 (50%)] Loss: -215936.718750\n",
      "Train Epoch: 212 [28160/54000 (52%)] Loss: -244943.562500\n",
      "Train Epoch: 212 [29568/54000 (55%)] Loss: -243916.375000\n",
      "Train Epoch: 212 [30976/54000 (57%)] Loss: -226956.421875\n",
      "Train Epoch: 212 [32384/54000 (60%)] Loss: -226747.218750\n",
      "Train Epoch: 212 [33792/54000 (63%)] Loss: -221892.671875\n",
      "Train Epoch: 212 [35200/54000 (65%)] Loss: -223223.625000\n",
      "Train Epoch: 212 [36608/54000 (68%)] Loss: -219965.375000\n",
      "Train Epoch: 212 [38016/54000 (70%)] Loss: -244222.937500\n",
      "Train Epoch: 212 [39424/54000 (73%)] Loss: -220310.156250\n",
      "Train Epoch: 212 [40832/54000 (76%)] Loss: -229626.609375\n",
      "Train Epoch: 212 [42240/54000 (78%)] Loss: -229429.843750\n",
      "Train Epoch: 212 [43648/54000 (81%)] Loss: -224193.796875\n",
      "Train Epoch: 212 [45056/54000 (83%)] Loss: -222837.703125\n",
      "Train Epoch: 212 [46464/54000 (86%)] Loss: -245354.671875\n",
      "Train Epoch: 212 [47872/54000 (89%)] Loss: -216814.921875\n",
      "Train Epoch: 212 [49280/54000 (91%)] Loss: -213371.234375\n",
      "Train Epoch: 212 [50688/54000 (94%)] Loss: -243498.593750\n",
      "Train Epoch: 212 [52096/54000 (96%)] Loss: -211462.500000\n",
      "    epoch          : 212\n",
      "    loss           : -223528.34087171053\n",
      "    val_loss       : -228432.9683748571\n",
      "Train Epoch: 213 [0/54000 (0%)] Loss: -245570.750000\n",
      "Train Epoch: 213 [1408/54000 (3%)] Loss: -222598.796875\n",
      "Train Epoch: 213 [2816/54000 (5%)] Loss: -218090.437500\n",
      "Train Epoch: 213 [4224/54000 (8%)] Loss: -226080.265625\n",
      "Train Epoch: 213 [5632/54000 (10%)] Loss: -227270.578125\n",
      "Train Epoch: 213 [7040/54000 (13%)] Loss: -230227.171875\n",
      "Train Epoch: 213 [8448/54000 (16%)] Loss: -219596.703125\n",
      "Train Epoch: 213 [9856/54000 (18%)] Loss: -225715.625000\n",
      "Train Epoch: 213 [11264/54000 (21%)] Loss: -245830.625000\n",
      "Train Epoch: 213 [12672/54000 (23%)] Loss: -219230.156250\n",
      "Train Epoch: 213 [14080/54000 (26%)] Loss: -218507.343750\n",
      "Train Epoch: 213 [15488/54000 (29%)] Loss: -225257.125000\n",
      "Train Epoch: 213 [16896/54000 (31%)] Loss: -214426.078125\n",
      "Train Epoch: 213 [18304/54000 (34%)] Loss: -246570.171875\n",
      "Train Epoch: 213 [19712/54000 (37%)] Loss: -228323.984375\n",
      "Train Epoch: 213 [21120/54000 (39%)] Loss: -219263.984375\n",
      "Train Epoch: 213 [22528/54000 (42%)] Loss: -224485.000000\n",
      "Train Epoch: 213 [23936/54000 (44%)] Loss: -230267.562500\n",
      "Train Epoch: 213 [25344/54000 (47%)] Loss: -216738.453125\n",
      "Train Epoch: 213 [26752/54000 (50%)] Loss: -218368.312500\n",
      "Train Epoch: 213 [28160/54000 (52%)] Loss: -227564.953125\n",
      "Train Epoch: 213 [29568/54000 (55%)] Loss: -227442.968750\n",
      "Train Epoch: 213 [30976/54000 (57%)] Loss: -227128.906250\n",
      "Train Epoch: 213 [32384/54000 (60%)] Loss: -210800.562500\n",
      "Train Epoch: 213 [33792/54000 (63%)] Loss: -214092.062500\n",
      "Train Epoch: 213 [35200/54000 (65%)] Loss: -246279.453125\n",
      "Train Epoch: 213 [36608/54000 (68%)] Loss: -223751.265625\n",
      "Train Epoch: 213 [38016/54000 (70%)] Loss: -226332.312500\n",
      "Train Epoch: 213 [39424/54000 (73%)] Loss: -216584.671875\n",
      "Train Epoch: 213 [40832/54000 (76%)] Loss: -228392.281250\n",
      "Train Epoch: 213 [42240/54000 (78%)] Loss: -247068.281250\n",
      "Train Epoch: 213 [43648/54000 (81%)] Loss: -216158.421875\n",
      "Train Epoch: 213 [45056/54000 (83%)] Loss: -215539.375000\n",
      "Train Epoch: 213 [46464/54000 (86%)] Loss: -217527.953125\n",
      "Train Epoch: 213 [47872/54000 (89%)] Loss: -216748.718750\n",
      "Train Epoch: 213 [49280/54000 (91%)] Loss: -218659.718750\n",
      "Train Epoch: 213 [50688/54000 (94%)] Loss: -247750.390625\n",
      "Train Epoch: 213 [52096/54000 (96%)] Loss: -216614.140625\n",
      "    epoch          : 213\n",
      "    loss           : -223573.7057416268\n",
      "    val_loss       : -228657.39271508192\n",
      "Train Epoch: 214 [0/54000 (0%)] Loss: -210916.078125\n",
      "Train Epoch: 214 [1408/54000 (3%)] Loss: -217830.093750\n",
      "Train Epoch: 214 [2816/54000 (5%)] Loss: -227538.984375\n",
      "Train Epoch: 214 [4224/54000 (8%)] Loss: -215925.062500\n",
      "Train Epoch: 214 [5632/54000 (10%)] Loss: -213559.156250\n",
      "Train Epoch: 214 [7040/54000 (13%)] Loss: -225413.140625\n",
      "Train Epoch: 214 [8448/54000 (16%)] Loss: -226506.109375\n",
      "Train Epoch: 214 [9856/54000 (18%)] Loss: -220910.546875\n",
      "Train Epoch: 214 [11264/54000 (21%)] Loss: -217244.578125\n",
      "Train Epoch: 214 [12672/54000 (23%)] Loss: -221706.343750\n",
      "Train Epoch: 214 [14080/54000 (26%)] Loss: -222773.046875\n",
      "Train Epoch: 214 [15488/54000 (29%)] Loss: -213761.812500\n",
      "Train Epoch: 214 [16896/54000 (31%)] Loss: -212693.468750\n",
      "Train Epoch: 214 [18304/54000 (34%)] Loss: -211545.031250\n",
      "Train Epoch: 214 [19712/54000 (37%)] Loss: -217970.437500\n",
      "Train Epoch: 214 [21120/54000 (39%)] Loss: -213264.046875\n",
      "Train Epoch: 214 [22528/54000 (42%)] Loss: -211193.671875\n",
      "Train Epoch: 214 [23936/54000 (44%)] Loss: -221735.421875\n",
      "Train Epoch: 214 [25344/54000 (47%)] Loss: -218990.906250\n",
      "Train Epoch: 214 [26752/54000 (50%)] Loss: -227990.421875\n",
      "Train Epoch: 214 [28160/54000 (52%)] Loss: -216872.515625\n",
      "Train Epoch: 214 [29568/54000 (55%)] Loss: -227999.203125\n",
      "Train Epoch: 214 [30976/54000 (57%)] Loss: -227342.187500\n",
      "Train Epoch: 214 [32384/54000 (60%)] Loss: -223977.656250\n",
      "Train Epoch: 214 [33792/54000 (63%)] Loss: -241292.078125\n",
      "Train Epoch: 214 [35200/54000 (65%)] Loss: -211398.375000\n",
      "Train Epoch: 214 [36608/54000 (68%)] Loss: -210938.625000\n",
      "Train Epoch: 214 [38016/54000 (70%)] Loss: -217695.687500\n",
      "Train Epoch: 214 [39424/54000 (73%)] Loss: -212677.093750\n",
      "Train Epoch: 214 [40832/54000 (76%)] Loss: -214844.687500\n",
      "Train Epoch: 214 [42240/54000 (78%)] Loss: -223688.375000\n",
      "Train Epoch: 214 [43648/54000 (81%)] Loss: -246950.515625\n",
      "Train Epoch: 214 [45056/54000 (83%)] Loss: -227998.234375\n",
      "Train Epoch: 214 [46464/54000 (86%)] Loss: -214803.890625\n",
      "Train Epoch: 214 [47872/54000 (89%)] Loss: -217353.984375\n",
      "Train Epoch: 214 [49280/54000 (91%)] Loss: -221393.171875\n",
      "Train Epoch: 214 [50688/54000 (94%)] Loss: -245271.031250\n",
      "Train Epoch: 214 [52096/54000 (96%)] Loss: -228106.875000\n",
      "    epoch          : 214\n",
      "    loss           : -223488.5614159689\n",
      "    val_loss       : -228743.8355623571\n",
      "Train Epoch: 215 [0/54000 (0%)] Loss: -243271.078125\n",
      "Train Epoch: 215 [1408/54000 (3%)] Loss: -227788.328125\n",
      "Train Epoch: 215 [2816/54000 (5%)] Loss: -229523.968750\n",
      "Train Epoch: 215 [4224/54000 (8%)] Loss: -218612.921875\n",
      "Train Epoch: 215 [5632/54000 (10%)] Loss: -217590.062500\n",
      "Train Epoch: 215 [7040/54000 (13%)] Loss: -222378.359375\n",
      "Train Epoch: 215 [8448/54000 (16%)] Loss: -218486.218750\n",
      "Train Epoch: 215 [9856/54000 (18%)] Loss: -220876.265625\n",
      "Train Epoch: 215 [11264/54000 (21%)] Loss: -216249.843750\n",
      "Train Epoch: 215 [12672/54000 (23%)] Loss: -218252.500000\n",
      "Train Epoch: 215 [14080/54000 (26%)] Loss: -222673.375000\n",
      "Train Epoch: 215 [15488/54000 (29%)] Loss: -221775.406250\n",
      "Train Epoch: 215 [16896/54000 (31%)] Loss: -221020.031250\n",
      "Train Epoch: 215 [18304/54000 (34%)] Loss: -228186.437500\n",
      "Train Epoch: 215 [19712/54000 (37%)] Loss: -226398.609375\n",
      "Train Epoch: 215 [21120/54000 (39%)] Loss: -228970.250000\n",
      "Train Epoch: 215 [22528/54000 (42%)] Loss: -214865.687500\n",
      "Train Epoch: 215 [23936/54000 (44%)] Loss: -242035.828125\n",
      "Train Epoch: 215 [25344/54000 (47%)] Loss: -218964.250000\n",
      "Train Epoch: 215 [26752/54000 (50%)] Loss: -219122.046875\n",
      "Train Epoch: 215 [28160/54000 (52%)] Loss: -226815.906250\n",
      "Train Epoch: 215 [29568/54000 (55%)] Loss: -220003.906250\n",
      "Train Epoch: 215 [30976/54000 (57%)] Loss: -245720.078125\n",
      "Train Epoch: 215 [32384/54000 (60%)] Loss: -225999.593750\n",
      "Train Epoch: 215 [33792/54000 (63%)] Loss: -212509.921875\n",
      "Train Epoch: 215 [35200/54000 (65%)] Loss: -228300.265625\n",
      "Train Epoch: 215 [36608/54000 (68%)] Loss: -221528.703125\n",
      "Train Epoch: 215 [38016/54000 (70%)] Loss: -243629.500000\n",
      "Train Epoch: 215 [39424/54000 (73%)] Loss: -229920.375000\n",
      "Train Epoch: 215 [40832/54000 (76%)] Loss: -211648.312500\n",
      "Train Epoch: 215 [42240/54000 (78%)] Loss: -215688.453125\n",
      "Train Epoch: 215 [43648/54000 (81%)] Loss: -242360.031250\n",
      "Train Epoch: 215 [45056/54000 (83%)] Loss: -229595.000000\n",
      "Train Epoch: 215 [46464/54000 (86%)] Loss: -229856.750000\n",
      "Train Epoch: 215 [47872/54000 (89%)] Loss: -221882.156250\n",
      "Train Epoch: 215 [49280/54000 (91%)] Loss: -221355.921875\n",
      "Train Epoch: 215 [50688/54000 (94%)] Loss: -247287.500000\n",
      "Train Epoch: 215 [52096/54000 (96%)] Loss: -223463.015625\n",
      "    epoch          : 215\n",
      "    loss           : -223649.40636214116\n",
      "    val_loss       : -228334.47091749238\n",
      "Train Epoch: 216 [0/54000 (0%)] Loss: -218345.781250\n",
      "Train Epoch: 216 [1408/54000 (3%)] Loss: -226664.046875\n",
      "Train Epoch: 216 [2816/54000 (5%)] Loss: -218083.109375\n",
      "Train Epoch: 216 [4224/54000 (8%)] Loss: -226205.640625\n",
      "Train Epoch: 216 [5632/54000 (10%)] Loss: -216812.250000\n",
      "Train Epoch: 216 [7040/54000 (13%)] Loss: -213452.500000\n",
      "Train Epoch: 216 [8448/54000 (16%)] Loss: -217666.781250\n",
      "Train Epoch: 216 [9856/54000 (18%)] Loss: -220477.546875\n",
      "Train Epoch: 216 [11264/54000 (21%)] Loss: -216483.125000\n",
      "Train Epoch: 216 [12672/54000 (23%)] Loss: -222429.125000\n",
      "Train Epoch: 216 [14080/54000 (26%)] Loss: -216434.500000\n",
      "Train Epoch: 216 [15488/54000 (29%)] Loss: -216679.718750\n",
      "Train Epoch: 216 [16896/54000 (31%)] Loss: -216720.781250\n",
      "Train Epoch: 216 [18304/54000 (34%)] Loss: -223159.609375\n",
      "Train Epoch: 216 [19712/54000 (37%)] Loss: -218326.937500\n",
      "Train Epoch: 216 [21120/54000 (39%)] Loss: -227665.968750\n",
      "Train Epoch: 216 [22528/54000 (42%)] Loss: -216662.562500\n",
      "Train Epoch: 216 [23936/54000 (44%)] Loss: -220776.750000\n",
      "Train Epoch: 216 [25344/54000 (47%)] Loss: -225489.421875\n",
      "Train Epoch: 216 [26752/54000 (50%)] Loss: -228417.203125\n",
      "Train Epoch: 216 [28160/54000 (52%)] Loss: -229159.656250\n",
      "Train Epoch: 216 [29568/54000 (55%)] Loss: -225893.546875\n",
      "Train Epoch: 216 [30976/54000 (57%)] Loss: -211438.828125\n",
      "Train Epoch: 216 [32384/54000 (60%)] Loss: -219430.421875\n",
      "Train Epoch: 216 [33792/54000 (63%)] Loss: -219605.390625\n",
      "Train Epoch: 216 [35200/54000 (65%)] Loss: -245803.953125\n",
      "Train Epoch: 216 [36608/54000 (68%)] Loss: -211333.828125\n",
      "Train Epoch: 216 [38016/54000 (70%)] Loss: -227494.750000\n",
      "Train Epoch: 216 [39424/54000 (73%)] Loss: -229416.343750\n",
      "Train Epoch: 216 [40832/54000 (76%)] Loss: -229538.890625\n",
      "Train Epoch: 216 [42240/54000 (78%)] Loss: -216765.687500\n",
      "Train Epoch: 216 [43648/54000 (81%)] Loss: -230500.156250\n",
      "Train Epoch: 216 [45056/54000 (83%)] Loss: -244163.218750\n",
      "Train Epoch: 216 [46464/54000 (86%)] Loss: -220087.843750\n",
      "Train Epoch: 216 [47872/54000 (89%)] Loss: -215949.437500\n",
      "Train Epoch: 216 [49280/54000 (91%)] Loss: -220291.546875\n",
      "Train Epoch: 216 [50688/54000 (94%)] Loss: -227524.687500\n",
      "Train Epoch: 216 [52096/54000 (96%)] Loss: -227312.609375\n",
      "    epoch          : 216\n",
      "    loss           : -223609.32692882774\n",
      "    val_loss       : -228982.37734613186\n",
      "Train Epoch: 217 [0/54000 (0%)] Loss: -217131.531250\n",
      "Train Epoch: 217 [1408/54000 (3%)] Loss: -216507.156250\n",
      "Train Epoch: 217 [2816/54000 (5%)] Loss: -227832.031250\n",
      "Train Epoch: 217 [4224/54000 (8%)] Loss: -225047.375000\n",
      "Train Epoch: 217 [5632/54000 (10%)] Loss: -212685.546875\n",
      "Train Epoch: 217 [7040/54000 (13%)] Loss: -214126.437500\n",
      "Train Epoch: 217 [8448/54000 (16%)] Loss: -217343.750000\n",
      "Train Epoch: 217 [9856/54000 (18%)] Loss: -228707.281250\n",
      "Train Epoch: 217 [11264/54000 (21%)] Loss: -213324.187500\n",
      "Train Epoch: 217 [12672/54000 (23%)] Loss: -217062.593750\n",
      "Train Epoch: 217 [14080/54000 (26%)] Loss: -246532.015625\n",
      "Train Epoch: 217 [15488/54000 (29%)] Loss: -229903.421875\n",
      "Train Epoch: 217 [16896/54000 (31%)] Loss: -218704.093750\n",
      "Train Epoch: 217 [18304/54000 (34%)] Loss: -244581.671875\n",
      "Train Epoch: 217 [19712/54000 (37%)] Loss: -226019.281250\n",
      "Train Epoch: 217 [21120/54000 (39%)] Loss: -221013.937500\n",
      "Train Epoch: 217 [22528/54000 (42%)] Loss: -216652.843750\n",
      "Train Epoch: 217 [23936/54000 (44%)] Loss: -222394.640625\n",
      "Train Epoch: 217 [25344/54000 (47%)] Loss: -219903.000000\n",
      "Train Epoch: 217 [26752/54000 (50%)] Loss: -212709.187500\n",
      "Train Epoch: 217 [28160/54000 (52%)] Loss: -214905.843750\n",
      "Train Epoch: 217 [29568/54000 (55%)] Loss: -217105.968750\n",
      "Train Epoch: 217 [30976/54000 (57%)] Loss: -225324.265625\n",
      "Train Epoch: 217 [32384/54000 (60%)] Loss: -245272.812500\n",
      "Train Epoch: 217 [33792/54000 (63%)] Loss: -223509.281250\n",
      "Train Epoch: 217 [35200/54000 (65%)] Loss: -231004.546875\n",
      "Train Epoch: 217 [36608/54000 (68%)] Loss: -211554.140625\n",
      "Train Epoch: 217 [38016/54000 (70%)] Loss: -214547.796875\n",
      "Train Epoch: 217 [39424/54000 (73%)] Loss: -217100.453125\n",
      "Train Epoch: 217 [40832/54000 (76%)] Loss: -215915.015625\n",
      "Train Epoch: 217 [42240/54000 (78%)] Loss: -216625.750000\n",
      "Train Epoch: 217 [43648/54000 (81%)] Loss: -217676.187500\n",
      "Train Epoch: 217 [45056/54000 (83%)] Loss: -243954.765625\n",
      "Train Epoch: 217 [46464/54000 (86%)] Loss: -213685.484375\n",
      "Train Epoch: 217 [47872/54000 (89%)] Loss: -216344.140625\n",
      "Train Epoch: 217 [49280/54000 (91%)] Loss: -226446.906250\n",
      "Train Epoch: 217 [50688/54000 (94%)] Loss: -217921.562500\n",
      "Train Epoch: 217 [52096/54000 (96%)] Loss: -213825.375000\n",
      "    epoch          : 217\n",
      "    loss           : -223661.92351973685\n",
      "    val_loss       : -228684.64450147675\n",
      "Train Epoch: 218 [0/54000 (0%)] Loss: -246248.781250\n",
      "Train Epoch: 218 [1408/54000 (3%)] Loss: -215670.218750\n",
      "Train Epoch: 218 [2816/54000 (5%)] Loss: -213510.031250\n",
      "Train Epoch: 218 [4224/54000 (8%)] Loss: -230765.406250\n",
      "Train Epoch: 218 [5632/54000 (10%)] Loss: -229826.703125\n",
      "Train Epoch: 218 [7040/54000 (13%)] Loss: -228713.906250\n",
      "Train Epoch: 218 [8448/54000 (16%)] Loss: -222139.703125\n",
      "Train Epoch: 218 [9856/54000 (18%)] Loss: -221826.828125\n",
      "Train Epoch: 218 [11264/54000 (21%)] Loss: -221414.812500\n",
      "Train Epoch: 218 [12672/54000 (23%)] Loss: -243594.906250\n",
      "Train Epoch: 218 [14080/54000 (26%)] Loss: -227909.953125\n",
      "Train Epoch: 218 [15488/54000 (29%)] Loss: -228241.734375\n",
      "Train Epoch: 218 [16896/54000 (31%)] Loss: -217056.125000\n",
      "Train Epoch: 218 [18304/54000 (34%)] Loss: -229322.718750\n",
      "Train Epoch: 218 [19712/54000 (37%)] Loss: -213807.750000\n",
      "Train Epoch: 218 [21120/54000 (39%)] Loss: -218197.500000\n",
      "Train Epoch: 218 [22528/54000 (42%)] Loss: -226536.468750\n",
      "Train Epoch: 218 [23936/54000 (44%)] Loss: -214714.875000\n",
      "Train Epoch: 218 [25344/54000 (47%)] Loss: -222622.093750\n",
      "Train Epoch: 218 [26752/54000 (50%)] Loss: -219444.140625\n",
      "Train Epoch: 218 [28160/54000 (52%)] Loss: -221778.937500\n",
      "Train Epoch: 218 [29568/54000 (55%)] Loss: -217750.890625\n",
      "Train Epoch: 218 [30976/54000 (57%)] Loss: -217604.484375\n",
      "Train Epoch: 218 [32384/54000 (60%)] Loss: -244899.140625\n",
      "Train Epoch: 218 [33792/54000 (63%)] Loss: -225084.140625\n",
      "Train Epoch: 218 [35200/54000 (65%)] Loss: -221641.343750\n",
      "Train Epoch: 218 [36608/54000 (68%)] Loss: -214198.609375\n",
      "Train Epoch: 218 [38016/54000 (70%)] Loss: -212616.828125\n",
      "Train Epoch: 218 [39424/54000 (73%)] Loss: -215065.828125\n",
      "Train Epoch: 218 [40832/54000 (76%)] Loss: -228451.296875\n",
      "Train Epoch: 218 [42240/54000 (78%)] Loss: -214236.234375\n",
      "Train Epoch: 218 [43648/54000 (81%)] Loss: -244104.062500\n",
      "Train Epoch: 218 [45056/54000 (83%)] Loss: -219657.875000\n",
      "Train Epoch: 218 [46464/54000 (86%)] Loss: -215656.515625\n",
      "Train Epoch: 218 [47872/54000 (89%)] Loss: -219405.906250\n",
      "Train Epoch: 218 [49280/54000 (91%)] Loss: -219576.000000\n",
      "Train Epoch: 218 [50688/54000 (94%)] Loss: -219566.453125\n",
      "Train Epoch: 218 [52096/54000 (96%)] Loss: -227104.515625\n",
      "    epoch          : 218\n",
      "    loss           : -223792.76188696173\n",
      "    val_loss       : -228458.6096310499\n",
      "Train Epoch: 219 [0/54000 (0%)] Loss: -214433.578125\n",
      "Train Epoch: 219 [1408/54000 (3%)] Loss: -212947.671875\n",
      "Train Epoch: 219 [2816/54000 (5%)] Loss: -217337.937500\n",
      "Train Epoch: 219 [4224/54000 (8%)] Loss: -223757.140625\n",
      "Train Epoch: 219 [5632/54000 (10%)] Loss: -211171.015625\n",
      "Train Epoch: 219 [7040/54000 (13%)] Loss: -243737.062500\n",
      "Train Epoch: 219 [8448/54000 (16%)] Loss: -218500.000000\n",
      "Train Epoch: 219 [9856/54000 (18%)] Loss: -228445.640625\n",
      "Train Epoch: 219 [11264/54000 (21%)] Loss: -215583.937500\n",
      "Train Epoch: 219 [12672/54000 (23%)] Loss: -214791.187500\n",
      "Train Epoch: 219 [14080/54000 (26%)] Loss: -228213.062500\n",
      "Train Epoch: 219 [15488/54000 (29%)] Loss: -247370.640625\n",
      "Train Epoch: 219 [16896/54000 (31%)] Loss: -214926.812500\n",
      "Train Epoch: 219 [18304/54000 (34%)] Loss: -213460.906250\n",
      "Train Epoch: 219 [19712/54000 (37%)] Loss: -214557.031250\n",
      "Train Epoch: 219 [21120/54000 (39%)] Loss: -217574.093750\n",
      "Train Epoch: 219 [22528/54000 (42%)] Loss: -222297.781250\n",
      "Train Epoch: 219 [23936/54000 (44%)] Loss: -216533.203125\n",
      "Train Epoch: 219 [25344/54000 (47%)] Loss: -241713.609375\n",
      "Train Epoch: 219 [26752/54000 (50%)] Loss: -227892.609375\n",
      "Train Epoch: 219 [28160/54000 (52%)] Loss: -226783.234375\n",
      "Train Epoch: 219 [29568/54000 (55%)] Loss: -230299.843750\n",
      "Train Epoch: 219 [30976/54000 (57%)] Loss: -220450.468750\n",
      "Train Epoch: 219 [32384/54000 (60%)] Loss: -229637.531250\n",
      "Train Epoch: 219 [33792/54000 (63%)] Loss: -227955.500000\n",
      "Train Epoch: 219 [35200/54000 (65%)] Loss: -245628.718750\n",
      "Train Epoch: 219 [36608/54000 (68%)] Loss: -216784.515625\n",
      "Train Epoch: 219 [38016/54000 (70%)] Loss: -211227.796875\n",
      "Train Epoch: 219 [39424/54000 (73%)] Loss: -213999.093750\n",
      "Train Epoch: 219 [40832/54000 (76%)] Loss: -226101.390625\n",
      "Train Epoch: 219 [42240/54000 (78%)] Loss: -247747.531250\n",
      "Train Epoch: 219 [43648/54000 (81%)] Loss: -229605.828125\n",
      "Train Epoch: 219 [45056/54000 (83%)] Loss: -244270.828125\n",
      "Train Epoch: 219 [46464/54000 (86%)] Loss: -220456.265625\n",
      "Train Epoch: 219 [47872/54000 (89%)] Loss: -224863.093750\n",
      "Train Epoch: 219 [49280/54000 (91%)] Loss: -223178.390625\n",
      "Train Epoch: 219 [50688/54000 (94%)] Loss: -226894.609375\n",
      "Train Epoch: 219 [52096/54000 (96%)] Loss: -244801.843750\n",
      "    epoch          : 219\n",
      "    loss           : -223732.10130083733\n",
      "    val_loss       : -228863.42689476942\n",
      "Train Epoch: 220 [0/54000 (0%)] Loss: -244944.250000\n",
      "Train Epoch: 220 [1408/54000 (3%)] Loss: -228556.125000\n",
      "Train Epoch: 220 [2816/54000 (5%)] Loss: -218685.750000\n",
      "Train Epoch: 220 [4224/54000 (8%)] Loss: -217181.015625\n",
      "Train Epoch: 220 [5632/54000 (10%)] Loss: -215766.687500\n",
      "Train Epoch: 220 [7040/54000 (13%)] Loss: -213377.750000\n",
      "Train Epoch: 220 [8448/54000 (16%)] Loss: -226367.031250\n",
      "Train Epoch: 220 [9856/54000 (18%)] Loss: -221282.250000\n",
      "Train Epoch: 220 [11264/54000 (21%)] Loss: -216173.578125\n",
      "Train Epoch: 220 [12672/54000 (23%)] Loss: -222471.718750\n",
      "Train Epoch: 220 [14080/54000 (26%)] Loss: -246320.156250\n",
      "Train Epoch: 220 [15488/54000 (29%)] Loss: -245172.765625\n",
      "Train Epoch: 220 [16896/54000 (31%)] Loss: -229487.218750\n",
      "Train Epoch: 220 [18304/54000 (34%)] Loss: -228315.000000\n",
      "Train Epoch: 220 [19712/54000 (37%)] Loss: -222185.781250\n",
      "Train Epoch: 220 [21120/54000 (39%)] Loss: -229911.812500\n",
      "Train Epoch: 220 [22528/54000 (42%)] Loss: -229412.046875\n",
      "Train Epoch: 220 [23936/54000 (44%)] Loss: -221998.703125\n",
      "Train Epoch: 220 [25344/54000 (47%)] Loss: -220950.546875\n",
      "Train Epoch: 220 [26752/54000 (50%)] Loss: -247327.078125\n",
      "Train Epoch: 220 [28160/54000 (52%)] Loss: -226380.296875\n",
      "Train Epoch: 220 [29568/54000 (55%)] Loss: -224923.281250\n",
      "Train Epoch: 220 [30976/54000 (57%)] Loss: -221172.234375\n",
      "Train Epoch: 220 [32384/54000 (60%)] Loss: -246757.437500\n",
      "Train Epoch: 220 [33792/54000 (63%)] Loss: -228233.093750\n",
      "Train Epoch: 220 [35200/54000 (65%)] Loss: -225021.734375\n",
      "Train Epoch: 220 [36608/54000 (68%)] Loss: -229021.406250\n",
      "Train Epoch: 220 [38016/54000 (70%)] Loss: -221241.843750\n",
      "Train Epoch: 220 [39424/54000 (73%)] Loss: -211417.265625\n",
      "Train Epoch: 220 [40832/54000 (76%)] Loss: -218825.953125\n",
      "Train Epoch: 220 [42240/54000 (78%)] Loss: -225389.734375\n",
      "Train Epoch: 220 [43648/54000 (81%)] Loss: -214811.843750\n",
      "Train Epoch: 220 [45056/54000 (83%)] Loss: -219083.609375\n",
      "Train Epoch: 220 [46464/54000 (86%)] Loss: -217834.500000\n",
      "Train Epoch: 220 [47872/54000 (89%)] Loss: -216723.093750\n",
      "Train Epoch: 220 [49280/54000 (91%)] Loss: -219495.250000\n",
      "Train Epoch: 220 [50688/54000 (94%)] Loss: -212442.953125\n",
      "Train Epoch: 220 [52096/54000 (96%)] Loss: -216772.687500\n",
      "    epoch          : 220\n",
      "    loss           : -224035.37866327752\n",
      "    val_loss       : -228784.5338402725\n",
      "Train Epoch: 221 [0/54000 (0%)] Loss: -226045.125000\n",
      "Train Epoch: 221 [1408/54000 (3%)] Loss: -221528.062500\n",
      "Train Epoch: 221 [2816/54000 (5%)] Loss: -229406.859375\n",
      "Train Epoch: 221 [4224/54000 (8%)] Loss: -213533.984375\n",
      "Train Epoch: 221 [5632/54000 (10%)] Loss: -215969.875000\n",
      "Train Epoch: 221 [7040/54000 (13%)] Loss: -213169.859375\n",
      "Train Epoch: 221 [8448/54000 (16%)] Loss: -216714.312500\n",
      "Train Epoch: 221 [9856/54000 (18%)] Loss: -227853.859375\n",
      "Train Epoch: 221 [11264/54000 (21%)] Loss: -217187.187500\n",
      "Train Epoch: 221 [12672/54000 (23%)] Loss: -226942.953125\n",
      "Train Epoch: 221 [14080/54000 (26%)] Loss: -225953.843750\n",
      "Train Epoch: 221 [15488/54000 (29%)] Loss: -217907.500000\n",
      "Train Epoch: 221 [16896/54000 (31%)] Loss: -216633.140625\n",
      "Train Epoch: 221 [18304/54000 (34%)] Loss: -217522.484375\n",
      "Train Epoch: 221 [19712/54000 (37%)] Loss: -247536.359375\n",
      "Train Epoch: 221 [21120/54000 (39%)] Loss: -215310.484375\n",
      "Train Epoch: 221 [22528/54000 (42%)] Loss: -228270.609375\n",
      "Train Epoch: 221 [23936/54000 (44%)] Loss: -226884.765625\n",
      "Train Epoch: 221 [25344/54000 (47%)] Loss: -244933.109375\n",
      "Train Epoch: 221 [26752/54000 (50%)] Loss: -227482.500000\n",
      "Train Epoch: 221 [28160/54000 (52%)] Loss: -216515.718750\n",
      "Train Epoch: 221 [29568/54000 (55%)] Loss: -217232.296875\n",
      "Train Epoch: 221 [30976/54000 (57%)] Loss: -247149.859375\n",
      "Train Epoch: 221 [32384/54000 (60%)] Loss: -217796.203125\n",
      "Train Epoch: 221 [33792/54000 (63%)] Loss: -214988.953125\n",
      "Train Epoch: 221 [35200/54000 (65%)] Loss: -216197.375000\n",
      "Train Epoch: 221 [36608/54000 (68%)] Loss: -245963.500000\n",
      "Train Epoch: 221 [38016/54000 (70%)] Loss: -218970.859375\n",
      "Train Epoch: 221 [39424/54000 (73%)] Loss: -228733.046875\n",
      "Train Epoch: 221 [40832/54000 (76%)] Loss: -211010.781250\n",
      "Train Epoch: 221 [42240/54000 (78%)] Loss: -224344.359375\n",
      "Train Epoch: 221 [43648/54000 (81%)] Loss: -223985.156250\n",
      "Train Epoch: 221 [45056/54000 (83%)] Loss: -225850.312500\n",
      "Train Epoch: 221 [46464/54000 (86%)] Loss: -226587.500000\n",
      "Train Epoch: 221 [47872/54000 (89%)] Loss: -213480.156250\n",
      "Train Epoch: 221 [49280/54000 (91%)] Loss: -244689.281250\n",
      "Train Epoch: 221 [50688/54000 (94%)] Loss: -223442.484375\n",
      "Train Epoch: 221 [52096/54000 (96%)] Loss: -211881.250000\n",
      "    epoch          : 221\n",
      "    loss           : -224001.91069826554\n",
      "    val_loss       : -228897.14837199886\n",
      "Train Epoch: 222 [0/54000 (0%)] Loss: -228341.578125\n",
      "Train Epoch: 222 [1408/54000 (3%)] Loss: -245290.984375\n",
      "Train Epoch: 222 [2816/54000 (5%)] Loss: -228573.031250\n",
      "Train Epoch: 222 [4224/54000 (8%)] Loss: -215505.750000\n",
      "Train Epoch: 222 [5632/54000 (10%)] Loss: -225472.796875\n",
      "Train Epoch: 222 [7040/54000 (13%)] Loss: -228648.640625\n",
      "Train Epoch: 222 [8448/54000 (16%)] Loss: -227567.578125\n",
      "Train Epoch: 222 [9856/54000 (18%)] Loss: -227104.375000\n",
      "Train Epoch: 222 [11264/54000 (21%)] Loss: -224168.671875\n",
      "Train Epoch: 222 [12672/54000 (23%)] Loss: -227613.343750\n",
      "Train Epoch: 222 [14080/54000 (26%)] Loss: -226655.984375\n",
      "Train Epoch: 222 [15488/54000 (29%)] Loss: -244742.609375\n",
      "Train Epoch: 222 [16896/54000 (31%)] Loss: -219549.718750\n",
      "Train Epoch: 222 [18304/54000 (34%)] Loss: -221641.500000\n",
      "Train Epoch: 222 [19712/54000 (37%)] Loss: -216028.281250\n",
      "Train Epoch: 222 [21120/54000 (39%)] Loss: -229134.562500\n",
      "Train Epoch: 222 [22528/54000 (42%)] Loss: -212329.859375\n",
      "Train Epoch: 222 [23936/54000 (44%)] Loss: -227196.687500\n",
      "Train Epoch: 222 [25344/54000 (47%)] Loss: -214270.343750\n",
      "Train Epoch: 222 [26752/54000 (50%)] Loss: -214861.250000\n",
      "Train Epoch: 222 [28160/54000 (52%)] Loss: -218602.968750\n",
      "Train Epoch: 222 [29568/54000 (55%)] Loss: -223101.796875\n",
      "Train Epoch: 222 [30976/54000 (57%)] Loss: -245893.906250\n",
      "Train Epoch: 222 [32384/54000 (60%)] Loss: -216895.343750\n",
      "Train Epoch: 222 [33792/54000 (63%)] Loss: -225076.750000\n",
      "Train Epoch: 222 [35200/54000 (65%)] Loss: -229661.703125\n",
      "Train Epoch: 222 [36608/54000 (68%)] Loss: -212937.921875\n",
      "Train Epoch: 222 [38016/54000 (70%)] Loss: -212890.734375\n",
      "Train Epoch: 222 [39424/54000 (73%)] Loss: -215231.515625\n",
      "Train Epoch: 222 [40832/54000 (76%)] Loss: -225374.062500\n",
      "Train Epoch: 222 [42240/54000 (78%)] Loss: -217272.531250\n",
      "Train Epoch: 222 [43648/54000 (81%)] Loss: -244112.734375\n",
      "Train Epoch: 222 [45056/54000 (83%)] Loss: -228951.687500\n",
      "Train Epoch: 222 [46464/54000 (86%)] Loss: -213588.734375\n",
      "Train Epoch: 222 [47872/54000 (89%)] Loss: -219099.281250\n",
      "Train Epoch: 222 [49280/54000 (91%)] Loss: -217517.437500\n",
      "Train Epoch: 222 [50688/54000 (94%)] Loss: -248132.281250\n",
      "Train Epoch: 222 [52096/54000 (96%)] Loss: -228079.109375\n",
      "    epoch          : 222\n",
      "    loss           : -223784.48781399522\n",
      "    val_loss       : -228258.49767768674\n",
      "Train Epoch: 223 [0/54000 (0%)] Loss: -245092.312500\n",
      "Train Epoch: 223 [1408/54000 (3%)] Loss: -217800.953125\n",
      "Train Epoch: 223 [2816/54000 (5%)] Loss: -218416.031250\n",
      "Train Epoch: 223 [4224/54000 (8%)] Loss: -214114.468750\n",
      "Train Epoch: 223 [5632/54000 (10%)] Loss: -217034.296875\n",
      "Train Epoch: 223 [7040/54000 (13%)] Loss: -218588.765625\n",
      "Train Epoch: 223 [8448/54000 (16%)] Loss: -245463.375000\n",
      "Train Epoch: 223 [9856/54000 (18%)] Loss: -218021.125000\n",
      "Train Epoch: 223 [11264/54000 (21%)] Loss: -228741.093750\n",
      "Train Epoch: 223 [12672/54000 (23%)] Loss: -227355.859375\n",
      "Train Epoch: 223 [14080/54000 (26%)] Loss: -213326.421875\n",
      "Train Epoch: 223 [15488/54000 (29%)] Loss: -216042.140625\n",
      "Train Epoch: 223 [16896/54000 (31%)] Loss: -214911.328125\n",
      "Train Epoch: 223 [18304/54000 (34%)] Loss: -223552.546875\n",
      "Train Epoch: 223 [19712/54000 (37%)] Loss: -214509.468750\n",
      "Train Epoch: 223 [21120/54000 (39%)] Loss: -218865.328125\n",
      "Train Epoch: 223 [22528/54000 (42%)] Loss: -248438.234375\n",
      "Train Epoch: 223 [23936/54000 (44%)] Loss: -228298.359375\n",
      "Train Epoch: 223 [25344/54000 (47%)] Loss: -224144.046875\n",
      "Train Epoch: 223 [26752/54000 (50%)] Loss: -221682.328125\n",
      "Train Epoch: 223 [28160/54000 (52%)] Loss: -228918.968750\n",
      "Train Epoch: 223 [29568/54000 (55%)] Loss: -221012.937500\n",
      "Train Epoch: 223 [30976/54000 (57%)] Loss: -221260.609375\n",
      "Train Epoch: 223 [32384/54000 (60%)] Loss: -225361.921875\n",
      "Train Epoch: 223 [33792/54000 (63%)] Loss: -214370.578125\n",
      "Train Epoch: 223 [35200/54000 (65%)] Loss: -217106.187500\n",
      "Train Epoch: 223 [36608/54000 (68%)] Loss: -216505.156250\n",
      "Train Epoch: 223 [38016/54000 (70%)] Loss: -229105.812500\n",
      "Train Epoch: 223 [39424/54000 (73%)] Loss: -221621.046875\n",
      "Train Epoch: 223 [40832/54000 (76%)] Loss: -216826.656250\n",
      "Train Epoch: 223 [42240/54000 (78%)] Loss: -219947.484375\n",
      "Train Epoch: 223 [43648/54000 (81%)] Loss: -218323.375000\n",
      "Train Epoch: 223 [45056/54000 (83%)] Loss: -247869.234375\n",
      "Train Epoch: 223 [46464/54000 (86%)] Loss: -227701.718750\n",
      "Train Epoch: 223 [47872/54000 (89%)] Loss: -223921.500000\n",
      "Train Epoch: 223 [49280/54000 (91%)] Loss: -223685.281250\n",
      "Train Epoch: 223 [50688/54000 (94%)] Loss: -225716.812500\n",
      "Train Epoch: 223 [52096/54000 (96%)] Loss: -226864.500000\n",
      "    epoch          : 223\n",
      "    loss           : -223876.51921351676\n",
      "    val_loss       : -228755.76224275914\n",
      "Train Epoch: 224 [0/54000 (0%)] Loss: -226715.046875\n",
      "Train Epoch: 224 [1408/54000 (3%)] Loss: -217980.500000\n",
      "Train Epoch: 224 [2816/54000 (5%)] Loss: -216662.734375\n",
      "Train Epoch: 224 [4224/54000 (8%)] Loss: -214815.312500\n",
      "Train Epoch: 224 [5632/54000 (10%)] Loss: -221388.375000\n",
      "Train Epoch: 224 [7040/54000 (13%)] Loss: -219236.875000\n",
      "Train Epoch: 224 [8448/54000 (16%)] Loss: -216710.031250\n",
      "Train Epoch: 224 [9856/54000 (18%)] Loss: -223834.687500\n",
      "Train Epoch: 224 [11264/54000 (21%)] Loss: -215170.312500\n",
      "Train Epoch: 224 [12672/54000 (23%)] Loss: -245705.734375\n",
      "Train Epoch: 224 [14080/54000 (26%)] Loss: -230415.593750\n",
      "Train Epoch: 224 [15488/54000 (29%)] Loss: -221350.328125\n",
      "Train Epoch: 224 [16896/54000 (31%)] Loss: -215335.484375\n",
      "Train Epoch: 224 [18304/54000 (34%)] Loss: -213547.093750\n",
      "Train Epoch: 224 [19712/54000 (37%)] Loss: -214979.984375\n",
      "Train Epoch: 224 [21120/54000 (39%)] Loss: -221648.953125\n",
      "Train Epoch: 224 [22528/54000 (42%)] Loss: -227819.281250\n",
      "Train Epoch: 224 [23936/54000 (44%)] Loss: -213336.625000\n",
      "Train Epoch: 224 [25344/54000 (47%)] Loss: -223205.109375\n",
      "Train Epoch: 224 [26752/54000 (50%)] Loss: -227473.515625\n",
      "Train Epoch: 224 [28160/54000 (52%)] Loss: -222252.531250\n",
      "Train Epoch: 224 [29568/54000 (55%)] Loss: -218877.656250\n",
      "Train Epoch: 224 [30976/54000 (57%)] Loss: -224856.937500\n",
      "Train Epoch: 224 [32384/54000 (60%)] Loss: -245157.968750\n",
      "Train Epoch: 224 [33792/54000 (63%)] Loss: -224998.046875\n",
      "Train Epoch: 224 [35200/54000 (65%)] Loss: -217676.468750\n",
      "Train Epoch: 224 [36608/54000 (68%)] Loss: -217472.687500\n",
      "Train Epoch: 224 [38016/54000 (70%)] Loss: -245952.015625\n",
      "Train Epoch: 224 [39424/54000 (73%)] Loss: -216804.625000\n",
      "Train Epoch: 224 [40832/54000 (76%)] Loss: -225266.437500\n",
      "Train Epoch: 224 [42240/54000 (78%)] Loss: -220325.843750\n",
      "Train Epoch: 224 [43648/54000 (81%)] Loss: -223921.531250\n",
      "Train Epoch: 224 [45056/54000 (83%)] Loss: -224497.359375\n",
      "Train Epoch: 224 [46464/54000 (86%)] Loss: -221827.453125\n",
      "Train Epoch: 224 [47872/54000 (89%)] Loss: -218153.593750\n",
      "Train Epoch: 224 [49280/54000 (91%)] Loss: -224899.453125\n",
      "Train Epoch: 224 [50688/54000 (94%)] Loss: -221834.375000\n",
      "Train Epoch: 224 [52096/54000 (96%)] Loss: -245753.234375\n",
      "    epoch          : 224\n",
      "    loss           : -224007.9882625598\n",
      "    val_loss       : -228527.45663824314\n",
      "Train Epoch: 225 [0/54000 (0%)] Loss: -228178.546875\n",
      "Train Epoch: 225 [1408/54000 (3%)] Loss: -246939.593750\n",
      "Train Epoch: 225 [2816/54000 (5%)] Loss: -224572.125000\n",
      "Train Epoch: 225 [4224/54000 (8%)] Loss: -223260.265625\n",
      "Train Epoch: 225 [5632/54000 (10%)] Loss: -218115.015625\n",
      "Train Epoch: 225 [7040/54000 (13%)] Loss: -243791.296875\n",
      "Train Epoch: 225 [8448/54000 (16%)] Loss: -213309.031250\n",
      "Train Epoch: 225 [9856/54000 (18%)] Loss: -228637.625000\n",
      "Train Epoch: 225 [11264/54000 (21%)] Loss: -226973.484375\n",
      "Train Epoch: 225 [12672/54000 (23%)] Loss: -226153.734375\n",
      "Train Epoch: 225 [14080/54000 (26%)] Loss: -216396.031250\n",
      "Train Epoch: 225 [15488/54000 (29%)] Loss: -220277.750000\n",
      "Train Epoch: 225 [16896/54000 (31%)] Loss: -231367.093750\n",
      "Train Epoch: 225 [18304/54000 (34%)] Loss: -226901.578125\n",
      "Train Epoch: 225 [19712/54000 (37%)] Loss: -247362.437500\n",
      "Train Epoch: 225 [21120/54000 (39%)] Loss: -214605.437500\n",
      "Train Epoch: 225 [22528/54000 (42%)] Loss: -221829.343750\n",
      "Train Epoch: 225 [23936/54000 (44%)] Loss: -226060.109375\n",
      "Train Epoch: 225 [25344/54000 (47%)] Loss: -247023.078125\n",
      "Train Epoch: 225 [26752/54000 (50%)] Loss: -216270.625000\n",
      "Train Epoch: 225 [28160/54000 (52%)] Loss: -215092.156250\n",
      "Train Epoch: 225 [29568/54000 (55%)] Loss: -218511.171875\n",
      "Train Epoch: 225 [30976/54000 (57%)] Loss: -227885.343750\n",
      "Train Epoch: 225 [32384/54000 (60%)] Loss: -228675.671875\n",
      "Train Epoch: 225 [33792/54000 (63%)] Loss: -245290.796875\n",
      "Train Epoch: 225 [35200/54000 (65%)] Loss: -223811.968750\n",
      "Train Epoch: 225 [36608/54000 (68%)] Loss: -221919.031250\n",
      "Train Epoch: 225 [38016/54000 (70%)] Loss: -225237.921875\n",
      "Train Epoch: 225 [39424/54000 (73%)] Loss: -217820.796875\n",
      "Train Epoch: 225 [40832/54000 (76%)] Loss: -245439.406250\n",
      "Train Epoch: 225 [42240/54000 (78%)] Loss: -228579.937500\n",
      "Train Epoch: 225 [43648/54000 (81%)] Loss: -229245.250000\n",
      "Train Epoch: 225 [45056/54000 (83%)] Loss: -216104.062500\n",
      "Train Epoch: 225 [46464/54000 (86%)] Loss: -247360.265625\n",
      "Train Epoch: 225 [47872/54000 (89%)] Loss: -223140.375000\n",
      "Train Epoch: 225 [49280/54000 (91%)] Loss: -222067.625000\n",
      "Train Epoch: 225 [50688/54000 (94%)] Loss: -223862.640625\n",
      "Train Epoch: 225 [52096/54000 (96%)] Loss: -214409.015625\n",
      "    epoch          : 225\n",
      "    loss           : -223967.91073564594\n",
      "    val_loss       : -228707.34941882623\n",
      "Train Epoch: 226 [0/54000 (0%)] Loss: -214529.312500\n",
      "Train Epoch: 226 [1408/54000 (3%)] Loss: -220076.468750\n",
      "Train Epoch: 226 [2816/54000 (5%)] Loss: -217931.281250\n",
      "Train Epoch: 226 [4224/54000 (8%)] Loss: -215822.593750\n",
      "Train Epoch: 226 [5632/54000 (10%)] Loss: -218059.468750\n",
      "Train Epoch: 226 [7040/54000 (13%)] Loss: -224076.046875\n",
      "Train Epoch: 226 [8448/54000 (16%)] Loss: -223418.750000\n",
      "Train Epoch: 226 [9856/54000 (18%)] Loss: -214698.109375\n",
      "Train Epoch: 226 [11264/54000 (21%)] Loss: -215522.984375\n",
      "Train Epoch: 226 [12672/54000 (23%)] Loss: -218727.093750\n",
      "Train Epoch: 226 [14080/54000 (26%)] Loss: -227738.968750\n",
      "Train Epoch: 226 [15488/54000 (29%)] Loss: -247580.156250\n",
      "Train Epoch: 226 [16896/54000 (31%)] Loss: -221745.437500\n",
      "Train Epoch: 226 [18304/54000 (34%)] Loss: -217753.656250\n",
      "Train Epoch: 226 [19712/54000 (37%)] Loss: -210712.000000\n",
      "Train Epoch: 226 [21120/54000 (39%)] Loss: -223467.562500\n",
      "Train Epoch: 226 [22528/54000 (42%)] Loss: -211409.234375\n",
      "Train Epoch: 226 [23936/54000 (44%)] Loss: -227817.234375\n",
      "Train Epoch: 226 [25344/54000 (47%)] Loss: -228159.968750\n",
      "Train Epoch: 226 [26752/54000 (50%)] Loss: -217706.234375\n",
      "Train Epoch: 226 [28160/54000 (52%)] Loss: -225357.328125\n",
      "Train Epoch: 226 [29568/54000 (55%)] Loss: -224264.375000\n",
      "Train Epoch: 226 [30976/54000 (57%)] Loss: -216113.828125\n",
      "Train Epoch: 226 [32384/54000 (60%)] Loss: -246821.468750\n",
      "Train Epoch: 226 [33792/54000 (63%)] Loss: -228536.031250\n",
      "Train Epoch: 226 [35200/54000 (65%)] Loss: -219167.062500\n",
      "Train Epoch: 226 [36608/54000 (68%)] Loss: -216894.765625\n",
      "Train Epoch: 226 [38016/54000 (70%)] Loss: -228352.343750\n",
      "Train Epoch: 226 [39424/54000 (73%)] Loss: -211485.171875\n",
      "Train Epoch: 226 [40832/54000 (76%)] Loss: -223016.921875\n",
      "Train Epoch: 226 [42240/54000 (78%)] Loss: -229938.328125\n",
      "Train Epoch: 226 [43648/54000 (81%)] Loss: -215089.093750\n",
      "Train Epoch: 226 [45056/54000 (83%)] Loss: -245435.234375\n",
      "Train Epoch: 226 [46464/54000 (86%)] Loss: -220197.984375\n",
      "Train Epoch: 226 [47872/54000 (89%)] Loss: -218700.656250\n",
      "Train Epoch: 226 [49280/54000 (91%)] Loss: -218357.906250\n",
      "Train Epoch: 226 [50688/54000 (94%)] Loss: -246310.281250\n",
      "Train Epoch: 226 [52096/54000 (96%)] Loss: -224860.250000\n",
      "    epoch          : 226\n",
      "    loss           : -224037.87597188994\n",
      "    val_loss       : -229017.4432521913\n",
      "Train Epoch: 227 [0/54000 (0%)] Loss: -215568.046875\n",
      "Train Epoch: 227 [1408/54000 (3%)] Loss: -211126.812500\n",
      "Train Epoch: 227 [2816/54000 (5%)] Loss: -244900.828125\n",
      "Train Epoch: 227 [4224/54000 (8%)] Loss: -218867.312500\n",
      "Train Epoch: 227 [5632/54000 (10%)] Loss: -216604.687500\n",
      "Train Epoch: 227 [7040/54000 (13%)] Loss: -226958.937500\n",
      "Train Epoch: 227 [8448/54000 (16%)] Loss: -247227.000000\n",
      "Train Epoch: 227 [9856/54000 (18%)] Loss: -216838.312500\n",
      "Train Epoch: 227 [11264/54000 (21%)] Loss: -215343.281250\n",
      "Train Epoch: 227 [12672/54000 (23%)] Loss: -228847.015625\n",
      "Train Epoch: 227 [14080/54000 (26%)] Loss: -226496.656250\n",
      "Train Epoch: 227 [15488/54000 (29%)] Loss: -226369.250000\n",
      "Train Epoch: 227 [16896/54000 (31%)] Loss: -217167.750000\n",
      "Train Epoch: 227 [18304/54000 (34%)] Loss: -216345.250000\n",
      "Train Epoch: 227 [19712/54000 (37%)] Loss: -244414.437500\n",
      "Train Epoch: 227 [21120/54000 (39%)] Loss: -227013.937500\n",
      "Train Epoch: 227 [22528/54000 (42%)] Loss: -219369.406250\n",
      "Train Epoch: 227 [23936/54000 (44%)] Loss: -217634.437500\n",
      "Train Epoch: 227 [25344/54000 (47%)] Loss: -223715.031250\n",
      "Train Epoch: 227 [26752/54000 (50%)] Loss: -228213.843750\n",
      "Train Epoch: 227 [28160/54000 (52%)] Loss: -215066.796875\n",
      "Train Epoch: 227 [29568/54000 (55%)] Loss: -244359.468750\n",
      "Train Epoch: 227 [30976/54000 (57%)] Loss: -216703.250000\n",
      "Train Epoch: 227 [32384/54000 (60%)] Loss: -226858.734375\n",
      "Train Epoch: 227 [33792/54000 (63%)] Loss: -247415.218750\n",
      "Train Epoch: 227 [35200/54000 (65%)] Loss: -220630.687500\n",
      "Train Epoch: 227 [36608/54000 (68%)] Loss: -229300.562500\n",
      "Train Epoch: 227 [38016/54000 (70%)] Loss: -222396.953125\n",
      "Train Epoch: 227 [39424/54000 (73%)] Loss: -227727.531250\n",
      "Train Epoch: 227 [40832/54000 (76%)] Loss: -230338.187500\n",
      "Train Epoch: 227 [42240/54000 (78%)] Loss: -215210.468750\n",
      "Train Epoch: 227 [43648/54000 (81%)] Loss: -228495.359375\n",
      "Train Epoch: 227 [45056/54000 (83%)] Loss: -225160.375000\n",
      "Train Epoch: 227 [46464/54000 (86%)] Loss: -217540.484375\n",
      "Train Epoch: 227 [47872/54000 (89%)] Loss: -217601.890625\n",
      "Train Epoch: 227 [49280/54000 (91%)] Loss: -220072.031250\n",
      "Train Epoch: 227 [50688/54000 (94%)] Loss: -244594.203125\n",
      "Train Epoch: 227 [52096/54000 (96%)] Loss: -227452.062500\n",
      "    epoch          : 227\n",
      "    loss           : -224116.55704246412\n",
      "    val_loss       : -229197.04333198362\n",
      "Train Epoch: 228 [0/54000 (0%)] Loss: -224565.203125\n",
      "Train Epoch: 228 [1408/54000 (3%)] Loss: -230318.750000\n",
      "Train Epoch: 228 [2816/54000 (5%)] Loss: -221375.500000\n",
      "Train Epoch: 228 [4224/54000 (8%)] Loss: -222286.859375\n",
      "Train Epoch: 228 [5632/54000 (10%)] Loss: -217948.625000\n",
      "Train Epoch: 228 [7040/54000 (13%)] Loss: -217096.328125\n",
      "Train Epoch: 228 [8448/54000 (16%)] Loss: -217621.718750\n",
      "Train Epoch: 228 [9856/54000 (18%)] Loss: -215145.546875\n",
      "Train Epoch: 228 [11264/54000 (21%)] Loss: -223141.468750\n",
      "Train Epoch: 228 [12672/54000 (23%)] Loss: -211973.187500\n",
      "Train Epoch: 228 [14080/54000 (26%)] Loss: -228316.500000\n",
      "Train Epoch: 228 [15488/54000 (29%)] Loss: -243540.250000\n",
      "Train Epoch: 228 [16896/54000 (31%)] Loss: -214431.546875\n",
      "Train Epoch: 228 [18304/54000 (34%)] Loss: -227638.718750\n",
      "Train Epoch: 228 [19712/54000 (37%)] Loss: -217220.906250\n",
      "Train Epoch: 228 [21120/54000 (39%)] Loss: -216316.765625\n",
      "Train Epoch: 228 [22528/54000 (42%)] Loss: -219800.062500\n",
      "Train Epoch: 228 [23936/54000 (44%)] Loss: -218814.750000\n",
      "Train Epoch: 228 [25344/54000 (47%)] Loss: -223963.484375\n",
      "Train Epoch: 228 [26752/54000 (50%)] Loss: -226780.437500\n",
      "Train Epoch: 228 [28160/54000 (52%)] Loss: -244643.984375\n",
      "Train Epoch: 228 [29568/54000 (55%)] Loss: -215774.859375\n",
      "Train Epoch: 228 [30976/54000 (57%)] Loss: -226849.890625\n",
      "Train Epoch: 228 [32384/54000 (60%)] Loss: -223230.718750\n",
      "Train Epoch: 228 [33792/54000 (63%)] Loss: -246414.390625\n",
      "Train Epoch: 228 [35200/54000 (65%)] Loss: -213778.609375\n",
      "Train Epoch: 228 [36608/54000 (68%)] Loss: -229487.593750\n",
      "Train Epoch: 228 [38016/54000 (70%)] Loss: -218855.968750\n",
      "Train Epoch: 228 [39424/54000 (73%)] Loss: -219798.328125\n",
      "Train Epoch: 228 [40832/54000 (76%)] Loss: -221887.140625\n",
      "Train Epoch: 228 [42240/54000 (78%)] Loss: -219319.062500\n",
      "Train Epoch: 228 [43648/54000 (81%)] Loss: -217908.453125\n",
      "Train Epoch: 228 [45056/54000 (83%)] Loss: -223023.421875\n",
      "Train Epoch: 228 [46464/54000 (86%)] Loss: -245455.453125\n",
      "Train Epoch: 228 [47872/54000 (89%)] Loss: -222936.328125\n",
      "Train Epoch: 228 [49280/54000 (91%)] Loss: -220471.640625\n",
      "Train Epoch: 228 [50688/54000 (94%)] Loss: -247856.859375\n",
      "Train Epoch: 228 [52096/54000 (96%)] Loss: -211448.796875\n",
      "    epoch          : 228\n",
      "    loss           : -224145.56276166267\n",
      "    val_loss       : -229212.187380907\n",
      "Train Epoch: 229 [0/54000 (0%)] Loss: -220864.093750\n",
      "Train Epoch: 229 [1408/54000 (3%)] Loss: -246493.031250\n",
      "Train Epoch: 229 [2816/54000 (5%)] Loss: -244419.171875\n",
      "Train Epoch: 229 [4224/54000 (8%)] Loss: -217328.859375\n",
      "Train Epoch: 229 [5632/54000 (10%)] Loss: -219753.281250\n",
      "Train Epoch: 229 [7040/54000 (13%)] Loss: -223604.062500\n",
      "Train Epoch: 229 [8448/54000 (16%)] Loss: -247785.156250\n",
      "Train Epoch: 229 [9856/54000 (18%)] Loss: -218697.906250\n",
      "Train Epoch: 229 [11264/54000 (21%)] Loss: -228531.234375\n",
      "Train Epoch: 229 [12672/54000 (23%)] Loss: -223606.906250\n",
      "Train Epoch: 229 [14080/54000 (26%)] Loss: -223528.812500\n",
      "Train Epoch: 229 [15488/54000 (29%)] Loss: -219208.937500\n",
      "Train Epoch: 229 [16896/54000 (31%)] Loss: -219731.296875\n",
      "Train Epoch: 229 [18304/54000 (34%)] Loss: -216208.500000\n",
      "Train Epoch: 229 [19712/54000 (37%)] Loss: -218661.656250\n",
      "Train Epoch: 229 [21120/54000 (39%)] Loss: -228319.203125\n",
      "Train Epoch: 229 [22528/54000 (42%)] Loss: -219480.531250\n",
      "Train Epoch: 229 [23936/54000 (44%)] Loss: -223365.078125\n",
      "Train Epoch: 229 [25344/54000 (47%)] Loss: -218689.062500\n",
      "Train Epoch: 229 [26752/54000 (50%)] Loss: -225376.796875\n",
      "Train Epoch: 229 [28160/54000 (52%)] Loss: -229070.500000\n",
      "Train Epoch: 229 [29568/54000 (55%)] Loss: -217812.906250\n",
      "Train Epoch: 229 [30976/54000 (57%)] Loss: -246675.546875\n",
      "Train Epoch: 229 [32384/54000 (60%)] Loss: -216382.531250\n",
      "Train Epoch: 229 [33792/54000 (63%)] Loss: -214423.500000\n",
      "Train Epoch: 229 [35200/54000 (65%)] Loss: -229199.093750\n",
      "Train Epoch: 229 [36608/54000 (68%)] Loss: -221083.343750\n",
      "Train Epoch: 229 [38016/54000 (70%)] Loss: -222834.765625\n",
      "Train Epoch: 229 [39424/54000 (73%)] Loss: -220415.250000\n",
      "Train Epoch: 229 [40832/54000 (76%)] Loss: -217337.890625\n",
      "Train Epoch: 229 [42240/54000 (78%)] Loss: -212074.250000\n",
      "Train Epoch: 229 [43648/54000 (81%)] Loss: -225402.453125\n",
      "Train Epoch: 229 [45056/54000 (83%)] Loss: -247420.109375\n",
      "Train Epoch: 229 [46464/54000 (86%)] Loss: -216935.687500\n",
      "Train Epoch: 229 [47872/54000 (89%)] Loss: -218775.859375\n",
      "Train Epoch: 229 [49280/54000 (91%)] Loss: -216378.312500\n",
      "Train Epoch: 229 [50688/54000 (94%)] Loss: -227568.968750\n",
      "Train Epoch: 229 [52096/54000 (96%)] Loss: -217199.453125\n",
      "    epoch          : 229\n",
      "    loss           : -224313.86296351676\n",
      "    val_loss       : -228738.16389576983\n",
      "Train Epoch: 230 [0/54000 (0%)] Loss: -226553.437500\n",
      "Train Epoch: 230 [1408/54000 (3%)] Loss: -226564.031250\n",
      "Train Epoch: 230 [2816/54000 (5%)] Loss: -220247.078125\n",
      "Train Epoch: 230 [4224/54000 (8%)] Loss: -226682.031250\n",
      "Train Epoch: 230 [5632/54000 (10%)] Loss: -215951.890625\n",
      "Train Epoch: 230 [7040/54000 (13%)] Loss: -212819.734375\n",
      "Train Epoch: 230 [8448/54000 (16%)] Loss: -218616.906250\n",
      "Train Epoch: 230 [9856/54000 (18%)] Loss: -219790.109375\n",
      "Train Epoch: 230 [11264/54000 (21%)] Loss: -216186.296875\n",
      "Train Epoch: 230 [12672/54000 (23%)] Loss: -225743.093750\n",
      "Train Epoch: 230 [14080/54000 (26%)] Loss: -247120.531250\n",
      "Train Epoch: 230 [15488/54000 (29%)] Loss: -220270.343750\n",
      "Train Epoch: 230 [16896/54000 (31%)] Loss: -224306.937500\n",
      "Train Epoch: 230 [18304/54000 (34%)] Loss: -215136.390625\n",
      "Train Epoch: 230 [19712/54000 (37%)] Loss: -227426.250000\n",
      "Train Epoch: 230 [21120/54000 (39%)] Loss: -229997.937500\n",
      "Train Epoch: 230 [22528/54000 (42%)] Loss: -222250.515625\n",
      "Train Epoch: 230 [23936/54000 (44%)] Loss: -218942.281250\n",
      "Train Epoch: 230 [25344/54000 (47%)] Loss: -214246.593750\n",
      "Train Epoch: 230 [26752/54000 (50%)] Loss: -214349.593750\n",
      "Train Epoch: 230 [28160/54000 (52%)] Loss: -225428.718750\n",
      "Train Epoch: 230 [29568/54000 (55%)] Loss: -248192.093750\n",
      "Train Epoch: 230 [30976/54000 (57%)] Loss: -226775.203125\n",
      "Train Epoch: 230 [32384/54000 (60%)] Loss: -223447.000000\n",
      "Train Epoch: 230 [33792/54000 (63%)] Loss: -228256.218750\n",
      "Train Epoch: 230 [35200/54000 (65%)] Loss: -229525.875000\n",
      "Train Epoch: 230 [36608/54000 (68%)] Loss: -221561.312500\n",
      "Train Epoch: 230 [38016/54000 (70%)] Loss: -209047.750000\n",
      "Train Epoch: 230 [39424/54000 (73%)] Loss: -223663.937500\n",
      "Train Epoch: 230 [40832/54000 (76%)] Loss: -223888.468750\n",
      "Train Epoch: 230 [42240/54000 (78%)] Loss: -227095.906250\n",
      "Train Epoch: 230 [43648/54000 (81%)] Loss: -220384.109375\n",
      "Train Epoch: 230 [45056/54000 (83%)] Loss: -217212.171875\n",
      "Train Epoch: 230 [46464/54000 (86%)] Loss: -221932.140625\n",
      "Train Epoch: 230 [47872/54000 (89%)] Loss: -217596.437500\n",
      "Train Epoch: 230 [49280/54000 (91%)] Loss: -214699.375000\n",
      "Train Epoch: 230 [50688/54000 (94%)] Loss: -225466.312500\n",
      "Train Epoch: 230 [52096/54000 (96%)] Loss: -225617.593750\n",
      "    epoch          : 230\n",
      "    loss           : -224134.63815789475\n",
      "    val_loss       : -228606.0388838605\n",
      "Train Epoch: 231 [0/54000 (0%)] Loss: -248422.125000\n",
      "Train Epoch: 231 [1408/54000 (3%)] Loss: -211011.796875\n",
      "Train Epoch: 231 [2816/54000 (5%)] Loss: -216131.437500\n",
      "Train Epoch: 231 [4224/54000 (8%)] Loss: -213850.531250\n",
      "Train Epoch: 231 [5632/54000 (10%)] Loss: -225665.546875\n",
      "Train Epoch: 231 [7040/54000 (13%)] Loss: -228988.281250\n",
      "Train Epoch: 231 [8448/54000 (16%)] Loss: -223887.156250\n",
      "Train Epoch: 231 [9856/54000 (18%)] Loss: -229364.937500\n",
      "Train Epoch: 231 [11264/54000 (21%)] Loss: -218986.500000\n",
      "Train Epoch: 231 [12672/54000 (23%)] Loss: -219103.296875\n",
      "Train Epoch: 231 [14080/54000 (26%)] Loss: -228585.359375\n",
      "Train Epoch: 231 [15488/54000 (29%)] Loss: -217980.781250\n",
      "Train Epoch: 231 [16896/54000 (31%)] Loss: -243820.312500\n",
      "Train Epoch: 231 [18304/54000 (34%)] Loss: -223042.312500\n",
      "Train Epoch: 231 [19712/54000 (37%)] Loss: -217813.187500\n",
      "Train Epoch: 231 [21120/54000 (39%)] Loss: -246933.671875\n",
      "Train Epoch: 231 [22528/54000 (42%)] Loss: -205974.171875\n",
      "Train Epoch: 231 [23936/54000 (44%)] Loss: -215843.156250\n",
      "Train Epoch: 231 [25344/54000 (47%)] Loss: -221197.312500\n",
      "Train Epoch: 231 [26752/54000 (50%)] Loss: -221034.656250\n",
      "Train Epoch: 231 [28160/54000 (52%)] Loss: -243858.546875\n",
      "Train Epoch: 231 [29568/54000 (55%)] Loss: -216803.437500\n",
      "Train Epoch: 231 [30976/54000 (57%)] Loss: -217575.921875\n",
      "Train Epoch: 231 [32384/54000 (60%)] Loss: -226830.484375\n",
      "Train Epoch: 231 [33792/54000 (63%)] Loss: -210667.843750\n",
      "Train Epoch: 231 [35200/54000 (65%)] Loss: -220955.375000\n",
      "Train Epoch: 231 [36608/54000 (68%)] Loss: -222761.921875\n",
      "Train Epoch: 231 [38016/54000 (70%)] Loss: -221221.234375\n",
      "Train Epoch: 231 [39424/54000 (73%)] Loss: -228879.718750\n",
      "Train Epoch: 231 [40832/54000 (76%)] Loss: -226237.281250\n",
      "Train Epoch: 231 [42240/54000 (78%)] Loss: -217709.765625\n",
      "Train Epoch: 231 [43648/54000 (81%)] Loss: -225156.656250\n",
      "Train Epoch: 231 [45056/54000 (83%)] Loss: -225981.437500\n",
      "Train Epoch: 231 [46464/54000 (86%)] Loss: -218460.156250\n",
      "Train Epoch: 231 [47872/54000 (89%)] Loss: -221072.390625\n",
      "Train Epoch: 231 [49280/54000 (91%)] Loss: -219300.171875\n",
      "Train Epoch: 231 [50688/54000 (94%)] Loss: -227613.328125\n",
      "Train Epoch: 231 [52096/54000 (96%)] Loss: -245168.703125\n",
      "    epoch          : 231\n",
      "    loss           : -224340.08156399522\n",
      "    val_loss       : -229266.9962723895\n",
      "Train Epoch: 232 [0/54000 (0%)] Loss: -245121.812500\n",
      "Train Epoch: 232 [1408/54000 (3%)] Loss: -219374.812500\n",
      "Train Epoch: 232 [2816/54000 (5%)] Loss: -228775.546875\n",
      "Train Epoch: 232 [4224/54000 (8%)] Loss: -227754.656250\n",
      "Train Epoch: 232 [5632/54000 (10%)] Loss: -213965.921875\n",
      "Train Epoch: 232 [7040/54000 (13%)] Loss: -248126.859375\n",
      "Train Epoch: 232 [8448/54000 (16%)] Loss: -216685.703125\n",
      "Train Epoch: 232 [9856/54000 (18%)] Loss: -214939.781250\n",
      "Train Epoch: 232 [11264/54000 (21%)] Loss: -224250.546875\n",
      "Train Epoch: 232 [12672/54000 (23%)] Loss: -225057.125000\n",
      "Train Epoch: 232 [14080/54000 (26%)] Loss: -224149.187500\n",
      "Train Epoch: 232 [15488/54000 (29%)] Loss: -227372.750000\n",
      "Train Epoch: 232 [16896/54000 (31%)] Loss: -247501.187500\n",
      "Train Epoch: 232 [18304/54000 (34%)] Loss: -218568.796875\n",
      "Train Epoch: 232 [19712/54000 (37%)] Loss: -218752.109375\n",
      "Train Epoch: 232 [21120/54000 (39%)] Loss: -221209.562500\n",
      "Train Epoch: 232 [22528/54000 (42%)] Loss: -228507.828125\n",
      "Train Epoch: 232 [23936/54000 (44%)] Loss: -218304.265625\n",
      "Train Epoch: 232 [25344/54000 (47%)] Loss: -228804.234375\n",
      "Train Epoch: 232 [26752/54000 (50%)] Loss: -213966.921875\n",
      "Train Epoch: 232 [28160/54000 (52%)] Loss: -222483.046875\n",
      "Train Epoch: 232 [29568/54000 (55%)] Loss: -218326.093750\n",
      "Train Epoch: 232 [30976/54000 (57%)] Loss: -218159.859375\n",
      "Train Epoch: 232 [32384/54000 (60%)] Loss: -222910.578125\n",
      "Train Epoch: 232 [33792/54000 (63%)] Loss: -226164.484375\n",
      "Train Epoch: 232 [35200/54000 (65%)] Loss: -214815.781250\n",
      "Train Epoch: 232 [36608/54000 (68%)] Loss: -228877.546875\n",
      "Train Epoch: 232 [38016/54000 (70%)] Loss: -243121.453125\n",
      "Train Epoch: 232 [39424/54000 (73%)] Loss: -218324.187500\n",
      "Train Epoch: 232 [40832/54000 (76%)] Loss: -222259.312500\n",
      "Train Epoch: 232 [42240/54000 (78%)] Loss: -218690.343750\n",
      "Train Epoch: 232 [43648/54000 (81%)] Loss: -227640.546875\n",
      "Train Epoch: 232 [45056/54000 (83%)] Loss: -244405.203125\n",
      "Train Epoch: 232 [46464/54000 (86%)] Loss: -212975.968750\n",
      "Train Epoch: 232 [47872/54000 (89%)] Loss: -225002.593750\n",
      "Train Epoch: 232 [49280/54000 (91%)] Loss: -225672.468750\n",
      "Train Epoch: 232 [50688/54000 (94%)] Loss: -228968.125000\n",
      "Train Epoch: 232 [52096/54000 (96%)] Loss: -225655.406250\n",
      "    epoch          : 232\n",
      "    loss           : -224376.0952452153\n",
      "    val_loss       : -229143.62523818598\n",
      "Train Epoch: 233 [0/54000 (0%)] Loss: -224840.640625\n",
      "Train Epoch: 233 [1408/54000 (3%)] Loss: -243343.031250\n",
      "Train Epoch: 233 [2816/54000 (5%)] Loss: -229173.968750\n",
      "Train Epoch: 233 [4224/54000 (8%)] Loss: -217748.812500\n",
      "Train Epoch: 233 [5632/54000 (10%)] Loss: -223704.328125\n",
      "Train Epoch: 233 [7040/54000 (13%)] Loss: -227642.890625\n",
      "Train Epoch: 233 [8448/54000 (16%)] Loss: -221483.625000\n",
      "Train Epoch: 233 [9856/54000 (18%)] Loss: -229468.921875\n",
      "Train Epoch: 233 [11264/54000 (21%)] Loss: -216311.078125\n",
      "Train Epoch: 233 [12672/54000 (23%)] Loss: -226147.812500\n",
      "Train Epoch: 233 [14080/54000 (26%)] Loss: -225314.593750\n",
      "Train Epoch: 233 [15488/54000 (29%)] Loss: -219398.546875\n",
      "Train Epoch: 233 [16896/54000 (31%)] Loss: -221732.265625\n",
      "Train Epoch: 233 [18304/54000 (34%)] Loss: -215848.218750\n",
      "Train Epoch: 233 [19712/54000 (37%)] Loss: -228482.109375\n",
      "Train Epoch: 233 [21120/54000 (39%)] Loss: -245220.734375\n",
      "Train Epoch: 233 [22528/54000 (42%)] Loss: -218747.578125\n",
      "Train Epoch: 233 [23936/54000 (44%)] Loss: -219829.375000\n",
      "Train Epoch: 233 [25344/54000 (47%)] Loss: -217990.078125\n",
      "Train Epoch: 233 [26752/54000 (50%)] Loss: -246254.375000\n",
      "Train Epoch: 233 [28160/54000 (52%)] Loss: -222509.171875\n",
      "Train Epoch: 233 [29568/54000 (55%)] Loss: -229791.359375\n",
      "Train Epoch: 233 [30976/54000 (57%)] Loss: -227591.640625\n",
      "Train Epoch: 233 [32384/54000 (60%)] Loss: -245371.437500\n",
      "Train Epoch: 233 [33792/54000 (63%)] Loss: -214711.734375\n",
      "Train Epoch: 233 [35200/54000 (65%)] Loss: -218902.937500\n",
      "Train Epoch: 233 [36608/54000 (68%)] Loss: -214315.796875\n",
      "Train Epoch: 233 [38016/54000 (70%)] Loss: -215132.156250\n",
      "Train Epoch: 233 [39424/54000 (73%)] Loss: -219905.156250\n",
      "Train Epoch: 233 [40832/54000 (76%)] Loss: -226417.593750\n",
      "Train Epoch: 233 [42240/54000 (78%)] Loss: -223959.984375\n",
      "Train Epoch: 233 [43648/54000 (81%)] Loss: -223963.781250\n",
      "Train Epoch: 233 [45056/54000 (83%)] Loss: -247998.828125\n",
      "Train Epoch: 233 [46464/54000 (86%)] Loss: -220548.109375\n",
      "Train Epoch: 233 [47872/54000 (89%)] Loss: -224903.187500\n",
      "Train Epoch: 233 [49280/54000 (91%)] Loss: -223879.906250\n",
      "Train Epoch: 233 [50688/54000 (94%)] Loss: -227799.359375\n",
      "Train Epoch: 233 [52096/54000 (96%)] Loss: -216380.687500\n",
      "    epoch          : 233\n",
      "    loss           : -224461.49409389953\n",
      "    val_loss       : -228739.57249190166\n",
      "Train Epoch: 234 [0/54000 (0%)] Loss: -246811.000000\n",
      "Train Epoch: 234 [1408/54000 (3%)] Loss: -229976.156250\n",
      "Train Epoch: 234 [2816/54000 (5%)] Loss: -228257.875000\n",
      "Train Epoch: 234 [4224/54000 (8%)] Loss: -227787.968750\n",
      "Train Epoch: 234 [5632/54000 (10%)] Loss: -217761.750000\n",
      "Train Epoch: 234 [7040/54000 (13%)] Loss: -217030.937500\n",
      "Train Epoch: 234 [8448/54000 (16%)] Loss: -223989.859375\n",
      "Train Epoch: 234 [9856/54000 (18%)] Loss: -215499.875000\n",
      "Train Epoch: 234 [11264/54000 (21%)] Loss: -219170.140625\n",
      "Train Epoch: 234 [12672/54000 (23%)] Loss: -247617.250000\n",
      "Train Epoch: 234 [14080/54000 (26%)] Loss: -228633.734375\n",
      "Train Epoch: 234 [15488/54000 (29%)] Loss: -217303.187500\n",
      "Train Epoch: 234 [16896/54000 (31%)] Loss: -216639.218750\n",
      "Train Epoch: 234 [18304/54000 (34%)] Loss: -220032.718750\n",
      "Train Epoch: 234 [19712/54000 (37%)] Loss: -247938.031250\n",
      "Train Epoch: 234 [21120/54000 (39%)] Loss: -225448.265625\n",
      "Train Epoch: 234 [22528/54000 (42%)] Loss: -229900.593750\n",
      "Train Epoch: 234 [23936/54000 (44%)] Loss: -229611.093750\n",
      "Train Epoch: 234 [25344/54000 (47%)] Loss: -226918.796875\n",
      "Train Epoch: 234 [26752/54000 (50%)] Loss: -246277.046875\n",
      "Train Epoch: 234 [28160/54000 (52%)] Loss: -215823.484375\n",
      "Train Epoch: 234 [29568/54000 (55%)] Loss: -224170.375000\n",
      "Train Epoch: 234 [30976/54000 (57%)] Loss: -227089.296875\n",
      "Train Epoch: 234 [32384/54000 (60%)] Loss: -246462.578125\n",
      "Train Epoch: 234 [33792/54000 (63%)] Loss: -219085.234375\n",
      "Train Epoch: 234 [35200/54000 (65%)] Loss: -215513.562500\n",
      "Train Epoch: 234 [36608/54000 (68%)] Loss: -220361.281250\n",
      "Train Epoch: 234 [38016/54000 (70%)] Loss: -247467.718750\n",
      "Train Epoch: 234 [39424/54000 (73%)] Loss: -248132.437500\n",
      "Train Epoch: 234 [40832/54000 (76%)] Loss: -229890.453125\n",
      "Train Epoch: 234 [42240/54000 (78%)] Loss: -227441.812500\n",
      "Train Epoch: 234 [43648/54000 (81%)] Loss: -214268.437500\n",
      "Train Epoch: 234 [45056/54000 (83%)] Loss: -247313.671875\n",
      "Train Epoch: 234 [46464/54000 (86%)] Loss: -223265.953125\n",
      "Train Epoch: 234 [47872/54000 (89%)] Loss: -226817.187500\n",
      "Train Epoch: 234 [49280/54000 (91%)] Loss: -226560.359375\n",
      "Train Epoch: 234 [50688/54000 (94%)] Loss: -218012.406250\n",
      "Train Epoch: 234 [52096/54000 (96%)] Loss: -226867.484375\n",
      "    epoch          : 234\n",
      "    loss           : -224382.9440041866\n",
      "    val_loss       : -228810.3894459794\n",
      "Train Epoch: 235 [0/54000 (0%)] Loss: -246437.484375\n",
      "Train Epoch: 235 [1408/54000 (3%)] Loss: -221341.468750\n",
      "Train Epoch: 235 [2816/54000 (5%)] Loss: -225986.015625\n",
      "Train Epoch: 235 [4224/54000 (8%)] Loss: -226844.593750\n",
      "Train Epoch: 235 [5632/54000 (10%)] Loss: -212746.156250\n",
      "Train Epoch: 235 [7040/54000 (13%)] Loss: -226420.343750\n",
      "Train Epoch: 235 [8448/54000 (16%)] Loss: -231203.421875\n",
      "Train Epoch: 235 [9856/54000 (18%)] Loss: -248158.812500\n",
      "Train Epoch: 235 [11264/54000 (21%)] Loss: -230106.656250\n",
      "Train Epoch: 235 [12672/54000 (23%)] Loss: -215507.171875\n",
      "Train Epoch: 235 [14080/54000 (26%)] Loss: -244876.953125\n",
      "Train Epoch: 235 [15488/54000 (29%)] Loss: -214890.296875\n",
      "Train Epoch: 235 [16896/54000 (31%)] Loss: -210996.734375\n",
      "Train Epoch: 235 [18304/54000 (34%)] Loss: -218825.625000\n",
      "Train Epoch: 235 [19712/54000 (37%)] Loss: -218018.406250\n",
      "Train Epoch: 235 [21120/54000 (39%)] Loss: -245765.937500\n",
      "Train Epoch: 235 [22528/54000 (42%)] Loss: -224069.156250\n",
      "Train Epoch: 235 [23936/54000 (44%)] Loss: -217587.625000\n",
      "Train Epoch: 235 [25344/54000 (47%)] Loss: -230113.421875\n",
      "Train Epoch: 235 [26752/54000 (50%)] Loss: -220276.468750\n",
      "Train Epoch: 235 [28160/54000 (52%)] Loss: -247354.687500\n",
      "Train Epoch: 235 [29568/54000 (55%)] Loss: -226516.171875\n",
      "Train Epoch: 235 [30976/54000 (57%)] Loss: -214087.671875\n",
      "Train Epoch: 235 [32384/54000 (60%)] Loss: -227070.531250\n",
      "Train Epoch: 235 [33792/54000 (63%)] Loss: -248313.250000\n",
      "Train Epoch: 235 [35200/54000 (65%)] Loss: -222147.625000\n",
      "Train Epoch: 235 [36608/54000 (68%)] Loss: -212575.875000\n",
      "Train Epoch: 235 [38016/54000 (70%)] Loss: -226577.656250\n",
      "Train Epoch: 235 [39424/54000 (73%)] Loss: -211592.765625\n",
      "Train Epoch: 235 [40832/54000 (76%)] Loss: -220115.328125\n",
      "Train Epoch: 235 [42240/54000 (78%)] Loss: -219450.562500\n",
      "Train Epoch: 235 [43648/54000 (81%)] Loss: -230666.140625\n",
      "Train Epoch: 235 [45056/54000 (83%)] Loss: -223217.593750\n",
      "Train Epoch: 235 [46464/54000 (86%)] Loss: -223574.109375\n",
      "Train Epoch: 235 [47872/54000 (89%)] Loss: -217833.828125\n",
      "Train Epoch: 235 [49280/54000 (91%)] Loss: -247462.750000\n",
      "Train Epoch: 235 [50688/54000 (94%)] Loss: -225173.406250\n",
      "Train Epoch: 235 [52096/54000 (96%)] Loss: -218524.093750\n",
      "    epoch          : 235\n",
      "    loss           : -224289.43611692585\n",
      "    val_loss       : -228885.56849037728\n",
      "Train Epoch: 236 [0/54000 (0%)] Loss: -247082.015625\n",
      "Train Epoch: 236 [1408/54000 (3%)] Loss: -215466.062500\n",
      "Train Epoch: 236 [2816/54000 (5%)] Loss: -222111.265625\n",
      "Train Epoch: 236 [4224/54000 (8%)] Loss: -220810.125000\n",
      "Train Epoch: 236 [5632/54000 (10%)] Loss: -217023.687500\n",
      "Train Epoch: 236 [7040/54000 (13%)] Loss: -216565.171875\n",
      "Train Epoch: 236 [8448/54000 (16%)] Loss: -228259.671875\n",
      "Train Epoch: 236 [9856/54000 (18%)] Loss: -215902.531250\n",
      "Train Epoch: 236 [11264/54000 (21%)] Loss: -226871.437500\n",
      "Train Epoch: 236 [12672/54000 (23%)] Loss: -226345.562500\n",
      "Train Epoch: 236 [14080/54000 (26%)] Loss: -228908.640625\n",
      "Train Epoch: 236 [15488/54000 (29%)] Loss: -247722.453125\n",
      "Train Epoch: 236 [16896/54000 (31%)] Loss: -221865.781250\n",
      "Train Epoch: 236 [18304/54000 (34%)] Loss: -216925.640625\n",
      "Train Epoch: 236 [19712/54000 (37%)] Loss: -219834.843750\n",
      "Train Epoch: 236 [21120/54000 (39%)] Loss: -248197.140625\n",
      "Train Epoch: 236 [22528/54000 (42%)] Loss: -217606.953125\n",
      "Train Epoch: 236 [23936/54000 (44%)] Loss: -218808.484375\n",
      "Train Epoch: 236 [25344/54000 (47%)] Loss: -214725.031250\n",
      "Train Epoch: 236 [26752/54000 (50%)] Loss: -247623.328125\n",
      "Train Epoch: 236 [28160/54000 (52%)] Loss: -221031.218750\n",
      "Train Epoch: 236 [29568/54000 (55%)] Loss: -220395.375000\n",
      "Train Epoch: 236 [30976/54000 (57%)] Loss: -227457.578125\n",
      "Train Epoch: 236 [32384/54000 (60%)] Loss: -226570.140625\n",
      "Train Epoch: 236 [33792/54000 (63%)] Loss: -218516.156250\n",
      "Train Epoch: 236 [35200/54000 (65%)] Loss: -247878.656250\n",
      "Train Epoch: 236 [36608/54000 (68%)] Loss: -217077.484375\n",
      "Train Epoch: 236 [38016/54000 (70%)] Loss: -228175.218750\n",
      "Train Epoch: 236 [39424/54000 (73%)] Loss: -247901.890625\n",
      "Train Epoch: 236 [40832/54000 (76%)] Loss: -229646.484375\n",
      "Train Epoch: 236 [42240/54000 (78%)] Loss: -216133.265625\n",
      "Train Epoch: 236 [43648/54000 (81%)] Loss: -218094.218750\n",
      "Train Epoch: 236 [45056/54000 (83%)] Loss: -219079.937500\n",
      "Train Epoch: 236 [46464/54000 (86%)] Loss: -218696.234375\n",
      "Train Epoch: 236 [47872/54000 (89%)] Loss: -221401.812500\n",
      "Train Epoch: 236 [49280/54000 (91%)] Loss: -222899.187500\n",
      "Train Epoch: 236 [50688/54000 (94%)] Loss: -229184.093750\n",
      "Train Epoch: 236 [52096/54000 (96%)] Loss: -225142.500000\n",
      "    epoch          : 236\n",
      "    loss           : -224498.6727347488\n",
      "    val_loss       : -229000.77990424924\n",
      "Train Epoch: 237 [0/54000 (0%)] Loss: -247911.796875\n",
      "Train Epoch: 237 [1408/54000 (3%)] Loss: -226337.843750\n",
      "Train Epoch: 237 [2816/54000 (5%)] Loss: -226064.171875\n",
      "Train Epoch: 237 [4224/54000 (8%)] Loss: -224985.703125\n",
      "Train Epoch: 237 [5632/54000 (10%)] Loss: -230598.234375\n",
      "Train Epoch: 237 [7040/54000 (13%)] Loss: -226865.375000\n",
      "Train Epoch: 237 [8448/54000 (16%)] Loss: -216440.500000\n",
      "Train Epoch: 237 [9856/54000 (18%)] Loss: -227623.828125\n",
      "Train Epoch: 237 [11264/54000 (21%)] Loss: -227601.609375\n",
      "Train Epoch: 237 [12672/54000 (23%)] Loss: -219342.140625\n",
      "Train Epoch: 237 [14080/54000 (26%)] Loss: -247447.921875\n",
      "Train Epoch: 237 [15488/54000 (29%)] Loss: -216137.296875\n",
      "Train Epoch: 237 [16896/54000 (31%)] Loss: -221875.812500\n",
      "Train Epoch: 237 [18304/54000 (34%)] Loss: -246138.968750\n",
      "Train Epoch: 237 [19712/54000 (37%)] Loss: -227883.406250\n",
      "Train Epoch: 237 [21120/54000 (39%)] Loss: -230244.453125\n",
      "Train Epoch: 237 [22528/54000 (42%)] Loss: -219324.765625\n",
      "Train Epoch: 237 [23936/54000 (44%)] Loss: -218941.531250\n",
      "Train Epoch: 237 [25344/54000 (47%)] Loss: -218524.437500\n",
      "Train Epoch: 237 [26752/54000 (50%)] Loss: -214287.296875\n",
      "Train Epoch: 237 [28160/54000 (52%)] Loss: -227408.031250\n",
      "Train Epoch: 237 [29568/54000 (55%)] Loss: -218189.140625\n",
      "Train Epoch: 237 [30976/54000 (57%)] Loss: -218159.703125\n",
      "Train Epoch: 237 [32384/54000 (60%)] Loss: -220848.593750\n",
      "Train Epoch: 237 [33792/54000 (63%)] Loss: -222773.718750\n",
      "Train Epoch: 237 [35200/54000 (65%)] Loss: -229530.546875\n",
      "Train Epoch: 237 [36608/54000 (68%)] Loss: -246000.218750\n",
      "Train Epoch: 237 [38016/54000 (70%)] Loss: -219228.234375\n",
      "Train Epoch: 237 [39424/54000 (73%)] Loss: -215186.781250\n",
      "Train Epoch: 237 [40832/54000 (76%)] Loss: -230275.031250\n",
      "Train Epoch: 237 [42240/54000 (78%)] Loss: -227606.406250\n",
      "Train Epoch: 237 [43648/54000 (81%)] Loss: -247791.187500\n",
      "Train Epoch: 237 [45056/54000 (83%)] Loss: -222167.703125\n",
      "Train Epoch: 237 [46464/54000 (86%)] Loss: -216470.671875\n",
      "Train Epoch: 237 [47872/54000 (89%)] Loss: -225868.265625\n",
      "Train Epoch: 237 [49280/54000 (91%)] Loss: -218406.000000\n",
      "Train Epoch: 237 [50688/54000 (94%)] Loss: -245143.656250\n",
      "Train Epoch: 237 [52096/54000 (96%)] Loss: -226544.671875\n",
      "    epoch          : 237\n",
      "    loss           : -224464.77676435406\n",
      "    val_loss       : -229176.92738900534\n",
      "Train Epoch: 238 [0/54000 (0%)] Loss: -246760.437500\n",
      "Train Epoch: 238 [1408/54000 (3%)] Loss: -226358.921875\n",
      "Train Epoch: 238 [2816/54000 (5%)] Loss: -222319.859375\n",
      "Train Epoch: 238 [4224/54000 (8%)] Loss: -217307.421875\n",
      "Train Epoch: 238 [5632/54000 (10%)] Loss: -223421.093750\n",
      "Train Epoch: 238 [7040/54000 (13%)] Loss: -246802.890625\n",
      "Train Epoch: 238 [8448/54000 (16%)] Loss: -214674.390625\n",
      "Train Epoch: 238 [9856/54000 (18%)] Loss: -220923.828125\n",
      "Train Epoch: 238 [11264/54000 (21%)] Loss: -222437.171875\n",
      "Train Epoch: 238 [12672/54000 (23%)] Loss: -221071.859375\n",
      "Train Epoch: 238 [14080/54000 (26%)] Loss: -215028.421875\n",
      "Train Epoch: 238 [15488/54000 (29%)] Loss: -222801.750000\n",
      "Train Epoch: 238 [16896/54000 (31%)] Loss: -247327.937500\n",
      "Train Epoch: 238 [18304/54000 (34%)] Loss: -227572.312500\n",
      "Train Epoch: 238 [19712/54000 (37%)] Loss: -216149.750000\n",
      "Train Epoch: 238 [21120/54000 (39%)] Loss: -214913.500000\n",
      "Train Epoch: 238 [22528/54000 (42%)] Loss: -227811.875000\n",
      "Train Epoch: 238 [23936/54000 (44%)] Loss: -227322.343750\n",
      "Train Epoch: 238 [25344/54000 (47%)] Loss: -222181.312500\n",
      "Train Epoch: 238 [26752/54000 (50%)] Loss: -219084.000000\n",
      "Train Epoch: 238 [28160/54000 (52%)] Loss: -217371.703125\n",
      "Train Epoch: 238 [29568/54000 (55%)] Loss: -215894.015625\n",
      "Train Epoch: 238 [30976/54000 (57%)] Loss: -218071.718750\n",
      "Train Epoch: 238 [32384/54000 (60%)] Loss: -240747.328125\n",
      "Train Epoch: 238 [33792/54000 (63%)] Loss: -219371.921875\n",
      "Train Epoch: 238 [35200/54000 (65%)] Loss: -225930.703125\n",
      "Train Epoch: 238 [36608/54000 (68%)] Loss: -246090.609375\n",
      "Train Epoch: 238 [38016/54000 (70%)] Loss: -228395.609375\n",
      "Train Epoch: 238 [39424/54000 (73%)] Loss: -219451.203125\n",
      "Train Epoch: 238 [40832/54000 (76%)] Loss: -228244.281250\n",
      "Train Epoch: 238 [42240/54000 (78%)] Loss: -214798.000000\n",
      "Train Epoch: 238 [43648/54000 (81%)] Loss: -224448.671875\n",
      "Train Epoch: 238 [45056/54000 (83%)] Loss: -222484.781250\n",
      "Train Epoch: 238 [46464/54000 (86%)] Loss: -220042.015625\n",
      "Train Epoch: 238 [47872/54000 (89%)] Loss: -216465.984375\n",
      "Train Epoch: 238 [49280/54000 (91%)] Loss: -217171.218750\n",
      "Train Epoch: 238 [50688/54000 (94%)] Loss: -248114.312500\n",
      "Train Epoch: 238 [52096/54000 (96%)] Loss: -228001.703125\n",
      "    epoch          : 238\n",
      "    loss           : -224442.53184808613\n",
      "    val_loss       : -229013.6514029154\n",
      "Train Epoch: 239 [0/54000 (0%)] Loss: -215452.390625\n",
      "Train Epoch: 239 [1408/54000 (3%)] Loss: -217987.234375\n",
      "Train Epoch: 239 [2816/54000 (5%)] Loss: -222312.421875\n",
      "Train Epoch: 239 [4224/54000 (8%)] Loss: -225974.750000\n",
      "Train Epoch: 239 [5632/54000 (10%)] Loss: -225728.125000\n",
      "Train Epoch: 239 [7040/54000 (13%)] Loss: -216527.000000\n",
      "Train Epoch: 239 [8448/54000 (16%)] Loss: -231173.343750\n",
      "Train Epoch: 239 [9856/54000 (18%)] Loss: -219444.500000\n",
      "Train Epoch: 239 [11264/54000 (21%)] Loss: -217101.984375\n",
      "Train Epoch: 239 [12672/54000 (23%)] Loss: -216421.343750\n",
      "Train Epoch: 239 [14080/54000 (26%)] Loss: -228507.078125\n",
      "Train Epoch: 239 [15488/54000 (29%)] Loss: -228213.093750\n",
      "Train Epoch: 239 [16896/54000 (31%)] Loss: -219556.890625\n",
      "Train Epoch: 239 [18304/54000 (34%)] Loss: -221093.468750\n",
      "Train Epoch: 239 [19712/54000 (37%)] Loss: -226173.718750\n",
      "Train Epoch: 239 [21120/54000 (39%)] Loss: -226435.781250\n",
      "Train Epoch: 239 [22528/54000 (42%)] Loss: -219980.250000\n",
      "Train Epoch: 239 [23936/54000 (44%)] Loss: -229234.265625\n",
      "Train Epoch: 239 [25344/54000 (47%)] Loss: -210705.140625\n",
      "Train Epoch: 239 [26752/54000 (50%)] Loss: -246746.531250\n",
      "Train Epoch: 239 [28160/54000 (52%)] Loss: -223954.609375\n",
      "Train Epoch: 239 [29568/54000 (55%)] Loss: -216554.796875\n",
      "Train Epoch: 239 [30976/54000 (57%)] Loss: -218203.937500\n",
      "Train Epoch: 239 [32384/54000 (60%)] Loss: -226662.453125\n",
      "Train Epoch: 239 [33792/54000 (63%)] Loss: -247824.234375\n",
      "Train Epoch: 239 [35200/54000 (65%)] Loss: -217581.359375\n",
      "Train Epoch: 239 [36608/54000 (68%)] Loss: -215743.484375\n",
      "Train Epoch: 239 [38016/54000 (70%)] Loss: -226936.859375\n",
      "Train Epoch: 239 [39424/54000 (73%)] Loss: -219452.593750\n",
      "Train Epoch: 239 [40832/54000 (76%)] Loss: -223559.031250\n",
      "Train Epoch: 239 [42240/54000 (78%)] Loss: -220600.250000\n",
      "Train Epoch: 239 [43648/54000 (81%)] Loss: -228461.093750\n",
      "Train Epoch: 239 [45056/54000 (83%)] Loss: -247839.562500\n",
      "Train Epoch: 239 [46464/54000 (86%)] Loss: -218270.875000\n",
      "Train Epoch: 239 [47872/54000 (89%)] Loss: -216262.046875\n",
      "Train Epoch: 239 [49280/54000 (91%)] Loss: -221313.531250\n",
      "Train Epoch: 239 [50688/54000 (94%)] Loss: -246151.765625\n",
      "Train Epoch: 239 [52096/54000 (96%)] Loss: -216233.859375\n",
      "    epoch          : 239\n",
      "    loss           : -224657.98093600478\n",
      "    val_loss       : -228992.5961735423\n",
      "Train Epoch: 240 [0/54000 (0%)] Loss: -244810.593750\n",
      "Train Epoch: 240 [1408/54000 (3%)] Loss: -229514.250000\n",
      "Train Epoch: 240 [2816/54000 (5%)] Loss: -212487.453125\n",
      "Train Epoch: 240 [4224/54000 (8%)] Loss: -212821.484375\n",
      "Train Epoch: 240 [5632/54000 (10%)] Loss: -220042.546875\n",
      "Train Epoch: 240 [7040/54000 (13%)] Loss: -216853.343750\n",
      "Train Epoch: 240 [8448/54000 (16%)] Loss: -228625.250000\n",
      "Train Epoch: 240 [9856/54000 (18%)] Loss: -215046.484375\n",
      "Train Epoch: 240 [11264/54000 (21%)] Loss: -227709.828125\n",
      "Train Epoch: 240 [12672/54000 (23%)] Loss: -244876.312500\n",
      "Train Epoch: 240 [14080/54000 (26%)] Loss: -220406.140625\n",
      "Train Epoch: 240 [15488/54000 (29%)] Loss: -218064.593750\n",
      "Train Epoch: 240 [16896/54000 (31%)] Loss: -227240.625000\n",
      "Train Epoch: 240 [18304/54000 (34%)] Loss: -247570.218750\n",
      "Train Epoch: 240 [19712/54000 (37%)] Loss: -211668.875000\n",
      "Train Epoch: 240 [21120/54000 (39%)] Loss: -227365.984375\n",
      "Train Epoch: 240 [22528/54000 (42%)] Loss: -229028.406250\n",
      "Train Epoch: 240 [23936/54000 (44%)] Loss: -246303.031250\n",
      "Train Epoch: 240 [25344/54000 (47%)] Loss: -229619.328125\n",
      "Train Epoch: 240 [26752/54000 (50%)] Loss: -229737.453125\n",
      "Train Epoch: 240 [28160/54000 (52%)] Loss: -223134.671875\n",
      "Train Epoch: 240 [29568/54000 (55%)] Loss: -217160.859375\n",
      "Train Epoch: 240 [30976/54000 (57%)] Loss: -217269.593750\n",
      "Train Epoch: 240 [32384/54000 (60%)] Loss: -218143.578125\n",
      "Train Epoch: 240 [33792/54000 (63%)] Loss: -214155.640625\n",
      "Train Epoch: 240 [35200/54000 (65%)] Loss: -218173.062500\n",
      "Train Epoch: 240 [36608/54000 (68%)] Loss: -219866.687500\n",
      "Train Epoch: 240 [38016/54000 (70%)] Loss: -219103.218750\n",
      "Train Epoch: 240 [39424/54000 (73%)] Loss: -245050.593750\n",
      "Train Epoch: 240 [40832/54000 (76%)] Loss: -228611.562500\n",
      "Train Epoch: 240 [42240/54000 (78%)] Loss: -227703.109375\n",
      "Train Epoch: 240 [43648/54000 (81%)] Loss: -228226.515625\n",
      "Train Epoch: 240 [45056/54000 (83%)] Loss: -247079.359375\n",
      "Train Epoch: 240 [46464/54000 (86%)] Loss: -213731.375000\n",
      "Train Epoch: 240 [47872/54000 (89%)] Loss: -218287.687500\n",
      "Train Epoch: 240 [49280/54000 (91%)] Loss: -224632.843750\n",
      "Train Epoch: 240 [50688/54000 (94%)] Loss: -217777.890625\n",
      "Train Epoch: 240 [52096/54000 (96%)] Loss: -217446.703125\n",
      "    epoch          : 240\n",
      "    loss           : -224517.54945424641\n",
      "    val_loss       : -228974.3868021151\n",
      "Train Epoch: 241 [0/54000 (0%)] Loss: -246828.015625\n",
      "Train Epoch: 241 [1408/54000 (3%)] Loss: -228128.781250\n",
      "Train Epoch: 241 [2816/54000 (5%)] Loss: -228370.687500\n",
      "Train Epoch: 241 [4224/54000 (8%)] Loss: -223528.156250\n",
      "Train Epoch: 241 [5632/54000 (10%)] Loss: -218921.531250\n",
      "Train Epoch: 241 [7040/54000 (13%)] Loss: -219406.281250\n",
      "Train Epoch: 241 [8448/54000 (16%)] Loss: -217400.359375\n",
      "Train Epoch: 241 [9856/54000 (18%)] Loss: -223715.562500\n",
      "Train Epoch: 241 [11264/54000 (21%)] Loss: -217073.687500\n",
      "Train Epoch: 241 [12672/54000 (23%)] Loss: -225335.812500\n",
      "Train Epoch: 241 [14080/54000 (26%)] Loss: -245120.296875\n",
      "Train Epoch: 241 [15488/54000 (29%)] Loss: -224299.640625\n",
      "Train Epoch: 241 [16896/54000 (31%)] Loss: -223296.531250\n",
      "Train Epoch: 241 [18304/54000 (34%)] Loss: -219972.968750\n",
      "Train Epoch: 241 [19712/54000 (37%)] Loss: -222031.140625\n",
      "Train Epoch: 241 [21120/54000 (39%)] Loss: -230491.609375\n",
      "Train Epoch: 241 [22528/54000 (42%)] Loss: -221096.312500\n",
      "Train Epoch: 241 [23936/54000 (44%)] Loss: -231334.546875\n",
      "Train Epoch: 241 [25344/54000 (47%)] Loss: -217600.140625\n",
      "Train Epoch: 241 [26752/54000 (50%)] Loss: -216858.656250\n",
      "Train Epoch: 241 [28160/54000 (52%)] Loss: -216757.812500\n",
      "Train Epoch: 241 [29568/54000 (55%)] Loss: -220228.062500\n",
      "Train Epoch: 241 [30976/54000 (57%)] Loss: -223319.468750\n",
      "Train Epoch: 241 [32384/54000 (60%)] Loss: -223032.000000\n",
      "Train Epoch: 241 [33792/54000 (63%)] Loss: -226659.375000\n",
      "Train Epoch: 241 [35200/54000 (65%)] Loss: -229177.515625\n",
      "Train Epoch: 241 [36608/54000 (68%)] Loss: -228689.125000\n",
      "Train Epoch: 241 [38016/54000 (70%)] Loss: -229922.671875\n",
      "Train Epoch: 241 [39424/54000 (73%)] Loss: -215566.250000\n",
      "Train Epoch: 241 [40832/54000 (76%)] Loss: -245732.781250\n",
      "Train Epoch: 241 [42240/54000 (78%)] Loss: -228001.812500\n",
      "Train Epoch: 241 [43648/54000 (81%)] Loss: -216342.625000\n",
      "Train Epoch: 241 [45056/54000 (83%)] Loss: -221821.640625\n",
      "Train Epoch: 241 [46464/54000 (86%)] Loss: -245364.843750\n",
      "Train Epoch: 241 [47872/54000 (89%)] Loss: -221854.109375\n",
      "Train Epoch: 241 [49280/54000 (91%)] Loss: -227057.875000\n",
      "Train Epoch: 241 [50688/54000 (94%)] Loss: -229020.046875\n",
      "Train Epoch: 241 [52096/54000 (96%)] Loss: -227852.500000\n",
      "    epoch          : 241\n",
      "    loss           : -224735.33208732057\n",
      "    val_loss       : -228971.46131859755\n",
      "Train Epoch: 242 [0/54000 (0%)] Loss: -214857.593750\n",
      "Train Epoch: 242 [1408/54000 (3%)] Loss: -215874.500000\n",
      "Train Epoch: 242 [2816/54000 (5%)] Loss: -227726.953125\n",
      "Train Epoch: 242 [4224/54000 (8%)] Loss: -217877.656250\n",
      "Train Epoch: 242 [5632/54000 (10%)] Loss: -223714.218750\n",
      "Train Epoch: 242 [7040/54000 (13%)] Loss: -246340.062500\n",
      "Train Epoch: 242 [8448/54000 (16%)] Loss: -216672.734375\n",
      "Train Epoch: 242 [9856/54000 (18%)] Loss: -218330.656250\n",
      "Train Epoch: 242 [11264/54000 (21%)] Loss: -216009.031250\n",
      "Train Epoch: 242 [12672/54000 (23%)] Loss: -218067.062500\n",
      "Train Epoch: 242 [14080/54000 (26%)] Loss: -245574.640625\n",
      "Train Epoch: 242 [15488/54000 (29%)] Loss: -227908.125000\n",
      "Train Epoch: 242 [16896/54000 (31%)] Loss: -226397.906250\n",
      "Train Epoch: 242 [18304/54000 (34%)] Loss: -246474.531250\n",
      "Train Epoch: 242 [19712/54000 (37%)] Loss: -221656.203125\n",
      "Train Epoch: 242 [21120/54000 (39%)] Loss: -219648.046875\n",
      "Train Epoch: 242 [22528/54000 (42%)] Loss: -217837.609375\n",
      "Train Epoch: 242 [23936/54000 (44%)] Loss: -224029.562500\n",
      "Train Epoch: 242 [25344/54000 (47%)] Loss: -248634.625000\n",
      "Train Epoch: 242 [26752/54000 (50%)] Loss: -226174.093750\n",
      "Train Epoch: 242 [28160/54000 (52%)] Loss: -228993.281250\n",
      "Train Epoch: 242 [29568/54000 (55%)] Loss: -228999.156250\n",
      "Train Epoch: 242 [30976/54000 (57%)] Loss: -219236.828125\n",
      "Train Epoch: 242 [32384/54000 (60%)] Loss: -215961.625000\n",
      "Train Epoch: 242 [33792/54000 (63%)] Loss: -248548.125000\n",
      "Train Epoch: 242 [35200/54000 (65%)] Loss: -215512.000000\n",
      "Train Epoch: 242 [36608/54000 (68%)] Loss: -228705.140625\n",
      "Train Epoch: 242 [38016/54000 (70%)] Loss: -222777.484375\n",
      "Train Epoch: 242 [39424/54000 (73%)] Loss: -219865.843750\n",
      "Train Epoch: 242 [40832/54000 (76%)] Loss: -209428.390625\n",
      "Train Epoch: 242 [42240/54000 (78%)] Loss: -214844.734375\n",
      "Train Epoch: 242 [43648/54000 (81%)] Loss: -228831.390625\n",
      "Train Epoch: 242 [45056/54000 (83%)] Loss: -247611.218750\n",
      "Train Epoch: 242 [46464/54000 (86%)] Loss: -227038.203125\n",
      "Train Epoch: 242 [47872/54000 (89%)] Loss: -221810.953125\n",
      "Train Epoch: 242 [49280/54000 (91%)] Loss: -219505.250000\n",
      "Train Epoch: 242 [50688/54000 (94%)] Loss: -217004.015625\n",
      "Train Epoch: 242 [52096/54000 (96%)] Loss: -226832.640625\n",
      "    epoch          : 242\n",
      "    loss           : -224664.87959778708\n",
      "    val_loss       : -229011.42776414825\n",
      "Train Epoch: 243 [0/54000 (0%)] Loss: -228228.046875\n",
      "Train Epoch: 243 [1408/54000 (3%)] Loss: -246239.218750\n",
      "Train Epoch: 243 [2816/54000 (5%)] Loss: -230331.625000\n",
      "Train Epoch: 243 [4224/54000 (8%)] Loss: -225052.890625\n",
      "Train Epoch: 243 [5632/54000 (10%)] Loss: -218135.906250\n",
      "Train Epoch: 243 [7040/54000 (13%)] Loss: -217840.375000\n",
      "Train Epoch: 243 [8448/54000 (16%)] Loss: -218588.531250\n",
      "Train Epoch: 243 [9856/54000 (18%)] Loss: -215602.453125\n",
      "Train Epoch: 243 [11264/54000 (21%)] Loss: -221419.812500\n",
      "Train Epoch: 243 [12672/54000 (23%)] Loss: -227819.781250\n",
      "Train Epoch: 243 [14080/54000 (26%)] Loss: -223887.359375\n",
      "Train Epoch: 243 [15488/54000 (29%)] Loss: -224051.687500\n",
      "Train Epoch: 243 [16896/54000 (31%)] Loss: -229107.468750\n",
      "Train Epoch: 243 [18304/54000 (34%)] Loss: -227782.750000\n",
      "Train Epoch: 243 [19712/54000 (37%)] Loss: -211797.578125\n",
      "Train Epoch: 243 [21120/54000 (39%)] Loss: -223266.843750\n",
      "Train Epoch: 243 [22528/54000 (42%)] Loss: -223860.515625\n",
      "Train Epoch: 243 [23936/54000 (44%)] Loss: -214390.203125\n",
      "Train Epoch: 243 [25344/54000 (47%)] Loss: -219810.250000\n",
      "Train Epoch: 243 [26752/54000 (50%)] Loss: -222379.203125\n",
      "Train Epoch: 243 [28160/54000 (52%)] Loss: -213341.421875\n",
      "Train Epoch: 243 [29568/54000 (55%)] Loss: -217276.687500\n",
      "Train Epoch: 243 [30976/54000 (57%)] Loss: -211743.453125\n",
      "Train Epoch: 243 [32384/54000 (60%)] Loss: -221812.593750\n",
      "Train Epoch: 243 [33792/54000 (63%)] Loss: -224327.015625\n",
      "Train Epoch: 243 [35200/54000 (65%)] Loss: -229919.906250\n",
      "Train Epoch: 243 [36608/54000 (68%)] Loss: -245135.640625\n",
      "Train Epoch: 243 [38016/54000 (70%)] Loss: -211871.500000\n",
      "Train Epoch: 243 [39424/54000 (73%)] Loss: -247479.906250\n",
      "Train Epoch: 243 [40832/54000 (76%)] Loss: -224554.921875\n",
      "Train Epoch: 243 [42240/54000 (78%)] Loss: -223184.093750\n",
      "Train Epoch: 243 [43648/54000 (81%)] Loss: -222608.437500\n",
      "Train Epoch: 243 [45056/54000 (83%)] Loss: -227431.953125\n",
      "Train Epoch: 243 [46464/54000 (86%)] Loss: -217751.968750\n",
      "Train Epoch: 243 [47872/54000 (89%)] Loss: -217844.421875\n",
      "Train Epoch: 243 [49280/54000 (91%)] Loss: -221651.187500\n",
      "Train Epoch: 243 [50688/54000 (94%)] Loss: -246376.546875\n",
      "Train Epoch: 243 [52096/54000 (96%)] Loss: -215811.250000\n",
      "    epoch          : 243\n",
      "    loss           : -224762.98650568182\n",
      "    val_loss       : -228951.54623189787\n",
      "Train Epoch: 244 [0/54000 (0%)] Loss: -247136.171875\n",
      "Train Epoch: 244 [1408/54000 (3%)] Loss: -212222.578125\n",
      "Train Epoch: 244 [2816/54000 (5%)] Loss: -229438.625000\n",
      "Train Epoch: 244 [4224/54000 (8%)] Loss: -218389.218750\n",
      "Train Epoch: 244 [5632/54000 (10%)] Loss: -228654.546875\n",
      "Train Epoch: 244 [7040/54000 (13%)] Loss: -230480.781250\n",
      "Train Epoch: 244 [8448/54000 (16%)] Loss: -244644.546875\n",
      "Train Epoch: 244 [9856/54000 (18%)] Loss: -212311.718750\n",
      "Train Epoch: 244 [11264/54000 (21%)] Loss: -214897.687500\n",
      "Train Epoch: 244 [12672/54000 (23%)] Loss: -225277.781250\n",
      "Train Epoch: 244 [14080/54000 (26%)] Loss: -216452.750000\n",
      "Train Epoch: 244 [15488/54000 (29%)] Loss: -244016.671875\n",
      "Train Epoch: 244 [16896/54000 (31%)] Loss: -221972.578125\n",
      "Train Epoch: 244 [18304/54000 (34%)] Loss: -222328.484375\n",
      "Train Epoch: 244 [19712/54000 (37%)] Loss: -248365.875000\n",
      "Train Epoch: 244 [21120/54000 (39%)] Loss: -224519.468750\n",
      "Train Epoch: 244 [22528/54000 (42%)] Loss: -229176.156250\n",
      "Train Epoch: 244 [23936/54000 (44%)] Loss: -229949.187500\n",
      "Train Epoch: 244 [25344/54000 (47%)] Loss: -212931.015625\n",
      "Train Epoch: 244 [26752/54000 (50%)] Loss: -218263.812500\n",
      "Train Epoch: 244 [28160/54000 (52%)] Loss: -217924.125000\n",
      "Train Epoch: 244 [29568/54000 (55%)] Loss: -220292.937500\n",
      "Train Epoch: 244 [30976/54000 (57%)] Loss: -225641.296875\n",
      "Train Epoch: 244 [32384/54000 (60%)] Loss: -228233.781250\n",
      "Train Epoch: 244 [33792/54000 (63%)] Loss: -221614.843750\n",
      "Train Epoch: 244 [35200/54000 (65%)] Loss: -222792.203125\n",
      "Train Epoch: 244 [36608/54000 (68%)] Loss: -218193.000000\n",
      "Train Epoch: 244 [38016/54000 (70%)] Loss: -219636.625000\n",
      "Train Epoch: 244 [39424/54000 (73%)] Loss: -213824.203125\n",
      "Train Epoch: 244 [40832/54000 (76%)] Loss: -220286.046875\n",
      "Train Epoch: 244 [42240/54000 (78%)] Loss: -227943.437500\n",
      "Train Epoch: 244 [43648/54000 (81%)] Loss: -230350.906250\n",
      "Train Epoch: 244 [45056/54000 (83%)] Loss: -219288.875000\n",
      "Train Epoch: 244 [46464/54000 (86%)] Loss: -225808.921875\n",
      "Train Epoch: 244 [47872/54000 (89%)] Loss: -219432.406250\n",
      "Train Epoch: 244 [49280/54000 (91%)] Loss: -224341.156250\n",
      "Train Epoch: 244 [50688/54000 (94%)] Loss: -247750.093750\n",
      "Train Epoch: 244 [52096/54000 (96%)] Loss: -219064.156250\n",
      "    epoch          : 244\n",
      "    loss           : -224704.85971142346\n",
      "    val_loss       : -229012.35037157012\n",
      "Train Epoch: 245 [0/54000 (0%)] Loss: -225947.187500\n",
      "Train Epoch: 245 [1408/54000 (3%)] Loss: -226299.500000\n",
      "Train Epoch: 245 [2816/54000 (5%)] Loss: -213238.968750\n",
      "Train Epoch: 245 [4224/54000 (8%)] Loss: -228084.562500\n",
      "Train Epoch: 245 [5632/54000 (10%)] Loss: -228874.000000\n",
      "Train Epoch: 245 [7040/54000 (13%)] Loss: -243752.140625\n",
      "Train Epoch: 245 [8448/54000 (16%)] Loss: -217068.656250\n",
      "Train Epoch: 245 [9856/54000 (18%)] Loss: -222145.625000\n",
      "Train Epoch: 245 [11264/54000 (21%)] Loss: -219377.781250\n",
      "Train Epoch: 245 [12672/54000 (23%)] Loss: -228375.437500\n",
      "Train Epoch: 245 [14080/54000 (26%)] Loss: -221405.906250\n",
      "Train Epoch: 245 [15488/54000 (29%)] Loss: -226702.828125\n",
      "Train Epoch: 245 [16896/54000 (31%)] Loss: -221087.843750\n",
      "Train Epoch: 245 [18304/54000 (34%)] Loss: -226822.781250\n",
      "Train Epoch: 245 [19712/54000 (37%)] Loss: -229231.921875\n",
      "Train Epoch: 245 [21120/54000 (39%)] Loss: -215747.546875\n",
      "Train Epoch: 245 [22528/54000 (42%)] Loss: -208161.218750\n",
      "Train Epoch: 245 [23936/54000 (44%)] Loss: -215075.578125\n",
      "Train Epoch: 245 [25344/54000 (47%)] Loss: -223619.703125\n",
      "Train Epoch: 245 [26752/54000 (50%)] Loss: -247653.468750\n",
      "Train Epoch: 245 [28160/54000 (52%)] Loss: -223485.000000\n",
      "Train Epoch: 245 [29568/54000 (55%)] Loss: -227542.328125\n",
      "Train Epoch: 245 [30976/54000 (57%)] Loss: -221462.531250\n",
      "Train Epoch: 245 [32384/54000 (60%)] Loss: -229019.375000\n",
      "Train Epoch: 245 [33792/54000 (63%)] Loss: -228610.750000\n",
      "Train Epoch: 245 [35200/54000 (65%)] Loss: -219391.484375\n",
      "Train Epoch: 245 [36608/54000 (68%)] Loss: -220476.750000\n",
      "Train Epoch: 245 [38016/54000 (70%)] Loss: -222052.406250\n",
      "Train Epoch: 245 [39424/54000 (73%)] Loss: -245262.359375\n",
      "Train Epoch: 245 [40832/54000 (76%)] Loss: -219868.171875\n",
      "Train Epoch: 245 [42240/54000 (78%)] Loss: -216924.375000\n",
      "Train Epoch: 245 [43648/54000 (81%)] Loss: -223115.843750\n",
      "Train Epoch: 245 [45056/54000 (83%)] Loss: -224514.203125\n",
      "Train Epoch: 245 [46464/54000 (86%)] Loss: -228386.109375\n",
      "Train Epoch: 245 [47872/54000 (89%)] Loss: -228063.093750\n",
      "Train Epoch: 245 [49280/54000 (91%)] Loss: -227058.890625\n",
      "Train Epoch: 245 [50688/54000 (94%)] Loss: -244926.625000\n",
      "Train Epoch: 245 [52096/54000 (96%)] Loss: -217353.218750\n",
      "    epoch          : 245\n",
      "    loss           : -224689.68824760764\n",
      "    val_loss       : -229546.7065489234\n",
      "Train Epoch: 246 [0/54000 (0%)] Loss: -225895.687500\n",
      "Train Epoch: 246 [1408/54000 (3%)] Loss: -221980.156250\n",
      "Train Epoch: 246 [2816/54000 (5%)] Loss: -225931.078125\n",
      "Train Epoch: 246 [4224/54000 (8%)] Loss: -227685.109375\n",
      "Train Epoch: 246 [5632/54000 (10%)] Loss: -215699.796875\n",
      "Train Epoch: 246 [7040/54000 (13%)] Loss: -227778.812500\n",
      "Train Epoch: 246 [8448/54000 (16%)] Loss: -230080.281250\n",
      "Train Epoch: 246 [9856/54000 (18%)] Loss: -230570.968750\n",
      "Train Epoch: 246 [11264/54000 (21%)] Loss: -219539.734375\n",
      "Train Epoch: 246 [12672/54000 (23%)] Loss: -216948.703125\n",
      "Train Epoch: 246 [14080/54000 (26%)] Loss: -248105.843750\n",
      "Train Epoch: 246 [15488/54000 (29%)] Loss: -245621.296875\n",
      "Train Epoch: 246 [16896/54000 (31%)] Loss: -230405.234375\n",
      "Train Epoch: 246 [18304/54000 (34%)] Loss: -215931.234375\n",
      "Train Epoch: 246 [19712/54000 (37%)] Loss: -220421.734375\n",
      "Train Epoch: 246 [21120/54000 (39%)] Loss: -226266.953125\n",
      "Train Epoch: 246 [22528/54000 (42%)] Loss: -216498.234375\n",
      "Train Epoch: 246 [23936/54000 (44%)] Loss: -219583.906250\n",
      "Train Epoch: 246 [25344/54000 (47%)] Loss: -224986.156250\n",
      "Train Epoch: 246 [26752/54000 (50%)] Loss: -218274.421875\n",
      "Train Epoch: 246 [28160/54000 (52%)] Loss: -217904.953125\n",
      "Train Epoch: 246 [29568/54000 (55%)] Loss: -224597.375000\n",
      "Train Epoch: 246 [30976/54000 (57%)] Loss: -220894.093750\n",
      "Train Epoch: 246 [32384/54000 (60%)] Loss: -227485.406250\n",
      "Train Epoch: 246 [33792/54000 (63%)] Loss: -244554.687500\n",
      "Train Epoch: 246 [35200/54000 (65%)] Loss: -213212.343750\n",
      "Train Epoch: 246 [36608/54000 (68%)] Loss: -218862.125000\n",
      "Train Epoch: 246 [38016/54000 (70%)] Loss: -228854.250000\n",
      "Train Epoch: 246 [39424/54000 (73%)] Loss: -220747.640625\n",
      "Train Epoch: 246 [40832/54000 (76%)] Loss: -245019.218750\n",
      "Train Epoch: 246 [42240/54000 (78%)] Loss: -230661.390625\n",
      "Train Epoch: 246 [43648/54000 (81%)] Loss: -219306.937500\n",
      "Train Epoch: 246 [45056/54000 (83%)] Loss: -246586.468750\n",
      "Train Epoch: 246 [46464/54000 (86%)] Loss: -218163.578125\n",
      "Train Epoch: 246 [47872/54000 (89%)] Loss: -218787.250000\n",
      "Train Epoch: 246 [49280/54000 (91%)] Loss: -224998.718750\n",
      "Train Epoch: 246 [50688/54000 (94%)] Loss: -216273.562500\n",
      "Train Epoch: 246 [52096/54000 (96%)] Loss: -226834.890625\n",
      "    epoch          : 246\n",
      "    loss           : -224856.25994318182\n",
      "    val_loss       : -229672.31545946075\n",
      "Train Epoch: 247 [0/54000 (0%)] Loss: -247143.703125\n",
      "Train Epoch: 247 [1408/54000 (3%)] Loss: -244846.468750\n",
      "Train Epoch: 247 [2816/54000 (5%)] Loss: -229998.031250\n",
      "Train Epoch: 247 [4224/54000 (8%)] Loss: -216117.812500\n",
      "Train Epoch: 247 [5632/54000 (10%)] Loss: -229628.062500\n",
      "Train Epoch: 247 [7040/54000 (13%)] Loss: -225548.687500\n",
      "Train Epoch: 247 [8448/54000 (16%)] Loss: -225309.796875\n",
      "Train Epoch: 247 [9856/54000 (18%)] Loss: -244988.500000\n",
      "Train Epoch: 247 [11264/54000 (21%)] Loss: -217833.093750\n",
      "Train Epoch: 247 [12672/54000 (23%)] Loss: -213205.125000\n",
      "Train Epoch: 247 [14080/54000 (26%)] Loss: -220161.015625\n",
      "Train Epoch: 247 [15488/54000 (29%)] Loss: -229626.781250\n",
      "Train Epoch: 247 [16896/54000 (31%)] Loss: -248949.687500\n",
      "Train Epoch: 247 [18304/54000 (34%)] Loss: -219373.875000\n",
      "Train Epoch: 247 [19712/54000 (37%)] Loss: -227594.968750\n",
      "Train Epoch: 247 [21120/54000 (39%)] Loss: -225341.937500\n",
      "Train Epoch: 247 [22528/54000 (42%)] Loss: -223165.718750\n",
      "Train Epoch: 247 [23936/54000 (44%)] Loss: -220685.062500\n",
      "Train Epoch: 247 [25344/54000 (47%)] Loss: -229309.031250\n",
      "Train Epoch: 247 [26752/54000 (50%)] Loss: -225483.375000\n",
      "Train Epoch: 247 [28160/54000 (52%)] Loss: -230453.812500\n",
      "Train Epoch: 247 [29568/54000 (55%)] Loss: -225698.359375\n",
      "Train Epoch: 247 [30976/54000 (57%)] Loss: -225625.328125\n",
      "Train Epoch: 247 [32384/54000 (60%)] Loss: -220465.937500\n",
      "Train Epoch: 247 [33792/54000 (63%)] Loss: -211961.468750\n",
      "Train Epoch: 247 [35200/54000 (65%)] Loss: -218110.703125\n",
      "Train Epoch: 247 [36608/54000 (68%)] Loss: -217502.375000\n",
      "Train Epoch: 247 [38016/54000 (70%)] Loss: -216547.796875\n",
      "Train Epoch: 247 [39424/54000 (73%)] Loss: -246411.562500\n",
      "Train Epoch: 247 [40832/54000 (76%)] Loss: -219496.359375\n",
      "Train Epoch: 247 [42240/54000 (78%)] Loss: -230366.187500\n",
      "Train Epoch: 247 [43648/54000 (81%)] Loss: -221510.765625\n",
      "Train Epoch: 247 [45056/54000 (83%)] Loss: -224305.593750\n",
      "Train Epoch: 247 [46464/54000 (86%)] Loss: -227340.000000\n",
      "Train Epoch: 247 [47872/54000 (89%)] Loss: -217824.031250\n",
      "Train Epoch: 247 [49280/54000 (91%)] Loss: -219409.265625\n",
      "Train Epoch: 247 [50688/54000 (94%)] Loss: -227390.062500\n",
      "Train Epoch: 247 [52096/54000 (96%)] Loss: -228815.937500\n",
      "    epoch          : 247\n",
      "    loss           : -224992.37339264355\n",
      "    val_loss       : -229086.52316358613\n",
      "Train Epoch: 248 [0/54000 (0%)] Loss: -248639.953125\n",
      "Train Epoch: 248 [1408/54000 (3%)] Loss: -219891.140625\n",
      "Train Epoch: 248 [2816/54000 (5%)] Loss: -215808.937500\n",
      "Train Epoch: 248 [4224/54000 (8%)] Loss: -227722.812500\n",
      "Train Epoch: 248 [5632/54000 (10%)] Loss: -220244.250000\n",
      "Train Epoch: 248 [7040/54000 (13%)] Loss: -230374.578125\n",
      "Train Epoch: 248 [8448/54000 (16%)] Loss: -213133.687500\n",
      "Train Epoch: 248 [9856/54000 (18%)] Loss: -218458.406250\n",
      "Train Epoch: 248 [11264/54000 (21%)] Loss: -218996.781250\n",
      "Train Epoch: 248 [12672/54000 (23%)] Loss: -223553.859375\n",
      "Train Epoch: 248 [14080/54000 (26%)] Loss: -221434.390625\n",
      "Train Epoch: 248 [15488/54000 (29%)] Loss: -212059.453125\n",
      "Train Epoch: 248 [16896/54000 (31%)] Loss: -227223.218750\n",
      "Train Epoch: 248 [18304/54000 (34%)] Loss: -213741.578125\n",
      "Train Epoch: 248 [19712/54000 (37%)] Loss: -227927.843750\n",
      "Train Epoch: 248 [21120/54000 (39%)] Loss: -248603.593750\n",
      "Train Epoch: 248 [22528/54000 (42%)] Loss: -224737.015625\n",
      "Train Epoch: 248 [23936/54000 (44%)] Loss: -217454.531250\n",
      "Train Epoch: 248 [25344/54000 (47%)] Loss: -220854.937500\n",
      "Train Epoch: 248 [26752/54000 (50%)] Loss: -246625.062500\n",
      "Train Epoch: 248 [28160/54000 (52%)] Loss: -226710.531250\n",
      "Train Epoch: 248 [29568/54000 (55%)] Loss: -226929.031250\n",
      "Train Epoch: 248 [30976/54000 (57%)] Loss: -228541.812500\n",
      "Train Epoch: 248 [32384/54000 (60%)] Loss: -228773.828125\n",
      "Train Epoch: 248 [33792/54000 (63%)] Loss: -230403.562500\n",
      "Train Epoch: 248 [35200/54000 (65%)] Loss: -221020.765625\n",
      "Train Epoch: 248 [36608/54000 (68%)] Loss: -215338.312500\n",
      "Train Epoch: 248 [38016/54000 (70%)] Loss: -217559.281250\n",
      "Train Epoch: 248 [39424/54000 (73%)] Loss: -215766.765625\n",
      "Train Epoch: 248 [40832/54000 (76%)] Loss: -245306.234375\n",
      "Train Epoch: 248 [42240/54000 (78%)] Loss: -226555.437500\n",
      "Train Epoch: 248 [43648/54000 (81%)] Loss: -222773.468750\n",
      "Train Epoch: 248 [45056/54000 (83%)] Loss: -248566.968750\n",
      "Train Epoch: 248 [46464/54000 (86%)] Loss: -218689.109375\n",
      "Train Epoch: 248 [47872/54000 (89%)] Loss: -216635.375000\n",
      "Train Epoch: 248 [49280/54000 (91%)] Loss: -218814.843750\n",
      "Train Epoch: 248 [50688/54000 (94%)] Loss: -218290.093750\n",
      "Train Epoch: 248 [52096/54000 (96%)] Loss: -244703.671875\n",
      "    epoch          : 248\n",
      "    loss           : -224867.82535885167\n",
      "    val_loss       : -228624.88020436358\n",
      "Train Epoch: 249 [0/54000 (0%)] Loss: -246479.531250\n",
      "Train Epoch: 249 [1408/54000 (3%)] Loss: -227054.468750\n",
      "Train Epoch: 249 [2816/54000 (5%)] Loss: -218705.234375\n",
      "Train Epoch: 249 [4224/54000 (8%)] Loss: -221894.406250\n",
      "Train Epoch: 249 [5632/54000 (10%)] Loss: -215301.265625\n",
      "Train Epoch: 249 [7040/54000 (13%)] Loss: -222341.687500\n",
      "Train Epoch: 249 [8448/54000 (16%)] Loss: -220120.500000\n",
      "Train Epoch: 249 [9856/54000 (18%)] Loss: -219332.625000\n",
      "Train Epoch: 249 [11264/54000 (21%)] Loss: -217550.593750\n",
      "Train Epoch: 249 [12672/54000 (23%)] Loss: -216828.546875\n",
      "Train Epoch: 249 [14080/54000 (26%)] Loss: -221806.500000\n",
      "Train Epoch: 249 [15488/54000 (29%)] Loss: -222573.750000\n",
      "Train Epoch: 249 [16896/54000 (31%)] Loss: -226611.546875\n",
      "Train Epoch: 249 [18304/54000 (34%)] Loss: -224443.312500\n",
      "Train Epoch: 249 [19712/54000 (37%)] Loss: -246783.500000\n",
      "Train Epoch: 249 [21120/54000 (39%)] Loss: -223601.687500\n",
      "Train Epoch: 249 [22528/54000 (42%)] Loss: -221919.875000\n",
      "Train Epoch: 249 [23936/54000 (44%)] Loss: -224377.875000\n",
      "Train Epoch: 249 [25344/54000 (47%)] Loss: -241146.718750\n",
      "Train Epoch: 249 [26752/54000 (50%)] Loss: -216143.218750\n",
      "Train Epoch: 249 [28160/54000 (52%)] Loss: -211555.234375\n",
      "Train Epoch: 249 [29568/54000 (55%)] Loss: -217751.921875\n",
      "Train Epoch: 249 [30976/54000 (57%)] Loss: -248037.140625\n",
      "Train Epoch: 249 [32384/54000 (60%)] Loss: -227565.000000\n",
      "Train Epoch: 249 [33792/54000 (63%)] Loss: -225677.703125\n",
      "Train Epoch: 249 [35200/54000 (65%)] Loss: -223062.000000\n",
      "Train Epoch: 249 [36608/54000 (68%)] Loss: -221344.031250\n",
      "Train Epoch: 249 [38016/54000 (70%)] Loss: -217209.562500\n",
      "Train Epoch: 249 [39424/54000 (73%)] Loss: -247869.046875\n",
      "Train Epoch: 249 [40832/54000 (76%)] Loss: -216490.000000\n",
      "Train Epoch: 249 [42240/54000 (78%)] Loss: -228189.046875\n",
      "Train Epoch: 249 [43648/54000 (81%)] Loss: -219119.218750\n",
      "Train Epoch: 249 [45056/54000 (83%)] Loss: -246870.109375\n",
      "Train Epoch: 249 [46464/54000 (86%)] Loss: -219786.875000\n",
      "Train Epoch: 249 [47872/54000 (89%)] Loss: -220482.875000\n",
      "Train Epoch: 249 [49280/54000 (91%)] Loss: -214701.656250\n",
      "Train Epoch: 249 [50688/54000 (94%)] Loss: -220385.453125\n",
      "Train Epoch: 249 [52096/54000 (96%)] Loss: -227116.609375\n",
      "    epoch          : 249\n",
      "    loss           : -225007.69478917465\n",
      "    val_loss       : -229313.25710389673\n",
      "Train Epoch: 250 [0/54000 (0%)] Loss: -219952.671875\n",
      "Train Epoch: 250 [1408/54000 (3%)] Loss: -215380.062500\n",
      "Train Epoch: 250 [2816/54000 (5%)] Loss: -216995.828125\n",
      "Train Epoch: 250 [4224/54000 (8%)] Loss: -230255.875000\n",
      "Train Epoch: 250 [5632/54000 (10%)] Loss: -216900.859375\n",
      "Train Epoch: 250 [7040/54000 (13%)] Loss: -228764.531250\n",
      "Train Epoch: 250 [8448/54000 (16%)] Loss: -214422.171875\n",
      "Train Epoch: 250 [9856/54000 (18%)] Loss: -228883.656250\n",
      "Train Epoch: 250 [11264/54000 (21%)] Loss: -228483.062500\n",
      "Train Epoch: 250 [12672/54000 (23%)] Loss: -228610.812500\n",
      "Train Epoch: 250 [14080/54000 (26%)] Loss: -224780.781250\n",
      "Train Epoch: 250 [15488/54000 (29%)] Loss: -245751.921875\n",
      "Train Epoch: 250 [16896/54000 (31%)] Loss: -222521.562500\n",
      "Train Epoch: 250 [18304/54000 (34%)] Loss: -226442.640625\n",
      "Train Epoch: 250 [19712/54000 (37%)] Loss: -243429.171875\n",
      "Train Epoch: 250 [21120/54000 (39%)] Loss: -227607.328125\n",
      "Train Epoch: 250 [22528/54000 (42%)] Loss: -219366.671875\n",
      "Train Epoch: 250 [23936/54000 (44%)] Loss: -217718.562500\n",
      "Train Epoch: 250 [25344/54000 (47%)] Loss: -221635.046875\n",
      "Train Epoch: 250 [26752/54000 (50%)] Loss: -218310.015625\n",
      "Train Epoch: 250 [28160/54000 (52%)] Loss: -217166.062500\n",
      "Train Epoch: 250 [29568/54000 (55%)] Loss: -222521.265625\n",
      "Train Epoch: 250 [30976/54000 (57%)] Loss: -221733.187500\n",
      "Train Epoch: 250 [32384/54000 (60%)] Loss: -220711.390625\n",
      "Train Epoch: 250 [33792/54000 (63%)] Loss: -223839.390625\n",
      "Train Epoch: 250 [35200/54000 (65%)] Loss: -227255.359375\n",
      "Train Epoch: 250 [36608/54000 (68%)] Loss: -218138.843750\n",
      "Train Epoch: 250 [38016/54000 (70%)] Loss: -247994.625000\n",
      "Train Epoch: 250 [39424/54000 (73%)] Loss: -224581.406250\n",
      "Train Epoch: 250 [40832/54000 (76%)] Loss: -230323.125000\n",
      "Train Epoch: 250 [42240/54000 (78%)] Loss: -228291.984375\n",
      "Train Epoch: 250 [43648/54000 (81%)] Loss: -229036.671875\n",
      "Train Epoch: 250 [45056/54000 (83%)] Loss: -222267.593750\n",
      "Train Epoch: 250 [46464/54000 (86%)] Loss: -221522.171875\n",
      "Train Epoch: 250 [47872/54000 (89%)] Loss: -222763.343750\n",
      "Train Epoch: 250 [49280/54000 (91%)] Loss: -219143.296875\n",
      "Train Epoch: 250 [50688/54000 (94%)] Loss: -229487.921875\n",
      "Train Epoch: 250 [52096/54000 (96%)] Loss: -227936.171875\n",
      "    epoch          : 250\n",
      "    loss           : -224949.23172099283\n",
      "    val_loss       : -228905.88943407012\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0508_092801/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [0/54000 (0%)] Loss: -245416.562500\n",
      "Train Epoch: 251 [1408/54000 (3%)] Loss: -215015.734375\n",
      "Train Epoch: 251 [2816/54000 (5%)] Loss: -218152.015625\n",
      "Train Epoch: 251 [4224/54000 (8%)] Loss: -228482.515625\n",
      "Train Epoch: 251 [5632/54000 (10%)] Loss: -230241.281250\n",
      "Train Epoch: 251 [7040/54000 (13%)] Loss: -247445.343750\n",
      "Train Epoch: 251 [8448/54000 (16%)] Loss: -216842.500000\n",
      "Train Epoch: 251 [9856/54000 (18%)] Loss: -218083.000000\n",
      "Train Epoch: 251 [11264/54000 (21%)] Loss: -224943.125000\n",
      "Train Epoch: 251 [12672/54000 (23%)] Loss: -228633.109375\n",
      "Train Epoch: 251 [14080/54000 (26%)] Loss: -218342.406250\n",
      "Train Epoch: 251 [15488/54000 (29%)] Loss: -218898.781250\n",
      "Train Epoch: 251 [16896/54000 (31%)] Loss: -222525.531250\n",
      "Train Epoch: 251 [18304/54000 (34%)] Loss: -222427.250000\n",
      "Train Epoch: 251 [19712/54000 (37%)] Loss: -226431.062500\n",
      "Train Epoch: 251 [21120/54000 (39%)] Loss: -221880.093750\n",
      "Train Epoch: 251 [22528/54000 (42%)] Loss: -209624.906250\n",
      "Train Epoch: 251 [23936/54000 (44%)] Loss: -217319.125000\n",
      "Train Epoch: 251 [25344/54000 (47%)] Loss: -223784.250000\n",
      "Train Epoch: 251 [26752/54000 (50%)] Loss: -222106.000000\n",
      "Train Epoch: 251 [28160/54000 (52%)] Loss: -244310.937500\n",
      "Train Epoch: 251 [29568/54000 (55%)] Loss: -227235.828125\n",
      "Train Epoch: 251 [30976/54000 (57%)] Loss: -224527.093750\n",
      "Train Epoch: 251 [32384/54000 (60%)] Loss: -218923.250000\n",
      "Train Epoch: 251 [33792/54000 (63%)] Loss: -228080.390625\n",
      "Train Epoch: 251 [35200/54000 (65%)] Loss: -219262.406250\n",
      "Train Epoch: 251 [36608/54000 (68%)] Loss: -222969.687500\n",
      "Train Epoch: 251 [38016/54000 (70%)] Loss: -246778.546875\n",
      "Train Epoch: 251 [39424/54000 (73%)] Loss: -229444.968750\n",
      "Train Epoch: 251 [40832/54000 (76%)] Loss: -228323.406250\n",
      "Train Epoch: 251 [42240/54000 (78%)] Loss: -216391.046875\n",
      "Train Epoch: 251 [43648/54000 (81%)] Loss: -224222.593750\n",
      "Train Epoch: 251 [45056/54000 (83%)] Loss: -247682.062500\n",
      "Train Epoch: 251 [46464/54000 (86%)] Loss: -221221.546875\n",
      "Train Epoch: 251 [47872/54000 (89%)] Loss: -211930.546875\n",
      "Train Epoch: 251 [49280/54000 (91%)] Loss: -224559.187500\n",
      "Train Epoch: 251 [50688/54000 (94%)] Loss: -247488.781250\n",
      "Train Epoch: 251 [52096/54000 (96%)] Loss: -214858.390625\n",
      "    epoch          : 251\n",
      "    loss           : -224952.79597787082\n",
      "    val_loss       : -229174.8329720846\n",
      "Train Epoch: 252 [0/54000 (0%)] Loss: -218144.484375\n",
      "Train Epoch: 252 [1408/54000 (3%)] Loss: -227986.484375\n",
      "Train Epoch: 252 [2816/54000 (5%)] Loss: -224399.218750\n",
      "Train Epoch: 252 [4224/54000 (8%)] Loss: -219501.281250\n",
      "Train Epoch: 252 [5632/54000 (10%)] Loss: -214093.953125\n",
      "Train Epoch: 252 [7040/54000 (13%)] Loss: -217888.656250\n",
      "Train Epoch: 252 [8448/54000 (16%)] Loss: -219948.593750\n",
      "Train Epoch: 252 [9856/54000 (18%)] Loss: -245101.546875\n",
      "Train Epoch: 252 [11264/54000 (21%)] Loss: -220486.218750\n",
      "Train Epoch: 252 [12672/54000 (23%)] Loss: -222590.890625\n",
      "Train Epoch: 252 [14080/54000 (26%)] Loss: -225256.828125\n",
      "Train Epoch: 252 [15488/54000 (29%)] Loss: -248106.765625\n",
      "Train Epoch: 252 [16896/54000 (31%)] Loss: -225218.250000\n",
      "Train Epoch: 252 [18304/54000 (34%)] Loss: -225698.109375\n",
      "Train Epoch: 252 [19712/54000 (37%)] Loss: -216592.953125\n",
      "Train Epoch: 252 [21120/54000 (39%)] Loss: -217176.468750\n",
      "Train Epoch: 252 [22528/54000 (42%)] Loss: -230424.812500\n",
      "Train Epoch: 252 [23936/54000 (44%)] Loss: -224683.546875\n",
      "Train Epoch: 252 [25344/54000 (47%)] Loss: -223854.468750\n",
      "Train Epoch: 252 [26752/54000 (50%)] Loss: -223870.953125\n",
      "Train Epoch: 252 [28160/54000 (52%)] Loss: -223503.656250\n",
      "Train Epoch: 252 [29568/54000 (55%)] Loss: -229715.140625\n",
      "Train Epoch: 252 [30976/54000 (57%)] Loss: -229297.859375\n",
      "Train Epoch: 252 [32384/54000 (60%)] Loss: -245821.781250\n",
      "Train Epoch: 252 [33792/54000 (63%)] Loss: -224573.890625\n",
      "Train Epoch: 252 [35200/54000 (65%)] Loss: -217733.750000\n",
      "Train Epoch: 252 [36608/54000 (68%)] Loss: -226315.250000\n",
      "Train Epoch: 252 [38016/54000 (70%)] Loss: -215463.468750\n",
      "Train Epoch: 252 [39424/54000 (73%)] Loss: -244989.828125\n",
      "Train Epoch: 252 [40832/54000 (76%)] Loss: -228484.781250\n",
      "Train Epoch: 252 [42240/54000 (78%)] Loss: -227852.890625\n",
      "Train Epoch: 252 [43648/54000 (81%)] Loss: -247183.562500\n",
      "Train Epoch: 252 [45056/54000 (83%)] Loss: -220339.453125\n",
      "Train Epoch: 252 [46464/54000 (86%)] Loss: -221115.296875\n",
      "Train Epoch: 252 [47872/54000 (89%)] Loss: -222867.484375\n",
      "Train Epoch: 252 [49280/54000 (91%)] Loss: -245330.234375\n",
      "Train Epoch: 252 [50688/54000 (94%)] Loss: -229272.031250\n",
      "Train Epoch: 252 [52096/54000 (96%)] Loss: -225850.437500\n",
      "    epoch          : 252\n",
      "    loss           : -224929.75235496412\n",
      "    val_loss       : -229119.09690000952\n",
      "Train Epoch: 253 [0/54000 (0%)] Loss: -246341.546875\n",
      "Train Epoch: 253 [1408/54000 (3%)] Loss: -229320.859375\n",
      "Train Epoch: 253 [2816/54000 (5%)] Loss: -213849.484375\n",
      "Train Epoch: 253 [4224/54000 (8%)] Loss: -218531.718750\n",
      "Train Epoch: 253 [5632/54000 (10%)] Loss: -218995.312500\n",
      "Train Epoch: 253 [7040/54000 (13%)] Loss: -217373.109375\n",
      "Train Epoch: 253 [8448/54000 (16%)] Loss: -216542.359375\n",
      "Train Epoch: 253 [9856/54000 (18%)] Loss: -223344.593750\n",
      "Train Epoch: 253 [11264/54000 (21%)] Loss: -222829.156250\n",
      "Train Epoch: 253 [12672/54000 (23%)] Loss: -247624.937500\n",
      "Train Epoch: 253 [14080/54000 (26%)] Loss: -229840.796875\n",
      "Train Epoch: 253 [15488/54000 (29%)] Loss: -227093.718750\n",
      "Train Epoch: 253 [16896/54000 (31%)] Loss: -219435.500000\n",
      "Train Epoch: 253 [18304/54000 (34%)] Loss: -223162.656250\n",
      "Train Epoch: 253 [19712/54000 (37%)] Loss: -218727.750000\n",
      "Train Epoch: 253 [21120/54000 (39%)] Loss: -211623.906250\n",
      "Train Epoch: 253 [22528/54000 (42%)] Loss: -227685.015625\n",
      "Train Epoch: 253 [23936/54000 (44%)] Loss: -218354.906250\n",
      "Train Epoch: 253 [25344/54000 (47%)] Loss: -243907.421875\n",
      "Train Epoch: 253 [26752/54000 (50%)] Loss: -217697.890625\n",
      "Train Epoch: 253 [28160/54000 (52%)] Loss: -220317.968750\n",
      "Train Epoch: 253 [29568/54000 (55%)] Loss: -224432.875000\n",
      "Train Epoch: 253 [30976/54000 (57%)] Loss: -218992.687500\n",
      "Train Epoch: 253 [32384/54000 (60%)] Loss: -247763.718750\n",
      "Train Epoch: 253 [33792/54000 (63%)] Loss: -229995.156250\n",
      "Train Epoch: 253 [35200/54000 (65%)] Loss: -222277.421875\n",
      "Train Epoch: 253 [36608/54000 (68%)] Loss: -221337.687500\n",
      "Train Epoch: 253 [38016/54000 (70%)] Loss: -228588.593750\n",
      "Train Epoch: 253 [39424/54000 (73%)] Loss: -220800.890625\n",
      "Train Epoch: 253 [40832/54000 (76%)] Loss: -216936.125000\n",
      "Train Epoch: 253 [42240/54000 (78%)] Loss: -217491.437500\n",
      "Train Epoch: 253 [43648/54000 (81%)] Loss: -244862.578125\n",
      "Train Epoch: 253 [45056/54000 (83%)] Loss: -216539.578125\n",
      "Train Epoch: 253 [46464/54000 (86%)] Loss: -226482.328125\n",
      "Train Epoch: 253 [47872/54000 (89%)] Loss: -219061.281250\n",
      "Train Epoch: 253 [49280/54000 (91%)] Loss: -221301.640625\n",
      "Train Epoch: 253 [50688/54000 (94%)] Loss: -248718.656250\n",
      "Train Epoch: 253 [52096/54000 (96%)] Loss: -220042.234375\n",
      "    epoch          : 253\n",
      "    loss           : -225099.15830592104\n",
      "    val_loss       : -228619.42248832888\n",
      "Train Epoch: 254 [0/54000 (0%)] Loss: -244288.906250\n",
      "Train Epoch: 254 [1408/54000 (3%)] Loss: -215197.109375\n",
      "Train Epoch: 254 [2816/54000 (5%)] Loss: -228881.546875\n",
      "Train Epoch: 254 [4224/54000 (8%)] Loss: -219156.515625\n",
      "Train Epoch: 254 [5632/54000 (10%)] Loss: -225831.734375\n",
      "Train Epoch: 254 [7040/54000 (13%)] Loss: -218620.875000\n",
      "Train Epoch: 254 [8448/54000 (16%)] Loss: -217062.703125\n",
      "Train Epoch: 254 [9856/54000 (18%)] Loss: -221633.375000\n",
      "Train Epoch: 254 [11264/54000 (21%)] Loss: -217799.875000\n",
      "Train Epoch: 254 [12672/54000 (23%)] Loss: -231075.953125\n",
      "Train Epoch: 254 [14080/54000 (26%)] Loss: -247643.015625\n",
      "Train Epoch: 254 [15488/54000 (29%)] Loss: -229748.218750\n",
      "Train Epoch: 254 [16896/54000 (31%)] Loss: -227955.312500\n",
      "Train Epoch: 254 [18304/54000 (34%)] Loss: -217878.187500\n",
      "Train Epoch: 254 [19712/54000 (37%)] Loss: -219997.484375\n",
      "Train Epoch: 254 [21120/54000 (39%)] Loss: -247885.281250\n",
      "Train Epoch: 254 [22528/54000 (42%)] Loss: -221971.734375\n",
      "Train Epoch: 254 [23936/54000 (44%)] Loss: -221454.203125\n",
      "Train Epoch: 254 [25344/54000 (47%)] Loss: -217088.328125\n",
      "Train Epoch: 254 [26752/54000 (50%)] Loss: -223799.437500\n",
      "Train Epoch: 254 [28160/54000 (52%)] Loss: -231340.421875\n",
      "Train Epoch: 254 [29568/54000 (55%)] Loss: -248573.828125\n",
      "Train Epoch: 254 [30976/54000 (57%)] Loss: -219942.984375\n",
      "Train Epoch: 254 [32384/54000 (60%)] Loss: -221746.875000\n",
      "Train Epoch: 254 [33792/54000 (63%)] Loss: -229778.031250\n",
      "Train Epoch: 254 [35200/54000 (65%)] Loss: -227702.125000\n",
      "Train Epoch: 254 [36608/54000 (68%)] Loss: -217204.734375\n",
      "Train Epoch: 254 [38016/54000 (70%)] Loss: -222132.031250\n",
      "Train Epoch: 254 [39424/54000 (73%)] Loss: -224542.687500\n",
      "Train Epoch: 254 [40832/54000 (76%)] Loss: -227687.421875\n",
      "Train Epoch: 254 [42240/54000 (78%)] Loss: -221023.734375\n",
      "Train Epoch: 254 [43648/54000 (81%)] Loss: -246811.312500\n",
      "Train Epoch: 254 [45056/54000 (83%)] Loss: -224446.453125\n",
      "Train Epoch: 254 [46464/54000 (86%)] Loss: -230172.187500\n",
      "Train Epoch: 254 [47872/54000 (89%)] Loss: -222954.187500\n",
      "Train Epoch: 254 [49280/54000 (91%)] Loss: -218215.171875\n",
      "Train Epoch: 254 [50688/54000 (94%)] Loss: -216491.953125\n",
      "Train Epoch: 254 [52096/54000 (96%)] Loss: -226539.203125\n",
      "    epoch          : 254\n",
      "    loss           : -225199.6484375\n",
      "    val_loss       : -229088.7549304497\n",
      "Train Epoch: 255 [0/54000 (0%)] Loss: -219537.343750\n",
      "Train Epoch: 255 [1408/54000 (3%)] Loss: -218642.109375\n",
      "Train Epoch: 255 [2816/54000 (5%)] Loss: -221049.984375\n",
      "Train Epoch: 255 [4224/54000 (8%)] Loss: -228184.968750\n",
      "Train Epoch: 255 [5632/54000 (10%)] Loss: -217408.546875\n",
      "Train Epoch: 255 [7040/54000 (13%)] Loss: -230868.515625\n",
      "Train Epoch: 255 [8448/54000 (16%)] Loss: -222729.500000\n",
      "Train Epoch: 255 [9856/54000 (18%)] Loss: -214151.500000\n",
      "Train Epoch: 255 [11264/54000 (21%)] Loss: -218034.921875\n",
      "Train Epoch: 255 [12672/54000 (23%)] Loss: -229021.406250\n",
      "Train Epoch: 255 [14080/54000 (26%)] Loss: -246754.562500\n",
      "Train Epoch: 255 [15488/54000 (29%)] Loss: -225156.968750\n",
      "Train Epoch: 255 [16896/54000 (31%)] Loss: -227757.218750\n",
      "Train Epoch: 255 [18304/54000 (34%)] Loss: -221229.625000\n",
      "Train Epoch: 255 [19712/54000 (37%)] Loss: -217237.796875\n",
      "Train Epoch: 255 [21120/54000 (39%)] Loss: -220121.343750\n",
      "Train Epoch: 255 [22528/54000 (42%)] Loss: -225717.093750\n",
      "Train Epoch: 255 [23936/54000 (44%)] Loss: -219518.812500\n",
      "Train Epoch: 255 [25344/54000 (47%)] Loss: -218931.078125\n",
      "Train Epoch: 255 [26752/54000 (50%)] Loss: -221918.437500\n",
      "Train Epoch: 255 [28160/54000 (52%)] Loss: -246693.546875\n",
      "Train Epoch: 255 [29568/54000 (55%)] Loss: -221663.000000\n",
      "Train Epoch: 255 [30976/54000 (57%)] Loss: -228699.000000\n",
      "Train Epoch: 255 [32384/54000 (60%)] Loss: -221025.484375\n",
      "Train Epoch: 255 [33792/54000 (63%)] Loss: -225445.750000\n",
      "Train Epoch: 255 [35200/54000 (65%)] Loss: -216514.718750\n",
      "Train Epoch: 255 [36608/54000 (68%)] Loss: -215245.218750\n",
      "Train Epoch: 255 [38016/54000 (70%)] Loss: -249497.015625\n",
      "Train Epoch: 255 [39424/54000 (73%)] Loss: -222306.703125\n",
      "Train Epoch: 255 [40832/54000 (76%)] Loss: -224862.937500\n",
      "Train Epoch: 255 [42240/54000 (78%)] Loss: -215477.343750\n",
      "Train Epoch: 255 [43648/54000 (81%)] Loss: -227407.437500\n",
      "Train Epoch: 255 [45056/54000 (83%)] Loss: -227155.156250\n",
      "Train Epoch: 255 [46464/54000 (86%)] Loss: -221149.750000\n",
      "Train Epoch: 255 [47872/54000 (89%)] Loss: -222242.156250\n",
      "Train Epoch: 255 [49280/54000 (91%)] Loss: -215618.218750\n",
      "Train Epoch: 255 [50688/54000 (94%)] Loss: -217544.250000\n",
      "Train Epoch: 255 [52096/54000 (96%)] Loss: -228998.203125\n",
      "    epoch          : 255\n",
      "    loss           : -225001.34255382774\n",
      "    val_loss       : -229347.06874642722\n",
      "Train Epoch: 256 [0/54000 (0%)] Loss: -228151.312500\n",
      "Train Epoch: 256 [1408/54000 (3%)] Loss: -222998.890625\n",
      "Train Epoch: 256 [2816/54000 (5%)] Loss: -248121.140625\n",
      "Train Epoch: 256 [4224/54000 (8%)] Loss: -220304.750000\n",
      "Train Epoch: 256 [5632/54000 (10%)] Loss: -216233.078125\n",
      "Train Epoch: 256 [7040/54000 (13%)] Loss: -226165.140625\n",
      "Train Epoch: 256 [8448/54000 (16%)] Loss: -224021.234375\n",
      "Train Epoch: 256 [9856/54000 (18%)] Loss: -246286.156250\n",
      "Train Epoch: 256 [11264/54000 (21%)] Loss: -224003.968750\n",
      "Train Epoch: 256 [12672/54000 (23%)] Loss: -228877.468750\n",
      "Train Epoch: 256 [14080/54000 (26%)] Loss: -221732.031250\n",
      "Train Epoch: 256 [15488/54000 (29%)] Loss: -219955.437500\n",
      "Train Epoch: 256 [16896/54000 (31%)] Loss: -247053.812500\n",
      "Train Epoch: 256 [18304/54000 (34%)] Loss: -228974.875000\n",
      "Train Epoch: 256 [19712/54000 (37%)] Loss: -230128.734375\n",
      "Train Epoch: 256 [21120/54000 (39%)] Loss: -214436.843750\n",
      "Train Epoch: 256 [22528/54000 (42%)] Loss: -223208.703125\n",
      "Train Epoch: 256 [23936/54000 (44%)] Loss: -228413.656250\n",
      "Train Epoch: 256 [25344/54000 (47%)] Loss: -222799.140625\n",
      "Train Epoch: 256 [26752/54000 (50%)] Loss: -210256.484375\n",
      "Train Epoch: 256 [28160/54000 (52%)] Loss: -224082.812500\n",
      "Train Epoch: 256 [29568/54000 (55%)] Loss: -248489.656250\n",
      "Train Epoch: 256 [30976/54000 (57%)] Loss: -223613.046875\n",
      "Train Epoch: 256 [32384/54000 (60%)] Loss: -218180.437500\n",
      "Train Epoch: 256 [33792/54000 (63%)] Loss: -219187.359375\n",
      "Train Epoch: 256 [35200/54000 (65%)] Loss: -247717.031250\n",
      "Train Epoch: 256 [36608/54000 (68%)] Loss: -230746.125000\n",
      "Train Epoch: 256 [38016/54000 (70%)] Loss: -229593.593750\n",
      "Train Epoch: 256 [39424/54000 (73%)] Loss: -220289.828125\n",
      "Train Epoch: 256 [40832/54000 (76%)] Loss: -217833.000000\n",
      "Train Epoch: 256 [42240/54000 (78%)] Loss: -228377.687500\n",
      "Train Epoch: 256 [43648/54000 (81%)] Loss: -217679.078125\n",
      "Train Epoch: 256 [45056/54000 (83%)] Loss: -215851.375000\n",
      "Train Epoch: 256 [46464/54000 (86%)] Loss: -221654.437500\n",
      "Train Epoch: 256 [47872/54000 (89%)] Loss: -221375.703125\n",
      "Train Epoch: 256 [49280/54000 (91%)] Loss: -220991.562500\n",
      "Train Epoch: 256 [50688/54000 (94%)] Loss: -230216.250000\n",
      "Train Epoch: 256 [52096/54000 (96%)] Loss: -218043.343750\n",
      "    epoch          : 256\n",
      "    loss           : -225209.08575059808\n",
      "    val_loss       : -229423.41869521723\n",
      "Train Epoch: 257 [0/54000 (0%)] Loss: -247678.765625\n",
      "Train Epoch: 257 [1408/54000 (3%)] Loss: -227183.375000\n",
      "Train Epoch: 257 [2816/54000 (5%)] Loss: -225681.937500\n",
      "Train Epoch: 257 [4224/54000 (8%)] Loss: -215731.203125\n",
      "Train Epoch: 257 [5632/54000 (10%)] Loss: -219555.437500\n",
      "Train Epoch: 257 [7040/54000 (13%)] Loss: -212951.156250\n",
      "Train Epoch: 257 [8448/54000 (16%)] Loss: -215592.640625\n",
      "Train Epoch: 257 [9856/54000 (18%)] Loss: -246062.671875\n",
      "Train Epoch: 257 [11264/54000 (21%)] Loss: -218981.375000\n",
      "Train Epoch: 257 [12672/54000 (23%)] Loss: -229648.781250\n",
      "Train Epoch: 257 [14080/54000 (26%)] Loss: -222977.031250\n",
      "Train Epoch: 257 [15488/54000 (29%)] Loss: -216900.937500\n",
      "Train Epoch: 257 [16896/54000 (31%)] Loss: -249013.062500\n",
      "Train Epoch: 257 [18304/54000 (34%)] Loss: -222662.250000\n",
      "Train Epoch: 257 [19712/54000 (37%)] Loss: -225635.218750\n",
      "Train Epoch: 257 [21120/54000 (39%)] Loss: -248853.156250\n",
      "Train Epoch: 257 [22528/54000 (42%)] Loss: -214285.500000\n",
      "Train Epoch: 257 [23936/54000 (44%)] Loss: -215766.656250\n",
      "Train Epoch: 257 [25344/54000 (47%)] Loss: -222817.281250\n",
      "Train Epoch: 257 [26752/54000 (50%)] Loss: -248257.000000\n",
      "Train Epoch: 257 [28160/54000 (52%)] Loss: -222987.234375\n",
      "Train Epoch: 257 [29568/54000 (55%)] Loss: -222320.718750\n",
      "Train Epoch: 257 [30976/54000 (57%)] Loss: -229722.468750\n",
      "Train Epoch: 257 [32384/54000 (60%)] Loss: -214423.843750\n",
      "Train Epoch: 257 [33792/54000 (63%)] Loss: -229057.531250\n",
      "Train Epoch: 257 [35200/54000 (65%)] Loss: -215959.671875\n",
      "Train Epoch: 257 [36608/54000 (68%)] Loss: -214834.609375\n",
      "Train Epoch: 257 [38016/54000 (70%)] Loss: -229042.812500\n",
      "Train Epoch: 257 [39424/54000 (73%)] Loss: -219642.890625\n",
      "Train Epoch: 257 [40832/54000 (76%)] Loss: -249035.765625\n",
      "Train Epoch: 257 [42240/54000 (78%)] Loss: -220073.687500\n",
      "Train Epoch: 257 [43648/54000 (81%)] Loss: -219243.187500\n",
      "Train Epoch: 257 [45056/54000 (83%)] Loss: -224445.609375\n",
      "Train Epoch: 257 [46464/54000 (86%)] Loss: -220774.734375\n",
      "Train Epoch: 257 [47872/54000 (89%)] Loss: -223880.781250\n",
      "Train Epoch: 257 [49280/54000 (91%)] Loss: -218775.687500\n",
      "Train Epoch: 257 [50688/54000 (94%)] Loss: -222819.171875\n",
      "Train Epoch: 257 [52096/54000 (96%)] Loss: -248844.406250\n",
      "    epoch          : 257\n",
      "    loss           : -225244.27938098085\n",
      "    val_loss       : -229347.73498237424\n",
      "Train Epoch: 258 [0/54000 (0%)] Loss: -229527.781250\n",
      "Train Epoch: 258 [1408/54000 (3%)] Loss: -245228.890625\n",
      "Train Epoch: 258 [2816/54000 (5%)] Loss: -221308.468750\n",
      "Train Epoch: 258 [4224/54000 (8%)] Loss: -219400.687500\n",
      "Train Epoch: 258 [5632/54000 (10%)] Loss: -231153.609375\n",
      "Train Epoch: 258 [7040/54000 (13%)] Loss: -219189.656250\n",
      "Train Epoch: 258 [8448/54000 (16%)] Loss: -221119.734375\n",
      "Train Epoch: 258 [9856/54000 (18%)] Loss: -244288.906250\n",
      "Train Epoch: 258 [11264/54000 (21%)] Loss: -215400.812500\n",
      "Train Epoch: 258 [12672/54000 (23%)] Loss: -229166.906250\n",
      "Train Epoch: 258 [14080/54000 (26%)] Loss: -213169.265625\n",
      "Train Epoch: 258 [15488/54000 (29%)] Loss: -227632.468750\n",
      "Train Epoch: 258 [16896/54000 (31%)] Loss: -228463.796875\n",
      "Train Epoch: 258 [18304/54000 (34%)] Loss: -218589.578125\n",
      "Train Epoch: 258 [19712/54000 (37%)] Loss: -219290.765625\n",
      "Train Epoch: 258 [21120/54000 (39%)] Loss: -228823.750000\n",
      "Train Epoch: 258 [22528/54000 (42%)] Loss: -229432.218750\n",
      "Train Epoch: 258 [23936/54000 (44%)] Loss: -228106.843750\n",
      "Train Epoch: 258 [25344/54000 (47%)] Loss: -230910.687500\n",
      "Train Epoch: 258 [26752/54000 (50%)] Loss: -219004.531250\n",
      "Train Epoch: 258 [28160/54000 (52%)] Loss: -219691.515625\n",
      "Train Epoch: 258 [29568/54000 (55%)] Loss: -216084.250000\n",
      "Train Epoch: 258 [30976/54000 (57%)] Loss: -227538.062500\n",
      "Train Epoch: 258 [32384/54000 (60%)] Loss: -216700.703125\n",
      "Train Epoch: 258 [33792/54000 (63%)] Loss: -222120.328125\n",
      "Train Epoch: 258 [35200/54000 (65%)] Loss: -223531.968750\n",
      "Train Epoch: 258 [36608/54000 (68%)] Loss: -221279.140625\n",
      "Train Epoch: 258 [38016/54000 (70%)] Loss: -230300.781250\n",
      "Train Epoch: 258 [39424/54000 (73%)] Loss: -221985.828125\n",
      "Train Epoch: 258 [40832/54000 (76%)] Loss: -221041.375000\n",
      "Train Epoch: 258 [42240/54000 (78%)] Loss: -228371.234375\n",
      "Train Epoch: 258 [43648/54000 (81%)] Loss: -245571.968750\n",
      "Train Epoch: 258 [45056/54000 (83%)] Loss: -224422.593750\n",
      "Train Epoch: 258 [46464/54000 (86%)] Loss: -215108.843750\n",
      "Train Epoch: 258 [47872/54000 (89%)] Loss: -223866.203125\n",
      "Train Epoch: 258 [49280/54000 (91%)] Loss: -248753.171875\n",
      "Train Epoch: 258 [50688/54000 (94%)] Loss: -222278.812500\n",
      "Train Epoch: 258 [52096/54000 (96%)] Loss: -217808.281250\n",
      "    epoch          : 258\n",
      "    loss           : -225218.42348235645\n",
      "    val_loss       : -229482.0916896913\n",
      "Train Epoch: 259 [0/54000 (0%)] Loss: -228046.281250\n",
      "Train Epoch: 259 [1408/54000 (3%)] Loss: -221096.171875\n",
      "Train Epoch: 259 [2816/54000 (5%)] Loss: -224598.250000\n",
      "Train Epoch: 259 [4224/54000 (8%)] Loss: -218041.656250\n",
      "Train Epoch: 259 [5632/54000 (10%)] Loss: -230196.015625\n",
      "Train Epoch: 259 [7040/54000 (13%)] Loss: -230881.531250\n",
      "Train Epoch: 259 [8448/54000 (16%)] Loss: -220590.484375\n",
      "Train Epoch: 259 [9856/54000 (18%)] Loss: -219317.078125\n",
      "Train Epoch: 259 [11264/54000 (21%)] Loss: -219317.828125\n",
      "Train Epoch: 259 [12672/54000 (23%)] Loss: -217798.484375\n",
      "Train Epoch: 259 [14080/54000 (26%)] Loss: -229357.640625\n",
      "Train Epoch: 259 [15488/54000 (29%)] Loss: -248173.343750\n",
      "Train Epoch: 259 [16896/54000 (31%)] Loss: -220117.234375\n",
      "Train Epoch: 259 [18304/54000 (34%)] Loss: -221913.812500\n",
      "Train Epoch: 259 [19712/54000 (37%)] Loss: -220661.703125\n",
      "Train Epoch: 259 [21120/54000 (39%)] Loss: -245406.406250\n",
      "Train Epoch: 259 [22528/54000 (42%)] Loss: -222274.531250\n",
      "Train Epoch: 259 [23936/54000 (44%)] Loss: -229557.140625\n",
      "Train Epoch: 259 [25344/54000 (47%)] Loss: -217668.000000\n",
      "Train Epoch: 259 [26752/54000 (50%)] Loss: -219717.343750\n",
      "Train Epoch: 259 [28160/54000 (52%)] Loss: -226565.187500\n",
      "Train Epoch: 259 [29568/54000 (55%)] Loss: -222571.234375\n",
      "Train Epoch: 259 [30976/54000 (57%)] Loss: -218916.765625\n",
      "Train Epoch: 259 [32384/54000 (60%)] Loss: -246375.859375\n",
      "Train Epoch: 259 [33792/54000 (63%)] Loss: -215905.625000\n",
      "Train Epoch: 259 [35200/54000 (65%)] Loss: -229083.640625\n",
      "Train Epoch: 259 [36608/54000 (68%)] Loss: -214647.640625\n",
      "Train Epoch: 259 [38016/54000 (70%)] Loss: -230102.750000\n",
      "Train Epoch: 259 [39424/54000 (73%)] Loss: -215513.375000\n",
      "Train Epoch: 259 [40832/54000 (76%)] Loss: -215320.812500\n",
      "Train Epoch: 259 [42240/54000 (78%)] Loss: -228122.718750\n",
      "Train Epoch: 259 [43648/54000 (81%)] Loss: -245650.406250\n",
      "Train Epoch: 259 [45056/54000 (83%)] Loss: -224667.750000\n",
      "Train Epoch: 259 [46464/54000 (86%)] Loss: -227370.281250\n",
      "Train Epoch: 259 [47872/54000 (89%)] Loss: -218446.218750\n",
      "Train Epoch: 259 [49280/54000 (91%)] Loss: -246298.734375\n",
      "Train Epoch: 259 [50688/54000 (94%)] Loss: -225300.093750\n",
      "Train Epoch: 259 [52096/54000 (96%)] Loss: -228020.812500\n",
      "    epoch          : 259\n",
      "    loss           : -225345.1957610646\n",
      "    val_loss       : -229339.3809665587\n",
      "Train Epoch: 260 [0/54000 (0%)] Loss: -222412.734375\n",
      "Train Epoch: 260 [1408/54000 (3%)] Loss: -216384.203125\n",
      "Train Epoch: 260 [2816/54000 (5%)] Loss: -229796.687500\n",
      "Train Epoch: 260 [4224/54000 (8%)] Loss: -220748.468750\n",
      "Train Epoch: 260 [5632/54000 (10%)] Loss: -226087.093750\n",
      "Train Epoch: 260 [7040/54000 (13%)] Loss: -226094.218750\n",
      "Train Epoch: 260 [8448/54000 (16%)] Loss: -224773.640625\n",
      "Train Epoch: 260 [9856/54000 (18%)] Loss: -222678.921875\n",
      "Train Epoch: 260 [11264/54000 (21%)] Loss: -229690.968750\n",
      "Train Epoch: 260 [12672/54000 (23%)] Loss: -219104.968750\n",
      "Train Epoch: 260 [14080/54000 (26%)] Loss: -229121.937500\n",
      "Train Epoch: 260 [15488/54000 (29%)] Loss: -221124.421875\n",
      "Train Epoch: 260 [16896/54000 (31%)] Loss: -220072.671875\n",
      "Train Epoch: 260 [18304/54000 (34%)] Loss: -222158.500000\n",
      "Train Epoch: 260 [19712/54000 (37%)] Loss: -219803.500000\n",
      "Train Epoch: 260 [21120/54000 (39%)] Loss: -221708.312500\n",
      "Train Epoch: 260 [22528/54000 (42%)] Loss: -229470.968750\n",
      "Train Epoch: 260 [23936/54000 (44%)] Loss: -230007.875000\n",
      "Train Epoch: 260 [25344/54000 (47%)] Loss: -221566.578125\n",
      "Train Epoch: 260 [26752/54000 (50%)] Loss: -229423.750000\n",
      "Train Epoch: 260 [28160/54000 (52%)] Loss: -217458.406250\n",
      "Train Epoch: 260 [29568/54000 (55%)] Loss: -218645.421875\n",
      "Train Epoch: 260 [30976/54000 (57%)] Loss: -221858.156250\n",
      "Train Epoch: 260 [32384/54000 (60%)] Loss: -216284.656250\n",
      "Train Epoch: 260 [33792/54000 (63%)] Loss: -218796.968750\n",
      "Train Epoch: 260 [35200/54000 (65%)] Loss: -219246.781250\n",
      "Train Epoch: 260 [36608/54000 (68%)] Loss: -248202.546875\n",
      "Train Epoch: 260 [38016/54000 (70%)] Loss: -230048.406250\n",
      "Train Epoch: 260 [39424/54000 (73%)] Loss: -217762.375000\n",
      "Train Epoch: 260 [40832/54000 (76%)] Loss: -227230.625000\n",
      "Train Epoch: 260 [42240/54000 (78%)] Loss: -247511.531250\n",
      "Train Epoch: 260 [43648/54000 (81%)] Loss: -226990.218750\n",
      "Train Epoch: 260 [45056/54000 (83%)] Loss: -228625.328125\n",
      "Train Epoch: 260 [46464/54000 (86%)] Loss: -221410.593750\n",
      "Train Epoch: 260 [47872/54000 (89%)] Loss: -225211.312500\n",
      "Train Epoch: 260 [49280/54000 (91%)] Loss: -217070.078125\n",
      "Train Epoch: 260 [50688/54000 (94%)] Loss: -244966.031250\n",
      "Train Epoch: 260 [52096/54000 (96%)] Loss: -226614.390625\n",
      "    epoch          : 260\n",
      "    loss           : -225269.08163875598\n",
      "    val_loss       : -228958.8900712176\n",
      "Train Epoch: 261 [0/54000 (0%)] Loss: -217728.109375\n",
      "Train Epoch: 261 [1408/54000 (3%)] Loss: -219635.250000\n",
      "Train Epoch: 261 [2816/54000 (5%)] Loss: -228618.484375\n",
      "Train Epoch: 261 [4224/54000 (8%)] Loss: -221896.843750\n",
      "Train Epoch: 261 [5632/54000 (10%)] Loss: -216086.593750\n",
      "Train Epoch: 261 [7040/54000 (13%)] Loss: -218641.171875\n",
      "Train Epoch: 261 [8448/54000 (16%)] Loss: -230619.000000\n",
      "Train Epoch: 261 [9856/54000 (18%)] Loss: -218127.750000\n",
      "Train Epoch: 261 [11264/54000 (21%)] Loss: -219174.437500\n",
      "Train Epoch: 261 [12672/54000 (23%)] Loss: -226161.890625\n",
      "Train Epoch: 261 [14080/54000 (26%)] Loss: -230473.812500\n",
      "Train Epoch: 261 [15488/54000 (29%)] Loss: -246080.546875\n",
      "Train Epoch: 261 [16896/54000 (31%)] Loss: -222949.156250\n",
      "Train Epoch: 261 [18304/54000 (34%)] Loss: -219948.828125\n",
      "Train Epoch: 261 [19712/54000 (37%)] Loss: -223769.828125\n",
      "Train Epoch: 261 [21120/54000 (39%)] Loss: -222195.718750\n",
      "Train Epoch: 261 [22528/54000 (42%)] Loss: -224364.062500\n",
      "Train Epoch: 261 [23936/54000 (44%)] Loss: -226219.093750\n",
      "Train Epoch: 261 [25344/54000 (47%)] Loss: -223556.531250\n",
      "Train Epoch: 261 [26752/54000 (50%)] Loss: -222306.359375\n",
      "Train Epoch: 261 [28160/54000 (52%)] Loss: -248035.625000\n",
      "Train Epoch: 261 [29568/54000 (55%)] Loss: -217238.968750\n",
      "Train Epoch: 261 [30976/54000 (57%)] Loss: -218300.031250\n",
      "Train Epoch: 261 [32384/54000 (60%)] Loss: -217601.093750\n",
      "Train Epoch: 261 [33792/54000 (63%)] Loss: -231370.812500\n",
      "Train Epoch: 261 [35200/54000 (65%)] Loss: -227153.484375\n",
      "Train Epoch: 261 [36608/54000 (68%)] Loss: -214608.890625\n",
      "Train Epoch: 261 [38016/54000 (70%)] Loss: -230338.406250\n",
      "Train Epoch: 261 [39424/54000 (73%)] Loss: -221406.140625\n",
      "Train Epoch: 261 [40832/54000 (76%)] Loss: -247026.562500\n",
      "Train Epoch: 261 [42240/54000 (78%)] Loss: -218067.312500\n",
      "Train Epoch: 261 [43648/54000 (81%)] Loss: -227204.343750\n",
      "Train Epoch: 261 [45056/54000 (83%)] Loss: -212750.609375\n",
      "Train Epoch: 261 [46464/54000 (86%)] Loss: -220321.125000\n",
      "Train Epoch: 261 [47872/54000 (89%)] Loss: -216593.234375\n",
      "Train Epoch: 261 [49280/54000 (91%)] Loss: -216226.687500\n",
      "Train Epoch: 261 [50688/54000 (94%)] Loss: -226527.375000\n",
      "Train Epoch: 261 [52096/54000 (96%)] Loss: -228966.906250\n",
      "    epoch          : 261\n",
      "    loss           : -225479.8163875598\n",
      "    val_loss       : -229225.42797256098\n",
      "Train Epoch: 262 [0/54000 (0%)] Loss: -247826.937500\n",
      "Train Epoch: 262 [1408/54000 (3%)] Loss: -229173.640625\n",
      "Train Epoch: 262 [2816/54000 (5%)] Loss: -218314.125000\n",
      "Train Epoch: 262 [4224/54000 (8%)] Loss: -218690.578125\n",
      "Train Epoch: 262 [5632/54000 (10%)] Loss: -221189.500000\n",
      "Train Epoch: 262 [7040/54000 (13%)] Loss: -215766.500000\n",
      "Train Epoch: 262 [8448/54000 (16%)] Loss: -218873.687500\n",
      "Train Epoch: 262 [9856/54000 (18%)] Loss: -227585.671875\n",
      "Train Epoch: 262 [11264/54000 (21%)] Loss: -219956.593750\n",
      "Train Epoch: 262 [12672/54000 (23%)] Loss: -224159.453125\n",
      "Train Epoch: 262 [14080/54000 (26%)] Loss: -218963.687500\n",
      "Train Epoch: 262 [15488/54000 (29%)] Loss: -247043.937500\n",
      "Train Epoch: 262 [16896/54000 (31%)] Loss: -215867.796875\n",
      "Train Epoch: 262 [18304/54000 (34%)] Loss: -219858.437500\n",
      "Train Epoch: 262 [19712/54000 (37%)] Loss: -222665.421875\n",
      "Train Epoch: 262 [21120/54000 (39%)] Loss: -248266.125000\n",
      "Train Epoch: 262 [22528/54000 (42%)] Loss: -228301.125000\n",
      "Train Epoch: 262 [23936/54000 (44%)] Loss: -230720.937500\n",
      "Train Epoch: 262 [25344/54000 (47%)] Loss: -217020.125000\n",
      "Train Epoch: 262 [26752/54000 (50%)] Loss: -246490.406250\n",
      "Train Epoch: 262 [28160/54000 (52%)] Loss: -222161.734375\n",
      "Train Epoch: 262 [29568/54000 (55%)] Loss: -216389.000000\n",
      "Train Epoch: 262 [30976/54000 (57%)] Loss: -226541.031250\n",
      "Train Epoch: 262 [32384/54000 (60%)] Loss: -224483.593750\n",
      "Train Epoch: 262 [33792/54000 (63%)] Loss: -230299.500000\n",
      "Train Epoch: 262 [35200/54000 (65%)] Loss: -228886.859375\n",
      "Train Epoch: 262 [36608/54000 (68%)] Loss: -217788.765625\n",
      "Train Epoch: 262 [38016/54000 (70%)] Loss: -218787.015625\n",
      "Train Epoch: 262 [39424/54000 (73%)] Loss: -222311.359375\n",
      "Train Epoch: 262 [40832/54000 (76%)] Loss: -229226.062500\n",
      "Train Epoch: 262 [42240/54000 (78%)] Loss: -228558.781250\n",
      "Train Epoch: 262 [43648/54000 (81%)] Loss: -219565.421875\n",
      "Train Epoch: 262 [45056/54000 (83%)] Loss: -218739.781250\n",
      "Train Epoch: 262 [46464/54000 (86%)] Loss: -218695.593750\n",
      "Train Epoch: 262 [47872/54000 (89%)] Loss: -219401.734375\n",
      "Train Epoch: 262 [49280/54000 (91%)] Loss: -223331.015625\n",
      "Train Epoch: 262 [50688/54000 (94%)] Loss: -228894.656250\n",
      "Train Epoch: 262 [52096/54000 (96%)] Loss: -230988.187500\n",
      "    epoch          : 262\n",
      "    loss           : -225566.96755382774\n",
      "    val_loss       : -228834.27211556784\n",
      "Train Epoch: 263 [0/54000 (0%)] Loss: -226627.171875\n",
      "Train Epoch: 263 [1408/54000 (3%)] Loss: -226456.796875\n",
      "Train Epoch: 263 [2816/54000 (5%)] Loss: -223724.500000\n",
      "Train Epoch: 263 [4224/54000 (8%)] Loss: -247421.593750\n",
      "Train Epoch: 263 [5632/54000 (10%)] Loss: -219666.937500\n",
      "Train Epoch: 263 [7040/54000 (13%)] Loss: -222946.937500\n",
      "Train Epoch: 263 [8448/54000 (16%)] Loss: -213174.531250\n",
      "Train Epoch: 263 [9856/54000 (18%)] Loss: -248720.500000\n",
      "Train Epoch: 263 [11264/54000 (21%)] Loss: -226198.296875\n",
      "Train Epoch: 263 [12672/54000 (23%)] Loss: -228311.437500\n",
      "Train Epoch: 263 [14080/54000 (26%)] Loss: -229611.890625\n",
      "Train Epoch: 263 [15488/54000 (29%)] Loss: -225720.171875\n",
      "Train Epoch: 263 [16896/54000 (31%)] Loss: -227703.218750\n",
      "Train Epoch: 263 [18304/54000 (34%)] Loss: -226814.078125\n",
      "Train Epoch: 263 [19712/54000 (37%)] Loss: -215594.140625\n",
      "Train Epoch: 263 [21120/54000 (39%)] Loss: -219211.093750\n",
      "Train Epoch: 263 [22528/54000 (42%)] Loss: -219712.671875\n",
      "Train Epoch: 263 [23936/54000 (44%)] Loss: -219039.843750\n",
      "Train Epoch: 263 [25344/54000 (47%)] Loss: -218256.156250\n",
      "Train Epoch: 263 [26752/54000 (50%)] Loss: -228236.703125\n",
      "Train Epoch: 263 [28160/54000 (52%)] Loss: -217508.421875\n",
      "Train Epoch: 263 [29568/54000 (55%)] Loss: -224726.031250\n",
      "Train Epoch: 263 [30976/54000 (57%)] Loss: -227051.843750\n",
      "Train Epoch: 263 [32384/54000 (60%)] Loss: -247705.640625\n",
      "Train Epoch: 263 [33792/54000 (63%)] Loss: -222790.687500\n",
      "Train Epoch: 263 [35200/54000 (65%)] Loss: -218409.281250\n",
      "Train Epoch: 263 [36608/54000 (68%)] Loss: -225041.562500\n",
      "Train Epoch: 263 [38016/54000 (70%)] Loss: -225796.843750\n",
      "Train Epoch: 263 [39424/54000 (73%)] Loss: -229760.468750\n",
      "Train Epoch: 263 [40832/54000 (76%)] Loss: -229434.062500\n",
      "Train Epoch: 263 [42240/54000 (78%)] Loss: -230331.671875\n",
      "Train Epoch: 263 [43648/54000 (81%)] Loss: -219105.281250\n",
      "Train Epoch: 263 [45056/54000 (83%)] Loss: -240470.093750\n",
      "Train Epoch: 263 [46464/54000 (86%)] Loss: -216292.328125\n",
      "Train Epoch: 263 [47872/54000 (89%)] Loss: -220329.687500\n",
      "Train Epoch: 263 [49280/54000 (91%)] Loss: -218459.968750\n",
      "Train Epoch: 263 [50688/54000 (94%)] Loss: -214035.453125\n",
      "Train Epoch: 263 [52096/54000 (96%)] Loss: -229340.750000\n",
      "    epoch          : 263\n",
      "    loss           : -225399.12937350478\n",
      "    val_loss       : -229126.64024390245\n",
      "Train Epoch: 264 [0/54000 (0%)] Loss: -215461.812500\n",
      "Train Epoch: 264 [1408/54000 (3%)] Loss: -223731.828125\n",
      "Train Epoch: 264 [2816/54000 (5%)] Loss: -229055.156250\n",
      "Train Epoch: 264 [4224/54000 (8%)] Loss: -219464.125000\n",
      "Train Epoch: 264 [5632/54000 (10%)] Loss: -219859.406250\n",
      "Train Epoch: 264 [7040/54000 (13%)] Loss: -227437.531250\n",
      "Train Epoch: 264 [8448/54000 (16%)] Loss: -225121.328125\n",
      "Train Epoch: 264 [9856/54000 (18%)] Loss: -219029.515625\n",
      "Train Epoch: 264 [11264/54000 (21%)] Loss: -216728.406250\n",
      "Train Epoch: 264 [12672/54000 (23%)] Loss: -222924.171875\n",
      "Train Epoch: 264 [14080/54000 (26%)] Loss: -220845.750000\n",
      "Train Epoch: 264 [15488/54000 (29%)] Loss: -229595.765625\n",
      "Train Epoch: 264 [16896/54000 (31%)] Loss: -228931.171875\n",
      "Train Epoch: 264 [18304/54000 (34%)] Loss: -219457.750000\n",
      "Train Epoch: 264 [19712/54000 (37%)] Loss: -214218.937500\n",
      "Train Epoch: 264 [21120/54000 (39%)] Loss: -219462.515625\n",
      "Train Epoch: 264 [22528/54000 (42%)] Loss: -221813.609375\n",
      "Train Epoch: 264 [23936/54000 (44%)] Loss: -223609.046875\n",
      "Train Epoch: 264 [25344/54000 (47%)] Loss: -249044.750000\n",
      "Train Epoch: 264 [26752/54000 (50%)] Loss: -248254.125000\n",
      "Train Epoch: 264 [28160/54000 (52%)] Loss: -226566.984375\n",
      "Train Epoch: 264 [29568/54000 (55%)] Loss: -220597.062500\n",
      "Train Epoch: 264 [30976/54000 (57%)] Loss: -249249.921875\n",
      "Train Epoch: 264 [32384/54000 (60%)] Loss: -218734.234375\n",
      "Train Epoch: 264 [33792/54000 (63%)] Loss: -231694.156250\n",
      "Train Epoch: 264 [35200/54000 (65%)] Loss: -229913.500000\n",
      "Train Epoch: 264 [36608/54000 (68%)] Loss: -229435.640625\n",
      "Train Epoch: 264 [38016/54000 (70%)] Loss: -216951.187500\n",
      "Train Epoch: 264 [39424/54000 (73%)] Loss: -214837.046875\n",
      "Train Epoch: 264 [40832/54000 (76%)] Loss: -221217.453125\n",
      "Train Epoch: 264 [42240/54000 (78%)] Loss: -219782.375000\n",
      "Train Epoch: 264 [43648/54000 (81%)] Loss: -224205.937500\n",
      "Train Epoch: 264 [45056/54000 (83%)] Loss: -217741.125000\n",
      "Train Epoch: 264 [46464/54000 (86%)] Loss: -221899.640625\n",
      "Train Epoch: 264 [47872/54000 (89%)] Loss: -222672.687500\n",
      "Train Epoch: 264 [49280/54000 (91%)] Loss: -227178.250000\n",
      "Train Epoch: 264 [50688/54000 (94%)] Loss: -249443.312500\n",
      "Train Epoch: 264 [52096/54000 (96%)] Loss: -229242.125000\n",
      "    epoch          : 264\n",
      "    loss           : -225386.78487589714\n",
      "    val_loss       : -229698.4795040968\n",
      "Train Epoch: 265 [0/54000 (0%)] Loss: -230641.062500\n",
      "Train Epoch: 265 [1408/54000 (3%)] Loss: -247602.125000\n",
      "Train Epoch: 265 [2816/54000 (5%)] Loss: -231351.875000\n",
      "Train Epoch: 265 [4224/54000 (8%)] Loss: -219824.000000\n",
      "Train Epoch: 265 [5632/54000 (10%)] Loss: -219709.703125\n",
      "Train Epoch: 265 [7040/54000 (13%)] Loss: -216361.078125\n",
      "Train Epoch: 265 [8448/54000 (16%)] Loss: -248215.593750\n",
      "Train Epoch: 265 [9856/54000 (18%)] Loss: -224340.593750\n",
      "Train Epoch: 265 [11264/54000 (21%)] Loss: -226540.796875\n",
      "Train Epoch: 265 [12672/54000 (23%)] Loss: -230814.406250\n",
      "Train Epoch: 265 [14080/54000 (26%)] Loss: -230221.562500\n",
      "Train Epoch: 265 [15488/54000 (29%)] Loss: -226539.187500\n",
      "Train Epoch: 265 [16896/54000 (31%)] Loss: -226917.109375\n",
      "Train Epoch: 265 [18304/54000 (34%)] Loss: -214425.203125\n",
      "Train Epoch: 265 [19712/54000 (37%)] Loss: -228636.359375\n",
      "Train Epoch: 265 [21120/54000 (39%)] Loss: -227571.453125\n",
      "Train Epoch: 265 [22528/54000 (42%)] Loss: -219486.468750\n",
      "Train Epoch: 265 [23936/54000 (44%)] Loss: -213368.234375\n",
      "Train Epoch: 265 [25344/54000 (47%)] Loss: -221094.640625\n",
      "Train Epoch: 265 [26752/54000 (50%)] Loss: -246278.093750\n",
      "Train Epoch: 265 [28160/54000 (52%)] Loss: -248962.093750\n",
      "Train Epoch: 265 [29568/54000 (55%)] Loss: -216285.140625\n",
      "Train Epoch: 265 [30976/54000 (57%)] Loss: -228287.125000\n",
      "Train Epoch: 265 [32384/54000 (60%)] Loss: -227813.843750\n",
      "Train Epoch: 265 [33792/54000 (63%)] Loss: -226063.781250\n",
      "Train Epoch: 265 [35200/54000 (65%)] Loss: -222028.468750\n",
      "Train Epoch: 265 [36608/54000 (68%)] Loss: -217590.750000\n",
      "Train Epoch: 265 [38016/54000 (70%)] Loss: -215517.078125\n",
      "Train Epoch: 265 [39424/54000 (73%)] Loss: -247844.062500\n",
      "Train Epoch: 265 [40832/54000 (76%)] Loss: -228525.781250\n",
      "Train Epoch: 265 [42240/54000 (78%)] Loss: -219328.953125\n",
      "Train Epoch: 265 [43648/54000 (81%)] Loss: -223168.687500\n",
      "Train Epoch: 265 [45056/54000 (83%)] Loss: -225328.062500\n",
      "Train Epoch: 265 [46464/54000 (86%)] Loss: -225705.312500\n",
      "Train Epoch: 265 [47872/54000 (89%)] Loss: -225997.000000\n",
      "Train Epoch: 265 [49280/54000 (91%)] Loss: -225882.421875\n",
      "Train Epoch: 265 [50688/54000 (94%)] Loss: -227754.375000\n",
      "Train Epoch: 265 [52096/54000 (96%)] Loss: -245569.187500\n",
      "    epoch          : 265\n",
      "    loss           : -225566.15281100478\n",
      "    val_loss       : -229250.06905011434\n",
      "Train Epoch: 266 [0/54000 (0%)] Loss: -231343.843750\n",
      "Train Epoch: 266 [1408/54000 (3%)] Loss: -229564.890625\n",
      "Train Epoch: 266 [2816/54000 (5%)] Loss: -230190.484375\n",
      "Train Epoch: 266 [4224/54000 (8%)] Loss: -221464.531250\n",
      "Train Epoch: 266 [5632/54000 (10%)] Loss: -217604.906250\n",
      "Train Epoch: 266 [7040/54000 (13%)] Loss: -229544.453125\n",
      "Train Epoch: 266 [8448/54000 (16%)] Loss: -217128.218750\n",
      "Train Epoch: 266 [9856/54000 (18%)] Loss: -226157.359375\n",
      "Train Epoch: 266 [11264/54000 (21%)] Loss: -230807.968750\n",
      "Train Epoch: 266 [12672/54000 (23%)] Loss: -230416.718750\n",
      "Train Epoch: 266 [14080/54000 (26%)] Loss: -231592.984375\n",
      "Train Epoch: 266 [15488/54000 (29%)] Loss: -228509.468750\n",
      "Train Epoch: 266 [16896/54000 (31%)] Loss: -218600.500000\n",
      "Train Epoch: 266 [18304/54000 (34%)] Loss: -219093.609375\n",
      "Train Epoch: 266 [19712/54000 (37%)] Loss: -228471.234375\n",
      "Train Epoch: 266 [21120/54000 (39%)] Loss: -224225.546875\n",
      "Train Epoch: 266 [22528/54000 (42%)] Loss: -217002.578125\n",
      "Train Epoch: 266 [23936/54000 (44%)] Loss: -248560.890625\n",
      "Train Epoch: 266 [25344/54000 (47%)] Loss: -219415.296875\n",
      "Train Epoch: 266 [26752/54000 (50%)] Loss: -214016.015625\n",
      "Train Epoch: 266 [28160/54000 (52%)] Loss: -222583.578125\n",
      "Train Epoch: 266 [29568/54000 (55%)] Loss: -219974.921875\n",
      "Train Epoch: 266 [30976/54000 (57%)] Loss: -221167.656250\n",
      "Train Epoch: 266 [32384/54000 (60%)] Loss: -218905.421875\n",
      "Train Epoch: 266 [33792/54000 (63%)] Loss: -217332.406250\n",
      "Train Epoch: 266 [35200/54000 (65%)] Loss: -229148.812500\n",
      "Train Epoch: 266 [36608/54000 (68%)] Loss: -246539.390625\n",
      "Train Epoch: 266 [38016/54000 (70%)] Loss: -221546.453125\n",
      "Train Epoch: 266 [39424/54000 (73%)] Loss: -228603.828125\n",
      "Train Epoch: 266 [40832/54000 (76%)] Loss: -221081.093750\n",
      "Train Epoch: 266 [42240/54000 (78%)] Loss: -248055.921875\n",
      "Train Epoch: 266 [43648/54000 (81%)] Loss: -223631.375000\n",
      "Train Epoch: 266 [45056/54000 (83%)] Loss: -220708.562500\n",
      "Train Epoch: 266 [46464/54000 (86%)] Loss: -218173.437500\n",
      "Train Epoch: 266 [47872/54000 (89%)] Loss: -218942.687500\n",
      "Train Epoch: 266 [49280/54000 (91%)] Loss: -220708.828125\n",
      "Train Epoch: 266 [50688/54000 (94%)] Loss: -214190.890625\n",
      "Train Epoch: 266 [52096/54000 (96%)] Loss: -217790.781250\n",
      "    epoch          : 266\n",
      "    loss           : -225548.44516297846\n",
      "    val_loss       : -229016.2149271151\n",
      "Train Epoch: 267 [0/54000 (0%)] Loss: -225223.734375\n",
      "Train Epoch: 267 [1408/54000 (3%)] Loss: -226740.875000\n",
      "Train Epoch: 267 [2816/54000 (5%)] Loss: -227126.812500\n",
      "Train Epoch: 267 [4224/54000 (8%)] Loss: -230326.921875\n",
      "Train Epoch: 267 [5632/54000 (10%)] Loss: -225969.281250\n",
      "Train Epoch: 267 [7040/54000 (13%)] Loss: -217643.750000\n",
      "Train Epoch: 267 [8448/54000 (16%)] Loss: -225122.875000\n",
      "Train Epoch: 267 [9856/54000 (18%)] Loss: -219247.906250\n",
      "Train Epoch: 267 [11264/54000 (21%)] Loss: -222952.750000\n",
      "Train Epoch: 267 [12672/54000 (23%)] Loss: -230621.515625\n",
      "Train Epoch: 267 [14080/54000 (26%)] Loss: -223592.656250\n",
      "Train Epoch: 267 [15488/54000 (29%)] Loss: -219518.500000\n",
      "Train Epoch: 267 [16896/54000 (31%)] Loss: -223790.906250\n",
      "Train Epoch: 267 [18304/54000 (34%)] Loss: -222615.093750\n",
      "Train Epoch: 267 [19712/54000 (37%)] Loss: -220474.375000\n",
      "Train Epoch: 267 [21120/54000 (39%)] Loss: -228213.250000\n",
      "Train Epoch: 267 [22528/54000 (42%)] Loss: -212258.234375\n",
      "Train Epoch: 267 [23936/54000 (44%)] Loss: -228157.656250\n",
      "Train Epoch: 267 [25344/54000 (47%)] Loss: -225119.562500\n",
      "Train Epoch: 267 [26752/54000 (50%)] Loss: -249474.296875\n",
      "Train Epoch: 267 [28160/54000 (52%)] Loss: -218879.140625\n",
      "Train Epoch: 267 [29568/54000 (55%)] Loss: -220392.406250\n",
      "Train Epoch: 267 [30976/54000 (57%)] Loss: -218980.078125\n",
      "Train Epoch: 267 [32384/54000 (60%)] Loss: -218165.078125\n",
      "Train Epoch: 267 [33792/54000 (63%)] Loss: -222588.125000\n",
      "Train Epoch: 267 [35200/54000 (65%)] Loss: -222981.234375\n",
      "Train Epoch: 267 [36608/54000 (68%)] Loss: -223178.468750\n",
      "Train Epoch: 267 [38016/54000 (70%)] Loss: -244867.390625\n",
      "Train Epoch: 267 [39424/54000 (73%)] Loss: -228615.312500\n",
      "Train Epoch: 267 [40832/54000 (76%)] Loss: -225545.468750\n",
      "Train Epoch: 267 [42240/54000 (78%)] Loss: -213420.281250\n",
      "Train Epoch: 267 [43648/54000 (81%)] Loss: -219195.718750\n",
      "Train Epoch: 267 [45056/54000 (83%)] Loss: -219221.718750\n",
      "Train Epoch: 267 [46464/54000 (86%)] Loss: -220499.187500\n",
      "Train Epoch: 267 [47872/54000 (89%)] Loss: -220776.640625\n",
      "Train Epoch: 267 [49280/54000 (91%)] Loss: -229351.718750\n",
      "Train Epoch: 267 [50688/54000 (94%)] Loss: -219626.890625\n",
      "Train Epoch: 267 [52096/54000 (96%)] Loss: -248287.312500\n",
      "    epoch          : 267\n",
      "    loss           : -225461.18764952154\n",
      "    val_loss       : -229302.52374714176\n",
      "Train Epoch: 268 [0/54000 (0%)] Loss: -227633.984375\n",
      "Train Epoch: 268 [1408/54000 (3%)] Loss: -216158.453125\n",
      "Train Epoch: 268 [2816/54000 (5%)] Loss: -216921.015625\n",
      "Train Epoch: 268 [4224/54000 (8%)] Loss: -218914.859375\n",
      "Train Epoch: 268 [5632/54000 (10%)] Loss: -246999.390625\n",
      "Train Epoch: 268 [7040/54000 (13%)] Loss: -224167.906250\n",
      "Train Epoch: 268 [8448/54000 (16%)] Loss: -219862.296875\n",
      "Train Epoch: 268 [9856/54000 (18%)] Loss: -217895.046875\n",
      "Train Epoch: 268 [11264/54000 (21%)] Loss: -230615.671875\n",
      "Train Epoch: 268 [12672/54000 (23%)] Loss: -245991.046875\n",
      "Train Epoch: 268 [14080/54000 (26%)] Loss: -223844.187500\n",
      "Train Epoch: 268 [15488/54000 (29%)] Loss: -227882.062500\n",
      "Train Epoch: 268 [16896/54000 (31%)] Loss: -223791.062500\n",
      "Train Epoch: 268 [18304/54000 (34%)] Loss: -248541.578125\n",
      "Train Epoch: 268 [19712/54000 (37%)] Loss: -219239.250000\n",
      "Train Epoch: 268 [21120/54000 (39%)] Loss: -215013.843750\n",
      "Train Epoch: 268 [22528/54000 (42%)] Loss: -230432.875000\n",
      "Train Epoch: 268 [23936/54000 (44%)] Loss: -246400.234375\n",
      "Train Epoch: 268 [25344/54000 (47%)] Loss: -228927.859375\n",
      "Train Epoch: 268 [26752/54000 (50%)] Loss: -218942.234375\n",
      "Train Epoch: 268 [28160/54000 (52%)] Loss: -222549.625000\n",
      "Train Epoch: 268 [29568/54000 (55%)] Loss: -229923.750000\n",
      "Train Epoch: 268 [30976/54000 (57%)] Loss: -219745.093750\n",
      "Train Epoch: 268 [32384/54000 (60%)] Loss: -217018.703125\n",
      "Train Epoch: 268 [33792/54000 (63%)] Loss: -213541.562500\n",
      "Train Epoch: 268 [35200/54000 (65%)] Loss: -219478.046875\n",
      "Train Epoch: 268 [36608/54000 (68%)] Loss: -224300.390625\n",
      "Train Epoch: 268 [38016/54000 (70%)] Loss: -229107.203125\n",
      "Train Epoch: 268 [39424/54000 (73%)] Loss: -231005.906250\n",
      "Train Epoch: 268 [40832/54000 (76%)] Loss: -215085.265625\n",
      "Train Epoch: 268 [42240/54000 (78%)] Loss: -217814.625000\n",
      "Train Epoch: 268 [43648/54000 (81%)] Loss: -219110.937500\n",
      "Train Epoch: 268 [45056/54000 (83%)] Loss: -247657.500000\n",
      "Train Epoch: 268 [46464/54000 (86%)] Loss: -219603.765625\n",
      "Train Epoch: 268 [47872/54000 (89%)] Loss: -217869.250000\n",
      "Train Epoch: 268 [49280/54000 (91%)] Loss: -220141.812500\n",
      "Train Epoch: 268 [50688/54000 (94%)] Loss: -219661.156250\n",
      "Train Epoch: 268 [52096/54000 (96%)] Loss: -213836.375000\n",
      "    epoch          : 268\n",
      "    loss           : -225331.65785735645\n",
      "    val_loss       : -229476.63066287158\n",
      "Train Epoch: 269 [0/54000 (0%)] Loss: -246004.093750\n",
      "Train Epoch: 269 [1408/54000 (3%)] Loss: -216281.343750\n",
      "Train Epoch: 269 [2816/54000 (5%)] Loss: -223588.984375\n",
      "Train Epoch: 269 [4224/54000 (8%)] Loss: -222033.015625\n",
      "Train Epoch: 269 [5632/54000 (10%)] Loss: -228106.734375\n",
      "Train Epoch: 269 [7040/54000 (13%)] Loss: -228677.281250\n",
      "Train Epoch: 269 [8448/54000 (16%)] Loss: -224069.984375\n",
      "Train Epoch: 269 [9856/54000 (18%)] Loss: -229282.687500\n",
      "Train Epoch: 269 [11264/54000 (21%)] Loss: -223358.500000\n",
      "Train Epoch: 269 [12672/54000 (23%)] Loss: -226939.250000\n",
      "Train Epoch: 269 [14080/54000 (26%)] Loss: -248156.687500\n",
      "Train Epoch: 269 [15488/54000 (29%)] Loss: -230936.468750\n",
      "Train Epoch: 269 [16896/54000 (31%)] Loss: -229048.734375\n",
      "Train Epoch: 269 [18304/54000 (34%)] Loss: -228412.171875\n",
      "Train Epoch: 269 [19712/54000 (37%)] Loss: -219979.156250\n",
      "Train Epoch: 269 [21120/54000 (39%)] Loss: -218700.765625\n",
      "Train Epoch: 269 [22528/54000 (42%)] Loss: -218686.375000\n",
      "Train Epoch: 269 [23936/54000 (44%)] Loss: -228838.796875\n",
      "Train Epoch: 269 [25344/54000 (47%)] Loss: -230461.953125\n",
      "Train Epoch: 269 [26752/54000 (50%)] Loss: -229440.156250\n",
      "Train Epoch: 269 [28160/54000 (52%)] Loss: -227806.453125\n",
      "Train Epoch: 269 [29568/54000 (55%)] Loss: -217470.421875\n",
      "Train Epoch: 269 [30976/54000 (57%)] Loss: -221911.312500\n",
      "Train Epoch: 269 [32384/54000 (60%)] Loss: -226874.062500\n",
      "Train Epoch: 269 [33792/54000 (63%)] Loss: -218721.062500\n",
      "Train Epoch: 269 [35200/54000 (65%)] Loss: -218728.218750\n",
      "Train Epoch: 269 [36608/54000 (68%)] Loss: -222571.781250\n",
      "Train Epoch: 269 [38016/54000 (70%)] Loss: -248031.906250\n",
      "Train Epoch: 269 [39424/54000 (73%)] Loss: -229905.203125\n",
      "Train Epoch: 269 [40832/54000 (76%)] Loss: -212888.859375\n",
      "Train Epoch: 269 [42240/54000 (78%)] Loss: -228733.156250\n",
      "Train Epoch: 269 [43648/54000 (81%)] Loss: -247415.437500\n",
      "Train Epoch: 269 [45056/54000 (83%)] Loss: -223635.156250\n",
      "Train Epoch: 269 [46464/54000 (86%)] Loss: -219367.625000\n",
      "Train Epoch: 269 [47872/54000 (89%)] Loss: -222331.000000\n",
      "Train Epoch: 269 [49280/54000 (91%)] Loss: -219169.343750\n",
      "Train Epoch: 269 [50688/54000 (94%)] Loss: -248374.765625\n",
      "Train Epoch: 269 [52096/54000 (96%)] Loss: -214094.093750\n",
      "    epoch          : 269\n",
      "    loss           : -225564.22622607654\n",
      "    val_loss       : -229526.3664133956\n",
      "Train Epoch: 270 [0/54000 (0%)] Loss: -246664.953125\n",
      "Train Epoch: 270 [1408/54000 (3%)] Loss: -229790.781250\n",
      "Train Epoch: 270 [2816/54000 (5%)] Loss: -222641.281250\n",
      "Train Epoch: 270 [4224/54000 (8%)] Loss: -219934.406250\n",
      "Train Epoch: 270 [5632/54000 (10%)] Loss: -228787.156250\n",
      "Train Epoch: 270 [7040/54000 (13%)] Loss: -229993.015625\n",
      "Train Epoch: 270 [8448/54000 (16%)] Loss: -223316.812500\n",
      "Train Epoch: 270 [9856/54000 (18%)] Loss: -229023.515625\n",
      "Train Epoch: 270 [11264/54000 (21%)] Loss: -221718.265625\n",
      "Train Epoch: 270 [12672/54000 (23%)] Loss: -221765.953125\n",
      "Train Epoch: 270 [14080/54000 (26%)] Loss: -222617.875000\n",
      "Train Epoch: 270 [15488/54000 (29%)] Loss: -227889.796875\n",
      "Train Epoch: 270 [16896/54000 (31%)] Loss: -213439.375000\n",
      "Train Epoch: 270 [18304/54000 (34%)] Loss: -219790.828125\n",
      "Train Epoch: 270 [19712/54000 (37%)] Loss: -219697.968750\n",
      "Train Epoch: 270 [21120/54000 (39%)] Loss: -223678.343750\n",
      "Train Epoch: 270 [22528/54000 (42%)] Loss: -217510.375000\n",
      "Train Epoch: 270 [23936/54000 (44%)] Loss: -246003.937500\n",
      "Train Epoch: 270 [25344/54000 (47%)] Loss: -229224.687500\n",
      "Train Epoch: 270 [26752/54000 (50%)] Loss: -220646.437500\n",
      "Train Epoch: 270 [28160/54000 (52%)] Loss: -221011.843750\n",
      "Train Epoch: 270 [29568/54000 (55%)] Loss: -221931.578125\n",
      "Train Epoch: 270 [30976/54000 (57%)] Loss: -218180.500000\n",
      "Train Epoch: 270 [32384/54000 (60%)] Loss: -221921.468750\n",
      "Train Epoch: 270 [33792/54000 (63%)] Loss: -225833.656250\n",
      "Train Epoch: 270 [35200/54000 (65%)] Loss: -228435.828125\n",
      "Train Epoch: 270 [36608/54000 (68%)] Loss: -218621.671875\n",
      "Train Epoch: 270 [38016/54000 (70%)] Loss: -220541.218750\n",
      "Train Epoch: 270 [39424/54000 (73%)] Loss: -247570.390625\n",
      "Train Epoch: 270 [40832/54000 (76%)] Loss: -228036.468750\n",
      "Train Epoch: 270 [42240/54000 (78%)] Loss: -231202.484375\n",
      "Train Epoch: 270 [43648/54000 (81%)] Loss: -219790.578125\n",
      "Train Epoch: 270 [45056/54000 (83%)] Loss: -246316.281250\n",
      "Train Epoch: 270 [46464/54000 (86%)] Loss: -222982.843750\n",
      "Train Epoch: 270 [47872/54000 (89%)] Loss: -214661.703125\n",
      "Train Epoch: 270 [49280/54000 (91%)] Loss: -223592.218750\n",
      "Train Epoch: 270 [50688/54000 (94%)] Loss: -218181.187500\n",
      "Train Epoch: 270 [52096/54000 (96%)] Loss: -212729.593750\n",
      "    epoch          : 270\n",
      "    loss           : -225657.66495962918\n",
      "    val_loss       : -228820.81975276297\n",
      "Train Epoch: 271 [0/54000 (0%)] Loss: -227610.734375\n",
      "Train Epoch: 271 [1408/54000 (3%)] Loss: -226968.390625\n",
      "Train Epoch: 271 [2816/54000 (5%)] Loss: -230706.156250\n",
      "Train Epoch: 271 [4224/54000 (8%)] Loss: -224051.984375\n",
      "Train Epoch: 271 [5632/54000 (10%)] Loss: -219032.140625\n",
      "Train Epoch: 271 [7040/54000 (13%)] Loss: -218917.484375\n",
      "Train Epoch: 271 [8448/54000 (16%)] Loss: -216594.828125\n",
      "Train Epoch: 271 [9856/54000 (18%)] Loss: -226624.265625\n",
      "Train Epoch: 271 [11264/54000 (21%)] Loss: -227909.265625\n",
      "Train Epoch: 271 [12672/54000 (23%)] Loss: -225583.640625\n",
      "Train Epoch: 271 [14080/54000 (26%)] Loss: -246173.500000\n",
      "Train Epoch: 271 [15488/54000 (29%)] Loss: -229770.765625\n",
      "Train Epoch: 271 [16896/54000 (31%)] Loss: -230099.890625\n",
      "Train Epoch: 271 [18304/54000 (34%)] Loss: -217479.859375\n",
      "Train Epoch: 271 [19712/54000 (37%)] Loss: -213676.343750\n",
      "Train Epoch: 271 [21120/54000 (39%)] Loss: -229847.718750\n",
      "Train Epoch: 271 [22528/54000 (42%)] Loss: -216653.937500\n",
      "Train Epoch: 271 [23936/54000 (44%)] Loss: -229945.437500\n",
      "Train Epoch: 271 [25344/54000 (47%)] Loss: -247128.156250\n",
      "Train Epoch: 271 [26752/54000 (50%)] Loss: -229435.468750\n",
      "Train Epoch: 271 [28160/54000 (52%)] Loss: -219754.781250\n",
      "Train Epoch: 271 [29568/54000 (55%)] Loss: -219038.265625\n",
      "Train Epoch: 271 [30976/54000 (57%)] Loss: -227437.656250\n",
      "Train Epoch: 271 [32384/54000 (60%)] Loss: -245518.265625\n",
      "Train Epoch: 271 [33792/54000 (63%)] Loss: -227381.859375\n",
      "Train Epoch: 271 [35200/54000 (65%)] Loss: -224634.640625\n",
      "Train Epoch: 271 [36608/54000 (68%)] Loss: -222647.968750\n",
      "Train Epoch: 271 [38016/54000 (70%)] Loss: -222907.031250\n",
      "Train Epoch: 271 [39424/54000 (73%)] Loss: -219882.937500\n",
      "Train Epoch: 271 [40832/54000 (76%)] Loss: -215330.125000\n",
      "Train Epoch: 271 [42240/54000 (78%)] Loss: -246560.375000\n",
      "Train Epoch: 271 [43648/54000 (81%)] Loss: -227102.125000\n",
      "Train Epoch: 271 [45056/54000 (83%)] Loss: -223359.718750\n",
      "Train Epoch: 271 [46464/54000 (86%)] Loss: -226876.468750\n",
      "Train Epoch: 271 [47872/54000 (89%)] Loss: -223316.859375\n",
      "Train Epoch: 271 [49280/54000 (91%)] Loss: -247144.750000\n",
      "Train Epoch: 271 [50688/54000 (94%)] Loss: -218189.562500\n",
      "Train Epoch: 271 [52096/54000 (96%)] Loss: -227716.531250\n",
      "    epoch          : 271\n",
      "    loss           : -225524.50646680623\n",
      "    val_loss       : -229556.94956411966\n",
      "Train Epoch: 272 [0/54000 (0%)] Loss: -221822.156250\n",
      "Train Epoch: 272 [1408/54000 (3%)] Loss: -226234.312500\n",
      "Train Epoch: 272 [2816/54000 (5%)] Loss: -221362.984375\n",
      "Train Epoch: 272 [4224/54000 (8%)] Loss: -221423.203125\n",
      "Train Epoch: 272 [5632/54000 (10%)] Loss: -218985.390625\n",
      "Train Epoch: 272 [7040/54000 (13%)] Loss: -227609.093750\n",
      "Train Epoch: 272 [8448/54000 (16%)] Loss: -229621.187500\n",
      "Train Epoch: 272 [9856/54000 (18%)] Loss: -217526.046875\n",
      "Train Epoch: 272 [11264/54000 (21%)] Loss: -223987.984375\n",
      "Train Epoch: 272 [12672/54000 (23%)] Loss: -248945.234375\n",
      "Train Epoch: 272 [14080/54000 (26%)] Loss: -227262.562500\n",
      "Train Epoch: 272 [15488/54000 (29%)] Loss: -220190.484375\n",
      "Train Epoch: 272 [16896/54000 (31%)] Loss: -215008.671875\n",
      "Train Epoch: 272 [18304/54000 (34%)] Loss: -224246.531250\n",
      "Train Epoch: 272 [19712/54000 (37%)] Loss: -219316.250000\n",
      "Train Epoch: 272 [21120/54000 (39%)] Loss: -224798.671875\n",
      "Train Epoch: 272 [22528/54000 (42%)] Loss: -220380.250000\n",
      "Train Epoch: 272 [23936/54000 (44%)] Loss: -229561.406250\n",
      "Train Epoch: 272 [25344/54000 (47%)] Loss: -230161.296875\n",
      "Train Epoch: 272 [26752/54000 (50%)] Loss: -221672.296875\n",
      "Train Epoch: 272 [28160/54000 (52%)] Loss: -228977.234375\n",
      "Train Epoch: 272 [29568/54000 (55%)] Loss: -249409.531250\n",
      "Train Epoch: 272 [30976/54000 (57%)] Loss: -223830.734375\n",
      "Train Epoch: 272 [32384/54000 (60%)] Loss: -220259.578125\n",
      "Train Epoch: 272 [33792/54000 (63%)] Loss: -221756.593750\n",
      "Train Epoch: 272 [35200/54000 (65%)] Loss: -222119.078125\n",
      "Train Epoch: 272 [36608/54000 (68%)] Loss: -224874.656250\n",
      "Train Epoch: 272 [38016/54000 (70%)] Loss: -221738.687500\n",
      "Train Epoch: 272 [39424/54000 (73%)] Loss: -215138.281250\n",
      "Train Epoch: 272 [40832/54000 (76%)] Loss: -220089.765625\n",
      "Train Epoch: 272 [42240/54000 (78%)] Loss: -228564.296875\n",
      "Train Epoch: 272 [43648/54000 (81%)] Loss: -229441.921875\n",
      "Train Epoch: 272 [45056/54000 (83%)] Loss: -219060.890625\n",
      "Train Epoch: 272 [46464/54000 (86%)] Loss: -218883.046875\n",
      "Train Epoch: 272 [47872/54000 (89%)] Loss: -227463.593750\n",
      "Train Epoch: 272 [49280/54000 (91%)] Loss: -225709.703125\n",
      "Train Epoch: 272 [50688/54000 (94%)] Loss: -248897.156250\n",
      "Train Epoch: 272 [52096/54000 (96%)] Loss: -227710.921875\n",
      "    epoch          : 272\n",
      "    loss           : -225739.29866925837\n",
      "    val_loss       : -229553.13149056784\n",
      "Train Epoch: 273 [0/54000 (0%)] Loss: -222266.421875\n",
      "Train Epoch: 273 [1408/54000 (3%)] Loss: -213173.906250\n",
      "Train Epoch: 273 [2816/54000 (5%)] Loss: -225285.671875\n",
      "Train Epoch: 273 [4224/54000 (8%)] Loss: -223971.562500\n",
      "Train Epoch: 273 [5632/54000 (10%)] Loss: -219543.531250\n",
      "Train Epoch: 273 [7040/54000 (13%)] Loss: -228892.468750\n",
      "Train Epoch: 273 [8448/54000 (16%)] Loss: -243966.031250\n",
      "Train Epoch: 273 [9856/54000 (18%)] Loss: -220188.093750\n",
      "Train Epoch: 273 [11264/54000 (21%)] Loss: -224618.718750\n",
      "Train Epoch: 273 [12672/54000 (23%)] Loss: -226325.281250\n",
      "Train Epoch: 273 [14080/54000 (26%)] Loss: -226496.781250\n",
      "Train Epoch: 273 [15488/54000 (29%)] Loss: -219406.203125\n",
      "Train Epoch: 273 [16896/54000 (31%)] Loss: -217591.234375\n",
      "Train Epoch: 273 [18304/54000 (34%)] Loss: -222842.125000\n",
      "Train Epoch: 273 [19712/54000 (37%)] Loss: -222572.562500\n",
      "Train Epoch: 273 [21120/54000 (39%)] Loss: -221710.000000\n",
      "Train Epoch: 273 [22528/54000 (42%)] Loss: -229021.078125\n",
      "Train Epoch: 273 [23936/54000 (44%)] Loss: -230659.015625\n",
      "Train Epoch: 273 [25344/54000 (47%)] Loss: -248904.859375\n",
      "Train Epoch: 273 [26752/54000 (50%)] Loss: -216187.875000\n",
      "Train Epoch: 273 [28160/54000 (52%)] Loss: -230117.187500\n",
      "Train Epoch: 273 [29568/54000 (55%)] Loss: -225279.156250\n",
      "Train Epoch: 273 [30976/54000 (57%)] Loss: -248914.093750\n",
      "Train Epoch: 273 [32384/54000 (60%)] Loss: -219857.453125\n",
      "Train Epoch: 273 [33792/54000 (63%)] Loss: -217796.125000\n",
      "Train Epoch: 273 [35200/54000 (65%)] Loss: -218846.375000\n",
      "Train Epoch: 273 [36608/54000 (68%)] Loss: -218287.875000\n",
      "Train Epoch: 273 [38016/54000 (70%)] Loss: -230241.718750\n",
      "Train Epoch: 273 [39424/54000 (73%)] Loss: -219310.781250\n",
      "Train Epoch: 273 [40832/54000 (76%)] Loss: -225236.250000\n",
      "Train Epoch: 273 [42240/54000 (78%)] Loss: -228324.531250\n",
      "Train Epoch: 273 [43648/54000 (81%)] Loss: -227460.000000\n",
      "Train Epoch: 273 [45056/54000 (83%)] Loss: -228280.906250\n",
      "Train Epoch: 273 [46464/54000 (86%)] Loss: -223750.812500\n",
      "Train Epoch: 273 [47872/54000 (89%)] Loss: -226715.234375\n",
      "Train Epoch: 273 [49280/54000 (91%)] Loss: -229508.015625\n",
      "Train Epoch: 273 [50688/54000 (94%)] Loss: -227162.109375\n",
      "Train Epoch: 273 [52096/54000 (96%)] Loss: -223564.234375\n",
      "    epoch          : 273\n",
      "    loss           : -225629.52702601676\n",
      "    val_loss       : -229767.03192883002\n",
      "Train Epoch: 274 [0/54000 (0%)] Loss: -223394.156250\n",
      "Train Epoch: 274 [1408/54000 (3%)] Loss: -221833.031250\n",
      "Train Epoch: 274 [2816/54000 (5%)] Loss: -224239.125000\n",
      "Train Epoch: 274 [4224/54000 (8%)] Loss: -217127.906250\n",
      "Train Epoch: 274 [5632/54000 (10%)] Loss: -222338.531250\n",
      "Train Epoch: 274 [7040/54000 (13%)] Loss: -220197.484375\n",
      "Train Epoch: 274 [8448/54000 (16%)] Loss: -219525.625000\n",
      "Train Epoch: 274 [9856/54000 (18%)] Loss: -225816.343750\n",
      "Train Epoch: 274 [11264/54000 (21%)] Loss: -228301.531250\n",
      "Train Epoch: 274 [12672/54000 (23%)] Loss: -219062.500000\n",
      "Train Epoch: 274 [14080/54000 (26%)] Loss: -230386.968750\n",
      "Train Epoch: 274 [15488/54000 (29%)] Loss: -230008.078125\n",
      "Train Epoch: 274 [16896/54000 (31%)] Loss: -227666.046875\n",
      "Train Epoch: 274 [18304/54000 (34%)] Loss: -217812.187500\n",
      "Train Epoch: 274 [19712/54000 (37%)] Loss: -221862.281250\n",
      "Train Epoch: 274 [21120/54000 (39%)] Loss: -217864.781250\n",
      "Train Epoch: 274 [22528/54000 (42%)] Loss: -221693.812500\n",
      "Train Epoch: 274 [23936/54000 (44%)] Loss: -216183.156250\n",
      "Train Epoch: 274 [25344/54000 (47%)] Loss: -219437.437500\n",
      "Train Epoch: 274 [26752/54000 (50%)] Loss: -247144.062500\n",
      "Train Epoch: 274 [28160/54000 (52%)] Loss: -223669.625000\n",
      "Train Epoch: 274 [29568/54000 (55%)] Loss: -224876.515625\n",
      "Train Epoch: 274 [30976/54000 (57%)] Loss: -218181.546875\n",
      "Train Epoch: 274 [32384/54000 (60%)] Loss: -245294.828125\n",
      "Train Epoch: 274 [33792/54000 (63%)] Loss: -218733.125000\n",
      "Train Epoch: 274 [35200/54000 (65%)] Loss: -228966.828125\n",
      "Train Epoch: 274 [36608/54000 (68%)] Loss: -223135.843750\n",
      "Train Epoch: 274 [38016/54000 (70%)] Loss: -246344.421875\n",
      "Train Epoch: 274 [39424/54000 (73%)] Loss: -229584.687500\n",
      "Train Epoch: 274 [40832/54000 (76%)] Loss: -220972.750000\n",
      "Train Epoch: 274 [42240/54000 (78%)] Loss: -220742.484375\n",
      "Train Epoch: 274 [43648/54000 (81%)] Loss: -247878.812500\n",
      "Train Epoch: 274 [45056/54000 (83%)] Loss: -215628.093750\n",
      "Train Epoch: 274 [46464/54000 (86%)] Loss: -223727.640625\n",
      "Train Epoch: 274 [47872/54000 (89%)] Loss: -212172.250000\n",
      "Train Epoch: 274 [49280/54000 (91%)] Loss: -221208.390625\n",
      "Train Epoch: 274 [50688/54000 (94%)] Loss: -219963.375000\n",
      "Train Epoch: 274 [52096/54000 (96%)] Loss: -217545.781250\n",
      "    epoch          : 274\n",
      "    loss           : -225691.92396830145\n",
      "    val_loss       : -229368.26193907202\n",
      "Train Epoch: 275 [0/54000 (0%)] Loss: -248542.203125\n",
      "Train Epoch: 275 [1408/54000 (3%)] Loss: -248425.687500\n",
      "Train Epoch: 275 [2816/54000 (5%)] Loss: -218537.312500\n",
      "Train Epoch: 275 [4224/54000 (8%)] Loss: -230857.156250\n",
      "Train Epoch: 275 [5632/54000 (10%)] Loss: -216462.781250\n",
      "Train Epoch: 275 [7040/54000 (13%)] Loss: -215964.031250\n",
      "Train Epoch: 275 [8448/54000 (16%)] Loss: -245805.453125\n",
      "Train Epoch: 275 [9856/54000 (18%)] Loss: -214853.109375\n",
      "Train Epoch: 275 [11264/54000 (21%)] Loss: -228515.609375\n",
      "Train Epoch: 275 [12672/54000 (23%)] Loss: -217021.359375\n",
      "Train Epoch: 275 [14080/54000 (26%)] Loss: -219180.406250\n",
      "Train Epoch: 275 [15488/54000 (29%)] Loss: -224370.484375\n",
      "Train Epoch: 275 [16896/54000 (31%)] Loss: -218569.843750\n",
      "Train Epoch: 275 [18304/54000 (34%)] Loss: -222453.109375\n",
      "Train Epoch: 275 [19712/54000 (37%)] Loss: -217601.468750\n",
      "Train Epoch: 275 [21120/54000 (39%)] Loss: -219152.968750\n",
      "Train Epoch: 275 [22528/54000 (42%)] Loss: -230475.218750\n",
      "Train Epoch: 275 [23936/54000 (44%)] Loss: -221475.437500\n",
      "Train Epoch: 275 [25344/54000 (47%)] Loss: -247112.953125\n",
      "Train Epoch: 275 [26752/54000 (50%)] Loss: -222932.281250\n",
      "Train Epoch: 275 [28160/54000 (52%)] Loss: -225709.718750\n",
      "Train Epoch: 275 [29568/54000 (55%)] Loss: -220833.093750\n",
      "Train Epoch: 275 [30976/54000 (57%)] Loss: -227859.140625\n",
      "Train Epoch: 275 [32384/54000 (60%)] Loss: -222557.812500\n",
      "Train Epoch: 275 [33792/54000 (63%)] Loss: -221387.046875\n",
      "Train Epoch: 275 [35200/54000 (65%)] Loss: -225319.500000\n",
      "Train Epoch: 275 [36608/54000 (68%)] Loss: -248485.359375\n",
      "Train Epoch: 275 [38016/54000 (70%)] Loss: -217996.031250\n",
      "Train Epoch: 275 [39424/54000 (73%)] Loss: -228550.593750\n",
      "Train Epoch: 275 [40832/54000 (76%)] Loss: -229950.078125\n",
      "Train Epoch: 275 [42240/54000 (78%)] Loss: -245944.984375\n",
      "Train Epoch: 275 [43648/54000 (81%)] Loss: -209558.562500\n",
      "Train Epoch: 275 [45056/54000 (83%)] Loss: -222628.296875\n",
      "Train Epoch: 275 [46464/54000 (86%)] Loss: -226376.781250\n",
      "Train Epoch: 275 [47872/54000 (89%)] Loss: -219159.718750\n",
      "Train Epoch: 275 [49280/54000 (91%)] Loss: -247142.359375\n",
      "Train Epoch: 275 [50688/54000 (94%)] Loss: -222323.875000\n",
      "Train Epoch: 275 [52096/54000 (96%)] Loss: -226986.656250\n",
      "    epoch          : 275\n",
      "    loss           : -225815.2864458732\n",
      "    val_loss       : -229512.89946765435\n",
      "Train Epoch: 276 [0/54000 (0%)] Loss: -250174.562500\n",
      "Train Epoch: 276 [1408/54000 (3%)] Loss: -223702.375000\n",
      "Train Epoch: 276 [2816/54000 (5%)] Loss: -220630.703125\n",
      "Train Epoch: 276 [4224/54000 (8%)] Loss: -224321.140625\n",
      "Train Epoch: 276 [5632/54000 (10%)] Loss: -219000.984375\n",
      "Train Epoch: 276 [7040/54000 (13%)] Loss: -229844.578125\n",
      "Train Epoch: 276 [8448/54000 (16%)] Loss: -217608.734375\n",
      "Train Epoch: 276 [9856/54000 (18%)] Loss: -229092.734375\n",
      "Train Epoch: 276 [11264/54000 (21%)] Loss: -218410.328125\n",
      "Train Epoch: 276 [12672/54000 (23%)] Loss: -215046.250000\n",
      "Train Epoch: 276 [14080/54000 (26%)] Loss: -230134.656250\n",
      "Train Epoch: 276 [15488/54000 (29%)] Loss: -215117.609375\n",
      "Train Epoch: 276 [16896/54000 (31%)] Loss: -223629.265625\n",
      "Train Epoch: 276 [18304/54000 (34%)] Loss: -221262.125000\n",
      "Train Epoch: 276 [19712/54000 (37%)] Loss: -219499.484375\n",
      "Train Epoch: 276 [21120/54000 (39%)] Loss: -216598.765625\n",
      "Train Epoch: 276 [22528/54000 (42%)] Loss: -246258.781250\n",
      "Train Epoch: 276 [23936/54000 (44%)] Loss: -227860.171875\n",
      "Train Epoch: 276 [25344/54000 (47%)] Loss: -223000.859375\n",
      "Train Epoch: 276 [26752/54000 (50%)] Loss: -229178.515625\n",
      "Train Epoch: 276 [28160/54000 (52%)] Loss: -220710.875000\n",
      "Train Epoch: 276 [29568/54000 (55%)] Loss: -229838.843750\n",
      "Train Epoch: 276 [30976/54000 (57%)] Loss: -248501.796875\n",
      "Train Epoch: 276 [32384/54000 (60%)] Loss: -215627.593750\n",
      "Train Epoch: 276 [33792/54000 (63%)] Loss: -220515.781250\n",
      "Train Epoch: 276 [35200/54000 (65%)] Loss: -215024.640625\n",
      "Train Epoch: 276 [36608/54000 (68%)] Loss: -227996.687500\n",
      "Train Epoch: 276 [38016/54000 (70%)] Loss: -231086.750000\n",
      "Train Epoch: 276 [39424/54000 (73%)] Loss: -228929.687500\n",
      "Train Epoch: 276 [40832/54000 (76%)] Loss: -223570.609375\n",
      "Train Epoch: 276 [42240/54000 (78%)] Loss: -246420.015625\n",
      "Train Epoch: 276 [43648/54000 (81%)] Loss: -219157.312500\n",
      "Train Epoch: 276 [45056/54000 (83%)] Loss: -224006.171875\n",
      "Train Epoch: 276 [46464/54000 (86%)] Loss: -219782.187500\n",
      "Train Epoch: 276 [47872/54000 (89%)] Loss: -219760.156250\n",
      "Train Epoch: 276 [49280/54000 (91%)] Loss: -218592.843750\n",
      "Train Epoch: 276 [50688/54000 (94%)] Loss: -224818.921875\n",
      "Train Epoch: 276 [52096/54000 (96%)] Loss: -214277.015625\n",
      "    epoch          : 276\n",
      "    loss           : -225759.38292464116\n",
      "    val_loss       : -229512.0692406631\n",
      "Train Epoch: 277 [0/54000 (0%)] Loss: -249251.843750\n",
      "Train Epoch: 277 [1408/54000 (3%)] Loss: -230059.171875\n",
      "Train Epoch: 277 [2816/54000 (5%)] Loss: -220606.703125\n",
      "Train Epoch: 277 [4224/54000 (8%)] Loss: -219231.937500\n",
      "Train Epoch: 277 [5632/54000 (10%)] Loss: -215753.875000\n",
      "Train Epoch: 277 [7040/54000 (13%)] Loss: -220483.125000\n",
      "Train Epoch: 277 [8448/54000 (16%)] Loss: -246720.968750\n",
      "Train Epoch: 277 [9856/54000 (18%)] Loss: -222577.781250\n",
      "Train Epoch: 277 [11264/54000 (21%)] Loss: -227074.656250\n",
      "Train Epoch: 277 [12672/54000 (23%)] Loss: -216748.453125\n",
      "Train Epoch: 277 [14080/54000 (26%)] Loss: -218638.468750\n",
      "Train Epoch: 277 [15488/54000 (29%)] Loss: -226710.406250\n",
      "Train Epoch: 277 [16896/54000 (31%)] Loss: -225109.031250\n",
      "Train Epoch: 277 [18304/54000 (34%)] Loss: -230774.000000\n",
      "Train Epoch: 277 [19712/54000 (37%)] Loss: -217763.218750\n",
      "Train Epoch: 277 [21120/54000 (39%)] Loss: -228904.656250\n",
      "Train Epoch: 277 [22528/54000 (42%)] Loss: -218500.000000\n",
      "Train Epoch: 277 [23936/54000 (44%)] Loss: -228142.875000\n",
      "Train Epoch: 277 [25344/54000 (47%)] Loss: -229318.890625\n",
      "Train Epoch: 277 [26752/54000 (50%)] Loss: -229145.828125\n",
      "Train Epoch: 277 [28160/54000 (52%)] Loss: -226371.765625\n",
      "Train Epoch: 277 [29568/54000 (55%)] Loss: -230300.625000\n",
      "Train Epoch: 277 [30976/54000 (57%)] Loss: -222553.281250\n",
      "Train Epoch: 277 [32384/54000 (60%)] Loss: -217651.750000\n",
      "Train Epoch: 277 [33792/54000 (63%)] Loss: -245826.125000\n",
      "Train Epoch: 277 [35200/54000 (65%)] Loss: -229265.187500\n",
      "Train Epoch: 277 [36608/54000 (68%)] Loss: -224035.203125\n",
      "Train Epoch: 277 [38016/54000 (70%)] Loss: -230512.031250\n",
      "Train Epoch: 277 [39424/54000 (73%)] Loss: -228891.718750\n",
      "Train Epoch: 277 [40832/54000 (76%)] Loss: -223990.343750\n",
      "Train Epoch: 277 [42240/54000 (78%)] Loss: -229947.000000\n",
      "Train Epoch: 277 [43648/54000 (81%)] Loss: -222031.468750\n",
      "Train Epoch: 277 [45056/54000 (83%)] Loss: -224387.640625\n",
      "Train Epoch: 277 [46464/54000 (86%)] Loss: -225545.687500\n",
      "Train Epoch: 277 [47872/54000 (89%)] Loss: -222884.421875\n",
      "Train Epoch: 277 [49280/54000 (91%)] Loss: -219119.875000\n",
      "Train Epoch: 277 [50688/54000 (94%)] Loss: -225037.765625\n",
      "Train Epoch: 277 [52096/54000 (96%)] Loss: -230378.687500\n",
      "    epoch          : 277\n",
      "    loss           : -225839.1278035287\n",
      "    val_loss       : -229197.28676996\n",
      "Train Epoch: 278 [0/54000 (0%)] Loss: -225193.421875\n",
      "Train Epoch: 278 [1408/54000 (3%)] Loss: -228025.765625\n",
      "Train Epoch: 278 [2816/54000 (5%)] Loss: -229548.296875\n",
      "Train Epoch: 278 [4224/54000 (8%)] Loss: -226540.765625\n",
      "Train Epoch: 278 [5632/54000 (10%)] Loss: -224414.468750\n",
      "Train Epoch: 278 [7040/54000 (13%)] Loss: -217776.312500\n",
      "Train Epoch: 278 [8448/54000 (16%)] Loss: -218280.296875\n",
      "Train Epoch: 278 [9856/54000 (18%)] Loss: -231347.156250\n",
      "Train Epoch: 278 [11264/54000 (21%)] Loss: -246512.015625\n",
      "Train Epoch: 278 [12672/54000 (23%)] Loss: -222699.968750\n",
      "Train Epoch: 278 [14080/54000 (26%)] Loss: -220649.531250\n",
      "Train Epoch: 278 [15488/54000 (29%)] Loss: -218528.406250\n",
      "Train Epoch: 278 [16896/54000 (31%)] Loss: -219376.281250\n",
      "Train Epoch: 278 [18304/54000 (34%)] Loss: -218451.953125\n",
      "Train Epoch: 278 [19712/54000 (37%)] Loss: -219734.906250\n",
      "Train Epoch: 278 [21120/54000 (39%)] Loss: -220422.968750\n",
      "Train Epoch: 278 [22528/54000 (42%)] Loss: -229691.015625\n",
      "Train Epoch: 278 [23936/54000 (44%)] Loss: -223850.078125\n",
      "Train Epoch: 278 [25344/54000 (47%)] Loss: -227395.125000\n",
      "Train Epoch: 278 [26752/54000 (50%)] Loss: -222081.468750\n",
      "Train Epoch: 278 [28160/54000 (52%)] Loss: -230716.546875\n",
      "Train Epoch: 278 [29568/54000 (55%)] Loss: -216276.234375\n",
      "Train Epoch: 278 [30976/54000 (57%)] Loss: -221213.578125\n",
      "Train Epoch: 278 [32384/54000 (60%)] Loss: -229404.093750\n",
      "Train Epoch: 278 [33792/54000 (63%)] Loss: -219294.531250\n",
      "Train Epoch: 278 [35200/54000 (65%)] Loss: -226558.750000\n",
      "Train Epoch: 278 [36608/54000 (68%)] Loss: -216889.187500\n",
      "Train Epoch: 278 [38016/54000 (70%)] Loss: -230309.578125\n",
      "Train Epoch: 278 [39424/54000 (73%)] Loss: -229568.937500\n",
      "Train Epoch: 278 [40832/54000 (76%)] Loss: -227221.250000\n",
      "Train Epoch: 278 [42240/54000 (78%)] Loss: -210215.015625\n",
      "Train Epoch: 278 [43648/54000 (81%)] Loss: -246444.875000\n",
      "Train Epoch: 278 [45056/54000 (83%)] Loss: -214879.328125\n",
      "Train Epoch: 278 [46464/54000 (86%)] Loss: -225480.968750\n",
      "Train Epoch: 278 [47872/54000 (89%)] Loss: -224459.328125\n",
      "Train Epoch: 278 [49280/54000 (91%)] Loss: -246042.562500\n",
      "Train Epoch: 278 [50688/54000 (94%)] Loss: -229046.968750\n",
      "Train Epoch: 278 [52096/54000 (96%)] Loss: -228424.859375\n",
      "    epoch          : 278\n",
      "    loss           : -225841.365430622\n",
      "    val_loss       : -229267.9244652725\n",
      "Train Epoch: 279 [0/54000 (0%)] Loss: -224530.312500\n",
      "Train Epoch: 279 [1408/54000 (3%)] Loss: -228120.437500\n",
      "Train Epoch: 279 [2816/54000 (5%)] Loss: -217549.750000\n",
      "Train Epoch: 279 [4224/54000 (8%)] Loss: -218418.265625\n",
      "Train Epoch: 279 [5632/54000 (10%)] Loss: -219876.156250\n",
      "Train Epoch: 279 [7040/54000 (13%)] Loss: -219175.062500\n",
      "Train Epoch: 279 [8448/54000 (16%)] Loss: -229601.437500\n",
      "Train Epoch: 279 [9856/54000 (18%)] Loss: -220889.000000\n",
      "Train Epoch: 279 [11264/54000 (21%)] Loss: -229046.171875\n",
      "Train Epoch: 279 [12672/54000 (23%)] Loss: -223473.531250\n",
      "Train Epoch: 279 [14080/54000 (26%)] Loss: -216767.937500\n",
      "Train Epoch: 279 [15488/54000 (29%)] Loss: -218998.484375\n",
      "Train Epoch: 279 [16896/54000 (31%)] Loss: -216763.250000\n",
      "Train Epoch: 279 [18304/54000 (34%)] Loss: -228276.593750\n",
      "Train Epoch: 279 [19712/54000 (37%)] Loss: -220177.968750\n",
      "Train Epoch: 279 [21120/54000 (39%)] Loss: -229730.640625\n",
      "Train Epoch: 279 [22528/54000 (42%)] Loss: -246015.765625\n",
      "Train Epoch: 279 [23936/54000 (44%)] Loss: -219269.625000\n",
      "Train Epoch: 279 [25344/54000 (47%)] Loss: -224945.015625\n",
      "Train Epoch: 279 [26752/54000 (50%)] Loss: -223528.937500\n",
      "Train Epoch: 279 [28160/54000 (52%)] Loss: -219695.234375\n",
      "Train Epoch: 279 [29568/54000 (55%)] Loss: -228131.906250\n",
      "Train Epoch: 279 [30976/54000 (57%)] Loss: -229500.734375\n",
      "Train Epoch: 279 [32384/54000 (60%)] Loss: -218827.484375\n",
      "Train Epoch: 279 [33792/54000 (63%)] Loss: -219671.000000\n",
      "Train Epoch: 279 [35200/54000 (65%)] Loss: -230580.734375\n",
      "Train Epoch: 279 [36608/54000 (68%)] Loss: -247618.796875\n",
      "Train Epoch: 279 [38016/54000 (70%)] Loss: -228082.890625\n",
      "Train Epoch: 279 [39424/54000 (73%)] Loss: -222062.328125\n",
      "Train Epoch: 279 [40832/54000 (76%)] Loss: -228287.390625\n",
      "Train Epoch: 279 [42240/54000 (78%)] Loss: -229768.421875\n",
      "Train Epoch: 279 [43648/54000 (81%)] Loss: -215382.203125\n",
      "Train Epoch: 279 [45056/54000 (83%)] Loss: -246175.781250\n",
      "Train Epoch: 279 [46464/54000 (86%)] Loss: -219948.921875\n",
      "Train Epoch: 279 [47872/54000 (89%)] Loss: -221852.843750\n",
      "Train Epoch: 279 [49280/54000 (91%)] Loss: -229694.687500\n",
      "Train Epoch: 279 [50688/54000 (94%)] Loss: -228361.343750\n",
      "Train Epoch: 279 [52096/54000 (96%)] Loss: -221176.687500\n",
      "    epoch          : 279\n",
      "    loss           : -226002.98714114833\n",
      "    val_loss       : -229229.960598085\n",
      "Train Epoch: 280 [0/54000 (0%)] Loss: -229260.328125\n",
      "Train Epoch: 280 [1408/54000 (3%)] Loss: -226945.937500\n",
      "Train Epoch: 280 [2816/54000 (5%)] Loss: -219201.093750\n",
      "Train Epoch: 280 [4224/54000 (8%)] Loss: -229561.234375\n",
      "Train Epoch: 280 [5632/54000 (10%)] Loss: -229074.750000\n",
      "Train Epoch: 280 [7040/54000 (13%)] Loss: -216927.562500\n",
      "Train Epoch: 280 [8448/54000 (16%)] Loss: -221236.093750\n",
      "Train Epoch: 280 [9856/54000 (18%)] Loss: -247413.593750\n",
      "Train Epoch: 280 [11264/54000 (21%)] Loss: -225163.234375\n",
      "Train Epoch: 280 [12672/54000 (23%)] Loss: -227596.109375\n",
      "Train Epoch: 280 [14080/54000 (26%)] Loss: -229132.531250\n",
      "Train Epoch: 280 [15488/54000 (29%)] Loss: -230015.781250\n",
      "Train Epoch: 280 [16896/54000 (31%)] Loss: -228438.921875\n",
      "Train Epoch: 280 [18304/54000 (34%)] Loss: -218842.531250\n",
      "Train Epoch: 280 [19712/54000 (37%)] Loss: -216066.734375\n",
      "Train Epoch: 280 [21120/54000 (39%)] Loss: -213778.171875\n",
      "Train Epoch: 280 [22528/54000 (42%)] Loss: -223759.296875\n",
      "Train Epoch: 280 [23936/54000 (44%)] Loss: -225291.468750\n",
      "Train Epoch: 280 [25344/54000 (47%)] Loss: -226695.187500\n",
      "Train Epoch: 280 [26752/54000 (50%)] Loss: -220418.187500\n",
      "Train Epoch: 280 [28160/54000 (52%)] Loss: -246782.265625\n",
      "Train Epoch: 280 [29568/54000 (55%)] Loss: -220109.625000\n",
      "Train Epoch: 280 [30976/54000 (57%)] Loss: -229137.000000\n",
      "Train Epoch: 280 [32384/54000 (60%)] Loss: -232190.375000\n",
      "Train Epoch: 280 [33792/54000 (63%)] Loss: -224919.046875\n",
      "Train Epoch: 280 [35200/54000 (65%)] Loss: -228754.968750\n",
      "Train Epoch: 280 [36608/54000 (68%)] Loss: -221736.812500\n",
      "Train Epoch: 280 [38016/54000 (70%)] Loss: -217740.796875\n",
      "Train Epoch: 280 [39424/54000 (73%)] Loss: -222506.046875\n",
      "Train Epoch: 280 [40832/54000 (76%)] Loss: -219448.390625\n",
      "Train Epoch: 280 [42240/54000 (78%)] Loss: -227222.812500\n",
      "Train Epoch: 280 [43648/54000 (81%)] Loss: -229763.968750\n",
      "Train Epoch: 280 [45056/54000 (83%)] Loss: -246107.718750\n",
      "Train Epoch: 280 [46464/54000 (86%)] Loss: -223690.125000\n",
      "Train Epoch: 280 [47872/54000 (89%)] Loss: -225059.625000\n",
      "Train Epoch: 280 [49280/54000 (91%)] Loss: -223931.640625\n",
      "Train Epoch: 280 [50688/54000 (94%)] Loss: -230475.234375\n",
      "Train Epoch: 280 [52096/54000 (96%)] Loss: -249868.546875\n",
      "    epoch          : 280\n",
      "    loss           : -225823.31223833733\n",
      "    val_loss       : -229538.63954720847\n",
      "Train Epoch: 281 [0/54000 (0%)] Loss: -249116.156250\n",
      "Train Epoch: 281 [1408/54000 (3%)] Loss: -220776.468750\n",
      "Train Epoch: 281 [2816/54000 (5%)] Loss: -219232.671875\n",
      "Train Epoch: 281 [4224/54000 (8%)] Loss: -222078.187500\n",
      "Train Epoch: 281 [5632/54000 (10%)] Loss: -221676.859375\n",
      "Train Epoch: 281 [7040/54000 (13%)] Loss: -220140.890625\n",
      "Train Epoch: 281 [8448/54000 (16%)] Loss: -223889.312500\n",
      "Train Epoch: 281 [9856/54000 (18%)] Loss: -220663.218750\n",
      "Train Epoch: 281 [11264/54000 (21%)] Loss: -248596.859375\n",
      "Train Epoch: 281 [12672/54000 (23%)] Loss: -210116.140625\n",
      "Train Epoch: 281 [14080/54000 (26%)] Loss: -219951.562500\n",
      "Train Epoch: 281 [15488/54000 (29%)] Loss: -230683.140625\n",
      "Train Epoch: 281 [16896/54000 (31%)] Loss: -219531.312500\n",
      "Train Epoch: 281 [18304/54000 (34%)] Loss: -221511.281250\n",
      "Train Epoch: 281 [19712/54000 (37%)] Loss: -230659.781250\n",
      "Train Epoch: 281 [21120/54000 (39%)] Loss: -247711.828125\n",
      "Train Epoch: 281 [22528/54000 (42%)] Loss: -221813.687500\n",
      "Train Epoch: 281 [23936/54000 (44%)] Loss: -214688.812500\n",
      "Train Epoch: 281 [25344/54000 (47%)] Loss: -222759.156250\n",
      "Train Epoch: 281 [26752/54000 (50%)] Loss: -228573.546875\n",
      "Train Epoch: 281 [28160/54000 (52%)] Loss: -248135.468750\n",
      "Train Epoch: 281 [29568/54000 (55%)] Loss: -227251.968750\n",
      "Train Epoch: 281 [30976/54000 (57%)] Loss: -215233.734375\n",
      "Train Epoch: 281 [32384/54000 (60%)] Loss: -224541.046875\n",
      "Train Epoch: 281 [33792/54000 (63%)] Loss: -220437.656250\n",
      "Train Epoch: 281 [35200/54000 (65%)] Loss: -217175.343750\n",
      "Train Epoch: 281 [36608/54000 (68%)] Loss: -220651.312500\n",
      "Train Epoch: 281 [38016/54000 (70%)] Loss: -220111.484375\n",
      "Train Epoch: 281 [39424/54000 (73%)] Loss: -225643.531250\n",
      "Train Epoch: 281 [40832/54000 (76%)] Loss: -219413.671875\n",
      "Train Epoch: 281 [42240/54000 (78%)] Loss: -221730.250000\n",
      "Train Epoch: 281 [43648/54000 (81%)] Loss: -220968.734375\n",
      "Train Epoch: 281 [45056/54000 (83%)] Loss: -247492.437500\n",
      "Train Epoch: 281 [46464/54000 (86%)] Loss: -218276.734375\n",
      "Train Epoch: 281 [47872/54000 (89%)] Loss: -218439.890625\n",
      "Train Epoch: 281 [49280/54000 (91%)] Loss: -226648.656250\n",
      "Train Epoch: 281 [50688/54000 (94%)] Loss: -227750.578125\n",
      "Train Epoch: 281 [52096/54000 (96%)] Loss: -246653.859375\n",
      "    epoch          : 281\n",
      "    loss           : -225896.9772353469\n",
      "    val_loss       : -229345.24189572217\n",
      "Train Epoch: 282 [0/54000 (0%)] Loss: -248794.265625\n",
      "Train Epoch: 282 [1408/54000 (3%)] Loss: -221792.031250\n",
      "Train Epoch: 282 [2816/54000 (5%)] Loss: -222041.156250\n",
      "Train Epoch: 282 [4224/54000 (8%)] Loss: -222619.953125\n",
      "Train Epoch: 282 [5632/54000 (10%)] Loss: -222981.562500\n",
      "Train Epoch: 282 [7040/54000 (13%)] Loss: -219220.828125\n",
      "Train Epoch: 282 [8448/54000 (16%)] Loss: -228367.578125\n",
      "Train Epoch: 282 [9856/54000 (18%)] Loss: -225824.750000\n",
      "Train Epoch: 282 [11264/54000 (21%)] Loss: -216288.921875\n",
      "Train Epoch: 282 [12672/54000 (23%)] Loss: -219303.156250\n",
      "Train Epoch: 282 [14080/54000 (26%)] Loss: -220254.234375\n",
      "Train Epoch: 282 [15488/54000 (29%)] Loss: -229930.640625\n",
      "Train Epoch: 282 [16896/54000 (31%)] Loss: -248225.781250\n",
      "Train Epoch: 282 [18304/54000 (34%)] Loss: -231561.031250\n",
      "Train Epoch: 282 [19712/54000 (37%)] Loss: -229907.500000\n",
      "Train Epoch: 282 [21120/54000 (39%)] Loss: -225280.734375\n",
      "Train Epoch: 282 [22528/54000 (42%)] Loss: -219378.984375\n",
      "Train Epoch: 282 [23936/54000 (44%)] Loss: -226889.625000\n",
      "Train Epoch: 282 [25344/54000 (47%)] Loss: -230217.765625\n",
      "Train Epoch: 282 [26752/54000 (50%)] Loss: -231110.562500\n",
      "Train Epoch: 282 [28160/54000 (52%)] Loss: -223513.609375\n",
      "Train Epoch: 282 [29568/54000 (55%)] Loss: -248612.484375\n",
      "Train Epoch: 282 [30976/54000 (57%)] Loss: -223363.375000\n",
      "Train Epoch: 282 [32384/54000 (60%)] Loss: -230291.703125\n",
      "Train Epoch: 282 [33792/54000 (63%)] Loss: -228586.296875\n",
      "Train Epoch: 282 [35200/54000 (65%)] Loss: -218353.250000\n",
      "Train Epoch: 282 [36608/54000 (68%)] Loss: -219803.062500\n",
      "Train Epoch: 282 [38016/54000 (70%)] Loss: -246548.906250\n",
      "Train Epoch: 282 [39424/54000 (73%)] Loss: -222475.875000\n",
      "Train Epoch: 282 [40832/54000 (76%)] Loss: -230526.843750\n",
      "Train Epoch: 282 [42240/54000 (78%)] Loss: -220897.781250\n",
      "Train Epoch: 282 [43648/54000 (81%)] Loss: -221711.968750\n",
      "Train Epoch: 282 [45056/54000 (83%)] Loss: -219879.625000\n",
      "Train Epoch: 282 [46464/54000 (86%)] Loss: -246654.078125\n",
      "Train Epoch: 282 [47872/54000 (89%)] Loss: -220468.953125\n",
      "Train Epoch: 282 [49280/54000 (91%)] Loss: -220127.703125\n",
      "Train Epoch: 282 [50688/54000 (94%)] Loss: -246272.375000\n",
      "Train Epoch: 282 [52096/54000 (96%)] Loss: -229372.968750\n",
      "    epoch          : 282\n",
      "    loss           : -226128.45970394736\n",
      "    val_loss       : -229392.84787657202\n",
      "Train Epoch: 283 [0/54000 (0%)] Loss: -245878.625000\n",
      "Train Epoch: 283 [1408/54000 (3%)] Loss: -223544.921875\n",
      "Train Epoch: 283 [2816/54000 (5%)] Loss: -221555.265625\n",
      "Train Epoch: 283 [4224/54000 (8%)] Loss: -215030.031250\n",
      "Train Epoch: 283 [5632/54000 (10%)] Loss: -221372.406250\n",
      "Train Epoch: 283 [7040/54000 (13%)] Loss: -247144.750000\n",
      "Train Epoch: 283 [8448/54000 (16%)] Loss: -216196.000000\n",
      "Train Epoch: 283 [9856/54000 (18%)] Loss: -224325.968750\n",
      "Train Epoch: 283 [11264/54000 (21%)] Loss: -223954.843750\n",
      "Train Epoch: 283 [12672/54000 (23%)] Loss: -225688.437500\n",
      "Train Epoch: 283 [14080/54000 (26%)] Loss: -219391.953125\n",
      "Train Epoch: 283 [15488/54000 (29%)] Loss: -231146.593750\n",
      "Train Epoch: 283 [16896/54000 (31%)] Loss: -227787.265625\n",
      "Train Epoch: 283 [18304/54000 (34%)] Loss: -213753.906250\n",
      "Train Epoch: 283 [19712/54000 (37%)] Loss: -218314.656250\n",
      "Train Epoch: 283 [21120/54000 (39%)] Loss: -224403.406250\n",
      "Train Epoch: 283 [22528/54000 (42%)] Loss: -230356.203125\n",
      "Train Epoch: 283 [23936/54000 (44%)] Loss: -225114.140625\n",
      "Train Epoch: 283 [25344/54000 (47%)] Loss: -223546.812500\n",
      "Train Epoch: 283 [26752/54000 (50%)] Loss: -249053.640625\n",
      "Train Epoch: 283 [28160/54000 (52%)] Loss: -218557.687500\n",
      "Train Epoch: 283 [29568/54000 (55%)] Loss: -231313.984375\n",
      "Train Epoch: 283 [30976/54000 (57%)] Loss: -228646.140625\n",
      "Train Epoch: 283 [32384/54000 (60%)] Loss: -224706.171875\n",
      "Train Epoch: 283 [33792/54000 (63%)] Loss: -247276.953125\n",
      "Train Epoch: 283 [35200/54000 (65%)] Loss: -216662.593750\n",
      "Train Epoch: 283 [36608/54000 (68%)] Loss: -230773.718750\n",
      "Train Epoch: 283 [38016/54000 (70%)] Loss: -247012.421875\n",
      "Train Epoch: 283 [39424/54000 (73%)] Loss: -227928.500000\n",
      "Train Epoch: 283 [40832/54000 (76%)] Loss: -224355.625000\n",
      "Train Epoch: 283 [42240/54000 (78%)] Loss: -219401.328125\n",
      "Train Epoch: 283 [43648/54000 (81%)] Loss: -245411.250000\n",
      "Train Epoch: 283 [45056/54000 (83%)] Loss: -227103.156250\n",
      "Train Epoch: 283 [46464/54000 (86%)] Loss: -216053.671875\n",
      "Train Epoch: 283 [47872/54000 (89%)] Loss: -224354.781250\n",
      "Train Epoch: 283 [49280/54000 (91%)] Loss: -246192.453125\n",
      "Train Epoch: 283 [50688/54000 (94%)] Loss: -223604.843750\n",
      "Train Epoch: 283 [52096/54000 (96%)] Loss: -226394.390625\n",
      "    epoch          : 283\n",
      "    loss           : -225985.34965610047\n",
      "    val_loss       : -229338.5405094798\n",
      "Train Epoch: 284 [0/54000 (0%)] Loss: -227021.281250\n",
      "Train Epoch: 284 [1408/54000 (3%)] Loss: -229537.906250\n",
      "Train Epoch: 284 [2816/54000 (5%)] Loss: -221432.656250\n",
      "Train Epoch: 284 [4224/54000 (8%)] Loss: -217605.515625\n",
      "Train Epoch: 284 [5632/54000 (10%)] Loss: -217471.218750\n",
      "Train Epoch: 284 [7040/54000 (13%)] Loss: -215111.828125\n",
      "Train Epoch: 284 [8448/54000 (16%)] Loss: -217703.984375\n",
      "Train Epoch: 284 [9856/54000 (18%)] Loss: -215422.203125\n",
      "Train Epoch: 284 [11264/54000 (21%)] Loss: -226062.218750\n",
      "Train Epoch: 284 [12672/54000 (23%)] Loss: -230107.125000\n",
      "Train Epoch: 284 [14080/54000 (26%)] Loss: -220630.859375\n",
      "Train Epoch: 284 [15488/54000 (29%)] Loss: -222959.687500\n",
      "Train Epoch: 284 [16896/54000 (31%)] Loss: -222850.531250\n",
      "Train Epoch: 284 [18304/54000 (34%)] Loss: -214234.453125\n",
      "Train Epoch: 284 [19712/54000 (37%)] Loss: -247606.593750\n",
      "Train Epoch: 284 [21120/54000 (39%)] Loss: -219046.625000\n",
      "Train Epoch: 284 [22528/54000 (42%)] Loss: -215529.140625\n",
      "Train Epoch: 284 [23936/54000 (44%)] Loss: -224938.687500\n",
      "Train Epoch: 284 [25344/54000 (47%)] Loss: -216657.343750\n",
      "Train Epoch: 284 [26752/54000 (50%)] Loss: -247204.265625\n",
      "Train Epoch: 284 [28160/54000 (52%)] Loss: -217600.203125\n",
      "Train Epoch: 284 [29568/54000 (55%)] Loss: -218338.656250\n",
      "Train Epoch: 284 [30976/54000 (57%)] Loss: -227053.781250\n",
      "Train Epoch: 284 [32384/54000 (60%)] Loss: -230731.828125\n",
      "Train Epoch: 284 [33792/54000 (63%)] Loss: -219512.062500\n",
      "Train Epoch: 284 [35200/54000 (65%)] Loss: -225498.953125\n",
      "Train Epoch: 284 [36608/54000 (68%)] Loss: -248759.890625\n",
      "Train Epoch: 284 [38016/54000 (70%)] Loss: -215816.296875\n",
      "Train Epoch: 284 [39424/54000 (73%)] Loss: -219619.343750\n",
      "Train Epoch: 284 [40832/54000 (76%)] Loss: -229688.515625\n",
      "Train Epoch: 284 [42240/54000 (78%)] Loss: -244985.406250\n",
      "Train Epoch: 284 [43648/54000 (81%)] Loss: -224639.468750\n",
      "Train Epoch: 284 [45056/54000 (83%)] Loss: -228135.265625\n",
      "Train Epoch: 284 [46464/54000 (86%)] Loss: -220155.156250\n",
      "Train Epoch: 284 [47872/54000 (89%)] Loss: -219712.609375\n",
      "Train Epoch: 284 [49280/54000 (91%)] Loss: -221063.687500\n",
      "Train Epoch: 284 [50688/54000 (94%)] Loss: -217982.000000\n",
      "Train Epoch: 284 [52096/54000 (96%)] Loss: -216654.703125\n",
      "    epoch          : 284\n",
      "    loss           : -226021.27044706937\n",
      "    val_loss       : -229832.50198289825\n",
      "Train Epoch: 285 [0/54000 (0%)] Loss: -222977.296875\n",
      "Train Epoch: 285 [1408/54000 (3%)] Loss: -225343.578125\n",
      "Train Epoch: 285 [2816/54000 (5%)] Loss: -216262.453125\n",
      "Train Epoch: 285 [4224/54000 (8%)] Loss: -230358.500000\n",
      "Train Epoch: 285 [5632/54000 (10%)] Loss: -223349.250000\n",
      "Train Epoch: 285 [7040/54000 (13%)] Loss: -224228.312500\n",
      "Train Epoch: 285 [8448/54000 (16%)] Loss: -218355.375000\n",
      "Train Epoch: 285 [9856/54000 (18%)] Loss: -220343.062500\n",
      "Train Epoch: 285 [11264/54000 (21%)] Loss: -220693.437500\n",
      "Train Epoch: 285 [12672/54000 (23%)] Loss: -219270.453125\n",
      "Train Epoch: 285 [14080/54000 (26%)] Loss: -221106.656250\n",
      "Train Epoch: 285 [15488/54000 (29%)] Loss: -219312.218750\n",
      "Train Epoch: 285 [16896/54000 (31%)] Loss: -224145.781250\n",
      "Train Epoch: 285 [18304/54000 (34%)] Loss: -215528.828125\n",
      "Train Epoch: 285 [19712/54000 (37%)] Loss: -249151.812500\n",
      "Train Epoch: 285 [21120/54000 (39%)] Loss: -230552.937500\n",
      "Train Epoch: 285 [22528/54000 (42%)] Loss: -223233.500000\n",
      "Train Epoch: 285 [23936/54000 (44%)] Loss: -228735.125000\n",
      "Train Epoch: 285 [25344/54000 (47%)] Loss: -225195.781250\n",
      "Train Epoch: 285 [26752/54000 (50%)] Loss: -248342.968750\n",
      "Train Epoch: 285 [28160/54000 (52%)] Loss: -213495.046875\n",
      "Train Epoch: 285 [29568/54000 (55%)] Loss: -227437.375000\n",
      "Train Epoch: 285 [30976/54000 (57%)] Loss: -229097.953125\n",
      "Train Epoch: 285 [32384/54000 (60%)] Loss: -246937.968750\n",
      "Train Epoch: 285 [33792/54000 (63%)] Loss: -221740.406250\n",
      "Train Epoch: 285 [35200/54000 (65%)] Loss: -224718.875000\n",
      "Train Epoch: 285 [36608/54000 (68%)] Loss: -220943.156250\n",
      "Train Epoch: 285 [38016/54000 (70%)] Loss: -246193.281250\n",
      "Train Epoch: 285 [39424/54000 (73%)] Loss: -219454.921875\n",
      "Train Epoch: 285 [40832/54000 (76%)] Loss: -219045.406250\n",
      "Train Epoch: 285 [42240/54000 (78%)] Loss: -248243.875000\n",
      "Train Epoch: 285 [43648/54000 (81%)] Loss: -229682.890625\n",
      "Train Epoch: 285 [45056/54000 (83%)] Loss: -221491.281250\n",
      "Train Epoch: 285 [46464/54000 (86%)] Loss: -221853.593750\n",
      "Train Epoch: 285 [47872/54000 (89%)] Loss: -218822.062500\n",
      "Train Epoch: 285 [49280/54000 (91%)] Loss: -221175.656250\n",
      "Train Epoch: 285 [50688/54000 (94%)] Loss: -219898.859375\n",
      "Train Epoch: 285 [52096/54000 (96%)] Loss: -221035.609375\n",
      "    epoch          : 285\n",
      "    loss           : -226141.83126495214\n",
      "    val_loss       : -229522.36472227515\n",
      "Train Epoch: 286 [0/54000 (0%)] Loss: -217683.859375\n",
      "Train Epoch: 286 [1408/54000 (3%)] Loss: -248368.796875\n",
      "Train Epoch: 286 [2816/54000 (5%)] Loss: -223852.906250\n",
      "Train Epoch: 286 [4224/54000 (8%)] Loss: -221197.937500\n",
      "Train Epoch: 286 [5632/54000 (10%)] Loss: -222031.640625\n",
      "Train Epoch: 286 [7040/54000 (13%)] Loss: -229214.593750\n",
      "Train Epoch: 286 [8448/54000 (16%)] Loss: -229349.515625\n",
      "Train Epoch: 286 [9856/54000 (18%)] Loss: -223041.562500\n",
      "Train Epoch: 286 [11264/54000 (21%)] Loss: -231599.765625\n",
      "Train Epoch: 286 [12672/54000 (23%)] Loss: -226364.218750\n",
      "Train Epoch: 286 [14080/54000 (26%)] Loss: -220837.328125\n",
      "Train Epoch: 286 [15488/54000 (29%)] Loss: -224500.000000\n",
      "Train Epoch: 286 [16896/54000 (31%)] Loss: -215644.609375\n",
      "Train Epoch: 286 [18304/54000 (34%)] Loss: -230007.765625\n",
      "Train Epoch: 286 [19712/54000 (37%)] Loss: -221092.156250\n",
      "Train Epoch: 286 [21120/54000 (39%)] Loss: -217688.875000\n",
      "Train Epoch: 286 [22528/54000 (42%)] Loss: -225156.593750\n",
      "Train Epoch: 286 [23936/54000 (44%)] Loss: -219636.187500\n",
      "Train Epoch: 286 [25344/54000 (47%)] Loss: -221790.125000\n",
      "Train Epoch: 286 [26752/54000 (50%)] Loss: -219558.093750\n",
      "Train Epoch: 286 [28160/54000 (52%)] Loss: -220873.359375\n",
      "Train Epoch: 286 [29568/54000 (55%)] Loss: -227343.937500\n",
      "Train Epoch: 286 [30976/54000 (57%)] Loss: -229787.218750\n",
      "Train Epoch: 286 [32384/54000 (60%)] Loss: -221768.125000\n",
      "Train Epoch: 286 [33792/54000 (63%)] Loss: -249103.531250\n",
      "Train Epoch: 286 [35200/54000 (65%)] Loss: -218055.281250\n",
      "Train Epoch: 286 [36608/54000 (68%)] Loss: -229548.156250\n",
      "Train Epoch: 286 [38016/54000 (70%)] Loss: -230354.218750\n",
      "Train Epoch: 286 [39424/54000 (73%)] Loss: -248498.296875\n",
      "Train Epoch: 286 [40832/54000 (76%)] Loss: -228509.640625\n",
      "Train Epoch: 286 [42240/54000 (78%)] Loss: -228869.906250\n",
      "Train Epoch: 286 [43648/54000 (81%)] Loss: -225723.250000\n",
      "Train Epoch: 286 [45056/54000 (83%)] Loss: -226265.578125\n",
      "Train Epoch: 286 [46464/54000 (86%)] Loss: -217556.265625\n",
      "Train Epoch: 286 [47872/54000 (89%)] Loss: -225335.031250\n",
      "Train Epoch: 286 [49280/54000 (91%)] Loss: -248114.500000\n",
      "Train Epoch: 286 [50688/54000 (94%)] Loss: -220719.562500\n",
      "Train Epoch: 286 [52096/54000 (96%)] Loss: -218647.609375\n",
      "    epoch          : 286\n",
      "    loss           : -226097.68006130384\n",
      "    val_loss       : -229814.0580876048\n",
      "Train Epoch: 287 [0/54000 (0%)] Loss: -223527.843750\n",
      "Train Epoch: 287 [1408/54000 (3%)] Loss: -229137.109375\n",
      "Train Epoch: 287 [2816/54000 (5%)] Loss: -220626.875000\n",
      "Train Epoch: 287 [4224/54000 (8%)] Loss: -230796.281250\n",
      "Train Epoch: 287 [5632/54000 (10%)] Loss: -229732.937500\n",
      "Train Epoch: 287 [7040/54000 (13%)] Loss: -224968.234375\n",
      "Train Epoch: 287 [8448/54000 (16%)] Loss: -223449.593750\n",
      "Train Epoch: 287 [9856/54000 (18%)] Loss: -246241.593750\n",
      "Train Epoch: 287 [11264/54000 (21%)] Loss: -230868.156250\n",
      "Train Epoch: 287 [12672/54000 (23%)] Loss: -228601.187500\n",
      "Train Epoch: 287 [14080/54000 (26%)] Loss: -227042.203125\n",
      "Train Epoch: 287 [15488/54000 (29%)] Loss: -220317.593750\n",
      "Train Epoch: 287 [16896/54000 (31%)] Loss: -249467.000000\n",
      "Train Epoch: 287 [18304/54000 (34%)] Loss: -227322.890625\n",
      "Train Epoch: 287 [19712/54000 (37%)] Loss: -213853.859375\n",
      "Train Epoch: 287 [21120/54000 (39%)] Loss: -223487.000000\n",
      "Train Epoch: 287 [22528/54000 (42%)] Loss: -224620.140625\n",
      "Train Epoch: 287 [23936/54000 (44%)] Loss: -215688.203125\n",
      "Train Epoch: 287 [25344/54000 (47%)] Loss: -232636.625000\n",
      "Train Epoch: 287 [26752/54000 (50%)] Loss: -221404.031250\n",
      "Train Epoch: 287 [28160/54000 (52%)] Loss: -221200.984375\n",
      "Train Epoch: 287 [29568/54000 (55%)] Loss: -228871.890625\n",
      "Train Epoch: 287 [30976/54000 (57%)] Loss: -223474.000000\n",
      "Train Epoch: 287 [32384/54000 (60%)] Loss: -227983.390625\n",
      "Train Epoch: 287 [33792/54000 (63%)] Loss: -217741.484375\n",
      "Train Epoch: 287 [35200/54000 (65%)] Loss: -223856.281250\n",
      "Train Epoch: 287 [36608/54000 (68%)] Loss: -216945.500000\n",
      "Train Epoch: 287 [38016/54000 (70%)] Loss: -218396.187500\n",
      "Train Epoch: 287 [39424/54000 (73%)] Loss: -221780.343750\n",
      "Train Epoch: 287 [40832/54000 (76%)] Loss: -230471.812500\n",
      "Train Epoch: 287 [42240/54000 (78%)] Loss: -222460.500000\n",
      "Train Epoch: 287 [43648/54000 (81%)] Loss: -228715.984375\n",
      "Train Epoch: 287 [45056/54000 (83%)] Loss: -220936.890625\n",
      "Train Epoch: 287 [46464/54000 (86%)] Loss: -222201.031250\n",
      "Train Epoch: 287 [47872/54000 (89%)] Loss: -224317.375000\n",
      "Train Epoch: 287 [49280/54000 (91%)] Loss: -227544.218750\n",
      "Train Epoch: 287 [50688/54000 (94%)] Loss: -214661.156250\n",
      "Train Epoch: 287 [52096/54000 (96%)] Loss: -230065.453125\n",
      "    epoch          : 287\n",
      "    loss           : -226111.45017194975\n",
      "    val_loss       : -229377.24564119664\n",
      "Train Epoch: 288 [0/54000 (0%)] Loss: -224206.656250\n",
      "Train Epoch: 288 [1408/54000 (3%)] Loss: -248468.671875\n",
      "Train Epoch: 288 [2816/54000 (5%)] Loss: -227369.234375\n",
      "Train Epoch: 288 [4224/54000 (8%)] Loss: -228429.390625\n",
      "Train Epoch: 288 [5632/54000 (10%)] Loss: -217460.515625\n",
      "Train Epoch: 288 [7040/54000 (13%)] Loss: -228518.000000\n",
      "Train Epoch: 288 [8448/54000 (16%)] Loss: -228312.734375\n",
      "Train Epoch: 288 [9856/54000 (18%)] Loss: -217281.312500\n",
      "Train Epoch: 288 [11264/54000 (21%)] Loss: -226585.218750\n",
      "Train Epoch: 288 [12672/54000 (23%)] Loss: -228025.125000\n",
      "Train Epoch: 288 [14080/54000 (26%)] Loss: -221368.656250\n",
      "Train Epoch: 288 [15488/54000 (29%)] Loss: -217222.468750\n",
      "Train Epoch: 288 [16896/54000 (31%)] Loss: -247353.234375\n",
      "Train Epoch: 288 [18304/54000 (34%)] Loss: -230868.296875\n",
      "Train Epoch: 288 [19712/54000 (37%)] Loss: -230616.109375\n",
      "Train Epoch: 288 [21120/54000 (39%)] Loss: -216393.921875\n",
      "Train Epoch: 288 [22528/54000 (42%)] Loss: -215917.859375\n",
      "Train Epoch: 288 [23936/54000 (44%)] Loss: -216003.937500\n",
      "Train Epoch: 288 [25344/54000 (47%)] Loss: -231059.578125\n",
      "Train Epoch: 288 [26752/54000 (50%)] Loss: -224132.562500\n",
      "Train Epoch: 288 [28160/54000 (52%)] Loss: -222969.468750\n",
      "Train Epoch: 288 [29568/54000 (55%)] Loss: -221592.703125\n",
      "Train Epoch: 288 [30976/54000 (57%)] Loss: -228163.406250\n",
      "Train Epoch: 288 [32384/54000 (60%)] Loss: -250080.000000\n",
      "Train Epoch: 288 [33792/54000 (63%)] Loss: -221310.140625\n",
      "Train Epoch: 288 [35200/54000 (65%)] Loss: -221758.937500\n",
      "Train Epoch: 288 [36608/54000 (68%)] Loss: -219238.921875\n",
      "Train Epoch: 288 [38016/54000 (70%)] Loss: -230361.734375\n",
      "Train Epoch: 288 [39424/54000 (73%)] Loss: -230839.187500\n",
      "Train Epoch: 288 [40832/54000 (76%)] Loss: -229542.312500\n",
      "Train Epoch: 288 [42240/54000 (78%)] Loss: -219335.765625\n",
      "Train Epoch: 288 [43648/54000 (81%)] Loss: -221118.609375\n",
      "Train Epoch: 288 [45056/54000 (83%)] Loss: -220281.578125\n",
      "Train Epoch: 288 [46464/54000 (86%)] Loss: -221703.890625\n",
      "Train Epoch: 288 [47872/54000 (89%)] Loss: -223174.343750\n",
      "Train Epoch: 288 [49280/54000 (91%)] Loss: -230325.890625\n",
      "Train Epoch: 288 [50688/54000 (94%)] Loss: -215292.375000\n",
      "Train Epoch: 288 [52096/54000 (96%)] Loss: -221070.203125\n",
      "    epoch          : 288\n",
      "    loss           : -226055.45630233255\n",
      "    val_loss       : -229745.88365806022\n",
      "Train Epoch: 289 [0/54000 (0%)] Loss: -223198.671875\n",
      "Train Epoch: 289 [1408/54000 (3%)] Loss: -224335.000000\n",
      "Train Epoch: 289 [2816/54000 (5%)] Loss: -229919.484375\n",
      "Train Epoch: 289 [4224/54000 (8%)] Loss: -230127.250000\n",
      "Train Epoch: 289 [5632/54000 (10%)] Loss: -221715.578125\n",
      "Train Epoch: 289 [7040/54000 (13%)] Loss: -246660.203125\n",
      "Train Epoch: 289 [8448/54000 (16%)] Loss: -222210.156250\n",
      "Train Epoch: 289 [9856/54000 (18%)] Loss: -231496.515625\n",
      "Train Epoch: 289 [11264/54000 (21%)] Loss: -216486.546875\n",
      "Train Epoch: 289 [12672/54000 (23%)] Loss: -216484.796875\n",
      "Train Epoch: 289 [14080/54000 (26%)] Loss: -217418.296875\n",
      "Train Epoch: 289 [15488/54000 (29%)] Loss: -223595.187500\n",
      "Train Epoch: 289 [16896/54000 (31%)] Loss: -217764.671875\n",
      "Train Epoch: 289 [18304/54000 (34%)] Loss: -225244.593750\n",
      "Train Epoch: 289 [19712/54000 (37%)] Loss: -220729.000000\n",
      "Train Epoch: 289 [21120/54000 (39%)] Loss: -225366.250000\n",
      "Train Epoch: 289 [22528/54000 (42%)] Loss: -227861.984375\n",
      "Train Epoch: 289 [23936/54000 (44%)] Loss: -217942.687500\n",
      "Train Epoch: 289 [25344/54000 (47%)] Loss: -226585.718750\n",
      "Train Epoch: 289 [26752/54000 (50%)] Loss: -227593.937500\n",
      "Train Epoch: 289 [28160/54000 (52%)] Loss: -230796.984375\n",
      "Train Epoch: 289 [29568/54000 (55%)] Loss: -220901.000000\n",
      "Train Epoch: 289 [30976/54000 (57%)] Loss: -212495.953125\n",
      "Train Epoch: 289 [32384/54000 (60%)] Loss: -217744.468750\n",
      "Train Epoch: 289 [33792/54000 (63%)] Loss: -228384.765625\n",
      "Train Epoch: 289 [35200/54000 (65%)] Loss: -220109.812500\n",
      "Train Epoch: 289 [36608/54000 (68%)] Loss: -225989.468750\n",
      "Train Epoch: 289 [38016/54000 (70%)] Loss: -223034.906250\n",
      "Train Epoch: 289 [39424/54000 (73%)] Loss: -224215.312500\n",
      "Train Epoch: 289 [40832/54000 (76%)] Loss: -228837.156250\n",
      "Train Epoch: 289 [42240/54000 (78%)] Loss: -228947.171875\n",
      "Train Epoch: 289 [43648/54000 (81%)] Loss: -227287.656250\n",
      "Train Epoch: 289 [45056/54000 (83%)] Loss: -218673.093750\n",
      "Train Epoch: 289 [46464/54000 (86%)] Loss: -212238.187500\n",
      "Train Epoch: 289 [47872/54000 (89%)] Loss: -229904.781250\n",
      "Train Epoch: 289 [49280/54000 (91%)] Loss: -214331.937500\n",
      "Train Epoch: 289 [50688/54000 (94%)] Loss: -218015.625000\n",
      "Train Epoch: 289 [52096/54000 (96%)] Loss: -229679.093750\n",
      "    epoch          : 289\n",
      "    loss           : -226081.2180771531\n",
      "    val_loss       : -229806.32206912158\n",
      "Train Epoch: 290 [0/54000 (0%)] Loss: -246892.656250\n",
      "Train Epoch: 290 [1408/54000 (3%)] Loss: -248147.812500\n",
      "Train Epoch: 290 [2816/54000 (5%)] Loss: -217080.234375\n",
      "Train Epoch: 290 [4224/54000 (8%)] Loss: -219793.156250\n",
      "Train Epoch: 290 [5632/54000 (10%)] Loss: -215946.000000\n",
      "Train Epoch: 290 [7040/54000 (13%)] Loss: -224022.375000\n",
      "Train Epoch: 290 [8448/54000 (16%)] Loss: -219119.937500\n",
      "Train Epoch: 290 [9856/54000 (18%)] Loss: -220432.718750\n",
      "Train Epoch: 290 [11264/54000 (21%)] Loss: -231282.875000\n",
      "Train Epoch: 290 [12672/54000 (23%)] Loss: -230492.218750\n",
      "Train Epoch: 290 [14080/54000 (26%)] Loss: -230396.640625\n",
      "Train Epoch: 290 [15488/54000 (29%)] Loss: -218224.437500\n",
      "Train Epoch: 290 [16896/54000 (31%)] Loss: -217084.687500\n",
      "Train Epoch: 290 [18304/54000 (34%)] Loss: -249351.218750\n",
      "Train Epoch: 290 [19712/54000 (37%)] Loss: -222987.828125\n",
      "Train Epoch: 290 [21120/54000 (39%)] Loss: -227040.062500\n",
      "Train Epoch: 290 [22528/54000 (42%)] Loss: -226563.000000\n",
      "Train Epoch: 290 [23936/54000 (44%)] Loss: -219805.453125\n",
      "Train Epoch: 290 [25344/54000 (47%)] Loss: -225048.078125\n",
      "Train Epoch: 290 [26752/54000 (50%)] Loss: -218117.093750\n",
      "Train Epoch: 290 [28160/54000 (52%)] Loss: -231361.937500\n",
      "Train Epoch: 290 [29568/54000 (55%)] Loss: -221256.828125\n",
      "Train Epoch: 290 [30976/54000 (57%)] Loss: -247669.062500\n",
      "Train Epoch: 290 [32384/54000 (60%)] Loss: -225373.796875\n",
      "Train Epoch: 290 [33792/54000 (63%)] Loss: -225361.546875\n",
      "Train Epoch: 290 [35200/54000 (65%)] Loss: -221898.875000\n",
      "Train Epoch: 290 [36608/54000 (68%)] Loss: -249653.281250\n",
      "Train Epoch: 290 [38016/54000 (70%)] Loss: -218245.375000\n",
      "Train Epoch: 290 [39424/54000 (73%)] Loss: -218132.734375\n",
      "Train Epoch: 290 [40832/54000 (76%)] Loss: -227848.156250\n",
      "Train Epoch: 290 [42240/54000 (78%)] Loss: -228624.281250\n",
      "Train Epoch: 290 [43648/54000 (81%)] Loss: -227839.375000\n",
      "Train Epoch: 290 [45056/54000 (83%)] Loss: -223511.140625\n",
      "Train Epoch: 290 [46464/54000 (86%)] Loss: -222278.125000\n",
      "Train Epoch: 290 [47872/54000 (89%)] Loss: -217959.359375\n",
      "Train Epoch: 290 [49280/54000 (91%)] Loss: -221166.515625\n",
      "Train Epoch: 290 [50688/54000 (94%)] Loss: -217900.609375\n",
      "Train Epoch: 290 [52096/54000 (96%)] Loss: -229040.828125\n",
      "    epoch          : 290\n",
      "    loss           : -226116.8552631579\n",
      "    val_loss       : -229354.84397627666\n",
      "Train Epoch: 291 [0/54000 (0%)] Loss: -230992.453125\n",
      "Train Epoch: 291 [1408/54000 (3%)] Loss: -247951.109375\n",
      "Train Epoch: 291 [2816/54000 (5%)] Loss: -222134.671875\n",
      "Train Epoch: 291 [4224/54000 (8%)] Loss: -229459.578125\n",
      "Train Epoch: 291 [5632/54000 (10%)] Loss: -215981.203125\n",
      "Train Epoch: 291 [7040/54000 (13%)] Loss: -248365.640625\n",
      "Train Epoch: 291 [8448/54000 (16%)] Loss: -227839.859375\n",
      "Train Epoch: 291 [9856/54000 (18%)] Loss: -223976.156250\n",
      "Train Epoch: 291 [11264/54000 (21%)] Loss: -225470.156250\n",
      "Train Epoch: 291 [12672/54000 (23%)] Loss: -215152.296875\n",
      "Train Epoch: 291 [14080/54000 (26%)] Loss: -217756.625000\n",
      "Train Epoch: 291 [15488/54000 (29%)] Loss: -219258.000000\n",
      "Train Epoch: 291 [16896/54000 (31%)] Loss: -218509.703125\n",
      "Train Epoch: 291 [18304/54000 (34%)] Loss: -228397.562500\n",
      "Train Epoch: 291 [19712/54000 (37%)] Loss: -247907.546875\n",
      "Train Epoch: 291 [21120/54000 (39%)] Loss: -220137.437500\n",
      "Train Epoch: 291 [22528/54000 (42%)] Loss: -229905.359375\n",
      "Train Epoch: 291 [23936/54000 (44%)] Loss: -228118.046875\n",
      "Train Epoch: 291 [25344/54000 (47%)] Loss: -230741.375000\n",
      "Train Epoch: 291 [26752/54000 (50%)] Loss: -224255.031250\n",
      "Train Epoch: 291 [28160/54000 (52%)] Loss: -224185.531250\n",
      "Train Epoch: 291 [29568/54000 (55%)] Loss: -216390.562500\n",
      "Train Epoch: 291 [30976/54000 (57%)] Loss: -220337.218750\n",
      "Train Epoch: 291 [32384/54000 (60%)] Loss: -227470.203125\n",
      "Train Epoch: 291 [33792/54000 (63%)] Loss: -229982.812500\n",
      "Train Epoch: 291 [35200/54000 (65%)] Loss: -220175.484375\n",
      "Train Epoch: 291 [36608/54000 (68%)] Loss: -227040.015625\n",
      "Train Epoch: 291 [38016/54000 (70%)] Loss: -223860.187500\n",
      "Train Epoch: 291 [39424/54000 (73%)] Loss: -227258.046875\n",
      "Train Epoch: 291 [40832/54000 (76%)] Loss: -223433.312500\n",
      "Train Epoch: 291 [42240/54000 (78%)] Loss: -229999.406250\n",
      "Train Epoch: 291 [43648/54000 (81%)] Loss: -231257.125000\n",
      "Train Epoch: 291 [45056/54000 (83%)] Loss: -230009.578125\n",
      "Train Epoch: 291 [46464/54000 (86%)] Loss: -248265.312500\n",
      "Train Epoch: 291 [47872/54000 (89%)] Loss: -221328.953125\n",
      "Train Epoch: 291 [49280/54000 (91%)] Loss: -220743.781250\n",
      "Train Epoch: 291 [50688/54000 (94%)] Loss: -246629.968750\n",
      "Train Epoch: 291 [52096/54000 (96%)] Loss: -215506.937500\n",
      "    epoch          : 291\n",
      "    loss           : -226266.85051584928\n",
      "    val_loss       : -229562.77351491046\n",
      "Train Epoch: 292 [0/54000 (0%)] Loss: -223912.687500\n",
      "Train Epoch: 292 [1408/54000 (3%)] Loss: -229450.593750\n",
      "Train Epoch: 292 [2816/54000 (5%)] Loss: -230335.562500\n",
      "Train Epoch: 292 [4224/54000 (8%)] Loss: -215877.078125\n",
      "Train Epoch: 292 [5632/54000 (10%)] Loss: -219555.328125\n",
      "Train Epoch: 292 [7040/54000 (13%)] Loss: -221092.187500\n",
      "Train Epoch: 292 [8448/54000 (16%)] Loss: -224178.296875\n",
      "Train Epoch: 292 [9856/54000 (18%)] Loss: -220568.078125\n",
      "Train Epoch: 292 [11264/54000 (21%)] Loss: -220811.234375\n",
      "Train Epoch: 292 [12672/54000 (23%)] Loss: -246970.812500\n",
      "Train Epoch: 292 [14080/54000 (26%)] Loss: -231057.609375\n",
      "Train Epoch: 292 [15488/54000 (29%)] Loss: -221938.312500\n",
      "Train Epoch: 292 [16896/54000 (31%)] Loss: -223257.625000\n",
      "Train Epoch: 292 [18304/54000 (34%)] Loss: -248229.312500\n",
      "Train Epoch: 292 [19712/54000 (37%)] Loss: -228682.875000\n",
      "Train Epoch: 292 [21120/54000 (39%)] Loss: -227149.093750\n",
      "Train Epoch: 292 [22528/54000 (42%)] Loss: -219026.234375\n",
      "Train Epoch: 292 [23936/54000 (44%)] Loss: -223525.250000\n",
      "Train Epoch: 292 [25344/54000 (47%)] Loss: -224058.625000\n",
      "Train Epoch: 292 [26752/54000 (50%)] Loss: -219678.859375\n",
      "Train Epoch: 292 [28160/54000 (52%)] Loss: -215712.421875\n",
      "Train Epoch: 292 [29568/54000 (55%)] Loss: -220474.156250\n",
      "Train Epoch: 292 [30976/54000 (57%)] Loss: -247762.531250\n",
      "Train Epoch: 292 [32384/54000 (60%)] Loss: -230240.046875\n",
      "Train Epoch: 292 [33792/54000 (63%)] Loss: -229297.390625\n",
      "Train Epoch: 292 [35200/54000 (65%)] Loss: -230647.375000\n",
      "Train Epoch: 292 [36608/54000 (68%)] Loss: -230073.265625\n",
      "Train Epoch: 292 [38016/54000 (70%)] Loss: -218492.265625\n",
      "Train Epoch: 292 [39424/54000 (73%)] Loss: -216924.562500\n",
      "Train Epoch: 292 [40832/54000 (76%)] Loss: -224236.578125\n",
      "Train Epoch: 292 [42240/54000 (78%)] Loss: -229520.593750\n",
      "Train Epoch: 292 [43648/54000 (81%)] Loss: -220335.500000\n",
      "Train Epoch: 292 [45056/54000 (83%)] Loss: -219686.015625\n",
      "Train Epoch: 292 [46464/54000 (86%)] Loss: -228469.625000\n",
      "Train Epoch: 292 [47872/54000 (89%)] Loss: -219482.843750\n",
      "Train Epoch: 292 [49280/54000 (91%)] Loss: -220915.906250\n",
      "Train Epoch: 292 [50688/54000 (94%)] Loss: -219587.312500\n",
      "Train Epoch: 292 [52096/54000 (96%)] Loss: -229739.937500\n",
      "    epoch          : 292\n",
      "    loss           : -226238.21392793063\n",
      "    val_loss       : -229474.29550543064\n",
      "Train Epoch: 293 [0/54000 (0%)] Loss: -226243.968750\n",
      "Train Epoch: 293 [1408/54000 (3%)] Loss: -246432.734375\n",
      "Train Epoch: 293 [2816/54000 (5%)] Loss: -221989.656250\n",
      "Train Epoch: 293 [4224/54000 (8%)] Loss: -221525.031250\n",
      "Train Epoch: 293 [5632/54000 (10%)] Loss: -221450.468750\n",
      "Train Epoch: 293 [7040/54000 (13%)] Loss: -218963.156250\n",
      "Train Epoch: 293 [8448/54000 (16%)] Loss: -228633.437500\n",
      "Train Epoch: 293 [9856/54000 (18%)] Loss: -247887.375000\n",
      "Train Epoch: 293 [11264/54000 (21%)] Loss: -216309.906250\n",
      "Train Epoch: 293 [12672/54000 (23%)] Loss: -249080.937500\n",
      "Train Epoch: 293 [14080/54000 (26%)] Loss: -221467.484375\n",
      "Train Epoch: 293 [15488/54000 (29%)] Loss: -219277.640625\n",
      "Train Epoch: 293 [16896/54000 (31%)] Loss: -247693.062500\n",
      "Train Epoch: 293 [18304/54000 (34%)] Loss: -246762.015625\n",
      "Train Epoch: 293 [19712/54000 (37%)] Loss: -229302.484375\n",
      "Train Epoch: 293 [21120/54000 (39%)] Loss: -222804.671875\n",
      "Train Epoch: 293 [22528/54000 (42%)] Loss: -230031.343750\n",
      "Train Epoch: 293 [23936/54000 (44%)] Loss: -228382.812500\n",
      "Train Epoch: 293 [25344/54000 (47%)] Loss: -223596.531250\n",
      "Train Epoch: 293 [26752/54000 (50%)] Loss: -228643.203125\n",
      "Train Epoch: 293 [28160/54000 (52%)] Loss: -246496.625000\n",
      "Train Epoch: 293 [29568/54000 (55%)] Loss: -230462.031250\n",
      "Train Epoch: 293 [30976/54000 (57%)] Loss: -222730.781250\n",
      "Train Epoch: 293 [32384/54000 (60%)] Loss: -221466.531250\n",
      "Train Epoch: 293 [33792/54000 (63%)] Loss: -220177.359375\n",
      "Train Epoch: 293 [35200/54000 (65%)] Loss: -218228.812500\n",
      "Train Epoch: 293 [36608/54000 (68%)] Loss: -214901.281250\n",
      "Train Epoch: 293 [38016/54000 (70%)] Loss: -219029.671875\n",
      "Train Epoch: 293 [39424/54000 (73%)] Loss: -217525.171875\n",
      "Train Epoch: 293 [40832/54000 (76%)] Loss: -230159.640625\n",
      "Train Epoch: 293 [42240/54000 (78%)] Loss: -214678.843750\n",
      "Train Epoch: 293 [43648/54000 (81%)] Loss: -228797.734375\n",
      "Train Epoch: 293 [45056/54000 (83%)] Loss: -221540.296875\n",
      "Train Epoch: 293 [46464/54000 (86%)] Loss: -221794.125000\n",
      "Train Epoch: 293 [47872/54000 (89%)] Loss: -222579.906250\n",
      "Train Epoch: 293 [49280/54000 (91%)] Loss: -227525.453125\n",
      "Train Epoch: 293 [50688/54000 (94%)] Loss: -246671.921875\n",
      "Train Epoch: 293 [52096/54000 (96%)] Loss: -227644.843750\n",
      "    epoch          : 293\n",
      "    loss           : -226294.56433163874\n",
      "    val_loss       : -229070.67927067453\n",
      "Train Epoch: 294 [0/54000 (0%)] Loss: -226341.328125\n",
      "Train Epoch: 294 [1408/54000 (3%)] Loss: -229119.296875\n",
      "Train Epoch: 294 [2816/54000 (5%)] Loss: -219636.609375\n",
      "Train Epoch: 294 [4224/54000 (8%)] Loss: -222099.125000\n",
      "Train Epoch: 294 [5632/54000 (10%)] Loss: -223930.750000\n",
      "Train Epoch: 294 [7040/54000 (13%)] Loss: -222543.500000\n",
      "Train Epoch: 294 [8448/54000 (16%)] Loss: -223782.187500\n",
      "Train Epoch: 294 [9856/54000 (18%)] Loss: -227565.031250\n",
      "Train Epoch: 294 [11264/54000 (21%)] Loss: -217844.359375\n",
      "Train Epoch: 294 [12672/54000 (23%)] Loss: -218041.375000\n",
      "Train Epoch: 294 [14080/54000 (26%)] Loss: -247147.906250\n",
      "Train Epoch: 294 [15488/54000 (29%)] Loss: -229187.437500\n",
      "Train Epoch: 294 [16896/54000 (31%)] Loss: -223964.015625\n",
      "Train Epoch: 294 [18304/54000 (34%)] Loss: -217882.343750\n",
      "Train Epoch: 294 [19712/54000 (37%)] Loss: -223976.156250\n",
      "Train Epoch: 294 [21120/54000 (39%)] Loss: -228374.468750\n",
      "Train Epoch: 294 [22528/54000 (42%)] Loss: -224612.625000\n",
      "Train Epoch: 294 [23936/54000 (44%)] Loss: -229615.187500\n",
      "Train Epoch: 294 [25344/54000 (47%)] Loss: -222464.625000\n",
      "Train Epoch: 294 [26752/54000 (50%)] Loss: -249300.656250\n",
      "Train Epoch: 294 [28160/54000 (52%)] Loss: -230981.078125\n",
      "Train Epoch: 294 [29568/54000 (55%)] Loss: -228654.187500\n",
      "Train Epoch: 294 [30976/54000 (57%)] Loss: -223272.156250\n",
      "Train Epoch: 294 [32384/54000 (60%)] Loss: -219533.343750\n",
      "Train Epoch: 294 [33792/54000 (63%)] Loss: -247447.921875\n",
      "Train Epoch: 294 [35200/54000 (65%)] Loss: -231087.656250\n",
      "Train Epoch: 294 [36608/54000 (68%)] Loss: -220409.218750\n",
      "Train Epoch: 294 [38016/54000 (70%)] Loss: -249592.968750\n",
      "Train Epoch: 294 [39424/54000 (73%)] Loss: -219076.906250\n",
      "Train Epoch: 294 [40832/54000 (76%)] Loss: -230797.046875\n",
      "Train Epoch: 294 [42240/54000 (78%)] Loss: -227717.984375\n",
      "Train Epoch: 294 [43648/54000 (81%)] Loss: -220688.531250\n",
      "Train Epoch: 294 [45056/54000 (83%)] Loss: -224026.593750\n",
      "Train Epoch: 294 [46464/54000 (86%)] Loss: -222573.906250\n",
      "Train Epoch: 294 [47872/54000 (89%)] Loss: -220142.203125\n",
      "Train Epoch: 294 [49280/54000 (91%)] Loss: -218340.328125\n",
      "Train Epoch: 294 [50688/54000 (94%)] Loss: -249690.968750\n",
      "Train Epoch: 294 [52096/54000 (96%)] Loss: -222883.546875\n",
      "    epoch          : 294\n",
      "    loss           : -226390.5859375\n",
      "    val_loss       : -229529.60881526297\n",
      "Train Epoch: 295 [0/54000 (0%)] Loss: -247031.187500\n",
      "Train Epoch: 295 [1408/54000 (3%)] Loss: -219232.343750\n",
      "Train Epoch: 295 [2816/54000 (5%)] Loss: -229620.093750\n",
      "Train Epoch: 295 [4224/54000 (8%)] Loss: -215669.343750\n",
      "Train Epoch: 295 [5632/54000 (10%)] Loss: -219624.265625\n",
      "Train Epoch: 295 [7040/54000 (13%)] Loss: -222580.046875\n",
      "Train Epoch: 295 [8448/54000 (16%)] Loss: -224860.671875\n",
      "Train Epoch: 295 [9856/54000 (18%)] Loss: -224854.250000\n",
      "Train Epoch: 295 [11264/54000 (21%)] Loss: -223784.875000\n",
      "Train Epoch: 295 [12672/54000 (23%)] Loss: -224263.187500\n",
      "Train Epoch: 295 [14080/54000 (26%)] Loss: -228771.093750\n",
      "Train Epoch: 295 [15488/54000 (29%)] Loss: -224592.671875\n",
      "Train Epoch: 295 [16896/54000 (31%)] Loss: -227894.781250\n",
      "Train Epoch: 295 [18304/54000 (34%)] Loss: -247162.328125\n",
      "Train Epoch: 295 [19712/54000 (37%)] Loss: -219001.796875\n",
      "Train Epoch: 295 [21120/54000 (39%)] Loss: -220489.937500\n",
      "Train Epoch: 295 [22528/54000 (42%)] Loss: -216644.593750\n",
      "Train Epoch: 295 [23936/54000 (44%)] Loss: -246676.093750\n",
      "Train Epoch: 295 [25344/54000 (47%)] Loss: -221790.437500\n",
      "Train Epoch: 295 [26752/54000 (50%)] Loss: -218468.984375\n",
      "Train Epoch: 295 [28160/54000 (52%)] Loss: -222754.593750\n",
      "Train Epoch: 295 [29568/54000 (55%)] Loss: -220696.796875\n",
      "Train Epoch: 295 [30976/54000 (57%)] Loss: -230227.234375\n",
      "Train Epoch: 295 [32384/54000 (60%)] Loss: -214610.250000\n",
      "Train Epoch: 295 [33792/54000 (63%)] Loss: -214289.671875\n",
      "Train Epoch: 295 [35200/54000 (65%)] Loss: -219411.031250\n",
      "Train Epoch: 295 [36608/54000 (68%)] Loss: -221083.531250\n",
      "Train Epoch: 295 [38016/54000 (70%)] Loss: -231683.234375\n",
      "Train Epoch: 295 [39424/54000 (73%)] Loss: -220093.359375\n",
      "Train Epoch: 295 [40832/54000 (76%)] Loss: -221554.375000\n",
      "Train Epoch: 295 [42240/54000 (78%)] Loss: -228399.875000\n",
      "Train Epoch: 295 [43648/54000 (81%)] Loss: -216670.734375\n",
      "Train Epoch: 295 [45056/54000 (83%)] Loss: -250001.593750\n",
      "Train Epoch: 295 [46464/54000 (86%)] Loss: -221170.250000\n",
      "Train Epoch: 295 [47872/54000 (89%)] Loss: -217823.015625\n",
      "Train Epoch: 295 [49280/54000 (91%)] Loss: -223019.062500\n",
      "Train Epoch: 295 [50688/54000 (94%)] Loss: -226766.953125\n",
      "Train Epoch: 295 [52096/54000 (96%)] Loss: -227988.125000\n",
      "    epoch          : 295\n",
      "    loss           : -226372.12952302632\n",
      "    val_loss       : -229594.37659584603\n",
      "Train Epoch: 296 [0/54000 (0%)] Loss: -231414.656250\n",
      "Train Epoch: 296 [1408/54000 (3%)] Loss: -246801.328125\n",
      "Train Epoch: 296 [2816/54000 (5%)] Loss: -217398.750000\n",
      "Train Epoch: 296 [4224/54000 (8%)] Loss: -220888.812500\n",
      "Train Epoch: 296 [5632/54000 (10%)] Loss: -229596.609375\n",
      "Train Epoch: 296 [7040/54000 (13%)] Loss: -226004.359375\n",
      "Train Epoch: 296 [8448/54000 (16%)] Loss: -218971.375000\n",
      "Train Epoch: 296 [9856/54000 (18%)] Loss: -217787.187500\n",
      "Train Epoch: 296 [11264/54000 (21%)] Loss: -229594.984375\n",
      "Train Epoch: 296 [12672/54000 (23%)] Loss: -227630.671875\n",
      "Train Epoch: 296 [14080/54000 (26%)] Loss: -223131.343750\n",
      "Train Epoch: 296 [15488/54000 (29%)] Loss: -218474.765625\n",
      "Train Epoch: 296 [16896/54000 (31%)] Loss: -220956.593750\n",
      "Train Epoch: 296 [18304/54000 (34%)] Loss: -221290.078125\n",
      "Train Epoch: 296 [19712/54000 (37%)] Loss: -227204.812500\n",
      "Train Epoch: 296 [21120/54000 (39%)] Loss: -249253.484375\n",
      "Train Epoch: 296 [22528/54000 (42%)] Loss: -225400.281250\n",
      "Train Epoch: 296 [23936/54000 (44%)] Loss: -220599.421875\n",
      "Train Epoch: 296 [25344/54000 (47%)] Loss: -222236.656250\n",
      "Train Epoch: 296 [26752/54000 (50%)] Loss: -220383.218750\n",
      "Train Epoch: 296 [28160/54000 (52%)] Loss: -246850.796875\n",
      "Train Epoch: 296 [29568/54000 (55%)] Loss: -217550.015625\n",
      "Train Epoch: 296 [30976/54000 (57%)] Loss: -219136.984375\n",
      "Train Epoch: 296 [32384/54000 (60%)] Loss: -228602.609375\n",
      "Train Epoch: 296 [33792/54000 (63%)] Loss: -230778.875000\n",
      "Train Epoch: 296 [35200/54000 (65%)] Loss: -220824.859375\n",
      "Train Epoch: 296 [36608/54000 (68%)] Loss: -219987.437500\n",
      "Train Epoch: 296 [38016/54000 (70%)] Loss: -219779.875000\n",
      "Train Epoch: 296 [39424/54000 (73%)] Loss: -220892.218750\n",
      "Train Epoch: 296 [40832/54000 (76%)] Loss: -217495.484375\n",
      "Train Epoch: 296 [42240/54000 (78%)] Loss: -228776.968750\n",
      "Train Epoch: 296 [43648/54000 (81%)] Loss: -227672.984375\n",
      "Train Epoch: 296 [45056/54000 (83%)] Loss: -249247.625000\n",
      "Train Epoch: 296 [46464/54000 (86%)] Loss: -224683.843750\n",
      "Train Epoch: 296 [47872/54000 (89%)] Loss: -219769.781250\n",
      "Train Epoch: 296 [49280/54000 (91%)] Loss: -223712.765625\n",
      "Train Epoch: 296 [50688/54000 (94%)] Loss: -218674.484375\n",
      "Train Epoch: 296 [52096/54000 (96%)] Loss: -229561.421875\n",
      "    epoch          : 296\n",
      "    loss           : -226209.42060406698\n",
      "    val_loss       : -229340.6629906631\n",
      "Train Epoch: 297 [0/54000 (0%)] Loss: -221487.796875\n",
      "Train Epoch: 297 [1408/54000 (3%)] Loss: -228159.203125\n",
      "Train Epoch: 297 [2816/54000 (5%)] Loss: -216653.781250\n",
      "Train Epoch: 297 [4224/54000 (8%)] Loss: -222698.375000\n",
      "Train Epoch: 297 [5632/54000 (10%)] Loss: -228371.812500\n",
      "Train Epoch: 297 [7040/54000 (13%)] Loss: -229873.343750\n",
      "Train Epoch: 297 [8448/54000 (16%)] Loss: -247071.078125\n",
      "Train Epoch: 297 [9856/54000 (18%)] Loss: -218776.500000\n",
      "Train Epoch: 297 [11264/54000 (21%)] Loss: -231183.265625\n",
      "Train Epoch: 297 [12672/54000 (23%)] Loss: -220025.578125\n",
      "Train Epoch: 297 [14080/54000 (26%)] Loss: -247535.781250\n",
      "Train Epoch: 297 [15488/54000 (29%)] Loss: -247337.671875\n",
      "Train Epoch: 297 [16896/54000 (31%)] Loss: -227141.578125\n",
      "Train Epoch: 297 [18304/54000 (34%)] Loss: -219815.062500\n",
      "Train Epoch: 297 [19712/54000 (37%)] Loss: -223294.859375\n",
      "Train Epoch: 297 [21120/54000 (39%)] Loss: -224086.328125\n",
      "Train Epoch: 297 [22528/54000 (42%)] Loss: -223001.203125\n",
      "Train Epoch: 297 [23936/54000 (44%)] Loss: -222865.359375\n",
      "Train Epoch: 297 [25344/54000 (47%)] Loss: -230881.843750\n",
      "Train Epoch: 297 [26752/54000 (50%)] Loss: -248373.187500\n",
      "Train Epoch: 297 [28160/54000 (52%)] Loss: -248842.921875\n",
      "Train Epoch: 297 [29568/54000 (55%)] Loss: -222449.000000\n",
      "Train Epoch: 297 [30976/54000 (57%)] Loss: -226219.656250\n",
      "Train Epoch: 297 [32384/54000 (60%)] Loss: -226533.968750\n",
      "Train Epoch: 297 [33792/54000 (63%)] Loss: -224879.984375\n",
      "Train Epoch: 297 [35200/54000 (65%)] Loss: -228129.687500\n",
      "Train Epoch: 297 [36608/54000 (68%)] Loss: -215051.078125\n",
      "Train Epoch: 297 [38016/54000 (70%)] Loss: -229762.578125\n",
      "Train Epoch: 297 [39424/54000 (73%)] Loss: -220845.656250\n",
      "Train Epoch: 297 [40832/54000 (76%)] Loss: -231043.890625\n",
      "Train Epoch: 297 [42240/54000 (78%)] Loss: -222994.375000\n",
      "Train Epoch: 297 [43648/54000 (81%)] Loss: -228486.750000\n",
      "Train Epoch: 297 [45056/54000 (83%)] Loss: -248699.812500\n",
      "Train Epoch: 297 [46464/54000 (86%)] Loss: -221793.296875\n",
      "Train Epoch: 297 [47872/54000 (89%)] Loss: -227442.156250\n",
      "Train Epoch: 297 [49280/54000 (91%)] Loss: -223673.765625\n",
      "Train Epoch: 297 [50688/54000 (94%)] Loss: -248090.718750\n",
      "Train Epoch: 297 [52096/54000 (96%)] Loss: -229856.562500\n",
      "    epoch          : 297\n",
      "    loss           : -226319.03053977274\n",
      "    val_loss       : -229293.65548185023\n",
      "Train Epoch: 298 [0/54000 (0%)] Loss: -248849.968750\n",
      "Train Epoch: 298 [1408/54000 (3%)] Loss: -218686.171875\n",
      "Train Epoch: 298 [2816/54000 (5%)] Loss: -227169.031250\n",
      "Train Epoch: 298 [4224/54000 (8%)] Loss: -219691.890625\n",
      "Train Epoch: 298 [5632/54000 (10%)] Loss: -221204.265625\n",
      "Train Epoch: 298 [7040/54000 (13%)] Loss: -223236.421875\n",
      "Train Epoch: 298 [8448/54000 (16%)] Loss: -225698.484375\n",
      "Train Epoch: 298 [9856/54000 (18%)] Loss: -226932.890625\n",
      "Train Epoch: 298 [11264/54000 (21%)] Loss: -222736.859375\n",
      "Train Epoch: 298 [12672/54000 (23%)] Loss: -222449.828125\n",
      "Train Epoch: 298 [14080/54000 (26%)] Loss: -224279.546875\n",
      "Train Epoch: 298 [15488/54000 (29%)] Loss: -230898.218750\n",
      "Train Epoch: 298 [16896/54000 (31%)] Loss: -249045.968750\n",
      "Train Epoch: 298 [18304/54000 (34%)] Loss: -225574.218750\n",
      "Train Epoch: 298 [19712/54000 (37%)] Loss: -222053.562500\n",
      "Train Epoch: 298 [21120/54000 (39%)] Loss: -221535.468750\n",
      "Train Epoch: 298 [22528/54000 (42%)] Loss: -247440.781250\n",
      "Train Epoch: 298 [23936/54000 (44%)] Loss: -222056.531250\n",
      "Train Epoch: 298 [25344/54000 (47%)] Loss: -227578.140625\n",
      "Train Epoch: 298 [26752/54000 (50%)] Loss: -225422.312500\n",
      "Train Epoch: 298 [28160/54000 (52%)] Loss: -247077.937500\n",
      "Train Epoch: 298 [29568/54000 (55%)] Loss: -226528.796875\n",
      "Train Epoch: 298 [30976/54000 (57%)] Loss: -227482.312500\n",
      "Train Epoch: 298 [32384/54000 (60%)] Loss: -221448.359375\n",
      "Train Epoch: 298 [33792/54000 (63%)] Loss: -217763.765625\n",
      "Train Epoch: 298 [35200/54000 (65%)] Loss: -223961.718750\n",
      "Train Epoch: 298 [36608/54000 (68%)] Loss: -221144.156250\n",
      "Train Epoch: 298 [38016/54000 (70%)] Loss: -223611.609375\n",
      "Train Epoch: 298 [39424/54000 (73%)] Loss: -225023.562500\n",
      "Train Epoch: 298 [40832/54000 (76%)] Loss: -229641.562500\n",
      "Train Epoch: 298 [42240/54000 (78%)] Loss: -248013.921875\n",
      "Train Epoch: 298 [43648/54000 (81%)] Loss: -219509.734375\n",
      "Train Epoch: 298 [45056/54000 (83%)] Loss: -221550.312500\n",
      "Train Epoch: 298 [46464/54000 (86%)] Loss: -221147.593750\n",
      "Train Epoch: 298 [47872/54000 (89%)] Loss: -224664.718750\n",
      "Train Epoch: 298 [49280/54000 (91%)] Loss: -224511.468750\n",
      "Train Epoch: 298 [50688/54000 (94%)] Loss: -217870.828125\n",
      "Train Epoch: 298 [52096/54000 (96%)] Loss: -230077.500000\n",
      "    epoch          : 298\n",
      "    loss           : -226499.36255233255\n",
      "    val_loss       : -229820.67933617567\n",
      "Train Epoch: 299 [0/54000 (0%)] Loss: -247113.671875\n",
      "Train Epoch: 299 [1408/54000 (3%)] Loss: -229459.640625\n",
      "Train Epoch: 299 [2816/54000 (5%)] Loss: -230955.765625\n",
      "Train Epoch: 299 [4224/54000 (8%)] Loss: -229597.921875\n",
      "Train Epoch: 299 [5632/54000 (10%)] Loss: -231826.109375\n",
      "Train Epoch: 299 [7040/54000 (13%)] Loss: -229079.171875\n",
      "Train Epoch: 299 [8448/54000 (16%)] Loss: -246746.906250\n",
      "Train Epoch: 299 [9856/54000 (18%)] Loss: -229030.312500\n",
      "Train Epoch: 299 [11264/54000 (21%)] Loss: -228748.218750\n",
      "Train Epoch: 299 [12672/54000 (23%)] Loss: -230148.109375\n",
      "Train Epoch: 299 [14080/54000 (26%)] Loss: -222147.453125\n",
      "Train Epoch: 299 [15488/54000 (29%)] Loss: -218825.140625\n",
      "Train Epoch: 299 [16896/54000 (31%)] Loss: -231826.812500\n",
      "Train Epoch: 299 [18304/54000 (34%)] Loss: -222022.812500\n",
      "Train Epoch: 299 [19712/54000 (37%)] Loss: -216843.875000\n",
      "Train Epoch: 299 [21120/54000 (39%)] Loss: -219845.062500\n",
      "Train Epoch: 299 [22528/54000 (42%)] Loss: -223574.031250\n",
      "Train Epoch: 299 [23936/54000 (44%)] Loss: -215583.812500\n",
      "Train Epoch: 299 [25344/54000 (47%)] Loss: -247326.609375\n",
      "Train Epoch: 299 [26752/54000 (50%)] Loss: -229769.687500\n",
      "Train Epoch: 299 [28160/54000 (52%)] Loss: -232570.062500\n",
      "Train Epoch: 299 [29568/54000 (55%)] Loss: -227221.609375\n",
      "Train Epoch: 299 [30976/54000 (57%)] Loss: -248724.609375\n",
      "Train Epoch: 299 [32384/54000 (60%)] Loss: -220013.687500\n",
      "Train Epoch: 299 [33792/54000 (63%)] Loss: -228765.375000\n",
      "Train Epoch: 299 [35200/54000 (65%)] Loss: -220359.703125\n",
      "Train Epoch: 299 [36608/54000 (68%)] Loss: -218841.000000\n",
      "Train Epoch: 299 [38016/54000 (70%)] Loss: -247077.093750\n",
      "Train Epoch: 299 [39424/54000 (73%)] Loss: -216949.062500\n",
      "Train Epoch: 299 [40832/54000 (76%)] Loss: -230327.343750\n",
      "Train Epoch: 299 [42240/54000 (78%)] Loss: -224712.015625\n",
      "Train Epoch: 299 [43648/54000 (81%)] Loss: -218955.484375\n",
      "Train Epoch: 299 [45056/54000 (83%)] Loss: -250161.625000\n",
      "Train Epoch: 299 [46464/54000 (86%)] Loss: -224839.156250\n",
      "Train Epoch: 299 [47872/54000 (89%)] Loss: -221601.437500\n",
      "Train Epoch: 299 [49280/54000 (91%)] Loss: -220925.046875\n",
      "Train Epoch: 299 [50688/54000 (94%)] Loss: -222777.453125\n",
      "Train Epoch: 299 [52096/54000 (96%)] Loss: -246798.609375\n",
      "    epoch          : 299\n",
      "    loss           : -226271.08283492824\n",
      "    val_loss       : -229284.48570884147\n",
      "Train Epoch: 300 [0/54000 (0%)] Loss: -248909.125000\n",
      "Train Epoch: 300 [1408/54000 (3%)] Loss: -222305.328125\n",
      "Train Epoch: 300 [2816/54000 (5%)] Loss: -217663.984375\n",
      "Train Epoch: 300 [4224/54000 (8%)] Loss: -220142.234375\n",
      "Train Epoch: 300 [5632/54000 (10%)] Loss: -228645.875000\n",
      "Train Epoch: 300 [7040/54000 (13%)] Loss: -222738.500000\n",
      "Train Epoch: 300 [8448/54000 (16%)] Loss: -221910.843750\n",
      "Train Epoch: 300 [9856/54000 (18%)] Loss: -216710.515625\n",
      "Train Epoch: 300 [11264/54000 (21%)] Loss: -217651.625000\n",
      "Train Epoch: 300 [12672/54000 (23%)] Loss: -219147.406250\n",
      "Train Epoch: 300 [14080/54000 (26%)] Loss: -229078.171875\n",
      "Train Epoch: 300 [15488/54000 (29%)] Loss: -232360.937500\n",
      "Train Epoch: 300 [16896/54000 (31%)] Loss: -222018.875000\n",
      "Train Epoch: 300 [18304/54000 (34%)] Loss: -217227.187500\n",
      "Train Epoch: 300 [19712/54000 (37%)] Loss: -231288.109375\n",
      "Train Epoch: 300 [21120/54000 (39%)] Loss: -220229.953125\n",
      "Train Epoch: 300 [22528/54000 (42%)] Loss: -221001.765625\n",
      "Train Epoch: 300 [23936/54000 (44%)] Loss: -214793.703125\n",
      "Train Epoch: 300 [25344/54000 (47%)] Loss: -220160.156250\n",
      "Train Epoch: 300 [26752/54000 (50%)] Loss: -248369.906250\n",
      "Train Epoch: 300 [28160/54000 (52%)] Loss: -228973.796875\n",
      "Train Epoch: 300 [29568/54000 (55%)] Loss: -217388.640625\n",
      "Train Epoch: 300 [30976/54000 (57%)] Loss: -229601.343750\n",
      "Train Epoch: 300 [32384/54000 (60%)] Loss: -249692.031250\n",
      "Train Epoch: 300 [33792/54000 (63%)] Loss: -219075.140625\n",
      "Train Epoch: 300 [35200/54000 (65%)] Loss: -228800.937500\n",
      "Train Epoch: 300 [36608/54000 (68%)] Loss: -218569.812500\n",
      "Train Epoch: 300 [38016/54000 (70%)] Loss: -218084.312500\n",
      "Train Epoch: 300 [39424/54000 (73%)] Loss: -245899.062500\n",
      "Train Epoch: 300 [40832/54000 (76%)] Loss: -218146.765625\n",
      "Train Epoch: 300 [42240/54000 (78%)] Loss: -230261.484375\n",
      "Train Epoch: 300 [43648/54000 (81%)] Loss: -247924.140625\n",
      "Train Epoch: 300 [45056/54000 (83%)] Loss: -223856.125000\n",
      "Train Epoch: 300 [46464/54000 (86%)] Loss: -228289.718750\n",
      "Train Epoch: 300 [47872/54000 (89%)] Loss: -218892.781250\n",
      "Train Epoch: 300 [49280/54000 (91%)] Loss: -217592.921875\n",
      "Train Epoch: 300 [50688/54000 (94%)] Loss: -246369.890625\n",
      "Train Epoch: 300 [52096/54000 (96%)] Loss: -220198.218750\n",
      "    epoch          : 300\n",
      "    loss           : -226452.06844348085\n",
      "    val_loss       : -229815.37301710175\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0508_092801/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [0/54000 (0%)] Loss: -249097.890625\n",
      "Train Epoch: 301 [1408/54000 (3%)] Loss: -227604.671875\n",
      "Train Epoch: 301 [2816/54000 (5%)] Loss: -227257.078125\n",
      "Train Epoch: 301 [4224/54000 (8%)] Loss: -230563.187500\n",
      "Train Epoch: 301 [5632/54000 (10%)] Loss: -230708.062500\n",
      "Train Epoch: 301 [7040/54000 (13%)] Loss: -230917.953125\n",
      "Train Epoch: 301 [8448/54000 (16%)] Loss: -245837.953125\n",
      "Train Epoch: 301 [9856/54000 (18%)] Loss: -227805.015625\n",
      "Train Epoch: 301 [11264/54000 (21%)] Loss: -222088.578125\n",
      "Train Epoch: 301 [12672/54000 (23%)] Loss: -220449.750000\n",
      "Train Epoch: 301 [14080/54000 (26%)] Loss: -229612.875000\n",
      "Train Epoch: 301 [15488/54000 (29%)] Loss: -230479.109375\n",
      "Train Epoch: 301 [16896/54000 (31%)] Loss: -221851.109375\n",
      "Train Epoch: 301 [18304/54000 (34%)] Loss: -218174.687500\n",
      "Train Epoch: 301 [19712/54000 (37%)] Loss: -217504.312500\n",
      "Train Epoch: 301 [21120/54000 (39%)] Loss: -250219.281250\n",
      "Train Epoch: 301 [22528/54000 (42%)] Loss: -228234.937500\n",
      "Train Epoch: 301 [23936/54000 (44%)] Loss: -227984.515625\n",
      "Train Epoch: 301 [25344/54000 (47%)] Loss: -229411.750000\n",
      "Train Epoch: 301 [26752/54000 (50%)] Loss: -226861.718750\n",
      "Train Epoch: 301 [28160/54000 (52%)] Loss: -249044.250000\n",
      "Train Epoch: 301 [29568/54000 (55%)] Loss: -217505.203125\n",
      "Train Epoch: 301 [30976/54000 (57%)] Loss: -228221.062500\n",
      "Train Epoch: 301 [32384/54000 (60%)] Loss: -230527.625000\n",
      "Train Epoch: 301 [33792/54000 (63%)] Loss: -225099.328125\n",
      "Train Epoch: 301 [35200/54000 (65%)] Loss: -227873.140625\n",
      "Train Epoch: 301 [36608/54000 (68%)] Loss: -247334.328125\n",
      "Train Epoch: 301 [38016/54000 (70%)] Loss: -221252.812500\n",
      "Train Epoch: 301 [39424/54000 (73%)] Loss: -218606.718750\n",
      "Train Epoch: 301 [40832/54000 (76%)] Loss: -227908.890625\n",
      "Train Epoch: 301 [42240/54000 (78%)] Loss: -230124.718750\n",
      "Train Epoch: 301 [43648/54000 (81%)] Loss: -225965.250000\n",
      "Train Epoch: 301 [45056/54000 (83%)] Loss: -219270.703125\n",
      "Train Epoch: 301 [46464/54000 (86%)] Loss: -218067.703125\n",
      "Train Epoch: 301 [47872/54000 (89%)] Loss: -226508.218750\n",
      "Train Epoch: 301 [49280/54000 (91%)] Loss: -220913.421875\n",
      "Train Epoch: 301 [50688/54000 (94%)] Loss: -229849.781250\n",
      "Train Epoch: 301 [52096/54000 (96%)] Loss: -248508.421875\n",
      "    epoch          : 301\n",
      "    loss           : -226349.57319078947\n",
      "    val_loss       : -229239.53233970085\n",
      "Train Epoch: 302 [0/54000 (0%)] Loss: -248320.187500\n",
      "Train Epoch: 302 [1408/54000 (3%)] Loss: -229210.375000\n",
      "Train Epoch: 302 [2816/54000 (5%)] Loss: -230027.562500\n",
      "Train Epoch: 302 [4224/54000 (8%)] Loss: -230224.687500\n",
      "Train Epoch: 302 [5632/54000 (10%)] Loss: -220783.437500\n",
      "Train Epoch: 302 [7040/54000 (13%)] Loss: -221008.203125\n",
      "Train Epoch: 302 [8448/54000 (16%)] Loss: -216032.125000\n",
      "Train Epoch: 302 [9856/54000 (18%)] Loss: -219251.765625\n",
      "Train Epoch: 302 [11264/54000 (21%)] Loss: -227924.656250\n",
      "Train Epoch: 302 [12672/54000 (23%)] Loss: -219549.937500\n",
      "Train Epoch: 302 [14080/54000 (26%)] Loss: -219003.484375\n",
      "Train Epoch: 302 [15488/54000 (29%)] Loss: -224809.187500\n",
      "Train Epoch: 302 [16896/54000 (31%)] Loss: -213707.578125\n",
      "Train Epoch: 302 [18304/54000 (34%)] Loss: -228743.218750\n",
      "Train Epoch: 302 [19712/54000 (37%)] Loss: -215779.203125\n",
      "Train Epoch: 302 [21120/54000 (39%)] Loss: -249278.171875\n",
      "Train Epoch: 302 [22528/54000 (42%)] Loss: -224856.234375\n",
      "Train Epoch: 302 [23936/54000 (44%)] Loss: -222705.187500\n",
      "Train Epoch: 302 [25344/54000 (47%)] Loss: -247435.578125\n",
      "Train Epoch: 302 [26752/54000 (50%)] Loss: -220861.546875\n",
      "Train Epoch: 302 [28160/54000 (52%)] Loss: -229907.125000\n",
      "Train Epoch: 302 [29568/54000 (55%)] Loss: -216018.062500\n",
      "Train Epoch: 302 [30976/54000 (57%)] Loss: -229809.593750\n",
      "Train Epoch: 302 [32384/54000 (60%)] Loss: -214099.781250\n",
      "Train Epoch: 302 [33792/54000 (63%)] Loss: -216667.671875\n",
      "Train Epoch: 302 [35200/54000 (65%)] Loss: -218492.546875\n",
      "Train Epoch: 302 [36608/54000 (68%)] Loss: -229621.312500\n",
      "Train Epoch: 302 [38016/54000 (70%)] Loss: -219244.187500\n",
      "Train Epoch: 302 [39424/54000 (73%)] Loss: -229846.062500\n",
      "Train Epoch: 302 [40832/54000 (76%)] Loss: -218058.796875\n",
      "Train Epoch: 302 [42240/54000 (78%)] Loss: -223705.625000\n",
      "Train Epoch: 302 [43648/54000 (81%)] Loss: -228832.187500\n",
      "Train Epoch: 302 [45056/54000 (83%)] Loss: -224802.546875\n",
      "Train Epoch: 302 [46464/54000 (86%)] Loss: -221343.062500\n",
      "Train Epoch: 302 [47872/54000 (89%)] Loss: -218823.625000\n",
      "Train Epoch: 302 [49280/54000 (91%)] Loss: -231745.687500\n",
      "Train Epoch: 302 [50688/54000 (94%)] Loss: -228676.250000\n",
      "Train Epoch: 302 [52096/54000 (96%)] Loss: -227055.250000\n",
      "    epoch          : 302\n",
      "    loss           : -226612.0870215311\n",
      "    val_loss       : -229766.23742973513\n",
      "Train Epoch: 303 [0/54000 (0%)] Loss: -221843.093750\n",
      "Train Epoch: 303 [1408/54000 (3%)] Loss: -220656.359375\n",
      "Train Epoch: 303 [2816/54000 (5%)] Loss: -229751.750000\n",
      "Train Epoch: 303 [4224/54000 (8%)] Loss: -217821.531250\n",
      "Train Epoch: 303 [5632/54000 (10%)] Loss: -219846.828125\n",
      "Train Epoch: 303 [7040/54000 (13%)] Loss: -246659.359375\n",
      "Train Epoch: 303 [8448/54000 (16%)] Loss: -229269.296875\n",
      "Train Epoch: 303 [9856/54000 (18%)] Loss: -224658.500000\n",
      "Train Epoch: 303 [11264/54000 (21%)] Loss: -226772.437500\n",
      "Train Epoch: 303 [12672/54000 (23%)] Loss: -221363.015625\n",
      "Train Epoch: 303 [14080/54000 (26%)] Loss: -222993.296875\n",
      "Train Epoch: 303 [15488/54000 (29%)] Loss: -228452.656250\n",
      "Train Epoch: 303 [16896/54000 (31%)] Loss: -221061.515625\n",
      "Train Epoch: 303 [18304/54000 (34%)] Loss: -218758.906250\n",
      "Train Epoch: 303 [19712/54000 (37%)] Loss: -247878.453125\n",
      "Train Epoch: 303 [21120/54000 (39%)] Loss: -216129.796875\n",
      "Train Epoch: 303 [22528/54000 (42%)] Loss: -224401.953125\n",
      "Train Epoch: 303 [23936/54000 (44%)] Loss: -226791.781250\n",
      "Train Epoch: 303 [25344/54000 (47%)] Loss: -227150.718750\n",
      "Train Epoch: 303 [26752/54000 (50%)] Loss: -224359.796875\n",
      "Train Epoch: 303 [28160/54000 (52%)] Loss: -224785.843750\n",
      "Train Epoch: 303 [29568/54000 (55%)] Loss: -219828.312500\n",
      "Train Epoch: 303 [30976/54000 (57%)] Loss: -225454.296875\n",
      "Train Epoch: 303 [32384/54000 (60%)] Loss: -246815.218750\n",
      "Train Epoch: 303 [33792/54000 (63%)] Loss: -225061.484375\n",
      "Train Epoch: 303 [35200/54000 (65%)] Loss: -219439.312500\n",
      "Train Epoch: 303 [36608/54000 (68%)] Loss: -249249.812500\n",
      "Train Epoch: 303 [38016/54000 (70%)] Loss: -221918.812500\n",
      "Train Epoch: 303 [39424/54000 (73%)] Loss: -228682.328125\n",
      "Train Epoch: 303 [40832/54000 (76%)] Loss: -220000.984375\n",
      "Train Epoch: 303 [42240/54000 (78%)] Loss: -229849.468750\n",
      "Train Epoch: 303 [43648/54000 (81%)] Loss: -217090.640625\n",
      "Train Epoch: 303 [45056/54000 (83%)] Loss: -224021.390625\n",
      "Train Epoch: 303 [46464/54000 (86%)] Loss: -218129.000000\n",
      "Train Epoch: 303 [47872/54000 (89%)] Loss: -224389.093750\n",
      "Train Epoch: 303 [49280/54000 (91%)] Loss: -249412.953125\n",
      "Train Epoch: 303 [50688/54000 (94%)] Loss: -227649.656250\n",
      "Train Epoch: 303 [52096/54000 (96%)] Loss: -228875.906250\n",
      "    epoch          : 303\n",
      "    loss           : -226446.95222787082\n",
      "    val_loss       : -229470.8595774581\n",
      "Train Epoch: 304 [0/54000 (0%)] Loss: -229806.671875\n",
      "Train Epoch: 304 [1408/54000 (3%)] Loss: -249895.343750\n",
      "Train Epoch: 304 [2816/54000 (5%)] Loss: -222860.921875\n",
      "Train Epoch: 304 [4224/54000 (8%)] Loss: -222360.125000\n",
      "Train Epoch: 304 [5632/54000 (10%)] Loss: -220333.062500\n",
      "Train Epoch: 304 [7040/54000 (13%)] Loss: -231527.703125\n",
      "Train Epoch: 304 [8448/54000 (16%)] Loss: -217793.031250\n",
      "Train Epoch: 304 [9856/54000 (18%)] Loss: -229261.593750\n",
      "Train Epoch: 304 [11264/54000 (21%)] Loss: -248827.156250\n",
      "Train Epoch: 304 [12672/54000 (23%)] Loss: -218734.187500\n",
      "Train Epoch: 304 [14080/54000 (26%)] Loss: -217231.406250\n",
      "Train Epoch: 304 [15488/54000 (29%)] Loss: -222685.296875\n",
      "Train Epoch: 304 [16896/54000 (31%)] Loss: -225204.875000\n",
      "Train Epoch: 304 [18304/54000 (34%)] Loss: -226142.921875\n",
      "Train Epoch: 304 [19712/54000 (37%)] Loss: -227959.750000\n",
      "Train Epoch: 304 [21120/54000 (39%)] Loss: -225629.406250\n",
      "Train Epoch: 304 [22528/54000 (42%)] Loss: -230291.906250\n",
      "Train Epoch: 304 [23936/54000 (44%)] Loss: -223145.703125\n",
      "Train Epoch: 304 [25344/54000 (47%)] Loss: -247454.328125\n",
      "Train Epoch: 304 [26752/54000 (50%)] Loss: -225227.765625\n",
      "Train Epoch: 304 [28160/54000 (52%)] Loss: -219443.281250\n",
      "Train Epoch: 304 [29568/54000 (55%)] Loss: -228394.968750\n",
      "Train Epoch: 304 [30976/54000 (57%)] Loss: -216894.250000\n",
      "Train Epoch: 304 [32384/54000 (60%)] Loss: -218057.234375\n",
      "Train Epoch: 304 [33792/54000 (63%)] Loss: -250500.812500\n",
      "Train Epoch: 304 [35200/54000 (65%)] Loss: -217003.546875\n",
      "Train Epoch: 304 [36608/54000 (68%)] Loss: -222171.171875\n",
      "Train Epoch: 304 [38016/54000 (70%)] Loss: -232040.390625\n",
      "Train Epoch: 304 [39424/54000 (73%)] Loss: -248994.203125\n",
      "Train Epoch: 304 [40832/54000 (76%)] Loss: -229321.703125\n",
      "Train Epoch: 304 [42240/54000 (78%)] Loss: -219138.718750\n",
      "Train Epoch: 304 [43648/54000 (81%)] Loss: -229018.078125\n",
      "Train Epoch: 304 [45056/54000 (83%)] Loss: -228084.406250\n",
      "Train Epoch: 304 [46464/54000 (86%)] Loss: -223635.453125\n",
      "Train Epoch: 304 [47872/54000 (89%)] Loss: -217750.656250\n",
      "Train Epoch: 304 [49280/54000 (91%)] Loss: -218207.593750\n",
      "Train Epoch: 304 [50688/54000 (94%)] Loss: -218218.187500\n",
      "Train Epoch: 304 [52096/54000 (96%)] Loss: -222572.218750\n",
      "    epoch          : 304\n",
      "    loss           : -226563.41570723685\n",
      "    val_loss       : -229735.3611256669\n",
      "Train Epoch: 305 [0/54000 (0%)] Loss: -248155.375000\n",
      "Train Epoch: 305 [1408/54000 (3%)] Loss: -228812.406250\n",
      "Train Epoch: 305 [2816/54000 (5%)] Loss: -226101.843750\n",
      "Train Epoch: 305 [4224/54000 (8%)] Loss: -217893.171875\n",
      "Train Epoch: 305 [5632/54000 (10%)] Loss: -219657.609375\n",
      "Train Epoch: 305 [7040/54000 (13%)] Loss: -228085.765625\n",
      "Train Epoch: 305 [8448/54000 (16%)] Loss: -228951.593750\n",
      "Train Epoch: 305 [9856/54000 (18%)] Loss: -221621.187500\n",
      "Train Epoch: 305 [11264/54000 (21%)] Loss: -221037.968750\n",
      "Train Epoch: 305 [12672/54000 (23%)] Loss: -225834.921875\n",
      "Train Epoch: 305 [14080/54000 (26%)] Loss: -216859.328125\n",
      "Train Epoch: 305 [15488/54000 (29%)] Loss: -225540.484375\n",
      "Train Epoch: 305 [16896/54000 (31%)] Loss: -220003.421875\n",
      "Train Epoch: 305 [18304/54000 (34%)] Loss: -217284.703125\n",
      "Train Epoch: 305 [19712/54000 (37%)] Loss: -248352.500000\n",
      "Train Epoch: 305 [21120/54000 (39%)] Loss: -227800.890625\n",
      "Train Epoch: 305 [22528/54000 (42%)] Loss: -229154.031250\n",
      "Train Epoch: 305 [23936/54000 (44%)] Loss: -230667.062500\n",
      "Train Epoch: 305 [25344/54000 (47%)] Loss: -229988.859375\n",
      "Train Epoch: 305 [26752/54000 (50%)] Loss: -224485.015625\n",
      "Train Epoch: 305 [28160/54000 (52%)] Loss: -230445.953125\n",
      "Train Epoch: 305 [29568/54000 (55%)] Loss: -220083.187500\n",
      "Train Epoch: 305 [30976/54000 (57%)] Loss: -249298.734375\n",
      "Train Epoch: 305 [32384/54000 (60%)] Loss: -229203.546875\n",
      "Train Epoch: 305 [33792/54000 (63%)] Loss: -218168.625000\n",
      "Train Epoch: 305 [35200/54000 (65%)] Loss: -229782.781250\n",
      "Train Epoch: 305 [36608/54000 (68%)] Loss: -247302.171875\n",
      "Train Epoch: 305 [38016/54000 (70%)] Loss: -224434.265625\n",
      "Train Epoch: 305 [39424/54000 (73%)] Loss: -220274.453125\n",
      "Train Epoch: 305 [40832/54000 (76%)] Loss: -231007.875000\n",
      "Train Epoch: 305 [42240/54000 (78%)] Loss: -247471.812500\n",
      "Train Epoch: 305 [43648/54000 (81%)] Loss: -219601.640625\n",
      "Train Epoch: 305 [45056/54000 (83%)] Loss: -221596.656250\n",
      "Train Epoch: 305 [46464/54000 (86%)] Loss: -224383.546875\n",
      "Train Epoch: 305 [47872/54000 (89%)] Loss: -221854.218750\n",
      "Train Epoch: 305 [49280/54000 (91%)] Loss: -217029.578125\n",
      "Train Epoch: 305 [50688/54000 (94%)] Loss: -248870.750000\n",
      "Train Epoch: 305 [52096/54000 (96%)] Loss: -217531.109375\n",
      "    epoch          : 305\n",
      "    loss           : -226563.73796351676\n",
      "    val_loss       : -229502.57261694933\n",
      "Train Epoch: 306 [0/54000 (0%)] Loss: -231911.437500\n",
      "Train Epoch: 306 [1408/54000 (3%)] Loss: -248895.968750\n",
      "Train Epoch: 306 [2816/54000 (5%)] Loss: -217557.781250\n",
      "Train Epoch: 306 [4224/54000 (8%)] Loss: -215212.656250\n",
      "Train Epoch: 306 [5632/54000 (10%)] Loss: -228107.968750\n",
      "Train Epoch: 306 [7040/54000 (13%)] Loss: -228816.687500\n",
      "Train Epoch: 306 [8448/54000 (16%)] Loss: -218605.171875\n",
      "Train Epoch: 306 [9856/54000 (18%)] Loss: -228266.296875\n",
      "Train Epoch: 306 [11264/54000 (21%)] Loss: -220922.421875\n",
      "Train Epoch: 306 [12672/54000 (23%)] Loss: -223237.906250\n",
      "Train Epoch: 306 [14080/54000 (26%)] Loss: -247951.921875\n",
      "Train Epoch: 306 [15488/54000 (29%)] Loss: -230674.000000\n",
      "Train Epoch: 306 [16896/54000 (31%)] Loss: -230404.078125\n",
      "Train Epoch: 306 [18304/54000 (34%)] Loss: -215305.109375\n",
      "Train Epoch: 306 [19712/54000 (37%)] Loss: -246047.968750\n",
      "Train Epoch: 306 [21120/54000 (39%)] Loss: -224283.859375\n",
      "Train Epoch: 306 [22528/54000 (42%)] Loss: -220454.421875\n",
      "Train Epoch: 306 [23936/54000 (44%)] Loss: -249272.609375\n",
      "Train Epoch: 306 [25344/54000 (47%)] Loss: -221859.171875\n",
      "Train Epoch: 306 [26752/54000 (50%)] Loss: -219534.156250\n",
      "Train Epoch: 306 [28160/54000 (52%)] Loss: -230075.453125\n",
      "Train Epoch: 306 [29568/54000 (55%)] Loss: -220266.593750\n",
      "Train Epoch: 306 [30976/54000 (57%)] Loss: -225782.687500\n",
      "Train Epoch: 306 [32384/54000 (60%)] Loss: -219171.921875\n",
      "Train Epoch: 306 [33792/54000 (63%)] Loss: -227878.765625\n",
      "Train Epoch: 306 [35200/54000 (65%)] Loss: -225786.546875\n",
      "Train Epoch: 306 [36608/54000 (68%)] Loss: -225796.234375\n",
      "Train Epoch: 306 [38016/54000 (70%)] Loss: -226341.812500\n",
      "Train Epoch: 306 [39424/54000 (73%)] Loss: -247305.156250\n",
      "Train Epoch: 306 [40832/54000 (76%)] Loss: -220294.218750\n",
      "Train Epoch: 306 [42240/54000 (78%)] Loss: -217404.390625\n",
      "Train Epoch: 306 [43648/54000 (81%)] Loss: -221330.187500\n",
      "Train Epoch: 306 [45056/54000 (83%)] Loss: -224093.968750\n",
      "Train Epoch: 306 [46464/54000 (86%)] Loss: -220176.765625\n",
      "Train Epoch: 306 [47872/54000 (89%)] Loss: -221094.187500\n",
      "Train Epoch: 306 [49280/54000 (91%)] Loss: -228582.421875\n",
      "Train Epoch: 306 [50688/54000 (94%)] Loss: -222638.125000\n",
      "Train Epoch: 306 [52096/54000 (96%)] Loss: -218837.375000\n",
      "    epoch          : 306\n",
      "    loss           : -226676.17411782296\n",
      "    val_loss       : -229830.09147532392\n",
      "Train Epoch: 307 [0/54000 (0%)] Loss: -230961.203125\n",
      "Train Epoch: 307 [1408/54000 (3%)] Loss: -220684.171875\n",
      "Train Epoch: 307 [2816/54000 (5%)] Loss: -220395.750000\n",
      "Train Epoch: 307 [4224/54000 (8%)] Loss: -221503.984375\n",
      "Train Epoch: 307 [5632/54000 (10%)] Loss: -220609.906250\n",
      "Train Epoch: 307 [7040/54000 (13%)] Loss: -222641.343750\n",
      "Train Epoch: 307 [8448/54000 (16%)] Loss: -230130.687500\n",
      "Train Epoch: 307 [9856/54000 (18%)] Loss: -214511.468750\n",
      "Train Epoch: 307 [11264/54000 (21%)] Loss: -232839.968750\n",
      "Train Epoch: 307 [12672/54000 (23%)] Loss: -223742.859375\n",
      "Train Epoch: 307 [14080/54000 (26%)] Loss: -224716.750000\n",
      "Train Epoch: 307 [15488/54000 (29%)] Loss: -220986.796875\n",
      "Train Epoch: 307 [16896/54000 (31%)] Loss: -225737.750000\n",
      "Train Epoch: 307 [18304/54000 (34%)] Loss: -219710.640625\n",
      "Train Epoch: 307 [19712/54000 (37%)] Loss: -229658.718750\n",
      "Train Epoch: 307 [21120/54000 (39%)] Loss: -231308.593750\n",
      "Train Epoch: 307 [22528/54000 (42%)] Loss: -231112.593750\n",
      "Train Epoch: 307 [23936/54000 (44%)] Loss: -220589.718750\n",
      "Train Epoch: 307 [25344/54000 (47%)] Loss: -225507.250000\n",
      "Train Epoch: 307 [26752/54000 (50%)] Loss: -222005.968750\n",
      "Train Epoch: 307 [28160/54000 (52%)] Loss: -220475.906250\n",
      "Train Epoch: 307 [29568/54000 (55%)] Loss: -229896.968750\n",
      "Train Epoch: 307 [30976/54000 (57%)] Loss: -219787.796875\n",
      "Train Epoch: 307 [32384/54000 (60%)] Loss: -220337.640625\n",
      "Train Epoch: 307 [33792/54000 (63%)] Loss: -248490.468750\n",
      "Train Epoch: 307 [35200/54000 (65%)] Loss: -231572.031250\n",
      "Train Epoch: 307 [36608/54000 (68%)] Loss: -232019.218750\n",
      "Train Epoch: 307 [38016/54000 (70%)] Loss: -222010.218750\n",
      "Train Epoch: 307 [39424/54000 (73%)] Loss: -248051.562500\n",
      "Train Epoch: 307 [40832/54000 (76%)] Loss: -224596.421875\n",
      "Train Epoch: 307 [42240/54000 (78%)] Loss: -231555.328125\n",
      "Train Epoch: 307 [43648/54000 (81%)] Loss: -222381.828125\n",
      "Train Epoch: 307 [45056/54000 (83%)] Loss: -215774.875000\n",
      "Train Epoch: 307 [46464/54000 (86%)] Loss: -218077.187500\n",
      "Train Epoch: 307 [47872/54000 (89%)] Loss: -227477.859375\n",
      "Train Epoch: 307 [49280/54000 (91%)] Loss: -249555.968750\n",
      "Train Epoch: 307 [50688/54000 (94%)] Loss: -219227.015625\n",
      "Train Epoch: 307 [52096/54000 (96%)] Loss: -225754.781250\n",
      "    epoch          : 307\n",
      "    loss           : -226689.14589563396\n",
      "    val_loss       : -229819.44596751142\n",
      "Train Epoch: 308 [0/54000 (0%)] Loss: -223942.015625\n",
      "Train Epoch: 308 [1408/54000 (3%)] Loss: -249320.875000\n",
      "Train Epoch: 308 [2816/54000 (5%)] Loss: -249445.515625\n",
      "Train Epoch: 308 [4224/54000 (8%)] Loss: -225325.531250\n",
      "Train Epoch: 308 [5632/54000 (10%)] Loss: -218019.468750\n",
      "Train Epoch: 308 [7040/54000 (13%)] Loss: -248247.015625\n",
      "Train Epoch: 308 [8448/54000 (16%)] Loss: -220250.343750\n",
      "Train Epoch: 308 [9856/54000 (18%)] Loss: -230569.875000\n",
      "Train Epoch: 308 [11264/54000 (21%)] Loss: -227057.984375\n",
      "Train Epoch: 308 [12672/54000 (23%)] Loss: -230508.687500\n",
      "Train Epoch: 308 [14080/54000 (26%)] Loss: -248096.140625\n",
      "Train Epoch: 308 [15488/54000 (29%)] Loss: -220372.203125\n",
      "Train Epoch: 308 [16896/54000 (31%)] Loss: -222605.343750\n",
      "Train Epoch: 308 [18304/54000 (34%)] Loss: -219068.187500\n",
      "Train Epoch: 308 [19712/54000 (37%)] Loss: -218327.781250\n",
      "Train Epoch: 308 [21120/54000 (39%)] Loss: -229754.531250\n",
      "Train Epoch: 308 [22528/54000 (42%)] Loss: -219452.765625\n",
      "Train Epoch: 308 [23936/54000 (44%)] Loss: -247185.312500\n",
      "Train Epoch: 308 [25344/54000 (47%)] Loss: -228740.140625\n",
      "Train Epoch: 308 [26752/54000 (50%)] Loss: -230093.375000\n",
      "Train Epoch: 308 [28160/54000 (52%)] Loss: -231109.171875\n",
      "Train Epoch: 308 [29568/54000 (55%)] Loss: -224868.328125\n",
      "Train Epoch: 308 [30976/54000 (57%)] Loss: -228098.656250\n",
      "Train Epoch: 308 [32384/54000 (60%)] Loss: -223049.765625\n",
      "Train Epoch: 308 [33792/54000 (63%)] Loss: -227806.687500\n",
      "Train Epoch: 308 [35200/54000 (65%)] Loss: -224035.062500\n",
      "Train Epoch: 308 [36608/54000 (68%)] Loss: -220977.500000\n",
      "Train Epoch: 308 [38016/54000 (70%)] Loss: -222181.796875\n",
      "Train Epoch: 308 [39424/54000 (73%)] Loss: -218745.906250\n",
      "Train Epoch: 308 [40832/54000 (76%)] Loss: -219595.515625\n",
      "Train Epoch: 308 [42240/54000 (78%)] Loss: -222247.968750\n",
      "Train Epoch: 308 [43648/54000 (81%)] Loss: -220449.796875\n",
      "Train Epoch: 308 [45056/54000 (83%)] Loss: -248381.500000\n",
      "Train Epoch: 308 [46464/54000 (86%)] Loss: -221059.531250\n",
      "Train Epoch: 308 [47872/54000 (89%)] Loss: -227055.578125\n",
      "Train Epoch: 308 [49280/54000 (91%)] Loss: -219007.109375\n",
      "Train Epoch: 308 [50688/54000 (94%)] Loss: -222269.140625\n",
      "Train Epoch: 308 [52096/54000 (96%)] Loss: -249425.453125\n",
      "    epoch          : 308\n",
      "    loss           : -226651.79265101676\n",
      "    val_loss       : -229870.73464891387\n",
      "Train Epoch: 309 [0/54000 (0%)] Loss: -230738.328125\n",
      "Train Epoch: 309 [1408/54000 (3%)] Loss: -247508.093750\n",
      "Train Epoch: 309 [2816/54000 (5%)] Loss: -224214.500000\n",
      "Train Epoch: 309 [4224/54000 (8%)] Loss: -229930.250000\n",
      "Train Epoch: 309 [5632/54000 (10%)] Loss: -222400.734375\n",
      "Train Epoch: 309 [7040/54000 (13%)] Loss: -218216.015625\n",
      "Train Epoch: 309 [8448/54000 (16%)] Loss: -221142.328125\n",
      "Train Epoch: 309 [9856/54000 (18%)] Loss: -218303.531250\n",
      "Train Epoch: 309 [11264/54000 (21%)] Loss: -223219.250000\n",
      "Train Epoch: 309 [12672/54000 (23%)] Loss: -222111.562500\n",
      "Train Epoch: 309 [14080/54000 (26%)] Loss: -222132.703125\n",
      "Train Epoch: 309 [15488/54000 (29%)] Loss: -225073.890625\n",
      "Train Epoch: 309 [16896/54000 (31%)] Loss: -227281.796875\n",
      "Train Epoch: 309 [18304/54000 (34%)] Loss: -230228.093750\n",
      "Train Epoch: 309 [19712/54000 (37%)] Loss: -228775.468750\n",
      "Train Epoch: 309 [21120/54000 (39%)] Loss: -228462.343750\n",
      "Train Epoch: 309 [22528/54000 (42%)] Loss: -231834.156250\n",
      "Train Epoch: 309 [23936/54000 (44%)] Loss: -230573.000000\n",
      "Train Epoch: 309 [25344/54000 (47%)] Loss: -226179.890625\n",
      "Train Epoch: 309 [26752/54000 (50%)] Loss: -218309.250000\n",
      "Train Epoch: 309 [28160/54000 (52%)] Loss: -217864.796875\n",
      "Train Epoch: 309 [29568/54000 (55%)] Loss: -227449.218750\n",
      "Train Epoch: 309 [30976/54000 (57%)] Loss: -216394.859375\n",
      "Train Epoch: 309 [32384/54000 (60%)] Loss: -222077.468750\n",
      "Train Epoch: 309 [33792/54000 (63%)] Loss: -218445.187500\n",
      "Train Epoch: 309 [35200/54000 (65%)] Loss: -231578.468750\n",
      "Train Epoch: 309 [36608/54000 (68%)] Loss: -221560.718750\n",
      "Train Epoch: 309 [38016/54000 (70%)] Loss: -231038.656250\n",
      "Train Epoch: 309 [39424/54000 (73%)] Loss: -224435.968750\n",
      "Train Epoch: 309 [40832/54000 (76%)] Loss: -228655.484375\n",
      "Train Epoch: 309 [42240/54000 (78%)] Loss: -219038.703125\n",
      "Train Epoch: 309 [43648/54000 (81%)] Loss: -219849.343750\n",
      "Train Epoch: 309 [45056/54000 (83%)] Loss: -218441.703125\n",
      "Train Epoch: 309 [46464/54000 (86%)] Loss: -222358.531250\n",
      "Train Epoch: 309 [47872/54000 (89%)] Loss: -226212.718750\n",
      "Train Epoch: 309 [49280/54000 (91%)] Loss: -222728.218750\n",
      "Train Epoch: 309 [50688/54000 (94%)] Loss: -248198.437500\n",
      "Train Epoch: 309 [52096/54000 (96%)] Loss: -248540.484375\n",
      "    epoch          : 309\n",
      "    loss           : -226723.06653708135\n",
      "    val_loss       : -229380.90651200459\n",
      "Train Epoch: 310 [0/54000 (0%)] Loss: -247934.812500\n",
      "Train Epoch: 310 [1408/54000 (3%)] Loss: -230505.375000\n",
      "Train Epoch: 310 [2816/54000 (5%)] Loss: -217546.593750\n",
      "Train Epoch: 310 [4224/54000 (8%)] Loss: -218027.625000\n",
      "Train Epoch: 310 [5632/54000 (10%)] Loss: -220420.187500\n",
      "Train Epoch: 310 [7040/54000 (13%)] Loss: -249128.687500\n",
      "Train Epoch: 310 [8448/54000 (16%)] Loss: -226036.906250\n",
      "Train Epoch: 310 [9856/54000 (18%)] Loss: -219074.765625\n",
      "Train Epoch: 310 [11264/54000 (21%)] Loss: -220944.484375\n",
      "Train Epoch: 310 [12672/54000 (23%)] Loss: -227320.375000\n",
      "Train Epoch: 310 [14080/54000 (26%)] Loss: -246734.875000\n",
      "Train Epoch: 310 [15488/54000 (29%)] Loss: -224121.765625\n",
      "Train Epoch: 310 [16896/54000 (31%)] Loss: -219905.265625\n",
      "Train Epoch: 310 [18304/54000 (34%)] Loss: -218192.390625\n",
      "Train Epoch: 310 [19712/54000 (37%)] Loss: -218008.703125\n",
      "Train Epoch: 310 [21120/54000 (39%)] Loss: -248382.859375\n",
      "Train Epoch: 310 [22528/54000 (42%)] Loss: -223254.843750\n",
      "Train Epoch: 310 [23936/54000 (44%)] Loss: -223646.312500\n",
      "Train Epoch: 310 [25344/54000 (47%)] Loss: -230422.140625\n",
      "Train Epoch: 310 [26752/54000 (50%)] Loss: -231135.484375\n",
      "Train Epoch: 310 [28160/54000 (52%)] Loss: -214768.265625\n",
      "Train Epoch: 310 [29568/54000 (55%)] Loss: -222406.531250\n",
      "Train Epoch: 310 [30976/54000 (57%)] Loss: -226846.750000\n",
      "Train Epoch: 310 [32384/54000 (60%)] Loss: -215839.078125\n",
      "Train Epoch: 310 [33792/54000 (63%)] Loss: -225052.187500\n",
      "Train Epoch: 310 [35200/54000 (65%)] Loss: -224650.265625\n",
      "Train Epoch: 310 [36608/54000 (68%)] Loss: -225328.687500\n",
      "Train Epoch: 310 [38016/54000 (70%)] Loss: -230725.078125\n",
      "Train Epoch: 310 [39424/54000 (73%)] Loss: -248613.625000\n",
      "Train Epoch: 310 [40832/54000 (76%)] Loss: -215187.609375\n",
      "Train Epoch: 310 [42240/54000 (78%)] Loss: -228032.734375\n",
      "Train Epoch: 310 [43648/54000 (81%)] Loss: -223367.453125\n",
      "Train Epoch: 310 [45056/54000 (83%)] Loss: -227774.593750\n",
      "Train Epoch: 310 [46464/54000 (86%)] Loss: -218604.265625\n",
      "Train Epoch: 310 [47872/54000 (89%)] Loss: -226724.140625\n",
      "Train Epoch: 310 [49280/54000 (91%)] Loss: -221226.984375\n",
      "Train Epoch: 310 [50688/54000 (94%)] Loss: -228886.859375\n",
      "Train Epoch: 310 [52096/54000 (96%)] Loss: -230184.562500\n",
      "    epoch          : 310\n",
      "    loss           : -226652.54007177035\n",
      "    val_loss       : -229680.9900319169\n",
      "Train Epoch: 311 [0/54000 (0%)] Loss: -222325.171875\n",
      "Train Epoch: 311 [1408/54000 (3%)] Loss: -248165.671875\n",
      "Train Epoch: 311 [2816/54000 (5%)] Loss: -216637.015625\n",
      "Train Epoch: 311 [4224/54000 (8%)] Loss: -226555.218750\n",
      "Train Epoch: 311 [5632/54000 (10%)] Loss: -223828.515625\n",
      "Train Epoch: 311 [7040/54000 (13%)] Loss: -226145.015625\n",
      "Train Epoch: 311 [8448/54000 (16%)] Loss: -217709.906250\n",
      "Train Epoch: 311 [9856/54000 (18%)] Loss: -220637.421875\n",
      "Train Epoch: 311 [11264/54000 (21%)] Loss: -218749.437500\n",
      "Train Epoch: 311 [12672/54000 (23%)] Loss: -225465.796875\n",
      "Train Epoch: 311 [14080/54000 (26%)] Loss: -248831.578125\n",
      "Train Epoch: 311 [15488/54000 (29%)] Loss: -229219.156250\n",
      "Train Epoch: 311 [16896/54000 (31%)] Loss: -222133.328125\n",
      "Train Epoch: 311 [18304/54000 (34%)] Loss: -221680.296875\n",
      "Train Epoch: 311 [19712/54000 (37%)] Loss: -217120.468750\n",
      "Train Epoch: 311 [21120/54000 (39%)] Loss: -230230.296875\n",
      "Train Epoch: 311 [22528/54000 (42%)] Loss: -230931.671875\n",
      "Train Epoch: 311 [23936/54000 (44%)] Loss: -229983.062500\n",
      "Train Epoch: 311 [25344/54000 (47%)] Loss: -228291.812500\n",
      "Train Epoch: 311 [26752/54000 (50%)] Loss: -216522.328125\n",
      "Train Epoch: 311 [28160/54000 (52%)] Loss: -219545.531250\n",
      "Train Epoch: 311 [29568/54000 (55%)] Loss: -249630.718750\n",
      "Train Epoch: 311 [30976/54000 (57%)] Loss: -229543.671875\n",
      "Train Epoch: 311 [32384/54000 (60%)] Loss: -230745.359375\n",
      "Train Epoch: 311 [33792/54000 (63%)] Loss: -226775.421875\n",
      "Train Epoch: 311 [35200/54000 (65%)] Loss: -231360.656250\n",
      "Train Epoch: 311 [36608/54000 (68%)] Loss: -249258.218750\n",
      "Train Epoch: 311 [38016/54000 (70%)] Loss: -231018.203125\n",
      "Train Epoch: 311 [39424/54000 (73%)] Loss: -230352.390625\n",
      "Train Epoch: 311 [40832/54000 (76%)] Loss: -220770.703125\n",
      "Train Epoch: 311 [42240/54000 (78%)] Loss: -248092.421875\n",
      "Train Epoch: 311 [43648/54000 (81%)] Loss: -219403.125000\n",
      "Train Epoch: 311 [45056/54000 (83%)] Loss: -219783.531250\n",
      "Train Epoch: 311 [46464/54000 (86%)] Loss: -222654.875000\n",
      "Train Epoch: 311 [47872/54000 (89%)] Loss: -220660.921875\n",
      "Train Epoch: 311 [49280/54000 (91%)] Loss: -230400.250000\n",
      "Train Epoch: 311 [50688/54000 (94%)] Loss: -221241.156250\n",
      "Train Epoch: 311 [52096/54000 (96%)] Loss: -230489.859375\n",
      "    epoch          : 311\n",
      "    loss           : -226757.42194976076\n",
      "    val_loss       : -229940.8065929878\n",
      "Train Epoch: 312 [0/54000 (0%)] Loss: -225309.437500\n",
      "Train Epoch: 312 [1408/54000 (3%)] Loss: -248438.734375\n",
      "Train Epoch: 312 [2816/54000 (5%)] Loss: -217593.734375\n",
      "Train Epoch: 312 [4224/54000 (8%)] Loss: -221909.281250\n",
      "Train Epoch: 312 [5632/54000 (10%)] Loss: -227676.500000\n",
      "Train Epoch: 312 [7040/54000 (13%)] Loss: -227929.953125\n",
      "Train Epoch: 312 [8448/54000 (16%)] Loss: -229082.718750\n",
      "Train Epoch: 312 [9856/54000 (18%)] Loss: -216857.281250\n",
      "Train Epoch: 312 [11264/54000 (21%)] Loss: -221032.265625\n",
      "Train Epoch: 312 [12672/54000 (23%)] Loss: -230596.468750\n",
      "Train Epoch: 312 [14080/54000 (26%)] Loss: -220942.187500\n",
      "Train Epoch: 312 [15488/54000 (29%)] Loss: -228524.562500\n",
      "Train Epoch: 312 [16896/54000 (31%)] Loss: -231932.546875\n",
      "Train Epoch: 312 [18304/54000 (34%)] Loss: -221301.890625\n",
      "Train Epoch: 312 [19712/54000 (37%)] Loss: -221260.171875\n",
      "Train Epoch: 312 [21120/54000 (39%)] Loss: -227297.171875\n",
      "Train Epoch: 312 [22528/54000 (42%)] Loss: -222461.328125\n",
      "Train Epoch: 312 [23936/54000 (44%)] Loss: -230400.578125\n",
      "Train Epoch: 312 [25344/54000 (47%)] Loss: -228862.406250\n",
      "Train Epoch: 312 [26752/54000 (50%)] Loss: -221195.390625\n",
      "Train Epoch: 312 [28160/54000 (52%)] Loss: -225476.250000\n",
      "Train Epoch: 312 [29568/54000 (55%)] Loss: -225815.437500\n",
      "Train Epoch: 312 [30976/54000 (57%)] Loss: -226781.531250\n",
      "Train Epoch: 312 [32384/54000 (60%)] Loss: -219709.390625\n",
      "Train Epoch: 312 [33792/54000 (63%)] Loss: -232168.265625\n",
      "Train Epoch: 312 [35200/54000 (65%)] Loss: -224774.406250\n",
      "Train Epoch: 312 [36608/54000 (68%)] Loss: -229383.718750\n",
      "Train Epoch: 312 [38016/54000 (70%)] Loss: -247100.531250\n",
      "Train Epoch: 312 [39424/54000 (73%)] Loss: -224013.781250\n",
      "Train Epoch: 312 [40832/54000 (76%)] Loss: -218585.625000\n",
      "Train Epoch: 312 [42240/54000 (78%)] Loss: -221359.250000\n",
      "Train Epoch: 312 [43648/54000 (81%)] Loss: -228680.468750\n",
      "Train Epoch: 312 [45056/54000 (83%)] Loss: -217947.921875\n",
      "Train Epoch: 312 [46464/54000 (86%)] Loss: -219671.968750\n",
      "Train Epoch: 312 [47872/54000 (89%)] Loss: -228045.859375\n",
      "Train Epoch: 312 [49280/54000 (91%)] Loss: -226046.328125\n",
      "Train Epoch: 312 [50688/54000 (94%)] Loss: -217798.218750\n",
      "Train Epoch: 312 [52096/54000 (96%)] Loss: -230186.687500\n",
      "    epoch          : 312\n",
      "    loss           : -226718.87985944975\n",
      "    val_loss       : -229750.78008288873\n",
      "Train Epoch: 313 [0/54000 (0%)] Loss: -249288.765625\n",
      "Train Epoch: 313 [1408/54000 (3%)] Loss: -218349.578125\n",
      "Train Epoch: 313 [2816/54000 (5%)] Loss: -230003.765625\n",
      "Train Epoch: 313 [4224/54000 (8%)] Loss: -217719.421875\n",
      "Train Epoch: 313 [5632/54000 (10%)] Loss: -230083.828125\n",
      "Train Epoch: 313 [7040/54000 (13%)] Loss: -220851.156250\n",
      "Train Epoch: 313 [8448/54000 (16%)] Loss: -247928.734375\n",
      "Train Epoch: 313 [9856/54000 (18%)] Loss: -224851.656250\n",
      "Train Epoch: 313 [11264/54000 (21%)] Loss: -219578.671875\n",
      "Train Epoch: 313 [12672/54000 (23%)] Loss: -248154.390625\n",
      "Train Epoch: 313 [14080/54000 (26%)] Loss: -219592.187500\n",
      "Train Epoch: 313 [15488/54000 (29%)] Loss: -229190.500000\n",
      "Train Epoch: 313 [16896/54000 (31%)] Loss: -225699.062500\n",
      "Train Epoch: 313 [18304/54000 (34%)] Loss: -249555.562500\n",
      "Train Epoch: 313 [19712/54000 (37%)] Loss: -220022.875000\n",
      "Train Epoch: 313 [21120/54000 (39%)] Loss: -225869.250000\n",
      "Train Epoch: 313 [22528/54000 (42%)] Loss: -222459.156250\n",
      "Train Epoch: 313 [23936/54000 (44%)] Loss: -248355.484375\n",
      "Train Epoch: 313 [25344/54000 (47%)] Loss: -228808.796875\n",
      "Train Epoch: 313 [26752/54000 (50%)] Loss: -229438.578125\n",
      "Train Epoch: 313 [28160/54000 (52%)] Loss: -221562.046875\n",
      "Train Epoch: 313 [29568/54000 (55%)] Loss: -218971.828125\n",
      "Train Epoch: 313 [30976/54000 (57%)] Loss: -248577.578125\n",
      "Train Epoch: 313 [32384/54000 (60%)] Loss: -222698.703125\n",
      "Train Epoch: 313 [33792/54000 (63%)] Loss: -219637.609375\n",
      "Train Epoch: 313 [35200/54000 (65%)] Loss: -225186.968750\n",
      "Train Epoch: 313 [36608/54000 (68%)] Loss: -217005.687500\n",
      "Train Epoch: 313 [38016/54000 (70%)] Loss: -248751.156250\n",
      "Train Epoch: 313 [39424/54000 (73%)] Loss: -227967.968750\n",
      "Train Epoch: 313 [40832/54000 (76%)] Loss: -230761.703125\n",
      "Train Epoch: 313 [42240/54000 (78%)] Loss: -231428.625000\n",
      "Train Epoch: 313 [43648/54000 (81%)] Loss: -221958.546875\n",
      "Train Epoch: 313 [45056/54000 (83%)] Loss: -222921.406250\n",
      "Train Epoch: 313 [46464/54000 (86%)] Loss: -218885.468750\n",
      "Train Epoch: 313 [47872/54000 (89%)] Loss: -224642.656250\n",
      "Train Epoch: 313 [49280/54000 (91%)] Loss: -217535.968750\n",
      "Train Epoch: 313 [50688/54000 (94%)] Loss: -228049.531250\n",
      "Train Epoch: 313 [52096/54000 (96%)] Loss: -230357.796875\n",
      "    epoch          : 313\n",
      "    loss           : -226819.38296202154\n",
      "    val_loss       : -229741.17690667874\n",
      "Train Epoch: 314 [0/54000 (0%)] Loss: -222487.968750\n",
      "Train Epoch: 314 [1408/54000 (3%)] Loss: -228050.984375\n",
      "Train Epoch: 314 [2816/54000 (5%)] Loss: -229917.562500\n",
      "Train Epoch: 314 [4224/54000 (8%)] Loss: -218291.531250\n",
      "Train Epoch: 314 [5632/54000 (10%)] Loss: -227637.328125\n",
      "Train Epoch: 314 [7040/54000 (13%)] Loss: -223676.140625\n",
      "Train Epoch: 314 [8448/54000 (16%)] Loss: -215998.703125\n",
      "Train Epoch: 314 [9856/54000 (18%)] Loss: -220958.203125\n",
      "Train Epoch: 314 [11264/54000 (21%)] Loss: -228423.156250\n",
      "Train Epoch: 314 [12672/54000 (23%)] Loss: -226350.953125\n",
      "Train Epoch: 314 [14080/54000 (26%)] Loss: -225981.984375\n",
      "Train Epoch: 314 [15488/54000 (29%)] Loss: -224233.562500\n",
      "Train Epoch: 314 [16896/54000 (31%)] Loss: -215654.562500\n",
      "Train Epoch: 314 [18304/54000 (34%)] Loss: -222002.437500\n",
      "Train Epoch: 314 [19712/54000 (37%)] Loss: -224973.343750\n",
      "Train Epoch: 314 [21120/54000 (39%)] Loss: -224967.781250\n",
      "Train Epoch: 314 [22528/54000 (42%)] Loss: -230393.906250\n",
      "Train Epoch: 314 [23936/54000 (44%)] Loss: -224806.984375\n",
      "Train Epoch: 314 [25344/54000 (47%)] Loss: -222012.703125\n",
      "Train Epoch: 314 [26752/54000 (50%)] Loss: -224011.500000\n",
      "Train Epoch: 314 [28160/54000 (52%)] Loss: -227052.406250\n",
      "Train Epoch: 314 [29568/54000 (55%)] Loss: -228684.031250\n",
      "Train Epoch: 314 [30976/54000 (57%)] Loss: -245966.343750\n",
      "Train Epoch: 314 [32384/54000 (60%)] Loss: -221295.687500\n",
      "Train Epoch: 314 [33792/54000 (63%)] Loss: -222854.546875\n",
      "Train Epoch: 314 [35200/54000 (65%)] Loss: -230148.421875\n",
      "Train Epoch: 314 [36608/54000 (68%)] Loss: -229724.031250\n",
      "Train Epoch: 314 [38016/54000 (70%)] Loss: -230200.546875\n",
      "Train Epoch: 314 [39424/54000 (73%)] Loss: -225456.656250\n",
      "Train Epoch: 314 [40832/54000 (76%)] Loss: -222368.953125\n",
      "Train Epoch: 314 [42240/54000 (78%)] Loss: -216455.671875\n",
      "Train Epoch: 314 [43648/54000 (81%)] Loss: -249614.218750\n",
      "Train Epoch: 314 [45056/54000 (83%)] Loss: -230260.062500\n",
      "Train Epoch: 314 [46464/54000 (86%)] Loss: -220849.656250\n",
      "Train Epoch: 314 [47872/54000 (89%)] Loss: -215842.625000\n",
      "Train Epoch: 314 [49280/54000 (91%)] Loss: -225797.296875\n",
      "Train Epoch: 314 [50688/54000 (94%)] Loss: -226290.515625\n",
      "Train Epoch: 314 [52096/54000 (96%)] Loss: -229490.000000\n",
      "    epoch          : 314\n",
      "    loss           : -226797.85986094497\n",
      "    val_loss       : -229981.92157131288\n",
      "Train Epoch: 315 [0/54000 (0%)] Loss: -246901.453125\n",
      "Train Epoch: 315 [1408/54000 (3%)] Loss: -229714.062500\n",
      "Train Epoch: 315 [2816/54000 (5%)] Loss: -226683.984375\n",
      "Train Epoch: 315 [4224/54000 (8%)] Loss: -231266.109375\n",
      "Train Epoch: 315 [5632/54000 (10%)] Loss: -221219.765625\n",
      "Train Epoch: 315 [7040/54000 (13%)] Loss: -247150.109375\n",
      "Train Epoch: 315 [8448/54000 (16%)] Loss: -230916.640625\n",
      "Train Epoch: 315 [9856/54000 (18%)] Loss: -229289.937500\n",
      "Train Epoch: 315 [11264/54000 (21%)] Loss: -221117.500000\n",
      "Train Epoch: 315 [12672/54000 (23%)] Loss: -222888.812500\n",
      "Train Epoch: 315 [14080/54000 (26%)] Loss: -247987.421875\n",
      "Train Epoch: 315 [15488/54000 (29%)] Loss: -221655.593750\n",
      "Train Epoch: 315 [16896/54000 (31%)] Loss: -227217.187500\n",
      "Train Epoch: 315 [18304/54000 (34%)] Loss: -228264.812500\n",
      "Train Epoch: 315 [19712/54000 (37%)] Loss: -227375.796875\n",
      "Train Epoch: 315 [21120/54000 (39%)] Loss: -217994.375000\n",
      "Train Epoch: 315 [22528/54000 (42%)] Loss: -220186.375000\n",
      "Train Epoch: 315 [23936/54000 (44%)] Loss: -248260.328125\n",
      "Train Epoch: 315 [25344/54000 (47%)] Loss: -225991.031250\n",
      "Train Epoch: 315 [26752/54000 (50%)] Loss: -230154.031250\n",
      "Train Epoch: 315 [28160/54000 (52%)] Loss: -222962.015625\n",
      "Train Epoch: 315 [29568/54000 (55%)] Loss: -230203.015625\n",
      "Train Epoch: 315 [30976/54000 (57%)] Loss: -230082.296875\n",
      "Train Epoch: 315 [32384/54000 (60%)] Loss: -232156.031250\n",
      "Train Epoch: 315 [33792/54000 (63%)] Loss: -223007.125000\n",
      "Train Epoch: 315 [35200/54000 (65%)] Loss: -219944.968750\n",
      "Train Epoch: 315 [36608/54000 (68%)] Loss: -221626.078125\n",
      "Train Epoch: 315 [38016/54000 (70%)] Loss: -217804.656250\n",
      "Train Epoch: 315 [39424/54000 (73%)] Loss: -223052.031250\n",
      "Train Epoch: 315 [40832/54000 (76%)] Loss: -225550.015625\n",
      "Train Epoch: 315 [42240/54000 (78%)] Loss: -250665.296875\n",
      "Train Epoch: 315 [43648/54000 (81%)] Loss: -230336.406250\n",
      "Train Epoch: 315 [45056/54000 (83%)] Loss: -229113.609375\n",
      "Train Epoch: 315 [46464/54000 (86%)] Loss: -225603.796875\n",
      "Train Epoch: 315 [47872/54000 (89%)] Loss: -224456.031250\n",
      "Train Epoch: 315 [49280/54000 (91%)] Loss: -231659.406250\n",
      "Train Epoch: 315 [50688/54000 (94%)] Loss: -223475.046875\n",
      "Train Epoch: 315 [52096/54000 (96%)] Loss: -220575.000000\n",
      "    epoch          : 315\n",
      "    loss           : -226779.45663875598\n",
      "    val_loss       : -229804.77305044778\n",
      "Train Epoch: 316 [0/54000 (0%)] Loss: -220067.437500\n",
      "Train Epoch: 316 [1408/54000 (3%)] Loss: -249847.062500\n",
      "Train Epoch: 316 [2816/54000 (5%)] Loss: -217465.625000\n",
      "Train Epoch: 316 [4224/54000 (8%)] Loss: -221491.843750\n",
      "Train Epoch: 316 [5632/54000 (10%)] Loss: -218680.984375\n",
      "Train Epoch: 316 [7040/54000 (13%)] Loss: -216269.281250\n",
      "Train Epoch: 316 [8448/54000 (16%)] Loss: -249081.656250\n",
      "Train Epoch: 316 [9856/54000 (18%)] Loss: -217829.968750\n",
      "Train Epoch: 316 [11264/54000 (21%)] Loss: -226603.281250\n",
      "Train Epoch: 316 [12672/54000 (23%)] Loss: -222211.312500\n",
      "Train Epoch: 316 [14080/54000 (26%)] Loss: -228368.843750\n",
      "Train Epoch: 316 [15488/54000 (29%)] Loss: -249218.468750\n",
      "Train Epoch: 316 [16896/54000 (31%)] Loss: -219171.718750\n",
      "Train Epoch: 316 [18304/54000 (34%)] Loss: -223971.281250\n",
      "Train Epoch: 316 [19712/54000 (37%)] Loss: -230730.187500\n",
      "Train Epoch: 316 [21120/54000 (39%)] Loss: -246992.093750\n",
      "Train Epoch: 316 [22528/54000 (42%)] Loss: -218684.765625\n",
      "Train Epoch: 316 [23936/54000 (44%)] Loss: -221552.593750\n",
      "Train Epoch: 316 [25344/54000 (47%)] Loss: -230860.578125\n",
      "Train Epoch: 316 [26752/54000 (50%)] Loss: -221594.343750\n",
      "Train Epoch: 316 [28160/54000 (52%)] Loss: -247814.781250\n",
      "Train Epoch: 316 [29568/54000 (55%)] Loss: -225447.343750\n",
      "Train Epoch: 316 [30976/54000 (57%)] Loss: -220821.375000\n",
      "Train Epoch: 316 [32384/54000 (60%)] Loss: -227318.406250\n",
      "Train Epoch: 316 [33792/54000 (63%)] Loss: -247886.843750\n",
      "Train Epoch: 316 [35200/54000 (65%)] Loss: -215919.875000\n",
      "Train Epoch: 316 [36608/54000 (68%)] Loss: -230291.453125\n",
      "Train Epoch: 316 [38016/54000 (70%)] Loss: -228893.390625\n",
      "Train Epoch: 316 [39424/54000 (73%)] Loss: -230969.062500\n",
      "Train Epoch: 316 [40832/54000 (76%)] Loss: -229333.000000\n",
      "Train Epoch: 316 [42240/54000 (78%)] Loss: -222158.421875\n",
      "Train Epoch: 316 [43648/54000 (81%)] Loss: -216465.484375\n",
      "Train Epoch: 316 [45056/54000 (83%)] Loss: -249787.328125\n",
      "Train Epoch: 316 [46464/54000 (86%)] Loss: -220873.687500\n",
      "Train Epoch: 316 [47872/54000 (89%)] Loss: -220305.578125\n",
      "Train Epoch: 316 [49280/54000 (91%)] Loss: -223361.062500\n",
      "Train Epoch: 316 [50688/54000 (94%)] Loss: -229450.234375\n",
      "Train Epoch: 316 [52096/54000 (96%)] Loss: -217094.765625\n",
      "    epoch          : 316\n",
      "    loss           : -226859.69949910286\n",
      "    val_loss       : -229166.31351824504\n",
      "Train Epoch: 317 [0/54000 (0%)] Loss: -249739.578125\n",
      "Train Epoch: 317 [1408/54000 (3%)] Loss: -249639.906250\n",
      "Train Epoch: 317 [2816/54000 (5%)] Loss: -230799.687500\n",
      "Train Epoch: 317 [4224/54000 (8%)] Loss: -230833.953125\n",
      "Train Epoch: 317 [5632/54000 (10%)] Loss: -222465.156250\n",
      "Train Epoch: 317 [7040/54000 (13%)] Loss: -221033.343750\n",
      "Train Epoch: 317 [8448/54000 (16%)] Loss: -219883.343750\n",
      "Train Epoch: 317 [9856/54000 (18%)] Loss: -248157.000000\n",
      "Train Epoch: 317 [11264/54000 (21%)] Loss: -225910.375000\n",
      "Train Epoch: 317 [12672/54000 (23%)] Loss: -217796.171875\n",
      "Train Epoch: 317 [14080/54000 (26%)] Loss: -220373.531250\n",
      "Train Epoch: 317 [15488/54000 (29%)] Loss: -220914.468750\n",
      "Train Epoch: 317 [16896/54000 (31%)] Loss: -229157.968750\n",
      "Train Epoch: 317 [18304/54000 (34%)] Loss: -229886.843750\n",
      "Train Epoch: 317 [19712/54000 (37%)] Loss: -229835.468750\n",
      "Train Epoch: 317 [21120/54000 (39%)] Loss: -220820.140625\n",
      "Train Epoch: 317 [22528/54000 (42%)] Loss: -223151.843750\n",
      "Train Epoch: 317 [23936/54000 (44%)] Loss: -225614.375000\n",
      "Train Epoch: 317 [25344/54000 (47%)] Loss: -225465.328125\n",
      "Train Epoch: 317 [26752/54000 (50%)] Loss: -226189.265625\n",
      "Train Epoch: 317 [28160/54000 (52%)] Loss: -220345.546875\n",
      "Train Epoch: 317 [29568/54000 (55%)] Loss: -221263.734375\n",
      "Train Epoch: 317 [30976/54000 (57%)] Loss: -224080.890625\n",
      "Train Epoch: 317 [32384/54000 (60%)] Loss: -225239.968750\n",
      "Train Epoch: 317 [33792/54000 (63%)] Loss: -229951.484375\n",
      "Train Epoch: 317 [35200/54000 (65%)] Loss: -219996.406250\n",
      "Train Epoch: 317 [36608/54000 (68%)] Loss: -248405.750000\n",
      "Train Epoch: 317 [38016/54000 (70%)] Loss: -217745.484375\n",
      "Train Epoch: 317 [39424/54000 (73%)] Loss: -227229.546875\n",
      "Train Epoch: 317 [40832/54000 (76%)] Loss: -225615.484375\n",
      "Train Epoch: 317 [42240/54000 (78%)] Loss: -230950.953125\n",
      "Train Epoch: 317 [43648/54000 (81%)] Loss: -229198.140625\n",
      "Train Epoch: 317 [45056/54000 (83%)] Loss: -224025.812500\n",
      "Train Epoch: 317 [46464/54000 (86%)] Loss: -225754.781250\n",
      "Train Epoch: 317 [47872/54000 (89%)] Loss: -230704.265625\n",
      "Train Epoch: 317 [49280/54000 (91%)] Loss: -231909.968750\n",
      "Train Epoch: 317 [50688/54000 (94%)] Loss: -229373.656250\n",
      "Train Epoch: 317 [52096/54000 (96%)] Loss: -229072.078125\n",
      "    epoch          : 317\n",
      "    loss           : -226994.12668211723\n",
      "    val_loss       : -229537.01527963034\n",
      "Train Epoch: 318 [0/54000 (0%)] Loss: -221803.421875\n",
      "Train Epoch: 318 [1408/54000 (3%)] Loss: -220840.812500\n",
      "Train Epoch: 318 [2816/54000 (5%)] Loss: -219332.000000\n",
      "Train Epoch: 318 [4224/54000 (8%)] Loss: -213568.468750\n",
      "Train Epoch: 318 [5632/54000 (10%)] Loss: -218321.578125\n",
      "Train Epoch: 318 [7040/54000 (13%)] Loss: -227137.312500\n",
      "Train Epoch: 318 [8448/54000 (16%)] Loss: -227932.078125\n",
      "Train Epoch: 318 [9856/54000 (18%)] Loss: -228119.984375\n",
      "Train Epoch: 318 [11264/54000 (21%)] Loss: -228313.781250\n",
      "Train Epoch: 318 [12672/54000 (23%)] Loss: -232407.781250\n",
      "Train Epoch: 318 [14080/54000 (26%)] Loss: -230079.390625\n",
      "Train Epoch: 318 [15488/54000 (29%)] Loss: -248339.421875\n",
      "Train Epoch: 318 [16896/54000 (31%)] Loss: -220822.062500\n",
      "Train Epoch: 318 [18304/54000 (34%)] Loss: -220416.765625\n",
      "Train Epoch: 318 [19712/54000 (37%)] Loss: -228345.828125\n",
      "Train Epoch: 318 [21120/54000 (39%)] Loss: -229687.812500\n",
      "Train Epoch: 318 [22528/54000 (42%)] Loss: -231414.312500\n",
      "Train Epoch: 318 [23936/54000 (44%)] Loss: -229624.000000\n",
      "Train Epoch: 318 [25344/54000 (47%)] Loss: -225942.125000\n",
      "Train Epoch: 318 [26752/54000 (50%)] Loss: -221075.281250\n",
      "Train Epoch: 318 [28160/54000 (52%)] Loss: -248163.250000\n",
      "Train Epoch: 318 [29568/54000 (55%)] Loss: -222389.078125\n",
      "Train Epoch: 318 [30976/54000 (57%)] Loss: -231467.718750\n",
      "Train Epoch: 318 [32384/54000 (60%)] Loss: -230847.156250\n",
      "Train Epoch: 318 [33792/54000 (63%)] Loss: -247704.484375\n",
      "Train Epoch: 318 [35200/54000 (65%)] Loss: -222260.109375\n",
      "Train Epoch: 318 [36608/54000 (68%)] Loss: -230032.046875\n",
      "Train Epoch: 318 [38016/54000 (70%)] Loss: -231314.312500\n",
      "Train Epoch: 318 [39424/54000 (73%)] Loss: -230788.328125\n",
      "Train Epoch: 318 [40832/54000 (76%)] Loss: -229877.640625\n",
      "Train Epoch: 318 [42240/54000 (78%)] Loss: -220125.562500\n",
      "Train Epoch: 318 [43648/54000 (81%)] Loss: -222724.015625\n",
      "Train Epoch: 318 [45056/54000 (83%)] Loss: -217753.187500\n",
      "Train Epoch: 318 [46464/54000 (86%)] Loss: -221263.625000\n",
      "Train Epoch: 318 [47872/54000 (89%)] Loss: -223054.031250\n",
      "Train Epoch: 318 [49280/54000 (91%)] Loss: -227067.656250\n",
      "Train Epoch: 318 [50688/54000 (94%)] Loss: -248512.531250\n",
      "Train Epoch: 318 [52096/54000 (96%)] Loss: -226094.968750\n",
      "    epoch          : 318\n",
      "    loss           : -226998.27463367226\n",
      "    val_loss       : -230055.14851491046\n",
      "Train Epoch: 319 [0/54000 (0%)] Loss: -248013.515625\n",
      "Train Epoch: 319 [1408/54000 (3%)] Loss: -229951.859375\n",
      "Train Epoch: 319 [2816/54000 (5%)] Loss: -220953.484375\n",
      "Train Epoch: 319 [4224/54000 (8%)] Loss: -220266.250000\n",
      "Train Epoch: 319 [5632/54000 (10%)] Loss: -233365.593750\n",
      "Train Epoch: 319 [7040/54000 (13%)] Loss: -218982.171875\n",
      "Train Epoch: 319 [8448/54000 (16%)] Loss: -218388.406250\n",
      "Train Epoch: 319 [9856/54000 (18%)] Loss: -231331.828125\n",
      "Train Epoch: 319 [11264/54000 (21%)] Loss: -221503.015625\n",
      "Train Epoch: 319 [12672/54000 (23%)] Loss: -220040.734375\n",
      "Train Epoch: 319 [14080/54000 (26%)] Loss: -217258.812500\n",
      "Train Epoch: 319 [15488/54000 (29%)] Loss: -247517.312500\n",
      "Train Epoch: 319 [16896/54000 (31%)] Loss: -224016.031250\n",
      "Train Epoch: 319 [18304/54000 (34%)] Loss: -230580.218750\n",
      "Train Epoch: 319 [19712/54000 (37%)] Loss: -222336.968750\n",
      "Train Epoch: 319 [21120/54000 (39%)] Loss: -232536.093750\n",
      "Train Epoch: 319 [22528/54000 (42%)] Loss: -221929.578125\n",
      "Train Epoch: 319 [23936/54000 (44%)] Loss: -225693.125000\n",
      "Train Epoch: 319 [25344/54000 (47%)] Loss: -208222.312500\n",
      "Train Epoch: 319 [26752/54000 (50%)] Loss: -222619.343750\n",
      "Train Epoch: 319 [28160/54000 (52%)] Loss: -249674.218750\n",
      "Train Epoch: 319 [29568/54000 (55%)] Loss: -231020.156250\n",
      "Train Epoch: 319 [30976/54000 (57%)] Loss: -222608.375000\n",
      "Train Epoch: 319 [32384/54000 (60%)] Loss: -248547.593750\n",
      "Train Epoch: 319 [33792/54000 (63%)] Loss: -226356.390625\n",
      "Train Epoch: 319 [35200/54000 (65%)] Loss: -226483.625000\n",
      "Train Epoch: 319 [36608/54000 (68%)] Loss: -218591.671875\n",
      "Train Epoch: 319 [38016/54000 (70%)] Loss: -230427.328125\n",
      "Train Epoch: 319 [39424/54000 (73%)] Loss: -229798.906250\n",
      "Train Epoch: 319 [40832/54000 (76%)] Loss: -220478.312500\n",
      "Train Epoch: 319 [42240/54000 (78%)] Loss: -229935.312500\n",
      "Train Epoch: 319 [43648/54000 (81%)] Loss: -217482.750000\n",
      "Train Epoch: 319 [45056/54000 (83%)] Loss: -249225.156250\n",
      "Train Epoch: 319 [46464/54000 (86%)] Loss: -218243.484375\n",
      "Train Epoch: 319 [47872/54000 (89%)] Loss: -219562.156250\n",
      "Train Epoch: 319 [49280/54000 (91%)] Loss: -224983.500000\n",
      "Train Epoch: 319 [50688/54000 (94%)] Loss: -228609.046875\n",
      "Train Epoch: 319 [52096/54000 (96%)] Loss: -217864.703125\n",
      "    epoch          : 319\n",
      "    loss           : -226830.12877541865\n",
      "    val_loss       : -229322.9399592702\n",
      "Train Epoch: 320 [0/54000 (0%)] Loss: -229795.468750\n",
      "Train Epoch: 320 [1408/54000 (3%)] Loss: -227970.328125\n",
      "Train Epoch: 320 [2816/54000 (5%)] Loss: -229089.531250\n",
      "Train Epoch: 320 [4224/54000 (8%)] Loss: -218960.906250\n",
      "Train Epoch: 320 [5632/54000 (10%)] Loss: -224060.953125\n",
      "Train Epoch: 320 [7040/54000 (13%)] Loss: -225142.781250\n",
      "Train Epoch: 320 [8448/54000 (16%)] Loss: -222024.984375\n",
      "Train Epoch: 320 [9856/54000 (18%)] Loss: -218247.109375\n",
      "Train Epoch: 320 [11264/54000 (21%)] Loss: -216155.218750\n",
      "Train Epoch: 320 [12672/54000 (23%)] Loss: -222049.390625\n",
      "Train Epoch: 320 [14080/54000 (26%)] Loss: -230729.187500\n",
      "Train Epoch: 320 [15488/54000 (29%)] Loss: -229935.578125\n",
      "Train Epoch: 320 [16896/54000 (31%)] Loss: -228506.875000\n",
      "Train Epoch: 320 [18304/54000 (34%)] Loss: -230296.859375\n",
      "Train Epoch: 320 [19712/54000 (37%)] Loss: -218454.093750\n",
      "Train Epoch: 320 [21120/54000 (39%)] Loss: -247981.437500\n",
      "Train Epoch: 320 [22528/54000 (42%)] Loss: -221501.531250\n",
      "Train Epoch: 320 [23936/54000 (44%)] Loss: -229157.687500\n",
      "Train Epoch: 320 [25344/54000 (47%)] Loss: -223975.531250\n",
      "Train Epoch: 320 [26752/54000 (50%)] Loss: -224598.375000\n",
      "Train Epoch: 320 [28160/54000 (52%)] Loss: -220471.218750\n",
      "Train Epoch: 320 [29568/54000 (55%)] Loss: -220565.703125\n",
      "Train Epoch: 320 [30976/54000 (57%)] Loss: -228262.656250\n",
      "Train Epoch: 320 [32384/54000 (60%)] Loss: -247309.656250\n",
      "Train Epoch: 320 [33792/54000 (63%)] Loss: -221575.703125\n",
      "Train Epoch: 320 [35200/54000 (65%)] Loss: -229794.140625\n",
      "Train Epoch: 320 [36608/54000 (68%)] Loss: -230928.968750\n",
      "Train Epoch: 320 [38016/54000 (70%)] Loss: -225066.093750\n",
      "Train Epoch: 320 [39424/54000 (73%)] Loss: -224402.875000\n",
      "Train Epoch: 320 [40832/54000 (76%)] Loss: -217832.640625\n",
      "Train Epoch: 320 [42240/54000 (78%)] Loss: -223698.812500\n",
      "Train Epoch: 320 [43648/54000 (81%)] Loss: -231227.281250\n",
      "Train Epoch: 320 [45056/54000 (83%)] Loss: -225168.875000\n",
      "Train Epoch: 320 [46464/54000 (86%)] Loss: -249065.968750\n",
      "Train Epoch: 320 [47872/54000 (89%)] Loss: -218967.859375\n",
      "Train Epoch: 320 [49280/54000 (91%)] Loss: -226304.796875\n",
      "Train Epoch: 320 [50688/54000 (94%)] Loss: -230738.703125\n",
      "Train Epoch: 320 [52096/54000 (96%)] Loss: -230692.625000\n",
      "    epoch          : 320\n",
      "    loss           : -227088.4444527512\n",
      "    val_loss       : -230050.98872784871\n",
      "Train Epoch: 321 [0/54000 (0%)] Loss: -249315.890625\n",
      "Train Epoch: 321 [1408/54000 (3%)] Loss: -217373.531250\n",
      "Train Epoch: 321 [2816/54000 (5%)] Loss: -224826.281250\n",
      "Train Epoch: 321 [4224/54000 (8%)] Loss: -229980.031250\n",
      "Train Epoch: 321 [5632/54000 (10%)] Loss: -222064.062500\n",
      "Train Epoch: 321 [7040/54000 (13%)] Loss: -219672.968750\n",
      "Train Epoch: 321 [8448/54000 (16%)] Loss: -247964.734375\n",
      "Train Epoch: 321 [9856/54000 (18%)] Loss: -217910.296875\n",
      "Train Epoch: 321 [11264/54000 (21%)] Loss: -228122.359375\n",
      "Train Epoch: 321 [12672/54000 (23%)] Loss: -231546.468750\n",
      "Train Epoch: 321 [14080/54000 (26%)] Loss: -247320.062500\n",
      "Train Epoch: 321 [15488/54000 (29%)] Loss: -229969.250000\n",
      "Train Epoch: 321 [16896/54000 (31%)] Loss: -220540.062500\n",
      "Train Epoch: 321 [18304/54000 (34%)] Loss: -223042.062500\n",
      "Train Epoch: 321 [19712/54000 (37%)] Loss: -222007.546875\n",
      "Train Epoch: 321 [21120/54000 (39%)] Loss: -218451.375000\n",
      "Train Epoch: 321 [22528/54000 (42%)] Loss: -220981.359375\n",
      "Train Epoch: 321 [23936/54000 (44%)] Loss: -226537.031250\n",
      "Train Epoch: 321 [25344/54000 (47%)] Loss: -219428.937500\n",
      "Train Epoch: 321 [26752/54000 (50%)] Loss: -248854.125000\n",
      "Train Epoch: 321 [28160/54000 (52%)] Loss: -222215.781250\n",
      "Train Epoch: 321 [29568/54000 (55%)] Loss: -224931.046875\n",
      "Train Epoch: 321 [30976/54000 (57%)] Loss: -220836.953125\n",
      "Train Epoch: 321 [32384/54000 (60%)] Loss: -246958.531250\n",
      "Train Epoch: 321 [33792/54000 (63%)] Loss: -231759.750000\n",
      "Train Epoch: 321 [35200/54000 (65%)] Loss: -220107.984375\n",
      "Train Epoch: 321 [36608/54000 (68%)] Loss: -219827.484375\n",
      "Train Epoch: 321 [38016/54000 (70%)] Loss: -224584.578125\n",
      "Train Epoch: 321 [39424/54000 (73%)] Loss: -230855.109375\n",
      "Train Epoch: 321 [40832/54000 (76%)] Loss: -249472.187500\n",
      "Train Epoch: 321 [42240/54000 (78%)] Loss: -232365.359375\n",
      "Train Epoch: 321 [43648/54000 (81%)] Loss: -231401.500000\n",
      "Train Epoch: 321 [45056/54000 (83%)] Loss: -230906.343750\n",
      "Train Epoch: 321 [46464/54000 (86%)] Loss: -224069.984375\n",
      "Train Epoch: 321 [47872/54000 (89%)] Loss: -226354.250000\n",
      "Train Epoch: 321 [49280/54000 (91%)] Loss: -217651.593750\n",
      "Train Epoch: 321 [50688/54000 (94%)] Loss: -247009.484375\n",
      "Train Epoch: 321 [52096/54000 (96%)] Loss: -222721.531250\n",
      "    epoch          : 321\n",
      "    loss           : -226951.07042464116\n",
      "    val_loss       : -229679.251155202\n",
      "Train Epoch: 322 [0/54000 (0%)] Loss: -248267.703125\n",
      "Train Epoch: 322 [1408/54000 (3%)] Loss: -220876.375000\n",
      "Train Epoch: 322 [2816/54000 (5%)] Loss: -231102.796875\n",
      "Train Epoch: 322 [4224/54000 (8%)] Loss: -230700.515625\n",
      "Train Epoch: 322 [5632/54000 (10%)] Loss: -225248.812500\n",
      "Train Epoch: 322 [7040/54000 (13%)] Loss: -217753.593750\n",
      "Train Epoch: 322 [8448/54000 (16%)] Loss: -220385.093750\n",
      "Train Epoch: 322 [9856/54000 (18%)] Loss: -217271.546875\n",
      "Train Epoch: 322 [11264/54000 (21%)] Loss: -221925.671875\n",
      "Train Epoch: 322 [12672/54000 (23%)] Loss: -218433.625000\n",
      "Train Epoch: 322 [14080/54000 (26%)] Loss: -250788.281250\n",
      "Train Epoch: 322 [15488/54000 (29%)] Loss: -219374.156250\n",
      "Train Epoch: 322 [16896/54000 (31%)] Loss: -220198.109375\n",
      "Train Epoch: 322 [18304/54000 (34%)] Loss: -219329.656250\n",
      "Train Epoch: 322 [19712/54000 (37%)] Loss: -248997.296875\n",
      "Train Epoch: 322 [21120/54000 (39%)] Loss: -230849.625000\n",
      "Train Epoch: 322 [22528/54000 (42%)] Loss: -226442.984375\n",
      "Train Epoch: 322 [23936/54000 (44%)] Loss: -220887.859375\n",
      "Train Epoch: 322 [25344/54000 (47%)] Loss: -219289.734375\n",
      "Train Epoch: 322 [26752/54000 (50%)] Loss: -249616.593750\n",
      "Train Epoch: 322 [28160/54000 (52%)] Loss: -224174.937500\n",
      "Train Epoch: 322 [29568/54000 (55%)] Loss: -231562.203125\n",
      "Train Epoch: 322 [30976/54000 (57%)] Loss: -231509.718750\n",
      "Train Epoch: 322 [32384/54000 (60%)] Loss: -221772.734375\n",
      "Train Epoch: 322 [33792/54000 (63%)] Loss: -230734.796875\n",
      "Train Epoch: 322 [35200/54000 (65%)] Loss: -224324.593750\n",
      "Train Epoch: 322 [36608/54000 (68%)] Loss: -245423.296875\n",
      "Train Epoch: 322 [38016/54000 (70%)] Loss: -219700.703125\n",
      "Train Epoch: 322 [39424/54000 (73%)] Loss: -218599.796875\n",
      "Train Epoch: 322 [40832/54000 (76%)] Loss: -225534.531250\n",
      "Train Epoch: 322 [42240/54000 (78%)] Loss: -224689.906250\n",
      "Train Epoch: 322 [43648/54000 (81%)] Loss: -220686.562500\n",
      "Train Epoch: 322 [45056/54000 (83%)] Loss: -224161.187500\n",
      "Train Epoch: 322 [46464/54000 (86%)] Loss: -221165.906250\n",
      "Train Epoch: 322 [47872/54000 (89%)] Loss: -223987.375000\n",
      "Train Epoch: 322 [49280/54000 (91%)] Loss: -218857.250000\n",
      "Train Epoch: 322 [50688/54000 (94%)] Loss: -247627.734375\n",
      "Train Epoch: 322 [52096/54000 (96%)] Loss: -221637.218750\n",
      "    epoch          : 322\n",
      "    loss           : -226983.80565938994\n",
      "    val_loss       : -229614.84585199124\n",
      "Train Epoch: 323 [0/54000 (0%)] Loss: -222353.218750\n",
      "Train Epoch: 323 [1408/54000 (3%)] Loss: -220197.437500\n",
      "Train Epoch: 323 [2816/54000 (5%)] Loss: -218607.171875\n",
      "Train Epoch: 323 [4224/54000 (8%)] Loss: -226234.609375\n",
      "Train Epoch: 323 [5632/54000 (10%)] Loss: -224915.375000\n",
      "Train Epoch: 323 [7040/54000 (13%)] Loss: -219099.734375\n",
      "Train Epoch: 323 [8448/54000 (16%)] Loss: -222147.937500\n",
      "Train Epoch: 323 [9856/54000 (18%)] Loss: -226404.421875\n",
      "Train Epoch: 323 [11264/54000 (21%)] Loss: -228154.343750\n",
      "Train Epoch: 323 [12672/54000 (23%)] Loss: -229264.734375\n",
      "Train Epoch: 323 [14080/54000 (26%)] Loss: -227387.984375\n",
      "Train Epoch: 323 [15488/54000 (29%)] Loss: -229897.343750\n",
      "Train Epoch: 323 [16896/54000 (31%)] Loss: -230058.515625\n",
      "Train Epoch: 323 [18304/54000 (34%)] Loss: -218426.718750\n",
      "Train Epoch: 323 [19712/54000 (37%)] Loss: -225749.156250\n",
      "Train Epoch: 323 [21120/54000 (39%)] Loss: -223489.593750\n",
      "Train Epoch: 323 [22528/54000 (42%)] Loss: -222221.687500\n",
      "Train Epoch: 323 [23936/54000 (44%)] Loss: -223519.812500\n",
      "Train Epoch: 323 [25344/54000 (47%)] Loss: -225777.250000\n",
      "Train Epoch: 323 [26752/54000 (50%)] Loss: -229507.734375\n",
      "Train Epoch: 323 [28160/54000 (52%)] Loss: -227381.500000\n",
      "Train Epoch: 323 [29568/54000 (55%)] Loss: -248488.781250\n",
      "Train Epoch: 323 [30976/54000 (57%)] Loss: -223551.125000\n",
      "Train Epoch: 323 [32384/54000 (60%)] Loss: -231872.640625\n",
      "Train Epoch: 323 [33792/54000 (63%)] Loss: -219125.015625\n",
      "Train Epoch: 323 [35200/54000 (65%)] Loss: -220990.921875\n",
      "Train Epoch: 323 [36608/54000 (68%)] Loss: -232121.250000\n",
      "Train Epoch: 323 [38016/54000 (70%)] Loss: -220156.734375\n",
      "Train Epoch: 323 [39424/54000 (73%)] Loss: -249448.468750\n",
      "Train Epoch: 323 [40832/54000 (76%)] Loss: -222032.031250\n",
      "Train Epoch: 323 [42240/54000 (78%)] Loss: -230440.421875\n",
      "Train Epoch: 323 [43648/54000 (81%)] Loss: -230659.875000\n",
      "Train Epoch: 323 [45056/54000 (83%)] Loss: -220194.031250\n",
      "Train Epoch: 323 [46464/54000 (86%)] Loss: -225089.265625\n",
      "Train Epoch: 323 [47872/54000 (89%)] Loss: -229667.015625\n",
      "Train Epoch: 323 [49280/54000 (91%)] Loss: -222265.875000\n",
      "Train Epoch: 323 [50688/54000 (94%)] Loss: -246941.390625\n",
      "Train Epoch: 323 [52096/54000 (96%)] Loss: -227792.625000\n",
      "    epoch          : 323\n",
      "    loss           : -227003.91219348085\n",
      "    val_loss       : -229940.14702029346\n",
      "Train Epoch: 324 [0/54000 (0%)] Loss: -249702.140625\n",
      "Train Epoch: 324 [1408/54000 (3%)] Loss: -218409.562500\n",
      "Train Epoch: 324 [2816/54000 (5%)] Loss: -217531.984375\n",
      "Train Epoch: 324 [4224/54000 (8%)] Loss: -221680.906250\n",
      "Train Epoch: 324 [5632/54000 (10%)] Loss: -230157.640625\n",
      "Train Epoch: 324 [7040/54000 (13%)] Loss: -222420.968750\n",
      "Train Epoch: 324 [8448/54000 (16%)] Loss: -225124.312500\n",
      "Train Epoch: 324 [9856/54000 (18%)] Loss: -250831.500000\n",
      "Train Epoch: 324 [11264/54000 (21%)] Loss: -227961.359375\n",
      "Train Epoch: 324 [12672/54000 (23%)] Loss: -221049.046875\n",
      "Train Epoch: 324 [14080/54000 (26%)] Loss: -225267.250000\n",
      "Train Epoch: 324 [15488/54000 (29%)] Loss: -250502.031250\n",
      "Train Epoch: 324 [16896/54000 (31%)] Loss: -226525.250000\n",
      "Train Epoch: 324 [18304/54000 (34%)] Loss: -226075.953125\n",
      "Train Epoch: 324 [19712/54000 (37%)] Loss: -216818.609375\n",
      "Train Epoch: 324 [21120/54000 (39%)] Loss: -222521.171875\n",
      "Train Epoch: 324 [22528/54000 (42%)] Loss: -222437.531250\n",
      "Train Epoch: 324 [23936/54000 (44%)] Loss: -231310.218750\n",
      "Train Epoch: 324 [25344/54000 (47%)] Loss: -231152.390625\n",
      "Train Epoch: 324 [26752/54000 (50%)] Loss: -218550.281250\n",
      "Train Epoch: 324 [28160/54000 (52%)] Loss: -250128.234375\n",
      "Train Epoch: 324 [29568/54000 (55%)] Loss: -225883.421875\n",
      "Train Epoch: 324 [30976/54000 (57%)] Loss: -221226.359375\n",
      "Train Epoch: 324 [32384/54000 (60%)] Loss: -228661.203125\n",
      "Train Epoch: 324 [33792/54000 (63%)] Loss: -250214.187500\n",
      "Train Epoch: 324 [35200/54000 (65%)] Loss: -228877.890625\n",
      "Train Epoch: 324 [36608/54000 (68%)] Loss: -227099.171875\n",
      "Train Epoch: 324 [38016/54000 (70%)] Loss: -218038.828125\n",
      "Train Epoch: 324 [39424/54000 (73%)] Loss: -219100.390625\n",
      "Train Epoch: 324 [40832/54000 (76%)] Loss: -249612.671875\n",
      "Train Epoch: 324 [42240/54000 (78%)] Loss: -231570.468750\n",
      "Train Epoch: 324 [43648/54000 (81%)] Loss: -250586.609375\n",
      "Train Epoch: 324 [45056/54000 (83%)] Loss: -225703.375000\n",
      "Train Epoch: 324 [46464/54000 (86%)] Loss: -220638.390625\n",
      "Train Epoch: 324 [47872/54000 (89%)] Loss: -220436.234375\n",
      "Train Epoch: 324 [49280/54000 (91%)] Loss: -229855.312500\n",
      "Train Epoch: 324 [50688/54000 (94%)] Loss: -249765.593750\n",
      "Train Epoch: 324 [52096/54000 (96%)] Loss: -220878.640625\n",
      "    epoch          : 324\n",
      "    loss           : -227231.29575358852\n",
      "    val_loss       : -229936.97463914825\n",
      "Train Epoch: 325 [0/54000 (0%)] Loss: -247995.125000\n",
      "Train Epoch: 325 [1408/54000 (3%)] Loss: -231019.218750\n",
      "Train Epoch: 325 [2816/54000 (5%)] Loss: -230634.625000\n",
      "Train Epoch: 325 [4224/54000 (8%)] Loss: -215148.515625\n",
      "Train Epoch: 325 [5632/54000 (10%)] Loss: -219805.312500\n",
      "Train Epoch: 325 [7040/54000 (13%)] Loss: -212716.671875\n",
      "Train Epoch: 325 [8448/54000 (16%)] Loss: -219425.703125\n",
      "Train Epoch: 325 [9856/54000 (18%)] Loss: -227770.234375\n",
      "Train Epoch: 325 [11264/54000 (21%)] Loss: -229593.406250\n",
      "Train Epoch: 325 [12672/54000 (23%)] Loss: -231138.890625\n",
      "Train Epoch: 325 [14080/54000 (26%)] Loss: -221784.296875\n",
      "Train Epoch: 325 [15488/54000 (29%)] Loss: -247256.265625\n",
      "Train Epoch: 325 [16896/54000 (31%)] Loss: -229142.750000\n",
      "Train Epoch: 325 [18304/54000 (34%)] Loss: -217136.812500\n",
      "Train Epoch: 325 [19712/54000 (37%)] Loss: -218730.515625\n",
      "Train Epoch: 325 [21120/54000 (39%)] Loss: -217181.484375\n",
      "Train Epoch: 325 [22528/54000 (42%)] Loss: -218402.125000\n",
      "Train Epoch: 325 [23936/54000 (44%)] Loss: -221327.281250\n",
      "Train Epoch: 325 [25344/54000 (47%)] Loss: -231553.890625\n",
      "Train Epoch: 325 [26752/54000 (50%)] Loss: -230512.375000\n",
      "Train Epoch: 325 [28160/54000 (52%)] Loss: -224280.406250\n",
      "Train Epoch: 325 [29568/54000 (55%)] Loss: -225226.781250\n",
      "Train Epoch: 325 [30976/54000 (57%)] Loss: -216815.609375\n",
      "Train Epoch: 325 [32384/54000 (60%)] Loss: -221927.609375\n",
      "Train Epoch: 325 [33792/54000 (63%)] Loss: -247337.703125\n",
      "Train Epoch: 325 [35200/54000 (65%)] Loss: -225963.984375\n",
      "Train Epoch: 325 [36608/54000 (68%)] Loss: -222191.031250\n",
      "Train Epoch: 325 [38016/54000 (70%)] Loss: -225157.093750\n",
      "Train Epoch: 325 [39424/54000 (73%)] Loss: -220735.125000\n",
      "Train Epoch: 325 [40832/54000 (76%)] Loss: -219145.265625\n",
      "Train Epoch: 325 [42240/54000 (78%)] Loss: -218616.765625\n",
      "Train Epoch: 325 [43648/54000 (81%)] Loss: -222968.890625\n",
      "Train Epoch: 325 [45056/54000 (83%)] Loss: -240480.515625\n",
      "Train Epoch: 325 [46464/54000 (86%)] Loss: -222440.218750\n",
      "Train Epoch: 325 [47872/54000 (89%)] Loss: -230530.765625\n",
      "Train Epoch: 325 [49280/54000 (91%)] Loss: -231428.343750\n",
      "Train Epoch: 325 [50688/54000 (94%)] Loss: -227718.906250\n",
      "Train Epoch: 325 [52096/54000 (96%)] Loss: -229584.156250\n",
      "    epoch          : 325\n",
      "    loss           : -227001.16092254786\n",
      "    val_loss       : -230166.48207650534\n",
      "Train Epoch: 326 [0/54000 (0%)] Loss: -248570.953125\n",
      "Train Epoch: 326 [1408/54000 (3%)] Loss: -231700.578125\n",
      "Train Epoch: 326 [2816/54000 (5%)] Loss: -229576.734375\n",
      "Train Epoch: 326 [4224/54000 (8%)] Loss: -221390.937500\n",
      "Train Epoch: 326 [5632/54000 (10%)] Loss: -230278.765625\n",
      "Train Epoch: 326 [7040/54000 (13%)] Loss: -217440.843750\n",
      "Train Epoch: 326 [8448/54000 (16%)] Loss: -217284.875000\n",
      "Train Epoch: 326 [9856/54000 (18%)] Loss: -231005.203125\n",
      "Train Epoch: 326 [11264/54000 (21%)] Loss: -220473.000000\n",
      "Train Epoch: 326 [12672/54000 (23%)] Loss: -215433.234375\n",
      "Train Epoch: 326 [14080/54000 (26%)] Loss: -222136.968750\n",
      "Train Epoch: 326 [15488/54000 (29%)] Loss: -231631.234375\n",
      "Train Epoch: 326 [16896/54000 (31%)] Loss: -230779.281250\n",
      "Train Epoch: 326 [18304/54000 (34%)] Loss: -227395.843750\n",
      "Train Epoch: 326 [19712/54000 (37%)] Loss: -218967.671875\n",
      "Train Epoch: 326 [21120/54000 (39%)] Loss: -226346.109375\n",
      "Train Epoch: 326 [22528/54000 (42%)] Loss: -217554.718750\n",
      "Train Epoch: 326 [23936/54000 (44%)] Loss: -248179.453125\n",
      "Train Epoch: 326 [25344/54000 (47%)] Loss: -220868.640625\n",
      "Train Epoch: 326 [26752/54000 (50%)] Loss: -219728.828125\n",
      "Train Epoch: 326 [28160/54000 (52%)] Loss: -214450.578125\n",
      "Train Epoch: 326 [29568/54000 (55%)] Loss: -231675.781250\n",
      "Train Epoch: 326 [30976/54000 (57%)] Loss: -232262.312500\n",
      "Train Epoch: 326 [32384/54000 (60%)] Loss: -224424.796875\n",
      "Train Epoch: 326 [33792/54000 (63%)] Loss: -230528.375000\n",
      "Train Epoch: 326 [35200/54000 (65%)] Loss: -218402.671875\n",
      "Train Epoch: 326 [36608/54000 (68%)] Loss: -249075.875000\n",
      "Train Epoch: 326 [38016/54000 (70%)] Loss: -215697.953125\n",
      "Train Epoch: 326 [39424/54000 (73%)] Loss: -219749.906250\n",
      "Train Epoch: 326 [40832/54000 (76%)] Loss: -222958.531250\n",
      "Train Epoch: 326 [42240/54000 (78%)] Loss: -226704.437500\n",
      "Train Epoch: 326 [43648/54000 (81%)] Loss: -247574.828125\n",
      "Train Epoch: 326 [45056/54000 (83%)] Loss: -216090.796875\n",
      "Train Epoch: 326 [46464/54000 (86%)] Loss: -227607.625000\n",
      "Train Epoch: 326 [47872/54000 (89%)] Loss: -227397.968750\n",
      "Train Epoch: 326 [49280/54000 (91%)] Loss: -220543.312500\n",
      "Train Epoch: 326 [50688/54000 (94%)] Loss: -249181.453125\n",
      "Train Epoch: 326 [52096/54000 (96%)] Loss: -229474.812500\n",
      "    epoch          : 326\n",
      "    loss           : -227082.48751495214\n",
      "    val_loss       : -229767.86621689214\n",
      "Train Epoch: 327 [0/54000 (0%)] Loss: -248081.312500\n",
      "Train Epoch: 327 [1408/54000 (3%)] Loss: -229759.421875\n",
      "Train Epoch: 327 [2816/54000 (5%)] Loss: -229183.234375\n",
      "Train Epoch: 327 [4224/54000 (8%)] Loss: -230356.406250\n",
      "Train Epoch: 327 [5632/54000 (10%)] Loss: -223215.140625\n",
      "Train Epoch: 327 [7040/54000 (13%)] Loss: -231090.640625\n",
      "Train Epoch: 327 [8448/54000 (16%)] Loss: -227008.781250\n",
      "Train Epoch: 327 [9856/54000 (18%)] Loss: -248851.453125\n",
      "Train Epoch: 327 [11264/54000 (21%)] Loss: -216551.953125\n",
      "Train Epoch: 327 [12672/54000 (23%)] Loss: -232227.531250\n",
      "Train Epoch: 327 [14080/54000 (26%)] Loss: -249054.187500\n",
      "Train Epoch: 327 [15488/54000 (29%)] Loss: -220127.109375\n",
      "Train Epoch: 327 [16896/54000 (31%)] Loss: -228210.750000\n",
      "Train Epoch: 327 [18304/54000 (34%)] Loss: -229799.671875\n",
      "Train Epoch: 327 [19712/54000 (37%)] Loss: -229815.359375\n",
      "Train Epoch: 327 [21120/54000 (39%)] Loss: -230872.078125\n",
      "Train Epoch: 327 [22528/54000 (42%)] Loss: -230891.984375\n",
      "Train Epoch: 327 [23936/54000 (44%)] Loss: -224075.062500\n",
      "Train Epoch: 327 [25344/54000 (47%)] Loss: -225291.671875\n",
      "Train Epoch: 327 [26752/54000 (50%)] Loss: -220895.921875\n",
      "Train Epoch: 327 [28160/54000 (52%)] Loss: -219509.703125\n",
      "Train Epoch: 327 [29568/54000 (55%)] Loss: -223151.484375\n",
      "Train Epoch: 327 [30976/54000 (57%)] Loss: -230377.265625\n",
      "Train Epoch: 327 [32384/54000 (60%)] Loss: -230521.296875\n",
      "Train Epoch: 327 [33792/54000 (63%)] Loss: -249874.078125\n",
      "Train Epoch: 327 [35200/54000 (65%)] Loss: -219943.046875\n",
      "Train Epoch: 327 [36608/54000 (68%)] Loss: -223410.125000\n",
      "Train Epoch: 327 [38016/54000 (70%)] Loss: -222549.281250\n",
      "Train Epoch: 327 [39424/54000 (73%)] Loss: -217116.406250\n",
      "Train Epoch: 327 [40832/54000 (76%)] Loss: -222588.937500\n",
      "Train Epoch: 327 [42240/54000 (78%)] Loss: -224330.203125\n",
      "Train Epoch: 327 [43648/54000 (81%)] Loss: -231962.453125\n",
      "Train Epoch: 327 [45056/54000 (83%)] Loss: -250518.000000\n",
      "Train Epoch: 327 [46464/54000 (86%)] Loss: -227348.062500\n",
      "Train Epoch: 327 [47872/54000 (89%)] Loss: -222022.625000\n",
      "Train Epoch: 327 [49280/54000 (91%)] Loss: -224272.437500\n",
      "Train Epoch: 327 [50688/54000 (94%)] Loss: -230697.843750\n",
      "Train Epoch: 327 [52096/54000 (96%)] Loss: -230794.921875\n",
      "    epoch          : 327\n",
      "    loss           : -227011.03678229664\n",
      "    val_loss       : -229853.0835615949\n",
      "Train Epoch: 328 [0/54000 (0%)] Loss: -249546.843750\n",
      "Train Epoch: 328 [1408/54000 (3%)] Loss: -221878.312500\n",
      "Train Epoch: 328 [2816/54000 (5%)] Loss: -222166.234375\n",
      "Train Epoch: 328 [4224/54000 (8%)] Loss: -219034.687500\n",
      "Train Epoch: 328 [5632/54000 (10%)] Loss: -223279.375000\n",
      "Train Epoch: 328 [7040/54000 (13%)] Loss: -222575.687500\n",
      "Train Epoch: 328 [8448/54000 (16%)] Loss: -247747.718750\n",
      "Train Epoch: 328 [9856/54000 (18%)] Loss: -230414.203125\n",
      "Train Epoch: 328 [11264/54000 (21%)] Loss: -228977.671875\n",
      "Train Epoch: 328 [12672/54000 (23%)] Loss: -228417.687500\n",
      "Train Epoch: 328 [14080/54000 (26%)] Loss: -219218.250000\n",
      "Train Epoch: 328 [15488/54000 (29%)] Loss: -248903.750000\n",
      "Train Epoch: 328 [16896/54000 (31%)] Loss: -224459.812500\n",
      "Train Epoch: 328 [18304/54000 (34%)] Loss: -224296.218750\n",
      "Train Epoch: 328 [19712/54000 (37%)] Loss: -228985.000000\n",
      "Train Epoch: 328 [21120/54000 (39%)] Loss: -226414.531250\n",
      "Train Epoch: 328 [22528/54000 (42%)] Loss: -220391.734375\n",
      "Train Epoch: 328 [23936/54000 (44%)] Loss: -220957.515625\n",
      "Train Epoch: 328 [25344/54000 (47%)] Loss: -225288.750000\n",
      "Train Epoch: 328 [26752/54000 (50%)] Loss: -225268.156250\n",
      "Train Epoch: 328 [28160/54000 (52%)] Loss: -222549.796875\n",
      "Train Epoch: 328 [29568/54000 (55%)] Loss: -225365.156250\n",
      "Train Epoch: 328 [30976/54000 (57%)] Loss: -219613.593750\n",
      "Train Epoch: 328 [32384/54000 (60%)] Loss: -222473.968750\n",
      "Train Epoch: 328 [33792/54000 (63%)] Loss: -220390.218750\n",
      "Train Epoch: 328 [35200/54000 (65%)] Loss: -249952.281250\n",
      "Train Epoch: 328 [36608/54000 (68%)] Loss: -228540.375000\n",
      "Train Epoch: 328 [38016/54000 (70%)] Loss: -233392.218750\n",
      "Train Epoch: 328 [39424/54000 (73%)] Loss: -224243.531250\n",
      "Train Epoch: 328 [40832/54000 (76%)] Loss: -226914.625000\n",
      "Train Epoch: 328 [42240/54000 (78%)] Loss: -226082.000000\n",
      "Train Epoch: 328 [43648/54000 (81%)] Loss: -227946.671875\n",
      "Train Epoch: 328 [45056/54000 (83%)] Loss: -227300.781250\n",
      "Train Epoch: 328 [46464/54000 (86%)] Loss: -222238.421875\n",
      "Train Epoch: 328 [47872/54000 (89%)] Loss: -224447.093750\n",
      "Train Epoch: 328 [49280/54000 (91%)] Loss: -230786.718750\n",
      "Train Epoch: 328 [50688/54000 (94%)] Loss: -220970.812500\n",
      "Train Epoch: 328 [52096/54000 (96%)] Loss: -244231.593750\n",
      "    epoch          : 328\n",
      "    loss           : -227169.73706638755\n",
      "    val_loss       : -229886.80083484185\n",
      "Train Epoch: 329 [0/54000 (0%)] Loss: -230028.625000\n",
      "Train Epoch: 329 [1408/54000 (3%)] Loss: -250629.453125\n",
      "Train Epoch: 329 [2816/54000 (5%)] Loss: -217514.312500\n",
      "Train Epoch: 329 [4224/54000 (8%)] Loss: -217328.093750\n",
      "Train Epoch: 329 [5632/54000 (10%)] Loss: -218169.187500\n",
      "Train Epoch: 329 [7040/54000 (13%)] Loss: -220050.640625\n",
      "Train Epoch: 329 [8448/54000 (16%)] Loss: -221430.359375\n",
      "Train Epoch: 329 [9856/54000 (18%)] Loss: -227948.609375\n",
      "Train Epoch: 329 [11264/54000 (21%)] Loss: -225146.125000\n",
      "Train Epoch: 329 [12672/54000 (23%)] Loss: -230509.031250\n",
      "Train Epoch: 329 [14080/54000 (26%)] Loss: -225398.312500\n",
      "Train Epoch: 329 [15488/54000 (29%)] Loss: -232128.437500\n",
      "Train Epoch: 329 [16896/54000 (31%)] Loss: -248677.359375\n",
      "Train Epoch: 329 [18304/54000 (34%)] Loss: -215911.484375\n",
      "Train Epoch: 329 [19712/54000 (37%)] Loss: -224258.906250\n",
      "Train Epoch: 329 [21120/54000 (39%)] Loss: -249245.468750\n",
      "Train Epoch: 329 [22528/54000 (42%)] Loss: -226021.187500\n",
      "Train Epoch: 329 [23936/54000 (44%)] Loss: -221915.656250\n",
      "Train Epoch: 329 [25344/54000 (47%)] Loss: -249294.593750\n",
      "Train Epoch: 329 [26752/54000 (50%)] Loss: -221667.500000\n",
      "Train Epoch: 329 [28160/54000 (52%)] Loss: -224341.312500\n",
      "Train Epoch: 329 [29568/54000 (55%)] Loss: -222198.343750\n",
      "Train Epoch: 329 [30976/54000 (57%)] Loss: -249496.875000\n",
      "Train Epoch: 329 [32384/54000 (60%)] Loss: -220443.250000\n",
      "Train Epoch: 329 [33792/54000 (63%)] Loss: -228464.437500\n",
      "Train Epoch: 329 [35200/54000 (65%)] Loss: -222454.453125\n",
      "Train Epoch: 329 [36608/54000 (68%)] Loss: -225243.312500\n",
      "Train Epoch: 329 [38016/54000 (70%)] Loss: -222047.250000\n",
      "Train Epoch: 329 [39424/54000 (73%)] Loss: -228129.156250\n",
      "Train Epoch: 329 [40832/54000 (76%)] Loss: -226161.781250\n",
      "Train Epoch: 329 [42240/54000 (78%)] Loss: -249449.953125\n",
      "Train Epoch: 329 [43648/54000 (81%)] Loss: -231248.875000\n",
      "Train Epoch: 329 [45056/54000 (83%)] Loss: -230469.156250\n",
      "Train Epoch: 329 [46464/54000 (86%)] Loss: -216997.578125\n",
      "Train Epoch: 329 [47872/54000 (89%)] Loss: -224475.750000\n",
      "Train Epoch: 329 [49280/54000 (91%)] Loss: -222317.218750\n",
      "Train Epoch: 329 [50688/54000 (94%)] Loss: -247507.125000\n",
      "Train Epoch: 329 [52096/54000 (96%)] Loss: -230247.218750\n",
      "    epoch          : 329\n",
      "    loss           : -227087.20387260764\n",
      "    val_loss       : -229892.3804365949\n",
      "Train Epoch: 330 [0/54000 (0%)] Loss: -231844.625000\n",
      "Train Epoch: 330 [1408/54000 (3%)] Loss: -249001.921875\n",
      "Train Epoch: 330 [2816/54000 (5%)] Loss: -229424.062500\n",
      "Train Epoch: 330 [4224/54000 (8%)] Loss: -231527.750000\n",
      "Train Epoch: 330 [5632/54000 (10%)] Loss: -229117.796875\n",
      "Train Epoch: 330 [7040/54000 (13%)] Loss: -221262.390625\n",
      "Train Epoch: 330 [8448/54000 (16%)] Loss: -247962.171875\n",
      "Train Epoch: 330 [9856/54000 (18%)] Loss: -226225.328125\n",
      "Train Epoch: 330 [11264/54000 (21%)] Loss: -224193.843750\n",
      "Train Epoch: 330 [12672/54000 (23%)] Loss: -248266.281250\n",
      "Train Epoch: 330 [14080/54000 (26%)] Loss: -219545.281250\n",
      "Train Epoch: 330 [15488/54000 (29%)] Loss: -224858.093750\n",
      "Train Epoch: 330 [16896/54000 (31%)] Loss: -229681.578125\n",
      "Train Epoch: 330 [18304/54000 (34%)] Loss: -225462.562500\n",
      "Train Epoch: 330 [19712/54000 (37%)] Loss: -249024.468750\n",
      "Train Epoch: 330 [21120/54000 (39%)] Loss: -220795.968750\n",
      "Train Epoch: 330 [22528/54000 (42%)] Loss: -228106.406250\n",
      "Train Epoch: 330 [23936/54000 (44%)] Loss: -221686.593750\n",
      "Train Epoch: 330 [25344/54000 (47%)] Loss: -249679.843750\n",
      "Train Epoch: 330 [26752/54000 (50%)] Loss: -220069.078125\n",
      "Train Epoch: 330 [28160/54000 (52%)] Loss: -226818.687500\n",
      "Train Epoch: 330 [29568/54000 (55%)] Loss: -230277.343750\n",
      "Train Epoch: 330 [30976/54000 (57%)] Loss: -222376.687500\n",
      "Train Epoch: 330 [32384/54000 (60%)] Loss: -229850.937500\n",
      "Train Epoch: 330 [33792/54000 (63%)] Loss: -220468.890625\n",
      "Train Epoch: 330 [35200/54000 (65%)] Loss: -230502.828125\n",
      "Train Epoch: 330 [36608/54000 (68%)] Loss: -225674.718750\n",
      "Train Epoch: 330 [38016/54000 (70%)] Loss: -225489.906250\n",
      "Train Epoch: 330 [39424/54000 (73%)] Loss: -249531.187500\n",
      "Train Epoch: 330 [40832/54000 (76%)] Loss: -230019.203125\n",
      "Train Epoch: 330 [42240/54000 (78%)] Loss: -223142.375000\n",
      "Train Epoch: 330 [43648/54000 (81%)] Loss: -247843.250000\n",
      "Train Epoch: 330 [45056/54000 (83%)] Loss: -218285.265625\n",
      "Train Epoch: 330 [46464/54000 (86%)] Loss: -228304.281250\n",
      "Train Epoch: 330 [47872/54000 (89%)] Loss: -246657.781250\n",
      "Train Epoch: 330 [49280/54000 (91%)] Loss: -226350.937500\n",
      "Train Epoch: 330 [50688/54000 (94%)] Loss: -224954.375000\n",
      "Train Epoch: 330 [52096/54000 (96%)] Loss: -230890.687500\n",
      "    epoch          : 330\n",
      "    loss           : -227226.33347039475\n",
      "    val_loss       : -229706.9721143769\n",
      "Train Epoch: 331 [0/54000 (0%)] Loss: -229958.875000\n",
      "Train Epoch: 331 [1408/54000 (3%)] Loss: -222491.796875\n",
      "Train Epoch: 331 [2816/54000 (5%)] Loss: -224730.718750\n",
      "Train Epoch: 331 [4224/54000 (8%)] Loss: -221683.265625\n",
      "Train Epoch: 331 [5632/54000 (10%)] Loss: -226459.953125\n",
      "Train Epoch: 331 [7040/54000 (13%)] Loss: -221595.937500\n",
      "Train Epoch: 331 [8448/54000 (16%)] Loss: -225266.312500\n",
      "Train Epoch: 331 [9856/54000 (18%)] Loss: -218012.765625\n",
      "Train Epoch: 331 [11264/54000 (21%)] Loss: -215775.562500\n",
      "Train Epoch: 331 [12672/54000 (23%)] Loss: -219591.937500\n",
      "Train Epoch: 331 [14080/54000 (26%)] Loss: -219887.531250\n",
      "Train Epoch: 331 [15488/54000 (29%)] Loss: -230051.968750\n",
      "Train Epoch: 331 [16896/54000 (31%)] Loss: -221053.343750\n",
      "Train Epoch: 331 [18304/54000 (34%)] Loss: -244609.031250\n",
      "Train Epoch: 331 [19712/54000 (37%)] Loss: -225139.921875\n",
      "Train Epoch: 331 [21120/54000 (39%)] Loss: -221640.906250\n",
      "Train Epoch: 331 [22528/54000 (42%)] Loss: -231069.312500\n",
      "Train Epoch: 331 [23936/54000 (44%)] Loss: -230405.875000\n",
      "Train Epoch: 331 [25344/54000 (47%)] Loss: -217118.171875\n",
      "Train Epoch: 331 [26752/54000 (50%)] Loss: -227946.359375\n",
      "Train Epoch: 331 [28160/54000 (52%)] Loss: -249786.718750\n",
      "Train Epoch: 331 [29568/54000 (55%)] Loss: -220932.859375\n",
      "Train Epoch: 331 [30976/54000 (57%)] Loss: -230134.375000\n",
      "Train Epoch: 331 [32384/54000 (60%)] Loss: -220677.718750\n",
      "Train Epoch: 331 [33792/54000 (63%)] Loss: -217835.390625\n",
      "Train Epoch: 331 [35200/54000 (65%)] Loss: -225996.703125\n",
      "Train Epoch: 331 [36608/54000 (68%)] Loss: -214397.843750\n",
      "Train Epoch: 331 [38016/54000 (70%)] Loss: -217655.796875\n",
      "Train Epoch: 331 [39424/54000 (73%)] Loss: -220782.750000\n",
      "Train Epoch: 331 [40832/54000 (76%)] Loss: -230177.187500\n",
      "Train Epoch: 331 [42240/54000 (78%)] Loss: -231085.515625\n",
      "Train Epoch: 331 [43648/54000 (81%)] Loss: -248268.750000\n",
      "Train Epoch: 331 [45056/54000 (83%)] Loss: -225319.500000\n",
      "Train Epoch: 331 [46464/54000 (86%)] Loss: -217901.234375\n",
      "Train Epoch: 331 [47872/54000 (89%)] Loss: -220866.578125\n",
      "Train Epoch: 331 [49280/54000 (91%)] Loss: -218953.265625\n",
      "Train Epoch: 331 [50688/54000 (94%)] Loss: -248762.593750\n",
      "Train Epoch: 331 [52096/54000 (96%)] Loss: -219501.125000\n",
      "    epoch          : 331\n",
      "    loss           : -227220.94482655503\n",
      "    val_loss       : -229870.87139743712\n",
      "Train Epoch: 332 [0/54000 (0%)] Loss: -248860.937500\n",
      "Train Epoch: 332 [1408/54000 (3%)] Loss: -222206.984375\n",
      "Train Epoch: 332 [2816/54000 (5%)] Loss: -229242.140625\n",
      "Train Epoch: 332 [4224/54000 (8%)] Loss: -228444.468750\n",
      "Train Epoch: 332 [5632/54000 (10%)] Loss: -229054.218750\n",
      "Train Epoch: 332 [7040/54000 (13%)] Loss: -218325.500000\n",
      "Train Epoch: 332 [8448/54000 (16%)] Loss: -219202.406250\n",
      "Train Epoch: 332 [9856/54000 (18%)] Loss: -249641.531250\n",
      "Train Epoch: 332 [11264/54000 (21%)] Loss: -215342.781250\n",
      "Train Epoch: 332 [12672/54000 (23%)] Loss: -220427.359375\n",
      "Train Epoch: 332 [14080/54000 (26%)] Loss: -223696.281250\n",
      "Train Epoch: 332 [15488/54000 (29%)] Loss: -221882.671875\n",
      "Train Epoch: 332 [16896/54000 (31%)] Loss: -220268.562500\n",
      "Train Epoch: 332 [18304/54000 (34%)] Loss: -220045.468750\n",
      "Train Epoch: 332 [19712/54000 (37%)] Loss: -216852.468750\n",
      "Train Epoch: 332 [21120/54000 (39%)] Loss: -216993.656250\n",
      "Train Epoch: 332 [22528/54000 (42%)] Loss: -230988.828125\n",
      "Train Epoch: 332 [23936/54000 (44%)] Loss: -229103.500000\n",
      "Train Epoch: 332 [25344/54000 (47%)] Loss: -230781.984375\n",
      "Train Epoch: 332 [26752/54000 (50%)] Loss: -219454.046875\n",
      "Train Epoch: 332 [28160/54000 (52%)] Loss: -248674.625000\n",
      "Train Epoch: 332 [29568/54000 (55%)] Loss: -226268.250000\n",
      "Train Epoch: 332 [30976/54000 (57%)] Loss: -220649.500000\n",
      "Train Epoch: 332 [32384/54000 (60%)] Loss: -217642.656250\n",
      "Train Epoch: 332 [33792/54000 (63%)] Loss: -248423.796875\n",
      "Train Epoch: 332 [35200/54000 (65%)] Loss: -223956.312500\n",
      "Train Epoch: 332 [36608/54000 (68%)] Loss: -222322.500000\n",
      "Train Epoch: 332 [38016/54000 (70%)] Loss: -218218.718750\n",
      "Train Epoch: 332 [39424/54000 (73%)] Loss: -249951.734375\n",
      "Train Epoch: 332 [40832/54000 (76%)] Loss: -225306.937500\n",
      "Train Epoch: 332 [42240/54000 (78%)] Loss: -230730.125000\n",
      "Train Epoch: 332 [43648/54000 (81%)] Loss: -249498.406250\n",
      "Train Epoch: 332 [45056/54000 (83%)] Loss: -230361.296875\n",
      "Train Epoch: 332 [46464/54000 (86%)] Loss: -221940.015625\n",
      "Train Epoch: 332 [47872/54000 (89%)] Loss: -220590.812500\n",
      "Train Epoch: 332 [49280/54000 (91%)] Loss: -217472.765625\n",
      "Train Epoch: 332 [50688/54000 (94%)] Loss: -225989.453125\n",
      "Train Epoch: 332 [52096/54000 (96%)] Loss: -224141.562500\n",
      "    epoch          : 332\n",
      "    loss           : -227141.61247757176\n",
      "    val_loss       : -230315.70579268291\n",
      "Train Epoch: 333 [0/54000 (0%)] Loss: -215587.718750\n",
      "Train Epoch: 333 [1408/54000 (3%)] Loss: -213834.265625\n",
      "Train Epoch: 333 [2816/54000 (5%)] Loss: -232104.734375\n",
      "Train Epoch: 333 [4224/54000 (8%)] Loss: -231250.921875\n",
      "Train Epoch: 333 [5632/54000 (10%)] Loss: -231852.906250\n",
      "Train Epoch: 333 [7040/54000 (13%)] Loss: -231503.250000\n",
      "Train Epoch: 333 [8448/54000 (16%)] Loss: -249843.406250\n",
      "Train Epoch: 333 [9856/54000 (18%)] Loss: -228006.593750\n",
      "Train Epoch: 333 [11264/54000 (21%)] Loss: -224984.140625\n",
      "Train Epoch: 333 [12672/54000 (23%)] Loss: -217315.937500\n",
      "Train Epoch: 333 [14080/54000 (26%)] Loss: -216866.750000\n",
      "Train Epoch: 333 [15488/54000 (29%)] Loss: -229052.234375\n",
      "Train Epoch: 333 [16896/54000 (31%)] Loss: -225911.875000\n",
      "Train Epoch: 333 [18304/54000 (34%)] Loss: -217572.140625\n",
      "Train Epoch: 333 [19712/54000 (37%)] Loss: -220799.953125\n",
      "Train Epoch: 333 [21120/54000 (39%)] Loss: -248347.718750\n",
      "Train Epoch: 333 [22528/54000 (42%)] Loss: -249292.312500\n",
      "Train Epoch: 333 [23936/54000 (44%)] Loss: -225905.500000\n",
      "Train Epoch: 333 [25344/54000 (47%)] Loss: -222291.843750\n",
      "Train Epoch: 333 [26752/54000 (50%)] Loss: -221417.906250\n",
      "Train Epoch: 333 [28160/54000 (52%)] Loss: -225754.718750\n",
      "Train Epoch: 333 [29568/54000 (55%)] Loss: -224560.343750\n",
      "Train Epoch: 333 [30976/54000 (57%)] Loss: -222759.781250\n",
      "Train Epoch: 333 [32384/54000 (60%)] Loss: -229188.671875\n",
      "Train Epoch: 333 [33792/54000 (63%)] Loss: -229709.828125\n",
      "Train Epoch: 333 [35200/54000 (65%)] Loss: -230939.343750\n",
      "Train Epoch: 333 [36608/54000 (68%)] Loss: -227599.875000\n",
      "Train Epoch: 333 [38016/54000 (70%)] Loss: -246770.500000\n",
      "Train Epoch: 333 [39424/54000 (73%)] Loss: -223845.265625\n",
      "Train Epoch: 333 [40832/54000 (76%)] Loss: -217532.203125\n",
      "Train Epoch: 333 [42240/54000 (78%)] Loss: -223874.906250\n",
      "Train Epoch: 333 [43648/54000 (81%)] Loss: -226422.281250\n",
      "Train Epoch: 333 [45056/54000 (83%)] Loss: -232056.250000\n",
      "Train Epoch: 333 [46464/54000 (86%)] Loss: -229208.234375\n",
      "Train Epoch: 333 [47872/54000 (89%)] Loss: -231171.687500\n",
      "Train Epoch: 333 [49280/54000 (91%)] Loss: -217163.984375\n",
      "Train Epoch: 333 [50688/54000 (94%)] Loss: -225728.718750\n",
      "Train Epoch: 333 [52096/54000 (96%)] Loss: -229470.421875\n",
      "    epoch          : 333\n",
      "    loss           : -227253.03379186604\n",
      "    val_loss       : -229856.89968202173\n",
      "Train Epoch: 334 [0/54000 (0%)] Loss: -231630.625000\n",
      "Train Epoch: 334 [1408/54000 (3%)] Loss: -218137.906250\n",
      "Train Epoch: 334 [2816/54000 (5%)] Loss: -230613.015625\n",
      "Train Epoch: 334 [4224/54000 (8%)] Loss: -220469.359375\n",
      "Train Epoch: 334 [5632/54000 (10%)] Loss: -224013.968750\n",
      "Train Epoch: 334 [7040/54000 (13%)] Loss: -219949.296875\n",
      "Train Epoch: 334 [8448/54000 (16%)] Loss: -228172.250000\n",
      "Train Epoch: 334 [9856/54000 (18%)] Loss: -218163.406250\n",
      "Train Epoch: 334 [11264/54000 (21%)] Loss: -226823.125000\n",
      "Train Epoch: 334 [12672/54000 (23%)] Loss: -217905.484375\n",
      "Train Epoch: 334 [14080/54000 (26%)] Loss: -247902.281250\n",
      "Train Epoch: 334 [15488/54000 (29%)] Loss: -220839.031250\n",
      "Train Epoch: 334 [16896/54000 (31%)] Loss: -218035.328125\n",
      "Train Epoch: 334 [18304/54000 (34%)] Loss: -221353.687500\n",
      "Train Epoch: 334 [19712/54000 (37%)] Loss: -250476.625000\n",
      "Train Epoch: 334 [21120/54000 (39%)] Loss: -230019.109375\n",
      "Train Epoch: 334 [22528/54000 (42%)] Loss: -220372.359375\n",
      "Train Epoch: 334 [23936/54000 (44%)] Loss: -250196.718750\n",
      "Train Epoch: 334 [25344/54000 (47%)] Loss: -222224.546875\n",
      "Train Epoch: 334 [26752/54000 (50%)] Loss: -219404.687500\n",
      "Train Epoch: 334 [28160/54000 (52%)] Loss: -230526.562500\n",
      "Train Epoch: 334 [29568/54000 (55%)] Loss: -228620.031250\n",
      "Train Epoch: 334 [30976/54000 (57%)] Loss: -229322.875000\n",
      "Train Epoch: 334 [32384/54000 (60%)] Loss: -228442.375000\n",
      "Train Epoch: 334 [33792/54000 (63%)] Loss: -220289.031250\n",
      "Train Epoch: 334 [35200/54000 (65%)] Loss: -248338.140625\n",
      "Train Epoch: 334 [36608/54000 (68%)] Loss: -223852.468750\n",
      "Train Epoch: 334 [38016/54000 (70%)] Loss: -226829.562500\n",
      "Train Epoch: 334 [39424/54000 (73%)] Loss: -218734.593750\n",
      "Train Epoch: 334 [40832/54000 (76%)] Loss: -230774.734375\n",
      "Train Epoch: 334 [42240/54000 (78%)] Loss: -228986.687500\n",
      "Train Epoch: 334 [43648/54000 (81%)] Loss: -229720.968750\n",
      "Train Epoch: 334 [45056/54000 (83%)] Loss: -219429.812500\n",
      "Train Epoch: 334 [46464/54000 (86%)] Loss: -227827.656250\n",
      "Train Epoch: 334 [47872/54000 (89%)] Loss: -222417.187500\n",
      "Train Epoch: 334 [49280/54000 (91%)] Loss: -221552.593750\n",
      "Train Epoch: 334 [50688/54000 (94%)] Loss: -215943.421875\n",
      "Train Epoch: 334 [52096/54000 (96%)] Loss: -221576.750000\n",
      "    epoch          : 334\n",
      "    loss           : -227153.24386961723\n",
      "    val_loss       : -230205.38700457316\n",
      "Train Epoch: 335 [0/54000 (0%)] Loss: -248243.671875\n",
      "Train Epoch: 335 [1408/54000 (3%)] Loss: -215395.468750\n",
      "Train Epoch: 335 [2816/54000 (5%)] Loss: -226014.359375\n",
      "Train Epoch: 335 [4224/54000 (8%)] Loss: -219739.093750\n",
      "Train Epoch: 335 [5632/54000 (10%)] Loss: -221438.890625\n",
      "Train Epoch: 335 [7040/54000 (13%)] Loss: -222946.875000\n",
      "Train Epoch: 335 [8448/54000 (16%)] Loss: -223689.781250\n",
      "Train Epoch: 335 [9856/54000 (18%)] Loss: -215787.125000\n",
      "Train Epoch: 335 [11264/54000 (21%)] Loss: -228395.671875\n",
      "Train Epoch: 335 [12672/54000 (23%)] Loss: -224039.734375\n",
      "Train Epoch: 335 [14080/54000 (26%)] Loss: -218829.328125\n",
      "Train Epoch: 335 [15488/54000 (29%)] Loss: -222404.859375\n",
      "Train Epoch: 335 [16896/54000 (31%)] Loss: -231134.921875\n",
      "Train Epoch: 335 [18304/54000 (34%)] Loss: -232555.328125\n",
      "Train Epoch: 335 [19712/54000 (37%)] Loss: -220119.343750\n",
      "Train Epoch: 335 [21120/54000 (39%)] Loss: -247601.546875\n",
      "Train Epoch: 335 [22528/54000 (42%)] Loss: -216729.531250\n",
      "Train Epoch: 335 [23936/54000 (44%)] Loss: -221027.000000\n",
      "Train Epoch: 335 [25344/54000 (47%)] Loss: -225582.515625\n",
      "Train Epoch: 335 [26752/54000 (50%)] Loss: -226805.531250\n",
      "Train Epoch: 335 [28160/54000 (52%)] Loss: -228415.515625\n",
      "Train Epoch: 335 [29568/54000 (55%)] Loss: -226650.937500\n",
      "Train Epoch: 335 [30976/54000 (57%)] Loss: -217746.078125\n",
      "Train Epoch: 335 [32384/54000 (60%)] Loss: -223514.718750\n",
      "Train Epoch: 335 [33792/54000 (63%)] Loss: -224035.312500\n",
      "Train Epoch: 335 [35200/54000 (65%)] Loss: -229914.078125\n",
      "Train Epoch: 335 [36608/54000 (68%)] Loss: -220141.078125\n",
      "Train Epoch: 335 [38016/54000 (70%)] Loss: -248438.656250\n",
      "Train Epoch: 335 [39424/54000 (73%)] Loss: -222046.609375\n",
      "Train Epoch: 335 [40832/54000 (76%)] Loss: -221226.500000\n",
      "Train Epoch: 335 [42240/54000 (78%)] Loss: -225227.421875\n",
      "Train Epoch: 335 [43648/54000 (81%)] Loss: -217823.062500\n",
      "Train Epoch: 335 [45056/54000 (83%)] Loss: -248005.796875\n",
      "Train Epoch: 335 [46464/54000 (86%)] Loss: -231109.531250\n",
      "Train Epoch: 335 [47872/54000 (89%)] Loss: -221916.562500\n",
      "Train Epoch: 335 [49280/54000 (91%)] Loss: -221603.796875\n",
      "Train Epoch: 335 [50688/54000 (94%)] Loss: -218523.781250\n",
      "Train Epoch: 335 [52096/54000 (96%)] Loss: -221224.687500\n",
      "    epoch          : 335\n",
      "    loss           : -227265.56152811006\n",
      "    val_loss       : -229445.68860756478\n",
      "Train Epoch: 336 [0/54000 (0%)] Loss: -223300.671875\n",
      "Train Epoch: 336 [1408/54000 (3%)] Loss: -231255.734375\n",
      "Train Epoch: 336 [2816/54000 (5%)] Loss: -220347.125000\n",
      "Train Epoch: 336 [4224/54000 (8%)] Loss: -228189.515625\n",
      "Train Epoch: 336 [5632/54000 (10%)] Loss: -225286.796875\n",
      "Train Epoch: 336 [7040/54000 (13%)] Loss: -228790.593750\n",
      "Train Epoch: 336 [8448/54000 (16%)] Loss: -222070.984375\n",
      "Train Epoch: 336 [9856/54000 (18%)] Loss: -229427.328125\n",
      "Train Epoch: 336 [11264/54000 (21%)] Loss: -221554.296875\n",
      "Train Epoch: 336 [12672/54000 (23%)] Loss: -219938.156250\n",
      "Train Epoch: 336 [14080/54000 (26%)] Loss: -229215.140625\n",
      "Train Epoch: 336 [15488/54000 (29%)] Loss: -225300.234375\n",
      "Train Epoch: 336 [16896/54000 (31%)] Loss: -222280.312500\n",
      "Train Epoch: 336 [18304/54000 (34%)] Loss: -232109.921875\n",
      "Train Epoch: 336 [19712/54000 (37%)] Loss: -248676.156250\n",
      "Train Epoch: 336 [21120/54000 (39%)] Loss: -219467.593750\n",
      "Train Epoch: 336 [22528/54000 (42%)] Loss: -230511.046875\n",
      "Train Epoch: 336 [23936/54000 (44%)] Loss: -224093.281250\n",
      "Train Epoch: 336 [25344/54000 (47%)] Loss: -249814.203125\n",
      "Train Epoch: 336 [26752/54000 (50%)] Loss: -222014.921875\n",
      "Train Epoch: 336 [28160/54000 (52%)] Loss: -231946.968750\n",
      "Train Epoch: 336 [29568/54000 (55%)] Loss: -231437.171875\n",
      "Train Epoch: 336 [30976/54000 (57%)] Loss: -221964.500000\n",
      "Train Epoch: 336 [32384/54000 (60%)] Loss: -229491.468750\n",
      "Train Epoch: 336 [33792/54000 (63%)] Loss: -232186.453125\n",
      "Train Epoch: 336 [35200/54000 (65%)] Loss: -220091.156250\n",
      "Train Epoch: 336 [36608/54000 (68%)] Loss: -220828.937500\n",
      "Train Epoch: 336 [38016/54000 (70%)] Loss: -247993.921875\n",
      "Train Epoch: 336 [39424/54000 (73%)] Loss: -222842.593750\n",
      "Train Epoch: 336 [40832/54000 (76%)] Loss: -227164.859375\n",
      "Train Epoch: 336 [42240/54000 (78%)] Loss: -230167.093750\n",
      "Train Epoch: 336 [43648/54000 (81%)] Loss: -223662.234375\n",
      "Train Epoch: 336 [45056/54000 (83%)] Loss: -248632.953125\n",
      "Train Epoch: 336 [46464/54000 (86%)] Loss: -219437.968750\n",
      "Train Epoch: 336 [47872/54000 (89%)] Loss: -225160.843750\n",
      "Train Epoch: 336 [49280/54000 (91%)] Loss: -218079.125000\n",
      "Train Epoch: 336 [50688/54000 (94%)] Loss: -215152.421875\n",
      "Train Epoch: 336 [52096/54000 (96%)] Loss: -245445.203125\n",
      "    epoch          : 336\n",
      "    loss           : -227261.08186303827\n",
      "    val_loss       : -229760.14569836127\n",
      "Train Epoch: 337 [0/54000 (0%)] Loss: -220406.109375\n",
      "Train Epoch: 337 [1408/54000 (3%)] Loss: -219427.171875\n",
      "Train Epoch: 337 [2816/54000 (5%)] Loss: -221587.875000\n",
      "Train Epoch: 337 [4224/54000 (8%)] Loss: -219711.062500\n",
      "Train Epoch: 337 [5632/54000 (10%)] Loss: -229127.484375\n",
      "Train Epoch: 337 [7040/54000 (13%)] Loss: -228857.156250\n",
      "Train Epoch: 337 [8448/54000 (16%)] Loss: -226192.187500\n",
      "Train Epoch: 337 [9856/54000 (18%)] Loss: -225006.968750\n",
      "Train Epoch: 337 [11264/54000 (21%)] Loss: -226928.937500\n",
      "Train Epoch: 337 [12672/54000 (23%)] Loss: -224653.625000\n",
      "Train Epoch: 337 [14080/54000 (26%)] Loss: -230228.500000\n",
      "Train Epoch: 337 [15488/54000 (29%)] Loss: -222043.937500\n",
      "Train Epoch: 337 [16896/54000 (31%)] Loss: -219865.000000\n",
      "Train Epoch: 337 [18304/54000 (34%)] Loss: -232222.640625\n",
      "Train Epoch: 337 [19712/54000 (37%)] Loss: -247581.625000\n",
      "Train Epoch: 337 [21120/54000 (39%)] Loss: -215078.093750\n",
      "Train Epoch: 337 [22528/54000 (42%)] Loss: -221214.890625\n",
      "Train Epoch: 337 [23936/54000 (44%)] Loss: -219706.203125\n",
      "Train Epoch: 337 [25344/54000 (47%)] Loss: -249175.125000\n",
      "Train Epoch: 337 [26752/54000 (50%)] Loss: -228456.437500\n",
      "Train Epoch: 337 [28160/54000 (52%)] Loss: -225683.000000\n",
      "Train Epoch: 337 [29568/54000 (55%)] Loss: -231576.468750\n",
      "Train Epoch: 337 [30976/54000 (57%)] Loss: -232815.515625\n",
      "Train Epoch: 337 [32384/54000 (60%)] Loss: -229871.046875\n",
      "Train Epoch: 337 [33792/54000 (63%)] Loss: -214148.421875\n",
      "Train Epoch: 337 [35200/54000 (65%)] Loss: -222290.234375\n",
      "Train Epoch: 337 [36608/54000 (68%)] Loss: -232154.234375\n",
      "Train Epoch: 337 [38016/54000 (70%)] Loss: -226065.296875\n",
      "Train Epoch: 337 [39424/54000 (73%)] Loss: -222038.656250\n",
      "Train Epoch: 337 [40832/54000 (76%)] Loss: -221829.156250\n",
      "Train Epoch: 337 [42240/54000 (78%)] Loss: -225476.046875\n",
      "Train Epoch: 337 [43648/54000 (81%)] Loss: -227015.281250\n",
      "Train Epoch: 337 [45056/54000 (83%)] Loss: -225957.250000\n",
      "Train Epoch: 337 [46464/54000 (86%)] Loss: -221578.046875\n",
      "Train Epoch: 337 [47872/54000 (89%)] Loss: -224739.984375\n",
      "Train Epoch: 337 [49280/54000 (91%)] Loss: -225552.078125\n",
      "Train Epoch: 337 [50688/54000 (94%)] Loss: -231058.296875\n",
      "Train Epoch: 337 [52096/54000 (96%)] Loss: -229872.484375\n",
      "    epoch          : 337\n",
      "    loss           : -227372.06059360047\n",
      "    val_loss       : -229900.22190000952\n",
      "Train Epoch: 338 [0/54000 (0%)] Loss: -228918.671875\n",
      "Train Epoch: 338 [1408/54000 (3%)] Loss: -249485.312500\n",
      "Train Epoch: 338 [2816/54000 (5%)] Loss: -228674.796875\n",
      "Train Epoch: 338 [4224/54000 (8%)] Loss: -221538.437500\n",
      "Train Epoch: 338 [5632/54000 (10%)] Loss: -225448.156250\n",
      "Train Epoch: 338 [7040/54000 (13%)] Loss: -225564.437500\n",
      "Train Epoch: 338 [8448/54000 (16%)] Loss: -229257.718750\n",
      "Train Epoch: 338 [9856/54000 (18%)] Loss: -221290.156250\n",
      "Train Epoch: 338 [11264/54000 (21%)] Loss: -219991.921875\n",
      "Train Epoch: 338 [12672/54000 (23%)] Loss: -219848.250000\n",
      "Train Epoch: 338 [14080/54000 (26%)] Loss: -229712.718750\n",
      "Train Epoch: 338 [15488/54000 (29%)] Loss: -228438.218750\n",
      "Train Epoch: 338 [16896/54000 (31%)] Loss: -220757.796875\n",
      "Train Epoch: 338 [18304/54000 (34%)] Loss: -229402.984375\n",
      "Train Epoch: 338 [19712/54000 (37%)] Loss: -231265.468750\n",
      "Train Epoch: 338 [21120/54000 (39%)] Loss: -226450.375000\n",
      "Train Epoch: 338 [22528/54000 (42%)] Loss: -227098.781250\n",
      "Train Epoch: 338 [23936/54000 (44%)] Loss: -221682.546875\n",
      "Train Epoch: 338 [25344/54000 (47%)] Loss: -219925.171875\n",
      "Train Epoch: 338 [26752/54000 (50%)] Loss: -222698.203125\n",
      "Train Epoch: 338 [28160/54000 (52%)] Loss: -218950.359375\n",
      "Train Epoch: 338 [29568/54000 (55%)] Loss: -219952.703125\n",
      "Train Epoch: 338 [30976/54000 (57%)] Loss: -224794.484375\n",
      "Train Epoch: 338 [32384/54000 (60%)] Loss: -219109.062500\n",
      "Train Epoch: 338 [33792/54000 (63%)] Loss: -219615.500000\n",
      "Train Epoch: 338 [35200/54000 (65%)] Loss: -248763.546875\n",
      "Train Epoch: 338 [36608/54000 (68%)] Loss: -217553.718750\n",
      "Train Epoch: 338 [38016/54000 (70%)] Loss: -223251.078125\n",
      "Train Epoch: 338 [39424/54000 (73%)] Loss: -222719.312500\n",
      "Train Epoch: 338 [40832/54000 (76%)] Loss: -220835.234375\n",
      "Train Epoch: 338 [42240/54000 (78%)] Loss: -231300.968750\n",
      "Train Epoch: 338 [43648/54000 (81%)] Loss: -247618.015625\n",
      "Train Epoch: 338 [45056/54000 (83%)] Loss: -224667.468750\n",
      "Train Epoch: 338 [46464/54000 (86%)] Loss: -228005.750000\n",
      "Train Epoch: 338 [47872/54000 (89%)] Loss: -227133.062500\n",
      "Train Epoch: 338 [49280/54000 (91%)] Loss: -221070.656250\n",
      "Train Epoch: 338 [50688/54000 (94%)] Loss: -227315.281250\n",
      "Train Epoch: 338 [52096/54000 (96%)] Loss: -216695.468750\n",
      "    epoch          : 338\n",
      "    loss           : -227277.08272278708\n",
      "    val_loss       : -229485.18531464366\n",
      "Train Epoch: 339 [0/54000 (0%)] Loss: -230460.515625\n",
      "Train Epoch: 339 [1408/54000 (3%)] Loss: -231380.906250\n",
      "Train Epoch: 339 [2816/54000 (5%)] Loss: -229946.234375\n",
      "Train Epoch: 339 [4224/54000 (8%)] Loss: -229054.593750\n",
      "Train Epoch: 339 [5632/54000 (10%)] Loss: -217885.906250\n",
      "Train Epoch: 339 [7040/54000 (13%)] Loss: -219835.515625\n",
      "Train Epoch: 339 [8448/54000 (16%)] Loss: -222602.359375\n",
      "Train Epoch: 339 [9856/54000 (18%)] Loss: -228486.171875\n",
      "Train Epoch: 339 [11264/54000 (21%)] Loss: -225517.031250\n",
      "Train Epoch: 339 [12672/54000 (23%)] Loss: -225504.093750\n",
      "Train Epoch: 339 [14080/54000 (26%)] Loss: -231281.515625\n",
      "Train Epoch: 339 [15488/54000 (29%)] Loss: -249568.359375\n",
      "Train Epoch: 339 [16896/54000 (31%)] Loss: -221326.093750\n",
      "Train Epoch: 339 [18304/54000 (34%)] Loss: -224560.312500\n",
      "Train Epoch: 339 [19712/54000 (37%)] Loss: -215083.468750\n",
      "Train Epoch: 339 [21120/54000 (39%)] Loss: -222424.250000\n",
      "Train Epoch: 339 [22528/54000 (42%)] Loss: -220562.593750\n",
      "Train Epoch: 339 [23936/54000 (44%)] Loss: -221419.750000\n",
      "Train Epoch: 339 [25344/54000 (47%)] Loss: -229975.687500\n",
      "Train Epoch: 339 [26752/54000 (50%)] Loss: -221057.015625\n",
      "Train Epoch: 339 [28160/54000 (52%)] Loss: -221204.750000\n",
      "Train Epoch: 339 [29568/54000 (55%)] Loss: -221248.125000\n",
      "Train Epoch: 339 [30976/54000 (57%)] Loss: -220420.093750\n",
      "Train Epoch: 339 [32384/54000 (60%)] Loss: -227927.640625\n",
      "Train Epoch: 339 [33792/54000 (63%)] Loss: -247444.718750\n",
      "Train Epoch: 339 [35200/54000 (65%)] Loss: -229295.328125\n",
      "Train Epoch: 339 [36608/54000 (68%)] Loss: -231967.062500\n",
      "Train Epoch: 339 [38016/54000 (70%)] Loss: -218973.562500\n",
      "Train Epoch: 339 [39424/54000 (73%)] Loss: -231012.796875\n",
      "Train Epoch: 339 [40832/54000 (76%)] Loss: -218369.953125\n",
      "Train Epoch: 339 [42240/54000 (78%)] Loss: -218118.828125\n",
      "Train Epoch: 339 [43648/54000 (81%)] Loss: -219744.937500\n",
      "Train Epoch: 339 [45056/54000 (83%)] Loss: -249318.218750\n",
      "Train Epoch: 339 [46464/54000 (86%)] Loss: -225159.906250\n",
      "Train Epoch: 339 [47872/54000 (89%)] Loss: -221424.984375\n",
      "Train Epoch: 339 [49280/54000 (91%)] Loss: -226063.671875\n",
      "Train Epoch: 339 [50688/54000 (94%)] Loss: -247619.156250\n",
      "Train Epoch: 339 [52096/54000 (96%)] Loss: -249855.765625\n",
      "    epoch          : 339\n",
      "    loss           : -227357.00441088516\n",
      "    val_loss       : -229667.59719178735\n",
      "Train Epoch: 340 [0/54000 (0%)] Loss: -248013.062500\n",
      "Train Epoch: 340 [1408/54000 (3%)] Loss: -248267.859375\n",
      "Train Epoch: 340 [2816/54000 (5%)] Loss: -230806.609375\n",
      "Train Epoch: 340 [4224/54000 (8%)] Loss: -231068.562500\n",
      "Train Epoch: 340 [5632/54000 (10%)] Loss: -230769.781250\n",
      "Train Epoch: 340 [7040/54000 (13%)] Loss: -224692.312500\n",
      "Train Epoch: 340 [8448/54000 (16%)] Loss: -230372.984375\n",
      "Train Epoch: 340 [9856/54000 (18%)] Loss: -230789.015625\n",
      "Train Epoch: 340 [11264/54000 (21%)] Loss: -219140.437500\n",
      "Train Epoch: 340 [12672/54000 (23%)] Loss: -229532.812500\n",
      "Train Epoch: 340 [14080/54000 (26%)] Loss: -225958.765625\n",
      "Train Epoch: 340 [15488/54000 (29%)] Loss: -250033.640625\n",
      "Train Epoch: 340 [16896/54000 (31%)] Loss: -226073.687500\n",
      "Train Epoch: 340 [18304/54000 (34%)] Loss: -207932.312500\n",
      "Train Epoch: 340 [19712/54000 (37%)] Loss: -220351.968750\n",
      "Train Epoch: 340 [21120/54000 (39%)] Loss: -220203.593750\n",
      "Train Epoch: 340 [22528/54000 (42%)] Loss: -222570.875000\n",
      "Train Epoch: 340 [23936/54000 (44%)] Loss: -219641.875000\n",
      "Train Epoch: 340 [25344/54000 (47%)] Loss: -228397.781250\n",
      "Train Epoch: 340 [26752/54000 (50%)] Loss: -228548.812500\n",
      "Train Epoch: 340 [28160/54000 (52%)] Loss: -249711.828125\n",
      "Train Epoch: 340 [29568/54000 (55%)] Loss: -231215.406250\n",
      "Train Epoch: 340 [30976/54000 (57%)] Loss: -226372.828125\n",
      "Train Epoch: 340 [32384/54000 (60%)] Loss: -229585.937500\n",
      "Train Epoch: 340 [33792/54000 (63%)] Loss: -250223.031250\n",
      "Train Epoch: 340 [35200/54000 (65%)] Loss: -219193.031250\n",
      "Train Epoch: 340 [36608/54000 (68%)] Loss: -215950.500000\n",
      "Train Epoch: 340 [38016/54000 (70%)] Loss: -248538.171875\n",
      "Train Epoch: 340 [39424/54000 (73%)] Loss: -221002.062500\n",
      "Train Epoch: 340 [40832/54000 (76%)] Loss: -220180.375000\n",
      "Train Epoch: 340 [42240/54000 (78%)] Loss: -231634.937500\n",
      "Train Epoch: 340 [43648/54000 (81%)] Loss: -249268.781250\n",
      "Train Epoch: 340 [45056/54000 (83%)] Loss: -220415.890625\n",
      "Train Epoch: 340 [46464/54000 (86%)] Loss: -216827.109375\n",
      "Train Epoch: 340 [47872/54000 (89%)] Loss: -226656.453125\n",
      "Train Epoch: 340 [49280/54000 (91%)] Loss: -225291.843750\n",
      "Train Epoch: 340 [50688/54000 (94%)] Loss: -248394.296875\n",
      "Train Epoch: 340 [52096/54000 (96%)] Loss: -219295.890625\n",
      "    epoch          : 340\n",
      "    loss           : -227519.5180173445\n",
      "    val_loss       : -229941.6227312786\n",
      "Train Epoch: 341 [0/54000 (0%)] Loss: -219304.968750\n",
      "Train Epoch: 341 [1408/54000 (3%)] Loss: -224220.875000\n",
      "Train Epoch: 341 [2816/54000 (5%)] Loss: -247162.390625\n",
      "Train Epoch: 341 [4224/54000 (8%)] Loss: -231585.437500\n",
      "Train Epoch: 341 [5632/54000 (10%)] Loss: -231402.781250\n",
      "Train Epoch: 341 [7040/54000 (13%)] Loss: -220680.093750\n",
      "Train Epoch: 341 [8448/54000 (16%)] Loss: -219586.921875\n",
      "Train Epoch: 341 [9856/54000 (18%)] Loss: -221662.906250\n",
      "Train Epoch: 341 [11264/54000 (21%)] Loss: -226601.812500\n",
      "Train Epoch: 341 [12672/54000 (23%)] Loss: -219073.218750\n",
      "Train Epoch: 341 [14080/54000 (26%)] Loss: -230920.156250\n",
      "Train Epoch: 341 [15488/54000 (29%)] Loss: -230278.468750\n",
      "Train Epoch: 341 [16896/54000 (31%)] Loss: -223964.312500\n",
      "Train Epoch: 341 [18304/54000 (34%)] Loss: -222817.984375\n",
      "Train Epoch: 341 [19712/54000 (37%)] Loss: -222224.781250\n",
      "Train Epoch: 341 [21120/54000 (39%)] Loss: -223667.718750\n",
      "Train Epoch: 341 [22528/54000 (42%)] Loss: -225880.671875\n",
      "Train Epoch: 341 [23936/54000 (44%)] Loss: -229515.656250\n",
      "Train Epoch: 341 [25344/54000 (47%)] Loss: -227463.171875\n",
      "Train Epoch: 341 [26752/54000 (50%)] Loss: -220906.578125\n",
      "Train Epoch: 341 [28160/54000 (52%)] Loss: -223822.031250\n",
      "Train Epoch: 341 [29568/54000 (55%)] Loss: -220719.796875\n",
      "Train Epoch: 341 [30976/54000 (57%)] Loss: -247079.859375\n",
      "Train Epoch: 341 [32384/54000 (60%)] Loss: -229930.578125\n",
      "Train Epoch: 341 [33792/54000 (63%)] Loss: -219776.406250\n",
      "Train Epoch: 341 [35200/54000 (65%)] Loss: -223996.453125\n",
      "Train Epoch: 341 [36608/54000 (68%)] Loss: -218911.875000\n",
      "Train Epoch: 341 [38016/54000 (70%)] Loss: -224008.656250\n",
      "Train Epoch: 341 [39424/54000 (73%)] Loss: -224781.187500\n",
      "Train Epoch: 341 [40832/54000 (76%)] Loss: -229694.187500\n",
      "Train Epoch: 341 [42240/54000 (78%)] Loss: -221355.156250\n",
      "Train Epoch: 341 [43648/54000 (81%)] Loss: -225724.375000\n",
      "Train Epoch: 341 [45056/54000 (83%)] Loss: -225795.546875\n",
      "Train Epoch: 341 [46464/54000 (86%)] Loss: -229202.156250\n",
      "Train Epoch: 341 [47872/54000 (89%)] Loss: -223082.687500\n",
      "Train Epoch: 341 [49280/54000 (91%)] Loss: -219697.062500\n",
      "Train Epoch: 341 [50688/54000 (94%)] Loss: -248761.406250\n",
      "Train Epoch: 341 [52096/54000 (96%)] Loss: -222457.203125\n",
      "    epoch          : 341\n",
      "    loss           : -227357.59554425837\n",
      "    val_loss       : -230160.34785275342\n",
      "Train Epoch: 342 [0/54000 (0%)] Loss: -248933.578125\n",
      "Train Epoch: 342 [1408/54000 (3%)] Loss: -227331.906250\n",
      "Train Epoch: 342 [2816/54000 (5%)] Loss: -224912.656250\n",
      "Train Epoch: 342 [4224/54000 (8%)] Loss: -222433.468750\n",
      "Train Epoch: 342 [5632/54000 (10%)] Loss: -224346.375000\n",
      "Train Epoch: 342 [7040/54000 (13%)] Loss: -231555.484375\n",
      "Train Epoch: 342 [8448/54000 (16%)] Loss: -220289.578125\n",
      "Train Epoch: 342 [9856/54000 (18%)] Loss: -222793.937500\n",
      "Train Epoch: 342 [11264/54000 (21%)] Loss: -217772.593750\n",
      "Train Epoch: 342 [12672/54000 (23%)] Loss: -230480.812500\n",
      "Train Epoch: 342 [14080/54000 (26%)] Loss: -220447.750000\n",
      "Train Epoch: 342 [15488/54000 (29%)] Loss: -249488.750000\n",
      "Train Epoch: 342 [16896/54000 (31%)] Loss: -225750.093750\n",
      "Train Epoch: 342 [18304/54000 (34%)] Loss: -216334.062500\n",
      "Train Epoch: 342 [19712/54000 (37%)] Loss: -218352.812500\n",
      "Train Epoch: 342 [21120/54000 (39%)] Loss: -229157.875000\n",
      "Train Epoch: 342 [22528/54000 (42%)] Loss: -230905.437500\n",
      "Train Epoch: 342 [23936/54000 (44%)] Loss: -224422.781250\n",
      "Train Epoch: 342 [25344/54000 (47%)] Loss: -221545.000000\n",
      "Train Epoch: 342 [26752/54000 (50%)] Loss: -219796.718750\n",
      "Train Epoch: 342 [28160/54000 (52%)] Loss: -218948.484375\n",
      "Train Epoch: 342 [29568/54000 (55%)] Loss: -228996.875000\n",
      "Train Epoch: 342 [30976/54000 (57%)] Loss: -228720.406250\n",
      "Train Epoch: 342 [32384/54000 (60%)] Loss: -226453.062500\n",
      "Train Epoch: 342 [33792/54000 (63%)] Loss: -229681.468750\n",
      "Train Epoch: 342 [35200/54000 (65%)] Loss: -229553.171875\n",
      "Train Epoch: 342 [36608/54000 (68%)] Loss: -224390.687500\n",
      "Train Epoch: 342 [38016/54000 (70%)] Loss: -249602.156250\n",
      "Train Epoch: 342 [39424/54000 (73%)] Loss: -230109.125000\n",
      "Train Epoch: 342 [40832/54000 (76%)] Loss: -231890.828125\n",
      "Train Epoch: 342 [42240/54000 (78%)] Loss: -225249.765625\n",
      "Train Epoch: 342 [43648/54000 (81%)] Loss: -220923.562500\n",
      "Train Epoch: 342 [45056/54000 (83%)] Loss: -249212.109375\n",
      "Train Epoch: 342 [46464/54000 (86%)] Loss: -226401.843750\n",
      "Train Epoch: 342 [47872/54000 (89%)] Loss: -225479.312500\n",
      "Train Epoch: 342 [49280/54000 (91%)] Loss: -230340.781250\n",
      "Train Epoch: 342 [50688/54000 (94%)] Loss: -227777.171875\n",
      "Train Epoch: 342 [52096/54000 (96%)] Loss: -229664.796875\n",
      "    epoch          : 342\n",
      "    loss           : -227265.65389503588\n",
      "    val_loss       : -230505.9814572218\n",
      "Train Epoch: 343 [0/54000 (0%)] Loss: -221666.234375\n",
      "Train Epoch: 343 [1408/54000 (3%)] Loss: -249675.375000\n",
      "Train Epoch: 343 [2816/54000 (5%)] Loss: -223124.281250\n",
      "Train Epoch: 343 [4224/54000 (8%)] Loss: -221801.687500\n",
      "Train Epoch: 343 [5632/54000 (10%)] Loss: -230092.046875\n",
      "Train Epoch: 343 [7040/54000 (13%)] Loss: -223846.437500\n",
      "Train Epoch: 343 [8448/54000 (16%)] Loss: -231421.609375\n",
      "Train Epoch: 343 [9856/54000 (18%)] Loss: -220463.421875\n",
      "Train Epoch: 343 [11264/54000 (21%)] Loss: -221116.171875\n",
      "Train Epoch: 343 [12672/54000 (23%)] Loss: -228061.687500\n",
      "Train Epoch: 343 [14080/54000 (26%)] Loss: -229520.062500\n",
      "Train Epoch: 343 [15488/54000 (29%)] Loss: -227427.671875\n",
      "Train Epoch: 343 [16896/54000 (31%)] Loss: -230257.515625\n",
      "Train Epoch: 343 [18304/54000 (34%)] Loss: -219710.875000\n",
      "Train Epoch: 343 [19712/54000 (37%)] Loss: -220666.000000\n",
      "Train Epoch: 343 [21120/54000 (39%)] Loss: -231064.000000\n",
      "Train Epoch: 343 [22528/54000 (42%)] Loss: -231948.343750\n",
      "Train Epoch: 343 [23936/54000 (44%)] Loss: -231656.015625\n",
      "Train Epoch: 343 [25344/54000 (47%)] Loss: -216952.218750\n",
      "Train Epoch: 343 [26752/54000 (50%)] Loss: -220582.937500\n",
      "Train Epoch: 343 [28160/54000 (52%)] Loss: -225404.375000\n",
      "Train Epoch: 343 [29568/54000 (55%)] Loss: -230051.140625\n",
      "Train Epoch: 343 [30976/54000 (57%)] Loss: -221175.640625\n",
      "Train Epoch: 343 [32384/54000 (60%)] Loss: -216789.265625\n",
      "Train Epoch: 343 [33792/54000 (63%)] Loss: -219804.890625\n",
      "Train Epoch: 343 [35200/54000 (65%)] Loss: -219657.484375\n",
      "Train Epoch: 343 [36608/54000 (68%)] Loss: -220617.359375\n",
      "Train Epoch: 343 [38016/54000 (70%)] Loss: -249572.156250\n",
      "Train Epoch: 343 [39424/54000 (73%)] Loss: -229383.562500\n",
      "Train Epoch: 343 [40832/54000 (76%)] Loss: -229696.687500\n",
      "Train Epoch: 343 [42240/54000 (78%)] Loss: -250223.968750\n",
      "Train Epoch: 343 [43648/54000 (81%)] Loss: -215602.109375\n",
      "Train Epoch: 343 [45056/54000 (83%)] Loss: -220971.937500\n",
      "Train Epoch: 343 [46464/54000 (86%)] Loss: -223011.218750\n",
      "Train Epoch: 343 [47872/54000 (89%)] Loss: -218237.453125\n",
      "Train Epoch: 343 [49280/54000 (91%)] Loss: -218517.453125\n",
      "Train Epoch: 343 [50688/54000 (94%)] Loss: -221796.093750\n",
      "Train Epoch: 343 [52096/54000 (96%)] Loss: -221243.562500\n",
      "    epoch          : 343\n",
      "    loss           : -227434.0249700957\n",
      "    val_loss       : -229772.16119831364\n",
      "Train Epoch: 344 [0/54000 (0%)] Loss: -218247.015625\n",
      "Train Epoch: 344 [1408/54000 (3%)] Loss: -231333.937500\n",
      "Train Epoch: 344 [2816/54000 (5%)] Loss: -229285.218750\n",
      "Train Epoch: 344 [4224/54000 (8%)] Loss: -222216.484375\n",
      "Train Epoch: 344 [5632/54000 (10%)] Loss: -232317.593750\n",
      "Train Epoch: 344 [7040/54000 (13%)] Loss: -226096.156250\n",
      "Train Epoch: 344 [8448/54000 (16%)] Loss: -231121.765625\n",
      "Train Epoch: 344 [9856/54000 (18%)] Loss: -226774.203125\n",
      "Train Epoch: 344 [11264/54000 (21%)] Loss: -250003.843750\n",
      "Train Epoch: 344 [12672/54000 (23%)] Loss: -219365.968750\n",
      "Train Epoch: 344 [14080/54000 (26%)] Loss: -231143.468750\n",
      "Train Epoch: 344 [15488/54000 (29%)] Loss: -249623.437500\n",
      "Train Epoch: 344 [16896/54000 (31%)] Loss: -223607.343750\n",
      "Train Epoch: 344 [18304/54000 (34%)] Loss: -248836.062500\n",
      "Train Epoch: 344 [19712/54000 (37%)] Loss: -231230.015625\n",
      "Train Epoch: 344 [21120/54000 (39%)] Loss: -224829.500000\n",
      "Train Epoch: 344 [22528/54000 (42%)] Loss: -218363.562500\n",
      "Train Epoch: 344 [23936/54000 (44%)] Loss: -228951.875000\n",
      "Train Epoch: 344 [25344/54000 (47%)] Loss: -228573.062500\n",
      "Train Epoch: 344 [26752/54000 (50%)] Loss: -248894.812500\n",
      "Train Epoch: 344 [28160/54000 (52%)] Loss: -222302.375000\n",
      "Train Epoch: 344 [29568/54000 (55%)] Loss: -220351.171875\n",
      "Train Epoch: 344 [30976/54000 (57%)] Loss: -228373.718750\n",
      "Train Epoch: 344 [32384/54000 (60%)] Loss: -248937.171875\n",
      "Train Epoch: 344 [33792/54000 (63%)] Loss: -224509.437500\n",
      "Train Epoch: 344 [35200/54000 (65%)] Loss: -230205.343750\n",
      "Train Epoch: 344 [36608/54000 (68%)] Loss: -214972.031250\n",
      "Train Epoch: 344 [38016/54000 (70%)] Loss: -249453.437500\n",
      "Train Epoch: 344 [39424/54000 (73%)] Loss: -219670.000000\n",
      "Train Epoch: 344 [40832/54000 (76%)] Loss: -229274.078125\n",
      "Train Epoch: 344 [42240/54000 (78%)] Loss: -220899.343750\n",
      "Train Epoch: 344 [43648/54000 (81%)] Loss: -248947.687500\n",
      "Train Epoch: 344 [45056/54000 (83%)] Loss: -225500.203125\n",
      "Train Epoch: 344 [46464/54000 (86%)] Loss: -223803.765625\n",
      "Train Epoch: 344 [47872/54000 (89%)] Loss: -229351.625000\n",
      "Train Epoch: 344 [49280/54000 (91%)] Loss: -226304.062500\n",
      "Train Epoch: 344 [50688/54000 (94%)] Loss: -223421.562500\n",
      "Train Epoch: 344 [52096/54000 (96%)] Loss: -231525.328125\n",
      "    epoch          : 344\n",
      "    loss           : -227395.65658642346\n",
      "    val_loss       : -230405.6597572885\n",
      "Train Epoch: 345 [0/54000 (0%)] Loss: -223158.843750\n",
      "Train Epoch: 345 [1408/54000 (3%)] Loss: -220929.250000\n",
      "Train Epoch: 345 [2816/54000 (5%)] Loss: -221473.140625\n",
      "Train Epoch: 345 [4224/54000 (8%)] Loss: -229903.421875\n",
      "Train Epoch: 345 [5632/54000 (10%)] Loss: -223648.734375\n",
      "Train Epoch: 345 [7040/54000 (13%)] Loss: -224134.750000\n",
      "Train Epoch: 345 [8448/54000 (16%)] Loss: -249478.109375\n",
      "Train Epoch: 345 [9856/54000 (18%)] Loss: -227658.625000\n",
      "Train Epoch: 345 [11264/54000 (21%)] Loss: -222466.671875\n",
      "Train Epoch: 345 [12672/54000 (23%)] Loss: -217616.140625\n",
      "Train Epoch: 345 [14080/54000 (26%)] Loss: -226107.140625\n",
      "Train Epoch: 345 [15488/54000 (29%)] Loss: -224447.078125\n",
      "Train Epoch: 345 [16896/54000 (31%)] Loss: -229690.093750\n",
      "Train Epoch: 345 [18304/54000 (34%)] Loss: -230336.468750\n",
      "Train Epoch: 345 [19712/54000 (37%)] Loss: -249306.375000\n",
      "Train Epoch: 345 [21120/54000 (39%)] Loss: -231838.000000\n",
      "Train Epoch: 345 [22528/54000 (42%)] Loss: -227381.109375\n",
      "Train Epoch: 345 [23936/54000 (44%)] Loss: -222276.718750\n",
      "Train Epoch: 345 [25344/54000 (47%)] Loss: -221601.718750\n",
      "Train Epoch: 345 [26752/54000 (50%)] Loss: -249847.468750\n",
      "Train Epoch: 345 [28160/54000 (52%)] Loss: -226931.296875\n",
      "Train Epoch: 345 [29568/54000 (55%)] Loss: -219765.593750\n",
      "Train Epoch: 345 [30976/54000 (57%)] Loss: -227032.312500\n",
      "Train Epoch: 345 [32384/54000 (60%)] Loss: -226999.281250\n",
      "Train Epoch: 345 [33792/54000 (63%)] Loss: -227429.906250\n",
      "Train Epoch: 345 [35200/54000 (65%)] Loss: -227145.234375\n",
      "Train Epoch: 345 [36608/54000 (68%)] Loss: -221154.843750\n",
      "Train Epoch: 345 [38016/54000 (70%)] Loss: -221181.093750\n",
      "Train Epoch: 345 [39424/54000 (73%)] Loss: -222187.187500\n",
      "Train Epoch: 345 [40832/54000 (76%)] Loss: -248705.281250\n",
      "Train Epoch: 345 [42240/54000 (78%)] Loss: -216259.906250\n",
      "Train Epoch: 345 [43648/54000 (81%)] Loss: -226498.562500\n",
      "Train Epoch: 345 [45056/54000 (83%)] Loss: -248402.609375\n",
      "Train Epoch: 345 [46464/54000 (86%)] Loss: -220748.953125\n",
      "Train Epoch: 345 [47872/54000 (89%)] Loss: -223128.500000\n",
      "Train Epoch: 345 [49280/54000 (91%)] Loss: -222956.734375\n",
      "Train Epoch: 345 [50688/54000 (94%)] Loss: -227626.500000\n",
      "Train Epoch: 345 [52096/54000 (96%)] Loss: -226628.015625\n",
      "    epoch          : 345\n",
      "    loss           : -227451.65621261962\n",
      "    val_loss       : -230018.44899247334\n",
      "Train Epoch: 346 [0/54000 (0%)] Loss: -248359.359375\n",
      "Train Epoch: 346 [1408/54000 (3%)] Loss: -229468.765625\n",
      "Train Epoch: 346 [2816/54000 (5%)] Loss: -225017.171875\n",
      "Train Epoch: 346 [4224/54000 (8%)] Loss: -231198.062500\n",
      "Train Epoch: 346 [5632/54000 (10%)] Loss: -231337.031250\n",
      "Train Epoch: 346 [7040/54000 (13%)] Loss: -222130.250000\n",
      "Train Epoch: 346 [8448/54000 (16%)] Loss: -248802.968750\n",
      "Train Epoch: 346 [9856/54000 (18%)] Loss: -227884.375000\n",
      "Train Epoch: 346 [11264/54000 (21%)] Loss: -230242.093750\n",
      "Train Epoch: 346 [12672/54000 (23%)] Loss: -225756.031250\n",
      "Train Epoch: 346 [14080/54000 (26%)] Loss: -247494.328125\n",
      "Train Epoch: 346 [15488/54000 (29%)] Loss: -225762.750000\n",
      "Train Epoch: 346 [16896/54000 (31%)] Loss: -218284.890625\n",
      "Train Epoch: 346 [18304/54000 (34%)] Loss: -230770.031250\n",
      "Train Epoch: 346 [19712/54000 (37%)] Loss: -230791.718750\n",
      "Train Epoch: 346 [21120/54000 (39%)] Loss: -230832.250000\n",
      "Train Epoch: 346 [22528/54000 (42%)] Loss: -230640.406250\n",
      "Train Epoch: 346 [23936/54000 (44%)] Loss: -225201.843750\n",
      "Train Epoch: 346 [25344/54000 (47%)] Loss: -230880.171875\n",
      "Train Epoch: 346 [26752/54000 (50%)] Loss: -250298.718750\n",
      "Train Epoch: 346 [28160/54000 (52%)] Loss: -230217.984375\n",
      "Train Epoch: 346 [29568/54000 (55%)] Loss: -224957.609375\n",
      "Train Epoch: 346 [30976/54000 (57%)] Loss: -222977.156250\n",
      "Train Epoch: 346 [32384/54000 (60%)] Loss: -221551.421875\n",
      "Train Epoch: 346 [33792/54000 (63%)] Loss: -219344.500000\n",
      "Train Epoch: 346 [35200/54000 (65%)] Loss: -222206.140625\n",
      "Train Epoch: 346 [36608/54000 (68%)] Loss: -222458.515625\n",
      "Train Epoch: 346 [38016/54000 (70%)] Loss: -229250.609375\n",
      "Train Epoch: 346 [39424/54000 (73%)] Loss: -229506.781250\n",
      "Train Epoch: 346 [40832/54000 (76%)] Loss: -224271.406250\n",
      "Train Epoch: 346 [42240/54000 (78%)] Loss: -222018.437500\n",
      "Train Epoch: 346 [43648/54000 (81%)] Loss: -231861.796875\n",
      "Train Epoch: 346 [45056/54000 (83%)] Loss: -249163.734375\n",
      "Train Epoch: 346 [46464/54000 (86%)] Loss: -220061.281250\n",
      "Train Epoch: 346 [47872/54000 (89%)] Loss: -221321.531250\n",
      "Train Epoch: 346 [49280/54000 (91%)] Loss: -230426.343750\n",
      "Train Epoch: 346 [50688/54000 (94%)] Loss: -226496.375000\n",
      "Train Epoch: 346 [52096/54000 (96%)] Loss: -248590.765625\n",
      "    epoch          : 346\n",
      "    loss           : -227572.78809061006\n",
      "    val_loss       : -230010.33440120047\n",
      "Train Epoch: 347 [0/54000 (0%)] Loss: -228837.562500\n",
      "Train Epoch: 347 [1408/54000 (3%)] Loss: -230959.765625\n",
      "Train Epoch: 347 [2816/54000 (5%)] Loss: -230019.609375\n",
      "Train Epoch: 347 [4224/54000 (8%)] Loss: -231584.609375\n",
      "Train Epoch: 347 [5632/54000 (10%)] Loss: -231545.312500\n",
      "Train Epoch: 347 [7040/54000 (13%)] Loss: -222159.203125\n",
      "Train Epoch: 347 [8448/54000 (16%)] Loss: -222252.312500\n",
      "Train Epoch: 347 [9856/54000 (18%)] Loss: -248413.703125\n",
      "Train Epoch: 347 [11264/54000 (21%)] Loss: -219220.750000\n",
      "Train Epoch: 347 [12672/54000 (23%)] Loss: -217802.359375\n",
      "Train Epoch: 347 [14080/54000 (26%)] Loss: -220219.625000\n",
      "Train Epoch: 347 [15488/54000 (29%)] Loss: -229141.906250\n",
      "Train Epoch: 347 [16896/54000 (31%)] Loss: -222182.609375\n",
      "Train Epoch: 347 [18304/54000 (34%)] Loss: -221969.937500\n",
      "Train Epoch: 347 [19712/54000 (37%)] Loss: -226667.984375\n",
      "Train Epoch: 347 [21120/54000 (39%)] Loss: -230892.562500\n",
      "Train Epoch: 347 [22528/54000 (42%)] Loss: -226472.984375\n",
      "Train Epoch: 347 [23936/54000 (44%)] Loss: -225583.765625\n",
      "Train Epoch: 347 [25344/54000 (47%)] Loss: -231084.500000\n",
      "Train Epoch: 347 [26752/54000 (50%)] Loss: -229589.187500\n",
      "Train Epoch: 347 [28160/54000 (52%)] Loss: -223343.609375\n",
      "Train Epoch: 347 [29568/54000 (55%)] Loss: -226706.265625\n",
      "Train Epoch: 347 [30976/54000 (57%)] Loss: -231525.125000\n",
      "Train Epoch: 347 [32384/54000 (60%)] Loss: -221375.062500\n",
      "Train Epoch: 347 [33792/54000 (63%)] Loss: -246850.062500\n",
      "Train Epoch: 347 [35200/54000 (65%)] Loss: -219094.578125\n",
      "Train Epoch: 347 [36608/54000 (68%)] Loss: -228165.187500\n",
      "Train Epoch: 347 [38016/54000 (70%)] Loss: -229046.109375\n",
      "Train Epoch: 347 [39424/54000 (73%)] Loss: -229708.937500\n",
      "Train Epoch: 347 [40832/54000 (76%)] Loss: -225093.750000\n",
      "Train Epoch: 347 [42240/54000 (78%)] Loss: -219603.718750\n",
      "Train Epoch: 347 [43648/54000 (81%)] Loss: -226511.531250\n",
      "Train Epoch: 347 [45056/54000 (83%)] Loss: -228120.375000\n",
      "Train Epoch: 347 [46464/54000 (86%)] Loss: -227067.250000\n",
      "Train Epoch: 347 [47872/54000 (89%)] Loss: -220161.218750\n",
      "Train Epoch: 347 [49280/54000 (91%)] Loss: -221236.937500\n",
      "Train Epoch: 347 [50688/54000 (94%)] Loss: -249162.687500\n",
      "Train Epoch: 347 [52096/54000 (96%)] Loss: -232782.343750\n",
      "    epoch          : 347\n",
      "    loss           : -227578.4828797847\n",
      "    val_loss       : -230149.43267077935\n",
      "Train Epoch: 348 [0/54000 (0%)] Loss: -249401.171875\n",
      "Train Epoch: 348 [1408/54000 (3%)] Loss: -224754.187500\n",
      "Train Epoch: 348 [2816/54000 (5%)] Loss: -248272.781250\n",
      "Train Epoch: 348 [4224/54000 (8%)] Loss: -230313.250000\n",
      "Train Epoch: 348 [5632/54000 (10%)] Loss: -219204.593750\n",
      "Train Epoch: 348 [7040/54000 (13%)] Loss: -232080.421875\n",
      "Train Epoch: 348 [8448/54000 (16%)] Loss: -220985.203125\n",
      "Train Epoch: 348 [9856/54000 (18%)] Loss: -248803.921875\n",
      "Train Epoch: 348 [11264/54000 (21%)] Loss: -221210.046875\n",
      "Train Epoch: 348 [12672/54000 (23%)] Loss: -220961.812500\n",
      "Train Epoch: 348 [14080/54000 (26%)] Loss: -229884.906250\n",
      "Train Epoch: 348 [15488/54000 (29%)] Loss: -250501.156250\n",
      "Train Epoch: 348 [16896/54000 (31%)] Loss: -214734.812500\n",
      "Train Epoch: 348 [18304/54000 (34%)] Loss: -223590.312500\n",
      "Train Epoch: 348 [19712/54000 (37%)] Loss: -230571.390625\n",
      "Train Epoch: 348 [21120/54000 (39%)] Loss: -230180.296875\n",
      "Train Epoch: 348 [22528/54000 (42%)] Loss: -232430.906250\n",
      "Train Epoch: 348 [23936/54000 (44%)] Loss: -226172.515625\n",
      "Train Epoch: 348 [25344/54000 (47%)] Loss: -221540.250000\n",
      "Train Epoch: 348 [26752/54000 (50%)] Loss: -226212.187500\n",
      "Train Epoch: 348 [28160/54000 (52%)] Loss: -217503.578125\n",
      "Train Epoch: 348 [29568/54000 (55%)] Loss: -220698.281250\n",
      "Train Epoch: 348 [30976/54000 (57%)] Loss: -221711.843750\n",
      "Train Epoch: 348 [32384/54000 (60%)] Loss: -227251.468750\n",
      "Train Epoch: 348 [33792/54000 (63%)] Loss: -218512.171875\n",
      "Train Epoch: 348 [35200/54000 (65%)] Loss: -221967.078125\n",
      "Train Epoch: 348 [36608/54000 (68%)] Loss: -219462.484375\n",
      "Train Epoch: 348 [38016/54000 (70%)] Loss: -222494.531250\n",
      "Train Epoch: 348 [39424/54000 (73%)] Loss: -230856.703125\n",
      "Train Epoch: 348 [40832/54000 (76%)] Loss: -230809.812500\n",
      "Train Epoch: 348 [42240/54000 (78%)] Loss: -248993.578125\n",
      "Train Epoch: 348 [43648/54000 (81%)] Loss: -230407.593750\n",
      "Train Epoch: 348 [45056/54000 (83%)] Loss: -220736.031250\n",
      "Train Epoch: 348 [46464/54000 (86%)] Loss: -228030.578125\n",
      "Train Epoch: 348 [47872/54000 (89%)] Loss: -219704.562500\n",
      "Train Epoch: 348 [49280/54000 (91%)] Loss: -231101.281250\n",
      "Train Epoch: 348 [50688/54000 (94%)] Loss: -220304.687500\n",
      "Train Epoch: 348 [52096/54000 (96%)] Loss: -229937.859375\n",
      "    epoch          : 348\n",
      "    loss           : -227375.02620364833\n",
      "    val_loss       : -229873.17893125952\n",
      "Train Epoch: 349 [0/54000 (0%)] Loss: -227270.281250\n",
      "Train Epoch: 349 [1408/54000 (3%)] Loss: -226884.484375\n",
      "Train Epoch: 349 [2816/54000 (5%)] Loss: -230773.828125\n",
      "Train Epoch: 349 [4224/54000 (8%)] Loss: -231991.375000\n",
      "Train Epoch: 349 [5632/54000 (10%)] Loss: -218148.562500\n",
      "Train Epoch: 349 [7040/54000 (13%)] Loss: -222921.046875\n",
      "Train Epoch: 349 [8448/54000 (16%)] Loss: -221747.875000\n",
      "Train Epoch: 349 [9856/54000 (18%)] Loss: -226130.125000\n",
      "Train Epoch: 349 [11264/54000 (21%)] Loss: -227854.843750\n",
      "Train Epoch: 349 [12672/54000 (23%)] Loss: -218011.562500\n",
      "Train Epoch: 349 [14080/54000 (26%)] Loss: -227363.703125\n",
      "Train Epoch: 349 [15488/54000 (29%)] Loss: -223795.859375\n",
      "Train Epoch: 349 [16896/54000 (31%)] Loss: -222148.187500\n",
      "Train Epoch: 349 [18304/54000 (34%)] Loss: -249087.171875\n",
      "Train Epoch: 349 [19712/54000 (37%)] Loss: -217720.171875\n",
      "Train Epoch: 349 [21120/54000 (39%)] Loss: -219971.796875\n",
      "Train Epoch: 349 [22528/54000 (42%)] Loss: -218338.468750\n",
      "Train Epoch: 349 [23936/54000 (44%)] Loss: -250091.906250\n",
      "Train Epoch: 349 [25344/54000 (47%)] Loss: -230856.328125\n",
      "Train Epoch: 349 [26752/54000 (50%)] Loss: -215736.359375\n",
      "Train Epoch: 349 [28160/54000 (52%)] Loss: -220726.953125\n",
      "Train Epoch: 349 [29568/54000 (55%)] Loss: -223100.468750\n",
      "Train Epoch: 349 [30976/54000 (57%)] Loss: -230694.656250\n",
      "Train Epoch: 349 [32384/54000 (60%)] Loss: -229814.890625\n",
      "Train Epoch: 349 [33792/54000 (63%)] Loss: -230925.937500\n",
      "Train Epoch: 349 [35200/54000 (65%)] Loss: -224395.343750\n",
      "Train Epoch: 349 [36608/54000 (68%)] Loss: -248221.937500\n",
      "Train Epoch: 349 [38016/54000 (70%)] Loss: -216350.687500\n",
      "Train Epoch: 349 [39424/54000 (73%)] Loss: -221136.375000\n",
      "Train Epoch: 349 [40832/54000 (76%)] Loss: -228664.671875\n",
      "Train Epoch: 349 [42240/54000 (78%)] Loss: -220131.843750\n",
      "Train Epoch: 349 [43648/54000 (81%)] Loss: -248192.375000\n",
      "Train Epoch: 349 [45056/54000 (83%)] Loss: -218361.234375\n",
      "Train Epoch: 349 [46464/54000 (86%)] Loss: -220704.062500\n",
      "Train Epoch: 349 [47872/54000 (89%)] Loss: -226968.453125\n",
      "Train Epoch: 349 [49280/54000 (91%)] Loss: -219746.765625\n",
      "Train Epoch: 349 [50688/54000 (94%)] Loss: -225451.562500\n",
      "Train Epoch: 349 [52096/54000 (96%)] Loss: -217525.937500\n",
      "    epoch          : 349\n",
      "    loss           : -227315.08945125598\n",
      "    val_loss       : -229800.55110280108\n",
      "Train Epoch: 350 [0/54000 (0%)] Loss: -249366.953125\n",
      "Train Epoch: 350 [1408/54000 (3%)] Loss: -243435.953125\n",
      "Train Epoch: 350 [2816/54000 (5%)] Loss: -229827.765625\n",
      "Train Epoch: 350 [4224/54000 (8%)] Loss: -217118.390625\n",
      "Train Epoch: 350 [5632/54000 (10%)] Loss: -226037.812500\n",
      "Train Epoch: 350 [7040/54000 (13%)] Loss: -221214.671875\n",
      "Train Epoch: 350 [8448/54000 (16%)] Loss: -226442.343750\n",
      "Train Epoch: 350 [9856/54000 (18%)] Loss: -250025.484375\n",
      "Train Epoch: 350 [11264/54000 (21%)] Loss: -221384.671875\n",
      "Train Epoch: 350 [12672/54000 (23%)] Loss: -229742.281250\n",
      "Train Epoch: 350 [14080/54000 (26%)] Loss: -227801.687500\n",
      "Train Epoch: 350 [15488/54000 (29%)] Loss: -229476.234375\n",
      "Train Epoch: 350 [16896/54000 (31%)] Loss: -226671.125000\n",
      "Train Epoch: 350 [18304/54000 (34%)] Loss: -231119.281250\n",
      "Train Epoch: 350 [19712/54000 (37%)] Loss: -226539.718750\n",
      "Train Epoch: 350 [21120/54000 (39%)] Loss: -248552.578125\n",
      "Train Epoch: 350 [22528/54000 (42%)] Loss: -225593.937500\n",
      "Train Epoch: 350 [23936/54000 (44%)] Loss: -217368.515625\n",
      "Train Epoch: 350 [25344/54000 (47%)] Loss: -218331.531250\n",
      "Train Epoch: 350 [26752/54000 (50%)] Loss: -226125.656250\n",
      "Train Epoch: 350 [28160/54000 (52%)] Loss: -219100.625000\n",
      "Train Epoch: 350 [29568/54000 (55%)] Loss: -226075.625000\n",
      "Train Epoch: 350 [30976/54000 (57%)] Loss: -228833.328125\n",
      "Train Epoch: 350 [32384/54000 (60%)] Loss: -224250.250000\n",
      "Train Epoch: 350 [33792/54000 (63%)] Loss: -249148.031250\n",
      "Train Epoch: 350 [35200/54000 (65%)] Loss: -230561.265625\n",
      "Train Epoch: 350 [36608/54000 (68%)] Loss: -220395.687500\n",
      "Train Epoch: 350 [38016/54000 (70%)] Loss: -225330.812500\n",
      "Train Epoch: 350 [39424/54000 (73%)] Loss: -224044.187500\n",
      "Train Epoch: 350 [40832/54000 (76%)] Loss: -223640.546875\n",
      "Train Epoch: 350 [42240/54000 (78%)] Loss: -224349.546875\n",
      "Train Epoch: 350 [43648/54000 (81%)] Loss: -225503.187500\n",
      "Train Epoch: 350 [45056/54000 (83%)] Loss: -227928.171875\n",
      "Train Epoch: 350 [46464/54000 (86%)] Loss: -228780.687500\n",
      "Train Epoch: 350 [47872/54000 (89%)] Loss: -247858.328125\n",
      "Train Epoch: 350 [49280/54000 (91%)] Loss: -220377.375000\n",
      "Train Epoch: 350 [50688/54000 (94%)] Loss: -220255.968750\n",
      "Train Epoch: 350 [52096/54000 (96%)] Loss: -218458.390625\n",
      "    epoch          : 350\n",
      "    loss           : -227611.93021082535\n",
      "    val_loss       : -229785.35923804308\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0508_092801/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [0/54000 (0%)] Loss: -249804.734375\n",
      "Train Epoch: 351 [1408/54000 (3%)] Loss: -232415.687500\n",
      "Train Epoch: 351 [2816/54000 (5%)] Loss: -217950.140625\n",
      "Train Epoch: 351 [4224/54000 (8%)] Loss: -220948.171875\n",
      "Train Epoch: 351 [5632/54000 (10%)] Loss: -220709.671875\n",
      "Train Epoch: 351 [7040/54000 (13%)] Loss: -222753.687500\n",
      "Train Epoch: 351 [8448/54000 (16%)] Loss: -231910.218750\n",
      "Train Epoch: 351 [9856/54000 (18%)] Loss: -231364.296875\n",
      "Train Epoch: 351 [11264/54000 (21%)] Loss: -229894.500000\n",
      "Train Epoch: 351 [12672/54000 (23%)] Loss: -221134.218750\n",
      "Train Epoch: 351 [14080/54000 (26%)] Loss: -227009.187500\n",
      "Train Epoch: 351 [15488/54000 (29%)] Loss: -215600.703125\n",
      "Train Epoch: 351 [16896/54000 (31%)] Loss: -225666.875000\n",
      "Train Epoch: 351 [18304/54000 (34%)] Loss: -229082.343750\n",
      "Train Epoch: 351 [19712/54000 (37%)] Loss: -247184.640625\n",
      "Train Epoch: 351 [21120/54000 (39%)] Loss: -220815.171875\n",
      "Train Epoch: 351 [22528/54000 (42%)] Loss: -219124.468750\n",
      "Train Epoch: 351 [23936/54000 (44%)] Loss: -228719.218750\n",
      "Train Epoch: 351 [25344/54000 (47%)] Loss: -221240.343750\n",
      "Train Epoch: 351 [26752/54000 (50%)] Loss: -224022.343750\n",
      "Train Epoch: 351 [28160/54000 (52%)] Loss: -231615.406250\n",
      "Train Epoch: 351 [29568/54000 (55%)] Loss: -221361.968750\n",
      "Train Epoch: 351 [30976/54000 (57%)] Loss: -222521.671875\n",
      "Train Epoch: 351 [32384/54000 (60%)] Loss: -221274.984375\n",
      "Train Epoch: 351 [33792/54000 (63%)] Loss: -230398.734375\n",
      "Train Epoch: 351 [35200/54000 (65%)] Loss: -247995.156250\n",
      "Train Epoch: 351 [36608/54000 (68%)] Loss: -220451.093750\n",
      "Train Epoch: 351 [38016/54000 (70%)] Loss: -223335.640625\n",
      "Train Epoch: 351 [39424/54000 (73%)] Loss: -225707.312500\n",
      "Train Epoch: 351 [40832/54000 (76%)] Loss: -231374.640625\n",
      "Train Epoch: 351 [42240/54000 (78%)] Loss: -231536.593750\n",
      "Train Epoch: 351 [43648/54000 (81%)] Loss: -219866.796875\n",
      "Train Epoch: 351 [45056/54000 (83%)] Loss: -221643.296875\n",
      "Train Epoch: 351 [46464/54000 (86%)] Loss: -224582.609375\n",
      "Train Epoch: 351 [47872/54000 (89%)] Loss: -231935.593750\n",
      "Train Epoch: 351 [49280/54000 (91%)] Loss: -219426.625000\n",
      "Train Epoch: 351 [50688/54000 (94%)] Loss: -221837.609375\n",
      "Train Epoch: 351 [52096/54000 (96%)] Loss: -232628.281250\n",
      "    epoch          : 351\n",
      "    loss           : -227555.69665819377\n",
      "    val_loss       : -230227.03789538873\n",
      "Train Epoch: 352 [0/54000 (0%)] Loss: -249587.687500\n",
      "Train Epoch: 352 [1408/54000 (3%)] Loss: -229932.812500\n",
      "Train Epoch: 352 [2816/54000 (5%)] Loss: -222397.781250\n",
      "Train Epoch: 352 [4224/54000 (8%)] Loss: -224175.843750\n",
      "Train Epoch: 352 [5632/54000 (10%)] Loss: -226288.781250\n",
      "Train Epoch: 352 [7040/54000 (13%)] Loss: -220698.687500\n",
      "Train Epoch: 352 [8448/54000 (16%)] Loss: -249550.296875\n",
      "Train Epoch: 352 [9856/54000 (18%)] Loss: -219017.265625\n",
      "Train Epoch: 352 [11264/54000 (21%)] Loss: -229669.015625\n",
      "Train Epoch: 352 [12672/54000 (23%)] Loss: -232094.812500\n",
      "Train Epoch: 352 [14080/54000 (26%)] Loss: -228406.453125\n",
      "Train Epoch: 352 [15488/54000 (29%)] Loss: -225799.968750\n",
      "Train Epoch: 352 [16896/54000 (31%)] Loss: -227323.437500\n",
      "Train Epoch: 352 [18304/54000 (34%)] Loss: -223773.406250\n",
      "Train Epoch: 352 [19712/54000 (37%)] Loss: -249110.359375\n",
      "Train Epoch: 352 [21120/54000 (39%)] Loss: -247825.640625\n",
      "Train Epoch: 352 [22528/54000 (42%)] Loss: -219459.781250\n",
      "Train Epoch: 352 [23936/54000 (44%)] Loss: -221342.328125\n",
      "Train Epoch: 352 [25344/54000 (47%)] Loss: -221548.750000\n",
      "Train Epoch: 352 [26752/54000 (50%)] Loss: -221800.968750\n",
      "Train Epoch: 352 [28160/54000 (52%)] Loss: -219419.328125\n",
      "Train Epoch: 352 [29568/54000 (55%)] Loss: -231609.562500\n",
      "Train Epoch: 352 [30976/54000 (57%)] Loss: -231702.078125\n",
      "Train Epoch: 352 [32384/54000 (60%)] Loss: -231054.562500\n",
      "Train Epoch: 352 [33792/54000 (63%)] Loss: -221551.281250\n",
      "Train Epoch: 352 [35200/54000 (65%)] Loss: -222132.265625\n",
      "Train Epoch: 352 [36608/54000 (68%)] Loss: -220669.000000\n",
      "Train Epoch: 352 [38016/54000 (70%)] Loss: -224398.296875\n",
      "Train Epoch: 352 [39424/54000 (73%)] Loss: -223868.703125\n",
      "Train Epoch: 352 [40832/54000 (76%)] Loss: -230724.593750\n",
      "Train Epoch: 352 [42240/54000 (78%)] Loss: -232168.609375\n",
      "Train Epoch: 352 [43648/54000 (81%)] Loss: -248446.359375\n",
      "Train Epoch: 352 [45056/54000 (83%)] Loss: -224809.640625\n",
      "Train Epoch: 352 [46464/54000 (86%)] Loss: -222374.187500\n",
      "Train Epoch: 352 [47872/54000 (89%)] Loss: -221574.750000\n",
      "Train Epoch: 352 [49280/54000 (91%)] Loss: -227099.609375\n",
      "Train Epoch: 352 [50688/54000 (94%)] Loss: -226525.359375\n",
      "Train Epoch: 352 [52096/54000 (96%)] Loss: -248604.796875\n",
      "    epoch          : 352\n",
      "    loss           : -227665.11845843302\n",
      "    val_loss       : -229573.74455745047\n",
      "Train Epoch: 353 [0/54000 (0%)] Loss: -213743.906250\n",
      "Train Epoch: 353 [1408/54000 (3%)] Loss: -219573.937500\n",
      "Train Epoch: 353 [2816/54000 (5%)] Loss: -248670.390625\n",
      "Train Epoch: 353 [4224/54000 (8%)] Loss: -220957.734375\n",
      "Train Epoch: 353 [5632/54000 (10%)] Loss: -222199.750000\n",
      "Train Epoch: 353 [7040/54000 (13%)] Loss: -223599.203125\n",
      "Train Epoch: 353 [8448/54000 (16%)] Loss: -229884.578125\n",
      "Train Epoch: 353 [9856/54000 (18%)] Loss: -225755.953125\n",
      "Train Epoch: 353 [11264/54000 (21%)] Loss: -228937.906250\n",
      "Train Epoch: 353 [12672/54000 (23%)] Loss: -231465.156250\n",
      "Train Epoch: 353 [14080/54000 (26%)] Loss: -221089.250000\n",
      "Train Epoch: 353 [15488/54000 (29%)] Loss: -249999.703125\n",
      "Train Epoch: 353 [16896/54000 (31%)] Loss: -220441.125000\n",
      "Train Epoch: 353 [18304/54000 (34%)] Loss: -231961.281250\n",
      "Train Epoch: 353 [19712/54000 (37%)] Loss: -231533.625000\n",
      "Train Epoch: 353 [21120/54000 (39%)] Loss: -219172.203125\n",
      "Train Epoch: 353 [22528/54000 (42%)] Loss: -221918.078125\n",
      "Train Epoch: 353 [23936/54000 (44%)] Loss: -231177.093750\n",
      "Train Epoch: 353 [25344/54000 (47%)] Loss: -231252.156250\n",
      "Train Epoch: 353 [26752/54000 (50%)] Loss: -225299.796875\n",
      "Train Epoch: 353 [28160/54000 (52%)] Loss: -228984.656250\n",
      "Train Epoch: 353 [29568/54000 (55%)] Loss: -220269.640625\n",
      "Train Epoch: 353 [30976/54000 (57%)] Loss: -218495.968750\n",
      "Train Epoch: 353 [32384/54000 (60%)] Loss: -248486.343750\n",
      "Train Epoch: 353 [33792/54000 (63%)] Loss: -225630.046875\n",
      "Train Epoch: 353 [35200/54000 (65%)] Loss: -225495.046875\n",
      "Train Epoch: 353 [36608/54000 (68%)] Loss: -220840.515625\n",
      "Train Epoch: 353 [38016/54000 (70%)] Loss: -231851.031250\n",
      "Train Epoch: 353 [39424/54000 (73%)] Loss: -220000.906250\n",
      "Train Epoch: 353 [40832/54000 (76%)] Loss: -221635.265625\n",
      "Train Epoch: 353 [42240/54000 (78%)] Loss: -227323.062500\n",
      "Train Epoch: 353 [43648/54000 (81%)] Loss: -224619.375000\n",
      "Train Epoch: 353 [45056/54000 (83%)] Loss: -248989.562500\n",
      "Train Epoch: 353 [46464/54000 (86%)] Loss: -219741.359375\n",
      "Train Epoch: 353 [47872/54000 (89%)] Loss: -225122.359375\n",
      "Train Epoch: 353 [49280/54000 (91%)] Loss: -221917.531250\n",
      "Train Epoch: 353 [50688/54000 (94%)] Loss: -216459.656250\n",
      "Train Epoch: 353 [52096/54000 (96%)] Loss: -222072.109375\n",
      "    epoch          : 353\n",
      "    loss           : -227595.34842254786\n",
      "    val_loss       : -229957.47553234565\n",
      "Train Epoch: 354 [0/54000 (0%)] Loss: -228119.015625\n",
      "Train Epoch: 354 [1408/54000 (3%)] Loss: -249818.906250\n",
      "Train Epoch: 354 [2816/54000 (5%)] Loss: -229926.671875\n",
      "Train Epoch: 354 [4224/54000 (8%)] Loss: -229635.953125\n",
      "Train Epoch: 354 [5632/54000 (10%)] Loss: -220717.093750\n",
      "Train Epoch: 354 [7040/54000 (13%)] Loss: -219827.546875\n",
      "Train Epoch: 354 [8448/54000 (16%)] Loss: -219737.015625\n",
      "Train Epoch: 354 [9856/54000 (18%)] Loss: -223513.171875\n",
      "Train Epoch: 354 [11264/54000 (21%)] Loss: -221926.171875\n",
      "Train Epoch: 354 [12672/54000 (23%)] Loss: -227408.187500\n",
      "Train Epoch: 354 [14080/54000 (26%)] Loss: -249003.031250\n",
      "Train Epoch: 354 [15488/54000 (29%)] Loss: -223614.937500\n",
      "Train Epoch: 354 [16896/54000 (31%)] Loss: -221878.062500\n",
      "Train Epoch: 354 [18304/54000 (34%)] Loss: -225858.984375\n",
      "Train Epoch: 354 [19712/54000 (37%)] Loss: -248915.406250\n",
      "Train Epoch: 354 [21120/54000 (39%)] Loss: -220983.296875\n",
      "Train Epoch: 354 [22528/54000 (42%)] Loss: -220471.406250\n",
      "Train Epoch: 354 [23936/54000 (44%)] Loss: -231602.218750\n",
      "Train Epoch: 354 [25344/54000 (47%)] Loss: -249847.500000\n",
      "Train Epoch: 354 [26752/54000 (50%)] Loss: -231091.484375\n",
      "Train Epoch: 354 [28160/54000 (52%)] Loss: -219785.937500\n",
      "Train Epoch: 354 [29568/54000 (55%)] Loss: -222213.500000\n",
      "Train Epoch: 354 [30976/54000 (57%)] Loss: -215921.750000\n",
      "Train Epoch: 354 [32384/54000 (60%)] Loss: -218657.796875\n",
      "Train Epoch: 354 [33792/54000 (63%)] Loss: -249405.468750\n",
      "Train Epoch: 354 [35200/54000 (65%)] Loss: -224268.687500\n",
      "Train Epoch: 354 [36608/54000 (68%)] Loss: -222059.875000\n",
      "Train Epoch: 354 [38016/54000 (70%)] Loss: -248606.109375\n",
      "Train Epoch: 354 [39424/54000 (73%)] Loss: -219715.703125\n",
      "Train Epoch: 354 [40832/54000 (76%)] Loss: -231476.937500\n",
      "Train Epoch: 354 [42240/54000 (78%)] Loss: -231787.546875\n",
      "Train Epoch: 354 [43648/54000 (81%)] Loss: -249658.703125\n",
      "Train Epoch: 354 [45056/54000 (83%)] Loss: -226886.656250\n",
      "Train Epoch: 354 [46464/54000 (86%)] Loss: -221451.906250\n",
      "Train Epoch: 354 [47872/54000 (89%)] Loss: -224012.156250\n",
      "Train Epoch: 354 [49280/54000 (91%)] Loss: -226551.984375\n",
      "Train Epoch: 354 [50688/54000 (94%)] Loss: -249526.609375\n",
      "Train Epoch: 354 [52096/54000 (96%)] Loss: -231257.296875\n",
      "    epoch          : 354\n",
      "    loss           : -227670.07808761962\n",
      "    val_loss       : -229972.13365806022\n",
      "Train Epoch: 355 [0/54000 (0%)] Loss: -248043.843750\n",
      "Train Epoch: 355 [1408/54000 (3%)] Loss: -230351.656250\n",
      "Train Epoch: 355 [2816/54000 (5%)] Loss: -224665.203125\n",
      "Train Epoch: 355 [4224/54000 (8%)] Loss: -226977.125000\n",
      "Train Epoch: 355 [5632/54000 (10%)] Loss: -219058.812500\n",
      "Train Epoch: 355 [7040/54000 (13%)] Loss: -226701.453125\n",
      "Train Epoch: 355 [8448/54000 (16%)] Loss: -248379.281250\n",
      "Train Epoch: 355 [9856/54000 (18%)] Loss: -217946.796875\n",
      "Train Epoch: 355 [11264/54000 (21%)] Loss: -218755.265625\n",
      "Train Epoch: 355 [12672/54000 (23%)] Loss: -227794.593750\n",
      "Train Epoch: 355 [14080/54000 (26%)] Loss: -247704.046875\n",
      "Train Epoch: 355 [15488/54000 (29%)] Loss: -225786.812500\n",
      "Train Epoch: 355 [16896/54000 (31%)] Loss: -218869.953125\n",
      "Train Epoch: 355 [18304/54000 (34%)] Loss: -225006.593750\n",
      "Train Epoch: 355 [19712/54000 (37%)] Loss: -223133.109375\n",
      "Train Epoch: 355 [21120/54000 (39%)] Loss: -223822.671875\n",
      "Train Epoch: 355 [22528/54000 (42%)] Loss: -232119.312500\n",
      "Train Epoch: 355 [23936/54000 (44%)] Loss: -249653.656250\n",
      "Train Epoch: 355 [25344/54000 (47%)] Loss: -230517.640625\n",
      "Train Epoch: 355 [26752/54000 (50%)] Loss: -230497.625000\n",
      "Train Epoch: 355 [28160/54000 (52%)] Loss: -227794.796875\n",
      "Train Epoch: 355 [29568/54000 (55%)] Loss: -226872.562500\n",
      "Train Epoch: 355 [30976/54000 (57%)] Loss: -231903.015625\n",
      "Train Epoch: 355 [32384/54000 (60%)] Loss: -221537.312500\n",
      "Train Epoch: 355 [33792/54000 (63%)] Loss: -224186.265625\n",
      "Train Epoch: 355 [35200/54000 (65%)] Loss: -221267.421875\n",
      "Train Epoch: 355 [36608/54000 (68%)] Loss: -221442.000000\n",
      "Train Epoch: 355 [38016/54000 (70%)] Loss: -219377.046875\n",
      "Train Epoch: 355 [39424/54000 (73%)] Loss: -216932.468750\n",
      "Train Epoch: 355 [40832/54000 (76%)] Loss: -227637.890625\n",
      "Train Epoch: 355 [42240/54000 (78%)] Loss: -226201.500000\n",
      "Train Epoch: 355 [43648/54000 (81%)] Loss: -220948.703125\n",
      "Train Epoch: 355 [45056/54000 (83%)] Loss: -231858.125000\n",
      "Train Epoch: 355 [46464/54000 (86%)] Loss: -227918.500000\n",
      "Train Epoch: 355 [47872/54000 (89%)] Loss: -227384.875000\n",
      "Train Epoch: 355 [49280/54000 (91%)] Loss: -225202.687500\n",
      "Train Epoch: 355 [50688/54000 (94%)] Loss: -248599.703125\n",
      "Train Epoch: 355 [52096/54000 (96%)] Loss: -230089.171875\n",
      "    epoch          : 355\n",
      "    loss           : -227705.86636513157\n",
      "    val_loss       : -229741.2114317359\n",
      "Train Epoch: 356 [0/54000 (0%)] Loss: -248516.812500\n",
      "Train Epoch: 356 [1408/54000 (3%)] Loss: -222981.515625\n",
      "Train Epoch: 356 [2816/54000 (5%)] Loss: -229665.375000\n",
      "Train Epoch: 356 [4224/54000 (8%)] Loss: -227866.328125\n",
      "Train Epoch: 356 [5632/54000 (10%)] Loss: -232114.000000\n",
      "Train Epoch: 356 [7040/54000 (13%)] Loss: -220991.468750\n",
      "Train Epoch: 356 [8448/54000 (16%)] Loss: -227707.703125\n",
      "Train Epoch: 356 [9856/54000 (18%)] Loss: -226242.187500\n",
      "Train Epoch: 356 [11264/54000 (21%)] Loss: -224213.796875\n",
      "Train Epoch: 356 [12672/54000 (23%)] Loss: -222917.046875\n",
      "Train Epoch: 356 [14080/54000 (26%)] Loss: -220927.437500\n",
      "Train Epoch: 356 [15488/54000 (29%)] Loss: -221549.562500\n",
      "Train Epoch: 356 [16896/54000 (31%)] Loss: -222205.656250\n",
      "Train Epoch: 356 [18304/54000 (34%)] Loss: -228765.109375\n",
      "Train Epoch: 356 [19712/54000 (37%)] Loss: -221071.109375\n",
      "Train Epoch: 356 [21120/54000 (39%)] Loss: -250062.125000\n",
      "Train Epoch: 356 [22528/54000 (42%)] Loss: -231272.937500\n",
      "Train Epoch: 356 [23936/54000 (44%)] Loss: -223336.390625\n",
      "Train Epoch: 356 [25344/54000 (47%)] Loss: -248148.000000\n",
      "Train Epoch: 356 [26752/54000 (50%)] Loss: -223383.265625\n",
      "Train Epoch: 356 [28160/54000 (52%)] Loss: -228663.640625\n",
      "Train Epoch: 356 [29568/54000 (55%)] Loss: -220213.421875\n",
      "Train Epoch: 356 [30976/54000 (57%)] Loss: -228596.687500\n",
      "Train Epoch: 356 [32384/54000 (60%)] Loss: -220050.531250\n",
      "Train Epoch: 356 [33792/54000 (63%)] Loss: -224618.593750\n",
      "Train Epoch: 356 [35200/54000 (65%)] Loss: -222354.296875\n",
      "Train Epoch: 356 [36608/54000 (68%)] Loss: -216735.968750\n",
      "Train Epoch: 356 [38016/54000 (70%)] Loss: -219076.328125\n",
      "Train Epoch: 356 [39424/54000 (73%)] Loss: -223988.281250\n",
      "Train Epoch: 356 [40832/54000 (76%)] Loss: -250115.859375\n",
      "Train Epoch: 356 [42240/54000 (78%)] Loss: -225256.781250\n",
      "Train Epoch: 356 [43648/54000 (81%)] Loss: -229312.000000\n",
      "Train Epoch: 356 [45056/54000 (83%)] Loss: -230107.171875\n",
      "Train Epoch: 356 [46464/54000 (86%)] Loss: -225502.281250\n",
      "Train Epoch: 356 [47872/54000 (89%)] Loss: -229009.734375\n",
      "Train Epoch: 356 [49280/54000 (91%)] Loss: -222789.500000\n",
      "Train Epoch: 356 [50688/54000 (94%)] Loss: -221109.890625\n",
      "Train Epoch: 356 [52096/54000 (96%)] Loss: -220212.765625\n",
      "    epoch          : 356\n",
      "    loss           : -227689.62885017943\n",
      "    val_loss       : -230364.9376786395\n",
      "Train Epoch: 357 [0/54000 (0%)] Loss: -220819.937500\n",
      "Train Epoch: 357 [1408/54000 (3%)] Loss: -217032.750000\n",
      "Train Epoch: 357 [2816/54000 (5%)] Loss: -222136.031250\n",
      "Train Epoch: 357 [4224/54000 (8%)] Loss: -222911.843750\n",
      "Train Epoch: 357 [5632/54000 (10%)] Loss: -221504.578125\n",
      "Train Epoch: 357 [7040/54000 (13%)] Loss: -225587.125000\n",
      "Train Epoch: 357 [8448/54000 (16%)] Loss: -248561.828125\n",
      "Train Epoch: 357 [9856/54000 (18%)] Loss: -219856.781250\n",
      "Train Epoch: 357 [11264/54000 (21%)] Loss: -232174.906250\n",
      "Train Epoch: 357 [12672/54000 (23%)] Loss: -232356.875000\n",
      "Train Epoch: 357 [14080/54000 (26%)] Loss: -249995.593750\n",
      "Train Epoch: 357 [15488/54000 (29%)] Loss: -222727.812500\n",
      "Train Epoch: 357 [16896/54000 (31%)] Loss: -224158.578125\n",
      "Train Epoch: 357 [18304/54000 (34%)] Loss: -250407.093750\n",
      "Train Epoch: 357 [19712/54000 (37%)] Loss: -232948.250000\n",
      "Train Epoch: 357 [21120/54000 (39%)] Loss: -231644.765625\n",
      "Train Epoch: 357 [22528/54000 (42%)] Loss: -220463.875000\n",
      "Train Epoch: 357 [23936/54000 (44%)] Loss: -225843.578125\n",
      "Train Epoch: 357 [25344/54000 (47%)] Loss: -221266.281250\n",
      "Train Epoch: 357 [26752/54000 (50%)] Loss: -249205.875000\n",
      "Train Epoch: 357 [28160/54000 (52%)] Loss: -221457.593750\n",
      "Train Epoch: 357 [29568/54000 (55%)] Loss: -221397.875000\n",
      "Train Epoch: 357 [30976/54000 (57%)] Loss: -227969.406250\n",
      "Train Epoch: 357 [32384/54000 (60%)] Loss: -221446.843750\n",
      "Train Epoch: 357 [33792/54000 (63%)] Loss: -250090.218750\n",
      "Train Epoch: 357 [35200/54000 (65%)] Loss: -225110.828125\n",
      "Train Epoch: 357 [36608/54000 (68%)] Loss: -230574.937500\n",
      "Train Epoch: 357 [38016/54000 (70%)] Loss: -222478.046875\n",
      "Train Epoch: 357 [39424/54000 (73%)] Loss: -250265.593750\n",
      "Train Epoch: 357 [40832/54000 (76%)] Loss: -223906.171875\n",
      "Train Epoch: 357 [42240/54000 (78%)] Loss: -215578.171875\n",
      "Train Epoch: 357 [43648/54000 (81%)] Loss: -231364.296875\n",
      "Train Epoch: 357 [45056/54000 (83%)] Loss: -249608.187500\n",
      "Train Epoch: 357 [46464/54000 (86%)] Loss: -222029.593750\n",
      "Train Epoch: 357 [47872/54000 (89%)] Loss: -220345.890625\n",
      "Train Epoch: 357 [49280/54000 (91%)] Loss: -223161.234375\n",
      "Train Epoch: 357 [50688/54000 (94%)] Loss: -220614.281250\n",
      "Train Epoch: 357 [52096/54000 (96%)] Loss: -248711.078125\n",
      "    epoch          : 357\n",
      "    loss           : -227722.98452452154\n",
      "    val_loss       : -230092.2472727706\n",
      "Train Epoch: 358 [0/54000 (0%)] Loss: -232728.406250\n",
      "Train Epoch: 358 [1408/54000 (3%)] Loss: -225863.437500\n",
      "Train Epoch: 358 [2816/54000 (5%)] Loss: -228938.890625\n",
      "Train Epoch: 358 [4224/54000 (8%)] Loss: -225948.375000\n",
      "Train Epoch: 358 [5632/54000 (10%)] Loss: -230058.359375\n",
      "Train Epoch: 358 [7040/54000 (13%)] Loss: -230160.265625\n",
      "Train Epoch: 358 [8448/54000 (16%)] Loss: -229525.812500\n",
      "Train Epoch: 358 [9856/54000 (18%)] Loss: -231757.734375\n",
      "Train Epoch: 358 [11264/54000 (21%)] Loss: -231855.953125\n",
      "Train Epoch: 358 [12672/54000 (23%)] Loss: -248875.687500\n",
      "Train Epoch: 358 [14080/54000 (26%)] Loss: -225342.437500\n",
      "Train Epoch: 358 [15488/54000 (29%)] Loss: -223323.031250\n",
      "Train Epoch: 358 [16896/54000 (31%)] Loss: -222950.953125\n",
      "Train Epoch: 358 [18304/54000 (34%)] Loss: -225583.890625\n",
      "Train Epoch: 358 [19712/54000 (37%)] Loss: -225965.718750\n",
      "Train Epoch: 358 [21120/54000 (39%)] Loss: -225467.812500\n",
      "Train Epoch: 358 [22528/54000 (42%)] Loss: -230638.890625\n",
      "Train Epoch: 358 [23936/54000 (44%)] Loss: -226404.906250\n",
      "Train Epoch: 358 [25344/54000 (47%)] Loss: -216449.140625\n",
      "Train Epoch: 358 [26752/54000 (50%)] Loss: -219505.656250\n",
      "Train Epoch: 358 [28160/54000 (52%)] Loss: -225396.546875\n",
      "Train Epoch: 358 [29568/54000 (55%)] Loss: -219713.125000\n",
      "Train Epoch: 358 [30976/54000 (57%)] Loss: -224515.984375\n",
      "Train Epoch: 358 [32384/54000 (60%)] Loss: -230642.921875\n",
      "Train Epoch: 358 [33792/54000 (63%)] Loss: -223081.625000\n",
      "Train Epoch: 358 [35200/54000 (65%)] Loss: -223906.078125\n",
      "Train Epoch: 358 [36608/54000 (68%)] Loss: -246760.015625\n",
      "Train Epoch: 358 [38016/54000 (70%)] Loss: -228987.500000\n",
      "Train Epoch: 358 [39424/54000 (73%)] Loss: -218568.687500\n",
      "Train Epoch: 358 [40832/54000 (76%)] Loss: -230464.718750\n",
      "Train Epoch: 358 [42240/54000 (78%)] Loss: -248420.125000\n",
      "Train Epoch: 358 [43648/54000 (81%)] Loss: -225556.218750\n",
      "Train Epoch: 358 [45056/54000 (83%)] Loss: -227166.343750\n",
      "Train Epoch: 358 [46464/54000 (86%)] Loss: -225985.328125\n",
      "Train Epoch: 358 [47872/54000 (89%)] Loss: -225510.390625\n",
      "Train Epoch: 358 [49280/54000 (91%)] Loss: -219380.937500\n",
      "Train Epoch: 358 [50688/54000 (94%)] Loss: -250102.312500\n",
      "Train Epoch: 358 [52096/54000 (96%)] Loss: -230352.312500\n",
      "    epoch          : 358\n",
      "    loss           : -227803.84561901915\n",
      "    val_loss       : -230192.4701969798\n",
      "Train Epoch: 359 [0/54000 (0%)] Loss: -250032.406250\n",
      "Train Epoch: 359 [1408/54000 (3%)] Loss: -223341.500000\n",
      "Train Epoch: 359 [2816/54000 (5%)] Loss: -223111.500000\n",
      "Train Epoch: 359 [4224/54000 (8%)] Loss: -224839.359375\n",
      "Train Epoch: 359 [5632/54000 (10%)] Loss: -220196.015625\n",
      "Train Epoch: 359 [7040/54000 (13%)] Loss: -224062.687500\n",
      "Train Epoch: 359 [8448/54000 (16%)] Loss: -247860.125000\n",
      "Train Epoch: 359 [9856/54000 (18%)] Loss: -223737.640625\n",
      "Train Epoch: 359 [11264/54000 (21%)] Loss: -231046.703125\n",
      "Train Epoch: 359 [12672/54000 (23%)] Loss: -249653.343750\n",
      "Train Epoch: 359 [14080/54000 (26%)] Loss: -217872.437500\n",
      "Train Epoch: 359 [15488/54000 (29%)] Loss: -220888.906250\n",
      "Train Epoch: 359 [16896/54000 (31%)] Loss: -231664.296875\n",
      "Train Epoch: 359 [18304/54000 (34%)] Loss: -230512.531250\n",
      "Train Epoch: 359 [19712/54000 (37%)] Loss: -218486.343750\n",
      "Train Epoch: 359 [21120/54000 (39%)] Loss: -218835.234375\n",
      "Train Epoch: 359 [22528/54000 (42%)] Loss: -220734.281250\n",
      "Train Epoch: 359 [23936/54000 (44%)] Loss: -230499.390625\n",
      "Train Epoch: 359 [25344/54000 (47%)] Loss: -222254.140625\n",
      "Train Epoch: 359 [26752/54000 (50%)] Loss: -222015.437500\n",
      "Train Epoch: 359 [28160/54000 (52%)] Loss: -219061.843750\n",
      "Train Epoch: 359 [29568/54000 (55%)] Loss: -219101.562500\n",
      "Train Epoch: 359 [30976/54000 (57%)] Loss: -217674.828125\n",
      "Train Epoch: 359 [32384/54000 (60%)] Loss: -224190.187500\n",
      "Train Epoch: 359 [33792/54000 (63%)] Loss: -248986.437500\n",
      "Train Epoch: 359 [35200/54000 (65%)] Loss: -225315.812500\n",
      "Train Epoch: 359 [36608/54000 (68%)] Loss: -220010.843750\n",
      "Train Epoch: 359 [38016/54000 (70%)] Loss: -225235.546875\n",
      "Train Epoch: 359 [39424/54000 (73%)] Loss: -226541.046875\n",
      "Train Epoch: 359 [40832/54000 (76%)] Loss: -247544.125000\n",
      "Train Epoch: 359 [42240/54000 (78%)] Loss: -223218.765625\n",
      "Train Epoch: 359 [43648/54000 (81%)] Loss: -220745.125000\n",
      "Train Epoch: 359 [45056/54000 (83%)] Loss: -227649.640625\n",
      "Train Epoch: 359 [46464/54000 (86%)] Loss: -230818.609375\n",
      "Train Epoch: 359 [47872/54000 (89%)] Loss: -229781.218750\n",
      "Train Epoch: 359 [49280/54000 (91%)] Loss: -216719.437500\n",
      "Train Epoch: 359 [50688/54000 (94%)] Loss: -221788.656250\n",
      "Train Epoch: 359 [52096/54000 (96%)] Loss: -232184.765625\n",
      "    epoch          : 359\n",
      "    loss           : -227641.4207909689\n",
      "    val_loss       : -230005.65362399962\n",
      "Train Epoch: 360 [0/54000 (0%)] Loss: -225000.031250\n",
      "Train Epoch: 360 [1408/54000 (3%)] Loss: -216883.984375\n",
      "Train Epoch: 360 [2816/54000 (5%)] Loss: -216351.421875\n",
      "Train Epoch: 360 [4224/54000 (8%)] Loss: -231337.718750\n",
      "Train Epoch: 360 [5632/54000 (10%)] Loss: -231697.593750\n",
      "Train Epoch: 360 [7040/54000 (13%)] Loss: -229995.906250\n",
      "Train Epoch: 360 [8448/54000 (16%)] Loss: -247390.765625\n",
      "Train Epoch: 360 [9856/54000 (18%)] Loss: -219639.265625\n",
      "Train Epoch: 360 [11264/54000 (21%)] Loss: -221247.453125\n",
      "Train Epoch: 360 [12672/54000 (23%)] Loss: -227991.546875\n",
      "Train Epoch: 360 [14080/54000 (26%)] Loss: -226402.625000\n",
      "Train Epoch: 360 [15488/54000 (29%)] Loss: -229674.781250\n",
      "Train Epoch: 360 [16896/54000 (31%)] Loss: -224470.437500\n",
      "Train Epoch: 360 [18304/54000 (34%)] Loss: -219118.609375\n",
      "Train Epoch: 360 [19712/54000 (37%)] Loss: -249998.500000\n",
      "Train Epoch: 360 [21120/54000 (39%)] Loss: -230626.515625\n",
      "Train Epoch: 360 [22528/54000 (42%)] Loss: -222295.343750\n",
      "Train Epoch: 360 [23936/54000 (44%)] Loss: -225533.656250\n",
      "Train Epoch: 360 [25344/54000 (47%)] Loss: -232688.250000\n",
      "Train Epoch: 360 [26752/54000 (50%)] Loss: -219850.765625\n",
      "Train Epoch: 360 [28160/54000 (52%)] Loss: -221867.593750\n",
      "Train Epoch: 360 [29568/54000 (55%)] Loss: -223211.546875\n",
      "Train Epoch: 360 [30976/54000 (57%)] Loss: -218348.656250\n",
      "Train Epoch: 360 [32384/54000 (60%)] Loss: -231476.296875\n",
      "Train Epoch: 360 [33792/54000 (63%)] Loss: -228084.468750\n",
      "Train Epoch: 360 [35200/54000 (65%)] Loss: -230266.125000\n",
      "Train Epoch: 360 [36608/54000 (68%)] Loss: -248894.718750\n",
      "Train Epoch: 360 [38016/54000 (70%)] Loss: -231270.593750\n",
      "Train Epoch: 360 [39424/54000 (73%)] Loss: -231675.203125\n",
      "Train Epoch: 360 [40832/54000 (76%)] Loss: -222262.750000\n",
      "Train Epoch: 360 [42240/54000 (78%)] Loss: -220929.500000\n",
      "Train Epoch: 360 [43648/54000 (81%)] Loss: -221765.171875\n",
      "Train Epoch: 360 [45056/54000 (83%)] Loss: -225202.296875\n",
      "Train Epoch: 360 [46464/54000 (86%)] Loss: -227583.812500\n",
      "Train Epoch: 360 [47872/54000 (89%)] Loss: -228909.453125\n",
      "Train Epoch: 360 [49280/54000 (91%)] Loss: -224980.656250\n",
      "Train Epoch: 360 [50688/54000 (94%)] Loss: -248229.515625\n",
      "Train Epoch: 360 [52096/54000 (96%)] Loss: -232341.468750\n",
      "    epoch          : 360\n",
      "    loss           : -227654.3444228469\n",
      "    val_loss       : -230042.12661371\n",
      "Train Epoch: 361 [0/54000 (0%)] Loss: -248874.703125\n",
      "Train Epoch: 361 [1408/54000 (3%)] Loss: -224715.343750\n",
      "Train Epoch: 361 [2816/54000 (5%)] Loss: -230387.890625\n",
      "Train Epoch: 361 [4224/54000 (8%)] Loss: -220094.187500\n",
      "Train Epoch: 361 [5632/54000 (10%)] Loss: -217785.578125\n",
      "Train Epoch: 361 [7040/54000 (13%)] Loss: -216597.562500\n",
      "Train Epoch: 361 [8448/54000 (16%)] Loss: -220999.031250\n",
      "Train Epoch: 361 [9856/54000 (18%)] Loss: -228014.359375\n",
      "Train Epoch: 361 [11264/54000 (21%)] Loss: -225360.906250\n",
      "Train Epoch: 361 [12672/54000 (23%)] Loss: -221437.750000\n",
      "Train Epoch: 361 [14080/54000 (26%)] Loss: -247925.187500\n",
      "Train Epoch: 361 [15488/54000 (29%)] Loss: -229847.671875\n",
      "Train Epoch: 361 [16896/54000 (31%)] Loss: -225487.296875\n",
      "Train Epoch: 361 [18304/54000 (34%)] Loss: -249330.718750\n",
      "Train Epoch: 361 [19712/54000 (37%)] Loss: -219248.125000\n",
      "Train Epoch: 361 [21120/54000 (39%)] Loss: -223911.109375\n",
      "Train Epoch: 361 [22528/54000 (42%)] Loss: -223042.562500\n",
      "Train Epoch: 361 [23936/54000 (44%)] Loss: -232602.625000\n",
      "Train Epoch: 361 [25344/54000 (47%)] Loss: -217395.125000\n",
      "Train Epoch: 361 [26752/54000 (50%)] Loss: -225155.406250\n",
      "Train Epoch: 361 [28160/54000 (52%)] Loss: -225453.031250\n",
      "Train Epoch: 361 [29568/54000 (55%)] Loss: -222605.437500\n",
      "Train Epoch: 361 [30976/54000 (57%)] Loss: -249289.625000\n",
      "Train Epoch: 361 [32384/54000 (60%)] Loss: -229518.703125\n",
      "Train Epoch: 361 [33792/54000 (63%)] Loss: -228494.625000\n",
      "Train Epoch: 361 [35200/54000 (65%)] Loss: -221603.703125\n",
      "Train Epoch: 361 [36608/54000 (68%)] Loss: -231660.312500\n",
      "Train Epoch: 361 [38016/54000 (70%)] Loss: -221391.140625\n",
      "Train Epoch: 361 [39424/54000 (73%)] Loss: -231552.968750\n",
      "Train Epoch: 361 [40832/54000 (76%)] Loss: -231809.203125\n",
      "Train Epoch: 361 [42240/54000 (78%)] Loss: -218556.671875\n",
      "Train Epoch: 361 [43648/54000 (81%)] Loss: -222010.750000\n",
      "Train Epoch: 361 [45056/54000 (83%)] Loss: -220529.781250\n",
      "Train Epoch: 361 [46464/54000 (86%)] Loss: -227986.359375\n",
      "Train Epoch: 361 [47872/54000 (89%)] Loss: -222940.453125\n",
      "Train Epoch: 361 [49280/54000 (91%)] Loss: -221378.531250\n",
      "Train Epoch: 361 [50688/54000 (94%)] Loss: -217965.484375\n",
      "Train Epoch: 361 [52096/54000 (96%)] Loss: -249803.812500\n",
      "    epoch          : 361\n",
      "    loss           : -227753.03588516745\n",
      "    val_loss       : -229875.95475657392\n",
      "Train Epoch: 362 [0/54000 (0%)] Loss: -250094.828125\n",
      "Train Epoch: 362 [1408/54000 (3%)] Loss: -231056.578125\n",
      "Train Epoch: 362 [2816/54000 (5%)] Loss: -230821.093750\n",
      "Train Epoch: 362 [4224/54000 (8%)] Loss: -218260.562500\n",
      "Train Epoch: 362 [5632/54000 (10%)] Loss: -221559.500000\n",
      "Train Epoch: 362 [7040/54000 (13%)] Loss: -231434.031250\n",
      "Train Epoch: 362 [8448/54000 (16%)] Loss: -249963.812500\n",
      "Train Epoch: 362 [9856/54000 (18%)] Loss: -226072.203125\n",
      "Train Epoch: 362 [11264/54000 (21%)] Loss: -229276.750000\n",
      "Train Epoch: 362 [12672/54000 (23%)] Loss: -230604.687500\n",
      "Train Epoch: 362 [14080/54000 (26%)] Loss: -222110.125000\n",
      "Train Epoch: 362 [15488/54000 (29%)] Loss: -248375.296875\n",
      "Train Epoch: 362 [16896/54000 (31%)] Loss: -220688.578125\n",
      "Train Epoch: 362 [18304/54000 (34%)] Loss: -224067.531250\n",
      "Train Epoch: 362 [19712/54000 (37%)] Loss: -221472.093750\n",
      "Train Epoch: 362 [21120/54000 (39%)] Loss: -229089.234375\n",
      "Train Epoch: 362 [22528/54000 (42%)] Loss: -224969.093750\n",
      "Train Epoch: 362 [23936/54000 (44%)] Loss: -225455.781250\n",
      "Train Epoch: 362 [25344/54000 (47%)] Loss: -217428.609375\n",
      "Train Epoch: 362 [26752/54000 (50%)] Loss: -220703.750000\n",
      "Train Epoch: 362 [28160/54000 (52%)] Loss: -220974.437500\n",
      "Train Epoch: 362 [29568/54000 (55%)] Loss: -221483.125000\n",
      "Train Epoch: 362 [30976/54000 (57%)] Loss: -217692.796875\n",
      "Train Epoch: 362 [32384/54000 (60%)] Loss: -225217.484375\n",
      "Train Epoch: 362 [33792/54000 (63%)] Loss: -219041.546875\n",
      "Train Epoch: 362 [35200/54000 (65%)] Loss: -220948.250000\n",
      "Train Epoch: 362 [36608/54000 (68%)] Loss: -221737.953125\n",
      "Train Epoch: 362 [38016/54000 (70%)] Loss: -229036.468750\n",
      "Train Epoch: 362 [39424/54000 (73%)] Loss: -231490.937500\n",
      "Train Epoch: 362 [40832/54000 (76%)] Loss: -248527.640625\n",
      "Train Epoch: 362 [42240/54000 (78%)] Loss: -229422.812500\n",
      "Train Epoch: 362 [43648/54000 (81%)] Loss: -229418.375000\n",
      "Train Epoch: 362 [45056/54000 (83%)] Loss: -222876.156250\n",
      "Train Epoch: 362 [46464/54000 (86%)] Loss: -225982.875000\n",
      "Train Epoch: 362 [47872/54000 (89%)] Loss: -227882.031250\n",
      "Train Epoch: 362 [49280/54000 (91%)] Loss: -220391.718750\n",
      "Train Epoch: 362 [50688/54000 (94%)] Loss: -218990.312500\n",
      "Train Epoch: 362 [52096/54000 (96%)] Loss: -221426.781250\n",
      "    epoch          : 362\n",
      "    loss           : -227811.5544632177\n",
      "    val_loss       : -230228.83390696454\n",
      "Train Epoch: 363 [0/54000 (0%)] Loss: -232192.250000\n",
      "Train Epoch: 363 [1408/54000 (3%)] Loss: -230063.500000\n",
      "Train Epoch: 363 [2816/54000 (5%)] Loss: -217821.781250\n",
      "Train Epoch: 363 [4224/54000 (8%)] Loss: -219597.703125\n",
      "Train Epoch: 363 [5632/54000 (10%)] Loss: -231378.843750\n",
      "Train Epoch: 363 [7040/54000 (13%)] Loss: -222119.703125\n",
      "Train Epoch: 363 [8448/54000 (16%)] Loss: -218427.578125\n",
      "Train Epoch: 363 [9856/54000 (18%)] Loss: -222723.843750\n",
      "Train Epoch: 363 [11264/54000 (21%)] Loss: -230773.781250\n",
      "Train Epoch: 363 [12672/54000 (23%)] Loss: -229196.281250\n",
      "Train Epoch: 363 [14080/54000 (26%)] Loss: -249246.796875\n",
      "Train Epoch: 363 [15488/54000 (29%)] Loss: -227552.625000\n",
      "Train Epoch: 363 [16896/54000 (31%)] Loss: -220088.812500\n",
      "Train Epoch: 363 [18304/54000 (34%)] Loss: -225233.187500\n",
      "Train Epoch: 363 [19712/54000 (37%)] Loss: -249273.125000\n",
      "Train Epoch: 363 [21120/54000 (39%)] Loss: -228393.015625\n",
      "Train Epoch: 363 [22528/54000 (42%)] Loss: -214134.390625\n",
      "Train Epoch: 363 [23936/54000 (44%)] Loss: -223300.187500\n",
      "Train Epoch: 363 [25344/54000 (47%)] Loss: -249592.562500\n",
      "Train Epoch: 363 [26752/54000 (50%)] Loss: -221082.312500\n",
      "Train Epoch: 363 [28160/54000 (52%)] Loss: -220283.593750\n",
      "Train Epoch: 363 [29568/54000 (55%)] Loss: -224750.187500\n",
      "Train Epoch: 363 [30976/54000 (57%)] Loss: -226299.531250\n",
      "Train Epoch: 363 [32384/54000 (60%)] Loss: -250071.656250\n",
      "Train Epoch: 363 [33792/54000 (63%)] Loss: -222003.046875\n",
      "Train Epoch: 363 [35200/54000 (65%)] Loss: -221073.046875\n",
      "Train Epoch: 363 [36608/54000 (68%)] Loss: -230958.421875\n",
      "Train Epoch: 363 [38016/54000 (70%)] Loss: -224694.703125\n",
      "Train Epoch: 363 [39424/54000 (73%)] Loss: -231153.578125\n",
      "Train Epoch: 363 [40832/54000 (76%)] Loss: -231291.250000\n",
      "Train Epoch: 363 [42240/54000 (78%)] Loss: -217511.765625\n",
      "Train Epoch: 363 [43648/54000 (81%)] Loss: -224712.656250\n",
      "Train Epoch: 363 [45056/54000 (83%)] Loss: -221296.890625\n",
      "Train Epoch: 363 [46464/54000 (86%)] Loss: -222781.078125\n",
      "Train Epoch: 363 [47872/54000 (89%)] Loss: -222073.171875\n",
      "Train Epoch: 363 [49280/54000 (91%)] Loss: -226486.187500\n",
      "Train Epoch: 363 [50688/54000 (94%)] Loss: -232705.265625\n",
      "Train Epoch: 363 [52096/54000 (96%)] Loss: -222865.218750\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   363: reducing learning rate of group 0 to 5.0000e-05.\n",
      "    epoch          : 363\n",
      "    loss           : -227867.62144886365\n",
      "    val_loss       : -229811.33914705602\n",
      "Train Epoch: 364 [0/54000 (0%)] Loss: -250337.140625\n",
      "Train Epoch: 364 [1408/54000 (3%)] Loss: -226325.296875\n",
      "Train Epoch: 364 [2816/54000 (5%)] Loss: -221601.234375\n",
      "Train Epoch: 364 [4224/54000 (8%)] Loss: -219615.062500\n",
      "Train Epoch: 364 [5632/54000 (10%)] Loss: -243793.187500\n",
      "Train Epoch: 364 [7040/54000 (13%)] Loss: -218172.921875\n",
      "Train Epoch: 364 [8448/54000 (16%)] Loss: -219454.671875\n",
      "Train Epoch: 364 [9856/54000 (18%)] Loss: -218577.812500\n",
      "Train Epoch: 364 [11264/54000 (21%)] Loss: -232567.437500\n",
      "Train Epoch: 364 [12672/54000 (23%)] Loss: -220927.000000\n",
      "Train Epoch: 364 [14080/54000 (26%)] Loss: -221582.609375\n",
      "Train Epoch: 364 [15488/54000 (29%)] Loss: -229120.000000\n",
      "Train Epoch: 364 [16896/54000 (31%)] Loss: -249388.328125\n",
      "Train Epoch: 364 [18304/54000 (34%)] Loss: -229128.406250\n",
      "Train Epoch: 364 [19712/54000 (37%)] Loss: -221181.312500\n",
      "Train Epoch: 364 [21120/54000 (39%)] Loss: -231817.093750\n",
      "Train Epoch: 364 [22528/54000 (42%)] Loss: -226021.406250\n",
      "Train Epoch: 364 [23936/54000 (44%)] Loss: -231976.921875\n",
      "Train Epoch: 364 [25344/54000 (47%)] Loss: -226158.968750\n",
      "Train Epoch: 364 [26752/54000 (50%)] Loss: -223041.000000\n",
      "Train Epoch: 364 [28160/54000 (52%)] Loss: -230726.921875\n",
      "Train Epoch: 364 [29568/54000 (55%)] Loss: -224805.843750\n",
      "Train Epoch: 364 [30976/54000 (57%)] Loss: -246857.468750\n",
      "Train Epoch: 364 [32384/54000 (60%)] Loss: -228922.703125\n",
      "Train Epoch: 364 [33792/54000 (63%)] Loss: -219955.062500\n",
      "Train Epoch: 364 [35200/54000 (65%)] Loss: -221247.218750\n",
      "Train Epoch: 364 [36608/54000 (68%)] Loss: -222178.781250\n",
      "Train Epoch: 364 [38016/54000 (70%)] Loss: -227151.109375\n",
      "Train Epoch: 364 [39424/54000 (73%)] Loss: -249909.312500\n",
      "Train Epoch: 364 [40832/54000 (76%)] Loss: -229924.125000\n",
      "Train Epoch: 364 [42240/54000 (78%)] Loss: -231291.156250\n",
      "Train Epoch: 364 [43648/54000 (81%)] Loss: -226216.906250\n",
      "Train Epoch: 364 [45056/54000 (83%)] Loss: -219554.062500\n",
      "Train Epoch: 364 [46464/54000 (86%)] Loss: -231977.656250\n",
      "Train Epoch: 364 [47872/54000 (89%)] Loss: -225142.250000\n",
      "Train Epoch: 364 [49280/54000 (91%)] Loss: -219589.453125\n",
      "Train Epoch: 364 [50688/54000 (94%)] Loss: -248330.390625\n",
      "Train Epoch: 364 [52096/54000 (96%)] Loss: -230187.296875\n",
      "    epoch          : 364\n",
      "    loss           : -228013.00878438994\n",
      "    val_loss       : -230414.3858374619\n",
      "Train Epoch: 365 [0/54000 (0%)] Loss: -221746.000000\n",
      "Train Epoch: 365 [1408/54000 (3%)] Loss: -221554.546875\n",
      "Train Epoch: 365 [2816/54000 (5%)] Loss: -232333.937500\n",
      "Train Epoch: 365 [4224/54000 (8%)] Loss: -222072.796875\n",
      "Train Epoch: 365 [5632/54000 (10%)] Loss: -225639.750000\n",
      "Train Epoch: 365 [7040/54000 (13%)] Loss: -226572.031250\n",
      "Train Epoch: 365 [8448/54000 (16%)] Loss: -231447.812500\n",
      "Train Epoch: 365 [9856/54000 (18%)] Loss: -223830.906250\n",
      "Train Epoch: 365 [11264/54000 (21%)] Loss: -226675.718750\n",
      "Train Epoch: 365 [12672/54000 (23%)] Loss: -220554.156250\n",
      "Train Epoch: 365 [14080/54000 (26%)] Loss: -232170.640625\n",
      "Train Epoch: 365 [15488/54000 (29%)] Loss: -249675.171875\n",
      "Train Epoch: 365 [16896/54000 (31%)] Loss: -221661.671875\n",
      "Train Epoch: 365 [18304/54000 (34%)] Loss: -218585.578125\n",
      "Train Epoch: 365 [19712/54000 (37%)] Loss: -249978.312500\n",
      "Train Epoch: 365 [21120/54000 (39%)] Loss: -228022.906250\n",
      "Train Epoch: 365 [22528/54000 (42%)] Loss: -221182.984375\n",
      "Train Epoch: 365 [23936/54000 (44%)] Loss: -227163.093750\n",
      "Train Epoch: 365 [25344/54000 (47%)] Loss: -223344.890625\n",
      "Train Epoch: 365 [26752/54000 (50%)] Loss: -220444.906250\n",
      "Train Epoch: 365 [28160/54000 (52%)] Loss: -225981.140625\n",
      "Train Epoch: 365 [29568/54000 (55%)] Loss: -221421.000000\n",
      "Train Epoch: 365 [30976/54000 (57%)] Loss: -220982.968750\n",
      "Train Epoch: 365 [32384/54000 (60%)] Loss: -229266.687500\n",
      "Train Epoch: 365 [33792/54000 (63%)] Loss: -217592.093750\n",
      "Train Epoch: 365 [35200/54000 (65%)] Loss: -231367.875000\n",
      "Train Epoch: 365 [36608/54000 (68%)] Loss: -250308.187500\n",
      "Train Epoch: 365 [38016/54000 (70%)] Loss: -221653.890625\n",
      "Train Epoch: 365 [39424/54000 (73%)] Loss: -225976.328125\n",
      "Train Epoch: 365 [40832/54000 (76%)] Loss: -230989.937500\n",
      "Train Epoch: 365 [42240/54000 (78%)] Loss: -248800.656250\n",
      "Train Epoch: 365 [43648/54000 (81%)] Loss: -232264.531250\n",
      "Train Epoch: 365 [45056/54000 (83%)] Loss: -231787.218750\n",
      "Train Epoch: 365 [46464/54000 (86%)] Loss: -221498.781250\n",
      "Train Epoch: 365 [47872/54000 (89%)] Loss: -220409.343750\n",
      "Train Epoch: 365 [49280/54000 (91%)] Loss: -222446.109375\n",
      "Train Epoch: 365 [50688/54000 (94%)] Loss: -218850.593750\n",
      "Train Epoch: 365 [52096/54000 (96%)] Loss: -229862.406250\n",
      "    epoch          : 365\n",
      "    loss           : -228125.57416267943\n",
      "    val_loss       : -230531.6660989901\n",
      "Train Epoch: 366 [0/54000 (0%)] Loss: -250280.093750\n",
      "Train Epoch: 366 [1408/54000 (3%)] Loss: -223983.531250\n",
      "Train Epoch: 366 [2816/54000 (5%)] Loss: -229982.171875\n",
      "Train Epoch: 366 [4224/54000 (8%)] Loss: -224536.562500\n",
      "Train Epoch: 366 [5632/54000 (10%)] Loss: -230640.375000\n",
      "Train Epoch: 366 [7040/54000 (13%)] Loss: -222522.359375\n",
      "Train Epoch: 366 [8448/54000 (16%)] Loss: -249265.703125\n",
      "Train Epoch: 366 [9856/54000 (18%)] Loss: -231234.718750\n",
      "Train Epoch: 366 [11264/54000 (21%)] Loss: -229208.140625\n",
      "Train Epoch: 366 [12672/54000 (23%)] Loss: -231661.265625\n",
      "Train Epoch: 366 [14080/54000 (26%)] Loss: -229958.656250\n",
      "Train Epoch: 366 [15488/54000 (29%)] Loss: -223689.375000\n",
      "Train Epoch: 366 [16896/54000 (31%)] Loss: -221275.687500\n",
      "Train Epoch: 366 [18304/54000 (34%)] Loss: -222119.421875\n",
      "Train Epoch: 366 [19712/54000 (37%)] Loss: -226917.828125\n",
      "Train Epoch: 366 [21120/54000 (39%)] Loss: -220124.234375\n",
      "Train Epoch: 366 [22528/54000 (42%)] Loss: -216131.390625\n",
      "Train Epoch: 366 [23936/54000 (44%)] Loss: -226131.500000\n",
      "Train Epoch: 366 [25344/54000 (47%)] Loss: -224150.937500\n",
      "Train Epoch: 366 [26752/54000 (50%)] Loss: -226246.281250\n",
      "Train Epoch: 366 [28160/54000 (52%)] Loss: -230682.484375\n",
      "Train Epoch: 366 [29568/54000 (55%)] Loss: -230192.281250\n",
      "Train Epoch: 366 [30976/54000 (57%)] Loss: -229993.906250\n",
      "Train Epoch: 366 [32384/54000 (60%)] Loss: -231421.828125\n",
      "Train Epoch: 366 [33792/54000 (63%)] Loss: -220785.890625\n",
      "Train Epoch: 366 [35200/54000 (65%)] Loss: -220892.343750\n",
      "Train Epoch: 366 [36608/54000 (68%)] Loss: -248657.484375\n",
      "Train Epoch: 366 [38016/54000 (70%)] Loss: -222752.484375\n",
      "Train Epoch: 366 [39424/54000 (73%)] Loss: -231640.250000\n",
      "Train Epoch: 366 [40832/54000 (76%)] Loss: -224236.718750\n",
      "Train Epoch: 366 [42240/54000 (78%)] Loss: -219004.015625\n",
      "Train Epoch: 366 [43648/54000 (81%)] Loss: -250503.156250\n",
      "Train Epoch: 366 [45056/54000 (83%)] Loss: -225498.140625\n",
      "Train Epoch: 366 [46464/54000 (86%)] Loss: -228730.531250\n",
      "Train Epoch: 366 [47872/54000 (89%)] Loss: -223371.453125\n",
      "Train Epoch: 366 [49280/54000 (91%)] Loss: -227076.656250\n",
      "Train Epoch: 366 [50688/54000 (94%)] Loss: -249596.562500\n",
      "Train Epoch: 366 [52096/54000 (96%)] Loss: -230949.187500\n",
      "    epoch          : 366\n",
      "    loss           : -227945.80528558613\n",
      "    val_loss       : -230650.06044564597\n",
      "Train Epoch: 367 [0/54000 (0%)] Loss: -224016.031250\n",
      "Train Epoch: 367 [1408/54000 (3%)] Loss: -249710.671875\n",
      "Train Epoch: 367 [2816/54000 (5%)] Loss: -222427.187500\n",
      "Train Epoch: 367 [4224/54000 (8%)] Loss: -221003.265625\n",
      "Train Epoch: 367 [5632/54000 (10%)] Loss: -223029.062500\n",
      "Train Epoch: 367 [7040/54000 (13%)] Loss: -228801.515625\n",
      "Train Epoch: 367 [8448/54000 (16%)] Loss: -218329.437500\n",
      "Train Epoch: 367 [9856/54000 (18%)] Loss: -249241.687500\n",
      "Train Epoch: 367 [11264/54000 (21%)] Loss: -225814.078125\n",
      "Train Epoch: 367 [12672/54000 (23%)] Loss: -229354.250000\n",
      "Train Epoch: 367 [14080/54000 (26%)] Loss: -224754.640625\n",
      "Train Epoch: 367 [15488/54000 (29%)] Loss: -224157.640625\n",
      "Train Epoch: 367 [16896/54000 (31%)] Loss: -227560.859375\n",
      "Train Epoch: 367 [18304/54000 (34%)] Loss: -231204.421875\n",
      "Train Epoch: 367 [19712/54000 (37%)] Loss: -223695.125000\n",
      "Train Epoch: 367 [21120/54000 (39%)] Loss: -223798.343750\n",
      "Train Epoch: 367 [22528/54000 (42%)] Loss: -228339.906250\n",
      "Train Epoch: 367 [23936/54000 (44%)] Loss: -222864.968750\n",
      "Train Epoch: 367 [25344/54000 (47%)] Loss: -219321.781250\n",
      "Train Epoch: 367 [26752/54000 (50%)] Loss: -227448.500000\n",
      "Train Epoch: 367 [28160/54000 (52%)] Loss: -221871.312500\n",
      "Train Epoch: 367 [29568/54000 (55%)] Loss: -249415.390625\n",
      "Train Epoch: 367 [30976/54000 (57%)] Loss: -227692.703125\n",
      "Train Epoch: 367 [32384/54000 (60%)] Loss: -225800.312500\n",
      "Train Epoch: 367 [33792/54000 (63%)] Loss: -247692.812500\n",
      "Train Epoch: 367 [35200/54000 (65%)] Loss: -221654.015625\n",
      "Train Epoch: 367 [36608/54000 (68%)] Loss: -232923.859375\n",
      "Train Epoch: 367 [38016/54000 (70%)] Loss: -220664.218750\n",
      "Train Epoch: 367 [39424/54000 (73%)] Loss: -222753.703125\n",
      "Train Epoch: 367 [40832/54000 (76%)] Loss: -231300.375000\n",
      "Train Epoch: 367 [42240/54000 (78%)] Loss: -223209.984375\n",
      "Train Epoch: 367 [43648/54000 (81%)] Loss: -223217.484375\n",
      "Train Epoch: 367 [45056/54000 (83%)] Loss: -218428.156250\n",
      "Train Epoch: 367 [46464/54000 (86%)] Loss: -227731.218750\n",
      "Train Epoch: 367 [47872/54000 (89%)] Loss: -218311.687500\n",
      "Train Epoch: 367 [49280/54000 (91%)] Loss: -228200.625000\n",
      "Train Epoch: 367 [50688/54000 (94%)] Loss: -230987.828125\n",
      "Train Epoch: 367 [52096/54000 (96%)] Loss: -231116.968750\n",
      "    epoch          : 367\n",
      "    loss           : -228021.26760616028\n",
      "    val_loss       : -230170.00902129384\n",
      "Train Epoch: 368 [0/54000 (0%)] Loss: -222186.640625\n",
      "Train Epoch: 368 [1408/54000 (3%)] Loss: -217080.937500\n",
      "Train Epoch: 368 [2816/54000 (5%)] Loss: -226127.921875\n",
      "Train Epoch: 368 [4224/54000 (8%)] Loss: -221456.203125\n",
      "Train Epoch: 368 [5632/54000 (10%)] Loss: -225481.500000\n",
      "Train Epoch: 368 [7040/54000 (13%)] Loss: -224834.187500\n",
      "Train Epoch: 368 [8448/54000 (16%)] Loss: -223313.187500\n",
      "Train Epoch: 368 [9856/54000 (18%)] Loss: -227856.296875\n",
      "Train Epoch: 368 [11264/54000 (21%)] Loss: -221857.359375\n",
      "Train Epoch: 368 [12672/54000 (23%)] Loss: -221945.343750\n",
      "Train Epoch: 368 [14080/54000 (26%)] Loss: -225507.093750\n",
      "Train Epoch: 368 [15488/54000 (29%)] Loss: -229858.750000\n",
      "Train Epoch: 368 [16896/54000 (31%)] Loss: -248760.359375\n",
      "Train Epoch: 368 [18304/54000 (34%)] Loss: -219927.765625\n",
      "Train Epoch: 368 [19712/54000 (37%)] Loss: -224862.343750\n",
      "Train Epoch: 368 [21120/54000 (39%)] Loss: -221011.125000\n",
      "Train Epoch: 368 [22528/54000 (42%)] Loss: -225572.234375\n",
      "Train Epoch: 368 [23936/54000 (44%)] Loss: -229458.343750\n",
      "Train Epoch: 368 [25344/54000 (47%)] Loss: -232665.562500\n",
      "Train Epoch: 368 [26752/54000 (50%)] Loss: -212703.703125\n",
      "Train Epoch: 368 [28160/54000 (52%)] Loss: -223389.562500\n",
      "Train Epoch: 368 [29568/54000 (55%)] Loss: -229580.078125\n",
      "Train Epoch: 368 [30976/54000 (57%)] Loss: -228124.281250\n",
      "Train Epoch: 368 [32384/54000 (60%)] Loss: -249063.968750\n",
      "Train Epoch: 368 [33792/54000 (63%)] Loss: -223663.125000\n",
      "Train Epoch: 368 [35200/54000 (65%)] Loss: -230903.937500\n",
      "Train Epoch: 368 [36608/54000 (68%)] Loss: -217986.171875\n",
      "Train Epoch: 368 [38016/54000 (70%)] Loss: -230833.390625\n",
      "Train Epoch: 368 [39424/54000 (73%)] Loss: -249949.984375\n",
      "Train Epoch: 368 [40832/54000 (76%)] Loss: -226773.812500\n",
      "Train Epoch: 368 [42240/54000 (78%)] Loss: -216266.375000\n",
      "Train Epoch: 368 [43648/54000 (81%)] Loss: -227639.656250\n",
      "Train Epoch: 368 [45056/54000 (83%)] Loss: -249853.531250\n",
      "Train Epoch: 368 [46464/54000 (86%)] Loss: -225478.875000\n",
      "Train Epoch: 368 [47872/54000 (89%)] Loss: -223341.937500\n",
      "Train Epoch: 368 [49280/54000 (91%)] Loss: -221905.593750\n",
      "Train Epoch: 368 [50688/54000 (94%)] Loss: -223448.812500\n",
      "Train Epoch: 368 [52096/54000 (96%)] Loss: -232278.265625\n",
      "    epoch          : 368\n",
      "    loss           : -228057.37354216506\n",
      "    val_loss       : -230371.81866306212\n",
      "Train Epoch: 369 [0/54000 (0%)] Loss: -248600.750000\n",
      "Train Epoch: 369 [1408/54000 (3%)] Loss: -249785.046875\n",
      "Train Epoch: 369 [2816/54000 (5%)] Loss: -221924.500000\n",
      "Train Epoch: 369 [4224/54000 (8%)] Loss: -222933.406250\n",
      "Train Epoch: 369 [5632/54000 (10%)] Loss: -222776.718750\n",
      "Train Epoch: 369 [7040/54000 (13%)] Loss: -230870.656250\n",
      "Train Epoch: 369 [8448/54000 (16%)] Loss: -222311.921875\n",
      "Train Epoch: 369 [9856/54000 (18%)] Loss: -230393.437500\n",
      "Train Epoch: 369 [11264/54000 (21%)] Loss: -221472.062500\n",
      "Train Epoch: 369 [12672/54000 (23%)] Loss: -228766.890625\n",
      "Train Epoch: 369 [14080/54000 (26%)] Loss: -250018.796875\n",
      "Train Epoch: 369 [15488/54000 (29%)] Loss: -249860.093750\n",
      "Train Epoch: 369 [16896/54000 (31%)] Loss: -222873.937500\n",
      "Train Epoch: 369 [18304/54000 (34%)] Loss: -222659.718750\n",
      "Train Epoch: 369 [19712/54000 (37%)] Loss: -223874.000000\n",
      "Train Epoch: 369 [21120/54000 (39%)] Loss: -231813.890625\n",
      "Train Epoch: 369 [22528/54000 (42%)] Loss: -232260.265625\n",
      "Train Epoch: 369 [23936/54000 (44%)] Loss: -249405.187500\n",
      "Train Epoch: 369 [25344/54000 (47%)] Loss: -221489.515625\n",
      "Train Epoch: 369 [26752/54000 (50%)] Loss: -222201.578125\n",
      "Train Epoch: 369 [28160/54000 (52%)] Loss: -222213.250000\n",
      "Train Epoch: 369 [29568/54000 (55%)] Loss: -222597.375000\n",
      "Train Epoch: 369 [30976/54000 (57%)] Loss: -232535.046875\n",
      "Train Epoch: 369 [32384/54000 (60%)] Loss: -250882.187500\n",
      "Train Epoch: 369 [33792/54000 (63%)] Loss: -230639.375000\n",
      "Train Epoch: 369 [35200/54000 (65%)] Loss: -225144.187500\n",
      "Train Epoch: 369 [36608/54000 (68%)] Loss: -231659.375000\n",
      "Train Epoch: 369 [38016/54000 (70%)] Loss: -232023.703125\n",
      "Train Epoch: 369 [39424/54000 (73%)] Loss: -217374.812500\n",
      "Train Epoch: 369 [40832/54000 (76%)] Loss: -230879.187500\n",
      "Train Epoch: 369 [42240/54000 (78%)] Loss: -232674.906250\n",
      "Train Epoch: 369 [43648/54000 (81%)] Loss: -247906.906250\n",
      "Train Epoch: 369 [45056/54000 (83%)] Loss: -219758.828125\n",
      "Train Epoch: 369 [46464/54000 (86%)] Loss: -224102.750000\n",
      "Train Epoch: 369 [47872/54000 (89%)] Loss: -225464.515625\n",
      "Train Epoch: 369 [49280/54000 (91%)] Loss: -250101.546875\n",
      "Train Epoch: 369 [50688/54000 (94%)] Loss: -223332.671875\n",
      "Train Epoch: 369 [52096/54000 (96%)] Loss: -231551.890625\n",
      "    epoch          : 369\n",
      "    loss           : -228127.72002093302\n",
      "    val_loss       : -230416.19323432737\n",
      "Train Epoch: 370 [0/54000 (0%)] Loss: -248825.781250\n",
      "Train Epoch: 370 [1408/54000 (3%)] Loss: -219306.187500\n",
      "Train Epoch: 370 [2816/54000 (5%)] Loss: -230469.437500\n",
      "Train Epoch: 370 [4224/54000 (8%)] Loss: -224645.468750\n",
      "Train Epoch: 370 [5632/54000 (10%)] Loss: -229895.031250\n",
      "Train Epoch: 370 [7040/54000 (13%)] Loss: -229393.640625\n",
      "Train Epoch: 370 [8448/54000 (16%)] Loss: -221481.593750\n",
      "Train Epoch: 370 [9856/54000 (18%)] Loss: -222003.515625\n",
      "Train Epoch: 370 [11264/54000 (21%)] Loss: -230533.734375\n",
      "Train Epoch: 370 [12672/54000 (23%)] Loss: -222080.890625\n",
      "Train Epoch: 370 [14080/54000 (26%)] Loss: -230171.187500\n",
      "Train Epoch: 370 [15488/54000 (29%)] Loss: -227146.906250\n",
      "Train Epoch: 370 [16896/54000 (31%)] Loss: -223436.609375\n",
      "Train Epoch: 370 [18304/54000 (34%)] Loss: -226614.515625\n",
      "Train Epoch: 370 [19712/54000 (37%)] Loss: -218144.281250\n",
      "Train Epoch: 370 [21120/54000 (39%)] Loss: -225384.468750\n",
      "Train Epoch: 370 [22528/54000 (42%)] Loss: -219251.593750\n",
      "Train Epoch: 370 [23936/54000 (44%)] Loss: -221481.171875\n",
      "Train Epoch: 370 [25344/54000 (47%)] Loss: -219029.625000\n",
      "Train Epoch: 370 [26752/54000 (50%)] Loss: -226835.625000\n",
      "Train Epoch: 370 [28160/54000 (52%)] Loss: -227209.906250\n",
      "Train Epoch: 370 [29568/54000 (55%)] Loss: -225476.671875\n",
      "Train Epoch: 370 [30976/54000 (57%)] Loss: -226704.859375\n",
      "Train Epoch: 370 [32384/54000 (60%)] Loss: -226306.593750\n",
      "Train Epoch: 370 [33792/54000 (63%)] Loss: -226431.875000\n",
      "Train Epoch: 370 [35200/54000 (65%)] Loss: -221762.171875\n",
      "Train Epoch: 370 [36608/54000 (68%)] Loss: -222182.781250\n",
      "Train Epoch: 370 [38016/54000 (70%)] Loss: -232564.750000\n",
      "Train Epoch: 370 [39424/54000 (73%)] Loss: -219258.703125\n",
      "Train Epoch: 370 [40832/54000 (76%)] Loss: -223328.500000\n",
      "Train Epoch: 370 [42240/54000 (78%)] Loss: -225408.093750\n",
      "Train Epoch: 370 [43648/54000 (81%)] Loss: -226673.718750\n",
      "Train Epoch: 370 [45056/54000 (83%)] Loss: -228819.843750\n",
      "Train Epoch: 370 [46464/54000 (86%)] Loss: -224746.328125\n",
      "Train Epoch: 370 [47872/54000 (89%)] Loss: -223410.343750\n",
      "Train Epoch: 370 [49280/54000 (91%)] Loss: -220579.125000\n",
      "Train Epoch: 370 [50688/54000 (94%)] Loss: -224449.890625\n",
      "Train Epoch: 370 [52096/54000 (96%)] Loss: -221533.468750\n",
      "    epoch          : 370\n",
      "    loss           : -228130.61778558613\n",
      "    val_loss       : -230403.82851205222\n",
      "Train Epoch: 371 [0/54000 (0%)] Loss: -247914.875000\n",
      "Train Epoch: 371 [1408/54000 (3%)] Loss: -223213.312500\n",
      "Train Epoch: 371 [2816/54000 (5%)] Loss: -222718.578125\n",
      "Train Epoch: 371 [4224/54000 (8%)] Loss: -220678.093750\n",
      "Train Epoch: 371 [5632/54000 (10%)] Loss: -227956.656250\n",
      "Train Epoch: 371 [7040/54000 (13%)] Loss: -249959.625000\n",
      "Train Epoch: 371 [8448/54000 (16%)] Loss: -226869.859375\n",
      "Train Epoch: 371 [9856/54000 (18%)] Loss: -231169.281250\n",
      "Train Epoch: 371 [11264/54000 (21%)] Loss: -220960.796875\n",
      "Train Epoch: 371 [12672/54000 (23%)] Loss: -231470.640625\n",
      "Train Epoch: 371 [14080/54000 (26%)] Loss: -227927.640625\n",
      "Train Epoch: 371 [15488/54000 (29%)] Loss: -227158.000000\n",
      "Train Epoch: 371 [16896/54000 (31%)] Loss: -223489.593750\n",
      "Train Epoch: 371 [18304/54000 (34%)] Loss: -231226.671875\n",
      "Train Epoch: 371 [19712/54000 (37%)] Loss: -216659.937500\n",
      "Train Epoch: 371 [21120/54000 (39%)] Loss: -222088.812500\n",
      "Train Epoch: 371 [22528/54000 (42%)] Loss: -231840.140625\n",
      "Train Epoch: 371 [23936/54000 (44%)] Loss: -249926.250000\n",
      "Train Epoch: 371 [25344/54000 (47%)] Loss: -227317.968750\n",
      "Train Epoch: 371 [26752/54000 (50%)] Loss: -222855.765625\n",
      "Train Epoch: 371 [28160/54000 (52%)] Loss: -232774.968750\n",
      "Train Epoch: 371 [29568/54000 (55%)] Loss: -223885.375000\n",
      "Train Epoch: 371 [30976/54000 (57%)] Loss: -223357.625000\n",
      "Train Epoch: 371 [32384/54000 (60%)] Loss: -249662.796875\n",
      "Train Epoch: 371 [33792/54000 (63%)] Loss: -224857.046875\n",
      "Train Epoch: 371 [35200/54000 (65%)] Loss: -216045.109375\n",
      "Train Epoch: 371 [36608/54000 (68%)] Loss: -232484.500000\n",
      "Train Epoch: 371 [38016/54000 (70%)] Loss: -220433.609375\n",
      "Train Epoch: 371 [39424/54000 (73%)] Loss: -218103.546875\n",
      "Train Epoch: 371 [40832/54000 (76%)] Loss: -232684.703125\n",
      "Train Epoch: 371 [42240/54000 (78%)] Loss: -232130.687500\n",
      "Train Epoch: 371 [43648/54000 (81%)] Loss: -249635.906250\n",
      "Train Epoch: 371 [45056/54000 (83%)] Loss: -226350.375000\n",
      "Train Epoch: 371 [46464/54000 (86%)] Loss: -229198.906250\n",
      "Train Epoch: 371 [47872/54000 (89%)] Loss: -222179.187500\n",
      "Train Epoch: 371 [49280/54000 (91%)] Loss: -230424.156250\n",
      "Train Epoch: 371 [50688/54000 (94%)] Loss: -249349.609375\n",
      "Train Epoch: 371 [52096/54000 (96%)] Loss: -231268.640625\n",
      "    epoch          : 371\n",
      "    loss           : -228098.85578648324\n",
      "    val_loss       : -230086.93358779536\n",
      "Train Epoch: 372 [0/54000 (0%)] Loss: -230770.421875\n",
      "Train Epoch: 372 [1408/54000 (3%)] Loss: -232082.250000\n",
      "Train Epoch: 372 [2816/54000 (5%)] Loss: -217963.890625\n",
      "Train Epoch: 372 [4224/54000 (8%)] Loss: -231834.171875\n",
      "Train Epoch: 372 [5632/54000 (10%)] Loss: -228435.640625\n",
      "Train Epoch: 372 [7040/54000 (13%)] Loss: -229067.359375\n",
      "Train Epoch: 372 [8448/54000 (16%)] Loss: -221940.875000\n",
      "Train Epoch: 372 [9856/54000 (18%)] Loss: -247888.562500\n",
      "Train Epoch: 372 [11264/54000 (21%)] Loss: -222508.625000\n",
      "Train Epoch: 372 [12672/54000 (23%)] Loss: -226436.250000\n",
      "Train Epoch: 372 [14080/54000 (26%)] Loss: -216246.390625\n",
      "Train Epoch: 372 [15488/54000 (29%)] Loss: -221808.812500\n",
      "Train Epoch: 372 [16896/54000 (31%)] Loss: -248326.921875\n",
      "Train Epoch: 372 [18304/54000 (34%)] Loss: -223395.625000\n",
      "Train Epoch: 372 [19712/54000 (37%)] Loss: -229669.781250\n",
      "Train Epoch: 372 [21120/54000 (39%)] Loss: -232683.578125\n",
      "Train Epoch: 372 [22528/54000 (42%)] Loss: -229816.718750\n",
      "Train Epoch: 372 [23936/54000 (44%)] Loss: -231417.015625\n",
      "Train Epoch: 372 [25344/54000 (47%)] Loss: -223767.031250\n",
      "Train Epoch: 372 [26752/54000 (50%)] Loss: -232324.500000\n",
      "Train Epoch: 372 [28160/54000 (52%)] Loss: -222385.671875\n",
      "Train Epoch: 372 [29568/54000 (55%)] Loss: -228014.703125\n",
      "Train Epoch: 372 [30976/54000 (57%)] Loss: -222810.031250\n",
      "Train Epoch: 372 [32384/54000 (60%)] Loss: -230481.750000\n",
      "Train Epoch: 372 [33792/54000 (63%)] Loss: -249292.125000\n",
      "Train Epoch: 372 [35200/54000 (65%)] Loss: -217512.156250\n",
      "Train Epoch: 372 [36608/54000 (68%)] Loss: -225842.125000\n",
      "Train Epoch: 372 [38016/54000 (70%)] Loss: -230514.687500\n",
      "Train Epoch: 372 [39424/54000 (73%)] Loss: -230222.093750\n",
      "Train Epoch: 372 [40832/54000 (76%)] Loss: -229920.812500\n",
      "Train Epoch: 372 [42240/54000 (78%)] Loss: -230959.343750\n",
      "Train Epoch: 372 [43648/54000 (81%)] Loss: -222272.281250\n",
      "Train Epoch: 372 [45056/54000 (83%)] Loss: -229405.687500\n",
      "Train Epoch: 372 [46464/54000 (86%)] Loss: -225065.984375\n",
      "Train Epoch: 372 [47872/54000 (89%)] Loss: -230232.875000\n",
      "Train Epoch: 372 [49280/54000 (91%)] Loss: -229217.750000\n",
      "Train Epoch: 372 [50688/54000 (94%)] Loss: -249637.875000\n",
      "Train Epoch: 372 [52096/54000 (96%)] Loss: -221615.062500\n",
      "    epoch          : 372\n",
      "    loss           : -227999.66376345695\n",
      "    val_loss       : -229842.92648389863\n",
      "Train Epoch: 373 [0/54000 (0%)] Loss: -221794.781250\n",
      "Train Epoch: 373 [1408/54000 (3%)] Loss: -249634.218750\n",
      "Train Epoch: 373 [2816/54000 (5%)] Loss: -231163.718750\n",
      "Train Epoch: 373 [4224/54000 (8%)] Loss: -222567.156250\n",
      "Train Epoch: 373 [5632/54000 (10%)] Loss: -224303.843750\n",
      "Train Epoch: 373 [7040/54000 (13%)] Loss: -222847.437500\n",
      "Train Epoch: 373 [8448/54000 (16%)] Loss: -224378.812500\n",
      "Train Epoch: 373 [9856/54000 (18%)] Loss: -224480.468750\n",
      "Train Epoch: 373 [11264/54000 (21%)] Loss: -230319.390625\n",
      "Train Epoch: 373 [12672/54000 (23%)] Loss: -228731.781250\n",
      "Train Epoch: 373 [14080/54000 (26%)] Loss: -247895.640625\n",
      "Train Epoch: 373 [15488/54000 (29%)] Loss: -233074.671875\n",
      "Train Epoch: 373 [16896/54000 (31%)] Loss: -232424.968750\n",
      "Train Epoch: 373 [18304/54000 (34%)] Loss: -223982.093750\n",
      "Train Epoch: 373 [19712/54000 (37%)] Loss: -224540.203125\n",
      "Train Epoch: 373 [21120/54000 (39%)] Loss: -225715.593750\n",
      "Train Epoch: 373 [22528/54000 (42%)] Loss: -223656.500000\n",
      "Train Epoch: 373 [23936/54000 (44%)] Loss: -221581.875000\n",
      "Train Epoch: 373 [25344/54000 (47%)] Loss: -231294.750000\n",
      "Train Epoch: 373 [26752/54000 (50%)] Loss: -231088.750000\n",
      "Train Epoch: 373 [28160/54000 (52%)] Loss: -223675.093750\n",
      "Train Epoch: 373 [29568/54000 (55%)] Loss: -229095.687500\n",
      "Train Epoch: 373 [30976/54000 (57%)] Loss: -250582.375000\n",
      "Train Epoch: 373 [32384/54000 (60%)] Loss: -221964.546875\n",
      "Train Epoch: 373 [33792/54000 (63%)] Loss: -229194.828125\n",
      "Train Epoch: 373 [35200/54000 (65%)] Loss: -226908.687500\n",
      "Train Epoch: 373 [36608/54000 (68%)] Loss: -221422.953125\n",
      "Train Epoch: 373 [38016/54000 (70%)] Loss: -249704.031250\n",
      "Train Epoch: 373 [39424/54000 (73%)] Loss: -231772.390625\n",
      "Train Epoch: 373 [40832/54000 (76%)] Loss: -249799.718750\n",
      "Train Epoch: 373 [42240/54000 (78%)] Loss: -226059.250000\n",
      "Train Epoch: 373 [43648/54000 (81%)] Loss: -228824.906250\n",
      "Train Epoch: 373 [45056/54000 (83%)] Loss: -230996.234375\n",
      "Train Epoch: 373 [46464/54000 (86%)] Loss: -224703.625000\n",
      "Train Epoch: 373 [47872/54000 (89%)] Loss: -228007.125000\n",
      "Train Epoch: 373 [49280/54000 (91%)] Loss: -229673.468750\n",
      "Train Epoch: 373 [50688/54000 (94%)] Loss: -250443.093750\n",
      "Train Epoch: 373 [52096/54000 (96%)] Loss: -231764.093750\n",
      "    epoch          : 373\n",
      "    loss           : -228121.76371860047\n",
      "    val_loss       : -230132.47641363376\n",
      "Train Epoch: 374 [0/54000 (0%)] Loss: -222730.140625\n",
      "Train Epoch: 374 [1408/54000 (3%)] Loss: -248164.453125\n",
      "Train Epoch: 374 [2816/54000 (5%)] Loss: -213568.796875\n",
      "Train Epoch: 374 [4224/54000 (8%)] Loss: -231712.343750\n",
      "Train Epoch: 374 [5632/54000 (10%)] Loss: -222237.375000\n",
      "Train Epoch: 374 [7040/54000 (13%)] Loss: -224720.875000\n",
      "Train Epoch: 374 [8448/54000 (16%)] Loss: -230016.734375\n",
      "Train Epoch: 374 [9856/54000 (18%)] Loss: -221732.593750\n",
      "Train Epoch: 374 [11264/54000 (21%)] Loss: -228334.890625\n",
      "Train Epoch: 374 [12672/54000 (23%)] Loss: -223673.859375\n",
      "Train Epoch: 374 [14080/54000 (26%)] Loss: -229001.953125\n",
      "Train Epoch: 374 [15488/54000 (29%)] Loss: -229827.734375\n",
      "Train Epoch: 374 [16896/54000 (31%)] Loss: -231314.468750\n",
      "Train Epoch: 374 [18304/54000 (34%)] Loss: -248674.515625\n",
      "Train Epoch: 374 [19712/54000 (37%)] Loss: -219380.531250\n",
      "Train Epoch: 374 [21120/54000 (39%)] Loss: -216312.031250\n",
      "Train Epoch: 374 [22528/54000 (42%)] Loss: -227996.125000\n",
      "Train Epoch: 374 [23936/54000 (44%)] Loss: -222890.984375\n",
      "Train Epoch: 374 [25344/54000 (47%)] Loss: -224422.218750\n",
      "Train Epoch: 374 [26752/54000 (50%)] Loss: -219136.890625\n",
      "Train Epoch: 374 [28160/54000 (52%)] Loss: -218077.812500\n",
      "Train Epoch: 374 [29568/54000 (55%)] Loss: -219206.531250\n",
      "Train Epoch: 374 [30976/54000 (57%)] Loss: -231099.875000\n",
      "Train Epoch: 374 [32384/54000 (60%)] Loss: -223535.328125\n",
      "Train Epoch: 374 [33792/54000 (63%)] Loss: -224585.421875\n",
      "Train Epoch: 374 [35200/54000 (65%)] Loss: -223765.656250\n",
      "Train Epoch: 374 [36608/54000 (68%)] Loss: -250116.468750\n",
      "Train Epoch: 374 [38016/54000 (70%)] Loss: -249662.828125\n",
      "Train Epoch: 374 [39424/54000 (73%)] Loss: -224931.968750\n",
      "Train Epoch: 374 [40832/54000 (76%)] Loss: -230603.156250\n",
      "Train Epoch: 374 [42240/54000 (78%)] Loss: -218829.968750\n",
      "Train Epoch: 374 [43648/54000 (81%)] Loss: -221797.000000\n",
      "Train Epoch: 374 [45056/54000 (83%)] Loss: -230547.343750\n",
      "Train Epoch: 374 [46464/54000 (86%)] Loss: -219735.000000\n",
      "Train Epoch: 374 [47872/54000 (89%)] Loss: -215620.703125\n",
      "Train Epoch: 374 [49280/54000 (91%)] Loss: -220987.609375\n",
      "Train Epoch: 374 [50688/54000 (94%)] Loss: -219571.937500\n",
      "Train Epoch: 374 [52096/54000 (96%)] Loss: -231794.828125\n",
      "    epoch          : 374\n",
      "    loss           : -227925.86987888755\n",
      "    val_loss       : -230515.1968428449\n",
      "Train Epoch: 375 [0/54000 (0%)] Loss: -245633.296875\n",
      "Train Epoch: 375 [1408/54000 (3%)] Loss: -221664.500000\n",
      "Train Epoch: 375 [2816/54000 (5%)] Loss: -228955.015625\n",
      "Train Epoch: 375 [4224/54000 (8%)] Loss: -226287.484375\n",
      "Train Epoch: 375 [5632/54000 (10%)] Loss: -221165.250000\n",
      "Train Epoch: 375 [7040/54000 (13%)] Loss: -249595.843750\n",
      "Train Epoch: 375 [8448/54000 (16%)] Loss: -224441.218750\n",
      "Train Epoch: 375 [9856/54000 (18%)] Loss: -220380.562500\n",
      "Train Epoch: 375 [11264/54000 (21%)] Loss: -228356.296875\n",
      "Train Epoch: 375 [12672/54000 (23%)] Loss: -226005.453125\n",
      "Train Epoch: 375 [14080/54000 (26%)] Loss: -218523.734375\n",
      "Train Epoch: 375 [15488/54000 (29%)] Loss: -220013.578125\n",
      "Train Epoch: 375 [16896/54000 (31%)] Loss: -221356.296875\n",
      "Train Epoch: 375 [18304/54000 (34%)] Loss: -217491.359375\n",
      "Train Epoch: 375 [19712/54000 (37%)] Loss: -249614.578125\n",
      "Train Epoch: 375 [21120/54000 (39%)] Loss: -221610.406250\n",
      "Train Epoch: 375 [22528/54000 (42%)] Loss: -228976.671875\n",
      "Train Epoch: 375 [23936/54000 (44%)] Loss: -223626.718750\n",
      "Train Epoch: 375 [25344/54000 (47%)] Loss: -231212.546875\n",
      "Train Epoch: 375 [26752/54000 (50%)] Loss: -247378.781250\n",
      "Train Epoch: 375 [28160/54000 (52%)] Loss: -220597.859375\n",
      "Train Epoch: 375 [29568/54000 (55%)] Loss: -226924.765625\n",
      "Train Epoch: 375 [30976/54000 (57%)] Loss: -232230.609375\n",
      "Train Epoch: 375 [32384/54000 (60%)] Loss: -224027.250000\n",
      "Train Epoch: 375 [33792/54000 (63%)] Loss: -227128.781250\n",
      "Train Epoch: 375 [35200/54000 (65%)] Loss: -227306.203125\n",
      "Train Epoch: 375 [36608/54000 (68%)] Loss: -221493.578125\n",
      "Train Epoch: 375 [38016/54000 (70%)] Loss: -218242.390625\n",
      "Train Epoch: 375 [39424/54000 (73%)] Loss: -248797.109375\n",
      "Train Epoch: 375 [40832/54000 (76%)] Loss: -231938.500000\n",
      "Train Epoch: 375 [42240/54000 (78%)] Loss: -229066.187500\n",
      "Train Epoch: 375 [43648/54000 (81%)] Loss: -229832.203125\n",
      "Train Epoch: 375 [45056/54000 (83%)] Loss: -249420.765625\n",
      "Train Epoch: 375 [46464/54000 (86%)] Loss: -225376.750000\n",
      "Train Epoch: 375 [47872/54000 (89%)] Loss: -225898.156250\n",
      "Train Epoch: 375 [49280/54000 (91%)] Loss: -221996.953125\n",
      "Train Epoch: 375 [50688/54000 (94%)] Loss: -220930.546875\n",
      "Train Epoch: 375 [52096/54000 (96%)] Loss: -230016.375000\n",
      "    epoch          : 375\n",
      "    loss           : -228239.68245364833\n",
      "    val_loss       : -229879.41179973324\n",
      "Train Epoch: 376 [0/54000 (0%)] Loss: -230405.562500\n",
      "Train Epoch: 376 [1408/54000 (3%)] Loss: -231314.031250\n",
      "Train Epoch: 376 [2816/54000 (5%)] Loss: -230532.515625\n",
      "Train Epoch: 376 [4224/54000 (8%)] Loss: -224506.031250\n",
      "Train Epoch: 376 [5632/54000 (10%)] Loss: -221328.890625\n",
      "Train Epoch: 376 [7040/54000 (13%)] Loss: -225862.156250\n",
      "Train Epoch: 376 [8448/54000 (16%)] Loss: -222432.093750\n",
      "Train Epoch: 376 [9856/54000 (18%)] Loss: -213006.765625\n",
      "Train Epoch: 376 [11264/54000 (21%)] Loss: -231401.437500\n",
      "Train Epoch: 376 [12672/54000 (23%)] Loss: -224103.125000\n",
      "Train Epoch: 376 [14080/54000 (26%)] Loss: -229290.015625\n",
      "Train Epoch: 376 [15488/54000 (29%)] Loss: -249765.984375\n",
      "Train Epoch: 376 [16896/54000 (31%)] Loss: -227126.531250\n",
      "Train Epoch: 376 [18304/54000 (34%)] Loss: -221846.000000\n",
      "Train Epoch: 376 [19712/54000 (37%)] Loss: -221024.687500\n",
      "Train Epoch: 376 [21120/54000 (39%)] Loss: -221789.171875\n",
      "Train Epoch: 376 [22528/54000 (42%)] Loss: -232168.828125\n",
      "Train Epoch: 376 [23936/54000 (44%)] Loss: -223527.203125\n",
      "Train Epoch: 376 [25344/54000 (47%)] Loss: -223417.500000\n",
      "Train Epoch: 376 [26752/54000 (50%)] Loss: -221680.515625\n",
      "Train Epoch: 376 [28160/54000 (52%)] Loss: -218805.140625\n",
      "Train Epoch: 376 [29568/54000 (55%)] Loss: -231202.078125\n",
      "Train Epoch: 376 [30976/54000 (57%)] Loss: -230178.671875\n",
      "Train Epoch: 376 [32384/54000 (60%)] Loss: -227016.468750\n",
      "Train Epoch: 376 [33792/54000 (63%)] Loss: -250383.937500\n",
      "Train Epoch: 376 [35200/54000 (65%)] Loss: -222763.375000\n",
      "Train Epoch: 376 [36608/54000 (68%)] Loss: -219471.031250\n",
      "Train Epoch: 376 [38016/54000 (70%)] Loss: -230599.578125\n",
      "Train Epoch: 376 [39424/54000 (73%)] Loss: -221700.156250\n",
      "Train Epoch: 376 [40832/54000 (76%)] Loss: -230652.578125\n",
      "Train Epoch: 376 [42240/54000 (78%)] Loss: -226789.656250\n",
      "Train Epoch: 376 [43648/54000 (81%)] Loss: -231965.859375\n",
      "Train Epoch: 376 [45056/54000 (83%)] Loss: -231334.484375\n",
      "Train Epoch: 376 [46464/54000 (86%)] Loss: -226859.578125\n",
      "Train Epoch: 376 [47872/54000 (89%)] Loss: -225142.562500\n",
      "Train Epoch: 376 [49280/54000 (91%)] Loss: -220175.000000\n",
      "Train Epoch: 376 [50688/54000 (94%)] Loss: -222556.203125\n",
      "Train Epoch: 376 [52096/54000 (96%)] Loss: -230241.500000\n",
      "    epoch          : 376\n",
      "    loss           : -228143.43839712918\n",
      "    val_loss       : -230228.9824456936\n",
      "Train Epoch: 377 [0/54000 (0%)] Loss: -249362.000000\n",
      "Train Epoch: 377 [1408/54000 (3%)] Loss: -231182.843750\n",
      "Train Epoch: 377 [2816/54000 (5%)] Loss: -231663.406250\n",
      "Train Epoch: 377 [4224/54000 (8%)] Loss: -228457.265625\n",
      "Train Epoch: 377 [5632/54000 (10%)] Loss: -227507.687500\n",
      "Train Epoch: 377 [7040/54000 (13%)] Loss: -227015.546875\n",
      "Train Epoch: 377 [8448/54000 (16%)] Loss: -231065.218750\n",
      "Train Epoch: 377 [9856/54000 (18%)] Loss: -226202.140625\n",
      "Train Epoch: 377 [11264/54000 (21%)] Loss: -218779.937500\n",
      "Train Epoch: 377 [12672/54000 (23%)] Loss: -217102.562500\n",
      "Train Epoch: 377 [14080/54000 (26%)] Loss: -249346.062500\n",
      "Train Epoch: 377 [15488/54000 (29%)] Loss: -232145.593750\n",
      "Train Epoch: 377 [16896/54000 (31%)] Loss: -229465.656250\n",
      "Train Epoch: 377 [18304/54000 (34%)] Loss: -218649.734375\n",
      "Train Epoch: 377 [19712/54000 (37%)] Loss: -219538.765625\n",
      "Train Epoch: 377 [21120/54000 (39%)] Loss: -218176.828125\n",
      "Train Epoch: 377 [22528/54000 (42%)] Loss: -216982.109375\n",
      "Train Epoch: 377 [23936/54000 (44%)] Loss: -220991.500000\n",
      "Train Epoch: 377 [25344/54000 (47%)] Loss: -227414.062500\n",
      "Train Epoch: 377 [26752/54000 (50%)] Loss: -225286.562500\n",
      "Train Epoch: 377 [28160/54000 (52%)] Loss: -231298.468750\n",
      "Train Epoch: 377 [29568/54000 (55%)] Loss: -225490.828125\n",
      "Train Epoch: 377 [30976/54000 (57%)] Loss: -231762.656250\n",
      "Train Epoch: 377 [32384/54000 (60%)] Loss: -250425.515625\n",
      "Train Epoch: 377 [33792/54000 (63%)] Loss: -224943.843750\n",
      "Train Epoch: 377 [35200/54000 (65%)] Loss: -230514.859375\n",
      "Train Epoch: 377 [36608/54000 (68%)] Loss: -223678.281250\n",
      "Train Epoch: 377 [38016/54000 (70%)] Loss: -223978.406250\n",
      "Train Epoch: 377 [39424/54000 (73%)] Loss: -225937.156250\n",
      "Train Epoch: 377 [40832/54000 (76%)] Loss: -229089.953125\n",
      "Train Epoch: 377 [42240/54000 (78%)] Loss: -224095.703125\n",
      "Train Epoch: 377 [43648/54000 (81%)] Loss: -225168.843750\n",
      "Train Epoch: 377 [45056/54000 (83%)] Loss: -230549.593750\n",
      "Train Epoch: 377 [46464/54000 (86%)] Loss: -227237.109375\n",
      "Train Epoch: 377 [47872/54000 (89%)] Loss: -221147.734375\n",
      "Train Epoch: 377 [49280/54000 (91%)] Loss: -222659.937500\n",
      "Train Epoch: 377 [50688/54000 (94%)] Loss: -250143.906250\n",
      "Train Epoch: 377 [52096/54000 (96%)] Loss: -231720.921875\n",
      "    epoch          : 377\n",
      "    loss           : -228181.74659838516\n",
      "    val_loss       : -230262.42255978467\n",
      "Train Epoch: 378 [0/54000 (0%)] Loss: -249094.312500\n",
      "Train Epoch: 378 [1408/54000 (3%)] Loss: -224373.093750\n",
      "Train Epoch: 378 [2816/54000 (5%)] Loss: -222409.609375\n",
      "Train Epoch: 378 [4224/54000 (8%)] Loss: -221644.062500\n",
      "Train Epoch: 378 [5632/54000 (10%)] Loss: -220241.937500\n",
      "Train Epoch: 378 [7040/54000 (13%)] Loss: -215370.968750\n",
      "Train Epoch: 378 [8448/54000 (16%)] Loss: -220911.234375\n",
      "Train Epoch: 378 [9856/54000 (18%)] Loss: -219356.578125\n",
      "Train Epoch: 378 [11264/54000 (21%)] Loss: -230085.625000\n",
      "Train Epoch: 378 [12672/54000 (23%)] Loss: -220774.562500\n",
      "Train Epoch: 378 [14080/54000 (26%)] Loss: -226587.390625\n",
      "Train Epoch: 378 [15488/54000 (29%)] Loss: -232188.875000\n",
      "Train Epoch: 378 [16896/54000 (31%)] Loss: -251138.812500\n",
      "Train Epoch: 378 [18304/54000 (34%)] Loss: -220458.265625\n",
      "Train Epoch: 378 [19712/54000 (37%)] Loss: -217644.859375\n",
      "Train Epoch: 378 [21120/54000 (39%)] Loss: -216727.468750\n",
      "Train Epoch: 378 [22528/54000 (42%)] Loss: -231516.250000\n",
      "Train Epoch: 378 [23936/54000 (44%)] Loss: -224275.437500\n",
      "Train Epoch: 378 [25344/54000 (47%)] Loss: -248920.328125\n",
      "Train Epoch: 378 [26752/54000 (50%)] Loss: -225662.937500\n",
      "Train Epoch: 378 [28160/54000 (52%)] Loss: -228170.703125\n",
      "Train Epoch: 378 [29568/54000 (55%)] Loss: -229428.640625\n",
      "Train Epoch: 378 [30976/54000 (57%)] Loss: -227861.468750\n",
      "Train Epoch: 378 [32384/54000 (60%)] Loss: -232534.937500\n",
      "Train Epoch: 378 [33792/54000 (63%)] Loss: -230950.078125\n",
      "Train Epoch: 378 [35200/54000 (65%)] Loss: -249297.312500\n",
      "Train Epoch: 378 [36608/54000 (68%)] Loss: -219566.937500\n",
      "Train Epoch: 378 [38016/54000 (70%)] Loss: -224936.546875\n",
      "Train Epoch: 378 [39424/54000 (73%)] Loss: -231439.328125\n",
      "Train Epoch: 378 [40832/54000 (76%)] Loss: -231040.046875\n",
      "Train Epoch: 378 [42240/54000 (78%)] Loss: -231254.078125\n",
      "Train Epoch: 378 [43648/54000 (81%)] Loss: -221750.203125\n",
      "Train Epoch: 378 [45056/54000 (83%)] Loss: -226973.546875\n",
      "Train Epoch: 378 [46464/54000 (86%)] Loss: -250963.437500\n",
      "Train Epoch: 378 [47872/54000 (89%)] Loss: -223162.218750\n",
      "Train Epoch: 378 [49280/54000 (91%)] Loss: -222169.984375\n",
      "Train Epoch: 378 [50688/54000 (94%)] Loss: -223090.359375\n",
      "Train Epoch: 378 [52096/54000 (96%)] Loss: -230244.140625\n",
      "    epoch          : 378\n",
      "    loss           : -228284.18693929425\n",
      "    val_loss       : -230551.2427889196\n",
      "Train Epoch: 379 [0/54000 (0%)] Loss: -248957.796875\n",
      "Train Epoch: 379 [1408/54000 (3%)] Loss: -225098.796875\n",
      "Train Epoch: 379 [2816/54000 (5%)] Loss: -218689.203125\n",
      "Train Epoch: 379 [4224/54000 (8%)] Loss: -222364.078125\n",
      "Train Epoch: 379 [5632/54000 (10%)] Loss: -226492.312500\n",
      "Train Epoch: 379 [7040/54000 (13%)] Loss: -228752.703125\n",
      "Train Epoch: 379 [8448/54000 (16%)] Loss: -230022.093750\n",
      "Train Epoch: 379 [9856/54000 (18%)] Loss: -221993.375000\n",
      "Train Epoch: 379 [11264/54000 (21%)] Loss: -223738.359375\n",
      "Train Epoch: 379 [12672/54000 (23%)] Loss: -229589.437500\n",
      "Train Epoch: 379 [14080/54000 (26%)] Loss: -226457.828125\n",
      "Train Epoch: 379 [15488/54000 (29%)] Loss: -222135.859375\n",
      "Train Epoch: 379 [16896/54000 (31%)] Loss: -228455.359375\n",
      "Train Epoch: 379 [18304/54000 (34%)] Loss: -250424.718750\n",
      "Train Epoch: 379 [19712/54000 (37%)] Loss: -218449.500000\n",
      "Train Epoch: 379 [21120/54000 (39%)] Loss: -221183.984375\n",
      "Train Epoch: 379 [22528/54000 (42%)] Loss: -230324.093750\n",
      "Train Epoch: 379 [23936/54000 (44%)] Loss: -250122.984375\n",
      "Train Epoch: 379 [25344/54000 (47%)] Loss: -231818.234375\n",
      "Train Epoch: 379 [26752/54000 (50%)] Loss: -218998.843750\n",
      "Train Epoch: 379 [28160/54000 (52%)] Loss: -226439.500000\n",
      "Train Epoch: 379 [29568/54000 (55%)] Loss: -224337.968750\n",
      "Train Epoch: 379 [30976/54000 (57%)] Loss: -219973.984375\n",
      "Train Epoch: 379 [32384/54000 (60%)] Loss: -248477.890625\n",
      "Train Epoch: 379 [33792/54000 (63%)] Loss: -220547.875000\n",
      "Train Epoch: 379 [35200/54000 (65%)] Loss: -220107.687500\n",
      "Train Epoch: 379 [36608/54000 (68%)] Loss: -220648.750000\n",
      "Train Epoch: 379 [38016/54000 (70%)] Loss: -230512.531250\n",
      "Train Epoch: 379 [39424/54000 (73%)] Loss: -232069.750000\n",
      "Train Epoch: 379 [40832/54000 (76%)] Loss: -230491.546875\n",
      "Train Epoch: 379 [42240/54000 (78%)] Loss: -223913.640625\n",
      "Train Epoch: 379 [43648/54000 (81%)] Loss: -225203.437500\n",
      "Train Epoch: 379 [45056/54000 (83%)] Loss: -218300.953125\n",
      "Train Epoch: 379 [46464/54000 (86%)] Loss: -220285.750000\n",
      "Train Epoch: 379 [47872/54000 (89%)] Loss: -222449.500000\n",
      "Train Epoch: 379 [49280/54000 (91%)] Loss: -231710.531250\n",
      "Train Epoch: 379 [50688/54000 (94%)] Loss: -230780.281250\n",
      "Train Epoch: 379 [52096/54000 (96%)] Loss: -229988.875000\n",
      "    epoch          : 379\n",
      "    loss           : -228147.78745514355\n",
      "    val_loss       : -230154.49780868902\n",
      "Train Epoch: 380 [0/54000 (0%)] Loss: -247914.781250\n",
      "Train Epoch: 380 [1408/54000 (3%)] Loss: -225414.062500\n",
      "Train Epoch: 380 [2816/54000 (5%)] Loss: -230791.500000\n",
      "Train Epoch: 380 [4224/54000 (8%)] Loss: -226386.031250\n",
      "Train Epoch: 380 [5632/54000 (10%)] Loss: -224663.734375\n",
      "Train Epoch: 380 [7040/54000 (13%)] Loss: -232140.718750\n",
      "Train Epoch: 380 [8448/54000 (16%)] Loss: -223110.921875\n",
      "Train Epoch: 380 [9856/54000 (18%)] Loss: -222775.062500\n",
      "Train Epoch: 380 [11264/54000 (21%)] Loss: -219905.843750\n",
      "Train Epoch: 380 [12672/54000 (23%)] Loss: -222010.468750\n",
      "Train Epoch: 380 [14080/54000 (26%)] Loss: -229662.843750\n",
      "Train Epoch: 380 [15488/54000 (29%)] Loss: -232036.937500\n",
      "Train Epoch: 380 [16896/54000 (31%)] Loss: -222575.687500\n",
      "Train Epoch: 380 [18304/54000 (34%)] Loss: -220299.750000\n",
      "Train Epoch: 380 [19712/54000 (37%)] Loss: -228619.718750\n",
      "Train Epoch: 380 [21120/54000 (39%)] Loss: -233177.000000\n",
      "Train Epoch: 380 [22528/54000 (42%)] Loss: -232500.078125\n",
      "Train Epoch: 380 [23936/54000 (44%)] Loss: -250215.875000\n",
      "Train Epoch: 380 [25344/54000 (47%)] Loss: -228026.937500\n",
      "Train Epoch: 380 [26752/54000 (50%)] Loss: -228341.859375\n",
      "Train Epoch: 380 [28160/54000 (52%)] Loss: -219462.593750\n",
      "Train Epoch: 380 [29568/54000 (55%)] Loss: -222210.328125\n",
      "Train Epoch: 380 [30976/54000 (57%)] Loss: -224930.734375\n",
      "Train Epoch: 380 [32384/54000 (60%)] Loss: -231984.718750\n",
      "Train Epoch: 380 [33792/54000 (63%)] Loss: -230654.468750\n",
      "Train Epoch: 380 [35200/54000 (65%)] Loss: -225056.687500\n",
      "Train Epoch: 380 [36608/54000 (68%)] Loss: -248854.062500\n",
      "Train Epoch: 380 [38016/54000 (70%)] Loss: -228455.281250\n",
      "Train Epoch: 380 [39424/54000 (73%)] Loss: -220395.750000\n",
      "Train Epoch: 380 [40832/54000 (76%)] Loss: -230779.984375\n",
      "Train Epoch: 380 [42240/54000 (78%)] Loss: -224355.390625\n",
      "Train Epoch: 380 [43648/54000 (81%)] Loss: -220392.218750\n",
      "Train Epoch: 380 [45056/54000 (83%)] Loss: -225510.062500\n",
      "Train Epoch: 380 [46464/54000 (86%)] Loss: -224195.140625\n",
      "Train Epoch: 380 [47872/54000 (89%)] Loss: -225311.531250\n",
      "Train Epoch: 380 [49280/54000 (91%)] Loss: -223104.093750\n",
      "Train Epoch: 380 [50688/54000 (94%)] Loss: -224802.593750\n",
      "Train Epoch: 380 [52096/54000 (96%)] Loss: -221167.703125\n",
      "    epoch          : 380\n",
      "    loss           : -228143.17064144736\n",
      "    val_loss       : -230377.04981064214\n",
      "Train Epoch: 381 [0/54000 (0%)] Loss: -249812.875000\n",
      "Train Epoch: 381 [1408/54000 (3%)] Loss: -233085.156250\n",
      "Train Epoch: 381 [2816/54000 (5%)] Loss: -222162.375000\n",
      "Train Epoch: 381 [4224/54000 (8%)] Loss: -224924.812500\n",
      "Train Epoch: 381 [5632/54000 (10%)] Loss: -223524.484375\n",
      "Train Epoch: 381 [7040/54000 (13%)] Loss: -231877.625000\n",
      "Train Epoch: 381 [8448/54000 (16%)] Loss: -231464.718750\n",
      "Train Epoch: 381 [9856/54000 (18%)] Loss: -221090.265625\n",
      "Train Epoch: 381 [11264/54000 (21%)] Loss: -226703.156250\n",
      "Train Epoch: 381 [12672/54000 (23%)] Loss: -249883.765625\n",
      "Train Epoch: 381 [14080/54000 (26%)] Loss: -223348.437500\n",
      "Train Epoch: 381 [15488/54000 (29%)] Loss: -232103.250000\n",
      "Train Epoch: 381 [16896/54000 (31%)] Loss: -223707.156250\n",
      "Train Epoch: 381 [18304/54000 (34%)] Loss: -221657.234375\n",
      "Train Epoch: 381 [19712/54000 (37%)] Loss: -231432.234375\n",
      "Train Epoch: 381 [21120/54000 (39%)] Loss: -222762.687500\n",
      "Train Epoch: 381 [22528/54000 (42%)] Loss: -250250.437500\n",
      "Train Epoch: 381 [23936/54000 (44%)] Loss: -226720.500000\n",
      "Train Epoch: 381 [25344/54000 (47%)] Loss: -222825.421875\n",
      "Train Epoch: 381 [26752/54000 (50%)] Loss: -232475.218750\n",
      "Train Epoch: 381 [28160/54000 (52%)] Loss: -229766.468750\n",
      "Train Epoch: 381 [29568/54000 (55%)] Loss: -220897.812500\n",
      "Train Epoch: 381 [30976/54000 (57%)] Loss: -220666.640625\n",
      "Train Epoch: 381 [32384/54000 (60%)] Loss: -227587.750000\n",
      "Train Epoch: 381 [33792/54000 (63%)] Loss: -250232.843750\n",
      "Train Epoch: 381 [35200/54000 (65%)] Loss: -225799.125000\n",
      "Train Epoch: 381 [36608/54000 (68%)] Loss: -231509.343750\n",
      "Train Epoch: 381 [38016/54000 (70%)] Loss: -249575.218750\n",
      "Train Epoch: 381 [39424/54000 (73%)] Loss: -250829.312500\n",
      "Train Epoch: 381 [40832/54000 (76%)] Loss: -232205.968750\n",
      "Train Epoch: 381 [42240/54000 (78%)] Loss: -223483.984375\n",
      "Train Epoch: 381 [43648/54000 (81%)] Loss: -225044.796875\n",
      "Train Epoch: 381 [45056/54000 (83%)] Loss: -230012.515625\n",
      "Train Epoch: 381 [46464/54000 (86%)] Loss: -226551.000000\n",
      "Train Epoch: 381 [47872/54000 (89%)] Loss: -230373.125000\n",
      "Train Epoch: 381 [49280/54000 (91%)] Loss: -220211.531250\n",
      "Train Epoch: 381 [50688/54000 (94%)] Loss: -247733.656250\n",
      "Train Epoch: 381 [52096/54000 (96%)] Loss: -231173.343750\n",
      "    epoch          : 381\n",
      "    loss           : -228304.60182416267\n",
      "    val_loss       : -230404.38490258192\n",
      "Train Epoch: 382 [0/54000 (0%)] Loss: -230857.984375\n",
      "Train Epoch: 382 [1408/54000 (3%)] Loss: -227410.390625\n",
      "Train Epoch: 382 [2816/54000 (5%)] Loss: -230586.765625\n",
      "Train Epoch: 382 [4224/54000 (8%)] Loss: -220716.843750\n",
      "Train Epoch: 382 [5632/54000 (10%)] Loss: -250376.828125\n",
      "Train Epoch: 382 [7040/54000 (13%)] Loss: -219953.421875\n",
      "Train Epoch: 382 [8448/54000 (16%)] Loss: -218732.593750\n",
      "Train Epoch: 382 [9856/54000 (18%)] Loss: -218295.281250\n",
      "Train Epoch: 382 [11264/54000 (21%)] Loss: -221998.812500\n",
      "Train Epoch: 382 [12672/54000 (23%)] Loss: -222120.218750\n",
      "Train Epoch: 382 [14080/54000 (26%)] Loss: -231036.718750\n",
      "Train Epoch: 382 [15488/54000 (29%)] Loss: -225771.281250\n",
      "Train Epoch: 382 [16896/54000 (31%)] Loss: -222314.984375\n",
      "Train Epoch: 382 [18304/54000 (34%)] Loss: -218124.750000\n",
      "Train Epoch: 382 [19712/54000 (37%)] Loss: -249145.562500\n",
      "Train Epoch: 382 [21120/54000 (39%)] Loss: -233231.578125\n",
      "Train Epoch: 382 [22528/54000 (42%)] Loss: -232304.750000\n",
      "Train Epoch: 382 [23936/54000 (44%)] Loss: -219536.359375\n",
      "Train Epoch: 382 [25344/54000 (47%)] Loss: -216295.265625\n",
      "Train Epoch: 382 [26752/54000 (50%)] Loss: -219046.078125\n",
      "Train Epoch: 382 [28160/54000 (52%)] Loss: -217574.359375\n",
      "Train Epoch: 382 [29568/54000 (55%)] Loss: -231171.390625\n",
      "Train Epoch: 382 [30976/54000 (57%)] Loss: -230901.531250\n",
      "Train Epoch: 382 [32384/54000 (60%)] Loss: -232013.609375\n",
      "Train Epoch: 382 [33792/54000 (63%)] Loss: -227322.281250\n",
      "Train Epoch: 382 [35200/54000 (65%)] Loss: -221097.062500\n",
      "Train Epoch: 382 [36608/54000 (68%)] Loss: -217742.250000\n",
      "Train Epoch: 382 [38016/54000 (70%)] Loss: -221931.187500\n",
      "Train Epoch: 382 [39424/54000 (73%)] Loss: -218866.687500\n",
      "Train Epoch: 382 [40832/54000 (76%)] Loss: -230356.218750\n",
      "Train Epoch: 382 [42240/54000 (78%)] Loss: -217280.421875\n",
      "Train Epoch: 382 [43648/54000 (81%)] Loss: -249389.125000\n",
      "Train Epoch: 382 [45056/54000 (83%)] Loss: -227407.421875\n",
      "Train Epoch: 382 [46464/54000 (86%)] Loss: -226673.843750\n",
      "Train Epoch: 382 [47872/54000 (89%)] Loss: -222176.218750\n",
      "Train Epoch: 382 [49280/54000 (91%)] Loss: -221988.078125\n",
      "Train Epoch: 382 [50688/54000 (94%)] Loss: -248653.984375\n",
      "Train Epoch: 382 [52096/54000 (96%)] Loss: -232021.281250\n",
      "    epoch          : 382\n",
      "    loss           : -228198.08234898324\n",
      "    val_loss       : -230201.84385122903\n",
      "Train Epoch: 383 [0/54000 (0%)] Loss: -248621.843750\n",
      "Train Epoch: 383 [1408/54000 (3%)] Loss: -230864.953125\n",
      "Train Epoch: 383 [2816/54000 (5%)] Loss: -223701.906250\n",
      "Train Epoch: 383 [4224/54000 (8%)] Loss: -225371.640625\n",
      "Train Epoch: 383 [5632/54000 (10%)] Loss: -229169.468750\n",
      "Train Epoch: 383 [7040/54000 (13%)] Loss: -229953.000000\n",
      "Train Epoch: 383 [8448/54000 (16%)] Loss: -250768.031250\n",
      "Train Epoch: 383 [9856/54000 (18%)] Loss: -224795.765625\n",
      "Train Epoch: 383 [11264/54000 (21%)] Loss: -230254.562500\n",
      "Train Epoch: 383 [12672/54000 (23%)] Loss: -221823.875000\n",
      "Train Epoch: 383 [14080/54000 (26%)] Loss: -229833.609375\n",
      "Train Epoch: 383 [15488/54000 (29%)] Loss: -228666.218750\n",
      "Train Epoch: 383 [16896/54000 (31%)] Loss: -222212.734375\n",
      "Train Epoch: 383 [18304/54000 (34%)] Loss: -225435.000000\n",
      "Train Epoch: 383 [19712/54000 (37%)] Loss: -216624.312500\n",
      "Train Epoch: 383 [21120/54000 (39%)] Loss: -230399.468750\n",
      "Train Epoch: 383 [22528/54000 (42%)] Loss: -230539.750000\n",
      "Train Epoch: 383 [23936/54000 (44%)] Loss: -232377.203125\n",
      "Train Epoch: 383 [25344/54000 (47%)] Loss: -225607.234375\n",
      "Train Epoch: 383 [26752/54000 (50%)] Loss: -226604.656250\n",
      "Train Epoch: 383 [28160/54000 (52%)] Loss: -250383.578125\n",
      "Train Epoch: 383 [29568/54000 (55%)] Loss: -222340.250000\n",
      "Train Epoch: 383 [30976/54000 (57%)] Loss: -225561.687500\n",
      "Train Epoch: 383 [32384/54000 (60%)] Loss: -226207.156250\n",
      "Train Epoch: 383 [33792/54000 (63%)] Loss: -231550.468750\n",
      "Train Epoch: 383 [35200/54000 (65%)] Loss: -250152.062500\n",
      "Train Epoch: 383 [36608/54000 (68%)] Loss: -220267.328125\n",
      "Train Epoch: 383 [38016/54000 (70%)] Loss: -221133.765625\n",
      "Train Epoch: 383 [39424/54000 (73%)] Loss: -232289.937500\n",
      "Train Epoch: 383 [40832/54000 (76%)] Loss: -248599.546875\n",
      "Train Epoch: 383 [42240/54000 (78%)] Loss: -229593.671875\n",
      "Train Epoch: 383 [43648/54000 (81%)] Loss: -229222.718750\n",
      "Train Epoch: 383 [45056/54000 (83%)] Loss: -223538.000000\n",
      "Train Epoch: 383 [46464/54000 (86%)] Loss: -249720.234375\n",
      "Train Epoch: 383 [47872/54000 (89%)] Loss: -225387.078125\n",
      "Train Epoch: 383 [49280/54000 (91%)] Loss: -250306.421875\n",
      "Train Epoch: 383 [50688/54000 (94%)] Loss: -225017.500000\n",
      "Train Epoch: 383 [52096/54000 (96%)] Loss: -222966.515625\n",
      "    epoch          : 383\n",
      "    loss           : -228248.43305173446\n",
      "    val_loss       : -230098.36562738186\n",
      "Train Epoch: 384 [0/54000 (0%)] Loss: -250309.343750\n",
      "Train Epoch: 384 [1408/54000 (3%)] Loss: -231069.093750\n",
      "Train Epoch: 384 [2816/54000 (5%)] Loss: -231024.421875\n",
      "Train Epoch: 384 [4224/54000 (8%)] Loss: -226111.781250\n",
      "Train Epoch: 384 [5632/54000 (10%)] Loss: -248796.265625\n",
      "Train Epoch: 384 [7040/54000 (13%)] Loss: -221391.515625\n",
      "Train Epoch: 384 [8448/54000 (16%)] Loss: -219590.578125\n",
      "Train Epoch: 384 [9856/54000 (18%)] Loss: -221534.187500\n",
      "Train Epoch: 384 [11264/54000 (21%)] Loss: -218474.906250\n",
      "Train Epoch: 384 [12672/54000 (23%)] Loss: -224755.015625\n",
      "Train Epoch: 384 [14080/54000 (26%)] Loss: -221510.671875\n",
      "Train Epoch: 384 [15488/54000 (29%)] Loss: -230345.031250\n",
      "Train Epoch: 384 [16896/54000 (31%)] Loss: -249221.140625\n",
      "Train Epoch: 384 [18304/54000 (34%)] Loss: -232612.828125\n",
      "Train Epoch: 384 [19712/54000 (37%)] Loss: -220611.921875\n",
      "Train Epoch: 384 [21120/54000 (39%)] Loss: -219873.671875\n",
      "Train Epoch: 384 [22528/54000 (42%)] Loss: -231846.156250\n",
      "Train Epoch: 384 [23936/54000 (44%)] Loss: -250673.968750\n",
      "Train Epoch: 384 [25344/54000 (47%)] Loss: -227062.343750\n",
      "Train Epoch: 384 [26752/54000 (50%)] Loss: -222934.328125\n",
      "Train Epoch: 384 [28160/54000 (52%)] Loss: -221468.468750\n",
      "Train Epoch: 384 [29568/54000 (55%)] Loss: -218974.312500\n",
      "Train Epoch: 384 [30976/54000 (57%)] Loss: -228338.625000\n",
      "Train Epoch: 384 [32384/54000 (60%)] Loss: -225664.656250\n",
      "Train Epoch: 384 [33792/54000 (63%)] Loss: -232108.531250\n",
      "Train Epoch: 384 [35200/54000 (65%)] Loss: -227268.296875\n",
      "Train Epoch: 384 [36608/54000 (68%)] Loss: -218312.125000\n",
      "Train Epoch: 384 [38016/54000 (70%)] Loss: -232332.593750\n",
      "Train Epoch: 384 [39424/54000 (73%)] Loss: -231802.109375\n",
      "Train Epoch: 384 [40832/54000 (76%)] Loss: -224085.156250\n",
      "Train Epoch: 384 [42240/54000 (78%)] Loss: -231005.937500\n",
      "Train Epoch: 384 [43648/54000 (81%)] Loss: -230564.515625\n",
      "Train Epoch: 384 [45056/54000 (83%)] Loss: -219741.953125\n",
      "Train Epoch: 384 [46464/54000 (86%)] Loss: -223592.375000\n",
      "Train Epoch: 384 [47872/54000 (89%)] Loss: -217527.656250\n",
      "Train Epoch: 384 [49280/54000 (91%)] Loss: -219746.140625\n",
      "Train Epoch: 384 [50688/54000 (94%)] Loss: -222073.171875\n",
      "Train Epoch: 384 [52096/54000 (96%)] Loss: -230251.609375\n",
      "    epoch          : 384\n",
      "    loss           : -228240.14873654305\n",
      "    val_loss       : -230253.49564119664\n",
      "Train Epoch: 385 [0/54000 (0%)] Loss: -220609.312500\n",
      "Train Epoch: 385 [1408/54000 (3%)] Loss: -249687.093750\n",
      "Train Epoch: 385 [2816/54000 (5%)] Loss: -223646.453125\n",
      "Train Epoch: 385 [4224/54000 (8%)] Loss: -222072.687500\n",
      "Train Epoch: 385 [5632/54000 (10%)] Loss: -227415.187500\n",
      "Train Epoch: 385 [7040/54000 (13%)] Loss: -227758.343750\n",
      "Train Epoch: 385 [8448/54000 (16%)] Loss: -231950.953125\n",
      "Train Epoch: 385 [9856/54000 (18%)] Loss: -231643.937500\n",
      "Train Epoch: 385 [11264/54000 (21%)] Loss: -249374.812500\n",
      "Train Epoch: 385 [12672/54000 (23%)] Loss: -226057.609375\n",
      "Train Epoch: 385 [14080/54000 (26%)] Loss: -230344.625000\n",
      "Train Epoch: 385 [15488/54000 (29%)] Loss: -223458.265625\n",
      "Train Epoch: 385 [16896/54000 (31%)] Loss: -223246.328125\n",
      "Train Epoch: 385 [18304/54000 (34%)] Loss: -233036.421875\n",
      "Train Epoch: 385 [19712/54000 (37%)] Loss: -218992.359375\n",
      "Train Epoch: 385 [21120/54000 (39%)] Loss: -231094.375000\n",
      "Train Epoch: 385 [22528/54000 (42%)] Loss: -250031.437500\n",
      "Train Epoch: 385 [23936/54000 (44%)] Loss: -224473.625000\n",
      "Train Epoch: 385 [25344/54000 (47%)] Loss: -215968.078125\n",
      "Train Epoch: 385 [26752/54000 (50%)] Loss: -225313.546875\n",
      "Train Epoch: 385 [28160/54000 (52%)] Loss: -231711.718750\n",
      "Train Epoch: 385 [29568/54000 (55%)] Loss: -220968.468750\n",
      "Train Epoch: 385 [30976/54000 (57%)] Loss: -220992.718750\n",
      "Train Epoch: 385 [32384/54000 (60%)] Loss: -233274.796875\n",
      "Train Epoch: 385 [33792/54000 (63%)] Loss: -229822.156250\n",
      "Train Epoch: 385 [35200/54000 (65%)] Loss: -231803.515625\n",
      "Train Epoch: 385 [36608/54000 (68%)] Loss: -225979.187500\n",
      "Train Epoch: 385 [38016/54000 (70%)] Loss: -223213.796875\n",
      "Train Epoch: 385 [39424/54000 (73%)] Loss: -249104.750000\n",
      "Train Epoch: 385 [40832/54000 (76%)] Loss: -223413.015625\n",
      "Train Epoch: 385 [42240/54000 (78%)] Loss: -220821.406250\n",
      "Train Epoch: 385 [43648/54000 (81%)] Loss: -221229.484375\n",
      "Train Epoch: 385 [45056/54000 (83%)] Loss: -220611.468750\n",
      "Train Epoch: 385 [46464/54000 (86%)] Loss: -221938.562500\n",
      "Train Epoch: 385 [47872/54000 (89%)] Loss: -222497.281250\n",
      "Train Epoch: 385 [49280/54000 (91%)] Loss: -225403.750000\n",
      "Train Epoch: 385 [50688/54000 (94%)] Loss: -216386.109375\n",
      "Train Epoch: 385 [52096/54000 (96%)] Loss: -248847.812500\n",
      "    epoch          : 385\n",
      "    loss           : -228150.02220394736\n",
      "    val_loss       : -230546.97669350228\n",
      "Train Epoch: 386 [0/54000 (0%)] Loss: -247775.312500\n",
      "Train Epoch: 386 [1408/54000 (3%)] Loss: -220339.328125\n",
      "Train Epoch: 386 [2816/54000 (5%)] Loss: -225760.968750\n",
      "Train Epoch: 386 [4224/54000 (8%)] Loss: -229307.296875\n",
      "Train Epoch: 386 [5632/54000 (10%)] Loss: -228593.218750\n",
      "Train Epoch: 386 [7040/54000 (13%)] Loss: -228475.421875\n",
      "Train Epoch: 386 [8448/54000 (16%)] Loss: -226597.406250\n",
      "Train Epoch: 386 [9856/54000 (18%)] Loss: -228083.062500\n",
      "Train Epoch: 386 [11264/54000 (21%)] Loss: -226960.656250\n",
      "Train Epoch: 386 [12672/54000 (23%)] Loss: -230163.062500\n",
      "Train Epoch: 386 [14080/54000 (26%)] Loss: -232239.296875\n",
      "Train Epoch: 386 [15488/54000 (29%)] Loss: -220936.421875\n",
      "Train Epoch: 386 [16896/54000 (31%)] Loss: -249042.359375\n",
      "Train Epoch: 386 [18304/54000 (34%)] Loss: -222938.171875\n",
      "Train Epoch: 386 [19712/54000 (37%)] Loss: -231812.078125\n",
      "Train Epoch: 386 [21120/54000 (39%)] Loss: -222442.828125\n",
      "Train Epoch: 386 [22528/54000 (42%)] Loss: -221840.156250\n",
      "Train Epoch: 386 [23936/54000 (44%)] Loss: -224287.781250\n",
      "Train Epoch: 386 [25344/54000 (47%)] Loss: -220674.406250\n",
      "Train Epoch: 386 [26752/54000 (50%)] Loss: -219467.937500\n",
      "Train Epoch: 386 [28160/54000 (52%)] Loss: -231599.875000\n",
      "Train Epoch: 386 [29568/54000 (55%)] Loss: -221826.156250\n",
      "Train Epoch: 386 [30976/54000 (57%)] Loss: -250493.218750\n",
      "Train Epoch: 386 [32384/54000 (60%)] Loss: -222030.015625\n",
      "Train Epoch: 386 [33792/54000 (63%)] Loss: -220711.203125\n",
      "Train Epoch: 386 [35200/54000 (65%)] Loss: -216716.328125\n",
      "Train Epoch: 386 [36608/54000 (68%)] Loss: -225889.640625\n",
      "Train Epoch: 386 [38016/54000 (70%)] Loss: -231864.312500\n",
      "Train Epoch: 386 [39424/54000 (73%)] Loss: -217916.406250\n",
      "Train Epoch: 386 [40832/54000 (76%)] Loss: -231715.453125\n",
      "Train Epoch: 386 [42240/54000 (78%)] Loss: -231524.187500\n",
      "Train Epoch: 386 [43648/54000 (81%)] Loss: -226165.781250\n",
      "Train Epoch: 386 [45056/54000 (83%)] Loss: -229094.312500\n",
      "Train Epoch: 386 [46464/54000 (86%)] Loss: -222856.125000\n",
      "Train Epoch: 386 [47872/54000 (89%)] Loss: -227306.406250\n",
      "Train Epoch: 386 [49280/54000 (91%)] Loss: -218546.437500\n",
      "Train Epoch: 386 [50688/54000 (94%)] Loss: -231194.781250\n",
      "Train Epoch: 386 [52096/54000 (96%)] Loss: -250073.203125\n",
      "    epoch          : 386\n",
      "    loss           : -228226.74252392346\n",
      "    val_loss       : -230412.5126476753\n",
      "Train Epoch: 387 [0/54000 (0%)] Loss: -229700.359375\n",
      "Train Epoch: 387 [1408/54000 (3%)] Loss: -223825.875000\n",
      "Train Epoch: 387 [2816/54000 (5%)] Loss: -250616.796875\n",
      "Train Epoch: 387 [4224/54000 (8%)] Loss: -228206.062500\n",
      "Train Epoch: 387 [5632/54000 (10%)] Loss: -228438.031250\n",
      "Train Epoch: 387 [7040/54000 (13%)] Loss: -220533.250000\n",
      "Train Epoch: 387 [8448/54000 (16%)] Loss: -228243.062500\n",
      "Train Epoch: 387 [9856/54000 (18%)] Loss: -228176.843750\n",
      "Train Epoch: 387 [11264/54000 (21%)] Loss: -228844.843750\n",
      "Train Epoch: 387 [12672/54000 (23%)] Loss: -225856.921875\n",
      "Train Epoch: 387 [14080/54000 (26%)] Loss: -232079.406250\n",
      "Train Epoch: 387 [15488/54000 (29%)] Loss: -223634.578125\n",
      "Train Epoch: 387 [16896/54000 (31%)] Loss: -221518.187500\n",
      "Train Epoch: 387 [18304/54000 (34%)] Loss: -224303.968750\n",
      "Train Epoch: 387 [19712/54000 (37%)] Loss: -222536.812500\n",
      "Train Epoch: 387 [21120/54000 (39%)] Loss: -220417.953125\n",
      "Train Epoch: 387 [22528/54000 (42%)] Loss: -230722.390625\n",
      "Train Epoch: 387 [23936/54000 (44%)] Loss: -231032.218750\n",
      "Train Epoch: 387 [25344/54000 (47%)] Loss: -231917.312500\n",
      "Train Epoch: 387 [26752/54000 (50%)] Loss: -225746.937500\n",
      "Train Epoch: 387 [28160/54000 (52%)] Loss: -232054.875000\n",
      "Train Epoch: 387 [29568/54000 (55%)] Loss: -227461.468750\n",
      "Train Epoch: 387 [30976/54000 (57%)] Loss: -222831.875000\n",
      "Train Epoch: 387 [32384/54000 (60%)] Loss: -250222.984375\n",
      "Train Epoch: 387 [33792/54000 (63%)] Loss: -222597.703125\n",
      "Train Epoch: 387 [35200/54000 (65%)] Loss: -230696.515625\n",
      "Train Epoch: 387 [36608/54000 (68%)] Loss: -226214.578125\n",
      "Train Epoch: 387 [38016/54000 (70%)] Loss: -251208.453125\n",
      "Train Epoch: 387 [39424/54000 (73%)] Loss: -227123.531250\n",
      "Train Epoch: 387 [40832/54000 (76%)] Loss: -229935.671875\n",
      "Train Epoch: 387 [42240/54000 (78%)] Loss: -223020.593750\n",
      "Train Epoch: 387 [43648/54000 (81%)] Loss: -231849.796875\n",
      "Train Epoch: 387 [45056/54000 (83%)] Loss: -220810.765625\n",
      "Train Epoch: 387 [46464/54000 (86%)] Loss: -248935.593750\n",
      "Train Epoch: 387 [47872/54000 (89%)] Loss: -223103.625000\n",
      "Train Epoch: 387 [49280/54000 (91%)] Loss: -221743.281250\n",
      "Train Epoch: 387 [50688/54000 (94%)] Loss: -216897.062500\n",
      "Train Epoch: 387 [52096/54000 (96%)] Loss: -249317.937500\n",
      "    epoch          : 387\n",
      "    loss           : -228257.48960825359\n",
      "    val_loss       : -230663.33821813072\n",
      "Train Epoch: 388 [0/54000 (0%)] Loss: -232440.546875\n",
      "Train Epoch: 388 [1408/54000 (3%)] Loss: -218433.828125\n",
      "Train Epoch: 388 [2816/54000 (5%)] Loss: -223164.453125\n",
      "Train Epoch: 388 [4224/54000 (8%)] Loss: -221301.687500\n",
      "Train Epoch: 388 [5632/54000 (10%)] Loss: -220995.390625\n",
      "Train Epoch: 388 [7040/54000 (13%)] Loss: -221525.250000\n",
      "Train Epoch: 388 [8448/54000 (16%)] Loss: -228277.656250\n",
      "Train Epoch: 388 [9856/54000 (18%)] Loss: -222424.781250\n",
      "Train Epoch: 388 [11264/54000 (21%)] Loss: -250497.718750\n",
      "Train Epoch: 388 [12672/54000 (23%)] Loss: -222030.921875\n",
      "Train Epoch: 388 [14080/54000 (26%)] Loss: -230700.531250\n",
      "Train Epoch: 388 [15488/54000 (29%)] Loss: -214381.906250\n",
      "Train Epoch: 388 [16896/54000 (31%)] Loss: -251199.218750\n",
      "Train Epoch: 388 [18304/54000 (34%)] Loss: -224176.281250\n",
      "Train Epoch: 388 [19712/54000 (37%)] Loss: -220122.718750\n",
      "Train Epoch: 388 [21120/54000 (39%)] Loss: -231177.578125\n",
      "Train Epoch: 388 [22528/54000 (42%)] Loss: -248985.953125\n",
      "Train Epoch: 388 [23936/54000 (44%)] Loss: -229421.031250\n",
      "Train Epoch: 388 [25344/54000 (47%)] Loss: -224377.578125\n",
      "Train Epoch: 388 [26752/54000 (50%)] Loss: -229519.343750\n",
      "Train Epoch: 388 [28160/54000 (52%)] Loss: -229596.703125\n",
      "Train Epoch: 388 [29568/54000 (55%)] Loss: -232296.171875\n",
      "Train Epoch: 388 [30976/54000 (57%)] Loss: -222737.296875\n",
      "Train Epoch: 388 [32384/54000 (60%)] Loss: -222739.093750\n",
      "Train Epoch: 388 [33792/54000 (63%)] Loss: -221207.593750\n",
      "Train Epoch: 388 [35200/54000 (65%)] Loss: -221765.718750\n",
      "Train Epoch: 388 [36608/54000 (68%)] Loss: -221415.734375\n",
      "Train Epoch: 388 [38016/54000 (70%)] Loss: -224908.828125\n",
      "Train Epoch: 388 [39424/54000 (73%)] Loss: -220839.421875\n",
      "Train Epoch: 388 [40832/54000 (76%)] Loss: -221946.796875\n",
      "Train Epoch: 388 [42240/54000 (78%)] Loss: -217497.625000\n",
      "Train Epoch: 388 [43648/54000 (81%)] Loss: -230100.781250\n",
      "Train Epoch: 388 [45056/54000 (83%)] Loss: -231852.843750\n",
      "Train Epoch: 388 [46464/54000 (86%)] Loss: -221857.265625\n",
      "Train Epoch: 388 [47872/54000 (89%)] Loss: -226085.734375\n",
      "Train Epoch: 388 [49280/54000 (91%)] Loss: -226055.093750\n",
      "Train Epoch: 388 [50688/54000 (94%)] Loss: -222205.265625\n",
      "Train Epoch: 388 [52096/54000 (96%)] Loss: -221538.421875\n",
      "    epoch          : 388\n",
      "    loss           : -228315.8198639354\n",
      "    val_loss       : -230652.90078363186\n",
      "Train Epoch: 389 [0/54000 (0%)] Loss: -224577.296875\n",
      "Train Epoch: 389 [1408/54000 (3%)] Loss: -228217.937500\n",
      "Train Epoch: 389 [2816/54000 (5%)] Loss: -224073.453125\n",
      "Train Epoch: 389 [4224/54000 (8%)] Loss: -231065.968750\n",
      "Train Epoch: 389 [5632/54000 (10%)] Loss: -222722.718750\n",
      "Train Epoch: 389 [7040/54000 (13%)] Loss: -219483.000000\n",
      "Train Epoch: 389 [8448/54000 (16%)] Loss: -226526.250000\n",
      "Train Epoch: 389 [9856/54000 (18%)] Loss: -229933.140625\n",
      "Train Epoch: 389 [11264/54000 (21%)] Loss: -222048.437500\n",
      "Train Epoch: 389 [12672/54000 (23%)] Loss: -230597.546875\n",
      "Train Epoch: 389 [14080/54000 (26%)] Loss: -226509.500000\n",
      "Train Epoch: 389 [15488/54000 (29%)] Loss: -231592.453125\n",
      "Train Epoch: 389 [16896/54000 (31%)] Loss: -250623.390625\n",
      "Train Epoch: 389 [18304/54000 (34%)] Loss: -223017.078125\n",
      "Train Epoch: 389 [19712/54000 (37%)] Loss: -216612.609375\n",
      "Train Epoch: 389 [21120/54000 (39%)] Loss: -229282.750000\n",
      "Train Epoch: 389 [22528/54000 (42%)] Loss: -228553.625000\n",
      "Train Epoch: 389 [23936/54000 (44%)] Loss: -221491.578125\n",
      "Train Epoch: 389 [25344/54000 (47%)] Loss: -226752.218750\n",
      "Train Epoch: 389 [26752/54000 (50%)] Loss: -224201.000000\n",
      "Train Epoch: 389 [28160/54000 (52%)] Loss: -231790.500000\n",
      "Train Epoch: 389 [29568/54000 (55%)] Loss: -231583.390625\n",
      "Train Epoch: 389 [30976/54000 (57%)] Loss: -219134.406250\n",
      "Train Epoch: 389 [32384/54000 (60%)] Loss: -218110.390625\n",
      "Train Epoch: 389 [33792/54000 (63%)] Loss: -220434.171875\n",
      "Train Epoch: 389 [35200/54000 (65%)] Loss: -222260.781250\n",
      "Train Epoch: 389 [36608/54000 (68%)] Loss: -225564.140625\n",
      "Train Epoch: 389 [38016/54000 (70%)] Loss: -228057.375000\n",
      "Train Epoch: 389 [39424/54000 (73%)] Loss: -227694.406250\n",
      "Train Epoch: 389 [40832/54000 (76%)] Loss: -223034.000000\n",
      "Train Epoch: 389 [42240/54000 (78%)] Loss: -229673.843750\n",
      "Train Epoch: 389 [43648/54000 (81%)] Loss: -221853.218750\n",
      "Train Epoch: 389 [45056/54000 (83%)] Loss: -222710.500000\n",
      "Train Epoch: 389 [46464/54000 (86%)] Loss: -222530.281250\n",
      "Train Epoch: 389 [47872/54000 (89%)] Loss: -219117.500000\n",
      "Train Epoch: 389 [49280/54000 (91%)] Loss: -226430.234375\n",
      "Train Epoch: 389 [50688/54000 (94%)] Loss: -250013.921875\n",
      "Train Epoch: 389 [52096/54000 (96%)] Loss: -222147.671875\n",
      "    epoch          : 389\n",
      "    loss           : -228179.87040221292\n",
      "    val_loss       : -230690.26045636434\n",
      "Train Epoch: 390 [0/54000 (0%)] Loss: -248260.000000\n",
      "Train Epoch: 390 [1408/54000 (3%)] Loss: -222354.468750\n",
      "Train Epoch: 390 [2816/54000 (5%)] Loss: -219717.031250\n",
      "Train Epoch: 390 [4224/54000 (8%)] Loss: -223526.546875\n",
      "Train Epoch: 390 [5632/54000 (10%)] Loss: -225808.109375\n",
      "Train Epoch: 390 [7040/54000 (13%)] Loss: -223902.125000\n",
      "Train Epoch: 390 [8448/54000 (16%)] Loss: -224410.890625\n",
      "Train Epoch: 390 [9856/54000 (18%)] Loss: -225238.000000\n",
      "Train Epoch: 390 [11264/54000 (21%)] Loss: -228298.375000\n",
      "Train Epoch: 390 [12672/54000 (23%)] Loss: -225824.281250\n",
      "Train Epoch: 390 [14080/54000 (26%)] Loss: -230641.531250\n",
      "Train Epoch: 390 [15488/54000 (29%)] Loss: -250328.406250\n",
      "Train Epoch: 390 [16896/54000 (31%)] Loss: -226914.015625\n",
      "Train Epoch: 390 [18304/54000 (34%)] Loss: -226881.218750\n",
      "Train Epoch: 390 [19712/54000 (37%)] Loss: -229799.906250\n",
      "Train Epoch: 390 [21120/54000 (39%)] Loss: -219548.265625\n",
      "Train Epoch: 390 [22528/54000 (42%)] Loss: -225897.781250\n",
      "Train Epoch: 390 [23936/54000 (44%)] Loss: -232008.828125\n",
      "Train Epoch: 390 [25344/54000 (47%)] Loss: -230459.703125\n",
      "Train Epoch: 390 [26752/54000 (50%)] Loss: -228223.718750\n",
      "Train Epoch: 390 [28160/54000 (52%)] Loss: -227300.859375\n",
      "Train Epoch: 390 [29568/54000 (55%)] Loss: -223338.875000\n",
      "Train Epoch: 390 [30976/54000 (57%)] Loss: -223941.140625\n",
      "Train Epoch: 390 [32384/54000 (60%)] Loss: -227215.578125\n",
      "Train Epoch: 390 [33792/54000 (63%)] Loss: -228158.531250\n",
      "Train Epoch: 390 [35200/54000 (65%)] Loss: -230470.812500\n",
      "Train Epoch: 390 [36608/54000 (68%)] Loss: -217910.359375\n",
      "Train Epoch: 390 [38016/54000 (70%)] Loss: -221721.203125\n",
      "Train Epoch: 390 [39424/54000 (73%)] Loss: -250451.437500\n",
      "Train Epoch: 390 [40832/54000 (76%)] Loss: -230718.578125\n",
      "Train Epoch: 390 [42240/54000 (78%)] Loss: -219413.953125\n",
      "Train Epoch: 390 [43648/54000 (81%)] Loss: -224196.968750\n",
      "Train Epoch: 390 [45056/54000 (83%)] Loss: -222805.890625\n",
      "Train Epoch: 390 [46464/54000 (86%)] Loss: -223889.875000\n",
      "Train Epoch: 390 [47872/54000 (89%)] Loss: -229332.937500\n",
      "Train Epoch: 390 [49280/54000 (91%)] Loss: -217649.968750\n",
      "Train Epoch: 390 [50688/54000 (94%)] Loss: -220911.718750\n",
      "Train Epoch: 390 [52096/54000 (96%)] Loss: -223495.531250\n",
      "    epoch          : 390\n",
      "    loss           : -228329.91114683016\n",
      "    val_loss       : -230429.36255478277\n",
      "Train Epoch: 391 [0/54000 (0%)] Loss: -249951.109375\n",
      "Train Epoch: 391 [1408/54000 (3%)] Loss: -225429.140625\n",
      "Train Epoch: 391 [2816/54000 (5%)] Loss: -226557.578125\n",
      "Train Epoch: 391 [4224/54000 (8%)] Loss: -222782.562500\n",
      "Train Epoch: 391 [5632/54000 (10%)] Loss: -230916.265625\n",
      "Train Epoch: 391 [7040/54000 (13%)] Loss: -225675.734375\n",
      "Train Epoch: 391 [8448/54000 (16%)] Loss: -214584.484375\n",
      "Train Epoch: 391 [9856/54000 (18%)] Loss: -229730.062500\n",
      "Train Epoch: 391 [11264/54000 (21%)] Loss: -216458.546875\n",
      "Train Epoch: 391 [12672/54000 (23%)] Loss: -222946.765625\n",
      "Train Epoch: 391 [14080/54000 (26%)] Loss: -225841.218750\n",
      "Train Epoch: 391 [15488/54000 (29%)] Loss: -231975.000000\n",
      "Train Epoch: 391 [16896/54000 (31%)] Loss: -228634.281250\n",
      "Train Epoch: 391 [18304/54000 (34%)] Loss: -229209.968750\n",
      "Train Epoch: 391 [19712/54000 (37%)] Loss: -220318.187500\n",
      "Train Epoch: 391 [21120/54000 (39%)] Loss: -231101.187500\n",
      "Train Epoch: 391 [22528/54000 (42%)] Loss: -227177.296875\n",
      "Train Epoch: 391 [23936/54000 (44%)] Loss: -221808.390625\n",
      "Train Epoch: 391 [25344/54000 (47%)] Loss: -230326.093750\n",
      "Train Epoch: 391 [26752/54000 (50%)] Loss: -217192.312500\n",
      "Train Epoch: 391 [28160/54000 (52%)] Loss: -222315.062500\n",
      "Train Epoch: 391 [29568/54000 (55%)] Loss: -216876.593750\n",
      "Train Epoch: 391 [30976/54000 (57%)] Loss: -228620.843750\n",
      "Train Epoch: 391 [32384/54000 (60%)] Loss: -231295.390625\n",
      "Train Epoch: 391 [33792/54000 (63%)] Loss: -226509.937500\n",
      "Train Epoch: 391 [35200/54000 (65%)] Loss: -221848.062500\n",
      "Train Epoch: 391 [36608/54000 (68%)] Loss: -229389.859375\n",
      "Train Epoch: 391 [38016/54000 (70%)] Loss: -220004.812500\n",
      "Train Epoch: 391 [39424/54000 (73%)] Loss: -220147.390625\n",
      "Train Epoch: 391 [40832/54000 (76%)] Loss: -218649.421875\n",
      "Train Epoch: 391 [42240/54000 (78%)] Loss: -231634.593750\n",
      "Train Epoch: 391 [43648/54000 (81%)] Loss: -229535.875000\n",
      "Train Epoch: 391 [45056/54000 (83%)] Loss: -222446.453125\n",
      "Train Epoch: 391 [46464/54000 (86%)] Loss: -249211.484375\n",
      "Train Epoch: 391 [47872/54000 (89%)] Loss: -225578.671875\n",
      "Train Epoch: 391 [49280/54000 (91%)] Loss: -221014.812500\n",
      "Train Epoch: 391 [50688/54000 (94%)] Loss: -221178.750000\n",
      "Train Epoch: 391 [52096/54000 (96%)] Loss: -217614.781250\n",
      "    epoch          : 391\n",
      "    loss           : -228176.19090161484\n",
      "    val_loss       : -229878.69147175114\n",
      "Train Epoch: 392 [0/54000 (0%)] Loss: -224650.718750\n",
      "Train Epoch: 392 [1408/54000 (3%)] Loss: -223635.718750\n",
      "Train Epoch: 392 [2816/54000 (5%)] Loss: -230970.046875\n",
      "Train Epoch: 392 [4224/54000 (8%)] Loss: -226942.609375\n",
      "Train Epoch: 392 [5632/54000 (10%)] Loss: -223194.531250\n",
      "Train Epoch: 392 [7040/54000 (13%)] Loss: -228216.125000\n",
      "Train Epoch: 392 [8448/54000 (16%)] Loss: -249789.812500\n",
      "Train Epoch: 392 [9856/54000 (18%)] Loss: -224188.125000\n",
      "Train Epoch: 392 [11264/54000 (21%)] Loss: -226776.359375\n",
      "Train Epoch: 392 [12672/54000 (23%)] Loss: -230820.312500\n",
      "Train Epoch: 392 [14080/54000 (26%)] Loss: -231575.640625\n",
      "Train Epoch: 392 [15488/54000 (29%)] Loss: -220175.312500\n",
      "Train Epoch: 392 [16896/54000 (31%)] Loss: -232832.453125\n",
      "Train Epoch: 392 [18304/54000 (34%)] Loss: -249187.593750\n",
      "Train Epoch: 392 [19712/54000 (37%)] Loss: -222815.593750\n",
      "Train Epoch: 392 [21120/54000 (39%)] Loss: -222489.406250\n",
      "Train Epoch: 392 [22528/54000 (42%)] Loss: -226521.265625\n",
      "Train Epoch: 392 [23936/54000 (44%)] Loss: -230651.859375\n",
      "Train Epoch: 392 [25344/54000 (47%)] Loss: -249362.843750\n",
      "Train Epoch: 392 [26752/54000 (50%)] Loss: -231443.796875\n",
      "Train Epoch: 392 [28160/54000 (52%)] Loss: -231629.343750\n",
      "Train Epoch: 392 [29568/54000 (55%)] Loss: -228754.359375\n",
      "Train Epoch: 392 [30976/54000 (57%)] Loss: -227682.000000\n",
      "Train Epoch: 392 [32384/54000 (60%)] Loss: -231952.203125\n",
      "Train Epoch: 392 [33792/54000 (63%)] Loss: -223776.343750\n",
      "Train Epoch: 392 [35200/54000 (65%)] Loss: -225152.562500\n",
      "Train Epoch: 392 [36608/54000 (68%)] Loss: -221917.343750\n",
      "Train Epoch: 392 [38016/54000 (70%)] Loss: -231587.984375\n",
      "Train Epoch: 392 [39424/54000 (73%)] Loss: -222881.093750\n",
      "Train Epoch: 392 [40832/54000 (76%)] Loss: -228242.281250\n",
      "Train Epoch: 392 [42240/54000 (78%)] Loss: -222325.765625\n",
      "Train Epoch: 392 [43648/54000 (81%)] Loss: -228192.031250\n",
      "Train Epoch: 392 [45056/54000 (83%)] Loss: -220884.687500\n",
      "Train Epoch: 392 [46464/54000 (86%)] Loss: -220963.890625\n",
      "Train Epoch: 392 [47872/54000 (89%)] Loss: -221005.531250\n",
      "Train Epoch: 392 [49280/54000 (91%)] Loss: -250094.656250\n",
      "Train Epoch: 392 [50688/54000 (94%)] Loss: -223742.843750\n",
      "Train Epoch: 392 [52096/54000 (96%)] Loss: -221563.906250\n",
      "    epoch          : 392\n",
      "    loss           : -228321.34270334928\n",
      "    val_loss       : -230244.22557998286\n",
      "Train Epoch: 393 [0/54000 (0%)] Loss: -249465.734375\n",
      "Train Epoch: 393 [1408/54000 (3%)] Loss: -231830.500000\n",
      "Train Epoch: 393 [2816/54000 (5%)] Loss: -230214.484375\n",
      "Train Epoch: 393 [4224/54000 (8%)] Loss: -231294.656250\n",
      "Train Epoch: 393 [5632/54000 (10%)] Loss: -221151.640625\n",
      "Train Epoch: 393 [7040/54000 (13%)] Loss: -222377.656250\n",
      "Train Epoch: 393 [8448/54000 (16%)] Loss: -229962.578125\n",
      "Train Epoch: 393 [9856/54000 (18%)] Loss: -231563.875000\n",
      "Train Epoch: 393 [11264/54000 (21%)] Loss: -219690.750000\n",
      "Train Epoch: 393 [12672/54000 (23%)] Loss: -218596.609375\n",
      "Train Epoch: 393 [14080/54000 (26%)] Loss: -223078.062500\n",
      "Train Epoch: 393 [15488/54000 (29%)] Loss: -223900.156250\n",
      "Train Epoch: 393 [16896/54000 (31%)] Loss: -224872.500000\n",
      "Train Epoch: 393 [18304/54000 (34%)] Loss: -232040.000000\n",
      "Train Epoch: 393 [19712/54000 (37%)] Loss: -249333.250000\n",
      "Train Epoch: 393 [21120/54000 (39%)] Loss: -227831.234375\n",
      "Train Epoch: 393 [22528/54000 (42%)] Loss: -217247.453125\n",
      "Train Epoch: 393 [23936/54000 (44%)] Loss: -226418.281250\n",
      "Train Epoch: 393 [25344/54000 (47%)] Loss: -248481.765625\n",
      "Train Epoch: 393 [26752/54000 (50%)] Loss: -219584.031250\n",
      "Train Epoch: 393 [28160/54000 (52%)] Loss: -231328.593750\n",
      "Train Epoch: 393 [29568/54000 (55%)] Loss: -231684.046875\n",
      "Train Epoch: 393 [30976/54000 (57%)] Loss: -250011.500000\n",
      "Train Epoch: 393 [32384/54000 (60%)] Loss: -221758.171875\n",
      "Train Epoch: 393 [33792/54000 (63%)] Loss: -230039.843750\n",
      "Train Epoch: 393 [35200/54000 (65%)] Loss: -230620.921875\n",
      "Train Epoch: 393 [36608/54000 (68%)] Loss: -229376.812500\n",
      "Train Epoch: 393 [38016/54000 (70%)] Loss: -229029.281250\n",
      "Train Epoch: 393 [39424/54000 (73%)] Loss: -231615.375000\n",
      "Train Epoch: 393 [40832/54000 (76%)] Loss: -222627.921875\n",
      "Train Epoch: 393 [42240/54000 (78%)] Loss: -229429.765625\n",
      "Train Epoch: 393 [43648/54000 (81%)] Loss: -220888.640625\n",
      "Train Epoch: 393 [45056/54000 (83%)] Loss: -249341.531250\n",
      "Train Epoch: 393 [46464/54000 (86%)] Loss: -219202.250000\n",
      "Train Epoch: 393 [47872/54000 (89%)] Loss: -230385.453125\n",
      "Train Epoch: 393 [49280/54000 (91%)] Loss: -223330.906250\n",
      "Train Epoch: 393 [50688/54000 (94%)] Loss: -226104.375000\n",
      "Train Epoch: 393 [52096/54000 (96%)] Loss: -250563.484375\n",
      "    epoch          : 393\n",
      "    loss           : -228466.7664847488\n",
      "    val_loss       : -230439.06195217225\n",
      "Train Epoch: 394 [0/54000 (0%)] Loss: -229253.406250\n",
      "Train Epoch: 394 [1408/54000 (3%)] Loss: -232539.718750\n",
      "Train Epoch: 394 [2816/54000 (5%)] Loss: -227575.031250\n",
      "Train Epoch: 394 [4224/54000 (8%)] Loss: -231326.437500\n",
      "Train Epoch: 394 [5632/54000 (10%)] Loss: -225427.515625\n",
      "Train Epoch: 394 [7040/54000 (13%)] Loss: -223487.765625\n",
      "Train Epoch: 394 [8448/54000 (16%)] Loss: -249904.515625\n",
      "Train Epoch: 394 [9856/54000 (18%)] Loss: -250723.593750\n",
      "Train Epoch: 394 [11264/54000 (21%)] Loss: -231399.375000\n",
      "Train Epoch: 394 [12672/54000 (23%)] Loss: -229842.062500\n",
      "Train Epoch: 394 [14080/54000 (26%)] Loss: -220075.968750\n",
      "Train Epoch: 394 [15488/54000 (29%)] Loss: -225964.656250\n",
      "Train Epoch: 394 [16896/54000 (31%)] Loss: -250837.515625\n",
      "Train Epoch: 394 [18304/54000 (34%)] Loss: -232853.328125\n",
      "Train Epoch: 394 [19712/54000 (37%)] Loss: -223951.875000\n",
      "Train Epoch: 394 [21120/54000 (39%)] Loss: -249725.687500\n",
      "Train Epoch: 394 [22528/54000 (42%)] Loss: -223430.625000\n",
      "Train Epoch: 394 [23936/54000 (44%)] Loss: -218893.218750\n",
      "Train Epoch: 394 [25344/54000 (47%)] Loss: -232349.734375\n",
      "Train Epoch: 394 [26752/54000 (50%)] Loss: -250445.109375\n",
      "Train Epoch: 394 [28160/54000 (52%)] Loss: -230158.984375\n",
      "Train Epoch: 394 [29568/54000 (55%)] Loss: -227369.203125\n",
      "Train Epoch: 394 [30976/54000 (57%)] Loss: -227899.375000\n",
      "Train Epoch: 394 [32384/54000 (60%)] Loss: -250537.531250\n",
      "Train Epoch: 394 [33792/54000 (63%)] Loss: -218394.593750\n",
      "Train Epoch: 394 [35200/54000 (65%)] Loss: -232307.203125\n",
      "Train Epoch: 394 [36608/54000 (68%)] Loss: -230412.937500\n",
      "Train Epoch: 394 [38016/54000 (70%)] Loss: -227225.875000\n",
      "Train Epoch: 394 [39424/54000 (73%)] Loss: -226372.734375\n",
      "Train Epoch: 394 [40832/54000 (76%)] Loss: -233737.093750\n",
      "Train Epoch: 394 [42240/54000 (78%)] Loss: -232515.984375\n",
      "Train Epoch: 394 [43648/54000 (81%)] Loss: -250914.218750\n",
      "Train Epoch: 394 [45056/54000 (83%)] Loss: -222479.484375\n",
      "Train Epoch: 394 [46464/54000 (86%)] Loss: -222369.312500\n",
      "Train Epoch: 394 [47872/54000 (89%)] Loss: -223175.250000\n",
      "Train Epoch: 394 [49280/54000 (91%)] Loss: -223498.500000\n",
      "Train Epoch: 394 [50688/54000 (94%)] Loss: -221783.718750\n",
      "Train Epoch: 394 [52096/54000 (96%)] Loss: -216150.156250\n",
      "    epoch          : 394\n",
      "    loss           : -228346.34812350478\n",
      "    val_loss       : -230809.0272484756\n",
      "Train Epoch: 395 [0/54000 (0%)] Loss: -248919.953125\n",
      "Train Epoch: 395 [1408/54000 (3%)] Loss: -233041.531250\n",
      "Train Epoch: 395 [2816/54000 (5%)] Loss: -229418.734375\n",
      "Train Epoch: 395 [4224/54000 (8%)] Loss: -231505.031250\n",
      "Train Epoch: 395 [5632/54000 (10%)] Loss: -221712.312500\n",
      "Train Epoch: 395 [7040/54000 (13%)] Loss: -249493.468750\n",
      "Train Epoch: 395 [8448/54000 (16%)] Loss: -221077.234375\n",
      "Train Epoch: 395 [9856/54000 (18%)] Loss: -223510.375000\n",
      "Train Epoch: 395 [11264/54000 (21%)] Loss: -220406.156250\n",
      "Train Epoch: 395 [12672/54000 (23%)] Loss: -231757.109375\n",
      "Train Epoch: 395 [14080/54000 (26%)] Loss: -227261.859375\n",
      "Train Epoch: 395 [15488/54000 (29%)] Loss: -227546.343750\n",
      "Train Epoch: 395 [16896/54000 (31%)] Loss: -229953.109375\n",
      "Train Epoch: 395 [18304/54000 (34%)] Loss: -220353.140625\n",
      "Train Epoch: 395 [19712/54000 (37%)] Loss: -216877.578125\n",
      "Train Epoch: 395 [21120/54000 (39%)] Loss: -220540.218750\n",
      "Train Epoch: 395 [22528/54000 (42%)] Loss: -231201.953125\n",
      "Train Epoch: 395 [23936/54000 (44%)] Loss: -233634.937500\n",
      "Train Epoch: 395 [25344/54000 (47%)] Loss: -249208.656250\n",
      "Train Epoch: 395 [26752/54000 (50%)] Loss: -221242.468750\n",
      "Train Epoch: 395 [28160/54000 (52%)] Loss: -224302.203125\n",
      "Train Epoch: 395 [29568/54000 (55%)] Loss: -219670.765625\n",
      "Train Epoch: 395 [30976/54000 (57%)] Loss: -233536.875000\n",
      "Train Epoch: 395 [32384/54000 (60%)] Loss: -229851.937500\n",
      "Train Epoch: 395 [33792/54000 (63%)] Loss: -249515.468750\n",
      "Train Epoch: 395 [35200/54000 (65%)] Loss: -230112.187500\n",
      "Train Epoch: 395 [36608/54000 (68%)] Loss: -221728.531250\n",
      "Train Epoch: 395 [38016/54000 (70%)] Loss: -223058.125000\n",
      "Train Epoch: 395 [39424/54000 (73%)] Loss: -220813.125000\n",
      "Train Epoch: 395 [40832/54000 (76%)] Loss: -223855.828125\n",
      "Train Epoch: 395 [42240/54000 (78%)] Loss: -231203.359375\n",
      "Train Epoch: 395 [43648/54000 (81%)] Loss: -219390.140625\n",
      "Train Epoch: 395 [45056/54000 (83%)] Loss: -222729.109375\n",
      "Train Epoch: 395 [46464/54000 (86%)] Loss: -247970.656250\n",
      "Train Epoch: 395 [47872/54000 (89%)] Loss: -223488.109375\n",
      "Train Epoch: 395 [49280/54000 (91%)] Loss: -231169.734375\n",
      "Train Epoch: 395 [50688/54000 (94%)] Loss: -250826.203125\n",
      "Train Epoch: 395 [52096/54000 (96%)] Loss: -232559.156250\n",
      "    epoch          : 395\n",
      "    loss           : -228303.84262858852\n",
      "    val_loss       : -230059.25700862234\n",
      "Train Epoch: 396 [0/54000 (0%)] Loss: -231956.656250\n",
      "Train Epoch: 396 [1408/54000 (3%)] Loss: -223596.406250\n",
      "Train Epoch: 396 [2816/54000 (5%)] Loss: -249508.031250\n",
      "Train Epoch: 396 [4224/54000 (8%)] Loss: -219553.562500\n",
      "Train Epoch: 396 [5632/54000 (10%)] Loss: -227333.671875\n",
      "Train Epoch: 396 [7040/54000 (13%)] Loss: -220569.828125\n",
      "Train Epoch: 396 [8448/54000 (16%)] Loss: -231597.750000\n",
      "Train Epoch: 396 [9856/54000 (18%)] Loss: -229828.656250\n",
      "Train Epoch: 396 [11264/54000 (21%)] Loss: -221594.953125\n",
      "Train Epoch: 396 [12672/54000 (23%)] Loss: -228313.500000\n",
      "Train Epoch: 396 [14080/54000 (26%)] Loss: -224106.328125\n",
      "Train Epoch: 396 [15488/54000 (29%)] Loss: -223408.890625\n",
      "Train Epoch: 396 [16896/54000 (31%)] Loss: -221837.218750\n",
      "Train Epoch: 396 [18304/54000 (34%)] Loss: -250464.187500\n",
      "Train Epoch: 396 [19712/54000 (37%)] Loss: -225979.531250\n",
      "Train Epoch: 396 [21120/54000 (39%)] Loss: -230870.265625\n",
      "Train Epoch: 396 [22528/54000 (42%)] Loss: -231760.656250\n",
      "Train Epoch: 396 [23936/54000 (44%)] Loss: -250612.500000\n",
      "Train Epoch: 396 [25344/54000 (47%)] Loss: -228411.250000\n",
      "Train Epoch: 396 [26752/54000 (50%)] Loss: -224171.531250\n",
      "Train Epoch: 396 [28160/54000 (52%)] Loss: -220588.562500\n",
      "Train Epoch: 396 [29568/54000 (55%)] Loss: -222291.406250\n",
      "Train Epoch: 396 [30976/54000 (57%)] Loss: -223004.171875\n",
      "Train Epoch: 396 [32384/54000 (60%)] Loss: -221687.531250\n",
      "Train Epoch: 396 [33792/54000 (63%)] Loss: -231137.906250\n",
      "Train Epoch: 396 [35200/54000 (65%)] Loss: -226124.218750\n",
      "Train Epoch: 396 [36608/54000 (68%)] Loss: -249431.500000\n",
      "Train Epoch: 396 [38016/54000 (70%)] Loss: -222651.093750\n",
      "Train Epoch: 396 [39424/54000 (73%)] Loss: -221584.296875\n",
      "Train Epoch: 396 [40832/54000 (76%)] Loss: -222428.265625\n",
      "Train Epoch: 396 [42240/54000 (78%)] Loss: -221544.437500\n",
      "Train Epoch: 396 [43648/54000 (81%)] Loss: -221108.093750\n",
      "Train Epoch: 396 [45056/54000 (83%)] Loss: -222198.187500\n",
      "Train Epoch: 396 [46464/54000 (86%)] Loss: -221653.906250\n",
      "Train Epoch: 396 [47872/54000 (89%)] Loss: -229726.968750\n",
      "Train Epoch: 396 [49280/54000 (91%)] Loss: -228156.375000\n",
      "Train Epoch: 396 [50688/54000 (94%)] Loss: -248437.718750\n",
      "Train Epoch: 396 [52096/54000 (96%)] Loss: -224226.765625\n",
      "    epoch          : 396\n",
      "    loss           : -228265.03812799044\n",
      "    val_loss       : -230272.18323051638\n",
      "Train Epoch: 397 [0/54000 (0%)] Loss: -250140.218750\n",
      "Train Epoch: 397 [1408/54000 (3%)] Loss: -231366.546875\n",
      "Train Epoch: 397 [2816/54000 (5%)] Loss: -221884.062500\n",
      "Train Epoch: 397 [4224/54000 (8%)] Loss: -228136.218750\n",
      "Train Epoch: 397 [5632/54000 (10%)] Loss: -249842.078125\n",
      "Train Epoch: 397 [7040/54000 (13%)] Loss: -221103.000000\n",
      "Train Epoch: 397 [8448/54000 (16%)] Loss: -220070.328125\n",
      "Train Epoch: 397 [9856/54000 (18%)] Loss: -222031.546875\n",
      "Train Epoch: 397 [11264/54000 (21%)] Loss: -229558.562500\n",
      "Train Epoch: 397 [12672/54000 (23%)] Loss: -223907.906250\n",
      "Train Epoch: 397 [14080/54000 (26%)] Loss: -232118.468750\n",
      "Train Epoch: 397 [15488/54000 (29%)] Loss: -250314.843750\n",
      "Train Epoch: 397 [16896/54000 (31%)] Loss: -224170.156250\n",
      "Train Epoch: 397 [18304/54000 (34%)] Loss: -229608.421875\n",
      "Train Epoch: 397 [19712/54000 (37%)] Loss: -230090.203125\n",
      "Train Epoch: 397 [21120/54000 (39%)] Loss: -219717.921875\n",
      "Train Epoch: 397 [22528/54000 (42%)] Loss: -229528.437500\n",
      "Train Epoch: 397 [23936/54000 (44%)] Loss: -226911.781250\n",
      "Train Epoch: 397 [25344/54000 (47%)] Loss: -231917.015625\n",
      "Train Epoch: 397 [26752/54000 (50%)] Loss: -226291.843750\n",
      "Train Epoch: 397 [28160/54000 (52%)] Loss: -231206.828125\n",
      "Train Epoch: 397 [29568/54000 (55%)] Loss: -230994.546875\n",
      "Train Epoch: 397 [30976/54000 (57%)] Loss: -219269.015625\n",
      "Train Epoch: 397 [32384/54000 (60%)] Loss: -217867.875000\n",
      "Train Epoch: 397 [33792/54000 (63%)] Loss: -249776.812500\n",
      "Train Epoch: 397 [35200/54000 (65%)] Loss: -233397.062500\n",
      "Train Epoch: 397 [36608/54000 (68%)] Loss: -228725.093750\n",
      "Train Epoch: 397 [38016/54000 (70%)] Loss: -224911.609375\n",
      "Train Epoch: 397 [39424/54000 (73%)] Loss: -223540.656250\n",
      "Train Epoch: 397 [40832/54000 (76%)] Loss: -224679.921875\n",
      "Train Epoch: 397 [42240/54000 (78%)] Loss: -226847.765625\n",
      "Train Epoch: 397 [43648/54000 (81%)] Loss: -225596.531250\n",
      "Train Epoch: 397 [45056/54000 (83%)] Loss: -248723.781250\n",
      "Train Epoch: 397 [46464/54000 (86%)] Loss: -230279.156250\n",
      "Train Epoch: 397 [47872/54000 (89%)] Loss: -222610.156250\n",
      "Train Epoch: 397 [49280/54000 (91%)] Loss: -230415.656250\n",
      "Train Epoch: 397 [50688/54000 (94%)] Loss: -225733.078125\n",
      "Train Epoch: 397 [52096/54000 (96%)] Loss: -218137.500000\n",
      "    epoch          : 397\n",
      "    loss           : -228400.3971291866\n",
      "    val_loss       : -230620.2048220751\n",
      "Train Epoch: 398 [0/54000 (0%)] Loss: -248724.625000\n",
      "Train Epoch: 398 [1408/54000 (3%)] Loss: -230374.718750\n",
      "Train Epoch: 398 [2816/54000 (5%)] Loss: -231437.437500\n",
      "Train Epoch: 398 [4224/54000 (8%)] Loss: -231278.015625\n",
      "Train Epoch: 398 [5632/54000 (10%)] Loss: -233150.625000\n",
      "Train Epoch: 398 [7040/54000 (13%)] Loss: -248556.609375\n",
      "Train Epoch: 398 [8448/54000 (16%)] Loss: -231675.265625\n",
      "Train Epoch: 398 [9856/54000 (18%)] Loss: -227968.890625\n",
      "Train Epoch: 398 [11264/54000 (21%)] Loss: -224800.656250\n",
      "Train Epoch: 398 [12672/54000 (23%)] Loss: -225337.828125\n",
      "Train Epoch: 398 [14080/54000 (26%)] Loss: -223081.609375\n",
      "Train Epoch: 398 [15488/54000 (29%)] Loss: -225517.531250\n",
      "Train Epoch: 398 [16896/54000 (31%)] Loss: -219080.625000\n",
      "Train Epoch: 398 [18304/54000 (34%)] Loss: -223333.562500\n",
      "Train Epoch: 398 [19712/54000 (37%)] Loss: -229864.734375\n",
      "Train Epoch: 398 [21120/54000 (39%)] Loss: -221784.843750\n",
      "Train Epoch: 398 [22528/54000 (42%)] Loss: -250148.187500\n",
      "Train Epoch: 398 [23936/54000 (44%)] Loss: -233173.359375\n",
      "Train Epoch: 398 [25344/54000 (47%)] Loss: -222140.390625\n",
      "Train Epoch: 398 [26752/54000 (50%)] Loss: -231351.687500\n",
      "Train Epoch: 398 [28160/54000 (52%)] Loss: -220896.687500\n",
      "Train Epoch: 398 [29568/54000 (55%)] Loss: -225797.250000\n",
      "Train Epoch: 398 [30976/54000 (57%)] Loss: -223713.421875\n",
      "Train Epoch: 398 [32384/54000 (60%)] Loss: -225202.453125\n",
      "Train Epoch: 398 [33792/54000 (63%)] Loss: -219286.687500\n",
      "Train Epoch: 398 [35200/54000 (65%)] Loss: -232679.109375\n",
      "Train Epoch: 398 [36608/54000 (68%)] Loss: -232794.812500\n",
      "Train Epoch: 398 [38016/54000 (70%)] Loss: -231641.328125\n",
      "Train Epoch: 398 [39424/54000 (73%)] Loss: -230918.171875\n",
      "Train Epoch: 398 [40832/54000 (76%)] Loss: -224924.843750\n",
      "Train Epoch: 398 [42240/54000 (78%)] Loss: -250392.171875\n",
      "Train Epoch: 398 [43648/54000 (81%)] Loss: -226995.656250\n",
      "Train Epoch: 398 [45056/54000 (83%)] Loss: -231182.281250\n",
      "Train Epoch: 398 [46464/54000 (86%)] Loss: -223239.218750\n",
      "Train Epoch: 398 [47872/54000 (89%)] Loss: -226859.437500\n",
      "Train Epoch: 398 [49280/54000 (91%)] Loss: -230079.812500\n",
      "Train Epoch: 398 [50688/54000 (94%)] Loss: -230595.093750\n",
      "Train Epoch: 398 [52096/54000 (96%)] Loss: -249888.546875\n",
      "    epoch          : 398\n",
      "    loss           : -228308.93264055025\n",
      "    val_loss       : -230694.51792349466\n",
      "Train Epoch: 399 [0/54000 (0%)] Loss: -249783.578125\n",
      "Train Epoch: 399 [1408/54000 (3%)] Loss: -226656.328125\n",
      "Train Epoch: 399 [2816/54000 (5%)] Loss: -228912.406250\n",
      "Train Epoch: 399 [4224/54000 (8%)] Loss: -230615.843750\n",
      "Train Epoch: 399 [5632/54000 (10%)] Loss: -232235.078125\n",
      "Train Epoch: 399 [7040/54000 (13%)] Loss: -248835.265625\n",
      "Train Epoch: 399 [8448/54000 (16%)] Loss: -250393.328125\n",
      "Train Epoch: 399 [9856/54000 (18%)] Loss: -217957.890625\n",
      "Train Epoch: 399 [11264/54000 (21%)] Loss: -232463.296875\n",
      "Train Epoch: 399 [12672/54000 (23%)] Loss: -248279.125000\n",
      "Train Epoch: 399 [14080/54000 (26%)] Loss: -232268.250000\n",
      "Train Epoch: 399 [15488/54000 (29%)] Loss: -231396.015625\n",
      "Train Epoch: 399 [16896/54000 (31%)] Loss: -221060.656250\n",
      "Train Epoch: 399 [18304/54000 (34%)] Loss: -221984.359375\n",
      "Train Epoch: 399 [19712/54000 (37%)] Loss: -219694.468750\n",
      "Train Epoch: 399 [21120/54000 (39%)] Loss: -224741.562500\n",
      "Train Epoch: 399 [22528/54000 (42%)] Loss: -221944.343750\n",
      "Train Epoch: 399 [23936/54000 (44%)] Loss: -227849.343750\n",
      "Train Epoch: 399 [25344/54000 (47%)] Loss: -225284.109375\n",
      "Train Epoch: 399 [26752/54000 (50%)] Loss: -248445.593750\n",
      "Train Epoch: 399 [28160/54000 (52%)] Loss: -230193.015625\n",
      "Train Epoch: 399 [29568/54000 (55%)] Loss: -230431.515625\n",
      "Train Epoch: 399 [30976/54000 (57%)] Loss: -249634.750000\n",
      "Train Epoch: 399 [32384/54000 (60%)] Loss: -217274.609375\n",
      "Train Epoch: 399 [33792/54000 (63%)] Loss: -226345.500000\n",
      "Train Epoch: 399 [35200/54000 (65%)] Loss: -230517.984375\n",
      "Train Epoch: 399 [36608/54000 (68%)] Loss: -223087.906250\n",
      "Train Epoch: 399 [38016/54000 (70%)] Loss: -208464.921875\n",
      "Train Epoch: 399 [39424/54000 (73%)] Loss: -232860.187500\n",
      "Train Epoch: 399 [40832/54000 (76%)] Loss: -230624.687500\n",
      "Train Epoch: 399 [42240/54000 (78%)] Loss: -232674.234375\n",
      "Train Epoch: 399 [43648/54000 (81%)] Loss: -223775.453125\n",
      "Train Epoch: 399 [45056/54000 (83%)] Loss: -225530.906250\n",
      "Train Epoch: 399 [46464/54000 (86%)] Loss: -227601.265625\n",
      "Train Epoch: 399 [47872/54000 (89%)] Loss: -229873.812500\n",
      "Train Epoch: 399 [49280/54000 (91%)] Loss: -225041.328125\n",
      "Train Epoch: 399 [50688/54000 (94%)] Loss: -232631.218750\n",
      "Train Epoch: 399 [52096/54000 (96%)] Loss: -249789.593750\n",
      "    epoch          : 399\n",
      "    loss           : -228290.74375747607\n",
      "    val_loss       : -230376.7908727134\n",
      "Train Epoch: 400 [0/54000 (0%)] Loss: -250905.046875\n",
      "Train Epoch: 400 [1408/54000 (3%)] Loss: -225060.281250\n",
      "Train Epoch: 400 [2816/54000 (5%)] Loss: -223789.656250\n",
      "Train Epoch: 400 [4224/54000 (8%)] Loss: -231414.031250\n",
      "Train Epoch: 400 [5632/54000 (10%)] Loss: -221422.218750\n",
      "Train Epoch: 400 [7040/54000 (13%)] Loss: -233226.515625\n",
      "Train Epoch: 400 [8448/54000 (16%)] Loss: -223744.828125\n",
      "Train Epoch: 400 [9856/54000 (18%)] Loss: -222000.703125\n",
      "Train Epoch: 400 [11264/54000 (21%)] Loss: -227452.078125\n",
      "Train Epoch: 400 [12672/54000 (23%)] Loss: -226583.203125\n",
      "Train Epoch: 400 [14080/54000 (26%)] Loss: -225659.671875\n",
      "Train Epoch: 400 [15488/54000 (29%)] Loss: -232483.296875\n",
      "Train Epoch: 400 [16896/54000 (31%)] Loss: -232718.796875\n",
      "Train Epoch: 400 [18304/54000 (34%)] Loss: -248345.062500\n",
      "Train Epoch: 400 [19712/54000 (37%)] Loss: -220264.484375\n",
      "Train Epoch: 400 [21120/54000 (39%)] Loss: -227864.406250\n",
      "Train Epoch: 400 [22528/54000 (42%)] Loss: -222183.890625\n",
      "Train Epoch: 400 [23936/54000 (44%)] Loss: -219675.031250\n",
      "Train Epoch: 400 [25344/54000 (47%)] Loss: -226848.812500\n",
      "Train Epoch: 400 [26752/54000 (50%)] Loss: -249113.703125\n",
      "Train Epoch: 400 [28160/54000 (52%)] Loss: -248497.296875\n",
      "Train Epoch: 400 [29568/54000 (55%)] Loss: -222851.906250\n",
      "Train Epoch: 400 [30976/54000 (57%)] Loss: -249891.437500\n",
      "Train Epoch: 400 [32384/54000 (60%)] Loss: -232665.562500\n",
      "Train Epoch: 400 [33792/54000 (63%)] Loss: -221747.421875\n",
      "Train Epoch: 400 [35200/54000 (65%)] Loss: -216660.921875\n",
      "Train Epoch: 400 [36608/54000 (68%)] Loss: -224750.437500\n",
      "Train Epoch: 400 [38016/54000 (70%)] Loss: -249587.437500\n",
      "Train Epoch: 400 [39424/54000 (73%)] Loss: -223458.328125\n",
      "Train Epoch: 400 [40832/54000 (76%)] Loss: -224738.859375\n",
      "Train Epoch: 400 [42240/54000 (78%)] Loss: -231647.421875\n",
      "Train Epoch: 400 [43648/54000 (81%)] Loss: -228190.687500\n",
      "Train Epoch: 400 [45056/54000 (83%)] Loss: -227619.968750\n",
      "Train Epoch: 400 [46464/54000 (86%)] Loss: -226566.625000\n",
      "Train Epoch: 400 [47872/54000 (89%)] Loss: -226734.625000\n",
      "Train Epoch: 400 [49280/54000 (91%)] Loss: -225728.500000\n",
      "Train Epoch: 400 [50688/54000 (94%)] Loss: -220473.031250\n",
      "Train Epoch: 400 [52096/54000 (96%)] Loss: -220828.843750\n",
      "    epoch          : 400\n",
      "    loss           : -228449.00594348085\n",
      "    val_loss       : -230520.76906678735\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0508_092801/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [0/54000 (0%)] Loss: -249403.437500\n",
      "Train Epoch: 401 [1408/54000 (3%)] Loss: -231567.343750\n",
      "Train Epoch: 401 [2816/54000 (5%)] Loss: -221261.031250\n",
      "Train Epoch: 401 [4224/54000 (8%)] Loss: -218613.437500\n",
      "Train Epoch: 401 [5632/54000 (10%)] Loss: -230104.390625\n",
      "Train Epoch: 401 [7040/54000 (13%)] Loss: -222569.625000\n",
      "Train Epoch: 401 [8448/54000 (16%)] Loss: -220868.875000\n",
      "Train Epoch: 401 [9856/54000 (18%)] Loss: -226352.093750\n",
      "Train Epoch: 401 [11264/54000 (21%)] Loss: -227103.078125\n",
      "Train Epoch: 401 [12672/54000 (23%)] Loss: -229749.921875\n",
      "Train Epoch: 401 [14080/54000 (26%)] Loss: -229935.796875\n",
      "Train Epoch: 401 [15488/54000 (29%)] Loss: -230077.593750\n",
      "Train Epoch: 401 [16896/54000 (31%)] Loss: -223671.250000\n",
      "Train Epoch: 401 [18304/54000 (34%)] Loss: -231010.062500\n",
      "Train Epoch: 401 [19712/54000 (37%)] Loss: -219805.609375\n",
      "Train Epoch: 401 [21120/54000 (39%)] Loss: -223171.281250\n",
      "Train Epoch: 401 [22528/54000 (42%)] Loss: -209663.093750\n",
      "Train Epoch: 401 [23936/54000 (44%)] Loss: -225759.171875\n",
      "Train Epoch: 401 [25344/54000 (47%)] Loss: -230771.562500\n",
      "Train Epoch: 401 [26752/54000 (50%)] Loss: -250688.781250\n",
      "Train Epoch: 401 [28160/54000 (52%)] Loss: -226387.593750\n",
      "Train Epoch: 401 [29568/54000 (55%)] Loss: -229889.515625\n",
      "Train Epoch: 401 [30976/54000 (57%)] Loss: -222025.609375\n",
      "Train Epoch: 401 [32384/54000 (60%)] Loss: -227813.000000\n",
      "Train Epoch: 401 [33792/54000 (63%)] Loss: -223866.515625\n",
      "Train Epoch: 401 [35200/54000 (65%)] Loss: -220979.562500\n",
      "Train Epoch: 401 [36608/54000 (68%)] Loss: -219012.890625\n",
      "Train Epoch: 401 [38016/54000 (70%)] Loss: -223841.218750\n",
      "Train Epoch: 401 [39424/54000 (73%)] Loss: -222994.953125\n",
      "Train Epoch: 401 [40832/54000 (76%)] Loss: -231098.968750\n",
      "Train Epoch: 401 [42240/54000 (78%)] Loss: -220539.984375\n",
      "Train Epoch: 401 [43648/54000 (81%)] Loss: -228445.062500\n",
      "Train Epoch: 401 [45056/54000 (83%)] Loss: -226629.500000\n",
      "Train Epoch: 401 [46464/54000 (86%)] Loss: -226531.750000\n",
      "Train Epoch: 401 [47872/54000 (89%)] Loss: -228567.937500\n",
      "Train Epoch: 401 [49280/54000 (91%)] Loss: -214884.312500\n",
      "Train Epoch: 401 [50688/54000 (94%)] Loss: -250063.781250\n",
      "Train Epoch: 401 [52096/54000 (96%)] Loss: -231674.640625\n",
      "    epoch          : 401\n",
      "    loss           : -228273.91133373205\n",
      "    val_loss       : -230155.2758134051\n",
      "Train Epoch: 402 [0/54000 (0%)] Loss: -225179.171875\n",
      "Train Epoch: 402 [1408/54000 (3%)] Loss: -229993.125000\n",
      "Train Epoch: 402 [2816/54000 (5%)] Loss: -228355.250000\n",
      "Train Epoch: 402 [4224/54000 (8%)] Loss: -217313.578125\n",
      "Train Epoch: 402 [5632/54000 (10%)] Loss: -218713.203125\n",
      "Train Epoch: 402 [7040/54000 (13%)] Loss: -231478.968750\n",
      "Train Epoch: 402 [8448/54000 (16%)] Loss: -218683.968750\n",
      "Train Epoch: 402 [9856/54000 (18%)] Loss: -218707.000000\n",
      "Train Epoch: 402 [11264/54000 (21%)] Loss: -227912.421875\n",
      "Train Epoch: 402 [12672/54000 (23%)] Loss: -231642.578125\n",
      "Train Epoch: 402 [14080/54000 (26%)] Loss: -231169.625000\n",
      "Train Epoch: 402 [15488/54000 (29%)] Loss: -223168.859375\n",
      "Train Epoch: 402 [16896/54000 (31%)] Loss: -250047.828125\n",
      "Train Epoch: 402 [18304/54000 (34%)] Loss: -232223.953125\n",
      "Train Epoch: 402 [19712/54000 (37%)] Loss: -223797.250000\n",
      "Train Epoch: 402 [21120/54000 (39%)] Loss: -228988.750000\n",
      "Train Epoch: 402 [22528/54000 (42%)] Loss: -221503.625000\n",
      "Train Epoch: 402 [23936/54000 (44%)] Loss: -222121.500000\n",
      "Train Epoch: 402 [25344/54000 (47%)] Loss: -226766.093750\n",
      "Train Epoch: 402 [26752/54000 (50%)] Loss: -228106.500000\n",
      "Train Epoch: 402 [28160/54000 (52%)] Loss: -250652.046875\n",
      "Train Epoch: 402 [29568/54000 (55%)] Loss: -218930.953125\n",
      "Train Epoch: 402 [30976/54000 (57%)] Loss: -232027.125000\n",
      "Train Epoch: 402 [32384/54000 (60%)] Loss: -232193.375000\n",
      "Train Epoch: 402 [33792/54000 (63%)] Loss: -248650.796875\n",
      "Train Epoch: 402 [35200/54000 (65%)] Loss: -250253.296875\n",
      "Train Epoch: 402 [36608/54000 (68%)] Loss: -225688.906250\n",
      "Train Epoch: 402 [38016/54000 (70%)] Loss: -227301.734375\n",
      "Train Epoch: 402 [39424/54000 (73%)] Loss: -250117.531250\n",
      "Train Epoch: 402 [40832/54000 (76%)] Loss: -220322.562500\n",
      "Train Epoch: 402 [42240/54000 (78%)] Loss: -227543.515625\n",
      "Train Epoch: 402 [43648/54000 (81%)] Loss: -226698.140625\n",
      "Train Epoch: 402 [45056/54000 (83%)] Loss: -223113.093750\n",
      "Train Epoch: 402 [46464/54000 (86%)] Loss: -231951.000000\n",
      "Train Epoch: 402 [47872/54000 (89%)] Loss: -227828.468750\n",
      "Train Epoch: 402 [49280/54000 (91%)] Loss: -222884.671875\n",
      "Train Epoch: 402 [50688/54000 (94%)] Loss: -222028.468750\n",
      "Train Epoch: 402 [52096/54000 (96%)] Loss: -218699.921875\n",
      "    epoch          : 402\n",
      "    loss           : -228371.84924491626\n",
      "    val_loss       : -230275.41371117567\n",
      "Train Epoch: 403 [0/54000 (0%)] Loss: -248948.421875\n",
      "Train Epoch: 403 [1408/54000 (3%)] Loss: -217661.406250\n",
      "Train Epoch: 403 [2816/54000 (5%)] Loss: -231081.500000\n",
      "Train Epoch: 403 [4224/54000 (8%)] Loss: -226257.437500\n",
      "Train Epoch: 403 [5632/54000 (10%)] Loss: -227914.640625\n",
      "Train Epoch: 403 [7040/54000 (13%)] Loss: -231079.828125\n",
      "Train Epoch: 403 [8448/54000 (16%)] Loss: -250858.609375\n",
      "Train Epoch: 403 [9856/54000 (18%)] Loss: -219427.109375\n",
      "Train Epoch: 403 [11264/54000 (21%)] Loss: -222176.375000\n",
      "Train Epoch: 403 [12672/54000 (23%)] Loss: -228645.453125\n",
      "Train Epoch: 403 [14080/54000 (26%)] Loss: -222680.875000\n",
      "Train Epoch: 403 [15488/54000 (29%)] Loss: -230572.515625\n",
      "Train Epoch: 403 [16896/54000 (31%)] Loss: -231411.156250\n",
      "Train Epoch: 403 [18304/54000 (34%)] Loss: -222534.656250\n",
      "Train Epoch: 403 [19712/54000 (37%)] Loss: -228256.187500\n",
      "Train Epoch: 403 [21120/54000 (39%)] Loss: -232377.453125\n",
      "Train Epoch: 403 [22528/54000 (42%)] Loss: -231173.953125\n",
      "Train Epoch: 403 [23936/54000 (44%)] Loss: -229745.625000\n",
      "Train Epoch: 403 [25344/54000 (47%)] Loss: -231896.531250\n",
      "Train Epoch: 403 [26752/54000 (50%)] Loss: -226207.906250\n",
      "Train Epoch: 403 [28160/54000 (52%)] Loss: -249705.687500\n",
      "Train Epoch: 403 [29568/54000 (55%)] Loss: -223813.781250\n",
      "Train Epoch: 403 [30976/54000 (57%)] Loss: -231019.562500\n",
      "Train Epoch: 403 [32384/54000 (60%)] Loss: -231675.218750\n",
      "Train Epoch: 403 [33792/54000 (63%)] Loss: -226080.156250\n",
      "Train Epoch: 403 [35200/54000 (65%)] Loss: -228574.109375\n",
      "Train Epoch: 403 [36608/54000 (68%)] Loss: -223055.828125\n",
      "Train Epoch: 403 [38016/54000 (70%)] Loss: -231992.421875\n",
      "Train Epoch: 403 [39424/54000 (73%)] Loss: -232713.625000\n",
      "Train Epoch: 403 [40832/54000 (76%)] Loss: -231045.609375\n",
      "Train Epoch: 403 [42240/54000 (78%)] Loss: -227553.828125\n",
      "Train Epoch: 403 [43648/54000 (81%)] Loss: -223379.625000\n",
      "Train Epoch: 403 [45056/54000 (83%)] Loss: -222544.109375\n",
      "Train Epoch: 403 [46464/54000 (86%)] Loss: -223529.796875\n",
      "Train Epoch: 403 [47872/54000 (89%)] Loss: -226120.718750\n",
      "Train Epoch: 403 [49280/54000 (91%)] Loss: -223860.578125\n",
      "Train Epoch: 403 [50688/54000 (94%)] Loss: -222730.640625\n",
      "Train Epoch: 403 [52096/54000 (96%)] Loss: -222529.453125\n",
      "    epoch          : 403\n",
      "    loss           : -228462.28304425837\n",
      "    val_loss       : -230681.91450314404\n",
      "Train Epoch: 404 [0/54000 (0%)] Loss: -225979.468750\n",
      "Train Epoch: 404 [1408/54000 (3%)] Loss: -248126.437500\n",
      "Train Epoch: 404 [2816/54000 (5%)] Loss: -250272.734375\n",
      "Train Epoch: 404 [4224/54000 (8%)] Loss: -222625.062500\n",
      "Train Epoch: 404 [5632/54000 (10%)] Loss: -223065.093750\n",
      "Train Epoch: 404 [7040/54000 (13%)] Loss: -222056.250000\n",
      "Train Epoch: 404 [8448/54000 (16%)] Loss: -224114.000000\n",
      "Train Epoch: 404 [9856/54000 (18%)] Loss: -217401.421875\n",
      "Train Epoch: 404 [11264/54000 (21%)] Loss: -222801.921875\n",
      "Train Epoch: 404 [12672/54000 (23%)] Loss: -221238.765625\n",
      "Train Epoch: 404 [14080/54000 (26%)] Loss: -250486.406250\n",
      "Train Epoch: 404 [15488/54000 (29%)] Loss: -232108.562500\n",
      "Train Epoch: 404 [16896/54000 (31%)] Loss: -229554.906250\n",
      "Train Epoch: 404 [18304/54000 (34%)] Loss: -250327.406250\n",
      "Train Epoch: 404 [19712/54000 (37%)] Loss: -227799.859375\n",
      "Train Epoch: 404 [21120/54000 (39%)] Loss: -231738.859375\n",
      "Train Epoch: 404 [22528/54000 (42%)] Loss: -230653.984375\n",
      "Train Epoch: 404 [23936/54000 (44%)] Loss: -223481.312500\n",
      "Train Epoch: 404 [25344/54000 (47%)] Loss: -231170.828125\n",
      "Train Epoch: 404 [26752/54000 (50%)] Loss: -220362.046875\n",
      "Train Epoch: 404 [28160/54000 (52%)] Loss: -225522.218750\n",
      "Train Epoch: 404 [29568/54000 (55%)] Loss: -226162.875000\n",
      "Train Epoch: 404 [30976/54000 (57%)] Loss: -222394.156250\n",
      "Train Epoch: 404 [32384/54000 (60%)] Loss: -225680.656250\n",
      "Train Epoch: 404 [33792/54000 (63%)] Loss: -230978.125000\n",
      "Train Epoch: 404 [35200/54000 (65%)] Loss: -249505.625000\n",
      "Train Epoch: 404 [36608/54000 (68%)] Loss: -225116.171875\n",
      "Train Epoch: 404 [38016/54000 (70%)] Loss: -225303.828125\n",
      "Train Epoch: 404 [39424/54000 (73%)] Loss: -219234.250000\n",
      "Train Epoch: 404 [40832/54000 (76%)] Loss: -227436.296875\n",
      "Train Epoch: 404 [42240/54000 (78%)] Loss: -223966.031250\n",
      "Train Epoch: 404 [43648/54000 (81%)] Loss: -249366.921875\n",
      "Train Epoch: 404 [45056/54000 (83%)] Loss: -223579.906250\n",
      "Train Epoch: 404 [46464/54000 (86%)] Loss: -221150.890625\n",
      "Train Epoch: 404 [47872/54000 (89%)] Loss: -222864.531250\n",
      "Train Epoch: 404 [49280/54000 (91%)] Loss: -219046.953125\n",
      "Train Epoch: 404 [50688/54000 (94%)] Loss: -250591.250000\n",
      "Train Epoch: 404 [52096/54000 (96%)] Loss: -218804.718750\n",
      "    epoch          : 404\n",
      "    loss           : -228452.6412604665\n",
      "    val_loss       : -230717.9292766292\n",
      "Train Epoch: 405 [0/54000 (0%)] Loss: -249937.765625\n",
      "Train Epoch: 405 [1408/54000 (3%)] Loss: -232716.500000\n",
      "Train Epoch: 405 [2816/54000 (5%)] Loss: -220713.187500\n",
      "Train Epoch: 405 [4224/54000 (8%)] Loss: -223752.062500\n",
      "Train Epoch: 405 [5632/54000 (10%)] Loss: -222195.109375\n",
      "Train Epoch: 405 [7040/54000 (13%)] Loss: -226873.406250\n",
      "Train Epoch: 405 [8448/54000 (16%)] Loss: -220982.546875\n",
      "Train Epoch: 405 [9856/54000 (18%)] Loss: -218518.734375\n",
      "Train Epoch: 405 [11264/54000 (21%)] Loss: -222807.218750\n",
      "Train Epoch: 405 [12672/54000 (23%)] Loss: -226716.921875\n",
      "Train Epoch: 405 [14080/54000 (26%)] Loss: -246068.234375\n",
      "Train Epoch: 405 [15488/54000 (29%)] Loss: -230100.281250\n",
      "Train Epoch: 405 [16896/54000 (31%)] Loss: -226163.812500\n",
      "Train Epoch: 405 [18304/54000 (34%)] Loss: -226619.406250\n",
      "Train Epoch: 405 [19712/54000 (37%)] Loss: -219437.828125\n",
      "Train Epoch: 405 [21120/54000 (39%)] Loss: -247115.609375\n",
      "Train Epoch: 405 [22528/54000 (42%)] Loss: -232425.406250\n",
      "Train Epoch: 405 [23936/54000 (44%)] Loss: -226788.015625\n",
      "Train Epoch: 405 [25344/54000 (47%)] Loss: -224224.765625\n",
      "Train Epoch: 405 [26752/54000 (50%)] Loss: -249009.500000\n",
      "Train Epoch: 405 [28160/54000 (52%)] Loss: -230292.656250\n",
      "Train Epoch: 405 [29568/54000 (55%)] Loss: -223253.812500\n",
      "Train Epoch: 405 [30976/54000 (57%)] Loss: -219900.109375\n",
      "Train Epoch: 405 [32384/54000 (60%)] Loss: -220856.093750\n",
      "Train Epoch: 405 [33792/54000 (63%)] Loss: -223375.500000\n",
      "Train Epoch: 405 [35200/54000 (65%)] Loss: -212284.578125\n",
      "Train Epoch: 405 [36608/54000 (68%)] Loss: -220042.562500\n",
      "Train Epoch: 405 [38016/54000 (70%)] Loss: -222098.812500\n",
      "Train Epoch: 405 [39424/54000 (73%)] Loss: -222661.593750\n",
      "Train Epoch: 405 [40832/54000 (76%)] Loss: -218805.046875\n",
      "Train Epoch: 405 [42240/54000 (78%)] Loss: -223780.609375\n",
      "Train Epoch: 405 [43648/54000 (81%)] Loss: -226394.906250\n",
      "Train Epoch: 405 [45056/54000 (83%)] Loss: -228018.093750\n",
      "Train Epoch: 405 [46464/54000 (86%)] Loss: -221253.703125\n",
      "Train Epoch: 405 [47872/54000 (89%)] Loss: -223086.125000\n",
      "Train Epoch: 405 [49280/54000 (91%)] Loss: -227871.828125\n",
      "Train Epoch: 405 [50688/54000 (94%)] Loss: -228714.781250\n",
      "Train Epoch: 405 [52096/54000 (96%)] Loss: -230403.953125\n",
      "    epoch          : 405\n",
      "    loss           : -228413.73018839714\n",
      "    val_loss       : -230380.09427996379\n",
      "Train Epoch: 406 [0/54000 (0%)] Loss: -249761.750000\n",
      "Train Epoch: 406 [1408/54000 (3%)] Loss: -219149.437500\n",
      "Train Epoch: 406 [2816/54000 (5%)] Loss: -230226.343750\n",
      "Train Epoch: 406 [4224/54000 (8%)] Loss: -223118.390625\n",
      "Train Epoch: 406 [5632/54000 (10%)] Loss: -249500.296875\n",
      "Train Epoch: 406 [7040/54000 (13%)] Loss: -219719.640625\n",
      "Train Epoch: 406 [8448/54000 (16%)] Loss: -223174.375000\n",
      "Train Epoch: 406 [9856/54000 (18%)] Loss: -229470.484375\n",
      "Train Epoch: 406 [11264/54000 (21%)] Loss: -221422.515625\n",
      "Train Epoch: 406 [12672/54000 (23%)] Loss: -220144.234375\n",
      "Train Epoch: 406 [14080/54000 (26%)] Loss: -220565.906250\n",
      "Train Epoch: 406 [15488/54000 (29%)] Loss: -224952.000000\n",
      "Train Epoch: 406 [16896/54000 (31%)] Loss: -224807.156250\n",
      "Train Epoch: 406 [18304/54000 (34%)] Loss: -221629.453125\n",
      "Train Epoch: 406 [19712/54000 (37%)] Loss: -224241.234375\n",
      "Train Epoch: 406 [21120/54000 (39%)] Loss: -249253.062500\n",
      "Train Epoch: 406 [22528/54000 (42%)] Loss: -217968.828125\n",
      "Train Epoch: 406 [23936/54000 (44%)] Loss: -221845.062500\n",
      "Train Epoch: 406 [25344/54000 (47%)] Loss: -227976.796875\n",
      "Train Epoch: 406 [26752/54000 (50%)] Loss: -223362.687500\n",
      "Train Epoch: 406 [28160/54000 (52%)] Loss: -219145.078125\n",
      "Train Epoch: 406 [29568/54000 (55%)] Loss: -223714.531250\n",
      "Train Epoch: 406 [30976/54000 (57%)] Loss: -228527.765625\n",
      "Train Epoch: 406 [32384/54000 (60%)] Loss: -230982.187500\n",
      "Train Epoch: 406 [33792/54000 (63%)] Loss: -242624.328125\n",
      "Train Epoch: 406 [35200/54000 (65%)] Loss: -222263.625000\n",
      "Train Epoch: 406 [36608/54000 (68%)] Loss: -222836.734375\n",
      "Train Epoch: 406 [38016/54000 (70%)] Loss: -219236.156250\n",
      "Train Epoch: 406 [39424/54000 (73%)] Loss: -232664.875000\n",
      "Train Epoch: 406 [40832/54000 (76%)] Loss: -251151.796875\n",
      "Train Epoch: 406 [42240/54000 (78%)] Loss: -222326.875000\n",
      "Train Epoch: 406 [43648/54000 (81%)] Loss: -229582.281250\n",
      "Train Epoch: 406 [45056/54000 (83%)] Loss: -214813.312500\n",
      "Train Epoch: 406 [46464/54000 (86%)] Loss: -223214.859375\n",
      "Train Epoch: 406 [47872/54000 (89%)] Loss: -221699.984375\n",
      "Train Epoch: 406 [49280/54000 (91%)] Loss: -226346.093750\n",
      "Train Epoch: 406 [50688/54000 (94%)] Loss: -231669.546875\n",
      "Train Epoch: 406 [52096/54000 (96%)] Loss: -221948.296875\n",
      "    epoch          : 406\n",
      "    loss           : -228262.26898923446\n",
      "    val_loss       : -230533.40544612234\n",
      "Train Epoch: 407 [0/54000 (0%)] Loss: -249937.156250\n",
      "Train Epoch: 407 [1408/54000 (3%)] Loss: -249519.562500\n",
      "Train Epoch: 407 [2816/54000 (5%)] Loss: -227416.468750\n",
      "Train Epoch: 407 [4224/54000 (8%)] Loss: -226972.468750\n",
      "Train Epoch: 407 [5632/54000 (10%)] Loss: -225937.750000\n",
      "Train Epoch: 407 [7040/54000 (13%)] Loss: -222522.734375\n",
      "Train Epoch: 407 [8448/54000 (16%)] Loss: -231491.218750\n",
      "Train Epoch: 407 [9856/54000 (18%)] Loss: -230941.140625\n",
      "Train Epoch: 407 [11264/54000 (21%)] Loss: -226984.546875\n",
      "Train Epoch: 407 [12672/54000 (23%)] Loss: -222118.953125\n",
      "Train Epoch: 407 [14080/54000 (26%)] Loss: -224309.250000\n",
      "Train Epoch: 407 [15488/54000 (29%)] Loss: -221205.437500\n",
      "Train Epoch: 407 [16896/54000 (31%)] Loss: -248351.421875\n",
      "Train Epoch: 407 [18304/54000 (34%)] Loss: -222966.109375\n",
      "Train Epoch: 407 [19712/54000 (37%)] Loss: -225904.468750\n",
      "Train Epoch: 407 [21120/54000 (39%)] Loss: -232057.484375\n",
      "Train Epoch: 407 [22528/54000 (42%)] Loss: -225552.359375\n",
      "Train Epoch: 407 [23936/54000 (44%)] Loss: -247486.062500\n",
      "Train Epoch: 407 [25344/54000 (47%)] Loss: -219794.578125\n",
      "Train Epoch: 407 [26752/54000 (50%)] Loss: -222015.125000\n",
      "Train Epoch: 407 [28160/54000 (52%)] Loss: -250167.406250\n",
      "Train Epoch: 407 [29568/54000 (55%)] Loss: -230623.968750\n",
      "Train Epoch: 407 [30976/54000 (57%)] Loss: -231528.375000\n",
      "Train Epoch: 407 [32384/54000 (60%)] Loss: -233192.312500\n",
      "Train Epoch: 407 [33792/54000 (63%)] Loss: -220507.921875\n",
      "Train Epoch: 407 [35200/54000 (65%)] Loss: -249820.359375\n",
      "Train Epoch: 407 [36608/54000 (68%)] Loss: -229656.578125\n",
      "Train Epoch: 407 [38016/54000 (70%)] Loss: -222103.609375\n",
      "Train Epoch: 407 [39424/54000 (73%)] Loss: -232035.125000\n",
      "Train Epoch: 407 [40832/54000 (76%)] Loss: -219357.390625\n",
      "Train Epoch: 407 [42240/54000 (78%)] Loss: -223297.593750\n",
      "Train Epoch: 407 [43648/54000 (81%)] Loss: -224855.281250\n",
      "Train Epoch: 407 [45056/54000 (83%)] Loss: -223821.765625\n",
      "Train Epoch: 407 [46464/54000 (86%)] Loss: -222331.203125\n",
      "Train Epoch: 407 [47872/54000 (89%)] Loss: -226630.500000\n",
      "Train Epoch: 407 [49280/54000 (91%)] Loss: -230307.796875\n",
      "Train Epoch: 407 [50688/54000 (94%)] Loss: -223391.843750\n",
      "Train Epoch: 407 [52096/54000 (96%)] Loss: -219857.687500\n",
      "    epoch          : 407\n",
      "    loss           : -228433.12970992824\n",
      "    val_loss       : -230323.4063452744\n",
      "Train Epoch: 408 [0/54000 (0%)] Loss: -218656.718750\n",
      "Train Epoch: 408 [1408/54000 (3%)] Loss: -248307.031250\n",
      "Train Epoch: 408 [2816/54000 (5%)] Loss: -231983.187500\n",
      "Train Epoch: 408 [4224/54000 (8%)] Loss: -219317.328125\n",
      "Train Epoch: 408 [5632/54000 (10%)] Loss: -232637.281250\n",
      "Train Epoch: 408 [7040/54000 (13%)] Loss: -223336.546875\n",
      "Train Epoch: 408 [8448/54000 (16%)] Loss: -219872.062500\n",
      "Train Epoch: 408 [9856/54000 (18%)] Loss: -224021.187500\n",
      "Train Epoch: 408 [11264/54000 (21%)] Loss: -225828.437500\n",
      "Train Epoch: 408 [12672/54000 (23%)] Loss: -227085.984375\n",
      "Train Epoch: 408 [14080/54000 (26%)] Loss: -231969.500000\n",
      "Train Epoch: 408 [15488/54000 (29%)] Loss: -223018.656250\n",
      "Train Epoch: 408 [16896/54000 (31%)] Loss: -224053.156250\n",
      "Train Epoch: 408 [18304/54000 (34%)] Loss: -223610.328125\n",
      "Train Epoch: 408 [19712/54000 (37%)] Loss: -227306.593750\n",
      "Train Epoch: 408 [21120/54000 (39%)] Loss: -223479.687500\n",
      "Train Epoch: 408 [22528/54000 (42%)] Loss: -220655.171875\n",
      "Train Epoch: 408 [23936/54000 (44%)] Loss: -222047.390625\n",
      "Train Epoch: 408 [25344/54000 (47%)] Loss: -232719.250000\n",
      "Train Epoch: 408 [26752/54000 (50%)] Loss: -231927.593750\n",
      "Train Epoch: 408 [28160/54000 (52%)] Loss: -232361.453125\n",
      "Train Epoch: 408 [29568/54000 (55%)] Loss: -221501.937500\n",
      "Train Epoch: 408 [30976/54000 (57%)] Loss: -227611.562500\n",
      "Train Epoch: 408 [32384/54000 (60%)] Loss: -229231.000000\n",
      "Train Epoch: 408 [33792/54000 (63%)] Loss: -218610.187500\n",
      "Train Epoch: 408 [35200/54000 (65%)] Loss: -227727.625000\n",
      "Train Epoch: 408 [36608/54000 (68%)] Loss: -225795.812500\n",
      "Train Epoch: 408 [38016/54000 (70%)] Loss: -250954.453125\n",
      "Train Epoch: 408 [39424/54000 (73%)] Loss: -230281.296875\n",
      "Train Epoch: 408 [40832/54000 (76%)] Loss: -232130.093750\n",
      "Train Epoch: 408 [42240/54000 (78%)] Loss: -231382.328125\n",
      "Train Epoch: 408 [43648/54000 (81%)] Loss: -231054.859375\n",
      "Train Epoch: 408 [45056/54000 (83%)] Loss: -223892.750000\n",
      "Train Epoch: 408 [46464/54000 (86%)] Loss: -230644.703125\n",
      "Train Epoch: 408 [47872/54000 (89%)] Loss: -224639.750000\n",
      "Train Epoch: 408 [49280/54000 (91%)] Loss: -218728.531250\n",
      "Train Epoch: 408 [50688/54000 (94%)] Loss: -247570.781250\n",
      "Train Epoch: 408 [52096/54000 (96%)] Loss: -220029.406250\n",
      "    epoch          : 408\n",
      "    loss           : -228457.83791866028\n",
      "    val_loss       : -230070.38982112234\n",
      "Train Epoch: 409 [0/54000 (0%)] Loss: -248646.359375\n",
      "Train Epoch: 409 [1408/54000 (3%)] Loss: -233074.218750\n",
      "Train Epoch: 409 [2816/54000 (5%)] Loss: -222322.656250\n",
      "Train Epoch: 409 [4224/54000 (8%)] Loss: -226631.562500\n",
      "Train Epoch: 409 [5632/54000 (10%)] Loss: -217813.421875\n",
      "Train Epoch: 409 [7040/54000 (13%)] Loss: -224048.609375\n",
      "Train Epoch: 409 [8448/54000 (16%)] Loss: -223516.609375\n",
      "Train Epoch: 409 [9856/54000 (18%)] Loss: -225433.734375\n",
      "Train Epoch: 409 [11264/54000 (21%)] Loss: -228673.718750\n",
      "Train Epoch: 409 [12672/54000 (23%)] Loss: -226553.234375\n",
      "Train Epoch: 409 [14080/54000 (26%)] Loss: -232389.562500\n",
      "Train Epoch: 409 [15488/54000 (29%)] Loss: -250316.906250\n",
      "Train Epoch: 409 [16896/54000 (31%)] Loss: -225542.750000\n",
      "Train Epoch: 409 [18304/54000 (34%)] Loss: -222383.812500\n",
      "Train Epoch: 409 [19712/54000 (37%)] Loss: -250750.625000\n",
      "Train Epoch: 409 [21120/54000 (39%)] Loss: -226574.015625\n",
      "Train Epoch: 409 [22528/54000 (42%)] Loss: -223142.015625\n",
      "Train Epoch: 409 [23936/54000 (44%)] Loss: -228471.406250\n",
      "Train Epoch: 409 [25344/54000 (47%)] Loss: -229679.234375\n",
      "Train Epoch: 409 [26752/54000 (50%)] Loss: -231273.140625\n",
      "Train Epoch: 409 [28160/54000 (52%)] Loss: -230058.171875\n",
      "Train Epoch: 409 [29568/54000 (55%)] Loss: -228637.640625\n",
      "Train Epoch: 409 [30976/54000 (57%)] Loss: -226455.593750\n",
      "Train Epoch: 409 [32384/54000 (60%)] Loss: -231918.578125\n",
      "Train Epoch: 409 [33792/54000 (63%)] Loss: -249321.109375\n",
      "Train Epoch: 409 [35200/54000 (65%)] Loss: -217707.937500\n",
      "Train Epoch: 409 [36608/54000 (68%)] Loss: -226898.921875\n",
      "Train Epoch: 409 [38016/54000 (70%)] Loss: -222445.359375\n",
      "Train Epoch: 409 [39424/54000 (73%)] Loss: -227157.906250\n",
      "Train Epoch: 409 [40832/54000 (76%)] Loss: -227479.796875\n",
      "Train Epoch: 409 [42240/54000 (78%)] Loss: -231917.484375\n",
      "Train Epoch: 409 [43648/54000 (81%)] Loss: -231740.265625\n",
      "Train Epoch: 409 [45056/54000 (83%)] Loss: -225731.421875\n",
      "Train Epoch: 409 [46464/54000 (86%)] Loss: -225646.531250\n",
      "Train Epoch: 409 [47872/54000 (89%)] Loss: -227808.546875\n",
      "Train Epoch: 409 [49280/54000 (91%)] Loss: -230060.937500\n",
      "Train Epoch: 409 [50688/54000 (94%)] Loss: -228383.625000\n",
      "Train Epoch: 409 [52096/54000 (96%)] Loss: -231022.265625\n",
      "    epoch          : 409\n",
      "    loss           : -228613.345506878\n",
      "    val_loss       : -230305.71624904725\n",
      "Train Epoch: 410 [0/54000 (0%)] Loss: -250415.906250\n",
      "Train Epoch: 410 [1408/54000 (3%)] Loss: -223320.312500\n",
      "Train Epoch: 410 [2816/54000 (5%)] Loss: -217407.031250\n",
      "Train Epoch: 410 [4224/54000 (8%)] Loss: -218466.015625\n",
      "Train Epoch: 410 [5632/54000 (10%)] Loss: -222200.312500\n",
      "Train Epoch: 410 [7040/54000 (13%)] Loss: -232414.156250\n",
      "Train Epoch: 410 [8448/54000 (16%)] Loss: -221313.484375\n",
      "Train Epoch: 410 [9856/54000 (18%)] Loss: -220688.703125\n",
      "Train Epoch: 410 [11264/54000 (21%)] Loss: -224083.812500\n",
      "Train Epoch: 410 [12672/54000 (23%)] Loss: -249092.500000\n",
      "Train Epoch: 410 [14080/54000 (26%)] Loss: -229352.562500\n",
      "Train Epoch: 410 [15488/54000 (29%)] Loss: -231487.234375\n",
      "Train Epoch: 410 [16896/54000 (31%)] Loss: -229769.718750\n",
      "Train Epoch: 410 [18304/54000 (34%)] Loss: -217941.296875\n",
      "Train Epoch: 410 [19712/54000 (37%)] Loss: -222078.109375\n",
      "Train Epoch: 410 [21120/54000 (39%)] Loss: -232443.671875\n",
      "Train Epoch: 410 [22528/54000 (42%)] Loss: -223615.312500\n",
      "Train Epoch: 410 [23936/54000 (44%)] Loss: -230755.500000\n",
      "Train Epoch: 410 [25344/54000 (47%)] Loss: -222953.765625\n",
      "Train Epoch: 410 [26752/54000 (50%)] Loss: -232415.062500\n",
      "Train Epoch: 410 [28160/54000 (52%)] Loss: -233359.656250\n",
      "Train Epoch: 410 [29568/54000 (55%)] Loss: -231322.500000\n",
      "Train Epoch: 410 [30976/54000 (57%)] Loss: -223940.375000\n",
      "Train Epoch: 410 [32384/54000 (60%)] Loss: -226135.937500\n",
      "Train Epoch: 410 [33792/54000 (63%)] Loss: -226564.765625\n",
      "Train Epoch: 410 [35200/54000 (65%)] Loss: -220699.250000\n",
      "Train Epoch: 410 [36608/54000 (68%)] Loss: -221829.531250\n",
      "Train Epoch: 410 [38016/54000 (70%)] Loss: -248663.687500\n",
      "Train Epoch: 410 [39424/54000 (73%)] Loss: -232203.218750\n",
      "Train Epoch: 410 [40832/54000 (76%)] Loss: -227194.453125\n",
      "Train Epoch: 410 [42240/54000 (78%)] Loss: -232255.156250\n",
      "Train Epoch: 410 [43648/54000 (81%)] Loss: -227355.140625\n",
      "Train Epoch: 410 [45056/54000 (83%)] Loss: -227189.437500\n",
      "Train Epoch: 410 [46464/54000 (86%)] Loss: -221239.328125\n",
      "Train Epoch: 410 [47872/54000 (89%)] Loss: -218819.953125\n",
      "Train Epoch: 410 [49280/54000 (91%)] Loss: -250344.625000\n",
      "Train Epoch: 410 [50688/54000 (94%)] Loss: -226889.062500\n",
      "Train Epoch: 410 [52096/54000 (96%)] Loss: -231259.546875\n",
      "    epoch          : 410\n",
      "    loss           : -228481.37085077752\n",
      "    val_loss       : -230348.50312619092\n",
      "Train Epoch: 411 [0/54000 (0%)] Loss: -223617.671875\n",
      "Train Epoch: 411 [1408/54000 (3%)] Loss: -221856.968750\n",
      "Train Epoch: 411 [2816/54000 (5%)] Loss: -225778.187500\n",
      "Train Epoch: 411 [4224/54000 (8%)] Loss: -228772.546875\n",
      "Train Epoch: 411 [5632/54000 (10%)] Loss: -229534.156250\n",
      "Train Epoch: 411 [7040/54000 (13%)] Loss: -232197.359375\n",
      "Train Epoch: 411 [8448/54000 (16%)] Loss: -222350.781250\n",
      "Train Epoch: 411 [9856/54000 (18%)] Loss: -223992.656250\n",
      "Train Epoch: 411 [11264/54000 (21%)] Loss: -228998.531250\n",
      "Train Epoch: 411 [12672/54000 (23%)] Loss: -219201.734375\n",
      "Train Epoch: 411 [14080/54000 (26%)] Loss: -225825.437500\n",
      "Train Epoch: 411 [15488/54000 (29%)] Loss: -226523.765625\n",
      "Train Epoch: 411 [16896/54000 (31%)] Loss: -250601.937500\n",
      "Train Epoch: 411 [18304/54000 (34%)] Loss: -232423.484375\n",
      "Train Epoch: 411 [19712/54000 (37%)] Loss: -221991.531250\n",
      "Train Epoch: 411 [21120/54000 (39%)] Loss: -249094.531250\n",
      "Train Epoch: 411 [22528/54000 (42%)] Loss: -249215.703125\n",
      "Train Epoch: 411 [23936/54000 (44%)] Loss: -223111.875000\n",
      "Train Epoch: 411 [25344/54000 (47%)] Loss: -224426.125000\n",
      "Train Epoch: 411 [26752/54000 (50%)] Loss: -244481.281250\n",
      "Train Epoch: 411 [28160/54000 (52%)] Loss: -220555.625000\n",
      "Train Epoch: 411 [29568/54000 (55%)] Loss: -218587.625000\n",
      "Train Epoch: 411 [30976/54000 (57%)] Loss: -232358.046875\n",
      "Train Epoch: 411 [32384/54000 (60%)] Loss: -251040.312500\n",
      "Train Epoch: 411 [33792/54000 (63%)] Loss: -221474.937500\n",
      "Train Epoch: 411 [35200/54000 (65%)] Loss: -223100.281250\n",
      "Train Epoch: 411 [36608/54000 (68%)] Loss: -222615.890625\n",
      "Train Epoch: 411 [38016/54000 (70%)] Loss: -232175.750000\n",
      "Train Epoch: 411 [39424/54000 (73%)] Loss: -220134.203125\n",
      "Train Epoch: 411 [40832/54000 (76%)] Loss: -222416.343750\n",
      "Train Epoch: 411 [42240/54000 (78%)] Loss: -224006.890625\n",
      "Train Epoch: 411 [43648/54000 (81%)] Loss: -229922.375000\n",
      "Train Epoch: 411 [45056/54000 (83%)] Loss: -224942.843750\n",
      "Train Epoch: 411 [46464/54000 (86%)] Loss: -226123.843750\n",
      "Train Epoch: 411 [47872/54000 (89%)] Loss: -228557.781250\n",
      "Train Epoch: 411 [49280/54000 (91%)] Loss: -222463.062500\n",
      "Train Epoch: 411 [50688/54000 (94%)] Loss: -219491.781250\n",
      "Train Epoch: 411 [52096/54000 (96%)] Loss: -222003.843750\n",
      "    epoch          : 411\n",
      "    loss           : -228376.92086572965\n",
      "    val_loss       : -230567.14962842988\n",
      "Train Epoch: 412 [0/54000 (0%)] Loss: -249827.078125\n",
      "Train Epoch: 412 [1408/54000 (3%)] Loss: -249674.953125\n",
      "Train Epoch: 412 [2816/54000 (5%)] Loss: -220147.218750\n",
      "Train Epoch: 412 [4224/54000 (8%)] Loss: -225116.687500\n",
      "Train Epoch: 412 [5632/54000 (10%)] Loss: -232505.593750\n",
      "Train Epoch: 412 [7040/54000 (13%)] Loss: -223691.859375\n",
      "Train Epoch: 412 [8448/54000 (16%)] Loss: -248494.593750\n",
      "Train Epoch: 412 [9856/54000 (18%)] Loss: -232383.468750\n",
      "Train Epoch: 412 [11264/54000 (21%)] Loss: -222871.171875\n",
      "Train Epoch: 412 [12672/54000 (23%)] Loss: -226461.843750\n",
      "Train Epoch: 412 [14080/54000 (26%)] Loss: -229919.453125\n",
      "Train Epoch: 412 [15488/54000 (29%)] Loss: -230644.734375\n",
      "Train Epoch: 412 [16896/54000 (31%)] Loss: -223102.031250\n",
      "Train Epoch: 412 [18304/54000 (34%)] Loss: -231639.281250\n",
      "Train Epoch: 412 [19712/54000 (37%)] Loss: -221928.546875\n",
      "Train Epoch: 412 [21120/54000 (39%)] Loss: -220529.625000\n",
      "Train Epoch: 412 [22528/54000 (42%)] Loss: -222754.921875\n",
      "Train Epoch: 412 [23936/54000 (44%)] Loss: -221537.312500\n",
      "Train Epoch: 412 [25344/54000 (47%)] Loss: -223679.468750\n",
      "Train Epoch: 412 [26752/54000 (50%)] Loss: -251461.343750\n",
      "Train Epoch: 412 [28160/54000 (52%)] Loss: -230758.703125\n",
      "Train Epoch: 412 [29568/54000 (55%)] Loss: -223492.578125\n",
      "Train Epoch: 412 [30976/54000 (57%)] Loss: -228291.500000\n",
      "Train Epoch: 412 [32384/54000 (60%)] Loss: -231559.000000\n",
      "Train Epoch: 412 [33792/54000 (63%)] Loss: -225396.031250\n",
      "Train Epoch: 412 [35200/54000 (65%)] Loss: -218979.984375\n",
      "Train Epoch: 412 [36608/54000 (68%)] Loss: -225766.718750\n",
      "Train Epoch: 412 [38016/54000 (70%)] Loss: -251355.343750\n",
      "Train Epoch: 412 [39424/54000 (73%)] Loss: -226894.218750\n",
      "Train Epoch: 412 [40832/54000 (76%)] Loss: -221056.406250\n",
      "Train Epoch: 412 [42240/54000 (78%)] Loss: -219973.859375\n",
      "Train Epoch: 412 [43648/54000 (81%)] Loss: -227575.453125\n",
      "Train Epoch: 412 [45056/54000 (83%)] Loss: -231124.843750\n",
      "Train Epoch: 412 [46464/54000 (86%)] Loss: -229004.281250\n",
      "Train Epoch: 412 [47872/54000 (89%)] Loss: -227290.218750\n",
      "Train Epoch: 412 [49280/54000 (91%)] Loss: -229554.015625\n",
      "Train Epoch: 412 [50688/54000 (94%)] Loss: -248082.000000\n",
      "Train Epoch: 412 [52096/54000 (96%)] Loss: -232381.671875\n",
      "    epoch          : 412\n",
      "    loss           : -228574.60477721292\n",
      "    val_loss       : -230241.74057974466\n",
      "Train Epoch: 413 [0/54000 (0%)] Loss: -229832.796875\n",
      "Train Epoch: 413 [1408/54000 (3%)] Loss: -230617.406250\n",
      "Train Epoch: 413 [2816/54000 (5%)] Loss: -222314.343750\n",
      "Train Epoch: 413 [4224/54000 (8%)] Loss: -219203.281250\n",
      "Train Epoch: 413 [5632/54000 (10%)] Loss: -229344.343750\n",
      "Train Epoch: 413 [7040/54000 (13%)] Loss: -228867.625000\n",
      "Train Epoch: 413 [8448/54000 (16%)] Loss: -231712.437500\n",
      "Train Epoch: 413 [9856/54000 (18%)] Loss: -225167.281250\n",
      "Train Epoch: 413 [11264/54000 (21%)] Loss: -221418.406250\n",
      "Train Epoch: 413 [12672/54000 (23%)] Loss: -222485.625000\n",
      "Train Epoch: 413 [14080/54000 (26%)] Loss: -221982.687500\n",
      "Train Epoch: 413 [15488/54000 (29%)] Loss: -222614.937500\n",
      "Train Epoch: 413 [16896/54000 (31%)] Loss: -250754.375000\n",
      "Train Epoch: 413 [18304/54000 (34%)] Loss: -231221.906250\n",
      "Train Epoch: 413 [19712/54000 (37%)] Loss: -227606.781250\n",
      "Train Epoch: 413 [21120/54000 (39%)] Loss: -231307.468750\n",
      "Train Epoch: 413 [22528/54000 (42%)] Loss: -222178.468750\n",
      "Train Epoch: 413 [23936/54000 (44%)] Loss: -222027.687500\n",
      "Train Epoch: 413 [25344/54000 (47%)] Loss: -216219.640625\n",
      "Train Epoch: 413 [26752/54000 (50%)] Loss: -224975.140625\n",
      "Train Epoch: 413 [28160/54000 (52%)] Loss: -249284.078125\n",
      "Train Epoch: 413 [29568/54000 (55%)] Loss: -226910.750000\n",
      "Train Epoch: 413 [30976/54000 (57%)] Loss: -230311.500000\n",
      "Train Epoch: 413 [32384/54000 (60%)] Loss: -249511.437500\n",
      "Train Epoch: 413 [33792/54000 (63%)] Loss: -223127.500000\n",
      "Train Epoch: 413 [35200/54000 (65%)] Loss: -227123.890625\n",
      "Train Epoch: 413 [36608/54000 (68%)] Loss: -228720.328125\n",
      "Train Epoch: 413 [38016/54000 (70%)] Loss: -230566.984375\n",
      "Train Epoch: 413 [39424/54000 (73%)] Loss: -222081.500000\n",
      "Train Epoch: 413 [40832/54000 (76%)] Loss: -232263.484375\n",
      "Train Epoch: 413 [42240/54000 (78%)] Loss: -214926.906250\n",
      "Train Epoch: 413 [43648/54000 (81%)] Loss: -224167.812500\n",
      "Train Epoch: 413 [45056/54000 (83%)] Loss: -219903.921875\n",
      "Train Epoch: 413 [46464/54000 (86%)] Loss: -230150.937500\n",
      "Train Epoch: 413 [47872/54000 (89%)] Loss: -230278.500000\n",
      "Train Epoch: 413 [49280/54000 (91%)] Loss: -220530.531250\n",
      "Train Epoch: 413 [50688/54000 (94%)] Loss: -226416.125000\n",
      "Train Epoch: 413 [52096/54000 (96%)] Loss: -231583.156250\n",
      "    epoch          : 413\n",
      "    loss           : -228373.1195798445\n",
      "    val_loss       : -230099.40597608613\n",
      "Train Epoch: 414 [0/54000 (0%)] Loss: -223089.718750\n",
      "Train Epoch: 414 [1408/54000 (3%)] Loss: -229461.171875\n",
      "Train Epoch: 414 [2816/54000 (5%)] Loss: -230171.078125\n",
      "Train Epoch: 414 [4224/54000 (8%)] Loss: -220555.015625\n",
      "Train Epoch: 414 [5632/54000 (10%)] Loss: -223990.953125\n",
      "Train Epoch: 414 [7040/54000 (13%)] Loss: -220385.234375\n",
      "Train Epoch: 414 [8448/54000 (16%)] Loss: -220925.281250\n",
      "Train Epoch: 414 [9856/54000 (18%)] Loss: -218348.031250\n",
      "Train Epoch: 414 [11264/54000 (21%)] Loss: -230602.875000\n",
      "Train Epoch: 414 [12672/54000 (23%)] Loss: -232234.406250\n",
      "Train Epoch: 414 [14080/54000 (26%)] Loss: -249821.187500\n",
      "Train Epoch: 414 [15488/54000 (29%)] Loss: -228790.359375\n",
      "Train Epoch: 414 [16896/54000 (31%)] Loss: -230722.500000\n",
      "Train Epoch: 414 [18304/54000 (34%)] Loss: -227050.703125\n",
      "Train Epoch: 414 [19712/54000 (37%)] Loss: -224781.937500\n",
      "Train Epoch: 414 [21120/54000 (39%)] Loss: -226258.687500\n",
      "Train Epoch: 414 [22528/54000 (42%)] Loss: -224805.359375\n",
      "Train Epoch: 414 [23936/54000 (44%)] Loss: -219621.265625\n",
      "Train Epoch: 414 [25344/54000 (47%)] Loss: -226271.031250\n",
      "Train Epoch: 414 [26752/54000 (50%)] Loss: -222693.156250\n",
      "Train Epoch: 414 [28160/54000 (52%)] Loss: -225662.015625\n",
      "Train Epoch: 414 [29568/54000 (55%)] Loss: -232653.625000\n",
      "Train Epoch: 414 [30976/54000 (57%)] Loss: -232090.562500\n",
      "Train Epoch: 414 [32384/54000 (60%)] Loss: -249467.781250\n",
      "Train Epoch: 414 [33792/54000 (63%)] Loss: -225129.703125\n",
      "Train Epoch: 414 [35200/54000 (65%)] Loss: -220096.531250\n",
      "Train Epoch: 414 [36608/54000 (68%)] Loss: -221140.312500\n",
      "Train Epoch: 414 [38016/54000 (70%)] Loss: -225729.921875\n",
      "Train Epoch: 414 [39424/54000 (73%)] Loss: -217958.109375\n",
      "Train Epoch: 414 [40832/54000 (76%)] Loss: -232910.671875\n",
      "Train Epoch: 414 [42240/54000 (78%)] Loss: -226050.109375\n",
      "Train Epoch: 414 [43648/54000 (81%)] Loss: -232664.781250\n",
      "Train Epoch: 414 [45056/54000 (83%)] Loss: -249659.078125\n",
      "Train Epoch: 414 [46464/54000 (86%)] Loss: -222433.718750\n",
      "Train Epoch: 414 [47872/54000 (89%)] Loss: -229379.937500\n",
      "Train Epoch: 414 [49280/54000 (91%)] Loss: -248151.187500\n",
      "Train Epoch: 414 [50688/54000 (94%)] Loss: -222718.906250\n",
      "Train Epoch: 414 [52096/54000 (96%)] Loss: -232113.687500\n",
      "    epoch          : 414\n",
      "    loss           : -228555.69774222487\n",
      "    val_loss       : -230508.59464915204\n",
      "Train Epoch: 415 [0/54000 (0%)] Loss: -232614.875000\n",
      "Train Epoch: 415 [1408/54000 (3%)] Loss: -223441.359375\n",
      "Train Epoch: 415 [2816/54000 (5%)] Loss: -223364.046875\n",
      "Train Epoch: 415 [4224/54000 (8%)] Loss: -233085.781250\n",
      "Train Epoch: 415 [5632/54000 (10%)] Loss: -227528.281250\n",
      "Train Epoch: 415 [7040/54000 (13%)] Loss: -226545.500000\n",
      "Train Epoch: 415 [8448/54000 (16%)] Loss: -224636.734375\n",
      "Train Epoch: 415 [9856/54000 (18%)] Loss: -232831.625000\n",
      "Train Epoch: 415 [11264/54000 (21%)] Loss: -222766.671875\n",
      "Train Epoch: 415 [12672/54000 (23%)] Loss: -228658.546875\n",
      "Train Epoch: 415 [14080/54000 (26%)] Loss: -217806.000000\n",
      "Train Epoch: 415 [15488/54000 (29%)] Loss: -222363.796875\n",
      "Train Epoch: 415 [16896/54000 (31%)] Loss: -248974.031250\n",
      "Train Epoch: 415 [18304/54000 (34%)] Loss: -218384.671875\n",
      "Train Epoch: 415 [19712/54000 (37%)] Loss: -226310.000000\n",
      "Train Epoch: 415 [21120/54000 (39%)] Loss: -218684.484375\n",
      "Train Epoch: 415 [22528/54000 (42%)] Loss: -227078.406250\n",
      "Train Epoch: 415 [23936/54000 (44%)] Loss: -221730.187500\n",
      "Train Epoch: 415 [25344/54000 (47%)] Loss: -223829.750000\n",
      "Train Epoch: 415 [26752/54000 (50%)] Loss: -219374.828125\n",
      "Train Epoch: 415 [28160/54000 (52%)] Loss: -221018.593750\n",
      "Train Epoch: 415 [29568/54000 (55%)] Loss: -229430.390625\n",
      "Train Epoch: 415 [30976/54000 (57%)] Loss: -230976.421875\n",
      "Train Epoch: 415 [32384/54000 (60%)] Loss: -230929.828125\n",
      "Train Epoch: 415 [33792/54000 (63%)] Loss: -222016.031250\n",
      "Train Epoch: 415 [35200/54000 (65%)] Loss: -218269.875000\n",
      "Train Epoch: 415 [36608/54000 (68%)] Loss: -231571.578125\n",
      "Train Epoch: 415 [38016/54000 (70%)] Loss: -249090.578125\n",
      "Train Epoch: 415 [39424/54000 (73%)] Loss: -231082.312500\n",
      "Train Epoch: 415 [40832/54000 (76%)] Loss: -222096.937500\n",
      "Train Epoch: 415 [42240/54000 (78%)] Loss: -226647.000000\n",
      "Train Epoch: 415 [43648/54000 (81%)] Loss: -227593.343750\n",
      "Train Epoch: 415 [45056/54000 (83%)] Loss: -219135.437500\n",
      "Train Epoch: 415 [46464/54000 (86%)] Loss: -221534.015625\n",
      "Train Epoch: 415 [47872/54000 (89%)] Loss: -229417.437500\n",
      "Train Epoch: 415 [49280/54000 (91%)] Loss: -230214.156250\n",
      "Train Epoch: 415 [50688/54000 (94%)] Loss: -231708.906250\n",
      "Train Epoch: 415 [52096/54000 (96%)] Loss: -220698.390625\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch   415: reducing learning rate of group 0 to 2.5000e-05.\n",
      "    epoch          : 415\n",
      "    loss           : -228385.44363038277\n",
      "    val_loss       : -230226.02303258385\n",
      "Train Epoch: 416 [0/54000 (0%)] Loss: -250122.687500\n",
      "Train Epoch: 416 [1408/54000 (3%)] Loss: -249498.343750\n",
      "Train Epoch: 416 [2816/54000 (5%)] Loss: -232622.750000\n",
      "Train Epoch: 416 [4224/54000 (8%)] Loss: -230006.265625\n",
      "Train Epoch: 416 [5632/54000 (10%)] Loss: -221100.968750\n",
      "Train Epoch: 416 [7040/54000 (13%)] Loss: -220729.593750\n",
      "Train Epoch: 416 [8448/54000 (16%)] Loss: -223632.375000\n",
      "Train Epoch: 416 [9856/54000 (18%)] Loss: -221188.390625\n",
      "Train Epoch: 416 [11264/54000 (21%)] Loss: -227437.359375\n",
      "Train Epoch: 416 [12672/54000 (23%)] Loss: -223574.625000\n",
      "Train Epoch: 416 [14080/54000 (26%)] Loss: -249125.031250\n",
      "Train Epoch: 416 [15488/54000 (29%)] Loss: -228264.218750\n",
      "Train Epoch: 416 [16896/54000 (31%)] Loss: -231643.734375\n",
      "Train Epoch: 416 [18304/54000 (34%)] Loss: -249764.953125\n",
      "Train Epoch: 416 [19712/54000 (37%)] Loss: -230670.687500\n",
      "Train Epoch: 416 [21120/54000 (39%)] Loss: -224089.031250\n",
      "Train Epoch: 416 [22528/54000 (42%)] Loss: -229846.656250\n",
      "Train Epoch: 416 [23936/54000 (44%)] Loss: -219955.343750\n",
      "Train Epoch: 416 [25344/54000 (47%)] Loss: -250499.156250\n",
      "Train Epoch: 416 [26752/54000 (50%)] Loss: -232974.984375\n",
      "Train Epoch: 416 [28160/54000 (52%)] Loss: -231800.640625\n",
      "Train Epoch: 416 [29568/54000 (55%)] Loss: -230760.312500\n",
      "Train Epoch: 416 [30976/54000 (57%)] Loss: -248755.390625\n",
      "Train Epoch: 416 [32384/54000 (60%)] Loss: -251105.515625\n",
      "Train Epoch: 416 [33792/54000 (63%)] Loss: -222836.500000\n",
      "Train Epoch: 416 [35200/54000 (65%)] Loss: -222734.656250\n",
      "Train Epoch: 416 [36608/54000 (68%)] Loss: -232593.031250\n",
      "Train Epoch: 416 [38016/54000 (70%)] Loss: -232189.343750\n",
      "Train Epoch: 416 [39424/54000 (73%)] Loss: -232143.203125\n",
      "Train Epoch: 416 [40832/54000 (76%)] Loss: -218269.218750\n",
      "Train Epoch: 416 [42240/54000 (78%)] Loss: -224932.906250\n",
      "Train Epoch: 416 [43648/54000 (81%)] Loss: -226827.968750\n",
      "Train Epoch: 416 [45056/54000 (83%)] Loss: -249306.593750\n",
      "Train Epoch: 416 [46464/54000 (86%)] Loss: -228713.656250\n",
      "Train Epoch: 416 [47872/54000 (89%)] Loss: -226161.437500\n",
      "Train Epoch: 416 [49280/54000 (91%)] Loss: -218811.437500\n",
      "Train Epoch: 416 [50688/54000 (94%)] Loss: -249055.796875\n",
      "Train Epoch: 416 [52096/54000 (96%)] Loss: -233109.687500\n",
      "    epoch          : 416\n",
      "    loss           : -228652.74947667465\n",
      "    val_loss       : -230352.96462342798\n",
      "Train Epoch: 417 [0/54000 (0%)] Loss: -249660.718750\n",
      "Train Epoch: 417 [1408/54000 (3%)] Loss: -251067.765625\n",
      "Train Epoch: 417 [2816/54000 (5%)] Loss: -231860.125000\n",
      "Train Epoch: 417 [4224/54000 (8%)] Loss: -222992.468750\n",
      "Train Epoch: 417 [5632/54000 (10%)] Loss: -222070.453125\n",
      "Train Epoch: 417 [7040/54000 (13%)] Loss: -245662.000000\n",
      "Train Epoch: 417 [8448/54000 (16%)] Loss: -229442.187500\n",
      "Train Epoch: 417 [9856/54000 (18%)] Loss: -229599.593750\n",
      "Train Epoch: 417 [11264/54000 (21%)] Loss: -214051.062500\n",
      "Train Epoch: 417 [12672/54000 (23%)] Loss: -227985.625000\n",
      "Train Epoch: 417 [14080/54000 (26%)] Loss: -231773.437500\n",
      "Train Epoch: 417 [15488/54000 (29%)] Loss: -249338.734375\n",
      "Train Epoch: 417 [16896/54000 (31%)] Loss: -227727.093750\n",
      "Train Epoch: 417 [18304/54000 (34%)] Loss: -248886.296875\n",
      "Train Epoch: 417 [19712/54000 (37%)] Loss: -226649.625000\n",
      "Train Epoch: 417 [21120/54000 (39%)] Loss: -225897.875000\n",
      "Train Epoch: 417 [22528/54000 (42%)] Loss: -218978.531250\n",
      "Train Epoch: 417 [23936/54000 (44%)] Loss: -222749.562500\n",
      "Train Epoch: 417 [25344/54000 (47%)] Loss: -229430.000000\n",
      "Train Epoch: 417 [26752/54000 (50%)] Loss: -225452.703125\n",
      "Train Epoch: 417 [28160/54000 (52%)] Loss: -231414.609375\n",
      "Train Epoch: 417 [29568/54000 (55%)] Loss: -229681.750000\n",
      "Train Epoch: 417 [30976/54000 (57%)] Loss: -230919.312500\n",
      "Train Epoch: 417 [32384/54000 (60%)] Loss: -248675.781250\n",
      "Train Epoch: 417 [33792/54000 (63%)] Loss: -230606.171875\n",
      "Train Epoch: 417 [35200/54000 (65%)] Loss: -224181.921875\n",
      "Train Epoch: 417 [36608/54000 (68%)] Loss: -222968.875000\n",
      "Train Epoch: 417 [38016/54000 (70%)] Loss: -226083.734375\n",
      "Train Epoch: 417 [39424/54000 (73%)] Loss: -222526.343750\n",
      "Train Epoch: 417 [40832/54000 (76%)] Loss: -230991.234375\n",
      "Train Epoch: 417 [42240/54000 (78%)] Loss: -230098.500000\n",
      "Train Epoch: 417 [43648/54000 (81%)] Loss: -232938.609375\n",
      "Train Epoch: 417 [45056/54000 (83%)] Loss: -231344.312500\n",
      "Train Epoch: 417 [46464/54000 (86%)] Loss: -225331.875000\n",
      "Train Epoch: 417 [47872/54000 (89%)] Loss: -222051.859375\n",
      "Train Epoch: 417 [49280/54000 (91%)] Loss: -225148.390625\n",
      "Train Epoch: 417 [50688/54000 (94%)] Loss: -222694.203125\n",
      "Train Epoch: 417 [52096/54000 (96%)] Loss: -230675.046875\n",
      "    epoch          : 417\n",
      "    loss           : -228614.9986916866\n",
      "    val_loss       : -230976.47930759337\n",
      "Train Epoch: 418 [0/54000 (0%)] Loss: -222505.750000\n",
      "Train Epoch: 418 [1408/54000 (3%)] Loss: -231354.656250\n",
      "Train Epoch: 418 [2816/54000 (5%)] Loss: -218887.515625\n",
      "Train Epoch: 418 [4224/54000 (8%)] Loss: -231753.234375\n",
      "Train Epoch: 418 [5632/54000 (10%)] Loss: -219993.000000\n",
      "Train Epoch: 418 [7040/54000 (13%)] Loss: -221307.718750\n",
      "Train Epoch: 418 [8448/54000 (16%)] Loss: -232663.625000\n",
      "Train Epoch: 418 [9856/54000 (18%)] Loss: -223208.171875\n",
      "Train Epoch: 418 [11264/54000 (21%)] Loss: -231193.921875\n",
      "Train Epoch: 418 [12672/54000 (23%)] Loss: -232020.296875\n",
      "Train Epoch: 418 [14080/54000 (26%)] Loss: -220572.078125\n",
      "Train Epoch: 418 [15488/54000 (29%)] Loss: -218605.015625\n",
      "Train Epoch: 418 [16896/54000 (31%)] Loss: -231615.656250\n",
      "Train Epoch: 418 [18304/54000 (34%)] Loss: -223325.093750\n",
      "Train Epoch: 418 [19712/54000 (37%)] Loss: -227239.671875\n",
      "Train Epoch: 418 [21120/54000 (39%)] Loss: -233058.859375\n",
      "Train Epoch: 418 [22528/54000 (42%)] Loss: -231677.796875\n",
      "Train Epoch: 418 [23936/54000 (44%)] Loss: -250536.125000\n",
      "Train Epoch: 418 [25344/54000 (47%)] Loss: -220098.531250\n",
      "Train Epoch: 418 [26752/54000 (50%)] Loss: -224647.687500\n",
      "Train Epoch: 418 [28160/54000 (52%)] Loss: -223911.437500\n",
      "Train Epoch: 418 [29568/54000 (55%)] Loss: -221958.218750\n",
      "Train Epoch: 418 [30976/54000 (57%)] Loss: -230881.375000\n",
      "Train Epoch: 418 [32384/54000 (60%)] Loss: -222326.640625\n",
      "Train Epoch: 418 [33792/54000 (63%)] Loss: -231166.906250\n",
      "Train Epoch: 418 [35200/54000 (65%)] Loss: -217029.734375\n",
      "Train Epoch: 418 [36608/54000 (68%)] Loss: -248621.921875\n",
      "Train Epoch: 418 [38016/54000 (70%)] Loss: -232382.281250\n",
      "Train Epoch: 418 [39424/54000 (73%)] Loss: -222365.562500\n",
      "Train Epoch: 418 [40832/54000 (76%)] Loss: -225519.000000\n",
      "Train Epoch: 418 [42240/54000 (78%)] Loss: -228241.250000\n",
      "Train Epoch: 418 [43648/54000 (81%)] Loss: -220462.734375\n",
      "Train Epoch: 418 [45056/54000 (83%)] Loss: -222807.250000\n",
      "Train Epoch: 418 [46464/54000 (86%)] Loss: -219236.390625\n",
      "Train Epoch: 418 [47872/54000 (89%)] Loss: -222050.578125\n",
      "Train Epoch: 418 [49280/54000 (91%)] Loss: -223665.984375\n",
      "Train Epoch: 418 [50688/54000 (94%)] Loss: -251112.515625\n",
      "Train Epoch: 418 [52096/54000 (96%)] Loss: -231835.015625\n",
      "    epoch          : 418\n",
      "    loss           : -228585.40639952154\n",
      "    val_loss       : -230849.10327148438\n",
      "Train Epoch: 419 [0/54000 (0%)] Loss: -224666.468750\n",
      "Train Epoch: 419 [1408/54000 (3%)] Loss: -221683.968750\n",
      "Train Epoch: 419 [2816/54000 (5%)] Loss: -250851.515625\n",
      "Train Epoch: 419 [4224/54000 (8%)] Loss: -230377.343750\n",
      "Train Epoch: 419 [5632/54000 (10%)] Loss: -228415.625000\n",
      "Train Epoch: 419 [7040/54000 (13%)] Loss: -220576.531250\n",
      "Train Epoch: 419 [8448/54000 (16%)] Loss: -230617.281250\n",
      "Train Epoch: 419 [9856/54000 (18%)] Loss: -225118.531250\n",
      "Train Epoch: 419 [11264/54000 (21%)] Loss: -220329.890625\n",
      "Train Epoch: 419 [12672/54000 (23%)] Loss: -230377.796875\n",
      "Train Epoch: 419 [14080/54000 (26%)] Loss: -223782.187500\n",
      "Train Epoch: 419 [15488/54000 (29%)] Loss: -232141.375000\n",
      "Train Epoch: 419 [16896/54000 (31%)] Loss: -232234.859375\n",
      "Train Epoch: 419 [18304/54000 (34%)] Loss: -230776.578125\n",
      "Train Epoch: 419 [19712/54000 (37%)] Loss: -221588.140625\n",
      "Train Epoch: 419 [21120/54000 (39%)] Loss: -249973.765625\n",
      "Train Epoch: 419 [22528/54000 (42%)] Loss: -232539.156250\n",
      "Train Epoch: 419 [23936/54000 (44%)] Loss: -229943.781250\n",
      "Train Epoch: 419 [25344/54000 (47%)] Loss: -224005.437500\n",
      "Train Epoch: 419 [26752/54000 (50%)] Loss: -231686.421875\n",
      "Train Epoch: 419 [28160/54000 (52%)] Loss: -224206.125000\n",
      "Train Epoch: 419 [29568/54000 (55%)] Loss: -227338.921875\n",
      "Train Epoch: 419 [30976/54000 (57%)] Loss: -221045.156250\n",
      "Train Epoch: 419 [32384/54000 (60%)] Loss: -225572.250000\n",
      "Train Epoch: 419 [33792/54000 (63%)] Loss: -249976.625000\n",
      "Train Epoch: 419 [35200/54000 (65%)] Loss: -224651.593750\n",
      "Train Epoch: 419 [36608/54000 (68%)] Loss: -230151.187500\n",
      "Train Epoch: 419 [38016/54000 (70%)] Loss: -249900.031250\n",
      "Train Epoch: 419 [39424/54000 (73%)] Loss: -221458.406250\n",
      "Train Epoch: 419 [40832/54000 (76%)] Loss: -221694.218750\n",
      "Train Epoch: 419 [42240/54000 (78%)] Loss: -229768.906250\n",
      "Train Epoch: 419 [43648/54000 (81%)] Loss: -249129.359375\n",
      "Train Epoch: 419 [45056/54000 (83%)] Loss: -222120.812500\n",
      "Train Epoch: 419 [46464/54000 (86%)] Loss: -226737.437500\n",
      "Train Epoch: 419 [47872/54000 (89%)] Loss: -227482.109375\n",
      "Train Epoch: 419 [49280/54000 (91%)] Loss: -223069.468750\n",
      "Train Epoch: 419 [50688/54000 (94%)] Loss: -248443.718750\n",
      "Train Epoch: 419 [52096/54000 (96%)] Loss: -233364.859375\n",
      "    epoch          : 419\n",
      "    loss           : -228517.27564294258\n",
      "    val_loss       : -230195.48201695885\n",
      "Train Epoch: 420 [0/54000 (0%)] Loss: -248310.296875\n",
      "Train Epoch: 420 [1408/54000 (3%)] Loss: -219082.031250\n",
      "Train Epoch: 420 [2816/54000 (5%)] Loss: -224453.843750\n",
      "Train Epoch: 420 [4224/54000 (8%)] Loss: -231055.375000\n",
      "Train Epoch: 420 [5632/54000 (10%)] Loss: -231959.468750\n",
      "Train Epoch: 420 [7040/54000 (13%)] Loss: -228281.156250\n",
      "Train Epoch: 420 [8448/54000 (16%)] Loss: -222743.812500\n",
      "Train Epoch: 420 [9856/54000 (18%)] Loss: -221989.781250\n",
      "Train Epoch: 420 [11264/54000 (21%)] Loss: -224864.468750\n",
      "Train Epoch: 420 [12672/54000 (23%)] Loss: -231642.015625\n",
      "Train Epoch: 420 [14080/54000 (26%)] Loss: -250324.171875\n",
      "Train Epoch: 420 [15488/54000 (29%)] Loss: -225184.359375\n",
      "Train Epoch: 420 [16896/54000 (31%)] Loss: -231666.734375\n",
      "Train Epoch: 420 [18304/54000 (34%)] Loss: -217145.312500\n",
      "Train Epoch: 420 [19712/54000 (37%)] Loss: -220822.968750\n",
      "Train Epoch: 420 [21120/54000 (39%)] Loss: -224227.531250\n",
      "Train Epoch: 420 [22528/54000 (42%)] Loss: -226232.093750\n",
      "Train Epoch: 420 [23936/54000 (44%)] Loss: -224869.484375\n",
      "Train Epoch: 420 [25344/54000 (47%)] Loss: -232849.453125\n",
      "Train Epoch: 420 [26752/54000 (50%)] Loss: -231175.859375\n",
      "Train Epoch: 420 [28160/54000 (52%)] Loss: -221042.671875\n",
      "Train Epoch: 420 [29568/54000 (55%)] Loss: -228424.656250\n",
      "Train Epoch: 420 [30976/54000 (57%)] Loss: -228134.406250\n",
      "Train Epoch: 420 [32384/54000 (60%)] Loss: -248735.937500\n",
      "Train Epoch: 420 [33792/54000 (63%)] Loss: -229584.156250\n",
      "Train Epoch: 420 [35200/54000 (65%)] Loss: -221599.843750\n",
      "Train Epoch: 420 [36608/54000 (68%)] Loss: -222373.515625\n",
      "Train Epoch: 420 [38016/54000 (70%)] Loss: -223218.718750\n",
      "Train Epoch: 420 [39424/54000 (73%)] Loss: -221799.531250\n",
      "Train Epoch: 420 [40832/54000 (76%)] Loss: -231879.843750\n",
      "Train Epoch: 420 [42240/54000 (78%)] Loss: -225544.171875\n",
      "Train Epoch: 420 [43648/54000 (81%)] Loss: -226269.453125\n",
      "Train Epoch: 420 [45056/54000 (83%)] Loss: -230245.937500\n",
      "Train Epoch: 420 [46464/54000 (86%)] Loss: -221644.906250\n",
      "Train Epoch: 420 [47872/54000 (89%)] Loss: -220371.656250\n",
      "Train Epoch: 420 [49280/54000 (91%)] Loss: -221932.578125\n",
      "Train Epoch: 420 [50688/54000 (94%)] Loss: -249634.656250\n",
      "Train Epoch: 420 [52096/54000 (96%)] Loss: -230871.781250\n",
      "    epoch          : 420\n",
      "    loss           : -228658.19392942585\n",
      "    val_loss       : -230792.13693311738\n",
      "Train Epoch: 421 [0/54000 (0%)] Loss: -249339.796875\n",
      "Train Epoch: 421 [1408/54000 (3%)] Loss: -230867.843750\n",
      "Train Epoch: 421 [2816/54000 (5%)] Loss: -230615.750000\n",
      "Train Epoch: 421 [4224/54000 (8%)] Loss: -230720.500000\n",
      "Train Epoch: 421 [5632/54000 (10%)] Loss: -221973.296875\n",
      "Train Epoch: 421 [7040/54000 (13%)] Loss: -222565.875000\n",
      "Train Epoch: 421 [8448/54000 (16%)] Loss: -226788.718750\n",
      "Train Epoch: 421 [9856/54000 (18%)] Loss: -217297.390625\n",
      "Train Epoch: 421 [11264/54000 (21%)] Loss: -221968.765625\n",
      "Train Epoch: 421 [12672/54000 (23%)] Loss: -219540.484375\n",
      "Train Epoch: 421 [14080/54000 (26%)] Loss: -222961.687500\n",
      "Train Epoch: 421 [15488/54000 (29%)] Loss: -231795.453125\n",
      "Train Epoch: 421 [16896/54000 (31%)] Loss: -249966.515625\n",
      "Train Epoch: 421 [18304/54000 (34%)] Loss: -228072.062500\n",
      "Train Epoch: 421 [19712/54000 (37%)] Loss: -232259.765625\n",
      "Train Epoch: 421 [21120/54000 (39%)] Loss: -229573.796875\n",
      "Train Epoch: 421 [22528/54000 (42%)] Loss: -249211.578125\n",
      "Train Epoch: 421 [23936/54000 (44%)] Loss: -248182.703125\n",
      "Train Epoch: 421 [25344/54000 (47%)] Loss: -228597.000000\n",
      "Train Epoch: 421 [26752/54000 (50%)] Loss: -221925.687500\n",
      "Train Epoch: 421 [28160/54000 (52%)] Loss: -250372.937500\n",
      "Train Epoch: 421 [29568/54000 (55%)] Loss: -228947.718750\n",
      "Train Epoch: 421 [30976/54000 (57%)] Loss: -229345.140625\n",
      "Train Epoch: 421 [32384/54000 (60%)] Loss: -227680.781250\n",
      "Train Epoch: 421 [33792/54000 (63%)] Loss: -220421.031250\n",
      "Train Epoch: 421 [35200/54000 (65%)] Loss: -219670.562500\n",
      "Train Epoch: 421 [36608/54000 (68%)] Loss: -218356.187500\n",
      "Train Epoch: 421 [38016/54000 (70%)] Loss: -218170.781250\n",
      "Train Epoch: 421 [39424/54000 (73%)] Loss: -248917.765625\n",
      "Train Epoch: 421 [40832/54000 (76%)] Loss: -231186.984375\n",
      "Train Epoch: 421 [42240/54000 (78%)] Loss: -223402.750000\n",
      "Train Epoch: 421 [43648/54000 (81%)] Loss: -224603.781250\n",
      "Train Epoch: 421 [45056/54000 (83%)] Loss: -224096.562500\n",
      "Train Epoch: 421 [46464/54000 (86%)] Loss: -226027.656250\n",
      "Train Epoch: 421 [47872/54000 (89%)] Loss: -227783.531250\n",
      "Train Epoch: 421 [49280/54000 (91%)] Loss: -223099.843750\n",
      "Train Epoch: 421 [50688/54000 (94%)] Loss: -249346.546875\n",
      "Train Epoch: 421 [52096/54000 (96%)] Loss: -233148.250000\n",
      "    epoch          : 421\n",
      "    loss           : -228683.6636513158\n",
      "    val_loss       : -230465.89319740853\n",
      "Train Epoch: 422 [0/54000 (0%)] Loss: -226678.250000\n",
      "Train Epoch: 422 [1408/54000 (3%)] Loss: -225547.125000\n",
      "Train Epoch: 422 [2816/54000 (5%)] Loss: -232098.343750\n",
      "Train Epoch: 422 [4224/54000 (8%)] Loss: -223895.343750\n",
      "Train Epoch: 422 [5632/54000 (10%)] Loss: -227590.843750\n",
      "Train Epoch: 422 [7040/54000 (13%)] Loss: -230625.375000\n",
      "Train Epoch: 422 [8448/54000 (16%)] Loss: -232192.375000\n",
      "Train Epoch: 422 [9856/54000 (18%)] Loss: -220620.453125\n",
      "Train Epoch: 422 [11264/54000 (21%)] Loss: -222843.312500\n",
      "Train Epoch: 422 [12672/54000 (23%)] Loss: -230604.390625\n",
      "Train Epoch: 422 [14080/54000 (26%)] Loss: -231877.015625\n",
      "Train Epoch: 422 [15488/54000 (29%)] Loss: -249545.640625\n",
      "Train Epoch: 422 [16896/54000 (31%)] Loss: -223945.906250\n",
      "Train Epoch: 422 [18304/54000 (34%)] Loss: -233990.343750\n",
      "Train Epoch: 422 [19712/54000 (37%)] Loss: -249443.796875\n",
      "Train Epoch: 422 [21120/54000 (39%)] Loss: -222622.781250\n",
      "Train Epoch: 422 [22528/54000 (42%)] Loss: -224822.093750\n",
      "Train Epoch: 422 [23936/54000 (44%)] Loss: -218514.406250\n",
      "Train Epoch: 422 [25344/54000 (47%)] Loss: -221406.500000\n",
      "Train Epoch: 422 [26752/54000 (50%)] Loss: -220202.421875\n",
      "Train Epoch: 422 [28160/54000 (52%)] Loss: -231790.281250\n",
      "Train Epoch: 422 [29568/54000 (55%)] Loss: -220314.406250\n",
      "Train Epoch: 422 [30976/54000 (57%)] Loss: -225573.312500\n",
      "Train Epoch: 422 [32384/54000 (60%)] Loss: -232164.953125\n",
      "Train Epoch: 422 [33792/54000 (63%)] Loss: -230768.781250\n",
      "Train Epoch: 422 [35200/54000 (65%)] Loss: -219709.625000\n",
      "Train Epoch: 422 [36608/54000 (68%)] Loss: -223771.265625\n",
      "Train Epoch: 422 [38016/54000 (70%)] Loss: -250769.750000\n",
      "Train Epoch: 422 [39424/54000 (73%)] Loss: -227427.281250\n",
      "Train Epoch: 422 [40832/54000 (76%)] Loss: -223217.406250\n",
      "Train Epoch: 422 [42240/54000 (78%)] Loss: -227667.609375\n",
      "Train Epoch: 422 [43648/54000 (81%)] Loss: -228238.484375\n",
      "Train Epoch: 422 [45056/54000 (83%)] Loss: -223886.921875\n",
      "Train Epoch: 422 [46464/54000 (86%)] Loss: -221770.609375\n",
      "Train Epoch: 422 [47872/54000 (89%)] Loss: -221375.453125\n",
      "Train Epoch: 422 [49280/54000 (91%)] Loss: -249014.234375\n",
      "Train Epoch: 422 [50688/54000 (94%)] Loss: -225459.109375\n",
      "Train Epoch: 422 [52096/54000 (96%)] Loss: -231662.281250\n",
      "    epoch          : 422\n",
      "    loss           : -228777.67651016745\n",
      "    val_loss       : -230876.57804758954\n",
      "Train Epoch: 423 [0/54000 (0%)] Loss: -223623.375000\n",
      "Train Epoch: 423 [1408/54000 (3%)] Loss: -209607.953125\n",
      "Train Epoch: 423 [2816/54000 (5%)] Loss: -222583.328125\n",
      "Train Epoch: 423 [4224/54000 (8%)] Loss: -220317.015625\n",
      "Train Epoch: 423 [5632/54000 (10%)] Loss: -228872.078125\n",
      "Train Epoch: 423 [7040/54000 (13%)] Loss: -222926.640625\n",
      "Train Epoch: 423 [8448/54000 (16%)] Loss: -221324.437500\n",
      "Train Epoch: 423 [9856/54000 (18%)] Loss: -249833.781250\n",
      "Train Epoch: 423 [11264/54000 (21%)] Loss: -227700.328125\n",
      "Train Epoch: 423 [12672/54000 (23%)] Loss: -220174.093750\n",
      "Train Epoch: 423 [14080/54000 (26%)] Loss: -231038.250000\n",
      "Train Epoch: 423 [15488/54000 (29%)] Loss: -217340.203125\n",
      "Train Epoch: 423 [16896/54000 (31%)] Loss: -222269.781250\n",
      "Train Epoch: 423 [18304/54000 (34%)] Loss: -218668.437500\n",
      "Train Epoch: 423 [19712/54000 (37%)] Loss: -217277.812500\n",
      "Train Epoch: 423 [21120/54000 (39%)] Loss: -221411.671875\n",
      "Train Epoch: 423 [22528/54000 (42%)] Loss: -231281.640625\n",
      "Train Epoch: 423 [23936/54000 (44%)] Loss: -221014.359375\n",
      "Train Epoch: 423 [25344/54000 (47%)] Loss: -223629.234375\n",
      "Train Epoch: 423 [26752/54000 (50%)] Loss: -222791.750000\n",
      "Train Epoch: 423 [28160/54000 (52%)] Loss: -225922.093750\n",
      "Train Epoch: 423 [29568/54000 (55%)] Loss: -227597.687500\n",
      "Train Epoch: 423 [30976/54000 (57%)] Loss: -217580.312500\n",
      "Train Epoch: 423 [32384/54000 (60%)] Loss: -218877.343750\n",
      "Train Epoch: 423 [33792/54000 (63%)] Loss: -223586.625000\n",
      "Train Epoch: 423 [35200/54000 (65%)] Loss: -219695.875000\n",
      "Train Epoch: 423 [36608/54000 (68%)] Loss: -222704.062500\n",
      "Train Epoch: 423 [38016/54000 (70%)] Loss: -229600.562500\n",
      "Train Epoch: 423 [39424/54000 (73%)] Loss: -227524.859375\n",
      "Train Epoch: 423 [40832/54000 (76%)] Loss: -222674.750000\n",
      "Train Epoch: 423 [42240/54000 (78%)] Loss: -230186.718750\n",
      "Train Epoch: 423 [43648/54000 (81%)] Loss: -223476.984375\n",
      "Train Epoch: 423 [45056/54000 (83%)] Loss: -249639.593750\n",
      "Train Epoch: 423 [46464/54000 (86%)] Loss: -222902.062500\n",
      "Train Epoch: 423 [47872/54000 (89%)] Loss: -222584.234375\n",
      "Train Epoch: 423 [49280/54000 (91%)] Loss: -218732.328125\n",
      "Train Epoch: 423 [50688/54000 (94%)] Loss: -229380.312500\n",
      "Train Epoch: 423 [52096/54000 (96%)] Loss: -220925.406250\n",
      "    epoch          : 423\n",
      "    loss           : -228476.52369916267\n",
      "    val_loss       : -231026.14180997523\n",
      "Train Epoch: 424 [0/54000 (0%)] Loss: -231033.796875\n",
      "Train Epoch: 424 [1408/54000 (3%)] Loss: -227076.500000\n",
      "Train Epoch: 424 [2816/54000 (5%)] Loss: -231725.375000\n",
      "Train Epoch: 424 [4224/54000 (8%)] Loss: -223570.937500\n",
      "Train Epoch: 424 [5632/54000 (10%)] Loss: -224306.125000\n",
      "Train Epoch: 424 [7040/54000 (13%)] Loss: -225381.781250\n",
      "Train Epoch: 424 [8448/54000 (16%)] Loss: -223694.453125\n",
      "Train Epoch: 424 [9856/54000 (18%)] Loss: -219309.406250\n",
      "Train Epoch: 424 [11264/54000 (21%)] Loss: -232452.750000\n",
      "Train Epoch: 424 [12672/54000 (23%)] Loss: -225523.171875\n",
      "Train Epoch: 424 [14080/54000 (26%)] Loss: -231678.968750\n",
      "Train Epoch: 424 [15488/54000 (29%)] Loss: -220432.250000\n",
      "Train Epoch: 424 [16896/54000 (31%)] Loss: -220617.796875\n",
      "Train Epoch: 424 [18304/54000 (34%)] Loss: -218178.906250\n",
      "Train Epoch: 424 [19712/54000 (37%)] Loss: -226938.250000\n",
      "Train Epoch: 424 [21120/54000 (39%)] Loss: -231756.312500\n",
      "Train Epoch: 424 [22528/54000 (42%)] Loss: -221798.515625\n",
      "Train Epoch: 424 [23936/54000 (44%)] Loss: -250426.234375\n",
      "Train Epoch: 424 [25344/54000 (47%)] Loss: -232407.687500\n",
      "Train Epoch: 424 [26752/54000 (50%)] Loss: -222989.734375\n",
      "Train Epoch: 424 [28160/54000 (52%)] Loss: -231674.109375\n",
      "Train Epoch: 424 [29568/54000 (55%)] Loss: -233200.015625\n",
      "Train Epoch: 424 [30976/54000 (57%)] Loss: -229140.281250\n",
      "Train Epoch: 424 [32384/54000 (60%)] Loss: -231736.609375\n",
      "Train Epoch: 424 [33792/54000 (63%)] Loss: -231179.437500\n",
      "Train Epoch: 424 [35200/54000 (65%)] Loss: -226271.718750\n",
      "Train Epoch: 424 [36608/54000 (68%)] Loss: -227723.078125\n",
      "Train Epoch: 424 [38016/54000 (70%)] Loss: -250926.562500\n",
      "Train Epoch: 424 [39424/54000 (73%)] Loss: -226380.500000\n",
      "Train Epoch: 424 [40832/54000 (76%)] Loss: -219667.328125\n",
      "Train Epoch: 424 [42240/54000 (78%)] Loss: -227244.015625\n",
      "Train Epoch: 424 [43648/54000 (81%)] Loss: -217253.406250\n",
      "Train Epoch: 424 [45056/54000 (83%)] Loss: -250533.937500\n",
      "Train Epoch: 424 [46464/54000 (86%)] Loss: -220888.781250\n",
      "Train Epoch: 424 [47872/54000 (89%)] Loss: -229813.062500\n",
      "Train Epoch: 424 [49280/54000 (91%)] Loss: -222829.859375\n",
      "Train Epoch: 424 [50688/54000 (94%)] Loss: -220507.406250\n",
      "Train Epoch: 424 [52096/54000 (96%)] Loss: -231936.343750\n",
      "    epoch          : 424\n",
      "    loss           : -228612.5596964713\n",
      "    val_loss       : -230571.1495093369\n",
      "Train Epoch: 425 [0/54000 (0%)] Loss: -249942.953125\n",
      "Train Epoch: 425 [1408/54000 (3%)] Loss: -221331.015625\n",
      "Train Epoch: 425 [2816/54000 (5%)] Loss: -226163.359375\n",
      "Train Epoch: 425 [4224/54000 (8%)] Loss: -221491.437500\n",
      "Train Epoch: 425 [5632/54000 (10%)] Loss: -223541.156250\n",
      "Train Epoch: 425 [7040/54000 (13%)] Loss: -223169.000000\n",
      "Train Epoch: 425 [8448/54000 (16%)] Loss: -247924.359375\n",
      "Train Epoch: 425 [9856/54000 (18%)] Loss: -224479.390625\n",
      "Train Epoch: 425 [11264/54000 (21%)] Loss: -226372.109375\n",
      "Train Epoch: 425 [12672/54000 (23%)] Loss: -233225.500000\n",
      "Train Epoch: 425 [14080/54000 (26%)] Loss: -232089.359375\n",
      "Train Epoch: 425 [15488/54000 (29%)] Loss: -230161.109375\n",
      "Train Epoch: 425 [16896/54000 (31%)] Loss: -232255.171875\n",
      "Train Epoch: 425 [18304/54000 (34%)] Loss: -221783.796875\n",
      "Train Epoch: 425 [19712/54000 (37%)] Loss: -224600.437500\n",
      "Train Epoch: 425 [21120/54000 (39%)] Loss: -222909.906250\n",
      "Train Epoch: 425 [22528/54000 (42%)] Loss: -221696.812500\n",
      "Train Epoch: 425 [23936/54000 (44%)] Loss: -228045.671875\n",
      "Train Epoch: 425 [25344/54000 (47%)] Loss: -232378.296875\n",
      "Train Epoch: 425 [26752/54000 (50%)] Loss: -223154.687500\n",
      "Train Epoch: 425 [28160/54000 (52%)] Loss: -221977.625000\n",
      "Train Epoch: 425 [29568/54000 (55%)] Loss: -233243.718750\n",
      "Train Epoch: 425 [30976/54000 (57%)] Loss: -231401.390625\n",
      "Train Epoch: 425 [32384/54000 (60%)] Loss: -248985.140625\n",
      "Train Epoch: 425 [33792/54000 (63%)] Loss: -230159.218750\n",
      "Train Epoch: 425 [35200/54000 (65%)] Loss: -223079.750000\n",
      "Train Epoch: 425 [36608/54000 (68%)] Loss: -219801.937500\n",
      "Train Epoch: 425 [38016/54000 (70%)] Loss: -232420.765625\n",
      "Train Epoch: 425 [39424/54000 (73%)] Loss: -222181.921875\n",
      "Train Epoch: 425 [40832/54000 (76%)] Loss: -220099.562500\n",
      "Train Epoch: 425 [42240/54000 (78%)] Loss: -221818.062500\n",
      "Train Epoch: 425 [43648/54000 (81%)] Loss: -250538.640625\n",
      "Train Epoch: 425 [45056/54000 (83%)] Loss: -225365.187500\n",
      "Train Epoch: 425 [46464/54000 (86%)] Loss: -231542.500000\n",
      "Train Epoch: 425 [47872/54000 (89%)] Loss: -227633.531250\n",
      "Train Epoch: 425 [49280/54000 (91%)] Loss: -225534.906250\n",
      "Train Epoch: 425 [50688/54000 (94%)] Loss: -249898.359375\n",
      "Train Epoch: 425 [52096/54000 (96%)] Loss: -223044.859375\n",
      "    epoch          : 425\n",
      "    loss           : -228657.17475328947\n",
      "    val_loss       : -230840.06595965129\n",
      "Train Epoch: 426 [0/54000 (0%)] Loss: -232280.281250\n",
      "Train Epoch: 426 [1408/54000 (3%)] Loss: -250544.640625\n",
      "Train Epoch: 426 [2816/54000 (5%)] Loss: -232656.796875\n",
      "Train Epoch: 426 [4224/54000 (8%)] Loss: -232477.875000\n",
      "Train Epoch: 426 [5632/54000 (10%)] Loss: -230752.625000\n",
      "Train Epoch: 426 [7040/54000 (13%)] Loss: -223736.203125\n",
      "Train Epoch: 426 [8448/54000 (16%)] Loss: -229747.484375\n",
      "Train Epoch: 426 [9856/54000 (18%)] Loss: -227480.015625\n",
      "Train Epoch: 426 [11264/54000 (21%)] Loss: -229322.937500\n",
      "Train Epoch: 426 [12672/54000 (23%)] Loss: -221997.453125\n",
      "Train Epoch: 426 [14080/54000 (26%)] Loss: -217166.953125\n",
      "Train Epoch: 426 [15488/54000 (29%)] Loss: -220134.562500\n",
      "Train Epoch: 426 [16896/54000 (31%)] Loss: -221361.390625\n",
      "Train Epoch: 426 [18304/54000 (34%)] Loss: -232984.250000\n",
      "Train Epoch: 426 [19712/54000 (37%)] Loss: -226262.234375\n",
      "Train Epoch: 426 [21120/54000 (39%)] Loss: -231691.812500\n",
      "Train Epoch: 426 [22528/54000 (42%)] Loss: -229045.437500\n",
      "Train Epoch: 426 [23936/54000 (44%)] Loss: -222842.625000\n",
      "Train Epoch: 426 [25344/54000 (47%)] Loss: -221785.546875\n",
      "Train Epoch: 426 [26752/54000 (50%)] Loss: -250290.484375\n",
      "Train Epoch: 426 [28160/54000 (52%)] Loss: -223472.343750\n",
      "Train Epoch: 426 [29568/54000 (55%)] Loss: -228486.968750\n",
      "Train Epoch: 426 [30976/54000 (57%)] Loss: -219551.265625\n",
      "Train Epoch: 426 [32384/54000 (60%)] Loss: -229297.625000\n",
      "Train Epoch: 426 [33792/54000 (63%)] Loss: -220439.750000\n",
      "Train Epoch: 426 [35200/54000 (65%)] Loss: -232032.875000\n",
      "Train Epoch: 426 [36608/54000 (68%)] Loss: -220506.328125\n",
      "Train Epoch: 426 [38016/54000 (70%)] Loss: -226574.562500\n",
      "Train Epoch: 426 [39424/54000 (73%)] Loss: -226366.687500\n",
      "Train Epoch: 426 [40832/54000 (76%)] Loss: -222585.484375\n",
      "Train Epoch: 426 [42240/54000 (78%)] Loss: -231880.531250\n",
      "Train Epoch: 426 [43648/54000 (81%)] Loss: -249730.531250\n",
      "Train Epoch: 426 [45056/54000 (83%)] Loss: -224093.093750\n",
      "Train Epoch: 426 [46464/54000 (86%)] Loss: -226129.859375\n",
      "Train Epoch: 426 [47872/54000 (89%)] Loss: -229129.687500\n",
      "Train Epoch: 426 [49280/54000 (91%)] Loss: -220595.062500\n",
      "Train Epoch: 426 [50688/54000 (94%)] Loss: -250865.953125\n",
      "Train Epoch: 426 [52096/54000 (96%)] Loss: -231879.328125\n",
      "    epoch          : 426\n",
      "    loss           : -228567.95626495214\n",
      "    val_loss       : -230586.07211080412\n",
      "Train Epoch: 427 [0/54000 (0%)] Loss: -227756.187500\n",
      "Train Epoch: 427 [1408/54000 (3%)] Loss: -226584.250000\n",
      "Train Epoch: 427 [2816/54000 (5%)] Loss: -224661.187500\n",
      "Train Epoch: 427 [4224/54000 (8%)] Loss: -223578.718750\n",
      "Train Epoch: 427 [5632/54000 (10%)] Loss: -224485.937500\n",
      "Train Epoch: 427 [7040/54000 (13%)] Loss: -226966.890625\n",
      "Train Epoch: 427 [8448/54000 (16%)] Loss: -222819.937500\n",
      "Train Epoch: 427 [9856/54000 (18%)] Loss: -230972.937500\n",
      "Train Epoch: 427 [11264/54000 (21%)] Loss: -223690.531250\n",
      "Train Epoch: 427 [12672/54000 (23%)] Loss: -232845.296875\n",
      "Train Epoch: 427 [14080/54000 (26%)] Loss: -249961.000000\n",
      "Train Epoch: 427 [15488/54000 (29%)] Loss: -220396.875000\n",
      "Train Epoch: 427 [16896/54000 (31%)] Loss: -220901.250000\n",
      "Train Epoch: 427 [18304/54000 (34%)] Loss: -219787.875000\n",
      "Train Epoch: 427 [19712/54000 (37%)] Loss: -250722.078125\n",
      "Train Epoch: 427 [21120/54000 (39%)] Loss: -249612.593750\n",
      "Train Epoch: 427 [22528/54000 (42%)] Loss: -223942.765625\n",
      "Train Epoch: 427 [23936/54000 (44%)] Loss: -222630.906250\n",
      "Train Epoch: 427 [25344/54000 (47%)] Loss: -221447.218750\n",
      "Train Epoch: 427 [26752/54000 (50%)] Loss: -249059.234375\n",
      "Train Epoch: 427 [28160/54000 (52%)] Loss: -223290.312500\n",
      "Train Epoch: 427 [29568/54000 (55%)] Loss: -222013.750000\n",
      "Train Epoch: 427 [30976/54000 (57%)] Loss: -230725.906250\n",
      "Train Epoch: 427 [32384/54000 (60%)] Loss: -232922.125000\n",
      "Train Epoch: 427 [33792/54000 (63%)] Loss: -247005.812500\n",
      "Train Epoch: 427 [35200/54000 (65%)] Loss: -224538.640625\n",
      "Train Epoch: 427 [36608/54000 (68%)] Loss: -219936.375000\n",
      "Train Epoch: 427 [38016/54000 (70%)] Loss: -251420.125000\n",
      "Train Epoch: 427 [39424/54000 (73%)] Loss: -225182.296875\n",
      "Train Epoch: 427 [40832/54000 (76%)] Loss: -227259.281250\n",
      "Train Epoch: 427 [42240/54000 (78%)] Loss: -229690.515625\n",
      "Train Epoch: 427 [43648/54000 (81%)] Loss: -221208.046875\n",
      "Train Epoch: 427 [45056/54000 (83%)] Loss: -248977.968750\n",
      "Train Epoch: 427 [46464/54000 (86%)] Loss: -222027.531250\n",
      "Train Epoch: 427 [47872/54000 (89%)] Loss: -219635.281250\n",
      "Train Epoch: 427 [49280/54000 (91%)] Loss: -249095.953125\n",
      "Train Epoch: 427 [50688/54000 (94%)] Loss: -223243.453125\n",
      "Train Epoch: 427 [52096/54000 (96%)] Loss: -230138.765625\n",
      "    epoch          : 427\n",
      "    loss           : -228645.16716507176\n",
      "    val_loss       : -230550.39819931402\n",
      "Train Epoch: 428 [0/54000 (0%)] Loss: -222733.046875\n",
      "Train Epoch: 428 [1408/54000 (3%)] Loss: -250388.937500\n",
      "Train Epoch: 428 [2816/54000 (5%)] Loss: -229752.312500\n",
      "Train Epoch: 428 [4224/54000 (8%)] Loss: -221647.453125\n",
      "Train Epoch: 428 [5632/54000 (10%)] Loss: -222939.156250\n",
      "Train Epoch: 428 [7040/54000 (13%)] Loss: -228553.187500\n",
      "Train Epoch: 428 [8448/54000 (16%)] Loss: -219759.796875\n",
      "Train Epoch: 428 [9856/54000 (18%)] Loss: -224077.484375\n",
      "Train Epoch: 428 [11264/54000 (21%)] Loss: -227458.406250\n",
      "Train Epoch: 428 [12672/54000 (23%)] Loss: -228026.375000\n",
      "Train Epoch: 428 [14080/54000 (26%)] Loss: -228413.328125\n",
      "Train Epoch: 428 [15488/54000 (29%)] Loss: -248501.875000\n",
      "Train Epoch: 428 [16896/54000 (31%)] Loss: -227858.250000\n",
      "Train Epoch: 428 [18304/54000 (34%)] Loss: -222779.687500\n",
      "Train Epoch: 428 [19712/54000 (37%)] Loss: -221987.781250\n",
      "Train Epoch: 428 [21120/54000 (39%)] Loss: -223554.062500\n",
      "Train Epoch: 428 [22528/54000 (42%)] Loss: -220093.968750\n",
      "Train Epoch: 428 [23936/54000 (44%)] Loss: -232091.265625\n",
      "Train Epoch: 428 [25344/54000 (47%)] Loss: -221960.718750\n",
      "Train Epoch: 428 [26752/54000 (50%)] Loss: -221078.500000\n",
      "Train Epoch: 428 [28160/54000 (52%)] Loss: -223615.171875\n",
      "Train Epoch: 428 [29568/54000 (55%)] Loss: -250477.234375\n",
      "Train Epoch: 428 [30976/54000 (57%)] Loss: -222871.265625\n",
      "Train Epoch: 428 [32384/54000 (60%)] Loss: -230828.625000\n",
      "Train Epoch: 428 [33792/54000 (63%)] Loss: -230418.484375\n",
      "Train Epoch: 428 [35200/54000 (65%)] Loss: -246023.984375\n",
      "Train Epoch: 428 [36608/54000 (68%)] Loss: -231457.250000\n",
      "Train Epoch: 428 [38016/54000 (70%)] Loss: -227738.515625\n",
      "Train Epoch: 428 [39424/54000 (73%)] Loss: -219414.812500\n",
      "Train Epoch: 428 [40832/54000 (76%)] Loss: -222800.500000\n",
      "Train Epoch: 428 [42240/54000 (78%)] Loss: -231643.406250\n",
      "Train Epoch: 428 [43648/54000 (81%)] Loss: -226868.265625\n",
      "Train Epoch: 428 [45056/54000 (83%)] Loss: -223682.109375\n",
      "Train Epoch: 428 [46464/54000 (86%)] Loss: -224061.000000\n",
      "Train Epoch: 428 [47872/54000 (89%)] Loss: -228530.562500\n",
      "Train Epoch: 428 [49280/54000 (91%)] Loss: -227579.343750\n",
      "Train Epoch: 428 [50688/54000 (94%)] Loss: -224047.000000\n",
      "Train Epoch: 428 [52096/54000 (96%)] Loss: -220510.000000\n",
      "    epoch          : 428\n",
      "    loss           : -228740.9455367823\n",
      "    val_loss       : -230546.60516506288\n",
      "Train Epoch: 429 [0/54000 (0%)] Loss: -250096.281250\n",
      "Train Epoch: 429 [1408/54000 (3%)] Loss: -224529.390625\n",
      "Train Epoch: 429 [2816/54000 (5%)] Loss: -226112.953125\n",
      "Train Epoch: 429 [4224/54000 (8%)] Loss: -223538.000000\n",
      "Train Epoch: 429 [5632/54000 (10%)] Loss: -227733.296875\n",
      "Train Epoch: 429 [7040/54000 (13%)] Loss: -227647.265625\n",
      "Train Epoch: 429 [8448/54000 (16%)] Loss: -223512.640625\n",
      "Train Epoch: 429 [9856/54000 (18%)] Loss: -230880.812500\n",
      "Train Epoch: 429 [11264/54000 (21%)] Loss: -229542.500000\n",
      "Train Epoch: 429 [12672/54000 (23%)] Loss: -250493.093750\n",
      "Train Epoch: 429 [14080/54000 (26%)] Loss: -225606.671875\n",
      "Train Epoch: 429 [15488/54000 (29%)] Loss: -227389.750000\n",
      "Train Epoch: 429 [16896/54000 (31%)] Loss: -249524.546875\n",
      "Train Epoch: 429 [18304/54000 (34%)] Loss: -224566.984375\n",
      "Train Epoch: 429 [19712/54000 (37%)] Loss: -223246.218750\n",
      "Train Epoch: 429 [21120/54000 (39%)] Loss: -250254.687500\n",
      "Train Epoch: 429 [22528/54000 (42%)] Loss: -223305.218750\n",
      "Train Epoch: 429 [23936/54000 (44%)] Loss: -222260.031250\n",
      "Train Epoch: 429 [25344/54000 (47%)] Loss: -223425.687500\n",
      "Train Epoch: 429 [26752/54000 (50%)] Loss: -231264.484375\n",
      "Train Epoch: 429 [28160/54000 (52%)] Loss: -227877.359375\n",
      "Train Epoch: 429 [29568/54000 (55%)] Loss: -227165.250000\n",
      "Train Epoch: 429 [30976/54000 (57%)] Loss: -230273.390625\n",
      "Train Epoch: 429 [32384/54000 (60%)] Loss: -228237.921875\n",
      "Train Epoch: 429 [33792/54000 (63%)] Loss: -219550.343750\n",
      "Train Epoch: 429 [35200/54000 (65%)] Loss: -223697.750000\n",
      "Train Epoch: 429 [36608/54000 (68%)] Loss: -223246.281250\n",
      "Train Epoch: 429 [38016/54000 (70%)] Loss: -223253.781250\n",
      "Train Epoch: 429 [39424/54000 (73%)] Loss: -222455.687500\n",
      "Train Epoch: 429 [40832/54000 (76%)] Loss: -223394.906250\n",
      "Train Epoch: 429 [42240/54000 (78%)] Loss: -224720.390625\n",
      "Train Epoch: 429 [43648/54000 (81%)] Loss: -225441.093750\n",
      "Train Epoch: 429 [45056/54000 (83%)] Loss: -221901.843750\n",
      "Train Epoch: 429 [46464/54000 (86%)] Loss: -228838.578125\n",
      "Train Epoch: 429 [47872/54000 (89%)] Loss: -228649.875000\n",
      "Train Epoch: 429 [49280/54000 (91%)] Loss: -230123.062500\n",
      "Train Epoch: 429 [50688/54000 (94%)] Loss: -233185.062500\n",
      "Train Epoch: 429 [52096/54000 (96%)] Loss: -250130.625000\n",
      "    epoch          : 429\n",
      "    loss           : -228613.98336572965\n",
      "    val_loss       : -230384.07437357088\n",
      "Train Epoch: 430 [0/54000 (0%)] Loss: -249458.250000\n",
      "Train Epoch: 430 [1408/54000 (3%)] Loss: -220823.906250\n",
      "Train Epoch: 430 [2816/54000 (5%)] Loss: -233036.812500\n",
      "Train Epoch: 430 [4224/54000 (8%)] Loss: -225638.640625\n",
      "Train Epoch: 430 [5632/54000 (10%)] Loss: -224024.328125\n",
      "Train Epoch: 430 [7040/54000 (13%)] Loss: -221058.093750\n",
      "Train Epoch: 430 [8448/54000 (16%)] Loss: -220506.578125\n",
      "Train Epoch: 430 [9856/54000 (18%)] Loss: -231841.312500\n",
      "Train Epoch: 430 [11264/54000 (21%)] Loss: -219381.343750\n",
      "Train Epoch: 430 [12672/54000 (23%)] Loss: -221354.359375\n",
      "Train Epoch: 430 [14080/54000 (26%)] Loss: -224523.765625\n",
      "Train Epoch: 430 [15488/54000 (29%)] Loss: -219342.828125\n",
      "Train Epoch: 430 [16896/54000 (31%)] Loss: -232310.812500\n",
      "Train Epoch: 430 [18304/54000 (34%)] Loss: -216440.812500\n",
      "Train Epoch: 430 [19712/54000 (37%)] Loss: -250143.750000\n",
      "Train Epoch: 430 [21120/54000 (39%)] Loss: -223546.203125\n",
      "Train Epoch: 430 [22528/54000 (42%)] Loss: -231188.375000\n",
      "Train Epoch: 430 [23936/54000 (44%)] Loss: -223030.375000\n",
      "Train Epoch: 430 [25344/54000 (47%)] Loss: -224031.281250\n",
      "Train Epoch: 430 [26752/54000 (50%)] Loss: -230917.500000\n",
      "Train Epoch: 430 [28160/54000 (52%)] Loss: -223407.218750\n",
      "Train Epoch: 430 [29568/54000 (55%)] Loss: -228647.562500\n",
      "Train Epoch: 430 [30976/54000 (57%)] Loss: -231583.593750\n",
      "Train Epoch: 430 [32384/54000 (60%)] Loss: -230820.468750\n",
      "Train Epoch: 430 [33792/54000 (63%)] Loss: -224975.375000\n",
      "Train Epoch: 430 [35200/54000 (65%)] Loss: -225172.468750\n",
      "Train Epoch: 430 [36608/54000 (68%)] Loss: -227449.296875\n",
      "Train Epoch: 430 [38016/54000 (70%)] Loss: -224029.937500\n",
      "Train Epoch: 430 [39424/54000 (73%)] Loss: -224207.312500\n",
      "Train Epoch: 430 [40832/54000 (76%)] Loss: -231442.437500\n",
      "Train Epoch: 430 [42240/54000 (78%)] Loss: -232075.156250\n",
      "Train Epoch: 430 [43648/54000 (81%)] Loss: -221527.578125\n",
      "Train Epoch: 430 [45056/54000 (83%)] Loss: -220257.046875\n",
      "Train Epoch: 430 [46464/54000 (86%)] Loss: -221561.312500\n",
      "Train Epoch: 430 [47872/54000 (89%)] Loss: -228072.843750\n",
      "Train Epoch: 430 [49280/54000 (91%)] Loss: -218993.812500\n",
      "Train Epoch: 430 [50688/54000 (94%)] Loss: -230754.125000\n",
      "Train Epoch: 430 [52096/54000 (96%)] Loss: -232094.234375\n",
      "    epoch          : 430\n",
      "    loss           : -228616.2947069378\n",
      "    val_loss       : -230618.0476491044\n",
      "Train Epoch: 431 [0/54000 (0%)] Loss: -250774.593750\n",
      "Train Epoch: 431 [1408/54000 (3%)] Loss: -228773.109375\n",
      "Train Epoch: 431 [2816/54000 (5%)] Loss: -231512.375000\n",
      "Train Epoch: 431 [4224/54000 (8%)] Loss: -232592.906250\n",
      "Train Epoch: 431 [5632/54000 (10%)] Loss: -220848.109375\n",
      "Train Epoch: 431 [7040/54000 (13%)] Loss: -228763.890625\n",
      "Train Epoch: 431 [8448/54000 (16%)] Loss: -229018.250000\n",
      "Train Epoch: 431 [9856/54000 (18%)] Loss: -224203.218750\n",
      "Train Epoch: 431 [11264/54000 (21%)] Loss: -231454.187500\n",
      "Train Epoch: 431 [12672/54000 (23%)] Loss: -231834.171875\n",
      "Train Epoch: 431 [14080/54000 (26%)] Loss: -230185.515625\n",
      "Train Epoch: 431 [15488/54000 (29%)] Loss: -227615.062500\n",
      "Train Epoch: 431 [16896/54000 (31%)] Loss: -232259.296875\n",
      "Train Epoch: 431 [18304/54000 (34%)] Loss: -217985.578125\n",
      "Train Epoch: 431 [19712/54000 (37%)] Loss: -231562.718750\n",
      "Train Epoch: 431 [21120/54000 (39%)] Loss: -230773.125000\n",
      "Train Epoch: 431 [22528/54000 (42%)] Loss: -226759.593750\n",
      "Train Epoch: 431 [23936/54000 (44%)] Loss: -251104.343750\n",
      "Train Epoch: 431 [25344/54000 (47%)] Loss: -221839.687500\n",
      "Train Epoch: 431 [26752/54000 (50%)] Loss: -230149.703125\n",
      "Train Epoch: 431 [28160/54000 (52%)] Loss: -230005.687500\n",
      "Train Epoch: 431 [29568/54000 (55%)] Loss: -222544.000000\n",
      "Train Epoch: 431 [30976/54000 (57%)] Loss: -225029.875000\n",
      "Train Epoch: 431 [32384/54000 (60%)] Loss: -230275.343750\n",
      "Train Epoch: 431 [33792/54000 (63%)] Loss: -220986.468750\n",
      "Train Epoch: 431 [35200/54000 (65%)] Loss: -219702.609375\n",
      "Train Epoch: 431 [36608/54000 (68%)] Loss: -249561.171875\n",
      "Train Epoch: 431 [38016/54000 (70%)] Loss: -227249.421875\n",
      "Train Epoch: 431 [39424/54000 (73%)] Loss: -226455.218750\n",
      "Train Epoch: 431 [40832/54000 (76%)] Loss: -230475.281250\n",
      "Train Epoch: 431 [42240/54000 (78%)] Loss: -227325.812500\n",
      "Train Epoch: 431 [43648/54000 (81%)] Loss: -225914.859375\n",
      "Train Epoch: 431 [45056/54000 (83%)] Loss: -222778.625000\n",
      "Train Epoch: 431 [46464/54000 (86%)] Loss: -229106.234375\n",
      "Train Epoch: 431 [47872/54000 (89%)] Loss: -231593.812500\n",
      "Train Epoch: 431 [49280/54000 (91%)] Loss: -223158.937500\n",
      "Train Epoch: 431 [50688/54000 (94%)] Loss: -231110.312500\n",
      "Train Epoch: 431 [52096/54000 (96%)] Loss: -230696.390625\n",
      "    epoch          : 431\n",
      "    loss           : -228575.2103020335\n",
      "    val_loss       : -230584.15121236662\n",
      "Train Epoch: 432 [0/54000 (0%)] Loss: -234295.531250\n",
      "Train Epoch: 432 [1408/54000 (3%)] Loss: -232055.078125\n",
      "Train Epoch: 432 [2816/54000 (5%)] Loss: -232192.765625\n",
      "Train Epoch: 432 [4224/54000 (8%)] Loss: -222274.156250\n",
      "Train Epoch: 432 [5632/54000 (10%)] Loss: -221193.515625\n",
      "Train Epoch: 432 [7040/54000 (13%)] Loss: -222987.390625\n",
      "Train Epoch: 432 [8448/54000 (16%)] Loss: -222797.312500\n",
      "Train Epoch: 432 [9856/54000 (18%)] Loss: -224306.671875\n",
      "Train Epoch: 432 [11264/54000 (21%)] Loss: -226294.843750\n",
      "Train Epoch: 432 [12672/54000 (23%)] Loss: -228084.125000\n",
      "Train Epoch: 432 [14080/54000 (26%)] Loss: -222837.359375\n",
      "Train Epoch: 432 [15488/54000 (29%)] Loss: -227776.718750\n",
      "Train Epoch: 432 [16896/54000 (31%)] Loss: -230483.781250\n",
      "Train Epoch: 432 [18304/54000 (34%)] Loss: -225041.046875\n",
      "Train Epoch: 432 [19712/54000 (37%)] Loss: -226671.109375\n",
      "Train Epoch: 432 [21120/54000 (39%)] Loss: -230996.562500\n",
      "Train Epoch: 432 [22528/54000 (42%)] Loss: -226855.296875\n",
      "Train Epoch: 432 [23936/54000 (44%)] Loss: -226239.281250\n",
      "Train Epoch: 432 [25344/54000 (47%)] Loss: -224968.828125\n",
      "Train Epoch: 432 [26752/54000 (50%)] Loss: -229049.250000\n",
      "Train Epoch: 432 [28160/54000 (52%)] Loss: -229858.687500\n",
      "Train Epoch: 432 [29568/54000 (55%)] Loss: -230298.781250\n",
      "Train Epoch: 432 [30976/54000 (57%)] Loss: -219819.562500\n",
      "Train Epoch: 432 [32384/54000 (60%)] Loss: -231156.359375\n",
      "Train Epoch: 432 [33792/54000 (63%)] Loss: -249904.750000\n",
      "Train Epoch: 432 [35200/54000 (65%)] Loss: -226705.625000\n",
      "Train Epoch: 432 [36608/54000 (68%)] Loss: -232346.671875\n",
      "Train Epoch: 432 [38016/54000 (70%)] Loss: -250224.218750\n",
      "Train Epoch: 432 [39424/54000 (73%)] Loss: -248872.671875\n",
      "Train Epoch: 432 [40832/54000 (76%)] Loss: -234065.843750\n",
      "Train Epoch: 432 [42240/54000 (78%)] Loss: -224372.421875\n",
      "Train Epoch: 432 [43648/54000 (81%)] Loss: -248824.031250\n",
      "Train Epoch: 432 [45056/54000 (83%)] Loss: -223578.937500\n",
      "Train Epoch: 432 [46464/54000 (86%)] Loss: -225277.281250\n",
      "Train Epoch: 432 [47872/54000 (89%)] Loss: -224234.890625\n",
      "Train Epoch: 432 [49280/54000 (91%)] Loss: -224041.562500\n",
      "Train Epoch: 432 [50688/54000 (94%)] Loss: -250045.781250\n",
      "Train Epoch: 432 [52096/54000 (96%)] Loss: -230695.375000\n",
      "    epoch          : 432\n",
      "    loss           : -228740.82782595695\n",
      "    val_loss       : -230146.57167611472\n",
      "Train Epoch: 433 [0/54000 (0%)] Loss: -249571.031250\n",
      "Train Epoch: 433 [1408/54000 (3%)] Loss: -227864.203125\n",
      "Train Epoch: 433 [2816/54000 (5%)] Loss: -231806.734375\n",
      "Train Epoch: 433 [4224/54000 (8%)] Loss: -223085.312500\n",
      "Train Epoch: 433 [5632/54000 (10%)] Loss: -232245.421875\n",
      "Train Epoch: 433 [7040/54000 (13%)] Loss: -249555.750000\n",
      "Train Epoch: 433 [8448/54000 (16%)] Loss: -230155.953125\n",
      "Train Epoch: 433 [9856/54000 (18%)] Loss: -225062.109375\n",
      "Train Epoch: 433 [11264/54000 (21%)] Loss: -250787.453125\n",
      "Train Epoch: 433 [12672/54000 (23%)] Loss: -223824.500000\n",
      "Train Epoch: 433 [14080/54000 (26%)] Loss: -230382.390625\n",
      "Train Epoch: 433 [15488/54000 (29%)] Loss: -226078.937500\n",
      "Train Epoch: 433 [16896/54000 (31%)] Loss: -221855.390625\n",
      "Train Epoch: 433 [18304/54000 (34%)] Loss: -227146.578125\n",
      "Train Epoch: 433 [19712/54000 (37%)] Loss: -250909.859375\n",
      "Train Epoch: 433 [21120/54000 (39%)] Loss: -224110.531250\n",
      "Train Epoch: 433 [22528/54000 (42%)] Loss: -233403.484375\n",
      "Train Epoch: 433 [23936/54000 (44%)] Loss: -227329.531250\n",
      "Train Epoch: 433 [25344/54000 (47%)] Loss: -232312.015625\n",
      "Train Epoch: 433 [26752/54000 (50%)] Loss: -231449.093750\n",
      "Train Epoch: 433 [28160/54000 (52%)] Loss: -241350.093750\n",
      "Train Epoch: 433 [29568/54000 (55%)] Loss: -229589.406250\n",
      "Train Epoch: 433 [30976/54000 (57%)] Loss: -228110.500000\n",
      "Train Epoch: 433 [32384/54000 (60%)] Loss: -223704.781250\n",
      "Train Epoch: 433 [33792/54000 (63%)] Loss: -231649.343750\n",
      "Train Epoch: 433 [35200/54000 (65%)] Loss: -226169.875000\n",
      "Train Epoch: 433 [36608/54000 (68%)] Loss: -250754.281250\n",
      "Train Epoch: 433 [38016/54000 (70%)] Loss: -224899.390625\n",
      "Train Epoch: 433 [39424/54000 (73%)] Loss: -223621.625000\n",
      "Train Epoch: 433 [40832/54000 (76%)] Loss: -220255.421875\n",
      "Train Epoch: 433 [42240/54000 (78%)] Loss: -228789.937500\n",
      "Train Epoch: 433 [43648/54000 (81%)] Loss: -248175.562500\n",
      "Train Epoch: 433 [45056/54000 (83%)] Loss: -224979.656250\n",
      "Train Epoch: 433 [46464/54000 (86%)] Loss: -227175.093750\n",
      "Train Epoch: 433 [47872/54000 (89%)] Loss: -230431.546875\n",
      "Train Epoch: 433 [49280/54000 (91%)] Loss: -220797.375000\n",
      "Train Epoch: 433 [50688/54000 (94%)] Loss: -250032.906250\n",
      "Train Epoch: 433 [52096/54000 (96%)] Loss: -220120.312500\n",
      "    epoch          : 433\n",
      "    loss           : -228619.37466357654\n",
      "    val_loss       : -230239.38835032392\n",
      "Train Epoch: 434 [0/54000 (0%)] Loss: -222798.921875\n",
      "Train Epoch: 434 [1408/54000 (3%)] Loss: -243864.656250\n",
      "Train Epoch: 434 [2816/54000 (5%)] Loss: -228498.421875\n",
      "Train Epoch: 434 [4224/54000 (8%)] Loss: -221888.375000\n",
      "Train Epoch: 434 [5632/54000 (10%)] Loss: -232207.296875\n",
      "Train Epoch: 434 [7040/54000 (13%)] Loss: -233277.015625\n",
      "Train Epoch: 434 [8448/54000 (16%)] Loss: -231428.265625\n",
      "Train Epoch: 434 [9856/54000 (18%)] Loss: -250729.937500\n",
      "Train Epoch: 434 [11264/54000 (21%)] Loss: -222460.921875\n",
      "Train Epoch: 434 [12672/54000 (23%)] Loss: -217466.046875\n",
      "Train Epoch: 434 [14080/54000 (26%)] Loss: -250005.859375\n",
      "Train Epoch: 434 [15488/54000 (29%)] Loss: -232550.328125\n",
      "Train Epoch: 434 [16896/54000 (31%)] Loss: -218244.500000\n",
      "Train Epoch: 434 [18304/54000 (34%)] Loss: -250938.734375\n",
      "Train Epoch: 434 [19712/54000 (37%)] Loss: -223796.578125\n",
      "Train Epoch: 434 [21120/54000 (39%)] Loss: -230443.421875\n",
      "Train Epoch: 434 [22528/54000 (42%)] Loss: -225415.187500\n",
      "Train Epoch: 434 [23936/54000 (44%)] Loss: -228554.250000\n",
      "Train Epoch: 434 [25344/54000 (47%)] Loss: -231778.093750\n",
      "Train Epoch: 434 [26752/54000 (50%)] Loss: -222014.156250\n",
      "Train Epoch: 434 [28160/54000 (52%)] Loss: -226451.031250\n",
      "Train Epoch: 434 [29568/54000 (55%)] Loss: -221907.500000\n",
      "Train Epoch: 434 [30976/54000 (57%)] Loss: -227558.562500\n",
      "Train Epoch: 434 [32384/54000 (60%)] Loss: -227508.109375\n",
      "Train Epoch: 434 [33792/54000 (63%)] Loss: -232281.781250\n",
      "Train Epoch: 434 [35200/54000 (65%)] Loss: -226600.312500\n",
      "Train Epoch: 434 [36608/54000 (68%)] Loss: -232440.093750\n",
      "Train Epoch: 434 [38016/54000 (70%)] Loss: -225292.062500\n",
      "Train Epoch: 434 [39424/54000 (73%)] Loss: -218850.500000\n",
      "Train Epoch: 434 [40832/54000 (76%)] Loss: -219236.875000\n",
      "Train Epoch: 434 [42240/54000 (78%)] Loss: -231424.546875\n",
      "Train Epoch: 434 [43648/54000 (81%)] Loss: -219173.687500\n",
      "Train Epoch: 434 [45056/54000 (83%)] Loss: -218709.656250\n",
      "Train Epoch: 434 [46464/54000 (86%)] Loss: -220927.046875\n",
      "Train Epoch: 434 [47872/54000 (89%)] Loss: -228293.015625\n",
      "Train Epoch: 434 [49280/54000 (91%)] Loss: -227126.609375\n",
      "Train Epoch: 434 [50688/54000 (94%)] Loss: -250397.750000\n",
      "Train Epoch: 434 [52096/54000 (96%)] Loss: -217985.578125\n",
      "    epoch          : 434\n",
      "    loss           : -228669.9286034689\n",
      "    val_loss       : -230734.25164348324\n",
      "Train Epoch: 435 [0/54000 (0%)] Loss: -222286.296875\n",
      "Train Epoch: 435 [1408/54000 (3%)] Loss: -226384.906250\n",
      "Train Epoch: 435 [2816/54000 (5%)] Loss: -223209.625000\n",
      "Train Epoch: 435 [4224/54000 (8%)] Loss: -231410.750000\n",
      "Train Epoch: 435 [5632/54000 (10%)] Loss: -232433.890625\n",
      "Train Epoch: 435 [7040/54000 (13%)] Loss: -230125.984375\n",
      "Train Epoch: 435 [8448/54000 (16%)] Loss: -224615.140625\n",
      "Train Epoch: 435 [9856/54000 (18%)] Loss: -229313.687500\n",
      "Train Epoch: 435 [11264/54000 (21%)] Loss: -231524.562500\n",
      "Train Epoch: 435 [12672/54000 (23%)] Loss: -249032.156250\n",
      "Train Epoch: 435 [14080/54000 (26%)] Loss: -220198.531250\n",
      "Train Epoch: 435 [15488/54000 (29%)] Loss: -230392.921875\n",
      "Train Epoch: 435 [16896/54000 (31%)] Loss: -221109.578125\n",
      "Train Epoch: 435 [18304/54000 (34%)] Loss: -232016.906250\n",
      "Train Epoch: 435 [19712/54000 (37%)] Loss: -223069.593750\n",
      "Train Epoch: 435 [21120/54000 (39%)] Loss: -231224.687500\n",
      "Train Epoch: 435 [22528/54000 (42%)] Loss: -231064.875000\n",
      "Train Epoch: 435 [23936/54000 (44%)] Loss: -231148.656250\n",
      "Train Epoch: 435 [25344/54000 (47%)] Loss: -221659.656250\n",
      "Train Epoch: 435 [26752/54000 (50%)] Loss: -232530.750000\n",
      "Train Epoch: 435 [28160/54000 (52%)] Loss: -231141.265625\n",
      "Train Epoch: 435 [29568/54000 (55%)] Loss: -217904.312500\n",
      "Train Epoch: 435 [30976/54000 (57%)] Loss: -249347.781250\n",
      "Train Epoch: 435 [32384/54000 (60%)] Loss: -219713.421875\n",
      "Train Epoch: 435 [33792/54000 (63%)] Loss: -230378.843750\n",
      "Train Epoch: 435 [35200/54000 (65%)] Loss: -226450.968750\n",
      "Train Epoch: 435 [36608/54000 (68%)] Loss: -226991.343750\n",
      "Train Epoch: 435 [38016/54000 (70%)] Loss: -247316.093750\n",
      "Train Epoch: 435 [39424/54000 (73%)] Loss: -231092.734375\n",
      "Train Epoch: 435 [40832/54000 (76%)] Loss: -232131.031250\n",
      "Train Epoch: 435 [42240/54000 (78%)] Loss: -221943.265625\n",
      "Train Epoch: 435 [43648/54000 (81%)] Loss: -222822.250000\n",
      "Train Epoch: 435 [45056/54000 (83%)] Loss: -249907.421875\n",
      "Train Epoch: 435 [46464/54000 (86%)] Loss: -222594.843750\n",
      "Train Epoch: 435 [47872/54000 (89%)] Loss: -224788.484375\n",
      "Train Epoch: 435 [49280/54000 (91%)] Loss: -232963.875000\n",
      "Train Epoch: 435 [50688/54000 (94%)] Loss: -226647.593750\n",
      "Train Epoch: 435 [52096/54000 (96%)] Loss: -218133.765625\n",
      "    epoch          : 435\n",
      "    loss           : -228566.8712993421\n",
      "    val_loss       : -230495.7310701696\n",
      "Train Epoch: 436 [0/54000 (0%)] Loss: -248920.593750\n",
      "Train Epoch: 436 [1408/54000 (3%)] Loss: -222635.546875\n",
      "Train Epoch: 436 [2816/54000 (5%)] Loss: -226417.062500\n",
      "Train Epoch: 436 [4224/54000 (8%)] Loss: -224842.078125\n",
      "Train Epoch: 436 [5632/54000 (10%)] Loss: -226068.078125\n",
      "Train Epoch: 436 [7040/54000 (13%)] Loss: -230800.531250\n",
      "Train Epoch: 436 [8448/54000 (16%)] Loss: -221798.125000\n",
      "Train Epoch: 436 [9856/54000 (18%)] Loss: -222635.296875\n",
      "Train Epoch: 436 [11264/54000 (21%)] Loss: -231984.437500\n",
      "Train Epoch: 436 [12672/54000 (23%)] Loss: -224036.062500\n",
      "Train Epoch: 436 [14080/54000 (26%)] Loss: -224870.156250\n",
      "Train Epoch: 436 [15488/54000 (29%)] Loss: -220270.625000\n",
      "Train Epoch: 436 [16896/54000 (31%)] Loss: -224136.421875\n",
      "Train Epoch: 436 [18304/54000 (34%)] Loss: -218087.703125\n",
      "Train Epoch: 436 [19712/54000 (37%)] Loss: -224562.843750\n",
      "Train Epoch: 436 [21120/54000 (39%)] Loss: -225423.062500\n",
      "Train Epoch: 436 [22528/54000 (42%)] Loss: -222928.171875\n",
      "Train Epoch: 436 [23936/54000 (44%)] Loss: -222670.359375\n",
      "Train Epoch: 436 [25344/54000 (47%)] Loss: -223020.953125\n",
      "Train Epoch: 436 [26752/54000 (50%)] Loss: -231783.093750\n",
      "Train Epoch: 436 [28160/54000 (52%)] Loss: -232389.218750\n",
      "Train Epoch: 436 [29568/54000 (55%)] Loss: -228313.343750\n",
      "Train Epoch: 436 [30976/54000 (57%)] Loss: -222754.921875\n",
      "Train Epoch: 436 [32384/54000 (60%)] Loss: -249770.578125\n",
      "Train Epoch: 436 [33792/54000 (63%)] Loss: -227083.750000\n",
      "Train Epoch: 436 [35200/54000 (65%)] Loss: -225650.281250\n",
      "Train Epoch: 436 [36608/54000 (68%)] Loss: -231627.156250\n",
      "Train Epoch: 436 [38016/54000 (70%)] Loss: -250387.250000\n",
      "Train Epoch: 436 [39424/54000 (73%)] Loss: -223779.859375\n",
      "Train Epoch: 436 [40832/54000 (76%)] Loss: -220737.234375\n",
      "Train Epoch: 436 [42240/54000 (78%)] Loss: -220139.437500\n",
      "Train Epoch: 436 [43648/54000 (81%)] Loss: -220541.171875\n",
      "Train Epoch: 436 [45056/54000 (83%)] Loss: -218698.406250\n",
      "Train Epoch: 436 [46464/54000 (86%)] Loss: -226455.218750\n",
      "Train Epoch: 436 [47872/54000 (89%)] Loss: -231183.250000\n",
      "Train Epoch: 436 [49280/54000 (91%)] Loss: -229889.609375\n",
      "Train Epoch: 436 [50688/54000 (94%)] Loss: -248871.578125\n",
      "Train Epoch: 436 [52096/54000 (96%)] Loss: -223816.625000\n",
      "    epoch          : 436\n",
      "    loss           : -228669.47914174641\n",
      "    val_loss       : -230568.99932712462\n",
      "Train Epoch: 437 [0/54000 (0%)] Loss: -232216.468750\n",
      "Train Epoch: 437 [1408/54000 (3%)] Loss: -218054.437500\n",
      "Train Epoch: 437 [2816/54000 (5%)] Loss: -249426.828125\n",
      "Train Epoch: 437 [4224/54000 (8%)] Loss: -225170.515625\n",
      "Train Epoch: 437 [5632/54000 (10%)] Loss: -223621.593750\n",
      "Train Epoch: 437 [7040/54000 (13%)] Loss: -217943.781250\n",
      "Train Epoch: 437 [8448/54000 (16%)] Loss: -222201.828125\n",
      "Train Epoch: 437 [9856/54000 (18%)] Loss: -230076.781250\n",
      "Train Epoch: 437 [11264/54000 (21%)] Loss: -228724.546875\n",
      "Train Epoch: 437 [12672/54000 (23%)] Loss: -231107.812500\n",
      "Train Epoch: 437 [14080/54000 (26%)] Loss: -219489.187500\n",
      "Train Epoch: 437 [15488/54000 (29%)] Loss: -224855.843750\n",
      "Train Epoch: 437 [16896/54000 (31%)] Loss: -219738.656250\n",
      "Train Epoch: 437 [18304/54000 (34%)] Loss: -225917.750000\n",
      "Train Epoch: 437 [19712/54000 (37%)] Loss: -232633.859375\n",
      "Train Epoch: 437 [21120/54000 (39%)] Loss: -249062.687500\n",
      "Train Epoch: 437 [22528/54000 (42%)] Loss: -218553.093750\n",
      "Train Epoch: 437 [23936/54000 (44%)] Loss: -221850.859375\n",
      "Train Epoch: 437 [25344/54000 (47%)] Loss: -227809.703125\n",
      "Train Epoch: 437 [26752/54000 (50%)] Loss: -230194.453125\n",
      "Train Epoch: 437 [28160/54000 (52%)] Loss: -225722.156250\n",
      "Train Epoch: 437 [29568/54000 (55%)] Loss: -220675.703125\n",
      "Train Epoch: 437 [30976/54000 (57%)] Loss: -232842.531250\n",
      "Train Epoch: 437 [32384/54000 (60%)] Loss: -232523.265625\n",
      "Train Epoch: 437 [33792/54000 (63%)] Loss: -231633.656250\n",
      "Train Epoch: 437 [35200/54000 (65%)] Loss: -223783.406250\n",
      "Train Epoch: 437 [36608/54000 (68%)] Loss: -219127.843750\n",
      "Train Epoch: 437 [38016/54000 (70%)] Loss: -229087.609375\n",
      "Train Epoch: 437 [39424/54000 (73%)] Loss: -230328.437500\n",
      "Train Epoch: 437 [40832/54000 (76%)] Loss: -231461.203125\n",
      "Train Epoch: 437 [42240/54000 (78%)] Loss: -224377.281250\n",
      "Train Epoch: 437 [43648/54000 (81%)] Loss: -248277.328125\n",
      "Train Epoch: 437 [45056/54000 (83%)] Loss: -224545.203125\n",
      "Train Epoch: 437 [46464/54000 (86%)] Loss: -232112.515625\n",
      "Train Epoch: 437 [47872/54000 (89%)] Loss: -221814.781250\n",
      "Train Epoch: 437 [49280/54000 (91%)] Loss: -219387.062500\n",
      "Train Epoch: 437 [50688/54000 (94%)] Loss: -248821.937500\n",
      "Train Epoch: 437 [52096/54000 (96%)] Loss: -231287.875000\n",
      "    epoch          : 437\n",
      "    loss           : -228650.0004111842\n",
      "    val_loss       : -230426.9623249333\n",
      "Train Epoch: 438 [0/54000 (0%)] Loss: -221879.828125\n",
      "Train Epoch: 438 [1408/54000 (3%)] Loss: -222338.390625\n",
      "Train Epoch: 438 [2816/54000 (5%)] Loss: -251086.156250\n",
      "Train Epoch: 438 [4224/54000 (8%)] Loss: -223109.921875\n",
      "Train Epoch: 438 [5632/54000 (10%)] Loss: -223842.703125\n",
      "Train Epoch: 438 [7040/54000 (13%)] Loss: -221168.843750\n",
      "Train Epoch: 438 [8448/54000 (16%)] Loss: -219818.515625\n",
      "Train Epoch: 438 [9856/54000 (18%)] Loss: -220403.750000\n",
      "Train Epoch: 438 [11264/54000 (21%)] Loss: -225580.062500\n",
      "Train Epoch: 438 [12672/54000 (23%)] Loss: -229268.781250\n",
      "Train Epoch: 438 [14080/54000 (26%)] Loss: -225025.656250\n",
      "Train Epoch: 438 [15488/54000 (29%)] Loss: -224221.328125\n",
      "Train Epoch: 438 [16896/54000 (31%)] Loss: -250243.000000\n",
      "Train Epoch: 438 [18304/54000 (34%)] Loss: -222595.781250\n",
      "Train Epoch: 438 [19712/54000 (37%)] Loss: -224759.281250\n",
      "Train Epoch: 438 [21120/54000 (39%)] Loss: -221792.562500\n",
      "Train Epoch: 438 [22528/54000 (42%)] Loss: -228526.031250\n",
      "Train Epoch: 438 [23936/54000 (44%)] Loss: -219793.375000\n",
      "Train Epoch: 438 [25344/54000 (47%)] Loss: -230531.937500\n",
      "Train Epoch: 438 [26752/54000 (50%)] Loss: -225755.218750\n",
      "Train Epoch: 438 [28160/54000 (52%)] Loss: -230985.578125\n",
      "Train Epoch: 438 [29568/54000 (55%)] Loss: -231104.187500\n",
      "Train Epoch: 438 [30976/54000 (57%)] Loss: -231709.515625\n",
      "Train Epoch: 438 [32384/54000 (60%)] Loss: -249277.234375\n",
      "Train Epoch: 438 [33792/54000 (63%)] Loss: -223792.796875\n",
      "Train Epoch: 438 [35200/54000 (65%)] Loss: -225950.968750\n",
      "Train Epoch: 438 [36608/54000 (68%)] Loss: -226269.703125\n",
      "Train Epoch: 438 [38016/54000 (70%)] Loss: -222499.312500\n",
      "Train Epoch: 438 [39424/54000 (73%)] Loss: -226893.703125\n",
      "Train Epoch: 438 [40832/54000 (76%)] Loss: -231206.781250\n",
      "Train Epoch: 438 [42240/54000 (78%)] Loss: -227756.343750\n",
      "Train Epoch: 438 [43648/54000 (81%)] Loss: -225560.500000\n",
      "Train Epoch: 438 [45056/54000 (83%)] Loss: -250262.687500\n",
      "Train Epoch: 438 [46464/54000 (86%)] Loss: -225586.203125\n",
      "Train Epoch: 438 [47872/54000 (89%)] Loss: -223131.156250\n",
      "Train Epoch: 438 [49280/54000 (91%)] Loss: -232073.718750\n",
      "Train Epoch: 438 [50688/54000 (94%)] Loss: -229800.218750\n",
      "Train Epoch: 438 [52096/54000 (96%)] Loss: -249561.437500\n",
      "    epoch          : 438\n",
      "    loss           : -228745.54908044258\n",
      "    val_loss       : -231079.336056593\n",
      "Train Epoch: 439 [0/54000 (0%)] Loss: -219348.578125\n",
      "Train Epoch: 439 [1408/54000 (3%)] Loss: -223735.671875\n",
      "Train Epoch: 439 [2816/54000 (5%)] Loss: -220244.437500\n",
      "Train Epoch: 439 [4224/54000 (8%)] Loss: -223100.000000\n",
      "Train Epoch: 439 [5632/54000 (10%)] Loss: -231560.343750\n",
      "Train Epoch: 439 [7040/54000 (13%)] Loss: -226766.781250\n",
      "Train Epoch: 439 [8448/54000 (16%)] Loss: -223326.250000\n",
      "Train Epoch: 439 [9856/54000 (18%)] Loss: -249398.609375\n",
      "Train Epoch: 439 [11264/54000 (21%)] Loss: -250784.156250\n",
      "Train Epoch: 439 [12672/54000 (23%)] Loss: -226858.734375\n",
      "Train Epoch: 439 [14080/54000 (26%)] Loss: -227332.875000\n",
      "Train Epoch: 439 [15488/54000 (29%)] Loss: -250293.859375\n",
      "Train Epoch: 439 [16896/54000 (31%)] Loss: -226020.078125\n",
      "Train Epoch: 439 [18304/54000 (34%)] Loss: -231095.671875\n",
      "Train Epoch: 439 [19712/54000 (37%)] Loss: -223415.593750\n",
      "Train Epoch: 439 [21120/54000 (39%)] Loss: -230357.109375\n",
      "Train Epoch: 439 [22528/54000 (42%)] Loss: -229790.453125\n",
      "Train Epoch: 439 [23936/54000 (44%)] Loss: -231006.562500\n",
      "Train Epoch: 439 [25344/54000 (47%)] Loss: -224766.796875\n",
      "Train Epoch: 439 [26752/54000 (50%)] Loss: -224245.515625\n",
      "Train Epoch: 439 [28160/54000 (52%)] Loss: -220678.093750\n",
      "Train Epoch: 439 [29568/54000 (55%)] Loss: -226578.843750\n",
      "Train Epoch: 439 [30976/54000 (57%)] Loss: -230491.250000\n",
      "Train Epoch: 439 [32384/54000 (60%)] Loss: -247873.234375\n",
      "Train Epoch: 439 [33792/54000 (63%)] Loss: -219513.906250\n",
      "Train Epoch: 439 [35200/54000 (65%)] Loss: -224392.250000\n",
      "Train Epoch: 439 [36608/54000 (68%)] Loss: -226315.750000\n",
      "Train Epoch: 439 [38016/54000 (70%)] Loss: -224450.062500\n",
      "Train Epoch: 439 [39424/54000 (73%)] Loss: -230738.765625\n",
      "Train Epoch: 439 [40832/54000 (76%)] Loss: -224025.546875\n",
      "Train Epoch: 439 [42240/54000 (78%)] Loss: -224238.375000\n",
      "Train Epoch: 439 [43648/54000 (81%)] Loss: -231176.031250\n",
      "Train Epoch: 439 [45056/54000 (83%)] Loss: -222629.375000\n",
      "Train Epoch: 439 [46464/54000 (86%)] Loss: -227968.343750\n",
      "Train Epoch: 439 [47872/54000 (89%)] Loss: -225259.750000\n",
      "Train Epoch: 439 [49280/54000 (91%)] Loss: -224596.890625\n",
      "Train Epoch: 439 [50688/54000 (94%)] Loss: -221596.812500\n",
      "Train Epoch: 439 [52096/54000 (96%)] Loss: -231889.593750\n",
      "    epoch          : 439\n",
      "    loss           : -228636.26211124402\n",
      "    val_loss       : -230801.89818740473\n",
      "Train Epoch: 440 [0/54000 (0%)] Loss: -248663.921875\n",
      "Train Epoch: 440 [1408/54000 (3%)] Loss: -216421.171875\n",
      "Train Epoch: 440 [2816/54000 (5%)] Loss: -224547.625000\n",
      "Train Epoch: 440 [4224/54000 (8%)] Loss: -219339.875000\n",
      "Train Epoch: 440 [5632/54000 (10%)] Loss: -232691.140625\n",
      "Train Epoch: 440 [7040/54000 (13%)] Loss: -223265.953125\n",
      "Train Epoch: 440 [8448/54000 (16%)] Loss: -224416.781250\n",
      "Train Epoch: 440 [9856/54000 (18%)] Loss: -222541.125000\n",
      "Train Epoch: 440 [11264/54000 (21%)] Loss: -220468.656250\n",
      "Train Epoch: 440 [12672/54000 (23%)] Loss: -228117.937500\n",
      "Train Epoch: 440 [14080/54000 (26%)] Loss: -249414.515625\n",
      "Train Epoch: 440 [15488/54000 (29%)] Loss: -228890.531250\n",
      "Train Epoch: 440 [16896/54000 (31%)] Loss: -221525.500000\n",
      "Train Epoch: 440 [18304/54000 (34%)] Loss: -231899.250000\n",
      "Train Epoch: 440 [19712/54000 (37%)] Loss: -222462.140625\n",
      "Train Epoch: 440 [21120/54000 (39%)] Loss: -224751.265625\n",
      "Train Epoch: 440 [22528/54000 (42%)] Loss: -231916.312500\n",
      "Train Epoch: 440 [23936/54000 (44%)] Loss: -222094.609375\n",
      "Train Epoch: 440 [25344/54000 (47%)] Loss: -222371.500000\n",
      "Train Epoch: 440 [26752/54000 (50%)] Loss: -222146.859375\n",
      "Train Epoch: 440 [28160/54000 (52%)] Loss: -224438.031250\n",
      "Train Epoch: 440 [29568/54000 (55%)] Loss: -226786.234375\n",
      "Train Epoch: 440 [30976/54000 (57%)] Loss: -231887.656250\n",
      "Train Epoch: 440 [32384/54000 (60%)] Loss: -231273.203125\n",
      "Train Epoch: 440 [33792/54000 (63%)] Loss: -228655.062500\n",
      "Train Epoch: 440 [35200/54000 (65%)] Loss: -229144.906250\n",
      "Train Epoch: 440 [36608/54000 (68%)] Loss: -232700.718750\n",
      "Train Epoch: 440 [38016/54000 (70%)] Loss: -225742.437500\n",
      "Train Epoch: 440 [39424/54000 (73%)] Loss: -249286.500000\n",
      "Train Epoch: 440 [40832/54000 (76%)] Loss: -225466.750000\n",
      "Train Epoch: 440 [42240/54000 (78%)] Loss: -220866.015625\n",
      "Train Epoch: 440 [43648/54000 (81%)] Loss: -229353.781250\n",
      "Train Epoch: 440 [45056/54000 (83%)] Loss: -223385.375000\n",
      "Train Epoch: 440 [46464/54000 (86%)] Loss: -218805.093750\n",
      "Train Epoch: 440 [47872/54000 (89%)] Loss: -221527.343750\n",
      "Train Epoch: 440 [49280/54000 (91%)] Loss: -231092.312500\n",
      "Train Epoch: 440 [50688/54000 (94%)] Loss: -223202.609375\n",
      "Train Epoch: 440 [52096/54000 (96%)] Loss: -233134.937500\n",
      "    epoch          : 440\n",
      "    loss           : -228643.40617523924\n",
      "    val_loss       : -230371.10702886814\n",
      "Train Epoch: 441 [0/54000 (0%)] Loss: -230672.640625\n",
      "Train Epoch: 441 [1408/54000 (3%)] Loss: -231006.234375\n",
      "Train Epoch: 441 [2816/54000 (5%)] Loss: -233275.546875\n",
      "Train Epoch: 441 [4224/54000 (8%)] Loss: -232118.312500\n",
      "Train Epoch: 441 [5632/54000 (10%)] Loss: -226784.312500\n",
      "Train Epoch: 441 [7040/54000 (13%)] Loss: -231296.625000\n",
      "Train Epoch: 441 [8448/54000 (16%)] Loss: -231169.296875\n",
      "Train Epoch: 441 [9856/54000 (18%)] Loss: -229675.625000\n",
      "Train Epoch: 441 [11264/54000 (21%)] Loss: -229862.640625\n",
      "Train Epoch: 441 [12672/54000 (23%)] Loss: -218622.218750\n",
      "Train Epoch: 441 [14080/54000 (26%)] Loss: -250726.500000\n",
      "Train Epoch: 441 [15488/54000 (29%)] Loss: -229858.140625\n",
      "Train Epoch: 441 [16896/54000 (31%)] Loss: -226368.109375\n",
      "Train Epoch: 441 [18304/54000 (34%)] Loss: -231768.250000\n",
      "Train Epoch: 441 [19712/54000 (37%)] Loss: -230966.343750\n",
      "Train Epoch: 441 [21120/54000 (39%)] Loss: -249230.921875\n",
      "Train Epoch: 441 [22528/54000 (42%)] Loss: -230655.062500\n",
      "Train Epoch: 441 [23936/54000 (44%)] Loss: -233191.828125\n",
      "Train Epoch: 441 [25344/54000 (47%)] Loss: -226369.687500\n",
      "Train Epoch: 441 [26752/54000 (50%)] Loss: -223532.734375\n",
      "Train Epoch: 441 [28160/54000 (52%)] Loss: -222533.718750\n",
      "Train Epoch: 441 [29568/54000 (55%)] Loss: -223957.203125\n",
      "Train Epoch: 441 [30976/54000 (57%)] Loss: -230293.625000\n",
      "Train Epoch: 441 [32384/54000 (60%)] Loss: -231572.062500\n",
      "Train Epoch: 441 [33792/54000 (63%)] Loss: -250572.468750\n",
      "Train Epoch: 441 [35200/54000 (65%)] Loss: -224227.953125\n",
      "Train Epoch: 441 [36608/54000 (68%)] Loss: -223004.718750\n",
      "Train Epoch: 441 [38016/54000 (70%)] Loss: -223563.640625\n",
      "Train Epoch: 441 [39424/54000 (73%)] Loss: -221647.859375\n",
      "Train Epoch: 441 [40832/54000 (76%)] Loss: -221969.187500\n",
      "Train Epoch: 441 [42240/54000 (78%)] Loss: -232644.218750\n",
      "Train Epoch: 441 [43648/54000 (81%)] Loss: -229087.578125\n",
      "Train Epoch: 441 [45056/54000 (83%)] Loss: -250143.203125\n",
      "Train Epoch: 441 [46464/54000 (86%)] Loss: -250118.562500\n",
      "Train Epoch: 441 [47872/54000 (89%)] Loss: -225447.500000\n",
      "Train Epoch: 441 [49280/54000 (91%)] Loss: -220842.000000\n",
      "Train Epoch: 441 [50688/54000 (94%)] Loss: -221012.531250\n",
      "Train Epoch: 441 [52096/54000 (96%)] Loss: -250706.390625\n",
      "    epoch          : 441\n",
      "    loss           : -228705.07734001195\n",
      "    val_loss       : -230368.84313667112\n",
      "Train Epoch: 442 [0/54000 (0%)] Loss: -223515.859375\n",
      "Train Epoch: 442 [1408/54000 (3%)] Loss: -249745.734375\n",
      "Train Epoch: 442 [2816/54000 (5%)] Loss: -222509.000000\n",
      "Train Epoch: 442 [4224/54000 (8%)] Loss: -225342.718750\n",
      "Train Epoch: 442 [5632/54000 (10%)] Loss: -224242.187500\n",
      "Train Epoch: 442 [7040/54000 (13%)] Loss: -223082.546875\n",
      "Train Epoch: 442 [8448/54000 (16%)] Loss: -221339.000000\n",
      "Train Epoch: 442 [9856/54000 (18%)] Loss: -226985.500000\n",
      "Train Epoch: 442 [11264/54000 (21%)] Loss: -223942.984375\n",
      "Train Epoch: 442 [12672/54000 (23%)] Loss: -222684.562500\n",
      "Train Epoch: 442 [14080/54000 (26%)] Loss: -227807.781250\n",
      "Train Epoch: 442 [15488/54000 (29%)] Loss: -229004.187500\n",
      "Train Epoch: 442 [16896/54000 (31%)] Loss: -225056.921875\n",
      "Train Epoch: 442 [18304/54000 (34%)] Loss: -226659.609375\n",
      "Train Epoch: 442 [19712/54000 (37%)] Loss: -228386.828125\n",
      "Train Epoch: 442 [21120/54000 (39%)] Loss: -226882.984375\n",
      "Train Epoch: 442 [22528/54000 (42%)] Loss: -223865.734375\n",
      "Train Epoch: 442 [23936/54000 (44%)] Loss: -226831.750000\n",
      "Train Epoch: 442 [25344/54000 (47%)] Loss: -232346.328125\n",
      "Train Epoch: 442 [26752/54000 (50%)] Loss: -223080.312500\n",
      "Train Epoch: 442 [28160/54000 (52%)] Loss: -225470.718750\n",
      "Train Epoch: 442 [29568/54000 (55%)] Loss: -223964.984375\n",
      "Train Epoch: 442 [30976/54000 (57%)] Loss: -220296.281250\n",
      "Train Epoch: 442 [32384/54000 (60%)] Loss: -217970.625000\n",
      "Train Epoch: 442 [33792/54000 (63%)] Loss: -228595.703125\n",
      "Train Epoch: 442 [35200/54000 (65%)] Loss: -250292.875000\n",
      "Train Epoch: 442 [36608/54000 (68%)] Loss: -222723.625000\n",
      "Train Epoch: 442 [38016/54000 (70%)] Loss: -227023.781250\n",
      "Train Epoch: 442 [39424/54000 (73%)] Loss: -226477.250000\n",
      "Train Epoch: 442 [40832/54000 (76%)] Loss: -248413.421875\n",
      "Train Epoch: 442 [42240/54000 (78%)] Loss: -219848.000000\n",
      "Train Epoch: 442 [43648/54000 (81%)] Loss: -221788.062500\n",
      "Train Epoch: 442 [45056/54000 (83%)] Loss: -226276.296875\n",
      "Train Epoch: 442 [46464/54000 (86%)] Loss: -251240.312500\n",
      "Train Epoch: 442 [47872/54000 (89%)] Loss: -220828.359375\n",
      "Train Epoch: 442 [49280/54000 (91%)] Loss: -218933.953125\n",
      "Train Epoch: 442 [50688/54000 (94%)] Loss: -222198.968750\n",
      "Train Epoch: 442 [52096/54000 (96%)] Loss: -220222.890625\n",
      "    epoch          : 442\n",
      "    loss           : -228668.34845992824\n",
      "    val_loss       : -230844.43918516577\n",
      "Train Epoch: 443 [0/54000 (0%)] Loss: -249050.312500\n",
      "Train Epoch: 443 [1408/54000 (3%)] Loss: -221896.531250\n",
      "Train Epoch: 443 [2816/54000 (5%)] Loss: -232284.140625\n",
      "Train Epoch: 443 [4224/54000 (8%)] Loss: -231830.406250\n",
      "Train Epoch: 443 [5632/54000 (10%)] Loss: -225966.390625\n",
      "Train Epoch: 443 [7040/54000 (13%)] Loss: -220638.437500\n",
      "Train Epoch: 443 [8448/54000 (16%)] Loss: -220488.046875\n",
      "Train Epoch: 443 [9856/54000 (18%)] Loss: -231241.781250\n",
      "Train Epoch: 443 [11264/54000 (21%)] Loss: -225962.609375\n",
      "Train Epoch: 443 [12672/54000 (23%)] Loss: -224763.406250\n",
      "Train Epoch: 443 [14080/54000 (26%)] Loss: -222522.250000\n",
      "Train Epoch: 443 [15488/54000 (29%)] Loss: -251231.078125\n",
      "Train Epoch: 443 [16896/54000 (31%)] Loss: -221325.578125\n",
      "Train Epoch: 443 [18304/54000 (34%)] Loss: -223594.625000\n",
      "Train Epoch: 443 [19712/54000 (37%)] Loss: -250681.953125\n",
      "Train Epoch: 443 [21120/54000 (39%)] Loss: -222777.250000\n",
      "Train Epoch: 443 [22528/54000 (42%)] Loss: -222562.859375\n",
      "Train Epoch: 443 [23936/54000 (44%)] Loss: -222680.984375\n",
      "Train Epoch: 443 [25344/54000 (47%)] Loss: -221334.359375\n",
      "Train Epoch: 443 [26752/54000 (50%)] Loss: -221660.281250\n",
      "Train Epoch: 443 [28160/54000 (52%)] Loss: -246945.078125\n",
      "Train Epoch: 443 [29568/54000 (55%)] Loss: -218594.859375\n",
      "Train Epoch: 443 [30976/54000 (57%)] Loss: -231847.375000\n",
      "Train Epoch: 443 [32384/54000 (60%)] Loss: -249418.609375\n",
      "Train Epoch: 443 [33792/54000 (63%)] Loss: -231861.875000\n",
      "Train Epoch: 443 [35200/54000 (65%)] Loss: -227320.062500\n",
      "Train Epoch: 443 [36608/54000 (68%)] Loss: -223241.359375\n",
      "Train Epoch: 443 [38016/54000 (70%)] Loss: -227231.343750\n",
      "Train Epoch: 443 [39424/54000 (73%)] Loss: -250592.921875\n",
      "Train Epoch: 443 [40832/54000 (76%)] Loss: -250536.937500\n",
      "Train Epoch: 443 [42240/54000 (78%)] Loss: -223210.812500\n",
      "Train Epoch: 443 [43648/54000 (81%)] Loss: -218283.843750\n",
      "Train Epoch: 443 [45056/54000 (83%)] Loss: -219793.609375\n",
      "Train Epoch: 443 [46464/54000 (86%)] Loss: -220202.140625\n",
      "Train Epoch: 443 [47872/54000 (89%)] Loss: -232511.031250\n",
      "Train Epoch: 443 [49280/54000 (91%)] Loss: -231567.609375\n",
      "Train Epoch: 443 [50688/54000 (94%)] Loss: -250644.859375\n",
      "Train Epoch: 443 [52096/54000 (96%)] Loss: -220306.359375\n",
      "    epoch          : 443\n",
      "    loss           : -228658.4841507177\n",
      "    val_loss       : -230482.58754525534\n",
      "Train Epoch: 444 [0/54000 (0%)] Loss: -231556.234375\n",
      "Train Epoch: 444 [1408/54000 (3%)] Loss: -232614.562500\n",
      "Train Epoch: 444 [2816/54000 (5%)] Loss: -226852.218750\n",
      "Train Epoch: 444 [4224/54000 (8%)] Loss: -226060.812500\n",
      "Train Epoch: 444 [5632/54000 (10%)] Loss: -228271.125000\n",
      "Train Epoch: 444 [7040/54000 (13%)] Loss: -222563.625000\n",
      "Train Epoch: 444 [8448/54000 (16%)] Loss: -223691.312500\n",
      "Train Epoch: 444 [9856/54000 (18%)] Loss: -231069.500000\n",
      "Train Epoch: 444 [11264/54000 (21%)] Loss: -222786.875000\n",
      "Train Epoch: 444 [12672/54000 (23%)] Loss: -248985.687500\n",
      "Train Epoch: 444 [14080/54000 (26%)] Loss: -224225.578125\n",
      "Train Epoch: 444 [15488/54000 (29%)] Loss: -231939.734375\n",
      "Train Epoch: 444 [16896/54000 (31%)] Loss: -220649.078125\n",
      "Train Epoch: 444 [18304/54000 (34%)] Loss: -228256.250000\n",
      "Train Epoch: 444 [19712/54000 (37%)] Loss: -220698.078125\n",
      "Train Epoch: 444 [21120/54000 (39%)] Loss: -232442.890625\n",
      "Train Epoch: 444 [22528/54000 (42%)] Loss: -233228.078125\n",
      "Train Epoch: 444 [23936/54000 (44%)] Loss: -249315.703125\n",
      "Train Epoch: 444 [25344/54000 (47%)] Loss: -230484.625000\n",
      "Train Epoch: 444 [26752/54000 (50%)] Loss: -227650.531250\n",
      "Train Epoch: 444 [28160/54000 (52%)] Loss: -227167.453125\n",
      "Train Epoch: 444 [29568/54000 (55%)] Loss: -250105.640625\n",
      "Train Epoch: 444 [30976/54000 (57%)] Loss: -232664.328125\n",
      "Train Epoch: 444 [32384/54000 (60%)] Loss: -231899.937500\n",
      "Train Epoch: 444 [33792/54000 (63%)] Loss: -249345.859375\n",
      "Train Epoch: 444 [35200/54000 (65%)] Loss: -225521.281250\n",
      "Train Epoch: 444 [36608/54000 (68%)] Loss: -223699.125000\n",
      "Train Epoch: 444 [38016/54000 (70%)] Loss: -220729.625000\n",
      "Train Epoch: 444 [39424/54000 (73%)] Loss: -220514.421875\n",
      "Train Epoch: 444 [40832/54000 (76%)] Loss: -250927.234375\n",
      "Train Epoch: 444 [42240/54000 (78%)] Loss: -224148.515625\n",
      "Train Epoch: 444 [43648/54000 (81%)] Loss: -221649.531250\n",
      "Train Epoch: 444 [45056/54000 (83%)] Loss: -221651.718750\n",
      "Train Epoch: 444 [46464/54000 (86%)] Loss: -222732.015625\n",
      "Train Epoch: 444 [47872/54000 (89%)] Loss: -225213.078125\n",
      "Train Epoch: 444 [49280/54000 (91%)] Loss: -222598.562500\n",
      "Train Epoch: 444 [50688/54000 (94%)] Loss: -219803.921875\n",
      "Train Epoch: 444 [52096/54000 (96%)] Loss: -217951.609375\n",
      "    epoch          : 444\n",
      "    loss           : -228769.0540146531\n",
      "    val_loss       : -230472.99048447027\n",
      "Train Epoch: 445 [0/54000 (0%)] Loss: -232380.406250\n",
      "Train Epoch: 445 [1408/54000 (3%)] Loss: -231013.500000\n",
      "Train Epoch: 445 [2816/54000 (5%)] Loss: -231471.171875\n",
      "Train Epoch: 445 [4224/54000 (8%)] Loss: -220741.031250\n",
      "Train Epoch: 445 [5632/54000 (10%)] Loss: -222686.187500\n",
      "Train Epoch: 445 [7040/54000 (13%)] Loss: -251076.187500\n",
      "Train Epoch: 445 [8448/54000 (16%)] Loss: -229134.468750\n",
      "Train Epoch: 445 [9856/54000 (18%)] Loss: -232237.250000\n",
      "Train Epoch: 445 [11264/54000 (21%)] Loss: -219984.125000\n",
      "Train Epoch: 445 [12672/54000 (23%)] Loss: -225481.250000\n",
      "Train Epoch: 445 [14080/54000 (26%)] Loss: -232773.843750\n",
      "Train Epoch: 445 [15488/54000 (29%)] Loss: -224616.906250\n",
      "Train Epoch: 445 [16896/54000 (31%)] Loss: -226336.750000\n",
      "Train Epoch: 445 [18304/54000 (34%)] Loss: -221056.765625\n",
      "Train Epoch: 445 [19712/54000 (37%)] Loss: -221008.531250\n",
      "Train Epoch: 445 [21120/54000 (39%)] Loss: -222919.000000\n",
      "Train Epoch: 445 [22528/54000 (42%)] Loss: -217484.718750\n",
      "Train Epoch: 445 [23936/54000 (44%)] Loss: -223441.562500\n",
      "Train Epoch: 445 [25344/54000 (47%)] Loss: -228083.093750\n",
      "Train Epoch: 445 [26752/54000 (50%)] Loss: -219350.421875\n",
      "Train Epoch: 445 [28160/54000 (52%)] Loss: -227150.640625\n",
      "Train Epoch: 445 [29568/54000 (55%)] Loss: -225000.406250\n",
      "Train Epoch: 445 [30976/54000 (57%)] Loss: -228219.578125\n",
      "Train Epoch: 445 [32384/54000 (60%)] Loss: -226275.937500\n",
      "Train Epoch: 445 [33792/54000 (63%)] Loss: -221055.359375\n",
      "Train Epoch: 445 [35200/54000 (65%)] Loss: -228826.156250\n",
      "Train Epoch: 445 [36608/54000 (68%)] Loss: -225500.015625\n",
      "Train Epoch: 445 [38016/54000 (70%)] Loss: -231667.937500\n",
      "Train Epoch: 445 [39424/54000 (73%)] Loss: -231466.656250\n",
      "Train Epoch: 445 [40832/54000 (76%)] Loss: -249369.890625\n",
      "Train Epoch: 445 [42240/54000 (78%)] Loss: -227640.687500\n",
      "Train Epoch: 445 [43648/54000 (81%)] Loss: -225437.437500\n",
      "Train Epoch: 445 [45056/54000 (83%)] Loss: -223255.296875\n",
      "Train Epoch: 445 [46464/54000 (86%)] Loss: -222569.140625\n",
      "Train Epoch: 445 [47872/54000 (89%)] Loss: -229302.515625\n",
      "Train Epoch: 445 [49280/54000 (91%)] Loss: -231324.656250\n",
      "Train Epoch: 445 [50688/54000 (94%)] Loss: -229480.828125\n",
      "Train Epoch: 445 [52096/54000 (96%)] Loss: -232920.281250\n",
      "    epoch          : 445\n",
      "    loss           : -228577.15045604066\n",
      "    val_loss       : -230360.01425543064\n",
      "Train Epoch: 446 [0/54000 (0%)] Loss: -232553.812500\n",
      "Train Epoch: 446 [1408/54000 (3%)] Loss: -232432.812500\n",
      "Train Epoch: 446 [2816/54000 (5%)] Loss: -222506.750000\n",
      "Train Epoch: 446 [4224/54000 (8%)] Loss: -227973.984375\n",
      "Train Epoch: 446 [5632/54000 (10%)] Loss: -221826.515625\n",
      "Train Epoch: 446 [7040/54000 (13%)] Loss: -228375.437500\n",
      "Train Epoch: 446 [8448/54000 (16%)] Loss: -221140.578125\n",
      "Train Epoch: 446 [9856/54000 (18%)] Loss: -220388.328125\n",
      "Train Epoch: 446 [11264/54000 (21%)] Loss: -217823.062500\n",
      "Train Epoch: 446 [12672/54000 (23%)] Loss: -220635.890625\n",
      "Train Epoch: 446 [14080/54000 (26%)] Loss: -221850.671875\n",
      "Train Epoch: 446 [15488/54000 (29%)] Loss: -221787.593750\n",
      "Train Epoch: 446 [16896/54000 (31%)] Loss: -226232.640625\n",
      "Train Epoch: 446 [18304/54000 (34%)] Loss: -223822.343750\n",
      "Train Epoch: 446 [19712/54000 (37%)] Loss: -248695.843750\n",
      "Train Epoch: 446 [21120/54000 (39%)] Loss: -248854.046875\n",
      "Train Epoch: 446 [22528/54000 (42%)] Loss: -221722.843750\n",
      "Train Epoch: 446 [23936/54000 (44%)] Loss: -231761.843750\n",
      "Train Epoch: 446 [25344/54000 (47%)] Loss: -224130.687500\n",
      "Train Epoch: 446 [26752/54000 (50%)] Loss: -223744.406250\n",
      "Train Epoch: 446 [28160/54000 (52%)] Loss: -223943.953125\n",
      "Train Epoch: 446 [29568/54000 (55%)] Loss: -218661.984375\n",
      "Train Epoch: 446 [30976/54000 (57%)] Loss: -231906.578125\n",
      "Train Epoch: 446 [32384/54000 (60%)] Loss: -224795.562500\n",
      "Train Epoch: 446 [33792/54000 (63%)] Loss: -221335.062500\n",
      "Train Epoch: 446 [35200/54000 (65%)] Loss: -217312.765625\n",
      "Train Epoch: 446 [36608/54000 (68%)] Loss: -231105.578125\n",
      "Train Epoch: 446 [38016/54000 (70%)] Loss: -219810.656250\n",
      "Train Epoch: 446 [39424/54000 (73%)] Loss: -220667.093750\n",
      "Train Epoch: 446 [40832/54000 (76%)] Loss: -249202.468750\n",
      "Train Epoch: 446 [42240/54000 (78%)] Loss: -231802.515625\n",
      "Train Epoch: 446 [43648/54000 (81%)] Loss: -223692.140625\n",
      "Train Epoch: 446 [45056/54000 (83%)] Loss: -229893.062500\n",
      "Train Epoch: 446 [46464/54000 (86%)] Loss: -229298.000000\n",
      "Train Epoch: 446 [47872/54000 (89%)] Loss: -219028.546875\n",
      "Train Epoch: 446 [49280/54000 (91%)] Loss: -225847.171875\n",
      "Train Epoch: 446 [50688/54000 (94%)] Loss: -223481.406250\n",
      "Train Epoch: 446 [52096/54000 (96%)] Loss: -231789.500000\n",
      "    epoch          : 446\n",
      "    loss           : -228677.32221889953\n",
      "    val_loss       : -230498.75276295733\n",
      "Train Epoch: 447 [0/54000 (0%)] Loss: -232959.187500\n",
      "Train Epoch: 447 [1408/54000 (3%)] Loss: -231557.093750\n",
      "Train Epoch: 447 [2816/54000 (5%)] Loss: -226386.156250\n",
      "Train Epoch: 447 [4224/54000 (8%)] Loss: -222081.875000\n",
      "Train Epoch: 447 [5632/54000 (10%)] Loss: -216835.531250\n",
      "Train Epoch: 447 [7040/54000 (13%)] Loss: -219812.234375\n",
      "Train Epoch: 447 [8448/54000 (16%)] Loss: -216689.921875\n",
      "Train Epoch: 447 [9856/54000 (18%)] Loss: -229602.125000\n",
      "Train Epoch: 447 [11264/54000 (21%)] Loss: -222604.843750\n",
      "Train Epoch: 447 [12672/54000 (23%)] Loss: -249564.859375\n",
      "Train Epoch: 447 [14080/54000 (26%)] Loss: -229416.046875\n",
      "Train Epoch: 447 [15488/54000 (29%)] Loss: -222846.546875\n",
      "Train Epoch: 447 [16896/54000 (31%)] Loss: -226995.531250\n",
      "Train Epoch: 447 [18304/54000 (34%)] Loss: -249484.531250\n",
      "Train Epoch: 447 [19712/54000 (37%)] Loss: -224418.625000\n",
      "Train Epoch: 447 [21120/54000 (39%)] Loss: -231996.859375\n",
      "Train Epoch: 447 [22528/54000 (42%)] Loss: -231620.984375\n",
      "Train Epoch: 447 [23936/54000 (44%)] Loss: -227327.437500\n",
      "Train Epoch: 447 [25344/54000 (47%)] Loss: -226541.000000\n",
      "Train Epoch: 447 [26752/54000 (50%)] Loss: -223409.500000\n",
      "Train Epoch: 447 [28160/54000 (52%)] Loss: -231214.859375\n",
      "Train Epoch: 447 [29568/54000 (55%)] Loss: -249170.640625\n",
      "Train Epoch: 447 [30976/54000 (57%)] Loss: -228957.187500\n",
      "Train Epoch: 447 [32384/54000 (60%)] Loss: -228474.593750\n",
      "Train Epoch: 447 [33792/54000 (63%)] Loss: -227447.000000\n",
      "Train Epoch: 447 [35200/54000 (65%)] Loss: -223288.359375\n",
      "Train Epoch: 447 [36608/54000 (68%)] Loss: -225398.500000\n",
      "Train Epoch: 447 [38016/54000 (70%)] Loss: -226055.671875\n",
      "Train Epoch: 447 [39424/54000 (73%)] Loss: -226850.703125\n",
      "Train Epoch: 447 [40832/54000 (76%)] Loss: -232567.890625\n",
      "Train Epoch: 447 [42240/54000 (78%)] Loss: -223940.250000\n",
      "Train Epoch: 447 [43648/54000 (81%)] Loss: -224147.171875\n",
      "Train Epoch: 447 [45056/54000 (83%)] Loss: -221282.640625\n",
      "Train Epoch: 447 [46464/54000 (86%)] Loss: -225627.046875\n",
      "Train Epoch: 447 [47872/54000 (89%)] Loss: -227570.687500\n",
      "Train Epoch: 447 [49280/54000 (91%)] Loss: -232030.890625\n",
      "Train Epoch: 447 [50688/54000 (94%)] Loss: -232537.625000\n",
      "Train Epoch: 447 [52096/54000 (96%)] Loss: -222621.031250\n",
      "    epoch          : 447\n",
      "    loss           : -228745.68779904305\n",
      "    val_loss       : -229931.5712890625\n",
      "Train Epoch: 448 [0/54000 (0%)] Loss: -250516.906250\n",
      "Train Epoch: 448 [1408/54000 (3%)] Loss: -222375.265625\n",
      "Train Epoch: 448 [2816/54000 (5%)] Loss: -222750.921875\n",
      "Train Epoch: 448 [4224/54000 (8%)] Loss: -231920.984375\n",
      "Train Epoch: 448 [5632/54000 (10%)] Loss: -229110.343750\n",
      "Train Epoch: 448 [7040/54000 (13%)] Loss: -226468.375000\n",
      "Train Epoch: 448 [8448/54000 (16%)] Loss: -223591.312500\n",
      "Train Epoch: 448 [9856/54000 (18%)] Loss: -230543.718750\n",
      "Train Epoch: 448 [11264/54000 (21%)] Loss: -223422.187500\n",
      "Train Epoch: 448 [12672/54000 (23%)] Loss: -251629.156250\n",
      "Train Epoch: 448 [14080/54000 (26%)] Loss: -231242.593750\n",
      "Train Epoch: 448 [15488/54000 (29%)] Loss: -219485.531250\n",
      "Train Epoch: 448 [16896/54000 (31%)] Loss: -225661.578125\n",
      "Train Epoch: 448 [18304/54000 (34%)] Loss: -232007.765625\n",
      "Train Epoch: 448 [19712/54000 (37%)] Loss: -230450.875000\n",
      "Train Epoch: 448 [21120/54000 (39%)] Loss: -220282.296875\n",
      "Train Epoch: 448 [22528/54000 (42%)] Loss: -249058.843750\n",
      "Train Epoch: 448 [23936/54000 (44%)] Loss: -232206.140625\n",
      "Train Epoch: 448 [25344/54000 (47%)] Loss: -228068.437500\n",
      "Train Epoch: 448 [26752/54000 (50%)] Loss: -222370.500000\n",
      "Train Epoch: 448 [28160/54000 (52%)] Loss: -250056.250000\n",
      "Train Epoch: 448 [29568/54000 (55%)] Loss: -227217.187500\n",
      "Train Epoch: 448 [30976/54000 (57%)] Loss: -230108.062500\n",
      "Train Epoch: 448 [32384/54000 (60%)] Loss: -223991.015625\n",
      "Train Epoch: 448 [33792/54000 (63%)] Loss: -248455.609375\n",
      "Train Epoch: 448 [35200/54000 (65%)] Loss: -219638.718750\n",
      "Train Epoch: 448 [36608/54000 (68%)] Loss: -224092.546875\n",
      "Train Epoch: 448 [38016/54000 (70%)] Loss: -250995.250000\n",
      "Train Epoch: 448 [39424/54000 (73%)] Loss: -225352.375000\n",
      "Train Epoch: 448 [40832/54000 (76%)] Loss: -226719.328125\n",
      "Train Epoch: 448 [42240/54000 (78%)] Loss: -220734.156250\n",
      "Train Epoch: 448 [43648/54000 (81%)] Loss: -231243.546875\n",
      "Train Epoch: 448 [45056/54000 (83%)] Loss: -228243.843750\n",
      "Train Epoch: 448 [46464/54000 (86%)] Loss: -227216.500000\n",
      "Train Epoch: 448 [47872/54000 (89%)] Loss: -219938.750000\n",
      "Train Epoch: 448 [49280/54000 (91%)] Loss: -226153.968750\n",
      "Train Epoch: 448 [50688/54000 (94%)] Loss: -249935.843750\n",
      "Train Epoch: 448 [52096/54000 (96%)] Loss: -232729.609375\n",
      "    epoch          : 448\n",
      "    loss           : -228745.71803977274\n",
      "    val_loss       : -230920.04229587462\n",
      "Train Epoch: 449 [0/54000 (0%)] Loss: -249012.734375\n",
      "Train Epoch: 449 [1408/54000 (3%)] Loss: -225152.796875\n",
      "Train Epoch: 449 [2816/54000 (5%)] Loss: -222221.156250\n",
      "Train Epoch: 449 [4224/54000 (8%)] Loss: -222131.000000\n",
      "Train Epoch: 449 [5632/54000 (10%)] Loss: -232183.453125\n",
      "Train Epoch: 449 [7040/54000 (13%)] Loss: -228508.406250\n",
      "Train Epoch: 449 [8448/54000 (16%)] Loss: -223667.656250\n",
      "Train Epoch: 449 [9856/54000 (18%)] Loss: -230346.343750\n",
      "Train Epoch: 449 [11264/54000 (21%)] Loss: -230963.421875\n",
      "Train Epoch: 449 [12672/54000 (23%)] Loss: -223189.046875\n",
      "Train Epoch: 449 [14080/54000 (26%)] Loss: -232445.062500\n",
      "Train Epoch: 449 [15488/54000 (29%)] Loss: -250438.828125\n",
      "Train Epoch: 449 [16896/54000 (31%)] Loss: -222187.328125\n",
      "Train Epoch: 449 [18304/54000 (34%)] Loss: -226436.718750\n",
      "Train Epoch: 449 [19712/54000 (37%)] Loss: -222684.328125\n",
      "Train Epoch: 449 [21120/54000 (39%)] Loss: -221750.453125\n",
      "Train Epoch: 449 [22528/54000 (42%)] Loss: -231220.734375\n",
      "Train Epoch: 449 [23936/54000 (44%)] Loss: -230687.875000\n",
      "Train Epoch: 449 [25344/54000 (47%)] Loss: -227998.375000\n",
      "Train Epoch: 449 [26752/54000 (50%)] Loss: -223961.718750\n",
      "Train Epoch: 449 [28160/54000 (52%)] Loss: -220342.843750\n",
      "Train Epoch: 449 [29568/54000 (55%)] Loss: -227629.218750\n",
      "Train Epoch: 449 [30976/54000 (57%)] Loss: -224104.218750\n",
      "Train Epoch: 449 [32384/54000 (60%)] Loss: -225981.500000\n",
      "Train Epoch: 449 [33792/54000 (63%)] Loss: -222443.078125\n",
      "Train Epoch: 449 [35200/54000 (65%)] Loss: -224739.812500\n",
      "Train Epoch: 449 [36608/54000 (68%)] Loss: -226683.593750\n",
      "Train Epoch: 449 [38016/54000 (70%)] Loss: -244799.000000\n",
      "Train Epoch: 449 [39424/54000 (73%)] Loss: -232130.906250\n",
      "Train Epoch: 449 [40832/54000 (76%)] Loss: -231991.500000\n",
      "Train Epoch: 449 [42240/54000 (78%)] Loss: -219131.984375\n",
      "Train Epoch: 449 [43648/54000 (81%)] Loss: -248184.828125\n",
      "Train Epoch: 449 [45056/54000 (83%)] Loss: -227302.703125\n",
      "Train Epoch: 449 [46464/54000 (86%)] Loss: -223988.515625\n",
      "Train Epoch: 449 [47872/54000 (89%)] Loss: -230800.421875\n",
      "Train Epoch: 449 [49280/54000 (91%)] Loss: -249511.781250\n",
      "Train Epoch: 449 [50688/54000 (94%)] Loss: -224675.546875\n",
      "Train Epoch: 449 [52096/54000 (96%)] Loss: -218567.046875\n",
      "    epoch          : 449\n",
      "    loss           : -228824.59087171053\n",
      "    val_loss       : -230418.44300209603\n",
      "Train Epoch: 450 [0/54000 (0%)] Loss: -249446.515625\n",
      "Train Epoch: 450 [1408/54000 (3%)] Loss: -232245.125000\n",
      "Train Epoch: 450 [2816/54000 (5%)] Loss: -230733.812500\n",
      "Train Epoch: 450 [4224/54000 (8%)] Loss: -221902.281250\n",
      "Train Epoch: 450 [5632/54000 (10%)] Loss: -222804.421875\n",
      "Train Epoch: 450 [7040/54000 (13%)] Loss: -229295.140625\n",
      "Train Epoch: 450 [8448/54000 (16%)] Loss: -222415.703125\n",
      "Train Epoch: 450 [9856/54000 (18%)] Loss: -222575.781250\n",
      "Train Epoch: 450 [11264/54000 (21%)] Loss: -232063.609375\n",
      "Train Epoch: 450 [12672/54000 (23%)] Loss: -223879.500000\n",
      "Train Epoch: 450 [14080/54000 (26%)] Loss: -250529.859375\n",
      "Train Epoch: 450 [15488/54000 (29%)] Loss: -228096.468750\n",
      "Train Epoch: 450 [16896/54000 (31%)] Loss: -220233.609375\n",
      "Train Epoch: 450 [18304/54000 (34%)] Loss: -223222.078125\n",
      "Train Epoch: 450 [19712/54000 (37%)] Loss: -231940.187500\n",
      "Train Epoch: 450 [21120/54000 (39%)] Loss: -233632.875000\n",
      "Train Epoch: 450 [22528/54000 (42%)] Loss: -217271.750000\n",
      "Train Epoch: 450 [23936/54000 (44%)] Loss: -223587.781250\n",
      "Train Epoch: 450 [25344/54000 (47%)] Loss: -219822.015625\n",
      "Train Epoch: 450 [26752/54000 (50%)] Loss: -249486.281250\n",
      "Train Epoch: 450 [28160/54000 (52%)] Loss: -227301.093750\n",
      "Train Epoch: 450 [29568/54000 (55%)] Loss: -228598.031250\n",
      "Train Epoch: 450 [30976/54000 (57%)] Loss: -220157.000000\n",
      "Train Epoch: 450 [32384/54000 (60%)] Loss: -220471.859375\n",
      "Train Epoch: 450 [33792/54000 (63%)] Loss: -225999.953125\n",
      "Train Epoch: 450 [35200/54000 (65%)] Loss: -218325.140625\n",
      "Train Epoch: 450 [36608/54000 (68%)] Loss: -225705.062500\n",
      "Train Epoch: 450 [38016/54000 (70%)] Loss: -226095.750000\n",
      "Train Epoch: 450 [39424/54000 (73%)] Loss: -224288.078125\n",
      "Train Epoch: 450 [40832/54000 (76%)] Loss: -226958.843750\n",
      "Train Epoch: 450 [42240/54000 (78%)] Loss: -228307.656250\n",
      "Train Epoch: 450 [43648/54000 (81%)] Loss: -218819.468750\n",
      "Train Epoch: 450 [45056/54000 (83%)] Loss: -225774.718750\n",
      "Train Epoch: 450 [46464/54000 (86%)] Loss: -228274.796875\n",
      "Train Epoch: 450 [47872/54000 (89%)] Loss: -223848.859375\n",
      "Train Epoch: 450 [49280/54000 (91%)] Loss: -223815.500000\n",
      "Train Epoch: 450 [50688/54000 (94%)] Loss: -218173.515625\n",
      "Train Epoch: 450 [52096/54000 (96%)] Loss: -222032.890625\n",
      "    epoch          : 450\n",
      "    loss           : -228575.13113038277\n",
      "    val_loss       : -230501.66868330791\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0508_092801/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [0/54000 (0%)] Loss: -248731.968750\n",
      "Train Epoch: 451 [1408/54000 (3%)] Loss: -225797.625000\n",
      "Train Epoch: 451 [2816/54000 (5%)] Loss: -222069.406250\n",
      "Train Epoch: 451 [4224/54000 (8%)] Loss: -224649.765625\n",
      "Train Epoch: 451 [5632/54000 (10%)] Loss: -250405.343750\n",
      "Train Epoch: 451 [7040/54000 (13%)] Loss: -220880.843750\n",
      "Train Epoch: 451 [8448/54000 (16%)] Loss: -222699.343750\n",
      "Train Epoch: 451 [9856/54000 (18%)] Loss: -231331.984375\n",
      "Train Epoch: 451 [11264/54000 (21%)] Loss: -230415.484375\n",
      "Train Epoch: 451 [12672/54000 (23%)] Loss: -249154.421875\n",
      "Train Epoch: 451 [14080/54000 (26%)] Loss: -232860.656250\n",
      "Train Epoch: 451 [15488/54000 (29%)] Loss: -226377.390625\n",
      "Train Epoch: 451 [16896/54000 (31%)] Loss: -230455.187500\n",
      "Train Epoch: 451 [18304/54000 (34%)] Loss: -250189.468750\n",
      "Train Epoch: 451 [19712/54000 (37%)] Loss: -222950.671875\n",
      "Train Epoch: 451 [21120/54000 (39%)] Loss: -223491.140625\n",
      "Train Epoch: 451 [22528/54000 (42%)] Loss: -226752.984375\n",
      "Train Epoch: 451 [23936/54000 (44%)] Loss: -226684.593750\n",
      "Train Epoch: 451 [25344/54000 (47%)] Loss: -222571.359375\n",
      "Train Epoch: 451 [26752/54000 (50%)] Loss: -227734.890625\n",
      "Train Epoch: 451 [28160/54000 (52%)] Loss: -232006.343750\n",
      "Train Epoch: 451 [29568/54000 (55%)] Loss: -221588.218750\n",
      "Train Epoch: 451 [30976/54000 (57%)] Loss: -218945.140625\n",
      "Train Epoch: 451 [32384/54000 (60%)] Loss: -222909.156250\n",
      "Train Epoch: 451 [33792/54000 (63%)] Loss: -232907.390625\n",
      "Train Epoch: 451 [35200/54000 (65%)] Loss: -232538.015625\n",
      "Train Epoch: 451 [36608/54000 (68%)] Loss: -249384.546875\n",
      "Train Epoch: 451 [38016/54000 (70%)] Loss: -227381.750000\n",
      "Train Epoch: 451 [39424/54000 (73%)] Loss: -226456.796875\n",
      "Train Epoch: 451 [40832/54000 (76%)] Loss: -249792.843750\n",
      "Train Epoch: 451 [42240/54000 (78%)] Loss: -224082.437500\n",
      "Train Epoch: 451 [43648/54000 (81%)] Loss: -226315.187500\n",
      "Train Epoch: 451 [45056/54000 (83%)] Loss: -233606.546875\n",
      "Train Epoch: 451 [46464/54000 (86%)] Loss: -221206.218750\n",
      "Train Epoch: 451 [47872/54000 (89%)] Loss: -226334.218750\n",
      "Train Epoch: 451 [49280/54000 (91%)] Loss: -221702.828125\n",
      "Train Epoch: 451 [50688/54000 (94%)] Loss: -251835.859375\n",
      "Train Epoch: 451 [52096/54000 (96%)] Loss: -225113.687500\n",
      "    epoch          : 451\n",
      "    loss           : -228882.3674117823\n",
      "    val_loss       : -230723.08173351752\n",
      "Train Epoch: 452 [0/54000 (0%)] Loss: -227419.796875\n",
      "Train Epoch: 452 [1408/54000 (3%)] Loss: -225938.937500\n",
      "Train Epoch: 452 [2816/54000 (5%)] Loss: -231076.812500\n",
      "Train Epoch: 452 [4224/54000 (8%)] Loss: -228047.671875\n",
      "Train Epoch: 452 [5632/54000 (10%)] Loss: -219684.343750\n",
      "Train Epoch: 452 [7040/54000 (13%)] Loss: -222813.656250\n",
      "Train Epoch: 452 [8448/54000 (16%)] Loss: -222579.718750\n",
      "Train Epoch: 452 [9856/54000 (18%)] Loss: -248118.203125\n",
      "Train Epoch: 452 [11264/54000 (21%)] Loss: -222093.406250\n",
      "Train Epoch: 452 [12672/54000 (23%)] Loss: -223221.171875\n",
      "Train Epoch: 452 [14080/54000 (26%)] Loss: -225182.500000\n",
      "Train Epoch: 452 [15488/54000 (29%)] Loss: -250266.875000\n",
      "Train Epoch: 452 [16896/54000 (31%)] Loss: -230298.281250\n",
      "Train Epoch: 452 [18304/54000 (34%)] Loss: -218400.687500\n",
      "Train Epoch: 452 [19712/54000 (37%)] Loss: -228892.671875\n",
      "Train Epoch: 452 [21120/54000 (39%)] Loss: -248917.718750\n",
      "Train Epoch: 452 [22528/54000 (42%)] Loss: -229366.156250\n",
      "Train Epoch: 452 [23936/54000 (44%)] Loss: -234120.171875\n",
      "Train Epoch: 452 [25344/54000 (47%)] Loss: -223865.109375\n",
      "Train Epoch: 452 [26752/54000 (50%)] Loss: -231832.656250\n",
      "Train Epoch: 452 [28160/54000 (52%)] Loss: -226804.218750\n",
      "Train Epoch: 452 [29568/54000 (55%)] Loss: -230282.593750\n",
      "Train Epoch: 452 [30976/54000 (57%)] Loss: -222025.062500\n",
      "Train Epoch: 452 [32384/54000 (60%)] Loss: -223474.968750\n",
      "Train Epoch: 452 [33792/54000 (63%)] Loss: -230652.265625\n",
      "Train Epoch: 452 [35200/54000 (65%)] Loss: -229912.593750\n",
      "Train Epoch: 452 [36608/54000 (68%)] Loss: -223149.312500\n",
      "Train Epoch: 452 [38016/54000 (70%)] Loss: -224196.718750\n",
      "Train Epoch: 452 [39424/54000 (73%)] Loss: -248980.484375\n",
      "Train Epoch: 452 [40832/54000 (76%)] Loss: -232147.906250\n",
      "Train Epoch: 452 [42240/54000 (78%)] Loss: -223901.859375\n",
      "Train Epoch: 452 [43648/54000 (81%)] Loss: -230280.906250\n",
      "Train Epoch: 452 [45056/54000 (83%)] Loss: -230959.968750\n",
      "Train Epoch: 452 [46464/54000 (86%)] Loss: -227191.546875\n",
      "Train Epoch: 452 [47872/54000 (89%)] Loss: -223706.156250\n",
      "Train Epoch: 452 [49280/54000 (91%)] Loss: -232834.390625\n",
      "Train Epoch: 452 [50688/54000 (94%)] Loss: -248403.421875\n",
      "Train Epoch: 452 [52096/54000 (96%)] Loss: -233159.000000\n",
      "    epoch          : 452\n",
      "    loss           : -228813.14361543063\n",
      "    val_loss       : -230634.84647127477\n",
      "Train Epoch: 453 [0/54000 (0%)] Loss: -250131.593750\n",
      "Train Epoch: 453 [1408/54000 (3%)] Loss: -217061.953125\n",
      "Train Epoch: 453 [2816/54000 (5%)] Loss: -231869.312500\n",
      "Train Epoch: 453 [4224/54000 (8%)] Loss: -222474.296875\n",
      "Train Epoch: 453 [5632/54000 (10%)] Loss: -221486.250000\n",
      "Train Epoch: 453 [7040/54000 (13%)] Loss: -223745.375000\n",
      "Train Epoch: 453 [8448/54000 (16%)] Loss: -221229.265625\n",
      "Train Epoch: 453 [9856/54000 (18%)] Loss: -223744.953125\n",
      "Train Epoch: 453 [11264/54000 (21%)] Loss: -232171.703125\n",
      "Train Epoch: 453 [12672/54000 (23%)] Loss: -229993.296875\n",
      "Train Epoch: 453 [14080/54000 (26%)] Loss: -227919.203125\n",
      "Train Epoch: 453 [15488/54000 (29%)] Loss: -249067.296875\n",
      "Train Epoch: 453 [16896/54000 (31%)] Loss: -225198.531250\n",
      "Train Epoch: 453 [18304/54000 (34%)] Loss: -216066.531250\n",
      "Train Epoch: 453 [19712/54000 (37%)] Loss: -222368.812500\n",
      "Train Epoch: 453 [21120/54000 (39%)] Loss: -231804.500000\n",
      "Train Epoch: 453 [22528/54000 (42%)] Loss: -231692.375000\n",
      "Train Epoch: 453 [23936/54000 (44%)] Loss: -225781.062500\n",
      "Train Epoch: 453 [25344/54000 (47%)] Loss: -231837.750000\n",
      "Train Epoch: 453 [26752/54000 (50%)] Loss: -229417.750000\n",
      "Train Epoch: 453 [28160/54000 (52%)] Loss: -250986.843750\n",
      "Train Epoch: 453 [29568/54000 (55%)] Loss: -222599.531250\n",
      "Train Epoch: 453 [30976/54000 (57%)] Loss: -223289.218750\n",
      "Train Epoch: 453 [32384/54000 (60%)] Loss: -233699.515625\n",
      "Train Epoch: 453 [33792/54000 (63%)] Loss: -223414.937500\n",
      "Train Epoch: 453 [35200/54000 (65%)] Loss: -224205.812500\n",
      "Train Epoch: 453 [36608/54000 (68%)] Loss: -221264.562500\n",
      "Train Epoch: 453 [38016/54000 (70%)] Loss: -221165.156250\n",
      "Train Epoch: 453 [39424/54000 (73%)] Loss: -232527.031250\n",
      "Train Epoch: 453 [40832/54000 (76%)] Loss: -232388.171875\n",
      "Train Epoch: 453 [42240/54000 (78%)] Loss: -231047.046875\n",
      "Train Epoch: 453 [43648/54000 (81%)] Loss: -227832.500000\n",
      "Train Epoch: 453 [45056/54000 (83%)] Loss: -228331.031250\n",
      "Train Epoch: 453 [46464/54000 (86%)] Loss: -229352.718750\n",
      "Train Epoch: 453 [47872/54000 (89%)] Loss: -225011.687500\n",
      "Train Epoch: 453 [49280/54000 (91%)] Loss: -220425.328125\n",
      "Train Epoch: 453 [50688/54000 (94%)] Loss: -218999.421875\n",
      "Train Epoch: 453 [52096/54000 (96%)] Loss: -250116.500000\n",
      "    epoch          : 453\n",
      "    loss           : -228656.6263083134\n",
      "    val_loss       : -230463.03840748858\n",
      "Train Epoch: 454 [0/54000 (0%)] Loss: -249979.171875\n",
      "Train Epoch: 454 [1408/54000 (3%)] Loss: -223267.765625\n",
      "Train Epoch: 454 [2816/54000 (5%)] Loss: -220878.687500\n",
      "Train Epoch: 454 [4224/54000 (8%)] Loss: -222061.343750\n",
      "Train Epoch: 454 [5632/54000 (10%)] Loss: -225158.750000\n",
      "Train Epoch: 454 [7040/54000 (13%)] Loss: -249477.218750\n",
      "Train Epoch: 454 [8448/54000 (16%)] Loss: -224010.375000\n",
      "Train Epoch: 454 [9856/54000 (18%)] Loss: -223654.234375\n",
      "Train Epoch: 454 [11264/54000 (21%)] Loss: -222969.906250\n",
      "Train Epoch: 454 [12672/54000 (23%)] Loss: -228871.281250\n",
      "Train Epoch: 454 [14080/54000 (26%)] Loss: -226441.656250\n",
      "Train Epoch: 454 [15488/54000 (29%)] Loss: -225180.000000\n",
      "Train Epoch: 454 [16896/54000 (31%)] Loss: -222986.812500\n",
      "Train Epoch: 454 [18304/54000 (34%)] Loss: -222144.031250\n",
      "Train Epoch: 454 [19712/54000 (37%)] Loss: -225668.875000\n",
      "Train Epoch: 454 [21120/54000 (39%)] Loss: -233684.781250\n",
      "Train Epoch: 454 [22528/54000 (42%)] Loss: -223151.562500\n",
      "Train Epoch: 454 [23936/54000 (44%)] Loss: -221246.078125\n",
      "Train Epoch: 454 [25344/54000 (47%)] Loss: -226634.000000\n",
      "Train Epoch: 454 [26752/54000 (50%)] Loss: -223569.000000\n",
      "Train Epoch: 454 [28160/54000 (52%)] Loss: -231414.078125\n",
      "Train Epoch: 454 [29568/54000 (55%)] Loss: -231026.562500\n",
      "Train Epoch: 454 [30976/54000 (57%)] Loss: -250616.062500\n",
      "Train Epoch: 454 [32384/54000 (60%)] Loss: -221207.375000\n",
      "Train Epoch: 454 [33792/54000 (63%)] Loss: -230731.843750\n",
      "Train Epoch: 454 [35200/54000 (65%)] Loss: -224868.406250\n",
      "Train Epoch: 454 [36608/54000 (68%)] Loss: -250004.125000\n",
      "Train Epoch: 454 [38016/54000 (70%)] Loss: -222300.406250\n",
      "Train Epoch: 454 [39424/54000 (73%)] Loss: -233934.625000\n",
      "Train Epoch: 454 [40832/54000 (76%)] Loss: -228042.218750\n",
      "Train Epoch: 454 [42240/54000 (78%)] Loss: -227946.062500\n",
      "Train Epoch: 454 [43648/54000 (81%)] Loss: -231977.171875\n",
      "Train Epoch: 454 [45056/54000 (83%)] Loss: -246233.968750\n",
      "Train Epoch: 454 [46464/54000 (86%)] Loss: -222990.125000\n",
      "Train Epoch: 454 [47872/54000 (89%)] Loss: -221980.296875\n",
      "Train Epoch: 454 [49280/54000 (91%)] Loss: -222933.953125\n",
      "Train Epoch: 454 [50688/54000 (94%)] Loss: -225340.140625\n",
      "Train Epoch: 454 [52096/54000 (96%)] Loss: -218265.031250\n",
      "    epoch          : 454\n",
      "    loss           : -228857.92284688994\n",
      "    val_loss       : -230776.52022198934\n",
      "Train Epoch: 455 [0/54000 (0%)] Loss: -224529.718750\n",
      "Train Epoch: 455 [1408/54000 (3%)] Loss: -223774.453125\n",
      "Train Epoch: 455 [2816/54000 (5%)] Loss: -231591.125000\n",
      "Train Epoch: 455 [4224/54000 (8%)] Loss: -230409.937500\n",
      "Train Epoch: 455 [5632/54000 (10%)] Loss: -221385.312500\n",
      "Train Epoch: 455 [7040/54000 (13%)] Loss: -221407.281250\n",
      "Train Epoch: 455 [8448/54000 (16%)] Loss: -223582.187500\n",
      "Train Epoch: 455 [9856/54000 (18%)] Loss: -230179.187500\n",
      "Train Epoch: 455 [11264/54000 (21%)] Loss: -230920.984375\n",
      "Train Epoch: 455 [12672/54000 (23%)] Loss: -231335.953125\n",
      "Train Epoch: 455 [14080/54000 (26%)] Loss: -223746.437500\n",
      "Train Epoch: 455 [15488/54000 (29%)] Loss: -226621.343750\n",
      "Train Epoch: 455 [16896/54000 (31%)] Loss: -230658.843750\n",
      "Train Epoch: 455 [18304/54000 (34%)] Loss: -224712.546875\n",
      "Train Epoch: 455 [19712/54000 (37%)] Loss: -224239.265625\n",
      "Train Epoch: 455 [21120/54000 (39%)] Loss: -218179.140625\n",
      "Train Epoch: 455 [22528/54000 (42%)] Loss: -223678.437500\n",
      "Train Epoch: 455 [23936/54000 (44%)] Loss: -222253.328125\n",
      "Train Epoch: 455 [25344/54000 (47%)] Loss: -220263.406250\n",
      "Train Epoch: 455 [26752/54000 (50%)] Loss: -232583.531250\n",
      "Train Epoch: 455 [28160/54000 (52%)] Loss: -224180.218750\n",
      "Train Epoch: 455 [29568/54000 (55%)] Loss: -227128.187500\n",
      "Train Epoch: 455 [30976/54000 (57%)] Loss: -228384.609375\n",
      "Train Epoch: 455 [32384/54000 (60%)] Loss: -230961.125000\n",
      "Train Epoch: 455 [33792/54000 (63%)] Loss: -226730.046875\n",
      "Train Epoch: 455 [35200/54000 (65%)] Loss: -231256.375000\n",
      "Train Epoch: 455 [36608/54000 (68%)] Loss: -230561.968750\n",
      "Train Epoch: 455 [38016/54000 (70%)] Loss: -220956.093750\n",
      "Train Epoch: 455 [39424/54000 (73%)] Loss: -230904.937500\n",
      "Train Epoch: 455 [40832/54000 (76%)] Loss: -249092.375000\n",
      "Train Epoch: 455 [42240/54000 (78%)] Loss: -224443.937500\n",
      "Train Epoch: 455 [43648/54000 (81%)] Loss: -230734.968750\n",
      "Train Epoch: 455 [45056/54000 (83%)] Loss: -227946.656250\n",
      "Train Epoch: 455 [46464/54000 (86%)] Loss: -225936.656250\n",
      "Train Epoch: 455 [47872/54000 (89%)] Loss: -219707.312500\n",
      "Train Epoch: 455 [49280/54000 (91%)] Loss: -230722.296875\n",
      "Train Epoch: 455 [50688/54000 (94%)] Loss: -231194.687500\n",
      "Train Epoch: 455 [52096/54000 (96%)] Loss: -231920.578125\n",
      "    epoch          : 455\n",
      "    loss           : -228813.9941312799\n",
      "    val_loss       : -230457.84597703887\n",
      "Train Epoch: 456 [0/54000 (0%)] Loss: -232545.906250\n",
      "Train Epoch: 456 [1408/54000 (3%)] Loss: -230925.250000\n",
      "Train Epoch: 456 [2816/54000 (5%)] Loss: -219994.046875\n",
      "Train Epoch: 456 [4224/54000 (8%)] Loss: -229239.875000\n",
      "Train Epoch: 456 [5632/54000 (10%)] Loss: -220703.078125\n",
      "Train Epoch: 456 [7040/54000 (13%)] Loss: -220101.984375\n",
      "Train Epoch: 456 [8448/54000 (16%)] Loss: -221176.281250\n",
      "Train Epoch: 456 [9856/54000 (18%)] Loss: -219904.359375\n",
      "Train Epoch: 456 [11264/54000 (21%)] Loss: -232096.078125\n",
      "Train Epoch: 456 [12672/54000 (23%)] Loss: -221521.937500\n",
      "Train Epoch: 456 [14080/54000 (26%)] Loss: -233066.937500\n",
      "Train Epoch: 456 [15488/54000 (29%)] Loss: -248598.265625\n",
      "Train Epoch: 456 [16896/54000 (31%)] Loss: -230187.343750\n",
      "Train Epoch: 456 [18304/54000 (34%)] Loss: -220229.750000\n",
      "Train Epoch: 456 [19712/54000 (37%)] Loss: -223022.093750\n",
      "Train Epoch: 456 [21120/54000 (39%)] Loss: -250825.000000\n",
      "Train Epoch: 456 [22528/54000 (42%)] Loss: -220388.078125\n",
      "Train Epoch: 456 [23936/54000 (44%)] Loss: -232661.718750\n",
      "Train Epoch: 456 [25344/54000 (47%)] Loss: -225584.406250\n",
      "Train Epoch: 456 [26752/54000 (50%)] Loss: -223957.718750\n",
      "Train Epoch: 456 [28160/54000 (52%)] Loss: -227844.750000\n",
      "Train Epoch: 456 [29568/54000 (55%)] Loss: -231885.687500\n",
      "Train Epoch: 456 [30976/54000 (57%)] Loss: -231424.656250\n",
      "Train Epoch: 456 [32384/54000 (60%)] Loss: -221697.296875\n",
      "Train Epoch: 456 [33792/54000 (63%)] Loss: -249450.203125\n",
      "Train Epoch: 456 [35200/54000 (65%)] Loss: -218584.125000\n",
      "Train Epoch: 456 [36608/54000 (68%)] Loss: -221411.156250\n",
      "Train Epoch: 456 [38016/54000 (70%)] Loss: -220262.953125\n",
      "Train Epoch: 456 [39424/54000 (73%)] Loss: -233044.281250\n",
      "Train Epoch: 456 [40832/54000 (76%)] Loss: -224989.171875\n",
      "Train Epoch: 456 [42240/54000 (78%)] Loss: -232096.015625\n",
      "Train Epoch: 456 [43648/54000 (81%)] Loss: -221454.187500\n",
      "Train Epoch: 456 [45056/54000 (83%)] Loss: -250839.656250\n",
      "Train Epoch: 456 [46464/54000 (86%)] Loss: -220852.125000\n",
      "Train Epoch: 456 [47872/54000 (89%)] Loss: -230131.578125\n",
      "Train Epoch: 456 [49280/54000 (91%)] Loss: -222580.250000\n",
      "Train Epoch: 456 [50688/54000 (94%)] Loss: -224337.843750\n",
      "Train Epoch: 456 [52096/54000 (96%)] Loss: -223635.125000\n",
      "    epoch          : 456\n",
      "    loss           : -228648.08361991626\n",
      "    val_loss       : -230234.03944955222\n",
      "Train Epoch: 457 [0/54000 (0%)] Loss: -220146.484375\n",
      "Train Epoch: 457 [1408/54000 (3%)] Loss: -227221.046875\n",
      "Train Epoch: 457 [2816/54000 (5%)] Loss: -250625.875000\n",
      "Train Epoch: 457 [4224/54000 (8%)] Loss: -221665.718750\n",
      "Train Epoch: 457 [5632/54000 (10%)] Loss: -223859.015625\n",
      "Train Epoch: 457 [7040/54000 (13%)] Loss: -226632.000000\n",
      "Train Epoch: 457 [8448/54000 (16%)] Loss: -223185.812500\n",
      "Train Epoch: 457 [9856/54000 (18%)] Loss: -220762.453125\n",
      "Train Epoch: 457 [11264/54000 (21%)] Loss: -219385.093750\n",
      "Train Epoch: 457 [12672/54000 (23%)] Loss: -230756.343750\n",
      "Train Epoch: 457 [14080/54000 (26%)] Loss: -232039.406250\n",
      "Train Epoch: 457 [15488/54000 (29%)] Loss: -244662.078125\n",
      "Train Epoch: 457 [16896/54000 (31%)] Loss: -229854.234375\n",
      "Train Epoch: 457 [18304/54000 (34%)] Loss: -229251.828125\n",
      "Train Epoch: 457 [19712/54000 (37%)] Loss: -223335.125000\n",
      "Train Epoch: 457 [21120/54000 (39%)] Loss: -222835.093750\n",
      "Train Epoch: 457 [22528/54000 (42%)] Loss: -221138.156250\n",
      "Train Epoch: 457 [23936/54000 (44%)] Loss: -224053.750000\n",
      "Train Epoch: 457 [25344/54000 (47%)] Loss: -224117.390625\n",
      "Train Epoch: 457 [26752/54000 (50%)] Loss: -219545.968750\n",
      "Train Epoch: 457 [28160/54000 (52%)] Loss: -221200.765625\n",
      "Train Epoch: 457 [29568/54000 (55%)] Loss: -221563.625000\n",
      "Train Epoch: 457 [30976/54000 (57%)] Loss: -223401.890625\n",
      "Train Epoch: 457 [32384/54000 (60%)] Loss: -221237.500000\n",
      "Train Epoch: 457 [33792/54000 (63%)] Loss: -223004.968750\n",
      "Train Epoch: 457 [35200/54000 (65%)] Loss: -230254.062500\n",
      "Train Epoch: 457 [36608/54000 (68%)] Loss: -221039.281250\n",
      "Train Epoch: 457 [38016/54000 (70%)] Loss: -248949.640625\n",
      "Train Epoch: 457 [39424/54000 (73%)] Loss: -219669.484375\n",
      "Train Epoch: 457 [40832/54000 (76%)] Loss: -232139.359375\n",
      "Train Epoch: 457 [42240/54000 (78%)] Loss: -224091.859375\n",
      "Train Epoch: 457 [43648/54000 (81%)] Loss: -249897.671875\n",
      "Train Epoch: 457 [45056/54000 (83%)] Loss: -232567.312500\n",
      "Train Epoch: 457 [46464/54000 (86%)] Loss: -226912.296875\n",
      "Train Epoch: 457 [47872/54000 (89%)] Loss: -224114.578125\n",
      "Train Epoch: 457 [49280/54000 (91%)] Loss: -218746.218750\n",
      "Train Epoch: 457 [50688/54000 (94%)] Loss: -223711.531250\n",
      "Train Epoch: 457 [52096/54000 (96%)] Loss: -231474.562500\n",
      "    epoch          : 457\n",
      "    loss           : -228734.01506429425\n",
      "    val_loss       : -230624.88369974276\n",
      "Train Epoch: 458 [0/54000 (0%)] Loss: -233537.015625\n",
      "Train Epoch: 458 [1408/54000 (3%)] Loss: -232360.234375\n",
      "Train Epoch: 458 [2816/54000 (5%)] Loss: -225180.281250\n",
      "Train Epoch: 458 [4224/54000 (8%)] Loss: -225021.359375\n",
      "Train Epoch: 458 [5632/54000 (10%)] Loss: -231900.093750\n",
      "Train Epoch: 458 [7040/54000 (13%)] Loss: -222007.328125\n",
      "Train Epoch: 458 [8448/54000 (16%)] Loss: -250712.812500\n",
      "Train Epoch: 458 [9856/54000 (18%)] Loss: -249629.000000\n",
      "Train Epoch: 458 [11264/54000 (21%)] Loss: -225279.953125\n",
      "Train Epoch: 458 [12672/54000 (23%)] Loss: -224282.890625\n",
      "Train Epoch: 458 [14080/54000 (26%)] Loss: -223213.593750\n",
      "Train Epoch: 458 [15488/54000 (29%)] Loss: -224487.046875\n",
      "Train Epoch: 458 [16896/54000 (31%)] Loss: -227969.546875\n",
      "Train Epoch: 458 [18304/54000 (34%)] Loss: -250286.593750\n",
      "Train Epoch: 458 [19712/54000 (37%)] Loss: -231553.734375\n",
      "Train Epoch: 458 [21120/54000 (39%)] Loss: -225851.062500\n",
      "Train Epoch: 458 [22528/54000 (42%)] Loss: -232582.921875\n",
      "Train Epoch: 458 [23936/54000 (44%)] Loss: -231738.031250\n",
      "Train Epoch: 458 [25344/54000 (47%)] Loss: -231107.156250\n",
      "Train Epoch: 458 [26752/54000 (50%)] Loss: -221128.015625\n",
      "Train Epoch: 458 [28160/54000 (52%)] Loss: -227067.906250\n",
      "Train Epoch: 458 [29568/54000 (55%)] Loss: -231789.937500\n",
      "Train Epoch: 458 [30976/54000 (57%)] Loss: -230596.000000\n",
      "Train Epoch: 458 [32384/54000 (60%)] Loss: -232828.218750\n",
      "Train Epoch: 458 [33792/54000 (63%)] Loss: -221337.937500\n",
      "Train Epoch: 458 [35200/54000 (65%)] Loss: -228628.000000\n",
      "Train Epoch: 458 [36608/54000 (68%)] Loss: -223081.687500\n",
      "Train Epoch: 458 [38016/54000 (70%)] Loss: -226885.156250\n",
      "Train Epoch: 458 [39424/54000 (73%)] Loss: -223133.937500\n",
      "Train Epoch: 458 [40832/54000 (76%)] Loss: -222946.968750\n",
      "Train Epoch: 458 [42240/54000 (78%)] Loss: -231993.187500\n",
      "Train Epoch: 458 [43648/54000 (81%)] Loss: -249783.750000\n",
      "Train Epoch: 458 [45056/54000 (83%)] Loss: -225914.500000\n",
      "Train Epoch: 458 [46464/54000 (86%)] Loss: -227554.875000\n",
      "Train Epoch: 458 [47872/54000 (89%)] Loss: -222423.406250\n",
      "Train Epoch: 458 [49280/54000 (91%)] Loss: -225982.250000\n",
      "Train Epoch: 458 [50688/54000 (94%)] Loss: -223366.890625\n",
      "Train Epoch: 458 [52096/54000 (96%)] Loss: -221716.000000\n",
      "    epoch          : 458\n",
      "    loss           : -228753.76308313396\n",
      "    val_loss       : -230928.30286537728\n",
      "Train Epoch: 459 [0/54000 (0%)] Loss: -248635.250000\n",
      "Train Epoch: 459 [1408/54000 (3%)] Loss: -223002.906250\n",
      "Train Epoch: 459 [2816/54000 (5%)] Loss: -223815.484375\n",
      "Train Epoch: 459 [4224/54000 (8%)] Loss: -227008.640625\n",
      "Train Epoch: 459 [5632/54000 (10%)] Loss: -230367.046875\n",
      "Train Epoch: 459 [7040/54000 (13%)] Loss: -228088.328125\n",
      "Train Epoch: 459 [8448/54000 (16%)] Loss: -227592.718750\n",
      "Train Epoch: 459 [9856/54000 (18%)] Loss: -222977.750000\n",
      "Train Epoch: 459 [11264/54000 (21%)] Loss: -221961.078125\n",
      "Train Epoch: 459 [12672/54000 (23%)] Loss: -217783.765625\n",
      "Train Epoch: 459 [14080/54000 (26%)] Loss: -233960.031250\n",
      "Train Epoch: 459 [15488/54000 (29%)] Loss: -248852.812500\n",
      "Train Epoch: 459 [16896/54000 (31%)] Loss: -223695.734375\n",
      "Train Epoch: 459 [18304/54000 (34%)] Loss: -233302.687500\n",
      "Train Epoch: 459 [19712/54000 (37%)] Loss: -219425.078125\n",
      "Train Epoch: 459 [21120/54000 (39%)] Loss: -232989.812500\n",
      "Train Epoch: 459 [22528/54000 (42%)] Loss: -219153.906250\n",
      "Train Epoch: 459 [23936/54000 (44%)] Loss: -231895.625000\n",
      "Train Epoch: 459 [25344/54000 (47%)] Loss: -230637.656250\n",
      "Train Epoch: 459 [26752/54000 (50%)] Loss: -231969.093750\n",
      "Train Epoch: 459 [28160/54000 (52%)] Loss: -230640.781250\n",
      "Train Epoch: 459 [29568/54000 (55%)] Loss: -226922.781250\n",
      "Train Epoch: 459 [30976/54000 (57%)] Loss: -224365.500000\n",
      "Train Epoch: 459 [32384/54000 (60%)] Loss: -250939.921875\n",
      "Train Epoch: 459 [33792/54000 (63%)] Loss: -217415.765625\n",
      "Train Epoch: 459 [35200/54000 (65%)] Loss: -231888.390625\n",
      "Train Epoch: 459 [36608/54000 (68%)] Loss: -231969.687500\n",
      "Train Epoch: 459 [38016/54000 (70%)] Loss: -221061.609375\n",
      "Train Epoch: 459 [39424/54000 (73%)] Loss: -222334.937500\n",
      "Train Epoch: 459 [40832/54000 (76%)] Loss: -224768.140625\n",
      "Train Epoch: 459 [42240/54000 (78%)] Loss: -228976.187500\n",
      "Train Epoch: 459 [43648/54000 (81%)] Loss: -228134.609375\n",
      "Train Epoch: 459 [45056/54000 (83%)] Loss: -227082.796875\n",
      "Train Epoch: 459 [46464/54000 (86%)] Loss: -231196.953125\n",
      "Train Epoch: 459 [47872/54000 (89%)] Loss: -228318.156250\n",
      "Train Epoch: 459 [49280/54000 (91%)] Loss: -249738.718750\n",
      "Train Epoch: 459 [50688/54000 (94%)] Loss: -220759.937500\n",
      "Train Epoch: 459 [52096/54000 (96%)] Loss: -233320.171875\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch   459: reducing learning rate of group 0 to 1.2500e-05.\n",
      "    epoch          : 459\n",
      "    loss           : -228827.44407894736\n",
      "    val_loss       : -230752.54475514483\n",
      "Train Epoch: 460 [0/54000 (0%)] Loss: -232413.906250\n",
      "Train Epoch: 460 [1408/54000 (3%)] Loss: -231939.000000\n",
      "Train Epoch: 460 [2816/54000 (5%)] Loss: -231901.609375\n",
      "Train Epoch: 460 [4224/54000 (8%)] Loss: -221688.312500\n",
      "Train Epoch: 460 [5632/54000 (10%)] Loss: -218140.015625\n",
      "Train Epoch: 460 [7040/54000 (13%)] Loss: -222917.562500\n",
      "Train Epoch: 460 [8448/54000 (16%)] Loss: -219329.265625\n",
      "Train Epoch: 460 [9856/54000 (18%)] Loss: -223151.468750\n",
      "Train Epoch: 460 [11264/54000 (21%)] Loss: -232682.500000\n",
      "Train Epoch: 460 [12672/54000 (23%)] Loss: -223360.093750\n",
      "Train Epoch: 460 [14080/54000 (26%)] Loss: -222309.375000\n",
      "Train Epoch: 460 [15488/54000 (29%)] Loss: -224465.859375\n",
      "Train Epoch: 460 [16896/54000 (31%)] Loss: -229363.875000\n",
      "Train Epoch: 460 [18304/54000 (34%)] Loss: -231933.843750\n",
      "Train Epoch: 460 [19712/54000 (37%)] Loss: -220404.421875\n",
      "Train Epoch: 460 [21120/54000 (39%)] Loss: -218463.390625\n",
      "Train Epoch: 460 [22528/54000 (42%)] Loss: -231993.500000\n",
      "Train Epoch: 460 [23936/54000 (44%)] Loss: -224535.625000\n",
      "Train Epoch: 460 [25344/54000 (47%)] Loss: -219211.468750\n",
      "Train Epoch: 460 [26752/54000 (50%)] Loss: -220904.718750\n",
      "Train Epoch: 460 [28160/54000 (52%)] Loss: -222404.812500\n",
      "Train Epoch: 460 [29568/54000 (55%)] Loss: -219676.453125\n",
      "Train Epoch: 460 [30976/54000 (57%)] Loss: -229016.687500\n",
      "Train Epoch: 460 [32384/54000 (60%)] Loss: -230751.281250\n",
      "Train Epoch: 460 [33792/54000 (63%)] Loss: -224987.390625\n",
      "Train Epoch: 460 [35200/54000 (65%)] Loss: -227595.828125\n",
      "Train Epoch: 460 [36608/54000 (68%)] Loss: -226097.390625\n",
      "Train Epoch: 460 [38016/54000 (70%)] Loss: -225498.187500\n",
      "Train Epoch: 460 [39424/54000 (73%)] Loss: -221169.515625\n",
      "Train Epoch: 460 [40832/54000 (76%)] Loss: -226237.593750\n",
      "Train Epoch: 460 [42240/54000 (78%)] Loss: -231456.578125\n",
      "Train Epoch: 460 [43648/54000 (81%)] Loss: -221418.125000\n",
      "Train Epoch: 460 [45056/54000 (83%)] Loss: -227194.531250\n",
      "Train Epoch: 460 [46464/54000 (86%)] Loss: -220659.343750\n",
      "Train Epoch: 460 [47872/54000 (89%)] Loss: -220047.421875\n",
      "Train Epoch: 460 [49280/54000 (91%)] Loss: -223172.328125\n",
      "Train Epoch: 460 [50688/54000 (94%)] Loss: -229142.343750\n",
      "Train Epoch: 460 [52096/54000 (96%)] Loss: -219596.343750\n",
      "    epoch          : 460\n",
      "    loss           : -228702.4053902512\n",
      "    val_loss       : -230474.5199063929\n",
      "Train Epoch: 461 [0/54000 (0%)] Loss: -221153.812500\n",
      "Train Epoch: 461 [1408/54000 (3%)] Loss: -229546.921875\n",
      "Train Epoch: 461 [2816/54000 (5%)] Loss: -229486.734375\n",
      "Train Epoch: 461 [4224/54000 (8%)] Loss: -230954.734375\n",
      "Train Epoch: 461 [5632/54000 (10%)] Loss: -218477.390625\n",
      "Train Epoch: 461 [7040/54000 (13%)] Loss: -231763.921875\n",
      "Train Epoch: 461 [8448/54000 (16%)] Loss: -231770.187500\n",
      "Train Epoch: 461 [9856/54000 (18%)] Loss: -220122.812500\n",
      "Train Epoch: 461 [11264/54000 (21%)] Loss: -222109.015625\n",
      "Train Epoch: 461 [12672/54000 (23%)] Loss: -228219.546875\n",
      "Train Epoch: 461 [14080/54000 (26%)] Loss: -228502.171875\n",
      "Train Epoch: 461 [15488/54000 (29%)] Loss: -223958.750000\n",
      "Train Epoch: 461 [16896/54000 (31%)] Loss: -222045.718750\n",
      "Train Epoch: 461 [18304/54000 (34%)] Loss: -225214.000000\n",
      "Train Epoch: 461 [19712/54000 (37%)] Loss: -230379.187500\n",
      "Train Epoch: 461 [21120/54000 (39%)] Loss: -223221.843750\n",
      "Train Epoch: 461 [22528/54000 (42%)] Loss: -229163.218750\n",
      "Train Epoch: 461 [23936/54000 (44%)] Loss: -218782.984375\n",
      "Train Epoch: 461 [25344/54000 (47%)] Loss: -222036.609375\n",
      "Train Epoch: 461 [26752/54000 (50%)] Loss: -232020.125000\n",
      "Train Epoch: 461 [28160/54000 (52%)] Loss: -226465.500000\n",
      "Train Epoch: 461 [29568/54000 (55%)] Loss: -221506.031250\n",
      "Train Epoch: 461 [30976/54000 (57%)] Loss: -220317.921875\n",
      "Train Epoch: 461 [32384/54000 (60%)] Loss: -249656.812500\n",
      "Train Epoch: 461 [33792/54000 (63%)] Loss: -227673.781250\n",
      "Train Epoch: 461 [35200/54000 (65%)] Loss: -227222.968750\n",
      "Train Epoch: 461 [36608/54000 (68%)] Loss: -228263.390625\n",
      "Train Epoch: 461 [38016/54000 (70%)] Loss: -250006.062500\n",
      "Train Epoch: 461 [39424/54000 (73%)] Loss: -233033.187500\n",
      "Train Epoch: 461 [40832/54000 (76%)] Loss: -232179.671875\n",
      "Train Epoch: 461 [42240/54000 (78%)] Loss: -233120.593750\n",
      "Train Epoch: 461 [43648/54000 (81%)] Loss: -221592.828125\n",
      "Train Epoch: 461 [45056/54000 (83%)] Loss: -218065.625000\n",
      "Train Epoch: 461 [46464/54000 (86%)] Loss: -221381.234375\n",
      "Train Epoch: 461 [47872/54000 (89%)] Loss: -233012.968750\n",
      "Train Epoch: 461 [49280/54000 (91%)] Loss: -220418.859375\n",
      "Train Epoch: 461 [50688/54000 (94%)] Loss: -249444.937500\n",
      "Train Epoch: 461 [52096/54000 (96%)] Loss: -250402.984375\n",
      "    epoch          : 461\n",
      "    loss           : -228761.74013157896\n",
      "    val_loss       : -230474.56837723896\n",
      "Train Epoch: 462 [0/54000 (0%)] Loss: -226278.703125\n",
      "Train Epoch: 462 [1408/54000 (3%)] Loss: -249577.281250\n",
      "Train Epoch: 462 [2816/54000 (5%)] Loss: -230618.921875\n",
      "Train Epoch: 462 [4224/54000 (8%)] Loss: -225491.625000\n",
      "Train Epoch: 462 [5632/54000 (10%)] Loss: -220712.062500\n",
      "Train Epoch: 462 [7040/54000 (13%)] Loss: -248891.171875\n",
      "Train Epoch: 462 [8448/54000 (16%)] Loss: -219448.312500\n",
      "Train Epoch: 462 [9856/54000 (18%)] Loss: -216433.234375\n",
      "Train Epoch: 462 [11264/54000 (21%)] Loss: -223552.765625\n",
      "Train Epoch: 462 [12672/54000 (23%)] Loss: -250993.031250\n",
      "Train Epoch: 462 [14080/54000 (26%)] Loss: -220484.531250\n",
      "Train Epoch: 462 [15488/54000 (29%)] Loss: -217521.093750\n",
      "Train Epoch: 462 [16896/54000 (31%)] Loss: -223086.812500\n",
      "Train Epoch: 462 [18304/54000 (34%)] Loss: -222954.109375\n",
      "Train Epoch: 462 [19712/54000 (37%)] Loss: -232865.500000\n",
      "Train Epoch: 462 [21120/54000 (39%)] Loss: -219983.609375\n",
      "Train Epoch: 462 [22528/54000 (42%)] Loss: -224812.031250\n",
      "Train Epoch: 462 [23936/54000 (44%)] Loss: -250120.437500\n",
      "Train Epoch: 462 [25344/54000 (47%)] Loss: -229980.421875\n",
      "Train Epoch: 462 [26752/54000 (50%)] Loss: -230006.875000\n",
      "Train Epoch: 462 [28160/54000 (52%)] Loss: -224544.359375\n",
      "Train Epoch: 462 [29568/54000 (55%)] Loss: -224674.515625\n",
      "Train Epoch: 462 [30976/54000 (57%)] Loss: -223047.875000\n",
      "Train Epoch: 462 [32384/54000 (60%)] Loss: -223048.531250\n",
      "Train Epoch: 462 [33792/54000 (63%)] Loss: -230365.906250\n",
      "Train Epoch: 462 [35200/54000 (65%)] Loss: -249223.093750\n",
      "Train Epoch: 462 [36608/54000 (68%)] Loss: -226822.031250\n",
      "Train Epoch: 462 [38016/54000 (70%)] Loss: -222846.406250\n",
      "Train Epoch: 462 [39424/54000 (73%)] Loss: -232841.953125\n",
      "Train Epoch: 462 [40832/54000 (76%)] Loss: -232169.359375\n",
      "Train Epoch: 462 [42240/54000 (78%)] Loss: -227925.921875\n",
      "Train Epoch: 462 [43648/54000 (81%)] Loss: -227198.062500\n",
      "Train Epoch: 462 [45056/54000 (83%)] Loss: -222698.578125\n",
      "Train Epoch: 462 [46464/54000 (86%)] Loss: -227095.859375\n",
      "Train Epoch: 462 [47872/54000 (89%)] Loss: -224634.406250\n",
      "Train Epoch: 462 [49280/54000 (91%)] Loss: -221737.750000\n",
      "Train Epoch: 462 [50688/54000 (94%)] Loss: -249631.578125\n",
      "Train Epoch: 462 [52096/54000 (96%)] Loss: -232452.171875\n",
      "    epoch          : 462\n",
      "    loss           : -228881.86247757176\n",
      "    val_loss       : -230945.16410418254\n",
      "Train Epoch: 463 [0/54000 (0%)] Loss: -220448.546875\n",
      "Train Epoch: 463 [1408/54000 (3%)] Loss: -250811.218750\n",
      "Train Epoch: 463 [2816/54000 (5%)] Loss: -231801.390625\n",
      "Train Epoch: 463 [4224/54000 (8%)] Loss: -227166.437500\n",
      "Train Epoch: 463 [5632/54000 (10%)] Loss: -219445.000000\n",
      "Train Epoch: 463 [7040/54000 (13%)] Loss: -249163.625000\n",
      "Train Epoch: 463 [8448/54000 (16%)] Loss: -223874.312500\n",
      "Train Epoch: 463 [9856/54000 (18%)] Loss: -231774.046875\n",
      "Train Epoch: 463 [11264/54000 (21%)] Loss: -220238.781250\n",
      "Train Epoch: 463 [12672/54000 (23%)] Loss: -220825.906250\n",
      "Train Epoch: 463 [14080/54000 (26%)] Loss: -220221.250000\n",
      "Train Epoch: 463 [15488/54000 (29%)] Loss: -221321.828125\n",
      "Train Epoch: 463 [16896/54000 (31%)] Loss: -229865.828125\n",
      "Train Epoch: 463 [18304/54000 (34%)] Loss: -250203.281250\n",
      "Train Epoch: 463 [19712/54000 (37%)] Loss: -228081.437500\n",
      "Train Epoch: 463 [21120/54000 (39%)] Loss: -225320.468750\n",
      "Train Epoch: 463 [22528/54000 (42%)] Loss: -225716.015625\n",
      "Train Epoch: 463 [23936/54000 (44%)] Loss: -221216.546875\n",
      "Train Epoch: 463 [25344/54000 (47%)] Loss: -250721.546875\n",
      "Train Epoch: 463 [26752/54000 (50%)] Loss: -221515.625000\n",
      "Train Epoch: 463 [28160/54000 (52%)] Loss: -232209.468750\n",
      "Train Epoch: 463 [29568/54000 (55%)] Loss: -228927.125000\n",
      "Train Epoch: 463 [30976/54000 (57%)] Loss: -248541.890625\n",
      "Train Epoch: 463 [32384/54000 (60%)] Loss: -233262.453125\n",
      "Train Epoch: 463 [33792/54000 (63%)] Loss: -222657.562500\n",
      "Train Epoch: 463 [35200/54000 (65%)] Loss: -222566.328125\n",
      "Train Epoch: 463 [36608/54000 (68%)] Loss: -220806.875000\n",
      "Train Epoch: 463 [38016/54000 (70%)] Loss: -220091.921875\n",
      "Train Epoch: 463 [39424/54000 (73%)] Loss: -227689.015625\n",
      "Train Epoch: 463 [40832/54000 (76%)] Loss: -223028.296875\n",
      "Train Epoch: 463 [42240/54000 (78%)] Loss: -232353.546875\n",
      "Train Epoch: 463 [43648/54000 (81%)] Loss: -219239.359375\n",
      "Train Epoch: 463 [45056/54000 (83%)] Loss: -229292.937500\n",
      "Train Epoch: 463 [46464/54000 (86%)] Loss: -250196.593750\n",
      "Train Epoch: 463 [47872/54000 (89%)] Loss: -224336.296875\n",
      "Train Epoch: 463 [49280/54000 (91%)] Loss: -224192.203125\n",
      "Train Epoch: 463 [50688/54000 (94%)] Loss: -231861.765625\n",
      "Train Epoch: 463 [52096/54000 (96%)] Loss: -221090.343750\n",
      "    epoch          : 463\n",
      "    loss           : -228737.87339264355\n",
      "    val_loss       : -230815.57144388338\n",
      "Train Epoch: 464 [0/54000 (0%)] Loss: -231234.312500\n",
      "Train Epoch: 464 [1408/54000 (3%)] Loss: -233342.234375\n",
      "Train Epoch: 464 [2816/54000 (5%)] Loss: -231107.328125\n",
      "Train Epoch: 464 [4224/54000 (8%)] Loss: -232671.625000\n",
      "Train Epoch: 464 [5632/54000 (10%)] Loss: -222211.484375\n",
      "Train Epoch: 464 [7040/54000 (13%)] Loss: -228393.281250\n",
      "Train Epoch: 464 [8448/54000 (16%)] Loss: -222161.000000\n",
      "Train Epoch: 464 [9856/54000 (18%)] Loss: -227331.328125\n",
      "Train Epoch: 464 [11264/54000 (21%)] Loss: -227000.921875\n",
      "Train Epoch: 464 [12672/54000 (23%)] Loss: -227570.593750\n",
      "Train Epoch: 464 [14080/54000 (26%)] Loss: -225784.812500\n",
      "Train Epoch: 464 [15488/54000 (29%)] Loss: -232543.312500\n",
      "Train Epoch: 464 [16896/54000 (31%)] Loss: -232563.578125\n",
      "Train Epoch: 464 [18304/54000 (34%)] Loss: -229998.593750\n",
      "Train Epoch: 464 [19712/54000 (37%)] Loss: -228198.156250\n",
      "Train Epoch: 464 [21120/54000 (39%)] Loss: -248505.125000\n",
      "Train Epoch: 464 [22528/54000 (42%)] Loss: -229491.671875\n",
      "Train Epoch: 464 [23936/54000 (44%)] Loss: -232856.312500\n",
      "Train Epoch: 464 [25344/54000 (47%)] Loss: -228527.531250\n",
      "Train Epoch: 464 [26752/54000 (50%)] Loss: -220457.343750\n",
      "Train Epoch: 464 [28160/54000 (52%)] Loss: -227739.343750\n",
      "Train Epoch: 464 [29568/54000 (55%)] Loss: -230717.984375\n",
      "Train Epoch: 464 [30976/54000 (57%)] Loss: -221225.765625\n",
      "Train Epoch: 464 [32384/54000 (60%)] Loss: -250203.781250\n",
      "Train Epoch: 464 [33792/54000 (63%)] Loss: -224116.125000\n",
      "Train Epoch: 464 [35200/54000 (65%)] Loss: -230498.828125\n",
      "Train Epoch: 464 [36608/54000 (68%)] Loss: -223309.187500\n",
      "Train Epoch: 464 [38016/54000 (70%)] Loss: -222274.406250\n",
      "Train Epoch: 464 [39424/54000 (73%)] Loss: -246991.468750\n",
      "Train Epoch: 464 [40832/54000 (76%)] Loss: -231767.718750\n",
      "Train Epoch: 464 [42240/54000 (78%)] Loss: -226030.890625\n",
      "Train Epoch: 464 [43648/54000 (81%)] Loss: -223578.937500\n",
      "Train Epoch: 464 [45056/54000 (83%)] Loss: -229086.828125\n",
      "Train Epoch: 464 [46464/54000 (86%)] Loss: -226615.656250\n",
      "Train Epoch: 464 [47872/54000 (89%)] Loss: -221717.796875\n",
      "Train Epoch: 464 [49280/54000 (91%)] Loss: -232902.671875\n",
      "Train Epoch: 464 [50688/54000 (94%)] Loss: -230924.312500\n",
      "Train Epoch: 464 [52096/54000 (96%)] Loss: -219910.265625\n",
      "    epoch          : 464\n",
      "    loss           : -228945.29870663874\n",
      "    val_loss       : -230301.4149318788\n",
      "Train Epoch: 465 [0/54000 (0%)] Loss: -250655.968750\n",
      "Train Epoch: 465 [1408/54000 (3%)] Loss: -224365.156250\n",
      "Train Epoch: 465 [2816/54000 (5%)] Loss: -232056.468750\n",
      "Train Epoch: 465 [4224/54000 (8%)] Loss: -218985.234375\n",
      "Train Epoch: 465 [5632/54000 (10%)] Loss: -232952.843750\n",
      "Train Epoch: 465 [7040/54000 (13%)] Loss: -223592.093750\n",
      "Train Epoch: 465 [8448/54000 (16%)] Loss: -231304.078125\n",
      "Train Epoch: 465 [9856/54000 (18%)] Loss: -223304.968750\n",
      "Train Epoch: 465 [11264/54000 (21%)] Loss: -224019.734375\n",
      "Train Epoch: 465 [12672/54000 (23%)] Loss: -218552.031250\n",
      "Train Epoch: 465 [14080/54000 (26%)] Loss: -233748.062500\n",
      "Train Epoch: 465 [15488/54000 (29%)] Loss: -220960.609375\n",
      "Train Epoch: 465 [16896/54000 (31%)] Loss: -219905.859375\n",
      "Train Epoch: 465 [18304/54000 (34%)] Loss: -229414.812500\n",
      "Train Epoch: 465 [19712/54000 (37%)] Loss: -218178.796875\n",
      "Train Epoch: 465 [21120/54000 (39%)] Loss: -230398.531250\n",
      "Train Epoch: 465 [22528/54000 (42%)] Loss: -231776.171875\n",
      "Train Epoch: 465 [23936/54000 (44%)] Loss: -224748.546875\n",
      "Train Epoch: 465 [25344/54000 (47%)] Loss: -224167.531250\n",
      "Train Epoch: 465 [26752/54000 (50%)] Loss: -223772.125000\n",
      "Train Epoch: 465 [28160/54000 (52%)] Loss: -224005.703125\n",
      "Train Epoch: 465 [29568/54000 (55%)] Loss: -219541.000000\n",
      "Train Epoch: 465 [30976/54000 (57%)] Loss: -230866.515625\n",
      "Train Epoch: 465 [32384/54000 (60%)] Loss: -220932.500000\n",
      "Train Epoch: 465 [33792/54000 (63%)] Loss: -218104.453125\n",
      "Train Epoch: 465 [35200/54000 (65%)] Loss: -220284.750000\n",
      "Train Epoch: 465 [36608/54000 (68%)] Loss: -231714.921875\n",
      "Train Epoch: 465 [38016/54000 (70%)] Loss: -232378.234375\n",
      "Train Epoch: 465 [39424/54000 (73%)] Loss: -223884.312500\n",
      "Train Epoch: 465 [40832/54000 (76%)] Loss: -231128.171875\n",
      "Train Epoch: 465 [42240/54000 (78%)] Loss: -231874.375000\n",
      "Train Epoch: 465 [43648/54000 (81%)] Loss: -213743.156250\n",
      "Train Epoch: 465 [45056/54000 (83%)] Loss: -249886.500000\n",
      "Train Epoch: 465 [46464/54000 (86%)] Loss: -225848.703125\n",
      "Train Epoch: 465 [47872/54000 (89%)] Loss: -226324.187500\n",
      "Train Epoch: 465 [49280/54000 (91%)] Loss: -230420.546875\n",
      "Train Epoch: 465 [50688/54000 (94%)] Loss: -222417.734375\n",
      "Train Epoch: 465 [52096/54000 (96%)] Loss: -220224.250000\n",
      "    epoch          : 465\n",
      "    loss           : -228755.41372607654\n",
      "    val_loss       : -230877.82263481326\n",
      "Train Epoch: 466 [0/54000 (0%)] Loss: -251179.156250\n",
      "Train Epoch: 466 [1408/54000 (3%)] Loss: -250766.218750\n",
      "Train Epoch: 466 [2816/54000 (5%)] Loss: -231871.328125\n",
      "Train Epoch: 466 [4224/54000 (8%)] Loss: -226395.734375\n",
      "Train Epoch: 466 [5632/54000 (10%)] Loss: -223918.906250\n",
      "Train Epoch: 466 [7040/54000 (13%)] Loss: -223046.625000\n",
      "Train Epoch: 466 [8448/54000 (16%)] Loss: -232169.812500\n",
      "Train Epoch: 466 [9856/54000 (18%)] Loss: -250441.312500\n",
      "Train Epoch: 466 [11264/54000 (21%)] Loss: -232432.390625\n",
      "Train Epoch: 466 [12672/54000 (23%)] Loss: -224310.343750\n",
      "Train Epoch: 466 [14080/54000 (26%)] Loss: -223741.546875\n",
      "Train Epoch: 466 [15488/54000 (29%)] Loss: -223824.781250\n",
      "Train Epoch: 466 [16896/54000 (31%)] Loss: -222963.593750\n",
      "Train Epoch: 466 [18304/54000 (34%)] Loss: -224998.625000\n",
      "Train Epoch: 466 [19712/54000 (37%)] Loss: -230909.937500\n",
      "Train Epoch: 466 [21120/54000 (39%)] Loss: -251077.656250\n",
      "Train Epoch: 466 [22528/54000 (42%)] Loss: -227241.062500\n",
      "Train Epoch: 466 [23936/54000 (44%)] Loss: -231004.156250\n",
      "Train Epoch: 466 [25344/54000 (47%)] Loss: -221316.687500\n",
      "Train Epoch: 466 [26752/54000 (50%)] Loss: -230826.781250\n",
      "Train Epoch: 466 [28160/54000 (52%)] Loss: -249854.187500\n",
      "Train Epoch: 466 [29568/54000 (55%)] Loss: -221326.156250\n",
      "Train Epoch: 466 [30976/54000 (57%)] Loss: -221795.296875\n",
      "Train Epoch: 466 [32384/54000 (60%)] Loss: -231410.375000\n",
      "Train Epoch: 466 [33792/54000 (63%)] Loss: -229931.453125\n",
      "Train Epoch: 466 [35200/54000 (65%)] Loss: -226094.687500\n",
      "Train Epoch: 466 [36608/54000 (68%)] Loss: -226124.437500\n",
      "Train Epoch: 466 [38016/54000 (70%)] Loss: -225709.125000\n",
      "Train Epoch: 466 [39424/54000 (73%)] Loss: -222292.781250\n",
      "Train Epoch: 466 [40832/54000 (76%)] Loss: -227452.671875\n",
      "Train Epoch: 466 [42240/54000 (78%)] Loss: -221876.062500\n",
      "Train Epoch: 466 [43648/54000 (81%)] Loss: -231485.328125\n",
      "Train Epoch: 466 [45056/54000 (83%)] Loss: -228227.015625\n",
      "Train Epoch: 466 [46464/54000 (86%)] Loss: -220238.343750\n",
      "Train Epoch: 466 [47872/54000 (89%)] Loss: -229405.734375\n",
      "Train Epoch: 466 [49280/54000 (91%)] Loss: -227011.187500\n",
      "Train Epoch: 466 [50688/54000 (94%)] Loss: -223577.000000\n",
      "Train Epoch: 466 [52096/54000 (96%)] Loss: -232318.500000\n",
      "    epoch          : 466\n",
      "    loss           : -228838.43652811006\n",
      "    val_loss       : -230833.68609470275\n",
      "Train Epoch: 467 [0/54000 (0%)] Loss: -249536.906250\n",
      "Train Epoch: 467 [1408/54000 (3%)] Loss: -233184.562500\n",
      "Train Epoch: 467 [2816/54000 (5%)] Loss: -227643.203125\n",
      "Train Epoch: 467 [4224/54000 (8%)] Loss: -221091.375000\n",
      "Train Epoch: 467 [5632/54000 (10%)] Loss: -228574.218750\n",
      "Train Epoch: 467 [7040/54000 (13%)] Loss: -230401.312500\n",
      "Train Epoch: 467 [8448/54000 (16%)] Loss: -220507.187500\n",
      "Train Epoch: 467 [9856/54000 (18%)] Loss: -222747.750000\n",
      "Train Epoch: 467 [11264/54000 (21%)] Loss: -228390.578125\n",
      "Train Epoch: 467 [12672/54000 (23%)] Loss: -231211.359375\n",
      "Train Epoch: 467 [14080/54000 (26%)] Loss: -217901.046875\n",
      "Train Epoch: 467 [15488/54000 (29%)] Loss: -222771.078125\n",
      "Train Epoch: 467 [16896/54000 (31%)] Loss: -225955.328125\n",
      "Train Epoch: 467 [18304/54000 (34%)] Loss: -250917.937500\n",
      "Train Epoch: 467 [19712/54000 (37%)] Loss: -218604.625000\n",
      "Train Epoch: 467 [21120/54000 (39%)] Loss: -222774.781250\n",
      "Train Epoch: 467 [22528/54000 (42%)] Loss: -227715.718750\n",
      "Train Epoch: 467 [23936/54000 (44%)] Loss: -218200.656250\n",
      "Train Epoch: 467 [25344/54000 (47%)] Loss: -231845.656250\n",
      "Train Epoch: 467 [26752/54000 (50%)] Loss: -231060.718750\n",
      "Train Epoch: 467 [28160/54000 (52%)] Loss: -223976.578125\n",
      "Train Epoch: 467 [29568/54000 (55%)] Loss: -232115.140625\n",
      "Train Epoch: 467 [30976/54000 (57%)] Loss: -227687.062500\n",
      "Train Epoch: 467 [32384/54000 (60%)] Loss: -249483.609375\n",
      "Train Epoch: 467 [33792/54000 (63%)] Loss: -231454.906250\n",
      "Train Epoch: 467 [35200/54000 (65%)] Loss: -224274.546875\n",
      "Train Epoch: 467 [36608/54000 (68%)] Loss: -225883.953125\n",
      "Train Epoch: 467 [38016/54000 (70%)] Loss: -250087.656250\n",
      "Train Epoch: 467 [39424/54000 (73%)] Loss: -231757.093750\n",
      "Train Epoch: 467 [40832/54000 (76%)] Loss: -233617.750000\n",
      "Train Epoch: 467 [42240/54000 (78%)] Loss: -230999.890625\n",
      "Train Epoch: 467 [43648/54000 (81%)] Loss: -231671.812500\n",
      "Train Epoch: 467 [45056/54000 (83%)] Loss: -223942.515625\n",
      "Train Epoch: 467 [46464/54000 (86%)] Loss: -216445.750000\n",
      "Train Epoch: 467 [47872/54000 (89%)] Loss: -222580.734375\n",
      "Train Epoch: 467 [49280/54000 (91%)] Loss: -218692.578125\n",
      "Train Epoch: 467 [50688/54000 (94%)] Loss: -220289.578125\n",
      "Train Epoch: 467 [52096/54000 (96%)] Loss: -231954.062500\n",
      "    epoch          : 467\n",
      "    loss           : -228856.98751495214\n",
      "    val_loss       : -230626.45754334985\n",
      "Train Epoch: 468 [0/54000 (0%)] Loss: -232528.453125\n",
      "Train Epoch: 468 [1408/54000 (3%)] Loss: -249619.140625\n",
      "Train Epoch: 468 [2816/54000 (5%)] Loss: -230701.484375\n",
      "Train Epoch: 468 [4224/54000 (8%)] Loss: -229088.468750\n",
      "Train Epoch: 468 [5632/54000 (10%)] Loss: -222325.203125\n",
      "Train Epoch: 468 [7040/54000 (13%)] Loss: -222163.046875\n",
      "Train Epoch: 468 [8448/54000 (16%)] Loss: -229830.656250\n",
      "Train Epoch: 468 [9856/54000 (18%)] Loss: -227465.328125\n",
      "Train Epoch: 468 [11264/54000 (21%)] Loss: -249403.625000\n",
      "Train Epoch: 468 [12672/54000 (23%)] Loss: -221140.453125\n",
      "Train Epoch: 468 [14080/54000 (26%)] Loss: -226432.828125\n",
      "Train Epoch: 468 [15488/54000 (29%)] Loss: -225995.250000\n",
      "Train Epoch: 468 [16896/54000 (31%)] Loss: -225357.515625\n",
      "Train Epoch: 468 [18304/54000 (34%)] Loss: -226104.156250\n",
      "Train Epoch: 468 [19712/54000 (37%)] Loss: -222057.031250\n",
      "Train Epoch: 468 [21120/54000 (39%)] Loss: -249869.078125\n",
      "Train Epoch: 468 [22528/54000 (42%)] Loss: -226807.421875\n",
      "Train Epoch: 468 [23936/54000 (44%)] Loss: -220692.281250\n",
      "Train Epoch: 468 [25344/54000 (47%)] Loss: -227000.843750\n",
      "Train Epoch: 468 [26752/54000 (50%)] Loss: -232515.734375\n",
      "Train Epoch: 468 [28160/54000 (52%)] Loss: -226632.468750\n",
      "Train Epoch: 468 [29568/54000 (55%)] Loss: -250421.843750\n",
      "Train Epoch: 468 [30976/54000 (57%)] Loss: -229269.562500\n",
      "Train Epoch: 468 [32384/54000 (60%)] Loss: -226558.062500\n",
      "Train Epoch: 468 [33792/54000 (63%)] Loss: -217243.234375\n",
      "Train Epoch: 468 [35200/54000 (65%)] Loss: -221294.546875\n",
      "Train Epoch: 468 [36608/54000 (68%)] Loss: -229695.062500\n",
      "Train Epoch: 468 [38016/54000 (70%)] Loss: -225557.359375\n",
      "Train Epoch: 468 [39424/54000 (73%)] Loss: -248462.390625\n",
      "Train Epoch: 468 [40832/54000 (76%)] Loss: -222726.765625\n",
      "Train Epoch: 468 [42240/54000 (78%)] Loss: -222942.671875\n",
      "Train Epoch: 468 [43648/54000 (81%)] Loss: -232661.968750\n",
      "Train Epoch: 468 [45056/54000 (83%)] Loss: -221402.750000\n",
      "Train Epoch: 468 [46464/54000 (86%)] Loss: -223981.812500\n",
      "Train Epoch: 468 [47872/54000 (89%)] Loss: -226403.828125\n",
      "Train Epoch: 468 [49280/54000 (91%)] Loss: -230962.968750\n",
      "Train Epoch: 468 [50688/54000 (94%)] Loss: -226229.125000\n",
      "Train Epoch: 468 [52096/54000 (96%)] Loss: -229278.500000\n",
      "    epoch          : 468\n",
      "    loss           : -228728.35963666267\n",
      "    val_loss       : -230601.06190453505\n",
      "Train Epoch: 469 [0/54000 (0%)] Loss: -250309.562500\n",
      "Train Epoch: 469 [1408/54000 (3%)] Loss: -223655.015625\n",
      "Train Epoch: 469 [2816/54000 (5%)] Loss: -225087.609375\n",
      "Train Epoch: 469 [4224/54000 (8%)] Loss: -224283.218750\n",
      "Train Epoch: 469 [5632/54000 (10%)] Loss: -225037.750000\n",
      "Train Epoch: 469 [7040/54000 (13%)] Loss: -222221.265625\n",
      "Train Epoch: 469 [8448/54000 (16%)] Loss: -218156.953125\n",
      "Train Epoch: 469 [9856/54000 (18%)] Loss: -227326.906250\n",
      "Train Epoch: 469 [11264/54000 (21%)] Loss: -221872.765625\n",
      "Train Epoch: 469 [12672/54000 (23%)] Loss: -219005.656250\n",
      "Train Epoch: 469 [14080/54000 (26%)] Loss: -231924.250000\n",
      "Train Epoch: 469 [15488/54000 (29%)] Loss: -220051.046875\n",
      "Train Epoch: 469 [16896/54000 (31%)] Loss: -222841.937500\n",
      "Train Epoch: 469 [18304/54000 (34%)] Loss: -226878.250000\n",
      "Train Epoch: 469 [19712/54000 (37%)] Loss: -231896.250000\n",
      "Train Epoch: 469 [21120/54000 (39%)] Loss: -223020.406250\n",
      "Train Epoch: 469 [22528/54000 (42%)] Loss: -220664.718750\n",
      "Train Epoch: 469 [23936/54000 (44%)] Loss: -250572.406250\n",
      "Train Epoch: 469 [25344/54000 (47%)] Loss: -221165.015625\n",
      "Train Epoch: 469 [26752/54000 (50%)] Loss: -220087.515625\n",
      "Train Epoch: 469 [28160/54000 (52%)] Loss: -231401.015625\n",
      "Train Epoch: 469 [29568/54000 (55%)] Loss: -249848.093750\n",
      "Train Epoch: 469 [30976/54000 (57%)] Loss: -232944.578125\n",
      "Train Epoch: 469 [32384/54000 (60%)] Loss: -224151.500000\n",
      "Train Epoch: 469 [33792/54000 (63%)] Loss: -230748.218750\n",
      "Train Epoch: 469 [35200/54000 (65%)] Loss: -226148.187500\n",
      "Train Epoch: 469 [36608/54000 (68%)] Loss: -249446.031250\n",
      "Train Epoch: 469 [38016/54000 (70%)] Loss: -225689.750000\n",
      "Train Epoch: 469 [39424/54000 (73%)] Loss: -219569.343750\n",
      "Train Epoch: 469 [40832/54000 (76%)] Loss: -222774.203125\n",
      "Train Epoch: 469 [42240/54000 (78%)] Loss: -228362.921875\n",
      "Train Epoch: 469 [43648/54000 (81%)] Loss: -227740.203125\n",
      "Train Epoch: 469 [45056/54000 (83%)] Loss: -227913.921875\n",
      "Train Epoch: 469 [46464/54000 (86%)] Loss: -220569.781250\n",
      "Train Epoch: 469 [47872/54000 (89%)] Loss: -222804.875000\n",
      "Train Epoch: 469 [49280/54000 (91%)] Loss: -224766.968750\n",
      "Train Epoch: 469 [50688/54000 (94%)] Loss: -225155.906250\n",
      "Train Epoch: 469 [52096/54000 (96%)] Loss: -226500.218750\n",
      "    epoch          : 469\n",
      "    loss           : -228843.97252541865\n",
      "    val_loss       : -230946.85801733995\n",
      "Train Epoch: 470 [0/54000 (0%)] Loss: -225410.843750\n",
      "Train Epoch: 470 [1408/54000 (3%)] Loss: -248884.984375\n",
      "Train Epoch: 470 [2816/54000 (5%)] Loss: -231181.562500\n",
      "Train Epoch: 470 [4224/54000 (8%)] Loss: -223404.390625\n",
      "Train Epoch: 470 [5632/54000 (10%)] Loss: -221796.343750\n",
      "Train Epoch: 470 [7040/54000 (13%)] Loss: -224395.343750\n",
      "Train Epoch: 470 [8448/54000 (16%)] Loss: -218883.640625\n",
      "Train Epoch: 470 [9856/54000 (18%)] Loss: -220132.296875\n",
      "Train Epoch: 470 [11264/54000 (21%)] Loss: -221677.796875\n",
      "Train Epoch: 470 [12672/54000 (23%)] Loss: -225480.531250\n",
      "Train Epoch: 470 [14080/54000 (26%)] Loss: -223369.546875\n",
      "Train Epoch: 470 [15488/54000 (29%)] Loss: -232251.109375\n",
      "Train Epoch: 470 [16896/54000 (31%)] Loss: -229539.750000\n",
      "Train Epoch: 470 [18304/54000 (34%)] Loss: -250276.609375\n",
      "Train Epoch: 470 [19712/54000 (37%)] Loss: -222773.000000\n",
      "Train Epoch: 470 [21120/54000 (39%)] Loss: -234189.156250\n",
      "Train Epoch: 470 [22528/54000 (42%)] Loss: -228984.218750\n",
      "Train Epoch: 470 [23936/54000 (44%)] Loss: -249270.328125\n",
      "Train Epoch: 470 [25344/54000 (47%)] Loss: -224117.687500\n",
      "Train Epoch: 470 [26752/54000 (50%)] Loss: -223545.218750\n",
      "Train Epoch: 470 [28160/54000 (52%)] Loss: -225779.250000\n",
      "Train Epoch: 470 [29568/54000 (55%)] Loss: -249821.531250\n",
      "Train Epoch: 470 [30976/54000 (57%)] Loss: -225514.109375\n",
      "Train Epoch: 470 [32384/54000 (60%)] Loss: -223853.093750\n",
      "Train Epoch: 470 [33792/54000 (63%)] Loss: -230911.625000\n",
      "Train Epoch: 470 [35200/54000 (65%)] Loss: -225572.000000\n",
      "Train Epoch: 470 [36608/54000 (68%)] Loss: -220887.000000\n",
      "Train Epoch: 470 [38016/54000 (70%)] Loss: -225373.437500\n",
      "Train Epoch: 470 [39424/54000 (73%)] Loss: -223471.562500\n",
      "Train Epoch: 470 [40832/54000 (76%)] Loss: -225851.953125\n",
      "Train Epoch: 470 [42240/54000 (78%)] Loss: -229819.781250\n",
      "Train Epoch: 470 [43648/54000 (81%)] Loss: -226412.125000\n",
      "Train Epoch: 470 [45056/54000 (83%)] Loss: -225956.203125\n",
      "Train Epoch: 470 [46464/54000 (86%)] Loss: -222167.734375\n",
      "Train Epoch: 470 [47872/54000 (89%)] Loss: -230513.046875\n",
      "Train Epoch: 470 [49280/54000 (91%)] Loss: -223990.312500\n",
      "Train Epoch: 470 [50688/54000 (94%)] Loss: -249634.828125\n",
      "Train Epoch: 470 [52096/54000 (96%)] Loss: -232129.625000\n",
      "    epoch          : 470\n",
      "    loss           : -228892.6625672847\n",
      "    val_loss       : -230338.45476848324\n",
      "Train Epoch: 471 [0/54000 (0%)] Loss: -224279.375000\n",
      "Train Epoch: 471 [1408/54000 (3%)] Loss: -220483.484375\n",
      "Train Epoch: 471 [2816/54000 (5%)] Loss: -223365.078125\n",
      "Train Epoch: 471 [4224/54000 (8%)] Loss: -230632.750000\n",
      "Train Epoch: 471 [5632/54000 (10%)] Loss: -222006.156250\n",
      "Train Epoch: 471 [7040/54000 (13%)] Loss: -229077.015625\n",
      "Train Epoch: 471 [8448/54000 (16%)] Loss: -248464.078125\n",
      "Train Epoch: 471 [9856/54000 (18%)] Loss: -227992.375000\n",
      "Train Epoch: 471 [11264/54000 (21%)] Loss: -224557.171875\n",
      "Train Epoch: 471 [12672/54000 (23%)] Loss: -231279.359375\n",
      "Train Epoch: 471 [14080/54000 (26%)] Loss: -250558.000000\n",
      "Train Epoch: 471 [15488/54000 (29%)] Loss: -225094.187500\n",
      "Train Epoch: 471 [16896/54000 (31%)] Loss: -223570.500000\n",
      "Train Epoch: 471 [18304/54000 (34%)] Loss: -223425.500000\n",
      "Train Epoch: 471 [19712/54000 (37%)] Loss: -220413.171875\n",
      "Train Epoch: 471 [21120/54000 (39%)] Loss: -230927.765625\n",
      "Train Epoch: 471 [22528/54000 (42%)] Loss: -221056.359375\n",
      "Train Epoch: 471 [23936/54000 (44%)] Loss: -228687.640625\n",
      "Train Epoch: 471 [25344/54000 (47%)] Loss: -251233.968750\n",
      "Train Epoch: 471 [26752/54000 (50%)] Loss: -225663.890625\n",
      "Train Epoch: 471 [28160/54000 (52%)] Loss: -221986.671875\n",
      "Train Epoch: 471 [29568/54000 (55%)] Loss: -224631.000000\n",
      "Train Epoch: 471 [30976/54000 (57%)] Loss: -225708.718750\n",
      "Train Epoch: 471 [32384/54000 (60%)] Loss: -251157.140625\n",
      "Train Epoch: 471 [33792/54000 (63%)] Loss: -231694.187500\n",
      "Train Epoch: 471 [35200/54000 (65%)] Loss: -232593.343750\n",
      "Train Epoch: 471 [36608/54000 (68%)] Loss: -231611.812500\n",
      "Train Epoch: 471 [38016/54000 (70%)] Loss: -225298.375000\n",
      "Train Epoch: 471 [39424/54000 (73%)] Loss: -223355.828125\n",
      "Train Epoch: 471 [40832/54000 (76%)] Loss: -230442.937500\n",
      "Train Epoch: 471 [42240/54000 (78%)] Loss: -223556.109375\n",
      "Train Epoch: 471 [43648/54000 (81%)] Loss: -226805.125000\n",
      "Train Epoch: 471 [45056/54000 (83%)] Loss: -250080.281250\n",
      "Train Epoch: 471 [46464/54000 (86%)] Loss: -227174.625000\n",
      "Train Epoch: 471 [47872/54000 (89%)] Loss: -227263.312500\n",
      "Train Epoch: 471 [49280/54000 (91%)] Loss: -226862.984375\n",
      "Train Epoch: 471 [50688/54000 (94%)] Loss: -248665.968750\n",
      "Train Epoch: 471 [52096/54000 (96%)] Loss: -231815.187500\n",
      "    epoch          : 471\n",
      "    loss           : -228711.4995514354\n",
      "    val_loss       : -230794.9149556974\n",
      "Train Epoch: 472 [0/54000 (0%)] Loss: -227173.343750\n",
      "Train Epoch: 472 [1408/54000 (3%)] Loss: -224986.968750\n",
      "Train Epoch: 472 [2816/54000 (5%)] Loss: -229495.484375\n",
      "Train Epoch: 472 [4224/54000 (8%)] Loss: -231873.531250\n",
      "Train Epoch: 472 [5632/54000 (10%)] Loss: -225491.359375\n",
      "Train Epoch: 472 [7040/54000 (13%)] Loss: -231628.187500\n",
      "Train Epoch: 472 [8448/54000 (16%)] Loss: -228738.875000\n",
      "Train Epoch: 472 [9856/54000 (18%)] Loss: -227719.171875\n",
      "Train Epoch: 472 [11264/54000 (21%)] Loss: -226357.640625\n",
      "Train Epoch: 472 [12672/54000 (23%)] Loss: -228547.953125\n",
      "Train Epoch: 472 [14080/54000 (26%)] Loss: -232102.218750\n",
      "Train Epoch: 472 [15488/54000 (29%)] Loss: -250242.875000\n",
      "Train Epoch: 472 [16896/54000 (31%)] Loss: -227510.250000\n",
      "Train Epoch: 472 [18304/54000 (34%)] Loss: -223979.062500\n",
      "Train Epoch: 472 [19712/54000 (37%)] Loss: -223467.125000\n",
      "Train Epoch: 472 [21120/54000 (39%)] Loss: -230781.609375\n",
      "Train Epoch: 472 [22528/54000 (42%)] Loss: -224726.453125\n",
      "Train Epoch: 472 [23936/54000 (44%)] Loss: -225353.968750\n",
      "Train Epoch: 472 [25344/54000 (47%)] Loss: -230863.843750\n",
      "Train Epoch: 472 [26752/54000 (50%)] Loss: -232194.546875\n",
      "Train Epoch: 472 [28160/54000 (52%)] Loss: -221902.343750\n",
      "Train Epoch: 472 [29568/54000 (55%)] Loss: -231699.734375\n",
      "Train Epoch: 472 [30976/54000 (57%)] Loss: -231443.140625\n",
      "Train Epoch: 472 [32384/54000 (60%)] Loss: -225995.562500\n",
      "Train Epoch: 472 [33792/54000 (63%)] Loss: -250196.734375\n",
      "Train Epoch: 472 [35200/54000 (65%)] Loss: -225851.859375\n",
      "Train Epoch: 472 [36608/54000 (68%)] Loss: -220568.031250\n",
      "Train Epoch: 472 [38016/54000 (70%)] Loss: -232760.109375\n",
      "Train Epoch: 472 [39424/54000 (73%)] Loss: -219606.109375\n",
      "Train Epoch: 472 [40832/54000 (76%)] Loss: -233312.031250\n",
      "Train Epoch: 472 [42240/54000 (78%)] Loss: -224846.687500\n",
      "Train Epoch: 472 [43648/54000 (81%)] Loss: -224256.812500\n",
      "Train Epoch: 472 [45056/54000 (83%)] Loss: -219520.328125\n",
      "Train Epoch: 472 [46464/54000 (86%)] Loss: -249513.875000\n",
      "Train Epoch: 472 [47872/54000 (89%)] Loss: -230439.359375\n",
      "Train Epoch: 472 [49280/54000 (91%)] Loss: -229130.593750\n",
      "Train Epoch: 472 [50688/54000 (94%)] Loss: -232782.937500\n",
      "Train Epoch: 472 [52096/54000 (96%)] Loss: -232217.843750\n",
      "    epoch          : 472\n",
      "    loss           : -228865.9342479067\n",
      "    val_loss       : -230873.31320264863\n",
      "Train Epoch: 473 [0/54000 (0%)] Loss: -229370.937500\n",
      "Train Epoch: 473 [1408/54000 (3%)] Loss: -227985.312500\n",
      "Train Epoch: 473 [2816/54000 (5%)] Loss: -221370.578125\n",
      "Train Epoch: 473 [4224/54000 (8%)] Loss: -230947.890625\n",
      "Train Epoch: 473 [5632/54000 (10%)] Loss: -225097.515625\n",
      "Train Epoch: 473 [7040/54000 (13%)] Loss: -223317.343750\n",
      "Train Epoch: 473 [8448/54000 (16%)] Loss: -223861.671875\n",
      "Train Epoch: 473 [9856/54000 (18%)] Loss: -220844.765625\n",
      "Train Epoch: 473 [11264/54000 (21%)] Loss: -217258.796875\n",
      "Train Epoch: 473 [12672/54000 (23%)] Loss: -224813.593750\n",
      "Train Epoch: 473 [14080/54000 (26%)] Loss: -224889.187500\n",
      "Train Epoch: 473 [15488/54000 (29%)] Loss: -231468.687500\n",
      "Train Epoch: 473 [16896/54000 (31%)] Loss: -223915.187500\n",
      "Train Epoch: 473 [18304/54000 (34%)] Loss: -226650.093750\n",
      "Train Epoch: 473 [19712/54000 (37%)] Loss: -226761.171875\n",
      "Train Epoch: 473 [21120/54000 (39%)] Loss: -228288.687500\n",
      "Train Epoch: 473 [22528/54000 (42%)] Loss: -227029.921875\n",
      "Train Epoch: 473 [23936/54000 (44%)] Loss: -227123.140625\n",
      "Train Epoch: 473 [25344/54000 (47%)] Loss: -249420.453125\n",
      "Train Epoch: 473 [26752/54000 (50%)] Loss: -228676.281250\n",
      "Train Epoch: 473 [28160/54000 (52%)] Loss: -228419.593750\n",
      "Train Epoch: 473 [29568/54000 (55%)] Loss: -223201.343750\n",
      "Train Epoch: 473 [30976/54000 (57%)] Loss: -250065.687500\n",
      "Train Epoch: 473 [32384/54000 (60%)] Loss: -250331.296875\n",
      "Train Epoch: 473 [33792/54000 (63%)] Loss: -220381.750000\n",
      "Train Epoch: 473 [35200/54000 (65%)] Loss: -227439.500000\n",
      "Train Epoch: 473 [36608/54000 (68%)] Loss: -231481.875000\n",
      "Train Epoch: 473 [38016/54000 (70%)] Loss: -220165.343750\n",
      "Train Epoch: 473 [39424/54000 (73%)] Loss: -226962.343750\n",
      "Train Epoch: 473 [40832/54000 (76%)] Loss: -228563.937500\n",
      "Train Epoch: 473 [42240/54000 (78%)] Loss: -226995.578125\n",
      "Train Epoch: 473 [43648/54000 (81%)] Loss: -226755.437500\n",
      "Train Epoch: 473 [45056/54000 (83%)] Loss: -223159.406250\n",
      "Train Epoch: 473 [46464/54000 (86%)] Loss: -225306.671875\n",
      "Train Epoch: 473 [47872/54000 (89%)] Loss: -223863.625000\n",
      "Train Epoch: 473 [49280/54000 (91%)] Loss: -222195.500000\n",
      "Train Epoch: 473 [50688/54000 (94%)] Loss: -250017.734375\n",
      "Train Epoch: 473 [52096/54000 (96%)] Loss: -220620.093750\n",
      "    epoch          : 473\n",
      "    loss           : -228870.00639204544\n",
      "    val_loss       : -230634.80496141387\n",
      "Train Epoch: 474 [0/54000 (0%)] Loss: -231477.906250\n",
      "Train Epoch: 474 [1408/54000 (3%)] Loss: -228986.906250\n",
      "Train Epoch: 474 [2816/54000 (5%)] Loss: -231664.000000\n",
      "Train Epoch: 474 [4224/54000 (8%)] Loss: -231733.546875\n",
      "Train Epoch: 474 [5632/54000 (10%)] Loss: -229847.656250\n",
      "Train Epoch: 474 [7040/54000 (13%)] Loss: -229326.140625\n",
      "Train Epoch: 474 [8448/54000 (16%)] Loss: -220065.406250\n",
      "Train Epoch: 474 [9856/54000 (18%)] Loss: -219840.250000\n",
      "Train Epoch: 474 [11264/54000 (21%)] Loss: -221998.953125\n",
      "Train Epoch: 474 [12672/54000 (23%)] Loss: -220671.281250\n",
      "Train Epoch: 474 [14080/54000 (26%)] Loss: -222545.375000\n",
      "Train Epoch: 474 [15488/54000 (29%)] Loss: -232980.562500\n",
      "Train Epoch: 474 [16896/54000 (31%)] Loss: -219085.218750\n",
      "Train Epoch: 474 [18304/54000 (34%)] Loss: -219429.984375\n",
      "Train Epoch: 474 [19712/54000 (37%)] Loss: -221764.703125\n",
      "Train Epoch: 474 [21120/54000 (39%)] Loss: -230151.296875\n",
      "Train Epoch: 474 [22528/54000 (42%)] Loss: -230882.734375\n",
      "Train Epoch: 474 [23936/54000 (44%)] Loss: -226747.125000\n",
      "Train Epoch: 474 [25344/54000 (47%)] Loss: -225446.593750\n",
      "Train Epoch: 474 [26752/54000 (50%)] Loss: -226039.328125\n",
      "Train Epoch: 474 [28160/54000 (52%)] Loss: -249554.968750\n",
      "Train Epoch: 474 [29568/54000 (55%)] Loss: -222008.656250\n",
      "Train Epoch: 474 [30976/54000 (57%)] Loss: -231895.687500\n",
      "Train Epoch: 474 [32384/54000 (60%)] Loss: -221950.546875\n",
      "Train Epoch: 474 [33792/54000 (63%)] Loss: -249884.468750\n",
      "Train Epoch: 474 [35200/54000 (65%)] Loss: -222107.640625\n",
      "Train Epoch: 474 [36608/54000 (68%)] Loss: -232035.078125\n",
      "Train Epoch: 474 [38016/54000 (70%)] Loss: -232257.312500\n",
      "Train Epoch: 474 [39424/54000 (73%)] Loss: -225643.796875\n",
      "Train Epoch: 474 [40832/54000 (76%)] Loss: -250743.125000\n",
      "Train Epoch: 474 [42240/54000 (78%)] Loss: -228776.984375\n",
      "Train Epoch: 474 [43648/54000 (81%)] Loss: -220811.953125\n",
      "Train Epoch: 474 [45056/54000 (83%)] Loss: -250713.078125\n",
      "Train Epoch: 474 [46464/54000 (86%)] Loss: -220064.343750\n",
      "Train Epoch: 474 [47872/54000 (89%)] Loss: -227220.125000\n",
      "Train Epoch: 474 [49280/54000 (91%)] Loss: -223945.687500\n",
      "Train Epoch: 474 [50688/54000 (94%)] Loss: -226625.859375\n",
      "Train Epoch: 474 [52096/54000 (96%)] Loss: -221996.656250\n",
      "    epoch          : 474\n",
      "    loss           : -228819.03259569377\n",
      "    val_loss       : -230548.36783060213\n",
      "Train Epoch: 475 [0/54000 (0%)] Loss: -249587.500000\n",
      "Train Epoch: 475 [1408/54000 (3%)] Loss: -248826.843750\n",
      "Train Epoch: 475 [2816/54000 (5%)] Loss: -232108.843750\n",
      "Train Epoch: 475 [4224/54000 (8%)] Loss: -232130.265625\n",
      "Train Epoch: 475 [5632/54000 (10%)] Loss: -224829.515625\n",
      "Train Epoch: 475 [7040/54000 (13%)] Loss: -224192.328125\n",
      "Train Epoch: 475 [8448/54000 (16%)] Loss: -225844.781250\n",
      "Train Epoch: 475 [9856/54000 (18%)] Loss: -226384.781250\n",
      "Train Epoch: 475 [11264/54000 (21%)] Loss: -227858.859375\n",
      "Train Epoch: 475 [12672/54000 (23%)] Loss: -229937.375000\n",
      "Train Epoch: 475 [14080/54000 (26%)] Loss: -251018.562500\n",
      "Train Epoch: 475 [15488/54000 (29%)] Loss: -218436.515625\n",
      "Train Epoch: 475 [16896/54000 (31%)] Loss: -231295.765625\n",
      "Train Epoch: 475 [18304/54000 (34%)] Loss: -226809.921875\n",
      "Train Epoch: 475 [19712/54000 (37%)] Loss: -220592.781250\n",
      "Train Epoch: 475 [21120/54000 (39%)] Loss: -222298.484375\n",
      "Train Epoch: 475 [22528/54000 (42%)] Loss: -233803.375000\n",
      "Train Epoch: 475 [23936/54000 (44%)] Loss: -221580.046875\n",
      "Train Epoch: 475 [25344/54000 (47%)] Loss: -223448.375000\n",
      "Train Epoch: 475 [26752/54000 (50%)] Loss: -250321.625000\n",
      "Train Epoch: 475 [28160/54000 (52%)] Loss: -222872.687500\n",
      "Train Epoch: 475 [29568/54000 (55%)] Loss: -222995.437500\n",
      "Train Epoch: 475 [30976/54000 (57%)] Loss: -249972.546875\n",
      "Train Epoch: 475 [32384/54000 (60%)] Loss: -218468.984375\n",
      "Train Epoch: 475 [33792/54000 (63%)] Loss: -232863.156250\n",
      "Train Epoch: 475 [35200/54000 (65%)] Loss: -225585.203125\n",
      "Train Epoch: 475 [36608/54000 (68%)] Loss: -224497.093750\n",
      "Train Epoch: 475 [38016/54000 (70%)] Loss: -224410.703125\n",
      "Train Epoch: 475 [39424/54000 (73%)] Loss: -232539.562500\n",
      "Train Epoch: 475 [40832/54000 (76%)] Loss: -233331.484375\n",
      "Train Epoch: 475 [42240/54000 (78%)] Loss: -230491.312500\n",
      "Train Epoch: 475 [43648/54000 (81%)] Loss: -221591.796875\n",
      "Train Epoch: 475 [45056/54000 (83%)] Loss: -231008.125000\n",
      "Train Epoch: 475 [46464/54000 (86%)] Loss: -222445.937500\n",
      "Train Epoch: 475 [47872/54000 (89%)] Loss: -223880.078125\n",
      "Train Epoch: 475 [49280/54000 (91%)] Loss: -222095.406250\n",
      "Train Epoch: 475 [50688/54000 (94%)] Loss: -251198.750000\n",
      "Train Epoch: 475 [52096/54000 (96%)] Loss: -222141.437500\n",
      "    epoch          : 475\n",
      "    loss           : -228901.01816686604\n",
      "    val_loss       : -230604.25479349276\n",
      "Train Epoch: 476 [0/54000 (0%)] Loss: -225270.375000\n",
      "Train Epoch: 476 [1408/54000 (3%)] Loss: -219455.359375\n",
      "Train Epoch: 476 [2816/54000 (5%)] Loss: -219240.187500\n",
      "Train Epoch: 476 [4224/54000 (8%)] Loss: -233483.968750\n",
      "Train Epoch: 476 [5632/54000 (10%)] Loss: -226137.421875\n",
      "Train Epoch: 476 [7040/54000 (13%)] Loss: -224346.593750\n",
      "Train Epoch: 476 [8448/54000 (16%)] Loss: -226617.890625\n",
      "Train Epoch: 476 [9856/54000 (18%)] Loss: -223755.468750\n",
      "Train Epoch: 476 [11264/54000 (21%)] Loss: -224075.828125\n",
      "Train Epoch: 476 [12672/54000 (23%)] Loss: -227713.015625\n",
      "Train Epoch: 476 [14080/54000 (26%)] Loss: -248119.468750\n",
      "Train Epoch: 476 [15488/54000 (29%)] Loss: -229073.625000\n",
      "Train Epoch: 476 [16896/54000 (31%)] Loss: -227344.000000\n",
      "Train Epoch: 476 [18304/54000 (34%)] Loss: -223087.375000\n",
      "Train Epoch: 476 [19712/54000 (37%)] Loss: -223092.296875\n",
      "Train Epoch: 476 [21120/54000 (39%)] Loss: -223935.187500\n",
      "Train Epoch: 476 [22528/54000 (42%)] Loss: -230932.031250\n",
      "Train Epoch: 476 [23936/54000 (44%)] Loss: -222900.328125\n",
      "Train Epoch: 476 [25344/54000 (47%)] Loss: -223565.093750\n",
      "Train Epoch: 476 [26752/54000 (50%)] Loss: -217568.062500\n",
      "Train Epoch: 476 [28160/54000 (52%)] Loss: -231462.203125\n",
      "Train Epoch: 476 [29568/54000 (55%)] Loss: -231984.703125\n",
      "Train Epoch: 476 [30976/54000 (57%)] Loss: -231703.312500\n",
      "Train Epoch: 476 [32384/54000 (60%)] Loss: -248614.671875\n",
      "Train Epoch: 476 [33792/54000 (63%)] Loss: -222395.593750\n",
      "Train Epoch: 476 [35200/54000 (65%)] Loss: -223863.937500\n",
      "Train Epoch: 476 [36608/54000 (68%)] Loss: -224050.843750\n",
      "Train Epoch: 476 [38016/54000 (70%)] Loss: -251487.656250\n",
      "Train Epoch: 476 [39424/54000 (73%)] Loss: -229744.609375\n",
      "Train Epoch: 476 [40832/54000 (76%)] Loss: -233423.187500\n",
      "Train Epoch: 476 [42240/54000 (78%)] Loss: -221966.984375\n",
      "Train Epoch: 476 [43648/54000 (81%)] Loss: -224791.781250\n",
      "Train Epoch: 476 [45056/54000 (83%)] Loss: -225211.765625\n",
      "Train Epoch: 476 [46464/54000 (86%)] Loss: -226908.343750\n",
      "Train Epoch: 476 [47872/54000 (89%)] Loss: -224511.515625\n",
      "Train Epoch: 476 [49280/54000 (91%)] Loss: -223607.500000\n",
      "Train Epoch: 476 [50688/54000 (94%)] Loss: -222188.281250\n",
      "Train Epoch: 476 [52096/54000 (96%)] Loss: -221386.343750\n",
      "    epoch          : 476\n",
      "    loss           : -228835.93563098085\n",
      "    val_loss       : -230595.17119616998\n",
      "Train Epoch: 477 [0/54000 (0%)] Loss: -225650.000000\n",
      "Train Epoch: 477 [1408/54000 (3%)] Loss: -231351.906250\n",
      "Train Epoch: 477 [2816/54000 (5%)] Loss: -224333.546875\n",
      "Train Epoch: 477 [4224/54000 (8%)] Loss: -223957.140625\n",
      "Train Epoch: 477 [5632/54000 (10%)] Loss: -229267.062500\n",
      "Train Epoch: 477 [7040/54000 (13%)] Loss: -231611.484375\n",
      "Train Epoch: 477 [8448/54000 (16%)] Loss: -222805.171875\n",
      "Train Epoch: 477 [9856/54000 (18%)] Loss: -226048.062500\n",
      "Train Epoch: 477 [11264/54000 (21%)] Loss: -220279.015625\n",
      "Train Epoch: 477 [12672/54000 (23%)] Loss: -221247.343750\n",
      "Train Epoch: 477 [14080/54000 (26%)] Loss: -223346.515625\n",
      "Train Epoch: 477 [15488/54000 (29%)] Loss: -223302.109375\n",
      "Train Epoch: 477 [16896/54000 (31%)] Loss: -249700.828125\n",
      "Train Epoch: 477 [18304/54000 (34%)] Loss: -232649.796875\n",
      "Train Epoch: 477 [19712/54000 (37%)] Loss: -232896.703125\n",
      "Train Epoch: 477 [21120/54000 (39%)] Loss: -230826.296875\n",
      "Train Epoch: 477 [22528/54000 (42%)] Loss: -231489.250000\n",
      "Train Epoch: 477 [23936/54000 (44%)] Loss: -230016.093750\n",
      "Train Epoch: 477 [25344/54000 (47%)] Loss: -228148.265625\n",
      "Train Epoch: 477 [26752/54000 (50%)] Loss: -223082.890625\n",
      "Train Epoch: 477 [28160/54000 (52%)] Loss: -227012.687500\n",
      "Train Epoch: 477 [29568/54000 (55%)] Loss: -222874.562500\n",
      "Train Epoch: 477 [30976/54000 (57%)] Loss: -232492.203125\n",
      "Train Epoch: 477 [32384/54000 (60%)] Loss: -219826.437500\n",
      "Train Epoch: 477 [33792/54000 (63%)] Loss: -232757.468750\n",
      "Train Epoch: 477 [35200/54000 (65%)] Loss: -218322.703125\n",
      "Train Epoch: 477 [36608/54000 (68%)] Loss: -219856.265625\n",
      "Train Epoch: 477 [38016/54000 (70%)] Loss: -247774.031250\n",
      "Train Epoch: 477 [39424/54000 (73%)] Loss: -248998.828125\n",
      "Train Epoch: 477 [40832/54000 (76%)] Loss: -225004.609375\n",
      "Train Epoch: 477 [42240/54000 (78%)] Loss: -229264.625000\n",
      "Train Epoch: 477 [43648/54000 (81%)] Loss: -221780.187500\n",
      "Train Epoch: 477 [45056/54000 (83%)] Loss: -228369.421875\n",
      "Train Epoch: 477 [46464/54000 (86%)] Loss: -229562.796875\n",
      "Train Epoch: 477 [47872/54000 (89%)] Loss: -221995.093750\n",
      "Train Epoch: 477 [49280/54000 (91%)] Loss: -221954.750000\n",
      "Train Epoch: 477 [50688/54000 (94%)] Loss: -225477.468750\n",
      "Train Epoch: 477 [52096/54000 (96%)] Loss: -249891.328125\n",
      "    epoch          : 477\n",
      "    loss           : -228935.20282595695\n",
      "    val_loss       : -230520.49179449314\n",
      "Train Epoch: 478 [0/54000 (0%)] Loss: -249552.968750\n",
      "Train Epoch: 478 [1408/54000 (3%)] Loss: -232325.406250\n",
      "Train Epoch: 478 [2816/54000 (5%)] Loss: -225376.203125\n",
      "Train Epoch: 478 [4224/54000 (8%)] Loss: -228966.062500\n",
      "Train Epoch: 478 [5632/54000 (10%)] Loss: -228785.984375\n",
      "Train Epoch: 478 [7040/54000 (13%)] Loss: -219380.265625\n",
      "Train Epoch: 478 [8448/54000 (16%)] Loss: -222377.156250\n",
      "Train Epoch: 478 [9856/54000 (18%)] Loss: -222065.500000\n",
      "Train Epoch: 478 [11264/54000 (21%)] Loss: -228718.593750\n",
      "Train Epoch: 478 [12672/54000 (23%)] Loss: -232002.468750\n",
      "Train Epoch: 478 [14080/54000 (26%)] Loss: -220367.859375\n",
      "Train Epoch: 478 [15488/54000 (29%)] Loss: -224453.828125\n",
      "Train Epoch: 478 [16896/54000 (31%)] Loss: -225419.093750\n",
      "Train Epoch: 478 [18304/54000 (34%)] Loss: -224123.546875\n",
      "Train Epoch: 478 [19712/54000 (37%)] Loss: -250457.734375\n",
      "Train Epoch: 478 [21120/54000 (39%)] Loss: -233904.359375\n",
      "Train Epoch: 478 [22528/54000 (42%)] Loss: -232215.812500\n",
      "Train Epoch: 478 [23936/54000 (44%)] Loss: -230288.015625\n",
      "Train Epoch: 478 [25344/54000 (47%)] Loss: -228541.093750\n",
      "Train Epoch: 478 [26752/54000 (50%)] Loss: -222433.406250\n",
      "Train Epoch: 478 [28160/54000 (52%)] Loss: -229868.781250\n",
      "Train Epoch: 478 [29568/54000 (55%)] Loss: -228193.000000\n",
      "Train Epoch: 478 [30976/54000 (57%)] Loss: -226039.953125\n",
      "Train Epoch: 478 [32384/54000 (60%)] Loss: -226128.968750\n",
      "Train Epoch: 478 [33792/54000 (63%)] Loss: -248437.421875\n",
      "Train Epoch: 478 [35200/54000 (65%)] Loss: -224615.000000\n",
      "Train Epoch: 478 [36608/54000 (68%)] Loss: -218563.109375\n",
      "Train Epoch: 478 [38016/54000 (70%)] Loss: -232774.000000\n",
      "Train Epoch: 478 [39424/54000 (73%)] Loss: -232723.078125\n",
      "Train Epoch: 478 [40832/54000 (76%)] Loss: -224213.796875\n",
      "Train Epoch: 478 [42240/54000 (78%)] Loss: -222028.875000\n",
      "Train Epoch: 478 [43648/54000 (81%)] Loss: -219225.296875\n",
      "Train Epoch: 478 [45056/54000 (83%)] Loss: -249013.187500\n",
      "Train Epoch: 478 [46464/54000 (86%)] Loss: -228014.781250\n",
      "Train Epoch: 478 [47872/54000 (89%)] Loss: -225245.468750\n",
      "Train Epoch: 478 [49280/54000 (91%)] Loss: -224546.812500\n",
      "Train Epoch: 478 [50688/54000 (94%)] Loss: -251373.359375\n",
      "Train Epoch: 478 [52096/54000 (96%)] Loss: -232477.718750\n",
      "    epoch          : 478\n",
      "    loss           : -228982.93865879186\n",
      "    val_loss       : -230411.59434546495\n",
      "Train Epoch: 479 [0/54000 (0%)] Loss: -250073.843750\n",
      "Train Epoch: 479 [1408/54000 (3%)] Loss: -249815.968750\n",
      "Train Epoch: 479 [2816/54000 (5%)] Loss: -232452.562500\n",
      "Train Epoch: 479 [4224/54000 (8%)] Loss: -233218.062500\n",
      "Train Epoch: 479 [5632/54000 (10%)] Loss: -220023.359375\n",
      "Train Epoch: 479 [7040/54000 (13%)] Loss: -223208.953125\n",
      "Train Epoch: 479 [8448/54000 (16%)] Loss: -225604.437500\n",
      "Train Epoch: 479 [9856/54000 (18%)] Loss: -218166.265625\n",
      "Train Epoch: 479 [11264/54000 (21%)] Loss: -220859.375000\n",
      "Train Epoch: 479 [12672/54000 (23%)] Loss: -222638.234375\n",
      "Train Epoch: 479 [14080/54000 (26%)] Loss: -222615.687500\n",
      "Train Epoch: 479 [15488/54000 (29%)] Loss: -228882.140625\n",
      "Train Epoch: 479 [16896/54000 (31%)] Loss: -230304.031250\n",
      "Train Epoch: 479 [18304/54000 (34%)] Loss: -231912.765625\n",
      "Train Epoch: 479 [19712/54000 (37%)] Loss: -223877.281250\n",
      "Train Epoch: 479 [21120/54000 (39%)] Loss: -232222.125000\n",
      "Train Epoch: 479 [22528/54000 (42%)] Loss: -233214.000000\n",
      "Train Epoch: 479 [23936/54000 (44%)] Loss: -222441.062500\n",
      "Train Epoch: 479 [25344/54000 (47%)] Loss: -221335.218750\n",
      "Train Epoch: 479 [26752/54000 (50%)] Loss: -227381.781250\n",
      "Train Epoch: 479 [28160/54000 (52%)] Loss: -224527.281250\n",
      "Train Epoch: 479 [29568/54000 (55%)] Loss: -229337.140625\n",
      "Train Epoch: 479 [30976/54000 (57%)] Loss: -224125.921875\n",
      "Train Epoch: 479 [32384/54000 (60%)] Loss: -226904.531250\n",
      "Train Epoch: 479 [33792/54000 (63%)] Loss: -231475.875000\n",
      "Train Epoch: 479 [35200/54000 (65%)] Loss: -231443.468750\n",
      "Train Epoch: 479 [36608/54000 (68%)] Loss: -219317.500000\n",
      "Train Epoch: 479 [38016/54000 (70%)] Loss: -251404.843750\n",
      "Train Epoch: 479 [39424/54000 (73%)] Loss: -221380.078125\n",
      "Train Epoch: 479 [40832/54000 (76%)] Loss: -232396.531250\n",
      "Train Epoch: 479 [42240/54000 (78%)] Loss: -232034.906250\n",
      "Train Epoch: 479 [43648/54000 (81%)] Loss: -229777.875000\n",
      "Train Epoch: 479 [45056/54000 (83%)] Loss: -232648.625000\n",
      "Train Epoch: 479 [46464/54000 (86%)] Loss: -224620.531250\n",
      "Train Epoch: 479 [47872/54000 (89%)] Loss: -223201.468750\n",
      "Train Epoch: 479 [49280/54000 (91%)] Loss: -225929.968750\n",
      "Train Epoch: 479 [50688/54000 (94%)] Loss: -251486.296875\n",
      "Train Epoch: 479 [52096/54000 (96%)] Loss: -231578.828125\n",
      "    epoch          : 479\n",
      "    loss           : -228914.0271381579\n",
      "    val_loss       : -230718.1523913872\n",
      "Train Epoch: 480 [0/54000 (0%)] Loss: -249821.250000\n",
      "Train Epoch: 480 [1408/54000 (3%)] Loss: -230180.906250\n",
      "Train Epoch: 480 [2816/54000 (5%)] Loss: -232262.046875\n",
      "Train Epoch: 480 [4224/54000 (8%)] Loss: -230228.875000\n",
      "Train Epoch: 480 [5632/54000 (10%)] Loss: -230590.953125\n",
      "Train Epoch: 480 [7040/54000 (13%)] Loss: -220327.078125\n",
      "Train Epoch: 480 [8448/54000 (16%)] Loss: -228214.140625\n",
      "Train Epoch: 480 [9856/54000 (18%)] Loss: -227014.968750\n",
      "Train Epoch: 480 [11264/54000 (21%)] Loss: -221117.984375\n",
      "Train Epoch: 480 [12672/54000 (23%)] Loss: -218769.546875\n",
      "Train Epoch: 480 [14080/54000 (26%)] Loss: -221200.359375\n",
      "Train Epoch: 480 [15488/54000 (29%)] Loss: -250271.265625\n",
      "Train Epoch: 480 [16896/54000 (31%)] Loss: -226028.156250\n",
      "Train Epoch: 480 [18304/54000 (34%)] Loss: -228841.265625\n",
      "Train Epoch: 480 [19712/54000 (37%)] Loss: -231742.140625\n",
      "Train Epoch: 480 [21120/54000 (39%)] Loss: -232988.406250\n",
      "Train Epoch: 480 [22528/54000 (42%)] Loss: -231982.078125\n",
      "Train Epoch: 480 [23936/54000 (44%)] Loss: -220874.750000\n",
      "Train Epoch: 480 [25344/54000 (47%)] Loss: -221590.281250\n",
      "Train Epoch: 480 [26752/54000 (50%)] Loss: -222099.281250\n",
      "Train Epoch: 480 [28160/54000 (52%)] Loss: -247870.515625\n",
      "Train Epoch: 480 [29568/54000 (55%)] Loss: -225927.968750\n",
      "Train Epoch: 480 [30976/54000 (57%)] Loss: -217850.187500\n",
      "Train Epoch: 480 [32384/54000 (60%)] Loss: -232861.593750\n",
      "Train Epoch: 480 [33792/54000 (63%)] Loss: -231616.515625\n",
      "Train Epoch: 480 [35200/54000 (65%)] Loss: -222278.531250\n",
      "Train Epoch: 480 [36608/54000 (68%)] Loss: -225874.875000\n",
      "Train Epoch: 480 [38016/54000 (70%)] Loss: -226652.765625\n",
      "Train Epoch: 480 [39424/54000 (73%)] Loss: -232351.531250\n",
      "Train Epoch: 480 [40832/54000 (76%)] Loss: -227985.609375\n",
      "Train Epoch: 480 [42240/54000 (78%)] Loss: -232683.375000\n",
      "Train Epoch: 480 [43648/54000 (81%)] Loss: -226020.843750\n",
      "Train Epoch: 480 [45056/54000 (83%)] Loss: -227677.921875\n",
      "Train Epoch: 480 [46464/54000 (86%)] Loss: -227044.234375\n",
      "Train Epoch: 480 [47872/54000 (89%)] Loss: -223540.937500\n",
      "Train Epoch: 480 [49280/54000 (91%)] Loss: -224470.609375\n",
      "Train Epoch: 480 [50688/54000 (94%)] Loss: -221191.312500\n",
      "Train Epoch: 480 [52096/54000 (96%)] Loss: -230132.750000\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch   480: reducing learning rate of group 0 to 6.2500e-06.\n",
      "    epoch          : 480\n",
      "    loss           : -228940.11460825359\n",
      "    val_loss       : -230866.3305842702\n",
      "Train Epoch: 481 [0/54000 (0%)] Loss: -226364.312500\n",
      "Train Epoch: 481 [1408/54000 (3%)] Loss: -220975.750000\n",
      "Train Epoch: 481 [2816/54000 (5%)] Loss: -243775.656250\n",
      "Train Epoch: 481 [4224/54000 (8%)] Loss: -228193.984375\n",
      "Train Epoch: 481 [5632/54000 (10%)] Loss: -222628.562500\n",
      "Train Epoch: 481 [7040/54000 (13%)] Loss: -222839.250000\n",
      "Train Epoch: 481 [8448/54000 (16%)] Loss: -223798.093750\n",
      "Train Epoch: 481 [9856/54000 (18%)] Loss: -223154.640625\n",
      "Train Epoch: 481 [11264/54000 (21%)] Loss: -220957.468750\n",
      "Train Epoch: 481 [12672/54000 (23%)] Loss: -229977.140625\n",
      "Train Epoch: 481 [14080/54000 (26%)] Loss: -222831.718750\n",
      "Train Epoch: 481 [15488/54000 (29%)] Loss: -250710.062500\n",
      "Train Epoch: 481 [16896/54000 (31%)] Loss: -223832.156250\n",
      "Train Epoch: 481 [18304/54000 (34%)] Loss: -232513.609375\n",
      "Train Epoch: 481 [19712/54000 (37%)] Loss: -232892.437500\n",
      "Train Epoch: 481 [21120/54000 (39%)] Loss: -225331.578125\n",
      "Train Epoch: 481 [22528/54000 (42%)] Loss: -222549.625000\n",
      "Train Epoch: 481 [23936/54000 (44%)] Loss: -219553.968750\n",
      "Train Epoch: 481 [25344/54000 (47%)] Loss: -219910.765625\n",
      "Train Epoch: 481 [26752/54000 (50%)] Loss: -217372.171875\n",
      "Train Epoch: 481 [28160/54000 (52%)] Loss: -219019.656250\n",
      "Train Epoch: 481 [29568/54000 (55%)] Loss: -221914.843750\n",
      "Train Epoch: 481 [30976/54000 (57%)] Loss: -230990.906250\n",
      "Train Epoch: 481 [32384/54000 (60%)] Loss: -222421.281250\n",
      "Train Epoch: 481 [33792/54000 (63%)] Loss: -249947.078125\n",
      "Train Epoch: 481 [35200/54000 (65%)] Loss: -229678.718750\n",
      "Train Epoch: 481 [36608/54000 (68%)] Loss: -232622.937500\n",
      "Train Epoch: 481 [38016/54000 (70%)] Loss: -227889.468750\n",
      "Train Epoch: 481 [39424/54000 (73%)] Loss: -248932.187500\n",
      "Train Epoch: 481 [40832/54000 (76%)] Loss: -223862.281250\n",
      "Train Epoch: 481 [42240/54000 (78%)] Loss: -223166.156250\n",
      "Train Epoch: 481 [43648/54000 (81%)] Loss: -230142.500000\n",
      "Train Epoch: 481 [45056/54000 (83%)] Loss: -250961.750000\n",
      "Train Epoch: 481 [46464/54000 (86%)] Loss: -226824.437500\n",
      "Train Epoch: 481 [47872/54000 (89%)] Loss: -225174.421875\n",
      "Train Epoch: 481 [49280/54000 (91%)] Loss: -220109.593750\n",
      "Train Epoch: 481 [50688/54000 (94%)] Loss: -251070.437500\n",
      "Train Epoch: 481 [52096/54000 (96%)] Loss: -230334.625000\n",
      "    epoch          : 481\n",
      "    loss           : -228867.34539473685\n",
      "    val_loss       : -231020.30586056592\n",
      "Train Epoch: 482 [0/54000 (0%)] Loss: -222850.906250\n",
      "Train Epoch: 482 [1408/54000 (3%)] Loss: -231087.593750\n",
      "Train Epoch: 482 [2816/54000 (5%)] Loss: -220709.265625\n",
      "Train Epoch: 482 [4224/54000 (8%)] Loss: -227767.375000\n",
      "Train Epoch: 482 [5632/54000 (10%)] Loss: -227389.968750\n",
      "Train Epoch: 482 [7040/54000 (13%)] Loss: -249336.312500\n",
      "Train Epoch: 482 [8448/54000 (16%)] Loss: -233218.375000\n",
      "Train Epoch: 482 [9856/54000 (18%)] Loss: -227744.734375\n",
      "Train Epoch: 482 [11264/54000 (21%)] Loss: -228994.515625\n",
      "Train Epoch: 482 [12672/54000 (23%)] Loss: -229187.187500\n",
      "Train Epoch: 482 [14080/54000 (26%)] Loss: -224082.234375\n",
      "Train Epoch: 482 [15488/54000 (29%)] Loss: -220640.187500\n",
      "Train Epoch: 482 [16896/54000 (31%)] Loss: -221840.468750\n",
      "Train Epoch: 482 [18304/54000 (34%)] Loss: -220266.140625\n",
      "Train Epoch: 482 [19712/54000 (37%)] Loss: -223497.078125\n",
      "Train Epoch: 482 [21120/54000 (39%)] Loss: -232363.375000\n",
      "Train Epoch: 482 [22528/54000 (42%)] Loss: -233075.562500\n",
      "Train Epoch: 482 [23936/54000 (44%)] Loss: -224092.296875\n",
      "Train Epoch: 482 [25344/54000 (47%)] Loss: -223502.968750\n",
      "Train Epoch: 482 [26752/54000 (50%)] Loss: -250254.796875\n",
      "Train Epoch: 482 [28160/54000 (52%)] Loss: -225523.718750\n",
      "Train Epoch: 482 [29568/54000 (55%)] Loss: -232484.812500\n",
      "Train Epoch: 482 [30976/54000 (57%)] Loss: -230427.437500\n",
      "Train Epoch: 482 [32384/54000 (60%)] Loss: -232531.796875\n",
      "Train Epoch: 482 [33792/54000 (63%)] Loss: -228142.203125\n",
      "Train Epoch: 482 [35200/54000 (65%)] Loss: -230716.437500\n",
      "Train Epoch: 482 [36608/54000 (68%)] Loss: -231925.312500\n",
      "Train Epoch: 482 [38016/54000 (70%)] Loss: -224955.406250\n",
      "Train Epoch: 482 [39424/54000 (73%)] Loss: -224994.765625\n",
      "Train Epoch: 482 [40832/54000 (76%)] Loss: -231961.109375\n",
      "Train Epoch: 482 [42240/54000 (78%)] Loss: -222889.812500\n",
      "Train Epoch: 482 [43648/54000 (81%)] Loss: -225713.421875\n",
      "Train Epoch: 482 [45056/54000 (83%)] Loss: -226649.687500\n",
      "Train Epoch: 482 [46464/54000 (86%)] Loss: -225437.406250\n",
      "Train Epoch: 482 [47872/54000 (89%)] Loss: -219401.921875\n",
      "Train Epoch: 482 [49280/54000 (91%)] Loss: -226556.203125\n",
      "Train Epoch: 482 [50688/54000 (94%)] Loss: -220218.218750\n",
      "Train Epoch: 482 [52096/54000 (96%)] Loss: -250777.125000\n",
      "    epoch          : 482\n",
      "    loss           : -228968.8778035287\n",
      "    val_loss       : -230512.15548780488\n",
      "Train Epoch: 483 [0/54000 (0%)] Loss: -250717.312500\n",
      "Train Epoch: 483 [1408/54000 (3%)] Loss: -222265.625000\n",
      "Train Epoch: 483 [2816/54000 (5%)] Loss: -220855.078125\n",
      "Train Epoch: 483 [4224/54000 (8%)] Loss: -231552.625000\n",
      "Train Epoch: 483 [5632/54000 (10%)] Loss: -219539.781250\n",
      "Train Epoch: 483 [7040/54000 (13%)] Loss: -231894.453125\n",
      "Train Epoch: 483 [8448/54000 (16%)] Loss: -223728.734375\n",
      "Train Epoch: 483 [9856/54000 (18%)] Loss: -224311.312500\n",
      "Train Epoch: 483 [11264/54000 (21%)] Loss: -227733.156250\n",
      "Train Epoch: 483 [12672/54000 (23%)] Loss: -231575.125000\n",
      "Train Epoch: 483 [14080/54000 (26%)] Loss: -227498.890625\n",
      "Train Epoch: 483 [15488/54000 (29%)] Loss: -218444.812500\n",
      "Train Epoch: 483 [16896/54000 (31%)] Loss: -223453.156250\n",
      "Train Epoch: 483 [18304/54000 (34%)] Loss: -232389.875000\n",
      "Train Epoch: 483 [19712/54000 (37%)] Loss: -232670.500000\n",
      "Train Epoch: 483 [21120/54000 (39%)] Loss: -231965.312500\n",
      "Train Epoch: 483 [22528/54000 (42%)] Loss: -233449.906250\n",
      "Train Epoch: 483 [23936/54000 (44%)] Loss: -232485.703125\n",
      "Train Epoch: 483 [25344/54000 (47%)] Loss: -219990.312500\n",
      "Train Epoch: 483 [26752/54000 (50%)] Loss: -232948.718750\n",
      "Train Epoch: 483 [28160/54000 (52%)] Loss: -223155.203125\n",
      "Train Epoch: 483 [29568/54000 (55%)] Loss: -251544.296875\n",
      "Train Epoch: 483 [30976/54000 (57%)] Loss: -220341.484375\n",
      "Train Epoch: 483 [32384/54000 (60%)] Loss: -220396.609375\n",
      "Train Epoch: 483 [33792/54000 (63%)] Loss: -250130.296875\n",
      "Train Epoch: 483 [35200/54000 (65%)] Loss: -232572.062500\n",
      "Train Epoch: 483 [36608/54000 (68%)] Loss: -226207.046875\n",
      "Train Epoch: 483 [38016/54000 (70%)] Loss: -231352.984375\n",
      "Train Epoch: 483 [39424/54000 (73%)] Loss: -223837.765625\n",
      "Train Epoch: 483 [40832/54000 (76%)] Loss: -248727.671875\n",
      "Train Epoch: 483 [42240/54000 (78%)] Loss: -229932.906250\n",
      "Train Epoch: 483 [43648/54000 (81%)] Loss: -226575.812500\n",
      "Train Epoch: 483 [45056/54000 (83%)] Loss: -217673.828125\n",
      "Train Epoch: 483 [46464/54000 (86%)] Loss: -217836.984375\n",
      "Train Epoch: 483 [47872/54000 (89%)] Loss: -224326.375000\n",
      "Train Epoch: 483 [49280/54000 (91%)] Loss: -225780.687500\n",
      "Train Epoch: 483 [50688/54000 (94%)] Loss: -223621.812500\n",
      "Train Epoch: 483 [52096/54000 (96%)] Loss: -231807.203125\n",
      "    epoch          : 483\n",
      "    loss           : -228863.8969422847\n",
      "    val_loss       : -230684.07165229612\n",
      "Train Epoch: 484 [0/54000 (0%)] Loss: -233634.875000\n",
      "Train Epoch: 484 [1408/54000 (3%)] Loss: -223249.640625\n",
      "Train Epoch: 484 [2816/54000 (5%)] Loss: -227059.437500\n",
      "Train Epoch: 484 [4224/54000 (8%)] Loss: -223636.031250\n",
      "Train Epoch: 484 [5632/54000 (10%)] Loss: -219685.312500\n",
      "Train Epoch: 484 [7040/54000 (13%)] Loss: -218489.656250\n",
      "Train Epoch: 484 [8448/54000 (16%)] Loss: -232247.906250\n",
      "Train Epoch: 484 [9856/54000 (18%)] Loss: -248379.187500\n",
      "Train Epoch: 484 [11264/54000 (21%)] Loss: -231883.843750\n",
      "Train Epoch: 484 [12672/54000 (23%)] Loss: -232332.875000\n",
      "Train Epoch: 484 [14080/54000 (26%)] Loss: -225388.734375\n",
      "Train Epoch: 484 [15488/54000 (29%)] Loss: -248584.656250\n",
      "Train Epoch: 484 [16896/54000 (31%)] Loss: -223859.406250\n",
      "Train Epoch: 484 [18304/54000 (34%)] Loss: -226080.531250\n",
      "Train Epoch: 484 [19712/54000 (37%)] Loss: -223670.406250\n",
      "Train Epoch: 484 [21120/54000 (39%)] Loss: -228458.937500\n",
      "Train Epoch: 484 [22528/54000 (42%)] Loss: -232573.515625\n",
      "Train Epoch: 484 [23936/54000 (44%)] Loss: -232179.515625\n",
      "Train Epoch: 484 [25344/54000 (47%)] Loss: -224297.031250\n",
      "Train Epoch: 484 [26752/54000 (50%)] Loss: -227524.843750\n",
      "Train Epoch: 484 [28160/54000 (52%)] Loss: -229614.812500\n",
      "Train Epoch: 484 [29568/54000 (55%)] Loss: -219217.546875\n",
      "Train Epoch: 484 [30976/54000 (57%)] Loss: -229911.421875\n",
      "Train Epoch: 484 [32384/54000 (60%)] Loss: -223520.000000\n",
      "Train Epoch: 484 [33792/54000 (63%)] Loss: -248256.375000\n",
      "Train Epoch: 484 [35200/54000 (65%)] Loss: -224448.968750\n",
      "Train Epoch: 484 [36608/54000 (68%)] Loss: -228301.625000\n",
      "Train Epoch: 484 [38016/54000 (70%)] Loss: -221031.234375\n",
      "Train Epoch: 484 [39424/54000 (73%)] Loss: -231641.625000\n",
      "Train Epoch: 484 [40832/54000 (76%)] Loss: -250029.031250\n",
      "Train Epoch: 484 [42240/54000 (78%)] Loss: -230378.984375\n",
      "Train Epoch: 484 [43648/54000 (81%)] Loss: -230228.125000\n",
      "Train Epoch: 484 [45056/54000 (83%)] Loss: -249230.187500\n",
      "Train Epoch: 484 [46464/54000 (86%)] Loss: -219232.296875\n",
      "Train Epoch: 484 [47872/54000 (89%)] Loss: -227152.093750\n",
      "Train Epoch: 484 [49280/54000 (91%)] Loss: -220026.578125\n",
      "Train Epoch: 484 [50688/54000 (94%)] Loss: -217475.906250\n",
      "Train Epoch: 484 [52096/54000 (96%)] Loss: -231511.468750\n",
      "    epoch          : 484\n",
      "    loss           : -228970.02915669855\n",
      "    val_loss       : -230692.62813810023\n",
      "Train Epoch: 485 [0/54000 (0%)] Loss: -234506.140625\n",
      "Train Epoch: 485 [1408/54000 (3%)] Loss: -249928.812500\n",
      "Train Epoch: 485 [2816/54000 (5%)] Loss: -227910.312500\n",
      "Train Epoch: 485 [4224/54000 (8%)] Loss: -233137.375000\n",
      "Train Epoch: 485 [5632/54000 (10%)] Loss: -220375.015625\n",
      "Train Epoch: 485 [7040/54000 (13%)] Loss: -220073.015625\n",
      "Train Epoch: 485 [8448/54000 (16%)] Loss: -228349.343750\n",
      "Train Epoch: 485 [9856/54000 (18%)] Loss: -214872.187500\n",
      "Train Epoch: 485 [11264/54000 (21%)] Loss: -222817.515625\n",
      "Train Epoch: 485 [12672/54000 (23%)] Loss: -224148.265625\n",
      "Train Epoch: 485 [14080/54000 (26%)] Loss: -222737.656250\n",
      "Train Epoch: 485 [15488/54000 (29%)] Loss: -250000.515625\n",
      "Train Epoch: 485 [16896/54000 (31%)] Loss: -228773.843750\n",
      "Train Epoch: 485 [18304/54000 (34%)] Loss: -224257.343750\n",
      "Train Epoch: 485 [19712/54000 (37%)] Loss: -231024.531250\n",
      "Train Epoch: 485 [21120/54000 (39%)] Loss: -231150.281250\n",
      "Train Epoch: 485 [22528/54000 (42%)] Loss: -205664.453125\n",
      "Train Epoch: 485 [23936/54000 (44%)] Loss: -229693.562500\n",
      "Train Epoch: 485 [25344/54000 (47%)] Loss: -224736.562500\n",
      "Train Epoch: 485 [26752/54000 (50%)] Loss: -233376.671875\n",
      "Train Epoch: 485 [28160/54000 (52%)] Loss: -232540.640625\n",
      "Train Epoch: 485 [29568/54000 (55%)] Loss: -231303.906250\n",
      "Train Epoch: 485 [30976/54000 (57%)] Loss: -223537.218750\n",
      "Train Epoch: 485 [32384/54000 (60%)] Loss: -224721.375000\n",
      "Train Epoch: 485 [33792/54000 (63%)] Loss: -251345.781250\n",
      "Train Epoch: 485 [35200/54000 (65%)] Loss: -222014.750000\n",
      "Train Epoch: 485 [36608/54000 (68%)] Loss: -231067.531250\n",
      "Train Epoch: 485 [38016/54000 (70%)] Loss: -223548.156250\n",
      "Train Epoch: 485 [39424/54000 (73%)] Loss: -229098.000000\n",
      "Train Epoch: 485 [40832/54000 (76%)] Loss: -232070.031250\n",
      "Train Epoch: 485 [42240/54000 (78%)] Loss: -224062.000000\n",
      "Train Epoch: 485 [43648/54000 (81%)] Loss: -229631.140625\n",
      "Train Epoch: 485 [45056/54000 (83%)] Loss: -230488.875000\n",
      "Train Epoch: 485 [46464/54000 (86%)] Loss: -223827.218750\n",
      "Train Epoch: 485 [47872/54000 (89%)] Loss: -222518.796875\n",
      "Train Epoch: 485 [49280/54000 (91%)] Loss: -224442.218750\n",
      "Train Epoch: 485 [50688/54000 (94%)] Loss: -249802.375000\n",
      "Train Epoch: 485 [52096/54000 (96%)] Loss: -232312.312500\n",
      "    epoch          : 485\n",
      "    loss           : -228861.1301958732\n",
      "    val_loss       : -230712.99743950076\n",
      "Train Epoch: 486 [0/54000 (0%)] Loss: -250907.031250\n",
      "Train Epoch: 486 [1408/54000 (3%)] Loss: -231899.781250\n",
      "Train Epoch: 486 [2816/54000 (5%)] Loss: -214442.750000\n",
      "Train Epoch: 486 [4224/54000 (8%)] Loss: -222762.203125\n",
      "Train Epoch: 486 [5632/54000 (10%)] Loss: -226381.687500\n",
      "Train Epoch: 486 [7040/54000 (13%)] Loss: -250711.218750\n",
      "Train Epoch: 486 [8448/54000 (16%)] Loss: -218850.171875\n",
      "Train Epoch: 486 [9856/54000 (18%)] Loss: -228083.312500\n",
      "Train Epoch: 486 [11264/54000 (21%)] Loss: -232875.734375\n",
      "Train Epoch: 486 [12672/54000 (23%)] Loss: -231265.031250\n",
      "Train Epoch: 486 [14080/54000 (26%)] Loss: -226113.578125\n",
      "Train Epoch: 486 [15488/54000 (29%)] Loss: -225711.875000\n",
      "Train Epoch: 486 [16896/54000 (31%)] Loss: -222578.812500\n",
      "Train Epoch: 486 [18304/54000 (34%)] Loss: -229552.625000\n",
      "Train Epoch: 486 [19712/54000 (37%)] Loss: -250386.656250\n",
      "Train Epoch: 486 [21120/54000 (39%)] Loss: -221803.640625\n",
      "Train Epoch: 486 [22528/54000 (42%)] Loss: -232775.031250\n",
      "Train Epoch: 486 [23936/54000 (44%)] Loss: -230796.171875\n",
      "Train Epoch: 486 [25344/54000 (47%)] Loss: -218326.171875\n",
      "Train Epoch: 486 [26752/54000 (50%)] Loss: -252196.093750\n",
      "Train Epoch: 486 [28160/54000 (52%)] Loss: -225236.515625\n",
      "Train Epoch: 486 [29568/54000 (55%)] Loss: -223451.796875\n",
      "Train Epoch: 486 [30976/54000 (57%)] Loss: -247499.796875\n",
      "Train Epoch: 486 [32384/54000 (60%)] Loss: -224004.906250\n",
      "Train Epoch: 486 [33792/54000 (63%)] Loss: -219724.312500\n",
      "Train Epoch: 486 [35200/54000 (65%)] Loss: -233148.281250\n",
      "Train Epoch: 486 [36608/54000 (68%)] Loss: -231511.968750\n",
      "Train Epoch: 486 [38016/54000 (70%)] Loss: -232262.046875\n",
      "Train Epoch: 486 [39424/54000 (73%)] Loss: -224774.375000\n",
      "Train Epoch: 486 [40832/54000 (76%)] Loss: -224020.703125\n",
      "Train Epoch: 486 [42240/54000 (78%)] Loss: -232131.906250\n",
      "Train Epoch: 486 [43648/54000 (81%)] Loss: -231956.812500\n",
      "Train Epoch: 486 [45056/54000 (83%)] Loss: -229487.031250\n",
      "Train Epoch: 486 [46464/54000 (86%)] Loss: -231019.156250\n",
      "Train Epoch: 486 [47872/54000 (89%)] Loss: -219817.156250\n",
      "Train Epoch: 486 [49280/54000 (91%)] Loss: -222886.203125\n",
      "Train Epoch: 486 [50688/54000 (94%)] Loss: -219516.890625\n",
      "Train Epoch: 486 [52096/54000 (96%)] Loss: -248424.796875\n",
      "    epoch          : 486\n",
      "    loss           : -228791.79455741626\n",
      "    val_loss       : -230615.95776367188\n",
      "Train Epoch: 487 [0/54000 (0%)] Loss: -248973.390625\n",
      "Train Epoch: 487 [1408/54000 (3%)] Loss: -232510.375000\n",
      "Train Epoch: 487 [2816/54000 (5%)] Loss: -222526.000000\n",
      "Train Epoch: 487 [4224/54000 (8%)] Loss: -225417.734375\n",
      "Train Epoch: 487 [5632/54000 (10%)] Loss: -233248.203125\n",
      "Train Epoch: 487 [7040/54000 (13%)] Loss: -227215.593750\n",
      "Train Epoch: 487 [8448/54000 (16%)] Loss: -229529.187500\n",
      "Train Epoch: 487 [9856/54000 (18%)] Loss: -231359.531250\n",
      "Train Epoch: 487 [11264/54000 (21%)] Loss: -229949.921875\n",
      "Train Epoch: 487 [12672/54000 (23%)] Loss: -222039.015625\n",
      "Train Epoch: 487 [14080/54000 (26%)] Loss: -228809.234375\n",
      "Train Epoch: 487 [15488/54000 (29%)] Loss: -249298.546875\n",
      "Train Epoch: 487 [16896/54000 (31%)] Loss: -231506.546875\n",
      "Train Epoch: 487 [18304/54000 (34%)] Loss: -225889.890625\n",
      "Train Epoch: 487 [19712/54000 (37%)] Loss: -227294.937500\n",
      "Train Epoch: 487 [21120/54000 (39%)] Loss: -247211.000000\n",
      "Train Epoch: 487 [22528/54000 (42%)] Loss: -231981.406250\n",
      "Train Epoch: 487 [23936/54000 (44%)] Loss: -229698.687500\n",
      "Train Epoch: 487 [25344/54000 (47%)] Loss: -221709.718750\n",
      "Train Epoch: 487 [26752/54000 (50%)] Loss: -219712.203125\n",
      "Train Epoch: 487 [28160/54000 (52%)] Loss: -226680.500000\n",
      "Train Epoch: 487 [29568/54000 (55%)] Loss: -221253.234375\n",
      "Train Epoch: 487 [30976/54000 (57%)] Loss: -227998.671875\n",
      "Train Epoch: 487 [32384/54000 (60%)] Loss: -233085.421875\n",
      "Train Epoch: 487 [33792/54000 (63%)] Loss: -249704.140625\n",
      "Train Epoch: 487 [35200/54000 (65%)] Loss: -248591.765625\n",
      "Train Epoch: 487 [36608/54000 (68%)] Loss: -232444.250000\n",
      "Train Epoch: 487 [38016/54000 (70%)] Loss: -227942.640625\n",
      "Train Epoch: 487 [39424/54000 (73%)] Loss: -249281.000000\n",
      "Train Epoch: 487 [40832/54000 (76%)] Loss: -224609.500000\n",
      "Train Epoch: 487 [42240/54000 (78%)] Loss: -231404.093750\n",
      "Train Epoch: 487 [43648/54000 (81%)] Loss: -231363.156250\n",
      "Train Epoch: 487 [45056/54000 (83%)] Loss: -230388.484375\n",
      "Train Epoch: 487 [46464/54000 (86%)] Loss: -250456.125000\n",
      "Train Epoch: 487 [47872/54000 (89%)] Loss: -218702.359375\n",
      "Train Epoch: 487 [49280/54000 (91%)] Loss: -222906.687500\n",
      "Train Epoch: 487 [50688/54000 (94%)] Loss: -218873.093750\n",
      "Train Epoch: 487 [52096/54000 (96%)] Loss: -220582.562500\n",
      "    epoch          : 487\n",
      "    loss           : -229012.67127691387\n",
      "    val_loss       : -230811.75713962462\n",
      "Train Epoch: 488 [0/54000 (0%)] Loss: -251759.671875\n",
      "Train Epoch: 488 [1408/54000 (3%)] Loss: -224175.359375\n",
      "Train Epoch: 488 [2816/54000 (5%)] Loss: -222301.328125\n",
      "Train Epoch: 488 [4224/54000 (8%)] Loss: -227583.000000\n",
      "Train Epoch: 488 [5632/54000 (10%)] Loss: -249736.546875\n",
      "Train Epoch: 488 [7040/54000 (13%)] Loss: -230017.250000\n",
      "Train Epoch: 488 [8448/54000 (16%)] Loss: -224090.671875\n",
      "Train Epoch: 488 [9856/54000 (18%)] Loss: -225819.296875\n",
      "Train Epoch: 488 [11264/54000 (21%)] Loss: -230192.515625\n",
      "Train Epoch: 488 [12672/54000 (23%)] Loss: -249990.640625\n",
      "Train Epoch: 488 [14080/54000 (26%)] Loss: -227144.781250\n",
      "Train Epoch: 488 [15488/54000 (29%)] Loss: -233372.640625\n",
      "Train Epoch: 488 [16896/54000 (31%)] Loss: -249722.828125\n",
      "Train Epoch: 488 [18304/54000 (34%)] Loss: -231654.250000\n",
      "Train Epoch: 488 [19712/54000 (37%)] Loss: -221762.250000\n",
      "Train Epoch: 488 [21120/54000 (39%)] Loss: -232237.687500\n",
      "Train Epoch: 488 [22528/54000 (42%)] Loss: -232667.093750\n",
      "Train Epoch: 488 [23936/54000 (44%)] Loss: -222833.312500\n",
      "Train Epoch: 488 [25344/54000 (47%)] Loss: -223953.937500\n",
      "Train Epoch: 488 [26752/54000 (50%)] Loss: -228284.562500\n",
      "Train Epoch: 488 [28160/54000 (52%)] Loss: -228834.125000\n",
      "Train Epoch: 488 [29568/54000 (55%)] Loss: -226578.500000\n",
      "Train Epoch: 488 [30976/54000 (57%)] Loss: -225539.609375\n",
      "Train Epoch: 488 [32384/54000 (60%)] Loss: -219995.593750\n",
      "Train Epoch: 488 [33792/54000 (63%)] Loss: -231277.046875\n",
      "Train Epoch: 488 [35200/54000 (65%)] Loss: -232240.046875\n",
      "Train Epoch: 488 [36608/54000 (68%)] Loss: -226584.406250\n",
      "Train Epoch: 488 [38016/54000 (70%)] Loss: -228024.062500\n",
      "Train Epoch: 488 [39424/54000 (73%)] Loss: -230813.093750\n",
      "Train Epoch: 488 [40832/54000 (76%)] Loss: -224738.500000\n",
      "Train Epoch: 488 [42240/54000 (78%)] Loss: -231196.203125\n",
      "Train Epoch: 488 [43648/54000 (81%)] Loss: -227711.078125\n",
      "Train Epoch: 488 [45056/54000 (83%)] Loss: -223256.484375\n",
      "Train Epoch: 488 [46464/54000 (86%)] Loss: -230258.062500\n",
      "Train Epoch: 488 [47872/54000 (89%)] Loss: -229267.703125\n",
      "Train Epoch: 488 [49280/54000 (91%)] Loss: -225494.593750\n",
      "Train Epoch: 488 [50688/54000 (94%)] Loss: -233024.140625\n",
      "Train Epoch: 488 [52096/54000 (96%)] Loss: -222238.640625\n",
      "    epoch          : 488\n",
      "    loss           : -229003.74102870814\n",
      "    val_loss       : -230385.48725109565\n",
      "Train Epoch: 489 [0/54000 (0%)] Loss: -221313.171875\n",
      "Train Epoch: 489 [1408/54000 (3%)] Loss: -221310.453125\n",
      "Train Epoch: 489 [2816/54000 (5%)] Loss: -223950.250000\n",
      "Train Epoch: 489 [4224/54000 (8%)] Loss: -231524.375000\n",
      "Train Epoch: 489 [5632/54000 (10%)] Loss: -230725.468750\n",
      "Train Epoch: 489 [7040/54000 (13%)] Loss: -226621.250000\n",
      "Train Epoch: 489 [8448/54000 (16%)] Loss: -227290.359375\n",
      "Train Epoch: 489 [9856/54000 (18%)] Loss: -232993.937500\n",
      "Train Epoch: 489 [11264/54000 (21%)] Loss: -223259.031250\n",
      "Train Epoch: 489 [12672/54000 (23%)] Loss: -221553.531250\n",
      "Train Epoch: 489 [14080/54000 (26%)] Loss: -248309.218750\n",
      "Train Epoch: 489 [15488/54000 (29%)] Loss: -220733.765625\n",
      "Train Epoch: 489 [16896/54000 (31%)] Loss: -222735.140625\n",
      "Train Epoch: 489 [18304/54000 (34%)] Loss: -227723.937500\n",
      "Train Epoch: 489 [19712/54000 (37%)] Loss: -223960.000000\n",
      "Train Epoch: 489 [21120/54000 (39%)] Loss: -228609.593750\n",
      "Train Epoch: 489 [22528/54000 (42%)] Loss: -231579.656250\n",
      "Train Epoch: 489 [23936/54000 (44%)] Loss: -233654.406250\n",
      "Train Epoch: 489 [25344/54000 (47%)] Loss: -223946.328125\n",
      "Train Epoch: 489 [26752/54000 (50%)] Loss: -221539.156250\n",
      "Train Epoch: 489 [28160/54000 (52%)] Loss: -222159.000000\n",
      "Train Epoch: 489 [29568/54000 (55%)] Loss: -232935.781250\n",
      "Train Epoch: 489 [30976/54000 (57%)] Loss: -221356.406250\n",
      "Train Epoch: 489 [32384/54000 (60%)] Loss: -248646.062500\n",
      "Train Epoch: 489 [33792/54000 (63%)] Loss: -233033.718750\n",
      "Train Epoch: 489 [35200/54000 (65%)] Loss: -232287.734375\n",
      "Train Epoch: 489 [36608/54000 (68%)] Loss: -230161.062500\n",
      "Train Epoch: 489 [38016/54000 (70%)] Loss: -230302.140625\n",
      "Train Epoch: 489 [39424/54000 (73%)] Loss: -233165.109375\n",
      "Train Epoch: 489 [40832/54000 (76%)] Loss: -249073.828125\n",
      "Train Epoch: 489 [42240/54000 (78%)] Loss: -230364.406250\n",
      "Train Epoch: 489 [43648/54000 (81%)] Loss: -223241.015625\n",
      "Train Epoch: 489 [45056/54000 (83%)] Loss: -226836.906250\n",
      "Train Epoch: 489 [46464/54000 (86%)] Loss: -249466.078125\n",
      "Train Epoch: 489 [47872/54000 (89%)] Loss: -232087.718750\n",
      "Train Epoch: 489 [49280/54000 (91%)] Loss: -219609.609375\n",
      "Train Epoch: 489 [50688/54000 (94%)] Loss: -230960.296875\n",
      "Train Epoch: 489 [52096/54000 (96%)] Loss: -232619.906250\n",
      "    epoch          : 489\n",
      "    loss           : -228832.49353319377\n",
      "    val_loss       : -230445.8936737805\n",
      "Validation performance didn't improve for 50 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
