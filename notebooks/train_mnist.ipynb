{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.5,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [256/54000 (0%)] Loss: 4914.030273\n",
      "Train Epoch: 1 [4352/54000 (8%)] Loss: -226942.593750\n",
      "Train Epoch: 1 [8448/54000 (16%)] Loss: -311377.125000\n",
      "Train Epoch: 1 [12544/54000 (23%)] Loss: -333501.687500\n",
      "Train Epoch: 1 [16640/54000 (31%)] Loss: -337481.312500\n",
      "Train Epoch: 1 [20736/54000 (38%)] Loss: -334666.625000\n",
      "Train Epoch: 1 [24832/54000 (46%)] Loss: -394226.781250\n",
      "Train Epoch: 1 [28928/54000 (54%)] Loss: -327557.781250\n",
      "Train Epoch: 1 [33024/54000 (61%)] Loss: -282443.093750\n",
      "Train Epoch: 1 [37120/54000 (69%)] Loss: -355077.937500\n",
      "Train Epoch: 1 [41216/54000 (76%)] Loss: -339570.625000\n",
      "Train Epoch: 1 [45312/54000 (84%)] Loss: -353777.218750\n",
      "Train Epoch: 1 [49408/54000 (91%)] Loss: -408224.156250\n",
      "    epoch          : 1\n",
      "    loss           : -313086.5893483232\n",
      "    val_loss       : -340989.05961914064\n",
      "Train Epoch: 2 [256/54000 (0%)] Loss: -417371.968750\n",
      "Train Epoch: 2 [4352/54000 (8%)] Loss: -315476.625000\n",
      "Train Epoch: 2 [8448/54000 (16%)] Loss: -360330.437500\n",
      "Train Epoch: 2 [12544/54000 (23%)] Loss: -261856.125000\n",
      "Train Epoch: 2 [16640/54000 (31%)] Loss: -368288.031250\n",
      "Train Epoch: 2 [20736/54000 (38%)] Loss: -330840.500000\n",
      "Train Epoch: 2 [24832/54000 (46%)] Loss: -339557.906250\n",
      "Train Epoch: 2 [28928/54000 (54%)] Loss: -425464.906250\n",
      "Train Epoch: 2 [33024/54000 (61%)] Loss: -345388.437500\n",
      "Train Epoch: 2 [37120/54000 (69%)] Loss: -361863.062500\n",
      "Train Epoch: 2 [41216/54000 (76%)] Loss: -355594.000000\n",
      "Train Epoch: 2 [45312/54000 (84%)] Loss: -354780.468750\n",
      "Train Epoch: 2 [49408/54000 (91%)] Loss: -433030.843750\n",
      "    epoch          : 2\n",
      "    loss           : -350018.0059451219\n",
      "    val_loss       : -353709.5562255859\n",
      "Train Epoch: 3 [256/54000 (0%)] Loss: -314480.031250\n",
      "Train Epoch: 3 [4352/54000 (8%)] Loss: -367238.125000\n",
      "Train Epoch: 3 [8448/54000 (16%)] Loss: -362623.531250\n",
      "Train Epoch: 3 [12544/54000 (23%)] Loss: -335113.250000\n",
      "Train Epoch: 3 [16640/54000 (31%)] Loss: -348767.812500\n",
      "Train Epoch: 3 [20736/54000 (38%)] Loss: -359711.187500\n",
      "Train Epoch: 3 [24832/54000 (46%)] Loss: -313539.562500\n",
      "Train Epoch: 3 [28928/54000 (54%)] Loss: -334828.750000\n",
      "Train Epoch: 3 [33024/54000 (61%)] Loss: -349990.750000\n",
      "Train Epoch: 3 [37120/54000 (69%)] Loss: -314528.687500\n",
      "Train Epoch: 3 [41216/54000 (76%)] Loss: -349834.531250\n",
      "Train Epoch: 3 [45312/54000 (84%)] Loss: -359790.437500\n",
      "Train Epoch: 3 [49408/54000 (91%)] Loss: -348939.718750\n",
      "    epoch          : 3\n",
      "    loss           : -354907.5332317073\n",
      "    val_loss       : -356246.63015136716\n",
      "Train Epoch: 4 [256/54000 (0%)] Loss: -320428.437500\n",
      "Train Epoch: 4 [4352/54000 (8%)] Loss: -349377.375000\n",
      "Train Epoch: 4 [8448/54000 (16%)] Loss: -320765.000000\n",
      "Train Epoch: 4 [12544/54000 (23%)] Loss: -318166.875000\n",
      "Train Epoch: 4 [16640/54000 (31%)] Loss: -359648.093750\n",
      "Train Epoch: 4 [20736/54000 (38%)] Loss: -365029.750000\n",
      "Train Epoch: 4 [24832/54000 (46%)] Loss: -354082.875000\n",
      "Train Epoch: 4 [28928/54000 (54%)] Loss: -447015.062500\n",
      "Train Epoch: 4 [33024/54000 (61%)] Loss: -320038.968750\n",
      "Train Epoch: 4 [37120/54000 (69%)] Loss: -323036.843750\n",
      "Train Epoch: 4 [41216/54000 (76%)] Loss: -364161.375000\n",
      "Train Epoch: 4 [45312/54000 (84%)] Loss: -327742.687500\n",
      "Train Epoch: 4 [49408/54000 (91%)] Loss: -348796.343750\n",
      "    epoch          : 4\n",
      "    loss           : -357878.62347560975\n",
      "    val_loss       : -362579.18752441404\n",
      "Train Epoch: 5 [256/54000 (0%)] Loss: -447430.281250\n",
      "Train Epoch: 5 [4352/54000 (8%)] Loss: -387780.718750\n",
      "Train Epoch: 5 [8448/54000 (16%)] Loss: -368925.031250\n",
      "Train Epoch: 5 [12544/54000 (23%)] Loss: -353441.593750\n",
      "Train Epoch: 5 [16640/54000 (31%)] Loss: -318544.781250\n",
      "Train Epoch: 5 [20736/54000 (38%)] Loss: -373627.343750\n",
      "Train Epoch: 5 [24832/54000 (46%)] Loss: -381287.343750\n",
      "Train Epoch: 5 [28928/54000 (54%)] Loss: -450263.750000\n",
      "Train Epoch: 5 [33024/54000 (61%)] Loss: -374785.000000\n",
      "Train Epoch: 5 [37120/54000 (69%)] Loss: -436700.625000\n",
      "Train Epoch: 5 [41216/54000 (76%)] Loss: -345117.750000\n",
      "Train Epoch: 5 [45312/54000 (84%)] Loss: -339897.312500\n",
      "Train Epoch: 5 [49408/54000 (91%)] Loss: -440105.750000\n",
      "    epoch          : 5\n",
      "    loss           : -363294.31402439025\n",
      "    val_loss       : -366132.3006591797\n",
      "Train Epoch: 6 [256/54000 (0%)] Loss: -349326.187500\n",
      "Train Epoch: 6 [4352/54000 (8%)] Loss: -395886.500000\n",
      "Train Epoch: 6 [8448/54000 (16%)] Loss: -332190.000000\n",
      "Train Epoch: 6 [12544/54000 (23%)] Loss: -375769.562500\n",
      "Train Epoch: 6 [16640/54000 (31%)] Loss: -454273.218750\n",
      "Train Epoch: 6 [20736/54000 (38%)] Loss: -375570.125000\n",
      "Train Epoch: 6 [24832/54000 (46%)] Loss: -348842.125000\n",
      "Train Epoch: 6 [28928/54000 (54%)] Loss: -343879.750000\n",
      "Train Epoch: 6 [33024/54000 (61%)] Loss: -362945.156250\n",
      "Train Epoch: 6 [37120/54000 (69%)] Loss: -447464.375000\n",
      "Train Epoch: 6 [41216/54000 (76%)] Loss: -371144.375000\n",
      "Train Epoch: 6 [45312/54000 (84%)] Loss: -389191.625000\n",
      "Train Epoch: 6 [49408/54000 (91%)] Loss: -460601.937500\n",
      "    epoch          : 6\n",
      "    loss           : -369605.29649390245\n",
      "    val_loss       : -372272.7970703125\n",
      "Train Epoch: 7 [256/54000 (0%)] Loss: -462401.562500\n",
      "Train Epoch: 7 [4352/54000 (8%)] Loss: -331938.062500\n",
      "Train Epoch: 7 [8448/54000 (16%)] Loss: -327366.406250\n",
      "Train Epoch: 7 [12544/54000 (23%)] Loss: -397062.343750\n",
      "Train Epoch: 7 [16640/54000 (31%)] Loss: -360551.625000\n",
      "Train Epoch: 7 [20736/54000 (38%)] Loss: -385152.500000\n",
      "Train Epoch: 7 [24832/54000 (46%)] Loss: -364048.937500\n",
      "Train Epoch: 7 [28928/54000 (54%)] Loss: -456947.062500\n",
      "Train Epoch: 7 [33024/54000 (61%)] Loss: -348795.250000\n",
      "Train Epoch: 7 [37120/54000 (69%)] Loss: -342584.156250\n",
      "Train Epoch: 7 [41216/54000 (76%)] Loss: -381536.187500\n",
      "Train Epoch: 7 [45312/54000 (84%)] Loss: -346396.875000\n",
      "Train Epoch: 7 [49408/54000 (91%)] Loss: -466294.500000\n",
      "    epoch          : 7\n",
      "    loss           : -376601.98125\n",
      "    val_loss       : -377306.8051025391\n",
      "Train Epoch: 8 [256/54000 (0%)] Loss: -387145.875000\n",
      "Train Epoch: 8 [4352/54000 (8%)] Loss: -339687.687500\n",
      "Train Epoch: 8 [8448/54000 (16%)] Loss: -384200.812500\n",
      "Train Epoch: 8 [12544/54000 (23%)] Loss: -363483.906250\n",
      "Train Epoch: 8 [16640/54000 (31%)] Loss: -368455.000000\n",
      "Train Epoch: 8 [20736/54000 (38%)] Loss: -368744.218750\n",
      "Train Epoch: 8 [24832/54000 (46%)] Loss: -344942.843750\n",
      "Train Epoch: 8 [28928/54000 (54%)] Loss: -465754.968750\n",
      "Train Epoch: 8 [33024/54000 (61%)] Loss: -389002.562500\n",
      "Train Epoch: 8 [37120/54000 (69%)] Loss: -349808.375000\n",
      "Train Epoch: 8 [41216/54000 (76%)] Loss: -391634.250000\n",
      "Train Epoch: 8 [45312/54000 (84%)] Loss: -371829.312500\n",
      "Train Epoch: 8 [49408/54000 (91%)] Loss: -456770.968750\n",
      "    epoch          : 8\n",
      "    loss           : -382860.6524390244\n",
      "    val_loss       : -382700.490625\n",
      "Train Epoch: 9 [256/54000 (0%)] Loss: -469032.187500\n",
      "Train Epoch: 9 [4352/54000 (8%)] Loss: -370270.125000\n",
      "Train Epoch: 9 [8448/54000 (16%)] Loss: -397368.000000\n",
      "Train Epoch: 9 [12544/54000 (23%)] Loss: -373791.875000\n",
      "Train Epoch: 9 [16640/54000 (31%)] Loss: -366763.375000\n",
      "Train Epoch: 9 [20736/54000 (38%)] Loss: -372266.375000\n",
      "Train Epoch: 9 [24832/54000 (46%)] Loss: -388828.562500\n",
      "Train Epoch: 9 [28928/54000 (54%)] Loss: -370540.843750\n",
      "Train Epoch: 9 [33024/54000 (61%)] Loss: -363151.687500\n",
      "Train Epoch: 9 [37120/54000 (69%)] Loss: -346983.531250\n",
      "Train Epoch: 9 [41216/54000 (76%)] Loss: -371847.437500\n",
      "Train Epoch: 9 [45312/54000 (84%)] Loss: -406758.437500\n",
      "Train Epoch: 9 [49408/54000 (91%)] Loss: -470172.406250\n",
      "    epoch          : 9\n",
      "    loss           : -388608.04405487806\n",
      "    val_loss       : -390827.2319580078\n",
      "Train Epoch: 10 [256/54000 (0%)] Loss: -463775.187500\n",
      "Train Epoch: 10 [4352/54000 (8%)] Loss: -377787.187500\n",
      "Train Epoch: 10 [8448/54000 (16%)] Loss: -374131.000000\n",
      "Train Epoch: 10 [12544/54000 (23%)] Loss: -389819.375000\n",
      "Train Epoch: 10 [16640/54000 (31%)] Loss: -336205.625000\n",
      "Train Epoch: 10 [20736/54000 (38%)] Loss: -377723.375000\n",
      "Train Epoch: 10 [24832/54000 (46%)] Loss: -393549.312500\n",
      "Train Epoch: 10 [28928/54000 (54%)] Loss: -366731.406250\n",
      "Train Epoch: 10 [33024/54000 (61%)] Loss: -368917.562500\n",
      "Train Epoch: 10 [37120/54000 (69%)] Loss: -345480.781250\n",
      "Train Epoch: 10 [41216/54000 (76%)] Loss: -366496.125000\n",
      "Train Epoch: 10 [45312/54000 (84%)] Loss: -354935.750000\n",
      "Train Epoch: 10 [49408/54000 (91%)] Loss: -476262.125000\n",
      "    epoch          : 10\n",
      "    loss           : -382290.8888719512\n",
      "    val_loss       : -392276.8689453125\n",
      "Train Epoch: 11 [256/54000 (0%)] Loss: -479882.875000\n",
      "Train Epoch: 11 [4352/54000 (8%)] Loss: -386907.375000\n",
      "Train Epoch: 11 [8448/54000 (16%)] Loss: -365574.437500\n",
      "Train Epoch: 11 [12544/54000 (23%)] Loss: -387642.781250\n",
      "Train Epoch: 11 [16640/54000 (31%)] Loss: -371483.906250\n",
      "Train Epoch: 11 [20736/54000 (38%)] Loss: -417154.375000\n",
      "Train Epoch: 11 [24832/54000 (46%)] Loss: -388134.406250\n",
      "Train Epoch: 11 [28928/54000 (54%)] Loss: -350701.250000\n",
      "Train Epoch: 11 [33024/54000 (61%)] Loss: -403581.937500\n",
      "Train Epoch: 11 [37120/54000 (69%)] Loss: -373029.437500\n",
      "Train Epoch: 11 [41216/54000 (76%)] Loss: -370712.500000\n",
      "Train Epoch: 11 [45312/54000 (84%)] Loss: -405539.343750\n",
      "Train Epoch: 11 [49408/54000 (91%)] Loss: -480883.187500\n",
      "    epoch          : 11\n",
      "    loss           : -395480.94390243903\n",
      "    val_loss       : -404341.06005859375\n",
      "Train Epoch: 12 [256/54000 (0%)] Loss: -391583.562500\n",
      "Train Epoch: 12 [4352/54000 (8%)] Loss: -380362.312500\n",
      "Train Epoch: 12 [8448/54000 (16%)] Loss: -424833.468750\n",
      "Train Epoch: 12 [12544/54000 (23%)] Loss: -487983.750000\n",
      "Train Epoch: 12 [16640/54000 (31%)] Loss: -485635.343750\n",
      "Train Epoch: 12 [20736/54000 (38%)] Loss: -426886.375000\n",
      "Train Epoch: 12 [24832/54000 (46%)] Loss: -418920.781250\n",
      "Train Epoch: 12 [28928/54000 (54%)] Loss: -481119.218750\n",
      "Train Epoch: 12 [33024/54000 (61%)] Loss: -393945.375000\n",
      "Train Epoch: 12 [37120/54000 (69%)] Loss: -429581.125000\n",
      "Train Epoch: 12 [41216/54000 (76%)] Loss: -381383.531250\n",
      "Train Epoch: 12 [45312/54000 (84%)] Loss: -387575.250000\n",
      "Train Epoch: 12 [49408/54000 (91%)] Loss: -482826.281250\n",
      "    epoch          : 12\n",
      "    loss           : -407247.95396341465\n",
      "    val_loss       : -411828.3203125\n",
      "Train Epoch: 13 [256/54000 (0%)] Loss: -491022.531250\n",
      "Train Epoch: 13 [4352/54000 (8%)] Loss: -410991.937500\n",
      "Train Epoch: 13 [8448/54000 (16%)] Loss: -398877.156250\n",
      "Train Epoch: 13 [12544/54000 (23%)] Loss: -412237.562500\n",
      "Train Epoch: 13 [16640/54000 (31%)] Loss: -398118.000000\n",
      "Train Epoch: 13 [20736/54000 (38%)] Loss: -385750.312500\n",
      "Train Epoch: 13 [24832/54000 (46%)] Loss: -422289.437500\n",
      "Train Epoch: 13 [28928/54000 (54%)] Loss: -389712.500000\n",
      "Train Epoch: 13 [33024/54000 (61%)] Loss: -411685.750000\n",
      "Train Epoch: 13 [37120/54000 (69%)] Loss: -430358.187500\n",
      "Train Epoch: 13 [41216/54000 (76%)] Loss: -409555.062500\n",
      "Train Epoch: 13 [45312/54000 (84%)] Loss: -383702.906250\n",
      "Train Epoch: 13 [49408/54000 (91%)] Loss: -486191.437500\n",
      "    epoch          : 13\n",
      "    loss           : -413172.8233231707\n",
      "    val_loss       : -417055.58955078124\n",
      "Train Epoch: 14 [256/54000 (0%)] Loss: -491820.281250\n",
      "Train Epoch: 14 [4352/54000 (8%)] Loss: -392412.906250\n",
      "Train Epoch: 14 [8448/54000 (16%)] Loss: -379996.093750\n",
      "Train Epoch: 14 [12544/54000 (23%)] Loss: -395283.000000\n",
      "Train Epoch: 14 [16640/54000 (31%)] Loss: -487533.281250\n",
      "Train Epoch: 14 [20736/54000 (38%)] Loss: -398291.218750\n",
      "Train Epoch: 14 [24832/54000 (46%)] Loss: -400516.875000\n",
      "Train Epoch: 14 [28928/54000 (54%)] Loss: -390795.687500\n",
      "Train Epoch: 14 [33024/54000 (61%)] Loss: -484458.187500\n",
      "Train Epoch: 14 [37120/54000 (69%)] Loss: -485849.062500\n",
      "Train Epoch: 14 [41216/54000 (76%)] Loss: -419931.437500\n",
      "Train Epoch: 14 [45312/54000 (84%)] Loss: -397080.093750\n",
      "Train Epoch: 14 [49408/54000 (91%)] Loss: -490524.312500\n",
      "    epoch          : 14\n",
      "    loss           : -414649.9301829268\n",
      "    val_loss       : -418950.4361328125\n",
      "Train Epoch: 15 [256/54000 (0%)] Loss: -407385.000000\n",
      "Train Epoch: 15 [4352/54000 (8%)] Loss: -394680.062500\n",
      "Train Epoch: 15 [8448/54000 (16%)] Loss: -435894.062500\n",
      "Train Epoch: 15 [12544/54000 (23%)] Loss: -418751.968750\n",
      "Train Epoch: 15 [16640/54000 (31%)] Loss: -411009.750000\n",
      "Train Epoch: 15 [20736/54000 (38%)] Loss: -441858.125000\n",
      "Train Epoch: 15 [24832/54000 (46%)] Loss: -408724.312500\n",
      "Train Epoch: 15 [28928/54000 (54%)] Loss: -397860.000000\n",
      "Train Epoch: 15 [33024/54000 (61%)] Loss: -393523.687500\n",
      "Train Epoch: 15 [37120/54000 (69%)] Loss: -406377.687500\n",
      "Train Epoch: 15 [41216/54000 (76%)] Loss: -403922.312500\n",
      "Train Epoch: 15 [45312/54000 (84%)] Loss: -434775.656250\n",
      "Train Epoch: 15 [49408/54000 (91%)] Loss: -494888.875000\n",
      "    epoch          : 15\n",
      "    loss           : -422506.33734756097\n",
      "    val_loss       : -422167.1610351562\n",
      "Train Epoch: 16 [256/54000 (0%)] Loss: -411358.187500\n",
      "Train Epoch: 16 [4352/54000 (8%)] Loss: -398663.281250\n",
      "Train Epoch: 16 [8448/54000 (16%)] Loss: -394473.406250\n",
      "Train Epoch: 16 [12544/54000 (23%)] Loss: -417535.750000\n",
      "Train Epoch: 16 [16640/54000 (31%)] Loss: -410747.468750\n",
      "Train Epoch: 16 [20736/54000 (38%)] Loss: -448082.687500\n",
      "Train Epoch: 16 [24832/54000 (46%)] Loss: -410514.625000\n",
      "Train Epoch: 16 [28928/54000 (54%)] Loss: -405771.250000\n",
      "Train Epoch: 16 [33024/54000 (61%)] Loss: -406477.000000\n",
      "Train Epoch: 16 [37120/54000 (69%)] Loss: -499607.812500\n",
      "Train Epoch: 16 [41216/54000 (76%)] Loss: -421286.500000\n",
      "Train Epoch: 16 [45312/54000 (84%)] Loss: -392011.687500\n",
      "Train Epoch: 16 [49408/54000 (91%)] Loss: -489479.281250\n",
      "    epoch          : 16\n",
      "    loss           : -423968.00457317074\n",
      "    val_loss       : -423103.3973388672\n",
      "Train Epoch: 17 [256/54000 (0%)] Loss: -495351.218750\n",
      "Train Epoch: 17 [4352/54000 (8%)] Loss: -402540.437500\n",
      "Train Epoch: 17 [8448/54000 (16%)] Loss: -400128.875000\n",
      "Train Epoch: 17 [12544/54000 (23%)] Loss: -387054.625000\n",
      "Train Epoch: 17 [16640/54000 (31%)] Loss: -407869.062500\n",
      "Train Epoch: 17 [20736/54000 (38%)] Loss: -431943.500000\n",
      "Train Epoch: 17 [24832/54000 (46%)] Loss: -420442.750000\n",
      "Train Epoch: 17 [28928/54000 (54%)] Loss: -403424.343750\n",
      "Train Epoch: 17 [33024/54000 (61%)] Loss: -444285.125000\n",
      "Train Epoch: 17 [37120/54000 (69%)] Loss: -395289.062500\n",
      "Train Epoch: 17 [41216/54000 (76%)] Loss: -402281.312500\n",
      "Train Epoch: 17 [45312/54000 (84%)] Loss: -410361.312500\n",
      "Train Epoch: 17 [49408/54000 (91%)] Loss: -493992.437500\n",
      "    epoch          : 17\n",
      "    loss           : -425308.14756097563\n",
      "    val_loss       : -428895.7496826172\n",
      "Train Epoch: 18 [256/54000 (0%)] Loss: -425271.406250\n",
      "Train Epoch: 18 [4352/54000 (8%)] Loss: -408795.781250\n",
      "Train Epoch: 18 [8448/54000 (16%)] Loss: -418411.375000\n",
      "Train Epoch: 18 [12544/54000 (23%)] Loss: -423704.812500\n",
      "Train Epoch: 18 [16640/54000 (31%)] Loss: -396820.906250\n",
      "Train Epoch: 18 [20736/54000 (38%)] Loss: -436471.312500\n",
      "Train Epoch: 18 [24832/54000 (46%)] Loss: -408438.125000\n",
      "Train Epoch: 18 [28928/54000 (54%)] Loss: -401161.687500\n",
      "Train Epoch: 18 [33024/54000 (61%)] Loss: -398570.312500\n",
      "Train Epoch: 18 [37120/54000 (69%)] Loss: -496521.343750\n",
      "Train Epoch: 18 [41216/54000 (76%)] Loss: -407379.250000\n",
      "Train Epoch: 18 [45312/54000 (84%)] Loss: -426236.531250\n",
      "Train Epoch: 18 [49408/54000 (91%)] Loss: -497994.312500\n",
      "    epoch          : 18\n",
      "    loss           : -424745.2493902439\n",
      "    val_loss       : -430309.584375\n",
      "Train Epoch: 19 [256/54000 (0%)] Loss: -500501.312500\n",
      "Train Epoch: 19 [4352/54000 (8%)] Loss: -417916.468750\n",
      "Train Epoch: 19 [8448/54000 (16%)] Loss: -444155.750000\n",
      "Train Epoch: 19 [12544/54000 (23%)] Loss: -421498.812500\n",
      "Train Epoch: 19 [16640/54000 (31%)] Loss: -415370.656250\n",
      "Train Epoch: 19 [20736/54000 (38%)] Loss: -407075.625000\n",
      "Train Epoch: 19 [24832/54000 (46%)] Loss: -431409.875000\n",
      "Train Epoch: 19 [28928/54000 (54%)] Loss: -417232.625000\n",
      "Train Epoch: 19 [33024/54000 (61%)] Loss: -409232.218750\n",
      "Train Epoch: 19 [37120/54000 (69%)] Loss: -441364.281250\n",
      "Train Epoch: 19 [41216/54000 (76%)] Loss: -400779.781250\n",
      "Train Epoch: 19 [45312/54000 (84%)] Loss: -432378.343750\n",
      "Train Epoch: 19 [49408/54000 (91%)] Loss: -497682.937500\n",
      "    epoch          : 19\n",
      "    loss           : -430457.7272865854\n",
      "    val_loss       : -432634.91337890626\n",
      "Train Epoch: 20 [256/54000 (0%)] Loss: -501150.562500\n",
      "Train Epoch: 20 [4352/54000 (8%)] Loss: -419198.437500\n",
      "Train Epoch: 20 [8448/54000 (16%)] Loss: -453725.375000\n",
      "Train Epoch: 20 [12544/54000 (23%)] Loss: -435224.968750\n",
      "Train Epoch: 20 [16640/54000 (31%)] Loss: -416809.000000\n",
      "Train Epoch: 20 [20736/54000 (38%)] Loss: -446419.687500\n",
      "Train Epoch: 20 [24832/54000 (46%)] Loss: -441682.625000\n",
      "Train Epoch: 20 [28928/54000 (54%)] Loss: -410815.875000\n",
      "Train Epoch: 20 [33024/54000 (61%)] Loss: -419021.500000\n",
      "Train Epoch: 20 [37120/54000 (69%)] Loss: -403819.062500\n",
      "Train Epoch: 20 [41216/54000 (76%)] Loss: -414109.093750\n",
      "Train Epoch: 20 [45312/54000 (84%)] Loss: -444456.468750\n",
      "Train Epoch: 20 [49408/54000 (91%)] Loss: -496562.125000\n",
      "    epoch          : 20\n",
      "    loss           : -431441.0945121951\n",
      "    val_loss       : -431673.65141601564\n",
      "Train Epoch: 21 [256/54000 (0%)] Loss: -504988.187500\n",
      "Train Epoch: 21 [4352/54000 (8%)] Loss: -421453.843750\n",
      "Train Epoch: 21 [8448/54000 (16%)] Loss: -415527.468750\n",
      "Train Epoch: 21 [12544/54000 (23%)] Loss: -433787.406250\n",
      "Train Epoch: 21 [16640/54000 (31%)] Loss: -403048.468750\n",
      "Train Epoch: 21 [20736/54000 (38%)] Loss: -455952.062500\n",
      "Train Epoch: 21 [24832/54000 (46%)] Loss: -411328.812500\n",
      "Train Epoch: 21 [28928/54000 (54%)] Loss: -415113.000000\n",
      "Train Epoch: 21 [33024/54000 (61%)] Loss: -410872.593750\n",
      "Train Epoch: 21 [37120/54000 (69%)] Loss: -405977.500000\n",
      "Train Epoch: 21 [41216/54000 (76%)] Loss: -436443.687500\n",
      "Train Epoch: 21 [45312/54000 (84%)] Loss: -444806.312500\n",
      "Train Epoch: 21 [49408/54000 (91%)] Loss: -434070.156250\n",
      "    epoch          : 21\n",
      "    loss           : -434417.24908536585\n",
      "    val_loss       : -433130.1053710937\n",
      "Train Epoch: 22 [256/54000 (0%)] Loss: -405653.406250\n",
      "Train Epoch: 22 [4352/54000 (8%)] Loss: -437164.187500\n",
      "Train Epoch: 22 [8448/54000 (16%)] Loss: -418806.000000\n",
      "Train Epoch: 22 [12544/54000 (23%)] Loss: -409570.906250\n",
      "Train Epoch: 22 [16640/54000 (31%)] Loss: -398292.718750\n",
      "Train Epoch: 22 [20736/54000 (38%)] Loss: -457645.937500\n",
      "Train Epoch: 22 [24832/54000 (46%)] Loss: -417115.781250\n",
      "Train Epoch: 22 [28928/54000 (54%)] Loss: -417597.562500\n",
      "Train Epoch: 22 [33024/54000 (61%)] Loss: -502807.750000\n",
      "Train Epoch: 22 [37120/54000 (69%)] Loss: -425657.625000\n",
      "Train Epoch: 22 [41216/54000 (76%)] Loss: -408152.875000\n",
      "Train Epoch: 22 [45312/54000 (84%)] Loss: -439181.187500\n",
      "Train Epoch: 22 [49408/54000 (91%)] Loss: -495899.062500\n",
      "    epoch          : 22\n",
      "    loss           : -432547.8362804878\n",
      "    val_loss       : -434817.5600341797\n",
      "Train Epoch: 23 [256/54000 (0%)] Loss: -503794.187500\n",
      "Train Epoch: 23 [4352/54000 (8%)] Loss: -403043.468750\n",
      "Train Epoch: 23 [8448/54000 (16%)] Loss: -426591.562500\n",
      "Train Epoch: 23 [12544/54000 (23%)] Loss: -501258.937500\n",
      "Train Epoch: 23 [16640/54000 (31%)] Loss: -423855.718750\n",
      "Train Epoch: 23 [20736/54000 (38%)] Loss: -451443.500000\n",
      "Train Epoch: 23 [24832/54000 (46%)] Loss: -440258.906250\n",
      "Train Epoch: 23 [28928/54000 (54%)] Loss: -413272.781250\n",
      "Train Epoch: 23 [33024/54000 (61%)] Loss: -417910.562500\n",
      "Train Epoch: 23 [37120/54000 (69%)] Loss: -501064.843750\n",
      "Train Epoch: 23 [41216/54000 (76%)] Loss: -414736.625000\n",
      "Train Epoch: 23 [45312/54000 (84%)] Loss: -421321.562500\n",
      "Train Epoch: 23 [49408/54000 (91%)] Loss: -501154.437500\n",
      "    epoch          : 23\n",
      "    loss           : -435203.4975609756\n",
      "    val_loss       : -435495.1166503906\n",
      "Train Epoch: 24 [256/54000 (0%)] Loss: -503359.718750\n",
      "Train Epoch: 24 [4352/54000 (8%)] Loss: -405088.500000\n",
      "Train Epoch: 24 [8448/54000 (16%)] Loss: -451863.281250\n",
      "Train Epoch: 24 [12544/54000 (23%)] Loss: -440870.281250\n",
      "Train Epoch: 24 [16640/54000 (31%)] Loss: -414194.125000\n",
      "Train Epoch: 24 [20736/54000 (38%)] Loss: -416610.687500\n",
      "Train Epoch: 24 [24832/54000 (46%)] Loss: -412010.125000\n",
      "Train Epoch: 24 [28928/54000 (54%)] Loss: -421047.125000\n",
      "Train Epoch: 24 [33024/54000 (61%)] Loss: -420652.687500\n",
      "Train Epoch: 24 [37120/54000 (69%)] Loss: -502493.375000\n",
      "Train Epoch: 24 [41216/54000 (76%)] Loss: -454235.687500\n",
      "Train Epoch: 24 [45312/54000 (84%)] Loss: -423983.468750\n",
      "Train Epoch: 24 [49408/54000 (91%)] Loss: -436165.500000\n",
      "    epoch          : 24\n",
      "    loss           : -437586.78079268296\n",
      "    val_loss       : -439326.68994140625\n",
      "Train Epoch: 25 [256/54000 (0%)] Loss: -503782.187500\n",
      "Train Epoch: 25 [4352/54000 (8%)] Loss: -447097.093750\n",
      "Train Epoch: 25 [8448/54000 (16%)] Loss: -420427.625000\n",
      "Train Epoch: 25 [12544/54000 (23%)] Loss: -408624.750000\n",
      "Train Epoch: 25 [16640/54000 (31%)] Loss: -427710.343750\n",
      "Train Epoch: 25 [20736/54000 (38%)] Loss: -457112.437500\n",
      "Train Epoch: 25 [24832/54000 (46%)] Loss: -433498.500000\n",
      "Train Epoch: 25 [28928/54000 (54%)] Loss: -424924.250000\n",
      "Train Epoch: 25 [33024/54000 (61%)] Loss: -446557.312500\n",
      "Train Epoch: 25 [37120/54000 (69%)] Loss: -434371.062500\n",
      "Train Epoch: 25 [41216/54000 (76%)] Loss: -411357.312500\n",
      "Train Epoch: 25 [45312/54000 (84%)] Loss: -425571.250000\n",
      "Train Epoch: 25 [49408/54000 (91%)] Loss: -501544.093750\n",
      "    epoch          : 25\n",
      "    loss           : -435983.7885670732\n",
      "    val_loss       : -437175.3584960938\n",
      "Train Epoch: 26 [256/54000 (0%)] Loss: -451030.843750\n",
      "Train Epoch: 26 [4352/54000 (8%)] Loss: -435118.062500\n",
      "Train Epoch: 26 [8448/54000 (16%)] Loss: -422006.843750\n",
      "Train Epoch: 26 [12544/54000 (23%)] Loss: -420632.000000\n",
      "Train Epoch: 26 [16640/54000 (31%)] Loss: -427582.062500\n",
      "Train Epoch: 26 [20736/54000 (38%)] Loss: -460084.875000\n",
      "Train Epoch: 26 [24832/54000 (46%)] Loss: -411967.125000\n",
      "Train Epoch: 26 [28928/54000 (54%)] Loss: -421913.156250\n",
      "Train Epoch: 26 [33024/54000 (61%)] Loss: -417160.437500\n",
      "Train Epoch: 26 [37120/54000 (69%)] Loss: -449471.312500\n",
      "Train Epoch: 26 [41216/54000 (76%)] Loss: -442985.250000\n",
      "Train Epoch: 26 [45312/54000 (84%)] Loss: -411314.656250\n",
      "Train Epoch: 26 [49408/54000 (91%)] Loss: -505851.750000\n",
      "    epoch          : 26\n",
      "    loss           : -439823.12393292686\n",
      "    val_loss       : -438669.5522705078\n",
      "Train Epoch: 27 [256/54000 (0%)] Loss: -455325.468750\n",
      "Train Epoch: 27 [4352/54000 (8%)] Loss: -408321.375000\n",
      "Train Epoch: 27 [8448/54000 (16%)] Loss: -458083.187500\n",
      "Train Epoch: 27 [12544/54000 (23%)] Loss: -414394.062500\n",
      "Train Epoch: 27 [16640/54000 (31%)] Loss: -420806.687500\n",
      "Train Epoch: 27 [20736/54000 (38%)] Loss: -420481.781250\n",
      "Train Epoch: 27 [24832/54000 (46%)] Loss: -414054.812500\n",
      "Train Epoch: 27 [28928/54000 (54%)] Loss: -421008.250000\n",
      "Train Epoch: 27 [33024/54000 (61%)] Loss: -445091.812500\n",
      "Train Epoch: 27 [37120/54000 (69%)] Loss: -413045.750000\n",
      "Train Epoch: 27 [41216/54000 (76%)] Loss: -425434.937500\n",
      "Train Epoch: 27 [45312/54000 (84%)] Loss: -461977.500000\n",
      "Train Epoch: 27 [49408/54000 (91%)] Loss: -501760.875000\n",
      "    epoch          : 27\n",
      "    loss           : -440250.87713414634\n",
      "    val_loss       : -441637.28564453125\n",
      "Train Epoch: 28 [256/54000 (0%)] Loss: -448760.218750\n",
      "Train Epoch: 28 [4352/54000 (8%)] Loss: -418550.250000\n",
      "Train Epoch: 28 [8448/54000 (16%)] Loss: -435799.937500\n",
      "Train Epoch: 28 [12544/54000 (23%)] Loss: -454938.312500\n",
      "Train Epoch: 28 [16640/54000 (31%)] Loss: -412870.656250\n",
      "Train Epoch: 28 [20736/54000 (38%)] Loss: -456195.750000\n",
      "Train Epoch: 28 [24832/54000 (46%)] Loss: -450016.593750\n",
      "Train Epoch: 28 [28928/54000 (54%)] Loss: -425610.687500\n",
      "Train Epoch: 28 [33024/54000 (61%)] Loss: -503901.375000\n",
      "Train Epoch: 28 [37120/54000 (69%)] Loss: -429436.281250\n",
      "Train Epoch: 28 [41216/54000 (76%)] Loss: -411495.093750\n",
      "Train Epoch: 28 [45312/54000 (84%)] Loss: -432627.468750\n",
      "Train Epoch: 28 [49408/54000 (91%)] Loss: -502152.531250\n",
      "    epoch          : 28\n",
      "    loss           : -440704.99466463417\n",
      "    val_loss       : -441972.6073242187\n",
      "Train Epoch: 29 [256/54000 (0%)] Loss: -506155.281250\n",
      "Train Epoch: 29 [4352/54000 (8%)] Loss: -454598.187500\n",
      "Train Epoch: 29 [8448/54000 (16%)] Loss: -459282.375000\n",
      "Train Epoch: 29 [12544/54000 (23%)] Loss: -445982.375000\n",
      "Train Epoch: 29 [16640/54000 (31%)] Loss: -507475.250000\n",
      "Train Epoch: 29 [20736/54000 (38%)] Loss: -424963.437500\n",
      "Train Epoch: 29 [24832/54000 (46%)] Loss: -419423.500000\n",
      "Train Epoch: 29 [28928/54000 (54%)] Loss: -423047.062500\n",
      "Train Epoch: 29 [33024/54000 (61%)] Loss: -420575.062500\n",
      "Train Epoch: 29 [37120/54000 (69%)] Loss: -454191.406250\n",
      "Train Epoch: 29 [41216/54000 (76%)] Loss: -420250.656250\n",
      "Train Epoch: 29 [45312/54000 (84%)] Loss: -438514.750000\n",
      "Train Epoch: 29 [49408/54000 (91%)] Loss: -505115.312500\n",
      "    epoch          : 29\n",
      "    loss           : -444493.07408536586\n",
      "    val_loss       : -445095.7586914062\n",
      "Train Epoch: 30 [256/54000 (0%)] Loss: -462186.406250\n",
      "Train Epoch: 30 [4352/54000 (8%)] Loss: -431296.500000\n",
      "Train Epoch: 30 [8448/54000 (16%)] Loss: -440397.437500\n",
      "Train Epoch: 30 [12544/54000 (23%)] Loss: -451793.687500\n",
      "Train Epoch: 30 [16640/54000 (31%)] Loss: -446386.187500\n",
      "Train Epoch: 30 [20736/54000 (38%)] Loss: -461286.062500\n",
      "Train Epoch: 30 [24832/54000 (46%)] Loss: -433790.906250\n",
      "Train Epoch: 30 [28928/54000 (54%)] Loss: -422879.437500\n",
      "Train Epoch: 30 [33024/54000 (61%)] Loss: -466233.968750\n",
      "Train Epoch: 30 [37120/54000 (69%)] Loss: -437600.781250\n",
      "Train Epoch: 30 [41216/54000 (76%)] Loss: -420648.250000\n",
      "Train Epoch: 30 [45312/54000 (84%)] Loss: -444357.281250\n",
      "Train Epoch: 30 [49408/54000 (91%)] Loss: -506289.156250\n",
      "    epoch          : 30\n",
      "    loss           : -444323.47926829266\n",
      "    val_loss       : -444733.9056640625\n",
      "Train Epoch: 31 [256/54000 (0%)] Loss: -507971.531250\n",
      "Train Epoch: 31 [4352/54000 (8%)] Loss: -441945.375000\n",
      "Train Epoch: 31 [8448/54000 (16%)] Loss: -432972.812500\n",
      "Train Epoch: 31 [12544/54000 (23%)] Loss: -421485.000000\n",
      "Train Epoch: 31 [16640/54000 (31%)] Loss: -453994.437500\n",
      "Train Epoch: 31 [20736/54000 (38%)] Loss: -462566.093750\n",
      "Train Epoch: 31 [24832/54000 (46%)] Loss: -439284.781250\n",
      "Train Epoch: 31 [28928/54000 (54%)] Loss: -506994.437500\n",
      "Train Epoch: 31 [33024/54000 (61%)] Loss: -462813.125000\n",
      "Train Epoch: 31 [37120/54000 (69%)] Loss: -455066.312500\n",
      "Train Epoch: 31 [41216/54000 (76%)] Loss: -427224.812500\n",
      "Train Epoch: 31 [45312/54000 (84%)] Loss: -439528.062500\n",
      "Train Epoch: 31 [49408/54000 (91%)] Loss: -415243.562500\n",
      "    epoch          : 31\n",
      "    loss           : -445731.8788109756\n",
      "    val_loss       : -445029.77314453124\n",
      "Train Epoch: 32 [256/54000 (0%)] Loss: -430497.937500\n",
      "Train Epoch: 32 [4352/54000 (8%)] Loss: -433565.625000\n",
      "Train Epoch: 32 [8448/54000 (16%)] Loss: -428506.000000\n",
      "Train Epoch: 32 [12544/54000 (23%)] Loss: -415947.031250\n",
      "Train Epoch: 32 [16640/54000 (31%)] Loss: -413689.562500\n",
      "Train Epoch: 32 [20736/54000 (38%)] Loss: -457650.375000\n",
      "Train Epoch: 32 [24832/54000 (46%)] Loss: -454026.125000\n",
      "Train Epoch: 32 [28928/54000 (54%)] Loss: -503872.406250\n",
      "Train Epoch: 32 [33024/54000 (61%)] Loss: -453158.000000\n",
      "Train Epoch: 32 [37120/54000 (69%)] Loss: -507172.875000\n",
      "Train Epoch: 32 [41216/54000 (76%)] Loss: -425699.875000\n",
      "Train Epoch: 32 [45312/54000 (84%)] Loss: -437679.562500\n",
      "Train Epoch: 32 [49408/54000 (91%)] Loss: -502821.187500\n",
      "    epoch          : 32\n",
      "    loss           : -444503.0403963415\n",
      "    val_loss       : -446578.36704101565\n",
      "Train Epoch: 33 [256/54000 (0%)] Loss: -508415.250000\n",
      "Train Epoch: 33 [4352/54000 (8%)] Loss: -423129.812500\n",
      "Train Epoch: 33 [8448/54000 (16%)] Loss: -463267.375000\n",
      "Train Epoch: 33 [12544/54000 (23%)] Loss: -443526.250000\n",
      "Train Epoch: 33 [16640/54000 (31%)] Loss: -506776.250000\n",
      "Train Epoch: 33 [20736/54000 (38%)] Loss: -462133.687500\n",
      "Train Epoch: 33 [24832/54000 (46%)] Loss: -454462.812500\n",
      "Train Epoch: 33 [28928/54000 (54%)] Loss: -463339.500000\n",
      "Train Epoch: 33 [33024/54000 (61%)] Loss: -447082.562500\n",
      "Train Epoch: 33 [37120/54000 (69%)] Loss: -507780.156250\n",
      "Train Epoch: 33 [41216/54000 (76%)] Loss: -439045.062500\n",
      "Train Epoch: 33 [45312/54000 (84%)] Loss: -456699.687500\n",
      "Train Epoch: 33 [49408/54000 (91%)] Loss: -431081.281250\n",
      "    epoch          : 33\n",
      "    loss           : -447996.39344512194\n",
      "    val_loss       : -445598.87646484375\n",
      "Train Epoch: 34 [256/54000 (0%)] Loss: -508453.125000\n",
      "Train Epoch: 34 [4352/54000 (8%)] Loss: -449961.125000\n",
      "Train Epoch: 34 [8448/54000 (16%)] Loss: -423796.312500\n",
      "Train Epoch: 34 [12544/54000 (23%)] Loss: -421430.062500\n",
      "Train Epoch: 34 [16640/54000 (31%)] Loss: -435998.937500\n",
      "Train Epoch: 34 [20736/54000 (38%)] Loss: -504045.718750\n",
      "Train Epoch: 34 [24832/54000 (46%)] Loss: -456364.843750\n",
      "Train Epoch: 34 [28928/54000 (54%)] Loss: -437346.187500\n",
      "Train Epoch: 34 [33024/54000 (61%)] Loss: -433678.687500\n",
      "Train Epoch: 34 [37120/54000 (69%)] Loss: -454525.375000\n",
      "Train Epoch: 34 [41216/54000 (76%)] Loss: -437010.156250\n",
      "Train Epoch: 34 [45312/54000 (84%)] Loss: -425008.937500\n",
      "Train Epoch: 34 [49408/54000 (91%)] Loss: -424562.468750\n",
      "    epoch          : 34\n",
      "    loss           : -448930.0528963415\n",
      "    val_loss       : -448473.2213378906\n",
      "Train Epoch: 35 [256/54000 (0%)] Loss: -508384.000000\n",
      "Train Epoch: 35 [4352/54000 (8%)] Loss: -452000.843750\n",
      "Train Epoch: 35 [8448/54000 (16%)] Loss: -436029.968750\n",
      "Train Epoch: 35 [12544/54000 (23%)] Loss: -452754.125000\n",
      "Train Epoch: 35 [16640/54000 (31%)] Loss: -426640.187500\n",
      "Train Epoch: 35 [20736/54000 (38%)] Loss: -467480.156250\n",
      "Train Epoch: 35 [24832/54000 (46%)] Loss: -449030.906250\n",
      "Train Epoch: 35 [28928/54000 (54%)] Loss: -429545.687500\n",
      "Train Epoch: 35 [33024/54000 (61%)] Loss: -453706.812500\n",
      "Train Epoch: 35 [37120/54000 (69%)] Loss: -456634.562500\n",
      "Train Epoch: 35 [41216/54000 (76%)] Loss: -425377.375000\n",
      "Train Epoch: 35 [45312/54000 (84%)] Loss: -450238.656250\n",
      "Train Epoch: 35 [49408/54000 (91%)] Loss: -504809.562500\n",
      "    epoch          : 35\n",
      "    loss           : -447975.2224085366\n",
      "    val_loss       : -447716.1416015625\n",
      "Train Epoch: 36 [256/54000 (0%)] Loss: -437219.125000\n",
      "Train Epoch: 36 [4352/54000 (8%)] Loss: -432010.625000\n",
      "Train Epoch: 36 [8448/54000 (16%)] Loss: -463654.093750\n",
      "Train Epoch: 36 [12544/54000 (23%)] Loss: -506526.406250\n",
      "Train Epoch: 36 [16640/54000 (31%)] Loss: -442921.750000\n",
      "Train Epoch: 36 [20736/54000 (38%)] Loss: -433094.187500\n",
      "Train Epoch: 36 [24832/54000 (46%)] Loss: -448785.781250\n",
      "Train Epoch: 36 [28928/54000 (54%)] Loss: -502813.125000\n",
      "Train Epoch: 36 [33024/54000 (61%)] Loss: -424276.250000\n",
      "Train Epoch: 36 [37120/54000 (69%)] Loss: -454340.437500\n",
      "Train Epoch: 36 [41216/54000 (76%)] Loss: -438437.437500\n",
      "Train Epoch: 36 [45312/54000 (84%)] Loss: -454957.562500\n",
      "Train Epoch: 36 [49408/54000 (91%)] Loss: -423947.375000\n",
      "    epoch          : 36\n",
      "    loss           : -449087.09847560973\n",
      "    val_loss       : -448370.39523925784\n",
      "Train Epoch: 37 [256/54000 (0%)] Loss: -507010.250000\n",
      "Train Epoch: 37 [4352/54000 (8%)] Loss: -421799.125000\n",
      "Train Epoch: 37 [8448/54000 (16%)] Loss: -435133.218750\n",
      "Train Epoch: 37 [12544/54000 (23%)] Loss: -441400.812500\n",
      "Train Epoch: 37 [16640/54000 (31%)] Loss: -444957.312500\n",
      "Train Epoch: 37 [20736/54000 (38%)] Loss: -466457.406250\n",
      "Train Epoch: 37 [24832/54000 (46%)] Loss: -449106.937500\n",
      "Train Epoch: 37 [28928/54000 (54%)] Loss: -435066.687500\n",
      "Train Epoch: 37 [33024/54000 (61%)] Loss: -445768.906250\n",
      "Train Epoch: 37 [37120/54000 (69%)] Loss: -456777.187500\n",
      "Train Epoch: 37 [41216/54000 (76%)] Loss: -457099.437500\n",
      "Train Epoch: 37 [45312/54000 (84%)] Loss: -465703.843750\n",
      "Train Epoch: 37 [49408/54000 (91%)] Loss: -507276.312500\n",
      "    epoch          : 37\n",
      "    loss           : -451212.7262195122\n",
      "    val_loss       : -452089.9639648438\n",
      "Train Epoch: 38 [256/54000 (0%)] Loss: -508338.687500\n",
      "Train Epoch: 38 [4352/54000 (8%)] Loss: -427062.687500\n",
      "Train Epoch: 38 [8448/54000 (16%)] Loss: -441049.937500\n",
      "Train Epoch: 38 [12544/54000 (23%)] Loss: -438361.687500\n",
      "Train Epoch: 38 [16640/54000 (31%)] Loss: -458513.812500\n",
      "Train Epoch: 38 [20736/54000 (38%)] Loss: -422900.250000\n",
      "Train Epoch: 38 [24832/54000 (46%)] Loss: -453282.218750\n",
      "Train Epoch: 38 [28928/54000 (54%)] Loss: -437281.750000\n",
      "Train Epoch: 38 [33024/54000 (61%)] Loss: -426358.562500\n",
      "Train Epoch: 38 [37120/54000 (69%)] Loss: -455621.812500\n",
      "Train Epoch: 38 [41216/54000 (76%)] Loss: -455225.812500\n",
      "Train Epoch: 38 [45312/54000 (84%)] Loss: -455500.468750\n",
      "Train Epoch: 38 [49408/54000 (91%)] Loss: -506838.187500\n",
      "    epoch          : 38\n",
      "    loss           : -451613.81600609754\n",
      "    val_loss       : -452263.817578125\n",
      "Train Epoch: 39 [256/54000 (0%)] Loss: -509421.625000\n",
      "Train Epoch: 39 [4352/54000 (8%)] Loss: -454705.687500\n",
      "Train Epoch: 39 [8448/54000 (16%)] Loss: -458213.250000\n",
      "Train Epoch: 39 [12544/54000 (23%)] Loss: -449655.000000\n",
      "Train Epoch: 39 [16640/54000 (31%)] Loss: -436258.187500\n",
      "Train Epoch: 39 [20736/54000 (38%)] Loss: -466098.718750\n",
      "Train Epoch: 39 [24832/54000 (46%)] Loss: -424672.312500\n",
      "Train Epoch: 39 [28928/54000 (54%)] Loss: -439474.031250\n",
      "Train Epoch: 39 [33024/54000 (61%)] Loss: -430801.062500\n",
      "Train Epoch: 39 [37120/54000 (69%)] Loss: -462754.500000\n",
      "Train Epoch: 39 [41216/54000 (76%)] Loss: -437144.656250\n",
      "Train Epoch: 39 [45312/54000 (84%)] Loss: -468674.906250\n",
      "Train Epoch: 39 [49408/54000 (91%)] Loss: -507864.187500\n",
      "    epoch          : 39\n",
      "    loss           : -452301.9217987805\n",
      "    val_loss       : -451284.52294921875\n",
      "Train Epoch: 40 [256/54000 (0%)] Loss: -509359.312500\n",
      "Train Epoch: 40 [4352/54000 (8%)] Loss: -456188.250000\n",
      "Train Epoch: 40 [8448/54000 (16%)] Loss: -469252.125000\n",
      "Train Epoch: 40 [12544/54000 (23%)] Loss: -425587.000000\n",
      "Train Epoch: 40 [16640/54000 (31%)] Loss: -429048.937500\n",
      "Train Epoch: 40 [20736/54000 (38%)] Loss: -435873.593750\n",
      "Train Epoch: 40 [24832/54000 (46%)] Loss: -427077.656250\n",
      "Train Epoch: 40 [28928/54000 (54%)] Loss: -507002.562500\n",
      "Train Epoch: 40 [33024/54000 (61%)] Loss: -428631.875000\n",
      "Train Epoch: 40 [37120/54000 (69%)] Loss: -447371.375000\n",
      "Train Epoch: 40 [41216/54000 (76%)] Loss: -466192.750000\n",
      "Train Epoch: 40 [45312/54000 (84%)] Loss: -468021.187500\n",
      "Train Epoch: 40 [49408/54000 (91%)] Loss: -507592.343750\n",
      "    epoch          : 40\n",
      "    loss           : -453184.1454268293\n",
      "    val_loss       : -452966.09736328124\n",
      "Train Epoch: 41 [256/54000 (0%)] Loss: -444140.812500\n",
      "Train Epoch: 41 [4352/54000 (8%)] Loss: -471501.750000\n",
      "Train Epoch: 41 [8448/54000 (16%)] Loss: -446444.875000\n",
      "Train Epoch: 41 [12544/54000 (23%)] Loss: -452549.625000\n",
      "Train Epoch: 41 [16640/54000 (31%)] Loss: -460538.468750\n",
      "Train Epoch: 41 [20736/54000 (38%)] Loss: -440156.125000\n",
      "Train Epoch: 41 [24832/54000 (46%)] Loss: -457724.250000\n",
      "Train Epoch: 41 [28928/54000 (54%)] Loss: -464620.625000\n",
      "Train Epoch: 41 [33024/54000 (61%)] Loss: -430609.687500\n",
      "Train Epoch: 41 [37120/54000 (69%)] Loss: -460178.000000\n",
      "Train Epoch: 41 [41216/54000 (76%)] Loss: -445110.687500\n",
      "Train Epoch: 41 [45312/54000 (84%)] Loss: -444927.312500\n",
      "Train Epoch: 41 [49408/54000 (91%)] Loss: -506782.312500\n",
      "    epoch          : 41\n",
      "    loss           : -453701.89984756097\n",
      "    val_loss       : -453219.1392578125\n",
      "Train Epoch: 42 [256/54000 (0%)] Loss: -509448.875000\n",
      "Train Epoch: 42 [4352/54000 (8%)] Loss: -452132.968750\n",
      "Train Epoch: 42 [8448/54000 (16%)] Loss: -456926.750000\n",
      "Train Epoch: 42 [12544/54000 (23%)] Loss: -433055.187500\n",
      "Train Epoch: 42 [16640/54000 (31%)] Loss: -452687.312500\n",
      "Train Epoch: 42 [20736/54000 (38%)] Loss: -465839.843750\n",
      "Train Epoch: 42 [24832/54000 (46%)] Loss: -439714.812500\n",
      "Train Epoch: 42 [28928/54000 (54%)] Loss: -442363.281250\n",
      "Train Epoch: 42 [33024/54000 (61%)] Loss: -466838.468750\n",
      "Train Epoch: 42 [37120/54000 (69%)] Loss: -453058.031250\n",
      "Train Epoch: 42 [41216/54000 (76%)] Loss: -431545.343750\n",
      "Train Epoch: 42 [45312/54000 (84%)] Loss: -471659.406250\n",
      "Train Epoch: 42 [49408/54000 (91%)] Loss: -506231.093750\n",
      "    epoch          : 42\n",
      "    loss           : -453878.11143292685\n",
      "    val_loss       : -453935.9772216797\n",
      "Train Epoch: 43 [256/54000 (0%)] Loss: -445714.812500\n",
      "Train Epoch: 43 [4352/54000 (8%)] Loss: -447479.406250\n",
      "Train Epoch: 43 [8448/54000 (16%)] Loss: -463618.750000\n",
      "Train Epoch: 43 [12544/54000 (23%)] Loss: -432457.843750\n",
      "Train Epoch: 43 [16640/54000 (31%)] Loss: -452904.312500\n",
      "Train Epoch: 43 [20736/54000 (38%)] Loss: -466619.156250\n",
      "Train Epoch: 43 [24832/54000 (46%)] Loss: -432747.750000\n",
      "Train Epoch: 43 [28928/54000 (54%)] Loss: -438669.062500\n",
      "Train Epoch: 43 [33024/54000 (61%)] Loss: -439576.875000\n",
      "Train Epoch: 43 [37120/54000 (69%)] Loss: -433542.500000\n",
      "Train Epoch: 43 [41216/54000 (76%)] Loss: -461550.593750\n",
      "Train Epoch: 43 [45312/54000 (84%)] Loss: -467815.812500\n",
      "Train Epoch: 43 [49408/54000 (91%)] Loss: -431731.250000\n",
      "    epoch          : 43\n",
      "    loss           : -454377.7986280488\n",
      "    val_loss       : -452291.4205078125\n",
      "Train Epoch: 44 [256/54000 (0%)] Loss: -509297.562500\n",
      "Train Epoch: 44 [4352/54000 (8%)] Loss: -443703.656250\n",
      "Train Epoch: 44 [8448/54000 (16%)] Loss: -469789.593750\n",
      "Train Epoch: 44 [12544/54000 (23%)] Loss: -507368.343750\n",
      "Train Epoch: 44 [16640/54000 (31%)] Loss: -507900.062500\n",
      "Train Epoch: 44 [20736/54000 (38%)] Loss: -468404.187500\n",
      "Train Epoch: 44 [24832/54000 (46%)] Loss: -437885.031250\n",
      "Train Epoch: 44 [28928/54000 (54%)] Loss: -447434.093750\n",
      "Train Epoch: 44 [33024/54000 (61%)] Loss: -433723.906250\n",
      "Train Epoch: 44 [37120/54000 (69%)] Loss: -433998.593750\n",
      "Train Epoch: 44 [41216/54000 (76%)] Loss: -432078.625000\n",
      "Train Epoch: 44 [45312/54000 (84%)] Loss: -433135.937500\n",
      "Train Epoch: 44 [49408/54000 (91%)] Loss: -506691.812500\n",
      "    epoch          : 44\n",
      "    loss           : -455559.9413109756\n",
      "    val_loss       : -453199.3419189453\n",
      "Train Epoch: 45 [256/54000 (0%)] Loss: -510696.656250\n",
      "Train Epoch: 45 [4352/54000 (8%)] Loss: -461332.000000\n",
      "Train Epoch: 45 [8448/54000 (16%)] Loss: -447483.562500\n",
      "Train Epoch: 45 [12544/54000 (23%)] Loss: -432321.500000\n",
      "Train Epoch: 45 [16640/54000 (31%)] Loss: -435583.750000\n",
      "Train Epoch: 45 [20736/54000 (38%)] Loss: -470007.437500\n",
      "Train Epoch: 45 [24832/54000 (46%)] Loss: -437683.968750\n",
      "Train Epoch: 45 [28928/54000 (54%)] Loss: -446575.312500\n",
      "Train Epoch: 45 [33024/54000 (61%)] Loss: -457654.562500\n",
      "Train Epoch: 45 [37120/54000 (69%)] Loss: -446518.312500\n",
      "Train Epoch: 45 [41216/54000 (76%)] Loss: -434250.812500\n",
      "Train Epoch: 45 [45312/54000 (84%)] Loss: -446506.062500\n",
      "Train Epoch: 45 [49408/54000 (91%)] Loss: -443177.312500\n",
      "    epoch          : 45\n",
      "    loss           : -456000.3782012195\n",
      "    val_loss       : -453975.962109375\n",
      "Train Epoch: 46 [256/54000 (0%)] Loss: -468775.156250\n",
      "Train Epoch: 46 [4352/54000 (8%)] Loss: -448030.843750\n",
      "Train Epoch: 46 [8448/54000 (16%)] Loss: -445944.593750\n",
      "Train Epoch: 46 [12544/54000 (23%)] Loss: -462814.031250\n",
      "Train Epoch: 46 [16640/54000 (31%)] Loss: -431731.312500\n",
      "Train Epoch: 46 [20736/54000 (38%)] Loss: -470390.468750\n",
      "Train Epoch: 46 [24832/54000 (46%)] Loss: -432102.031250\n",
      "Train Epoch: 46 [28928/54000 (54%)] Loss: -443425.375000\n",
      "Train Epoch: 46 [33024/54000 (61%)] Loss: -508186.312500\n",
      "Train Epoch: 46 [37120/54000 (69%)] Loss: -427180.875000\n",
      "Train Epoch: 46 [41216/54000 (76%)] Loss: -449581.812500\n",
      "Train Epoch: 46 [45312/54000 (84%)] Loss: -455276.687500\n",
      "Train Epoch: 46 [49408/54000 (91%)] Loss: -507811.250000\n",
      "    epoch          : 46\n",
      "    loss           : -455418.86387195124\n",
      "    val_loss       : -454516.0811523438\n",
      "Train Epoch: 47 [256/54000 (0%)] Loss: -507913.250000\n",
      "Train Epoch: 47 [4352/54000 (8%)] Loss: -452986.718750\n",
      "Train Epoch: 47 [8448/54000 (16%)] Loss: -441511.750000\n",
      "Train Epoch: 47 [12544/54000 (23%)] Loss: -456119.375000\n",
      "Train Epoch: 47 [16640/54000 (31%)] Loss: -511409.093750\n",
      "Train Epoch: 47 [20736/54000 (38%)] Loss: -471519.687500\n",
      "Train Epoch: 47 [24832/54000 (46%)] Loss: -460992.125000\n",
      "Train Epoch: 47 [28928/54000 (54%)] Loss: -449973.781250\n",
      "Train Epoch: 47 [33024/54000 (61%)] Loss: -454662.656250\n",
      "Train Epoch: 47 [37120/54000 (69%)] Loss: -509960.000000\n",
      "Train Epoch: 47 [41216/54000 (76%)] Loss: -458396.562500\n",
      "Train Epoch: 47 [45312/54000 (84%)] Loss: -463463.062500\n",
      "Train Epoch: 47 [49408/54000 (91%)] Loss: -446490.531250\n",
      "    epoch          : 47\n",
      "    loss           : -456331.75335365854\n",
      "    val_loss       : -454666.92919921875\n",
      "Train Epoch: 48 [256/54000 (0%)] Loss: -509807.093750\n",
      "Train Epoch: 48 [4352/54000 (8%)] Loss: -458686.625000\n",
      "Train Epoch: 48 [8448/54000 (16%)] Loss: -436898.468750\n",
      "Train Epoch: 48 [12544/54000 (23%)] Loss: -454530.750000\n",
      "Train Epoch: 48 [16640/54000 (31%)] Loss: -462241.156250\n",
      "Train Epoch: 48 [20736/54000 (38%)] Loss: -473534.937500\n",
      "Train Epoch: 48 [24832/54000 (46%)] Loss: -450092.000000\n",
      "Train Epoch: 48 [28928/54000 (54%)] Loss: -508476.312500\n",
      "Train Epoch: 48 [33024/54000 (61%)] Loss: -453909.156250\n",
      "Train Epoch: 48 [37120/54000 (69%)] Loss: -510347.250000\n",
      "Train Epoch: 48 [41216/54000 (76%)] Loss: -452287.562500\n",
      "Train Epoch: 48 [45312/54000 (84%)] Loss: -463485.187500\n",
      "Train Epoch: 48 [49408/54000 (91%)] Loss: -508815.500000\n",
      "    epoch          : 48\n",
      "    loss           : -457344.1760670732\n",
      "    val_loss       : -455192.5096435547\n",
      "Train Epoch: 49 [256/54000 (0%)] Loss: -510372.437500\n",
      "Train Epoch: 49 [4352/54000 (8%)] Loss: -438123.468750\n",
      "Train Epoch: 49 [8448/54000 (16%)] Loss: -448162.750000\n",
      "Train Epoch: 49 [12544/54000 (23%)] Loss: -460332.812500\n",
      "Train Epoch: 49 [16640/54000 (31%)] Loss: -438651.218750\n",
      "Train Epoch: 49 [20736/54000 (38%)] Loss: -471792.375000\n",
      "Train Epoch: 49 [24832/54000 (46%)] Loss: -437595.625000\n",
      "Train Epoch: 49 [28928/54000 (54%)] Loss: -439742.187500\n",
      "Train Epoch: 49 [33024/54000 (61%)] Loss: -455802.437500\n",
      "Train Epoch: 49 [37120/54000 (69%)] Loss: -464834.625000\n",
      "Train Epoch: 49 [41216/54000 (76%)] Loss: -446688.656250\n",
      "Train Epoch: 49 [45312/54000 (84%)] Loss: -448196.375000\n",
      "Train Epoch: 49 [49408/54000 (91%)] Loss: -507464.937500\n",
      "    epoch          : 49\n",
      "    loss           : -458366.831097561\n",
      "    val_loss       : -456682.1043701172\n",
      "Train Epoch: 50 [256/54000 (0%)] Loss: -510593.156250\n",
      "Train Epoch: 50 [4352/54000 (8%)] Loss: -467442.593750\n",
      "Train Epoch: 50 [8448/54000 (16%)] Loss: -438080.875000\n",
      "Train Epoch: 50 [12544/54000 (23%)] Loss: -509006.281250\n",
      "Train Epoch: 50 [16640/54000 (31%)] Loss: -447703.500000\n",
      "Train Epoch: 50 [20736/54000 (38%)] Loss: -471661.875000\n",
      "Train Epoch: 50 [24832/54000 (46%)] Loss: -454951.312500\n",
      "Train Epoch: 50 [28928/54000 (54%)] Loss: -466368.156250\n",
      "Train Epoch: 50 [33024/54000 (61%)] Loss: -452226.687500\n",
      "Train Epoch: 50 [37120/54000 (69%)] Loss: -433015.437500\n",
      "Train Epoch: 50 [41216/54000 (76%)] Loss: -433128.500000\n",
      "Train Epoch: 50 [45312/54000 (84%)] Loss: -450018.406250\n",
      "Train Epoch: 50 [49408/54000 (91%)] Loss: -443821.687500\n",
      "    epoch          : 50\n",
      "    loss           : -458153.29359756096\n",
      "    val_loss       : -456582.6713134766\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0702_131016/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [256/54000 (0%)] Loss: -510199.812500\n",
      "Train Epoch: 51 [4352/54000 (8%)] Loss: -452958.187500\n",
      "Train Epoch: 51 [8448/54000 (16%)] Loss: -472837.781250\n",
      "Train Epoch: 51 [12544/54000 (23%)] Loss: -509101.500000\n",
      "Train Epoch: 51 [16640/54000 (31%)] Loss: -444786.437500\n",
      "Train Epoch: 51 [20736/54000 (38%)] Loss: -509128.125000\n",
      "Train Epoch: 51 [24832/54000 (46%)] Loss: -434135.531250\n",
      "Train Epoch: 51 [28928/54000 (54%)] Loss: -441562.468750\n",
      "Train Epoch: 51 [33024/54000 (61%)] Loss: -509637.500000\n",
      "Train Epoch: 51 [37120/54000 (69%)] Loss: -510001.250000\n",
      "Train Epoch: 51 [41216/54000 (76%)] Loss: -458041.125000\n",
      "Train Epoch: 51 [45312/54000 (84%)] Loss: -447596.125000\n",
      "Train Epoch: 51 [49408/54000 (91%)] Loss: -508486.062500\n",
      "    epoch          : 51\n",
      "    loss           : -457949.7710365854\n",
      "    val_loss       : -457465.46552734374\n",
      "Train Epoch: 52 [256/54000 (0%)] Loss: -472806.468750\n",
      "Train Epoch: 52 [4352/54000 (8%)] Loss: -461271.656250\n",
      "Train Epoch: 52 [8448/54000 (16%)] Loss: -467090.062500\n",
      "Train Epoch: 52 [12544/54000 (23%)] Loss: -460967.125000\n",
      "Train Epoch: 52 [16640/54000 (31%)] Loss: -439786.875000\n",
      "Train Epoch: 52 [20736/54000 (38%)] Loss: -445482.593750\n",
      "Train Epoch: 52 [24832/54000 (46%)] Loss: -451331.562500\n",
      "Train Epoch: 52 [28928/54000 (54%)] Loss: -449713.500000\n",
      "Train Epoch: 52 [33024/54000 (61%)] Loss: -435489.156250\n",
      "Train Epoch: 52 [37120/54000 (69%)] Loss: -438241.781250\n",
      "Train Epoch: 52 [41216/54000 (76%)] Loss: -473001.875000\n",
      "Train Epoch: 52 [45312/54000 (84%)] Loss: -465278.187500\n",
      "Train Epoch: 52 [49408/54000 (91%)] Loss: -509192.000000\n",
      "    epoch          : 52\n",
      "    loss           : -459082.5125\n",
      "    val_loss       : -457737.7732421875\n",
      "Train Epoch: 53 [256/54000 (0%)] Loss: -510572.375000\n",
      "Train Epoch: 53 [4352/54000 (8%)] Loss: -463345.375000\n",
      "Train Epoch: 53 [8448/54000 (16%)] Loss: -452433.406250\n",
      "Train Epoch: 53 [12544/54000 (23%)] Loss: -440864.125000\n",
      "Train Epoch: 53 [16640/54000 (31%)] Loss: -452406.000000\n",
      "Train Epoch: 53 [20736/54000 (38%)] Loss: -446741.500000\n",
      "Train Epoch: 53 [24832/54000 (46%)] Loss: -455684.406250\n",
      "Train Epoch: 53 [28928/54000 (54%)] Loss: -438069.687500\n",
      "Train Epoch: 53 [33024/54000 (61%)] Loss: -447643.468750\n",
      "Train Epoch: 53 [37120/54000 (69%)] Loss: -509844.687500\n",
      "Train Epoch: 53 [41216/54000 (76%)] Loss: -448736.343750\n",
      "Train Epoch: 53 [45312/54000 (84%)] Loss: -440045.125000\n",
      "Train Epoch: 53 [49408/54000 (91%)] Loss: -508788.968750\n",
      "    epoch          : 53\n",
      "    loss           : -459562.9495426829\n",
      "    val_loss       : -458582.6583007813\n",
      "Train Epoch: 54 [256/54000 (0%)] Loss: -511314.156250\n",
      "Train Epoch: 54 [4352/54000 (8%)] Loss: -441931.687500\n",
      "Train Epoch: 54 [8448/54000 (16%)] Loss: -450692.937500\n",
      "Train Epoch: 54 [12544/54000 (23%)] Loss: -510333.375000\n",
      "Train Epoch: 54 [16640/54000 (31%)] Loss: -459765.625000\n",
      "Train Epoch: 54 [20736/54000 (38%)] Loss: -438347.437500\n",
      "Train Epoch: 54 [24832/54000 (46%)] Loss: -464759.812500\n",
      "Train Epoch: 54 [28928/54000 (54%)] Loss: -508475.250000\n",
      "Train Epoch: 54 [33024/54000 (61%)] Loss: -447286.343750\n",
      "Train Epoch: 54 [37120/54000 (69%)] Loss: -443340.156250\n",
      "Train Epoch: 54 [41216/54000 (76%)] Loss: -460139.343750\n",
      "Train Epoch: 54 [45312/54000 (84%)] Loss: -466175.812500\n",
      "Train Epoch: 54 [49408/54000 (91%)] Loss: -509662.062500\n",
      "    epoch          : 54\n",
      "    loss           : -458843.79649390245\n",
      "    val_loss       : -458995.41923828126\n",
      "Train Epoch: 55 [256/54000 (0%)] Loss: -475945.687500\n",
      "Train Epoch: 55 [4352/54000 (8%)] Loss: -440542.625000\n",
      "Train Epoch: 55 [8448/54000 (16%)] Loss: -442839.125000\n",
      "Train Epoch: 55 [12544/54000 (23%)] Loss: -508912.718750\n",
      "Train Epoch: 55 [16640/54000 (31%)] Loss: -452560.156250\n",
      "Train Epoch: 55 [20736/54000 (38%)] Loss: -447274.500000\n",
      "Train Epoch: 55 [24832/54000 (46%)] Loss: -464995.656250\n",
      "Train Epoch: 55 [28928/54000 (54%)] Loss: -445479.375000\n",
      "Train Epoch: 55 [33024/54000 (61%)] Loss: -510185.312500\n",
      "Train Epoch: 55 [37120/54000 (69%)] Loss: -439607.468750\n",
      "Train Epoch: 55 [41216/54000 (76%)] Loss: -448236.437500\n",
      "Train Epoch: 55 [45312/54000 (84%)] Loss: -465228.625000\n",
      "Train Epoch: 55 [49408/54000 (91%)] Loss: -507875.312500\n",
      "    epoch          : 55\n",
      "    loss           : -460403.21966463415\n",
      "    val_loss       : -457033.7457519531\n",
      "Train Epoch: 56 [256/54000 (0%)] Loss: -470948.437500\n",
      "Train Epoch: 56 [4352/54000 (8%)] Loss: -437231.062500\n",
      "Train Epoch: 56 [8448/54000 (16%)] Loss: -441470.750000\n",
      "Train Epoch: 56 [12544/54000 (23%)] Loss: -466598.468750\n",
      "Train Epoch: 56 [16640/54000 (31%)] Loss: -461631.562500\n",
      "Train Epoch: 56 [20736/54000 (38%)] Loss: -449282.625000\n",
      "Train Epoch: 56 [24832/54000 (46%)] Loss: -466068.812500\n",
      "Train Epoch: 56 [28928/54000 (54%)] Loss: -451264.031250\n",
      "Train Epoch: 56 [33024/54000 (61%)] Loss: -450407.906250\n",
      "Train Epoch: 56 [37120/54000 (69%)] Loss: -508870.125000\n",
      "Train Epoch: 56 [41216/54000 (76%)] Loss: -441877.531250\n",
      "Train Epoch: 56 [45312/54000 (84%)] Loss: -451023.375000\n",
      "Train Epoch: 56 [49408/54000 (91%)] Loss: -508642.875000\n",
      "    epoch          : 56\n",
      "    loss           : -459743.2024390244\n",
      "    val_loss       : -458171.60031738284\n",
      "Train Epoch: 57 [256/54000 (0%)] Loss: -509872.562500\n",
      "Train Epoch: 57 [4352/54000 (8%)] Loss: -458459.218750\n",
      "Train Epoch: 57 [8448/54000 (16%)] Loss: -451585.656250\n",
      "Train Epoch: 57 [12544/54000 (23%)] Loss: -459669.031250\n",
      "Train Epoch: 57 [16640/54000 (31%)] Loss: -509345.187500\n",
      "Train Epoch: 57 [20736/54000 (38%)] Loss: -509223.312500\n",
      "Train Epoch: 57 [24832/54000 (46%)] Loss: -436499.531250\n",
      "Train Epoch: 57 [28928/54000 (54%)] Loss: -441837.937500\n",
      "Train Epoch: 57 [33024/54000 (61%)] Loss: -445835.437500\n",
      "Train Epoch: 57 [37120/54000 (69%)] Loss: -509950.093750\n",
      "Train Epoch: 57 [41216/54000 (76%)] Loss: -463302.500000\n",
      "Train Epoch: 57 [45312/54000 (84%)] Loss: -460859.687500\n",
      "Train Epoch: 57 [49408/54000 (91%)] Loss: -451013.500000\n",
      "    epoch          : 57\n",
      "    loss           : -459180.33856707317\n",
      "    val_loss       : -456579.22314453125\n",
      "Train Epoch: 58 [256/54000 (0%)] Loss: -511743.750000\n",
      "Train Epoch: 58 [4352/54000 (8%)] Loss: -448312.656250\n",
      "Train Epoch: 58 [8448/54000 (16%)] Loss: -445687.843750\n",
      "Train Epoch: 58 [12544/54000 (23%)] Loss: -439383.625000\n",
      "Train Epoch: 58 [16640/54000 (31%)] Loss: -438686.718750\n",
      "Train Epoch: 58 [20736/54000 (38%)] Loss: -475672.625000\n",
      "Train Epoch: 58 [24832/54000 (46%)] Loss: -440408.750000\n",
      "Train Epoch: 58 [28928/54000 (54%)] Loss: -441155.687500\n",
      "Train Epoch: 58 [33024/54000 (61%)] Loss: -507883.718750\n",
      "Train Epoch: 58 [37120/54000 (69%)] Loss: -435829.437500\n",
      "Train Epoch: 58 [41216/54000 (76%)] Loss: -449961.562500\n",
      "Train Epoch: 58 [45312/54000 (84%)] Loss: -466070.750000\n",
      "Train Epoch: 58 [49408/54000 (91%)] Loss: -510242.812500\n",
      "    epoch          : 58\n",
      "    loss           : -460793.82774390245\n",
      "    val_loss       : -460042.8087890625\n",
      "Train Epoch: 59 [256/54000 (0%)] Loss: -474274.718750\n",
      "Train Epoch: 59 [4352/54000 (8%)] Loss: -461176.812500\n",
      "Train Epoch: 59 [8448/54000 (16%)] Loss: -474449.281250\n",
      "Train Epoch: 59 [12544/54000 (23%)] Loss: -470092.093750\n",
      "Train Epoch: 59 [16640/54000 (31%)] Loss: -454770.156250\n",
      "Train Epoch: 59 [20736/54000 (38%)] Loss: -475562.250000\n",
      "Train Epoch: 59 [24832/54000 (46%)] Loss: -464689.125000\n",
      "Train Epoch: 59 [28928/54000 (54%)] Loss: -508532.718750\n",
      "Train Epoch: 59 [33024/54000 (61%)] Loss: -440152.937500\n",
      "Train Epoch: 59 [37120/54000 (69%)] Loss: -446689.937500\n",
      "Train Epoch: 59 [41216/54000 (76%)] Loss: -431362.812500\n",
      "Train Epoch: 59 [45312/54000 (84%)] Loss: -452889.343750\n",
      "Train Epoch: 59 [49408/54000 (91%)] Loss: -510080.531250\n",
      "    epoch          : 59\n",
      "    loss           : -459373.8557926829\n",
      "    val_loss       : -457510.18454589846\n",
      "Train Epoch: 60 [256/54000 (0%)] Loss: -510177.468750\n",
      "Train Epoch: 60 [4352/54000 (8%)] Loss: -440851.781250\n",
      "Train Epoch: 60 [8448/54000 (16%)] Loss: -441379.968750\n",
      "Train Epoch: 60 [12544/54000 (23%)] Loss: -468088.000000\n",
      "Train Epoch: 60 [16640/54000 (31%)] Loss: -437327.343750\n",
      "Train Epoch: 60 [20736/54000 (38%)] Loss: -471756.906250\n",
      "Train Epoch: 60 [24832/54000 (46%)] Loss: -456193.000000\n",
      "Train Epoch: 60 [28928/54000 (54%)] Loss: -508020.187500\n",
      "Train Epoch: 60 [33024/54000 (61%)] Loss: -439475.625000\n",
      "Train Epoch: 60 [37120/54000 (69%)] Loss: -468138.468750\n",
      "Train Epoch: 60 [41216/54000 (76%)] Loss: -440116.406250\n",
      "Train Epoch: 60 [45312/54000 (84%)] Loss: -452350.250000\n",
      "Train Epoch: 60 [49408/54000 (91%)] Loss: -510299.125000\n",
      "    epoch          : 60\n",
      "    loss           : -461140.7774390244\n",
      "    val_loss       : -460509.42412109376\n",
      "Train Epoch: 61 [256/54000 (0%)] Loss: -452130.687500\n",
      "Train Epoch: 61 [4352/54000 (8%)] Loss: -460619.437500\n",
      "Train Epoch: 61 [8448/54000 (16%)] Loss: -452141.406250\n",
      "Train Epoch: 61 [12544/54000 (23%)] Loss: -460247.000000\n",
      "Train Epoch: 61 [16640/54000 (31%)] Loss: -443752.343750\n",
      "Train Epoch: 61 [20736/54000 (38%)] Loss: -473703.656250\n",
      "Train Epoch: 61 [24832/54000 (46%)] Loss: -466909.593750\n",
      "Train Epoch: 61 [28928/54000 (54%)] Loss: -509881.625000\n",
      "Train Epoch: 61 [33024/54000 (61%)] Loss: -440617.406250\n",
      "Train Epoch: 61 [37120/54000 (69%)] Loss: -444512.375000\n",
      "Train Epoch: 61 [41216/54000 (76%)] Loss: -455547.937500\n",
      "Train Epoch: 61 [45312/54000 (84%)] Loss: -455929.375000\n",
      "Train Epoch: 61 [49408/54000 (91%)] Loss: -509667.375000\n",
      "    epoch          : 61\n",
      "    loss           : -461780.2650914634\n",
      "    val_loss       : -459644.65932617185\n",
      "Train Epoch: 62 [256/54000 (0%)] Loss: -511572.812500\n",
      "Train Epoch: 62 [4352/54000 (8%)] Loss: -470177.843750\n",
      "Train Epoch: 62 [8448/54000 (16%)] Loss: -453369.375000\n",
      "Train Epoch: 62 [12544/54000 (23%)] Loss: -463980.125000\n",
      "Train Epoch: 62 [16640/54000 (31%)] Loss: -440327.656250\n",
      "Train Epoch: 62 [20736/54000 (38%)] Loss: -475048.593750\n",
      "Train Epoch: 62 [24832/54000 (46%)] Loss: -467959.937500\n",
      "Train Epoch: 62 [28928/54000 (54%)] Loss: -510702.750000\n",
      "Train Epoch: 62 [33024/54000 (61%)] Loss: -510692.250000\n",
      "Train Epoch: 62 [37120/54000 (69%)] Loss: -443273.531250\n",
      "Train Epoch: 62 [41216/54000 (76%)] Loss: -451004.031250\n",
      "Train Epoch: 62 [45312/54000 (84%)] Loss: -474523.593750\n",
      "Train Epoch: 62 [49408/54000 (91%)] Loss: -508739.843750\n",
      "    epoch          : 62\n",
      "    loss           : -462402.237347561\n",
      "    val_loss       : -457472.5278564453\n",
      "Train Epoch: 63 [256/54000 (0%)] Loss: -511712.500000\n",
      "Train Epoch: 63 [4352/54000 (8%)] Loss: -444496.750000\n",
      "Train Epoch: 63 [8448/54000 (16%)] Loss: -450986.500000\n",
      "Train Epoch: 63 [12544/54000 (23%)] Loss: -457858.750000\n",
      "Train Epoch: 63 [16640/54000 (31%)] Loss: -468409.531250\n",
      "Train Epoch: 63 [20736/54000 (38%)] Loss: -474854.968750\n",
      "Train Epoch: 63 [24832/54000 (46%)] Loss: -452896.906250\n",
      "Train Epoch: 63 [28928/54000 (54%)] Loss: -508545.343750\n",
      "Train Epoch: 63 [33024/54000 (61%)] Loss: -462543.781250\n",
      "Train Epoch: 63 [37120/54000 (69%)] Loss: -509346.500000\n",
      "Train Epoch: 63 [41216/54000 (76%)] Loss: -471739.812500\n",
      "Train Epoch: 63 [45312/54000 (84%)] Loss: -459029.125000\n",
      "Train Epoch: 63 [49408/54000 (91%)] Loss: -511796.937500\n",
      "    epoch          : 63\n",
      "    loss           : -461437.6670731707\n",
      "    val_loss       : -459099.62529296876\n",
      "Train Epoch: 64 [256/54000 (0%)] Loss: -445418.187500\n",
      "Train Epoch: 64 [4352/54000 (8%)] Loss: -461863.031250\n",
      "Train Epoch: 64 [8448/54000 (16%)] Loss: -442974.000000\n",
      "Train Epoch: 64 [12544/54000 (23%)] Loss: -468576.125000\n",
      "Train Epoch: 64 [16640/54000 (31%)] Loss: -452710.125000\n",
      "Train Epoch: 64 [20736/54000 (38%)] Loss: -475647.875000\n",
      "Train Epoch: 64 [24832/54000 (46%)] Loss: -456209.906250\n",
      "Train Epoch: 64 [28928/54000 (54%)] Loss: -440659.968750\n",
      "Train Epoch: 64 [33024/54000 (61%)] Loss: -434025.625000\n",
      "Train Epoch: 64 [37120/54000 (69%)] Loss: -449346.312500\n",
      "Train Epoch: 64 [41216/54000 (76%)] Loss: -439176.500000\n",
      "Train Epoch: 64 [45312/54000 (84%)] Loss: -470405.750000\n",
      "Train Epoch: 64 [49408/54000 (91%)] Loss: -508236.812500\n",
      "    epoch          : 64\n",
      "    loss           : -460490.55076219514\n",
      "    val_loss       : -457647.66965332034\n",
      "Train Epoch: 65 [256/54000 (0%)] Loss: -512932.125000\n",
      "Train Epoch: 65 [4352/54000 (8%)] Loss: -470676.875000\n",
      "Train Epoch: 65 [8448/54000 (16%)] Loss: -444083.562500\n",
      "Train Epoch: 65 [12544/54000 (23%)] Loss: -470725.156250\n",
      "Train Epoch: 65 [16640/54000 (31%)] Loss: -464291.125000\n",
      "Train Epoch: 65 [20736/54000 (38%)] Loss: -475395.937500\n",
      "Train Epoch: 65 [24832/54000 (46%)] Loss: -459940.187500\n",
      "Train Epoch: 65 [28928/54000 (54%)] Loss: -454049.500000\n",
      "Train Epoch: 65 [33024/54000 (61%)] Loss: -511045.156250\n",
      "Train Epoch: 65 [37120/54000 (69%)] Loss: -510011.906250\n",
      "Train Epoch: 65 [41216/54000 (76%)] Loss: -475309.687500\n",
      "Train Epoch: 65 [45312/54000 (84%)] Loss: -455153.875000\n",
      "Train Epoch: 65 [49408/54000 (91%)] Loss: -448122.281250\n",
      "    epoch          : 65\n",
      "    loss           : -463142.2068597561\n",
      "    val_loss       : -460380.0088867188\n",
      "Train Epoch: 66 [256/54000 (0%)] Loss: -511537.562500\n",
      "Train Epoch: 66 [4352/54000 (8%)] Loss: -458543.031250\n",
      "Train Epoch: 66 [8448/54000 (16%)] Loss: -453568.062500\n",
      "Train Epoch: 66 [12544/54000 (23%)] Loss: -510185.625000\n",
      "Train Epoch: 66 [16640/54000 (31%)] Loss: -458832.937500\n",
      "Train Epoch: 66 [20736/54000 (38%)] Loss: -452583.218750\n",
      "Train Epoch: 66 [24832/54000 (46%)] Loss: -444598.687500\n",
      "Train Epoch: 66 [28928/54000 (54%)] Loss: -511416.437500\n",
      "Train Epoch: 66 [33024/54000 (61%)] Loss: -464014.312500\n",
      "Train Epoch: 66 [37120/54000 (69%)] Loss: -508800.000000\n",
      "Train Epoch: 66 [41216/54000 (76%)] Loss: -445790.312500\n",
      "Train Epoch: 66 [45312/54000 (84%)] Loss: -444257.125000\n",
      "Train Epoch: 66 [49408/54000 (91%)] Loss: -509942.250000\n",
      "    epoch          : 66\n",
      "    loss           : -462751.65533536585\n",
      "    val_loss       : -458876.7994873047\n",
      "Train Epoch: 67 [256/54000 (0%)] Loss: -455366.187500\n",
      "Train Epoch: 67 [4352/54000 (8%)] Loss: -452694.375000\n",
      "Train Epoch: 67 [8448/54000 (16%)] Loss: -443075.718750\n",
      "Train Epoch: 67 [12544/54000 (23%)] Loss: -509754.437500\n",
      "Train Epoch: 67 [16640/54000 (31%)] Loss: -509773.468750\n",
      "Train Epoch: 67 [20736/54000 (38%)] Loss: -450780.218750\n",
      "Train Epoch: 67 [24832/54000 (46%)] Loss: -441185.468750\n",
      "Train Epoch: 67 [28928/54000 (54%)] Loss: -443880.125000\n",
      "Train Epoch: 67 [33024/54000 (61%)] Loss: -444624.937500\n",
      "Train Epoch: 67 [37120/54000 (69%)] Loss: -511497.250000\n",
      "Train Epoch: 67 [41216/54000 (76%)] Loss: -460608.531250\n",
      "Train Epoch: 67 [45312/54000 (84%)] Loss: -471415.375000\n",
      "Train Epoch: 67 [49408/54000 (91%)] Loss: -508446.250000\n",
      "    epoch          : 67\n",
      "    loss           : -462631.531097561\n",
      "    val_loss       : -460968.7267578125\n",
      "Train Epoch: 68 [256/54000 (0%)] Loss: -453891.437500\n",
      "Train Epoch: 68 [4352/54000 (8%)] Loss: -468145.750000\n",
      "Train Epoch: 68 [8448/54000 (16%)] Loss: -475946.968750\n",
      "Train Epoch: 68 [12544/54000 (23%)] Loss: -461538.250000\n",
      "Train Epoch: 68 [16640/54000 (31%)] Loss: -465330.843750\n",
      "Train Epoch: 68 [20736/54000 (38%)] Loss: -475980.687500\n",
      "Train Epoch: 68 [24832/54000 (46%)] Loss: -463738.656250\n",
      "Train Epoch: 68 [28928/54000 (54%)] Loss: -504591.906250\n",
      "Train Epoch: 68 [33024/54000 (61%)] Loss: -464639.500000\n",
      "Train Epoch: 68 [37120/54000 (69%)] Loss: -470434.312500\n",
      "Train Epoch: 68 [41216/54000 (76%)] Loss: -444467.500000\n",
      "Train Epoch: 68 [45312/54000 (84%)] Loss: -452341.375000\n",
      "Train Epoch: 68 [49408/54000 (91%)] Loss: -509166.187500\n",
      "    epoch          : 68\n",
      "    loss           : -460930.7487804878\n",
      "    val_loss       : -459315.1154296875\n",
      "Train Epoch: 69 [256/54000 (0%)] Loss: -467070.937500\n",
      "Train Epoch: 69 [4352/54000 (8%)] Loss: -443881.062500\n",
      "Train Epoch: 69 [8448/54000 (16%)] Loss: -473642.625000\n",
      "Train Epoch: 69 [12544/54000 (23%)] Loss: -459827.593750\n",
      "Train Epoch: 69 [16640/54000 (31%)] Loss: -510340.656250\n",
      "Train Epoch: 69 [20736/54000 (38%)] Loss: -476385.281250\n",
      "Train Epoch: 69 [24832/54000 (46%)] Loss: -443535.031250\n",
      "Train Epoch: 69 [28928/54000 (54%)] Loss: -448322.531250\n",
      "Train Epoch: 69 [33024/54000 (61%)] Loss: -464716.468750\n",
      "Train Epoch: 69 [37120/54000 (69%)] Loss: -444555.250000\n",
      "Train Epoch: 69 [41216/54000 (76%)] Loss: -452002.343750\n",
      "Train Epoch: 69 [45312/54000 (84%)] Loss: -470073.906250\n",
      "Train Epoch: 69 [49408/54000 (91%)] Loss: -510413.875000\n",
      "    epoch          : 69\n",
      "    loss           : -462179.4760670732\n",
      "    val_loss       : -461647.60546875\n",
      "Train Epoch: 70 [256/54000 (0%)] Loss: -460974.687500\n",
      "Train Epoch: 70 [4352/54000 (8%)] Loss: -462436.250000\n",
      "Train Epoch: 70 [8448/54000 (16%)] Loss: -454962.906250\n",
      "Train Epoch: 70 [12544/54000 (23%)] Loss: -465375.625000\n",
      "Train Epoch: 70 [16640/54000 (31%)] Loss: -453893.843750\n",
      "Train Epoch: 70 [20736/54000 (38%)] Loss: -477042.968750\n",
      "Train Epoch: 70 [24832/54000 (46%)] Loss: -461832.125000\n",
      "Train Epoch: 70 [28928/54000 (54%)] Loss: -449528.093750\n",
      "Train Epoch: 70 [33024/54000 (61%)] Loss: -457042.250000\n",
      "Train Epoch: 70 [37120/54000 (69%)] Loss: -510766.375000\n",
      "Train Epoch: 70 [41216/54000 (76%)] Loss: -460173.156250\n",
      "Train Epoch: 70 [45312/54000 (84%)] Loss: -453682.500000\n",
      "Train Epoch: 70 [49408/54000 (91%)] Loss: -509904.156250\n",
      "    epoch          : 70\n",
      "    loss           : -464238.7606707317\n",
      "    val_loss       : -461503.6062255859\n",
      "Train Epoch: 71 [256/54000 (0%)] Loss: -510846.937500\n",
      "Train Epoch: 71 [4352/54000 (8%)] Loss: -462969.250000\n",
      "Train Epoch: 71 [8448/54000 (16%)] Loss: -456227.031250\n",
      "Train Epoch: 71 [12544/54000 (23%)] Loss: -449243.500000\n",
      "Train Epoch: 71 [16640/54000 (31%)] Loss: -462223.375000\n",
      "Train Epoch: 71 [20736/54000 (38%)] Loss: -476545.375000\n",
      "Train Epoch: 71 [24832/54000 (46%)] Loss: -444732.156250\n",
      "Train Epoch: 71 [28928/54000 (54%)] Loss: -449515.406250\n",
      "Train Epoch: 71 [33024/54000 (61%)] Loss: -510784.625000\n",
      "Train Epoch: 71 [37120/54000 (69%)] Loss: -446681.250000\n",
      "Train Epoch: 71 [41216/54000 (76%)] Loss: -466244.187500\n",
      "Train Epoch: 71 [45312/54000 (84%)] Loss: -443137.218750\n",
      "Train Epoch: 71 [49408/54000 (91%)] Loss: -509118.312500\n",
      "    epoch          : 71\n",
      "    loss           : -464229.01417682925\n",
      "    val_loss       : -460421.7587158203\n",
      "Train Epoch: 72 [256/54000 (0%)] Loss: -511350.937500\n",
      "Train Epoch: 72 [4352/54000 (8%)] Loss: -457887.968750\n",
      "Train Epoch: 72 [8448/54000 (16%)] Loss: -442338.437500\n",
      "Train Epoch: 72 [12544/54000 (23%)] Loss: -462530.375000\n",
      "Train Epoch: 72 [16640/54000 (31%)] Loss: -450246.625000\n",
      "Train Epoch: 72 [20736/54000 (38%)] Loss: -472879.281250\n",
      "Train Epoch: 72 [24832/54000 (46%)] Loss: -442344.937500\n",
      "Train Epoch: 72 [28928/54000 (54%)] Loss: -507469.218750\n",
      "Train Epoch: 72 [33024/54000 (61%)] Loss: -441362.875000\n",
      "Train Epoch: 72 [37120/54000 (69%)] Loss: -443722.656250\n",
      "Train Epoch: 72 [41216/54000 (76%)] Loss: -457854.812500\n",
      "Train Epoch: 72 [45312/54000 (84%)] Loss: -453882.593750\n",
      "Train Epoch: 72 [49408/54000 (91%)] Loss: -510130.093750\n",
      "    epoch          : 72\n",
      "    loss           : -461795.7385670732\n",
      "    val_loss       : -461707.347265625\n",
      "Train Epoch: 73 [256/54000 (0%)] Loss: -454223.781250\n",
      "Train Epoch: 73 [4352/54000 (8%)] Loss: -465490.343750\n",
      "Train Epoch: 73 [8448/54000 (16%)] Loss: -476188.687500\n",
      "Train Epoch: 73 [12544/54000 (23%)] Loss: -510200.062500\n",
      "Train Epoch: 73 [16640/54000 (31%)] Loss: -470140.781250\n",
      "Train Epoch: 73 [20736/54000 (38%)] Loss: -477549.312500\n",
      "Train Epoch: 73 [24832/54000 (46%)] Loss: -463492.875000\n",
      "Train Epoch: 73 [28928/54000 (54%)] Loss: -509699.562500\n",
      "Train Epoch: 73 [33024/54000 (61%)] Loss: -461423.000000\n",
      "Train Epoch: 73 [37120/54000 (69%)] Loss: -455024.468750\n",
      "Train Epoch: 73 [41216/54000 (76%)] Loss: -458490.968750\n",
      "Train Epoch: 73 [45312/54000 (84%)] Loss: -469610.562500\n",
      "Train Epoch: 73 [49408/54000 (91%)] Loss: -510749.750000\n",
      "    epoch          : 73\n",
      "    loss           : -463896.7902439024\n",
      "    val_loss       : -461928.4903320313\n",
      "Train Epoch: 74 [256/54000 (0%)] Loss: -453610.125000\n",
      "Train Epoch: 74 [4352/54000 (8%)] Loss: -445830.718750\n",
      "Train Epoch: 74 [8448/54000 (16%)] Loss: -455105.531250\n",
      "Train Epoch: 74 [12544/54000 (23%)] Loss: -445574.187500\n",
      "Train Epoch: 74 [16640/54000 (31%)] Loss: -463392.281250\n",
      "Train Epoch: 74 [20736/54000 (38%)] Loss: -449229.250000\n",
      "Train Epoch: 74 [24832/54000 (46%)] Loss: -449743.781250\n",
      "Train Epoch: 74 [28928/54000 (54%)] Loss: -511182.968750\n",
      "Train Epoch: 74 [33024/54000 (61%)] Loss: -452891.093750\n",
      "Train Epoch: 74 [37120/54000 (69%)] Loss: -469827.125000\n",
      "Train Epoch: 74 [41216/54000 (76%)] Loss: -451902.312500\n",
      "Train Epoch: 74 [45312/54000 (84%)] Loss: -462410.781250\n",
      "Train Epoch: 74 [49408/54000 (91%)] Loss: -510373.406250\n",
      "    epoch          : 74\n",
      "    loss           : -463090.037804878\n",
      "    val_loss       : -461568.813671875\n",
      "Train Epoch: 75 [256/54000 (0%)] Loss: -475853.812500\n",
      "Train Epoch: 75 [4352/54000 (8%)] Loss: -453410.750000\n",
      "Train Epoch: 75 [8448/54000 (16%)] Loss: -444953.531250\n",
      "Train Epoch: 75 [12544/54000 (23%)] Loss: -469459.062500\n",
      "Train Epoch: 75 [16640/54000 (31%)] Loss: -459584.750000\n",
      "Train Epoch: 75 [20736/54000 (38%)] Loss: -475491.250000\n",
      "Train Epoch: 75 [24832/54000 (46%)] Loss: -469401.687500\n",
      "Train Epoch: 75 [28928/54000 (54%)] Loss: -450611.968750\n",
      "Train Epoch: 75 [33024/54000 (61%)] Loss: -460662.812500\n",
      "Train Epoch: 75 [37120/54000 (69%)] Loss: -453671.093750\n",
      "Train Epoch: 75 [41216/54000 (76%)] Loss: -474073.375000\n",
      "Train Epoch: 75 [45312/54000 (84%)] Loss: -456569.000000\n",
      "Train Epoch: 75 [49408/54000 (91%)] Loss: -465201.875000\n",
      "    epoch          : 75\n",
      "    loss           : -463765.66143292683\n",
      "    val_loss       : -460936.16748046875\n",
      "Train Epoch: 76 [256/54000 (0%)] Loss: -512659.875000\n",
      "Train Epoch: 76 [4352/54000 (8%)] Loss: -446757.531250\n",
      "Train Epoch: 76 [8448/54000 (16%)] Loss: -452749.593750\n",
      "Train Epoch: 76 [12544/54000 (23%)] Loss: -442414.125000\n",
      "Train Epoch: 76 [16640/54000 (31%)] Loss: -460621.250000\n",
      "Train Epoch: 76 [20736/54000 (38%)] Loss: -477705.187500\n",
      "Train Epoch: 76 [24832/54000 (46%)] Loss: -465809.093750\n",
      "Train Epoch: 76 [28928/54000 (54%)] Loss: -448602.062500\n",
      "Train Epoch: 76 [33024/54000 (61%)] Loss: -468853.687500\n",
      "Train Epoch: 76 [37120/54000 (69%)] Loss: -470416.031250\n",
      "Train Epoch: 76 [41216/54000 (76%)] Loss: -453168.750000\n",
      "Train Epoch: 76 [45312/54000 (84%)] Loss: -443987.531250\n",
      "Train Epoch: 76 [49408/54000 (91%)] Loss: -445789.312500\n",
      "    epoch          : 76\n",
      "    loss           : -463525.0013719512\n",
      "    val_loss       : -459742.6831542969\n",
      "Train Epoch: 77 [256/54000 (0%)] Loss: -510914.531250\n",
      "Train Epoch: 77 [4352/54000 (8%)] Loss: -470100.437500\n",
      "Train Epoch: 77 [8448/54000 (16%)] Loss: -456057.093750\n",
      "Train Epoch: 77 [12544/54000 (23%)] Loss: -456505.500000\n",
      "Train Epoch: 77 [16640/54000 (31%)] Loss: -447265.937500\n",
      "Train Epoch: 77 [20736/54000 (38%)] Loss: -454488.437500\n",
      "Train Epoch: 77 [24832/54000 (46%)] Loss: -469502.218750\n",
      "Train Epoch: 77 [28928/54000 (54%)] Loss: -453515.500000\n",
      "Train Epoch: 77 [33024/54000 (61%)] Loss: -443618.531250\n",
      "Train Epoch: 77 [37120/54000 (69%)] Loss: -511970.375000\n",
      "Train Epoch: 77 [41216/54000 (76%)] Loss: -464841.187500\n",
      "Train Epoch: 77 [45312/54000 (84%)] Loss: -455095.937500\n",
      "Train Epoch: 77 [49408/54000 (91%)] Loss: -512141.000000\n",
      "    epoch          : 77\n",
      "    loss           : -464784.8987804878\n",
      "    val_loss       : -462011.35205078125\n",
      "Train Epoch: 78 [256/54000 (0%)] Loss: -474977.312500\n",
      "Train Epoch: 78 [4352/54000 (8%)] Loss: -464859.187500\n",
      "Train Epoch: 78 [8448/54000 (16%)] Loss: -445008.000000\n",
      "Train Epoch: 78 [12544/54000 (23%)] Loss: -454410.625000\n",
      "Train Epoch: 78 [16640/54000 (31%)] Loss: -457654.531250\n",
      "Train Epoch: 78 [20736/54000 (38%)] Loss: -473614.937500\n",
      "Train Epoch: 78 [24832/54000 (46%)] Loss: -457546.375000\n",
      "Train Epoch: 78 [28928/54000 (54%)] Loss: -450314.062500\n",
      "Train Epoch: 78 [33024/54000 (61%)] Loss: -461415.312500\n",
      "Train Epoch: 78 [37120/54000 (69%)] Loss: -471664.468750\n",
      "Train Epoch: 78 [41216/54000 (76%)] Loss: -465448.875000\n",
      "Train Epoch: 78 [45312/54000 (84%)] Loss: -468686.625000\n",
      "Train Epoch: 78 [49408/54000 (91%)] Loss: -511419.750000\n",
      "    epoch          : 78\n",
      "    loss           : -465122.8082317073\n",
      "    val_loss       : -462802.77568359376\n",
      "Train Epoch: 79 [256/54000 (0%)] Loss: -512079.968750\n",
      "Train Epoch: 79 [4352/54000 (8%)] Loss: -447825.375000\n",
      "Train Epoch: 79 [8448/54000 (16%)] Loss: -442899.250000\n",
      "Train Epoch: 79 [12544/54000 (23%)] Loss: -470460.593750\n",
      "Train Epoch: 79 [16640/54000 (31%)] Loss: -512867.093750\n",
      "Train Epoch: 79 [20736/54000 (38%)] Loss: -454893.500000\n",
      "Train Epoch: 79 [24832/54000 (46%)] Loss: -451956.500000\n",
      "Train Epoch: 79 [28928/54000 (54%)] Loss: -476331.437500\n",
      "Train Epoch: 79 [33024/54000 (61%)] Loss: -468451.250000\n",
      "Train Epoch: 79 [37120/54000 (69%)] Loss: -462332.718750\n",
      "Train Epoch: 79 [41216/54000 (76%)] Loss: -452778.593750\n",
      "Train Epoch: 79 [45312/54000 (84%)] Loss: -455946.968750\n",
      "Train Epoch: 79 [49408/54000 (91%)] Loss: -509370.187500\n",
      "    epoch          : 79\n",
      "    loss           : -463838.05853658536\n",
      "    val_loss       : -459145.00842285156\n",
      "Train Epoch: 80 [256/54000 (0%)] Loss: -511818.875000\n",
      "Train Epoch: 80 [4352/54000 (8%)] Loss: -462653.437500\n",
      "Train Epoch: 80 [8448/54000 (16%)] Loss: -455528.500000\n",
      "Train Epoch: 80 [12544/54000 (23%)] Loss: -468006.250000\n",
      "Train Epoch: 80 [16640/54000 (31%)] Loss: -471855.750000\n",
      "Train Epoch: 80 [20736/54000 (38%)] Loss: -475996.687500\n",
      "Train Epoch: 80 [24832/54000 (46%)] Loss: -445475.812500\n",
      "Train Epoch: 80 [28928/54000 (54%)] Loss: -451283.750000\n",
      "Train Epoch: 80 [33024/54000 (61%)] Loss: -455420.562500\n",
      "Train Epoch: 80 [37120/54000 (69%)] Loss: -509925.937500\n",
      "Train Epoch: 80 [41216/54000 (76%)] Loss: -452553.500000\n",
      "Train Epoch: 80 [45312/54000 (84%)] Loss: -451726.750000\n",
      "Train Epoch: 80 [49408/54000 (91%)] Loss: -444594.750000\n",
      "    epoch          : 80\n",
      "    loss           : -464184.5356707317\n",
      "    val_loss       : -463141.9120117187\n",
      "Train Epoch: 81 [256/54000 (0%)] Loss: -461166.781250\n",
      "Train Epoch: 81 [4352/54000 (8%)] Loss: -458845.000000\n",
      "Train Epoch: 81 [8448/54000 (16%)] Loss: -445938.843750\n",
      "Train Epoch: 81 [12544/54000 (23%)] Loss: -468072.375000\n",
      "Train Epoch: 81 [16640/54000 (31%)] Loss: -476590.187500\n",
      "Train Epoch: 81 [20736/54000 (38%)] Loss: -448487.781250\n",
      "Train Epoch: 81 [24832/54000 (46%)] Loss: -465821.750000\n",
      "Train Epoch: 81 [28928/54000 (54%)] Loss: -509750.375000\n",
      "Train Epoch: 81 [33024/54000 (61%)] Loss: -445053.031250\n",
      "Train Epoch: 81 [37120/54000 (69%)] Loss: -446160.906250\n",
      "Train Epoch: 81 [41216/54000 (76%)] Loss: -465635.218750\n",
      "Train Epoch: 81 [45312/54000 (84%)] Loss: -470953.031250\n",
      "Train Epoch: 81 [49408/54000 (91%)] Loss: -511557.187500\n",
      "    epoch          : 81\n",
      "    loss           : -464135.0076219512\n",
      "    val_loss       : -461712.1583251953\n",
      "Train Epoch: 82 [256/54000 (0%)] Loss: -472514.625000\n",
      "Train Epoch: 82 [4352/54000 (8%)] Loss: -445781.937500\n",
      "Train Epoch: 82 [8448/54000 (16%)] Loss: -477108.000000\n",
      "Train Epoch: 82 [12544/54000 (23%)] Loss: -450616.187500\n",
      "Train Epoch: 82 [16640/54000 (31%)] Loss: -463762.437500\n",
      "Train Epoch: 82 [20736/54000 (38%)] Loss: -457381.843750\n",
      "Train Epoch: 82 [24832/54000 (46%)] Loss: -463195.375000\n",
      "Train Epoch: 82 [28928/54000 (54%)] Loss: -477687.000000\n",
      "Train Epoch: 82 [33024/54000 (61%)] Loss: -447818.906250\n",
      "Train Epoch: 82 [37120/54000 (69%)] Loss: -471175.250000\n",
      "Train Epoch: 82 [41216/54000 (76%)] Loss: -468009.625000\n",
      "Train Epoch: 82 [45312/54000 (84%)] Loss: -472694.187500\n",
      "Train Epoch: 82 [49408/54000 (91%)] Loss: -511552.125000\n",
      "    epoch          : 82\n",
      "    loss           : -466490.3617378049\n",
      "    val_loss       : -462794.54638671875\n",
      "Train Epoch: 83 [256/54000 (0%)] Loss: -510203.156250\n",
      "Train Epoch: 83 [4352/54000 (8%)] Loss: -449292.718750\n",
      "Train Epoch: 83 [8448/54000 (16%)] Loss: -456169.250000\n",
      "Train Epoch: 83 [12544/54000 (23%)] Loss: -462193.312500\n",
      "Train Epoch: 83 [16640/54000 (31%)] Loss: -450046.625000\n",
      "Train Epoch: 83 [20736/54000 (38%)] Loss: -477846.593750\n",
      "Train Epoch: 83 [24832/54000 (46%)] Loss: -456392.062500\n",
      "Train Epoch: 83 [28928/54000 (54%)] Loss: -510301.187500\n",
      "Train Epoch: 83 [33024/54000 (61%)] Loss: -463302.437500\n",
      "Train Epoch: 83 [37120/54000 (69%)] Loss: -456038.031250\n",
      "Train Epoch: 83 [41216/54000 (76%)] Loss: -452865.281250\n",
      "Train Epoch: 83 [45312/54000 (84%)] Loss: -473653.062500\n",
      "Train Epoch: 83 [49408/54000 (91%)] Loss: -452450.343750\n",
      "    epoch          : 83\n",
      "    loss           : -464962.70655487804\n",
      "    val_loss       : -460787.6403076172\n",
      "Train Epoch: 84 [256/54000 (0%)] Loss: -511992.312500\n",
      "Train Epoch: 84 [4352/54000 (8%)] Loss: -453323.593750\n",
      "Train Epoch: 84 [8448/54000 (16%)] Loss: -462568.750000\n",
      "Train Epoch: 84 [12544/54000 (23%)] Loss: -465260.781250\n",
      "Train Epoch: 84 [16640/54000 (31%)] Loss: -456442.562500\n",
      "Train Epoch: 84 [20736/54000 (38%)] Loss: -470185.875000\n",
      "Train Epoch: 84 [24832/54000 (46%)] Loss: -477410.656250\n",
      "Train Epoch: 84 [28928/54000 (54%)] Loss: -454866.875000\n",
      "Train Epoch: 84 [33024/54000 (61%)] Loss: -462861.187500\n",
      "Train Epoch: 84 [37120/54000 (69%)] Loss: -471197.000000\n",
      "Train Epoch: 84 [41216/54000 (76%)] Loss: -448496.218750\n",
      "Train Epoch: 84 [45312/54000 (84%)] Loss: -467707.125000\n",
      "Train Epoch: 84 [49408/54000 (91%)] Loss: -509924.187500\n",
      "    epoch          : 84\n",
      "    loss           : -464452.0089939024\n",
      "    val_loss       : -461324.0068359375\n",
      "Train Epoch: 85 [256/54000 (0%)] Loss: -511589.375000\n",
      "Train Epoch: 85 [4352/54000 (8%)] Loss: -469254.406250\n",
      "Train Epoch: 85 [8448/54000 (16%)] Loss: -454938.968750\n",
      "Train Epoch: 85 [12544/54000 (23%)] Loss: -472904.312500\n",
      "Train Epoch: 85 [16640/54000 (31%)] Loss: -469545.000000\n",
      "Train Epoch: 85 [20736/54000 (38%)] Loss: -478316.343750\n",
      "Train Epoch: 85 [24832/54000 (46%)] Loss: -451132.500000\n",
      "Train Epoch: 85 [28928/54000 (54%)] Loss: -451028.593750\n",
      "Train Epoch: 85 [33024/54000 (61%)] Loss: -446435.500000\n",
      "Train Epoch: 85 [37120/54000 (69%)] Loss: -512204.687500\n",
      "Train Epoch: 85 [41216/54000 (76%)] Loss: -455435.750000\n",
      "Train Epoch: 85 [45312/54000 (84%)] Loss: -454508.375000\n",
      "Train Epoch: 85 [49408/54000 (91%)] Loss: -510988.812500\n",
      "    epoch          : 85\n",
      "    loss           : -466103.94801829266\n",
      "    val_loss       : -463741.846484375\n",
      "Train Epoch: 86 [256/54000 (0%)] Loss: -512199.875000\n",
      "Train Epoch: 86 [4352/54000 (8%)] Loss: -451032.718750\n",
      "Train Epoch: 86 [8448/54000 (16%)] Loss: -475808.562500\n",
      "Train Epoch: 86 [12544/54000 (23%)] Loss: -469064.250000\n",
      "Train Epoch: 86 [16640/54000 (31%)] Loss: -460245.875000\n",
      "Train Epoch: 86 [20736/54000 (38%)] Loss: -477586.312500\n",
      "Train Epoch: 86 [24832/54000 (46%)] Loss: -459310.468750\n",
      "Train Epoch: 86 [28928/54000 (54%)] Loss: -508983.437500\n",
      "Train Epoch: 86 [33024/54000 (61%)] Loss: -456681.875000\n",
      "Train Epoch: 86 [37120/54000 (69%)] Loss: -450325.250000\n",
      "Train Epoch: 86 [41216/54000 (76%)] Loss: -454303.156250\n",
      "Train Epoch: 86 [45312/54000 (84%)] Loss: -446373.593750\n",
      "Train Epoch: 86 [49408/54000 (91%)] Loss: -510595.187500\n",
      "    epoch          : 86\n",
      "    loss           : -465261.0088414634\n",
      "    val_loss       : -462135.0462890625\n",
      "Train Epoch: 87 [256/54000 (0%)] Loss: -511922.906250\n",
      "Train Epoch: 87 [4352/54000 (8%)] Loss: -457454.843750\n",
      "Train Epoch: 87 [8448/54000 (16%)] Loss: -447893.937500\n",
      "Train Epoch: 87 [12544/54000 (23%)] Loss: -469747.281250\n",
      "Train Epoch: 87 [16640/54000 (31%)] Loss: -448515.906250\n",
      "Train Epoch: 87 [20736/54000 (38%)] Loss: -476179.312500\n",
      "Train Epoch: 87 [24832/54000 (46%)] Loss: -461979.531250\n",
      "Train Epoch: 87 [28928/54000 (54%)] Loss: -454711.718750\n",
      "Train Epoch: 87 [33024/54000 (61%)] Loss: -509678.718750\n",
      "Train Epoch: 87 [37120/54000 (69%)] Loss: -457522.437500\n",
      "Train Epoch: 87 [41216/54000 (76%)] Loss: -452390.687500\n",
      "Train Epoch: 87 [45312/54000 (84%)] Loss: -451682.250000\n",
      "Train Epoch: 87 [49408/54000 (91%)] Loss: -508317.562500\n",
      "    epoch          : 87\n",
      "    loss           : -463988.5693597561\n",
      "    val_loss       : -460940.17607421876\n",
      "Train Epoch: 88 [256/54000 (0%)] Loss: -511150.625000\n",
      "Train Epoch: 88 [4352/54000 (8%)] Loss: -445272.375000\n",
      "Train Epoch: 88 [8448/54000 (16%)] Loss: -466442.625000\n",
      "Train Epoch: 88 [12544/54000 (23%)] Loss: -456416.468750\n",
      "Train Epoch: 88 [16640/54000 (31%)] Loss: -472319.812500\n",
      "Train Epoch: 88 [20736/54000 (38%)] Loss: -479043.968750\n",
      "Train Epoch: 88 [24832/54000 (46%)] Loss: -457158.375000\n",
      "Train Epoch: 88 [28928/54000 (54%)] Loss: -453543.625000\n",
      "Train Epoch: 88 [33024/54000 (61%)] Loss: -476212.250000\n",
      "Train Epoch: 88 [37120/54000 (69%)] Loss: -457931.531250\n",
      "Train Epoch: 88 [41216/54000 (76%)] Loss: -466398.625000\n",
      "Train Epoch: 88 [45312/54000 (84%)] Loss: -460486.500000\n",
      "Train Epoch: 88 [49408/54000 (91%)] Loss: -511208.406250\n",
      "    epoch          : 88\n",
      "    loss           : -465294.0987804878\n",
      "    val_loss       : -460715.04821777344\n",
      "Train Epoch: 89 [256/54000 (0%)] Loss: -512015.250000\n",
      "Train Epoch: 89 [4352/54000 (8%)] Loss: -466978.500000\n",
      "Train Epoch: 89 [8448/54000 (16%)] Loss: -478663.500000\n",
      "Train Epoch: 89 [12544/54000 (23%)] Loss: -457426.250000\n",
      "Train Epoch: 89 [16640/54000 (31%)] Loss: -457923.781250\n",
      "Train Epoch: 89 [20736/54000 (38%)] Loss: -459029.687500\n",
      "Train Epoch: 89 [24832/54000 (46%)] Loss: -457423.062500\n",
      "Train Epoch: 89 [28928/54000 (54%)] Loss: -511477.031250\n",
      "Train Epoch: 89 [33024/54000 (61%)] Loss: -461290.187500\n",
      "Train Epoch: 89 [37120/54000 (69%)] Loss: -446757.781250\n",
      "Train Epoch: 89 [41216/54000 (76%)] Loss: -462314.218750\n",
      "Train Epoch: 89 [45312/54000 (84%)] Loss: -457097.343750\n",
      "Train Epoch: 89 [49408/54000 (91%)] Loss: -447744.437500\n",
      "    epoch          : 89\n",
      "    loss           : -466264.26463414636\n",
      "    val_loss       : -463774.46044921875\n",
      "Train Epoch: 90 [256/54000 (0%)] Loss: -461195.000000\n",
      "Train Epoch: 90 [4352/54000 (8%)] Loss: -469377.000000\n",
      "Train Epoch: 90 [8448/54000 (16%)] Loss: -479428.343750\n",
      "Train Epoch: 90 [12544/54000 (23%)] Loss: -469893.343750\n",
      "Train Epoch: 90 [16640/54000 (31%)] Loss: -455283.218750\n",
      "Train Epoch: 90 [20736/54000 (38%)] Loss: -477361.593750\n",
      "Train Epoch: 90 [24832/54000 (46%)] Loss: -460411.625000\n",
      "Train Epoch: 90 [28928/54000 (54%)] Loss: -455457.750000\n",
      "Train Epoch: 90 [33024/54000 (61%)] Loss: -476773.875000\n",
      "Train Epoch: 90 [37120/54000 (69%)] Loss: -473129.906250\n",
      "Train Epoch: 90 [41216/54000 (76%)] Loss: -448497.750000\n",
      "Train Epoch: 90 [45312/54000 (84%)] Loss: -462894.968750\n",
      "Train Epoch: 90 [49408/54000 (91%)] Loss: -510653.937500\n",
      "    epoch          : 90\n",
      "    loss           : -466792.274695122\n",
      "    val_loss       : -461792.29604492185\n",
      "Train Epoch: 91 [256/54000 (0%)] Loss: -459690.406250\n",
      "Train Epoch: 91 [4352/54000 (8%)] Loss: -466066.687500\n",
      "Train Epoch: 91 [8448/54000 (16%)] Loss: -456348.906250\n",
      "Train Epoch: 91 [12544/54000 (23%)] Loss: -508356.093750\n",
      "Train Epoch: 91 [16640/54000 (31%)] Loss: -454319.562500\n",
      "Train Epoch: 91 [20736/54000 (38%)] Loss: -476084.687500\n",
      "Train Epoch: 91 [24832/54000 (46%)] Loss: -454480.125000\n",
      "Train Epoch: 91 [28928/54000 (54%)] Loss: -477972.312500\n",
      "Train Epoch: 91 [33024/54000 (61%)] Loss: -456316.531250\n",
      "Train Epoch: 91 [37120/54000 (69%)] Loss: -456609.750000\n",
      "Train Epoch: 91 [41216/54000 (76%)] Loss: -477674.937500\n",
      "Train Epoch: 91 [45312/54000 (84%)] Loss: -448400.343750\n",
      "Train Epoch: 91 [49408/54000 (91%)] Loss: -511737.312500\n",
      "    epoch          : 91\n",
      "    loss           : -465816.54420731706\n",
      "    val_loss       : -463697.5263671875\n",
      "Train Epoch: 92 [256/54000 (0%)] Loss: -462703.750000\n",
      "Train Epoch: 92 [4352/54000 (8%)] Loss: -458329.000000\n",
      "Train Epoch: 92 [8448/54000 (16%)] Loss: -459259.312500\n",
      "Train Epoch: 92 [12544/54000 (23%)] Loss: -450605.312500\n",
      "Train Epoch: 92 [16640/54000 (31%)] Loss: -457736.562500\n",
      "Train Epoch: 92 [20736/54000 (38%)] Loss: -480216.156250\n",
      "Train Epoch: 92 [24832/54000 (46%)] Loss: -462955.562500\n",
      "Train Epoch: 92 [28928/54000 (54%)] Loss: -512030.250000\n",
      "Train Epoch: 92 [33024/54000 (61%)] Loss: -452694.687500\n",
      "Train Epoch: 92 [37120/54000 (69%)] Loss: -466068.125000\n",
      "Train Epoch: 92 [41216/54000 (76%)] Loss: -450532.906250\n",
      "Train Epoch: 92 [45312/54000 (84%)] Loss: -451319.187500\n",
      "Train Epoch: 92 [49408/54000 (91%)] Loss: -507609.625000\n",
      "    epoch          : 92\n",
      "    loss           : -465801.04954268294\n",
      "    val_loss       : -460335.2740234375\n",
      "Train Epoch: 93 [256/54000 (0%)] Loss: -447260.406250\n",
      "Train Epoch: 93 [4352/54000 (8%)] Loss: -464789.500000\n",
      "Train Epoch: 93 [8448/54000 (16%)] Loss: -476811.312500\n",
      "Train Epoch: 93 [12544/54000 (23%)] Loss: -446290.562500\n",
      "Train Epoch: 93 [16640/54000 (31%)] Loss: -444869.250000\n",
      "Train Epoch: 93 [20736/54000 (38%)] Loss: -455450.468750\n",
      "Train Epoch: 93 [24832/54000 (46%)] Loss: -449192.968750\n",
      "Train Epoch: 93 [28928/54000 (54%)] Loss: -510363.062500\n",
      "Train Epoch: 93 [33024/54000 (61%)] Loss: -510905.718750\n",
      "Train Epoch: 93 [37120/54000 (69%)] Loss: -511167.187500\n",
      "Train Epoch: 93 [41216/54000 (76%)] Loss: -456908.375000\n",
      "Train Epoch: 93 [45312/54000 (84%)] Loss: -461818.937500\n",
      "Train Epoch: 93 [49408/54000 (91%)] Loss: -508087.250000\n",
      "    epoch          : 93\n",
      "    loss           : -465879.0387195122\n",
      "    val_loss       : -460169.14848632814\n",
      "Train Epoch: 94 [256/54000 (0%)] Loss: -511153.906250\n",
      "Train Epoch: 94 [4352/54000 (8%)] Loss: -457435.000000\n",
      "Train Epoch: 94 [8448/54000 (16%)] Loss: -458677.562500\n",
      "Train Epoch: 94 [12544/54000 (23%)] Loss: -468673.937500\n",
      "Train Epoch: 94 [16640/54000 (31%)] Loss: -512175.312500\n",
      "Train Epoch: 94 [20736/54000 (38%)] Loss: -457052.250000\n",
      "Train Epoch: 94 [24832/54000 (46%)] Loss: -460057.593750\n",
      "Train Epoch: 94 [28928/54000 (54%)] Loss: -451990.062500\n",
      "Train Epoch: 94 [33024/54000 (61%)] Loss: -509832.500000\n",
      "Train Epoch: 94 [37120/54000 (69%)] Loss: -507171.500000\n",
      "Train Epoch: 94 [41216/54000 (76%)] Loss: -467060.906250\n",
      "Train Epoch: 94 [45312/54000 (84%)] Loss: -469602.625000\n",
      "Train Epoch: 94 [49408/54000 (91%)] Loss: -454798.750000\n",
      "    epoch          : 94\n",
      "    loss           : -464710.6631097561\n",
      "    val_loss       : -462758.88935546874\n",
      "Train Epoch: 95 [256/54000 (0%)] Loss: -512158.312500\n",
      "Train Epoch: 95 [4352/54000 (8%)] Loss: -449065.031250\n",
      "Train Epoch: 95 [8448/54000 (16%)] Loss: -469473.812500\n",
      "Train Epoch: 95 [12544/54000 (23%)] Loss: -463187.218750\n",
      "Train Epoch: 95 [16640/54000 (31%)] Loss: -462791.312500\n",
      "Train Epoch: 95 [20736/54000 (38%)] Loss: -455765.875000\n",
      "Train Epoch: 95 [24832/54000 (46%)] Loss: -471964.062500\n",
      "Train Epoch: 95 [28928/54000 (54%)] Loss: -474751.250000\n",
      "Train Epoch: 95 [33024/54000 (61%)] Loss: -444774.343750\n",
      "Train Epoch: 95 [37120/54000 (69%)] Loss: -455604.156250\n",
      "Train Epoch: 95 [41216/54000 (76%)] Loss: -457855.031250\n",
      "Train Epoch: 95 [45312/54000 (84%)] Loss: -457411.843750\n",
      "Train Epoch: 95 [49408/54000 (91%)] Loss: -510974.187500\n",
      "    epoch          : 95\n",
      "    loss           : -465603.6382621951\n",
      "    val_loss       : -463209.6268798828\n",
      "Train Epoch: 96 [256/54000 (0%)] Loss: -513432.500000\n",
      "Train Epoch: 96 [4352/54000 (8%)] Loss: -448493.468750\n",
      "Train Epoch: 96 [8448/54000 (16%)] Loss: -458519.812500\n",
      "Train Epoch: 96 [12544/54000 (23%)] Loss: -474285.468750\n",
      "Train Epoch: 96 [16640/54000 (31%)] Loss: -512537.218750\n",
      "Train Epoch: 96 [20736/54000 (38%)] Loss: -478320.437500\n",
      "Train Epoch: 96 [24832/54000 (46%)] Loss: -467076.187500\n",
      "Train Epoch: 96 [28928/54000 (54%)] Loss: -509757.437500\n",
      "Train Epoch: 96 [33024/54000 (61%)] Loss: -458301.500000\n",
      "Train Epoch: 96 [37120/54000 (69%)] Loss: -465679.562500\n",
      "Train Epoch: 96 [41216/54000 (76%)] Loss: -455434.937500\n",
      "Train Epoch: 96 [45312/54000 (84%)] Loss: -457832.593750\n",
      "Train Epoch: 96 [49408/54000 (91%)] Loss: -511549.906250\n",
      "    epoch          : 96\n",
      "    loss           : -467126.5539634146\n",
      "    val_loss       : -463856.765234375\n",
      "Train Epoch: 97 [256/54000 (0%)] Loss: -459908.125000\n",
      "Train Epoch: 97 [4352/54000 (8%)] Loss: -469510.468750\n",
      "Train Epoch: 97 [8448/54000 (16%)] Loss: -479413.968750\n",
      "Train Epoch: 97 [12544/54000 (23%)] Loss: -471854.875000\n",
      "Train Epoch: 97 [16640/54000 (31%)] Loss: -456725.593750\n",
      "Train Epoch: 97 [20736/54000 (38%)] Loss: -455464.656250\n",
      "Train Epoch: 97 [24832/54000 (46%)] Loss: -464274.812500\n",
      "Train Epoch: 97 [28928/54000 (54%)] Loss: -511919.375000\n",
      "Train Epoch: 97 [33024/54000 (61%)] Loss: -447069.312500\n",
      "Train Epoch: 97 [37120/54000 (69%)] Loss: -509629.843750\n",
      "Train Epoch: 97 [41216/54000 (76%)] Loss: -460619.968750\n",
      "Train Epoch: 97 [45312/54000 (84%)] Loss: -450449.500000\n",
      "Train Epoch: 97 [49408/54000 (91%)] Loss: -510108.437500\n",
      "    epoch          : 97\n",
      "    loss           : -465680.28658536583\n",
      "    val_loss       : -463298.30693359376\n",
      "Train Epoch: 98 [256/54000 (0%)] Loss: -513701.375000\n",
      "Train Epoch: 98 [4352/54000 (8%)] Loss: -450371.875000\n",
      "Train Epoch: 98 [8448/54000 (16%)] Loss: -458424.812500\n",
      "Train Epoch: 98 [12544/54000 (23%)] Loss: -470710.250000\n",
      "Train Epoch: 98 [16640/54000 (31%)] Loss: -473739.500000\n",
      "Train Epoch: 98 [20736/54000 (38%)] Loss: -477691.593750\n",
      "Train Epoch: 98 [24832/54000 (46%)] Loss: -450944.843750\n",
      "Train Epoch: 98 [28928/54000 (54%)] Loss: -511154.156250\n",
      "Train Epoch: 98 [33024/54000 (61%)] Loss: -451646.843750\n",
      "Train Epoch: 98 [37120/54000 (69%)] Loss: -477208.125000\n",
      "Train Epoch: 98 [41216/54000 (76%)] Loss: -469165.062500\n",
      "Train Epoch: 98 [45312/54000 (84%)] Loss: -474285.687500\n",
      "Train Epoch: 98 [49408/54000 (91%)] Loss: -510444.312500\n",
      "    epoch          : 98\n",
      "    loss           : -467086.0368902439\n",
      "    val_loss       : -463427.873046875\n",
      "Train Epoch: 99 [256/54000 (0%)] Loss: -512272.031250\n",
      "Train Epoch: 99 [4352/54000 (8%)] Loss: -457687.812500\n",
      "Train Epoch: 99 [8448/54000 (16%)] Loss: -465295.750000\n",
      "Train Epoch: 99 [12544/54000 (23%)] Loss: -471079.906250\n",
      "Train Epoch: 99 [16640/54000 (31%)] Loss: -449341.031250\n",
      "Train Epoch: 99 [20736/54000 (38%)] Loss: -477187.812500\n",
      "Train Epoch: 99 [24832/54000 (46%)] Loss: -450673.468750\n",
      "Train Epoch: 99 [28928/54000 (54%)] Loss: -477188.687500\n",
      "Train Epoch: 99 [33024/54000 (61%)] Loss: -468654.562500\n",
      "Train Epoch: 99 [37120/54000 (69%)] Loss: -463781.625000\n",
      "Train Epoch: 99 [41216/54000 (76%)] Loss: -451379.687500\n",
      "Train Epoch: 99 [45312/54000 (84%)] Loss: -477027.312500\n",
      "Train Epoch: 99 [49408/54000 (91%)] Loss: -446124.000000\n",
      "    epoch          : 99\n",
      "    loss           : -466734.7737804878\n",
      "    val_loss       : -463869.5712890625\n",
      "Train Epoch: 100 [256/54000 (0%)] Loss: -512491.375000\n",
      "Train Epoch: 100 [4352/54000 (8%)] Loss: -468879.343750\n",
      "Train Epoch: 100 [8448/54000 (16%)] Loss: -458928.562500\n",
      "Train Epoch: 100 [12544/54000 (23%)] Loss: -449087.312500\n",
      "Train Epoch: 100 [16640/54000 (31%)] Loss: -510053.781250\n",
      "Train Epoch: 100 [20736/54000 (38%)] Loss: -479846.000000\n",
      "Train Epoch: 100 [24832/54000 (46%)] Loss: -471807.937500\n",
      "Train Epoch: 100 [28928/54000 (54%)] Loss: -452557.031250\n",
      "Train Epoch: 100 [33024/54000 (61%)] Loss: -458486.218750\n",
      "Train Epoch: 100 [37120/54000 (69%)] Loss: -471990.593750\n",
      "Train Epoch: 100 [41216/54000 (76%)] Loss: -478192.250000\n",
      "Train Epoch: 100 [45312/54000 (84%)] Loss: -478375.281250\n",
      "Train Epoch: 100 [49408/54000 (91%)] Loss: -455812.625000\n",
      "    epoch          : 100\n",
      "    loss           : -467051.41493902437\n",
      "    val_loss       : -461274.8796875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0702_131016/checkpoint-epoch100.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAECategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=96, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=96, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=160, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=160, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=160, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=160, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=288, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=288, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=288, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=288, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=424, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=424, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_3_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=424, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=424, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=192, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=192, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=192, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=192, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=320, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=320, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=320, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=456, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=456, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_6_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=456, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=456, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=384, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=384, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=520, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=520, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_8_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=520, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=520, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=648, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=648, out_features=784, bias=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_9_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=648, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=648, out_features=512, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0_0): StandardNormal()\n",
       "    (global_element_1_0): StandardNormal()\n",
       "    (global_element_2_0): StandardNormal()\n",
       "    (global_element_3_0): StandardNormal()\n",
       "    (global_element_4_0): StandardNormal()\n",
       "  )\n",
       "  (guide_embedding): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=49, bias=True)\n",
       "    (1): BatchNorm1d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=49, out_features=49, bias=True)\n",
       "    (4): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (guide_confidences): Sequential(\n",
       "    (0): Linear(in_features=49, out_features=2, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAATY0lEQVR4nO3df4zcdZ3H8eer221Lf0FrbekvqSJIK3iF1GIOcoFwKGI88BKMvdOUO7ReIv6IxNPgXaiXu9Az4q87JanS0AKCnMBBlLsDyiFRA7qUUoogFCj0x9IWerSl0na7+74/5lszXff7me3O7M7sfl6PZLIz857vfN8znVe/35nvj48iAjMb+UY1uwEzGxoOu1kmHHazTDjsZplw2M0y4bCbZcJhH2EkLZd08wCnfZekxyXtk/S5RvfWaJLeJukNSW3N7mU4cNgbRNK5kn4laY+k3ZJ+Kem9ze7rGP098FBETIqI7za7mVoi4uWImBgR3c3uZThw2BtA0mTgp8C/AVOB2cDXgIPN7GsATgKeKiu20hJU0uhmTj8cOeyNcSpARNwaEd0R8WZE3BcRGwAknSzpQUmvSXpV0i2STjgysaTNkr4kaYOk/ZJukDRD0n8Vq9QPSJpSPHaepJC0TNJ2SZ2SriprTNL7ijWO1yU9Iem8ksc9CJwP/HuxanyqpBslXS/pXkn7gfMlHS9pjaRdkl6S9A+SRhXPcXmxRvOtYn4vSPrT4v4tknZKWpro9SFJ10r6dbGGdLekqb1e9xWSXgYerLpvdPGYWZLuKdasNkn6VNVzL5f0E0k3S9oLXN6vf9mRJCJ8qfMCTAZeA1YDHwSm9Kq/E7gQGAu8FXgY+HZVfTPwCDCDylrBTmAdcGYxzYPANcVj5wEB3ApMAM4AdgF/XtSXAzcX12cXfV1M5T/2C4vbby15HQ8Bn6y6fSOwBzinmH4csAa4G5hU9PIscEXx+MuBw8DfAG3APwMvA98rXsf7gX3AxMT8twGnF6/tjqrXcuR1rylqx1XdN7p4zM+B7xd9Lizelwuq3pcu4NLitRzX7M/NkH9Om93ASLkA84twbC0+8PcAM0oeeynweNXtzcBfV92+A7i+6vZngf8srh/5gJ9WVf86cENxvTrsXwZu6jXv/wGWlvTVV9jXVN1uo/LVZEHVfZ+m8j3/SNifq6qdUfQ6o+q+14CFifmvqLq9ADhUzPfI635HVf0PYQfmAt3ApKr6tcCNVe/Lw83+nDTz4tX4BomIpyPi8oiYQ2XJNAv4NoCk6ZJuk7StWIW8GZjW6yl2VF1/s4/bE3s9fkvV9ZeK+fV2EnBZsUr9uqTXgXOBmcfw0qrnMw0YU8yvet6zq2737puIqPVayub3EtDO0e/VFvo2C9gdEfsSvZVNmwWHfRBExDNUloqnF3ddS2UJ9J6ImAx8HFCds5lbdf1twPY+HrOFypL9hKrLhIhYcQzzqT4s8lUqq8In9Zr3tmN4vlp6v66uYr599VNtOzBV0qREb1kf4umwN4Ck0yRdJWlOcXsusITK93CofL99A3hd0mzgSw2Y7T9KGi/p3VS+I/+4j8fcDHxY0gcktUkaJ+m8I30eq6hs4rod+BdJkySdBHyxmE+jfFzSAknjgX8CfhL92LQWEVuAXwHXFq/zPcAVwC0N7G1Yc9gbYx9wNvBo8av1I8BG4Miv5F8DzqLyY9fPgDsbMM+fA5uAtcA3IuK+3g8oAnAJcDWVH6u2UPmPpp5/988C+4EXgF8APwJW1fF8vd1EZa3oFSo/tB3Lzj1LqHyP3w7cReVHzfsb2NuwpuLHCxsmJM0DXgTaI+Jwc7tpLEkPUflx8YfN7mUk8pLdLBMOu1kmvBpvlgkv2c0yMaQHA4zR2BjHhKGcpVlWDrCfQ3Gwz3046j1y6CLgO1R2Z/xhrZ01xjGBs3VBPbM0s4RHY21pbcCr8cXhjt+jcuDHAmCJpAUDfT4zG1z1fGdfDGyKiBci4hBwG5UdOMysBdUT9tkcfWDBVo4+6ACA4rjrDkkdXcPuXA5mI0c9Ye/rR4A/2o4XESsjYlFELGpnbB2zM7N61BP2rRx9hNIc+j7yysxaQD1h/w1wiqS3SxoDfIzKCRvMrAUNeNNbRByWdCWVM5+0AasiovRkhWbWXHVtZ4+Ie4F7G9SLmQ0i7y5rlgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZqGsUV+uftsmTk/WeNw8k69HdPfCZ99QxrY0odYVd0mZgH9ANHI6IRY1oyswarxFL9vMj4tUGPI+ZDSJ/ZzfLRL1hD+A+SY9JWtbXAyQtk9QhqaOLg3XOzswGqt7V+HMiYruk6cD9kp6JiIerHxARK4GVAJM1Neqcn5kNUF1L9ojYXvzdCdwFLG5EU2bWeAMOu6QJkiYduQ68H9jYqMbMrLHqWY2fAdwl6cjz/Cgi/rshXbUgjS5/q074eXo7+pmTtyTrXdGWrO85fFyyftczf1JaO+HB9LTTHt+brI/al94HgD37kuWe13aX1uLw4fRzW0MNOOwR8QJQ/ikzs5biTW9mmXDYzTLhsJtlwmE3y4TDbpYJH+LaX23lm8ceffbtyUlfOnFKsv6Xc9cn6y/un5Osz13TXlo77pHfJqfteWN/sh5jx6anP+PkZL17Qbr3lK3njUvWx5Zv1QNgyrNdpbVxDzyRnDa6DqWffBjykt0sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4S3s/dTHCw/pdaCq7clp923+G3J+vc/fH6yPv9f0xuUxzy/rrTWXeeppGudxnrU+mfT9fbyfQBqmT55frL+rqufStYnjC7/N3tkavpEyMff8kiyPhx5yW6WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcLb2Rvg8Cs7kvUJD6RPt7xgXfp49/j974+5p0ZR4jh+gJ5D5ceMA3CgxqmoE8b/rHz/AYDHp783WZ+25OXS2vHPN+89bRYv2c0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTHg7eyNEJMs9NbaTa1763Oov/m36ePjxr5TPf8adm5LTdieGVIbmDqtca94z1nYm61s+VD6U9uzfp88L35OsDk81l+ySVknaKWlj1X1TJd0v6bnib3qvEDNruv6sxt8IXNTrvq8AayPiFGBtcdvMWljNsEfEw0Dvdb1LgNXF9dXApQ3uy8wabKA/0M2IiE6A4u/0sgdKWiapQ1JHF+XnBDOzwTXov8ZHxMqIWBQRi9pJDxJoZoNnoGHfIWkmQPF3Z+NaMrPBMNCw3wMsLa4vBe5uTDtmNlhqbmeXdCtwHjBN0lbgGmAFcLukK4CXgcsGs8lhr8Z2eLa9kixf81f3Jet7e44rra3a/xfJaY//UY1BzlvYm++clqzv35z4eG/a0OBuWl/NsEfEkpLSBQ3uxcwGkXeXNcuEw26WCYfdLBMOu1kmHHazTPgQ1xbQ/fqeZH31ZR9I1g/MmlRam9rxXHLaGJMeUrnnQH1DPtej7YTjk/VtZ41J1k/+j/JDi2sddjwSeclulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XC29mHgZ4NzyTrYxJHazZvK3lF25TyEw+/+Ln5yWkPnpwe7nn+V8uHZAY4vHVbsp4bL9nNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0x4O7vVpe0tU5P16x77aWnt1Pa1yWk/9LsPJ+s9r+9N1muewjszXrKbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZpnwdnZLGn3ijGT9ggfS56WfP2b8gOfduXdysj7zwK4BP3eOai7ZJa2StFPSxqr7lkvaJml9cbl4cNs0s3r1ZzX+RuCiPu7/VkQsLC73NrYtM2u0mmGPiIeB3UPQi5kNonp+oLtS0oZiNb/0RGOSlknqkNTRxcE6Zmdm9Rho2K8HTgYWAp3AdWUPjIiVEbEoIha1M3aAszOzeg0o7BGxIyK6I6IH+AGwuLFtmVmjDSjskmZW3fwIsLHssWbWGmpuZ5d0K3AeME3SVuAa4DxJC4EANgOfHsQerR6j2pLlHVeenazvWXA4Wd909zuS9W0XPVpau2TKuuS0+/ePS9bt2NQMe0Qs6ePuGwahFzMbRN5d1iwTDrtZJhx2s0w47GaZcNjNMuFDXEeA1OmcP/bLxHjOwLwxjyXrK86+MFnvfvXVZP2p5e2ltY6LliWnHXVOerMho5Su21G8ZDfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuHt7MNA2wnHJ+srHis/3+f89vLt3ACnPfjJZP2dux5P1muJrkOltePueyI57eTpZyXrGp3++MZBnwatmpfsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmvJ29FdQ63fNN6WGT57b1lNY2daW3NZ/6d88m6+XPXL84VL4NHmDiK+nTWGvm9PQMnt+cmHmkpx2BvGQ3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLRnyGb5wJrgBOpbHZdGRHfkTQV+DEwj8qwzR+NiP8bvFZHsMXvTpavW3Bzsr6lu/z/7M988fPJacfvLx9SedDV2NY9fvPeZL1nysS6nj83/VmyHwauioj5wPuAz0haAHwFWBsRpwBri9tm1qJqhj0iOiNiXXF9H/A0MBu4BFhdPGw1cOlgNWlm9Tum7+yS5gFnAo8CMyKiEyr/IQA19l00s2bqd9glTQTuAL4QEekvU0dPt0xSh6SOLnxOMLNm6VfYJbVTCfotEXFncfcOSTOL+kxgZ1/TRsTKiFgUEYvaGduIns1sAGqGXZKAG4CnI+KbVaV7gKXF9aXA3Y1vz8wapT+HuJ4DfAJ4UtL64r6rgRXA7ZKuAF4GLhucFke+tj1vJuu3716crK/bNae0NuX+3yanHcxDWOt2MH0I7Kjtfa5M/kF3I3sZAWqGPSJ+AZQNhH1BY9sxs8HiPejMMuGwm2XCYTfLhMNulgmH3SwTDrtZJnwq6RbQ/cymZP3xb56drO8+o2zLKEzp3j6gnoZC25Qpyfr2D85M1mf+oLOR7Yx4XrKbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZpnwdvZWUOOUx5NvS5/uecoD00prmjwpPesawybH4fSwyah8Gz/A6JPmltZeO3dW+qlrHJBeszc7ipfsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmvJ19OKixHT727Sutdb/nlOS03afNTtb3zxqTrL92eno7+/jO8vqsm55KTtu9941kPXp8Zvhj4SW7WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpaJmtvZJc0F1gAnUhnOe2VEfEfScuBTwK7ioVdHxL2D1aiV6zlwoLz46yeT07bVeO6pc9Lb4Sd0zkjW2zueK611J/YPsMbrz041h4GrImKdpEnAY5LuL2rfiohvDF57ZtYoNcMeEZ1AZ3F9n6SngfR/92bWco7pO7ukecCZwJHzJF0paYOkVZL6HMtH0jJJHZI6ujhYV7NmNnD9DrukicAdwBciYi9wPXAysJDKkv+6vqaLiJURsSgiFrUztgEtm9lA9CvsktqpBP2WiLgTICJ2RER3RPQAPwAWD16bZlavmmGXJOAG4OmI+GbV/dVDbH4E2Nj49sysUfrza/w5wCeAJyWtL+67GlgiaSEQwGbg04PSoTXV4a3bkvW2GvWeRjZjdenPr/G/APo6KNnb1M2GEe9BZ5YJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTKhqDEccENnJu0CXqq6axrw6pA1cGxatbdW7Qvc20A1sreTIuKtfRWGNOx/NHOpIyIWNa2BhFbtrVX7Avc2UEPVm1fjzTLhsJtlotlhX9nk+ae0am+t2he4t4Eakt6a+p3dzIZOs5fsZjZEHHazTDQl7JIukvQ7SZskfaUZPZSRtFnSk5LWS+poci+rJO2UtLHqvqmS7pf0XPG3zzH2mtTbcknbivduvaSLm9TbXEn/K+lpSU9J+nxxf1Pfu0RfQ/K+Dfl3dkltwLPAhcBW4DfAkoj47ZA2UkLSZmBRRDR9BwxJfwa8AayJiNOL+74O7I6IFcV/lFMi4sst0tty4I1mD+NdjFY0s3qYceBS4HKa+N4l+vooQ/C+NWPJvhjYFBEvRMQh4Dbgkib00fIi4mFgd6+7LwFWF9dXU/mwDLmS3lpCRHRGxLri+j7gyDDjTX3vEn0NiWaEfTawper2VlprvPcA7pP0mKRlzW6mDzMiohMqHx5gepP76a3mMN5Dqdcw4y3z3g1k+PN6NSPsfQ0l1Urb/86JiLOADwKfKVZXrX/6NYz3UOljmPGWMNDhz+vVjLBvBeZW3Z4DbG9CH32KiO3F353AXbTeUNQ7joygW/zd2eR+/qCVhvHua5hxWuC9a+bw580I+2+AUyS9XdIY4GPAPU3o449ImlD8cIKkCcD7ab2hqO8BlhbXlwJ3N7GXo7TKMN5lw4zT5Peu6cOfR8SQX4CLqfwi/zzw1Wb0UNLXO4AnistTze4NuJXKal0XlTWiK4C3AGuB54q/U1uot5uAJ4ENVII1s0m9nUvlq+EGYH1xubjZ712iryF537y7rFkmvAedWSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpaJ/wdGOPPoNgcxKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
