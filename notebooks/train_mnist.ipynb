{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 100,\n",
    "    \"cooldown\": 100,\n",
    "    \"factor\": 0.5,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 1491.588135\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: 221.713593\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: 448.854736\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -120.617188\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -162.221588\n",
      "    epoch          : 1\n",
      "    loss           : 168.09768321726582\n",
      "    val_loss       : -143.97828626232223\n",
      "    val_log_likelihood: 532.6282119750977\n",
      "    val_log_marginal: 490.4593846000731\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -393.104370\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -264.909760\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -206.834625\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -498.586548\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -197.609711\n",
      "    epoch          : 2\n",
      "    loss           : -231.28924424577468\n",
      "    val_loss       : -261.6683377311565\n",
      "    val_log_likelihood: 554.8510482788085\n",
      "    val_log_marginal: 514.9086550660431\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -223.697189\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -300.125824\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -210.000122\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -452.323608\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -371.371948\n",
      "    epoch          : 3\n",
      "    loss           : -339.4396523768359\n",
      "    val_loss       : -368.3253571698442\n",
      "    val_log_likelihood: 568.1715255737305\n",
      "    val_log_marginal: 536.9889405485243\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -828.914551\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -438.050903\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -369.335632\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -270.849854\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -388.498932\n",
      "    epoch          : 4\n",
      "    loss           : -360.8207090774385\n",
      "    val_loss       : -405.20526164788754\n",
      "    val_log_likelihood: 535.2561248779297\n",
      "    val_log_marginal: 491.42668228298425\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -329.403412\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -640.984375\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -403.960083\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -185.230576\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -34.449348\n",
      "    epoch          : 5\n",
      "    loss           : -407.0217265138532\n",
      "    val_loss       : -312.6237294013612\n",
      "    val_log_likelihood: 454.1506652832031\n",
      "    val_log_marginal: 410.29986385963855\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -272.694733\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -312.464905\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -243.481552\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -428.192566\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -310.858521\n",
      "    epoch          : 6\n",
      "    loss           : -429.7113701282161\n",
      "    val_loss       : -437.9022336677648\n",
      "    val_log_likelihood: 594.8244354248047\n",
      "    val_log_marginal: 538.6112696625293\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -527.477173\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -466.046478\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -195.115204\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -282.613037\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -421.479889\n",
      "    epoch          : 7\n",
      "    loss           : -484.3095589061775\n",
      "    val_loss       : -493.28324737427755\n",
      "    val_log_likelihood: 659.2938018798828\n",
      "    val_log_marginal: 625.4691976744682\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -873.567505\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -322.931000\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -335.416687\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -807.929810\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -480.199554\n",
      "    epoch          : 8\n",
      "    loss           : -491.8190006596027\n",
      "    val_loss       : -517.9047041370534\n",
      "    val_log_likelihood: 696.2901489257813\n",
      "    val_log_marginal: 667.1089148357511\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -796.359009\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -511.875641\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -504.742798\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -223.072464\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -465.608215\n",
      "    epoch          : 9\n",
      "    loss           : -533.1938872384553\n",
      "    val_loss       : -536.6430296623148\n",
      "    val_log_likelihood: 633.9395629882813\n",
      "    val_log_marginal: 604.3117476038635\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -929.874512\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -374.234985\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -688.112183\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -230.365997\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -619.010376\n",
      "    epoch          : 10\n",
      "    loss           : -558.5585296933014\n",
      "    val_loss       : -553.3688727689907\n",
      "    val_log_likelihood: 659.3538681030274\n",
      "    val_log_marginal: 600.2138071447611\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -931.892029\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -677.249512\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -240.950760\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -577.016724\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -671.625366\n",
      "    epoch          : 11\n",
      "    loss           : -575.3283584141495\n",
      "    val_loss       : -651.8605641397647\n",
      "    val_log_likelihood: 750.0177215576172\n",
      "    val_log_marginal: 723.4933276012231\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -995.235413\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -558.451965\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -362.846497\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -917.310242\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -643.705444\n",
      "    epoch          : 12\n",
      "    loss           : -635.5339660644531\n",
      "    val_loss       : -657.9439986506477\n",
      "    val_log_likelihood: 788.1122924804688\n",
      "    val_log_marginal: 768.3982400570064\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -612.268127\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -709.753052\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -587.843079\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -289.122284\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -729.304016\n",
      "    epoch          : 13\n",
      "    loss           : -658.1718487125812\n",
      "    val_loss       : -661.3440563061274\n",
      "    val_log_likelihood: 769.9732666015625\n",
      "    val_log_marginal: 752.3732037778943\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -654.635559\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -651.464111\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -650.621826\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -364.714142\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -545.011597\n",
      "    epoch          : 14\n",
      "    loss           : -671.1560581320583\n",
      "    val_loss       : -666.7363129146397\n",
      "    val_log_likelihood: 776.8834075927734\n",
      "    val_log_marginal: 758.8730074822903\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -1000.147156\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -557.802307\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -883.872192\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -602.796143\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -808.168213\n",
      "    epoch          : 15\n",
      "    loss           : -685.3387139953009\n",
      "    val_loss       : -715.1837074758485\n",
      "    val_log_likelihood: 814.6692138671875\n",
      "    val_log_marginal: 792.0260932072997\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -1105.262939\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -676.979309\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -838.988770\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -835.695435\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -724.657959\n",
      "    epoch          : 16\n",
      "    loss           : -713.5160048078783\n",
      "    val_loss       : -747.6455832871609\n",
      "    val_log_likelihood: 879.6581420898438\n",
      "    val_log_marginal: 856.8784730233252\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -1098.604370\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -630.522949\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -511.871948\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -866.116394\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -813.596802\n",
      "    epoch          : 17\n",
      "    loss           : -754.3419527865873\n",
      "    val_loss       : -807.1402850813232\n",
      "    val_log_likelihood: 963.8250305175782\n",
      "    val_log_marginal: 941.0716923832894\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -1262.314209\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -673.722412\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -527.946838\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -990.245850\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -541.563599\n",
      "    epoch          : 18\n",
      "    loss           : -783.6467170337639\n",
      "    val_loss       : -859.8310363632627\n",
      "    val_log_likelihood: 980.9504547119141\n",
      "    val_log_marginal: 956.0195642717183\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -1118.469116\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -922.416382\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -954.510193\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -1043.235962\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -793.718384\n",
      "    epoch          : 19\n",
      "    loss           : -824.6551259862314\n",
      "    val_loss       : -883.3517034148797\n",
      "    val_log_likelihood: 1068.7063110351562\n",
      "    val_log_marginal: 1027.8360819920897\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -954.595398\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -781.494263\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -978.282104\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -874.346130\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -1086.772949\n",
      "    epoch          : 20\n",
      "    loss           : -817.3771727911317\n",
      "    val_loss       : -884.0323901517316\n",
      "    val_log_likelihood: 1166.75869140625\n",
      "    val_log_marginal: 1137.450443355739\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -1187.626587\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -823.525391\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -751.993896\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -725.868347\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -715.521362\n",
      "    epoch          : 21\n",
      "    loss           : -871.329664475847\n",
      "    val_loss       : -900.273236490041\n",
      "    val_log_likelihood: 1189.9790649414062\n",
      "    val_log_marginal: 1169.6405180435627\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -887.047974\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -619.955566\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -860.984009\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -934.397095\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -896.089111\n",
      "    epoch          : 22\n",
      "    loss           : -921.1289920618038\n",
      "    val_loss       : -933.4700704203918\n",
      "    val_log_likelihood: 1152.1515808105469\n",
      "    val_log_marginal: 1123.058167378977\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -1199.102661\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -874.248413\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -872.336365\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -1129.199707\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -877.211853\n",
      "    epoch          : 23\n",
      "    loss           : -934.777711773863\n",
      "    val_loss       : -1046.4957992789336\n",
      "    val_log_likelihood: 1238.4542541503906\n",
      "    val_log_marginal: 1213.6806870900095\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -1418.673706\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -993.228760\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -1146.326416\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -1070.541748\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -1181.625977\n",
      "    epoch          : 24\n",
      "    loss           : -954.9505458114171\n",
      "    val_loss       : -942.8128834471106\n",
      "    val_log_likelihood: 1187.774237060547\n",
      "    val_log_marginal: 1169.9592947527767\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -1277.970215\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -783.647400\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -1310.810669\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -887.939819\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -895.936340\n",
      "    epoch          : 25\n",
      "    loss           : -941.4279389334197\n",
      "    val_loss       : -960.3089727892541\n",
      "    val_log_likelihood: 1175.6816772460938\n",
      "    val_log_marginal: 1147.0354240115732\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -974.218384\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -827.772400\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -782.792603\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -1200.233032\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -932.960266\n",
      "    epoch          : 26\n",
      "    loss           : -920.6783960927831\n",
      "    val_loss       : -867.4718341123313\n",
      "    val_log_likelihood: 1167.8453979492188\n",
      "    val_log_marginal: 1138.720641781017\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -1511.143555\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -708.725403\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -645.802612\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -1424.895264\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -830.365051\n",
      "    epoch          : 27\n",
      "    loss           : -965.8101338301555\n",
      "    val_loss       : -971.4651866083034\n",
      "    val_log_likelihood: 1271.1903076171875\n",
      "    val_log_marginal: 1247.8018380293415\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -1137.729858\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -1126.292725\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -1251.534668\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -1465.684082\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -487.333862\n",
      "    epoch          : 28\n",
      "    loss           : -1012.1474576138033\n",
      "    val_loss       : -1010.0502509252168\n",
      "    val_log_likelihood: 1299.3921630859375\n",
      "    val_log_marginal: 1276.1091329745948\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -989.008606\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -1041.329346\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -837.384521\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -1290.899658\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -1287.059326\n",
      "    epoch          : 29\n",
      "    loss           : -990.3180109911626\n",
      "    val_loss       : -997.5489632518962\n",
      "    val_log_likelihood: 1299.6484619140624\n",
      "    val_log_marginal: 1281.1570046983659\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -1313.788574\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -725.968750\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -964.001099\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -1193.545532\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -964.052124\n",
      "    epoch          : 30\n",
      "    loss           : -1021.2166358267907\n",
      "    val_loss       : -1024.506550347805\n",
      "    val_log_likelihood: 1265.4442504882813\n",
      "    val_log_marginal: 1244.9115358404815\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -1344.292358\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -895.246338\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -490.485107\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -1085.536377\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -1148.387451\n",
      "    epoch          : 31\n",
      "    loss           : -1007.222132616704\n",
      "    val_loss       : -1079.0364107113332\n",
      "    val_log_likelihood: 1253.3331420898437\n",
      "    val_log_marginal: 1234.224342989549\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -1229.916748\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -931.433167\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -1079.589478\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -1397.386719\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -1011.587830\n",
      "    epoch          : 32\n",
      "    loss           : -1013.023720920676\n",
      "    val_loss       : -1017.6775131250731\n",
      "    val_log_likelihood: 1288.7634033203126\n",
      "    val_log_marginal: 1268.4315744113178\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -800.460632\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -1070.677856\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -956.545776\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -1280.131592\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -664.075195\n",
      "    epoch          : 33\n",
      "    loss           : -1029.1981793394184\n",
      "    val_loss       : -978.4388325452804\n",
      "    val_log_likelihood: 1318.7668334960938\n",
      "    val_log_marginal: 1295.3848210431636\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -1340.895386\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -523.457153\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -1021.267700\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -1075.982178\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -1142.379028\n",
      "    epoch          : 34\n",
      "    loss           : -1036.9497318078975\n",
      "    val_loss       : -948.763499241881\n",
      "    val_log_likelihood: 1328.9165161132812\n",
      "    val_log_marginal: 1303.9941656712442\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -1477.818481\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -1111.899414\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -1032.330811\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -1334.720459\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -854.937317\n",
      "    epoch          : 35\n",
      "    loss           : -1037.5277842908802\n",
      "    val_loss       : -993.058311067801\n",
      "    val_log_likelihood: 1316.2162658691407\n",
      "    val_log_marginal: 1291.288722749427\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -1495.890991\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -707.406738\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -872.985352\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -1528.538940\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -911.911621\n",
      "    epoch          : 36\n",
      "    loss           : -1000.8551554160543\n",
      "    val_loss       : -1066.7926693886518\n",
      "    val_log_likelihood: 1341.8490112304687\n",
      "    val_log_marginal: 1316.645642419532\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -1448.742676\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -1175.068604\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -1039.813965\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -914.030518\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -912.749390\n",
      "    epoch          : 37\n",
      "    loss           : -1047.1907342589727\n",
      "    val_loss       : -953.0448749919422\n",
      "    val_log_likelihood: 1281.7870971679688\n",
      "    val_log_marginal: 1235.4248646125197\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -1003.356689\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -1316.654541\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -757.638245\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -1031.744141\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -986.162354\n",
      "    epoch          : 38\n",
      "    loss           : -1051.5691667311262\n",
      "    val_loss       : -965.0470382024535\n",
      "    val_log_likelihood: 1309.5340209960937\n",
      "    val_log_marginal: 1287.1651817087084\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -1152.651001\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -964.272766\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -1142.339966\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -1127.997803\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -936.595276\n",
      "    epoch          : 39\n",
      "    loss           : -1060.1661056669632\n",
      "    val_loss       : -1065.5223175474443\n",
      "    val_log_likelihood: 1300.2632751464844\n",
      "    val_log_marginal: 1279.7103922192007\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -1487.256592\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -1046.168457\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -1065.601929\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -756.255615\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -1219.497559\n",
      "    epoch          : 40\n",
      "    loss           : -1074.618886211131\n",
      "    val_loss       : -1091.367458868958\n",
      "    val_log_likelihood: 1309.365771484375\n",
      "    val_log_marginal: 1287.7666892193258\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -1489.525146\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -1255.651611\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -836.387451\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -1503.103271\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -1126.054199\n",
      "    epoch          : 41\n",
      "    loss           : -1120.434178116298\n",
      "    val_loss       : -1145.4820954337715\n",
      "    val_log_likelihood: 1351.1359619140626\n",
      "    val_log_marginal: 1330.4933363229036\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -1492.514160\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -1138.442993\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -945.860474\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -951.312378\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -1134.889893\n",
      "    epoch          : 42\n",
      "    loss           : -1135.3117645565826\n",
      "    val_loss       : -1092.2489267687313\n",
      "    val_log_likelihood: 1363.3527954101562\n",
      "    val_log_marginal: 1340.5378994207829\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -1479.141602\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -808.534363\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -739.886108\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -1496.763794\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -1108.968018\n",
      "    epoch          : 43\n",
      "    loss           : -1129.9051145043704\n",
      "    val_loss       : -1190.9288562400266\n",
      "    val_log_likelihood: 1382.4993041992188\n",
      "    val_log_marginal: 1361.3788349654526\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -1516.329712\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -1100.789062\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -864.820496\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -828.340332\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -1327.829590\n",
      "    epoch          : 44\n",
      "    loss           : -1150.6548093285892\n",
      "    val_loss       : -1153.263156254217\n",
      "    val_log_likelihood: 1389.3565673828125\n",
      "    val_log_marginal: 1369.7164899654686\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -1450.038696\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -1117.340332\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -1077.306396\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -1138.004395\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -987.715332\n",
      "    epoch          : 45\n",
      "    loss           : -1184.55302021291\n",
      "    val_loss       : -1173.9222773165443\n",
      "    val_log_likelihood: 1375.38349609375\n",
      "    val_log_marginal: 1355.9267181411385\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -1401.612305\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -1071.084961\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -874.549500\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -1352.080078\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -1332.218628\n",
      "    epoch          : 46\n",
      "    loss           : -1188.0375390383276\n",
      "    val_loss       : -1173.90446958635\n",
      "    val_log_likelihood: 1354.5578979492188\n",
      "    val_log_marginal: 1332.3789446517826\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -1306.584106\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -961.022766\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -1105.934692\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -1271.926147\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -1139.678711\n",
      "    epoch          : 47\n",
      "    loss           : -1199.1750439936573\n",
      "    val_loss       : -1224.2259429515339\n",
      "    val_log_likelihood: 1362.9275024414062\n",
      "    val_log_marginal: 1342.5656575530768\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -1408.280151\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -1217.373657\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -1087.758057\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -1181.843506\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -1259.525024\n",
      "    epoch          : 48\n",
      "    loss           : -1202.515747674621\n",
      "    val_loss       : -1193.7100090629422\n",
      "    val_log_likelihood: 1341.5515380859374\n",
      "    val_log_marginal: 1317.9119687091559\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -1649.360107\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -1042.365479\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -965.971069\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -1320.041992\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -1348.086792\n",
      "    epoch          : 49\n",
      "    loss           : -1198.171401826462\n",
      "    val_loss       : -1222.3039219913073\n",
      "    val_log_likelihood: 1386.0869018554688\n",
      "    val_log_marginal: 1362.9427450947464\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -1698.554077\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -1069.894897\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -726.135132\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -1263.621582\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -1154.370361\n",
      "    epoch          : 50\n",
      "    loss           : -1202.1390840133818\n",
      "    val_loss       : -1218.8487017308362\n",
      "    val_log_likelihood: 1394.2852661132813\n",
      "    val_log_marginal: 1366.2618393085897\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -1450.548584\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -1389.859009\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -899.205200\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -1260.207153\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -1220.456787\n",
      "    epoch          : 51\n",
      "    loss           : -1211.5678245619972\n",
      "    val_loss       : -1184.0206775264814\n",
      "    val_log_likelihood: 1402.7889770507813\n",
      "    val_log_marginal: 1377.327307190001\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -1541.610962\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -1028.570068\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -962.560913\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -1246.561768\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -1213.641724\n",
      "    epoch          : 52\n",
      "    loss           : -1235.214638889426\n",
      "    val_loss       : -1236.0244269986638\n",
      "    val_log_likelihood: 1426.8719848632813\n",
      "    val_log_marginal: 1404.5956767313182\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -1597.718018\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -1332.968872\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -1087.205566\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -1406.527222\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -1088.979980\n",
      "    epoch          : 53\n",
      "    loss           : -1248.1447258373298\n",
      "    val_loss       : -1197.5531716816126\n",
      "    val_log_likelihood: 1348.659375\n",
      "    val_log_marginal: 1324.6967321395873\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -1481.295898\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -1488.068237\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -1152.419434\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -1025.158203\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -702.150208\n",
      "    epoch          : 54\n",
      "    loss           : -1230.043785775062\n",
      "    val_loss       : -1279.7591433033347\n",
      "    val_log_likelihood: 1348.2935180664062\n",
      "    val_log_marginal: 1329.6839579340071\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -1559.751831\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -1224.674438\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -1040.575439\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -1268.762451\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -1308.650269\n",
      "    epoch          : 55\n",
      "    loss           : -1217.0188072884437\n",
      "    val_loss       : -1268.4449482679368\n",
      "    val_log_likelihood: 1372.3100341796876\n",
      "    val_log_marginal: 1350.7931600954385\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -1535.849243\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -1257.139648\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -1239.134766\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -771.472290\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -785.808350\n",
      "    epoch          : 56\n",
      "    loss           : -1218.4030604598545\n",
      "    val_loss       : -1233.3775008860976\n",
      "    val_log_likelihood: 1429.6466430664063\n",
      "    val_log_marginal: 1407.5666376441718\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -1689.084473\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -1264.015137\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -1109.856689\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -1535.218384\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -1393.589600\n",
      "    epoch          : 57\n",
      "    loss           : -1248.9007882599783\n",
      "    val_loss       : -1183.3824139131234\n",
      "    val_log_likelihood: 1407.7104614257812\n",
      "    val_log_marginal: 1387.7283262681217\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -1591.366699\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -1279.949463\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -1207.890137\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -1698.698486\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -1374.937256\n",
      "    epoch          : 58\n",
      "    loss           : -1243.2942359848778\n",
      "    val_loss       : -1300.425059271045\n",
      "    val_log_likelihood: 1410.5787963867188\n",
      "    val_log_marginal: 1393.4427697215228\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -1582.440430\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -1291.399414\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -1431.208740\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -1311.373047\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -1345.602051\n",
      "    epoch          : 59\n",
      "    loss           : -1256.591187127746\n",
      "    val_loss       : -1330.6955700463616\n",
      "    val_log_likelihood: 1407.4423461914062\n",
      "    val_log_marginal: 1386.7559953268617\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -1705.700928\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -966.578613\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -1357.237061\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -1482.167725\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -1218.740234\n",
      "    epoch          : 60\n",
      "    loss           : -1267.1709872708461\n",
      "    val_loss       : -1245.8945630437695\n",
      "    val_log_likelihood: 1411.4587646484374\n",
      "    val_log_marginal: 1392.9013219360263\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -1453.246582\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -1204.422485\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -1313.613037\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -1406.026611\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -1111.351562\n",
      "    epoch          : 61\n",
      "    loss           : -1251.4269989315826\n",
      "    val_loss       : -1285.8331302878446\n",
      "    val_log_likelihood: 1405.4064575195312\n",
      "    val_log_marginal: 1381.8170344917114\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -1079.984741\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -1164.701538\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -1181.096436\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -1015.706543\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -1385.869507\n",
      "    epoch          : 62\n",
      "    loss           : -1256.2746128799893\n",
      "    val_loss       : -1222.4077227442526\n",
      "    val_log_likelihood: 1394.9083618164063\n",
      "    val_log_marginal: 1375.0296441957355\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -1689.658203\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -1036.178223\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -1333.088379\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -1133.804443\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -1109.106812\n",
      "    epoch          : 63\n",
      "    loss           : -1240.0211266243812\n",
      "    val_loss       : -1293.182596152462\n",
      "    val_log_likelihood: 1432.485546875\n",
      "    val_log_marginal: 1407.765877848491\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -1725.149902\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -1206.141113\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -1008.615112\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -1420.604126\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -1380.791504\n",
      "    epoch          : 64\n",
      "    loss           : -1245.2274586894725\n",
      "    val_loss       : -1237.3554342902266\n",
      "    val_log_likelihood: 1412.499169921875\n",
      "    val_log_marginal: 1389.7500540606677\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -1686.186646\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -1450.212402\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -1359.231689\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -1257.319336\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -1159.445312\n",
      "    epoch          : 65\n",
      "    loss           : -1271.4639946965888\n",
      "    val_loss       : -1186.0479081094265\n",
      "    val_log_likelihood: 1407.7935913085937\n",
      "    val_log_marginal: 1388.475724876672\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -1500.621948\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -1418.911987\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -1161.510986\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -1482.723389\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -1311.262085\n",
      "    epoch          : 66\n",
      "    loss           : -1241.9011792475635\n",
      "    val_loss       : -1193.0919366832823\n",
      "    val_log_likelihood: 1403.3967895507812\n",
      "    val_log_marginal: 1384.6558715309948\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -775.916382\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -1281.990845\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -1319.997070\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -1078.556763\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -1335.720215\n",
      "    epoch          : 67\n",
      "    loss           : -1228.032305122602\n",
      "    val_loss       : -1239.5876025913283\n",
      "    val_log_likelihood: 1394.1646484375\n",
      "    val_log_marginal: 1366.7172074969858\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -1477.352905\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -1096.698242\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -1426.283447\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -1212.640747\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -1335.784790\n",
      "    epoch          : 68\n",
      "    loss           : -1242.422025472811\n",
      "    val_loss       : -1185.9108517385089\n",
      "    val_log_likelihood: 1387.7249145507812\n",
      "    val_log_marginal: 1364.7975775238126\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -1435.304810\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -997.432861\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -1034.583740\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -1194.735474\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -1332.986694\n",
      "    epoch          : 69\n",
      "    loss           : -1261.0273612749459\n",
      "    val_loss       : -1308.178391327057\n",
      "    val_log_likelihood: 1423.3970947265625\n",
      "    val_log_marginal: 1401.7189206644894\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -1598.366699\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -1207.821167\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -1238.784058\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -1721.853516\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -1187.478516\n",
      "    epoch          : 70\n",
      "    loss           : -1282.661891823948\n",
      "    val_loss       : -1271.0555499318057\n",
      "    val_log_likelihood: 1404.5656616210938\n",
      "    val_log_marginal: 1380.9108860168606\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -1213.716187\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -1333.053223\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -1196.981445\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -1178.535522\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -1266.085083\n",
      "    epoch          : 71\n",
      "    loss           : -1275.088733031018\n",
      "    val_loss       : -1337.643250023201\n",
      "    val_log_likelihood: 1429.9828857421876\n",
      "    val_log_marginal: 1405.1525659482927\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -1476.075439\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -1474.180664\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -1316.037476\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -1398.937866\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -1330.650391\n",
      "    epoch          : 72\n",
      "    loss           : -1299.2898420579363\n",
      "    val_loss       : -1261.7470982976258\n",
      "    val_log_likelihood: 1460.0400634765624\n",
      "    val_log_marginal: 1434.957864877954\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -1654.384399\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -1193.615234\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -1283.599365\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -1084.604980\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -1261.438843\n",
      "    epoch          : 73\n",
      "    loss           : -1304.0309550974628\n",
      "    val_loss       : -1311.8826474340633\n",
      "    val_log_likelihood: 1421.7423950195312\n",
      "    val_log_marginal: 1401.1592704944312\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -1689.090088\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -1044.489990\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -1344.229492\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -1334.370605\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -1359.337891\n",
      "    epoch          : 74\n",
      "    loss           : -1270.5088845432394\n",
      "    val_loss       : -1250.5087588354013\n",
      "    val_log_likelihood: 1443.253466796875\n",
      "    val_log_marginal: 1420.1827924467623\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -1700.257568\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -1064.774170\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -1136.973877\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -1361.941406\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -1210.415283\n",
      "    epoch          : 75\n",
      "    loss           : -1256.115419293394\n",
      "    val_loss       : -1263.1695482053794\n",
      "    val_log_likelihood: 1413.8321655273437\n",
      "    val_log_marginal: 1390.4986734807492\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -1666.090698\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -1167.308960\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -1257.296509\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -1350.560669\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -1251.718384\n",
      "    epoch          : 76\n",
      "    loss           : -1284.538599599706\n",
      "    val_loss       : -1273.7853356274777\n",
      "    val_log_likelihood: 1395.65009765625\n",
      "    val_log_marginal: 1369.696391440928\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -1649.223755\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -1370.616211\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -1025.458496\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -1126.809082\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -1118.295654\n",
      "    epoch          : 77\n",
      "    loss           : -1298.5224675848933\n",
      "    val_loss       : -1228.694079157524\n",
      "    val_log_likelihood: 1452.34072265625\n",
      "    val_log_marginal: 1427.820216597244\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -1493.094727\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -1335.012085\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -1308.039062\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -1434.232300\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -1371.405273\n",
      "    epoch          : 78\n",
      "    loss           : -1294.6495766214805\n",
      "    val_loss       : -1261.6523499281145\n",
      "    val_log_likelihood: 1461.4722534179687\n",
      "    val_log_marginal: 1434.699604204297\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -1609.804199\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -1280.901367\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -1173.563354\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -1276.544922\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -1236.538574\n",
      "    epoch          : 79\n",
      "    loss           : -1317.8087345538754\n",
      "    val_loss       : -1343.4221763958224\n",
      "    val_log_likelihood: 1452.327490234375\n",
      "    val_log_marginal: 1431.4418973643333\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -1635.399048\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -1328.667236\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -1418.780151\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -1738.486328\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -1296.637939\n",
      "    epoch          : 80\n",
      "    loss           : -1322.8021149588103\n",
      "    val_loss       : -1333.9077703456394\n",
      "    val_log_likelihood: 1455.5651123046875\n",
      "    val_log_marginal: 1425.7682303927838\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -1605.360962\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -1328.281372\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -1278.258179\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -1421.851807\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -1345.465576\n",
      "    epoch          : 81\n",
      "    loss           : -1343.8712726253093\n",
      "    val_loss       : -1330.596965505462\n",
      "    val_log_likelihood: 1454.3249389648438\n",
      "    val_log_marginal: 1432.731014462933\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -1752.441650\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -1170.743164\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -1397.573608\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -1228.987915\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -1168.647095\n",
      "    epoch          : 82\n",
      "    loss           : -1292.6247081190052\n",
      "    val_loss       : -1303.6764815444126\n",
      "    val_log_likelihood: 1424.811181640625\n",
      "    val_log_marginal: 1393.125557688251\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -1721.577759\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -1372.307983\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -1421.620239\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -1246.186157\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -1295.602783\n",
      "    epoch          : 83\n",
      "    loss           : -1333.8778595877166\n",
      "    val_loss       : -1332.6831047656947\n",
      "    val_log_likelihood: 1444.1433471679688\n",
      "    val_log_marginal: 1418.7280232768505\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -1691.084839\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -1395.587158\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -1454.217773\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -1562.663086\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -1392.597290\n",
      "    epoch          : 84\n",
      "    loss           : -1353.324083384901\n",
      "    val_loss       : -1337.0785041844472\n",
      "    val_log_likelihood: 1446.96396484375\n",
      "    val_log_marginal: 1426.422857838869\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -1674.168945\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -1213.280884\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -1028.063721\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -1123.522827\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -1297.971558\n",
      "    epoch          : 85\n",
      "    loss           : -1354.4987974261294\n",
      "    val_loss       : -1387.7112540950068\n",
      "    val_log_likelihood: 1448.9292846679687\n",
      "    val_log_marginal: 1420.5222275238484\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -1433.300415\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -1238.816284\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -1262.896973\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -1431.728638\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -1281.285400\n",
      "    epoch          : 86\n",
      "    loss           : -1379.2083969871596\n",
      "    val_loss       : -1375.4256096668541\n",
      "    val_log_likelihood: 1466.5066284179688\n",
      "    val_log_marginal: 1446.0233317803606\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -1717.972900\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -1306.610718\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -1408.293945\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -1410.171631\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -1272.795532\n",
      "    epoch          : 87\n",
      "    loss           : -1389.4925017404084\n",
      "    val_loss       : -1373.6014059476554\n",
      "    val_log_likelihood: 1473.3685668945313\n",
      "    val_log_marginal: 1450.9979469735176\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -1683.858398\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -1413.903076\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -1286.632324\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -1447.209839\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -1376.966919\n",
      "    epoch          : 88\n",
      "    loss           : -1386.3160315787438\n",
      "    val_loss       : -1391.74210167462\n",
      "    val_log_likelihood: 1469.9252319335938\n",
      "    val_log_marginal: 1446.961671761796\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -1700.576294\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -1450.696533\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -1404.315186\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -1245.003418\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -1350.319336\n",
      "    epoch          : 89\n",
      "    loss           : -1390.8239625232054\n",
      "    val_loss       : -1383.2687745231203\n",
      "    val_log_likelihood: 1453.0653686523438\n",
      "    val_log_marginal: 1430.2333386152982\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -1256.746948\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -1281.986938\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -1505.509033\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -1319.175171\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -1269.429443\n",
      "    epoch          : 90\n",
      "    loss           : -1388.5443465733292\n",
      "    val_loss       : -1403.371071653813\n",
      "    val_log_likelihood: 1453.6383911132812\n",
      "    val_log_marginal: 1430.3690258476884\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -1765.208252\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -1343.386841\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -1203.868896\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -1741.544678\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -1326.896729\n",
      "    epoch          : 91\n",
      "    loss           : -1398.2804269318533\n",
      "    val_loss       : -1379.2030057081952\n",
      "    val_log_likelihood: 1475.891357421875\n",
      "    val_log_marginal: 1459.3573191747068\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -1744.081543\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -1354.626465\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -1405.524048\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -1270.468262\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -1381.777222\n",
      "    epoch          : 92\n",
      "    loss           : -1400.7455849222617\n",
      "    val_loss       : -1400.993028060347\n",
      "    val_log_likelihood: 1483.896044921875\n",
      "    val_log_marginal: 1460.1607658818364\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -1799.857544\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -1343.886475\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -1482.487793\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -1319.606567\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -1389.312378\n",
      "    epoch          : 93\n",
      "    loss           : -1409.7106425974628\n",
      "    val_loss       : -1406.3738477795384\n",
      "    val_log_likelihood: 1492.2035888671876\n",
      "    val_log_marginal: 1473.5586802400649\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -1693.811523\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -1199.761841\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -1225.903320\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -1490.394287\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -1410.177856\n",
      "    epoch          : 94\n",
      "    loss           : -1405.002256487856\n",
      "    val_loss       : -1391.4391136445106\n",
      "    val_log_likelihood: 1495.5172485351563\n",
      "    val_log_marginal: 1475.413032244891\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -1174.638672\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -1382.572632\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -1375.211426\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -1786.136963\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -1439.795898\n",
      "    epoch          : 95\n",
      "    loss           : -1411.4694425375155\n",
      "    val_loss       : -1402.7906369048171\n",
      "    val_log_likelihood: 1491.9807739257812\n",
      "    val_log_marginal: 1467.3567982554437\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -1636.084351\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -1293.804810\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -1228.004639\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -1459.492432\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -1281.823242\n",
      "    epoch          : 96\n",
      "    loss           : -1401.085692150758\n",
      "    val_loss       : -1427.6845115011558\n",
      "    val_log_likelihood: 1491.919189453125\n",
      "    val_log_marginal: 1471.6781304974108\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -1550.245850\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -1325.016602\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -1289.843994\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -1363.586304\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -1384.424927\n",
      "    epoch          : 97\n",
      "    loss           : -1420.1538992400217\n",
      "    val_loss       : -1430.344009286724\n",
      "    val_log_likelihood: 1499.1701049804688\n",
      "    val_log_marginal: 1479.7650528136642\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -1656.623047\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -1343.327881\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -1299.549194\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -1449.684082\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -1428.308594\n",
      "    epoch          : 98\n",
      "    loss           : -1410.9701519956684\n",
      "    val_loss       : -1419.1317362420261\n",
      "    val_log_likelihood: 1504.329296875\n",
      "    val_log_marginal: 1484.997853393108\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -1714.878784\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -1529.243896\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -1264.785889\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -1286.105957\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -1383.979492\n",
      "    epoch          : 99\n",
      "    loss           : -1403.9054897799351\n",
      "    val_loss       : -1403.5610670129768\n",
      "    val_log_likelihood: 1491.58876953125\n",
      "    val_log_marginal: 1466.1472621895373\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -1504.820068\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -1400.559937\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -1242.869507\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -1258.930054\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -1442.018555\n",
      "    epoch          : 100\n",
      "    loss           : -1423.9300754660428\n",
      "    val_loss       : -1440.015043869149\n",
      "    val_log_likelihood: 1496.0130004882812\n",
      "    val_log_marginal: 1469.9269543681294\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -1749.294189\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -1493.907104\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -1283.160645\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -1764.694336\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -1447.688110\n",
      "    epoch          : 101\n",
      "    loss           : -1422.5781008276608\n",
      "    val_loss       : -1415.9251097791828\n",
      "    val_log_likelihood: 1500.615576171875\n",
      "    val_log_marginal: 1483.6548018082976\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -1345.851318\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -1438.443115\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -1239.479248\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -1412.062744\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -1420.583008\n",
      "    epoch          : 102\n",
      "    loss           : -1425.4215329614017\n",
      "    val_loss       : -1442.646870334726\n",
      "    val_log_likelihood: 1505.4019897460937\n",
      "    val_log_marginal: 1480.9177838772534\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -1753.214355\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -1472.321777\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -1478.684326\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -1460.879395\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -1381.405518\n",
      "    epoch          : 103\n",
      "    loss           : -1435.2644175916614\n",
      "    val_loss       : -1400.5285423494875\n",
      "    val_log_likelihood: 1481.249267578125\n",
      "    val_log_marginal: 1460.2370331767947\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -1735.524536\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -1531.919556\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -1517.169678\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -1439.783325\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -1349.861328\n",
      "    epoch          : 104\n",
      "    loss           : -1418.4247067895267\n",
      "    val_loss       : -1430.2332638737746\n",
      "    val_log_likelihood: 1501.0984008789062\n",
      "    val_log_marginal: 1481.1756972394883\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -1349.823730\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -1491.834961\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -1316.070679\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -1270.479370\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -1404.575073\n",
      "    epoch          : 105\n",
      "    loss           : -1430.1598093769337\n",
      "    val_loss       : -1438.008573779743\n",
      "    val_log_likelihood: 1515.1965209960938\n",
      "    val_log_marginal: 1495.5832921668887\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -1593.402100\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -1546.375000\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -1425.481934\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -1308.946045\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -1357.599121\n",
      "    epoch          : 106\n",
      "    loss           : -1432.3995566793008\n",
      "    val_loss       : -1435.4731879207307\n",
      "    val_log_likelihood: 1512.652197265625\n",
      "    val_log_marginal: 1492.4435658212751\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -1764.810547\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -1485.514893\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -1476.371094\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -1177.728271\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -1320.756348\n",
      "    epoch          : 107\n",
      "    loss           : -1417.8675295385983\n",
      "    val_loss       : -1420.989519982692\n",
      "    val_log_likelihood: 1519.2079833984376\n",
      "    val_log_marginal: 1494.6999047006575\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -1762.298340\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -1376.721680\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -1297.558594\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -1369.869019\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -1441.076416\n",
      "    epoch          : 108\n",
      "    loss           : -1443.0438135732518\n",
      "    val_loss       : -1407.6967078088783\n",
      "    val_log_likelihood: 1506.5601684570313\n",
      "    val_log_marginal: 1486.034736296162\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -1783.518066\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -1318.454712\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -1451.305664\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -1281.729370\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -1395.882324\n",
      "    epoch          : 109\n",
      "    loss           : -1413.9414473429765\n",
      "    val_loss       : -1430.6298710410483\n",
      "    val_log_likelihood: 1506.7536865234374\n",
      "    val_log_marginal: 1484.7183972597122\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -1693.907715\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -1409.153320\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -1307.778442\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -1275.634399\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -1363.655762\n",
      "    epoch          : 110\n",
      "    loss           : -1424.8035103070854\n",
      "    val_loss       : -1437.4748012993484\n",
      "    val_log_likelihood: 1505.9159545898438\n",
      "    val_log_marginal: 1481.3256287388504\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -1734.549561\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -1531.134399\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -1289.672241\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -1389.111206\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -1379.883789\n",
      "    epoch          : 111\n",
      "    loss           : -1443.6895655263768\n",
      "    val_loss       : -1444.537707556598\n",
      "    val_log_likelihood: 1516.7285400390624\n",
      "    val_log_marginal: 1496.4945912644266\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -1408.322021\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -1375.184937\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -1419.962158\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -1408.346558\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -1403.971680\n",
      "    epoch          : 112\n",
      "    loss           : -1451.2569495474938\n",
      "    val_loss       : -1443.0187071536668\n",
      "    val_log_likelihood: 1523.4125610351562\n",
      "    val_log_marginal: 1498.9740574233233\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -1475.062012\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -1345.548096\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -1260.412720\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -1765.563721\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -1411.981323\n",
      "    epoch          : 113\n",
      "    loss           : -1444.0838393409654\n",
      "    val_loss       : -1444.8506546858698\n",
      "    val_log_likelihood: 1529.9272094726562\n",
      "    val_log_marginal: 1506.962391686812\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -1190.018311\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -1350.442749\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -1473.477295\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -1268.222900\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -1430.053223\n",
      "    epoch          : 114\n",
      "    loss           : -1429.5806038733756\n",
      "    val_loss       : -1415.946342676319\n",
      "    val_log_likelihood: 1523.2448486328126\n",
      "    val_log_marginal: 1492.5530510320482\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -1524.811768\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -1396.915161\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -1284.109375\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -1450.337891\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -1357.822266\n",
      "    epoch          : 115\n",
      "    loss           : -1441.8470217260983\n",
      "    val_loss       : -1444.630186061468\n",
      "    val_log_likelihood: 1524.2243041992188\n",
      "    val_log_marginal: 1499.0812821626664\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -1764.098877\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -1416.773193\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -1453.246460\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -1259.244873\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -1454.101562\n",
      "    epoch          : 116\n",
      "    loss           : -1437.4765310759592\n",
      "    val_loss       : -1438.6438078952021\n",
      "    val_log_likelihood: 1509.7328002929687\n",
      "    val_log_marginal: 1490.6054416809231\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -1762.458374\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -1341.941406\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -1259.186279\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -1753.374268\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -1467.671631\n",
      "    epoch          : 117\n",
      "    loss           : -1448.8275895826887\n",
      "    val_loss       : -1458.4311629898846\n",
      "    val_log_likelihood: 1522.3711059570312\n",
      "    val_log_marginal: 1498.2109420947731\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -1791.350342\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -1533.696655\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -1536.114258\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -1285.510620\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -1254.724609\n",
      "    epoch          : 118\n",
      "    loss           : -1456.8663982731282\n",
      "    val_loss       : -1444.9152828984893\n",
      "    val_log_likelihood: 1525.3745239257812\n",
      "    val_log_marginal: 1501.3887284077705\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -1805.899414\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -1569.995483\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -1347.487793\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -1554.256958\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -1353.896240\n",
      "    epoch          : 119\n",
      "    loss           : -1453.3850436068997\n",
      "    val_loss       : -1424.5119422086514\n",
      "    val_log_likelihood: 1513.0350830078125\n",
      "    val_log_marginal: 1486.9025368276984\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -1797.585938\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -1499.092407\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -1328.439697\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -1458.686523\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -1444.753418\n",
      "    epoch          : 120\n",
      "    loss           : -1454.7719400235922\n",
      "    val_loss       : -1456.8390647932888\n",
      "    val_log_likelihood: 1511.7018798828126\n",
      "    val_log_marginal: 1485.1362443249673\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -1405.122437\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -1523.770874\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -1514.716187\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -1405.127319\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -1346.522217\n",
      "    epoch          : 121\n",
      "    loss           : -1463.820393477336\n",
      "    val_loss       : -1443.1236017234623\n",
      "    val_log_likelihood: 1529.7580200195312\n",
      "    val_log_marginal: 1511.4256173651665\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -1758.614380\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -1345.363892\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -1554.964111\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -1286.929443\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -1375.479858\n",
      "    epoch          : 122\n",
      "    loss           : -1457.5744858543471\n",
      "    val_loss       : -1467.868903326243\n",
      "    val_log_likelihood: 1529.416796875\n",
      "    val_log_marginal: 1501.6447101166038\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -1809.653931\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -1408.539551\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -1411.107300\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -1491.918579\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -1437.676514\n",
      "    epoch          : 123\n",
      "    loss           : -1473.1565847868967\n",
      "    val_loss       : -1456.8590526972898\n",
      "    val_log_likelihood: 1534.3748046875\n",
      "    val_log_marginal: 1516.8751589711756\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -1788.604492\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -1418.153320\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -1478.157959\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -1481.413086\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -1489.998047\n",
      "    epoch          : 124\n",
      "    loss           : -1476.5220379215657\n",
      "    val_loss       : -1485.0224373396486\n",
      "    val_log_likelihood: 1538.8469848632812\n",
      "    val_log_marginal: 1511.8335779048502\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -1809.615479\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -1550.099243\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -1483.133545\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -1855.167847\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -1470.758911\n",
      "    epoch          : 125\n",
      "    loss           : -1476.3565347501547\n",
      "    val_loss       : -1463.1319285728969\n",
      "    val_log_likelihood: 1531.655322265625\n",
      "    val_log_marginal: 1508.9509753942489\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -1827.389648\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -1488.908203\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -1498.984619\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -1472.515625\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -1444.714600\n",
      "    epoch          : 126\n",
      "    loss           : -1456.8733864963644\n",
      "    val_loss       : -1436.1826487511396\n",
      "    val_log_likelihood: 1478.074658203125\n",
      "    val_log_marginal: 1454.7880035292358\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -1633.639526\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -1475.063965\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -1312.266357\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -1304.432861\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -1277.612793\n",
      "    epoch          : 127\n",
      "    loss           : -1439.6756168780942\n",
      "    val_loss       : -1450.5222545407712\n",
      "    val_log_likelihood: 1517.9285888671875\n",
      "    val_log_marginal: 1497.455362154916\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -1507.695801\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -1545.711914\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -1465.926758\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -1423.962158\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -1432.798584\n",
      "    epoch          : 128\n",
      "    loss           : -1451.081924891708\n",
      "    val_loss       : -1417.0710482984782\n",
      "    val_log_likelihood: 1522.2517578125\n",
      "    val_log_marginal: 1498.2640388730913\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -1753.624756\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -1236.151978\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -1343.485840\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -1452.220093\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -1450.481812\n",
      "    epoch          : 129\n",
      "    loss           : -1445.4601168490872\n",
      "    val_loss       : -1419.3191740618088\n",
      "    val_log_likelihood: 1533.651318359375\n",
      "    val_log_marginal: 1509.0741267673823\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -1537.111816\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -1303.233765\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -1415.539429\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -1422.293945\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -1457.813232\n",
      "    epoch          : 130\n",
      "    loss           : -1460.4789132599783\n",
      "    val_loss       : -1454.8136574961245\n",
      "    val_log_likelihood: 1545.4731689453124\n",
      "    val_log_marginal: 1515.4531467277557\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -1796.368896\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -1380.584106\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -1390.175781\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -1441.425049\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -1469.064453\n",
      "    epoch          : 131\n",
      "    loss           : -1467.9752100576268\n",
      "    val_loss       : -1477.826187304873\n",
      "    val_log_likelihood: 1538.81923828125\n",
      "    val_log_marginal: 1514.4157676920295\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -1790.345093\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -1447.392822\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -1438.552002\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -1455.033203\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -1479.329590\n",
      "    epoch          : 132\n",
      "    loss           : -1461.9356471902072\n",
      "    val_loss       : -1463.0044488946908\n",
      "    val_log_likelihood: 1536.2119873046875\n",
      "    val_log_marginal: 1508.543590434268\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -1809.971558\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -1404.447876\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -1498.170898\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -1437.788696\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -1475.147339\n",
      "    epoch          : 133\n",
      "    loss           : -1463.469722936649\n",
      "    val_loss       : -1478.5882693180815\n",
      "    val_log_likelihood: 1529.9264526367188\n",
      "    val_log_marginal: 1510.9657216716557\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -1823.874756\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -1556.252930\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -1271.821289\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -1831.965210\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -1500.800903\n",
      "    epoch          : 134\n",
      "    loss           : -1474.644829778388\n",
      "    val_loss       : -1451.1291537775658\n",
      "    val_log_likelihood: 1500.8052978515625\n",
      "    val_log_marginal: 1481.4221515674567\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -1768.711304\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -1397.304321\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -1548.749756\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -1452.821289\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -1423.514893\n",
      "    epoch          : 135\n",
      "    loss           : -1456.9300331644492\n",
      "    val_loss       : -1475.0633312778548\n",
      "    val_log_likelihood: 1536.157958984375\n",
      "    val_log_marginal: 1513.0521097704768\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -1427.239990\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -1546.253174\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -1566.730957\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -1487.807739\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -1425.683716\n",
      "    epoch          : 136\n",
      "    loss           : -1482.8158841274753\n",
      "    val_loss       : -1488.1283429061064\n",
      "    val_log_likelihood: 1548.1740356445312\n",
      "    val_log_marginal: 1528.7484456069767\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -1815.560547\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -1337.188843\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -1290.257812\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -1412.791504\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -1459.136719\n",
      "    epoch          : 137\n",
      "    loss           : -1470.0805784924196\n",
      "    val_loss       : -1467.5517898399382\n",
      "    val_log_likelihood: 1533.9688720703125\n",
      "    val_log_marginal: 1509.5533365026117\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -1820.837036\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -1584.928345\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -1538.297852\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -1372.252319\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -1442.669434\n",
      "    epoch          : 138\n",
      "    loss           : -1479.237522238552\n",
      "    val_loss       : -1476.56834803205\n",
      "    val_log_likelihood: 1550.2075073242188\n",
      "    val_log_marginal: 1520.494776840508\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -1779.377808\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -1384.677490\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -1418.982422\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -1343.965210\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -1441.874756\n",
      "    epoch          : 139\n",
      "    loss           : -1482.1626737991182\n",
      "    val_loss       : -1475.4865482837893\n",
      "    val_log_likelihood: 1557.2865966796876\n",
      "    val_log_marginal: 1533.9490404210985\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -1776.775879\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -1582.329590\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -1442.510742\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -1444.117676\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -1454.669067\n",
      "    epoch          : 140\n",
      "    loss           : -1471.0754890064202\n",
      "    val_loss       : -1468.6805679407903\n",
      "    val_log_likelihood: 1539.3239990234374\n",
      "    val_log_marginal: 1511.4153606794775\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -1794.512695\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -1416.554443\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -1504.344238\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -1495.305542\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -1460.956909\n",
      "    epoch          : 141\n",
      "    loss           : -1492.0322591951578\n",
      "    val_loss       : -1468.5135005217976\n",
      "    val_log_likelihood: 1543.6714599609375\n",
      "    val_log_marginal: 1521.9047282144427\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -1803.538940\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -1353.666992\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -1328.458008\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -1363.781494\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -1440.339600\n",
      "    epoch          : 142\n",
      "    loss           : -1481.8016635403774\n",
      "    val_loss       : -1481.6183808288538\n",
      "    val_log_likelihood: 1559.0471069335938\n",
      "    val_log_marginal: 1540.5326030869037\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -1833.552979\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -1402.778809\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -1576.080200\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -1501.689819\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -1494.147339\n",
      "    epoch          : 143\n",
      "    loss           : -1488.1166871325804\n",
      "    val_loss       : -1477.592277941853\n",
      "    val_log_likelihood: 1553.5992431640625\n",
      "    val_log_marginal: 1526.1343932650984\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -1795.077637\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -1544.332031\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -1391.235474\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -1807.007568\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -1370.878662\n",
      "    epoch          : 144\n",
      "    loss           : -1484.9972310585551\n",
      "    val_loss       : -1488.6764160935768\n",
      "    val_log_likelihood: 1546.657666015625\n",
      "    val_log_marginal: 1519.7642948441207\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -1811.337891\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -1585.431152\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -1584.909912\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -1370.305664\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -1528.748047\n",
      "    epoch          : 145\n",
      "    loss           : -1494.5458658048422\n",
      "    val_loss       : -1491.7022373118439\n",
      "    val_log_likelihood: 1550.4511840820312\n",
      "    val_log_marginal: 1522.179043501988\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -1806.000488\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -1540.831055\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -1511.191406\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -1427.190918\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -1481.650269\n",
      "    epoch          : 146\n",
      "    loss           : -1485.7641154374226\n",
      "    val_loss       : -1495.7552980422042\n",
      "    val_log_likelihood: 1554.8671508789062\n",
      "    val_log_marginal: 1534.9523223970086\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -1810.683472\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -1492.519531\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -1515.774292\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -1569.319336\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -1355.024902\n",
      "    epoch          : 147\n",
      "    loss           : -1500.7754824798885\n",
      "    val_loss       : -1506.9417290194892\n",
      "    val_log_likelihood: 1563.7418090820313\n",
      "    val_log_marginal: 1546.2955324713141\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -1855.878052\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -1471.340332\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -1386.231812\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -1485.982544\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -1476.738647\n",
      "    epoch          : 148\n",
      "    loss           : -1505.641468614635\n",
      "    val_loss       : -1508.7820517856628\n",
      "    val_log_likelihood: 1570.346923828125\n",
      "    val_log_marginal: 1543.123715017736\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -1829.848145\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -1419.010742\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -1548.450073\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -1323.711060\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -1412.036621\n",
      "    epoch          : 149\n",
      "    loss           : -1492.9068651860302\n",
      "    val_loss       : -1490.6746425511315\n",
      "    val_log_likelihood: 1557.7658081054688\n",
      "    val_log_marginal: 1527.810538963601\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -1592.371582\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -1563.726318\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -1444.762207\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -1482.801270\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -1489.940186\n",
      "    epoch          : 150\n",
      "    loss           : -1486.094645585164\n",
      "    val_loss       : -1486.8048610256053\n",
      "    val_log_likelihood: 1546.4297973632813\n",
      "    val_log_marginal: 1527.4778323642909\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -1773.967773\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -1417.471069\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -1419.040771\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -1779.451660\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -1440.971069\n",
      "    epoch          : 151\n",
      "    loss           : -1474.5379916653774\n",
      "    val_loss       : -1481.508105818834\n",
      "    val_log_likelihood: 1542.4898315429687\n",
      "    val_log_marginal: 1518.5778734274209\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -1799.793823\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -1436.146606\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -1572.288330\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -1356.186646\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -1451.532104\n",
      "    epoch          : 152\n",
      "    loss           : -1481.106986772896\n",
      "    val_loss       : -1468.2547661704011\n",
      "    val_log_likelihood: 1540.9073608398437\n",
      "    val_log_marginal: 1511.3593700020435\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -1808.137817\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -1388.293945\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -1497.943481\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -1759.694336\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -1443.532349\n",
      "    epoch          : 153\n",
      "    loss           : -1455.9052021290997\n",
      "    val_loss       : -1456.502365999855\n",
      "    val_log_likelihood: 1568.526513671875\n",
      "    val_log_marginal: 1542.5396529056131\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -1713.362427\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -1430.413208\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -1498.796021\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -1509.913208\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -1512.668823\n",
      "    epoch          : 154\n",
      "    loss           : -1474.7393460415378\n",
      "    val_loss       : -1493.715694251284\n",
      "    val_log_likelihood: 1567.59267578125\n",
      "    val_log_marginal: 1533.7208799686282\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -1806.370483\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -1479.158691\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -1492.459229\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -1516.062988\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -1448.517578\n",
      "    epoch          : 155\n",
      "    loss           : -1490.4069800046411\n",
      "    val_loss       : -1506.7958659179508\n",
      "    val_log_likelihood: 1576.2274780273438\n",
      "    val_log_marginal: 1552.770977930352\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -1831.714844\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -1587.132446\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -1466.037720\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -1454.614990\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -1460.050049\n",
      "    epoch          : 156\n",
      "    loss           : -1497.480362391708\n",
      "    val_loss       : -1470.8355749591253\n",
      "    val_log_likelihood: 1561.854150390625\n",
      "    val_log_marginal: 1535.943041838333\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -1768.016968\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -1406.303223\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -1414.621338\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -1453.795776\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -1440.135498\n",
      "    epoch          : 157\n",
      "    loss           : -1506.7821214128248\n",
      "    val_loss       : -1507.468535977509\n",
      "    val_log_likelihood: 1571.373193359375\n",
      "    val_log_marginal: 1543.0676106821745\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -1828.441650\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -1449.238281\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -1437.610352\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -1820.755371\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -1474.286865\n",
      "    epoch          : 158\n",
      "    loss           : -1499.2204771136294\n",
      "    val_loss       : -1499.0674041060731\n",
      "    val_log_likelihood: 1552.3800659179688\n",
      "    val_log_marginal: 1525.9374186631292\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -1569.450195\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -1427.943970\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -1464.660156\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -1341.664795\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -1424.048950\n",
      "    epoch          : 159\n",
      "    loss           : -1484.3388007135675\n",
      "    val_loss       : -1481.965409896057\n",
      "    val_log_likelihood: 1568.927978515625\n",
      "    val_log_marginal: 1542.9640845235438\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -1839.048340\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -1520.665771\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -1416.358398\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -1499.587646\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -1415.008789\n",
      "    epoch          : 160\n",
      "    loss           : -1488.1161891823947\n",
      "    val_loss       : -1495.1090660389514\n",
      "    val_log_likelihood: 1570.2721069335937\n",
      "    val_log_marginal: 1539.1037202190607\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -1430.188477\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -1596.722168\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -1458.478516\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -1543.710083\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -1487.333740\n",
      "    epoch          : 161\n",
      "    loss           : -1500.2718433342357\n",
      "    val_loss       : -1500.290849411022\n",
      "    val_log_likelihood: 1564.5401977539063\n",
      "    val_log_marginal: 1542.4585871934892\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -1775.534668\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -1417.806396\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -1327.556396\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -1608.687378\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -1517.173584\n",
      "    epoch          : 162\n",
      "    loss           : -1493.8958027150372\n",
      "    val_loss       : -1511.6272411338985\n",
      "    val_log_likelihood: 1577.0027954101563\n",
      "    val_log_marginal: 1553.27876153253\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -1448.836792\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -1552.289795\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -1533.376343\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -1341.668945\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -1456.453369\n",
      "    epoch          : 163\n",
      "    loss           : -1488.7723884204827\n",
      "    val_loss       : -1506.4035590473563\n",
      "    val_log_likelihood: 1582.3171264648438\n",
      "    val_log_marginal: 1551.9451787848025\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -1780.180664\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -1373.888306\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -1432.493164\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -1787.053955\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -1535.771606\n",
      "    epoch          : 164\n",
      "    loss           : -1509.5866783821937\n",
      "    val_loss       : -1509.8219957092776\n",
      "    val_log_likelihood: 1581.2784423828125\n",
      "    val_log_marginal: 1552.338476922363\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -1808.875732\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -1374.090576\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -1521.012817\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -1485.833130\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -1443.472168\n",
      "    epoch          : 165\n",
      "    loss           : -1503.0481029548268\n",
      "    val_loss       : -1479.1970244579948\n",
      "    val_log_likelihood: 1581.0509643554688\n",
      "    val_log_marginal: 1564.3169494703411\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -1721.042480\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -1402.829102\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -1394.327393\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -1784.483521\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -1420.154297\n",
      "    epoch          : 166\n",
      "    loss           : -1484.8356063389542\n",
      "    val_loss       : -1495.4486969245597\n",
      "    val_log_likelihood: 1562.4576538085937\n",
      "    val_log_marginal: 1530.3590629540383\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -1815.106445\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -1549.924561\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -1558.197510\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -1559.011475\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -1426.400513\n",
      "    epoch          : 167\n",
      "    loss           : -1501.1695387434252\n",
      "    val_loss       : -1502.9131347534246\n",
      "    val_log_likelihood: 1581.4724853515625\n",
      "    val_log_marginal: 1554.6764369461685\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -1761.592529\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -1445.072754\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -1496.740234\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -1436.621826\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -1508.386475\n",
      "    epoch          : 168\n",
      "    loss           : -1507.6987449721535\n",
      "    val_loss       : -1510.3811196421273\n",
      "    val_log_likelihood: 1573.5131103515625\n",
      "    val_log_marginal: 1550.5849096670747\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -1806.906860\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -1411.588989\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -1572.455566\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -1429.997559\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -1486.752197\n",
      "    epoch          : 169\n",
      "    loss           : -1509.767330358524\n",
      "    val_loss       : -1507.172973506432\n",
      "    val_log_likelihood: 1589.7646484375\n",
      "    val_log_marginal: 1558.3855976682157\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -1836.697754\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -1587.830322\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -1542.145020\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -1599.281250\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -1465.756348\n",
      "    epoch          : 170\n",
      "    loss           : -1517.4546889503404\n",
      "    val_loss       : -1516.9348235694692\n",
      "    val_log_likelihood: 1579.79013671875\n",
      "    val_log_marginal: 1554.2120348736644\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -1816.016724\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -1379.446899\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -1392.235107\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -1825.516357\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -1610.158813\n",
      "    epoch          : 171\n",
      "    loss           : -1517.540712262144\n",
      "    val_loss       : -1523.772387530189\n",
      "    val_log_likelihood: 1580.5313598632813\n",
      "    val_log_marginal: 1563.2799573604018\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -1832.942261\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -1592.120850\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -1440.458252\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -1399.894775\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -1491.819824\n",
      "    epoch          : 172\n",
      "    loss           : -1516.7670004060953\n",
      "    val_loss       : -1488.4131875723601\n",
      "    val_log_likelihood: 1560.6648681640625\n",
      "    val_log_marginal: 1536.2150608703494\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -1714.218750\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -1558.500732\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -1432.369629\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -1397.714355\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -1494.916504\n",
      "    epoch          : 173\n",
      "    loss           : -1506.7051977780786\n",
      "    val_loss       : -1508.778966834303\n",
      "    val_log_likelihood: 1570.8788818359376\n",
      "    val_log_marginal: 1541.473494907841\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -1824.345581\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -1343.345093\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -1500.860596\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -1326.673462\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -1398.136963\n",
      "    epoch          : 174\n",
      "    loss           : -1508.068524955523\n",
      "    val_loss       : -1519.7510298555717\n",
      "    val_log_likelihood: 1578.3937133789063\n",
      "    val_log_marginal: 1545.2271299943327\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -1852.449951\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -1608.459473\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -1388.357300\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -1529.977295\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -1562.289307\n",
      "    epoch          : 175\n",
      "    loss           : -1519.4881301728806\n",
      "    val_loss       : -1499.1196539727039\n",
      "    val_log_likelihood: 1572.9252807617188\n",
      "    val_log_marginal: 1550.6839423172175\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -1817.147461\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -1446.066284\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -1363.790039\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -1848.094116\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -1519.424316\n",
      "    epoch          : 176\n",
      "    loss           : -1520.8446214128248\n",
      "    val_loss       : -1536.7643991454504\n",
      "    val_log_likelihood: 1590.6759521484375\n",
      "    val_log_marginal: 1567.0570602800697\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -1854.592773\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -1461.566772\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -1532.282104\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -1534.048828\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -1542.978882\n",
      "    epoch          : 177\n",
      "    loss           : -1527.5785419728497\n",
      "    val_loss       : -1528.655818097014\n",
      "    val_log_likelihood: 1595.4317138671875\n",
      "    val_log_marginal: 1567.6772745650262\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -1833.359253\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -1443.816528\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -1587.731934\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -1839.057007\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -1554.098145\n",
      "    epoch          : 178\n",
      "    loss           : -1517.7886684908726\n",
      "    val_loss       : -1505.3616406323388\n",
      "    val_log_likelihood: 1590.2491455078125\n",
      "    val_log_marginal: 1567.4892368145288\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -1417.659668\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -1452.362549\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -1381.019897\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -1401.809814\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -1579.788818\n",
      "    epoch          : 179\n",
      "    loss           : -1523.2684386602723\n",
      "    val_loss       : -1518.6590157407336\n",
      "    val_log_likelihood: 1576.1273315429687\n",
      "    val_log_marginal: 1550.2902899444102\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -1819.582520\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -1428.947754\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -1598.356689\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -1849.074219\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -1445.301758\n",
      "    epoch          : 180\n",
      "    loss           : -1518.1712042175898\n",
      "    val_loss       : -1511.1398811309598\n",
      "    val_log_likelihood: 1586.9316040039062\n",
      "    val_log_marginal: 1549.9869324296712\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -1847.948486\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -1454.352905\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -1566.819702\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -1836.694580\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -1540.788452\n",
      "    epoch          : 181\n",
      "    loss           : -1525.5601516572556\n",
      "    val_loss       : -1528.4025726922787\n",
      "    val_log_likelihood: 1580.0707885742188\n",
      "    val_log_marginal: 1559.579444539547\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -1861.833252\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -1577.740479\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -1400.961426\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -1363.273193\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -1502.977051\n",
      "    epoch          : 182\n",
      "    loss           : -1519.287596447633\n",
      "    val_loss       : -1499.1502820813098\n",
      "    val_log_likelihood: 1592.5885620117188\n",
      "    val_log_marginal: 1570.2501394413412\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -1831.215820\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -1608.240723\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -1434.136719\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -1740.524780\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -1513.001221\n",
      "    epoch          : 183\n",
      "    loss           : -1478.2039251044246\n",
      "    val_loss       : -1491.1971330136062\n",
      "    val_log_likelihood: 1546.57890625\n",
      "    val_log_marginal: 1518.8340439531953\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -1802.555542\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -1397.760620\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -1513.717407\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -1803.992798\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -1518.649170\n",
      "    epoch          : 184\n",
      "    loss           : -1511.4762216700185\n",
      "    val_loss       : -1505.8808320721612\n",
      "    val_log_likelihood: 1567.6790771484375\n",
      "    val_log_marginal: 1542.2543233890087\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -1598.131348\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -1372.870850\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -1578.824585\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -1430.399780\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -1439.349609\n",
      "    epoch          : 185\n",
      "    loss           : -1503.8854182781558\n",
      "    val_loss       : -1474.2935254842043\n",
      "    val_log_likelihood: 1567.676904296875\n",
      "    val_log_marginal: 1540.029573438689\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -1761.375977\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -1392.309082\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -1451.713989\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -1424.291870\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -1431.322754\n",
      "    epoch          : 186\n",
      "    loss           : -1492.9268702138768\n",
      "    val_loss       : -1499.2049631069415\n",
      "    val_log_likelihood: 1574.3681640625\n",
      "    val_log_marginal: 1543.2375203896313\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -1786.330322\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -1483.963989\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -1388.972656\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -1564.010498\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -1399.171631\n",
      "    epoch          : 187\n",
      "    loss           : -1503.1579867825649\n",
      "    val_loss       : -1489.145441192109\n",
      "    val_log_likelihood: 1577.1921752929688\n",
      "    val_log_marginal: 1545.2126201864332\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -1810.439941\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -1414.933350\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -1292.980957\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -1489.812256\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -1456.091309\n",
      "    epoch          : 188\n",
      "    loss           : -1507.7590610013149\n",
      "    val_loss       : -1509.9835302404128\n",
      "    val_log_likelihood: 1582.8305786132812\n",
      "    val_log_marginal: 1554.1097954012453\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -1849.671387\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -1448.309692\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -1360.851807\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -1511.633057\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -1485.952515\n",
      "    epoch          : 189\n",
      "    loss           : -1496.1813843982054\n",
      "    val_loss       : -1504.8984690749087\n",
      "    val_log_likelihood: 1580.1311279296874\n",
      "    val_log_marginal: 1553.1393028058112\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -1795.641846\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -1390.860229\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -1386.459106\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -1820.307373\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -1508.798096\n",
      "    epoch          : 190\n",
      "    loss           : -1511.290202225789\n",
      "    val_loss       : -1502.3956898866222\n",
      "    val_log_likelihood: 1554.5838134765625\n",
      "    val_log_marginal: 1528.5991554319858\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -1784.544312\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -1398.964355\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -1514.381104\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -1570.402710\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -1507.707520\n",
      "    epoch          : 191\n",
      "    loss           : -1504.285289197865\n",
      "    val_loss       : -1497.0308376178145\n",
      "    val_log_likelihood: 1569.1085571289063\n",
      "    val_log_marginal: 1542.831624117866\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -1572.050049\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -1450.882080\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -1585.413818\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -1575.026855\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -1446.156982\n",
      "    epoch          : 192\n",
      "    loss           : -1503.4999637414912\n",
      "    val_loss       : -1505.823607073538\n",
      "    val_log_likelihood: 1572.6803344726563\n",
      "    val_log_marginal: 1551.6218994546682\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -1808.793701\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -1600.084351\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -1553.753052\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -1392.179321\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -1332.334229\n",
      "    epoch          : 193\n",
      "    loss           : -1501.1738232905323\n",
      "    val_loss       : -1501.1631212834268\n",
      "    val_log_likelihood: 1578.5148681640626\n",
      "    val_log_marginal: 1557.1600445318968\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -1764.655762\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -1383.794678\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -1520.158203\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -1826.482056\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -1429.280396\n",
      "    epoch          : 194\n",
      "    loss           : -1500.7400917581992\n",
      "    val_loss       : -1510.350355545804\n",
      "    val_log_likelihood: 1568.42509765625\n",
      "    val_log_marginal: 1545.0220145922153\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -1855.112671\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -1422.335938\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -1420.274536\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -1510.724487\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -1477.231323\n",
      "    epoch          : 195\n",
      "    loss           : -1515.2006896368348\n",
      "    val_loss       : -1505.917820031289\n",
      "    val_log_likelihood: 1571.6048706054687\n",
      "    val_log_marginal: 1547.7429924275725\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -1821.167358\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -1444.644653\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -1370.703003\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -1517.786987\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -1499.868408\n",
      "    epoch          : 196\n",
      "    loss           : -1516.6696209293782\n",
      "    val_loss       : -1507.3622365680524\n",
      "    val_log_likelihood: 1586.6391357421876\n",
      "    val_log_marginal: 1558.1218788340689\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -1806.279785\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -1478.880249\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -1516.766602\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -1818.847412\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -1504.197144\n",
      "    epoch          : 197\n",
      "    loss           : -1518.340524201346\n",
      "    val_loss       : -1501.632705431152\n",
      "    val_log_likelihood: 1566.9403076171875\n",
      "    val_log_marginal: 1537.4017395205797\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -1830.765503\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -1423.963867\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -1387.147095\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -1828.011108\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -1427.843262\n",
      "    epoch          : 198\n",
      "    loss           : -1525.410538172958\n",
      "    val_loss       : -1525.113058817014\n",
      "    val_log_likelihood: 1593.3210571289062\n",
      "    val_log_marginal: 1563.2174172475934\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -1847.352783\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -1452.787476\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -1562.081299\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -1850.184692\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -1508.649902\n",
      "    epoch          : 199\n",
      "    loss           : -1530.4579220006963\n",
      "    val_loss       : -1532.459110836964\n",
      "    val_log_likelihood: 1601.4300415039063\n",
      "    val_log_marginal: 1577.0628322578966\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -1835.441162\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -1421.274536\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -1534.248047\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -1835.729980\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -1478.432739\n",
      "    epoch          : 200\n",
      "    loss           : -1531.614880250232\n",
      "    val_loss       : -1531.7667236390523\n",
      "    val_log_likelihood: 1589.0802368164063\n",
      "    val_log_marginal: 1563.625744201243\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -1478.342407\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -1477.133301\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -1360.729004\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -1453.979858\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -1495.332764\n",
      "    epoch          : 201\n",
      "    loss           : -1515.548151299505\n",
      "    val_loss       : -1512.6341644967906\n",
      "    val_log_likelihood: 1585.6168823242188\n",
      "    val_log_marginal: 1555.7913254726677\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -1800.067627\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -1477.195801\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -1399.452271\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -1511.451904\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -1527.743652\n",
      "    epoch          : 202\n",
      "    loss           : -1524.008923218982\n",
      "    val_loss       : -1534.4299729535355\n",
      "    val_log_likelihood: 1600.782568359375\n",
      "    val_log_marginal: 1579.115353533253\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -1846.701660\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -1468.149414\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -1431.422852\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -1513.946045\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -1518.627319\n",
      "    epoch          : 203\n",
      "    loss           : -1530.9182539836015\n",
      "    val_loss       : -1523.3893100085668\n",
      "    val_log_likelihood: 1599.6647827148438\n",
      "    val_log_marginal: 1566.9824967909603\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -1825.890991\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -1469.619019\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -1432.773071\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -1518.474121\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -1507.645996\n",
      "    epoch          : 204\n",
      "    loss           : -1530.562010510133\n",
      "    val_loss       : -1537.1699492239393\n",
      "    val_log_likelihood: 1596.4712158203124\n",
      "    val_log_marginal: 1566.0345451645553\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -1867.343262\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -1449.286133\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -1480.324219\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -1465.435669\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -1512.233643\n",
      "    epoch          : 205\n",
      "    loss           : -1529.1094438911664\n",
      "    val_loss       : -1525.1106432575732\n",
      "    val_log_likelihood: 1599.7109497070312\n",
      "    val_log_marginal: 1564.7812206938863\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -1805.218018\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -1444.832153\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -1507.447021\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -1799.271240\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -1552.277100\n",
      "    epoch          : 206\n",
      "    loss           : -1531.6113112043627\n",
      "    val_loss       : -1502.3596314113588\n",
      "    val_log_likelihood: 1599.3020874023437\n",
      "    val_log_marginal: 1571.8912485253065\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -1830.493652\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -1633.641846\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -1522.455078\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -1577.309814\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -1550.211914\n",
      "    epoch          : 207\n",
      "    loss           : -1528.3328253113398\n",
      "    val_loss       : -1506.5289599564858\n",
      "    val_log_likelihood: 1608.7242797851563\n",
      "    val_log_marginal: 1576.3133216075598\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -1616.984375\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -1407.750366\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -1430.017578\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -1478.714233\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -1465.018311\n",
      "    epoch          : 208\n",
      "    loss           : -1527.1211372602104\n",
      "    val_loss       : -1524.7576961052605\n",
      "    val_log_likelihood: 1576.9708129882813\n",
      "    val_log_marginal: 1552.52584053576\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -1870.207031\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -1398.461304\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -1578.159912\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -1864.530029\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -1520.857544\n",
      "    epoch          : 209\n",
      "    loss           : -1531.156683893487\n",
      "    val_loss       : -1524.9366204680875\n",
      "    val_log_likelihood: 1588.9945434570313\n",
      "    val_log_marginal: 1563.909496837443\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -1610.234131\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -1638.394043\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -1599.666504\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -1852.921387\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -1432.223389\n",
      "    epoch          : 210\n",
      "    loss           : -1531.026204024211\n",
      "    val_loss       : -1515.2307047675363\n",
      "    val_log_likelihood: 1575.4678955078125\n",
      "    val_log_marginal: 1550.128058699891\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -1834.433105\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -1500.436646\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -1600.619141\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -1384.603149\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -1465.674561\n",
      "    epoch          : 211\n",
      "    loss           : -1533.2406017945545\n",
      "    val_loss       : -1537.858000506088\n",
      "    val_log_likelihood: 1602.170361328125\n",
      "    val_log_marginal: 1573.221869916469\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -1847.215698\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -1493.568359\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -1470.884766\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -1484.971680\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -1431.052979\n",
      "    epoch          : 212\n",
      "    loss           : -1527.2309920811417\n",
      "    val_loss       : -1464.2213095719926\n",
      "    val_log_likelihood: 1547.4728515625\n",
      "    val_log_marginal: 1522.2100306011737\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -1638.931519\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -1412.147949\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -1584.394287\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -1445.493652\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -1523.029663\n",
      "    epoch          : 213\n",
      "    loss           : -1497.0835867400217\n",
      "    val_loss       : -1505.8647907460108\n",
      "    val_log_likelihood: 1576.4668212890624\n",
      "    val_log_marginal: 1547.610902491957\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -1611.346680\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -1589.252808\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -1603.418823\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -1525.001953\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -1463.625732\n",
      "    epoch          : 214\n",
      "    loss           : -1518.1671408473856\n",
      "    val_loss       : -1519.3159284370952\n",
      "    val_log_likelihood: 1569.5973388671875\n",
      "    val_log_marginal: 1537.029433824867\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -1749.158569\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -1591.164062\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -1399.294556\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -1392.247803\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -1468.886841\n",
      "    epoch          : 215\n",
      "    loss           : -1523.424289816677\n",
      "    val_loss       : -1526.2884198520333\n",
      "    val_log_likelihood: 1587.55078125\n",
      "    val_log_marginal: 1567.9943924069405\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -1824.320190\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -1526.915283\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -1381.834473\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -1503.388062\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -1509.968506\n",
      "    epoch          : 216\n",
      "    loss           : -1534.1880632155012\n",
      "    val_loss       : -1534.1505358239635\n",
      "    val_log_likelihood: 1593.075439453125\n",
      "    val_log_marginal: 1571.4041515950114\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -1821.732910\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -1475.695801\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -1430.876953\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -1596.027222\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -1458.254395\n",
      "    epoch          : 217\n",
      "    loss           : -1529.9743048035273\n",
      "    val_loss       : -1500.5744287843816\n",
      "    val_log_likelihood: 1567.39111328125\n",
      "    val_log_marginal: 1538.659891498834\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -1792.753174\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -1582.377930\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -1412.456543\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -1443.525757\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -1444.925293\n",
      "    epoch          : 218\n",
      "    loss           : -1518.2398693726795\n",
      "    val_loss       : -1521.5283541886135\n",
      "    val_log_likelihood: 1570.6927856445313\n",
      "    val_log_marginal: 1539.141074419394\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -1816.210449\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -1615.105347\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -1487.167480\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -1528.562988\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -1466.442505\n",
      "    epoch          : 219\n",
      "    loss           : -1525.9991382561107\n",
      "    val_loss       : -1516.6035466277972\n",
      "    val_log_likelihood: 1587.903466796875\n",
      "    val_log_marginal: 1565.5930950496345\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -1645.999390\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -1457.281006\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -1430.802002\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -1484.201172\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -1466.069824\n",
      "    epoch          : 220\n",
      "    loss           : -1531.2472419361077\n",
      "    val_loss       : -1532.5584507383405\n",
      "    val_log_likelihood: 1590.3534057617187\n",
      "    val_log_marginal: 1565.523052785918\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -1836.706299\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -1435.395264\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -1531.319702\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -1580.026123\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -1513.808838\n",
      "    epoch          : 221\n",
      "    loss           : -1532.2305364325496\n",
      "    val_loss       : -1529.5303272189572\n",
      "    val_log_likelihood: 1575.8337158203126\n",
      "    val_log_marginal: 1552.414937325567\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -1589.320801\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -1399.849365\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -1515.797974\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -1436.909790\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -1534.334229\n",
      "    epoch          : 222\n",
      "    loss           : -1532.1561315555384\n",
      "    val_loss       : -1533.4356169929729\n",
      "    val_log_likelihood: 1590.4946044921876\n",
      "    val_log_marginal: 1561.3909745071082\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -1836.786133\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -1614.019775\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -1441.744751\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -1530.171387\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -1513.313965\n",
      "    epoch          : 223\n",
      "    loss           : -1540.1101775216584\n",
      "    val_loss       : -1538.363093114458\n",
      "    val_log_likelihood: 1592.9036010742188\n",
      "    val_log_marginal: 1564.4447755098342\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -1616.749512\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -1602.529907\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -1582.967773\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -1517.053345\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -1438.119141\n",
      "    epoch          : 224\n",
      "    loss           : -1540.4978861289449\n",
      "    val_loss       : -1540.9971025007776\n",
      "    val_log_likelihood: 1594.12431640625\n",
      "    val_log_marginal: 1561.5998438593\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -1840.416016\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -1504.101318\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -1614.047852\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -1476.351807\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -1440.868652\n",
      "    epoch          : 225\n",
      "    loss           : -1539.4118205155476\n",
      "    val_loss       : -1521.2292212754487\n",
      "    val_log_likelihood: 1594.0129760742188\n",
      "    val_log_marginal: 1569.75284367688\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -1840.287354\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -1455.203735\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -1412.704468\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -1505.264648\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -1526.770386\n",
      "    epoch          : 226\n",
      "    loss           : -1543.6431691386913\n",
      "    val_loss       : -1546.9533367821016\n",
      "    val_log_likelihood: 1607.364208984375\n",
      "    val_log_marginal: 1574.7863939840347\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -1859.743896\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -1653.732666\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -1609.443726\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -1391.213989\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -1549.217529\n",
      "    epoch          : 227\n",
      "    loss           : -1550.3176378306775\n",
      "    val_loss       : -1551.7436260418967\n",
      "    val_log_likelihood: 1607.0117797851562\n",
      "    val_log_marginal: 1579.3897418767215\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -1870.618042\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -1485.521118\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -1426.077393\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -1437.262695\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -1523.411865\n",
      "    epoch          : 228\n",
      "    loss           : -1546.3369370262221\n",
      "    val_loss       : -1542.5550601292402\n",
      "    val_log_likelihood: 1598.7261474609375\n",
      "    val_log_marginal: 1563.6760933801531\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -1826.532593\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -1443.294312\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -1604.087646\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -1422.844116\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -1527.405151\n",
      "    epoch          : 229\n",
      "    loss           : -1544.0956281907488\n",
      "    val_loss       : -1525.3954160158523\n",
      "    val_log_likelihood: 1566.10966796875\n",
      "    val_log_marginal: 1540.6004546966403\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -1834.709473\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -1416.354126\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -1588.960938\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -1530.155029\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -1445.583740\n",
      "    epoch          : 230\n",
      "    loss           : -1537.8904570022432\n",
      "    val_loss       : -1540.2208758979104\n",
      "    val_log_likelihood: 1590.719970703125\n",
      "    val_log_marginal: 1568.231382983923\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -1848.630249\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -1452.069824\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -1428.330200\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -1549.038574\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -1534.625000\n",
      "    epoch          : 231\n",
      "    loss           : -1541.9602232073794\n",
      "    val_loss       : -1538.6271906369366\n",
      "    val_log_likelihood: 1616.3229125976563\n",
      "    val_log_marginal: 1587.6495661575348\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -1855.374023\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -1463.314453\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -1443.543213\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -1857.464111\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -1502.619019\n",
      "    epoch          : 232\n",
      "    loss           : -1520.859184038521\n",
      "    val_loss       : -1492.5315157162026\n",
      "    val_log_likelihood: 1568.096728515625\n",
      "    val_log_marginal: 1543.3025321807713\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -1793.312500\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -1571.764160\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -1518.653076\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -1492.888184\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -1429.703979\n",
      "    epoch          : 233\n",
      "    loss           : -1524.418186301052\n",
      "    val_loss       : -1550.569708321523\n",
      "    val_log_likelihood: 1614.1106567382812\n",
      "    val_log_marginal: 1586.523436421901\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -1814.426758\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -1448.626587\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -1618.504517\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -1567.745239\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -1536.718506\n",
      "    epoch          : 234\n",
      "    loss           : -1544.0939627165842\n",
      "    val_loss       : -1548.1175267956219\n",
      "    val_log_likelihood: 1624.4117797851563\n",
      "    val_log_marginal: 1586.5523560352624\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -1815.305786\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -1632.682373\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -1583.545410\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -1564.607544\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -1518.295044\n",
      "    epoch          : 235\n",
      "    loss           : -1547.6725639116646\n",
      "    val_loss       : -1537.2621705560014\n",
      "    val_log_likelihood: 1599.0119262695312\n",
      "    val_log_marginal: 1561.5310704022645\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -1457.176758\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -1475.430542\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -1585.126953\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -1525.594238\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -1499.480591\n",
      "    epoch          : 236\n",
      "    loss           : -1542.6849123510983\n",
      "    val_loss       : -1533.3201539843344\n",
      "    val_log_likelihood: 1608.604296875\n",
      "    val_log_marginal: 1581.1686173969956\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -1858.443481\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -1476.719971\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -1524.807739\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -1434.735840\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -1458.446045\n",
      "    epoch          : 237\n",
      "    loss           : -1531.17463789836\n",
      "    val_loss       : -1530.5422104981728\n",
      "    val_log_likelihood: 1583.3636474609375\n",
      "    val_log_marginal: 1552.614630419761\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -1836.855957\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -1583.580322\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -1437.768555\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -1405.769653\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -1487.513184\n",
      "    epoch          : 238\n",
      "    loss           : -1533.8741998955754\n",
      "    val_loss       : -1535.9899791610428\n",
      "    val_log_likelihood: 1600.2670776367188\n",
      "    val_log_marginal: 1571.8384202536195\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -1873.804443\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -1635.699463\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -1521.125732\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -1527.379150\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -1564.822266\n",
      "    epoch          : 239\n",
      "    loss           : -1540.0548107789295\n",
      "    val_loss       : -1536.7907664133236\n",
      "    val_log_likelihood: 1620.6608764648438\n",
      "    val_log_marginal: 1585.550088256225\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -1869.245239\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -1473.546997\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -1546.433472\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -1590.008545\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -1538.769531\n",
      "    epoch          : 240\n",
      "    loss           : -1550.6184976407797\n",
      "    val_loss       : -1544.230221431423\n",
      "    val_log_likelihood: 1614.3960693359375\n",
      "    val_log_marginal: 1588.7836504172533\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -1826.781006\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -1510.008301\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -1352.285156\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -1462.095703\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -1488.161987\n",
      "    epoch          : 241\n",
      "    loss           : -1531.2480299543627\n",
      "    val_loss       : -1496.04822556274\n",
      "    val_log_likelihood: 1617.3582153320312\n",
      "    val_log_marginal: 1580.4871465992182\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -1850.453857\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -1338.899658\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -1636.477661\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -1475.105957\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -1457.457520\n",
      "    epoch          : 242\n",
      "    loss           : -1524.7153392829518\n",
      "    val_loss       : -1530.9780837335625\n",
      "    val_log_likelihood: 1624.2552734375\n",
      "    val_log_marginal: 1597.9317800145595\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -1858.834961\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -1570.247192\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -1515.582275\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -1532.660767\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -1561.515747\n",
      "    epoch          : 243\n",
      "    loss           : -1536.4808953917852\n",
      "    val_loss       : -1532.8693007220513\n",
      "    val_log_likelihood: 1593.1359619140626\n",
      "    val_log_marginal: 1565.9429288119077\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -1831.216675\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -1383.502197\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -1466.297119\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -1593.713135\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -1541.642578\n",
      "    epoch          : 244\n",
      "    loss           : -1531.951035301284\n",
      "    val_loss       : -1540.9668812753632\n",
      "    val_log_likelihood: 1611.898291015625\n",
      "    val_log_marginal: 1575.9220309145749\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -1792.884766\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -1459.900635\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -1539.058350\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -1485.665894\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -1538.558350\n",
      "    epoch          : 245\n",
      "    loss           : -1541.707304397432\n",
      "    val_loss       : -1544.0141135654412\n",
      "    val_log_likelihood: 1611.7047241210937\n",
      "    val_log_marginal: 1585.5765033341945\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -1801.501587\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -1623.969482\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -1428.720947\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -1431.416992\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -1509.012207\n",
      "    epoch          : 246\n",
      "    loss           : -1541.747123491646\n",
      "    val_loss       : -1538.772354960814\n",
      "    val_log_likelihood: 1611.7890258789062\n",
      "    val_log_marginal: 1573.0154264923185\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -1805.851929\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -1634.661743\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -1576.165283\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -1576.990356\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -1563.544189\n",
      "    epoch          : 247\n",
      "    loss           : -1535.6863663172958\n",
      "    val_loss       : -1527.210975279566\n",
      "    val_log_likelihood: 1620.740966796875\n",
      "    val_log_marginal: 1593.845647933334\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -1794.598633\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -1538.218872\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -1431.431763\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -1381.046143\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -1515.627930\n",
      "    epoch          : 248\n",
      "    loss           : -1539.5818499574566\n",
      "    val_loss       : -1521.3333836128936\n",
      "    val_log_likelihood: 1595.758349609375\n",
      "    val_log_marginal: 1562.958918524161\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -1454.514893\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -1615.498535\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -1576.659668\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -1468.265869\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -1524.338013\n",
      "    epoch          : 249\n",
      "    loss           : -1534.665909266708\n",
      "    val_loss       : -1528.356453847699\n",
      "    val_log_likelihood: 1589.1789916992188\n",
      "    val_log_marginal: 1560.0372148036956\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -1813.785400\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -1471.293457\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -1558.917725\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -1820.011841\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -1487.104492\n",
      "    epoch          : 250\n",
      "    loss           : -1536.9589892094677\n",
      "    val_loss       : -1521.988839573972\n",
      "    val_log_likelihood: 1597.2710327148438\n",
      "    val_log_marginal: 1565.7340083010508\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -1819.775513\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -1456.686157\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -1421.538330\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -1446.024170\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -1554.486450\n",
      "    epoch          : 251\n",
      "    loss           : -1546.6271223313738\n",
      "    val_loss       : -1545.3070487503894\n",
      "    val_log_likelihood: 1616.3743286132812\n",
      "    val_log_marginal: 1585.326673818007\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -1856.458862\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -1621.990112\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -1550.011719\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -1620.787598\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -1461.646484\n",
      "    epoch          : 252\n",
      "    loss           : -1537.6570259320854\n",
      "    val_loss       : -1542.1068898942322\n",
      "    val_log_likelihood: 1625.9820922851563\n",
      "    val_log_marginal: 1599.0789923079312\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -1638.456421\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -1594.548096\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -1475.472900\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -1440.124146\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -1448.277344\n",
      "    epoch          : 253\n",
      "    loss           : -1553.5769997776144\n",
      "    val_loss       : -1534.2017013818026\n",
      "    val_log_likelihood: 1618.0417236328126\n",
      "    val_log_marginal: 1596.8480977412314\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -1857.408203\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -1589.526611\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -1447.492676\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -1442.052490\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -1555.434448\n",
      "    epoch          : 254\n",
      "    loss           : -1556.437282448948\n",
      "    val_loss       : -1560.2324578486382\n",
      "    val_log_likelihood: 1632.1162841796875\n",
      "    val_log_marginal: 1603.0780277285726\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -1818.952881\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -1495.215332\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -1492.555908\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -1873.495361\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -1556.393188\n",
      "    epoch          : 255\n",
      "    loss           : -1562.221460927831\n",
      "    val_loss       : -1550.6447463895195\n",
      "    val_log_likelihood: 1633.310791015625\n",
      "    val_log_marginal: 1610.5546310305594\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -1524.616577\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -1512.731201\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -1418.336426\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -1488.224854\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -1557.171631\n",
      "    epoch          : 256\n",
      "    loss           : -1552.964842541383\n",
      "    val_loss       : -1544.2456712882965\n",
      "    val_log_likelihood: 1617.639306640625\n",
      "    val_log_marginal: 1583.8885245494544\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -1858.871094\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -1569.241699\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -1430.067139\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -1480.415894\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -1520.847778\n",
      "    epoch          : 257\n",
      "    loss           : -1550.5769997776144\n",
      "    val_loss       : -1549.0131289213896\n",
      "    val_log_likelihood: 1630.9982543945312\n",
      "    val_log_marginal: 1599.4957896810024\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -1872.661987\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -1444.666260\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -1606.333740\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -1558.908936\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -1582.894043\n",
      "    epoch          : 258\n",
      "    loss           : -1559.7915196182705\n",
      "    val_loss       : -1551.4673451946117\n",
      "    val_log_likelihood: 1639.708837890625\n",
      "    val_log_marginal: 1608.4867316726595\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -1864.249146\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -1596.992188\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -1548.833496\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -1534.383057\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -1439.566162\n",
      "    epoch          : 259\n",
      "    loss           : -1532.4877881342823\n",
      "    val_loss       : -1510.3469570929185\n",
      "    val_log_likelihood: 1604.2140502929688\n",
      "    val_log_marginal: 1579.7837786275893\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -1495.906128\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -1582.831055\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -1426.299194\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -1472.827881\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -1491.617920\n",
      "    epoch          : 260\n",
      "    loss           : -1526.6053986502166\n",
      "    val_loss       : -1544.588795300387\n",
      "    val_log_likelihood: 1604.2180297851562\n",
      "    val_log_marginal: 1577.208309358731\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -1863.307861\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -1511.915649\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -1530.034424\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -1522.515381\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -1474.602783\n",
      "    epoch          : 261\n",
      "    loss           : -1547.643852007271\n",
      "    val_loss       : -1545.7557253400796\n",
      "    val_log_likelihood: 1620.5981323242188\n",
      "    val_log_marginal: 1587.6032288271933\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -1853.454712\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -1631.085938\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -1531.954102\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -1403.627563\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -1515.077515\n",
      "    epoch          : 262\n",
      "    loss           : -1549.0519898669554\n",
      "    val_loss       : -1543.9329992851242\n",
      "    val_log_likelihood: 1619.9539916992187\n",
      "    val_log_marginal: 1588.6949448790401\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -1867.536255\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -1520.472168\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -1487.880737\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -1520.720215\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -1526.329224\n",
      "    epoch          : 263\n",
      "    loss           : -1550.3778535446318\n",
      "    val_loss       : -1554.4916348082945\n",
      "    val_log_likelihood: 1636.917626953125\n",
      "    val_log_marginal: 1616.8645561926066\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -1841.360962\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -1484.588501\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -1606.142334\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -1503.448608\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -1476.241211\n",
      "    epoch          : 264\n",
      "    loss           : -1558.179114615563\n",
      "    val_loss       : -1561.1000189696438\n",
      "    val_log_likelihood: 1629.0561767578124\n",
      "    val_log_marginal: 1594.552288231626\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -1863.489746\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -1472.803101\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -1309.329346\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -1351.471680\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -1495.872192\n",
      "    epoch          : 265\n",
      "    loss           : -1529.3606525081218\n",
      "    val_loss       : -1528.1549978656694\n",
      "    val_log_likelihood: 1587.5008911132813\n",
      "    val_log_marginal: 1562.5632923528551\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -1818.520020\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -1447.772339\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -1500.522949\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -1611.373901\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -1422.952637\n",
      "    epoch          : 266\n",
      "    loss           : -1537.6030406385364\n",
      "    val_loss       : -1550.559936513938\n",
      "    val_log_likelihood: 1621.1115356445312\n",
      "    val_log_marginal: 1586.2613278452307\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -1835.073975\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -1383.255859\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -1468.823853\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -1844.437866\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -1575.208008\n",
      "    epoch          : 267\n",
      "    loss           : -1536.3867078724475\n",
      "    val_loss       : -1539.5174015582538\n",
      "    val_log_likelihood: 1604.2687133789063\n",
      "    val_log_marginal: 1564.5831028025598\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -1583.087769\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -1541.782471\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -1540.027222\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -1570.113037\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -1534.604858\n",
      "    epoch          : 268\n",
      "    loss           : -1540.252006304146\n",
      "    val_loss       : -1520.1070759392344\n",
      "    val_log_likelihood: 1619.3313598632812\n",
      "    val_log_marginal: 1592.58077830486\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -1846.344116\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -1462.752686\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -1507.277222\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -1563.367920\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -1549.457397\n",
      "    epoch          : 269\n",
      "    loss           : -1547.8238138633199\n",
      "    val_loss       : -1549.4110595696607\n",
      "    val_log_likelihood: 1625.4163940429687\n",
      "    val_log_marginal: 1603.2898519724608\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -1872.563477\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -1462.074829\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -1615.694336\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -1414.818970\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -1460.999023\n",
      "    epoch          : 270\n",
      "    loss           : -1549.1859771426361\n",
      "    val_loss       : -1539.8713730082848\n",
      "    val_log_likelihood: 1607.1408447265626\n",
      "    val_log_marginal: 1581.9322579413652\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -1875.214478\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -1486.003296\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -1603.320679\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -1519.052368\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -1479.280273\n",
      "    epoch          : 271\n",
      "    loss           : -1545.162218150526\n",
      "    val_loss       : -1553.7678343650884\n",
      "    val_log_likelihood: 1618.2302978515625\n",
      "    val_log_marginal: 1594.4716961119325\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -1856.853027\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -1409.224609\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -1597.449219\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -1877.886719\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -1537.113525\n",
      "    epoch          : 272\n",
      "    loss           : -1540.7908657564976\n",
      "    val_loss       : -1510.1904261036775\n",
      "    val_log_likelihood: 1610.5951538085938\n",
      "    val_log_marginal: 1580.1984317656606\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -1828.642456\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -1580.657104\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -1484.166382\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -1468.645264\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -1493.439941\n",
      "    epoch          : 273\n",
      "    loss           : -1531.6559055441678\n",
      "    val_loss       : -1542.8712190896272\n",
      "    val_log_likelihood: 1633.3983642578125\n",
      "    val_log_marginal: 1604.6883750941604\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -1867.428589\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -1414.826172\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -1590.163208\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -1423.704102\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -1467.153320\n",
      "    epoch          : 274\n",
      "    loss           : -1540.5002634784962\n",
      "    val_loss       : -1548.5688964532687\n",
      "    val_log_likelihood: 1633.5254150390624\n",
      "    val_log_marginal: 1603.133261576295\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -1821.826538\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -1620.995605\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -1447.222412\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -1554.594238\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -1473.815674\n",
      "    epoch          : 275\n",
      "    loss           : -1552.8542395865563\n",
      "    val_loss       : -1534.4532607087865\n",
      "    val_log_likelihood: 1636.7336791992188\n",
      "    val_log_marginal: 1610.7070406132318\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -1859.698242\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -1626.430298\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -1560.619995\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -1515.783691\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -1558.079590\n",
      "    epoch          : 276\n",
      "    loss           : -1550.2471960086634\n",
      "    val_loss       : -1549.046425983403\n",
      "    val_log_likelihood: 1633.2289306640625\n",
      "    val_log_marginal: 1605.939991775155\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -1642.956543\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -1521.256592\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -1406.238281\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -1605.223145\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -1550.043457\n",
      "    epoch          : 277\n",
      "    loss           : -1555.6735658551206\n",
      "    val_loss       : -1566.7312963598408\n",
      "    val_log_likelihood: 1646.9384887695312\n",
      "    val_log_marginal: 1608.1833930619061\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -1840.129883\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -1612.922607\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -1444.812500\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -1860.198853\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -1507.013184\n",
      "    epoch          : 278\n",
      "    loss           : -1562.0792465965346\n",
      "    val_loss       : -1562.1035237242468\n",
      "    val_log_likelihood: 1645.811083984375\n",
      "    val_log_marginal: 1605.6672057091253\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -1657.360352\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -1658.809814\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -1640.941284\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -1417.256226\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -1548.501465\n",
      "    epoch          : 279\n",
      "    loss           : -1563.6003816812345\n",
      "    val_loss       : -1538.7609689632432\n",
      "    val_log_likelihood: 1626.713623046875\n",
      "    val_log_marginal: 1602.8724169258028\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -1837.098389\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -1497.374878\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -1605.713867\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -1470.293945\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -1500.444458\n",
      "    epoch          : 280\n",
      "    loss           : -1551.100555722076\n",
      "    val_loss       : -1562.8691333492286\n",
      "    val_log_likelihood: 1633.0961791992188\n",
      "    val_log_marginal: 1600.0478923480957\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -1875.617920\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -1622.677368\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -1556.435547\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -1848.043335\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -1474.785767\n",
      "    epoch          : 281\n",
      "    loss           : -1562.3792422455135\n",
      "    val_loss       : -1547.1894875573926\n",
      "    val_log_likelihood: 1637.531298828125\n",
      "    val_log_marginal: 1609.264164929092\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -1845.141357\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -1494.199463\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -1496.144409\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -1581.637085\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -1505.103149\n",
      "    epoch          : 282\n",
      "    loss           : -1556.1027517790842\n",
      "    val_loss       : -1528.5344101498836\n",
      "    val_log_likelihood: 1634.05634765625\n",
      "    val_log_marginal: 1595.574017266184\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -1388.895996\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -1635.637207\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -1613.707764\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -1460.238159\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -1550.685181\n",
      "    epoch          : 283\n",
      "    loss           : -1554.133925636216\n",
      "    val_loss       : -1543.2552748136222\n",
      "    val_log_likelihood: 1633.0251220703126\n",
      "    val_log_marginal: 1611.6315281059592\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -1841.741577\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -1618.314941\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -1475.588989\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -1466.972900\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -1552.154053\n",
      "    epoch          : 284\n",
      "    loss           : -1557.4384487643101\n",
      "    val_loss       : -1562.8235906559044\n",
      "    val_log_likelihood: 1623.7166748046875\n",
      "    val_log_marginal: 1573.9511323783547\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -1874.867065\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -1496.135986\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -1485.242065\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -1489.094604\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -1539.529907\n",
      "    epoch          : 285\n",
      "    loss           : -1553.2843911954672\n",
      "    val_loss       : -1553.6312883166597\n",
      "    val_log_likelihood: 1643.3038696289063\n",
      "    val_log_marginal: 1607.7670109849423\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -1662.555908\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -1530.879883\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -1531.176392\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -1895.392822\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -1532.586426\n",
      "    epoch          : 286\n",
      "    loss           : -1557.7150637182858\n",
      "    val_loss       : -1556.628988575656\n",
      "    val_log_likelihood: 1648.8868774414063\n",
      "    val_log_marginal: 1613.1341024775058\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -1854.335815\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -1499.120728\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -1447.321655\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -1453.914795\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -1470.651855\n",
      "    epoch          : 287\n",
      "    loss           : -1557.1881284808169\n",
      "    val_loss       : -1551.6241486947051\n",
      "    val_log_likelihood: 1650.7625244140625\n",
      "    val_log_marginal: 1623.458460105583\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -1877.854736\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -1497.911865\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -1536.762695\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -1519.634766\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -1487.619873\n",
      "    epoch          : 288\n",
      "    loss           : -1568.549559338258\n",
      "    val_loss       : -1555.5869702755474\n",
      "    val_log_likelihood: 1643.490771484375\n",
      "    val_log_marginal: 1613.9204468496143\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -1485.735352\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -1516.069580\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -1598.851685\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -1500.055664\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -1469.889526\n",
      "    epoch          : 289\n",
      "    loss           : -1557.1088722153465\n",
      "    val_loss       : -1538.172809971217\n",
      "    val_log_likelihood: 1636.6266723632812\n",
      "    val_log_marginal: 1595.6115167748183\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -1896.581543\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -1392.558472\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -1482.140625\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -1862.766357\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -1510.833252\n",
      "    epoch          : 290\n",
      "    loss           : -1559.8929213722154\n",
      "    val_loss       : -1555.835601242911\n",
      "    val_log_likelihood: 1647.7169921875\n",
      "    val_log_marginal: 1615.1090834829956\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -1865.566528\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -1532.450684\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -1551.684448\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -1840.763184\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -1551.212524\n",
      "    epoch          : 291\n",
      "    loss           : -1561.6417997756807\n",
      "    val_loss       : -1554.403060316015\n",
      "    val_log_likelihood: 1647.2804077148437\n",
      "    val_log_marginal: 1603.8238085474818\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -1883.517700\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -1533.496460\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -1564.341431\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -1313.709473\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -1520.167480\n",
      "    epoch          : 292\n",
      "    loss           : -1554.8381673982828\n",
      "    val_loss       : -1537.44846200021\n",
      "    val_log_likelihood: 1630.8582153320312\n",
      "    val_log_marginal: 1605.0179269649088\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -1862.133057\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -1491.162476\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -1615.911133\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -1548.203613\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -1451.691895\n",
      "    epoch          : 293\n",
      "    loss           : -1559.6855613784035\n",
      "    val_loss       : -1554.5277100234293\n",
      "    val_log_likelihood: 1646.40107421875\n",
      "    val_log_marginal: 1608.1294314600527\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -1850.455322\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -1498.110718\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -1464.356201\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -1545.308105\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -1572.550659\n",
      "    epoch          : 294\n",
      "    loss           : -1561.929577515857\n",
      "    val_loss       : -1558.071300303843\n",
      "    val_log_likelihood: 1639.3518676757812\n",
      "    val_log_marginal: 1610.9667569220067\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -1886.056030\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -1495.702637\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -1580.331299\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -1432.362427\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -1524.208740\n",
      "    epoch          : 295\n",
      "    loss           : -1553.9243357441212\n",
      "    val_loss       : -1559.3236443225294\n",
      "    val_log_likelihood: 1631.15546875\n",
      "    val_log_marginal: 1600.9589335866272\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -1845.843140\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -1532.123657\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -1399.816772\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -1859.574951\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -1538.370117\n",
      "    epoch          : 296\n",
      "    loss           : -1562.8410136912128\n",
      "    val_loss       : -1552.8057638363912\n",
      "    val_log_likelihood: 1628.8145751953125\n",
      "    val_log_marginal: 1590.592304047197\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -1872.218262\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -1511.104736\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -1419.404785\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -1855.682007\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -1508.903320\n",
      "    epoch          : 297\n",
      "    loss           : -1554.901664507271\n",
      "    val_loss       : -1551.4060588630848\n",
      "    val_log_likelihood: 1635.4948486328126\n",
      "    val_log_marginal: 1604.1990697789938\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -1813.787964\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -1421.766846\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -1599.419067\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -1471.867065\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -1582.421509\n",
      "    epoch          : 298\n",
      "    loss           : -1565.3926530592512\n",
      "    val_loss       : -1576.2724378827959\n",
      "    val_log_likelihood: 1645.0555786132813\n",
      "    val_log_marginal: 1606.816300611943\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -1839.499512\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -1441.988403\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -1557.304565\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -1604.720703\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -1577.113281\n",
      "    epoch          : 299\n",
      "    loss           : -1569.9121287128712\n",
      "    val_loss       : -1576.680310401693\n",
      "    val_log_likelihood: 1637.5342529296875\n",
      "    val_log_marginal: 1598.5797389540942\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -1657.775391\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -1446.028076\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -1622.854248\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -1416.031250\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -1553.078125\n",
      "    epoch          : 300\n",
      "    loss           : -1570.0487072633045\n",
      "    val_loss       : -1558.9186731895431\n",
      "    val_log_likelihood: 1628.86845703125\n",
      "    val_log_marginal: 1602.992540179938\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -1871.627930\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -1541.240723\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -1608.749878\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -1871.962769\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -1435.750244\n",
      "    epoch          : 301\n",
      "    loss           : -1567.3994213142018\n",
      "    val_loss       : -1560.0886750723234\n",
      "    val_log_likelihood: 1644.868701171875\n",
      "    val_log_marginal: 1611.5923529241234\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -1854.760498\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -1595.063843\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -1619.317993\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -1480.708984\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -1537.908325\n",
      "    epoch          : 302\n",
      "    loss           : -1562.4878787805537\n",
      "    val_loss       : -1562.2986300846562\n",
      "    val_log_likelihood: 1634.1756103515625\n",
      "    val_log_marginal: 1607.7591704010963\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -1877.349976\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -1512.144043\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -1562.461182\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -1561.508789\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -1546.100830\n",
      "    epoch          : 303\n",
      "    loss           : -1565.264377707302\n",
      "    val_loss       : -1564.1454136767425\n",
      "    val_log_likelihood: 1652.59169921875\n",
      "    val_log_marginal: 1628.125204476714\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -1871.158325\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -1631.051270\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -1631.933228\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -1567.499756\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -1584.647339\n",
      "    epoch          : 304\n",
      "    loss           : -1573.3519275023205\n",
      "    val_loss       : -1574.3565399167128\n",
      "    val_log_likelihood: 1644.1191040039062\n",
      "    val_log_marginal: 1614.439042012766\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -1872.858276\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -1462.014893\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -1395.047729\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -1491.790161\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -1557.213135\n",
      "    epoch          : 305\n",
      "    loss           : -1572.824053169477\n",
      "    val_loss       : -1573.055640658457\n",
      "    val_log_likelihood: 1651.8968139648437\n",
      "    val_log_marginal: 1617.4211166599482\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -1852.255493\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -1537.444092\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -1605.753662\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -1545.135254\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -1423.394165\n",
      "    epoch          : 306\n",
      "    loss           : -1566.7269746383818\n",
      "    val_loss       : -1552.6348221973517\n",
      "    val_log_likelihood: 1630.6053466796875\n",
      "    val_log_marginal: 1603.9498685183175\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -1833.080322\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -1473.963623\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -1623.256836\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -1564.496216\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -1573.150635\n",
      "    epoch          : 307\n",
      "    loss           : -1564.174914671643\n",
      "    val_loss       : -1572.9623077396304\n",
      "    val_log_likelihood: 1652.0589721679687\n",
      "    val_log_marginal: 1617.5969203345478\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -1885.826294\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -1665.867554\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -1476.160889\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -1498.216553\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -1478.538208\n",
      "    epoch          : 308\n",
      "    loss           : -1574.0059717763768\n",
      "    val_loss       : -1566.1906040502713\n",
      "    val_log_likelihood: 1657.551171875\n",
      "    val_log_marginal: 1621.9824032109232\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -1858.170898\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -1486.479614\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -1556.495483\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -1494.104126\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -1523.802490\n",
      "    epoch          : 309\n",
      "    loss           : -1578.766845703125\n",
      "    val_loss       : -1573.9663828380405\n",
      "    val_log_likelihood: 1643.3498168945312\n",
      "    val_log_marginal: 1608.845256451145\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -1893.432617\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -1518.708740\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -1628.205200\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -1515.493530\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -1600.478516\n",
      "    epoch          : 310\n",
      "    loss           : -1576.2936153600713\n",
      "    val_loss       : -1578.9569623962045\n",
      "    val_log_likelihood: 1662.2546508789062\n",
      "    val_log_marginal: 1623.0011878356338\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -1864.915039\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -1538.477295\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -1560.791138\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -1556.950073\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -1580.663696\n",
      "    epoch          : 311\n",
      "    loss           : -1575.8093757251702\n",
      "    val_loss       : -1538.9990709965118\n",
      "    val_log_likelihood: 1660.4562866210938\n",
      "    val_log_marginal: 1640.2900116853416\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -1386.567139\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -1496.843506\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -1507.353760\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -1595.017822\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -1543.839722\n",
      "    epoch          : 312\n",
      "    loss           : -1571.605960657101\n",
      "    val_loss       : -1549.1009800423867\n",
      "    val_log_likelihood: 1643.7528076171875\n",
      "    val_log_marginal: 1611.0396620817482\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -1917.897339\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -1507.111450\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -1634.603027\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -1414.859985\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -1579.150879\n",
      "    epoch          : 313\n",
      "    loss           : -1563.2243168896969\n",
      "    val_loss       : -1552.8348592911848\n",
      "    val_log_likelihood: 1640.8659423828126\n",
      "    val_log_marginal: 1612.458820594851\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -1846.249512\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -1508.804688\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -1582.120850\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -1853.772461\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -1575.910400\n",
      "    epoch          : 314\n",
      "    loss           : -1564.4459192257116\n",
      "    val_loss       : -1567.6303395464085\n",
      "    val_log_likelihood: 1664.4819458007812\n",
      "    val_log_marginal: 1628.6945913568138\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -1884.279785\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -1469.853027\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -1517.485840\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -1624.117554\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -1483.505859\n",
      "    epoch          : 315\n",
      "    loss           : -1575.6251281133973\n",
      "    val_loss       : -1558.3624934501015\n",
      "    val_log_likelihood: 1661.7556396484374\n",
      "    val_log_marginal: 1631.6883025463671\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -1878.593628\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -1528.592651\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -1538.955322\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -1842.978760\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -1562.800049\n",
      "    epoch          : 316\n",
      "    loss           : -1566.5122783396503\n",
      "    val_loss       : -1486.1767701898702\n",
      "    val_log_likelihood: 1600.0691772460937\n",
      "    val_log_marginal: 1572.2123037863523\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -1858.417236\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -1505.636108\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -1459.872192\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -1434.810547\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -1531.416748\n",
      "    epoch          : 317\n",
      "    loss           : -1553.7894855159343\n",
      "    val_loss       : -1561.5628621582873\n",
      "    val_log_likelihood: 1644.6882080078126\n",
      "    val_log_marginal: 1588.696065235138\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -1884.674805\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -1491.430176\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -1482.937988\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -1496.711426\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -1564.800537\n",
      "    epoch          : 318\n",
      "    loss           : -1565.3259760790531\n",
      "    val_loss       : -1565.158764771279\n",
      "    val_log_likelihood: 1633.6346801757813\n",
      "    val_log_marginal: 1605.757416285202\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -1840.780762\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -1495.263794\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -1503.921387\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -1541.939575\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -1576.048218\n",
      "    epoch          : 319\n",
      "    loss           : -1573.203558893487\n",
      "    val_loss       : -1588.6636974345893\n",
      "    val_log_likelihood: 1655.5960205078125\n",
      "    val_log_marginal: 1632.8154942326248\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -1519.072266\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -1513.203491\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -1456.214233\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -1569.452637\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -1630.213623\n",
      "    epoch          : 320\n",
      "    loss           : -1581.637126053914\n",
      "    val_loss       : -1568.8313556241803\n",
      "    val_log_likelihood: 1658.4827758789063\n",
      "    val_log_marginal: 1633.7047425061464\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -1851.608398\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -1513.124023\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -1543.924072\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -1892.710693\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -1618.613403\n",
      "    epoch          : 321\n",
      "    loss           : -1570.4944077293471\n",
      "    val_loss       : -1548.7785493160598\n",
      "    val_log_likelihood: 1658.8374633789062\n",
      "    val_log_marginal: 1628.8737116534262\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -1682.380127\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -1628.105225\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -1445.754272\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -1550.149902\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -1554.165771\n",
      "    epoch          : 322\n",
      "    loss           : -1571.867241887763\n",
      "    val_loss       : -1578.419974740967\n",
      "    val_log_likelihood: 1657.3886352539062\n",
      "    val_log_marginal: 1616.713377737999\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -1502.801392\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -1556.108765\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -1466.795532\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -1906.534668\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -1486.991333\n",
      "    epoch          : 323\n",
      "    loss           : -1578.2600049311573\n",
      "    val_loss       : -1593.442438473925\n",
      "    val_log_likelihood: 1665.7134521484375\n",
      "    val_log_marginal: 1620.4992709621788\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -1841.371094\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -1468.902588\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -1437.026733\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -1484.222290\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -1574.645264\n",
      "    epoch          : 324\n",
      "    loss           : -1570.4851025874073\n",
      "    val_loss       : -1569.534578459803\n",
      "    val_log_likelihood: 1659.8560302734375\n",
      "    val_log_marginal: 1617.999957370758\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -1893.123047\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -1489.012085\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -1456.373535\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -1831.241455\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -1589.274658\n",
      "    epoch          : 325\n",
      "    loss           : -1579.7440511873453\n",
      "    val_loss       : -1575.1483608995563\n",
      "    val_log_likelihood: 1645.4724853515625\n",
      "    val_log_marginal: 1617.275341130793\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -1823.058472\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -1503.321533\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -1545.623291\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -1539.157959\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -1578.043945\n",
      "    epoch          : 326\n",
      "    loss           : -1567.307942305461\n",
      "    val_loss       : -1569.7594779485837\n",
      "    val_log_likelihood: 1657.7705322265624\n",
      "    val_log_marginal: 1615.5791229739784\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -1659.391602\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -1487.913330\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -1575.413086\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -1472.730713\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -1468.861084\n",
      "    epoch          : 327\n",
      "    loss           : -1571.6934741936107\n",
      "    val_loss       : -1563.2535192666576\n",
      "    val_log_likelihood: 1667.0975952148438\n",
      "    val_log_marginal: 1631.214920600131\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -1867.042725\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -1526.493164\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -1474.707886\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -1615.090820\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -1561.349487\n",
      "    epoch          : 328\n",
      "    loss           : -1580.0941101678527\n",
      "    val_loss       : -1566.7112001811154\n",
      "    val_log_likelihood: 1653.8316650390625\n",
      "    val_log_marginal: 1626.0945085778833\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -1522.325806\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -1656.555542\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -1500.334351\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -1498.420654\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -1502.743896\n",
      "    epoch          : 329\n",
      "    loss           : -1579.528372283029\n",
      "    val_loss       : -1582.4578906539828\n",
      "    val_log_likelihood: 1665.3874389648438\n",
      "    val_log_marginal: 1634.472756878659\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -1651.504883\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -1501.219238\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -1565.658813\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -1620.483276\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -1507.206055\n",
      "    epoch          : 330\n",
      "    loss           : -1577.094426825495\n",
      "    val_loss       : -1572.6492605549283\n",
      "    val_log_likelihood: 1647.5371704101562\n",
      "    val_log_marginal: 1616.2120120696723\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -1520.777222\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -1691.670410\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -1668.596802\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -1491.842163\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -1416.440430\n",
      "    epoch          : 331\n",
      "    loss           : -1557.356499700263\n",
      "    val_loss       : -1541.895952118095\n",
      "    val_log_likelihood: 1607.7164306640625\n",
      "    val_log_marginal: 1572.5119911339873\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -1856.937500\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -1598.683594\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -1620.798096\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -1575.721924\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -1489.870605\n",
      "    epoch          : 332\n",
      "    loss           : -1563.557290860922\n",
      "    val_loss       : -1554.8853163562715\n",
      "    val_log_likelihood: 1640.4561401367187\n",
      "    val_log_marginal: 1610.6093437485397\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -1868.878174\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -1443.003540\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -1607.667114\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -1838.866089\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -1560.598022\n",
      "    epoch          : 333\n",
      "    loss           : -1551.8213071917544\n",
      "    val_loss       : -1574.210269613564\n",
      "    val_log_likelihood: 1638.4091064453125\n",
      "    val_log_marginal: 1610.8745648175477\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -1894.793945\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -1529.793945\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -1475.589111\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -1620.174805\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -1590.457031\n",
      "    epoch          : 334\n",
      "    loss           : -1573.0791293606899\n",
      "    val_loss       : -1576.3969602887519\n",
      "    val_log_likelihood: 1657.9051147460937\n",
      "    val_log_marginal: 1628.6671163935214\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -1881.552246\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -1460.496826\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -1431.821167\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -1577.558350\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -1555.950439\n",
      "    epoch          : 335\n",
      "    loss           : -1581.6225936436417\n",
      "    val_loss       : -1577.7234497925267\n",
      "    val_log_likelihood: 1655.862158203125\n",
      "    val_log_marginal: 1617.8699610266835\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -1658.095459\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -1687.240112\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -1598.923950\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -1482.106445\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -1577.254028\n",
      "    epoch          : 336\n",
      "    loss           : -1581.4448580600247\n",
      "    val_loss       : -1582.541485254839\n",
      "    val_log_likelihood: 1659.1865600585938\n",
      "    val_log_marginal: 1633.316129689291\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -1885.765991\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -1457.813965\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -1624.638672\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -1880.241943\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -1562.104980\n",
      "    epoch          : 337\n",
      "    loss           : -1582.7035141746596\n",
      "    val_loss       : -1582.187619657349\n",
      "    val_log_likelihood: 1640.162890625\n",
      "    val_log_marginal: 1604.0594077873975\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -1852.273071\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -1640.609253\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -1598.949097\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -1879.100708\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -1575.020508\n",
      "    epoch          : 338\n",
      "    loss           : -1583.4344554938893\n",
      "    val_loss       : -1563.8897338252514\n",
      "    val_log_likelihood: 1663.3029296875\n",
      "    val_log_marginal: 1638.8004619009794\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -1902.516602\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -1501.072754\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -1652.540405\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -1510.536133\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -1491.101929\n",
      "    epoch          : 339\n",
      "    loss           : -1585.6484314569152\n",
      "    val_loss       : -1599.3902315565385\n",
      "    val_log_likelihood: 1663.68515625\n",
      "    val_log_marginal: 1634.2643671602011\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -1675.428467\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -1683.302246\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -1535.040283\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -1472.350098\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -1577.150513\n",
      "    epoch          : 340\n",
      "    loss           : -1569.1537602490719\n",
      "    val_loss       : -1558.5221658263356\n",
      "    val_log_likelihood: 1634.0744873046874\n",
      "    val_log_marginal: 1609.0663215368986\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -1841.613037\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -1478.406128\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -1610.823608\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -1471.656738\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -1549.508423\n",
      "    epoch          : 341\n",
      "    loss           : -1561.072103670328\n",
      "    val_loss       : -1566.9592446041293\n",
      "    val_log_likelihood: 1631.5607055664063\n",
      "    val_log_marginal: 1596.868124167621\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -1674.783081\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -1469.307983\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -1481.071899\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -1597.003418\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -1567.439575\n",
      "    epoch          : 342\n",
      "    loss           : -1572.318713499768\n",
      "    val_loss       : -1568.0610161496327\n",
      "    val_log_likelihood: 1632.8188598632812\n",
      "    val_log_marginal: 1588.5655347757042\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -1476.620361\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -1481.056396\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -1459.126099\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -1558.201172\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -1570.612305\n",
      "    epoch          : 343\n",
      "    loss           : -1569.251466052367\n",
      "    val_loss       : -1571.083229708951\n",
      "    val_log_likelihood: 1651.768115234375\n",
      "    val_log_marginal: 1614.2137970797717\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -1839.078491\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -1629.814453\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -1564.332886\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -1853.077881\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -1586.633911\n",
      "    epoch          : 344\n",
      "    loss           : -1581.9217589727723\n",
      "    val_loss       : -1581.9450375408865\n",
      "    val_log_likelihood: 1647.4300170898437\n",
      "    val_log_marginal: 1608.146780972928\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -1512.663574\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -1505.820923\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -1509.830078\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -1498.773926\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -1481.563477\n",
      "    epoch          : 345\n",
      "    loss           : -1579.8639471979425\n",
      "    val_loss       : -1563.7521673694253\n",
      "    val_log_likelihood: 1638.8333374023437\n",
      "    val_log_marginal: 1608.3095176991076\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -1861.516235\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -1493.969971\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -1635.781250\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -1885.480347\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -1578.510620\n",
      "    epoch          : 346\n",
      "    loss           : -1570.0231305112934\n",
      "    val_loss       : -1564.0836627805606\n",
      "    val_log_likelihood: 1644.5218994140625\n",
      "    val_log_marginal: 1616.6699175514282\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -1839.518555\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -1469.262207\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -1499.661621\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -1355.749146\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -1479.879883\n",
      "    epoch          : 347\n",
      "    loss           : -1569.9229712155786\n",
      "    val_loss       : -1560.967555642966\n",
      "    val_log_likelihood: 1647.2187622070312\n",
      "    val_log_marginal: 1615.4539710704237\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -1869.848999\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -1538.864258\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -1454.880249\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -1528.450928\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -1593.644531\n",
      "    epoch          : 348\n",
      "    loss           : -1560.574464099242\n",
      "    val_loss       : -1552.8821921457536\n",
      "    val_log_likelihood: 1632.8561157226563\n",
      "    val_log_marginal: 1598.1373294085265\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -1792.892090\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -1454.357544\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -1580.692749\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -1492.726440\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -1506.461304\n",
      "    epoch          : 349\n",
      "    loss           : -1566.2531786625927\n",
      "    val_loss       : -1547.3196687389166\n",
      "    val_log_likelihood: 1662.29580078125\n",
      "    val_log_marginal: 1640.8667939521372\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -1806.216797\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -1489.525879\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -1547.366943\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -1536.269775\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -1595.048706\n",
      "    epoch          : 350\n",
      "    loss           : -1561.6313645768873\n",
      "    val_loss       : -1563.8766557351687\n",
      "    val_log_likelihood: 1660.6644165039063\n",
      "    val_log_marginal: 1616.7085956986994\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -1876.522705\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -1548.082642\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -1459.692017\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -1583.707764\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -1560.412231\n",
      "    epoch          : 351\n",
      "    loss           : -1566.2998977510056\n",
      "    val_loss       : -1572.0640290480108\n",
      "    val_log_likelihood: 1671.8040405273437\n",
      "    val_log_marginal: 1625.6749796811491\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -1881.250000\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -1509.576172\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -1614.994263\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -1478.036255\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -1561.275146\n",
      "    epoch          : 352\n",
      "    loss           : -1571.3404927773051\n",
      "    val_loss       : -1566.0749252086505\n",
      "    val_log_likelihood: 1671.0871948242188\n",
      "    val_log_marginal: 1642.6062253903597\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -1879.052124\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -1475.931030\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -1631.535156\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -1588.107056\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -1615.578735\n",
      "    epoch          : 353\n",
      "    loss           : -1585.8660417311262\n",
      "    val_loss       : -1585.9378253099508\n",
      "    val_log_likelihood: 1683.093701171875\n",
      "    val_log_marginal: 1632.5212932813913\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -1875.198486\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -1663.392700\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -1439.519775\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -1566.311890\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -1636.531494\n",
      "    epoch          : 354\n",
      "    loss           : -1565.1816176612779\n",
      "    val_loss       : -1558.8561658759602\n",
      "    val_log_likelihood: 1647.8927612304688\n",
      "    val_log_marginal: 1604.185441255197\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -1886.928101\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -1459.254150\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -1451.000122\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -1850.058838\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -1569.968994\n",
      "    epoch          : 355\n",
      "    loss           : -1563.042073164836\n",
      "    val_loss       : -1569.1349997625687\n",
      "    val_log_likelihood: 1646.0791137695312\n",
      "    val_log_marginal: 1607.407090704888\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -1873.278564\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -1640.515381\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -1621.742798\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -1600.891724\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -1467.596924\n",
      "    epoch          : 356\n",
      "    loss           : -1576.229437799737\n",
      "    val_loss       : -1573.356277948711\n",
      "    val_log_likelihood: 1670.0041870117188\n",
      "    val_log_marginal: 1627.1603112798184\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -1507.847168\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -1690.743530\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -1638.268311\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -1620.937378\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -1613.587646\n",
      "    epoch          : 357\n",
      "    loss           : -1579.7909116839419\n",
      "    val_loss       : -1573.6905225337482\n",
      "    val_log_likelihood: 1634.47197265625\n",
      "    val_log_marginal: 1607.11823894009\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -1868.696045\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -1443.016113\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -1626.919189\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -1575.555908\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -1494.608887\n",
      "    epoch          : 358\n",
      "    loss           : -1576.5093909537438\n",
      "    val_loss       : -1559.0551186192781\n",
      "    val_log_likelihood: 1651.3385375976563\n",
      "    val_log_marginal: 1621.1525137174874\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -1863.838867\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -1528.320068\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -1500.123169\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -1474.108154\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -1500.843506\n",
      "    epoch          : 359\n",
      "    loss           : -1562.7708957785428\n",
      "    val_loss       : -1552.6666434125043\n",
      "    val_log_likelihood: 1671.2603881835937\n",
      "    val_log_marginal: 1624.603810126707\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -1784.144043\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -1439.952148\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -1503.905762\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -1607.343628\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -1553.649536\n",
      "    epoch          : 360\n",
      "    loss           : -1543.3537658087098\n",
      "    val_loss       : -1562.539781832788\n",
      "    val_log_likelihood: 1634.8713500976562\n",
      "    val_log_marginal: 1608.3429629001766\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -1825.383179\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -1443.546143\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -1476.653931\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -1873.691406\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -1534.808594\n",
      "    epoch          : 361\n",
      "    loss           : -1567.3581615485768\n",
      "    val_loss       : -1574.261636263039\n",
      "    val_log_likelihood: 1660.5975463867187\n",
      "    val_log_marginal: 1626.5072073213755\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -1864.556519\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -1505.734497\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -1638.934448\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -1508.693970\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -1567.890137\n",
      "    epoch          : 362\n",
      "    loss           : -1576.915010055693\n",
      "    val_loss       : -1555.556771937851\n",
      "    val_log_likelihood: 1639.228662109375\n",
      "    val_log_marginal: 1606.5130663584919\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -1633.148682\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -1675.147827\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -1552.785645\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -1882.989136\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -1531.884399\n",
      "    epoch          : 363\n",
      "    loss           : -1567.229847520885\n",
      "    val_loss       : -1562.9062821551227\n",
      "    val_log_likelihood: 1656.1477416992188\n",
      "    val_log_marginal: 1626.900906642899\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -1495.862305\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -1510.261230\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -1593.562744\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -1605.688965\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -1554.185303\n",
      "    epoch          : 364\n",
      "    loss           : -1573.7371463586787\n",
      "    val_loss       : -1580.7353305707686\n",
      "    val_log_likelihood: 1680.1822387695313\n",
      "    val_log_marginal: 1636.2144749365748\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -1848.893555\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -1430.900146\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -1553.355835\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -1535.021484\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -1620.469727\n",
      "    epoch          : 365\n",
      "    loss           : -1580.6706180383662\n",
      "    val_loss       : -1555.7677042122\n",
      "    val_log_likelihood: 1619.5479614257813\n",
      "    val_log_marginal: 1585.703469992429\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -1865.008179\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -1665.003174\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -1551.128174\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -1843.088379\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -1594.203735\n",
      "    epoch          : 366\n",
      "    loss           : -1569.2894202506188\n",
      "    val_loss       : -1579.336876801215\n",
      "    val_log_likelihood: 1657.0358642578126\n",
      "    val_log_marginal: 1624.5824253059923\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -1866.022339\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -1505.517090\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -1631.615967\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -1837.060059\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -1502.728516\n",
      "    epoch          : 367\n",
      "    loss           : -1583.6018864093442\n",
      "    val_loss       : -1563.380267949868\n",
      "    val_log_likelihood: 1635.6065307617187\n",
      "    val_log_marginal: 1595.7108061287552\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -1868.477173\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -1490.533936\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -1623.127563\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -1604.341431\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -1529.376221\n",
      "    epoch          : 368\n",
      "    loss           : -1578.065018757735\n",
      "    val_loss       : -1558.1445682027377\n",
      "    val_log_likelihood: 1671.249609375\n",
      "    val_log_marginal: 1643.6662050370128\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -1836.603638\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -1524.948608\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -1668.289307\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -1467.037720\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -1496.321289\n",
      "    epoch          : 369\n",
      "    loss           : -1572.1398720316367\n",
      "    val_loss       : -1582.8472020327113\n",
      "    val_log_likelihood: 1669.6797607421875\n",
      "    val_log_marginal: 1633.3860833734275\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -1622.784546\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -1507.316895\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -1600.284546\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -1543.589600\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -1577.924316\n",
      "    epoch          : 370\n",
      "    loss           : -1571.2172416460396\n",
      "    val_loss       : -1581.8214526792058\n",
      "    val_log_likelihood: 1645.0848022460937\n",
      "    val_log_marginal: 1605.795174601674\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -1812.179321\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -1505.714722\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -1578.802490\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -1589.763428\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -1457.977051\n",
      "    epoch          : 371\n",
      "    loss           : -1570.650147692992\n",
      "    val_loss       : -1587.1822554395535\n",
      "    val_log_likelihood: 1641.1014404296875\n",
      "    val_log_marginal: 1610.8656840596348\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -1583.121338\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -1491.922974\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -1495.036377\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -1553.038818\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -1529.751831\n",
      "    epoch          : 372\n",
      "    loss           : -1582.4876672725866\n",
      "    val_loss       : -1585.04270281801\n",
      "    val_log_likelihood: 1672.5971557617188\n",
      "    val_log_marginal: 1641.320559946075\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -1894.578125\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -1538.645020\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -1564.598877\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -1893.370605\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -1590.121582\n",
      "    epoch          : 373\n",
      "    loss           : -1591.653782004177\n",
      "    val_loss       : -1563.7369523592292\n",
      "    val_log_likelihood: 1678.8130615234375\n",
      "    val_log_marginal: 1644.562903983891\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -1813.048828\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -1510.841797\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -1647.422241\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -1570.457886\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -1487.188110\n",
      "    epoch          : 374\n",
      "    loss           : -1587.799560546875\n",
      "    val_loss       : -1576.1283080737107\n",
      "    val_log_likelihood: 1687.9750610351562\n",
      "    val_log_marginal: 1651.331414026022\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -1834.220215\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -1526.343628\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -1640.313477\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -1628.909424\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -1494.387817\n",
      "    epoch          : 375\n",
      "    loss           : -1586.862738580987\n",
      "    val_loss       : -1581.8421184066683\n",
      "    val_log_likelihood: 1669.5466552734374\n",
      "    val_log_marginal: 1635.8484970390796\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -1852.491699\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -1683.888306\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -1457.731201\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -1546.686401\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -1470.145996\n",
      "    epoch          : 376\n",
      "    loss           : -1589.6202718904703\n",
      "    val_loss       : -1575.3264587508515\n",
      "    val_log_likelihood: 1655.4525634765625\n",
      "    val_log_marginal: 1622.2666496351362\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -1871.326538\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -1598.408691\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -1573.461670\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -1665.216187\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -1574.797119\n",
      "    epoch          : 377\n",
      "    loss           : -1595.3909344059407\n",
      "    val_loss       : -1588.4792874562554\n",
      "    val_log_likelihood: 1674.6343627929687\n",
      "    val_log_marginal: 1643.7101159252227\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -1887.112183\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -1523.800171\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -1557.551514\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -1582.951538\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -1484.611938\n",
      "    epoch          : 378\n",
      "    loss           : -1595.3415285620358\n",
      "    val_loss       : -1572.9541636951267\n",
      "    val_log_likelihood: 1687.00205078125\n",
      "    val_log_marginal: 1648.5210756186395\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -1869.251465\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -1580.985107\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -1533.680298\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -1579.662476\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -1503.653076\n",
      "    epoch          : 379\n",
      "    loss           : -1591.6804767268718\n",
      "    val_loss       : -1599.0938838344068\n",
      "    val_log_likelihood: 1668.9968383789062\n",
      "    val_log_marginal: 1635.0145660307257\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -1880.903564\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -1713.692749\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -1493.249878\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -1561.263794\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -1512.012207\n",
      "    epoch          : 380\n",
      "    loss           : -1597.4347939066367\n",
      "    val_loss       : -1599.6866393612697\n",
      "    val_log_likelihood: 1685.9225952148438\n",
      "    val_log_marginal: 1657.4254801534116\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -1883.607056\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -1508.119995\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -1655.146484\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -1576.568726\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -1601.324707\n",
      "    epoch          : 381\n",
      "    loss           : -1599.6023517268718\n",
      "    val_loss       : -1588.5789822020567\n",
      "    val_log_likelihood: 1691.7176391601563\n",
      "    val_log_marginal: 1658.307975736633\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -1865.720093\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -1669.640625\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -1660.145752\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -1601.151367\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -1499.053223\n",
      "    epoch          : 382\n",
      "    loss           : -1599.5441290222773\n",
      "    val_loss       : -1587.1219649966806\n",
      "    val_log_likelihood: 1689.9599731445312\n",
      "    val_log_marginal: 1657.547816639766\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -1878.870728\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -1478.507324\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -1569.208496\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -1633.725464\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -1548.028442\n",
      "    epoch          : 383\n",
      "    loss           : -1596.178657758354\n",
      "    val_loss       : -1604.6967248591595\n",
      "    val_log_likelihood: 1682.3732788085938\n",
      "    val_log_marginal: 1645.0777214627713\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -1530.967896\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -1571.966919\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -1492.540894\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -1586.062622\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -1532.200317\n",
      "    epoch          : 384\n",
      "    loss           : -1598.6946550123762\n",
      "    val_loss       : -1595.6618192911149\n",
      "    val_log_likelihood: 1661.0035400390625\n",
      "    val_log_marginal: 1630.7305755421519\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -1888.279541\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -1562.150635\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -1555.800171\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -1601.464600\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -1494.430298\n",
      "    epoch          : 385\n",
      "    loss           : -1584.544813099474\n",
      "    val_loss       : -1591.6918394731358\n",
      "    val_log_likelihood: 1683.8700073242187\n",
      "    val_log_marginal: 1649.5996044896542\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -1854.712402\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -1564.519409\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -1612.237671\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -1600.805664\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -1602.985107\n",
      "    epoch          : 386\n",
      "    loss           : -1593.3893547435798\n",
      "    val_loss       : -1593.3839764320292\n",
      "    val_log_likelihood: 1660.1366577148438\n",
      "    val_log_marginal: 1632.2517118107528\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -1885.613647\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -1540.173950\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -1522.760742\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -1654.939453\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -1582.361816\n",
      "    epoch          : 387\n",
      "    loss           : -1591.3649551844833\n",
      "    val_loss       : -1582.5276670536957\n",
      "    val_log_likelihood: 1670.65751953125\n",
      "    val_log_marginal: 1645.7661181505769\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -1867.635254\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -1489.768311\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -1549.996094\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -1872.525024\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -1524.311768\n",
      "    epoch          : 388\n",
      "    loss           : -1595.8042149307705\n",
      "    val_loss       : -1592.5744566930457\n",
      "    val_log_likelihood: 1677.4681396484375\n",
      "    val_log_marginal: 1644.631159063801\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -1859.808838\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -1446.076294\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -1634.969604\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -1593.715454\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -1559.055542\n",
      "    epoch          : 389\n",
      "    loss           : -1589.4785724299968\n",
      "    val_loss       : -1581.653983003646\n",
      "    val_log_likelihood: 1665.9548950195312\n",
      "    val_log_marginal: 1621.785454327613\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -1882.380493\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -1497.032593\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -1624.377930\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -1474.625244\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -1496.653931\n",
      "    epoch          : 390\n",
      "    loss           : -1582.3687816657643\n",
      "    val_loss       : -1567.7949323134496\n",
      "    val_log_likelihood: 1658.7406982421876\n",
      "    val_log_marginal: 1621.9691255807877\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -1838.318359\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -1435.672852\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -1570.474121\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -1598.131592\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -1574.912964\n",
      "    epoch          : 391\n",
      "    loss           : -1587.8139140818378\n",
      "    val_loss       : -1588.2919685438276\n",
      "    val_log_likelihood: 1673.3515380859376\n",
      "    val_log_marginal: 1639.75644909814\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -1675.710938\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -1527.615845\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -1441.012939\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -1558.303833\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -1574.152344\n",
      "    epoch          : 392\n",
      "    loss           : -1590.899444277924\n",
      "    val_loss       : -1588.7903992717154\n",
      "    val_log_likelihood: 1693.5126831054688\n",
      "    val_log_marginal: 1649.343351843208\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -1508.776123\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -1669.147705\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -1629.897705\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -1439.309326\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -1575.367432\n",
      "    epoch          : 393\n",
      "    loss           : -1584.8502221437964\n",
      "    val_loss       : -1577.1366418271325\n",
      "    val_log_likelihood: 1665.7556884765625\n",
      "    val_log_marginal: 1632.44418967776\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -1526.653809\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -1692.387939\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -1633.678101\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -1866.577515\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -1578.956543\n",
      "    epoch          : 394\n",
      "    loss           : -1586.8319115969214\n",
      "    val_loss       : -1581.1698933472858\n",
      "    val_log_likelihood: 1676.3698364257812\n",
      "    val_log_marginal: 1653.5685305099935\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -1868.926758\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -1551.180908\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -1481.854004\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -1577.231201\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -1510.364258\n",
      "    epoch          : 395\n",
      "    loss           : -1594.9923736270111\n",
      "    val_loss       : -1593.0470598335378\n",
      "    val_log_likelihood: 1688.8913208007812\n",
      "    val_log_marginal: 1644.8461649592966\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -1857.491699\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -1518.311523\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -1478.552002\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -1671.360596\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -1517.724243\n",
      "    epoch          : 396\n",
      "    loss           : -1601.3948140663676\n",
      "    val_loss       : -1608.4006022536196\n",
      "    val_log_likelihood: 1677.4336059570312\n",
      "    val_log_marginal: 1645.1546733837574\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -1879.800293\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -1526.952026\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -1662.098022\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -1503.518921\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -1580.908569\n",
      "    epoch          : 397\n",
      "    loss           : -1591.781792669013\n",
      "    val_loss       : -1579.1785767700524\n",
      "    val_log_likelihood: 1682.9151733398437\n",
      "    val_log_marginal: 1652.96810747236\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -1477.528442\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -1470.373535\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -1562.433594\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -1874.880981\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -1556.080200\n",
      "    epoch          : 398\n",
      "    loss           : -1597.2795978206218\n",
      "    val_loss       : -1598.7154650565237\n",
      "    val_log_likelihood: 1674.9093627929688\n",
      "    val_log_marginal: 1626.24837792702\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -1896.833130\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -1562.427979\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -1565.350464\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -1495.761230\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -1519.272217\n",
      "    epoch          : 399\n",
      "    loss           : -1606.2604738745358\n",
      "    val_loss       : -1600.003607787378\n",
      "    val_log_likelihood: 1690.3534912109376\n",
      "    val_log_marginal: 1645.3783052254562\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -1518.466919\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -1544.221436\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -1535.127930\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -1438.113525\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -1543.890503\n",
      "    epoch          : 400\n",
      "    loss           : -1604.2307527749845\n",
      "    val_loss       : -1614.6125098744408\n",
      "    val_log_likelihood: 1689.1337646484376\n",
      "    val_log_marginal: 1647.31898551099\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch400.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -1894.730103\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -1669.308716\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -1466.285767\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -1527.269897\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -1500.769775\n",
      "    epoch          : 401\n",
      "    loss           : -1600.5499593904703\n",
      "    val_loss       : -1584.0032419568859\n",
      "    val_log_likelihood: 1681.2387817382812\n",
      "    val_log_marginal: 1640.8774262212216\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -1688.411621\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -1445.732910\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -1525.884521\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -1554.573242\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -1613.094116\n",
      "    epoch          : 402\n",
      "    loss           : -1589.6884741452661\n",
      "    val_loss       : -1586.1323913931847\n",
      "    val_log_likelihood: 1673.9881469726563\n",
      "    val_log_marginal: 1627.9517654374242\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -1869.553955\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -1492.085693\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -1441.677368\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -1658.539185\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -1515.535889\n",
      "    epoch          : 403\n",
      "    loss           : -1590.4741827332148\n",
      "    val_loss       : -1600.2537994879299\n",
      "    val_log_likelihood: 1676.3682373046875\n",
      "    val_log_marginal: 1641.1188884221017\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -1857.076660\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -1631.707764\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -1517.550049\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -1612.288330\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -1460.450195\n",
      "    epoch          : 404\n",
      "    loss           : -1587.070716178063\n",
      "    val_loss       : -1573.6321361011826\n",
      "    val_log_likelihood: 1694.57763671875\n",
      "    val_log_marginal: 1661.4252075709403\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -1625.364258\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -1436.344238\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -1650.691162\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -1511.833252\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -1557.602661\n",
      "    epoch          : 405\n",
      "    loss           : -1594.6555248298268\n",
      "    val_loss       : -1606.2162023220212\n",
      "    val_log_likelihood: 1687.1223266601562\n",
      "    val_log_marginal: 1658.329481074959\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -1860.981934\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -1710.241943\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -1685.391357\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -1666.168945\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -1641.547607\n",
      "    epoch          : 406\n",
      "    loss           : -1608.4982861811573\n",
      "    val_loss       : -1591.5222107175737\n",
      "    val_log_likelihood: 1703.0505615234374\n",
      "    val_log_marginal: 1664.7842832755298\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -1860.799316\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -1656.753784\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -1525.743652\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -1528.906860\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -1506.372803\n",
      "    epoch          : 407\n",
      "    loss           : -1598.9998851813893\n",
      "    val_loss       : -1602.7857610990293\n",
      "    val_log_likelihood: 1666.3417358398438\n",
      "    val_log_marginal: 1629.947514365241\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -1871.219116\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -1535.403320\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -1618.337158\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -1578.851196\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -1505.968018\n",
      "    epoch          : 408\n",
      "    loss           : -1601.4312683709777\n",
      "    val_loss       : -1604.518784839101\n",
      "    val_log_likelihood: 1687.69208984375\n",
      "    val_log_marginal: 1653.9357872288674\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -1890.368896\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -1515.493164\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -1523.355225\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -1589.520020\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -1564.553955\n",
      "    epoch          : 409\n",
      "    loss           : -1589.5953187848081\n",
      "    val_loss       : -1601.6445814841427\n",
      "    val_log_likelihood: 1667.4964233398437\n",
      "    val_log_marginal: 1633.4447736609727\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -1878.628296\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -1519.944580\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -1514.810547\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -1592.215332\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -1558.345337\n",
      "    epoch          : 410\n",
      "    loss           : -1593.040744894802\n",
      "    val_loss       : -1598.3194203439168\n",
      "    val_log_likelihood: 1696.1742065429687\n",
      "    val_log_marginal: 1662.0072240188717\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -1867.977295\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -1544.528442\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -1553.906372\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -1556.427246\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -1551.425415\n",
      "    epoch          : 411\n",
      "    loss           : -1600.6508293529548\n",
      "    val_loss       : -1594.6085431141778\n",
      "    val_log_likelihood: 1651.6029052734375\n",
      "    val_log_marginal: 1616.70195007734\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -1905.252197\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -1493.881836\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -1479.555298\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -1585.602051\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -1513.152466\n",
      "    epoch          : 412\n",
      "    loss           : -1584.2032797029703\n",
      "    val_loss       : -1603.7298003131523\n",
      "    val_log_likelihood: 1681.7384765625\n",
      "    val_log_marginal: 1647.5446932919324\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -1892.281372\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -1681.150635\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -1500.307739\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -1610.212646\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -1561.881958\n",
      "    epoch          : 413\n",
      "    loss           : -1590.6058180403002\n",
      "    val_loss       : -1587.616521695163\n",
      "    val_log_likelihood: 1662.880810546875\n",
      "    val_log_marginal: 1628.9276585068553\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -1911.725464\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -1645.199097\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -1471.467285\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -1643.139648\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -1631.806763\n",
      "    epoch          : 414\n",
      "    loss           : -1599.956084902924\n",
      "    val_loss       : -1589.2962594365702\n",
      "    val_log_likelihood: 1681.2659423828125\n",
      "    val_log_marginal: 1649.3715366758406\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -1906.359253\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -1616.907227\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -1646.473999\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -1857.012085\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -1550.945435\n",
      "    epoch          : 415\n",
      "    loss           : -1580.0462815690748\n",
      "    val_loss       : -1586.4426141164265\n",
      "    val_log_likelihood: 1684.4104125976562\n",
      "    val_log_marginal: 1633.9646979209035\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -1824.039062\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -1460.813477\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -1607.613159\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -1666.603271\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -1530.429321\n",
      "    epoch          : 416\n",
      "    loss           : -1597.0114093440593\n",
      "    val_loss       : -1601.11538303243\n",
      "    val_log_likelihood: 1690.7821044921875\n",
      "    val_log_marginal: 1650.366669004783\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -1889.911743\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -1491.022705\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -1451.664307\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -1863.751953\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -1591.219727\n",
      "    epoch          : 417\n",
      "    loss           : -1600.2837083268873\n",
      "    val_loss       : -1579.2572808161378\n",
      "    val_log_likelihood: 1674.5264282226562\n",
      "    val_log_marginal: 1642.1271939840167\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -1865.950562\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -1539.356201\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -1465.179321\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -1534.734741\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -1488.415649\n",
      "    epoch          : 418\n",
      "    loss           : -1592.424320032101\n",
      "    val_loss       : -1579.767236988619\n",
      "    val_log_likelihood: 1686.5016357421875\n",
      "    val_log_marginal: 1652.8036718640476\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -1882.824463\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -1514.676025\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -1653.380981\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -1862.164795\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -1505.244751\n",
      "    epoch          : 419\n",
      "    loss           : -1596.5994232479889\n",
      "    val_loss       : -1590.9105933452956\n",
      "    val_log_likelihood: 1676.0271850585937\n",
      "    val_log_marginal: 1640.6077277809381\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -1880.297119\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -1501.301147\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -1451.030518\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -1518.406860\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -1639.029053\n",
      "    epoch          : 420\n",
      "    loss           : -1598.392928623917\n",
      "    val_loss       : -1600.924386575073\n",
      "    val_log_likelihood: 1696.9173950195313\n",
      "    val_log_marginal: 1660.8994199674576\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -1871.097656\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -1652.194458\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -1596.652832\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -1913.245605\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -1536.217041\n",
      "    epoch          : 421\n",
      "    loss           : -1593.8941722907643\n",
      "    val_loss       : -1600.5823107363656\n",
      "    val_log_likelihood: 1694.69453125\n",
      "    val_log_marginal: 1656.6128187458876\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -1901.435669\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -1637.362061\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -1631.004883\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -1889.149170\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -1621.645996\n",
      "    epoch          : 422\n",
      "    loss           : -1601.684729849938\n",
      "    val_loss       : -1616.1829239356332\n",
      "    val_log_likelihood: 1696.7673706054688\n",
      "    val_log_marginal: 1661.7649033948778\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -1681.810791\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -1525.633789\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -1697.025024\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -1545.032959\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -1598.806763\n",
      "    epoch          : 423\n",
      "    loss           : -1601.6346798131963\n",
      "    val_loss       : -1623.377799382247\n",
      "    val_log_likelihood: 1698.9692749023438\n",
      "    val_log_marginal: 1667.257019553706\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -1904.523315\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -1662.564331\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -1643.933472\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -1614.002563\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -1517.340454\n",
      "    epoch          : 424\n",
      "    loss           : -1607.0867049717667\n",
      "    val_loss       : -1606.4004788959398\n",
      "    val_log_likelihood: 1685.1977416992188\n",
      "    val_log_marginal: 1652.7571512199938\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -1901.807861\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -1526.066650\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -1473.998535\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -1882.302490\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -1488.307251\n",
      "    epoch          : 425\n",
      "    loss           : -1594.4009828473081\n",
      "    val_loss       : -1575.2513493484817\n",
      "    val_log_likelihood: 1670.7798706054687\n",
      "    val_log_marginal: 1635.6972761346592\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -1864.714111\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -1473.741699\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -1561.800049\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -1662.226318\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -1505.663086\n",
      "    epoch          : 426\n",
      "    loss           : -1585.9502376140933\n",
      "    val_loss       : -1593.6221115626395\n",
      "    val_log_likelihood: 1684.2383911132813\n",
      "    val_log_marginal: 1645.1737978231163\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -1887.811279\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -1535.421143\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -1546.315063\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -1440.076904\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -1514.100464\n",
      "    epoch          : 427\n",
      "    loss           : -1596.8043261235302\n",
      "    val_loss       : -1594.949088920094\n",
      "    val_log_likelihood: 1693.5404052734375\n",
      "    val_log_marginal: 1669.1398221410811\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -1907.193481\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -1559.376465\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -1643.054932\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -1505.784424\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -1562.095215\n",
      "    epoch          : 428\n",
      "    loss           : -1606.509874400526\n",
      "    val_loss       : -1596.7240983645431\n",
      "    val_log_likelihood: 1692.8922607421875\n",
      "    val_log_marginal: 1651.5696618851275\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -1885.532837\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -1521.278809\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -1447.902344\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -1604.066895\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -1549.950684\n",
      "    epoch          : 429\n",
      "    loss           : -1601.2279330716274\n",
      "    val_loss       : -1598.2624956374057\n",
      "    val_log_likelihood: 1690.8952758789062\n",
      "    val_log_marginal: 1656.1239723622798\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -1919.117920\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -1540.205688\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -1566.575073\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -1504.632812\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -1540.869873\n",
      "    epoch          : 430\n",
      "    loss           : -1613.1358026183477\n",
      "    val_loss       : -1604.1042090584524\n",
      "    val_log_likelihood: 1698.03671875\n",
      "    val_log_marginal: 1669.231403248757\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -1688.227051\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -1701.782959\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -1628.261719\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -1612.290039\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -1610.949219\n",
      "    epoch          : 431\n",
      "    loss           : -1592.021973864867\n",
      "    val_loss       : -1585.4639482839964\n",
      "    val_log_likelihood: 1684.7702758789062\n",
      "    val_log_marginal: 1651.4731400635094\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -1888.924316\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -1699.269653\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -1524.275269\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -1616.293335\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -1565.614746\n",
      "    epoch          : 432\n",
      "    loss           : -1597.66954478651\n",
      "    val_loss       : -1585.7959355344065\n",
      "    val_log_likelihood: 1662.31123046875\n",
      "    val_log_marginal: 1624.5237795244902\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -1854.997192\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -1643.096191\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -1573.694458\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -1881.284302\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -1603.941406\n",
      "    epoch          : 433\n",
      "    loss           : -1607.8912595239017\n",
      "    val_loss       : -1571.558967856597\n",
      "    val_log_likelihood: 1647.5295532226562\n",
      "    val_log_marginal: 1598.7421879876406\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -1602.844727\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -1525.642334\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -1588.460327\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -1512.860107\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -1488.009766\n",
      "    epoch          : 434\n",
      "    loss           : -1584.3066563370205\n",
      "    val_loss       : -1598.2364171910099\n",
      "    val_log_likelihood: 1678.7564575195313\n",
      "    val_log_marginal: 1643.6312523665977\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -1643.829956\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -1570.731445\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -1524.881104\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -1650.535522\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -1550.818848\n",
      "    epoch          : 435\n",
      "    loss           : -1597.9386022586634\n",
      "    val_loss       : -1603.2611525083892\n",
      "    val_log_likelihood: 1697.1878295898437\n",
      "    val_log_marginal: 1657.484137777239\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -1854.491211\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -1475.236572\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -1590.612793\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -1607.684570\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -1601.622192\n",
      "    epoch          : 436\n",
      "    loss           : -1607.5749173306003\n",
      "    val_loss       : -1614.022671258636\n",
      "    val_log_likelihood: 1690.9612060546874\n",
      "    val_log_marginal: 1656.9456544041634\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -1892.923218\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -1687.373779\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -1587.549561\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -1660.874878\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -1617.783203\n",
      "    epoch          : 437\n",
      "    loss           : -1613.6913820776608\n",
      "    val_loss       : -1595.9739730905742\n",
      "    val_log_likelihood: 1680.6948120117188\n",
      "    val_log_marginal: 1643.2907154593618\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -1541.354614\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -1517.952881\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -1600.716431\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -1613.820801\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -1575.663696\n",
      "    epoch          : 438\n",
      "    loss           : -1609.7939344349475\n",
      "    val_loss       : -1610.1085256038234\n",
      "    val_log_likelihood: 1706.7511474609375\n",
      "    val_log_marginal: 1669.2107248906045\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -1514.761719\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -1567.751831\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -1546.576538\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -1626.767212\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -1571.131348\n",
      "    epoch          : 439\n",
      "    loss           : -1599.3524883005878\n",
      "    val_loss       : -1587.2052784352563\n",
      "    val_log_likelihood: 1662.0723876953125\n",
      "    val_log_marginal: 1625.4808329477905\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -1867.731934\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -1656.270630\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -1492.092285\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -1567.640259\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -1573.475098\n",
      "    epoch          : 440\n",
      "    loss           : -1597.9155732711943\n",
      "    val_loss       : -1598.0109264510684\n",
      "    val_log_likelihood: 1666.0867309570312\n",
      "    val_log_marginal: 1616.8073182519524\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -1887.468262\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -1663.734863\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -1517.188721\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -1632.079468\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -1607.909180\n",
      "    epoch          : 441\n",
      "    loss           : -1607.177056340888\n",
      "    val_loss       : -1611.0351952200756\n",
      "    val_log_likelihood: 1658.2368774414062\n",
      "    val_log_marginal: 1623.22606706433\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -1556.235718\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -1538.680542\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -1672.638794\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -1650.412231\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -1604.416260\n",
      "    epoch          : 442\n",
      "    loss           : -1612.1093314897896\n",
      "    val_loss       : -1614.365500839241\n",
      "    val_log_likelihood: 1682.7303955078125\n",
      "    val_log_marginal: 1656.0482556298375\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -1558.400635\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -1553.305664\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -1513.458374\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -1865.507690\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -1646.753418\n",
      "    epoch          : 443\n",
      "    loss           : -1602.276558148979\n",
      "    val_loss       : -1605.5830203226767\n",
      "    val_log_likelihood: 1664.8690063476563\n",
      "    val_log_marginal: 1634.3079702626915\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -1863.852661\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -1536.095093\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -1536.586548\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -1554.447754\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -1606.056396\n",
      "    epoch          : 444\n",
      "    loss           : -1608.2813817392482\n",
      "    val_loss       : -1591.4155099200084\n",
      "    val_log_likelihood: 1662.0456787109374\n",
      "    val_log_marginal: 1635.401016932726\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -1847.301147\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -1584.796753\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -1482.369873\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -1878.468750\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -1514.535278\n",
      "    epoch          : 445\n",
      "    loss           : -1592.7346590249845\n",
      "    val_loss       : -1615.634120996762\n",
      "    val_log_likelihood: 1676.2357299804687\n",
      "    val_log_marginal: 1642.7651051357388\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -1890.409424\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -1520.978149\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -1463.870483\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -1516.248779\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -1534.279541\n",
      "    epoch          : 446\n",
      "    loss           : -1599.565810401841\n",
      "    val_loss       : -1597.950202924665\n",
      "    val_log_likelihood: 1678.1595581054687\n",
      "    val_log_marginal: 1634.5574776310473\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -1898.570557\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -1534.755737\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -1609.485962\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -1410.496704\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -1538.501587\n",
      "    epoch          : 447\n",
      "    loss           : -1601.9560619392018\n",
      "    val_loss       : -1604.732294035703\n",
      "    val_log_likelihood: 1679.6430053710938\n",
      "    val_log_marginal: 1655.8181522969157\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -1904.326904\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -1544.186768\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -1573.522339\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -1568.469116\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -1536.677368\n",
      "    epoch          : 448\n",
      "    loss           : -1601.7385930731746\n",
      "    val_loss       : -1605.5968976796605\n",
      "    val_log_likelihood: 1689.415625\n",
      "    val_log_marginal: 1655.2523026302456\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -1502.421753\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -1538.664551\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -1543.791016\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -1593.539185\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -1620.484131\n",
      "    epoch          : 449\n",
      "    loss           : -1612.885199518487\n",
      "    val_loss       : -1621.6074209240264\n",
      "    val_log_likelihood: 1691.6289916992187\n",
      "    val_log_marginal: 1648.0834270603955\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -1908.788574\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -1549.303955\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -1457.128418\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -1865.671387\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -1522.971191\n",
      "    epoch          : 450\n",
      "    loss           : -1611.13433294013\n",
      "    val_loss       : -1608.835766139254\n",
      "    val_log_likelihood: 1696.3824096679687\n",
      "    val_log_marginal: 1655.3704061899334\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -1914.836548\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -1477.877930\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -1577.624146\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -1657.305786\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -1600.666992\n",
      "    epoch          : 451\n",
      "    loss           : -1607.0909230449413\n",
      "    val_loss       : -1610.5079789309762\n",
      "    val_log_likelihood: 1686.5841552734375\n",
      "    val_log_marginal: 1651.161188943684\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -1875.381104\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -1589.267334\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -1646.266113\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -1900.434082\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -1515.869263\n",
      "    epoch          : 452\n",
      "    loss           : -1618.946725373221\n",
      "    val_loss       : -1612.4349440677092\n",
      "    val_log_likelihood: 1694.1073852539062\n",
      "    val_log_marginal: 1664.1993613474071\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -1876.181030\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -1682.446411\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -1630.273804\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -1544.015015\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -1524.561279\n",
      "    epoch          : 453\n",
      "    loss           : -1618.4589505337253\n",
      "    val_loss       : -1606.9901627874\n",
      "    val_log_likelihood: 1713.0429931640624\n",
      "    val_log_marginal: 1666.1608846627175\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -1883.499390\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -1538.850952\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -1619.843994\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -1561.045776\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -1640.208862\n",
      "    epoch          : 454\n",
      "    loss           : -1618.0489501953125\n",
      "    val_loss       : -1618.8114654193632\n",
      "    val_log_likelihood: 1716.845849609375\n",
      "    val_log_marginal: 1676.8619521263986\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -1908.363281\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -1638.058716\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -1516.608887\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -1619.297363\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -1587.145752\n",
      "    epoch          : 455\n",
      "    loss           : -1622.2868193069307\n",
      "    val_loss       : -1620.8558878048323\n",
      "    val_log_likelihood: 1719.8311401367187\n",
      "    val_log_marginal: 1692.7486222524196\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -1898.030640\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -1570.859009\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -1570.294922\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -1594.471924\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -1500.954346\n",
      "    epoch          : 456\n",
      "    loss           : -1610.3030474067914\n",
      "    val_loss       : -1619.9323993573896\n",
      "    val_log_likelihood: 1690.8082397460937\n",
      "    val_log_marginal: 1641.9969359479844\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -1873.219482\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -1529.973145\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -1636.400146\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -1536.707031\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -1501.439697\n",
      "    epoch          : 457\n",
      "    loss           : -1613.3651727355352\n",
      "    val_loss       : -1617.3075427709148\n",
      "    val_log_likelihood: 1709.2839477539062\n",
      "    val_log_marginal: 1675.88545175232\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -1889.277344\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -1662.916992\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -1536.535400\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -1875.389404\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -1570.746094\n",
      "    epoch          : 458\n",
      "    loss           : -1608.1969936861851\n",
      "    val_loss       : -1602.3900886829942\n",
      "    val_log_likelihood: 1706.5549072265626\n",
      "    val_log_marginal: 1669.3191628083587\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -1903.005737\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -1483.177246\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -1639.107056\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -1519.501099\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -1524.498779\n",
      "    epoch          : 459\n",
      "    loss           : -1608.482825553063\n",
      "    val_loss       : -1597.9411820149048\n",
      "    val_log_likelihood: 1671.1388305664063\n",
      "    val_log_marginal: 1638.1756800800563\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -1895.216064\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -1674.612915\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -1494.294434\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -1536.008179\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -1503.453247\n",
      "    epoch          : 460\n",
      "    loss           : -1595.9756765837717\n",
      "    val_loss       : -1603.5156961029395\n",
      "    val_log_likelihood: 1662.7025146484375\n",
      "    val_log_marginal: 1627.7325387343765\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -1873.584351\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -1564.141724\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -1584.067871\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -1519.177856\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -1528.651978\n",
      "    epoch          : 461\n",
      "    loss           : -1608.778755414604\n",
      "    val_loss       : -1600.5315291349775\n",
      "    val_log_likelihood: 1681.1472290039062\n",
      "    val_log_marginal: 1642.6777852237224\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -1551.354980\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -1659.136353\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -1664.875854\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -1501.919800\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -1607.637451\n",
      "    epoch          : 462\n",
      "    loss           : -1605.4705605081992\n",
      "    val_loss       : -1613.136807806138\n",
      "    val_log_likelihood: 1694.1558227539062\n",
      "    val_log_marginal: 1662.628562841192\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -1681.183472\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -1495.770264\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -1592.151245\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -1597.637207\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -1603.572632\n",
      "    epoch          : 463\n",
      "    loss           : -1607.852835173654\n",
      "    val_loss       : -1614.0237986340187\n",
      "    val_log_likelihood: 1692.0739868164062\n",
      "    val_log_marginal: 1658.4963829439134\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -1493.221436\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -1602.541260\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -1488.937622\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -1491.746704\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -1500.921997\n",
      "    epoch          : 464\n",
      "    loss           : -1615.2077213702817\n",
      "    val_loss       : -1619.3538826500996\n",
      "    val_log_likelihood: 1697.1532470703125\n",
      "    val_log_marginal: 1664.8763067707423\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -1883.082764\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -1545.655029\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -1611.291992\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -1902.997559\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -1604.131348\n",
      "    epoch          : 465\n",
      "    loss           : -1620.8884688273515\n",
      "    val_loss       : -1626.9528183182701\n",
      "    val_log_likelihood: 1699.6887817382812\n",
      "    val_log_marginal: 1666.096516130492\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -1931.119873\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -1539.006714\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -1527.963623\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -1881.991455\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -1619.391479\n",
      "    epoch          : 466\n",
      "    loss           : -1607.5974991297958\n",
      "    val_loss       : -1592.0900979390367\n",
      "    val_log_likelihood: 1702.7352661132813\n",
      "    val_log_marginal: 1664.022050969675\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -1878.731689\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -1685.612427\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -1665.554199\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -1477.945312\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -1659.489380\n",
      "    epoch          : 467\n",
      "    loss           : -1615.8803880143873\n",
      "    val_loss       : -1624.400991732627\n",
      "    val_log_likelihood: 1692.2699829101562\n",
      "    val_log_marginal: 1653.5564707323908\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -1510.119507\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -1693.329834\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -1684.782349\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -1903.935059\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -1606.039795\n",
      "    epoch          : 468\n",
      "    loss           : -1616.0493405785892\n",
      "    val_loss       : -1615.3866926736198\n",
      "    val_log_likelihood: 1697.1509521484375\n",
      "    val_log_marginal: 1659.6736610608077\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -1557.061523\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -1541.594238\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -1580.623047\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -1681.104370\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -1577.199341\n",
      "    epoch          : 469\n",
      "    loss           : -1617.6418215307858\n",
      "    val_loss       : -1615.1197956128976\n",
      "    val_log_likelihood: 1678.401171875\n",
      "    val_log_marginal: 1639.4525613993405\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -1903.209595\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -1513.438232\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -1494.939941\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -1495.462402\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -1521.133179\n",
      "    epoch          : 470\n",
      "    loss           : -1614.3853530128404\n",
      "    val_loss       : -1606.80711530447\n",
      "    val_log_likelihood: 1697.0916748046875\n",
      "    val_log_marginal: 1657.7272008333355\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -1904.119141\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -1541.134277\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -1523.399292\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -1610.794678\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -1607.463867\n",
      "    epoch          : 471\n",
      "    loss           : -1612.4830008025217\n",
      "    val_loss       : -1596.9818464563227\n",
      "    val_log_likelihood: 1688.4145751953124\n",
      "    val_log_marginal: 1660.5500428192317\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -1923.115479\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -1559.792969\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -1490.015381\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -1515.369507\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -1622.341309\n",
      "    epoch          : 472\n",
      "    loss           : -1615.574297310102\n",
      "    val_loss       : -1608.7577068792657\n",
      "    val_log_likelihood: 1699.2891357421875\n",
      "    val_log_marginal: 1659.935523214191\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -1892.912964\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -1519.555176\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -1525.126709\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -1543.773438\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -1548.868164\n",
      "    epoch          : 473\n",
      "    loss           : -1618.2455402034343\n",
      "    val_loss       : -1601.7599167878739\n",
      "    val_log_likelihood: 1714.2783081054688\n",
      "    val_log_marginal: 1668.1091461706906\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -1912.249512\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -1550.294800\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -1651.236938\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -1501.749634\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -1503.689941\n",
      "    epoch          : 474\n",
      "    loss           : -1613.3353827448175\n",
      "    val_loss       : -1602.0807893347926\n",
      "    val_log_likelihood: 1719.7601806640625\n",
      "    val_log_marginal: 1683.829582251236\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -1908.963989\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -1722.393433\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -1647.136475\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -1550.467407\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -1551.321899\n",
      "    epoch          : 475\n",
      "    loss           : -1613.395860728651\n",
      "    val_loss       : -1618.158577849809\n",
      "    val_log_likelihood: 1711.9668823242187\n",
      "    val_log_marginal: 1671.3951334141195\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -1898.445923\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -1518.856201\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -1633.959473\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -1681.338257\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -1539.857910\n",
      "    epoch          : 476\n",
      "    loss           : -1616.220676535427\n",
      "    val_loss       : -1613.5425772882068\n",
      "    val_log_likelihood: 1694.7561889648437\n",
      "    val_log_marginal: 1657.391021534428\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -1897.951904\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -1543.653809\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -1534.129517\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -1895.262329\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -1498.298462\n",
      "    epoch          : 477\n",
      "    loss           : -1612.4595149578433\n",
      "    val_loss       : -1597.5623608944938\n",
      "    val_log_likelihood: 1666.4250854492188\n",
      "    val_log_marginal: 1633.7448084048926\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -1886.531250\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -1571.592041\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -1675.960449\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -1662.648926\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -1617.669922\n",
      "    epoch          : 478\n",
      "    loss           : -1616.384251962794\n",
      "    val_loss       : -1609.5508916951717\n",
      "    val_log_likelihood: 1703.6309692382813\n",
      "    val_log_marginal: 1665.2729748580605\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -1887.320923\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -1586.176270\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -1654.762939\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -1477.613281\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -1655.092773\n",
      "    epoch          : 479\n",
      "    loss           : -1623.8346019782643\n",
      "    val_loss       : -1624.2465649785474\n",
      "    val_log_likelihood: 1710.8417236328125\n",
      "    val_log_marginal: 1677.6865590900184\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -1890.958618\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -1680.687134\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -1648.199463\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -1606.526001\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -1597.523438\n",
      "    epoch          : 480\n",
      "    loss           : -1623.3653068920173\n",
      "    val_loss       : -1617.488499834854\n",
      "    val_log_likelihood: 1701.7890625\n",
      "    val_log_marginal: 1668.1078993249685\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -1882.719849\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -1556.046387\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -1611.511963\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -1899.025513\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -1599.722656\n",
      "    epoch          : 481\n",
      "    loss           : -1627.418379679765\n",
      "    val_loss       : -1625.4550152775832\n",
      "    val_log_likelihood: 1708.9327514648437\n",
      "    val_log_marginal: 1678.1715879235417\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -1899.642822\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -1540.129028\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -1629.114014\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -1585.831177\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -1588.822266\n",
      "    epoch          : 482\n",
      "    loss           : -1617.5594724145267\n",
      "    val_loss       : -1602.4692775341682\n",
      "    val_log_likelihood: 1703.1222900390626\n",
      "    val_log_marginal: 1664.190841230005\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -1519.912231\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -1525.048584\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -1594.577393\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -1537.413940\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -1585.835327\n",
      "    epoch          : 483\n",
      "    loss           : -1610.4285876585705\n",
      "    val_loss       : -1612.795239620842\n",
      "    val_log_likelihood: 1707.7157470703125\n",
      "    val_log_marginal: 1665.5094039458781\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -1900.496460\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -1692.456665\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -1601.952026\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -1656.937866\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -1597.624878\n",
      "    epoch          : 484\n",
      "    loss           : -1617.7223746422494\n",
      "    val_loss       : -1613.5315353708343\n",
      "    val_log_likelihood: 1695.637158203125\n",
      "    val_log_marginal: 1659.417401823774\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -1901.355713\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -1476.260498\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -1538.803345\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -1514.931274\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -1605.654907\n",
      "    epoch          : 485\n",
      "    loss           : -1622.7155640857054\n",
      "    val_loss       : -1616.3341973170639\n",
      "    val_log_likelihood: 1698.4385131835938\n",
      "    val_log_marginal: 1651.6646231487393\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -1904.993530\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -1546.386963\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -1566.643311\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -1636.074463\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -1554.300781\n",
      "    epoch          : 486\n",
      "    loss           : -1627.3436581451115\n",
      "    val_loss       : -1628.9154564512894\n",
      "    val_log_likelihood: 1698.3186401367188\n",
      "    val_log_marginal: 1657.8719034533947\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -1898.665161\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -1514.145264\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -1512.288208\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -1591.182129\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -1593.050903\n",
      "    epoch          : 487\n",
      "    loss           : -1609.8873278929455\n",
      "    val_loss       : -1592.2675373697653\n",
      "    val_log_likelihood: 1690.6057861328125\n",
      "    val_log_marginal: 1635.1681542731822\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -1911.194092\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -1538.076782\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -1529.816162\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -1889.756836\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -1518.967896\n",
      "    epoch          : 488\n",
      "    loss           : -1615.704641814279\n",
      "    val_loss       : -1613.2205317003652\n",
      "    val_log_likelihood: 1708.3149169921876\n",
      "    val_log_marginal: 1668.3133765630423\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -1685.243164\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -1589.193359\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -1532.998535\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -1575.557495\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -1579.610229\n",
      "    epoch          : 489\n",
      "    loss           : -1616.5249917814047\n",
      "    val_loss       : -1611.4490373709239\n",
      "    val_log_likelihood: 1684.8612670898438\n",
      "    val_log_marginal: 1657.4829122856258\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -1861.448364\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -1511.027100\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -1570.167236\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -1679.325684\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -1498.690918\n",
      "    epoch          : 490\n",
      "    loss           : -1618.9315523959622\n",
      "    val_loss       : -1611.4894235237502\n",
      "    val_log_likelihood: 1710.1180541992187\n",
      "    val_log_marginal: 1675.1401792887598\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -1900.454346\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -1571.193848\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -1567.377197\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -1644.834473\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -1572.131836\n",
      "    epoch          : 491\n",
      "    loss           : -1610.4891006922958\n",
      "    val_loss       : -1590.629066215083\n",
      "    val_log_likelihood: 1669.0999877929687\n",
      "    val_log_marginal: 1630.6342025861145\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -1867.420654\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -1510.175903\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -1625.314453\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -1662.236572\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -1653.393921\n",
      "    epoch          : 492\n",
      "    loss           : -1609.0007420908107\n",
      "    val_loss       : -1618.2591208796948\n",
      "    val_log_likelihood: 1703.4655639648438\n",
      "    val_log_marginal: 1659.1673558704556\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -1521.696411\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -1554.150024\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -1509.654907\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -1871.691528\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -1595.438599\n",
      "    epoch          : 493\n",
      "    loss           : -1617.468046584932\n",
      "    val_loss       : -1619.9853605564683\n",
      "    val_log_likelihood: 1691.621142578125\n",
      "    val_log_marginal: 1652.8441150341182\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -1859.053223\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -1540.312378\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -1521.867310\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -1506.884155\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -1524.062256\n",
      "    epoch          : 494\n",
      "    loss           : -1618.122773727568\n",
      "    val_loss       : -1603.6139254458249\n",
      "    val_log_likelihood: 1667.8700439453125\n",
      "    val_log_marginal: 1633.4772079531103\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -1527.250244\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -1692.367188\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -1503.103394\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -1534.842529\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -1517.653076\n",
      "    epoch          : 495\n",
      "    loss           : -1612.364394386216\n",
      "    val_loss       : -1600.422604236845\n",
      "    val_log_likelihood: 1663.7596923828125\n",
      "    val_log_marginal: 1627.3773295011372\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -1903.497437\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -1480.897583\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -1549.691040\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -1644.542969\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -1530.996094\n",
      "    epoch          : 496\n",
      "    loss           : -1613.8009093633973\n",
      "    val_loss       : -1612.017855171673\n",
      "    val_log_likelihood: 1683.81767578125\n",
      "    val_log_marginal: 1655.3882862962782\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -1643.168335\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -1695.903931\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -1483.860229\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -1521.126953\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -1617.335327\n",
      "    epoch          : 497\n",
      "    loss           : -1613.5260517384747\n",
      "    val_loss       : -1619.0329285993241\n",
      "    val_log_likelihood: 1688.8792236328125\n",
      "    val_log_marginal: 1645.1818691030144\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -1557.224609\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -1686.535767\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -1615.319580\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -1610.896851\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -1623.029663\n",
      "    epoch          : 498\n",
      "    loss           : -1625.4937526589572\n",
      "    val_loss       : -1619.2288213806228\n",
      "    val_log_likelihood: 1708.221435546875\n",
      "    val_log_marginal: 1668.9604541443289\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -1707.116455\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -1575.898560\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -1631.474121\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -1590.337280\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -1591.714844\n",
      "    epoch          : 499\n",
      "    loss           : -1628.643908812268\n",
      "    val_loss       : -1580.050851053372\n",
      "    val_log_likelihood: 1707.4375244140624\n",
      "    val_log_marginal: 1677.4487643431871\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -1926.829590\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -1595.247314\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -1668.161621\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -1550.426514\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -1628.595825\n",
      "    epoch          : 500\n",
      "    loss           : -1611.3940816444926\n",
      "    val_loss       : -1621.371936595533\n",
      "    val_log_likelihood: 1702.1083984375\n",
      "    val_log_marginal: 1667.1210664648563\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch500.pth ...\n",
      "Train Epoch: 501 [512/54000 (1%)] Loss: -1703.714966\n",
      "Train Epoch: 501 [11776/54000 (22%)] Loss: -1648.248535\n",
      "Train Epoch: 501 [23040/54000 (43%)] Loss: -1596.395020\n",
      "Train Epoch: 501 [34304/54000 (64%)] Loss: -1502.131226\n",
      "Train Epoch: 501 [45568/54000 (84%)] Loss: -1528.260986\n",
      "    epoch          : 501\n",
      "    loss           : -1615.547253297107\n",
      "    val_loss       : -1618.3652787989006\n",
      "    val_log_likelihood: 1712.15302734375\n",
      "    val_log_marginal: 1666.631822544709\n",
      "Train Epoch: 502 [512/54000 (1%)] Loss: -1903.226562\n",
      "Train Epoch: 502 [11776/54000 (22%)] Loss: -1483.370850\n",
      "Train Epoch: 502 [23040/54000 (43%)] Loss: -1486.282837\n",
      "Train Epoch: 502 [34304/54000 (64%)] Loss: -1497.609497\n",
      "Train Epoch: 502 [45568/54000 (84%)] Loss: -1605.052979\n",
      "    epoch          : 502\n",
      "    loss           : -1623.84917669013\n",
      "    val_loss       : -1625.3823431191036\n",
      "    val_log_likelihood: 1706.527880859375\n",
      "    val_log_marginal: 1680.5222611602396\n",
      "Train Epoch: 503 [512/54000 (1%)] Loss: -1927.532715\n",
      "Train Epoch: 503 [11776/54000 (22%)] Loss: -1554.989624\n",
      "Train Epoch: 503 [23040/54000 (43%)] Loss: -1548.075439\n",
      "Train Epoch: 503 [34304/54000 (64%)] Loss: -1472.885498\n",
      "Train Epoch: 503 [45568/54000 (84%)] Loss: -1657.688965\n",
      "    epoch          : 503\n",
      "    loss           : -1624.905814897896\n",
      "    val_loss       : -1628.792947250046\n",
      "    val_log_likelihood: 1697.4982543945312\n",
      "    val_log_marginal: 1657.9447427615523\n",
      "Train Epoch: 504 [512/54000 (1%)] Loss: -1891.626099\n",
      "Train Epoch: 504 [11776/54000 (22%)] Loss: -1668.914917\n",
      "Train Epoch: 504 [23040/54000 (43%)] Loss: -1670.652344\n",
      "Train Epoch: 504 [34304/54000 (64%)] Loss: -1510.846680\n",
      "Train Epoch: 504 [45568/54000 (84%)] Loss: -1523.123047\n",
      "    epoch          : 504\n",
      "    loss           : -1623.6128207669399\n",
      "    val_loss       : -1632.4994607424364\n",
      "    val_log_likelihood: 1699.4616821289062\n",
      "    val_log_marginal: 1666.0762499697507\n",
      "Train Epoch: 505 [512/54000 (1%)] Loss: -1926.165771\n",
      "Train Epoch: 505 [11776/54000 (22%)] Loss: -1567.428345\n",
      "Train Epoch: 505 [23040/54000 (43%)] Loss: -1630.705444\n",
      "Train Epoch: 505 [34304/54000 (64%)] Loss: -1899.797607\n",
      "Train Epoch: 505 [45568/54000 (84%)] Loss: -1592.216553\n",
      "    epoch          : 505\n",
      "    loss           : -1626.4632894685953\n",
      "    val_loss       : -1626.008790054731\n",
      "    val_log_likelihood: 1691.8419799804688\n",
      "    val_log_marginal: 1653.0696581706404\n",
      "Train Epoch: 506 [512/54000 (1%)] Loss: -1735.531738\n",
      "Train Epoch: 506 [11776/54000 (22%)] Loss: -1590.318726\n",
      "Train Epoch: 506 [23040/54000 (43%)] Loss: -1547.086670\n",
      "Train Epoch: 506 [34304/54000 (64%)] Loss: -1668.291138\n",
      "Train Epoch: 506 [45568/54000 (84%)] Loss: -1519.031494\n",
      "    epoch          : 506\n",
      "    loss           : -1619.2788194713025\n",
      "    val_loss       : -1617.5332180030643\n",
      "    val_log_likelihood: 1694.7488037109374\n",
      "    val_log_marginal: 1662.3636512365192\n",
      "Train Epoch: 507 [512/54000 (1%)] Loss: -1564.984619\n",
      "Train Epoch: 507 [11776/54000 (22%)] Loss: -1689.020508\n",
      "Train Epoch: 507 [23040/54000 (43%)] Loss: -1594.334839\n",
      "Train Epoch: 507 [34304/54000 (64%)] Loss: -1897.586426\n",
      "Train Epoch: 507 [45568/54000 (84%)] Loss: -1573.750244\n",
      "    epoch          : 507\n",
      "    loss           : -1616.8765119798113\n",
      "    val_loss       : -1624.7367235006764\n",
      "    val_log_likelihood: 1690.3414672851563\n",
      "    val_log_marginal: 1658.6691542141139\n",
      "Train Epoch: 508 [512/54000 (1%)] Loss: -1901.541992\n",
      "Train Epoch: 508 [11776/54000 (22%)] Loss: -1681.168823\n",
      "Train Epoch: 508 [23040/54000 (43%)] Loss: -1682.968628\n",
      "Train Epoch: 508 [34304/54000 (64%)] Loss: -1635.690186\n",
      "Train Epoch: 508 [45568/54000 (84%)] Loss: -1644.205566\n",
      "    epoch          : 508\n",
      "    loss           : -1628.7467609065593\n",
      "    val_loss       : -1631.8072204736993\n",
      "    val_log_likelihood: 1717.0924194335937\n",
      "    val_log_marginal: 1666.3677629280924\n",
      "Train Epoch: 509 [512/54000 (1%)] Loss: -1940.891357\n",
      "Train Epoch: 509 [11776/54000 (22%)] Loss: -1537.384155\n",
      "Train Epoch: 509 [23040/54000 (43%)] Loss: -1669.575439\n",
      "Train Epoch: 509 [34304/54000 (64%)] Loss: -1579.648804\n",
      "Train Epoch: 509 [45568/54000 (84%)] Loss: -1558.173828\n",
      "    epoch          : 509\n",
      "    loss           : -1631.9768380646658\n",
      "    val_loss       : -1627.7480568751694\n",
      "    val_log_likelihood: 1705.0790161132813\n",
      "    val_log_marginal: 1669.5528172086924\n",
      "Train Epoch: 510 [512/54000 (1%)] Loss: -1636.569824\n",
      "Train Epoch: 510 [11776/54000 (22%)] Loss: -1565.396240\n",
      "Train Epoch: 510 [23040/54000 (43%)] Loss: -1463.255859\n",
      "Train Epoch: 510 [34304/54000 (64%)] Loss: -1614.706177\n",
      "Train Epoch: 510 [45568/54000 (84%)] Loss: -1581.789307\n",
      "    epoch          : 510\n",
      "    loss           : -1614.5130639406714\n",
      "    val_loss       : -1603.6520157488994\n",
      "    val_log_likelihood: 1688.1712280273437\n",
      "    val_log_marginal: 1642.4202282235026\n",
      "Train Epoch: 511 [512/54000 (1%)] Loss: -1881.605591\n",
      "Train Epoch: 511 [11776/54000 (22%)] Loss: -1444.956543\n",
      "Train Epoch: 511 [23040/54000 (43%)] Loss: -1668.465820\n",
      "Train Epoch: 511 [34304/54000 (64%)] Loss: -1601.447876\n",
      "Train Epoch: 511 [45568/54000 (84%)] Loss: -1611.861084\n",
      "    epoch          : 511\n",
      "    loss           : -1611.8561878770886\n",
      "    val_loss       : -1620.619115264993\n",
      "    val_log_likelihood: 1692.755419921875\n",
      "    val_log_marginal: 1654.9756313646872\n",
      "Train Epoch: 512 [512/54000 (1%)] Loss: -1937.420776\n",
      "Train Epoch: 512 [11776/54000 (22%)] Loss: -1685.773438\n",
      "Train Epoch: 512 [23040/54000 (43%)] Loss: -1631.452881\n",
      "Train Epoch: 512 [34304/54000 (64%)] Loss: -1635.294922\n",
      "Train Epoch: 512 [45568/54000 (84%)] Loss: -1610.417725\n",
      "    epoch          : 512\n",
      "    loss           : -1611.5597963238706\n",
      "    val_loss       : -1612.8895189966074\n",
      "    val_log_likelihood: 1709.8598022460938\n",
      "    val_log_marginal: 1679.7781120002269\n",
      "Train Epoch: 513 [512/54000 (1%)] Loss: -1905.481445\n",
      "Train Epoch: 513 [11776/54000 (22%)] Loss: -1547.368164\n",
      "Train Epoch: 513 [23040/54000 (43%)] Loss: -1507.006714\n",
      "Train Epoch: 513 [34304/54000 (64%)] Loss: -1610.553101\n",
      "Train Epoch: 513 [45568/54000 (84%)] Loss: -1603.877930\n",
      "    epoch          : 513\n",
      "    loss           : -1621.0903646639078\n",
      "    val_loss       : -1623.7461389253847\n",
      "    val_log_likelihood: 1717.5428100585937\n",
      "    val_log_marginal: 1679.2125189222395\n",
      "Train Epoch: 514 [512/54000 (1%)] Loss: -1907.186035\n",
      "Train Epoch: 514 [11776/54000 (22%)] Loss: -1718.984131\n",
      "Train Epoch: 514 [23040/54000 (43%)] Loss: -1488.874512\n",
      "Train Epoch: 514 [34304/54000 (64%)] Loss: -1679.192505\n",
      "Train Epoch: 514 [45568/54000 (84%)] Loss: -1634.315918\n",
      "    epoch          : 514\n",
      "    loss           : -1618.3941734993812\n",
      "    val_loss       : -1618.5595824270508\n",
      "    val_log_likelihood: 1726.7236206054688\n",
      "    val_log_marginal: 1673.0275420244784\n",
      "Train Epoch: 515 [512/54000 (1%)] Loss: -1515.795654\n",
      "Train Epoch: 515 [11776/54000 (22%)] Loss: -1559.294434\n",
      "Train Epoch: 515 [23040/54000 (43%)] Loss: -1566.815430\n",
      "Train Epoch: 515 [34304/54000 (64%)] Loss: -1576.214355\n",
      "Train Epoch: 515 [45568/54000 (84%)] Loss: -1464.785767\n",
      "    epoch          : 515\n",
      "    loss           : -1590.6705515644337\n",
      "    val_loss       : -1583.2318014641291\n",
      "    val_log_likelihood: 1661.9853271484376\n",
      "    val_log_marginal: 1619.8957860440016\n",
      "Train Epoch: 516 [512/54000 (1%)] Loss: -1873.250610\n",
      "Train Epoch: 516 [11776/54000 (22%)] Loss: -1599.548828\n",
      "Train Epoch: 516 [23040/54000 (43%)] Loss: -1599.923584\n",
      "Train Epoch: 516 [34304/54000 (64%)] Loss: -1508.675293\n",
      "Train Epoch: 516 [45568/54000 (84%)] Loss: -1560.227783\n",
      "    epoch          : 516\n",
      "    loss           : -1602.3203741394648\n",
      "    val_loss       : -1602.8317412198521\n",
      "    val_log_likelihood: 1698.4565307617188\n",
      "    val_log_marginal: 1664.412246436253\n",
      "Train Epoch: 517 [512/54000 (1%)] Loss: -1710.001587\n",
      "Train Epoch: 517 [11776/54000 (22%)] Loss: -1615.670166\n",
      "Train Epoch: 517 [23040/54000 (43%)] Loss: -1620.795898\n",
      "Train Epoch: 517 [34304/54000 (64%)] Loss: -1578.012573\n",
      "Train Epoch: 517 [45568/54000 (84%)] Loss: -1591.572266\n",
      "    epoch          : 517\n",
      "    loss           : -1612.6874939569152\n",
      "    val_loss       : -1617.6780310945585\n",
      "    val_log_likelihood: 1677.9611694335938\n",
      "    val_log_marginal: 1643.7961040899158\n",
      "Train Epoch: 518 [512/54000 (1%)] Loss: -1862.562134\n",
      "Train Epoch: 518 [11776/54000 (22%)] Loss: -1508.255981\n",
      "Train Epoch: 518 [23040/54000 (43%)] Loss: -1525.589355\n",
      "Train Epoch: 518 [34304/54000 (64%)] Loss: -1666.741943\n",
      "Train Epoch: 518 [45568/54000 (84%)] Loss: -1548.407593\n",
      "    epoch          : 518\n",
      "    loss           : -1618.4778581373762\n",
      "    val_loss       : -1607.7310646661558\n",
      "    val_log_likelihood: 1700.7280395507812\n",
      "    val_log_marginal: 1664.7120788827538\n",
      "Train Epoch: 519 [512/54000 (1%)] Loss: -1687.314941\n",
      "Train Epoch: 519 [11776/54000 (22%)] Loss: -1560.793335\n",
      "Train Epoch: 519 [23040/54000 (43%)] Loss: -1518.389282\n",
      "Train Epoch: 519 [34304/54000 (64%)] Loss: -1654.261963\n",
      "Train Epoch: 519 [45568/54000 (84%)] Loss: -1526.433472\n",
      "    epoch          : 519\n",
      "    loss           : -1623.6731621770575\n",
      "    val_loss       : -1616.4875177320093\n",
      "    val_log_likelihood: 1702.1273559570313\n",
      "    val_log_marginal: 1660.7067145995795\n",
      "Train Epoch: 520 [512/54000 (1%)] Loss: -1876.961792\n",
      "Train Epoch: 520 [11776/54000 (22%)] Loss: -1523.536133\n",
      "Train Epoch: 520 [23040/54000 (43%)] Loss: -1572.721191\n",
      "Train Epoch: 520 [34304/54000 (64%)] Loss: -1686.232910\n",
      "Train Epoch: 520 [45568/54000 (84%)] Loss: -1678.063721\n",
      "    epoch          : 520\n",
      "    loss           : -1619.7996258121907\n",
      "    val_loss       : -1617.0340745222754\n",
      "    val_log_likelihood: 1701.7515991210937\n",
      "    val_log_marginal: 1660.6451726760715\n",
      "Train Epoch: 521 [512/54000 (1%)] Loss: -1911.771240\n",
      "Train Epoch: 521 [11776/54000 (22%)] Loss: -1625.609375\n",
      "Train Epoch: 521 [23040/54000 (43%)] Loss: -1552.367676\n",
      "Train Epoch: 521 [34304/54000 (64%)] Loss: -1599.363525\n",
      "Train Epoch: 521 [45568/54000 (84%)] Loss: -1516.325195\n",
      "    epoch          : 521\n",
      "    loss           : -1624.4840849319307\n",
      "    val_loss       : -1611.6089345629327\n",
      "    val_log_likelihood: 1701.88525390625\n",
      "    val_log_marginal: 1671.6179537449032\n",
      "Train Epoch: 522 [512/54000 (1%)] Loss: -1911.908447\n",
      "Train Epoch: 522 [11776/54000 (22%)] Loss: -1517.955566\n",
      "Train Epoch: 522 [23040/54000 (43%)] Loss: -1550.472656\n",
      "Train Epoch: 522 [34304/54000 (64%)] Loss: -1657.638794\n",
      "Train Epoch: 522 [45568/54000 (84%)] Loss: -1599.149414\n",
      "    epoch          : 522\n",
      "    loss           : -1629.1148923364017\n",
      "    val_loss       : -1626.5225921786391\n",
      "    val_log_likelihood: 1693.4946533203124\n",
      "    val_log_marginal: 1651.000902793929\n",
      "Train Epoch: 523 [512/54000 (1%)] Loss: -1885.547363\n",
      "Train Epoch: 523 [11776/54000 (22%)] Loss: -1575.549316\n",
      "Train Epoch: 523 [23040/54000 (43%)] Loss: -1646.956055\n",
      "Train Epoch: 523 [34304/54000 (64%)] Loss: -1619.781616\n",
      "Train Epoch: 523 [45568/54000 (84%)] Loss: -1526.498047\n",
      "    epoch          : 523\n",
      "    loss           : -1626.6438338780167\n",
      "    val_loss       : -1619.663438623771\n",
      "    val_log_likelihood: 1705.8424072265625\n",
      "    val_log_marginal: 1665.244187553972\n",
      "Train Epoch: 524 [512/54000 (1%)] Loss: -1941.809082\n",
      "Train Epoch: 524 [11776/54000 (22%)] Loss: -1576.524658\n",
      "Train Epoch: 524 [23040/54000 (43%)] Loss: -1535.205933\n",
      "Train Epoch: 524 [34304/54000 (64%)] Loss: -1591.013672\n",
      "Train Epoch: 524 [45568/54000 (84%)] Loss: -1540.566162\n",
      "    epoch          : 524\n",
      "    loss           : -1620.2283210376702\n",
      "    val_loss       : -1598.3309272316285\n",
      "    val_log_likelihood: 1702.1174682617188\n",
      "    val_log_marginal: 1671.3335511412472\n",
      "Train Epoch: 525 [512/54000 (1%)] Loss: -1884.399658\n",
      "Train Epoch: 525 [11776/54000 (22%)] Loss: -1522.897949\n",
      "Train Epoch: 525 [23040/54000 (43%)] Loss: -1662.276123\n",
      "Train Epoch: 525 [34304/54000 (64%)] Loss: -1576.461304\n",
      "Train Epoch: 525 [45568/54000 (84%)] Loss: -1605.996582\n",
      "    epoch          : 525\n",
      "    loss           : -1624.2345454149906\n",
      "    val_loss       : -1606.4590711208061\n",
      "    val_log_likelihood: 1699.2569580078125\n",
      "    val_log_marginal: 1656.9029375504701\n",
      "Train Epoch: 526 [512/54000 (1%)] Loss: -1885.452515\n",
      "Train Epoch: 526 [11776/54000 (22%)] Loss: -1740.377563\n",
      "Train Epoch: 526 [23040/54000 (43%)] Loss: -1682.430420\n",
      "Train Epoch: 526 [34304/54000 (64%)] Loss: -1653.662964\n",
      "Train Epoch: 526 [45568/54000 (84%)] Loss: -1680.773438\n",
      "    epoch          : 526\n",
      "    loss           : -1624.996692015393\n",
      "    val_loss       : -1608.9157726192848\n",
      "    val_log_likelihood: 1707.3084838867187\n",
      "    val_log_marginal: 1663.617815773189\n",
      "Train Epoch: 527 [512/54000 (1%)] Loss: -1735.942993\n",
      "Train Epoch: 527 [11776/54000 (22%)] Loss: -1564.270630\n",
      "Train Epoch: 527 [23040/54000 (43%)] Loss: -1677.444580\n",
      "Train Epoch: 527 [34304/54000 (64%)] Loss: -1641.947510\n",
      "Train Epoch: 527 [45568/54000 (84%)] Loss: -1627.286987\n",
      "    epoch          : 527\n",
      "    loss           : -1628.4547505898051\n",
      "    val_loss       : -1633.9145296366885\n",
      "    val_log_likelihood: 1703.6602416992187\n",
      "    val_log_marginal: 1667.0336810514332\n",
      "Train Epoch: 528 [512/54000 (1%)] Loss: -1898.124512\n",
      "Train Epoch: 528 [11776/54000 (22%)] Loss: -1761.275269\n",
      "Train Epoch: 528 [23040/54000 (43%)] Loss: -1494.504761\n",
      "Train Epoch: 528 [34304/54000 (64%)] Loss: -1534.091431\n",
      "Train Epoch: 528 [45568/54000 (84%)] Loss: -1537.301270\n",
      "    epoch          : 528\n",
      "    loss           : -1627.6097931814666\n",
      "    val_loss       : -1618.2424980918877\n",
      "    val_log_likelihood: 1692.3646484375\n",
      "    val_log_marginal: 1647.5654572945089\n",
      "Train Epoch: 529 [512/54000 (1%)] Loss: -1871.353516\n",
      "Train Epoch: 529 [11776/54000 (22%)] Loss: -1724.837891\n",
      "Train Epoch: 529 [23040/54000 (43%)] Loss: -1519.313110\n",
      "Train Epoch: 529 [34304/54000 (64%)] Loss: -1708.075195\n",
      "Train Epoch: 529 [45568/54000 (84%)] Loss: -1520.858398\n",
      "    epoch          : 529\n",
      "    loss           : -1626.9937671623607\n",
      "    val_loss       : -1612.9112853256986\n",
      "    val_log_likelihood: 1694.3325317382812\n",
      "    val_log_marginal: 1648.3556058403105\n",
      "Train Epoch: 530 [512/54000 (1%)] Loss: -1754.401855\n",
      "Train Epoch: 530 [11776/54000 (22%)] Loss: -1527.711670\n",
      "Train Epoch: 530 [23040/54000 (43%)] Loss: -1675.517090\n",
      "Train Epoch: 530 [34304/54000 (64%)] Loss: -1856.231567\n",
      "Train Epoch: 530 [45568/54000 (84%)] Loss: -1501.028442\n",
      "    epoch          : 530\n",
      "    loss           : -1613.0213901028774\n",
      "    val_loss       : -1622.0560506941752\n",
      "    val_log_likelihood: 1700.6161010742187\n",
      "    val_log_marginal: 1656.6726661726832\n",
      "Train Epoch: 531 [512/54000 (1%)] Loss: -1865.146729\n",
      "Train Epoch: 531 [11776/54000 (22%)] Loss: -1494.161133\n",
      "Train Epoch: 531 [23040/54000 (43%)] Loss: -1526.954346\n",
      "Train Epoch: 531 [34304/54000 (64%)] Loss: -1577.633789\n",
      "Train Epoch: 531 [45568/54000 (84%)] Loss: -1533.215820\n",
      "    epoch          : 531\n",
      "    loss           : -1625.9116924021503\n",
      "    val_loss       : -1624.4102081873455\n",
      "    val_log_likelihood: 1704.3917846679688\n",
      "    val_log_marginal: 1659.351782683283\n",
      "Train Epoch: 532 [512/54000 (1%)] Loss: -1900.501221\n",
      "Train Epoch: 532 [11776/54000 (22%)] Loss: -1531.705444\n",
      "Train Epoch: 532 [23040/54000 (43%)] Loss: -1516.999023\n",
      "Train Epoch: 532 [34304/54000 (64%)] Loss: -1685.356201\n",
      "Train Epoch: 532 [45568/54000 (84%)] Loss: -1527.049927\n",
      "    epoch          : 532\n",
      "    loss           : -1618.4499378770886\n",
      "    val_loss       : -1608.2695885785856\n",
      "    val_log_likelihood: 1715.7753540039062\n",
      "    val_log_marginal: 1680.1367650636216\n",
      "Train Epoch: 533 [512/54000 (1%)] Loss: -1912.386475\n",
      "Train Epoch: 533 [11776/54000 (22%)] Loss: -1663.965210\n",
      "Train Epoch: 533 [23040/54000 (43%)] Loss: -1555.950195\n",
      "Train Epoch: 533 [34304/54000 (64%)] Loss: -1690.537720\n",
      "Train Epoch: 533 [45568/54000 (84%)] Loss: -1516.568237\n",
      "    epoch          : 533\n",
      "    loss           : -1628.81190052599\n",
      "    val_loss       : -1618.7116350346246\n",
      "    val_log_likelihood: 1701.1239868164062\n",
      "    val_log_marginal: 1666.6138057198375\n",
      "Train Epoch: 534 [512/54000 (1%)] Loss: -1709.520020\n",
      "Train Epoch: 534 [11776/54000 (22%)] Loss: -1715.725464\n",
      "Train Epoch: 534 [23040/54000 (43%)] Loss: -1677.659302\n",
      "Train Epoch: 534 [34304/54000 (64%)] Loss: -1629.911865\n",
      "Train Epoch: 534 [45568/54000 (84%)] Loss: -1534.561890\n",
      "    epoch          : 534\n",
      "    loss           : -1627.147839234607\n",
      "    val_loss       : -1645.909978158772\n",
      "    val_log_likelihood: 1725.6242919921874\n",
      "    val_log_marginal: 1696.9463237125426\n",
      "Train Epoch: 535 [512/54000 (1%)] Loss: -1914.848999\n",
      "Train Epoch: 535 [11776/54000 (22%)] Loss: -1667.037720\n",
      "Train Epoch: 535 [23040/54000 (43%)] Loss: -1487.457397\n",
      "Train Epoch: 535 [34304/54000 (64%)] Loss: -1525.114746\n",
      "Train Epoch: 535 [45568/54000 (84%)] Loss: -1664.651611\n",
      "    epoch          : 535\n",
      "    loss           : -1632.3326416015625\n",
      "    val_loss       : -1618.124289362319\n",
      "    val_log_likelihood: 1720.44130859375\n",
      "    val_log_marginal: 1690.692237554118\n",
      "Train Epoch: 536 [512/54000 (1%)] Loss: -1932.173462\n",
      "Train Epoch: 536 [11776/54000 (22%)] Loss: -1513.440430\n",
      "Train Epoch: 536 [23040/54000 (43%)] Loss: -1505.676758\n",
      "Train Epoch: 536 [34304/54000 (64%)] Loss: -1485.514282\n",
      "Train Epoch: 536 [45568/54000 (84%)] Loss: -1479.636719\n",
      "    epoch          : 536\n",
      "    loss           : -1616.2403588625464\n",
      "    val_loss       : -1618.3450928226114\n",
      "    val_log_likelihood: 1715.0949584960938\n",
      "    val_log_marginal: 1685.9254799678922\n",
      "Train Epoch: 537 [512/54000 (1%)] Loss: -1902.545410\n",
      "Train Epoch: 537 [11776/54000 (22%)] Loss: -1690.661987\n",
      "Train Epoch: 537 [23040/54000 (43%)] Loss: -1520.389160\n",
      "Train Epoch: 537 [34304/54000 (64%)] Loss: -1588.688965\n",
      "Train Epoch: 537 [45568/54000 (84%)] Loss: -1620.522461\n",
      "    epoch          : 537\n",
      "    loss           : -1616.4286770962253\n",
      "    val_loss       : -1617.72794166822\n",
      "    val_log_likelihood: 1704.2536865234374\n",
      "    val_log_marginal: 1663.6707281220704\n",
      "Train Epoch: 538 [512/54000 (1%)] Loss: -1669.616089\n",
      "Train Epoch: 538 [11776/54000 (22%)] Loss: -1534.400879\n",
      "Train Epoch: 538 [23040/54000 (43%)] Loss: -1556.046143\n",
      "Train Epoch: 538 [34304/54000 (64%)] Loss: -1619.353516\n",
      "Train Epoch: 538 [45568/54000 (84%)] Loss: -1573.688965\n",
      "    epoch          : 538\n",
      "    loss           : -1624.9250379505725\n",
      "    val_loss       : -1640.5497286100872\n",
      "    val_log_likelihood: 1703.9337890625\n",
      "    val_log_marginal: 1668.4507922276855\n",
      "Train Epoch: 539 [512/54000 (1%)] Loss: -1929.250977\n",
      "Train Epoch: 539 [11776/54000 (22%)] Loss: -1563.553955\n",
      "Train Epoch: 539 [23040/54000 (43%)] Loss: -1492.347778\n",
      "Train Epoch: 539 [34304/54000 (64%)] Loss: -1568.613037\n",
      "Train Epoch: 539 [45568/54000 (84%)] Loss: -1662.452881\n",
      "    epoch          : 539\n",
      "    loss           : -1613.6345456567142\n",
      "    val_loss       : -1608.5753525786101\n",
      "    val_log_likelihood: 1726.1524169921875\n",
      "    val_log_marginal: 1695.5078577477484\n",
      "Train Epoch: 540 [512/54000 (1%)] Loss: -1890.768921\n",
      "Train Epoch: 540 [11776/54000 (22%)] Loss: -1505.761719\n",
      "Train Epoch: 540 [23040/54000 (43%)] Loss: -1588.572510\n",
      "Train Epoch: 540 [34304/54000 (64%)] Loss: -1439.462646\n",
      "Train Epoch: 540 [45568/54000 (84%)] Loss: -1620.004761\n",
      "    epoch          : 540\n",
      "    loss           : -1607.7512787167389\n",
      "    val_loss       : -1609.724055717606\n",
      "    val_log_likelihood: 1688.96357421875\n",
      "    val_log_marginal: 1656.6839276667683\n",
      "Train Epoch: 541 [512/54000 (1%)] Loss: -1900.119019\n",
      "Train Epoch: 541 [11776/54000 (22%)] Loss: -1522.072266\n",
      "Train Epoch: 541 [23040/54000 (43%)] Loss: -1566.034058\n",
      "Train Epoch: 541 [34304/54000 (64%)] Loss: -1672.367920\n",
      "Train Epoch: 541 [45568/54000 (84%)] Loss: -1522.798096\n",
      "    epoch          : 541\n",
      "    loss           : -1621.0188967260983\n",
      "    val_loss       : -1620.92926712865\n",
      "    val_log_likelihood: 1725.8852172851562\n",
      "    val_log_marginal: 1690.6755249064415\n",
      "Train Epoch: 542 [512/54000 (1%)] Loss: -1894.478516\n",
      "Train Epoch: 542 [11776/54000 (22%)] Loss: -1675.893677\n",
      "Train Epoch: 542 [23040/54000 (43%)] Loss: -1456.547607\n",
      "Train Epoch: 542 [34304/54000 (64%)] Loss: -1643.440674\n",
      "Train Epoch: 542 [45568/54000 (84%)] Loss: -1462.012451\n",
      "    epoch          : 542\n",
      "    loss           : -1607.9347648998298\n",
      "    val_loss       : -1614.5046869959683\n",
      "    val_log_likelihood: 1685.1559692382812\n",
      "    val_log_marginal: 1646.9022278875113\n",
      "Train Epoch: 543 [512/54000 (1%)] Loss: -1868.847290\n",
      "Train Epoch: 543 [11776/54000 (22%)] Loss: -1468.834473\n",
      "Train Epoch: 543 [23040/54000 (43%)] Loss: -1620.067139\n",
      "Train Epoch: 543 [34304/54000 (64%)] Loss: -1579.935181\n",
      "Train Epoch: 543 [45568/54000 (84%)] Loss: -1551.010986\n",
      "    epoch          : 543\n",
      "    loss           : -1609.1419508528002\n",
      "    val_loss       : -1574.9340700255707\n",
      "    val_log_likelihood: 1706.5864990234375\n",
      "    val_log_marginal: 1668.642536278814\n",
      "Train Epoch: 544 [512/54000 (1%)] Loss: -1693.637939\n",
      "Train Epoch: 544 [11776/54000 (22%)] Loss: -1600.084717\n",
      "Train Epoch: 544 [23040/54000 (43%)] Loss: -1591.477783\n",
      "Train Epoch: 544 [34304/54000 (64%)] Loss: -1550.307007\n",
      "Train Epoch: 544 [45568/54000 (84%)] Loss: -1654.823120\n",
      "    epoch          : 544\n",
      "    loss           : -1607.99647083849\n",
      "    val_loss       : -1600.1024588525295\n",
      "    val_log_likelihood: 1705.4305908203125\n",
      "    val_log_marginal: 1664.8661345101893\n",
      "Train Epoch: 545 [512/54000 (1%)] Loss: -1586.695190\n",
      "Train Epoch: 545 [11776/54000 (22%)] Loss: -1666.998779\n",
      "Train Epoch: 545 [23040/54000 (43%)] Loss: -1531.220215\n",
      "Train Epoch: 545 [34304/54000 (64%)] Loss: -1615.852295\n",
      "Train Epoch: 545 [45568/54000 (84%)] Loss: -1620.096436\n",
      "    epoch          : 545\n",
      "    loss           : -1617.5483555557705\n",
      "    val_loss       : -1618.366811885126\n",
      "    val_log_likelihood: 1723.395458984375\n",
      "    val_log_marginal: 1669.962303091213\n",
      "Train Epoch: 546 [512/54000 (1%)] Loss: -1869.584473\n",
      "Train Epoch: 546 [11776/54000 (22%)] Loss: -1553.692993\n",
      "Train Epoch: 546 [23040/54000 (43%)] Loss: -1640.284790\n",
      "Train Epoch: 546 [34304/54000 (64%)] Loss: -1515.455811\n",
      "Train Epoch: 546 [45568/54000 (84%)] Loss: -1642.979614\n",
      "    epoch          : 546\n",
      "    loss           : -1618.2624789700649\n",
      "    val_loss       : -1627.751416550856\n",
      "    val_log_likelihood: 1722.4648681640624\n",
      "    val_log_marginal: 1686.495574798435\n",
      "Train Epoch: 547 [512/54000 (1%)] Loss: -1908.797852\n",
      "Train Epoch: 547 [11776/54000 (22%)] Loss: -1527.740234\n",
      "Train Epoch: 547 [23040/54000 (43%)] Loss: -1582.545654\n",
      "Train Epoch: 547 [34304/54000 (64%)] Loss: -1527.557007\n",
      "Train Epoch: 547 [45568/54000 (84%)] Loss: -1479.203613\n",
      "    epoch          : 547\n",
      "    loss           : -1623.062577351485\n",
      "    val_loss       : -1631.1497552649118\n",
      "    val_log_likelihood: 1729.7768432617188\n",
      "    val_log_marginal: 1695.177977213636\n",
      "Train Epoch: 548 [512/54000 (1%)] Loss: -1848.069336\n",
      "Train Epoch: 548 [11776/54000 (22%)] Loss: -1510.378418\n",
      "Train Epoch: 548 [23040/54000 (43%)] Loss: -1545.369385\n",
      "Train Epoch: 548 [34304/54000 (64%)] Loss: -1859.746582\n",
      "Train Epoch: 548 [45568/54000 (84%)] Loss: -1491.680298\n",
      "    epoch          : 548\n",
      "    loss           : -1626.491617032797\n",
      "    val_loss       : -1621.3566573832184\n",
      "    val_log_likelihood: 1703.2545166015625\n",
      "    val_log_marginal: 1656.3180583383887\n",
      "Train Epoch: 549 [512/54000 (1%)] Loss: -1706.992920\n",
      "Train Epoch: 549 [11776/54000 (22%)] Loss: -1602.055420\n",
      "Train Epoch: 549 [23040/54000 (43%)] Loss: -1562.270752\n",
      "Train Epoch: 549 [34304/54000 (64%)] Loss: -1553.851562\n",
      "Train Epoch: 549 [45568/54000 (84%)] Loss: -1482.364990\n",
      "    epoch          : 549\n",
      "    loss           : -1628.1300471844058\n",
      "    val_loss       : -1634.7617432197555\n",
      "    val_log_likelihood: 1726.4370971679687\n",
      "    val_log_marginal: 1686.8426628228276\n",
      "Train Epoch: 550 [512/54000 (1%)] Loss: -1879.357910\n",
      "Train Epoch: 550 [11776/54000 (22%)] Loss: -1562.935669\n",
      "Train Epoch: 550 [23040/54000 (43%)] Loss: -1474.016724\n",
      "Train Epoch: 550 [34304/54000 (64%)] Loss: -1490.683350\n",
      "Train Epoch: 550 [45568/54000 (84%)] Loss: -1552.818726\n",
      "    epoch          : 550\n",
      "    loss           : -1629.97352161974\n",
      "    val_loss       : -1615.5156550725922\n",
      "    val_log_likelihood: 1702.5167846679688\n",
      "    val_log_marginal: 1666.1690341521055\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch550.pth ...\n",
      "Train Epoch: 551 [512/54000 (1%)] Loss: -1902.344604\n",
      "Train Epoch: 551 [11776/54000 (22%)] Loss: -1694.620483\n",
      "Train Epoch: 551 [23040/54000 (43%)] Loss: -1513.917114\n",
      "Train Epoch: 551 [34304/54000 (64%)] Loss: -1614.212402\n",
      "Train Epoch: 551 [45568/54000 (84%)] Loss: -1649.980469\n",
      "    epoch          : 551\n",
      "    loss           : -1629.3833104501857\n",
      "    val_loss       : -1621.4059479008429\n",
      "    val_log_likelihood: 1701.5729736328126\n",
      "    val_log_marginal: 1664.862958887592\n",
      "Train Epoch: 552 [512/54000 (1%)] Loss: -1877.959595\n",
      "Train Epoch: 552 [11776/54000 (22%)] Loss: -1733.824829\n",
      "Train Epoch: 552 [23040/54000 (43%)] Loss: -1653.660034\n",
      "Train Epoch: 552 [34304/54000 (64%)] Loss: -1614.637695\n",
      "Train Epoch: 552 [45568/54000 (84%)] Loss: -1550.105103\n",
      "    epoch          : 552\n",
      "    loss           : -1637.809677879409\n",
      "    val_loss       : -1625.1363295311107\n",
      "    val_log_likelihood: 1736.8783447265625\n",
      "    val_log_marginal: 1693.0239121284335\n",
      "Train Epoch: 553 [512/54000 (1%)] Loss: -1906.344971\n",
      "Train Epoch: 553 [11776/54000 (22%)] Loss: -1570.762085\n",
      "Train Epoch: 553 [23040/54000 (43%)] Loss: -1638.752930\n",
      "Train Epoch: 553 [34304/54000 (64%)] Loss: -1521.540527\n",
      "Train Epoch: 553 [45568/54000 (84%)] Loss: -1528.374512\n",
      "    epoch          : 553\n",
      "    loss           : -1625.89903697401\n",
      "    val_loss       : -1617.829396918416\n",
      "    val_log_likelihood: 1723.9690551757812\n",
      "    val_log_marginal: 1672.5753089759498\n",
      "Train Epoch: 554 [512/54000 (1%)] Loss: -1698.683594\n",
      "Train Epoch: 554 [11776/54000 (22%)] Loss: -1689.869019\n",
      "Train Epoch: 554 [23040/54000 (43%)] Loss: -1588.658325\n",
      "Train Epoch: 554 [34304/54000 (64%)] Loss: -1615.279053\n",
      "Train Epoch: 554 [45568/54000 (84%)] Loss: -1604.699951\n",
      "    epoch          : 554\n",
      "    loss           : -1625.9825536142482\n",
      "    val_loss       : -1636.5267169427127\n",
      "    val_log_likelihood: 1722.47177734375\n",
      "    val_log_marginal: 1684.8869448486716\n",
      "Train Epoch: 555 [512/54000 (1%)] Loss: -1890.404541\n",
      "Train Epoch: 555 [11776/54000 (22%)] Loss: -1705.756226\n",
      "Train Epoch: 555 [23040/54000 (43%)] Loss: -1565.303223\n",
      "Train Epoch: 555 [34304/54000 (64%)] Loss: -1647.639893\n",
      "Train Epoch: 555 [45568/54000 (84%)] Loss: -1539.436157\n",
      "    epoch          : 555\n",
      "    loss           : -1623.4403559618656\n",
      "    val_loss       : -1628.4104278727434\n",
      "    val_log_likelihood: 1724.6702514648437\n",
      "    val_log_marginal: 1684.0781568832695\n",
      "Train Epoch: 556 [512/54000 (1%)] Loss: -1929.036865\n",
      "Train Epoch: 556 [11776/54000 (22%)] Loss: -1681.758301\n",
      "Train Epoch: 556 [23040/54000 (43%)] Loss: -1514.087402\n",
      "Train Epoch: 556 [34304/54000 (64%)] Loss: -1539.291504\n",
      "Train Epoch: 556 [45568/54000 (84%)] Loss: -1613.069336\n",
      "    epoch          : 556\n",
      "    loss           : -1632.3780420888768\n",
      "    val_loss       : -1621.1980889325962\n",
      "    val_log_likelihood: 1731.8786499023438\n",
      "    val_log_marginal: 1696.5481266573072\n",
      "Train Epoch: 557 [512/54000 (1%)] Loss: -1692.045898\n",
      "Train Epoch: 557 [11776/54000 (22%)] Loss: -1559.100098\n",
      "Train Epoch: 557 [23040/54000 (43%)] Loss: -1472.466064\n",
      "Train Epoch: 557 [34304/54000 (64%)] Loss: -1484.135620\n",
      "Train Epoch: 557 [45568/54000 (84%)] Loss: -1478.355957\n",
      "    epoch          : 557\n",
      "    loss           : -1611.2566631052753\n",
      "    val_loss       : -1607.4869758702814\n",
      "    val_log_likelihood: 1692.5882568359375\n",
      "    val_log_marginal: 1660.9270645968616\n",
      "Train Epoch: 558 [512/54000 (1%)] Loss: -1894.835327\n",
      "Train Epoch: 558 [11776/54000 (22%)] Loss: -1541.877563\n",
      "Train Epoch: 558 [23040/54000 (43%)] Loss: -1618.812134\n",
      "Train Epoch: 558 [34304/54000 (64%)] Loss: -1586.962646\n",
      "Train Epoch: 558 [45568/54000 (84%)] Loss: -1610.603027\n",
      "    epoch          : 558\n",
      "    loss           : -1609.0620866530012\n",
      "    val_loss       : -1604.0203744230791\n",
      "    val_log_likelihood: 1683.4025390625\n",
      "    val_log_marginal: 1639.3560663934798\n",
      "Train Epoch: 559 [512/54000 (1%)] Loss: -1874.517700\n",
      "Train Epoch: 559 [11776/54000 (22%)] Loss: -1603.035767\n",
      "Train Epoch: 559 [23040/54000 (43%)] Loss: -1574.382080\n",
      "Train Epoch: 559 [34304/54000 (64%)] Loss: -1616.761475\n",
      "Train Epoch: 559 [45568/54000 (84%)] Loss: -1614.162598\n",
      "    epoch          : 559\n",
      "    loss           : -1613.9005187383973\n",
      "    val_loss       : -1624.0428934156894\n",
      "    val_log_likelihood: 1700.5661376953126\n",
      "    val_log_marginal: 1654.215456038341\n",
      "Train Epoch: 560 [512/54000 (1%)] Loss: -1909.531738\n",
      "Train Epoch: 560 [11776/54000 (22%)] Loss: -1573.319336\n",
      "Train Epoch: 560 [23040/54000 (43%)] Loss: -1539.374512\n",
      "Train Epoch: 560 [34304/54000 (64%)] Loss: -1625.762451\n",
      "Train Epoch: 560 [45568/54000 (84%)] Loss: -1639.507080\n",
      "    epoch          : 560\n",
      "    loss           : -1624.7339181427908\n",
      "    val_loss       : -1628.0552953463048\n",
      "    val_log_likelihood: 1709.0531616210938\n",
      "    val_log_marginal: 1663.0204471439124\n",
      "Train Epoch: 561 [512/54000 (1%)] Loss: -1710.892700\n",
      "Train Epoch: 561 [11776/54000 (22%)] Loss: -1566.822144\n",
      "Train Epoch: 561 [23040/54000 (43%)] Loss: -1688.864014\n",
      "Train Epoch: 561 [34304/54000 (64%)] Loss: -1674.027710\n",
      "Train Epoch: 561 [45568/54000 (84%)] Loss: -1630.514893\n",
      "    epoch          : 561\n",
      "    loss           : -1629.5289004486385\n",
      "    val_loss       : -1618.4398160743526\n",
      "    val_log_likelihood: 1686.9023681640624\n",
      "    val_log_marginal: 1649.3523929049638\n",
      "Train Epoch: 562 [512/54000 (1%)] Loss: -1902.003052\n",
      "Train Epoch: 562 [11776/54000 (22%)] Loss: -1560.412109\n",
      "Train Epoch: 562 [23040/54000 (43%)] Loss: -1560.510010\n",
      "Train Epoch: 562 [34304/54000 (64%)] Loss: -1592.183594\n",
      "Train Epoch: 562 [45568/54000 (84%)] Loss: -1617.471802\n",
      "    epoch          : 562\n",
      "    loss           : -1622.7247302366955\n",
      "    val_loss       : -1611.64434838593\n",
      "    val_log_likelihood: 1693.5234008789062\n",
      "    val_log_marginal: 1654.4769767425955\n",
      "Train Epoch: 563 [512/54000 (1%)] Loss: -1887.729492\n",
      "Train Epoch: 563 [11776/54000 (22%)] Loss: -1646.647217\n",
      "Train Epoch: 563 [23040/54000 (43%)] Loss: -1652.745605\n",
      "Train Epoch: 563 [34304/54000 (64%)] Loss: -1593.954712\n",
      "Train Epoch: 563 [45568/54000 (84%)] Loss: -1655.440430\n",
      "    epoch          : 563\n",
      "    loss           : -1626.2775600924351\n",
      "    val_loss       : -1612.412706716638\n",
      "    val_log_likelihood: 1715.7383666992187\n",
      "    val_log_marginal: 1676.9119496285916\n",
      "Train Epoch: 564 [512/54000 (1%)] Loss: -1925.940430\n",
      "Train Epoch: 564 [11776/54000 (22%)] Loss: -1536.125488\n",
      "Train Epoch: 564 [23040/54000 (43%)] Loss: -1646.952393\n",
      "Train Epoch: 564 [34304/54000 (64%)] Loss: -1541.824707\n",
      "Train Epoch: 564 [45568/54000 (84%)] Loss: -1612.549683\n",
      "    epoch          : 564\n",
      "    loss           : -1625.9692068572092\n",
      "    val_loss       : -1625.7104202557355\n",
      "    val_log_likelihood: 1723.9248779296875\n",
      "    val_log_marginal: 1680.8160320106895\n",
      "Train Epoch: 565 [512/54000 (1%)] Loss: -1907.832764\n",
      "Train Epoch: 565 [11776/54000 (22%)] Loss: -1685.144775\n",
      "Train Epoch: 565 [23040/54000 (43%)] Loss: -1515.774170\n",
      "Train Epoch: 565 [34304/54000 (64%)] Loss: -1529.551636\n",
      "Train Epoch: 565 [45568/54000 (84%)] Loss: -1623.506592\n",
      "    epoch          : 565\n",
      "    loss           : -1629.8017723159035\n",
      "    val_loss       : -1630.5429631114937\n",
      "    val_log_likelihood: 1708.6494018554688\n",
      "    val_log_marginal: 1664.8501220662147\n",
      "Train Epoch: 566 [512/54000 (1%)] Loss: -1647.877563\n",
      "Train Epoch: 566 [11776/54000 (22%)] Loss: -1503.281982\n",
      "Train Epoch: 566 [23040/54000 (43%)] Loss: -1545.644897\n",
      "Train Epoch: 566 [34304/54000 (64%)] Loss: -1691.803955\n",
      "Train Epoch: 566 [45568/54000 (84%)] Loss: -1621.134766\n",
      "    epoch          : 566\n",
      "    loss           : -1629.8917816464264\n",
      "    val_loss       : -1620.6082362003624\n",
      "    val_log_likelihood: 1698.066552734375\n",
      "    val_log_marginal: 1669.0231504626572\n",
      "Train Epoch: 567 [512/54000 (1%)] Loss: -1922.201172\n",
      "Train Epoch: 567 [11776/54000 (22%)] Loss: -1710.697998\n",
      "Train Epoch: 567 [23040/54000 (43%)] Loss: -1605.470581\n",
      "Train Epoch: 567 [34304/54000 (64%)] Loss: -1598.397827\n",
      "Train Epoch: 567 [45568/54000 (84%)] Loss: -1554.479126\n",
      "    epoch          : 567\n",
      "    loss           : -1615.3232059289912\n",
      "    val_loss       : -1612.8956074491143\n",
      "    val_log_likelihood: 1695.5537719726562\n",
      "    val_log_marginal: 1648.8259184416384\n",
      "Train Epoch: 568 [512/54000 (1%)] Loss: -1910.017578\n",
      "Train Epoch: 568 [11776/54000 (22%)] Loss: -1542.122803\n",
      "Train Epoch: 568 [23040/54000 (43%)] Loss: -1551.489502\n",
      "Train Epoch: 568 [34304/54000 (64%)] Loss: -1618.547241\n",
      "Train Epoch: 568 [45568/54000 (84%)] Loss: -1556.340210\n",
      "    epoch          : 568\n",
      "    loss           : -1625.6868703105663\n",
      "    val_loss       : -1631.850697415322\n",
      "    val_log_likelihood: 1707.6054931640624\n",
      "    val_log_marginal: 1666.766182410717\n",
      "Train Epoch: 569 [512/54000 (1%)] Loss: -1893.415771\n",
      "Train Epoch: 569 [11776/54000 (22%)] Loss: -1686.264404\n",
      "Train Epoch: 569 [23040/54000 (43%)] Loss: -1531.334717\n",
      "Train Epoch: 569 [34304/54000 (64%)] Loss: -1513.467773\n",
      "Train Epoch: 569 [45568/54000 (84%)] Loss: -1619.351196\n",
      "    epoch          : 569\n",
      "    loss           : -1623.0918113784035\n",
      "    val_loss       : -1618.9894330077805\n",
      "    val_log_likelihood: 1703.5968505859375\n",
      "    val_log_marginal: 1668.0552539795638\n",
      "Train Epoch: 570 [512/54000 (1%)] Loss: -1912.853760\n",
      "Train Epoch: 570 [11776/54000 (22%)] Loss: -1686.985474\n",
      "Train Epoch: 570 [23040/54000 (43%)] Loss: -1580.883057\n",
      "Train Epoch: 570 [34304/54000 (64%)] Loss: -1678.300293\n",
      "Train Epoch: 570 [45568/54000 (84%)] Loss: -1574.744751\n",
      "    epoch          : 570\n",
      "    loss           : -1628.3428314511139\n",
      "    val_loss       : -1625.1224481266922\n",
      "    val_log_likelihood: 1711.05048828125\n",
      "    val_log_marginal: 1675.7420444525778\n",
      "Train Epoch: 571 [512/54000 (1%)] Loss: -1881.427002\n",
      "Train Epoch: 571 [11776/54000 (22%)] Loss: -1756.907837\n",
      "Train Epoch: 571 [23040/54000 (43%)] Loss: -1531.991943\n",
      "Train Epoch: 571 [34304/54000 (64%)] Loss: -1902.637939\n",
      "Train Epoch: 571 [45568/54000 (84%)] Loss: -1646.658325\n",
      "    epoch          : 571\n",
      "    loss           : -1637.021129041615\n",
      "    val_loss       : -1623.1558405913413\n",
      "    val_log_likelihood: 1723.47470703125\n",
      "    val_log_marginal: 1692.4605386331677\n",
      "Train Epoch: 572 [512/54000 (1%)] Loss: -1884.930420\n",
      "Train Epoch: 572 [11776/54000 (22%)] Loss: -1539.829346\n",
      "Train Epoch: 572 [23040/54000 (43%)] Loss: -1508.880493\n",
      "Train Epoch: 572 [34304/54000 (64%)] Loss: -1884.260498\n",
      "Train Epoch: 572 [45568/54000 (84%)] Loss: -1634.954834\n",
      "    epoch          : 572\n",
      "    loss           : -1631.3089950108292\n",
      "    val_loss       : -1623.9691040004604\n",
      "    val_log_likelihood: 1716.2831909179688\n",
      "    val_log_marginal: 1676.7384367093443\n",
      "Train Epoch: 573 [512/54000 (1%)] Loss: -1909.242432\n",
      "Train Epoch: 573 [11776/54000 (22%)] Loss: -1673.088501\n",
      "Train Epoch: 573 [23040/54000 (43%)] Loss: -1534.036987\n",
      "Train Epoch: 573 [34304/54000 (64%)] Loss: -1621.934326\n",
      "Train Epoch: 573 [45568/54000 (84%)] Loss: -1647.960938\n",
      "    epoch          : 573\n",
      "    loss           : -1637.6166496654548\n",
      "    val_loss       : -1629.4773476943374\n",
      "    val_log_likelihood: 1705.9214599609375\n",
      "    val_log_marginal: 1663.4987395193427\n",
      "Train Epoch: 574 [512/54000 (1%)] Loss: -1907.795288\n",
      "Train Epoch: 574 [11776/54000 (22%)] Loss: -1542.887207\n",
      "Train Epoch: 574 [23040/54000 (43%)] Loss: -1565.498169\n",
      "Train Epoch: 574 [34304/54000 (64%)] Loss: -1637.599121\n",
      "Train Epoch: 574 [45568/54000 (84%)] Loss: -1516.757324\n",
      "    epoch          : 574\n",
      "    loss           : -1618.7507650545328\n",
      "    val_loss       : -1576.5493477033451\n",
      "    val_log_likelihood: 1707.0216064453125\n",
      "    val_log_marginal: 1662.1864290982485\n",
      "Train Epoch: 575 [512/54000 (1%)] Loss: -1930.392578\n",
      "Train Epoch: 575 [11776/54000 (22%)] Loss: -1539.854858\n",
      "Train Epoch: 575 [23040/54000 (43%)] Loss: -1446.687012\n",
      "Train Epoch: 575 [34304/54000 (64%)] Loss: -1603.609009\n",
      "Train Epoch: 575 [45568/54000 (84%)] Loss: -1608.751953\n",
      "    epoch          : 575\n",
      "    loss           : -1609.9620941464264\n",
      "    val_loss       : -1613.1537605202757\n",
      "    val_log_likelihood: 1705.2588745117187\n",
      "    val_log_marginal: 1654.9609660010785\n",
      "Train Epoch: 576 [512/54000 (1%)] Loss: -1890.275024\n",
      "Train Epoch: 576 [11776/54000 (22%)] Loss: -1563.471680\n",
      "Train Epoch: 576 [23040/54000 (43%)] Loss: -1494.570801\n",
      "Train Epoch: 576 [34304/54000 (64%)] Loss: -1498.664673\n",
      "Train Epoch: 576 [45568/54000 (84%)] Loss: -1650.496826\n",
      "    epoch          : 576\n",
      "    loss           : -1623.4669419090346\n",
      "    val_loss       : -1631.8467703131028\n",
      "    val_log_likelihood: 1714.2476806640625\n",
      "    val_log_marginal: 1680.1974703609944\n",
      "Train Epoch: 577 [512/54000 (1%)] Loss: -1914.851929\n",
      "Train Epoch: 577 [11776/54000 (22%)] Loss: -1572.392578\n",
      "Train Epoch: 577 [23040/54000 (43%)] Loss: -1665.025269\n",
      "Train Epoch: 577 [34304/54000 (64%)] Loss: -1883.858032\n",
      "Train Epoch: 577 [45568/54000 (84%)] Loss: -1625.859253\n",
      "    epoch          : 577\n",
      "    loss           : -1602.6808513981282\n",
      "    val_loss       : -1608.9349964463152\n",
      "    val_log_likelihood: 1684.768896484375\n",
      "    val_log_marginal: 1645.4672153647837\n",
      "Train Epoch: 578 [512/54000 (1%)] Loss: -1901.921875\n",
      "Train Epoch: 578 [11776/54000 (22%)] Loss: -1517.430420\n",
      "Train Epoch: 578 [23040/54000 (43%)] Loss: -1669.108154\n",
      "Train Epoch: 578 [34304/54000 (64%)] Loss: -1496.774170\n",
      "Train Epoch: 578 [45568/54000 (84%)] Loss: -1562.673096\n",
      "    epoch          : 578\n",
      "    loss           : -1623.104326606977\n",
      "    val_loss       : -1620.1351030104793\n",
      "    val_log_likelihood: 1707.4211669921874\n",
      "    val_log_marginal: 1657.836053759232\n",
      "Train Epoch: 579 [512/54000 (1%)] Loss: -1904.276855\n",
      "Train Epoch: 579 [11776/54000 (22%)] Loss: -1681.146729\n",
      "Train Epoch: 579 [23040/54000 (43%)] Loss: -1691.728394\n",
      "Train Epoch: 579 [34304/54000 (64%)] Loss: -1639.181396\n",
      "Train Epoch: 579 [45568/54000 (84%)] Loss: -1568.460205\n",
      "    epoch          : 579\n",
      "    loss           : -1628.6175815091274\n",
      "    val_loss       : -1630.726118823979\n",
      "    val_log_likelihood: 1710.186962890625\n",
      "    val_log_marginal: 1681.3077869135886\n",
      "Train Epoch: 580 [512/54000 (1%)] Loss: -1914.546143\n",
      "Train Epoch: 580 [11776/54000 (22%)] Loss: -1697.057983\n",
      "Train Epoch: 580 [23040/54000 (43%)] Loss: -1594.046509\n",
      "Train Epoch: 580 [34304/54000 (64%)] Loss: -1512.178101\n",
      "Train Epoch: 580 [45568/54000 (84%)] Loss: -1612.968262\n",
      "    epoch          : 580\n",
      "    loss           : -1620.756156694771\n",
      "    val_loss       : -1622.0722720670515\n",
      "    val_log_likelihood: 1687.4855834960938\n",
      "    val_log_marginal: 1651.1722958821804\n",
      "Train Epoch: 581 [512/54000 (1%)] Loss: -1899.572144\n",
      "Train Epoch: 581 [11776/54000 (22%)] Loss: -1540.113281\n",
      "Train Epoch: 581 [23040/54000 (43%)] Loss: -1528.263916\n",
      "Train Epoch: 581 [34304/54000 (64%)] Loss: -1606.092773\n",
      "Train Epoch: 581 [45568/54000 (84%)] Loss: -1553.116333\n",
      "    epoch          : 581\n",
      "    loss           : -1630.6816345819152\n",
      "    val_loss       : -1623.922412728984\n",
      "    val_log_likelihood: 1699.607275390625\n",
      "    val_log_marginal: 1653.1902384731918\n",
      "Train Epoch: 582 [512/54000 (1%)] Loss: -1724.488037\n",
      "Train Epoch: 582 [11776/54000 (22%)] Loss: -1595.758545\n",
      "Train Epoch: 582 [23040/54000 (43%)] Loss: -1706.634521\n",
      "Train Epoch: 582 [34304/54000 (64%)] Loss: -1649.546265\n",
      "Train Epoch: 582 [45568/54000 (84%)] Loss: -1631.949951\n",
      "    epoch          : 582\n",
      "    loss           : -1635.092769811649\n",
      "    val_loss       : -1635.0662665720097\n",
      "    val_log_likelihood: 1712.7767822265625\n",
      "    val_log_marginal: 1667.3551387924701\n",
      "Train Epoch: 583 [512/54000 (1%)] Loss: -1705.188599\n",
      "Train Epoch: 583 [11776/54000 (22%)] Loss: -1588.248657\n",
      "Train Epoch: 583 [23040/54000 (43%)] Loss: -1540.172119\n",
      "Train Epoch: 583 [34304/54000 (64%)] Loss: -1901.786011\n",
      "Train Epoch: 583 [45568/54000 (84%)] Loss: -1656.842285\n",
      "    epoch          : 583\n",
      "    loss           : -1636.5703741394648\n",
      "    val_loss       : -1629.1905692718924\n",
      "    val_log_likelihood: 1716.8449462890626\n",
      "    val_log_marginal: 1667.4824486896396\n",
      "Train Epoch: 584 [512/54000 (1%)] Loss: -1888.825195\n",
      "Train Epoch: 584 [11776/54000 (22%)] Loss: -1681.424194\n",
      "Train Epoch: 584 [23040/54000 (43%)] Loss: -1553.104004\n",
      "Train Epoch: 584 [34304/54000 (64%)] Loss: -1903.608154\n",
      "Train Epoch: 584 [45568/54000 (84%)] Loss: -1631.706421\n",
      "    epoch          : 584\n",
      "    loss           : -1637.1828673712098\n",
      "    val_loss       : -1625.2183982930146\n",
      "    val_log_likelihood: 1705.6670654296875\n",
      "    val_log_marginal: 1667.424384047091\n",
      "Train Epoch: 585 [512/54000 (1%)] Loss: -1910.656494\n",
      "Train Epoch: 585 [11776/54000 (22%)] Loss: -1571.883423\n",
      "Train Epoch: 585 [23040/54000 (43%)] Loss: -1675.610229\n",
      "Train Epoch: 585 [34304/54000 (64%)] Loss: -1520.407959\n",
      "Train Epoch: 585 [45568/54000 (84%)] Loss: -1626.747559\n",
      "    epoch          : 585\n",
      "    loss           : -1637.884247128326\n",
      "    val_loss       : -1620.0957746149973\n",
      "    val_log_likelihood: 1702.2845336914063\n",
      "    val_log_marginal: 1667.6352884758264\n",
      "Train Epoch: 586 [512/54000 (1%)] Loss: -1913.465820\n",
      "Train Epoch: 586 [11776/54000 (22%)] Loss: -1556.164185\n",
      "Train Epoch: 586 [23040/54000 (43%)] Loss: -1573.252930\n",
      "Train Epoch: 586 [34304/54000 (64%)] Loss: -1665.308716\n",
      "Train Epoch: 586 [45568/54000 (84%)] Loss: -1558.529663\n",
      "    epoch          : 586\n",
      "    loss           : -1631.2796159498762\n",
      "    val_loss       : -1628.3808442709037\n",
      "    val_log_likelihood: 1725.7493774414063\n",
      "    val_log_marginal: 1675.0250361781568\n",
      "Train Epoch: 587 [512/54000 (1%)] Loss: -1923.717285\n",
      "Train Epoch: 587 [11776/54000 (22%)] Loss: -1580.777344\n",
      "Train Epoch: 587 [23040/54000 (43%)] Loss: -1674.062866\n",
      "Train Epoch: 587 [34304/54000 (64%)] Loss: -1698.541626\n",
      "Train Epoch: 587 [45568/54000 (84%)] Loss: -1667.923828\n",
      "    epoch          : 587\n",
      "    loss           : -1640.2061477510056\n",
      "    val_loss       : -1634.5120856988244\n",
      "    val_log_likelihood: 1729.7764770507813\n",
      "    val_log_marginal: 1700.8570815056562\n",
      "Train Epoch: 588 [512/54000 (1%)] Loss: -1888.455566\n",
      "Train Epoch: 588 [11776/54000 (22%)] Loss: -1617.309692\n",
      "Train Epoch: 588 [23040/54000 (43%)] Loss: -1532.980347\n",
      "Train Epoch: 588 [34304/54000 (64%)] Loss: -1901.803223\n",
      "Train Epoch: 588 [45568/54000 (84%)] Loss: -1586.295288\n",
      "    epoch          : 588\n",
      "    loss           : -1639.9909547107054\n",
      "    val_loss       : -1627.8721178936771\n",
      "    val_log_likelihood: 1718.797998046875\n",
      "    val_log_marginal: 1688.0573948927224\n",
      "Train Epoch: 589 [512/54000 (1%)] Loss: -1919.150879\n",
      "Train Epoch: 589 [11776/54000 (22%)] Loss: -1573.360840\n",
      "Train Epoch: 589 [23040/54000 (43%)] Loss: -1554.471680\n",
      "Train Epoch: 589 [34304/54000 (64%)] Loss: -1538.138062\n",
      "Train Epoch: 589 [45568/54000 (84%)] Loss: -1549.863647\n",
      "    epoch          : 589\n",
      "    loss           : -1615.1092710589419\n",
      "    val_loss       : -1618.4735148818231\n",
      "    val_log_likelihood: 1707.04560546875\n",
      "    val_log_marginal: 1668.3178128037603\n",
      "Train Epoch: 590 [512/54000 (1%)] Loss: -1659.572510\n",
      "Train Epoch: 590 [11776/54000 (22%)] Loss: -1514.723389\n",
      "Train Epoch: 590 [23040/54000 (43%)] Loss: -1511.131836\n",
      "Train Epoch: 590 [34304/54000 (64%)] Loss: -1673.674805\n",
      "Train Epoch: 590 [45568/54000 (84%)] Loss: -1439.766602\n",
      "    epoch          : 590\n",
      "    loss           : -1617.6773464089572\n",
      "    val_loss       : -1605.8976598748006\n",
      "    val_log_likelihood: 1708.2029541015625\n",
      "    val_log_marginal: 1669.420636381954\n",
      "Train Epoch: 591 [512/54000 (1%)] Loss: -1909.760132\n",
      "Train Epoch: 591 [11776/54000 (22%)] Loss: -1531.396240\n",
      "Train Epoch: 591 [23040/54000 (43%)] Loss: -1541.833740\n",
      "Train Epoch: 591 [34304/54000 (64%)] Loss: -1604.456665\n",
      "Train Epoch: 591 [45568/54000 (84%)] Loss: -1529.689575\n",
      "    epoch          : 591\n",
      "    loss           : -1624.286054252398\n",
      "    val_loss       : -1620.2743452577852\n",
      "    val_log_likelihood: 1712.2190551757812\n",
      "    val_log_marginal: 1680.0127045299857\n",
      "Train Epoch: 592 [512/54000 (1%)] Loss: -1900.037964\n",
      "Train Epoch: 592 [11776/54000 (22%)] Loss: -1684.931274\n",
      "Train Epoch: 592 [23040/54000 (43%)] Loss: -1688.325195\n",
      "Train Epoch: 592 [34304/54000 (64%)] Loss: -1621.243896\n",
      "Train Epoch: 592 [45568/54000 (84%)] Loss: -1565.615845\n",
      "    epoch          : 592\n",
      "    loss           : -1635.673370059174\n",
      "    val_loss       : -1632.342724037729\n",
      "    val_log_likelihood: 1726.9409545898438\n",
      "    val_log_marginal: 1663.1689154494554\n",
      "Train Epoch: 593 [512/54000 (1%)] Loss: -1904.698242\n",
      "Train Epoch: 593 [11776/54000 (22%)] Loss: -1540.880127\n",
      "Train Epoch: 593 [23040/54000 (43%)] Loss: -1555.249756\n",
      "Train Epoch: 593 [34304/54000 (64%)] Loss: -1609.596924\n",
      "Train Epoch: 593 [45568/54000 (84%)] Loss: -1615.865601\n",
      "    epoch          : 593\n",
      "    loss           : -1640.5301537844214\n",
      "    val_loss       : -1636.1847467662767\n",
      "    val_log_likelihood: 1702.1522827148438\n",
      "    val_log_marginal: 1661.2893504850567\n",
      "Train Epoch: 594 [512/54000 (1%)] Loss: -1709.304199\n",
      "Train Epoch: 594 [11776/54000 (22%)] Loss: -1575.918457\n",
      "Train Epoch: 594 [23040/54000 (43%)] Loss: -1509.780151\n",
      "Train Epoch: 594 [34304/54000 (64%)] Loss: -1673.048340\n",
      "Train Epoch: 594 [45568/54000 (84%)] Loss: -1587.781250\n",
      "    epoch          : 594\n",
      "    loss           : -1639.3482110051825\n",
      "    val_loss       : -1648.230864390824\n",
      "    val_log_likelihood: 1727.5614624023438\n",
      "    val_log_marginal: 1697.1372471120208\n",
      "Train Epoch: 595 [512/54000 (1%)] Loss: -1910.056152\n",
      "Train Epoch: 595 [11776/54000 (22%)] Loss: -1682.576660\n",
      "Train Epoch: 595 [23040/54000 (43%)] Loss: -1730.829712\n",
      "Train Epoch: 595 [34304/54000 (64%)] Loss: -1532.738281\n",
      "Train Epoch: 595 [45568/54000 (84%)] Loss: -1652.448120\n",
      "    epoch          : 595\n",
      "    loss           : -1639.261719958617\n",
      "    val_loss       : -1644.8300808826461\n",
      "    val_log_likelihood: 1718.0202270507812\n",
      "    val_log_marginal: 1691.983938767761\n",
      "Train Epoch: 596 [512/54000 (1%)] Loss: -1902.058105\n",
      "Train Epoch: 596 [11776/54000 (22%)] Loss: -1583.451904\n",
      "Train Epoch: 596 [23040/54000 (43%)] Loss: -1706.080688\n",
      "Train Epoch: 596 [34304/54000 (64%)] Loss: -1648.506592\n",
      "Train Epoch: 596 [45568/54000 (84%)] Loss: -1621.228638\n",
      "    epoch          : 596\n",
      "    loss           : -1642.0092024094988\n",
      "    val_loss       : -1640.971142260451\n",
      "    val_log_likelihood: 1727.0096313476563\n",
      "    val_log_marginal: 1690.4208149600775\n",
      "Train Epoch: 597 [512/54000 (1%)] Loss: -1670.120728\n",
      "Train Epoch: 597 [11776/54000 (22%)] Loss: -1597.187500\n",
      "Train Epoch: 597 [23040/54000 (43%)] Loss: -1583.011475\n",
      "Train Epoch: 597 [34304/54000 (64%)] Loss: -1671.961792\n",
      "Train Epoch: 597 [45568/54000 (84%)] Loss: -1498.907959\n",
      "    epoch          : 597\n",
      "    loss           : -1629.4857093131188\n",
      "    val_loss       : -1606.9764712690376\n",
      "    val_log_likelihood: 1706.2446899414062\n",
      "    val_log_marginal: 1666.7951781891286\n",
      "Train Epoch: 598 [512/54000 (1%)] Loss: -1918.654785\n",
      "Train Epoch: 598 [11776/54000 (22%)] Loss: -1638.686890\n",
      "Train Epoch: 598 [23040/54000 (43%)] Loss: -1506.845703\n",
      "Train Epoch: 598 [34304/54000 (64%)] Loss: -1675.415405\n",
      "Train Epoch: 598 [45568/54000 (84%)] Loss: -1510.934937\n",
      "    epoch          : 598\n",
      "    loss           : -1626.393716642172\n",
      "    val_loss       : -1627.5096371199936\n",
      "    val_log_likelihood: 1725.2568237304688\n",
      "    val_log_marginal: 1687.9448985807599\n",
      "Train Epoch: 599 [512/54000 (1%)] Loss: -1897.308105\n",
      "Train Epoch: 599 [11776/54000 (22%)] Loss: -1643.709961\n",
      "Train Epoch: 599 [23040/54000 (43%)] Loss: -1548.655884\n",
      "Train Epoch: 599 [34304/54000 (64%)] Loss: -1584.539795\n",
      "Train Epoch: 599 [45568/54000 (84%)] Loss: -1509.308838\n",
      "    epoch          : 599\n",
      "    loss           : -1639.0783788095607\n",
      "    val_loss       : -1638.171596536599\n",
      "    val_log_likelihood: 1729.2328857421876\n",
      "    val_log_marginal: 1692.9682892519982\n",
      "Train Epoch: 600 [512/54000 (1%)] Loss: -1900.732544\n",
      "Train Epoch: 600 [11776/54000 (22%)] Loss: -1548.691040\n",
      "Train Epoch: 600 [23040/54000 (43%)] Loss: -1658.306274\n",
      "Train Epoch: 600 [34304/54000 (64%)] Loss: -1501.869751\n",
      "Train Epoch: 600 [45568/54000 (84%)] Loss: -1641.801270\n",
      "    epoch          : 600\n",
      "    loss           : -1636.5613252243193\n",
      "    val_loss       : -1622.837943202816\n",
      "    val_log_likelihood: 1727.6357055664062\n",
      "    val_log_marginal: 1699.3316275406628\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch600.pth ...\n",
      "Train Epoch: 601 [512/54000 (1%)] Loss: -1900.567261\n",
      "Train Epoch: 601 [11776/54000 (22%)] Loss: -1727.414917\n",
      "Train Epoch: 601 [23040/54000 (43%)] Loss: -1560.145264\n",
      "Train Epoch: 601 [34304/54000 (64%)] Loss: -1929.165894\n",
      "Train Epoch: 601 [45568/54000 (84%)] Loss: -1554.535522\n",
      "    epoch          : 601\n",
      "    loss           : -1634.64892578125\n",
      "    val_loss       : -1635.0525086307898\n",
      "    val_log_likelihood: 1701.7076293945313\n",
      "    val_log_marginal: 1658.2280215859414\n",
      "Train Epoch: 602 [512/54000 (1%)] Loss: -1925.092041\n",
      "Train Epoch: 602 [11776/54000 (22%)] Loss: -1596.172241\n",
      "Train Epoch: 602 [23040/54000 (43%)] Loss: -1704.190186\n",
      "Train Epoch: 602 [34304/54000 (64%)] Loss: -1898.819946\n",
      "Train Epoch: 602 [45568/54000 (84%)] Loss: -1617.595581\n",
      "    epoch          : 602\n",
      "    loss           : -1623.8285794399753\n",
      "    val_loss       : -1611.3340628146193\n",
      "    val_log_likelihood: 1693.073388671875\n",
      "    val_log_marginal: 1661.6374352041632\n",
      "Train Epoch: 603 [512/54000 (1%)] Loss: -1907.135742\n",
      "Train Epoch: 603 [11776/54000 (22%)] Loss: -1637.336792\n",
      "Train Epoch: 603 [23040/54000 (43%)] Loss: -1545.760132\n",
      "Train Epoch: 603 [34304/54000 (64%)] Loss: -1484.256226\n",
      "Train Epoch: 603 [45568/54000 (84%)] Loss: -1679.993408\n",
      "    epoch          : 603\n",
      "    loss           : -1623.763587271813\n",
      "    val_loss       : -1630.6999545500614\n",
      "    val_log_likelihood: 1702.9523681640626\n",
      "    val_log_marginal: 1665.3113682512194\n",
      "Train Epoch: 604 [512/54000 (1%)] Loss: -1903.271851\n",
      "Train Epoch: 604 [11776/54000 (22%)] Loss: -1579.648438\n",
      "Train Epoch: 604 [23040/54000 (43%)] Loss: -1643.733765\n",
      "Train Epoch: 604 [34304/54000 (64%)] Loss: -1645.767578\n",
      "Train Epoch: 604 [45568/54000 (84%)] Loss: -1647.132202\n",
      "    epoch          : 604\n",
      "    loss           : -1630.3420192605197\n",
      "    val_loss       : -1631.7043649661355\n",
      "    val_log_likelihood: 1703.4635864257812\n",
      "    val_log_marginal: 1672.7689168650656\n",
      "Train Epoch: 605 [512/54000 (1%)] Loss: -1908.454590\n",
      "Train Epoch: 605 [11776/54000 (22%)] Loss: -1669.032227\n",
      "Train Epoch: 605 [23040/54000 (43%)] Loss: -1682.027100\n",
      "Train Epoch: 605 [34304/54000 (64%)] Loss: -1565.254639\n",
      "Train Epoch: 605 [45568/54000 (84%)] Loss: -1613.386841\n",
      "    epoch          : 605\n",
      "    loss           : -1639.1206284324721\n",
      "    val_loss       : -1643.690064705722\n",
      "    val_log_likelihood: 1713.2570190429688\n",
      "    val_log_marginal: 1675.2737959001213\n",
      "Train Epoch: 606 [512/54000 (1%)] Loss: -1891.345459\n",
      "Train Epoch: 606 [11776/54000 (22%)] Loss: -1555.112427\n",
      "Train Epoch: 606 [23040/54000 (43%)] Loss: -1629.436035\n",
      "Train Epoch: 606 [34304/54000 (64%)] Loss: -1573.213989\n",
      "Train Epoch: 606 [45568/54000 (84%)] Loss: -1655.638428\n",
      "    epoch          : 606\n",
      "    loss           : -1644.0746067160428\n",
      "    val_loss       : -1624.2790465796365\n",
      "    val_log_likelihood: 1724.4191772460938\n",
      "    val_log_marginal: 1681.041649338603\n",
      "Train Epoch: 607 [512/54000 (1%)] Loss: -1896.635620\n",
      "Train Epoch: 607 [11776/54000 (22%)] Loss: -1579.722656\n",
      "Train Epoch: 607 [23040/54000 (43%)] Loss: -1569.979492\n",
      "Train Epoch: 607 [34304/54000 (64%)] Loss: -1652.607422\n",
      "Train Epoch: 607 [45568/54000 (84%)] Loss: -1621.436523\n",
      "    epoch          : 607\n",
      "    loss           : -1640.987600798654\n",
      "    val_loss       : -1616.110411863029\n",
      "    val_log_likelihood: 1720.1905639648437\n",
      "    val_log_marginal: 1679.4981743991375\n",
      "Train Epoch: 608 [512/54000 (1%)] Loss: -1905.812744\n",
      "Train Epoch: 608 [11776/54000 (22%)] Loss: -1707.667114\n",
      "Train Epoch: 608 [23040/54000 (43%)] Loss: -1611.961426\n",
      "Train Epoch: 608 [34304/54000 (64%)] Loss: -1540.350586\n",
      "Train Epoch: 608 [45568/54000 (84%)] Loss: -1638.911499\n",
      "    epoch          : 608\n",
      "    loss           : -1641.121641253481\n",
      "    val_loss       : -1635.3183283676394\n",
      "    val_log_likelihood: 1731.2334228515624\n",
      "    val_log_marginal: 1698.0486432608218\n",
      "Train Epoch: 609 [512/54000 (1%)] Loss: -1737.538818\n",
      "Train Epoch: 609 [11776/54000 (22%)] Loss: -1724.710205\n",
      "Train Epoch: 609 [23040/54000 (43%)] Loss: -1552.047974\n",
      "Train Epoch: 609 [34304/54000 (64%)] Loss: -1607.376709\n",
      "Train Epoch: 609 [45568/54000 (84%)] Loss: -1631.907715\n",
      "    epoch          : 609\n",
      "    loss           : -1642.969128297107\n",
      "    val_loss       : -1638.444547898788\n",
      "    val_log_likelihood: 1730.33232421875\n",
      "    val_log_marginal: 1688.0872537005694\n",
      "Train Epoch: 610 [512/54000 (1%)] Loss: -1900.270996\n",
      "Train Epoch: 610 [11776/54000 (22%)] Loss: -1543.954712\n",
      "Train Epoch: 610 [23040/54000 (43%)] Loss: -1528.197021\n",
      "Train Epoch: 610 [34304/54000 (64%)] Loss: -1634.388672\n",
      "Train Epoch: 610 [45568/54000 (84%)] Loss: -1589.276733\n",
      "    epoch          : 610\n",
      "    loss           : -1642.2675720819152\n",
      "    val_loss       : -1630.0037151175552\n",
      "    val_log_likelihood: 1715.8127807617188\n",
      "    val_log_marginal: 1675.9938824336975\n",
      "Train Epoch: 611 [512/54000 (1%)] Loss: -1915.818604\n",
      "Train Epoch: 611 [11776/54000 (22%)] Loss: -1497.248169\n",
      "Train Epoch: 611 [23040/54000 (43%)] Loss: -1552.628662\n",
      "Train Epoch: 611 [34304/54000 (64%)] Loss: -1608.461914\n",
      "Train Epoch: 611 [45568/54000 (84%)] Loss: -1631.545166\n",
      "    epoch          : 611\n",
      "    loss           : -1646.3338345064976\n",
      "    val_loss       : -1635.749877789896\n",
      "    val_log_likelihood: 1716.21943359375\n",
      "    val_log_marginal: 1676.1165531624108\n",
      "Train Epoch: 612 [512/54000 (1%)] Loss: -1928.695312\n",
      "Train Epoch: 612 [11776/54000 (22%)] Loss: -1562.202881\n",
      "Train Epoch: 612 [23040/54000 (43%)] Loss: -1636.661743\n",
      "Train Epoch: 612 [34304/54000 (64%)] Loss: -1666.049561\n",
      "Train Epoch: 612 [45568/54000 (84%)] Loss: -1632.276611\n",
      "    epoch          : 612\n",
      "    loss           : -1644.5058158647896\n",
      "    val_loss       : -1652.3055925321764\n",
      "    val_log_likelihood: 1721.8207885742188\n",
      "    val_log_marginal: 1666.775042387098\n",
      "Train Epoch: 613 [512/54000 (1%)] Loss: -1903.083496\n",
      "Train Epoch: 613 [11776/54000 (22%)] Loss: -1564.325562\n",
      "Train Epoch: 613 [23040/54000 (43%)] Loss: -1535.472046\n",
      "Train Epoch: 613 [34304/54000 (64%)] Loss: -1660.924072\n",
      "Train Epoch: 613 [45568/54000 (84%)] Loss: -1517.985229\n",
      "    epoch          : 613\n",
      "    loss           : -1643.6187997950185\n",
      "    val_loss       : -1620.6479193895125\n",
      "    val_log_likelihood: 1708.5035888671875\n",
      "    val_log_marginal: 1666.9169896401465\n",
      "Train Epoch: 614 [512/54000 (1%)] Loss: -1860.291016\n",
      "Train Epoch: 614 [11776/54000 (22%)] Loss: -1580.959229\n",
      "Train Epoch: 614 [23040/54000 (43%)] Loss: -1628.544434\n",
      "Train Epoch: 614 [34304/54000 (64%)] Loss: -1534.614624\n",
      "Train Epoch: 614 [45568/54000 (84%)] Loss: -1537.712891\n",
      "    epoch          : 614\n",
      "    loss           : -1624.4608371847928\n",
      "    val_loss       : -1632.5863015549257\n",
      "    val_log_likelihood: 1719.5229248046876\n",
      "    val_log_marginal: 1677.7565271489323\n",
      "Train Epoch: 615 [512/54000 (1%)] Loss: -1889.826904\n",
      "Train Epoch: 615 [11776/54000 (22%)] Loss: -1621.229858\n",
      "Train Epoch: 615 [23040/54000 (43%)] Loss: -1504.054199\n",
      "Train Epoch: 615 [34304/54000 (64%)] Loss: -1617.382935\n",
      "Train Epoch: 615 [45568/54000 (84%)] Loss: -1632.805664\n",
      "    epoch          : 615\n",
      "    loss           : -1635.1451222636913\n",
      "    val_loss       : -1632.04939276753\n",
      "    val_log_likelihood: 1731.376171875\n",
      "    val_log_marginal: 1696.4335570715368\n",
      "Train Epoch: 616 [512/54000 (1%)] Loss: -1877.328247\n",
      "Train Epoch: 616 [11776/54000 (22%)] Loss: -1690.002563\n",
      "Train Epoch: 616 [23040/54000 (43%)] Loss: -1466.272461\n",
      "Train Epoch: 616 [34304/54000 (64%)] Loss: -1572.561523\n",
      "Train Epoch: 616 [45568/54000 (84%)] Loss: -1602.242310\n",
      "    epoch          : 616\n",
      "    loss           : -1630.6485547358448\n",
      "    val_loss       : -1618.4324066255242\n",
      "    val_log_likelihood: 1722.171142578125\n",
      "    val_log_marginal: 1687.4864622920752\n",
      "Train Epoch: 617 [512/54000 (1%)] Loss: -1819.690918\n",
      "Train Epoch: 617 [11776/54000 (22%)] Loss: -1561.713623\n",
      "Train Epoch: 617 [23040/54000 (43%)] Loss: -1568.891968\n",
      "Train Epoch: 617 [34304/54000 (64%)] Loss: -1605.568848\n",
      "Train Epoch: 617 [45568/54000 (84%)] Loss: -1616.017822\n",
      "    epoch          : 617\n",
      "    loss           : -1630.5228223139698\n",
      "    val_loss       : -1637.4017518449575\n",
      "    val_log_likelihood: 1724.381640625\n",
      "    val_log_marginal: 1692.4737864162773\n",
      "Train Epoch: 618 [512/54000 (1%)] Loss: -1883.280518\n",
      "Train Epoch: 618 [11776/54000 (22%)] Loss: -1562.010742\n",
      "Train Epoch: 618 [23040/54000 (43%)] Loss: -1692.666992\n",
      "Train Epoch: 618 [34304/54000 (64%)] Loss: -1480.798828\n",
      "Train Epoch: 618 [45568/54000 (84%)] Loss: -1611.633057\n",
      "    epoch          : 618\n",
      "    loss           : -1622.0257024481746\n",
      "    val_loss       : -1628.549830184132\n",
      "    val_log_likelihood: 1714.0484985351563\n",
      "    val_log_marginal: 1676.6578974913805\n",
      "Train Epoch: 619 [512/54000 (1%)] Loss: -1777.885864\n",
      "Train Epoch: 619 [11776/54000 (22%)] Loss: -1573.220703\n",
      "Train Epoch: 619 [23040/54000 (43%)] Loss: -1468.968872\n",
      "Train Epoch: 619 [34304/54000 (64%)] Loss: -1526.338013\n",
      "Train Epoch: 619 [45568/54000 (84%)] Loss: -1659.395142\n",
      "    epoch          : 619\n",
      "    loss           : -1624.607506478187\n",
      "    val_loss       : -1612.408152479399\n",
      "    val_log_likelihood: 1721.803955078125\n",
      "    val_log_marginal: 1687.5669874187558\n",
      "Train Epoch: 620 [512/54000 (1%)] Loss: -1902.125244\n",
      "Train Epoch: 620 [11776/54000 (22%)] Loss: -1552.908447\n",
      "Train Epoch: 620 [23040/54000 (43%)] Loss: -1530.206543\n",
      "Train Epoch: 620 [34304/54000 (64%)] Loss: -1657.300537\n",
      "Train Epoch: 620 [45568/54000 (84%)] Loss: -1538.195068\n",
      "    epoch          : 620\n",
      "    loss           : -1629.7284999129795\n",
      "    val_loss       : -1641.9379862160422\n",
      "    val_log_likelihood: 1739.9551147460938\n",
      "    val_log_marginal: 1698.2015519250185\n",
      "Train Epoch: 621 [512/54000 (1%)] Loss: -1919.537354\n",
      "Train Epoch: 621 [11776/54000 (22%)] Loss: -1677.438843\n",
      "Train Epoch: 621 [23040/54000 (43%)] Loss: -1477.718750\n",
      "Train Epoch: 621 [34304/54000 (64%)] Loss: -1526.098633\n",
      "Train Epoch: 621 [45568/54000 (84%)] Loss: -1631.533447\n",
      "    epoch          : 621\n",
      "    loss           : -1638.6335654683633\n",
      "    val_loss       : -1637.9529253811575\n",
      "    val_log_likelihood: 1741.9974487304687\n",
      "    val_log_marginal: 1705.7941364214282\n",
      "Train Epoch: 622 [512/54000 (1%)] Loss: -1891.732666\n",
      "Train Epoch: 622 [11776/54000 (22%)] Loss: -1622.161987\n",
      "Train Epoch: 622 [23040/54000 (43%)] Loss: -1593.958984\n",
      "Train Epoch: 622 [34304/54000 (64%)] Loss: -1704.215576\n",
      "Train Epoch: 622 [45568/54000 (84%)] Loss: -1606.702026\n",
      "    epoch          : 622\n",
      "    loss           : -1643.2217558303682\n",
      "    val_loss       : -1646.55725969309\n",
      "    val_log_likelihood: 1725.3875244140625\n",
      "    val_log_marginal: 1681.5479982312768\n",
      "Train Epoch: 623 [512/54000 (1%)] Loss: -1889.635376\n",
      "Train Epoch: 623 [11776/54000 (22%)] Loss: -1618.378418\n",
      "Train Epoch: 623 [23040/54000 (43%)] Loss: -1512.117676\n",
      "Train Epoch: 623 [34304/54000 (64%)] Loss: -1686.227295\n",
      "Train Epoch: 623 [45568/54000 (84%)] Loss: -1618.262695\n",
      "    epoch          : 623\n",
      "    loss           : -1645.6496787496133\n",
      "    val_loss       : -1643.0169356334022\n",
      "    val_log_likelihood: 1744.675244140625\n",
      "    val_log_marginal: 1704.3350327756257\n",
      "Train Epoch: 624 [512/54000 (1%)] Loss: -1904.014404\n",
      "Train Epoch: 624 [11776/54000 (22%)] Loss: -1549.850098\n",
      "Train Epoch: 624 [23040/54000 (43%)] Loss: -1704.814453\n",
      "Train Epoch: 624 [34304/54000 (64%)] Loss: -1643.832275\n",
      "Train Epoch: 624 [45568/54000 (84%)] Loss: -1628.304565\n",
      "    epoch          : 624\n",
      "    loss           : -1651.2493413037594\n",
      "    val_loss       : -1643.2473449942656\n",
      "    val_log_likelihood: 1720.5586059570312\n",
      "    val_log_marginal: 1669.1074365377426\n",
      "Train Epoch: 625 [512/54000 (1%)] Loss: -1917.834839\n",
      "Train Epoch: 625 [11776/54000 (22%)] Loss: -1562.564209\n",
      "Train Epoch: 625 [23040/54000 (43%)] Loss: -1509.632568\n",
      "Train Epoch: 625 [34304/54000 (64%)] Loss: -1587.682251\n",
      "Train Epoch: 625 [45568/54000 (84%)] Loss: -1641.578125\n",
      "    epoch          : 625\n",
      "    loss           : -1644.753144821318\n",
      "    val_loss       : -1626.8256998181344\n",
      "    val_log_likelihood: 1739.2536254882812\n",
      "    val_log_marginal: 1697.223654032126\n",
      "Train Epoch: 626 [512/54000 (1%)] Loss: -1605.055176\n",
      "Train Epoch: 626 [11776/54000 (22%)] Loss: -1628.016724\n",
      "Train Epoch: 626 [23040/54000 (43%)] Loss: -1482.888672\n",
      "Train Epoch: 626 [34304/54000 (64%)] Loss: -1582.302368\n",
      "Train Epoch: 626 [45568/54000 (84%)] Loss: -1646.496582\n",
      "    epoch          : 626\n",
      "    loss           : -1633.322943659112\n",
      "    val_loss       : -1650.4596403244882\n",
      "    val_log_likelihood: 1744.5791137695312\n",
      "    val_log_marginal: 1712.542774118483\n",
      "Train Epoch: 627 [512/54000 (1%)] Loss: -1713.872070\n",
      "Train Epoch: 627 [11776/54000 (22%)] Loss: -1684.357422\n",
      "Train Epoch: 627 [23040/54000 (43%)] Loss: -1557.710327\n",
      "Train Epoch: 627 [34304/54000 (64%)] Loss: -1670.920898\n",
      "Train Epoch: 627 [45568/54000 (84%)] Loss: -1685.869385\n",
      "    epoch          : 627\n",
      "    loss           : -1649.0853017674815\n",
      "    val_loss       : -1634.6423865589313\n",
      "    val_log_likelihood: 1715.105029296875\n",
      "    val_log_marginal: 1682.5448915291577\n",
      "Train Epoch: 628 [512/54000 (1%)] Loss: -1905.792603\n",
      "Train Epoch: 628 [11776/54000 (22%)] Loss: -1530.368408\n",
      "Train Epoch: 628 [23040/54000 (43%)] Loss: -1658.044189\n",
      "Train Epoch: 628 [34304/54000 (64%)] Loss: -1599.264526\n",
      "Train Epoch: 628 [45568/54000 (84%)] Loss: -1564.191040\n",
      "    epoch          : 628\n",
      "    loss           : -1624.3374216816212\n",
      "    val_loss       : -1615.3824269740842\n",
      "    val_log_likelihood: 1672.6310546875\n",
      "    val_log_marginal: 1636.218749523908\n",
      "Train Epoch: 629 [512/54000 (1%)] Loss: -1888.103882\n",
      "Train Epoch: 629 [11776/54000 (22%)] Loss: -1715.839478\n",
      "Train Epoch: 629 [23040/54000 (43%)] Loss: -1524.581055\n",
      "Train Epoch: 629 [34304/54000 (64%)] Loss: -1606.052246\n",
      "Train Epoch: 629 [45568/54000 (84%)] Loss: -1552.468750\n",
      "    epoch          : 629\n",
      "    loss           : -1622.2723714998453\n",
      "    val_loss       : -1636.913620928023\n",
      "    val_log_likelihood: 1695.9044921875\n",
      "    val_log_marginal: 1660.8200676977635\n",
      "Train Epoch: 630 [512/54000 (1%)] Loss: -1573.152222\n",
      "Train Epoch: 630 [11776/54000 (22%)] Loss: -1574.594482\n",
      "Train Epoch: 630 [23040/54000 (43%)] Loss: -1641.782959\n",
      "Train Epoch: 630 [34304/54000 (64%)] Loss: -1624.878296\n",
      "Train Epoch: 630 [45568/54000 (84%)] Loss: -1646.179443\n",
      "    epoch          : 630\n",
      "    loss           : -1646.470781685102\n",
      "    val_loss       : -1643.1003044009208\n",
      "    val_log_likelihood: 1712.8209228515625\n",
      "    val_log_marginal: 1682.343135471642\n",
      "Train Epoch: 631 [512/54000 (1%)] Loss: -1933.437012\n",
      "Train Epoch: 631 [11776/54000 (22%)] Loss: -1538.184937\n",
      "Train Epoch: 631 [23040/54000 (43%)] Loss: -1538.323242\n",
      "Train Epoch: 631 [34304/54000 (64%)] Loss: -1662.316772\n",
      "Train Epoch: 631 [45568/54000 (84%)] Loss: -1555.134033\n",
      "    epoch          : 631\n",
      "    loss           : -1642.7814880975402\n",
      "    val_loss       : -1635.9028786168433\n",
      "    val_log_likelihood: 1709.5738037109375\n",
      "    val_log_marginal: 1676.9681992124765\n",
      "Train Epoch: 632 [512/54000 (1%)] Loss: -1721.864624\n",
      "Train Epoch: 632 [11776/54000 (22%)] Loss: -1566.902344\n",
      "Train Epoch: 632 [23040/54000 (43%)] Loss: -1675.472656\n",
      "Train Epoch: 632 [34304/54000 (64%)] Loss: -1900.797119\n",
      "Train Epoch: 632 [45568/54000 (84%)] Loss: -1525.289795\n",
      "    epoch          : 632\n",
      "    loss           : -1634.0881118019029\n",
      "    val_loss       : -1641.4077908962965\n",
      "    val_log_likelihood: 1701.9602294921874\n",
      "    val_log_marginal: 1658.718636767912\n",
      "Train Epoch: 633 [512/54000 (1%)] Loss: -1896.143188\n",
      "Train Epoch: 633 [11776/54000 (22%)] Loss: -1563.358398\n",
      "Train Epoch: 633 [23040/54000 (43%)] Loss: -1542.770386\n",
      "Train Epoch: 633 [34304/54000 (64%)] Loss: -1900.219360\n",
      "Train Epoch: 633 [45568/54000 (84%)] Loss: -1561.964355\n",
      "    epoch          : 633\n",
      "    loss           : -1643.9937780399134\n",
      "    val_loss       : -1642.0989682135173\n",
      "    val_log_likelihood: 1722.5825561523438\n",
      "    val_log_marginal: 1670.2602374922485\n",
      "Train Epoch: 634 [512/54000 (1%)] Loss: -1911.148193\n",
      "Train Epoch: 634 [11776/54000 (22%)] Loss: -1555.798828\n",
      "Train Epoch: 634 [23040/54000 (43%)] Loss: -1540.859375\n",
      "Train Epoch: 634 [34304/54000 (64%)] Loss: -1558.038452\n",
      "Train Epoch: 634 [45568/54000 (84%)] Loss: -1592.643066\n",
      "    epoch          : 634\n",
      "    loss           : -1647.5737498066212\n",
      "    val_loss       : -1646.7128094956279\n",
      "    val_log_likelihood: 1732.8080932617188\n",
      "    val_log_marginal: 1687.4187622252853\n",
      "Train Epoch: 635 [512/54000 (1%)] Loss: -1900.428589\n",
      "Train Epoch: 635 [11776/54000 (22%)] Loss: -1580.789062\n",
      "Train Epoch: 635 [23040/54000 (43%)] Loss: -1679.904175\n",
      "Train Epoch: 635 [34304/54000 (64%)] Loss: -1920.313965\n",
      "Train Epoch: 635 [45568/54000 (84%)] Loss: -1568.425049\n",
      "    epoch          : 635\n",
      "    loss           : -1640.8768117168163\n",
      "    val_loss       : -1630.4546067914926\n",
      "    val_log_likelihood: 1742.0796875\n",
      "    val_log_marginal: 1709.9207615219057\n",
      "Train Epoch: 636 [512/54000 (1%)] Loss: -1588.341553\n",
      "Train Epoch: 636 [11776/54000 (22%)] Loss: -1600.319092\n",
      "Train Epoch: 636 [23040/54000 (43%)] Loss: -1508.831787\n",
      "Train Epoch: 636 [34304/54000 (64%)] Loss: -1928.006958\n",
      "Train Epoch: 636 [45568/54000 (84%)] Loss: -1666.020142\n",
      "    epoch          : 636\n",
      "    loss           : -1645.513889426052\n",
      "    val_loss       : -1653.4776311076246\n",
      "    val_log_likelihood: 1712.3236328125\n",
      "    val_log_marginal: 1675.281966196373\n",
      "Train Epoch: 637 [512/54000 (1%)] Loss: -1922.775635\n",
      "Train Epoch: 637 [11776/54000 (22%)] Loss: -1781.371826\n",
      "Train Epoch: 637 [23040/54000 (43%)] Loss: -1677.788086\n",
      "Train Epoch: 637 [34304/54000 (64%)] Loss: -1668.323364\n",
      "Train Epoch: 637 [45568/54000 (84%)] Loss: -1634.091553\n",
      "    epoch          : 637\n",
      "    loss           : -1642.2417922822556\n",
      "    val_loss       : -1626.9373832412996\n",
      "    val_log_likelihood: 1704.9170654296875\n",
      "    val_log_marginal: 1657.3037514496596\n",
      "Train Epoch: 638 [512/54000 (1%)] Loss: -1919.789795\n",
      "Train Epoch: 638 [11776/54000 (22%)] Loss: -1554.423096\n",
      "Train Epoch: 638 [23040/54000 (43%)] Loss: -1514.302002\n",
      "Train Epoch: 638 [34304/54000 (64%)] Loss: -1586.430176\n",
      "Train Epoch: 638 [45568/54000 (84%)] Loss: -1538.349854\n",
      "    epoch          : 638\n",
      "    loss           : -1641.450980913521\n",
      "    val_loss       : -1630.8068801547402\n",
      "    val_log_likelihood: 1725.0905883789062\n",
      "    val_log_marginal: 1688.815630769357\n",
      "Train Epoch: 639 [512/54000 (1%)] Loss: -1894.624023\n",
      "Train Epoch: 639 [11776/54000 (22%)] Loss: -1558.704590\n",
      "Train Epoch: 639 [23040/54000 (43%)] Loss: -1530.029053\n",
      "Train Epoch: 639 [34304/54000 (64%)] Loss: -1563.345215\n",
      "Train Epoch: 639 [45568/54000 (84%)] Loss: -1648.174561\n",
      "    epoch          : 639\n",
      "    loss           : -1647.7504048866801\n",
      "    val_loss       : -1640.697346319724\n",
      "    val_log_likelihood: 1731.5332275390624\n",
      "    val_log_marginal: 1700.8297884825618\n",
      "Train Epoch: 640 [512/54000 (1%)] Loss: -1919.805908\n",
      "Train Epoch: 640 [11776/54000 (22%)] Loss: -1529.566895\n",
      "Train Epoch: 640 [23040/54000 (43%)] Loss: -1539.863037\n",
      "Train Epoch: 640 [34304/54000 (64%)] Loss: -1886.005859\n",
      "Train Epoch: 640 [45568/54000 (84%)] Loss: -1633.238037\n",
      "    epoch          : 640\n",
      "    loss           : -1644.8778644221843\n",
      "    val_loss       : -1639.0361600192264\n",
      "    val_log_likelihood: 1711.9600830078125\n",
      "    val_log_marginal: 1673.5359930429609\n",
      "Train Epoch: 641 [512/54000 (1%)] Loss: -1620.083496\n",
      "Train Epoch: 641 [11776/54000 (22%)] Loss: -1558.753418\n",
      "Train Epoch: 641 [23040/54000 (43%)] Loss: -1572.317627\n",
      "Train Epoch: 641 [34304/54000 (64%)] Loss: -1521.286743\n",
      "Train Epoch: 641 [45568/54000 (84%)] Loss: -1599.615723\n",
      "    epoch          : 641\n",
      "    loss           : -1638.1261276396194\n",
      "    val_loss       : -1643.7923495707103\n",
      "    val_log_likelihood: 1724.1193481445312\n",
      "    val_log_marginal: 1678.0032040927558\n",
      "Train Epoch: 642 [512/54000 (1%)] Loss: -1761.537354\n",
      "Train Epoch: 642 [11776/54000 (22%)] Loss: -1684.760742\n",
      "Train Epoch: 642 [23040/54000 (43%)] Loss: -1510.584229\n",
      "Train Epoch: 642 [34304/54000 (64%)] Loss: -1705.855591\n",
      "Train Epoch: 642 [45568/54000 (84%)] Loss: -1635.524658\n",
      "    epoch          : 642\n",
      "    loss           : -1635.5363745358911\n",
      "    val_loss       : -1633.742641179543\n",
      "    val_log_likelihood: 1725.38291015625\n",
      "    val_log_marginal: 1685.705971691385\n",
      "Train Epoch: 643 [512/54000 (1%)] Loss: -1910.586426\n",
      "Train Epoch: 643 [11776/54000 (22%)] Loss: -1567.003662\n",
      "Train Epoch: 643 [23040/54000 (43%)] Loss: -1513.510742\n",
      "Train Epoch: 643 [34304/54000 (64%)] Loss: -1497.115234\n",
      "Train Epoch: 643 [45568/54000 (84%)] Loss: -1668.740234\n",
      "    epoch          : 643\n",
      "    loss           : -1637.2037184309252\n",
      "    val_loss       : -1624.5877469520085\n",
      "    val_log_likelihood: 1730.597607421875\n",
      "    val_log_marginal: 1694.369704977423\n",
      "Train Epoch: 644 [512/54000 (1%)] Loss: -1907.712402\n",
      "Train Epoch: 644 [11776/54000 (22%)] Loss: -1609.183350\n",
      "Train Epoch: 644 [23040/54000 (43%)] Loss: -1633.478271\n",
      "Train Epoch: 644 [34304/54000 (64%)] Loss: -1932.107178\n",
      "Train Epoch: 644 [45568/54000 (84%)] Loss: -1659.600830\n",
      "    epoch          : 644\n",
      "    loss           : -1646.8487754293008\n",
      "    val_loss       : -1647.3080571414903\n",
      "    val_log_likelihood: 1743.004638671875\n",
      "    val_log_marginal: 1702.1049347441644\n",
      "Train Epoch: 645 [512/54000 (1%)] Loss: -1913.315430\n",
      "Train Epoch: 645 [11776/54000 (22%)] Loss: -1562.167480\n",
      "Train Epoch: 645 [23040/54000 (43%)] Loss: -1516.769897\n",
      "Train Epoch: 645 [34304/54000 (64%)] Loss: -1906.361206\n",
      "Train Epoch: 645 [45568/54000 (84%)] Loss: -1575.850830\n",
      "    epoch          : 645\n",
      "    loss           : -1646.0152769183169\n",
      "    val_loss       : -1630.3692380731925\n",
      "    val_log_likelihood: 1729.8201293945312\n",
      "    val_log_marginal: 1686.3564284499735\n",
      "Train Epoch: 646 [512/54000 (1%)] Loss: -1749.219238\n",
      "Train Epoch: 646 [11776/54000 (22%)] Loss: -1578.996216\n",
      "Train Epoch: 646 [23040/54000 (43%)] Loss: -1533.805542\n",
      "Train Epoch: 646 [34304/54000 (64%)] Loss: -1588.153809\n",
      "Train Epoch: 646 [45568/54000 (84%)] Loss: -1633.564453\n",
      "    epoch          : 646\n",
      "    loss           : -1640.6821083597617\n",
      "    val_loss       : -1638.9321184251457\n",
      "    val_log_likelihood: 1744.2291381835937\n",
      "    val_log_marginal: 1709.1931668445468\n",
      "Train Epoch: 647 [512/54000 (1%)] Loss: -1738.380615\n",
      "Train Epoch: 647 [11776/54000 (22%)] Loss: -1604.919312\n",
      "Train Epoch: 647 [23040/54000 (43%)] Loss: -1527.456055\n",
      "Train Epoch: 647 [34304/54000 (64%)] Loss: -1711.625732\n",
      "Train Epoch: 647 [45568/54000 (84%)] Loss: -1586.628052\n",
      "    epoch          : 647\n",
      "    loss           : -1647.231256768255\n",
      "    val_loss       : -1649.4451254572718\n",
      "    val_log_likelihood: 1734.4984497070313\n",
      "    val_log_marginal: 1697.8253836371005\n",
      "Train Epoch: 648 [512/54000 (1%)] Loss: -1875.421143\n",
      "Train Epoch: 648 [11776/54000 (22%)] Loss: -1595.937866\n",
      "Train Epoch: 648 [23040/54000 (43%)] Loss: -1695.961670\n",
      "Train Epoch: 648 [34304/54000 (64%)] Loss: -1712.620483\n",
      "Train Epoch: 648 [45568/54000 (84%)] Loss: -1568.578125\n",
      "    epoch          : 648\n",
      "    loss           : -1651.0699378287438\n",
      "    val_loss       : -1653.2377071807161\n",
      "    val_log_likelihood: 1726.09375\n",
      "    val_log_marginal: 1682.881636148691\n",
      "Train Epoch: 649 [512/54000 (1%)] Loss: -1601.700806\n",
      "Train Epoch: 649 [11776/54000 (22%)] Loss: -1593.159424\n",
      "Train Epoch: 649 [23040/54000 (43%)] Loss: -1522.563477\n",
      "Train Epoch: 649 [34304/54000 (64%)] Loss: -1648.170288\n",
      "Train Epoch: 649 [45568/54000 (84%)] Loss: -1648.442749\n",
      "    epoch          : 649\n",
      "    loss           : -1648.7689390276919\n",
      "    val_loss       : -1635.4958563197404\n",
      "    val_log_likelihood: 1744.0509643554688\n",
      "    val_log_marginal: 1698.4505954071878\n",
      "Train Epoch: 650 [512/54000 (1%)] Loss: -1620.738525\n",
      "Train Epoch: 650 [11776/54000 (22%)] Loss: -1614.201050\n",
      "Train Epoch: 650 [23040/54000 (43%)] Loss: -1539.109375\n",
      "Train Epoch: 650 [34304/54000 (64%)] Loss: -1908.155518\n",
      "Train Epoch: 650 [45568/54000 (84%)] Loss: -1503.705566\n",
      "    epoch          : 650\n",
      "    loss           : -1643.4892783589883\n",
      "    val_loss       : -1645.423418956995\n",
      "    val_log_likelihood: 1750.5922729492188\n",
      "    val_log_marginal: 1717.2457369636782\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch650.pth ...\n",
      "Train Epoch: 651 [512/54000 (1%)] Loss: -1910.586426\n",
      "Train Epoch: 651 [11776/54000 (22%)] Loss: -1637.733887\n",
      "Train Epoch: 651 [23040/54000 (43%)] Loss: -1641.016113\n",
      "Train Epoch: 651 [34304/54000 (64%)] Loss: -1678.567017\n",
      "Train Epoch: 651 [45568/54000 (84%)] Loss: -1666.790771\n",
      "    epoch          : 651\n",
      "    loss           : -1653.0227751779084\n",
      "    val_loss       : -1651.9908676127902\n",
      "    val_log_likelihood: 1739.4484252929688\n",
      "    val_log_marginal: 1696.3816848199815\n",
      "Train Epoch: 652 [512/54000 (1%)] Loss: -1927.552490\n",
      "Train Epoch: 652 [11776/54000 (22%)] Loss: -1588.058838\n",
      "Train Epoch: 652 [23040/54000 (43%)] Loss: -1550.090942\n",
      "Train Epoch: 652 [34304/54000 (64%)] Loss: -1521.713379\n",
      "Train Epoch: 652 [45568/54000 (84%)] Loss: -1631.844482\n",
      "    epoch          : 652\n",
      "    loss           : -1659.5759458636294\n",
      "    val_loss       : -1649.7695124601946\n",
      "    val_log_likelihood: 1746.4402587890625\n",
      "    val_log_marginal: 1708.9512443646788\n",
      "Train Epoch: 653 [512/54000 (1%)] Loss: -1919.517334\n",
      "Train Epoch: 653 [11776/54000 (22%)] Loss: -1711.975342\n",
      "Train Epoch: 653 [23040/54000 (43%)] Loss: -1515.999512\n",
      "Train Epoch: 653 [34304/54000 (64%)] Loss: -1920.014160\n",
      "Train Epoch: 653 [45568/54000 (84%)] Loss: -1648.118408\n",
      "    epoch          : 653\n",
      "    loss           : -1650.7156220993193\n",
      "    val_loss       : -1637.9182803299277\n",
      "    val_log_likelihood: 1746.8169921875\n",
      "    val_log_marginal: 1703.6947005424656\n",
      "Train Epoch: 654 [512/54000 (1%)] Loss: -1916.110229\n",
      "Train Epoch: 654 [11776/54000 (22%)] Loss: -1726.307251\n",
      "Train Epoch: 654 [23040/54000 (43%)] Loss: -1671.917114\n",
      "Train Epoch: 654 [34304/54000 (64%)] Loss: -1904.255005\n",
      "Train Epoch: 654 [45568/54000 (84%)] Loss: -1652.651367\n",
      "    epoch          : 654\n",
      "    loss           : -1648.2807955600247\n",
      "    val_loss       : -1645.288419245649\n",
      "    val_log_likelihood: 1725.137451171875\n",
      "    val_log_marginal: 1692.1457156255842\n",
      "Train Epoch: 655 [512/54000 (1%)] Loss: -1920.200195\n",
      "Train Epoch: 655 [11776/54000 (22%)] Loss: -1706.049072\n",
      "Train Epoch: 655 [23040/54000 (43%)] Loss: -1674.659302\n",
      "Train Epoch: 655 [34304/54000 (64%)] Loss: -1921.698486\n",
      "Train Epoch: 655 [45568/54000 (84%)] Loss: -1549.700684\n",
      "    epoch          : 655\n",
      "    loss           : -1645.201087271813\n",
      "    val_loss       : -1647.810078809131\n",
      "    val_log_likelihood: 1746.3540893554687\n",
      "    val_log_marginal: 1688.947172125429\n",
      "Train Epoch: 656 [512/54000 (1%)] Loss: -1930.387573\n",
      "Train Epoch: 656 [11776/54000 (22%)] Loss: -1586.068848\n",
      "Train Epoch: 656 [23040/54000 (43%)] Loss: -1691.872681\n",
      "Train Epoch: 656 [34304/54000 (64%)] Loss: -1604.232544\n",
      "Train Epoch: 656 [45568/54000 (84%)] Loss: -1683.700195\n",
      "    epoch          : 656\n",
      "    loss           : -1654.2890999671256\n",
      "    val_loss       : -1656.7126210148447\n",
      "    val_log_likelihood: 1728.7831176757813\n",
      "    val_log_marginal: 1681.5313097693027\n",
      "Train Epoch: 657 [512/54000 (1%)] Loss: -1901.836182\n",
      "Train Epoch: 657 [11776/54000 (22%)] Loss: -1574.866089\n",
      "Train Epoch: 657 [23040/54000 (43%)] Loss: -1550.500610\n",
      "Train Epoch: 657 [34304/54000 (64%)] Loss: -1599.257568\n",
      "Train Epoch: 657 [45568/54000 (84%)] Loss: -1621.881714\n",
      "    epoch          : 657\n",
      "    loss           : -1631.7019611018718\n",
      "    val_loss       : -1624.8854921429418\n",
      "    val_log_likelihood: 1707.9303344726563\n",
      "    val_log_marginal: 1672.9288268949836\n",
      "Train Epoch: 658 [512/54000 (1%)] Loss: -1646.556885\n",
      "Train Epoch: 658 [11776/54000 (22%)] Loss: -1599.367432\n",
      "Train Epoch: 658 [23040/54000 (43%)] Loss: -1602.788696\n",
      "Train Epoch: 658 [34304/54000 (64%)] Loss: -1908.470215\n",
      "Train Epoch: 658 [45568/54000 (84%)] Loss: -1621.330566\n",
      "    epoch          : 658\n",
      "    loss           : -1637.7529792407952\n",
      "    val_loss       : -1633.4992633935994\n",
      "    val_log_likelihood: 1724.8533325195312\n",
      "    val_log_marginal: 1684.995946438238\n",
      "Train Epoch: 659 [512/54000 (1%)] Loss: -1900.283203\n",
      "Train Epoch: 659 [11776/54000 (22%)] Loss: -1718.587280\n",
      "Train Epoch: 659 [23040/54000 (43%)] Loss: -1543.707275\n",
      "Train Epoch: 659 [34304/54000 (64%)] Loss: -1902.564209\n",
      "Train Epoch: 659 [45568/54000 (84%)] Loss: -1645.293823\n",
      "    epoch          : 659\n",
      "    loss           : -1647.3569795211943\n",
      "    val_loss       : -1643.4083866458386\n",
      "    val_log_likelihood: 1730.695751953125\n",
      "    val_log_marginal: 1695.8423061262815\n",
      "Train Epoch: 660 [512/54000 (1%)] Loss: -1578.055908\n",
      "Train Epoch: 660 [11776/54000 (22%)] Loss: -1576.435303\n",
      "Train Epoch: 660 [23040/54000 (43%)] Loss: -1529.455322\n",
      "Train Epoch: 660 [34304/54000 (64%)] Loss: -1819.845947\n",
      "Train Epoch: 660 [45568/54000 (84%)] Loss: -1612.707764\n",
      "    epoch          : 660\n",
      "    loss           : -1628.939317759901\n",
      "    val_loss       : -1628.1102157849818\n",
      "    val_log_likelihood: 1694.2248901367188\n",
      "    val_log_marginal: 1650.8784949470312\n",
      "Train Epoch: 661 [512/54000 (1%)] Loss: -1903.744629\n",
      "Train Epoch: 661 [11776/54000 (22%)] Loss: -1721.366455\n",
      "Train Epoch: 661 [23040/54000 (43%)] Loss: -1559.134644\n",
      "Train Epoch: 661 [34304/54000 (64%)] Loss: -1624.536987\n",
      "Train Epoch: 661 [45568/54000 (84%)] Loss: -1545.159790\n",
      "    epoch          : 661\n",
      "    loss           : -1642.2311878770886\n",
      "    val_loss       : -1640.2708457163535\n",
      "    val_log_likelihood: 1697.9311645507812\n",
      "    val_log_marginal: 1648.7009218648077\n",
      "Train Epoch: 662 [512/54000 (1%)] Loss: -1876.308716\n",
      "Train Epoch: 662 [11776/54000 (22%)] Loss: -1721.110229\n",
      "Train Epoch: 662 [23040/54000 (43%)] Loss: -1539.540039\n",
      "Train Epoch: 662 [34304/54000 (64%)] Loss: -1605.691406\n",
      "Train Epoch: 662 [45568/54000 (84%)] Loss: -1625.490845\n",
      "    epoch          : 662\n",
      "    loss           : -1637.9398471341274\n",
      "    val_loss       : -1642.426773698628\n",
      "    val_log_likelihood: 1709.3253295898437\n",
      "    val_log_marginal: 1677.598764980957\n",
      "Train Epoch: 663 [512/54000 (1%)] Loss: -1921.073242\n",
      "Train Epoch: 663 [11776/54000 (22%)] Loss: -1607.846436\n",
      "Train Epoch: 663 [23040/54000 (43%)] Loss: -1616.615356\n",
      "Train Epoch: 663 [34304/54000 (64%)] Loss: -1629.347168\n",
      "Train Epoch: 663 [45568/54000 (84%)] Loss: -1536.185181\n",
      "    epoch          : 663\n",
      "    loss           : -1645.9900832978806\n",
      "    val_loss       : -1634.6395687473937\n",
      "    val_log_likelihood: 1715.16748046875\n",
      "    val_log_marginal: 1669.0469167541712\n",
      "Train Epoch: 664 [512/54000 (1%)] Loss: -1912.080566\n",
      "Train Epoch: 664 [11776/54000 (22%)] Loss: -1541.463135\n",
      "Train Epoch: 664 [23040/54000 (43%)] Loss: -1532.937500\n",
      "Train Epoch: 664 [34304/54000 (64%)] Loss: -1558.351074\n",
      "Train Epoch: 664 [45568/54000 (84%)] Loss: -1632.112061\n",
      "    epoch          : 664\n",
      "    loss           : -1647.3509618173732\n",
      "    val_loss       : -1633.1841759943404\n",
      "    val_log_likelihood: 1720.763134765625\n",
      "    val_log_marginal: 1686.218757424876\n",
      "Train Epoch: 665 [512/54000 (1%)] Loss: -1932.230835\n",
      "Train Epoch: 665 [11776/54000 (22%)] Loss: -1681.963501\n",
      "Train Epoch: 665 [23040/54000 (43%)] Loss: -1482.332886\n",
      "Train Epoch: 665 [34304/54000 (64%)] Loss: -1893.613525\n",
      "Train Epoch: 665 [45568/54000 (84%)] Loss: -1633.354736\n",
      "    epoch          : 665\n",
      "    loss           : -1647.9460364615563\n",
      "    val_loss       : -1640.8464925193227\n",
      "    val_log_likelihood: 1708.8996215820312\n",
      "    val_log_marginal: 1670.6047903811966\n",
      "Train Epoch: 666 [512/54000 (1%)] Loss: -1916.621582\n",
      "Train Epoch: 666 [11776/54000 (22%)] Loss: -1520.390991\n",
      "Train Epoch: 666 [23040/54000 (43%)] Loss: -1638.242310\n",
      "Train Epoch: 666 [34304/54000 (64%)] Loss: -1647.025391\n",
      "Train Epoch: 666 [45568/54000 (84%)] Loss: -1589.011475\n",
      "    epoch          : 666\n",
      "    loss           : -1643.5515040029393\n",
      "    val_loss       : -1644.1971059740522\n",
      "    val_log_likelihood: 1728.2279663085938\n",
      "    val_log_marginal: 1687.58953801468\n",
      "Train Epoch: 667 [512/54000 (1%)] Loss: -1901.043457\n",
      "Train Epoch: 667 [11776/54000 (22%)] Loss: -1731.577148\n",
      "Train Epoch: 667 [23040/54000 (43%)] Loss: -1543.985352\n",
      "Train Epoch: 667 [34304/54000 (64%)] Loss: -1556.018188\n",
      "Train Epoch: 667 [45568/54000 (84%)] Loss: -1632.145142\n",
      "    epoch          : 667\n",
      "    loss           : -1649.5851591506807\n",
      "    val_loss       : -1651.5138024047017\n",
      "    val_log_likelihood: 1740.8236328125\n",
      "    val_log_marginal: 1706.2284105662256\n",
      "Train Epoch: 668 [512/54000 (1%)] Loss: -1728.615479\n",
      "Train Epoch: 668 [11776/54000 (22%)] Loss: -1560.407715\n",
      "Train Epoch: 668 [23040/54000 (43%)] Loss: -1636.715698\n",
      "Train Epoch: 668 [34304/54000 (64%)] Loss: -1640.747070\n",
      "Train Epoch: 668 [45568/54000 (84%)] Loss: -1566.517334\n",
      "    epoch          : 668\n",
      "    loss           : -1655.888564308091\n",
      "    val_loss       : -1646.2601668602788\n",
      "    val_log_likelihood: 1725.625146484375\n",
      "    val_log_marginal: 1685.7909134790302\n",
      "Train Epoch: 669 [512/54000 (1%)] Loss: -1925.243652\n",
      "Train Epoch: 669 [11776/54000 (22%)] Loss: -1599.180908\n",
      "Train Epoch: 669 [23040/54000 (43%)] Loss: -1537.176270\n",
      "Train Epoch: 669 [34304/54000 (64%)] Loss: -1914.418335\n",
      "Train Epoch: 669 [45568/54000 (84%)] Loss: -1628.029541\n",
      "    epoch          : 669\n",
      "    loss           : -1652.2882817334469\n",
      "    val_loss       : -1639.7959203199484\n",
      "    val_log_likelihood: 1725.15400390625\n",
      "    val_log_marginal: 1674.997071011737\n",
      "Train Epoch: 670 [512/54000 (1%)] Loss: -1908.725952\n",
      "Train Epoch: 670 [11776/54000 (22%)] Loss: -1723.976929\n",
      "Train Epoch: 670 [23040/54000 (43%)] Loss: -1559.056274\n",
      "Train Epoch: 670 [34304/54000 (64%)] Loss: -1562.602295\n",
      "Train Epoch: 670 [45568/54000 (84%)] Loss: -1530.343750\n",
      "    epoch          : 670\n",
      "    loss           : -1649.8743823967357\n",
      "    val_loss       : -1654.2128389002755\n",
      "    val_log_likelihood: 1738.4612426757812\n",
      "    val_log_marginal: 1694.0073881093413\n",
      "Train Epoch: 671 [512/54000 (1%)] Loss: -1906.510376\n",
      "Train Epoch: 671 [11776/54000 (22%)] Loss: -1581.624756\n",
      "Train Epoch: 671 [23040/54000 (43%)] Loss: -1498.972412\n",
      "Train Epoch: 671 [34304/54000 (64%)] Loss: -1548.509033\n",
      "Train Epoch: 671 [45568/54000 (84%)] Loss: -1527.833130\n",
      "    epoch          : 671\n",
      "    loss           : -1647.4373730952198\n",
      "    val_loss       : -1642.9203900705093\n",
      "    val_log_likelihood: 1716.4901733398438\n",
      "    val_log_marginal: 1674.397627488151\n",
      "Train Epoch: 672 [512/54000 (1%)] Loss: -1909.938599\n",
      "Train Epoch: 672 [11776/54000 (22%)] Loss: -1556.059692\n",
      "Train Epoch: 672 [23040/54000 (43%)] Loss: -1654.728027\n",
      "Train Epoch: 672 [34304/54000 (64%)] Loss: -1712.288452\n",
      "Train Epoch: 672 [45568/54000 (84%)] Loss: -1613.626343\n",
      "    epoch          : 672\n",
      "    loss           : -1632.5062292117884\n",
      "    val_loss       : -1634.8833313373848\n",
      "    val_log_likelihood: 1720.483837890625\n",
      "    val_log_marginal: 1677.0185353022068\n",
      "Train Epoch: 673 [512/54000 (1%)] Loss: -1940.146362\n",
      "Train Epoch: 673 [11776/54000 (22%)] Loss: -1599.934692\n",
      "Train Epoch: 673 [23040/54000 (43%)] Loss: -1685.283081\n",
      "Train Epoch: 673 [34304/54000 (64%)] Loss: -1540.634155\n",
      "Train Epoch: 673 [45568/54000 (84%)] Loss: -1531.193726\n",
      "    epoch          : 673\n",
      "    loss           : -1643.6365447091584\n",
      "    val_loss       : -1618.8384036091156\n",
      "    val_log_likelihood: 1721.4235107421875\n",
      "    val_log_marginal: 1683.8664607457818\n",
      "Train Epoch: 674 [512/54000 (1%)] Loss: -1926.128418\n",
      "Train Epoch: 674 [11776/54000 (22%)] Loss: -1749.739380\n",
      "Train Epoch: 674 [23040/54000 (43%)] Loss: -1564.718750\n",
      "Train Epoch: 674 [34304/54000 (64%)] Loss: -1569.941406\n",
      "Train Epoch: 674 [45568/54000 (84%)] Loss: -1628.022705\n",
      "    epoch          : 674\n",
      "    loss           : -1646.0218493773205\n",
      "    val_loss       : -1646.7483384303748\n",
      "    val_log_likelihood: 1719.2217163085938\n",
      "    val_log_marginal: 1676.8883484050632\n",
      "Train Epoch: 675 [512/54000 (1%)] Loss: -1911.241699\n",
      "Train Epoch: 675 [11776/54000 (22%)] Loss: -1548.557373\n",
      "Train Epoch: 675 [23040/54000 (43%)] Loss: -1627.675293\n",
      "Train Epoch: 675 [34304/54000 (64%)] Loss: -1921.395508\n",
      "Train Epoch: 675 [45568/54000 (84%)] Loss: -1523.951172\n",
      "    epoch          : 675\n",
      "    loss           : -1645.9309734684407\n",
      "    val_loss       : -1642.9489403762855\n",
      "    val_log_likelihood: 1717.2978759765624\n",
      "    val_log_marginal: 1671.4313258063048\n",
      "Train Epoch: 676 [512/54000 (1%)] Loss: -1918.882935\n",
      "Train Epoch: 676 [11776/54000 (22%)] Loss: -1616.498291\n",
      "Train Epoch: 676 [23040/54000 (43%)] Loss: -1677.076904\n",
      "Train Epoch: 676 [34304/54000 (64%)] Loss: -1619.948242\n",
      "Train Epoch: 676 [45568/54000 (84%)] Loss: -1619.248779\n",
      "    epoch          : 676\n",
      "    loss           : -1648.652858620823\n",
      "    val_loss       : -1649.52852447927\n",
      "    val_log_likelihood: 1744.4500122070312\n",
      "    val_log_marginal: 1708.587195782125\n",
      "Train Epoch: 677 [512/54000 (1%)] Loss: -1881.590942\n",
      "Train Epoch: 677 [11776/54000 (22%)] Loss: -1591.489502\n",
      "Train Epoch: 677 [23040/54000 (43%)] Loss: -1558.985718\n",
      "Train Epoch: 677 [34304/54000 (64%)] Loss: -1643.624756\n",
      "Train Epoch: 677 [45568/54000 (84%)] Loss: -1537.499878\n",
      "    epoch          : 677\n",
      "    loss           : -1643.3529862507735\n",
      "    val_loss       : -1648.975303748995\n",
      "    val_log_likelihood: 1705.9067626953124\n",
      "    val_log_marginal: 1670.702303534746\n",
      "Train Epoch: 678 [512/54000 (1%)] Loss: -1729.765137\n",
      "Train Epoch: 678 [11776/54000 (22%)] Loss: -1570.373047\n",
      "Train Epoch: 678 [23040/54000 (43%)] Loss: -1588.883911\n",
      "Train Epoch: 678 [34304/54000 (64%)] Loss: -1671.639648\n",
      "Train Epoch: 678 [45568/54000 (84%)] Loss: -1643.743652\n",
      "    epoch          : 678\n",
      "    loss           : -1645.6911754041614\n",
      "    val_loss       : -1651.6597117221913\n",
      "    val_log_likelihood: 1732.9892456054688\n",
      "    val_log_marginal: 1689.4209128338844\n",
      "Train Epoch: 679 [512/54000 (1%)] Loss: -1715.577881\n",
      "Train Epoch: 679 [11776/54000 (22%)] Loss: -1563.951660\n",
      "Train Epoch: 679 [23040/54000 (43%)] Loss: -1592.478638\n",
      "Train Epoch: 679 [34304/54000 (64%)] Loss: -1680.071533\n",
      "Train Epoch: 679 [45568/54000 (84%)] Loss: -1640.439209\n",
      "    epoch          : 679\n",
      "    loss           : -1654.1026115795173\n",
      "    val_loss       : -1650.0826124327257\n",
      "    val_log_likelihood: 1732.3226684570313\n",
      "    val_log_marginal: 1691.2934658039362\n",
      "Train Epoch: 680 [512/54000 (1%)] Loss: -1590.922852\n",
      "Train Epoch: 680 [11776/54000 (22%)] Loss: -1580.142822\n",
      "Train Epoch: 680 [23040/54000 (43%)] Loss: -1594.476929\n",
      "Train Epoch: 680 [34304/54000 (64%)] Loss: -1553.813354\n",
      "Train Epoch: 680 [45568/54000 (84%)] Loss: -1624.380493\n",
      "    epoch          : 680\n",
      "    loss           : -1652.3346757038985\n",
      "    val_loss       : -1651.8463187431917\n",
      "    val_log_likelihood: 1720.732470703125\n",
      "    val_log_marginal: 1681.5450944922864\n",
      "Train Epoch: 681 [512/54000 (1%)] Loss: -1577.887939\n",
      "Train Epoch: 681 [11776/54000 (22%)] Loss: -1530.910278\n",
      "Train Epoch: 681 [23040/54000 (43%)] Loss: -1619.762329\n",
      "Train Epoch: 681 [34304/54000 (64%)] Loss: -1551.802979\n",
      "Train Epoch: 681 [45568/54000 (84%)] Loss: -1639.563721\n",
      "    epoch          : 681\n",
      "    loss           : -1653.2171860496596\n",
      "    val_loss       : -1656.27837275872\n",
      "    val_log_likelihood: 1732.5777099609375\n",
      "    val_log_marginal: 1694.0467851955443\n",
      "Train Epoch: 682 [512/54000 (1%)] Loss: -1935.302734\n",
      "Train Epoch: 682 [11776/54000 (22%)] Loss: -1635.651489\n",
      "Train Epoch: 682 [23040/54000 (43%)] Loss: -1687.236572\n",
      "Train Epoch: 682 [34304/54000 (64%)] Loss: -1552.536499\n",
      "Train Epoch: 682 [45568/54000 (84%)] Loss: -1539.312988\n",
      "    epoch          : 682\n",
      "    loss           : -1644.5546210260675\n",
      "    val_loss       : -1636.992647992447\n",
      "    val_log_likelihood: 1723.97470703125\n",
      "    val_log_marginal: 1684.8165042895987\n",
      "Train Epoch: 683 [512/54000 (1%)] Loss: -1947.984619\n",
      "Train Epoch: 683 [11776/54000 (22%)] Loss: -1564.364258\n",
      "Train Epoch: 683 [23040/54000 (43%)] Loss: -1502.759766\n",
      "Train Epoch: 683 [34304/54000 (64%)] Loss: -1923.815918\n",
      "Train Epoch: 683 [45568/54000 (84%)] Loss: -1617.960449\n",
      "    epoch          : 683\n",
      "    loss           : -1644.2955624419865\n",
      "    val_loss       : -1641.9264403841457\n",
      "    val_log_likelihood: 1734.2685424804688\n",
      "    val_log_marginal: 1688.0929881643503\n",
      "Train Epoch: 684 [512/54000 (1%)] Loss: -1918.253052\n",
      "Train Epoch: 684 [11776/54000 (22%)] Loss: -1580.580566\n",
      "Train Epoch: 684 [23040/54000 (43%)] Loss: -1672.935791\n",
      "Train Epoch: 684 [34304/54000 (64%)] Loss: -1650.703613\n",
      "Train Epoch: 684 [45568/54000 (84%)] Loss: -1661.599365\n",
      "    epoch          : 684\n",
      "    loss           : -1655.3303959912594\n",
      "    val_loss       : -1637.5322510718368\n",
      "    val_log_likelihood: 1747.6159423828126\n",
      "    val_log_marginal: 1713.5229992367326\n",
      "Train Epoch: 685 [512/54000 (1%)] Loss: -1941.717773\n",
      "Train Epoch: 685 [11776/54000 (22%)] Loss: -1707.278320\n",
      "Train Epoch: 685 [23040/54000 (43%)] Loss: -1715.627686\n",
      "Train Epoch: 685 [34304/54000 (64%)] Loss: -1660.333862\n",
      "Train Epoch: 685 [45568/54000 (84%)] Loss: -1672.098267\n",
      "    epoch          : 685\n",
      "    loss           : -1658.132760529471\n",
      "    val_loss       : -1652.4479028199798\n",
      "    val_log_likelihood: 1729.31259765625\n",
      "    val_log_marginal: 1676.4711669158191\n",
      "Train Epoch: 686 [512/54000 (1%)] Loss: -1910.462402\n",
      "Train Epoch: 686 [11776/54000 (22%)] Loss: -1629.482422\n",
      "Train Epoch: 686 [23040/54000 (43%)] Loss: -1645.511353\n",
      "Train Epoch: 686 [34304/54000 (64%)] Loss: -1908.450073\n",
      "Train Epoch: 686 [45568/54000 (84%)] Loss: -1557.247803\n",
      "    epoch          : 686\n",
      "    loss           : -1647.7527568552753\n",
      "    val_loss       : -1645.5617264447733\n",
      "    val_log_likelihood: 1730.9\n",
      "    val_log_marginal: 1696.0279813729226\n",
      "Train Epoch: 687 [512/54000 (1%)] Loss: -1748.603638\n",
      "Train Epoch: 687 [11776/54000 (22%)] Loss: -1649.942993\n",
      "Train Epoch: 687 [23040/54000 (43%)] Loss: -1647.920898\n",
      "Train Epoch: 687 [34304/54000 (64%)] Loss: -1661.385498\n",
      "Train Epoch: 687 [45568/54000 (84%)] Loss: -1609.939209\n",
      "    epoch          : 687\n",
      "    loss           : -1650.0387047493812\n",
      "    val_loss       : -1658.4512493720279\n",
      "    val_log_likelihood: 1746.5756713867188\n",
      "    val_log_marginal: 1705.6516998052598\n",
      "Train Epoch: 688 [512/54000 (1%)] Loss: -1924.489136\n",
      "Train Epoch: 688 [11776/54000 (22%)] Loss: -1517.303955\n",
      "Train Epoch: 688 [23040/54000 (43%)] Loss: -1507.345947\n",
      "Train Epoch: 688 [34304/54000 (64%)] Loss: -1905.320435\n",
      "Train Epoch: 688 [45568/54000 (84%)] Loss: -1571.056641\n",
      "    epoch          : 688\n",
      "    loss           : -1630.5549086769029\n",
      "    val_loss       : -1625.4690221714786\n",
      "    val_log_likelihood: 1707.0497314453125\n",
      "    val_log_marginal: 1660.8111548412592\n",
      "Train Epoch: 689 [512/54000 (1%)] Loss: -1909.217529\n",
      "Train Epoch: 689 [11776/54000 (22%)] Loss: -1763.817993\n",
      "Train Epoch: 689 [23040/54000 (43%)] Loss: -1627.753418\n",
      "Train Epoch: 689 [34304/54000 (64%)] Loss: -1637.129517\n",
      "Train Epoch: 689 [45568/54000 (84%)] Loss: -1661.342285\n",
      "    epoch          : 689\n",
      "    loss           : -1644.6053793123453\n",
      "    val_loss       : -1647.3306665191426\n",
      "    val_log_likelihood: 1722.2762573242187\n",
      "    val_log_marginal: 1684.2682840231805\n",
      "Train Epoch: 690 [512/54000 (1%)] Loss: -1533.550903\n",
      "Train Epoch: 690 [11776/54000 (22%)] Loss: -1732.746826\n",
      "Train Epoch: 690 [23040/54000 (43%)] Loss: -1521.364258\n",
      "Train Epoch: 690 [34304/54000 (64%)] Loss: -1696.045288\n",
      "Train Epoch: 690 [45568/54000 (84%)] Loss: -1617.833008\n",
      "    epoch          : 690\n",
      "    loss           : -1647.8017735245205\n",
      "    val_loss       : -1644.4135059122927\n",
      "    val_log_likelihood: 1747.8199340820313\n",
      "    val_log_marginal: 1694.6885612800718\n",
      "Train Epoch: 691 [512/54000 (1%)] Loss: -1922.879395\n",
      "Train Epoch: 691 [11776/54000 (22%)] Loss: -1743.321167\n",
      "Train Epoch: 691 [23040/54000 (43%)] Loss: -1548.039307\n",
      "Train Epoch: 691 [34304/54000 (64%)] Loss: -1628.846924\n",
      "Train Epoch: 691 [45568/54000 (84%)] Loss: -1610.284912\n",
      "    epoch          : 691\n",
      "    loss           : -1653.3758726214419\n",
      "    val_loss       : -1632.1143854896538\n",
      "    val_log_likelihood: 1748.7288452148437\n",
      "    val_log_marginal: 1707.1453086405993\n",
      "Train Epoch: 692 [512/54000 (1%)] Loss: -1856.219727\n",
      "Train Epoch: 692 [11776/54000 (22%)] Loss: -1627.098999\n",
      "Train Epoch: 692 [23040/54000 (43%)] Loss: -1593.575439\n",
      "Train Epoch: 692 [34304/54000 (64%)] Loss: -1674.542236\n",
      "Train Epoch: 692 [45568/54000 (84%)] Loss: -1603.647949\n",
      "    epoch          : 692\n",
      "    loss           : -1649.4172387453589\n",
      "    val_loss       : -1644.7312489020637\n",
      "    val_log_likelihood: 1743.1529541015625\n",
      "    val_log_marginal: 1712.1457960646599\n",
      "Train Epoch: 693 [512/54000 (1%)] Loss: -1841.096069\n",
      "Train Epoch: 693 [11776/54000 (22%)] Loss: -1603.013306\n",
      "Train Epoch: 693 [23040/54000 (43%)] Loss: -1639.984375\n",
      "Train Epoch: 693 [34304/54000 (64%)] Loss: -1566.583862\n",
      "Train Epoch: 693 [45568/54000 (84%)] Loss: -1548.301392\n",
      "    epoch          : 693\n",
      "    loss           : -1653.7923028020575\n",
      "    val_loss       : -1635.652033393178\n",
      "    val_log_likelihood: 1744.4953125\n",
      "    val_log_marginal: 1705.2992063287645\n",
      "Train Epoch: 694 [512/54000 (1%)] Loss: -1665.507324\n",
      "Train Epoch: 694 [11776/54000 (22%)] Loss: -1542.463257\n",
      "Train Epoch: 694 [23040/54000 (43%)] Loss: -1608.170654\n",
      "Train Epoch: 694 [34304/54000 (64%)] Loss: -1838.904785\n",
      "Train Epoch: 694 [45568/54000 (84%)] Loss: -1634.453979\n",
      "    epoch          : 694\n",
      "    loss           : -1649.1829870242884\n",
      "    val_loss       : -1655.0689564002678\n",
      "    val_log_likelihood: 1748.2430053710937\n",
      "    val_log_marginal: 1720.5298458040356\n",
      "Train Epoch: 695 [512/54000 (1%)] Loss: -1902.598389\n",
      "Train Epoch: 695 [11776/54000 (22%)] Loss: -1560.986816\n",
      "Train Epoch: 695 [23040/54000 (43%)] Loss: -1543.728516\n",
      "Train Epoch: 695 [34304/54000 (64%)] Loss: -1636.715576\n",
      "Train Epoch: 695 [45568/54000 (84%)] Loss: -1527.123291\n",
      "    epoch          : 695\n",
      "    loss           : -1649.9672621925279\n",
      "    val_loss       : -1641.0905409500933\n",
      "    val_log_likelihood: 1740.2589233398437\n",
      "    val_log_marginal: 1711.2378732893617\n",
      "Train Epoch: 696 [512/54000 (1%)] Loss: -1912.313965\n",
      "Train Epoch: 696 [11776/54000 (22%)] Loss: -1565.894287\n",
      "Train Epoch: 696 [23040/54000 (43%)] Loss: -1558.656494\n",
      "Train Epoch: 696 [34304/54000 (64%)] Loss: -1645.913086\n",
      "Train Epoch: 696 [45568/54000 (84%)] Loss: -1571.093262\n",
      "    epoch          : 696\n",
      "    loss           : -1653.2215757464419\n",
      "    val_loss       : -1640.0908124135808\n",
      "    val_log_likelihood: 1701.4962890625\n",
      "    val_log_marginal: 1667.1625069089234\n",
      "Train Epoch: 697 [512/54000 (1%)] Loss: -1569.128174\n",
      "Train Epoch: 697 [11776/54000 (22%)] Loss: -1771.158203\n",
      "Train Epoch: 697 [23040/54000 (43%)] Loss: -1621.073364\n",
      "Train Epoch: 697 [34304/54000 (64%)] Loss: -1625.161255\n",
      "Train Epoch: 697 [45568/54000 (84%)] Loss: -1606.985840\n",
      "    epoch          : 697\n",
      "    loss           : -1645.0909387569616\n",
      "    val_loss       : -1642.9738687865436\n",
      "    val_log_likelihood: 1710.0458984375\n",
      "    val_log_marginal: 1673.2275787334888\n",
      "Train Epoch: 698 [512/54000 (1%)] Loss: -1733.627441\n",
      "Train Epoch: 698 [11776/54000 (22%)] Loss: -1709.697021\n",
      "Train Epoch: 698 [23040/54000 (43%)] Loss: -1594.106079\n",
      "Train Epoch: 698 [34304/54000 (64%)] Loss: -1639.950562\n",
      "Train Epoch: 698 [45568/54000 (84%)] Loss: -1568.560425\n",
      "    epoch          : 698\n",
      "    loss           : -1645.4976250676825\n",
      "    val_loss       : -1636.3705940121786\n",
      "    val_log_likelihood: 1738.6443969726563\n",
      "    val_log_marginal: 1692.8288932057053\n",
      "Train Epoch: 699 [512/54000 (1%)] Loss: -1891.368164\n",
      "Train Epoch: 699 [11776/54000 (22%)] Loss: -1534.144653\n",
      "Train Epoch: 699 [23040/54000 (43%)] Loss: -1693.210815\n",
      "Train Epoch: 699 [34304/54000 (64%)] Loss: -1632.748291\n",
      "Train Epoch: 699 [45568/54000 (84%)] Loss: -1648.780884\n",
      "    epoch          : 699\n",
      "    loss           : -1658.9795502011139\n",
      "    val_loss       : -1657.8190397896803\n",
      "    val_log_likelihood: 1737.3791748046874\n",
      "    val_log_marginal: 1694.4262306913733\n",
      "Train Epoch: 700 [512/54000 (1%)] Loss: -1909.731689\n",
      "Train Epoch: 700 [11776/54000 (22%)] Loss: -1607.341675\n",
      "Train Epoch: 700 [23040/54000 (43%)] Loss: -1546.959106\n",
      "Train Epoch: 700 [34304/54000 (64%)] Loss: -1536.231445\n",
      "Train Epoch: 700 [45568/54000 (84%)] Loss: -1637.629883\n",
      "    epoch          : 700\n",
      "    loss           : -1648.1497742303527\n",
      "    val_loss       : -1637.1989689198322\n",
      "    val_log_likelihood: 1741.6154174804688\n",
      "    val_log_marginal: 1686.5599473871291\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch700.pth ...\n",
      "Train Epoch: 701 [512/54000 (1%)] Loss: -1892.162354\n",
      "Train Epoch: 701 [11776/54000 (22%)] Loss: -1609.727905\n",
      "Train Epoch: 701 [23040/54000 (43%)] Loss: -1696.469727\n",
      "Train Epoch: 701 [34304/54000 (64%)] Loss: -1509.851685\n",
      "Train Epoch: 701 [45568/54000 (84%)] Loss: -1549.663818\n",
      "    epoch          : 701\n",
      "    loss           : -1654.5340370706992\n",
      "    val_loss       : -1640.253050862439\n",
      "    val_log_likelihood: 1727.3672729492187\n",
      "    val_log_marginal: 1684.085872778669\n",
      "Train Epoch: 702 [512/54000 (1%)] Loss: -1904.046509\n",
      "Train Epoch: 702 [11776/54000 (22%)] Loss: -1592.421143\n",
      "Train Epoch: 702 [23040/54000 (43%)] Loss: -1664.052979\n",
      "Train Epoch: 702 [34304/54000 (64%)] Loss: -1605.650391\n",
      "Train Epoch: 702 [45568/54000 (84%)] Loss: -1640.877686\n",
      "    epoch          : 702\n",
      "    loss           : -1649.886203879177\n",
      "    val_loss       : -1653.9749868579208\n",
      "    val_log_likelihood: 1734.3168212890625\n",
      "    val_log_marginal: 1688.2308283530176\n",
      "Train Epoch: 703 [512/54000 (1%)] Loss: -1872.430908\n",
      "Train Epoch: 703 [11776/54000 (22%)] Loss: -1580.461548\n",
      "Train Epoch: 703 [23040/54000 (43%)] Loss: -1511.191650\n",
      "Train Epoch: 703 [34304/54000 (64%)] Loss: -1569.790527\n",
      "Train Epoch: 703 [45568/54000 (84%)] Loss: -1650.291260\n",
      "    epoch          : 703\n",
      "    loss           : -1652.267443968518\n",
      "    val_loss       : -1641.478378725145\n",
      "    val_log_likelihood: 1726.8890380859375\n",
      "    val_log_marginal: 1678.0737295292317\n",
      "Train Epoch: 704 [512/54000 (1%)] Loss: -1895.090332\n",
      "Train Epoch: 704 [11776/54000 (22%)] Loss: -1713.980347\n",
      "Train Epoch: 704 [23040/54000 (43%)] Loss: -1578.635010\n",
      "Train Epoch: 704 [34304/54000 (64%)] Loss: -1924.683105\n",
      "Train Epoch: 704 [45568/54000 (84%)] Loss: -1649.737061\n",
      "    epoch          : 704\n",
      "    loss           : -1656.4894185585551\n",
      "    val_loss       : -1656.3049472188577\n",
      "    val_log_likelihood: 1726.81337890625\n",
      "    val_log_marginal: 1680.7701229710133\n",
      "Train Epoch: 705 [512/54000 (1%)] Loss: -1914.678467\n",
      "Train Epoch: 705 [11776/54000 (22%)] Loss: -1688.073975\n",
      "Train Epoch: 705 [23040/54000 (43%)] Loss: -1602.367554\n",
      "Train Epoch: 705 [34304/54000 (64%)] Loss: -1680.146484\n",
      "Train Epoch: 705 [45568/54000 (84%)] Loss: -1654.524902\n",
      "    epoch          : 705\n",
      "    loss           : -1659.9580102297339\n",
      "    val_loss       : -1658.4684224327095\n",
      "    val_log_likelihood: 1753.0861206054688\n",
      "    val_log_marginal: 1716.3996926087887\n",
      "Train Epoch: 706 [512/54000 (1%)] Loss: -1726.129761\n",
      "Train Epoch: 706 [11776/54000 (22%)] Loss: -1565.942017\n",
      "Train Epoch: 706 [23040/54000 (43%)] Loss: -1577.106689\n",
      "Train Epoch: 706 [34304/54000 (64%)] Loss: -1606.837280\n",
      "Train Epoch: 706 [45568/54000 (84%)] Loss: -1654.184937\n",
      "    epoch          : 706\n",
      "    loss           : -1652.295624081451\n",
      "    val_loss       : -1649.7444838047027\n",
      "    val_log_likelihood: 1724.8735107421876\n",
      "    val_log_marginal: 1681.5895237624645\n",
      "Train Epoch: 707 [512/54000 (1%)] Loss: -1920.359253\n",
      "Train Epoch: 707 [11776/54000 (22%)] Loss: -1577.825806\n",
      "Train Epoch: 707 [23040/54000 (43%)] Loss: -1640.486206\n",
      "Train Epoch: 707 [34304/54000 (64%)] Loss: -1568.624023\n",
      "Train Epoch: 707 [45568/54000 (84%)] Loss: -1635.443848\n",
      "    epoch          : 707\n",
      "    loss           : -1651.5640893312964\n",
      "    val_loss       : -1644.2729667915962\n",
      "    val_log_likelihood: 1740.722021484375\n",
      "    val_log_marginal: 1697.4760736051946\n",
      "Train Epoch: 708 [512/54000 (1%)] Loss: -1868.475098\n",
      "Train Epoch: 708 [11776/54000 (22%)] Loss: -1556.294434\n",
      "Train Epoch: 708 [23040/54000 (43%)] Loss: -1554.578613\n",
      "Train Epoch: 708 [34304/54000 (64%)] Loss: -1572.235352\n",
      "Train Epoch: 708 [45568/54000 (84%)] Loss: -1587.981445\n",
      "    epoch          : 708\n",
      "    loss           : -1648.3558119972154\n",
      "    val_loss       : -1645.940846388042\n",
      "    val_log_likelihood: 1727.4174926757812\n",
      "    val_log_marginal: 1682.8976373128594\n",
      "Train Epoch: 709 [512/54000 (1%)] Loss: -1745.811890\n",
      "Train Epoch: 709 [11776/54000 (22%)] Loss: -1641.037476\n",
      "Train Epoch: 709 [23040/54000 (43%)] Loss: -1671.206299\n",
      "Train Epoch: 709 [34304/54000 (64%)] Loss: -1905.970703\n",
      "Train Epoch: 709 [45568/54000 (84%)] Loss: -1569.240845\n",
      "    epoch          : 709\n",
      "    loss           : -1657.1879181814666\n",
      "    val_loss       : -1645.6683521796017\n",
      "    val_log_likelihood: 1743.8438232421875\n",
      "    val_log_marginal: 1699.8536895260215\n",
      "Train Epoch: 710 [512/54000 (1%)] Loss: -1908.030884\n",
      "Train Epoch: 710 [11776/54000 (22%)] Loss: -1563.009155\n",
      "Train Epoch: 710 [23040/54000 (43%)] Loss: -1539.427612\n",
      "Train Epoch: 710 [34304/54000 (64%)] Loss: -1625.020752\n",
      "Train Epoch: 710 [45568/54000 (84%)] Loss: -1585.520996\n",
      "    epoch          : 710\n",
      "    loss           : -1651.600996867265\n",
      "    val_loss       : -1646.4960600290447\n",
      "    val_log_likelihood: 1740.735107421875\n",
      "    val_log_marginal: 1693.5066601525991\n",
      "Train Epoch: 711 [512/54000 (1%)] Loss: -1737.057129\n",
      "Train Epoch: 711 [11776/54000 (22%)] Loss: -1603.287842\n",
      "Train Epoch: 711 [23040/54000 (43%)] Loss: -1588.496094\n",
      "Train Epoch: 711 [34304/54000 (64%)] Loss: -1623.479736\n",
      "Train Epoch: 711 [45568/54000 (84%)] Loss: -1704.846069\n",
      "    epoch          : 711\n",
      "    loss           : -1662.9338209699877\n",
      "    val_loss       : -1648.5550575325265\n",
      "    val_log_likelihood: 1730.4388427734375\n",
      "    val_log_marginal: 1686.8645902760327\n",
      "Train Epoch: 712 [512/54000 (1%)] Loss: -1887.570312\n",
      "Train Epoch: 712 [11776/54000 (22%)] Loss: -1557.859863\n",
      "Train Epoch: 712 [23040/54000 (43%)] Loss: -1498.890381\n",
      "Train Epoch: 712 [34304/54000 (64%)] Loss: -1700.423462\n",
      "Train Epoch: 712 [45568/54000 (84%)] Loss: -1574.962036\n",
      "    epoch          : 712\n",
      "    loss           : -1651.245201790687\n",
      "    val_loss       : -1649.0220211586916\n",
      "    val_log_likelihood: 1750.3289794921875\n",
      "    val_log_marginal: 1712.3731225606055\n",
      "Train Epoch: 713 [512/54000 (1%)] Loss: -1917.918213\n",
      "Train Epoch: 713 [11776/54000 (22%)] Loss: -1760.179932\n",
      "Train Epoch: 713 [23040/54000 (43%)] Loss: -1599.258057\n",
      "Train Epoch: 713 [34304/54000 (64%)] Loss: -1721.184448\n",
      "Train Epoch: 713 [45568/54000 (84%)] Loss: -1638.717285\n",
      "    epoch          : 713\n",
      "    loss           : -1653.3513086904393\n",
      "    val_loss       : -1638.6055181086995\n",
      "    val_log_likelihood: 1753.3839477539063\n",
      "    val_log_marginal: 1719.0016906946898\n",
      "Train Epoch: 714 [512/54000 (1%)] Loss: -1626.401367\n",
      "Train Epoch: 714 [11776/54000 (22%)] Loss: -1580.647827\n",
      "Train Epoch: 714 [23040/54000 (43%)] Loss: -1504.957642\n",
      "Train Epoch: 714 [34304/54000 (64%)] Loss: -1651.156372\n",
      "Train Epoch: 714 [45568/54000 (84%)] Loss: -1644.456665\n",
      "    epoch          : 714\n",
      "    loss           : -1660.600234229966\n",
      "    val_loss       : -1663.9708954980597\n",
      "    val_log_likelihood: 1738.9628662109376\n",
      "    val_log_marginal: 1688.0908151905983\n",
      "Train Epoch: 715 [512/54000 (1%)] Loss: -1940.143066\n",
      "Train Epoch: 715 [11776/54000 (22%)] Loss: -1597.000244\n",
      "Train Epoch: 715 [23040/54000 (43%)] Loss: -1556.368164\n",
      "Train Epoch: 715 [34304/54000 (64%)] Loss: -1529.716553\n",
      "Train Epoch: 715 [45568/54000 (84%)] Loss: -1678.154175\n",
      "    epoch          : 715\n",
      "    loss           : -1669.2884944500308\n",
      "    val_loss       : -1674.045252860617\n",
      "    val_log_likelihood: 1756.2485961914062\n",
      "    val_log_marginal: 1713.188038642332\n",
      "Train Epoch: 716 [512/54000 (1%)] Loss: -1900.459351\n",
      "Train Epoch: 716 [11776/54000 (22%)] Loss: -1612.909302\n",
      "Train Epoch: 716 [23040/54000 (43%)] Loss: -1633.860840\n",
      "Train Epoch: 716 [34304/54000 (64%)] Loss: -1567.096680\n",
      "Train Epoch: 716 [45568/54000 (84%)] Loss: -1561.066650\n",
      "    epoch          : 716\n",
      "    loss           : -1664.3389372872834\n",
      "    val_loss       : -1660.8234132084995\n",
      "    val_log_likelihood: 1724.9380737304687\n",
      "    val_log_marginal: 1683.8953979983162\n",
      "Train Epoch: 717 [512/54000 (1%)] Loss: -1919.975342\n",
      "Train Epoch: 717 [11776/54000 (22%)] Loss: -1621.270020\n",
      "Train Epoch: 717 [23040/54000 (43%)] Loss: -1665.002441\n",
      "Train Epoch: 717 [34304/54000 (64%)] Loss: -1586.151367\n",
      "Train Epoch: 717 [45568/54000 (84%)] Loss: -1576.984985\n",
      "    epoch          : 717\n",
      "    loss           : -1666.1050191444926\n",
      "    val_loss       : -1664.1307041015475\n",
      "    val_log_likelihood: 1735.646240234375\n",
      "    val_log_marginal: 1702.1652783911675\n",
      "Train Epoch: 718 [512/54000 (1%)] Loss: -1733.248047\n",
      "Train Epoch: 718 [11776/54000 (22%)] Loss: -1579.933594\n",
      "Train Epoch: 718 [23040/54000 (43%)] Loss: -1609.815430\n",
      "Train Epoch: 718 [34304/54000 (64%)] Loss: -1694.332764\n",
      "Train Epoch: 718 [45568/54000 (84%)] Loss: -1649.384277\n",
      "    epoch          : 718\n",
      "    loss           : -1668.6172829807394\n",
      "    val_loss       : -1626.7605135242454\n",
      "    val_log_likelihood: 1714.4833862304688\n",
      "    val_log_marginal: 1673.7802501536905\n",
      "Train Epoch: 719 [512/54000 (1%)] Loss: -1601.979492\n",
      "Train Epoch: 719 [11776/54000 (22%)] Loss: -1603.655762\n",
      "Train Epoch: 719 [23040/54000 (43%)] Loss: -1649.679077\n",
      "Train Epoch: 719 [34304/54000 (64%)] Loss: -1646.987427\n",
      "Train Epoch: 719 [45568/54000 (84%)] Loss: -1653.890381\n",
      "    epoch          : 719\n",
      "    loss           : -1656.5942322381652\n",
      "    val_loss       : -1665.7505090321415\n",
      "    val_log_likelihood: 1737.1391845703124\n",
      "    val_log_marginal: 1687.8473700099814\n",
      "Train Epoch: 720 [512/54000 (1%)] Loss: -1922.428711\n",
      "Train Epoch: 720 [11776/54000 (22%)] Loss: -1594.682373\n",
      "Train Epoch: 720 [23040/54000 (43%)] Loss: -1662.018311\n",
      "Train Epoch: 720 [34304/54000 (64%)] Loss: -1745.329102\n",
      "Train Epoch: 720 [45568/54000 (84%)] Loss: -1679.795654\n",
      "    epoch          : 720\n",
      "    loss           : -1665.497235893023\n",
      "    val_loss       : -1665.275326876901\n",
      "    val_log_likelihood: 1737.919580078125\n",
      "    val_log_marginal: 1698.6881333369824\n",
      "Train Epoch: 721 [512/54000 (1%)] Loss: -1559.364380\n",
      "Train Epoch: 721 [11776/54000 (22%)] Loss: -1594.740967\n",
      "Train Epoch: 721 [23040/54000 (43%)] Loss: -1682.434814\n",
      "Train Epoch: 721 [34304/54000 (64%)] Loss: -1613.599609\n",
      "Train Epoch: 721 [45568/54000 (84%)] Loss: -1555.629272\n",
      "    epoch          : 721\n",
      "    loss           : -1664.1518349222617\n",
      "    val_loss       : -1659.7290408568456\n",
      "    val_log_likelihood: 1743.0117553710938\n",
      "    val_log_marginal: 1702.3130379348993\n",
      "Train Epoch: 722 [512/54000 (1%)] Loss: -1919.367432\n",
      "Train Epoch: 722 [11776/54000 (22%)] Loss: -1644.202393\n",
      "Train Epoch: 722 [23040/54000 (43%)] Loss: -1633.241943\n",
      "Train Epoch: 722 [34304/54000 (64%)] Loss: -1689.179321\n",
      "Train Epoch: 722 [45568/54000 (84%)] Loss: -1709.494873\n",
      "    epoch          : 722\n",
      "    loss           : -1664.7211491046567\n",
      "    val_loss       : -1670.0671081786975\n",
      "    val_log_likelihood: 1746.7276733398437\n",
      "    val_log_marginal: 1703.0689907696099\n",
      "Train Epoch: 723 [512/54000 (1%)] Loss: -1926.082642\n",
      "Train Epoch: 723 [11776/54000 (22%)] Loss: -1570.594238\n",
      "Train Epoch: 723 [23040/54000 (43%)] Loss: -1631.109009\n",
      "Train Epoch: 723 [34304/54000 (64%)] Loss: -1670.500488\n",
      "Train Epoch: 723 [45568/54000 (84%)] Loss: -1675.013916\n",
      "    epoch          : 723\n",
      "    loss           : -1670.7308555074258\n",
      "    val_loss       : -1666.3096839765087\n",
      "    val_log_likelihood: 1760.7757202148437\n",
      "    val_log_marginal: 1721.4666069824248\n",
      "Train Epoch: 724 [512/54000 (1%)] Loss: -1936.166748\n",
      "Train Epoch: 724 [11776/54000 (22%)] Loss: -1748.872559\n",
      "Train Epoch: 724 [23040/54000 (43%)] Loss: -1662.877563\n",
      "Train Epoch: 724 [34304/54000 (64%)] Loss: -1591.571899\n",
      "Train Epoch: 724 [45568/54000 (84%)] Loss: -1663.062012\n",
      "    epoch          : 724\n",
      "    loss           : -1674.2485303217823\n",
      "    val_loss       : -1661.7885943422093\n",
      "    val_log_likelihood: 1753.0353881835938\n",
      "    val_log_marginal: 1716.1865338101984\n",
      "Train Epoch: 725 [512/54000 (1%)] Loss: -1917.280396\n",
      "Train Epoch: 725 [11776/54000 (22%)] Loss: -1634.224854\n",
      "Train Epoch: 725 [23040/54000 (43%)] Loss: -1533.007935\n",
      "Train Epoch: 725 [34304/54000 (64%)] Loss: -1890.028809\n",
      "Train Epoch: 725 [45568/54000 (84%)] Loss: -1652.786499\n",
      "    epoch          : 725\n",
      "    loss           : -1663.4159842009592\n",
      "    val_loss       : -1658.914579362236\n",
      "    val_log_likelihood: 1756.025390625\n",
      "    val_log_marginal: 1727.126121263206\n",
      "Train Epoch: 726 [512/54000 (1%)] Loss: -1753.125000\n",
      "Train Epoch: 726 [11776/54000 (22%)] Loss: -1584.549072\n",
      "Train Epoch: 726 [23040/54000 (43%)] Loss: -1619.604492\n",
      "Train Epoch: 726 [34304/54000 (64%)] Loss: -1570.670654\n",
      "Train Epoch: 726 [45568/54000 (84%)] Loss: -1668.684326\n",
      "    epoch          : 726\n",
      "    loss           : -1660.2188200997834\n",
      "    val_loss       : -1662.0718378286808\n",
      "    val_log_likelihood: 1742.9081909179688\n",
      "    val_log_marginal: 1713.8773589041084\n",
      "Train Epoch: 727 [512/54000 (1%)] Loss: -1925.000610\n",
      "Train Epoch: 727 [11776/54000 (22%)] Loss: -1706.586182\n",
      "Train Epoch: 727 [23040/54000 (43%)] Loss: -1650.018311\n",
      "Train Epoch: 727 [34304/54000 (64%)] Loss: -1572.170166\n",
      "Train Epoch: 727 [45568/54000 (84%)] Loss: -1655.032959\n",
      "    epoch          : 727\n",
      "    loss           : -1663.7980932858911\n",
      "    val_loss       : -1654.7415619776584\n",
      "    val_log_likelihood: 1722.506982421875\n",
      "    val_log_marginal: 1688.4387271743267\n",
      "Train Epoch: 728 [512/54000 (1%)] Loss: -1601.751587\n",
      "Train Epoch: 728 [11776/54000 (22%)] Loss: -1585.003662\n",
      "Train Epoch: 728 [23040/54000 (43%)] Loss: -1520.023682\n",
      "Train Epoch: 728 [34304/54000 (64%)] Loss: -1590.314941\n",
      "Train Epoch: 728 [45568/54000 (84%)] Loss: -1627.904541\n",
      "    epoch          : 728\n",
      "    loss           : -1661.6416656191986\n",
      "    val_loss       : -1663.3238061711193\n",
      "    val_log_likelihood: 1746.6078857421876\n",
      "    val_log_marginal: 1708.9556128818542\n",
      "Train Epoch: 729 [512/54000 (1%)] Loss: -1918.358276\n",
      "Train Epoch: 729 [11776/54000 (22%)] Loss: -1573.091431\n",
      "Train Epoch: 729 [23040/54000 (43%)] Loss: -1562.554932\n",
      "Train Epoch: 729 [34304/54000 (64%)] Loss: -1907.668945\n",
      "Train Epoch: 729 [45568/54000 (84%)] Loss: -1628.428955\n",
      "    epoch          : 729\n",
      "    loss           : -1664.5243197903774\n",
      "    val_loss       : -1662.547785405442\n",
      "    val_log_likelihood: 1747.3030395507812\n",
      "    val_log_marginal: 1707.4097532100975\n",
      "Train Epoch: 730 [512/54000 (1%)] Loss: -1919.832520\n",
      "Train Epoch: 730 [11776/54000 (22%)] Loss: -1619.446777\n",
      "Train Epoch: 730 [23040/54000 (43%)] Loss: -1565.291382\n",
      "Train Epoch: 730 [34304/54000 (64%)] Loss: -1914.552856\n",
      "Train Epoch: 730 [45568/54000 (84%)] Loss: -1539.208618\n",
      "    epoch          : 730\n",
      "    loss           : -1667.1130238145886\n",
      "    val_loss       : -1655.1005112963728\n",
      "    val_log_likelihood: 1739.6910766601563\n",
      "    val_log_marginal: 1707.5865640282013\n",
      "Train Epoch: 731 [512/54000 (1%)] Loss: -1929.087036\n",
      "Train Epoch: 731 [11776/54000 (22%)] Loss: -1631.190430\n",
      "Train Epoch: 731 [23040/54000 (43%)] Loss: -1625.643311\n",
      "Train Epoch: 731 [34304/54000 (64%)] Loss: -1547.220947\n",
      "Train Epoch: 731 [45568/54000 (84%)] Loss: -1558.246460\n",
      "    epoch          : 731\n",
      "    loss           : -1659.8891263149753\n",
      "    val_loss       : -1657.1406677726657\n",
      "    val_log_likelihood: 1727.057470703125\n",
      "    val_log_marginal: 1692.2912473443896\n",
      "Train Epoch: 732 [512/54000 (1%)] Loss: -1906.507935\n",
      "Train Epoch: 732 [11776/54000 (22%)] Loss: -1591.947998\n",
      "Train Epoch: 732 [23040/54000 (43%)] Loss: -1591.970947\n",
      "Train Epoch: 732 [34304/54000 (64%)] Loss: -1640.519409\n",
      "Train Epoch: 732 [45568/54000 (84%)] Loss: -1561.607178\n",
      "    epoch          : 732\n",
      "    loss           : -1662.2298257657797\n",
      "    val_loss       : -1666.9913978425786\n",
      "    val_log_likelihood: 1737.5110961914063\n",
      "    val_log_marginal: 1702.1919929664582\n",
      "Train Epoch: 733 [512/54000 (1%)] Loss: -1924.321045\n",
      "Train Epoch: 733 [11776/54000 (22%)] Loss: -1739.028809\n",
      "Train Epoch: 733 [23040/54000 (43%)] Loss: -1685.774414\n",
      "Train Epoch: 733 [34304/54000 (64%)] Loss: -1926.617554\n",
      "Train Epoch: 733 [45568/54000 (84%)] Loss: -1650.969482\n",
      "    epoch          : 733\n",
      "    loss           : -1669.195341506807\n",
      "    val_loss       : -1667.3301353656686\n",
      "    val_log_likelihood: 1751.58505859375\n",
      "    val_log_marginal: 1714.6988659247756\n",
      "Train Epoch: 734 [512/54000 (1%)] Loss: -1914.477539\n",
      "Train Epoch: 734 [11776/54000 (22%)] Loss: -1611.591431\n",
      "Train Epoch: 734 [23040/54000 (43%)] Loss: -1627.741699\n",
      "Train Epoch: 734 [34304/54000 (64%)] Loss: -1670.245850\n",
      "Train Epoch: 734 [45568/54000 (84%)] Loss: -1597.896362\n",
      "    epoch          : 734\n",
      "    loss           : -1671.5715936339727\n",
      "    val_loss       : -1680.2340619603171\n",
      "    val_log_likelihood: 1734.00927734375\n",
      "    val_log_marginal: 1701.9338153898716\n",
      "Train Epoch: 735 [512/54000 (1%)] Loss: -1926.241577\n",
      "Train Epoch: 735 [11776/54000 (22%)] Loss: -1580.744019\n",
      "Train Epoch: 735 [23040/54000 (43%)] Loss: -1573.178223\n",
      "Train Epoch: 735 [34304/54000 (64%)] Loss: -1671.662476\n",
      "Train Epoch: 735 [45568/54000 (84%)] Loss: -1517.678467\n",
      "    epoch          : 735\n",
      "    loss           : -1664.593340278852\n",
      "    val_loss       : -1662.30960355727\n",
      "    val_log_likelihood: 1748.2457153320313\n",
      "    val_log_marginal: 1714.0063600528986\n",
      "Train Epoch: 736 [512/54000 (1%)] Loss: -1924.608398\n",
      "Train Epoch: 736 [11776/54000 (22%)] Loss: -1761.567749\n",
      "Train Epoch: 736 [23040/54000 (43%)] Loss: -1709.230713\n",
      "Train Epoch: 736 [34304/54000 (64%)] Loss: -1606.251343\n",
      "Train Epoch: 736 [45568/54000 (84%)] Loss: -1668.273926\n",
      "    epoch          : 736\n",
      "    loss           : -1671.811089544013\n",
      "    val_loss       : -1678.0948097558692\n",
      "    val_log_likelihood: 1752.9201416015626\n",
      "    val_log_marginal: 1711.4304691012949\n",
      "Train Epoch: 737 [512/54000 (1%)] Loss: -1923.048584\n",
      "Train Epoch: 737 [11776/54000 (22%)] Loss: -1617.839478\n",
      "Train Epoch: 737 [23040/54000 (43%)] Loss: -1625.042358\n",
      "Train Epoch: 737 [34304/54000 (64%)] Loss: -1578.230713\n",
      "Train Epoch: 737 [45568/54000 (84%)] Loss: -1573.315674\n",
      "    epoch          : 737\n",
      "    loss           : -1666.5685986811573\n",
      "    val_loss       : -1654.161915191356\n",
      "    val_log_likelihood: 1750.2650268554687\n",
      "    val_log_marginal: 1710.9238746207207\n",
      "Train Epoch: 738 [512/54000 (1%)] Loss: -1925.235229\n",
      "Train Epoch: 738 [11776/54000 (22%)] Loss: -1619.534058\n",
      "Train Epoch: 738 [23040/54000 (43%)] Loss: -1579.231201\n",
      "Train Epoch: 738 [34304/54000 (64%)] Loss: -1691.548706\n",
      "Train Epoch: 738 [45568/54000 (84%)] Loss: -1645.509521\n",
      "    epoch          : 738\n",
      "    loss           : -1663.194066415919\n",
      "    val_loss       : -1664.9846570425666\n",
      "    val_log_likelihood: 1752.358984375\n",
      "    val_log_marginal: 1713.323246134445\n",
      "Train Epoch: 739 [512/54000 (1%)] Loss: -1889.974487\n",
      "Train Epoch: 739 [11776/54000 (22%)] Loss: -1582.258545\n",
      "Train Epoch: 739 [23040/54000 (43%)] Loss: -1673.710083\n",
      "Train Epoch: 739 [34304/54000 (64%)] Loss: -1660.751953\n",
      "Train Epoch: 739 [45568/54000 (84%)] Loss: -1641.755981\n",
      "    epoch          : 739\n",
      "    loss           : -1667.0300498433633\n",
      "    val_loss       : -1645.486559464503\n",
      "    val_log_likelihood: 1754.4310913085938\n",
      "    val_log_marginal: 1707.738812559098\n",
      "Train Epoch: 740 [512/54000 (1%)] Loss: -1886.570068\n",
      "Train Epoch: 740 [11776/54000 (22%)] Loss: -1722.482910\n",
      "Train Epoch: 740 [23040/54000 (43%)] Loss: -1579.648193\n",
      "Train Epoch: 740 [34304/54000 (64%)] Loss: -1674.265137\n",
      "Train Epoch: 740 [45568/54000 (84%)] Loss: -1683.451782\n",
      "    epoch          : 740\n",
      "    loss           : -1661.885094368812\n",
      "    val_loss       : -1649.389859662857\n",
      "    val_log_likelihood: 1760.0132446289062\n",
      "    val_log_marginal: 1721.7278607733547\n",
      "Train Epoch: 741 [512/54000 (1%)] Loss: -1871.115967\n",
      "Train Epoch: 741 [11776/54000 (22%)] Loss: -1584.292969\n",
      "Train Epoch: 741 [23040/54000 (43%)] Loss: -1586.610107\n",
      "Train Epoch: 741 [34304/54000 (64%)] Loss: -1673.911621\n",
      "Train Epoch: 741 [45568/54000 (84%)] Loss: -1694.321289\n",
      "    epoch          : 741\n",
      "    loss           : -1656.397352161974\n",
      "    val_loss       : -1647.9620789431035\n",
      "    val_log_likelihood: 1733.3666870117188\n",
      "    val_log_marginal: 1694.3855328414588\n",
      "Train Epoch: 742 [512/54000 (1%)] Loss: -1932.760010\n",
      "Train Epoch: 742 [11776/54000 (22%)] Loss: -1572.397949\n",
      "Train Epoch: 742 [23040/54000 (43%)] Loss: -1632.981812\n",
      "Train Epoch: 742 [34304/54000 (64%)] Loss: -1619.228882\n",
      "Train Epoch: 742 [45568/54000 (84%)] Loss: -1581.004028\n",
      "    epoch          : 742\n",
      "    loss           : -1654.5906196820854\n",
      "    val_loss       : -1655.431006497983\n",
      "    val_log_likelihood: 1723.2666259765624\n",
      "    val_log_marginal: 1681.334698119387\n",
      "Train Epoch: 743 [512/54000 (1%)] Loss: -1923.267822\n",
      "Train Epoch: 743 [11776/54000 (22%)] Loss: -1625.988281\n",
      "Train Epoch: 743 [23040/54000 (43%)] Loss: -1532.201172\n",
      "Train Epoch: 743 [34304/54000 (64%)] Loss: -1568.399414\n",
      "Train Epoch: 743 [45568/54000 (84%)] Loss: -1688.279541\n",
      "    epoch          : 743\n",
      "    loss           : -1659.278427879409\n",
      "    val_loss       : -1650.6058124109172\n",
      "    val_log_likelihood: 1734.4826416015626\n",
      "    val_log_marginal: 1693.5725865740328\n",
      "Train Epoch: 744 [512/54000 (1%)] Loss: -1909.743164\n",
      "Train Epoch: 744 [11776/54000 (22%)] Loss: -1535.519165\n",
      "Train Epoch: 744 [23040/54000 (43%)] Loss: -1586.475342\n",
      "Train Epoch: 744 [34304/54000 (64%)] Loss: -1616.030518\n",
      "Train Epoch: 744 [45568/54000 (84%)] Loss: -1683.526367\n",
      "    epoch          : 744\n",
      "    loss           : -1643.2674270478806\n",
      "    val_loss       : -1653.1571676998399\n",
      "    val_log_likelihood: 1742.4487426757812\n",
      "    val_log_marginal: 1703.164578556642\n",
      "Train Epoch: 745 [512/54000 (1%)] Loss: -1585.523438\n",
      "Train Epoch: 745 [11776/54000 (22%)] Loss: -1722.420288\n",
      "Train Epoch: 745 [23040/54000 (43%)] Loss: -1631.814941\n",
      "Train Epoch: 745 [34304/54000 (64%)] Loss: -1705.391113\n",
      "Train Epoch: 745 [45568/54000 (84%)] Loss: -1650.079346\n",
      "    epoch          : 745\n",
      "    loss           : -1664.7875916131652\n",
      "    val_loss       : -1660.4735099338927\n",
      "    val_log_likelihood: 1736.7809692382812\n",
      "    val_log_marginal: 1692.9222554761916\n",
      "Train Epoch: 746 [512/54000 (1%)] Loss: -1916.633667\n",
      "Train Epoch: 746 [11776/54000 (22%)] Loss: -1703.835327\n",
      "Train Epoch: 746 [23040/54000 (43%)] Loss: -1553.001587\n",
      "Train Epoch: 746 [34304/54000 (64%)] Loss: -1707.195801\n",
      "Train Epoch: 746 [45568/54000 (84%)] Loss: -1627.630859\n",
      "    epoch          : 746\n",
      "    loss           : -1665.7433864480197\n",
      "    val_loss       : -1662.299943899829\n",
      "    val_log_likelihood: 1742.3415161132812\n",
      "    val_log_marginal: 1706.6324333377183\n",
      "Train Epoch: 747 [512/54000 (1%)] Loss: -1913.794434\n",
      "Train Epoch: 747 [11776/54000 (22%)] Loss: -1641.292480\n",
      "Train Epoch: 747 [23040/54000 (43%)] Loss: -1697.947754\n",
      "Train Epoch: 747 [34304/54000 (64%)] Loss: -1930.300293\n",
      "Train Epoch: 747 [45568/54000 (84%)] Loss: -1678.559937\n",
      "    epoch          : 747\n",
      "    loss           : -1669.739227597076\n",
      "    val_loss       : -1669.9181405361742\n",
      "    val_log_likelihood: 1757.0042236328125\n",
      "    val_log_marginal: 1719.42730182074\n",
      "Train Epoch: 748 [512/54000 (1%)] Loss: -1926.605469\n",
      "Train Epoch: 748 [11776/54000 (22%)] Loss: -1762.163086\n",
      "Train Epoch: 748 [23040/54000 (43%)] Loss: -1643.130859\n",
      "Train Epoch: 748 [34304/54000 (64%)] Loss: -1704.407715\n",
      "Train Epoch: 748 [45568/54000 (84%)] Loss: -1662.990723\n",
      "    epoch          : 748\n",
      "    loss           : -1669.155735129177\n",
      "    val_loss       : -1670.8063782698475\n",
      "    val_log_likelihood: 1741.134375\n",
      "    val_log_marginal: 1696.4933174863459\n",
      "Train Epoch: 749 [512/54000 (1%)] Loss: -1901.817993\n",
      "Train Epoch: 749 [11776/54000 (22%)] Loss: -1725.761963\n",
      "Train Epoch: 749 [23040/54000 (43%)] Loss: -1594.974121\n",
      "Train Epoch: 749 [34304/54000 (64%)] Loss: -1679.018677\n",
      "Train Epoch: 749 [45568/54000 (84%)] Loss: -1551.293091\n",
      "    epoch          : 749\n",
      "    loss           : -1661.474907903388\n",
      "    val_loss       : -1657.4435949855483\n",
      "    val_log_likelihood: 1736.5959350585938\n",
      "    val_log_marginal: 1695.7333471386155\n",
      "Train Epoch: 750 [512/54000 (1%)] Loss: -1927.534058\n",
      "Train Epoch: 750 [11776/54000 (22%)] Loss: -1561.306030\n",
      "Train Epoch: 750 [23040/54000 (43%)] Loss: -1577.413696\n",
      "Train Epoch: 750 [34304/54000 (64%)] Loss: -1674.658691\n",
      "Train Epoch: 750 [45568/54000 (84%)] Loss: -1570.287109\n",
      "    epoch          : 750\n",
      "    loss           : -1664.0347900390625\n",
      "    val_loss       : -1668.3241566613317\n",
      "    val_log_likelihood: 1743.6101684570312\n",
      "    val_log_marginal: 1706.3079536437988\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch750.pth ...\n",
      "Train Epoch: 751 [512/54000 (1%)] Loss: -1926.731689\n",
      "Train Epoch: 751 [11776/54000 (22%)] Loss: -1757.294556\n",
      "Train Epoch: 751 [23040/54000 (43%)] Loss: -1604.502686\n",
      "Train Epoch: 751 [34304/54000 (64%)] Loss: -1659.293579\n",
      "Train Epoch: 751 [45568/54000 (84%)] Loss: -1684.546997\n",
      "    epoch          : 751\n",
      "    loss           : -1672.5937475827661\n",
      "    val_loss       : -1665.1642670189963\n",
      "    val_log_likelihood: 1746.5790405273438\n",
      "    val_log_marginal: 1704.4388241507113\n",
      "Train Epoch: 752 [512/54000 (1%)] Loss: -1581.446289\n",
      "Train Epoch: 752 [11776/54000 (22%)] Loss: -1673.284912\n",
      "Train Epoch: 752 [23040/54000 (43%)] Loss: -1693.822998\n",
      "Train Epoch: 752 [34304/54000 (64%)] Loss: -1585.320801\n",
      "Train Epoch: 752 [45568/54000 (84%)] Loss: -1593.844971\n",
      "    epoch          : 752\n",
      "    loss           : -1669.6153987469058\n",
      "    val_loss       : -1669.1858432214708\n",
      "    val_log_likelihood: 1746.9711059570313\n",
      "    val_log_marginal: 1706.378757218644\n",
      "Train Epoch: 753 [512/54000 (1%)] Loss: -1922.773193\n",
      "Train Epoch: 753 [11776/54000 (22%)] Loss: -1595.486816\n",
      "Train Epoch: 753 [23040/54000 (43%)] Loss: -1659.272949\n",
      "Train Epoch: 753 [34304/54000 (64%)] Loss: -1590.897095\n",
      "Train Epoch: 753 [45568/54000 (84%)] Loss: -1642.169678\n",
      "    epoch          : 753\n",
      "    loss           : -1671.116424862701\n",
      "    val_loss       : -1683.7406821171753\n",
      "    val_log_likelihood: 1746.7075439453124\n",
      "    val_log_marginal: 1696.1188260130584\n",
      "Train Epoch: 754 [512/54000 (1%)] Loss: -1918.150879\n",
      "Train Epoch: 754 [11776/54000 (22%)] Loss: -1661.184937\n",
      "Train Epoch: 754 [23040/54000 (43%)] Loss: -1689.801025\n",
      "Train Epoch: 754 [34304/54000 (64%)] Loss: -1663.736206\n",
      "Train Epoch: 754 [45568/54000 (84%)] Loss: -1660.879639\n",
      "    epoch          : 754\n",
      "    loss           : -1677.2459728883045\n",
      "    val_loss       : -1674.7250415894202\n",
      "    val_log_likelihood: 1758.5728881835937\n",
      "    val_log_marginal: 1718.9619291849435\n",
      "Train Epoch: 755 [512/54000 (1%)] Loss: -1891.549683\n",
      "Train Epoch: 755 [11776/54000 (22%)] Loss: -1634.112549\n",
      "Train Epoch: 755 [23040/54000 (43%)] Loss: -1610.182373\n",
      "Train Epoch: 755 [34304/54000 (64%)] Loss: -1551.191040\n",
      "Train Epoch: 755 [45568/54000 (84%)] Loss: -1655.744385\n",
      "    epoch          : 755\n",
      "    loss           : -1673.6967265818378\n",
      "    val_loss       : -1676.3473262680695\n",
      "    val_log_likelihood: 1757.6810668945313\n",
      "    val_log_marginal: 1704.935673565045\n",
      "Train Epoch: 756 [512/54000 (1%)] Loss: -1921.280029\n",
      "Train Epoch: 756 [11776/54000 (22%)] Loss: -1631.259277\n",
      "Train Epoch: 756 [23040/54000 (43%)] Loss: -1538.980835\n",
      "Train Epoch: 756 [34304/54000 (64%)] Loss: -1598.779053\n",
      "Train Epoch: 756 [45568/54000 (84%)] Loss: -1621.566650\n",
      "    epoch          : 756\n",
      "    loss           : -1662.0329215172494\n",
      "    val_loss       : -1656.2257195938378\n",
      "    val_log_likelihood: 1756.207763671875\n",
      "    val_log_marginal: 1714.9271982726088\n",
      "Train Epoch: 757 [512/54000 (1%)] Loss: -1907.386963\n",
      "Train Epoch: 757 [11776/54000 (22%)] Loss: -1592.409058\n",
      "Train Epoch: 757 [23040/54000 (43%)] Loss: -1571.851807\n",
      "Train Epoch: 757 [34304/54000 (64%)] Loss: -1639.263916\n",
      "Train Epoch: 757 [45568/54000 (84%)] Loss: -1566.919556\n",
      "    epoch          : 757\n",
      "    loss           : -1657.3824209081065\n",
      "    val_loss       : -1672.1487266397103\n",
      "    val_log_likelihood: 1756.637890625\n",
      "    val_log_marginal: 1724.539488530159\n",
      "Train Epoch: 758 [512/54000 (1%)] Loss: -1934.598267\n",
      "Train Epoch: 758 [11776/54000 (22%)] Loss: -1717.481567\n",
      "Train Epoch: 758 [23040/54000 (43%)] Loss: -1668.277588\n",
      "Train Epoch: 758 [34304/54000 (64%)] Loss: -1620.066162\n",
      "Train Epoch: 758 [45568/54000 (84%)] Loss: -1654.806274\n",
      "    epoch          : 758\n",
      "    loss           : -1651.845947265625\n",
      "    val_loss       : -1651.968363676127\n",
      "    val_log_likelihood: 1753.3496337890624\n",
      "    val_log_marginal: 1714.0702291641385\n",
      "Train Epoch: 759 [512/54000 (1%)] Loss: -1881.463623\n",
      "Train Epoch: 759 [11776/54000 (22%)] Loss: -1545.921143\n",
      "Train Epoch: 759 [23040/54000 (43%)] Loss: -1639.981201\n",
      "Train Epoch: 759 [34304/54000 (64%)] Loss: -1911.944824\n",
      "Train Epoch: 759 [45568/54000 (84%)] Loss: -1635.057373\n",
      "    epoch          : 759\n",
      "    loss           : -1651.6157021097617\n",
      "    val_loss       : -1646.0918316804805\n",
      "    val_log_likelihood: 1745.9894165039063\n",
      "    val_log_marginal: 1709.4430611886085\n",
      "Train Epoch: 760 [512/54000 (1%)] Loss: -1917.883179\n",
      "Train Epoch: 760 [11776/54000 (22%)] Loss: -1606.075684\n",
      "Train Epoch: 760 [23040/54000 (43%)] Loss: -1640.710205\n",
      "Train Epoch: 760 [34304/54000 (64%)] Loss: -1628.247192\n",
      "Train Epoch: 760 [45568/54000 (84%)] Loss: -1491.255249\n",
      "    epoch          : 760\n",
      "    loss           : -1652.780518786742\n",
      "    val_loss       : -1644.7676463111304\n",
      "    val_log_likelihood: 1752.2068237304688\n",
      "    val_log_marginal: 1710.4024502001703\n",
      "Train Epoch: 761 [512/54000 (1%)] Loss: -1755.503662\n",
      "Train Epoch: 761 [11776/54000 (22%)] Loss: -1524.930786\n",
      "Train Epoch: 761 [23040/54000 (43%)] Loss: -1721.978638\n",
      "Train Epoch: 761 [34304/54000 (64%)] Loss: -1670.120361\n",
      "Train Epoch: 761 [45568/54000 (84%)] Loss: -1675.784424\n",
      "    epoch          : 761\n",
      "    loss           : -1660.5505189801206\n",
      "    val_loss       : -1642.279660046473\n",
      "    val_log_likelihood: 1745.691015625\n",
      "    val_log_marginal: 1714.4001867029815\n",
      "Train Epoch: 762 [512/54000 (1%)] Loss: -1902.265869\n",
      "Train Epoch: 762 [11776/54000 (22%)] Loss: -1604.025879\n",
      "Train Epoch: 762 [23040/54000 (43%)] Loss: -1558.115234\n",
      "Train Epoch: 762 [34304/54000 (64%)] Loss: -1598.629272\n",
      "Train Epoch: 762 [45568/54000 (84%)] Loss: -1628.694824\n",
      "    epoch          : 762\n",
      "    loss           : -1661.831868086711\n",
      "    val_loss       : -1641.396271591168\n",
      "    val_log_likelihood: 1708.0334716796874\n",
      "    val_log_marginal: 1674.1641723874957\n",
      "Train Epoch: 763 [512/54000 (1%)] Loss: -1913.369873\n",
      "Train Epoch: 763 [11776/54000 (22%)] Loss: -1581.316040\n",
      "Train Epoch: 763 [23040/54000 (43%)] Loss: -1665.968994\n",
      "Train Epoch: 763 [34304/54000 (64%)] Loss: -1909.359375\n",
      "Train Epoch: 763 [45568/54000 (84%)] Loss: -1540.844971\n",
      "    epoch          : 763\n",
      "    loss           : -1647.4810984394337\n",
      "    val_loss       : -1647.8354646809398\n",
      "    val_log_likelihood: 1715.4405029296875\n",
      "    val_log_marginal: 1683.937005380541\n",
      "Train Epoch: 764 [512/54000 (1%)] Loss: -1919.654541\n",
      "Train Epoch: 764 [11776/54000 (22%)] Loss: -1609.021484\n",
      "Train Epoch: 764 [23040/54000 (43%)] Loss: -1618.020020\n",
      "Train Epoch: 764 [34304/54000 (64%)] Loss: -1623.326782\n",
      "Train Epoch: 764 [45568/54000 (84%)] Loss: -1579.759521\n",
      "    epoch          : 764\n",
      "    loss           : -1646.3372210512066\n",
      "    val_loss       : -1656.9116438547148\n",
      "    val_log_likelihood: 1737.1176635742188\n",
      "    val_log_marginal: 1692.102792667225\n",
      "Train Epoch: 765 [512/54000 (1%)] Loss: -1908.315674\n",
      "Train Epoch: 765 [11776/54000 (22%)] Loss: -1620.615967\n",
      "Train Epoch: 765 [23040/54000 (43%)] Loss: -1590.684448\n",
      "Train Epoch: 765 [34304/54000 (64%)] Loss: -1640.337769\n",
      "Train Epoch: 765 [45568/54000 (84%)] Loss: -1642.448730\n",
      "    epoch          : 765\n",
      "    loss           : -1658.786919622138\n",
      "    val_loss       : -1653.1605869049206\n",
      "    val_log_likelihood: 1735.4562255859375\n",
      "    val_log_marginal: 1693.6527228910477\n",
      "Train Epoch: 766 [512/54000 (1%)] Loss: -1935.506592\n",
      "Train Epoch: 766 [11776/54000 (22%)] Loss: -1694.705078\n",
      "Train Epoch: 766 [23040/54000 (43%)] Loss: -1595.096924\n",
      "Train Epoch: 766 [34304/54000 (64%)] Loss: -1900.481934\n",
      "Train Epoch: 766 [45568/54000 (84%)] Loss: -1644.347656\n",
      "    epoch          : 766\n",
      "    loss           : -1658.7705525313274\n",
      "    val_loss       : -1647.6328935338183\n",
      "    val_log_likelihood: 1730.96962890625\n",
      "    val_log_marginal: 1698.0629601169378\n",
      "Train Epoch: 767 [512/54000 (1%)] Loss: -1909.595947\n",
      "Train Epoch: 767 [11776/54000 (22%)] Loss: -1610.136963\n",
      "Train Epoch: 767 [23040/54000 (43%)] Loss: -1635.204346\n",
      "Train Epoch: 767 [34304/54000 (64%)] Loss: -1918.837036\n",
      "Train Epoch: 767 [45568/54000 (84%)] Loss: -1520.742676\n",
      "    epoch          : 767\n",
      "    loss           : -1652.879151599242\n",
      "    val_loss       : -1641.3671613902784\n",
      "    val_log_likelihood: 1733.768603515625\n",
      "    val_log_marginal: 1694.2928300097585\n",
      "Train Epoch: 768 [512/54000 (1%)] Loss: -1903.445679\n",
      "Train Epoch: 768 [11776/54000 (22%)] Loss: -1539.356812\n",
      "Train Epoch: 768 [23040/54000 (43%)] Loss: -1671.225708\n",
      "Train Epoch: 768 [34304/54000 (64%)] Loss: -1589.895020\n",
      "Train Epoch: 768 [45568/54000 (84%)] Loss: -1666.003662\n",
      "    epoch          : 768\n",
      "    loss           : -1646.0062014135983\n",
      "    val_loss       : -1646.0580826832913\n",
      "    val_log_likelihood: 1743.7998901367187\n",
      "    val_log_marginal: 1699.3176954757423\n",
      "Train Epoch: 769 [512/54000 (1%)] Loss: -1914.855225\n",
      "Train Epoch: 769 [11776/54000 (22%)] Loss: -1556.530518\n",
      "Train Epoch: 769 [23040/54000 (43%)] Loss: -1542.627441\n",
      "Train Epoch: 769 [34304/54000 (64%)] Loss: -1659.890625\n",
      "Train Epoch: 769 [45568/54000 (84%)] Loss: -1625.994385\n",
      "    epoch          : 769\n",
      "    loss           : -1653.6432984607054\n",
      "    val_loss       : -1659.026644492615\n",
      "    val_log_likelihood: 1730.0959594726562\n",
      "    val_log_marginal: 1698.5452480491251\n",
      "Train Epoch: 770 [512/54000 (1%)] Loss: -1902.547607\n",
      "Train Epoch: 770 [11776/54000 (22%)] Loss: -1586.960205\n",
      "Train Epoch: 770 [23040/54000 (43%)] Loss: -1541.950439\n",
      "Train Epoch: 770 [34304/54000 (64%)] Loss: -1587.959717\n",
      "Train Epoch: 770 [45568/54000 (84%)] Loss: -1626.293457\n",
      "    epoch          : 770\n",
      "    loss           : -1660.903045956451\n",
      "    val_loss       : -1665.9997292981484\n",
      "    val_log_likelihood: 1750.2033447265626\n",
      "    val_log_marginal: 1713.115971276909\n",
      "Train Epoch: 771 [512/54000 (1%)] Loss: -1924.001465\n",
      "Train Epoch: 771 [11776/54000 (22%)] Loss: -1613.708862\n",
      "Train Epoch: 771 [23040/54000 (43%)] Loss: -1566.234863\n",
      "Train Epoch: 771 [34304/54000 (64%)] Loss: -1563.743652\n",
      "Train Epoch: 771 [45568/54000 (84%)] Loss: -1568.747925\n",
      "    epoch          : 771\n",
      "    loss           : -1661.006866152924\n",
      "    val_loss       : -1653.0207355805674\n",
      "    val_log_likelihood: 1730.7926025390625\n",
      "    val_log_marginal: 1691.0752081062644\n",
      "Train Epoch: 772 [512/54000 (1%)] Loss: -1727.241211\n",
      "Train Epoch: 772 [11776/54000 (22%)] Loss: -1561.367432\n",
      "Train Epoch: 772 [23040/54000 (43%)] Loss: -1686.887085\n",
      "Train Epoch: 772 [34304/54000 (64%)] Loss: -1918.904297\n",
      "Train Epoch: 772 [45568/54000 (84%)] Loss: -1570.935303\n",
      "    epoch          : 772\n",
      "    loss           : -1659.738903687732\n",
      "    val_loss       : -1666.3835285661742\n",
      "    val_log_likelihood: 1737.3364379882812\n",
      "    val_log_marginal: 1705.8333390891553\n",
      "Train Epoch: 773 [512/54000 (1%)] Loss: -1934.476074\n",
      "Train Epoch: 773 [11776/54000 (22%)] Loss: -1593.373779\n",
      "Train Epoch: 773 [23040/54000 (43%)] Loss: -1550.749512\n",
      "Train Epoch: 773 [34304/54000 (64%)] Loss: -1634.328491\n",
      "Train Epoch: 773 [45568/54000 (84%)] Loss: -1652.292725\n",
      "    epoch          : 773\n",
      "    loss           : -1665.2156088045328\n",
      "    val_loss       : -1651.3154654875398\n",
      "    val_log_likelihood: 1737.7937866210937\n",
      "    val_log_marginal: 1694.088277627155\n",
      "Train Epoch: 774 [512/54000 (1%)] Loss: -1947.514160\n",
      "Train Epoch: 774 [11776/54000 (22%)] Loss: -1649.578857\n",
      "Train Epoch: 774 [23040/54000 (43%)] Loss: -1594.721436\n",
      "Train Epoch: 774 [34304/54000 (64%)] Loss: -1683.265137\n",
      "Train Epoch: 774 [45568/54000 (84%)] Loss: -1536.595459\n",
      "    epoch          : 774\n",
      "    loss           : -1665.421571637144\n",
      "    val_loss       : -1656.6463808549568\n",
      "    val_log_likelihood: 1737.236865234375\n",
      "    val_log_marginal: 1701.9398116778582\n",
      "Train Epoch: 775 [512/54000 (1%)] Loss: -1912.144287\n",
      "Train Epoch: 775 [11776/54000 (22%)] Loss: -1722.912598\n",
      "Train Epoch: 775 [23040/54000 (43%)] Loss: -1580.612427\n",
      "Train Epoch: 775 [34304/54000 (64%)] Loss: -1652.600098\n",
      "Train Epoch: 775 [45568/54000 (84%)] Loss: -1692.062256\n",
      "    epoch          : 775\n",
      "    loss           : -1663.3497229849938\n",
      "    val_loss       : -1661.3796391914598\n",
      "    val_log_likelihood: 1717.668115234375\n",
      "    val_log_marginal: 1675.953542742133\n",
      "Train Epoch: 776 [512/54000 (1%)] Loss: -1917.111572\n",
      "Train Epoch: 776 [11776/54000 (22%)] Loss: -1746.069824\n",
      "Train Epoch: 776 [23040/54000 (43%)] Loss: -1614.434082\n",
      "Train Epoch: 776 [34304/54000 (64%)] Loss: -1629.889648\n",
      "Train Epoch: 776 [45568/54000 (84%)] Loss: -1658.525391\n",
      "    epoch          : 776\n",
      "    loss           : -1662.2917758450649\n",
      "    val_loss       : -1666.6961505483837\n",
      "    val_log_likelihood: 1739.2191650390625\n",
      "    val_log_marginal: 1704.670282433927\n",
      "Train Epoch: 777 [512/54000 (1%)] Loss: -1922.061279\n",
      "Train Epoch: 777 [11776/54000 (22%)] Loss: -1731.937988\n",
      "Train Epoch: 777 [23040/54000 (43%)] Loss: -1517.893555\n",
      "Train Epoch: 777 [34304/54000 (64%)] Loss: -1603.098877\n",
      "Train Epoch: 777 [45568/54000 (84%)] Loss: -1648.483521\n",
      "    epoch          : 777\n",
      "    loss           : -1654.5500669573794\n",
      "    val_loss       : -1633.625679346826\n",
      "    val_log_likelihood: 1707.2740356445313\n",
      "    val_log_marginal: 1673.9898338291794\n",
      "Train Epoch: 778 [512/54000 (1%)] Loss: -1882.657715\n",
      "Train Epoch: 778 [11776/54000 (22%)] Loss: -1556.643677\n",
      "Train Epoch: 778 [23040/54000 (43%)] Loss: -1642.636108\n",
      "Train Epoch: 778 [34304/54000 (64%)] Loss: -1914.584717\n",
      "Train Epoch: 778 [45568/54000 (84%)] Loss: -1660.572388\n",
      "    epoch          : 778\n",
      "    loss           : -1658.855034856513\n",
      "    val_loss       : -1652.8818090276793\n",
      "    val_log_likelihood: 1729.64375\n",
      "    val_log_marginal: 1688.466681676358\n",
      "Train Epoch: 779 [512/54000 (1%)] Loss: -1910.576660\n",
      "Train Epoch: 779 [11776/54000 (22%)] Loss: -1573.964355\n",
      "Train Epoch: 779 [23040/54000 (43%)] Loss: -1530.298706\n",
      "Train Epoch: 779 [34304/54000 (64%)] Loss: -1706.005981\n",
      "Train Epoch: 779 [45568/54000 (84%)] Loss: -1651.585938\n",
      "    epoch          : 779\n",
      "    loss           : -1666.4044709158416\n",
      "    val_loss       : -1670.8268765673042\n",
      "    val_log_likelihood: 1755.2364624023437\n",
      "    val_log_marginal: 1707.873429515958\n",
      "Train Epoch: 780 [512/54000 (1%)] Loss: -1936.853149\n",
      "Train Epoch: 780 [11776/54000 (22%)] Loss: -1761.058350\n",
      "Train Epoch: 780 [23040/54000 (43%)] Loss: -1711.427979\n",
      "Train Epoch: 780 [34304/54000 (64%)] Loss: -1717.732178\n",
      "Train Epoch: 780 [45568/54000 (84%)] Loss: -1655.129883\n",
      "    epoch          : 780\n",
      "    loss           : -1671.3499405360458\n",
      "    val_loss       : -1662.7710094059817\n",
      "    val_log_likelihood: 1752.4801635742188\n",
      "    val_log_marginal: 1717.9077142383903\n",
      "Train Epoch: 781 [512/54000 (1%)] Loss: -1924.840454\n",
      "Train Epoch: 781 [11776/54000 (22%)] Loss: -1597.935791\n",
      "Train Epoch: 781 [23040/54000 (43%)] Loss: -1636.713379\n",
      "Train Epoch: 781 [34304/54000 (64%)] Loss: -1618.582153\n",
      "Train Epoch: 781 [45568/54000 (84%)] Loss: -1655.021729\n",
      "    epoch          : 781\n",
      "    loss           : -1673.6524658203125\n",
      "    val_loss       : -1668.9394303365611\n",
      "    val_log_likelihood: 1758.2239135742188\n",
      "    val_log_marginal: 1727.7810200043023\n",
      "Train Epoch: 782 [512/54000 (1%)] Loss: -1580.705566\n",
      "Train Epoch: 782 [11776/54000 (22%)] Loss: -1742.449219\n",
      "Train Epoch: 782 [23040/54000 (43%)] Loss: -1647.413818\n",
      "Train Epoch: 782 [34304/54000 (64%)] Loss: -1614.435181\n",
      "Train Epoch: 782 [45568/54000 (84%)] Loss: -1673.707520\n",
      "    epoch          : 782\n",
      "    loss           : -1665.4759823638615\n",
      "    val_loss       : -1657.0433704013005\n",
      "    val_log_likelihood: 1728.4876953125\n",
      "    val_log_marginal: 1696.015732182935\n",
      "Train Epoch: 783 [512/54000 (1%)] Loss: -1893.907104\n",
      "Train Epoch: 783 [11776/54000 (22%)] Loss: -1732.575195\n",
      "Train Epoch: 783 [23040/54000 (43%)] Loss: -1639.884888\n",
      "Train Epoch: 783 [34304/54000 (64%)] Loss: -1640.046753\n",
      "Train Epoch: 783 [45568/54000 (84%)] Loss: -1617.900879\n",
      "    epoch          : 783\n",
      "    loss           : -1664.7921311784498\n",
      "    val_loss       : -1653.1436817447654\n",
      "    val_log_likelihood: 1754.2549194335938\n",
      "    val_log_marginal: 1713.9261653386868\n",
      "Train Epoch: 784 [512/54000 (1%)] Loss: -1914.349976\n",
      "Train Epoch: 784 [11776/54000 (22%)] Loss: -1644.170166\n",
      "Train Epoch: 784 [23040/54000 (43%)] Loss: -1564.531982\n",
      "Train Epoch: 784 [34304/54000 (64%)] Loss: -1534.597046\n",
      "Train Epoch: 784 [45568/54000 (84%)] Loss: -1667.600220\n",
      "    epoch          : 784\n",
      "    loss           : -1667.6220268022896\n",
      "    val_loss       : -1671.5055428996682\n",
      "    val_log_likelihood: 1753.9828369140625\n",
      "    val_log_marginal: 1708.6868180087135\n",
      "Train Epoch: 785 [512/54000 (1%)] Loss: -1918.066284\n",
      "Train Epoch: 785 [11776/54000 (22%)] Loss: -1703.773193\n",
      "Train Epoch: 785 [23040/54000 (43%)] Loss: -1650.427002\n",
      "Train Epoch: 785 [34304/54000 (64%)] Loss: -1595.987549\n",
      "Train Epoch: 785 [45568/54000 (84%)] Loss: -1552.412720\n",
      "    epoch          : 785\n",
      "    loss           : -1652.253987227336\n",
      "    val_loss       : -1644.1762860811316\n",
      "    val_log_likelihood: 1726.19150390625\n",
      "    val_log_marginal: 1693.7238644067197\n",
      "Train Epoch: 786 [512/54000 (1%)] Loss: -1574.243530\n",
      "Train Epoch: 786 [11776/54000 (22%)] Loss: -1569.007935\n",
      "Train Epoch: 786 [23040/54000 (43%)] Loss: -1563.237061\n",
      "Train Epoch: 786 [34304/54000 (64%)] Loss: -1622.135742\n",
      "Train Epoch: 786 [45568/54000 (84%)] Loss: -1580.604004\n",
      "    epoch          : 786\n",
      "    loss           : -1662.3579862991182\n",
      "    val_loss       : -1663.6707790916785\n",
      "    val_log_likelihood: 1742.0595703125\n",
      "    val_log_marginal: 1701.9187944620849\n",
      "Train Epoch: 787 [512/54000 (1%)] Loss: -1920.879883\n",
      "Train Epoch: 787 [11776/54000 (22%)] Loss: -1756.847900\n",
      "Train Epoch: 787 [23040/54000 (43%)] Loss: -1632.754639\n",
      "Train Epoch: 787 [34304/54000 (64%)] Loss: -1526.254395\n",
      "Train Epoch: 787 [45568/54000 (84%)] Loss: -1637.529297\n",
      "    epoch          : 787\n",
      "    loss           : -1659.0470707959469\n",
      "    val_loss       : -1654.8181657949463\n",
      "    val_log_likelihood: 1728.43828125\n",
      "    val_log_marginal: 1683.7516279064118\n",
      "Train Epoch: 788 [512/54000 (1%)] Loss: -1922.849609\n",
      "Train Epoch: 788 [11776/54000 (22%)] Loss: -1645.064697\n",
      "Train Epoch: 788 [23040/54000 (43%)] Loss: -1682.140137\n",
      "Train Epoch: 788 [34304/54000 (64%)] Loss: -1921.926758\n",
      "Train Epoch: 788 [45568/54000 (84%)] Loss: -1587.817383\n",
      "    epoch          : 788\n",
      "    loss           : -1663.3433342357673\n",
      "    val_loss       : -1647.0008341956884\n",
      "    val_log_likelihood: 1732.2693603515625\n",
      "    val_log_marginal: 1701.4786923900247\n",
      "Train Epoch: 789 [512/54000 (1%)] Loss: -1925.760132\n",
      "Train Epoch: 789 [11776/54000 (22%)] Loss: -1639.415771\n",
      "Train Epoch: 789 [23040/54000 (43%)] Loss: -1663.397949\n",
      "Train Epoch: 789 [34304/54000 (64%)] Loss: -1655.598877\n",
      "Train Epoch: 789 [45568/54000 (84%)] Loss: -1694.649658\n",
      "    epoch          : 789\n",
      "    loss           : -1665.0885239402846\n",
      "    val_loss       : -1667.1199342004954\n",
      "    val_log_likelihood: 1743.9361938476563\n",
      "    val_log_marginal: 1711.1943984217942\n",
      "Train Epoch: 790 [512/54000 (1%)] Loss: -1917.959717\n",
      "Train Epoch: 790 [11776/54000 (22%)] Loss: -1728.786987\n",
      "Train Epoch: 790 [23040/54000 (43%)] Loss: -1566.980225\n",
      "Train Epoch: 790 [34304/54000 (64%)] Loss: -1706.019043\n",
      "Train Epoch: 790 [45568/54000 (84%)] Loss: -1673.085938\n",
      "    epoch          : 790\n",
      "    loss           : -1667.2346517732828\n",
      "    val_loss       : -1669.9158348471858\n",
      "    val_log_likelihood: 1742.3585571289063\n",
      "    val_log_marginal: 1696.023809885249\n",
      "Train Epoch: 791 [512/54000 (1%)] Loss: -1911.396240\n",
      "Train Epoch: 791 [11776/54000 (22%)] Loss: -1615.661865\n",
      "Train Epoch: 791 [23040/54000 (43%)] Loss: -1694.465820\n",
      "Train Epoch: 791 [34304/54000 (64%)] Loss: -1590.397827\n",
      "Train Epoch: 791 [45568/54000 (84%)] Loss: -1585.503296\n",
      "    epoch          : 791\n",
      "    loss           : -1672.6719221360613\n",
      "    val_loss       : -1678.6268135232851\n",
      "    val_log_likelihood: 1741.4213989257812\n",
      "    val_log_marginal: 1701.5118340294807\n",
      "Train Epoch: 792 [512/54000 (1%)] Loss: -1932.437866\n",
      "Train Epoch: 792 [11776/54000 (22%)] Loss: -1730.541016\n",
      "Train Epoch: 792 [23040/54000 (43%)] Loss: -1702.170898\n",
      "Train Epoch: 792 [34304/54000 (64%)] Loss: -1698.155396\n",
      "Train Epoch: 792 [45568/54000 (84%)] Loss: -1647.150269\n",
      "    epoch          : 792\n",
      "    loss           : -1674.4397323155167\n",
      "    val_loss       : -1669.338494723756\n",
      "    val_log_likelihood: 1760.2110107421875\n",
      "    val_log_marginal: 1725.3978362023831\n",
      "Train Epoch: 793 [512/54000 (1%)] Loss: -1925.307251\n",
      "Train Epoch: 793 [11776/54000 (22%)] Loss: -1772.347656\n",
      "Train Epoch: 793 [23040/54000 (43%)] Loss: -1637.141602\n",
      "Train Epoch: 793 [34304/54000 (64%)] Loss: -1709.985107\n",
      "Train Epoch: 793 [45568/54000 (84%)] Loss: -1654.232910\n",
      "    epoch          : 793\n",
      "    loss           : -1674.4062524172339\n",
      "    val_loss       : -1672.057204761263\n",
      "    val_log_likelihood: 1740.2880981445312\n",
      "    val_log_marginal: 1708.8082612641156\n",
      "Train Epoch: 794 [512/54000 (1%)] Loss: -1926.790405\n",
      "Train Epoch: 794 [11776/54000 (22%)] Loss: -1603.552368\n",
      "Train Epoch: 794 [23040/54000 (43%)] Loss: -1696.263062\n",
      "Train Epoch: 794 [34304/54000 (64%)] Loss: -1924.862061\n",
      "Train Epoch: 794 [45568/54000 (84%)] Loss: -1655.803345\n",
      "    epoch          : 794\n",
      "    loss           : -1677.121933738784\n",
      "    val_loss       : -1684.1475491927006\n",
      "    val_log_likelihood: 1758.31171875\n",
      "    val_log_marginal: 1704.1597593568265\n",
      "Train Epoch: 795 [512/54000 (1%)] Loss: -1734.561890\n",
      "Train Epoch: 795 [11776/54000 (22%)] Loss: -1621.017944\n",
      "Train Epoch: 795 [23040/54000 (43%)] Loss: -1699.010132\n",
      "Train Epoch: 795 [34304/54000 (64%)] Loss: -1592.536377\n",
      "Train Epoch: 795 [45568/54000 (84%)] Loss: -1624.399780\n",
      "    epoch          : 795\n",
      "    loss           : -1678.4317892848856\n",
      "    val_loss       : -1681.6864196005276\n",
      "    val_log_likelihood: 1757.8284057617188\n",
      "    val_log_marginal: 1711.9843677863478\n",
      "Train Epoch: 796 [512/54000 (1%)] Loss: -1923.207275\n",
      "Train Epoch: 796 [11776/54000 (22%)] Loss: -1667.345093\n",
      "Train Epoch: 796 [23040/54000 (43%)] Loss: -1711.196777\n",
      "Train Epoch: 796 [34304/54000 (64%)] Loss: -1665.815552\n",
      "Train Epoch: 796 [45568/54000 (84%)] Loss: -1706.057129\n",
      "    epoch          : 796\n",
      "    loss           : -1677.3860056659962\n",
      "    val_loss       : -1682.7671274165623\n",
      "    val_log_likelihood: 1735.9613159179687\n",
      "    val_log_marginal: 1700.471852671355\n",
      "Train Epoch: 797 [512/54000 (1%)] Loss: -1925.307617\n",
      "Train Epoch: 797 [11776/54000 (22%)] Loss: -1565.780273\n",
      "Train Epoch: 797 [23040/54000 (43%)] Loss: -1526.474731\n",
      "Train Epoch: 797 [34304/54000 (64%)] Loss: -1653.084229\n",
      "Train Epoch: 797 [45568/54000 (84%)] Loss: -1658.276855\n",
      "    epoch          : 797\n",
      "    loss           : -1677.212975228187\n",
      "    val_loss       : -1674.7569519469514\n",
      "    val_log_likelihood: 1738.6630859375\n",
      "    val_log_marginal: 1701.266316819191\n",
      "Train Epoch: 798 [512/54000 (1%)] Loss: -1919.645752\n",
      "Train Epoch: 798 [11776/54000 (22%)] Loss: -1589.938721\n",
      "Train Epoch: 798 [23040/54000 (43%)] Loss: -1709.994263\n",
      "Train Epoch: 798 [34304/54000 (64%)] Loss: -1718.117188\n",
      "Train Epoch: 798 [45568/54000 (84%)] Loss: -1705.088867\n",
      "    epoch          : 798\n",
      "    loss           : -1678.9527563718286\n",
      "    val_loss       : -1667.1446906415745\n",
      "    val_log_likelihood: 1758.0503173828124\n",
      "    val_log_marginal: 1716.0951517678798\n",
      "Train Epoch: 799 [512/54000 (1%)] Loss: -1896.729980\n",
      "Train Epoch: 799 [11776/54000 (22%)] Loss: -1588.565430\n",
      "Train Epoch: 799 [23040/54000 (43%)] Loss: -1654.313965\n",
      "Train Epoch: 799 [34304/54000 (64%)] Loss: -1562.246826\n",
      "Train Epoch: 799 [45568/54000 (84%)] Loss: -1606.728271\n",
      "    epoch          : 799\n",
      "    loss           : -1671.0485271793782\n",
      "    val_loss       : -1663.3398776126094\n",
      "    val_log_likelihood: 1742.8688110351563\n",
      "    val_log_marginal: 1709.550390401855\n",
      "Train Epoch: 800 [512/54000 (1%)] Loss: -1940.819824\n",
      "Train Epoch: 800 [11776/54000 (22%)] Loss: -1590.821289\n",
      "Train Epoch: 800 [23040/54000 (43%)] Loss: -1561.854370\n",
      "Train Epoch: 800 [34304/54000 (64%)] Loss: -1657.607544\n",
      "Train Epoch: 800 [45568/54000 (84%)] Loss: -1696.485107\n",
      "    epoch          : 800\n",
      "    loss           : -1668.9481382464419\n",
      "    val_loss       : -1675.6225523091853\n",
      "    val_log_likelihood: 1751.35078125\n",
      "    val_log_marginal: 1719.1242119763047\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch800.pth ...\n",
      "Train Epoch: 801 [512/54000 (1%)] Loss: -1931.769775\n",
      "Train Epoch: 801 [11776/54000 (22%)] Loss: -1654.946045\n",
      "Train Epoch: 801 [23040/54000 (43%)] Loss: -1674.230347\n",
      "Train Epoch: 801 [34304/54000 (64%)] Loss: -1652.633423\n",
      "Train Epoch: 801 [45568/54000 (84%)] Loss: -1575.326294\n",
      "    epoch          : 801\n",
      "    loss           : -1666.7532318417389\n",
      "    val_loss       : -1642.6073909257539\n",
      "    val_log_likelihood: 1737.0126098632813\n",
      "    val_log_marginal: 1700.349904051423\n",
      "Train Epoch: 802 [512/54000 (1%)] Loss: -1736.230225\n",
      "Train Epoch: 802 [11776/54000 (22%)] Loss: -1634.258301\n",
      "Train Epoch: 802 [23040/54000 (43%)] Loss: -1646.358643\n",
      "Train Epoch: 802 [34304/54000 (64%)] Loss: -1601.491211\n",
      "Train Epoch: 802 [45568/54000 (84%)] Loss: -1679.022705\n",
      "    epoch          : 802\n",
      "    loss           : -1658.4335877069152\n",
      "    val_loss       : -1662.4373606870881\n",
      "    val_log_likelihood: 1740.753759765625\n",
      "    val_log_marginal: 1692.0791394084692\n",
      "Train Epoch: 803 [512/54000 (1%)] Loss: -1934.379150\n",
      "Train Epoch: 803 [11776/54000 (22%)] Loss: -1721.703247\n",
      "Train Epoch: 803 [23040/54000 (43%)] Loss: -1711.189819\n",
      "Train Epoch: 803 [34304/54000 (64%)] Loss: -1654.049438\n",
      "Train Epoch: 803 [45568/54000 (84%)] Loss: -1609.110840\n",
      "    epoch          : 803\n",
      "    loss           : -1673.6243425123762\n",
      "    val_loss       : -1664.65311571192\n",
      "    val_log_likelihood: 1752.4014526367187\n",
      "    val_log_marginal: 1707.2199960727244\n",
      "Train Epoch: 804 [512/54000 (1%)] Loss: -1723.418579\n",
      "Train Epoch: 804 [11776/54000 (22%)] Loss: -1554.534668\n",
      "Train Epoch: 804 [23040/54000 (43%)] Loss: -1557.009033\n",
      "Train Epoch: 804 [34304/54000 (64%)] Loss: -1715.081787\n",
      "Train Epoch: 804 [45568/54000 (84%)] Loss: -1656.742310\n",
      "    epoch          : 804\n",
      "    loss           : -1673.1763021639078\n",
      "    val_loss       : -1681.7036142547615\n",
      "    val_log_likelihood: 1743.8011840820313\n",
      "    val_log_marginal: 1707.0473424129189\n",
      "Train Epoch: 805 [512/54000 (1%)] Loss: -1906.990845\n",
      "Train Epoch: 805 [11776/54000 (22%)] Loss: -1568.692139\n",
      "Train Epoch: 805 [23040/54000 (43%)] Loss: -1706.728027\n",
      "Train Epoch: 805 [34304/54000 (64%)] Loss: -1653.477417\n",
      "Train Epoch: 805 [45568/54000 (84%)] Loss: -1690.176025\n",
      "    epoch          : 805\n",
      "    loss           : -1673.781876063583\n",
      "    val_loss       : -1651.8488983088173\n",
      "    val_log_likelihood: 1735.2012573242187\n",
      "    val_log_marginal: 1691.6107063952834\n",
      "Train Epoch: 806 [512/54000 (1%)] Loss: -1920.571411\n",
      "Train Epoch: 806 [11776/54000 (22%)] Loss: -1628.316650\n",
      "Train Epoch: 806 [23040/54000 (43%)] Loss: -1633.302979\n",
      "Train Epoch: 806 [34304/54000 (64%)] Loss: -1673.963989\n",
      "Train Epoch: 806 [45568/54000 (84%)] Loss: -1668.163940\n",
      "    epoch          : 806\n",
      "    loss           : -1672.6502625116027\n",
      "    val_loss       : -1674.6146037584172\n",
      "    val_log_likelihood: 1756.5386352539062\n",
      "    val_log_marginal: 1719.673939261213\n",
      "Train Epoch: 807 [512/54000 (1%)] Loss: -1929.990723\n",
      "Train Epoch: 807 [11776/54000 (22%)] Loss: -1734.655029\n",
      "Train Epoch: 807 [23040/54000 (43%)] Loss: -1568.433105\n",
      "Train Epoch: 807 [34304/54000 (64%)] Loss: -1716.351074\n",
      "Train Epoch: 807 [45568/54000 (84%)] Loss: -1682.819092\n",
      "    epoch          : 807\n",
      "    loss           : -1685.2761315071937\n",
      "    val_loss       : -1670.42504545236\n",
      "    val_log_likelihood: 1748.3197387695313\n",
      "    val_log_marginal: 1701.002326318249\n",
      "Train Epoch: 808 [512/54000 (1%)] Loss: -1749.819336\n",
      "Train Epoch: 808 [11776/54000 (22%)] Loss: -1609.375000\n",
      "Train Epoch: 808 [23040/54000 (43%)] Loss: -1572.459717\n",
      "Train Epoch: 808 [34304/54000 (64%)] Loss: -1930.346924\n",
      "Train Epoch: 808 [45568/54000 (84%)] Loss: -1659.745117\n",
      "    epoch          : 808\n",
      "    loss           : -1678.815647238552\n",
      "    val_loss       : -1660.103470350709\n",
      "    val_log_likelihood: 1752.4326171875\n",
      "    val_log_marginal: 1714.7960987888277\n",
      "Train Epoch: 809 [512/54000 (1%)] Loss: -1918.196167\n",
      "Train Epoch: 809 [11776/54000 (22%)] Loss: -1631.459839\n",
      "Train Epoch: 809 [23040/54000 (43%)] Loss: -1547.925415\n",
      "Train Epoch: 809 [34304/54000 (64%)] Loss: -1730.068481\n",
      "Train Epoch: 809 [45568/54000 (84%)] Loss: -1692.792236\n",
      "    epoch          : 809\n",
      "    loss           : -1678.389400671024\n",
      "    val_loss       : -1668.5298819985242\n",
      "    val_log_likelihood: 1758.4662353515625\n",
      "    val_log_marginal: 1724.7088862542064\n",
      "Train Epoch: 810 [512/54000 (1%)] Loss: -1912.591675\n",
      "Train Epoch: 810 [11776/54000 (22%)] Loss: -1621.383667\n",
      "Train Epoch: 810 [23040/54000 (43%)] Loss: -1624.575562\n",
      "Train Epoch: 810 [34304/54000 (64%)] Loss: -1674.586914\n",
      "Train Epoch: 810 [45568/54000 (84%)] Loss: -1690.940063\n",
      "    epoch          : 810\n",
      "    loss           : -1674.1733035852412\n",
      "    val_loss       : -1678.7829920608551\n",
      "    val_log_likelihood: 1755.5165893554688\n",
      "    val_log_marginal: 1713.8236885394901\n",
      "Train Epoch: 811 [512/54000 (1%)] Loss: -1935.535400\n",
      "Train Epoch: 811 [11776/54000 (22%)] Loss: -1612.603149\n",
      "Train Epoch: 811 [23040/54000 (43%)] Loss: -1559.021484\n",
      "Train Epoch: 811 [34304/54000 (64%)] Loss: -1669.345337\n",
      "Train Epoch: 811 [45568/54000 (84%)] Loss: -1639.009033\n",
      "    epoch          : 811\n",
      "    loss           : -1673.8123283763923\n",
      "    val_loss       : -1663.278906534426\n",
      "    val_log_likelihood: 1765.039111328125\n",
      "    val_log_marginal: 1723.8835407614708\n",
      "Train Epoch: 812 [512/54000 (1%)] Loss: -1924.188477\n",
      "Train Epoch: 812 [11776/54000 (22%)] Loss: -1633.577393\n",
      "Train Epoch: 812 [23040/54000 (43%)] Loss: -1598.100220\n",
      "Train Epoch: 812 [34304/54000 (64%)] Loss: -1918.378540\n",
      "Train Epoch: 812 [45568/54000 (84%)] Loss: -1604.784546\n",
      "    epoch          : 812\n",
      "    loss           : -1662.3146066193533\n",
      "    val_loss       : -1639.2412283632905\n",
      "    val_log_likelihood: 1768.2265747070312\n",
      "    val_log_marginal: 1721.8901100985706\n",
      "Train Epoch: 813 [512/54000 (1%)] Loss: -1930.574951\n",
      "Train Epoch: 813 [11776/54000 (22%)] Loss: -1579.964355\n",
      "Train Epoch: 813 [23040/54000 (43%)] Loss: -1593.136963\n",
      "Train Epoch: 813 [34304/54000 (64%)] Loss: -1744.145752\n",
      "Train Epoch: 813 [45568/54000 (84%)] Loss: -1617.727783\n",
      "    epoch          : 813\n",
      "    loss           : -1662.8935546875\n",
      "    val_loss       : -1658.662966587674\n",
      "    val_log_likelihood: 1763.1323974609375\n",
      "    val_log_marginal: 1724.9144711010158\n",
      "Train Epoch: 814 [512/54000 (1%)] Loss: -1921.647583\n",
      "Train Epoch: 814 [11776/54000 (22%)] Loss: -1608.968872\n",
      "Train Epoch: 814 [23040/54000 (43%)] Loss: -1727.830078\n",
      "Train Epoch: 814 [34304/54000 (64%)] Loss: -1679.103027\n",
      "Train Epoch: 814 [45568/54000 (84%)] Loss: -1571.571411\n",
      "    epoch          : 814\n",
      "    loss           : -1671.0342993405786\n",
      "    val_loss       : -1661.1384386776015\n",
      "    val_log_likelihood: 1740.6384887695312\n",
      "    val_log_marginal: 1700.7691990640014\n",
      "Train Epoch: 815 [512/54000 (1%)] Loss: -1914.878052\n",
      "Train Epoch: 815 [11776/54000 (22%)] Loss: -1581.343628\n",
      "Train Epoch: 815 [23040/54000 (43%)] Loss: -1532.733887\n",
      "Train Epoch: 815 [34304/54000 (64%)] Loss: -1672.312988\n",
      "Train Epoch: 815 [45568/54000 (84%)] Loss: -1643.152222\n",
      "    epoch          : 815\n",
      "    loss           : -1672.6446255221226\n",
      "    val_loss       : -1678.5961549727247\n",
      "    val_log_likelihood: 1762.5276977539063\n",
      "    val_log_marginal: 1719.3769838169217\n",
      "Train Epoch: 816 [512/54000 (1%)] Loss: -1926.945068\n",
      "Train Epoch: 816 [11776/54000 (22%)] Loss: -1623.198608\n",
      "Train Epoch: 816 [23040/54000 (43%)] Loss: -1675.430054\n",
      "Train Epoch: 816 [34304/54000 (64%)] Loss: -1727.682983\n",
      "Train Epoch: 816 [45568/54000 (84%)] Loss: -1666.843018\n",
      "    epoch          : 816\n",
      "    loss           : -1679.2824235670637\n",
      "    val_loss       : -1682.6988698568196\n",
      "    val_log_likelihood: 1759.2066040039062\n",
      "    val_log_marginal: 1711.594405398518\n",
      "Train Epoch: 817 [512/54000 (1%)] Loss: -1927.145752\n",
      "Train Epoch: 817 [11776/54000 (22%)] Loss: -1571.857300\n",
      "Train Epoch: 817 [23040/54000 (43%)] Loss: -1648.094238\n",
      "Train Epoch: 817 [34304/54000 (64%)] Loss: -1673.670532\n",
      "Train Epoch: 817 [45568/54000 (84%)] Loss: -1635.287842\n",
      "    epoch          : 817\n",
      "    loss           : -1677.63020430461\n",
      "    val_loss       : -1665.1416190982795\n",
      "    val_log_likelihood: 1745.148046875\n",
      "    val_log_marginal: 1704.0451944598171\n",
      "Train Epoch: 818 [512/54000 (1%)] Loss: -1925.800659\n",
      "Train Epoch: 818 [11776/54000 (22%)] Loss: -1609.662842\n",
      "Train Epoch: 818 [23040/54000 (43%)] Loss: -1678.128174\n",
      "Train Epoch: 818 [34304/54000 (64%)] Loss: -1721.958984\n",
      "Train Epoch: 818 [45568/54000 (84%)] Loss: -1568.982788\n",
      "    epoch          : 818\n",
      "    loss           : -1676.2516352587406\n",
      "    val_loss       : -1664.355029103998\n",
      "    val_log_likelihood: 1767.2251708984375\n",
      "    val_log_marginal: 1729.554687549919\n",
      "Train Epoch: 819 [512/54000 (1%)] Loss: -1945.002563\n",
      "Train Epoch: 819 [11776/54000 (22%)] Loss: -1604.763550\n",
      "Train Epoch: 819 [23040/54000 (43%)] Loss: -1614.934937\n",
      "Train Epoch: 819 [34304/54000 (64%)] Loss: -1930.625610\n",
      "Train Epoch: 819 [45568/54000 (84%)] Loss: -1679.811157\n",
      "    epoch          : 819\n",
      "    loss           : -1674.1235822923113\n",
      "    val_loss       : -1662.9189233131706\n",
      "    val_log_likelihood: 1756.3659057617188\n",
      "    val_log_marginal: 1714.2979530271143\n",
      "Train Epoch: 820 [512/54000 (1%)] Loss: -1930.951416\n",
      "Train Epoch: 820 [11776/54000 (22%)] Loss: -1709.197876\n",
      "Train Epoch: 820 [23040/54000 (43%)] Loss: -1621.513306\n",
      "Train Epoch: 820 [34304/54000 (64%)] Loss: -1676.676636\n",
      "Train Epoch: 820 [45568/54000 (84%)] Loss: -1685.690308\n",
      "    epoch          : 820\n",
      "    loss           : -1671.7541745629642\n",
      "    val_loss       : -1670.6975641098804\n",
      "    val_log_likelihood: 1760.8624389648437\n",
      "    val_log_marginal: 1728.2827678684146\n",
      "Train Epoch: 821 [512/54000 (1%)] Loss: -1939.160889\n",
      "Train Epoch: 821 [11776/54000 (22%)] Loss: -1619.308472\n",
      "Train Epoch: 821 [23040/54000 (43%)] Loss: -1622.421753\n",
      "Train Epoch: 821 [34304/54000 (64%)] Loss: -1901.417969\n",
      "Train Epoch: 821 [45568/54000 (84%)] Loss: -1650.728638\n",
      "    epoch          : 821\n",
      "    loss           : -1671.0110189607828\n",
      "    val_loss       : -1671.719720980525\n",
      "    val_log_likelihood: 1758.6333862304687\n",
      "    val_log_marginal: 1712.5437178332359\n",
      "Train Epoch: 822 [512/54000 (1%)] Loss: -1782.343750\n",
      "Train Epoch: 822 [11776/54000 (22%)] Loss: -1622.899170\n",
      "Train Epoch: 822 [23040/54000 (43%)] Loss: -1674.250488\n",
      "Train Epoch: 822 [34304/54000 (64%)] Loss: -1620.166382\n",
      "Train Epoch: 822 [45568/54000 (84%)] Loss: -1643.549561\n",
      "    epoch          : 822\n",
      "    loss           : -1673.8255663579052\n",
      "    val_loss       : -1668.2094003849663\n",
      "    val_log_likelihood: 1753.2601928710938\n",
      "    val_log_marginal: 1713.9045319747179\n",
      "Train Epoch: 823 [512/54000 (1%)] Loss: -1625.613525\n",
      "Train Epoch: 823 [11776/54000 (22%)] Loss: -1758.813110\n",
      "Train Epoch: 823 [23040/54000 (43%)] Loss: -1577.614502\n",
      "Train Epoch: 823 [34304/54000 (64%)] Loss: -1576.031738\n",
      "Train Epoch: 823 [45568/54000 (84%)] Loss: -1633.384277\n",
      "    epoch          : 823\n",
      "    loss           : -1672.2006038250308\n",
      "    val_loss       : -1658.5030757764355\n",
      "    val_log_likelihood: 1739.6211059570312\n",
      "    val_log_marginal: 1693.2052291251719\n",
      "Train Epoch: 824 [512/54000 (1%)] Loss: -1920.344727\n",
      "Train Epoch: 824 [11776/54000 (22%)] Loss: -1753.773560\n",
      "Train Epoch: 824 [23040/54000 (43%)] Loss: -1719.878418\n",
      "Train Epoch: 824 [34304/54000 (64%)] Loss: -1697.213013\n",
      "Train Epoch: 824 [45568/54000 (84%)] Loss: -1644.617188\n",
      "    epoch          : 824\n",
      "    loss           : -1670.7103754931156\n",
      "    val_loss       : -1662.87017659517\n",
      "    val_log_likelihood: 1741.591455078125\n",
      "    val_log_marginal: 1696.7472525892063\n",
      "Train Epoch: 825 [512/54000 (1%)] Loss: -1955.382568\n",
      "Train Epoch: 825 [11776/54000 (22%)] Loss: -1617.324707\n",
      "Train Epoch: 825 [23040/54000 (43%)] Loss: -1658.449097\n",
      "Train Epoch: 825 [34304/54000 (64%)] Loss: -1619.488159\n",
      "Train Epoch: 825 [45568/54000 (84%)] Loss: -1593.901123\n",
      "    epoch          : 825\n",
      "    loss           : -1676.710938708617\n",
      "    val_loss       : -1683.1392705874518\n",
      "    val_log_likelihood: 1752.2345703125\n",
      "    val_log_marginal: 1705.4629067473113\n",
      "Train Epoch: 826 [512/54000 (1%)] Loss: -1949.226562\n",
      "Train Epoch: 826 [11776/54000 (22%)] Loss: -1707.790039\n",
      "Train Epoch: 826 [23040/54000 (43%)] Loss: -1611.661865\n",
      "Train Epoch: 826 [34304/54000 (64%)] Loss: -1953.797607\n",
      "Train Epoch: 826 [45568/54000 (84%)] Loss: -1633.060303\n",
      "    epoch          : 826\n",
      "    loss           : -1680.983451616646\n",
      "    val_loss       : -1675.1989575616085\n",
      "    val_log_likelihood: 1752.2783447265624\n",
      "    val_log_marginal: 1721.3947375014425\n",
      "Train Epoch: 827 [512/54000 (1%)] Loss: -1929.427246\n",
      "Train Epoch: 827 [11776/54000 (22%)] Loss: -1626.840820\n",
      "Train Epoch: 827 [23040/54000 (43%)] Loss: -1635.785400\n",
      "Train Epoch: 827 [34304/54000 (64%)] Loss: -1727.735107\n",
      "Train Epoch: 827 [45568/54000 (84%)] Loss: -1660.266846\n",
      "    epoch          : 827\n",
      "    loss           : -1665.9381139774134\n",
      "    val_loss       : -1662.6720600765198\n",
      "    val_log_likelihood: 1760.9185180664062\n",
      "    val_log_marginal: 1724.995726726204\n",
      "Train Epoch: 828 [512/54000 (1%)] Loss: -1955.517822\n",
      "Train Epoch: 828 [11776/54000 (22%)] Loss: -1786.069458\n",
      "Train Epoch: 828 [23040/54000 (43%)] Loss: -1548.706665\n",
      "Train Epoch: 828 [34304/54000 (64%)] Loss: -1658.263062\n",
      "Train Epoch: 828 [45568/54000 (84%)] Loss: -1603.258423\n",
      "    epoch          : 828\n",
      "    loss           : -1667.9402338915531\n",
      "    val_loss       : -1675.3664248847404\n",
      "    val_log_likelihood: 1750.57763671875\n",
      "    val_log_marginal: 1704.3106345336885\n",
      "Train Epoch: 829 [512/54000 (1%)] Loss: -1913.057739\n",
      "Train Epoch: 829 [11776/54000 (22%)] Loss: -1728.847168\n",
      "Train Epoch: 829 [23040/54000 (43%)] Loss: -1548.277588\n",
      "Train Epoch: 829 [34304/54000 (64%)] Loss: -1711.195068\n",
      "Train Epoch: 829 [45568/54000 (84%)] Loss: -1549.937744\n",
      "    epoch          : 829\n",
      "    loss           : -1665.8406994508045\n",
      "    val_loss       : -1635.6496014063246\n",
      "    val_log_likelihood: 1716.9160400390624\n",
      "    val_log_marginal: 1670.49211910069\n",
      "Train Epoch: 830 [512/54000 (1%)] Loss: -1836.948975\n",
      "Train Epoch: 830 [11776/54000 (22%)] Loss: -1602.129639\n",
      "Train Epoch: 830 [23040/54000 (43%)] Loss: -1596.581055\n",
      "Train Epoch: 830 [34304/54000 (64%)] Loss: -1608.839111\n",
      "Train Epoch: 830 [45568/54000 (84%)] Loss: -1638.399902\n",
      "    epoch          : 830\n",
      "    loss           : -1660.6358304165378\n",
      "    val_loss       : -1667.2852098188364\n",
      "    val_log_likelihood: 1746.2396240234375\n",
      "    val_log_marginal: 1695.274775730446\n",
      "Train Epoch: 831 [512/54000 (1%)] Loss: -1732.037109\n",
      "Train Epoch: 831 [11776/54000 (22%)] Loss: -1614.607544\n",
      "Train Epoch: 831 [23040/54000 (43%)] Loss: -1722.070923\n",
      "Train Epoch: 831 [34304/54000 (64%)] Loss: -1927.706665\n",
      "Train Epoch: 831 [45568/54000 (84%)] Loss: -1637.272949\n",
      "    epoch          : 831\n",
      "    loss           : -1662.251657013846\n",
      "    val_loss       : -1658.1875175010414\n",
      "    val_log_likelihood: 1728.6413208007812\n",
      "    val_log_marginal: 1694.938810364902\n",
      "Train Epoch: 832 [512/54000 (1%)] Loss: -1945.307861\n",
      "Train Epoch: 832 [11776/54000 (22%)] Loss: -1505.251709\n",
      "Train Epoch: 832 [23040/54000 (43%)] Loss: -1630.885620\n",
      "Train Epoch: 832 [34304/54000 (64%)] Loss: -1663.237671\n",
      "Train Epoch: 832 [45568/54000 (84%)] Loss: -1676.713379\n",
      "    epoch          : 832\n",
      "    loss           : -1668.461831876547\n",
      "    val_loss       : -1665.8120130007155\n",
      "    val_log_likelihood: 1752.3721923828125\n",
      "    val_log_marginal: 1714.3348459489644\n",
      "Train Epoch: 833 [512/54000 (1%)] Loss: -1922.691406\n",
      "Train Epoch: 833 [11776/54000 (22%)] Loss: -1724.562744\n",
      "Train Epoch: 833 [23040/54000 (43%)] Loss: -1670.483032\n",
      "Train Epoch: 833 [34304/54000 (64%)] Loss: -1715.088867\n",
      "Train Epoch: 833 [45568/54000 (84%)] Loss: -1583.411255\n",
      "    epoch          : 833\n",
      "    loss           : -1675.6936603206218\n",
      "    val_loss       : -1667.1980248479172\n",
      "    val_log_likelihood: 1758.433154296875\n",
      "    val_log_marginal: 1703.2975812360644\n",
      "Train Epoch: 834 [512/54000 (1%)] Loss: -1606.038208\n",
      "Train Epoch: 834 [11776/54000 (22%)] Loss: -1763.340576\n",
      "Train Epoch: 834 [23040/54000 (43%)] Loss: -1626.409668\n",
      "Train Epoch: 834 [34304/54000 (64%)] Loss: -1666.207764\n",
      "Train Epoch: 834 [45568/54000 (84%)] Loss: -1636.277588\n",
      "    epoch          : 834\n",
      "    loss           : -1666.8647364248143\n",
      "    val_loss       : -1652.5355163692498\n",
      "    val_log_likelihood: 1735.415771484375\n",
      "    val_log_marginal: 1698.9104894168674\n",
      "Train Epoch: 835 [512/54000 (1%)] Loss: -1927.235840\n",
      "Train Epoch: 835 [11776/54000 (22%)] Loss: -1608.187744\n",
      "Train Epoch: 835 [23040/54000 (43%)] Loss: -1561.044312\n",
      "Train Epoch: 835 [34304/54000 (64%)] Loss: -1561.113525\n",
      "Train Epoch: 835 [45568/54000 (84%)] Loss: -1676.673706\n",
      "    epoch          : 835\n",
      "    loss           : -1672.7799374419865\n",
      "    val_loss       : -1670.8434071401134\n",
      "    val_log_likelihood: 1751.1732299804687\n",
      "    val_log_marginal: 1698.027277689735\n",
      "Train Epoch: 836 [512/54000 (1%)] Loss: -1934.682983\n",
      "Train Epoch: 836 [11776/54000 (22%)] Loss: -1751.664307\n",
      "Train Epoch: 836 [23040/54000 (43%)] Loss: -1693.567871\n",
      "Train Epoch: 836 [34304/54000 (64%)] Loss: -1572.667480\n",
      "Train Epoch: 836 [45568/54000 (84%)] Loss: -1665.986816\n",
      "    epoch          : 836\n",
      "    loss           : -1681.2873450553063\n",
      "    val_loss       : -1686.8563219834118\n",
      "    val_log_likelihood: 1755.6289672851562\n",
      "    val_log_marginal: 1711.9228851482271\n",
      "Train Epoch: 837 [512/54000 (1%)] Loss: -1922.803223\n",
      "Train Epoch: 837 [11776/54000 (22%)] Loss: -1554.760010\n",
      "Train Epoch: 837 [23040/54000 (43%)] Loss: -1578.660645\n",
      "Train Epoch: 837 [34304/54000 (64%)] Loss: -1919.128174\n",
      "Train Epoch: 837 [45568/54000 (84%)] Loss: -1447.099976\n",
      "    epoch          : 837\n",
      "    loss           : -1675.0456953898515\n",
      "    val_loss       : -1662.2587920915335\n",
      "    val_log_likelihood: 1755.1892700195312\n",
      "    val_log_marginal: 1718.6353314775974\n",
      "Train Epoch: 838 [512/54000 (1%)] Loss: -1706.108398\n",
      "Train Epoch: 838 [11776/54000 (22%)] Loss: -1592.310181\n",
      "Train Epoch: 838 [23040/54000 (43%)] Loss: -1616.375366\n",
      "Train Epoch: 838 [34304/54000 (64%)] Loss: -1679.388184\n",
      "Train Epoch: 838 [45568/54000 (84%)] Loss: -1584.302979\n",
      "    epoch          : 838\n",
      "    loss           : -1670.7941785755725\n",
      "    val_loss       : -1677.7397561659104\n",
      "    val_log_likelihood: 1754.8207641601562\n",
      "    val_log_marginal: 1721.40096604079\n",
      "Train Epoch: 839 [512/54000 (1%)] Loss: -1909.576172\n",
      "Train Epoch: 839 [11776/54000 (22%)] Loss: -1564.780518\n",
      "Train Epoch: 839 [23040/54000 (43%)] Loss: -1554.045654\n",
      "Train Epoch: 839 [34304/54000 (64%)] Loss: -1659.329224\n",
      "Train Epoch: 839 [45568/54000 (84%)] Loss: -1655.022095\n",
      "    epoch          : 839\n",
      "    loss           : -1663.3566326481282\n",
      "    val_loss       : -1655.3042809762992\n",
      "    val_log_likelihood: 1751.197265625\n",
      "    val_log_marginal: 1710.9791811700911\n",
      "Train Epoch: 840 [512/54000 (1%)] Loss: -1900.124756\n",
      "Train Epoch: 840 [11776/54000 (22%)] Loss: -1599.993774\n",
      "Train Epoch: 840 [23040/54000 (43%)] Loss: -1571.586792\n",
      "Train Epoch: 840 [34304/54000 (64%)] Loss: -1620.553955\n",
      "Train Epoch: 840 [45568/54000 (84%)] Loss: -1658.064697\n",
      "    epoch          : 840\n",
      "    loss           : -1671.0366682298113\n",
      "    val_loss       : -1667.8139348331838\n",
      "    val_log_likelihood: 1761.14892578125\n",
      "    val_log_marginal: 1719.1718536756932\n",
      "Train Epoch: 841 [512/54000 (1%)] Loss: -1728.234131\n",
      "Train Epoch: 841 [11776/54000 (22%)] Loss: -1582.455811\n",
      "Train Epoch: 841 [23040/54000 (43%)] Loss: -1542.120972\n",
      "Train Epoch: 841 [34304/54000 (64%)] Loss: -1533.362671\n",
      "Train Epoch: 841 [45568/54000 (84%)] Loss: -1685.280762\n",
      "    epoch          : 841\n",
      "    loss           : -1671.4224249207148\n",
      "    val_loss       : -1672.9798814019189\n",
      "    val_log_likelihood: 1751.523876953125\n",
      "    val_log_marginal: 1722.3521403454245\n",
      "Train Epoch: 842 [512/54000 (1%)] Loss: -1748.630127\n",
      "Train Epoch: 842 [11776/54000 (22%)] Loss: -1559.861816\n",
      "Train Epoch: 842 [23040/54000 (43%)] Loss: -1654.931885\n",
      "Train Epoch: 842 [34304/54000 (64%)] Loss: -1616.601196\n",
      "Train Epoch: 842 [45568/54000 (84%)] Loss: -1544.468994\n",
      "    epoch          : 842\n",
      "    loss           : -1667.395560991646\n",
      "    val_loss       : -1664.7346919653937\n",
      "    val_log_likelihood: 1742.529638671875\n",
      "    val_log_marginal: 1698.6086338602006\n",
      "Train Epoch: 843 [512/54000 (1%)] Loss: -1951.154053\n",
      "Train Epoch: 843 [11776/54000 (22%)] Loss: -1621.226685\n",
      "Train Epoch: 843 [23040/54000 (43%)] Loss: -1607.805420\n",
      "Train Epoch: 843 [34304/54000 (64%)] Loss: -1630.747070\n",
      "Train Epoch: 843 [45568/54000 (84%)] Loss: -1591.744995\n",
      "    epoch          : 843\n",
      "    loss           : -1666.1733869798113\n",
      "    val_loss       : -1661.8448594305664\n",
      "    val_log_likelihood: 1741.84287109375\n",
      "    val_log_marginal: 1706.4868797257543\n",
      "Train Epoch: 844 [512/54000 (1%)] Loss: -1919.542236\n",
      "Train Epoch: 844 [11776/54000 (22%)] Loss: -1578.071533\n",
      "Train Epoch: 844 [23040/54000 (43%)] Loss: -1681.677612\n",
      "Train Epoch: 844 [34304/54000 (64%)] Loss: -1580.578857\n",
      "Train Epoch: 844 [45568/54000 (84%)] Loss: -1719.163086\n",
      "    epoch          : 844\n",
      "    loss           : -1677.6751636467357\n",
      "    val_loss       : -1673.7247153560631\n",
      "    val_log_likelihood: 1765.7610473632812\n",
      "    val_log_marginal: 1721.9166879955678\n",
      "Train Epoch: 845 [512/54000 (1%)] Loss: -1606.859131\n",
      "Train Epoch: 845 [11776/54000 (22%)] Loss: -1775.024780\n",
      "Train Epoch: 845 [23040/54000 (43%)] Loss: -1719.637573\n",
      "Train Epoch: 845 [34304/54000 (64%)] Loss: -1658.953369\n",
      "Train Epoch: 845 [45568/54000 (84%)] Loss: -1696.508667\n",
      "    epoch          : 845\n",
      "    loss           : -1681.2705718691986\n",
      "    val_loss       : -1675.2384694272653\n",
      "    val_log_likelihood: 1746.4517700195313\n",
      "    val_log_marginal: 1705.5778376422822\n",
      "Train Epoch: 846 [512/54000 (1%)] Loss: -1925.321289\n",
      "Train Epoch: 846 [11776/54000 (22%)] Loss: -1758.801147\n",
      "Train Epoch: 846 [23040/54000 (43%)] Loss: -1606.267578\n",
      "Train Epoch: 846 [34304/54000 (64%)] Loss: -1570.873779\n",
      "Train Epoch: 846 [45568/54000 (84%)] Loss: -1675.758179\n",
      "    epoch          : 846\n",
      "    loss           : -1675.3787358350094\n",
      "    val_loss       : -1680.1719724724069\n",
      "    val_log_likelihood: 1758.1757568359376\n",
      "    val_log_marginal: 1715.0742903169244\n",
      "Train Epoch: 847 [512/54000 (1%)] Loss: -1934.139771\n",
      "Train Epoch: 847 [11776/54000 (22%)] Loss: -1568.867432\n",
      "Train Epoch: 847 [23040/54000 (43%)] Loss: -1586.003174\n",
      "Train Epoch: 847 [34304/54000 (64%)] Loss: -1708.559082\n",
      "Train Epoch: 847 [45568/54000 (84%)] Loss: -1664.168579\n",
      "    epoch          : 847\n",
      "    loss           : -1680.1932469736232\n",
      "    val_loss       : -1665.752153326571\n",
      "    val_log_likelihood: 1733.1726928710937\n",
      "    val_log_marginal: 1693.5362947482615\n",
      "Train Epoch: 848 [512/54000 (1%)] Loss: -1641.634521\n",
      "Train Epoch: 848 [11776/54000 (22%)] Loss: -1737.125610\n",
      "Train Epoch: 848 [23040/54000 (43%)] Loss: -1580.708252\n",
      "Train Epoch: 848 [34304/54000 (64%)] Loss: -1942.942139\n",
      "Train Epoch: 848 [45568/54000 (84%)] Loss: -1677.670776\n",
      "    epoch          : 848\n",
      "    loss           : -1676.0879824798885\n",
      "    val_loss       : -1669.960997761134\n",
      "    val_log_likelihood: 1755.165283203125\n",
      "    val_log_marginal: 1719.0666257631033\n",
      "Train Epoch: 849 [512/54000 (1%)] Loss: -1922.198120\n",
      "Train Epoch: 849 [11776/54000 (22%)] Loss: -1742.694336\n",
      "Train Epoch: 849 [23040/54000 (43%)] Loss: -1630.627686\n",
      "Train Epoch: 849 [34304/54000 (64%)] Loss: -1574.066895\n",
      "Train Epoch: 849 [45568/54000 (84%)] Loss: -1680.547607\n",
      "    epoch          : 849\n",
      "    loss           : -1677.0761102355352\n",
      "    val_loss       : -1677.025540103577\n",
      "    val_log_likelihood: 1759.7220458984375\n",
      "    val_log_marginal: 1720.1522001951932\n",
      "Train Epoch: 850 [512/54000 (1%)] Loss: -1646.064209\n",
      "Train Epoch: 850 [11776/54000 (22%)] Loss: -1760.762085\n",
      "Train Epoch: 850 [23040/54000 (43%)] Loss: -1602.901123\n",
      "Train Epoch: 850 [34304/54000 (64%)] Loss: -1958.177979\n",
      "Train Epoch: 850 [45568/54000 (84%)] Loss: -1695.009766\n",
      "    epoch          : 850\n",
      "    loss           : -1682.8629319596998\n",
      "    val_loss       : -1691.0841041572392\n",
      "    val_log_likelihood: 1760.0110961914063\n",
      "    val_log_marginal: 1723.611734488979\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch850.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 851 [512/54000 (1%)] Loss: -1944.068848\n",
      "Train Epoch: 851 [11776/54000 (22%)] Loss: -1594.485229\n",
      "Train Epoch: 851 [23040/54000 (43%)] Loss: -1604.522949\n",
      "Train Epoch: 851 [34304/54000 (64%)] Loss: -1711.809814\n",
      "Train Epoch: 851 [45568/54000 (84%)] Loss: -1673.291870\n",
      "    epoch          : 851\n",
      "    loss           : -1683.9272485109839\n",
      "    val_loss       : -1675.0065612832084\n",
      "    val_log_likelihood: 1762.00869140625\n",
      "    val_log_marginal: 1713.3982954565436\n",
      "Train Epoch: 852 [512/54000 (1%)] Loss: -1922.553833\n",
      "Train Epoch: 852 [11776/54000 (22%)] Loss: -1608.878906\n",
      "Train Epoch: 852 [23040/54000 (43%)] Loss: -1639.109497\n",
      "Train Epoch: 852 [34304/54000 (64%)] Loss: -1615.365845\n",
      "Train Epoch: 852 [45568/54000 (84%)] Loss: -1658.478027\n",
      "    epoch          : 852\n",
      "    loss           : -1678.5308898321473\n",
      "    val_loss       : -1669.0295788821763\n",
      "    val_log_likelihood: 1749.858984375\n",
      "    val_log_marginal: 1713.6165630303324\n",
      "Train Epoch: 853 [512/54000 (1%)] Loss: -1920.571533\n",
      "Train Epoch: 853 [11776/54000 (22%)] Loss: -1503.467407\n",
      "Train Epoch: 853 [23040/54000 (43%)] Loss: -1526.786987\n",
      "Train Epoch: 853 [34304/54000 (64%)] Loss: -1658.181396\n",
      "Train Epoch: 853 [45568/54000 (84%)] Loss: -1666.549194\n",
      "    epoch          : 853\n",
      "    loss           : -1664.0728771851795\n",
      "    val_loss       : -1668.0141869287938\n",
      "    val_log_likelihood: 1745.704931640625\n",
      "    val_log_marginal: 1710.8890164360405\n",
      "Train Epoch: 854 [512/54000 (1%)] Loss: -1920.936401\n",
      "Train Epoch: 854 [11776/54000 (22%)] Loss: -1541.211792\n",
      "Train Epoch: 854 [23040/54000 (43%)] Loss: -1704.468018\n",
      "Train Epoch: 854 [34304/54000 (64%)] Loss: -1574.332031\n",
      "Train Epoch: 854 [45568/54000 (84%)] Loss: -1695.755493\n",
      "    epoch          : 854\n",
      "    loss           : -1668.967010800201\n",
      "    val_loss       : -1666.9643342763186\n",
      "    val_log_likelihood: 1758.8580078125\n",
      "    val_log_marginal: 1717.798280763626\n",
      "Train Epoch: 855 [512/54000 (1%)] Loss: -1937.344360\n",
      "Train Epoch: 855 [11776/54000 (22%)] Loss: -1739.041016\n",
      "Train Epoch: 855 [23040/54000 (43%)] Loss: -1604.583496\n",
      "Train Epoch: 855 [34304/54000 (64%)] Loss: -1944.847900\n",
      "Train Epoch: 855 [45568/54000 (84%)] Loss: -1578.981567\n",
      "    epoch          : 855\n",
      "    loss           : -1672.4818864576887\n",
      "    val_loss       : -1663.2481427876278\n",
      "    val_log_likelihood: 1749.7271240234375\n",
      "    val_log_marginal: 1715.5639632377774\n",
      "Train Epoch: 856 [512/54000 (1%)] Loss: -1923.871948\n",
      "Train Epoch: 856 [11776/54000 (22%)] Loss: -1615.798340\n",
      "Train Epoch: 856 [23040/54000 (43%)] Loss: -1560.226074\n",
      "Train Epoch: 856 [34304/54000 (64%)] Loss: -1650.864990\n",
      "Train Epoch: 856 [45568/54000 (84%)] Loss: -1688.381470\n",
      "    epoch          : 856\n",
      "    loss           : -1665.330051535427\n",
      "    val_loss       : -1647.5312406589278\n",
      "    val_log_likelihood: 1744.364501953125\n",
      "    val_log_marginal: 1704.3424948010593\n",
      "Train Epoch: 857 [512/54000 (1%)] Loss: -1924.427490\n",
      "Train Epoch: 857 [11776/54000 (22%)] Loss: -1673.783813\n",
      "Train Epoch: 857 [23040/54000 (43%)] Loss: -1652.056396\n",
      "Train Epoch: 857 [34304/54000 (64%)] Loss: -1688.543091\n",
      "Train Epoch: 857 [45568/54000 (84%)] Loss: -1649.037842\n",
      "    epoch          : 857\n",
      "    loss           : -1664.2718409170018\n",
      "    val_loss       : -1662.1181837867946\n",
      "    val_log_likelihood: 1744.5575073242187\n",
      "    val_log_marginal: 1689.4539498500526\n",
      "Train Epoch: 858 [512/54000 (1%)] Loss: -1663.806030\n",
      "Train Epoch: 858 [11776/54000 (22%)] Loss: -1631.138428\n",
      "Train Epoch: 858 [23040/54000 (43%)] Loss: -1694.829834\n",
      "Train Epoch: 858 [34304/54000 (64%)] Loss: -1626.015747\n",
      "Train Epoch: 858 [45568/54000 (84%)] Loss: -1555.111572\n",
      "    epoch          : 858\n",
      "    loss           : -1670.7344487256344\n",
      "    val_loss       : -1680.6123813060112\n",
      "    val_log_likelihood: 1757.1812377929687\n",
      "    val_log_marginal: 1719.588061203435\n",
      "Train Epoch: 859 [512/54000 (1%)] Loss: -1926.126953\n",
      "Train Epoch: 859 [11776/54000 (22%)] Loss: -1686.246826\n",
      "Train Epoch: 859 [23040/54000 (43%)] Loss: -1565.177734\n",
      "Train Epoch: 859 [34304/54000 (64%)] Loss: -1926.054688\n",
      "Train Epoch: 859 [45568/54000 (84%)] Loss: -1598.163940\n",
      "    epoch          : 859\n",
      "    loss           : -1671.1615449508818\n",
      "    val_loss       : -1662.6128637745046\n",
      "    val_log_likelihood: 1746.0751953125\n",
      "    val_log_marginal: 1709.642052835971\n",
      "Train Epoch: 860 [512/54000 (1%)] Loss: -1925.144775\n",
      "Train Epoch: 860 [11776/54000 (22%)] Loss: -1608.547363\n",
      "Train Epoch: 860 [23040/54000 (43%)] Loss: -1726.317871\n",
      "Train Epoch: 860 [34304/54000 (64%)] Loss: -1909.572388\n",
      "Train Epoch: 860 [45568/54000 (84%)] Loss: -1602.350830\n",
      "    epoch          : 860\n",
      "    loss           : -1671.2903013323794\n",
      "    val_loss       : -1668.0319840570912\n",
      "    val_log_likelihood: 1753.7333618164062\n",
      "    val_log_marginal: 1709.9189990386367\n",
      "Train Epoch: 861 [512/54000 (1%)] Loss: -1926.463501\n",
      "Train Epoch: 861 [11776/54000 (22%)] Loss: -1635.466919\n",
      "Train Epoch: 861 [23040/54000 (43%)] Loss: -1585.020996\n",
      "Train Epoch: 861 [34304/54000 (64%)] Loss: -1597.688232\n",
      "Train Epoch: 861 [45568/54000 (84%)] Loss: -1657.208618\n",
      "    epoch          : 861\n",
      "    loss           : -1678.8239371422494\n",
      "    val_loss       : -1672.6884616806171\n",
      "    val_log_likelihood: 1766.12177734375\n",
      "    val_log_marginal: 1727.987517201528\n",
      "Train Epoch: 862 [512/54000 (1%)] Loss: -1927.684570\n",
      "Train Epoch: 862 [11776/54000 (22%)] Loss: -1676.709961\n",
      "Train Epoch: 862 [23040/54000 (43%)] Loss: -1595.656372\n",
      "Train Epoch: 862 [34304/54000 (64%)] Loss: -1593.089478\n",
      "Train Epoch: 862 [45568/54000 (84%)] Loss: -1648.828613\n",
      "    epoch          : 862\n",
      "    loss           : -1668.3225810740253\n",
      "    val_loss       : -1663.928230271209\n",
      "    val_log_likelihood: 1757.3295166015625\n",
      "    val_log_marginal: 1720.139584390074\n",
      "Train Epoch: 863 [512/54000 (1%)] Loss: -1909.767090\n",
      "Train Epoch: 863 [11776/54000 (22%)] Loss: -1759.370361\n",
      "Train Epoch: 863 [23040/54000 (43%)] Loss: -1527.546631\n",
      "Train Epoch: 863 [34304/54000 (64%)] Loss: -1657.607422\n",
      "Train Epoch: 863 [45568/54000 (84%)] Loss: -1592.501343\n",
      "    epoch          : 863\n",
      "    loss           : -1674.2402923886139\n",
      "    val_loss       : -1676.465362736862\n",
      "    val_log_likelihood: 1743.6729858398437\n",
      "    val_log_marginal: 1700.2099667817354\n",
      "Train Epoch: 864 [512/54000 (1%)] Loss: -1922.995850\n",
      "Train Epoch: 864 [11776/54000 (22%)] Loss: -1645.692139\n",
      "Train Epoch: 864 [23040/54000 (43%)] Loss: -1658.715820\n",
      "Train Epoch: 864 [34304/54000 (64%)] Loss: -1701.363037\n",
      "Train Epoch: 864 [45568/54000 (84%)] Loss: -1566.814941\n",
      "    epoch          : 864\n",
      "    loss           : -1672.3593544535117\n",
      "    val_loss       : -1649.7382189578377\n",
      "    val_log_likelihood: 1715.074609375\n",
      "    val_log_marginal: 1684.7744855698197\n",
      "Train Epoch: 865 [512/54000 (1%)] Loss: -1911.539307\n",
      "Train Epoch: 865 [11776/54000 (22%)] Loss: -1583.043335\n",
      "Train Epoch: 865 [23040/54000 (43%)] Loss: -1576.576050\n",
      "Train Epoch: 865 [34304/54000 (64%)] Loss: -1580.376709\n",
      "Train Epoch: 865 [45568/54000 (84%)] Loss: -1673.945190\n",
      "    epoch          : 865\n",
      "    loss           : -1661.6237575417697\n",
      "    val_loss       : -1655.422975790687\n",
      "    val_log_likelihood: 1754.2692749023438\n",
      "    val_log_marginal: 1708.6884829364717\n",
      "Train Epoch: 866 [512/54000 (1%)] Loss: -1928.130371\n",
      "Train Epoch: 866 [11776/54000 (22%)] Loss: -1781.349609\n",
      "Train Epoch: 866 [23040/54000 (43%)] Loss: -1638.832520\n",
      "Train Epoch: 866 [34304/54000 (64%)] Loss: -1676.456055\n",
      "Train Epoch: 866 [45568/54000 (84%)] Loss: -1687.605225\n",
      "    epoch          : 866\n",
      "    loss           : -1673.6153008489325\n",
      "    val_loss       : -1672.8943638930098\n",
      "    val_log_likelihood: 1764.5588623046874\n",
      "    val_log_marginal: 1718.4705668780953\n",
      "Train Epoch: 867 [512/54000 (1%)] Loss: -1919.611206\n",
      "Train Epoch: 867 [11776/54000 (22%)] Loss: -1762.398926\n",
      "Train Epoch: 867 [23040/54000 (43%)] Loss: -1715.687256\n",
      "Train Epoch: 867 [34304/54000 (64%)] Loss: -1690.226318\n",
      "Train Epoch: 867 [45568/54000 (84%)] Loss: -1648.135742\n",
      "    epoch          : 867\n",
      "    loss           : -1681.9364569635675\n",
      "    val_loss       : -1666.009874452185\n",
      "    val_log_likelihood: 1747.972509765625\n",
      "    val_log_marginal: 1711.831691094488\n",
      "Train Epoch: 868 [512/54000 (1%)] Loss: -1919.903687\n",
      "Train Epoch: 868 [11776/54000 (22%)] Loss: -1551.377808\n",
      "Train Epoch: 868 [23040/54000 (43%)] Loss: -1560.886963\n",
      "Train Epoch: 868 [34304/54000 (64%)] Loss: -1697.085449\n",
      "Train Epoch: 868 [45568/54000 (84%)] Loss: -1655.924805\n",
      "    epoch          : 868\n",
      "    loss           : -1674.9326582804765\n",
      "    val_loss       : -1676.4225966314784\n",
      "    val_log_likelihood: 1752.0331420898438\n",
      "    val_log_marginal: 1713.1486463233828\n",
      "Train Epoch: 869 [512/54000 (1%)] Loss: -1915.508545\n",
      "Train Epoch: 869 [11776/54000 (22%)] Loss: -1622.899292\n",
      "Train Epoch: 869 [23040/54000 (43%)] Loss: -1637.093994\n",
      "Train Epoch: 869 [34304/54000 (64%)] Loss: -1601.239258\n",
      "Train Epoch: 869 [45568/54000 (84%)] Loss: -1559.368408\n",
      "    epoch          : 869\n",
      "    loss           : -1684.221054832534\n",
      "    val_loss       : -1681.3516852796079\n",
      "    val_log_likelihood: 1756.9990478515624\n",
      "    val_log_marginal: 1711.1566428039223\n",
      "Train Epoch: 870 [512/54000 (1%)] Loss: -1938.772217\n",
      "Train Epoch: 870 [11776/54000 (22%)] Loss: -1638.469116\n",
      "Train Epoch: 870 [23040/54000 (43%)] Loss: -1653.841309\n",
      "Train Epoch: 870 [34304/54000 (64%)] Loss: -1925.316895\n",
      "Train Epoch: 870 [45568/54000 (84%)] Loss: -1745.794189\n",
      "    epoch          : 870\n",
      "    loss           : -1683.537383731049\n",
      "    val_loss       : -1692.6063419568352\n",
      "    val_log_likelihood: 1755.9429443359375\n",
      "    val_log_marginal: 1707.3925758570433\n",
      "Train Epoch: 871 [512/54000 (1%)] Loss: -1944.445068\n",
      "Train Epoch: 871 [11776/54000 (22%)] Loss: -1753.580200\n",
      "Train Epoch: 871 [23040/54000 (43%)] Loss: -1567.192383\n",
      "Train Epoch: 871 [34304/54000 (64%)] Loss: -1652.144043\n",
      "Train Epoch: 871 [45568/54000 (84%)] Loss: -1613.466797\n",
      "    epoch          : 871\n",
      "    loss           : -1686.6444829053219\n",
      "    val_loss       : -1674.068193592783\n",
      "    val_log_likelihood: 1758.6558471679687\n",
      "    val_log_marginal: 1717.1721538748593\n",
      "Train Epoch: 872 [512/54000 (1%)] Loss: -1607.204102\n",
      "Train Epoch: 872 [11776/54000 (22%)] Loss: -1582.296631\n",
      "Train Epoch: 872 [23040/54000 (43%)] Loss: -1562.075562\n",
      "Train Epoch: 872 [34304/54000 (64%)] Loss: -1700.924805\n",
      "Train Epoch: 872 [45568/54000 (84%)] Loss: -1670.575928\n",
      "    epoch          : 872\n",
      "    loss           : -1683.1556481087562\n",
      "    val_loss       : -1671.2226826949045\n",
      "    val_log_likelihood: 1742.4836669921874\n",
      "    val_log_marginal: 1697.2437026292087\n",
      "Train Epoch: 873 [512/54000 (1%)] Loss: -1956.347046\n",
      "Train Epoch: 873 [11776/54000 (22%)] Loss: -1587.425049\n",
      "Train Epoch: 873 [23040/54000 (43%)] Loss: -1709.203003\n",
      "Train Epoch: 873 [34304/54000 (64%)] Loss: -1939.918823\n",
      "Train Epoch: 873 [45568/54000 (84%)] Loss: -1697.743164\n",
      "    epoch          : 873\n",
      "    loss           : -1677.2839548847462\n",
      "    val_loss       : -1681.0514779965392\n",
      "    val_log_likelihood: 1760.7091796875\n",
      "    val_log_marginal: 1719.8439723644574\n",
      "Train Epoch: 874 [512/54000 (1%)] Loss: -1922.489990\n",
      "Train Epoch: 874 [11776/54000 (22%)] Loss: -1752.043213\n",
      "Train Epoch: 874 [23040/54000 (43%)] Loss: -1582.952637\n",
      "Train Epoch: 874 [34304/54000 (64%)] Loss: -1708.272705\n",
      "Train Epoch: 874 [45568/54000 (84%)] Loss: -1618.809937\n",
      "    epoch          : 874\n",
      "    loss           : -1685.6724104173113\n",
      "    val_loss       : -1684.585984120611\n",
      "    val_log_likelihood: 1760.4401245117188\n",
      "    val_log_marginal: 1721.3151985667646\n",
      "Train Epoch: 875 [512/54000 (1%)] Loss: -1941.345215\n",
      "Train Epoch: 875 [11776/54000 (22%)] Loss: -1682.933105\n",
      "Train Epoch: 875 [23040/54000 (43%)] Loss: -1618.628418\n",
      "Train Epoch: 875 [34304/54000 (64%)] Loss: -1719.594727\n",
      "Train Epoch: 875 [45568/54000 (84%)] Loss: -1682.409546\n",
      "    epoch          : 875\n",
      "    loss           : -1687.7060981977104\n",
      "    val_loss       : -1691.2549077762292\n",
      "    val_log_likelihood: 1765.2830322265625\n",
      "    val_log_marginal: 1738.2531208213418\n",
      "Train Epoch: 876 [512/54000 (1%)] Loss: -1944.376709\n",
      "Train Epoch: 876 [11776/54000 (22%)] Loss: -1756.898682\n",
      "Train Epoch: 876 [23040/54000 (43%)] Loss: -1592.760132\n",
      "Train Epoch: 876 [34304/54000 (64%)] Loss: -1926.326416\n",
      "Train Epoch: 876 [45568/54000 (84%)] Loss: -1712.869141\n",
      "    epoch          : 876\n",
      "    loss           : -1692.8257742400217\n",
      "    val_loss       : -1692.5413657912984\n",
      "    val_log_likelihood: 1771.1431396484375\n",
      "    val_log_marginal: 1733.5371508125215\n",
      "Train Epoch: 877 [512/54000 (1%)] Loss: -1940.016724\n",
      "Train Epoch: 877 [11776/54000 (22%)] Loss: -1629.668701\n",
      "Train Epoch: 877 [23040/54000 (43%)] Loss: -1684.204834\n",
      "Train Epoch: 877 [34304/54000 (64%)] Loss: -1682.664551\n",
      "Train Epoch: 877 [45568/54000 (84%)] Loss: -1708.088989\n",
      "    epoch          : 877\n",
      "    loss           : -1689.5005221225247\n",
      "    val_loss       : -1683.1596603406592\n",
      "    val_log_likelihood: 1763.8430297851562\n",
      "    val_log_marginal: 1714.6726933009923\n",
      "Train Epoch: 878 [512/54000 (1%)] Loss: -1930.173462\n",
      "Train Epoch: 878 [11776/54000 (22%)] Loss: -1604.346436\n",
      "Train Epoch: 878 [23040/54000 (43%)] Loss: -1703.490723\n",
      "Train Epoch: 878 [34304/54000 (64%)] Loss: -1921.069092\n",
      "Train Epoch: 878 [45568/54000 (84%)] Loss: -1683.538452\n",
      "    epoch          : 878\n",
      "    loss           : -1692.6533142694152\n",
      "    val_loss       : -1697.7097921665759\n",
      "    val_log_likelihood: 1767.970458984375\n",
      "    val_log_marginal: 1735.2682008426636\n",
      "Train Epoch: 879 [512/54000 (1%)] Loss: -1779.033203\n",
      "Train Epoch: 879 [11776/54000 (22%)] Loss: -1763.043213\n",
      "Train Epoch: 879 [23040/54000 (43%)] Loss: -1617.780762\n",
      "Train Epoch: 879 [34304/54000 (64%)] Loss: -1697.575806\n",
      "Train Epoch: 879 [45568/54000 (84%)] Loss: -1681.921021\n",
      "    epoch          : 879\n",
      "    loss           : -1689.4230243947247\n",
      "    val_loss       : -1673.2786947842687\n",
      "    val_log_likelihood: 1768.8869140625\n",
      "    val_log_marginal: 1726.0465869355946\n",
      "Train Epoch: 880 [512/54000 (1%)] Loss: -1938.437500\n",
      "Train Epoch: 880 [11776/54000 (22%)] Loss: -1635.825928\n",
      "Train Epoch: 880 [23040/54000 (43%)] Loss: -1597.267334\n",
      "Train Epoch: 880 [34304/54000 (64%)] Loss: -1690.606445\n",
      "Train Epoch: 880 [45568/54000 (84%)] Loss: -1579.357422\n",
      "    epoch          : 880\n",
      "    loss           : -1682.0001256961634\n",
      "    val_loss       : -1692.3960686213336\n",
      "    val_log_likelihood: 1765.5954223632812\n",
      "    val_log_marginal: 1728.8095216628165\n",
      "Train Epoch: 881 [512/54000 (1%)] Loss: -1935.184326\n",
      "Train Epoch: 881 [11776/54000 (22%)] Loss: -1758.212402\n",
      "Train Epoch: 881 [23040/54000 (43%)] Loss: -1731.961060\n",
      "Train Epoch: 881 [34304/54000 (64%)] Loss: -1663.816528\n",
      "Train Epoch: 881 [45568/54000 (84%)] Loss: -1585.996094\n",
      "    epoch          : 881\n",
      "    loss           : -1684.899547010365\n",
      "    val_loss       : -1666.739856517408\n",
      "    val_log_likelihood: 1745.0056518554688\n",
      "    val_log_marginal: 1710.8032163608818\n",
      "Train Epoch: 882 [512/54000 (1%)] Loss: -1914.613281\n",
      "Train Epoch: 882 [11776/54000 (22%)] Loss: -1690.133179\n",
      "Train Epoch: 882 [23040/54000 (43%)] Loss: -1662.182861\n",
      "Train Epoch: 882 [34304/54000 (64%)] Loss: -1624.164062\n",
      "Train Epoch: 882 [45568/54000 (84%)] Loss: -1652.281128\n",
      "    epoch          : 882\n",
      "    loss           : -1671.6102331180384\n",
      "    val_loss       : -1680.672232382372\n",
      "    val_log_likelihood: 1743.135693359375\n",
      "    val_log_marginal: 1706.3057817097754\n",
      "Train Epoch: 883 [512/54000 (1%)] Loss: -1918.452759\n",
      "Train Epoch: 883 [11776/54000 (22%)] Loss: -1587.221680\n",
      "Train Epoch: 883 [23040/54000 (43%)] Loss: -1602.636963\n",
      "Train Epoch: 883 [34304/54000 (64%)] Loss: -1630.603638\n",
      "Train Epoch: 883 [45568/54000 (84%)] Loss: -1687.918701\n",
      "    epoch          : 883\n",
      "    loss           : -1678.6457978805693\n",
      "    val_loss       : -1679.7715628187173\n",
      "    val_log_likelihood: 1752.9606689453126\n",
      "    val_log_marginal: 1710.1412827007473\n",
      "Train Epoch: 884 [512/54000 (1%)] Loss: -1934.019287\n",
      "Train Epoch: 884 [11776/54000 (22%)] Loss: -1741.195068\n",
      "Train Epoch: 884 [23040/54000 (43%)] Loss: -1578.402344\n",
      "Train Epoch: 884 [34304/54000 (64%)] Loss: -1566.391235\n",
      "Train Epoch: 884 [45568/54000 (84%)] Loss: -1700.549194\n",
      "    epoch          : 884\n",
      "    loss           : -1684.3694016862623\n",
      "    val_loss       : -1686.1535049091094\n",
      "    val_log_likelihood: 1764.0295776367188\n",
      "    val_log_marginal: 1721.9806807793677\n",
      "Train Epoch: 885 [512/54000 (1%)] Loss: -1929.548340\n",
      "Train Epoch: 885 [11776/54000 (22%)] Loss: -1749.071045\n",
      "Train Epoch: 885 [23040/54000 (43%)] Loss: -1584.036133\n",
      "Train Epoch: 885 [34304/54000 (64%)] Loss: -1727.550903\n",
      "Train Epoch: 885 [45568/54000 (84%)] Loss: -1667.098145\n",
      "    epoch          : 885\n",
      "    loss           : -1690.8849360399906\n",
      "    val_loss       : -1677.1053391858936\n",
      "    val_log_likelihood: 1766.285693359375\n",
      "    val_log_marginal: 1733.227122566849\n",
      "Train Epoch: 886 [512/54000 (1%)] Loss: -1929.928711\n",
      "Train Epoch: 886 [11776/54000 (22%)] Loss: -1633.629272\n",
      "Train Epoch: 886 [23040/54000 (43%)] Loss: -1587.707275\n",
      "Train Epoch: 886 [34304/54000 (64%)] Loss: -1598.083008\n",
      "Train Epoch: 886 [45568/54000 (84%)] Loss: -1600.952393\n",
      "    epoch          : 886\n",
      "    loss           : -1683.3339505337253\n",
      "    val_loss       : -1660.9433295173571\n",
      "    val_log_likelihood: 1749.6433471679688\n",
      "    val_log_marginal: 1701.0675875652582\n",
      "Train Epoch: 887 [512/54000 (1%)] Loss: -1877.297119\n",
      "Train Epoch: 887 [11776/54000 (22%)] Loss: -1614.341675\n",
      "Train Epoch: 887 [23040/54000 (43%)] Loss: -1720.115234\n",
      "Train Epoch: 887 [34304/54000 (64%)] Loss: -1924.737305\n",
      "Train Epoch: 887 [45568/54000 (84%)] Loss: -1672.771362\n",
      "    epoch          : 887\n",
      "    loss           : -1677.972198184174\n",
      "    val_loss       : -1674.2660515362397\n",
      "    val_log_likelihood: 1762.3482299804687\n",
      "    val_log_marginal: 1723.215633219853\n",
      "Train Epoch: 888 [512/54000 (1%)] Loss: -1571.588379\n",
      "Train Epoch: 888 [11776/54000 (22%)] Loss: -1754.896973\n",
      "Train Epoch: 888 [23040/54000 (43%)] Loss: -1545.051514\n",
      "Train Epoch: 888 [34304/54000 (64%)] Loss: -1569.273560\n",
      "Train Epoch: 888 [45568/54000 (84%)] Loss: -1669.355347\n",
      "    epoch          : 888\n",
      "    loss           : -1673.5861731803063\n",
      "    val_loss       : -1690.6524345133453\n",
      "    val_log_likelihood: 1764.0217895507812\n",
      "    val_log_marginal: 1713.6757670149207\n",
      "Train Epoch: 889 [512/54000 (1%)] Loss: -1920.667480\n",
      "Train Epoch: 889 [11776/54000 (22%)] Loss: -1760.859253\n",
      "Train Epoch: 889 [23040/54000 (43%)] Loss: -1615.803589\n",
      "Train Epoch: 889 [34304/54000 (64%)] Loss: -1914.078735\n",
      "Train Epoch: 889 [45568/54000 (84%)] Loss: -1672.670898\n",
      "    epoch          : 889\n",
      "    loss           : -1690.227188563583\n",
      "    val_loss       : -1686.6035944189875\n",
      "    val_log_likelihood: 1757.277587890625\n",
      "    val_log_marginal: 1710.452566107735\n",
      "Train Epoch: 890 [512/54000 (1%)] Loss: -1950.472778\n",
      "Train Epoch: 890 [11776/54000 (22%)] Loss: -1620.111328\n",
      "Train Epoch: 890 [23040/54000 (43%)] Loss: -1581.664429\n",
      "Train Epoch: 890 [34304/54000 (64%)] Loss: -1648.418579\n",
      "Train Epoch: 890 [45568/54000 (84%)] Loss: -1700.484619\n",
      "    epoch          : 890\n",
      "    loss           : -1691.6359935798268\n",
      "    val_loss       : -1691.6358714150265\n",
      "    val_log_likelihood: 1767.930126953125\n",
      "    val_log_marginal: 1732.093550863117\n",
      "Train Epoch: 891 [512/54000 (1%)] Loss: -1928.563721\n",
      "Train Epoch: 891 [11776/54000 (22%)] Loss: -1720.439941\n",
      "Train Epoch: 891 [23040/54000 (43%)] Loss: -1721.818848\n",
      "Train Epoch: 891 [34304/54000 (64%)] Loss: -1899.662842\n",
      "Train Epoch: 891 [45568/54000 (84%)] Loss: -1565.368408\n",
      "    epoch          : 891\n",
      "    loss           : -1684.7272622892172\n",
      "    val_loss       : -1682.194045155216\n",
      "    val_log_likelihood: 1765.1484741210938\n",
      "    val_log_marginal: 1710.3760552413762\n",
      "Train Epoch: 892 [512/54000 (1%)] Loss: -1941.721558\n",
      "Train Epoch: 892 [11776/54000 (22%)] Loss: -1601.483643\n",
      "Train Epoch: 892 [23040/54000 (43%)] Loss: -1596.142334\n",
      "Train Epoch: 892 [34304/54000 (64%)] Loss: -1564.301514\n",
      "Train Epoch: 892 [45568/54000 (84%)] Loss: -1667.088745\n",
      "    epoch          : 892\n",
      "    loss           : -1687.0330254583075\n",
      "    val_loss       : -1703.4782730059699\n",
      "    val_log_likelihood: 1766.0307006835938\n",
      "    val_log_marginal: 1724.8512466218322\n",
      "Train Epoch: 893 [512/54000 (1%)] Loss: -1923.640991\n",
      "Train Epoch: 893 [11776/54000 (22%)] Loss: -1625.318359\n",
      "Train Epoch: 893 [23040/54000 (43%)] Loss: -1720.793457\n",
      "Train Epoch: 893 [34304/54000 (64%)] Loss: -1563.968384\n",
      "Train Epoch: 893 [45568/54000 (84%)] Loss: -1608.156982\n",
      "    epoch          : 893\n",
      "    loss           : -1683.1474548944152\n",
      "    val_loss       : -1672.0882783563807\n",
      "    val_log_likelihood: 1736.6468505859375\n",
      "    val_log_marginal: 1700.4196033731102\n",
      "Train Epoch: 894 [512/54000 (1%)] Loss: -1914.156372\n",
      "Train Epoch: 894 [11776/54000 (22%)] Loss: -1537.184937\n",
      "Train Epoch: 894 [23040/54000 (43%)] Loss: -1593.548340\n",
      "Train Epoch: 894 [34304/54000 (64%)] Loss: -1601.043213\n",
      "Train Epoch: 894 [45568/54000 (84%)] Loss: -1607.704346\n",
      "    epoch          : 894\n",
      "    loss           : -1679.4471858562808\n",
      "    val_loss       : -1665.3654671100899\n",
      "    val_log_likelihood: 1751.4628295898438\n",
      "    val_log_marginal: 1712.9883911799639\n",
      "Train Epoch: 895 [512/54000 (1%)] Loss: -1908.713745\n",
      "Train Epoch: 895 [11776/54000 (22%)] Loss: -1654.107544\n",
      "Train Epoch: 895 [23040/54000 (43%)] Loss: -1665.611694\n",
      "Train Epoch: 895 [34304/54000 (64%)] Loss: -1715.316406\n",
      "Train Epoch: 895 [45568/54000 (84%)] Loss: -1561.074707\n",
      "    epoch          : 895\n",
      "    loss           : -1672.0848594136758\n",
      "    val_loss       : -1674.8378134976142\n",
      "    val_log_likelihood: 1765.032470703125\n",
      "    val_log_marginal: 1718.5158775705845\n",
      "Train Epoch: 896 [512/54000 (1%)] Loss: -1920.730347\n",
      "Train Epoch: 896 [11776/54000 (22%)] Loss: -1597.832642\n",
      "Train Epoch: 896 [23040/54000 (43%)] Loss: -1567.255981\n",
      "Train Epoch: 896 [34304/54000 (64%)] Loss: -1642.772705\n",
      "Train Epoch: 896 [45568/54000 (84%)] Loss: -1663.266113\n",
      "    epoch          : 896\n",
      "    loss           : -1679.2431966951578\n",
      "    val_loss       : -1664.5162080794573\n",
      "    val_log_likelihood: 1737.7326904296874\n",
      "    val_log_marginal: 1700.9964779779316\n",
      "Train Epoch: 897 [512/54000 (1%)] Loss: -1889.608276\n",
      "Train Epoch: 897 [11776/54000 (22%)] Loss: -1603.175781\n",
      "Train Epoch: 897 [23040/54000 (43%)] Loss: -1650.272217\n",
      "Train Epoch: 897 [34304/54000 (64%)] Loss: -1739.127197\n",
      "Train Epoch: 897 [45568/54000 (84%)] Loss: -1733.806152\n",
      "    epoch          : 897\n",
      "    loss           : -1660.1788414681312\n",
      "    val_loss       : -1663.732314351201\n",
      "    val_log_likelihood: 1755.1914306640624\n",
      "    val_log_marginal: 1721.63441767022\n",
      "Train Epoch: 898 [512/54000 (1%)] Loss: -1868.505981\n",
      "Train Epoch: 898 [11776/54000 (22%)] Loss: -1769.485352\n",
      "Train Epoch: 898 [23040/54000 (43%)] Loss: -1712.944336\n",
      "Train Epoch: 898 [34304/54000 (64%)] Loss: -1676.019043\n",
      "Train Epoch: 898 [45568/54000 (84%)] Loss: -1591.679199\n",
      "    epoch          : 898\n",
      "    loss           : -1671.0323389638768\n",
      "    val_loss       : -1680.6534212616273\n",
      "    val_log_likelihood: 1746.2343872070312\n",
      "    val_log_marginal: 1710.3127963934094\n",
      "Train Epoch: 899 [512/54000 (1%)] Loss: -1904.278442\n",
      "Train Epoch: 899 [11776/54000 (22%)] Loss: -1587.078003\n",
      "Train Epoch: 899 [23040/54000 (43%)] Loss: -1566.513794\n",
      "Train Epoch: 899 [34304/54000 (64%)] Loss: -1707.482300\n",
      "Train Epoch: 899 [45568/54000 (84%)] Loss: -1684.921143\n",
      "    epoch          : 899\n",
      "    loss           : -1676.7284406907488\n",
      "    val_loss       : -1669.9387375216932\n",
      "    val_log_likelihood: 1753.4232421875\n",
      "    val_log_marginal: 1696.64528571181\n",
      "Train Epoch: 900 [512/54000 (1%)] Loss: -1889.623901\n",
      "Train Epoch: 900 [11776/54000 (22%)] Loss: -1605.928101\n",
      "Train Epoch: 900 [23040/54000 (43%)] Loss: -1708.403076\n",
      "Train Epoch: 900 [34304/54000 (64%)] Loss: -1705.868164\n",
      "Train Epoch: 900 [45568/54000 (84%)] Loss: -1663.910278\n",
      "    epoch          : 900\n",
      "    loss           : -1675.4949479811262\n",
      "    val_loss       : -1669.7679751677438\n",
      "    val_log_likelihood: 1755.1578979492188\n",
      "    val_log_marginal: 1720.2035072978586\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch900.pth ...\n",
      "Train Epoch: 901 [512/54000 (1%)] Loss: -1921.782837\n",
      "Train Epoch: 901 [11776/54000 (22%)] Loss: -1708.847412\n",
      "Train Epoch: 901 [23040/54000 (43%)] Loss: -1575.135254\n",
      "Train Epoch: 901 [34304/54000 (64%)] Loss: -1604.027588\n",
      "Train Epoch: 901 [45568/54000 (84%)] Loss: -1602.597900\n",
      "    epoch          : 901\n",
      "    loss           : -1681.3107982673268\n",
      "    val_loss       : -1680.6648278064094\n",
      "    val_log_likelihood: 1739.0379516601563\n",
      "    val_log_marginal: 1704.300554134324\n",
      "Train Epoch: 902 [512/54000 (1%)] Loss: -1911.520996\n",
      "Train Epoch: 902 [11776/54000 (22%)] Loss: -1770.158447\n",
      "Train Epoch: 902 [23040/54000 (43%)] Loss: -1735.629517\n",
      "Train Epoch: 902 [34304/54000 (64%)] Loss: -1923.373901\n",
      "Train Epoch: 902 [45568/54000 (84%)] Loss: -1695.929199\n",
      "    epoch          : 902\n",
      "    loss           : -1681.8571366413985\n",
      "    val_loss       : -1680.6079671121201\n",
      "    val_log_likelihood: 1761.7893310546874\n",
      "    val_log_marginal: 1722.508259852603\n",
      "Train Epoch: 903 [512/54000 (1%)] Loss: -1910.708252\n",
      "Train Epoch: 903 [11776/54000 (22%)] Loss: -1625.998657\n",
      "Train Epoch: 903 [23040/54000 (43%)] Loss: -1703.247559\n",
      "Train Epoch: 903 [34304/54000 (64%)] Loss: -1680.181396\n",
      "Train Epoch: 903 [45568/54000 (84%)] Loss: -1634.134399\n",
      "    epoch          : 903\n",
      "    loss           : -1675.5467952312808\n",
      "    val_loss       : -1667.1622908744962\n",
      "    val_log_likelihood: 1761.6140747070312\n",
      "    val_log_marginal: 1714.30095673725\n",
      "Train Epoch: 904 [512/54000 (1%)] Loss: -1930.942139\n",
      "Train Epoch: 904 [11776/54000 (22%)] Loss: -1627.776367\n",
      "Train Epoch: 904 [23040/54000 (43%)] Loss: -1677.218872\n",
      "Train Epoch: 904 [34304/54000 (64%)] Loss: -1725.704102\n",
      "Train Epoch: 904 [45568/54000 (84%)] Loss: -1652.125732\n",
      "    epoch          : 904\n",
      "    loss           : -1678.9114180461015\n",
      "    val_loss       : -1674.0624866993167\n",
      "    val_log_likelihood: 1763.0059204101562\n",
      "    val_log_marginal: 1718.6995797004552\n",
      "Train Epoch: 905 [512/54000 (1%)] Loss: -1912.383545\n",
      "Train Epoch: 905 [11776/54000 (22%)] Loss: -1611.305298\n",
      "Train Epoch: 905 [23040/54000 (43%)] Loss: -1724.532471\n",
      "Train Epoch: 905 [34304/54000 (64%)] Loss: -1671.022095\n",
      "Train Epoch: 905 [45568/54000 (84%)] Loss: -1670.212158\n",
      "    epoch          : 905\n",
      "    loss           : -1685.1357059289912\n",
      "    val_loss       : -1679.0924825075083\n",
      "    val_log_likelihood: 1765.9852416992187\n",
      "    val_log_marginal: 1732.8089069060982\n",
      "Train Epoch: 906 [512/54000 (1%)] Loss: -1754.238770\n",
      "Train Epoch: 906 [11776/54000 (22%)] Loss: -1569.380737\n",
      "Train Epoch: 906 [23040/54000 (43%)] Loss: -1648.454346\n",
      "Train Epoch: 906 [34304/54000 (64%)] Loss: -1554.036743\n",
      "Train Epoch: 906 [45568/54000 (84%)] Loss: -1682.131348\n",
      "    epoch          : 906\n",
      "    loss           : -1683.3235769743967\n",
      "    val_loss       : -1678.3570245003327\n",
      "    val_log_likelihood: 1759.9702880859375\n",
      "    val_log_marginal: 1716.3804626334459\n",
      "Train Epoch: 907 [512/54000 (1%)] Loss: -1892.031494\n",
      "Train Epoch: 907 [11776/54000 (22%)] Loss: -1596.446899\n",
      "Train Epoch: 907 [23040/54000 (43%)] Loss: -1591.654053\n",
      "Train Epoch: 907 [34304/54000 (64%)] Loss: -1626.439941\n",
      "Train Epoch: 907 [45568/54000 (84%)] Loss: -1642.037231\n",
      "    epoch          : 907\n",
      "    loss           : -1681.905248056544\n",
      "    val_loss       : -1672.6383628309704\n",
      "    val_log_likelihood: 1747.2703857421875\n",
      "    val_log_marginal: 1710.9397785142064\n",
      "Train Epoch: 908 [512/54000 (1%)] Loss: -1927.232666\n",
      "Train Epoch: 908 [11776/54000 (22%)] Loss: -1739.920288\n",
      "Train Epoch: 908 [23040/54000 (43%)] Loss: -1646.208862\n",
      "Train Epoch: 908 [34304/54000 (64%)] Loss: -1938.828369\n",
      "Train Epoch: 908 [45568/54000 (84%)] Loss: -1686.202515\n",
      "    epoch          : 908\n",
      "    loss           : -1671.1151171391552\n",
      "    val_loss       : -1667.1515621658414\n",
      "    val_log_likelihood: 1763.7217163085938\n",
      "    val_log_marginal: 1724.1449084427209\n",
      "Train Epoch: 909 [512/54000 (1%)] Loss: -1909.290894\n",
      "Train Epoch: 909 [11776/54000 (22%)] Loss: -1624.091309\n",
      "Train Epoch: 909 [23040/54000 (43%)] Loss: -1581.486694\n",
      "Train Epoch: 909 [34304/54000 (64%)] Loss: -1574.403931\n",
      "Train Epoch: 909 [45568/54000 (84%)] Loss: -1640.019775\n",
      "    epoch          : 909\n",
      "    loss           : -1642.2738206315748\n",
      "    val_loss       : -1637.0500970461405\n",
      "    val_log_likelihood: 1719.2597290039062\n",
      "    val_log_marginal: 1681.005465218425\n",
      "Train Epoch: 910 [512/54000 (1%)] Loss: -1902.064209\n",
      "Train Epoch: 910 [11776/54000 (22%)] Loss: -1615.467651\n",
      "Train Epoch: 910 [23040/54000 (43%)] Loss: -1577.482910\n",
      "Train Epoch: 910 [34304/54000 (64%)] Loss: -1638.669189\n",
      "Train Epoch: 910 [45568/54000 (84%)] Loss: -1672.726562\n",
      "    epoch          : 910\n",
      "    loss           : -1661.1231121403157\n",
      "    val_loss       : -1665.4831475144251\n",
      "    val_log_likelihood: 1745.3492797851563\n",
      "    val_log_marginal: 1700.70122923702\n",
      "Train Epoch: 911 [512/54000 (1%)] Loss: -1938.822510\n",
      "Train Epoch: 911 [11776/54000 (22%)] Loss: -1584.767578\n",
      "Train Epoch: 911 [23040/54000 (43%)] Loss: -1542.409790\n",
      "Train Epoch: 911 [34304/54000 (64%)] Loss: -1520.125244\n",
      "Train Epoch: 911 [45568/54000 (84%)] Loss: -1625.694336\n",
      "    epoch          : 911\n",
      "    loss           : -1661.403943958849\n",
      "    val_loss       : -1665.0822943818755\n",
      "    val_log_likelihood: 1747.0417846679688\n",
      "    val_log_marginal: 1705.707401255518\n",
      "Train Epoch: 912 [512/54000 (1%)] Loss: -1918.345093\n",
      "Train Epoch: 912 [11776/54000 (22%)] Loss: -1588.204468\n",
      "Train Epoch: 912 [23040/54000 (43%)] Loss: -1674.998535\n",
      "Train Epoch: 912 [34304/54000 (64%)] Loss: -1684.418945\n",
      "Train Epoch: 912 [45568/54000 (84%)] Loss: -1619.234253\n",
      "    epoch          : 912\n",
      "    loss           : -1669.639270140393\n",
      "    val_loss       : -1678.1110622983426\n",
      "    val_log_likelihood: 1746.1183471679688\n",
      "    val_log_marginal: 1705.802700851485\n",
      "Train Epoch: 913 [512/54000 (1%)] Loss: -1743.388916\n",
      "Train Epoch: 913 [11776/54000 (22%)] Loss: -1650.130615\n",
      "Train Epoch: 913 [23040/54000 (43%)] Loss: -1559.815186\n",
      "Train Epoch: 913 [34304/54000 (64%)] Loss: -1619.106812\n",
      "Train Epoch: 913 [45568/54000 (84%)] Loss: -1644.026123\n",
      "    epoch          : 913\n",
      "    loss           : -1679.7470316367574\n",
      "    val_loss       : -1673.3484825689345\n",
      "    val_log_likelihood: 1748.3916748046875\n",
      "    val_log_marginal: 1693.6448810994625\n",
      "Train Epoch: 914 [512/54000 (1%)] Loss: -1912.904053\n",
      "Train Epoch: 914 [11776/54000 (22%)] Loss: -1766.489258\n",
      "Train Epoch: 914 [23040/54000 (43%)] Loss: -1596.922485\n",
      "Train Epoch: 914 [34304/54000 (64%)] Loss: -1645.991211\n",
      "Train Epoch: 914 [45568/54000 (84%)] Loss: -1647.475464\n",
      "    epoch          : 914\n",
      "    loss           : -1673.737007367729\n",
      "    val_loss       : -1677.2914321145042\n",
      "    val_log_likelihood: 1756.8869384765626\n",
      "    val_log_marginal: 1722.654199341312\n",
      "Train Epoch: 915 [512/54000 (1%)] Loss: -1919.605225\n",
      "Train Epoch: 915 [11776/54000 (22%)] Loss: -1591.208740\n",
      "Train Epoch: 915 [23040/54000 (43%)] Loss: -1702.073853\n",
      "Train Epoch: 915 [34304/54000 (64%)] Loss: -1574.118042\n",
      "Train Epoch: 915 [45568/54000 (84%)] Loss: -1651.306641\n",
      "    epoch          : 915\n",
      "    loss           : -1675.4733173634747\n",
      "    val_loss       : -1673.9631617547943\n",
      "    val_log_likelihood: 1745.5635986328125\n",
      "    val_log_marginal: 1707.7760146066546\n",
      "Train Epoch: 916 [512/54000 (1%)] Loss: -1638.486328\n",
      "Train Epoch: 916 [11776/54000 (22%)] Loss: -1728.748169\n",
      "Train Epoch: 916 [23040/54000 (43%)] Loss: -1694.489868\n",
      "Train Epoch: 916 [34304/54000 (64%)] Loss: -1680.669312\n",
      "Train Epoch: 916 [45568/54000 (84%)] Loss: -1642.311768\n",
      "    epoch          : 916\n",
      "    loss           : -1678.0821569461634\n",
      "    val_loss       : -1678.5935586166568\n",
      "    val_log_likelihood: 1757.1972534179688\n",
      "    val_log_marginal: 1708.1868381109089\n",
      "Train Epoch: 917 [512/54000 (1%)] Loss: -1943.098511\n",
      "Train Epoch: 917 [11776/54000 (22%)] Loss: -1744.926025\n",
      "Train Epoch: 917 [23040/54000 (43%)] Loss: -1577.885376\n",
      "Train Epoch: 917 [34304/54000 (64%)] Loss: -1617.847778\n",
      "Train Epoch: 917 [45568/54000 (84%)] Loss: -1645.032837\n",
      "    epoch          : 917\n",
      "    loss           : -1681.1895510229733\n",
      "    val_loss       : -1678.8915605646557\n",
      "    val_log_likelihood: 1745.3099975585938\n",
      "    val_log_marginal: 1705.8176816314458\n",
      "Train Epoch: 918 [512/54000 (1%)] Loss: -1745.943237\n",
      "Train Epoch: 918 [11776/54000 (22%)] Loss: -1700.325439\n",
      "Train Epoch: 918 [23040/54000 (43%)] Loss: -1716.584351\n",
      "Train Epoch: 918 [34304/54000 (64%)] Loss: -1918.227661\n",
      "Train Epoch: 918 [45568/54000 (84%)] Loss: -1635.277954\n",
      "    epoch          : 918\n",
      "    loss           : -1671.8181780824566\n",
      "    val_loss       : -1656.7639779224992\n",
      "    val_log_likelihood: 1749.20751953125\n",
      "    val_log_marginal: 1704.6604940891266\n",
      "Train Epoch: 919 [512/54000 (1%)] Loss: -1738.656982\n",
      "Train Epoch: 919 [11776/54000 (22%)] Loss: -1582.347534\n",
      "Train Epoch: 919 [23040/54000 (43%)] Loss: -1576.954224\n",
      "Train Epoch: 919 [34304/54000 (64%)] Loss: -1708.564087\n",
      "Train Epoch: 919 [45568/54000 (84%)] Loss: -1606.117432\n",
      "    epoch          : 919\n",
      "    loss           : -1665.4555035581684\n",
      "    val_loss       : -1662.5733850990423\n",
      "    val_log_likelihood: 1748.806103515625\n",
      "    val_log_marginal: 1697.995836597681\n",
      "Train Epoch: 920 [512/54000 (1%)] Loss: -1914.193359\n",
      "Train Epoch: 920 [11776/54000 (22%)] Loss: -1626.280029\n",
      "Train Epoch: 920 [23040/54000 (43%)] Loss: -1666.631226\n",
      "Train Epoch: 920 [34304/54000 (64%)] Loss: -1925.671021\n",
      "Train Epoch: 920 [45568/54000 (84%)] Loss: -1711.572266\n",
      "    epoch          : 920\n",
      "    loss           : -1684.66796875\n",
      "    val_loss       : -1686.1593363011257\n",
      "    val_log_likelihood: 1757.56669921875\n",
      "    val_log_marginal: 1716.8655541460962\n",
      "Train Epoch: 921 [512/54000 (1%)] Loss: -1931.877563\n",
      "Train Epoch: 921 [11776/54000 (22%)] Loss: -1608.157959\n",
      "Train Epoch: 921 [23040/54000 (43%)] Loss: -1651.878662\n",
      "Train Epoch: 921 [34304/54000 (64%)] Loss: -1641.636597\n",
      "Train Epoch: 921 [45568/54000 (84%)] Loss: -1690.677979\n",
      "    epoch          : 921\n",
      "    loss           : -1677.8131043084777\n",
      "    val_loss       : -1683.7844066611492\n",
      "    val_log_likelihood: 1754.2367797851562\n",
      "    val_log_marginal: 1718.0733476273715\n",
      "Train Epoch: 922 [512/54000 (1%)] Loss: -1938.234619\n",
      "Train Epoch: 922 [11776/54000 (22%)] Loss: -1754.688599\n",
      "Train Epoch: 922 [23040/54000 (43%)] Loss: -1667.863037\n",
      "Train Epoch: 922 [34304/54000 (64%)] Loss: -1632.466553\n",
      "Train Epoch: 922 [45568/54000 (84%)] Loss: -1683.248047\n",
      "    epoch          : 922\n",
      "    loss           : -1681.6816490853187\n",
      "    val_loss       : -1674.9064195583574\n",
      "    val_log_likelihood: 1760.2491821289063\n",
      "    val_log_marginal: 1718.9400150692302\n",
      "Train Epoch: 923 [512/54000 (1%)] Loss: -1921.176636\n",
      "Train Epoch: 923 [11776/54000 (22%)] Loss: -1635.494263\n",
      "Train Epoch: 923 [23040/54000 (43%)] Loss: -1632.404297\n",
      "Train Epoch: 923 [34304/54000 (64%)] Loss: -1634.717529\n",
      "Train Epoch: 923 [45568/54000 (84%)] Loss: -1661.288696\n",
      "    epoch          : 923\n",
      "    loss           : -1685.0657427192914\n",
      "    val_loss       : -1678.7528652381152\n",
      "    val_log_likelihood: 1760.6864013671875\n",
      "    val_log_marginal: 1718.5958252869546\n",
      "Train Epoch: 924 [512/54000 (1%)] Loss: -1949.269287\n",
      "Train Epoch: 924 [11776/54000 (22%)] Loss: -1636.732788\n",
      "Train Epoch: 924 [23040/54000 (43%)] Loss: -1572.755127\n",
      "Train Epoch: 924 [34304/54000 (64%)] Loss: -1627.065186\n",
      "Train Epoch: 924 [45568/54000 (84%)] Loss: -1564.147583\n",
      "    epoch          : 924\n",
      "    loss           : -1686.5673852297339\n",
      "    val_loss       : -1678.8386085361242\n",
      "    val_log_likelihood: 1767.264013671875\n",
      "    val_log_marginal: 1717.5963546600192\n",
      "Train Epoch: 925 [512/54000 (1%)] Loss: -1910.226807\n",
      "Train Epoch: 925 [11776/54000 (22%)] Loss: -1651.085938\n",
      "Train Epoch: 925 [23040/54000 (43%)] Loss: -1688.681885\n",
      "Train Epoch: 925 [34304/54000 (64%)] Loss: -1616.335815\n",
      "Train Epoch: 925 [45568/54000 (84%)] Loss: -1593.783691\n",
      "    epoch          : 925\n",
      "    loss           : -1687.4124514135983\n",
      "    val_loss       : -1681.8328267605975\n",
      "    val_log_likelihood: 1771.8630981445312\n",
      "    val_log_marginal: 1733.0383596334607\n",
      "Train Epoch: 926 [512/54000 (1%)] Loss: -1908.817505\n",
      "Train Epoch: 926 [11776/54000 (22%)] Loss: -1586.870483\n",
      "Train Epoch: 926 [23040/54000 (43%)] Loss: -1637.415894\n",
      "Train Epoch: 926 [34304/54000 (64%)] Loss: -1613.847900\n",
      "Train Epoch: 926 [45568/54000 (84%)] Loss: -1582.502563\n",
      "    epoch          : 926\n",
      "    loss           : -1689.1037368019029\n",
      "    val_loss       : -1693.434390962217\n",
      "    val_log_likelihood: 1772.787548828125\n",
      "    val_log_marginal: 1737.104054284841\n",
      "Train Epoch: 927 [512/54000 (1%)] Loss: -1931.937256\n",
      "Train Epoch: 927 [11776/54000 (22%)] Loss: -1739.479370\n",
      "Train Epoch: 927 [23040/54000 (43%)] Loss: -1578.266113\n",
      "Train Epoch: 927 [34304/54000 (64%)] Loss: -1719.944092\n",
      "Train Epoch: 927 [45568/54000 (84%)] Loss: -1682.027710\n",
      "    epoch          : 927\n",
      "    loss           : -1690.2841796875\n",
      "    val_loss       : -1685.6828903286719\n",
      "    val_log_likelihood: 1758.5748168945313\n",
      "    val_log_marginal: 1719.1971241873514\n",
      "Train Epoch: 928 [512/54000 (1%)] Loss: -1928.364014\n",
      "Train Epoch: 928 [11776/54000 (22%)] Loss: -1640.141968\n",
      "Train Epoch: 928 [23040/54000 (43%)] Loss: -1694.994385\n",
      "Train Epoch: 928 [34304/54000 (64%)] Loss: -1937.089355\n",
      "Train Epoch: 928 [45568/54000 (84%)] Loss: -1677.666016\n",
      "    epoch          : 928\n",
      "    loss           : -1690.4491849087253\n",
      "    val_loss       : -1689.2982399728148\n",
      "    val_log_likelihood: 1763.0472412109375\n",
      "    val_log_marginal: 1713.4564102500676\n",
      "Train Epoch: 929 [512/54000 (1%)] Loss: -1704.005615\n",
      "Train Epoch: 929 [11776/54000 (22%)] Loss: -1739.996948\n",
      "Train Epoch: 929 [23040/54000 (43%)] Loss: -1692.223022\n",
      "Train Epoch: 929 [34304/54000 (64%)] Loss: -1719.227417\n",
      "Train Epoch: 929 [45568/54000 (84%)] Loss: -1681.326172\n",
      "    epoch          : 929\n",
      "    loss           : -1687.8012538192295\n",
      "    val_loss       : -1688.0591293617151\n",
      "    val_log_likelihood: 1747.6414428710937\n",
      "    val_log_marginal: 1699.1813216209412\n",
      "Train Epoch: 930 [512/54000 (1%)] Loss: -1936.381226\n",
      "Train Epoch: 930 [11776/54000 (22%)] Loss: -1653.087646\n",
      "Train Epoch: 930 [23040/54000 (43%)] Loss: -1612.410645\n",
      "Train Epoch: 930 [34304/54000 (64%)] Loss: -1606.216064\n",
      "Train Epoch: 930 [45568/54000 (84%)] Loss: -1659.460083\n",
      "    epoch          : 930\n",
      "    loss           : -1687.9238970161664\n",
      "    val_loss       : -1686.4856079590506\n",
      "    val_log_likelihood: 1761.5322998046875\n",
      "    val_log_marginal: 1710.6995111245662\n",
      "Train Epoch: 931 [512/54000 (1%)] Loss: -1930.136963\n",
      "Train Epoch: 931 [11776/54000 (22%)] Loss: -1759.531982\n",
      "Train Epoch: 931 [23040/54000 (43%)] Loss: -1615.440674\n",
      "Train Epoch: 931 [34304/54000 (64%)] Loss: -1924.731934\n",
      "Train Epoch: 931 [45568/54000 (84%)] Loss: -1682.320435\n",
      "    epoch          : 931\n",
      "    loss           : -1690.575330677599\n",
      "    val_loss       : -1683.8247133879922\n",
      "    val_log_likelihood: 1769.0627319335938\n",
      "    val_log_marginal: 1721.4052637510001\n",
      "Train Epoch: 932 [512/54000 (1%)] Loss: -1934.285645\n",
      "Train Epoch: 932 [11776/54000 (22%)] Loss: -1685.668213\n",
      "Train Epoch: 932 [23040/54000 (43%)] Loss: -1657.959229\n",
      "Train Epoch: 932 [34304/54000 (64%)] Loss: -1649.049683\n",
      "Train Epoch: 932 [45568/54000 (84%)] Loss: -1588.752808\n",
      "    epoch          : 932\n",
      "    loss           : -1690.9097102703433\n",
      "    val_loss       : -1686.2649006017484\n",
      "    val_log_likelihood: 1754.3010620117188\n",
      "    val_log_marginal: 1702.3077252481132\n",
      "Train Epoch: 933 [512/54000 (1%)] Loss: -1935.999512\n",
      "Train Epoch: 933 [11776/54000 (22%)] Loss: -1759.057617\n",
      "Train Epoch: 933 [23040/54000 (43%)] Loss: -1670.239502\n",
      "Train Epoch: 933 [34304/54000 (64%)] Loss: -1663.380981\n",
      "Train Epoch: 933 [45568/54000 (84%)] Loss: -1625.089478\n",
      "    epoch          : 933\n",
      "    loss           : -1686.7895314433788\n",
      "    val_loss       : -1685.670690468885\n",
      "    val_log_likelihood: 1750.0906616210937\n",
      "    val_log_marginal: 1712.5606366749853\n",
      "Train Epoch: 934 [512/54000 (1%)] Loss: -1940.871094\n",
      "Train Epoch: 934 [11776/54000 (22%)] Loss: -1626.196411\n",
      "Train Epoch: 934 [23040/54000 (43%)] Loss: -1678.238892\n",
      "Train Epoch: 934 [34304/54000 (64%)] Loss: -1636.810913\n",
      "Train Epoch: 934 [45568/54000 (84%)] Loss: -1588.702881\n",
      "    epoch          : 934\n",
      "    loss           : -1683.695282284576\n",
      "    val_loss       : -1680.5694610010833\n",
      "    val_log_likelihood: 1769.479931640625\n",
      "    val_log_marginal: 1720.8150027625263\n",
      "Train Epoch: 935 [512/54000 (1%)] Loss: -1624.912842\n",
      "Train Epoch: 935 [11776/54000 (22%)] Loss: -1595.071289\n",
      "Train Epoch: 935 [23040/54000 (43%)] Loss: -1656.539795\n",
      "Train Epoch: 935 [34304/54000 (64%)] Loss: -1699.824219\n",
      "Train Epoch: 935 [45568/54000 (84%)] Loss: -1662.156738\n",
      "    epoch          : 935\n",
      "    loss           : -1686.0742417137221\n",
      "    val_loss       : -1666.6342741867527\n",
      "    val_log_likelihood: 1751.1908081054687\n",
      "    val_log_marginal: 1714.6615063454956\n",
      "Train Epoch: 936 [512/54000 (1%)] Loss: -1914.965332\n",
      "Train Epoch: 936 [11776/54000 (22%)] Loss: -1599.183960\n",
      "Train Epoch: 936 [23040/54000 (43%)] Loss: -1579.373779\n",
      "Train Epoch: 936 [34304/54000 (64%)] Loss: -1706.444702\n",
      "Train Epoch: 936 [45568/54000 (84%)] Loss: -1591.805420\n",
      "    epoch          : 936\n",
      "    loss           : -1679.6839515006188\n",
      "    val_loss       : -1674.287073739432\n",
      "    val_log_likelihood: 1767.8650756835937\n",
      "    val_log_marginal: 1725.3207450442017\n",
      "Train Epoch: 937 [512/54000 (1%)] Loss: -1750.753906\n",
      "Train Epoch: 937 [11776/54000 (22%)] Loss: -1630.851807\n",
      "Train Epoch: 937 [23040/54000 (43%)] Loss: -1593.911133\n",
      "Train Epoch: 937 [34304/54000 (64%)] Loss: -1698.113159\n",
      "Train Epoch: 937 [45568/54000 (84%)] Loss: -1679.567627\n",
      "    epoch          : 937\n",
      "    loss           : -1682.357939163057\n",
      "    val_loss       : -1682.1611061966978\n",
      "    val_log_likelihood: 1767.4714233398438\n",
      "    val_log_marginal: 1727.9613433882594\n",
      "Train Epoch: 938 [512/54000 (1%)] Loss: -1641.803223\n",
      "Train Epoch: 938 [11776/54000 (22%)] Loss: -1662.251709\n",
      "Train Epoch: 938 [23040/54000 (43%)] Loss: -1670.935547\n",
      "Train Epoch: 938 [34304/54000 (64%)] Loss: -1630.570679\n",
      "Train Epoch: 938 [45568/54000 (84%)] Loss: -1575.239868\n",
      "    epoch          : 938\n",
      "    loss           : -1673.3953615698483\n",
      "    val_loss       : -1599.596428911481\n",
      "    val_log_likelihood: 1728.876025390625\n",
      "    val_log_marginal: 1697.3244909316302\n",
      "Train Epoch: 939 [512/54000 (1%)] Loss: -1920.310059\n",
      "Train Epoch: 939 [11776/54000 (22%)] Loss: -1669.408203\n",
      "Train Epoch: 939 [23040/54000 (43%)] Loss: -1610.962036\n",
      "Train Epoch: 939 [34304/54000 (64%)] Loss: -1643.435547\n",
      "Train Epoch: 939 [45568/54000 (84%)] Loss: -1631.417603\n",
      "    epoch          : 939\n",
      "    loss           : -1637.6376904780323\n",
      "    val_loss       : -1652.9330470891668\n",
      "    val_log_likelihood: 1748.0862426757812\n",
      "    val_log_marginal: 1704.9618387231662\n",
      "Train Epoch: 940 [512/54000 (1%)] Loss: -1931.591797\n",
      "Train Epoch: 940 [11776/54000 (22%)] Loss: -1587.135132\n",
      "Train Epoch: 940 [23040/54000 (43%)] Loss: -1582.651123\n",
      "Train Epoch: 940 [34304/54000 (64%)] Loss: -1935.008057\n",
      "Train Epoch: 940 [45568/54000 (84%)] Loss: -1652.072876\n",
      "    epoch          : 940\n",
      "    loss           : -1662.532230188351\n",
      "    val_loss       : -1669.3301423531025\n",
      "    val_log_likelihood: 1767.1063354492187\n",
      "    val_log_marginal: 1714.45300575234\n",
      "Train Epoch: 941 [512/54000 (1%)] Loss: -1739.929810\n",
      "Train Epoch: 941 [11776/54000 (22%)] Loss: -1615.757446\n",
      "Train Epoch: 941 [23040/54000 (43%)] Loss: -1547.483643\n",
      "Train Epoch: 941 [34304/54000 (64%)] Loss: -1617.045410\n",
      "Train Epoch: 941 [45568/54000 (84%)] Loss: -1653.979248\n",
      "    epoch          : 941\n",
      "    loss           : -1669.4699223584469\n",
      "    val_loss       : -1663.419538510777\n",
      "    val_log_likelihood: 1757.3360229492187\n",
      "    val_log_marginal: 1723.4900553051382\n",
      "Train Epoch: 942 [512/54000 (1%)] Loss: -1929.731567\n",
      "Train Epoch: 942 [11776/54000 (22%)] Loss: -1547.146362\n",
      "Train Epoch: 942 [23040/54000 (43%)] Loss: -1671.399048\n",
      "Train Epoch: 942 [34304/54000 (64%)] Loss: -1913.405640\n",
      "Train Epoch: 942 [45568/54000 (84%)] Loss: -1656.023315\n",
      "    epoch          : 942\n",
      "    loss           : -1674.8848611057394\n",
      "    val_loss       : -1683.3063645023853\n",
      "    val_log_likelihood: 1765.0902221679687\n",
      "    val_log_marginal: 1710.3821441933512\n",
      "Train Epoch: 943 [512/54000 (1%)] Loss: -1916.739014\n",
      "Train Epoch: 943 [11776/54000 (22%)] Loss: -1631.907104\n",
      "Train Epoch: 943 [23040/54000 (43%)] Loss: -1639.336670\n",
      "Train Epoch: 943 [34304/54000 (64%)] Loss: -1589.868652\n",
      "Train Epoch: 943 [45568/54000 (84%)] Loss: -1673.810181\n",
      "    epoch          : 943\n",
      "    loss           : -1682.2326974396658\n",
      "    val_loss       : -1679.9821674424225\n",
      "    val_log_likelihood: 1771.5599243164063\n",
      "    val_log_marginal: 1721.6668195929378\n",
      "Train Epoch: 944 [512/54000 (1%)] Loss: -1933.277832\n",
      "Train Epoch: 944 [11776/54000 (22%)] Loss: -1606.695923\n",
      "Train Epoch: 944 [23040/54000 (43%)] Loss: -1612.856201\n",
      "Train Epoch: 944 [34304/54000 (64%)] Loss: -1618.566895\n",
      "Train Epoch: 944 [45568/54000 (84%)] Loss: -1696.919189\n",
      "    epoch          : 944\n",
      "    loss           : -1684.4507029316212\n",
      "    val_loss       : -1684.9758813552558\n",
      "    val_log_likelihood: 1773.028466796875\n",
      "    val_log_marginal: 1732.51108343862\n",
      "Train Epoch: 945 [512/54000 (1%)] Loss: -1928.292969\n",
      "Train Epoch: 945 [11776/54000 (22%)] Loss: -1617.996094\n",
      "Train Epoch: 945 [23040/54000 (43%)] Loss: -1637.281006\n",
      "Train Epoch: 945 [34304/54000 (64%)] Loss: -1926.443970\n",
      "Train Epoch: 945 [45568/54000 (84%)] Loss: -1593.602295\n",
      "    epoch          : 945\n",
      "    loss           : -1690.7514853902383\n",
      "    val_loss       : -1693.9036550672724\n",
      "    val_log_likelihood: 1769.5348999023438\n",
      "    val_log_marginal: 1721.2096432209014\n",
      "Train Epoch: 946 [512/54000 (1%)] Loss: -1754.437500\n",
      "Train Epoch: 946 [11776/54000 (22%)] Loss: -1635.750488\n",
      "Train Epoch: 946 [23040/54000 (43%)] Loss: -1583.057861\n",
      "Train Epoch: 946 [34304/54000 (64%)] Loss: -1926.087158\n",
      "Train Epoch: 946 [45568/54000 (84%)] Loss: -1587.977295\n",
      "    epoch          : 946\n",
      "    loss           : -1686.8330078125\n",
      "    val_loss       : -1697.3873527598566\n",
      "    val_log_likelihood: 1761.5128540039063\n",
      "    val_log_marginal: 1722.9185371242463\n",
      "Train Epoch: 947 [512/54000 (1%)] Loss: -1909.130493\n",
      "Train Epoch: 947 [11776/54000 (22%)] Loss: -1727.870483\n",
      "Train Epoch: 947 [23040/54000 (43%)] Loss: -1678.121704\n",
      "Train Epoch: 947 [34304/54000 (64%)] Loss: -1627.286377\n",
      "Train Epoch: 947 [45568/54000 (84%)] Loss: -1686.633911\n",
      "    epoch          : 947\n",
      "    loss           : -1680.3012622795482\n",
      "    val_loss       : -1673.8077097044327\n",
      "    val_log_likelihood: 1757.6124755859375\n",
      "    val_log_marginal: 1718.9518201809376\n",
      "Train Epoch: 948 [512/54000 (1%)] Loss: -1706.916870\n",
      "Train Epoch: 948 [11776/54000 (22%)] Loss: -1745.193115\n",
      "Train Epoch: 948 [23040/54000 (43%)] Loss: -1580.519775\n",
      "Train Epoch: 948 [34304/54000 (64%)] Loss: -1924.412720\n",
      "Train Epoch: 948 [45568/54000 (84%)] Loss: -1563.188965\n",
      "    epoch          : 948\n",
      "    loss           : -1685.0733848043008\n",
      "    val_loss       : -1684.5220510468819\n",
      "    val_log_likelihood: 1762.4222045898437\n",
      "    val_log_marginal: 1727.427433120832\n",
      "Train Epoch: 949 [512/54000 (1%)] Loss: -1760.072998\n",
      "Train Epoch: 949 [11776/54000 (22%)] Loss: -1721.901367\n",
      "Train Epoch: 949 [23040/54000 (43%)] Loss: -1592.120361\n",
      "Train Epoch: 949 [34304/54000 (64%)] Loss: -1604.720215\n",
      "Train Epoch: 949 [45568/54000 (84%)] Loss: -1697.385498\n",
      "    epoch          : 949\n",
      "    loss           : -1682.8605485670637\n",
      "    val_loss       : -1694.4362391266973\n",
      "    val_log_likelihood: 1770.96796875\n",
      "    val_log_marginal: 1734.2683243587612\n",
      "Train Epoch: 950 [512/54000 (1%)] Loss: -1914.073853\n",
      "Train Epoch: 950 [11776/54000 (22%)] Loss: -1743.213623\n",
      "Train Epoch: 950 [23040/54000 (43%)] Loss: -1669.338135\n",
      "Train Epoch: 950 [34304/54000 (64%)] Loss: -1729.278442\n",
      "Train Epoch: 950 [45568/54000 (84%)] Loss: -1663.124146\n",
      "    epoch          : 950\n",
      "    loss           : -1687.2359087349164\n",
      "    val_loss       : -1680.8957951135003\n",
      "    val_log_likelihood: 1755.7758544921876\n",
      "    val_log_marginal: 1712.514369912818\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch950.pth ...\n",
      "Train Epoch: 951 [512/54000 (1%)] Loss: -1934.857666\n",
      "Train Epoch: 951 [11776/54000 (22%)] Loss: -1645.852295\n",
      "Train Epoch: 951 [23040/54000 (43%)] Loss: -1557.221313\n",
      "Train Epoch: 951 [34304/54000 (64%)] Loss: -1584.657471\n",
      "Train Epoch: 951 [45568/54000 (84%)] Loss: -1721.856567\n",
      "    epoch          : 951\n",
      "    loss           : -1685.536756458849\n",
      "    val_loss       : -1664.258703719545\n",
      "    val_log_likelihood: 1737.6205932617188\n",
      "    val_log_marginal: 1696.7144495688378\n",
      "Train Epoch: 952 [512/54000 (1%)] Loss: -1898.481323\n",
      "Train Epoch: 952 [11776/54000 (22%)] Loss: -1582.815063\n",
      "Train Epoch: 952 [23040/54000 (43%)] Loss: -1666.361450\n",
      "Train Epoch: 952 [34304/54000 (64%)] Loss: -1579.156372\n",
      "Train Epoch: 952 [45568/54000 (84%)] Loss: -1670.292969\n",
      "    epoch          : 952\n",
      "    loss           : -1673.6066278136602\n",
      "    val_loss       : -1669.2553362162785\n",
      "    val_log_likelihood: 1757.4957275390625\n",
      "    val_log_marginal: 1714.0613862298428\n",
      "Train Epoch: 953 [512/54000 (1%)] Loss: -1943.682373\n",
      "Train Epoch: 953 [11776/54000 (22%)] Loss: -1722.670410\n",
      "Train Epoch: 953 [23040/54000 (43%)] Loss: -1555.717773\n",
      "Train Epoch: 953 [34304/54000 (64%)] Loss: -1605.821045\n",
      "Train Epoch: 953 [45568/54000 (84%)] Loss: -1643.273926\n",
      "    epoch          : 953\n",
      "    loss           : -1680.0324827892946\n",
      "    val_loss       : -1671.9221017203295\n",
      "    val_log_likelihood: 1759.659375\n",
      "    val_log_marginal: 1715.0240414362402\n",
      "Train Epoch: 954 [512/54000 (1%)] Loss: -1902.052246\n",
      "Train Epoch: 954 [11776/54000 (22%)] Loss: -1751.896240\n",
      "Train Epoch: 954 [23040/54000 (43%)] Loss: -1667.366943\n",
      "Train Epoch: 954 [34304/54000 (64%)] Loss: -1590.635132\n",
      "Train Epoch: 954 [45568/54000 (84%)] Loss: -1635.026001\n",
      "    epoch          : 954\n",
      "    loss           : -1678.1534097501547\n",
      "    val_loss       : -1653.861301134061\n",
      "    val_log_likelihood: 1733.0117309570312\n",
      "    val_log_marginal: 1692.8331856224686\n",
      "Train Epoch: 955 [512/54000 (1%)] Loss: -1619.408691\n",
      "Train Epoch: 955 [11776/54000 (22%)] Loss: -1574.277832\n",
      "Train Epoch: 955 [23040/54000 (43%)] Loss: -1718.780396\n",
      "Train Epoch: 955 [34304/54000 (64%)] Loss: -1565.488281\n",
      "Train Epoch: 955 [45568/54000 (84%)] Loss: -1639.699341\n",
      "    epoch          : 955\n",
      "    loss           : -1672.9324939085705\n",
      "    val_loss       : -1672.1891536554322\n",
      "    val_log_likelihood: 1755.1684692382812\n",
      "    val_log_marginal: 1706.0480773299932\n",
      "Train Epoch: 956 [512/54000 (1%)] Loss: -1936.100830\n",
      "Train Epoch: 956 [11776/54000 (22%)] Loss: -1625.786865\n",
      "Train Epoch: 956 [23040/54000 (43%)] Loss: -1634.916138\n",
      "Train Epoch: 956 [34304/54000 (64%)] Loss: -1935.460815\n",
      "Train Epoch: 956 [45568/54000 (84%)] Loss: -1657.219727\n",
      "    epoch          : 956\n",
      "    loss           : -1688.0196823271194\n",
      "    val_loss       : -1683.9129529785364\n",
      "    val_log_likelihood: 1762.149951171875\n",
      "    val_log_marginal: 1721.5141277704388\n",
      "Train Epoch: 957 [512/54000 (1%)] Loss: -1923.718506\n",
      "Train Epoch: 957 [11776/54000 (22%)] Loss: -1762.083496\n",
      "Train Epoch: 957 [23040/54000 (43%)] Loss: -1582.379639\n",
      "Train Epoch: 957 [34304/54000 (64%)] Loss: -1643.286865\n",
      "Train Epoch: 957 [45568/54000 (84%)] Loss: -1612.813477\n",
      "    epoch          : 957\n",
      "    loss           : -1671.6873440884128\n",
      "    val_loss       : -1668.6487788932398\n",
      "    val_log_likelihood: 1729.5823486328125\n",
      "    val_log_marginal: 1688.0422354712423\n",
      "Train Epoch: 958 [512/54000 (1%)] Loss: -1911.266357\n",
      "Train Epoch: 958 [11776/54000 (22%)] Loss: -1590.279907\n",
      "Train Epoch: 958 [23040/54000 (43%)] Loss: -1586.920166\n",
      "Train Epoch: 958 [34304/54000 (64%)] Loss: -1931.739014\n",
      "Train Epoch: 958 [45568/54000 (84%)] Loss: -1687.011475\n",
      "    epoch          : 958\n",
      "    loss           : -1678.1283805016244\n",
      "    val_loss       : -1683.796064176131\n",
      "    val_log_likelihood: 1762.1385375976563\n",
      "    val_log_marginal: 1717.6678811561317\n",
      "Train Epoch: 959 [512/54000 (1%)] Loss: -1739.397217\n",
      "Train Epoch: 959 [11776/54000 (22%)] Loss: -1594.458496\n",
      "Train Epoch: 959 [23040/54000 (43%)] Loss: -1676.445190\n",
      "Train Epoch: 959 [34304/54000 (64%)] Loss: -1679.839600\n",
      "Train Epoch: 959 [45568/54000 (84%)] Loss: -1567.194336\n",
      "    epoch          : 959\n",
      "    loss           : -1685.4881035833075\n",
      "    val_loss       : -1672.5345637406222\n",
      "    val_log_likelihood: 1766.1925415039063\n",
      "    val_log_marginal: 1722.2259242281318\n",
      "Train Epoch: 960 [512/54000 (1%)] Loss: -1932.224121\n",
      "Train Epoch: 960 [11776/54000 (22%)] Loss: -1622.381592\n",
      "Train Epoch: 960 [23040/54000 (43%)] Loss: -1575.771484\n",
      "Train Epoch: 960 [34304/54000 (64%)] Loss: -1674.125122\n",
      "Train Epoch: 960 [45568/54000 (84%)] Loss: -1646.162842\n",
      "    epoch          : 960\n",
      "    loss           : -1681.1583167349938\n",
      "    val_loss       : -1676.28186460305\n",
      "    val_log_likelihood: 1765.8653686523437\n",
      "    val_log_marginal: 1721.5680447477848\n",
      "Train Epoch: 961 [512/54000 (1%)] Loss: -1918.364380\n",
      "Train Epoch: 961 [11776/54000 (22%)] Loss: -1789.876099\n",
      "Train Epoch: 961 [23040/54000 (43%)] Loss: -1578.625122\n",
      "Train Epoch: 961 [34304/54000 (64%)] Loss: -1551.253662\n",
      "Train Epoch: 961 [45568/54000 (84%)] Loss: -1607.940796\n",
      "    epoch          : 961\n",
      "    loss           : -1684.1796016881963\n",
      "    val_loss       : -1685.6055687003768\n",
      "    val_log_likelihood: 1755.374658203125\n",
      "    val_log_marginal: 1720.8482939615847\n",
      "Train Epoch: 962 [512/54000 (1%)] Loss: -1948.501953\n",
      "Train Epoch: 962 [11776/54000 (22%)] Loss: -1785.777100\n",
      "Train Epoch: 962 [23040/54000 (43%)] Loss: -1690.199097\n",
      "Train Epoch: 962 [34304/54000 (64%)] Loss: -1664.895386\n",
      "Train Epoch: 962 [45568/54000 (84%)] Loss: -1644.929199\n",
      "    epoch          : 962\n",
      "    loss           : -1684.0367141572556\n",
      "    val_loss       : -1667.6947421034797\n",
      "    val_log_likelihood: 1746.5274047851562\n",
      "    val_log_marginal: 1702.5498880598693\n",
      "Train Epoch: 963 [512/54000 (1%)] Loss: -1908.256348\n",
      "Train Epoch: 963 [11776/54000 (22%)] Loss: -1606.222290\n",
      "Train Epoch: 963 [23040/54000 (43%)] Loss: -1725.020264\n",
      "Train Epoch: 963 [34304/54000 (64%)] Loss: -1632.104004\n",
      "Train Epoch: 963 [45568/54000 (84%)] Loss: -1647.427979\n",
      "    epoch          : 963\n",
      "    loss           : -1676.326198464573\n",
      "    val_loss       : -1674.2418256375008\n",
      "    val_log_likelihood: 1762.4049926757812\n",
      "    val_log_marginal: 1717.8482485346497\n",
      "Train Epoch: 964 [512/54000 (1%)] Loss: -1921.657715\n",
      "Train Epoch: 964 [11776/54000 (22%)] Loss: -1730.817383\n",
      "Train Epoch: 964 [23040/54000 (43%)] Loss: -1684.097290\n",
      "Train Epoch: 964 [34304/54000 (64%)] Loss: -1671.137451\n",
      "Train Epoch: 964 [45568/54000 (84%)] Loss: -1667.336182\n",
      "    epoch          : 964\n",
      "    loss           : -1682.9162815207303\n",
      "    val_loss       : -1678.8272985881194\n",
      "    val_log_likelihood: 1751.5485595703126\n",
      "    val_log_marginal: 1712.537530266121\n",
      "Train Epoch: 965 [512/54000 (1%)] Loss: -1664.773926\n",
      "Train Epoch: 965 [11776/54000 (22%)] Loss: -1636.153076\n",
      "Train Epoch: 965 [23040/54000 (43%)] Loss: -1598.924072\n",
      "Train Epoch: 965 [34304/54000 (64%)] Loss: -1927.208374\n",
      "Train Epoch: 965 [45568/54000 (84%)] Loss: -1666.093262\n",
      "    epoch          : 965\n",
      "    loss           : -1685.4170598700496\n",
      "    val_loss       : -1682.809570868034\n",
      "    val_log_likelihood: 1765.8356201171875\n",
      "    val_log_marginal: 1729.5789516355842\n",
      "Train Epoch: 966 [512/54000 (1%)] Loss: -1941.958740\n",
      "Train Epoch: 966 [11776/54000 (22%)] Loss: -1752.853149\n",
      "Train Epoch: 966 [23040/54000 (43%)] Loss: -1678.757568\n",
      "Train Epoch: 966 [34304/54000 (64%)] Loss: -1659.305908\n",
      "Train Epoch: 966 [45568/54000 (84%)] Loss: -1718.566895\n",
      "    epoch          : 966\n",
      "    loss           : -1689.8969774907177\n",
      "    val_loss       : -1686.8258600553497\n",
      "    val_log_likelihood: 1764.81171875\n",
      "    val_log_marginal: 1722.261935378611\n",
      "Train Epoch: 967 [512/54000 (1%)] Loss: -1928.082642\n",
      "Train Epoch: 967 [11776/54000 (22%)] Loss: -1643.004272\n",
      "Train Epoch: 967 [23040/54000 (43%)] Loss: -1723.387451\n",
      "Train Epoch: 967 [34304/54000 (64%)] Loss: -1653.181152\n",
      "Train Epoch: 967 [45568/54000 (84%)] Loss: -1649.808960\n",
      "    epoch          : 967\n",
      "    loss           : -1688.3038148785581\n",
      "    val_loss       : -1687.1772836208343\n",
      "    val_log_likelihood: 1768.9765258789062\n",
      "    val_log_marginal: 1719.647423115\n",
      "Train Epoch: 968 [512/54000 (1%)] Loss: -1935.366333\n",
      "Train Epoch: 968 [11776/54000 (22%)] Loss: -1770.735229\n",
      "Train Epoch: 968 [23040/54000 (43%)] Loss: -1591.962646\n",
      "Train Epoch: 968 [34304/54000 (64%)] Loss: -1663.515259\n",
      "Train Epoch: 968 [45568/54000 (84%)] Loss: -1596.472778\n",
      "    epoch          : 968\n",
      "    loss           : -1692.0392981803063\n",
      "    val_loss       : -1689.736975256726\n",
      "    val_log_likelihood: 1774.1235229492188\n",
      "    val_log_marginal: 1725.8255964837967\n",
      "Train Epoch: 969 [512/54000 (1%)] Loss: -1928.202148\n",
      "Train Epoch: 969 [11776/54000 (22%)] Loss: -1643.210205\n",
      "Train Epoch: 969 [23040/54000 (43%)] Loss: -1640.062622\n",
      "Train Epoch: 969 [34304/54000 (64%)] Loss: -1605.635498\n",
      "Train Epoch: 969 [45568/54000 (84%)] Loss: -1703.463379\n",
      "    epoch          : 969\n",
      "    loss           : -1694.7096539487934\n",
      "    val_loss       : -1691.2004865858703\n",
      "    val_log_likelihood: 1763.6288696289062\n",
      "    val_log_marginal: 1734.8556925438345\n",
      "Train Epoch: 970 [512/54000 (1%)] Loss: -1941.663330\n",
      "Train Epoch: 970 [11776/54000 (22%)] Loss: -1655.816895\n",
      "Train Epoch: 970 [23040/54000 (43%)] Loss: -1743.455078\n",
      "Train Epoch: 970 [34304/54000 (64%)] Loss: -1709.254395\n",
      "Train Epoch: 970 [45568/54000 (84%)] Loss: -1709.576660\n",
      "    epoch          : 970\n",
      "    loss           : -1692.2500374671256\n",
      "    val_loss       : -1694.3129132401198\n",
      "    val_log_likelihood: 1765.207958984375\n",
      "    val_log_marginal: 1729.582471935734\n",
      "Train Epoch: 971 [512/54000 (1%)] Loss: -1930.941284\n",
      "Train Epoch: 971 [11776/54000 (22%)] Loss: -1648.915161\n",
      "Train Epoch: 971 [23040/54000 (43%)] Loss: -1583.663330\n",
      "Train Epoch: 971 [34304/54000 (64%)] Loss: -1688.026855\n",
      "Train Epoch: 971 [45568/54000 (84%)] Loss: -1672.015381\n",
      "    epoch          : 971\n",
      "    loss           : -1690.3618345355044\n",
      "    val_loss       : -1678.3595340464265\n",
      "    val_log_likelihood: 1748.2011352539062\n",
      "    val_log_marginal: 1713.777775165066\n",
      "Train Epoch: 972 [512/54000 (1%)] Loss: -1783.674072\n",
      "Train Epoch: 972 [11776/54000 (22%)] Loss: -1571.858154\n",
      "Train Epoch: 972 [23040/54000 (43%)] Loss: -1697.097046\n",
      "Train Epoch: 972 [34304/54000 (64%)] Loss: -1626.814941\n",
      "Train Epoch: 972 [45568/54000 (84%)] Loss: -1648.825684\n",
      "    epoch          : 972\n",
      "    loss           : -1691.9011157951732\n",
      "    val_loss       : -1695.2678338649682\n",
      "    val_log_likelihood: 1755.4536743164062\n",
      "    val_log_marginal: 1707.0411641482265\n",
      "Train Epoch: 973 [512/54000 (1%)] Loss: -1950.294800\n",
      "Train Epoch: 973 [11776/54000 (22%)] Loss: -1688.788086\n",
      "Train Epoch: 973 [23040/54000 (43%)] Loss: -1676.573975\n",
      "Train Epoch: 973 [34304/54000 (64%)] Loss: -1702.640381\n",
      "Train Epoch: 973 [45568/54000 (84%)] Loss: -1637.445190\n",
      "    epoch          : 973\n",
      "    loss           : -1688.0296884668935\n",
      "    val_loss       : -1691.7352035632357\n",
      "    val_log_likelihood: 1759.1927978515625\n",
      "    val_log_marginal: 1711.7556419361383\n",
      "Train Epoch: 974 [512/54000 (1%)] Loss: -1936.591187\n",
      "Train Epoch: 974 [11776/54000 (22%)] Loss: -1616.691895\n",
      "Train Epoch: 974 [23040/54000 (43%)] Loss: -1575.841553\n",
      "Train Epoch: 974 [34304/54000 (64%)] Loss: -1598.956543\n",
      "Train Epoch: 974 [45568/54000 (84%)] Loss: -1584.417725\n",
      "    epoch          : 974\n",
      "    loss           : -1685.2053355604114\n",
      "    val_loss       : -1689.6036885758863\n",
      "    val_log_likelihood: 1769.1131713867187\n",
      "    val_log_marginal: 1713.5083236921578\n",
      "Train Epoch: 975 [512/54000 (1%)] Loss: -1944.476562\n",
      "Train Epoch: 975 [11776/54000 (22%)] Loss: -1635.084473\n",
      "Train Epoch: 975 [23040/54000 (43%)] Loss: -1675.793945\n",
      "Train Epoch: 975 [34304/54000 (64%)] Loss: -1617.490234\n",
      "Train Epoch: 975 [45568/54000 (84%)] Loss: -1699.014404\n",
      "    epoch          : 975\n",
      "    loss           : -1688.612145150062\n",
      "    val_loss       : -1685.518263129238\n",
      "    val_log_likelihood: 1765.3349243164062\n",
      "    val_log_marginal: 1725.9831017959864\n",
      "Train Epoch: 976 [512/54000 (1%)] Loss: -1931.781860\n",
      "Train Epoch: 976 [11776/54000 (22%)] Loss: -1627.079224\n",
      "Train Epoch: 976 [23040/54000 (43%)] Loss: -1687.827148\n",
      "Train Epoch: 976 [34304/54000 (64%)] Loss: -1596.567749\n",
      "Train Epoch: 976 [45568/54000 (84%)] Loss: -1594.784058\n",
      "    epoch          : 976\n",
      "    loss           : -1691.6647211962406\n",
      "    val_loss       : -1691.7648521651513\n",
      "    val_log_likelihood: 1770.5807006835937\n",
      "    val_log_marginal: 1728.8050761699676\n",
      "Train Epoch: 977 [512/54000 (1%)] Loss: -1956.331665\n",
      "Train Epoch: 977 [11776/54000 (22%)] Loss: -1717.842285\n",
      "Train Epoch: 977 [23040/54000 (43%)] Loss: -1609.708618\n",
      "Train Epoch: 977 [34304/54000 (64%)] Loss: -1661.155762\n",
      "Train Epoch: 977 [45568/54000 (84%)] Loss: -1579.680176\n",
      "    epoch          : 977\n",
      "    loss           : -1688.6517587793935\n",
      "    val_loss       : -1688.4995637348852\n",
      "    val_log_likelihood: 1765.4681762695313\n",
      "    val_log_marginal: 1717.4009629447014\n",
      "Train Epoch: 978 [512/54000 (1%)] Loss: -1748.991211\n",
      "Train Epoch: 978 [11776/54000 (22%)] Loss: -1717.860229\n",
      "Train Epoch: 978 [23040/54000 (43%)] Loss: -1577.444580\n",
      "Train Epoch: 978 [34304/54000 (64%)] Loss: -1609.857300\n",
      "Train Epoch: 978 [45568/54000 (84%)] Loss: -1689.151489\n",
      "    epoch          : 978\n",
      "    loss           : -1687.107181360226\n",
      "    val_loss       : -1658.7290762335062\n",
      "    val_log_likelihood: 1741.3288696289062\n",
      "    val_log_marginal: 1704.4758110359312\n",
      "Train Epoch: 979 [512/54000 (1%)] Loss: -1922.400635\n",
      "Train Epoch: 979 [11776/54000 (22%)] Loss: -1550.845703\n",
      "Train Epoch: 979 [23040/54000 (43%)] Loss: -1672.235718\n",
      "Train Epoch: 979 [34304/54000 (64%)] Loss: -1621.249146\n",
      "Train Epoch: 979 [45568/54000 (84%)] Loss: -1673.301758\n",
      "    epoch          : 979\n",
      "    loss           : -1679.6877054648824\n",
      "    val_loss       : -1683.417773303762\n",
      "    val_log_likelihood: 1756.0044189453124\n",
      "    val_log_marginal: 1716.4162131257356\n",
      "Train Epoch: 980 [512/54000 (1%)] Loss: -1929.919556\n",
      "Train Epoch: 980 [11776/54000 (22%)] Loss: -1597.903076\n",
      "Train Epoch: 980 [23040/54000 (43%)] Loss: -1605.907104\n",
      "Train Epoch: 980 [34304/54000 (64%)] Loss: -1551.550415\n",
      "Train Epoch: 980 [45568/54000 (84%)] Loss: -1574.670166\n",
      "    epoch          : 980\n",
      "    loss           : -1666.7143083326887\n",
      "    val_loss       : -1646.964587026462\n",
      "    val_log_likelihood: 1735.6538818359375\n",
      "    val_log_marginal: 1689.8537762571127\n",
      "Train Epoch: 981 [512/54000 (1%)] Loss: -1878.689697\n",
      "Train Epoch: 981 [11776/54000 (22%)] Loss: -1557.499390\n",
      "Train Epoch: 981 [23040/54000 (43%)] Loss: -1688.367798\n",
      "Train Epoch: 981 [34304/54000 (64%)] Loss: -1536.797485\n",
      "Train Epoch: 981 [45568/54000 (84%)] Loss: -1657.581787\n",
      "    epoch          : 981\n",
      "    loss           : -1663.0835541073639\n",
      "    val_loss       : -1665.1362162488513\n",
      "    val_log_likelihood: 1739.7400146484374\n",
      "    val_log_marginal: 1707.7017841216177\n",
      "Train Epoch: 982 [512/54000 (1%)] Loss: -1762.966309\n",
      "Train Epoch: 982 [11776/54000 (22%)] Loss: -1747.401611\n",
      "Train Epoch: 982 [23040/54000 (43%)] Loss: -1595.464600\n",
      "Train Epoch: 982 [34304/54000 (64%)] Loss: -1605.040405\n",
      "Train Epoch: 982 [45568/54000 (84%)] Loss: -1617.383545\n",
      "    epoch          : 982\n",
      "    loss           : -1677.4459228515625\n",
      "    val_loss       : -1676.53951405501\n",
      "    val_log_likelihood: 1752.69921875\n",
      "    val_log_marginal: 1709.5725761871786\n",
      "Train Epoch: 983 [512/54000 (1%)] Loss: -1926.190063\n",
      "Train Epoch: 983 [11776/54000 (22%)] Loss: -1593.505615\n",
      "Train Epoch: 983 [23040/54000 (43%)] Loss: -1700.450195\n",
      "Train Epoch: 983 [34304/54000 (64%)] Loss: -1933.229736\n",
      "Train Epoch: 983 [45568/54000 (84%)] Loss: -1712.017944\n",
      "    epoch          : 983\n",
      "    loss           : -1680.8257319384281\n",
      "    val_loss       : -1689.536207840219\n",
      "    val_log_likelihood: 1748.6192138671875\n",
      "    val_log_marginal: 1712.3912157475947\n",
      "Train Epoch: 984 [512/54000 (1%)] Loss: -1927.061401\n",
      "Train Epoch: 984 [11776/54000 (22%)] Loss: -1624.889404\n",
      "Train Epoch: 984 [23040/54000 (43%)] Loss: -1725.776489\n",
      "Train Epoch: 984 [34304/54000 (64%)] Loss: -1591.006104\n",
      "Train Epoch: 984 [45568/54000 (84%)] Loss: -1666.913574\n",
      "    epoch          : 984\n",
      "    loss           : -1691.1806713142018\n",
      "    val_loss       : -1682.7722842854448\n",
      "    val_log_likelihood: 1751.4903198242187\n",
      "    val_log_marginal: 1706.0929861180484\n",
      "Train Epoch: 985 [512/54000 (1%)] Loss: -1924.530151\n",
      "Train Epoch: 985 [11776/54000 (22%)] Loss: -1768.929688\n",
      "Train Epoch: 985 [23040/54000 (43%)] Loss: -1617.662231\n",
      "Train Epoch: 985 [34304/54000 (64%)] Loss: -1564.483398\n",
      "Train Epoch: 985 [45568/54000 (84%)] Loss: -1671.496094\n",
      "    epoch          : 985\n",
      "    loss           : -1691.89464244276\n",
      "    val_loss       : -1655.6283182628454\n",
      "    val_log_likelihood: 1755.2006225585938\n",
      "    val_log_marginal: 1709.1883019432425\n",
      "Train Epoch: 986 [512/54000 (1%)] Loss: -1945.324463\n",
      "Train Epoch: 986 [11776/54000 (22%)] Loss: -1763.391724\n",
      "Train Epoch: 986 [23040/54000 (43%)] Loss: -1614.608154\n",
      "Train Epoch: 986 [34304/54000 (64%)] Loss: -1585.207031\n",
      "Train Epoch: 986 [45568/54000 (84%)] Loss: -1654.214111\n",
      "    epoch          : 986\n",
      "    loss           : -1676.5394770556156\n",
      "    val_loss       : -1676.5074764067308\n",
      "    val_log_likelihood: 1754.9302734375\n",
      "    val_log_marginal: 1704.2398841775953\n",
      "Train Epoch: 987 [512/54000 (1%)] Loss: -1919.435303\n",
      "Train Epoch: 987 [11776/54000 (22%)] Loss: -1734.622437\n",
      "Train Epoch: 987 [23040/54000 (43%)] Loss: -1717.143188\n",
      "Train Epoch: 987 [34304/54000 (64%)] Loss: -1589.229736\n",
      "Train Epoch: 987 [45568/54000 (84%)] Loss: -1630.906250\n",
      "    epoch          : 987\n",
      "    loss           : -1678.0801385558477\n",
      "    val_loss       : -1680.1175648904405\n",
      "    val_log_likelihood: 1744.1950073242188\n",
      "    val_log_marginal: 1699.5111086279153\n",
      "Train Epoch: 988 [512/54000 (1%)] Loss: -1924.350952\n",
      "Train Epoch: 988 [11776/54000 (22%)] Loss: -1741.003174\n",
      "Train Epoch: 988 [23040/54000 (43%)] Loss: -1704.064697\n",
      "Train Epoch: 988 [34304/54000 (64%)] Loss: -1596.104736\n",
      "Train Epoch: 988 [45568/54000 (84%)] Loss: -1666.283813\n",
      "    epoch          : 988\n",
      "    loss           : -1681.811223700495\n",
      "    val_loss       : -1679.2366870476865\n",
      "    val_log_likelihood: 1747.8677368164062\n",
      "    val_log_marginal: 1710.4415739342571\n",
      "Train Epoch: 989 [512/54000 (1%)] Loss: -1933.127197\n",
      "Train Epoch: 989 [11776/54000 (22%)] Loss: -1767.866455\n",
      "Train Epoch: 989 [23040/54000 (43%)] Loss: -1738.694336\n",
      "Train Epoch: 989 [34304/54000 (64%)] Loss: -1660.423096\n",
      "Train Epoch: 989 [45568/54000 (84%)] Loss: -1585.307129\n",
      "    epoch          : 989\n",
      "    loss           : -1684.8939293587562\n",
      "    val_loss       : -1674.8289660054259\n",
      "    val_log_likelihood: 1733.4047607421876\n",
      "    val_log_marginal: 1698.4138174161315\n",
      "Train Epoch: 990 [512/54000 (1%)] Loss: -1921.948730\n",
      "Train Epoch: 990 [11776/54000 (22%)] Loss: -1662.051636\n",
      "Train Epoch: 990 [23040/54000 (43%)] Loss: -1624.726807\n",
      "Train Epoch: 990 [34304/54000 (64%)] Loss: -1589.192993\n",
      "Train Epoch: 990 [45568/54000 (84%)] Loss: -1673.212646\n",
      "    epoch          : 990\n",
      "    loss           : -1681.243625754177\n",
      "    val_loss       : -1680.592848564405\n",
      "    val_log_likelihood: 1753.8905639648438\n",
      "    val_log_marginal: 1714.3308681745082\n",
      "Train Epoch: 991 [512/54000 (1%)] Loss: -1767.404419\n",
      "Train Epoch: 991 [11776/54000 (22%)] Loss: -1623.045776\n",
      "Train Epoch: 991 [23040/54000 (43%)] Loss: -1721.916138\n",
      "Train Epoch: 991 [34304/54000 (64%)] Loss: -1694.842285\n",
      "Train Epoch: 991 [45568/54000 (84%)] Loss: -1649.275269\n",
      "    epoch          : 991\n",
      "    loss           : -1684.935355913521\n",
      "    val_loss       : -1677.900551218912\n",
      "    val_log_likelihood: 1749.4165649414062\n",
      "    val_log_marginal: 1719.263818159327\n",
      "Train Epoch: 992 [512/54000 (1%)] Loss: -1931.967773\n",
      "Train Epoch: 992 [11776/54000 (22%)] Loss: -1755.773804\n",
      "Train Epoch: 992 [23040/54000 (43%)] Loss: -1592.038574\n",
      "Train Epoch: 992 [34304/54000 (64%)] Loss: -1588.781006\n",
      "Train Epoch: 992 [45568/54000 (84%)] Loss: -1691.552490\n",
      "    epoch          : 992\n",
      "    loss           : -1687.5887112759128\n",
      "    val_loss       : -1684.6339198277333\n",
      "    val_log_likelihood: 1750.401611328125\n",
      "    val_log_marginal: 1711.5746356397874\n",
      "Train Epoch: 993 [512/54000 (1%)] Loss: -1923.573975\n",
      "Train Epoch: 993 [11776/54000 (22%)] Loss: -1671.102783\n",
      "Train Epoch: 993 [23040/54000 (43%)] Loss: -1588.766846\n",
      "Train Epoch: 993 [34304/54000 (64%)] Loss: -1585.742188\n",
      "Train Epoch: 993 [45568/54000 (84%)] Loss: -1583.841553\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00993: reducing learning rate of group 0 to 5.0000e-04.\n",
      "    epoch          : 993\n",
      "    loss           : -1687.4555905785892\n",
      "    val_loss       : -1684.3552220768295\n",
      "    val_log_likelihood: 1748.9282958984375\n",
      "    val_log_marginal: 1708.8911546714603\n",
      "Train Epoch: 994 [512/54000 (1%)] Loss: -1936.430664\n",
      "Train Epoch: 994 [11776/54000 (22%)] Loss: -1765.321777\n",
      "Train Epoch: 994 [23040/54000 (43%)] Loss: -1637.383423\n",
      "Train Epoch: 994 [34304/54000 (64%)] Loss: -1721.791748\n",
      "Train Epoch: 994 [45568/54000 (84%)] Loss: -1615.015625\n",
      "    epoch          : 994\n",
      "    loss           : -1692.9868236579518\n",
      "    val_loss       : -1685.5797094812617\n",
      "    val_log_likelihood: 1759.4131469726562\n",
      "    val_log_marginal: 1719.2392766669393\n",
      "Train Epoch: 995 [512/54000 (1%)] Loss: -1565.718140\n",
      "Train Epoch: 995 [11776/54000 (22%)] Loss: -1617.381470\n",
      "Train Epoch: 995 [23040/54000 (43%)] Loss: -1694.179932\n",
      "Train Epoch: 995 [34304/54000 (64%)] Loss: -1604.564209\n",
      "Train Epoch: 995 [45568/54000 (84%)] Loss: -1597.346069\n",
      "    epoch          : 995\n",
      "    loss           : -1695.088052579672\n",
      "    val_loss       : -1690.4915329627693\n",
      "    val_log_likelihood: 1764.1837036132813\n",
      "    val_log_marginal: 1727.0726036645472\n",
      "Train Epoch: 996 [512/54000 (1%)] Loss: -1799.893555\n",
      "Train Epoch: 996 [11776/54000 (22%)] Loss: -1656.957764\n",
      "Train Epoch: 996 [23040/54000 (43%)] Loss: -1581.509033\n",
      "Train Epoch: 996 [34304/54000 (64%)] Loss: -1943.787354\n",
      "Train Epoch: 996 [45568/54000 (84%)] Loss: -1621.156738\n",
      "    epoch          : 996\n",
      "    loss           : -1698.9650745958386\n",
      "    val_loss       : -1697.2149420027622\n",
      "    val_log_likelihood: 1761.0789916992187\n",
      "    val_log_marginal: 1725.4249740239234\n",
      "Train Epoch: 997 [512/54000 (1%)] Loss: -1934.186035\n",
      "Train Epoch: 997 [11776/54000 (22%)] Loss: -1635.917236\n",
      "Train Epoch: 997 [23040/54000 (43%)] Loss: -1581.030396\n",
      "Train Epoch: 997 [34304/54000 (64%)] Loss: -1665.379761\n",
      "Train Epoch: 997 [45568/54000 (84%)] Loss: -1733.697266\n",
      "    epoch          : 997\n",
      "    loss           : -1694.9234026918316\n",
      "    val_loss       : -1695.2099936218\n",
      "    val_log_likelihood: 1767.8059814453125\n",
      "    val_log_marginal: 1726.8215052913874\n",
      "Train Epoch: 998 [512/54000 (1%)] Loss: -1795.910400\n",
      "Train Epoch: 998 [11776/54000 (22%)] Loss: -1638.842529\n",
      "Train Epoch: 998 [23040/54000 (43%)] Loss: -1702.044434\n",
      "Train Epoch: 998 [34304/54000 (64%)] Loss: -1734.437500\n",
      "Train Epoch: 998 [45568/54000 (84%)] Loss: -1665.538696\n",
      "    epoch          : 998\n",
      "    loss           : -1698.09342850789\n",
      "    val_loss       : -1704.1620940501803\n",
      "    val_log_likelihood: 1770.4539672851563\n",
      "    val_log_marginal: 1730.908411892131\n",
      "Train Epoch: 999 [512/54000 (1%)] Loss: -1758.846924\n",
      "Train Epoch: 999 [11776/54000 (22%)] Loss: -1602.918701\n",
      "Train Epoch: 999 [23040/54000 (43%)] Loss: -1660.863525\n",
      "Train Epoch: 999 [34304/54000 (64%)] Loss: -1939.490967\n",
      "Train Epoch: 999 [45568/54000 (84%)] Loss: -1708.538818\n",
      "    epoch          : 999\n",
      "    loss           : -1701.7446990060334\n",
      "    val_loss       : -1695.096178004332\n",
      "    val_log_likelihood: 1765.7692016601563\n",
      "    val_log_marginal: 1730.8762254372239\n",
      "Train Epoch: 1000 [512/54000 (1%)] Loss: -1782.498657\n",
      "Train Epoch: 1000 [11776/54000 (22%)] Loss: -1646.356201\n",
      "Train Epoch: 1000 [23040/54000 (43%)] Loss: -1587.970947\n",
      "Train Epoch: 1000 [34304/54000 (64%)] Loss: -1594.500000\n",
      "Train Epoch: 1000 [45568/54000 (84%)] Loss: -1720.154663\n",
      "    epoch          : 1000\n",
      "    loss           : -1701.530839070235\n",
      "    val_loss       : -1695.3517962621524\n",
      "    val_log_likelihood: 1762.1600463867187\n",
      "    val_log_marginal: 1723.2640057192082\n",
      "Saving checkpoint: saved/models/Mnist_VaeOperad/1119_152224/checkpoint-epoch1000.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VaeOperadicModel(\n",
       "  (_operad): FreeOperad(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=106, bias=True)\n",
       "        (1): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=106, out_features=106, bias=True)\n",
       "        (4): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=106, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=114, bias=True)\n",
       "        (1): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=114, out_features=114, bias=True)\n",
       "        (4): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=114, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_10): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=130, bias=True)\n",
       "        (1): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=130, out_features=130, bias=True)\n",
       "        (4): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=130, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_11): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_12): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=162, bias=True)\n",
       "        (1): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=162, out_features=162, bias=True)\n",
       "        (4): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=162, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_13): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_14): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=40, bias=True)\n",
       "  )\n",
       "  (encoders): ModuleDict(\n",
       "    ($p(Z^{32} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (3): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=96, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{64} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (3): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=192, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{128} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=384, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=32, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | \\mathbb{R}^{16})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{64} | \\mathbb{R}^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (3): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=192, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{128} | \\mathbb{R}^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=384, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | \\mathbb{R}^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=64, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | \\mathbb{R}^{32})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{128} | \\mathbb{R}^{64})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=384, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196} | \\mathbb{R}^{64})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=128, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | \\mathbb{R}^{64})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{196} | \\mathbb{R}^{128})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=588, out_features=588, bias=True)\n",
       "        (3): LayerNorm((588,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=588, out_features=256, bias=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(X^{784} | \\mathbb{R}^{128})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(X^{784} | \\mathbb{R}^{196})$): MlpEncoder(\n",
       "      (outcoder): Sequential(\n",
       "        (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (3): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Linear(in_features=784, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    ($p(Z^{16})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{64})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{196})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{128})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "    ($p(Z^{32})$): MlpEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApQAAAHzCAYAAACe1o1DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAWgUlEQVR4nO3dMVMbZ9eA4ZNvUlHJDDWFXNG8hQS/IKJNJVDj1qhNxY5/AYOqtOA2DUZVWq1/AUatK+QZ1xhv5Zav8Kt9jQE7yYm9MrquKlq04uAic8+z+6x+ur6+vg4AAPiH/q/pAQAA+LEJSgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkPJz0wN8ydu3b+Py8rLpMQAAGre2thbr6+tNj3GnhQ3Kt2/fxsbGRnz48KHpUQAAGreyshKvX79eyKhc2KC8vLyMDx8+xB9//BEbGxtNjwMA0JjXr1/HkydP4vLyUlD+ExsbG9HpdJoeAwCAe9iUAwBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghLgL6qqKoqiiOl0euP4aDSK8Xgco9EoZrPZrfeXZXnrHICH5OemBwBoQlVV0Wq1/tY5r169iqqqbhwbj8cREdHv96Oqqnj69Gmcnp5GRMTOzk5MJpOI+BidnU4nPTfAIrJCCSyd4+PjuLq6unFsZ2cnHj9+HEVRxGg0iuPj4+h2u9Htduv39Hq9WxF6dXUV7969i4iIVqtVr0SWZRntdjtms1lUVRX7+/sR8TEsAR4aK5TAUplOp7G6uhrtdvvWzy4uLur/Ho/HMZvN4s2bN1/8vN3d3Xj69GlUVRVlWdahOpvN6svf8+N7e3uxt7cXRVHE4eHhv/hXATTLCiWwVA4ODqLf7984VpbljcCbTqfx9OnTOD8//+pl8VarFaenpzGbzaLT6dwI1fnrfr8fRVHU74+IG/daAvzoBCWwNKqqunNlcnNzsz5eVVX88ssvcXp6eud77/rM+f2RZVnGs2fPIuLj5fH5/ZZVVcXq6mp9zmAwqO+9BHgIXPIGHoyqquLFixdxfn4eOzs7ERExmUxiOBxGu92OFy9exNbW1q3zPl2F/OWXX+LZs2fR6/Vuve/z3dqdTidarVa0Wq0oyzIiol79bLfb0e1260vn84068/OKoqjvqwT40QlK4MEoyzL29vbi8ePHMRwO613VRVHE6elpXFxcxObm5r3n7+zsxObm5r2h1+v17gzNvb29O99/3/GIuLUpCOBH5pI38GDMH90TEXVMfv5cyPvuiSyKIqqqiqOjo/rYfNURgC+zQgk8KGVZ3lhFnEwmsb29HREfL21//hzJiI87usfjcZyfn9fHbJoB+OusUAIPytnZWf3syPmje+aXnh8/fnwrFOc7uieTyY3Vy+FweOfl7X/Lp5t0AH50ghJ4UOaXqcfjcRwdHcXLly/rn/V6vTg7O6tfz3d0b25u1l+dOBwO49GjR3/7W3T+jul0Wq+aAjwELnkDD0pVVfWK5OfPm5x/c81cq9WK9+/f3/qMT++j/BZOTk5iOBx+098B8D1ZoQQejLIsv/p92cPhsNFnQM7v4fwrz7gE+FEISuBBmM1mcXh4GFVVfXFDTa/Xi6urqzs353wPBwcHvnYReHBc8gYehHa7HZPJ5C+9d29vr7GgFJPAQ2SFElhK33LTDcCyEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACDl56YH+JrXr183PQIAQKMWvYcWNijX1tZiZWUlnjx50vQoAACNW1lZibW1tabHuNNP19fX100PcZ+3b9/G5eVl02MAS+y3336LiIjff/+90TkA1tbWYn19vekx7rSwK5QREevr6wv7Dwcsh1arFRERnU6n2UEAFphNOQAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgDuUVVVFEUR0+n0xvHRaBTj8ThGo1HMZrNb7y/L8tY58JD93PQAALCoXr16FVVV3Tg2Ho8jIqLf70dVVfH06dM4PT2NiIidnZ2YTCYR8TE6O53Od50XmmKFEoClUpZldLvdGI1G0e12oyiK2N7ejp2dnSjL8sZ7e71etFqtG8eurq7i3bt3ERHRarXqlciyLKPdbsdsNouqqmJ/f/+7/D2wCAQlAEul1+vFYDCI/f39GAwGcXh4GJPJJLa2tqKqqltR+bnd3d06GsfjcVxdXUVExGw2qy9/l2UZx8fH3/xvgUUhKAHgv/r9/lfvfWy1WnF6ehqz2Sw6nU602+36Z/PX/X4/iqL41uPCwhCUAPCJ+eXs+1RVVd8fWZZlPHv2LCI+rnzO77esqipWV1e/9aiwMGzKAWDplWUZZ2dnURRFHYjz45+uWHY6nWi1WtFqtepL4/1+PyIi2u12dLvdGI/HMZvN6o06sAwEJQBL6+zsLLrdbgwGg2i323F4eHjj571eL3q93q3z9vb27vy8+47DQ+eSNwBLa2trK54/f17v5P78EUHAXyMoAVhqnU4nqqqK4XAYBwcHTY8DPyRBCcDS29/fj6Ojo9je3q4fXA78de6hBGCplGUZJycnERFxcnJSP+Zna2srptNp/bP5Zhvg6366vr6+bnoIgEX166+/RkTEn3/+2fAkAIvLJW8AAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASPm56QG+5O3bt3F5edn0GMAS+89//hMREdPptOFJgGW3trYW6+vrTY9xp5+ur6+vmx7iLm/fvo2NjY348OFD06MAADRuZWUlXr9+vZBRubArlJeXl/Hhw4f4448/YmNjo+lxAAAa8/r163jy5ElcXl4Kyn9iY2MjOp1O02MAAHAPm3IAAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoASWzs7OTlRVdev4eDyOsizj+Pg4ptPpnedlVVUVRVHc+vzRaBTj8ThGo1HMZrNb7y/L8s6ZABbBz00PAPBPVFUVrVbrb583m82iLMvodrsREXF1dRVv3rypf7a/vx8REUVRRKfTqc/7POj+6e9/9erVrZgdj8cREdHv96Oqqnj69Gmcnp5GxMeInUwmEfExOj+dCWBRWKEEfjjHx8dxdXV149hoNIpHjx5Ft9utw286ncajR49iZ2enjraqquL9+/dxcXERp6en8fLly2i1WtFqteLo6KheKRwMBvVnzwOw3W7Xx8qyrFcS54E6Go2i2+1GURSxvb0dOzs7UZbljTl7vd6tEL26uop3795FRESr1arnL8sy2u12zGazqKqqjt3RaJT55wP41wlK4IcynU5jdXX1RtxFROzv78fu7m6srq7Wq3itVisODw/j9PQ0+v1+RMSNFb5Xr17deH14eBiTySSKorgRfWVZRq/Xu3emXq8Xg8Eg9vf3YzAY1J+ztbUVVVXdisrP7e7u1tE4Ho/rWJ7NZjei9fj4OCIi9vb2oiiKr/1TAXw3ghL4oRwcHNRx+LnhcBhlWUZVVXXI7e3t3fneoihid3e3fl2WZVxdXcVkMomjo6MYDocR8TFgvxSTX9Pv979672Or1YrT09OYzWbR6XRuxPL8db/fryNyHruf3msJ0CRBCfwwqqq6tTL5qU6nE51OJ4qiiOPj43tjMuJjQH66CjmbzWJzczMiPq44fn7/5Hg8jtlsVq8S/h3zy9n3qaqqvj+yLMt49uxZPcf8cntVVbG6ulqfMxgM6sv4AE2zKQdYGFVVxYsXL+L8/LzeUT2ZTGI4HEa73Y4XL17E1tbWFz9jOBzGcDiM6+vrv/W79/b2buywnt9DOY/Ur122/lxZlnF2dhZFUdSBOD/+6Yplp9Op7+Gc/475Cmy73Y5ut1vH7Hyjzvy8oijq+yoBmiQogYUxv0T9+PHjGA6H9SphURRxenoaFxcX9SrifeYrel+77/H8/PzWsS/FWa/Xi4uLi6/+DWdnZ9HtdmMwGES73Y7Dw8Nbn3PXXPetpn5plfXzjUkATXHJG1gY88fmRPxv88znz2T80qN65pe59/b24ujo6FuOeq+tra14/vx5Peddz7sEeGgEJbBQPl9ZnEwmsb29HREfN6PcF2jj8Tg2Nzej1WrFcDiM8XjcWMx1Op2oqiqGw2EcHBw0MgPA9yQogYUyv2Qc8b/H5swv+z5+/PjOnc3j8TharVa9qjnfGf1PNtD8W/b39+Po6Ci2t7e/2eaZTzfpADRJUAILZb4xZTwex9HRUbx8+bL+Wa/Xi7Ozs/r1dDqNnZ2dW1+JON/0cnBw8F2isizLODk5idFoFCcnJ3VAbm1txXQ6jYODg389KqfTab1yC9C0n67/7lbI72Q6nUa3243z83NfNQZL5PHjx1/c/LKzs3Njt3NTxuPxrWdGfk9FUdS734GHb9G7yAolsDDKsvzq/yjn90cus7u+ChKgSYISWAiz2SwODw+jqqovfgNMr9eLq6urpd49fXBwcOtxRABN8hxKYCG02+2YTCZ/6b17e3uNB2Wv1/viI4y+JTEJLBorlMAPqamYW5TfD7BIBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApPzc9ABf8/r166ZHAABo1KL30MIG5draWqysrMSTJ0+aHgUAoHErKyuxtrbW9Bh3+un6+vq66SHu8/bt27i8vGx6DGCJ/fbbbxER8fvvvzc6B8Da2lqsr683PcadFnaFMiJifX19Yf/hgOXQarUiIqLT6TQ7CMACsykHAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBIB77OzsRFVVt46Px+MoyzKOj49jOp3eeR4sk5+bHgAAFtFsNouyLKPb7UZExNXVVbx586b+2f7+fkREFEURnU6nPq8syzsjEx4yQQnAUinLMoqiiMFgECcnJ9Hr9WI6nUar1YrhcBi9Xi8iIqqqivfv30dE1IHYarUiIuLo6CgiItrtdgwGg/qz56uZ7Xb7O/01sBhc8gZgqfR6vRgMBrG/vx+DwSAODw9jMpnE1tZWVFUVZVlGRNxYdXz16tWN1/NziqKoIzPiY6zOgxSWiaAEgP/q9/u3LlcXRRG7u7v167Is4+rqKiaTSRwdHcVwOIyIj6uYYpJlJSgB4BPv3r278bosyxurkLPZLDY3NyPi42rn5/dPjsfjmM1mcXx8/F3mhUXgHkoAll5ZlnF2dhZFUcSzZ8+++N69vb0YjUYxm80iIup7KDudTnQ6nfqSOSwTQQnA0jo7O4tutxuDwSDa7XYcHh7ees/5+fmtY/Md3nfp9XpxcXHxr84Ji84lbwCW1tbWVjx//ry+pH3XMyeBrxOUACy1TqcTVVXFcDiMg4ODpseBH5KgBGDp7e/vx9HRUWxvb8d4PG56HPjhuIcSgKVSlmWcnJxERMTJyUm02+3o9/uxtbUV0+m0/lm/329yTPih/HR9fX3d9BAAi+rXX3+NiIg///yz4UkAFpdL3gAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQ8tP19fV100MAAPDjskIJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAEDK/wNJ1bsKicoqKQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApQAAAHzCAYAAACe1o1DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAVeElEQVR4nO3dMVMbZ7uA4cdnUlHJDDWFqGi+QoJfkKVNJVmN209qU7HjX8CgKi24TYNRlVbrX4BRS4UyQ43xVm45hY90TMB24sexZHRdTcyuJB4oMve8u+/y5Pb29jYAAOAr/c+iBwAA4McmKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQMpPix7gc66uruL6+nrRYwAALNzGxkZsbm4ueowHLW1QXl1dxfb2drx//37RowAALNza2lpcXFwsZVQubVBeX1/H+/fv4/fff4/t7e1FjwMAsDAXFxfx/PnzuL6+FpRfY3t7O1qt1qLHAADgE2zKAQAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQnwFeq6jrIsYzKZPHi8qqp75wAeq58WPQDAotV1HY1G4x+9582bN1HX9b3j3W43xuNxREQMh8NotVrfYEKA5WaFElhpx8fHcXNzc+dYt9uNra2tKMsyhsNhHB8fR7vdjna7PX9NURT3IrSqqmg2mzGdTqOu69jf34+ID2EJ8JhZoQRW1mQyifX19Wg2m/fOXV5ezv89Go1iOp3Gn3/++dnPm06nMZ1OI+JDXN7c3ES/349+vx9lWcbh4eG3/QEAloSgBFbWwcFBnJ6e3jlWVdWd8JtMJvHf//43zs/P/9Zl8VarFc1mM5rNZjx9+jT6/f78fdPp9MF4BfjRueQNrKS6rh+Mu52dnfnxuq7j559/jtPT078VgkVRzO+rrOs61tfX5+d6vV6MRqNvMzzAkrFCCTxKdV3Hq1ev4vz8PLrdbkREjMfjGAwG0Ww249WrV7G7u3vvfR+vQv7888/x4sWLKIri3uv+uot7tjLZbrfnl8g/Xv1stVpRluX8vkqAx0RQAo9SVVXR7/dja2srBoPBfLd1WZZxenoal5eXsbOz88n3d7vd2NnZ+WQAFkXxYGj2+/1PfuZfN/8APBYueQOPUqfTmV9+nsXkbMNMxOcfFVSWZdR1HUdHR/NjVVX9a7MC/OisUAKPVlVVd1YRx+Nx7O3tRcSHS9sPPUdyNBrFaDSK8/Pz+bGPQxSA+6xQAo/W2dnZ/NmRs0f6zC5Jb21t3QvF2Y7u8Xh8Z/VyMBg8eHn7n/p4kw7AYyIogUdrdpl6NBrF0dFRvH79en6uKIo4Ozubfz3b0b2zsxOj0SiGw2EMBoN4+vTpP/4rOg+ZTCbz1VGAx8Ylb+DRqut6viLZ6XTunJv9RZuZRqMR7969u/cZH99HmXFychKDweCbfBbAsrFCCTxKVVV98e9oDwaD7/JsyNm9mh5qDjxWghJ4dKbTaRweHkZd15/dUFMURdzc3Dy4OedbOjg48GcXgUfNJW/g0Wk2mzEej//Wa/v9/r8elGISeOysUAIr71tsugFYZYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACk/LToAb7k4uJi0SMAACzUsvfQ0gblxsZGrK2txfPnzxc9CgDAwq2trcXGxsaix3jQk9vb29tFD/EpV1dXcX19vegxgBX266+/RkTEb7/9ttA5ADY2NmJzc3PRYzxoaVcoIyI2NzeX9hcHrIZGoxEREa1Wa7GDACwxm3IAAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAACmCEgCAFEEJAECKoAQAIEVQAgCQIigBAEgRlAAApAhKAABSBCUAfEJd11GWZUwmkwePV1V17xysop8WPQAALKs3b95EXdf3jne73RiPxxERMRwOo9VqfefJYLlYoQRgpVRVFe12O4bDYbTb7SjLMvb29qLb7UZVVXdeWxRFNBqNe+9vNpsxnU6jruvY39//jtPDcrJCCcBKKYoier3ePARn/x0Oh1HXdVRVFUVRfPL90+k0ptNpRHyIy5ubm+j3+//+4LDErFACwP/pdDp/657IVqsVzWYzOp1OlGX5HSaD5SYoAeAjb9++/ez5oijm91XWdR3r6+vfYSpYbi55A7DyqqqKs7OzKMsyXrx4cef4xyuWs5XJdrsdo9EoptNpnJ6eLmJkWCqCEoCVdXZ2Fu12O3q9XjSbzTg8PLxzviiKB++ndM8k3OWSNwAra3d3N16+fDnfyf3QI4KALxOUAKy0VqsVdV3HYDCIg4ODRY8DPyRBCcDK29/fj6Ojo9jb24vRaLToceCH4x5KAFZKVVVxcnISEREnJyfzx//s7u7GZDKZn+t0OoscE34oT25vb28XPQTAsvrll18iIuKPP/5Y8CQAy8slbwAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBI+WnRA3zO1dVVXF9fL3oMYIX95z//iYiIyWSy4EmAVbexsRGbm5uLHuNBT25vb28XPcRDrq6uYnt7O96/f7/oUQAAFm5tbS0uLi6WMiqXdoXy+vo63r9/H7///ntsb28vehwAgIW5uLiI58+fx/X1taD8Gtvb29FqtRY9BgAAn2BTDgAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUwMrqdrtR1/W946PRKKqqiuPj45hMJg++L6uu6yjL8t7nz45XVfXg9wZYRoIS+KE9FIR/x3Q6jaqqot1ux9bWVjx9+jTquo66rmM6nUZRFNHv9+Pk5OTO+/4ael/7/d+8efPge7vdbhweHkZRFFFV1Vd9NsD3JiiBH9bx8XHc3NzcOTYcDuPp06fRbrfn4TeZTOLp06fR7XZjNBpFxIcQfPfuXVxeXsbp6Wm8fv06Go1GNBqNODo6iuFwGKPRKHq93vyzZwHYbDbnx6qqiul0Ov93u92O4XAY7XY7yrKMvb296Ha79+KwKIpoNBp3jlVVFc1mM6bTadR1Hfv7+/OfCWCZCUrghzSZTGJ9ff1O3EVE7O/vx7Nnz2J9fT1arVZERDQajTg8PIzT09PodDoREfNzER9WCz/++vDwMMbjcZRleSf6qqqKoig+OVNRFNHr9WJ/fz96vd78c3Z3d6Ou6y+uOE6n0ztxenx8HBER/X4/yrL8G78VgMUQlMAP6eDgYB6HfzUYDKKqqvkl7Kqqot/vP/jasizj2bNn86+rqoqbm5sYj8dxdHQUg8EgIj4E7Odi8ks6nc7fuiey1WpFs9mMTqczj8hZ1M5iE2DZCErgh1PX9b2VyY+1Wq1otVpRlmUcHx9/MiYjPgTkx6uQ0+k0dnZ2IuLDiuPHK5dVVcVoNIrpdDpfPfwn3r59+9nzRVHML6vXdR3r6+vzc71eb365HmDZ/LToAQD+qq7rePXqVZyfn893VI/H4xgMBtFsNuPVq1exu7v72c8YDAYxGAzi9vb2H33vfr8fw+Fwvho4u4dyFqn/dKNMVVVxdnYWZVnGixcv7hz/eMVytjLZbrfn0Xp6enrnfFmW8/sqAZaJoASWzuwS9dbWVgwGg/kqYVmWcXp6GpeXl/NVxE+ZrfR96b7H8/Pze8c+F21FUcTl5eUXf4azs7Not9vR6/Wi2WzG4eHhvc95aK7Prab+dQMSwLJwyRtYOp1OZx6Es5j8+P7Buq7v7ZD+2Owyd7/fj6Ojo39z1E/a3d2Nly9fzuf82scLAfwIBCWwlP66sjgej2Nvby8iPmxS+VSgjUaj2NnZiUajEYPBIEaj0cJirtVqRV3XMRgM4uDgYCEzAHwPghJYSrNLxhH//zid2eXgra2tB3c8j0ajaDQa81XN2X2JX7OB5lvZ39+Po6Oj2NvbS2+q+XiTDsAyEZTAUpptfhmNRnF0dBSvX7+enyuKIs7OzuZfTyaT6Ha79/4k4mzTy8HBwXeJyqqq4uTkJIbDYZycnMwDcnd3NyaTSRwcHHx1VE4mk/kKLcCyeXL7T7dAfieTySTa7Xacn5/feWwHsBq2trY+u/ml2+3e2QW9KKPRaL4S+m8qy3K+yx1YPcveRVYogaVTVdUX/4c5uz9yFTz0Jx8BlomgBJbKdDqNw8PDqOv6s38ZpiiKuLm5WYnd0wcHB/ceOwSwTDyHElgqzWYzxuPx33ptv99feFAWRfHZRxh9C2ISWHZWKIEf2r8dc8v+/QGWgaAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgBRBCQBAiqAEACBFUAIAkCIoAQBIEZQAAKQISgAAUgQlAAApghIAgJSfFj3Al1xcXCx6BACAhVr2HlraoNzY2Ii1tbV4/vz5okcBAFi4tbW12NjYWPQYD3pye3t7u+ghPuXq6iqur68XPQawwn799deIiPjtt98WOgfAxsZGbG5uLnqMBy3tCmVExObm5tL+4oDV0Gg0IiKi1WotdhCAJWZTDgAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAn9DtdqOu63vHR6NRVFUVx8fHMZlMHnwfrJKfFj0AACyj6XQaVVVFu92OiIibm5v4888/5+f29/cjIqIsy2i1WvP3VVX1YGTCYyYoAVgpVVVFWZbR6/Xi5OQkiqKIyWQSjUYjBoNBFEURERF1Xce7d+8iIuaB2Gg0IiLi6OgoIiKazWb0er35Z89WM5vN5nf6aWA5uOQNwEopiiJ6vV7s7+9Hr9eLw8PDGI/Hsbu7G3VdR1VVERF3Vh3fvHlz5+vZe8qynEdmxIdYnQUprBJBCQD/p9Pp3LtcXZZlPHv2bP51VVVxc3MT4/E4jo6OYjAYRMSHVUwxyaoSlADwkbdv3975uqqqO6uQ0+k0dnZ2IuLDaudf758cjUYxnU7j+Pj4u8wLy8A9lACsvKqq4uzsLMqyjBcvXnz2tf1+P4bDYUyn04iI+T2UrVYrWq3W/JI5rBJBCcDKOjs7i3a7Hb1eL5rNZhweHt57zfn5+b1jsx3eDymKIi4vL7/pnLDsXPIGYGXt7u7Gy5cv55e0H3rmJPBlghKAldZqtaKu6xgMBnFwcLDoceCHJCgBWHn7+/txdHQUe3t7MRqNFj0O/HDcQwnASqmqKk5OTiIi4uTkJJrNZnQ6ndjd3Y3JZDI/1+l0Fjkm/FCe3N7e3i56CIBl9csvv0RExB9//LHgSQCWl0veAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJDy5Pb29nbRQwAA8OOyQgkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQIqgBAAgRVACAJAiKAEASBGUAACkCEoAAFIEJQAAKYISAIAUQQkAQMr/AtpiGKxya/4SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAGvCAYAAACAbQgEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa1klEQVR4nO3dYWwj6X3f8Z/2do93l6w04dlxvLm9xrO+i9MG7pWigLq+NgY0atGgRYCAwr1qgCBYCQnQdwUJAS2MfVGo1LuiKVCuiqK4FwVWJJwaQY0WZNMAje22Wo5dty6cpJxcInvtXG7FkdbrM73anb5QOV5KlFYPxf2TIr8fgAA54p/zcPSAPz0zDx/NJEmSCAAAA5dG3QAAwPQgdAAAZggdAIAZQgcAYIbQAQCYIXQAAGYIHQCAGUIHAGDm8qgbgOmysbGR3r9//75WV1dVq9VULBZH2CopDEOVSiVFUaRWq3Wu12o0GqpWq5KkpaUlFQqFYTQRmAiEDsysrq5qdXVVuVwu3ba8vDzCFv1YLpdTqVTS6urquV9raWlJ7XZbd+/eHULLRq8bxt0gBc6D02sws7W11RM4krS5uTmi1hyXzWbP/RphGMr3fXmepyAIFATBEFo2WktLS3rnnXdG3QxMCEY6MBPHsaIoku/76TbP87SwsDDCVg2f53mjbsJQTUJwYnwQOjCTy+W0tLSkSqXS80H29PWcOI51+/Zt+b6ver3eczqu0WioVCpJOhwhRVGk3d1dNZtNVSoV3b59W9lsVnfu3NHa2tqxumw2m57Oi+NY9+/fV7lcfma7NzY25Pt+GpgnXaMJw1CVSkVRFKU1nuepVCrJ932trq6qXq9LUrrf7vMkKYqi9FgM+l77Ocv77z7naDvfeeedE691PavtJ71nTLkEMNJqtRLf9xNJiaQkCIKkXq/3PKdYLCatVit97Pt+0m6308f1ej3xfb+nzvf9pFgspo+r1WqSy+V6XrdarSaSel67WCwmKysr6eNms5n4vt9TVygUkmq1mj4OgiBpNpsnvsd+r9FtT71eT5rNZtrWQqHQ8z5arVYSBMG532s/Z3n/J7XzpONyWttPei2A0IG5er2eFIvFJJfLJZJ6PtQLhUJSqVTSx0EQ9Py82WwmR/9WCoKg50Ot1Wolnucd2+fRD+d2u93zQXz0w7XVah3bV6VS6fmgPqrfB3S9Xj/2Os1m81gbkyRJP6jP8177Ocv779fOfu/pLG0/6bUATq/B3NMX2Eulkm7evJmesurOkOpe/9nd3dXu7m5P/dPXhKTDayg3btxwbofnefI8L734f1Sj0ZDneWo0Gum2VqulKIqc93X09e/evdt3n93Tit3jM6z32k+/99+vTUcN2nZAYvYajMRxrFqtdmx7uVxWHMeK41jS4XWR5eVlbW1tyff9kX5wxXEs3/fTkAyCQOVyOb1G4eLo5ILu+x03Z5kEcda2T9qECgwHoQMz29vbfbd3L7jHcazFxUWtra1pZWUl3SZpoNHFs3TD7qSL8Llcru9+hxEYQRD0fe0oisxm8z3r/Z9kHNqOi4vQgZnbt2/3nKqSDk9hdU+tRVF07EOwe2otDMMTX/esIRCGYc9z19fXtbKycuJoKggC5fP5YyO0ra2tM+3vNLlcTkEQ9ByP7ns8bQWD8wSe6/s/yaBtByRxpQ822u12UqlUknq9npTL5Z7b04rFYlIsFpN6vZ7U6/Wk1WqlM8iazWZSKBQSSWlduVxOPM9LZ8I9/ZxisZjOfOteSK9Wqz1t6Dpad7RNlUolqVarPZMajjr6Gs1mM6nX60kQBInneUm5XO6ZPfb0a1cqlZ79nue99vOs939SO/u141ltf9Z7xnSbSZIkGWHmASa63x1pNpujbspITPv7x/jg9BoAwAyhAwAwQ+hg4jUaDZXLZYVh2POvFabFtL9/jBeu6QAAzDDSAQCYIXQAAGbGYu21J0+e6N69e7p69apmZmZG3RwAgIMkSfTgwQNdu3ZNly6dPpYZi9C5d++erl+/PupmAADOYWdnR6+99tqpzxmL0Ll69aok6W39si7ryohbAwBwcaBH+gN9Kf0sP83QQieKItVqtfQ/LHYXbDyL7im1y7qiyzOEDgBcKP9/DvRZLo8MLXSWl5fTJTaiKNLNmzfT/40CAIA0pNlrR5c5933/2GrCAAAMJXQajYay2WzPtmw2e+py9ACA6TOU02sn/Y+Po/9muKvT6ajT6aSP9/f3h9EMAMCYe65fDj0pjNbX1zU3N5femC4NANNhKKHjed6xUc3u7u6Js9fW1ta0t7eX3nZ2dobRDADAmBtK6ARB0Hd7Pp/vuz2TyWh2drbnBgCYfEMJnaP/Yz2KIuXz+TN/TwcAMB2G9j2darWqUqmkhYUFbW9v8x0dAMAxY/H/dPb39zU3N6fP6VdYkQAALpiD5JF+X1/U3t7eMy+X8K8NAABmCB0AgBlCBwBghtABAJghdAAAZggdAIAZQgcAYIbQAQCYIXQAAGYIHQCAGUIHAGCG0AEAmCF0AABmCB0AgBlCBwBghtABAJghdAAAZggdAIAZQgcAYIbQAQCYIXQAAGYIHQCAGUIHAGCG0AEAmCF0AABmCB0AgBlCBwBghtABAJghdAAAZggdAIAZQgcAYIbQAQCYIXQAAGYIHQCAGUIHAGCG0AEAmCF0AABmCB0AgBlCBwBghtABAJghdAAAZggdAIAZQgcAYIbQAQCYIXQAAGYIHQCAGUIHAGCG0AEAmCF0AABmCB0AgBlCBwBghtABAJghdAAAZggdAIAZQgcAYGZooROGocIwlCRFUZTeBwCga2ihU6lUND8/r5mZGa2ursr3/WG9NABgQlwe1gvNz8+r3W5LkjzPG9bLAgAmyNBCRyJsAACnG1roxHGsWq0mSdre3j71FFun01Gn00kf7+/vD6sZAIAxNrTQWVlZSUc6vu9raWlJrVar73PX19d169atYe0aAHBBDG0iQRRF6X3f9xVFUc+2p62trWlvby+97ezsDKsZAIAxNpSRThiGWlxcTCcSdGWz2b7Pz2QyymQyw9g1AOACGcpIx/d9lcvl9HGj0VChUGBiAQCgx1BGOp7nKZ/Pa2NjQ57nqdVqqVqtDuOlAQATZGgTCXK5nHK53LBeDgAwgYb6PR1glN77p58ZqM77lnvNlR88ca65+sd7zjVPvjFA4wb0wkdeda7Z/6VPOtc8zsw41wxyvF/59g+cayTp0oePnGsef/MPB9rXNGLBTwCAGUIHAGCG0AEAmCF0AABmCB0AgBlCBwBghtABAJghdAAAZggdAIAZQgcAYIbQAQCYIXQAAGZY8BPP3Uz+F51rdtYS55rF17/mXCNJ/+mVt5xr3nj3+841lot3DuLxB/edazpzbzrXfPC2+4KaM99/wbnm9f/4snONJL38nccD1eFsGOkAAMwQOgAAM4QOAMAMoQMAMEPoAADMEDoAADOEDgDADKEDADBD6AAAzBA6AAAzhA4AwAyhAwAwQ+gAAMywyjScdP7ugnPNvV/7kXPNb//Vf+dc82///G3nGklKLruvaJ187ZsD7WvSxD/vXvPZT/1f55qPZfada770wV93rpEk/xvu+3oy0J6mEyMdAIAZQgcAYIbQAQCYIXQAAGYIHQCAGUIHAGCG0AEAmCF0AABmCB0AgBlCBwBghtABAJghdAAAZljwE06+9+s/dK757OvvOdf8q3ufc675zr/+pHONJL3x7lcHqoN08NPui7n+vVf/p3PN4wH+Pv69ARf8PPj2dwaqw9kw0gEAmCF0AABmCB0AgBlCBwBghtABAJghdAAAZggdAIAZQgcAYIbQAQCYIXQAAGYIHQCAGUIHAGCGBT+n1JO33xqo7pWXPnSuufvd6+47+rLnXHLt3a+47wfnsvDGe841P3flA+ea3/7eonPNtd+JnGsk6WCgKpwVIx0AgBmn0AnDUPPz88e2R1GkjY0N1Wo1bWxsKI7jYbUPADBBznx6rVaryfd9hWF47GfLy8tqNpuSDgPo5s2bqlarw2slAGAinDl0CoVC3+1R1Hve1Pd9NRqN87UKADCRzn1Np9FoKJvN9mzLZrN9R0QAgOl27tlrJ12/2d3dPbGm0+mo0+mkj/f398/bDADABfDcZq+dNplgfX1dc3Nz6e369QGm1AIALpxzh47necdGNbu7u/I878SatbU17e3tpbednZ3zNgMAcAGcO3SCIOi7PZ/Pn1iTyWQ0OzvbcwMATL6BQufpU2e+7/f8LIoi5fP5U0c6AIDpdOaJBI1GQ/V6XdLhNZmFhYV0GnW1WlWpVNLCwoK2t7f5jg4AoK8zh04QBAqCQOVy+djPfN9Pt5/0fR4AAFjwc0rNPEkGqjt44n5G9uHey841n7r9v51rHjtX4Gl/8Zufca55I+P+fby/dNl90dgv/59POte8+d27zjV4/ljwEwBghtABAJghdAAAZggdAIAZQgcAYIbQAQCYIXQAAGYIHQCAGUIHAGCG0AEAmCF0AABmCB0AgBlCBwBghlWmp9SVe+2B6r7/nY871/zMf51xrnm8v+9cg0OdX14YqO5Xf/O/ONf84498y7nm5s7fca65/rv8fTwp+E0CAMwQOgAAM4QOAMAMoQMAMEPoAADMEDoAADOEDgDADKEDADBD6AAAzBA6AAAzhA4AwAyhAwAww4KfU+rRz2YHqkuuPHGueXD9inPNVecKdD38rb2B6n7Du+tc8x9+8BHnmm/8y08713hf/KpzDcYTIx0AgBlCBwBghtABAJghdAAAZggdAIAZQgcAYIbQAQCYIXQAAGYIHQCAGUIHAGCG0AEAmCF0AABmWPBzSl1+f3+guhff/5hzzcEr7vv583/4N5xrPvYvvuK+ozH3Z593Pw7/6MYXBtrX7z5807nmn/3nv+9c88a7LN45zRjpAADMEDoAADOEDgDADKEDADBD6AAAzBA6AAAzhA4AwAyhAwAwQ+gAAMwQOgAAM4QOAMAMoQMAMMOCn9NqZmagshdj97qHn/6h+37+2ofONR/8bfcFKyXpxcuPB6pzdWPuvnPNp19yXxzzN+a+51wjSf/+4U8613zqn/yRc43N0ca4YqQDADDjFDphGGp+fr7v9jAMJUlRFKX3AQB42plDp1arSVLfQKlUKpqfn9fMzIxWV1fl+/7wWggAmBhnvqZTKBRO/Nn8/Lza7bYkyfO8czcKADCZhjaRgLABADzLUEInjuP09Nv29vYzT7F1Oh11Op308f7+YP86GQBwsQwldFZWVtKRju/7WlpaUqvVOvH56+vrunXr1jB2DQC4QIYyZTqKovS+7/uKoqhn21Fra2va29tLbzs7O8NoBgBgzJ17pBOGoRYXF9OJBF3ZbPbEmkwmo0wmc95dAwAumIFGOnEcp/d931e5XE4fNxoNFQoFJhYAAI4580in0WioXq9LOrwms7CwkIZLPp/XxsaGPM9Tq9VStVp9bg0GAFxcZw6dIAgUBEHPqKYrl8spl8sNtWEAgMnDgp9T6vEfnTy78DQ/mvtp55rLV9yXePwrH3VftPLlFx4510jS3/TcF638+sPXnWv+QfYrzjXxk5edawb1z99bdK55sf2nz6ElmGQs+AkAMEPoAADMEDoAADOEDgDADKEDADBD6AAAzBA6AAAzhA4AwAyhAwAwQ+gAAMwQOgAAM4QOAMAMoQMAMMMq03Dyo1fdV4y+MpM41/zwsXvXPEgG+xvqlUsd55q3fuLPnGvmMy8610jux3tQSTJjti9ML0Y6AAAzhA4AwAyhAwAwQ+gAAMwQOgAAM4QOAMAMoQMAMEPoAADMEDoAADOEDgDADKEDADBD6AAAzLDgJ5y8+Vv/w7km/rXPONf8r1/8pHPNk8vuC4tK0tev/axzzaVL7vv69qe2nWv+1k9+y7nmL1956FwjSXsfvuRcc+3nXneuOXjPfbFUTA5GOgAAM4QOAMAMoQMAMEPoAADMEDoAADOEDgDADKEDADBD6AAAzBA6AAAzhA4AwAyhAwAwQ+gAAMyw4CeeO+/dr7rXDL8ZJ0o++5ZzzZMr7n+v/Ztf/SXnmm/lf8a55m3vj51rJOnhhy861xy8574gKaYbIx0AgBlCBwBghtABAJghdAAAZggdAIAZQgcAYIbQAQCYIXQAAGYIHQCAGUIHAGCG0AEAmCF0AABmWPATU2/my193rnlhgP28/hMLzjX/7dVPONd8/Bf2nGsk6eD9l92LLg1wJJ48dq/BxGCkAwAw4zTSCcNQjUZDkrS9va3NzU15nidJiqJItVpNvu8riiKtrKykPwMAQHIMnUajoWKxKEna2NjQ4uKims2mJGl5eTm9H0WRbt68qWq1OuTmAgAusjOfXgvDUOvr6+njQqGgMAwVRZGiKOp5ru/76YgIAICuM4dOLpfT5uZm+jiOY0lSNptVo9FQNpvteX42m1UYhsNpJQBgIjidXisUCun9O3fuKAgCeZ6XBtBRu7u7fbd3Oh11Op308f7+vkszAAAX1ECz1+I4Vq1We+Y1m5PCaH19XXNzc+nt+vXrgzQDAHDBDBQ6pVJJ9Xo9nZ3med6xUc3u7u6Js9fW1ta0t7eX3nZ2dgZpBgDggnEOnY2NDZVKJfm+rziOFcexgiDo+9x8Pt93eyaT0ezsbM8NADD5nEKnVqspl8ulgbO1tSXP8+T7fs/zoihSPp/nezoAgB5nnkgQRZGWl5d7tnmep5WVFUlStVpVqVTSwsKCtre3+Y4OAOCYM4eO7/tKkuTUn5fLZUm9s9wAAOhiwU/AyA9/yn1xzLde/xPnmhsvve9cI0kff/MvnGsOPveWc83l32s612BysOAnAMAMoQMAMEPoAADMEDoAADOEDgDADKEDADBD6AAAzBA6AAAzhA4AwAyhAwAwQ+gAAMwQOgAAM4QOAMAMq0wDRi49cq/57kP3/6q7573iviNJH335oXPNtz/xMeeaV50rMEkY6QAAzBA6AAAzhA4AwAyhAwAwQ+gAAMwQOgAAM4QOAMAMoQMAMEPoAADMEDoAADOEDgDADKEDADDDgp+AkYOXZpxr7n8w51zztavXnWsk6aMvfd+5ZueFgXaFKcZIBwBghtABAJghdAAAZggdAIAZQgcAYIbQAQCYIXQAAGYIHQCAGUIHAGCG0AEAmCF0AABmCB0AgBkW/ASMfOR3vulck3nwC841Yf7nnWsk6eX33Rckfe1Lf+pcc+BcgUnCSAcAYIbQAQCYIXQAAGYIHQCAGUIHAGCG0AEAmCF0AABmCB0AgBlCBwBghtABAJghdAAAZggdAIAZFvwEjDze33eueeUL/9255hNfcC4ZGIt3whUjHQCAGaeRThiGajQakqTt7W1tbm7K87z0Z5KUy+UURZHiOFYulxtuawEAF5rTSKfRaKhYLKpYLGphYUGLi4vpzyqViubn5zUzM6PV1VX5vj/0xgIALrYzh04YhlpfX08fFwoFhWGoKIokSfPz82q322q326rX6+kICACArjOfXsvlctrc3Ewfx3EsScpms+k2ggYAcBqnazqFQiG9f+fOHQVBkAZNHMeq1WqSDq/3nHaKrdPpqNPppI/3B5jVAwC4eAaaMt0NmGazmW5bWVlJA8j3fS0tLanVavWtX19f161btwbZNQDgAhtoynSpVDp23aZ7bUc6DJ0oinq2PW1tbU17e3vpbWdnZ5BmAAAuGOeRzsbGhkqlknzfT6/rRFGkxcVFtdvtnuc+fb3naZlMRplMxr21AIALzWmkU6vVlMvl0sDZ2tqS53nyfV/lcjl9XqPRUKFQYGIBAKDHTJIkyVmeGEWRbty40bPN87x0dNP94qjneWq1Wj0h9Cz7+/uam5vT5/QrujxzxaH5AIBRO0ge6ff1Re3t7Wl2dvbU55759Jrv+zotn3K5HCsQAABOxdprAAAzhA4AwAyhAwAwQ+gAAMwQOgAAM4QOAMAMoQMAMEPoAADMEDoAADOEDgDADKEDADBD6AAAzBA6AAAzhA4AwAyhAwAwQ+gAAMwQOgAAM4QOAMAMoQMAMEPoAADMEDoAADOEDgDADKEDADBD6AAAzBA6AAAzhA4AwMzlUTdAkpIkkSQd6JGUjLgxAAAnB3ok6cef5acZi9B58OCBJOkP9KURtwQAMKgHDx5obm7u1OfMJGeJpufsyZMnunfvnq5evaqZmZl0+/7+vq5fv66dnR3Nzs6OsIWjxXE4xHE4xHE4xHE4NA7HIUkSPXjwQNeuXdOlS6dftRmLkc6lS5f02muvnfjz2dnZqe5UXRyHQxyHQxyHQxyHQ6M+Ds8a4XQxkQAAYIbQAQCYGevQyWQy+vznP69MJjPqpowUx+EQx+EQx+EQx+HQRTsOYzGRAAAwHcZ6pAMAmCyEDgDADKEDADAzFt/T6SeKItVqNfm+ryiKtLKyIs/zRt0sc2EYSpJyuZyiKFIcx8rlciNu1fMXhqFu3rypZrPZs33a+sVJx2Ha+kUYhmo0GpKk7e1tbW5upr/3aeoTpx2HC9MnkjGVy+XS+61WKykUCiNszeisrKwkOlyRLgmCIGm326Nu0nNXrVaTZrOZ9Oue09QvTjsO09YvyuVyz/2n+8E09YnTjsNF6RNjGTqtVqvnYCZJknieN6LWjFalUkna7fbYdqDn6eiH7bT2i36hM039otls9vyeW61WIilptVpT1SdOOw5JcnH6xFhe02k0Gspmsz3bstlsOnycNp7nTezpAhf0i17T0i9yuZw2NzfTx3EcSzr83U9TnzjtOHRdhD4xltd0ugfzqN3dXduGjIE4jlWr1SQdnsNdXV2V7/sjbtVo0C9+bNr6RaFQSO/fuXNHQRDI87yp6xMnHQfp4vSJsQydk5zUwSbZ0xdFfd/X0tKSWq3WaBs1ZugX09Mvuh+sRydW9HveJOt3HC5KnxjL02ue5x37S2V3d3fsh43PQxRF6f3u7Jynt00T+sWPTWu/KJVKqtfr6e98WvvE0eMgXZw+MZahEwRB3+35fN64JaMVhqEWFxePbT96Dnta0C8OTWu/2NjYUKlUku/7iuNYcRxPZZ/odxwuUp8Yy9A5eh4yiiLl8/mJ/+vlKN/3VS6X08eNRkOFQmGqjsPTp0mmuV8cPQ7T1i9qtZpyuVz6Qbu1tSXP86auT5x2HC5KnxjbBT+jKFKlUtHCwoK2t7e1trY2lgfweet+GczzPLVarZ6ONakajYbq9bo2NjZULBa1sLCQXkCdpn5x2nGYpn4RRZFu3LjRs83zPLXb7fTn09AnnnUcLkqfGNvQAQBMnrE8vQYAmEyEDgDADKEDADBD6AAAzBA6AAAzhA4AwAyhAwAwQ+gAAMwQOgAAM4QOAMAMoQMAMPP/AJ76GBHRx1jVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    path.draw()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:categorical_bpl] *",
   "language": "python",
   "name": "conda-env-categorical_bpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
