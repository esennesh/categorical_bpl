{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fd13021c4f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)\n",
    "model.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/54000 (0%)] Loss: 3145.828125\n",
      "Train Epoch: 1 [1408/54000 (3%)] Loss: 4409.257812\n",
      "Train Epoch: 1 [2816/54000 (5%)] Loss: 6334.980469\n",
      "Train Epoch: 1 [4224/54000 (8%)] Loss: -4770.560547\n",
      "Train Epoch: 1 [5632/54000 (10%)] Loss: 368.576172\n",
      "Train Epoch: 1 [7040/54000 (13%)] Loss: -5267.797852\n",
      "Train Epoch: 1 [8448/54000 (16%)] Loss: -3761.570312\n",
      "Train Epoch: 1 [9856/54000 (18%)] Loss: -17588.156250\n",
      "Train Epoch: 1 [11264/54000 (21%)] Loss: -43442.789062\n",
      "Train Epoch: 1 [12672/54000 (23%)] Loss: -47145.226562\n",
      "Train Epoch: 1 [14080/54000 (26%)] Loss: -41664.878906\n",
      "Train Epoch: 1 [15488/54000 (29%)] Loss: -47832.398438\n",
      "Train Epoch: 1 [16896/54000 (31%)] Loss: -67810.515625\n",
      "Train Epoch: 1 [18304/54000 (34%)] Loss: -65398.742188\n",
      "Train Epoch: 1 [19712/54000 (37%)] Loss: -72938.984375\n",
      "Train Epoch: 1 [21120/54000 (39%)] Loss: -73249.867188\n",
      "Train Epoch: 1 [22528/54000 (42%)] Loss: -79522.125000\n",
      "Train Epoch: 1 [23936/54000 (44%)] Loss: -46688.210938\n",
      "Train Epoch: 1 [25344/54000 (47%)] Loss: -83693.578125\n",
      "Train Epoch: 1 [26752/54000 (50%)] Loss: -52047.562500\n",
      "Train Epoch: 1 [28160/54000 (52%)] Loss: -86765.601562\n",
      "Train Epoch: 1 [29568/54000 (55%)] Loss: -43027.523438\n",
      "Train Epoch: 1 [30976/54000 (57%)] Loss: -45853.417969\n",
      "Train Epoch: 1 [32384/54000 (60%)] Loss: -73636.195312\n",
      "Train Epoch: 1 [33792/54000 (63%)] Loss: -38991.933594\n",
      "Train Epoch: 1 [35200/54000 (65%)] Loss: -92788.656250\n",
      "Train Epoch: 1 [36608/54000 (68%)] Loss: -64000.066406\n",
      "Train Epoch: 1 [38016/54000 (70%)] Loss: -66884.187500\n",
      "Train Epoch: 1 [39424/54000 (73%)] Loss: -111850.585938\n",
      "Train Epoch: 1 [40832/54000 (76%)] Loss: -76120.421875\n",
      "Train Epoch: 1 [42240/54000 (78%)] Loss: -119176.718750\n",
      "Train Epoch: 1 [43648/54000 (81%)] Loss: -133401.968750\n",
      "Train Epoch: 1 [45056/54000 (83%)] Loss: -78950.781250\n",
      "Train Epoch: 1 [46464/54000 (86%)] Loss: -114116.164062\n",
      "Train Epoch: 1 [47872/54000 (89%)] Loss: -113012.437500\n",
      "Train Epoch: 1 [49280/54000 (91%)] Loss: -75792.148438\n",
      "Train Epoch: 1 [50688/54000 (94%)] Loss: -85894.414062\n",
      "Train Epoch: 1 [52096/54000 (96%)] Loss: -136275.921875\n",
      "    epoch          : 1\n",
      "    loss           : -56257.99693013606\n",
      "    val_loss       : -104617.75671684451\n",
      "Train Epoch: 2 [0/54000 (0%)] Loss: -85640.664062\n",
      "Train Epoch: 2 [1408/54000 (3%)] Loss: -83185.421875\n",
      "Train Epoch: 2 [2816/54000 (5%)] Loss: -77622.343750\n",
      "Train Epoch: 2 [4224/54000 (8%)] Loss: -120377.265625\n",
      "Train Epoch: 2 [5632/54000 (10%)] Loss: -90076.351562\n",
      "Train Epoch: 2 [7040/54000 (13%)] Loss: -92609.937500\n",
      "Train Epoch: 2 [8448/54000 (16%)] Loss: -98192.750000\n",
      "Train Epoch: 2 [9856/54000 (18%)] Loss: -127715.390625\n",
      "Train Epoch: 2 [11264/54000 (21%)] Loss: -89276.078125\n",
      "Train Epoch: 2 [12672/54000 (23%)] Loss: -144575.703125\n",
      "Train Epoch: 2 [14080/54000 (26%)] Loss: -137799.734375\n",
      "Train Epoch: 2 [15488/54000 (29%)] Loss: -109687.492188\n",
      "Train Epoch: 2 [16896/54000 (31%)] Loss: -136684.984375\n",
      "Train Epoch: 2 [18304/54000 (34%)] Loss: -97842.343750\n",
      "Train Epoch: 2 [19712/54000 (37%)] Loss: -98400.062500\n",
      "Train Epoch: 2 [21120/54000 (39%)] Loss: -85976.898438\n",
      "Train Epoch: 2 [22528/54000 (42%)] Loss: -130629.390625\n",
      "Train Epoch: 2 [23936/54000 (44%)] Loss: -96921.304688\n",
      "Train Epoch: 2 [25344/54000 (47%)] Loss: -146585.218750\n",
      "Train Epoch: 2 [26752/54000 (50%)] Loss: -139203.265625\n",
      "Train Epoch: 2 [28160/54000 (52%)] Loss: -88438.156250\n",
      "Train Epoch: 2 [29568/54000 (55%)] Loss: -104985.703125\n",
      "Train Epoch: 2 [30976/54000 (57%)] Loss: -150045.906250\n",
      "Train Epoch: 2 [32384/54000 (60%)] Loss: -152847.656250\n",
      "Train Epoch: 2 [33792/54000 (63%)] Loss: -155976.984375\n",
      "Train Epoch: 2 [35200/54000 (65%)] Loss: -147013.578125\n",
      "Train Epoch: 2 [36608/54000 (68%)] Loss: -145723.062500\n",
      "Train Epoch: 2 [38016/54000 (70%)] Loss: -160392.562500\n",
      "Train Epoch: 2 [39424/54000 (73%)] Loss: -149072.859375\n",
      "Train Epoch: 2 [40832/54000 (76%)] Loss: -168298.015625\n",
      "Train Epoch: 2 [42240/54000 (78%)] Loss: -137864.593750\n",
      "Train Epoch: 2 [43648/54000 (81%)] Loss: -183726.406250\n",
      "Train Epoch: 2 [45056/54000 (83%)] Loss: -173591.281250\n",
      "Train Epoch: 2 [46464/54000 (86%)] Loss: -136290.500000\n",
      "Train Epoch: 2 [47872/54000 (89%)] Loss: -161394.781250\n",
      "Train Epoch: 2 [49280/54000 (91%)] Loss: -168004.312500\n",
      "Train Epoch: 2 [50688/54000 (94%)] Loss: -155229.015625\n",
      "Train Epoch: 2 [52096/54000 (96%)] Loss: -155102.843750\n",
      "    epoch          : 2\n",
      "    loss           : -133318.70125598085\n",
      "    val_loss       : -167036.6661585366\n",
      "Train Epoch: 3 [0/54000 (0%)] Loss: -166716.656250\n",
      "Train Epoch: 3 [1408/54000 (3%)] Loss: -166917.703125\n",
      "Train Epoch: 3 [2816/54000 (5%)] Loss: -99617.093750\n",
      "Train Epoch: 3 [4224/54000 (8%)] Loss: -174609.250000\n",
      "Train Epoch: 3 [5632/54000 (10%)] Loss: -146446.312500\n",
      "Train Epoch: 3 [7040/54000 (13%)] Loss: -167724.796875\n",
      "Train Epoch: 3 [8448/54000 (16%)] Loss: -194844.640625\n",
      "Train Epoch: 3 [9856/54000 (18%)] Loss: -145463.500000\n",
      "Train Epoch: 3 [11264/54000 (21%)] Loss: -173498.781250\n",
      "Train Epoch: 3 [12672/54000 (23%)] Loss: -179147.281250\n",
      "Train Epoch: 3 [14080/54000 (26%)] Loss: -173507.625000\n",
      "Train Epoch: 3 [15488/54000 (29%)] Loss: -185126.625000\n",
      "Train Epoch: 3 [16896/54000 (31%)] Loss: -173958.468750\n",
      "Train Epoch: 3 [18304/54000 (34%)] Loss: -158825.875000\n",
      "Train Epoch: 3 [19712/54000 (37%)] Loss: -176582.890625\n",
      "Train Epoch: 3 [21120/54000 (39%)] Loss: -176138.375000\n",
      "Train Epoch: 3 [22528/54000 (42%)] Loss: -159538.468750\n",
      "Train Epoch: 3 [23936/54000 (44%)] Loss: -179203.437500\n",
      "Train Epoch: 3 [25344/54000 (47%)] Loss: -177987.484375\n",
      "Train Epoch: 3 [26752/54000 (50%)] Loss: -183415.031250\n",
      "Train Epoch: 3 [28160/54000 (52%)] Loss: -185109.406250\n",
      "Train Epoch: 3 [29568/54000 (55%)] Loss: -182633.640625\n",
      "Train Epoch: 3 [30976/54000 (57%)] Loss: -177193.562500\n",
      "Train Epoch: 3 [32384/54000 (60%)] Loss: -197936.125000\n",
      "Train Epoch: 3 [33792/54000 (63%)] Loss: -176514.437500\n",
      "Train Epoch: 3 [35200/54000 (65%)] Loss: -156546.343750\n",
      "Train Epoch: 3 [36608/54000 (68%)] Loss: -169810.687500\n",
      "Train Epoch: 3 [38016/54000 (70%)] Loss: -148136.562500\n",
      "Train Epoch: 3 [39424/54000 (73%)] Loss: -171274.234375\n",
      "Train Epoch: 3 [40832/54000 (76%)] Loss: -174934.421875\n",
      "Train Epoch: 3 [42240/54000 (78%)] Loss: -181556.843750\n",
      "Train Epoch: 3 [43648/54000 (81%)] Loss: -172026.406250\n",
      "Train Epoch: 3 [45056/54000 (83%)] Loss: -183489.187500\n",
      "Train Epoch: 3 [46464/54000 (86%)] Loss: -180102.609375\n",
      "Train Epoch: 3 [47872/54000 (89%)] Loss: -178718.187500\n",
      "Train Epoch: 3 [49280/54000 (91%)] Loss: -179270.062500\n",
      "Train Epoch: 3 [50688/54000 (94%)] Loss: -199105.250000\n",
      "Train Epoch: 3 [52096/54000 (96%)] Loss: -200182.500000\n",
      "    epoch          : 3\n",
      "    loss           : -172265.87430846292\n",
      "    val_loss       : -179217.82545731709\n",
      "Train Epoch: 4 [0/54000 (0%)] Loss: -200346.625000\n",
      "Train Epoch: 4 [1408/54000 (3%)] Loss: -179416.937500\n",
      "Train Epoch: 4 [2816/54000 (5%)] Loss: -187662.000000\n",
      "Train Epoch: 4 [4224/54000 (8%)] Loss: -180780.906250\n",
      "Train Epoch: 4 [5632/54000 (10%)] Loss: -172173.203125\n",
      "Train Epoch: 4 [7040/54000 (13%)] Loss: -187480.578125\n",
      "Train Epoch: 4 [8448/54000 (16%)] Loss: -187684.031250\n",
      "Train Epoch: 4 [9856/54000 (18%)] Loss: -150697.328125\n",
      "Train Epoch: 4 [11264/54000 (21%)] Loss: -184346.000000\n",
      "Train Epoch: 4 [12672/54000 (23%)] Loss: -174571.703125\n",
      "Train Epoch: 4 [14080/54000 (26%)] Loss: -180802.546875\n",
      "Train Epoch: 4 [15488/54000 (29%)] Loss: -172548.609375\n",
      "Train Epoch: 4 [16896/54000 (31%)] Loss: -172772.843750\n",
      "Train Epoch: 4 [18304/54000 (34%)] Loss: -160764.031250\n",
      "Train Epoch: 4 [19712/54000 (37%)] Loss: -164416.687500\n",
      "Train Epoch: 4 [21120/54000 (39%)] Loss: -153477.421875\n",
      "Train Epoch: 4 [22528/54000 (42%)] Loss: -180515.656250\n",
      "Train Epoch: 4 [23936/54000 (44%)] Loss: -190194.171875\n",
      "Train Epoch: 4 [25344/54000 (47%)] Loss: -181718.312500\n",
      "Train Epoch: 4 [26752/54000 (50%)] Loss: -190476.531250\n",
      "Train Epoch: 4 [28160/54000 (52%)] Loss: -184203.765625\n",
      "Train Epoch: 4 [29568/54000 (55%)] Loss: -175382.093750\n",
      "Train Epoch: 4 [30976/54000 (57%)] Loss: -184973.312500\n",
      "Train Epoch: 4 [32384/54000 (60%)] Loss: -164773.562500\n",
      "Train Epoch: 4 [33792/54000 (63%)] Loss: -178175.343750\n",
      "Train Epoch: 4 [35200/54000 (65%)] Loss: -164434.218750\n",
      "Train Epoch: 4 [36608/54000 (68%)] Loss: -186896.406250\n",
      "Train Epoch: 4 [38016/54000 (70%)] Loss: -174565.312500\n",
      "Train Epoch: 4 [39424/54000 (73%)] Loss: -174580.156250\n",
      "Train Epoch: 4 [40832/54000 (76%)] Loss: -156308.843750\n",
      "Train Epoch: 4 [42240/54000 (78%)] Loss: -157476.953125\n",
      "Train Epoch: 4 [43648/54000 (81%)] Loss: -158415.718750\n",
      "Train Epoch: 4 [45056/54000 (83%)] Loss: -203286.906250\n",
      "Train Epoch: 4 [46464/54000 (86%)] Loss: -178038.640625\n",
      "Train Epoch: 4 [47872/54000 (89%)] Loss: -176723.875000\n",
      "Train Epoch: 4 [49280/54000 (91%)] Loss: -175227.171875\n",
      "Train Epoch: 4 [50688/54000 (94%)] Loss: -135176.953125\n",
      "Train Epoch: 4 [52096/54000 (96%)] Loss: -181192.750000\n",
      "    epoch          : 4\n",
      "    loss           : -177053.78242748205\n",
      "    val_loss       : -177113.15577362804\n",
      "Train Epoch: 5 [0/54000 (0%)] Loss: -199634.140625\n",
      "Train Epoch: 5 [1408/54000 (3%)] Loss: -178131.625000\n",
      "Train Epoch: 5 [2816/54000 (5%)] Loss: -120179.156250\n",
      "Train Epoch: 5 [4224/54000 (8%)] Loss: -182465.906250\n",
      "Train Epoch: 5 [5632/54000 (10%)] Loss: -181290.937500\n",
      "Train Epoch: 5 [7040/54000 (13%)] Loss: -177711.578125\n",
      "Train Epoch: 5 [8448/54000 (16%)] Loss: -185484.218750\n",
      "Train Epoch: 5 [9856/54000 (18%)] Loss: -163726.906250\n",
      "Train Epoch: 5 [11264/54000 (21%)] Loss: -178205.281250\n",
      "Train Epoch: 5 [12672/54000 (23%)] Loss: -178478.015625\n",
      "Train Epoch: 5 [14080/54000 (26%)] Loss: -180632.250000\n",
      "Train Epoch: 5 [15488/54000 (29%)] Loss: -164835.453125\n",
      "Train Epoch: 5 [16896/54000 (31%)] Loss: -165498.218750\n",
      "Train Epoch: 5 [18304/54000 (34%)] Loss: -183526.093750\n",
      "Train Epoch: 5 [19712/54000 (37%)] Loss: -186233.328125\n",
      "Train Epoch: 5 [21120/54000 (39%)] Loss: -180778.468750\n",
      "Train Epoch: 5 [22528/54000 (42%)] Loss: -169442.062500\n",
      "Train Epoch: 5 [23936/54000 (44%)] Loss: -159022.265625\n",
      "Train Epoch: 5 [25344/54000 (47%)] Loss: -183797.359375\n",
      "Train Epoch: 5 [26752/54000 (50%)] Loss: -186840.812500\n",
      "Train Epoch: 5 [28160/54000 (52%)] Loss: -157815.078125\n",
      "Train Epoch: 5 [29568/54000 (55%)] Loss: -182835.968750\n",
      "Train Epoch: 5 [30976/54000 (57%)] Loss: -177664.093750\n",
      "Train Epoch: 5 [32384/54000 (60%)] Loss: -166261.484375\n",
      "Train Epoch: 5 [33792/54000 (63%)] Loss: -202879.031250\n",
      "Train Epoch: 5 [35200/54000 (65%)] Loss: -182627.812500\n",
      "Train Epoch: 5 [36608/54000 (68%)] Loss: -183887.546875\n",
      "Train Epoch: 5 [38016/54000 (70%)] Loss: -204621.093750\n",
      "Train Epoch: 5 [39424/54000 (73%)] Loss: -161282.812500\n",
      "Train Epoch: 5 [40832/54000 (76%)] Loss: -192031.500000\n",
      "Train Epoch: 5 [42240/54000 (78%)] Loss: -178890.031250\n",
      "Train Epoch: 5 [43648/54000 (81%)] Loss: -189876.531250\n",
      "Train Epoch: 5 [45056/54000 (83%)] Loss: -188532.312500\n",
      "Train Epoch: 5 [46464/54000 (86%)] Loss: -158847.875000\n",
      "Train Epoch: 5 [47872/54000 (89%)] Loss: -180893.343750\n",
      "Train Epoch: 5 [49280/54000 (91%)] Loss: -168431.765625\n",
      "Train Epoch: 5 [50688/54000 (94%)] Loss: -163658.765625\n",
      "Train Epoch: 5 [52096/54000 (96%)] Loss: -167684.656250\n",
      "    epoch          : 5\n",
      "    loss           : -178955.08517120214\n",
      "    val_loss       : -182270.83927210365\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 6 [0/54000 (0%)] Loss: -180937.312500\n",
      "Train Epoch: 6 [1408/54000 (3%)] Loss: -179107.500000\n",
      "Train Epoch: 6 [2816/54000 (5%)] Loss: -182290.796875\n",
      "Train Epoch: 6 [4224/54000 (8%)] Loss: -190517.031250\n",
      "Train Epoch: 6 [5632/54000 (10%)] Loss: -185179.671875\n",
      "Train Epoch: 6 [7040/54000 (13%)] Loss: -189915.343750\n",
      "Train Epoch: 6 [8448/54000 (16%)] Loss: -183900.468750\n",
      "Train Epoch: 6 [9856/54000 (18%)] Loss: -188662.890625\n",
      "Train Epoch: 6 [11264/54000 (21%)] Loss: -156392.671875\n",
      "Train Epoch: 6 [12672/54000 (23%)] Loss: -189907.218750\n",
      "Train Epoch: 6 [14080/54000 (26%)] Loss: -205744.906250\n",
      "Train Epoch: 6 [15488/54000 (29%)] Loss: -191290.578125\n",
      "Train Epoch: 6 [16896/54000 (31%)] Loss: -187991.406250\n",
      "Train Epoch: 6 [18304/54000 (34%)] Loss: -168938.921875\n",
      "Train Epoch: 6 [19712/54000 (37%)] Loss: -167170.953125\n",
      "Train Epoch: 6 [21120/54000 (39%)] Loss: -162301.421875\n",
      "Train Epoch: 6 [22528/54000 (42%)] Loss: -189896.187500\n",
      "Train Epoch: 6 [23936/54000 (44%)] Loss: -206940.406250\n",
      "Train Epoch: 6 [25344/54000 (47%)] Loss: -196338.187500\n",
      "Train Epoch: 6 [26752/54000 (50%)] Loss: -178389.906250\n",
      "Train Epoch: 6 [28160/54000 (52%)] Loss: -187237.015625\n",
      "Train Epoch: 6 [29568/54000 (55%)] Loss: -189425.687500\n",
      "Train Epoch: 6 [30976/54000 (57%)] Loss: -178414.093750\n",
      "Train Epoch: 6 [32384/54000 (60%)] Loss: -188409.515625\n",
      "Train Epoch: 6 [33792/54000 (63%)] Loss: -185928.218750\n",
      "Train Epoch: 6 [35200/54000 (65%)] Loss: -171578.718750\n",
      "Train Epoch: 6 [36608/54000 (68%)] Loss: -168513.656250\n",
      "Train Epoch: 6 [38016/54000 (70%)] Loss: -205586.421875\n",
      "Train Epoch: 6 [39424/54000 (73%)] Loss: -163932.671875\n",
      "Train Epoch: 6 [40832/54000 (76%)] Loss: -184921.156250\n",
      "Train Epoch: 6 [42240/54000 (78%)] Loss: -180527.968750\n",
      "Train Epoch: 6 [43648/54000 (81%)] Loss: -194103.875000\n",
      "Train Epoch: 6 [45056/54000 (83%)] Loss: -207976.406250\n",
      "Train Epoch: 6 [46464/54000 (86%)] Loss: -193569.656250\n",
      "Train Epoch: 6 [47872/54000 (89%)] Loss: -185957.437500\n",
      "Train Epoch: 6 [49280/54000 (91%)] Loss: -186860.906250\n",
      "Train Epoch: 6 [50688/54000 (94%)] Loss: -185952.812500\n",
      "Train Epoch: 6 [52096/54000 (96%)] Loss: -195961.593750\n",
      "    epoch          : 6\n",
      "    loss           : -182646.8339750299\n",
      "    val_loss       : -191235.29230182926\n",
      "Train Epoch: 7 [0/54000 (0%)] Loss: -208209.046875\n",
      "Train Epoch: 7 [1408/54000 (3%)] Loss: -188560.062500\n",
      "Train Epoch: 7 [2816/54000 (5%)] Loss: -188976.593750\n",
      "Train Epoch: 7 [4224/54000 (8%)] Loss: -189579.843750\n",
      "Train Epoch: 7 [5632/54000 (10%)] Loss: -169474.156250\n",
      "Train Epoch: 7 [7040/54000 (13%)] Loss: -191480.953125\n",
      "Train Epoch: 7 [8448/54000 (16%)] Loss: -190516.312500\n",
      "Train Epoch: 7 [9856/54000 (18%)] Loss: -181201.218750\n",
      "Train Epoch: 7 [11264/54000 (21%)] Loss: -190991.906250\n",
      "Train Epoch: 7 [12672/54000 (23%)] Loss: -190556.296875\n",
      "Train Epoch: 7 [14080/54000 (26%)] Loss: -188741.968750\n",
      "Train Epoch: 7 [15488/54000 (29%)] Loss: -186598.625000\n",
      "Train Epoch: 7 [16896/54000 (31%)] Loss: -146740.687500\n",
      "Train Epoch: 7 [18304/54000 (34%)] Loss: -195811.562500\n",
      "Train Epoch: 7 [19712/54000 (37%)] Loss: -188540.312500\n",
      "Train Epoch: 7 [21120/54000 (39%)] Loss: -187471.406250\n",
      "Train Epoch: 7 [22528/54000 (42%)] Loss: -209991.968750\n",
      "Train Epoch: 7 [23936/54000 (44%)] Loss: -184873.750000\n",
      "Train Epoch: 7 [25344/54000 (47%)] Loss: -186953.531250\n",
      "Train Epoch: 7 [26752/54000 (50%)] Loss: -170451.046875\n",
      "Train Epoch: 7 [28160/54000 (52%)] Loss: -166849.312500\n",
      "Train Epoch: 7 [29568/54000 (55%)] Loss: -186656.796875\n",
      "Train Epoch: 7 [30976/54000 (57%)] Loss: -186948.640625\n",
      "Train Epoch: 7 [32384/54000 (60%)] Loss: -188523.218750\n",
      "Train Epoch: 7 [33792/54000 (63%)] Loss: -190789.843750\n",
      "Train Epoch: 7 [35200/54000 (65%)] Loss: -183058.656250\n",
      "Train Epoch: 7 [36608/54000 (68%)] Loss: -190757.031250\n",
      "Train Epoch: 7 [38016/54000 (70%)] Loss: -188059.468750\n",
      "Train Epoch: 7 [39424/54000 (73%)] Loss: -172681.890625\n",
      "Train Epoch: 7 [40832/54000 (76%)] Loss: -167730.265625\n",
      "Train Epoch: 7 [42240/54000 (78%)] Loss: -184469.875000\n",
      "Train Epoch: 7 [43648/54000 (81%)] Loss: -210933.171875\n",
      "Train Epoch: 7 [45056/54000 (83%)] Loss: -211579.375000\n",
      "Train Epoch: 7 [46464/54000 (86%)] Loss: -182761.312500\n",
      "Train Epoch: 7 [47872/54000 (89%)] Loss: -190842.859375\n",
      "Train Epoch: 7 [49280/54000 (91%)] Loss: -152525.609375\n",
      "Train Epoch: 7 [50688/54000 (94%)] Loss: -189709.750000\n",
      "Train Epoch: 7 [52096/54000 (96%)] Loss: -191681.062500\n",
      "    epoch          : 7\n",
      "    loss           : -185478.93196770336\n",
      "    val_loss       : -191003.27886814025\n",
      "Train Epoch: 8 [0/54000 (0%)] Loss: -209379.500000\n",
      "Train Epoch: 8 [1408/54000 (3%)] Loss: -184242.125000\n",
      "Train Epoch: 8 [2816/54000 (5%)] Loss: -185664.468750\n",
      "Train Epoch: 8 [4224/54000 (8%)] Loss: -179147.937500\n",
      "Train Epoch: 8 [5632/54000 (10%)] Loss: -183716.093750\n",
      "Train Epoch: 8 [7040/54000 (13%)] Loss: -169601.015625\n",
      "Train Epoch: 8 [8448/54000 (16%)] Loss: -168993.843750\n",
      "Train Epoch: 8 [9856/54000 (18%)] Loss: -188030.156250\n",
      "Train Epoch: 8 [11264/54000 (21%)] Loss: -185259.750000\n",
      "Train Epoch: 8 [12672/54000 (23%)] Loss: -195659.828125\n",
      "Train Epoch: 8 [14080/54000 (26%)] Loss: -189151.578125\n",
      "Train Epoch: 8 [15488/54000 (29%)] Loss: -192252.781250\n",
      "Train Epoch: 8 [16896/54000 (31%)] Loss: -183162.687500\n",
      "Train Epoch: 8 [18304/54000 (34%)] Loss: -198701.500000\n",
      "Train Epoch: 8 [19712/54000 (37%)] Loss: -183316.468750\n",
      "Train Epoch: 8 [21120/54000 (39%)] Loss: -182126.921875\n",
      "Train Epoch: 8 [22528/54000 (42%)] Loss: -174458.265625\n",
      "Train Epoch: 8 [23936/54000 (44%)] Loss: -167292.031250\n",
      "Train Epoch: 8 [25344/54000 (47%)] Loss: -183891.750000\n",
      "Train Epoch: 8 [26752/54000 (50%)] Loss: -174146.484375\n",
      "Train Epoch: 8 [28160/54000 (52%)] Loss: -170967.375000\n",
      "Train Epoch: 8 [29568/54000 (55%)] Loss: -122746.273438\n",
      "Train Epoch: 8 [30976/54000 (57%)] Loss: -193793.500000\n",
      "Train Epoch: 8 [32384/54000 (60%)] Loss: -172314.171875\n",
      "Train Epoch: 8 [33792/54000 (63%)] Loss: -211228.687500\n",
      "Train Epoch: 8 [35200/54000 (65%)] Loss: -173823.062500\n",
      "Train Epoch: 8 [36608/54000 (68%)] Loss: -185763.625000\n",
      "Train Epoch: 8 [38016/54000 (70%)] Loss: -199996.781250\n",
      "Train Epoch: 8 [39424/54000 (73%)] Loss: -187695.812500\n",
      "Train Epoch: 8 [40832/54000 (76%)] Loss: -211216.625000\n",
      "Train Epoch: 8 [42240/54000 (78%)] Loss: -183915.375000\n",
      "Train Epoch: 8 [43648/54000 (81%)] Loss: -188974.156250\n",
      "Train Epoch: 8 [45056/54000 (83%)] Loss: -188532.625000\n",
      "Train Epoch: 8 [46464/54000 (86%)] Loss: -190304.218750\n",
      "Train Epoch: 8 [47872/54000 (89%)] Loss: -175641.812500\n",
      "Train Epoch: 8 [49280/54000 (91%)] Loss: -191530.093750\n",
      "Train Epoch: 8 [50688/54000 (94%)] Loss: -193366.500000\n",
      "Train Epoch: 8 [52096/54000 (96%)] Loss: -174422.187500\n",
      "    epoch          : 8\n",
      "    loss           : -186736.17959404906\n",
      "    val_loss       : -196247.18140243902\n",
      "Train Epoch: 9 [0/54000 (0%)] Loss: -193663.515625\n",
      "Train Epoch: 9 [1408/54000 (3%)] Loss: -171952.562500\n",
      "Train Epoch: 9 [2816/54000 (5%)] Loss: -171759.812500\n",
      "Train Epoch: 9 [4224/54000 (8%)] Loss: -197846.343750\n",
      "Train Epoch: 9 [5632/54000 (10%)] Loss: -186907.359375\n",
      "Train Epoch: 9 [7040/54000 (13%)] Loss: -186395.218750\n",
      "Train Epoch: 9 [8448/54000 (16%)] Loss: -211042.781250\n",
      "Train Epoch: 9 [9856/54000 (18%)] Loss: -195611.218750\n",
      "Train Epoch: 9 [11264/54000 (21%)] Loss: -192663.265625\n",
      "Train Epoch: 9 [12672/54000 (23%)] Loss: -189529.218750\n",
      "Train Epoch: 9 [14080/54000 (26%)] Loss: -208562.109375\n",
      "Train Epoch: 9 [15488/54000 (29%)] Loss: -188875.343750\n",
      "Train Epoch: 9 [16896/54000 (31%)] Loss: -190538.562500\n",
      "Train Epoch: 9 [18304/54000 (34%)] Loss: -186925.781250\n",
      "Train Epoch: 9 [19712/54000 (37%)] Loss: -205040.312500\n",
      "Train Epoch: 9 [21120/54000 (39%)] Loss: -184483.906250\n",
      "Train Epoch: 9 [22528/54000 (42%)] Loss: -184838.203125\n",
      "Train Epoch: 9 [23936/54000 (44%)] Loss: -174171.515625\n",
      "Train Epoch: 9 [25344/54000 (47%)] Loss: -173928.625000\n",
      "Train Epoch: 9 [26752/54000 (50%)] Loss: -212277.218750\n",
      "Train Epoch: 9 [28160/54000 (52%)] Loss: -208688.687500\n",
      "Train Epoch: 9 [29568/54000 (55%)] Loss: -185251.375000\n",
      "Train Epoch: 9 [30976/54000 (57%)] Loss: -192225.093750\n",
      "Train Epoch: 9 [32384/54000 (60%)] Loss: -153472.031250\n",
      "Train Epoch: 9 [33792/54000 (63%)] Loss: -191216.125000\n",
      "Train Epoch: 9 [35200/54000 (65%)] Loss: -186305.281250\n",
      "Train Epoch: 9 [36608/54000 (68%)] Loss: -195083.796875\n",
      "Train Epoch: 9 [38016/54000 (70%)] Loss: -173363.937500\n",
      "Train Epoch: 9 [39424/54000 (73%)] Loss: -174870.265625\n",
      "Train Epoch: 9 [40832/54000 (76%)] Loss: -194554.531250\n",
      "Train Epoch: 9 [42240/54000 (78%)] Loss: -192392.593750\n",
      "Train Epoch: 9 [43648/54000 (81%)] Loss: -176421.609375\n",
      "Train Epoch: 9 [45056/54000 (83%)] Loss: -187355.656250\n",
      "Train Epoch: 9 [46464/54000 (86%)] Loss: -189716.953125\n",
      "Train Epoch: 9 [47872/54000 (89%)] Loss: -189470.812500\n",
      "Train Epoch: 9 [49280/54000 (91%)] Loss: -190313.937500\n",
      "Train Epoch: 9 [50688/54000 (94%)] Loss: -191626.437500\n",
      "Train Epoch: 9 [52096/54000 (96%)] Loss: -176161.093750\n",
      "    epoch          : 9\n",
      "    loss           : -187967.76854066984\n",
      "    val_loss       : -195314.61680640245\n",
      "Train Epoch: 10 [0/54000 (0%)] Loss: -186464.875000\n",
      "Train Epoch: 10 [1408/54000 (3%)] Loss: -186868.875000\n",
      "Train Epoch: 10 [2816/54000 (5%)] Loss: -189361.078125\n",
      "Train Epoch: 10 [4224/54000 (8%)] Loss: -188145.203125\n",
      "Train Epoch: 10 [5632/54000 (10%)] Loss: -186323.562500\n",
      "Train Epoch: 10 [7040/54000 (13%)] Loss: -172944.375000\n",
      "Train Epoch: 10 [8448/54000 (16%)] Loss: -191118.421875\n",
      "Train Epoch: 10 [9856/54000 (18%)] Loss: -207563.000000\n",
      "Train Epoch: 10 [11264/54000 (21%)] Loss: -200406.312500\n",
      "Train Epoch: 10 [12672/54000 (23%)] Loss: -175004.531250\n",
      "Train Epoch: 10 [14080/54000 (26%)] Loss: -192344.281250\n",
      "Train Epoch: 10 [15488/54000 (29%)] Loss: -189967.156250\n",
      "Train Epoch: 10 [16896/54000 (31%)] Loss: -187052.937500\n",
      "Train Epoch: 10 [18304/54000 (34%)] Loss: -176413.406250\n",
      "Train Epoch: 10 [19712/54000 (37%)] Loss: -201230.687500\n",
      "Train Epoch: 10 [21120/54000 (39%)] Loss: -208648.218750\n",
      "Train Epoch: 10 [22528/54000 (42%)] Loss: -202565.687500\n",
      "Train Epoch: 10 [23936/54000 (44%)] Loss: -187562.250000\n",
      "Train Epoch: 10 [25344/54000 (47%)] Loss: -185899.718750\n",
      "Train Epoch: 10 [26752/54000 (50%)] Loss: -201655.937500\n",
      "Train Epoch: 10 [28160/54000 (52%)] Loss: -173008.296875\n",
      "Train Epoch: 10 [29568/54000 (55%)] Loss: -187205.687500\n",
      "Train Epoch: 10 [30976/54000 (57%)] Loss: -193022.281250\n",
      "Train Epoch: 10 [32384/54000 (60%)] Loss: -194955.781250\n",
      "Train Epoch: 10 [33792/54000 (63%)] Loss: -214080.734375\n",
      "Train Epoch: 10 [35200/54000 (65%)] Loss: -178037.015625\n",
      "Train Epoch: 10 [36608/54000 (68%)] Loss: -201349.093750\n",
      "Train Epoch: 10 [38016/54000 (70%)] Loss: -203464.968750\n",
      "Train Epoch: 10 [39424/54000 (73%)] Loss: -159409.812500\n",
      "Train Epoch: 10 [40832/54000 (76%)] Loss: -194201.156250\n",
      "Train Epoch: 10 [42240/54000 (78%)] Loss: -195237.687500\n",
      "Train Epoch: 10 [43648/54000 (81%)] Loss: -186774.593750\n",
      "Train Epoch: 10 [45056/54000 (83%)] Loss: -172279.734375\n",
      "Train Epoch: 10 [46464/54000 (86%)] Loss: -187739.218750\n",
      "Train Epoch: 10 [47872/54000 (89%)] Loss: -190827.187500\n",
      "Train Epoch: 10 [49280/54000 (91%)] Loss: -185472.281250\n",
      "Train Epoch: 10 [50688/54000 (94%)] Loss: -188766.484375\n",
      "Train Epoch: 10 [52096/54000 (96%)] Loss: -176589.531250\n",
      "    epoch          : 10\n",
      "    loss           : -190412.0573041268\n",
      "    val_loss       : -200257.94073932926\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 11 [0/54000 (0%)] Loss: -214880.671875\n",
      "Train Epoch: 11 [1408/54000 (3%)] Loss: -196012.468750\n",
      "Train Epoch: 11 [2816/54000 (5%)] Loss: -195726.781250\n",
      "Train Epoch: 11 [4224/54000 (8%)] Loss: -174631.156250\n",
      "Train Epoch: 11 [5632/54000 (10%)] Loss: -202198.531250\n",
      "Train Epoch: 11 [7040/54000 (13%)] Loss: -193811.718750\n",
      "Train Epoch: 11 [8448/54000 (16%)] Loss: -214992.625000\n",
      "Train Epoch: 11 [9856/54000 (18%)] Loss: -214957.734375\n",
      "Train Epoch: 11 [11264/54000 (21%)] Loss: -204758.453125\n",
      "Train Epoch: 11 [12672/54000 (23%)] Loss: -190084.140625\n",
      "Train Epoch: 11 [14080/54000 (26%)] Loss: -190572.062500\n",
      "Train Epoch: 11 [15488/54000 (29%)] Loss: -191500.468750\n",
      "Train Epoch: 11 [16896/54000 (31%)] Loss: -202595.234375\n",
      "Train Epoch: 11 [18304/54000 (34%)] Loss: -196102.000000\n",
      "Train Epoch: 11 [19712/54000 (37%)] Loss: -179882.828125\n",
      "Train Epoch: 11 [21120/54000 (39%)] Loss: -215872.687500\n",
      "Train Epoch: 11 [22528/54000 (42%)] Loss: -178384.687500\n",
      "Train Epoch: 11 [23936/54000 (44%)] Loss: -194889.437500\n",
      "Train Epoch: 11 [25344/54000 (47%)] Loss: -189658.750000\n",
      "Train Epoch: 11 [26752/54000 (50%)] Loss: -193764.078125\n",
      "Train Epoch: 11 [28160/54000 (52%)] Loss: -190305.156250\n",
      "Train Epoch: 11 [29568/54000 (55%)] Loss: -192524.906250\n",
      "Train Epoch: 11 [30976/54000 (57%)] Loss: -190183.593750\n",
      "Train Epoch: 11 [32384/54000 (60%)] Loss: -191846.515625\n",
      "Train Epoch: 11 [33792/54000 (63%)] Loss: -171492.578125\n",
      "Train Epoch: 11 [35200/54000 (65%)] Loss: -175967.968750\n",
      "Train Epoch: 11 [36608/54000 (68%)] Loss: -177337.562500\n",
      "Train Epoch: 11 [38016/54000 (70%)] Loss: -200916.000000\n",
      "Train Epoch: 11 [39424/54000 (73%)] Loss: -215935.718750\n",
      "Train Epoch: 11 [40832/54000 (76%)] Loss: -203039.156250\n",
      "Train Epoch: 11 [42240/54000 (78%)] Loss: -205277.421875\n",
      "Train Epoch: 11 [43648/54000 (81%)] Loss: -188329.718750\n",
      "Train Epoch: 11 [45056/54000 (83%)] Loss: -191789.968750\n",
      "Train Epoch: 11 [46464/54000 (86%)] Loss: -186906.343750\n",
      "Train Epoch: 11 [47872/54000 (89%)] Loss: -188686.671875\n",
      "Train Epoch: 11 [49280/54000 (91%)] Loss: -186261.078125\n",
      "Train Epoch: 11 [50688/54000 (94%)] Loss: -196182.687500\n",
      "Train Epoch: 11 [52096/54000 (96%)] Loss: -196403.093750\n",
      "    epoch          : 11\n",
      "    loss           : -192439.4272951555\n",
      "    val_loss       : -201796.09737042684\n",
      "Train Epoch: 12 [0/54000 (0%)] Loss: -189166.734375\n",
      "Train Epoch: 12 [1408/54000 (3%)] Loss: -215938.562500\n",
      "Train Epoch: 12 [2816/54000 (5%)] Loss: -201649.000000\n",
      "Train Epoch: 12 [4224/54000 (8%)] Loss: -178316.296875\n",
      "Train Epoch: 12 [5632/54000 (10%)] Loss: -178465.812500\n",
      "Train Epoch: 12 [7040/54000 (13%)] Loss: -176479.937500\n",
      "Train Epoch: 12 [8448/54000 (16%)] Loss: -191692.968750\n",
      "Train Epoch: 12 [9856/54000 (18%)] Loss: -217062.968750\n",
      "Train Epoch: 12 [11264/54000 (21%)] Loss: -195548.468750\n",
      "Train Epoch: 12 [12672/54000 (23%)] Loss: -191586.843750\n",
      "Train Epoch: 12 [14080/54000 (26%)] Loss: -187853.062500\n",
      "Train Epoch: 12 [15488/54000 (29%)] Loss: -190181.546875\n",
      "Train Epoch: 12 [16896/54000 (31%)] Loss: -198199.125000\n",
      "Train Epoch: 12 [18304/54000 (34%)] Loss: -183330.703125\n",
      "Train Epoch: 12 [19712/54000 (37%)] Loss: -179336.250000\n",
      "Train Epoch: 12 [21120/54000 (39%)] Loss: -178858.656250\n",
      "Train Epoch: 12 [22528/54000 (42%)] Loss: -192519.187500\n",
      "Train Epoch: 12 [23936/54000 (44%)] Loss: -191263.062500\n",
      "Train Epoch: 12 [25344/54000 (47%)] Loss: -202838.328125\n",
      "Train Epoch: 12 [26752/54000 (50%)] Loss: -194266.750000\n",
      "Train Epoch: 12 [28160/54000 (52%)] Loss: -193204.828125\n",
      "Train Epoch: 12 [29568/54000 (55%)] Loss: -189380.187500\n",
      "Train Epoch: 12 [30976/54000 (57%)] Loss: -191809.734375\n",
      "Train Epoch: 12 [32384/54000 (60%)] Loss: -193899.812500\n",
      "Train Epoch: 12 [33792/54000 (63%)] Loss: -173730.343750\n",
      "Train Epoch: 12 [35200/54000 (65%)] Loss: -180592.812500\n",
      "Train Epoch: 12 [36608/54000 (68%)] Loss: -191437.875000\n",
      "Train Epoch: 12 [38016/54000 (70%)] Loss: -188669.968750\n",
      "Train Epoch: 12 [39424/54000 (73%)] Loss: -205492.750000\n",
      "Train Epoch: 12 [40832/54000 (76%)] Loss: -203461.156250\n",
      "Train Epoch: 12 [42240/54000 (78%)] Loss: -196453.625000\n",
      "Train Epoch: 12 [43648/54000 (81%)] Loss: -191178.312500\n",
      "Train Epoch: 12 [45056/54000 (83%)] Loss: -192088.281250\n",
      "Train Epoch: 12 [46464/54000 (86%)] Loss: -193067.531250\n",
      "Train Epoch: 12 [47872/54000 (89%)] Loss: -200584.375000\n",
      "Train Epoch: 12 [49280/54000 (91%)] Loss: -198056.015625\n",
      "Train Epoch: 12 [50688/54000 (94%)] Loss: -188429.093750\n",
      "Train Epoch: 12 [52096/54000 (96%)] Loss: -198691.859375\n",
      "    epoch          : 12\n",
      "    loss           : -193456.02568032296\n",
      "    val_loss       : -202994.55144817074\n",
      "Train Epoch: 13 [0/54000 (0%)] Loss: -196221.375000\n",
      "Train Epoch: 13 [1408/54000 (3%)] Loss: -199691.328125\n",
      "Train Epoch: 13 [2816/54000 (5%)] Loss: -204144.187500\n",
      "Train Epoch: 13 [4224/54000 (8%)] Loss: -197241.718750\n",
      "Train Epoch: 13 [5632/54000 (10%)] Loss: -191060.203125\n",
      "Train Epoch: 13 [7040/54000 (13%)] Loss: -181527.031250\n",
      "Train Epoch: 13 [8448/54000 (16%)] Loss: -176984.500000\n",
      "Train Epoch: 13 [9856/54000 (18%)] Loss: -205053.921875\n",
      "Train Epoch: 13 [11264/54000 (21%)] Loss: -176154.250000\n",
      "Train Epoch: 13 [12672/54000 (23%)] Loss: -179374.593750\n",
      "Train Epoch: 13 [14080/54000 (26%)] Loss: -199885.062500\n",
      "Train Epoch: 13 [15488/54000 (29%)] Loss: -198688.875000\n",
      "Train Epoch: 13 [16896/54000 (31%)] Loss: -191832.187500\n",
      "Train Epoch: 13 [18304/54000 (34%)] Loss: -194792.765625\n",
      "Train Epoch: 13 [19712/54000 (37%)] Loss: -217876.281250\n",
      "Train Epoch: 13 [21120/54000 (39%)] Loss: -193696.625000\n",
      "Train Epoch: 13 [22528/54000 (42%)] Loss: -199886.453125\n",
      "Train Epoch: 13 [23936/54000 (44%)] Loss: -194320.937500\n",
      "Train Epoch: 13 [25344/54000 (47%)] Loss: -205743.718750\n",
      "Train Epoch: 13 [26752/54000 (50%)] Loss: -196901.750000\n",
      "Train Epoch: 13 [28160/54000 (52%)] Loss: -207349.531250\n",
      "Train Epoch: 13 [29568/54000 (55%)] Loss: -196390.000000\n",
      "Train Epoch: 13 [30976/54000 (57%)] Loss: -182783.890625\n",
      "Train Epoch: 13 [32384/54000 (60%)] Loss: -183920.968750\n",
      "Train Epoch: 13 [33792/54000 (63%)] Loss: -178647.937500\n",
      "Train Epoch: 13 [35200/54000 (65%)] Loss: -190735.000000\n",
      "Train Epoch: 13 [36608/54000 (68%)] Loss: -197944.937500\n",
      "Train Epoch: 13 [38016/54000 (70%)] Loss: -218193.656250\n",
      "Train Epoch: 13 [39424/54000 (73%)] Loss: -192014.250000\n",
      "Train Epoch: 13 [40832/54000 (76%)] Loss: -204588.703125\n",
      "Train Epoch: 13 [42240/54000 (78%)] Loss: -193542.656250\n",
      "Train Epoch: 13 [43648/54000 (81%)] Loss: -199177.812500\n",
      "Train Epoch: 13 [45056/54000 (83%)] Loss: -200442.687500\n",
      "Train Epoch: 13 [46464/54000 (86%)] Loss: -179465.468750\n",
      "Train Epoch: 13 [47872/54000 (89%)] Loss: -201114.843750\n",
      "Train Epoch: 13 [49280/54000 (91%)] Loss: -183975.390625\n",
      "Train Epoch: 13 [50688/54000 (94%)] Loss: -192953.921875\n",
      "Train Epoch: 13 [52096/54000 (96%)] Loss: -191249.750000\n",
      "    epoch          : 13\n",
      "    loss           : -194868.6720992823\n",
      "    val_loss       : -204022.23589939025\n",
      "Train Epoch: 14 [0/54000 (0%)] Loss: -219174.812500\n",
      "Train Epoch: 14 [1408/54000 (3%)] Loss: -179121.562500\n",
      "Train Epoch: 14 [2816/54000 (5%)] Loss: -181623.812500\n",
      "Train Epoch: 14 [4224/54000 (8%)] Loss: -197038.375000\n",
      "Train Epoch: 14 [5632/54000 (10%)] Loss: -198951.843750\n",
      "Train Epoch: 14 [7040/54000 (13%)] Loss: -189314.968750\n",
      "Train Epoch: 14 [8448/54000 (16%)] Loss: -181291.578125\n",
      "Train Epoch: 14 [9856/54000 (18%)] Loss: -204805.750000\n",
      "Train Epoch: 14 [11264/54000 (21%)] Loss: -179652.562500\n",
      "Train Epoch: 14 [12672/54000 (23%)] Loss: -180160.187500\n",
      "Train Epoch: 14 [14080/54000 (26%)] Loss: -191821.187500\n",
      "Train Epoch: 14 [15488/54000 (29%)] Loss: -190451.156250\n",
      "Train Epoch: 14 [16896/54000 (31%)] Loss: -198140.734375\n",
      "Train Epoch: 14 [18304/54000 (34%)] Loss: -191657.406250\n",
      "Train Epoch: 14 [19712/54000 (37%)] Loss: -199921.984375\n",
      "Train Epoch: 14 [21120/54000 (39%)] Loss: -183728.828125\n",
      "Train Epoch: 14 [22528/54000 (42%)] Loss: -182424.781250\n",
      "Train Epoch: 14 [23936/54000 (44%)] Loss: -207965.796875\n",
      "Train Epoch: 14 [25344/54000 (47%)] Loss: -198904.187500\n",
      "Train Epoch: 14 [26752/54000 (50%)] Loss: -195887.937500\n",
      "Train Epoch: 14 [28160/54000 (52%)] Loss: -193543.125000\n",
      "Train Epoch: 14 [29568/54000 (55%)] Loss: -196836.593750\n",
      "Train Epoch: 14 [30976/54000 (57%)] Loss: -196029.640625\n",
      "Train Epoch: 14 [32384/54000 (60%)] Loss: -220119.859375\n",
      "Train Epoch: 14 [33792/54000 (63%)] Loss: -196122.281250\n",
      "Train Epoch: 14 [35200/54000 (65%)] Loss: -191255.656250\n",
      "Train Epoch: 14 [36608/54000 (68%)] Loss: -181745.937500\n",
      "Train Epoch: 14 [38016/54000 (70%)] Loss: -220491.062500\n",
      "Train Epoch: 14 [39424/54000 (73%)] Loss: -199629.062500\n",
      "Train Epoch: 14 [40832/54000 (76%)] Loss: -199115.312500\n",
      "Train Epoch: 14 [42240/54000 (78%)] Loss: -190702.531250\n",
      "Train Epoch: 14 [43648/54000 (81%)] Loss: -219280.187500\n",
      "Train Epoch: 14 [45056/54000 (83%)] Loss: -192039.531250\n",
      "Train Epoch: 14 [46464/54000 (86%)] Loss: -178120.156250\n",
      "Train Epoch: 14 [47872/54000 (89%)] Loss: -191932.312500\n",
      "Train Epoch: 14 [49280/54000 (91%)] Loss: -192035.484375\n",
      "Train Epoch: 14 [50688/54000 (94%)] Loss: -201677.281250\n",
      "Train Epoch: 14 [52096/54000 (96%)] Loss: -198224.140625\n",
      "    epoch          : 14\n",
      "    loss           : -196209.66241776315\n",
      "    val_loss       : -205437.80449695123\n",
      "Train Epoch: 15 [0/54000 (0%)] Loss: -198837.593750\n",
      "Train Epoch: 15 [1408/54000 (3%)] Loss: -200205.656250\n",
      "Train Epoch: 15 [2816/54000 (5%)] Loss: -195184.171875\n",
      "Train Epoch: 15 [4224/54000 (8%)] Loss: -193084.531250\n",
      "Train Epoch: 15 [5632/54000 (10%)] Loss: -196030.140625\n",
      "Train Epoch: 15 [7040/54000 (13%)] Loss: -192781.109375\n",
      "Train Epoch: 15 [8448/54000 (16%)] Loss: -179470.031250\n",
      "Train Epoch: 15 [9856/54000 (18%)] Loss: -198333.687500\n",
      "Train Epoch: 15 [11264/54000 (21%)] Loss: -200561.718750\n",
      "Train Epoch: 15 [12672/54000 (23%)] Loss: -184257.546875\n",
      "Train Epoch: 15 [14080/54000 (26%)] Loss: -200829.203125\n",
      "Train Epoch: 15 [15488/54000 (29%)] Loss: -201174.343750\n",
      "Train Epoch: 15 [16896/54000 (31%)] Loss: -189999.953125\n",
      "Train Epoch: 15 [18304/54000 (34%)] Loss: -208329.609375\n",
      "Train Epoch: 15 [19712/54000 (37%)] Loss: -201625.000000\n",
      "Train Epoch: 15 [21120/54000 (39%)] Loss: -200513.031250\n",
      "Train Epoch: 15 [22528/54000 (42%)] Loss: -193384.687500\n",
      "Train Epoch: 15 [23936/54000 (44%)] Loss: -192747.859375\n",
      "Train Epoch: 15 [25344/54000 (47%)] Loss: -181203.640625\n",
      "Train Epoch: 15 [26752/54000 (50%)] Loss: -183581.546875\n",
      "Train Epoch: 15 [28160/54000 (52%)] Loss: -175235.281250\n",
      "Train Epoch: 15 [29568/54000 (55%)] Loss: -182342.343750\n",
      "Train Epoch: 15 [30976/54000 (57%)] Loss: -183707.156250\n",
      "Train Epoch: 15 [32384/54000 (60%)] Loss: -199809.031250\n",
      "Train Epoch: 15 [33792/54000 (63%)] Loss: -206613.218750\n",
      "Train Epoch: 15 [35200/54000 (65%)] Loss: -219400.578125\n",
      "Train Epoch: 15 [36608/54000 (68%)] Loss: -179672.765625\n",
      "Train Epoch: 15 [38016/54000 (70%)] Loss: -194578.328125\n",
      "Train Epoch: 15 [39424/54000 (73%)] Loss: -195590.031250\n",
      "Train Epoch: 15 [40832/54000 (76%)] Loss: -201950.343750\n",
      "Train Epoch: 15 [42240/54000 (78%)] Loss: -198530.687500\n",
      "Train Epoch: 15 [43648/54000 (81%)] Loss: -178860.687500\n",
      "Train Epoch: 15 [45056/54000 (83%)] Loss: -179103.234375\n",
      "Train Epoch: 15 [46464/54000 (86%)] Loss: -192934.718750\n",
      "Train Epoch: 15 [47872/54000 (89%)] Loss: -195617.781250\n",
      "Train Epoch: 15 [49280/54000 (91%)] Loss: -199847.562500\n",
      "Train Epoch: 15 [50688/54000 (94%)] Loss: -184686.218750\n",
      "Train Epoch: 15 [52096/54000 (96%)] Loss: -183545.265625\n",
      "    epoch          : 15\n",
      "    loss           : -197075.150194378\n",
      "    val_loss       : -206660.63433689025\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch15.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 16 [0/54000 (0%)] Loss: -184613.031250\n",
      "Train Epoch: 16 [1408/54000 (3%)] Loss: -220750.812500\n",
      "Train Epoch: 16 [2816/54000 (5%)] Loss: -181601.625000\n",
      "Train Epoch: 16 [4224/54000 (8%)] Loss: -199772.343750\n",
      "Train Epoch: 16 [5632/54000 (10%)] Loss: -201256.156250\n",
      "Train Epoch: 16 [7040/54000 (13%)] Loss: -199125.859375\n",
      "Train Epoch: 16 [8448/54000 (16%)] Loss: -194537.375000\n",
      "Train Epoch: 16 [9856/54000 (18%)] Loss: -193336.125000\n",
      "Train Epoch: 16 [11264/54000 (21%)] Loss: -206812.281250\n",
      "Train Epoch: 16 [12672/54000 (23%)] Loss: -200895.687500\n",
      "Train Epoch: 16 [14080/54000 (26%)] Loss: -221739.437500\n",
      "Train Epoch: 16 [15488/54000 (29%)] Loss: -195102.562500\n",
      "Train Epoch: 16 [16896/54000 (31%)] Loss: -198036.875000\n",
      "Train Epoch: 16 [18304/54000 (34%)] Loss: -195901.718750\n",
      "Train Epoch: 16 [19712/54000 (37%)] Loss: -184186.062500\n",
      "Train Epoch: 16 [21120/54000 (39%)] Loss: -220913.609375\n",
      "Train Epoch: 16 [22528/54000 (42%)] Loss: -201534.468750\n",
      "Train Epoch: 16 [23936/54000 (44%)] Loss: -202333.187500\n",
      "Train Epoch: 16 [25344/54000 (47%)] Loss: -200429.625000\n",
      "Train Epoch: 16 [26752/54000 (50%)] Loss: -200536.812500\n",
      "Train Epoch: 16 [28160/54000 (52%)] Loss: -201502.078125\n",
      "Train Epoch: 16 [29568/54000 (55%)] Loss: -195454.312500\n",
      "Train Epoch: 16 [30976/54000 (57%)] Loss: -195430.218750\n",
      "Train Epoch: 16 [32384/54000 (60%)] Loss: -220411.046875\n",
      "Train Epoch: 16 [33792/54000 (63%)] Loss: -196431.875000\n",
      "Train Epoch: 16 [35200/54000 (65%)] Loss: -195722.546875\n",
      "Train Epoch: 16 [36608/54000 (68%)] Loss: -194915.312500\n",
      "Train Epoch: 16 [38016/54000 (70%)] Loss: -197385.281250\n",
      "Train Epoch: 16 [39424/54000 (73%)] Loss: -185540.843750\n",
      "Train Epoch: 16 [40832/54000 (76%)] Loss: -184689.937500\n",
      "Train Epoch: 16 [42240/54000 (78%)] Loss: -202471.125000\n",
      "Train Epoch: 16 [43648/54000 (81%)] Loss: -200337.593750\n",
      "Train Epoch: 16 [45056/54000 (83%)] Loss: -202649.000000\n",
      "Train Epoch: 16 [46464/54000 (86%)] Loss: -194810.343750\n",
      "Train Epoch: 16 [47872/54000 (89%)] Loss: -182957.843750\n",
      "Train Epoch: 16 [49280/54000 (91%)] Loss: -190538.093750\n",
      "Train Epoch: 16 [50688/54000 (94%)] Loss: -221758.718750\n",
      "Train Epoch: 16 [52096/54000 (96%)] Loss: -194380.625000\n",
      "    epoch          : 16\n",
      "    loss           : -197974.01031698566\n",
      "    val_loss       : -207715.25514481709\n",
      "Train Epoch: 17 [0/54000 (0%)] Loss: -180523.312500\n",
      "Train Epoch: 17 [1408/54000 (3%)] Loss: -181116.437500\n",
      "Train Epoch: 17 [2816/54000 (5%)] Loss: -201446.234375\n",
      "Train Epoch: 17 [4224/54000 (8%)] Loss: -201300.968750\n",
      "Train Epoch: 17 [5632/54000 (10%)] Loss: -198911.562500\n",
      "Train Epoch: 17 [7040/54000 (13%)] Loss: -201237.203125\n",
      "Train Epoch: 17 [8448/54000 (16%)] Loss: -220968.640625\n",
      "Train Epoch: 17 [9856/54000 (18%)] Loss: -195480.625000\n",
      "Train Epoch: 17 [11264/54000 (21%)] Loss: -201388.687500\n",
      "Train Epoch: 17 [12672/54000 (23%)] Loss: -201736.406250\n",
      "Train Epoch: 17 [14080/54000 (26%)] Loss: -194280.125000\n",
      "Train Epoch: 17 [15488/54000 (29%)] Loss: -194944.500000\n",
      "Train Epoch: 17 [16896/54000 (31%)] Loss: -220914.218750\n",
      "Train Epoch: 17 [18304/54000 (34%)] Loss: -209485.250000\n",
      "Train Epoch: 17 [19712/54000 (37%)] Loss: -186456.968750\n",
      "Train Epoch: 17 [21120/54000 (39%)] Loss: -187853.500000\n",
      "Train Epoch: 17 [22528/54000 (42%)] Loss: -179944.203125\n",
      "Train Epoch: 17 [23936/54000 (44%)] Loss: -196814.187500\n",
      "Train Epoch: 17 [25344/54000 (47%)] Loss: -187852.328125\n",
      "Train Epoch: 17 [26752/54000 (50%)] Loss: -201440.828125\n",
      "Train Epoch: 17 [28160/54000 (52%)] Loss: -180205.687500\n",
      "Train Epoch: 17 [29568/54000 (55%)] Loss: -221465.468750\n",
      "Train Epoch: 17 [30976/54000 (57%)] Loss: -198753.750000\n",
      "Train Epoch: 17 [32384/54000 (60%)] Loss: -184026.000000\n",
      "Train Epoch: 17 [33792/54000 (63%)] Loss: -210683.812500\n",
      "Train Epoch: 17 [35200/54000 (65%)] Loss: -222723.718750\n",
      "Train Epoch: 17 [36608/54000 (68%)] Loss: -197102.687500\n",
      "Train Epoch: 17 [38016/54000 (70%)] Loss: -200723.687500\n",
      "Train Epoch: 17 [39424/54000 (73%)] Loss: -203743.218750\n",
      "Train Epoch: 17 [40832/54000 (76%)] Loss: -187425.046875\n",
      "Train Epoch: 17 [42240/54000 (78%)] Loss: -200987.125000\n",
      "Train Epoch: 17 [43648/54000 (81%)] Loss: -195858.453125\n",
      "Train Epoch: 17 [45056/54000 (83%)] Loss: -221324.250000\n",
      "Train Epoch: 17 [46464/54000 (86%)] Loss: -196944.531250\n",
      "Train Epoch: 17 [47872/54000 (89%)] Loss: -196343.671875\n",
      "Train Epoch: 17 [49280/54000 (91%)] Loss: -184481.968750\n",
      "Train Epoch: 17 [50688/54000 (94%)] Loss: -203230.093750\n",
      "Train Epoch: 17 [52096/54000 (96%)] Loss: -193723.718750\n",
      "    epoch          : 17\n",
      "    loss           : -199265.04694976076\n",
      "    val_loss       : -208534.53410823172\n",
      "Train Epoch: 18 [0/54000 (0%)] Loss: -201864.687500\n",
      "Train Epoch: 18 [1408/54000 (3%)] Loss: -204192.906250\n",
      "Train Epoch: 18 [2816/54000 (5%)] Loss: -188599.875000\n",
      "Train Epoch: 18 [4224/54000 (8%)] Loss: -181729.718750\n",
      "Train Epoch: 18 [5632/54000 (10%)] Loss: -186895.234375\n",
      "Train Epoch: 18 [7040/54000 (13%)] Loss: -194990.750000\n",
      "Train Epoch: 18 [8448/54000 (16%)] Loss: -194330.812500\n",
      "Train Epoch: 18 [9856/54000 (18%)] Loss: -183952.078125\n",
      "Train Epoch: 18 [11264/54000 (21%)] Loss: -209575.812500\n",
      "Train Epoch: 18 [12672/54000 (23%)] Loss: -203981.375000\n",
      "Train Epoch: 18 [14080/54000 (26%)] Loss: -202811.562500\n",
      "Train Epoch: 18 [15488/54000 (29%)] Loss: -203049.687500\n",
      "Train Epoch: 18 [16896/54000 (31%)] Loss: -198221.000000\n",
      "Train Epoch: 18 [18304/54000 (34%)] Loss: -193957.750000\n",
      "Train Epoch: 18 [19712/54000 (37%)] Loss: -223477.171875\n",
      "Train Epoch: 18 [21120/54000 (39%)] Loss: -201714.031250\n",
      "Train Epoch: 18 [22528/54000 (42%)] Loss: -199850.937500\n",
      "Train Epoch: 18 [23936/54000 (44%)] Loss: -185542.468750\n",
      "Train Epoch: 18 [25344/54000 (47%)] Loss: -209774.343750\n",
      "Train Epoch: 18 [26752/54000 (50%)] Loss: -222578.109375\n",
      "Train Epoch: 18 [28160/54000 (52%)] Loss: -196735.468750\n",
      "Train Epoch: 18 [29568/54000 (55%)] Loss: -195462.156250\n",
      "Train Epoch: 18 [30976/54000 (57%)] Loss: -195898.187500\n",
      "Train Epoch: 18 [32384/54000 (60%)] Loss: -202647.875000\n",
      "Train Epoch: 18 [33792/54000 (63%)] Loss: -187377.000000\n",
      "Train Epoch: 18 [35200/54000 (65%)] Loss: -186452.468750\n",
      "Train Epoch: 18 [36608/54000 (68%)] Loss: -181551.734375\n",
      "Train Epoch: 18 [38016/54000 (70%)] Loss: -195854.031250\n",
      "Train Epoch: 18 [39424/54000 (73%)] Loss: -223307.859375\n",
      "Train Epoch: 18 [40832/54000 (76%)] Loss: -202818.484375\n",
      "Train Epoch: 18 [42240/54000 (78%)] Loss: -199742.500000\n",
      "Train Epoch: 18 [43648/54000 (81%)] Loss: -205054.968750\n",
      "Train Epoch: 18 [45056/54000 (83%)] Loss: -195029.968750\n",
      "Train Epoch: 18 [46464/54000 (86%)] Loss: -199038.593750\n",
      "Train Epoch: 18 [47872/54000 (89%)] Loss: -197432.781250\n",
      "Train Epoch: 18 [49280/54000 (91%)] Loss: -197110.531250\n",
      "Train Epoch: 18 [50688/54000 (94%)] Loss: -204155.375000\n",
      "Train Epoch: 18 [52096/54000 (96%)] Loss: -204503.906250\n",
      "    epoch          : 18\n",
      "    loss           : -200228.8096590909\n",
      "    val_loss       : -209907.2364710366\n",
      "Train Epoch: 19 [0/54000 (0%)] Loss: -222276.375000\n",
      "Train Epoch: 19 [1408/54000 (3%)] Loss: -223367.437500\n",
      "Train Epoch: 19 [2816/54000 (5%)] Loss: -178927.343750\n",
      "Train Epoch: 19 [4224/54000 (8%)] Loss: -186327.828125\n",
      "Train Epoch: 19 [5632/54000 (10%)] Loss: -210870.375000\n",
      "Train Epoch: 19 [7040/54000 (13%)] Loss: -199452.328125\n",
      "Train Epoch: 19 [8448/54000 (16%)] Loss: -197827.031250\n",
      "Train Epoch: 19 [9856/54000 (18%)] Loss: -198614.281250\n",
      "Train Epoch: 19 [11264/54000 (21%)] Loss: -196527.437500\n",
      "Train Epoch: 19 [12672/54000 (23%)] Loss: -196501.312500\n",
      "Train Epoch: 19 [14080/54000 (26%)] Loss: -200640.437500\n",
      "Train Epoch: 19 [15488/54000 (29%)] Loss: -205031.156250\n",
      "Train Epoch: 19 [16896/54000 (31%)] Loss: -202581.656250\n",
      "Train Epoch: 19 [18304/54000 (34%)] Loss: -209027.218750\n",
      "Train Epoch: 19 [19712/54000 (37%)] Loss: -188161.500000\n",
      "Train Epoch: 19 [21120/54000 (39%)] Loss: -186480.640625\n",
      "Train Epoch: 19 [22528/54000 (42%)] Loss: -222621.156250\n",
      "Train Epoch: 19 [23936/54000 (44%)] Loss: -194831.812500\n",
      "Train Epoch: 19 [25344/54000 (47%)] Loss: -198997.906250\n",
      "Train Epoch: 19 [26752/54000 (50%)] Loss: -204999.625000\n",
      "Train Epoch: 19 [28160/54000 (52%)] Loss: -222875.703125\n",
      "Train Epoch: 19 [29568/54000 (55%)] Loss: -199316.375000\n",
      "Train Epoch: 19 [30976/54000 (57%)] Loss: -200213.265625\n",
      "Train Epoch: 19 [32384/54000 (60%)] Loss: -209567.609375\n",
      "Train Epoch: 19 [33792/54000 (63%)] Loss: -203186.625000\n",
      "Train Epoch: 19 [35200/54000 (65%)] Loss: -187700.671875\n",
      "Train Epoch: 19 [36608/54000 (68%)] Loss: -190106.968750\n",
      "Train Epoch: 19 [38016/54000 (70%)] Loss: -187238.312500\n",
      "Train Epoch: 19 [39424/54000 (73%)] Loss: -212040.296875\n",
      "Train Epoch: 19 [40832/54000 (76%)] Loss: -210728.859375\n",
      "Train Epoch: 19 [42240/54000 (78%)] Loss: -201724.750000\n",
      "Train Epoch: 19 [43648/54000 (81%)] Loss: -195417.437500\n",
      "Train Epoch: 19 [45056/54000 (83%)] Loss: -164587.031250\n",
      "Train Epoch: 19 [46464/54000 (86%)] Loss: -196275.000000\n",
      "Train Epoch: 19 [47872/54000 (89%)] Loss: -205314.781250\n",
      "Train Epoch: 19 [49280/54000 (91%)] Loss: -200065.593750\n",
      "Train Epoch: 19 [50688/54000 (94%)] Loss: -203070.406250\n",
      "Train Epoch: 19 [52096/54000 (96%)] Loss: -205753.750000\n",
      "    epoch          : 19\n",
      "    loss           : -200826.08866626795\n",
      "    val_loss       : -207563.75495426828\n",
      "Train Epoch: 20 [0/54000 (0%)] Loss: -195807.281250\n",
      "Train Epoch: 20 [1408/54000 (3%)] Loss: -205633.812500\n",
      "Train Epoch: 20 [2816/54000 (5%)] Loss: -197937.843750\n",
      "Train Epoch: 20 [4224/54000 (8%)] Loss: -200946.656250\n",
      "Train Epoch: 20 [5632/54000 (10%)] Loss: -199369.718750\n",
      "Train Epoch: 20 [7040/54000 (13%)] Loss: -197813.437500\n",
      "Train Epoch: 20 [8448/54000 (16%)] Loss: -202400.281250\n",
      "Train Epoch: 20 [9856/54000 (18%)] Loss: -211615.093750\n",
      "Train Epoch: 20 [11264/54000 (21%)] Loss: -203919.062500\n",
      "Train Epoch: 20 [12672/54000 (23%)] Loss: -204930.156250\n",
      "Train Epoch: 20 [14080/54000 (26%)] Loss: -223351.656250\n",
      "Train Epoch: 20 [15488/54000 (29%)] Loss: -212152.343750\n",
      "Train Epoch: 20 [16896/54000 (31%)] Loss: -196872.406250\n",
      "Train Epoch: 20 [18304/54000 (34%)] Loss: -185004.625000\n",
      "Train Epoch: 20 [19712/54000 (37%)] Loss: -185648.000000\n",
      "Train Epoch: 20 [21120/54000 (39%)] Loss: -188204.656250\n",
      "Train Epoch: 20 [22528/54000 (42%)] Loss: -204895.406250\n",
      "Train Epoch: 20 [23936/54000 (44%)] Loss: -205579.562500\n",
      "Train Epoch: 20 [25344/54000 (47%)] Loss: -189600.937500\n",
      "Train Epoch: 20 [26752/54000 (50%)] Loss: -212986.437500\n",
      "Train Epoch: 20 [28160/54000 (52%)] Loss: -192416.062500\n",
      "Train Epoch: 20 [29568/54000 (55%)] Loss: -224325.921875\n",
      "Train Epoch: 20 [30976/54000 (57%)] Loss: -200481.781250\n",
      "Train Epoch: 20 [32384/54000 (60%)] Loss: -224025.562500\n",
      "Train Epoch: 20 [33792/54000 (63%)] Loss: -198426.468750\n",
      "Train Epoch: 20 [35200/54000 (65%)] Loss: -202883.046875\n",
      "Train Epoch: 20 [36608/54000 (68%)] Loss: -187930.187500\n",
      "Train Epoch: 20 [38016/54000 (70%)] Loss: -203082.031250\n",
      "Train Epoch: 20 [39424/54000 (73%)] Loss: -211065.312500\n",
      "Train Epoch: 20 [40832/54000 (76%)] Loss: -202852.343750\n",
      "Train Epoch: 20 [42240/54000 (78%)] Loss: -202645.046875\n",
      "Train Epoch: 20 [43648/54000 (81%)] Loss: -189365.421875\n",
      "Train Epoch: 20 [45056/54000 (83%)] Loss: -200776.750000\n",
      "Train Epoch: 20 [46464/54000 (86%)] Loss: -223108.750000\n",
      "Train Epoch: 20 [47872/54000 (89%)] Loss: -206145.687500\n",
      "Train Epoch: 20 [49280/54000 (91%)] Loss: -200343.593750\n",
      "Train Epoch: 20 [50688/54000 (94%)] Loss: -223945.562500\n",
      "Train Epoch: 20 [52096/54000 (96%)] Loss: -205255.250000\n",
      "    epoch          : 20\n",
      "    loss           : -201680.6640625\n",
      "    val_loss       : -209244.91577743902\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch20.pth ...\n",
      "Train Epoch: 21 [0/54000 (0%)] Loss: -187999.187500\n",
      "Train Epoch: 21 [1408/54000 (3%)] Loss: -205424.437500\n",
      "Train Epoch: 21 [2816/54000 (5%)] Loss: -202590.437500\n",
      "Train Epoch: 21 [4224/54000 (8%)] Loss: -185520.031250\n",
      "Train Epoch: 21 [5632/54000 (10%)] Loss: -191340.078125\n",
      "Train Epoch: 21 [7040/54000 (13%)] Loss: -191681.375000\n",
      "Train Epoch: 21 [8448/54000 (16%)] Loss: -225141.609375\n",
      "Train Epoch: 21 [9856/54000 (18%)] Loss: -190004.562500\n",
      "Train Epoch: 21 [11264/54000 (21%)] Loss: -191878.375000\n",
      "Train Epoch: 21 [12672/54000 (23%)] Loss: -204166.000000\n",
      "Train Epoch: 21 [14080/54000 (26%)] Loss: -225162.109375\n",
      "Train Epoch: 21 [15488/54000 (29%)] Loss: -198209.562500\n",
      "Train Epoch: 21 [16896/54000 (31%)] Loss: -213223.671875\n",
      "Train Epoch: 21 [18304/54000 (34%)] Loss: -190205.125000\n",
      "Train Epoch: 21 [19712/54000 (37%)] Loss: -205472.375000\n",
      "Train Epoch: 21 [21120/54000 (39%)] Loss: -190270.500000\n",
      "Train Epoch: 21 [22528/54000 (42%)] Loss: -191386.703125\n",
      "Train Epoch: 21 [23936/54000 (44%)] Loss: -190689.250000\n",
      "Train Epoch: 21 [25344/54000 (47%)] Loss: -200412.281250\n",
      "Train Epoch: 21 [26752/54000 (50%)] Loss: -198931.406250\n",
      "Train Epoch: 21 [28160/54000 (52%)] Loss: -199130.968750\n",
      "Train Epoch: 21 [29568/54000 (55%)] Loss: -201178.921875\n",
      "Train Epoch: 21 [30976/54000 (57%)] Loss: -204006.093750\n",
      "Train Epoch: 21 [32384/54000 (60%)] Loss: -225549.593750\n",
      "Train Epoch: 21 [33792/54000 (63%)] Loss: -206893.875000\n",
      "Train Epoch: 21 [35200/54000 (65%)] Loss: -189781.500000\n",
      "Train Epoch: 21 [36608/54000 (68%)] Loss: -190996.296875\n",
      "Train Epoch: 21 [38016/54000 (70%)] Loss: -226187.562500\n",
      "Train Epoch: 21 [39424/54000 (73%)] Loss: -213188.109375\n",
      "Train Epoch: 21 [40832/54000 (76%)] Loss: -212576.968750\n",
      "Train Epoch: 21 [42240/54000 (78%)] Loss: -211945.906250\n",
      "Train Epoch: 21 [43648/54000 (81%)] Loss: -201387.718750\n",
      "Train Epoch: 21 [45056/54000 (83%)] Loss: -225738.484375\n",
      "Train Epoch: 21 [46464/54000 (86%)] Loss: -213674.187500\n",
      "Train Epoch: 21 [47872/54000 (89%)] Loss: -201513.687500\n",
      "Train Epoch: 21 [49280/54000 (91%)] Loss: -200356.750000\n",
      "Train Epoch: 21 [50688/54000 (94%)] Loss: -187631.656250\n",
      "Train Epoch: 21 [52096/54000 (96%)] Loss: -205697.437500\n",
      "    epoch          : 21\n",
      "    loss           : -203117.0716208134\n",
      "    val_loss       : -212996.16082317074\n",
      "Train Epoch: 22 [0/54000 (0%)] Loss: -200737.281250\n",
      "Train Epoch: 22 [1408/54000 (3%)] Loss: -200417.625000\n",
      "Train Epoch: 22 [2816/54000 (5%)] Loss: -197995.687500\n",
      "Train Epoch: 22 [4224/54000 (8%)] Loss: -206695.687500\n",
      "Train Epoch: 22 [5632/54000 (10%)] Loss: -203728.187500\n",
      "Train Epoch: 22 [7040/54000 (13%)] Loss: -201930.078125\n",
      "Train Epoch: 22 [8448/54000 (16%)] Loss: -200366.359375\n",
      "Train Epoch: 22 [9856/54000 (18%)] Loss: -226006.656250\n",
      "Train Epoch: 22 [11264/54000 (21%)] Loss: -207486.562500\n",
      "Train Epoch: 22 [12672/54000 (23%)] Loss: -200718.609375\n",
      "Train Epoch: 22 [14080/54000 (26%)] Loss: -200373.078125\n",
      "Train Epoch: 22 [15488/54000 (29%)] Loss: -206118.187500\n",
      "Train Epoch: 22 [16896/54000 (31%)] Loss: -204555.718750\n",
      "Train Epoch: 22 [18304/54000 (34%)] Loss: -191165.203125\n",
      "Train Epoch: 22 [19712/54000 (37%)] Loss: -203707.406250\n",
      "Train Epoch: 22 [21120/54000 (39%)] Loss: -213009.906250\n",
      "Train Epoch: 22 [22528/54000 (42%)] Loss: -191251.531250\n",
      "Train Epoch: 22 [23936/54000 (44%)] Loss: -190686.375000\n",
      "Train Epoch: 22 [25344/54000 (47%)] Loss: -206329.750000\n",
      "Train Epoch: 22 [26752/54000 (50%)] Loss: -194472.250000\n",
      "Train Epoch: 22 [28160/54000 (52%)] Loss: -192935.687500\n",
      "Train Epoch: 22 [29568/54000 (55%)] Loss: -200094.125000\n",
      "Train Epoch: 22 [30976/54000 (57%)] Loss: -204233.453125\n",
      "Train Epoch: 22 [32384/54000 (60%)] Loss: -206488.281250\n",
      "Train Epoch: 22 [33792/54000 (63%)] Loss: -205921.093750\n",
      "Train Epoch: 22 [35200/54000 (65%)] Loss: -213698.718750\n",
      "Train Epoch: 22 [36608/54000 (68%)] Loss: -213600.718750\n",
      "Train Epoch: 22 [38016/54000 (70%)] Loss: -215336.375000\n",
      "Train Epoch: 22 [39424/54000 (73%)] Loss: -225749.828125\n",
      "Train Epoch: 22 [40832/54000 (76%)] Loss: -190827.156250\n",
      "Train Epoch: 22 [42240/54000 (78%)] Loss: -203764.500000\n",
      "Train Epoch: 22 [43648/54000 (81%)] Loss: -199653.250000\n",
      "Train Epoch: 22 [45056/54000 (83%)] Loss: -201329.093750\n",
      "Train Epoch: 22 [46464/54000 (86%)] Loss: -204596.718750\n",
      "Train Epoch: 22 [47872/54000 (89%)] Loss: -198111.718750\n",
      "Train Epoch: 22 [49280/54000 (91%)] Loss: -206635.062500\n",
      "Train Epoch: 22 [50688/54000 (94%)] Loss: -200479.312500\n",
      "Train Epoch: 22 [52096/54000 (96%)] Loss: -226350.500000\n",
      "    epoch          : 22\n",
      "    loss           : -203966.74020633972\n",
      "    val_loss       : -213820.41825457316\n",
      "Train Epoch: 23 [0/54000 (0%)] Loss: -226891.312500\n",
      "Train Epoch: 23 [1408/54000 (3%)] Loss: -197776.750000\n",
      "Train Epoch: 23 [2816/54000 (5%)] Loss: -191867.875000\n",
      "Train Epoch: 23 [4224/54000 (8%)] Loss: -202680.500000\n",
      "Train Epoch: 23 [5632/54000 (10%)] Loss: -203029.500000\n",
      "Train Epoch: 23 [7040/54000 (13%)] Loss: -205249.609375\n",
      "Train Epoch: 23 [8448/54000 (16%)] Loss: -213213.062500\n",
      "Train Epoch: 23 [9856/54000 (18%)] Loss: -208088.375000\n",
      "Train Epoch: 23 [11264/54000 (21%)] Loss: -212700.750000\n",
      "Train Epoch: 23 [12672/54000 (23%)] Loss: -207090.000000\n",
      "Train Epoch: 23 [14080/54000 (26%)] Loss: -226458.500000\n",
      "Train Epoch: 23 [15488/54000 (29%)] Loss: -200030.218750\n",
      "Train Epoch: 23 [16896/54000 (31%)] Loss: -191183.375000\n",
      "Train Epoch: 23 [18304/54000 (34%)] Loss: -194734.656250\n",
      "Train Epoch: 23 [19712/54000 (37%)] Loss: -227149.031250\n",
      "Train Epoch: 23 [21120/54000 (39%)] Loss: -195073.515625\n",
      "Train Epoch: 23 [22528/54000 (42%)] Loss: -199707.875000\n",
      "Train Epoch: 23 [23936/54000 (44%)] Loss: -202724.000000\n",
      "Train Epoch: 23 [25344/54000 (47%)] Loss: -227504.968750\n",
      "Train Epoch: 23 [26752/54000 (50%)] Loss: -206227.406250\n",
      "Train Epoch: 23 [28160/54000 (52%)] Loss: -202263.125000\n",
      "Train Epoch: 23 [29568/54000 (55%)] Loss: -201142.812500\n",
      "Train Epoch: 23 [30976/54000 (57%)] Loss: -199613.718750\n",
      "Train Epoch: 23 [32384/54000 (60%)] Loss: -201143.046875\n",
      "Train Epoch: 23 [33792/54000 (63%)] Loss: -192077.203125\n",
      "Train Epoch: 23 [35200/54000 (65%)] Loss: -207733.468750\n",
      "Train Epoch: 23 [36608/54000 (68%)] Loss: -226992.562500\n",
      "Train Epoch: 23 [38016/54000 (70%)] Loss: -200746.625000\n",
      "Train Epoch: 23 [39424/54000 (73%)] Loss: -214011.562500\n",
      "Train Epoch: 23 [40832/54000 (76%)] Loss: -206863.437500\n",
      "Train Epoch: 23 [42240/54000 (78%)] Loss: -208403.937500\n",
      "Train Epoch: 23 [43648/54000 (81%)] Loss: -211201.296875\n",
      "Train Epoch: 23 [45056/54000 (83%)] Loss: -200685.250000\n",
      "Train Epoch: 23 [46464/54000 (86%)] Loss: -192017.671875\n",
      "Train Epoch: 23 [47872/54000 (89%)] Loss: -227800.859375\n",
      "Train Epoch: 23 [49280/54000 (91%)] Loss: -200298.625000\n",
      "Train Epoch: 23 [50688/54000 (94%)] Loss: -203314.937500\n",
      "Train Epoch: 23 [52096/54000 (96%)] Loss: -205823.531250\n",
      "    epoch          : 23\n",
      "    loss           : -205424.13359748805\n",
      "    val_loss       : -215255.86566310975\n",
      "Train Epoch: 24 [0/54000 (0%)] Loss: -227777.984375\n",
      "Train Epoch: 24 [1408/54000 (3%)] Loss: -200949.000000\n",
      "Train Epoch: 24 [2816/54000 (5%)] Loss: -194580.984375\n",
      "Train Epoch: 24 [4224/54000 (8%)] Loss: -201245.781250\n",
      "Train Epoch: 24 [5632/54000 (10%)] Loss: -207221.734375\n",
      "Train Epoch: 24 [7040/54000 (13%)] Loss: -195100.578125\n",
      "Train Epoch: 24 [8448/54000 (16%)] Loss: -190074.578125\n",
      "Train Epoch: 24 [9856/54000 (18%)] Loss: -204149.781250\n",
      "Train Epoch: 24 [11264/54000 (21%)] Loss: -196367.859375\n",
      "Train Epoch: 24 [12672/54000 (23%)] Loss: -214046.187500\n",
      "Train Epoch: 24 [14080/54000 (26%)] Loss: -227842.000000\n",
      "Train Epoch: 24 [15488/54000 (29%)] Loss: -204279.437500\n",
      "Train Epoch: 24 [16896/54000 (31%)] Loss: -207431.562500\n",
      "Train Epoch: 24 [18304/54000 (34%)] Loss: -196480.843750\n",
      "Train Epoch: 24 [19712/54000 (37%)] Loss: -202546.359375\n",
      "Train Epoch: 24 [21120/54000 (39%)] Loss: -227198.781250\n",
      "Train Epoch: 24 [22528/54000 (42%)] Loss: -193836.203125\n",
      "Train Epoch: 24 [23936/54000 (44%)] Loss: -193804.437500\n",
      "Train Epoch: 24 [25344/54000 (47%)] Loss: -209453.218750\n",
      "Train Epoch: 24 [26752/54000 (50%)] Loss: -228018.328125\n",
      "Train Epoch: 24 [28160/54000 (52%)] Loss: -214779.218750\n",
      "Train Epoch: 24 [29568/54000 (55%)] Loss: -208857.406250\n",
      "Train Epoch: 24 [30976/54000 (57%)] Loss: -216350.843750\n",
      "Train Epoch: 24 [32384/54000 (60%)] Loss: -216608.281250\n",
      "Train Epoch: 24 [33792/54000 (63%)] Loss: -216188.218750\n",
      "Train Epoch: 24 [35200/54000 (65%)] Loss: -213610.703125\n",
      "Train Epoch: 24 [36608/54000 (68%)] Loss: -196750.593750\n",
      "Train Epoch: 24 [38016/54000 (70%)] Loss: -190779.984375\n",
      "Train Epoch: 24 [39424/54000 (73%)] Loss: -202460.562500\n",
      "Train Epoch: 24 [40832/54000 (76%)] Loss: -193594.968750\n",
      "Train Epoch: 24 [42240/54000 (78%)] Loss: -202744.281250\n",
      "Train Epoch: 24 [43648/54000 (81%)] Loss: -209777.250000\n",
      "Train Epoch: 24 [45056/54000 (83%)] Loss: -202160.468750\n",
      "Train Epoch: 24 [46464/54000 (86%)] Loss: -208158.328125\n",
      "Train Epoch: 24 [47872/54000 (89%)] Loss: -204032.281250\n",
      "Train Epoch: 24 [49280/54000 (91%)] Loss: -202853.609375\n",
      "Train Epoch: 24 [50688/54000 (94%)] Loss: -193931.203125\n",
      "Train Epoch: 24 [52096/54000 (96%)] Loss: -195638.375000\n",
      "    epoch          : 24\n",
      "    loss           : -206475.42280950956\n",
      "    val_loss       : -215919.40777439025\n",
      "Train Epoch: 25 [0/54000 (0%)] Loss: -203922.750000\n",
      "Train Epoch: 25 [1408/54000 (3%)] Loss: -210311.375000\n",
      "Train Epoch: 25 [2816/54000 (5%)] Loss: -194615.312500\n",
      "Train Epoch: 25 [4224/54000 (8%)] Loss: -202610.750000\n",
      "Train Epoch: 25 [5632/54000 (10%)] Loss: -195037.656250\n",
      "Train Epoch: 25 [7040/54000 (13%)] Loss: -215170.812500\n",
      "Train Epoch: 25 [8448/54000 (16%)] Loss: -228352.609375\n",
      "Train Epoch: 25 [9856/54000 (18%)] Loss: -203834.531250\n",
      "Train Epoch: 25 [11264/54000 (21%)] Loss: -211240.218750\n",
      "Train Epoch: 25 [12672/54000 (23%)] Loss: -208371.187500\n",
      "Train Epoch: 25 [14080/54000 (26%)] Loss: -211160.781250\n",
      "Train Epoch: 25 [15488/54000 (29%)] Loss: -204438.875000\n",
      "Train Epoch: 25 [16896/54000 (31%)] Loss: -208440.421875\n",
      "Train Epoch: 25 [18304/54000 (34%)] Loss: -194102.703125\n",
      "Train Epoch: 25 [19712/54000 (37%)] Loss: -193168.750000\n",
      "Train Epoch: 25 [21120/54000 (39%)] Loss: -203733.843750\n",
      "Train Epoch: 25 [22528/54000 (42%)] Loss: -202542.843750\n",
      "Train Epoch: 25 [23936/54000 (44%)] Loss: -204575.000000\n",
      "Train Epoch: 25 [25344/54000 (47%)] Loss: -195452.031250\n",
      "Train Epoch: 25 [26752/54000 (50%)] Loss: -202810.656250\n",
      "Train Epoch: 25 [28160/54000 (52%)] Loss: -203378.093750\n",
      "Train Epoch: 25 [29568/54000 (55%)] Loss: -203606.500000\n",
      "Train Epoch: 25 [30976/54000 (57%)] Loss: -205735.187500\n",
      "Train Epoch: 25 [32384/54000 (60%)] Loss: -212948.390625\n",
      "Train Epoch: 25 [33792/54000 (63%)] Loss: -208116.687500\n",
      "Train Epoch: 25 [35200/54000 (65%)] Loss: -196939.312500\n",
      "Train Epoch: 25 [36608/54000 (68%)] Loss: -228725.218750\n",
      "Train Epoch: 25 [38016/54000 (70%)] Loss: -201719.718750\n",
      "Train Epoch: 25 [39424/54000 (73%)] Loss: -203955.000000\n",
      "Train Epoch: 25 [40832/54000 (76%)] Loss: -217097.687500\n",
      "Train Epoch: 25 [42240/54000 (78%)] Loss: -195444.937500\n",
      "Train Epoch: 25 [43648/54000 (81%)] Loss: -203075.093750\n",
      "Train Epoch: 25 [45056/54000 (83%)] Loss: -204171.781250\n",
      "Train Epoch: 25 [46464/54000 (86%)] Loss: -201354.125000\n",
      "Train Epoch: 25 [47872/54000 (89%)] Loss: -209177.453125\n",
      "Train Epoch: 25 [49280/54000 (91%)] Loss: -203701.812500\n",
      "Train Epoch: 25 [50688/54000 (94%)] Loss: -229511.218750\n",
      "Train Epoch: 25 [52096/54000 (96%)] Loss: -195132.750000\n",
      "    epoch          : 25\n",
      "    loss           : -207418.97745962918\n",
      "    val_loss       : -216905.57945884147\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch25.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 26 [0/54000 (0%)] Loss: -212029.765625\n",
      "Train Epoch: 26 [1408/54000 (3%)] Loss: -209472.437500\n",
      "Train Epoch: 26 [2816/54000 (5%)] Loss: -194265.562500\n",
      "Train Epoch: 26 [4224/54000 (8%)] Loss: -195717.390625\n",
      "Train Epoch: 26 [5632/54000 (10%)] Loss: -195566.546875\n",
      "Train Epoch: 26 [7040/54000 (13%)] Loss: -211152.828125\n",
      "Train Epoch: 26 [8448/54000 (16%)] Loss: -230003.796875\n",
      "Train Epoch: 26 [9856/54000 (18%)] Loss: -202697.906250\n",
      "Train Epoch: 26 [11264/54000 (21%)] Loss: -216076.937500\n",
      "Train Epoch: 26 [12672/54000 (23%)] Loss: -196110.312500\n",
      "Train Epoch: 26 [14080/54000 (26%)] Loss: -210799.812500\n",
      "Train Epoch: 26 [15488/54000 (29%)] Loss: -212578.046875\n",
      "Train Epoch: 26 [16896/54000 (31%)] Loss: -207798.328125\n",
      "Train Epoch: 26 [18304/54000 (34%)] Loss: -205951.062500\n",
      "Train Epoch: 26 [19712/54000 (37%)] Loss: -207264.140625\n",
      "Train Epoch: 26 [21120/54000 (39%)] Loss: -204277.968750\n",
      "Train Epoch: 26 [22528/54000 (42%)] Loss: -204419.875000\n",
      "Train Epoch: 26 [23936/54000 (44%)] Loss: -201928.906250\n",
      "Train Epoch: 26 [25344/54000 (47%)] Loss: -228777.515625\n",
      "Train Epoch: 26 [26752/54000 (50%)] Loss: -195217.781250\n",
      "Train Epoch: 26 [28160/54000 (52%)] Loss: -212005.906250\n",
      "Train Epoch: 26 [29568/54000 (55%)] Loss: -208975.375000\n",
      "Train Epoch: 26 [30976/54000 (57%)] Loss: -206267.437500\n",
      "Train Epoch: 26 [32384/54000 (60%)] Loss: -216513.781250\n",
      "Train Epoch: 26 [33792/54000 (63%)] Loss: -216663.656250\n",
      "Train Epoch: 26 [35200/54000 (65%)] Loss: -196645.812500\n",
      "Train Epoch: 26 [36608/54000 (68%)] Loss: -211780.750000\n",
      "Train Epoch: 26 [38016/54000 (70%)] Loss: -229174.578125\n",
      "Train Epoch: 26 [39424/54000 (73%)] Loss: -202872.828125\n",
      "Train Epoch: 26 [40832/54000 (76%)] Loss: -196514.796875\n",
      "Train Epoch: 26 [42240/54000 (78%)] Loss: -197145.593750\n",
      "Train Epoch: 26 [43648/54000 (81%)] Loss: -204699.093750\n",
      "Train Epoch: 26 [45056/54000 (83%)] Loss: -204909.875000\n",
      "Train Epoch: 26 [46464/54000 (86%)] Loss: -203359.156250\n",
      "Train Epoch: 26 [47872/54000 (89%)] Loss: -204942.437500\n",
      "Train Epoch: 26 [49280/54000 (91%)] Loss: -194185.062500\n",
      "Train Epoch: 26 [50688/54000 (94%)] Loss: -230127.671875\n",
      "Train Epoch: 26 [52096/54000 (96%)] Loss: -212530.468750\n",
      "    epoch          : 26\n",
      "    loss           : -208395.97704844497\n",
      "    val_loss       : -217882.86985518291\n",
      "Train Epoch: 27 [0/54000 (0%)] Loss: -202723.125000\n",
      "Train Epoch: 27 [1408/54000 (3%)] Loss: -229310.625000\n",
      "Train Epoch: 27 [2816/54000 (5%)] Loss: -204012.921875\n",
      "Train Epoch: 27 [4224/54000 (8%)] Loss: -205308.125000\n",
      "Train Epoch: 27 [5632/54000 (10%)] Loss: -204713.359375\n",
      "Train Epoch: 27 [7040/54000 (13%)] Loss: -204039.687500\n",
      "Train Epoch: 27 [8448/54000 (16%)] Loss: -212624.187500\n",
      "Train Epoch: 27 [9856/54000 (18%)] Loss: -211462.468750\n",
      "Train Epoch: 27 [11264/54000 (21%)] Loss: -210635.437500\n",
      "Train Epoch: 27 [12672/54000 (23%)] Loss: -197044.093750\n",
      "Train Epoch: 27 [14080/54000 (26%)] Loss: -213016.531250\n",
      "Train Epoch: 27 [15488/54000 (29%)] Loss: -208979.156250\n",
      "Train Epoch: 27 [16896/54000 (31%)] Loss: -203016.921875\n",
      "Train Epoch: 27 [18304/54000 (34%)] Loss: -195702.750000\n",
      "Train Epoch: 27 [19712/54000 (37%)] Loss: -230173.593750\n",
      "Train Epoch: 27 [21120/54000 (39%)] Loss: -197873.656250\n",
      "Train Epoch: 27 [22528/54000 (42%)] Loss: -217160.562500\n",
      "Train Epoch: 27 [23936/54000 (44%)] Loss: -210936.625000\n",
      "Train Epoch: 27 [25344/54000 (47%)] Loss: -208725.703125\n",
      "Train Epoch: 27 [26752/54000 (50%)] Loss: -230722.453125\n",
      "Train Epoch: 27 [28160/54000 (52%)] Loss: -205429.812500\n",
      "Train Epoch: 27 [29568/54000 (55%)] Loss: -210001.812500\n",
      "Train Epoch: 27 [30976/54000 (57%)] Loss: -230810.265625\n",
      "Train Epoch: 27 [32384/54000 (60%)] Loss: -207450.125000\n",
      "Train Epoch: 27 [33792/54000 (63%)] Loss: -217905.843750\n",
      "Train Epoch: 27 [35200/54000 (65%)] Loss: -217750.640625\n",
      "Train Epoch: 27 [36608/54000 (68%)] Loss: -192646.328125\n",
      "Train Epoch: 27 [38016/54000 (70%)] Loss: -194935.156250\n",
      "Train Epoch: 27 [39424/54000 (73%)] Loss: -229863.531250\n",
      "Train Epoch: 27 [40832/54000 (76%)] Loss: -212443.656250\n",
      "Train Epoch: 27 [42240/54000 (78%)] Loss: -204497.093750\n",
      "Train Epoch: 27 [43648/54000 (81%)] Loss: -231555.000000\n",
      "Train Epoch: 27 [45056/54000 (83%)] Loss: -204009.625000\n",
      "Train Epoch: 27 [46464/54000 (86%)] Loss: -215375.468750\n",
      "Train Epoch: 27 [47872/54000 (89%)] Loss: -204273.531250\n",
      "Train Epoch: 27 [49280/54000 (91%)] Loss: -195355.843750\n",
      "Train Epoch: 27 [50688/54000 (94%)] Loss: -205416.953125\n",
      "Train Epoch: 27 [52096/54000 (96%)] Loss: -211560.687500\n",
      "    epoch          : 27\n",
      "    loss           : -209181.10918809808\n",
      "    val_loss       : -219225.96227134147\n",
      "Train Epoch: 28 [0/54000 (0%)] Loss: -218727.828125\n",
      "Train Epoch: 28 [1408/54000 (3%)] Loss: -218136.875000\n",
      "Train Epoch: 28 [2816/54000 (5%)] Loss: -204007.718750\n",
      "Train Epoch: 28 [4224/54000 (8%)] Loss: -210846.281250\n",
      "Train Epoch: 28 [5632/54000 (10%)] Loss: -205748.281250\n",
      "Train Epoch: 28 [7040/54000 (13%)] Loss: -206611.093750\n",
      "Train Epoch: 28 [8448/54000 (16%)] Loss: -204750.296875\n",
      "Train Epoch: 28 [9856/54000 (18%)] Loss: -198357.031250\n",
      "Train Epoch: 28 [11264/54000 (21%)] Loss: -211801.000000\n",
      "Train Epoch: 28 [12672/54000 (23%)] Loss: -206487.687500\n",
      "Train Epoch: 28 [14080/54000 (26%)] Loss: -231592.015625\n",
      "Train Epoch: 28 [15488/54000 (29%)] Loss: -217715.000000\n",
      "Train Epoch: 28 [16896/54000 (31%)] Loss: -218252.515625\n",
      "Train Epoch: 28 [18304/54000 (34%)] Loss: -230803.875000\n",
      "Train Epoch: 28 [19712/54000 (37%)] Loss: -212268.578125\n",
      "Train Epoch: 28 [21120/54000 (39%)] Loss: -196101.937500\n",
      "Train Epoch: 28 [22528/54000 (42%)] Loss: -202051.968750\n",
      "Train Epoch: 28 [23936/54000 (44%)] Loss: -206352.531250\n",
      "Train Epoch: 28 [25344/54000 (47%)] Loss: -205728.187500\n",
      "Train Epoch: 28 [26752/54000 (50%)] Loss: -203004.843750\n",
      "Train Epoch: 28 [28160/54000 (52%)] Loss: -207889.468750\n",
      "Train Epoch: 28 [29568/54000 (55%)] Loss: -211268.718750\n",
      "Train Epoch: 28 [30976/54000 (57%)] Loss: -230396.218750\n",
      "Train Epoch: 28 [32384/54000 (60%)] Loss: -217702.250000\n",
      "Train Epoch: 28 [33792/54000 (63%)] Loss: -217710.406250\n",
      "Train Epoch: 28 [35200/54000 (65%)] Loss: -231268.593750\n",
      "Train Epoch: 28 [36608/54000 (68%)] Loss: -213449.593750\n",
      "Train Epoch: 28 [38016/54000 (70%)] Loss: -198250.156250\n",
      "Train Epoch: 28 [39424/54000 (73%)] Loss: -218623.906250\n",
      "Train Epoch: 28 [40832/54000 (76%)] Loss: -218086.000000\n",
      "Train Epoch: 28 [42240/54000 (78%)] Loss: -209867.421875\n",
      "Train Epoch: 28 [43648/54000 (81%)] Loss: -231651.843750\n",
      "Train Epoch: 28 [45056/54000 (83%)] Loss: -212460.500000\n",
      "Train Epoch: 28 [46464/54000 (86%)] Loss: -213505.109375\n",
      "Train Epoch: 28 [47872/54000 (89%)] Loss: -204581.968750\n",
      "Train Epoch: 28 [49280/54000 (91%)] Loss: -211743.687500\n",
      "Train Epoch: 28 [50688/54000 (94%)] Loss: -230864.281250\n",
      "Train Epoch: 28 [52096/54000 (96%)] Loss: -213921.093750\n",
      "    epoch          : 28\n",
      "    loss           : -210089.44785436604\n",
      "    val_loss       : -219508.4453125\n",
      "Train Epoch: 29 [0/54000 (0%)] Loss: -207913.625000\n",
      "Train Epoch: 29 [1408/54000 (3%)] Loss: -207048.937500\n",
      "Train Epoch: 29 [2816/54000 (5%)] Loss: -213982.187500\n",
      "Train Epoch: 29 [4224/54000 (8%)] Loss: -211974.750000\n",
      "Train Epoch: 29 [5632/54000 (10%)] Loss: -199639.218750\n",
      "Train Epoch: 29 [7040/54000 (13%)] Loss: -218499.781250\n",
      "Train Epoch: 29 [8448/54000 (16%)] Loss: -217289.328125\n",
      "Train Epoch: 29 [9856/54000 (18%)] Loss: -231081.250000\n",
      "Train Epoch: 29 [11264/54000 (21%)] Loss: -208084.640625\n",
      "Train Epoch: 29 [12672/54000 (23%)] Loss: -206954.703125\n",
      "Train Epoch: 29 [14080/54000 (26%)] Loss: -212150.031250\n",
      "Train Epoch: 29 [15488/54000 (29%)] Loss: -231585.375000\n",
      "Train Epoch: 29 [16896/54000 (31%)] Loss: -232432.625000\n",
      "Train Epoch: 29 [18304/54000 (34%)] Loss: -217430.703125\n",
      "Train Epoch: 29 [19712/54000 (37%)] Loss: -198581.875000\n",
      "Train Epoch: 29 [21120/54000 (39%)] Loss: -231454.531250\n",
      "Train Epoch: 29 [22528/54000 (42%)] Loss: -201473.593750\n",
      "Train Epoch: 29 [23936/54000 (44%)] Loss: -199992.562500\n",
      "Train Epoch: 29 [25344/54000 (47%)] Loss: -199193.781250\n",
      "Train Epoch: 29 [26752/54000 (50%)] Loss: -212015.265625\n",
      "Train Epoch: 29 [28160/54000 (52%)] Loss: -215462.953125\n",
      "Train Epoch: 29 [29568/54000 (55%)] Loss: -213874.140625\n",
      "Train Epoch: 29 [30976/54000 (57%)] Loss: -218703.578125\n",
      "Train Epoch: 29 [32384/54000 (60%)] Loss: -207483.968750\n",
      "Train Epoch: 29 [33792/54000 (63%)] Loss: -218979.875000\n",
      "Train Epoch: 29 [35200/54000 (65%)] Loss: -200637.812500\n",
      "Train Epoch: 29 [36608/54000 (68%)] Loss: -205766.812500\n",
      "Train Epoch: 29 [38016/54000 (70%)] Loss: -198504.484375\n",
      "Train Epoch: 29 [39424/54000 (73%)] Loss: -206821.343750\n",
      "Train Epoch: 29 [40832/54000 (76%)] Loss: -198992.218750\n",
      "Train Epoch: 29 [42240/54000 (78%)] Loss: -202459.843750\n",
      "Train Epoch: 29 [43648/54000 (81%)] Loss: -195664.437500\n",
      "Train Epoch: 29 [45056/54000 (83%)] Loss: -208401.265625\n",
      "Train Epoch: 29 [46464/54000 (86%)] Loss: -205673.875000\n",
      "Train Epoch: 29 [47872/54000 (89%)] Loss: -213440.046875\n",
      "Train Epoch: 29 [49280/54000 (91%)] Loss: -199026.875000\n",
      "Train Epoch: 29 [50688/54000 (94%)] Loss: -232587.687500\n",
      "Train Epoch: 29 [52096/54000 (96%)] Loss: -207474.640625\n",
      "    epoch          : 29\n",
      "    loss           : -210616.84610496412\n",
      "    val_loss       : -220346.62157012196\n",
      "Train Epoch: 30 [0/54000 (0%)] Loss: -214021.875000\n",
      "Train Epoch: 30 [1408/54000 (3%)] Loss: -201845.000000\n",
      "Train Epoch: 30 [2816/54000 (5%)] Loss: -204699.265625\n",
      "Train Epoch: 30 [4224/54000 (8%)] Loss: -213424.625000\n",
      "Train Epoch: 30 [5632/54000 (10%)] Loss: -208257.093750\n",
      "Train Epoch: 30 [7040/54000 (13%)] Loss: -207191.937500\n",
      "Train Epoch: 30 [8448/54000 (16%)] Loss: -207441.500000\n",
      "Train Epoch: 30 [9856/54000 (18%)] Loss: -214424.375000\n",
      "Train Epoch: 30 [11264/54000 (21%)] Loss: -196714.750000\n",
      "Train Epoch: 30 [12672/54000 (23%)] Loss: -197293.812500\n",
      "Train Epoch: 30 [14080/54000 (26%)] Loss: -202118.593750\n",
      "Train Epoch: 30 [15488/54000 (29%)] Loss: -197190.187500\n",
      "Train Epoch: 30 [16896/54000 (31%)] Loss: -205698.312500\n",
      "Train Epoch: 30 [18304/54000 (34%)] Loss: -206731.375000\n",
      "Train Epoch: 30 [19712/54000 (37%)] Loss: -214691.406250\n",
      "Train Epoch: 30 [21120/54000 (39%)] Loss: -204895.406250\n",
      "Train Epoch: 30 [22528/54000 (42%)] Loss: -220144.812500\n",
      "Train Epoch: 30 [23936/54000 (44%)] Loss: -213914.875000\n",
      "Train Epoch: 30 [25344/54000 (47%)] Loss: -206548.171875\n",
      "Train Epoch: 30 [26752/54000 (50%)] Loss: -211624.968750\n",
      "Train Epoch: 30 [28160/54000 (52%)] Loss: -199325.234375\n",
      "Train Epoch: 30 [29568/54000 (55%)] Loss: -199563.750000\n",
      "Train Epoch: 30 [30976/54000 (57%)] Loss: -205382.531250\n",
      "Train Epoch: 30 [32384/54000 (60%)] Loss: -208014.859375\n",
      "Train Epoch: 30 [33792/54000 (63%)] Loss: -208506.765625\n",
      "Train Epoch: 30 [35200/54000 (65%)] Loss: -214535.296875\n",
      "Train Epoch: 30 [36608/54000 (68%)] Loss: -209375.000000\n",
      "Train Epoch: 30 [38016/54000 (70%)] Loss: -232500.000000\n",
      "Train Epoch: 30 [39424/54000 (73%)] Loss: -221719.781250\n",
      "Train Epoch: 30 [40832/54000 (76%)] Loss: -204825.093750\n",
      "Train Epoch: 30 [42240/54000 (78%)] Loss: -213245.156250\n",
      "Train Epoch: 30 [43648/54000 (81%)] Loss: -206079.062500\n",
      "Train Epoch: 30 [45056/54000 (83%)] Loss: -205759.500000\n",
      "Train Epoch: 30 [46464/54000 (86%)] Loss: -202041.687500\n",
      "Train Epoch: 30 [47872/54000 (89%)] Loss: -198544.187500\n",
      "Train Epoch: 30 [49280/54000 (91%)] Loss: -200338.031250\n",
      "Train Epoch: 30 [50688/54000 (94%)] Loss: -200437.140625\n",
      "Train Epoch: 30 [52096/54000 (96%)] Loss: -211705.875000\n",
      "    epoch          : 30\n",
      "    loss           : -211620.1640625\n",
      "    val_loss       : -220855.9959984756\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch30.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 31 [0/54000 (0%)] Loss: -234145.437500\n",
      "Train Epoch: 31 [1408/54000 (3%)] Loss: -212668.062500\n",
      "Train Epoch: 31 [2816/54000 (5%)] Loss: -220655.062500\n",
      "Train Epoch: 31 [4224/54000 (8%)] Loss: -221446.796875\n",
      "Train Epoch: 31 [5632/54000 (10%)] Loss: -215515.875000\n",
      "Train Epoch: 31 [7040/54000 (13%)] Loss: -208562.843750\n",
      "Train Epoch: 31 [8448/54000 (16%)] Loss: -209848.234375\n",
      "Train Epoch: 31 [9856/54000 (18%)] Loss: -198215.281250\n",
      "Train Epoch: 31 [11264/54000 (21%)] Loss: -208590.406250\n",
      "Train Epoch: 31 [12672/54000 (23%)] Loss: -215452.500000\n",
      "Train Epoch: 31 [14080/54000 (26%)] Loss: -233612.046875\n",
      "Train Epoch: 31 [15488/54000 (29%)] Loss: -216779.328125\n",
      "Train Epoch: 31 [16896/54000 (31%)] Loss: -219676.359375\n",
      "Train Epoch: 31 [18304/54000 (34%)] Loss: -207546.781250\n",
      "Train Epoch: 31 [19712/54000 (37%)] Loss: -211979.406250\n",
      "Train Epoch: 31 [21120/54000 (39%)] Loss: -233407.718750\n",
      "Train Epoch: 31 [22528/54000 (42%)] Loss: -208591.812500\n",
      "Train Epoch: 31 [23936/54000 (44%)] Loss: -208221.906250\n",
      "Train Epoch: 31 [25344/54000 (47%)] Loss: -206743.968750\n",
      "Train Epoch: 31 [26752/54000 (50%)] Loss: -233814.453125\n",
      "Train Epoch: 31 [28160/54000 (52%)] Loss: -201898.593750\n",
      "Train Epoch: 31 [29568/54000 (55%)] Loss: -207821.890625\n",
      "Train Epoch: 31 [30976/54000 (57%)] Loss: -214042.828125\n",
      "Train Epoch: 31 [32384/54000 (60%)] Loss: -212995.406250\n",
      "Train Epoch: 31 [33792/54000 (63%)] Loss: -233573.718750\n",
      "Train Epoch: 31 [35200/54000 (65%)] Loss: -205565.625000\n",
      "Train Epoch: 31 [36608/54000 (68%)] Loss: -202519.687500\n",
      "Train Epoch: 31 [38016/54000 (70%)] Loss: -208153.218750\n",
      "Train Epoch: 31 [39424/54000 (73%)] Loss: -199890.687500\n",
      "Train Epoch: 31 [40832/54000 (76%)] Loss: -208624.953125\n",
      "Train Epoch: 31 [42240/54000 (78%)] Loss: -209110.781250\n",
      "Train Epoch: 31 [43648/54000 (81%)] Loss: -201543.312500\n",
      "Train Epoch: 31 [45056/54000 (83%)] Loss: -207175.781250\n",
      "Train Epoch: 31 [46464/54000 (86%)] Loss: -208741.765625\n",
      "Train Epoch: 31 [47872/54000 (89%)] Loss: -206539.296875\n",
      "Train Epoch: 31 [49280/54000 (91%)] Loss: -209237.203125\n",
      "Train Epoch: 31 [50688/54000 (94%)] Loss: -234020.875000\n",
      "Train Epoch: 31 [52096/54000 (96%)] Loss: -214032.140625\n",
      "    epoch          : 31\n",
      "    loss           : -212254.52597936604\n",
      "    val_loss       : -221693.4237804878\n",
      "Train Epoch: 32 [0/54000 (0%)] Loss: -213981.281250\n",
      "Train Epoch: 32 [1408/54000 (3%)] Loss: -208840.656250\n",
      "Train Epoch: 32 [2816/54000 (5%)] Loss: -208245.765625\n",
      "Train Epoch: 32 [4224/54000 (8%)] Loss: -223002.781250\n",
      "Train Epoch: 32 [5632/54000 (10%)] Loss: -221011.562500\n",
      "Train Epoch: 32 [7040/54000 (13%)] Loss: -207777.312500\n",
      "Train Epoch: 32 [8448/54000 (16%)] Loss: -215232.468750\n",
      "Train Epoch: 32 [9856/54000 (18%)] Loss: -200030.250000\n",
      "Train Epoch: 32 [11264/54000 (21%)] Loss: -204489.281250\n",
      "Train Epoch: 32 [12672/54000 (23%)] Loss: -217716.062500\n",
      "Train Epoch: 32 [14080/54000 (26%)] Loss: -234340.093750\n",
      "Train Epoch: 32 [15488/54000 (29%)] Loss: -209462.125000\n",
      "Train Epoch: 32 [16896/54000 (31%)] Loss: -220732.984375\n",
      "Train Epoch: 32 [18304/54000 (34%)] Loss: -207182.156250\n",
      "Train Epoch: 32 [19712/54000 (37%)] Loss: -208834.937500\n",
      "Train Epoch: 32 [21120/54000 (39%)] Loss: -203228.937500\n",
      "Train Epoch: 32 [22528/54000 (42%)] Loss: -202072.828125\n",
      "Train Epoch: 32 [23936/54000 (44%)] Loss: -207256.000000\n",
      "Train Epoch: 32 [25344/54000 (47%)] Loss: -202337.531250\n",
      "Train Epoch: 32 [26752/54000 (50%)] Loss: -210159.078125\n",
      "Train Epoch: 32 [28160/54000 (52%)] Loss: -200346.031250\n",
      "Train Epoch: 32 [29568/54000 (55%)] Loss: -210382.312500\n",
      "Train Epoch: 32 [30976/54000 (57%)] Loss: -209669.562500\n",
      "Train Epoch: 32 [32384/54000 (60%)] Loss: -215644.593750\n",
      "Train Epoch: 32 [33792/54000 (63%)] Loss: -207728.562500\n",
      "Train Epoch: 32 [35200/54000 (65%)] Loss: -221989.000000\n",
      "Train Epoch: 32 [36608/54000 (68%)] Loss: -219946.484375\n",
      "Train Epoch: 32 [38016/54000 (70%)] Loss: -201515.015625\n",
      "Train Epoch: 32 [39424/54000 (73%)] Loss: -209825.062500\n",
      "Train Epoch: 32 [40832/54000 (76%)] Loss: -212862.187500\n",
      "Train Epoch: 32 [42240/54000 (78%)] Loss: -233565.109375\n",
      "Train Epoch: 32 [43648/54000 (81%)] Loss: -215890.562500\n",
      "Train Epoch: 32 [45056/54000 (83%)] Loss: -208958.625000\n",
      "Train Epoch: 32 [46464/54000 (86%)] Loss: -213731.546875\n",
      "Train Epoch: 32 [47872/54000 (89%)] Loss: -208255.593750\n",
      "Train Epoch: 32 [49280/54000 (91%)] Loss: -200592.218750\n",
      "Train Epoch: 32 [50688/54000 (94%)] Loss: -207476.562500\n",
      "Train Epoch: 32 [52096/54000 (96%)] Loss: -200310.031250\n",
      "    epoch          : 32\n",
      "    loss           : -212969.31664922248\n",
      "    val_loss       : -222176.01333841463\n",
      "Train Epoch: 33 [0/54000 (0%)] Loss: -234420.734375\n",
      "Train Epoch: 33 [1408/54000 (3%)] Loss: -214181.406250\n",
      "Train Epoch: 33 [2816/54000 (5%)] Loss: -210989.531250\n",
      "Train Epoch: 33 [4224/54000 (8%)] Loss: -220859.843750\n",
      "Train Epoch: 33 [5632/54000 (10%)] Loss: -202575.375000\n",
      "Train Epoch: 33 [7040/54000 (13%)] Loss: -216348.515625\n",
      "Train Epoch: 33 [8448/54000 (16%)] Loss: -203269.718750\n",
      "Train Epoch: 33 [9856/54000 (18%)] Loss: -207391.187500\n",
      "Train Epoch: 33 [11264/54000 (21%)] Loss: -208425.687500\n",
      "Train Epoch: 33 [12672/54000 (23%)] Loss: -234149.421875\n",
      "Train Epoch: 33 [14080/54000 (26%)] Loss: -207718.109375\n",
      "Train Epoch: 33 [15488/54000 (29%)] Loss: -214891.125000\n",
      "Train Epoch: 33 [16896/54000 (31%)] Loss: -234799.500000\n",
      "Train Epoch: 33 [18304/54000 (34%)] Loss: -203615.000000\n",
      "Train Epoch: 33 [19712/54000 (37%)] Loss: -216201.312500\n",
      "Train Epoch: 33 [21120/54000 (39%)] Loss: -201372.140625\n",
      "Train Epoch: 33 [22528/54000 (42%)] Loss: -201528.593750\n",
      "Train Epoch: 33 [23936/54000 (44%)] Loss: -204989.156250\n",
      "Train Epoch: 33 [25344/54000 (47%)] Loss: -233268.906250\n",
      "Train Epoch: 33 [26752/54000 (50%)] Loss: -211325.218750\n",
      "Train Epoch: 33 [28160/54000 (52%)] Loss: -207750.625000\n",
      "Train Epoch: 33 [29568/54000 (55%)] Loss: -233209.968750\n",
      "Train Epoch: 33 [30976/54000 (57%)] Loss: -216791.390625\n",
      "Train Epoch: 33 [32384/54000 (60%)] Loss: -212613.859375\n",
      "Train Epoch: 33 [33792/54000 (63%)] Loss: -213475.562500\n",
      "Train Epoch: 33 [35200/54000 (65%)] Loss: -215795.453125\n",
      "Train Epoch: 33 [36608/54000 (68%)] Loss: -199173.187500\n",
      "Train Epoch: 33 [38016/54000 (70%)] Loss: -221649.250000\n",
      "Train Epoch: 33 [39424/54000 (73%)] Loss: -217497.656250\n",
      "Train Epoch: 33 [40832/54000 (76%)] Loss: -220432.390625\n",
      "Train Epoch: 33 [42240/54000 (78%)] Loss: -216434.062500\n",
      "Train Epoch: 33 [43648/54000 (81%)] Loss: -214377.375000\n",
      "Train Epoch: 33 [45056/54000 (83%)] Loss: -211095.046875\n",
      "Train Epoch: 33 [46464/54000 (86%)] Loss: -210695.437500\n",
      "Train Epoch: 33 [47872/54000 (89%)] Loss: -208579.265625\n",
      "Train Epoch: 33 [49280/54000 (91%)] Loss: -207340.468750\n",
      "Train Epoch: 33 [50688/54000 (94%)] Loss: -217725.343750\n",
      "Train Epoch: 33 [52096/54000 (96%)] Loss: -233577.750000\n",
      "    epoch          : 33\n",
      "    loss           : -213541.40935257176\n",
      "    val_loss       : -222708.82660060975\n",
      "Train Epoch: 34 [0/54000 (0%)] Loss: -210449.250000\n",
      "Train Epoch: 34 [1408/54000 (3%)] Loss: -234341.843750\n",
      "Train Epoch: 34 [2816/54000 (5%)] Loss: -234990.718750\n",
      "Train Epoch: 34 [4224/54000 (8%)] Loss: -202107.625000\n",
      "Train Epoch: 34 [5632/54000 (10%)] Loss: -199820.218750\n",
      "Train Epoch: 34 [7040/54000 (13%)] Loss: -214845.640625\n",
      "Train Epoch: 34 [8448/54000 (16%)] Loss: -205032.687500\n",
      "Train Epoch: 34 [9856/54000 (18%)] Loss: -208248.531250\n",
      "Train Epoch: 34 [11264/54000 (21%)] Loss: -211227.671875\n",
      "Train Epoch: 34 [12672/54000 (23%)] Loss: -221000.500000\n",
      "Train Epoch: 34 [14080/54000 (26%)] Loss: -211489.078125\n",
      "Train Epoch: 34 [15488/54000 (29%)] Loss: -234720.343750\n",
      "Train Epoch: 34 [16896/54000 (31%)] Loss: -214408.343750\n",
      "Train Epoch: 34 [18304/54000 (34%)] Loss: -211003.406250\n",
      "Train Epoch: 34 [19712/54000 (37%)] Loss: -210508.953125\n",
      "Train Epoch: 34 [21120/54000 (39%)] Loss: -210896.812500\n",
      "Train Epoch: 34 [22528/54000 (42%)] Loss: -204213.531250\n",
      "Train Epoch: 34 [23936/54000 (44%)] Loss: -211447.562500\n",
      "Train Epoch: 34 [25344/54000 (47%)] Loss: -235330.515625\n",
      "Train Epoch: 34 [26752/54000 (50%)] Loss: -221636.531250\n",
      "Train Epoch: 34 [28160/54000 (52%)] Loss: -218286.609375\n",
      "Train Epoch: 34 [29568/54000 (55%)] Loss: -215580.281250\n",
      "Train Epoch: 34 [30976/54000 (57%)] Loss: -216979.921875\n",
      "Train Epoch: 34 [32384/54000 (60%)] Loss: -203474.468750\n",
      "Train Epoch: 34 [33792/54000 (63%)] Loss: -234662.906250\n",
      "Train Epoch: 34 [35200/54000 (65%)] Loss: -206746.687500\n",
      "Train Epoch: 34 [36608/54000 (68%)] Loss: -205348.937500\n",
      "Train Epoch: 34 [38016/54000 (70%)] Loss: -209097.031250\n",
      "Train Epoch: 34 [39424/54000 (73%)] Loss: -207937.156250\n",
      "Train Epoch: 34 [40832/54000 (76%)] Loss: -221915.015625\n",
      "Train Epoch: 34 [42240/54000 (78%)] Loss: -211025.046875\n",
      "Train Epoch: 34 [43648/54000 (81%)] Loss: -217433.531250\n",
      "Train Epoch: 34 [45056/54000 (83%)] Loss: -207935.671875\n",
      "Train Epoch: 34 [46464/54000 (86%)] Loss: -208291.218750\n",
      "Train Epoch: 34 [47872/54000 (89%)] Loss: -214429.093750\n",
      "Train Epoch: 34 [49280/54000 (91%)] Loss: -211383.718750\n",
      "Train Epoch: 34 [50688/54000 (94%)] Loss: -211358.687500\n",
      "Train Epoch: 34 [52096/54000 (96%)] Loss: -203562.078125\n",
      "    epoch          : 34\n",
      "    loss           : -214034.11516895934\n",
      "    val_loss       : -223071.0234375\n",
      "Train Epoch: 35 [0/54000 (0%)] Loss: -209579.953125\n",
      "Train Epoch: 35 [1408/54000 (3%)] Loss: -215578.625000\n",
      "Train Epoch: 35 [2816/54000 (5%)] Loss: -218506.093750\n",
      "Train Epoch: 35 [4224/54000 (8%)] Loss: -209353.750000\n",
      "Train Epoch: 35 [5632/54000 (10%)] Loss: -203263.343750\n",
      "Train Epoch: 35 [7040/54000 (13%)] Loss: -210378.484375\n",
      "Train Epoch: 35 [8448/54000 (16%)] Loss: -212599.750000\n",
      "Train Epoch: 35 [9856/54000 (18%)] Loss: -210509.312500\n",
      "Train Epoch: 35 [11264/54000 (21%)] Loss: -204618.015625\n",
      "Train Epoch: 35 [12672/54000 (23%)] Loss: -215485.843750\n",
      "Train Epoch: 35 [14080/54000 (26%)] Loss: -216501.015625\n",
      "Train Epoch: 35 [15488/54000 (29%)] Loss: -235100.750000\n",
      "Train Epoch: 35 [16896/54000 (31%)] Loss: -210804.125000\n",
      "Train Epoch: 35 [18304/54000 (34%)] Loss: -195724.984375\n",
      "Train Epoch: 35 [19712/54000 (37%)] Loss: -235210.953125\n",
      "Train Epoch: 35 [21120/54000 (39%)] Loss: -203355.171875\n",
      "Train Epoch: 35 [22528/54000 (42%)] Loss: -205517.000000\n",
      "Train Epoch: 35 [23936/54000 (44%)] Loss: -215280.750000\n",
      "Train Epoch: 35 [25344/54000 (47%)] Loss: -216904.125000\n",
      "Train Epoch: 35 [26752/54000 (50%)] Loss: -235345.250000\n",
      "Train Epoch: 35 [28160/54000 (52%)] Loss: -235998.281250\n",
      "Train Epoch: 35 [29568/54000 (55%)] Loss: -206096.062500\n",
      "Train Epoch: 35 [30976/54000 (57%)] Loss: -217605.500000\n",
      "Train Epoch: 35 [32384/54000 (60%)] Loss: -209849.093750\n",
      "Train Epoch: 35 [33792/54000 (63%)] Loss: -210430.921875\n",
      "Train Epoch: 35 [35200/54000 (65%)] Loss: -202995.437500\n",
      "Train Epoch: 35 [36608/54000 (68%)] Loss: -211774.031250\n",
      "Train Epoch: 35 [38016/54000 (70%)] Loss: -235138.781250\n",
      "Train Epoch: 35 [39424/54000 (73%)] Loss: -201993.281250\n",
      "Train Epoch: 35 [40832/54000 (76%)] Loss: -218108.343750\n",
      "Train Epoch: 35 [42240/54000 (78%)] Loss: -222059.125000\n",
      "Train Epoch: 35 [43648/54000 (81%)] Loss: -218335.078125\n",
      "Train Epoch: 35 [45056/54000 (83%)] Loss: -215179.531250\n",
      "Train Epoch: 35 [46464/54000 (86%)] Loss: -210969.484375\n",
      "Train Epoch: 35 [47872/54000 (89%)] Loss: -204819.812500\n",
      "Train Epoch: 35 [49280/54000 (91%)] Loss: -210351.812500\n",
      "Train Epoch: 35 [50688/54000 (94%)] Loss: -208249.906250\n",
      "Train Epoch: 35 [52096/54000 (96%)] Loss: -211584.859375\n",
      "    epoch          : 35\n",
      "    loss           : -214541.69441537082\n",
      "    val_loss       : -223678.13891006098\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch35.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 36 [0/54000 (0%)] Loss: -218864.875000\n",
      "Train Epoch: 36 [1408/54000 (3%)] Loss: -217382.843750\n",
      "Train Epoch: 36 [2816/54000 (5%)] Loss: -223603.656250\n",
      "Train Epoch: 36 [4224/54000 (8%)] Loss: -213035.078125\n",
      "Train Epoch: 36 [5632/54000 (10%)] Loss: -210282.031250\n",
      "Train Epoch: 36 [7040/54000 (13%)] Loss: -203704.812500\n",
      "Train Epoch: 36 [8448/54000 (16%)] Loss: -223420.953125\n",
      "Train Epoch: 36 [9856/54000 (18%)] Loss: -202959.812500\n",
      "Train Epoch: 36 [11264/54000 (21%)] Loss: -200984.281250\n",
      "Train Epoch: 36 [12672/54000 (23%)] Loss: -219027.703125\n",
      "Train Epoch: 36 [14080/54000 (26%)] Loss: -207666.781250\n",
      "Train Epoch: 36 [15488/54000 (29%)] Loss: -204566.500000\n",
      "Train Epoch: 36 [16896/54000 (31%)] Loss: -223514.000000\n",
      "Train Epoch: 36 [18304/54000 (34%)] Loss: -221106.218750\n",
      "Train Epoch: 36 [19712/54000 (37%)] Loss: -218419.890625\n",
      "Train Epoch: 36 [21120/54000 (39%)] Loss: -235368.187500\n",
      "Train Epoch: 36 [22528/54000 (42%)] Loss: -219072.500000\n",
      "Train Epoch: 36 [23936/54000 (44%)] Loss: -212696.406250\n",
      "Train Epoch: 36 [25344/54000 (47%)] Loss: -204025.250000\n",
      "Train Epoch: 36 [26752/54000 (50%)] Loss: -197729.750000\n",
      "Train Epoch: 36 [28160/54000 (52%)] Loss: -203676.046875\n",
      "Train Epoch: 36 [29568/54000 (55%)] Loss: -202192.187500\n",
      "Train Epoch: 36 [30976/54000 (57%)] Loss: -212205.250000\n",
      "Train Epoch: 36 [32384/54000 (60%)] Loss: -221896.687500\n",
      "Train Epoch: 36 [33792/54000 (63%)] Loss: -223156.375000\n",
      "Train Epoch: 36 [35200/54000 (65%)] Loss: -218589.375000\n",
      "Train Epoch: 36 [36608/54000 (68%)] Loss: -212816.609375\n",
      "Train Epoch: 36 [38016/54000 (70%)] Loss: -235881.796875\n",
      "Train Epoch: 36 [39424/54000 (73%)] Loss: -202858.593750\n",
      "Train Epoch: 36 [40832/54000 (76%)] Loss: -204366.062500\n",
      "Train Epoch: 36 [42240/54000 (78%)] Loss: -206372.593750\n",
      "Train Epoch: 36 [43648/54000 (81%)] Loss: -235735.312500\n",
      "Train Epoch: 36 [45056/54000 (83%)] Loss: -210410.718750\n",
      "Train Epoch: 36 [46464/54000 (86%)] Loss: -209326.875000\n",
      "Train Epoch: 36 [47872/54000 (89%)] Loss: -211220.906250\n",
      "Train Epoch: 36 [49280/54000 (91%)] Loss: -236242.734375\n",
      "Train Epoch: 36 [50688/54000 (94%)] Loss: -218834.968750\n",
      "Train Epoch: 36 [52096/54000 (96%)] Loss: -210501.781250\n",
      "    epoch          : 36\n",
      "    loss           : -214778.5933388158\n",
      "    val_loss       : -224080.3485137195\n",
      "Train Epoch: 37 [0/54000 (0%)] Loss: -211535.140625\n",
      "Train Epoch: 37 [1408/54000 (3%)] Loss: -210570.250000\n",
      "Train Epoch: 37 [2816/54000 (5%)] Loss: -236650.687500\n",
      "Train Epoch: 37 [4224/54000 (8%)] Loss: -210954.843750\n",
      "Train Epoch: 37 [5632/54000 (10%)] Loss: -205866.843750\n",
      "Train Epoch: 37 [7040/54000 (13%)] Loss: -216675.187500\n",
      "Train Epoch: 37 [8448/54000 (16%)] Loss: -205416.593750\n",
      "Train Epoch: 37 [9856/54000 (18%)] Loss: -236338.156250\n",
      "Train Epoch: 37 [11264/54000 (21%)] Loss: -212028.625000\n",
      "Train Epoch: 37 [12672/54000 (23%)] Loss: -211137.203125\n",
      "Train Epoch: 37 [14080/54000 (26%)] Loss: -212942.265625\n",
      "Train Epoch: 37 [15488/54000 (29%)] Loss: -236351.968750\n",
      "Train Epoch: 37 [16896/54000 (31%)] Loss: -212138.296875\n",
      "Train Epoch: 37 [18304/54000 (34%)] Loss: -218780.406250\n",
      "Train Epoch: 37 [19712/54000 (37%)] Loss: -202029.437500\n",
      "Train Epoch: 37 [21120/54000 (39%)] Loss: -215173.000000\n",
      "Train Epoch: 37 [22528/54000 (42%)] Loss: -236085.921875\n",
      "Train Epoch: 37 [23936/54000 (44%)] Loss: -218051.625000\n",
      "Train Epoch: 37 [25344/54000 (47%)] Loss: -206555.796875\n",
      "Train Epoch: 37 [26752/54000 (50%)] Loss: -205318.343750\n",
      "Train Epoch: 37 [28160/54000 (52%)] Loss: -210868.921875\n",
      "Train Epoch: 37 [29568/54000 (55%)] Loss: -209965.125000\n",
      "Train Epoch: 37 [30976/54000 (57%)] Loss: -204150.390625\n",
      "Train Epoch: 37 [32384/54000 (60%)] Loss: -209210.437500\n",
      "Train Epoch: 37 [33792/54000 (63%)] Loss: -215140.468750\n",
      "Train Epoch: 37 [35200/54000 (65%)] Loss: -236707.156250\n",
      "Train Epoch: 37 [36608/54000 (68%)] Loss: -217537.156250\n",
      "Train Epoch: 37 [38016/54000 (70%)] Loss: -221787.875000\n",
      "Train Epoch: 37 [39424/54000 (73%)] Loss: -214572.531250\n",
      "Train Epoch: 37 [40832/54000 (76%)] Loss: -217493.375000\n",
      "Train Epoch: 37 [42240/54000 (78%)] Loss: -212052.906250\n",
      "Train Epoch: 37 [43648/54000 (81%)] Loss: -216572.750000\n",
      "Train Epoch: 37 [45056/54000 (83%)] Loss: -210405.312500\n",
      "Train Epoch: 37 [46464/54000 (86%)] Loss: -236400.421875\n",
      "Train Epoch: 37 [47872/54000 (89%)] Loss: -207173.109375\n",
      "Train Epoch: 37 [49280/54000 (91%)] Loss: -212104.000000\n",
      "Train Epoch: 37 [50688/54000 (94%)] Loss: -218560.187500\n",
      "Train Epoch: 37 [52096/54000 (96%)] Loss: -237136.125000\n",
      "    epoch          : 37\n",
      "    loss           : -215413.7738486842\n",
      "    val_loss       : -224406.48990091463\n",
      "Train Epoch: 38 [0/54000 (0%)] Loss: -236269.312500\n",
      "Train Epoch: 38 [1408/54000 (3%)] Loss: -212146.140625\n",
      "Train Epoch: 38 [2816/54000 (5%)] Loss: -211971.906250\n",
      "Train Epoch: 38 [4224/54000 (8%)] Loss: -210050.562500\n",
      "Train Epoch: 38 [5632/54000 (10%)] Loss: -210946.781250\n",
      "Train Epoch: 38 [7040/54000 (13%)] Loss: -223150.562500\n",
      "Train Epoch: 38 [8448/54000 (16%)] Loss: -237276.296875\n",
      "Train Epoch: 38 [9856/54000 (18%)] Loss: -223020.531250\n",
      "Train Epoch: 38 [11264/54000 (21%)] Loss: -205311.750000\n",
      "Train Epoch: 38 [12672/54000 (23%)] Loss: -203142.468750\n",
      "Train Epoch: 38 [14080/54000 (26%)] Loss: -206496.500000\n",
      "Train Epoch: 38 [15488/54000 (29%)] Loss: -211085.015625\n",
      "Train Epoch: 38 [16896/54000 (31%)] Loss: -209583.406250\n",
      "Train Epoch: 38 [18304/54000 (34%)] Loss: -219117.984375\n",
      "Train Epoch: 38 [19712/54000 (37%)] Loss: -209265.703125\n",
      "Train Epoch: 38 [21120/54000 (39%)] Loss: -221935.796875\n",
      "Train Epoch: 38 [22528/54000 (42%)] Loss: -209320.000000\n",
      "Train Epoch: 38 [23936/54000 (44%)] Loss: -220310.156250\n",
      "Train Epoch: 38 [25344/54000 (47%)] Loss: -211871.937500\n",
      "Train Epoch: 38 [26752/54000 (50%)] Loss: -204480.453125\n",
      "Train Epoch: 38 [28160/54000 (52%)] Loss: -209521.031250\n",
      "Train Epoch: 38 [29568/54000 (55%)] Loss: -212206.437500\n",
      "Train Epoch: 38 [30976/54000 (57%)] Loss: -211336.500000\n",
      "Train Epoch: 38 [32384/54000 (60%)] Loss: -237558.656250\n",
      "Train Epoch: 38 [33792/54000 (63%)] Loss: -219721.906250\n",
      "Train Epoch: 38 [35200/54000 (65%)] Loss: -218203.500000\n",
      "Train Epoch: 38 [36608/54000 (68%)] Loss: -217871.375000\n",
      "Train Epoch: 38 [38016/54000 (70%)] Loss: -225290.593750\n",
      "Train Epoch: 38 [39424/54000 (73%)] Loss: -236048.546875\n",
      "Train Epoch: 38 [40832/54000 (76%)] Loss: -202352.718750\n",
      "Train Epoch: 38 [42240/54000 (78%)] Loss: -210199.875000\n",
      "Train Epoch: 38 [43648/54000 (81%)] Loss: -236835.156250\n",
      "Train Epoch: 38 [45056/54000 (83%)] Loss: -236290.156250\n",
      "Train Epoch: 38 [46464/54000 (86%)] Loss: -202879.500000\n",
      "Train Epoch: 38 [47872/54000 (89%)] Loss: -205676.484375\n",
      "Train Epoch: 38 [49280/54000 (91%)] Loss: -211360.125000\n",
      "Train Epoch: 38 [50688/54000 (94%)] Loss: -237354.687500\n",
      "Train Epoch: 38 [52096/54000 (96%)] Loss: -207309.250000\n",
      "    epoch          : 38\n",
      "    loss           : -215857.0549117823\n",
      "    val_loss       : -224836.25362042684\n",
      "Train Epoch: 39 [0/54000 (0%)] Loss: -201499.406250\n",
      "Train Epoch: 39 [1408/54000 (3%)] Loss: -206779.609375\n",
      "Train Epoch: 39 [2816/54000 (5%)] Loss: -237276.578125\n",
      "Train Epoch: 39 [4224/54000 (8%)] Loss: -202910.046875\n",
      "Train Epoch: 39 [5632/54000 (10%)] Loss: -214247.500000\n",
      "Train Epoch: 39 [7040/54000 (13%)] Loss: -211756.500000\n",
      "Train Epoch: 39 [8448/54000 (16%)] Loss: -236215.937500\n",
      "Train Epoch: 39 [9856/54000 (18%)] Loss: -208494.437500\n",
      "Train Epoch: 39 [11264/54000 (21%)] Loss: -203793.312500\n",
      "Train Epoch: 39 [12672/54000 (23%)] Loss: -218848.687500\n",
      "Train Epoch: 39 [14080/54000 (26%)] Loss: -224166.921875\n",
      "Train Epoch: 39 [15488/54000 (29%)] Loss: -237308.500000\n",
      "Train Epoch: 39 [16896/54000 (31%)] Loss: -209124.406250\n",
      "Train Epoch: 39 [18304/54000 (34%)] Loss: -213370.875000\n",
      "Train Epoch: 39 [19712/54000 (37%)] Loss: -205569.718750\n",
      "Train Epoch: 39 [21120/54000 (39%)] Loss: -206617.093750\n",
      "Train Epoch: 39 [22528/54000 (42%)] Loss: -203521.546875\n",
      "Train Epoch: 39 [23936/54000 (44%)] Loss: -207108.500000\n",
      "Train Epoch: 39 [25344/54000 (47%)] Loss: -206464.140625\n",
      "Train Epoch: 39 [26752/54000 (50%)] Loss: -237288.640625\n",
      "Train Epoch: 39 [28160/54000 (52%)] Loss: -208411.250000\n",
      "Train Epoch: 39 [29568/54000 (55%)] Loss: -217336.796875\n",
      "Train Epoch: 39 [30976/54000 (57%)] Loss: -213240.968750\n",
      "Train Epoch: 39 [32384/54000 (60%)] Loss: -217035.812500\n",
      "Train Epoch: 39 [33792/54000 (63%)] Loss: -217166.250000\n",
      "Train Epoch: 39 [35200/54000 (65%)] Loss: -201615.343750\n",
      "Train Epoch: 39 [36608/54000 (68%)] Loss: -221744.312500\n",
      "Train Epoch: 39 [38016/54000 (70%)] Loss: -211485.093750\n",
      "Train Epoch: 39 [39424/54000 (73%)] Loss: -219294.156250\n",
      "Train Epoch: 39 [40832/54000 (76%)] Loss: -237039.921875\n",
      "Train Epoch: 39 [42240/54000 (78%)] Loss: -212542.343750\n",
      "Train Epoch: 39 [43648/54000 (81%)] Loss: -206845.281250\n",
      "Train Epoch: 39 [45056/54000 (83%)] Loss: -216756.218750\n",
      "Train Epoch: 39 [46464/54000 (86%)] Loss: -211244.812500\n",
      "Train Epoch: 39 [47872/54000 (89%)] Loss: -211988.921875\n",
      "Train Epoch: 39 [49280/54000 (91%)] Loss: -210970.062500\n",
      "Train Epoch: 39 [50688/54000 (94%)] Loss: -205074.109375\n",
      "Train Epoch: 39 [52096/54000 (96%)] Loss: -218694.312500\n",
      "    epoch          : 39\n",
      "    loss           : -216033.69557416267\n",
      "    val_loss       : -221084.2608612805\n",
      "Train Epoch: 40 [0/54000 (0%)] Loss: -236826.531250\n",
      "Train Epoch: 40 [1408/54000 (3%)] Loss: -219606.562500\n",
      "Train Epoch: 40 [2816/54000 (5%)] Loss: -215984.093750\n",
      "Train Epoch: 40 [4224/54000 (8%)] Loss: -215434.093750\n",
      "Train Epoch: 40 [5632/54000 (10%)] Loss: -213835.921875\n",
      "Train Epoch: 40 [7040/54000 (13%)] Loss: -215489.312500\n",
      "Train Epoch: 40 [8448/54000 (16%)] Loss: -210425.890625\n",
      "Train Epoch: 40 [9856/54000 (18%)] Loss: -205670.093750\n",
      "Train Epoch: 40 [11264/54000 (21%)] Loss: -173429.031250\n",
      "Train Epoch: 40 [12672/54000 (23%)] Loss: -212577.546875\n",
      "Train Epoch: 40 [14080/54000 (26%)] Loss: -218115.671875\n",
      "Train Epoch: 40 [15488/54000 (29%)] Loss: -218554.312500\n",
      "Train Epoch: 40 [16896/54000 (31%)] Loss: -219514.078125\n",
      "Train Epoch: 40 [18304/54000 (34%)] Loss: -207400.968750\n",
      "Train Epoch: 40 [19712/54000 (37%)] Loss: -213103.484375\n",
      "Train Epoch: 40 [21120/54000 (39%)] Loss: -205878.125000\n",
      "Train Epoch: 40 [22528/54000 (42%)] Loss: -131677.484375\n",
      "Train Epoch: 40 [23936/54000 (44%)] Loss: -212866.343750\n",
      "Train Epoch: 40 [25344/54000 (47%)] Loss: -222942.343750\n",
      "Train Epoch: 40 [26752/54000 (50%)] Loss: -225045.234375\n",
      "Train Epoch: 40 [28160/54000 (52%)] Loss: -229725.531250\n",
      "Train Epoch: 40 [29568/54000 (55%)] Loss: -218209.375000\n",
      "Train Epoch: 40 [30976/54000 (57%)] Loss: -217949.671875\n",
      "Train Epoch: 40 [32384/54000 (60%)] Loss: -230686.046875\n",
      "Train Epoch: 40 [33792/54000 (63%)] Loss: -216089.687500\n",
      "Train Epoch: 40 [35200/54000 (65%)] Loss: -221720.125000\n",
      "Train Epoch: 40 [36608/54000 (68%)] Loss: -206982.109375\n",
      "Train Epoch: 40 [38016/54000 (70%)] Loss: -207984.218750\n",
      "Train Epoch: 40 [39424/54000 (73%)] Loss: -212603.375000\n",
      "Train Epoch: 40 [40832/54000 (76%)] Loss: -207213.593750\n",
      "Train Epoch: 40 [42240/54000 (78%)] Loss: -223803.812500\n",
      "Train Epoch: 40 [43648/54000 (81%)] Loss: -211540.468750\n",
      "Train Epoch: 40 [45056/54000 (83%)] Loss: -237702.187500\n",
      "Train Epoch: 40 [46464/54000 (86%)] Loss: -218406.343750\n",
      "Train Epoch: 40 [47872/54000 (89%)] Loss: -210965.546875\n",
      "Train Epoch: 40 [49280/54000 (91%)] Loss: -207531.468750\n",
      "Train Epoch: 40 [50688/54000 (94%)] Loss: -215696.093750\n",
      "Train Epoch: 40 [52096/54000 (96%)] Loss: -237526.578125\n",
      "    epoch          : 40\n",
      "    loss           : -215135.78184808613\n",
      "    val_loss       : -225179.52953506098\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch40.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 41 [0/54000 (0%)] Loss: -214564.796875\n",
      "Train Epoch: 41 [1408/54000 (3%)] Loss: -215234.453125\n",
      "Train Epoch: 41 [2816/54000 (5%)] Loss: -220050.531250\n",
      "Train Epoch: 41 [4224/54000 (8%)] Loss: -214458.781250\n",
      "Train Epoch: 41 [5632/54000 (10%)] Loss: -223438.531250\n",
      "Train Epoch: 41 [7040/54000 (13%)] Loss: -213501.093750\n",
      "Train Epoch: 41 [8448/54000 (16%)] Loss: -212772.031250\n",
      "Train Epoch: 41 [9856/54000 (18%)] Loss: -209975.000000\n",
      "Train Epoch: 41 [11264/54000 (21%)] Loss: -212199.781250\n",
      "Train Epoch: 41 [12672/54000 (23%)] Loss: -205650.156250\n",
      "Train Epoch: 41 [14080/54000 (26%)] Loss: -219963.000000\n",
      "Train Epoch: 41 [15488/54000 (29%)] Loss: -216736.406250\n",
      "Train Epoch: 41 [16896/54000 (31%)] Loss: -223336.609375\n",
      "Train Epoch: 41 [18304/54000 (34%)] Loss: -223960.656250\n",
      "Train Epoch: 41 [19712/54000 (37%)] Loss: -208521.640625\n",
      "Train Epoch: 41 [21120/54000 (39%)] Loss: -205645.250000\n",
      "Train Epoch: 41 [22528/54000 (42%)] Loss: -204492.781250\n",
      "Train Epoch: 41 [23936/54000 (44%)] Loss: -220990.968750\n",
      "Train Epoch: 41 [25344/54000 (47%)] Loss: -205917.093750\n",
      "Train Epoch: 41 [26752/54000 (50%)] Loss: -219229.859375\n",
      "Train Epoch: 41 [28160/54000 (52%)] Loss: -237716.406250\n",
      "Train Epoch: 41 [29568/54000 (55%)] Loss: -212524.625000\n",
      "Train Epoch: 41 [30976/54000 (57%)] Loss: -212241.296875\n",
      "Train Epoch: 41 [32384/54000 (60%)] Loss: -223766.500000\n",
      "Train Epoch: 41 [33792/54000 (63%)] Loss: -211624.937500\n",
      "Train Epoch: 41 [35200/54000 (65%)] Loss: -231784.656250\n",
      "Train Epoch: 41 [36608/54000 (68%)] Loss: -207097.171875\n",
      "Train Epoch: 41 [38016/54000 (70%)] Loss: -220779.125000\n",
      "Train Epoch: 41 [39424/54000 (73%)] Loss: -219431.531250\n",
      "Train Epoch: 41 [40832/54000 (76%)] Loss: -221058.046875\n",
      "Train Epoch: 41 [42240/54000 (78%)] Loss: -218740.750000\n",
      "Train Epoch: 41 [43648/54000 (81%)] Loss: -218017.234375\n",
      "Train Epoch: 41 [45056/54000 (83%)] Loss: -213331.375000\n",
      "Train Epoch: 41 [46464/54000 (86%)] Loss: -237025.687500\n",
      "Train Epoch: 41 [47872/54000 (89%)] Loss: -219108.500000\n",
      "Train Epoch: 41 [49280/54000 (91%)] Loss: -212063.359375\n",
      "Train Epoch: 41 [50688/54000 (94%)] Loss: -237528.250000\n",
      "Train Epoch: 41 [52096/54000 (96%)] Loss: -207221.343750\n",
      "    epoch          : 41\n",
      "    loss           : -216582.66548295456\n",
      "    val_loss       : -224507.5899390244\n",
      "Train Epoch: 42 [0/54000 (0%)] Loss: -212957.812500\n",
      "Train Epoch: 42 [1408/54000 (3%)] Loss: -237056.468750\n",
      "Train Epoch: 42 [2816/54000 (5%)] Loss: -205817.000000\n",
      "Train Epoch: 42 [4224/54000 (8%)] Loss: -219702.656250\n",
      "Train Epoch: 42 [5632/54000 (10%)] Loss: -205452.265625\n",
      "Train Epoch: 42 [7040/54000 (13%)] Loss: -217201.875000\n",
      "Train Epoch: 42 [8448/54000 (16%)] Loss: -224494.406250\n",
      "Train Epoch: 42 [9856/54000 (18%)] Loss: -216596.968750\n",
      "Train Epoch: 42 [11264/54000 (21%)] Loss: -219027.062500\n",
      "Train Epoch: 42 [12672/54000 (23%)] Loss: -213745.765625\n",
      "Train Epoch: 42 [14080/54000 (26%)] Loss: -225367.093750\n",
      "Train Epoch: 42 [15488/54000 (29%)] Loss: -219675.562500\n",
      "Train Epoch: 42 [16896/54000 (31%)] Loss: -208920.843750\n",
      "Train Epoch: 42 [18304/54000 (34%)] Loss: -225543.218750\n",
      "Train Epoch: 42 [19712/54000 (37%)] Loss: -223987.187500\n",
      "Train Epoch: 42 [21120/54000 (39%)] Loss: -224589.468750\n",
      "Train Epoch: 42 [22528/54000 (42%)] Loss: -224318.250000\n",
      "Train Epoch: 42 [23936/54000 (44%)] Loss: -204892.890625\n",
      "Train Epoch: 42 [25344/54000 (47%)] Loss: -221624.390625\n",
      "Train Epoch: 42 [26752/54000 (50%)] Loss: -218623.562500\n",
      "Train Epoch: 42 [28160/54000 (52%)] Loss: -208034.250000\n",
      "Train Epoch: 42 [29568/54000 (55%)] Loss: -217459.375000\n",
      "Train Epoch: 42 [30976/54000 (57%)] Loss: -209699.578125\n",
      "Train Epoch: 42 [32384/54000 (60%)] Loss: -216128.171875\n",
      "Train Epoch: 42 [33792/54000 (63%)] Loss: -219177.250000\n",
      "Train Epoch: 42 [35200/54000 (65%)] Loss: -212149.500000\n",
      "Train Epoch: 42 [36608/54000 (68%)] Loss: -223654.218750\n",
      "Train Epoch: 42 [38016/54000 (70%)] Loss: -224631.562500\n",
      "Train Epoch: 42 [39424/54000 (73%)] Loss: -206780.640625\n",
      "Train Epoch: 42 [40832/54000 (76%)] Loss: -237972.781250\n",
      "Train Epoch: 42 [42240/54000 (78%)] Loss: -208014.765625\n",
      "Train Epoch: 42 [43648/54000 (81%)] Loss: -218827.328125\n",
      "Train Epoch: 42 [45056/54000 (83%)] Loss: -211816.515625\n",
      "Train Epoch: 42 [46464/54000 (86%)] Loss: -219355.218750\n",
      "Train Epoch: 42 [47872/54000 (89%)] Loss: -214006.312500\n",
      "Train Epoch: 42 [49280/54000 (91%)] Loss: -214638.312500\n",
      "Train Epoch: 42 [50688/54000 (94%)] Loss: -210044.390625\n",
      "Train Epoch: 42 [52096/54000 (96%)] Loss: -209854.906250\n",
      "    epoch          : 42\n",
      "    loss           : -217120.29066985645\n",
      "    val_loss       : -225286.48894817074\n",
      "Train Epoch: 43 [0/54000 (0%)] Loss: -238053.562500\n",
      "Train Epoch: 43 [1408/54000 (3%)] Loss: -213193.687500\n",
      "Train Epoch: 43 [2816/54000 (5%)] Loss: -211938.109375\n",
      "Train Epoch: 43 [4224/54000 (8%)] Loss: -214781.718750\n",
      "Train Epoch: 43 [5632/54000 (10%)] Loss: -208247.109375\n",
      "Train Epoch: 43 [7040/54000 (13%)] Loss: -224988.000000\n",
      "Train Epoch: 43 [8448/54000 (16%)] Loss: -238766.968750\n",
      "Train Epoch: 43 [9856/54000 (18%)] Loss: -205673.859375\n",
      "Train Epoch: 43 [11264/54000 (21%)] Loss: -216826.156250\n",
      "Train Epoch: 43 [12672/54000 (23%)] Loss: -220610.468750\n",
      "Train Epoch: 43 [14080/54000 (26%)] Loss: -222907.250000\n",
      "Train Epoch: 43 [15488/54000 (29%)] Loss: -223067.718750\n",
      "Train Epoch: 43 [16896/54000 (31%)] Loss: -214137.218750\n",
      "Train Epoch: 43 [18304/54000 (34%)] Loss: -204839.812500\n",
      "Train Epoch: 43 [19712/54000 (37%)] Loss: -221973.937500\n",
      "Train Epoch: 43 [21120/54000 (39%)] Loss: -219821.000000\n",
      "Train Epoch: 43 [22528/54000 (42%)] Loss: -209100.390625\n",
      "Train Epoch: 43 [23936/54000 (44%)] Loss: -207076.000000\n",
      "Train Epoch: 43 [25344/54000 (47%)] Loss: -208053.718750\n",
      "Train Epoch: 43 [26752/54000 (50%)] Loss: -206813.062500\n",
      "Train Epoch: 43 [28160/54000 (52%)] Loss: -212453.109375\n",
      "Train Epoch: 43 [29568/54000 (55%)] Loss: -223824.562500\n",
      "Train Epoch: 43 [30976/54000 (57%)] Loss: -225107.703125\n",
      "Train Epoch: 43 [32384/54000 (60%)] Loss: -224014.046875\n",
      "Train Epoch: 43 [33792/54000 (63%)] Loss: -220446.406250\n",
      "Train Epoch: 43 [35200/54000 (65%)] Loss: -219335.406250\n",
      "Train Epoch: 43 [36608/54000 (68%)] Loss: -213104.562500\n",
      "Train Epoch: 43 [38016/54000 (70%)] Loss: -213134.312500\n",
      "Train Epoch: 43 [39424/54000 (73%)] Loss: -237015.625000\n",
      "Train Epoch: 43 [40832/54000 (76%)] Loss: -214047.593750\n",
      "Train Epoch: 43 [42240/54000 (78%)] Loss: -218527.093750\n",
      "Train Epoch: 43 [43648/54000 (81%)] Loss: -206075.812500\n",
      "Train Epoch: 43 [45056/54000 (83%)] Loss: -223154.875000\n",
      "Train Epoch: 43 [46464/54000 (86%)] Loss: -213429.218750\n",
      "Train Epoch: 43 [47872/54000 (89%)] Loss: -213723.187500\n",
      "Train Epoch: 43 [49280/54000 (91%)] Loss: -219263.390625\n",
      "Train Epoch: 43 [50688/54000 (94%)] Loss: -218829.500000\n",
      "Train Epoch: 43 [52096/54000 (96%)] Loss: -219051.578125\n",
      "    epoch          : 43\n",
      "    loss           : -217269.98788875598\n",
      "    val_loss       : -225939.62328506098\n",
      "Train Epoch: 44 [0/54000 (0%)] Loss: -212004.890625\n",
      "Train Epoch: 44 [1408/54000 (3%)] Loss: -213718.296875\n",
      "Train Epoch: 44 [2816/54000 (5%)] Loss: -223747.265625\n",
      "Train Epoch: 44 [4224/54000 (8%)] Loss: -215859.921875\n",
      "Train Epoch: 44 [5632/54000 (10%)] Loss: -213813.625000\n",
      "Train Epoch: 44 [7040/54000 (13%)] Loss: -239068.250000\n",
      "Train Epoch: 44 [8448/54000 (16%)] Loss: -219287.093750\n",
      "Train Epoch: 44 [9856/54000 (18%)] Loss: -207625.375000\n",
      "Train Epoch: 44 [11264/54000 (21%)] Loss: -212378.250000\n",
      "Train Epoch: 44 [12672/54000 (23%)] Loss: -238544.578125\n",
      "Train Epoch: 44 [14080/54000 (26%)] Loss: -209281.578125\n",
      "Train Epoch: 44 [15488/54000 (29%)] Loss: -212279.656250\n",
      "Train Epoch: 44 [16896/54000 (31%)] Loss: -210246.968750\n",
      "Train Epoch: 44 [18304/54000 (34%)] Loss: -221606.843750\n",
      "Train Epoch: 44 [19712/54000 (37%)] Loss: -238242.125000\n",
      "Train Epoch: 44 [21120/54000 (39%)] Loss: -210823.593750\n",
      "Train Epoch: 44 [22528/54000 (42%)] Loss: -207287.859375\n",
      "Train Epoch: 44 [23936/54000 (44%)] Loss: -209138.000000\n",
      "Train Epoch: 44 [25344/54000 (47%)] Loss: -207179.578125\n",
      "Train Epoch: 44 [26752/54000 (50%)] Loss: -221178.328125\n",
      "Train Epoch: 44 [28160/54000 (52%)] Loss: -225281.703125\n",
      "Train Epoch: 44 [29568/54000 (55%)] Loss: -224370.484375\n",
      "Train Epoch: 44 [30976/54000 (57%)] Loss: -214000.093750\n",
      "Train Epoch: 44 [32384/54000 (60%)] Loss: -239063.171875\n",
      "Train Epoch: 44 [33792/54000 (63%)] Loss: -219788.531250\n",
      "Train Epoch: 44 [35200/54000 (65%)] Loss: -220843.640625\n",
      "Train Epoch: 44 [36608/54000 (68%)] Loss: -224971.687500\n",
      "Train Epoch: 44 [38016/54000 (70%)] Loss: -209060.546875\n",
      "Train Epoch: 44 [39424/54000 (73%)] Loss: -224019.656250\n",
      "Train Epoch: 44 [40832/54000 (76%)] Loss: -225156.625000\n",
      "Train Epoch: 44 [42240/54000 (78%)] Loss: -207334.000000\n",
      "Train Epoch: 44 [43648/54000 (81%)] Loss: -220304.156250\n",
      "Train Epoch: 44 [45056/54000 (83%)] Loss: -211061.687500\n",
      "Train Epoch: 44 [46464/54000 (86%)] Loss: -220386.171875\n",
      "Train Epoch: 44 [47872/54000 (89%)] Loss: -212940.156250\n",
      "Train Epoch: 44 [49280/54000 (91%)] Loss: -218375.562500\n",
      "Train Epoch: 44 [50688/54000 (94%)] Loss: -233221.562500\n",
      "Train Epoch: 44 [52096/54000 (96%)] Loss: -221362.765625\n",
      "    epoch          : 44\n",
      "    loss           : -217806.1139354067\n",
      "    val_loss       : -226042.05354420733\n",
      "Train Epoch: 45 [0/54000 (0%)] Loss: -221162.437500\n",
      "Train Epoch: 45 [1408/54000 (3%)] Loss: -221085.750000\n",
      "Train Epoch: 45 [2816/54000 (5%)] Loss: -206582.687500\n",
      "Train Epoch: 45 [4224/54000 (8%)] Loss: -218096.500000\n",
      "Train Epoch: 45 [5632/54000 (10%)] Loss: -209401.921875\n",
      "Train Epoch: 45 [7040/54000 (13%)] Loss: -205339.187500\n",
      "Train Epoch: 45 [8448/54000 (16%)] Loss: -213605.687500\n",
      "Train Epoch: 45 [9856/54000 (18%)] Loss: -219285.156250\n",
      "Train Epoch: 45 [11264/54000 (21%)] Loss: -208902.312500\n",
      "Train Epoch: 45 [12672/54000 (23%)] Loss: -213214.187500\n",
      "Train Epoch: 45 [14080/54000 (26%)] Loss: -221969.968750\n",
      "Train Epoch: 45 [15488/54000 (29%)] Loss: -239678.156250\n",
      "Train Epoch: 45 [16896/54000 (31%)] Loss: -212077.796875\n",
      "Train Epoch: 45 [18304/54000 (34%)] Loss: -215525.781250\n",
      "Train Epoch: 45 [19712/54000 (37%)] Loss: -210029.281250\n",
      "Train Epoch: 45 [21120/54000 (39%)] Loss: -239495.562500\n",
      "Train Epoch: 45 [22528/54000 (42%)] Loss: -218748.687500\n",
      "Train Epoch: 45 [23936/54000 (44%)] Loss: -221612.281250\n",
      "Train Epoch: 45 [25344/54000 (47%)] Loss: -238930.437500\n",
      "Train Epoch: 45 [26752/54000 (50%)] Loss: -214305.703125\n",
      "Train Epoch: 45 [28160/54000 (52%)] Loss: -215241.656250\n",
      "Train Epoch: 45 [29568/54000 (55%)] Loss: -222728.328125\n",
      "Train Epoch: 45 [30976/54000 (57%)] Loss: -221561.812500\n",
      "Train Epoch: 45 [32384/54000 (60%)] Loss: -213146.625000\n",
      "Train Epoch: 45 [33792/54000 (63%)] Loss: -208755.453125\n",
      "Train Epoch: 45 [35200/54000 (65%)] Loss: -219386.718750\n",
      "Train Epoch: 45 [36608/54000 (68%)] Loss: -220192.031250\n",
      "Train Epoch: 45 [38016/54000 (70%)] Loss: -216551.968750\n",
      "Train Epoch: 45 [39424/54000 (73%)] Loss: -215345.125000\n",
      "Train Epoch: 45 [40832/54000 (76%)] Loss: -220135.625000\n",
      "Train Epoch: 45 [42240/54000 (78%)] Loss: -220097.687500\n",
      "Train Epoch: 45 [43648/54000 (81%)] Loss: -214146.656250\n",
      "Train Epoch: 45 [45056/54000 (83%)] Loss: -238983.921875\n",
      "Train Epoch: 45 [46464/54000 (86%)] Loss: -214426.500000\n",
      "Train Epoch: 45 [47872/54000 (89%)] Loss: -212724.625000\n",
      "Train Epoch: 45 [49280/54000 (91%)] Loss: -217544.484375\n",
      "Train Epoch: 45 [50688/54000 (94%)] Loss: -213278.312500\n",
      "Train Epoch: 45 [52096/54000 (96%)] Loss: -212915.656250\n",
      "    epoch          : 45\n",
      "    loss           : -218273.5017194976\n",
      "    val_loss       : -226561.2538109756\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch45.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 46 [0/54000 (0%)] Loss: -220952.437500\n",
      "Train Epoch: 46 [1408/54000 (3%)] Loss: -215199.343750\n",
      "Train Epoch: 46 [2816/54000 (5%)] Loss: -214007.796875\n",
      "Train Epoch: 46 [4224/54000 (8%)] Loss: -211064.906250\n",
      "Train Epoch: 46 [5632/54000 (10%)] Loss: -213472.593750\n",
      "Train Epoch: 46 [7040/54000 (13%)] Loss: -212570.296875\n",
      "Train Epoch: 46 [8448/54000 (16%)] Loss: -212125.515625\n",
      "Train Epoch: 46 [9856/54000 (18%)] Loss: -215669.156250\n",
      "Train Epoch: 46 [11264/54000 (21%)] Loss: -213477.375000\n",
      "Train Epoch: 46 [12672/54000 (23%)] Loss: -214785.500000\n",
      "Train Epoch: 46 [14080/54000 (26%)] Loss: -213246.500000\n",
      "Train Epoch: 46 [15488/54000 (29%)] Loss: -215361.609375\n",
      "Train Epoch: 46 [16896/54000 (31%)] Loss: -224815.781250\n",
      "Train Epoch: 46 [18304/54000 (34%)] Loss: -223786.171875\n",
      "Train Epoch: 46 [19712/54000 (37%)] Loss: -226247.656250\n",
      "Train Epoch: 46 [21120/54000 (39%)] Loss: -219068.250000\n",
      "Train Epoch: 46 [22528/54000 (42%)] Loss: -219366.765625\n",
      "Train Epoch: 46 [23936/54000 (44%)] Loss: -210016.281250\n",
      "Train Epoch: 46 [25344/54000 (47%)] Loss: -212522.718750\n",
      "Train Epoch: 46 [26752/54000 (50%)] Loss: -214941.687500\n",
      "Train Epoch: 46 [28160/54000 (52%)] Loss: -207764.906250\n",
      "Train Epoch: 46 [29568/54000 (55%)] Loss: -212259.375000\n",
      "Train Epoch: 46 [30976/54000 (57%)] Loss: -212397.125000\n",
      "Train Epoch: 46 [32384/54000 (60%)] Loss: -210147.984375\n",
      "Train Epoch: 46 [33792/54000 (63%)] Loss: -217334.000000\n",
      "Train Epoch: 46 [35200/54000 (65%)] Loss: -216886.171875\n",
      "Train Epoch: 46 [36608/54000 (68%)] Loss: -224257.671875\n",
      "Train Epoch: 46 [38016/54000 (70%)] Loss: -217935.093750\n",
      "Train Epoch: 46 [39424/54000 (73%)] Loss: -218179.406250\n",
      "Train Epoch: 46 [40832/54000 (76%)] Loss: -215355.562500\n",
      "Train Epoch: 46 [42240/54000 (78%)] Loss: -214555.062500\n",
      "Train Epoch: 46 [43648/54000 (81%)] Loss: -220698.281250\n",
      "Train Epoch: 46 [45056/54000 (83%)] Loss: -240189.828125\n",
      "Train Epoch: 46 [46464/54000 (86%)] Loss: -210706.296875\n",
      "Train Epoch: 46 [47872/54000 (89%)] Loss: -207573.078125\n",
      "Train Epoch: 46 [49280/54000 (91%)] Loss: -219731.531250\n",
      "Train Epoch: 46 [50688/54000 (94%)] Loss: -239960.343750\n",
      "Train Epoch: 46 [52096/54000 (96%)] Loss: -222522.437500\n",
      "    epoch          : 46\n",
      "    loss           : -218618.78289473685\n",
      "    val_loss       : -226375.29134908537\n",
      "Train Epoch: 47 [0/54000 (0%)] Loss: -221629.500000\n",
      "Train Epoch: 47 [1408/54000 (3%)] Loss: -220765.328125\n",
      "Train Epoch: 47 [2816/54000 (5%)] Loss: -220449.890625\n",
      "Train Epoch: 47 [4224/54000 (8%)] Loss: -218518.296875\n",
      "Train Epoch: 47 [5632/54000 (10%)] Loss: -214962.187500\n",
      "Train Epoch: 47 [7040/54000 (13%)] Loss: -206850.218750\n",
      "Train Epoch: 47 [8448/54000 (16%)] Loss: -226141.828125\n",
      "Train Epoch: 47 [9856/54000 (18%)] Loss: -224890.859375\n",
      "Train Epoch: 47 [11264/54000 (21%)] Loss: -213958.218750\n",
      "Train Epoch: 47 [12672/54000 (23%)] Loss: -212586.921875\n",
      "Train Epoch: 47 [14080/54000 (26%)] Loss: -239127.093750\n",
      "Train Epoch: 47 [15488/54000 (29%)] Loss: -208401.875000\n",
      "Train Epoch: 47 [16896/54000 (31%)] Loss: -208658.812500\n",
      "Train Epoch: 47 [18304/54000 (34%)] Loss: -210113.187500\n",
      "Train Epoch: 47 [19712/54000 (37%)] Loss: -217821.078125\n",
      "Train Epoch: 47 [21120/54000 (39%)] Loss: -221416.968750\n",
      "Train Epoch: 47 [22528/54000 (42%)] Loss: -220348.531250\n",
      "Train Epoch: 47 [23936/54000 (44%)] Loss: -221190.328125\n",
      "Train Epoch: 47 [25344/54000 (47%)] Loss: -208205.312500\n",
      "Train Epoch: 47 [26752/54000 (50%)] Loss: -210004.343750\n",
      "Train Epoch: 47 [28160/54000 (52%)] Loss: -220580.562500\n",
      "Train Epoch: 47 [29568/54000 (55%)] Loss: -211576.515625\n",
      "Train Epoch: 47 [30976/54000 (57%)] Loss: -240162.531250\n",
      "Train Epoch: 47 [32384/54000 (60%)] Loss: -213829.968750\n",
      "Train Epoch: 47 [33792/54000 (63%)] Loss: -214295.250000\n",
      "Train Epoch: 47 [35200/54000 (65%)] Loss: -221805.921875\n",
      "Train Epoch: 47 [36608/54000 (68%)] Loss: -225564.234375\n",
      "Train Epoch: 47 [38016/54000 (70%)] Loss: -210756.218750\n",
      "Train Epoch: 47 [39424/54000 (73%)] Loss: -210344.781250\n",
      "Train Epoch: 47 [40832/54000 (76%)] Loss: -239594.703125\n",
      "Train Epoch: 47 [42240/54000 (78%)] Loss: -209266.906250\n",
      "Train Epoch: 47 [43648/54000 (81%)] Loss: -219149.187500\n",
      "Train Epoch: 47 [45056/54000 (83%)] Loss: -240384.609375\n",
      "Train Epoch: 47 [46464/54000 (86%)] Loss: -219852.500000\n",
      "Train Epoch: 47 [47872/54000 (89%)] Loss: -210985.437500\n",
      "Train Epoch: 47 [49280/54000 (91%)] Loss: -209079.937500\n",
      "Train Epoch: 47 [50688/54000 (94%)] Loss: -214507.453125\n",
      "Train Epoch: 47 [52096/54000 (96%)] Loss: -222139.750000\n",
      "    epoch          : 47\n",
      "    loss           : -218862.62514952154\n",
      "    val_loss       : -226848.2364710366\n",
      "Train Epoch: 48 [0/54000 (0%)] Loss: -208951.406250\n",
      "Train Epoch: 48 [1408/54000 (3%)] Loss: -209577.000000\n",
      "Train Epoch: 48 [2816/54000 (5%)] Loss: -217079.968750\n",
      "Train Epoch: 48 [4224/54000 (8%)] Loss: -216397.625000\n",
      "Train Epoch: 48 [5632/54000 (10%)] Loss: -219751.750000\n",
      "Train Epoch: 48 [7040/54000 (13%)] Loss: -221851.406250\n",
      "Train Epoch: 48 [8448/54000 (16%)] Loss: -210409.484375\n",
      "Train Epoch: 48 [9856/54000 (18%)] Loss: -211668.781250\n",
      "Train Epoch: 48 [11264/54000 (21%)] Loss: -218568.687500\n",
      "Train Epoch: 48 [12672/54000 (23%)] Loss: -239866.890625\n",
      "Train Epoch: 48 [14080/54000 (26%)] Loss: -214519.062500\n",
      "Train Epoch: 48 [15488/54000 (29%)] Loss: -206940.968750\n",
      "Train Epoch: 48 [16896/54000 (31%)] Loss: -223255.640625\n",
      "Train Epoch: 48 [18304/54000 (34%)] Loss: -219963.562500\n",
      "Train Epoch: 48 [19712/54000 (37%)] Loss: -240255.656250\n",
      "Train Epoch: 48 [21120/54000 (39%)] Loss: -218276.437500\n",
      "Train Epoch: 48 [22528/54000 (42%)] Loss: -216654.640625\n",
      "Train Epoch: 48 [23936/54000 (44%)] Loss: -227207.015625\n",
      "Train Epoch: 48 [25344/54000 (47%)] Loss: -225975.187500\n",
      "Train Epoch: 48 [26752/54000 (50%)] Loss: -214898.062500\n",
      "Train Epoch: 48 [28160/54000 (52%)] Loss: -214683.593750\n",
      "Train Epoch: 48 [29568/54000 (55%)] Loss: -219851.140625\n",
      "Train Epoch: 48 [30976/54000 (57%)] Loss: -226007.468750\n",
      "Train Epoch: 48 [32384/54000 (60%)] Loss: -233526.828125\n",
      "Train Epoch: 48 [33792/54000 (63%)] Loss: -208354.125000\n",
      "Train Epoch: 48 [35200/54000 (65%)] Loss: -223971.328125\n",
      "Train Epoch: 48 [36608/54000 (68%)] Loss: -225302.343750\n",
      "Train Epoch: 48 [38016/54000 (70%)] Loss: -224013.718750\n",
      "Train Epoch: 48 [39424/54000 (73%)] Loss: -211408.125000\n",
      "Train Epoch: 48 [40832/54000 (76%)] Loss: -225520.250000\n",
      "Train Epoch: 48 [42240/54000 (78%)] Loss: -225515.343750\n",
      "Train Epoch: 48 [43648/54000 (81%)] Loss: -225576.281250\n",
      "Train Epoch: 48 [45056/54000 (83%)] Loss: -221133.281250\n",
      "Train Epoch: 48 [46464/54000 (86%)] Loss: -217017.875000\n",
      "Train Epoch: 48 [47872/54000 (89%)] Loss: -220763.140625\n",
      "Train Epoch: 48 [49280/54000 (91%)] Loss: -214845.562500\n",
      "Train Epoch: 48 [50688/54000 (94%)] Loss: -240440.671875\n",
      "Train Epoch: 48 [52096/54000 (96%)] Loss: -213872.312500\n",
      "    epoch          : 48\n",
      "    loss           : -219110.28125\n",
      "    val_loss       : -227038.28982469512\n",
      "Train Epoch: 49 [0/54000 (0%)] Loss: -221674.109375\n",
      "Train Epoch: 49 [1408/54000 (3%)] Loss: -240795.140625\n",
      "Train Epoch: 49 [2816/54000 (5%)] Loss: -214653.125000\n",
      "Train Epoch: 49 [4224/54000 (8%)] Loss: -211306.359375\n",
      "Train Epoch: 49 [5632/54000 (10%)] Loss: -221672.312500\n",
      "Train Epoch: 49 [7040/54000 (13%)] Loss: -221228.406250\n",
      "Train Epoch: 49 [8448/54000 (16%)] Loss: -225666.531250\n",
      "Train Epoch: 49 [9856/54000 (18%)] Loss: -208980.093750\n",
      "Train Epoch: 49 [11264/54000 (21%)] Loss: -221577.109375\n",
      "Train Epoch: 49 [12672/54000 (23%)] Loss: -210359.062500\n",
      "Train Epoch: 49 [14080/54000 (26%)] Loss: -211451.218750\n",
      "Train Epoch: 49 [15488/54000 (29%)] Loss: -213379.000000\n",
      "Train Epoch: 49 [16896/54000 (31%)] Loss: -221725.843750\n",
      "Train Epoch: 49 [18304/54000 (34%)] Loss: -215777.031250\n",
      "Train Epoch: 49 [19712/54000 (37%)] Loss: -215162.937500\n",
      "Train Epoch: 49 [21120/54000 (39%)] Loss: -215164.968750\n",
      "Train Epoch: 49 [22528/54000 (42%)] Loss: -210334.687500\n",
      "Train Epoch: 49 [23936/54000 (44%)] Loss: -225695.343750\n",
      "Train Epoch: 49 [25344/54000 (47%)] Loss: -222866.406250\n",
      "Train Epoch: 49 [26752/54000 (50%)] Loss: -209549.890625\n",
      "Train Epoch: 49 [28160/54000 (52%)] Loss: -211041.015625\n",
      "Train Epoch: 49 [29568/54000 (55%)] Loss: -222002.781250\n",
      "Train Epoch: 49 [30976/54000 (57%)] Loss: -220057.937500\n",
      "Train Epoch: 49 [32384/54000 (60%)] Loss: -241107.093750\n",
      "Train Epoch: 49 [33792/54000 (63%)] Loss: -213557.031250\n",
      "Train Epoch: 49 [35200/54000 (65%)] Loss: -212992.968750\n",
      "Train Epoch: 49 [36608/54000 (68%)] Loss: -219618.125000\n",
      "Train Epoch: 49 [38016/54000 (70%)] Loss: -217223.875000\n",
      "Train Epoch: 49 [39424/54000 (73%)] Loss: -225761.546875\n",
      "Train Epoch: 49 [40832/54000 (76%)] Loss: -214374.531250\n",
      "Train Epoch: 49 [42240/54000 (78%)] Loss: -217410.312500\n",
      "Train Epoch: 49 [43648/54000 (81%)] Loss: -226723.500000\n",
      "Train Epoch: 49 [45056/54000 (83%)] Loss: -211480.156250\n",
      "Train Epoch: 49 [46464/54000 (86%)] Loss: -212910.562500\n",
      "Train Epoch: 49 [47872/54000 (89%)] Loss: -219622.562500\n",
      "Train Epoch: 49 [49280/54000 (91%)] Loss: -222183.281250\n",
      "Train Epoch: 49 [50688/54000 (94%)] Loss: -210559.687500\n",
      "Train Epoch: 49 [52096/54000 (96%)] Loss: -209626.937500\n",
      "    epoch          : 49\n",
      "    loss           : -219411.15565191387\n",
      "    val_loss       : -226852.11604420733\n",
      "Train Epoch: 50 [0/54000 (0%)] Loss: -227145.312500\n",
      "Train Epoch: 50 [1408/54000 (3%)] Loss: -226127.906250\n",
      "Train Epoch: 50 [2816/54000 (5%)] Loss: -219392.640625\n",
      "Train Epoch: 50 [4224/54000 (8%)] Loss: -224598.250000\n",
      "Train Epoch: 50 [5632/54000 (10%)] Loss: -213907.843750\n",
      "Train Epoch: 50 [7040/54000 (13%)] Loss: -212675.765625\n",
      "Train Epoch: 50 [8448/54000 (16%)] Loss: -211670.468750\n",
      "Train Epoch: 50 [9856/54000 (18%)] Loss: -226471.218750\n",
      "Train Epoch: 50 [11264/54000 (21%)] Loss: -211480.250000\n",
      "Train Epoch: 50 [12672/54000 (23%)] Loss: -220075.250000\n",
      "Train Epoch: 50 [14080/54000 (26%)] Loss: -218759.656250\n",
      "Train Epoch: 50 [15488/54000 (29%)] Loss: -225293.031250\n",
      "Train Epoch: 50 [16896/54000 (31%)] Loss: -215207.062500\n",
      "Train Epoch: 50 [18304/54000 (34%)] Loss: -214553.015625\n",
      "Train Epoch: 50 [19712/54000 (37%)] Loss: -239669.375000\n",
      "Train Epoch: 50 [21120/54000 (39%)] Loss: -220553.312500\n",
      "Train Epoch: 50 [22528/54000 (42%)] Loss: -217616.390625\n",
      "Train Epoch: 50 [23936/54000 (44%)] Loss: -210129.187500\n",
      "Train Epoch: 50 [25344/54000 (47%)] Loss: -215671.203125\n",
      "Train Epoch: 50 [26752/54000 (50%)] Loss: -215629.406250\n",
      "Train Epoch: 50 [28160/54000 (52%)] Loss: -215377.890625\n",
      "Train Epoch: 50 [29568/54000 (55%)] Loss: -225828.937500\n",
      "Train Epoch: 50 [30976/54000 (57%)] Loss: -208738.281250\n",
      "Train Epoch: 50 [32384/54000 (60%)] Loss: -225666.656250\n",
      "Train Epoch: 50 [33792/54000 (63%)] Loss: -226072.921875\n",
      "Train Epoch: 50 [35200/54000 (65%)] Loss: -221493.218750\n",
      "Train Epoch: 50 [36608/54000 (68%)] Loss: -207893.093750\n",
      "Train Epoch: 50 [38016/54000 (70%)] Loss: -234337.125000\n",
      "Train Epoch: 50 [39424/54000 (73%)] Loss: -215470.187500\n",
      "Train Epoch: 50 [40832/54000 (76%)] Loss: -222618.531250\n",
      "Train Epoch: 50 [42240/54000 (78%)] Loss: -215785.968750\n",
      "Train Epoch: 50 [43648/54000 (81%)] Loss: -217164.718750\n",
      "Train Epoch: 50 [45056/54000 (83%)] Loss: -212923.703125\n",
      "Train Epoch: 50 [46464/54000 (86%)] Loss: -227088.531250\n",
      "Train Epoch: 50 [47872/54000 (89%)] Loss: -213905.937500\n",
      "Train Epoch: 50 [49280/54000 (91%)] Loss: -219966.625000\n",
      "Train Epoch: 50 [50688/54000 (94%)] Loss: -240553.750000\n",
      "Train Epoch: 50 [52096/54000 (96%)] Loss: -221414.921875\n",
      "    epoch          : 50\n",
      "    loss           : -219159.45331190192\n",
      "    val_loss       : -224042.25933689025\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [0/54000 (0%)] Loss: -241938.359375\n",
      "Train Epoch: 51 [1408/54000 (3%)] Loss: -215089.937500\n",
      "Train Epoch: 51 [2816/54000 (5%)] Loss: -217377.015625\n",
      "Train Epoch: 51 [4224/54000 (8%)] Loss: -212033.312500\n",
      "Train Epoch: 51 [5632/54000 (10%)] Loss: -211908.593750\n",
      "Train Epoch: 51 [7040/54000 (13%)] Loss: -220821.703125\n",
      "Train Epoch: 51 [8448/54000 (16%)] Loss: -212389.781250\n",
      "Train Epoch: 51 [9856/54000 (18%)] Loss: -214551.062500\n",
      "Train Epoch: 51 [11264/54000 (21%)] Loss: -213870.218750\n",
      "Train Epoch: 51 [12672/54000 (23%)] Loss: -214990.281250\n",
      "Train Epoch: 51 [14080/54000 (26%)] Loss: -235241.468750\n",
      "Train Epoch: 51 [15488/54000 (29%)] Loss: -221015.609375\n",
      "Train Epoch: 51 [16896/54000 (31%)] Loss: -212783.562500\n",
      "Train Epoch: 51 [18304/54000 (34%)] Loss: -226265.265625\n",
      "Train Epoch: 51 [19712/54000 (37%)] Loss: -211076.812500\n",
      "Train Epoch: 51 [21120/54000 (39%)] Loss: -211060.718750\n",
      "Train Epoch: 51 [22528/54000 (42%)] Loss: -226865.359375\n",
      "Train Epoch: 51 [23936/54000 (44%)] Loss: -226492.968750\n",
      "Train Epoch: 51 [25344/54000 (47%)] Loss: -222434.250000\n",
      "Train Epoch: 51 [26752/54000 (50%)] Loss: -219777.281250\n",
      "Train Epoch: 51 [28160/54000 (52%)] Loss: -241317.015625\n",
      "Train Epoch: 51 [29568/54000 (55%)] Loss: -217494.312500\n",
      "Train Epoch: 51 [30976/54000 (57%)] Loss: -210649.500000\n",
      "Train Epoch: 51 [32384/54000 (60%)] Loss: -225126.109375\n",
      "Train Epoch: 51 [33792/54000 (63%)] Loss: -211109.953125\n",
      "Train Epoch: 51 [35200/54000 (65%)] Loss: -226550.062500\n",
      "Train Epoch: 51 [36608/54000 (68%)] Loss: -211224.968750\n",
      "Train Epoch: 51 [38016/54000 (70%)] Loss: -219438.031250\n",
      "Train Epoch: 51 [39424/54000 (73%)] Loss: -215182.453125\n",
      "Train Epoch: 51 [40832/54000 (76%)] Loss: -215628.265625\n",
      "Train Epoch: 51 [42240/54000 (78%)] Loss: -235590.062500\n",
      "Train Epoch: 51 [43648/54000 (81%)] Loss: -240193.781250\n",
      "Train Epoch: 51 [45056/54000 (83%)] Loss: -212204.125000\n",
      "Train Epoch: 51 [46464/54000 (86%)] Loss: -217295.640625\n",
      "Train Epoch: 51 [47872/54000 (89%)] Loss: -213853.109375\n",
      "Train Epoch: 51 [49280/54000 (91%)] Loss: -241506.531250\n",
      "Train Epoch: 51 [50688/54000 (94%)] Loss: -216602.406250\n",
      "Train Epoch: 51 [52096/54000 (96%)] Loss: -224452.781250\n",
      "    epoch          : 51\n",
      "    loss           : -219623.35346889953\n",
      "    val_loss       : -226928.31135670733\n",
      "Train Epoch: 52 [0/54000 (0%)] Loss: -222918.937500\n",
      "Train Epoch: 52 [1408/54000 (3%)] Loss: -226611.000000\n",
      "Train Epoch: 52 [2816/54000 (5%)] Loss: -225875.750000\n",
      "Train Epoch: 52 [4224/54000 (8%)] Loss: -223172.468750\n",
      "Train Epoch: 52 [5632/54000 (10%)] Loss: -211216.781250\n",
      "Train Epoch: 52 [7040/54000 (13%)] Loss: -227784.750000\n",
      "Train Epoch: 52 [8448/54000 (16%)] Loss: -211365.437500\n",
      "Train Epoch: 52 [9856/54000 (18%)] Loss: -224591.218750\n",
      "Train Epoch: 52 [11264/54000 (21%)] Loss: -215983.546875\n",
      "Train Epoch: 52 [12672/54000 (23%)] Loss: -216682.343750\n",
      "Train Epoch: 52 [14080/54000 (26%)] Loss: -216256.578125\n",
      "Train Epoch: 52 [15488/54000 (29%)] Loss: -215359.781250\n",
      "Train Epoch: 52 [16896/54000 (31%)] Loss: -218892.531250\n",
      "Train Epoch: 52 [18304/54000 (34%)] Loss: -241349.843750\n",
      "Train Epoch: 52 [19712/54000 (37%)] Loss: -220208.515625\n",
      "Train Epoch: 52 [21120/54000 (39%)] Loss: -219279.156250\n",
      "Train Epoch: 52 [22528/54000 (42%)] Loss: -210021.781250\n",
      "Train Epoch: 52 [23936/54000 (44%)] Loss: -221192.437500\n",
      "Train Epoch: 52 [25344/54000 (47%)] Loss: -221223.187500\n",
      "Train Epoch: 52 [26752/54000 (50%)] Loss: -215226.125000\n",
      "Train Epoch: 52 [28160/54000 (52%)] Loss: -212934.078125\n",
      "Train Epoch: 52 [29568/54000 (55%)] Loss: -215316.921875\n",
      "Train Epoch: 52 [30976/54000 (57%)] Loss: -223489.328125\n",
      "Train Epoch: 52 [32384/54000 (60%)] Loss: -220704.625000\n",
      "Train Epoch: 52 [33792/54000 (63%)] Loss: -219909.750000\n",
      "Train Epoch: 52 [35200/54000 (65%)] Loss: -209555.078125\n",
      "Train Epoch: 52 [36608/54000 (68%)] Loss: -226428.937500\n",
      "Train Epoch: 52 [38016/54000 (70%)] Loss: -212228.468750\n",
      "Train Epoch: 52 [39424/54000 (73%)] Loss: -216539.562500\n",
      "Train Epoch: 52 [40832/54000 (76%)] Loss: -211019.875000\n",
      "Train Epoch: 52 [42240/54000 (78%)] Loss: -217025.312500\n",
      "Train Epoch: 52 [43648/54000 (81%)] Loss: -216706.109375\n",
      "Train Epoch: 52 [45056/54000 (83%)] Loss: -221539.984375\n",
      "Train Epoch: 52 [46464/54000 (86%)] Loss: -216432.312500\n",
      "Train Epoch: 52 [47872/54000 (89%)] Loss: -216646.312500\n",
      "Train Epoch: 52 [49280/54000 (91%)] Loss: -214632.875000\n",
      "Train Epoch: 52 [50688/54000 (94%)] Loss: -211631.625000\n",
      "Train Epoch: 52 [52096/54000 (96%)] Loss: -222548.343750\n",
      "    epoch          : 52\n",
      "    loss           : -219714.67180023924\n",
      "    val_loss       : -225467.92054115853\n",
      "Train Epoch: 53 [0/54000 (0%)] Loss: -217139.125000\n",
      "Train Epoch: 53 [1408/54000 (3%)] Loss: -217184.125000\n",
      "Train Epoch: 53 [2816/54000 (5%)] Loss: -221594.687500\n",
      "Train Epoch: 53 [4224/54000 (8%)] Loss: -224225.875000\n",
      "Train Epoch: 53 [5632/54000 (10%)] Loss: -212103.468750\n",
      "Train Epoch: 53 [7040/54000 (13%)] Loss: -214623.125000\n",
      "Train Epoch: 53 [8448/54000 (16%)] Loss: -214738.203125\n",
      "Train Epoch: 53 [9856/54000 (18%)] Loss: -218541.187500\n",
      "Train Epoch: 53 [11264/54000 (21%)] Loss: -214688.484375\n",
      "Train Epoch: 53 [12672/54000 (23%)] Loss: -216589.656250\n",
      "Train Epoch: 53 [14080/54000 (26%)] Loss: -219602.015625\n",
      "Train Epoch: 53 [15488/54000 (29%)] Loss: -221331.515625\n",
      "Train Epoch: 53 [16896/54000 (31%)] Loss: -215719.937500\n",
      "Train Epoch: 53 [18304/54000 (34%)] Loss: -225272.296875\n",
      "Train Epoch: 53 [19712/54000 (37%)] Loss: -218015.937500\n",
      "Train Epoch: 53 [21120/54000 (39%)] Loss: -218620.843750\n",
      "Train Epoch: 53 [22528/54000 (42%)] Loss: -216267.093750\n",
      "Train Epoch: 53 [23936/54000 (44%)] Loss: -215721.312500\n",
      "Train Epoch: 53 [25344/54000 (47%)] Loss: -226497.656250\n",
      "Train Epoch: 53 [26752/54000 (50%)] Loss: -214318.843750\n",
      "Train Epoch: 53 [28160/54000 (52%)] Loss: -213240.812500\n",
      "Train Epoch: 53 [29568/54000 (55%)] Loss: -213704.000000\n",
      "Train Epoch: 53 [30976/54000 (57%)] Loss: -214303.281250\n",
      "Train Epoch: 53 [32384/54000 (60%)] Loss: -241691.343750\n",
      "Train Epoch: 53 [33792/54000 (63%)] Loss: -224084.250000\n",
      "Train Epoch: 53 [35200/54000 (65%)] Loss: -222109.125000\n",
      "Train Epoch: 53 [36608/54000 (68%)] Loss: -219410.953125\n",
      "Train Epoch: 53 [38016/54000 (70%)] Loss: -219884.515625\n",
      "Train Epoch: 53 [39424/54000 (73%)] Loss: -224511.406250\n",
      "Train Epoch: 53 [40832/54000 (76%)] Loss: -242286.671875\n",
      "Train Epoch: 53 [42240/54000 (78%)] Loss: -214204.390625\n",
      "Train Epoch: 53 [43648/54000 (81%)] Loss: -215165.687500\n",
      "Train Epoch: 53 [45056/54000 (83%)] Loss: -217179.468750\n",
      "Train Epoch: 53 [46464/54000 (86%)] Loss: -211422.812500\n",
      "Train Epoch: 53 [47872/54000 (89%)] Loss: -215210.312500\n",
      "Train Epoch: 53 [49280/54000 (91%)] Loss: -235558.468750\n",
      "Train Epoch: 53 [50688/54000 (94%)] Loss: -222120.234375\n",
      "Train Epoch: 53 [52096/54000 (96%)] Loss: -224880.828125\n",
      "    epoch          : 53\n",
      "    loss           : -220244.42935107654\n",
      "    val_loss       : -227442.29363567074\n",
      "Train Epoch: 54 [0/54000 (0%)] Loss: -229461.609375\n",
      "Train Epoch: 54 [1408/54000 (3%)] Loss: -241386.843750\n",
      "Train Epoch: 54 [2816/54000 (5%)] Loss: -217015.843750\n",
      "Train Epoch: 54 [4224/54000 (8%)] Loss: -226066.578125\n",
      "Train Epoch: 54 [5632/54000 (10%)] Loss: -210290.031250\n",
      "Train Epoch: 54 [7040/54000 (13%)] Loss: -220401.000000\n",
      "Train Epoch: 54 [8448/54000 (16%)] Loss: -220310.953125\n",
      "Train Epoch: 54 [9856/54000 (18%)] Loss: -220298.640625\n",
      "Train Epoch: 54 [11264/54000 (21%)] Loss: -223707.562500\n",
      "Train Epoch: 54 [12672/54000 (23%)] Loss: -217608.312500\n",
      "Train Epoch: 54 [14080/54000 (26%)] Loss: -209516.406250\n",
      "Train Epoch: 54 [15488/54000 (29%)] Loss: -213425.484375\n",
      "Train Epoch: 54 [16896/54000 (31%)] Loss: -215453.156250\n",
      "Train Epoch: 54 [18304/54000 (34%)] Loss: -210596.531250\n",
      "Train Epoch: 54 [19712/54000 (37%)] Loss: -226986.875000\n",
      "Train Epoch: 54 [21120/54000 (39%)] Loss: -226704.953125\n",
      "Train Epoch: 54 [22528/54000 (42%)] Loss: -226342.843750\n",
      "Train Epoch: 54 [23936/54000 (44%)] Loss: -225838.078125\n",
      "Train Epoch: 54 [25344/54000 (47%)] Loss: -218651.609375\n",
      "Train Epoch: 54 [26752/54000 (50%)] Loss: -241168.281250\n",
      "Train Epoch: 54 [28160/54000 (52%)] Loss: -214145.687500\n",
      "Train Epoch: 54 [29568/54000 (55%)] Loss: -216559.906250\n",
      "Train Epoch: 54 [30976/54000 (57%)] Loss: -226968.187500\n",
      "Train Epoch: 54 [32384/54000 (60%)] Loss: -216320.093750\n",
      "Train Epoch: 54 [33792/54000 (63%)] Loss: -216711.640625\n",
      "Train Epoch: 54 [35200/54000 (65%)] Loss: -215497.062500\n",
      "Train Epoch: 54 [36608/54000 (68%)] Loss: -215509.656250\n",
      "Train Epoch: 54 [38016/54000 (70%)] Loss: -208732.593750\n",
      "Train Epoch: 54 [39424/54000 (73%)] Loss: -211605.718750\n",
      "Train Epoch: 54 [40832/54000 (76%)] Loss: -227405.875000\n",
      "Train Epoch: 54 [42240/54000 (78%)] Loss: -208400.546875\n",
      "Train Epoch: 54 [43648/54000 (81%)] Loss: -215117.031250\n",
      "Train Epoch: 54 [45056/54000 (83%)] Loss: -241948.906250\n",
      "Train Epoch: 54 [46464/54000 (86%)] Loss: -220626.437500\n",
      "Train Epoch: 54 [47872/54000 (89%)] Loss: -212432.281250\n",
      "Train Epoch: 54 [49280/54000 (91%)] Loss: -220238.500000\n",
      "Train Epoch: 54 [50688/54000 (94%)] Loss: -220477.718750\n",
      "Train Epoch: 54 [52096/54000 (96%)] Loss: -234923.218750\n",
      "    epoch          : 54\n",
      "    loss           : -220551.20065789475\n",
      "    val_loss       : -227484.58250762196\n",
      "Train Epoch: 55 [0/54000 (0%)] Loss: -242205.531250\n",
      "Train Epoch: 55 [1408/54000 (3%)] Loss: -216086.656250\n",
      "Train Epoch: 55 [2816/54000 (5%)] Loss: -218708.656250\n",
      "Train Epoch: 55 [4224/54000 (8%)] Loss: -214065.296875\n",
      "Train Epoch: 55 [5632/54000 (10%)] Loss: -222735.562500\n",
      "Train Epoch: 55 [7040/54000 (13%)] Loss: -224800.734375\n",
      "Train Epoch: 55 [8448/54000 (16%)] Loss: -222908.656250\n",
      "Train Epoch: 55 [9856/54000 (18%)] Loss: -224066.781250\n",
      "Train Epoch: 55 [11264/54000 (21%)] Loss: -227633.953125\n",
      "Train Epoch: 55 [12672/54000 (23%)] Loss: -208543.734375\n",
      "Train Epoch: 55 [14080/54000 (26%)] Loss: -211874.375000\n",
      "Train Epoch: 55 [15488/54000 (29%)] Loss: -222636.140625\n",
      "Train Epoch: 55 [16896/54000 (31%)] Loss: -242691.453125\n",
      "Train Epoch: 55 [18304/54000 (34%)] Loss: -215875.062500\n",
      "Train Epoch: 55 [19712/54000 (37%)] Loss: -212663.687500\n",
      "Train Epoch: 55 [21120/54000 (39%)] Loss: -222771.375000\n",
      "Train Epoch: 55 [22528/54000 (42%)] Loss: -221548.093750\n",
      "Train Epoch: 55 [23936/54000 (44%)] Loss: -211777.718750\n",
      "Train Epoch: 55 [25344/54000 (47%)] Loss: -223565.062500\n",
      "Train Epoch: 55 [26752/54000 (50%)] Loss: -216732.984375\n",
      "Train Epoch: 55 [28160/54000 (52%)] Loss: -242203.656250\n",
      "Train Epoch: 55 [29568/54000 (55%)] Loss: -206911.312500\n",
      "Train Epoch: 55 [30976/54000 (57%)] Loss: -223736.437500\n",
      "Train Epoch: 55 [32384/54000 (60%)] Loss: -241899.187500\n",
      "Train Epoch: 55 [33792/54000 (63%)] Loss: -218360.125000\n",
      "Train Epoch: 55 [35200/54000 (65%)] Loss: -216530.359375\n",
      "Train Epoch: 55 [36608/54000 (68%)] Loss: -225441.046875\n",
      "Train Epoch: 55 [38016/54000 (70%)] Loss: -224462.406250\n",
      "Train Epoch: 55 [39424/54000 (73%)] Loss: -226248.703125\n",
      "Train Epoch: 55 [40832/54000 (76%)] Loss: -222114.718750\n",
      "Train Epoch: 55 [42240/54000 (78%)] Loss: -212243.375000\n",
      "Train Epoch: 55 [43648/54000 (81%)] Loss: -218665.406250\n",
      "Train Epoch: 55 [45056/54000 (83%)] Loss: -241327.890625\n",
      "Train Epoch: 55 [46464/54000 (86%)] Loss: -220954.671875\n",
      "Train Epoch: 55 [47872/54000 (89%)] Loss: -215070.218750\n",
      "Train Epoch: 55 [49280/54000 (91%)] Loss: -217835.687500\n",
      "Train Epoch: 55 [50688/54000 (94%)] Loss: -242110.078125\n",
      "Train Epoch: 55 [52096/54000 (96%)] Loss: -223556.625000\n",
      "    epoch          : 55\n",
      "    loss           : -220607.60597338516\n",
      "    val_loss       : -227909.14005335365\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch55.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 56 [0/54000 (0%)] Loss: -241689.156250\n",
      "Train Epoch: 56 [1408/54000 (3%)] Loss: -217460.187500\n",
      "Train Epoch: 56 [2816/54000 (5%)] Loss: -227740.546875\n",
      "Train Epoch: 56 [4224/54000 (8%)] Loss: -212689.687500\n",
      "Train Epoch: 56 [5632/54000 (10%)] Loss: -226766.843750\n",
      "Train Epoch: 56 [7040/54000 (13%)] Loss: -227588.656250\n",
      "Train Epoch: 56 [8448/54000 (16%)] Loss: -218398.906250\n",
      "Train Epoch: 56 [9856/54000 (18%)] Loss: -215403.484375\n",
      "Train Epoch: 56 [11264/54000 (21%)] Loss: -200782.468750\n",
      "Train Epoch: 56 [12672/54000 (23%)] Loss: -209408.453125\n",
      "Train Epoch: 56 [14080/54000 (26%)] Loss: -215621.156250\n",
      "Train Epoch: 56 [15488/54000 (29%)] Loss: -217980.515625\n",
      "Train Epoch: 56 [16896/54000 (31%)] Loss: -222749.062500\n",
      "Train Epoch: 56 [18304/54000 (34%)] Loss: -242817.187500\n",
      "Train Epoch: 56 [19712/54000 (37%)] Loss: -211458.656250\n",
      "Train Epoch: 56 [21120/54000 (39%)] Loss: -221571.859375\n",
      "Train Epoch: 56 [22528/54000 (42%)] Loss: -226786.078125\n",
      "Train Epoch: 56 [23936/54000 (44%)] Loss: -213566.406250\n",
      "Train Epoch: 56 [25344/54000 (47%)] Loss: -215645.421875\n",
      "Train Epoch: 56 [26752/54000 (50%)] Loss: -214623.328125\n",
      "Train Epoch: 56 [28160/54000 (52%)] Loss: -222827.875000\n",
      "Train Epoch: 56 [29568/54000 (55%)] Loss: -219985.343750\n",
      "Train Epoch: 56 [30976/54000 (57%)] Loss: -221691.875000\n",
      "Train Epoch: 56 [32384/54000 (60%)] Loss: -216197.687500\n",
      "Train Epoch: 56 [33792/54000 (63%)] Loss: -218158.765625\n",
      "Train Epoch: 56 [35200/54000 (65%)] Loss: -242796.187500\n",
      "Train Epoch: 56 [36608/54000 (68%)] Loss: -217855.859375\n",
      "Train Epoch: 56 [38016/54000 (70%)] Loss: -219016.953125\n",
      "Train Epoch: 56 [39424/54000 (73%)] Loss: -219576.734375\n",
      "Train Epoch: 56 [40832/54000 (76%)] Loss: -210713.375000\n",
      "Train Epoch: 56 [42240/54000 (78%)] Loss: -220919.687500\n",
      "Train Epoch: 56 [43648/54000 (81%)] Loss: -218173.671875\n",
      "Train Epoch: 56 [45056/54000 (83%)] Loss: -217179.671875\n",
      "Train Epoch: 56 [46464/54000 (86%)] Loss: -222272.140625\n",
      "Train Epoch: 56 [47872/54000 (89%)] Loss: -212774.718750\n",
      "Train Epoch: 56 [49280/54000 (91%)] Loss: -219260.562500\n",
      "Train Epoch: 56 [50688/54000 (94%)] Loss: -210994.281250\n",
      "Train Epoch: 56 [52096/54000 (96%)] Loss: -225474.781250\n",
      "    epoch          : 56\n",
      "    loss           : -220756.66503438994\n",
      "    val_loss       : -227869.1368140244\n",
      "Train Epoch: 57 [0/54000 (0%)] Loss: -230681.187500\n",
      "Train Epoch: 57 [1408/54000 (3%)] Loss: -217848.625000\n",
      "Train Epoch: 57 [2816/54000 (5%)] Loss: -223425.250000\n",
      "Train Epoch: 57 [4224/54000 (8%)] Loss: -228094.875000\n",
      "Train Epoch: 57 [5632/54000 (10%)] Loss: -227779.468750\n",
      "Train Epoch: 57 [7040/54000 (13%)] Loss: -227245.625000\n",
      "Train Epoch: 57 [8448/54000 (16%)] Loss: -215948.468750\n",
      "Train Epoch: 57 [9856/54000 (18%)] Loss: -220359.812500\n",
      "Train Epoch: 57 [11264/54000 (21%)] Loss: -220872.875000\n",
      "Train Epoch: 57 [12672/54000 (23%)] Loss: -224013.593750\n",
      "Train Epoch: 57 [14080/54000 (26%)] Loss: -220931.234375\n",
      "Train Epoch: 57 [15488/54000 (29%)] Loss: -219619.500000\n",
      "Train Epoch: 57 [16896/54000 (31%)] Loss: -243055.843750\n",
      "Train Epoch: 57 [18304/54000 (34%)] Loss: -220621.781250\n",
      "Train Epoch: 57 [19712/54000 (37%)] Loss: -212550.906250\n",
      "Train Epoch: 57 [21120/54000 (39%)] Loss: -219990.546875\n",
      "Train Epoch: 57 [22528/54000 (42%)] Loss: -224440.718750\n",
      "Train Epoch: 57 [23936/54000 (44%)] Loss: -227832.921875\n",
      "Train Epoch: 57 [25344/54000 (47%)] Loss: -220696.640625\n",
      "Train Epoch: 57 [26752/54000 (50%)] Loss: -225812.921875\n",
      "Train Epoch: 57 [28160/54000 (52%)] Loss: -215925.625000\n",
      "Train Epoch: 57 [29568/54000 (55%)] Loss: -219324.187500\n",
      "Train Epoch: 57 [30976/54000 (57%)] Loss: -216853.218750\n",
      "Train Epoch: 57 [32384/54000 (60%)] Loss: -223793.281250\n",
      "Train Epoch: 57 [33792/54000 (63%)] Loss: -214344.718750\n",
      "Train Epoch: 57 [35200/54000 (65%)] Loss: -209480.125000\n",
      "Train Epoch: 57 [36608/54000 (68%)] Loss: -219827.687500\n",
      "Train Epoch: 57 [38016/54000 (70%)] Loss: -219454.359375\n",
      "Train Epoch: 57 [39424/54000 (73%)] Loss: -217268.046875\n",
      "Train Epoch: 57 [40832/54000 (76%)] Loss: -222074.937500\n",
      "Train Epoch: 57 [42240/54000 (78%)] Loss: -218312.218750\n",
      "Train Epoch: 57 [43648/54000 (81%)] Loss: -217479.484375\n",
      "Train Epoch: 57 [45056/54000 (83%)] Loss: -217418.468750\n",
      "Train Epoch: 57 [46464/54000 (86%)] Loss: -222395.078125\n",
      "Train Epoch: 57 [47872/54000 (89%)] Loss: -220917.296875\n",
      "Train Epoch: 57 [49280/54000 (91%)] Loss: -216493.625000\n",
      "Train Epoch: 57 [50688/54000 (94%)] Loss: -243591.312500\n",
      "Train Epoch: 57 [52096/54000 (96%)] Loss: -223585.718750\n",
      "    epoch          : 57\n",
      "    loss           : -220910.2371785287\n",
      "    val_loss       : -228259.46646341463\n",
      "Train Epoch: 58 [0/54000 (0%)] Loss: -242589.156250\n",
      "Train Epoch: 58 [1408/54000 (3%)] Loss: -215355.843750\n",
      "Train Epoch: 58 [2816/54000 (5%)] Loss: -208879.968750\n",
      "Train Epoch: 58 [4224/54000 (8%)] Loss: -225840.796875\n",
      "Train Epoch: 58 [5632/54000 (10%)] Loss: -227401.093750\n",
      "Train Epoch: 58 [7040/54000 (13%)] Loss: -216386.125000\n",
      "Train Epoch: 58 [8448/54000 (16%)] Loss: -222398.500000\n",
      "Train Epoch: 58 [9856/54000 (18%)] Loss: -226710.140625\n",
      "Train Epoch: 58 [11264/54000 (21%)] Loss: -222803.687500\n",
      "Train Epoch: 58 [12672/54000 (23%)] Loss: -226108.625000\n",
      "Train Epoch: 58 [14080/54000 (26%)] Loss: -216672.296875\n",
      "Train Epoch: 58 [15488/54000 (29%)] Loss: -211800.125000\n",
      "Train Epoch: 58 [16896/54000 (31%)] Loss: -215439.140625\n",
      "Train Epoch: 58 [18304/54000 (34%)] Loss: -223320.875000\n",
      "Train Epoch: 58 [19712/54000 (37%)] Loss: -224981.500000\n",
      "Train Epoch: 58 [21120/54000 (39%)] Loss: -216138.250000\n",
      "Train Epoch: 58 [22528/54000 (42%)] Loss: -243199.265625\n",
      "Train Epoch: 58 [23936/54000 (44%)] Loss: -209972.953125\n",
      "Train Epoch: 58 [25344/54000 (47%)] Loss: -217826.843750\n",
      "Train Epoch: 58 [26752/54000 (50%)] Loss: -211440.625000\n",
      "Train Epoch: 58 [28160/54000 (52%)] Loss: -213472.750000\n",
      "Train Epoch: 58 [29568/54000 (55%)] Loss: -213845.687500\n",
      "Train Epoch: 58 [30976/54000 (57%)] Loss: -221450.687500\n",
      "Train Epoch: 58 [32384/54000 (60%)] Loss: -223392.875000\n",
      "Train Epoch: 58 [33792/54000 (63%)] Loss: -223607.687500\n",
      "Train Epoch: 58 [35200/54000 (65%)] Loss: -211646.703125\n",
      "Train Epoch: 58 [36608/54000 (68%)] Loss: -216725.375000\n",
      "Train Epoch: 58 [38016/54000 (70%)] Loss: -219770.281250\n",
      "Train Epoch: 58 [39424/54000 (73%)] Loss: -219225.187500\n",
      "Train Epoch: 58 [40832/54000 (76%)] Loss: -220395.359375\n",
      "Train Epoch: 58 [42240/54000 (78%)] Loss: -211289.906250\n",
      "Train Epoch: 58 [43648/54000 (81%)] Loss: -212692.531250\n",
      "Train Epoch: 58 [45056/54000 (83%)] Loss: -221521.796875\n",
      "Train Epoch: 58 [46464/54000 (86%)] Loss: -218439.078125\n",
      "Train Epoch: 58 [47872/54000 (89%)] Loss: -219293.593750\n",
      "Train Epoch: 58 [49280/54000 (91%)] Loss: -214451.406250\n",
      "Train Epoch: 58 [50688/54000 (94%)] Loss: -224128.656250\n",
      "Train Epoch: 58 [52096/54000 (96%)] Loss: -224300.843750\n",
      "    epoch          : 58\n",
      "    loss           : -221324.73344049044\n",
      "    val_loss       : -228204.15377286586\n",
      "Train Epoch: 59 [0/54000 (0%)] Loss: -225033.812500\n",
      "Train Epoch: 59 [1408/54000 (3%)] Loss: -243324.000000\n",
      "Train Epoch: 59 [2816/54000 (5%)] Loss: -217990.140625\n",
      "Train Epoch: 59 [4224/54000 (8%)] Loss: -213477.968750\n",
      "Train Epoch: 59 [5632/54000 (10%)] Loss: -219117.937500\n",
      "Train Epoch: 59 [7040/54000 (13%)] Loss: -217423.468750\n",
      "Train Epoch: 59 [8448/54000 (16%)] Loss: -228119.578125\n",
      "Train Epoch: 59 [9856/54000 (18%)] Loss: -216808.187500\n",
      "Train Epoch: 59 [11264/54000 (21%)] Loss: -225798.656250\n",
      "Train Epoch: 59 [12672/54000 (23%)] Loss: -242611.781250\n",
      "Train Epoch: 59 [14080/54000 (26%)] Loss: -223750.906250\n",
      "Train Epoch: 59 [15488/54000 (29%)] Loss: -219860.640625\n",
      "Train Epoch: 59 [16896/54000 (31%)] Loss: -221168.875000\n",
      "Train Epoch: 59 [18304/54000 (34%)] Loss: -244082.687500\n",
      "Train Epoch: 59 [19712/54000 (37%)] Loss: -217543.906250\n",
      "Train Epoch: 59 [21120/54000 (39%)] Loss: -208586.437500\n",
      "Train Epoch: 59 [22528/54000 (42%)] Loss: -213744.078125\n",
      "Train Epoch: 59 [23936/54000 (44%)] Loss: -217721.000000\n",
      "Train Epoch: 59 [25344/54000 (47%)] Loss: -219118.609375\n",
      "Train Epoch: 59 [26752/54000 (50%)] Loss: -241870.703125\n",
      "Train Epoch: 59 [28160/54000 (52%)] Loss: -214323.156250\n",
      "Train Epoch: 59 [29568/54000 (55%)] Loss: -214726.718750\n",
      "Train Epoch: 59 [30976/54000 (57%)] Loss: -221932.703125\n",
      "Train Epoch: 59 [32384/54000 (60%)] Loss: -225836.937500\n",
      "Train Epoch: 59 [33792/54000 (63%)] Loss: -242724.531250\n",
      "Train Epoch: 59 [35200/54000 (65%)] Loss: -227664.781250\n",
      "Train Epoch: 59 [36608/54000 (68%)] Loss: -215628.437500\n",
      "Train Epoch: 59 [38016/54000 (70%)] Loss: -218624.312500\n",
      "Train Epoch: 59 [39424/54000 (73%)] Loss: -243523.843750\n",
      "Train Epoch: 59 [40832/54000 (76%)] Loss: -216441.921875\n",
      "Train Epoch: 59 [42240/54000 (78%)] Loss: -223344.671875\n",
      "Train Epoch: 59 [43648/54000 (81%)] Loss: -218764.500000\n",
      "Train Epoch: 59 [45056/54000 (83%)] Loss: -244019.687500\n",
      "Train Epoch: 59 [46464/54000 (86%)] Loss: -224403.781250\n",
      "Train Epoch: 59 [47872/54000 (89%)] Loss: -212518.593750\n",
      "Train Epoch: 59 [49280/54000 (91%)] Loss: -214722.265625\n",
      "Train Epoch: 59 [50688/54000 (94%)] Loss: -244092.828125\n",
      "Train Epoch: 59 [52096/54000 (96%)] Loss: -211652.140625\n",
      "    epoch          : 59\n",
      "    loss           : -221537.86049641148\n",
      "    val_loss       : -228366.30583079267\n",
      "Train Epoch: 60 [0/54000 (0%)] Loss: -212623.906250\n",
      "Train Epoch: 60 [1408/54000 (3%)] Loss: -213951.640625\n",
      "Train Epoch: 60 [2816/54000 (5%)] Loss: -220076.093750\n",
      "Train Epoch: 60 [4224/54000 (8%)] Loss: -227823.390625\n",
      "Train Epoch: 60 [5632/54000 (10%)] Loss: -214871.031250\n",
      "Train Epoch: 60 [7040/54000 (13%)] Loss: -212285.500000\n",
      "Train Epoch: 60 [8448/54000 (16%)] Loss: -217265.640625\n",
      "Train Epoch: 60 [9856/54000 (18%)] Loss: -214146.656250\n",
      "Train Epoch: 60 [11264/54000 (21%)] Loss: -242407.078125\n",
      "Train Epoch: 60 [12672/54000 (23%)] Loss: -221815.796875\n",
      "Train Epoch: 60 [14080/54000 (26%)] Loss: -224617.656250\n",
      "Train Epoch: 60 [15488/54000 (29%)] Loss: -216548.171875\n",
      "Train Epoch: 60 [16896/54000 (31%)] Loss: -219538.937500\n",
      "Train Epoch: 60 [18304/54000 (34%)] Loss: -226568.250000\n",
      "Train Epoch: 60 [19712/54000 (37%)] Loss: -243451.390625\n",
      "Train Epoch: 60 [21120/54000 (39%)] Loss: -226941.359375\n",
      "Train Epoch: 60 [22528/54000 (42%)] Loss: -212208.218750\n",
      "Train Epoch: 60 [23936/54000 (44%)] Loss: -213183.187500\n",
      "Train Epoch: 60 [25344/54000 (47%)] Loss: -212397.296875\n",
      "Train Epoch: 60 [26752/54000 (50%)] Loss: -213310.000000\n",
      "Train Epoch: 60 [28160/54000 (52%)] Loss: -217235.921875\n",
      "Train Epoch: 60 [29568/54000 (55%)] Loss: -242658.437500\n",
      "Train Epoch: 60 [30976/54000 (57%)] Loss: -218668.531250\n",
      "Train Epoch: 60 [32384/54000 (60%)] Loss: -217326.031250\n",
      "Train Epoch: 60 [33792/54000 (63%)] Loss: -212658.187500\n",
      "Train Epoch: 60 [35200/54000 (65%)] Loss: -215761.390625\n",
      "Train Epoch: 60 [36608/54000 (68%)] Loss: -216823.687500\n",
      "Train Epoch: 60 [38016/54000 (70%)] Loss: -227528.328125\n",
      "Train Epoch: 60 [39424/54000 (73%)] Loss: -218377.375000\n",
      "Train Epoch: 60 [40832/54000 (76%)] Loss: -215257.062500\n",
      "Train Epoch: 60 [42240/54000 (78%)] Loss: -228739.187500\n",
      "Train Epoch: 60 [43648/54000 (81%)] Loss: -215067.593750\n",
      "Train Epoch: 60 [45056/54000 (83%)] Loss: -217607.250000\n",
      "Train Epoch: 60 [46464/54000 (86%)] Loss: -215874.734375\n",
      "Train Epoch: 60 [47872/54000 (89%)] Loss: -213288.406250\n",
      "Train Epoch: 60 [49280/54000 (91%)] Loss: -243230.125000\n",
      "Train Epoch: 60 [50688/54000 (94%)] Loss: -243827.187500\n",
      "Train Epoch: 60 [52096/54000 (96%)] Loss: -213936.281250\n",
      "    epoch          : 60\n",
      "    loss           : -221728.05973385167\n",
      "    val_loss       : -228449.96894054877\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch60.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 61 [0/54000 (0%)] Loss: -219417.843750\n",
      "Train Epoch: 61 [1408/54000 (3%)] Loss: -216344.250000\n",
      "Train Epoch: 61 [2816/54000 (5%)] Loss: -224818.500000\n",
      "Train Epoch: 61 [4224/54000 (8%)] Loss: -210726.531250\n",
      "Train Epoch: 61 [5632/54000 (10%)] Loss: -215244.890625\n",
      "Train Epoch: 61 [7040/54000 (13%)] Loss: -215866.828125\n",
      "Train Epoch: 61 [8448/54000 (16%)] Loss: -243997.875000\n",
      "Train Epoch: 61 [9856/54000 (18%)] Loss: -228762.171875\n",
      "Train Epoch: 61 [11264/54000 (21%)] Loss: -222336.468750\n",
      "Train Epoch: 61 [12672/54000 (23%)] Loss: -217139.593750\n",
      "Train Epoch: 61 [14080/54000 (26%)] Loss: -222960.031250\n",
      "Train Epoch: 61 [15488/54000 (29%)] Loss: -244408.781250\n",
      "Train Epoch: 61 [16896/54000 (31%)] Loss: -218687.468750\n",
      "Train Epoch: 61 [18304/54000 (34%)] Loss: -222740.796875\n",
      "Train Epoch: 61 [19712/54000 (37%)] Loss: -228699.390625\n",
      "Train Epoch: 61 [21120/54000 (39%)] Loss: -224938.968750\n",
      "Train Epoch: 61 [22528/54000 (42%)] Loss: -223624.593750\n",
      "Train Epoch: 61 [23936/54000 (44%)] Loss: -227202.125000\n",
      "Train Epoch: 61 [25344/54000 (47%)] Loss: -216349.593750\n",
      "Train Epoch: 61 [26752/54000 (50%)] Loss: -227813.671875\n",
      "Train Epoch: 61 [28160/54000 (52%)] Loss: -227690.890625\n",
      "Train Epoch: 61 [29568/54000 (55%)] Loss: -227209.515625\n",
      "Train Epoch: 61 [30976/54000 (57%)] Loss: -227246.250000\n",
      "Train Epoch: 61 [32384/54000 (60%)] Loss: -217733.421875\n",
      "Train Epoch: 61 [33792/54000 (63%)] Loss: -212011.046875\n",
      "Train Epoch: 61 [35200/54000 (65%)] Loss: -243061.578125\n",
      "Train Epoch: 61 [36608/54000 (68%)] Loss: -212172.812500\n",
      "Train Epoch: 61 [38016/54000 (70%)] Loss: -217553.890625\n",
      "Train Epoch: 61 [39424/54000 (73%)] Loss: -221297.703125\n",
      "Train Epoch: 61 [40832/54000 (76%)] Loss: -243441.062500\n",
      "Train Epoch: 61 [42240/54000 (78%)] Loss: -220735.125000\n",
      "Train Epoch: 61 [43648/54000 (81%)] Loss: -214730.250000\n",
      "Train Epoch: 61 [45056/54000 (83%)] Loss: -224262.109375\n",
      "Train Epoch: 61 [46464/54000 (86%)] Loss: -218587.312500\n",
      "Train Epoch: 61 [47872/54000 (89%)] Loss: -215849.468750\n",
      "Train Epoch: 61 [49280/54000 (91%)] Loss: -222240.562500\n",
      "Train Epoch: 61 [50688/54000 (94%)] Loss: -213332.718750\n",
      "Train Epoch: 61 [52096/54000 (96%)] Loss: -215127.109375\n",
      "    epoch          : 61\n",
      "    loss           : -221907.12582236843\n",
      "    val_loss       : -228394.68464176828\n",
      "Train Epoch: 62 [0/54000 (0%)] Loss: -243630.218750\n",
      "Train Epoch: 62 [1408/54000 (3%)] Loss: -221078.328125\n",
      "Train Epoch: 62 [2816/54000 (5%)] Loss: -217068.640625\n",
      "Train Epoch: 62 [4224/54000 (8%)] Loss: -228401.546875\n",
      "Train Epoch: 62 [5632/54000 (10%)] Loss: -214778.218750\n",
      "Train Epoch: 62 [7040/54000 (13%)] Loss: -243632.781250\n",
      "Train Epoch: 62 [8448/54000 (16%)] Loss: -218807.031250\n",
      "Train Epoch: 62 [9856/54000 (18%)] Loss: -211544.343750\n",
      "Train Epoch: 62 [11264/54000 (21%)] Loss: -213367.437500\n",
      "Train Epoch: 62 [12672/54000 (23%)] Loss: -217186.781250\n",
      "Train Epoch: 62 [14080/54000 (26%)] Loss: -244371.921875\n",
      "Train Epoch: 62 [15488/54000 (29%)] Loss: -217548.703125\n",
      "Train Epoch: 62 [16896/54000 (31%)] Loss: -224304.281250\n",
      "Train Epoch: 62 [18304/54000 (34%)] Loss: -214026.656250\n",
      "Train Epoch: 62 [19712/54000 (37%)] Loss: -226915.140625\n",
      "Train Epoch: 62 [21120/54000 (39%)] Loss: -217758.390625\n",
      "Train Epoch: 62 [22528/54000 (42%)] Loss: -227357.437500\n",
      "Train Epoch: 62 [23936/54000 (44%)] Loss: -217818.921875\n",
      "Train Epoch: 62 [25344/54000 (47%)] Loss: -243881.265625\n",
      "Train Epoch: 62 [26752/54000 (50%)] Loss: -227011.421875\n",
      "Train Epoch: 62 [28160/54000 (52%)] Loss: -220058.546875\n",
      "Train Epoch: 62 [29568/54000 (55%)] Loss: -213390.968750\n",
      "Train Epoch: 62 [30976/54000 (57%)] Loss: -210857.234375\n",
      "Train Epoch: 62 [32384/54000 (60%)] Loss: -222697.781250\n",
      "Train Epoch: 62 [33792/54000 (63%)] Loss: -225105.656250\n",
      "Train Epoch: 62 [35200/54000 (65%)] Loss: -244666.921875\n",
      "Train Epoch: 62 [36608/54000 (68%)] Loss: -216323.500000\n",
      "Train Epoch: 62 [38016/54000 (70%)] Loss: -213516.281250\n",
      "Train Epoch: 62 [39424/54000 (73%)] Loss: -224611.546875\n",
      "Train Epoch: 62 [40832/54000 (76%)] Loss: -219730.937500\n",
      "Train Epoch: 62 [42240/54000 (78%)] Loss: -215118.875000\n",
      "Train Epoch: 62 [43648/54000 (81%)] Loss: -228157.234375\n",
      "Train Epoch: 62 [45056/54000 (83%)] Loss: -225509.406250\n",
      "Train Epoch: 62 [46464/54000 (86%)] Loss: -224820.921875\n",
      "Train Epoch: 62 [47872/54000 (89%)] Loss: -217985.765625\n",
      "Train Epoch: 62 [49280/54000 (91%)] Loss: -222401.000000\n",
      "Train Epoch: 62 [50688/54000 (94%)] Loss: -244096.312500\n",
      "Train Epoch: 62 [52096/54000 (96%)] Loss: -224218.421875\n",
      "    epoch          : 62\n",
      "    loss           : -222081.83754485645\n",
      "    val_loss       : -228471.61432926828\n",
      "Train Epoch: 63 [0/54000 (0%)] Loss: -220439.953125\n",
      "Train Epoch: 63 [1408/54000 (3%)] Loss: -243525.421875\n",
      "Train Epoch: 63 [2816/54000 (5%)] Loss: -216399.906250\n",
      "Train Epoch: 63 [4224/54000 (8%)] Loss: -219100.343750\n",
      "Train Epoch: 63 [5632/54000 (10%)] Loss: -218955.515625\n",
      "Train Epoch: 63 [7040/54000 (13%)] Loss: -218880.703125\n",
      "Train Epoch: 63 [8448/54000 (16%)] Loss: -222461.265625\n",
      "Train Epoch: 63 [9856/54000 (18%)] Loss: -213703.015625\n",
      "Train Epoch: 63 [11264/54000 (21%)] Loss: -220269.281250\n",
      "Train Epoch: 63 [12672/54000 (23%)] Loss: -223713.343750\n",
      "Train Epoch: 63 [14080/54000 (26%)] Loss: -221868.671875\n",
      "Train Epoch: 63 [15488/54000 (29%)] Loss: -220145.000000\n",
      "Train Epoch: 63 [16896/54000 (31%)] Loss: -221923.375000\n",
      "Train Epoch: 63 [18304/54000 (34%)] Loss: -217366.296875\n",
      "Train Epoch: 63 [19712/54000 (37%)] Loss: -219807.343750\n",
      "Train Epoch: 63 [21120/54000 (39%)] Loss: -213233.343750\n",
      "Train Epoch: 63 [22528/54000 (42%)] Loss: -218823.375000\n",
      "Train Epoch: 63 [23936/54000 (44%)] Loss: -222554.765625\n",
      "Train Epoch: 63 [25344/54000 (47%)] Loss: -244592.687500\n",
      "Train Epoch: 63 [26752/54000 (50%)] Loss: -212209.906250\n",
      "Train Epoch: 63 [28160/54000 (52%)] Loss: -227380.906250\n",
      "Train Epoch: 63 [29568/54000 (55%)] Loss: -215755.687500\n",
      "Train Epoch: 63 [30976/54000 (57%)] Loss: -213739.562500\n",
      "Train Epoch: 63 [32384/54000 (60%)] Loss: -244036.031250\n",
      "Train Epoch: 63 [33792/54000 (63%)] Loss: -206430.687500\n",
      "Train Epoch: 63 [35200/54000 (65%)] Loss: -225959.437500\n",
      "Train Epoch: 63 [36608/54000 (68%)] Loss: -212305.609375\n",
      "Train Epoch: 63 [38016/54000 (70%)] Loss: -227471.812500\n",
      "Train Epoch: 63 [39424/54000 (73%)] Loss: -212913.062500\n",
      "Train Epoch: 63 [40832/54000 (76%)] Loss: -228008.562500\n",
      "Train Epoch: 63 [42240/54000 (78%)] Loss: -216022.906250\n",
      "Train Epoch: 63 [43648/54000 (81%)] Loss: -223595.015625\n",
      "Train Epoch: 63 [45056/54000 (83%)] Loss: -223576.953125\n",
      "Train Epoch: 63 [46464/54000 (86%)] Loss: -219184.656250\n",
      "Train Epoch: 63 [47872/54000 (89%)] Loss: -217006.562500\n",
      "Train Epoch: 63 [49280/54000 (91%)] Loss: -218198.984375\n",
      "Train Epoch: 63 [50688/54000 (94%)] Loss: -224617.843750\n",
      "Train Epoch: 63 [52096/54000 (96%)] Loss: -243994.218750\n",
      "    epoch          : 63\n",
      "    loss           : -222081.10832834928\n",
      "    val_loss       : -228738.1827362805\n",
      "Train Epoch: 64 [0/54000 (0%)] Loss: -214172.031250\n",
      "Train Epoch: 64 [1408/54000 (3%)] Loss: -226024.671875\n",
      "Train Epoch: 64 [2816/54000 (5%)] Loss: -211156.125000\n",
      "Train Epoch: 64 [4224/54000 (8%)] Loss: -218320.312500\n",
      "Train Epoch: 64 [5632/54000 (10%)] Loss: -215956.062500\n",
      "Train Epoch: 64 [7040/54000 (13%)] Loss: -228171.968750\n",
      "Train Epoch: 64 [8448/54000 (16%)] Loss: -213269.593750\n",
      "Train Epoch: 64 [9856/54000 (18%)] Loss: -212148.859375\n",
      "Train Epoch: 64 [11264/54000 (21%)] Loss: -222663.171875\n",
      "Train Epoch: 64 [12672/54000 (23%)] Loss: -225874.343750\n",
      "Train Epoch: 64 [14080/54000 (26%)] Loss: -219037.984375\n",
      "Train Epoch: 64 [15488/54000 (29%)] Loss: -225032.187500\n",
      "Train Epoch: 64 [16896/54000 (31%)] Loss: -215677.968750\n",
      "Train Epoch: 64 [18304/54000 (34%)] Loss: -216883.953125\n",
      "Train Epoch: 64 [19712/54000 (37%)] Loss: -215117.093750\n",
      "Train Epoch: 64 [21120/54000 (39%)] Loss: -222877.125000\n",
      "Train Epoch: 64 [22528/54000 (42%)] Loss: -227002.656250\n",
      "Train Epoch: 64 [23936/54000 (44%)] Loss: -226104.843750\n",
      "Train Epoch: 64 [25344/54000 (47%)] Loss: -221954.187500\n",
      "Train Epoch: 64 [26752/54000 (50%)] Loss: -223071.734375\n",
      "Train Epoch: 64 [28160/54000 (52%)] Loss: -227538.921875\n",
      "Train Epoch: 64 [29568/54000 (55%)] Loss: -218744.250000\n",
      "Train Epoch: 64 [30976/54000 (57%)] Loss: -215895.781250\n",
      "Train Epoch: 64 [32384/54000 (60%)] Loss: -244541.531250\n",
      "Train Epoch: 64 [33792/54000 (63%)] Loss: -219838.453125\n",
      "Train Epoch: 64 [35200/54000 (65%)] Loss: -219812.062500\n",
      "Train Epoch: 64 [36608/54000 (68%)] Loss: -214957.796875\n",
      "Train Epoch: 64 [38016/54000 (70%)] Loss: -227107.656250\n",
      "Train Epoch: 64 [39424/54000 (73%)] Loss: -214962.750000\n",
      "Train Epoch: 64 [40832/54000 (76%)] Loss: -213841.093750\n",
      "Train Epoch: 64 [42240/54000 (78%)] Loss: -224856.640625\n",
      "Train Epoch: 64 [43648/54000 (81%)] Loss: -221313.234375\n",
      "Train Epoch: 64 [45056/54000 (83%)] Loss: -244428.203125\n",
      "Train Epoch: 64 [46464/54000 (86%)] Loss: -217782.406250\n",
      "Train Epoch: 64 [47872/54000 (89%)] Loss: -221809.812500\n",
      "Train Epoch: 64 [49280/54000 (91%)] Loss: -213198.000000\n",
      "Train Epoch: 64 [50688/54000 (94%)] Loss: -226440.250000\n",
      "Train Epoch: 64 [52096/54000 (96%)] Loss: -217593.750000\n",
      "    epoch          : 64\n",
      "    loss           : -222088.3600104665\n",
      "    val_loss       : -228518.99161585365\n",
      "Train Epoch: 65 [0/54000 (0%)] Loss: -216634.843750\n",
      "Train Epoch: 65 [1408/54000 (3%)] Loss: -214691.531250\n",
      "Train Epoch: 65 [2816/54000 (5%)] Loss: -225028.343750\n",
      "Train Epoch: 65 [4224/54000 (8%)] Loss: -224018.515625\n",
      "Train Epoch: 65 [5632/54000 (10%)] Loss: -215583.250000\n",
      "Train Epoch: 65 [7040/54000 (13%)] Loss: -227547.171875\n",
      "Train Epoch: 65 [8448/54000 (16%)] Loss: -244870.828125\n",
      "Train Epoch: 65 [9856/54000 (18%)] Loss: -217070.562500\n",
      "Train Epoch: 65 [11264/54000 (21%)] Loss: -216858.484375\n",
      "Train Epoch: 65 [12672/54000 (23%)] Loss: -219495.093750\n",
      "Train Epoch: 65 [14080/54000 (26%)] Loss: -243707.125000\n",
      "Train Epoch: 65 [15488/54000 (29%)] Loss: -219818.250000\n",
      "Train Epoch: 65 [16896/54000 (31%)] Loss: -225301.531250\n",
      "Train Epoch: 65 [18304/54000 (34%)] Loss: -244529.125000\n",
      "Train Epoch: 65 [19712/54000 (37%)] Loss: -226697.640625\n",
      "Train Epoch: 65 [21120/54000 (39%)] Loss: -214501.875000\n",
      "Train Epoch: 65 [22528/54000 (42%)] Loss: -220357.281250\n",
      "Train Epoch: 65 [23936/54000 (44%)] Loss: -225172.015625\n",
      "Train Epoch: 65 [25344/54000 (47%)] Loss: -243492.312500\n",
      "Train Epoch: 65 [26752/54000 (50%)] Loss: -225507.421875\n",
      "Train Epoch: 65 [28160/54000 (52%)] Loss: -223013.640625\n",
      "Train Epoch: 65 [29568/54000 (55%)] Loss: -215748.171875\n",
      "Train Epoch: 65 [30976/54000 (57%)] Loss: -216973.406250\n",
      "Train Epoch: 65 [32384/54000 (60%)] Loss: -227246.125000\n",
      "Train Epoch: 65 [33792/54000 (63%)] Loss: -173778.718750\n",
      "Train Epoch: 65 [35200/54000 (65%)] Loss: -227686.671875\n",
      "Train Epoch: 65 [36608/54000 (68%)] Loss: -213157.750000\n",
      "Train Epoch: 65 [38016/54000 (70%)] Loss: -214844.718750\n",
      "Train Epoch: 65 [39424/54000 (73%)] Loss: -244044.703125\n",
      "Train Epoch: 65 [40832/54000 (76%)] Loss: -218135.875000\n",
      "Train Epoch: 65 [42240/54000 (78%)] Loss: -225107.109375\n",
      "Train Epoch: 65 [43648/54000 (81%)] Loss: -222650.640625\n",
      "Train Epoch: 65 [45056/54000 (83%)] Loss: -210971.125000\n",
      "Train Epoch: 65 [46464/54000 (86%)] Loss: -244666.031250\n",
      "Train Epoch: 65 [47872/54000 (89%)] Loss: -213484.484375\n",
      "Train Epoch: 65 [49280/54000 (91%)] Loss: -214529.281250\n",
      "Train Epoch: 65 [50688/54000 (94%)] Loss: -215002.968750\n",
      "Train Epoch: 65 [52096/54000 (96%)] Loss: -244276.718750\n",
      "    epoch          : 65\n",
      "    loss           : -222302.81892942585\n",
      "    val_loss       : -228629.32526676828\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch65.pth ...\n",
      "Train Epoch: 66 [0/54000 (0%)] Loss: -243365.015625\n",
      "Train Epoch: 66 [1408/54000 (3%)] Loss: -223766.968750\n",
      "Train Epoch: 66 [2816/54000 (5%)] Loss: -215386.765625\n",
      "Train Epoch: 66 [4224/54000 (8%)] Loss: -226763.281250\n",
      "Train Epoch: 66 [5632/54000 (10%)] Loss: -225799.406250\n",
      "Train Epoch: 66 [7040/54000 (13%)] Loss: -245438.750000\n",
      "Train Epoch: 66 [8448/54000 (16%)] Loss: -214384.203125\n",
      "Train Epoch: 66 [9856/54000 (18%)] Loss: -219769.281250\n",
      "Train Epoch: 66 [11264/54000 (21%)] Loss: -219220.640625\n",
      "Train Epoch: 66 [12672/54000 (23%)] Loss: -245018.546875\n",
      "Train Epoch: 66 [14080/54000 (26%)] Loss: -215677.406250\n",
      "Train Epoch: 66 [15488/54000 (29%)] Loss: -216093.562500\n",
      "Train Epoch: 66 [16896/54000 (31%)] Loss: -212800.812500\n",
      "Train Epoch: 66 [18304/54000 (34%)] Loss: -217140.046875\n",
      "Train Epoch: 66 [19712/54000 (37%)] Loss: -217258.375000\n",
      "Train Epoch: 66 [21120/54000 (39%)] Loss: -218080.187500\n",
      "Train Epoch: 66 [22528/54000 (42%)] Loss: -217350.500000\n",
      "Train Epoch: 66 [23936/54000 (44%)] Loss: -218603.828125\n",
      "Train Epoch: 66 [25344/54000 (47%)] Loss: -227797.937500\n",
      "Train Epoch: 66 [26752/54000 (50%)] Loss: -226858.875000\n",
      "Train Epoch: 66 [28160/54000 (52%)] Loss: -215360.250000\n",
      "Train Epoch: 66 [29568/54000 (55%)] Loss: -245405.468750\n",
      "Train Epoch: 66 [30976/54000 (57%)] Loss: -225657.015625\n",
      "Train Epoch: 66 [32384/54000 (60%)] Loss: -224775.656250\n",
      "Train Epoch: 66 [33792/54000 (63%)] Loss: -223970.375000\n",
      "Train Epoch: 66 [35200/54000 (65%)] Loss: -214930.500000\n",
      "Train Epoch: 66 [36608/54000 (68%)] Loss: -244166.625000\n",
      "Train Epoch: 66 [38016/54000 (70%)] Loss: -212733.906250\n",
      "Train Epoch: 66 [39424/54000 (73%)] Loss: -217700.671875\n",
      "Train Epoch: 66 [40832/54000 (76%)] Loss: -225868.828125\n",
      "Train Epoch: 66 [42240/54000 (78%)] Loss: -244288.359375\n",
      "Train Epoch: 66 [43648/54000 (81%)] Loss: -220381.093750\n",
      "Train Epoch: 66 [45056/54000 (83%)] Loss: -224236.125000\n",
      "Train Epoch: 66 [46464/54000 (86%)] Loss: -219358.281250\n",
      "Train Epoch: 66 [47872/54000 (89%)] Loss: -219146.046875\n",
      "Train Epoch: 66 [49280/54000 (91%)] Loss: -216248.921875\n",
      "Train Epoch: 66 [50688/54000 (94%)] Loss: -243211.375000\n",
      "Train Epoch: 66 [52096/54000 (96%)] Loss: -213216.562500\n",
      "    epoch          : 66\n",
      "    loss           : -222708.79982805025\n",
      "    val_loss       : -229030.5097179878\n",
      "Train Epoch: 67 [0/54000 (0%)] Loss: -222023.812500\n",
      "Train Epoch: 67 [1408/54000 (3%)] Loss: -226593.937500\n",
      "Train Epoch: 67 [2816/54000 (5%)] Loss: -215635.531250\n",
      "Train Epoch: 67 [4224/54000 (8%)] Loss: -244595.906250\n",
      "Train Epoch: 67 [5632/54000 (10%)] Loss: -218491.296875\n",
      "Train Epoch: 67 [7040/54000 (13%)] Loss: -209805.968750\n",
      "Train Epoch: 67 [8448/54000 (16%)] Loss: -219950.312500\n",
      "Train Epoch: 67 [9856/54000 (18%)] Loss: -215869.875000\n",
      "Train Epoch: 67 [11264/54000 (21%)] Loss: -213765.843750\n",
      "Train Epoch: 67 [12672/54000 (23%)] Loss: -213026.281250\n",
      "Train Epoch: 67 [14080/54000 (26%)] Loss: -219124.437500\n",
      "Train Epoch: 67 [15488/54000 (29%)] Loss: -222013.546875\n",
      "Train Epoch: 67 [16896/54000 (31%)] Loss: -216828.218750\n",
      "Train Epoch: 67 [18304/54000 (34%)] Loss: -220827.125000\n",
      "Train Epoch: 67 [19712/54000 (37%)] Loss: -223951.437500\n",
      "Train Epoch: 67 [21120/54000 (39%)] Loss: -222333.312500\n",
      "Train Epoch: 67 [22528/54000 (42%)] Loss: -222018.531250\n",
      "Train Epoch: 67 [23936/54000 (44%)] Loss: -221947.515625\n",
      "Train Epoch: 67 [25344/54000 (47%)] Loss: -214002.250000\n",
      "Train Epoch: 67 [26752/54000 (50%)] Loss: -216375.750000\n",
      "Train Epoch: 67 [28160/54000 (52%)] Loss: -216448.437500\n",
      "Train Epoch: 67 [29568/54000 (55%)] Loss: -224752.437500\n",
      "Train Epoch: 67 [30976/54000 (57%)] Loss: -226057.015625\n",
      "Train Epoch: 67 [32384/54000 (60%)] Loss: -221677.250000\n",
      "Train Epoch: 67 [33792/54000 (63%)] Loss: -220225.609375\n",
      "Train Epoch: 67 [35200/54000 (65%)] Loss: -221766.062500\n",
      "Train Epoch: 67 [36608/54000 (68%)] Loss: -219421.562500\n",
      "Train Epoch: 67 [38016/54000 (70%)] Loss: -218648.640625\n",
      "Train Epoch: 67 [39424/54000 (73%)] Loss: -216078.812500\n",
      "Train Epoch: 67 [40832/54000 (76%)] Loss: -223626.875000\n",
      "Train Epoch: 67 [42240/54000 (78%)] Loss: -214137.531250\n",
      "Train Epoch: 67 [43648/54000 (81%)] Loss: -223843.906250\n",
      "Train Epoch: 67 [45056/54000 (83%)] Loss: -244533.187500\n",
      "Train Epoch: 67 [46464/54000 (86%)] Loss: -216202.093750\n",
      "Train Epoch: 67 [47872/54000 (89%)] Loss: -226391.234375\n",
      "Train Epoch: 67 [49280/54000 (91%)] Loss: -226128.375000\n",
      "Train Epoch: 67 [50688/54000 (94%)] Loss: -221675.562500\n",
      "Train Epoch: 67 [52096/54000 (96%)] Loss: -216306.421875\n",
      "    epoch          : 67\n",
      "    loss           : -222479.67768764953\n",
      "    val_loss       : -228873.93330792684\n",
      "Train Epoch: 68 [0/54000 (0%)] Loss: -219655.078125\n",
      "Train Epoch: 68 [1408/54000 (3%)] Loss: -245325.390625\n",
      "Train Epoch: 68 [2816/54000 (5%)] Loss: -228932.781250\n",
      "Train Epoch: 68 [4224/54000 (8%)] Loss: -212861.218750\n",
      "Train Epoch: 68 [5632/54000 (10%)] Loss: -227391.093750\n",
      "Train Epoch: 68 [7040/54000 (13%)] Loss: -220471.484375\n",
      "Train Epoch: 68 [8448/54000 (16%)] Loss: -219202.437500\n",
      "Train Epoch: 68 [9856/54000 (18%)] Loss: -219778.593750\n",
      "Train Epoch: 68 [11264/54000 (21%)] Loss: -228062.656250\n",
      "Train Epoch: 68 [12672/54000 (23%)] Loss: -222251.546875\n",
      "Train Epoch: 68 [14080/54000 (26%)] Loss: -215180.593750\n",
      "Train Epoch: 68 [15488/54000 (29%)] Loss: -218219.468750\n",
      "Train Epoch: 68 [16896/54000 (31%)] Loss: -225684.125000\n",
      "Train Epoch: 68 [18304/54000 (34%)] Loss: -219307.937500\n",
      "Train Epoch: 68 [19712/54000 (37%)] Loss: -213732.359375\n",
      "Train Epoch: 68 [21120/54000 (39%)] Loss: -214681.421875\n",
      "Train Epoch: 68 [22528/54000 (42%)] Loss: -243669.500000\n",
      "Train Epoch: 68 [23936/54000 (44%)] Loss: -222806.375000\n",
      "Train Epoch: 68 [25344/54000 (47%)] Loss: -219599.843750\n",
      "Train Epoch: 68 [26752/54000 (50%)] Loss: -228034.281250\n",
      "Train Epoch: 68 [28160/54000 (52%)] Loss: -227895.968750\n",
      "Train Epoch: 68 [29568/54000 (55%)] Loss: -224934.281250\n",
      "Train Epoch: 68 [30976/54000 (57%)] Loss: -211531.687500\n",
      "Train Epoch: 68 [32384/54000 (60%)] Loss: -216755.703125\n",
      "Train Epoch: 68 [33792/54000 (63%)] Loss: -216983.015625\n",
      "Train Epoch: 68 [35200/54000 (65%)] Loss: -216268.875000\n",
      "Train Epoch: 68 [36608/54000 (68%)] Loss: -214061.375000\n",
      "Train Epoch: 68 [38016/54000 (70%)] Loss: -228365.687500\n",
      "Train Epoch: 68 [39424/54000 (73%)] Loss: -218893.296875\n",
      "Train Epoch: 68 [40832/54000 (76%)] Loss: -220330.656250\n",
      "Train Epoch: 68 [42240/54000 (78%)] Loss: -223491.625000\n",
      "Train Epoch: 68 [43648/54000 (81%)] Loss: -223620.062500\n",
      "Train Epoch: 68 [45056/54000 (83%)] Loss: -225966.187500\n",
      "Train Epoch: 68 [46464/54000 (86%)] Loss: -217375.484375\n",
      "Train Epoch: 68 [47872/54000 (89%)] Loss: -216410.953125\n",
      "Train Epoch: 68 [49280/54000 (91%)] Loss: -220240.171875\n",
      "Train Epoch: 68 [50688/54000 (94%)] Loss: -225029.375000\n",
      "Train Epoch: 68 [52096/54000 (96%)] Loss: -225041.625000\n",
      "    epoch          : 68\n",
      "    loss           : -223033.75927033494\n",
      "    val_loss       : -229084.32450457316\n",
      "Train Epoch: 69 [0/54000 (0%)] Loss: -217051.953125\n",
      "Train Epoch: 69 [1408/54000 (3%)] Loss: -213904.031250\n",
      "Train Epoch: 69 [2816/54000 (5%)] Loss: -228448.359375\n",
      "Train Epoch: 69 [4224/54000 (8%)] Loss: -216446.515625\n",
      "Train Epoch: 69 [5632/54000 (10%)] Loss: -219336.671875\n",
      "Train Epoch: 69 [7040/54000 (13%)] Loss: -220903.187500\n",
      "Train Epoch: 69 [8448/54000 (16%)] Loss: -245307.187500\n",
      "Train Epoch: 69 [9856/54000 (18%)] Loss: -245497.015625\n",
      "Train Epoch: 69 [11264/54000 (21%)] Loss: -217448.015625\n",
      "Train Epoch: 69 [12672/54000 (23%)] Loss: -224968.500000\n",
      "Train Epoch: 69 [14080/54000 (26%)] Loss: -225950.000000\n",
      "Train Epoch: 69 [15488/54000 (29%)] Loss: -245806.546875\n",
      "Train Epoch: 69 [16896/54000 (31%)] Loss: -218569.078125\n",
      "Train Epoch: 69 [18304/54000 (34%)] Loss: -219903.750000\n",
      "Train Epoch: 69 [19712/54000 (37%)] Loss: -221357.250000\n",
      "Train Epoch: 69 [21120/54000 (39%)] Loss: -228233.406250\n",
      "Train Epoch: 69 [22528/54000 (42%)] Loss: -244914.843750\n",
      "Train Epoch: 69 [23936/54000 (44%)] Loss: -216530.312500\n",
      "Train Epoch: 69 [25344/54000 (47%)] Loss: -219825.390625\n",
      "Train Epoch: 69 [26752/54000 (50%)] Loss: -216645.125000\n",
      "Train Epoch: 69 [28160/54000 (52%)] Loss: -220306.296875\n",
      "Train Epoch: 69 [29568/54000 (55%)] Loss: -213592.718750\n",
      "Train Epoch: 69 [30976/54000 (57%)] Loss: -222921.531250\n",
      "Train Epoch: 69 [32384/54000 (60%)] Loss: -216836.406250\n",
      "Train Epoch: 69 [33792/54000 (63%)] Loss: -211702.171875\n",
      "Train Epoch: 69 [35200/54000 (65%)] Loss: -224816.843750\n",
      "Train Epoch: 69 [36608/54000 (68%)] Loss: -217818.750000\n",
      "Train Epoch: 69 [38016/54000 (70%)] Loss: -227964.453125\n",
      "Train Epoch: 69 [39424/54000 (73%)] Loss: -244516.984375\n",
      "Train Epoch: 69 [40832/54000 (76%)] Loss: -214578.875000\n",
      "Train Epoch: 69 [42240/54000 (78%)] Loss: -214820.687500\n",
      "Train Epoch: 69 [43648/54000 (81%)] Loss: -219757.593750\n",
      "Train Epoch: 69 [45056/54000 (83%)] Loss: -218888.671875\n",
      "Train Epoch: 69 [46464/54000 (86%)] Loss: -220042.156250\n",
      "Train Epoch: 69 [47872/54000 (89%)] Loss: -217117.843750\n",
      "Train Epoch: 69 [49280/54000 (91%)] Loss: -222088.312500\n",
      "Train Epoch: 69 [50688/54000 (94%)] Loss: -224357.906250\n",
      "Train Epoch: 69 [52096/54000 (96%)] Loss: -224757.250000\n",
      "    epoch          : 69\n",
      "    loss           : -223110.54616477274\n",
      "    val_loss       : -229250.09832317074\n",
      "Train Epoch: 70 [0/54000 (0%)] Loss: -245093.812500\n",
      "Train Epoch: 70 [1408/54000 (3%)] Loss: -217365.625000\n",
      "Train Epoch: 70 [2816/54000 (5%)] Loss: -215023.812500\n",
      "Train Epoch: 70 [4224/54000 (8%)] Loss: -215242.015625\n",
      "Train Epoch: 70 [5632/54000 (10%)] Loss: -213290.734375\n",
      "Train Epoch: 70 [7040/54000 (13%)] Loss: -219234.250000\n",
      "Train Epoch: 70 [8448/54000 (16%)] Loss: -219776.593750\n",
      "Train Epoch: 70 [9856/54000 (18%)] Loss: -215601.734375\n",
      "Train Epoch: 70 [11264/54000 (21%)] Loss: -227137.109375\n",
      "Train Epoch: 70 [12672/54000 (23%)] Loss: -228891.062500\n",
      "Train Epoch: 70 [14080/54000 (26%)] Loss: -214396.750000\n",
      "Train Epoch: 70 [15488/54000 (29%)] Loss: -227701.046875\n",
      "Train Epoch: 70 [16896/54000 (31%)] Loss: -219720.843750\n",
      "Train Epoch: 70 [18304/54000 (34%)] Loss: -218774.406250\n",
      "Train Epoch: 70 [19712/54000 (37%)] Loss: -213384.890625\n",
      "Train Epoch: 70 [21120/54000 (39%)] Loss: -228515.468750\n",
      "Train Epoch: 70 [22528/54000 (42%)] Loss: -216490.281250\n",
      "Train Epoch: 70 [23936/54000 (44%)] Loss: -215305.843750\n",
      "Train Epoch: 70 [25344/54000 (47%)] Loss: -215657.906250\n",
      "Train Epoch: 70 [26752/54000 (50%)] Loss: -221057.062500\n",
      "Train Epoch: 70 [28160/54000 (52%)] Loss: -218449.875000\n",
      "Train Epoch: 70 [29568/54000 (55%)] Loss: -219372.062500\n",
      "Train Epoch: 70 [30976/54000 (57%)] Loss: -225595.031250\n",
      "Train Epoch: 70 [32384/54000 (60%)] Loss: -224044.875000\n",
      "Train Epoch: 70 [33792/54000 (63%)] Loss: -216635.703125\n",
      "Train Epoch: 70 [35200/54000 (65%)] Loss: -229268.562500\n",
      "Train Epoch: 70 [36608/54000 (68%)] Loss: -227495.250000\n",
      "Train Epoch: 70 [38016/54000 (70%)] Loss: -220173.156250\n",
      "Train Epoch: 70 [39424/54000 (73%)] Loss: -224555.546875\n",
      "Train Epoch: 70 [40832/54000 (76%)] Loss: -214637.109375\n",
      "Train Epoch: 70 [42240/54000 (78%)] Loss: -219128.187500\n",
      "Train Epoch: 70 [43648/54000 (81%)] Loss: -219794.109375\n",
      "Train Epoch: 70 [45056/54000 (83%)] Loss: -213295.718750\n",
      "Train Epoch: 70 [46464/54000 (86%)] Loss: -215474.093750\n",
      "Train Epoch: 70 [47872/54000 (89%)] Loss: -224004.750000\n",
      "Train Epoch: 70 [49280/54000 (91%)] Loss: -222652.953125\n",
      "Train Epoch: 70 [50688/54000 (94%)] Loss: -244929.937500\n",
      "Train Epoch: 70 [52096/54000 (96%)] Loss: -227199.828125\n",
      "    epoch          : 70\n",
      "    loss           : -222983.14600777513\n",
      "    val_loss       : -228805.83631859755\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch70.pth ...\n",
      "Train Epoch: 71 [0/54000 (0%)] Loss: -245333.281250\n",
      "Train Epoch: 71 [1408/54000 (3%)] Loss: -239822.468750\n",
      "Train Epoch: 71 [2816/54000 (5%)] Loss: -244808.765625\n",
      "Train Epoch: 71 [4224/54000 (8%)] Loss: -228032.593750\n",
      "Train Epoch: 71 [5632/54000 (10%)] Loss: -211489.031250\n",
      "Train Epoch: 71 [7040/54000 (13%)] Loss: -226699.375000\n",
      "Train Epoch: 71 [8448/54000 (16%)] Loss: -228951.062500\n",
      "Train Epoch: 71 [9856/54000 (18%)] Loss: -226210.843750\n",
      "Train Epoch: 71 [11264/54000 (21%)] Loss: -218496.531250\n",
      "Train Epoch: 71 [12672/54000 (23%)] Loss: -246165.062500\n",
      "Train Epoch: 71 [14080/54000 (26%)] Loss: -221182.281250\n",
      "Train Epoch: 71 [15488/54000 (29%)] Loss: -224097.921875\n",
      "Train Epoch: 71 [16896/54000 (31%)] Loss: -223567.468750\n",
      "Train Epoch: 71 [18304/54000 (34%)] Loss: -214112.437500\n",
      "Train Epoch: 71 [19712/54000 (37%)] Loss: -115763.093750\n",
      "Train Epoch: 71 [21120/54000 (39%)] Loss: -218911.265625\n",
      "Train Epoch: 71 [22528/54000 (42%)] Loss: -226163.906250\n",
      "Train Epoch: 71 [23936/54000 (44%)] Loss: -226908.000000\n",
      "Train Epoch: 71 [25344/54000 (47%)] Loss: -219436.812500\n",
      "Train Epoch: 71 [26752/54000 (50%)] Loss: -227731.500000\n",
      "Train Epoch: 71 [28160/54000 (52%)] Loss: -215378.437500\n",
      "Train Epoch: 71 [29568/54000 (55%)] Loss: -227365.500000\n",
      "Train Epoch: 71 [30976/54000 (57%)] Loss: -228351.531250\n",
      "Train Epoch: 71 [32384/54000 (60%)] Loss: -229032.343750\n",
      "Train Epoch: 71 [33792/54000 (63%)] Loss: -228727.218750\n",
      "Train Epoch: 71 [35200/54000 (65%)] Loss: -227288.218750\n",
      "Train Epoch: 71 [36608/54000 (68%)] Loss: -217414.500000\n",
      "Train Epoch: 71 [38016/54000 (70%)] Loss: -246018.421875\n",
      "Train Epoch: 71 [39424/54000 (73%)] Loss: -226477.953125\n",
      "Train Epoch: 71 [40832/54000 (76%)] Loss: -216072.531250\n",
      "Train Epoch: 71 [42240/54000 (78%)] Loss: -214982.609375\n",
      "Train Epoch: 71 [43648/54000 (81%)] Loss: -214602.781250\n",
      "Train Epoch: 71 [45056/54000 (83%)] Loss: -227763.656250\n",
      "Train Epoch: 71 [46464/54000 (86%)] Loss: -214620.656250\n",
      "Train Epoch: 71 [47872/54000 (89%)] Loss: -217477.562500\n",
      "Train Epoch: 71 [49280/54000 (91%)] Loss: -213290.718750\n",
      "Train Epoch: 71 [50688/54000 (94%)] Loss: -216641.234375\n",
      "Train Epoch: 71 [52096/54000 (96%)] Loss: -226534.562500\n",
      "    epoch          : 71\n",
      "    loss           : -222791.35911333733\n",
      "    val_loss       : -229147.37557164635\n",
      "Train Epoch: 72 [0/54000 (0%)] Loss: -226932.703125\n",
      "Train Epoch: 72 [1408/54000 (3%)] Loss: -246032.093750\n",
      "Train Epoch: 72 [2816/54000 (5%)] Loss: -226731.796875\n",
      "Train Epoch: 72 [4224/54000 (8%)] Loss: -227402.359375\n",
      "Train Epoch: 72 [5632/54000 (10%)] Loss: -228743.093750\n",
      "Train Epoch: 72 [7040/54000 (13%)] Loss: -246577.765625\n",
      "Train Epoch: 72 [8448/54000 (16%)] Loss: -213247.718750\n",
      "Train Epoch: 72 [9856/54000 (18%)] Loss: -215340.093750\n",
      "Train Epoch: 72 [11264/54000 (21%)] Loss: -214199.718750\n",
      "Train Epoch: 72 [12672/54000 (23%)] Loss: -222412.906250\n",
      "Train Epoch: 72 [14080/54000 (26%)] Loss: -244939.734375\n",
      "Train Epoch: 72 [15488/54000 (29%)] Loss: -213149.609375\n",
      "Train Epoch: 72 [16896/54000 (31%)] Loss: -215568.531250\n",
      "Train Epoch: 72 [18304/54000 (34%)] Loss: -218831.421875\n",
      "Train Epoch: 72 [19712/54000 (37%)] Loss: -221357.640625\n",
      "Train Epoch: 72 [21120/54000 (39%)] Loss: -219174.609375\n",
      "Train Epoch: 72 [22528/54000 (42%)] Loss: -214919.218750\n",
      "Train Epoch: 72 [23936/54000 (44%)] Loss: -213944.156250\n",
      "Train Epoch: 72 [25344/54000 (47%)] Loss: -223358.187500\n",
      "Train Epoch: 72 [26752/54000 (50%)] Loss: -222420.343750\n",
      "Train Epoch: 72 [28160/54000 (52%)] Loss: -219396.812500\n",
      "Train Epoch: 72 [29568/54000 (55%)] Loss: -218276.281250\n",
      "Train Epoch: 72 [30976/54000 (57%)] Loss: -214296.250000\n",
      "Train Epoch: 72 [32384/54000 (60%)] Loss: -226096.109375\n",
      "Train Epoch: 72 [33792/54000 (63%)] Loss: -228138.218750\n",
      "Train Epoch: 72 [35200/54000 (65%)] Loss: -219059.531250\n",
      "Train Epoch: 72 [36608/54000 (68%)] Loss: -228701.968750\n",
      "Train Epoch: 72 [38016/54000 (70%)] Loss: -213095.546875\n",
      "Train Epoch: 72 [39424/54000 (73%)] Loss: -245072.359375\n",
      "Train Epoch: 72 [40832/54000 (76%)] Loss: -222680.265625\n",
      "Train Epoch: 72 [42240/54000 (78%)] Loss: -220330.000000\n",
      "Train Epoch: 72 [43648/54000 (81%)] Loss: -220084.843750\n",
      "Train Epoch: 72 [45056/54000 (83%)] Loss: -245079.421875\n",
      "Train Epoch: 72 [46464/54000 (86%)] Loss: -214983.453125\n",
      "Train Epoch: 72 [47872/54000 (89%)] Loss: -219944.312500\n",
      "Train Epoch: 72 [49280/54000 (91%)] Loss: -217126.109375\n",
      "Train Epoch: 72 [50688/54000 (94%)] Loss: -215079.343750\n",
      "Train Epoch: 72 [52096/54000 (96%)] Loss: -227448.593750\n",
      "    epoch          : 72\n",
      "    loss           : -223456.53203498805\n",
      "    val_loss       : -229358.79858993902\n",
      "Train Epoch: 73 [0/54000 (0%)] Loss: -227875.203125\n",
      "Train Epoch: 73 [1408/54000 (3%)] Loss: -245156.421875\n",
      "Train Epoch: 73 [2816/54000 (5%)] Loss: -212938.750000\n",
      "Train Epoch: 73 [4224/54000 (8%)] Loss: -220256.250000\n",
      "Train Epoch: 73 [5632/54000 (10%)] Loss: -220428.953125\n",
      "Train Epoch: 73 [7040/54000 (13%)] Loss: -218631.500000\n",
      "Train Epoch: 73 [8448/54000 (16%)] Loss: -247225.203125\n",
      "Train Epoch: 73 [9856/54000 (18%)] Loss: -214597.859375\n",
      "Train Epoch: 73 [11264/54000 (21%)] Loss: -228582.406250\n",
      "Train Epoch: 73 [12672/54000 (23%)] Loss: -219950.500000\n",
      "Train Epoch: 73 [14080/54000 (26%)] Loss: -222424.765625\n",
      "Train Epoch: 73 [15488/54000 (29%)] Loss: -245455.812500\n",
      "Train Epoch: 73 [16896/54000 (31%)] Loss: -217507.750000\n",
      "Train Epoch: 73 [18304/54000 (34%)] Loss: -217773.875000\n",
      "Train Epoch: 73 [19712/54000 (37%)] Loss: -245441.234375\n",
      "Train Epoch: 73 [21120/54000 (39%)] Loss: -225073.781250\n",
      "Train Epoch: 73 [22528/54000 (42%)] Loss: -221598.906250\n",
      "Train Epoch: 73 [23936/54000 (44%)] Loss: -223558.218750\n",
      "Train Epoch: 73 [25344/54000 (47%)] Loss: -224251.609375\n",
      "Train Epoch: 73 [26752/54000 (50%)] Loss: -215228.218750\n",
      "Train Epoch: 73 [28160/54000 (52%)] Loss: -211798.312500\n",
      "Train Epoch: 73 [29568/54000 (55%)] Loss: -217719.421875\n",
      "Train Epoch: 73 [30976/54000 (57%)] Loss: -219975.265625\n",
      "Train Epoch: 73 [32384/54000 (60%)] Loss: -245927.421875\n",
      "Train Epoch: 73 [33792/54000 (63%)] Loss: -226740.640625\n",
      "Train Epoch: 73 [35200/54000 (65%)] Loss: -220150.031250\n",
      "Train Epoch: 73 [36608/54000 (68%)] Loss: -215576.906250\n",
      "Train Epoch: 73 [38016/54000 (70%)] Loss: -214527.125000\n",
      "Train Epoch: 73 [39424/54000 (73%)] Loss: -212849.281250\n",
      "Train Epoch: 73 [40832/54000 (76%)] Loss: -223799.671875\n",
      "Train Epoch: 73 [42240/54000 (78%)] Loss: -214963.156250\n",
      "Train Epoch: 73 [43648/54000 (81%)] Loss: -215581.812500\n",
      "Train Epoch: 73 [45056/54000 (83%)] Loss: -224148.281250\n",
      "Train Epoch: 73 [46464/54000 (86%)] Loss: -219588.390625\n",
      "Train Epoch: 73 [47872/54000 (89%)] Loss: -219002.843750\n",
      "Train Epoch: 73 [49280/54000 (91%)] Loss: -217322.593750\n",
      "Train Epoch: 73 [50688/54000 (94%)] Loss: -245435.343750\n",
      "Train Epoch: 73 [52096/54000 (96%)] Loss: -215558.625000\n",
      "    epoch          : 73\n",
      "    loss           : -223691.7408791866\n",
      "    val_loss       : -229133.90948932926\n",
      "Train Epoch: 74 [0/54000 (0%)] Loss: -221570.609375\n",
      "Train Epoch: 74 [1408/54000 (3%)] Loss: -216234.359375\n",
      "Train Epoch: 74 [2816/54000 (5%)] Loss: -211210.468750\n",
      "Train Epoch: 74 [4224/54000 (8%)] Loss: -217035.656250\n",
      "Train Epoch: 74 [5632/54000 (10%)] Loss: -214743.062500\n",
      "Train Epoch: 74 [7040/54000 (13%)] Loss: -212545.250000\n",
      "Train Epoch: 74 [8448/54000 (16%)] Loss: -229711.593750\n",
      "Train Epoch: 74 [9856/54000 (18%)] Loss: -216280.609375\n",
      "Train Epoch: 74 [11264/54000 (21%)] Loss: -220124.562500\n",
      "Train Epoch: 74 [12672/54000 (23%)] Loss: -217903.125000\n",
      "Train Epoch: 74 [14080/54000 (26%)] Loss: -220061.578125\n",
      "Train Epoch: 74 [15488/54000 (29%)] Loss: -216375.937500\n",
      "Train Epoch: 74 [16896/54000 (31%)] Loss: -216079.171875\n",
      "Train Epoch: 74 [18304/54000 (34%)] Loss: -246373.265625\n",
      "Train Epoch: 74 [19712/54000 (37%)] Loss: -223298.406250\n",
      "Train Epoch: 74 [21120/54000 (39%)] Loss: -216465.656250\n",
      "Train Epoch: 74 [22528/54000 (42%)] Loss: -222753.406250\n",
      "Train Epoch: 74 [23936/54000 (44%)] Loss: -222330.375000\n",
      "Train Epoch: 74 [25344/54000 (47%)] Loss: -222640.625000\n",
      "Train Epoch: 74 [26752/54000 (50%)] Loss: -224459.250000\n",
      "Train Epoch: 74 [28160/54000 (52%)] Loss: -221000.125000\n",
      "Train Epoch: 74 [29568/54000 (55%)] Loss: -220875.312500\n",
      "Train Epoch: 74 [30976/54000 (57%)] Loss: -228433.437500\n",
      "Train Epoch: 74 [32384/54000 (60%)] Loss: -218412.375000\n",
      "Train Epoch: 74 [33792/54000 (63%)] Loss: -227978.000000\n",
      "Train Epoch: 74 [35200/54000 (65%)] Loss: -229101.234375\n",
      "Train Epoch: 74 [36608/54000 (68%)] Loss: -228761.843750\n",
      "Train Epoch: 74 [38016/54000 (70%)] Loss: -222299.640625\n",
      "Train Epoch: 74 [39424/54000 (73%)] Loss: -224839.140625\n",
      "Train Epoch: 74 [40832/54000 (76%)] Loss: -222240.406250\n",
      "Train Epoch: 74 [42240/54000 (78%)] Loss: -224583.843750\n",
      "Train Epoch: 74 [43648/54000 (81%)] Loss: -223961.312500\n",
      "Train Epoch: 74 [45056/54000 (83%)] Loss: -214664.203125\n",
      "Train Epoch: 74 [46464/54000 (86%)] Loss: -245917.109375\n",
      "Train Epoch: 74 [47872/54000 (89%)] Loss: -217979.406250\n",
      "Train Epoch: 74 [49280/54000 (91%)] Loss: -217848.156250\n",
      "Train Epoch: 74 [50688/54000 (94%)] Loss: -246365.656250\n",
      "Train Epoch: 74 [52096/54000 (96%)] Loss: -213325.156250\n",
      "    epoch          : 74\n",
      "    loss           : -223827.59176883972\n",
      "    val_loss       : -229399.12042682926\n",
      "Train Epoch: 75 [0/54000 (0%)] Loss: -227408.609375\n",
      "Train Epoch: 75 [1408/54000 (3%)] Loss: -226507.406250\n",
      "Train Epoch: 75 [2816/54000 (5%)] Loss: -215304.125000\n",
      "Train Epoch: 75 [4224/54000 (8%)] Loss: -227608.625000\n",
      "Train Epoch: 75 [5632/54000 (10%)] Loss: -219701.015625\n",
      "Train Epoch: 75 [7040/54000 (13%)] Loss: -218601.078125\n",
      "Train Epoch: 75 [8448/54000 (16%)] Loss: -245505.406250\n",
      "Train Epoch: 75 [9856/54000 (18%)] Loss: -215278.312500\n",
      "Train Epoch: 75 [11264/54000 (21%)] Loss: -218543.781250\n",
      "Train Epoch: 75 [12672/54000 (23%)] Loss: -218901.890625\n",
      "Train Epoch: 75 [14080/54000 (26%)] Loss: -217943.921875\n",
      "Train Epoch: 75 [15488/54000 (29%)] Loss: -246303.250000\n",
      "Train Epoch: 75 [16896/54000 (31%)] Loss: -226249.671875\n",
      "Train Epoch: 75 [18304/54000 (34%)] Loss: -228022.578125\n",
      "Train Epoch: 75 [19712/54000 (37%)] Loss: -216987.515625\n",
      "Train Epoch: 75 [21120/54000 (39%)] Loss: -246143.343750\n",
      "Train Epoch: 75 [22528/54000 (42%)] Loss: -226047.343750\n",
      "Train Epoch: 75 [23936/54000 (44%)] Loss: -218679.687500\n",
      "Train Epoch: 75 [25344/54000 (47%)] Loss: -216352.062500\n",
      "Train Epoch: 75 [26752/54000 (50%)] Loss: -220990.640625\n",
      "Train Epoch: 75 [28160/54000 (52%)] Loss: -245906.093750\n",
      "Train Epoch: 75 [29568/54000 (55%)] Loss: -215791.031250\n",
      "Train Epoch: 75 [30976/54000 (57%)] Loss: -214131.015625\n",
      "Train Epoch: 75 [32384/54000 (60%)] Loss: -226334.921875\n",
      "Train Epoch: 75 [33792/54000 (63%)] Loss: -246891.093750\n",
      "Train Epoch: 75 [35200/54000 (65%)] Loss: -216411.484375\n",
      "Train Epoch: 75 [36608/54000 (68%)] Loss: -215312.437500\n",
      "Train Epoch: 75 [38016/54000 (70%)] Loss: -245831.250000\n",
      "Train Epoch: 75 [39424/54000 (73%)] Loss: -217097.500000\n",
      "Train Epoch: 75 [40832/54000 (76%)] Loss: -226392.046875\n",
      "Train Epoch: 75 [42240/54000 (78%)] Loss: -217860.656250\n",
      "Train Epoch: 75 [43648/54000 (81%)] Loss: -217036.000000\n",
      "Train Epoch: 75 [45056/54000 (83%)] Loss: -228701.703125\n",
      "Train Epoch: 75 [46464/54000 (86%)] Loss: -229238.718750\n",
      "Train Epoch: 75 [47872/54000 (89%)] Loss: -228766.046875\n",
      "Train Epoch: 75 [49280/54000 (91%)] Loss: -229950.375000\n",
      "Train Epoch: 75 [50688/54000 (94%)] Loss: -222174.140625\n",
      "Train Epoch: 75 [52096/54000 (96%)] Loss: -225123.000000\n",
      "    epoch          : 75\n",
      "    loss           : -223885.36614084928\n",
      "    val_loss       : -228644.34794207316\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch75.pth ...\n",
      "Train Epoch: 76 [0/54000 (0%)] Loss: -228525.687500\n",
      "Train Epoch: 76 [1408/54000 (3%)] Loss: -219765.171875\n",
      "Train Epoch: 76 [2816/54000 (5%)] Loss: -220865.359375\n",
      "Train Epoch: 76 [4224/54000 (8%)] Loss: -228973.187500\n",
      "Train Epoch: 76 [5632/54000 (10%)] Loss: -216312.359375\n",
      "Train Epoch: 76 [7040/54000 (13%)] Loss: -215870.265625\n",
      "Train Epoch: 76 [8448/54000 (16%)] Loss: -217644.468750\n",
      "Train Epoch: 76 [9856/54000 (18%)] Loss: -214293.906250\n",
      "Train Epoch: 76 [11264/54000 (21%)] Loss: -217786.078125\n",
      "Train Epoch: 76 [12672/54000 (23%)] Loss: -227612.062500\n",
      "Train Epoch: 76 [14080/54000 (26%)] Loss: -245659.187500\n",
      "Train Epoch: 76 [15488/54000 (29%)] Loss: -220537.984375\n",
      "Train Epoch: 76 [16896/54000 (31%)] Loss: -222781.781250\n",
      "Train Epoch: 76 [18304/54000 (34%)] Loss: -221706.640625\n",
      "Train Epoch: 76 [19712/54000 (37%)] Loss: -245632.078125\n",
      "Train Epoch: 76 [21120/54000 (39%)] Loss: -209725.406250\n",
      "Train Epoch: 76 [22528/54000 (42%)] Loss: -219340.203125\n",
      "Train Epoch: 76 [23936/54000 (44%)] Loss: -221763.687500\n",
      "Train Epoch: 76 [25344/54000 (47%)] Loss: -222310.593750\n",
      "Train Epoch: 76 [26752/54000 (50%)] Loss: -228805.109375\n",
      "Train Epoch: 76 [28160/54000 (52%)] Loss: -212646.843750\n",
      "Train Epoch: 76 [29568/54000 (55%)] Loss: -217085.609375\n",
      "Train Epoch: 76 [30976/54000 (57%)] Loss: -226548.125000\n",
      "Train Epoch: 76 [32384/54000 (60%)] Loss: -225937.843750\n",
      "Train Epoch: 76 [33792/54000 (63%)] Loss: -220166.000000\n",
      "Train Epoch: 76 [35200/54000 (65%)] Loss: -216807.625000\n",
      "Train Epoch: 76 [36608/54000 (68%)] Loss: -213717.218750\n",
      "Train Epoch: 76 [38016/54000 (70%)] Loss: -246400.828125\n",
      "Train Epoch: 76 [39424/54000 (73%)] Loss: -217568.437500\n",
      "Train Epoch: 76 [40832/54000 (76%)] Loss: -227684.296875\n",
      "Train Epoch: 76 [42240/54000 (78%)] Loss: -214099.750000\n",
      "Train Epoch: 76 [43648/54000 (81%)] Loss: -216310.187500\n",
      "Train Epoch: 76 [45056/54000 (83%)] Loss: -246074.906250\n",
      "Train Epoch: 76 [46464/54000 (86%)] Loss: -229023.468750\n",
      "Train Epoch: 76 [47872/54000 (89%)] Loss: -227668.218750\n",
      "Train Epoch: 76 [49280/54000 (91%)] Loss: -218624.656250\n",
      "Train Epoch: 76 [50688/54000 (94%)] Loss: -243903.578125\n",
      "Train Epoch: 76 [52096/54000 (96%)] Loss: -224980.203125\n",
      "    epoch          : 76\n",
      "    loss           : -223821.38307416267\n",
      "    val_loss       : -229176.20464939025\n",
      "Train Epoch: 77 [0/54000 (0%)] Loss: -219604.750000\n",
      "Train Epoch: 77 [1408/54000 (3%)] Loss: -244876.031250\n",
      "Train Epoch: 77 [2816/54000 (5%)] Loss: -218413.468750\n",
      "Train Epoch: 77 [4224/54000 (8%)] Loss: -217045.265625\n",
      "Train Epoch: 77 [5632/54000 (10%)] Loss: -216658.000000\n",
      "Train Epoch: 77 [7040/54000 (13%)] Loss: -216195.500000\n",
      "Train Epoch: 77 [8448/54000 (16%)] Loss: -245783.640625\n",
      "Train Epoch: 77 [9856/54000 (18%)] Loss: -222462.859375\n",
      "Train Epoch: 77 [11264/54000 (21%)] Loss: -216376.593750\n",
      "Train Epoch: 77 [12672/54000 (23%)] Loss: -227654.234375\n",
      "Train Epoch: 77 [14080/54000 (26%)] Loss: -228599.296875\n",
      "Train Epoch: 77 [15488/54000 (29%)] Loss: -247062.015625\n",
      "Train Epoch: 77 [16896/54000 (31%)] Loss: -221569.859375\n",
      "Train Epoch: 77 [18304/54000 (34%)] Loss: -217992.000000\n",
      "Train Epoch: 77 [19712/54000 (37%)] Loss: -219257.218750\n",
      "Train Epoch: 77 [21120/54000 (39%)] Loss: -246082.500000\n",
      "Train Epoch: 77 [22528/54000 (42%)] Loss: -220192.250000\n",
      "Train Epoch: 77 [23936/54000 (44%)] Loss: -224809.843750\n",
      "Train Epoch: 77 [25344/54000 (47%)] Loss: -215856.765625\n",
      "Train Epoch: 77 [26752/54000 (50%)] Loss: -216056.390625\n",
      "Train Epoch: 77 [28160/54000 (52%)] Loss: -211705.796875\n",
      "Train Epoch: 77 [29568/54000 (55%)] Loss: -215806.218750\n",
      "Train Epoch: 77 [30976/54000 (57%)] Loss: -215051.906250\n",
      "Train Epoch: 77 [32384/54000 (60%)] Loss: -219307.375000\n",
      "Train Epoch: 77 [33792/54000 (63%)] Loss: -226114.500000\n",
      "Train Epoch: 77 [35200/54000 (65%)] Loss: -246758.671875\n",
      "Train Epoch: 77 [36608/54000 (68%)] Loss: -213524.781250\n",
      "Train Epoch: 77 [38016/54000 (70%)] Loss: -228222.531250\n",
      "Train Epoch: 77 [39424/54000 (73%)] Loss: -226240.765625\n",
      "Train Epoch: 77 [40832/54000 (76%)] Loss: -219855.125000\n",
      "Train Epoch: 77 [42240/54000 (78%)] Loss: -216853.546875\n",
      "Train Epoch: 77 [43648/54000 (81%)] Loss: -224685.609375\n",
      "Train Epoch: 77 [45056/54000 (83%)] Loss: -224344.312500\n",
      "Train Epoch: 77 [46464/54000 (86%)] Loss: -220733.875000\n",
      "Train Epoch: 77 [47872/54000 (89%)] Loss: -218884.703125\n",
      "Train Epoch: 77 [49280/54000 (91%)] Loss: -245930.312500\n",
      "Train Epoch: 77 [50688/54000 (94%)] Loss: -219068.765625\n",
      "Train Epoch: 77 [52096/54000 (96%)] Loss: -226860.484375\n",
      "    epoch          : 77\n",
      "    loss           : -223923.85016073566\n",
      "    val_loss       : -229619.98875762196\n",
      "Train Epoch: 78 [0/54000 (0%)] Loss: -216934.750000\n",
      "Train Epoch: 78 [1408/54000 (3%)] Loss: -219347.593750\n",
      "Train Epoch: 78 [2816/54000 (5%)] Loss: -216150.765625\n",
      "Train Epoch: 78 [4224/54000 (8%)] Loss: -217749.375000\n",
      "Train Epoch: 78 [5632/54000 (10%)] Loss: -228133.968750\n",
      "Train Epoch: 78 [7040/54000 (13%)] Loss: -228835.031250\n",
      "Train Epoch: 78 [8448/54000 (16%)] Loss: -219807.859375\n",
      "Train Epoch: 78 [9856/54000 (18%)] Loss: -219487.875000\n",
      "Train Epoch: 78 [11264/54000 (21%)] Loss: -214970.531250\n",
      "Train Epoch: 78 [12672/54000 (23%)] Loss: -226123.843750\n",
      "Train Epoch: 78 [14080/54000 (26%)] Loss: -219233.250000\n",
      "Train Epoch: 78 [15488/54000 (29%)] Loss: -228181.140625\n",
      "Train Epoch: 78 [16896/54000 (31%)] Loss: -223697.187500\n",
      "Train Epoch: 78 [18304/54000 (34%)] Loss: -246829.375000\n",
      "Train Epoch: 78 [19712/54000 (37%)] Loss: -227373.046875\n",
      "Train Epoch: 78 [21120/54000 (39%)] Loss: -216424.562500\n",
      "Train Epoch: 78 [22528/54000 (42%)] Loss: -221454.250000\n",
      "Train Epoch: 78 [23936/54000 (44%)] Loss: -223134.703125\n",
      "Train Epoch: 78 [25344/54000 (47%)] Loss: -246893.140625\n",
      "Train Epoch: 78 [26752/54000 (50%)] Loss: -219403.781250\n",
      "Train Epoch: 78 [28160/54000 (52%)] Loss: -213193.343750\n",
      "Train Epoch: 78 [29568/54000 (55%)] Loss: -217016.437500\n",
      "Train Epoch: 78 [30976/54000 (57%)] Loss: -215327.265625\n",
      "Train Epoch: 78 [32384/54000 (60%)] Loss: -220559.296875\n",
      "Train Epoch: 78 [33792/54000 (63%)] Loss: -212544.281250\n",
      "Train Epoch: 78 [35200/54000 (65%)] Loss: -225048.671875\n",
      "Train Epoch: 78 [36608/54000 (68%)] Loss: -220089.656250\n",
      "Train Epoch: 78 [38016/54000 (70%)] Loss: -215850.937500\n",
      "Train Epoch: 78 [39424/54000 (73%)] Loss: -228349.671875\n",
      "Train Epoch: 78 [40832/54000 (76%)] Loss: -229830.312500\n",
      "Train Epoch: 78 [42240/54000 (78%)] Loss: -228203.062500\n",
      "Train Epoch: 78 [43648/54000 (81%)] Loss: -225316.078125\n",
      "Train Epoch: 78 [45056/54000 (83%)] Loss: -247055.437500\n",
      "Train Epoch: 78 [46464/54000 (86%)] Loss: -223806.515625\n",
      "Train Epoch: 78 [47872/54000 (89%)] Loss: -216806.812500\n",
      "Train Epoch: 78 [49280/54000 (91%)] Loss: -217758.375000\n",
      "Train Epoch: 78 [50688/54000 (94%)] Loss: -226894.750000\n",
      "Train Epoch: 78 [52096/54000 (96%)] Loss: -246077.046875\n",
      "    epoch          : 78\n",
      "    loss           : -224324.0507625598\n",
      "    val_loss       : -229107.36756859755\n",
      "Train Epoch: 79 [0/54000 (0%)] Loss: -245935.718750\n",
      "Train Epoch: 79 [1408/54000 (3%)] Loss: -227685.703125\n",
      "Train Epoch: 79 [2816/54000 (5%)] Loss: -218665.484375\n",
      "Train Epoch: 79 [4224/54000 (8%)] Loss: -223355.093750\n",
      "Train Epoch: 79 [5632/54000 (10%)] Loss: -216749.453125\n",
      "Train Epoch: 79 [7040/54000 (13%)] Loss: -224838.593750\n",
      "Train Epoch: 79 [8448/54000 (16%)] Loss: -245202.437500\n",
      "Train Epoch: 79 [9856/54000 (18%)] Loss: -217086.312500\n",
      "Train Epoch: 79 [11264/54000 (21%)] Loss: -215215.187500\n",
      "Train Epoch: 79 [12672/54000 (23%)] Loss: -227879.312500\n",
      "Train Epoch: 79 [14080/54000 (26%)] Loss: -229977.046875\n",
      "Train Epoch: 79 [15488/54000 (29%)] Loss: -216600.531250\n",
      "Train Epoch: 79 [16896/54000 (31%)] Loss: -221948.000000\n",
      "Train Epoch: 79 [18304/54000 (34%)] Loss: -220617.046875\n",
      "Train Epoch: 79 [19712/54000 (37%)] Loss: -246718.750000\n",
      "Train Epoch: 79 [21120/54000 (39%)] Loss: -226484.312500\n",
      "Train Epoch: 79 [22528/54000 (42%)] Loss: -215942.343750\n",
      "Train Epoch: 79 [23936/54000 (44%)] Loss: -228520.468750\n",
      "Train Epoch: 79 [25344/54000 (47%)] Loss: -219193.078125\n",
      "Train Epoch: 79 [26752/54000 (50%)] Loss: -246805.296875\n",
      "Train Epoch: 79 [28160/54000 (52%)] Loss: -224448.640625\n",
      "Train Epoch: 79 [29568/54000 (55%)] Loss: -220489.843750\n",
      "Train Epoch: 79 [30976/54000 (57%)] Loss: -215920.062500\n",
      "Train Epoch: 79 [32384/54000 (60%)] Loss: -225880.343750\n",
      "Train Epoch: 79 [33792/54000 (63%)] Loss: -246180.500000\n",
      "Train Epoch: 79 [35200/54000 (65%)] Loss: -226608.531250\n",
      "Train Epoch: 79 [36608/54000 (68%)] Loss: -221435.609375\n",
      "Train Epoch: 79 [38016/54000 (70%)] Loss: -245562.765625\n",
      "Train Epoch: 79 [39424/54000 (73%)] Loss: -228359.468750\n",
      "Train Epoch: 79 [40832/54000 (76%)] Loss: -220279.953125\n",
      "Train Epoch: 79 [42240/54000 (78%)] Loss: -229504.625000\n",
      "Train Epoch: 79 [43648/54000 (81%)] Loss: -245734.531250\n",
      "Train Epoch: 79 [45056/54000 (83%)] Loss: -211475.765625\n",
      "Train Epoch: 79 [46464/54000 (86%)] Loss: -224137.953125\n",
      "Train Epoch: 79 [47872/54000 (89%)] Loss: -217912.625000\n",
      "Train Epoch: 79 [49280/54000 (91%)] Loss: -246230.859375\n",
      "Train Epoch: 79 [50688/54000 (94%)] Loss: -217246.656250\n",
      "Train Epoch: 79 [52096/54000 (96%)] Loss: -225015.203125\n",
      "    epoch          : 79\n",
      "    loss           : -224455.08320873205\n",
      "    val_loss       : -229551.01581554877\n",
      "Train Epoch: 80 [0/54000 (0%)] Loss: -246552.468750\n",
      "Train Epoch: 80 [1408/54000 (3%)] Loss: -218884.812500\n",
      "Train Epoch: 80 [2816/54000 (5%)] Loss: -221924.031250\n",
      "Train Epoch: 80 [4224/54000 (8%)] Loss: -218626.031250\n",
      "Train Epoch: 80 [5632/54000 (10%)] Loss: -222286.000000\n",
      "Train Epoch: 80 [7040/54000 (13%)] Loss: -217765.015625\n",
      "Train Epoch: 80 [8448/54000 (16%)] Loss: -214352.562500\n",
      "Train Epoch: 80 [9856/54000 (18%)] Loss: -224741.859375\n",
      "Train Epoch: 80 [11264/54000 (21%)] Loss: -228220.234375\n",
      "Train Epoch: 80 [12672/54000 (23%)] Loss: -218580.453125\n",
      "Train Epoch: 80 [14080/54000 (26%)] Loss: -216004.671875\n",
      "Train Epoch: 80 [15488/54000 (29%)] Loss: -246852.937500\n",
      "Train Epoch: 80 [16896/54000 (31%)] Loss: -221331.125000\n",
      "Train Epoch: 80 [18304/54000 (34%)] Loss: -227725.078125\n",
      "Train Epoch: 80 [19712/54000 (37%)] Loss: -246671.828125\n",
      "Train Epoch: 80 [21120/54000 (39%)] Loss: -215366.406250\n",
      "Train Epoch: 80 [22528/54000 (42%)] Loss: -219404.468750\n",
      "Train Epoch: 80 [23936/54000 (44%)] Loss: -217451.093750\n",
      "Train Epoch: 80 [25344/54000 (47%)] Loss: -219953.625000\n",
      "Train Epoch: 80 [26752/54000 (50%)] Loss: -246953.656250\n",
      "Train Epoch: 80 [28160/54000 (52%)] Loss: -226588.078125\n",
      "Train Epoch: 80 [29568/54000 (55%)] Loss: -226500.078125\n",
      "Train Epoch: 80 [30976/54000 (57%)] Loss: -229447.531250\n",
      "Train Epoch: 80 [32384/54000 (60%)] Loss: -246755.000000\n",
      "Train Epoch: 80 [33792/54000 (63%)] Loss: -214855.625000\n",
      "Train Epoch: 80 [35200/54000 (65%)] Loss: -222080.343750\n",
      "Train Epoch: 80 [36608/54000 (68%)] Loss: -227139.640625\n",
      "Train Epoch: 80 [38016/54000 (70%)] Loss: -229556.609375\n",
      "Train Epoch: 80 [39424/54000 (73%)] Loss: -228621.750000\n",
      "Train Epoch: 80 [40832/54000 (76%)] Loss: -214734.906250\n",
      "Train Epoch: 80 [42240/54000 (78%)] Loss: -221053.421875\n",
      "Train Epoch: 80 [43648/54000 (81%)] Loss: -244849.812500\n",
      "Train Epoch: 80 [45056/54000 (83%)] Loss: -247048.031250\n",
      "Train Epoch: 80 [46464/54000 (86%)] Loss: -224058.234375\n",
      "Train Epoch: 80 [47872/54000 (89%)] Loss: -219388.593750\n",
      "Train Epoch: 80 [49280/54000 (91%)] Loss: -213724.562500\n",
      "Train Epoch: 80 [50688/54000 (94%)] Loss: -217602.281250\n",
      "Train Epoch: 80 [52096/54000 (96%)] Loss: -216754.000000\n",
      "    epoch          : 80\n",
      "    loss           : -224519.60324461723\n",
      "    val_loss       : -229572.52477134147\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch80.pth ...\n",
      "Train Epoch: 81 [0/54000 (0%)] Loss: -247098.875000\n",
      "Train Epoch: 81 [1408/54000 (3%)] Loss: -228033.031250\n",
      "Train Epoch: 81 [2816/54000 (5%)] Loss: -220769.125000\n",
      "Train Epoch: 81 [4224/54000 (8%)] Loss: -225126.968750\n",
      "Train Epoch: 81 [5632/54000 (10%)] Loss: -228547.906250\n",
      "Train Epoch: 81 [7040/54000 (13%)] Loss: -246075.140625\n",
      "Train Epoch: 81 [8448/54000 (16%)] Loss: -225106.109375\n",
      "Train Epoch: 81 [9856/54000 (18%)] Loss: -224589.875000\n",
      "Train Epoch: 81 [11264/54000 (21%)] Loss: -220661.546875\n",
      "Train Epoch: 81 [12672/54000 (23%)] Loss: -246847.625000\n",
      "Train Epoch: 81 [14080/54000 (26%)] Loss: -229316.453125\n",
      "Train Epoch: 81 [15488/54000 (29%)] Loss: -218410.109375\n",
      "Train Epoch: 81 [16896/54000 (31%)] Loss: -225351.093750\n",
      "Train Epoch: 81 [18304/54000 (34%)] Loss: -216030.046875\n",
      "Train Epoch: 81 [19712/54000 (37%)] Loss: -229148.343750\n",
      "Train Epoch: 81 [21120/54000 (39%)] Loss: -217058.750000\n",
      "Train Epoch: 81 [22528/54000 (42%)] Loss: -216388.875000\n",
      "Train Epoch: 81 [23936/54000 (44%)] Loss: -224105.093750\n",
      "Train Epoch: 81 [25344/54000 (47%)] Loss: -224909.250000\n",
      "Train Epoch: 81 [26752/54000 (50%)] Loss: -229012.062500\n",
      "Train Epoch: 81 [28160/54000 (52%)] Loss: -226434.375000\n",
      "Train Epoch: 81 [29568/54000 (55%)] Loss: -229834.281250\n",
      "Train Epoch: 81 [30976/54000 (57%)] Loss: -246797.921875\n",
      "Train Epoch: 81 [32384/54000 (60%)] Loss: -213654.250000\n",
      "Train Epoch: 81 [33792/54000 (63%)] Loss: -218341.015625\n",
      "Train Epoch: 81 [35200/54000 (65%)] Loss: -217995.062500\n",
      "Train Epoch: 81 [36608/54000 (68%)] Loss: -217754.406250\n",
      "Train Epoch: 81 [38016/54000 (70%)] Loss: -219733.359375\n",
      "Train Epoch: 81 [39424/54000 (73%)] Loss: -227938.578125\n",
      "Train Epoch: 81 [40832/54000 (76%)] Loss: -213842.906250\n",
      "Train Epoch: 81 [42240/54000 (78%)] Loss: -216672.843750\n",
      "Train Epoch: 81 [43648/54000 (81%)] Loss: -221084.500000\n",
      "Train Epoch: 81 [45056/54000 (83%)] Loss: -220844.593750\n",
      "Train Epoch: 81 [46464/54000 (86%)] Loss: -225850.984375\n",
      "Train Epoch: 81 [47872/54000 (89%)] Loss: -246341.562500\n",
      "Train Epoch: 81 [49280/54000 (91%)] Loss: -221730.312500\n",
      "Train Epoch: 81 [50688/54000 (94%)] Loss: -218174.875000\n",
      "Train Epoch: 81 [52096/54000 (96%)] Loss: -225652.625000\n",
      "    epoch          : 81\n",
      "    loss           : -224616.759569378\n",
      "    val_loss       : -229745.9315929878\n",
      "Train Epoch: 82 [0/54000 (0%)] Loss: -247312.750000\n",
      "Train Epoch: 82 [1408/54000 (3%)] Loss: -216386.312500\n",
      "Train Epoch: 82 [2816/54000 (5%)] Loss: -220598.703125\n",
      "Train Epoch: 82 [4224/54000 (8%)] Loss: -214813.203125\n",
      "Train Epoch: 82 [5632/54000 (10%)] Loss: -228650.281250\n",
      "Train Epoch: 82 [7040/54000 (13%)] Loss: -226050.000000\n",
      "Train Epoch: 82 [8448/54000 (16%)] Loss: -218054.859375\n",
      "Train Epoch: 82 [9856/54000 (18%)] Loss: -222710.687500\n",
      "Train Epoch: 82 [11264/54000 (21%)] Loss: -220992.875000\n",
      "Train Epoch: 82 [12672/54000 (23%)] Loss: -216233.578125\n",
      "Train Epoch: 82 [14080/54000 (26%)] Loss: -225589.437500\n",
      "Train Epoch: 82 [15488/54000 (29%)] Loss: -228069.937500\n",
      "Train Epoch: 82 [16896/54000 (31%)] Loss: -226983.687500\n",
      "Train Epoch: 82 [18304/54000 (34%)] Loss: -219223.250000\n",
      "Train Epoch: 82 [19712/54000 (37%)] Loss: -221908.578125\n",
      "Train Epoch: 82 [21120/54000 (39%)] Loss: -213921.375000\n",
      "Train Epoch: 82 [22528/54000 (42%)] Loss: -218964.875000\n",
      "Train Epoch: 82 [23936/54000 (44%)] Loss: -224513.031250\n",
      "Train Epoch: 82 [25344/54000 (47%)] Loss: -218356.843750\n",
      "Train Epoch: 82 [26752/54000 (50%)] Loss: -229769.968750\n",
      "Train Epoch: 82 [28160/54000 (52%)] Loss: -217497.531250\n",
      "Train Epoch: 82 [29568/54000 (55%)] Loss: -227838.656250\n",
      "Train Epoch: 82 [30976/54000 (57%)] Loss: -220679.031250\n",
      "Train Epoch: 82 [32384/54000 (60%)] Loss: -221251.843750\n",
      "Train Epoch: 82 [33792/54000 (63%)] Loss: -221396.218750\n",
      "Train Epoch: 82 [35200/54000 (65%)] Loss: -220811.437500\n",
      "Train Epoch: 82 [36608/54000 (68%)] Loss: -221949.265625\n",
      "Train Epoch: 82 [38016/54000 (70%)] Loss: -214629.062500\n",
      "Train Epoch: 82 [39424/54000 (73%)] Loss: -246744.718750\n",
      "Train Epoch: 82 [40832/54000 (76%)] Loss: -220718.859375\n",
      "Train Epoch: 82 [42240/54000 (78%)] Loss: -219138.656250\n",
      "Train Epoch: 82 [43648/54000 (81%)] Loss: -225576.765625\n",
      "Train Epoch: 82 [45056/54000 (83%)] Loss: -226581.703125\n",
      "Train Epoch: 82 [46464/54000 (86%)] Loss: -221417.250000\n",
      "Train Epoch: 82 [47872/54000 (89%)] Loss: -221990.671875\n",
      "Train Epoch: 82 [49280/54000 (91%)] Loss: -218643.687500\n",
      "Train Epoch: 82 [50688/54000 (94%)] Loss: -245461.921875\n",
      "Train Epoch: 82 [52096/54000 (96%)] Loss: -218142.437500\n",
      "    epoch          : 82\n",
      "    loss           : -224777.44475179425\n",
      "    val_loss       : -229618.98266006098\n",
      "Train Epoch: 83 [0/54000 (0%)] Loss: -228241.062500\n",
      "Train Epoch: 83 [1408/54000 (3%)] Loss: -218565.718750\n",
      "Train Epoch: 83 [2816/54000 (5%)] Loss: -221676.140625\n",
      "Train Epoch: 83 [4224/54000 (8%)] Loss: -226026.250000\n",
      "Train Epoch: 83 [5632/54000 (10%)] Loss: -225596.859375\n",
      "Train Epoch: 83 [7040/54000 (13%)] Loss: -217772.296875\n",
      "Train Epoch: 83 [8448/54000 (16%)] Loss: -225222.171875\n",
      "Train Epoch: 83 [9856/54000 (18%)] Loss: -225374.000000\n",
      "Train Epoch: 83 [11264/54000 (21%)] Loss: -215474.312500\n",
      "Train Epoch: 83 [12672/54000 (23%)] Loss: -229812.953125\n",
      "Train Epoch: 83 [14080/54000 (26%)] Loss: -223387.234375\n",
      "Train Epoch: 83 [15488/54000 (29%)] Loss: -222673.375000\n",
      "Train Epoch: 83 [16896/54000 (31%)] Loss: -246300.000000\n",
      "Train Epoch: 83 [18304/54000 (34%)] Loss: -218404.906250\n",
      "Train Epoch: 83 [19712/54000 (37%)] Loss: -217051.812500\n",
      "Train Epoch: 83 [21120/54000 (39%)] Loss: -221569.125000\n",
      "Train Epoch: 83 [22528/54000 (42%)] Loss: -229909.921875\n",
      "Train Epoch: 83 [23936/54000 (44%)] Loss: -222991.843750\n",
      "Train Epoch: 83 [25344/54000 (47%)] Loss: -216200.781250\n",
      "Train Epoch: 83 [26752/54000 (50%)] Loss: -220174.593750\n",
      "Train Epoch: 83 [28160/54000 (52%)] Loss: -222426.718750\n",
      "Train Epoch: 83 [29568/54000 (55%)] Loss: -224926.437500\n",
      "Train Epoch: 83 [30976/54000 (57%)] Loss: -223188.656250\n",
      "Train Epoch: 83 [32384/54000 (60%)] Loss: -216759.390625\n",
      "Train Epoch: 83 [33792/54000 (63%)] Loss: -215068.218750\n",
      "Train Epoch: 83 [35200/54000 (65%)] Loss: -221929.828125\n",
      "Train Epoch: 83 [36608/54000 (68%)] Loss: -219110.281250\n",
      "Train Epoch: 83 [38016/54000 (70%)] Loss: -246826.187500\n",
      "Train Epoch: 83 [39424/54000 (73%)] Loss: -229286.250000\n",
      "Train Epoch: 83 [40832/54000 (76%)] Loss: -227497.750000\n",
      "Train Epoch: 83 [42240/54000 (78%)] Loss: -215863.890625\n",
      "Train Epoch: 83 [43648/54000 (81%)] Loss: -217611.609375\n",
      "Train Epoch: 83 [45056/54000 (83%)] Loss: -219036.968750\n",
      "Train Epoch: 83 [46464/54000 (86%)] Loss: -218337.968750\n",
      "Train Epoch: 83 [47872/54000 (89%)] Loss: -221112.906250\n",
      "Train Epoch: 83 [49280/54000 (91%)] Loss: -216394.046875\n",
      "Train Epoch: 83 [50688/54000 (94%)] Loss: -247213.312500\n",
      "Train Epoch: 83 [52096/54000 (96%)] Loss: -227096.109375\n",
      "    epoch          : 83\n",
      "    loss           : -224841.95177930623\n",
      "    val_loss       : -229680.57278963414\n",
      "Train Epoch: 84 [0/54000 (0%)] Loss: -229596.250000\n",
      "Train Epoch: 84 [1408/54000 (3%)] Loss: -218792.828125\n",
      "Train Epoch: 84 [2816/54000 (5%)] Loss: -216333.375000\n",
      "Train Epoch: 84 [4224/54000 (8%)] Loss: -229971.156250\n",
      "Train Epoch: 84 [5632/54000 (10%)] Loss: -227651.531250\n",
      "Train Epoch: 84 [7040/54000 (13%)] Loss: -229206.125000\n",
      "Train Epoch: 84 [8448/54000 (16%)] Loss: -226505.500000\n",
      "Train Epoch: 84 [9856/54000 (18%)] Loss: -228626.890625\n",
      "Train Epoch: 84 [11264/54000 (21%)] Loss: -222298.015625\n",
      "Train Epoch: 84 [12672/54000 (23%)] Loss: -229804.546875\n",
      "Train Epoch: 84 [14080/54000 (26%)] Loss: -219576.937500\n",
      "Train Epoch: 84 [15488/54000 (29%)] Loss: -227412.750000\n",
      "Train Epoch: 84 [16896/54000 (31%)] Loss: -221917.281250\n",
      "Train Epoch: 84 [18304/54000 (34%)] Loss: -223668.968750\n",
      "Train Epoch: 84 [19712/54000 (37%)] Loss: -223475.343750\n",
      "Train Epoch: 84 [21120/54000 (39%)] Loss: -223813.781250\n",
      "Train Epoch: 84 [22528/54000 (42%)] Loss: -226194.828125\n",
      "Train Epoch: 84 [23936/54000 (44%)] Loss: -247313.000000\n",
      "Train Epoch: 84 [25344/54000 (47%)] Loss: -212642.281250\n",
      "Train Epoch: 84 [26752/54000 (50%)] Loss: -217745.546875\n",
      "Train Epoch: 84 [28160/54000 (52%)] Loss: -220536.812500\n",
      "Train Epoch: 84 [29568/54000 (55%)] Loss: -229929.859375\n",
      "Train Epoch: 84 [30976/54000 (57%)] Loss: -246059.828125\n",
      "Train Epoch: 84 [32384/54000 (60%)] Loss: -217776.406250\n",
      "Train Epoch: 84 [33792/54000 (63%)] Loss: -219397.421875\n",
      "Train Epoch: 84 [35200/54000 (65%)] Loss: -222816.328125\n",
      "Train Epoch: 84 [36608/54000 (68%)] Loss: -226267.937500\n",
      "Train Epoch: 84 [38016/54000 (70%)] Loss: -225766.625000\n",
      "Train Epoch: 84 [39424/54000 (73%)] Loss: -229705.453125\n",
      "Train Epoch: 84 [40832/54000 (76%)] Loss: -246569.437500\n",
      "Train Epoch: 84 [42240/54000 (78%)] Loss: -217364.906250\n",
      "Train Epoch: 84 [43648/54000 (81%)] Loss: -212490.593750\n",
      "Train Epoch: 84 [45056/54000 (83%)] Loss: -217865.093750\n",
      "Train Epoch: 84 [46464/54000 (86%)] Loss: -246354.593750\n",
      "Train Epoch: 84 [47872/54000 (89%)] Loss: -217677.484375\n",
      "Train Epoch: 84 [49280/54000 (91%)] Loss: -220515.250000\n",
      "Train Epoch: 84 [50688/54000 (94%)] Loss: -217845.328125\n",
      "Train Epoch: 84 [52096/54000 (96%)] Loss: -219319.843750\n",
      "    epoch          : 84\n",
      "    loss           : -225087.90681070575\n",
      "    val_loss       : -229809.53105945123\n",
      "Train Epoch: 85 [0/54000 (0%)] Loss: -222766.218750\n",
      "Train Epoch: 85 [1408/54000 (3%)] Loss: -219056.640625\n",
      "Train Epoch: 85 [2816/54000 (5%)] Loss: -218682.609375\n",
      "Train Epoch: 85 [4224/54000 (8%)] Loss: -218356.468750\n",
      "Train Epoch: 85 [5632/54000 (10%)] Loss: -216349.656250\n",
      "Train Epoch: 85 [7040/54000 (13%)] Loss: -227877.656250\n",
      "Train Epoch: 85 [8448/54000 (16%)] Loss: -216555.125000\n",
      "Train Epoch: 85 [9856/54000 (18%)] Loss: -220967.578125\n",
      "Train Epoch: 85 [11264/54000 (21%)] Loss: -247739.562500\n",
      "Train Epoch: 85 [12672/54000 (23%)] Loss: -225742.109375\n",
      "Train Epoch: 85 [14080/54000 (26%)] Loss: -221381.984375\n",
      "Train Epoch: 85 [15488/54000 (29%)] Loss: -217746.875000\n",
      "Train Epoch: 85 [16896/54000 (31%)] Loss: -247458.921875\n",
      "Train Epoch: 85 [18304/54000 (34%)] Loss: -217635.625000\n",
      "Train Epoch: 85 [19712/54000 (37%)] Loss: -223074.593750\n",
      "Train Epoch: 85 [21120/54000 (39%)] Loss: -222341.593750\n",
      "Train Epoch: 85 [22528/54000 (42%)] Loss: -217156.062500\n",
      "Train Epoch: 85 [23936/54000 (44%)] Loss: -218243.921875\n",
      "Train Epoch: 85 [25344/54000 (47%)] Loss: -224727.234375\n",
      "Train Epoch: 85 [26752/54000 (50%)] Loss: -218955.750000\n",
      "Train Epoch: 85 [28160/54000 (52%)] Loss: -246861.671875\n",
      "Train Epoch: 85 [29568/54000 (55%)] Loss: -222645.218750\n",
      "Train Epoch: 85 [30976/54000 (57%)] Loss: -222151.937500\n",
      "Train Epoch: 85 [32384/54000 (60%)] Loss: -229187.875000\n",
      "Train Epoch: 85 [33792/54000 (63%)] Loss: -216611.890625\n",
      "Train Epoch: 85 [35200/54000 (65%)] Loss: -247478.968750\n",
      "Train Epoch: 85 [36608/54000 (68%)] Loss: -217396.859375\n",
      "Train Epoch: 85 [38016/54000 (70%)] Loss: -217781.437500\n",
      "Train Epoch: 85 [39424/54000 (73%)] Loss: -222055.812500\n",
      "Train Epoch: 85 [40832/54000 (76%)] Loss: -228241.609375\n",
      "Train Epoch: 85 [42240/54000 (78%)] Loss: -230046.500000\n",
      "Train Epoch: 85 [43648/54000 (81%)] Loss: -227213.343750\n",
      "Train Epoch: 85 [45056/54000 (83%)] Loss: -221227.062500\n",
      "Train Epoch: 85 [46464/54000 (86%)] Loss: -219938.453125\n",
      "Train Epoch: 85 [47872/54000 (89%)] Loss: -228174.296875\n",
      "Train Epoch: 85 [49280/54000 (91%)] Loss: -215375.562500\n",
      "Train Epoch: 85 [50688/54000 (94%)] Loss: -217078.687500\n",
      "Train Epoch: 85 [52096/54000 (96%)] Loss: -218679.750000\n",
      "    epoch          : 85\n",
      "    loss           : -225139.89320424641\n",
      "    val_loss       : -229962.72198932926\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch85.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 86 [0/54000 (0%)] Loss: -247188.656250\n",
      "Train Epoch: 86 [1408/54000 (3%)] Loss: -212409.187500\n",
      "Train Epoch: 86 [2816/54000 (5%)] Loss: -225810.968750\n",
      "Train Epoch: 86 [4224/54000 (8%)] Loss: -226358.375000\n",
      "Train Epoch: 86 [5632/54000 (10%)] Loss: -215949.984375\n",
      "Train Epoch: 86 [7040/54000 (13%)] Loss: -226194.359375\n",
      "Train Epoch: 86 [8448/54000 (16%)] Loss: -224476.718750\n",
      "Train Epoch: 86 [9856/54000 (18%)] Loss: -216481.875000\n",
      "Train Epoch: 86 [11264/54000 (21%)] Loss: -219389.859375\n",
      "Train Epoch: 86 [12672/54000 (23%)] Loss: -219458.296875\n",
      "Train Epoch: 86 [14080/54000 (26%)] Loss: -247913.437500\n",
      "Train Epoch: 86 [15488/54000 (29%)] Loss: -228780.828125\n",
      "Train Epoch: 86 [16896/54000 (31%)] Loss: -225953.265625\n",
      "Train Epoch: 86 [18304/54000 (34%)] Loss: -217722.359375\n",
      "Train Epoch: 86 [19712/54000 (37%)] Loss: -248423.125000\n",
      "Train Epoch: 86 [21120/54000 (39%)] Loss: -222477.031250\n",
      "Train Epoch: 86 [22528/54000 (42%)] Loss: -221630.156250\n",
      "Train Epoch: 86 [23936/54000 (44%)] Loss: -219875.031250\n",
      "Train Epoch: 86 [25344/54000 (47%)] Loss: -218375.281250\n",
      "Train Epoch: 86 [26752/54000 (50%)] Loss: -247562.578125\n",
      "Train Epoch: 86 [28160/54000 (52%)] Loss: -222361.875000\n",
      "Train Epoch: 86 [29568/54000 (55%)] Loss: -225373.078125\n",
      "Train Epoch: 86 [30976/54000 (57%)] Loss: -221654.593750\n",
      "Train Epoch: 86 [32384/54000 (60%)] Loss: -227288.703125\n",
      "Train Epoch: 86 [33792/54000 (63%)] Loss: -218781.468750\n",
      "Train Epoch: 86 [35200/54000 (65%)] Loss: -228603.125000\n",
      "Train Epoch: 86 [36608/54000 (68%)] Loss: -221991.375000\n",
      "Train Epoch: 86 [38016/54000 (70%)] Loss: -248140.656250\n",
      "Train Epoch: 86 [39424/54000 (73%)] Loss: -228546.781250\n",
      "Train Epoch: 86 [40832/54000 (76%)] Loss: -216929.750000\n",
      "Train Epoch: 86 [42240/54000 (78%)] Loss: -216930.406250\n",
      "Train Epoch: 86 [43648/54000 (81%)] Loss: -246161.953125\n",
      "Train Epoch: 86 [45056/54000 (83%)] Loss: -221639.375000\n",
      "Train Epoch: 86 [46464/54000 (86%)] Loss: -221858.250000\n",
      "Train Epoch: 86 [47872/54000 (89%)] Loss: -224611.015625\n",
      "Train Epoch: 86 [49280/54000 (91%)] Loss: -219402.468750\n",
      "Train Epoch: 86 [50688/54000 (94%)] Loss: -246507.093750\n",
      "Train Epoch: 86 [52096/54000 (96%)] Loss: -227909.375000\n",
      "    epoch          : 86\n",
      "    loss           : -225228.42303379186\n",
      "    val_loss       : -229902.84108231709\n",
      "Train Epoch: 87 [0/54000 (0%)] Loss: -247230.515625\n",
      "Train Epoch: 87 [1408/54000 (3%)] Loss: -223108.375000\n",
      "Train Epoch: 87 [2816/54000 (5%)] Loss: -226149.421875\n",
      "Train Epoch: 87 [4224/54000 (8%)] Loss: -220008.562500\n",
      "Train Epoch: 87 [5632/54000 (10%)] Loss: -247718.156250\n",
      "Train Epoch: 87 [7040/54000 (13%)] Loss: -218576.093750\n",
      "Train Epoch: 87 [8448/54000 (16%)] Loss: -222469.562500\n",
      "Train Epoch: 87 [9856/54000 (18%)] Loss: -222491.500000\n",
      "Train Epoch: 87 [11264/54000 (21%)] Loss: -229156.640625\n",
      "Train Epoch: 87 [12672/54000 (23%)] Loss: -228918.578125\n",
      "Train Epoch: 87 [14080/54000 (26%)] Loss: -230263.953125\n",
      "Train Epoch: 87 [15488/54000 (29%)] Loss: -218442.453125\n",
      "Train Epoch: 87 [16896/54000 (31%)] Loss: -218450.406250\n",
      "Train Epoch: 87 [18304/54000 (34%)] Loss: -247983.062500\n",
      "Train Epoch: 87 [19712/54000 (37%)] Loss: -216446.765625\n",
      "Train Epoch: 87 [21120/54000 (39%)] Loss: -218932.781250\n",
      "Train Epoch: 87 [22528/54000 (42%)] Loss: -221363.781250\n",
      "Train Epoch: 87 [23936/54000 (44%)] Loss: -241134.875000\n",
      "Train Epoch: 87 [25344/54000 (47%)] Loss: -222421.656250\n",
      "Train Epoch: 87 [26752/54000 (50%)] Loss: -226456.484375\n",
      "Train Epoch: 87 [28160/54000 (52%)] Loss: -226672.812500\n",
      "Train Epoch: 87 [29568/54000 (55%)] Loss: -220688.250000\n",
      "Train Epoch: 87 [30976/54000 (57%)] Loss: -228637.500000\n",
      "Train Epoch: 87 [32384/54000 (60%)] Loss: -230590.937500\n",
      "Train Epoch: 87 [33792/54000 (63%)] Loss: -218535.781250\n",
      "Train Epoch: 87 [35200/54000 (65%)] Loss: -217895.156250\n",
      "Train Epoch: 87 [36608/54000 (68%)] Loss: -214721.625000\n",
      "Train Epoch: 87 [38016/54000 (70%)] Loss: -216132.812500\n",
      "Train Epoch: 87 [39424/54000 (73%)] Loss: -215168.796875\n",
      "Train Epoch: 87 [40832/54000 (76%)] Loss: -225184.218750\n",
      "Train Epoch: 87 [42240/54000 (78%)] Loss: -230086.781250\n",
      "Train Epoch: 87 [43648/54000 (81%)] Loss: -220061.718750\n",
      "Train Epoch: 87 [45056/54000 (83%)] Loss: -221850.250000\n",
      "Train Epoch: 87 [46464/54000 (86%)] Loss: -222328.734375\n",
      "Train Epoch: 87 [47872/54000 (89%)] Loss: -229175.812500\n",
      "Train Epoch: 87 [49280/54000 (91%)] Loss: -220298.109375\n",
      "Train Epoch: 87 [50688/54000 (94%)] Loss: -226928.828125\n",
      "Train Epoch: 87 [52096/54000 (96%)] Loss: -216758.203125\n",
      "    epoch          : 87\n",
      "    loss           : -225345.55737888755\n",
      "    val_loss       : -230126.5537347561\n",
      "Train Epoch: 88 [0/54000 (0%)] Loss: -222047.671875\n",
      "Train Epoch: 88 [1408/54000 (3%)] Loss: -227539.375000\n",
      "Train Epoch: 88 [2816/54000 (5%)] Loss: -223102.593750\n",
      "Train Epoch: 88 [4224/54000 (8%)] Loss: -217356.937500\n",
      "Train Epoch: 88 [5632/54000 (10%)] Loss: -218862.656250\n",
      "Train Epoch: 88 [7040/54000 (13%)] Loss: -228748.703125\n",
      "Train Epoch: 88 [8448/54000 (16%)] Loss: -228854.625000\n",
      "Train Epoch: 88 [9856/54000 (18%)] Loss: -217813.625000\n",
      "Train Epoch: 88 [11264/54000 (21%)] Loss: -218695.000000\n",
      "Train Epoch: 88 [12672/54000 (23%)] Loss: -219277.890625\n",
      "Train Epoch: 88 [14080/54000 (26%)] Loss: -226175.562500\n",
      "Train Epoch: 88 [15488/54000 (29%)] Loss: -247571.500000\n",
      "Train Epoch: 88 [16896/54000 (31%)] Loss: -225570.796875\n",
      "Train Epoch: 88 [18304/54000 (34%)] Loss: -215826.093750\n",
      "Train Epoch: 88 [19712/54000 (37%)] Loss: -217616.937500\n",
      "Train Epoch: 88 [21120/54000 (39%)] Loss: -223991.578125\n",
      "Train Epoch: 88 [22528/54000 (42%)] Loss: -223924.625000\n",
      "Train Epoch: 88 [23936/54000 (44%)] Loss: -217260.531250\n",
      "Train Epoch: 88 [25344/54000 (47%)] Loss: -223849.656250\n",
      "Train Epoch: 88 [26752/54000 (50%)] Loss: -223004.375000\n",
      "Train Epoch: 88 [28160/54000 (52%)] Loss: -222141.468750\n",
      "Train Epoch: 88 [29568/54000 (55%)] Loss: -247813.406250\n",
      "Train Epoch: 88 [30976/54000 (57%)] Loss: -226503.000000\n",
      "Train Epoch: 88 [32384/54000 (60%)] Loss: -228252.921875\n",
      "Train Epoch: 88 [33792/54000 (63%)] Loss: -217340.500000\n",
      "Train Epoch: 88 [35200/54000 (65%)] Loss: -229753.937500\n",
      "Train Epoch: 88 [36608/54000 (68%)] Loss: -218862.531250\n",
      "Train Epoch: 88 [38016/54000 (70%)] Loss: -221192.859375\n",
      "Train Epoch: 88 [39424/54000 (73%)] Loss: -219513.484375\n",
      "Train Epoch: 88 [40832/54000 (76%)] Loss: -218628.656250\n",
      "Train Epoch: 88 [42240/54000 (78%)] Loss: -246719.218750\n",
      "Train Epoch: 88 [43648/54000 (81%)] Loss: -214969.000000\n",
      "Train Epoch: 88 [45056/54000 (83%)] Loss: -219257.328125\n",
      "Train Epoch: 88 [46464/54000 (86%)] Loss: -248370.046875\n",
      "Train Epoch: 88 [47872/54000 (89%)] Loss: -218588.187500\n",
      "Train Epoch: 88 [49280/54000 (91%)] Loss: -217821.281250\n",
      "Train Epoch: 88 [50688/54000 (94%)] Loss: -221710.640625\n",
      "Train Epoch: 88 [52096/54000 (96%)] Loss: -247949.000000\n",
      "    epoch          : 88\n",
      "    loss           : -225411.69875149522\n",
      "    val_loss       : -229966.9647484756\n",
      "Train Epoch: 89 [0/54000 (0%)] Loss: -228659.843750\n",
      "Train Epoch: 89 [1408/54000 (3%)] Loss: -222496.250000\n",
      "Train Epoch: 89 [2816/54000 (5%)] Loss: -215866.328125\n",
      "Train Epoch: 89 [4224/54000 (8%)] Loss: -221166.937500\n",
      "Train Epoch: 89 [5632/54000 (10%)] Loss: -223459.296875\n",
      "Train Epoch: 89 [7040/54000 (13%)] Loss: -218408.250000\n",
      "Train Epoch: 89 [8448/54000 (16%)] Loss: -248072.859375\n",
      "Train Epoch: 89 [9856/54000 (18%)] Loss: -223680.812500\n",
      "Train Epoch: 89 [11264/54000 (21%)] Loss: -218463.468750\n",
      "Train Epoch: 89 [12672/54000 (23%)] Loss: -248009.875000\n",
      "Train Epoch: 89 [14080/54000 (26%)] Loss: -218588.906250\n",
      "Train Epoch: 89 [15488/54000 (29%)] Loss: -221940.406250\n",
      "Train Epoch: 89 [16896/54000 (31%)] Loss: -217305.718750\n",
      "Train Epoch: 89 [18304/54000 (34%)] Loss: -247756.578125\n",
      "Train Epoch: 89 [19712/54000 (37%)] Loss: -247010.593750\n",
      "Train Epoch: 89 [21120/54000 (39%)] Loss: -229691.156250\n",
      "Train Epoch: 89 [22528/54000 (42%)] Loss: -220355.218750\n",
      "Train Epoch: 89 [23936/54000 (44%)] Loss: -226760.406250\n",
      "Train Epoch: 89 [25344/54000 (47%)] Loss: -215986.234375\n",
      "Train Epoch: 89 [26752/54000 (50%)] Loss: -218072.015625\n",
      "Train Epoch: 89 [28160/54000 (52%)] Loss: -215979.984375\n",
      "Train Epoch: 89 [29568/54000 (55%)] Loss: -219517.218750\n",
      "Train Epoch: 89 [30976/54000 (57%)] Loss: -225128.781250\n",
      "Train Epoch: 89 [32384/54000 (60%)] Loss: -229426.156250\n",
      "Train Epoch: 89 [33792/54000 (63%)] Loss: -228560.234375\n",
      "Train Epoch: 89 [35200/54000 (65%)] Loss: -219228.250000\n",
      "Train Epoch: 89 [36608/54000 (68%)] Loss: -218779.484375\n",
      "Train Epoch: 89 [38016/54000 (70%)] Loss: -223786.140625\n",
      "Train Epoch: 89 [39424/54000 (73%)] Loss: -221818.515625\n",
      "Train Epoch: 89 [40832/54000 (76%)] Loss: -229930.656250\n",
      "Train Epoch: 89 [42240/54000 (78%)] Loss: -217676.062500\n",
      "Train Epoch: 89 [43648/54000 (81%)] Loss: -227778.125000\n",
      "Train Epoch: 89 [45056/54000 (83%)] Loss: -229511.046875\n",
      "Train Epoch: 89 [46464/54000 (86%)] Loss: -218652.656250\n",
      "Train Epoch: 89 [47872/54000 (89%)] Loss: -227187.187500\n",
      "Train Epoch: 89 [49280/54000 (91%)] Loss: -225605.250000\n",
      "Train Epoch: 89 [50688/54000 (94%)] Loss: -217479.156250\n",
      "Train Epoch: 89 [52096/54000 (96%)] Loss: -248180.843750\n",
      "    epoch          : 89\n",
      "    loss           : -225658.82644288277\n",
      "    val_loss       : -229869.2677210366\n",
      "Train Epoch: 90 [0/54000 (0%)] Loss: -248030.281250\n",
      "Train Epoch: 90 [1408/54000 (3%)] Loss: -222479.218750\n",
      "Train Epoch: 90 [2816/54000 (5%)] Loss: -222380.531250\n",
      "Train Epoch: 90 [4224/54000 (8%)] Loss: -221819.859375\n",
      "Train Epoch: 90 [5632/54000 (10%)] Loss: -218788.937500\n",
      "Train Epoch: 90 [7040/54000 (13%)] Loss: -217137.875000\n",
      "Train Epoch: 90 [8448/54000 (16%)] Loss: -222560.031250\n",
      "Train Epoch: 90 [9856/54000 (18%)] Loss: -219179.093750\n",
      "Train Epoch: 90 [11264/54000 (21%)] Loss: -220132.656250\n",
      "Train Epoch: 90 [12672/54000 (23%)] Loss: -229292.515625\n",
      "Train Epoch: 90 [14080/54000 (26%)] Loss: -218808.968750\n",
      "Train Epoch: 90 [15488/54000 (29%)] Loss: -221216.468750\n",
      "Train Epoch: 90 [16896/54000 (31%)] Loss: -247884.171875\n",
      "Train Epoch: 90 [18304/54000 (34%)] Loss: -247961.031250\n",
      "Train Epoch: 90 [19712/54000 (37%)] Loss: -227046.484375\n",
      "Train Epoch: 90 [21120/54000 (39%)] Loss: -219002.218750\n",
      "Train Epoch: 90 [22528/54000 (42%)] Loss: -226501.468750\n",
      "Train Epoch: 90 [23936/54000 (44%)] Loss: -223119.625000\n",
      "Train Epoch: 90 [25344/54000 (47%)] Loss: -246168.343750\n",
      "Train Epoch: 90 [26752/54000 (50%)] Loss: -216370.953125\n",
      "Train Epoch: 90 [28160/54000 (52%)] Loss: -229140.562500\n",
      "Train Epoch: 90 [29568/54000 (55%)] Loss: -218199.250000\n",
      "Train Epoch: 90 [30976/54000 (57%)] Loss: -217524.000000\n",
      "Train Epoch: 90 [32384/54000 (60%)] Loss: -217625.843750\n",
      "Train Epoch: 90 [33792/54000 (63%)] Loss: -219937.625000\n",
      "Train Epoch: 90 [35200/54000 (65%)] Loss: -223779.609375\n",
      "Train Epoch: 90 [36608/54000 (68%)] Loss: -227604.921875\n",
      "Train Epoch: 90 [38016/54000 (70%)] Loss: -247158.968750\n",
      "Train Epoch: 90 [39424/54000 (73%)] Loss: -218865.421875\n",
      "Train Epoch: 90 [40832/54000 (76%)] Loss: -230927.781250\n",
      "Train Epoch: 90 [42240/54000 (78%)] Loss: -221327.750000\n",
      "Train Epoch: 90 [43648/54000 (81%)] Loss: -229430.546875\n",
      "Train Epoch: 90 [45056/54000 (83%)] Loss: -246799.328125\n",
      "Train Epoch: 90 [46464/54000 (86%)] Loss: -248710.468750\n",
      "Train Epoch: 90 [47872/54000 (89%)] Loss: -218434.531250\n",
      "Train Epoch: 90 [49280/54000 (91%)] Loss: -226588.687500\n",
      "Train Epoch: 90 [50688/54000 (94%)] Loss: -247433.406250\n",
      "Train Epoch: 90 [52096/54000 (96%)] Loss: -218509.109375\n",
      "    epoch          : 90\n",
      "    loss           : -225637.03577302632\n",
      "    val_loss       : -230009.76276676828\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch90.pth ...\n",
      "Train Epoch: 91 [0/54000 (0%)] Loss: -218599.453125\n",
      "Train Epoch: 91 [1408/54000 (3%)] Loss: -220989.437500\n",
      "Train Epoch: 91 [2816/54000 (5%)] Loss: -222881.515625\n",
      "Train Epoch: 91 [4224/54000 (8%)] Loss: -222948.312500\n",
      "Train Epoch: 91 [5632/54000 (10%)] Loss: -228719.796875\n",
      "Train Epoch: 91 [7040/54000 (13%)] Loss: -220095.812500\n",
      "Train Epoch: 91 [8448/54000 (16%)] Loss: -246888.187500\n",
      "Train Epoch: 91 [9856/54000 (18%)] Loss: -229627.500000\n",
      "Train Epoch: 91 [11264/54000 (21%)] Loss: -218862.593750\n",
      "Train Epoch: 91 [12672/54000 (23%)] Loss: -219828.734375\n",
      "Train Epoch: 91 [14080/54000 (26%)] Loss: -220209.921875\n",
      "Train Epoch: 91 [15488/54000 (29%)] Loss: -219532.468750\n",
      "Train Epoch: 91 [16896/54000 (31%)] Loss: -219055.156250\n",
      "Train Epoch: 91 [18304/54000 (34%)] Loss: -219929.531250\n",
      "Train Epoch: 91 [19712/54000 (37%)] Loss: -219796.718750\n",
      "Train Epoch: 91 [21120/54000 (39%)] Loss: -218917.265625\n",
      "Train Epoch: 91 [22528/54000 (42%)] Loss: -221618.703125\n",
      "Train Epoch: 91 [23936/54000 (44%)] Loss: -229807.687500\n",
      "Train Epoch: 91 [25344/54000 (47%)] Loss: -230137.062500\n",
      "Train Epoch: 91 [26752/54000 (50%)] Loss: -229951.921875\n",
      "Train Epoch: 91 [28160/54000 (52%)] Loss: -222541.171875\n",
      "Train Epoch: 91 [29568/54000 (55%)] Loss: -224112.015625\n",
      "Train Epoch: 91 [30976/54000 (57%)] Loss: -217441.687500\n",
      "Train Epoch: 91 [32384/54000 (60%)] Loss: -216492.265625\n",
      "Train Epoch: 91 [33792/54000 (63%)] Loss: -218461.156250\n",
      "Train Epoch: 91 [35200/54000 (65%)] Loss: -217223.062500\n",
      "Train Epoch: 91 [36608/54000 (68%)] Loss: -219466.796875\n",
      "Train Epoch: 91 [38016/54000 (70%)] Loss: -247174.656250\n",
      "Train Epoch: 91 [39424/54000 (73%)] Loss: -223017.843750\n",
      "Train Epoch: 91 [40832/54000 (76%)] Loss: -230284.406250\n",
      "Train Epoch: 91 [42240/54000 (78%)] Loss: -220071.125000\n",
      "Train Epoch: 91 [43648/54000 (81%)] Loss: -229528.218750\n",
      "Train Epoch: 91 [45056/54000 (83%)] Loss: -248105.906250\n",
      "Train Epoch: 91 [46464/54000 (86%)] Loss: -227449.421875\n",
      "Train Epoch: 91 [47872/54000 (89%)] Loss: -228025.906250\n",
      "Train Epoch: 91 [49280/54000 (91%)] Loss: -225660.515625\n",
      "Train Epoch: 91 [50688/54000 (94%)] Loss: -246483.593750\n",
      "Train Epoch: 91 [52096/54000 (96%)] Loss: -230212.421875\n",
      "    epoch          : 91\n",
      "    loss           : -225794.79287529906\n",
      "    val_loss       : -230042.67701981709\n",
      "Train Epoch: 92 [0/54000 (0%)] Loss: -229100.781250\n",
      "Train Epoch: 92 [1408/54000 (3%)] Loss: -229965.765625\n",
      "Train Epoch: 92 [2816/54000 (5%)] Loss: -219468.125000\n",
      "Train Epoch: 92 [4224/54000 (8%)] Loss: -223863.593750\n",
      "Train Epoch: 92 [5632/54000 (10%)] Loss: -216662.218750\n",
      "Train Epoch: 92 [7040/54000 (13%)] Loss: -217287.750000\n",
      "Train Epoch: 92 [8448/54000 (16%)] Loss: -213512.562500\n",
      "Train Epoch: 92 [9856/54000 (18%)] Loss: -226996.343750\n",
      "Train Epoch: 92 [11264/54000 (21%)] Loss: -229973.171875\n",
      "Train Epoch: 92 [12672/54000 (23%)] Loss: -222801.468750\n",
      "Train Epoch: 92 [14080/54000 (26%)] Loss: -223781.484375\n",
      "Train Epoch: 92 [15488/54000 (29%)] Loss: -225956.687500\n",
      "Train Epoch: 92 [16896/54000 (31%)] Loss: -222840.812500\n",
      "Train Epoch: 92 [18304/54000 (34%)] Loss: -217615.062500\n",
      "Train Epoch: 92 [19712/54000 (37%)] Loss: -219204.703125\n",
      "Train Epoch: 92 [21120/54000 (39%)] Loss: -219992.812500\n",
      "Train Epoch: 92 [22528/54000 (42%)] Loss: -222733.875000\n",
      "Train Epoch: 92 [23936/54000 (44%)] Loss: -220462.343750\n",
      "Train Epoch: 92 [25344/54000 (47%)] Loss: -221708.578125\n",
      "Train Epoch: 92 [26752/54000 (50%)] Loss: -218550.000000\n",
      "Train Epoch: 92 [28160/54000 (52%)] Loss: -247930.312500\n",
      "Train Epoch: 92 [29568/54000 (55%)] Loss: -230248.656250\n",
      "Train Epoch: 92 [30976/54000 (57%)] Loss: -223606.812500\n",
      "Train Epoch: 92 [32384/54000 (60%)] Loss: -227966.875000\n",
      "Train Epoch: 92 [33792/54000 (63%)] Loss: -217458.750000\n",
      "Train Epoch: 92 [35200/54000 (65%)] Loss: -228396.296875\n",
      "Train Epoch: 92 [36608/54000 (68%)] Loss: -220894.875000\n",
      "Train Epoch: 92 [38016/54000 (70%)] Loss: -219190.875000\n",
      "Train Epoch: 92 [39424/54000 (73%)] Loss: -223644.296875\n",
      "Train Epoch: 92 [40832/54000 (76%)] Loss: -229345.437500\n",
      "Train Epoch: 92 [42240/54000 (78%)] Loss: -219800.687500\n",
      "Train Epoch: 92 [43648/54000 (81%)] Loss: -229104.640625\n",
      "Train Epoch: 92 [45056/54000 (83%)] Loss: -221939.906250\n",
      "Train Epoch: 92 [46464/54000 (86%)] Loss: -224461.734375\n",
      "Train Epoch: 92 [47872/54000 (89%)] Loss: -225841.781250\n",
      "Train Epoch: 92 [49280/54000 (91%)] Loss: -222827.937500\n",
      "Train Epoch: 92 [50688/54000 (94%)] Loss: -246948.875000\n",
      "Train Epoch: 92 [52096/54000 (96%)] Loss: -228752.843750\n",
      "    epoch          : 92\n",
      "    loss           : -225867.1657819976\n",
      "    val_loss       : -229907.80602134147\n",
      "Train Epoch: 93 [0/54000 (0%)] Loss: -226970.468750\n",
      "Train Epoch: 93 [1408/54000 (3%)] Loss: -222447.250000\n",
      "Train Epoch: 93 [2816/54000 (5%)] Loss: -217937.796875\n",
      "Train Epoch: 93 [4224/54000 (8%)] Loss: -218199.671875\n",
      "Train Epoch: 93 [5632/54000 (10%)] Loss: -216598.109375\n",
      "Train Epoch: 93 [7040/54000 (13%)] Loss: -230552.718750\n",
      "Train Epoch: 93 [8448/54000 (16%)] Loss: -225229.609375\n",
      "Train Epoch: 93 [9856/54000 (18%)] Loss: -219859.421875\n",
      "Train Epoch: 93 [11264/54000 (21%)] Loss: -219498.234375\n",
      "Train Epoch: 93 [12672/54000 (23%)] Loss: -221603.843750\n",
      "Train Epoch: 93 [14080/54000 (26%)] Loss: -227097.671875\n",
      "Train Epoch: 93 [15488/54000 (29%)] Loss: -224884.625000\n",
      "Train Epoch: 93 [16896/54000 (31%)] Loss: -246718.937500\n",
      "Train Epoch: 93 [18304/54000 (34%)] Loss: -223459.281250\n",
      "Train Epoch: 93 [19712/54000 (37%)] Loss: -223516.593750\n",
      "Train Epoch: 93 [21120/54000 (39%)] Loss: -229922.234375\n",
      "Train Epoch: 93 [22528/54000 (42%)] Loss: -218550.562500\n",
      "Train Epoch: 93 [23936/54000 (44%)] Loss: -216698.937500\n",
      "Train Epoch: 93 [25344/54000 (47%)] Loss: -221851.062500\n",
      "Train Epoch: 93 [26752/54000 (50%)] Loss: -247303.312500\n",
      "Train Epoch: 93 [28160/54000 (52%)] Loss: -227658.281250\n",
      "Train Epoch: 93 [29568/54000 (55%)] Loss: -227553.593750\n",
      "Train Epoch: 93 [30976/54000 (57%)] Loss: -219598.671875\n",
      "Train Epoch: 93 [32384/54000 (60%)] Loss: -222761.281250\n",
      "Train Epoch: 93 [33792/54000 (63%)] Loss: -222979.156250\n",
      "Train Epoch: 93 [35200/54000 (65%)] Loss: -231280.812500\n",
      "Train Epoch: 93 [36608/54000 (68%)] Loss: -222258.359375\n",
      "Train Epoch: 93 [38016/54000 (70%)] Loss: -247991.750000\n",
      "Train Epoch: 93 [39424/54000 (73%)] Loss: -215888.062500\n",
      "Train Epoch: 93 [40832/54000 (76%)] Loss: -217483.750000\n",
      "Train Epoch: 93 [42240/54000 (78%)] Loss: -227312.656250\n",
      "Train Epoch: 93 [43648/54000 (81%)] Loss: -218377.546875\n",
      "Train Epoch: 93 [45056/54000 (83%)] Loss: -222737.000000\n",
      "Train Epoch: 93 [46464/54000 (86%)] Loss: -225252.968750\n",
      "Train Epoch: 93 [47872/54000 (89%)] Loss: -218893.984375\n",
      "Train Epoch: 93 [49280/54000 (91%)] Loss: -248454.765625\n",
      "Train Epoch: 93 [50688/54000 (94%)] Loss: -217621.875000\n",
      "Train Epoch: 93 [52096/54000 (96%)] Loss: -228691.781250\n",
      "    epoch          : 93\n",
      "    loss           : -225984.55921052632\n",
      "    val_loss       : -230160.08650914635\n",
      "Train Epoch: 94 [0/54000 (0%)] Loss: -226197.546875\n",
      "Train Epoch: 94 [1408/54000 (3%)] Loss: -229737.078125\n",
      "Train Epoch: 94 [2816/54000 (5%)] Loss: -228729.468750\n",
      "Train Epoch: 94 [4224/54000 (8%)] Loss: -231196.406250\n",
      "Train Epoch: 94 [5632/54000 (10%)] Loss: -219648.843750\n",
      "Train Epoch: 94 [7040/54000 (13%)] Loss: -222938.656250\n",
      "Train Epoch: 94 [8448/54000 (16%)] Loss: -247295.046875\n",
      "Train Epoch: 94 [9856/54000 (18%)] Loss: -217576.687500\n",
      "Train Epoch: 94 [11264/54000 (21%)] Loss: -230099.750000\n",
      "Train Epoch: 94 [12672/54000 (23%)] Loss: -222022.453125\n",
      "Train Epoch: 94 [14080/54000 (26%)] Loss: -247847.531250\n",
      "Train Epoch: 94 [15488/54000 (29%)] Loss: -224317.718750\n",
      "Train Epoch: 94 [16896/54000 (31%)] Loss: -217544.875000\n",
      "Train Epoch: 94 [18304/54000 (34%)] Loss: -217162.328125\n",
      "Train Epoch: 94 [19712/54000 (37%)] Loss: -246440.343750\n",
      "Train Epoch: 94 [21120/54000 (39%)] Loss: -221521.406250\n",
      "Train Epoch: 94 [22528/54000 (42%)] Loss: -222797.125000\n",
      "Train Epoch: 94 [23936/54000 (44%)] Loss: -219976.203125\n",
      "Train Epoch: 94 [25344/54000 (47%)] Loss: -219720.390625\n",
      "Train Epoch: 94 [26752/54000 (50%)] Loss: -216620.687500\n",
      "Train Epoch: 94 [28160/54000 (52%)] Loss: -213938.750000\n",
      "Train Epoch: 94 [29568/54000 (55%)] Loss: -248486.265625\n",
      "Train Epoch: 94 [30976/54000 (57%)] Loss: -228727.953125\n",
      "Train Epoch: 94 [32384/54000 (60%)] Loss: -219558.750000\n",
      "Train Epoch: 94 [33792/54000 (63%)] Loss: -225857.781250\n",
      "Train Epoch: 94 [35200/54000 (65%)] Loss: -221616.859375\n",
      "Train Epoch: 94 [36608/54000 (68%)] Loss: -227218.750000\n",
      "Train Epoch: 94 [38016/54000 (70%)] Loss: -231042.359375\n",
      "Train Epoch: 94 [39424/54000 (73%)] Loss: -230212.390625\n",
      "Train Epoch: 94 [40832/54000 (76%)] Loss: -226411.718750\n",
      "Train Epoch: 94 [42240/54000 (78%)] Loss: -217516.343750\n",
      "Train Epoch: 94 [43648/54000 (81%)] Loss: -229980.656250\n",
      "Train Epoch: 94 [45056/54000 (83%)] Loss: -248715.562500\n",
      "Train Epoch: 94 [46464/54000 (86%)] Loss: -221020.156250\n",
      "Train Epoch: 94 [47872/54000 (89%)] Loss: -218793.406250\n",
      "Train Epoch: 94 [49280/54000 (91%)] Loss: -218244.671875\n",
      "Train Epoch: 94 [50688/54000 (94%)] Loss: -248347.593750\n",
      "Train Epoch: 94 [52096/54000 (96%)] Loss: -217901.656250\n",
      "    epoch          : 94\n",
      "    loss           : -226082.92408044258\n",
      "    val_loss       : -230248.38643292684\n",
      "Train Epoch: 95 [0/54000 (0%)] Loss: -248707.687500\n",
      "Train Epoch: 95 [1408/54000 (3%)] Loss: -230015.781250\n",
      "Train Epoch: 95 [2816/54000 (5%)] Loss: -228374.281250\n",
      "Train Epoch: 95 [4224/54000 (8%)] Loss: -230031.765625\n",
      "Train Epoch: 95 [5632/54000 (10%)] Loss: -219325.734375\n",
      "Train Epoch: 95 [7040/54000 (13%)] Loss: -223143.656250\n",
      "Train Epoch: 95 [8448/54000 (16%)] Loss: -223500.593750\n",
      "Train Epoch: 95 [9856/54000 (18%)] Loss: -224795.187500\n",
      "Train Epoch: 95 [11264/54000 (21%)] Loss: -217931.015625\n",
      "Train Epoch: 95 [12672/54000 (23%)] Loss: -229639.703125\n",
      "Train Epoch: 95 [14080/54000 (26%)] Loss: -219856.312500\n",
      "Train Epoch: 95 [15488/54000 (29%)] Loss: -221725.578125\n",
      "Train Epoch: 95 [16896/54000 (31%)] Loss: -219176.468750\n",
      "Train Epoch: 95 [18304/54000 (34%)] Loss: -221329.140625\n",
      "Train Epoch: 95 [19712/54000 (37%)] Loss: -222637.578125\n",
      "Train Epoch: 95 [21120/54000 (39%)] Loss: -215345.468750\n",
      "Train Epoch: 95 [22528/54000 (42%)] Loss: -229956.562500\n",
      "Train Epoch: 95 [23936/54000 (44%)] Loss: -229959.156250\n",
      "Train Epoch: 95 [25344/54000 (47%)] Loss: -217246.921875\n",
      "Train Epoch: 95 [26752/54000 (50%)] Loss: -218489.687500\n",
      "Train Epoch: 95 [28160/54000 (52%)] Loss: -226507.781250\n",
      "Train Epoch: 95 [29568/54000 (55%)] Loss: -217746.000000\n",
      "Train Epoch: 95 [30976/54000 (57%)] Loss: -218825.062500\n",
      "Train Epoch: 95 [32384/54000 (60%)] Loss: -222335.906250\n",
      "Train Epoch: 95 [33792/54000 (63%)] Loss: -248611.875000\n",
      "Train Epoch: 95 [35200/54000 (65%)] Loss: -222029.937500\n",
      "Train Epoch: 95 [36608/54000 (68%)] Loss: -218921.765625\n",
      "Train Epoch: 95 [38016/54000 (70%)] Loss: -222452.593750\n",
      "Train Epoch: 95 [39424/54000 (73%)] Loss: -249058.609375\n",
      "Train Epoch: 95 [40832/54000 (76%)] Loss: -230297.734375\n",
      "Train Epoch: 95 [42240/54000 (78%)] Loss: -228583.796875\n",
      "Train Epoch: 95 [43648/54000 (81%)] Loss: -215664.500000\n",
      "Train Epoch: 95 [45056/54000 (83%)] Loss: -216180.234375\n",
      "Train Epoch: 95 [46464/54000 (86%)] Loss: -224155.562500\n",
      "Train Epoch: 95 [47872/54000 (89%)] Loss: -221394.000000\n",
      "Train Epoch: 95 [49280/54000 (91%)] Loss: -220003.531250\n",
      "Train Epoch: 95 [50688/54000 (94%)] Loss: -219244.625000\n",
      "Train Epoch: 95 [52096/54000 (96%)] Loss: -217197.625000\n",
      "    epoch          : 95\n",
      "    loss           : -226069.29952900717\n",
      "    val_loss       : -230357.6962652439\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch95.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 96 [0/54000 (0%)] Loss: -248136.093750\n",
      "Train Epoch: 96 [1408/54000 (3%)] Loss: -229216.687500\n",
      "Train Epoch: 96 [2816/54000 (5%)] Loss: -229508.062500\n",
      "Train Epoch: 96 [4224/54000 (8%)] Loss: -219024.000000\n",
      "Train Epoch: 96 [5632/54000 (10%)] Loss: -229566.890625\n",
      "Train Epoch: 96 [7040/54000 (13%)] Loss: -218001.515625\n",
      "Train Epoch: 96 [8448/54000 (16%)] Loss: -218571.468750\n",
      "Train Epoch: 96 [9856/54000 (18%)] Loss: -247731.375000\n",
      "Train Epoch: 96 [11264/54000 (21%)] Loss: -217605.437500\n",
      "Train Epoch: 96 [12672/54000 (23%)] Loss: -226024.468750\n",
      "Train Epoch: 96 [14080/54000 (26%)] Loss: -228392.593750\n",
      "Train Epoch: 96 [15488/54000 (29%)] Loss: -231318.656250\n",
      "Train Epoch: 96 [16896/54000 (31%)] Loss: -247844.062500\n",
      "Train Epoch: 96 [18304/54000 (34%)] Loss: -222442.968750\n",
      "Train Epoch: 96 [19712/54000 (37%)] Loss: -218978.531250\n",
      "Train Epoch: 96 [21120/54000 (39%)] Loss: -219413.859375\n",
      "Train Epoch: 96 [22528/54000 (42%)] Loss: -218612.640625\n",
      "Train Epoch: 96 [23936/54000 (44%)] Loss: -228565.546875\n",
      "Train Epoch: 96 [25344/54000 (47%)] Loss: -223526.406250\n",
      "Train Epoch: 96 [26752/54000 (50%)] Loss: -220506.531250\n",
      "Train Epoch: 96 [28160/54000 (52%)] Loss: -219257.562500\n",
      "Train Epoch: 96 [29568/54000 (55%)] Loss: -224562.781250\n",
      "Train Epoch: 96 [30976/54000 (57%)] Loss: -231235.531250\n",
      "Train Epoch: 96 [32384/54000 (60%)] Loss: -247140.281250\n",
      "Train Epoch: 96 [33792/54000 (63%)] Loss: -227659.578125\n",
      "Train Epoch: 96 [35200/54000 (65%)] Loss: -227227.562500\n",
      "Train Epoch: 96 [36608/54000 (68%)] Loss: -222719.906250\n",
      "Train Epoch: 96 [38016/54000 (70%)] Loss: -221785.281250\n",
      "Train Epoch: 96 [39424/54000 (73%)] Loss: -248574.031250\n",
      "Train Epoch: 96 [40832/54000 (76%)] Loss: -223029.062500\n",
      "Train Epoch: 96 [42240/54000 (78%)] Loss: -218922.781250\n",
      "Train Epoch: 96 [43648/54000 (81%)] Loss: -220005.265625\n",
      "Train Epoch: 96 [45056/54000 (83%)] Loss: -219411.906250\n",
      "Train Epoch: 96 [46464/54000 (86%)] Loss: -218999.843750\n",
      "Train Epoch: 96 [47872/54000 (89%)] Loss: -227764.312500\n",
      "Train Epoch: 96 [49280/54000 (91%)] Loss: -228938.093750\n",
      "Train Epoch: 96 [50688/54000 (94%)] Loss: -230290.640625\n",
      "Train Epoch: 96 [52096/54000 (96%)] Loss: -225827.812500\n",
      "    epoch          : 96\n",
      "    loss           : -226191.23650568182\n",
      "    val_loss       : -230336.4512195122\n",
      "Train Epoch: 97 [0/54000 (0%)] Loss: -248774.953125\n",
      "Train Epoch: 97 [1408/54000 (3%)] Loss: -248281.062500\n",
      "Train Epoch: 97 [2816/54000 (5%)] Loss: -219467.031250\n",
      "Train Epoch: 97 [4224/54000 (8%)] Loss: -216287.937500\n",
      "Train Epoch: 97 [5632/54000 (10%)] Loss: -229823.218750\n",
      "Train Epoch: 97 [7040/54000 (13%)] Loss: -231834.187500\n",
      "Train Epoch: 97 [8448/54000 (16%)] Loss: -219686.093750\n",
      "Train Epoch: 97 [9856/54000 (18%)] Loss: -219170.265625\n",
      "Train Epoch: 97 [11264/54000 (21%)] Loss: -220980.687500\n",
      "Train Epoch: 97 [12672/54000 (23%)] Loss: -230648.234375\n",
      "Train Epoch: 97 [14080/54000 (26%)] Loss: -223345.515625\n",
      "Train Epoch: 97 [15488/54000 (29%)] Loss: -248087.125000\n",
      "Train Epoch: 97 [16896/54000 (31%)] Loss: -222858.750000\n",
      "Train Epoch: 97 [18304/54000 (34%)] Loss: -218180.093750\n",
      "Train Epoch: 97 [19712/54000 (37%)] Loss: -229491.093750\n",
      "Train Epoch: 97 [21120/54000 (39%)] Loss: -248633.531250\n",
      "Train Epoch: 97 [22528/54000 (42%)] Loss: -216069.562500\n",
      "Train Epoch: 97 [23936/54000 (44%)] Loss: -224250.203125\n",
      "Train Epoch: 97 [25344/54000 (47%)] Loss: -216931.312500\n",
      "Train Epoch: 97 [26752/54000 (50%)] Loss: -231254.187500\n",
      "Train Epoch: 97 [28160/54000 (52%)] Loss: -221508.062500\n",
      "Train Epoch: 97 [29568/54000 (55%)] Loss: -229588.562500\n",
      "Train Epoch: 97 [30976/54000 (57%)] Loss: -248311.234375\n",
      "Train Epoch: 97 [32384/54000 (60%)] Loss: -219734.296875\n",
      "Train Epoch: 97 [33792/54000 (63%)] Loss: -219429.656250\n",
      "Train Epoch: 97 [35200/54000 (65%)] Loss: -221249.218750\n",
      "Train Epoch: 97 [36608/54000 (68%)] Loss: -220353.890625\n",
      "Train Epoch: 97 [38016/54000 (70%)] Loss: -218880.562500\n",
      "Train Epoch: 97 [39424/54000 (73%)] Loss: -230178.578125\n",
      "Train Epoch: 97 [40832/54000 (76%)] Loss: -229900.953125\n",
      "Train Epoch: 97 [42240/54000 (78%)] Loss: -229810.843750\n",
      "Train Epoch: 97 [43648/54000 (81%)] Loss: -223136.890625\n",
      "Train Epoch: 97 [45056/54000 (83%)] Loss: -224073.312500\n",
      "Train Epoch: 97 [46464/54000 (86%)] Loss: -227449.156250\n",
      "Train Epoch: 97 [47872/54000 (89%)] Loss: -219453.781250\n",
      "Train Epoch: 97 [49280/54000 (91%)] Loss: -223806.250000\n",
      "Train Epoch: 97 [50688/54000 (94%)] Loss: -247978.984375\n",
      "Train Epoch: 97 [52096/54000 (96%)] Loss: -230179.406250\n",
      "    epoch          : 97\n",
      "    loss           : -226323.42804276315\n",
      "    val_loss       : -230095.3633765244\n",
      "Train Epoch: 98 [0/54000 (0%)] Loss: -248006.843750\n",
      "Train Epoch: 98 [1408/54000 (3%)] Loss: -222527.937500\n",
      "Train Epoch: 98 [2816/54000 (5%)] Loss: -228806.328125\n",
      "Train Epoch: 98 [4224/54000 (8%)] Loss: -231078.515625\n",
      "Train Epoch: 98 [5632/54000 (10%)] Loss: -248145.718750\n",
      "Train Epoch: 98 [7040/54000 (13%)] Loss: -219913.343750\n",
      "Train Epoch: 98 [8448/54000 (16%)] Loss: -229739.125000\n",
      "Train Epoch: 98 [9856/54000 (18%)] Loss: -219353.671875\n",
      "Train Epoch: 98 [11264/54000 (21%)] Loss: -222712.000000\n",
      "Train Epoch: 98 [12672/54000 (23%)] Loss: -228730.156250\n",
      "Train Epoch: 98 [14080/54000 (26%)] Loss: -248040.000000\n",
      "Train Epoch: 98 [15488/54000 (29%)] Loss: -224798.343750\n",
      "Train Epoch: 98 [16896/54000 (31%)] Loss: -226271.453125\n",
      "Train Epoch: 98 [18304/54000 (34%)] Loss: -224365.937500\n",
      "Train Epoch: 98 [19712/54000 (37%)] Loss: -229812.953125\n",
      "Train Epoch: 98 [21120/54000 (39%)] Loss: -247276.875000\n",
      "Train Epoch: 98 [22528/54000 (42%)] Loss: -231216.218750\n",
      "Train Epoch: 98 [23936/54000 (44%)] Loss: -223196.265625\n",
      "Train Epoch: 98 [25344/54000 (47%)] Loss: -225055.781250\n",
      "Train Epoch: 98 [26752/54000 (50%)] Loss: -249334.468750\n",
      "Train Epoch: 98 [28160/54000 (52%)] Loss: -219181.781250\n",
      "Train Epoch: 98 [29568/54000 (55%)] Loss: -219629.796875\n",
      "Train Epoch: 98 [30976/54000 (57%)] Loss: -247049.359375\n",
      "Train Epoch: 98 [32384/54000 (60%)] Loss: -224781.281250\n",
      "Train Epoch: 98 [33792/54000 (63%)] Loss: -229535.343750\n",
      "Train Epoch: 98 [35200/54000 (65%)] Loss: -230716.093750\n",
      "Train Epoch: 98 [36608/54000 (68%)] Loss: -222178.890625\n",
      "Train Epoch: 98 [38016/54000 (70%)] Loss: -223100.671875\n",
      "Train Epoch: 98 [39424/54000 (73%)] Loss: -221020.265625\n",
      "Train Epoch: 98 [40832/54000 (76%)] Loss: -223832.000000\n",
      "Train Epoch: 98 [42240/54000 (78%)] Loss: -227758.343750\n",
      "Train Epoch: 98 [43648/54000 (81%)] Loss: -219810.515625\n",
      "Train Epoch: 98 [45056/54000 (83%)] Loss: -219242.750000\n",
      "Train Epoch: 98 [46464/54000 (86%)] Loss: -219145.828125\n",
      "Train Epoch: 98 [47872/54000 (89%)] Loss: -223551.687500\n",
      "Train Epoch: 98 [49280/54000 (91%)] Loss: -228207.593750\n",
      "Train Epoch: 98 [50688/54000 (94%)] Loss: -248638.656250\n",
      "Train Epoch: 98 [52096/54000 (96%)] Loss: -226855.031250\n",
      "    epoch          : 98\n",
      "    loss           : -226405.9335750598\n",
      "    val_loss       : -230169.71036585365\n",
      "Train Epoch: 99 [0/54000 (0%)] Loss: -228098.437500\n",
      "Train Epoch: 99 [1408/54000 (3%)] Loss: -227785.578125\n",
      "Train Epoch: 99 [2816/54000 (5%)] Loss: -229945.250000\n",
      "Train Epoch: 99 [4224/54000 (8%)] Loss: -229172.968750\n",
      "Train Epoch: 99 [5632/54000 (10%)] Loss: -217298.953125\n",
      "Train Epoch: 99 [7040/54000 (13%)] Loss: -221192.593750\n",
      "Train Epoch: 99 [8448/54000 (16%)] Loss: -223824.000000\n",
      "Train Epoch: 99 [9856/54000 (18%)] Loss: -218821.718750\n",
      "Train Epoch: 99 [11264/54000 (21%)] Loss: -223673.031250\n",
      "Train Epoch: 99 [12672/54000 (23%)] Loss: -222645.468750\n",
      "Train Epoch: 99 [14080/54000 (26%)] Loss: -223655.359375\n",
      "Train Epoch: 99 [15488/54000 (29%)] Loss: -246823.593750\n",
      "Train Epoch: 99 [16896/54000 (31%)] Loss: -219752.468750\n",
      "Train Epoch: 99 [18304/54000 (34%)] Loss: -229500.812500\n",
      "Train Epoch: 99 [19712/54000 (37%)] Loss: -231348.265625\n",
      "Train Epoch: 99 [21120/54000 (39%)] Loss: -229681.171875\n",
      "Train Epoch: 99 [22528/54000 (42%)] Loss: -229486.703125\n",
      "Train Epoch: 99 [23936/54000 (44%)] Loss: -219518.609375\n",
      "Train Epoch: 99 [25344/54000 (47%)] Loss: -219067.093750\n",
      "Train Epoch: 99 [26752/54000 (50%)] Loss: -248213.140625\n",
      "Train Epoch: 99 [28160/54000 (52%)] Loss: -226777.156250\n",
      "Train Epoch: 99 [29568/54000 (55%)] Loss: -227848.406250\n",
      "Train Epoch: 99 [30976/54000 (57%)] Loss: -222029.625000\n",
      "Train Epoch: 99 [32384/54000 (60%)] Loss: -247397.593750\n",
      "Train Epoch: 99 [33792/54000 (63%)] Loss: -228514.968750\n",
      "Train Epoch: 99 [35200/54000 (65%)] Loss: -230734.328125\n",
      "Train Epoch: 99 [36608/54000 (68%)] Loss: -248978.031250\n",
      "Train Epoch: 99 [38016/54000 (70%)] Loss: -248796.937500\n",
      "Train Epoch: 99 [39424/54000 (73%)] Loss: -218118.250000\n",
      "Train Epoch: 99 [40832/54000 (76%)] Loss: -218959.625000\n",
      "Train Epoch: 99 [42240/54000 (78%)] Loss: -218489.156250\n",
      "Train Epoch: 99 [43648/54000 (81%)] Loss: -230495.437500\n",
      "Train Epoch: 99 [45056/54000 (83%)] Loss: -220546.500000\n",
      "Train Epoch: 99 [46464/54000 (86%)] Loss: -224139.921875\n",
      "Train Epoch: 99 [47872/54000 (89%)] Loss: -225615.718750\n",
      "Train Epoch: 99 [49280/54000 (91%)] Loss: -218905.906250\n",
      "Train Epoch: 99 [50688/54000 (94%)] Loss: -248681.515625\n",
      "Train Epoch: 99 [52096/54000 (96%)] Loss: -229635.703125\n",
      "    epoch          : 99\n",
      "    loss           : -226468.91679126795\n",
      "    val_loss       : -230103.4735137195\n",
      "Train Epoch: 100 [0/54000 (0%)] Loss: -248634.937500\n",
      "Train Epoch: 100 [1408/54000 (3%)] Loss: -230546.046875\n",
      "Train Epoch: 100 [2816/54000 (5%)] Loss: -228510.687500\n",
      "Train Epoch: 100 [4224/54000 (8%)] Loss: -229493.765625\n",
      "Train Epoch: 100 [5632/54000 (10%)] Loss: -219346.859375\n",
      "Train Epoch: 100 [7040/54000 (13%)] Loss: -218375.984375\n",
      "Train Epoch: 100 [8448/54000 (16%)] Loss: -219268.000000\n",
      "Train Epoch: 100 [9856/54000 (18%)] Loss: -219794.812500\n",
      "Train Epoch: 100 [11264/54000 (21%)] Loss: -218590.281250\n",
      "Train Epoch: 100 [12672/54000 (23%)] Loss: -248467.546875\n",
      "Train Epoch: 100 [14080/54000 (26%)] Loss: -228649.406250\n",
      "Train Epoch: 100 [15488/54000 (29%)] Loss: -225500.843750\n",
      "Train Epoch: 100 [16896/54000 (31%)] Loss: -224328.578125\n",
      "Train Epoch: 100 [18304/54000 (34%)] Loss: -248357.625000\n",
      "Train Epoch: 100 [19712/54000 (37%)] Loss: -228417.406250\n",
      "Train Epoch: 100 [21120/54000 (39%)] Loss: -223410.406250\n",
      "Train Epoch: 100 [22528/54000 (42%)] Loss: -228288.968750\n",
      "Train Epoch: 100 [23936/54000 (44%)] Loss: -223220.781250\n",
      "Train Epoch: 100 [25344/54000 (47%)] Loss: -219193.796875\n",
      "Train Epoch: 100 [26752/54000 (50%)] Loss: -219831.000000\n",
      "Train Epoch: 100 [28160/54000 (52%)] Loss: -223794.859375\n",
      "Train Epoch: 100 [29568/54000 (55%)] Loss: -219387.921875\n",
      "Train Epoch: 100 [30976/54000 (57%)] Loss: -220723.093750\n",
      "Train Epoch: 100 [32384/54000 (60%)] Loss: -228045.312500\n",
      "Train Epoch: 100 [33792/54000 (63%)] Loss: -230477.937500\n",
      "Train Epoch: 100 [35200/54000 (65%)] Loss: -231832.656250\n",
      "Train Epoch: 100 [36608/54000 (68%)] Loss: -224795.875000\n",
      "Train Epoch: 100 [38016/54000 (70%)] Loss: -220414.187500\n",
      "Train Epoch: 100 [39424/54000 (73%)] Loss: -217586.937500\n",
      "Train Epoch: 100 [40832/54000 (76%)] Loss: -224957.937500\n",
      "Train Epoch: 100 [42240/54000 (78%)] Loss: -228173.656250\n",
      "Train Epoch: 100 [43648/54000 (81%)] Loss: -231836.203125\n",
      "Train Epoch: 100 [45056/54000 (83%)] Loss: -248662.640625\n",
      "Train Epoch: 100 [46464/54000 (86%)] Loss: -224618.281250\n",
      "Train Epoch: 100 [47872/54000 (89%)] Loss: -226185.687500\n",
      "Train Epoch: 100 [49280/54000 (91%)] Loss: -219150.218750\n",
      "Train Epoch: 100 [50688/54000 (94%)] Loss: -218868.640625\n",
      "Train Epoch: 100 [52096/54000 (96%)] Loss: -230222.437500\n",
      "    epoch          : 100\n",
      "    loss           : -226553.0540146531\n",
      "    val_loss       : -230391.609375\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0502_113235/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
