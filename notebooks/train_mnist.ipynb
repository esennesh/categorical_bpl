{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eli/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [11:17:37] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 50,\n",
    "    \"factor\": 0.1,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 521251.875000\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: 165962.812500\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -342528.250000\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -432152.625000\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -285557.937500\n",
      "    epoch          : 1\n",
      "    loss           : -116066.02691831683\n",
      "    val_loss       : -247751.18281539678\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -341665.375000\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -338387.062500\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -214675.781250\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -281941.750000\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -95408.820312\n",
      "    epoch          : 2\n",
      "    loss           : -272623.3972772277\n",
      "    val_loss       : -284026.973649174\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -542623.250000\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -215582.296875\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -365133.687500\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -115200.710938\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -288986.625000\n",
      "    epoch          : 3\n",
      "    loss           : -298591.42775371287\n",
      "    val_loss       : -281022.31482961477\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -554880.625000\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -338225.812500\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -213480.625000\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -98125.664062\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -291108.625000\n",
      "    epoch          : 4\n",
      "    loss           : -304101.5402227723\n",
      "    val_loss       : -316354.2709702969\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -559695.500000\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -295576.312500\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -288530.593750\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -349156.562500\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -179432.687500\n",
      "    epoch          : 5\n",
      "    loss           : -357965.53519492573\n",
      "    val_loss       : -388353.3191771507\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -435427.375000\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -416263.718750\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -252413.281250\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -378228.968750\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -433525.312500\n",
      "    epoch          : 6\n",
      "    loss           : -416602.92837252474\n",
      "    val_loss       : -437899.55646853446\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -647175.875000\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -382626.093750\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -377188.500000\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -487362.531250\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -500424.406250\n",
      "    epoch          : 7\n",
      "    loss           : -467904.1401608911\n",
      "    val_loss       : -493195.5949317932\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -703728.000000\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -510245.437500\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -560866.250000\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -558884.125000\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -522822.750000\n",
      "    epoch          : 8\n",
      "    loss           : -503346.0606435643\n",
      "    val_loss       : -503204.7371119976\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -693858.125000\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -453736.718750\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -363316.500000\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -476250.593750\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -535056.562500\n",
      "    epoch          : 9\n",
      "    loss           : -507580.39851485146\n",
      "    val_loss       : -521434.3595559597\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -715014.500000\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -554027.500000\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -535098.062500\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -487922.750000\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -359847.500000\n",
      "    epoch          : 10\n",
      "    loss           : -525093.3638613861\n",
      "    val_loss       : -521681.92272729875\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -698298.875000\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -497837.000000\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -456575.125000\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -547909.187500\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -574900.125000\n",
      "    epoch          : 11\n",
      "    loss           : -540076.9972153465\n",
      "    val_loss       : -577189.0558560372\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -744928.375000\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -552026.250000\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -534137.875000\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -662527.875000\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -669455.375000\n",
      "    epoch          : 12\n",
      "    loss           : -616912.1259282178\n",
      "    val_loss       : -639359.4901446343\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -782642.750000\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -685955.437500\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -561176.812500\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -633317.937500\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -652426.812500\n",
      "    epoch          : 13\n",
      "    loss           : -646257.5139232674\n",
      "    val_loss       : -653299.5800263404\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -693979.500000\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -628982.062500\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -573524.687500\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -499595.281250\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -688738.000000\n",
      "    epoch          : 14\n",
      "    loss           : -656407.2874381188\n",
      "    val_loss       : -657477.2150645733\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -793304.812500\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -695197.000000\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -581268.812500\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -514994.250000\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -692864.750000\n",
      "    epoch          : 15\n",
      "    loss           : -663506.0225866337\n",
      "    val_loss       : -665395.1869844437\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -705321.250000\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -637259.125000\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -674072.187500\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -690291.125000\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -656187.000000\n",
      "    epoch          : 16\n",
      "    loss           : -669574.8483910891\n",
      "    val_loss       : -671034.8570766449\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -823737.250000\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -710708.875000\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -590561.750000\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -580896.500000\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -698385.000000\n",
      "    epoch          : 17\n",
      "    loss           : -671327.5049504951\n",
      "    val_loss       : -674656.3176772117\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -834199.437500\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -736748.500000\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -656979.062500\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -674794.125000\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -688855.625000\n",
      "    epoch          : 18\n",
      "    loss           : -676624.8824257426\n",
      "    val_loss       : -680678.3300762177\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -846944.312500\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -654215.000000\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -738832.750000\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -542119.687500\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -702340.250000\n",
      "    epoch          : 19\n",
      "    loss           : -680137.7824876237\n",
      "    val_loss       : -683201.8933541297\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -728058.750000\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -577818.125000\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -659297.750000\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -662504.875000\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -704301.937500\n",
      "    epoch          : 20\n",
      "    loss           : -684822.7165841584\n",
      "    val_loss       : -686593.4060512542\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -861201.250000\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -656711.312500\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -556954.625000\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -860106.687500\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -654706.250000\n",
      "    epoch          : 21\n",
      "    loss           : -682814.843440594\n",
      "    val_loss       : -677504.5168428421\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -850057.375000\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -647167.250000\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -594321.250000\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -661186.875000\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -672496.250000\n",
      "    epoch          : 22\n",
      "    loss           : -685526.6280940594\n",
      "    val_loss       : -690080.604616642\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -863156.062500\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -641500.750000\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -589419.687500\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -567108.812500\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -707242.375000\n",
      "    epoch          : 23\n",
      "    loss           : -687359.8298267326\n",
      "    val_loss       : -689529.993366623\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -860855.625000\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -616853.812500\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -608713.375000\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -614349.125000\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -713543.437500\n",
      "    epoch          : 24\n",
      "    loss           : -693597.4356435643\n",
      "    val_loss       : -699870.8369737625\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -871072.625000\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -680264.937500\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -710359.625000\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -717533.625000\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -709032.562500\n",
      "    epoch          : 25\n",
      "    loss           : -706521.5074257426\n",
      "    val_loss       : -702224.9099446774\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -868101.312500\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -758566.875000\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -767632.250000\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -600215.812500\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -694838.437500\n",
      "    epoch          : 26\n",
      "    loss           : -707451.8799504951\n",
      "    val_loss       : -696551.6711170196\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -853373.625000\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -683506.250000\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -675507.250000\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -707228.875000\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -705152.375000\n",
      "    epoch          : 27\n",
      "    loss           : -704320.4882425743\n",
      "    val_loss       : -709265.3180284023\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -751882.125000\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -752761.000000\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -632432.937500\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -687150.000000\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -708474.812500\n",
      "    epoch          : 28\n",
      "    loss           : -712609.3564356435\n",
      "    val_loss       : -716436.848363781\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -881662.062500\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -693968.875000\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -616737.937500\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -730926.375000\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -698837.937500\n",
      "    epoch          : 29\n",
      "    loss           : -722028.9832920792\n",
      "    val_loss       : -724447.846694851\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -894127.875000\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -686655.500000\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -699072.375000\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -896444.500000\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -631986.437500\n",
      "    epoch          : 30\n",
      "    loss           : -725574.0352722772\n",
      "    val_loss       : -724355.6729994297\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -688014.437500\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -775406.687500\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -786430.750000\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -739968.937500\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -701159.375000\n",
      "    epoch          : 31\n",
      "    loss           : -729898.3125\n",
      "    val_loss       : -733353.1982353211\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -904305.375000\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -692716.375000\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -647592.125000\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -709382.062500\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -703249.500000\n",
      "    epoch          : 32\n",
      "    loss           : -729593.0946782178\n",
      "    val_loss       : -717596.2447628021\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -908520.812500\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -695672.437500\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -648567.000000\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -702065.187500\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -746251.250000\n",
      "    epoch          : 33\n",
      "    loss           : -736896.9127475248\n",
      "    val_loss       : -738528.1593221665\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -912953.125000\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -789998.875000\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -773784.500000\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -642236.375000\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -725417.625000\n",
      "    epoch          : 34\n",
      "    loss           : -734930.8824257426\n",
      "    val_loss       : -732736.1900335312\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -912563.875000\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -780862.375000\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -650523.312500\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -743002.062500\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -733518.125000\n",
      "    epoch          : 35\n",
      "    loss           : -738091.7580445545\n",
      "    val_loss       : -737427.3694005013\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -778703.750000\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -692442.875000\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -652145.875000\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -650743.937500\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -729770.250000\n",
      "    epoch          : 36\n",
      "    loss           : -741118.4077970297\n",
      "    val_loss       : -739831.3733822822\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -911516.937500\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -701064.500000\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -713046.812500\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -726457.375000\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -712749.812500\n",
      "    epoch          : 37\n",
      "    loss           : -741972.7042079208\n",
      "    val_loss       : -743132.2150883675\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -919553.500000\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -702857.250000\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -666450.250000\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -708136.000000\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -723201.875000\n",
      "    epoch          : 38\n",
      "    loss           : -743104.8384900991\n",
      "    val_loss       : -742051.2128865242\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -785390.062500\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -659126.937500\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -644288.500000\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -665771.625000\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -743839.875000\n",
      "    epoch          : 39\n",
      "    loss           : -740533.2128712871\n",
      "    val_loss       : -741812.1837423325\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -920210.750000\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -703551.625000\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -707975.187500\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -707046.750000\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -741588.875000\n",
      "    epoch          : 40\n",
      "    loss           : -737244.3446782178\n",
      "    val_loss       : -729346.8420680047\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -917142.562500\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -677381.562500\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -680104.937500\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -793843.687500\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -708806.000000\n",
      "    epoch          : 41\n",
      "    loss           : -745813.6410891089\n",
      "    val_loss       : -744314.5356384277\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -921165.500000\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -687580.812500\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -680207.750000\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -703625.750000\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -782560.125000\n",
      "    epoch          : 42\n",
      "    loss           : -741480.4938118812\n",
      "    val_loss       : -743912.412150383\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -671078.812500\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -705638.000000\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -727302.500000\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -725994.750000\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -786775.500000\n",
      "    epoch          : 43\n",
      "    loss           : -747917.4764851485\n",
      "    val_loss       : -748879.9259633065\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -922770.375000\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -791103.812500\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -727810.000000\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -690060.250000\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -748414.750000\n",
      "    epoch          : 44\n",
      "    loss           : -751960.1008663366\n",
      "    val_loss       : -753175.9957143783\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -929401.187500\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -720656.625000\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -796039.125000\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -714063.375000\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -691905.000000\n",
      "    epoch          : 45\n",
      "    loss           : -748259.6967821782\n",
      "    val_loss       : -750797.3085507869\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -927427.437500\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -716235.000000\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -672743.437500\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -750309.875000\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -733670.062500\n",
      "    epoch          : 46\n",
      "    loss           : -753321.8533415842\n",
      "    val_loss       : -751914.3358371735\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -926803.375000\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -718472.375000\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -683185.375000\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -796260.125000\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -720070.750000\n",
      "    epoch          : 47\n",
      "    loss           : -755971.2902227723\n",
      "    val_loss       : -756607.684447956\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -931500.875000\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -741229.312500\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -802632.375000\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -681267.750000\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -729532.000000\n",
      "    epoch          : 48\n",
      "    loss           : -756200.2555693069\n",
      "    val_loss       : -755568.3550739288\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -933425.062500\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -718457.500000\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -694319.750000\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -756909.937500\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -754182.437500\n",
      "    epoch          : 49\n",
      "    loss           : -756739.5525990099\n",
      "    val_loss       : -754501.4304607392\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -787204.937500\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -719448.500000\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -740836.375000\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -738667.500000\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -725016.000000\n",
      "    epoch          : 50\n",
      "    loss           : -755269.4300742574\n",
      "    val_loss       : -754595.2266777039\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/1109_111738/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -930947.250000\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -719800.750000\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -792829.375000\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -723489.437500\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -694708.375000\n",
      "    epoch          : 51\n",
      "    loss           : -757383.4300742574\n",
      "    val_loss       : -760083.2926559448\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -935947.375000\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -799386.062500\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -764034.437500\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -681662.750000\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -729805.625000\n",
      "    epoch          : 52\n",
      "    loss           : -750029.1138613861\n",
      "    val_loss       : -752689.4424126626\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -926014.500000\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -732509.687500\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -729468.375000\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -685835.750000\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -726760.562500\n",
      "    epoch          : 53\n",
      "    loss           : -751017.7456683168\n",
      "    val_loss       : -739867.8536881447\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -904659.500000\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -703023.562500\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -760468.000000\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -927646.000000\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -761404.875000\n",
      "    epoch          : 54\n",
      "    loss           : -756759.7982673268\n",
      "    val_loss       : -758510.867649889\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -928740.250000\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -801424.625000\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -800658.562500\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -726975.625000\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -744334.812500\n",
      "    epoch          : 55\n",
      "    loss           : -761580.6008663366\n",
      "    val_loss       : -758216.1305512429\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -925720.187500\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -737278.750000\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -685540.875000\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -705988.812500\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -737266.312500\n",
      "    epoch          : 56\n",
      "    loss           : -764751.4040841584\n",
      "    val_loss       : -765861.0682578087\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -936725.125000\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -721906.750000\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -738820.625000\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -763278.562500\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -712534.500000\n",
      "    epoch          : 57\n",
      "    loss           : -765597.8199257426\n",
      "    val_loss       : -765749.7553606987\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -929885.125000\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -733031.250000\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -708139.000000\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -741916.437500\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -743816.125000\n",
      "    epoch          : 58\n",
      "    loss           : -768367.8353960396\n",
      "    val_loss       : -768260.4657290459\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -936529.875000\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -797845.312500\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -685802.937500\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -703581.500000\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -750309.125000\n",
      "    epoch          : 59\n",
      "    loss           : -765240.7555693069\n",
      "    val_loss       : -770685.6533071517\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -939285.187500\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -698231.875000\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -817978.562500\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -938149.875000\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -779549.750000\n",
      "    epoch          : 60\n",
      "    loss           : -771497.1268564357\n",
      "    val_loss       : -768571.9636665344\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -938006.562500\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -732665.375000\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -749772.375000\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -713170.750000\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -750942.187500\n",
      "    epoch          : 61\n",
      "    loss           : -773865.1924504951\n",
      "    val_loss       : -771391.7104447365\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -936008.687500\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -814577.000000\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -782323.750000\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -942232.000000\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -746976.875000\n",
      "    epoch          : 62\n",
      "    loss           : -774923.458539604\n",
      "    val_loss       : -775285.7655329704\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -938673.500000\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -730013.812500\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -715598.000000\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -820414.000000\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -716388.062500\n",
      "    epoch          : 63\n",
      "    loss           : -777418.0061881188\n",
      "    val_loss       : -778258.593929863\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -942081.750000\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -739931.625000\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -744778.687500\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -705660.375000\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -765430.312500\n",
      "    epoch          : 64\n",
      "    loss           : -780615.6584158416\n",
      "    val_loss       : -778998.4118742943\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -825230.125000\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -714672.812500\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -775974.125000\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -754578.062500\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -754118.750000\n",
      "    epoch          : 65\n",
      "    loss           : -771320.2939356435\n",
      "    val_loss       : -775871.9513244629\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -817454.000000\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -738062.125000\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -715926.312500\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -944449.187500\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -781272.125000\n",
      "    epoch          : 66\n",
      "    loss           : -779729.5030940594\n",
      "    val_loss       : -778367.1076667786\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -946160.750000\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -734902.625000\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -759333.875000\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -821354.062500\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -783115.250000\n",
      "    epoch          : 67\n",
      "    loss           : -778525.2871287129\n",
      "    val_loss       : -781346.2879751206\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -947927.125000\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -825651.750000\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -725938.937500\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -713128.187500\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -752545.687500\n",
      "    epoch          : 68\n",
      "    loss           : -784133.1008663366\n",
      "    val_loss       : -781496.338377142\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -950653.125000\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -735236.875000\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -754105.125000\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -823667.187500\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -752889.375000\n",
      "    epoch          : 69\n",
      "    loss           : -783063.8391089109\n",
      "    val_loss       : -782442.9152846336\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -822908.437500\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -739336.500000\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -826470.375000\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -832295.500000\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -786668.437500\n",
      "    epoch          : 70\n",
      "    loss           : -783997.4783415842\n",
      "    val_loss       : -764119.7300519943\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -947162.687500\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -735300.625000\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -703077.375000\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -752040.250000\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -745386.750000\n",
      "    epoch          : 71\n",
      "    loss           : -776583.426980198\n",
      "    val_loss       : -771264.6379096985\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -948578.750000\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -818139.625000\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -819143.937500\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -705744.062500\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -709997.687500\n",
      "    epoch          : 72\n",
      "    loss           : -777740.6676980198\n",
      "    val_loss       : -780057.661961937\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -949256.187500\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -826318.125000\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -718954.500000\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -949620.250000\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -753821.000000\n",
      "    epoch          : 73\n",
      "    loss           : -784644.771039604\n",
      "    val_loss       : -784862.9928440094\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -947265.250000\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -748181.687500\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -778703.437500\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -776527.500000\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -784919.000000\n",
      "    epoch          : 74\n",
      "    loss           : -787916.4746287129\n",
      "    val_loss       : -787217.6086251258\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -950063.312500\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -746738.312500\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -744619.625000\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -709422.625000\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -789947.500000\n",
      "    epoch          : 75\n",
      "    loss           : -782649.8991336634\n",
      "    val_loss       : -785067.2445054054\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -948634.375000\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -717631.625000\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -760150.125000\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -785659.500000\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -783491.000000\n",
      "    epoch          : 76\n",
      "    loss           : -789234.5074257426\n",
      "    val_loss       : -787686.5663448334\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -945120.625000\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -746305.000000\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -788399.062500\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -954482.875000\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -781502.937500\n",
      "    epoch          : 77\n",
      "    loss           : -786023.2165841584\n",
      "    val_loss       : -744149.6480799675\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -944693.250000\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -745270.812500\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -776758.000000\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -947740.562500\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -783164.812500\n",
      "    epoch          : 78\n",
      "    loss           : -776600.8948019802\n",
      "    val_loss       : -783142.6655266762\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -945989.312500\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -814019.125000\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -709967.875000\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -755455.625000\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -778405.812500\n",
      "    epoch          : 79\n",
      "    loss           : -786221.6274752475\n",
      "    val_loss       : -786485.1848094941\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -951801.937500\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -748237.562500\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -791262.250000\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -786221.000000\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -779029.250000\n",
      "    epoch          : 80\n",
      "    loss           : -787722.1992574257\n",
      "    val_loss       : -781068.031161213\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -946308.000000\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -824641.312500\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -729504.250000\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -790710.750000\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -784652.250000\n",
      "    epoch          : 81\n",
      "    loss           : -787311.7103960396\n",
      "    val_loss       : -788441.250586319\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -952693.250000\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -743987.625000\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -715474.000000\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -732647.875000\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -791269.125000\n",
      "    epoch          : 82\n",
      "    loss           : -790931.2425742574\n",
      "    val_loss       : -790142.5643332482\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -951419.125000\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -752254.125000\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -793680.187500\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -828647.875000\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -790171.812500\n",
      "    epoch          : 83\n",
      "    loss           : -792788.2555693069\n",
      "    val_loss       : -788951.8384943008\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -954186.062500\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -746029.062500\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -709777.375000\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -778551.000000\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -784269.562500\n",
      "    epoch          : 84\n",
      "    loss           : -783456.4647277228\n",
      "    val_loss       : -778064.5772065163\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -927574.562500\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -740065.437500\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -824851.187500\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -759393.625000\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -795878.562500\n",
      "    epoch          : 85\n",
      "    loss           : -789397.1775990099\n",
      "    val_loss       : -791617.2483664512\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -955000.375000\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -826869.812500\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -704522.437500\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -949199.375000\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -792318.687500\n",
      "    epoch          : 86\n",
      "    loss           : -784462.5952970297\n",
      "    val_loss       : -786067.4966918945\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -953747.875000\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -835418.125000\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -734491.187500\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -830282.812500\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -788236.000000\n",
      "    epoch          : 87\n",
      "    loss           : -791475.603960396\n",
      "    val_loss       : -790857.5357232094\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -746219.062500\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -751549.062500\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -715584.750000\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -732634.062500\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -772966.500000\n",
      "    epoch          : 88\n",
      "    loss           : -791431.8471534654\n",
      "    val_loss       : -790345.9290173531\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -751267.750000\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -834423.562500\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -744448.062500\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -756605.625000\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -758435.500000\n",
      "    epoch          : 89\n",
      "    loss           : -793047.8112623763\n",
      "    val_loss       : -789764.2637505531\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -955264.937500\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -748547.812500\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -734912.437500\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -774123.687500\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -782160.625000\n",
      "    epoch          : 90\n",
      "    loss           : -791912.5754950495\n",
      "    val_loss       : -793504.2546417236\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -954401.125000\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -833651.500000\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -714777.000000\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -791577.812500\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -761203.000000\n",
      "    epoch          : 91\n",
      "    loss           : -796193.4071782178\n",
      "    val_loss       : -795359.7673189163\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -956884.500000\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -751910.625000\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -744093.937500\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -959058.500000\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -793092.500000\n",
      "    epoch          : 92\n",
      "    loss           : -799406.6305693069\n",
      "    val_loss       : -798801.6770640373\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -840215.812500\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -754679.375000\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -745398.562500\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -726739.687500\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -794945.875000\n",
      "    epoch          : 93\n",
      "    loss           : -799241.1961633663\n",
      "    val_loss       : -771278.3795988082\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -730985.937500\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -745154.687500\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -792968.125000\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -793240.250000\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -791333.875000\n",
      "    epoch          : 94\n",
      "    loss           : -793965.0439356435\n",
      "    val_loss       : -796819.7707523346\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -958558.375000\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -796285.437500\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -720378.875000\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -960446.937500\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -758243.625000\n",
      "    epoch          : 95\n",
      "    loss           : -800594.0618811881\n",
      "    val_loss       : -801091.7757703781\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -763856.437500\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -851257.125000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -720197.000000\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -962911.500000\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -804004.187500\n",
      "    epoch          : 96\n",
      "    loss           : -801739.2679455446\n",
      "    val_loss       : -789503.2623001098\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -749846.562500\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -847574.062500\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -760532.250000\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -962612.875000\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -732059.562500\n",
      "    epoch          : 97\n",
      "    loss           : -797444.593440594\n",
      "    val_loss       : -796373.7964229584\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -955648.687500\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -757932.687500\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -741059.312500\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -739934.375000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -758324.250000\n",
      "    epoch          : 98\n",
      "    loss           : -800278.5810643565\n",
      "    val_loss       : -799321.8920144081\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -849525.875000\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -754788.812500\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -834509.625000\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -965161.000000\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -797851.187500\n",
      "    epoch          : 99\n",
      "    loss           : -797414.6547029703\n",
      "    val_loss       : -792473.2304995537\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -960231.437500\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -745881.625000\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -797605.125000\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -709971.250000\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -716851.125000\n",
      "    epoch          : 100\n",
      "    loss           : -789167.8657178218\n",
      "    val_loss       : -787574.663851738\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/1109_111738/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -830118.812500\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -744759.500000\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -814869.875000\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -797473.062500\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -743636.000000\n",
      "    epoch          : 101\n",
      "    loss           : -794780.9146039604\n",
      "    val_loss       : -797273.5084551811\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -962076.625000\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -763239.750000\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -790432.562500\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -823751.375000\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -786398.562500\n",
      "    epoch          : 102\n",
      "    loss           : -780506.3675742574\n",
      "    val_loss       : -793629.6500308036\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -950819.062500\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -774959.562500\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -826692.375000\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -755545.750000\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -790087.500000\n",
      "    epoch          : 103\n",
      "    loss           : -791788.5495049505\n",
      "    val_loss       : -796921.3965072632\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -761404.312500\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -722616.312500\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -744884.687500\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -798660.375000\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -764064.750000\n",
      "    epoch          : 104\n",
      "    loss           : -800957.7741336634\n",
      "    val_loss       : -800349.2084902764\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -964737.500000\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -831483.875000\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -712292.062500\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -785470.625000\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -791009.125000\n",
      "    epoch          : 105\n",
      "    loss           : -796165.5284653465\n",
      "    val_loss       : -796467.4034154892\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -841562.312500\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -729776.687500\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -838750.500000\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -961871.125000\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -804520.312500\n",
      "    epoch          : 106\n",
      "    loss           : -801224.9777227723\n",
      "    val_loss       : -802282.9460996628\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -963897.125000\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -749008.625000\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -734303.312500\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -811982.875000\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -753835.875000\n",
      "    epoch          : 107\n",
      "    loss           : -787617.9399752475\n",
      "    val_loss       : -793168.6670445442\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -959375.375000\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -743745.125000\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -720210.187500\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -720194.437500\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -752258.437500\n",
      "    epoch          : 108\n",
      "    loss           : -791618.3805693069\n",
      "    val_loss       : -794404.7606517791\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -958485.875000\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -755327.625000\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -760944.625000\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -830276.437500\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -793204.125000\n",
      "    epoch          : 109\n",
      "    loss           : -798456.208539604\n",
      "    val_loss       : -800226.168794632\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -845610.312500\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -759527.875000\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -797660.625000\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -794568.437500\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -801916.000000\n",
      "    epoch          : 110\n",
      "    loss           : -803754.5717821782\n",
      "    val_loss       : -802731.8748550415\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -961226.375000\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -766857.812500\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -760384.187500\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -749796.500000\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -811828.875000\n",
      "    epoch          : 111\n",
      "    loss           : -804773.4733910891\n",
      "    val_loss       : -804859.2450370789\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -965742.187500\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -753509.062500\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -734738.625000\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -797853.000000\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -795315.187500\n",
      "    epoch          : 112\n",
      "    loss           : -804658.8514851485\n",
      "    val_loss       : -804655.3528798104\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -968488.000000\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -801921.875000\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -807994.000000\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -735537.625000\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -794764.312500\n",
      "    epoch          : 113\n",
      "    loss           : -804530.3948019802\n",
      "    val_loss       : -801700.2638871192\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -967266.875000\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -756717.437500\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -839790.125000\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -807753.687500\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -735880.812500\n",
      "    epoch          : 114\n",
      "    loss           : -804291.1423267326\n",
      "    val_loss       : -795364.1404297829\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -944051.250000\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -794342.625000\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -728510.125000\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -957498.312500\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -745697.500000\n",
      "    epoch          : 115\n",
      "    loss           : -802849.3310643565\n",
      "    val_loss       : -786532.8061497689\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -952143.562500\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -846223.562500\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -741661.375000\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -763675.125000\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -801239.000000\n",
      "    epoch          : 116\n",
      "    loss           : -799994.8521039604\n",
      "    val_loss       : -804481.0050747872\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -770463.750000\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -852347.187500\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -842682.000000\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -967225.750000\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -794883.062500\n",
      "    epoch          : 117\n",
      "    loss           : -805712.6008663366\n",
      "    val_loss       : -797688.4057147026\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -966067.562500\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -756958.625000\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -841599.312500\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -806475.750000\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -744656.000000\n",
      "    epoch          : 118\n",
      "    loss           : -802529.4387376237\n",
      "    val_loss       : -801400.8219604492\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -960147.187500\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -849970.062500\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -747600.625000\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -793664.562500\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -773165.375000\n",
      "    epoch          : 119\n",
      "    loss           : -784682.3638613861\n",
      "    val_loss       : -789193.2779152871\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -833751.625000\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -751959.375000\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -761918.250000\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -732700.187500\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -797549.312500\n",
      "    epoch          : 120\n",
      "    loss           : -796283.8242574257\n",
      "    val_loss       : -800175.5275649071\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -936951.625000\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -843406.375000\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -801602.250000\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -758580.500000\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -755378.875000\n",
      "    epoch          : 121\n",
      "    loss           : -803338.9504950495\n",
      "    val_loss       : -804838.4633559227\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -853149.000000\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -743922.625000\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -757868.000000\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -802309.687500\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -762649.750000\n",
      "    epoch          : 122\n",
      "    loss           : -803073.7159653465\n",
      "    val_loss       : -802954.5937135697\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -769247.625000\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -845403.375000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -803629.750000\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -838501.000000\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -768489.250000\n",
      "    epoch          : 123\n",
      "    loss           : -806004.4566831683\n",
      "    val_loss       : -806587.4930517196\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -959869.250000\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -763245.000000\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -794014.125000\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -757999.312500\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -796941.687500\n",
      "    epoch          : 124\n",
      "    loss           : -807518.1095297029\n",
      "    val_loss       : -806693.2082440376\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -959616.500000\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -803400.687500\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -763257.875000\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -762357.812500\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -807764.125000\n",
      "    epoch          : 125\n",
      "    loss           : -809319.4467821782\n",
      "    val_loss       : -807819.4247157096\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -859051.187500\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -750937.312500\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -734191.125000\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -765787.750000\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -807038.500000\n",
      "    epoch          : 126\n",
      "    loss           : -805401.1311881188\n",
      "    val_loss       : -807185.1883192062\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -958381.437500\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -801884.125000\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -802117.937500\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -800840.125000\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -808856.250000\n",
      "    epoch          : 127\n",
      "    loss           : -810519.1986386139\n",
      "    val_loss       : -811889.3840291977\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -774126.125000\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -774261.000000\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -807986.312500\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -843345.312500\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -799284.062500\n",
      "    epoch          : 128\n",
      "    loss           : -812559.6986386139\n",
      "    val_loss       : -812343.8385016441\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -967340.062500\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -858740.312500\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -810042.750000\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -842694.375000\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -799563.375000\n",
      "    epoch          : 129\n",
      "    loss           : -809450.3415841584\n",
      "    val_loss       : -808136.5393119812\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -963696.062500\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -776655.625000\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -755543.750000\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -728704.250000\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -758736.562500\n",
      "    epoch          : 130\n",
      "    loss           : -797470.8483910891\n",
      "    val_loss       : -800630.4547476768\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -964756.187500\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -768016.937500\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -832493.312500\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -794318.000000\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -767227.437500\n",
      "    epoch          : 131\n",
      "    loss           : -803176.875\n",
      "    val_loss       : -806215.2750811577\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -957111.875000\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -849111.187500\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -837425.875000\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -965704.750000\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -763332.687500\n",
      "    epoch          : 132\n",
      "    loss           : -811095.6404702971\n",
      "    val_loss       : -811435.9086979866\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -775987.000000\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -859633.937500\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -845083.625000\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -801251.750000\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -809667.812500\n",
      "    epoch          : 133\n",
      "    loss           : -812562.3087871287\n",
      "    val_loss       : -810303.9386248589\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -966837.562500\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -853631.125000\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -765269.937500\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -968232.312500\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -802166.937500\n",
      "    epoch          : 134\n",
      "    loss           : -811557.8700495049\n",
      "    val_loss       : -810897.6648019791\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -972514.625000\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -741009.125000\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -749304.312500\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -762958.062500\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -769172.875000\n",
      "    epoch          : 135\n",
      "    loss           : -814647.9102722772\n",
      "    val_loss       : -815516.1966535568\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -846773.875000\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -844313.625000\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -772953.187500\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -766299.750000\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -810049.500000\n",
      "    epoch          : 136\n",
      "    loss           : -811950.7017326732\n",
      "    val_loss       : -801267.2901247025\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -969628.687500\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -725958.000000\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -806535.000000\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -970954.750000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -800763.812500\n",
      "    epoch          : 137\n",
      "    loss           : -806936.9412128713\n",
      "    val_loss       : -805090.8140418052\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -854376.812500\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -770094.625000\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -762940.062500\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -763174.375000\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -803129.250000\n",
      "    epoch          : 138\n",
      "    loss           : -811312.4022277228\n",
      "    val_loss       : -811959.8506934165\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -777288.875000\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -856735.750000\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -765250.812500\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -750532.500000\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -809950.937500\n",
      "    epoch          : 139\n",
      "    loss           : -809689.6082920792\n",
      "    val_loss       : -811564.9017604828\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -953489.125000\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -845316.812500\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -747263.187500\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -770282.625000\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -804984.625000\n",
      "    epoch          : 140\n",
      "    loss           : -812995.343440594\n",
      "    val_loss       : -791844.2118762017\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -958898.125000\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -855903.875000\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -754079.625000\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -734980.750000\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -772904.625000\n",
      "    epoch          : 141\n",
      "    loss           : -806498.9034653465\n",
      "    val_loss       : -811108.8793493271\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -957416.687500\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -769354.187500\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -836110.625000\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -729461.437500\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -798726.750000\n",
      "    epoch          : 142\n",
      "    loss           : -807168.8972772277\n",
      "    val_loss       : -808085.1701627731\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -800904.937500\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -813755.687500\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -817509.812500\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -766966.125000\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -799614.187500\n",
      "    epoch          : 143\n",
      "    loss           : -814686.9009900991\n",
      "    val_loss       : -813901.5920080185\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -969620.312500\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -815972.687500\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -748421.625000\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -750212.812500\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -809655.250000\n",
      "    epoch          : 144\n",
      "    loss           : -818627.4486386139\n",
      "    val_loss       : -816914.0751595497\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -811295.562500\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -777305.000000\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -788473.812500\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -743289.125000\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -811666.250000\n",
      "    epoch          : 145\n",
      "    loss           : -801999.0594059406\n",
      "    val_loss       : -810838.6794604302\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -961994.812500\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -775845.500000\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -810395.625000\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -967153.875000\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -773957.875000\n",
      "    epoch          : 146\n",
      "    loss           : -814892.2524752475\n",
      "    val_loss       : -815810.3236815452\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -967555.500000\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -808627.312500\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -844751.125000\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -770224.375000\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -774573.250000\n",
      "    epoch          : 147\n",
      "    loss           : -818323.4449257426\n",
      "    val_loss       : -817040.221611023\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -971521.125000\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -867199.000000\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -765723.812500\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -971985.375000\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -817650.875000\n",
      "    epoch          : 148\n",
      "    loss           : -814557.8780940594\n",
      "    val_loss       : -814339.507943058\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -975266.875000\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -838973.375000\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -746818.250000\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -974424.250000\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -808041.562500\n",
      "    epoch          : 149\n",
      "    loss           : -817264.4851485149\n",
      "    val_loss       : -819066.963339901\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -974241.375000\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -776973.062500\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -809820.187500\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -771615.875000\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -814490.687500\n",
      "    epoch          : 150\n",
      "    loss           : -816707.6856435643\n",
      "    val_loss       : -819048.015782547\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/1109_111738/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -981310.250000\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -850179.062500\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -783602.375000\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -771261.687500\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -817945.125000\n",
      "    epoch          : 151\n",
      "    loss           : -819572.9028465346\n",
      "    val_loss       : -815899.9860142708\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -975509.562500\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -787069.000000\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -837291.062500\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -960768.625000\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -769534.750000\n",
      "    epoch          : 152\n",
      "    loss           : -817036.6033415842\n",
      "    val_loss       : -817931.2427321434\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -974849.625000\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -749286.000000\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -769585.000000\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -973416.750000\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -819205.562500\n",
      "    epoch          : 153\n",
      "    loss           : -819555.8446782178\n",
      "    val_loss       : -819543.314735508\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -973631.625000\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -780114.000000\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -847701.500000\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -769956.437500\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -814199.250000\n",
      "    epoch          : 154\n",
      "    loss           : -812118.7060643565\n",
      "    val_loss       : -814764.5447685241\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -969650.000000\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -743586.062500\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -778254.250000\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -815404.812500\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -801399.625000\n",
      "    epoch          : 155\n",
      "    loss           : -814272.0129950495\n",
      "    val_loss       : -818494.0047409534\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -973874.187500\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -864449.500000\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -794845.375000\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -832508.187500\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -772869.062500\n",
      "    epoch          : 156\n",
      "    loss           : -810582.8217821782\n",
      "    val_loss       : -813512.464880085\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -967505.875000\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -785196.750000\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -810998.937500\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -806185.062500\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -775684.500000\n",
      "    epoch          : 157\n",
      "    loss           : -820108.6392326732\n",
      "    val_loss       : -821459.0725611448\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -978959.000000\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -873696.562500\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -812878.000000\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -830779.000000\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -774752.750000\n",
      "    epoch          : 158\n",
      "    loss           : -818287.4207920792\n",
      "    val_loss       : -818033.1543860197\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -976162.062500\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -791620.500000\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -770089.375000\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -799844.062500\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -803414.375000\n",
      "    epoch          : 159\n",
      "    loss           : -818783.3372524752\n",
      "    val_loss       : -818456.032063818\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -973542.625000\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -749913.437500\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -811790.875000\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -767724.062500\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -784212.000000\n",
      "    epoch          : 160\n",
      "    loss           : -813818.2926980198\n",
      "    val_loss       : -812930.3950100422\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -969645.500000\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -778245.000000\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -777236.062500\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -776718.250000\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -774672.125000\n",
      "    epoch          : 161\n",
      "    loss           : -820436.8081683168\n",
      "    val_loss       : -817634.9593809128\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -973515.687500\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -786417.250000\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -769976.000000\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -812512.187500\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -813963.875000\n",
      "    epoch          : 162\n",
      "    loss           : -818510.0922029703\n",
      "    val_loss       : -811236.409546423\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -934060.562500\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -857568.250000\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -852486.750000\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -954874.250000\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -817645.437500\n",
      "    epoch          : 163\n",
      "    loss           : -819401.1967821782\n",
      "    val_loss       : -815138.3553558945\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -774923.750000\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -787909.437500\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -819059.500000\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -809114.875000\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -777484.500000\n",
      "    epoch          : 164\n",
      "    loss           : -819927.7264851485\n",
      "    val_loss       : -821647.9573032856\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -973456.812500\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -788641.187500\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -810642.625000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -975181.062500\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -814523.375000\n",
      "    epoch          : 165\n",
      "    loss           : -822223.8917079208\n",
      "    val_loss       : -823368.4199271202\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -973289.500000\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -784524.937500\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -776832.375000\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -819315.437500\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -814067.875000\n",
      "    epoch          : 166\n",
      "    loss           : -825214.5928217822\n",
      "    val_loss       : -818043.4887041568\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -971074.750000\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -854975.000000\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -749132.812500\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -976153.312500\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -815807.125000\n",
      "    epoch          : 167\n",
      "    loss           : -819083.7951732674\n",
      "    val_loss       : -819100.0924196243\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -977985.062500\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -786621.750000\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -847034.312500\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -822721.625000\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -818561.625000\n",
      "    epoch          : 168\n",
      "    loss           : -824026.8589108911\n",
      "    val_loss       : -821274.4867560386\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -979695.875000\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -783638.000000\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -817557.500000\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -844578.625000\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -819924.812500\n",
      "    epoch          : 169\n",
      "    loss           : -821527.1905940594\n",
      "    val_loss       : -821031.8453302383\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -956911.125000\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -860536.812500\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -755033.500000\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -975771.812500\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -842561.750000\n",
      "    epoch          : 170\n",
      "    loss           : -818661.4832920792\n",
      "    val_loss       : -819488.0586569787\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -980353.750000\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -787380.125000\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -850075.437500\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -735340.625000\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -820311.375000\n",
      "    epoch          : 171\n",
      "    loss           : -821531.917079208\n",
      "    val_loss       : -822555.4193567276\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -876059.812500\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -792470.250000\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -855090.500000\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -821052.250000\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -810997.187500\n",
      "    epoch          : 172\n",
      "    loss           : -826892.489480198\n",
      "    val_loss       : -826463.481489563\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -982482.500000\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -779331.500000\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -756399.625000\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -747101.500000\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -812349.000000\n",
      "    epoch          : 173\n",
      "    loss           : -826752.0705445545\n",
      "    val_loss       : -825386.2351179123\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -983224.375000\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -812840.125000\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -809538.500000\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -817007.187500\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -825283.750000\n",
      "    epoch          : 174\n",
      "    loss           : -826619.7524752475\n",
      "    val_loss       : -825729.6236760139\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -984650.250000\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -791418.562500\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -756361.375000\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -822113.750000\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -818754.375000\n",
      "    epoch          : 175\n",
      "    loss           : -829882.1132425743\n",
      "    val_loss       : -829415.9232160568\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -985708.375000\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -795049.812500\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -861480.562500\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -815538.000000\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -783707.375000\n",
      "    epoch          : 176\n",
      "    loss           : -830871.1027227723\n",
      "    val_loss       : -829387.3519233227\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -981952.375000\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -883138.750000\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -819091.250000\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -791194.937500\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -777053.000000\n",
      "    epoch          : 177\n",
      "    loss           : -827355.4777227723\n",
      "    val_loss       : -820155.3840829849\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -870804.625000\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -759856.187500\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -787689.437500\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -821152.437500\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -810800.500000\n",
      "    epoch          : 178\n",
      "    loss           : -825893.2178217822\n",
      "    val_loss       : -824487.0914795876\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -872416.000000\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -793627.375000\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -780169.000000\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -823963.812500\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -816329.750000\n",
      "    epoch          : 179\n",
      "    loss           : -827787.8564356435\n",
      "    val_loss       : -827324.088541317\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -984058.250000\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -796565.937500\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -760083.625000\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -820929.187500\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -787969.187500\n",
      "    epoch          : 180\n",
      "    loss           : -831190.4220297029\n",
      "    val_loss       : -831186.7986374379\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -884614.375000\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -772952.562500\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -736826.562500\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -744164.187500\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -816402.875000\n",
      "    epoch          : 181\n",
      "    loss           : -801357.7493811881\n",
      "    val_loss       : -816594.738967514\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -976635.375000\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -840555.375000\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -784296.750000\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -817757.250000\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -783800.937500\n",
      "    epoch          : 182\n",
      "    loss           : -818870.4832920792\n",
      "    val_loss       : -819913.6617399215\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -798284.687500\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -778508.062500\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -815941.875000\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -859759.062500\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -825967.125000\n",
      "    epoch          : 183\n",
      "    loss           : -824072.6806930694\n",
      "    val_loss       : -825055.8224918604\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -979412.625000\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -785918.312500\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -791078.062500\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -983805.062500\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -829340.687500\n",
      "    epoch          : 184\n",
      "    loss           : -830402.0897277228\n",
      "    val_loss       : -830626.049531746\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -983600.625000\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -794426.000000\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -860078.500000\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -981293.250000\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -827089.437500\n",
      "    epoch          : 185\n",
      "    loss           : -829028.25\n",
      "    val_loss       : -829852.7107802391\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -983642.500000\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -796650.625000\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -779796.750000\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -858336.812500\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -784288.125000\n",
      "    epoch          : 186\n",
      "    loss           : -827834.1070544554\n",
      "    val_loss       : -811034.4168313623\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -967626.875000\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -789170.375000\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -760462.375000\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -982768.187500\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -788860.125000\n",
      "    epoch          : 187\n",
      "    loss           : -825562.9950495049\n",
      "    val_loss       : -826890.7261795759\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -977806.375000\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -842718.437500\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -786219.500000\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -983396.187500\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -826973.500000\n",
      "    epoch          : 188\n",
      "    loss           : -827883.280940594\n",
      "    val_loss       : -830117.1943016291\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -983160.437500\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -879538.437500\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -855315.750000\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -984976.125000\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -815463.437500\n",
      "    epoch          : 189\n",
      "    loss           : -831742.6757425743\n",
      "    val_loss       : -831330.4860862732\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -979098.125000\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -818116.500000\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -770602.875000\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -863155.625000\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -827761.812500\n",
      "    epoch          : 190\n",
      "    loss           : -834270.6509900991\n",
      "    val_loss       : -829205.8877192021\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -981028.875000\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -801508.625000\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -763863.937500\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -826741.875000\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -792592.500000\n",
      "    epoch          : 191\n",
      "    loss           : -834490.0674504951\n",
      "    val_loss       : -833135.7027478814\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -984600.625000\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -820781.812500\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -817354.687500\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -824049.375000\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -776196.875000\n",
      "    epoch          : 192\n",
      "    loss           : -827368.1794554455\n",
      "    val_loss       : -816905.2225550174\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -970783.250000\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -827473.062500\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -859725.500000\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -764477.812500\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -789234.625000\n",
      "    epoch          : 193\n",
      "    loss           : -825273.1392326732\n",
      "    val_loss       : -829151.388329649\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -983766.125000\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -803032.812500\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -729301.562500\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -973795.625000\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -777877.875000\n",
      "    epoch          : 194\n",
      "    loss           : -820464.8527227723\n",
      "    val_loss       : -818049.8544270754\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -981021.500000\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -760966.250000\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -858985.750000\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -967859.500000\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -823340.125000\n",
      "    epoch          : 195\n",
      "    loss           : -824085.7060643565\n",
      "    val_loss       : -827214.9155426503\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -985850.687500\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -793164.187500\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -768331.062500\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -988827.312500\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -823873.625000\n",
      "    epoch          : 196\n",
      "    loss           : -833790.9115099009\n",
      "    val_loss       : -833109.0617961406\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -986188.250000\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -883744.250000\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -827120.500000\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -841290.312500\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -787715.000000\n",
      "    epoch          : 197\n",
      "    loss           : -836591.7648514851\n",
      "    val_loss       : -821607.0578527689\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -972405.687500\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -796578.500000\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -757318.125000\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -793113.937500\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -820120.437500\n",
      "    epoch          : 198\n",
      "    loss           : -828164.5711633663\n",
      "    val_loss       : -834262.1152921915\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -986801.687500\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -879119.875000\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -819948.375000\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -987080.000000\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -789709.437500\n",
      "    epoch          : 199\n",
      "    loss           : -836583.6503712871\n",
      "    val_loss       : -831759.4588831663\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -986839.250000\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -872729.875000\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -791939.625000\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -799305.625000\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -819618.437500\n",
      "    epoch          : 200\n",
      "    loss           : -833510.6528465346\n",
      "    val_loss       : -835979.7750397443\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/1109_111738/checkpoint-epoch200.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -886928.937500\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -803281.500000\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -771414.687500\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -824383.125000\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -792612.375000\n",
      "    epoch          : 201\n",
      "    loss           : -836424.1014851485\n",
      "    val_loss       : -832124.2961323023\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -985877.125000\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -800329.062500\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -866438.937500\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -803816.750000\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -822955.000000\n",
      "    epoch          : 202\n",
      "    loss           : -838864.9919554455\n",
      "    val_loss       : -837327.7208357335\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -986474.750000\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -884532.375000\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -797156.875000\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -944948.250000\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -820685.375000\n",
      "    epoch          : 203\n",
      "    loss           : -833254.792079208\n",
      "    val_loss       : -832605.3788969994\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -965508.937500\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -886944.000000\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -817169.000000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -826725.000000\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -826038.437500\n",
      "    epoch          : 204\n",
      "    loss           : -836586.5705445545\n",
      "    val_loss       : -814963.8282705307\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -963135.000000\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -795881.187500\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -782472.812500\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -760166.437500\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -827499.250000\n",
      "    epoch          : 205\n",
      "    loss           : -831585.9665841584\n",
      "    val_loss       : -837501.1330004692\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -986802.062500\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -794891.875000\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -783682.687500\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -790858.375000\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -822426.125000\n",
      "    epoch          : 206\n",
      "    loss           : -833077.1522277228\n",
      "    val_loss       : -835829.8234748602\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -982426.187500\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -762651.500000\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -862126.250000\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -870676.125000\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -804803.125000\n",
      "    epoch          : 207\n",
      "    loss           : -839265.6497524752\n",
      "    val_loss       : -838638.625216794\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -986788.687500\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -806016.250000\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -858641.750000\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -873655.687500\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -796899.375000\n",
      "    epoch          : 208\n",
      "    loss           : -840479.9133663366\n",
      "    val_loss       : -839516.8246802449\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -988696.437500\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -776911.250000\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -827166.437500\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -792010.687500\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -837000.500000\n",
      "    epoch          : 209\n",
      "    loss           : -839890.7679455446\n",
      "    val_loss       : -834486.521546483\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -984061.437500\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -798055.875000\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -802873.812500\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -769144.250000\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -806220.625000\n",
      "    epoch          : 210\n",
      "    loss           : -833217.4220297029\n",
      "    val_loss       : -828927.1087324142\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -977374.625000\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -889259.375000\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -772775.625000\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -986453.312500\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -791320.750000\n",
      "    epoch          : 211\n",
      "    loss           : -837152.0012376237\n",
      "    val_loss       : -836417.6866609573\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -983581.187500\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -889229.500000\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -769686.437500\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -788254.000000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -825316.125000\n",
      "    epoch          : 212\n",
      "    loss           : -839561.7784653465\n",
      "    val_loss       : -834673.8503868103\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -982171.562500\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -773836.687500\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -839059.500000\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -836828.875000\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -870848.437500\n",
      "    epoch          : 213\n",
      "    loss           : -840795.4900990099\n",
      "    val_loss       : -834067.3339424372\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -986942.500000\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -805088.875000\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -811747.000000\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -991798.437500\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -827989.312500\n",
      "    epoch          : 214\n",
      "    loss           : -836408.25\n",
      "    val_loss       : -828369.8122512341\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -792336.625000\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -792668.687500\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -867468.562500\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -822999.000000\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -838610.375000\n",
      "    epoch          : 215\n",
      "    loss           : -836324.5\n",
      "    val_loss       : -839757.3698759079\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -988679.000000\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -806422.062500\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -803328.562500\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -797896.312500\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -872046.500000\n",
      "    epoch          : 216\n",
      "    loss           : -836034.3836633663\n",
      "    val_loss       : -835047.516156602\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -986886.375000\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -885407.937500\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -774243.875000\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -986858.937500\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -832502.500000\n",
      "    epoch          : 217\n",
      "    loss           : -841485.4201732674\n",
      "    val_loss       : -841584.8558072805\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -987327.875000\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -800631.500000\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -778666.875000\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -839723.750000\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -821873.375000\n",
      "    epoch          : 218\n",
      "    loss           : -839014.7827970297\n",
      "    val_loss       : -837414.4335964203\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -780085.750000\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -775063.250000\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -785416.000000\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -778484.125000\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -823933.562500\n",
      "    epoch          : 219\n",
      "    loss           : -828599.7091584158\n",
      "    val_loss       : -827718.9847419501\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -958237.750000\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -767556.750000\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -867742.562500\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -719200.562500\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -816298.375000\n",
      "    epoch          : 220\n",
      "    loss           : -825103.2735148515\n",
      "    val_loss       : -823063.2409653664\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -965669.000000\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -883472.625000\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -790132.250000\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -820988.250000\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -734625.500000\n",
      "    epoch          : 221\n",
      "    loss           : -823639.4189356435\n",
      "    val_loss       : -804116.7147004127\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -773004.625000\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -757154.625000\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -762964.062500\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -788487.937500\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -771786.500000\n",
      "    epoch          : 222\n",
      "    loss           : -824523.9832920792\n",
      "    val_loss       : -832405.986618638\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -979759.312500\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -804861.125000\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -866381.687500\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -797774.312500\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -792733.125000\n",
      "    epoch          : 223\n",
      "    loss           : -838615.8626237623\n",
      "    val_loss       : -836682.1882547379\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -980698.125000\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -806030.000000\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -834596.125000\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -986219.875000\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -828513.187500\n",
      "    epoch          : 224\n",
      "    loss           : -838946.7821782178\n",
      "    val_loss       : -841267.3553237915\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -806863.562500\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -804491.250000\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -771981.312500\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -793294.125000\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -838774.625000\n",
      "    epoch          : 225\n",
      "    loss           : -838317.8923267326\n",
      "    val_loss       : -834116.267905426\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -984662.500000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -867292.250000\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -794129.125000\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -796670.562500\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -827370.250000\n",
      "    epoch          : 226\n",
      "    loss           : -834579.8025990099\n",
      "    val_loss       : -825988.4428462982\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -976018.875000\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -799417.625000\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -824677.250000\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -774776.125000\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -828129.187500\n",
      "    epoch          : 227\n",
      "    loss           : -837081.3681930694\n",
      "    val_loss       : -830992.0923271179\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -879376.062500\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -804242.437500\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -830026.375000\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -806979.437500\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -796587.437500\n",
      "    epoch          : 228\n",
      "    loss           : -842135.292079208\n",
      "    val_loss       : -843317.4920015335\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -991440.250000\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -810485.125000\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -874329.687500\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -806037.687500\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -839099.937500\n",
      "    epoch          : 229\n",
      "    loss           : -844289.9430693069\n",
      "    val_loss       : -841226.4659038543\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -893114.312500\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -882634.000000\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -805212.562500\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -838287.375000\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -833764.250000\n",
      "    epoch          : 230\n",
      "    loss           : -844267.9486386139\n",
      "    val_loss       : -840221.1080443382\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -987774.125000\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -809249.625000\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -868885.500000\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -794255.500000\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -800727.750000\n",
      "    epoch          : 231\n",
      "    loss           : -838244.0532178218\n",
      "    val_loss       : -818896.1962431908\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -969705.375000\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -870712.937500\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -719554.562500\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -788147.125000\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -818704.625000\n",
      "    epoch          : 232\n",
      "    loss           : -826914.9090346535\n",
      "    val_loss       : -835350.6880912781\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -984737.937500\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -776514.250000\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -831023.000000\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -832656.500000\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -794433.625000\n",
      "    epoch          : 233\n",
      "    loss           : -840359.270420792\n",
      "    val_loss       : -839154.5591505527\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -986471.562500\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -803920.500000\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -777204.625000\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -806619.250000\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -820042.625000\n",
      "    epoch          : 234\n",
      "    loss           : -841259.7642326732\n",
      "    val_loss       : -834350.7327127457\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -984303.625000\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -809628.687500\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -781052.750000\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -972450.000000\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -819825.000000\n",
      "    epoch          : 235\n",
      "    loss           : -826395.5891089109\n",
      "    val_loss       : -832877.3088538169\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -983519.687500\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -807603.875000\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -765911.000000\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -983704.187500\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -794657.625000\n",
      "    epoch          : 236\n",
      "    loss           : -837870.4851485149\n",
      "    val_loss       : -839122.4515155792\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -991482.625000\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -835725.625000\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -833550.375000\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -837333.375000\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -835835.250000\n",
      "    epoch          : 237\n",
      "    loss           : -842698.5185643565\n",
      "    val_loss       : -842573.3852992058\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -989451.937500\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -778007.187500\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -857243.250000\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -826156.000000\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -775414.625000\n",
      "    epoch          : 238\n",
      "    loss           : -835677.832920792\n",
      "    val_loss       : -838626.0003392219\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -989530.750000\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -801104.125000\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -812881.125000\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -987901.375000\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -787938.875000\n",
      "    epoch          : 239\n",
      "    loss           : -844466.6689356435\n",
      "    val_loss       : -838087.4410665513\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -984154.562500\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -803229.437500\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -866825.750000\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -796354.500000\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -788868.625000\n",
      "    epoch          : 240\n",
      "    loss           : -839873.9028465346\n",
      "    val_loss       : -835225.7268146991\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -986135.812500\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -806498.500000\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -812348.500000\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -873122.062500\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -875443.812500\n",
      "    epoch          : 241\n",
      "    loss           : -844119.4003712871\n",
      "    val_loss       : -846434.1188363075\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -993347.812500\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -802282.062500\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -816170.375000\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -873556.250000\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -832407.500000\n",
      "    epoch          : 242\n",
      "    loss           : -845821.7475247525\n",
      "    val_loss       : -846815.4849886894\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -893847.750000\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -819720.437500\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -826403.375000\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -812875.812500\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -829265.187500\n",
      "    epoch          : 243\n",
      "    loss           : -845392.3261138614\n",
      "    val_loss       : -842663.6333526612\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -987835.250000\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -820512.687500\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -822350.937500\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -874176.687500\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -797215.937500\n",
      "    epoch          : 244\n",
      "    loss           : -846189.8985148515\n",
      "    val_loss       : -848105.983010292\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -992169.500000\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -816722.125000\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -787963.375000\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -994064.437500\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -833071.062500\n",
      "    epoch          : 245\n",
      "    loss           : -848722.0866336634\n",
      "    val_loss       : -839154.529549408\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -987128.812500\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -883152.375000\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -838146.187500\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -838040.000000\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -842464.687500\n",
      "    epoch          : 246\n",
      "    loss           : -842820.0290841584\n",
      "    val_loss       : -847382.1662722587\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -896485.312500\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -814521.125000\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -820941.750000\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -711302.375000\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -811606.000000\n",
      "    epoch          : 247\n",
      "    loss           : -835909.4461633663\n",
      "    val_loss       : -825897.0798676491\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -872401.000000\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -860174.125000\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -830644.375000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -984389.187500\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -822796.812500\n",
      "    epoch          : 248\n",
      "    loss           : -836748.6676980198\n",
      "    val_loss       : -836987.822187233\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -988450.062500\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -892531.437500\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -770082.500000\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -803040.250000\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -833770.687500\n",
      "    epoch          : 249\n",
      "    loss           : -845102.7202970297\n",
      "    val_loss       : -841135.8143164634\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -815065.000000\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -814148.500000\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -775722.187500\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -986446.687500\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -828294.187500\n",
      "    epoch          : 250\n",
      "    loss           : -844757.2103960396\n",
      "    val_loss       : -832624.3653393745\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/1109_111738/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -987396.250000\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -880908.750000\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -830974.375000\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -814275.875000\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -835480.062500\n",
      "    epoch          : 251\n",
      "    loss           : -839675.5501237623\n",
      "    val_loss       : -840226.7789351463\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -988728.375000\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -765212.375000\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -802657.687500\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -809101.312500\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -802148.250000\n",
      "    epoch          : 252\n",
      "    loss           : -839808.7456683168\n",
      "    val_loss       : -843588.626992035\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -990175.062500\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -898682.437500\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -810846.250000\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -781992.375000\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -837064.937500\n",
      "    epoch          : 253\n",
      "    loss           : -840688.042079208\n",
      "    val_loss       : -843767.0932863236\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -980392.687500\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -806512.625000\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -843992.625000\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -872283.000000\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -830488.937500\n",
      "    epoch          : 254\n",
      "    loss           : -846048.291460396\n",
      "    val_loss       : -845976.2197971344\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -988667.937500\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -900541.750000\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -819802.875000\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -992957.562500\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -845801.937500\n",
      "    epoch          : 255\n",
      "    loss           : -850005.1373762377\n",
      "    val_loss       : -847934.9546190262\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -990747.937500\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -849524.750000\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -781475.125000\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -849139.375000\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -836921.125000\n",
      "    epoch          : 256\n",
      "    loss           : -851152.0897277228\n",
      "    val_loss       : -849230.3729618073\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -994434.000000\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -900653.250000\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -822870.562500\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -852659.312500\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -846096.562500\n",
      "    epoch          : 257\n",
      "    loss           : -851948.7518564357\n",
      "    val_loss       : -849498.8796626091\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -994491.125000\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -821319.125000\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -802263.875000\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -798605.625000\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -835466.187500\n",
      "    epoch          : 258\n",
      "    loss           : -850813.8261138614\n",
      "    val_loss       : -849419.3775315285\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -993681.437500\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -901914.687500\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -824897.625000\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -824862.625000\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -851533.187500\n",
      "    epoch          : 259\n",
      "    loss           : -852043.0136138614\n",
      "    val_loss       : -851233.9940114975\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -997273.187500\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -890703.750000\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -851411.687500\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -991541.000000\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -851570.750000\n",
      "    epoch          : 260\n",
      "    loss           : -851991.0643564357\n",
      "    val_loss       : -846852.9873103142\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -978159.312500\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -899243.250000\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -826653.562500\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -840805.000000\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -806023.000000\n",
      "    epoch          : 261\n",
      "    loss           : -852356.7716584158\n",
      "    val_loss       : -852643.3710229874\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -997720.937500\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -844074.000000\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -834376.812500\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -837044.312500\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -839201.750000\n",
      "    epoch          : 262\n",
      "    loss           : -851318.4740099009\n",
      "    val_loss       : -849563.6191058159\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -994103.937500\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -801863.750000\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -801011.375000\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -824806.375000\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -841144.125000\n",
      "    epoch          : 263\n",
      "    loss           : -850258.0668316832\n",
      "    val_loss       : -849406.4058093071\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -810332.625000\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -869767.937500\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -772166.625000\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -814152.750000\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -826278.125000\n",
      "    epoch          : 264\n",
      "    loss           : -839567.5216584158\n",
      "    val_loss       : -845324.4929310798\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -990392.750000\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -818662.625000\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -823311.000000\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -825845.875000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -801391.125000\n",
      "    epoch          : 265\n",
      "    loss           : -850910.2029702971\n",
      "    val_loss       : -851593.5646847725\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -993827.937500\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -825151.000000\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -876709.250000\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -799813.125000\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -800992.625000\n",
      "    epoch          : 266\n",
      "    loss           : -852281.8143564357\n",
      "    val_loss       : -850194.4910676002\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -993308.250000\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -825018.187500\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -879266.187500\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -836846.875000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -838342.500000\n",
      "    epoch          : 267\n",
      "    loss           : -853703.3700495049\n",
      "    val_loss       : -852932.745549965\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -995621.562500\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -819165.687500\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -798807.312500\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -832535.375000\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -830229.875000\n",
      "    epoch          : 268\n",
      "    loss           : -854277.7716584158\n",
      "    val_loss       : -852639.1460652351\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -995295.625000\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -807145.125000\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -836184.062500\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -878264.000000\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -802868.750000\n",
      "    epoch          : 269\n",
      "    loss           : -849929.5754950495\n",
      "    val_loss       : -848583.0523208618\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -986150.812500\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -793563.500000\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -744574.937500\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -820881.625000\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -827245.375000\n",
      "    epoch          : 270\n",
      "    loss           : -835961.0470297029\n",
      "    val_loss       : -825182.1378942489\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -973038.625000\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -763161.750000\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -806671.562500\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -784460.750000\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -830047.125000\n",
      "    epoch          : 271\n",
      "    loss           : -837776.3397277228\n",
      "    val_loss       : -844636.2811136246\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -984634.250000\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -810803.437500\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -782330.062500\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -882767.812500\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -798406.000000\n",
      "    epoch          : 272\n",
      "    loss           : -846952.875\n",
      "    val_loss       : -844841.247025776\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -969674.187500\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -898443.375000\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -870226.687500\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -770081.312500\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -835812.250000\n",
      "    epoch          : 273\n",
      "    loss           : -844648.3886138614\n",
      "    val_loss       : -841422.1342669487\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -984690.875000\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -844500.437500\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -776506.687500\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -846279.250000\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -801866.250000\n",
      "    epoch          : 274\n",
      "    loss           : -848678.2803217822\n",
      "    val_loss       : -851051.4901956558\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -786042.062500\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -855080.125000\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -863661.437500\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -792926.625000\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -823942.687500\n",
      "    epoch          : 275\n",
      "    loss           : -844202.7506188119\n",
      "    val_loss       : -848992.4258704185\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -993292.000000\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -814432.062500\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -826977.625000\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -808870.062500\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -801889.625000\n",
      "    epoch          : 276\n",
      "    loss           : -853023.1676980198\n",
      "    val_loss       : -852057.2201082229\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -994003.000000\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -816859.375000\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -845231.312500\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -826513.250000\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -833306.625000\n",
      "    epoch          : 277\n",
      "    loss           : -851328.3304455446\n",
      "    val_loss       : -851617.7411036491\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -989454.000000\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -817495.500000\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -803931.062500\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -830420.250000\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -853718.437500\n",
      "    epoch          : 278\n",
      "    loss           : -853035.3694306931\n",
      "    val_loss       : -852586.8042121887\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -996749.187500\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -817685.500000\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -784493.625000\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -816338.687500\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -841111.375000\n",
      "    epoch          : 279\n",
      "    loss           : -850794.3811881188\n",
      "    val_loss       : -852904.3216993331\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -995623.000000\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -822353.125000\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -826962.937500\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -852881.000000\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -855575.750000\n",
      "    epoch          : 280\n",
      "    loss           : -853471.7957920792\n",
      "    val_loss       : -854312.5151474953\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -996434.750000\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -820226.375000\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -830095.750000\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -880276.625000\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -805803.187500\n",
      "    epoch          : 281\n",
      "    loss           : -856490.1342821782\n",
      "    val_loss       : -854713.3210400582\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -998589.250000\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -822176.500000\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -802224.875000\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -805811.625000\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -805019.625000\n",
      "    epoch          : 282\n",
      "    loss           : -856083.6893564357\n",
      "    val_loss       : -854224.3012649536\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -998987.562500\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -824029.937500\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -877728.875000\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -839640.687500\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -831587.625000\n",
      "    epoch          : 283\n",
      "    loss           : -855883.6615099009\n",
      "    val_loss       : -855244.4815048218\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -992079.375000\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -827156.500000\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -883754.875000\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -992484.000000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -807905.875000\n",
      "    epoch          : 284\n",
      "    loss           : -854200.2345297029\n",
      "    val_loss       : -854919.508638382\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -904069.562500\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -994056.812500\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -780658.625000\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -824915.375000\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -802523.937500\n",
      "    epoch          : 285\n",
      "    loss           : -852988.6163366337\n",
      "    val_loss       : -853217.3935575485\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -902585.750000\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -788751.312500\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -885203.750000\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -843589.750000\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -833252.125000\n",
      "    epoch          : 286\n",
      "    loss           : -850967.5061881188\n",
      "    val_loss       : -840726.1122816086\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -974515.750000\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -798756.125000\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -799493.625000\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -834214.437500\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -871964.187500\n",
      "    epoch          : 287\n",
      "    loss           : -850389.6293316832\n",
      "    val_loss       : -852635.4604968071\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -902830.312500\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -815308.750000\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -824176.812500\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -870975.187500\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -825854.937500\n",
      "    epoch          : 288\n",
      "    loss           : -846270.2128712871\n",
      "    val_loss       : -848369.8685562133\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -895436.187500\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -822132.500000\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -841305.625000\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -852393.000000\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -826065.250000\n",
      "    epoch          : 289\n",
      "    loss           : -849571.0259900991\n",
      "    val_loss       : -838741.6360165596\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -948610.750000\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -894151.437500\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -829940.062500\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -872909.125000\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -809052.000000\n",
      "    epoch          : 290\n",
      "    loss           : -846537.4443069306\n",
      "    val_loss       : -851608.4466554641\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -979649.812500\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -811584.500000\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -849310.812500\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -856987.250000\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -855235.312500\n",
      "    epoch          : 291\n",
      "    loss           : -855184.728960396\n",
      "    val_loss       : -847111.7112744332\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -986548.375000\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -819959.125000\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -830893.625000\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -871520.625000\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -804205.312500\n",
      "    epoch          : 292\n",
      "    loss           : -853587.6509900991\n",
      "    val_loss       : -854753.2349068641\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -989001.875000\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -867382.687500\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -838168.625000\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -841674.625000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -857204.000000\n",
      "    epoch          : 293\n",
      "    loss           : -852122.156559406\n",
      "    val_loss       : -845771.361598301\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -989560.000000\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -788499.000000\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -790273.375000\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -803016.812500\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -803848.812500\n",
      "    epoch          : 294\n",
      "    loss           : -853973.8384900991\n",
      "    val_loss       : -854013.2184449196\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -992286.000000\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -798688.312500\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -790614.625000\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -876024.875000\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -806401.062500\n",
      "    epoch          : 295\n",
      "    loss           : -850147.9424504951\n",
      "    val_loss       : -853356.7814322471\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -988069.187500\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -881590.000000\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -791535.187500\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -854926.375000\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -825820.687500\n",
      "    epoch          : 296\n",
      "    loss           : -852368.8849009901\n",
      "    val_loss       : -851203.6924102784\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -988886.750000\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -830309.250000\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -837869.312500\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -804472.875000\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -797338.750000\n",
      "    epoch          : 297\n",
      "    loss           : -847496.6509900991\n",
      "    val_loss       : -844786.7735793113\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -973531.250000\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -782448.625000\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -829426.062500\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -873310.250000\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -840767.437500\n",
      "    epoch          : 298\n",
      "    loss           : -848502.4826732674\n",
      "    val_loss       : -839845.590087986\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -985800.062500\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -780287.812500\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -876619.937500\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -828073.687500\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -819118.000000\n",
      "    epoch          : 299\n",
      "    loss           : -847006.0012376237\n",
      "    val_loss       : -848415.5246793746\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -985656.812500\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -852633.625000\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -849455.500000\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -777501.875000\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -842997.187500\n",
      "    epoch          : 300\n",
      "    loss           : -850257.7376237623\n",
      "    val_loss       : -852851.9079898834\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/1109_111738/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -817038.125000\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -910507.250000\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -839281.000000\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -982285.125000\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -824039.750000\n",
      "    epoch          : 301\n",
      "    loss           : -851402.8452970297\n",
      "    val_loss       : -845726.3347507477\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -986250.187500\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -817513.375000\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -840928.562500\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -781276.062500\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -792218.062500\n",
      "    epoch          : 302\n",
      "    loss           : -851185.9306930694\n",
      "    val_loss       : -852939.211452961\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -986988.125000\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -823797.875000\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -842108.750000\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -992338.562500\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -856651.187500\n",
      "    epoch          : 303\n",
      "    loss           : -857206.9115099009\n",
      "    val_loss       : -840774.0212347985\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -895132.625000\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -811188.375000\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -856954.625000\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -880121.750000\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -878881.562500\n",
      "    epoch          : 304\n",
      "    loss           : -849134.0074257426\n",
      "    val_loss       : -855313.1885176658\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -995156.812500\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -825627.687500\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -797522.187500\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -848488.312500\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -859278.687500\n",
      "    epoch          : 305\n",
      "    loss           : -853533.916460396\n",
      "    val_loss       : -856159.8560871125\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -997482.937500\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -819895.062500\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -830823.875000\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -994046.375000\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -877435.812500\n",
      "    epoch          : 306\n",
      "    loss           : -856580.5903465346\n",
      "    val_loss       : -849847.1449438095\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -991161.625000\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -806758.625000\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -805919.375000\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -879911.312500\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -856012.812500\n",
      "    epoch          : 307\n",
      "    loss           : -856400.6590346535\n",
      "    val_loss       : -851955.2900069237\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -988780.687500\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -837089.125000\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -875229.812500\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -836990.812500\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -842749.250000\n",
      "    epoch          : 308\n",
      "    loss           : -854787.6082920792\n",
      "    val_loss       : -847481.9391289711\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -984806.687500\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -820473.687500\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -844172.687500\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -880654.500000\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -851909.312500\n",
      "    epoch          : 309\n",
      "    loss           : -855703.6850247525\n",
      "    val_loss       : -854376.3769432068\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -985389.812500\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -826883.187500\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -843160.875000\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -806481.625000\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -840538.875000\n",
      "    epoch          : 310\n",
      "    loss           : -857538.1305693069\n",
      "    val_loss       : -855786.7121920586\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -992225.000000\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -823188.000000\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -854414.250000\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -835798.812500\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -845255.250000\n",
      "    epoch          : 311\n",
      "    loss           : -858171.2363861386\n",
      "    val_loss       : -858274.1027467728\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -1001820.375000\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -912808.000000\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -846237.437500\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -860883.625000\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -873516.500000\n",
      "    epoch          : 312\n",
      "    loss           : -857101.6856435643\n",
      "    val_loss       : -851637.750150299\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -996671.125000\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -857127.125000\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -830297.562500\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -800604.500000\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -808447.937500\n",
      "    epoch          : 313\n",
      "    loss           : -856400.4034653465\n",
      "    val_loss       : -851402.0856768608\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -990652.625000\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -821484.937500\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -791618.250000\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -843306.125000\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -807750.125000\n",
      "    epoch          : 314\n",
      "    loss           : -857475.4981435643\n",
      "    val_loss       : -856476.6879818917\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -833977.375000\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -822819.250000\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -836187.375000\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -845304.125000\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -830375.000000\n",
      "    epoch          : 315\n",
      "    loss           : -859519.6287128713\n",
      "    val_loss       : -857808.5645669937\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -997764.125000\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -828077.062500\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -794195.875000\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -839410.875000\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -842769.937500\n",
      "    epoch          : 316\n",
      "    loss           : -861923.7339108911\n",
      "    val_loss       : -860307.5691301345\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -1001193.312500\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -801592.250000\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -864237.062500\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -846042.375000\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -845703.500000\n",
      "    epoch          : 317\n",
      "    loss           : -848436.7970297029\n",
      "    val_loss       : -851590.1277486801\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -905343.000000\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -902616.125000\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -882943.937500\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -845760.562500\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -882652.125000\n",
      "    epoch          : 318\n",
      "    loss           : -856497.3094059406\n",
      "    val_loss       : -855116.3121776581\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -995286.375000\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -818985.750000\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -889267.625000\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -829313.875000\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -831061.125000\n",
      "    epoch          : 319\n",
      "    loss           : -854170.8112623763\n",
      "    val_loss       : -853700.9851322174\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -993649.125000\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -824596.687500\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -780440.812500\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -992220.437500\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -803933.375000\n",
      "    epoch          : 320\n",
      "    loss           : -857854.1175742574\n",
      "    val_loss       : -849459.2866937637\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -980895.375000\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -822026.312500\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -881848.375000\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -832686.062500\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -845382.062500\n",
      "    epoch          : 321\n",
      "    loss           : -856743.5847772277\n",
      "    val_loss       : -854829.4499148369\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -987453.375000\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -900212.000000\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -832128.750000\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -879820.562500\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -845443.875000\n",
      "    epoch          : 322\n",
      "    loss           : -856005.7271039604\n",
      "    val_loss       : -857990.7821155548\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -993421.875000\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -825634.125000\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -798522.812500\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -810908.000000\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -814310.875000\n",
      "    epoch          : 323\n",
      "    loss           : -860432.0575495049\n",
      "    val_loss       : -860847.7341026306\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -908143.000000\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -823883.437500\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -869004.875000\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -985790.250000\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -810971.312500\n",
      "    epoch          : 324\n",
      "    loss           : -857091.3483910891\n",
      "    val_loss       : -855298.8410413743\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -878967.562500\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -795579.562500\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -850615.437500\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -833434.312500\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -862228.125000\n",
      "    epoch          : 325\n",
      "    loss           : -859648.2091584158\n",
      "    val_loss       : -857317.1162994385\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -905273.312500\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -809931.625000\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -845096.812500\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -872949.875000\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -808666.187500\n",
      "    epoch          : 326\n",
      "    loss           : -854257.4863861386\n",
      "    val_loss       : -857442.7167590142\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -995109.062500\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -796673.687500\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -805212.250000\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -812288.750000\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -843779.125000\n",
      "    epoch          : 327\n",
      "    loss           : -858725.719059406\n",
      "    val_loss       : -859234.4149510383\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -995679.187500\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -826910.000000\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -849837.000000\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -996779.562500\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -808208.937500\n",
      "    epoch          : 328\n",
      "    loss           : -862451.5445544554\n",
      "    val_loss       : -862059.262263298\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -998054.562500\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -829490.625000\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -841420.250000\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -998530.562500\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -841877.750000\n",
      "    epoch          : 329\n",
      "    loss           : -861284.8254950495\n",
      "    val_loss       : -854323.6088300705\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -989366.687500\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -820552.625000\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -884454.500000\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -982537.125000\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -845577.500000\n",
      "    epoch          : 330\n",
      "    loss           : -855784.8347772277\n",
      "    val_loss       : -854308.6437399865\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -822875.562500\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -819742.625000\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -834814.625000\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -971089.500000\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -816017.375000\n",
      "    epoch          : 331\n",
      "    loss           : -856262.9418316832\n",
      "    val_loss       : -858348.4795386314\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -903267.625000\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -828605.812500\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -815256.250000\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -801446.125000\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -860758.625000\n",
      "    epoch          : 332\n",
      "    loss           : -857383.8341584158\n",
      "    val_loss       : -855125.1391130447\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -996289.125000\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -789313.500000\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -847527.875000\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -883023.812500\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -815262.250000\n",
      "    epoch          : 333\n",
      "    loss           : -858772.0581683168\n",
      "    val_loss       : -861085.4545678139\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -908222.125000\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -799330.375000\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -797896.750000\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -837316.312500\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -851010.562500\n",
      "    epoch          : 334\n",
      "    loss           : -862682.3050742574\n",
      "    val_loss       : -854432.3818950653\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -991475.937500\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -912312.687500\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -797343.625000\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -792713.187500\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -853582.687500\n",
      "    epoch          : 335\n",
      "    loss           : -857818.6800742574\n",
      "    val_loss       : -856661.9408185005\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -996725.187500\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -832713.000000\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -834893.250000\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -998952.625000\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -808132.250000\n",
      "    epoch          : 336\n",
      "    loss           : -861362.6757425743\n",
      "    val_loss       : -858294.2255977631\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -993150.937500\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -848680.125000\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -887874.062500\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -995864.125000\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -857316.250000\n",
      "    epoch          : 337\n",
      "    loss           : -861104.0160891089\n",
      "    val_loss       : -858105.2336608886\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -997857.875000\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -831008.000000\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -815906.312500\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -815089.687500\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -859420.062500\n",
      "    epoch          : 338\n",
      "    loss           : -861876.5012376237\n",
      "    val_loss       : -857869.5723477363\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -992756.687500\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -828074.125000\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -880114.875000\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -796920.250000\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -779825.437500\n",
      "    epoch          : 339\n",
      "    loss           : -850438.9102722772\n",
      "    val_loss       : -815888.1288477897\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -992204.250000\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -808330.750000\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -792087.937500\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -842541.250000\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -836932.375000\n",
      "    epoch          : 340\n",
      "    loss           : -838476.7110148515\n",
      "    val_loss       : -852385.6337286949\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -890629.062500\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -841938.937500\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -858435.125000\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -981030.000000\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -801365.062500\n",
      "    epoch          : 341\n",
      "    loss           : -853221.3886138614\n",
      "    val_loss       : -850477.0405513763\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -891532.500000\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -824502.000000\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -881227.500000\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -856395.937500\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -851248.437500\n",
      "    epoch          : 342\n",
      "    loss           : -858671.9603960396\n",
      "    val_loss       : -859671.1102871895\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -997344.812500\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -827111.500000\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -805165.750000\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -984066.000000\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -790192.750000\n",
      "    epoch          : 343\n",
      "    loss           : -854683.8787128713\n",
      "    val_loss       : -845833.0730042458\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -986607.000000\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -857030.250000\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -839895.000000\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -790626.625000\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -849971.625000\n",
      "    epoch          : 344\n",
      "    loss           : -857376.5513613861\n",
      "    val_loss       : -858452.735390091\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -992621.875000\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -836338.500000\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -785431.187500\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -995954.812500\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -812900.125000\n",
      "    epoch          : 345\n",
      "    loss           : -859568.0173267326\n",
      "    val_loss       : -858604.2976021767\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -996436.687500\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -841769.937500\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -883669.937500\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -843305.000000\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -847708.375000\n",
      "    epoch          : 346\n",
      "    loss           : -861822.1720297029\n",
      "    val_loss       : -860770.1383145333\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -995162.875000\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -795453.875000\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -797764.250000\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -858630.750000\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -842230.187500\n",
      "    epoch          : 347\n",
      "    loss           : -855540.8193069306\n",
      "    val_loss       : -859180.1683440208\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -995482.500000\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -831996.437500\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -791329.437500\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -996772.750000\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -812783.750000\n",
      "    epoch          : 348\n",
      "    loss           : -861842.9938118812\n",
      "    val_loss       : -861963.1800945282\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -994510.750000\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -831223.125000\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -802280.375000\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -846213.312500\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -861129.375000\n",
      "    epoch          : 349\n",
      "    loss           : -863318.0476485149\n",
      "    val_loss       : -862555.6076211929\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -996277.125000\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -848579.750000\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -803837.375000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -887485.250000\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -842238.000000\n",
      "    epoch          : 350\n",
      "    loss           : -863924.1008663366\n",
      "    val_loss       : -864068.2682195663\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/1109_111738/checkpoint-epoch350.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -1000115.625000\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -826872.437500\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -874357.750000\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -881532.875000\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -828124.000000\n",
      "    epoch          : 351\n",
      "    loss           : -856753.4832920792\n",
      "    val_loss       : -855232.7165874481\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -993347.375000\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -786616.500000\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -830533.437500\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -787740.812500\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -883228.812500\n",
      "    epoch          : 352\n",
      "    loss           : -851510.5092821782\n",
      "    val_loss       : -854244.2035693169\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -904876.625000\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -861906.000000\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -788267.875000\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -837463.625000\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -835883.625000\n",
      "    epoch          : 353\n",
      "    loss           : -858026.3162128713\n",
      "    val_loss       : -858102.3243111611\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -993739.937500\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -820887.000000\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -879914.562500\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -882895.937500\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -811272.812500\n",
      "    epoch          : 354\n",
      "    loss           : -862164.8589108911\n",
      "    val_loss       : -863491.4493372918\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -999476.000000\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -829336.000000\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -887180.125000\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -802381.000000\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -862620.375000\n",
      "    epoch          : 355\n",
      "    loss           : -863961.1720297029\n",
      "    val_loss       : -859624.9457385063\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -995690.437500\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -912270.000000\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -852164.437500\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -860664.250000\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -834501.937500\n",
      "    epoch          : 356\n",
      "    loss           : -854838.5018564357\n",
      "    val_loss       : -843743.3138908386\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -965059.750000\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -830839.125000\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -855567.812500\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -883418.125000\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -851992.750000\n",
      "    epoch          : 357\n",
      "    loss           : -860043.0785891089\n",
      "    val_loss       : -862981.7451311111\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -1000185.500000\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -911503.562500\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -801434.125000\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -997860.937500\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -860694.375000\n",
      "    epoch          : 358\n",
      "    loss           : -865064.7722772277\n",
      "    val_loss       : -862726.9541786194\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -1000674.125000\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -830883.625000\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -847018.625000\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -845477.687500\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -820146.500000\n",
      "    epoch          : 359\n",
      "    loss           : -866123.7660891089\n",
      "    val_loss       : -866773.5929722786\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -1003244.312500\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -911807.250000\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -846267.125000\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -1000383.750000\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -813965.375000\n",
      "    epoch          : 360\n",
      "    loss           : -866857.7407178218\n",
      "    val_loss       : -866459.5370013237\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -1001997.812500\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -890492.125000\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -890991.875000\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -856996.125000\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -864390.125000\n",
      "    epoch          : 361\n",
      "    loss           : -866193.0303217822\n",
      "    val_loss       : -862803.5148581505\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -1002025.625000\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -908955.000000\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -855504.375000\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -847398.062500\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -811969.750000\n",
      "    epoch          : 362\n",
      "    loss           : -864818.4356435643\n",
      "    val_loss       : -865187.2802282333\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -831849.250000\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -831881.125000\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -813327.687500\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -999887.375000\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -848573.250000\n",
      "    epoch          : 363\n",
      "    loss           : -865570.9832920792\n",
      "    val_loss       : -862197.4899846077\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -997466.000000\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -907149.625000\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -857347.500000\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -847774.437500\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -870707.187500\n",
      "    epoch          : 364\n",
      "    loss           : -864503.8391089109\n",
      "    val_loss       : -865370.3626395225\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -998922.375000\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -855599.000000\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -831909.875000\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -801440.125000\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -810673.750000\n",
      "    epoch          : 365\n",
      "    loss           : -862293.5977722772\n",
      "    val_loss       : -862988.0004014969\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -858853.125000\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -832020.062500\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -857091.062500\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -851822.937500\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -858887.312500\n",
      "    epoch          : 366\n",
      "    loss           : -853667.1113861386\n",
      "    val_loss       : -858633.7348731041\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -997831.625000\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -818747.125000\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -799231.312500\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -839396.375000\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -853009.375000\n",
      "    epoch          : 367\n",
      "    loss           : -856627.4740099009\n",
      "    val_loss       : -846674.9550608635\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -997064.375000\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -775996.500000\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -797285.875000\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -998475.125000\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -858510.062500\n",
      "    epoch          : 368\n",
      "    loss           : -855276.0600247525\n",
      "    val_loss       : -860108.6939913749\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -999175.750000\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -838546.187500\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -797768.562500\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -751508.562500\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -843965.000000\n",
      "    epoch          : 369\n",
      "    loss           : -857130.1435643565\n",
      "    val_loss       : -858120.7808240413\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -998620.187500\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -830391.750000\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -852508.312500\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -856099.875000\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -854448.687500\n",
      "    epoch          : 370\n",
      "    loss           : -862359.5198019802\n",
      "    val_loss       : -864185.4924592972\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -834799.500000\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -838973.312500\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -885784.750000\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -852204.000000\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -854554.312500\n",
      "    epoch          : 371\n",
      "    loss           : -863159.4368811881\n",
      "    val_loss       : -860030.1729754448\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -990064.750000\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -821236.687500\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -839186.937500\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -843246.750000\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -886389.125000\n",
      "    epoch          : 372\n",
      "    loss           : -864675.7073019802\n",
      "    val_loss       : -864824.5620711327\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -1002845.937500\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -826662.562500\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -841101.250000\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -994769.125000\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -801537.625000\n",
      "    epoch          : 373\n",
      "    loss           : -862186.8279702971\n",
      "    val_loss       : -855922.2399123192\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -999063.750000\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -828685.750000\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -874629.312500\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -850201.875000\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -814783.812500\n",
      "    epoch          : 374\n",
      "    loss           : -860823.3193069306\n",
      "    val_loss       : -861774.9683526993\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -999891.375000\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -868149.875000\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -798575.125000\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -865536.875000\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -846739.875000\n",
      "    epoch          : 375\n",
      "    loss           : -860582.1361386139\n",
      "    val_loss       : -864133.7530846596\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -883437.875000\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -919254.500000\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -882278.000000\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -892012.687500\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -841827.187500\n",
      "    epoch          : 376\n",
      "    loss           : -865759.2141089109\n",
      "    val_loss       : -864351.4357393265\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -997021.000000\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -833646.250000\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -852271.000000\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -809053.000000\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -853398.812500\n",
      "    epoch          : 377\n",
      "    loss           : -865532.0971534654\n",
      "    val_loss       : -864988.0390900612\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -912460.875000\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -828655.187500\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -884557.812500\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -857102.937500\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -858340.562500\n",
      "    epoch          : 378\n",
      "    loss           : -867316.3384900991\n",
      "    val_loss       : -862475.7911063194\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -1000097.937500\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -903038.062500\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -851352.562500\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -839545.625000\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -860384.250000\n",
      "    epoch          : 379\n",
      "    loss           : -858585.7970297029\n",
      "    val_loss       : -864260.0710612297\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -997232.250000\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -883491.187500\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -855082.750000\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -869484.625000\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -822641.687500\n",
      "    epoch          : 380\n",
      "    loss           : -865787.5365099009\n",
      "    val_loss       : -865399.0192779541\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -844046.625000\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -836636.375000\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -880491.437500\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -807879.750000\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -855748.562500\n",
      "    epoch          : 381\n",
      "    loss           : -866563.7004950495\n",
      "    val_loss       : -865538.881415844\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -997494.000000\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -826515.000000\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -801958.562500\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -993382.875000\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -856228.625000\n",
      "    epoch          : 382\n",
      "    loss           : -860324.521039604\n",
      "    val_loss       : -858375.873044014\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -993638.625000\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -844641.750000\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -866501.875000\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -985227.500000\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -850468.687500\n",
      "    epoch          : 383\n",
      "    loss           : -864490.9857673268\n",
      "    val_loss       : -864242.9090233803\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -826832.687500\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -889992.437500\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -881637.187500\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -842739.437500\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -856567.687500\n",
      "    epoch          : 384\n",
      "    loss           : -860553.6862623763\n",
      "    val_loss       : -862232.4532520294\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -996106.000000\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -911172.875000\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -814670.375000\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -995982.437500\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -854609.062500\n",
      "    epoch          : 385\n",
      "    loss           : -864345.0185643565\n",
      "    val_loss       : -864722.8860405922\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -999071.000000\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -834927.750000\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -868183.875000\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -840688.375000\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -819701.250000\n",
      "    epoch          : 386\n",
      "    loss           : -866258.301980198\n",
      "    val_loss       : -861273.0519834518\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -905162.437500\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -814963.500000\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -837372.000000\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -995976.375000\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -814512.125000\n",
      "    epoch          : 387\n",
      "    loss           : -857267.4777227723\n",
      "    val_loss       : -862016.4113570213\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -996598.875000\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -907656.937500\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -887651.687500\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -887494.625000\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -857155.625000\n",
      "    epoch          : 388\n",
      "    loss           : -866417.489480198\n",
      "    val_loss       : -866044.2157876969\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -999538.875000\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -882293.187500\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -841095.625000\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -888859.812500\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -813412.687500\n",
      "    epoch          : 389\n",
      "    loss           : -865104.3193069306\n",
      "    val_loss       : -863601.5565690994\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -995313.000000\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -818461.875000\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -845511.312500\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -852603.625000\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -851182.125000\n",
      "    epoch          : 390\n",
      "    loss           : -862724.5099009901\n",
      "    val_loss       : -863343.4321506501\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -997579.500000\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -818985.625000\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -880686.312500\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -988139.562500\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -816207.625000\n",
      "    epoch          : 391\n",
      "    loss           : -854862.2772277228\n",
      "    val_loss       : -855421.7729678154\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -991162.250000\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -807051.375000\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -805560.687500\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -808159.062500\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -817008.875000\n",
      "    epoch          : 392\n",
      "    loss           : -847528.1912128713\n",
      "    val_loss       : -832641.6016010285\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -979166.062500\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -806897.312500\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -841221.250000\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -851044.125000\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -850686.437500\n",
      "    epoch          : 393\n",
      "    loss           : -852924.7945544554\n",
      "    val_loss       : -858908.5875452996\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -820303.625000\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -909169.250000\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -884197.875000\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -861220.500000\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -848831.687500\n",
      "    epoch          : 394\n",
      "    loss           : -862142.7283415842\n",
      "    val_loss       : -857428.604614544\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -996380.000000\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -824420.875000\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -802906.062500\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -886632.750000\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -840435.375000\n",
      "    epoch          : 395\n",
      "    loss           : -862616.8087871287\n",
      "    val_loss       : -864069.8113441467\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -1001017.125000\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -916319.062500\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -802761.125000\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -892131.875000\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -858243.500000\n",
      "    epoch          : 396\n",
      "    loss           : -867647.2419554455\n",
      "    val_loss       : -865207.630957985\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -998857.687500\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -862607.500000\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -854039.250000\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -994495.125000\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -853982.375000\n",
      "    epoch          : 397\n",
      "    loss           : -865725.1169554455\n",
      "    val_loss       : -865461.4861685752\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -995799.437500\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -889557.312500\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -848834.625000\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -994239.750000\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -863089.875000\n",
      "    epoch          : 398\n",
      "    loss           : -868692.6058168317\n",
      "    val_loss       : -867371.3278300285\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -1000712.250000\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -812517.000000\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -866485.375000\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -1002435.750000\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -814952.000000\n",
      "    epoch          : 399\n",
      "    loss           : -867632.7865099009\n",
      "    val_loss       : -857839.7740750313\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -991514.312500\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -833850.125000\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -820327.812500\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -832837.437500\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -848319.062500\n",
      "    epoch          : 400\n",
      "    loss           : -856973.7283415842\n",
      "    val_loss       : -862086.7173988342\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/1109_111738/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -831968.562500\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -833954.437500\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -886578.125000\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -847997.062500\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -851147.375000\n",
      "    epoch          : 401\n",
      "    loss           : -866370.3310643565\n",
      "    val_loss       : -866347.2396436691\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -997878.687500\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -893285.125000\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -872495.000000\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -845885.625000\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -861244.812500\n",
      "    epoch          : 402\n",
      "    loss           : -864297.5680693069\n",
      "    val_loss       : -866408.7664235116\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -1001922.375000\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -833771.187500\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -853725.500000\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -866229.562500\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -819261.687500\n",
      "    epoch          : 403\n",
      "    loss           : -869405.4628712871\n",
      "    val_loss       : -865865.9999426842\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -998971.250000\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -836202.750000\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -809175.687500\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -846505.375000\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -862331.875000\n",
      "    epoch          : 404\n",
      "    loss           : -869208.3372524752\n",
      "    val_loss       : -868314.4220168113\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -1001525.062500\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -836176.312500\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -840461.937500\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -879746.875000\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -885134.125000\n",
      "    epoch          : 405\n",
      "    loss           : -862201.1169554455\n",
      "    val_loss       : -850502.5417596817\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -987295.750000\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -806184.875000\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -885337.875000\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -821293.125000\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -819197.250000\n",
      "    epoch          : 406\n",
      "    loss           : -858378.8663366337\n",
      "    val_loss       : -864239.3813478469\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -993860.187500\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -835158.187500\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -853749.000000\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -998853.437500\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -817251.937500\n",
      "    epoch          : 407\n",
      "    loss           : -866623.6280940594\n",
      "    val_loss       : -866363.7839927673\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -999660.875000\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -915560.750000\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -854650.250000\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -1000315.500000\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -865406.250000\n",
      "    epoch          : 408\n",
      "    loss           : -868256.292079208\n",
      "    val_loss       : -865247.927026844\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -1001677.812500\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -833515.000000\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -868024.062500\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -887036.187500\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -816292.437500\n",
      "    epoch          : 409\n",
      "    loss           : -868833.1435643565\n",
      "    val_loss       : -868474.427817154\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -1005423.625000\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -838235.562500\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -805611.125000\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -819418.000000\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -865822.750000\n",
      "    epoch          : 410\n",
      "    loss           : -870921.6231435643\n",
      "    val_loss       : -869563.4887818337\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -836269.687500\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -819877.250000\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -837824.875000\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -893732.125000\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -816868.750000\n",
      "    epoch          : 411\n",
      "    loss           : -870831.0779702971\n",
      "    val_loss       : -868728.8301503181\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -1000176.000000\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -828695.500000\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -794186.812500\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -988620.187500\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -843873.312500\n",
      "    epoch          : 412\n",
      "    loss           : -856046.3935643565\n",
      "    val_loss       : -856323.441990757\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -987995.000000\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -912305.750000\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -885342.062500\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -992034.687500\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -850753.562500\n",
      "    epoch          : 413\n",
      "    loss           : -861812.2735148515\n",
      "    val_loss       : -862322.7456520081\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -997065.812500\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -830700.125000\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -801728.375000\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -1001420.375000\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -815311.750000\n",
      "    epoch          : 414\n",
      "    loss           : -866551.9053217822\n",
      "    val_loss       : -864873.3625062943\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -997437.187500\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -913156.125000\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -868178.562500\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -867461.062500\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -822761.750000\n",
      "    epoch          : 415\n",
      "    loss           : -868806.9845297029\n",
      "    val_loss       : -868246.8442809104\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -998296.750000\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -835003.750000\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -814580.437500\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -856638.812500\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -854663.937500\n",
      "    epoch          : 416\n",
      "    loss           : -869997.7450495049\n",
      "    val_loss       : -868721.4405570984\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -1000213.062500\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -916180.500000\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -810958.250000\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -1003280.562500\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -817229.437500\n",
      "    epoch          : 417\n",
      "    loss           : -869420.0668316832\n",
      "    val_loss       : -863465.2851906776\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -1001731.500000\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -829693.500000\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -805320.875000\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -893299.687500\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -817298.125000\n",
      "    epoch          : 418\n",
      "    loss           : -868522.6800742574\n",
      "    val_loss       : -869407.7760763168\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -1002229.187500\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -855209.250000\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -837258.875000\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -818353.937500\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -856892.562500\n",
      "    epoch          : 419\n",
      "    loss           : -871906.354579208\n",
      "    val_loss       : -870034.5848567009\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -1004433.000000\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -918549.750000\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -818288.312500\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -825834.937500\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -807835.750000\n",
      "    epoch          : 420\n",
      "    loss           : -869289.0587871287\n",
      "    val_loss       : -864994.8692170143\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -999421.250000\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -838615.000000\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -802764.000000\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -895866.000000\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -868001.937500\n",
      "    epoch          : 421\n",
      "    loss           : -870769.5587871287\n",
      "    val_loss       : -870588.666324997\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -838402.750000\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -922126.125000\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -853926.000000\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -824767.625000\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -821412.750000\n",
      "    epoch          : 422\n",
      "    loss           : -873079.1831683168\n",
      "    val_loss       : -871219.4534761428\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -1004095.187500\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -840429.125000\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -822711.000000\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -859645.062500\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -845699.875000\n",
      "    epoch          : 423\n",
      "    loss           : -870865.6212871287\n",
      "    val_loss       : -869964.5821504593\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -1004427.062500\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -920645.750000\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -893386.312500\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -1005486.625000\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -863212.875000\n",
      "    epoch          : 424\n",
      "    loss           : -872444.7407178218\n",
      "    val_loss       : -870584.3106294632\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -1003657.187500\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -842068.250000\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -811775.062500\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -851909.687500\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -821352.625000\n",
      "    epoch          : 425\n",
      "    loss           : -870433.8805693069\n",
      "    val_loss       : -868026.3914034844\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -915424.687500\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -833803.000000\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -851848.500000\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -851179.187500\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -851227.875000\n",
      "    epoch          : 426\n",
      "    loss           : -868722.8038366337\n",
      "    val_loss       : -866756.6910180092\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -1001941.500000\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -907655.500000\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -865008.875000\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -839879.875000\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -849227.812500\n",
      "    epoch          : 427\n",
      "    loss           : -866069.8310643565\n",
      "    val_loss       : -868593.5973893165\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -1003005.062500\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -809950.125000\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -865945.375000\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -871150.125000\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -850524.000000\n",
      "    epoch          : 428\n",
      "    loss           : -869208.2060643565\n",
      "    val_loss       : -865015.6459090232\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -1003454.750000\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -861460.312500\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -852216.250000\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -864648.500000\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -854406.500000\n",
      "    epoch          : 429\n",
      "    loss           : -867117.7240099009\n",
      "    val_loss       : -867684.1423832893\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -1005230.187500\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -826020.250000\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -853877.437500\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -867554.750000\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -831214.187500\n",
      "    epoch          : 430\n",
      "    loss           : -864754.9443069306\n",
      "    val_loss       : -861041.2083128929\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -994490.500000\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -805112.875000\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -860602.625000\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -811927.062500\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -810496.000000\n",
      "    epoch          : 431\n",
      "    loss           : -863344.9090346535\n",
      "    val_loss       : -862751.2629561424\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -905382.125000\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -834664.375000\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -860547.687500\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -883516.500000\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -858789.687500\n",
      "    epoch          : 432\n",
      "    loss           : -864374.9207920792\n",
      "    val_loss       : -856447.8702014923\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -998536.125000\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -909740.937500\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -806655.625000\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -998361.562500\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -820430.687500\n",
      "    epoch          : 433\n",
      "    loss           : -867194.6448019802\n",
      "    val_loss       : -867600.1618593216\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -1000883.062500\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -820120.500000\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -845081.625000\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -835501.062500\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -838504.250000\n",
      "    epoch          : 434\n",
      "    loss           : -853975.0006188119\n",
      "    val_loss       : -858303.9336265564\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -896597.562500\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -835987.000000\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -803811.000000\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -866148.125000\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -840580.812500\n",
      "    epoch          : 435\n",
      "    loss           : -858050.4461633663\n",
      "    val_loss       : -850074.271866703\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -795842.000000\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -802628.437500\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -835854.500000\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -982387.875000\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -785572.562500\n",
      "    epoch          : 436\n",
      "    loss           : -841595.0290841584\n",
      "    val_loss       : -842708.2786830902\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -976991.125000\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -801920.000000\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -846007.750000\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -842303.875000\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -808244.687500\n",
      "    epoch          : 437\n",
      "    loss           : -854029.8756188119\n",
      "    val_loss       : -861080.0101359368\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -907125.562500\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -856388.000000\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -787150.625000\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -852706.937500\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -860901.375000\n",
      "    epoch          : 438\n",
      "    loss           : -862449.3143564357\n",
      "    val_loss       : -863584.3337164879\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -1000715.375000\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -916115.562500\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -853627.437500\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -859024.625000\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -815297.687500\n",
      "    epoch          : 439\n",
      "    loss           : -866044.4603960396\n",
      "    val_loss       : -863772.8884150505\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -997324.750000\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -919616.625000\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -888041.062500\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -816473.250000\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -855741.687500\n",
      "    epoch          : 440\n",
      "    loss           : -868250.0866336634\n",
      "    val_loss       : -862289.7019392013\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -986343.375000\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -839073.750000\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -860999.125000\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -863652.125000\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -845316.437500\n",
      "    epoch          : 441\n",
      "    loss           : -862294.5841584158\n",
      "    val_loss       : -863204.2833530426\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -982943.312500\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -909152.000000\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -863756.625000\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -856592.437500\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -865286.562500\n",
      "    epoch          : 442\n",
      "    loss           : -866458.1621287129\n",
      "    val_loss       : -866639.1991088868\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -912651.062500\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -838068.375000\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -867261.812500\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -848608.750000\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -820381.125000\n",
      "    epoch          : 443\n",
      "    loss           : -869101.8502475248\n",
      "    val_loss       : -869065.6507005692\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -1002894.062500\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -835677.875000\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -851109.687500\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -1002133.875000\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -813414.125000\n",
      "    epoch          : 444\n",
      "    loss           : -870268.4535891089\n",
      "    val_loss       : -864619.6403095245\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -1002538.375000\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -836764.625000\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -860122.437500\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -887381.625000\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -863379.312500\n",
      "    epoch          : 445\n",
      "    loss           : -868589.7506188119\n",
      "    val_loss       : -869703.987516117\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -1004820.812500\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -842840.375000\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -856446.875000\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -851531.375000\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -864320.812500\n",
      "    epoch          : 446\n",
      "    loss           : -868474.2939356435\n",
      "    val_loss       : -866667.4099136352\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -1004225.750000\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -817249.437500\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -867901.812500\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -976848.250000\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -851387.000000\n",
      "    epoch          : 447\n",
      "    loss           : -867889.5321782178\n",
      "    val_loss       : -862707.7693380356\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -912374.750000\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -913323.750000\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -885534.375000\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -859495.187500\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -860780.312500\n",
      "    epoch          : 448\n",
      "    loss           : -864858.7982673268\n",
      "    val_loss       : -863107.3538653373\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -997306.187500\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -834627.000000\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -845826.937500\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -819256.562500\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -815844.125000\n",
      "    epoch          : 449\n",
      "    loss           : -866439.8267326732\n",
      "    val_loss       : -863061.7540529252\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -997514.875000\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -830667.000000\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -855275.375000\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -858747.187500\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -889938.562500\n",
      "    epoch          : 450\n",
      "    loss           : -868532.8694306931\n",
      "    val_loss       : -860649.8183364868\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/1109_111738/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -999146.125000\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -826740.062500\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -862697.375000\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -838344.375000\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -861963.000000\n",
      "    epoch          : 451\n",
      "    loss           : -857675.1435643565\n",
      "    val_loss       : -860507.9485523223\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -995301.562500\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -912056.312500\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -879728.562500\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -869599.375000\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -848492.187500\n",
      "    epoch          : 452\n",
      "    loss           : -859077.9634900991\n",
      "    val_loss       : -858543.8032308578\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -992961.187500\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -904055.437500\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -863103.312500\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -818259.125000\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -854122.625000\n",
      "    epoch          : 453\n",
      "    loss           : -865951.3384900991\n",
      "    val_loss       : -855943.2150609016\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -994585.812500\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -828854.937500\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -843364.500000\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -876890.562500\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -814032.000000\n",
      "    epoch          : 454\n",
      "    loss           : -860658.4325495049\n",
      "    val_loss       : -849584.7352386474\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -898570.625000\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -882085.062500\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -882160.375000\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -1001464.937500\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -816015.375000\n",
      "    epoch          : 455\n",
      "    loss           : -861698.7326732674\n",
      "    val_loss       : -864097.2788559913\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -1002448.375000\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -832861.500000\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -864415.062500\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -824757.000000\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -846890.812500\n",
      "    epoch          : 456\n",
      "    loss           : -862311.8279702971\n",
      "    val_loss       : -859642.7937541008\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -999007.312500\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -890591.875000\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -832598.375000\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -833792.750000\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -853339.125000\n",
      "    epoch          : 457\n",
      "    loss           : -851200.8044554455\n",
      "    val_loss       : -859424.107353592\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -907440.375000\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -858851.000000\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -803962.437500\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -1003134.250000\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -798090.875000\n",
      "    epoch          : 458\n",
      "    loss           : -857774.5191831683\n",
      "    val_loss       : -849136.108541584\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -983386.437500\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -795866.250000\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -891216.437500\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -857696.750000\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -855849.187500\n",
      "    epoch          : 459\n",
      "    loss           : -862657.0587871287\n",
      "    val_loss       : -865255.3388977051\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -1001875.000000\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -828380.250000\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -857179.937500\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -887292.000000\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -813939.312500\n",
      "    epoch          : 460\n",
      "    loss           : -865637.9733910891\n",
      "    val_loss       : -857019.1720072746\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -999889.250000\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -832719.250000\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -808676.562500\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -821323.687500\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -870178.562500\n",
      "    epoch          : 461\n",
      "    loss           : -865708.2283415842\n",
      "    val_loss       : -868064.2317738533\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -1001171.250000\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -804585.437500\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -838389.125000\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -852495.812500\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -864500.000000\n",
      "    epoch          : 462\n",
      "    loss           : -871165.6373762377\n",
      "    val_loss       : -870059.701543045\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -1004429.500000\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -919128.125000\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -813996.250000\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -812724.250000\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -859411.812500\n",
      "    epoch          : 463\n",
      "    loss           : -868539.7821782178\n",
      "    val_loss       : -869058.1342335701\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -1001265.375000\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -833549.875000\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -870352.562500\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -856364.062500\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -869856.500000\n",
      "    epoch          : 464\n",
      "    loss           : -870666.9189356435\n",
      "    val_loss       : -869773.2825465202\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -1003068.375000\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -827660.750000\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -888327.062500\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -1002721.937500\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -858325.625000\n",
      "    epoch          : 465\n",
      "    loss           : -871371.1652227723\n",
      "    val_loss       : -869906.3697278022\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -1003231.500000\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -889553.125000\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -888908.562500\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -851691.875000\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -860798.687500\n",
      "    epoch          : 466\n",
      "    loss           : -869871.707920792\n",
      "    val_loss       : -856317.4667099953\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -986880.000000\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -856514.000000\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -796744.000000\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -871852.562500\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -855240.250000\n",
      "    epoch          : 467\n",
      "    loss           : -865352.1577970297\n",
      "    val_loss       : -868441.1138279915\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -1001406.062500\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -827471.000000\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -853934.375000\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -854634.875000\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -869613.750000\n",
      "    epoch          : 468\n",
      "    loss           : -871051.8143564357\n",
      "    val_loss       : -869008.5581703186\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -1003688.187500\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -811617.750000\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -892243.750000\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -861243.125000\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -863535.875000\n",
      "    epoch          : 469\n",
      "    loss           : -870022.145420792\n",
      "    val_loss       : -866743.3097487449\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -1004051.500000\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -821024.125000\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -867677.500000\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -856256.250000\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -866850.312500\n",
      "    epoch          : 470\n",
      "    loss           : -867757.5272277228\n",
      "    val_loss       : -869816.1385853768\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -1003062.437500\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -837574.750000\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -844700.875000\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -859243.625000\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -857452.750000\n",
      "    epoch          : 471\n",
      "    loss           : -868769.0872524752\n",
      "    val_loss       : -868372.3544370651\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -1001737.000000\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -842050.625000\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -809743.312500\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -889728.625000\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -894082.750000\n",
      "    epoch          : 472\n",
      "    loss           : -872177.7475247525\n",
      "    val_loss       : -870083.2163355828\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -1005482.625000\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -810485.500000\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -792072.937500\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -854547.000000\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -832377.750000\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch   473: reducing learning rate of group 0 to 1.0000e-04.\n",
      "    epoch          : 473\n",
      "    loss           : -862331.4443069306\n",
      "    val_loss       : -854924.1163603782\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -990368.187500\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -821792.562500\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -882309.375000\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -999042.000000\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -858489.312500\n",
      "    epoch          : 474\n",
      "    loss           : -860335.8168316832\n",
      "    val_loss       : -861835.095157814\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -908970.000000\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -910411.687500\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -837347.000000\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -839738.187500\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -863210.312500\n",
      "    epoch          : 475\n",
      "    loss           : -864934.0952970297\n",
      "    val_loss       : -864456.1701669693\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -1000762.687500\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -803375.062500\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -804525.625000\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -1000322.750000\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -861912.125000\n",
      "    epoch          : 476\n",
      "    loss           : -866565.989480198\n",
      "    val_loss       : -865346.9408830643\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -832543.687500\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -916172.625000\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -861733.500000\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -999187.625000\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -865053.437500\n",
      "    epoch          : 477\n",
      "    loss           : -867755.5860148515\n",
      "    val_loss       : -866145.1540664673\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -997577.812500\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -808866.250000\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -866794.375000\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -806977.250000\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -849256.187500\n",
      "    epoch          : 478\n",
      "    loss           : -869299.1602722772\n",
      "    val_loss       : -867897.3370366096\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -1003990.625000\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -839132.812500\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -886759.375000\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -863433.687500\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -860362.125000\n",
      "    epoch          : 479\n",
      "    loss           : -869873.5872524752\n",
      "    val_loss       : -868115.6269152642\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -999485.937500\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -867367.875000\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -889671.437500\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -858747.562500\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -865412.812500\n",
      "    epoch          : 480\n",
      "    loss           : -870543.4653465346\n",
      "    val_loss       : -868956.1247399331\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -1001677.125000\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -835472.687500\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -810476.312500\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -850273.812500\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -853122.562500\n",
      "    epoch          : 481\n",
      "    loss           : -871174.323019802\n",
      "    val_loss       : -869748.050862217\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -1003154.875000\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -837316.375000\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -862064.000000\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -1002739.187500\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -861971.625000\n",
      "    epoch          : 482\n",
      "    loss           : -872024.2722772277\n",
      "    val_loss       : -869950.5910822868\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -917197.875000\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -837278.000000\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -809962.875000\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -856044.875000\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -863865.875000\n",
      "    epoch          : 483\n",
      "    loss           : -872333.1992574257\n",
      "    val_loss       : -870283.0736485481\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -1003466.437500\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -836262.875000\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -854076.125000\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -821031.250000\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -823946.687500\n",
      "    epoch          : 484\n",
      "    loss           : -872748.6466584158\n",
      "    val_loss       : -871337.7421002388\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -837654.812500\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -896253.250000\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -893076.125000\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -809825.875000\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -862160.250000\n",
      "    epoch          : 485\n",
      "    loss           : -873314.7574257426\n",
      "    val_loss       : -871797.5383742333\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -1002527.562500\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -836157.375000\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -856817.937500\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -853752.625000\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -823714.375000\n",
      "    epoch          : 486\n",
      "    loss           : -873540.1967821782\n",
      "    val_loss       : -871547.5740486145\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -1004110.625000\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -918514.562500\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -894613.125000\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -869113.000000\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -870426.000000\n",
      "    epoch          : 487\n",
      "    loss           : -874047.6540841584\n",
      "    val_loss       : -872030.4206464768\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -1003852.312500\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -840296.000000\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -865023.437500\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -897004.625000\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -895880.062500\n",
      "    epoch          : 488\n",
      "    loss           : -874106.7493811881\n",
      "    val_loss       : -872168.2634799958\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -1004347.500000\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -840078.750000\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -811516.875000\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -822082.875000\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -826083.062500\n",
      "    epoch          : 489\n",
      "    loss           : -874347.6021039604\n",
      "    val_loss       : -872401.3703839302\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -1007431.250000\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -893873.125000\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -843004.000000\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -852694.000000\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -871862.000000\n",
      "    epoch          : 490\n",
      "    loss           : -874569.1751237623\n",
      "    val_loss       : -873549.9088162422\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -838690.187500\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -838366.625000\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -871318.562500\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -828046.000000\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -865659.000000\n",
      "    epoch          : 491\n",
      "    loss           : -874919.2599009901\n",
      "    val_loss       : -873089.6636939049\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -1005350.375000\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -811368.500000\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -872834.750000\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -849461.687500\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -867345.500000\n",
      "    epoch          : 492\n",
      "    loss           : -875195.4207920792\n",
      "    val_loss       : -873097.9372838974\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -1005462.875000\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -837989.312500\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -858569.250000\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -819203.250000\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -864353.687500\n",
      "    epoch          : 493\n",
      "    loss           : -875478.1584158416\n",
      "    val_loss       : -873159.3605558395\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -1002253.812500\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -843748.500000\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -865892.375000\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -855670.000000\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -868121.125000\n",
      "    epoch          : 494\n",
      "    loss           : -875201.9047029703\n",
      "    val_loss       : -873450.9329330444\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -1005677.437500\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -844171.500000\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -897071.125000\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -857212.125000\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -872561.875000\n",
      "    epoch          : 495\n",
      "    loss           : -875616.2821782178\n",
      "    val_loss       : -873445.2438537597\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -1005120.750000\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -840096.500000\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -858251.875000\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -892391.125000\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -821825.312500\n",
      "    epoch          : 496\n",
      "    loss           : -875655.4071782178\n",
      "    val_loss       : -873419.4187215805\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -1004853.500000\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -921796.125000\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -867470.625000\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -868185.000000\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -866728.750000\n",
      "    epoch          : 497\n",
      "    loss           : -875852.1782178218\n",
      "    val_loss       : -874008.6469867707\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -1006589.875000\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -922415.875000\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -894223.562500\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -860299.000000\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -864071.250000\n",
      "    epoch          : 498\n",
      "    loss           : -876107.3490099009\n",
      "    val_loss       : -874461.7134738922\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -1004331.687500\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -821639.062500\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -896285.250000\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -856697.000000\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -831323.937500\n",
      "    epoch          : 499\n",
      "    loss           : -876245.1831683168\n",
      "    val_loss       : -874686.9377361297\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -839654.937500\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -844196.500000\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -893347.875000\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -875969.000000\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -863336.625000\n",
      "    epoch          : 500\n",
      "    loss           : -876381.0012376237\n",
      "    val_loss       : -874808.0768427849\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/1109_111738/checkpoint-epoch500.pth ...\n",
      "Saving current best: model_best.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VaeCategoryModel(\n",
       "  (_category): FreeCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=12, bias=True)\n",
       "        (1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (4): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=12, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=12, bias=True)\n",
       "        (1): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (4): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=12, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=20, bias=True)\n",
       "        (1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=20, out_features=20, bias=True)\n",
       "        (4): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=20, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=20, bias=True)\n",
       "        (1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=20, out_features=20, bias=True)\n",
       "        (4): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=20, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=36, bias=True)\n",
       "        (1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (4): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=36, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=36, bias=True)\n",
       "        (1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (4): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=36, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=68, bias=True)\n",
       "        (1): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=68, out_features=68, bias=True)\n",
       "        (4): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=68, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=68, bias=True)\n",
       "        (1): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=68, out_features=68, bias=True)\n",
       "        (4): LayerNorm((68,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=68, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=102, bias=True)\n",
       "        (1): LayerNorm((102,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=102, out_features=102, bias=True)\n",
       "        (4): LayerNorm((102,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=102, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=102, bias=True)\n",
       "        (1): LayerNorm((102,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=102, out_features=102, bias=True)\n",
       "        (4): LayerNorm((102,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=102, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_10): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_11): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (conv_layers): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=2744, out_features=16, bias=True)\n",
       "        (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_12): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_13): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=24, bias=True)\n",
       "        (1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=24, out_features=24, bias=True)\n",
       "        (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=24, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_14): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_15): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=40, bias=True)\n",
       "        (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=40, out_features=40, bias=True)\n",
       "        (4): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=40, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_16): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_17): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=72, bias=True)\n",
       "        (1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=72, out_features=72, bias=True)\n",
       "        (4): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=72, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_18): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=106, bias=True)\n",
       "        (1): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=106, out_features=106, bias=True)\n",
       "        (4): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=106, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_19): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=106, bias=True)\n",
       "        (1): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=106, out_features=106, bias=True)\n",
       "        (4): LayerNorm((106,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=106, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_20): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_21): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (conv_layers): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=2744, out_features=32, bias=True)\n",
       "        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_22): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_23): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=48, bias=True)\n",
       "        (1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (4): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=48, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_24): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_25): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=80, bias=True)\n",
       "        (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=80, out_features=80, bias=True)\n",
       "        (4): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=80, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_26): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=114, bias=True)\n",
       "        (1): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=114, out_features=114, bias=True)\n",
       "        (4): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=114, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_27): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=114, bias=True)\n",
       "        (1): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=114, out_features=114, bias=True)\n",
       "        (4): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=114, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_28): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_29): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (conv_layers): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=2744, out_features=64, bias=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_30): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_31): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_32): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=130, bias=True)\n",
       "        (1): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=130, out_features=130, bias=True)\n",
       "        (4): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=130, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_33): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=130, bias=True)\n",
       "        (1): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=130, out_features=130, bias=True)\n",
       "        (4): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=130, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_34): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_35): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (conv_layers): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=2744, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_36): DensityDecoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=162, bias=True)\n",
       "        (1): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=162, out_features=162, bias=True)\n",
       "        (4): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=162, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_37): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=162, bias=True)\n",
       "        (1): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=162, out_features=162, bias=True)\n",
       "        (4): LayerNorm((162,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=162, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_38): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_39): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (conv_layers): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=2744, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_40): DensityDecoder(\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=196, out_features=2744, bias=True)\n",
       "        (1): LayerNorm((2744,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=2744, out_features=2744, bias=True)\n",
       "      )\n",
       "      (conv_layers): Sequential(\n",
       "        (0): ConvTranspose2d(56, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): ConvTranspose2d(28, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (generator_41): DensityEncoder(\n",
       "      (distribution): DiagonalGaussian()\n",
       "      (conv_layers): Sequential(\n",
       "        (0): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(56, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (dense_layers): Sequential(\n",
       "        (0): Linear(in_features=2744, out_features=392, bias=True)\n",
       "        (1): LayerNorm((392,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=392, out_features=392, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "    (global_element_5): StandardNormal()\n",
       "  )\n",
       "  (guide_temperatures): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_weights): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=96, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAATA0lEQVR4nO3de7CcdX3H8feH5ORCLkMSIIQkkIDhLgY9IBIvgIqCItjWC20FOtrYjlTsIJbaaYkz1tJ6AYu3iYUShAZtlYGp0UIDiKCkOcEkBEghYIBcyJWQiyHknHz7xz7pLIezvz3Zy9lNfp/XzM7Z3e8++3x3z/mc59nnsj9FBGZ24Duo1Q2Y2cBw2M0y4bCbZcJhN8uEw26WCYfdLBMO+wFG0ixJt9U47fGSfiNpm6TPNrq3RpP0R5LuaXUf+wuHvUEkvV3SryS9LGmzpIclnd7qvvbRF4AHImJURPxzq5upJiJuj4jzWt3H/sJhbwBJo4H/BG4ExgITgS8Bu1rZVw2OBh6vVJQ0aAB7SZI0uI5pJSm7v/3sXnCTHAcQEXMjoicidkbEPRGxFEDSsZLuk7RJ0kZJt0s6ZO/EklZKulrSUkk7JN0kabyknxWr1P8taUzx2CmSQtJMSWskrZV0VaXGJJ1ZrHFskbRE0tkVHncfcA7wLUnbJR0n6RZJ35U0T9IO4BxJJ0p6oHi+xyV9qOw5bpH0naLv7cXazRGSbpD0kqTlkk5L9BqSPivp2eJ9+ureUEq6vHi+6yVtBmYV9z1UNv1ZkhYWa1cLJZ1VVntA0t9Lehj4HXBM4vd5YIoIX+q8AKOBTcAc4HxgTK/6G4D3AkOBw4AHgRvK6iuBR4DxlNYK1gOPAqcV09wHXFs8dgoQwFxgBPBGYAPwnqI+C7ituD6x6OsCSv/Y31vcPqzC63gA+FTZ7VuAl4EZxfSjgBXAF4EhwLnANuD4ssdvBN4CDCv6/i1wKTAI+DJwf+J9DOB+SmtHRwFP7e0HuBzoBv4CGAwML+57qKiPBV4CPlHULylujyt7bc8DJxf1jlb/3Qz0xUv2BoiIrcDbKf2xfh/YIOluSeOL+oqIuDcidkXEBuAbwLt6Pc2NEbEuIlYDvwQWRMRvImIXcCel4Jf7UkTsiIjHgH+l9Mfd2x8D8yJiXkTsiYh7gS5K4e+vuyLi4YjYA0wHRgLXRcSrEXEfpY8v5fO+MyIWRcQrRd+vRMStEdED/LCP19HbP0bE5oh4Hrih13OviYgbI6I7Inb2mu4DwNMR8YOiPhdYDlxY9phbIuLxor57H96DA4LD3iAR8WREXB4Rk4BTgCMp/bEi6XBJd0haLWkrcBtwaK+nWFd2fWcft0f2evwLZdefK+bX29HAR4pV7i2StlD6pzRhH15a+XyOBF4ogl8+74llt/f1daTm1/t1vUBlRxaPL9e7t9T0BzyHvQkiYjmlVdpTirv+gdJS/9SIGE1pias6ZzO57PpRwJo+HvMC8IOIOKTsMiIirtuH+ZSfFrkGmNxr49ZRwOp9eL5qUq8rdYrmGkr/3Mr17i3rUzwd9gaQdIKkqyRNKm5PprT6+UjxkFHAdmCLpInA1Q2Y7d9KOljSycCfUFpF7u024EJJ75M0SNIwSWfv7bMGC4AdwBckdRQb+y4E7qjx+fpytaQxxXt4JX2/rr7MA46T9IeSBkv6GHASpY8ZhsPeKNuAtwILiq3WjwDLgL1byb8EvJnSxq6fAj9pwDx/QWlj2XzgaxHxuoNLIuIF4CJKG9Q2UFrSX02Nv/eIeBX4EKWNkBuB7wCXFmsyjXIXsAhYTOm9uqmfvW0CPkjpPd9E6ZiBD0bExgb2tl9TsaXS9hOSplDawt0REd0tbqehJAUwLSJWtLqXA5GX7GaZcNjNMuHVeLNMeMlulomaTyaoxRANjWGMGMhZmmXlFXbwauzq8xiOusIu6f3ANykd9/wv1Q7WGMYI3qp31zNLM0tYEPMr1mpejS9Od/w2pX2uJwGXSDqp1uczs+aq5zP7GcCKiHi2ONjiDkoHcJhZG6on7BN57YkFq3jtSQcAFOddd0nq2r3ffZeD2YGjnrD3tRHgdfvxImJ2RHRGRGcHQ+uYnZnVo56wr+K1ZyhNou8zr8ysDdQT9oXANElTJQ0BPg7c3Zi2zKzRat71FhHdkq4A/ovSrrebI6LilxWaWWvVtZ89IuZROo/YzNqcD5c1y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMDOiQzZafg4YNq1h7ZtZpyWl3j+1O1kc+3ZGsH/nVXyXrufGS3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhPezW1Mtv+HUirUr3/Gz5LTHDFmfrH9u56U19ZSrusIuaSWwDegBuiOisxFNmVnjNWLJfk5EbGzA85hZE/kzu1km6g17APdIWiRpZl8PkDRTUpekrt3sqnN2ZlarelfjZ0TEGkmHA/dKWh4RD5Y/ICJmA7MBRmts1Dk/M6tRXUv2iFhT/FwP3Amc0YimzKzxag67pBGSRu29DpwHLGtUY2bWWPWsxo8H7pS093n+LSJ+3pCubL+x4vozk/Uvn/PvFWvnDn8uOe3crZX30QMcvNbbl/dFzWGPiGeBNzWwFzNrIv9rNMuEw26WCYfdLBMOu1kmHHazTPgUV0va9YHTk/UZZz6RrB8yaEfF2t+tfV9y2vkLT0nWj//5lmR9T7KaHy/ZzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMeD975g5604nJ+stT08Mi74n08mLW8g9VrG1ZNi457eSHe9LzXpzex2+v5SW7WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJ72c/wK3667OS9bMuXpKsv/hiehCfRT8/KVkf37W7Ym3cuq3JaQetTo8X2p2sWm9esptlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmfB+9gPA6msq70v/9qe+l5z2+I70vu53PHNFsn7EE+lzzkc8sa5irWfMqOS0W8+akqyP+kXlffgAPZs2J+u5qbpkl3SzpPWSlpXdN1bSvZKeLn6OaW6bZlav/qzG3wK8v9d91wDzI2IaML+4bWZtrGrYI+JBoPf60EXAnOL6HODixrZlZo1W6wa68RGxFqD4eXilB0qaKalLUtdudtU4OzOrV9O3xkfE7IjojIjODoY2e3ZmVkGtYV8naQJA8XN941oys2aoNex3A5cV1y8D7mpMO2bWLFX3s0uaC5wNHCppFXAtcB3wI0mfBJ4HPtLMJnN30KknJOvLPvudmp/7gZ2jk3U9PzxZ7+lIn+/+uxPGV6xtOin9nfS7xqWfe/jaycm6fu397OWqhj0iLqlQeneDezGzJvLhsmaZcNjNMuGwm2XCYTfLhMNulgmf4rofGP6tTU177u+9eHayPuRlJetbpqXr3SMq14/pfD49bZXhoNdtnJSsH/HrZDk7XrKbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZpnwfvY28NT3zkjWf/uG2TU/9+3bxiXrC5Ydm36Co9IDIx921EvJ+rsmrKhY+7Nxv0xO+/DOKcn6dTN6fw/qaw36aeXX1vPUM8lpD0ResptlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmfB+9jbwp2f9oq7p/2dX5aGLr110YXLaIRvTfwLdw9Nf53zC2MpDMgOcfPDqirVJg9MjBP3ByDXJ+pKpTyTriyedVrE2+KnkpAckL9nNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0x4P/sAiBnTk/WXupck69duODlZv2P5WyrWDrkvPeRyla9mZ9uU9PfCL143MVk/anjl892Hjl6fnnl61kwbnt7Hv7jK9LmpumSXdLOk9ZKWld03S9JqSYuLywXNbdPM6tWf1fhbgL6+EuT6iJheXOY1ti0za7SqYY+IB4HNA9CLmTVRPRvorpC0tFjNH1PpQZJmSuqS1LWbXXXMzszqUWvYvwscC0wH1gJfr/TAiJgdEZ0R0dlB+sQHM2uemsIeEesioici9gDfB9Jfj2pmLVdT2CVNKLv5YWBZpceaWXuoup9d0lzgbOBQSauAa4GzJU0HAlgJfLp5Le7/Nrzp4GT9p8+m96OzZHSyPHXeyxVrsSg9SPlBI0Yk64dPnZysP/uxiptrANg+uXkf3W5Ydm6yfvT8RU2b9/6oatgj4pI+7r6pCb2YWRP5cFmzTDjsZplw2M0y4bCbZcJhN8uET3EdAFHlXdaj6V1rk77yq/Tz72tDZfbs2JGsD3olfYjz0DduSdavPPSBRHVkctqP/7bKrrWPPpas22t5yW6WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcL72QfA4J3pPeEHr69nT3l9fvd7b03WV71vT7K+8C03JOuHDqq8L/2rm49NTrvl3PQxALZvvGQ3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLh/ewNMHjikcl697D02MHbJ6XrQ84/PVnfNrnyr3Hz6d3Jaf/jPTcm66cOGZSsdyj9VdSXPvfOirV1b9uanBYPF9ZQXrKbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZpnoz5DNk4FbgSOAPcDsiPimpLHAD4EplIZt/mhEvNS8VttYT0+yvPX4dP3o419M1k+8YF2y/pnD7q9YO3nI8OS0MKRKPe1tS34/WR99/jN1Pb81Tn+W7N3AVRFxInAm8BlJJwHXAPMjYhowv7htZm2qatgjYm1EPFpc3wY8CUwELgLmFA+bA1zcpB7NrAH26TO7pCnAacACYHxErIXSPwTg8IZ3Z2YN0++wSxoJ/Bj4XERUO6i5fLqZkrokde32sc5mLdOvsEvqoBT02yPiJ8Xd6yRNKOoTgPV9TRsRsyOiMyI6OxjaiJ7NrAZVwy5JwE3AkxHxjbLS3cBlxfXLgLsa356ZNUp/TnGdAXwCeEzS4uK+LwLXAT+S9EngeeAjTelwPxA96a9bjoPTu96+Oe2HyfqpQ4ZV6aDa7rXaHXfrnyfrU6/5ddPmbY1VNewR8RBQ6YTrdze2HTNrFh9BZ5YJh90sEw67WSYcdrNMOOxmmXDYzTLhr5JugJ4NG5L1QVWGJh6h9Nc91+PH20cn67MvvThZn/qI96MfKLxkN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4f3sA+DYzz+SrJ836PPpJ4h0+Zg7K3/d10G//E16YpZWqduBwkt2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwT3s/eBt7wl+n98GaN4CW7WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpaJqmGXNFnS/ZKelPS4pCuL+2dJWi1pcXG5oPntmlmt+nNQTTdwVUQ8KmkUsEjSvUXt+oj4WvPaM7NGqRr2iFgLrC2ub5P0JDCx2Y2ZWWPt02d2SVOA04AFxV1XSFoq6WZJYypMM1NSl6Su3VT++iQza65+h13SSODHwOciYivwXeBYYDqlJf/X+5ouImZHRGdEdHYwtP6Ozawm/Qq7pA5KQb89In4CEBHrIqInIvYA3wfOaF6bZlav/myNF3AT8GREfKPs/gllD/swsKzx7ZlZo/Rna/wM4BPAY5IWF/d9EbhE0nRKX3S8Evh0E/ozswbpz9b4hwD1UZrX+HbMrFl8BJ1ZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhCJi4GYmbQCeK7vrUGDjgDWwb9q1t3btC9xbrRrZ29ERcVhfhQEN++tmLnVFRGfLGkho197atS9wb7UaqN68Gm+WCYfdLBOtDvvsFs8/pV17a9e+wL3VakB6a+lndjMbOK1espvZAHHYzTLRkrBLer+k/5W0QtI1reihEkkrJT1WDEPd1eJebpa0XtKysvvGSrpX0tPFzz7H2GtRb20xjHdimPGWvnetHv58wD+zSxoEPAW8F1gFLAQuiYgnBrSRCiStBDojouUHYEh6J7AduDUiTinu+ydgc0RcV/yjHBMRf9Umvc0Ctrd6GO9itKIJ5cOMAxcDl9PC9y7R10cZgPetFUv2M4AVEfFsRLwK3AFc1II+2l5EPAhs7nX3RcCc4vocSn8sA65Cb20hItZGxKPF9W3A3mHGW/reJfoaEK0I+0TghbLbq2iv8d4DuEfSIkkzW91MH8ZHxFoo/fEAh7e4n96qDuM9kHoNM942710tw5/XqxVh72soqXba/zcjIt4MnA98plhdtf7p1zDeA6WPYcbbQq3Dn9erFWFfBUwuuz0JWNOCPvoUEWuKn+uBO2m/oajX7R1Bt/i5vsX9/L92Gsa7r2HGaYP3rpXDn7ci7AuBaZKmShoCfBy4uwV9vI6kEcWGEySNAM6j/Yaivhu4rLh+GXBXC3t5jXYZxrvSMOO0+L1r+fDnETHgF+ACSlvknwH+phU9VOjrGGBJcXm81b0Bcymt1u2mtEb0SWAcMB94uvg5to16+wHwGLCUUrAmtKi3t1P6aLgUWFxcLmj1e5foa0DeNx8ua5YJH0FnlgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2Xi/wBgw9ODNh2mNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n",
      "Substituting symbol R from STIXGeneral\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    path.draw()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:categorical_bpl] *",
   "language": "python",
   "name": "conda-env-categorical_bpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
