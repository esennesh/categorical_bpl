{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/AnacondaProjects/categorical_bpl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='mnist_config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optim_args': {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": True\n",
    "    },\n",
    "    \"patience\": 20,\n",
    "    \"factor\": 0.5,\n",
    "    \"verbose\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = config.init_obj('optimizer', pyro.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [512/54000 (1%)] Loss: 79788.046875\n",
      "Train Epoch: 1 [11776/54000 (22%)] Loss: -49623.750000\n",
      "Train Epoch: 1 [23040/54000 (43%)] Loss: -62972.355469\n",
      "Train Epoch: 1 [34304/54000 (64%)] Loss: -67834.453125\n",
      "Train Epoch: 1 [45568/54000 (84%)] Loss: -75471.390625\n",
      "    epoch          : 1\n",
      "    loss           : -56648.67330600248\n",
      "    val_loss       : -74589.801171875\n",
      "Train Epoch: 2 [512/54000 (1%)] Loss: -59266.597656\n",
      "Train Epoch: 2 [11776/54000 (22%)] Loss: -74053.539062\n",
      "Train Epoch: 2 [23040/54000 (43%)] Loss: -86578.859375\n",
      "Train Epoch: 2 [34304/54000 (64%)] Loss: -95349.656250\n",
      "Train Epoch: 2 [45568/54000 (84%)] Loss: -93481.468750\n",
      "    epoch          : 2\n",
      "    loss           : -86687.90733292079\n",
      "    val_loss       : -100351.463671875\n",
      "Train Epoch: 3 [512/54000 (1%)] Loss: -89897.328125\n",
      "Train Epoch: 3 [11776/54000 (22%)] Loss: -107130.203125\n",
      "Train Epoch: 3 [23040/54000 (43%)] Loss: -111655.226562\n",
      "Train Epoch: 3 [34304/54000 (64%)] Loss: -126409.585938\n",
      "Train Epoch: 3 [45568/54000 (84%)] Loss: -118845.203125\n",
      "    epoch          : 3\n",
      "    loss           : -112433.89820544554\n",
      "    val_loss       : -125073.1880859375\n",
      "Train Epoch: 4 [512/54000 (1%)] Loss: -121359.500000\n",
      "Train Epoch: 4 [11776/54000 (22%)] Loss: -127560.171875\n",
      "Train Epoch: 4 [23040/54000 (43%)] Loss: -137127.406250\n",
      "Train Epoch: 4 [34304/54000 (64%)] Loss: -131814.593750\n",
      "Train Epoch: 4 [45568/54000 (84%)] Loss: -140671.593750\n",
      "    epoch          : 4\n",
      "    loss           : -135252.70900371287\n",
      "    val_loss       : -146183.9517578125\n",
      "Train Epoch: 5 [512/54000 (1%)] Loss: -141926.468750\n",
      "Train Epoch: 5 [11776/54000 (22%)] Loss: -157136.140625\n",
      "Train Epoch: 5 [23040/54000 (43%)] Loss: -160635.765625\n",
      "Train Epoch: 5 [34304/54000 (64%)] Loss: -156367.296875\n",
      "Train Epoch: 5 [45568/54000 (84%)] Loss: -161355.875000\n",
      "    epoch          : 5\n",
      "    loss           : -156181.62113242573\n",
      "    val_loss       : -166467.10234375\n",
      "Train Epoch: 6 [512/54000 (1%)] Loss: -164362.984375\n",
      "Train Epoch: 6 [11776/54000 (22%)] Loss: -166056.140625\n",
      "Train Epoch: 6 [23040/54000 (43%)] Loss: -170740.468750\n",
      "Train Epoch: 6 [34304/54000 (64%)] Loss: -183563.250000\n",
      "Train Epoch: 6 [45568/54000 (84%)] Loss: -182402.125000\n",
      "    epoch          : 6\n",
      "    loss           : -176311.7057549505\n",
      "    val_loss       : -186598.71171875\n",
      "Train Epoch: 7 [512/54000 (1%)] Loss: -186827.015625\n",
      "Train Epoch: 7 [11776/54000 (22%)] Loss: -186159.765625\n",
      "Train Epoch: 7 [23040/54000 (43%)] Loss: -198635.828125\n",
      "Train Epoch: 7 [34304/54000 (64%)] Loss: -199364.437500\n",
      "Train Epoch: 7 [45568/54000 (84%)] Loss: -202010.906250\n",
      "    epoch          : 7\n",
      "    loss           : -195369.40640470298\n",
      "    val_loss       : -204966.6556640625\n",
      "Train Epoch: 8 [512/54000 (1%)] Loss: -208384.265625\n",
      "Train Epoch: 8 [11776/54000 (22%)] Loss: -207924.687500\n",
      "Train Epoch: 8 [23040/54000 (43%)] Loss: -218293.000000\n",
      "Train Epoch: 8 [34304/54000 (64%)] Loss: -210690.453125\n",
      "Train Epoch: 8 [45568/54000 (84%)] Loss: -222181.875000\n",
      "    epoch          : 8\n",
      "    loss           : -213751.81528465348\n",
      "    val_loss       : -222619.2126953125\n",
      "Train Epoch: 9 [512/54000 (1%)] Loss: -229009.312500\n",
      "Train Epoch: 9 [11776/54000 (22%)] Loss: -225664.203125\n",
      "Train Epoch: 9 [23040/54000 (43%)] Loss: -225648.843750\n",
      "Train Epoch: 9 [34304/54000 (64%)] Loss: -242458.578125\n",
      "Train Epoch: 9 [45568/54000 (84%)] Loss: -235727.953125\n",
      "    epoch          : 9\n",
      "    loss           : -231330.21581064357\n",
      "    val_loss       : -239868.6927734375\n",
      "Train Epoch: 10 [512/54000 (1%)] Loss: -249523.953125\n",
      "Train Epoch: 10 [11776/54000 (22%)] Loss: -245729.562500\n",
      "Train Epoch: 10 [23040/54000 (43%)] Loss: -247605.078125\n",
      "Train Epoch: 10 [34304/54000 (64%)] Loss: -248437.843750\n",
      "Train Epoch: 10 [45568/54000 (84%)] Loss: -253299.609375\n",
      "    epoch          : 10\n",
      "    loss           : -247387.06837871287\n",
      "    val_loss       : -255391.68720703124\n",
      "Train Epoch: 11 [512/54000 (1%)] Loss: -267808.687500\n",
      "Train Epoch: 11 [11776/54000 (22%)] Loss: -255965.718750\n",
      "Train Epoch: 11 [23040/54000 (43%)] Loss: -252040.906250\n",
      "Train Epoch: 11 [34304/54000 (64%)] Loss: -279788.687500\n",
      "Train Epoch: 11 [45568/54000 (84%)] Loss: -268926.812500\n",
      "    epoch          : 11\n",
      "    loss           : -263189.2181311881\n",
      "    val_loss       : -270841.81181640626\n",
      "Train Epoch: 12 [512/54000 (1%)] Loss: -285950.500000\n",
      "Train Epoch: 12 [11776/54000 (22%)] Loss: -280681.968750\n",
      "Train Epoch: 12 [23040/54000 (43%)] Loss: -263844.906250\n",
      "Train Epoch: 12 [34304/54000 (64%)] Loss: -277815.250000\n",
      "Train Epoch: 12 [45568/54000 (84%)] Loss: -282623.500000\n",
      "    epoch          : 12\n",
      "    loss           : -277682.7357673267\n",
      "    val_loss       : -285030.77373046876\n",
      "Train Epoch: 13 [512/54000 (1%)] Loss: -303431.187500\n",
      "Train Epoch: 13 [11776/54000 (22%)] Loss: -285171.375000\n",
      "Train Epoch: 13 [23040/54000 (43%)] Loss: -279951.750000\n",
      "Train Epoch: 13 [34304/54000 (64%)] Loss: -276398.125000\n",
      "Train Epoch: 13 [45568/54000 (84%)] Loss: -293088.625000\n",
      "    epoch          : 13\n",
      "    loss           : -291544.463490099\n",
      "    val_loss       : -297904.6268554687\n",
      "Train Epoch: 14 [512/54000 (1%)] Loss: -318716.937500\n",
      "Train Epoch: 14 [11776/54000 (22%)] Loss: -297743.687500\n",
      "Train Epoch: 14 [23040/54000 (43%)] Loss: -313519.812500\n",
      "Train Epoch: 14 [34304/54000 (64%)] Loss: -306978.500000\n",
      "Train Epoch: 14 [45568/54000 (84%)] Loss: -311870.187500\n",
      "    epoch          : 14\n",
      "    loss           : -304966.3140470297\n",
      "    val_loss       : -311545.5096679687\n",
      "Train Epoch: 15 [512/54000 (1%)] Loss: -334426.937500\n",
      "Train Epoch: 15 [11776/54000 (22%)] Loss: -319885.250000\n",
      "Train Epoch: 15 [23040/54000 (43%)] Loss: -327303.031250\n",
      "Train Epoch: 15 [34304/54000 (64%)] Loss: -316866.750000\n",
      "Train Epoch: 15 [45568/54000 (84%)] Loss: -321240.562500\n",
      "    epoch          : 15\n",
      "    loss           : -317452.63056930696\n",
      "    val_loss       : -323527.5643554687\n",
      "Train Epoch: 16 [512/54000 (1%)] Loss: -348893.531250\n",
      "Train Epoch: 16 [11776/54000 (22%)] Loss: -322834.562500\n",
      "Train Epoch: 16 [23040/54000 (43%)] Loss: -325821.187500\n",
      "Train Epoch: 16 [34304/54000 (64%)] Loss: -341751.625000\n",
      "Train Epoch: 16 [45568/54000 (84%)] Loss: -331864.031250\n",
      "    epoch          : 16\n",
      "    loss           : -329796.5959158416\n",
      "    val_loss       : -335537.1154296875\n",
      "Train Epoch: 17 [512/54000 (1%)] Loss: -322934.968750\n",
      "Train Epoch: 17 [11776/54000 (22%)] Loss: -333973.781250\n",
      "Train Epoch: 17 [23040/54000 (43%)] Loss: -316491.406250\n",
      "Train Epoch: 17 [34304/54000 (64%)] Loss: -354165.250000\n",
      "Train Epoch: 17 [45568/54000 (84%)] Loss: -346195.812500\n",
      "    epoch          : 17\n",
      "    loss           : -340921.0776608911\n",
      "    val_loss       : -345870.091796875\n",
      "Train Epoch: 18 [512/54000 (1%)] Loss: -338936.750000\n",
      "Train Epoch: 18 [11776/54000 (22%)] Loss: -356524.093750\n",
      "Train Epoch: 18 [23040/54000 (43%)] Loss: -326321.250000\n",
      "Train Epoch: 18 [34304/54000 (64%)] Loss: -364921.500000\n",
      "Train Epoch: 18 [45568/54000 (84%)] Loss: -351523.093750\n",
      "    epoch          : 18\n",
      "    loss           : -351960.08261138614\n",
      "    val_loss       : -356921.67431640625\n",
      "Train Epoch: 19 [512/54000 (1%)] Loss: -387936.125000\n",
      "Train Epoch: 19 [11776/54000 (22%)] Loss: -353425.906250\n",
      "Train Epoch: 19 [23040/54000 (43%)] Loss: -363649.812500\n",
      "Train Epoch: 19 [34304/54000 (64%)] Loss: -335632.125000\n",
      "Train Epoch: 19 [45568/54000 (84%)] Loss: -367784.625000\n",
      "    epoch          : 19\n",
      "    loss           : -361995.52877475246\n",
      "    val_loss       : -367358.0248046875\n",
      "Train Epoch: 20 [512/54000 (1%)] Loss: -399831.093750\n",
      "Train Epoch: 20 [11776/54000 (22%)] Loss: -364296.718750\n",
      "Train Epoch: 20 [23040/54000 (43%)] Loss: -355085.437500\n",
      "Train Epoch: 20 [34304/54000 (64%)] Loss: -357737.937500\n",
      "Train Epoch: 20 [45568/54000 (84%)] Loss: -369941.281250\n",
      "    epoch          : 20\n",
      "    loss           : -372084.3078589109\n",
      "    val_loss       : -376285.9862304687\n",
      "Train Epoch: 21 [512/54000 (1%)] Loss: -410282.843750\n",
      "Train Epoch: 21 [11776/54000 (22%)] Loss: -386788.593750\n",
      "Train Epoch: 21 [23040/54000 (43%)] Loss: -381380.812500\n",
      "Train Epoch: 21 [34304/54000 (64%)] Loss: -376938.687500\n",
      "Train Epoch: 21 [45568/54000 (84%)] Loss: -381063.375000\n",
      "    epoch          : 21\n",
      "    loss           : -380995.0869430693\n",
      "    val_loss       : -386451.9765625\n",
      "Train Epoch: 22 [512/54000 (1%)] Loss: -422600.625000\n",
      "Train Epoch: 22 [11776/54000 (22%)] Loss: -383555.812500\n",
      "Train Epoch: 22 [23040/54000 (43%)] Loss: -358403.968750\n",
      "Train Epoch: 22 [34304/54000 (64%)] Loss: -429070.687500\n",
      "Train Epoch: 22 [45568/54000 (84%)] Loss: -388791.218750\n",
      "    epoch          : 22\n",
      "    loss           : -391444.4170792079\n",
      "    val_loss       : -395524.3184570313\n",
      "Train Epoch: 23 [512/54000 (1%)] Loss: -432449.593750\n",
      "Train Epoch: 23 [11776/54000 (22%)] Loss: -388457.562500\n",
      "Train Epoch: 23 [23040/54000 (43%)] Loss: -367849.375000\n",
      "Train Epoch: 23 [34304/54000 (64%)] Loss: -396883.312500\n",
      "Train Epoch: 23 [45568/54000 (84%)] Loss: -399851.687500\n",
      "    epoch          : 23\n",
      "    loss           : -399903.0618811881\n",
      "    val_loss       : -403792.28291015624\n",
      "Train Epoch: 24 [512/54000 (1%)] Loss: -443633.250000\n",
      "Train Epoch: 24 [11776/54000 (22%)] Loss: -399779.406250\n",
      "Train Epoch: 24 [23040/54000 (43%)] Loss: -423140.437500\n",
      "Train Epoch: 24 [34304/54000 (64%)] Loss: -404012.625000\n",
      "Train Epoch: 24 [45568/54000 (84%)] Loss: -407989.000000\n",
      "    epoch          : 24\n",
      "    loss           : -408013.9931930693\n",
      "    val_loss       : -412215.8008789063\n",
      "Train Epoch: 25 [512/54000 (1%)] Loss: -452775.750000\n",
      "Train Epoch: 25 [11776/54000 (22%)] Loss: -409199.500000\n",
      "Train Epoch: 25 [23040/54000 (43%)] Loss: -429777.906250\n",
      "Train Epoch: 25 [34304/54000 (64%)] Loss: -413524.062500\n",
      "Train Epoch: 25 [45568/54000 (84%)] Loss: -414081.156250\n",
      "    epoch          : 25\n",
      "    loss           : -416178.1027227723\n",
      "    val_loss       : -419000.7771484375\n",
      "Train Epoch: 26 [512/54000 (1%)] Loss: -461459.906250\n",
      "Train Epoch: 26 [11776/54000 (22%)] Loss: -413989.656250\n",
      "Train Epoch: 26 [23040/54000 (43%)] Loss: -425313.312500\n",
      "Train Epoch: 26 [34304/54000 (64%)] Loss: -390032.781250\n",
      "Train Epoch: 26 [45568/54000 (84%)] Loss: -431123.875000\n",
      "    epoch          : 26\n",
      "    loss           : -423720.599009901\n",
      "    val_loss       : -427984.8853515625\n",
      "Train Epoch: 27 [512/54000 (1%)] Loss: -470666.968750\n",
      "Train Epoch: 27 [11776/54000 (22%)] Loss: -422491.500000\n",
      "Train Epoch: 27 [23040/54000 (43%)] Loss: -449071.875000\n",
      "Train Epoch: 27 [34304/54000 (64%)] Loss: -448469.562500\n",
      "Train Epoch: 27 [45568/54000 (84%)] Loss: -429738.531250\n",
      "    epoch          : 27\n",
      "    loss           : -431867.239789604\n",
      "    val_loss       : -433904.06318359374\n",
      "Train Epoch: 28 [512/54000 (1%)] Loss: -450570.843750\n",
      "Train Epoch: 28 [11776/54000 (22%)] Loss: -430272.093750\n",
      "Train Epoch: 28 [23040/54000 (43%)] Loss: -412461.906250\n",
      "Train Epoch: 28 [34304/54000 (64%)] Loss: -482648.062500\n",
      "Train Epoch: 28 [45568/54000 (84%)] Loss: -441061.937500\n",
      "    epoch          : 28\n",
      "    loss           : -437673.89851485146\n",
      "    val_loss       : -439956.0506835937\n",
      "Train Epoch: 29 [512/54000 (1%)] Loss: -485151.718750\n",
      "Train Epoch: 29 [11776/54000 (22%)] Loss: -458233.250000\n",
      "Train Epoch: 29 [23040/54000 (43%)] Loss: -463315.687500\n",
      "Train Epoch: 29 [34304/54000 (64%)] Loss: -492069.937500\n",
      "Train Epoch: 29 [45568/54000 (84%)] Loss: -438782.312500\n",
      "    epoch          : 29\n",
      "    loss           : -445274.50804455444\n",
      "    val_loss       : -448990.3927734375\n",
      "Train Epoch: 30 [512/54000 (1%)] Loss: -495130.937500\n",
      "Train Epoch: 30 [11776/54000 (22%)] Loss: -443168.718750\n",
      "Train Epoch: 30 [23040/54000 (43%)] Loss: -429874.406250\n",
      "Train Epoch: 30 [34304/54000 (64%)] Loss: -455131.562500\n",
      "Train Epoch: 30 [45568/54000 (84%)] Loss: -446863.250000\n",
      "    epoch          : 30\n",
      "    loss           : -452483.4198638614\n",
      "    val_loss       : -455840.651953125\n",
      "Train Epoch: 31 [512/54000 (1%)] Loss: -502661.343750\n",
      "Train Epoch: 31 [11776/54000 (22%)] Loss: -446935.812500\n",
      "Train Epoch: 31 [23040/54000 (43%)] Loss: -477527.375000\n",
      "Train Epoch: 31 [34304/54000 (64%)] Loss: -439611.906250\n",
      "Train Epoch: 31 [45568/54000 (84%)] Loss: -436925.687500\n",
      "    epoch          : 31\n",
      "    loss           : -459076.57209158415\n",
      "    val_loss       : -459685.2291015625\n",
      "Train Epoch: 32 [512/54000 (1%)] Loss: -509337.125000\n",
      "Train Epoch: 32 [11776/54000 (22%)] Loss: -440773.062500\n",
      "Train Epoch: 32 [23040/54000 (43%)] Loss: -485896.812500\n",
      "Train Epoch: 32 [34304/54000 (64%)] Loss: -515819.531250\n",
      "Train Epoch: 32 [45568/54000 (84%)] Loss: -456466.625000\n",
      "    epoch          : 32\n",
      "    loss           : -465079.9563737624\n",
      "    val_loss       : -467537.2130859375\n",
      "Train Epoch: 33 [512/54000 (1%)] Loss: -517250.000000\n",
      "Train Epoch: 33 [11776/54000 (22%)] Loss: -460312.062500\n",
      "Train Epoch: 33 [23040/54000 (43%)] Loss: -466073.500000\n",
      "Train Epoch: 33 [34304/54000 (64%)] Loss: -521209.781250\n",
      "Train Epoch: 33 [45568/54000 (84%)] Loss: -464456.000000\n",
      "    epoch          : 33\n",
      "    loss           : -471389.54981435643\n",
      "    val_loss       : -472374.128125\n",
      "Train Epoch: 34 [512/54000 (1%)] Loss: -523476.656250\n",
      "Train Epoch: 34 [11776/54000 (22%)] Loss: -493549.250000\n",
      "Train Epoch: 34 [23040/54000 (43%)] Loss: -466854.156250\n",
      "Train Epoch: 34 [34304/54000 (64%)] Loss: -474101.625000\n",
      "Train Epoch: 34 [45568/54000 (84%)] Loss: -472721.000000\n",
      "    epoch          : 34\n",
      "    loss           : -478179.02475247526\n",
      "    val_loss       : -480917.20361328125\n",
      "Train Epoch: 35 [512/54000 (1%)] Loss: -531216.187500\n",
      "Train Epoch: 35 [11776/54000 (22%)] Loss: -485974.781250\n",
      "Train Epoch: 35 [23040/54000 (43%)] Loss: -457170.906250\n",
      "Train Epoch: 35 [34304/54000 (64%)] Loss: -473556.000000\n",
      "Train Epoch: 35 [45568/54000 (84%)] Loss: -501117.843750\n",
      "    epoch          : 35\n",
      "    loss           : -482965.77568069304\n",
      "    val_loss       : -481047.8965820313\n",
      "Train Epoch: 36 [512/54000 (1%)] Loss: -537376.187500\n",
      "Train Epoch: 36 [11776/54000 (22%)] Loss: -503492.031250\n",
      "Train Epoch: 36 [23040/54000 (43%)] Loss: -462034.468750\n",
      "Train Epoch: 36 [34304/54000 (64%)] Loss: -509714.250000\n",
      "Train Epoch: 36 [45568/54000 (84%)] Loss: -481886.187500\n",
      "    epoch          : 36\n",
      "    loss           : -488663.3678836634\n",
      "    val_loss       : -492437.8591796875\n",
      "Train Epoch: 37 [512/54000 (1%)] Loss: -545034.000000\n",
      "Train Epoch: 37 [11776/54000 (22%)] Loss: -512961.062500\n",
      "Train Epoch: 37 [23040/54000 (43%)] Loss: -453298.125000\n",
      "Train Epoch: 37 [34304/54000 (64%)] Loss: -517139.468750\n",
      "Train Epoch: 37 [45568/54000 (84%)] Loss: -501068.250000\n",
      "    epoch          : 37\n",
      "    loss           : -495299.1349009901\n",
      "    val_loss       : -494618.643359375\n",
      "Train Epoch: 38 [512/54000 (1%)] Loss: -550124.750000\n",
      "Train Epoch: 38 [11776/54000 (22%)] Loss: -516621.812500\n",
      "Train Epoch: 38 [23040/54000 (43%)] Loss: -521114.187500\n",
      "Train Epoch: 38 [34304/54000 (64%)] Loss: -520855.281250\n",
      "Train Epoch: 38 [45568/54000 (84%)] Loss: -491724.312500\n",
      "    epoch          : 38\n",
      "    loss           : -500195.09498762374\n",
      "    val_loss       : -503001.65\n",
      "Train Epoch: 39 [512/54000 (1%)] Loss: -556592.375000\n",
      "Train Epoch: 39 [11776/54000 (22%)] Loss: -493187.312500\n",
      "Train Epoch: 39 [23040/54000 (43%)] Loss: -526643.875000\n",
      "Train Epoch: 39 [34304/54000 (64%)] Loss: -478788.625000\n",
      "Train Epoch: 39 [45568/54000 (84%)] Loss: -513344.406250\n",
      "    epoch          : 39\n",
      "    loss           : -505655.833539604\n",
      "    val_loss       : -505818.22216796875\n",
      "Train Epoch: 40 [512/54000 (1%)] Loss: -560246.875000\n",
      "Train Epoch: 40 [11776/54000 (22%)] Loss: -480386.812500\n",
      "Train Epoch: 40 [23040/54000 (43%)] Loss: -468689.843750\n",
      "Train Epoch: 40 [34304/54000 (64%)] Loss: -506014.062500\n",
      "Train Epoch: 40 [45568/54000 (84%)] Loss: -506595.281250\n",
      "    epoch          : 40\n",
      "    loss           : -510500.3719059406\n",
      "    val_loss       : -513019.3044921875\n",
      "Train Epoch: 41 [512/54000 (1%)] Loss: -535681.500000\n",
      "Train Epoch: 41 [11776/54000 (22%)] Loss: -535741.750000\n",
      "Train Epoch: 41 [23040/54000 (43%)] Loss: -483877.500000\n",
      "Train Epoch: 41 [34304/54000 (64%)] Loss: -506230.218750\n",
      "Train Epoch: 41 [45568/54000 (84%)] Loss: -501580.250000\n",
      "    epoch          : 41\n",
      "    loss           : -515664.7042079208\n",
      "    val_loss       : -517911.17900390626\n",
      "Train Epoch: 42 [512/54000 (1%)] Loss: -574141.062500\n",
      "Train Epoch: 42 [11776/54000 (22%)] Loss: -521470.062500\n",
      "Train Epoch: 42 [23040/54000 (43%)] Loss: -523653.718750\n",
      "Train Epoch: 42 [34304/54000 (64%)] Loss: -514212.000000\n",
      "Train Epoch: 42 [45568/54000 (84%)] Loss: -511780.437500\n",
      "    epoch          : 42\n",
      "    loss           : -518330.72493811883\n",
      "    val_loss       : -522263.3552734375\n",
      "Train Epoch: 43 [512/54000 (1%)] Loss: -542221.000000\n",
      "Train Epoch: 43 [11776/54000 (22%)] Loss: -516115.312500\n",
      "Train Epoch: 43 [23040/54000 (43%)] Loss: -520671.343750\n",
      "Train Epoch: 43 [34304/54000 (64%)] Loss: -498263.750000\n",
      "Train Epoch: 43 [45568/54000 (84%)] Loss: -515621.937500\n",
      "    epoch          : 43\n",
      "    loss           : -526191.5965346535\n",
      "    val_loss       : -528636.562890625\n",
      "Train Epoch: 44 [512/54000 (1%)] Loss: -585927.437500\n",
      "Train Epoch: 44 [11776/54000 (22%)] Loss: -521069.437500\n",
      "Train Epoch: 44 [23040/54000 (43%)] Loss: -502741.250000\n",
      "Train Epoch: 44 [34304/54000 (64%)] Loss: -517185.843750\n",
      "Train Epoch: 44 [45568/54000 (84%)] Loss: -535920.875000\n",
      "    epoch          : 44\n",
      "    loss           : -530481.1342821782\n",
      "    val_loss       : -531184.303125\n",
      "Train Epoch: 45 [512/54000 (1%)] Loss: -590518.750000\n",
      "Train Epoch: 45 [11776/54000 (22%)] Loss: -522229.218750\n",
      "Train Epoch: 45 [23040/54000 (43%)] Loss: -557685.750000\n",
      "Train Epoch: 45 [34304/54000 (64%)] Loss: -506040.562500\n",
      "Train Epoch: 45 [45568/54000 (84%)] Loss: -531680.375000\n",
      "    epoch          : 45\n",
      "    loss           : -534865.1321163366\n",
      "    val_loss       : -536818.2495117188\n",
      "Train Epoch: 46 [512/54000 (1%)] Loss: -594828.500000\n",
      "Train Epoch: 46 [11776/54000 (22%)] Loss: -557284.437500\n",
      "Train Epoch: 46 [23040/54000 (43%)] Loss: -512512.562500\n",
      "Train Epoch: 46 [34304/54000 (64%)] Loss: -598762.687500\n",
      "Train Epoch: 46 [45568/54000 (84%)] Loss: -528162.375000\n",
      "    epoch          : 46\n",
      "    loss           : -539214.5136138614\n",
      "    val_loss       : -540062.6384765625\n",
      "Train Epoch: 47 [512/54000 (1%)] Loss: -599468.437500\n",
      "Train Epoch: 47 [11776/54000 (22%)] Loss: -557697.000000\n",
      "Train Epoch: 47 [23040/54000 (43%)] Loss: -568369.312500\n",
      "Train Epoch: 47 [34304/54000 (64%)] Loss: -538626.250000\n",
      "Train Epoch: 47 [45568/54000 (84%)] Loss: -545705.312500\n",
      "    epoch          : 47\n",
      "    loss           : -541944.9978341584\n",
      "    val_loss       : -544397.6305664063\n",
      "Train Epoch: 48 [512/54000 (1%)] Loss: -604593.750000\n",
      "Train Epoch: 48 [11776/54000 (22%)] Loss: -513545.593750\n",
      "Train Epoch: 48 [23040/54000 (43%)] Loss: -552237.625000\n",
      "Train Epoch: 48 [34304/54000 (64%)] Loss: -549723.937500\n",
      "Train Epoch: 48 [45568/54000 (84%)] Loss: -534872.750000\n",
      "    epoch          : 48\n",
      "    loss           : -547611.7128712871\n",
      "    val_loss       : -549287.1552734375\n",
      "Train Epoch: 49 [512/54000 (1%)] Loss: -610496.625000\n",
      "Train Epoch: 49 [11776/54000 (22%)] Loss: -555891.125000\n",
      "Train Epoch: 49 [23040/54000 (43%)] Loss: -508987.875000\n",
      "Train Epoch: 49 [34304/54000 (64%)] Loss: -537605.375000\n",
      "Train Epoch: 49 [45568/54000 (84%)] Loss: -548259.750000\n",
      "    epoch          : 49\n",
      "    loss           : -552446.1175742574\n",
      "    val_loss       : -553187.2083007812\n",
      "Train Epoch: 50 [512/54000 (1%)] Loss: -615352.375000\n",
      "Train Epoch: 50 [11776/54000 (22%)] Loss: -542541.812500\n",
      "Train Epoch: 50 [23040/54000 (43%)] Loss: -558513.000000\n",
      "Train Epoch: 50 [34304/54000 (64%)] Loss: -550479.312500\n",
      "Train Epoch: 50 [45568/54000 (84%)] Loss: -556130.437500\n",
      "    epoch          : 50\n",
      "    loss           : -556547.5816831683\n",
      "    val_loss       : -558364.2609375\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0716_225739/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [512/54000 (1%)] Loss: -620878.187500\n",
      "Train Epoch: 51 [11776/54000 (22%)] Loss: -549319.375000\n",
      "Train Epoch: 51 [23040/54000 (43%)] Loss: -515690.062500\n",
      "Train Epoch: 51 [34304/54000 (64%)] Loss: -553036.375000\n",
      "Train Epoch: 51 [45568/54000 (84%)] Loss: -562130.875000\n",
      "    epoch          : 51\n",
      "    loss           : -559506.1432549505\n",
      "    val_loss       : -559560.83359375\n",
      "Train Epoch: 52 [512/54000 (1%)] Loss: -624506.750000\n",
      "Train Epoch: 52 [11776/54000 (22%)] Loss: -586316.437500\n",
      "Train Epoch: 52 [23040/54000 (43%)] Loss: -551311.687500\n",
      "Train Epoch: 52 [34304/54000 (64%)] Loss: -628857.875000\n",
      "Train Epoch: 52 [45568/54000 (84%)] Loss: -570611.375000\n",
      "    epoch          : 52\n",
      "    loss           : -564750.2196782178\n",
      "    val_loss       : -561836.2528320312\n",
      "Train Epoch: 53 [512/54000 (1%)] Loss: -591359.875000\n",
      "Train Epoch: 53 [11776/54000 (22%)] Loss: -548418.687500\n",
      "Train Epoch: 53 [23040/54000 (43%)] Loss: -560509.750000\n",
      "Train Epoch: 53 [34304/54000 (64%)] Loss: -520782.031250\n",
      "Train Epoch: 53 [45568/54000 (84%)] Loss: -563821.625000\n",
      "    epoch          : 53\n",
      "    loss           : -566002.7144183168\n",
      "    val_loss       : -568982.0525390625\n",
      "Train Epoch: 54 [512/54000 (1%)] Loss: -633312.625000\n",
      "Train Epoch: 54 [11776/54000 (22%)] Loss: -559216.687500\n",
      "Train Epoch: 54 [23040/54000 (43%)] Loss: -596899.250000\n",
      "Train Epoch: 54 [34304/54000 (64%)] Loss: -574631.562500\n",
      "Train Epoch: 54 [45568/54000 (84%)] Loss: -569048.875000\n",
      "    epoch          : 54\n",
      "    loss           : -572109.1002475248\n",
      "    val_loss       : -573638.9016601562\n",
      "Train Epoch: 55 [512/54000 (1%)] Loss: -638796.500000\n",
      "Train Epoch: 55 [11776/54000 (22%)] Loss: -563280.125000\n",
      "Train Epoch: 55 [23040/54000 (43%)] Loss: -579410.250000\n",
      "Train Epoch: 55 [34304/54000 (64%)] Loss: -600221.375000\n",
      "Train Epoch: 55 [45568/54000 (84%)] Loss: -571961.812500\n",
      "    epoch          : 55\n",
      "    loss           : -576498.0816831683\n",
      "    val_loss       : -575909.746875\n",
      "Train Epoch: 56 [512/54000 (1%)] Loss: -641919.750000\n",
      "Train Epoch: 56 [11776/54000 (22%)] Loss: -601397.500000\n",
      "Train Epoch: 56 [23040/54000 (43%)] Loss: -532850.000000\n",
      "Train Epoch: 56 [34304/54000 (64%)] Loss: -544741.625000\n",
      "Train Epoch: 56 [45568/54000 (84%)] Loss: -563742.375000\n",
      "    epoch          : 56\n",
      "    loss           : -579650.6101485149\n",
      "    val_loss       : -581069.9303710938\n",
      "Train Epoch: 57 [512/54000 (1%)] Loss: -646586.812500\n",
      "Train Epoch: 57 [11776/54000 (22%)] Loss: -549321.937500\n",
      "Train Epoch: 57 [23040/54000 (43%)] Loss: -537079.500000\n",
      "Train Epoch: 57 [34304/54000 (64%)] Loss: -568609.937500\n",
      "Train Epoch: 57 [45568/54000 (84%)] Loss: -576870.562500\n",
      "    epoch          : 57\n",
      "    loss           : -582936.3558168317\n",
      "    val_loss       : -583580.626171875\n",
      "Train Epoch: 58 [512/54000 (1%)] Loss: -609889.062500\n",
      "Train Epoch: 58 [11776/54000 (22%)] Loss: -575384.875000\n",
      "Train Epoch: 58 [23040/54000 (43%)] Loss: -611023.562500\n",
      "Train Epoch: 58 [34304/54000 (64%)] Loss: -611412.250000\n",
      "Train Epoch: 58 [45568/54000 (84%)] Loss: -569948.250000\n",
      "    epoch          : 58\n",
      "    loss           : -586553.8131188119\n",
      "    val_loss       : -586710.9413085937\n",
      "Train Epoch: 59 [512/54000 (1%)] Loss: -653989.437500\n",
      "Train Epoch: 59 [11776/54000 (22%)] Loss: -611205.312500\n",
      "Train Epoch: 59 [23040/54000 (43%)] Loss: -546714.250000\n",
      "Train Epoch: 59 [34304/54000 (64%)] Loss: -658198.625000\n",
      "Train Epoch: 59 [45568/54000 (84%)] Loss: -572617.000000\n",
      "    epoch          : 59\n",
      "    loss           : -590052.0445544554\n",
      "    val_loss       : -591482.2387695312\n",
      "Train Epoch: 60 [512/54000 (1%)] Loss: -658616.562500\n",
      "Train Epoch: 60 [11776/54000 (22%)] Loss: -616688.187500\n",
      "Train Epoch: 60 [23040/54000 (43%)] Loss: -559843.375000\n",
      "Train Epoch: 60 [34304/54000 (64%)] Loss: -597952.000000\n",
      "Train Epoch: 60 [45568/54000 (84%)] Loss: -596807.937500\n",
      "    epoch          : 60\n",
      "    loss           : -592728.1082920792\n",
      "    val_loss       : -592000.5674804688\n",
      "Train Epoch: 61 [512/54000 (1%)] Loss: -661326.500000\n",
      "Train Epoch: 61 [11776/54000 (22%)] Loss: -580045.000000\n",
      "Train Epoch: 61 [23040/54000 (43%)] Loss: -601124.000000\n",
      "Train Epoch: 61 [34304/54000 (64%)] Loss: -581612.625000\n",
      "Train Epoch: 61 [45568/54000 (84%)] Loss: -593166.250000\n",
      "    epoch          : 61\n",
      "    loss           : -596734.5903465346\n",
      "    val_loss       : -598566.2717773437\n",
      "Train Epoch: 62 [512/54000 (1%)] Loss: -666340.687500\n",
      "Train Epoch: 62 [11776/54000 (22%)] Loss: -587622.750000\n",
      "Train Epoch: 62 [23040/54000 (43%)] Loss: -605223.125000\n",
      "Train Epoch: 62 [34304/54000 (64%)] Loss: -584835.687500\n",
      "Train Epoch: 62 [45568/54000 (84%)] Loss: -597277.437500\n",
      "    epoch          : 62\n",
      "    loss           : -601073.8818069306\n",
      "    val_loss       : -600937.40234375\n",
      "Train Epoch: 63 [512/54000 (1%)] Loss: -669327.125000\n",
      "Train Epoch: 63 [11776/54000 (22%)] Loss: -569578.687500\n",
      "Train Epoch: 63 [23040/54000 (43%)] Loss: -565815.187500\n",
      "Train Epoch: 63 [34304/54000 (64%)] Loss: -673698.437500\n",
      "Train Epoch: 63 [45568/54000 (84%)] Loss: -608110.187500\n",
      "    epoch          : 63\n",
      "    loss           : -604184.6392326732\n",
      "    val_loss       : -603608.9865234375\n",
      "Train Epoch: 64 [512/54000 (1%)] Loss: -673782.125000\n",
      "Train Epoch: 64 [11776/54000 (22%)] Loss: -623724.000000\n",
      "Train Epoch: 64 [23040/54000 (43%)] Loss: -589149.812500\n",
      "Train Epoch: 64 [34304/54000 (64%)] Loss: -571979.625000\n",
      "Train Epoch: 64 [45568/54000 (84%)] Loss: -559545.062500\n",
      "    epoch          : 64\n",
      "    loss           : -606319.4381188119\n",
      "    val_loss       : -607035.8559570312\n",
      "Train Epoch: 65 [512/54000 (1%)] Loss: -673918.312500\n",
      "Train Epoch: 65 [11776/54000 (22%)] Loss: -571597.437500\n",
      "Train Epoch: 65 [23040/54000 (43%)] Loss: -601161.125000\n",
      "Train Epoch: 65 [34304/54000 (64%)] Loss: -593346.625000\n",
      "Train Epoch: 65 [45568/54000 (84%)] Loss: -606715.000000\n",
      "    epoch          : 65\n",
      "    loss           : -609554.4554455446\n",
      "    val_loss       : -610158.6078125\n",
      "Train Epoch: 66 [512/54000 (1%)] Loss: -680213.812500\n",
      "Train Epoch: 66 [11776/54000 (22%)] Loss: -567850.437500\n",
      "Train Epoch: 66 [23040/54000 (43%)] Loss: -571440.562500\n",
      "Train Epoch: 66 [34304/54000 (64%)] Loss: -609743.937500\n",
      "Train Epoch: 66 [45568/54000 (84%)] Loss: -618809.500000\n",
      "    epoch          : 66\n",
      "    loss           : -613388.2419554455\n",
      "    val_loss       : -611049.269921875\n",
      "Train Epoch: 67 [512/54000 (1%)] Loss: -681848.312500\n",
      "Train Epoch: 67 [11776/54000 (22%)] Loss: -593306.062500\n",
      "Train Epoch: 67 [23040/54000 (43%)] Loss: -622134.812500\n",
      "Train Epoch: 67 [34304/54000 (64%)] Loss: -582613.625000\n",
      "Train Epoch: 67 [45568/54000 (84%)] Loss: -600881.937500\n",
      "    epoch          : 67\n",
      "    loss           : -615641.4486386139\n",
      "    val_loss       : -617260.0422851562\n",
      "Train Epoch: 68 [512/54000 (1%)] Loss: -689270.500000\n",
      "Train Epoch: 68 [11776/54000 (22%)] Loss: -641102.000000\n",
      "Train Epoch: 68 [23040/54000 (43%)] Loss: -602789.000000\n",
      "Train Epoch: 68 [34304/54000 (64%)] Loss: -690539.125000\n",
      "Train Epoch: 68 [45568/54000 (84%)] Loss: -616603.250000\n",
      "    epoch          : 68\n",
      "    loss           : -620132.844059406\n",
      "    val_loss       : -620634.7724609375\n",
      "Train Epoch: 69 [512/54000 (1%)] Loss: -692409.625000\n",
      "Train Epoch: 69 [11776/54000 (22%)] Loss: -607279.062500\n",
      "Train Epoch: 69 [23040/54000 (43%)] Loss: -620511.500000\n",
      "Train Epoch: 69 [34304/54000 (64%)] Loss: -692371.250000\n",
      "Train Epoch: 69 [45568/54000 (84%)] Loss: -601898.312500\n",
      "    epoch          : 69\n",
      "    loss           : -621623.7227722772\n",
      "    val_loss       : -620064.0395507812\n",
      "Train Epoch: 70 [512/54000 (1%)] Loss: -694704.437500\n",
      "Train Epoch: 70 [11776/54000 (22%)] Loss: -649328.125000\n",
      "Train Epoch: 70 [23040/54000 (43%)] Loss: -606443.875000\n",
      "Train Epoch: 70 [34304/54000 (64%)] Loss: -606782.625000\n",
      "Train Epoch: 70 [45568/54000 (84%)] Loss: -602801.500000\n",
      "    epoch          : 70\n",
      "    loss           : -625077.7945544554\n",
      "    val_loss       : -623575.5387695313\n",
      "Train Epoch: 71 [512/54000 (1%)] Loss: -697188.500000\n",
      "Train Epoch: 71 [11776/54000 (22%)] Loss: -609711.000000\n",
      "Train Epoch: 71 [23040/54000 (43%)] Loss: -651252.000000\n",
      "Train Epoch: 71 [34304/54000 (64%)] Loss: -701367.000000\n",
      "Train Epoch: 71 [45568/54000 (84%)] Loss: -635498.437500\n",
      "    epoch          : 71\n",
      "    loss           : -628726.6553217822\n",
      "    val_loss       : -629725.732421875\n",
      "Train Epoch: 72 [512/54000 (1%)] Loss: -658975.125000\n",
      "Train Epoch: 72 [11776/54000 (22%)] Loss: -594697.500000\n",
      "Train Epoch: 72 [23040/54000 (43%)] Loss: -613744.187500\n",
      "Train Epoch: 72 [34304/54000 (64%)] Loss: -614342.750000\n",
      "Train Epoch: 72 [45568/54000 (84%)] Loss: -611650.812500\n",
      "    epoch          : 72\n",
      "    loss           : -631738.6157178218\n",
      "    val_loss       : -631039.2071289063\n",
      "Train Epoch: 73 [512/54000 (1%)] Loss: -704966.750000\n",
      "Train Epoch: 73 [11776/54000 (22%)] Loss: -616810.687500\n",
      "Train Epoch: 73 [23040/54000 (43%)] Loss: -613407.500000\n",
      "Train Epoch: 73 [34304/54000 (64%)] Loss: -613043.000000\n",
      "Train Epoch: 73 [45568/54000 (84%)] Loss: -631727.562500\n",
      "    epoch          : 73\n",
      "    loss           : -632563.4554455446\n",
      "    val_loss       : -632663.6430664062\n",
      "Train Epoch: 74 [512/54000 (1%)] Loss: -705274.062500\n",
      "Train Epoch: 74 [11776/54000 (22%)] Loss: -615765.812500\n",
      "Train Epoch: 74 [23040/54000 (43%)] Loss: -616007.250000\n",
      "Train Epoch: 74 [34304/54000 (64%)] Loss: -589444.937500\n",
      "Train Epoch: 74 [45568/54000 (84%)] Loss: -641511.125000\n",
      "    epoch          : 74\n",
      "    loss           : -635881.1033415842\n",
      "    val_loss       : -637921.057421875\n",
      "Train Epoch: 75 [512/54000 (1%)] Loss: -713121.125000\n",
      "Train Epoch: 75 [11776/54000 (22%)] Loss: -623356.812500\n",
      "Train Epoch: 75 [23040/54000 (43%)] Loss: -666479.437500\n",
      "Train Epoch: 75 [34304/54000 (64%)] Loss: -663962.750000\n",
      "Train Epoch: 75 [45568/54000 (84%)] Loss: -639844.625000\n",
      "    epoch          : 75\n",
      "    loss           : -639350.1008663366\n",
      "    val_loss       : -637559.310546875\n",
      "Train Epoch: 76 [512/54000 (1%)] Loss: -622216.812500\n",
      "Train Epoch: 76 [11776/54000 (22%)] Loss: -624980.687500\n",
      "Train Epoch: 76 [23040/54000 (43%)] Loss: -662776.687500\n",
      "Train Epoch: 76 [34304/54000 (64%)] Loss: -718674.125000\n",
      "Train Epoch: 76 [45568/54000 (84%)] Loss: -598565.125000\n",
      "    epoch          : 76\n",
      "    loss           : -641938.958539604\n",
      "    val_loss       : -642365.8573242187\n",
      "Train Epoch: 77 [512/54000 (1%)] Loss: -671797.375000\n",
      "Train Epoch: 77 [11776/54000 (22%)] Loss: -624970.000000\n",
      "Train Epoch: 77 [23040/54000 (43%)] Loss: -603182.687500\n",
      "Train Epoch: 77 [34304/54000 (64%)] Loss: -622771.250000\n",
      "Train Epoch: 77 [45568/54000 (84%)] Loss: -642518.062500\n",
      "    epoch          : 77\n",
      "    loss           : -644963.1899752475\n",
      "    val_loss       : -642941.8553710937\n",
      "Train Epoch: 78 [512/54000 (1%)] Loss: -627797.125000\n",
      "Train Epoch: 78 [11776/54000 (22%)] Loss: -625569.625000\n",
      "Train Epoch: 78 [23040/54000 (43%)] Loss: -605651.687500\n",
      "Train Epoch: 78 [34304/54000 (64%)] Loss: -722917.937500\n",
      "Train Epoch: 78 [45568/54000 (84%)] Loss: -645429.312500\n",
      "    epoch          : 78\n",
      "    loss           : -646501.6206683168\n",
      "    val_loss       : -648080.48046875\n",
      "Train Epoch: 79 [512/54000 (1%)] Loss: -676845.250000\n",
      "Train Epoch: 79 [11776/54000 (22%)] Loss: -633591.125000\n",
      "Train Epoch: 79 [23040/54000 (43%)] Loss: -654453.875000\n",
      "Train Epoch: 79 [34304/54000 (64%)] Loss: -626056.750000\n",
      "Train Epoch: 79 [45568/54000 (84%)] Loss: -629375.187500\n",
      "    epoch          : 79\n",
      "    loss           : -650288.5810643565\n",
      "    val_loss       : -649677.3458984375\n",
      "Train Epoch: 80 [512/54000 (1%)] Loss: -727053.125000\n",
      "Train Epoch: 80 [11776/54000 (22%)] Loss: -679347.125000\n",
      "Train Epoch: 80 [23040/54000 (43%)] Loss: -608509.250000\n",
      "Train Epoch: 80 [34304/54000 (64%)] Loss: -729463.625000\n",
      "Train Epoch: 80 [45568/54000 (84%)] Loss: -659567.312500\n",
      "    epoch          : 80\n",
      "    loss           : -652048.406559406\n",
      "    val_loss       : -653882.0958984375\n",
      "Train Epoch: 81 [512/54000 (1%)] Loss: -731711.437500\n",
      "Train Epoch: 81 [11776/54000 (22%)] Loss: -679671.125000\n",
      "Train Epoch: 81 [23040/54000 (43%)] Loss: -682488.187500\n",
      "Train Epoch: 81 [34304/54000 (64%)] Loss: -608542.875000\n",
      "Train Epoch: 81 [45568/54000 (84%)] Loss: -649217.000000\n",
      "    epoch          : 81\n",
      "    loss           : -655639.5594059406\n",
      "    val_loss       : -653750.2047851563\n",
      "Train Epoch: 82 [512/54000 (1%)] Loss: -637241.187500\n",
      "Train Epoch: 82 [11776/54000 (22%)] Loss: -649072.562500\n",
      "Train Epoch: 82 [23040/54000 (43%)] Loss: -617086.437500\n",
      "Train Epoch: 82 [34304/54000 (64%)] Loss: -659936.625000\n",
      "Train Epoch: 82 [45568/54000 (84%)] Loss: -606384.500000\n",
      "    epoch          : 82\n",
      "    loss           : -656203.1806930694\n",
      "    val_loss       : -656051.0704101563\n",
      "Train Epoch: 83 [512/54000 (1%)] Loss: -734581.125000\n",
      "Train Epoch: 83 [11776/54000 (22%)] Loss: -685386.750000\n",
      "Train Epoch: 83 [23040/54000 (43%)] Loss: -617442.937500\n",
      "Train Epoch: 83 [34304/54000 (64%)] Loss: -737681.187500\n",
      "Train Epoch: 83 [45568/54000 (84%)] Loss: -665929.875000\n",
      "    epoch          : 83\n",
      "    loss           : -660496.5655940594\n",
      "    val_loss       : -661032.4318359375\n",
      "Train Epoch: 84 [512/54000 (1%)] Loss: -738498.375000\n",
      "Train Epoch: 84 [11776/54000 (22%)] Loss: -624205.312500\n",
      "Train Epoch: 84 [23040/54000 (43%)] Loss: -689371.375000\n",
      "Train Epoch: 84 [34304/54000 (64%)] Loss: -739897.750000\n",
      "Train Epoch: 84 [45568/54000 (84%)] Loss: -624176.437500\n",
      "    epoch          : 84\n",
      "    loss           : -662773.3626237623\n",
      "    val_loss       : -661394.1724609375\n",
      "Train Epoch: 85 [512/54000 (1%)] Loss: -738195.812500\n",
      "Train Epoch: 85 [11776/54000 (22%)] Loss: -689970.625000\n",
      "Train Epoch: 85 [23040/54000 (43%)] Loss: -690820.937500\n",
      "Train Epoch: 85 [34304/54000 (64%)] Loss: -646595.375000\n",
      "Train Epoch: 85 [45568/54000 (84%)] Loss: -662936.375000\n",
      "    epoch          : 85\n",
      "    loss           : -665752.7543316832\n",
      "    val_loss       : -664962.1377929688\n",
      "Train Epoch: 86 [512/54000 (1%)] Loss: -744590.625000\n",
      "Train Epoch: 86 [11776/54000 (22%)] Loss: -690580.937500\n",
      "Train Epoch: 86 [23040/54000 (43%)] Loss: -612368.937500\n",
      "Train Epoch: 86 [34304/54000 (64%)] Loss: -668426.375000\n",
      "Train Epoch: 86 [45568/54000 (84%)] Loss: -672108.937500\n",
      "    epoch          : 86\n",
      "    loss           : -665272.1299504951\n",
      "    val_loss       : -666795.2310546875\n",
      "Train Epoch: 87 [512/54000 (1%)] Loss: -746456.812500\n",
      "Train Epoch: 87 [11776/54000 (22%)] Loss: -697873.750000\n",
      "Train Epoch: 87 [23040/54000 (43%)] Loss: -694807.000000\n",
      "Train Epoch: 87 [34304/54000 (64%)] Loss: -629392.250000\n",
      "Train Epoch: 87 [45568/54000 (84%)] Loss: -651528.750000\n",
      "    epoch          : 87\n",
      "    loss           : -670597.0600247525\n",
      "    val_loss       : -670486.4736328125\n",
      "Train Epoch: 88 [512/54000 (1%)] Loss: -697906.437500\n",
      "Train Epoch: 88 [11776/54000 (22%)] Loss: -698890.375000\n",
      "Train Epoch: 88 [23040/54000 (43%)] Loss: -632473.937500\n",
      "Train Epoch: 88 [34304/54000 (64%)] Loss: -751842.250000\n",
      "Train Epoch: 88 [45568/54000 (84%)] Loss: -679074.250000\n",
      "    epoch          : 88\n",
      "    loss           : -673042.9300742574\n",
      "    val_loss       : -669035.6387695313\n",
      "Train Epoch: 89 [512/54000 (1%)] Loss: -752102.125000\n",
      "Train Epoch: 89 [11776/54000 (22%)] Loss: -702908.375000\n",
      "Train Epoch: 89 [23040/54000 (43%)] Loss: -633903.937500\n",
      "Train Epoch: 89 [34304/54000 (64%)] Loss: -650144.750000\n",
      "Train Epoch: 89 [45568/54000 (84%)] Loss: -671820.625000\n",
      "    epoch          : 89\n",
      "    loss           : -673734.4238861386\n",
      "    val_loss       : -675273.8260742187\n",
      "Train Epoch: 90 [512/54000 (1%)] Loss: -754996.562500\n",
      "Train Epoch: 90 [11776/54000 (22%)] Loss: -655735.125000\n",
      "Train Epoch: 90 [23040/54000 (43%)] Loss: -635104.625000\n",
      "Train Epoch: 90 [34304/54000 (64%)] Loss: -633103.687500\n",
      "Train Epoch: 90 [45568/54000 (84%)] Loss: -681195.375000\n",
      "    epoch          : 90\n",
      "    loss           : -676732.8551980198\n",
      "    val_loss       : -675029.8024414063\n",
      "Train Epoch: 91 [512/54000 (1%)] Loss: -642378.875000\n",
      "Train Epoch: 91 [11776/54000 (22%)] Loss: -704958.375000\n",
      "Train Epoch: 91 [23040/54000 (43%)] Loss: -658740.375000\n",
      "Train Epoch: 91 [34304/54000 (64%)] Loss: -657873.812500\n",
      "Train Epoch: 91 [45568/54000 (84%)] Loss: -675393.687500\n",
      "    epoch          : 91\n",
      "    loss           : -678070.5297029703\n",
      "    val_loss       : -678575.178125\n",
      "Train Epoch: 92 [512/54000 (1%)] Loss: -760813.375000\n",
      "Train Epoch: 92 [11776/54000 (22%)] Loss: -660529.625000\n",
      "Train Epoch: 92 [23040/54000 (43%)] Loss: -681535.250000\n",
      "Train Epoch: 92 [34304/54000 (64%)] Loss: -705223.187500\n",
      "Train Epoch: 92 [45568/54000 (84%)] Loss: -681865.625000\n",
      "    epoch          : 92\n",
      "    loss           : -681537.5334158416\n",
      "    val_loss       : -681087.4571289063\n",
      "Train Epoch: 93 [512/54000 (1%)] Loss: -760590.875000\n",
      "Train Epoch: 93 [11776/54000 (22%)] Loss: -710275.625000\n",
      "Train Epoch: 93 [23040/54000 (43%)] Loss: -659506.125000\n",
      "Train Epoch: 93 [34304/54000 (64%)] Loss: -636411.500000\n",
      "Train Epoch: 93 [45568/54000 (84%)] Loss: -678950.062500\n",
      "    epoch          : 93\n",
      "    loss           : -682867.2722772277\n",
      "    val_loss       : -681968.7228515625\n",
      "Train Epoch: 94 [512/54000 (1%)] Loss: -764064.187500\n",
      "Train Epoch: 94 [11776/54000 (22%)] Loss: -715393.437500\n",
      "Train Epoch: 94 [23040/54000 (43%)] Loss: -674408.187500\n",
      "Train Epoch: 94 [34304/54000 (64%)] Loss: -636459.125000\n",
      "Train Epoch: 94 [45568/54000 (84%)] Loss: -660473.875000\n",
      "    epoch          : 94\n",
      "    loss           : -683051.9882425743\n",
      "    val_loss       : -683770.4327148438\n",
      "Train Epoch: 95 [512/54000 (1%)] Loss: -767476.937500\n",
      "Train Epoch: 95 [11776/54000 (22%)] Loss: -666989.250000\n",
      "Train Epoch: 95 [23040/54000 (43%)] Loss: -645988.625000\n",
      "Train Epoch: 95 [34304/54000 (64%)] Loss: -646948.000000\n",
      "Train Epoch: 95 [45568/54000 (84%)] Loss: -689713.812500\n",
      "    epoch          : 95\n",
      "    loss           : -688513.4232673268\n",
      "    val_loss       : -687162.2751953125\n",
      "Train Epoch: 96 [512/54000 (1%)] Loss: -771699.875000\n",
      "Train Epoch: 96 [11776/54000 (22%)] Loss: -638752.500000\n",
      "Train Epoch: 96 [23040/54000 (43%)] Loss: -642146.875000\n",
      "Train Epoch: 96 [34304/54000 (64%)] Loss: -647433.125000\n",
      "Train Epoch: 96 [45568/54000 (84%)] Loss: -664006.875000\n",
      "    epoch          : 96\n",
      "    loss           : -689494.5693069306\n",
      "    val_loss       : -688120.7677734375\n",
      "Train Epoch: 97 [512/54000 (1%)] Loss: -772490.125000\n",
      "Train Epoch: 97 [11776/54000 (22%)] Loss: -673105.500000\n",
      "Train Epoch: 97 [23040/54000 (43%)] Loss: -650250.625000\n",
      "Train Epoch: 97 [34304/54000 (64%)] Loss: -653273.187500\n",
      "Train Epoch: 97 [45568/54000 (84%)] Loss: -690204.562500\n",
      "    epoch          : 97\n",
      "    loss           : -692013.5705445545\n",
      "    val_loss       : -691510.5826171875\n",
      "Train Epoch: 98 [512/54000 (1%)] Loss: -776197.000000\n",
      "Train Epoch: 98 [11776/54000 (22%)] Loss: -671187.000000\n",
      "Train Epoch: 98 [23040/54000 (43%)] Loss: -650096.000000\n",
      "Train Epoch: 98 [34304/54000 (64%)] Loss: -652735.875000\n",
      "Train Epoch: 98 [45568/54000 (84%)] Loss: -649634.875000\n",
      "    epoch          : 98\n",
      "    loss           : -693435.3409653465\n",
      "    val_loss       : -692547.2463867187\n",
      "Train Epoch: 99 [512/54000 (1%)] Loss: -778501.187500\n",
      "Train Epoch: 99 [11776/54000 (22%)] Loss: -722505.937500\n",
      "Train Epoch: 99 [23040/54000 (43%)] Loss: -717351.000000\n",
      "Train Epoch: 99 [34304/54000 (64%)] Loss: -699131.125000\n",
      "Train Epoch: 99 [45568/54000 (84%)] Loss: -669166.875000\n",
      "    epoch          : 99\n",
      "    loss           : -695903.2747524752\n",
      "    val_loss       : -696103.463671875\n",
      "Train Epoch: 100 [512/54000 (1%)] Loss: -780052.437500\n",
      "Train Epoch: 100 [11776/54000 (22%)] Loss: -721814.125000\n",
      "Train Epoch: 100 [23040/54000 (43%)] Loss: -670169.687500\n",
      "Train Epoch: 100 [34304/54000 (64%)] Loss: -673664.375000\n",
      "Train Epoch: 100 [45568/54000 (84%)] Loss: -692455.500000\n",
      "    epoch          : 100\n",
      "    loss           : -697202.0470297029\n",
      "    val_loss       : -696849.00625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0716_225739/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 101 [512/54000 (1%)] Loss: -781986.937500\n",
      "Train Epoch: 101 [11776/54000 (22%)] Loss: -679514.000000\n",
      "Train Epoch: 101 [23040/54000 (43%)] Loss: -650147.062500\n",
      "Train Epoch: 101 [34304/54000 (64%)] Loss: -668680.625000\n",
      "Train Epoch: 101 [45568/54000 (84%)] Loss: -691863.250000\n",
      "    epoch          : 101\n",
      "    loss           : -698124.1237623763\n",
      "    val_loss       : -696735.0688476562\n",
      "Train Epoch: 102 [512/54000 (1%)] Loss: -784968.187500\n",
      "Train Epoch: 102 [11776/54000 (22%)] Loss: -724395.625000\n",
      "Train Epoch: 102 [23040/54000 (43%)] Loss: -651903.125000\n",
      "Train Epoch: 102 [34304/54000 (64%)] Loss: -782639.875000\n",
      "Train Epoch: 102 [45568/54000 (84%)] Loss: -701117.000000\n",
      "    epoch          : 102\n",
      "    loss           : -699230.1256188119\n",
      "    val_loss       : -701658.1484375\n",
      "Train Epoch: 103 [512/54000 (1%)] Loss: -788358.500000\n",
      "Train Epoch: 103 [11776/54000 (22%)] Loss: -686503.750000\n",
      "Train Epoch: 103 [23040/54000 (43%)] Loss: -706829.562500\n",
      "Train Epoch: 103 [34304/54000 (64%)] Loss: -788013.437500\n",
      "Train Epoch: 103 [45568/54000 (84%)] Loss: -709950.687500\n",
      "    epoch          : 103\n",
      "    loss           : -705136.1404702971\n",
      "    val_loss       : -703112.1517578125\n",
      "Train Epoch: 104 [512/54000 (1%)] Loss: -790330.375000\n",
      "Train Epoch: 104 [11776/54000 (22%)] Loss: -729819.062500\n",
      "Train Epoch: 104 [23040/54000 (43%)] Loss: -710305.687500\n",
      "Train Epoch: 104 [34304/54000 (64%)] Loss: -663748.062500\n",
      "Train Epoch: 104 [45568/54000 (84%)] Loss: -661501.000000\n",
      "    epoch          : 104\n",
      "    loss           : -706347.6237623763\n",
      "    val_loss       : -705507.2232421875\n",
      "Train Epoch: 105 [512/54000 (1%)] Loss: -793358.500000\n",
      "Train Epoch: 105 [11776/54000 (22%)] Loss: -687046.000000\n",
      "Train Epoch: 105 [23040/54000 (43%)] Loss: -666759.000000\n",
      "Train Epoch: 105 [34304/54000 (64%)] Loss: -664984.937500\n",
      "Train Epoch: 105 [45568/54000 (84%)] Loss: -679900.000000\n",
      "    epoch          : 105\n",
      "    loss           : -708724.9517326732\n",
      "    val_loss       : -706861.8858398438\n",
      "Train Epoch: 106 [512/54000 (1%)] Loss: -792109.187500\n",
      "Train Epoch: 106 [11776/54000 (22%)] Loss: -733222.687500\n",
      "Train Epoch: 106 [23040/54000 (43%)] Loss: -734322.375000\n",
      "Train Epoch: 106 [34304/54000 (64%)] Loss: -667010.250000\n",
      "Train Epoch: 106 [45568/54000 (84%)] Loss: -714851.250000\n",
      "    epoch          : 106\n",
      "    loss           : -708832.5705445545\n",
      "    val_loss       : -707567.2361328125\n",
      "Train Epoch: 107 [512/54000 (1%)] Loss: -796275.687500\n",
      "Train Epoch: 107 [11776/54000 (22%)] Loss: -691210.625000\n",
      "Train Epoch: 107 [23040/54000 (43%)] Loss: -671710.625000\n",
      "Train Epoch: 107 [34304/54000 (64%)] Loss: -715101.750000\n",
      "Train Epoch: 107 [45568/54000 (84%)] Loss: -686570.625000\n",
      "    epoch          : 107\n",
      "    loss           : -711348.1243811881\n",
      "    val_loss       : -710443.6291992187\n",
      "Train Epoch: 108 [512/54000 (1%)] Loss: -741192.562500\n",
      "Train Epoch: 108 [11776/54000 (22%)] Loss: -691388.562500\n",
      "Train Epoch: 108 [23040/54000 (43%)] Loss: -671053.750000\n",
      "Train Epoch: 108 [34304/54000 (64%)] Loss: -670195.750000\n",
      "Train Epoch: 108 [45568/54000 (84%)] Loss: -684663.312500\n",
      "    epoch          : 108\n",
      "    loss           : -713250.1967821782\n",
      "    val_loss       : -711571.1954101563\n",
      "Train Epoch: 109 [512/54000 (1%)] Loss: -746083.125000\n",
      "Train Epoch: 109 [11776/54000 (22%)] Loss: -688721.062500\n",
      "Train Epoch: 109 [23040/54000 (43%)] Loss: -675628.625000\n",
      "Train Epoch: 109 [34304/54000 (64%)] Loss: -717340.312500\n",
      "Train Epoch: 109 [45568/54000 (84%)] Loss: -690047.125000\n",
      "    epoch          : 109\n",
      "    loss           : -714709.5996287129\n",
      "    val_loss       : -713652.7327148437\n",
      "Train Epoch: 110 [512/54000 (1%)] Loss: -801612.062500\n",
      "Train Epoch: 110 [11776/54000 (22%)] Loss: -743867.625000\n",
      "Train Epoch: 110 [23040/54000 (43%)] Loss: -671812.625000\n",
      "Train Epoch: 110 [34304/54000 (64%)] Loss: -804771.625000\n",
      "Train Epoch: 110 [45568/54000 (84%)] Loss: -712946.312500\n",
      "    epoch          : 110\n",
      "    loss           : -716694.2599009901\n",
      "    val_loss       : -715470.4296875\n",
      "Train Epoch: 111 [512/54000 (1%)] Loss: -804411.187500\n",
      "Train Epoch: 111 [11776/54000 (22%)] Loss: -671505.125000\n",
      "Train Epoch: 111 [23040/54000 (43%)] Loss: -722125.812500\n",
      "Train Epoch: 111 [34304/54000 (64%)] Loss: -804919.625000\n",
      "Train Epoch: 111 [45568/54000 (84%)] Loss: -741857.687500\n",
      "    epoch          : 111\n",
      "    loss           : -718324.4201732674\n",
      "    val_loss       : -715305.4231445312\n",
      "Train Epoch: 112 [512/54000 (1%)] Loss: -804647.062500\n",
      "Train Epoch: 112 [11776/54000 (22%)] Loss: -698488.125000\n",
      "Train Epoch: 112 [23040/54000 (43%)] Loss: -670246.812500\n",
      "Train Epoch: 112 [34304/54000 (64%)] Loss: -806504.125000\n",
      "Train Epoch: 112 [45568/54000 (84%)] Loss: -726204.125000\n",
      "    epoch          : 112\n",
      "    loss           : -718991.1596534654\n",
      "    val_loss       : -718661.9635742188\n",
      "Train Epoch: 113 [512/54000 (1%)] Loss: -806713.000000\n",
      "Train Epoch: 113 [11776/54000 (22%)] Loss: -692326.000000\n",
      "Train Epoch: 113 [23040/54000 (43%)] Loss: -674575.750000\n",
      "Train Epoch: 113 [34304/54000 (64%)] Loss: -696785.062500\n",
      "Train Epoch: 113 [45568/54000 (84%)] Loss: -698617.375000\n",
      "    epoch          : 113\n",
      "    loss           : -721850.833539604\n",
      "    val_loss       : -720777.1665039062\n",
      "Train Epoch: 114 [512/54000 (1%)] Loss: -809753.000000\n",
      "Train Epoch: 114 [11776/54000 (22%)] Loss: -749025.062500\n",
      "Train Epoch: 114 [23040/54000 (43%)] Loss: -728596.500000\n",
      "Train Epoch: 114 [34304/54000 (64%)] Loss: -811480.437500\n",
      "Train Epoch: 114 [45568/54000 (84%)] Loss: -724771.500000\n",
      "    epoch          : 114\n",
      "    loss           : -724333.2419554455\n",
      "    val_loss       : -723173.4073242188\n",
      "Train Epoch: 115 [512/54000 (1%)] Loss: -813182.312500\n",
      "Train Epoch: 115 [11776/54000 (22%)] Loss: -749199.687500\n",
      "Train Epoch: 115 [23040/54000 (43%)] Loss: -684805.375000\n",
      "Train Epoch: 115 [34304/54000 (64%)] Loss: -697686.250000\n",
      "Train Epoch: 115 [45568/54000 (84%)] Loss: -729400.062500\n",
      "    epoch          : 115\n",
      "    loss           : -725882.2363861386\n",
      "    val_loss       : -723306.1509765625\n",
      "Train Epoch: 116 [512/54000 (1%)] Loss: -701658.812500\n",
      "Train Epoch: 116 [11776/54000 (22%)] Loss: -703467.000000\n",
      "Train Epoch: 116 [23040/54000 (43%)] Loss: -753789.375000\n",
      "Train Epoch: 116 [34304/54000 (64%)] Loss: -681258.125000\n",
      "Train Epoch: 116 [45568/54000 (84%)] Loss: -724928.875000\n",
      "    epoch          : 116\n",
      "    loss           : -726727.6745049505\n",
      "    val_loss       : -726224.7375\n",
      "Train Epoch: 117 [512/54000 (1%)] Loss: -816494.000000\n",
      "Train Epoch: 117 [11776/54000 (22%)] Loss: -706271.562500\n",
      "Train Epoch: 117 [23040/54000 (43%)] Loss: -684930.750000\n",
      "Train Epoch: 117 [34304/54000 (64%)] Loss: -748476.875000\n",
      "Train Epoch: 117 [45568/54000 (84%)] Loss: -686032.687500\n",
      "    epoch          : 117\n",
      "    loss           : -728328.9832920792\n",
      "    val_loss       : -725158.408984375\n",
      "Train Epoch: 118 [512/54000 (1%)] Loss: -819168.500000\n",
      "Train Epoch: 118 [11776/54000 (22%)] Loss: -699295.625000\n",
      "Train Epoch: 118 [23040/54000 (43%)] Loss: -725517.812500\n",
      "Train Epoch: 118 [34304/54000 (64%)] Loss: -699828.750000\n",
      "Train Epoch: 118 [45568/54000 (84%)] Loss: -682800.750000\n",
      "    epoch          : 118\n",
      "    loss           : -728024.4925742574\n",
      "    val_loss       : -727716.2529296875\n",
      "Train Epoch: 119 [512/54000 (1%)] Loss: -762157.125000\n",
      "Train Epoch: 119 [11776/54000 (22%)] Loss: -681236.250000\n",
      "Train Epoch: 119 [23040/54000 (43%)] Loss: -702326.625000\n",
      "Train Epoch: 119 [34304/54000 (64%)] Loss: -731270.625000\n",
      "Train Epoch: 119 [45568/54000 (84%)] Loss: -735873.312500\n",
      "    epoch          : 119\n",
      "    loss           : -729752.4226485149\n",
      "    val_loss       : -730253.9245117188\n",
      "Train Epoch: 120 [512/54000 (1%)] Loss: -823301.312500\n",
      "Train Epoch: 120 [11776/54000 (22%)] Loss: -707617.437500\n",
      "Train Epoch: 120 [23040/54000 (43%)] Loss: -762367.750000\n",
      "Train Epoch: 120 [34304/54000 (64%)] Loss: -762311.687500\n",
      "Train Epoch: 120 [45568/54000 (84%)] Loss: -740400.187500\n",
      "    epoch          : 120\n",
      "    loss           : -733862.5649752475\n",
      "    val_loss       : -732149.3905273437\n",
      "Train Epoch: 121 [512/54000 (1%)] Loss: -823271.687500\n",
      "Train Epoch: 121 [11776/54000 (22%)] Loss: -711441.625000\n",
      "Train Epoch: 121 [23040/54000 (43%)] Loss: -761227.750000\n",
      "Train Epoch: 121 [34304/54000 (64%)] Loss: -693542.625000\n",
      "Train Epoch: 121 [45568/54000 (84%)] Loss: -742133.000000\n",
      "    epoch          : 121\n",
      "    loss           : -735572.8824257426\n",
      "    val_loss       : -732743.494921875\n",
      "Train Epoch: 122 [512/54000 (1%)] Loss: -825611.000000\n",
      "Train Epoch: 122 [11776/54000 (22%)] Loss: -686094.000000\n",
      "Train Epoch: 122 [23040/54000 (43%)] Loss: -691142.187500\n",
      "Train Epoch: 122 [34304/54000 (64%)] Loss: -695268.375000\n",
      "Train Epoch: 122 [45568/54000 (84%)] Loss: -747860.875000\n",
      "    epoch          : 122\n",
      "    loss           : -737131.8131188119\n",
      "    val_loss       : -735932.8861328125\n",
      "Train Epoch: 123 [512/54000 (1%)] Loss: -827948.250000\n",
      "Train Epoch: 123 [11776/54000 (22%)] Loss: -716503.750000\n",
      "Train Epoch: 123 [23040/54000 (43%)] Loss: -761724.937500\n",
      "Train Epoch: 123 [34304/54000 (64%)] Loss: -738830.187500\n",
      "Train Epoch: 123 [45568/54000 (84%)] Loss: -743183.187500\n",
      "    epoch          : 123\n",
      "    loss           : -737880.0897277228\n",
      "    val_loss       : -736753.069921875\n",
      "Train Epoch: 124 [512/54000 (1%)] Loss: -829017.937500\n",
      "Train Epoch: 124 [11776/54000 (22%)] Loss: -720348.250000\n",
      "Train Epoch: 124 [23040/54000 (43%)] Loss: -763316.500000\n",
      "Train Epoch: 124 [34304/54000 (64%)] Loss: -698821.062500\n",
      "Train Epoch: 124 [45568/54000 (84%)] Loss: -743919.562500\n",
      "    epoch          : 124\n",
      "    loss           : -741207.3180693069\n",
      "    val_loss       : -739009.3897460938\n",
      "Train Epoch: 125 [512/54000 (1%)] Loss: -771886.062500\n",
      "Train Epoch: 125 [11776/54000 (22%)] Loss: -716910.250000\n",
      "Train Epoch: 125 [23040/54000 (43%)] Loss: -736881.250000\n",
      "Train Epoch: 125 [34304/54000 (64%)] Loss: -832307.437500\n",
      "Train Epoch: 125 [45568/54000 (84%)] Loss: -739093.625000\n",
      "    epoch          : 125\n",
      "    loss           : -740216.5767326732\n",
      "    val_loss       : -736348.4961914063\n",
      "Train Epoch: 126 [512/54000 (1%)] Loss: -831111.187500\n",
      "Train Epoch: 126 [11776/54000 (22%)] Loss: -773209.125000\n",
      "Train Epoch: 126 [23040/54000 (43%)] Loss: -690364.062500\n",
      "Train Epoch: 126 [34304/54000 (64%)] Loss: -700910.750000\n",
      "Train Epoch: 126 [45568/54000 (84%)] Loss: -712586.187500\n",
      "    epoch          : 126\n",
      "    loss           : -741344.3378712871\n",
      "    val_loss       : -741595.5682617187\n",
      "Train Epoch: 127 [512/54000 (1%)] Loss: -836900.687500\n",
      "Train Epoch: 127 [11776/54000 (22%)] Loss: -723838.062500\n",
      "Train Epoch: 127 [23040/54000 (43%)] Loss: -767153.375000\n",
      "Train Epoch: 127 [34304/54000 (64%)] Loss: -748340.812500\n",
      "Train Epoch: 127 [45568/54000 (84%)] Loss: -700283.937500\n",
      "    epoch          : 127\n",
      "    loss           : -743956.1974009901\n",
      "    val_loss       : -742537.958203125\n",
      "Train Epoch: 128 [512/54000 (1%)] Loss: -835232.750000\n",
      "Train Epoch: 128 [11776/54000 (22%)] Loss: -721311.375000\n",
      "Train Epoch: 128 [23040/54000 (43%)] Loss: -701518.187500\n",
      "Train Epoch: 128 [34304/54000 (64%)] Loss: -746842.312500\n",
      "Train Epoch: 128 [45568/54000 (84%)] Loss: -749669.562500\n",
      "    epoch          : 128\n",
      "    loss           : -745918.5965346535\n",
      "    val_loss       : -743177.3142578125\n",
      "Train Epoch: 129 [512/54000 (1%)] Loss: -777596.125000\n",
      "Train Epoch: 129 [11776/54000 (22%)] Loss: -724124.875000\n",
      "Train Epoch: 129 [23040/54000 (43%)] Loss: -699609.375000\n",
      "Train Epoch: 129 [34304/54000 (64%)] Loss: -747827.500000\n",
      "Train Epoch: 129 [45568/54000 (84%)] Loss: -716092.375000\n",
      "    epoch          : 129\n",
      "    loss           : -746264.2778465346\n",
      "    val_loss       : -746373.1625976562\n",
      "Train Epoch: 130 [512/54000 (1%)] Loss: -839711.750000\n",
      "Train Epoch: 130 [11776/54000 (22%)] Loss: -703621.750000\n",
      "Train Epoch: 130 [23040/54000 (43%)] Loss: -701111.875000\n",
      "Train Epoch: 130 [34304/54000 (64%)] Loss: -843616.875000\n",
      "Train Epoch: 130 [45568/54000 (84%)] Loss: -721416.375000\n",
      "    epoch          : 130\n",
      "    loss           : -749856.6584158416\n",
      "    val_loss       : -747158.5260742188\n",
      "Train Epoch: 131 [512/54000 (1%)] Loss: -844079.875000\n",
      "Train Epoch: 131 [11776/54000 (22%)] Loss: -701713.250000\n",
      "Train Epoch: 131 [23040/54000 (43%)] Loss: -774467.062500\n",
      "Train Epoch: 131 [34304/54000 (64%)] Loss: -841872.687500\n",
      "Train Epoch: 131 [45568/54000 (84%)] Loss: -751887.000000\n",
      "    epoch          : 131\n",
      "    loss           : -750522.3032178218\n",
      "    val_loss       : -748344.9341796875\n",
      "Train Epoch: 132 [512/54000 (1%)] Loss: -845727.500000\n",
      "Train Epoch: 132 [11776/54000 (22%)] Loss: -701696.000000\n",
      "Train Epoch: 132 [23040/54000 (43%)] Loss: -775512.000000\n",
      "Train Epoch: 132 [34304/54000 (64%)] Loss: -752187.375000\n",
      "Train Epoch: 132 [45568/54000 (84%)] Loss: -756759.000000\n",
      "    epoch          : 132\n",
      "    loss           : -751581.8774752475\n",
      "    val_loss       : -747674.9767578125\n",
      "Train Epoch: 133 [512/54000 (1%)] Loss: -846048.375000\n",
      "Train Epoch: 133 [11776/54000 (22%)] Loss: -775810.000000\n",
      "Train Epoch: 133 [23040/54000 (43%)] Loss: -721736.375000\n",
      "Train Epoch: 133 [34304/54000 (64%)] Loss: -714864.000000\n",
      "Train Epoch: 133 [45568/54000 (84%)] Loss: -711890.000000\n",
      "    epoch          : 133\n",
      "    loss           : -751250.6590346535\n",
      "    val_loss       : -751380.3995117188\n",
      "Train Epoch: 134 [512/54000 (1%)] Loss: -849237.000000\n",
      "Train Epoch: 134 [11776/54000 (22%)] Loss: -783530.937500\n",
      "Train Epoch: 134 [23040/54000 (43%)] Loss: -780118.750000\n",
      "Train Epoch: 134 [34304/54000 (64%)] Loss: -781875.125000\n",
      "Train Epoch: 134 [45568/54000 (84%)] Loss: -711436.062500\n",
      "    epoch          : 134\n",
      "    loss           : -755057.5810643565\n",
      "    val_loss       : -752902.094140625\n",
      "Train Epoch: 135 [512/54000 (1%)] Loss: -851548.625000\n",
      "Train Epoch: 135 [11776/54000 (22%)] Loss: -703763.312500\n",
      "Train Epoch: 135 [23040/54000 (43%)] Loss: -759614.750000\n",
      "Train Epoch: 135 [34304/54000 (64%)] Loss: -756327.250000\n",
      "Train Epoch: 135 [45568/54000 (84%)] Loss: -763554.937500\n",
      "    epoch          : 135\n",
      "    loss           : -756011.6905940594\n",
      "    val_loss       : -752524.3262695313\n",
      "Train Epoch: 136 [512/54000 (1%)] Loss: -790878.125000\n",
      "Train Epoch: 136 [11776/54000 (22%)] Loss: -784982.125000\n",
      "Train Epoch: 136 [23040/54000 (43%)] Loss: -779292.375000\n",
      "Train Epoch: 136 [34304/54000 (64%)] Loss: -849604.750000\n",
      "Train Epoch: 136 [45568/54000 (84%)] Loss: -760078.187500\n",
      "    epoch          : 136\n",
      "    loss           : -756599.8075495049\n",
      "    val_loss       : -755367.099609375\n",
      "Train Epoch: 137 [512/54000 (1%)] Loss: -852466.125000\n",
      "Train Epoch: 137 [11776/54000 (22%)] Loss: -735780.375000\n",
      "Train Epoch: 137 [23040/54000 (43%)] Loss: -716529.750000\n",
      "Train Epoch: 137 [34304/54000 (64%)] Loss: -730277.875000\n",
      "Train Epoch: 137 [45568/54000 (84%)] Loss: -727963.812500\n",
      "    epoch          : 137\n",
      "    loss           : -759954.3422029703\n",
      "    val_loss       : -756108.86328125\n",
      "Train Epoch: 138 [512/54000 (1%)] Loss: -855125.250000\n",
      "Train Epoch: 138 [11776/54000 (22%)] Loss: -695863.375000\n",
      "Train Epoch: 138 [23040/54000 (43%)] Loss: -783437.125000\n",
      "Train Epoch: 138 [34304/54000 (64%)] Loss: -727766.562500\n",
      "Train Epoch: 138 [45568/54000 (84%)] Loss: -715728.812500\n",
      "    epoch          : 138\n",
      "    loss           : -759200.5649752475\n",
      "    val_loss       : -757656.7059570312\n",
      "Train Epoch: 139 [512/54000 (1%)] Loss: -856031.437500\n",
      "Train Epoch: 139 [11776/54000 (22%)] Loss: -788578.812500\n",
      "Train Epoch: 139 [23040/54000 (43%)] Loss: -757401.000000\n",
      "Train Epoch: 139 [34304/54000 (64%)] Loss: -734931.437500\n",
      "Train Epoch: 139 [45568/54000 (84%)] Loss: -766772.000000\n",
      "    epoch          : 139\n",
      "    loss           : -761480.3551980198\n",
      "    val_loss       : -759227.790234375\n",
      "Train Epoch: 140 [512/54000 (1%)] Loss: -797277.125000\n",
      "Train Epoch: 140 [11776/54000 (22%)] Loss: -712601.687500\n",
      "Train Epoch: 140 [23040/54000 (43%)] Loss: -717745.500000\n",
      "Train Epoch: 140 [34304/54000 (64%)] Loss: -858769.125000\n",
      "Train Epoch: 140 [45568/54000 (84%)] Loss: -763211.375000\n",
      "    epoch          : 140\n",
      "    loss           : -762684.8706683168\n",
      "    val_loss       : -759880.1071289063\n",
      "Train Epoch: 141 [512/54000 (1%)] Loss: -795093.687500\n",
      "Train Epoch: 141 [11776/54000 (22%)] Loss: -717468.437500\n",
      "Train Epoch: 141 [23040/54000 (43%)] Loss: -763004.625000\n",
      "Train Epoch: 141 [34304/54000 (64%)] Loss: -733782.812500\n",
      "Train Epoch: 141 [45568/54000 (84%)] Loss: -731406.125000\n",
      "    epoch          : 141\n",
      "    loss           : -764071.6794554455\n",
      "    val_loss       : -760328.2848632813\n",
      "Train Epoch: 142 [512/54000 (1%)] Loss: -796513.375000\n",
      "Train Epoch: 142 [11776/54000 (22%)] Loss: -735106.937500\n",
      "Train Epoch: 142 [23040/54000 (43%)] Loss: -758940.812500\n",
      "Train Epoch: 142 [34304/54000 (64%)] Loss: -758623.375000\n",
      "Train Epoch: 142 [45568/54000 (84%)] Loss: -765985.250000\n",
      "    epoch          : 142\n",
      "    loss           : -760554.6373762377\n",
      "    val_loss       : -761545.8872070312\n",
      "Train Epoch: 143 [512/54000 (1%)] Loss: -860390.125000\n",
      "Train Epoch: 143 [11776/54000 (22%)] Loss: -737850.875000\n",
      "Train Epoch: 143 [23040/54000 (43%)] Loss: -739332.187500\n",
      "Train Epoch: 143 [34304/54000 (64%)] Loss: -863558.500000\n",
      "Train Epoch: 143 [45568/54000 (84%)] Loss: -763794.062500\n",
      "    epoch          : 143\n",
      "    loss           : -767450.6349009901\n",
      "    val_loss       : -765701.3314453125\n",
      "Train Epoch: 144 [512/54000 (1%)] Loss: -863758.187500\n",
      "Train Epoch: 144 [11776/54000 (22%)] Loss: -721209.687500\n",
      "Train Epoch: 144 [23040/54000 (43%)] Loss: -770908.250000\n",
      "Train Epoch: 144 [34304/54000 (64%)] Loss: -774772.687500\n",
      "Train Epoch: 144 [45568/54000 (84%)] Loss: -773877.562500\n",
      "    epoch          : 144\n",
      "    loss           : -770093.5612623763\n",
      "    val_loss       : -766592.9521484375\n",
      "Train Epoch: 145 [512/54000 (1%)] Loss: -864608.250000\n",
      "Train Epoch: 145 [11776/54000 (22%)] Loss: -799758.062500\n",
      "Train Epoch: 145 [23040/54000 (43%)] Loss: -796439.875000\n",
      "Train Epoch: 145 [34304/54000 (64%)] Loss: -866629.125000\n",
      "Train Epoch: 145 [45568/54000 (84%)] Loss: -771923.187500\n",
      "    epoch          : 145\n",
      "    loss           : -769646.2134900991\n",
      "    val_loss       : -765988.82265625\n",
      "Train Epoch: 146 [512/54000 (1%)] Loss: -801160.625000\n",
      "Train Epoch: 146 [11776/54000 (22%)] Loss: -790589.687500\n",
      "Train Epoch: 146 [23040/54000 (43%)] Loss: -794385.250000\n",
      "Train Epoch: 146 [34304/54000 (64%)] Loss: -725422.250000\n",
      "Train Epoch: 146 [45568/54000 (84%)] Loss: -736342.500000\n",
      "    epoch          : 146\n",
      "    loss           : -770376.7153465346\n",
      "    val_loss       : -768201.1364257813\n",
      "Train Epoch: 147 [512/54000 (1%)] Loss: -867323.000000\n",
      "Train Epoch: 147 [11776/54000 (22%)] Loss: -726991.500000\n",
      "Train Epoch: 147 [23040/54000 (43%)] Loss: -723000.000000\n",
      "Train Epoch: 147 [34304/54000 (64%)] Loss: -739453.750000\n",
      "Train Epoch: 147 [45568/54000 (84%)] Loss: -737602.937500\n",
      "    epoch          : 147\n",
      "    loss           : -771581.6652227723\n",
      "    val_loss       : -768088.4389648438\n",
      "Train Epoch: 148 [512/54000 (1%)] Loss: -869357.812500\n",
      "Train Epoch: 148 [11776/54000 (22%)] Loss: -745202.500000\n",
      "Train Epoch: 148 [23040/54000 (43%)] Loss: -780003.375000\n",
      "Train Epoch: 148 [34304/54000 (64%)] Loss: -870738.937500\n",
      "Train Epoch: 148 [45568/54000 (84%)] Loss: -778477.500000\n",
      "    epoch          : 148\n",
      "    loss           : -772884.8805693069\n",
      "    val_loss       : -769129.6891601563\n",
      "Train Epoch: 149 [512/54000 (1%)] Loss: -870171.000000\n",
      "Train Epoch: 149 [11776/54000 (22%)] Loss: -802115.125000\n",
      "Train Epoch: 149 [23040/54000 (43%)] Loss: -728019.875000\n",
      "Train Epoch: 149 [34304/54000 (64%)] Loss: -872386.062500\n",
      "Train Epoch: 149 [45568/54000 (84%)] Loss: -794976.750000\n",
      "    epoch          : 149\n",
      "    loss           : -774121.510519802\n",
      "    val_loss       : -771546.094921875\n",
      "Train Epoch: 150 [512/54000 (1%)] Loss: -870813.000000\n",
      "Train Epoch: 150 [11776/54000 (22%)] Loss: -730167.750000\n",
      "Train Epoch: 150 [23040/54000 (43%)] Loss: -728799.000000\n",
      "Train Epoch: 150 [34304/54000 (64%)] Loss: -872970.500000\n",
      "Train Epoch: 150 [45568/54000 (84%)] Loss: -742568.062500\n",
      "    epoch          : 150\n",
      "    loss           : -776006.6274752475\n",
      "    val_loss       : -772340.1809570312\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0716_225739/checkpoint-epoch150.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 151 [512/54000 (1%)] Loss: -874922.187500\n",
      "Train Epoch: 151 [11776/54000 (22%)] Loss: -791810.250000\n",
      "Train Epoch: 151 [23040/54000 (43%)] Loss: -772209.937500\n",
      "Train Epoch: 151 [34304/54000 (64%)] Loss: -779207.687500\n",
      "Train Epoch: 151 [45568/54000 (84%)] Loss: -782839.375000\n",
      "    epoch          : 151\n",
      "    loss           : -775628.5928217822\n",
      "    val_loss       : -773591.2326171875\n",
      "Train Epoch: 152 [512/54000 (1%)] Loss: -872456.625000\n",
      "Train Epoch: 152 [11776/54000 (22%)] Loss: -751657.750000\n",
      "Train Epoch: 152 [23040/54000 (43%)] Loss: -728070.875000\n",
      "Train Epoch: 152 [34304/54000 (64%)] Loss: -877309.500000\n",
      "Train Epoch: 152 [45568/54000 (84%)] Loss: -744840.875000\n",
      "    epoch          : 152\n",
      "    loss           : -777751.7431930694\n",
      "    val_loss       : -775940.1685546875\n",
      "Train Epoch: 153 [512/54000 (1%)] Loss: -877145.750000\n",
      "Train Epoch: 153 [11776/54000 (22%)] Loss: -751904.812500\n",
      "Train Epoch: 153 [23040/54000 (43%)] Loss: -806265.750000\n",
      "Train Epoch: 153 [34304/54000 (64%)] Loss: -731844.000000\n",
      "Train Epoch: 153 [45568/54000 (84%)] Loss: -780073.937500\n",
      "    epoch          : 153\n",
      "    loss           : -780004.3551980198\n",
      "    val_loss       : -776639.128125\n",
      "Train Epoch: 154 [512/54000 (1%)] Loss: -878448.312500\n",
      "Train Epoch: 154 [11776/54000 (22%)] Loss: -810737.687500\n",
      "Train Epoch: 154 [23040/54000 (43%)] Loss: -734939.125000\n",
      "Train Epoch: 154 [34304/54000 (64%)] Loss: -733715.750000\n",
      "Train Epoch: 154 [45568/54000 (84%)] Loss: -782060.187500\n",
      "    epoch          : 154\n",
      "    loss           : -780046.6974009901\n",
      "    val_loss       : -776989.4330078125\n",
      "Train Epoch: 155 [512/54000 (1%)] Loss: -875982.937500\n",
      "Train Epoch: 155 [11776/54000 (22%)] Loss: -749849.750000\n",
      "Train Epoch: 155 [23040/54000 (43%)] Loss: -733135.750000\n",
      "Train Epoch: 155 [34304/54000 (64%)] Loss: -740630.375000\n",
      "Train Epoch: 155 [45568/54000 (84%)] Loss: -779004.875000\n",
      "    epoch          : 155\n",
      "    loss           : -780608.9325495049\n",
      "    val_loss       : -778348.4971679688\n",
      "Train Epoch: 156 [512/54000 (1%)] Loss: -880471.687500\n",
      "Train Epoch: 156 [11776/54000 (22%)] Loss: -811218.625000\n",
      "Train Epoch: 156 [23040/54000 (43%)] Loss: -806589.812500\n",
      "Train Epoch: 156 [34304/54000 (64%)] Loss: -878215.875000\n",
      "Train Epoch: 156 [45568/54000 (84%)] Loss: -787470.625000\n",
      "    epoch          : 156\n",
      "    loss           : -781278.1738861386\n",
      "    val_loss       : -779449.3153320312\n",
      "Train Epoch: 157 [512/54000 (1%)] Loss: -883260.250000\n",
      "Train Epoch: 157 [11776/54000 (22%)] Loss: -813295.000000\n",
      "Train Epoch: 157 [23040/54000 (43%)] Loss: -738299.375000\n",
      "Train Epoch: 157 [34304/54000 (64%)] Loss: -739715.500000\n",
      "Train Epoch: 157 [45568/54000 (84%)] Loss: -752535.187500\n",
      "    epoch          : 157\n",
      "    loss           : -784856.5959158416\n",
      "    val_loss       : -781342.63671875\n",
      "Train Epoch: 158 [512/54000 (1%)] Loss: -883562.187500\n",
      "Train Epoch: 158 [11776/54000 (22%)] Loss: -813844.500000\n",
      "Train Epoch: 158 [23040/54000 (43%)] Loss: -813197.625000\n",
      "Train Epoch: 158 [34304/54000 (64%)] Loss: -756340.875000\n",
      "Train Epoch: 158 [45568/54000 (84%)] Loss: -782447.312500\n",
      "    epoch          : 158\n",
      "    loss           : -785493.7759900991\n",
      "    val_loss       : -781477.9081054687\n",
      "Train Epoch: 159 [512/54000 (1%)] Loss: -884241.750000\n",
      "Train Epoch: 159 [11776/54000 (22%)] Loss: -809256.937500\n",
      "Train Epoch: 159 [23040/54000 (43%)] Loss: -814425.625000\n",
      "Train Epoch: 159 [34304/54000 (64%)] Loss: -792982.750000\n",
      "Train Epoch: 159 [45568/54000 (84%)] Loss: -753642.125000\n",
      "    epoch          : 159\n",
      "    loss           : -786269.6683168317\n",
      "    val_loss       : -784063.0796875\n",
      "Train Epoch: 160 [512/54000 (1%)] Loss: -886904.875000\n",
      "Train Epoch: 160 [11776/54000 (22%)] Loss: -820106.562500\n",
      "Train Epoch: 160 [23040/54000 (43%)] Loss: -812917.875000\n",
      "Train Epoch: 160 [34304/54000 (64%)] Loss: -746304.125000\n",
      "Train Epoch: 160 [45568/54000 (84%)] Loss: -788640.687500\n",
      "    epoch          : 160\n",
      "    loss           : -787151.0086633663\n",
      "    val_loss       : -782012.937109375\n",
      "Train Epoch: 161 [512/54000 (1%)] Loss: -887260.500000\n",
      "Train Epoch: 161 [11776/54000 (22%)] Loss: -759433.125000\n",
      "Train Epoch: 161 [23040/54000 (43%)] Loss: -746522.312500\n",
      "Train Epoch: 161 [34304/54000 (64%)] Loss: -787039.625000\n",
      "Train Epoch: 161 [45568/54000 (84%)] Loss: -790133.000000\n",
      "    epoch          : 161\n",
      "    loss           : -787738.5915841584\n",
      "    val_loss       : -781946.7172851562\n",
      "Train Epoch: 162 [512/54000 (1%)] Loss: -887881.312500\n",
      "Train Epoch: 162 [11776/54000 (22%)] Loss: -811468.375000\n",
      "Train Epoch: 162 [23040/54000 (43%)] Loss: -759467.375000\n",
      "Train Epoch: 162 [34304/54000 (64%)] Loss: -746853.562500\n",
      "Train Epoch: 162 [45568/54000 (84%)] Loss: -796784.250000\n",
      "    epoch          : 162\n",
      "    loss           : -788190.8514851485\n",
      "    val_loss       : -786513.553515625\n",
      "Train Epoch: 163 [512/54000 (1%)] Loss: -890581.812500\n",
      "Train Epoch: 163 [11776/54000 (22%)] Loss: -740690.375000\n",
      "Train Epoch: 163 [23040/54000 (43%)] Loss: -749591.500000\n",
      "Train Epoch: 163 [34304/54000 (64%)] Loss: -747585.687500\n",
      "Train Epoch: 163 [45568/54000 (84%)] Loss: -794674.687500\n",
      "    epoch          : 163\n",
      "    loss           : -790667.6918316832\n",
      "    val_loss       : -783983.0872070312\n",
      "Train Epoch: 164 [512/54000 (1%)] Loss: -890256.312500\n",
      "Train Epoch: 164 [11776/54000 (22%)] Loss: -805849.062500\n",
      "Train Epoch: 164 [23040/54000 (43%)] Loss: -745551.375000\n",
      "Train Epoch: 164 [34304/54000 (64%)] Loss: -748224.375000\n",
      "Train Epoch: 164 [45568/54000 (84%)] Loss: -796801.812500\n",
      "    epoch          : 164\n",
      "    loss           : -789661.4888613861\n",
      "    val_loss       : -788382.3901367188\n",
      "Train Epoch: 165 [512/54000 (1%)] Loss: -893340.375000\n",
      "Train Epoch: 165 [11776/54000 (22%)] Loss: -766875.250000\n",
      "Train Epoch: 165 [23040/54000 (43%)] Loss: -795730.625000\n",
      "Train Epoch: 165 [34304/54000 (64%)] Loss: -750780.125000\n",
      "Train Epoch: 165 [45568/54000 (84%)] Loss: -795536.125000\n",
      "    epoch          : 165\n",
      "    loss           : -794028.4016089109\n",
      "    val_loss       : -789040.6399414062\n",
      "Train Epoch: 166 [512/54000 (1%)] Loss: -894713.687500\n",
      "Train Epoch: 166 [11776/54000 (22%)] Loss: -824974.562500\n",
      "Train Epoch: 166 [23040/54000 (43%)] Loss: -791413.187500\n",
      "Train Epoch: 166 [34304/54000 (64%)] Loss: -743178.937500\n",
      "Train Epoch: 166 [45568/54000 (84%)] Loss: -746824.750000\n",
      "    epoch          : 166\n",
      "    loss           : -792697.8589108911\n",
      "    val_loss       : -790397.3584960938\n",
      "Train Epoch: 167 [512/54000 (1%)] Loss: -893671.125000\n",
      "Train Epoch: 167 [11776/54000 (22%)] Loss: -761211.750000\n",
      "Train Epoch: 167 [23040/54000 (43%)] Loss: -748747.875000\n",
      "Train Epoch: 167 [34304/54000 (64%)] Loss: -794123.687500\n",
      "Train Epoch: 167 [45568/54000 (84%)] Loss: -798834.875000\n",
      "    epoch          : 167\n",
      "    loss           : -795456.0872524752\n",
      "    val_loss       : -791444.50703125\n",
      "Train Epoch: 168 [512/54000 (1%)] Loss: -895711.375000\n",
      "Train Epoch: 168 [11776/54000 (22%)] Loss: -763137.812500\n",
      "Train Epoch: 168 [23040/54000 (43%)] Loss: -799970.062500\n",
      "Train Epoch: 168 [34304/54000 (64%)] Loss: -821294.750000\n",
      "Train Epoch: 168 [45568/54000 (84%)] Loss: -795841.500000\n",
      "    epoch          : 168\n",
      "    loss           : -794698.6553217822\n",
      "    val_loss       : -790583.8680664062\n",
      "Train Epoch: 169 [512/54000 (1%)] Loss: -830223.375000\n",
      "Train Epoch: 169 [11776/54000 (22%)] Loss: -762793.312500\n",
      "Train Epoch: 169 [23040/54000 (43%)] Loss: -820952.312500\n",
      "Train Epoch: 169 [34304/54000 (64%)] Loss: -799558.750000\n",
      "Train Epoch: 169 [45568/54000 (84%)] Loss: -804073.625000\n",
      "    epoch          : 169\n",
      "    loss           : -796601.7388613861\n",
      "    val_loss       : -794148.9010742188\n",
      "Train Epoch: 170 [512/54000 (1%)] Loss: -895907.125000\n",
      "Train Epoch: 170 [11776/54000 (22%)] Loss: -773893.375000\n",
      "Train Epoch: 170 [23040/54000 (43%)] Loss: -824422.750000\n",
      "Train Epoch: 170 [34304/54000 (64%)] Loss: -754525.000000\n",
      "Train Epoch: 170 [45568/54000 (84%)] Loss: -766800.687500\n",
      "    epoch          : 170\n",
      "    loss           : -798228.6831683168\n",
      "    val_loss       : -794754.6883789062\n",
      "Train Epoch: 171 [512/54000 (1%)] Loss: -897632.250000\n",
      "Train Epoch: 171 [11776/54000 (22%)] Loss: -776273.500000\n",
      "Train Epoch: 171 [23040/54000 (43%)] Loss: -805063.000000\n",
      "Train Epoch: 171 [34304/54000 (64%)] Loss: -802943.312500\n",
      "Train Epoch: 171 [45568/54000 (84%)] Loss: -801600.875000\n",
      "    epoch          : 171\n",
      "    loss           : -799809.4152227723\n",
      "    val_loss       : -794539.225390625\n",
      "Train Epoch: 172 [512/54000 (1%)] Loss: -901010.250000\n",
      "Train Epoch: 172 [11776/54000 (22%)] Loss: -752342.000000\n",
      "Train Epoch: 172 [23040/54000 (43%)] Loss: -802600.000000\n",
      "Train Epoch: 172 [34304/54000 (64%)] Loss: -901220.500000\n",
      "Train Epoch: 172 [45568/54000 (84%)] Loss: -802471.562500\n",
      "    epoch          : 172\n",
      "    loss           : -800839.0860148515\n",
      "    val_loss       : -797550.344140625\n",
      "Train Epoch: 173 [512/54000 (1%)] Loss: -905652.125000\n",
      "Train Epoch: 173 [11776/54000 (22%)] Loss: -802413.562500\n",
      "Train Epoch: 173 [23040/54000 (43%)] Loss: -754249.875000\n",
      "Train Epoch: 173 [34304/54000 (64%)] Loss: -803156.125000\n",
      "Train Epoch: 173 [45568/54000 (84%)] Loss: -793795.375000\n",
      "    epoch          : 173\n",
      "    loss           : -799727.6163366337\n",
      "    val_loss       : -794827.3840820312\n",
      "Train Epoch: 174 [512/54000 (1%)] Loss: -899964.250000\n",
      "Train Epoch: 174 [11776/54000 (22%)] Loss: -775879.812500\n",
      "Train Epoch: 174 [23040/54000 (43%)] Loss: -826783.250000\n",
      "Train Epoch: 174 [34304/54000 (64%)] Loss: -806800.687500\n",
      "Train Epoch: 174 [45568/54000 (84%)] Loss: -803482.562500\n",
      "    epoch          : 174\n",
      "    loss           : -802753.5408415842\n",
      "    val_loss       : -799112.6127929688\n",
      "Train Epoch: 175 [512/54000 (1%)] Loss: -903270.562500\n",
      "Train Epoch: 175 [11776/54000 (22%)] Loss: -757113.125000\n",
      "Train Epoch: 175 [23040/54000 (43%)] Loss: -831345.375000\n",
      "Train Epoch: 175 [34304/54000 (64%)] Loss: -762703.625000\n",
      "Train Epoch: 175 [45568/54000 (84%)] Loss: -806753.625000\n",
      "    epoch          : 175\n",
      "    loss           : -803937.9709158416\n",
      "    val_loss       : -799591.654296875\n",
      "Train Epoch: 176 [512/54000 (1%)] Loss: -774654.625000\n",
      "Train Epoch: 176 [11776/54000 (22%)] Loss: -836204.812500\n",
      "Train Epoch: 176 [23040/54000 (43%)] Loss: -755112.812500\n",
      "Train Epoch: 176 [34304/54000 (64%)] Loss: -825120.250000\n",
      "Train Epoch: 176 [45568/54000 (84%)] Loss: -808224.500000\n",
      "    epoch          : 176\n",
      "    loss           : -804373.1577970297\n",
      "    val_loss       : -800994.2247070313\n",
      "Train Epoch: 177 [512/54000 (1%)] Loss: -839157.125000\n",
      "Train Epoch: 177 [11776/54000 (22%)] Loss: -840266.000000\n",
      "Train Epoch: 177 [23040/54000 (43%)] Loss: -768161.437500\n",
      "Train Epoch: 177 [34304/54000 (64%)] Loss: -803499.437500\n",
      "Train Epoch: 177 [45568/54000 (84%)] Loss: -767150.125000\n",
      "    epoch          : 177\n",
      "    loss           : -806194.3347772277\n",
      "    val_loss       : -801306.4286132812\n",
      "Train Epoch: 178 [512/54000 (1%)] Loss: -842230.562500\n",
      "Train Epoch: 178 [11776/54000 (22%)] Loss: -777894.312500\n",
      "Train Epoch: 178 [23040/54000 (43%)] Loss: -832422.625000\n",
      "Train Epoch: 178 [34304/54000 (64%)] Loss: -832703.062500\n",
      "Train Epoch: 178 [45568/54000 (84%)] Loss: -809810.937500\n",
      "    epoch          : 178\n",
      "    loss           : -807026.6714108911\n",
      "    val_loss       : -801469.8833007812\n",
      "Train Epoch: 179 [512/54000 (1%)] Loss: -909622.000000\n",
      "Train Epoch: 179 [11776/54000 (22%)] Loss: -837136.000000\n",
      "Train Epoch: 179 [23040/54000 (43%)] Loss: -833842.125000\n",
      "Train Epoch: 179 [34304/54000 (64%)] Loss: -910611.750000\n",
      "Train Epoch: 179 [45568/54000 (84%)] Loss: -771629.625000\n",
      "    epoch          : 179\n",
      "    loss           : -807027.906559406\n",
      "    val_loss       : -803258.3900390625\n",
      "Train Epoch: 180 [512/54000 (1%)] Loss: -910042.312500\n",
      "Train Epoch: 180 [11776/54000 (22%)] Loss: -822800.125000\n",
      "Train Epoch: 180 [23040/54000 (43%)] Loss: -831120.625000\n",
      "Train Epoch: 180 [34304/54000 (64%)] Loss: -813332.125000\n",
      "Train Epoch: 180 [45568/54000 (84%)] Loss: -808667.750000\n",
      "    epoch          : 180\n",
      "    loss           : -806422.3756188119\n",
      "    val_loss       : -803871.8940429688\n",
      "Train Epoch: 181 [512/54000 (1%)] Loss: -841351.812500\n",
      "Train Epoch: 181 [11776/54000 (22%)] Loss: -770956.500000\n",
      "Train Epoch: 181 [23040/54000 (43%)] Loss: -837224.812500\n",
      "Train Epoch: 181 [34304/54000 (64%)] Loss: -808613.312500\n",
      "Train Epoch: 181 [45568/54000 (84%)] Loss: -772716.437500\n",
      "    epoch          : 181\n",
      "    loss           : -810340.2308168317\n",
      "    val_loss       : -806318.854296875\n",
      "Train Epoch: 182 [512/54000 (1%)] Loss: -769970.750000\n",
      "Train Epoch: 182 [11776/54000 (22%)] Loss: -771741.375000\n",
      "Train Epoch: 182 [23040/54000 (43%)] Loss: -768605.125000\n",
      "Train Epoch: 182 [34304/54000 (64%)] Loss: -762812.500000\n",
      "Train Epoch: 182 [45568/54000 (84%)] Loss: -812436.187500\n",
      "    epoch          : 182\n",
      "    loss           : -809508.9876237623\n",
      "    val_loss       : -805147.8584960938\n",
      "Train Epoch: 183 [512/54000 (1%)] Loss: -783765.875000\n",
      "Train Epoch: 183 [11776/54000 (22%)] Loss: -846165.375000\n",
      "Train Epoch: 183 [23040/54000 (43%)] Loss: -836765.062500\n",
      "Train Epoch: 183 [34304/54000 (64%)] Loss: -812013.500000\n",
      "Train Epoch: 183 [45568/54000 (84%)] Loss: -814562.875000\n",
      "    epoch          : 183\n",
      "    loss           : -810167.0736386139\n",
      "    val_loss       : -805851.445703125\n",
      "Train Epoch: 184 [512/54000 (1%)] Loss: -916070.750000\n",
      "Train Epoch: 184 [11776/54000 (22%)] Loss: -834767.125000\n",
      "Train Epoch: 184 [23040/54000 (43%)] Loss: -812052.500000\n",
      "Train Epoch: 184 [34304/54000 (64%)] Loss: -777625.687500\n",
      "Train Epoch: 184 [45568/54000 (84%)] Loss: -770326.750000\n",
      "    epoch          : 184\n",
      "    loss           : -810209.7883663366\n",
      "    val_loss       : -804436.4350585938\n",
      "Train Epoch: 185 [512/54000 (1%)] Loss: -914796.875000\n",
      "Train Epoch: 185 [11776/54000 (22%)] Loss: -808593.687500\n",
      "Train Epoch: 185 [23040/54000 (43%)] Loss: -811969.500000\n",
      "Train Epoch: 185 [34304/54000 (64%)] Loss: -819449.000000\n",
      "Train Epoch: 185 [45568/54000 (84%)] Loss: -816952.250000\n",
      "    epoch          : 185\n",
      "    loss           : -812220.1806930694\n",
      "    val_loss       : -808181.8768554687\n",
      "Train Epoch: 186 [512/54000 (1%)] Loss: -916158.625000\n",
      "Train Epoch: 186 [11776/54000 (22%)] Loss: -784251.250000\n",
      "Train Epoch: 186 [23040/54000 (43%)] Loss: -780768.250000\n",
      "Train Epoch: 186 [34304/54000 (64%)] Loss: -778655.437500\n",
      "Train Epoch: 186 [45568/54000 (84%)] Loss: -812866.187500\n",
      "    epoch          : 186\n",
      "    loss           : -813368.2264851485\n",
      "    val_loss       : -809309.8357421875\n",
      "Train Epoch: 187 [512/54000 (1%)] Loss: -918291.750000\n",
      "Train Epoch: 187 [11776/54000 (22%)] Loss: -784637.062500\n",
      "Train Epoch: 187 [23040/54000 (43%)] Loss: -821150.625000\n",
      "Train Epoch: 187 [34304/54000 (64%)] Loss: -819878.125000\n",
      "Train Epoch: 187 [45568/54000 (84%)] Loss: -820542.437500\n",
      "    epoch          : 187\n",
      "    loss           : -814288.7029702971\n",
      "    val_loss       : -810283.9635742188\n",
      "Train Epoch: 188 [512/54000 (1%)] Loss: -918383.812500\n",
      "Train Epoch: 188 [11776/54000 (22%)] Loss: -844878.250000\n",
      "Train Epoch: 188 [23040/54000 (43%)] Loss: -840405.500000\n",
      "Train Epoch: 188 [34304/54000 (64%)] Loss: -838619.875000\n",
      "Train Epoch: 188 [45568/54000 (84%)] Loss: -779047.625000\n",
      "    epoch          : 188\n",
      "    loss           : -814763.2543316832\n",
      "    val_loss       : -811685.7768554688\n",
      "Train Epoch: 189 [512/54000 (1%)] Loss: -918692.625000\n",
      "Train Epoch: 189 [11776/54000 (22%)] Loss: -790011.500000\n",
      "Train Epoch: 189 [23040/54000 (43%)] Loss: -774714.937500\n",
      "Train Epoch: 189 [34304/54000 (64%)] Loss: -783314.500000\n",
      "Train Epoch: 189 [45568/54000 (84%)] Loss: -819048.187500\n",
      "    epoch          : 189\n",
      "    loss           : -816942.416460396\n",
      "    val_loss       : -808320.8220703125\n",
      "Train Epoch: 190 [512/54000 (1%)] Loss: -919372.625000\n",
      "Train Epoch: 190 [11776/54000 (22%)] Loss: -787107.250000\n",
      "Train Epoch: 190 [23040/54000 (43%)] Loss: -763987.687500\n",
      "Train Epoch: 190 [34304/54000 (64%)] Loss: -835592.687500\n",
      "Train Epoch: 190 [45568/54000 (84%)] Loss: -784253.500000\n",
      "    epoch          : 190\n",
      "    loss           : -815135.9139851485\n",
      "    val_loss       : -812330.5720703125\n",
      "Train Epoch: 191 [512/54000 (1%)] Loss: -924040.500000\n",
      "Train Epoch: 191 [11776/54000 (22%)] Loss: -792396.000000\n",
      "Train Epoch: 191 [23040/54000 (43%)] Loss: -819997.562500\n",
      "Train Epoch: 191 [34304/54000 (64%)] Loss: -847359.125000\n",
      "Train Epoch: 191 [45568/54000 (84%)] Loss: -820696.250000\n",
      "    epoch          : 191\n",
      "    loss           : -819401.4672029703\n",
      "    val_loss       : -814510.351171875\n",
      "Train Epoch: 192 [512/54000 (1%)] Loss: -923472.875000\n",
      "Train Epoch: 192 [11776/54000 (22%)] Loss: -789745.437500\n",
      "Train Epoch: 192 [23040/54000 (43%)] Loss: -772134.125000\n",
      "Train Epoch: 192 [34304/54000 (64%)] Loss: -785938.000000\n",
      "Train Epoch: 192 [45568/54000 (84%)] Loss: -824114.625000\n",
      "    epoch          : 192\n",
      "    loss           : -819263.3261138614\n",
      "    val_loss       : -814819.674609375\n",
      "Train Epoch: 193 [512/54000 (1%)] Loss: -854210.375000\n",
      "Train Epoch: 193 [11776/54000 (22%)] Loss: -790852.750000\n",
      "Train Epoch: 193 [23040/54000 (43%)] Loss: -783984.687500\n",
      "Train Epoch: 193 [34304/54000 (64%)] Loss: -819924.062500\n",
      "Train Epoch: 193 [45568/54000 (84%)] Loss: -824736.000000\n",
      "    epoch          : 193\n",
      "    loss           : -820509.4560643565\n",
      "    val_loss       : -814440.0407226563\n",
      "Train Epoch: 194 [512/54000 (1%)] Loss: -857680.125000\n",
      "Train Epoch: 194 [11776/54000 (22%)] Loss: -927155.750000\n",
      "Train Epoch: 194 [23040/54000 (43%)] Loss: -771590.000000\n",
      "Train Epoch: 194 [34304/54000 (64%)] Loss: -783148.875000\n",
      "Train Epoch: 194 [45568/54000 (84%)] Loss: -780524.187500\n",
      "    epoch          : 194\n",
      "    loss           : -821701.0866336634\n",
      "    val_loss       : -816398.8439453125\n",
      "Train Epoch: 195 [512/54000 (1%)] Loss: -792691.000000\n",
      "Train Epoch: 195 [11776/54000 (22%)] Loss: -852490.125000\n",
      "Train Epoch: 195 [23040/54000 (43%)] Loss: -840394.187500\n",
      "Train Epoch: 195 [34304/54000 (64%)] Loss: -926746.875000\n",
      "Train Epoch: 195 [45568/54000 (84%)] Loss: -785537.750000\n",
      "    epoch          : 195\n",
      "    loss           : -821097.2976485149\n",
      "    val_loss       : -817661.337109375\n",
      "Train Epoch: 196 [512/54000 (1%)] Loss: -927489.250000\n",
      "Train Epoch: 196 [11776/54000 (22%)] Loss: -773012.125000\n",
      "Train Epoch: 196 [23040/54000 (43%)] Loss: -770769.500000\n",
      "Train Epoch: 196 [34304/54000 (64%)] Loss: -925331.937500\n",
      "Train Epoch: 196 [45568/54000 (84%)] Loss: -829505.125000\n",
      "    epoch          : 196\n",
      "    loss           : -823566.9344059406\n",
      "    val_loss       : -817453.4711914062\n",
      "Train Epoch: 197 [512/54000 (1%)] Loss: -927295.062500\n",
      "Train Epoch: 197 [11776/54000 (22%)] Loss: -769224.500000\n",
      "Train Epoch: 197 [23040/54000 (43%)] Loss: -769070.500000\n",
      "Train Epoch: 197 [34304/54000 (64%)] Loss: -822139.937500\n",
      "Train Epoch: 197 [45568/54000 (84%)] Loss: -831863.187500\n",
      "    epoch          : 197\n",
      "    loss           : -824037.3465346535\n",
      "    val_loss       : -819129.1862304688\n",
      "Train Epoch: 198 [512/54000 (1%)] Loss: -860036.000000\n",
      "Train Epoch: 198 [11776/54000 (22%)] Loss: -771096.750000\n",
      "Train Epoch: 198 [23040/54000 (43%)] Loss: -823199.562500\n",
      "Train Epoch: 198 [34304/54000 (64%)] Loss: -848459.062500\n",
      "Train Epoch: 198 [45568/54000 (84%)] Loss: -827019.375000\n",
      "    epoch          : 198\n",
      "    loss           : -823837.6608910891\n",
      "    val_loss       : -818897.7114257812\n",
      "Train Epoch: 199 [512/54000 (1%)] Loss: -932019.812500\n",
      "Train Epoch: 199 [11776/54000 (22%)] Loss: -775737.312500\n",
      "Train Epoch: 199 [23040/54000 (43%)] Loss: -782374.375000\n",
      "Train Epoch: 199 [34304/54000 (64%)] Loss: -842069.312500\n",
      "Train Epoch: 199 [45568/54000 (84%)] Loss: -829659.500000\n",
      "    epoch          : 199\n",
      "    loss           : -824092.728960396\n",
      "    val_loss       : -819876.4033203125\n",
      "Train Epoch: 200 [512/54000 (1%)] Loss: -930766.875000\n",
      "Train Epoch: 200 [11776/54000 (22%)] Loss: -858158.125000\n",
      "Train Epoch: 200 [23040/54000 (43%)] Loss: -784934.500000\n",
      "Train Epoch: 200 [34304/54000 (64%)] Loss: -824055.937500\n",
      "Train Epoch: 200 [45568/54000 (84%)] Loss: -790243.875000\n",
      "    epoch          : 200\n",
      "    loss           : -825353.4077970297\n",
      "    val_loss       : -818208.9752929688\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0716_225739/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [512/54000 (1%)] Loss: -930454.062500\n",
      "Train Epoch: 201 [11776/54000 (22%)] Loss: -814460.687500\n",
      "Train Epoch: 201 [23040/54000 (43%)] Loss: -791736.250000\n",
      "Train Epoch: 201 [34304/54000 (64%)] Loss: -851722.437500\n",
      "Train Epoch: 201 [45568/54000 (84%)] Loss: -830993.500000\n",
      "    epoch          : 201\n",
      "    loss           : -826011.2568069306\n",
      "    val_loss       : -822116.7174804688\n",
      "Train Epoch: 202 [512/54000 (1%)] Loss: -934440.500000\n",
      "Train Epoch: 202 [11776/54000 (22%)] Loss: -849394.500000\n",
      "Train Epoch: 202 [23040/54000 (43%)] Loss: -779601.437500\n",
      "Train Epoch: 202 [34304/54000 (64%)] Loss: -931894.250000\n",
      "Train Epoch: 202 [45568/54000 (84%)] Loss: -830325.375000\n",
      "    epoch          : 202\n",
      "    loss           : -828426.0637376237\n",
      "    val_loss       : -823188.700390625\n",
      "Train Epoch: 203 [512/54000 (1%)] Loss: -789634.125000\n",
      "Train Epoch: 203 [11776/54000 (22%)] Loss: -774643.875000\n",
      "Train Epoch: 203 [23040/54000 (43%)] Loss: -792352.062500\n",
      "Train Epoch: 203 [34304/54000 (64%)] Loss: -793872.187500\n",
      "Train Epoch: 203 [45568/54000 (84%)] Loss: -831229.312500\n",
      "    epoch          : 203\n",
      "    loss           : -828817.4523514851\n",
      "    val_loss       : -822952.4359375\n",
      "Train Epoch: 204 [512/54000 (1%)] Loss: -936183.875000\n",
      "Train Epoch: 204 [11776/54000 (22%)] Loss: -800726.875000\n",
      "Train Epoch: 204 [23040/54000 (43%)] Loss: -780748.500000\n",
      "Train Epoch: 204 [34304/54000 (64%)] Loss: -830867.250000\n",
      "Train Epoch: 204 [45568/54000 (84%)] Loss: -793820.125000\n",
      "    epoch          : 204\n",
      "    loss           : -829228.5581683168\n",
      "    val_loss       : -823452.921484375\n",
      "Train Epoch: 205 [512/54000 (1%)] Loss: -933412.000000\n",
      "Train Epoch: 205 [11776/54000 (22%)] Loss: -864328.000000\n",
      "Train Epoch: 205 [23040/54000 (43%)] Loss: -831682.000000\n",
      "Train Epoch: 205 [34304/54000 (64%)] Loss: -936344.125000\n",
      "Train Epoch: 205 [45568/54000 (84%)] Loss: -798228.000000\n",
      "    epoch          : 205\n",
      "    loss           : -829927.9096534654\n",
      "    val_loss       : -824659.0765625\n",
      "Train Epoch: 206 [512/54000 (1%)] Loss: -936337.687500\n",
      "Train Epoch: 206 [11776/54000 (22%)] Loss: -795917.750000\n",
      "Train Epoch: 206 [23040/54000 (43%)] Loss: -796300.750000\n",
      "Train Epoch: 206 [34304/54000 (64%)] Loss: -857175.750000\n",
      "Train Epoch: 206 [45568/54000 (84%)] Loss: -838149.625000\n",
      "    epoch          : 206\n",
      "    loss           : -832413.4381188119\n",
      "    val_loss       : -826414.6153320313\n",
      "Train Epoch: 207 [512/54000 (1%)] Loss: -935964.812500\n",
      "Train Epoch: 207 [11776/54000 (22%)] Loss: -834536.062500\n",
      "Train Epoch: 207 [23040/54000 (43%)] Loss: -840894.062500\n",
      "Train Epoch: 207 [34304/54000 (64%)] Loss: -791575.062500\n",
      "Train Epoch: 207 [45568/54000 (84%)] Loss: -833898.750000\n",
      "    epoch          : 207\n",
      "    loss           : -833378.3372524752\n",
      "    val_loss       : -826769.899609375\n",
      "Train Epoch: 208 [512/54000 (1%)] Loss: -939222.500000\n",
      "Train Epoch: 208 [11776/54000 (22%)] Loss: -805083.937500\n",
      "Train Epoch: 208 [23040/54000 (43%)] Loss: -788303.312500\n",
      "Train Epoch: 208 [34304/54000 (64%)] Loss: -788437.062500\n",
      "Train Epoch: 208 [45568/54000 (84%)] Loss: -792753.562500\n",
      "    epoch          : 208\n",
      "    loss           : -831373.9201732674\n",
      "    val_loss       : -823855.795703125\n",
      "Train Epoch: 209 [512/54000 (1%)] Loss: -939551.312500\n",
      "Train Epoch: 209 [11776/54000 (22%)] Loss: -803748.875000\n",
      "Train Epoch: 209 [23040/54000 (43%)] Loss: -857413.250000\n",
      "Train Epoch: 209 [34304/54000 (64%)] Loss: -775172.500000\n",
      "Train Epoch: 209 [45568/54000 (84%)] Loss: -796535.312500\n",
      "    epoch          : 209\n",
      "    loss           : -829897.6837871287\n",
      "    val_loss       : -827017.1340820312\n",
      "Train Epoch: 210 [512/54000 (1%)] Loss: -938760.437500\n",
      "Train Epoch: 210 [11776/54000 (22%)] Loss: -867962.375000\n",
      "Train Epoch: 210 [23040/54000 (43%)] Loss: -791276.125000\n",
      "Train Epoch: 210 [34304/54000 (64%)] Loss: -800638.500000\n",
      "Train Epoch: 210 [45568/54000 (84%)] Loss: -797027.625000\n",
      "    epoch          : 210\n",
      "    loss           : -834007.2017326732\n",
      "    val_loss       : -829716.1536132812\n",
      "Train Epoch: 211 [512/54000 (1%)] Loss: -941884.500000\n",
      "Train Epoch: 211 [11776/54000 (22%)] Loss: -837190.437500\n",
      "Train Epoch: 211 [23040/54000 (43%)] Loss: -787537.625000\n",
      "Train Epoch: 211 [34304/54000 (64%)] Loss: -859695.937500\n",
      "Train Epoch: 211 [45568/54000 (84%)] Loss: -834030.375000\n",
      "    epoch          : 211\n",
      "    loss           : -836040.6120049505\n",
      "    val_loss       : -829268.8887695313\n",
      "Train Epoch: 212 [512/54000 (1%)] Loss: -941336.812500\n",
      "Train Epoch: 212 [11776/54000 (22%)] Loss: -804228.812500\n",
      "Train Epoch: 212 [23040/54000 (43%)] Loss: -835396.062500\n",
      "Train Epoch: 212 [34304/54000 (64%)] Loss: -942256.375000\n",
      "Train Epoch: 212 [45568/54000 (84%)] Loss: -839873.750000\n",
      "    epoch          : 212\n",
      "    loss           : -835733.3824257426\n",
      "    val_loss       : -830646.4961914063\n",
      "Train Epoch: 213 [512/54000 (1%)] Loss: -944667.312500\n",
      "Train Epoch: 213 [11776/54000 (22%)] Loss: -870382.562500\n",
      "Train Epoch: 213 [23040/54000 (43%)] Loss: -839952.687500\n",
      "Train Epoch: 213 [34304/54000 (64%)] Loss: -787342.250000\n",
      "Train Epoch: 213 [45568/54000 (84%)] Loss: -840151.625000\n",
      "    epoch          : 213\n",
      "    loss           : -837347.2141089109\n",
      "    val_loss       : -828687.4197265625\n",
      "Train Epoch: 214 [512/54000 (1%)] Loss: -943597.062500\n",
      "Train Epoch: 214 [11776/54000 (22%)] Loss: -807303.500000\n",
      "Train Epoch: 214 [23040/54000 (43%)] Loss: -781682.875000\n",
      "Train Epoch: 214 [34304/54000 (64%)] Loss: -862230.750000\n",
      "Train Epoch: 214 [45568/54000 (84%)] Loss: -797711.500000\n",
      "    epoch          : 214\n",
      "    loss           : -834176.2636138614\n",
      "    val_loss       : -831191.2572265625\n",
      "Train Epoch: 215 [512/54000 (1%)] Loss: -940558.250000\n",
      "Train Epoch: 215 [11776/54000 (22%)] Loss: -872944.875000\n",
      "Train Epoch: 215 [23040/54000 (43%)] Loss: -840350.875000\n",
      "Train Epoch: 215 [34304/54000 (64%)] Loss: -796588.875000\n",
      "Train Epoch: 215 [45568/54000 (84%)] Loss: -856404.812500\n",
      "    epoch          : 215\n",
      "    loss           : -838275.7469059406\n",
      "    val_loss       : -830688.56875\n",
      "Train Epoch: 216 [512/54000 (1%)] Loss: -943052.625000\n",
      "Train Epoch: 216 [11776/54000 (22%)] Loss: -871264.625000\n",
      "Train Epoch: 216 [23040/54000 (43%)] Loss: -867203.187500\n",
      "Train Epoch: 216 [34304/54000 (64%)] Loss: -840998.625000\n",
      "Train Epoch: 216 [45568/54000 (84%)] Loss: -846823.750000\n",
      "    epoch          : 216\n",
      "    loss           : -838994.2153465346\n",
      "    val_loss       : -833992.5346679688\n",
      "Train Epoch: 217 [512/54000 (1%)] Loss: -876807.250000\n",
      "Train Epoch: 217 [11776/54000 (22%)] Loss: -801021.250000\n",
      "Train Epoch: 217 [23040/54000 (43%)] Loss: -790975.562500\n",
      "Train Epoch: 217 [34304/54000 (64%)] Loss: -845757.500000\n",
      "Train Epoch: 217 [45568/54000 (84%)] Loss: -845194.125000\n",
      "    epoch          : 217\n",
      "    loss           : -840546.5532178218\n",
      "    val_loss       : -833020.0244140625\n",
      "Train Epoch: 218 [512/54000 (1%)] Loss: -946209.625000\n",
      "Train Epoch: 218 [11776/54000 (22%)] Loss: -811035.937500\n",
      "Train Epoch: 218 [23040/54000 (43%)] Loss: -843910.625000\n",
      "Train Epoch: 218 [34304/54000 (64%)] Loss: -867584.750000\n",
      "Train Epoch: 218 [45568/54000 (84%)] Loss: -803298.375000\n",
      "    epoch          : 218\n",
      "    loss           : -840099.7679455446\n",
      "    val_loss       : -834117.8186523437\n",
      "Train Epoch: 219 [512/54000 (1%)] Loss: -948588.250000\n",
      "Train Epoch: 219 [11776/54000 (22%)] Loss: -812856.812500\n",
      "Train Epoch: 219 [23040/54000 (43%)] Loss: -867180.062500\n",
      "Train Epoch: 219 [34304/54000 (64%)] Loss: -950259.687500\n",
      "Train Epoch: 219 [45568/54000 (84%)] Loss: -843514.375000\n",
      "    epoch          : 219\n",
      "    loss           : -842283.3972772277\n",
      "    val_loss       : -835777.94921875\n",
      "Train Epoch: 220 [512/54000 (1%)] Loss: -949834.250000\n",
      "Train Epoch: 220 [11776/54000 (22%)] Loss: -811380.312500\n",
      "Train Epoch: 220 [23040/54000 (43%)] Loss: -795128.812500\n",
      "Train Epoch: 220 [34304/54000 (64%)] Loss: -795642.875000\n",
      "Train Epoch: 220 [45568/54000 (84%)] Loss: -801127.750000\n",
      "    epoch          : 220\n",
      "    loss           : -841138.9108910891\n",
      "    val_loss       : -834881.8640625\n",
      "Train Epoch: 221 [512/54000 (1%)] Loss: -950322.562500\n",
      "Train Epoch: 221 [11776/54000 (22%)] Loss: -870265.562500\n",
      "Train Epoch: 221 [23040/54000 (43%)] Loss: -799088.750000\n",
      "Train Epoch: 221 [34304/54000 (64%)] Loss: -869507.312500\n",
      "Train Epoch: 221 [45568/54000 (84%)] Loss: -805573.937500\n",
      "    epoch          : 221\n",
      "    loss           : -841319.4653465346\n",
      "    val_loss       : -836649.3774414062\n",
      "Train Epoch: 222 [512/54000 (1%)] Loss: -951035.250000\n",
      "Train Epoch: 222 [11776/54000 (22%)] Loss: -816552.062500\n",
      "Train Epoch: 222 [23040/54000 (43%)] Loss: -867227.000000\n",
      "Train Epoch: 222 [34304/54000 (64%)] Loss: -848486.875000\n",
      "Train Epoch: 222 [45568/54000 (84%)] Loss: -848953.750000\n",
      "    epoch          : 222\n",
      "    loss           : -844077.9474009901\n",
      "    val_loss       : -836293.5243164062\n",
      "Train Epoch: 223 [512/54000 (1%)] Loss: -952194.125000\n",
      "Train Epoch: 223 [11776/54000 (22%)] Loss: -815611.500000\n",
      "Train Epoch: 223 [23040/54000 (43%)] Loss: -872339.437500\n",
      "Train Epoch: 223 [34304/54000 (64%)] Loss: -865322.437500\n",
      "Train Epoch: 223 [45568/54000 (84%)] Loss: -804943.875000\n",
      "    epoch          : 223\n",
      "    loss           : -844145.9133663366\n",
      "    val_loss       : -839017.6047851562\n",
      "Train Epoch: 224 [512/54000 (1%)] Loss: -952520.500000\n",
      "Train Epoch: 224 [11776/54000 (22%)] Loss: -818246.812500\n",
      "Train Epoch: 224 [23040/54000 (43%)] Loss: -844557.687500\n",
      "Train Epoch: 224 [34304/54000 (64%)] Loss: -868132.812500\n",
      "Train Epoch: 224 [45568/54000 (84%)] Loss: -846706.812500\n",
      "    epoch          : 224\n",
      "    loss           : -844928.0655940594\n",
      "    val_loss       : -838138.9573242187\n",
      "Train Epoch: 225 [512/54000 (1%)] Loss: -952053.875000\n",
      "Train Epoch: 225 [11776/54000 (22%)] Loss: -850348.875000\n",
      "Train Epoch: 225 [23040/54000 (43%)] Loss: -791135.062500\n",
      "Train Epoch: 225 [34304/54000 (64%)] Loss: -953832.375000\n",
      "Train Epoch: 225 [45568/54000 (84%)] Loss: -846533.812500\n",
      "    epoch          : 225\n",
      "    loss           : -844391.957920792\n",
      "    val_loss       : -839047.817578125\n",
      "Train Epoch: 226 [512/54000 (1%)] Loss: -953663.625000\n",
      "Train Epoch: 226 [11776/54000 (22%)] Loss: -784158.000000\n",
      "Train Epoch: 226 [23040/54000 (43%)] Loss: -803948.000000\n",
      "Train Epoch: 226 [34304/54000 (64%)] Loss: -845216.625000\n",
      "Train Epoch: 226 [45568/54000 (84%)] Loss: -807214.625000\n",
      "    epoch          : 226\n",
      "    loss           : -845021.2667079208\n",
      "    val_loss       : -839493.8442382812\n",
      "Train Epoch: 227 [512/54000 (1%)] Loss: -953364.500000\n",
      "Train Epoch: 227 [11776/54000 (22%)] Loss: -795159.062500\n",
      "Train Epoch: 227 [23040/54000 (43%)] Loss: -803320.500000\n",
      "Train Epoch: 227 [34304/54000 (64%)] Loss: -803044.375000\n",
      "Train Epoch: 227 [45568/54000 (84%)] Loss: -849042.562500\n",
      "    epoch          : 227\n",
      "    loss           : -845883.5080445545\n",
      "    val_loss       : -840475.6453125\n",
      "Train Epoch: 228 [512/54000 (1%)] Loss: -957859.875000\n",
      "Train Epoch: 228 [11776/54000 (22%)] Loss: -880648.750000\n",
      "Train Epoch: 228 [23040/54000 (43%)] Loss: -807076.125000\n",
      "Train Epoch: 228 [34304/54000 (64%)] Loss: -842373.812500\n",
      "Train Epoch: 228 [45568/54000 (84%)] Loss: -799525.125000\n",
      "    epoch          : 228\n",
      "    loss           : -845281.5012376237\n",
      "    val_loss       : -840963.4711914062\n",
      "Train Epoch: 229 [512/54000 (1%)] Loss: -955775.625000\n",
      "Train Epoch: 229 [11776/54000 (22%)] Loss: -865082.437500\n",
      "Train Epoch: 229 [23040/54000 (43%)] Loss: -803415.562500\n",
      "Train Epoch: 229 [34304/54000 (64%)] Loss: -800935.500000\n",
      "Train Epoch: 229 [45568/54000 (84%)] Loss: -814113.875000\n",
      "    epoch          : 229\n",
      "    loss           : -849209.9888613861\n",
      "    val_loss       : -842649.487109375\n",
      "Train Epoch: 230 [512/54000 (1%)] Loss: -957111.437500\n",
      "Train Epoch: 230 [11776/54000 (22%)] Loss: -816783.750000\n",
      "Train Epoch: 230 [23040/54000 (43%)] Loss: -805902.625000\n",
      "Train Epoch: 230 [34304/54000 (64%)] Loss: -852887.562500\n",
      "Train Epoch: 230 [45568/54000 (84%)] Loss: -809499.250000\n",
      "    epoch          : 230\n",
      "    loss           : -849486.4071782178\n",
      "    val_loss       : -841255.3909179687\n",
      "Train Epoch: 231 [512/54000 (1%)] Loss: -957337.375000\n",
      "Train Epoch: 231 [11776/54000 (22%)] Loss: -820626.125000\n",
      "Train Epoch: 231 [23040/54000 (43%)] Loss: -875403.750000\n",
      "Train Epoch: 231 [34304/54000 (64%)] Loss: -806767.125000\n",
      "Train Epoch: 231 [45568/54000 (84%)] Loss: -799064.500000\n",
      "    epoch          : 231\n",
      "    loss           : -850786.4319306931\n",
      "    val_loss       : -843739.8140625\n",
      "Train Epoch: 232 [512/54000 (1%)] Loss: -889428.375000\n",
      "Train Epoch: 232 [11776/54000 (22%)] Loss: -814408.312500\n",
      "Train Epoch: 232 [23040/54000 (43%)] Loss: -797344.375000\n",
      "Train Epoch: 232 [34304/54000 (64%)] Loss: -959108.437500\n",
      "Train Epoch: 232 [45568/54000 (84%)] Loss: -810326.312500\n",
      "    epoch          : 232\n",
      "    loss           : -850383.1256188119\n",
      "    val_loss       : -842489.9098632813\n",
      "Train Epoch: 233 [512/54000 (1%)] Loss: -957424.312500\n",
      "Train Epoch: 233 [11776/54000 (22%)] Loss: -880858.750000\n",
      "Train Epoch: 233 [23040/54000 (43%)] Loss: -802294.562500\n",
      "Train Epoch: 233 [34304/54000 (64%)] Loss: -958970.375000\n",
      "Train Epoch: 233 [45568/54000 (84%)] Loss: -845892.500000\n",
      "    epoch          : 233\n",
      "    loss           : -847018.5680693069\n",
      "    val_loss       : -841536.8405273438\n",
      "Train Epoch: 234 [512/54000 (1%)] Loss: -813354.875000\n",
      "Train Epoch: 234 [11776/54000 (22%)] Loss: -795559.625000\n",
      "Train Epoch: 234 [23040/54000 (43%)] Loss: -799055.250000\n",
      "Train Epoch: 234 [34304/54000 (64%)] Loss: -812129.625000\n",
      "Train Epoch: 234 [45568/54000 (84%)] Loss: -854338.062500\n",
      "    epoch          : 234\n",
      "    loss           : -850532.2902227723\n",
      "    val_loss       : -844790.3701171875\n",
      "Train Epoch: 235 [512/54000 (1%)] Loss: -962715.625000\n",
      "Train Epoch: 235 [11776/54000 (22%)] Loss: -881699.000000\n",
      "Train Epoch: 235 [23040/54000 (43%)] Loss: -815676.250000\n",
      "Train Epoch: 235 [34304/54000 (64%)] Loss: -813686.000000\n",
      "Train Epoch: 235 [45568/54000 (84%)] Loss: -855794.875000\n",
      "    epoch          : 235\n",
      "    loss           : -853069.2091584158\n",
      "    val_loss       : -845570.7163085938\n",
      "Train Epoch: 236 [512/54000 (1%)] Loss: -961343.375000\n",
      "Train Epoch: 236 [11776/54000 (22%)] Loss: -804669.000000\n",
      "Train Epoch: 236 [23040/54000 (43%)] Loss: -876454.437500\n",
      "Train Epoch: 236 [34304/54000 (64%)] Loss: -960427.250000\n",
      "Train Epoch: 236 [45568/54000 (84%)] Loss: -848012.125000\n",
      "    epoch          : 236\n",
      "    loss           : -852050.2357673268\n",
      "    val_loss       : -845016.059375\n",
      "Train Epoch: 237 [512/54000 (1%)] Loss: -963005.687500\n",
      "Train Epoch: 237 [11776/54000 (22%)] Loss: -875759.062500\n",
      "Train Epoch: 237 [23040/54000 (43%)] Loss: -802218.000000\n",
      "Train Epoch: 237 [34304/54000 (64%)] Loss: -815095.312500\n",
      "Train Epoch: 237 [45568/54000 (84%)] Loss: -815168.375000\n",
      "    epoch          : 237\n",
      "    loss           : -853524.5662128713\n",
      "    val_loss       : -847172.5221679688\n",
      "Train Epoch: 238 [512/54000 (1%)] Loss: -963064.250000\n",
      "Train Epoch: 238 [11776/54000 (22%)] Loss: -884707.500000\n",
      "Train Epoch: 238 [23040/54000 (43%)] Loss: -813734.625000\n",
      "Train Epoch: 238 [34304/54000 (64%)] Loss: -961286.375000\n",
      "Train Epoch: 238 [45568/54000 (84%)] Loss: -821572.125000\n",
      "    epoch          : 238\n",
      "    loss           : -854392.3873762377\n",
      "    val_loss       : -846530.7756835937\n",
      "Train Epoch: 239 [512/54000 (1%)] Loss: -964041.187500\n",
      "Train Epoch: 239 [11776/54000 (22%)] Loss: -830744.812500\n",
      "Train Epoch: 239 [23040/54000 (43%)] Loss: -804909.000000\n",
      "Train Epoch: 239 [34304/54000 (64%)] Loss: -816490.687500\n",
      "Train Epoch: 239 [45568/54000 (84%)] Loss: -817933.125000\n",
      "    epoch          : 239\n",
      "    loss           : -855108.5117574257\n",
      "    val_loss       : -848494.168359375\n",
      "Train Epoch: 240 [512/54000 (1%)] Loss: -965712.625000\n",
      "Train Epoch: 240 [11776/54000 (22%)] Loss: -816128.125000\n",
      "Train Epoch: 240 [23040/54000 (43%)] Loss: -822750.500000\n",
      "Train Epoch: 240 [34304/54000 (64%)] Loss: -861583.000000\n",
      "Train Epoch: 240 [45568/54000 (84%)] Loss: -811329.062500\n",
      "    epoch          : 240\n",
      "    loss           : -854818.4047029703\n",
      "    val_loss       : -846949.8016601562\n",
      "Train Epoch: 241 [512/54000 (1%)] Loss: -962099.500000\n",
      "Train Epoch: 241 [11776/54000 (22%)] Loss: -886955.750000\n",
      "Train Epoch: 241 [23040/54000 (43%)] Loss: -815326.437500\n",
      "Train Epoch: 241 [34304/54000 (64%)] Loss: -816504.875000\n",
      "Train Epoch: 241 [45568/54000 (84%)] Loss: -854746.750000\n",
      "    epoch          : 241\n",
      "    loss           : -854680.8465346535\n",
      "    val_loss       : -847949.1758789063\n",
      "Train Epoch: 242 [512/54000 (1%)] Loss: -965051.187500\n",
      "Train Epoch: 242 [11776/54000 (22%)] Loss: -819511.250000\n",
      "Train Epoch: 242 [23040/54000 (43%)] Loss: -806706.812500\n",
      "Train Epoch: 242 [34304/54000 (64%)] Loss: -965895.187500\n",
      "Train Epoch: 242 [45568/54000 (84%)] Loss: -813714.750000\n",
      "    epoch          : 242\n",
      "    loss           : -852497.4115099009\n",
      "    val_loss       : -846294.5028320312\n",
      "Train Epoch: 243 [512/54000 (1%)] Loss: -963307.187500\n",
      "Train Epoch: 243 [11776/54000 (22%)] Loss: -800504.062500\n",
      "Train Epoch: 243 [23040/54000 (43%)] Loss: -855098.687500\n",
      "Train Epoch: 243 [34304/54000 (64%)] Loss: -966173.437500\n",
      "Train Epoch: 243 [45568/54000 (84%)] Loss: -863441.562500\n",
      "    epoch          : 243\n",
      "    loss           : -855185.4653465346\n",
      "    val_loss       : -850077.0077148437\n",
      "Train Epoch: 244 [512/54000 (1%)] Loss: -966835.437500\n",
      "Train Epoch: 244 [11776/54000 (22%)] Loss: -805781.375000\n",
      "Train Epoch: 244 [23040/54000 (43%)] Loss: -814845.562500\n",
      "Train Epoch: 244 [34304/54000 (64%)] Loss: -881258.625000\n",
      "Train Epoch: 244 [45568/54000 (84%)] Loss: -861448.250000\n",
      "    epoch          : 244\n",
      "    loss           : -856821.3428217822\n",
      "    val_loss       : -848867.6162109375\n",
      "Train Epoch: 245 [512/54000 (1%)] Loss: -968899.250000\n",
      "Train Epoch: 245 [11776/54000 (22%)] Loss: -820432.375000\n",
      "Train Epoch: 245 [23040/54000 (43%)] Loss: -880424.250000\n",
      "Train Epoch: 245 [34304/54000 (64%)] Loss: -826563.250000\n",
      "Train Epoch: 245 [45568/54000 (84%)] Loss: -860150.000000\n",
      "    epoch          : 245\n",
      "    loss           : -857935.3428217822\n",
      "    val_loss       : -850892.3166015625\n",
      "Train Epoch: 246 [512/54000 (1%)] Loss: -969081.750000\n",
      "Train Epoch: 246 [11776/54000 (22%)] Loss: -828080.500000\n",
      "Train Epoch: 246 [23040/54000 (43%)] Loss: -822255.500000\n",
      "Train Epoch: 246 [34304/54000 (64%)] Loss: -864079.750000\n",
      "Train Epoch: 246 [45568/54000 (84%)] Loss: -863718.000000\n",
      "    epoch          : 246\n",
      "    loss           : -858712.2246287129\n",
      "    val_loss       : -850662.7869140625\n",
      "Train Epoch: 247 [512/54000 (1%)] Loss: -968857.187500\n",
      "Train Epoch: 247 [11776/54000 (22%)] Loss: -893052.250000\n",
      "Train Epoch: 247 [23040/54000 (43%)] Loss: -817359.875000\n",
      "Train Epoch: 247 [34304/54000 (64%)] Loss: -883312.375000\n",
      "Train Epoch: 247 [45568/54000 (84%)] Loss: -861410.750000\n",
      "    epoch          : 247\n",
      "    loss           : -857724.6738861386\n",
      "    val_loss       : -851163.6740234375\n",
      "Train Epoch: 248 [512/54000 (1%)] Loss: -967609.375000\n",
      "Train Epoch: 248 [11776/54000 (22%)] Loss: -891886.875000\n",
      "Train Epoch: 248 [23040/54000 (43%)] Loss: -810669.125000\n",
      "Train Epoch: 248 [34304/54000 (64%)] Loss: -968840.375000\n",
      "Train Epoch: 248 [45568/54000 (84%)] Loss: -861764.250000\n",
      "    epoch          : 248\n",
      "    loss           : -857447.4591584158\n",
      "    val_loss       : -851875.1381835938\n",
      "Train Epoch: 249 [512/54000 (1%)] Loss: -969902.750000\n",
      "Train Epoch: 249 [11776/54000 (22%)] Loss: -889352.187500\n",
      "Train Epoch: 249 [23040/54000 (43%)] Loss: -882838.750000\n",
      "Train Epoch: 249 [34304/54000 (64%)] Loss: -859367.312500\n",
      "Train Epoch: 249 [45568/54000 (84%)] Loss: -863436.937500\n",
      "    epoch          : 249\n",
      "    loss           : -859651.0525990099\n",
      "    val_loss       : -852774.0485351563\n",
      "Train Epoch: 250 [512/54000 (1%)] Loss: -971089.000000\n",
      "Train Epoch: 250 [11776/54000 (22%)] Loss: -828162.625000\n",
      "Train Epoch: 250 [23040/54000 (43%)] Loss: -888472.125000\n",
      "Train Epoch: 250 [34304/54000 (64%)] Loss: -882526.125000\n",
      "Train Epoch: 250 [45568/54000 (84%)] Loss: -824645.312500\n",
      "    epoch          : 250\n",
      "    loss           : -861136.9993811881\n",
      "    val_loss       : -852705.419921875\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0716_225739/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [512/54000 (1%)] Loss: -973828.000000\n",
      "Train Epoch: 251 [11776/54000 (22%)] Loss: -824183.125000\n",
      "Train Epoch: 251 [23040/54000 (43%)] Loss: -805554.500000\n",
      "Train Epoch: 251 [34304/54000 (64%)] Loss: -859920.625000\n",
      "Train Epoch: 251 [45568/54000 (84%)] Loss: -863882.375000\n",
      "    epoch          : 251\n",
      "    loss           : -859396.2277227723\n",
      "    val_loss       : -851585.1778320313\n",
      "Train Epoch: 252 [512/54000 (1%)] Loss: -970466.375000\n",
      "Train Epoch: 252 [11776/54000 (22%)] Loss: -805541.437500\n",
      "Train Epoch: 252 [23040/54000 (43%)] Loss: -861170.250000\n",
      "Train Epoch: 252 [34304/54000 (64%)] Loss: -883363.562500\n",
      "Train Epoch: 252 [45568/54000 (84%)] Loss: -817594.250000\n",
      "    epoch          : 252\n",
      "    loss           : -860398.7425742574\n",
      "    val_loss       : -852387.728515625\n",
      "Train Epoch: 253 [512/54000 (1%)] Loss: -972200.812500\n",
      "Train Epoch: 253 [11776/54000 (22%)] Loss: -825223.750000\n",
      "Train Epoch: 253 [23040/54000 (43%)] Loss: -822833.062500\n",
      "Train Epoch: 253 [34304/54000 (64%)] Loss: -864600.375000\n",
      "Train Epoch: 253 [45568/54000 (84%)] Loss: -869691.250000\n",
      "    epoch          : 253\n",
      "    loss           : -861499.0228960396\n",
      "    val_loss       : -853067.9305664062\n",
      "Train Epoch: 254 [512/54000 (1%)] Loss: -972484.125000\n",
      "Train Epoch: 254 [11776/54000 (22%)] Loss: -874181.937500\n",
      "Train Epoch: 254 [23040/54000 (43%)] Loss: -878595.625000\n",
      "Train Epoch: 254 [34304/54000 (64%)] Loss: -820243.812500\n",
      "Train Epoch: 254 [45568/54000 (84%)] Loss: -816567.187500\n",
      "    epoch          : 254\n",
      "    loss           : -858806.905940594\n",
      "    val_loss       : -853378.5303710938\n",
      "Train Epoch: 255 [512/54000 (1%)] Loss: -968660.375000\n",
      "Train Epoch: 255 [11776/54000 (22%)] Loss: -886925.687500\n",
      "Train Epoch: 255 [23040/54000 (43%)] Loss: -868902.625000\n",
      "Train Epoch: 255 [34304/54000 (64%)] Loss: -819204.875000\n",
      "Train Epoch: 255 [45568/54000 (84%)] Loss: -865283.625000\n",
      "    epoch          : 255\n",
      "    loss           : -861394.2456683168\n",
      "    val_loss       : -854144.2853515625\n",
      "Train Epoch: 256 [512/54000 (1%)] Loss: -975879.625000\n",
      "Train Epoch: 256 [11776/54000 (22%)] Loss: -863906.375000\n",
      "Train Epoch: 256 [23040/54000 (43%)] Loss: -865109.062500\n",
      "Train Epoch: 256 [34304/54000 (64%)] Loss: -890488.750000\n",
      "Train Epoch: 256 [45568/54000 (84%)] Loss: -867501.000000\n",
      "    epoch          : 256\n",
      "    loss           : -863444.2165841584\n",
      "    val_loss       : -856199.916015625\n",
      "Train Epoch: 257 [512/54000 (1%)] Loss: -974920.937500\n",
      "Train Epoch: 257 [11776/54000 (22%)] Loss: -810665.625000\n",
      "Train Epoch: 257 [23040/54000 (43%)] Loss: -864656.250000\n",
      "Train Epoch: 257 [34304/54000 (64%)] Loss: -868206.750000\n",
      "Train Epoch: 257 [45568/54000 (84%)] Loss: -863897.750000\n",
      "    epoch          : 257\n",
      "    loss           : -863197.5847772277\n",
      "    val_loss       : -855069.8084960937\n",
      "Train Epoch: 258 [512/54000 (1%)] Loss: -973230.875000\n",
      "Train Epoch: 258 [11776/54000 (22%)] Loss: -835082.625000\n",
      "Train Epoch: 258 [23040/54000 (43%)] Loss: -811494.312500\n",
      "Train Epoch: 258 [34304/54000 (64%)] Loss: -822529.937500\n",
      "Train Epoch: 258 [45568/54000 (84%)] Loss: -817617.500000\n",
      "    epoch          : 258\n",
      "    loss           : -863401.2691831683\n",
      "    val_loss       : -855783.5846679688\n",
      "Train Epoch: 259 [512/54000 (1%)] Loss: -977414.625000\n",
      "Train Epoch: 259 [11776/54000 (22%)] Loss: -864151.375000\n",
      "Train Epoch: 259 [23040/54000 (43%)] Loss: -815546.750000\n",
      "Train Epoch: 259 [34304/54000 (64%)] Loss: -864691.312500\n",
      "Train Epoch: 259 [45568/54000 (84%)] Loss: -865602.500000\n",
      "    epoch          : 259\n",
      "    loss           : -863578.3743811881\n",
      "    val_loss       : -854175.6908203125\n",
      "Train Epoch: 260 [512/54000 (1%)] Loss: -974560.500000\n",
      "Train Epoch: 260 [11776/54000 (22%)] Loss: -892274.750000\n",
      "Train Epoch: 260 [23040/54000 (43%)] Loss: -811753.562500\n",
      "Train Epoch: 260 [34304/54000 (64%)] Loss: -974790.062500\n",
      "Train Epoch: 260 [45568/54000 (84%)] Loss: -865393.687500\n",
      "    epoch          : 260\n",
      "    loss           : -861592.4040841584\n",
      "    val_loss       : -855963.7983398438\n",
      "Train Epoch: 261 [512/54000 (1%)] Loss: -900374.875000\n",
      "Train Epoch: 261 [11776/54000 (22%)] Loss: -865688.687500\n",
      "Train Epoch: 261 [23040/54000 (43%)] Loss: -809219.875000\n",
      "Train Epoch: 261 [34304/54000 (64%)] Loss: -865976.750000\n",
      "Train Epoch: 261 [45568/54000 (84%)] Loss: -823403.375000\n",
      "    epoch          : 261\n",
      "    loss           : -864721.4950495049\n",
      "    val_loss       : -857927.99765625\n",
      "Train Epoch: 262 [512/54000 (1%)] Loss: -977107.562500\n",
      "Train Epoch: 262 [11776/54000 (22%)] Loss: -903210.375000\n",
      "Train Epoch: 262 [23040/54000 (43%)] Loss: -831446.000000\n",
      "Train Epoch: 262 [34304/54000 (64%)] Loss: -825353.875000\n",
      "Train Epoch: 262 [45568/54000 (84%)] Loss: -871822.375000\n",
      "    epoch          : 262\n",
      "    loss           : -866656.1745049505\n",
      "    val_loss       : -857240.189453125\n",
      "Train Epoch: 263 [512/54000 (1%)] Loss: -978529.875000\n",
      "Train Epoch: 263 [11776/54000 (22%)] Loss: -830146.562500\n",
      "Train Epoch: 263 [23040/54000 (43%)] Loss: -888552.437500\n",
      "Train Epoch: 263 [34304/54000 (64%)] Loss: -864828.250000\n",
      "Train Epoch: 263 [45568/54000 (84%)] Loss: -867179.125000\n",
      "    epoch          : 263\n",
      "    loss           : -863409.6788366337\n",
      "    val_loss       : -856928.4407226562\n",
      "Train Epoch: 264 [512/54000 (1%)] Loss: -899270.250000\n",
      "Train Epoch: 264 [11776/54000 (22%)] Loss: -904149.812500\n",
      "Train Epoch: 264 [23040/54000 (43%)] Loss: -814989.812500\n",
      "Train Epoch: 264 [34304/54000 (64%)] Loss: -977295.187500\n",
      "Train Epoch: 264 [45568/54000 (84%)] Loss: -873345.500000\n",
      "    epoch          : 264\n",
      "    loss           : -866058.0736386139\n",
      "    val_loss       : -857818.2603515625\n",
      "Train Epoch: 265 [512/54000 (1%)] Loss: -978926.125000\n",
      "Train Epoch: 265 [11776/54000 (22%)] Loss: -898636.687500\n",
      "Train Epoch: 265 [23040/54000 (43%)] Loss: -889750.375000\n",
      "Train Epoch: 265 [34304/54000 (64%)] Loss: -978733.250000\n",
      "Train Epoch: 265 [45568/54000 (84%)] Loss: -869120.250000\n",
      "    epoch          : 265\n",
      "    loss           : -866511.7747524752\n",
      "    val_loss       : -858213.4926757812\n",
      "Train Epoch: 266 [512/54000 (1%)] Loss: -978934.875000\n",
      "Train Epoch: 266 [11776/54000 (22%)] Loss: -804413.187500\n",
      "Train Epoch: 266 [23040/54000 (43%)] Loss: -814144.375000\n",
      "Train Epoch: 266 [34304/54000 (64%)] Loss: -978248.937500\n",
      "Train Epoch: 266 [45568/54000 (84%)] Loss: -867820.000000\n",
      "    epoch          : 266\n",
      "    loss           : -863087.2172029703\n",
      "    val_loss       : -857236.987890625\n",
      "Train Epoch: 267 [512/54000 (1%)] Loss: -980132.250000\n",
      "Train Epoch: 267 [11776/54000 (22%)] Loss: -835174.875000\n",
      "Train Epoch: 267 [23040/54000 (43%)] Loss: -870520.750000\n",
      "Train Epoch: 267 [34304/54000 (64%)] Loss: -872923.250000\n",
      "Train Epoch: 267 [45568/54000 (84%)] Loss: -826970.062500\n",
      "    epoch          : 267\n",
      "    loss           : -866705.9801980198\n",
      "    val_loss       : -858317.3803710938\n",
      "Train Epoch: 268 [512/54000 (1%)] Loss: -977632.187500\n",
      "Train Epoch: 268 [11776/54000 (22%)] Loss: -885335.562500\n",
      "Train Epoch: 268 [23040/54000 (43%)] Loss: -814125.625000\n",
      "Train Epoch: 268 [34304/54000 (64%)] Loss: -815665.437500\n",
      "Train Epoch: 268 [45568/54000 (84%)] Loss: -868885.125000\n",
      "    epoch          : 268\n",
      "    loss           : -868346.8415841584\n",
      "    val_loss       : -859289.358984375\n",
      "Train Epoch: 269 [512/54000 (1%)] Loss: -901419.125000\n",
      "Train Epoch: 269 [11776/54000 (22%)] Loss: -818129.812500\n",
      "Train Epoch: 269 [23040/54000 (43%)] Loss: -886078.750000\n",
      "Train Epoch: 269 [34304/54000 (64%)] Loss: -813922.125000\n",
      "Train Epoch: 269 [45568/54000 (84%)] Loss: -828822.125000\n",
      "    epoch          : 269\n",
      "    loss           : -866258.9696782178\n",
      "    val_loss       : -860056.71875\n",
      "Train Epoch: 270 [512/54000 (1%)] Loss: -983257.250000\n",
      "Train Epoch: 270 [11776/54000 (22%)] Loss: -837767.937500\n",
      "Train Epoch: 270 [23040/54000 (43%)] Loss: -829275.125000\n",
      "Train Epoch: 270 [34304/54000 (64%)] Loss: -832429.125000\n",
      "Train Epoch: 270 [45568/54000 (84%)] Loss: -868904.875000\n",
      "    epoch          : 270\n",
      "    loss           : -868638.385519802\n",
      "    val_loss       : -859579.4614257812\n",
      "Train Epoch: 271 [512/54000 (1%)] Loss: -982624.562500\n",
      "Train Epoch: 271 [11776/54000 (22%)] Loss: -811003.312500\n",
      "Train Epoch: 271 [23040/54000 (43%)] Loss: -869048.562500\n",
      "Train Epoch: 271 [34304/54000 (64%)] Loss: -980739.062500\n",
      "Train Epoch: 271 [45568/54000 (84%)] Loss: -817064.000000\n",
      "    epoch          : 271\n",
      "    loss           : -865143.8787128713\n",
      "    val_loss       : -856473.7079101562\n",
      "Train Epoch: 272 [512/54000 (1%)] Loss: -977320.937500\n",
      "Train Epoch: 272 [11776/54000 (22%)] Loss: -818645.625000\n",
      "Train Epoch: 272 [23040/54000 (43%)] Loss: -824699.000000\n",
      "Train Epoch: 272 [34304/54000 (64%)] Loss: -829741.875000\n",
      "Train Epoch: 272 [45568/54000 (84%)] Loss: -827115.250000\n",
      "    epoch          : 272\n",
      "    loss           : -866279.6349009901\n",
      "    val_loss       : -860262.6580078125\n",
      "Train Epoch: 273 [512/54000 (1%)] Loss: -981637.250000\n",
      "Train Epoch: 273 [11776/54000 (22%)] Loss: -903236.750000\n",
      "Train Epoch: 273 [23040/54000 (43%)] Loss: -833334.875000\n",
      "Train Epoch: 273 [34304/54000 (64%)] Loss: -871328.187500\n",
      "Train Epoch: 273 [45568/54000 (84%)] Loss: -894156.375000\n",
      "    epoch          : 273\n",
      "    loss           : -869790.5259900991\n",
      "    val_loss       : -859533.9514648437\n",
      "Train Epoch: 274 [512/54000 (1%)] Loss: -905834.312500\n",
      "Train Epoch: 274 [11776/54000 (22%)] Loss: -837510.250000\n",
      "Train Epoch: 274 [23040/54000 (43%)] Loss: -892518.812500\n",
      "Train Epoch: 274 [34304/54000 (64%)] Loss: -870594.250000\n",
      "Train Epoch: 274 [45568/54000 (84%)] Loss: -871217.500000\n",
      "    epoch          : 274\n",
      "    loss           : -869337.207920792\n",
      "    val_loss       : -859995.1965820312\n",
      "Train Epoch: 275 [512/54000 (1%)] Loss: -904715.687500\n",
      "Train Epoch: 275 [11776/54000 (22%)] Loss: -814201.500000\n",
      "Train Epoch: 275 [23040/54000 (43%)] Loss: -890865.062500\n",
      "Train Epoch: 275 [34304/54000 (64%)] Loss: -870487.375000\n",
      "Train Epoch: 275 [45568/54000 (84%)] Loss: -820302.812500\n",
      "    epoch          : 275\n",
      "    loss           : -867761.0451732674\n",
      "    val_loss       : -860404.2790039063\n",
      "Train Epoch: 276 [512/54000 (1%)] Loss: -981712.812500\n",
      "Train Epoch: 276 [11776/54000 (22%)] Loss: -836424.500000\n",
      "Train Epoch: 276 [23040/54000 (43%)] Loss: -825693.625000\n",
      "Train Epoch: 276 [34304/54000 (64%)] Loss: -831564.000000\n",
      "Train Epoch: 276 [45568/54000 (84%)] Loss: -874778.500000\n",
      "    epoch          : 276\n",
      "    loss           : -869609.9709158416\n",
      "    val_loss       : -860376.4975585938\n",
      "Train Epoch: 277 [512/54000 (1%)] Loss: -982735.875000\n",
      "Train Epoch: 277 [11776/54000 (22%)] Loss: -816848.312500\n",
      "Train Epoch: 277 [23040/54000 (43%)] Loss: -816468.375000\n",
      "Train Epoch: 277 [34304/54000 (64%)] Loss: -893142.937500\n",
      "Train Epoch: 277 [45568/54000 (84%)] Loss: -873373.000000\n",
      "    epoch          : 277\n",
      "    loss           : -868981.479579208\n",
      "    val_loss       : -860470.6296875\n",
      "Train Epoch: 278 [512/54000 (1%)] Loss: -987177.062500\n",
      "Train Epoch: 278 [11776/54000 (22%)] Loss: -834268.000000\n",
      "Train Epoch: 278 [23040/54000 (43%)] Loss: -892422.187500\n",
      "Train Epoch: 278 [34304/54000 (64%)] Loss: -876187.500000\n",
      "Train Epoch: 278 [45568/54000 (84%)] Loss: -820023.250000\n",
      "    epoch          : 278\n",
      "    loss           : -870795.2741336634\n",
      "    val_loss       : -861166.2419921875\n",
      "Train Epoch: 279 [512/54000 (1%)] Loss: -985370.125000\n",
      "Train Epoch: 279 [11776/54000 (22%)] Loss: -831610.625000\n",
      "Train Epoch: 279 [23040/54000 (43%)] Loss: -818333.937500\n",
      "Train Epoch: 279 [34304/54000 (64%)] Loss: -869784.125000\n",
      "Train Epoch: 279 [45568/54000 (84%)] Loss: -875815.875000\n",
      "    epoch          : 279\n",
      "    loss           : -871109.948019802\n",
      "    val_loss       : -861448.7853515625\n",
      "Train Epoch: 280 [512/54000 (1%)] Loss: -987148.687500\n",
      "Train Epoch: 280 [11776/54000 (22%)] Loss: -831625.625000\n",
      "Train Epoch: 280 [23040/54000 (43%)] Loss: -876583.250000\n",
      "Train Epoch: 280 [34304/54000 (64%)] Loss: -887261.750000\n",
      "Train Epoch: 280 [45568/54000 (84%)] Loss: -868784.687500\n",
      "    epoch          : 280\n",
      "    loss           : -869708.6287128713\n",
      "    val_loss       : -857715.61640625\n",
      "Train Epoch: 281 [512/54000 (1%)] Loss: -983177.562500\n",
      "Train Epoch: 281 [11776/54000 (22%)] Loss: -894265.625000\n",
      "Train Epoch: 281 [23040/54000 (43%)] Loss: -806828.375000\n",
      "Train Epoch: 281 [34304/54000 (64%)] Loss: -828903.625000\n",
      "Train Epoch: 281 [45568/54000 (84%)] Loss: -872453.687500\n",
      "    epoch          : 281\n",
      "    loss           : -866323.0631188119\n",
      "    val_loss       : -861347.6609375\n",
      "Train Epoch: 282 [512/54000 (1%)] Loss: -986107.500000\n",
      "Train Epoch: 282 [11776/54000 (22%)] Loss: -835419.812500\n",
      "Train Epoch: 282 [23040/54000 (43%)] Loss: -870547.312500\n",
      "Train Epoch: 282 [34304/54000 (64%)] Loss: -822824.687500\n",
      "Train Epoch: 282 [45568/54000 (84%)] Loss: -872165.687500\n",
      "    epoch          : 282\n",
      "    loss           : -870140.5841584158\n",
      "    val_loss       : -862500.8130859375\n",
      "Train Epoch: 283 [512/54000 (1%)] Loss: -986660.750000\n",
      "Train Epoch: 283 [11776/54000 (22%)] Loss: -835975.687500\n",
      "Train Epoch: 283 [23040/54000 (43%)] Loss: -872219.625000\n",
      "Train Epoch: 283 [34304/54000 (64%)] Loss: -818071.125000\n",
      "Train Epoch: 283 [45568/54000 (84%)] Loss: -830216.437500\n",
      "    epoch          : 283\n",
      "    loss           : -872061.5222772277\n",
      "    val_loss       : -861835.1837890625\n",
      "Train Epoch: 284 [512/54000 (1%)] Loss: -987980.750000\n",
      "Train Epoch: 284 [11776/54000 (22%)] Loss: -904316.062500\n",
      "Train Epoch: 284 [23040/54000 (43%)] Loss: -830024.875000\n",
      "Train Epoch: 284 [34304/54000 (64%)] Loss: -875913.625000\n",
      "Train Epoch: 284 [45568/54000 (84%)] Loss: -878285.062500\n",
      "    epoch          : 284\n",
      "    loss           : -872489.1342821782\n",
      "    val_loss       : -862137.3453125\n",
      "Train Epoch: 285 [512/54000 (1%)] Loss: -988206.937500\n",
      "Train Epoch: 285 [11776/54000 (22%)] Loss: -839221.687500\n",
      "Train Epoch: 285 [23040/54000 (43%)] Loss: -896154.187500\n",
      "Train Epoch: 285 [34304/54000 (64%)] Loss: -988147.375000\n",
      "Train Epoch: 285 [45568/54000 (84%)] Loss: -879053.062500\n",
      "    epoch          : 285\n",
      "    loss           : -872620.8100247525\n",
      "    val_loss       : -862619.0395507812\n",
      "Train Epoch: 286 [512/54000 (1%)] Loss: -988186.062500\n",
      "Train Epoch: 286 [11776/54000 (22%)] Loss: -907549.500000\n",
      "Train Epoch: 286 [23040/54000 (43%)] Loss: -875582.000000\n",
      "Train Epoch: 286 [34304/54000 (64%)] Loss: -893327.000000\n",
      "Train Epoch: 286 [45568/54000 (84%)] Loss: -878230.312500\n",
      "    epoch          : 286\n",
      "    loss           : -871875.7283415842\n",
      "    val_loss       : -858896.3740234375\n",
      "Train Epoch: 287 [512/54000 (1%)] Loss: -988233.687500\n",
      "Train Epoch: 287 [11776/54000 (22%)] Loss: -836749.625000\n",
      "Train Epoch: 287 [23040/54000 (43%)] Loss: -875156.875000\n",
      "Train Epoch: 287 [34304/54000 (64%)] Loss: -987293.562500\n",
      "Train Epoch: 287 [45568/54000 (84%)] Loss: -872343.375000\n",
      "    epoch          : 287\n",
      "    loss           : -870584.6893564357\n",
      "    val_loss       : -862591.6793945313\n",
      "Train Epoch: 288 [512/54000 (1%)] Loss: -985894.000000\n",
      "Train Epoch: 288 [11776/54000 (22%)] Loss: -836626.812500\n",
      "Train Epoch: 288 [23040/54000 (43%)] Loss: -877258.937500\n",
      "Train Epoch: 288 [34304/54000 (64%)] Loss: -868568.375000\n",
      "Train Epoch: 288 [45568/54000 (84%)] Loss: -877571.687500\n",
      "    epoch          : 288\n",
      "    loss           : -871017.4337871287\n",
      "    val_loss       : -862058.1516601562\n",
      "Train Epoch: 289 [512/54000 (1%)] Loss: -988621.250000\n",
      "Train Epoch: 289 [11776/54000 (22%)] Loss: -819588.937500\n",
      "Train Epoch: 289 [23040/54000 (43%)] Loss: -832316.000000\n",
      "Train Epoch: 289 [34304/54000 (64%)] Loss: -831000.312500\n",
      "Train Epoch: 289 [45568/54000 (84%)] Loss: -835363.937500\n",
      "    epoch          : 289\n",
      "    loss           : -872627.2654702971\n",
      "    val_loss       : -863584.8926757813\n",
      "Train Epoch: 290 [512/54000 (1%)] Loss: -908898.437500\n",
      "Train Epoch: 290 [11776/54000 (22%)] Loss: -837727.937500\n",
      "Train Epoch: 290 [23040/54000 (43%)] Loss: -896009.437500\n",
      "Train Epoch: 290 [34304/54000 (64%)] Loss: -818552.000000\n",
      "Train Epoch: 290 [45568/54000 (84%)] Loss: -879959.375000\n",
      "    epoch          : 290\n",
      "    loss           : -873232.6349009901\n",
      "    val_loss       : -864180.938671875\n",
      "Train Epoch: 291 [512/54000 (1%)] Loss: -988136.875000\n",
      "Train Epoch: 291 [11776/54000 (22%)] Loss: -836581.312500\n",
      "Train Epoch: 291 [23040/54000 (43%)] Loss: -840672.062500\n",
      "Train Epoch: 291 [34304/54000 (64%)] Loss: -990371.250000\n",
      "Train Epoch: 291 [45568/54000 (84%)] Loss: -876452.312500\n",
      "    epoch          : 291\n",
      "    loss           : -873675.2852722772\n",
      "    val_loss       : -862090.3162109375\n",
      "Train Epoch: 292 [512/54000 (1%)] Loss: -990507.687500\n",
      "Train Epoch: 292 [11776/54000 (22%)] Loss: -836876.812500\n",
      "Train Epoch: 292 [23040/54000 (43%)] Loss: -813530.500000\n",
      "Train Epoch: 292 [34304/54000 (64%)] Loss: -882759.062500\n",
      "Train Epoch: 292 [45568/54000 (84%)] Loss: -879271.312500\n",
      "    epoch          : 292\n",
      "    loss           : -871641.3032178218\n",
      "    val_loss       : -863452.1981445312\n",
      "Train Epoch: 293 [512/54000 (1%)] Loss: -990175.250000\n",
      "Train Epoch: 293 [11776/54000 (22%)] Loss: -908157.250000\n",
      "Train Epoch: 293 [23040/54000 (43%)] Loss: -829813.687500\n",
      "Train Epoch: 293 [34304/54000 (64%)] Loss: -990645.500000\n",
      "Train Epoch: 293 [45568/54000 (84%)] Loss: -833263.625000\n",
      "    epoch          : 293\n",
      "    loss           : -873658.0457920792\n",
      "    val_loss       : -863701.75078125\n",
      "Train Epoch: 294 [512/54000 (1%)] Loss: -914307.250000\n",
      "Train Epoch: 294 [11776/54000 (22%)] Loss: -840431.562500\n",
      "Train Epoch: 294 [23040/54000 (43%)] Loss: -822033.312500\n",
      "Train Epoch: 294 [34304/54000 (64%)] Loss: -837120.375000\n",
      "Train Epoch: 294 [45568/54000 (84%)] Loss: -837098.875000\n",
      "    epoch          : 294\n",
      "    loss           : -873697.4022277228\n",
      "    val_loss       : -863572.4004882813\n",
      "Train Epoch: 295 [512/54000 (1%)] Loss: -990036.250000\n",
      "Train Epoch: 295 [11776/54000 (22%)] Loss: -831465.500000\n",
      "Train Epoch: 295 [23040/54000 (43%)] Loss: -812050.187500\n",
      "Train Epoch: 295 [34304/54000 (64%)] Loss: -875969.062500\n",
      "Train Epoch: 295 [45568/54000 (84%)] Loss: -829384.750000\n",
      "    epoch          : 295\n",
      "    loss           : -872311.2153465346\n",
      "    val_loss       : -864211.5375976562\n",
      "Train Epoch: 296 [512/54000 (1%)] Loss: -991690.375000\n",
      "Train Epoch: 296 [11776/54000 (22%)] Loss: -895958.500000\n",
      "Train Epoch: 296 [23040/54000 (43%)] Loss: -818750.687500\n",
      "Train Epoch: 296 [34304/54000 (64%)] Loss: -825844.875000\n",
      "Train Epoch: 296 [45568/54000 (84%)] Loss: -832601.937500\n",
      "    epoch          : 296\n",
      "    loss           : -874929.5117574257\n",
      "    val_loss       : -864211.203125\n",
      "Train Epoch: 297 [512/54000 (1%)] Loss: -992154.750000\n",
      "Train Epoch: 297 [11776/54000 (22%)] Loss: -838417.375000\n",
      "Train Epoch: 297 [23040/54000 (43%)] Loss: -820101.687500\n",
      "Train Epoch: 297 [34304/54000 (64%)] Loss: -881346.812500\n",
      "Train Epoch: 297 [45568/54000 (84%)] Loss: -877196.250000\n",
      "    epoch          : 297\n",
      "    loss           : -875203.8861386139\n",
      "    val_loss       : -864468.19140625\n",
      "Train Epoch: 298 [512/54000 (1%)] Loss: -897310.125000\n",
      "Train Epoch: 298 [11776/54000 (22%)] Loss: -899185.125000\n",
      "Train Epoch: 298 [23040/54000 (43%)] Loss: -836280.500000\n",
      "Train Epoch: 298 [34304/54000 (64%)] Loss: -832632.750000\n",
      "Train Epoch: 298 [45568/54000 (84%)] Loss: -881315.937500\n",
      "    epoch          : 298\n",
      "    loss           : -873349.8459158416\n",
      "    val_loss       : -864526.8201171875\n",
      "Train Epoch: 299 [512/54000 (1%)] Loss: -991818.000000\n",
      "Train Epoch: 299 [11776/54000 (22%)] Loss: -836031.812500\n",
      "Train Epoch: 299 [23040/54000 (43%)] Loss: -837647.812500\n",
      "Train Epoch: 299 [34304/54000 (64%)] Loss: -826431.875000\n",
      "Train Epoch: 299 [45568/54000 (84%)] Loss: -875907.375000\n",
      "    epoch          : 299\n",
      "    loss           : -873345.2233910891\n",
      "    val_loss       : -862297.8640625\n",
      "Train Epoch: 300 [512/54000 (1%)] Loss: -991869.187500\n",
      "Train Epoch: 300 [11776/54000 (22%)] Loss: -837143.750000\n",
      "Train Epoch: 300 [23040/54000 (43%)] Loss: -834884.125000\n",
      "Train Epoch: 300 [34304/54000 (64%)] Loss: -896227.812500\n",
      "Train Epoch: 300 [45568/54000 (84%)] Loss: -881259.750000\n",
      "    epoch          : 300\n",
      "    loss           : -873615.7691831683\n",
      "    val_loss       : -865378.3720703125\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0716_225739/checkpoint-epoch300.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 301 [512/54000 (1%)] Loss: -993447.375000\n",
      "Train Epoch: 301 [11776/54000 (22%)] Loss: -841039.375000\n",
      "Train Epoch: 301 [23040/54000 (43%)] Loss: -820471.375000\n",
      "Train Epoch: 301 [34304/54000 (64%)] Loss: -991737.000000\n",
      "Train Epoch: 301 [45568/54000 (84%)] Loss: -882256.375000\n",
      "    epoch          : 301\n",
      "    loss           : -875632.0600247525\n",
      "    val_loss       : -865484.9493164063\n",
      "Train Epoch: 302 [512/54000 (1%)] Loss: -994003.125000\n",
      "Train Epoch: 302 [11776/54000 (22%)] Loss: -897922.250000\n",
      "Train Epoch: 302 [23040/54000 (43%)] Loss: -878410.312500\n",
      "Train Epoch: 302 [34304/54000 (64%)] Loss: -991771.937500\n",
      "Train Epoch: 302 [45568/54000 (84%)] Loss: -832883.687500\n",
      "    epoch          : 302\n",
      "    loss           : -876300.7759900991\n",
      "    val_loss       : -865967.4889648438\n",
      "Train Epoch: 303 [512/54000 (1%)] Loss: -993126.250000\n",
      "Train Epoch: 303 [11776/54000 (22%)] Loss: -842571.750000\n",
      "Train Epoch: 303 [23040/54000 (43%)] Loss: -839797.125000\n",
      "Train Epoch: 303 [34304/54000 (64%)] Loss: -825210.375000\n",
      "Train Epoch: 303 [45568/54000 (84%)] Loss: -882603.812500\n",
      "    epoch          : 303\n",
      "    loss           : -875675.7753712871\n",
      "    val_loss       : -865444.6869140625\n",
      "Train Epoch: 304 [512/54000 (1%)] Loss: -841977.875000\n",
      "Train Epoch: 304 [11776/54000 (22%)] Loss: -905516.125000\n",
      "Train Epoch: 304 [23040/54000 (43%)] Loss: -843341.312500\n",
      "Train Epoch: 304 [34304/54000 (64%)] Loss: -880732.500000\n",
      "Train Epoch: 304 [45568/54000 (84%)] Loss: -834398.000000\n",
      "    epoch          : 304\n",
      "    loss           : -876748.2091584158\n",
      "    val_loss       : -865780.64453125\n",
      "Train Epoch: 305 [512/54000 (1%)] Loss: -994372.625000\n",
      "Train Epoch: 305 [11776/54000 (22%)] Loss: -907749.437500\n",
      "Train Epoch: 305 [23040/54000 (43%)] Loss: -882861.000000\n",
      "Train Epoch: 305 [34304/54000 (64%)] Loss: -885404.000000\n",
      "Train Epoch: 305 [45568/54000 (84%)] Loss: -826244.750000\n",
      "    epoch          : 305\n",
      "    loss           : -877795.0365099009\n",
      "    val_loss       : -865639.1833984375\n",
      "Train Epoch: 306 [512/54000 (1%)] Loss: -994979.750000\n",
      "Train Epoch: 306 [11776/54000 (22%)] Loss: -883182.625000\n",
      "Train Epoch: 306 [23040/54000 (43%)] Loss: -885646.625000\n",
      "Train Epoch: 306 [34304/54000 (64%)] Loss: -995152.187500\n",
      "Train Epoch: 306 [45568/54000 (84%)] Loss: -884923.500000\n",
      "    epoch          : 306\n",
      "    loss           : -876466.3867574257\n",
      "    val_loss       : -863852.5073242188\n",
      "Train Epoch: 307 [512/54000 (1%)] Loss: -992849.750000\n",
      "Train Epoch: 307 [11776/54000 (22%)] Loss: -843080.125000\n",
      "Train Epoch: 307 [23040/54000 (43%)] Loss: -844553.375000\n",
      "Train Epoch: 307 [34304/54000 (64%)] Loss: -994323.750000\n",
      "Train Epoch: 307 [45568/54000 (84%)] Loss: -831721.000000\n",
      "    epoch          : 307\n",
      "    loss           : -876241.2865099009\n",
      "    val_loss       : -866910.70234375\n",
      "Train Epoch: 308 [512/54000 (1%)] Loss: -992963.625000\n",
      "Train Epoch: 308 [11776/54000 (22%)] Loss: -905290.312500\n",
      "Train Epoch: 308 [23040/54000 (43%)] Loss: -842376.812500\n",
      "Train Epoch: 308 [34304/54000 (64%)] Loss: -897291.812500\n",
      "Train Epoch: 308 [45568/54000 (84%)] Loss: -880146.312500\n",
      "    epoch          : 308\n",
      "    loss           : -877431.5816831683\n",
      "    val_loss       : -864571.6584960937\n",
      "Train Epoch: 309 [512/54000 (1%)] Loss: -991137.750000\n",
      "Train Epoch: 309 [11776/54000 (22%)] Loss: -832635.562500\n",
      "Train Epoch: 309 [23040/54000 (43%)] Loss: -842109.000000\n",
      "Train Epoch: 309 [34304/54000 (64%)] Loss: -990740.250000\n",
      "Train Epoch: 309 [45568/54000 (84%)] Loss: -867511.312500\n",
      "    epoch          : 309\n",
      "    loss           : -872738.0148514851\n",
      "    val_loss       : -861892.2862304688\n",
      "Train Epoch: 310 [512/54000 (1%)] Loss: -829021.750000\n",
      "Train Epoch: 310 [11776/54000 (22%)] Loss: -836044.312500\n",
      "Train Epoch: 310 [23040/54000 (43%)] Loss: -878520.125000\n",
      "Train Epoch: 310 [34304/54000 (64%)] Loss: -992297.312500\n",
      "Train Epoch: 310 [45568/54000 (84%)] Loss: -872123.687500\n",
      "    epoch          : 310\n",
      "    loss           : -872965.3483910891\n",
      "    val_loss       : -864841.9232421875\n",
      "Train Epoch: 311 [512/54000 (1%)] Loss: -916775.562500\n",
      "Train Epoch: 311 [11776/54000 (22%)] Loss: -881214.625000\n",
      "Train Epoch: 311 [23040/54000 (43%)] Loss: -879806.687500\n",
      "Train Epoch: 311 [34304/54000 (64%)] Loss: -882386.312500\n",
      "Train Epoch: 311 [45568/54000 (84%)] Loss: -878933.000000\n",
      "    epoch          : 311\n",
      "    loss           : -877080.7196782178\n",
      "    val_loss       : -866872.6538085938\n",
      "Train Epoch: 312 [512/54000 (1%)] Loss: -997642.000000\n",
      "Train Epoch: 312 [11776/54000 (22%)] Loss: -896141.250000\n",
      "Train Epoch: 312 [23040/54000 (43%)] Loss: -898536.875000\n",
      "Train Epoch: 312 [34304/54000 (64%)] Loss: -899844.562500\n",
      "Train Epoch: 312 [45568/54000 (84%)] Loss: -831679.750000\n",
      "    epoch          : 312\n",
      "    loss           : -877936.3768564357\n",
      "    val_loss       : -866600.1263671875\n",
      "Train Epoch: 313 [512/54000 (1%)] Loss: -998142.000000\n",
      "Train Epoch: 313 [11776/54000 (22%)] Loss: -839883.375000\n",
      "Train Epoch: 313 [23040/54000 (43%)] Loss: -881486.687500\n",
      "Train Epoch: 313 [34304/54000 (64%)] Loss: -839221.437500\n",
      "Train Epoch: 313 [45568/54000 (84%)] Loss: -882557.000000\n",
      "    epoch          : 313\n",
      "    loss           : -877460.5612623763\n",
      "    val_loss       : -866502.7249023437\n",
      "Train Epoch: 314 [512/54000 (1%)] Loss: -886657.250000\n",
      "Train Epoch: 314 [11776/54000 (22%)] Loss: -876145.875000\n",
      "Train Epoch: 314 [23040/54000 (43%)] Loss: -842343.750000\n",
      "Train Epoch: 314 [34304/54000 (64%)] Loss: -879677.500000\n",
      "Train Epoch: 314 [45568/54000 (84%)] Loss: -883587.000000\n",
      "    epoch          : 314\n",
      "    loss           : -876698.3910891089\n",
      "    val_loss       : -866979.7693359375\n",
      "Train Epoch: 315 [512/54000 (1%)] Loss: -994402.500000\n",
      "Train Epoch: 315 [11776/54000 (22%)] Loss: -882825.875000\n",
      "Train Epoch: 315 [23040/54000 (43%)] Loss: -845452.062500\n",
      "Train Epoch: 315 [34304/54000 (64%)] Loss: -997034.437500\n",
      "Train Epoch: 315 [45568/54000 (84%)] Loss: -877846.250000\n",
      "    epoch          : 315\n",
      "    loss           : -878729.1441831683\n",
      "    val_loss       : -864375.8036132812\n",
      "Train Epoch: 316 [512/54000 (1%)] Loss: -998175.375000\n",
      "Train Epoch: 316 [11776/54000 (22%)] Loss: -868041.687500\n",
      "Train Epoch: 316 [23040/54000 (43%)] Loss: -822132.625000\n",
      "Train Epoch: 316 [34304/54000 (64%)] Loss: -877601.375000\n",
      "Train Epoch: 316 [45568/54000 (84%)] Loss: -826175.687500\n",
      "    epoch          : 316\n",
      "    loss           : -874099.8762376237\n",
      "    val_loss       : -865376.3811523437\n",
      "Train Epoch: 317 [512/54000 (1%)] Loss: -996501.375000\n",
      "Train Epoch: 317 [11776/54000 (22%)] Loss: -843820.625000\n",
      "Train Epoch: 317 [23040/54000 (43%)] Loss: -831648.375000\n",
      "Train Epoch: 317 [34304/54000 (64%)] Loss: -884822.750000\n",
      "Train Epoch: 317 [45568/54000 (84%)] Loss: -881532.625000\n",
      "    epoch          : 317\n",
      "    loss           : -877605.926980198\n",
      "    val_loss       : -866568.8462890625\n",
      "Train Epoch: 318 [512/54000 (1%)] Loss: -995708.000000\n",
      "Train Epoch: 318 [11776/54000 (22%)] Loss: -825969.062500\n",
      "Train Epoch: 318 [23040/54000 (43%)] Loss: -882644.562500\n",
      "Train Epoch: 318 [34304/54000 (64%)] Loss: -877316.062500\n",
      "Train Epoch: 318 [45568/54000 (84%)] Loss: -880468.875000\n",
      "    epoch          : 318\n",
      "    loss           : -876632.5959158416\n",
      "    val_loss       : -865896.8051757812\n",
      "Train Epoch: 319 [512/54000 (1%)] Loss: -914099.250000\n",
      "Train Epoch: 319 [11776/54000 (22%)] Loss: -834347.562500\n",
      "Train Epoch: 319 [23040/54000 (43%)] Loss: -824209.625000\n",
      "Train Epoch: 319 [34304/54000 (64%)] Loss: -999952.625000\n",
      "Train Epoch: 319 [45568/54000 (84%)] Loss: -886343.500000\n",
      "    epoch          : 319\n",
      "    loss           : -878562.1126237623\n",
      "    val_loss       : -867169.3530273438\n",
      "Train Epoch: 320 [512/54000 (1%)] Loss: -996739.750000\n",
      "Train Epoch: 320 [11776/54000 (22%)] Loss: -845304.750000\n",
      "Train Epoch: 320 [23040/54000 (43%)] Loss: -888257.750000\n",
      "Train Epoch: 320 [34304/54000 (64%)] Loss: -823202.437500\n",
      "Train Epoch: 320 [45568/54000 (84%)] Loss: -885820.375000\n",
      "    epoch          : 320\n",
      "    loss           : -880057.9758663366\n",
      "    val_loss       : -868016.7083007812\n",
      "Train Epoch: 321 [512/54000 (1%)] Loss: -1001397.875000\n",
      "Train Epoch: 321 [11776/54000 (22%)] Loss: -913512.625000\n",
      "Train Epoch: 321 [23040/54000 (43%)] Loss: -831895.437500\n",
      "Train Epoch: 321 [34304/54000 (64%)] Loss: -882754.750000\n",
      "Train Epoch: 321 [45568/54000 (84%)] Loss: -883303.562500\n",
      "    epoch          : 321\n",
      "    loss           : -879655.3149752475\n",
      "    val_loss       : -866321.3482421875\n",
      "Train Epoch: 322 [512/54000 (1%)] Loss: -914105.500000\n",
      "Train Epoch: 322 [11776/54000 (22%)] Loss: -872382.937500\n",
      "Train Epoch: 322 [23040/54000 (43%)] Loss: -842484.875000\n",
      "Train Epoch: 322 [34304/54000 (64%)] Loss: -891527.062500\n",
      "Train Epoch: 322 [45568/54000 (84%)] Loss: -875837.937500\n",
      "    epoch          : 322\n",
      "    loss           : -875563.5111386139\n",
      "    val_loss       : -864546.4452148437\n",
      "Train Epoch: 323 [512/54000 (1%)] Loss: -999589.750000\n",
      "Train Epoch: 323 [11776/54000 (22%)] Loss: -840928.937500\n",
      "Train Epoch: 323 [23040/54000 (43%)] Loss: -817234.625000\n",
      "Train Epoch: 323 [34304/54000 (64%)] Loss: -997811.125000\n",
      "Train Epoch: 323 [45568/54000 (84%)] Loss: -831866.625000\n",
      "    epoch          : 323\n",
      "    loss           : -876801.9548267326\n",
      "    val_loss       : -867158.3053710938\n",
      "Train Epoch: 324 [512/54000 (1%)] Loss: -999549.562500\n",
      "Train Epoch: 324 [11776/54000 (22%)] Loss: -881353.312500\n",
      "Train Epoch: 324 [23040/54000 (43%)] Loss: -884071.375000\n",
      "Train Epoch: 324 [34304/54000 (64%)] Loss: -827730.875000\n",
      "Train Epoch: 324 [45568/54000 (84%)] Loss: -885981.437500\n",
      "    epoch          : 324\n",
      "    loss           : -880003.8681930694\n",
      "    val_loss       : -868041.401171875\n",
      "Train Epoch: 325 [512/54000 (1%)] Loss: -996139.625000\n",
      "Train Epoch: 325 [11776/54000 (22%)] Loss: -846763.250000\n",
      "Train Epoch: 325 [23040/54000 (43%)] Loss: -831947.125000\n",
      "Train Epoch: 325 [34304/54000 (64%)] Loss: -900890.000000\n",
      "Train Epoch: 325 [45568/54000 (84%)] Loss: -886489.250000\n",
      "    epoch          : 325\n",
      "    loss           : -880456.7778465346\n",
      "    val_loss       : -866834.9907226562\n",
      "Train Epoch: 326 [512/54000 (1%)] Loss: -1001404.375000\n",
      "Train Epoch: 326 [11776/54000 (22%)] Loss: -837317.125000\n",
      "Train Epoch: 326 [23040/54000 (43%)] Loss: -843970.312500\n",
      "Train Epoch: 326 [34304/54000 (64%)] Loss: -875818.312500\n",
      "Train Epoch: 326 [45568/54000 (84%)] Loss: -831324.500000\n",
      "    epoch          : 326\n",
      "    loss           : -879199.1299504951\n",
      "    val_loss       : -868056.39375\n",
      "Train Epoch: 327 [512/54000 (1%)] Loss: -995973.125000\n",
      "Train Epoch: 327 [11776/54000 (22%)] Loss: -844857.375000\n",
      "Train Epoch: 327 [23040/54000 (43%)] Loss: -880941.187500\n",
      "Train Epoch: 327 [34304/54000 (64%)] Loss: -899530.062500\n",
      "Train Epoch: 327 [45568/54000 (84%)] Loss: -831004.187500\n",
      "    epoch          : 327\n",
      "    loss           : -879519.3118811881\n",
      "    val_loss       : -868328.895703125\n",
      "Train Epoch: 328 [512/54000 (1%)] Loss: -1001126.375000\n",
      "Train Epoch: 328 [11776/54000 (22%)] Loss: -886925.625000\n",
      "Train Epoch: 328 [23040/54000 (43%)] Loss: -829778.125000\n",
      "Train Epoch: 328 [34304/54000 (64%)] Loss: -885829.000000\n",
      "Train Epoch: 328 [45568/54000 (84%)] Loss: -829432.750000\n",
      "    epoch          : 328\n",
      "    loss           : -880864.2431930694\n",
      "    val_loss       : -868581.5041015625\n",
      "Train Epoch: 329 [512/54000 (1%)] Loss: -916964.500000\n",
      "Train Epoch: 329 [11776/54000 (22%)] Loss: -845989.875000\n",
      "Train Epoch: 329 [23040/54000 (43%)] Loss: -838567.937500\n",
      "Train Epoch: 329 [34304/54000 (64%)] Loss: -834059.500000\n",
      "Train Epoch: 329 [45568/54000 (84%)] Loss: -879219.750000\n",
      "    epoch          : 329\n",
      "    loss           : -879976.5773514851\n",
      "    val_loss       : -866795.1057617187\n",
      "Train Epoch: 330 [512/54000 (1%)] Loss: -915562.437500\n",
      "Train Epoch: 330 [11776/54000 (22%)] Loss: -840543.250000\n",
      "Train Epoch: 330 [23040/54000 (43%)] Loss: -832136.750000\n",
      "Train Epoch: 330 [34304/54000 (64%)] Loss: -825754.625000\n",
      "Train Epoch: 330 [45568/54000 (84%)] Loss: -886356.250000\n",
      "    epoch          : 330\n",
      "    loss           : -877602.5816831683\n",
      "    val_loss       : -867452.9141601563\n",
      "Train Epoch: 331 [512/54000 (1%)] Loss: -1002037.500000\n",
      "Train Epoch: 331 [11776/54000 (22%)] Loss: -844419.937500\n",
      "Train Epoch: 331 [23040/54000 (43%)] Loss: -898000.562500\n",
      "Train Epoch: 331 [34304/54000 (64%)] Loss: -846918.250000\n",
      "Train Epoch: 331 [45568/54000 (84%)] Loss: -886843.875000\n",
      "    epoch          : 331\n",
      "    loss           : -880322.0754950495\n",
      "    val_loss       : -868483.7133789062\n",
      "Train Epoch: 332 [512/54000 (1%)] Loss: -837751.000000\n",
      "Train Epoch: 332 [11776/54000 (22%)] Loss: -846851.750000\n",
      "Train Epoch: 332 [23040/54000 (43%)] Loss: -834609.625000\n",
      "Train Epoch: 332 [34304/54000 (64%)] Loss: -899800.562500\n",
      "Train Epoch: 332 [45568/54000 (84%)] Loss: -847491.562500\n",
      "    epoch          : 332\n",
      "    loss           : -881217.4344059406\n",
      "    val_loss       : -868935.453515625\n",
      "Train Epoch: 333 [512/54000 (1%)] Loss: -828562.812500\n",
      "Train Epoch: 333 [11776/54000 (22%)] Loss: -916386.750000\n",
      "Train Epoch: 333 [23040/54000 (43%)] Loss: -900871.937500\n",
      "Train Epoch: 333 [34304/54000 (64%)] Loss: -881297.375000\n",
      "Train Epoch: 333 [45568/54000 (84%)] Loss: -835289.625000\n",
      "    epoch          : 333\n",
      "    loss           : -881091.6064356435\n",
      "    val_loss       : -867805.3723632812\n",
      "Train Epoch: 334 [512/54000 (1%)] Loss: -998796.812500\n",
      "Train Epoch: 334 [11776/54000 (22%)] Loss: -915436.937500\n",
      "Train Epoch: 334 [23040/54000 (43%)] Loss: -847675.500000\n",
      "Train Epoch: 334 [34304/54000 (64%)] Loss: -896398.937500\n",
      "Train Epoch: 334 [45568/54000 (84%)] Loss: -885565.125000\n",
      "    epoch          : 334\n",
      "    loss           : -880787.8756188119\n",
      "    val_loss       : -867356.8525390625\n",
      "Train Epoch: 335 [512/54000 (1%)] Loss: -998865.125000\n",
      "Train Epoch: 335 [11776/54000 (22%)] Loss: -845578.625000\n",
      "Train Epoch: 335 [23040/54000 (43%)] Loss: -882036.875000\n",
      "Train Epoch: 335 [34304/54000 (64%)] Loss: -1001992.625000\n",
      "Train Epoch: 335 [45568/54000 (84%)] Loss: -887094.250000\n",
      "    epoch          : 335\n",
      "    loss           : -879923.8094059406\n",
      "    val_loss       : -868089.850390625\n",
      "Train Epoch: 336 [512/54000 (1%)] Loss: -998858.375000\n",
      "Train Epoch: 336 [11776/54000 (22%)] Loss: -915170.000000\n",
      "Train Epoch: 336 [23040/54000 (43%)] Loss: -827962.375000\n",
      "Train Epoch: 336 [34304/54000 (64%)] Loss: -848516.625000\n",
      "Train Epoch: 336 [45568/54000 (84%)] Loss: -879472.125000\n",
      "    epoch          : 336\n",
      "    loss           : -878326.146039604\n",
      "    val_loss       : -865246.6830078125\n",
      "Train Epoch: 337 [512/54000 (1%)] Loss: -998688.500000\n",
      "Train Epoch: 337 [11776/54000 (22%)] Loss: -834725.375000\n",
      "Train Epoch: 337 [23040/54000 (43%)] Loss: -878916.062500\n",
      "Train Epoch: 337 [34304/54000 (64%)] Loss: -880631.375000\n",
      "Train Epoch: 337 [45568/54000 (84%)] Loss: -827969.750000\n",
      "    epoch          : 337\n",
      "    loss           : -877082.375\n",
      "    val_loss       : -868840.994140625\n",
      "Train Epoch: 338 [512/54000 (1%)] Loss: -1003924.875000\n",
      "Train Epoch: 338 [11776/54000 (22%)] Loss: -848099.000000\n",
      "Train Epoch: 338 [23040/54000 (43%)] Loss: -887564.625000\n",
      "Train Epoch: 338 [34304/54000 (64%)] Loss: -886999.187500\n",
      "Train Epoch: 338 [45568/54000 (84%)] Loss: -880847.750000\n",
      "    epoch          : 338\n",
      "    loss           : -881502.853960396\n",
      "    val_loss       : -867877.043359375\n",
      "Train Epoch: 339 [512/54000 (1%)] Loss: -1001811.375000\n",
      "Train Epoch: 339 [11776/54000 (22%)] Loss: -906614.062500\n",
      "Train Epoch: 339 [23040/54000 (43%)] Loss: -824301.125000\n",
      "Train Epoch: 339 [34304/54000 (64%)] Loss: -885613.312500\n",
      "Train Epoch: 339 [45568/54000 (84%)] Loss: -884867.062500\n",
      "    epoch          : 339\n",
      "    loss           : -879000.989480198\n",
      "    val_loss       : -867370.9893554688\n",
      "Train Epoch: 340 [512/54000 (1%)] Loss: -999085.750000\n",
      "Train Epoch: 340 [11776/54000 (22%)] Loss: -826767.125000\n",
      "Train Epoch: 340 [23040/54000 (43%)] Loss: -850065.125000\n",
      "Train Epoch: 340 [34304/54000 (64%)] Loss: -999850.562500\n",
      "Train Epoch: 340 [45568/54000 (84%)] Loss: -826552.937500\n",
      "    epoch          : 340\n",
      "    loss           : -880991.8341584158\n",
      "    val_loss       : -868678.88046875\n",
      "Train Epoch: 341 [512/54000 (1%)] Loss: -1003407.187500\n",
      "Train Epoch: 341 [11776/54000 (22%)] Loss: -892573.875000\n",
      "Train Epoch: 341 [23040/54000 (43%)] Loss: -831485.500000\n",
      "Train Epoch: 341 [34304/54000 (64%)] Loss: -1002636.125000\n",
      "Train Epoch: 341 [45568/54000 (84%)] Loss: -886244.562500\n",
      "    epoch          : 341\n",
      "    loss           : -880880.7537128713\n",
      "    val_loss       : -868475.280078125\n",
      "Train Epoch: 342 [512/54000 (1%)] Loss: -846853.437500\n",
      "Train Epoch: 342 [11776/54000 (22%)] Loss: -910165.062500\n",
      "Train Epoch: 342 [23040/54000 (43%)] Loss: -884109.812500\n",
      "Train Epoch: 342 [34304/54000 (64%)] Loss: -885447.375000\n",
      "Train Epoch: 342 [45568/54000 (84%)] Loss: -851216.750000\n",
      "    epoch          : 342\n",
      "    loss           : -881791.8118811881\n",
      "    val_loss       : -868220.03515625\n",
      "Train Epoch: 343 [512/54000 (1%)] Loss: -1001329.000000\n",
      "Train Epoch: 343 [11776/54000 (22%)] Loss: -820027.250000\n",
      "Train Epoch: 343 [23040/54000 (43%)] Loss: -886244.500000\n",
      "Train Epoch: 343 [34304/54000 (64%)] Loss: -835471.375000\n",
      "Train Epoch: 343 [45568/54000 (84%)] Loss: -834627.875000\n",
      "    epoch          : 343\n",
      "    loss           : -880165.5482673268\n",
      "    val_loss       : -868134.725\n",
      "Train Epoch: 344 [512/54000 (1%)] Loss: -914207.375000\n",
      "Train Epoch: 344 [11776/54000 (22%)] Loss: -842724.312500\n",
      "Train Epoch: 344 [23040/54000 (43%)] Loss: -888534.687500\n",
      "Train Epoch: 344 [34304/54000 (64%)] Loss: -1002422.500000\n",
      "Train Epoch: 344 [45568/54000 (84%)] Loss: -885296.500000\n",
      "    epoch          : 344\n",
      "    loss           : -881708.4696782178\n",
      "    val_loss       : -869056.202734375\n",
      "Train Epoch: 345 [512/54000 (1%)] Loss: -1001783.562500\n",
      "Train Epoch: 345 [11776/54000 (22%)] Loss: -834967.500000\n",
      "Train Epoch: 345 [23040/54000 (43%)] Loss: -835172.875000\n",
      "Train Epoch: 345 [34304/54000 (64%)] Loss: -900679.875000\n",
      "Train Epoch: 345 [45568/54000 (84%)] Loss: -881813.062500\n",
      "    epoch          : 345\n",
      "    loss           : -881662.030940594\n",
      "    val_loss       : -867916.8765625\n",
      "Train Epoch: 346 [512/54000 (1%)] Loss: -1003925.125000\n",
      "Train Epoch: 346 [11776/54000 (22%)] Loss: -825058.500000\n",
      "Train Epoch: 346 [23040/54000 (43%)] Loss: -894822.250000\n",
      "Train Epoch: 346 [34304/54000 (64%)] Loss: -882812.375000\n",
      "Train Epoch: 346 [45568/54000 (84%)] Loss: -885503.562500\n",
      "    epoch          : 346\n",
      "    loss           : -879287.771039604\n",
      "    val_loss       : -866969.5330078125\n",
      "Train Epoch: 347 [512/54000 (1%)] Loss: -822700.187500\n",
      "Train Epoch: 347 [11776/54000 (22%)] Loss: -828830.125000\n",
      "Train Epoch: 347 [23040/54000 (43%)] Loss: -843426.250000\n",
      "Train Epoch: 347 [34304/54000 (64%)] Loss: -882919.312500\n",
      "Train Epoch: 347 [45568/54000 (84%)] Loss: -885765.312500\n",
      "    epoch          : 347\n",
      "    loss           : -879594.6095297029\n",
      "    val_loss       : -868822.3576171875\n",
      "Train Epoch: 348 [512/54000 (1%)] Loss: -1002423.250000\n",
      "Train Epoch: 348 [11776/54000 (22%)] Loss: -918420.250000\n",
      "Train Epoch: 348 [23040/54000 (43%)] Loss: -828798.812500\n",
      "Train Epoch: 348 [34304/54000 (64%)] Loss: -1003541.312500\n",
      "Train Epoch: 348 [45568/54000 (84%)] Loss: -882503.250000\n",
      "    epoch          : 348\n",
      "    loss           : -881143.5470297029\n",
      "    val_loss       : -869403.5321289062\n",
      "Train Epoch: 349 [512/54000 (1%)] Loss: -1001993.375000\n",
      "Train Epoch: 349 [11776/54000 (22%)] Loss: -886607.562500\n",
      "Train Epoch: 349 [23040/54000 (43%)] Loss: -828391.562500\n",
      "Train Epoch: 349 [34304/54000 (64%)] Loss: -896765.000000\n",
      "Train Epoch: 349 [45568/54000 (84%)] Loss: -888709.937500\n",
      "    epoch          : 349\n",
      "    loss           : -881501.416460396\n",
      "    val_loss       : -868813.9763671875\n",
      "Train Epoch: 350 [512/54000 (1%)] Loss: -918577.500000\n",
      "Train Epoch: 350 [11776/54000 (22%)] Loss: -915374.000000\n",
      "Train Epoch: 350 [23040/54000 (43%)] Loss: -886259.625000\n",
      "Train Epoch: 350 [34304/54000 (64%)] Loss: -904417.625000\n",
      "Train Epoch: 350 [45568/54000 (84%)] Loss: -887143.625000\n",
      "    epoch          : 350\n",
      "    loss           : -882294.6070544554\n",
      "    val_loss       : -869845.4639648438\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0716_225739/checkpoint-epoch350.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 351 [512/54000 (1%)] Loss: -1005834.875000\n",
      "Train Epoch: 351 [11776/54000 (22%)] Loss: -914370.000000\n",
      "Train Epoch: 351 [23040/54000 (43%)] Loss: -853008.000000\n",
      "Train Epoch: 351 [34304/54000 (64%)] Loss: -901330.875000\n",
      "Train Epoch: 351 [45568/54000 (84%)] Loss: -837256.187500\n",
      "    epoch          : 351\n",
      "    loss           : -883114.3589108911\n",
      "    val_loss       : -869650.2038085938\n",
      "Train Epoch: 352 [512/54000 (1%)] Loss: -1005197.000000\n",
      "Train Epoch: 352 [11776/54000 (22%)] Loss: -885323.875000\n",
      "Train Epoch: 352 [23040/54000 (43%)] Loss: -831418.875000\n",
      "Train Epoch: 352 [34304/54000 (64%)] Loss: -889807.062500\n",
      "Train Epoch: 352 [45568/54000 (84%)] Loss: -889809.750000\n",
      "    epoch          : 352\n",
      "    loss           : -882527.2017326732\n",
      "    val_loss       : -869289.8381835937\n",
      "Train Epoch: 353 [512/54000 (1%)] Loss: -1001874.375000\n",
      "Train Epoch: 353 [11776/54000 (22%)] Loss: -843077.562500\n",
      "Train Epoch: 353 [23040/54000 (43%)] Loss: -827235.125000\n",
      "Train Epoch: 353 [34304/54000 (64%)] Loss: -899474.750000\n",
      "Train Epoch: 353 [45568/54000 (84%)] Loss: -847906.125000\n",
      "    epoch          : 353\n",
      "    loss           : -881545.020420792\n",
      "    val_loss       : -869903.9926757812\n",
      "Train Epoch: 354 [512/54000 (1%)] Loss: -1006195.250000\n",
      "Train Epoch: 354 [11776/54000 (22%)] Loss: -916826.062500\n",
      "Train Epoch: 354 [23040/54000 (43%)] Loss: -835137.187500\n",
      "Train Epoch: 354 [34304/54000 (64%)] Loss: -902650.187500\n",
      "Train Epoch: 354 [45568/54000 (84%)] Loss: -832922.562500\n",
      "    epoch          : 354\n",
      "    loss           : -883162.0575495049\n",
      "    val_loss       : -869307.7451171875\n",
      "Train Epoch: 355 [512/54000 (1%)] Loss: -918050.000000\n",
      "Train Epoch: 355 [11776/54000 (22%)] Loss: -897299.375000\n",
      "Train Epoch: 355 [23040/54000 (43%)] Loss: -821632.000000\n",
      "Train Epoch: 355 [34304/54000 (64%)] Loss: -901520.437500\n",
      "Train Epoch: 355 [45568/54000 (84%)] Loss: -834679.937500\n",
      "    epoch          : 355\n",
      "    loss           : -880600.1652227723\n",
      "    val_loss       : -869361.540234375\n",
      "Train Epoch: 356 [512/54000 (1%)] Loss: -1006157.500000\n",
      "Train Epoch: 356 [11776/54000 (22%)] Loss: -850366.875000\n",
      "Train Epoch: 356 [23040/54000 (43%)] Loss: -899437.875000\n",
      "Train Epoch: 356 [34304/54000 (64%)] Loss: -903733.375000\n",
      "Train Epoch: 356 [45568/54000 (84%)] Loss: -888811.375000\n",
      "    epoch          : 356\n",
      "    loss           : -884010.9969059406\n",
      "    val_loss       : -870522.8126953125\n",
      "Train Epoch: 357 [512/54000 (1%)] Loss: -1006073.937500\n",
      "Train Epoch: 357 [11776/54000 (22%)] Loss: -919613.937500\n",
      "Train Epoch: 357 [23040/54000 (43%)] Loss: -886866.312500\n",
      "Train Epoch: 357 [34304/54000 (64%)] Loss: -904108.812500\n",
      "Train Epoch: 357 [45568/54000 (84%)] Loss: -828054.250000\n",
      "    epoch          : 357\n",
      "    loss           : -883723.4622524752\n",
      "    val_loss       : -868207.1086914062\n",
      "Train Epoch: 358 [512/54000 (1%)] Loss: -1002450.125000\n",
      "Train Epoch: 358 [11776/54000 (22%)] Loss: -889070.875000\n",
      "Train Epoch: 358 [23040/54000 (43%)] Loss: -904532.375000\n",
      "Train Epoch: 358 [34304/54000 (64%)] Loss: -838751.187500\n",
      "Train Epoch: 358 [45568/54000 (84%)] Loss: -893400.125000\n",
      "    epoch          : 358\n",
      "    loss           : -883322.2252475248\n",
      "    val_loss       : -870669.3234375\n",
      "Train Epoch: 359 [512/54000 (1%)] Loss: -1005688.750000\n",
      "Train Epoch: 359 [11776/54000 (22%)] Loss: -916350.375000\n",
      "Train Epoch: 359 [23040/54000 (43%)] Loss: -891420.250000\n",
      "Train Epoch: 359 [34304/54000 (64%)] Loss: -833924.750000\n",
      "Train Epoch: 359 [45568/54000 (84%)] Loss: -832832.250000\n",
      "    epoch          : 359\n",
      "    loss           : -884597.4975247525\n",
      "    val_loss       : -871193.4505859375\n",
      "Train Epoch: 360 [512/54000 (1%)] Loss: -1005580.000000\n",
      "Train Epoch: 360 [11776/54000 (22%)] Loss: -915437.937500\n",
      "Train Epoch: 360 [23040/54000 (43%)] Loss: -827169.375000\n",
      "Train Epoch: 360 [34304/54000 (64%)] Loss: -882616.562500\n",
      "Train Epoch: 360 [45568/54000 (84%)] Loss: -842604.875000\n",
      "    epoch          : 360\n",
      "    loss           : -880183.8013613861\n",
      "    val_loss       : -866994.9749023437\n",
      "Train Epoch: 361 [512/54000 (1%)] Loss: -844674.000000\n",
      "Train Epoch: 361 [11776/54000 (22%)] Loss: -909027.250000\n",
      "Train Epoch: 361 [23040/54000 (43%)] Loss: -893453.125000\n",
      "Train Epoch: 361 [34304/54000 (64%)] Loss: -901334.250000\n",
      "Train Epoch: 361 [45568/54000 (84%)] Loss: -887812.125000\n",
      "    epoch          : 361\n",
      "    loss           : -881197.9783415842\n",
      "    val_loss       : -870898.901171875\n",
      "Train Epoch: 362 [512/54000 (1%)] Loss: -1005723.375000\n",
      "Train Epoch: 362 [11776/54000 (22%)] Loss: -917833.625000\n",
      "Train Epoch: 362 [23040/54000 (43%)] Loss: -902148.000000\n",
      "Train Epoch: 362 [34304/54000 (64%)] Loss: -837510.375000\n",
      "Train Epoch: 362 [45568/54000 (84%)] Loss: -890590.437500\n",
      "    epoch          : 362\n",
      "    loss           : -884325.7400990099\n",
      "    val_loss       : -870860.0431640625\n",
      "Train Epoch: 363 [512/54000 (1%)] Loss: -1004127.250000\n",
      "Train Epoch: 363 [11776/54000 (22%)] Loss: -856502.437500\n",
      "Train Epoch: 363 [23040/54000 (43%)] Loss: -899896.562500\n",
      "Train Epoch: 363 [34304/54000 (64%)] Loss: -1005335.250000\n",
      "Train Epoch: 363 [45568/54000 (84%)] Loss: -889373.125000\n",
      "    epoch          : 363\n",
      "    loss           : -884253.6206683168\n",
      "    val_loss       : -870110.8758789062\n",
      "Train Epoch: 364 [512/54000 (1%)] Loss: -1005134.375000\n",
      "Train Epoch: 364 [11776/54000 (22%)] Loss: -887783.250000\n",
      "Train Epoch: 364 [23040/54000 (43%)] Loss: -902415.937500\n",
      "Train Epoch: 364 [34304/54000 (64%)] Loss: -892132.937500\n",
      "Train Epoch: 364 [45568/54000 (84%)] Loss: -830447.500000\n",
      "    epoch          : 364\n",
      "    loss           : -884365.6429455446\n",
      "    val_loss       : -869024.6232421875\n",
      "Train Epoch: 365 [512/54000 (1%)] Loss: -1005269.875000\n",
      "Train Epoch: 365 [11776/54000 (22%)] Loss: -914282.125000\n",
      "Train Epoch: 365 [23040/54000 (43%)] Loss: -887207.562500\n",
      "Train Epoch: 365 [34304/54000 (64%)] Loss: -893145.000000\n",
      "Train Epoch: 365 [45568/54000 (84%)] Loss: -889998.562500\n",
      "    epoch          : 365\n",
      "    loss           : -884750.0024752475\n",
      "    val_loss       : -870417.873828125\n",
      "Train Epoch: 366 [512/54000 (1%)] Loss: -918585.375000\n",
      "Train Epoch: 366 [11776/54000 (22%)] Loss: -849753.687500\n",
      "Train Epoch: 366 [23040/54000 (43%)] Loss: -848662.000000\n",
      "Train Epoch: 366 [34304/54000 (64%)] Loss: -838428.625000\n",
      "Train Epoch: 366 [45568/54000 (84%)] Loss: -837796.250000\n",
      "    epoch          : 366\n",
      "    loss           : -885248.7827970297\n",
      "    val_loss       : -870750.9775390625\n",
      "Train Epoch: 367 [512/54000 (1%)] Loss: -1007430.187500\n",
      "Train Epoch: 367 [11776/54000 (22%)] Loss: -844885.250000\n",
      "Train Epoch: 367 [23040/54000 (43%)] Loss: -889884.000000\n",
      "Train Epoch: 367 [34304/54000 (64%)] Loss: -899663.937500\n",
      "Train Epoch: 367 [45568/54000 (84%)] Loss: -837019.375000\n",
      "    epoch          : 367\n",
      "    loss           : -885187.4535891089\n",
      "    val_loss       : -868802.5450195313\n",
      "Train Epoch: 368 [512/54000 (1%)] Loss: -1005604.750000\n",
      "Train Epoch: 368 [11776/54000 (22%)] Loss: -849732.375000\n",
      "Train Epoch: 368 [23040/54000 (43%)] Loss: -889789.375000\n",
      "Train Epoch: 368 [34304/54000 (64%)] Loss: -849622.062500\n",
      "Train Epoch: 368 [45568/54000 (84%)] Loss: -821846.000000\n",
      "    epoch          : 368\n",
      "    loss           : -880848.854579208\n",
      "    val_loss       : -867708.6581054687\n",
      "Train Epoch: 369 [512/54000 (1%)] Loss: -1004552.250000\n",
      "Train Epoch: 369 [11776/54000 (22%)] Loss: -906929.625000\n",
      "Train Epoch: 369 [23040/54000 (43%)] Loss: -832132.000000\n",
      "Train Epoch: 369 [34304/54000 (64%)] Loss: -887525.312500\n",
      "Train Epoch: 369 [45568/54000 (84%)] Loss: -892063.000000\n",
      "    epoch          : 369\n",
      "    loss           : -882633.5798267326\n",
      "    val_loss       : -870946.4288085938\n",
      "Train Epoch: 370 [512/54000 (1%)] Loss: -1004780.000000\n",
      "Train Epoch: 370 [11776/54000 (22%)] Loss: -845802.437500\n",
      "Train Epoch: 370 [23040/54000 (43%)] Loss: -902107.125000\n",
      "Train Epoch: 370 [34304/54000 (64%)] Loss: -841670.687500\n",
      "Train Epoch: 370 [45568/54000 (84%)] Loss: -890977.312500\n",
      "    epoch          : 370\n",
      "    loss           : -885866.3725247525\n",
      "    val_loss       : -870574.00234375\n",
      "Train Epoch: 371 [512/54000 (1%)] Loss: -919642.625000\n",
      "Train Epoch: 371 [11776/54000 (22%)] Loss: -854105.250000\n",
      "Train Epoch: 371 [23040/54000 (43%)] Loss: -851262.500000\n",
      "Train Epoch: 371 [34304/54000 (64%)] Loss: -836374.875000\n",
      "Train Epoch: 371 [45568/54000 (84%)] Loss: -840058.750000\n",
      "    epoch          : 371\n",
      "    loss           : -886037.6021039604\n",
      "    val_loss       : -871189.2334960938\n",
      "Train Epoch: 372 [512/54000 (1%)] Loss: -1010022.187500\n",
      "Train Epoch: 372 [11776/54000 (22%)] Loss: -851763.187500\n",
      "Train Epoch: 372 [23040/54000 (43%)] Loss: -901245.750000\n",
      "Train Epoch: 372 [34304/54000 (64%)] Loss: -1009836.375000\n",
      "Train Epoch: 372 [45568/54000 (84%)] Loss: -890276.625000\n",
      "    epoch          : 372\n",
      "    loss           : -886115.8063118812\n",
      "    val_loss       : -871326.3168945312\n",
      "Train Epoch: 373 [512/54000 (1%)] Loss: -1004591.312500\n",
      "Train Epoch: 373 [11776/54000 (22%)] Loss: -845986.500000\n",
      "Train Epoch: 373 [23040/54000 (43%)] Loss: -821088.750000\n",
      "Train Epoch: 373 [34304/54000 (64%)] Loss: -856336.250000\n",
      "Train Epoch: 373 [45568/54000 (84%)] Loss: -837132.375000\n",
      "    epoch          : 373\n",
      "    loss           : -883804.9238861386\n",
      "    val_loss       : -870637.5806640625\n",
      "Train Epoch: 374 [512/54000 (1%)] Loss: -1005259.500000\n",
      "Train Epoch: 374 [11776/54000 (22%)] Loss: -829626.250000\n",
      "Train Epoch: 374 [23040/54000 (43%)] Loss: -836801.625000\n",
      "Train Epoch: 374 [34304/54000 (64%)] Loss: -889321.500000\n",
      "Train Epoch: 374 [45568/54000 (84%)] Loss: -834292.187500\n",
      "    epoch          : 374\n",
      "    loss           : -884345.0983910891\n",
      "    val_loss       : -870810.425\n",
      "Train Epoch: 375 [512/54000 (1%)] Loss: -1008081.125000\n",
      "Train Epoch: 375 [11776/54000 (22%)] Loss: -919413.562500\n",
      "Train Epoch: 375 [23040/54000 (43%)] Loss: -900762.250000\n",
      "Train Epoch: 375 [34304/54000 (64%)] Loss: -852480.875000\n",
      "Train Epoch: 375 [45568/54000 (84%)] Loss: -851333.625000\n",
      "    epoch          : 375\n",
      "    loss           : -884231.707920792\n",
      "    val_loss       : -869682.7010742187\n",
      "Train Epoch: 376 [512/54000 (1%)] Loss: -1005126.937500\n",
      "Train Epoch: 376 [11776/54000 (22%)] Loss: -846569.937500\n",
      "Train Epoch: 376 [23040/54000 (43%)] Loss: -880731.500000\n",
      "Train Epoch: 376 [34304/54000 (64%)] Loss: -849324.312500\n",
      "Train Epoch: 376 [45568/54000 (84%)] Loss: -891060.937500\n",
      "    epoch          : 376\n",
      "    loss           : -882451.7827970297\n",
      "    val_loss       : -868866.8258789063\n",
      "Train Epoch: 377 [512/54000 (1%)] Loss: -1006448.250000\n",
      "Train Epoch: 377 [11776/54000 (22%)] Loss: -857126.562500\n",
      "Train Epoch: 377 [23040/54000 (43%)] Loss: -835153.937500\n",
      "Train Epoch: 377 [34304/54000 (64%)] Loss: -857153.125000\n",
      "Train Epoch: 377 [45568/54000 (84%)] Loss: -840694.687500\n",
      "    epoch          : 377\n",
      "    loss           : -885409.7660891089\n",
      "    val_loss       : -869116.9311523438\n",
      "Train Epoch: 378 [512/54000 (1%)] Loss: -1006088.625000\n",
      "Train Epoch: 378 [11776/54000 (22%)] Loss: -839939.125000\n",
      "Train Epoch: 378 [23040/54000 (43%)] Loss: -900537.750000\n",
      "Train Epoch: 378 [34304/54000 (64%)] Loss: -900430.625000\n",
      "Train Epoch: 378 [45568/54000 (84%)] Loss: -890090.375000\n",
      "    epoch          : 378\n",
      "    loss           : -885297.1441831683\n",
      "    val_loss       : -869740.6043945312\n",
      "Train Epoch: 379 [512/54000 (1%)] Loss: -1007691.125000\n",
      "Train Epoch: 379 [11776/54000 (22%)] Loss: -912809.937500\n",
      "Train Epoch: 379 [23040/54000 (43%)] Loss: -874804.187500\n",
      "Train Epoch: 379 [34304/54000 (64%)] Loss: -819796.562500\n",
      "Train Epoch: 379 [45568/54000 (84%)] Loss: -886511.875000\n",
      "    epoch          : 379\n",
      "    loss           : -881141.5946782178\n",
      "    val_loss       : -870158.09453125\n",
      "Train Epoch: 380 [512/54000 (1%)] Loss: -1007080.812500\n",
      "Train Epoch: 380 [11776/54000 (22%)] Loss: -846417.312500\n",
      "Train Epoch: 380 [23040/54000 (43%)] Loss: -890437.375000\n",
      "Train Epoch: 380 [34304/54000 (64%)] Loss: -833192.750000\n",
      "Train Epoch: 380 [45568/54000 (84%)] Loss: -894458.375000\n",
      "    epoch          : 380\n",
      "    loss           : -885912.7951732674\n",
      "    val_loss       : -871404.573828125\n",
      "Train Epoch: 381 [512/54000 (1%)] Loss: -1008498.000000\n",
      "Train Epoch: 381 [11776/54000 (22%)] Loss: -851218.312500\n",
      "Train Epoch: 381 [23040/54000 (43%)] Loss: -893862.687500\n",
      "Train Epoch: 381 [34304/54000 (64%)] Loss: -893476.000000\n",
      "Train Epoch: 381 [45568/54000 (84%)] Loss: -835173.375000\n",
      "    epoch          : 381\n",
      "    loss           : -886110.4263613861\n",
      "    val_loss       : -870185.59296875\n",
      "Train Epoch: 382 [512/54000 (1%)] Loss: -850877.750000\n",
      "Train Epoch: 382 [11776/54000 (22%)] Loss: -921313.500000\n",
      "Train Epoch: 382 [23040/54000 (43%)] Loss: -888685.250000\n",
      "Train Epoch: 382 [34304/54000 (64%)] Loss: -852481.250000\n",
      "Train Epoch: 382 [45568/54000 (84%)] Loss: -887096.687500\n",
      "    epoch          : 382\n",
      "    loss           : -883476.2017326732\n",
      "    val_loss       : -869547.4536132812\n",
      "Train Epoch: 383 [512/54000 (1%)] Loss: -1005650.750000\n",
      "Train Epoch: 383 [11776/54000 (22%)] Loss: -851242.562500\n",
      "Train Epoch: 383 [23040/54000 (43%)] Loss: -888663.750000\n",
      "Train Epoch: 383 [34304/54000 (64%)] Loss: -894707.500000\n",
      "Train Epoch: 383 [45568/54000 (84%)] Loss: -878403.625000\n",
      "    epoch          : 383\n",
      "    loss           : -883599.4214108911\n",
      "    val_loss       : -869250.7598632813\n",
      "Train Epoch: 384 [512/54000 (1%)] Loss: -1008152.750000\n",
      "Train Epoch: 384 [11776/54000 (22%)] Loss: -913363.000000\n",
      "Train Epoch: 384 [23040/54000 (43%)] Loss: -851053.750000\n",
      "Train Epoch: 384 [34304/54000 (64%)] Loss: -860114.875000\n",
      "Train Epoch: 384 [45568/54000 (84%)] Loss: -836292.562500\n",
      "    epoch          : 384\n",
      "    loss           : -884829.3780940594\n",
      "    val_loss       : -871358.1580078125\n",
      "Train Epoch: 385 [512/54000 (1%)] Loss: -1008589.125000\n",
      "Train Epoch: 385 [11776/54000 (22%)] Loss: -830200.562500\n",
      "Train Epoch: 385 [23040/54000 (43%)] Loss: -837889.375000\n",
      "Train Epoch: 385 [34304/54000 (64%)] Loss: -852432.500000\n",
      "Train Epoch: 385 [45568/54000 (84%)] Loss: -827708.125000\n",
      "    epoch          : 385\n",
      "    loss           : -884207.8533415842\n",
      "    val_loss       : -870734.2295898438\n",
      "Train Epoch: 386 [512/54000 (1%)] Loss: -1007299.875000\n",
      "Train Epoch: 386 [11776/54000 (22%)] Loss: -920995.187500\n",
      "Train Epoch: 386 [23040/54000 (43%)] Loss: -898203.875000\n",
      "Train Epoch: 386 [34304/54000 (64%)] Loss: -902729.125000\n",
      "Train Epoch: 386 [45568/54000 (84%)] Loss: -890150.750000\n",
      "    epoch          : 386\n",
      "    loss           : -885821.2110148515\n",
      "    val_loss       : -871392.4067382812\n",
      "Train Epoch: 387 [512/54000 (1%)] Loss: -1009825.312500\n",
      "Train Epoch: 387 [11776/54000 (22%)] Loss: -909490.250000\n",
      "Train Epoch: 387 [23040/54000 (43%)] Loss: -855249.437500\n",
      "Train Epoch: 387 [34304/54000 (64%)] Loss: -830898.125000\n",
      "Train Epoch: 387 [45568/54000 (84%)] Loss: -892277.375000\n",
      "    epoch          : 387\n",
      "    loss           : -885325.4189356435\n",
      "    val_loss       : -870152.7799804688\n",
      "Train Epoch: 388 [512/54000 (1%)] Loss: -1007365.375000\n",
      "Train Epoch: 388 [11776/54000 (22%)] Loss: -838014.125000\n",
      "Train Epoch: 388 [23040/54000 (43%)] Loss: -852528.500000\n",
      "Train Epoch: 388 [34304/54000 (64%)] Loss: -844059.125000\n",
      "Train Epoch: 388 [45568/54000 (84%)] Loss: -836135.500000\n",
      "    epoch          : 388\n",
      "    loss           : -886979.9900990099\n",
      "    val_loss       : -870947.7348632812\n",
      "Train Epoch: 389 [512/54000 (1%)] Loss: -1008248.125000\n",
      "Train Epoch: 389 [11776/54000 (22%)] Loss: -848917.875000\n",
      "Train Epoch: 389 [23040/54000 (43%)] Loss: -835650.437500\n",
      "Train Epoch: 389 [34304/54000 (64%)] Loss: -886276.125000\n",
      "Train Epoch: 389 [45568/54000 (84%)] Loss: -890679.375000\n",
      "    epoch          : 389\n",
      "    loss           : -885034.978960396\n",
      "    val_loss       : -869663.2466796875\n",
      "Train Epoch: 390 [512/54000 (1%)] Loss: -923091.812500\n",
      "Train Epoch: 390 [11776/54000 (22%)] Loss: -830756.750000\n",
      "Train Epoch: 390 [23040/54000 (43%)] Loss: -883975.375000\n",
      "Train Epoch: 390 [34304/54000 (64%)] Loss: -836587.625000\n",
      "Train Epoch: 390 [45568/54000 (84%)] Loss: -892192.000000\n",
      "    epoch          : 390\n",
      "    loss           : -884874.4715346535\n",
      "    val_loss       : -871784.6698242187\n",
      "Train Epoch: 391 [512/54000 (1%)] Loss: -1009316.062500\n",
      "Train Epoch: 391 [11776/54000 (22%)] Loss: -845850.312500\n",
      "Train Epoch: 391 [23040/54000 (43%)] Loss: -851413.562500\n",
      "Train Epoch: 391 [34304/54000 (64%)] Loss: -886614.812500\n",
      "Train Epoch: 391 [45568/54000 (84%)] Loss: -837403.500000\n",
      "    epoch          : 391\n",
      "    loss           : -885367.2883663366\n",
      "    val_loss       : -871956.7338867188\n",
      "Train Epoch: 392 [512/54000 (1%)] Loss: -1008799.062500\n",
      "Train Epoch: 392 [11776/54000 (22%)] Loss: -905949.562500\n",
      "Train Epoch: 392 [23040/54000 (43%)] Loss: -889590.812500\n",
      "Train Epoch: 392 [34304/54000 (64%)] Loss: -901290.875000\n",
      "Train Epoch: 392 [45568/54000 (84%)] Loss: -889409.000000\n",
      "    epoch          : 392\n",
      "    loss           : -886090.4288366337\n",
      "    val_loss       : -870313.0349609375\n",
      "Train Epoch: 393 [512/54000 (1%)] Loss: -1009329.375000\n",
      "Train Epoch: 393 [11776/54000 (22%)] Loss: -891314.937500\n",
      "Train Epoch: 393 [23040/54000 (43%)] Loss: -832958.875000\n",
      "Train Epoch: 393 [34304/54000 (64%)] Loss: -1004598.937500\n",
      "Train Epoch: 393 [45568/54000 (84%)] Loss: -834571.187500\n",
      "    epoch          : 393\n",
      "    loss           : -883395.8861386139\n",
      "    val_loss       : -870112.2110351563\n",
      "Train Epoch: 394 [512/54000 (1%)] Loss: -1009179.437500\n",
      "Train Epoch: 394 [11776/54000 (22%)] Loss: -890378.187500\n",
      "Train Epoch: 394 [23040/54000 (43%)] Loss: -848878.750000\n",
      "Train Epoch: 394 [34304/54000 (64%)] Loss: -882840.187500\n",
      "Train Epoch: 394 [45568/54000 (84%)] Loss: -831599.750000\n",
      "    epoch          : 394\n",
      "    loss           : -883087.2797029703\n",
      "    val_loss       : -870368.7958007812\n",
      "Train Epoch: 395 [512/54000 (1%)] Loss: -1008379.750000\n",
      "Train Epoch: 395 [11776/54000 (22%)] Loss: -918171.312500\n",
      "Train Epoch: 395 [23040/54000 (43%)] Loss: -855975.875000\n",
      "Train Epoch: 395 [34304/54000 (64%)] Loss: -894319.687500\n",
      "Train Epoch: 395 [45568/54000 (84%)] Loss: -892400.250000\n",
      "    epoch          : 395\n",
      "    loss           : -885831.2233910891\n",
      "    val_loss       : -871250.559375\n",
      "Train Epoch: 396 [512/54000 (1%)] Loss: -842787.625000\n",
      "Train Epoch: 396 [11776/54000 (22%)] Loss: -907930.437500\n",
      "Train Epoch: 396 [23040/54000 (43%)] Loss: -891478.000000\n",
      "Train Epoch: 396 [34304/54000 (64%)] Loss: -887340.562500\n",
      "Train Epoch: 396 [45568/54000 (84%)] Loss: -855270.125000\n",
      "    epoch          : 396\n",
      "    loss           : -883569.3910891089\n",
      "    val_loss       : -871182.6758789063\n",
      "Train Epoch: 397 [512/54000 (1%)] Loss: -920893.562500\n",
      "Train Epoch: 397 [11776/54000 (22%)] Loss: -919168.375000\n",
      "Train Epoch: 397 [23040/54000 (43%)] Loss: -902848.750000\n",
      "Train Epoch: 397 [34304/54000 (64%)] Loss: -1008942.812500\n",
      "Train Epoch: 397 [45568/54000 (84%)] Loss: -892722.000000\n",
      "    epoch          : 397\n",
      "    loss           : -886341.3527227723\n",
      "    val_loss       : -871619.197265625\n",
      "Train Epoch: 398 [512/54000 (1%)] Loss: -1009926.750000\n",
      "Train Epoch: 398 [11776/54000 (22%)] Loss: -831611.500000\n",
      "Train Epoch: 398 [23040/54000 (43%)] Loss: -854923.250000\n",
      "Train Epoch: 398 [34304/54000 (64%)] Loss: -887559.500000\n",
      "Train Epoch: 398 [45568/54000 (84%)] Loss: -832942.500000\n",
      "    epoch          : 398\n",
      "    loss           : -886304.030940594\n",
      "    val_loss       : -870755.9110351562\n",
      "Train Epoch: 399 [512/54000 (1%)] Loss: -1008140.625000\n",
      "Train Epoch: 399 [11776/54000 (22%)] Loss: -855189.125000\n",
      "Train Epoch: 399 [23040/54000 (43%)] Loss: -893005.000000\n",
      "Train Epoch: 399 [34304/54000 (64%)] Loss: -861664.250000\n",
      "Train Epoch: 399 [45568/54000 (84%)] Loss: -887654.125000\n",
      "    epoch          : 399\n",
      "    loss           : -886720.7277227723\n",
      "    val_loss       : -870732.1926757812\n",
      "Train Epoch: 400 [512/54000 (1%)] Loss: -1011001.750000\n",
      "Train Epoch: 400 [11776/54000 (22%)] Loss: -922308.437500\n",
      "Train Epoch: 400 [23040/54000 (43%)] Loss: -853804.125000\n",
      "Train Epoch: 400 [34304/54000 (64%)] Loss: -856744.437500\n",
      "Train Epoch: 400 [45568/54000 (84%)] Loss: -831798.125000\n",
      "    epoch          : 400\n",
      "    loss           : -887267.6243811881\n",
      "    val_loss       : -871412.2833007813\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0716_225739/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [512/54000 (1%)] Loss: -1009451.875000\n",
      "Train Epoch: 401 [11776/54000 (22%)] Loss: -917735.687500\n",
      "Train Epoch: 401 [23040/54000 (43%)] Loss: -900982.875000\n",
      "Train Epoch: 401 [34304/54000 (64%)] Loss: -890365.750000\n",
      "Train Epoch: 401 [45568/54000 (84%)] Loss: -893733.875000\n",
      "    epoch          : 401\n",
      "    loss           : -887586.3985148515\n",
      "    val_loss       : -872154.3041992188\n",
      "Train Epoch: 402 [512/54000 (1%)] Loss: -852973.250000\n",
      "Train Epoch: 402 [11776/54000 (22%)] Loss: -919356.062500\n",
      "Train Epoch: 402 [23040/54000 (43%)] Loss: -898422.750000\n",
      "Train Epoch: 402 [34304/54000 (64%)] Loss: -835052.750000\n",
      "Train Epoch: 402 [45568/54000 (84%)] Loss: -889490.000000\n",
      "    epoch          : 402\n",
      "    loss           : -886512.2271039604\n",
      "    val_loss       : -871824.8438476563\n",
      "Train Epoch: 403 [512/54000 (1%)] Loss: -1006784.375000\n",
      "Train Epoch: 403 [11776/54000 (22%)] Loss: -855788.062500\n",
      "Train Epoch: 403 [23040/54000 (43%)] Loss: -829345.375000\n",
      "Train Epoch: 403 [34304/54000 (64%)] Loss: -833131.187500\n",
      "Train Epoch: 403 [45568/54000 (84%)] Loss: -887285.250000\n",
      "    epoch          : 403\n",
      "    loss           : -886659.2549504951\n",
      "    val_loss       : -871122.0446289063\n",
      "Train Epoch: 404 [512/54000 (1%)] Loss: -1011909.000000\n",
      "Train Epoch: 404 [11776/54000 (22%)] Loss: -918333.375000\n",
      "Train Epoch: 404 [23040/54000 (43%)] Loss: -841229.062500\n",
      "Train Epoch: 404 [34304/54000 (64%)] Loss: -837267.625000\n",
      "Train Epoch: 404 [45568/54000 (84%)] Loss: -890868.937500\n",
      "    epoch          : 404\n",
      "    loss           : -887651.3929455446\n",
      "    val_loss       : -872147.6510742188\n",
      "Train Epoch: 405 [512/54000 (1%)] Loss: -1011967.750000\n",
      "Train Epoch: 405 [11776/54000 (22%)] Loss: -846271.187500\n",
      "Train Epoch: 405 [23040/54000 (43%)] Loss: -859465.750000\n",
      "Train Epoch: 405 [34304/54000 (64%)] Loss: -856649.000000\n",
      "Train Epoch: 405 [45568/54000 (84%)] Loss: -832904.875000\n",
      "    epoch          : 405\n",
      "    loss           : -886981.3620049505\n",
      "    val_loss       : -871114.1767578125\n",
      "Train Epoch: 406 [512/54000 (1%)] Loss: -1009670.562500\n",
      "Train Epoch: 406 [11776/54000 (22%)] Loss: -920795.625000\n",
      "Train Epoch: 406 [23040/54000 (43%)] Loss: -834180.375000\n",
      "Train Epoch: 406 [34304/54000 (64%)] Loss: -888833.312500\n",
      "Train Epoch: 406 [45568/54000 (84%)] Loss: -886328.875000\n",
      "    epoch          : 406\n",
      "    loss           : -886151.3211633663\n",
      "    val_loss       : -869793.43359375\n",
      "Train Epoch: 407 [512/54000 (1%)] Loss: -1012364.062500\n",
      "Train Epoch: 407 [11776/54000 (22%)] Loss: -822920.875000\n",
      "Train Epoch: 407 [23040/54000 (43%)] Loss: -856471.500000\n",
      "Train Epoch: 407 [34304/54000 (64%)] Loss: -853144.750000\n",
      "Train Epoch: 407 [45568/54000 (84%)] Loss: -830690.125000\n",
      "    epoch          : 407\n",
      "    loss           : -885210.9251237623\n",
      "    val_loss       : -869308.8864257813\n",
      "Train Epoch: 408 [512/54000 (1%)] Loss: -1011791.750000\n",
      "Train Epoch: 408 [11776/54000 (22%)] Loss: -845907.875000\n",
      "Train Epoch: 408 [23040/54000 (43%)] Loss: -837412.312500\n",
      "Train Epoch: 408 [34304/54000 (64%)] Loss: -901054.687500\n",
      "Train Epoch: 408 [45568/54000 (84%)] Loss: -836085.937500\n",
      "    epoch          : 408\n",
      "    loss           : -887534.8681930694\n",
      "    val_loss       : -871052.7975585938\n",
      "Train Epoch: 409 [512/54000 (1%)] Loss: -1008011.937500\n",
      "Train Epoch: 409 [11776/54000 (22%)] Loss: -899708.000000\n",
      "Train Epoch: 409 [23040/54000 (43%)] Loss: -835542.687500\n",
      "Train Epoch: 409 [34304/54000 (64%)] Loss: -856936.312500\n",
      "Train Epoch: 409 [45568/54000 (84%)] Loss: -894463.687500\n",
      "    epoch          : 409\n",
      "    loss           : -886983.9845297029\n",
      "    val_loss       : -872276.79140625\n",
      "Train Epoch: 410 [512/54000 (1%)] Loss: -921069.625000\n",
      "Train Epoch: 410 [11776/54000 (22%)] Loss: -857698.812500\n",
      "Train Epoch: 410 [23040/54000 (43%)] Loss: -908707.562500\n",
      "Train Epoch: 410 [34304/54000 (64%)] Loss: -893579.750000\n",
      "Train Epoch: 410 [45568/54000 (84%)] Loss: -891653.250000\n",
      "    epoch          : 410\n",
      "    loss           : -888829.6584158416\n",
      "    val_loss       : -871230.7353515625\n",
      "Train Epoch: 411 [512/54000 (1%)] Loss: -1008920.750000\n",
      "Train Epoch: 411 [11776/54000 (22%)] Loss: -845879.125000\n",
      "Train Epoch: 411 [23040/54000 (43%)] Loss: -891368.750000\n",
      "Train Epoch: 411 [34304/54000 (64%)] Loss: -892576.125000\n",
      "Train Epoch: 411 [45568/54000 (84%)] Loss: -890698.875000\n",
      "    epoch          : 411\n",
      "    loss           : -888819.9022277228\n",
      "    val_loss       : -870783.7734375\n",
      "Train Epoch: 412 [512/54000 (1%)] Loss: -1010187.812500\n",
      "Train Epoch: 412 [11776/54000 (22%)] Loss: -848254.625000\n",
      "Train Epoch: 412 [23040/54000 (43%)] Loss: -827660.875000\n",
      "Train Epoch: 412 [34304/54000 (64%)] Loss: -1009923.750000\n",
      "Train Epoch: 412 [45568/54000 (84%)] Loss: -860667.375000\n",
      "    epoch          : 412\n",
      "    loss           : -886998.8910891089\n",
      "    val_loss       : -870624.5146484375\n",
      "Train Epoch: 413 [512/54000 (1%)] Loss: -836111.500000\n",
      "Train Epoch: 413 [11776/54000 (22%)] Loss: -850368.875000\n",
      "Train Epoch: 413 [23040/54000 (43%)] Loss: -898743.750000\n",
      "Train Epoch: 413 [34304/54000 (64%)] Loss: -836484.625000\n",
      "Train Epoch: 413 [45568/54000 (84%)] Loss: -890701.187500\n",
      "    epoch          : 413\n",
      "    loss           : -885121.3279702971\n",
      "    val_loss       : -871124.4725585937\n",
      "Train Epoch: 414 [512/54000 (1%)] Loss: -1009820.500000\n",
      "Train Epoch: 414 [11776/54000 (22%)] Loss: -904224.000000\n",
      "Train Epoch: 414 [23040/54000 (43%)] Loss: -903256.125000\n",
      "Train Epoch: 414 [34304/54000 (64%)] Loss: -906417.625000\n",
      "Train Epoch: 414 [45568/54000 (84%)] Loss: -893471.312500\n",
      "    epoch          : 414\n",
      "    loss           : -888065.7797029703\n",
      "    val_loss       : -870805.3693359375\n",
      "Train Epoch: 415 [512/54000 (1%)] Loss: -1011033.000000\n",
      "Train Epoch: 415 [11776/54000 (22%)] Loss: -914349.875000\n",
      "Train Epoch: 415 [23040/54000 (43%)] Loss: -834237.937500\n",
      "Train Epoch: 415 [34304/54000 (64%)] Loss: -907120.812500\n",
      "Train Epoch: 415 [45568/54000 (84%)] Loss: -889400.312500\n",
      "    epoch          : 415\n",
      "    loss           : -886652.4498762377\n",
      "    val_loss       : -869404.389453125\n",
      "Train Epoch: 416 [512/54000 (1%)] Loss: -833013.875000\n",
      "Train Epoch: 416 [11776/54000 (22%)] Loss: -851935.625000\n",
      "Train Epoch: 416 [23040/54000 (43%)] Loss: -891463.750000\n",
      "Train Epoch: 416 [34304/54000 (64%)] Loss: -1010348.500000\n",
      "Train Epoch: 416 [45568/54000 (84%)] Loss: -836581.250000\n",
      "    epoch          : 416\n",
      "    loss           : -887037.8601485149\n",
      "    val_loss       : -872343.5879882813\n",
      "Train Epoch: 417 [512/54000 (1%)] Loss: -1014630.250000\n",
      "Train Epoch: 417 [11776/54000 (22%)] Loss: -892320.687500\n",
      "Train Epoch: 417 [23040/54000 (43%)] Loss: -886943.250000\n",
      "Train Epoch: 417 [34304/54000 (64%)] Loss: -833892.062500\n",
      "Train Epoch: 417 [45568/54000 (84%)] Loss: -888247.125000\n",
      "    epoch          : 417\n",
      "    loss           : -886595.4740099009\n",
      "    val_loss       : -870629.7111328125\n",
      "Train Epoch: 418 [512/54000 (1%)] Loss: -1009410.125000\n",
      "Train Epoch: 418 [11776/54000 (22%)] Loss: -833969.875000\n",
      "Train Epoch: 418 [23040/54000 (43%)] Loss: -862741.437500\n",
      "Train Epoch: 418 [34304/54000 (64%)] Loss: -843699.750000\n",
      "Train Epoch: 418 [45568/54000 (84%)] Loss: -837671.250000\n",
      "    epoch          : 418\n",
      "    loss           : -888580.6299504951\n",
      "    val_loss       : -872180.81171875\n",
      "Train Epoch: 419 [512/54000 (1%)] Loss: -1012606.437500\n",
      "Train Epoch: 419 [11776/54000 (22%)] Loss: -838878.750000\n",
      "Train Epoch: 419 [23040/54000 (43%)] Loss: -835720.937500\n",
      "Train Epoch: 419 [34304/54000 (64%)] Loss: -893117.125000\n",
      "Train Epoch: 419 [45568/54000 (84%)] Loss: -891586.062500\n",
      "    epoch          : 419\n",
      "    loss           : -889734.8818069306\n",
      "    val_loss       : -872033.4375\n",
      "Train Epoch: 420 [512/54000 (1%)] Loss: -1012375.312500\n",
      "Train Epoch: 420 [11776/54000 (22%)] Loss: -904737.687500\n",
      "Train Epoch: 420 [23040/54000 (43%)] Loss: -842331.750000\n",
      "Train Epoch: 420 [34304/54000 (64%)] Loss: -839961.125000\n",
      "Train Epoch: 420 [45568/54000 (84%)] Loss: -889554.375000\n",
      "    epoch          : 420\n",
      "    loss           : -889426.1448019802\n",
      "    val_loss       : -872199.3939453125\n",
      "Train Epoch: 421 [512/54000 (1%)] Loss: -1012057.875000\n",
      "Train Epoch: 421 [11776/54000 (22%)] Loss: -848685.437500\n",
      "Train Epoch: 421 [23040/54000 (43%)] Loss: -890634.000000\n",
      "Train Epoch: 421 [34304/54000 (64%)] Loss: -891022.000000\n",
      "Train Epoch: 421 [45568/54000 (84%)] Loss: -890081.375000\n",
      "    epoch          : 421\n",
      "    loss           : -885606.4956683168\n",
      "    val_loss       : -871358.4299804687\n",
      "Train Epoch: 422 [512/54000 (1%)] Loss: -1012139.500000\n",
      "Train Epoch: 422 [11776/54000 (22%)] Loss: -831053.000000\n",
      "Train Epoch: 422 [23040/54000 (43%)] Loss: -901013.000000\n",
      "Train Epoch: 422 [34304/54000 (64%)] Loss: -861005.375000\n",
      "Train Epoch: 422 [45568/54000 (84%)] Loss: -894616.625000\n",
      "    epoch          : 422\n",
      "    loss           : -886487.1478960396\n",
      "    val_loss       : -871469.6111328125\n",
      "Train Epoch: 423 [512/54000 (1%)] Loss: -1012552.062500\n",
      "Train Epoch: 423 [11776/54000 (22%)] Loss: -851258.937500\n",
      "Train Epoch: 423 [23040/54000 (43%)] Loss: -836834.812500\n",
      "Train Epoch: 423 [34304/54000 (64%)] Loss: -890886.937500\n",
      "Train Epoch: 423 [45568/54000 (84%)] Loss: -897037.875000\n",
      "    epoch          : 423\n",
      "    loss           : -887902.843440594\n",
      "    val_loss       : -872015.4217773437\n",
      "Train Epoch: 424 [512/54000 (1%)] Loss: -1010814.000000\n",
      "Train Epoch: 424 [11776/54000 (22%)] Loss: -853767.187500\n",
      "Train Epoch: 424 [23040/54000 (43%)] Loss: -842832.562500\n",
      "Train Epoch: 424 [34304/54000 (64%)] Loss: -896464.125000\n",
      "Train Epoch: 424 [45568/54000 (84%)] Loss: -839958.500000\n",
      "    epoch          : 424\n",
      "    loss           : -889453.1070544554\n",
      "    val_loss       : -871947.61640625\n",
      "Train Epoch: 425 [512/54000 (1%)] Loss: -920986.437500\n",
      "Train Epoch: 425 [11776/54000 (22%)] Loss: -853711.750000\n",
      "Train Epoch: 425 [23040/54000 (43%)] Loss: -889369.875000\n",
      "Train Epoch: 425 [34304/54000 (64%)] Loss: -888837.625000\n",
      "Train Epoch: 425 [45568/54000 (84%)] Loss: -888812.312500\n",
      "    epoch          : 425\n",
      "    loss           : -886917.6113861386\n",
      "    val_loss       : -870533.2547851562\n",
      "Train Epoch: 426 [512/54000 (1%)] Loss: -835992.125000\n",
      "Train Epoch: 426 [11776/54000 (22%)] Loss: -920406.062500\n",
      "Train Epoch: 426 [23040/54000 (43%)] Loss: -835031.437500\n",
      "Train Epoch: 426 [34304/54000 (64%)] Loss: -895809.875000\n",
      "Train Epoch: 426 [45568/54000 (84%)] Loss: -841264.875000\n",
      "    epoch          : 426\n",
      "    loss           : -887984.5841584158\n",
      "    val_loss       : -872167.0489257813\n",
      "Train Epoch: 427 [512/54000 (1%)] Loss: -1008572.000000\n",
      "Train Epoch: 427 [11776/54000 (22%)] Loss: -924481.375000\n",
      "Train Epoch: 427 [23040/54000 (43%)] Loss: -861177.312500\n",
      "Train Epoch: 427 [34304/54000 (64%)] Loss: -901408.562500\n",
      "Train Epoch: 427 [45568/54000 (84%)] Loss: -835711.062500\n",
      "    epoch          : 427\n",
      "    loss           : -888696.771039604\n",
      "    val_loss       : -871089.8643554688\n",
      "Train Epoch: 428 [512/54000 (1%)] Loss: -1013329.375000\n",
      "Train Epoch: 428 [11776/54000 (22%)] Loss: -832221.500000\n",
      "Train Epoch: 428 [23040/54000 (43%)] Loss: -833071.375000\n",
      "Train Epoch: 428 [34304/54000 (64%)] Loss: -891049.375000\n",
      "Train Epoch: 428 [45568/54000 (84%)] Loss: -856927.375000\n",
      "    epoch          : 428\n",
      "    loss           : -887336.7425742574\n",
      "    val_loss       : -871543.126171875\n",
      "Train Epoch: 429 [512/54000 (1%)] Loss: -1011812.562500\n",
      "Train Epoch: 429 [11776/54000 (22%)] Loss: -921640.937500\n",
      "Train Epoch: 429 [23040/54000 (43%)] Loss: -852060.125000\n",
      "Train Epoch: 429 [34304/54000 (64%)] Loss: -860766.250000\n",
      "Train Epoch: 429 [45568/54000 (84%)] Loss: -890249.062500\n",
      "    epoch          : 429\n",
      "    loss           : -888860.4226485149\n",
      "    val_loss       : -871189.5301757812\n",
      "Train Epoch: 430 [512/54000 (1%)] Loss: -1014918.000000\n",
      "Train Epoch: 430 [11776/54000 (22%)] Loss: -845425.500000\n",
      "Train Epoch: 430 [23040/54000 (43%)] Loss: -889999.125000\n",
      "Train Epoch: 430 [34304/54000 (64%)] Loss: -856697.062500\n",
      "Train Epoch: 430 [45568/54000 (84%)] Loss: -895865.875000\n",
      "    epoch          : 430\n",
      "    loss           : -887798.771039604\n",
      "    val_loss       : -872514.948046875\n",
      "Train Epoch: 431 [512/54000 (1%)] Loss: -1011494.875000\n",
      "Train Epoch: 431 [11776/54000 (22%)] Loss: -892700.750000\n",
      "Train Epoch: 431 [23040/54000 (43%)] Loss: -832769.625000\n",
      "Train Epoch: 431 [34304/54000 (64%)] Loss: -892102.875000\n",
      "Train Epoch: 431 [45568/54000 (84%)] Loss: -837524.875000\n",
      "    epoch          : 431\n",
      "    loss           : -888220.6404702971\n",
      "    val_loss       : -871605.2765625\n",
      "Train Epoch: 432 [512/54000 (1%)] Loss: -1012670.062500\n",
      "Train Epoch: 432 [11776/54000 (22%)] Loss: -849282.250000\n",
      "Train Epoch: 432 [23040/54000 (43%)] Loss: -836904.500000\n",
      "Train Epoch: 432 [34304/54000 (64%)] Loss: -853543.500000\n",
      "Train Epoch: 432 [45568/54000 (84%)] Loss: -841872.250000\n",
      "    epoch          : 432\n",
      "    loss           : -888578.0136138614\n",
      "    val_loss       : -871939.7342773437\n",
      "Train Epoch: 433 [512/54000 (1%)] Loss: -1012023.000000\n",
      "Train Epoch: 433 [11776/54000 (22%)] Loss: -852725.625000\n",
      "Train Epoch: 433 [23040/54000 (43%)] Loss: -894806.750000\n",
      "Train Epoch: 433 [34304/54000 (64%)] Loss: -896466.625000\n",
      "Train Epoch: 433 [45568/54000 (84%)] Loss: -862228.500000\n",
      "    epoch          : 433\n",
      "    loss           : -890415.9851485149\n",
      "    val_loss       : -872482.6041015625\n",
      "Train Epoch: 434 [512/54000 (1%)] Loss: -1013133.250000\n",
      "Train Epoch: 434 [11776/54000 (22%)] Loss: -908257.562500\n",
      "Train Epoch: 434 [23040/54000 (43%)] Loss: -836282.250000\n",
      "Train Epoch: 434 [34304/54000 (64%)] Loss: -1014489.500000\n",
      "Train Epoch: 434 [45568/54000 (84%)] Loss: -895329.375000\n",
      "    epoch          : 434\n",
      "    loss           : -890023.9362623763\n",
      "    val_loss       : -870891.8706054688\n",
      "Train Epoch: 435 [512/54000 (1%)] Loss: -1013977.125000\n",
      "Train Epoch: 435 [11776/54000 (22%)] Loss: -921610.312500\n",
      "Train Epoch: 435 [23040/54000 (43%)] Loss: -882492.500000\n",
      "Train Epoch: 435 [34304/54000 (64%)] Loss: -900851.625000\n",
      "Train Epoch: 435 [45568/54000 (84%)] Loss: -902154.125000\n",
      "    epoch          : 435\n",
      "    loss           : -884247.4672029703\n",
      "    val_loss       : -870139.7420898437\n",
      "Train Epoch: 436 [512/54000 (1%)] Loss: -1011129.375000\n",
      "Train Epoch: 436 [11776/54000 (22%)] Loss: -919194.750000\n",
      "Train Epoch: 436 [23040/54000 (43%)] Loss: -904848.125000\n",
      "Train Epoch: 436 [34304/54000 (64%)] Loss: -894341.625000\n",
      "Train Epoch: 436 [45568/54000 (84%)] Loss: -895976.375000\n",
      "    epoch          : 436\n",
      "    loss           : -887850.2413366337\n",
      "    val_loss       : -872068.7104492188\n",
      "Train Epoch: 437 [512/54000 (1%)] Loss: -1009943.125000\n",
      "Train Epoch: 437 [11776/54000 (22%)] Loss: -916974.250000\n",
      "Train Epoch: 437 [23040/54000 (43%)] Loss: -835227.437500\n",
      "Train Epoch: 437 [34304/54000 (64%)] Loss: -837370.125000\n",
      "Train Epoch: 437 [45568/54000 (84%)] Loss: -860470.062500\n",
      "    epoch          : 437\n",
      "    loss           : -887086.1448019802\n",
      "    val_loss       : -871775.7685546875\n",
      "Train Epoch: 438 [512/54000 (1%)] Loss: -1014769.000000\n",
      "Train Epoch: 438 [11776/54000 (22%)] Loss: -843461.500000\n",
      "Train Epoch: 438 [23040/54000 (43%)] Loss: -835932.000000\n",
      "Train Epoch: 438 [34304/54000 (64%)] Loss: -861459.812500\n",
      "Train Epoch: 438 [45568/54000 (84%)] Loss: -838510.562500\n",
      "    epoch          : 438\n",
      "    loss           : -890046.2097772277\n",
      "    val_loss       : -872707.0821289063\n",
      "Train Epoch: 439 [512/54000 (1%)] Loss: -1013640.625000\n",
      "Train Epoch: 439 [11776/54000 (22%)] Loss: -852590.812500\n",
      "Train Epoch: 439 [23040/54000 (43%)] Loss: -890707.312500\n",
      "Train Epoch: 439 [34304/54000 (64%)] Loss: -884830.375000\n",
      "Train Epoch: 439 [45568/54000 (84%)] Loss: -853569.625000\n",
      "    epoch          : 439\n",
      "    loss           : -887214.6212871287\n",
      "    val_loss       : -870840.870703125\n",
      "Train Epoch: 440 [512/54000 (1%)] Loss: -851405.625000\n",
      "Train Epoch: 440 [11776/54000 (22%)] Loss: -850879.625000\n",
      "Train Epoch: 440 [23040/54000 (43%)] Loss: -857028.000000\n",
      "Train Epoch: 440 [34304/54000 (64%)] Loss: -895643.437500\n",
      "Train Epoch: 440 [45568/54000 (84%)] Loss: -892513.625000\n",
      "    epoch          : 440\n",
      "    loss           : -888045.2642326732\n",
      "    val_loss       : -872270.848828125\n",
      "Train Epoch: 441 [512/54000 (1%)] Loss: -922949.187500\n",
      "Train Epoch: 441 [11776/54000 (22%)] Loss: -851456.687500\n",
      "Train Epoch: 441 [23040/54000 (43%)] Loss: -894112.625000\n",
      "Train Epoch: 441 [34304/54000 (64%)] Loss: -897886.062500\n",
      "Train Epoch: 441 [45568/54000 (84%)] Loss: -894291.687500\n",
      "    epoch          : 441\n",
      "    loss           : -888852.9573019802\n",
      "    val_loss       : -870245.8217773438\n",
      "Train Epoch: 442 [512/54000 (1%)] Loss: -1011700.875000\n",
      "Train Epoch: 442 [11776/54000 (22%)] Loss: -917935.437500\n",
      "Train Epoch: 442 [23040/54000 (43%)] Loss: -833322.187500\n",
      "Train Epoch: 442 [34304/54000 (64%)] Loss: -901488.625000\n",
      "Train Epoch: 442 [45568/54000 (84%)] Loss: -888520.687500\n",
      "    epoch          : 442\n",
      "    loss           : -889293.1478960396\n",
      "    val_loss       : -872373.5967773438\n",
      "Train Epoch: 443 [512/54000 (1%)] Loss: -922493.500000\n",
      "Train Epoch: 443 [11776/54000 (22%)] Loss: -850698.375000\n",
      "Train Epoch: 443 [23040/54000 (43%)] Loss: -861691.437500\n",
      "Train Epoch: 443 [34304/54000 (64%)] Loss: -835880.562500\n",
      "Train Epoch: 443 [45568/54000 (84%)] Loss: -839323.000000\n",
      "    epoch          : 443\n",
      "    loss           : -888745.6967821782\n",
      "    val_loss       : -872546.1841796875\n",
      "Train Epoch: 444 [512/54000 (1%)] Loss: -1013422.125000\n",
      "Train Epoch: 444 [11776/54000 (22%)] Loss: -853379.250000\n",
      "Train Epoch: 444 [23040/54000 (43%)] Loss: -840207.437500\n",
      "Train Epoch: 444 [34304/54000 (64%)] Loss: -894278.750000\n",
      "Train Epoch: 444 [45568/54000 (84%)] Loss: -894600.750000\n",
      "    epoch          : 444\n",
      "    loss           : -889921.5266089109\n",
      "    val_loss       : -869847.2744140625\n",
      "Train Epoch: 445 [512/54000 (1%)] Loss: -836394.187500\n",
      "Train Epoch: 445 [11776/54000 (22%)] Loss: -923754.062500\n",
      "Train Epoch: 445 [23040/54000 (43%)] Loss: -899110.812500\n",
      "Train Epoch: 445 [34304/54000 (64%)] Loss: -903448.437500\n",
      "Train Epoch: 445 [45568/54000 (84%)] Loss: -892737.312500\n",
      "    epoch          : 445\n",
      "    loss           : -888442.5779702971\n",
      "    val_loss       : -872266.4518554688\n",
      "Train Epoch: 446 [512/54000 (1%)] Loss: -1011066.875000\n",
      "Train Epoch: 446 [11776/54000 (22%)] Loss: -860159.375000\n",
      "Train Epoch: 446 [23040/54000 (43%)] Loss: -908108.250000\n",
      "Train Epoch: 446 [34304/54000 (64%)] Loss: -892378.250000\n",
      "Train Epoch: 446 [45568/54000 (84%)] Loss: -837612.375000\n",
      "    epoch          : 446\n",
      "    loss           : -889717.2048267326\n",
      "    val_loss       : -871410.984375\n",
      "Train Epoch: 447 [512/54000 (1%)] Loss: -1013719.250000\n",
      "Train Epoch: 447 [11776/54000 (22%)] Loss: -849846.437500\n",
      "Train Epoch: 447 [23040/54000 (43%)] Loss: -832785.812500\n",
      "Train Epoch: 447 [34304/54000 (64%)] Loss: -894937.875000\n",
      "Train Epoch: 447 [45568/54000 (84%)] Loss: -894344.000000\n",
      "    epoch          : 447\n",
      "    loss           : -889272.1107673268\n",
      "    val_loss       : -872723.6314453125\n",
      "Train Epoch: 448 [512/54000 (1%)] Loss: -1014293.250000\n",
      "Train Epoch: 448 [11776/54000 (22%)] Loss: -923349.812500\n",
      "Train Epoch: 448 [23040/54000 (43%)] Loss: -841499.875000\n",
      "Train Epoch: 448 [34304/54000 (64%)] Loss: -858777.125000\n",
      "Train Epoch: 448 [45568/54000 (84%)] Loss: -893936.000000\n",
      "    epoch          : 448\n",
      "    loss           : -889990.1051980198\n",
      "    val_loss       : -872220.6833007813\n",
      "Train Epoch: 449 [512/54000 (1%)] Loss: -1011700.812500\n",
      "Train Epoch: 449 [11776/54000 (22%)] Loss: -854628.250000\n",
      "Train Epoch: 449 [23040/54000 (43%)] Loss: -899193.250000\n",
      "Train Epoch: 449 [34304/54000 (64%)] Loss: -895845.562500\n",
      "Train Epoch: 449 [45568/54000 (84%)] Loss: -896790.000000\n",
      "    epoch          : 449\n",
      "    loss           : -891320.6373762377\n",
      "    val_loss       : -872815.8638671875\n",
      "Train Epoch: 450 [512/54000 (1%)] Loss: -1013421.437500\n",
      "Train Epoch: 450 [11776/54000 (22%)] Loss: -837896.250000\n",
      "Train Epoch: 450 [23040/54000 (43%)] Loss: -837317.875000\n",
      "Train Epoch: 450 [34304/54000 (64%)] Loss: -860266.000000\n",
      "Train Epoch: 450 [45568/54000 (84%)] Loss: -890851.625000\n",
      "    epoch          : 450\n",
      "    loss           : -891510.6615099009\n",
      "    val_loss       : -872814.5572265625\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0716_225739/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [512/54000 (1%)] Loss: -1013213.125000\n",
      "Train Epoch: 451 [11776/54000 (22%)] Loss: -857370.875000\n",
      "Train Epoch: 451 [23040/54000 (43%)] Loss: -832922.625000\n",
      "Train Epoch: 451 [34304/54000 (64%)] Loss: -891657.812500\n",
      "Train Epoch: 451 [45568/54000 (84%)] Loss: -885472.000000\n",
      "    epoch          : 451\n",
      "    loss           : -888522.3824257426\n",
      "    val_loss       : -869086.8251953125\n",
      "Train Epoch: 452 [512/54000 (1%)] Loss: -1010993.000000\n",
      "Train Epoch: 452 [11776/54000 (22%)] Loss: -845439.250000\n",
      "Train Epoch: 452 [23040/54000 (43%)] Loss: -829301.000000\n",
      "Train Epoch: 452 [34304/54000 (64%)] Loss: -892851.937500\n",
      "Train Epoch: 452 [45568/54000 (84%)] Loss: -860808.625000\n",
      "    epoch          : 452\n",
      "    loss           : -886628.3025990099\n",
      "    val_loss       : -871894.7956054688\n",
      "Train Epoch: 453 [512/54000 (1%)] Loss: -1013206.812500\n",
      "Train Epoch: 453 [11776/54000 (22%)] Loss: -857744.375000\n",
      "Train Epoch: 453 [23040/54000 (43%)] Loss: -863393.125000\n",
      "Train Epoch: 453 [34304/54000 (64%)] Loss: -839155.812500\n",
      "Train Epoch: 453 [45568/54000 (84%)] Loss: -894113.562500\n",
      "    epoch          : 453\n",
      "    loss           : -890337.9201732674\n",
      "    val_loss       : -873288.515625\n",
      "Train Epoch: 454 [512/54000 (1%)] Loss: -1015413.937500\n",
      "Train Epoch: 454 [11776/54000 (22%)] Loss: -922503.750000\n",
      "Train Epoch: 454 [23040/54000 (43%)] Loss: -858498.562500\n",
      "Train Epoch: 454 [34304/54000 (64%)] Loss: -1012502.437500\n",
      "Train Epoch: 454 [45568/54000 (84%)] Loss: -842602.812500\n",
      "    epoch          : 454\n",
      "    loss           : -890581.291460396\n",
      "    val_loss       : -872627.2890625\n",
      "Train Epoch: 455 [512/54000 (1%)] Loss: -1016811.937500\n",
      "Train Epoch: 455 [11776/54000 (22%)] Loss: -864462.187500\n",
      "Train Epoch: 455 [23040/54000 (43%)] Loss: -836214.687500\n",
      "Train Epoch: 455 [34304/54000 (64%)] Loss: -892363.375000\n",
      "Train Epoch: 455 [45568/54000 (84%)] Loss: -858169.187500\n",
      "    epoch          : 455\n",
      "    loss           : -890894.5686881188\n",
      "    val_loss       : -871590.1165039062\n",
      "Train Epoch: 456 [512/54000 (1%)] Loss: -1015233.875000\n",
      "Train Epoch: 456 [11776/54000 (22%)] Loss: -848008.687500\n",
      "Train Epoch: 456 [23040/54000 (43%)] Loss: -858090.250000\n",
      "Train Epoch: 456 [34304/54000 (64%)] Loss: -892612.750000\n",
      "Train Epoch: 456 [45568/54000 (84%)] Loss: -895668.375000\n",
      "    epoch          : 456\n",
      "    loss           : -888007.4764851485\n",
      "    val_loss       : -872276.3454101563\n",
      "Train Epoch: 457 [512/54000 (1%)] Loss: -1011903.750000\n",
      "Train Epoch: 457 [11776/54000 (22%)] Loss: -833273.312500\n",
      "Train Epoch: 457 [23040/54000 (43%)] Loss: -859525.625000\n",
      "Train Epoch: 457 [34304/54000 (64%)] Loss: -903111.250000\n",
      "Train Epoch: 457 [45568/54000 (84%)] Loss: -841909.500000\n",
      "    epoch          : 457\n",
      "    loss           : -889181.9077970297\n",
      "    val_loss       : -871235.8986328125\n",
      "Train Epoch: 458 [512/54000 (1%)] Loss: -1015799.375000\n",
      "Train Epoch: 458 [11776/54000 (22%)] Loss: -833530.062500\n",
      "Train Epoch: 458 [23040/54000 (43%)] Loss: -902853.687500\n",
      "Train Epoch: 458 [34304/54000 (64%)] Loss: -895472.875000\n",
      "Train Epoch: 458 [45568/54000 (84%)] Loss: -861735.312500\n",
      "    epoch          : 458\n",
      "    loss           : -890119.9715346535\n",
      "    val_loss       : -872806.0416015625\n",
      "Train Epoch: 459 [512/54000 (1%)] Loss: -1014483.000000\n",
      "Train Epoch: 459 [11776/54000 (22%)] Loss: -917571.250000\n",
      "Train Epoch: 459 [23040/54000 (43%)] Loss: -827182.812500\n",
      "Train Epoch: 459 [34304/54000 (64%)] Loss: -1014988.250000\n",
      "Train Epoch: 459 [45568/54000 (84%)] Loss: -842411.375000\n",
      "    epoch          : 459\n",
      "    loss           : -889867.2648514851\n",
      "    val_loss       : -872517.5576171875\n",
      "Train Epoch: 460 [512/54000 (1%)] Loss: -1013757.375000\n",
      "Train Epoch: 460 [11776/54000 (22%)] Loss: -855615.750000\n",
      "Train Epoch: 460 [23040/54000 (43%)] Loss: -896735.875000\n",
      "Train Epoch: 460 [34304/54000 (64%)] Loss: -858270.500000\n",
      "Train Epoch: 460 [45568/54000 (84%)] Loss: -837113.062500\n",
      "    epoch          : 460\n",
      "    loss           : -889835.9566831683\n",
      "    val_loss       : -870386.9729492187\n",
      "Train Epoch: 461 [512/54000 (1%)] Loss: -1011327.937500\n",
      "Train Epoch: 461 [11776/54000 (22%)] Loss: -922892.062500\n",
      "Train Epoch: 461 [23040/54000 (43%)] Loss: -889257.625000\n",
      "Train Epoch: 461 [34304/54000 (64%)] Loss: -895131.500000\n",
      "Train Epoch: 461 [45568/54000 (84%)] Loss: -886623.375000\n",
      "    epoch          : 461\n",
      "    loss           : -888382.0327970297\n",
      "    val_loss       : -871344.36953125\n",
      "Train Epoch: 462 [512/54000 (1%)] Loss: -1010122.937500\n",
      "Train Epoch: 462 [11776/54000 (22%)] Loss: -919931.625000\n",
      "Train Epoch: 462 [23040/54000 (43%)] Loss: -907975.937500\n",
      "Train Epoch: 462 [34304/54000 (64%)] Loss: -1016119.375000\n",
      "Train Epoch: 462 [45568/54000 (84%)] Loss: -892825.750000\n",
      "    epoch          : 462\n",
      "    loss           : -888826.667079208\n",
      "    val_loss       : -872292.6330078125\n",
      "Train Epoch: 463 [512/54000 (1%)] Loss: -908903.187500\n",
      "Train Epoch: 463 [11776/54000 (22%)] Loss: -850430.812500\n",
      "Train Epoch: 463 [23040/54000 (43%)] Loss: -842444.812500\n",
      "Train Epoch: 463 [34304/54000 (64%)] Loss: -864095.062500\n",
      "Train Epoch: 463 [45568/54000 (84%)] Loss: -838288.250000\n",
      "    epoch          : 463\n",
      "    loss           : -890591.7728960396\n",
      "    val_loss       : -872343.1357421875\n",
      "Train Epoch: 464 [512/54000 (1%)] Loss: -1015786.625000\n",
      "Train Epoch: 464 [11776/54000 (22%)] Loss: -854326.187500\n",
      "Train Epoch: 464 [23040/54000 (43%)] Loss: -894164.187500\n",
      "Train Epoch: 464 [34304/54000 (64%)] Loss: -1013704.125000\n",
      "Train Epoch: 464 [45568/54000 (84%)] Loss: -890684.000000\n",
      "    epoch          : 464\n",
      "    loss           : -891025.4405940594\n",
      "    val_loss       : -871376.942578125\n",
      "Train Epoch: 465 [512/54000 (1%)] Loss: -1015300.062500\n",
      "Train Epoch: 465 [11776/54000 (22%)] Loss: -904658.875000\n",
      "Train Epoch: 465 [23040/54000 (43%)] Loss: -854062.312500\n",
      "Train Epoch: 465 [34304/54000 (64%)] Loss: -1015587.562500\n",
      "Train Epoch: 465 [45568/54000 (84%)] Loss: -895039.812500\n",
      "    epoch          : 465\n",
      "    loss           : -889853.0996287129\n",
      "    val_loss       : -871660.0466796875\n",
      "Train Epoch: 466 [512/54000 (1%)] Loss: -1015316.562500\n",
      "Train Epoch: 466 [11776/54000 (22%)] Loss: -840579.375000\n",
      "Train Epoch: 466 [23040/54000 (43%)] Loss: -837687.375000\n",
      "Train Epoch: 466 [34304/54000 (64%)] Loss: -1013025.187500\n",
      "Train Epoch: 466 [45568/54000 (84%)] Loss: -861558.562500\n",
      "    epoch          : 466\n",
      "    loss           : -890018.4721534654\n",
      "    val_loss       : -871091.9200195313\n",
      "Train Epoch: 467 [512/54000 (1%)] Loss: -1015137.125000\n",
      "Train Epoch: 467 [11776/54000 (22%)] Loss: -903140.125000\n",
      "Train Epoch: 467 [23040/54000 (43%)] Loss: -887254.437500\n",
      "Train Epoch: 467 [34304/54000 (64%)] Loss: -854675.000000\n",
      "Train Epoch: 467 [45568/54000 (84%)] Loss: -887380.937500\n",
      "    epoch          : 467\n",
      "    loss           : -886781.344059406\n",
      "    val_loss       : -872682.2975585938\n",
      "Train Epoch: 468 [512/54000 (1%)] Loss: -1016875.000000\n",
      "Train Epoch: 468 [11776/54000 (22%)] Loss: -927791.437500\n",
      "Train Epoch: 468 [23040/54000 (43%)] Loss: -864721.250000\n",
      "Train Epoch: 468 [34304/54000 (64%)] Loss: -839294.250000\n",
      "Train Epoch: 468 [45568/54000 (84%)] Loss: -895537.375000\n",
      "    epoch          : 468\n",
      "    loss           : -891038.4158415842\n",
      "    val_loss       : -871416.3556640625\n",
      "Train Epoch: 469 [512/54000 (1%)] Loss: -1013462.375000\n",
      "Train Epoch: 469 [11776/54000 (22%)] Loss: -858786.000000\n",
      "Train Epoch: 469 [23040/54000 (43%)] Loss: -862800.375000\n",
      "Train Epoch: 469 [34304/54000 (64%)] Loss: -884223.625000\n",
      "Train Epoch: 469 [45568/54000 (84%)] Loss: -890931.625000\n",
      "    epoch          : 469\n",
      "    loss           : -888137.9084158416\n",
      "    val_loss       : -870388.8247070312\n",
      "Train Epoch: 470 [512/54000 (1%)] Loss: -1014881.250000\n",
      "Train Epoch: 470 [11776/54000 (22%)] Loss: -914708.250000\n",
      "Train Epoch: 470 [23040/54000 (43%)] Loss: -877893.125000\n",
      "Train Epoch: 470 [34304/54000 (64%)] Loss: -839815.875000\n",
      "Train Epoch: 470 [45568/54000 (84%)] Loss: -895865.375000\n",
      "    epoch          : 470\n",
      "    loss           : -887985.2623762377\n",
      "    val_loss       : -872794.930859375\n",
      "Train Epoch: 471 [512/54000 (1%)] Loss: -1016242.250000\n",
      "Train Epoch: 471 [11776/54000 (22%)] Loss: -857595.312500\n",
      "Train Epoch: 471 [23040/54000 (43%)] Loss: -840293.375000\n",
      "Train Epoch: 471 [34304/54000 (64%)] Loss: -894465.750000\n",
      "Train Epoch: 471 [45568/54000 (84%)] Loss: -839129.000000\n",
      "    epoch          : 471\n",
      "    loss           : -891319.6287128713\n",
      "    val_loss       : -872161.7\n",
      "Train Epoch: 472 [512/54000 (1%)] Loss: -1014728.062500\n",
      "Train Epoch: 472 [11776/54000 (22%)] Loss: -853446.750000\n",
      "Train Epoch: 472 [23040/54000 (43%)] Loss: -903351.687500\n",
      "Train Epoch: 472 [34304/54000 (64%)] Loss: -859680.500000\n",
      "Train Epoch: 472 [45568/54000 (84%)] Loss: -897446.875000\n",
      "    epoch          : 472\n",
      "    loss           : -890052.8316831683\n",
      "    val_loss       : -872516.5887695312\n",
      "Train Epoch: 473 [512/54000 (1%)] Loss: -1017266.812500\n",
      "Train Epoch: 473 [11776/54000 (22%)] Loss: -858825.750000\n",
      "Train Epoch: 473 [23040/54000 (43%)] Loss: -903320.687500\n",
      "Train Epoch: 473 [34304/54000 (64%)] Loss: -844425.500000\n",
      "Train Epoch: 473 [45568/54000 (84%)] Loss: -890530.812500\n",
      "    epoch          : 473\n",
      "    loss           : -891297.0686881188\n",
      "    val_loss       : -871994.601953125\n",
      "Train Epoch: 474 [512/54000 (1%)] Loss: -1016441.250000\n",
      "Train Epoch: 474 [11776/54000 (22%)] Loss: -894905.625000\n",
      "Train Epoch: 474 [23040/54000 (43%)] Loss: -910384.875000\n",
      "Train Epoch: 474 [34304/54000 (64%)] Loss: -866552.375000\n",
      "Train Epoch: 474 [45568/54000 (84%)] Loss: -893792.500000\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch   474: reducing learning rate of group 0 to 5.0000e-04.\n",
      "    epoch          : 474\n",
      "    loss           : -892282.0464108911\n",
      "    val_loss       : -872629.4065429687\n",
      "Train Epoch: 475 [512/54000 (1%)] Loss: -1015703.750000\n",
      "Train Epoch: 475 [11776/54000 (22%)] Loss: -856069.187500\n",
      "Train Epoch: 475 [23040/54000 (43%)] Loss: -845272.875000\n",
      "Train Epoch: 475 [34304/54000 (64%)] Loss: -846595.500000\n",
      "Train Epoch: 475 [45568/54000 (84%)] Loss: -842910.750000\n",
      "    epoch          : 475\n",
      "    loss           : -894373.9220297029\n",
      "    val_loss       : -873134.0327148438\n",
      "Train Epoch: 476 [512/54000 (1%)] Loss: -1016195.437500\n",
      "Train Epoch: 476 [11776/54000 (22%)] Loss: -842632.187500\n",
      "Train Epoch: 476 [23040/54000 (43%)] Loss: -898645.187500\n",
      "Train Epoch: 476 [34304/54000 (64%)] Loss: -899321.000000\n",
      "Train Epoch: 476 [45568/54000 (84%)] Loss: -848156.250000\n",
      "    epoch          : 476\n",
      "    loss           : -894649.9938118812\n",
      "    val_loss       : -872744.97734375\n",
      "Train Epoch: 477 [512/54000 (1%)] Loss: -926913.750000\n",
      "Train Epoch: 477 [11776/54000 (22%)] Loss: -858986.125000\n",
      "Train Epoch: 477 [23040/54000 (43%)] Loss: -844065.000000\n",
      "Train Epoch: 477 [34304/54000 (64%)] Loss: -899951.625000\n",
      "Train Epoch: 477 [45568/54000 (84%)] Loss: -900096.125000\n",
      "    epoch          : 477\n",
      "    loss           : -894717.6256188119\n",
      "    val_loss       : -873233.0094726563\n",
      "Train Epoch: 478 [512/54000 (1%)] Loss: -1017619.000000\n",
      "Train Epoch: 478 [11776/54000 (22%)] Loss: -867592.375000\n",
      "Train Epoch: 478 [23040/54000 (43%)] Loss: -911022.437500\n",
      "Train Epoch: 478 [34304/54000 (64%)] Loss: -909003.062500\n",
      "Train Epoch: 478 [45568/54000 (84%)] Loss: -898449.250000\n",
      "    epoch          : 478\n",
      "    loss           : -894955.5705445545\n",
      "    val_loss       : -873434.4663085938\n",
      "Train Epoch: 479 [512/54000 (1%)] Loss: -928777.500000\n",
      "Train Epoch: 479 [11776/54000 (22%)] Loss: -859302.937500\n",
      "Train Epoch: 479 [23040/54000 (43%)] Loss: -911577.500000\n",
      "Train Epoch: 479 [34304/54000 (64%)] Loss: -901533.125000\n",
      "Train Epoch: 479 [45568/54000 (84%)] Loss: -899054.125000\n",
      "    epoch          : 479\n",
      "    loss           : -894891.2753712871\n",
      "    val_loss       : -872487.0486328125\n",
      "Train Epoch: 480 [512/54000 (1%)] Loss: -1015673.250000\n",
      "Train Epoch: 480 [11776/54000 (22%)] Loss: -857270.187500\n",
      "Train Epoch: 480 [23040/54000 (43%)] Loss: -907675.937500\n",
      "Train Epoch: 480 [34304/54000 (64%)] Loss: -1017890.625000\n",
      "Train Epoch: 480 [45568/54000 (84%)] Loss: -901720.125000\n",
      "    epoch          : 480\n",
      "    loss           : -894991.8267326732\n",
      "    val_loss       : -873296.1206054688\n",
      "Train Epoch: 481 [512/54000 (1%)] Loss: -1017057.812500\n",
      "Train Epoch: 481 [11776/54000 (22%)] Loss: -859470.312500\n",
      "Train Epoch: 481 [23040/54000 (43%)] Loss: -910207.125000\n",
      "Train Epoch: 481 [34304/54000 (64%)] Loss: -899985.937500\n",
      "Train Epoch: 481 [45568/54000 (84%)] Loss: -900327.125000\n",
      "    epoch          : 481\n",
      "    loss           : -894961.1775990099\n",
      "    val_loss       : -873651.4830078125\n",
      "Train Epoch: 482 [512/54000 (1%)] Loss: -1017620.687500\n",
      "Train Epoch: 482 [11776/54000 (22%)] Loss: -857898.937500\n",
      "Train Epoch: 482 [23040/54000 (43%)] Loss: -865987.687500\n",
      "Train Epoch: 482 [34304/54000 (64%)] Loss: -895945.312500\n",
      "Train Epoch: 482 [45568/54000 (84%)] Loss: -901351.625000\n",
      "    epoch          : 482\n",
      "    loss           : -895086.6646039604\n",
      "    val_loss       : -873449.9350585938\n",
      "Train Epoch: 483 [512/54000 (1%)] Loss: -1017979.312500\n",
      "Train Epoch: 483 [11776/54000 (22%)] Loss: -923983.062500\n",
      "Train Epoch: 483 [23040/54000 (43%)] Loss: -868913.375000\n",
      "Train Epoch: 483 [34304/54000 (64%)] Loss: -908304.812500\n",
      "Train Epoch: 483 [45568/54000 (84%)] Loss: -901208.250000\n",
      "    epoch          : 483\n",
      "    loss           : -895187.4678217822\n",
      "    val_loss       : -873161.0220703125\n",
      "Train Epoch: 484 [512/54000 (1%)] Loss: -1019605.625000\n",
      "Train Epoch: 484 [11776/54000 (22%)] Loss: -858751.750000\n",
      "Train Epoch: 484 [23040/54000 (43%)] Loss: -842570.125000\n",
      "Train Epoch: 484 [34304/54000 (64%)] Loss: -1016442.000000\n",
      "Train Epoch: 484 [45568/54000 (84%)] Loss: -894830.937500\n",
      "    epoch          : 484\n",
      "    loss           : -895103.9839108911\n",
      "    val_loss       : -872864.0509765625\n",
      "Train Epoch: 485 [512/54000 (1%)] Loss: -1016369.750000\n",
      "Train Epoch: 485 [11776/54000 (22%)] Loss: -853795.750000\n",
      "Train Epoch: 485 [23040/54000 (43%)] Loss: -843300.812500\n",
      "Train Epoch: 485 [34304/54000 (64%)] Loss: -901279.000000\n",
      "Train Epoch: 485 [45568/54000 (84%)] Loss: -899078.750000\n",
      "    epoch          : 485\n",
      "    loss           : -895240.3657178218\n",
      "    val_loss       : -872816.498046875\n",
      "Train Epoch: 486 [512/54000 (1%)] Loss: -1016941.562500\n",
      "Train Epoch: 486 [11776/54000 (22%)] Loss: -841826.187500\n",
      "Train Epoch: 486 [23040/54000 (43%)] Loss: -899020.187500\n",
      "Train Epoch: 486 [34304/54000 (64%)] Loss: -868868.750000\n",
      "Train Epoch: 486 [45568/54000 (84%)] Loss: -899696.562500\n",
      "    epoch          : 486\n",
      "    loss           : -895232.1788366337\n",
      "    val_loss       : -872603.9942382813\n",
      "Train Epoch: 487 [512/54000 (1%)] Loss: -1016974.687500\n",
      "Train Epoch: 487 [11776/54000 (22%)] Loss: -925230.187500\n",
      "Train Epoch: 487 [23040/54000 (43%)] Loss: -839426.750000\n",
      "Train Epoch: 487 [34304/54000 (64%)] Loss: -899921.187500\n",
      "Train Epoch: 487 [45568/54000 (84%)] Loss: -899175.500000\n",
      "    epoch          : 487\n",
      "    loss           : -895192.2568069306\n",
      "    val_loss       : -872881.1735351563\n",
      "Train Epoch: 488 [512/54000 (1%)] Loss: -1019350.500000\n",
      "Train Epoch: 488 [11776/54000 (22%)] Loss: -857551.125000\n",
      "Train Epoch: 488 [23040/54000 (43%)] Loss: -909004.812500\n",
      "Train Epoch: 488 [34304/54000 (64%)] Loss: -1017748.062500\n",
      "Train Epoch: 488 [45568/54000 (84%)] Loss: -895668.875000\n",
      "    epoch          : 488\n",
      "    loss           : -895404.0550742574\n",
      "    val_loss       : -873168.649609375\n",
      "Train Epoch: 489 [512/54000 (1%)] Loss: -925687.812500\n",
      "Train Epoch: 489 [11776/54000 (22%)] Loss: -909143.625000\n",
      "Train Epoch: 489 [23040/54000 (43%)] Loss: -912043.125000\n",
      "Train Epoch: 489 [34304/54000 (64%)] Loss: -1017119.750000\n",
      "Train Epoch: 489 [45568/54000 (84%)] Loss: -899963.437500\n",
      "    epoch          : 489\n",
      "    loss           : -895300.0680693069\n",
      "    val_loss       : -872507.1791992188\n",
      "Train Epoch: 490 [512/54000 (1%)] Loss: -1016314.812500\n",
      "Train Epoch: 490 [11776/54000 (22%)] Loss: -860324.312500\n",
      "Train Epoch: 490 [23040/54000 (43%)] Loss: -841276.187500\n",
      "Train Epoch: 490 [34304/54000 (64%)] Loss: -868397.125000\n",
      "Train Epoch: 490 [45568/54000 (84%)] Loss: -898971.625000\n",
      "    epoch          : 490\n",
      "    loss           : -895351.6386138614\n",
      "    val_loss       : -872497.026953125\n",
      "Train Epoch: 491 [512/54000 (1%)] Loss: -1015809.562500\n",
      "Train Epoch: 491 [11776/54000 (22%)] Loss: -857394.187500\n",
      "Train Epoch: 491 [23040/54000 (43%)] Loss: -908183.937500\n",
      "Train Epoch: 491 [34304/54000 (64%)] Loss: -1017486.750000\n",
      "Train Epoch: 491 [45568/54000 (84%)] Loss: -844375.000000\n",
      "    epoch          : 491\n",
      "    loss           : -895418.0990099009\n",
      "    val_loss       : -872227.3517578125\n",
      "Train Epoch: 492 [512/54000 (1%)] Loss: -1018716.437500\n",
      "Train Epoch: 492 [11776/54000 (22%)] Loss: -859796.937500\n",
      "Train Epoch: 492 [23040/54000 (43%)] Loss: -844248.500000\n",
      "Train Epoch: 492 [34304/54000 (64%)] Loss: -845876.812500\n",
      "Train Epoch: 492 [45568/54000 (84%)] Loss: -901494.750000\n",
      "    epoch          : 492\n",
      "    loss           : -895502.2636138614\n",
      "    val_loss       : -872729.0723632813\n",
      "Train Epoch: 493 [512/54000 (1%)] Loss: -1016087.312500\n",
      "Train Epoch: 493 [11776/54000 (22%)] Loss: -861230.875000\n",
      "Train Epoch: 493 [23040/54000 (43%)] Loss: -871532.187500\n",
      "Train Epoch: 493 [34304/54000 (64%)] Loss: -899287.187500\n",
      "Train Epoch: 493 [45568/54000 (84%)] Loss: -904837.437500\n",
      "    epoch          : 493\n",
      "    loss           : -895477.3824257426\n",
      "    val_loss       : -872851.825390625\n",
      "Train Epoch: 494 [512/54000 (1%)] Loss: -1017251.625000\n",
      "Train Epoch: 494 [11776/54000 (22%)] Loss: -861927.875000\n",
      "Train Epoch: 494 [23040/54000 (43%)] Loss: -870852.625000\n",
      "Train Epoch: 494 [34304/54000 (64%)] Loss: -872540.500000\n",
      "Train Epoch: 494 [45568/54000 (84%)] Loss: -898310.500000\n",
      "    epoch          : 494\n",
      "    loss           : -895549.4040841584\n",
      "    val_loss       : -873365.587890625\n",
      "Train Epoch: 495 [512/54000 (1%)] Loss: -1017034.687500\n",
      "Train Epoch: 495 [11776/54000 (22%)] Loss: -861820.125000\n",
      "Train Epoch: 495 [23040/54000 (43%)] Loss: -909010.250000\n",
      "Train Epoch: 495 [34304/54000 (64%)] Loss: -847750.937500\n",
      "Train Epoch: 495 [45568/54000 (84%)] Loss: -900328.500000\n",
      "    epoch          : 495\n",
      "    loss           : -895563.8081683168\n",
      "    val_loss       : -873746.9805664063\n",
      "Train Epoch: 496 [512/54000 (1%)] Loss: -1018024.812500\n",
      "Train Epoch: 496 [11776/54000 (22%)] Loss: -861188.250000\n",
      "Train Epoch: 496 [23040/54000 (43%)] Loss: -910211.437500\n",
      "Train Epoch: 496 [34304/54000 (64%)] Loss: -898055.875000\n",
      "Train Epoch: 496 [45568/54000 (84%)] Loss: -900888.125000\n",
      "    epoch          : 496\n",
      "    loss           : -895629.5618811881\n",
      "    val_loss       : -872404.3459960937\n",
      "Train Epoch: 497 [512/54000 (1%)] Loss: -1016504.875000\n",
      "Train Epoch: 497 [11776/54000 (22%)] Loss: -860208.312500\n",
      "Train Epoch: 497 [23040/54000 (43%)] Loss: -847038.750000\n",
      "Train Epoch: 497 [34304/54000 (64%)] Loss: -901283.062500\n",
      "Train Epoch: 497 [45568/54000 (84%)] Loss: -869946.187500\n",
      "    epoch          : 497\n",
      "    loss           : -895638.1912128713\n",
      "    val_loss       : -872185.6646484375\n",
      "Train Epoch: 498 [512/54000 (1%)] Loss: -1016512.812500\n",
      "Train Epoch: 498 [11776/54000 (22%)] Loss: -858178.750000\n",
      "Train Epoch: 498 [23040/54000 (43%)] Loss: -902589.750000\n",
      "Train Epoch: 498 [34304/54000 (64%)] Loss: -1018714.000000\n",
      "Train Epoch: 498 [45568/54000 (84%)] Loss: -898464.937500\n",
      "    epoch          : 498\n",
      "    loss           : -895650.5340346535\n",
      "    val_loss       : -873048.2364257813\n",
      "Train Epoch: 499 [512/54000 (1%)] Loss: -1018789.875000\n",
      "Train Epoch: 499 [11776/54000 (22%)] Loss: -925445.500000\n",
      "Train Epoch: 499 [23040/54000 (43%)] Loss: -900473.312500\n",
      "Train Epoch: 499 [34304/54000 (64%)] Loss: -894135.562500\n",
      "Train Epoch: 499 [45568/54000 (84%)] Loss: -898245.562500\n",
      "    epoch          : 499\n",
      "    loss           : -895736.8638613861\n",
      "    val_loss       : -872433.8201171875\n",
      "Train Epoch: 500 [512/54000 (1%)] Loss: -1018753.250000\n",
      "Train Epoch: 500 [11776/54000 (22%)] Loss: -873569.687500\n",
      "Train Epoch: 500 [23040/54000 (43%)] Loss: -845692.562500\n",
      "Train Epoch: 500 [34304/54000 (64%)] Loss: -1015754.812500\n",
      "Train Epoch: 500 [45568/54000 (84%)] Loss: -900464.750000\n",
      "    epoch          : 500\n",
      "    loss           : -895744.9344059406\n",
      "    val_loss       : -872361.91328125\n",
      "Saving checkpoint: saved/models/Mnist_VaeCategory/0716_225739/checkpoint-epoch500.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAECategoryModel(\n",
       "  (_category): CartesianCategory(\n",
       "    (generator_0): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_0_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "        (1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=96, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_1_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=160, bias=True)\n",
       "        (1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (4): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=160, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_2_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=288, bias=True)\n",
       "        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=288, out_features=288, bias=True)\n",
       "        (4): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=288, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_3): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_3_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=424, bias=True)\n",
       "        (1): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=424, out_features=424, bias=True)\n",
       "        (4): LayerNorm((424,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=424, out_features=64, bias=True)\n",
       "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_4_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=192, bias=True)\n",
       "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (4): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=192, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_5_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=320, bias=True)\n",
       "        (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (4): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=320, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_6): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_6_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=456, bias=True)\n",
       "        (1): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=456, out_features=456, bias=True)\n",
       "        (4): LayerNorm((456,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=456, out_features=128, bias=True)\n",
       "        (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): StandardNormal()\n",
       "      (combination_layer): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_7_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=384, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_8): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_8_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=520, bias=True)\n",
       "        (1): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=520, out_features=520, bias=True)\n",
       "        (4): LayerNorm((520,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=520, out_features=256, bias=True)\n",
       "        (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (generator_9): DensityDecoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=784, bias=True)\n",
       "        (7): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): ContinuousBernoulliModel()\n",
       "    )\n",
       "    (generator_9_dagger): DensityEncoder(\n",
       "      (neural_layers): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=648, bias=True)\n",
       "        (1): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): Linear(in_features=648, out_features=648, bias=True)\n",
       "        (4): LayerNorm((648,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): PReLU(num_parameters=1)\n",
       "        (6): Linear(in_features=648, out_features=512, bias=True)\n",
       "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (distribution): DiagonalGaussian(\n",
       "        (parameterization): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (global_element_0): StandardNormal()\n",
       "    (global_element_1): StandardNormal()\n",
       "    (global_element_2): StandardNormal()\n",
       "    (global_element_3): StandardNormal()\n",
       "    (global_element_4): StandardNormal()\n",
       "  )\n",
       "  (guide_embedding): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (guide_confidences): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (guide_arrow_distances): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Linear(in_features=256, out_features=15, bias=True)\n",
       "    (4): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAR4klEQVR4nO3dfbBcdX3H8fcnySVAHiQxkIkhhgdjlacGvQVrKMVBKGIV7BTHDEropI3TCoWRWhxrC7Y6MIyKj8UGyRAEoY6ISVukxPCQohW5QIDQIA8xkJBIgAiEQJN7b779Y0/octk9e+/u2T2b+/u8ZnZ29/zO2fPdvfu55+x5+ikiMLPRb0zZBZhZZzjsZolw2M0S4bCbJcJhN0uEw26WCId9lJF0saRrm5z2dyTdL2mbpL8uuraiSXqrpJcljS27lj2Bw14QScdJ+rmkFyVtlfQzSb9Xdl0j9LfAHRExKSK+UXYxjUTEUxExMSIGy65lT+CwF0DSZODfgW8CU4GZwBeAHWXW1YTZwMP1GrtpCSppXJnT74kc9mK8HSAiro+IwYh4NSJujYgHASQdKuk2Sc9Lek7SdZL22z2xpPWSPiPpQUnbJV0labqkn2Sr1D+VNCUb9yBJIWmRpE2SNku6oF5hkt6TrXG8IOkBSSfUGe824H3At7JV47dLulrSFZJulrQdeJ+kN0m6RtKzkp6U9HlJY7LXODtbo7k8m986Se/Nhm+QtEXSgpxa75B0iaRfZmtIyyRNHfK+F0p6Critati4bJy3SFqerVk9Lukvql77Ykk/lHStpJeAs4f1lx1NIsK3Fm/AZOB5YCnwAWDKkPa3AScB44H9gVXA16ra1wO/AKZTWSvYAtwHHJ1NcxtwUTbuQUAA1wMTgCOBZ4H3Z+0XA9dmj2dmdZ1K5R/7Sdnz/eu8jzuAP696fjXwIjAvm35v4BpgGTApq+VRYGE2/tnAAPBnwFjgi8BTwLez93EysA2YmDP/p4Ejsvd2Y9V72f2+r8na9qkaNi4b507gn7M652afy4lVn0s/cHr2XvYp+3vT8e9p2QWMlhvwziwcG7Mv/HJgep1xTwfur3q+Hjiz6vmNwBVVz88Ffpw93v0Ff0dV+2XAVdnj6rBfCHxvyLz/E1hQp65aYb+m6vlYKj9NDqsa9kkqv/N3h/2xqrYjs1qnVw17HpibM/9Lq54fBuzM5rv7fR9S1f5a2IFZwCAwqar9EuDqqs9lVdnfkzJvXo0vSESsjYizI+JAKkumtwBfA5B0gKQbJD2drUJeC0wb8hLPVD1+tcbziUPG31D1+MlsfkPNBs7IVqlfkPQCcBwwYwRvrXo+04C9svlVz3tm1fOhdRMRjd5Lvfk9CfTw+s9qA7W9BdgaEdtyaqs3bRIc9jaIiEeoLBWPyAZdQmUJdFRETAY+DqjF2cyqevxWYFONcTZQWbLvV3WbEBGXjmA+1adFPkdlVXj2kHk/PYLXa2To++rP5lurnmqbgKmSJuXUlvQpng57ASS9Q9IFkg7Mns8C5lP5HQ6V37cvAy9Imgl8poDZ/r2kfSUdTuU38r/WGOda4EOS/kjSWEl7Szphd50jFZVdXD8AviRpkqTZwKez+RTl45IOk7Qv8I/AD2MYu9YiYgPwc+CS7H0eBSwEriuwtj2aw16MbcCxwN3ZVutfAGuA3VvJvwC8i8rGrv8AflTAPO8EHgdWAl+OiFuHjpAF4DTgc1Q2Vm2g8o+mlb/7ucB2YB1wF/B9YEkLrzfU96isFf2Gyoa2kRzcM5/K7/hNwE1UNmquKLC2PZqyjRe2h5B0EPBroCciBsqtpliS7qCycfG7ZdcyGnnJbpYIh90sEV6NN0uEl+xmiejoyQB7aXzszYROztIsKf/LdnbGjprHcLR65tApwNepHM743UYHa+zNBI7Via3M0sxy3B0r67Y1vRqfne74bSonfhwGzJd0WLOvZ2bt1cpv9mOAxyNiXUTsBG6gcgCHmXWhVsI+k9efWLCR1590AEB23nWfpL7+Pe5aDmajRythr7UR4A378SJicUT0RkRvD+NbmJ2ZtaKVsG/k9WcoHUjtM6/MrAu0EvZ7gDmSDpa0F/AxKhdsMLMu1PSut4gYkHQOlSufjAWWRETdixWaWbla2s8eETcDNxdUi5m1kQ+XNUuEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRHS0y2azETnmyNxm3f+r3Pbo31lkNXs8L9nNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0R4P7uV5uVbDsltX3b4d3Lbz5x9fJHljHothV3SemAbMAgMRERvEUWZWfGKWLK/LyKeK+B1zKyN/JvdLBGthj2AWyXdK2lRrREkLZLUJ6mvnx0tzs7MmtXqavy8iNgk6QBghaRHImJV9QgRsRhYDDBZU6PF+ZlZk1paskfEpux+C3ATcEwRRZlZ8ZoOu6QJkibtfgycDKwpqjAzK1Yrq/HTgZsk7X6d70fELYVUZaNH5ftR04ojbmgwcU9+867BkdeTsKbDHhHrgN8tsBYzayPvejNLhMNulgiH3SwRDrtZIhx2s0T4FFdrq/6T3l23bd8x9+dO+/DOV4suJ2lespslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmifB+9lHu6Qvfm9t+4NfuzW2PHa1dSuzDl/+06WkX/NOnc9vfzH83/dop8pLdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uE97OPAhs+X39f+q2LLsuddtHiD+W2Dzbaz55zqWiA86esz58+x7Sl9+S2u3uhkfGS3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhPez7wka7Mu+/y+/XrdtXX/+//PB3/62qZJ223Xc3AZj5F8bPk8MDDQ9rb1RwyW7pCWStkhaUzVsqqQVkh7L7qe0t0wza9VwVuOvBk4ZMuyzwMqImAOszJ6bWRdrGPaIWAVsHTL4NGBp9ngpcHrBdZlZwZrdQDc9IjYDZPcH1BtR0iJJfZL6+mntemZm1ry2b42PiMUR0RsRvT2Mb/fszKyOZsP+jKQZANn9luJKMrN2aDbsy4EF2eMFwLJiyjGzdmm4n13S9cAJwDRJG4GLgEuBH0haCDwFnNHOIlP3D0/kX9t9vHrqtn1w+Tm5087h7qZqeu31v3N709O+uMv9r3dSw7BHxPw6TScWXIuZtZEPlzVLhMNulgiH3SwRDrtZIhx2s0T4FNcuMO7g2bnt8/Zendt+yyv1j0ycc94vm6ppuM7db12DMeovT/70zL9qMGXzp8faG3nJbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwvvZu8C/3Hldbvsru/bKbf/Ghz9avzEebaak16gnf95j1fzyYsyd3o/eSV6ymyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJ8H72Dlj3/fxujQ8cl3+++vLt++a273ps/UhL+n9jxuY2T7szf96taLQPXz0Nvp4NurIeM3lS3bbBZ5/LnXY0dhftJbtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgjvZy/A2MmTc9sf+cMlDV4h/3/u+/d5Ibd9wwOb67a9OJC/n/yPJz+Q237UXnvntrfiV9/MP/7gvD+4Nbf9mieOzW2fdOWb6rbtc8vW3GlHo4ZLdklLJG2RtKZq2MWSnpa0Orud2t4yzaxVw1mNvxo4pcbwyyNibna7udiyzKxoDcMeEauA9NZ5zEaZVjbQnSPpwWw1f0q9kSQtktQnqa+fHS3Mzsxa0WzYrwAOBeYCm4Gv1BsxIhZHRG9E9PZQvwNCM2uvpsIeEc9ExGBE7AKuBI4ptiwzK1pTYZc0o+rpR4A19cY1s+7QcD+7pOuBE4BpkjYCFwEnSJoLBLAe+GQba+x6L538ztz2sVrV0uvvOyb/vO9P7behhVdv3370Rv7m+J/ktv/JxLW57f+271G57eP/6zd12wb7d+ZOOxo1DHtEzK8x+Ko21GJmbeTDZc0S4bCbJcJhN0uEw26WCIfdLBGKiI7NbLKmxrE6sWPz65gGl2N+btmhue3fPvz63Pbtkb/r7YFXZ9dtmzbupdxpz5qcf0nlVn1w3ml12wZ+/WTutBqXv7NoNF7uuVV3x0peiq01r7HtJbtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulghfSroIuwZzm6d96NHc9ot4d5HVDJF/meuzNrW2n/3H2yfmtjfal57H+9GL5SW7WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYI72e3llwx521ll2DD5CW7WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpaIhmGXNEvS7ZLWSnpY0nnZ8KmSVkh6LLuf0v5yzaxZw1myDwAXRMQ7gfcAn5J0GPBZYGVEzAFWZs/NrEs1DHtEbI6I+7LH24C1wEzgNGBpNtpS4PR2FWlmrRvRb3ZJBwFHA3cD0yNiM1T+IQAHFF2cmRVn2GGXNBG4ETg/IvI7EHv9dIsk9Unq62dHMzWaWQGGFXZJPVSCfl1E/Cgb/IykGVn7DGBLrWkjYnFE9EZEbw/ji6jZzJownK3xAq4C1kbEV6ualgMLsscLgGXFl2dmRRnOKa7zgE8AD0lanQ37HHAp8ANJC4GngDPaU6K1Yv2Xfr/BGKtzW295xWtjo0XDsEfEXUDN/p6BUdjZutno5CPozBLhsJslwmE3S4TDbpYIh90sEQ67WSJ8KelR7mdnfbnBGBNyWz999cLc9ln8fIQVWVm8ZDdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuH97KPcs4P1zk6ueNOYwdz2WV/0fvTRwkt2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwR3s8+yp1/8Lzcdo0d2+AVBoorxkrlJbtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNuloiGYZc0S9LtktZKeljSednwiyU9LWl1dju1/eXaiEXk3mJgIPdmo8dwDqoZAC6IiPskTQLulbQia7s8Ihr1QmBmXaBh2CNiM7A5e7xN0lpgZrsLM7Nijeg3u6SDgKOBu7NB50h6UNISSVPqTLNIUp+kvn52tFSsmTVv2GGXNBG4ETg/Il4CrgAOBeZSWfJ/pdZ0EbE4InojoreH8QWUbGbNGFbYJfVQCfp1EfEjgIh4JiIGI2IXcCVwTPvKNLNWDWdrvICrgLUR8dWq4TOqRvsIsKb48sysKMPZGj8P+ATwkKTV2bDPAfMlzQUCWA98si0VmlkhhrM1/i6g1sXHby6+HDNrFx9BZ5YIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRKhiOjczKRngSerBk0DnutYASPTrbV1a13g2ppVZG2zI2L/Wg0dDfsbZi71RURvaQXk6NbaurUucG3N6lRtXo03S4TDbpaIssO+uOT55+nW2rq1LnBtzepIbaX+Zjezzil7yW5mHeKwmyWilLBLOkXSryQ9LumzZdRQj6T1kh7KuqHuK7mWJZK2SFpTNWyqpBWSHsvua/axV1JtXdGNd04346V+dmV3f97x3+ySxgKPAicBG4F7gPkR8T8dLaQOSeuB3ogo/QAMSccDLwPXRMQR2bDLgK0RcWn2j3JKRFzYJbVdDLxcdjfeWW9FM6q7GQdOB86mxM8up66P0oHPrYwl+zHA4xGxLiJ2AjcAp5VQR9eLiFXA1iGDTwOWZo+XUvmydFyd2rpCRGyOiPuyx9uA3d2Ml/rZ5dTVEWWEfSawoer5Rrqrv/cAbpV0r6RFZRdTw/SI2AyVLw9wQMn1DNWwG+9OGtLNeNd8ds10f96qMsJeqyupbtr/Ny8i3gV8APhUtrpqwzOsbrw7pUY3412h2e7PW1VG2DcCs6qeHwhsKqGOmiJiU3a/BbiJ7uuK+pndPehm91tKruc13dSNd61uxumCz67M7s/LCPs9wBxJB0vaC/gYsLyEOt5A0oRswwmSJgAn031dUS8HFmSPFwDLSqzldbqlG+963YxT8mdXevfnEdHxG3AqlS3yTwB/V0YNdeo6BHgguz1cdm3A9VRW6/qprBEtBN4MrAQey+6ndlFt3wMeAh6kEqwZJdV2HJWfhg8Cq7PbqWV/djl1deRz8+GyZonwEXRmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSL+D93mf2VTfXhJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    path, sample = model(None)\n",
    "    sample = sample.view(28, 28).detach().cpu().numpy()\n",
    "    path.draw()\n",
    "\n",
    "    plt.title('Sample from prior')\n",
    "    plt.imshow(sample)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoutingCategories] *",
   "language": "python",
   "name": "conda-env-RoutingCategories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
